Graph-Based 
Social Media 
Analysis
Edited by
Ioannis Pitas
Chapman & Hall/CRC 
Data Mining and Knowledge Discovery Series


Graph-Based 
Social Media 
Analysis

Chapman & Hall/CRC 
Data Mining and Knowledge Discovery Series
PUBLISHED TITLES
SERIES EDITOR
Vipin Kumar
University of Minnesota
Department of Computer Science and Engineering
Minneapolis, Minnesota, U.S.A.
AIMS AND SCOPE
This series aims to capture new developments and applications in data mining and knowledge 
discovery, while summarizing the computational tools and techniques useful in data analysis. This 
series encourages the integration of mathematical, statistical, and computational methods and 
techniques through the publication of a broad range of textbooks, reference works, and hand-
books. The inclusion of concrete examples and applications is highly encouraged. The scope of the 
series includes, but is not limited to, titles in the areas of data mining and knowledge discovery 
methods and applications, modeling, algorithms, theory and foundations, data and knowledge 
visualization, data mining systems and tools, and privacy and security issues. 
ACCELERATING DISCOVERY : MINING UNSTRUCTURED INFORMATION FOR 
HYPOTHESIS GENERATION 
Scott Spangler
ADVANCES IN MACHINE LEARNING AND DATA MINING FOR ASTRONOMY 
Michael J. Way, Jeffrey D. Scargle, Kamal M. Ali, and Ashok N. Srivastava
BIOLOGICAL DATA MINING 
Jake Y. Chen and Stefano Lonardi
COMPUTATIONAL BUSINESS ANALYTICS 
Subrata Das
COMPUTATIONAL INTELLIGENT DATA ANALYSIS FOR SUSTAINABLE 
DEVELOPMENT 
Ting Yu, Nitesh V. Chawla, and Simeon Simoff
COMPUTATIONAL METHODS OF FEATURE SELECTION 
Huan Liu and Hiroshi Motoda
CONSTRAINED CLUSTERING: ADVANCES IN ALGORITHMS, THEORY,  
AND APPLICATIONS 
Sugato Basu, Ian Davidson, and Kiri L. Wagstaff
CONTRAST DATA MINING: CONCEPTS, ALGORITHMS, AND APPLICATIONS  
Guozhu Dong and James Bailey
DATA CLASSIFICATION: ALGORITHMS AND APPLICATIONS 
Charu C. Aggarawal

DATA CLUSTERING: ALGORITHMS AND APPLICATIONS 
Charu C. Aggarawal and Chandan K. Reddy
DATA CLUSTERING IN C++: AN OBJECT-ORIENTED APPROACH 
Guojun Gan
DATA MINING FOR DESIGN AND MARKETING 
Yukio Ohsawa and Katsutoshi Yada 
DATA MINING WITH R: LEARNING WITH CASE STUDIES 
Luís Torgo
EVENT MINING: ALGORITHMS AND APPLICATIONS 
Tao Li
FOUNDATIONS OF PREDICTIVE ANALYTICS 
James Wu and Stephen Coggeshall
GEOGRAPHIC DATA MINING AND KNOWLEDGE DISCOVERY,  
SECOND EDITION 
Harvey J. Miller and Jiawei Han
GRAPH-BASED SOCIAL MEDIA ANALYSIS 
Ioannis Pitas
HANDBOOK OF EDUCATIONAL DATA MINING 
Cristóbal Romero, Sebastian Ventura, Mykola Pechenizkiy, and Ryan S.J.d. Baker
HEALTHCARE DATA ANALYTICS  
Chandan K. Reddy and Charu C. Aggarwal 
INFORMATION DISCOVERY ON ELECTRONIC HEALTH RECORDS 
Vagelis Hristidis
INTELLIGENT TECHNOLOGIES FOR WEB APPLICATIONS  
Priti Srinivas Sajja and Rajendra Akerkar
INTRODUCTION TO PRIVACY-PRESERVING DATA PUBLISHING: CONCEPTS 
AND TECHNIQUES 
Benjamin C. M. Fung, Ke Wang, Ada Wai-Chee Fu, and Philip S. Yu
KNOWLEDGE DISCOVERY FOR COUNTERTERRORISM AND  
LAW ENFORCEMENT 
David Skillicorn
KNOWLEDGE DISCOVERY FROM DATA STREAMS 
João Gama
MACHINE LEARNING AND KNOWLEDGE DISCOVERY FOR  
ENGINEERING SYSTEMS HEALTH MANAGEMENT 
Ashok N. Srivastava and Jiawei Han
MINING SOFTWARE SPECIFICATIONS: METHODOLOGIES AND APPLICATIONS 
David Lo, Siau-Cheng Khoo, Jiawei Han, and Chao Liu

MULTIMEDIA DATA MINING: A SYSTEMATIC INTRODUCTION TO  
CONCEPTS AND THEORY 
Zhongfei Zhang and Ruofei Zhang
MUSIC DATA MINING  
Tao Li, Mitsunori Ogihara, and George Tzanetakis
NEXT GENERATION OF DATA MINING 
Hillol Kargupta, Jiawei Han, Philip S. Yu, Rajeev Motwani, and Vipin Kumar
RAPIDMINER: DATA MINING USE CASES AND BUSINESS ANALYTICS 
APPLICATIONS 
Markus Hofmann and Ralf Klinkenberg
RELATIONAL DATA CLUSTERING: MODELS, ALGORITHMS,  
AND APPLICATIONS 
Bo Long, Zhongfei Zhang, and Philip S. Yu
SERVICE-ORIENTED DISTRIBUTED KNOWLEDGE DISCOVERY  
Domenico Talia and Paolo Trunfio
SPECTRAL FEATURE SELECTION FOR DATA MINING  
Zheng Alan Zhao and Huan Liu
STATISTICAL DATA MINING USING SAS APPLICATIONS, SECOND EDITION 
George Fernandez
SUPPORT VECTOR MACHINES: OPTIMIZATION BASED THEORY, 
ALGORITHMS, AND EXTENSIONS 
Naiyang Deng, Yingjie Tian, and Chunhua Zhang
TEMPORAL DATA MINING 
Theophano Mitsa
TEXT MINING: CLASSIFICATION, CLUSTERING, AND APPLICATIONS 
Ashok N. Srivastava and Mehran Sahami
THE TOP TEN ALGORITHMS IN DATA MINING 
Xindong Wu and Vipin Kumar 
UNDERSTANDING COMPLEX DATASETS: DATA MINING WITH MATRIX 
DECOMPOSITIONS
David Skillicorn

Graph-Based 
Social Media 
Analysis
Edited by
Ioannis Pitas
Aristotle University of Thessaloniki
Greece

CRC Press
Taylor & Francis Group
6000 Broken Sound Parkway NW, Suite 300
Boca Raton, FL 33487-2742
© 2016 by Taylor & Francis Group, LLC
CRC Press is an imprint of Taylor & Francis Group, an Informa business
No claim to original U.S. Government works
Version Date: 20151029
International Standard Book Number-13: 978-1-4987-1905-6 (eBook - PDF)
This book contains information obtained from authentic and highly regarded sources. Reasonable efforts have been 
made to publish reliable data and information, but the author and publisher cannot assume responsibility for the valid-
ity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright 
holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this 
form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may 
rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, or uti-
lized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopy-
ing, microfilming, and recording, or in any information storage or retrieval system, without written permission from the 
publishers.
For permission to photocopy or use material electronically from this work, please access www.copyright.com (http://
www.copyright.com/) or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 
978-750-8400. CCC is a not-for-profit organization that provides licenses and registration for a variety of users. For 
organizations that have been granted a photocopy license by the CCC, a separate system of payment has been arranged.
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and are used only for 
identification and explanation without intent to infringe.
Visit the Taylor & Francis Web site at
http://www.taylorandfrancis.com
and the CRC Press Web site at
http://www.crcpress.com

Contents
Preface
xiii
Contributors
xv
Editor Biography
xvii
1
Graphs in Social and Digital Media
1
Alexandros Iosiﬁdis, Nikolaos Tsapanos and Ioannis Pitas
1.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Dominant social networking/media platforms . . . . . . . . . . . . . . . . .
3
1.3
Collecting data from social media sites
. . . . . . . . . . . . . . . . . . . .
5
1.4
Social media graphs
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.4.1
Graphs from Facebook data . . . . . . . . . . . . . . . . . . . . . . .
8
1.4.2
Graphs from Twitter data . . . . . . . . . . . . . . . . . . . . . . . .
10
1.4.3
Graphs from bibliographic data . . . . . . . . . . . . . . . . . . . . .
12
1.5
Graph storage formats and visualization . . . . . . . . . . . . . . . . . . . .
14
1.6
Big data issues in social and digital media
. . . . . . . . . . . . . . . . . .
15
1.7
Distributed computing platforms . . . . . . . . . . . . . . . . . . . . . . . .
15
1.8
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
Bibliography
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
2
Mathematical Preliminaries: Graphs and Matrices
21
Nikolaos Tsapanos, Alexandros Iosiﬁdis and Ioannis Pitas
2.1
Graph basics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
2.2
Linear algebra tools
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
2.3
Matrix decompositions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
2.4
Vector and matrix derivatives
. . . . . . . . . . . . . . . . . . . . . . . . .
31
Bibliography
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
3
Algebraic Graph Analysis
35
Nikolaos Tsapanos, Anastasios Tefas and Ioannis Pitas
3.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
3.2
Spectral graph theory
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
3.2.1
Adjacency and Laplacian matrix . . . . . . . . . . . . . . . . . . . .
36
3.2.2
Similarity matrix and nearest neighbor graph . . . . . . . . . . . . .
37
3.3
Applications of graph analysis
. . . . . . . . . . . . . . . . . . . . . . . . .
38
3.4
Random graph generation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
3.4.1
Desirable random graph properties . . . . . . . . . . . . . . . . . . .
41
3.4.2
Random graph generation models . . . . . . . . . . . . . . . . . . . .
41
3.4.3
Spectral graph generation . . . . . . . . . . . . . . . . . . . . . . . .
43
3.5
Graph clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
3.5.1
Global clustering algorithms . . . . . . . . . . . . . . . . . . . . . . .
46
vii

viii
Contents
3.5.2
Local clustering algorithms
. . . . . . . . . . . . . . . . . . . . . . .
48
3.5.3
Spectral clustering algorithms . . . . . . . . . . . . . . . . . . . . . .
48
3.5.4
Overlapping community detection
. . . . . . . . . . . . . . . . . . .
50
3.6
Graph matching
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
3.6.1
Spectral graph matching . . . . . . . . . . . . . . . . . . . . . . . . .
53
3.6.2
Frequent subgraph mining . . . . . . . . . . . . . . . . . . . . . . . .
54
3.7
Random walks
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
3.8
Graph anomaly detection
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
3.8.1
Spectral anomaly detection . . . . . . . . . . . . . . . . . . . . . . .
57
3.9
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
Bibliography
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
4
Web Search Based on Ranking
67
Andrea Tagarelli and Santosh Kabbur and George Karypis
4.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
4.2
Information Retrieval Background
. . . . . . . . . . . . . . . . . . . . . . .
69
4.2.1
Document representation
. . . . . . . . . . . . . . . . . . . . . . . .
69
4.2.2
Retrieval models . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
4.3
Relevance Beyond the Web Page Text . . . . . . . . . . . . . . . . . . . . .
72
4.3.1
Anchor text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
4.3.2
Query expansion . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
4.4
Centrality and Prestige
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
4.4.1
Basic measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
4.4.2
Eigenvector centrality and prestige . . . . . . . . . . . . . . . . . . .
80
4.4.3
PageRank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
4.4.4
Hubs and authorities . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
4.4.5
SimRank
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
4.5
Topic-Sensitive Ranking . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
88
4.5.1
Content as topic . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
4.5.2
Trust as topic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
4.6
Ranking in Heterogeneous Networks
. . . . . . . . . . . . . . . . . . . . . .
92
4.6.1
Ranking in heterogeneous information networks . . . . . . . . . . . .
93
4.6.2
Ranking-Based clustering . . . . . . . . . . . . . . . . . . . . . . . .
95
4.7
Organizing Search Results
. . . . . . . . . . . . . . . . . . . . . . . . . . .
97
4.8
Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
Bibliography
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
100
5
Label Propagation and Information Diﬀusion in Graphs
107
Eftychia Fotiadou, Olga Zoidi and Ioannis Pitas
5.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
108
5.2
Graph construction approaches . . . . . . . . . . . . . . . . . . . . . . . . .
109
5.2.1
Neighborhood approaches . . . . . . . . . . . . . . . . . . . . . . . .
110
5.2.2
Local reconstruction approaches
. . . . . . . . . . . . . . . . . . . .
111
5.2.3
Metric learning approaches
. . . . . . . . . . . . . . . . . . . . . . .
113
5.2.4
Scalable graph construction methods . . . . . . . . . . . . . . . . . .
118
5.3
Label inference methods
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
120
5.3.1
Iterative algorithms
. . . . . . . . . . . . . . . . . . . . . . . . . . .
120
5.3.2
Random walks
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
122
5.3.3
Graph regularization . . . . . . . . . . . . . . . . . . . . . . . . . . .
123
5.3.4
Graph kernel regularization . . . . . . . . . . . . . . . . . . . . . . .
127
5.3.5
Inductive label inference . . . . . . . . . . . . . . . . . . . . . . . . .
128

Contents
ix
5.3.6
Label propagation on data with multiple representations . . . . . . .
129
5.3.7
Label propagation on hypergraphs . . . . . . . . . . . . . . . . . . .
131
5.3.8
Label propagation initialization . . . . . . . . . . . . . . . . . . . . .
132
5.3.9
Applications in digital media . . . . . . . . . . . . . . . . . . . . . .
133
5.4
Diﬀusion processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
134
5.4.1
Diﬀusion in physics . . . . . . . . . . . . . . . . . . . . . . . . . . . .
134
5.4.2
Diﬀusion in sociology
. . . . . . . . . . . . . . . . . . . . . . . . . .
135
5.4.3
Diﬀusion in social media . . . . . . . . . . . . . . . . . . . . . . . . .
135
5.5
Social network diﬀusion models
. . . . . . . . . . . . . . . . . . . . . . . .
136
5.5.1
Game theoretical diﬀusion models
. . . . . . . . . . . . . . . . . . .
137
5.5.2
Epidemic diﬀusion models . . . . . . . . . . . . . . . . . . . . . . . .
137
5.5.3
Threshold diﬀusion models
. . . . . . . . . . . . . . . . . . . . . . .
138
5.5.4
Cascade diﬀusion models
. . . . . . . . . . . . . . . . . . . . . . . .
139
5.5.5
Inﬂuence maximization
. . . . . . . . . . . . . . . . . . . . . . . . .
140
5.5.6
Cross-Media information diﬀusion
. . . . . . . . . . . . . . . . . . .
142
5.5.7
Other applications of information diﬀusion
. . . . . . . . . . . . . .
143
5.6
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
145
Bibliography
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
146
6
Graph-Based Pattern Classiﬁcation and Dimensionality Reduction
163
Alexandros Iosiﬁdis and Ioannis Pitas
6.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
163
6.2
Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
164
6.3
Unsupervised Methods
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
166
6.3.1
Locality Preserving Projections . . . . . . . . . . . . . . . . . . . . .
166
6.3.2
Locally Linear Embedding . . . . . . . . . . . . . . . . . . . . . . . .
167
6.3.3
ISOMAP
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
168
6.3.4
Laplacian Embedding
. . . . . . . . . . . . . . . . . . . . . . . . . .
168
6.3.5
Diﬀusion Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
168
6.4
Supervised Methods
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
169
6.4.1
Linear Discriminant Analysis . . . . . . . . . . . . . . . . . . . . . .
169
6.4.2
Marginal Fisher Analysis
. . . . . . . . . . . . . . . . . . . . . . . .
171
6.4.3
Local Fisher Discriminant Analysis . . . . . . . . . . . . . . . . . . .
171
6.4.4
Graph Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . .
172
6.4.5
Minimum Class Variance Extreme Learning Machine . . . . . . . . .
173
6.4.6
Minimum Class Variance Support Vector Machine
. . . . . . . . . .
174
6.4.7
Graph Embedded Support Vector Machines . . . . . . . . . . . . . .
174
6.5
Semi-Supervised Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . .
175
6.5.1
Semi-Supervised Discriminant Analysis
. . . . . . . . . . . . . . . .
176
6.5.2
Laplacian Support Vector Machine . . . . . . . . . . . . . . . . . . .
176
6.5.3
Semi-Supervised Extreme Learning Machine . . . . . . . . . . . . . .
177
6.6
Applications
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
177
6.7
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
178
Bibliography
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
179
7
Matrix and Tensor Factorization with Recommender System Applica-
tions
187
Panagiotis Symeonidis
7.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
187
7.2
Singular Value Decomposition on Matrices for Recommender Systems
. . .
189
7.2.1
Applying the SVD and Preserving the Largest Singular Values
. . .
190

x
Contents
7.2.2
Generating the Neighborhood of Users/Items . . . . . . . . . . . . .
191
7.2.3
Generating the Recommendation List
. . . . . . . . . . . . . . . . .
191
7.2.4
Inserting a Test User in the c-dimensional Space
. . . . . . . . . . .
192
7.2.5
Other Factorization Methods . . . . . . . . . . . . . . . . . . . . . .
192
7.3
Higher Order Singular Value Decomposition (HOSVD) on Tensors
. . . . .
193
7.3.1
From SVD to HOSVD . . . . . . . . . . . . . . . . . . . . . . . . . .
193
7.3.2
HOSVD for Recommendations in Social Tagging Systems . . . . . .
196
7.3.3
Handling the Sparsity Problem . . . . . . . . . . . . . . . . . . . . .
200
7.3.4
Inserting New Users, Tags, or Items
. . . . . . . . . . . . . . . . . .
201
7.3.5
Other Scalable Factorization Models . . . . . . . . . . . . . . . . . .
204
7.4
A Real Geo-Social System-Based on HOSVD
. . . . . . . . . . . . . . . . .
205
7.4.1
GeoSocialRec Website . . . . . . . . . . . . . . . . . . . . . . . . . .
205
7.4.2
GeoSocialRec Database and Recommendation Engine
. . . . . . . .
207
7.4.3
Experiments
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
208
7.5
Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
210
Bibliography
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
210
8
Multimedia Social Search Based on Hypergraph Learning
215
Constantine Kotropoulos
8.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
215
8.2
Hypergraphs
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
218
8.2.1
Uniform hypergraphs . . . . . . . . . . . . . . . . . . . . . . . . . . .
220
8.3
Game-Theoretic approaches to uniform hypergraph clustering
. . . . . . .
223
8.4
Spectral clustering for arbitrary hypergraphs
. . . . . . . . . . . . . . . . .
229
8.5
Ranking on hypergraphs
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
238
8.5.1
Enforcing structural constraints . . . . . . . . . . . . . . . . . . . . .
239
8.5.2
Learning hyperedge weights . . . . . . . . . . . . . . . . . . . . . . .
241
8.6
Applications
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
243
8.6.1
High-order web link analysis
. . . . . . . . . . . . . . . . . . . . . .
243
8.6.2
Hypergraph matching for object recognition . . . . . . . . . . . . . .
247
8.6.3
Music recommendation and personalized music tagging
. . . . . . .
249
8.6.4
Simultaneous image tagging and geo-location prediction . . . . . . .
251
8.6.5
Social image search exploiting joint visual-textual information . . . .
254
8.6.6
Annotation, classiﬁcation, and tourism recommendation driven by
probabilistic latent semantic analysis . . . . . . . . . . . . . . . . . .
256
8.7
Big data: Randomized methods for matrix/hypermatrix decompositions . .
261
8.8
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
265
8.9
Acknowledgments
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
267
Bibliography
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
267
9
Graph Signal Processing in Social Media
275
Sunil Narang
9.1
Motivation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
275
9.2
Graph signal processing (GSP) . . . . . . . . . . . . . . . . . . . . . . . . .
277
9.2.1
Basics of graph signal processing . . . . . . . . . . . . . . . . . . . .
277
9.2.2
Spectral representation of graph signals
. . . . . . . . . . . . . . . .
279
9.2.3
Downsampling in graphs . . . . . . . . . . . . . . . . . . . . . . . . .
280
9.2.4
Graph wavelets and ﬁlterbanks . . . . . . . . . . . . . . . . . . . . .
282
9.3
Applications
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
282
9.3.1
Information diﬀusion pattern analysis
. . . . . . . . . . . . . . . . .
282
9.3.2
Interpolation in graphs . . . . . . . . . . . . . . . . . . . . . . . . . .
284

Contents
xi
9.3.2.1
Movie recommendation system . . . . . . . . . . . . . . . .
286
9.4
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
289
Bibliography
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
289
10 Big Data Analytics for Social Networks
293
Brian Baingana, Panagiotis Traganitis, Georgios Giannakis and
Gonzalo Mateos
10.1 Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
294
10.1.1 Signal processing for big data . . . . . . . . . . . . . . . . . . . . . .
294
10.1.2 Social network analytics problems
. . . . . . . . . . . . . . . . . . .
295
10.2 Visualizing and reducing dimension in social nets
. . . . . . . . . . . . . .
296
10.2.1 Kernel-based graph embedding . . . . . . . . . . . . . . . . . . . . .
296
10.2.2 Centrality-constraints
. . . . . . . . . . . . . . . . . . . . . . . . . .
298
10.2.3 Numerical tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
300
10.2.4 Visualization of dynamic social networks . . . . . . . . . . . . . . . .
300
10.3 Inference and imputation on social graphs
. . . . . . . . . . . . . . . . . .
303
10.3.1 Distributed anomaly detection for social graphs . . . . . . . . . . . .
303
10.3.1.1 Anomaly detection via sparse plus low-rank decomposition
303
10.3.1.2 In-network processing algorithm . . . . . . . . . . . . . . .
305
10.3.1.3 Numerical tests
. . . . . . . . . . . . . . . . . . . . . . . .
306
10.3.2 Prediction from partially-observed network processes . . . . . . . . .
307
10.3.2.1 Semi-supervised prediction of network processes
. . . . . .
308
10.3.2.2 Data-driven dictionary learning
. . . . . . . . . . . . . . .
309
10.3.2.3 Numerical tests
. . . . . . . . . . . . . . . . . . . . . . . .
310
10.4 Unveiling communities in social networks
. . . . . . . . . . . . . . . . . . .
311
10.4.1 Big data spectral clustering . . . . . . . . . . . . . . . . . . . . . . .
312
10.4.1.1 Numerical tests
. . . . . . . . . . . . . . . . . . . . . . . .
315
10.4.2 Robust kernel PCA
. . . . . . . . . . . . . . . . . . . . . . . . . . .
316
10.4.2.1 Numerical tests
. . . . . . . . . . . . . . . . . . . . . . . .
319
10.5 Topology tracking from information cascades
. . . . . . . . . . . . . . . . .
319
10.5.1 Dynamic SEMs for tracking cascades . . . . . . . . . . . . . . . . . .
321
10.5.1.1 Model and problem statement
. . . . . . . . . . . . . . . .
322
10.5.1.2 Exponentially-weighted least-squares estimator . . . . . . .
323
10.5.2 Topology tracking algorithm
. . . . . . . . . . . . . . . . . . . . . .
324
10.5.2.1 Accelerated convergence . . . . . . . . . . . . . . . . . . . .
326
10.5.3 Real-Time operation . . . . . . . . . . . . . . . . . . . . . . . . . . .
327
10.5.3.1 Premature termination
. . . . . . . . . . . . . . . . . . . .
327
10.5.3.2 Stochastic gradient descent iterations . . . . . . . . . . . .
327
10.5.4 Experiments on real data
. . . . . . . . . . . . . . . . . . . . . . . .
328
10.6 Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
330
10.7 Acknowledgments
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
330
Bibliography
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
330
11 Semantic Model Adaptation for Evolving Big Social Data
341
Nikoletta Bassiou and Constantine Kotropoulos
11.1 Introduction to Social Data Evolution . . . . . . . . . . . . . . . . . . . . .
341
11.2 Latent Model Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
343
11.2.1 Incremental Latent Semantic Analysis . . . . . . . . . . . . . . . . .
343
11.2.2 Incremental Probabilistic Latent Semantic Analysis . . . . . . . . . .
346
11.2.3 Incremental Latent Dirichlet Allocation . . . . . . . . . . . . . . . .
355
11.3 Incremental Spectral Clustering
. . . . . . . . . . . . . . . . . . . . . . . .
359

xii
Contents
11.4 Tensor Model Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
362
11.4.1 Basic Tensor Concepts . . . . . . . . . . . . . . . . . . . . . . . . . .
362
11.4.2 Incremental Tensor Analysis . . . . . . . . . . . . . . . . . . . . . . .
363
11.5 Parallel and Distributed Approaches for Big Data Analysis
. . . . . . . . .
368
11.5.1 Parallel Probabilistic Latent Semantic Analysis . . . . . . . . . . . .
368
11.5.2 Parallel Latent Dirichlet Allocation . . . . . . . . . . . . . . . . . . .
369
11.5.3 Parallel Spectral Clustering . . . . . . . . . . . . . . . . . . . . . . .
371
11.5.4 Distributed Tensor Decomposition . . . . . . . . . . . . . . . . . . .
373
11.6 Applications to Evolving Social Data Analysis
. . . . . . . . . . . . . . . .
375
11.6.1 Incremental Label Propagation . . . . . . . . . . . . . . . . . . . . .
375
11.6.2 Incremental Graph Clustering in Dynamic Social Networks
. . . . .
376
11.7 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
379
Bibliography
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
381
12 Big Graph Storage, Processing and Visualization
391
Jaroslav Pokorny and Vaclav Snasel
12.1 Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
391
12.2 Basic Notions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
393
12.3 Big Graph Data Storage
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
395
12.3.1 DBMS Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . .
395
12.3.2 Graph DBMSs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
397
12.3.3 Storing and indexing graph structures . . . . . . . . . . . . . . . . .
399
12.4 Graph Data Processing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
401
12.4.1 Querying graphs in relational DBMS . . . . . . . . . . . . . . . . . .
402
12.4.2 Graph querying in Datalog
. . . . . . . . . . . . . . . . . . . . . . .
403
12.4.3 Query languages in graph DBMS . . . . . . . . . . . . . . . . . . . .
403
12.5 Graph Data Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . .
405
12.5.1 Static graph visualization . . . . . . . . . . . . . . . . . . . . . . . .
406
12.5.2 Dynamic graph visualization
. . . . . . . . . . . . . . . . . . . . . .
408
12.6 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
409
Bibliography
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
411
Index
417

Preface
The great recent changes in the World Wide Web (WWW) facilitated media information
sharing among users, user-based content creation and remote collaboration. Nowadays, so-
cial networks allow users to post public proﬁles and share them with their friends, thus
creating virtual communities and networks. Posting relevant media data (notably images,
videos, music and text) in social media sites enabled a dramatic increase of multimedia data
storage, communication and consumption. Social media are concentrated on the creation
and exchange of user-generated content, allowing users to create, search, share, rate and
access multimedia data, thus creating a totally new media experience. Furthermore, we
now see a rapid convergence of classical broadcasted media and social media, where content
seamlessly ﬂows and is exchanged between these two worlds.
This edited book focuses on the use of graph analysis in the study of social media
and digital media. It covers the following topics: graphs in social media, graph and hyper-
graph fundamentals, mathematical foundations coming from linear algebra, algebraic graph
analysis, graph clustering, community detection, graph matching, web search based on rank-
ing, label propagation and diﬀusion in social media, graph-based pattern recognition and
machine learning, matrix and tensor decomposition factorization in multimedia recommen-
dations, multimedia social search based on hypergraph learning, graph signal processing,
big data approaches for social media analytics, evolving social data analysis and big graph
storage, processing and visualization. Emphasis is on big data aspects, as social media ad-
dress inherently strong big data issues related both to the size of the stored multimedia
content and to the social graph size.
This book addresses an important scientiﬁc and technological challenge, namely the
conﬂuence of graph analysis and network theory with linear algebra, digital media, machine
learning, big data analysis and signal processing. I ﬁrmly believe that this convergence
can develop novel ground-breaking approaches in social media and digital media analysis,
towards unleashing their full potential. To this end, novel lines of research can be followed in:
a) graph-based approaches in media representation (e.g., graph signals), machine learning
(e.g., graph embedding techniques), use of context (notably rich tags and associated graphs),
b) big data analysis methods (distributed, approximate, sub-sampled or adaptive analysis)
and c) incremental methods to address the dynamic/evolving nature of social media data.
It can be used as a textbook in undergraduate or graduate courses on digital media,
social media or social networks. Its targeted audience is University students in computer
science, electrical engineering, mathematics, information science, digital media or sociology
departments that want good scientiﬁc and technical coverage of social media and digital
media analysis. The number of relevant courses oﬀered steadily increases with the popularity
of social media. Furthermore, almost all major Universities oﬀer courses on web topics, or
on digital media/multimedia, which have special parts devoted to social media/networks.
Students in the above mentioned ﬁelds can now have a complete reference textbook in
graph-based social media analysis. Finally, scientists and engineers working in social media
and in digital media production and distribution, as well as the general public with scientiﬁc
interests, can obtain a thorough understanding of social media analysis.
xiii

xiv
Preface
The practical outcome for the audience can be the following:
1. Understanding the social media structure.
2. Understanding graph theory, particularly the algebraic description and analysis of
graphs and their use in social media studies.
3. Helping scientists, artists and sociologists understand complex social media phenom-
ena, like information diﬀusion, marketing and recommendation systems in social media
and evolving systems.
4. Acquiring expertise to analyze the social and digital media markets.
5. Providing insight into processing, storing and visualizing big social media data and
social graphs.
As an editor, I would like to thank the various chapter authors for their excellent work
in compiling a very good overview of graph-based social media analysis. Furthermore, I am
grateful to Mr. N. Tsapanos and Ms. F. Patrona for helping to prepare this edited book.
Finally, I would also like to thank the following reviewers of the various book chapters: G.
Arce, B. Baingana, N. Barbalios, N. Bassiou, A. Bors, T. Chen, A. Delopoulos, X. Dong,
B. Huet, G. Giannakis, W. Hu, A. Iosiﬁdis, G. Karypis, E. Koﬁdis, I. Kompatsiaris, C.
Kotropoulos, O. Lezoray, A. Lykas, Y. Manolopoulos, S. Narang, J. Pokorny, D. Rafailidis,
P. Simeonidis, A. Tefas, S. Theodoridis, N. Tsitsas, A. Vakali, P. Vandergheynst, N. Vretos
and O. Zoidi.
Acknowledgements. Chapters 1 and 5 are related to the research done within the EU
funded FP7 R & D projects IMPART and 3DVTS, respectively.
Prof. Ioannis Pitas
Department of Informatics
Aristotle University of Thessaloniki
Thessaloniki, Greece

Contributors
Brian Baingana
University of Minnesota
Minneapolis, U.S.A.
Nikoletta Bassiou
Aristotle University of Thessaloniki
Thessaloniki, Greece
Eftychia Fotiadou
Aristotle University of Thessaloniki
Thessaloniki, Greece
Georgios Giannakis
University of Minnesota
Minneapolis, U.S.A.
Alexandros Iosiﬁdis
Aristotle University of Thessaloniki
Thessaloniki, Greece
Santosh Kabbur
University of Minnesota
Minneapolis, U.S.A.
George Karypis
University of Minnesota
Minneapolis, U.S.A.
Constantine Kotropoulos
Aristotle University of Thessaloniki
Thessaloniki, Greece
Gonzalo Mateos
University of Rochester
New York, U.S.A.
Sunil Narang
University of Southern California
Los Angeles, U.S.A.
Ioannis Pitas
Aristotle University of Thessaloniki
Thessaloniki, Greece
Jaroslav Pokorny
Charles University
Prague, Czech Republic
Vaclav Snasel
Charles University
Prague, Czech Republic
Panagiotis Symeonidis
Aristotle University of Thessaloniki
Thessaloniki, Greece
Andrea Tagarelli
University of Calabria
Arcavacata di Rende (CS), Italy
Anastasios Tefas
Aristotle University of Thessaloniki
Thessaloniki, Greece
Panagiotis Traganitis
University of Minnesota
Minneapolis, U.S.A.
Nikolaos Tsapanos
Aristotle University of Thessaloniki
Thessaloniki, Greece
Olga Zoidi
Aristotle University of Thessaloniki
Thessaloniki, Greece
xv


Editor Biography
Prof. Ioannis Pitas (IEEE fellow, IEEE Distinguished Lec-
turer, EURASIP fellow) received a diploma and PhD degree from
the Department of Electrical Engineering, Aristotle University of
Thessaloniki, Greece. Since 1994, he has been a Professor in the
Department of Informatics of the same University. He has served
as a Visiting Professor at several universities. His current inter-
ests are in the areas of intelligent digital media, image/video
processing, machine learning and human centered computing.
He has published over 800 papers, contributed to 44 books in
his areas of interest and edited or (co-)authored another 10 books. He has also been a mem-
ber of the program committee of many scientiﬁc conferences and workshops. In the past,
he served as Associate Editor or co-Editor of eight international journals and was General
or Technical Chair of four international conferences. He participated in 68 R&D projects,
primarily funded by the European Union and is/was a principal investigator/researcher in
40 such projects. He has 20600+ citations to his work and an h-index of 67+ (2015).
Contact information:
Prof. Ioannis Pitas, Department of Informatics
Aristotle University of Thessaloniki,
Thessaloniki 54124, Greece
http://www.aiia.csd.auth.gr/
Email: pitas@aiia.csd.auth.gr
xvii


Chapter 1
Graphs in Social and Digital Media
Alexandros Iosiﬁdis, Nikolaos Tsapanos and Ioannis Pitas
Aristotle University of Thessaloniki, Greece
1.1
Introduction ....................................................................
1
1.2
Dominant social networking/media platforms .................................
3
1.3
Collecting data from social media sites ........................................
5
1.4
Social media graphs ............................................................
8
1.4.1
Graphs from Facebook data ...........................................
8
1.4.2
Graphs from Twitter data .............................................
10
1.4.3
Graphs from bibliographic data .......................................
12
1.5
Graph storage formats and visualization ......................................
14
1.6
Big data issues in social and digital media ....................................
15
1.7
Distributed computing platforms ..............................................
15
1.8
Conclusions .....................................................................
18
Bibliography ....................................................................
18
1.1
Introduction
Over the past few years, great changes have occurred in the World Wide Web (WWW).
Web evolution has introduced new applications, which facilitate the interactive exchange of
information, user-based content creation, and remote collaboration. Contrary to the static
web pages prevalent in the past, where users could only access information, current web
applications allow users to communicate, upload, and modify content. Social networks are
web-based services that allow their users to create a public proﬁle, create a list of users
with whom they share connections and material, and view such connections within the
system [Ell08]. Such prominent social network applications, which have nowadays become
part of everyday life and are deﬁnitely a very important factor in web development, con-
tinue growing, while many web services renovate to meet growing demand and provide new
functionalities.
Naturally, these trends have dramatically increased data ﬂow, especially that of multi-
media content (images, videos, music, text), which is now more than ever “user generated.”
One of the most important revelations in this transition is undoubtedly the emergence of
social media. Social media is the social interaction among people in which they create,
share, or exchange information and ideas in virtual communities and networks [ABHH08].
Social media have also been deﬁned as a group of Internet-based applications that build on
the ideological and technological foundations of Web 2.0 and that allow the creation and
exchange of user-generated content [KH10]. These applications allow users to join commu-
nities and create, ﬁnd, share, rate, and access the multimedia data that is in the respective
social media sites. They also allow people to ﬁnd, connect, inform, and inspire others. It
1

2
Graph-Based Social Media Analysis
should be noted here that the term social media is used both for the above described tools,
as well as for the multimedia data describing social activity of the users. In this book this
term will be used interchangeably for describing both tools and media data.
Typically, traditional media, e.g., newspapers, magazines, TV, and radio, provide one-
way information diﬀusion, since the audience can read or listen, but cannot share his/her
opinion on a subject, but to a very limited extent. On the contrary, social media provide
two-way information diﬀusion that allows the user to share his/her information with other
users, access their information, and communicate with them.
This information communication and sharing can be simple, such as commenting on
one’s status on social networks, or rating a video in video sharing sites, or more complex
such as movie recommendation, based on user proﬁling and clustering techniques. Note that
social media are tools with which users can generate content, contrary to social networking
websites, whose purpose is to connect users and allow interaction among them. Social media
popularity has oﬀered countless opportunities for creating not only personal, but also pro-
fessional relationships. This is why webometrics, i.e., web statistics and analytics sites, like
Alexa and Google Analytics, show that the most popular websites are Social Media-based
pages [Ell08].
Some popular Social Media platforms and their purpose follow:
• Social networking sites: Facebook, Hi5, Last.FM, LinkedIn, MySpace.
• Video and image sharing sites: YouTube, Flickr, Picasa.
• Collaborative work sites: Wikipedia, Wikia, Wikitravel.
• Social bookmarking services: Del.icio.us, Digg, Blinklist, Simpy.
• Blogging and micro-blogging: Blogger, ExpressionEngine, LiveJournal, Twitter, Word-
press.
• Virtual worlds for social gaming: Active Worlds, Forterra Systems, Second Life.
Before proceeding to the presentation of popular social networks and their relevant
data, we describe how such networks can be depicted by exploiting graphs [dSP65]. Let us
consider a toy example social network of a small number of users. By exploiting social media
data, each user i can be represented by a feature vector vi, as illustrated in Figure 1.1.1.
Such a feature vector vi is an abstraction of the media data (e.g., images, video, tags) that
reside inside a user model i. It must be noted though that, typically, such data are highly
unstructured and heterogeneous. As will be further described in the following sections, this
social network can be represented by a graph G(V, E, W), where each graph node vi ∈V
represents a unique user and graph edges eij ∈E between nodes exist, whenever a relation
connects two users i and j. The matrix W contains the weight values corresponding to the
graph edges eij. Edges eij can be established by using graph node relationships deﬁned
by the corresponding social networks. For example, an edge eij connects two users i and
j if they have a “friendship” relationship in a social network. Alternatively, edges eij can
be established by using similarity measures deﬁned over the node representations of G.
For example, an edge eij will connect two users i and j, if the user representations vi
and vj are “similar,” according to a similarity measure s(vi, vj). In the ﬁrst case, the
corresponding graph is usually unweighted, i.e., wij = 1 or 0, in the case where the users
i and j are connected or unconnected, respectively. In the latter case, the corresponding
graph is usually weighted, i.e., wij take values deﬁned by the similarity measure s(vi, vj).
Typically, such similarities are scaled in the range [0, 1] or [−1, 1]. We consider that the i-th
and j-th graph nodes are connected, if eij exists or, equivalently, if wij ̸= 0.

Graphs in Social and Digital Media
3
FIGURE 1.1.1: A toy example of a social graph.
1.2
Dominant social networking/media platforms
In this section, some of the most widely used social media/network platforms, namely
YouTube, Facebook, Twitter, Instagram, Flickr and LinkedIn, are presented, along with the
services and information they provide to users. These platforms are not only very popular,
but they also cover a wide range of social networking services. Subsequently, Blogs and
bibliographic databases are brieﬂy described.
YouTube is a video sharing website, where users can upload, view, rate videos, and com-
ment on them. It was established in 2005 as an independent video sharing service. In 2006,
it became part of Google’s services. Registration and use is free of charge, since its proﬁts
come from advertisements. According to YouTube statistics, 100 hours of video are uploaded
every minute [You]. It is also estimated that nearly 2 billion videos are being viewed daily.
Most videos in YouTube come from regular users and some come from businesses, orga-
nizations, etc. YouTube content is typically user-generated. Two other interesting aspects
of YouTube are a) the ability to embed videos in other sites and b) the YouTube recom-
mendation system. The latter is a very important and complex mechanism that employs
information about user activities and interests, in order to recommend videos.
Founded in 2004, Facebook is a social networking service that connects people with
friends, family, etc. As of March 2014, it had over one billion users, making it the largest
social network on the web, in terms of user base and global reach. After registration, users
are able to update their proﬁle and add other users in their friend list. Each user is able
to upload photographs and videos, exchange messages with other users, share his or her
thoughts by updating the status of his/her proﬁle, share external website URLs, play games
and much more. Facebook users can decide whether they want their proﬁle media content
to be public or hidden. In addition, users are able to create web pages, called groups, which
are devoted to common interests with other users, and advertisement web pages, where they
can, e.g., promote their business. One of the most characteristic Facebook features is the
Like button. Moreover, the users are able to comment on photos, videos and status of their
friends. This allows users to express their opinion about a status, a photo, a brand etc, thus
providing information for both content and user proﬁle analysis.

4
Graph-Based Social Media Analysis
Twitter was launched in 2006 as a social platform, whose main purpose was the creation
of microblogs, i.e., posts (tweets) that are no more than 140 characters long. Today, with
more than 280 million active users, it has become a popular platform of asynchronous
communication, where people can exchange ideas, and opinions, and promote themselves
or their companies. Compared to other social networking sites, like Facebook, it provides
fewer networking services. It has been designed to provide each user with a proﬁle containing
some personal information, according to user’s privacy settings. A user’s proﬁle is the ‘place’
where one’s posts can be seen. The relationship established between two users is that of
followee-follower. A user (follower), by choosing to follow another Twitter user (followee),
has the ability to see all the followee’s posts on his/her homepage. Thus, a user homepage
turns into a sort of newspaper, where all relevant posts are gathered and can be viewed.
Hashtags, i.e., one or more words collected into unspaced labels preﬁxed by the # sign,
are one of the core features of Twitter, helping users discover content and other people
relevant to their interests. Twitter is available on all sorts of portable devices, such as
tablets, smartphones, etc.
Founded in 2010, Instagram is a photo/video sharing and social networking service. Its
name is a combination of “instant camera” and “telegram.” It is now owned by Facebook. As
of November 2013, Instagram had over 200 million monthly active users, sharing more than
20 billion photos and videos. Instagram users can capture pictures or videos, edit them by
using mobile application tools (crop, rotate, apply predeﬁned ﬁlters and tilt-shift) and share
them with their followers. Each photo or video can be associated with a certain Foursquare
venue. This is made possible by using the GPS on the capture device and the search feature
provided by Instagram. Regarding its social aspect, Instagram users can connect with others
by using the Follow function. The service has adopted the same follower model also used in
Twitter. If a user is characterized as a “public” person, he or she can be followed by anyone,
whereas, if someone is characterized as a “private” person, he/she must approve each follow
request separately. Users can like or comment on photos shared by public or private proﬁles
that they follow. Similar to Twitter, hash tags are one of the core features of Instagram,
helping users to discover content and other people.
Flickr is a website where people can upload and share pictures or videos. It was created
in 2004 and became part of Yahoo! services. Today it has more than 87 million users. The
platform oﬀers two kinds of user accounts: Free and Pro. Users of Free accounts can upload
up to 300 MB of information every month, while users of Pro accounts have unlimited space
and bandwidth. The site automatically deletes users who are not active for more than 90
days. Besides uploading and downloading pictures and videos, users also have the ability
to comment, describe, tag, categorize data and edit privacy settings, i.e., specify which
users can view their posts and categorize their data. The users can also include data in
third party sites, like Facebook, Twitter, several blogs, etc. Last, the services of Flickr are
available on portable devices and can be also accessed via RSS feeds, emails and posts from
other sites.
LinkedIn is a business-oriented social network. It was created in 2003 and today has
more than 259 million users in more than 200 countries [Hem13]. The basic functionality
of LinkedIn allows users to create proﬁles and connections with other users. Each user
is able to update his/her professional proﬁle, exchange messages with other users, share
his/her thoughts by uploading the status of his/her proﬁle, comment on the status of his/her
connections, follow company accounts and seek for professional opportunities. In addition,
users are able to create and follow Groups, which mainly concern professional and career
issues. Groups keep their members informed through messages containing updates to the
group.
Blogs (derived from the word weblog) are informational sites, in which content is up-
loaded in the form of posts, which mostly consist of text and, sometimes, images, embedded

Graphs in Social and Digital Media
5
video clips or sound ﬁles. Each post is usually tagged with relevant labels that also serve
as search terms. Newer posts are usually sorted before older ones, in reverse chronological
order. Originally, each blog was the work of a single person and often covered a single sub-
ject. Recently, multi-author blogs (MABs) have been developed, where a large number of
authors are allowed to write posts. Examples of MABs are blogs operated by newspapers
and universities. The majority of blogs is interactive, allowing visitors to interact by com-
menting the posts. Thus, blogs can be considered to be social networking services. A typical
blog includes text, images, videos and links to other web pages or blogs.
In scientiﬁc publishing, information appearing in bibliographic databases, like Thomson
ISI’s Web of Science and MEDLINE, has been exploited for the study of scientiﬁc activity
and the importance of scientiﬁc articles, journals and scientists, typically by counting cita-
tions to each paper. Such information is studied by bibliometrics [DB09]. It has attracted
much attention, since the realization that bibliographic data have a natural mathematical
representation using graph structures as citations are essentially links between one paper
and the papers to which it refers [dSP65]. Such structures have led to the creation of au-
thor networks and citation networks, describing connections between scientists and speciﬁc
contributions, respectively [dSP65].
1.3
Collecting data from social media sites
Social media can provide social metrics, i.e., measurable data that can be accessed by
users. Social metrics are very important, in that they can be used in social graph analysis
to extract useful information, termed as social analytics [Sch96]. For example, the number
of views of a YouTube video, or the number of “Likes” of a Facebook group post can be
further analyzed in order to deﬁne trends or cascades. Collecting data from social media
may also be handy, in order to visualize social graphs, their structure and their evolution.
Such social graphs may be either static, describing a ‘snapshot’ of the social network in
time instance t = t0, or evolving over time, having the form G(V(t), E(t), W(t)). In the
latter case, transitions of the graph structures from an initial state to future states can be
evaluated and analyzed [PBV+07].
To satisfy the need of gathering data, social media sites provide Application Programming
Interfaces (APIs) that allow users to interact with their database and have access to media
content data, metadata, statistics, and other social metrics. In the following, we describe
some of the functionalities and the data types that are available for download.
Being one of the largest video databases on the web, YouTube has one of the most
powerful and complete APIs. The available YouTube data can be either video-oriented or
user-oriented and can be seen in Tables 1.3.1 and 1.3.2, respectively. Some of the above
data can only be accessed if their creator allows public access to them. In addition, the
API allows a programmer to use the application/platform and its services. For instance one
can search for videos, ﬁlter the results, upload videos, etc. A very important feature is that
of YouTube recommended videos. While displaying search results, YouTube returns a list
of, at most, 50 recommended videos to a speciﬁc user. Such a feature can be very useful,
since it allows access to parts of the YouTube recommendation graph for further analysis
[TPP14].
Facebook has a vast API that provides a high level of interaction with their database. The
data types that can be accessed from Facebook are shown in Table 1.3.3. The Facebook API
provides almost all data that appear in a user proﬁle. However, the user needs the correct

6
Graph-Based Social Media Analysis
TABLE 1.3.1: Video-Oriented data available in YouTube.
Type
Data
Entry
Unique video ID
Video title
Author name and address
Video category (e.g., comedy)
Date published
Last update
Video description
Video keywords
Video duration in seconds
Minimum, Maximum, Average rating
Number of users who rated
Thumbnails
Content/User
Number of views
Statistics
Number of users having the video to their favorite list
User’s last access on YouTube
Number of videos the user has viewed
Number of users subscribed to this channel
Total upload views on the user’s channel
Comments
Number of comments
Comments feed link
TABLE 1.3.2: User-Oriented data available in YouTube.
Data
ID (URI)
First name
Last name
Age
Gender
Location
Relationship
Occupation
School
List of hobbies
List of movies
List of music
List of books
Hometown
Various descriptions regarding the user
permissions and access token to access them. Data of particular interest for further analysis
of the Facebook graph include the “Likes,” expressing the user’s appreciation of a status,
a photo, a brand, etc., which can be used in combination with users’ personal details to
estimate trends, e.g., on new products. In addition, information related to tags appearing in

Graphs in Social and Digital Media
7
TABLE 1.3.3: Data available on Facebook.
Type
Data
Users
Person using Facebook (ID, name)
List of user’s friends
Photo album
Facebook message
User’s photos published in Facebook
User’s tag of the user at a place in an object
Post published in Facebook
Object
Set of comments on a particular object
Set of Likes on a particular object
Shares of a particular object
Usage metrics for several object types
App
Facebook app (ID)
An app link object created by an app
Review of a Facebook app
User gaining a game achievement in an App
Games achievement type created by an App
Others
Comment published on any other node
Link shared on Facebook
Facebook group
Status message or post published to Facebook
Message thread in Facebook Messages
Event
Video published in Facebook
Web domain claimed within Facebook Insights
Facebook Page
Facebook Page milestone
Oﬀer published by a Page
Details of a payment made via Facebook
Test user created by a Facebook App
Thumbnails
Information related to shares, Open Graph
and App Links about a URL
media data, like photos, can be exploited in order to create diﬀerent types of social graphs,
e.g., graphs of closely related Facebook friends, as detailed in Section 1.4.
Tweets are the fundamental ingredients of Twitter. They consist of 140 character long
messages, along with additional meta-data. The data available on the Twitter platform are
illustrated in Table 1.3.4.
Similar to YouTube, Flickr allows users to access vital information via its API. Specif-
ically, it provides an XML description that contains the information appearing in Table
1.3.5.
Regarding author and citation networks, analysis of huge bibliographic datasets allows
the collection of data that can be exploited for the analysis of scientiﬁc activity. Such
data include the name of a scientist, his/her aﬃliation and research ﬁelds, a list of papers
published in journals and conferences, a list of journals and conferences where his/her work
has been published, the number of citations and a list of papers referring to his/her work,
as well as the relationships between scientists, like advisor, advisee and co-author.

8
Graph-Based Social Media Analysis
TABLE 1.3.4: Data available in Twitter.
Type
Data
Users
User’s ID
User’s name
Location
Number of followers
Number of favorite tweets
Number of tweets the user has made
Number of friends the user has
Followers
The followees that a person is following
The users that follow a person
Relationship between two users (e.g. friendship)
Tweet
A speciﬁc tweet (ID)
100 most recent re-tweets of the tweet
Up to 100 user id’s who have re-tweeted the tweet (id)
Hashtags appearing in the tweet
Media data (e.g., images) appearing in the tweet
Links appearing in the tweet
Users mentioning the tweet
Additional information about a tweet’s content
Search
A collection of tweets that answer a certain question
Favorites
20 of the most recent tweets that were favored by the user
Number of comments
Trends
Top 10 most popular topics for a particular WOEID
(Yahoo! Where On Earth ID)
1.4
Social media graphs
After roughly describing some of the most popular social networks and the data that
can be retrieved from them by using the corresponding APIs, we describe relevant graph
structures that can be constructed and analyzed, as described in subsequent chapters. We
shall focus our attention on graph structures that can be constructed by using Facebook
and Twitter data. While the remaining social media are very popular, most of their users
either do not give permission to access their data, or there are no user-item connections.
For example, most YouTube users exploit the ability to view videos without creating an
account.
1.4.1
Graphs from Facebook data
As previously described, friendship is the basic relationship between two Facebook users.
By using this information, a friendship graph G(V, E, W) can be constructed, where each
graph node vi ∈V represents a unique user (ID) and graph edges eij ∈E denote a friendship
relationship between two users, i and j. Matrix W contains the weight values corresponding
to the graph edges eij. Because the friendship relationship is undirected, the friendship graph
is also undirected, i.e., wij = wji. Since the friends of a user typically also have their own
friends, the friendship graph of the user can be expanded to include friends of friends. An

Graphs in Social and Digital Media
9
TABLE 1.3.5: Data available in Flickr.
Type
Data
User
Username
User’s name
Location
URL of user’s photos
URL of user’s proﬁle
Date of ﬁrst photo
Total number of user photos
Photo
Title
Description
Photo owner
Visibility, i.e., public or private photo
Dates
Date published
Date the photo was taken
Last update
Editability, i.e., if a user can edit this picture or not
Statistics
Number of photo views
Number of comments
Number of user “favorites”
example of such a friendship graph depicting a friend and friend-of-friend relationship is
shown in Figure 1.4.1. The weights of the edges appearing in this graph are equal to one,
wij = 1. It can be observed that, in this graph three groups (person clusters) are formed
indicating that the nodes (persons) belonging to each cluster are better connected to each
other, when compared to nodes belonging to diﬀerent clusters.
Using such graphs, one can estimate the number of friendship relationships needed, in
order to connect two users. This is also known as the number of hops used to connect the two
users. It has been estimated that the mean number of hops needed to connect two random
nodes in the Facebook friendship graph is equal to 4.74 [UKBM11, BBR+12]. Furthermore,
research ﬁndings indicate that the number of users that can be accessed within three hops
is relatively small, while each additional hop enlarges considerably the number of users that
can be accessed. This is also known as the Six Degrees of Separation theory [Bar03].
Since it is obvious that the structure of such a dense friendship graph can represent only
aspects of the real relationships between the users, data provided by the Facebook API can
be exploited, in order to construct graph structures that better describe the relationships
between users. For instance, one can exploit the photo tagging information provided by
the users, when tagging themselves and their friends in their photos, in order to explore
groups of friends that are also related in real life. For example, if two persons are both
tagged in multiple photos, then there is high probability that they are strongly related in
real life as well. By using this additional information, the second degree friendship graph
in Figure 1.4.1 can be transformed to the one shown in Figure 1.4.2. Since the number of
mutual photo tagging of two users may vary, the graph shown in Figure 1.4.2 is a weighted
graph, where the weight of an edge connecting two graph nodes represents the total number
of common photo tags for two connected users. By observing this graph, it can be seen
that several small groups of closely related friends are created (also noted as cliques). This
indicates that the users forming these groups are probably more strongly related in real life
too. Therefore, it is manifested that real relationships between a person and other users

10
Graph-Based Social Media Analysis
FIGURE 1.4.1: A graph describing the friendship relationships between a user and the
friends of his/her friends on Facebook.
can be revealed by exploiting data available in social media, e.g., in Facebook. It should be
noted here that, while tagging information used in this example should be provided by the
users, ways for automatic metadata extraction, using computer vision techniques have also
been explored, e.g., by employing face veriﬁcation techniques [TYRW14].
Several other statistics related to users, like the relationship between the age of connected
users, can also be revealed by analyzing the available Facebook data [UKBM11]. Users tend
to connect to other users belonging to the same age category, while most of the users are
likely to have multiple friends belonging to the age range of the 20s [UKBM11]. Another
statistic related to the Facebook friendship graph refers to degree of each user node, i.e.,
the number of friends a user has [UKBM11].
Finally, an interesting graph structure can be constructed by using the available Face-
book data to form the adjacency matrix A of the friendship graph constructed by taking into
account the residence information of the Facebook users living in 54 countries [UKBM11].
The block structure of this adjacency matrix proves that most of the Facebook friendships
are between users living in the same country. Speciﬁcally, 84.2% of the graph edges connect
users living to the same country. In addition, this adjacency matrix can show friendships
between users living in diﬀerent countries.
1.4.2
Graphs from Twitter data
As previously described, the basic relationship between two Twitter users is that of
following. By using this information, a follower graph G(V, E, W) can be constructed, where
each node vi ∈V of the graph represents a unique user (ID) and graph edges eij ∈E
between nodes exist whenever a followee-follower relationship connects two users, where
user i follows user j. The matrix W contains the weight values corresponding to the graph
edges eij. Because such a relationship is non-symmetric, the follower graph is a directed

Graphs in Social and Digital Media
11
FIGURE 1.4.2: A graph describing the friendship relationships between a user and the
friends of his/her friends on Facebook exploiting photo tagging information.
one, i.e., wij can be diﬀerent from wji. Since the followees that a user follows can also follow
other users, the follower graph of a user can be expanded, so as to include followers of the
followers. An example of such a graph is shown in Figure 1.4.3. It should be noted here
that the graphs in Figures 1.4.1 and 1.4.3 have been created using the proﬁles of the same
person. Compared to the Facebook friendship graph, it can be seen that the Twitter graph
is much bigger and denser, consisting of 90,000 nodes. This can be attributed to Twitter’s
inherent characteristics, where the objective is to connect with multiple users and exchange
ideas, essentially forming an active newspaper, as has been described in Section 1.3. Most
of this paragraph also applies to Instagram, as it is similar to Twitter in this regard. An
example of an Instagram graph can be found in Figure 1.4.4.
By observing the graph in Figure 1.4.3, it can be seen that its analysis for determining
meaningful clusters or cliques is diﬃcult. However, by exploiting the data provided by the
Twitter API, one can construct graph structures that better describe user interests. For
instance, one can exploit the hashtag information provided by the users, when they express
their opinion on a subject, in order to explore groups of users who share interests. By using
this additional information, the enhanced ﬁrst-degree follower graph of the same person can
be seen in Figure 1.4.5. This graph is formed by the users followed by a particular user
(the same as in Figures 1.4.1 and 1.4.3), where the edge weight connecting the followee
and a follower is equal to 1, if they have not included any common hashtag in their last
200 tweets. The edge weight is increased by 1, for every common hashtag used by the two
users. That is, the weight of the edge connecting the follower and his/her j-th followee is
equal to wj = N + 1, where N is the number of common hashags in their last 200 tweets.
Similar to the Facebook case, it can be seen that by observing the second-degree followee-
follower graph in Figure 1.4.5, users with common interests can be revealed. That kind of
information can be exploited in various ways, e.g., it can be useful to advertisers on the
Twitter platform who want to target the most suitable customers.

12
Graph-Based Social Media Analysis
FIGURE 1.4.3: A followee-follower graph between a user and the followers of his/her fol-
lowers on Twitter.
1.4.3
Graphs from bibliographic data
As previously described, bibliographic data can be exploited in order to describe relation-
ships between scientists, or between their publications. A co-authorship graph describing
the scientiﬁc network of an author is illustrated in Figure 1.4.6. As can be seen, author
graphs are weighted, i.e. the weight of an edge connecting two authors depends on the
number of papers co-authored by the two authors, corresponding to the two graph nodes.
Furthermore, additional information describing author relationships, e.g. advisor-advisee
relations, is described by using diﬀerent node colors. Given that author networks describe
information related to paper authorship, such networks are undirected. However, citation,
as well as advisor-advisee networks are directed and unweighted. In particular, citation and
FIGURE 1.4.4: A followee-follower Instagram graph describing the relationships between a
user and the followers of his/her followers.

Graphs in Social and Digital Media
13
FIGURE 1.4.5: The ﬁrst degree followee-follower Twitter graph of a user enhanced by using
hashtag information.
FIGURE 1.4.6: A co-authorship graph of the editor of this book (I. Pitas).
citation networks form the basics of bibliometrics [DB09]. Such a citation network is shown
in Figure 1.4.7. In such networks, inﬂuential papers (like P1 in Figure 1.4.7) describing new
concepts that are frequently cited essentially form cascades.

14
Graph-Based Social Media Analysis
FIGURE 1.4.7: Citation network.
1.5
Graph storage formats and visualization
After constructing a social graph, one should be able to store it so that it can be used
for future processing. To this end, several ﬁle formats have been designed. The most widely
used ones are: Trivial Graph Format (TGF), DOT format, Graph Modelling Language (or
Graph Meta Language) (GML) format, Graph eXchange Language (GXL) format, eXten-
sible Graph Markup and Modeling Language (XGMML) format, Directed Graph Markup
Language (DGML) format, GraphML format and XCM format and XTM format.
Graph ﬁle formats can be categorized in plain text-based formats and XML-based for-
mats. TGF, DOT and GML formats belong to the ﬁrst category. They have been designed
to have simple structure. A simple graph description can be made by using a list of nodes
followed by a list of edges, which specify node pairs. The XML-based graph ﬁle formats use
an XML-based syntax. Such a graph description contains a “graph” element that consists
of “node” and “edge” elements. Attributes “ID,” “source” and “target” are used in order to
deﬁne graph nodes and edges. Finally, it should be noted that some of these maps have been
designed speciﬁcally for the description of graphs deﬁned from geographical map topologies,
i.e., the XCM and XTM ones.
In addition, visualization applications, open source libraries and tools for graph manip-
ulation have been developed. The most widely used graph visualization applications and
softwares include Gephi, Graph Visualization Software (Graphviz), Cytoscape, and Tulip.
Graph manipulation libraries and tools include igraph, Open Graph Drawing Framework
(OGDF), and yEd.

Graphs in Social and Digital Media
15
1.6
Big data issues in social and digital media
An underlying problem of graph-based social media analysis methods is their com-
putational complexity, which may scale cubically with respect to the number of data
to be analyzed. For instance, for the analysis of a graph G(V, E) consisting of N nodes
vi ∈V, i = 1, . . . , N using algebraic graph-based dimensionality reduction and classiﬁcation
methods discussed in Chapter 6, one should compute, store and analyze the corresponding
graph weight matrix W ∈RN×N. The computation of W usually has a computational
complexity equal to O(DN 2), where D denotes the dimensionality of vi, while the applica-
tion of algebraic graph analysis techniques on W usually requires the eigendecomposition
or inversion of W, having a computational complexity equal to O(N 3) [PC99]. Taking into
account the size of social media graphs, e.g. a graph describing the relationships between
the members of a Facebook group may consist of hundreds of thousands of nodes N, while
the number N of images appearing in the proﬁles of the members of a Facebook group
may be in the order of millions. In such cases, the application of algebraic graph analysis
methods in a single computer may be practically impossible.
At such size scales, a single computer is simply unable to handle the required processing
tasks, as it does not have enough memory, or even disk space, to store all the required
data. In order to overcome restrictions concerning the cardinality of social media data
sets, three approaches can be followed: a) approximate solutions, b) incremental learning
approaches and c) distributed computing approaches. Approximate solutions tackle the
problem by solving an approximation of the original problem that requires less memory and
computational cost. Common approaches to this end operate on a subset of the available
data [AMS02, BW09], or approximate Linear Algebra methods, e.g. matrix multiplication
and Singular Value Decomposition, and their application in analysis tasks [SS00, WS01,
DM05]. Incremental learning approaches create an initial model by employing a (usually
small) part of the data. Subsequently, they update the model by exploiting the remaining
data, used in small batches. Detailed descriptions of several incremental learning methods
will be given in Chapter 11.
In general, distributed computing may provide the only viable means to handle big
(social) media processing or analysis tasks. Data can be distributed for storage in individual
nodes of a computing cluster, or a computing cloud. Each node can contribute its processing
cores and memory to the distributed system, bypassing the cost and hardware limits of
having a single computer with the same amount of processing cores and memory. Clusters
are also highly scalable, as nodes can be easily added, removed, or replaced. Chapter 10 is
dedicated to the description of various big media data analysis tasks.
1.7
Distributed computing platforms
In distributed computing, diﬀerent computers are connected through a network, while
communication between them is performed through messages for coordination or data ex-
change purposes [CDK05]. Such machines form a computing cluster. The usual architecture
of a computing cluster involves a master node coordinating several worker nodes. Its main
advantage over super-computers with extraordinary large numbers of CPUs and amount of
RAM is the scaling capability of a computing cluster, which may also use super-computers

16
Graph-Based Social Media Analysis
as worker nodes. The scalability of computing clusters allows them to handle Big Data
problems, which may be too big for even the most powerful single machine.
Message Passing Interface (MPI ) is a standardized, language-independent protocol for
distributed computations that involves several processes running on several diﬀerent com-
puters [GLDS96]. It provides speciﬁcations for message-passing distributed processing li-
brary development, so that such libraries can have the same behavior and be compatible
with each other. The current revision of the protocol is MPI-2, while MPI-3 is under devel-
opment.
The MapReduce distributed programming model [DS08] was invented by Google specif-
ically for handling extremely big datasets. It was inspired by the corresponding map and
reduce primitives oﬀered by functional programming languages, such as Lisp. In general,
it consists of two steps. The ﬁrst one applies a function on all the elements separately
(Map) and the second one collects the results, using a commutative and associative opera-
tion (Reduce). An advantage of this model, over using a standard MPI system, is that the
programmer does not have to handle the low-level details of an implementation, e.g., data
distribution to the worker nodes, fault-tolerance, or load balancing, because it provides tools
for writing high-level programs for clusters. While it may not provide a suitable solution
for every possible problem, the MapReduce model particularly lends itself to problems that
involve running simple operations on a large number of data elements on several worker
nodes.
As the name implies, there are two major components in the MapReduce program-
ming model. The Map command, when every worker applies a user deﬁned function to
each element of the dataset. Each worker can then return the results to the master node,
thus computing that function output for the entire dataset. Additionally, using the Reduce
command, a worker applies a commutative and associative operation to collect the data
elements, or the results of a previously mapped function, into a single result. As the opera-
tion is commutative and associative, the results for each worker are independent from other
workers and they can also be combined in the same way on the master node. A variation
of the Reduce command is ReduceByKey [DS08], in which, given a distributed set of (key,
value) pairs and a target operation, the operation is performed on the value parts for each
key separately. If there were k total keys, then the output would be k (key, total) pairs,
where each total is the result of performing the operation only on the value parts that
are associated with the speciﬁc key. MapReduce has been successfully used in a number
of big media data problems, e.g., distributed big graph mining [KF12] or machine learning
[GKP+11].
There are several diﬀerent implementations of the MapReduce framework besides
Google’s own, such as Apache Hadoop [SKRC10], which is also an open source project.
MapReduce implementations often come with a distributed ﬁle system, like the Google File
System (GFS) or the Hadoop File System (HDFS), as such ﬁle systems synergize very well
with and are, at most times, required by MapReduce tasks. Both GFS and HDFS are typ-
ically used for storing extremely big ﬁles. These ﬁle systems split a big ﬁle into blocks (or
chunks) and distribute them to the computers serving as computing cluster nodes, allowing
for multiple copies of a block to be stored in several diﬀerent computers for fault tolerance.
File blocks can also be redistributed, in order to balance the load. GFS includes a single
Master node, which holds the metadata regarding the location of ﬁle chunks and it is not
involved in any actual data transfers. It instead directs an application to the node that
contains the requested chunk. When running Map or Reduce tasks, a node that already
contains the dataset elements is selected to perform the task on them, instead of a random
node, thus reducing the communication costs between nodes. However, such ﬁle systems are
not well suited for frequent ﬁle reads and writes.

Graphs in Social and Digital Media
17
(a)
(b)
FIGURE 1.7.1: The diﬀerence between Hadoop and Spark DAG scheduling: a) Hadoop’s
two-stage MapReduce and b) a more complex DAG than Spark is capable of.
The Apache Spark cluster computing framework [ZCF+10] builds upon Hadoop, in order
to improve computation speed. It is also compatible with HDFS. Its advantages over Hadoop
include a) its ability to create and operate on the more complex Directed Acyclic Graph
(DAG) for scheduling tasks rather than Hadoop’s two-stage MapReduce DAG scheduling,
as illustrated in Figure 1.7.1, and b) its ability to cache data in the distributed memory. The
data in memory are stored in collections of elements called Resilient Distributed Datasets
(RDDs), which support Map and Reduce operations and, additionally, other operations,
such as join and ﬁlter. It also allows data from RDDs to “spill” to the hard drive, in
case the amount of memory is insuﬃcient. All the above memory handling and DAG task
scheduling is done by the framework, without input from the programmer. To form the
cluster, one computer runs the master component. Computers join the cluster by connecting
a worker component to the master. The cluster is very ﬂexible: dedicated computers may
always operate in the cluster, while other computers, e.g., workstations, can contribute any
resources they can spare, while still allowing people to work on them.
Finally, it is possible for programmers to create their own DAG schedule of distributed
tasks through Apache Storm, a distributed computation system. In order to perform a dis-
tributed task, a so-called topology has to be created. A topology can include Spouts and
Bolts. The data ﬂow is done through streams. Streams are generated by Spouts, while Bolts
perform computations on streams they receive from Spouts or other Bolts and output a
processed stream. Both of these entities can have various levels of parallelism. The user can
create a network of Spouts and Bolts by deﬁning directed connections between them. The
resulting network is packaged into a topology, which can then be submitted for execution
to a Storm cluster. Again, the user only needs to deﬁne the Spouts, Bolts, and the topol-
ogy. Everything else is handled by the system. An alternative to Storm can be found in
Microsoft’s Dryad project.

18
Graph-Based Social Media Analysis
1.8
Conclusions
In this chapter, an overview of the dominant social networks and social media plat-
forms to date has been given. It has been shown that social networks can be represented
by using graph structures. First-degree and second-degree graphs of the dominant social
networks have been illustrated and discussed. Such graphs, created by using social media
data available through the corresponding APIs, can be exploited for further analysis, as will
be discussed in the following chapters. Issues concerning the computational complexity and
memory requirements of graph-based social media analysis methods have been discussed.
Finally, big data issues and distributed computing platforms that are useful in social media
analysis are reviewed.
Bibliography
[ABHH08] T. Ahlqvist, A. Back, M. Halonen, and S. Heinonen. Social media road maps
exploring the futures triggered by social media. VTT, 2008.
[AMS02]
D. Achilioptas, G. McSherry, and B. Scholkopf. Sampling techniques for kernel
methods. In Proc. Neural Information Processing Systems, 2002.
[Bar03]
A. L. Barab´asi. Linked: How Everything is Connected to Everything Else and
What It Means for Business. Plume, 2003.
[BBR+12]
L. Backstrom, P. Boldi, M. Rosa, J. Ugander, and S. Vigna. Four degrees of
separation. In Proc. of the 4th Annual ACM Web Science Conference, pages
33–42, 2012.
[BW09]
M. A. Belabbas and P. J. Wolfe. Spectral methods in machine learning and new
strategies for very large datasets. In Proceedings of the National Academy of
Sciences, volume 106, pages 369–374, 2009.
[CDK05]
G. Coulouris, J. Dollimore, and T. Kingberg. Distributed Systems: Concepts and
Design (International Computer Science). Addison-Wesley Longman, 2005.
[DB09]
N. De Bellis. Bibliometrics and citation analysis: from the science citation index
to cybermetrics. Scarecrow Press, 2009.
[DM05]
P. Drineas and M. W. Mahoney. On the Nystrom Method for Approximat-
ing a Gram Matrix for Improved Kernel-based Learning. Journal of Machine
Learning Research, 6:2153–2275, 2005.
[DS08]
J. Dean and G. Sanjay. MapReduce: simpliﬁed data processing on large clusters.
ACM Communications Magazine, 51(1):107–113, 2008.
[dSP65]
D. J. de Solla Price. The pattern of bibliographic references indicates the nature
of scientiﬁc research front. Science, 149:510–515, 1965.
[Ell08]
N. Ellison. Social network sites: Deﬁnition, history, and scholarship. Journal of
Computer-Mediated Communication, 13:21–23, 2008.

Graphs in Social and Digital Media
19
[GKP+11] A. Ghoting, B. Krishnamurhty, E. Pednault, B. Reinwald, V. Sindhwani,
S. Takikonda, Y. Tian, and S. Vaithyanathan. SystemML: Declarative machine
learning on MapReduce. In Proc. IEEE International Conference on Data En-
gineering, pages 231–242, 2011.
[GLDS96]
W. Gropp, E. Lusk, N. Doss, and A. Skjellum. A high-performance, portable
implementation of the MPI message passing interface standard. Parallel com-
puting, 22(6):789–828, 1996.
[Hem13]
J. Hempel. LinkedIn: How it’s changing business. Fortune, pages 69–74, 2013.
[KF12]
U. Kang and C. Faloutsos. Big graph mining: algorithms and discoveries. ACM
SIGKDD Explorations, 14(2):29–36, 2012.
[KH10]
A. M. Kaplan and M. Haenlein. Users of the world, unite! The challenges and
opportunities of social media. Business Horizons, 53(1):59–68, 2010.
[PBV+07]
G. Palla, A. Barabasi, T. Vicsek, Y. Chi, S. Zhu, X. Song, J. Tatemura, and
B. L. Tseng. Quantifying social group evolution. Nature, 446:664–667, 2007.
[PC99]
V. Y. Pan and Z. Q. Chen. The complexity of the matrix eigenproblem. In Proc.
of the 31st annual ACM Symposium on Theory of Computing, pages 507–516,
1999.
[Sch96]
L. H. Schmidt. Commonness across cultures. In Cross-Cultural Conversation:
Initiation, pages 119–132. Oxford University Press, 1996.
[SKRC10]
K. Shvachko, H. Kuang, S. Radia, and R. Chansler. The Hadoop distributed ﬁle
system. In Proc. IEEE Symposium on Mass Storage Systems and Technologies,
pages 1–10, 2010.
[SS00]
J. Smola and B. Scholkopf. Sparse greedy matrix approximation for machine
learning. In Proc. International Conference on Machine Learning, 2000.
[TPP14]
I. Tsingalis, I. Pipilis, and I. Pitas.
A statistical and clustering study on
YouTube 2D and 3D video recommendation graph. In Proc. International Sym-
posium on Communications, Control and Signal Processing, 2014.
[TYRW14] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface: Closing the gap
to human-level performance in face veriﬁcation. In Proc. IEEE Conference on
Computer Vision and Pattern Recognition, 2014.
[UKBM11] J. Ugander, B. Karrer, L. Backstrom, and C. Marlow. The anatomy of the
Facebook social graph. Computing Research Repository, 2011.
[WS01]
C. K. I. Williams and M. Seeger. Using the Nystrom method to speed up kernel
machines.
In Proc. Neural Information Processing Systems, pages 682–688,
2001.
[You]
http://www.youtube.com/yt/press/statistics.html.
[ZCF+10]
M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and I. Stoica. Spark:
Cluster computing with working sets. In USENIX Conference on Hot Topics
in Cloud Computing, 2010.


Chapter 2
Mathematical Preliminaries: Graphs and
Matrices
Nikolaos Tsapanos, Alexandros Iosiﬁdis and Ioannis Pitas
Aristotle University of Thessaloniki, Greece
2.1
Graph basics ...................................................................
21
2.2
Linear algebra tools ............................................................
24
2.3
Matrix decompositions .........................................................
28
2.4
Vector and matrix derivatives
.................................................
31
Bibliography ....................................................................
31
2.1
Graph basics
Formally, a graph G = (V, E) is an ordered pair of a set of vertices V = {vi} and a
set of edges E ⊆V × V [BM76, Wes01]. Graphs are abstract constructs that can model
relationships (edges) between entities (vertices). The edge (ui, uj) connecting vertices ui
and uj is incident to ui and uj and signiﬁes that vertex ui is adjacent to uj. The graph is
called undirected, if (vi, vj) ∈E implies (vj, vi) ∈E. If the order of the vertices in an edge
(source-sink) is important, then the graph is called directed (or digraph).
If a graph has more than one distinct edge connecting the same vertices, then it is called a
multigraph [GY99]. The number of edges that connect vertex vi with other vertices is called
degree di of the vertex. In directed graphs, one can distinguish between the in-degree and
the out-degree of a vertex, which are the number of the incoming and the outgoing edges,
respectively. Vertices with a degree equal to 0 have no connections and are characterized as
isolates. The degree is one factor in determining the importance of a vertex in the graph.
Vertices with a high degree are more likely to be considered important. If all graph vertices
have the same number of connections, i.e., they have the same number of incident edges,
the graph is a regular one. In the opposite case, the graph is characterized as irregular.
A graph where every vertex is connected to all other vertices is called a complete graph.
The vertices that are connected with vi form the neighborhood of vi. It is possible that edges
in both directed and undirected graphs have a weight value, in which case the degree of a
vertex is determined by the sum of the weights of its incident edges. It is also possible that
vertices, edges, or both, have either labels or attributes associated with them, resulting in
a labeled or an attributed graph, respectively. Typically, in social media, graph vertices may
refer to media content, e.g., images or video, which are vectorial data. In such cases, each
graph vertex vi can be described by a feature vector xi ∈RD, where D is the dimensionality
of the vector.
A subgraph S of G is a graph whose vertex and edge sets are a subset of the vertex and
edge sets of G, respectively. A subgraph is called an induced subgraph of G, if vertices vi
and vj being connected in G implies that they are also connected in S.
21

22
Graph-Based Social Media Analysis


0 1 0 0 0 0 0 1
1 0 1 0 0 0 0 0
0 1 0 1 0 0 0 0
0 0 1 0 1 0 0 0
0 0 0 1 0 1 0 0
0 0 0 0 1 0 1 0
0 0 0 0 0 1 0 1
1 0 0 0 0 0 1 0




0 1 0 1 0 0 0 0
1 0 1 0 0 1 0 1
0 1 0 1 0 1 0 0
1 0 1 0 0 0 1 1
0 0 0 0 0 1 0 1
0 1 1 0 1 0 0 0
0 0 0 1 0 0 0 1
0 1 0 1 1 0 1 0


(a)
(b)
FIGURE 2.1.1: Example graphs and their respective adjacency matrices. a) Ring graph and
b) arbitrary graph.
An ordered sequence of connected vertices (vi, . . . , vk, . . . , vj) that starts with vertex vi
and ends with vertex vj forms a path between vi and vj. The graph is connected, iﬀthere
is a path from any vertex to any other vertex in the graph. The shortest path between
two vertices is called geodesic and its length is called the distance of the two vertices. The
longest distance between two vertices is called the diameter of the graph.
The adjacency matrix A of a graph G = (V, E) is a square |V| × |V| matrix such that:
aij = A(i, j) =
(
1, if (vi, vj) ∈E
0, otherwise,
(2.1.1)
where | · | denotes the cardinality of a set. Some example graphs and their adjacency ma-
trices can be seen in Figure 2.1.1. It is easy to prove that the elements of the k-th power
Ak ≜AA . . . A (k times) of the adjacency matrix A provide the total number of possible
paths between the corresponding vertices that have exactly length k [GY99]. If the graph
is undirected, the adjacency matrix A is a symmetric matrix, i.e., A = AT , where AT
denotes the transpose of A. If the graph is weighted, the adjacency matrix can contain the
numerical weight of each edge in the appropriate entry in the adjacency matrix, in which
case it is also called the graph weight matrix, typically denoted by W = [wij]. In the case
where W denotes only connections between the graph vertices, i.e., wij = {0, 1}, W is
identical to the graph adjacency matrix A.
An alternative matrix representation of a graph is provided by its Laplacian matrix
L = D −A [Chu97], where D is a diagonal matrix, whose i-th diagonal entry is the
degree di of vertex ui. If the graph is weighted, the Laplacian matrix is similarly deﬁned
as L = D −W, where W is the weight matrix and the entries of the degree matrix D are
given by di = P
j wij. The normalized Laplacian matrix L is deﬁned as:
ℓij = L(i, j) =







1, if i = j and di ̸= 0
−
1
√
didj , if (vi, vj) ∈E
0, otherwise
,
(2.1.2)
or in a more compact form:
L = I −D−1
2 AD−1
2 ,
(2.1.3)
where I ∈RN×N is the identity matrix and X ≜Y
1
2 , iﬀX2 = Y, so that D−1
2 is essentially
a diagonal matrix, whose elements are
1
√di . If the graph is weighted, the corresponding
normalized Laplacian matrix is deﬁned as:
L = I −D−1
2 WD−1
2 .
(2.1.4)

Mathematical Preliminaries: Graphs and Matrices
23
FIGURE 2.1.2: Bipartite graph describing user recommendations.
If the vertices of a graph can be partitioned into two disjoint sets V1 and V2, such that
every edge e ∈E connects a vertex from one set to a vertex in the other set, the graph is
called bipartite. Such graphs arise in recommendation systems, where one vertex set can
represent users and the other set can represent items. If a user is associated with an item,
an edge connects their respective vertices, as shown in Figure 2.1.2.
Graphs are also often used to represent similarity between samples of a multimedia
dataset. Let us assume that a set of digital media data, e.g., a set of N facial images, has
been preprocessed so that each sample i = 1, . . . , N is represented by a D-dimensional
vector xi ∈RD, i = 1, . . . , N. Let us also deﬁne a similarity measure sij = s(xi, xj) that
is used to measure the similarity between two vectors xi and xj. sij = s(·, ·) may be any
similarity measure providing non-negative values (usually 0 ≤sij ≤1). The most widely
adopted choice is the heat kernel (also known as the diﬀusion kernel), deﬁned by [KL02]:
s(xi, xj) = e−
∥xi−xj ∥2
2
2σ2
,
(2.1.5)
where ∥· ∥2 denotes the L2 norm of a vector and σ is a scaling parameter for the Euclidean
distance between xi and xj. By using sij, i = 1, . . . , N, j = 1, . . . , N, we can form the
graph weight matrix W ∈RN×N, W = [sij]. That is, we can assume that the vectors xi
are embedded in a graph G = (V, E, W), where V = {xi}N
i=1 denotes the graph vertex set
and E the set of edges connecting xi. In this context, wij = sij denotes the weight value of
the edge connecting the graph vertices xi and xj.
It is also possible to use multiple similarity measures, in order to obtain a labeled
multigraph. In this case, there can be multiple edges connecting the same vertices [BR12].
Edges with the same label can correspond to similarities between vertices according to the
same measure, e.g., color similarity or motion similarity in the case of digital videos residing
on graph vertices.
A hypergraph is a graph whose edges may connect more than two vertices [Ber89] and
can be used to represent multiple relationships between vertices. A hypergraph H is deﬁned
as a pair H = (V, E), where V = {vi} is a ﬁnite set of vertices and E = {ej} is a set of non-
empty subsets of vertices, called hyperedges, representing relationships between vertices.
If no hyperedge contains more than two vertices, the hypergraph becomes equivalent to a
normal graph. An example of a hypergraph is shown in Figure 2.1.3, where the set of vertices
is V = {v1, v2, v3, v4, v5, v6, v7, v8} and the set of hyperedges is E = {e1, e2, e3}, where
e1 = {v1, v2, v5, v7}, e2 = {v2, v3, v4, v8} and e3 = {v1, v5, v6}. In a weighted hypergraph,
denoted by H = (V, E, w), each hyperedge ej is assigned a weight w(ej). If vi ∈ej, the
hyperedge ej ∈E is incident to the vertex vi ∈V. A hypergraph can be represented by the
incidence matrix H ∈R|V|×|E|, where:
H(i, j) =
( 1,
if vi ∈ej
0,
if vi ̸∈ej.
(2.1.6)

24
Graph-Based Social Media Analysis
e1
e2
e3
v1
1
0
1
v2
1
1
0
v3
0
1
0
v4
0
1
0
v5
1
0
1
v6
0
0
1
v7
1
0
0
v8
0
1
0
FIGURE 2.1.3: Example of a hypergraph.
The degree of a vertex vi ∈V is equal to the number of its incident edges. In weighted
hypergraphs, the degree of a vertex vi is deﬁned as the sum of the weights associated with its
incident hyperedges d(vi) = P
ej∈E w(ej)H(i, j). In a similar way, the degree of a hyperedge
ej ∈E is equal to its cardinality δ(ej) = P
vi∈V H(i, j). The hypergraph adjacency matrix
A, as deﬁned by Zhou et al. in [ZHS07] according to the random walk model, is calculated
by:
A = HWHT −Dv,
(2.1.7)
where W and Dv are the diagonal matrices of the hyperedge and vertex weights, respec-
tively. The normalized Laplacian matrix of a hypergraph is deﬁned accordingly:
L = I −1
2D−1/2
v
HWHT D−1/2
v
= 1
2

I −D−1/2
v
AD−1/2
v

.
(2.1.8)
2.2
Linear algebra tools
Linear algebra [Str88] plays an important role in graph analysis. Therefore, its tools are
reviewed in this section.
There are various products deﬁned between two matrices. The matrix product Z ≜XY
of N × P matrix X and P × M matrix Y is the N × M matrix Z, whose elements are given
by:
zij =
P
X
k=1
xikykj.
(2.2.1)
The Hadamard product Z ≜X ◦Y of N × M matrices X and Y is the N × M matrix Z,
whose elements are given by:
zij = xijyij.
(2.2.2)
The Kronecker product Z ≜X⊗Y of N ×M matrix X and P ×Q matrix Y is the NP ×MQ

Mathematical Preliminaries: Graphs and Matrices
25
matrix Z, whose elements are given by:
Z =


X(1, 1)Y
X(1, 2)Y
. . .
X(1, n)Y
X(2, 1)Y
X(2, 2)Y
. . .
X(2, n)Y
...
...
...
...
X(n, 1)Y
X(n, 2)Y
. . .
X(n, n)Y


.
(2.2.3)
The Khatri-Rao product Z ≜X ⊙Y of two likewise partitioned block matrices, namely
N × M matrix X and P × Q matrix Y, is the likewise partitioned block matrix Z, whose
blocks are given by:
Zij = Xij ⊗Yij.
(2.2.4)
The rank of a matrix A ∈RN×M is deﬁned to be the number of its linearly independent
column (or row) vectors. In general, rank(A) ≤min (N, M). Matrix A is a full rank matrix,
iﬀrank(A) = min (N, M), otherwise A is rank deﬁcient. There are two important subspaces
associated with the N × M matrix A. The range R(A) of A is a subspace of RN deﬁned
as follows:
R(A) = {y ∈RN : there exists at least one x ∈RM, such that Ax = y}.
(2.2.5)
The null space N(A) of A is a subspace of RM deﬁned by:
N(A) = {x ∈RM : Ax = 0N},
(2.2.6)
where 0N ∈RN is a vector having all its elements equal to zero. By using the above notation,
the following relation holds:
rank(A) + dim(N(A)) = min(N, M),
(2.2.7)
where dim(·) denotes the vector space dimension.
The Frobenius norm ∥A∥F of a matrix is given by:
∥A∥F =
v
u
u
t
N
X
i=1
M
X
j=1
aij2, or ∥A∥F =
q
tr(AT A),
(2.2.8)
where tr(·) is the trace of the matrix, i.e., the sum of the elements of its diagonal.
A square matrix A ∈RN×N is called invertible (or non-singular), if there is a matrix
B ∈RN×N such that:
AB = I
and
BA = I.
(2.2.9)
In this case, matrix B = A−1 is called the inverse of A and vice versa. A square matrix
that is not invertible is called singular. A square matrix A is singular, if its rank is less
than N. A symmetric matrix A is positive-deﬁnite, iﬀ∀x ∈RN, xT Ax > 0.
An eigenvector of a real-valued, square matrix A ∈RN×N is a non-zero vector v ∈RN
that satisﬁes the following equation:
Av = λv,
(2.2.10)
meaning that the vector Av follows the direction of v. λ is the eigenvalue of A corresponding
to the eigenvector v. The eigenvalues are solutions of the equation:
det(A −λI) = 0,
(2.2.11)

26
Graph-Based Social Media Analysis
FIGURE 2.2.1: Singular Value Decomposition of matrix A.
where det(·) denotes the determinant of a matrix. Any symmetric matrix A has real eigen-
values and can always be written in the form:
A = VΛVT ,
(2.2.12)
where Λ is a diagonal N × N matrix, whose diagonal values are the distinct matrix eigen-
values λi, i = 1, . . . , N and matrix V = [v1, v2, . . . , vN] is an orthogonal matrix, i.e.,
VVT = VT V = I, formed by the eigenvectors vi, i = 1, . . . , N. Equation (2.2.12) provides
the eigendecomposition of A.
Example 2.2.1. Matrix eigenanalysis.
Matrix A =


1
2
3
2
5
4
3
4
2

has the following eigenvalues and eigenvectors:
λ1 = −1.8, v1 = [0.6, 0.3, −0.7]T , λ2 = 2.6, v2 = [0.7, −0.6, 0.3]T ,
λ3 = 9.2, v3 = [0.4, 0.7, 0.6]T .
For simplicity, the eigenvalues and eigenvector entries have been rounded to the ﬁrst
digit.
The Singular Value Decomposition (SVD) [GVL96] of a N × M matrix A is given by:
A = UΣVT ,
(2.2.13)
where Σ is a N × M matrix, whose diagonal elements are the min (N, M) singular values
d1 ≥d2 ≥. . . ≥dmin(N,M) ≥0 of A, as shown in Figure 2.2.1. It can be proven that the
singular values of A are the square roots of the eigenvalues of matrix AT A. The rank of a
rectangular N × M matrix A is equal to the number of the non-zero singular values di.
The columns of U, V are the left and right singular vectors of A, i.e., the eigenvectors of
the N ×N AAT and M ×M AT A matrices, respectively. Matrices U and V are orthogonal,
i.e., UT U = UUT = I and VT V = VVT = I. They are not uniquely deﬁned, as, e.g., −U
and −V can also be used towards the same result. The columns of matrix U corresponding
to the non-zero singular values span the range of matrix A, i.e., they provide an orthonormal

Mathematical Preliminaries: Graphs and Matrices
27
basis for that space. The columns of matrix V corresponding to the zero singular values
span the null space of matrix A. If matrix A is square, symmetric and positive deﬁnite,
then its SVD coincides with its eigendecomposition.
Example 2.2.2. Matrix SVD.
Matrix A =


5
2
7
3
2
9
4
9
0
6
1
5


has the following SVD:
U =


−0.4
0.9
0.2
−0.8
−0.2
−0.6
−0.4
−0.4
0.8

, Σ =


16.8
0
0
0
0
7
0
0
0
0
0.4
0

,
V =


−0.2
0.6
−0.1
−0.8
−0.6
−0.4
0.6
−0.2
−0.4
0.7
0.2
0.6
−0.6
−0.2
−0.7
0.1


.
Therefore, it has three singular values: d1 = 16.8, d2 = 7 and d3 = 0.4. For simplicity,
the singular values and singular vector entries have been rounded to the ﬁrst digit.
A square N × N matrix A is non-singular, i.e., it can be inverted, iﬀall its singular
values are non-zero d1 ≥d2 ≥. . . ≥dmin (N,M) > 0. The inverse of a non-singular N × N
matrix A can be written as:
A−1 = VΣ−1UT .
(2.2.14)
The Frobenius norm ∥A∥F of a matrix can be found from its singular values:
∥A∥F =
sX
i
di
2.
(2.2.15)
A homogeneous system of N linear equations and M unknowns described by a N × M
matrix A [GVL96]:
Ax = 0,
(2.2.16)
satisfying N ≥M −1 and rank (A) = M −1, has two solutions: a) the trivial solution x = 0
and b) a non-trivial solution up to a scale factor, provided by the SVD decomposition of
matrix A = UΣVT . Namely, it is equal to the scaled column of matrix V, x = λvi, whose
corresponding singular value is zero di = 0. Since the rank of matrix A is rank(A) = M −1,
all other singular values are non zero (positive). In practice, due to numerical errors, we
choose the singular value di that has the lowest absolute value (close to zero).
A system of N linear equations with N unknowns residing in vector x described by a
N × N matrix A:
Ax = b
(2.2.17)
has the following solution:
x = A−1b,
(2.2.18)

28
Graph-Based Social Media Analysis
FIGURE 2.3.1: Cholesky decomposition of matrix A.
provided that matrix A is non-singular. Numerous methods exist for the numerical solu-
tion of linear equation systems [Str88, Mey00], whose presentation is beyond the overview
provided by this section.
If N > M, the linear system of N equations and M unknowns Ax = b is over-
determined, i.e., it has more equations than unknowns. In this case, no exact solution
may exist, if b does not belong to the space spanned by the columns of A. Such a system
has the following least squares solution [LH74]:
x = (AT A)−1AT b.
(2.2.19)
The matrix A† ≜(AT A)−1AT is called the (left) generalized inverse (or Moore-Penrose
pseudoinverse) of matrix A [Jam78].
2.3
Matrix decompositions
Cholesky decomposition (or Cholesky triangle, or Cholesky factorization) decomposes an
N × N symmetric, positive-deﬁnite matrix (SPD) A into the product of a lower triangular
matrix with positive diagonal elements L and its transpose LT [Wil88]:
A = LLT ,
(2.3.1)
as shown in Figure 2.3.1. Equation (2.3.1) can be written as:
A =
 
a11
aT
21
a21
A22
!
=
 
l11
0T
l21
L22
!  
l11
lT
21
0
LT
22
!
=
=
 
l2
11
l11lT
21
l11l21
l21lT
21 + L22LT
22
!
.
(2.3.2)
It is obvious that l11 = √a11 and l21 =
1
l11 a21. Then, the computation proceeds to recur-
sively compute L22 through the equation:
A22 −l21lT
21 = L22LT
22.
(2.3.3)

Mathematical Preliminaries: Graphs and Matrices
29
FIGURE 2.3.2: CUR approximation of matrix A.
Example 2.3.1. Cholesky decomposition.
Matrix A =


9
3
1
3
9
3
1
3
9

has the following Cholesky decomposition:
L =


3
0
0
1
2.8
0
0.3
0.9
2.8

, LT =


3
1
0.3
0
2.8
0.9
0
0
2.8

.
For simplicity, the matrix entries have been rounded to the ﬁrst digit.
Cholesky decomposition is mainly used for the numerical solution of linear equations Ax =
b, when A is SPD. We ﬁrst compute the matrix L as described above. Then, we solve
the equation Ly = b, where y = LT x, using forward substitution and, ﬁnally, we solve
LT x = b using back substitution. This approach, when applicable, is preferable to other
decompositions, in terms of both eﬃciency and numerical stability.
CUR approximation is described by three matrices C ∈RN×c, U ∈Rc×r, R ∈Rr×M
that, when multiplied, closely approximate a given matrix A ∈RN×M, i.e., A ≈CUR.
In other words, given the matrix A we have to ﬁnd the three matrices C, U, R such that
the approximation error ∥A −CUR∥F is small [DKML04]. Figure 2.3.2 illustrates CUR
approximation.
Matrix C contains c columns of A, while R contains r rows of A. Given speciﬁc instances
of matrices C and R, then U is a c × r matrix provided by U = C†AR†, where C† is the
Moore-Penrose pseudoinverse of C (respectively, R) [MD09]. The most important issue is
how to select these columns and rows [DKML04]. Two methods are more popular for this
problem subspace, namely sampling with replacement, which allows a row or column to be
selected multiple times, and sampling without replacement, which does not. It is necessary
to estimate the absolute ∥A −CUR∥F and the relative error bound, in order to decide
which one is better [DMM08]. Thus, it is obvious that CUR matrix approximation is not
unique.

30
Graph-Based Social Media Analysis
Example 2.3.2. Matrix CUR approximation.
Matrix A =


5
2
7
3
2
9
4
9
0
6
1
5
6
8
0
2


can be approximated with the following matrices:
C =


5
3
2
9
0
5
6
2


, U =
" 0.2
−0.3
0.1
0.1
#
, R =
" 2
9
4
9
0
6
1
5
#
.
For simplicity, the entries of matrix U have been rounded to the ﬁrst digit. The error
of this approximation is ∥A −CUR∥F = 9.1796.
CUR approximation is less accurate than SVD, but has certain advantages. CUR, unlike
SVD, maintains sparsity. If A is big and sparse, then matrices C and R are also big and
sparse, while matrices U and V of the SVD would be dense. Moreover, since the matrices
C and R are made of a subset of data samples and variables respectively, it is much easier
to interpret them, than the SVD left and right singular vectors, which represent the data
in a transformed space.
Another matrix decomposition technique is the Non-negative Matrix Factorization
(NMF), whose purpose is to factorize a non-negative matrix X into two matrices H, F,
which are also non-negative [PT94]. NMF has important applications in cases, where the
data contained in matrix X are non-negative by deﬁnition, e.g., when vectors xi, i = 1, . . . , N
represent vectorized images. Since the problem can not be solved exactly nor is there a
unique solution, it is commonly approximated numerically [LS01]:
X ≈HF.
(2.3.4)
If X is an N × M matrix, then H and F are N × P and P × M matrices, respectively.
Usually, P is smaller than N and M. Typically, matrix X contains M feature data vectors
X = [x1, x2, . . . , xM]. In that case, equation (2.3.4) can be written as: xi ≈Hfi, i =
1, . . . , M. This means that each data vector xi is approximated by a linear combination of
the columns of H (called basis vectors) weighted by the components of fi [SD05]. In the
case of images, the columns of H represent basis images.
There are diﬀerent types of non-negative matrix factorizations, each using diﬀerent cost
functions for measuring the divergence between X and HF and possibly regularizing H
and/or F matrices [LS01]. The cost functions quantify the approximation error in (2.3.4).
The Frobenius norm (i.e., the Euclidean distance) between matrices K and L is lower
bounded by zero and clearly vanishes, iﬀK = L. The Kullback-Leibler divergence [KL51]:
D(K||L) =
X
ij
(kij log kij
lij
−kij + lij)
(2.3.5)
is also lower bounded by zero and vanishes iﬀK = L. Thus, two NMF variants arise from

Mathematical Preliminaries: Graphs and Matrices
31
the following minimization problems:
• Minimize ∥X −HF∥2, s.t. H, F ≥0.
(2.3.6)
• Minimize D(X||HF), s.t. H, F ≥0.
(2.3.7)
Neither the Frobenius norm nor Kullback-Leibler divergence are convex in H and F
simultaneously. Gradient descent can be applied for minimizing either (2.3.6) or (2.3.7)
[Lin07], but has slow convergence and is sensitive to the choice of the step size. The so-called
multiplicative update rules [LS01] can be used for iteratively solving the above mentioned
minimization problems. They take the following forms for the Frobenius norm:
F(t+1) = F(t)
(HTX)(t)
(HTHF)(t)
, H(t+1) = H(t)
(XFT)(t)
(HFFT)(t)
(2.3.8)
and KL divergence:
F(t+1) = F(t)
P
i H(t)X(t)/(HF)(t)
P
k H(t)
, H(t+1) = H(t)
P
m F(t)X(t)/(HF)(t)
P
n F(t)
,
(2.3.9)
respectively, where t is an iteration index.
2.4
Vector and matrix derivatives
In the following, we provide some frequently used vector and matrix derivatives [Bel70]:
∇x
 xT a

=
∇x
 aT x

= a
(2.4.1)
∇X
 aT Xb

=
abT
(2.4.2)
∇X
 aT XT b

=
baT
(2.4.3)
∇x
 xT Ax

=
 A + AT 
x
(2.4.4)
∇X
 bT XT AXc

=
AT XbcT + DXcbT
(2.4.5)
∇X

(Xb + c)T A (Xb + c)

=
 A + AT 
(Xb + c) bT
(2.4.6)
∇X tr (X)
=
I
(2.4.7)
∇X tr (XA)
=
AT
(2.4.8)
∇X tr (AXB)
=
AT BT
(2.4.9)
∇X tr
 AXT B

=
BA
(2.4.10)
∇X tr
 AXBXT C

=
AT CT XBT + CAXB.
(2.4.11)
Bibliography
[Bel70]
R. Bellman. Introduction to matrix analysis. McGraw-Hill, 2nd edition, 1970.
[Ber89]
C. Berge. Hypergraphs: combinatorics of ﬁnite sets, volume 45. North Holland,
1989.

32
Graph-Based Social Media Analysis
[BM76]
J. A. Bondy and U. S. R. Murty. Graph theory with applications, volume 6.
Macmillan London, 1976.
[BR12]
R. Balakrishnan and K. Ranganathan. A textbook of graph theory. Springer
Science & Business Media, 2012.
[Chu97]
F. R. K. Chung. Spectral Graph Theory. American Mathematical Society, 1997.
[DKML04] P. Drineas, R. Kannan, M. W. Mahoney, and A. Let. Fast Monte Carlo algo-
rithms for matrices III: Computing a compressed approximate matrix decom-
position. SIAM Journal on Computing, 36, 2004.
[DMM08]
P. Drineas, M. W. Mahoney, and S. Muthukrishnan. Relative-error CUR matrix
decompositions. SIAM Journal on Matrix Analysis and Applications, 30(2):844–
881, 2008.
[GVL96]
G. H. Golub and C. F. Van Loan. Matrix Computations. JHU Press, 3rd edition,
1996.
[GY99]
J. L. Gross and J. Yellen. Graph theory and its applications. CRC Press, 1999.
[Jam78]
M. James. The generalised inverse. The Mathematical Gazette, pages 109–114,
1978.
[KL51]
S. Kullback and R. A. Leibler. On information and suﬃciency. The Annals of
Mathematical Statistics, pages 79–86, 1951.
[KL02]
R. I. Kondor and J. D. Laﬀerty. Diﬀusion kernels on graphs and other discrete
input spaces. International Conference on Machine Learning, 2002.
[LH74]
C. L. Lawson and R. J. Hanson. Solving least squares problems, volume 161.
SIAM, 1974.
[Lin07]
C.-J. Lin.
Projected gradient methods for nonnegative matrix factorization.
Neural computation, 19(10):2756–2779, 2007.
[LS01]
D. D. Lee and H. S. Seung. Algorithms for Non-negative Matrix Factorization.
In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural
Information Processing Systems 13, pages 556–562. MIT Press, 2001.
[MD09]
M. W. Mahoney and P. Drineas. CUR matrix decompositions for improved
data analysis. Proceedings of the National Academy of Sciences, 106(3):697–
702, 2009.
[Mey00]
C. D. Meyer. Matrix Analysis and Applied Linear Algebra. SIAM Press, 2000.
[PT94]
P. Paatero and U. Tapper. Positive matrix factorization: A non-negative factor
model with optimal utilization of error estimates of data values. Environmetrics,
5(2):111–126, 1994.
[SD05]
S. Sra and I. S. Dhillon. Generalized nonnegative matrix approximations with
bregman divergences. In Advances in neural information processing systems,
pages 283–290, 2005.
[Str88]
G. Strang. Linear Algebra and Its Applications. Brooks Cole, 1988.
[Wes01]
D. B. West. Introduction to graph theory, volume 2. Prentice Hall, 2001.

Mathematical Preliminaries: Graphs and Matrices
33
[Wil88]
J. H. Wilkinson. The Algebraic Eigenvalue Problem. Clarendon Press, 1988.
[ZHS07]
D. Zhou, J. Huang, and B. Sch¨olkopf.
Learning with hypergraphs: Cluster-
ing, classiﬁcation, and embedding. In Proc. Advances in Neural Information
Processing Systems, volume 19, page 1601, 2007.


Chapter 3
Algebraic Graph Analysis
Nikolaos Tsapanos, Anastasios Tefas and Ioannis Pitas
Aristotle University of Thessaloniki, Greece
3.1
Introduction ....................................................................
35
3.2
Spectral graph theory ..........................................................
36
3.2.1
Adjacency and Laplacian matrix ......................................
36
3.2.2
Similarity matrix and nearest neighbor graph ........................
37
3.3
Applications of graph analysis .................................................
38
3.4
Random graph generation .....................................................
40
3.4.1
Desirable random graph properties ....................................
41
3.4.2
Random graph generation models .....................................
41
3.4.3
Spectral graph generation .............................................
43
3.5
Graph clustering ...............................................................
45
3.5.1
Global clustering algorithms ..........................................
46
3.5.2
Local clustering algorithms ............................................
48
3.5.3
Spectral clustering algorithms .........................................
48
3.5.4
Overlapping community detection ....................................
50
3.6
Graph matching ................................................................
51
3.6.1
Spectral graph matching ..............................................
53
3.6.2
Frequent subgraph mining .............................................
54
3.7
Random walks ..................................................................
54
3.8
Graph anomaly detection ......................................................
56
3.8.1
Spectral anomaly detection ............................................
57
3.9
Conclusions .....................................................................
58
Bibliography ....................................................................
59
3.1
Introduction
Graphs provide an intuitive way of representing connected or interacting entities [GY99].
One can very easily model and study various real-life and scientiﬁc structures using graphs,
e.g., web pages linking to each other [BV03], friendship in social networks, the relative
location of image features [DPZ01], transportation networks, electric circuits and computer
network topologies. Mathematicians have been studying graph theory since the 18th century,
starting with a published paper on the Seven Bridges of Konigsberg by Euler [Big93].
Many solutions to classical graph problems involve combinatorics, either with respect
to vertex/edge selection or to vertex/edge ordering. As such, the best available algorithms
are super-polynomial in complexity [GJ90], making their solution intractable, when large
graphs are involved. While understanding and studying graphs seems deceptively easy, es-
pecially when the size of a graph is relatively small, the nature of graphs makes them very
35

36
Graph-Based Social Media Analysis
diﬃcult to represent in vector form in order to handle them with algebraic methods directly.
Most diﬃculties stem from the fact that the same graph can have radically dissimilar inter-
connectivity under diﬀerent vertex permutations. While there are several graph invariant
properties, most of them are scalars and, as such, are not particularly useful in meaning-
fully representing a graph. The graph spectrum is an invariant that provides a much richer
representation of the graph and is the subject of spectral graph theory [Chu97].
This chapter is meant as an introduction to the intersection of graph theory and algebraic
or statistical approaches. It is organized as follows: Section 3.2 provides a quick introduction
to spectral graph theory. Applications of the various graph analysis tasks are summarized
in Section 3.3. The chapter continues by surveying techniques for several subﬁelds, starting
with random graph generation in Section 3.4 and continuing with graph clustering and
community detection in Section 3.5, graph matching techniques in Section 3.6, random
walks in Section 3.7 and anomaly detection in Section 3.8. Finally, Section 3.9 concludes
this chapter.
3.2
Spectral graph theory
3.2.1
Adjacency and Laplacian matrix
The most well-known representation of a graph with n vertices is the adjacency matrix
A ∈{0, 1}n×n of the graph. An adjacency matrix is a means of representing which vertices
(or nodes) of a graph are adjacent to each other. That is, if the vertex i is adjacent (con-
nected/related) to the vertex j then A(i, j) = 1. The adjacency matrix of an undirected
simple graph is symmetric (i.e., A(i, j) = A(j, i)), and, therefore, has a complete set of real
eigenvalues and an orthogonal eigenvector basis. Spectral graph theory studies the relation-
ships between the set of eigenvalues and eigenvectors of a graph matrix representation and
various properties of the graph. In general, the eigenvalues of its adjacency matrix A are
given by solving the following eigenanalysis problem:
Aui = λiui, i = 0 . . . n −1
(3.2.1)
and are related to various graph properties. The graph spectrum is deﬁned to be the set of
these eigenvalues. The vector ui that can satisfy (3.2.1) for a given λi is the corresponding
eigenvector of A. Alternatively, the eigenvalues of a matrix are deﬁned as the roots of the
characteristic polynomial of the matrix:
|A −λI| = 0,
(3.2.2)
where | · | denotes the matrix determinant. The characteristic polynomial of the adjacency
matrix A is of n-th degree and, as such, has n roots (eigenvalues). Some eigenvalues may
coincide, in which case the algebraic multiplicity of an eigenvalue λi is the number of times
λi appears as a solution to (3.2.2). Finally, the eigenvalues and eigenvectors of an adjacency
matrix A leads to its eigendecomposition [Fra00]:
A = UΛUT ,
(3.2.3)
where Λ is a diagonal matrix (i.e., [Λ]ii = λi) containing the n eigenvalues and the columns
of matrix U = [u1, u2, . . . , un] contain the corresponding eigenvectors.
The graph spectrum can be calculated using the adjacency matrix A or other related
matrices, notably the normalized Laplacian one. The Laplacian matrix L is a related matrix

Algebraic Graph Analysis
37
representation of a graph that is given by:
L = D −A,
(3.2.4)
where D is the degree matrix of the graph. The degree matrix is a diagonal matrix containing
information about the degree of each vertex (i.e., the number of edges attached to each
vertex [D]ii = P
j[A]ij). For undirected graphs the Laplacian matrix is symmetric positive
semi-deﬁnite (i.e., λi ≥0 ∀i). The symmetric normalized Laplacian is deﬁned as
Lsym := D−1/2LD−1/2 = I −D−1/2AD−1/2
(3.2.5)
and the random walk normalized Laplacian is deﬁned as
Lrw := D−1L = I −D−1A.
(3.2.6)
The normalized Laplacians are used in various graph analysis tasks, such as graph clustering
and random walks on graphs.
The eigenvalues λi of any graph matrix representation are typically sorted in non-
decreasing order λ0 ≤λ1 ≤· · · ≤λn−1. The graph spectrum calculated using diﬀerent
graph matrices are not the same and, depending on which matrix was used to calculate the
spectrum, the eigenvalues can be in diﬀerent value ranges and relate to graph properties
in diﬀerent ways. The graph spectrum coming from any such matrix is invariant to graph
isomorphisms, i.e., the calculated eigenvalues are invariant to vertex permutations [Chu97].
Unfortunately, it is possible that two non-isomorphic graphs are co-spectral, i.e., have the
same eigenvalues [Chu97]. Thus, comparing the two graph spectra is not a completely reli-
able way to test graph isomorphisms.
We will now provide an overview of some bounds and interesting properties of graph
spectra calculated from the normalized Laplacian matrices. The interested reader may refer
to [Chu97] for more in-depth discussion and proofs. The normalized Laplacian eigenvalues
of a graph G with n vertices satisfy:
0 = λ0 ≤λ1 ≤· · · ≤λn−1 ≤2.
(3.2.7)
The smallest eigenvalue, λ0 is always zero, because a vector 1 having all entries equal to
1, is an eigenvector for λ = 0 for any Laplacian matrix. The equality λn−1 = 2 applies,
if and only if G is bipartite. The sum of the eigenvalues is always equal to Pn−1
i=0 λi = n.
The second smallest eigenvalue λ1 is the so-called algebraic connectivity of the graph. If
λ1 > 0 then G is connected. Otherwise, the multiplicity of eigenvalue 0 is equal to the
number of connected components in G [Chu97]. This follows directly from the fact that the
spectrum of the union of two disjoint graphs is the union of their respective eigenvalue sets.
This property is very useful in designing spectral graph clustering algorithms, as it will be
described in Section 3.5.
3.2.2
Similarity matrix and nearest neighbor graph
In many cases, the graph that connects the data is not available. In order to use spectral
graph analysis techniques, a graph has to be constructed over the data [vL07]. The data
samples xi
∈
Rn are usually in vectorial form and they form the data matrix X =
[x1, x2, . . . , xn]. In order to use graph analysis techniques on such a dataset, we form a
weighted graph, where the data samples reside on the graph nodes and the edges between
the nodes are constructed based on various strategies. The adjacency matrix of such a
graph is typically a similarity matrix, containing scores that measure the similarity between
two data samples. The value [S]ij in the matrix is usually calculated using the heat kernel

38
Graph-Based Social Media Analysis
e−||xi−xj||2/2σ2 [AR13]. It is evident that the similarity matrix represents a complete graph,
whose edge weights are equal to the similarity between the corresponding graph vertices
that are connected by each edge. Alternatively, one can construct sparse adjacency matrices
based on the similarity matrix, as described below.
The nearest neighbor graph is constructed using the similarity matrix of the data and a
speciﬁc method to prune it to produce the adjacency graph. A well-known approach is to
produce symmetric nearest neighbor adjacency matrices as follows:
A(i, j) =
(
S(i, j) or 1,
i ∈N(j) or j ∈N(i)
0,
otherwise,
(3.2.8)
where N(j) denotes the set containing the indices of the k-nearest neighbors of sample j.
Alternatively, one can use a threshold ϵ on the similarity, in order to ﬁnd the neighbors of
the vertex i. In that case the graph is called ϵ-neighborhood graph [vL07]. Obviously, the
adjacency matrix in (3.2.5) and (3.2.6) can be replaced by S to produce the corresponding
Laplacian matrices. The way the adjacency graph is constructed aﬀects spectral graph
analysis [MM13]. All the above matrices capture data geometry and can be used in various
tasks, ranging from clustering and classiﬁcation to label propagation and dimensionality
reduction.
3.3
Applications of graph analysis
Graphs provide a ﬂexible, powerful, and useful tool for the representation of a wide
variety of entities. As such, the applications of the graph related tasks surveyed in this
Chapter are numerous. A concise summary of applications can be found in Table 3.3.1.
Random graph generation is useful for providing realistic synthetic graphs, so that graph-
based algorithms can be tested for performance and scaling capabilities. As it is often the
case, real graph data are limited in size, or completely unavailable. Furthermore, the deter-
mination of the ground truth may be almost impossible. In such cases, randomly generated
graphs provide the only means for algorithms to be tested. The temporal evolution of real
graphs can also be studied and predicted by appropriate random graph generation mod-
els. It is also possible to generate random graphs for scheduling simulations on distributed
systems [CMP+10].
Graph clustering is a very useful tool for the analysis of graph-based data. Graph connec-
tivity is easier to analyze when the graph is clustered and interesting substructures become
easier to identify [MGSZ02]. The computational load can also be split for various graph
analysis algorithms, as, depending on the task, clusters can be processed separately [Ide04].
It can be employed to improve the performance of tag recommendation methods in social
media [PKV10], i.e., for the vast quantities of user generated content (photographs, video
and audio clips) that are available online. This is mostly accomplished by ﬁnding additional
appropriate tags for a media item that the original user/owner may have omitted, in order
to ﬁnd more items that are relevant to user searches. It can also help graph compression
algorithms to lower the bits per link required to encode the graph. In database systems,
graph clustering can improve the speed with which relative data are accessed through more
appropriate paging [DRSS96]. Graph compression algorithms are also more eﬃcient, if a
good graph clustering can be attained [BRSV11].
Graph matching has a wide variety of applications in pattern recognition and computer
vision [DPZ01]. There has been an abundance of graph-based works in 2D [EF86] and 3D

Algebraic Graph Analysis
39
[SF83] image analysis [CFSV04]. Biometric identiﬁcation problems, such as face recogni-
tion, can be expressed in terms of graph matching, more speciﬁcally elastic graph matching
[TKP01]. In this framework, vertices are arranged in a square grid (graph) and then over-
laid on a training image. The image features (for example, 2D Gabor ﬁlter bank output)
at the image location of every vertex are computed and stored. In order to classify a test
image, the trained grid vertices are placed on a test image and are displaced, trying to
ﬁnd a location where the image features of the training grid vertex most closely match
the features of the test image. This procedure deforms the grid (graph). The strain on the
graph edges cause by this deformation, and the graph dissimilarity to the local test image
features, can be used to measure the similarity between the training and test image. The
same technique, with some appropriate modiﬁcations, can also be used for facial expression
recognition [KP07]. Several image registration [PF97] and retrieval [HH98] techniques are
based on graph matching [CCLS07]. A graph can be used to capture the relative positions
of various image features, with respect to each other. This provides a more abstract image
representation that avoids the most common problems that plague appearance based im-
age representation methods, such as illumination changes and geometric transformations.
Reliable similarity measurements between two images can be obtained, provided that there
are enough correctly established correspondences between the vertices of the two image
representation graphs. Graph matching techniques for document processing, such as opti-
cal character recognition (OCR) [CL90], string matching [FGK95], and symbol recognition
[LMV01] have also been developed, though they can be considered as subcases of image
registration.
Random walks have found applications in local graph clustering, as a random walk start-
ing from any vertex is more likely to remain in the vertex cluster than to move to another
cluster. Another application of random walks is to estimate the number of elements in a
set, or the volume of an object, whose exact computation would otherwise be intractable,
through random sampling. Mobile agents often perform random walks, for example in wire-
less networks, and some web spiders crawl the web in this fashion. Therefore, random walks
can be used to model their behavior [Ber09] and provide performance metrics for various
algorithms, like how long it is expected to take for a mobile agent to reach a target node, or
until two agents reach the same node [KKR06], under a proposed scheme. Random walks
can even be used for image segmentation. By seeding some pixels with a label, e.g., through
user initialization, the probability of pixels belonging to each label propagates through the
image as a random walk that is less likely to cross image edges [Gra06]. Google’s PageRank
models the behavior of a web surfer largely as a random walk.
Anomaly detection methods can be used for fraud detection; as stolen credit cards and
ﬁnancial scandal detection, such an example that had a signiﬁcant impact was the Enron
scandal [SA05]. Using a measure of graph entropy, important nodes can be identiﬁed through
the magnitude of the change in graph entropy caused by their removal from the graph.
Network intrusions, attempts to gain unauthorized access to systems and online attacks
can be classiﬁed as anomalies. A general way of detecting such attacks is by measuring
the deviation of the network under attack when compared to its normal operation. Once
detected, these attacks can be prevented or remedied [Ide04].
With the proliferation of social networks and social media in the last few years, there has
been an increasing interest in their analysis, while businesses already viewed them as po-
tential new markets to be exploited. Several, if not all, tasks related to social networks map
well to graph analysis algorithms that are reviewed in this chapter. Community detection,
i.e., ﬁnding groups of users that are densely connected with each other, is almost synony-
mous to vertex clustering in graphs. Recommendation systems, ﬁnding pictures, music, or
video clips that are relevant to speciﬁc user interests can be provided with more and better
suggestions based on social graph clustering according to user interests. It is sometimes

40
Graph-Based Social Media Analysis
TABLE 3.3.1: Overview of graph analysis applications.
Graph analysis task
Application
Field
Graph generation
Algorithm testing
Software engineering
Network evolution
Social network analysis
Algorithm simulation
Graph clustering
Data storage
Database systems
Data compression
Popularity prediction
Social network analysis
Tag recommendation
Substructure indentiﬁcation
Computer networks
Network usage optimization
Graph matching
2D,3D Image analysis
Computer vision
Face recognition
Face veriﬁcation
Object registration/retrieval
Document analysis
Language engineering
Molecular structure study
Computational chemistry
Random walks
Enumeration
Multiple
Volume computation
Computational geometry
Mobile agent modelling
Distributed systems
Web crawling
Internet computing
Anomaly detection
System intrusion detection
Computer security
Network attack detection
Financial fraud detection
Law enforcement
Inﬂuential individual detection
Social network analysis
possible to classify communities with a label. Centrality is one way of identifying important
vertices in a social network. Graph anomaly detection methods provide alternative inter-
pretations on what is important, because vertex importance can have various meanings.
For example, inﬂuential people, including but not limited to celebrities, can help to faster
propagate ideas, news, and stories. People that pose security threats, such as criminals, can
also be considered important and their identiﬁcation would be of great interest. Information
diﬀusion is another important issue that can be very well described by graphs. With the
plethora of new ideas born daily and their rapid propagation provided by social networks,
it is interesting to be able to predict which idea will catch on and where they will originate
from, thus allowing people to identify, predict, or capitalize on new trends. This can be
done by studying diﬀusion mechanisms over graphs.
3.4
Random graph generation
Being able to randomly generate graphs that have certain properties can be very useful
in many applications [WS98]. For example, one may wish to generate graphs with predeter-
mined vertex clusters, in order to test a clustering algorithm, since ground truth is diﬃcult
to deﬁne on real data sets. A generation method that models the growth of, e.g., a social

Algebraic Graph Analysis
41
network may well be used to study the future evolution of the said network and, thus, test
the scalability of the relevant graph analysis algorithms.
3.4.1
Desirable random graph properties
There are several ways to evaluate the clustering results in graph vertex clustering. A
measure to evaluate how densely connected the graph vertices are is provided by the average
clustering coeﬃcient [WS98]. A vertex clustering coeﬃcient is deﬁned as the fraction of
edges in the induced subgraph of the vertex neighbors (excluding the vertex) over the total
possible edges of that subgraph. The average graph clustering coeﬃcient is obtained by
averaging the clustering coeﬃcient of all the graph vertices.
In various real life graphs, such as the ones corresponding to social networks, two major
observations have been made: a) the vertex degrees follow a power law, i.e., they have a
heavy-tailed probability distribution and b) the diameter of these graphs (i.e., the greatest
distance between any pair of graph vertices) actually decreases as they grow larger over time,
contrary to expectations [LKF05]. The small diameter of large social networks, otherwise
known as the small world phenomenon, was studied as early as in the 1960s [Mil67]. It
appears that almost everyone in the world can be connected to almost everyone else with
a relatively small number of mutual acquaintances, typically about 6 or 7. Studies in web
social networks indicate that this holds even as the network graph expands with new vertices
[LKF05]. Therefore, any generation model for such graphs should produce graphs that retain
these two properties, which will subsequently be described in detail.
A random variable x is formally said to follow a power law, if its probability distribution
p(x) has the form p(x) ∝x−α, where typically 2 < α < 3 [CSN09]. Usually, it is assumed
that x ≥xmin to avoid indeterminacy (division by zero). Such probability distributions are
called heavy-tailed, because they do not approach zero quite as fast as x increases. This,
in eﬀect, means that extreme values, such as very high vertex degrees, are not as unlikely
to appear as other distributions would suggest (e.g., the Gaussian one). An interesting
property of a power law is that it is scale invariant. Supposing that p(x) = x−α, one can
easily see that:
p(cx) = (cx)−α = c−αx−α = c−αp(x).
(3.4.1)
Graphs, whose vertex degrees follow a power law, are called scale-free graphs (or networks)
[BB03]. Another interesting property of a power law distribution is that its logarithmic
function ln p(x) vs. ln x (in the graph case, the logarithm of number of vertices having
a given degree vs the logarithm of that vertex degree) plot is closely approximated by a
straight line. This method of plotting is usually referred to as a log-log plot. An example
of the power law in a graph constructed from video data obtained from YouTube [TPP14]
can be seen in Figure 3.4.1, where the log-log plot appears to follow the power law within
the degree range from 10 to 1000.
3.4.2
Random graph generation models
The Erd¨os-R´enyi method is the simplest one for generating a graph with n vertices, by
connecting any two vertices by an edge with probability p [Gil59]. The degree distribution of
graphs generated using this method is a Poisson one. This is not a particularly good model,
as the uniformly random edge placement can not guarantee the presence of dense clusters.
Hence, it is rather unlikely that the generated graph will have the desirable properties
observed in real networks. This generation model has been thoroughly studied in [ER60].
The Watts-Strogatz method [WS98] starts from a graph, whose vertices are placed
on a circle and are ordered numerically. Each vertex is connected to its adjacent vertex

42
Graph-Based Social Media Analysis
(a)
(b)
FIGURE 3.4.1: The power law in a graph constructed from data obtained from YouTube:
a) The degree distribution and b) the corresponding log-log plot.
and every kn-th vertex for some integers kn. In a more formal way, an edge (ui, uj) ∈
E is generated, iﬀi ≡j mod kn. Then some edges are relocated, avoiding self loops and
duplicate connections, with probability p. Depending on the value of p, the graph can re-
tain most of its original structure for p values close to 0 and become completely random
for values close to 1. For intermediate p values, the generated graphs have high clustering
coeﬃcients and low diameters. However, the probability distribution of the vertex degrees
does not follow a power law.
In order to satisfy the power law distribution of vertex degrees, [RB02] employs a pref-
erential attachment procedure, which favors increasing the degree of vertices already having
high degrees (“the rich get richer” approach), in order to ensure that there will be suﬃcient
vertices with much higher degrees than average and, thus, satisfy the power law distribu-
tion. Starting from an initial set of vertices with degree 1, vertices are incrementally added
into the current graph. Any new vertex gets a set amount of new connections. However, the
selection of its neighbors is not uniformly random. An existing vertex ui has a
deg(ui)
P
j deg(uj)
probability of being chosen for the new vertex neighborhood. This makes practical sense,
as, for example, there are popular people in communities and anyone new in the community
is very likely to encounter the popular people ﬁrst. In the case of web pages, an informative
web page has many incoming links, and new web pages on the same subject are more likely
to link to it. The drawback of this method, however, is that the diameter tends to increase
logarithmically with the number of vertices.
A random graph generation model that actually produces graphs that have a high clus-
tering coeﬃcient, a small diameter, and whose degree distribution observes a power law is
presented in [LCKF05]. The method is based on the Kronecker multiplication A of the n×n
matrix A1 and the m × m matrix A2:
A =


a1(1, 1)A2
a1(1, 2)A2
. . .
a1(1, n)A2
a1(2, 1)A2
a1(2, 2)A2
. . .
a1(2, n)A2
...
...
...
...
a1(n, 1)A2
a1(n, 2)A2
. . .
a1(n, n)A2


.
(3.4.2)
It is easy to see that the Kronecker product A is an mn×mn matrix. The Kronecker product
of two graphs is described by the product of their adjacency matrices. The k-th Kronecker

Algebraic Graph Analysis
43
power of a graph G can be deﬁned by the k Kronecker multiplications of its adjacency matrix
by itself. Figure 3.4.2 presents an example of a graph Kronecker multiplication by itself. The
main idea for its use in graph generation is raising a suitable seed graph G to the appropriate
power, in order to obtain a graph of the required size. To include certain randomness in the
generation procedure, instead of using the binary adjacency matrix A of G, it is suggested
to replace the 1s and 0s with probabilities p(1) and p(0) (0 ≤p(0) < p(1) ≤1), respectively,
in the adjacency matrix and, thus, obtain a probability matrix. When raised to the k-th
Kronecker power, the elements of the resulting matrix will be the product of l times p(1)
and (k −l) times p(0), 0 ≤j ≤k. This Kronecker product provides a probability that the
corresponding element in the ﬁnal graph adjacency matrix will be 1; otherwise it will be
zero. The adjacency matrix is determined accordingly. Graphs generated using this method
are proven to maintain the non-increasing diameter and, with a careful choice of the seed
graph G, have a power law probability distribution of the vertex degree [LCKF05].
A1 =


1
1
0
1
1
1
0
1
1

, A2 = A1 ⊗A1 =
A1
A1
0
A1
A1
A1
0
A1
A1
FIGURE 3.4.2: The second Kronecker power of an example graph G.
According to [MX07], the graphs generated by Kronecker multiplication are not search-
able by a distributed greedy algorithm. In order to rectify this, a Kronecker-like operation
is used in [BBHW10], in which a seed graph G is not Kronecker multiplied by itself but by
suitable graphs Hi from a family of graphs H. Depending on the deﬁnition and parameter
selection of the graphs Hi, this model can generate graphs similar to the Watts-Strogatz
method and the original Kronecker product method. The authors, however, suggest select-
ing the graphs Hi in such a way, so that, in the ﬁnal probability matrix, the probability of
vertices ui and uj being connected is determined by the Hamming distance h(i, j) of their
indices i and j:
p(ui, uj) =
(
1, if h(i, j) ≤1
h(i, j)−α, otherwise.
(3.4.3)
The resulting graphs are essentially extensions of a n-dimensional hypercube. There has
been evidence that this assumption is not far from reality [KPBV09]. It can be proven that
such graphs are searchable [Kle00].
3.4.3
Spectral graph generation
A generative model for graphs that is based on spectral graph theory is presented in
[XH06]. The method begins by embedding the graph into a heat kernel Ht, which is achieved
by exponentiating the elements of Λ of the Laplacian eigenspectrum L = UΛUT and then
performing a Young-Householder decomposition of the resulting matrix Ht to retrieve the
coordinate matrix Y [Bro87].
Ht = Ue−1
2 ΛtUT = YT Y.
(3.4.4)
The columns of matrix Y are the coordinates of the mapping of the corresponding eigenvec-
tor to the heat kernel space. Given a set of graphs T = {G1, G2, . . . , GN} (not necessarily
of the same size), the coordinate matrices are truncated to remove extraneous eigenvectors

44
Graph-Based Social Media Analysis
that the smallest graph does not have. Let ˆYi be the truncated coordinate matrix of graph
Gi, after the eigenvectors of the least signiﬁcant eigenvalues have been removed. The largest
graph is selected as the reference graph for the generative model.
Since the proper order of the eigenvectors may not be the same for every sample graph,
the correspondences between the retained eigenvectors of the reference graph and each of
the rest of the sample graphs must be established. The Scott and Longuet-Higgins algorithm
[SLh90] is used to ﬁnd these correspondences, which are stored in the binary correspondence
matrix Ci for each graph Gi. The mean and the covariance matrix of the distribution of
graph embeddings over T are calculated as follows:
ˆX = 1
N
N
X
1
CT
i ˆYi
(3.4.5)
Σ = 1
N
N
X
1
(CT
i ˆYi −ˆX)(CT
i ˆYi −ˆX)T .
(3.4.6)
In a ﬁnal preprocessing step, the covariance matrix Σ is eigendecomposed into:
Σ = ΨΓΨT .
(3.4.7)
Where Γ denotes the diagonal matrix of eigenvalues and Ψ denotes the matrix, whose
columns are formed by the eigenvectors, ordered by the corresponding eigenvalues in Γ. An
input observation map on the heat kernel space is ﬁt into the model through a parameter
vector b, whose optimal value b∗is calculated as:
b∗= arg min
b
( ˆY −ˆX −Ψb)T ( ˆY −ˆX −Ψb) = ΨT ( ˆY −ˆX).
(3.4.8)
In order to reconstruct the adjacency matrix of a graph embedded in the heat kernel at
point ˆY, the coordinate matrix ˆY∗is reconstructed ﬁrst:
ˆY∗= ˆX + ΨΨT ( ˆY −ˆX).
(3.4.9)
Then the heat kernel and the Laplacian matrix are reconstructed as follows:
ˆH∗
t = ( ˆY∗)T ˆY∗
(3.4.10)
ˆΛ∗= −1
N ln ˆH∗
t .
(3.4.11)
Finally, the reconstructed graph adjacency matrix can be recovered from its reconstructed
Laplacian:
A∗= D −D
1
2 ˆΛ∗D
1
2 ,
(3.4.12)
according to (3.2.4).
The incorporation of the eigenvalues into the long-vector eigenvector representation in
(3.4.7) can lead to reconstruction issues [WW07]. Thus, it is suggested that the eigenvalues
and the concatenated eigenvectors forming the aforementioned long-vector are split up. Let e
be the eigenvalue vector and z be the eigenvector long-vector (concatenated eigenvectors).
As before, the means ¯e and ¯z and covariance matrices Σe and Σz are calculated. The
covariance matrices are eigendecomposed into Γe, Ψe and Γz, Ψz, respectively. Parameter
vectors be and bz are used to ﬁt the data into the generative model:
ˆe = ¯e + Ψebe, ˆz = ¯z + Ψzbz.
(3.4.13)

Algebraic Graph Analysis
45
In order to generate a new graph, random parameter vectors be and bz are drawn from a
normal distribution and vectors ˆe and ˆz are converted into the eigenvalue matrix ˆΛ and
eigenvector matrix ˆΦ, respectively. Since the parameter vectors were randomly generated,
the eigenvectors may not form an orthogonal base. This is corrected by projecting the
eigenvectors to form an orthogonal matrix ˆΦO:
ˆΦO = (ˆΦˆΦT )−1
2 ˆΦ.
(3.4.14)
The graph Laplacian is reconstructed as per equations (3.4.11), (3.4.12). However, due to
the split between eigenvalues and eigenvectors and the orthogonalization step, the adjacency
matrix contains negative entries. A ﬁnal thresholding on its values is needed to acquire the
adjacency matrix of the randomly generated graph that is similar to the original sample
set.
When compared to non-spectral graph generation techniques, using the graph spectrum
allows for generating variants of an already existing graph. It is, therefore, better suited for
testing graph similarity algorithms. When the changes remain small-scale, the realism of
the generated graph is not far from the original graph, in case real data are used for seeding
the generator. However, there are no theoretical guarantees that the generated graphs will
retain the power law distribution of vertex degrees, or the small diameter, particularly if
the eigenvector displacement is signiﬁcant.
3.5
Graph clustering
The graph clustering objective is to gather graph vertices into groups (clusters), so that
the vertices in each cluster are closely related to each other. There is no formal deﬁnition
of what a cluster is and exactly what properties it has. In general, vertices of the same
cluster should be heavily connected to other vertices within the same cluster, while being
sparsely connected to the rest of the graph. In general, interconnection can be measured by
the density of the cluster induced subgraph, while connectivity outside the cluster can be
measured by the size of the graph cut removing the cluster from the graph.
A thorough survey on graph clustering is provided in [Sch07]. There are two broad cate-
gories of clustering algorithms: global and local ones. As the name implies, global algorithms
(a)
(b)
FIGURE 3.5.1: The adjacency matrices of the same graph: a) where vertices are ordered
in such a way that they are grouped together, if they belong to the same cluster, b) under
arbitrary vertex ordering.

46
Graph-Based Social Media Analysis
require knowledge of the entire graph. Since many real graphs, such as the Internet, can
number vertices in the billions, global algorithms become unsuitable for such cases. Global
methods can be further subdivided into iterative, divisive, agglomerative and hierarchical
ones. Local clustering algorithms use adjacency lists of a vertex and its neighbors.
3.5.1
Global clustering algorithms
Iterative methods, in general, go through all vertices and assign them to clusters. The
decision can be ﬁnal, but it can usually be revised, by going through the vertices again and
changing vertex assignments, based on the optimization of a metric and on previous results.
It is also possible to process one vertex at a time and gradually create and update clusters
based on what has been encountered thus far. Such methods are called online ones.
Clusters need not be rigidly deﬁned and subclusters can be contained within the same
cluster. This is a reasonable hypothesis in real-life applications. For example, companies have
departments and employees, who, while still closely interconnected with other people in the
same company are even more interconnected with employees in the same department. This
implies a hierarchical structure of clusters to which a vertex can belong. There are several
clustering methods that accommodate this. An illustration of this concept can be found
in Figure 3.5.2, which shows a graph vertex dendrogram, where vertices are grouped into
increasingly larger clusters. At each dendrogram level, vertices are considered to belong to
the same cluster if they have a common ancestor.
FIGURE 3.5.2: A dendrogram of graph vertices providing a hierarchical clustering.
An online hierarchical clustering method can be divisive, starting from the entire graph
in one cluster, then recursively splitting clusters oﬀin a top-down fashion. The majority of
these methods boil down to selecting proper graph cuts at each step. The size of the cuts,
i.e., the number of edges removed, should be minimal. It can be proven that the minimal
cut can be acquired through a maximum-ﬂow algorithm [CSRL01]. The issue that arises,
however, is how to prevent the algorithm from choosing trivial cuts, like isolated vertices
of 2,3-cliques, over bigger cuts that separate clusters. Imposing size restrictions on the cut
renders the graph clustering problem NP-hard [Sch07].
Another class of divisive clustering methods is based on the removal of appropriate
edges. The betweenness measure of an edge [Fre77] is deﬁned as the number of closest
paths between any two vertices that go through that edge. The idea is that edges with high
betweenness are more likely to connect the vertices of a cluster to ones belonging to the rest
of the graph, rather than vertices in the same cluster. By iteratively removing the edge with
the highest current betweenness, and recomputing the measure for the remaining graph, it
is expected that the clusters will eventually become disconnected from each other.
Agglomerative online hierarchical clustering methods are an alternative to the divisive
approaches. They start from an empty set of clusters and then add clusters or assign new

Algebraic Graph Analysis
47
processed vertices to existing clusters. It is even possible to merge two clusters into a bigger
one. This usually involves the optimization of some ﬁtness measure [Sch07].
An issue with such approaches, however, is that the order in which the vertices are
presented to the algorithm can signiﬁcantly change the clustering output. If an actual clus-
ter is presented in sequence to an algorithm, it may assign incoming vertices to diﬀerent
clusters, thus splitting the cluster up. Another possibility is that the ﬁrst few vertices of
a cluster get assigned to an already existing cluster and, thus, both the existing cluster
and the new cluster will be sub-optimal choices. If the decisions that an algorithm makes
are irreversible, then the clustering results can be quite inadequate. To rectify this, most
algorithms can revise intermediate clustering results at later iterations guided by a global
clustering suitability measure.
Taking a cue from physics, one can consider the graph edges as resistances in an electrical
circuit, connected to the corresponding vertices. If a battery was connected to this circuit,
the voltage would spread in all the vertices, as Figure 3.5.3 illustrates. Vertices belonging to
the same cluster are expected to have similar voltage values, as they are densely connecting
the cluster vertices. Alternatively, vertices can be classiﬁed as being closer to the source
of electricity or closer to the sink of electricity, depending on their voltage. Thus, a graph
can be split in two, with the process being recursively applied to the new subgraphs. The
computational concerns of this approach can be eﬃciently tackled with circuit analysis tools.
The issue with this approach is that there is no way to determine the best placement of the
battery ends (source and sink).
A stochastic, global approach to graph clustering is presented in [TPN12]. As illustrated
in Figure 3.5.1, when vertices belonging to the same cluster are placed in consecutive places
in the ordering of the adjacency matrix, intense square blocks appear along the diagonal.
In terms of Discrete Cosine Transform (DCT) [Pit00], this means that the sum of the
diagonal elements of the DCT matrix are maximized, when the vertices are properly ordered.
In a proof of the concept of the reverse, Simulated Annealing (SA) was used, in order to
maximize the objective function, which is the sum of the k ﬁrst diagonal elements of the DCT
transform of the graph adjacency matrix. At each iteration, two randomly selected vertices
switch places in the adjacency matrix order and the change in the sum is computed. If the
objective function increases, the transition is automatically accepted. If it decreases, then
the transition is accepted stochastically, as per standard SA operation. Results indicated
that the method can be eﬀective in graph clustering.
FIGURE 3.5.3: The graph as a circuit comprised of resistances.

48
Graph-Based Social Media Analysis
3.5.2
Local clustering algorithms
As mentioned before, some graphs (such as the World Wide Web) are extremely large.
Thus, any clustering algorithm with superlinear complexity is not a feasible option. Fur-
thermore, storing the entire adjacency matrix in memory is impossible. Local clustering
techniques are designed to operate with only a small part of the graph being visible at a
time, usually through vertex adjacency lists or subgraphs. First, a seed vertex u is selected.
Then the task is to ﬁnd which other vertices belong to the same cluster with u, by ac-
cessing its neighborhood up to a small maximum distance. There are several heuristic or
probabilistic local search methods that can expand the cluster of u, by optimizing a suitable
criterion. Simulated annealing can also be used for local graph clustering, as it has the ad-
vantage that it can escape local optima by stochastically accepting worse solutions, in order
to ﬁnd even better ones later down the road. Local clustering algorithms can also be used
in order to obtain a global graph clustering, e.g., by running a local clustering algorithm
for every vertex and then combining the results through a voting scheme to determine the
global clusters.
Random walks can be exploited to cluster graph vertices [vD00]. The probabilities of a
random walk ending at a given vertex is closely related to the spectrum of the transition
matrix. The basic idea is that, when a vertex is visited, then the random walk will most
likely visit vertices belonging to the same cluster, before eventually proceeding to a vertex
of a diﬀerent cluster. Figure 3.5.4 shows an illustrative example of this idea for two clusters
being connected through an edge. A random walk algorithm can only move from one cluster
to another one through the edge with the highlighted vertices. Even when the walk is at
those vertices, it is still more likely to remain within the cluster.
FIGURE 3.5.4: Random walk in a two-cluster graph.
3.5.3
Spectral clustering algorithms
Spectral graph clustering techniques were inspired by the observation that, when a graph
is comprised of k disjoint cliques, the k smallest eigenvalues of the normalized Laplacian are
0 and the i-th corresponding eigenvector (0 ≤i ≤k −1) has distinct, non-zero values for
vertices that belong to the i-th clique [Sch07]. Adding edges to such a graph will gradually
cause the eigenvalues to increase (except for the ﬁrst, smallest eigenvalue, which will always
be 0) and the corresponding eigenvectors to change slightly with each edge addition.
A large family of spectral vertex clustering techniques are based on spectral bisection
[GM98]. These techniques use the so-called Fiedler vector [Fie73], i.e., the eigenvector u1
corresponding to the second smallest eigenvalue λ1, 0 = λ0 ≤λ1 ≤· · · ≤λn−1 ≤2 of the
Laplacian matrix of a graph. This eigenvalue is also called the algebraic connectivity of a
graph. In the simplest case, in which there are two clusters, each of which has strong internal
connectivity and are sparsely connected with each other, then the Fiedler vector entries
corresponding to vertices of one cluster will be positive, while the entries corresponding to
vertices of the other will be negative. This provides a bisection of the graph.

Algebraic Graph Analysis
49
In general, there are two ways the Fiedler vector can be used to bisect a graph. For an
edge-based bisection, after the Fiedler vector has been computed, the vertices are split into
two groups, based on the relation of their relevant Fiedler vector entry to the median of
all such entries. Finally, edges between these two vertex groups are cut. For vertex-based
bisections, there are several options [HK92]: a) the sign of the entry is used to split the
vertices, b) the largest gap in values is found and the vertices are split accordingly [OTNP14],
or c) the graph is split at the value that provides the best cut quotient [GM98]. A special
option uses the normalized cut measure and is also known as the Ncut approach [SM00].
The Fielder vector is also useful, when trying to ﬁnd the central vertex of a neighborhood
[QH04]. An application of such a graph clustering technique to facial image clustering is
presented in [VSP11, OTNP14].
The most common algorithms for spectral clustering use one of the normalized Lapla-
cians presented in Section 3.2 and perform eigenanalysis to extract the r eigenvectors that
correspond to the smallest eigenvalues λ1 ≤λ2 ≤· · · ≤λr of the normalized Laplacian,
excluding λ0 = 0. The ﬁrst r eigenvectors are stored in a n × r matrix U. The rows of
this matrix U are the new data representations. A standard clustering algorithm (e.g., k-
means) can be used to cluster them. This new data representation has been generally found
to oﬀer better data clusterability [JB03, KVV04]. It is obvious that the construction of the
similarity graph as well as the selection of the normalized Laplacian plays a crucial role
in clustering performance [vL07, MM13]. Several variants of the basic algorithm have been
proposed in the literature [vL07]. Some of them are discussed below.
In a weighted complete graph, the ﬁrst r eigenvectors of matrix D−1/2WD−1/2 are
computed and stored in a n×r matrix U. These eigenvectors can be used to compute a r-way
normalized cut cost function [JB03]. The orthogonal projection operator deﬁned by matrix
UUT is compared to the orthogonal projection matrix Π0 = Pr
i=1 D1/2eieT
i D1/2/(eT
i Dei)
of a proposed partition into clusters, as indicated by the binary cluster membership vectors
ei. The comparison is performed using the Frobenius norm, thus implicitly comparing the
subspaces spanned by the columns of the ideal cluster assignment matrix, represented by
UUT , and the proposed partition, represented by Π0 [GvL96]. This norm provides the cost
function for the r-way cut-based clustering algorithm.
A method to split a graph into k clusters using k-way normalized cuts is presented in
[YS03]. The task is formulated as a relaxed trace maximization problem:
maximize 1
k tr(ZT WZ),
(3.5.1)
where Z = X(XT DX)−1
2 , D is the diagonal of the graph weight matrix W and X is a
n × k cluster assignment matrix. The solution to this maximization problem is found by
forming matrix Z using the top k eigenvectors of the weight matrix W. It is interesting that
the weighted kernel k-means clustering algorithm can also be formulated as a similar trace
maximization problem tr(YT W
1
2 KW
1
2 Y), where K is the kernel matrix, W is the weight
matrix and Y is an orthonormal matrix containing parameters related to the k centers in the
kernel space [DGK04]. Thus, the k-way normalized cuts and kernel k-means are equivalent
problems.
This is useful, as using the kernel k-means algorithm to perform k-way normalized cuts
circumvents eigenvector computation. Computing the eigenvectors is a time and memory
consuming task, more speciﬁcally it runs in practically O(n3) time and O(n2) space. This
renders classic spectral approaches infeasible for a large number of graph nodes. On the other
hand, kernel k-means can be implemented in a way that takes O(nz) time and memory,
where nz is the number of non-zero elements of the weight matrix. It must be noted that nz
can be equal to n2 in the worst case. Alternatively, an algorithm to perform kernel k-means

50
Graph-Based Social Media Analysis
using only a small fraction of the rows of the kernel matrix, with minimal losses in the
clustering performance, was proposed in [CJHJ11].
Spectral graph clustering techniques require less user input, in terms of determining pa-
rameters and are, therefore, more automated. They are also more elegant from a theoretical
stand point. Detecting trivial clusters (isolated 2-, 3-cliques) is more easily avoided, when
using spectral techniques. Generally speaking, it is more diﬃcult for spectral techniques to
result in very bad clustering. Since they require the Laplacian matrix of the graph, they
cannot be employed for extremely large graphs, due to memory limitations. Even if a large
graph could ﬁt into memory, the eigenanalysis process would take a signiﬁcant amount of
time. In these cases, the kernel k-means algorithm provides a feasible alternative. The use
of kernel k-means in clustering big datasets of facial images is presented in [TTNP15].
3.5.4
Overlapping community detection
While graph clustering techniques can also be used for community detection purposes,
the exclusive assignment of a graph vertex to a speciﬁc cluster can be inaccurate, as there
can be several overlapping communities that a vertex belongs to, as can be seen in Figure
3.5.5. For example, a person can be considered to belong to the community of their family,
of their co-workers and of a hobby group, among many others. As such, several overlapping
community detection techniques have been developed [For10, XKS13].
FIGURE 3.5.5: Two overlapping communities. Their common vertices are highlighted by
the dotted ellipse.
A rather popular approach to overlapping community detection is the Clique Percolation
Method (CPM ) [PDFV05], based on k-cliques, which are fully connected subgraphs of k-
nodes. Two k-cliques are considered to be adjacent if they have k −1 vertices in common.
Several adjacent k-cliques form a k-clique chain. A k-clique community is deﬁned to be a
maximal k-clique chain. This allows vertices to belong to overlapping communities, while
also tolerating vertices in the same community that are not directly connected by an edge.
The method begins by ﬁnding maximal cliques. Although this is an NP-complete problem,
it was found that it can run in a reasonable amount of time on sparse graphs, such as
those encountered in real problems. Let nc be the number of maximal cliques found. A
nc × nc clique-clique overlap matrix is then computed. The communities correspond to the
connected components of this matrix [EB98]. An implementation of CPM by the original
authors named CFinder1 can be found in [APF+06]. The method can be accelerated by
starting from an empty set of cliques. It continues by sequentially inserting a new edge
and by updating the existing cliques. This extension is called Sequential Clique Percolation
(SCP) [KKKS08] and can also be eﬃciently applied to weighted graphs, by processing the
edges in descending weight order.
Some alternative overlapping community detection approaches are presented in
1http://cﬁnder.org

Algebraic Graph Analysis
51
[BGK+05]. Rank Removal works by assigning a score to each vertex, corresponding to
the importance of that vertex, using, e.g., betweenness, or even PageRank [BP98]. The
most important vertices are then removed, until the graph only consists of disconnected
components no larger than a user-deﬁned cardinality. These components form the core of
their respective community. The removed vertices are then added back to the graph and
included in a community, if doing so increases a chosen metric, measuring the internal vs.
external connectivity of that community. It is possible to add the same vertex to more
than one community, thus allowing communities to overlap on such vertices. Iterative Scan
begins with a community using a seed vertex, then adds and removes vertices, greedily
maximizing an appropriate metric similar to Rank Removal, until no change can further
improve that metric. Once such a point is reached, the currently included vertices form a
community and a new seed vertex is selected. The process continues using new seeds, until
the process fails to produce a community that has not already been detected a set number
of times. It has been found that applying Iterative Scan on the community cores detected
by Rank Removal provides the best results [BGMI05]. Finally, because it is possible for
Iterative Scan to produce disconnected communities, a variant called Connected Iterative
Scan checks each community for correctness after every insertion and deletion [Kel09].
It is also possible to work on graph edges, instead of graph vertices, for overlapping
community detection. The main concept of such approaches is to work on the line graph
of the original graph, i.e., the graph in which the original graph edges are the new graph
vertices. Two vertices of the new graph are connected, iﬀthe corresponding edges in the
original graph are incident to the same vertex. Performing clustering on the line graph
to obtain a non-overlapping vertex partition results in an edge partition in the original
graph. Communities can then be formed, by assigning the vertices incident to every edge
in a partition to the corresponding community. While edge partition is non-overlapping,
a vertex can belong to every community its edges were clustered into, thus allowing for
overlapping vertices. Approaches based on this concept utilize hierarchical clustering into a
dendrogram of edge communities [ABL10] and random walks [EL09]. The construction of
the line graph is not always necessary, as [KJ11] presents an extension of Infomap [RB08]
to directly ﬁnd edge communities.
A spectral community detection approach [WS05] is designed to ﬁnd k communities
and involves computing the top k −1 eigenvectors. These vectors are used to obtain an
embedding of the graph in a Euclidean space and perform traditional clustering methods
in that space, such as k-means. This, however, results in non-overlapping communities. An
extension of this approach is presented in [ZWZ07] and can perform overlapping spectral
community detection through fuzzy clustering. Fuzzy c-means is used to acquire a fuzzy
cluster membership vector for each vertex, whose k entries correspond to the probability
that the vertex belongs to each of the k clusters. These entries are non-negative and their
sum is 1. Thresholding the cluster memberships can assign a vertex to multiple communities.
3.6
Graph matching
In graph matching, the basic task is to establish a correspondence between the vertices
of two graphs, whose vertices can have additional features, such as those illustrated, with
the assistance of a tool provided by [ZD13], in Figure 3.6.1. In general, ﬁnding the opti-
mal correspondence can be reduced to solving the following quadratic assignment problem

52
Graph-Based Social Media Analysis
FIGURE 3.6.1: Graph matching between two graphs, whose vertices contain image features.
[Ans03]:
c∗= arg max
c

X
i
X
j
cijf(i, j) +
X
i
X
j
X
k
X
l
cikcjlg(i, j, k, l)

,
(3.6.1)
where cij = 1, iﬀelement i (a vertex, in the case of graphs) is assigned to element j and
0 otherwise. Function f() is a matching score function between two elements and function
g() is, respectively, a matching function of the pair wise assignment between elements i and
j to elements k and l. Various restrictions can be further imposed, such as P
j cij = 1, ∀i,
so that no double assignments to the same element are permitted. However, the quadratic
assignment problem is NP-hard. An extensive survey of both exact (isomorphic) and inexact
graph matching can be found in [CFSV04].
Continuous optimization methods have been adapted to suit the graph matching prob-
lem. By deﬁning matching in terms of a continuous function, known optimization methods
can be used to compute the function maximum and then use it to retrieve the respective
graph matching. A technique based on relaxation labeling [FE73] employs a probability
vector for every vertex of the ﬁrst graph, representing the probability that the vertex is
matched with each vertex of the other graph. In an iterative process, these probabilities are
recalculated, based on the probabilities of neighboring vertices, until a termination criterion
is met, e.g., when the optimization reaches a static point, or when the maximum number
of iterations is reached. Insight on the probabilistic framework behind this approach can be
found in [CKP95].
The problem of matching an input graph G1 = (V1, E1, A1) with a relational model
graph G2 = (V2, E2, A2), where A1 and A2 contain data associated with each vertex of V1
and V2, respectively, is tackled in [WH97], by introducing a maximum a-posteriori (MAP)
based consistency measure to rectify initialization errors and vertices that correspond to,
essentially, noise, in order to improve matching performance [SH85]. The relational graph
model G2 is allowed to be appended with dummy vertices, so that the cardinality of its
subgraph vertex set matches the cardinality of the vertex set of G1. MAP criterion calculates
the probability of a matching f given the data A1 and A2, using the probabilities of vertex
v1 of G1 matching vertex v2 of G2, given their individual data in a Bayesian framework.
The optimal matching is determined by ﬁnding the individual matching assignments that
maximize this probability. Relational consistency is measured on a vertex neighborhood
scale, using dictionaries of possible relations between the input graph and the model graph
and a Hamming distance measure. In a post-processing step, unmatched vertices in G1 are
discarded.

Algebraic Graph Analysis
53
A way to parameterize the matching function (3.6.1), so that it favors matching graphs
the way a human would match them, by using manually annotated graph matches, is pro-
posed in [CCLS07]. Given a pair of graphs G1 and G2, and a manually provided graph
match c, the objective is to ﬁnd the parameters of a predictor:
gw(G1, G2) = arg max
c
⟨w, Φ(G1, G2, c)⟩,
(3.6.2)
i.e., the weights w of a linear discriminant function and the feature function Φ for which
the error:
1
N
N
X
n=1
∆(gw(Gn
1, Gn
2), cn) + λΩ(w)
(3.6.3)
is minimized, when the graph matching is c. This is essentially an inverse optimization
problem [AO01] that the authors solve, by utilizing the large margin approach [TJHA05].
Once these parameters have been properly determined, the minimization of (3.6.3) provides
better matches than the solutions to the original quadratic assignment problem.
3.6.1
Spectral graph matching
The isomorphism, or near isomorphism, of two graphs G1 and G2 with adjacency matri-
ces A1 and A2, respectively, can be described as the task of ﬁnding a permutation matrix
C, such that CA1CT = A2 [XK01]. This problem can then be reformulated as ﬁnding the
matrix C∗minimizing the trace (sum of diagonal elements) of:
C∗= arg min
C
tr

(CA1CT −A2)(CA1CT −A2)T 
.
(3.6.4)
This can be approximated with a matrix Φ that is orthogonal, but does not follow the
additional restrictions that the permutation matrix C does. It can be proven that:
min
Φ tr

(ΦA1ΦT −A2)(ΦA1ΦT −A2)T 
=
X
i
(λi −µi)2,
(3.6.5)
where λi and µi are the ordered eigenvalues of A1 and A2, respectively, thus indicating
that graphs can be compared through their spectra.
A spectral approach to graph matching can be found in [SB92], where the graph vertices
represent feature points in an image. The task is to ﬁnd correspondences between the
feature points in two diﬀerent images. Instead of an adjacency matrix, a proximity matrix
H is constructed by measuring the Euclidean distance between any two features and using a
Gaussian activation function to obtain the ﬁnal elements of H for each image. The spectrum
of H is then calculated to be used for graph matching and the eigenvectors are ordered in a
decreasing eigenvalue order to become the column vectors of matrix E. Since the number of
features is not necessarily the same, the least important eigenvectors, i.e., the eigenvectors
corresponding to the smallest eigenvalues of the largest graph, are discarded. If E1 and E2
are the matrices of the two images, the sign of the eigenvectors in E2 are determined in such
a way that the orientation of the orthogonal axes of the eigenvectors agree with those of E1.
The rows of E1, E2 form a set of feature vectors for the image features they correspond to.
These feature vectors from both images are correlated and the resulting matrix Z indicates
the dissimilarity score for each pair of features.
Building upon the work in [SB92] described above, in order to improve robustness to
noise and outliers, the use of diﬀerent activation functions on the elements of H, such as the
sigmoidal function, or an increasing weighting function or a Euclidean weighting function, is
investigated in [CH03]. Additionally, instead of simply assigning a correspondence between

54
Graph-Based Social Media Analysis
features with the lowest dissimilarity score, an extension of the Expectation-Maximization
(EM) algorithm [CH98] is used to determine correspondences. Finally, the fact that the
eigenvalue order has been compromised, due to noise, is taken into account, by incorporating
the probability that rows have been misplaced in the formulations used in the EM algorithm.
Spectral graph matching methods are more deterministic, though probabilities can still
be incorporated in them. They also need less user input, in order to perform as well. The
graph spectrum can also constitute a descriptor of a graph for feature extraction purposes.
3.6.2
Frequent subgraph mining
The goal of Frequent subgraph mining (FSM) algorithms is to ﬁnd all the frequent
subgraphs occurring in either a set of medium sized graphs (called “transactions” in the
literature) or a single, very big graph. Since there are no spectral approaches in this ﬁeld,
we will cover it only brieﬂy. For a more detailed survey, the interested reader may refer
to [JCZ13]. There are two main components to FSM: isomorphism testing and candidate
generation. As previously mentioned, subgraph isomorphism is NP-complete and there is no
known polynomial algorithm for graph isomorphism. Two isomorphic graphs always have
the same labeling when canonically labeled; they are not two separate conditions for being
able to easily test isomorphism. However, the canonical labeling algorithm itself has an
exponential worst-case running time. Candidate generation can either follow the Breadth
First Search (BFS) approach, in which candidates of k + 1 size are considered only after
all candidates of size k have been checked, or the Depth First Search (DFS) approach. In
both cases, the aim it to avoid generating duplicate candidates. Frequent subtree mining is
a subﬁeld of FSM that includes more specialized algorithms.
3.7
Random walks
Random walks, as the name suggests, are graph vertex traversals that randomly move
from one vertex to a neighboring one. Most applications of random walks relate to sampling
and approximation [Lov93], the most obvious being the uniform generation of random data.
The following measures are of particular interest, when a random walk is concerned: access
time, cover time, and mixing rate. The access time of vertex ui from vertex uj is the expected
number of steps a random walk takes to reach the former vertex, starting from the latter
one. The cover time is the expected number of steps the walk takes to reach every vertex at
least once. The mixing rate measures how fast the walk converges to its limiting probability
distribution. Supposing that the walk is currently at vertex u, the probability of each of its
neighbors being selected for a visit is assumed to be uniform, i.e., 1/d(u), where d(u) is the
degree of vertex u. We can gather all the transition probabilities in a matrix M and model
the random walk as a Markov chain following the rule:
p(t+1) = MT p(t),
(3.7.1)
where p(t) is the vector of the probabilities p(t)
i
that the walk is at vertex ui after t steps,
given an initial probability vector p(0). Alternatively, p(t) can be connected to p(0) by:
p(t) = (MT )tp(0).
(3.7.2)
If the walk is allowed to go on inﬁnitely on a graph G = (V, E), the stationary probability

Algebraic Graph Analysis
55
distribution converges to:
π(ui) = d(ui)
|E| ,
(3.7.3)
assuming that G is not bipartite.
Suppose that there is a set whose elements have n characteristics, and the problem we
want to solve is that of enumeration, i.e., to estimate N, the number of all the possible
elements of the set [Bab79]. If N is exponential on n, then exhaustively enumerating every
element becomes intractable. It is possible, however, to estimate the population of the set
by randomly sampling the elements with a random walk traversal on the characteristics
and studying how the number N (t) of known elements increases after t iterations. Some
restrictions apply, e.g., that the initial set size is known, which is trivially 1, when starting
from a random element of the set. It must also be possible to uniformly generate new random
set elements in polynomial time. Finally, the ratio of consecutive population measurements
N (t+1)/N (t) must be polynomially bound. If these restrictions apply, then the total number
can be extrapolated from the ratios N (t+1)/N (t).
A similar application is ﬁnding the volume of a convex body that lacks an analytical
mathematic expression for this task [DF88]. The exact computation of the volume of an
n-dimensional polytope, i.e., a ﬂat sided geometric shape generalized to n dimensions, is NP-
hard [DF88]. The complexity of approximation algorithms that have a bounded error for this
volume computation is also exponential [Ele86]. However, a randomized polynomial time
algorithm, similar to the above approach, can provide estimates whose error is theoretically
improbable to exceed any given positive number.
There are some applications, in which the desired stationary probability distribution of
the random walk is not uniform, so that it may be used to generate random elements with
a non-uniform distribution. For example, a stochastic optimization algorithm may prefer to
make transition chains with vertices near optima rather than chains that randomly lead away
from these optima. Random walks can be modiﬁed, so that their stationary distribution
converges to any arbitrary probability distribution function F(). In an idea similar to the
Monte Carlo approach, whenever a transition from vertex ui to vertex uj is decided and
F(uj) < F(ui), then the transition is performed with a further probability of F(uj)/F(ui),
otherwise the walk stays in ui. It can be proven that the stationary distribution QF of this
modiﬁed random walk is given by [MRR+53]:
QF (u) =
F(u)
P
v∈V F(v).
(3.7.4)
There are many interesting connections between the spectral graph theory and random
walk measures, as thoroughly described in [Chu97], [Lov93]. Access time can be expressed
as a function of the graph eigenvectors. A function of either the second, or the second and
last eigenvalue of the Laplacian matrix provides an upper bound for the relative pairwise
graph vertex distance. In a vertex transitive graph, a random walk of total s steps converges
to the uniform distribution in a number of steps bound by a function of the eigenvalues
of the Laplacian matrix. The mixing rate on a non-bipartite graph is the absolute value of
either the second or the last eigenvalue of the adjacency matrix, whichever has the greater
absolute value.
The PageRank algorithm, developed by the founders of Google, models the behavior of
a web surfer as a random walk on the graph of the World Wide Web [BP98]. This “random
surfer” may click on any link in the current web page he is visiting with the same probability,
while periodically being “bored” and jumping to another page randomly. The probability
that this surfer will visit a web page constitutes its PageRank. This probability is given
by the dominant eigenvector, i.e., the eigenvalue corresponding to the eigenvalue with the
largest magnitude, of the graph transition probability matrix [BL06].

56
Graph-Based Social Media Analysis
3.8
Graph anomaly detection
Graph anomalies can refer to basic graph properties, such as the presence or absence of
a vertex or edge, or even the existence of “anomalous” subgraphs, such as the near-stars
or near-cliques. Near star refers to a graph vertex, whose neighbors are scarcely connected
with each other. Near clique refers to very densely connected subgraphs. Heavy vicinity
refers to the case when most of the total weight of the vertex edges is distributed to only a
few edges. Finally, Dominant heavy link refers to a single edge having extraordinarily large
weight compared to other edges. A more extensive recent survey on anomaly detection that
includes graph based algorithms can be found in [CBK09].
In fraud detection, the objective is to detect suspicious transactions in graph structures.
It is reasonable to expect that fraudulent activities will be camouﬂaged by the perpetrators
to look like legitimate transactions, to avoid raising suspicion. Three algorithms for the
detection of suspicious presence or absence of graph elements, as well as suspicious changes
in the labels of graph elements are presented in [EH07]. All three algorithms use breadth-ﬁrst
search and the Minimum Description Length (MDL), in order to extract a graph pattern
that is considered normative, i.e., it can be used to best compress the graph in MDL terms.
In the ﬁrst algorithm, changes in the normative pattern are measured using the cost of the
editing operation that causes a change in the description length of the entire graph, when
it is compressed with the normative pattern. This algorithm detects anomalous vertex label
changes. The second algorithm approximates the probability of additional vertices or edges
being inserted into the normative pattern to produce a new pattern, according to whether
they lead to patterns that match other possible changes, when compared with the total
number of derivative patterns coming from all possible changes. Changes being below a
threshold are considered anomalous additions. Another algorithm investigates sub-patterns
of the normative pattern by removing elements, until the sub-pattern is no longer normative,
at which point the cost of transformation and frequency of the sub-pattern is thresholded.
Thus, anomalous absences are detected.
Graph similarity measures between two web graphs can also be used for anomaly de-
tection [PDG10]. Three requirements must be satisﬁed by suitable measures: a) scalability,
so that its time complexity is at most linear, b) sensitivity, so that it is easily aﬀected
by diﬀerences in the more important vertices and edges, and c) that it must be aﬀected
by additional data stored in graph elements, besides the graph connectivity information.
Several similarity measures were proposed. Vertex and edge overlap is computed through
edit distance. Vertex ranking is measured according to a formula inspired by Spearman’s
formula [WA79]. Vertex vector similarity is measured by comparing the quality values for
vertices that are matched between the two graphs. Edge vector similarity is measured by
comparing the quality of vertices adjacent to both edges. Sequence similarity is measured by
how many short paths the two graphs share, taking advantage of the fact that the graphs are
web graphs. Finally, a variation of SimHash [Cha02] is used to measure signature similarity.
Changes in compression ratio and graph entropy can also be used to detect graph anoma-
lies [NC03], using an established graph compression technique [CH00], which iteratively ﬁnds
a substructure that best compresses the graph in terms of MDL and replaces every instance
of the substructure with a single vertex. An anomalous rating metric a is deﬁned by:
a = 1 −1
n
n
X
i=1

(n −i + 1)DLi−1(G) −DLi(G)
DL0(G)

,
(3.8.1)
where DLi(G) is the graph description length at iteration i. This metric ranges in [0, 1]

Algebraic Graph Analysis
57
and the higher it is, the more anomalous the subgraph is considered to be. For any given
subgraph, it starts at value 1 and, as the compression proceeds, drops, until no further
compression is possible. The value is then thresholded to decide whether a subgraph is
anomalous or not.
Another measure of graph irregularity can be derived from the conditional entropy
H(X|Y ):
H(X|Y ) =
X
y∈Y
X
x∈X
P(y)P(x|y) log P(x|y),
(3.8.2)
measuring the remaining uncertainty of a variable X after variable Y is known, under
the assumption that exactly one “event” from set X has occurred. The deﬁnition can be
expanded to:
H(X|Y ) =
X
y∈Y
X
x∈X
((P(y)P(x|y) log P(x|y)) + ((1 −P(x|y)) log(1 −P(x|y))))
(3.8.3)
that now suitably models multiple ways in which a given subgraph X can be extended to a
bigger structure.
Using a related, edge-based deﬁnition of entropy, a similar approach is proposed in
[SA05] for detecting important vertices in multi-graphs (graphs that allow for more than
one edge between the same vertices). The vertex entropy is measured as the sum of the
entropy values of its edges:
E(i) = H(vi, P) =
X
e∈N(vi)
p(e) log( 1
p(e)),
(3.8.4)
where the probability of each edge e can be calculated from the multi-graph. Important
vertices are assumed to inﬂuence their neighbors, paths of edges can be included in the
computation of each vertex entropy. The suitability of combining two or more edges in a
path can be measured using edge data similarity, e.g., the similar subjects in emails, or
proximity in time (if such information is available). In order to rank the vertex importance,
the so-called vertex eﬀect is determined by:
Eﬀecti =
EN(i)
log( EN(i)
E(i) )
,
(3.8.5)
where E(i) is the entropy of vertex ui and EN(i) is the entropy of the rest of the graph,
when ui and all its edges are removed from the graph. The larger the eﬀect value is, the
more important the vertex is considered to be.
3.8.1
Spectral anomaly detection
Anomalous vertices in weighted graphs can be detected based on their egonet [AMF10],
i.e., the induced subgraph of a vertex and its neighborhood. The anomalies that can be
identiﬁed are near star, near clique, heavy vicinity and dominant heavy links. Four basic
vertex-based features are computed, namely the neighbor number Ni, the number of edges
in the egonet Ei, the total edge weight of the egonet Wi and the principal eigenvalue λi of the
egonet weighted adjacency matrix. These features have been observed to follow power laws,
as the graph increases in size and are, thus, considered desirable features. The appropriate
pair of features to use for each anomaly type has been determined to be Ni and Ei for
near-star and near-clique, Wi and Ei for heavy vicinity and λi and Wi for dominant edge.

58
Graph-Based Social Media Analysis
Since the Probability Density Function (PDF) of every feature follows a power law, then,
for each pair of features x and y, the following equation holds [AMF10]:
∃x, θ ∈R : y = cxθ.
(3.8.6)
The outlier score of a vertex ui can be evaluated by employing the appropriate features x
and y as follows:
o(ui) = max (y, cxθ)
min (y, cxθ) log |y −cxθ| + 1.
(3.8.7)
Finally, this measure can be complemented by a probability density based outlier detection
measure for improved performance [BKNS00].
Anomalous behavior detection in a time series of weighted graphs, each graph repre-
senting the state of a graph at diﬀerent times, is presented in [Ide04]. The basic feature
vector used is the eigenvector corresponding to the highest eigenvalue of the weight matrix,
referred to as the activity vector. An attempt to cluster the graph vertices is made using the
activity vector. If graph clustering produces more than one cluster, each cluster is handled
separately. In order to establish the typical, non-anomalous behavior, the method requires
training data, in which no anomalies are present. The activity vectors from the various
training samples are collected to form the dependency matrix U. The reference activity
vector for non-anomalous behavior is selected to be the principal eigenvector of matrix
UT U. Graph anomaly is measured by comparing the angle (inner product) that the activ-
ity vector of the test graph forms with the reference vector for non-anomalous behavior. If
the inner product is close to 0, the vectors are perpendicular and, thus, the test graph is
extremely anomalous. If the inner product is close to 1, the test graph coincides with the
average typical, non-anomalous graph.
The inability of graph spectra to take into account vertex features renders them unable
to detect anomalies based on those features. However, they remain strong tools for detecting
structural graph anomalies.
3.9
Conclusions
In this chapter, we have presented an overview to the wide ﬁeld of the combination of
graph theory and algebraic or statistical techniques. It is our hope that readers interested
in exploring these ﬁelds will ﬁnd this survey useful to quickly get a good idea of problem
deﬁnitions, notably the ones based on graph spectrum.
We have chosen to focus this survey on statistical and algebraic approaches, because of
their potential and eﬃciency. In contrast to other combinatorial or algorithmic approaches
that are more likely to involve ad hoc elements, statistical and algebraic approaches have
a solid theoretical framework guiding their application to the various graph related tasks.
Spectral techniques, in particular, have found use in several graph related tasks, because the
graph spectrum provides a powerful graph representation. It reveals several graph properties
and can be used as an eﬃcient graph descriptor.
We started by providing standard graph theory and the spectral graph theory back-
ground that is required knowledge for anyone interested in working in the ﬁeld. Then, we
have provided a quick overview of the applications of techniques from all the aforemen-
tioned subﬁelds. We also covered graph compression, which is essential in handling large
graphs and the theoretical background for the random generation of graphs. Finally, we

Algebraic Graph Analysis
59
overviewed graph clustering, community detection, graph matching, random walks, and
anomaly detection.
Techniques for graph related tasks have been under constant development for relatively
small applications. The proliferation and growing popularity of social media and social net-
works, however, has further rekindled scientiﬁc interest in these issues in the past several
years. With the advances in computer processors, computer memory, and compression tech-
niques, it is now feasible to work on big graphs, such as on Facebook or YouTube data. The
material and scientiﬁc interest for future research seems abundant.
Bibliography
[ABL10]
Y.-Y. Ahn, J. P. Bagrow, and S. Lehmann. Link communities reveal multiscale
complexity in networks. Nature, 466(7307):761–764, 2010.
[AMF10]
L. Akoglu, M. McGlohon, and C. Faloutsos. OddBall: Spotting anomalies in
weighted graphs. In M. J. Zaki, J. X. Yu, B. Ravindran, and V. Pudi, editors,
Proc. Advances in Knowledge Discovery and Data Mining, volume 6119, pages
410–421, 2010.
[Ans03]
K. M. Anstreicher. Recent advances in the solution of quadratic assignment
problems. Mathematical Programming, 97(1-2):27–42, 2003.
[AO01]
R. K. Ahuja and J. B. Orlin. Inverse optimization. Operations Research, 49:771–
783, 2001.
[APF+06]
B. Adamcsek, G. Palla, I. J. Farkas, I. Der´enyi, and T. Vicsek. Cﬁnder: lo-
cating cliques and overlapping modules in biological networks. Bioinformatics,
22(8):1021–1023, 2006.
[AR13]
C. C. Aggarwal and C. K. Reddy. Data clustering: algorithms and applications.
CRC Press, 2013.
[Bab79]
L. Babai. Monte-Carlo algorithms in graph isomorphism testing. Universit´e de
Montr´eal Technical Report, DMS, pages 79–10, 1979.
[BB03]
A.-L. Barab´asi and E. Bonabeau. Scale-free networks. Scientiﬁc American,
288(5):50–59, 2003.
[BBHW10] E. Bodine-Baron, B. Hassibi, and A. Wierman. Distance-dependent Kronecker
graphs for modeling social networks. IEEE Journal of Selected Topics in Signal
Processing, 4(4):718–731, 2010.
[Ber09]
R. Beraldi. Biased random walks in uniform wireless networks. IEEE Transac-
tions on Mobile Computing, 8(4):500–513, 2009.
[BGK+05] J. Baumes, M. K. Goldberg, M. S. Krishnamoorthy, M. M. Ismail, and N. Pre-
ston. Finding communities by clustering a graph into overlapping subgraphs.
In Proc. IADIS Applied Computing, pages 97–104, 2005.
[BGMI05]
J. Baumes, M. Goldberg, and M. Magdon-Ismail.
Eﬃcient identiﬁcation of
overlapping communities. In Proc. Intelligence and Security Informatics, pages
27–36. 2005.

60
Graph-Based Social Media Analysis
[Big93]
N. Biggs. Algebraic graph theory. Cambridge University Press, 1993.
[BKNS00]
M. M. Breunig, H.-P. Kriegel, R. T. Ng, and J. Sander.
LOF: Identifying
density-based local outliers. In Proc. of the 2000 ACM SIGMOD International
Conference on Management of Data, volume 29, pages 93–104, 2000.
[BL06]
K. Bryan and T. Leise. The $25,000,000,000 eigenvector: the linear algebra
behind Google. SIAM Review, 48(3):569–581, 2006.
[BP98]
S. Brin and L. Page. The anatomy of a large-scale hypertextual web search
engine. Computer Networks and ISDN Systems, 30(1-7):107–117, 1998.
[Bro87]
M. Browne. The young-householder algorithm and the least squares multidi-
mensional scaling of squared distances. Journal of Classiﬁcation, 4(2):175–190,
1987.
[BRSV11]
P. Boldi, M. Rosa, M. Santini, and S. Vigna.
Layered label propagation: a
multiresolution coordinate-free ordering for compressing social networks.
In
Proc. of the 20th International Conference on World Wide Web, pages 587–
596, 2011.
[BV03]
P. Boldi and S. Vigna. The webgraph framework I: Compression techniques. In
Proc. of the 13th International World Wide Web Conference, pages 595–601,
2003.
[CBK09]
V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: A survey. ACM
Computing Surveys, 41(3):15:1–15:58, 2009.
[CCLS07]
T. S. Caetano, L. Cheng, Q. V. Le, and A. J. Smola. Learning graph matching.
In Proc. of the 11th IEEE International Conference on Computer Vision, pages
1–8, 2007.
[CFSV04]
D. Conte, P. Foggia, C. Sansone, and M. Vento. Thirty years of graph match-
ing in pattern recognition. International Journal of Pattern Recognition and
Artiﬁcial Intelligence, 18(03):265–298, 2004.
[CH98]
A. D. J. Cross and E. R. Hancock.
Graph matching with a dual-step EM
algorithm. IEEE Transactions on Pattern Analysis and Machine Intelligence,
20:1236–1253, 1998.
[CH00]
D. J. Cook and L. B. Holder.
Graph-based data mining.
IEEE Intelligent
Systems, 15:32–41, 2000.
[CH03]
M. Carcassoni and E. Hancock.
Spectral correspondence for point pattern
matching. Pattern Recognition, 36(1):193–204, 2003.
[Cha02]
M. S. Charikar.
Similarity estimation techniques from rounding algorithms.
In Proc. of the 34th annual ACM Symposium on Theory of Computing, pages
380–388, 2002.
[Chu97]
F. R. K. Chung. Spectral Graph Theory. American Mathematical Society, 1997.
[CJHJ11]
R. Chitta, R. Jin, T. C. Havens, and A. K. Jain. Approximate kernel k-means:
Solution to large scale kernel clustering. In Proc. of the 17th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, pages 895–
903, 2011.

Algebraic Graph Analysis
61
[CKP95]
W. J. Christmas, J. Kittler, and M. Petrou. Structural matching in computer
vision using probabilistic relaxation. IEEE Transactions on Pattern Analysis
Machine Intelligence, 17:749–764, 1995.
[CL90]
L.-H. Chen and J.-R. Lieh. Handwritten character recognition using a 2-layer
random graph model by relaxation matching. Pattern Recognition, 23:1189–
1205, 1990.
[CMP+10] D. Cordeiro, G. Mouni´e, S. Perarnau, D. Trystram, J.-M. Vincent, and F. Wag-
ner.
Random graph generation for scheduling simulations.
In Proc. of the
3rd International Conference on Simulation Tools and Techniques, pages 1–10,
2010.
[CSN09]
A. Clauset, C. R. Shalizi, and M. E. Newman.
Power-law distributions in
empirical data. SIAM Review, 51(4):661–703, 2009.
[CSRL01]
T. H. Cormen, C. Stein, R. L. Rivest, and C. E. Leiserson. Introduction to
Algorithms. McGraw-Hill, 2nd edition, 2001.
[DF88]
M. E. Dyer and A. M. Frieze. On the complexity of computing the volume of
a polyhedron. SIAM Journal on Computing, 17:967–974, 1988.
[DGK04]
I. S. Dhillon, Y. Guan, and B. Kulis. Kernel k-means: Spectral clustering and
normalized cuts. In Proc. of the 10th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pages 551–556, 2004.
[DPZ01]
S. Dickinson, M. Pelillo, and R. Zabih. Introduction to the special section on
graph algorithms in computer vision. IEEE Transactions on Pattern Analysis
Machine Intelligence, 23:1049–1052, 2001.
[DRSS96]
A. A. Diwan, S. Rane, S. Seshadri, and S. Sudarshan. Clustering techniques for
minimizing external path length. In Proc. of the International Conference on
Very Large Databases, pages 342–353, 1996.
[EB98]
M. G. Everett and S. P. Borgatti.
Analyzing clique overlap.
Connections,
21(1):49–61, 1998.
[EF86]
M. A. Eshera and K.-S. Fu. An image understanding system using attributed
symbolic representation and inexact graph-matching. IEEE Transactions on
Pattern Analysis and Machine Intelligence, (5):604–618, 1986.
[EH07]
W. Eberle and L. Holder. Anomaly detection in data represented as graphs.
Intelligent Data Analysis, 11(6):663–689, 2007.
[EL09]
T. S. Evans and R. Lambiotte. Line graphs, link partitions, and overlapping
communities. Physical Review E, 80(1):016105, 2009.
[Ele86]
G. Elekes. A geometric inequality and the complexity of computing volume.
Discrete & Computational Geometry, 1:289–292, 1986.
[ER60]
P. Erdos and A. Renyi. On the evolution of random graphs. Publication of the
Mathematical Institute of the Hungarian Academy of Sciences, 5:17–61, 1960.
[FE73]
M. A. Fischler and R. A. Elschlager.
The Representation and Matching of
Pictorial Structures. IEEE Transactions on Computers, C-22(1):67–92, 1973.

62
Graph-Based Social Media Analysis
[FGK95]
A. Filatov, A. Gitis, and I. Kil. Graph-based handwritten digit string recogni-
tion. In Proc. of the 3rd International Conference on Document Analysis and
Recognition, volume 2, pages 845–848, 1995.
[Fie73]
M. Fiedler. Algebraic connectivity of graphs. Czechoslovak Mathematical Jour-
nal, 23(2):298–305, 1973.
[For10]
S. Fortunato. Community detection in graphs. Physics Reports, 486(3):75–174,
2010.
[Fra00]
J. Franklin. Matrix Theory. Dover Publications, 2000.
[Fre77]
L. C. Freeman. A Set of Measures of Centrality Based on Betweenness. So-
ciometry, 40(1):35–41, 1977.
[Gil59]
E. N. Gilbert. Random graphs. Annals of Mathematical Statistics, 30(4):1141–
1144, 1959.
[GJ90]
M. R. Garey and D. S. Johnson. Computers and Intractability; A Guide to the
Theory of NP-Completeness. W. H. Freeman & Co., 1990.
[GM98]
S. Guattery and G. L. Miller. On the quality of spectral separators. SIAM
Journal on Matrix Analysis and Applications, 19:701–719, 1998.
[Gra06]
L. Grady. Random walks for image segmentation. IEEE Transactions on Pat-
tern Analysis Machine Intelligence, 28:1768–1783, 2006.
[GvL96]
G. H. Golub and C. F. van Loan. Matrix computations (3rd ed.). Johns Hopkins
University Press, 1996.
[GY99]
J. Gross and J. Yellen. Graph theory and its applications. CRC Press, 1999.
[HH98]
B. Huet and E. R. Hancock. Fuzzy relational distance for large-scale object
recognition. In Proc. IEEE Computer Society Conference on Computer Vision
and Pattern Recognition, pages 138–143, 1998.
[HK92]
L. Hagen and A. B. Kahng. New spectral methods for ratio cut partitioning
and clustering. IEEE Transactions on Computer-Aided Design of Integrated
Circuits and Systems, 11(9):1074–1085, 1992.
[Ide04]
T. Ide. Eigenspace-based anomaly detection in computer systems. In Proc. of
the 10th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, pages 440–449, 2004.
[JB03]
M. I. Jordan and F. R. Bach. Learning spectral clustering. In Proc. Advances
in Neural Information Processing Systems, 2003.
[JCZ13]
C. Jiang, F. Coenen, and M. Zito.
A survey of frequent subgraph mining
algorithms. The Knowledge Engineering Review, 28:75–105, 2013.
[Kel09]
S. Kelley. The existence and discovery of overlapping communities in large-scale
networks. Rensselaer Polytechnic Institute, 2009.
[KJ11]
Y. Kim and H. Jeong. Map equation for link communities. Physical Review E,
84(2):026110, 2011.
[KKKS08]
J. M. Kumpula, M. Kivel¨a, K. Kaski, and J. Saram¨aki. Sequential algorithm
for fast clique percolation. Physical Review E, 78(2):026109, 2008.

Algebraic Graph Analysis
63
[KKR06]
E. Kranakis, D. Krizanc, and S. Rajsbaum. Mobile agent rendezvous: A survey.
In Structural Information and Communication Complexity, pages 1–9. Springer,
2006.
[Kle00]
J. Kleinberg. The small-world phenomenon: An algorithmic perspective. In
Proc. of the 32nd ACM Symposium on Theory of Computing, pages 163–170,
2000.
[KP07]
I. Kotsia and I. Pitas. Facial expression recognition in image sequences using ge-
ometric deformation features and support vector machines. IEEE Transactions
on Image Processing, 16(1):172–187, 2007.
[KPBV09] D. Krioukov, F. Papadopoulos, M. Boguna, and A. Vahdat. Greedy Forwarding
in Scale-Free Networks Embedded in Hyperbolic Metric Spaces. In Proc. of the
11th ACM SIGMETRICS Workshop on Mathematical Performance Modeling
and Analysis, 2009.
[KVV04]
R. Kannan, S. Vempala, and A. Vetta. On clusterings: Good, bad and spectral.
Journal of the ACM, 51(3):497–515, 2004.
[LCKF05]
J. Leskovec, D. Chakrabarti, J. Kleinberg, and C. Faloutsos. Realistic, math-
ematically tractable graph generation and evolution, using Kronecker multipli-
cation. In Proc. Knowledge Discovery in Databases, pages 133–145, 2005.
[LKF05]
J. Leskovec, J. Kleinberg, and C. Faloutsos. Graphs over time: densiﬁcation
laws, shrinking diameters and possible explanations. In Proc. of the 11th ACM
SIGKDD International Conference on Knowledge Discovery in Data Mining,
pages 177–187, 2005.
[LMV01]
J. Llado´os, E. Mart´ı, and J. J. Villanueva. Symbol recognition by error-tolerant
subgraph matching between region adjacency graphs. IEEE Transactions on
Pattern Analysis Machine Intelligence, 23:1137–1143, 2001.
[Lov93]
L. Lov´asz. Random walks on graphs: A survey. Combinatorics: Paul Erdos is
Eighty, 2:1–46, 1993.
[MGSZ02]
M. Mihail, C. Gkantsidis, A. Saberi, and E. Zegura. On the semantics of Internet
topologies. In Proc. IPAM Workshop on Large Scale Communication Networks.,
2002.
[Mil67]
S. Milgram. The small-world problem. Psychology Today, 1(1):61–67, 1967.
[MM13]
M. H. M. Maier, U. von Luxburg. How the result of graph clustering methods
depends on the construction of the graph. ESAIM: Probability and Statistics,
(17):370–418, 2013.
[MRR+53] N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller.
Equation of State Calculations by Fast Computing Machines. The Journal of
Chemical Physics, 21(6):1087–1092, 1953.
[MX07]
M. Mahdian and Y. Xu.
Stochastic Kronecker graphs.
In Proc. of the 5th
International Conference on Algorithms and Models for the Web-graph, pages
179–186, 2007.
[NC03]
C. C. Noble and D. J. Cook.
Graph-based anomaly detection.
In Proc. of
the 9th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, pages 631–636, 2003.

64
Graph-Based Social Media Analysis
[OTNP14] G. Orfanidis, A. Tefas, N. Nikolaidis, and I. Pitas.
Facial image clustering
in stereo videos using local binary patterns and double spectral analysis. In
Proc. IEEE Symposium on Computational Intelligence and Data Mining, pages
217–221, 2014.
[PDFV05]
G. Palla, I. Der´enyi, I. Farkas, and T. Vicsek.
Uncovering the overlapping
community structure of complex networks in nature and society.
Nature,
435(7043):814–818, 2005.
[PDG10]
P. Papadimitriou, A. Dasdan, and H. Garcia-Molina. Web graph similarity for
anomaly detection. Journal of Internet Services and Applications, 1(1):19–30,
2010.
[PF97]
E. G. M. Petrakis and C. Faloutsos.
Similarity searching in medical image
databases. IEEE Transactions on Knowledge and Data Engineering, 9(3):435–
447, 1997.
[Pit00]
I. Pitas. Digital Image Processing Algorithms and Applications. Wiley, 2000.
[PKV10]
S. Papadopoulos, Y. Kompatsiaris, and A. Vakali. A graph-based clustering
scheme for identifying related tags in folksonomies. In Proc. of the 12th In-
ternational Conference on Data Warehousing and Knowledge Discovery, pages
65–76, 2010.
[QH04]
H. Qiu and E. R. Hancock.
Graph matching and clustering using spectral
partitions. Pattern Recognition, 39:22–34, 2004.
[RB02]
A. R´eka and A.-L. Barab´asi. Statistical mechanics of complex networks. Reviews
of Modern Physics, 74:47–97, 2002.
[RB08]
M. Rosvall and C. T. Bergstrom. Maps of random walks on complex networks
reveal community structure. Proceedings of the National Academy of Sciences,
105(4):1118–1123, 2008.
[SA05]
J. Shetty and J. Adibi. Discovering important nodes through graph entropy
the case of Enron email database. In Proc. of the 3rd International Workshop
on Link Discovery, pages 74–81, 2005.
[SB92]
L. S. Shapiro and J. M. Brady. Feature-based correspondence: an eigenvector
approach. Image and Vision Computing, 10(5):283–288, 1992.
[Sch07]
S. E. Schaeﬀer. Graph clustering. Computer Science Review, 1(1):27–64, 2007.
[SF83]
A. Sanfeliu and K. Fu. A distance measure between attributed relational graphs
for pattern recognition. IEEE Transactions on Systems, Man, and Cybernetics,
13:353–362, 1983.
[SH85]
L. G. Shapiro and R. M. Haralick. A metric for comparing relational descrip-
tions. IEEE Transactions on Pattern Analysis Machine Intelligence, 7:90–94,
1985.
[SLh90]
G. L. Scott and H. C. Longuet-Higgins. Feature grouping by relocalisation of
eigenvectors of proximity matrix. In Proc. of British Machine Vision Confer-
ence, pages 103–108, 1990.
[SM00]
J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 22(8):888–905, 2000.

Algebraic Graph Analysis
65
[TJHA05]
I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin meth-
ods for structured and interdependent output variables. Journal of Machine
Learning Research, 6:1453–1484, 2005.
[TKP01]
A. Tefas, C. Kotropoulos, and I. Pitas. Using support vector machines to en-
hance the performance of elastic graph matching for frontal face authentication.
IEEE Transactions on Pattern Analysis Machine Intelligence, 23(7):735–746,
2001.
[TPN12]
N. Tsapanos, I. Pitas, and N. Nikolaidis. Graph representations using adja-
cency matrix transforms for clustering.
In Proc. 16th IEEE Mediterranean
Electrotechnical Conference, pages 383–386, 2012.
[TPP14]
I. Tsingalis, I. Pipilis, and I. Pitas.
A statistical and clustering study on
YouTube 2D and 3D video recommendation graph. In Proc. of the 6th IEEE
International Symposium on Communications, Control and Signal Processing,
pages 294–297, 2014.
[TTNP15]
N. Tsapanos, A. Tefas, N. Nikolaidis, and I. Pitas. A distributed framework
for trimmed kernel k-means clustering. Pattern Recognition, 48(8):2685 – 2698,
2015.
[vD00]
S. van Dongen. Graph Clustering by Flow Simulation. PhD thesis, University
of Utrecht, 2000.
[vL07]
U. von Luxburg. A Tutorial on Spectral Clustering. Statistics and Computing,
(17):395–416, 2007.
[VSP11]
N. Vretos, V. Solachidis, and I. Pitas. A mutual information based face cluster-
ing algorithm for movie content analysis. Image Vision Computing, 29:693–705,
2011.
[WA79]
J. B. Wilcox and L. M. Austin. A method for computing the average spear-
man rank correlation coeﬃcient from ordinally structured confusion matrices.
Journal of Marketing Research, 16(3):p426–428, 1979.
[WH97]
R. C. Wilson and E. R. Hancock. Structural matching by discrete relaxation.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 19:634–648,
1997.
[WS98]
D. J. Watts and S. H. Strogatz. Collective dynamics of ‘small-world’ networks.
Nature, 393:440–442, 1998.
[WS05]
S. White and P. Smyth. A spectral clustering approach to ﬁnding communities
in graph. In Proc. SIAM International Conference on Data Mining, pages 76–
84, 2005.
[WW07]
D. White and R. C. Wilson. Spectral generative models for graphs. In Proc.
of the 14th International Conference on Image Analysis and Processing, pages
35–42, 2007.
[XH06]
B. Xiao and E. Hancock. A spectral generative model for graph structure. In
Structural, Syntactic, and Statistical Pattern Recognition, volume 4109, pages
173–181. Springer, 2006.

66
Graph-Based Social Media Analysis
[XK01]
L. Xu and I. King. A PCA approach for fast retrieval of structural patterns in
attributed graphs. IEEE Transactions on Systems, Man and Cybernetics, Part
B: Cybernetics, 31(5):812–817, 2001.
[XKS13]
J. Xie, S. Kelley, and B. K. Szymanski.
Overlapping community detection
in networks: The state-of-the-art and comparative study.
ACM Computing
Surveys, 45(4):43, 2013.
[YS03]
S. X. Yu and J. Shi. Multiclass spectral clustering. In Proc. of the 9th IEEE
International Conference on Computer Vision, pages 313–319, 2003.
[ZD13]
F. Zhou and F. De la Torre.
Deformable graph matching.
In Proc. IEEE
Conference on Computer Vision and Pattern Recognition, 2013.
[ZWZ07]
S. Zhang, R.-S. Wang, and X.-S. Zhang. Identiﬁcation of overlapping commu-
nity structure in complex networks using fuzzy c-means clustering. Physica A:
Statistical Mechanics and its Applications, 374(1):483–490, 2007.

Chapter 4
Web Search Based on Ranking
Andrea Tagarelli
University of Calabria, Italy
Santosh Kabbur and George Karypis
University of Minnesota, U.S.A.
4.1
Introduction ....................................................................
67
4.2
Information Retrieval Background .............................................
69
4.2.1
Document representation ..............................................
69
4.2.2
Retrieval models .......................................................
71
4.3
Relevance Beyond the Web Page Text .........................................
72
4.3.1
Anchor text ............................................................
72
4.3.2
Query expansion .......................................................
73
4.4
Centrality and Prestige ........................................................
76
4.4.1
Basic measures .........................................................
77
4.4.2
Eigenvector centrality and prestige ....................................
80
4.4.3
PageRank ..............................................................
81
4.4.4
Hubs and authorities ..................................................
84
4.4.5
SimRank ...............................................................
87
4.5
Topic-Sensitive Ranking .......................................................
88
4.5.1
Content as topic .......................................................
89
4.5.2
Trust as topic ..........................................................
91
4.6
Ranking in Heterogeneous Networks ...........................................
92
4.6.1
Ranking in heterogeneous information networks ......................
93
4.6.2
Ranking-Based clustering ..............................................
95
4.7
Organizing Search Results .....................................................
97
Clustering Results ..............................................................
98
4.8
Conclusion ......................................................................
99
Emerging Trends ...............................................................
99
Bibliography ....................................................................
100
4.1
Introduction
Web search is the process of identifying and ranking the web pages that are the most
relevant to a user’s query. Though this is similar to the task performed by traditional
Information Retrieval (IR) systems, the nature of the underlying document collection (i.e.,
the Web) and the widely varying needs and characteristics of its users, have made web
search a research ﬁeld of its own.
In traditional IR systems, the document collection consists of a set of text-based doc-
uments, whereas web pages have signiﬁcantly richer content such as layout markup tags,
67

68
Graph-Based Social Media Analysis
scripts, images, and videos. This additional information can be leveraged during web search
in order to identify the important aspects of a web page. Related to that is the fact that
the web pages are linked with each other via hyperlinks. This provides valuable information
as to what a web page is all about; particularly, by inspecting the content of the web pages
that are connected to it via in-links or out-links, one can go beyond the content that is
provided by the web page itself. In addition, by analyzing the network formed by the web
pages, web search systems can identify which are the important query-relevant web pages
and thus produce better rankings. The fact that the web pages belong to diﬀerent websites
also leads to a document collection that has a hierarchical organization. This within-website
hierarchy can be leveraged to transfer information across web pages from the same website
(and as such, inﬂuence the ranking) and to organize the results in more meaningful ways.
Finally, another important diﬀerence of web pages is that their content is not always cu-
rated and it can be of low quality, intentionally misleading, and span many topics that are
sometimes unrelated to each other. This creates many challenges associated with how to
determine the web pages that are both relevant to a user’s query and at the same time are
of high-quality and do not contain unnecessary information.
Unlike traditional IR systems, whose users are often quite familiar with the query lan-
guage supported by them and the various operators and query reformulations that are
required to retrieve the desired set of documents, web search systems need to support users
with signiﬁcantly varying levels of sophistication and expertise. To address this challenge, a
signiﬁcant amount of eﬀort has been devoted to developing search interfaces that allow users
to eﬃciently navigate through the results and easily reformulate their queries. In addition,
users’ information needs vary signiﬁcantly and include (i) informational queries that cover
a broad topic (e.g., cars, Minnesota), (ii) navigational queries that seek a speciﬁc website or
a web page (e.g., Facebook, YouTube), and (iii) transactional queries that show an intent of
performing an action (e.g., purchase a product, download software). In order to retrieve the
most relevant results, web search systems must identify how the diﬀerent web pages relate
to these needs, and for each query it must automatically identify the users’ (latent) needs.
Though this is a diﬃcult problem to solve for an isolated query and an isolated user, the
problem becomes tractable by leveraging information across diﬀerent users with the same
(or similar) query and across the diﬀerent queries from the same user.
In this chapter we discuss various methods that have been developed to address the
above web structure, content, and user-related aspects of web search systems. Note that
we will restrict our discussion to text-based queries and web pages, although many of
the methods and models here discussed are applicable to media rich web pages consist-
ing of images and videos. Note also that this chapter will not cover the important issue
of the evaluation of ranking methods; the interested reader is referred to works that dis-
cuss assessment criteria for ranking problems, such as mean average precision (MAP) and
precision@n, mean reciprocal rank, binary preference function (Bpref) [BV04], Kendall-
tau rank correlation [Abd07], normalized discounted cumulative gain (nDCG) [JK02], and
Fagin’s intersection metric [FKS03].
We organize the contents of the rest of this chapter into seven sections. Section 4.2
provides background information on classic document representation and retrieval models.
Section 4.3 discusses essential hyperlink-related aspects and implications in web search,
with emphasis on query expansion techniques, such as latent semantic analysis (LSI), reg-
ularization frameworks (e.g., RLSI, RRMF), and (pseudo-)relevance feedback. Section 4.4
focuses on link analysis research, thus oﬀering an overview of prominent centrality and
prestige methods, including PageRank and HITS; note that while most of the centrality
and prestige methods have been originally conceived for web search, they have also ap-
plied equally to related environments such as, e.g., collaboration networks, bibliographic
networks, and social media networks. Section 4.5 discusses topic-sensitive web search and

Web Search Based on Ranking
69
TABLE 4.1.1: Road map of this chapter.
Aspect/Section
Section 4.2
Section 4.3
Section 4.4
Section 4.5
Section 4.6
Section 4.7
Web page
–
Anchor Text,
–
–
–
–
Structure
Query
Expansion
Hyperlinking,
–
Anchor Text
Centrality,
–
Heterogeneous
–
Network
Prestige,
Networks
Topology
PageRank
Low Quality
LSI
RRMF
Hubs &
TrustRank,
–
–
Content
Authorities
Topic-sensitive
Ranking
Organizational
–
–
–
–
RankClus,
Clustering
Structure
NetClus
Results
Diverse Topics
–
–
–
Topic-sensitive
–
Clustering
Ranking
Results
User Needs
LSI
RRMF,
–
–
–
Clustering
Query
Results
Expansion
ranking approaches, from both the natural perspective of “content as topic” (e.g., topic-
biased PageRank, TwitterRank) and the alternative perspective of “trust as topic” (e.g.,
TrustRank). The subject of Section 4.6 is heterogeneous information networks (and clas-
sic related methods such as, e.g., ObjectRank, PopRank, RankClus, NetClus), which can
enhance searching and ranking solutions in order to better capture the manifold structure
and semantics underlying diﬀerent types of networked entities and their relationships. Sec-
tion 4.7 focuses on how web search results can be organized, e.g., through a clustering task.
A mapping of the aforementioned sections and the various aspects that they apply to is
shown in Table 4.1.1. Section 4.8 ﬁnally provides concluding remarks and a discussion on
emerging research trends.
4.2
Information Retrieval Background
In this section, a brief introduction of the diﬀerent representation of the documents
and the queries is provided. The interested reader is referred to textbooks, such as,
e.g., [BYBRN99, MRS08] for extensive overviews of document modeling and processing
techniques.
4.2.1
Document representation
The most widely used approach to document representation is known as the bag-of-words
model. According to this model, documents are represented as sets of terms that they are
comprised of, therefore the appearance order of the terms is ignored. A term-document
matrix (also known as a matrix inverted index) is used for representing a collection of
documents, where rows of the matrix correspond to the terms in the collection and the
columns correspond to the documents. Various schemes are used to determine the value of
the each entry (called a weight) in the matrix. Such weights are then used to quantify the
importance of the term on the corresponding page for ranking. One such weighting scheme is
called term-frequency, tf (t, d), where the frequency of terms in documents, f(t, d), is used as
the weight in the matrix. The frequencies of terms are used directly (i.e., raw frequencies),

70
Graph-Based Social Media Analysis
or the raw frequencies are scaled using a function (e.g., log(f(t, d))), or augmented with a
constant value to prevent bias toward the length of the document, or converted to binary
frequencies (i.e., tf (t, d)= 1 if f(t, d) > 0, else 0). One downside of using only the term-
frequency for weighting is that the weights of commonly occurring terms will be high, and
it might not be useful to rank documents when the query contains one of these terms. In
such a case, most of the documents in the corpus will be returned as relevant.
TF-IDF weighting. One way to handle this limitation of term-frequency is by down
weighting the terms that occur very frequently in the documents and increasing the weight of
terms that occur rarely. This is achieved by using a factor called inverse-document-frequency
(IDF) [Jon72]. The IDF of a term is a measure of how much information the term provides.
IDF is formally deﬁned as:
idf(t, D) = log
|D|
1 + |d ∈D : t ∈d| ,
(4.2.1)
where D is the set of documents in the corpus. The term |d ∈D : t ∈d| gives the count of
documents that contain the term t. 1 is added to count, to handle cases where the given
term is not present in any of the documents.
The IDF is used along with the term-frequency to compute the weights in the term-
document matrix. This measure is called TF-IDF (term-frequency inverse-document-
frequency) [Jon72, SB88]. TF-IDF for a given pair of term and document is deﬁned as
the product of the term-frequency of the term in the document and the IDF score of the
term in the corpus, i.e., tfidf(t, d, D) = tf(t, d) × idf(t, D). Thus, a high TF-IDF score
for a given term is achieved by having a high frequency in the given document and a low
frequency (i.e., occurs rarely) in the corpus of documents. In recent years, diﬀerent variants
of TF-IDF weighting schemes have been adopted. Notable ones include using a logarithm
scale for computing TF and normalizing the TF relative to the maximum frequency term
in the document.
Dimensionality reduction.
The number of unique terms in a text (also termed
features) corpus is normally very high. Dimensionality reduction techniques aim to alleviate
this problem by decreasing noise in the term space. This can be accomplished by a feature
selection approach, which aims to choose an optimal subset of features given some objective
function, or a feature extraction approach, which seeks a lower-dimensional space mapping
of the original feature space. The simplest selection technique prunes features with low or
high document frequency: frequently occurring terms are deemed uninformative, while rare
terms constitute noise; stop-words, which are lexicon-speciﬁc, frequent, non-content-bearing
terms, are also removed.
Feature extraction technique algorithms project the data to some lower dimensional
space. The earliest approach to linear projection, namely Principal Component Analysis
(PCA) [Hot33], provides a low-rank representation of the covariance matrix of the features.
PCA utilizes an orthogonal transformation based on Singular Value Decomposition (SVD)
to convert a set of observations of possibly correlated variables into a set of linearly uncor-
related variables called principal components. In the text domain, a well-known technique is
Latent Semantic Indexing (LSI) [DDL+90]. LSI was designed as an indexing and retrieval
method that takes advantage of the semantic structure (implicit higher-order structure) in
the association of terms with documents. LSI is in fact based on the principle that terms
present in the same contexts tend to have similar meanings. Therefore, LSI aims to over-
come problems associated with synonymies. Similar to PCA, LSI applies a rank reduced
SVD on the constructed term-document matrix to identify the relationship between the
terms and concepts present in the text. Given a corpus of documents containing m unique
terms and n documents, a weighted term-document matrix C is constructed with each row

Web Search Based on Ranking
71
representing a term and each column representing a document, i.e., C ∈Rm×n. SVD is
then used to decompose C into three matrices:
C = USVT,
(4.2.2)
such that U and V have orthonormal columns, i.e., UTU = I, VTV = I and S is a diagonal
matrix containing the singular values which are positive and are in decreasing order along
the primary diagonal, i.e., S(1, 1) ≥S(2, 2) ≥. . . ≥S(r, r) > 0, S(i, j) = 0 where i ̸= j and
r is the rank of the matrix C. The matrices obtained by decomposition are then truncated
by retaining only the top k singular values in S, where k ≪m, n. Correspondingly, only the
ﬁrst k columns of U and V are retained. Thus, the given matrix C is approximated as the
product of these truncated matrices. That is,
C ≈ˆC = UkSkVT
k.
(4.2.3)
ˆC is called the rank-k approximation of C. It can be shown that ˆC is the rank-k model
with the best possible least squares ﬁt to C. The geometric interpretation of the SVD
model allows the rows the reduced matrices to be considered as the points corresponding
to the documents and terms in a k-dimensional vector space. This representation helps
to compute distances (or similarities) between the points in the space. The amount of
dimension reduction, that is the value of k, is typically small compared to m and n, and the
exact value is obtained based on the value which gives the best retrieval performance. Some
empirical studies [Bra08a] on LSI have shown that a small value of k in the range of 300 to
400 for moderate (hundreds of thousands) to large (millions) sized document collections is
suﬃcient to obtain the best retrieval performance.
4.2.2
Retrieval models
An information retrieval model typically encompasses three elements, namely, docu-
ments, queries, and ranking functions. The goal of the model is to predict which documents
are relevant to the user’s query and to rank the relevant documents in the order of the pre-
dicted likelihood of relevance to the user. Using the representation of documents presented
earlier, this section explains various classical retrieval models. The inputs to these models
are the set of documents (in the form of an inverted index) and a query (in the form of a
list of words). The output is the ranked list of matched documents in the order of their rel-
evance to the given query. All of these methods assume that the underlying representation
for documents is bag-of-words.
Boolean retrieval model. The Boolean retrieval model is one of the earliest models
proposed for matching documents and queries. In this model, the documents are represented
as sets of terms and the queries are the boolean expressions of the terms. Rules from boolean
algebra are then used to extract the exact-match documents with the query represented
using boolean operators AND, OR, and NOT. A boolean result (i.e., either 0 or 1) is
obtained by applying a boolean expression on a document. Thus, there is no notion of
ranking for the retrieved documents. A document is either relevant or not relevant. There is
no mechanism to quantify to what extent the retrieved documents are relevant. Also, there
is no notion of importance for diﬀerent words in the queries, i.e., all words are treated with
equal weight.
Vector space model. The vector space model is an algebraic model where the doc-
uments and queries are represented as vectors. The number of dimensions of such a vec-
tor space is determined by the number of distinct terms (also called index terms) in the
document corpus. Each distinct term of the corpus is represented as one dimension. The

72
Graph-Based Social Media Analysis
documents and queries are then represented as vectors in such a space based on the terms
that they contain. A document dj and a query ql are represented as column vectors,
dT
j = (w1,j, w2,j, . . . , wm,j)
qT
l = (w1,l, w2,l, . . . , wm,l),
where m is the number of unique terms in the corpus. If a term t is present in a document dj
or a query ql, its corresponding value wt,j and wt,l is non-zero. Diﬀerent weighting schemes
(Section 4.2.1) are used to set the non-zero value corresponding to a term present in the
document or the query.
Representation of documents as vectors helps in utilizing vector algebra techniques to
compute similarities between the documents. Vector similarity measures can be utilized
to compute the similarity between the documents corresponding to the vectors. One such
popular vector similarity measure is the cosine similarity. Given two documents, di and
dj, the cosine similarity between them is given by:
cos(di, dj) =
di · dT
j
∥di∥∥dj∥=
Pm
k=1 dk,i × dk,j
pPm
k=1 (dk,i)2pPm
k=1 (dk,j)2 ,
(4.2.4)
where the numerator term is the vector dot product (or inner product) between the two vec-
tors and the denominator is the product of the Euclidean lengths of the vectors. The denom-
inator normalizes the eﬀect of diﬀerent document lengths. Apart from the cosine similarity,
other vector similarity measures can be utilized to compute document similarities, such as
the Pearson correlation, the Jaccard similarity, and the normalized Euclidean similarity. A
detailed comparison of diﬀerent vector similarity measures can be found in [TSK06].
Vector similarity can also be extended to compute similarities between documents and
queries. For a given query, it can be represented as a bag-of-words document and the cosine
similarity can be used to compute the similarity scores with each of the documents in
the corpus. The obtained similarity scores can then be used to rank the documents. Unlike
boolean retrieval models which retrieve only exact-matched documents, using a model which
computes continuous ranking score like vector similarities, allows documents with partial
match also to be retrieved.
4.3
Relevance Beyond the Web Page Text
A typical web document (or web page) is an HTML document consisting of nested
HTML elements. There are many elements beyond the text which are associated with web
documents. One such important aspect is hyperlinks. Hyperlinks provide a navigational
structure to the collection of web documents. Hyperlinks are created by using the anchor
tag (⟨a⟩) coded in HTML. This linking structure of the web pages can be utilized to improve
the retrieval and ranking of web pages. The rest of this section will provide a brief overview
of various methods which are used to compute the relevance of a web page for a query by
utilizing information beyond the textual content of the given web page.
4.3.1
Anchor text
The anchor text is the visible and clickable text in a hyperlink. Anchor text is meant to
give the user a context about the hyperlink’s target destination. Thus, the anchor text in
most hyperlinks can be considered as a good description of the target web page.

Web Search Based on Ranking
73
The textual content present in a web page can be thought of as the self-description of
that page. Many times, the self-description is not suﬃcient or complete. For a given web
page, collecting all the anchor texts from the inlinking pages provides a good description
of the target web page. It also has been observed that users tend to submit very short web
search queries consisting of few search terms on average, and in many ways the anchor text
associated with the inlinks for a web page also shares this characteristic. Thus, the anchor
text can be deemed a short summarization of the contents of the target web page in the
context of the corresponding inlinking documents [EM03]. There can even be cases where
a given web page is the most relevant page for a query, even though none of the words in
the query are present in the web page. The relevance in such cases is ascertained with the
help of anchor text.
Thus, associating the anchor text with the target web pages has many advantages for
web search. First, by augmenting the web page content [McB94] with the anchor text helps
to better represent the web page. Second, the summarization of the web pages provided by
anchor texts are often more accurate than the pages themselves [BP98]. Third, anchor text
helps to summarize diﬀerent content type on the web (e.g., images, videos, and programs)
which cannot be directly indexed by the text-based web search methods [BP98].
However, the usage of anchor text from the web to rank web pages can potentially
lead to gaming the search engine. Groups of people with a malicious intent can potentially
spam by linking a speciﬁc site with a spam keyword. There have been many instances
where orchestrated spam campaigns are run against speciﬁc sites. Web search methods
have evolved to detect and act on such anchor text spamming attempts.
Extending the usage of anchor text to compute the relevance, and text, surrounding the
anchor text is also utilized as part of the anchor text to augment the contents of the target
web page [CDR+98]; speciﬁcally, in cases where the anchor text is not really informative,
while the neighboring text can potentially have valuable information (e.g., “to know more
about Web Search, ⟨a⟩click here⟨/a⟩”).
4.3.2
Query expansion
Query expansion is the process of reformulating the user query to improve the quality
of the search results retrieved by the search engine. Given the diversity of the information
sources on the Web, the vocabularies of the authors of the content vary greatly. While
formulating the query, users might use a vocabulary diﬀerent from the one used by the
authors of the content. This raises a fundamental problem of term mismatch. This issue is
also known as lexical ambiguity, and it involves synonymy and polysemy of terms. Query
expansion aims to solve this problem and is primarily employed with the assumption that
the user may not always formulate the search query using the appropriate terms. In this
process, given a user search query, the terms constituting the query are examined and new
words or phrases are added to generate an expanded query. This expanded query is then
used to match and retrieve the documents from the index.
Many diﬀerent techniques are employed to expand the query. These techniques can be
broadly divided into two classes: global analysis and local analysis.
Global analysis. Global analysis consists of techniques which are independent of the
query and the results returned from it. These techniques compute a global (corpus-wide)
term similarity matrix using various methods. The most widely used methods among them
use the co-occurrence of the terms in the corpus, synonyms from authoritative sources like
thesauri or lexical ontologies (e.g., WordNet [Fel98]), or computing the related terms using
the query logs. To expand a query, the similarity matrix is used to add terms which are most
similar to the ones present in the query. One of the earliest techniques in this category is

74
Graph-Based Social Media Analysis
term clustering [Jon71], which clusters document terms based on their co-occurrence. The
query is then expanded using the terms present in the same cluster. Techniques that involve
correcting the misspelled query term are also used in practice.
Another popular global analysis approach is based on linear projection techniques, such
as Latent Semantic Indexing (LSI), which has been previously discussed in Section 4.2. In
eﬀect, semantic connections among documents (and queries) may exist even if they do not
share terms. For example, assuming that “car” and “auto” co-occurring in a document are
related to each other, they not only are expected to occur in similar sets of documents
but also make other co-occurring terms indirectly related to each other, e.g., if a document
contains “car” and “engine” and another document contains “auto” and “motor,” then
“engine” and “motor” have some reciprocal relatedness. Generally speaking, some rows
and/or columns of the term-document matrix of a corpus may be somewhat “redundant”,
thus the matrix may have a rank far lower than its original feature dimension. In LSI, the
obtained truncated matrices Uk and Vk can then be used to compute similarity between
two terms, two documents, or a term and a document. From the geometric interpretation of
the rows of the truncated matrices as vectors in the k-dimensional space, the dot product
between corresponding rows of Uk gives the similarity between the two terms. Similarly, Vk
can be used to compute the similarity between two documents by taking the dot product
of the rows corresponding to the two documents. The similarity between a term and a
document, is given by the dot product between the row corresponding to the term in Uk
and the row corresponding to the document in Vk. Such similarities can then be used for
various tasks such as query expansion, document clustering, and ﬁnding representation for
pseudo-documents. Each of the k latent dimensions can be thought of as the latent (or
abstract) topic through which the terms and the documents are related.
The above point makes the concept of dimensionality reduction similar to topic modeling.
Topic modeling is a statistical modeling technique used to discover the abstract “topics” in
a collection of documents. The basic assumption is that a document can be represented as
a mixture of probability distributions over its constituent terms, where each component of
the mixture refers to a main topic. The document representation is obtained by a generative
process, i.e., a probabilistic process that expresses document features as being generated by
a number of latent variables. Topic modeling methods, such as the popular Latent Dirichlet
Allocation (LDA) [BNJ03], can be directly employed for performing global analysis based
query expansion. Just as a brief mention about LDA, the generative process performed by
LDA consists of three levels that involve the whole corpus, the documents, and the terms
of each document. The algorithm ﬁrst samples, for each document, a distribution over
collection topics from a Dirichlet distribution, and for each topic, it samples a distribution
over terms from a Dirichlet distribution. For each document and term position, it selects a
single topic according to the topic distribution speciﬁc to the sampled document. Finally,
each term is then sampled from a multinomial (i.e., categorical) distribution over terms
speciﬁc to the sampled topic.
One of the main challenges associated with LSI is scalability and performance. LSI
requires relatively high computational and memory performance. Given the size of the Web
(i.e., billions of documents), scaling LSI is a huge challenge. In one of the recent methods
called Regularized Latent Semantic Indexing (RLSI) [WXLC11], parallelization is employed
to scale the topic modeling. RLSI formalizes the problem of topic modeling as a problem
of minimizing a quadratic loss function regularized by ℓ1 and ℓ2 norms. This regularized
formulation helps to decompose the problem into multiple sub-optimization problems, which
can be solved in parallel.
Relation Regularized Matrix Factorization (RRMF) [LY09] is a technique which extends
the LSI to utilize the web page characteristics. It uses hyperlinks (relations) between the
web pages and simultaneously models both the links and the content information into

Web Search Based on Ranking
75
a lower-dimensional latent space. The dimensions corresponding to this latent space are
known as latent factors, and the terms and documents are modeled as vectors (known as
latent vectors) in this low dimensional latent space. The intuition behind this method is
to make the latent representation of two web pages which are linked, to be as close as
possible. RRMF uses the links between the pages to regularize their latent factors. Given
the term-document matrix C ∈Rm×n, adjacency matrix A ∈Rn×n containing the links
between the web pages, i.e., A(i, j) = 1 if a link exists between web page i and web page j,
and A(i, j) = 0 otherwise, the latent factors for terms and web pages are learned by solving
the following regularized optimization problem:
min
U,V
1
2∥C −UVT∥2 + α
2 (∥U∥2 + ∥V∥2) + β
2 tr(VLVT),
(4.3.1)
where U and V are the latent factor matrices for terms and web pages respectively, i.e.,
U ∈Rm×k represents the latent k-dimensional representations of all the terms in the form of
latent vectors and V ∈Rn×k, the latent k-dimensional representations of all the documents,
and tr(VLVT) is the link based regularization term and L is the Laplacian matrix of the
adjacency matrix, A. This regularization term is obtained by minimizing the diﬀerence
between the latent factors of the linked web pages. The link regularization term is computed
as:
f = 1
2
n
X
i=1
n
X
j=1
aij∥vi −vj∥2
= tr(VTLV),
(4.3.2)
where vi and vj are the vectors corresponding to the documents i and j, L = D −A is the
Laplacian of adjacency matrix A and D is the diagonal matrix whose diagonal elements
dii = P
j aij. RRMF learns the parameters using a linear time learning algorithm, thus it
is suitable for large datasets of web scale.
Local analysis. Local analysis methods are designed to adjust a query relative to the
results returned to the initial query. Local analysis uses only some of the initially retrieved
documents for expanding the subsequent queries by the same user. A well known class of
local analysis techniques is called the relevance feedback method [Roc71, SB97]. In this class
of methods, the expansion terms are extracted from relevant documents of the initial set of
results. The relevance of the initial set of retrieved documents is obtained via user feedback,
i.e., the user marks the initial set of retrieved documents as relevant or not relevant. In
a real-world commercial search engine case, users rarely provide such relevance feedback.
Thus, relevance feedback methods are rarely used in present day search engines.
To overcome the shortcoming of lack of relevance feedback from the users, a pseudo-
relevance feedback (also known as blind relevance feedback) method is used [BYBRN99]. This
method improves on manual relevance feedback, but automates the feedback loop. The top-
ranked documents of the initial query are assumed to be relevant and the expansion terms
for the subsequent queries are extracted from such top-ranked documents. This approach
has the drawback that, if the initial set of retrieved top-ranked documents are actually
irrelevant, then the words drawn from these documents and added to subsequent queries
are likely to be unrelated to the topic. Thus, it can potentially worsen the quality of the
results. However, in practice it is shown to work reasonably well and performs better than
the global analysis methods [XC96]. In one of the pseudo-relevance feedback based methods
[XJW09], Wikipedia pages have been used to improve the retrieval performance. Speciﬁcally,
the pseudo relevant documents are constructed using the top ranked Wikipedia pages for
the given query and the Wikipedia entity pages corresponding to the query. Then, the term

76
Graph-Based Social Media Analysis
distributions and the structure of Wikipedia pages is used to select the speciﬁc expansion
terms. In a related method [AECC08], Wikipedia’s anchor texts and hyperlinks are used
to expand user’s initial query. In another method [KZ04], anchor texts from the web page
corpus are ranked using several criteria such as the number of occurrences of the anchor
text, and whether the link originates from the same domain or a diﬀerent domain. The
highest-ranked anchor texts which have common terms with the given query are used for
the query expansion.
More recently, indirect sources of evidence are used as a surrogate for users’ relevance
feedback. This is called indirect or implicit relevance feedback. Although indirect feedback
in general is less reliable compared to explicit feedback, implicit feedback can be collected
in large quantities at no extra cost to the user. Generally, implicit feedback is more useful
than the pseudo-relevance feedback, as users’ actions are used as feedback. Given the large
scale of web search queries and user interaction with the results, collecting implicit feedback
is easy. The feedback thus collected can be used for query expansion. In one of the methods,
used by commercial search engines, user actions (e.g., browsing) and other interactions on
the retrieved documents for the initial query are assumed to be relevant to the user query
[KB01]. These actions are used to deduce the relevancy of the retrieved documents and the
conclusions are then used to expand the subsequent queries.
User interactions based on web search logs are used to compute query expansion terms.
There are two main classes of techniques which are based on web search logs [CR12]. The
ﬁrst class of methods treats the individual queries as documents and extracts features
which are related to the original user query. Some methods make use of associated retrieved
results (pseudo-relevance feedback), while some do not [HCO03, JRMG06, YSC09]. The
second class of techniques exploits the relation of queries and retrieval results to provide
additional or greater context in ﬁnding expansion features. Example approaches include
ﬁnding queries associated with the same documents [BSWZ03] or user clicks [BB00], and
extracting terms directly from clicked results [CWNM03, RVT+07]. In [CWNM03], corre-
lation between query terms and document terms is extracted by analyzing user query logs.
The computed correlations are then used to select high-quality expansion terms for new
queries. In another method [XZC+04], the user click-through data is used to associate the
queries with the clicked web pages. These query terms can be taken as user short summa-
rization of the web page and similar to anchor text, these query terms are augmented to
the web page content of the clicked pages while indexing.
These methods can be further extended to exploit the characteristics of the web pages
for ﬁnding the query expansion terms. A personalized query expansion technique [CFN07]
selects the query expansion terms based on the user’s personal information repository which
stores the user’s personal collection of tracked behavior (e.g., previous search queries, search
results clicked, web pages visited). Clickstream mining is also used to suggest queries (a
form of query expansion) based on the user queries, click-through data and the search
context [MYKL08, CJP+08]. Using the web search query logs, the user query reformulation
strategies are studied [HE09] to better understand how web searchers reﬁne queries and
form a theoretical foundation for query reformulation.
4.4
Centrality and Prestige
As we previously discussed, hyperlinks provide a valuable source of information for web
search. In fact, the analysis of the hyperlink structure of the Web, commonly called link

Web Search Based on Ranking
77
analysis, has been successfully used for improving both the retrieving of web documents (i.e.,
which web pages to crawl) and the scoring of web documents according to some notion of
“quality” (i.e., how to rank web pages). The latter is in general meant either as the relevance
of the documents with respect to a user query or as some query-independent, intrinsic notion
of centrality. In network theory and analysis, the identiﬁcation of the “most central” nodes in
the network (e.g., documents in the web network, actors in a social network, etc.) represents
a core task. The term centrality commonly resembles that of importance or prominence of a
vertex in a network, i.e., the status of being located in strategic locations within the network.
However, there is no unique deﬁnition of centrality, as for instance one may postulate that
a vertex is important if it is involved in many direct interactions, or if it connects two large
components (i.e., if it acts as a bridge), or if it allows for quick transfer of the information
also by accounting for indirect paths that involve intermediaries. Consequently, there are
only very few desiderata for a centrality measure, which can be expressed as follows:
• A vertex centrality is a function that assigns a real-valued score to each vertex in a
network. The higher the score, the more important or prominent the vertex is for the
network.
• If two graphs G1, G2 are isomorphic and m(v) denotes the mapping function from a
node v in G1 to some node v′ in G2, then the centrality of v in G1 needs to be the
same as the centrality of m(v) = v′ in G2. In other words, the centrality of a vertex
depends on the structure of the network.
The term centrality is originally designed for undirected networks. In the case of direc-
tional relations, which imply directed networks, the term centrality is still used and refers
to the “choices made,” or out-degrees of vertices, while the term prestige is introduced to
examine the “choices received,” or in-degrees of vertices [WF94]. Moreover, the vertex cen-
trality scores can be aggregated over all vertices in order to obtain a single, network-level
measure of centrality, or alternatively centralization, which aims to provide a clue to the
variability of the individual vertex centrality scores with respect to a given centrality notion.
In the following, we will provide an overview of the most prominent measures of centrality
and prestige, and their deﬁnitions for undirected and directed networks. Particularly, we
will focus on two well-known methods, namely PageRank and Hubs & Authorities, which
have been widely applied to web search contexts.
Through the rest of this section and in the subsequent sections of this chapter, we will
denote with G = (V, E) a network graph, which consists of two sets, V and E, such that
V ̸= ∅and E is a set of pairs of elements of V. If the pairs in E are ordered, the graph is
said to be directed, otherwise it is undirected. The elements in V are the vertices (or nodes)
of G, while the elements in E are the edges (or links) of G.
4.4.1
Basic measures
Vertex-level centrality. The most intuitive measure of centrality for any vertex v ∈V
is the degree centrality, which is deﬁned as the number of edges incident with v, or degree
of v:
cD(v) = deg(v).
(4.4.1)
Being dependent only on adjacent neighbors of a vertex, this type of centrality focuses on
the most “visible” vertices in the network, as those that act as the major point of relational
information; by contrast, vertices with low degrees are peripheral in the network. Moreover,
the degree centrality depends on the graph size: indeed, since the highest degree for a

78
Graph-Based Social Media Analysis
network (without loops) is |V| −1, the relative degree centrality is:
c
cD(v) = cD(v)
|V| −1 = deg(v)
|V| −1.
(4.4.2)
The above measure is independent of the graph size, and hence it can be compared across
networks of diﬀerent sizes.
The deﬁnitions of both absolute and relative degree centrality and degree prestige of
a vertex in a directed network are straightforward. In that case, the degree and the set
of neighbors have two components: we denote with Bi = {vj|(vj, vi) ∈E} the set of in-
neighbors (or “backward” vertices) of vi, and with Ri = {vj|(vi, vj) ∈E} the set of out-
neighbors (or “reference” vertices) of vi. The sizes of sets Bi and Ri are the in-degree and
the out-degree of vi, denoted as in(vi) and out(vi), respectively.
Note also that the degree centrality is also the starting point for various other measures;
for instance, the span of a vertex, which is deﬁned as the fraction of links in the network
that involves the vertex or its neighbors, and the ego density, which is the ratio of the degree
of the vertex to the theoretical maximum number of links in the network.
Unlike degree centrality, closeness centrality also takes into account indirect links be-
tween vertices in the network, in order to score higher those vertices that can quickly interact
with all others because of their shorter distance to the other vertices [Sab66]:
cC(v) =
1
P
u∈V d(v, u),
(4.4.3)
where d(v, u) denotes the graph theoretic, or geodesic, distance (i.e., length of the shortest
path) between vertices v, u. Since a vertex has the highest closeness if it has all the other
vertices as neighbors, the relative closeness centrality is deﬁned as:
c
cC(v) = (|V| −1)cC(v) =
|V| −1
P
u∈V d(v, u).
(4.4.4)
In the case of directed networks, closeness centrality and prestige can be computed according
to outgoing links (i.e., how many hops are needed to reach all other vertices from the selected
one) or incoming links (i.e., how many hops are needed to reach the selected vertex from
all other vertices), respectively. Note that the closeness centrality is only meaningful for a
connected network—in fact, the geodesics to a vertex that is not reachable from any other
vertex are inﬁnitely long. One remedy to this issue is to deﬁne closeness by focusing on
distances from the vertex v to only the vertices that are in the inﬂuence range of v (i.e.,
the set of vertices reachable from v) [WF94].
Besides (shortest) distance, another important property refers to the ability of a vertex
to have control over the ﬂow of information in the network. The idea behind betweenness
centrality is to compute the centrality of a vertex v as the fraction of the shortest paths
between all pairs of vertices that pass through v [Fre77]:
cB(v) =
X
u,z∈V,u̸=v,z̸=v
mu,z(v)
mu,z(V),
(4.4.5)
where mu,z(v) is the number of shortest paths between u and z and passing through v, and
mu,z(V) is the total number of shortest paths between u and z. This centrality is minimum
(zero) when the vertex does not fall on any geodesic, and maximum when the vertex falls
on all geodesics, which is equal to (|V| −1)(|V| −2)/2. Analogously to the other centrality
measures, it’s recommended to standardize the betweenness to obtain a relative betweenness
centrality:
c
cB(v) =
2cB(v)
(|V| −1)(|V| −2),
(4.4.6)

Web Search Based on Ranking
79
which should be divided by 2 for directed networks. Note that, unlike closeness, betweenness
can be computed even if the network is disconnected.
It should be noted that the computation of betweenness centrality is the most resource-
intensive among the above discussed measures: while standard algorithms based on Di-
jkstra’s or breadth-ﬁrst search methods require O(|V|3) time and O(|V|2) space, algo-
rithms designed for large, sparse networks require O(|V| + |E|) space and O(|V||E|) and
O(|V||E| + |V|2 log |V|) time on unweighted and weighted networks, respectively [Bra01].
A number of variants of betweenness centrality has also been investigated; for instance,
in [Bra08b], an extension of betweenness to edges is obtained by replacing the term mu,z(v)
in equation (4.4.5) by a term mu,z(e) calculating the number of shortest (u, z)-paths con-
taining the edge e. An application of this version of edge betweenness is the clustering
approach by Newman and Girvan [NG04], where edges of maximum betweenness are re-
moved iteratively to decompose a graph into relatively dense subgraphs.
Besides the computational complexity issue, a criticism of betweenness centrality is that
it assumes that all geodesics are equally likely when calculating if a vertex falls on a par-
ticular geodesic. However, a vertex with a large in-degree is more likely to be found on
a geodesic. Moreover, in many contexts, it may be equally likely that other paths than
geodesics are chosen for the information propagation; therefore the paths between ver-
tices should be weighted depending on their length. The index deﬁned by Stephenson and
Zelen [SZ89] builds upon the above generalization, by accounting for all paths, including
geodesics, and assigning them with weights, which are computed as the inverse of the path
lengths (geodesics are given unitary weights). The same researchers also developed an in-
formation centrality measure, which focuses on the information contained in all paths that
originate and end at a speciﬁc vertex. The information of a vertex is a function of all the
information for paths ﬂowing out from the vertex, which in turn is inversely related to
the variance in the transmission of a signal from one vertex to another. Formally, given
an undirected network, possibly with weighted edges, a |V| × |V| matrix X is computed as
follows: the i-th diagonal entry is equal to 1 plus the sum of weights for all incoming links
to vertex vi, and the (i, j)-th oﬀ-diagonal entry is equal to 1, if vi and vj are not adjacent,
otherwise it is equal to 1 minus the weight of the edge between vi and vj. For any vertex
vi, the information centrality is deﬁned as:
cI(vi) =
1
yii +
1
|V|(P
vj∈V yjj −2 P
vj∈V yij),
(4.4.7)
where {yij} are the entries of the matrix Y = X−1. Since function cI is only lower bounded
(the minimum is zero), the relative information centrality for any vertex vi is obtained by
dividing cI(vi) by the sum of the cI values for all vertices.
Network-Level centrality.
A basic network-level measure of degree centrality is
simply derived by taking into account the (standardized) average of the degrees:
P
v∈V cD(v)
|V||V −1|
=
P
v∈V c
cD(v)
|V|
,
(4.4.8)
which is exactly the density of the network.
Focusing on a global notion of closeness, a simpliﬁcation of this type of centrality stems
from the graph-theoretic center of a network. This is in turn based on the notion of eccen-
tricity of a vertex v, i.e., the distance to a vertex farthest from v. Speciﬁcally, the Jordan
center of a network is the subset of vertices that have the lowest maximum distance to all
other vertices, i.e., the subset of vertices within the radius of a network.
A unifying view of network-level centrality is based on the notion of network centraliza-

80
Graph-Based Social Media Analysis
tion, which expresses how the vertices in the network graph G diﬀer in centrality [Fre79]:
C(G) =
P
v∈V maxC −C(v)
max P
v∈V maxC −C(v),
(4.4.9)
where C(·) is a function that expresses a selected measure of relative centrality, and maxC is
the maximum value of relative centrality over all vertices in the network graph. Therefore,
centralization is lower when more vertices have similar centrality, and higher when one or a
few vertices dominate the other vertices; as extreme cases, a star network and a regular (e.g.,
cycle) network have centralization equal to 1 and 0, respectively. According to the type of
centrality considered, the network centralization assumes a diﬀerent form. More speciﬁcally,
considering the degree, closeness, and betweenness centralities, the denominator in equation
(4.4.9) is equal to (n −1)(n −2), (n −1)(n −2)/(2n −3), and (n −1), respectively.
4.4.2
Eigenvector centrality and prestige
None of the previously discussed measures reﬂects the importance of the vertices that
interact with the target vertex when looking at (in)degree or distance aspects. Intuitively, if
the inﬂuence range of a vertex involves many prestigious vertices, then the prestige of that
vertex should also be high; conversely, the prestige should be low if the involved vertices
are peripheral. Generally speaking, a vertex’s prestige should depend on the prestige of the
vertices that point to it, and their prestige should also depend on the vertices that point to
them, and so on “ad inﬁnitum” [See49]. It should be noted that the literature usually refers
to the above property as status, or rank.
The idea behind status or rank prestige by Seeley, denoted by function r(·), can be
formalized as follows:
r(v) =
X
u∈V
A(u, v)r(u),
(4.4.10)
where A(u, v) is equal to 1 if u points to v (i.e., u is an in-neighbor of v), and 0 otherwise.
Equation (4.4.10) corresponds to a set of |V| linear equations (with |V| unknowns) which
can be rewritten as:
r = ATr,
(4.4.11)
where r is a vector of size |V| storing all rank scores, and A is the adjacency matrix. Or,
rearranging terms, we obtain (I −AT)r = 0, where I is the identity matrix of size |V|.
Katz [Kat53] ﬁrst recommended to manipulate the matrix A by constraining every
row in A to have sum equal to 1, thus enabling equation (4.4.11) to have ﬁnite solution.
In eﬀect, equation (4.4.11) is a characteristic equation used to ﬁnd the eigensystem of a
matrix, in which r is an eigenvector of AT corresponding to an eigenvalue of 1. In general,
equation (4.4.11) has no non-zero solution unless AT has an eigenvalue of 1.
A generalization of equation (4.4.11) was suggested by Bonacich [Bon72], where the
assumption is that the status of each vertex is proportional (but not necessarily equal) to
the weighted sum of the vertices to whom it is connected. The result, known as eigenvector
centrality, is expressed as follows:
λr = ATr.
(4.4.12)
Note that the above equation has |V| solutions corresponding to |V| values of λ. Therefore,
the general solution can be expressed as a matrix equation:
λR = ATR,
(4.4.13)
where R is a |V|×|V| matrix whose columns are the eigenvectors of AT and λ is a diagonal
matrix of eigenvalues.

Web Search Based on Ranking
81
Katz [Kat53] also proposed to introduce in equation (4.4.11) an “attenuation parameter”
α ∈(0, 1) to adjust for the lower importance of longer paths between vertices. The result,
known as Katz centrality, measures the prestige as a weighted sum of all the powers of the
adjacency matrix:
r =
∞
X
i=1
αi(AT)ir.
(4.4.14)
When α is small, Katz centrality tends to probe only the local structure of the network;
as α grows, more distant vertices contribute to the centrality of a given vertex. Note also
that the inﬁnite sum in the above equation converges to r = [(I −αAT)−1 −I]1 as long as
|α| < 1/λ1, where λ1 is the ﬁrst eigenvalue of AT.
All the above measures may fail in producing meaningful results for networks that
contain vertices with null in-degree: in fact, according to the assumption that a vertex has
no status if it does not receive choices from other vertices, vertices with null in-degree do
not contribute to the status of any other vertex. A solution to this problem is to allow every
vertex some status that is independent of its connections to other vertices. The Bonacich
& Lloyd centrality [BL01], probably better known as alpha-centrality, is deﬁned as:
r = αATr + e,
(4.4.15)
where e is a |V|-dimensional vector reﬂecting exogenous source of information or status,
which is assumed to a vector of ones. Moreover, parameter α here reﬂects the relative
importance of endogenous versus exogenous factors in determining the vertex prestiges.
The solution of equation (4.4.15) is:
r = (I −αAT)−1e.
(4.4.16)
It can easily be proved that equation (4.4.16) and equation (4.4.14) diﬀer only by a constant
(i.e., one) [BL01].
4.4.3
PageRank
In [BP98], Brin and Page presented PageRank, the Google’s patented ranking algorithm.
There are four key ideas behind PageRank. The ﬁrst two are also shared with the previously
discussed eigenvector centrality methods, that is: a page is prestigious if it is chosen (pointed
to) by other pages, and the prestige of a page is determined by summing the prestige values
of all pages that point to that page. The third idea is that the prestige of a page is propagated
to its out-neighbors as distributed proportionally. Let W be a |V| × |V| matrix such that
columns refer to those vertices whose status is determined by the connections received from
the row vertices:
W(i, j) =
(
1/out(vi)
if (vi, vj) ∈E
0
otherwise.
(4.4.17)
Note that W = D−1
outA, where A is the adjacency matrix and Dout is a diagonal matrix
storing the out-degrees of the vertices (i.e., Dout = diag(A1)). Using matrix W, the ﬁrst
three ideas underlying the PageRank can be expressed as r = WTr, or equivalently, for
every vi ∈V:
r(vi) =
X
vj∈Bi
r(vj)
out(vj).
(4.4.18)
Therefore, vector r is the unique eigenvector of the matrix corresponding to eigenvalue 1. It
should be noted that equation (4.4.18) is well-deﬁned only if the graph is strongly connected

82
Graph-Based Social Media Analysis
(i.e., every vertex can be reached from any other vertex). Under this assumption, this equa-
tion has an interpretation based on random walks, called the random surfer model [BP98].
It can be shown that vector r is proportional to the stationary probability distribution
of the random walk on the underlying graph. It should be remarked that, in contrast to
PageRank, alpha-centrality does not have a natural interpretation in terms of probability
distribution, i.e., the sum of the values in the alpha-centrality vector (cf. equation (4.4.15))
is not necessarily equal to 1.
However, the assumption of graph connectivity behind equation (4.4.18) needs to be
relaxed for the practical application of PageRank, since the Web and, in general, real-world
networks, are far from being strongly connected. It might be useful to recall here that the
Web and many other directed networks have a structure which is characterized by ﬁve types
of components (cf., e.g., [RU11]): (i) a large strongly connected component (SCC), (ii) an
in-component, which contains vertices that can reach the SCC but are not reachable from
the SCC, and an out-component, which contains vertices that are reachable from the SCC
but cannot reach the SCC, (iii) in-tendrils and out-tendrils, which are vertices that are
only connected to the out-component (via out-links) and vertices that are only connected
to the in-component (via in-links), (iv) tubes, which are vertices reachable from the in-
component and able to reach the out-component, but have neither in-links nor out-links
with the SCC, and (v) isolated components, which contain vertices that are disconnected
from each of the previous components. Most of these components violate the assumptions
needed for the convergence of a Markov process. In particular, when a random surfer enters
the out-component, she will eventually get stuck in it; as a result, vertices that are not
in the out-component will receive a zero rank, i.e., one cannot distinguish the prestige of
such vertices. More speciﬁcally, equation (4.4.18) needs to be modiﬁed to prevent anomalies
that are caused by two types of structures: rank sinks, or “spider traps,” and rank leaks,
or “dead ends.” The former are sets of vertices that have no links outwards, the latter are
individual vertices with no out-links.
If leak vertices were directly represented in matrix W, they would correspond to rows of
zero, thus making W substochastic: as a result, by reiterating equation (4.4.18) for a certain
number k of times (i.e., by computing WTkr), then some or all of the entries in r will go
to 0. To solve this issue, two approaches can be suggested: (i) modiﬁcation of the network
structure, and (ii) modiﬁcation of the random surfer behavior. In the ﬁrst case, leak vertices
could be removed from the network so that they will receive zero rank; alternatively, leak
vertices could be “virtually” linked back to their in-neighbors, or even to all other vertices.
The result will be a row-stochastic matrix, that is, a matrix that is identical to W except
that it will have the columns corresponding to leak vertices that sum to 1. If we denote
with d a vector indexing the leak vertices (i.e., d(i) = 1 if vi has no outlinks, and d(i) = 0
otherwise), this row-stochastic matrix S is deﬁned as:
S = W + d1T/|V|.
(4.4.19)
However, equation (4.4.19) will not solve the problem of sinks. Therefore, Page and
Brin [BP98] also proposed to modify the random surfer behavior by allowing for teleporta-
tion, i.e., the random surfer who gets stuck in a sink, or simply gets “bored” occasionally,
can move by randomly jumping to any other vertex in the network. This is the fourth idea
behind the PageRank measure, which is implemented by a damping factor α ∈(0, 1) that
enables to weigh the mixture of random walk and random teleportation:
r = αSTr + (1 −α)p.
(4.4.20)
Above, vector p, usually called a personalization vector, is by default set to 1/|V|, but it
can be any probability vector. Equation (4.4.20) can be rewritten as:
G = αS + (1 −α)E,
(4.4.21)

Web Search Based on Ranking
83
where E = 1pT = 11T/|V|. The convex combination of S and E makes the resulting
“Google matrix” G to be both stochastic and irreducible.1 This is important to ensure (i) the
existence and uniqueness of the PageRank vector as stationary the probability distribution
π, and (ii) the convergence of the underlying Markov chain (at a certain iteration k, i.e.,
π(k+1) = Gπ(k)) independently of the initialization of the rank vector.2
Computing PageRank.
As previously indicated, the computation of PageRank re-
quires solving equation (4.4.21), which is equivalent to ﬁnding the principal eigenvector of
matrix G. Therefore, similar to other eigenvector centrality methods, the power iteration
algorithm is commonly used. Starting from any random vector r(0), it iterates through equa-
tion (4.4.20) until some termination criterion is met; typically, the power method is assumed
to terminate when the residual (as measured by the diﬀerence of successive iterations) is
below some predetermined threshold. Actually, as ﬁrst observed by Haveliwala (cf. [LM05]),
the ranking of the PageRank scores are more important than the scores themselves, that
is, the power method can be iterated until ranking stability is achieved, thus leading to a
signiﬁcant saving of iterations on some datasets.
The power iteration method lends itself to eﬃcient implementation thanks to the spar-
sity of real-world network graphs. Indeed, computing and storing matrix S (cf. equa-
tion (4.4.19)), and hence G, is not required, since the power method can be rewritten
as [LM05]:
πT(k+1) = πT(k)G = απT(k)W + (απT(k)d)1T/|V| + (1 −α)pT,
(4.4.22)
which indicates that only sparse vector/matrix multiplications are required. When imple-
mented in this way, each step of the power iteration method requires nonzero(W) opera-
tions, where nonzero(W) is the number of nonzero entries in W, which approximates to
O(|V|).
Choosing the damping factor. The damping factor α is by default set to 0.85. This
choice actually has several explanations. One is intuitively based on the empirical observa-
tion that a web surfer is likely to navigate by following 6 hyperlinks (before discontinuing
this navigation chain and randomly jumping on another page), which corresponds to a
probability α = 1 −(1/6) ≈0.85. In addition, there are also computational reasons. With
the default value of 0.85, the power method is expected to converge in about 114 iterations
for a termination tolerance threshold of 1.0E-8 [LM05]. Moreover, since the second largest
eigenvalue of G is α [Mey00], it can be shown that the asymptotic rate of convergence of
the power method is −log10 0.85 ≈0.07, which means that about 14 iterations are needed
for each step of accuracy improvement (in terms of digits).
In general, higher values of α imply that the hyperlink structure is more accurately
taken into account, however, along with slower convergence and higher sensitivity issues.
In fact, experiments with various settings of α have shown that there can be signiﬁcant
variation in rankings produced by diﬀerent values of α, especially when α approaches 1;
more precisely, signiﬁcant variations are usually observed for the mid-low ranks, while the
top of the ranking is usually only slightly aﬀected [Pre02, LM05].
Choosing the personalization vector. As previously discussed, the personalization
vector p can be replaced with any vector whose non-negative components sum up to 1. This
1A matrix is said irreducible if every vertex in its graph is reachable from every other vertex.
2Recall that the property of irreducibility of a matrix is related to those of primitivity and aperiodicity.
A nonnegative, irreducible matrix is said to be primitive if it has only one eigenvalue on its spectral circle;
a simple test by Frobenius states that a matrix X is primitive if and only if Xk > 0 for some k > 0, which is
useful to determine whether the power method applied to X will converge [Mey00]. An irreducible Markov
chain with a primitive transition matrix is called an aperiodic chain.

84
Graph-Based Social Media Analysis
hence includes the possibility that the vertices in V might be diﬀerently considered when
the random surfer restarts her chain by selecting a vertex v with probability p(v), which is
not necessarily uniform over all the vertices.
The teleportation probability p(v) can be determined to be proportional to the score the
vertex v obtains with respect to an external criterion of importance, or to the contribution
that the vertex gives to a certain topological characteristic of the network. For instance,
one may want to assign any vertex with a teleportation probability that is proportional to
the in-degree of the vertex, i.e.,
p(v) =
in(v)
P
u∈V in(u).
The personalization vector can also be used to boost the PageRank score for a speciﬁc
subset of vertices that are relevant to a certain topic, thus making the PageRank topic-
sensitive. We shall elaborate on this point later in Section 4.5.
4.4.4
Hubs and authorities
A diﬀerent approach to the computation of vertex prestige is based on the notions of
hubs and authorities. In a web search context, given a user query, authority pages are ones
most likely to be relevant to the query, while hub pages act as indices of authority pages
without necessarily being authorities themselves. These two types of web pages are related
to each other by a mutual reinforcement mechanism: in fact, if a page is relevant to a query,
one would expect that it will be pointed to by many other pages; moreover, pages pointing
to a relevant page are likely to point as well to other relevant pages, thus inducing a kind
of bipartite graph where pages that are relevant by content (authorities) are endorsed by
special pages that are relevant because they contain hyperlinks to locate relevant contents
(hubs)—although, it may be the case that a page is both an authority and a hub.
The above intuition is implemented by the Kleinberg’s HITS (Hyperlink Induced Topic
Search) algorithm [Kle98, Kle99]. Like PageRank and other eigenvector centrality methods,
HITS still handles an iterative computation of a ﬁxpoint involving eigenvector equations;
however, it originally views the prestige of a page as a two-dimensional notion, thus resulting
in two ranking scores for every vertex in the network. Also in contrast to PageRank, HITS,
produces ranking scores that are query-dependent. In fact, HITS assumes that hubs and
authorities are identiﬁed and ranked for vertices that belong to a query-focused subnetwork.
This is usually formed by an initial set of randomly selected pages containing the query
terms, which is expanded by also including the neighborhoods of those pages.
Let a and h be two vectors storing the authority and hub scores, respectively. The hub
score of a vertex can be expressed as proportional to the sum of the authority scores of its
out-neighbors; analogously, the authority score of a vertex can be expressed as proportional
to the sum of the hub scores of its in-neighbors. Formally, HITS equations are deﬁned as:
a = µATh
(4.4.23)
h = λAa,
(4.4.24)
where µ, λ are two (unknown) scaling constants that are needed to avoid the possibility that
the authority and hub scores will grow beyond bounds; in practice, a and h are normalized,
so that the largest value in each of the vectors equals 1 (or, alternatively, all values in each
of the vectors sum up to 1). Therefore, HITS works as follows:
1. For every vertex in the expanded query-focused subnetwork, initialize hub and au-
thority score (e.g., to 1).

Web Search Based on Ranking
85
2. Compute the following steps until convergence (i.e., a termination tolerance threshold
is reached):
(a) authority vector a using equation (4.4.23);
(b) hub vector h using equation (4.4.24);
(c) normalize a and h.
Note that, at the ﬁrst iteration, a and h are none other than the vertex in-degrees and the
out-degrees, respectively.
By substituting equation (4.4.23) and equation (4.4.24) in each other, hub and authority
can in principle be computed independently of each other, through the computation of
AAT (for the hub vector) and ATA (for the authority vector). Note that, the (i, j)-th
entry in matrix AAT corresponds to the number of pages jointly referred by pages i and
j; analogously, the (i, j)-th entry in matrix ATA corresponds to the number of pages that
jointly point to pages i and j. However, both matrix products lead to matrices that are not as
sparse, hence the only convenient way to compute a and h is iteratively in a mutual fashion
as described above. In this regard, just as in the case of PageRank, the rate of convergence
of HITS depends on the eigenvalue gap, and the ordering of hubs and authorities becomes
stable with much less iterations than the actual scores.
It should be noted that the assumption of identifying authorities by means of hubs might
not hold in information networks other than the Web; for instance, in citation networks, im-
portant authors typically acknowledge other important authors. This has somehow impacted
on the probably less popularity of HITS with respect to PageRank—which, conversely, has
been successfully applied in many other contexts, including citation and collaboration net-
works, lexical/semantic networks inferred from natural language texts, recommender sys-
tems, and social networks.
The TKC eﬀect.
Beyond limited applicability, HITS seems to suﬀer from two is-
sues that are related to both the precision and coverage of the query search results. More
precisely, while the coverage of search results directly aﬀects the size of the subnetwork,
the precision can signiﬁcantly impact on the tightly knit communities (TKC) eﬀect, which
occurs when relatively many pages are identiﬁed as authoritative via link analysis although
they actually pertain to only one aspect of the target topic; for instance, this is the case
when hubs point both to actual relevant pages and to pages that are instead relevant to
“related topics” [LM00]. The latter phenomenon is also called topic drift.
While the TKC eﬀect can be attenuated by accounting for the analysis of contents and/or
the anchor texts of the web pages (e.g., [BH98, CDR+98]), other link analysis approaches
have been developed to avoid overly favoring the authorities of tightly knit communities.
Lempel and Morgan [LM00] propose the Stochastic Approach for Link Structure Analysis,
dubbed SALSA. This is a variation of Kleinberg’s algorithm: it constructs an expanded
query-focused subnetwork in the same way as HITS, and likewise it computes an authority
and a hub score for each vertex in the neighborhood graph (and these scores can be viewed as
the principal eigenvectors of two matrices). However, instead of using the straight adjacency
matrix, SALSA weighs the entries according to their in- and out-degrees. More precisely,
the authority scores are determined by the stationary distribution of a two-step Markov
chain through random walking over in-neighbors of a page and then random walking over
out-neighbors of a page, while the hub scores are determined similarly with inverted order
of the two steps in the Markov chain. Formally, the Markov chain for authority scores has
transition probabilities:
pa(i, j) =
X
vq∈Bi∩Bj
1
in(vi)
1
out(vk)
(4.4.25)

86
Graph-Based Social Media Analysis
●
●
●
●
●
●
●
●
●
●
(a) closeness
(b) betweenness
●
●
●
●
●
●
●
●
●
●
●
●
(c) eigenvector centrality
(d) PageRank
●
●
●
●
●
●
●
●
●
●
●
●
(e) Kleinberg’s hub score
(f) Kleinberg’s authority score
FIGURE 4.4.1: Comparison of ranking performance of diﬀerent centrality methods on the
same example graph. Node size is proportional to the node degree. Lighter gray-levels
correspond to higher rank scores.

Web Search Based on Ranking
87
and the Markov chain for hub scores has transition probabilities:
ph(i, j) =
X
vq∈Ri∩Rj
1
out(vi)
1
in(vk).
(4.4.26)
Lempel and Morgan proved that the authority stationary distribution a is such that
a(vi) = in(vi)/ S
v∈V in(v), and that the hub stationary distribution h is such that h(vi) =
out(vi)/ S
v∈V out(v). Therefore, SALSA does not follow the mutual reinforcement principle
used in HITS, since hub and authority scores of a vertex depend only on the local links of
the vertex. Also, in the special case of a single-component network, SALSA can be seen as
a one-step truncated version of HITS [BRRT01]. Nevertheless, the TKC eﬀect is overcome
in SALSA through random walks on the hub-authority bipartite network, which imply that
authorities can be identiﬁed by looking at diﬀerent communities.
Figure 4.4.1 shows an illustrative comparison of various centrality methods discussed
in this section, on the same example network graph. The nodes in the graph are colored
using a gray palette, such that lighter gray-levels correspond to higher ranking scores that
a particular centrality method has produced over that graph.
4.4.5
SimRank
SimRank [JW02] is a general, iteratively mutual reinforced similarity measure on a link
graph, which is applicable in any domain with object-to-object relationships. The main
intuition behind SimRank is that “two objects are similar if they are related to similar
objects.” For instance, on a hyperlinked document domain like the Web, two web pages can
be regarded as similar if there exist hyperlinks between them, or in a recommender system,
we might say that two users are similar if they rate similar items (and, in a mutual rein-
forcement fashion, two items are similar if they are rated by similar users). The underlying
model of SimRank is the “random surfer-pairs model”, i.e., SimRank yields a ranking of
vertex pairs. The basic SimRank equation formalizes the intuition that two objects are sim-
ilar if they are referenced by similar objects. Given any two vertices u and v, their similarity,
denoted as S(u, v), is deﬁned as 1 if u = v, otherwise an iterative process is performed, in
which the similarity between u and v is recursively calculated in terms of in-neighbors of u
and v, respectively. The generic step of random walk of this process is deﬁned as:
S(u, v) = α
1
|Bu||Bv|
X
i∈Bu
X
j∈Bv
S(i, j),
(4.4.27)
where α is a constant between 0 and 1. As a particular case, if either u or v has no in-
neighbors, then S(u, v) = 0. It should be noted that equation (4.4.27) expresses the average
similarity between in-neighbors of u and in-neighbors of v. Moreover, it is easy to see that
SimRank scores are symmetric.
The basic SimRank equation lends itself to several variations, which account for diﬀerent
contingencies in a network graph. One of these variations allows for resembling the HITS
algorithm (cf. Section 4.4.4), since it considers that vertices in a graph may take on diﬀerent
roles, like hub and authority for importance. Within this view, equation (4.4.27) can be
replaced by two mutually reinforcing functions that express the similarity of any two vertices
in terms of either their in-neighbors or out-neighbors:
S1(u, v) = α1
1
|Ru||Rv|
X
i∈Ru
X
j∈Rv
S2(i, j)
(4.4.28)

88
Graph-Based Social Media Analysis
and
S2(u, v) = α2
1
|Bu||Bv|
X
i∈Bu
X
j∈Bv
S1(i, j),
(4.4.29)
where constants α1, α2 have the same semantics as α in equation (4.4.27). Another variation
of SimRank is the min-max variation, which captures the commonality underlying two
similarity notions that express the endorsement of one vertex towards the choices of another
vertex, and vice versa. Given vertices u, v, two intermediate terms are deﬁned as:
Su(u, v) = α 1
|Ru|
X
i∈Ru
max
j∈Rv S(i, j)
(4.4.30)
and
Sv(u, v) = α 1
|Rv|
X
i∈Rv
max
j∈Ru S(i, j),
(4.4.31)
with the ﬁnal similarity score computed as S(u, v) = min{Su(u, v), Sv(u, v)}, which ensures
that each vertex chooses the other’s choices.
SimRank is a computationally expensive method. The space required for each iteration
is simply O(|V|2), whereas the time required is O(I|V|2d), where d denotes the average of
|Bu||Bv| over all vertex-pairs u, v, and I is the number of iterations. One way to reduce
the computational burden is to prune the link graph, which avoids computing the simi-
larity for every vertex-pair by considering only vertex-pairs within a certain radius from
each other [JW02]. Many other methods to speed up the SimRank computation have been
developed in the literature. For instance, Fogaras and Racz [FR05] proposed a probabilistic
approximation based on the Monte Carlo method. Lizorkin et al. [LVGT08] proposed diﬀer-
ent optimization techniques, including partial sums memorization that can reduce repeated
calculations of the similarity among diﬀerent pairs by caching part of similarity summa-
tions for later reuse. Antonellis et al. [AGC08] extended SimRank using evidence factor for
incident nodes and link weights. More recently, Yu et al. [YLZ13] proposed a ﬁne-grained
memoization method to share the common parts among diﬀerent partial sums; the same
authors also studied eﬃcient incremental SimRank computation over evolving graphs. At
the time of the writing of this chapter, the most recent study is that by Du et al. [DLC+15],
which has focused on SimRank problems in uncertain graphs.
4.5
Topic-Sensitive Ranking
In this section we discuss main approaches to make the process of ranking web docu-
ments, or similarly their corresponding users, topic-sensitive. The general goal is to drive the
ranking mechanism in such a way that the obtained ordering and scoring reﬂects a target
scenario in which the vertices in the network are to be evaluated based on their relevance
to a topic of interest. The term topic is here intentionally used with two diﬀerent meanings,
which correspond to diﬀerent perspectives of quality of web resources: the one normally
refers to the content of web documents, whereas the other one refers to the relation of web
documents with web spammers, and more speciﬁcally to their trustworthiness, or likelihood
of not being a spammer’s target.

Web Search Based on Ranking
89
4.5.1
Content as topic
As previously mentioned in Section 4.4, the PageRank personalization vector p can be
replaced with any probability vector deﬁned to boost the PageRank score for a speciﬁc
subset of vertices that are relevant to a particular topic of interest.
A natural way to implement the above idea is to make the teleportation query-dependent,
in such a way that a vertex (page) is more likely to be chosen if it covers the query terms.
More precisely, if we denote with B ⊆V a subset of vertices of interest, then p = 1/|V| is
replaced with another vector biased by B, pB whose entries are set to 1/|B| only for those
vertices that belong to B, and zero otherwise. Because of the concentration of random walk
restarts at vertices from B, these vertices will obtain a higher PageRank score than they
obtained using a conventional (non-topic-biased) PageRank.
Intuitively, this way of altering the behavior of random surﬁng reﬂects the diﬀerent
preferences and interests that the random surfer may have and specify as terms in a query.
Moreover, for eﬃcient indexing and computation, the subset B usually corresponds to a
relatively small selection of vertices that cover some small number of topics. For instance,
one might want to constrain the restart of the random walk to select only pages that are
classiﬁed under one or more categories (e.g., “politics,” “sports,” “technology,” and so on)
of the Wikipedia topic classiﬁcation system3 or any other publicly available web directory.
The consequence of this topic-biased selection is that not only will the random surfer be at
pages that are identiﬁed as relevant to the selected topics, but also any neighboring page,
and any page reachable along a short path (from one of the known relevant pages) will be
likely relevant as well.
Topic aﬃnity and user ranking. Determining topic aﬃnity in web sources is central
to identifying web pages that are related to a set of target pages. Topic aﬃnity can be
measured by using one or a combination of the following three main approaches.
• Text-Based methods. Besides cosine similarity in vector space models (e.g., [WVS96]),
text-based approaches can also involve resemblance measures (e.g., Jaccard or Dice
coeﬃcients) which are deﬁned in terms of the overlap between two documents modeled
as sets of text chunks [BGMZ97].
• Link-Based methods.
The link-based topic aﬃnity approach has traditionally bor-
rowed from citation analysis, since the hyperlinking system of endorsement is in anal-
ogy with the citation mechanism in research collaboration networks. Particularly,
co-citation analysis is eﬀective in detecting cores of articles or authors given a par-
ticular subject matter. Early applications of co-citation analysis to topical aﬃnity
detection and web document clustering include [ER90, Lar96, PP97]. Essentially, a
citation-based measure of topic aﬃnity can be formalized as a co-citation strength
or, alternately, as a bibliographic-coupling (also called co-reference) strength; i.e., two
documents are related proportionally to the frequency with which they are cited to-
gether (resp., the frequency with which they have references in common). Furthermore,
similarity between two documents can also be evaluated in terms of the number of
direct paths between the two documents.
• Usage-Based methods. Finally, the usage-based topic aﬃnity approach is based on the
assumption that the interaction of users with web resources (stored via user access
and activity logs) can aid in improving the quality of content, thus increasing the
performance of web search systems. This approach is strictly related to techniques
of web personalization and adaptivity based on customization or optimization of the
3http://en.wikipedia.org/wiki/Category:Main topic classiﬁcations.

90
Graph-Based Social Media Analysis
users’ navigational experiences, as originally studied by Perkowitz and Etzioni [PE97,
PE99].
Topic aﬃnity detection is, however, not only essential to characterize similarity and
relatedness of web pages by content, but is also helpful in driving the topic-sensitive ranking
of web sources and their users.
TwitterRank. Topic-sensitive ranking in combination with topic aﬃnity measures is in
fact widely used in online user communities, such as social media networks and collaboration
networks. An exemplary method is represented by the TwitterRank algorithm [WLJH10],
which was originally designed to compute the prestige or inﬂuence of users in the Twitter
environment; the algorithm can, however, be applied to other platforms similar to Twitter,
or in general to any social network providing microblogging services. A directed graph
G = (V, E) is used to model followship relations among users of Twitter, i.e., there is an
edge from vertex vi to vertex vj if the i-th user follows the j-th user. Therefore, according
to PageRank, the higher the number and inﬂuence of the followers, the higher the inﬂuence
of a user of Twitter. Besides the PageRank principle, a key assumption in TwitterRank is
that, since followship presumes content consumption (i.e., reading, or replying to tweets),
the inﬂuence a user has on each follower is determined by the relative amount of content
the follower received from her. This means that in TwitterRank a random surfer performs
a topic-speciﬁc random walk. Moreover, since users generally have diﬀerent interests in
various topics, the inﬂuence of Twitter users also varies with respect to diﬀerent topics.
Formally, the (stochastic) transition probability matrix Pt used in TwitterRank is speciﬁed
contextually to a topic t, and deﬁned in such a way that, for any users vi, vj:
Pt(i, j) =
|Tj|
P
(vi,vk)∈E |Tk|simt(i, j),
(4.5.1)
where Tj is the set of tweets published by user vj, and simt(i, j) is the similarity between
vi and vj with respect to topic t.
To compute the similarity between two users conditionally to a given topic, TwitterRank
evaluates the diﬀerence between the probability that the two users are interested in the same
topic t:
simt(i, j) = 1 −|d
DT it −d
DT jt|,
(4.5.2)
where d
DT is the row-normalized version of the document-topic matrix DT, whose (i, t)-th
entry stores the number of times a word in the tweets by user vi has been assigned to topic
t. The document-topic matrix represents a low-dimensional representation of a collection of
documents, where a document corresponds to the set of tweets of a user. This document
representation can in principle be obtained by using some linear projection technique, such
as LSI (cf. Section 4.3), which is able to provide a low dimensional mapping from a high
dimensional vector space, using an orthogonal transformation based on singular value de-
composition to convert a set of observations of possibly correlated variables into a set of lin-
early uncorrelated variables (or components). A document-topic representation can also be
obtained via statistical topic modeling, which assumes that a document can be represented
as a mixture of probability distributions over its constituent terms, where each component
of the mixture refers to a main topic. While still using a “bag of words” assumption (a
document is treated as a vector of word counts), the document representation is obtained
by a generative process, i.e., a probabilistic process that expresses document features as
being generated by a number of latent variables. Compared to conventional vector-space
modeling, statistical topic models are generally able to involve (latent) semantic aspects un-
derlying correlations between words to leverage the structure of topics within a document.
TwitterRank utilizes the well-known Latent Dirichlet Allocation (LDA) method [BNJ03].

Web Search Based on Ranking
91
As a variant of topic-biased PageRank, the TwitterRank equation, for a given topic t, is
deﬁned as:
rt = αPtrt + (1 −α)et,
(4.5.3)
where et is the t-th (normalized) column vector of the DT matrix. By aggregating over
all topics, the global TwitterRank vector is given as: r = P
t ωtrt, where ωt is the weight
associated to topic t. The authors of TwitterRank suggest a number of ways to compute
these topic weights. One of the ways is to set ωt as the prior probabilities of the various
topics, estimated proportionally to the number of times unique words have been assigned to
corresponding topics. Alternatively, rt can be set as the probabilities that a particular user
vi is interested in diﬀerent topics, which are calculated according to the number of times
words in vi’s tweets have been assigned to corresponding topics as captured in DT.
4.5.2
Trust as topic
PageRank is vulnerable to adversarial information retrieval. In fact, link spamming
techniques can enable web pages to achieve higher scores than what they actually deserve.
To do this, spammers normally create the so-called spam farms, i.e., collections of pages
whose role is to support the artiﬁcial increase of the PageRank score of target pages. In a
typical scenario, a spam farm owns a certain number n of supporting pages, each of which
has a bidirectional connection only with the target page. Moreover, the target page also has
incoming links from outside the spam farm; this is made possible by applying one or more
link spamming strategies, such as inviting others to post comments on the spammer site
(target page). It can easily be demonstrated that the PageRank score r(s) of the spammer’s
target page can be computed as:
r(s) = r(ns)
1 −α2 +
α
1 + α
n
N ,
(4.5.4)
assuming that a certain amount r(ns) of PageRank comes from outside the spam farm (i.e.,
from the pages not owned by the spammer but linked to the target page), and that there are
N pages on the Web. Therefore, the size and structure of the spam farm can be manipulated
to amplify the PageRank score of the spammer’s target page.
Combating link spam has been a necessary task for developers of web search systems in
the last few years. One approach is to locate the spam farms, knowing that, as we previously
discussed, they may have a typical structure where one page links to a very large number of
pages, each of which links back to it. However, this approach is not scalable since spammers
can always develop farm structures that diﬀer from the known ones.
TrustRank. A diﬀerent approach is instead to make the PageRank aware of spam or,
in general, untrustworthy pages, by inducing topic-sensitivity in order to lower the score of
those pages. Within this view, a well-known method that was introduced to combat web
spam and ﬁnally detect trustworthy pages is TrustRank [GGMP04]. Basically, the algorithm
ﬁrst selects a small seed set of pages whose “spam status” needs to be determined. A human
expert then examines the seed pages, and tells the algorithm if they are spam (bad pages)
or not (good pages). Finally, the algorithm identiﬁes other pages that are likely to be good
based on their connectivity with the good seed pages. The pages in the seed set are classiﬁed
using a function called oracle, which is as:
O(p) =
( 0
if p is bad
1
if p is good.
(4.5.5)
However, at a large scale, oracle invocations are expensive, and in fact they are used

92
Graph-Based Social Media Analysis
only over the seed set. Therefore, to evaluate pages without relying on the oracle function,
the likelihood that a given page p is good will be estimated. A key assumption used in
TrustRank to identify good pages is the so-called approximate isolation principle, that is,
“high-quality pages are unlikely to point to spam or low-quality pages.” Upon this principle,
a trust function T is deﬁned that yields a range of values between 0 (bad) and 1 (good).
Ideally, for any page p, T(p) gives the probability that p is good: T(p) = Pr[O(p) = 1].
Desirable properties for the trust function are:
• Ordered Trust Property:
T(p) < T(q) ⇔Pr[O(p) = 1] < Pr[O(q) = 1]
T(p) = T(q) ⇔Pr[O(p) = 1] = Pr[O(q) = 1]
• Threshold Trust Property:
T(p) > δ ⇔O(p) = 1.
Given the network graph G = (V, E), TrustRank ﬁrst computes the seed set, characterized
by a vector s, via the following iterative equation:
s = βUs + (1 −β) 1
|V|1,
(4.5.6)
where β is a decaying factor ranging between 0 and 1, and U is the “inverse” connectivity
matrix of the graph:
U(i, j) =
(
1/in(vj)
if (vi, vj) ∈E
0
otherwise.
(4.5.7)
Note that pages in the seed set should be well-connected to other pages in order to propagate
trust to many pages quickly. Therefore, they are chosen among those that have a large out-
degree. For this purpose, PageRank is computed by reversing the in-links with the out-links
in the graph; here a high PageRank score will indicate that trust can ﬂow with a small
number of hops along the out-links.
Once the s vector is computed, it is sorted in decreasing order according to the probabil-
ity that every vertex belongs to the seed set. Only the top-L vertices are retained in the seed
set. The next step consists of the computation of the personalization vector p, such that
p(vi) = 1 if the vertex vi belongs to the seed set and is a good page, and 0 otherwise. The
TrustRank vector is ﬁnally computed using the basic PageRank equation (cf. Section 4.4,
equation (4.4.20)), with personalization vector set to the normalized p.
4.6
Ranking in Heterogeneous Networks
So far we have discussed information networks under the common assumption of repre-
sentation as homogeneous networks, i.e., nodes are objects of the same entity type (e.g., web
pages, users) and links are relationships of the same type (e.g., hypertext linkage, friend-
ship). However, nodes and node relations can be of diﬀerent types. For instance, in a research
publication network context, nodes can represent authors, publications, and venues, while
relations can be of type “written by” (between publication nodes and author nodes), “cited
by” (between publication nodes), co-authorship (between author nodes), and so on. As an-
other example, an online social network consists not only of persons, but also of diﬀerent

Web Search Based on Ranking
93
objects like photos, tags, texts, and so on; moreover, diﬀerent kinds of relations may occur
among diﬀerent objects (e.g., a photo may be labeled with a certain tag, a person can up-
load a photo, write a text, or request friendship to another person). Similar scenarios can
be found in a variety of application domains, including online e-commerce systems, medical
systems, and many others. Consequently, such real-world networks might be conveniently
modeled as heterogeneous or typed networks, in order to better capture the (possibly subtly)
diﬀerent semantics underlying the diﬀerent types of entities and relationships.
Following [SH12], given a set of vertex types T and a set of edge types R, a heterogeneous
information network (HIN) is deﬁned as a directed graph G = (V, E) with a vertex type
mapping function τ : V →T and an edge type mapping function φ : E →R, where each
vertex v ∈V belongs to one particular vertex type τ(v) ∈T , each edge e ∈E belongs to a
particular relation φ(e) ∈R. If two edges belong to the same relation type, they share the
same starting vertex type as well as the ending vertex type. Moreover, it holds that either
|T | > 1 or |R| > 1; otherwise, as a particular case, the information network is homogeneous.
The network schema, denoted as SG = (T , R), is a meta template for a heterogeneous
network G = (V, E) with set of vertex types T and set of edge types R.
In [SH12], Sun and Han provide a set of suggestions to guide systematic analysis of
HINs, which are reported as follows.
1. Information propagation. A ﬁrst challenge is how to propagate information across het-
erogeneous types of nodes and links; in particular, how to compute ranking scores,
similarity scores, and clusters, and how to make good use of class labels, across het-
erogeneous nodes and links. Objects in HINs are interdependent and knowledge can
only be mined using the holistic information in a network.
2. Exploring network meta structures. The network schema provides a meta structure
of the information network. It provides guidance on the search and mining of the
network and helps analyze and understand the semantic meaning of the objects and
relations in the network. Meta-path-based similarity searches and mining methods can
be useful to explore network meta structures.
3. User-Guided exploration of information networks. A certain weighted combination
of relations or meta-paths may best ﬁt a speciﬁc application for a particular user.
Therefore, it is often desirable to automatically select the relation (or meta-path)
combinations with appropriate weights for a particular search or mining task based
on a user’s guidance or feedback. User-Guided or feedback-based network exploration
can be a useful strategy.
4.6.1
Ranking in heterogeneous information networks
Ranking models are central to address the new challenges in managing and mining large-
scale heterogeneous information networks. In fact, many proposals have been developed for
a variety of tasks such as keyword search in databases (e.g., [BHP04]), Web object ranking
(e.g., [NZWM05]), expert search in digital libraries (e.g., [GMG11, ZFT+11, DHLK12]),
link prediction (e.g., [LK07, DLC11]), recommender systems and Web personalization
(e.g., [HSG10, LSK+11, VNL+11]), and sense ranking in tree-structured data [IT14]. Some
work has also been developed using path-level features in the ranking models, such as
the path-constrained random walk [LC10] and PathSim [SHY+11] for top-k similarity
searches based on meta-paths. Moreover, there has been an increasing interest in integrat-
ing ranking with mining tasks, like the case of ranking-based clustering addressed by the

94
Graph-Based Social Media Analysis
RankClus [SHZ+09] and NetClus [SYH09] methods. In the following, we focus on ranking
in heterogeneous information networks and provide a brief overview of the main methods.
ObjectRank. One of the ﬁrst attempts to use a random-walk model over a heteroge-
neous network is represented by ObjectRank [BHP04]. The algorithm is an adaptation of
topic-sensitive PageRank to a keyword search task in databases modeled as labeled graphs.
The HIN framework in ObjectRank consists of a data graph, a schema graph and an
authority transfer graph. The data graph GD(VD, ED) is a labeled directed graph where every
node v has a label λ(v) and a set of keywords. Nodes in VD represent database objects
which may have a sub-structure (i.e., each node has a tuple of attribute name/attribute
value pairs). Moreover, each edge e ∈ED is labeled with a “role” λ(e) which describes the
relation between the connected nodes.
The schema graph GS(VS, ES), is a directed graph which describes the structure of GD,
i.e., it deﬁnes the set of node and edge labels. A data graph GD(VD, ED) conforms to a
schema graph GS(VS, ES) if there is a unique assignment µ such that:
1. for every node v ∈VD there is a node µ(v) ∈VS such that λ(v) = λ(µ(v));
2. for every edge e ∈ED from node u to node v there is an edge µ(e) ∈ES from µ(u) to
µ(v) and λ(e) = λ(µ(e)).
The authority transfer graph can refer to both a schema graph or a data graph. The authority
transfer schema graph GA(VS, EA) reﬂects the authority ﬂow through the edges of the graph.
In particular, for each edge in ES two authority transfer edges are created, which carry the
label of the schema graph edge forward and backward and are annotated with a (potentially
diﬀerent) authority transfer rate. The authority transfer schema graph can be based on a
trial and error process or on a domain expert task.
A data graph conforms to an authority transfer schema graph if it conforms to the
corresponding schema graph. From a data graph GD(VD, ED) and a conforming authority
transfer schema graph GA(VS, EA) a authority transfer data graph GAD(VD, EAD) can be
derived. Edges of the authority transfer data graph are annotated with authority transfer
rates as well, controlled by a formula which propagates the authority from a node based on
the number of its outgoing edges.
ObjectRank can be used to obtain a keyword-speciﬁc ranking as well as a global ranking.
Given a keyword w, the keyword-speciﬁc ObjectRank is a biased PageRank in which the base
set is built upon the set of nodes containing the keyword w:
rw = αArw + 1 −α
|S(w)|s,
(4.6.1)
where S(w) denotes the base set speciﬁc to w, and si = 1 if vi ∈S(w) and si = 0 otherwise.
The global ObjectRank is basically a standard PageRank. The ﬁnal score of a node given a
keyword w is then obtained by combining the keyword-speciﬁc rank and the global rank.
In [BHP04], Balmin et al. also discussed an optimization of the ranking task in the case
of directed acyclic graphs (DAGs). More speciﬁcally, the authors showed how to serialize
the ObjectRank evaluation over single-pass ObjectRank calculations for disjoint, non-empty
subsets L1, . . . , Lq obtained by partitioning the original set of vertices in a DAG. Upon a
topological ordering of Lh (h = 1..q) that imposes no backlink from every vertex in Lj to
any vertex in Li, with i < j, the ranking of nodes is ﬁrst computed on L1 ignoring the rest
of the graph, then only the ranking scores of vertices in L1 connected to vertices in L2 are
reused to calculate ObjectRank for L2, and so on.
PopRank.
In [NZWM05], PopRank is proposed to rank heterogeneous web objects
of a speciﬁc domain by using both web links and object relationship links. The rank of an

Web Search Based on Ranking
95
object is calculated based on the ranks of objects of diﬀerent types connected to it, and a
parameter called the popularity propagation factor is associated with every type of relation
between objects of diﬀerent types.
The PopRank score vector rτ for objects of type τ0 is deﬁned as a combination of the
individual popularity r, and the inﬂuence from objects of other types:
rτ = αr + (1 −α)
X
τt
γτtτ0MT
τtτ0rτt,
(4.6.2)
where γτtτ0 is the popularity propagation factor of the relationship link from an object of
type τt to an object of type τ0 and P
τt γτtτ0 = 1, Mτtτ0 is the row-normalized adjacency
matrix between type τt and type τ0 , and rτt is the PopRank score vector for type τt. In order
to learn the popularity propagation factor γτtτ0, a simulated annealing-based algorithm is
proposed, according to partial ranking lists given by domain experts.
Bipartite SimRank. The SimRank algorithm discussed in Section 4.4.5 can naturally
be extended to bipartite networks such as, e.g., user-item rating networks. Intuitively, the
similarity of users and the similarity of items are mutually reinforcing notions that can be
formalized by two equations analytically similar to equation (4.4.28) and equation (4.4.29).
More precisely, assuming that edges are directed from vertices of type 1 (e.g., users) to
vertices of type 2 (e.g., items), and using superscripts (1) and (2) to denote the two types
of vertices, respectively, we have the following equations:
S(u(1), v(1)) = α1
1
|Ru(1)||Rv(1)|
X
i∈Ru(1)
X
j∈Rv(1)
S(i, j),
(4.6.3)
and
S(u(2), v(2)) = α2
1
|Bu(2)||Bv(2)|
X
i∈Bu(2)
X
j∈Bv(2)
S(i, j),
(4.6.4)
where constants α1, α2 have the same semantics as α in equation (4.4.27). Equation (4.6.3)
corresponds to the similarity between vertices of type-1 is the average similarity between
the vertices (of type-2) that they refer to (e.g., items that the two users rated), whereas
equation (4.6.4) corresponds to the similarity between vertices of type-2 is the average
similarity between the vertices (of type-1) that they are referred to (e.g., the users who
rated the two items).
4.6.2
Ranking-Based clustering
Given the diversity of node and link types that characterizes HINs, it is also important
to understand how the various nodes of diﬀerent types can be grouped together. An eﬀective
solution is to “integrate” ranking and clustering tasks. This is the basic idea behind methods
such as RankClus and NetClus, which will be discussed next.
RankClus. In [SHZ+09], the RankClus algorithm is introduced, which integrates clus-
tering and ranking on a bi-typed information network G = (V, E), such that V = V0 ∪V1,
with V0 ∩V1 = ∅. Hence, the nodes in the network belong to one of two predetermined
types, hereinafter denoted as τ0, τ1. The authors use a bibliographic network as a running
example, which contains venues and authors as nodes. Two types of links are considered:
author-venue publication links, with edge weights indicating the number of papers an author
has published in a venue, and co-authorship links, with edge weights indicating the num-
ber of times two authors have collaborated. A formal deﬁnition of a bi-typed information
network is reported as follows.

96
Graph-Based Social Media Analysis
A key issue in clustering tasks over network objects is that, unlike in traditional attribute
based datasets, object features are not explicit. RankClus explores rank distribution for each
cluster to generate new measures for target objects, which are low-dimensional. The clusters
are improved under the new measure space. More importantly, this measure can be further
enhanced during the iterations of the algorithm, so that the quality of clustering and ranking
can be mutually enhanced in RankClus.
Two ranking functions over bi-typed bibliographic network are deﬁned in [SHZ+09]:
Simple Ranking and Authority Ranking. Simple Ranking is based on the number of publi-
cations, which is proportional to the number of papers accepted by a venue or published
by an author. Using this measure, authors publishing more papers will have a higher rank
score, even if these papers are all in junk venues. Authority Ranking is deﬁned to give an
object a higher rank score if it has more authority. Iterative rank score formulas for authors
and venues are deﬁned based on two principles: (i) highly ranked authors publish many
papers in highly ranked venues, and (ii) highly ranked venues attract many papers from
highly ranked authors. When considering the co-author information, the rank of an author
is enhanced if s/he co-authors with many highly ranked authors.
Diﬀerently from Simple Ranking (which takes into account only the neighborhood of
a node), the score of an object with Authority Ranking is based on the score propagation
over the whole network. Assuming initial (e.g., random) partition of K clusters {Ck}K
k=1
of nodes of target type τ0 of a bi-typed information network, the conditional rank of τ1-
type nodes should be very diﬀerent for each of the K clusters of τ0-type nodes (e.g., in the
bibliographic network case, the rank of authors should be diﬀerent for each venue-cluster).
The idea is that, for each cluster Ck, conditional rank of V1, rV1|Ck, can be viewed as a rank
distribution of V1, which in fact is a measure for cluster Ck. Then, for each node v ∈V0, the
distribution of object u ∈V1 can be viewed as a mixed model over K conditional ranks of
V1, and thus can be represented as a K dimensional vector in the new space [SHZ+09]. The
authors use an expectation-maximization algorithm to estimate parameters of the mixed
model for each target object, and then deﬁne a cosine similarity based distance measure
between an object and a cluster.
Given a bi-typed information network G = (V0 ∪V1, E), the ranking functions for V0 and
V1, and a number K of clusters, RankClus produces K clusters over V0 with conditional
rank scores for each v ∈V0, and conditional rank scores for each u ∈V1. The main steps of
the RankClus algorithm are summarized as follows [SH12].
• Step 0 - Initialization: Assign each target node a cluster label from 1 to K randomly.
• Step 1 - Ranking for each cluster: Calculate conditional ranks for nodes of type V1
and V0 and within-cluster ranks for nodes of type V0. If any cluster is empty, the
algorithm needs to restart in order to produce K clusters.
• Step 2 - Estimation of the cluster membership vectors for the target objects: Estimate
the parameter of the mixted model, obtain new representations for each target object
and centers for each target cluster: sv and sCk.
• Step 3 - Cluster adjustment: Calculate the distance from each object to each cluster
center and assign it to the closest cluster.
• Repeat Steps 1, 2, and 3 until clusters are stable or change by a very small ratio ϵ, or
until a predeﬁned maximum number of iterations is reached.
NetClus.
NetClus [SYH09] extends RankClus from bi-type information networks
to multi-typed heterogeneous networks with a star network schema, where the objects

Web Search Based on Ranking
97
of diﬀerent types are connected via a unique “center” type. An information network,
G = (V, E, W), with T + 1 types of objects such that V = {Vt}T
t=0, has a star network
schema if ∀e = (vi, vj) ∈E, vi ∈V0 ∧vj ∈Vt(t ̸= 0) or vice versa. Type τ0 is called the
center or target type, whereas τt(t ̸= 0) are attribute types.
Examples of star networks are tagging networks, usually centered on a tagging event,
and bibliographic networks, which are centered on papers. In general, a star network schema
can be used to map any n-nary relation set (e.g., records in a relational database, with each
tuple in the relation as the center object and all attribute entities linking to the center
object).
NetClus aims to discover a set of sub-network clusters, and within each cluster a gen-
erative model for target objects is built given the ranking distributions of attribute objects
in the network. This ranking distribution is calculated using an authority ranking process
based on a power iteration method that combines the weight matrices deﬁned between the
various types and the center type. The clusters generated are not groups of single typed
objects but a set of sub-networks with the same topology as the input network, called net-
clusters. Each net-cluster is a sub-layer representing a concept of community of the network,
which is an induced network from the clustered target objects, and attached with statistical
information for each object in the network.
NetClus maps each target object, i.e., that from the center type, into a K-dimensional
vector measure, where K is the number of clusters speciﬁed by the user. The probabilistic
generative model for the target objects in each net-cluster is ranking-based, which factorizes
a net-cluster into T independent components, where T is the number of attribute types.
NetClus uses the same ranking functions deﬁned for RankClus (Simple Ranking and Au-
thority Ranking) adapted to the star network case. The core steps of the NetClus algorithm,
given the desired number of clusters K, are summarized as follows [SH12].
• Step 0: Generate initial partitions for target objects and induce initial net-clusters
from the original network according to the partitions, i.e., {C0
k}K
k=1.
• Step 1: For each net-cluster, build ranking-based probabilistic generative model, i.e.,
{P(v|Ct
k)}K
k=1.
• Step 2: For each target object, calculate the posterior probabilities (P(Ct
k|v)) and
update their cluster assignment according to the new measure deﬁned by the posterior
probabilities to each cluster.
• Step 3: Repeat Step 1 and 2 until the clusters do not change signiﬁcantly, i.e.,
{C∗
k}K
k=1 = {Ct
k}K
k=1 = {Ct−1
k
}K
k=1.
• Step 4: Calculate the posterior probabilities for each attribute object (P(C∗
k|v)) in
each net-cluster.
4.7
Organizing Search Results
The utility of web search methods depends on multiple factors. The primary ones are the
underlying retrieval method and the ranking function. The organization of search results
and the way the search results are presented also form an important aspect of the utility
of a web search method. If the retrieved results for a user query are homogeneous, then

98
Graph-Based Social Media Analysis
simply presenting the ranked list as it is, is a fairly good strategy. In case of queries which
are ambiguous the result set can be diverse (e.g., queries which span multiple topics), a
simple ranked list presentation of the results will not be eﬀective. A better organization of
the results is needed to assist the users to quickly ﬁnd the information they are looking for.
This section deals with the diﬀerent methods used to organize the search results.
Clustering Results
One way to reduce the ambiguity of the search results, which span multiple topics, is
by clustering the top ranked documents into multiple natural clusters. These clusters are
expected to correspond to diﬀerent subtopics associated with the general query topic. A
label is generated for each of the identiﬁed clusters, and results grouped according to their
cluster are presented along with the identiﬁed cluster label. The clustering algorithms that
can be employed for clustering results on-the-ﬂy need to have a few speciﬁc properties, the
ability to cluster results eﬃciently based on the snippets associated with them instead of
the whole document, so that the user experience is not aﬀected; fast enough so that it can
be used in an online setting, so that the model can be built incrementally utilizing the
learning done so far; and the ability to generate a label for each of the clusters, so that the
user can relate to the clusters produced.
Text-based clustering.
One of the earliest methods is called Suﬃx Tree Clustering
(STC) [ZE98], which creates clusters based on phrases shared between the documents. STC
treats documents as strings instead of bags-of-words. This allows the algorithm to make use
of proximity information between words. A suﬃx tree is then created to eﬃciently identify
sets of documents that share common phrases and this information is used to create clus-
ters. It was shown that STC was both faster and more eﬃcient than the standard clustering
algorithms, and using snippets was as eﬀective as using whole documents. Computing mean-
ingful names for the clusters is a challenge. A supervised learning was proposed by Zeng et.
al. [ZHMM04]. In this method, the clustering problem was re-formalized as a phrase rank-
ing problem. Given a query and the ranked list of documents from the web search engine
results, the method builds a regression model, and then extracts and ranks salient phrases
as candidate cluster names. The documents are assigned to relevant salient phrases to form
candidate clusters, and the ﬁnal clusters are then generated by merging these candidate
clusters.
Utilizing hyperlinking information. A web hyperlink can be thought of as a state-
ment to indicate that the linked document is related to the document linking to it. Thus,
clustering of search results can also be performed by utilizing the hyperlinking structure
between the documents. The classical approach of clustering documents is by computing
similarities using the content based features (i.e., the features derived from the textual con-
tent of the document). The linking structure can be used alone or along with content based
features to compute the similarities. Many methods have been developed which utilize the
co-citation, bibliographic coupling, and direct paths based similarity between the web pages
to compute the clusters, as mentioned in Section 4.5. Such link based features can be used
to compute correlation coeﬃcients, which can then be used to do cluster analysis. Cluster-
ing based on linking features’ information has been shown to produce high quality clusters
[Lar96].
Many hybrid approaches have also been developed which utilize both text based fea-
tures and linking features to cluster the web pages. In one of the simple extensions termed
a content-link clustering [WVS96] algorithm, the similarity between the documents is com-
puted as the maximum between the text similarity and the link based similarity. The com-
puted similarity is then used to do the cluster analysis. In another approach [PPR96],

Web Search Based on Ranking
99
the combined features are represented as feature vectors for the documents. An activation
spreading technique is used to cluster the collection of documents. Activation spreading
techniques start by activating a node and then spreading the value of the node to all its
connected nodes. This process is continued and eventually the nodes which have the highest
scores are considered to be the nodes relevant to the initial node.
Other hybrid approaches can utilize hyperlink information in combination with usage-
based features (cf. Section 4.5). A typical clustering framework in this category, known as
PageGather [PE97, PE99], requires the calculation of co-occurrence frequencies between
web pages. This information is then used to identify the maximal cliques or connected
components in the graph built over the page similarity graph. A ranking of clusters is
ﬁnally obtained based on the average co-occurrence frequency between all pairs of web
pages in a cluster.
4.8
Conclusion
The web search methods presented in this chapter focused on providing an overview of
existing information retrieval methods, followed by methods which relate to utilizing the
hyperlink structure and network topology of the web pages, identifying the high quality
and diversity of the web content, and addressing the diﬀerent user needs. Some methods
designed for web-related contexts, like ranking in heterogeneous information networks, were
also presented in this chapter; the ever increasing closeness of the web with such diﬀerent
information network types makes the application and extensions of web search methods
relevant in the context of various information networks. This chapter concludes by providing
a brief discussion on some of the emerging trends in the area of web search.
Emerging Trends
The ever increasing utility of web search has enabled many emerging trends in this area.
One such area is Learning to Rank (LTR), which specializes in constructing ranking models
based on diﬀerent machine learning methods. In these methods, the query-document pairs
are usually represented by a bag-of-words model using numerical vectors (feature vectors).
LTR methods utilize various features associated with web pages and queries, such as struc-
tural features (based on HTML tags), language modeling features, PageRank, and query
popularity. These features are divided into three groups, namely: (i) query-independent or
static features, i.e., features that depend only on the document and not on the query (e.g.,
PageRank), (ii) query-dependent or dynamic features, i.e., features that depend both on
the document and the query (e.g., TF-IDF score), and (iii) query features, i.e., features that
depend only on the query (e.g., query popularity). Based on the input training data and the
loss function used, LTR approaches are broadly categorized into three types, namely: (i)
the pointwise approach, which approximates the LTR problem as a regression problem by
representing each query-document pair with a real valued or ordinal score; (ii) the pairwise
approach, which approximates the LTR problem as a classiﬁcation problem and a binary
classiﬁer is used to classify the documents in pairs as relevant or not relevant; and (iii) the
listwise approach, which optimizes either pointwise or pairwise evaluation criteria on the
complete list of queries in the training data. In recent years, several approaches have been
proposed corresponding to optimizing diﬀerent metrics. For further reading, Liu [Liu09] has
provided a good survey of the diﬀerent LTR approaches.

100
Graph-Based Social Media Analysis
Another area of focus in web search is on user personalization, i.e., diﬀerent users search-
ing the same query may observe diﬀerent results. Given the diverse set of user needs, mod-
eling the users based on their historical data helps to better customize the results for users.
The web search systems typically collect various user data like queries, clicks, browsing
behavior, etc. Therefore, the challenge lies in fusing these diﬀerent information sources to
provide better personalized results. To introduce personalization into web search, many new
methods and extensions to existing methods have been proposed in the recent years. Dou et
al. have provided in [DSW07] an extensive evaluation of various web search personalization
methods.
The increasing popularity of recommender systems and its similarity to personalized web
search has attracted signiﬁcant attention in recent years. Collaborative Filtering (CF) based
methods, which utilize the preferences from diﬀerent users to compute recommendations,
are currently the most eﬀective method for recommender systems. Matrix factorization
based methods (LSI, RRMF), which were discussed earlier in this chapter, are one of the
most popular tools for CF based tasks. Fusing the CF and the retrieval methods is one of
the active research areas. A major goal of these methods is to personalize the web search
by utilizing preferences from other users [WWWB12].
Bibliography
[Abd07]
H. Abdi. The Kendall Rank Correlation Coeﬃcient. In Encyclopedia of Mea-
surement and Statistics. 2007.
[AECC08]
J. Arguello, J. L. Elsas, J. Callan, and J. G. Carbonell. Document representa-
tion and query expansion models for blog recommendation. Proc. International
Conference Weblogs and Social Media, 2008(0):1, 2008.
[AGC08]
I. Antonellis, H. Garcia-Molina, and C. Chang. SimRank++: query rewriting
through link analysis of the click graph. Proc. of the Very Large Data Bases
Endowment, 1(1):408–421, 2008.
[BB00]
D. Beeferman and A. Berger.
Agglomerative clustering of a search engine
query log. In Proc. ACM International Conference on Knowledge Discovery
and Data Mining, pages 407–416, 2000.
[BGMZ97]
A. Broder, S. Glassman, M. Manasse, and G. Zweig. Syntactic clustering of
the web. In Proc. ACM Conference on World Wide Web, 1997.
[BH98]
K. Bharat and M. R. Henzinger. Improved algorithms for topic distillation in
hyperlinked environments. In Proc. of the ACM Conference on Research and
Development in Information Retrieval, pages 104–111, 1998.
[BHP04]
A. Balmin, V. Hristidis, and Y. Papakonstantinou. ObjectRank: Authority-
Based Keyword Search in Databases. In Proc. International Conference on
Very Large Data Bases, pages 564–575, 2004.
[BL01]
P. Bonacich and P. Lloyd. Eigenvector-like measures of centrality for asym-
metric relations. Social Networks, 23:191–201, 2001.
[BNJ03]
D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet Allocation. Journal
of Machine Learning Research, 3(4–5):993–1022, 2003.

Web Search Based on Ranking
101
[Bon72]
P. Bonacich. Factoring and weighing approaches to status scores and clique
identiﬁcation. Journal of Mathematical Sociology, 2:113–120, 1972.
[BP98]
S. Brin and L. Page. The anatomy of a large-scale hypertextual Web search
engine. Computer Networks, 30(1-7):107–117, 1998.
[Bra01]
U. Brandes. A faster algorithm for betweenness centrality. Journal of Mathe-
matical Sociology, 25(2):163–177, 2001.
[Bra08a]
R. B. Bradford. An empirical study of required dimensionality for large-scale
latent semantic indexing applications. In Proc. ACM Conference on Informa-
tion and Knowledge Management, pages 153–162, 2008.
[Bra08b]
U. Brandes. On variants of shortest-path betweenness centrality and their
generic computation. Social Networks, 30(2):136–145, 2008.
[BRRT01]
A. Borodin, G. O. Roberts, J. S. Rosenthal, and P. Tsaparas. Finding author-
ities and hubs from link structures on the World Wide Web. In Proc. of the
ACM Conference on World Wide Web, pages 415–429, 2001.
[BSWZ03]
B. Billerbeck, F. Scholer, H. E. Williams, and J. Zobel. Query expansion using
associated queries. In Proc. ACM Conference on Information and Knowledge
Management, pages 2–9, 2003.
[BV04]
C. Buckley and E. M. Voorhees. Retrieval evaluation with incomplete informa-
tion. In Proc. ACM Conference on Research and Development in Information
Retrieval, pages 25–32, 2004.
[BYBRN99] R. Baeza-Yates and B. B. Ribeiro-Neto. Modern information retrieval, volume
463. ACM press New York, 1999.
[CDR+98]
S. Chakrabarti, B. Dom, P. Raghavan, S. Rajagopalan, D. Gibson, and J. M.
Kleinberg. Automatic resource compilation by analyzing hyperlink structure
and associated text. In Proc. of the ACM Conference on World Wide Web,
pages 65–74, 1998.
[CFN07]
P.-A. Chirita, C. S. Firan, and W. Nejdl. Personalized query expansion for the
web. In Proc. ACM Conference on Research and Development in Information
Retrieval, pages 7–14, 2007.
[CJP+08]
H. Cao, D. Jiang, J. Pei, Q. He, Z. Liao, E. Chen, and H. Li. Context-aware
query suggestion by mining click-through and session data. In Proc. ACM
International Conference on Knowledge Discovery and Data Mining, pages
875–883. ACM, 2008.
[CR12]
C. Carpineto and G. Romano.
A survey of automatic query expansion in
information retrieval. ACM Computing Surveys, 44(1):1, 2012.
[CWNM03]
H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. Ma. Query expansion by mining user
logs. IEEE Knowledge and Data Engineering, 15(4):829–839, 2003.
[DDL+90]
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A.
Harshman. Indexing by latent semantic analysis. Journal of the American
Society for Information Science, 41(6):391–407, 1990.

102
Graph-Based Social Media Analysis
[DHLK12]
H. Deng, J. Han, M. R. Lyu, and I. King. Modeling and exploiting hetero-
geneous bibliographic networks for expertise ranking. In Proc. International
Joint Conference on Digital Libraries, pages 71–80, 2012.
[DLC11]
D. A. Davis, R. Lichtenwalter, and N. V. Chawla. Multi-relational link predic-
tion in heterogeneous information networks. In Proc. International Conference
on Advances in Social Networks Analysis and Mining, pages 281–288, 2011.
[DLC+15]
L. Du, C. Li, H. Chen, L. Tan, and Y. Zhang. Probabilistic SimRank compu-
tation over uncertain graphs. Information Sciences, 295:521–535, 2015.
[DSW07]
Z. Dou, R. Song, and J.-R. Wen.
A large-scale evaluation and analysis of
personalized search strategies.
In Proc. ACM Conference on World Wide
Web, pages 581–590, 2007.
[EM03]
N. Eiron and K. S. McCurley. Analysis of anchor text for web search. In
Proc. ACM Conference on Research and Development in Information Re-
trieval, pages 459–460, 2003.
[ER90]
L. Egghe and R. Rousseau. Introduction to Informetrics. Elsevier Science
Publishers. Amsterdam, The Netherlands, 1990.
[Fel98]
C. Fellbaum, editor. WordNet: An Electronic Lexical Database. Cambridge,
MA: MIT Press, 1998.
[FKS03]
R. Fagin, R. Kumar, and D. Sivakumar.
Comparing Top k Lists.
SIAM
Journal on Discrete Mathematics, 17(1):134–160, 2003.
[FR05]
D. Fogaras and B. Racz. Scaling link-based similarity search. In Proc. ACM
Conference on World Wide Web, pages 641–650, 2005.
[Fre77]
L. C. Freeman. A set of measures of centrality based on betweenness. Sociom-
etry, 40:35–41, 1977.
[Fre79]
L. C. Freeman. Centrality in social networks conceptual clariﬁcation. Social
Networks, 1(3):215–239, 1979.
[GGMP04]
Z. Gy¨ongyi, H. Garcia-Molina, and J. O. Pedersen. Combating Web Spam
with TrustRank. In Proc. International Conference on Very Large Data Bases,
pages 576–587, 2004.
[GMG11]
S. D. Gollapalli, P. Mitra, and C. L. Giles. Ranking authors in digital libraries.
In Proc. International Joint Conference on Digital Libraries, pages 251–254,
2011.
[HCO03]
C.-K. Huang, L.-F. Chien, and Y.-J. Oyang. Relevant term suggestion in in-
teractive web search based on contextual information in query session logs.
Journal of the American Society for Information Science and Technology,
54(7):638–649, 2003.
[HE09]
J. Huang and E. N. Efthimiadis. Analyzing and evaluating query reformulation
strategies in web search logs. In Proc. ACM Conference on Information and
Knowledge Management, pages 77–86, 2009.
[Hot33]
H. Hotelling. Analysis of a complex of statistical variables into principal com-
ponents. Journal of Educational Psychology, 24:417–441, 1933.

Web Search Based on Ranking
103
[HSG10]
S. E. Helou, C. Salzmann, and D. Gillet. The 3A Personalized, Contextual
and Relation-Based Recommender System. J. UCS, 16(16):2179–2195, 2010.
[IT14]
R. Interdonato and A. Tagarelli. Multi-Relational PageRank for Tree Structure
Sense Ranking. World Wide Web Journal, 2014.
[JK02]
K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of IR tech-
niques. ACM Tsansactions on Information Systems, 20(4):422–446, 2002.
[Jon71]
K. S. Jones. Automatic keyword classiﬁcation for information retrieval. Ar-
chon Books, 1971.
[Jon72]
K. S. Jones. A statistical interpretation of term speciﬁcity and its application
in retrieval. Journal of Documentation, 28:11–21, 1972.
[JRMG06]
R. Jones, B. Rey, O. Madani, and W. Greiner. Generating query substitutions.
In Proc. ACM Conference on World Wide Web, pages 387–396, 2006.
[JW02]
G. Jeh and J. Widom. SimRank: a measure of structural-context similarity.
In Proc. ACM International Conference on Knowledge Discovery and Data
Mining, pages 538–543, 2002.
[Kat53]
L. Katz. A new status index derived from sociometric analysis. Psychometrika,
18(1):39–43, 1953.
[KB01]
D. Kelly and N. J. Belkin. Reading time, scrolling and interaction: exploring
implicit sources of user preferences for relevance feedback. In Proc. ACM Con-
ference on Research and Development in Information Retrieval, pages 408–409,
2001.
[Kle98]
J. M. Kleinberg. Authoritative sources in a hyperlinked environment. In Proc.
of the ACM-SIAM Symposium on Discrete Algorithms, pages 668–677, 1998.
[Kle99]
J. M. Kleinberg. Authoritative sources in a hyperlinked environment. Journal
of ACM, 46(5):604–632, 1999.
[KZ04]
R. Kraft and J. Zien. Mining anchor text for query reﬁnement. In Proc. ACM
Conference on World Wide Web, pages 666–674, 2004.
[Lar96]
R. R. Larson. Bibliometrics of the world wide web: An exploratory analysis of
the intellectual structure of cyberspace. In Proc. Annual Meeting American
Society for Information Science, volume 33, pages 71–78, 1996.
[LC10]
N. Lao and W. W. Cohen. Relational retrieval using a combination of path-
constrained random walks. Machine Learning, 81(1):53–67, 2010.
[Liu09]
T.-Y. Liu. Learning to rank for information retrieval. Foundations and Trends
in Information Retrieval, 3(3):225–331, 2009.
[LK07]
D. Liben-Nowell and J. M. Kleinberg. The link-prediction problem for so-
cial networks. Journal of the American Society for Information Science and
Technology, 58(7):1019–1031, 2007.
[LM00]
R. Lempel and S. Moran. The stochastic approach for link-structure analysis
(SALSA) and the TKC eﬀect. Computer Networks, 33(1-6):387–401, 2000.

104
Graph-Based Social Media Analysis
[LM05]
A. N. Langville and C. D. Meyer. Deeper inside PageRank. Internet Mathe-
matics, 1(3):335–400, 2005.
[LSK+11]
S. Lee, S. Song, M. Kahng, D. Lee, and S. Lee.
Random walk based en-
tity ranking on graph for multidimensional recommendation. In Proc. ACM
Conference on Recommender Systems, pages 93–100, 2011.
[LVGT08]
D. Lizorkin, P. Velikhov, M. N. Grinev, and D. Turdakov. Accuracy estimate
and optimization techniques for SimRank computation.
Proc. of the Very
Large Data Bases Endowment, 1(1):422–433, 2008.
[LY09]
W.-J. Li and D.-Y. Yeung.
Relation regularized matrix factorization.
In
Proc. International Joint Conference on Artiﬁcial Intelligence, pages 1126–
1131, 2009.
[McB94]
O. A. McBryan. GENVL and WWWW: Tools for taming the web. In Proc.
ACM Conference on World Wide Web, volume 341. Geneva, 1994.
[Mey00]
C. D. Meyer. Matrix Analysis and Applied Linear Algebra. SIAM, 2000.
[MRS08]
C. D. Manning, P. Raghavan, and H. Sch¨utze. Introduction to information
retrieval. Cambridge University Press, 2008.
[MYKL08]
H. Ma, H. Yang, I. King, and M. R. Lyu. Learning latent semantic relations
from clickthrough data for query suggestion. In Proc. ACM Conference on
Information and Knowledge Management, pages 709–718, 2008.
[NG04]
M. E. J. Newman and M. Girvan. Finding and evaluating community structure
in networks. Physical Review E, 69, 026113, 2004.
[NZWM05]
Z. Nie, Y. Zhang, J.-R. Wen, and W.-Y. Ma. Object-level ranking: bringing
order to Web objects. In Proc. ACM Conference on World Wide Web, pages
567–574, 2005.
[PE97]
M. Perkowitz and O. Etzioni. Adaptive web sites: an AI challenge. In Proc.
International Joint Conference on Artiﬁcial Intelligence, 1997.
[PE99]
M. Perkowitz and O. Etzioni. Towards adaptive web sites: conceptual frame-
work and case study. In Proc. ACM Conference on World Wide Web, 1999.
[PP97]
J. Pitkow and P. Pirolli. Life, death and lawfulness on the electronic frontier.
In Proc. ACM SIGCHI Conference on Human Factors in Computing Systems,
1997.
[PPR96]
P. Pirolli, J. Pitkow, and R. Rao. Silk from a sow’s ear: Extracting usable
structures from the web. In Proc. ACM SIGCHI Conference on Human Fac-
tors in Computing Systems, pages 118–125, 1996.
[Pre02]
L. Pretto. A theoretical analysis of PageRank. In Proc. of the International
Symposium on String Processing and Information Retrieval, pages 131–144,
2002.
[Roc71]
J. J. Rocchio.
Relevance feedback in information retrieval.
Prentice-Hall,
Englewood Cliﬀs NJ, 1971.
[RU11]
A. Rajaraman and J. D. Ullman. Mining of Massive Datasets. Cambridge
University Press, 2011.

Web Search Based on Ranking
105
[RVT+07]
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and Y. Liu. Statistical
machine translation for query expansion in answer retrieval. In Proc. Annual
Meeting of the Association for Computational Linguistics, volume 45, page
464, 2007.
[Sab66]
G. Sabidussi. The centrality index of a graph. Psychometrika, 31:581–603,
1966.
[SB88]
G. Salton and C. Buckley. Term-weighting approaches in automatic text re-
trieval. Information Processing and Management, 24(5):513–523, 1988.
[SB97]
G. Salton and C. Buckley. Improving retrieval performance by relevance feed-
back. Readings in information retrieval, 24(5), 1997.
[See49]
J. R. Seeley. The net of reciprocal inﬂuence: a problem in treating sociometric
data. Canadian Journal of Psychology, 3:234–240, 1949.
[SH12]
Y. Sun and J. Han.
Mining Heterogeneous Information Networks: Princi-
ples and Methodologies. Synthesis Lectures on Data Mining and Knowledge
Discovery. Morgan & Claypool Publishers, 2012.
[SHY+11]
Y. Sun, J. Han, X. Yan, P. S. Yu, and T. Wu. PathSim: Meta Path-Based
Top-K Similarity Search in Heterogeneous Information Networks. Proceedings
of the VLDB Endowment, 4(11):992–1003, 2011.
[SHZ+09]
Y. Sun, J. Han, P. Zhao, Z. Yin, H. Cheng, and T. Wu. RankClus: integrating
clustering with ranking for heterogeneous information network analysis. In
Proc. International Conference on Extending Database Technology, pages 565–
576, 2009.
[SYH09]
Y. Sun, Y. Yu, and J. Han. Ranking-based clustering of heterogeneous in-
formation networks with star network schema. In Proc. ACM International
Conference on Knowledge Discovery and Data Mining, pages 797–806, 2009.
[SZ89]
K. Stephenson and M. Zelen. Rethinking centrality: Methods and applications.
Social Networks, 11:1–37, 1989.
[TSK06]
P. N. Tan, M. Steinbach, and V. Kumar. Introduction to Data Mining. Pearson
Addison-Wesley Boston, 2006.
[VNL+11]
V. Vasuki, N. Natarajan, Z. Lu, B. Savas, and I. S. Dhillon. Scalable Aﬃliation
Recommendation using Auxiliary Networks. ACM Tsansactions on Intelligent
Systems and Technology, 3(1):3, 2011.
[WF94]
S. Wasserman and K. Faust. Social Network Analysis: Methods and Applica-
tions. Cambridge University Press, 1994.
[WLJH10]
J. Weng, E. P. Lim, J. Jiang, and Q. He. TwitterRank: Finding Topic-Sensitive
Inﬂuential Twitterers. In Proc. ACM Conference on Web Search and Web
Data Mining, pages 261–270, 2010.
[WVS96]
R. Weiss, B. V´elez, and M. A. Sheldon. Hypursuit: A hierarchical network
search engine that exploits content-link hypertext clustering. In Proc. ACM
Conference on Hypertext, pages 180–193, 1996.
[WWWB12] J. Weston, C. Wang, R. Weiss, and A. Berenzweig. Latent collaborative re-
trieval. arXiv preprint arXiv:1206.4603, 2012.

106
Graph-Based Social Media Analysis
[WXLC11]
Q. Wang, J. Xu, H. Li, and N. Craswell. Regularized latent semantic index-
ing. In Proc. ACM Conference on Research and Development in Information
Retrieval, pages 685–694, 2011.
[XC96]
J. Xu and W. B. Croft. Query expansion using local and global document anal-
ysis. In Proc. ACM Conference on Research and Development in Information
Retrieval, pages 4–11, 1996.
[XJW09]
Y. Xu, G. J. F. Jones, and B. Wang.
Query dependent pseudo-relevance
feedback based on Wikipedia. In Proc. ACM Conference on Research and
Development in Information Retrieval, pages 59–66, 2009.
[XZC+04]
G.-R. Xue, H.-J. Zeng, Z. Chen, Y. Yu, W.-Y. Ma, W. S. Xi, and W. G.
Fan. Optimizing web search using web click-through data. In Proc. ACM
Conference on Information and Knowledge Management, pages 118–126, 2004.
[YLZ13]
W. Yu, X. Lin, and W. Zhang. Towards eﬃcient SimRank computation on
large networks. In Proc. International Conference on Data Engineering, pages
601–612, 2013.
[YSC09]
Z. Yin, M. Shokouhi, and N. Craswell. Query expansion using external evi-
dence. In Advances in Information Retrieval, pages 362–374. Springer, 2009.
[ZE98]
O. Zamir and O. Etzioni. Web document clustering: A feasibility demonstra-
tion. In Proc. ACM Conference on Research and Development in Information
Retrieval, pages 46–54, 1998.
[ZFT+11]
M. Zhang, S. Feng, J. Tang, B. A. Ojokoh, and G. Liu. Co-Ranking Multiple
Entities in a Heterogeneous Network: Integrating Temporal Factor and Users’
Bookmarks.
In Proc. International Conference on Asian Digital Libraries,
pages 202–211, 2011.
[ZHMM04]
H.-J. Zeng, Q.-C. He, Z. C. W.-Y. Ma, and J. Ma. Learning to cluster web
search results. In Proc. ACM Conference on Research and Development in
Information Retrieval, pages 210–217, 2004.

Chapter 5
Label Propagation and Information
Diﬀusion in Graphs
Eftychia Fotiadou, Olga Zoidi and Ioannis Pitas
Aristotle University of Thessaloniki, Greece
5.1
Introduction ....................................................................
108
5.2
Graph construction approaches ................................................
109
5.2.1
Neighborhood approaches .............................................
110
5.2.2
Local reconstruction approaches ......................................
111
5.2.3
Metric learning approaches ............................................
113
5.2.4
Scalable graph construction methods .................................
118
5.3
Label inference methods .......................................................
120
5.3.1
Iterative algorithms ....................................................
120
5.3.2
Random walks .........................................................
122
5.3.3
Graph regularization ..................................................
123
5.3.4
Graph kernel regularization ...........................................
127
5.3.5
Inductive label inference ...............................................
128
5.3.6
Label propagation on data with multiple representations ............
129
5.3.7
Label propagation on hypergraphs ....................................
131
5.3.8
Label propagation initialization .......................................
132
5.3.9
Applications in digital media ..........................................
133
5.4
Diﬀusion processes .............................................................
134
5.4.1
Diﬀusion in physics ....................................................
134
5.4.2
Diﬀusion in sociology ..................................................
135
5.4.3
Diﬀusion in social media ..............................................
135
5.5
Social network diﬀusion models ................................................
136
5.5.1
Game theoretical diﬀusion models ....................................
137
5.5.2
Epidemic diﬀusion models .............................................
137
5.5.3
Threshold diﬀusion models ............................................
138
5.5.4
Cascade diﬀusion models ..............................................
139
5.5.5
Inﬂuence maximization ................................................
140
5.5.6
Cross-Media information diﬀusion ....................................
142
5.5.7
Other applications of information diﬀusion ...........................
143
5.6
Conclusions .....................................................................
145
Bibliography ....................................................................
146
107

108
Graph-Based Social Media Analysis
5.1
Introduction
The present day is characterized by the ability to directly access a huge volume of any
kind of information, especially through the Internet. Furthermore, an expansion of the on-
line multimedia sharing communities and social network services, which enable users to
interact with each other, as well as to upload, create and share multimedia content, such
as images, videos and audio, has been observed in the last few years. This has led to an
enormous increase in the volume of the on-line available multimedia data. The eﬀective
handling of information at such scales requires the development of special algorithms.
Multimedia objects, e.g. videos, images, and music, can be characterized (labeled) by
labels (tags) that are assigned to them and describe their semantic content. This process, re-
ferred to as annotation (or labeling), is necessary for semantic search in multimedia or social
media databases. In large data collections, such as the on-line multimedia sharing websites
and social networks, the tags are usually user-contributed. However, manually annotating
multimedia objects becomes infeasible as the volume of the data grows. Additionally, in
many cases, users may provide incorrect or incomplete tags. Furthermore, expert domain
knowledge may be required for assigning proper tags. For the aforementioned reasons, semi-
automatic labeling of a large number of multimedia data items is of great importance.
To this end, one solution can be label propagation, whose objective is to disseminate the
labels of a small set of annotated data to a larger set of unlabeled data. Label propagation
algorithms make two common assumptions: ﬁrst, the labels of the initially labeled data
should remain unchanged. Second, data samples that are similar (or “close”) to each other
or lie in the same structure (e.g., cluster or manifold) should be assigned the same label. In
order to describe the label propagation process, a graph-based approach is usually adopted.
Graph nodes correspond to data samples, while graph edges reﬂect their relationships.
For example, edge weights may represent pairwise similarities (or distances) between the
data samples. Label inference can be subsequently performed through graph paths that
connect labeled to unlabeled nodes. The graph construction method, namely the adopted
pairwise similarity/distance function, is crucial to the label propagation performance, since
it regulates the way the labels are spread through the graph paths. Figure 5.1.1a depicts
the graph of a recommendation network, which models how the recommendation of an idea
or a product is propagated through a network of individuals, represented by graph nodes.
The graph of Figure 5.1.1b represents a network of “related” videos from the YouTube
video-sharing website, where each video is related to other relevant videos. In this case,
graph nodes correspond to videos, while graph edges connect a video with its related videos.
Label propagation algorithms belong to the more general class of semi-supervised classiﬁers,
since they exploit information from unlabeled data during the learning process, instead of
relying merely on labeled data for training, as is the case in supervised learning. As a
result, semi-supervised classiﬁers can provide a better understanding of the data structure
and lead to improved classiﬁcation performance in comparison to supervised classiﬁers.
Transductive semi-supervised classiﬁers learn a local representation of the data feature
space and can be applied in a speciﬁc dataset consisting of labeled and unlabeled data,
while inductive classiﬁers learn a global representation of the data feature space and can
be applied to “unseen” data that were not included in the original dataset. The majority
of label propagation algorithms are transductive classiﬁers.
Label propagation can be thought of as an information (label) diﬀusion process over
the data graph. In general, information diﬀusion studies how ideas are spread through a
network, usually a social system. While label propagation is classiﬁcation problem, infor-
mation diﬀusion in social networks focuses on diﬀerent issues, such as the mechanisms

Label Propagation and Information Diﬀusion in Graphs
109
(a)
(b)
FIGURE 5.1.1: a) Recommendation network graph, b) YouTube video relation graph. (See
color insert.)
governing the diﬀusion process, the inﬂuence of the network structure on the diﬀusion, or
the maximization of the information spread.
In order to study the information diﬀusion within a social network, various types of
models can be adopted, such as epidemic, game theoretical, threshold and cascade models.
Information diﬀusion methods are commonly used in viral marketing applications or, in
general, when maximizing the information spread is desirable. They are also employed in
collaborative ﬁltering systems, in community detection algorithms, as well as in the study
of citation networks. Finally, information diﬀusion methods ﬁnd application in emergency
management, where providing the public with useful and valid information is crucial. Apart
from studying information ﬂow in social networks, label propagation methods ﬁnd numerous
applications in multimedia content annotation [TYH+09], medical imaging [HHA+06], biol-
ogy [LK03, WLI+05, HK10] and language analysis [NJT05, YJZ+06, ZK09, RR09, SSUB11].
The current chapter focuses on the study of graph-based label propagation algorithms,
also referred to as graph-based semi-supervised learning (SSL). Furthermore, diﬀusion con-
cepts, with a focus on social networks, are discussed. The label propagation procedure con-
sists of two steps: a) the construction of a graph representing the data and the relationships
between them and b) label inference, based on the constructed graph. Graph construction
methods suitable for label propagation, are discussed in Section 5.2, while label inference
algorithms are presented in Section 5.3. In Section 5.4, the notions of diﬀusion in physics as
well as in social networks are brieﬂy reviewed. Moreover, in Section 5.5, models for diﬀusion
in social networks, as well as the problem of inﬂuence maximization are discussed.
5.2
Graph construction approaches
In graph-based label propagation, the labels are diﬀused through a graph, where nodes
correspond to entities, such as multimedia objects, while the edge weights represent their
similarity. The similarity graph should reﬂect the relationships between the entities being
labeled, with respect to the speciﬁc label propagation task. For this reason, the construc-
tion of the similarity graph is critical to label propagation performance. In the following

110
Graph-Based Social Media Analysis
(a)
(b)
FIGURE 5.2.1: (a) 4-NN neighborhood, (b) e-neighborhood.
subsections, various graph construction methods are discussed. These methods are divided
into three categories: neighborhood, local reconstruction and metric learning ones.
5.2.1
Neighborhood approaches
According to neighborhood methods, the graph is constructed by connecting each node to
its closest ones, where “closeness” between nodes is determined by a distance or similarity
function. Neighborhood methods construct sparse graphs. Two often utilized neighborhood
graphs are the k-Nearest Neighbor (k-NN) and the e-neighborhood graphs [Tal09], which
are illustrated in Figure 5.2.1.
In k-NN graphs, each node is connected to its k nearest neighbors. k-NN graphs are
asymmetric, since the participation of a node j to the set of i-th node closest neighbors
does not guarantee the participation of node i to the set of closest neighbors of node j.
Furthermore, graph construction with the k-NN method produces irregular graphs, since
certain nodes end up with a higher degree than others. Irregular graphs may aﬀect label
propagation negatively, leading to degenerate solutions, as most nodes may be assigned
the same label to those of the high degree nodes. In several label propagation methods,
[BSS+08, TC09], this problem is dealt with by discounting the importance of high degree
nodes.
In e-neighborhood graphs, a node is connected to all the nodes within a predeﬁned
distance e > 0. Such a graph construction is quite sensitive to parameter e selection. Ad-
ditionally, it often leads to graphs having disconnected components. For the above reasons,
the k-NN method shows advantages over the e-neighborhood one and is usually preferred
in practice.
As already mentioned, with respect to the label propagation task, regular graphs are
preferred. As opposed to k-NN and e-neighborhood graphs, the b-matching based method
proposed in [JWC09] guarantees resulting graph regularity. The b-matching method involves
two steps:
• Graph sparsiﬁcation: during this step, starting with a complete graph, the edges are
selected that will be present in the ﬁnal, sparse graph.

Label Propagation and Information Diﬀusion in Graphs
111
• Edge re-weighting: weights are learned for the edges that were selected in the previous
step.
The sparsiﬁcation step generates a binary matrix P ∈{0, 1}N×N, whose entries Pij deter-
mine whether an edge will be present between data samples xi and xj in the ﬁnal matrix
(Pij = 1) or not (Pij = 0). The calculation of matrix P is performed by minimizing the
following objective function:
min
P
X
ij
PijDij,
(5.2.1)
subject to (s.t. in short):
X
j
Pij = b, Pii = 0, Pij = Pji, ∀i, j = 1, . . . , N.
(5.2.2)
The distance matrix D is calculated from Dij =
p
Wii + Wjj −2Wij, where W denotes
the weight matrix. During edge re-weighting, the weights for the selected edges are learned,
using three diﬀerent weighting schemes: a) binary weights (W = P), b) Gaussian kernel
weights:
Wij = Pij exp

−d(xi, xj)
2σ2

,
(5.2.3)
where d(xi, xj) is some distance function between the node feature vectors xi, xj and σ is
the kernel bandwidth or, c) Locally Linear Reconstruction (LLR), motivated by the Locally
Linear Embedding (LLE) algorithm [RS00], which seeks the coeﬃcients that minimize the
reconstruction error:
min
W
X
i
∥xi −
X
j
PijWijxj∥2,
(5.2.4)
s.t.
X
j
Wij = 1, Wij ≥0.
(5.2.5)
5.2.2
Local reconstruction approaches
In contrast to the aforementioned methods, that utilize pairwise distances between nodes
to construct the similarity graph, local reconstruction methods take neighborhood informa-
tion into consideration, by expressing each node as a combination of its neighboring nodes.
In the Linear Neighborhood Propagation (LNP) method introduced in [WZ06], the neigh-
borhood of each node is regarded as a linear patch and each node is reconstructed by a
linear combination of its k nearest neighbors. The whole graph is, therefore, approximated
by a number of overlapping linear patches. The edge weights in each patch are determined
using a quadratic programming solver. The objective of the LNP method is to minimize the
reconstruction error of each node by its k nearest neighbors:
min
W
X
i
∥xi −
X
j:xj∈N(xi)
Wijxj∥2,
(5.2.6)
s.t.
X
j
Wij = 1, Wij ≥0,
(5.2.7)
where N(xi) denotes the neighborhood of node i, while Wij expresses the contribution of
xj to the reconstruction of xi. The values of the reconstruction weights Wij can be regarded

112
Graph-Based Social Media Analysis
as a similarity measure between nodes i and j. The reconstruction error of a node i can
take the form:
ϵi = ∥xi −
X
j:xj∈N(xi)
Wijxj∥2
= ∥
X
j:xj∈N(xi)
Wij(xi −xj)∥2
=
X
j,k:xj,xk∈N(xi)
WijWik(xi −xj)T (xi −xk)
=
X
j,k:xj,xk∈N(xi)
WijGi
jkWik,
(5.2.8)
where Gi
jk = (xi −xj)T (xi −xk) denotes the (j, k)-th entry of the local Gram matrix Gi of
point xi. The Gram matrix contains all the possible inner products between data samples.
Therefore, the reconstruction weights for each node can be calculated by solving a quadratic
programming problem (one for each node) of the form:
min
Wij
X
j,k:xj,xk∈N(xi)
WijGi
jkWik,
(5.2.9)
s.t.
X
j
Wij = 1, Wij ≥0.
(5.2.10)
Once the reconstruction weights are computed, a sparse weight matrix W is constructed,
corresponding to a sparse graph, where each node is connected only to its k nearest neigh-
bors. In [THQ+08], a Kernel Linear Neighborhood Propagation (KLNP) method is proposed,
which is a non-linear extension of the LNP method. Using a kernel mapping of the form
φ : X →Φ, the data are mapped into a high dimensional space and the objective function
in this case is expressed as:
min
Wij ∥φ(xi) −
X
j:φ(xj)∈N(φ(xi))
Wijφ(xj)∥2,
(5.2.11)
(5.2.12)
s.t.
X
j
Wij = 1, Wij ≥0.
(5.2.13)
Another graph construction method based on LNP is the Correlative Linear Neighborhood
Propagation (CLNP) method [THW+09], which addresses the problem of video annotation.
The CLNP method incorporates the semantic correlations among the data labels during
graph construction.
In [DKS09], a hard and an a-soft method for graph construction are proposed. Hard
graphs are deﬁned as those where each node has a weighted degree di of at least 1, with
di = P
j Wij. The hard graph construction method introduced in [DKS09] minimizes the
objective function:
min
W
X
i
∥dixi −
X
j
Wijxj∥2.
(5.2.14)
The a-soft graph construction allows the weighted degrees of some outlier nodes to take a
lower value. This is achieved by relaxing the constraint on the weighted degree d, which is
now expressed by:
X
i
(max(0, 1 −di))2 ≤aN,
(5.2.15)

Label Propagation and Information Diﬀusion in Graphs
113
where a is a hyper-parameter.
In [TYH+09], a graph construction method is introduced as part of an image annotation
framework. The proposed method, which is motivated by the study [WYG+09] is robust
to noise. More importantly, it constrains the images used to reconstruct a sample image to
be semantically similar to the sample image. Speciﬁcally, an one-vs-all sparse reconstruc-
tion of each sample, based on the l1-norm minimization is proposed. Each sample xi is
reconstructed by solving the following minimization problem:
min
wi ∥wi∥1,
(5.2.16)
s.t.
xi = Biwi,
(5.2.17)
where Bi is the matrix formed by all samples except xi and wi is the vector of the recon-
struction coeﬃcients. Subsequently, the edge weight from sample j to sample i is determined
by:
wij =



wi(j),
if j < i
wi(j −1),
if j > i
0 ,
if j = i ,
(5.2.18)
where wi(j) is the j-th element of vector wi. The aforementioned process removes a signiﬁ-
cant number of edges between nodes that are semantically unrelated, rendering the resulting
graph more eﬃcient for label propagation. In a later work [THY+11], a one-vs-kNN sparse
graph construction approach is proposed, where each sample is reconstructed from its k
nearest neighbors, instead of using all of the remaining dataset samples. The reconstruction
is calculated as in equation (5.2.16), with the diﬀerence that, only the k nearest neighbors
of sample i, denoted by ip, p ∈{1, 2, ..., k}, are used in the matrix Bi. The weights are
selected using the following rule:
wij =
 wi(p),
if xj ∈N(xi) and j = ip
0,
if xj /∈N(xi),
(5.2.19)
where wi(p) is the p-th element of vector wi.
The method introduced in [WWZ+09] extends the NLP algorithm to hypergraphs, where
hyperedges connect two or more nodes, thus simultaneously representing multiple relation-
ships between nodes. Given the graph G = (V, E) with nodes V and edges E, the hyper-
graph G′ = (V, E′) is constructed, where V is the set of nodes of graph G and E′ is the
set of hyperedges. If graph G is cast into a ﬁrst-order Intrinsic Gaussian Markov Random
Field (IGMRF) framework [RH05], then the hypergraph G′ can be cast into a second-order
IGMRF framework, where the increment for hyperedge e′
i is calculated by:
di = yi −
X
j∈Ni
Wijyj,
X
j∈Ni
Wij = 1,
(5.2.20)
where yi is the label of node i and Ni is the set of neighboring nodes to node i. The hyper-
edges’ weights Wij are calculated by solving the quadratic problem expressed by (5.2.6).
5.2.3
Metric learning approaches
Graph construction methods involve the selection of a distance metric, which is used to
estimate the similarity between samples (nodes). The choice of the distance metric strongly
aﬀects the resulting graph and, consequently, the performance of the label propagation
process. When no further information about the samples is available, graph construction
algorithms usually employ the Euclidean distance to calculate the distances between nodes.

114
Graph-Based Social Media Analysis
(a)
(b)
FIGURE 5.2.2: Equidistant points from the origin 0 for (a) Mahalanobis distance (A ̸= I)
and (b) Euclidean distance (A = I).
In this case, the metric is not able to capture the underlying structure (e.g., data clusters
or manifolds) that may exist in the data. On the other hand, if prior knowledge regarding
the similarity between data samples is available (labeled data or clustering information
for example), it can be incorporated in the distance metric. In general, metric learning
algorithms calculate a Mahalanobis distance between the samples, expressed by:
dA(xi, xj) =
q
(xi −xj)T A(xi −xj),
(5.2.21)
where A ∈RN×N is a positive semi-deﬁnite matrix, that incorporates the constraints
derived from the prior knowledge on the data. By setting the matrix A equal to the identity
matrix IN ∈RN×N, the distance expressed in equation (5.2.21) coincides with the Euclidean
distance, as illustrated in Figure 5.2.2. The constraints imposed by matrix A may refer to
pairs of similar samples (e.g., when the distance between two similar samples must be smaller
than a threshold) or to dissimilar ones (e.g., when the distance between dissimilar samples
must be greater than a threshold). The graph weights Wij are subsequently calculated
according to a function of the form:
Wij ∝exp{−dA(xi, xj)}.
(5.2.22)
The objective of metric learning algorithms is to minimize a cost function f(A) subject to
a set of constraints.
The method introduced in [XNJR02] minimizes the squared sum of distances between
similar samples, under the constraint that the sum of distances between dissimilar data
samples is greater than a threshold. In [SSSN04], an on-line distance metric learning algo-
rithm is proposed, where each sample is constrained to be closer to all samples with the
same label than to any sample with a diﬀerent label.
Information-Theoretic Metric Learning (ITML) [DKJ+07] assumes that prior knowl-
edge of the distances between the data samples is available, denoted by the Mahalanobis
distance parameter matrix A0. It considers similarity and dissimilarity constraints between

Label Propagation and Information Diﬀusion in Graphs
115
pairs of data samples, such that, two data samples xi, xj are similar, if their Mahalanobis
distance is below an upper limit dA(xi, xj) ≤u and dissimilar, if it exceeds a lower limit
dA(xi, xj) ≥l. The objective is to ﬁnd a Mahalanobis distance function parametrized by the
matrix A, which has to be as close as possible to the prior Mahalanobis distance function,
parameterized by A0.
To measure the distance between two Mahalanobis functions, an information-theoretic
approach can be followed. There exists a one-to-one correspondence between the two Ma-
halanobis distance functions and the set of equal-mean multivariate Gaussian distributions.
Taking this bijection into account, the distance between two Mahalanobis distance func-
tions parametrized by A and A0 can be expressed as the Kullback-Leibler (KL) divergence
between the corresponding Gaussians p(x; A) and p(x; A0):
KL(p(x; A0)||p(x; A)) =
Z
p(x; A0) log p(x; A0)
p(x; A) dx.
(5.2.23)
The aforementioned relative entropy is shown to be equivalent to the Log-Determinant
(Log-Det) divergence between the matrices A and A0, which is given by:
Dld(A, A0) = tr(AA−1
0 ) −log det(AA−1
0 ) −N.
(5.2.24)
The metric learning problem takes the form of the following minimization problem:
min
A KL(p(x; A0)||p(x; A) = min
A
1
2Dld(A, A0),
s.t. dA(xi, xj) ≤u (i, j) ∈S
dA(xi, xj) ≥l (i, j) ∈D,
(5.2.25)
where S and D denote the sets of similar and dissimilar data sample pairs, respectively. A
later study [DD08] deals with the problem of tractability of Mahalanobis distance learning,
as data dimensionality grows. Two new algorithms are proposed, based on the Log-Det
divergence, which are suitable for learning Mahalanobis distance functions in high dimen-
sions.
In [GRHS04], a learning method called Neighborhood Component Analysis (NCA) is
developed, under a k-Nearest Neighbor (k-NN) classiﬁcation perspective. It seeks a linear
transformation of the feature space, such that, in the transformed space, the k-NN objective
is satisﬁed, i.e. the k-nearest neighbors belong to the same class. Matrix A takes the form
A = ΦT Φ, where Φ denotes the transformation matrix. The objective of the NCA method
is to maximize the expected number of correctly classiﬁed nodes:
f(Φ) =
X
i
X
j∈Ci
exp
 −∥Φxi −Φxj∥2
2

P
k̸=i exp (−∥Φxi −Φxk∥2
2),
(5.2.26)
where Ci denotes the set of nodes with the same label as node xi. In [QTZ+09], the problem
of high data dimensionality in combination with a small number of available data samples is
addressed, referred to often as the “curse of dimensionality.” Departing from the observation
that the concentration matrix Σ−1 (i.e., the inverse of the covariance matrix Σ) is often
sparse in the high dimensional space, since the correlation among the diﬀerent dimensions is
weak, Sparse Distance Metric Learning (SDML) is proposed, which imposes a sparse prior
on the oﬀ-diagonal elements of the Mahalanobis matrix. The SDML problem is formulated
as:
min
A

tr(A−1
0 A + γXLXT A) −log det(A) + λ||A||1,off
	
,
(5.2.27)
where λ is a trade-oﬀparameter between the sparsity prior and the matrix A0, γ is a

116
Graph-Based Social Media Analysis
FIGURE 5.2.3: Schematic illustration of LMNN (adapted from [WS09]).
positive trade-oﬀparameter, L is the graph Laplacian and ||A||1,off = P
i̸=j |Aij| is the
oﬀ-diagonal l1-norm.
Another approach to learning a Mahalanobis distance metric using the k-NN rule is
introduced in the Large Margin Nearest Neighbor (LMNN) algorithm [WS09]. The objective
of the method is to learn a distance metric, such that the k nearest neighbors of a node xi are
forced to share the same label yi with xi, while diﬀerently labeled nodes are separated by a
large margin. The algorithm, which exhibits parallels to Support Vector Machines (SVMs),
seeks for a linear transformation Φ of the feature space, which fulﬁlls the aforementioned
objective. In more detail, the k-nearest neighbors of a node xi with the same label yi are
deﬁned as the target nodes. Ideally, these nodes should be closer to xi than to any other
diﬀerently labeled node, thus forming a perimeter around node xi, which should not be
violated by diﬀerently labeled nodes. Nodes with a label other than yi trying to enter this
perimeter are called impostors. The goal of the LMNN algorithm is to pull the target nodes
close to each other, while pushing the diﬀerently labeled nodes away from each other. This
is achieved by minimizing the following cost function:
ϵ(Φ) =(1 −µ)
X
j⇝i
||Φ(xi −xj)||2+
µ
X
i,j⇝i
X
l
(1 −yil)

1 + ||Φ(xi −xj)||2 −||Φ(xi −xl)||2
+ ,
(5.2.28)
where the second term [z]+ = max(z, 0) is the hinge loss. The indicator variable yil is equal
to 1, if i and l have the same label and equal to 0, otherwise, while j ⇝i indicates that j
is a target node of i. The ﬁrst term of the cost function penalizes large distances between
each node and its target nodes, while the second one penalizes small distances between each
node and all nodes having diﬀerent labels. Parameter µ takes values in the interval [0, 1]
and controls the trade-oﬀbetween the two terms in equation (5.2.28). It should be noted
that the ﬁrst term of the cost function penalizes large distances only between a data sample
xi and its target nodes, and not among all nodes having the same label as xi. A schematic
illustration of LMNN algorithm is depicted in Figure 5.2.3.
Similarly to [GRHS04] and [WS09], a Support Vector Machine approach to metric learn-
ing, called Metric Learning SVM (MLSVM), was proposed in [NG08].
The distance metric learning method introduced in [BBM04] learns a diﬀerent metric in
each cluster, allowing, thus, diﬀerent shapes for the clusters. The proposed algorithm, called

Label Propagation and Information Diﬀusion in Graphs
117
Metric Pairwise Constrained K-Means (MPCK-Means), assumes sets of pairwise must-link
constraints, i.e. if (xi, xj) ∈S , then, data samples xi and xj must be in the same cluster,
and cannot-link constraints, i.e. if (xi, xj) ∈D, then, xi and xj must be in diﬀerent clusters.
An individual weight matrix Ac is sought in each cluster c, which minimizes the sum of
squared Euclidean distances between the samples of the cluster and the cluster centroids
µc and maximizes the complete data log-likelihood:
arg min
Ac



X
xi∈X
 d2
Ac(xi, µc) −log(det(Ac))

+
X
(xi,xj)∈S
wij1[li ̸= lj]+
X
(xi,xj)∈D
¯wij1[li = lj]


,
(5.2.29)
where X denotes the whole dataset, d2
Ac(xi, µc) is given by equation (5.2.21), li is the cluster
assignment (label) for sample i, and 1 is the indicator function 1[true] = 1 and 1[false] = 0.
The constraints are incorporated in the aforementioned objective function through weights
wij and ¯wij, which express the cost of violating the must-link and cannot-link constraints,
respectively.
In [CTLC12], a Two-Dimensional Smooth Metric Learning (2DSML) framework suitable
for visual analysis is proposed. Departing from the observation that the existing metric
learning algorithms consider image pixels as independent and represent images of size N1 ×
N2 pixels with vectors in the RN1×N2 space, the correlations existing between neighboring
pixels in an image are taken into account and an image is treated as a function deﬁned
on an N1 × N2 grid. The proposed regularization framework incorporates a discretized
Laplacian penalty term, which ensures smoothness in two dimensions and incorporates the
prior information regarding the spatial correlations of neighboring pixels.
A method for Multi-Instance Multi-Label metric learning is proposed in [JSWZHZ09].
Multi-Instance Multi-Label learning refers to problems where each example consists of a
collection (or bag) of instances, e.g. key points, and can be assigned multiple labels. For
example, in an image annotation application, each image may contain various objects be-
longing to various classes. The proposed algorithm involves two steps, which are applied
iteratively: ﬁrst, the relationships between the instances in the bags and the class labels
assigned to the bags are estimated. Subsequently, a distance measure is learned based on
the estimated relationships.
The metric learning algorithms discussed so far are supervised, since they utilize prior
knowledge derived from the labeled data. In [DTC10], a new semi-supervised metric learning
framework, called Inference Driven Metric Learning (IDML), is proposed, which exploits in-
formation from both labeled and unlabeled data. IDML combines supervised metric learning
algorithms, such as the previously discussed ITML and LMNN, with transductive graph-
based label propagation methods. Speciﬁcally, graph-based SSL is used to infer labels for the
unlabeled samples and, subsequently, samples that exhibited high assigned label conﬁdence
during the label inference process are appended to the labeled set in the next iteration of
metric learning. The aforementioned steps are executed iteratively, until no new samples
can be added to the labeled data set. Another semi-supervised metric learning method is
introduced in [HLC08]. The proposed algorithm, called Laplacian Regularized Metric Learn-
ing (LRML), incorporates information from unlabeled data through the graph Laplacian.
Finally, a semi-supervised metric learning algorithm, called Semi-Supervised Sparse Metric
Learning (S3ML), is introduced in [LMT+10], based on Sparse Distance Metric Learning
(SDML) [QTZ+09].

118
Graph-Based Social Media Analysis
In the case that label information is not available for the data, unsupervised metric
learning is equivalent to manifold learning [LV07] (or dimensionality reduction methods
[Fod02]). The objective of these techniques is to learn a low-dimensional manifold from the
data, such that the geometric relationships between data samples are preserved [YR06].
Commonly used unsupervised dimensionality reduction methods include Principal Compo-
nent Analysis (PCA) [Jol02], Multidimensional Scaling (MDS) [BG05], Locally Linear Em-
bedding (LLE) [SR03], the Laplacian Eigenmap [BN03] and ISOMAP [TDSL00]. In [YR06],
an extensive study on distance metric learning, as well as its relationship to dimensionality
reduction is provided.
5.2.4
Scalable graph construction methods
A drawback common in the graph construction methods reviewed so far is that they
are not scalable, i.e., they are not capable of handling large graphs. As the size of the
dataset grows, their computational time becomes prohibitive. However, in many real-world
applications, and especially in those concerning multimedia data, a large number of samples,
e.g., images or videos, is usually involved. In order to address this issue, several methods
have been recently proposed, which combine good graph approximations with computational
eﬃciency.
In [ZHGL13], a method for approximate k-NN graph construction is proposed, which is
based on a locality sensitive hashing (LSH) algorithm [GIM99]. The dataset is divided in
smaller subsets of equal size and the k nearest neighbors of a sample are sought for within the
subset it belongs to. Ideally, similar samples should be in the same subset, in order for the k
nearest neighbors of a sample within the subset to be identical to its real k nearest neighbors.
Additionally, the subset should have a small size, to ensure a small number of comparisons
during the k-NN search. In order to divide the dataset, the LSH algorithm is adopted.
LSH applies a hashing function on the data samples and maps them into buckets (subsets),
according to their hash codes, such that similar samples are mapped to the same bucket.
In order to improve the approximation performance, the process is repeated multiple times,
by applying diﬀerent hashing functions and the resulting approximate graphs are combined
into a ﬁnal graph. Typical LSH algorithms do not produce equal-sized subsets, which is
a drawback in the case of k-NN graph construction. In order to alleviate this problem, a
modiﬁcation of the LSH is introduced, which is based on a linear projection of the data
hash codes. The proposed method is fast, accurate and easy to paralellize, while it can be
utilized along with any similarity measure.
In [HWLH12], a two-stage graph construction method for large image datasets is intro-
duced. In the ﬁrst stage, the samples are divided into overlapping subsets, called image pools,
using an LSH method. Speciﬁcally, the MinHash [Bro97] algorithm is adopted to separate
the samples into non-overlapping subsets. In order to enhance the hashing result, MinHash
is applied repeatedly on the data, producing diﬀerent divisions (hashing tables). While the
subsets in the same hashing table do not overlap, overlapping may occur between subsets
from diﬀerent hashing tables. Image pools are subsequently formed by joining the overlap-
ping subsets from diﬀerent hashing tables. In the second stage of the method, the pair-wise
similarities between samples belonging to the same pool are calculated and the graph is
constructed. In order to handle the computational cost when dealing with large datasets,
both steps are performed using the MapReduce parallel programming model [DG08]. An-
other LSH-based approach for large scale graph construction can be found in [CMYC10],
where LSH is combined with l1-graph construction in each individual bucket.
In [CFS09], a divide and conquer [Ben80] approach is proposed for k-NN graph compu-
tation in large scale and high-dimensional data. Each divide step separates the data samples
in possibly overlapping subsets and computes k-NN graphs for each subset, while each con-

Label Propagation and Information Diﬀusion in Graphs
119
quer step combines the resulting graphs in a ﬁnal kNN graph. In more detail, in the divide
step, a Lanczos [Lan50] procedure is adopted, which performs spectral bisection on the data
graph recursively. This procedure is combined with two diﬀerent methods for dividing a set.
According to the ﬁrst one, referred to as the overlap method, the current set is divided into
two overlapping subsets. The second method, referred to as the glue method, divides the
current set into two disjoint subsets, while a third set consisting of data samples common to
the two subsets is used in order to merge the two k-NN graphs in the conquer step. During
the conquer step, the k-NN graphs computed by the divide step are combined into a ﬁnal
k-NN graph.
The algorithm proposed in [WWZ+12] follows a divide and conquer process in order
to partition the whole dataset into smaller subsets, within which neighborhood graphs are
calculated. In more detail, the dataset is recursively divided into non-overlapping subsets,
until the size of each subset reaches a pre-deﬁned value. Since a single division is not capable
of connecting a node with neighboring nodes belonging to diﬀerent subsets, multiple random
divisions are performed. As a result, a node is associated with multiple neighborhood sets,
i.e. sets of neighboring nodes, each resulting from a diﬀerent division. The neighbors of a
node are deﬁned as the k nearest nodes in the union of the neighborhood sets. Furthermore,
a neighborhood propagation scheme is incorporated in the algorithm, in order to enhance
the quality of the approximation. According to this scheme, true neighboring nodes of a
node i are discovered within the neighborhoods of nodes that have already been identiﬁed
as neighbors of i.
In [KTT+12], a graph construction method is introduced, which calculates a match
graph for large image databases, i.e. a graph where nodes correspond to images and edges
express matching relationships between images. Two images are linked with an edge, if a
correspondence, e.g., a similarity measure between them has been calculated. This process
is referred to as linking or veriﬁcation. The proposed algorithm departs from a sparse graph
which gradually becomes dense, through an iterative process which predicts the existence
of edges between candidate matching images (nodes). The initial sparse graph G0 is formed
by verifying a small number of images. Subsequently, the iterative process is applied on this
graph, which alternates between two steps:
• Prediction: at time step t, for each unveriﬁed edge, a conﬁdence measure is estimated,
which expresses the probability that a candidate edge is a real edge.
• Veriﬁcation: the candidate edges that exhibited the m highest conﬁdence values are
veriﬁed and graph Gt is updated accordingly.
Other approaches for large-scale match graph construction can be found in [HGO+10,
PSZ11].
In [YWZ12], the computational cost of the NCA method [GRHS04] is addressed, when
dealing with large-scale or high dimensionality data. The proposed Fast Neighborhood Com-
ponent Analysis (FNCA) exhibits a signiﬁcantly lower computational complexity in com-
parison to NCA. Furthermore, it is modiﬁed, so that it can be applied to a non-linear metric
learning scenario, by using the kernel trick.
The k-NN fused Lasso graph introduced in [ZLP13] is suitable for large scale high-
dimensional datasets. The proposed algorithm extends the l1 graph construction by incor-
porating structured sparsity. The local structure of the data is taken into consideration by
assuming that the reconstruction coeﬃcients of a data sample have values similar to those
of its nearest neighbors. Furthermore, the k-NN method in combination with a kernel is
adopted by the graph construction process, in order to render the algorithm scalable. The
NN-Descent, proposed in [DML11], is an eﬃcient algorithm for constructing an approximate
k-NN graph. The main idea behind this algorithm is to improve the k-NN approximation for

120
Graph-Based Social Media Analysis
a sample by examining its neighbors’ neighbors. In more detail, departing from a random
approximate k-NN graph for each data sample, the approximation is improved through an
iterative process, which performs comparisons between the sample and samples belonging
to the k-neighborhood of its neighbors.
Another method for scalable graph construction, which is based on anchor points, is
proposed in [LHC10]. The main idea behind this method is to use a small number of samples
(anchors), which approximate the local neighborhood structure. These anchor points are
also utilized during label inference. Finally, the greedy ﬁltering method proposed in [PPLJ14]
constructs an approximate k-NN graph by a preﬁx ﬁltering method [LPSL10] on the data
samples’ feature vectors.
5.3
Label inference methods
Once the graph representing the data sample similarities has been constructed, label
propagation is performed on the graph nodes, by means of a label inference method that
determines how the labels are spread from the set of labeled nodes to the unlabeled nodes.
Therefore, label inference is essentially an information diﬀusion process over the similarity
graph.
Let us assume a set of labeled data, denoted with XL = {xi, i = 1, ..., nl}, which are
assigned labels from the set L = {lj}L
j=1 and a set of unlabeled data XU = {xi, i = 1, ..., nu}.
The entire dataset, consisting of the labeled and the unlabeled data, can be denoted by X =
{x1, . . . , xnl, xnl+1, . . . , xN}, N = nl + nu. The label vector y = [y1, . . . , ynl, 0, . . . , 0]T =
[yL, yU]T ∈LN contains the labels of the labeled data in the ﬁrst nl positions and takes the
value 0 in the last nu positions. In matrix notation, the label matrix Y ∈RN×L is deﬁned as
the matrix having entries Yil = 1, if the i-th sample has the l-th label and Yil = 0, otherwise.
In correspondence to the label vector and the label matrix, we denote the estimated label
vector and label matrix by ˆy and ˆY respectively. With respect to the rules that govern
label dissemination, as well as the type and the number of the graphs they apply to, label
inference methods can be divided in various categories that are described subsequently.
5.3.1
Iterative algorithms
Iterative label propagation algorithms disseminate labels from labeled to unlabeled data
gradually, in an iterative way. In each iteration, the labels are updated according to some
rule, until the algorithm converges to a stationary state, as t →∞. Since the stationary
state of the iterative algorithms can be estimated in advance, in practice, these methods
can be applied in a single step.
An early iterative approach is proposed in [ZG02]. Initially, a fully connected graph is
constructed from both labeled and unlabeled nodes, where the edge weights are calculated
by:
Wij = exp(−||xi −xj||2
σ2
),
(5.3.1)
where parameter σ > 0 controls the weight values. In each iteration, the labels are propa-
gated through the edges, according to a transition matrix T of size N × N, whose entries
are given by:
Tij ≜P(j →i) =
Wij
PN
k=1 Wkj
.
(5.3.2)

Label Propagation and Information Diﬀusion in Graphs
121
Each entry Tij of the transition matrix expresses the probability of propagating the label
of the node j to node i. The node labels are estimated in each iteration according to the
following update rule:
ˆY(t+1) = T ˆY
(t),
(5.3.3)
where ˆY(t) and ˆY(t+1) denote the estimated labels at times t and t + 1 respectively. In this
algorithm, the estimated labels of the originally labeled data are constrained to be equal to
the initial labels. In order to achieve improved accuracy and robustness, prior knowledge
of class labels can be incorporated during label inference. The prior probabilities for each
class j are estimated from the labeled examples as:
pj = 1
nl
nl
X
i=1
Yij.
(5.3.4)
Two diﬀerent methods are proposed that utilize class prior knowledge, namely class mass
normalization and label bidding. Assuming that each unlabeled example is assigned a vector
ˆyi = {ˆyi1, ..., ˆyiL}, corresponding to a row in matrix ˆY, whose entries express the probability
that the example belongs to each of the L classes, the mass of class j is deﬁned as:
mj = 1
nu
N
X
i=nl+1
ˆYij.
(5.3.5)
In class mass normalization, the j-th entry of ˆyi, i = nl + 1, ..., N (xi ∈XU) is scaled by
the factor wj =
pj
mj and the label assignment for the unlabeled sample xi is performed
according to argmaxj{wj ˆYij}. Class mass normalization does not require the knowledge
of the exact label (class) proportions. However, if the exact class proportions are known,
a label bidding heuristic can be employed. Let us denote by cj the number of unlabeled
examples that are assigned the label lj, with P
j cj = nu. In each iteration, the unlabeled
example xi with the highest class probability maxj{ ˆYij} is found and assigned the label
ljmax, where jmax = argmaxj{ ˆYij}, if the number of the already assigned labels ljmax does
not exceed cjmax. Otherwise it is ignored and the next highest class probability is searched.
In [BDR06], the proposed label propagation algorithm is based on the Jacobi iterative
method for linear equations [BBC+94]. The update rules for the labeled and unlabeled
nodes take the form:
ˆy(t+1)
i
=
P
j Wij ˆy(t)
j
+ 1
µy(t)
i
P
j Wij + 1
µ + ϵ
, i = 1, ..., nl
ˆy(t+1)
i
=
P
j Wij ˆy(t)
j
P
j Wij + ϵ,
i = nl + 1, ..., N,
(5.3.6)
respectively, where µ is a parameter that regulates the weight of the labeled node and ϵ > 0
a regularization parameter which prevents numerical problems in case the denominator
becomes too small. In this algorithm, the estimated labels for the originally labeled nodes
are allowed to diﬀer from the initial labels.
In [ZBL+04], an iterative method that utilizes the graph Laplacian is introduced. It ﬁrst
constructs a weight matrix similar to the one used in [ZG02]. Subsequently, the normalized
graph Laplacian is calculated by:
L = D−1/2LD−1/2 = I −D−1/2WD−1/2.
(5.3.7)

122
Graph-Based Social Media Analysis
In each iteration, the labels of both labeled and unlabeled nodes are updated, according to
a classiﬁcation matrix F ∈RN×L given by:
F(t+1) = µSF(t) + (1 −µ)Y, with F0 = Y,
(5.3.8)
where S = D−1/2WD−1/2 and µ is a parameter taking values in (0, 1). If F∗is the limit
of F upon convergence, a node i is ﬁnally labeled according to arg maxj{F ∗
ij}. The afore-
mentioned method was incorporated in the graph-based active learning method proposed
in [LYZZ08]. An algorithm similar to [ZBL+04] is introduced by Wang et al. [WZ06], where
a function f ∈RN estimates the node labels, utilizing the following update rule:
f (t+1) = αWf (t) + (1 −α)y, with f 0 = y.
(5.3.9)
Parameter α takes values in the range (0, 1) and expresses the fraction of label information
that a node receives from its neighbors and f (t) are the label predictions.
5.3.2
Random walks
Label inference can be performed through random walks on the similarity graph, starting
from labeled nodes of diﬀerent classes. In [SJ02], Markov random walks on the graph are
considered, where the transition probability from node i to a neighbor k is calculated by:
pik =
Wik
PN
j=1 Wij
.
(5.3.10)
The probability that a walk, which started at some node with label ystart, reaches node k
after t steps is given by:
P t(ystart|k) =
N
X
i=1
P(y = ystart|i)P0|t(i|k),
(5.3.11)
where P(y|i) denotes the probability that node i has the label y and P0|t(i|k) is the prob-
ability of reaching node k starting from node i in t steps. The node k is assigned the label
ystart, if the aforementioned probability is greater than 0.5. The probabilities P(y|i) are
estimated using two proposed alternatives: an iterative Expectation Maximization EM al-
gorithm, or by maximizing a margin-based criterion that leads to a closed-form solution.
The method proposed in [ZGL03] can be interpreted as a random walk in the graph, where
an alternative formulation of the random walks to [SJ02] is considered. Speciﬁcally, a walk
starts from an unlabeled node i and traverses the graph until it arrives at a labeled node
k with a probability given by a function f(i). Node i is, thus, assigned the same label as
node k with a probability f(i). Unlike [SJ02], the solution does not depend on the length t
of the walk.
In [ZS04], a two-class classiﬁcation scenario is examined, where the label assignment
decision is taken according to the commute time Gij, i.e., the number of steps required for
a random walk initiated at node i to reach node j and then get back to i. The transition
probability matrix is calculated as:
P = (1 −a)I + aD−1W,
(5.3.12)
where parameter a takes values in the range (0, 1). The classiﬁcation decision is taken by
comparing the commute times ¯Gij from node i to the labeled nodes of diﬀerent classes with
labels L = {+1, −1}:
p+(xi) =
X
j|yj=1
¯Gij, p−(xi) =
X
j|yj=−1
¯Gij,
(5.3.13)
where ¯G = (I −aD−1/2WD−1/2)−1.

Label Propagation and Information Diﬀusion in Graphs
123
The random walks introduced in [Azr07] start from unlabeled nodes, while labeled nodes
serve as absorbing states, i.e., the random walk stops upon visiting a labeled node. The prob-
ability that an unlabeled node i is assigned the label l is calculated by the total probability
that a random walk initiated at node i stops/is absorbed at a node labeled with l.
The Adsorption algorithm introduced in [BSS+08] can be interpreted as a random walk
on a graph. In order to estimate the label of a labeled or unlabeled node i , a random walk
is initiated at node i and continues according to three alternative choices [TC09]:
• Injection: with a probability pinj
i
, the walk stops and the predeﬁned label vector yi is
returned. For the unlabeled nodes, a probability pinj
i
= 0 is assumed.
• Continue: with a probability pcont
i
, the walk continues to another neighboring node j,
according to the value of the transition probability Pr[j|i] calculated by:
Pr[j|i] =



Wji
P
k:(k,i)∈E
Wki ,
if (j, i) ∈E
0,
otherwise.
(5.3.14)
• Abandon: with a probability pabnd
i
, the walk is abandoned and a dummy label is
assigned, expressing uncertainty about the node label. The dummy label is introduced
in order to reduce the eﬀect of nodes with a large number of connections. High degree
nodes are considered unreliable, since they may have connections to nodes which are
not similar to each other. Therefore, the random walk should be abandoned upon
visiting a high degree node. The higher the degree of a node, the higher the value of
pabnd is.
For the aforementioned probabilities, it holds that: pinj
i
, pcont
i
, pabnd
i
≥0 and pinj
i
+ pcont
i
+
pabnd
i
= 1.
5.3.3
Graph regularization
Graph regularization-based label propagation methods utilize a classiﬁcation vector
f ∈RN (or a matrix F ∈RN×L alternatively), deﬁned on labeled and unlabeled data
(nodes), which disseminates the labels from the set of labeled nodes to the set of unlabeled
nodes. This vector should ensure that the labels of the originally labeled nodes remain un-
changed, while nodes that are similar to each other or belong to the same structure (e.g.,
cluster, manifold) are assigned the same label. According to the second assumption, vector
f should be smooth over the graph. The aforementioned assumptions can be expressed by
a regularization framework of the following form:
min
f {αC(fL) + βS(f)},
(5.3.15)
where C(fL) denotes a cost function deﬁned on the labeled nodes, which penalizes the dif-
ference between the estimated labels and the initial labels, while S(f) applies a smoothness
constraint to the entire graph. The parameters α and β express the trade-oﬀbetween the
two terms. The smoothness constraint is usually expressed as:
S(f) = f T Sf,
(5.3.16)
where S is a smoothing matrix.
In [ZBL+04], a simple method for label propagation is proposed, that minimizes the
following cost function:
Q(F) = 1
2
N
X
i,j=1
Wij


fi
√Dii
−
fj
p
Djj


2
+ µ
N
X
i=1
||fi −yi||2 ,
(5.3.17)

124
Graph-Based Social Media Analysis
where µ > 0 is a regularization parameter and fi the i-th row of F. The closed form solution
is calculated by:
˜F = β(I −αS)−1Y.
(5.3.18)
This regularization framework is proven to be equivalent to the iterative process described by
equation (5.3.8). In each iteration, the graph nodes receive information from their neighbors,
while maintaining the information of their initial state (label). In the algorithms proposed
in [WZ06] and [THY+11], the cost function has the form of equation (5.3.15) and the
smoothness matrix is S = I −W, where W denotes the weight matrix.
The label propagation algorithm proposed in [ZGL03] is based on a Gaussian Random
Field model, deﬁned on the graph. The regularization problem consists of the minimization
of the quadratic energy function, expressed as f T Lf, while preserving the labels of the
initially labeled nodes. This is achieved by setting parameter a in equation (5.3.15) to ∞.
The minimum energy function satisﬁes the harmonic property, i.e., it is equivalent to the
average energy of the neighboring nodes. The relationship between the Gaussian random
ﬁelds and the Gaussian processes were studied in [ZLG03], using a spectral transformation
on the graph Laplacian matrix.
Two diﬀerent algorithms based on regularization frameworks were proposed in [BMN04].
The ﬁrst one, referred to as Tikhonov regularization, assumes that labeled nodes may exhibit
multiplicities, i.e. they may appear multiple times with the same or with a diﬀerent label
y. As a result, the number of the available labeled samples k may diﬀer from the number
of labeled nodes nl. Furthermore, it is assumed that labels may be noisy. The proposed
regularization framework minimizes the following objective function:
min
f={f1,f2,...,fN}
(
1
k
X
i
(fi −˜yi)2 + γf T Sf
)
,
s.t.
X
fi = 0,
(5.3.19)
where γ ∈R and ˜yi are elements of ˜y = {y1 −¯y, y2 −¯y, ..., yk −¯y}, with ¯y =
1
k
P
i yi.
Matrix S is a smoothness matrix, e.g., S = L or S = Lp, p ∈N, where L is the graph
Laplacian. The solution to the above minimization problem is obtained by:
˜f = (kγS + Ik)−1(yΣ + µ1),
(5.3.20)
where yΣ = {P
i y1i, P
i y2i, ..., P
i ynli, 0, ..., 0} is a vector of length N containing the sums
of the labels assigned to the same node of the graph and 1 a vector of entries equal to one.
Ik is a diagonal matrix:
Ik ≜diag(m1, m2, ..., mnl, 0, ..., 0),
(5.3.21)
where mi denotes the number of occurrences of node i in the labeled samples set. In the
interpolated regularization algorithm proposed in [BMN04], the objective function takes the
form:
min
f={˜y1,˜y2,...,˜yk,fk+1,...,fn}

f T Sf
	
,
(5.3.22)
where S is a smoothness matrix as in (5.3.19). Unlike the Tikhonov regularization, in the
interpolated regularization it is assumed that labels y do not contain noise. In addition,
nodes are not allowed to appear multiple times in the sample (k = nl). Matrix S can be
written in the form:
S =
"
S1
S2
ST
2
S3
#
,
(5.3.23)

Label Propagation and Information Diﬀusion in Graphs
125
where S1 is a k × k matrix, S2 a k × (N −k) matrix and S3 a (N −k) × (N −k) matrix.
The solution to (5.3.22) is calculated by:
˜f = S−1
3 ST
2 ({˜y1, ..., ˜yk}T + µ1).
(5.3.24)
In [WJC08], the Graph Transduction via Alternating Minimization (GTAM) algorithm
is presented, that is capable of handling degenerate cases. They may occur, for example,
when there are imbalances in the distribution of class labels or in the portion of the labeled
data, or when there is noise or outliers in the dataset. This is achieved by minimizing an
objective function, which is deﬁned both on the classiﬁcation function F and the label
matrix Y:
min
F,Y

tr

FT LF + µ(F −VY)T (F −VY)
		
,
(5.3.25)
where µ > 0 is a trade-oﬀparameter between the two terms, L is the normalized graph
Laplacian and matrix V serves as a node regularizer that balances the inﬂuence of labels
from diﬀerent classes. The aforementioned objective function involves two variables, F and
Y, which are minimized through an alternating minimization scheme. Since F is a contin-
uous classiﬁcation matrix, optimization over F is obtained by:
˜F = (L/µ + I)−1VY.
(5.3.26)
Minimizing with respect to Y is not straightforward, since it is a binary matrix. For this
reason, optimization is obtained by replacing F in equation (5.3.25) with ˜F and subsequently
applying a greedy approach.
In the measure propagation algorithm proposed in [SB11], the objective function is
based on the minimization of the Kullback-Leibler divergence between probability measures,
which encode label membership probabilities. More precisely, two probability measures for
the nodes of the graph are deﬁned: pi(l), i = 1, .., N is deﬁned for each node i of the
graph and corresponds to the probability that node i is assigned the label l, l ∈L, while
rj(l), j = 1, ..., nl is deﬁned only on the labeled nodes of the graph, and expresses the
probability distribution of the labeled nodes. The objective function is expressed as:
min
p
nl
X
i=1
DKL(ri||pi) + µ
N
X
i=1
X
j∈N(i)
wijDKL(pi||pj) −ν
N
X
i=1
H(pi),
(5.3.27)
where µ, ν > 0 are hyper-parameters and DKL is the Kullback-Leibler divergence between
pi and qj, given by DKL(p||q) = P
y p(y) log p(y)
q(y). H denotes the Shannon entropy of p:
H(p) = −
X
y
p(y) log p(y).
(5.3.28)
The ﬁrst two terms in the objective function represent the constraints expressed in equation
(5.3.15). The additional constraint introduced by the third term, enforces the probability
distributions pi to be close to the uniform distribution. The aforementioned optimization
problem does not admit a closed form solution. To this end, a method of multipliers (MOM)
[Ber99] is employed instead.
A modiﬁcation of the Adsorption algorithm [BSS+08] is introduced in [TC09], where
learning is stated as a convex optimization problem. The proposed method assumes matri-
ces containing the predeﬁned and the estimated labels, Y and F ∈RN×(L+1) (L denotes
the number of labels) respectively. An alternative deﬁnition of the weight matrix is also
introduced, where the weights between two nodes i and j are expressed as W ′
ij = pcont
i
Wij.
Furthermore, a matrix R ∈RN×(L+1) is deﬁned, where the ﬁrst L columns are equal to zero

126
Graph-Based Social Media Analysis
and the last column contains the abandon probabilities pabnd
i
. The optimization problem
minimizes the following cost function:
C(F) =
X
l
µ1 (yl −fl)T S (yl −fl) + µ2f T
l Lfl + µ3 ||fl −rl||2 ,
(5.3.29)
where yl, fl and rl denote the l-th column of the matrices Y, F and R respectively, while
S ∈RN×N is a diagonal matrix with Sii = pinj
i
. Matrix L is calculated as:
L = D + ¯D + W′ + W′T ,
(5.3.30)
where D and ¯D are N × N diagonal matrices with
Dii =
X
j
W ′
ji, ¯Dii =
X
j
W ′
ij.
(5.3.31)
In the above optimization framework, the ﬁrst two terms match the requirements of (5.3.15),
while the third term discounts high degree nodes. The importance of each term is controlled
by the weight parameters µ1, µ2, µ3 > 0. The solution is obtained by:
fl = (µ1S + µ2L + µ3I)−1(µ1Syl + µ3Rl).
(5.3.32)
Both the Adsorption algorithm [BSS+08], as well as its modiﬁed version [TC09], try
to reduce the inﬂuence of high degree nodes, since they may connect dissimilar nodes. In
contrast to these methods, the Transduction Algorithm with Conﬁdence (TACO) algorithm
[OC12] takes into account the level of agreement among the labels of node neighbors and
utilizes a measure of conﬁdence regarding the label assignment of each node. The TACO
algorithm does not reduce the inﬂuence of all high degree nodes, but only of those that
exhibit a low conﬁdence in their predicted labels. Two parameter sets are deﬁned on graph
nodes: the ﬁrst one consists of the score vectors fi, ∈RL, expressing the extent to which a
node i belongs to each of the L classes (labels). Furthermore, for each node i, a diagonal ma-
trix Σi ∈RL×L is calculated, whose entries represent the uncertainties of the corresponding
scores in fi. The objective function takes the form:
min
f,Σ



1
4
N
X
i,j=1
wij

(fi −fj)T (Σ−1
i
+ Σ−1
j )(fi −fj)

+ 1
2
nl
X
i=1

(fi −yi)T (Σ−1
i
+ 1
γ I)(fi −yi)

+α
N
X
i=1
trΣi −β
N
X
i=1
log detΣi
)
,
(5.3.33)
where α, β, γ are positive parameters. The solution to the aforementioned optimization is
obtained through an iterative algorithm.
In [BC01], a two-class label propagation problem is regarded as a clustering problem and
a graph mincuts algorithm [CGK+97]. Speciﬁcally, this algorithm discovers the minimum
total weight of a set of edges, whose removal partitions the graph into two sets of nodes:
one consisting of nodes with label 1 and one consisting of nodes with label −1. The graph
mincut is expressed by the following objective function:
min
f {a(fL −YL)T (fL −YL) + 1
2f T Lf},
(5.3.34)

Label Propagation and Information Diﬀusion in Graphs
127
where a →∞and L is the graph Laplacian, under the constraint fi ∈{0, 1}. In a later study,
the aforementioned graph mincuts approach is extended, by adding artiﬁcial random noise
on the edge weights [BLRR04]. The proposed algorithm exhibits improved performance and
also provides a conﬁdence score for the assigned labels.
In [Joa03], spectral graph partitioning is performed through the constrained ratiocut
algorithm that adds a quadratic penalty to the objective function of the standard ratio cut
[HK92]:
min
f {f T Lf + c(f −g)T C(f −g)},
(5.3.35)
s.t. f T 1 = 0 and f T f = N,
(5.3.36)
where c is a regularization parameter, L is the graph Laplacian and C is a diagonal matrix,
whose i-th diagonal element contains a misclassiﬁcation cost for node i.
5.3.4
Graph kernel regularization
In manifold regularization methods with graph kernels, the smoothness constraint
(5.3.16) is written in the form:
S(f) = ∥f∥H = f T Kf,
(5.3.37)
where K is a kernel matrix that is associated with the Reproducing Kernel Hilbert Space
(RKHS) H. A kernel matrix is equivalent to the Gram matrix. Graph kernels capture the
local and global structure of the data space. A function K is considered to be a kernel
function if it is symmetric and positive semi-deﬁnite. The exponentiation of any symmetric
matrix H results in a symmetric and positive semi-deﬁnite matrix K:
K = eβH = lim
n→∞

1 + βH
n
n
,
(5.3.38)
which can be used to deﬁne an exponential family of kernel functions, where H is the so-
called the generator and β is a bandwidth parameter [KL02]. The exponential kernel K
has the property that if the generator H represents the local structure of the data space,
then K represents the global structure of the data space. Let us consider an unweighted,
undirected graph G and an exponential kernel function with a generator of the form:
H =



1,
node i connected to node j
−di,
i=j
0,
otherwise,
(5.3.39)
where di is the degree of node i. It can be observed that matrix H is the negative of the
Laplacian matrix of graph G. By diﬀerentiating equation (5.3.38) with respect to β the
following diﬀerential equation is obtained:
dK
dβ = HK = −LK,
(5.3.40)
which is the heat equation on graph G that is subsequently described in Section 5.4. The
resulting kernels are called diﬀusion or heat kernels.
Kernel families on graphs can be alternatively derived from the spectral analysis of
the normalized graph Laplacian matrix [SK03]. The eigenvectors v and eigenvalues λ of the
normalized graph Laplacian matrix L contain information about the graph partitions, which

128
Graph-Based Social Media Analysis
renders them an important tool for graph clustering. A class of regularization functionals
on graphs can be deﬁned [SK03] to be of the form:
⟨f, f⟩H = ⟨f, r(L)f⟩,
(5.3.41)
where
r(L) :=
X
i
r(λi)vivT
i
(5.3.42)
and ⟨·⟩H is the inner vector product in the RKHS H with kernel:
K =
X
i
r−1(λi)vivT
i .
(5.3.43)
All kernel functions are derived from (5.3.43) with a proper choice of spectral transform r(λ)
of the Laplacian matrix. For example, the diﬀusion kernel in equation (5.3.38) is obtained
for r(λ) = exp (σ2/λ) and the regularized Laplacian [ZLG03]:
K = L + I/σ2
(5.3.44)
is obtained for r(λ) = λ + 1/σ2.
In [BNS06], a general manifold regularization framework is proposed, where the objective
function takes the form:
1
nl
nl
X
i=1
V (xi, yi, f) + γA∥f∥2
H + γI∥f∥2
I,
(5.3.45)
where γA and γI are regularization parameters. The ﬁrst term is the general form of the cost
function on the labeled data, ∥f∥2
H is a regularization term in the RKHS of the kernel K
and ∥f∥2
I is a regularization term of the geometry of the probability distribution. Laplacian
Regularized Least Squares (LapRLS) and Laplacian Support Vector Machines (LapSVM)
[GCCVMMC08] can be regarded as special cases of manifold regularization.
5.3.5
Inductive label inference
Unlike transductive label propagation methods, which are applied on a speciﬁc dataset,
inductive methods learn a global representation from the available data and can be applied
to unseen data samples. An early work on inductive semi-supervised learning can be found
in [YTZ04]. The proposed method is based on the regularization framework introduced
in [ZBL+04] and adopts RBF basis functions. Another inductive method is introduced in
[ZL05], which combines harmonic mixture models with graph-based semi-supervised learn-
ing.
In [BDR06], a common framework for diﬀerent label propagation algorithms is proposed,
where a quadratic cost function is minimized. The closed-form solution of the aforemen-
tioned cost function is obtained by solving a linear system of size equal to the number of
the data samples. This cost criterion provides an extension of the diﬀerent label propa-
gation algorithms to the inductive setting. Assuming that all the available data samples
X = {x1, . . . , xN} have been assigned labels ˆy = {ˆy1, . . . , ˆyN} by a label propagation
method, the aim is to determine the label of a new data sample x. A new data sample x is
incorporated in the graph, with a new weight matrix Wx. The objective is the minimization
of the following cost function:
c + µ

X
j
Wx(x, xj)(ˆy −ˆyj)2 + ϵˆy2

,
µ, ϵ > 0,
(5.3.46)

Label Propagation and Information Diﬀusion in Graphs
129
with respect to the new label ˆy, where c is a constant. By setting the ﬁrst derivative with
respect to ˆy to zero, the minimum of (5.3.46) is calculated as:
ˆy =
P
j Wx(x, xj)ˆyj
Wx(x, xj) + ϵ .
(5.3.47)
In case the weight matrix Wx is constructed using the k-Nearest Neighbors, then (5.3.47)
becomes equivalent to k-NN classiﬁcation. If Wx is calculated using the Gaussian kernel,
then (5.3.47) is equivalent to the Nadaraya-Watson kernel regression [Bie87].
5.3.6
Label propagation on data with multiple representations
The methods reviewed so far assume a single data representation. However, in many
real world scenarios, multimedia data can be represented in multiple feature spaces. As an
example, in the case of 3D video data, each video (node) can be represented by feature
vectors describing color or disparity (depth). In such cases, a separate similarity graph can
be constructed for each of these representations. The information from the multiple data
representations can be fused in two ways. Fusion can take place during graph construction
(early fusion), e.g., the feature vectors of each representation can be concatenated into
a single feature vector. Alternatively, fusion can be performed at the decision level (late
fusion), e.g., by combining the classiﬁcation results derived from separate classiﬁcation
schemes for each representation. Late fusion is also referred to as multi-modal fusion or
multi-modality learning [WHH+09].
In an early study [JCST01] convex combinations of independent kernels were adopted:
K(x1, x2) = αK(x1, x2) + (1 −α)K(x1, x2), 0 ≤α ≤1.
(5.3.48)
Such kernels are considered independent if they come from independent data representa-
tions. Similar approaches can be found in [AHP05, TSS05, SN05], where a convex combi-
nation of graph Laplacians is employed.
In [THL+05], the multi-modality learning problem is approached by extending the reg-
ularization framework proposed in [ZBL+04] to the case of multiple graphs. Each feature
type (modality) is represented by an individual graph. Two alternative schemes, a linear
and a sequential one, are proposed for fusing information from diﬀerent modalities. In more
detail, assuming two feature types, each data sample is associated with two feature vec-
tors xi =

xa
i , xb
i
	
, i = 1, ..., N, corresponding to each of the modalities a,b. For the ﬁrst
feature type, the weight matrix Wa of size N × N is constructed by measuring the simi-
larity between data samples in the modality a. Furthermore, the degree matrix Da, as well
as the normalized weight matrix Sa = (Da)−1/2Wa(Da)−1/2 can be calculated. Similarly,
matrices Wb, Db and Sb are deﬁned for the second modality b.
In the linear fusion scheme, the smoothness constraints introduced by Sa, Sb, as well as
the manifold constraint imposed by Y are fused simultaneously through a weighted sum.
In this case, the objective function results directly from (5.3.17) and is expressed as:
Q(F) =µ
N
X
i,j=1
Waij


fi
√Daii
−
fj
p
Dajj


2
+ η
N
X
i,j=1
Wbij


fi
√Dbii
−
fj
p
Dbjj


2
+
ϵ
N
X
i=1
||fi −yi||2 ,
(5.3.49)
where µ, η, ϵ ∈(0, 1) and µ+η+ϵ = 1 are weights that control the trade-oﬀamong the three

130
Graph-Based Social Media Analysis
constraint terms. The above equation can be also written in the following, more compact
form:
Q(F) = tr
 µFT (I −Sa)F + ηFT (I −Sb)F + ϵ(F −Y)T (F −Y)

.
(5.3.50)
Solving the above optimization problem leads to:
˜F = (1 −µ −η)(I −µSa −ηSb)−1Y.
(5.3.51)
In the sequential fusion scheme, the constraints are fused sequentially, leading to a
minimization framework solved in two steps:
˜F1 = arg min
F
tr
 µFT (I −Sa)F + (1 −µ)(F −Y)T (F −Y)

(5.3.52)
˜F2 = arg min
F
tr

ηFT (I −Sb)F + (1 −η)(F −˜F1)T (F −˜F1)

,
(5.3.53)
where the regularization parameters 0 < µ, η < 1 regulate the trade-oﬀbetween the two
constraints in each minimization equation. In the ﬁrst step, an optimal ˜F1 for the constraints
Sa, Y is found, while the second step determines an optimal ˜F2, under the constraints
imposed by Sb and ˜F1. The classiﬁcation decision is ﬁnally taken according to the values of
˜F2. The solution to the sequential fusion scheme is calculated by:
˜F2 = (1 −µ)(1 −η)(I −ηSb)−1(I −µSa)−1Y.
(5.3.54)
Similar approaches to [THL+05] can be also found in [WHY+07] and [WHH+09].
In [ZB07], a diﬀerent approach to the multiple representations problem is introduced.
The proposed algorithm associates each graph with a Markov chain, similar to [ZHS05], and
constructs Markov mixture models. The method proposed in [XWTQ07], exploits informa-
tion from 2D images, as well as from 3D points reconstructed from multiple view images, in
order to perform multiple view image segmentation. More precisely, three separate graphs
are constructed: a similarity graph between 3D point coordinates, a 2D colour similarity
graph and ﬁnally, a graph that measures the patch histogram similarity between two joint
points. Joint points are vectors containing the 3D coordinates of a point and its correspond-
ing patches in all the images. Subsequently, a single graph, which represents the similarity
between two joint points, is constructed, by summing the three graphs.
In [ZNP13], two methods for person identity label propagation in facial images obtained
from stereo videos are introduced. The proposed methods combine information from the
left and the right channel of a stereo video and are based on the LNP algorithm, proposed
in [WZ06]. Initially, a complete graph is constructed for each channel and the corresponding
weight matrices Wleft and Wright are calculated, based on the mutual information (MI)
between 2-D hue and saturation histograms, extracted from the facial images. Subsequently,
using the LNP algorithm on the k-nearest neighbors, a sparse graph is constructed from the
aforementioned complete graphs, with weight matrices WLknn and WRknn, for the left and
right channel respectively. The ﬁrst proposed method applies label propagation to the left
and right channels separately, using the LNP algorithm, thus producing two classiﬁcation
matrices FL and FR. Each stereo facial image is then assigned the label that corresponds
to the maximum column of the matrix:
Fmax
ij
= max(FL
ij, FR
ij).
(5.3.55)
In the second method [ZNP13], label propagation is performed on the average graph weight
matrix WS of the left and right channels:
WS = 1
2WLknn + 1
2WRknn.
(5.3.56)

Label Propagation and Information Diﬀusion in Graphs
131
In a later study [ZTNP14], another novel method is introduced for propagating personal
identity labels across facial images extracted from stereo videos. The proposed algorithm
calculates a projection for each data representation, using a technique based on Locality
Preserving Projections (LPP) [HN04], which incorporates side information in the form of
pairwise similarity and dissimilarity constraints. Subsequently, label propagation is applied
using both an early and a late fusion scheme. In the early fusion case, the ﬁnal data rep-
resentation is calculated as a linear combination of the projections of the diﬀerent data
representations. Label inference can then be performed using a method similar to the one
proposed in [ZBL+04]. Late fusion is performed jointly on all diﬀerent representations, by
expressing the regularization framework proposed in [ZBL+04] as a weighted sum of multiple
objective functions.
5.3.7
Label propagation on hypergraphs
The label propagation methods reviewed so far consider pairwise relationships between
data samples. Nevertheless, real-world applications usually exhibit more complex relation-
ships, which involve an arbitrary number of data samples. For example, in an application,
the data may be characterized by multiple labels. In such cases, typical pairwise relation-
ships represented by graphs are not suﬃcient to capture the relationships between the
samples, which results in suboptimal solutions. In order to eﬃciently represent the complex
relationships between data samples, hypergraphs can be adopted.
Let us assume a weighted hypergraph H = (E, V, w) and an initially labeled subset of
nodes S ⊂V, with labels L = {lj, j = 1, ..., L}. Label propagation algorithms in hyper-
graphs assign labels to the unlabeled nodes, so that nodes belonging to the same hyperedge
are constrained to be assigned the same label. In [ZHS07], a hypergraph transductive label
propagation method was introduced that utilizes hypergraph clustering. Given a classiﬁ-
cation function f : V →R|V|, the classiﬁcation decision is given by a framework of the
form:
arg min
f {Remp(f) + µΩ(f)},
(5.3.57)
where Remp(f) is an empirical loss term, Ω(f) is the clustering objective function, and µ is
a regularization parameter.
In [CJ04] and [Tsu05], two information regularization frameworks for hypergraph label
propagation are presented. The methods employ label probability distributions, instead
of deterministic labels. The framework in [CJ04] minimizes the mixture-type information
regularizer (m-regularizer), while the framework in [Tsu05] minimizes the exponential-type
information regularizer (e-regularizer), which is the dual of m-regularizer. The advantage
of e-regularizer over m-regularizer is that it has a closed form solution.
The method proposed in [SJY08] performs multiple label propagation using hypergraph
spectral learning. It is based on the property that the hypergraph spectrum captures the
correlation among labels. Furthermore, an approximate hypergraph spectral learning frame-
work is introduced that is suitable for handling large scale multi-label propagation prob-
lems. It is based on the approximation of the hypergraph Laplacian matrix L ∈RN×N by
L = HHT , where H ∈RN×L has orthonormal columns.
Another multi-label propagation method, called Rank-HLapSVM , is presented in
[CZW+09]. It is based on hypergraph normalization. Its objective is the minimization of
the ranking loss, while retaining a large margin. It incorporates the hypergraph Laplacian

132
Graph-Based Social Media Analysis
regularizer tr{FT LF} in the objective function of Ranking-SVM [EW01]:
min
F
1
2
L
X
i=1
∥wi∥2 + 1
2λtr{FT LF} + C
N
X
i=1
1
|yi||¯yi|
X
(p,q)∈yi×¯yi
ξipq,
s.t. ⟨wp −wq, xi⟩≥1 −ξipq, (p, q) ∈yi × ¯yi, ξipq ≥0,
(5.3.58)
where yi ⊂L is a subset of labels, ¯yi ⊂L is its complementary set and ξipq are slack
variables. In [WWS+09], multi-label propagation with multiple hypergraphs was employed
for music style classiﬁcation. The method integrates three information sources: audio sig-
nals, music style correlations and music tag information/correlations. The multiple hyper-
graphs are combined in a single hypergraph that models the correlations between diﬀerent
modalities. This is done by constructing a hyperedge for each category that contains all
the nodes that are relevant to the same category. Subsequently, the method performs hy-
pergraph Laplacian regularization of the form tr{FT LF}, similar to the single graph case
described in subsection 5.3.3. This regularization ensures that the label assignment function
F ∈RN×L is smooth on the hypergraph nodes. Hypergraph Laplacian regularization for
semi-supervised label propagation is also applied in [DY08, THK09]. In [DY08], a random
walk interpretation of hypergraph Laplacian regularization is also presented, as well as the
extension of the normalized and the ratio cut (presented in subsection 5.3.3) to hypergraphs.
5.3.8
Label propagation initialization
The initialization of label propagation methods, i.e., the selection of the data samples
that are manually initialized with labels, is crucial to the classiﬁcation performance of the
propagation algorithm [ZTNP14]. It has been observed from experiments that the classiﬁ-
cation accuracy of label propagation methods diﬀers signiﬁcantly for random initializations
of the labeled data set XL. The problem of initializing label propagation methods is ad-
dressed in [ZTNP14]. The method is based on the intuition that the classiﬁcation accuracy
is high when it starts from the most representative class samples and from samples that lie
to the border between classes. The initialization method in [ZTNP14] follows an iterative
procedure for inserting data samples partially to the initially labeled data set L. According
to [ZTNP14], the most representative class samples are the ones that correspond to cluster
centers while the data that lie to the border between classes are the ones in which the two
highest label scores are similar. The method was employed for person identity labels on
facial images extracted from stereo videos.
At the ﬁrst step of the method, a clustering algorithm is employed on the data in order to
extract information for the data structure and select the most representative data samples
in each cluster. The most representative data sample of a cluster is considered the sample
with the highest similarity to all other samples in the cluster, or equivalently, the sample
with the highest within-cluster degree centrality di:
di =
X
j∈Nc
Wij,
(5.3.59)
where Nc the set of data that belong to cluster c and W the similarity graph. It can be
observed that the number of data that enter the set XL is equal to the number of the data
clusters. Then, the initial state matrix Y is computed and label propagation is performed
on the data set according to the decision rule:
l(xi) = arg max
j
Fij,
(5.3.60)

Label Propagation and Information Diﬀusion in Graphs
133
where F is computed by minimizing the regularization framework (5.3.17) introduced in
[ZBL+04].
The values in matrix F are an indication of the “certainty” with which the nodes are
assigned a label. More speciﬁcally, nodes in which the two highest Fij values are very
close to each other are less likely to be assigned the correct label since they lie in a “border”
region between two classes. Label assignment to these nodes is considered to be “uncertain”.
Therefore, the nodes which were assigned a label with the least certainty form the next set
of nodes that will be manually labeled and inserted in the labeled nodes set XL. Then, the
initial state matrix Y is updated, to include the newly inserted labeled samples and label
propagation is performed again according to (5.3.18) and (5.3.60). The procedure is repeated
and the labeled set XL is enriched with a predetermined number of nodes at the time with
the least label assignment certainty, until the initially labeled dataset XL cardinality is a
determined percentage of the overall data number. Usually, the procedure is repeated until
5 −10% of the data enter the set XL.
5.3.9
Applications in digital media
The increasing spread of digital media acquisition devices, e.g. cameras and video/audio
recorders, has led to a radical expansion of the data residing in users’ collections. Further-
more, a vast amount of multimedia data is created, accessed and processed on the Web:
on-line multimedia sharing communities, such as Flickr, Picassa and YouTube, as well as on-
line social networks, such as Facebook, Google+ and Twitter, allow users to share, rank and
annotate multimedia objects. Since the aforementioned websites have grown in popularity
over the last years, a huge amount of on-line multimedia data is available.
Label propagation for annotation of video can be performed at pixel level [CC10, BGC10,
VG12]. Labels of initially labeled pixels in a small set of video frames are propagated to pixels
in the remaining video frames. In this case, a pixel label refers to the semantic concept of the
entity it belongs to (e.g., an object depicted in a video frame). The aforementioned process
results in intra video frame segmentation [CC11]. Furthermore, label propagation can be
applied at a video frame level, i.e., instead of single pixels, labels may characterize entire
video frames [QHR+07, THW+09, WHTH09], video snippets [ZXZ+12], or moving/still
regions, e.g. persons that appear in the video frames [CCC11]. In these cases, graph nodes
represent video frames, video snippets or regions of interest (ROIs) that enclose an object
or a person, respectively.
In image annotation methods, label propagation can be also performed in both pixel
and image level. The objective of pixel level methods is mainly to automate the im-
age segmentation process, rather than to propagate labels across images. These meth-
ods assume that the labels of a small subset of pixels, called seeds, is initially known.
Labels of seed pixels are subsequently propagated to the remaining, unlabeled image
pixels [Gra06, WWL07, RLF12, KGF12, YWY+13, ML13]. In methods applied at im-
age level, each image is represented by global feature descriptors and is treated as a
graph node [GMVS09, CMYC10, HOSS13, HLL+14]. Several methods combine image-
and word-based graph learning, by incorporating textual information derived from la-
bels [LLL+09, LHWH13]. As a result, these methods achieve better consistency be-
tween image similarity and label similarity. Furthermore, instead of propagating each la-
bel individually, several recently proposed multi-label propagation methods spread dif-
ferent labels simultaneously, taking the relationships among the labels into account
[CMYC10, BNMY11, LHWH13].
Label propagation methods in digital media are governed by the same rules and assump-
tions as any graph-based label propagation method. Therefore, any of the graph construction
and label inference methods presented in Sections 5.2 and 5.3 can be applied to the case of

134
Graph-Based Social Media Analysis
digital media data. Moreover, the intrinsic characteristics of digital media data can also be
exploited in the label propagation process. Throughout Sections 5.2 and 5.3 several meth-
ods have been presented that exploit this kind of information for improving the propagation
performance.
In many cases, the dataset labels contain “noise,” which negatively aﬀects the perfor-
mance of the label propagation algorithms. Noise may result from incorrect or incomplete
annotations provided by users or from poor performance of the label propagation algorithm.
In order to deal with the label noise existing in image datasets and to provide labels of im-
proved quality, several methods, referred to as retagging or tag ranking algorithms, have
been proposed [WJZZ06, LHY+09, THY+11, LYHZ11, TLLZ14].
Finally, label propagation algorithms ﬁnd application in recommendation systems
[SK09]. Recommendation methods that exploit information derived from user ratings and
preferences are usually referred to as collaborative ﬁltering methods. In addition, content-
based recommendation algorithms utilize information related to the multimedia content
itself, derived either from metadata or by data feature extraction. Recommendation meth-
ods in large video collections perform label propagation on multiple video graphs, and the
recommendation results are personalized for each user, [LLH+07, THQ+07, BSS+08]. They
exploit co-view information represented by co-view graphs, where graph nodes correspond
to videos, while edge weights represent the number of users that have watched the two
videos. Furthermore, recommendation, apart from multimedia objects, may concern groups
of users with similar interests to a user. In the method proposed in [YJHL11], visual con-
tent of images along with text annotations are employed by a label propagation scheme, to
recommend user groups according to users’ personal photo collections.
5.4
Diﬀusion processes
Diﬀusion and label propagation are two closely related research ﬁelds, since label prop-
agation can be regarded as an information diﬀusion process over a graph. In the following
subsections, the notions of diﬀusion in physics, in sociology, as well as in social media are
brieﬂy discussed.
5.4.1
Diﬀusion in physics
Diﬀusion in physics is used to describe the ﬂow of energy or mass within a medium,
which is common in a bundle of physical processes referred to as transport phenomena, such
as molecular diﬀusion and heat transfer [AF67].
Molecular diﬀusion refers to the motion of liquid or gas molecules, due to thermal energy
dissipation. The rate of diﬀusion depends upon the temperature, the viscosity of the ﬂuid,
as well as the particle mass. The molecules move from a region with high concentration to
one with a lower concentration, which results in a gradual mixing of the material. When
the concentrations of the compartments of the material being mixed become equal, diﬀusive
equilibrium is reached. Molecular diﬀusion is described by Fick’s second law:
∂φ
∂t = D(∂2φ
∂x2 + ∂2φ
∂y2 + ∂2φ
∂z2 ) ,
(5.4.1)
where φ(x, y, z, t) denotes the concentration over the axes x, y and z over time t, while D
is the diﬀusion coeﬃcient that controls the rate of diﬀusion.

Label Propagation and Information Diﬀusion in Graphs
135
Heat conduction refers to the transfer of energy within and between bodies, as a result
of a temperature gradient. Heat ﬂows spontaneously from bodies (or body parts) of higher
temperatures to bodies of lower temperatures, until a thermal equilibrium state is reached,
when the bodies have the same temperature. The evolution of temperature T(x, y, z, t) in
a homogeneous, ﬁnite, three-dimensional body is described by the heat equation:
∂T
∂t = γ(∂2T
∂x2 + ∂2T
∂y2 + ∂2T
∂z2 ).
(5.4.2)
In the above equation, T(x, y, z, t) is the spatio-temporal temperature diﬀusion over x, y, z
and t, while γ denotes heat diﬀusivity, which is the quotient of thermal conductivity κ and
heat capacity c. As already mentioned (subsection 5.3.4), the heat equation on a graph G,
is associated with the graph Laplacian [CY00, KL02]:
dKt
dt
= −LKt,
(5.4.3)
where L is the graph Laplacian and Kt the heat kernel (equation (5.3.38)), which satisﬁes the
initial condition K0 = I. The heat equation on graphs has been adopted in the construction
of models of inﬂuence [MYLK08, ZL13, BLL+14], as well as in label propagation [WKL11].
5.4.2
Diﬀusion in sociology
The way information and ideas are communicated within social groups has long been a
topic of research in sociology. The Diﬀusion of Innovations theory [Rog95] deﬁnes diﬀusion
as the procedure through which an innovation, such as a new idea or product, is spread and
adopted over time by the members of a social system. A social system is deﬁned as “a set of
interrelated units that are engaged in joint problem-solving to accomplish a common goal.”
These units can be individual persons, groups of people, corporations or subsystems. The
social system constitutes the context of innovation diﬀusion. Its characteristics determine
the rate of adoption, i.e., the number of social units that adopt the innovation in a certain
time period. The following parameters are decisive in spreading and adopting an innovation:
compatibility, complexity, trialability and observability to those within the social system.
The degree of eagerness of a social unit to adopt an innovation characterizes its inova-
tiveness. The social units are divided into ﬁve categories, according to their inovativeness:
innovators, early adopters, early majority, late majority, who are the ﬁrst 2.5%, next 13.5%,
next 34%, next 34%, respectively, of the social units that adopt an innovation and, ﬁnally,
laggards, who are the last 16% of the social units that adopt an innovation. This theory,
especially the concept of the adopters categories, has been used in several studies of infor-
mation diﬀusion in social networks [KKT03, KKT05, MYLK08].
5.4.3
Diﬀusion in social media
The evolution of the Internet over the last decade has created new forms of social commu-
nities and interactions. The expansion of on-line social network services, such as Facebook,
Google+ and Twitter, is closely related to the emergence of Web 2.0, that oﬀers the users
the possibility to share information and interact. The development and the growing popu-
larity of such networks is indicative of the transition from older on-line communities where
users participated according to their interests (for example business, music, technology),
to generic social networking services, where users have the opportunity to communicate to
each other, as well as to contribute and share all kind of information.

136
Graph-Based Social Media Analysis
FIGURE 5.4.1: Information cascades in a social network.
Today, on-line social networks play a key role in information spread and have radically
inﬂuenced the way ideas, news and trends are communicated among individuals. Information
cascades in a network of individuals occur as information or behaviors spread from node to
node through the network. For example, a user A in a social network may read an article
and share it with user B, who may in turn share it with his/her friends and so on. Such an
example of information cascades is illustrated in Figure 5.4.1, where nodes correspond to
individuals and arrows indicate the information ﬂow from one individual to another.
Understanding the underlying mechanisms of information diﬀusion in on-line social net-
works can be useful in various cases, such as tracking the evolution of speciﬁc topics [LBK09],
preventing misinformation [NYTE12] and optimizing marketing campaigns [KKT03]. As a
consequence, the study of information diﬀusion cascades in social networks has become an
active research ﬁeld in the last years.
5.5
Social network diﬀusion models
Modeling the way information is spread among the individuals in a social network is
crucial to a diversity of applications, such as viral marketing or emergency management.
For this purpose, several approaches have been proposed. In epidemic diﬀusion models, in-
formation diﬀusion is compared to the spread of an infectious disease within a population.
Furthermore, game theoretical approaches have been employed, where social interactions
and individual’s behaviors are modeled using concepts from game theory. Finally, the com-
monly used threshold and cascade models are based on relationships of inﬂuence between
individuals.
Apart from exploring the ways in which information is spread within a network, several
studies are concerned with the problem of inﬂuence maximization. This consists of discov-

Label Propagation and Information Diﬀusion in Graphs
137
ering the most inﬂuential individuals (nodes) of the network, such that, when information is
disseminated through these nodes, maximal information spread is achieved. Inﬂuence max-
imization can be of great importance in marketing applications, where the aim is to reach
the greatest possible number of customers in a target group.
In the following subsections, we shall discuss the most important models that have
been proposed for the study of information diﬀusion in social networks, as well as for inﬂu-
ence maximization. Finally, other applications of information diﬀusion, apart from inﬂuence
maximization, are also reviewed.
5.5.1
Game theoretical diﬀusion models
Social interactions can be modeled using concepts adopted from game theory [vNM44].
Individuals are regarded as players, participating in a game (the social interaction), where
each individual may choose among a number of strategies, corresponding to diﬀerent behav-
iors. Each available choice or strategy is assigned a payoﬀ, according to a payoﬀfunction.
This payoﬀdepends not only on player’s choice, but also on choices made by the other
players. In classic game theory, the players select the choices that lead to the maximization
of their payoﬀ. Usually, each player is more likely to interact with certain other players,
for example with his/her friends in a real-world scenario, than interacting with all other
players. The local nature of such interactions is captured by local interaction games, studied
in [Blu93, Ell93, Mor00].
Classic game theory assumes that the players’ choices are driven by rationality. However,
this fact does not reﬂect real world situations, because people do not always make decisions
according to strictly rational criteria. Furthermore, in real-world situations, individuals tend
to alter their behavior, as a result of the experience gained through the interactions and
through observation of the behaviors of other people. These aspects of human behavior
can be better captured using concepts from evolution game theory [Smi72], which studies
the dynamics of the behavior of large populations of players that repeatedly play a game,
subjected to evolutionary changes. The player behaviors adjust over time, as a result of
experience gained through repetitions.
5.5.2
Epidemic diﬀusion models
The spread of information through individuals in a social network bears similarities to
the transmission of infectious diseases through the population. As a result, various models
used in epidemiology have been adopted in information diﬀusion studies. Epidemic models
have been developed in order to study how infectious diseases break out and spread over a
population [KM27, Bai57, PSV01]. These models describe the disease cycle in a host, using
diﬀerent terms to characterize each stage; for example, when the host is susceptible to the
disease (S), becomes infected/infectious (I), or has recovered/is no longer infectious (R). In
the Susceptible-Infected-Recovered (SIR) epidemic model, a person is initially susceptible to
the disease, then becomes infected, and ﬁnally recovers. In the recovered stage, the person
becomes immune to the disease and is no longer able to infect others. On the contrary, in the
Susceptible-Infected-Recovered-Susceptible (SIRS) epidemic model, the recovered individuals
become susceptible to the disease again. According to the Susceptible-Infected-Susceptible
(SIS) model, the persons do not become immune to the disease after the infection, but they
return directly to the susceptible stage. Finally, Susceptible-Infected (SI) models are used
to describe fatal diseases, where the infected individual never returns to the recovered or
susceptible stages.
In a social interaction context, the notions of susceptibility, infection, and recovery can
be adopted to describe the relationship between a person and the mode of being spread

138
Graph-Based Social Media Analysis
(e.g., information). For example, a susceptible person does not yet know the information,
an infected person gets to know and can transmit the information, while a recovered person
forgets it or has lost interest in it. In [WYH+13], two epidemiology-driven models are pro-
posed, which extend the SI and the SIR models respectively, for the analysis of information
diﬀusion in online social networks.
The suitability of the SIR model for large real-world networks is investigated in [BLT12].
[OR13] explores how the structure of the underlying network aﬀects the diﬀusion process,
using the SIR model. In [YQZC12] a SIR model is used to study the information spread
taking place in a conjoint framework, including conventional communication, as well as
interaction in online social networks. Another work adopting a SIR model can be found
in [WSC11], where the spread of violent or extremist topics in social media is explored.
The population is divided into three diﬀerent classes: the susceptible class consists of users
who are interested in a topic and read relevant posts. The infected users are those who
write posts or answer to threads, once they have read posts associated to a topic. Finally,
the recovered class consists of authors whose posts become uninfective. In [XL10], the SIS
model was used to simulate the diﬀusion process in a synthetic Barab´asi-Albert network
and in Facebook, in order to explore their diﬀerences. The proposed method takes into con-
sideration the decaying infectiousness of a topic, as well as the generation of new sub-topics
during diﬀusion. Another method utilizing the SIS model for social network simulation is
proposed in [SKM09], to study the problem of discovering inﬂuential nodes in a network.
5.5.3
Threshold diﬀusion models
Threshold diﬀusion models were originally proposed by Granovetter [Gra78] and
Schelling [Sch78] and assume that the adoption of an innovation by an individual depends
on the number of individuals that have already adopted it. A social network can be mod-
eled by a graph G = (V, E), where nodes represent individuals and edges their relationships
of inﬂuence. The edge weights express the probability that a node inﬂuences its neighbor.
Furthermore, each node is assigned a threshold value. A node is considered to be active or
inactive, if it has or has not yet adopted the innovation, respectively. Each node is activated
when the fraction of its active neighbors surpasses its threshold value.
The Linear Threshold (LT) diﬀusion model [Gra78] has received much attention. Ac-
cording to this model, each edge is assigned a weight wij representing the probability that
the node i is inﬂuenced by the node j, with P
j∈Ni wij ≤1. Additionally, each node i is
assigned a threshold value θi, that represents the fraction of its neighbors required to have
adopted the innovation, so that i adopts it. The Linear Threshold model assumes that a
subset of nodes S ⊂V are initially active. The diﬀusion process subsequently unfolds at
discrete time steps as follows: at time t, the nodes that were activated at t−1 remain active,
while an inactive node i is activated if:
X
j active neighbor of i
wij ≥θi .
(5.5.1)
Given the threshold values θi, the LT diﬀusion process is deterministic. However, this hy-
pothesis can be lifted [KKT03], if the thresholds are selected uniformly at random in the
range [0, 1]. In [KKT03], a General Threshold (GT) model is introduced, which constitutes
an extension of the LT model. In this model, the threshold θi of a node i is substituted by
a monotone activation function fi, deﬁned on the set of its neighboring active nodes, which
takes values in [0, 1]. Taking the aforementioned function into account and following the
diﬀusion procedure discussed above, a node i becomes active at time step t, if fi(N) ≥θi,
where N denotes the set of its neighbors that are active at time t −1. Threshold θi is
uniformly chosen at random in the interval [0, 1].

Label Propagation and Information Diﬀusion in Graphs
139
A Linear Threshold model is also adopted in [Wat02], where global cascades on random
networks are studied. Global cascades are cascades that occur infrequently, are triggered
by a small seed set of nodes and aﬀect a large portion of the network. The emergence
of a global cascade and its governing mechanism have large complexity and vary from
system to system. Global cascades can be observed in social and economic systems, as
well as in cascades occurring upon failures in physical infrastructure networks. In [Wat02],
global cascades are modeled using a sparse random network of interacting individuals, whose
decisions are determined by their neighbors’ decisions, in accordance to a threshold rule.
Initially, every individual is in state 0 (inactive). At time t = 0, a small fraction of nodes
in state 1 (active) perturbs the graph. At subsequent time steps, the nodes update their
states, according to the threshold rule, in a random order. It is assumed that every node in
the random graph can be adjacent to at most one seed member, an approximation that is
exact only in an inﬁnite network. As consequence, the seed can grow only if at least one of
its neighbors has a threshold θ, such that θ ≤1/k. These nodes are called vulnerable. The
success of a cascade depends more on the number and the connectivity of the vulnerable
nodes and less on the number of seeds. For a global cascade to occur, it is required that the
largest connected vulnerable cluster occupies a ﬁnite fraction of an inﬁnite network.
In [AOY11], a stochastic linear threshold model is proposed, that extends the LT model
by capturing path dependence, which refers to the fact that the outcome of the diﬀusion may
depend heavily on minor shocks or insigniﬁcant events. In contrast to the LT model, in the
proposed model, individuals do not necessarily adopt the innovation, if the fraction of their
active neighbors surpasses its threshold value. Alternatively, there is a non-zero probability
that the individual rejects the adoption. Therefore, at time step t, if the fraction of the active
neighbors of a node i is greater than a threshold value, node i will consider the adoption
of the innovation and subsequently decide whether to adopt it or not. The outcome of this
decision is regarded as a Bernoulli trial, with a parameter p ∈[0, 1] denoting the likelihood
that the node adopts the innovation, conditioned upon the consideration.
The behavior of the LT model, when applied on a multi-layered social network was
studied in [MKJ13]. Multi-layered social networks (MSNs), also referred to as multiplex
networks, are networks consisting of more than one layer, each of them representing a
diﬀerent type of interaction between individuals, thus providing a more natural and realistic
model of communication in a social network. The nodes are shared between the layers,
while their linking varies according to the relationships between the nodes, given the type
of interaction corresponding to the layer. In [MKJ13], the authors study how diﬀusion in
the multi-layered network is inﬂuenced by the layer number and type, using the LT model.
Finally, a generalization of the LT model is introduced in [PBS10]. In contrast to other
diﬀusion methods, the proposed algorithm considers multiple cascades on the graph, while
the nodes can switch between them. Speciﬁcally, the proposed model assumes that K dif-
ferent cascades propagate in a graph G = (V, E). Each node can be either activated by one
of the cascades or be inactive, thus being in one of K +1 states. The cascades are simulated
using a stochastic graph coloring process, which is proven to be equivalent to a rapidly
mixing Markov chain.
5.5.4
Cascade diﬀusion models
Similarly to threshold models, cascade models rely on the hypothesis that the decisions
of individuals are inﬂuenced by the decisions of other people they interact with. Again,
cascade models can be represented with graphs, where the nodes are characterized as active,
when they have adopted the innovation, and inactive otherwise. A commonly used model is
the Independent Cascade (IC) model [GLM01]. According to this model, each node i that
becomes active at the (discrete) time instance t is given a single chance to activate each

140
Graph-Based Social Media Analysis
of its currently inactive neighbors j with a probability pji. Node j will become activated
at time t + 1, if the attempt of i, or of any other neighbor of j that was also activated at
time t, is successful. Once node i has made an attempt to activate its inactive neighbors, it
remains active, but it cannot continue activating its neighboring nodes.
A generalization of the IC model is presented in [KKT03]. Unlike the IC model, the
proposed General Cascade (GC) model takes into account previous attempts for activating
of a node performed by its neighbors. More speciﬁcally, the probability that the node i
activates a neighbor j is calculated by an incremental function pj(i, S) ∈[0, 1], where S
represents the set of active neighbors of j that have already attempted to activate j. It
should be noted that, similar to the IC model, at time step t, all the active neighbors of
j attempt to activate it arbitrarily. Therefore, the order in which the attempts take place
does not aﬀect the result of the diﬀusion process. Additionally, if a node i activates node j,
further contaminations (activations) by the rest of the neighbors at the same time have no
eﬀect. As a result, it is unknown in these models, who contaminated whom. The algorithm
proposed in [GRLK10] raises this assumption, by considering the diﬀusion process in the
continuous time domain. Notions are introduced, such as the hit time tj, describing the
time when a node j is contaminated by a speciﬁc contagion c initiated at some node and
the incubation time, that is, the time it takes for a contaminated node v to contaminate
its neighbors after the hit time. Then the problem of tracing the paths of diﬀusion in the
network is explored.
According to the Independent Cascade (IC) model, each newly activated node is given
exactly one chance to activate its inactive neighbors. However, in real-world scenarios, this
assumption does not hold, since social interactions are history-dependent. For example, a
consumer is usually exposed multiple times to the same advertisement, before he/she decides
to buy a product. This fact is taken into consideration in the History Sensitive Cascade
Model (HSCM) proposed in [Zha09], which is a modiﬁcation of the IC model. The HSCM
model allows the newly activated nodes more than one chance to inﬂuence their neighbors.
5.5.5
Inﬂuence maximization
In many applications, the objective of information diﬀusion is to inﬂuence the largest
possible population within a social network. Examples of such cases include marketing
strategies for reaching a wide public to sell products to, pre-election campaigns, or the
promotion of certain concepts or ideas, in general. The maximization of inﬂuence problem
seeks to answer the following question: which set of individuals should the diﬀusion process
start from, in order to obtain the maximum spread? In terms of label propagation on
multimedia data, this is related to determining the initial set of labeled data that optimize
propagation [ZNP13].
In an early study [DR01], inﬂuence maximization is studied from a viral marketing
perspective. Viral marketing aims at increasing product sales, by exploiting the inﬂuence
between customers. Instead of treating all the customers in the same way, a company may
seek a suitable group of customers, who are the most inﬂuential, to promote a marketing plan
to. As opposed to the intrinsic value of a customer, measured by the expected proﬁt from
directly selling to him/her, the notion of the network value of a customer is introduced,
describing the expected proﬁt from sales to other individuals that indirectly result from
selling to a particular customer. This is a consequence of the inﬂuence one customer has on
other customers, and the inﬂuence that those customers have on others and so on.
The network value of a customer depends on a number of factors. First, high network
connectivity plays an important role. However, successful product marketing also depends
on the positive or negative rating of the customer. Second, the inﬂuence between customers
must be asymmetric. A customer with high network value should inﬂuence others much

Label Propagation and Information Diﬀusion in Graphs
141
more than others inﬂuence him. Third, the network value of a customer is not related just
to his/her immediate neighbors, but these neighbors can, in turn, inﬂuence other people.
This implies that, even if a customer is not highly connected, he may have a strong network
value, if his/her neighbors are highly connected. In [DR01] the market is modeled as a
probabilistic model of interaction, utilizing a Markov random ﬁeld. Heuristics are used for
choosing customers with a large overall eﬀect on the network. A method is proposed to infer
the necessary inﬂuence data. A later work [RD02] extends the ideas presented in [DR01], by
adopting a linear model to represent the inﬂuence between the nodes, which signiﬁcantly
simpliﬁes the computations.
Another approach to inﬂuencing maximization is introduced in [KKT03], by ﬁnding
a set of k initially active nodes that yield the largest expected cascade. In contrast to
[DR01], which adopted descriptive models for node interaction, operational models are
used in [KKT03], such as the Linear Threshold and Independent Cascade models discussed
earlier, that represent step-by-step adoption dynamics. It is proven that the optimization
problem of discovering the nodes that maximize the inﬂuence is NP-hard. Therefore, the
inﬂuence maximization problem is addressed by a greedy hill-climbing algorithm, developed
in a general framework based on submodular functions. In later work [KKT05], it is proven
that the maximization problem can be extended to a very general cascade model, named
Decreasing Cascade model. According to the Independent Cascade model, the probability
pj(i) that a node j is activated by an active neighbor i is independent of any previous failed
attempts conducted by other nodes. Conversely, in the General Cascade model, the inﬂu-
ence probabilities depend on previous attempts for node activation by its active neighbors.
In this case, probabilities are expressed by an increasing function pj(i, S) ∈[0, 1], where
S represents the set of active neighbors of j that have already attempted to activate it.
According to the Decreasing Cascade model, the functions pj(i, S) are considered to be
non-increasing in S, which means:
pj(i, S) ≥pj(i, T ) for S ⊆T ,
(5.5.2)
where T denotes the set of nodes. The Decreasing Cascade model assumes that the prob-
ability of active node i activating an inactive node j decreases if other nodes have already
attempted to activate it, since node j is more “marketing saturated.”
In [LKG+07], a problem similar to inﬂuence maximization is studied, namely that of
outbreak detection. This constitutes selecting a set of nodes in a network, such that the
detection of the spread of contamination or information is achieved as quickly as possible.
The method proposed in [LKG+07] takes into consideration the submodularity property of
the inﬂuence maximization objective, in order to reduce the number of evaluations of the
inﬂuence spread function and to obtain solutions that are close to optimal ones. The pro-
posed algorithm, called Cost-Eﬀective Lazy Forward selection (CELF) performs 700 times
faster than the simple greedy algorithm of [KKT03]. In [CWY09] and later in [CWW10], a
new scalable and tunable heuristic is proposed, that is capable of handling inﬂuence maxi-
mization in large-scale social networks, which outperforms the algorithms of [KKT03] and
[LKG+07]. Other inﬂuence maximization methods that optimize the CELF algorithm, can
be found in [GLL11a] and [GLL11b].
In [CYZ10], a method for scalable inﬂuence maximization under the Linear Threshold
model is proposed, which adopts Directed Acyclic Graphs (DAGs) in order to propagate
inﬂuence in linear time. The proposed algorithm for inﬂuence maximization, called a Local
Directed Acyclic Graph (LDAG), constructs a local acyclic graph for each node i. Subse-
quently, inﬂuence is propagated from the seed set to node i only through its local DAG,
according to the Linear Threshold model.
In [KS06], two new diﬀusion models are proposed, based on the Independent Cascade
model and is studied the problem of inﬂuence maximization under these models. According

142
Graph-Based Social Media Analysis
to the ﬁrst one, called the Shortest-Path Model (SPM), each node is activated only through
the shortest paths from the seed set, i.e., the nodes that are initially active. More speciﬁcally,
denoting the seed set by A and the graph distance from node j to node i by d(j, i), the
distance between A and node i is deﬁned as d(A, i) = minj∈A d(j, i). Therefore, in the
Shortest-Path model, each node can become active only at time step t = d(A, i). In the
second model introduced in [KS06], an extension to the SPM model called the SP1M model
is proposed, where each node has a chance to be activated only at time steps t = d(A, i) or
t = d(A, i) + 1.
In [VK11], the spread of inﬂuence is studied, by adopting the Linear Threshold model.
Given an initial activation set, recursive expressions for the expected number of the ﬁnally
inﬂuenced nodes are derived, which can be interpreted using acyclic path probabilities
in discrete Markov chains. Furthermore, the aforementioned study is applied on diﬀerent
network topologies, such as star, ring, and mesh to derive the optimal initial set for these
networks.
Another problem closely related to inﬂuence maximization is the diﬀusion of multiple,
competing innovations within a network. A real-world scenario reﬂecting this case is the
competition between companies selling products of the same type, in a quest to dominate
the market. In [BKS07], a model is introduced which modiﬁes the IC model, so as to
simulate multiple competing innovations. In the proposed model the notion of activation
time is introduced, in order to take into account the multiple activation attempts on a node,
taking place at the same time step. Therefore, an inactive node i adopts the innovation
being spread by the ﬁrst neighboring node that managed to activate it. In [KOW08], the
diﬀusion of competing rumors was studied, using a game theoretical approach. Other game
theoretical approaches to the competing diﬀusion processes problem can be also found in
[AFPT10, TAM12, SA12, GK12].
Although in applications such as viral marketing campaigns the objective is to promote a
product or an idea to as many individuals as possible, other applications aim at minimizing
the inﬂuence spread. Such an example is the limitation of misinformation. This problem
bears similarities to the diﬀusion of competing innovations in a network, where the “good”
information competes against misinformation. The techniques used in these scenarios are
similar to those of inﬂuence maximization. The problem of limitation of misinformation can
be deﬁned as discovering a small set of nodes, such that if “good” information is disseminated
to them, the eﬀect of the misinformation is minimized, i.e., it reaches the smallest possible
number of individuals [BAEA11, NYTE12]. In [BAEA11], a Multi-Campaign Independent
Cascade Model (MCICM) and a Campaign-Oblivious Independent Cascade Model (COICM)
are introduced, based on the Independent Cascade model, which describe the diﬀusion of
two competing campaigns (a “good” and a “bad” one), that take place simultaneously in
a network. In contrast to the Independent Cascade model, the proposed methods consider
the order in which activation attempts occur, when more than two active neighbors try to
activate a node i at the same time step. If multiple nodes try to activate an inactive node
at the same time step, the “good” campaign is prioritized over the “bad”. In the MCICM
model, each newly activated node is given a single chance to activate its inactive neighbor i
in one of the campaigns, with diﬀerent probabilities for the two campaigns. In the COICM
model, which is similar to the one proposed in [BKS07], the probabilities that a node is
activated are the same for the two campaigns. Finally, in [KG13], evolutionary game theory
is employed in order to study the spread of misinformation in online social networks.
5.5.6
Cross-Media information diﬀusion
As already mentioned, online social media play a key role in the spread of trends, news,
and opinions. Furthermore, upon the occurrence of an important event, for example elec-

Label Propagation and Information Diﬀusion in Graphs
143
tions, natural disasters, or sports events, covered by conventional broadcast audiovisual (AV)
media, increased user activity in social media is observed. Such activity reﬂects people’s im-
mediate reactions to the broadcasted event, as well as the process of information/concept
diﬀusion from broadcast media to social media. As an example, during the live broadcasting
of a football game on television, relevent tweets emerge in Twitter, while increased tweeting
activity takes place upon highlights of the game. By monitoring and analyzing this process,
trending and shifting in public opinion can be discovered. This kind of information can be
useful in various scenarios, such as in policy making, journalistic investigation, and market
analysis.
Jointly broadcasted AV content analysis and social media analysis are relatively new re-
search areas. Work in this ﬁeld mainly targets the semantic description of large-scale broad-
casting events, such as political debates or speeches, through social media analysis, primarily
tweets published by viewers, while watching the event [SKC09, DNKS10, HIF13, SCA14].
To this end, statistical tweet analysis is performed, in order to detect the major trends and
changes in the semantic structure and content of the event. Moreover, textual tweet analysis
is performed, in order to extract the semantic concepts of the event, as well as the sentiments
caused to the audience. The aforementioned works employ hashtags and time information
for relating tweets with speciﬁc video timestamps. Furthermore, several methods have been
developed for describing broadcasted AV content, by combining transcribed text and visual
metadata [CMC05, IDF+05, GWZ+13, Xu14]. Information describing social network activ-
ity during a broadcasted event (e.g., textual/visual information, likes or re-tweets), can be
combined with broadcasted AV content description, as shown in Figure 5.5.1. In this way,
e.g., the tweet number can be mapped on the audiovisual content timeline, together with
actor/speaker timelines. Moreover, audiovisual and social media content description can be
combined in a joint rich AV and social media content descriptor.
5.5.7
Other applications of information diﬀusion
Diﬀusion methods ﬁnd application in collaborative ﬁltering algorithms. These methods
are commonly employed by recommendation systems, in order to make predictions about
the interests of a user, by collecting information regarding the preferences and tastes of other
users, expressed through ratings [TH01]. Recommendation systems focus on algorithms for
matching users based on their preferences, and weighting the interests of users with similar
taste to produce a recommendation for the information seeker. In [SZZZ10], a recommen-
dation model, which uses ternary relationships among users, objects and tags is proposed.
A new measure of user similarity is introduced, which incorporates user preferences of both
collected objects and used tags. This similarity measure is calculated using a diﬀusion-based
process.
Furthermore, information diﬀusion methods are adopted for community detection. In
[AHH11] and [HAHH12] game theory is adopted for the detection of overlapping communi-
ties, i.e., communities whose nodes may belong to more than one community simultaneously.
Community formation is considered as an iterative game, where players try to join commu-
nities consisting of individuals that share similar interests with them. In [BBM13], another
method for community detection is proposed, which is based on information cascades in a
social graph.
Another ﬁeld of study of information diﬀusion is the citation network. In [STA09], cita-
tion networks of publications in computer science are studied from an information diﬀusion
perspective. The structural features of the information paths through these networks, as
well as their impact on the information ﬂow are analyzed. Additionally, variations in in-
formation diﬀusion for speciﬁc subsets of citation networks are investigated: books versus
conference and journal articles, as well as coverage of diﬀerent computer science domains,

144
Graph-Based Social Media Analysis
FIGURE 5.5.1: Mapping social media descriptions on broadcasted AV content timeline.

Label Propagation and Information Diﬀusion in Graphs
145
and diﬀerent time periods. The basic consideration is that many citations are evidence of
information ﬂow from one article (and its authors), to another one.
Finally, information diﬀusion methods ﬁnd application in emergency situation manage-
ment, where mass media function as channels for information spread. In the last decade,
Internet has played a dominant role in the diﬀusion of news, especially in cases of emer-
gency. In [YLL09], challenges for information diﬀusion in the area of emergency events
management are discussed. An overview of diﬀusion models, such as threshold and cascade
models is provided. Additionally, four types of emergency situations are investigated using
the aforementioned diﬀusion models: natural disasters, technological disasters, public health
incidents, and security incidents. In [HTW+12], a diﬀusion model is employed in order to
study information cascades on Twitter, which occur as a response to an actual crisis event
and its accompanying alerts or warning messages from emergency managers. The types of
information exchanged during a crisis situation are deﬁned. Furthermore, the way messages
spread among the users on Twitter, including the kinds of information cascades or patterns
are observed. These patterns provide knowledge on information ﬂow through the network.
The proposed method can reveal properties of the diﬀusion process during the emergency
event, oﬀering emergency managers more eﬀective ways to facilitate the spreading through
social media or to impede the spread of information. In [ZXP+11], diﬀusion of information
regarding disaster preparation in Twitter was studied. User’s re-tweeting behavior was ana-
lyzed by studying the factors that might aﬀect their decisions, such as content and network
inﬂuences, as well as time-related factors.
5.6
Conclusions
In this chapter we discussed graph-based label propagation, with a focus on multime-
dia applications. Label propagation is a powerful semi-supervised classiﬁcation method. In
contrast to the classical supervised classiﬁcation methods, label propagation methods re-
quire only a few data with known labels from which label inference commences. The basic
steps of all the label propagation methods were reviewed, namely graph construction and
label inference. Graph construction methods were grouped into three categories, according
to the method employed for calculating the edge weights. A common property of all graph
construction methods is that they employ local information. This way, the constructed
graphs are more robust to noisy data and outliers. Label inference methods are divided into
groups, according to the graph type employed (one or more graphs, hypergraphs, etc.) and
the label inference rule (iterative, manifold regularization, etc.). Label inference methods
employ both local and holistic data information for robust propagation performance. Since
label propagation constitutes a diﬀusion process, several topics regarding information dif-
fusion in social networks have been also discussed. Particular emphasis was placed on the
review of the most important models for information diﬀusion, as well as on the inﬂuence
maximization problem.
Label propagation methods have been successfully employed in several multimedia-
related applications, such as annotation of multimedia content and recommendation sys-
tems. Nevertheless, several limitations need to be resolved, such as the insuﬃciency of the
available labeled data, the curse of dimensionality, as well as the choice of appropriate
distance measures and graph construction techniques for the data.

146
Graph-Based Social Media Analysis
Regarding information diﬀusion, certain challenges have yet to be overcome, such as the
selection of the right diﬀusion model for representing real-world networks and the develop-
ment of models that reﬂect realistic properties of social interactions.
Bibliography
[AF67]
M. Alonso and E. J. Finn. Fundamental University Physics. Addison-
Wesley Publishing, 1967.
[AFPT10]
N. Alon, M. Feldman, A. D. Procaccia, and M. Tennenholtz. A Note on
Competitive Diﬀusion Through Social Networks. Information Processing
Letters, 110(6):221–225, 2010.
[AHH11]
H. Alvari, S. Hashemi, and A. Hamzeh. Detecting Overlapping Commu-
nities in Social Networks by Game Theory and Structural Equivalence
Concept. In Proc. 3rd International Conference on Artiﬁcial Intelligence
and Computational Intelligence, volume II, pages 620–630, 2011.
[AHP05]
A. Argyriou, M. Herbster, and M. Pontil. Combining Graph Laplacians
for Semi-Supervised Learning. In Proc. Advances in Neural Information
Processing Systems 18, pages 67–74, 2005.
[AOY11]
D. Acemoglu, A. Ozdaglar, and E. Yildiz. Diﬀusion of innovations in social
networks. In Proc. 50th IEEE Conference on Decision and Control and
European Control Conference, pages 2329–2334, 2011.
[Azr07]
A. Azran. The Rendezvous Algorithm: Multiclass Semi-supervised Learn-
ing with Markov Random Walks. In Proc. 24th International Conference
on Machine Learning, pages 49–56, 2007.
[BAEA11]
C. Budak, D. Agrawal, and A. El Abbadi. Limiting the Spread of Misin-
formation in Social Networks. In Proc. 20th International Conference on
World Wide Web, pages 665–674, 2011.
[Bai57]
N. T. J. Bailey. The mathematical theory of epidemics. Griﬃn, 1957.
[BBC+94]
R. Barrett, M. Berry, T. F. Chan, J. Demmel, J. Donato, J. Dongarra,
V. Eijkhout, R. Pozo, C. Romine, and H. V. der Vorst. Templates for the
Solution of Linear Systems: Building Blocks for Iterative Methods. SIAM,
2nd edition, 1994.
[BBM04]
M. Bilenko, S. Basu, and R. Mooney. Integrating constraints and metric
learning in semi-supervised clustering. In Proc. 21st International Con-
ference on Machine Learning, pages 11–18, 2004.
[BBM13]
N. Barbieri, F. Bonchi, and G. Manco. Cascade-based Community De-
tection. In Proc. 6th ACM International Conference on Web Search and
Data Mining, pages 33–42, 2013.
[BC01]
A. Blum and S. Chawla.
Learning from Labeled and Unlabeled Data
using Graph Mincuts. In Proc. 18th International Conference on Machine
Learning, pages 19–26, 2001.

Label Propagation and Information Diﬀusion in Graphs
147
[BDR06]
Y. Bengio, O. Delalleau, and N. L. Roux.
Label Propagation and
Quadratic Criterion. In Semi-Supervised Learning, pages 193–216. MIT
Press, 2006.
[Ben80]
J. L. Bentley. Multidimensional Divide-and-Conquer. Communications of
the ACM, 23(4):214–229, 1980.
[Ber99]
D. P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, 2nd edition,
1999.
[BG05]
I. Borg and P. J. F. Groenen. Modern Multidimensional Scaling. Springer,
2005.
[BGC10]
V. Badrinarayanan, F. Galasso, and R. Cipolla. Label propagation in video
sequences. In Proc. IEEE Conference on Computer Vision and Pattern
Recognition, pages 3265–3272, 2010.
[Bie87]
H. J. Bierens. Kernel estimators of regression functions. In Advances in
Econometrics: 5th World Congress of the Econometric Society, volume 1,
pages 99–144, 1987.
[BKS07]
S. Bharathi, D. Kempe, and M. Salek. Competitive Inﬂuence Maximiza-
tion in Social Networks. In Proc. 3rd International Conference on Internet
and Network Economics, pages 306–311, 2007.
[BLL+14]
S. Bourigault, C. Lagnier, S. Lamprier, L. Denoyer, and P. Gallinari.
Learning Social Network Embeddings for Predicting Information Diﬀu-
sion.
In Proc. 7th ACM International Conference on Web Search and
Data Mining, pages 393–402, 2014.
[BLRR04]
A. Blum, J. Laﬀerty, M. R. Rwebangira, and R. Reddy. Semi-supervised
learning using randomized mincuts. In Proc. 21st International Conference
on Machine Learning, pages 13–20, 2004.
[BLT12]
D. F. Bernardes, M. Latapy, and F. Tarissan. Relevance of SIR Model
for Real-world Spreading Phenomena: Experiments on a Large-scale P2P
System. In Proc. IEEE/ACM International Conference on Advances in
Social Networks Analysis and Mining, pages 327–334, 2012.
[Blu93]
L. E. Blume. The Statistical Mechanics of Strategic Interaction. Games
and Economic Behavior, 5:387–424, 1993.
[BMN04]
M. Belkin, I. Matveeva, and P. Niyogi. Regularization and semi-supervised
learning on large graphs. In Learning Theory, Lecture Notes in Computer
Science, volume 3120, pages 624–638. Springer, 2004.
[BN03]
M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction
and data representation. Neural computation, 15(6):1373–1396, 2003.
[BNMY11]
B. K. Bao, B. Ni, Y. Mu, and S. Yan.
Eﬃcient Region-Aware Large
Graph Construction Towards Scalable Multi-Label Propagation. Pattern
Recognition, 44(3):598–606, 2011.
[BNS06]
M. Belkin, P. Niyogi, and V. Sindhwani. Manifold Regularization: A Ge-
ometric Framework for Learning from Labeled and Unlabeled Examples.
Journal of Machine Learning Research, 7:2399–2434, 2006.

148
Graph-Based Social Media Analysis
[Bro97]
A. Z. Broder. On the resemblance and containment of documents. In
Proc. Compression and Complexity of Sequences, pages 21–29, 1997.
[BSS+08]
S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik, S. Kumar,
D. Ravichandran, and M. Aly.
Video suggestion and discovery for
YouTube: taking random walks through the view graph. In Proc. 17th
international conference on World Wide Web, pages 895–904, 2008.
[CC10]
A. Y. C. Chen and J. J. Corso.
Propagating multi-class pixel labels
throughout video frames. In Proc. Western New York Image Processing
Workshop, pages 14–17, 2010.
[CC11]
A. Y. C. Chen and J. J. Corso. Temporally consistent multi-class video-
object segmentation with the Video Graph-Shifts algorithm.
In Proc.
IEEE Workshop on Applications of Computer Vision, pages 614–621,
2011.
[CCC11]
D. Coppi, S. Calderara, and R. Cucchiara.
People appearance tracing
in video by spectral graph transduction.
In Proc. IEEE International
Conference on Computer Vision Workshops, pages 920–927, 2011.
[CFS09]
J. Chen, H. Fang, and Y. Saad. Fast Approximate kNN Graph Construc-
tion for High Dimensional Data via Recursive Lanczos Bisection. Journal
of Machine Learning Research, 10:1989–2012, 2009.
[CGK+97]
C. S. Chekuri, A. V. Goldberg, D. R. Karger, M. S. Levine, and C. Stein.
Experimental Study of Minimum Cut Algorithms. In Proc. 8th Annual
ACM-SIAM Symposium on Discrete Algorithms, pages 324–333, 1997.
[CJ04]
A. Corduneanu and T. Jaakkola. Distributed information regularization
on graphs. Proc. Neural Information Proccessing Systems, pages 297–304,
2004.
[CMC05]
S. F. Chang, R. Manmatha, and T. S. Chua. Combining text and audio-
visual features in video indexing. In Proc. IEEE International Conference
on Acoustics, Speech, and Signal Processing, volume 5, pages 1005–1008,
2005.
[CMYC10]
X. Chen, Y. Mu, S. Yan, and T. S. Chua. Eﬃcient Large-scale Image
Annotation by Probabilistic Collaborative Multi-label Propagation.
In
Proc. International Conference on Multimedia, pages 35–44, 2010.
[CTLC12]
X. Chen, Z. Tong, H. Liu, and D. Cai.
Metric learning with two-
dimensional smoothness for visual analysis. In Proc. IEEE Conference
on Computer Vision and Pattern Recognition, pages 2533–2538, 2012.
[CWW10]
W. Chen, C. Wang, and Y. Wang. Scalable Inﬂuence Maximization for
Prevalent Viral Marketing in Large-scale Social Networks. In Proc. 16th
ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, pages 1029–1038, 2010.
[CWY09]
W. Chen, Y. Wang, and S. Yang.
Eﬃcient Inﬂuence Maximization in
Social Networks. In Proc. 15th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pages 199–208, 2009.

Label Propagation and Information Diﬀusion in Graphs
149
[CY00]
F. Chung and Y. Yau. Discrete Green’s Functions. Journal of Combina-
torial Theory, Series A, 91(12):191 – 214, 2000.
[CYZ10]
W. Chen, Y. Yuan, and L. Zhang. Scalable Inﬂuence Maximization in
Social Networks Under the Linear Threshold Model. In Proc. IEEE In-
ternational Conference on Data Mining, pages 88–97, 2010.
[CZW+09]
G. Chen, J. Zhang, F. Wang, C. Zhang, and Y. Gao. Eﬃcient multi-label
classiﬁcation with hypergraph regularization. In Proc. IEEE Conference
on Computer Vision and Pattern Recognition, pages 1658–1665, 2009.
[DD08]
J. V. Davis and I. S. Dhillon. Structured Metric Learning for High Dimen-
sional Problems. In Proc. 14th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pages 195–203, 2008.
[DG08]
J. Dean and S. Ghemawat. MapReduce: Simpliﬁed Data Processing on
Large Clusters. Communications of the ACM, 51(1):107–113, 2008.
[DKJ+07]
J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. Dhillon.
Information-
theoretic metric learning. In Proc. 24th International Conference on Ma-
chine Learning, pages 209–216, 2007.
[DKS09]
S. I. Daitch, J. A. Kelner, and D. A. Spielman. Fitting a graph to vector
data. In Proc. 26th Annual International Conference on Machine Learn-
ing, pages 201–208, 2009.
[DML11]
W. Dong, C. Moses, and K. Li. Eﬃcient K-nearest Neighbor Graph Con-
struction for Generic Similarity Measures.
In Proc. 20th International
Conference on World Wide Web, pages 577–586, 2011.
[DNKS10]
N. Diakopoulos, M. Naaman, and F. Kivran-Swaine.
Diamonds in the
rough: Social media visual analytics for journalistic inquiry. In Proc. IEEE
Symposium on Visual Analytics Science and Technology, pages 115–122,
Oct 2010.
[DR01]
P. Domingos and M. Richardson. Mining the Network Value of Customers.
In Proc. 7th ACM SIGKDD International Conference on Knowledge Dis-
covery and Data Mining, pages 57–66, 2001.
[DTC10]
P. S. Dhillon, P. P. Talukdar, and K. Crammer. Inference Driven Met-
ric Learning (IDML) for Graph Construction. Technical Report (CIS),
University of Pennsylvania, 2010.
[DY08]
L. Ding and A. Yilmaz. Image segmentation as learning on hypergraphs.
In Proc. 7th International Conference on Machine Learning and Applica-
tions, pages 247–252, 2008.
[Ell93]
G. Ellison. Learning, Local Interaction and Coordination. Econometrica,
61(5):1047–1071, 1993.
[EW01]
A. Elisseeﬀand J. Weston. A kernel method for multi-labelled classiﬁca-
tion. Proc. Advances in Neural Information Processing Systems, 14:681–
687, 2001.
[Fod02]
I. Fodor. A Survey of Dimension Reduction Techniques. Technical Report,
Center for Applied Scientiﬁc Computing, Lawrence Livermore National
Laboratory, 2002.

150
Graph-Based Social Media Analysis
[GCCVMMC08] L. Gomez-Chova, G. Camps-Valls, J. Munoz-Mari, and J. Calpe. Semisu-
pervised Image Classiﬁcation With Laplacian Support Vector Machines.
IEEE Geoscience and Remote Sensing Letters, 5(3):336 –340, 2008.
[GIM99]
A. Gionis, P. Indyk, and R. Motwani. Similarity Search in High Dimen-
sions via Hashing. In Proc. 25th International Conference on Very Large
Data Bases, pages 518–529, 1999.
[GK12]
S. Goyal and M. Kearns. Competitive Contagion in Networks. In Proc.
44th Annual ACM Symposium on Theory of Computing, pages 759–774,
2012.
[GLL11a]
A. Goyal, W. Lu, and L. V. S. Lakshmanan. CELF++: Optimizing the
Greedy Algorithm for Inﬂuence Maximization in Social Networks. In Proc.
20th International Conference Companion on World Wide Web, pages 47–
48, 2011.
[GLL11b]
A. Goyal, W. Lu, and L. V. S. Lakshmanan.
SIMPATH: An Eﬃcient
Algorithm for Inﬂuence Maximization Under the Linear Threshold Model.
In Proc. IEEE 11th International Conference on Data Mining, pages 211–
220, 2011.
[GLM01]
J. Goldenberg, B. Libai, and E. Muller. Talk of the Network: A Complex
Systems Look at the Underlying Process of Word-of-Mouth. Marketing
Letters, 12(3):211–223, 2001.
[GMVS09]
M. Guillaumin, T. Mensink, J. Verbeek, and C. Schmid. TagProp: Dis-
criminative metric learning in nearest neighbor models for image auto-
annotation. In Proc. IEEE 12th International Conference on Computer
Vision, pages 309–316, 2009.
[Gra78]
M. Granovetter. Threshold models of collective behavior. American Jour-
nal of Sociology, 83(6):1420–1433, 1978.
[Gra06]
L. Grady. Random Walks for Image Segmentation. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 28(11):1768–1783, 2006.
[GRHS04]
J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov. Neighbour-
hood components analysis. In Proc. Advances in Neural Information Pro-
cessing Systems, volume 17, pages 513–520, 2004.
[GRLK10]
M. Gomez Rodriguez, J. Leskovec, and A. Krause. Inferring Networks
of Diﬀusion and Inﬂuence. In Proc. 16th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pages 1019–1028,
2010.
[GWZ+13]
Y. Gao, M. Wang, Z. J. Zha, J. Shen, X. Li, and X. Wu. Visual-Textual
Joint Relevance Learning for Tag-Based Social Image Search. IEEE Trans-
actions on Image Processing, 22(1):363–376, 2013.
[HAHH12]
A. Hajibagheri, H. Alvari, A. Hamzeh, and S. Hashemi. Community Detec-
tion in Social Networks Using Information Diﬀusion. In Proc. IEEE/ACM
International Conference on Advances in Social Networks Analysis and
Mining, pages 702–703, 2012.

Label Propagation and Information Diﬀusion in Graphs
151
[HGO+10]
K. Heath, N. Gelfand, M. Ovsjanikov, M. Aanjaneya, and L. Guibas. Im-
age webs: Computing and exploiting connectivity in image collections.
In Proc. IEEE Conference on Computer Vision and Pattern Recognition,
pages 3432–3439, 2010.
[HHA+06]
R. A. Heckemann, J. V. Hajnal, P. Aljabar, D. Rueckert, and A. Ham-
mers.
Automatic anatomical brain MRI segmentation combining label
propagation and decision fusion. NeuroImage, 33(1):115 – 126, 2006.
[HIF13]
S. Huron, P. Isenberg, and J. D. Fekete. PolemicTweet: Video Annotation
and Analysis through Tagged Tweets. In Proc. of the IFIP Conference on
Human-Computer Interaction, pages 135–152, 2013.
[HK92]
L. Hagen and A. B. Kahng. New spectral methods for ratio cut parti-
tioning and clustering. IEEE Transactions on Computer-Aided Design of
Integrated Circuits and Systems, 11(9):1074 –1085, 1992.
[HK10]
T. Hwang and R. Kuang. A heterogeneous label propagation algorithm
for disease gene discovery. In Proc. SIAM International Conference on
Data Mining, pages 583–594, 2010.
[HLC08]
S. C. H. Hoi, W. Liu, and S. F. Chang. Semi-supervised distance metric
learning for collaborative image retrieval. In Proc. IEEE Conference on
Computer Vision and Pattern Recognition, pages 1–7, 2008.
[HLL+14]
L. Huang, Y. Liu, X. Liu, X. Wang, and B. Lang. Graph-based active semi-
supervised learning: A new perspective for relieving multi-class annotation
labor. In Proc. IEEE International Conference on Multimedia and Expo,
pages 1–6, 2014.
[HN04]
X. He and P. Niyogi. Locality preserving projections. In Proc. Advances
in Neural Information Processing Systems 16, pages 153–160, 2004.
[HOSS13]
M. E. Houle, V. Oria, S. Satoh, and J. Sun.
Annotation Propagation
in Image Databases Using Similarity Graphs. ACM Trans. Multimedia
Comput. Commun. Appl., 10(1):7:1–7:21, 2013.
[HTW+12]
C. Hui, Y. Tyshchuk, W. A. Wallace, M. Magdon-Ismail, and M. Goldberg.
Information Cascades in Social Media in Response to a Crisis: A Prelim-
inary Model and a Case Study. In Proc. 21st International Conference
Companion on World Wide Web, pages 653–656, 2012.
[HWLH12]
L. C. Hsieh, G. L. Wu, W. Y. Lee, and W. Hsu. Two-stage sparse graph
construction using MinHash on MapReduce. In Proc. IEEE International
Conference on Acoustics, Speech and Signal Processing, pages 1013–1016,
2012.
[IDF+05]
G. Iyengar, P. Duygulu, S. Feng, P. Ircing, S. P. Khudanpur, D. Klakow,
M. R. Krause, R. Manmatha, H. J. Nock, D. Petkova, B. Pytlik, and
P. Virga. Joint Visual-Text Modeling for Automatic Retrieval of Multi-
media Documents. In Proc. 13th Annual ACM International Conference
on Multimedia, pages 21–30, 2005.
[JCST01]
T. Joachims, N. Cristianini, and J. Shawe-Taylor. Composite Kernels for
Hypertext Categorisation. In Proc. International Conference on Machine
Learning, pages 250–257, 2001.

152
Graph-Based Social Media Analysis
[Joa03]
T. Joachims. Transductive learning via spectral graph partitioning. In
Proc. International Conference on Machine Learning, pages 290–297,
2003.
[Jol02]
I. T. Jolliﬀe. Principal Component Analysis. Springer, 2nd edition, 2002.
[JSWZHZ09]
R. Jin, S. Shijun Wang, and Z. H. Zhi-Hua Zhou. Learning a distance
metric from multi-instance multi-label data. In Proc. IEEE Conference
on Computer Vision and Pattern Recognition, pages 896–902, 2009.
[JWC09]
T. Jebara, J. Wang, and S. Chang. Graph construction and b-matching for
semi-supervised learning. In Proc. 26th Annual International Conference
on Machine Learning, pages 441–448, 2009.
[KG13]
K. K. Kumar and G. Geethakumari.
Information diﬀusion model for
spread of misinformation in online social networks. In Proc. International
Conference on Advances in Computing, Communications and Informatics,
pages 1172–1177, 2013.
[KGF12]
D. Kuettel, M. Guillaumin, and V. Ferrari. Segmentation Propagation
in Imagenet. In Proc. 12th European Conference on Computer Vision,
volume VII, pages 459–473, 2012.
[KKT03]
D. Kempe, J. Kleinberg, and E. Tardos. Maximizing the spread of inﬂu-
ence through a social network. In Proc. 9th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pages 137–146,
2003.
[KKT05]
D. Kempe, J. Kleinberg, and E. Tardos. Inﬂuential Nodes in a Diﬀusion
Model for Social Networks. In Proc. 32nd International Conference on
Automata, Languages and Programming, pages 1127–1138, 2005.
[KL02]
R. I. Kondor and J. Laﬀerty. Diﬀusion kernels on graphs and other discrete
structures. In Proc. 19th International Conference on Machine Learning,
pages 315–322, 2002.
[KM27]
W. O. Kermack and A. McKendrick. A Contribution to the Mathematical
Theory of Epidemics. Proc. of the Royal Society of London. Series A, Con-
taining Papers of a Mathematical and Physical Character, 115(772):700–
721, 1927.
[KOW08]
J. Kostka, Y. Oswald, and R. Wattenhofer. Word of mouth: Rumor dis-
semination in social networks. In Structural Information and Communica-
tion Complexity, Lecture Notes in Computer Science, volume 5058, pages
185–196. Springer, 2008.
[KS06]
M. Kimura and K. Saito. Tractable Models for Information Diﬀusion in
Social Networks.
In Proc. 10th European Conference on Principle and
Practice of Knowledge Discovery in Databases, pages 259–271, 2006.
[KTT+12]
K. I. Kim, J. Tompkin, M. Theobald, J. Kautz, and C. Theobalt. Match
Graph Construction for Large Image Databases. In Proc. 12th European
Conference on Computer Vision, volume I, pages 272–285, 2012.

Label Propagation and Information Diﬀusion in Graphs
153
[Lan50]
C. Lanczos. An iterative method for the solution of the eigenvalue problem
of linear diﬀerential and integral.
Journal of Research of the National
Bureau of Standards, 45:255–282, 1950.
[LBK09]
J. Leskovec, L. Backstrom, and J. Kleinberg.
Meme-Tracking and the
Dynamics of the News Cycle. In Proc. 15th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pages 497–506,
2009.
[LHC10]
W. Liu, J. He, and S. Chang. Large graph construction for scalable semi-
supervised learning. In Proc. International Conference on Machine Learn-
ing, pages 679–686, 2010.
[LHWH13]
W. Y. Lee, L. C. Hsieh, G. L. Wu, and W. Hsu.
Graph-based Semi-
supervised Learning with Multi-Modality Propagation for Large-Scale Im-
age Datasets. Journal of Visual Communication and Image Representa-
tion, 24(3):295–302, 2013.
[LHY+09]
D. Liu, X. S. Hua, L. Yang, M. Wang, and H. Zhang. Tag Ranking. In
Proc. 18th International Conference on World Wide Web, pages 351–360,
2009.
[LK03]
S. Letovsky and S. Kasif. Predicting protein function from protein/protein
interaction data: a probabilistic approach.
Bioinformatics, 19:197–204,
2003.
[LKG+07]
J. Leskovec, A. Krause, C. Guestrin, C. Faloutsos, J. VanBriesen, and
N. Glance.
Cost-eﬀective Outbreak Detection in Networks.
In Proc.
13th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, pages 420–429, 2007.
[LLH+07]
J. Liu, W. Lai, X. Hua, Y. Huang, and S. Li. Video search re-ranking
via multi-graph propagation. In Proc. 15th International Conference on
Multimedia, pages 208–217, 2007.
[LLL+09]
J. Liu, M. Li, Q. Liu, H. Lu, and S. Ma. Image Annotation via Graph
Learning. Pattern Recognition, 42(2):218–228, 2009.
[LMT+10]
W. Liu, S. Ma, D. Tao, J. Liu, and P. Liu. Semi-Supervised Sparse Metric
Learning Using Alternating Linearization Optimization.
In Proc. 16th
ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, pages 1139–1148, 2010.
[LPSL10]
D. Lee, J. Park, J. Shim, and S. Lee. An Eﬃcient Similarity Join Algorithm
with Cosine Similarity Predicate. In Database and Expert Systems Appli-
cations, Lecture Notes in Computer Science, volume 6262, pages 422–436.
Springer, 2010.
[LV07]
J. A. Lee and M. Verleysen.
Nonlinear Dimensionality Reduction.
Springer, 2007.
[LYHZ11]
D. Liu, S. Yan, X. S. Hua, and H. J. Zhang. Image Retagging Using Collab-
orative Tag Propagation. IEEE Transactions on Multimedia, 13(4):702–
712, 2011.

154
Graph-Based Social Media Analysis
[LYZZ08]
J. Long, J. Yin, W. Zhao, and E. Zhu. Graph-Based Active Learning Based
on Label Propagation. In V. Torra and Y. Narukawa, editors, Modeling
Decisions for Artiﬁcial Intelligence, Lecture Notes in Computer Science,
volume 5285, pages 179–190. Springer, 2008.
[MKJ13]
R. Michalski, P. Kazienko, and J. Jankowski. Convince a Dozen More and
Succeed – The Inﬂuence in Multi-layered Social Networks. In Proc. Inter-
national Conference on Signal-Image Technology Internet-Based Systems,
pages 499–505, 2013.
[ML13]
T. Ma and L. J. Latecki. Graph Transduction Learning with Connectiv-
ity Constraints with Application to Multiple Foreground Cosegmentation.
In Proc. IEEE Conference on Computer Vision and Pattern Recognition,
pages 1955–1962, 2013.
[Mor00]
S. Morris. Contagion. Review of Economic Studies, 67:57–78, 2000.
[MYLK08]
H. Ma, H. Yang, M. R. Lyu, and I. King. Mining Social Networks Using
Heat Diﬀusion Processes for Marketing Candidates Selection.
In Proc.
17th ACM Conference on Information and Knowledge Management, pages
233–242, 2008.
[NG08]
N. Nguyen and Y. Guo.
Metric learning: A support vector approach.
Machine Learning and Knowledge Discovery in Databases, pages 125–136,
2008.
[NJT05]
Z. Y. Niu, D. H. Ji, and C. L. Tan. Word sense disambiguation using label
propagation based semi-supervised learning. In Proc. 43rd Annual Meeting
on Association for Computational Linguistics, pages 395–402, 2005.
[NYTE12]
N. P. Nguyen, G. Yan, M. T. Thai, and S. Eidenbenz. Containment of
Misinformation Spread in Online Social Networks. In Proc. 4th Annual
ACM Web Science Conference, pages 213–222, 2012.
[OC12]
M. Orbach and K. Crammer. Graph-Based Transduction with Conﬁdence.
In Proc. European Conference on Machine Learning and Knowledge Dis-
covery in Databases, volume II, pages 323–338, 2012.
[OR13]
M. Opuszko and J. Ruhland. Impact of the Network Structure on the
SIR Model Spreading Phenomena in Online Networks. In Proc. The 8th
International Multi-Conference on Computing in the Global Information
Technology, pages 22–28, 2013.
[PBS10]
N. Pathak, A. Banerjee, and J. Srivastava. A Generalized Linear Threshold
Model for Multiple Cascades. In Proc. IEEE 10th International Confer-
ence on Data Mining, pages 965–970, 2010.
[PPLJ14]
Y. Park, S. Park, S. Lee, and W. Jung. Greedy ﬁltering: A scalable algo-
rithm for k-nearest neighbor graph construction. In Database Systems for
Advanced Applications, Lecture Notes in Computer Science, volume 8421,
pages 327–341. Springer, 2014.
[PSV01]
R. Pastor-Satorras and A. Vespignani. Epidemic Dynamics and Endemic
States in Complex Networks. Physical Review E, page 066117, 2001.

Label Propagation and Information Diﬀusion in Graphs
155
[PSZ11]
J. Philbin, J. Sivic, and A. Zisserman. Geometric Latent Dirichlet Alloca-
tion on a Matching Graph for Large-Scale Image Datasets. International
Journal of Computer Vision, 95(2):138–153, 2011.
[QHR+07]
G. J. Qi, X. S. Hua, Y. Rui, J. Tang, T. Mei, and H. J. Zhang. Correlative
multi-label video annotation. In Proc. 15th International Conference on
Multimedia, pages 17–26, 2007.
[QTZ+09]
G. J. Qi, J. Tang, Z. J. Zha, T. S. Chua, and H. J. Zhang.
An Eﬃ-
cient Sparse Metric Learning in High-dimensional Space via L1-Penalized
Log-Determinant Regularization. In Proc. 26th Annual International Con-
ference on Machine Learning, pages 841–848, 2009.
[RD02]
M. Richardson and P. Domingos. Mining knowledge-sharing sites for vi-
ral marketing. In Proc. 8th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 61–70, 2002.
[RH05]
H. Rue and L. Held. Gaussian Markov Random Fields: Theory and Ap-
plications. Chapman & Hall, 2005.
[RLF12]
M. Rubinstein, C. Liu, and W. T. Freeman. Annotation Propagation in
Large Image Databases via Dense Image Correspondence. In Lecture Notes
in Computer Science, volume 7574, pages 85–99. Springer, 2012.
[Rog95]
E. Rogers. The Diﬀusion of Innovation. Free Press, 1995.
[RR09]
D. Rao and D. Ravichandran. Semi-supervised polarity lexicon induction.
In Proc. 12th Conference of the European Chapter of the Association for
Computational Linguistics, pages 675–682, 2009.
[RS00]
S. T. Roweis and L. K. Saul.
Nonlinear Dimensionality Reduction by
Locally Linear Embedding. Science, 290(5500):2323–2326, 2000.
[SA12]
S. Simon and K. Apt. Choosing Products in Social Networks. In Inter-
net and Network Economics, Lecture Notes in Computer Science, volume
7695, pages 100–113. Springer, 2012.
[SB11]
A. Subramanya and J. Bilmes. Semi-Supervised Learning with Measure
Propagation. Journal of Machine Learning Research, 12:3311–3370, 2011.
[SCA14]
P. Sinha, A. D. Choudhury, and A. K. Agrawal. Sentiment analysis of
Wimbledon tweets. In Proc. ACM WWW Microposts workshop, 2014.
[Sch78]
T. Schelling. Micromotives and Macrobehavior. Norton, 1978.
[SJ02]
M. Szummer and T. Jaakkola. Partially labeled classiﬁcation with Markov
random walks. In Proc. Advances in Neural Information Processing Sys-
tems, pages 945–952, 2002.
[SJY08]
L. Sun, S. Ji, and J. Ye.
Hypergraph spectral learning for multi-label
classiﬁcation. In Proc. 14th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 668–676, 2008.
[SK03]
A. Smola and R. Kondor.
Kernels and Regularization on Graphs.
In
Proc. Conference on Learning Theory and Kernel Machines, pages 144–
158, 2003.

156
Graph-Based Social Media Analysis
[SK09]
X. Su and T. M. Khoshgoftaar. A survey of Collaborative Filtering Tech-
niques. Advances in Artiﬁcial Intelligence, 2009:4:2–4:2, 2009.
[SKC09]
D. A. Shamma, L. Kennedy, and E. F. Churchill. Tweet the Debates:
Understanding Community Annotation of Uncollected Sources. In Proc.
1st SIGMM Workshop on Social Media, pages 3–10, 2009.
[SKM09]
K. Saito, M. Kimura, and H. Motoda. Discovering Inﬂuential Nodes for
SIS Models in Social Networks. In Discovery Science, Lecture Notes in
Computer Science, volume 5808, pages 302–316. Springer, 2009.
[Smi72]
J. M. Smith. On evolution. Edinburgh University Press, 1972.
[SN05]
V. Sindhwani and P. Niyogi. A co-regularized approach to semi-supervised
learning with multiple views. In Proc. ICML Workshop on Learning with
Multiple Views, 2005.
[SR03]
L. K. Saul and S. T. Roweis. Think globally, ﬁt locally: unsupervised learn-
ing of low dimensional manifolds. Journal of Machine Learning Research,
4:119–155, 2003.
[SSSN04]
S. Shalev-Shwartz, Y. Singer, and A. Y. Ng. Online and Batch Learning
of Pseudo-Metrics. In Proc. 21st International Conference on Machine
Learning, pages 94–101, 2004.
[SSUB11]
M. Speriosu, N. Sudan, S. Upadhyay, and J. Baldridge. Twitter polarity
classiﬁcation with label propagation over lexical links and the follower
graph. In Proc. 1st Workshop on Unsupervised Learning in NLP, pages
53–63, 2011.
[STA09]
X. Shi, B. Tseng, and L. Adamic. Information diﬀusion in computer sci-
ence citation networks. In Proc. International Conference on Weblogs and
Social Media, 2009.
[SZZZ10]
M. S. Shang, Z. K. Zhang, T. Zhoub, and Y. C. Zhang. Collaborative
ﬁltering with diﬀusion-based similarity on tripartite graphs. Physica A,
389(6):1259–1264, 2010.
[Tal09]
P. P. Talukdar. Topics in Graph Construction for Semi-Supervised Learn-
ing. Technical Report, University of Pennsylvania, 2009.
[TAM12]
V. Tzoumas, C. Amanatidis, and E. Markakis. A Game-Theoretic Analysis
of a Competitive Diﬀusion Process over Social Networks. In Internet and
Network Economics, Lecture Notes in Computer Science, volume 7695,
pages 1–14. Springer, 2012.
[TC09]
P. P. Talukdar and K. Crammer. New Regularized Algorithms for Trans-
ductive Learning.
In Proc. European Conference on Machine Learning
and Knowledge Discovery in Databases: Part II, pages 442–457, 2009.
[TDSL00]
J. B. Tenenbaum, V. De Silva, and J. C. Langford.
A global ge-
ometric framework for nonlinear dimensionality reduction.
Science,
290(5500):2319–2323, 2000.
[TH01]
L. Terveen and W. Hill. Beyond Recommender Systems: Helping Peo-
ple Help Each Other. In HCI In The New Millennium, Jack Carroll ed.
Addison-Wesley, 2001.

Label Propagation and Information Diﬀusion in Graphs
157
[THK09]
Z. Tian, T. Hwang, and R. Kuang. A hypergraph-based learning algorithm
for classifying gene expression and arrayCGH data with prior knowledge.
Bioinformatics, 25(21):2831–2838, 2009.
[THL+05]
H. Tong, J. He, M. Li, C. Zhang, and W. Y. Ma. Graph based multi-
modality learning. In Proc. 13th Annual ACM International Conference
on Multimedia, pages 862–871, 2005.
[THQ+07]
J. Tang, X. S. Hua, G.-J. Qi, T. Mei, and X. Wu. Anisotropic Manifold
Ranking for Video Annotation. In Proc. IEEE International Conference
on Multimedia and Expo, pages 492–495, 2007.
[THQ+08]
J. Tang, X.-S. Hua, G.-J. Qi, Y. Song, and X. Wu. Video Annotation
Based on Kernel Linear Neighborhood Propagation. IEEE Transactions
on Multimedia, 10(4):620 –628, 2008.
[THW+09]
J. Tang, X.-S. Hua, M. Wang, Z. Gu, G.-J. Qi, and X. Wu. Correlative
Linear Neighborhood Propagation for Video Annotation. IEEE Transac-
tions on Systems, Man, and Cybernetics, Part B: Cybernetics, 39(2):409
–416, 2009.
[THY+11]
J. Tang, R. Hong, S. Yan, T. S. Chua, G.-J. Qi, and R. Jain. Image anno-
tation by kNN-sparse graph-based label propagation over noisily tagged
web images. ACM Transactions on Intelligent Systems and Technology,
2(2):14:1–14:15, 2011.
[TLLZ14]
J. Tang, M. Li, Z. Li, and C. Zhao. Tag ranking based on salient region
graph propagation. Multimedia Systems, pages 1–9, 2014.
[TSS05]
K. Tsuda, H. Shin, and B. Sch¨olkopf.
Fast protein classiﬁcation with
multiple networks. Bioinformatics, 21(2):59–65, 2005.
[Tsu05]
K. Tsuda. Propagating distributions on a hypergraph by dual informa-
tion regularization. In Proc. 22nd International Conference on Machine
Learning, pages 920–927, 2005.
[TYH+09]
J. Tang, S. Yan, R. Hong, G. J. Qi, and T. S. Chua. Inferring semantic
concepts from community-contributed images and noisy tags. In Proc.
17th ACM International Conference on Multimedia, pages 223–232, 2009.
[VG12]
S. Vijayanarasimhan and K. Grauman. Active frame selection for label
propagation in videos. In Proc. 12th European conference on Computer
Vision, volume V, pages 496–509, 2012.
[VK11]
S. Venkatramanan and A. Kumar. Information dissemination in socially
aware networks under the linear threshold model. In National Conference
on Communications, pages 1–5, 2011.
[vNM44]
J. von Neumann and O. Morgenstern. Theory of Games and Economic
Behavior. Princeton University Press, 1944.
[Wat02]
D. J. Watts. A simple model of global cascades on random networks. In
Proc. of the National Academy of Sciences, pages 5766–5771, 2002.
[WHH+09]
M. Wang, X. S. Hua, R. Hong, J. Tang, G. J. Qi, and Y. Song. Uniﬁed
video annotation via multigraph learning. IEEE Transactions on Circuits
and Systems for Video Technology, 19(5):733–746, 2009.

158
Graph-Based Social Media Analysis
[WHTH09]
M. Wang, X. S. Hua, J. Tang, and R. Hong. Beyond Distance Measure-
ment: Constructing Neighborhood Similarity for Video Annotation. IEEE
Transactions on Multimedia, 11(3):465–476, 2009.
[WHY+07]
M. Wang, X. S. Hua, X. Yuan, Y. Song, and L. R. Dai. Optimizing multi-
graph learning: towards a uniﬁed video annotation scheme. In Proc. 15th
International Conference on Multimedia, pages 862–871, 2007.
[WJC08]
J. Wang, T. Jebara, and S. F. Chang.
Graph Transduction via Alter-
nating Minimization. In Proc. 25th International Conference on Machine
Learning, pages 1144–1151, 2008.
[WJZZ06]
C. Wang, F. Jing, L. Zhang, and H. J. Zhang. Image Annotation Re-
ﬁnement Using Random Walk with Restarts. In Proc. 14th Annual ACM
International Conference on Multimedia, pages 647–650, 2006.
[WKL11]
D. Wang, I. King, and K. S. Leung. “Like Attracts Like!”– A Social Rec-
ommendation Framework Through Label Propagation. In Proc. Workshop
on Social Web Search and Mining: Content Analysis Under Crisis, 2011.
[WLI+05]
J. Weston, C. Leslie, E. Ie, D. Zhou, and A. Elisseeﬀ. Semi-supervised
protein classiﬁcation using cluster kernels. Bioinformatics, 21:3241–3247,
2005.
[WS09]
K. Q. Weinberger and L. K. Saul. Distance Metric Learning for Large
Margin Nearest Neighbor Classiﬁcation.
Journal of Machine Learning
Research, 10:207–244, 2009.
[WSC11]
J. Woo, J. Son, and H. Chen. An SIR model for violent topic diﬀusion in
social media. In Proc. IEEE International Conference on Intelligence and
Security Informatics, pages 15–19, 2011.
[WWL07]
F. Wang, X. Wang, and T. Li. Eﬃcient label propagation for interactive
image segmentation. In Proc. 6th International Conference on Machine
Learning and Applications, pages 136–141, 2007.
[WWS+09]
F. Wang, X. Wang, B. Shao, T. Li, and M. Ogihara.
Tag integrated
multi-label music style classiﬁcation with hypergraph. In Proc. 10th In-
ternational Society for Music Information Retrieval, pages 363–368, 2009.
[WWZ+09]
J. Wang, F. Wang, C. Zhang, H. C. Shen, and L. Quan. Linear Neighbor-
hood Propagation and Its Applications. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 31(9):1600–1615, 2009.
[WWZ+12]
J. Wang, J. Wang, G. Zeng, Z. Tu, R. Gan, and S. Li. Scalable k-NN
graph construction for visual descriptors. In Proc. IEEE Conference on
Computer Vision and Pattern Recognition, pages 1106–1113, June 2012.
[WYG+09]
J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma. Robust Face
Recognition via Sparse Representation. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 31(2):210–227, 2009.
[WYH+13]
Z. Wei, Y. Yanqing, T. Hanlin, D. Qiwei, and L. Taowei. Information Dif-
fusion Model Based on Social Network. In Proc. International Conference
of Modern Computer Science and Applications, Advances in Intelligent
Systems and Computing, volume 191, pages 145–150. Springer, 2013.

Label Propagation and Information Diﬀusion in Graphs
159
[WZ06]
F. Wang and C. Zhang. Label propagation through linear neighborhoods.
In Proc. 23rd International Conference on Machine Learning, pages 985–
992, 2006.
[XL10]
B. Xu and L. Liu. Information diﬀusion through online social networks.
In Proc. IEEE International Conference on Emergency Management and
Management Sciences, pages 53–56, 2010.
[XNJR02]
E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell.
Distance Metric
Learning, with Application to Clustering with Side-Information. In Proc.
Advances in Neural Information Processing Systems 15, volume 15, pages
505–512, 2002.
[Xu14]
J. Xu. Joint visual and textual mining on social media. In Proc. IEEE
International Conference on Data Mining Workshop, pages 1189–1190,
Dec 2014.
[XWTQ07]
J. Xiao, J. Wang, P. Tan, and L. Quan. Joint aﬃnity propagation for
multiple view segmentation. In Proc. IEEE 11th International Conference
on Computer Vision, pages 1–7, 2007.
[YJHL11]
J. Yu, X. Jin, J. Han, and J. Luo. Collection-Based Sparse Label Prop-
agation and Its Application on Social Group Suggestion from Photos.
ACM Transactions on Intelligent Systems and Technology, 2(2):12:1–
12:21, 2011.
[YJZ+06]
L. Yang, D. Ji, G. Zhou, Y. Nie, and G. Xiao. Document re-ranking using
cluster validation and label propagation. In Proc. 15th ACM International
Conference on Information and Knowledge Management, pages 690–697,
2006.
[YLL09]
W. You, L. Liu, and C. Lv. Towards Emergency Management: Review on
Diﬀusion Models and Their Applications. In Proc. International Sympo-
sium on Emergency Management, pages 633–636, 2009.
[YQZC12]
O. Yagan, D. Qian, J. Zhang, and D. Cochran. Information diﬀusion in
overlaying social-physical networks. In Proc. 48th Annual Conference in
Information Sciences and Systems, pages 1–6, 2012.
[YR06]
L. Yang and J. Rong. Distance Metric Learning: A Comprehensive Survey.
Technical Report, Michigan State University, 2006.
[YTZ04]
K. Yu, V. Tresp, and D. Zhou.
Semi-supervised Induction with Basis
Functions. Technical Report No. 141, Max Planck Institute for Biological
Cybernetics, 2004.
[YWY+13]
Z. Yong, L. Weishi, Z. Yang, Z. Gang, Q. Dongxiang, Z. Qi, H. Ying,
W. Haifeng, H. Xiaobo, and H. Jiaming. Brain MRI Segmentation with
Label Propagation. International Journal of Emerging Trends & Technol-
ogy in Computer Science, 2(5):158–163, 2013.
[YWZ12]
W. Yang, K. Wang, and W. Zuo. Fast neighborhood component analysis.
Neurocomputing, 83:31 – 37, 2012.

160
Graph-Based Social Media Analysis
[ZB07]
D. Zhou and C. J. C. Burges. Spectral clustering and transductive learning
with multiple views. In Proc. 24th International Conference on Machine
Learning, pages 1159–1166, 2007.
[ZBL+04]
D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Sch¨olkopf. Learning
with local and global consistency. In Proc. Advances in Neural Information
Processing Systems 16, pages 321–328, 2004.
[ZG02]
X. Zhu and Z. Ghahramani. Learning from Labeled and Unlabeled Data
with Label Propagation. Technical report, School of CS, CMU, 2002.
[ZGL03]
X. Zhu, Z. Ghahramani, and J. Laﬀerty. Semi-Supervised Learning Using
Gaussian Fields and Harmonic Functions.
In Proc. 20th International
Conference on Machine Learning, pages 912–919, 2003.
[Zha09]
Y. Zhang. A deterministic model for history sensitive cascade in diﬀusion
networks. In Proc. IEEE International Conference on Systems, Man and
Cybernetics, pages 1977–1982, 2009.
[ZHGL13]
Y. M. Zhang, K. Huang, G. Geng, and C. L. Liu.
Fast kNN Graph
Construction with Locality Sensitive Hashing. In Machine Learning and
Knowledge Discovery in Databases, Lecture Notes in Computer Science,
volume 8189, pages 660–674. Springer, 2013.
[ZHS05]
D. Zhou, J. Huang, and B. Sch¨olkopf. Learning from labeled and unlabeled
data on a directed graph.
In Proc. 22nd International Conference on
Machine Learning, pages 1036–1043, 2005.
[ZHS07]
D. Zhou, J. Huang, and B. Sch¨olkopf. Learning with hypergraphs: Clus-
tering, classiﬁcation, and embedding. In Proc. Advances in Neural Infor-
mation Processing Systems, volume 19, page 1601, 2007.
[ZK09]
G. D. Zhou and F. Kong. Global learning of noun phrase anaphoricity
in coreference resolution via label propagation. In Proc. Conference on
Empirical Methods in Natural Language Processing, volume 2, pages 978–
986, 2009.
[ZL05]
X. Zhu and J. Laﬀerty. Harmonic Mixtures: Combining Mixture Models
and Graph-Based Methods for Inductive and Scalable Semi-Supervised
Learning. In Proc. 22nd International Conference on Machine Learning,
pages 1052–1059, 2005.
[ZL13]
Y. Zhou and L. Liu. Social Inﬂuence Based Clustering of Heterogeneous
Information Networks. In Proc. 19th ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining, 2013.
[ZLG03]
X. Zhu, J. Laﬀerty, and Z. Ghahramani. Semi-Supervised Learning: From
Gaussian Fields to Gaussian Processes. Technical report, School of CS,
CMU, 2003.
[ZLP13]
G. Zhou, Z. Lu, and Y. Peng.
L1-graph construction using structured
sparsity. Neurocomputing, 120(0):441–452, 2013.
[ZNP13]
O. Zoidi, N. Nikolaidis, and I. Pitas. Exploiting clustering and stereo in-
formation in label propagation on facial images. In Proc. IEEE Workshop
on Computational Intelligence in Biometrics and Identity Management,
2013.

Label Propagation and Information Diﬀusion in Graphs
161
[ZS04]
D. Zhou and B. Sch¨olkopf. Learning from labeled and unlabeled data using
random walks. In Proc. 26th DAGM Symposium on Pattern Recognition,
pages 237–244, 2004.
[ZTNP14]
O. Zoidi, A. Tefas, N. Nikolaidis, and I. Pitas. Person Identity Label Prop-
agation in Stereo Videos. IEEE Transactions on Multimedia, 16(5):1358–
1368, 2014.
[ZXP+11]
J. Zhu, F. Xiong, D. Piao, Y. Liu, and Y. Zhang. Statistically Modeling
the Eﬀectiveness of Disaster Information in Social Media. In Proc. IEEE
Global Humanitarian Technology Conference, pages 431–436, 2011.
[ZXZ+12]
T. Zhang, C. Xu, G. Zhu, S. Liu, and H. Lu. A Generic Framework for
Video Annotation via Semi-Supervised Learning. IEEE Transactions on
Multimedia, 14(4):1206–1219, 2012.


Chapter 6
Graph-Based Pattern Classiﬁcation and
Dimensionality Reduction
Alexandros Iosiﬁdis and Ioannis Pitas
Aristotle University of Thessaloniki, Greece
6.1
Introduction ....................................................................
163
6.2
Notations .......................................................................
164
6.3
Unsupervised Methods .........................................................
166
6.3.1
Locality Preserving Projections .......................................
166
6.3.2
Locally Linear Embedding ............................................
167
6.3.3
ISOMAP ...............................................................
168
6.3.4
Laplacian Embedding .................................................
168
6.3.5
Diﬀusion Maps .........................................................
168
6.4
Supervised Methods ............................................................
169
6.4.1
Linear Discriminant Analysis ..........................................
169
6.4.2
Marginal Fisher Analysis ..............................................
171
6.4.3
Local Fisher Discriminant Analysis ...................................
171
6.4.4
Graph Embedding .....................................................
172
6.4.5
Minimum Class Variance Extreme Learning Machine ................
173
6.4.6
Minimum Class Variance Support Vector Machine ...................
174
6.4.7
Graph Embedded Support Vector Machines ..........................
174
6.5
Semi-Supervised Methods ......................................................
175
6.5.1
Semi-Supervised Discriminant Analysis ...............................
176
6.5.2
Laplacian Support Vector Machine
...................................
176
6.5.3
Semi-Supervised Extreme Learning Machine .........................
177
6.6
Applications ....................................................................
177
6.7
Conclusions .....................................................................
178
Bibliography ....................................................................
179
6.1
Introduction
Digital media, like images, videos, etc., play an important role in social networks. For
example, users in social networks are able to post images depicting themselves and/or some
of their friends in a location, while other users may see some of these images and rate them.
Such rates can be used to describe connections between users or between users and locations,
etc. Thus, digital media content analysis is of particular interest in social networks. Digital
media representations are usually high-dimensional. For example, a facial image of size
200 × 150 pixels can be represented by a 30000-dimensional vector obtained by using each
pixel coordinate as a diﬀerent dimension. Therefore, signiﬁcant eﬀorts have been devoted to
deriving low-dimensional data representations that retain properties of interest of the data,
163

164
Graph-Based Social Media Analysis
like pair-wise distances, data dispersion, and class discrimination [YXZ+07, RR13]. Such
low-dimensional data representations are essential in order to reduce the computational
cost and the physical memory used to store data, especially in social network applications,
where the cardinality of the data sets is enormous. In addition, machine learning methods
that are able to classify digital media data are required, in order to proceed with automatic
data categorization toward decision making and/or recommending appropriate digital media
content to users [KZP06].
Graphs have been proven to be a powerful representation of data relations and have
been used in many pattern recognition and machine learning methods, in order to express
geometric data relations [WHL05]. It should be noted here that graphs have also been used
for data representation [KTI03, VBGS06, ZTP07b, RB09a, CMC+09, BR11, GDIHK11].
However, in this chapter, we shall focus our attention on methods employing graph struc-
tures in order to express data relationships. Such relationships may be computed ei-
ther for vectorial data, or for graph-based data representations (e.g., similarity graphs)
[NB05, JH05, VSKB10]. For the latter case, several works have been proposed that can
be used in order to “embed” a set of graph data representations in vectorial spaces
[WHL05, PD05, EWH07, RNB07, RB09b, LRLB13, BPRB13, LRLB13]. An additional
reason for exploiting graph based techniques is that this approach can be used to directly
capture information appearing in social media/network structures due to the natural rela-
tionship between social networks and graphs, as discussed in Chapter 1.
In this chapter, we describe some of the most widely used methods for data dimension-
ality reduction and classiﬁcation that assume an underlying graph structure. For simplicity,
we focus on methods exploiting one undirected graph structure, while methods exploiting di-
rected graph structures, [SP11, ZHS05, JM07, ZZY+08] and hyper-graphs [LHM11, PF12],
also exist. We divide them into three categories, i.e., unsupervised, supervised and semi-
supervised, based on their requirement for manual (user-derived) annotation.
6.2
Notations
In this section we introduce notations that will be used in the entire chapter. Let us
assume that a set of digital media data, e.g., a set of N facial images, has been preprocessed
so that each sample i = 1, . . . , N is represented by a D-dimensional vector xi ∈RD. Let us
also deﬁne a similarity measure sij = s(xi, xj) that is used to measure the similarity between
two vectors xi and xj. Any similarity measure providing non-negative values (usually 0 ≤
sij ≤1) can be used to this end. The most widely adopted choice is the heat kernel (also
known as the diﬀusion kernel or RBF kernel) [KL02], deﬁned as follows:
s(xi, xj) = exp

−∥xi −xj∥2
2
2σ2

,
(6.2.1)
where ∥· ∥2 denotes the L2 norm of a vector and σ is a parameter used in order to scale the
Euclidean distance between xi and xj. It can be easily proven that 0 ≤s(xi, xj) ≤1. By
using wij = s(xi, xj), i = 1, . . . , N, j = 1, . . . , N, we can form the graph weight matrix W ∈
RN×N. That is, we can assume that the vectors xi are embedded in a graph G = (V, E, W),
where V = {xi}N
i=1 denotes the graph vertex set and E the set of edges connecting xi.
wij denotes the weight value of the edge connecting the graph vertices xi and xj. Such a
toy graph is shown in Figure 6.2.1, where graph nodes are facial images and graph edges’
weights denote image similarities. In the case where W denotes only connections between

Graph-Based Pattern Classiﬁcation and Dimensionality Reduction
165
(a)
(b)
FIGURE 6.2.1: a) Facial image graph, b) similarity matrix.
the graph vertices, i.e., Wij = {0, 1}, W is identical to the graph adjacency matrix A
deﬁned in Chapter 2.
G may be either fully connected, i.e., wij are known for every i = 1, . . . , N, j = 1, . . . , N,
or not. In the ﬁrst case, the relationships between a sample and all the remaining ones can
be exploited. However, as will be described in the following sections, there are cases where
we are interested in graph structures that describe the relationships between neighboring
graph vertices only. Such graph structures can be obtained either by using an appropriate
scale value σ in equation (6.2.1), or by using a node neighborhood deﬁnition. Two of the
most widely used neighborhood deﬁnitions are used in the k-NN and the ϵ-ball graphs,
respectively [RS00]. The k-NN graph exploits only the graph weights of a node i that
correspond to its k nearest neighboring nodes. That is, the distances between the node i
and the remaining N −1 nodes are calculated, sorted in a ascending orders and the weights
that correspond to the nodes providing the k smallest distance values are included in W.
The ϵ-ball graph exploits the weights wij that satisfy ∥xi −xj∥2 ≤ϵ. From the deﬁnition
of these graph structures, it can be seen that the ϵ-ball graph corresponds to a symmetric
weight matrix, i.e. W = WT , while the k-NN graph may not correspond to a symmetric
weight matrix. A symmetric weight matrix W for the k-NN graph can be obtained by using
the graph weights that correspond to nodes that are k nearest neighbors of each other at
the same time. Appropriate k and ϵ parameter values can be obtained by applying a trial
and error, or cross-validation procedure, where the optimal k, ϵ values are determined out
of a set of predeﬁned values based e.g., on classiﬁcation performance. It should be noted
here that the k-NN graph is usually preferred, as it adapts to the dataset properties, while
an appropriate value of ϵ should be deﬁned for each dataset independently.
In the case of unsupervised learning, no class labels are used. It is assumed that the
only available information is the geometric data relationship encoded in the graph G. This
information can be exploited to partition the graph vertex set into smaller clusters (graph
clustering) [Sch07, NdC11]. It can also be used to embed graph vertices in a low-dimensional
feature space, while preserving geometrical data properties [YXZ+07]. In the case of super-
vised learning, each graph vertex xi is accompanied by a class label ci ∈C, C = {C1, . . . , CC},
e.g., the ID of the person depicted in facial image i. This information can be exploited in
training, to learn a mapping xi
f(xi)
→ci. Once learned, this mapping can be used to map a
new test sample xt (not belonging to the training set V) to one of the classes in C. In the case
of binary or multi-class classiﬁcation, the class label set is usually deﬁned as C = {−1, 1}, or
C = {1, . . . , C}, respectively. In regression problems, we typically have ci ∈R. Supervised
approaches can also be employed in order to embed the graph vertices in a low-dimensional

166
Graph-Based Social Media Analysis
feature space, while optimizing some class discrimination criteria deﬁned over the graph
vertices [YXZ+07].
In semi-supervised learning, it is assumed that some of the graph vertices are accompa-
nied by a class label, while the remaining ones are not. That is, out of the N node vectors
xi, i = 1, . . . , l, l + 1, . . . , N in total, we have only l labeled ones ci, i = 1, . . . , l. We assume
that the ﬁrst l graph vertices are labeled and the remaining u = N −l ones are not (usu-
ally, u ≫l). Semi-supervised classiﬁcation approaches can be divided into two categories,
namely transductive and inductive ones. In the ﬁrst case, the objective is the exploitation of
the geometric data relationships encoded in the graph G and the label information available
for the labeled vertices, in order to assign labels to the unlabeled vertices (data items).
This problem is usually referred to as label propagation and is presented in Chapter 5.
Inductive methods exploit the geometric data relationships encoded in the graph G and the
label information available for the labeled vertices, in order to learn a mapping xi
f(xi)
→ci
that can be used in order to map a new test sample xt (not belonging to V) to one of
the classes in C. Clearly, after learning f(·), it can be used to map the unlabeled vertices
xi, i = l + 1, . . . , N, to one of the classes in C. In this sense, transduction is a special case
of the inductive approach. Semi-supervised dimensionality reduction approaches can also
be employed to embed the graph vertices in a low-dimensional feature space optimizing a
certain class discrimination criterion deﬁned over the labeled graph vertices and preserving
geometrical data properties.
6.3
Unsupervised Methods
In this section, we describe unsupervised dimensionality reduction methods exploiting
graph structures. Methods related to graph-based clustering are presented in Chapter 3.
The objective in unsupervised dimensionality reduction methods is the determination of an
appropriate data mapping process that maps the (usually) high-dimensional data xi ∈RD
to a reduced-dimensionality feature space Rd, typically d ≪D, which preserves some inter-
esting properties of the original data xi, i = 1, . . . , N. Such properties may be the pairwise
distances between data (Multidimensional Scaling (MDS) [Kru64a, Kru64b]), the dataset
variance used in Principal Component Analysis (PCA) [HW10] or the local geometric data
structure employed in Locally Linear Embedding (LLE) [SR03].
6.3.1
Locality Preserving Projections
Locality Preserving Projections (LPP) [HN04] ﬁnds a low-dimensional embedding of
the original data xi ∈RD, so that nearby samples in the high-dimensional space RD re-
main placed nearby and are similarly co-located with respect to one another in the low-
dimensional space Rd. First, it ﬁnds the K nearest neighbors of each sample xi based on
its Euclidean distances from the remaining samples j ̸= i that form the neighborhood of
vertex i. Then it constructs a neighborhood graph G(V, E) such that E contains the edges
eij connecting neighboring graph vertices in V = {x1, . . . , xN}. The graph weight matrix
W ∈RN×N is subsequently deﬁned as follows:
Wij =
(
1,
i ∈Nj or j ∈Ni
0,
otherwise,
(6.3.1)

Graph-Based Pattern Classiﬁcation and Dimensionality Reduction
167
where Ni denotes the neighborhood of xi. The global embedding Y ∈Rd×N of the samples
xi is given by:
P = arg max
PT P=I
N
X
i=1
N
X
j=1
Wij∥PT xi −PT xj∥2
2 = arg max
PT P=I
tr

PT XLXT P

,
(6.3.2)
where L = D −W is the graph Laplacian matrix and D ∈RN×N is a diagonal matrix,
whose elements are equal to Dii = PN
j=1 Wij. The solution of the optimization problem
(6.3.2) is found by performing eigenanalysis of the matrix B = XLXT and retaining the
eigenvectors corresponding to the d smallest eigenvalues. After the calculation of P, the data
xi are mapped to the reduced-dimensionality feature space Rd by applying yi = PT xi.
6.3.2
Locally Linear Embedding
Locally Linear Embedding (LLE) [SR03] ﬁnds a low-dimensional embedding of the orig-
inal data xi ∈RD, where nearby samples in the high-dimensional space RD remain placed
nearby and are similarly co-located, with respect to one another, in the low-dimensional
space Rd. However, this is done in a diﬀerent way than in LPP. The main diﬀerence is
that LLE employs a weighted graph, while LPP employs a unweighted graph. In its sim-
plest formulation, LLE deﬁnes the K-nearest neighbors of each sample xi, based on its
Euclidean distances from the remaining samples j ̸= i and constructs a neighborhood
graph G(V, E), such that E contains the edges eij connecting neighboring graph vertices in
V = {x1, . . . , xN}.
Subsequently, a local ﬁtting step is performed. That is, each sample xi is approximated
by its neighbors xj, j ∈Ni according to ﬁtting weights wij by solving for:
min
P
j∈Ni wij=1∥xi −
X
j∈Ni
wij (xj −xi) ∥2
2.
(6.3.3)
This can be done by solving a regularized least squares problem. That is, we create the
Gram matrix Ci ∈RK having its elements equal to Ci
jk = (xj −xi)T (xk −xi) and obtain
the weight vector wi by solving for wi =
 Ci −µI
−1 1, where µ ≥0 is a regularization
parameter and 1 ∈RK is a vector of ones. wi is subsequently normalized to have unit l1
norm.
After the calculation of the weight vectors wi, i = 1, . . . , N, we deﬁne the weight matrix
W ∈RN×N having its elements equal to:
Wij =
(
wij,
j ∈Ni
0,
otherwise.
(6.3.4)
The global embedding Y ∈Rd×N of the samples xi is found by minimizing:
min
Y
N
X
i=1
∥yi −
N
X
j=1
Wijyj∥2
2 = tr

Y(I −W)T (I −W)YT 
.
(6.3.5)
The optimization problem (6.3.5) is solved by performing eigenanalysis of the positive
semi-deﬁnite matrix B = (I −W)T (I −W). Subsequently, the eigenvalues λ1, . . . , λN
are sorted in ascending order and the eigenvectors v1, . . . , vd corresponding to the small-
est eigenvalues λ1, . . . , λd are retained. The optimal embedding is ﬁnally given by Y =
h
1
√λ1 v1, . . . ,
1
√λd vd
iT
. An advantage of LLE, when compared to LPP, is that additional
local geometric information can be exploited by exploiting a weighted graph, rather than
the unweighted graph used in LPP.

168
Graph-Based Social Media Analysis
6.3.3
ISOMAP
ISOMAP [TS00] determines a low-dimensional embedding of the original data xi ∈
RD, so that the pairwise geodesic distances between the data are preserved in the low-
dimensional space Rd. ISOMAP constructs a neighborhood graph G(V, E), such that E
contains the edges eij connecting neighboring graph vertices i, j in V = {x1, . . . , xN}, if
j ∈Ni. Then, the elements of the graph weight matrix W are set to Wij = ∥xi −xj∥2.
Subsequently, the shortest path distances:
dij =
min
P={xi,...,xj}
 ∥xi −xt1∥2 + · · · + ∥xtk−1 −xj∥2

(6.3.6)
are computed, denoting the length of the graph shortest path P = {xi, . . . , xp1, . . . ,
xpk−1, . . . , xj} connecting the vertices i and j and spanning k edges [Ata98].
Subsequently, the standard MDS algorithm [FC11] is applied by using the matrix
D ∈RN×N, where Dij = d2
ij. That is, eigenanalysis is performed on the symmetric ma-
trix B = −1
2HDHT , where H = I −1
N 11T . Subsequently, the eigenvalues λ1, . . . , λN are
sorted in descending order λ1 ≥λ2 ≥· · · ≥λN and the top d eigenvectors v1, . . . , vd corre-
sponding to the d biggest eigenvalues are retained. The optimal embedding is ﬁnally given
by Y = Λ
1
2 [v1, . . . , vd]T . Similarly to LLE, an advantage of ISOMAP, when compared to
LPP is that, by exploiting a weighted graph, additional local geometric information can be
exploited. However, ISOMAP requires additional computational cost in order to calculate
the shortest path distances dij.
6.3.4
Laplacian Embedding
Laplacian Eigenmaps (LE) [BN03] compute a low-dimensional embedding of the orig-
inal data xi ∈RD, with the property that nearby samples in the high-dimensional space
RD remain placed nearby in the low-dimensional space Rd. LE constructs a neighbor-
ing graph G(V, E) such that E contains the edges connecting neighboring graph ver-
tices in V = {x1, . . . , xN} and the elements of the graph weight matrix W are set to
Wij = exp

−∥xi−xj∥2
2
2σ2

. Subsequently, the optimal embedding is given by minimizing:
min
Y
N
X
i=1
N
X
j=1
∥yi −yj∥2
2Wij = min
Y tr
 YT LY

,
(6.3.7)
where tr(·) denotes the trace of a matrix, L = D −W is the graph Laplacian matrix and
D ∈RN×N is a diagonal matrix having its elements equal to Dii = PN
j=1 Wij. In order
to remove any arbitrary scaling factor in the embedding, the constraint YT DY = I is set,
leading to the following optimization problem:
min
YT DY=Itr
 YT LY

.
(6.3.8)
The objective function (6.3.8) is minimized by solving the generalized eigendecomposition
problem Ly = λDy and retaining the eigenvectors corresponding to the smallest eigenval-
ues. By exploiting a complete weighted graph structure, it can exploit both local and global
geometric information, depending on the value of the parameter σ. This is an advantage in
the cases where a smooth low-dimensional embedding is searched for.
6.3.5
Diﬀusion Maps
Diﬀusion Maps [CL06] are a method for the analysis of the geometry of general datasets,
based on the deﬁnition of Markov chains [Nor98]. For a ﬁxed value ϵ, the isotropic diﬀusion

Graph-Based Pattern Classiﬁcation and Dimensionality Reduction
169
kernel can be deﬁned:
kϵ(xi, xj) = exp

−∥xi −xj∥2
2
4ϵ

.
(6.3.9)
Assuming that the transition probability between the vertices xi and xj is proportional to
kϵ(xi, xj), the Markov matrix M ∈RN×N can be constructed, which has elements equal
to:
Mij = kϵ(xi, xj)
pϵ(xi)
,
(6.3.10)
where pϵ(xi) is a normalization constant given by:
pϵ(xi) =
N
X
i=1
kϵ(xi, xj).
(6.3.11)
For large values of ϵ, M is fully connected. Thus, it has eigenvalues λ1 > λ2 ≥· · · ≥λN
with λ1 = 1 and right eigenvectors ul, l = 1, . . . , N. The diﬀusion distance at time t is
deﬁned as follows:
D2
t (xi, xj) =
X
k
(p(xk, t|xi) −p(xk, t|xj))2 w(xk),
(6.3.12)
where p(xk, t|xi) is the probability that the random walk is located at xk at time t, given
a starting location xi at time t = 0. Using a weight function w(xk) =
1
pϵ(xk) [NLCK05], we
obtain:
D2
t (xi, xj) =
X
l
λ2t
l (ul(i) −ul(j))2 ,
(6.3.13)
where ul(i) denotes the i-th element of ul. The diﬀusion map at time t of xi is deﬁned, as
in the normalized graph Laplacian case [SM97, Wei99, NJW02], by:
yt
i = [λt
1u1(i), λt
2u2(i), . . . , λt
dud(i)]T .
(6.3.14)
6.4
Supervised Methods
As has been previously mentioned, supervised methods employ label information that
is available for graph vertices, in order to determine graph node vector mappings to a low-
dimensional feature space while optimizing certain class discrimination criteria that are
deﬁned over the labeled graph vertices. In this section, we ﬁrst describe some of the most
widely employed supervised dimensionality reduction methods that assume an underlying
graph structure. Their graph embedding formulation follows. Subsequently, we describe
supervised classiﬁcation methods that exploit criteria used in supervised dimensionality
reduction.
6.4.1
Linear Discriminant Analysis
Linear Discriminant Analysis (LDA) [DHS00] determines a projection subspace Rd for
data projection, where the samples belonging to the same class come close to each another,
while samples belonging to diﬀerent classes distance themselves as far as possible. This is

170
Graph-Based Social Media Analysis
achieved by minimizing the within-class data scatter and maximizing the between-class data
scatter that are expressed by the following matrices:
Sw =
C
X
c=1
X
xi∈Cc
(xi −µc) (xi −µc)T
(6.4.1)
Sb =
C
X
c=1
Nc (µc −µ) (µc −µ)T ,
(6.4.2)
respectively, where Nc denotes the number of samples belonging to class c, µc
=
1
Nc
P
xi∈Cc xi is the arithmetic mean vector of class c and µ =
1
N
PN
i=1 xi is the arith-
metic mean vector of the entire training dataset.
Although it has not been proposed as a graph-based method, LDA can be deﬁned by
using graph notation. Let us assume that the training samples xi are employed in order to
construct a graph G(V, E) [YXZ+07]. Two graph weight matrices Ww and Wb are deﬁned,
having elements equal to:
Ww,ij =
(
1
Nci ,
cj = ci
0,
otherwise
(6.4.3)
Wb,ij =
(
1
Nci −1
N ,
cj = ci
−1
N ,
otherwise.
(6.4.4)
By using Ww and Wb, the within-class and between-class scatter matrices are described
by:
Sw = V (Dw −Ww) VT
(6.4.5)
Sb = V (Db −Wb) VT ,
(6.4.6)
where Dw, Db are diagonal matrices having their elements equal to Dw,ii = PN
j=1 Ww,ij
and Db,ii = PN
j=1 Wb,ij, respectively.
The optimal data projection matrix P ∈RD×d can be subsequently obtained by solving
the trace ratio optimization problem deﬁned by [WYX+07, JNZ09]:
P = arg max
P
tr(PT SbP)
tr(PT SwP).
(6.4.7)
Since the trace ratio problem does not have a direct closed-form globally optimal solution,
it is conventionally approximated by solving the ratio trace problem deﬁned by:
P = arg max
P
tr
  PT SwP
−1  PT SbP
 
,
(6.4.8)
which is equivalent to the optimization problem:
Swp = λSbp, λ ̸= 0
(6.4.9)
and can be solved by performing the eigenanalysis of matrix S = S−1
b Sw, if Sb is invertible,
or S = S−1
w Sb, if Sw is invertible.
Although the original trace ratio problem (6.4.7) does not have a closed form solution,
it has been shown that it can be converted to an equivalent trace diﬀerence problem having
the form [WYX+07, JNZ09]:
P = arg max
PT P=I
trace

PT (Sb −λSw) P

,
(6.4.10)

Graph-Based Pattern Classiﬁcation and Dimensionality Reduction
171
which is solved by applying an eﬃcient constrained maximization algorithm based on the
Newton-Raphson method [WYX+07, JNZ09]. After the calculation of P, the data xi are
mapped to the reduced-dimensionality feature space Rd by applying yi = PT xi.
While LDA has been shown to provide satisfactory performance in a variety of appli-
cations, its underlying assumption is the use of unimodal classes following normal distri-
butions. However, this assumption is rather restrictive. In order to take into account class
multimodality, Subclass Discriminant Analysis (SDA) [ZM06] and Clustering-based Dis-
criminant Analysis (CDA) [CH03] have been proposed. In these approaches, it is assumed
that each class is formed by several unimodal subclasses, each following a normal distribu-
tion. Therefore, the scatter matrices (6.4.1), (6.4.2) are modiﬁed accordingly. In addition,
in order to overcome the class normality assumption, the adoption of optimized class repre-
sentations has been proposed in [ITP13b]. It has been shown that both of these approaches
outperform LDA in the cases where the assumptions of LDA are not met.
6.4.2
Marginal Fisher Analysis
In Marginal Fisher Analysis (MFA) [YXZ+07], the deﬁnition of the intra-class and
between-class relationships are described by following a local approach. It is assumed
that the data are embedded in an intrinsic graph G(V, E, W) and a penalty graph
G(p)(V, E, W(p)), where the matrix W expresses the local relationships between the data
belonging to the same class and the matrix W(p) expresses the penalty weights used to
increase inter-class discrimination. W, W(p) are deﬁned by:
Wij =





1,
ci = cj and j ∈Ni
1,
ci = cj and i ∈Nj
0,
otherwise
(6.4.11)
W (p)
ij
=





1,
ci ̸= cj and j ∈Ni
1,
ci ̸= cj and i ∈Nj
0,
otherwise.
(6.4.12)
Similarly to LDA, the optimal data projection matrix P is obtained by solving for:
P = arg min
P
tr
 PT X(D −W)XT P

tr
 PT X(Dp −W(p))XT P
,
(6.4.13)
where D, Dp are diagonal matrices having their elements equal to Dii = PN
j=1 Wij and
Dp
ii = PN
j=1 W (p)
ij , respectively. P can be calculated by solving the generalized eigenanalysis
problem X(D −W)XT q = λX(Dp −W(p))XT q.
After the calculation of P, the data xi are mapped to the reduced-dimensionality fea-
ture space Rd by applying yi = PT xi. MFA can exploit local intra-class and inter-class
information by employing local graph structures. This is advantageous in cases where the
assumptions of LDA are not met [YXZ+07].
6.4.3
Local Fisher Discriminant Analysis
Local Fisher Discriminant Analysis (LFDA) [Sug07] deﬁnes the within-class and
between-class relationships, by using graph relationships. It is assumed that the data are
embedded in an intrinsic graph G(V, E, ˜
W(w)) and a penalty graph G(b)(V, E, ˜
W(b)), where
matrix ˜
W(w) expresses local relationships between data belonging to the same class, while

172
Graph-Based Social Media Analysis
the matrix ˜
W(b) expresses local relationships between data placed at the borders of diﬀerent
classes. ˜
W(w), ˜
W(b) are deﬁned by:
W (w)
ij
=
(
sij
Nci ,
cj = ci
0,
otherwise
(6.4.14)
W (b)
ij
=
(
sij

1
N −
1
Nci

,
cj = ci
1
N ,
otherwise,
(6.4.15)
where sij is a measure of similarity between xi and xj, such as the heat kernel function
(6.2.1). The optimal data projection matrix P is obtained by maximizing the objective
function:
P = arg max
P
tr
 
PT ˜S(w)P
−1 
PT ˜S(b)P
 
,
(6.4.16)
where the matrices ˜S(w), ˜S(b) are deﬁned as follows:
˜S(w) = 1
2
N
X
i=1
N
X
j=1
˜W (w)
ij
(xi −xj) (xi −xj)T
(6.4.17)
˜S(b) = 1
2
N
X
i=1
N
X
j=1
˜W (b)
ij (xi −xj) (xi −xj)T .
(6.4.18)
P can be calculated by solving the generalized eigenanalysis problem ˜S(w)q = λ˜S(b)q.
After the calculation of P, the data xi are mapped to the reduced-dimensionality feature
space Rd by applying yi = PT xi. By employing weighted local graph structures, LFDA
can exploit local intra-class and inter-class information using additional geometric informa-
tion, when compared to MFA. Sometimes, this is advantageous in terms of classiﬁcation
performance [Sug07].
6.4.4
Graph Embedding
It has been shown in [YXZ+07] that a wide range of (linear and non-linear) dimension-
ality reduction criteria can be described from a graph embedding point of view. Let G(V, E)
be an undirected weighted graph, where we assume that the training samples xi reside on
graph vertices and W ∈RN×N is the corresponding graph weight (or adjacency) matrix.
The diagonal matrix D ∈RN×N and the graph Laplacian matrix L ∈RN×N are deﬁned
by Dii = P
i Wij, i = 1, ..., N and L = D −W, respectively. The graph Laplacian matrix
L can be employed, in order to describe criteria exploited in several subspace learning tech-
niques, like LDA, ISOMAP, LLE, LE, etc. Let us denote by LX the graph Laplacian matrix
describing a certain criterion X. Then, the criterion X can be modelled using a matrix of
the form:
SX = XLXXT .
(6.4.19)
For example, the total scatter matrix employed in Principal Component Analysis (PCA)
[DHS00] can be expressed as follows:
ST = XLT XT .
(6.4.20)
The within-class and between-class scatter matrices employed in LDA have the form:
Sw = XLwXT
(6.4.21)

Graph-Based Pattern Classiﬁcation and Dimensionality Reduction
173
Sb = NST −Sw.
(6.4.22)
The corresponding graph Laplacian matrices LT , Lw are given by:
LT
=
I −1
N eeT
(6.4.23)
Lw
=
I −
C
X
c=1
1
Nc
ececT ,
(6.4.24)
where e ∈RN is a vector of ones, I ∈RN×N is the identity matrix and ec ∈RN is a vector
with ec
j = 1, if cj = C and ec
l = 0 otherwise. After the calculation of the graph Laplacian
matrices describing the criteria of interest, the data projection matrix can be obtained by
minimization:
P =
arg min
tr(PT XLpXT P)
tr
 PT XLXT P

,
(6.4.25)
where L is the graph Laplacian matrix describing the criterion to be minimized and Lp
describes the penalty criterion (to be maximized).
6.4.5
Minimum Class Variance Extreme Learning Machine
Minimum Class Variance Extreme Learning Machine (MCVELM) [ITP13a] is an algo-
rithm for Single-hidden Layer Feedforward Neural (SLFN) network training that exploits
nonlinear data relationships. For a classiﬁcation problem involving vectors xi ∈RD, each
belonging to one of the C classes, the network should consist of D input, L hidden and
C output neurons. Usually, the number L of hidden layer neurons is much greater than
the number C of action classes that are involved in the classiﬁcation problem, i.e., L ≫C
[HZS04, HZDZ12].
The network target vectors ti = [ti1, ..., tiC]T , each corresponding to a vector xi, are set
to tik = 1, when ci = k, and to tik = −1 otherwise. The network input weights Win ∈RD×L
and the hidden layer bias values b ∈RL are randomly assigned, while the network output
weights Wout ∈RL×C are analytically calculated. It has been shown that almost any
nonlinear piecewise continuous activation functions Φ(·) can be used for the calculation
of the network hidden layer outputs, like the sigmoid, sine, Gaussian, hard-limiting, and
Radial Basis Functions (RBF), Fourier series, etc [HCS06, HZDZ12]. By using Win and b,
each training vector xi is mapped to a vector φi, corresponding to the network hidden layer
output.
The network output weights are subsequently determined by solving the following opti-
mization problem:
min
Wout
1
2∥S
1
2wWout∥2
F + γ
2
N
X
i=1
∥ξi∥2
2,
(6.4.26)
subject to the constraints:
WT
outφi = ti −ξi, i = 1, ..., N,
(6.4.27)
where ξi ∈RC is the error vector corresponding to training vector xi and γ is a parameter
denoting the importance of the training error in the optimization problem. Sw is the within-
class variance matrix used in LDA (6.4.5) to describe class dispersions. When the classes are
multimodal in the feature space determined by the network hidden layer outputs, the intra-
class dispersion can be described accordingly [ITP13a]. An extension of the above-described
formulation that exploits the total scatter of the data (6.4.20) has also been proposed in
[ITP14b].

174
Graph-Based Social Media Analysis
By substituting (6.4.27) in the optimization problem (6.4.26) and solving for
ϑLP
ϑWout = 0,
Wout is given by:
Wout =

ΦΦT + 1
γ Sw
−1
ΦTT .
(6.4.28)
An extension of the algorithm that exploits local class information deﬁned on a neigh-
borhood graph for Wout calculation has been proposed in [ITP14a], where the following
optimization problem is solved:
min
Wout
 
1
2∥Wout∥2
F + γ
2
N
X
i=1
∥ξi∥2
2 + λ
2 tr

WT
out(ΦLΦT )Wout
!
,
(6.4.29)
s.t. WT
outφi = ti −ξi, i = 1, ..., N.
(6.4.30)
In (6.4.29), tr(·) is the trace operator and L is a graph Laplacian matrix calculated by using
a class neighboring graph on φi. The network output weights Wout are given by:
Wout =
 
Φ

I + λ
γ L

ΦT + 1
γ I
!−1
ΦTT .
(6.4.31)
ELM-based classiﬁcation schemes have been found to be both eﬃcient and eﬀective
[HCS06, HZDZ12]. When compared to standard neural network training approaches, like
the Back-Propagation [RHW86] and the Levenberg-Marquardt [HM94] algorithms, ELM
requires lower human supervision and leads to faster network training. ELM algorithms
exploiting class variance criteria are able to enhance its classiﬁcation performance [ITP13a,
ITP14a, ITP14b].
6.4.6
Minimum Class Variance Support Vector Machine
Minimum Class Variance SVM (MCVSVM) [TKP01, ZTP07c] exploits statistical class
properties in the SVM optimization process. Speciﬁcally, it modiﬁes the regularizer of the
SVM formulation, in order to exploit the intra-class dispersion of the training data used in
LDA (6.4.5), leading to the following SVM formulation:
min
w,b
1
2wT Sww + γ
N
X
i=1
ξi,
wT Sww ≥0,
(6.4.32)
subject to the constraints:
ci
 wT xi + b

≥1 −ξi,
ξi ≥0,
i = 1, . . . , N.
(6.4.33)
Non-linear decision functions have been determined by exploiting the kernel trick [SS01,
BNS06]. In the cases of multi-modal classes, i.e., classes formed by multiple subclasses, the
intra-class dispersion can be described accordingly [OT12, GMK12]. MCVSVM, exploits
both information relating to the support vectors and the class compactness by incorporating
the within-class variance of the training data in the SVM optimization problem. In several
cases, this leads to enhanced classiﬁcation performance [OT12, GMK12].
6.4.7
Graph Embedded Support Vector Machines
Graph Embedded SVM [AT12] (GESVM) extends MCVSVM, in order to incorporate
geometrical criteria of the data used in the Graph Embedding framework [YXZ+07]. It

Graph-Based Pattern Classiﬁcation and Dimensionality Reduction
175
assumes that the data have been embedded in a graph G(V, E), with a graph weight matrix
W expressing geometric data relationships. The following regularizer is incorporated in the
SVM formulation:
J(w) = wT  XLXT 
w,
(6.4.34)
leading to the following SVM optimization problem:
min
w,b
1
2∥w∥2
2 + γ
N
X
i=1
ξi + λ
2 wT  XLXT 
w,
(6.4.35)
subject to the constraints:
ci
 wT xi + b

≥1 −ξi,
ξi ≥0,
i = 1, . . . , N.
(6.4.36)
In the above, γ, λ are the two regularization parameters relating to the training error of the
SVM classiﬁer and the trade-oﬀbetween the two regularization terms, respectively. Non-
linear decision functions can been determined by exploiting the kernel trick [SS01, BNS06].
Depending on the structure of the graph employed by the GESVM, global or local class
information is incorporated in the SVM optimization problem. In several cases, this leads
to enhanced classiﬁcation performance.
6.5
Semi-Supervised Methods
Semi-Supervised methods employ labeling information that is available for some graph
vertices and the geometric data relationships that are encoded in the graph, in order to
either determine a data mapping to a low-dimensional feature space, or to learn a mapping
xi
f(xi)
→ci that can be used in order to map a new test sample xt (not belonging to the graph)
to one of the classes in C. In this section, we ﬁrst describe an extension of the supervised
dimensionality reduction methods that is able to exploit both labeled and unlabeled data.
Subsequently, we describe semi-supervised classiﬁcation methods that exploit geometric
information provided by the graph structure, in order to enhance classiﬁcation performance.
Before starting the description of these methods, we brieﬂy present the assumptions
imposed on the structure of the underlying data distribution. Semi-supervised methods
make use of at least one of the following assumptions [CSZ06]:
1. Smoothness assumption. Data which are close to each other are more likely to belong
to the same class.
2. Cluster assumption. The data tend to form discrete clusters. Data in each cluster are
more likely to belong to the same class.
3. Manifold assumption. The data lie on a manifold of much lower dimensionality than
that of the original data domain RD.
From the above, the ﬁrst assumption indicates the need of simple decision boundaries, re-
siding in low data-density regions. The second assumption indicates that classes may be
multimodal, i.e., classes may be formed by multiple clusters. Finally, the third assump-
tion is useful in cases where the input space RD is high-dimensional, in order to reduce
dimensionality to avoid the so-called curse of dimensionality [Mem00].

176
Graph-Based Social Media Analysis
6.5.1
Semi-Supervised Discriminant Analysis
Semi-Supervised Discriminant Analysis
(SDA) [CHH07] aims at ﬁnding a low-
dimensional feature space by using discriminant information inferred from the labeled data
and geometrical information inferred both from the labeled and unlabeled data. It is as-
sumed that the data are embedded in a graph G(V, E), with a graph weight matrix W
expressing the local relationships of both the labeled and unlabeled graph nodes. W is
deﬁned by:
Wij =
(
wij,
i ∈Nj and j ∈Ni
0,
otherwise.
(6.5.1)
The geometrical data structure is expressed by incorporating the following regularizer:
J(P) = PT XLXT P,
(6.5.2)
in the LDA optimization problem. By using J(P), the optimal data projection is given by:
P = arg max
P
tr(PT SbP)
tr(PT (ST + αXLXT ) P),
(6.5.3)
where α is a regularization parameter determining the importance of the geometrical data
structure in the optimization problem and ST is the total scatter matrix calculated by using
both the labeled and the unlabeled data. Furthermore, an extension of SDA that exploits
global and local geometrical data structure by incorporating regularization terms related
to PCA and LPP has been described in [ZLPW13]. P can be calculated by solving the
generalized eigenanalysis problem Sbq = λ
 ST + αXLXT 
q.
After the calculation of P, the data xi are mapped to the reduced-dimensionality feature
space Rd by applying yi = PT xi.
6.5.2
Laplacian Support Vector Machine
Laplacian Support Vector Machine (LapSVM) has been proposed within the context of
Manifold Regularization [BNS06, MB11]. It incorporates information relating to the geo-
metrical structure of both the labeled and unlabeled data in the SVM optimization process.
In its simplest form, it assumes that the data have been embedded in a graph G(V, E),
with a graph weight matrix W expressing local relationships of both the labeled and un-
labeled data. The k-nearest neighbor graph based on the heat kernel is usually employed.
The following regularizer is incorporated in the SVM formulation:
J(w) = wT  XLXT 
w,
(6.5.4)
leading to the following SVM optimization problem:
min
w,b ∥w∥2
2 + c1
N
X
i=1
ξi + c2
N 2 wT  XLXT 
w,
(6.5.5)
where L is the graph Laplacian matrix calculated by using both the labeled and unlabeled
data. This optimization is subject to the constraints:
ci
 wT xi + b

≥1 −ξi,
ξi ≥0,
i = 1, . . . , N.
(6.5.6)
In the above, c1, c2 are the regularization parameters relating the training error and the
Laplacian regularization. The normalized graph Laplacian:
˜L = D−1
2 LD−1
2
(6.5.7)
can also be used. Non-linear decision functions can been determined by exploiting the kernel
trick [SS01, BNS06]. Instead of using regularization based on similarity metrics, regularizers
exploiting sparsity constraints can be used [FGQZ11]. An extension of the above described
formulation has also been proposed for semi-supervised feature selection in [XKLJ10].

Graph-Based Pattern Classiﬁcation and Dimensionality Reduction
177
6.5.3
Semi-Supervised Extreme Learning Machine
Semi-Supervised Extreme Learning Machine (S-ELM) [LCLZ11] is an algorithm for
SLFN network training that can incorporate geometric information related to both labeled
and unlabeled training data in ELM training. As in the MCVELM algorithm described
in Section 6.4.5, the SELM input weights Win ∈RD×L and the hidden layer bias values
b ∈RL are randomly assigned, while the network output weights Wout ∈RL×C are ana-
lytically calculated. The network target vectors ti ∈RC are set to a) tij = 1 for vectors
belonging to class j, i.e., ci = j, b) tij = −1 for vectors not belonging to class j, i.e., and
ci ̸= j and c) tij = 0, for unlabeled vectors.
Let us denote by φi, i = 1, . . . , l, . . . , N the hidden layer outputs for the entire training
set. SELM solves the following optimization problem for the calculation of the network
output weights:
Wout
=
arg min
Wout
∥WT
outΦ −T∥F ,
s.t.
N
X
i=1
N
X
j=1
wij
 WT
outφi −WT
outφj
2 = 0,
(6.5.8)
where Φ = [φ1, . . . , φN] is a matrix containing the network hidden layer outputs, T ∈
RC×N is a matrix containing the network target vectors ti and wij is a value denoting the
similarity between φi and φj in the feature space determined by the network hidden layer
outputs. That is, in SELM, it is assumed that the data representations in the so-called ELM
space are embedded in a graph G(V, E), where V contains the nodes {φ1, . . . , φN} and the
correspoinding graph weight matrix W expresses the similarity of the training data in the
ELM space according to a metric, usually the heat kernel function. The calculation of the
data similarity values win in the ELM space RL, rather than in the input space RD, has
the advantage that nonlinear data relationships can be better exploited.
By solving (6.5.8), Wout is given by:
Wout =
  J + λLT 
Φ
† JTT .
(6.5.9)
The diagonal matrix J = diag(1, 1, . . . , 0, 0) has the ﬁrst l diagonal entries equal to 1
and the rest equal to 0. L is the Graph Laplacian matrix [BNS06] encoding the similarity
between the training vectors and A† =
 AAT −1 A is the Moore-Penrose pseudoinverse
of AT . A regularized version of the SELM algorithm has been proposed in [HSGW14]. In
addition, a regularized version of the SELM algorithm that also exploits discriminan criteria
has been proposed in [ITP14c]. Semi-supervised learning using ELM-approaches has been
shown to be eﬀective, since it has been shown that it can outperform related approaches
[HSGW14, ITP14c], like LapSVM.
6.6
Applications
In this section, we discuss some problems, where graph-based pattern recognition and
machine learning techniques have been applied, focusing on areas relating to social media
analysis.
Recent advances in technological equipment, like digital cameras, smart-phones, etc.,
have led to an explosive increase of the captured digital media, e.g., images and videos.
As expected, most of these data are acquired in order to describe human presence and

178
Graph-Based Social Media Analysis
activity and are exploited, e.g., for monitoring (visual surveillance and security) or for per-
sonal/social use and entertainment. Images play an important role in many applications,
including social media. For example, social network users may share images or rate the
images of their friends, or tag them in their images [WGLF10, DY11]. Two tasks that are
of particular interest in social media tagging and annotation propagation are image seg-
mentation and face recognition [Pit00]. In the ﬁrst case, the objective is to automatically
partition an image in homogeneous regions. This is an important pre-processing step that
facilitates other image processing tasks, e.g., face/object recognition. Graph-based tech-
niques have been found to be eﬀective in this task, since, by exploiting graph structures,
both the quantitative and topological relationships of the image regions can be exploited
[SM00, FH04, KH09, ZFFX10, JLFW10, JT11].
Face recognition assigns a person ID label to a facial image depicting this person. Graph-
based techniques have been employed for this task in both the image representation and clas-
siﬁcation phases. In the ﬁrst case, graph-based representations combined with elastic graph
matching have been found to be eﬀective [WFKM97, TKP01, TKP02, ZTP07a, ZTP07b].
In the latter case, machine learning techniques that exploit an underlying graph structure
can be employed to determine a low-dimensional manifold where the facial images reside
[RS00, TS00, MK01, BN03, YXZ+07], or to classify the facial images [BNS06, ZTP07c,
AT12, HZDZ12].
Video content analysis, especially human action recognition from videos, has been a very
active research ﬁeld nowadays. Several graph-based approaches have been proposed to this
end, including spatio-temporal graphs [BT11, GZSRC11, CWSL12, AMA14] and generative
Delaunay graphs [TH06]. Methods exploiting graph structures have also been employed in
human action classiﬁcation [YZLZ10, DFHP10, DFHP10, TCFL12, ITP13a, ITP14c].
Recently, social media, i.e., images and videos, have been exploited for automatic social
behaviour inference [WCW09, EPL09, GCR09, YLPR09, CZG09, DY10, WGLF10]. This
approach exploits the fact that the people depicted in social media images or videos often
share social relationships, e.g., are members of the same family [WGLF10], colleagues in
work or school, athletes in a football game [PY10], opponents in a wrangle [LW09], etc.
Furthermore, the social relationship between the depicted persons inﬂuences their relative
position and appearance in the image/video. Thus, social media analysis can be exploited to
automatically infer such social relationships in movies, personal collections, sports, surveil-
lance, etc. An analysis that has been shown to provide promising results describes such
relationships by using graph structures, in which graph nodes represent the diﬀerent ac-
tors/persons and graph edges express the corresponding social relationships. Such relation-
ships may be considered to be static, i.e., not changing in time [WCW09, YLPR09], or
evolving [YCZ+09, DY11].
6.7
Conclusions
In this chapter, some of the most widely used methods for dimensionality reduction and
classiﬁcation that assume an underlying graph structure have been described. The methods
have been divided into three categories, i.e. unsupervised, supervised and semi-supervised,
based on their requirement of manual (user-derived) annotation. Their application in social
media analysis has also been discussed by reviewing recent work in diﬀerent applications.

Graph-Based Pattern Classiﬁcation and Dimensionality Reduction
179
Bibliography
[AMA14]
B. N. Aoun, M. Mejdoub, and C. B. Amar. Graph-based approach for human
action recognition using spatio-temporal features. Journal of Visual Commu-
nication and Image Representation, 25(2):329–338, 2014.
[AT12]
G. Arvanitidis and A. Tefas. Exploiting graph embedding in support vector
machines. In Proc. IEEE International Workshop on Machine Learning for
Signal Processing, 2012.
[Ata98]
M. J. Atallah. Basic Graph Algorithms In Algorithms and Theory of Compu-
tation Handbook. CRC Press, 1998.
[BN03]
M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction
and data representation. Neural Computation, 15(6):1373–1396, 2003.
[BNS06]
M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A geomet-
ric framework for learning from labeled and unlabeled examples. Journal of
Machine Learning Research, 7:2399–2434, 2006.
[BPRB13]
E. Z. Borzeshi, M. Piccardi, K. Riesen, and H. Bunke. Discriminative prototype
selection methods for graph embedding. Pattern Recognition, 46(6):1648–1657,
2013.
[BR11]
H. Bunke and K. Riesen. Recent advances in graph-based pattern recognition
with applications in document analysis. Pattern Recognition, 44(5):1057–1067,
2011.
[BT11]
W. Brendel and S. Todorovic. Learning spatiotemporal graphs of human ac-
tivities. In Proc. International Conference on Computer Vision, 2011.
[CH03]
X. W. Chen and T. Huang. Facial expression recognition: A clustering based
approach. Pattern Recognition Letters, 24(9):1295–1302, 2003.
[CHH07]
D. Cai, X. He, and J. Han. Semi-supervised discriminant analysis. In Proc.
International Conference on Computer Vision, pages 1–7, 2007.
[CL06]
R. R. Coifman and S. Lafon.
Diﬀusion maps.
Applied and Computational
Harmonic Analysis, 21(1):5–30, 2006.
[CMC+09]
T. S. Caetano, J. J. McAuley, L. Cheng, Q. V. Le, and A. J. Smola. Learn-
ing graph matching.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 31(6):1048–1058, 2009.
[CSZ06]
O. Chapelle, B. Scholkopf, and A. Zien. Semi-supervised learning. MIT Press,
2006.
[CWSL12]
O. Celiktutan, C. Wolf, B. Sankur, and E. Lombardi. Real-time exact graph
matching with application in human action recognition. In Proc. International
Conference on Human Behavior Understanding, 2012.
[CZG09]
J. Chen, O. Zaiane, and R. Goebel. Detecting communities in social networks
using max-min modularity. SIAM International Conference on Data Mining,
2009.

180
Graph-Based Social Media Analysis
[DFHP10]
L. Ding, Q. Fan, J. Hsiao, and S. Pankanti. Graph-based event detection from
realistic videos using weak feature correspondence. In Proc. IEEE International
Conference on Accoustics, Speech, and Signal Processing, 2010.
[DHS00]
R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classiﬁcation, 2nd ed. Wiley,
2000.
[DY10]
L. Ding and A. Yilmaz. Learning relations among movie characters: A social
network perspective. In Proc. European Conference on Computer Vision, 2010.
[DY11]
L. Ding and A. Yilmaz. Inferring social relations from visual concepts. In Proc.
International Conference on Computer Vision, 2011.
[EPL09]
N. Eagle, A. Pentland, and D. Lazer. Inferring friendship network structure
by using mobile phone data. In Proc. of the National Academy of Sciences,
volume 106, pages 15274–15278, 2009.
[EWH07]
D. Emms, R. Wilson, and E. Hancock. Graph embedding using quantum com-
mute times. In Graph-Based Representations in Pattern Recognition. Springer,
2007.
[FC11]
S. L. France and J. D. Carroll. Two-way multidimensional scaling: A review.
IEEE Transactions on Systems, Man and Cybernetics - Part C: Applications
and Reviews, 41(5):644–661, 2011.
[FGQZ11]
M. Fan, N. Gu, H. Qiao, and B. Zhang.
Sparse regularization for semi-
supervised classiﬁcation. Pattern Recognition, 44:1777–1784, 2011.
[FH04]
P. F. Felzenszwalb and D. P. Huttenlocher. Eﬃcient graph-based image seg-
mentation. International Journal of Computer Vision, 59(2):167–181, 2004.
[GCR09]
W. Ge, R. Collins, and B. Ruback. Automatically detecting the small group
structure of a crowd. Applications of Computer Vision, 2009.
[GDIHK11] R. Gonzalez-Diaz, A. Ion, M. I. Ham, and W. G. Kropatsch. Invariant rep-
resentative cocycles of cohomology generators using irregular graph pyramids.
Computer Vision and Image Understanding, 115(7):1011–1022, 2011.
[GMK12]
N. Gkalelis, V. Mezaris, and I. Kompatsiaris. Linear subclass support vector
machines. IEEE Signal Processing Letters, 19(9):575–5784, 2012.
[GZSRC11] U. Gaur, Y. Zhu, A. Song, and A. Roy-Chowdhury. A string of feature graphs
model for recognition of complex activities in natural videos. In Proc. Inter-
national Conference on Computer Vision, 2011.
[HCS06]
G. B. Huang, L. Chen, and C. K. Siew. Universal approximation using incre-
mental constructive feedforward networks with random hidden nodes. IEEE
Transactions on Neural Networks, 17(4):879–892, 2006.
[HM94]
M. T. Hagan and M. B. Menhaj.
Training feedforward networks with the
Marquardt algorithm. IEEE Transactions on Neural Networks, 5(6):989–993,
1994.
[HN04]
X. He and P. Niyogi. Locality preserving projections. In Proc. Neural Infor-
mation Processing Systems Conference, 2004.

Graph-Based Pattern Classiﬁcation and Dimensionality Reduction
181
[HSGW14]
G. Huang, S. Song, J. N. D. Gupta, and C. Wu.
Semi-supervised and un-
supervised extreme learning machines.
IEEE Transactions on Cybernetics,
44(12):2405–2417, 2014.
[HW10]
Abdi. H. and L. J. Williams. Principal component analysis. Wiley Interdisci-
plinary Reviews: Computational Statistics, 2:433–459, 2010.
[HZDZ12]
G. B. Huang, H. Zhou, X. Ding, and R. Zhang. Extreme learning machine for
regression and multiclass classiﬁcation. IEEE Transactions on Systems, Man,
and Cybernetics, Part B: Cybernetics, 42(2):513–529, 2012.
[HZS04]
G. B. Huang, Q. Y. Zhu, and C. K. Siew. Extreme learning machine: a new
learning scheme of feedforward neural networks. In Proc. IEEE International
Joint Conference on Neural Networks, 2004.
[ITP13a]
A. Iosiﬁdis, A. Tefas, and I. Pitas. Minimum class variance extreme learning
machine for human action recognition. IEEE Transactions on Circuits and
Systems for Video Technology, 23(11):1968–1979, 2013.
[ITP13b]
A. Iosiﬁdis, A. Tefas, and I. Pitas. On the optimal class representation in linear
discriminant analysis. IEEE Transactions on Neural Networks and Learning
Systems, 24(9):1491–1497, 2013.
[ITP14a]
A. Iosiﬁdis, A. Tefas, and I. Pitas. Exploiting local class information in extreme
learning machine. In Proc. International Conference on Neural Computation
Theory and Applications, 2014.
[ITP14b]
A. Iosiﬁdis, A. Tefas, and I. Pitas. Minimum variance extreme learning machine
for human action recognition.
In Proc. IEEE International Conference on
Acoustics, Speech and Signal Processing, 2014.
[ITP14c]
A. Iosiﬁdis, A. Tefas, and I. Pitas. Semi-supervised classiﬁcation of human
actions based on neural networks. In Proc. IEEE International Conference on
Pattern Recognition, 2014.
[JH05]
D. Justice and A. Hero.
A binary linear programming formulation of the
graph edit distance. IEEE Transactions on on Pattern Analysis and Machine
Intelligence, 28(8):1200–1214, 2005.
[JLFW10]
M. Jiang, C. Li, J. Feng, and L. Wang. Segmentation via NCuts and lossy
minimum description length: a uniﬁed approach. In Proc. Asian Conference
on Computer Vision, 2010.
[JM07]
J. Johns and S. Mahadevan. Constructing basis functions from directed graphs
for value function approximation. In Proc. International Conference on Ma-
chine Learning, 2007.
[JNZ09]
Y. Jia, F. Nie, and C. Zhang. Trace ratio problem revisited. IEEE Transactions
on Neural Networks, 20(4):729–735, 2009.
[JT11]
S. Jouili and S. Tabbone. Towards performance evaluation of graph-based rep-
resentation. In Proc. International Conference on Graph-based Representations
in Pattern Recognition, 2011.
[KH09]
H. Kim and A. Hilton. Graph-based foreground extraction in extended color
space. In Proc. IEEE International Conference on Image Processing, 2009.

182
Graph-Based Social Media Analysis
[KL02]
R. I. Kondor and J. D. Laﬀerty. Diﬀusion kernels on graphs and other discrete
input spaces. In Proc. International Conference on Machine Learning, 2002.
[Kru64a]
J. B. Kruskal.
Multidimensional scaling by optimizing goodness of ﬁt to a
nonmetric hypothesis. Psychometrika, 29:1–27, 1964.
[Kru64b]
J. B. Kruskal. Nonmetric multidimensional scaling: A numerical method. Psy-
chometrika, 29:115–129, 1964.
[KTI03]
H. Kashima, K. Tsuda, and A. Inokuchi. Marginalized kernels between labeled
graphs. In Proc. International Conference on Machine Learning, 2003.
[KZP06]
S. B. Kotsiantis, I. D. Zaharakis, and P. E. Pintelas.
Machine learning: a
review of classiﬁcation and combining techniques. Artiﬁcial Intelligence Review,
26(3):159–190, 2006.
[LCLZ11]
J. Liu, Y. Cheng, M. Liu, and Z. Zhao. Semi-supervised ELM with application
in sparse calibrated location estimation. Neurocomputing, 74:2566–2572, 2011.
[LHM11]
Q. Liu, Y. Huang, and D. N. Metaxas. Hypergraph with sampling for image
retrieval. Pattern Recognition, 44(10):2255–2262, 2011.
[LRLB13]
M. M. Luqman, J. Y. Ramel, J. Llados, and T. Brouard. Fuzzy multilevel
graph embedding. Pattern Recognition, 46:551–565, 2013.
[LW09]
J. Lin and W. Wang. Weakly-supervised violence detection in movies with
audio and video based co-training. Advances in Multimedia Information Pro-
cessing, 2009.
[MB11]
S. Melacci and M. Belkin. Laplacian support vector machines trained in the
primal. Journal of Machine Learning Research, 12:1149–1184, 2011.
[Mem00]
A. Memoire. High-dimensional data analysis: The curses and blessings of di-
mensionality. In Proc. American Mathematical Society Conference Math Chal-
lenges of the 21st Century, 2000.
[MK01]
A. M. Martinez and A. C. Kak. PCA versus LDA. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 23(2):228–233, 2001.
[NB05]
M. Neuhaus and H. Bunke. Self-organizing maps for learning the edit costs in
graph matching. IEEE Transactions on Systems, Man, and Cybernetics, Part
B: Cybernetics, 35(3):503–514, 2005.
[NdC11]
M. Nascimento and A. de Carvalho. Spectral methods for graph clustering - a
survey. European Journal of Operational Research, 211(2):221–231, 2011.
[NJW02]
A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering: Analysis and an
algorithm. Advances in Neural Information Processing Systems, 2002.
[NLCK05]
B. Nadler, S. Lafon, R. R. Coifman, and I. G. Kevrekidis. Diﬀusion maps,
spectral clustering and eigenfunctions of Fokker-Planck operators. Advances in
Neural Information Processing Systems, 2005.
[Nor98]
J. R. Norris. Markov chains. Cambridge University Press, 1998.
[OT12]
G. Orphanidis and A. Tefas. Exploiting subclass information in support vector
machines.
In Proc. IEEE International Conference on Pattern Recognition,
2012.

Graph-Based Pattern Classiﬁcation and Dimensionality Reduction
183
[PD05]
E. Pekalska and R. Duin. The dissimilarity representation for pattern recogni-
tion: foundations and applications. World Scientiﬁc Publishing, 2005.
[PF12]
L. Pu and B. Faltings. Hypergraph learning with hyperedge expansion. In
Proc. European Conference on Machine Learning and Knowledge Discovery in
Databases, 2012.
[Pit00]
I. Pitas. Digital Image Processing Algorithms and Applications. Wiley, 2000.
[PY10]
K. J. Park and A. Yilmaz. Social network approach to analysis of soccer games.
In Proc. IEEE International Conference on Pattern Recognition, 2010.
[RB09a]
K. Riesen and H. Bunke. Approximate graph edit distance computation by
means of bipartite graph matching. Image and Vision Computing, 27(7):950–
959, 2009.
[RB09b]
K. Riesen and H. Bunke. Graph classiﬁcation by means of Lipschitz embedding.
IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics,
39(6):1472–1483, 2009.
[RHW86]
D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations
by back-propagating errors. Nature, 323(6088):533–536, 1986.
[RNB07]
K. Riesen, M. Neuhaus, and H. Bunke. Graph embedding in vector spaces by
means of prototype selection. In Proc. International Conference on Graph-based
Representations in Pattern Recognition, 2007.
[RR13]
A. Romey and S. Ruggieri. A multidisciplinary survey on discrimination anal-
ysis. The Knowledge Engineering Review, FirstView:1–57, 2013.
[RS00]
S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally
linear embedding. Science, 290(5500):2323–2326, 2000.
[Sch07]
S. E. Schaeﬀer. Graph clustering. Computer Science Review, 1(1):27–64, 2007.
[SM97]
J Shi and J. Malik. Normalized cuts and image segmentation. In Proc. IEEE
Conference on Computer Vision and Pattern Recognition, 1997.
[SM00]
J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, 22(8):888–905, 2000.
[SP11]
V. Satuluri and S. Parthasarathy.
Symmetrizations for clustering directed
graphs. In Proc. International Conference on Extending Database Technology,
2011.
[SR03]
L. K. Saul and S. T. Roweis. Think globally, ﬁt locally: Unsupervised learning
of low dimensional manifolds. Journal of Machine Learning Research, 4:119–
155, 2003.
[SS01]
B. Scholkopf and A. J. Smola. Learning with kernels. MIT Press, 2001.
[Sug07]
M. Sugiyama. Dimensionality reduction of multimodal labeled data by local
ﬁsher discriminant analysis. Journal of Machine Learning Research, 8:1027–
1061, 2007.
[TCFL12]
C. C. Tseng, J. C. Chen, C. H. Fang, and J. J. J. Line. Human action recogni-
tion based on graph-embedded spatio-temporal subspace. Pattern Recognition,
45(10):3611–3624, 2012.

184
Graph-Based Social Media Analysis
[TH06]
A. Torsello and E. R. Hancock. Learning shape-classes using a mixture of tree-
unions.
IEEE Transactions on Pattern Analysis and Machine Intelligence,
28(6):954–967, 2006.
[TKP01]
A. Tefas, C. Kotropoulos, and I. Pitas.
Using support vector machines to
enhance the performance of elastic graph matching for frontal face authen-
tication. IEEE Transactions on Pattern Analysis and Machine Intelligence,
23(7):735–746, 2001.
[TKP02]
A. Tefas, C. Kotropoulos, and I. Pitas. Face veriﬁcation using elastic graph
matching based on morphological signal decomposition.
Signal Processing,
82(6):833–851, 2002.
[TS00]
J. Tenenbaum and J. C. Silva, V. andLangford. A global geometric framework
for nonlinear dimensionality reduction. Science, 290:2319–2323, 2000.
[VBGS06]
S. V. N. Vishwanathan, K. M. Borgwardt, O. Guttman, and A. J. Smola.
Kernel extrapolation. Neurocomputing, 69(7):721–729, 2006.
[VSKB10]
S. V. N. Vishwanathan, N. N. Schraudolph, R. Kondor, and K. M. Borgwardt.
Graph kernels. Journal of Machine Learning Research, 11(4):1201–1242, 2010.
[WCW09]
C. Y. Weng, W. T. Chu, and J. L. Wu.
Rolenet: Movie analysis from the
perspective of social networks. IEEE Transactions on Multimedia, 11(2):256–
271, 2009.
[Wei99]
Y. Weis. Segmentation using eigenvectors: A unifying view. In Proc. IEEE
Conference on Computer Vision and Pattern Recognition, 1999.
[WFKM97] L Wiskott, J. M. Fellous, N. Kuiger, and C. Malsburg. Face recognition by
elastic bunch graph matching. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 19(7):775–779, 1997.
[WGLF10]
G. Weang, A. Gallagher, J. Luo, and D. Forshyth.
Seeing people in social
context: Recognizing people and social relationships. European Conference on
Computer Vision, 2010.
[WHL05]
R. Wilson, E. Hancock, and B. Luo.
Pattern vectors from algebraic graph
theory.
IEEE Transactions on Pattern Analysis and Machine Intelligence,
27(7):1112–1124, 2005.
[WYX+07] H. Wang, S. Yan, D. Xu, X. Tang, and T. Huang. Trace ratio vs. ratio trace
for dimensionality reduction. In Proc. IEEE Conference on Computer Vision
and Pattern Recognition, 2007.
[XKLJ10]
Z. Xu, I. Kin, M. R. T. Lyu, and R. Jin. Discriminative semi-supervised feature
selection via manifold regularization. IEEE Transactions on Neural Networks,
21(7):1033–1047, 2010.
[YCZ+09]
T. Yang, Y. Chi, S. Zhu, Y. Gong, and R. Jin. A Bayesian approach toward
ﬁnding communities and their evolutions in dynamic social networks. In Proc.
SIAM International Conference on Data Mining, 2009.
[YLPR09]
T. Yu, S. N. Lim, K. Patwardhan, and N. Rkahnstoever. Monitoring, recogniz-
ing and discovering social networks. In Proc. IEEE Conference on Computer
Vision and Pattern Recognition, 2009.

Graph-Based Pattern Classiﬁcation and Dimensionality Reduction
185
[YXZ+07]
S. Yan, D. Xu, B. Zhang, H. J. Zhang, Q. Yang, and S. Lin. Graph embed-
ding and extensions: A general framework for dimensionality reduction. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 29(1):40–50, 2007.
[YZLZ10]
Y. Yuan, H. Zheng, Z. Li, and D. Zhang. Video action recognition with spatio-
temporal graph embedding and spline modeling. In Proc. International Con-
ference on Acoustics, Speech and Signal Processing, 2010.
[ZFFX10]
B. Zhao, L. Fei-Fei, and E. P. Xing. Image segmentation with topic random
ﬁeld. In Proc. European Conference on Computer Vision, 2010.
[ZHS05]
D. Zhou, J. Huang, and B. Scholkopf. Learning from labeled and unlabeled data
on a directed graph. In Proc. International Conference on Machine Learning,
2005.
[ZLPW13]
X. Zhao, X. Li, C. Pang, and S. Wang. Human action recognition based on
semi-supervised discriminant analysis with global constraint. Neurocomputing,
105:45–50, 2013.
[ZM06]
Z. Zhu and A. Martnez. Subclass discriminant analysis. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 28(8):1274–1286, 2006.
[ZTP07a]
S. Zafeiriou, A. Tefas, and I. Pitas. The discriminant elastic graph matching
algorithm applied to frontal face veriﬁcation. Pattern Recognition, 40(10):2798–
2810, 2007.
[ZTP07b]
S. Zafeiriou, A. Tefas, and I. Pitas. Learning discriminant person-speciﬁc facial
models using expandable graphs. IEEE Transactions on Information Forensics
and Security, 2(1):55–68, 2007.
[ZTP07c]
S. Zafeiriou, A. Tefas, and I. Pitas. Minimum class variance support vector
machines. IEEE Transactions on Image Processing, 16(10):2551–2564, 2007.
[ZZY+08]
D. Zhou, S. Zhu, K. Yu, X. Song, B. L. Tseng, H. Zha, and C. L. Giles. Learn-
ing multiple graphs for document recommendations.
In Proc. International
Conference on World Wide Web, 2008.


Chapter 7
Matrix and Tensor Factorization with
Recommender System Applications
Panagiotis Symeonidis
Aristotle University of Thessaloniki, Greece
7.1
Introduction ....................................................................
187
7.2
Singular Value Decomposition on Matrices for Recommender Systems .......
189
7.2.1
Applying the SVD and Preserving the Largest Singular Values ......
190
7.2.2
Generating the Neighborhood of Users/Items ........................
191
7.2.3
Generating the Recommendation List .................................
191
7.2.4
Inserting a Test User in the c-dimensional Space .....................
192
7.2.5
Other Factorization Methods ..........................................
192
7.3
Higher Order Singular Value Decomposition (HOSVD) on Tensors ..........
193
7.3.1
From SVD to HOSVD .................................................
193
7.3.2
HOSVD for Recommendations in Social Tagging Systems ...........
196
7.3.3
Handling the Sparsity Problem ........................................
200
7.3.4
Inserting New Users, Tags, or Items ..................................
201
7.3.5
Other Scalable Factorization Models ..................................
204
7.4
A Real Geo-Social System-Based on HOSVD .................................
205
7.4.1
GeoSocialRec Website .................................................
205
7.4.2
GeoSocialRec Database and Recommendation Engine ...............
207
7.4.3
Experiments ...........................................................
208
7.5
Conclusion ......................................................................
210
Bibliography ....................................................................
210
7.1
Introduction
Representing data in lower dimensional spaces has been extensively used in many dis-
ciplines, such as natural language and image processing, data mining and information re-
trieval [DDF+90]. Recommender systems deal with challenging issues such as scalability,
noise, and sparsity, and thus, matrix and tensor factorization techniques appear as an in-
teresting tool to be exploited. Symeonidis et al. [SNPM06, Sym07], for example, used SVD
for the prediction of items/ratings in recommender systems. They assumed that there is
only a small number of factors inﬂuencing the users’ preferences, and that a user’s prefer-
ence for an item is determined by how each factor applies to the user and the item. More
187

188
Graph-Based Social Media Analysis
recently, due to the Netﬂix challenge, research on matrix factorization methods, a class of
latent factor models, gained renewed momentum in the recommender systems literature,
given that many of the best performing methods used on the challenge were based on ma-
trix factorization techniques [Kor08, SM08, Kor09]. Please note that the Netﬂix challenge
was a competition for the best recommender system algorithm to predict user ratings for
movies. The competition was held by Netﬂix (http://www.netﬂixprize.com/), an on-line
DVD-rental service. In this chapter we describe matrix and tensor factorization techniques
(i.e., SVD on matrices and HOSVD on tensors) in recommender systems and social tag-
ging systems, respectively. In addition, we present a real-world recommender system for
Location-Based Social Networks, which employs tensor decomposition techniques.
While technology is developed fast, data become larger as well, and as the size of data
grows so does the diﬃculty of processing it. One way to view and process data easily is as
a matrix, which is a rectangular array of numbers. However, matrices suﬀer from big data
as well. The dimensions of matrices keep on growing fast and this fact makes analysts’ jobs
more diﬃcult. The problem of dimensionality reduction appears when data are in fact of a
higher dimension than being manageable. Dimensionality reduction attempts to reduce the
dimensionality of data to a manageable size, while keeping as much of the original important
information as possible.
The “information overload” problem aﬀects our everyday experience while we are search-
ing for knowledge on a topic. To overcome this problem, we often rely on suggestions from
others who have more experience of the topic. In Web, this is attained with the usage of
Collaborative Filtering (CF), which provides recommendations based on the suggestions of
users who have similar preferences. Since CF is able to capture the particular preferences of
a user, it has become one of the most popular methods in recommender systems. The clas-
sic CF (i.e., user-based CF and item-based CF) methods are also known as memory-based
methods and constitute the ﬁrst subgroup of the categorization of CF systems. Memory-
based methods ﬁrst load the rating matrix into the main memory and afterwards provide
recommendations based on the relationship between a user-item pair and the rest of the
rating matrix.
The second main category of CF algorithms is known as model-based algorithms, which
recommend by ﬁrst developing a model of user ratings for items. These methods ﬁt a pa-
rameterized model to the given rating matrix and provide recommendations based on the
ﬁtted model. It has been shown that model-based algorithms can eﬃciently handle scala-
bility and improve accuracy of recommendations in large datasets. Model-based approaches
can combine the eﬀectiveness of the nearest-neighbor CF algorithms in terms of accuracy,
with the eﬃciency in terms of execution time.
SVD is a technique that has been extensively used in informational retrieval. It detects
latent relationships between documents and terms. In CF, SVD can be used to form user
trends from individual preferences, by detecting latent relationships between users and
items. Therefore, with SVD, a higher level representation of the original user-item matrix
is produced, which presents a three-fold advantage: (i) it contains the main trends of user
preferences, (ii) noise is removed, (iii) it is much more condensed than the original matrix,
thus it favors scalability.
In the following, we describe matrix and tensor factorization techniques in Sections 7.2
and 7.3, respectively. Finally, in Section 7.4, we present a real-world recommender system,
which is based on HOSVD.

Matrix and Tensor Factorization with Recommender System Applications
189
7.2
Singular Value Decomposition on Matrices for Recommender
Systems
A well-known latent factor model for matrix decomposition is singular value decompo-
sition. The singular value decomposition (SVD) [Str06] of a matrix AI1×I2 can be written
as a product of three matrices, as shown in equation (7.2.1):
AI1×I2 = UI1×I1 · SI1×I2 · VT
I2×I2,
(7.2.1)
where U is the matrix with the left singular vectors of A, VT is the transpose of the
matrix V with the right singular vectors of A, and S is the diagonal matrix of ordered
singular values of A. Please note that the singular values determined by the factorization
of equation (7.2.1) are unique and satisfy σ1 ≥σ2 ≥σ3 ≥· · · ≥σI2 ≥0.
By preserving only the largest c < min{I1, I2} singular values of S, SVD results in
matrix ˆA, which is an approximation of A. In information retrieval, this technique is used
by LSI [FDD+88], to deal with the latent semantic associations of terms in texts and to
reveal the major trends in A.
To perform the SVD over a user-item matrix A, we tune the value of parameter c, of
singular values (i.e., dimensions) with the objective to reveal the major trends. The tuning
of c, is determined by the rank of matrix A. A rule of thumb, for deﬁning parameter c
is to compute the sum of elements in the main diagonal of the S matrix (also known as
the nuclear norm). Next, we preserve a suﬃcient percentage of this sum for the creation
of an approximation of the original matrix A. If we have the allowance to use less infor-
mation percentage with similar results, we just have to reduce the value of c and sum the
corresponding elements of the main diagonal of the S matrix. Therefore, a c-dimensional
space is created and each of the c dimensions corresponds to a distinctive rating trend.
Next, given the current ratings of the target user u, we enter a pseudo-user vector in the
c-dimensional space. Finally, we ﬁnd the k nearest neighbors of the pseudo-user vector in
the c-dimensional space and apply either user- or item-based similarity to compute the top-
N recommended items. Conclusively, the provided recommendations consider the existence
of user rating trends, as the similarities are computed in the reduced c-dimensional space,
where dimensions correspond to trends.
To simplify the discussion, we will use the running example illustrated in Figure 7.2.1,
where I1, . . . , I4 are items and U1, . . . , U4 are users. As shown, the example data set is
divided into a training set and a test set. The null cells(no rating) are presented as zeros.
I1
I2
I3
I4
U1
4
1
1
4
U2
1
4
2
0
U3
2
1
4
5
(a)
I1
I2
I3
I4
U4
1
4
1
0
(b)
FIGURE 7.2.1: (a) Training Set (3 × 4), (b) Test Set (1 × 4).

190
Graph-Based Social Media Analysis
7.2.1
Applying the SVD and Preserving the Largest Singular Values
Initially, we apply the SVD to a n × m matrix A (i.e., the training data of our running
example) that produces the decomposition shown in Equation (7.2.2). The matrices of our
running example are shown in Figure 7.2.2.
An×m = Un×n · Sn×m · VT
m×m.
(7.2.2)
4
1
1
4
1
4
2
0
2
1
4
5
−0.61
0.28
−0.74
−0.29
−0.95
−0.12
−0.74
0.14
0.66
An×m
Un×n
8.87
0
0
0
0
4.01
0
0
0
0
2.51
0
−0.47
−0.28
−0.47
−0.69
0.11
−0.85
−0.27
0.45
−0.71
−0.23
0.66
0.13
−0.52
0.39
−0.53
0.55
Sn×m
VT
m×m
FIGURE 7.2.2: Example of: An×m (initial matrix A), Un×m (left singular vectors of A),
Sn×m (singular values of A), VT
m×m (right singular vectors of A).
It is possible to reduce the n × m matrix S to have only c largest singular values. Then,
the reconstructed matrix is the closest rank-c approximation of the initial matrix A, as it
is shown in equation (7.2.3) and Figure 7.2.3:
A∗
n×m = Un×c · Sc×c · VT
c×m.
(7.2.3)
2.69
0.57
2.22
4.25
0.78
3.93
2.21
0.04
3.17
1.38
2.92
4.78
−0.61
0.28
−0.29
−0.95
−0.74
0.14
A∗n×i
Un×c
8.87
0
0
4.01
−0.47
−0.28
−0.47
−0.69
0.11
−0.85
−0.27
0.45
Sc×c
VT
c×m
FIGURE 7.2.3: Example of: A∗n×m (approximation matrix of A), Un×c (left singular vec-
tors of A∗), Sc×c (singular values of A∗), VT
c×m (right singular vectors of A∗).
We tune the number c of singular values (i.e., dimensions) with the objective to reveal the
major trends. The tuning of c is determined by the information percentage that is preserved
compared to the original matrix. Therefore, a c-dimensional space is created and each of
the c dimensions corresponds to a distinctive rating trend. We have to notice that in the
running example we create a 2-dimensional space using 83,7% of the total information of
the matrix (12,88/15,39). Please note that the number 15,39 is the sum of elements in the
main diagonal of Sc×c (singular values of A∗).

Matrix and Tensor Factorization with Recommender System Applications
191
7.2.2
Generating the Neighborhood of Users/Items
Having reduced the dimensional representation of the original space, we form the neigh-
borhoods of users/items in that space. Please note that the original space consists of two
subspaces:
- range of (A) whose U (see Figure 7.2.3) is an orthonormal basis. This vector space is
the column space of A, referring to users,
- range of (AT ) whose V (see Figure 7.2.3) is an orthonormal basis. This vector space
is the row space of A, referring to items.
In particular, there are two subspaces: The ﬁrst is the range of A, whose matrix Un×c is
its orthonormal basis. This vector space is the column space of A and refers to users. The
second is the range of AT , whose matrix Vm×c is its orthonormal basis. This vector space is
the row space of A and refers to items. A user-based approach relies on the predicted value
of a rating that a user gives on an item Ij. This value is computed as an aggregation of the
ratings of the user neighborhood (i.e., similar users) on this particular item. Whereas, an
item-based approach takes into consideration only the user-item rating matrix (e.g., a user
rated a movie with a rate of 3).
For the user-based approach, we ﬁnd the k nearest neighbors of pseudo user vector in
the c-dimensional space. The similarities between training and test users can be based on
Cosine Similarity. First, we compute the matrix Un×c · Sc×c and then we perform vector
similarity among rows. This n × c matrix is the c-dimensional representation for the n
users. For the item based approach, we ﬁnd the k nearest neighbors of item vector in the
c-dimensional space. First, we compute the matrix Sc×c · VT
c×m and then we determine the
vector similarity among columns. This c × m matrix is the c-dimensional representation for
the m items.
7.2.3
Generating the Recommendation List
The most often used technique for the generation of the top-N list of recommended
items, is the one that counts the frequency of each item inside the found neighborhood,
and recommends the N most frequent ones. Henceforth, this technique is denoted as Most-
Frequent item recommendation (MF). Based on MF, we sort (in descending order) the items
according to their frequency in the found neighborhood of the target user, and recommend
the ﬁrst N of them.
As another method, someone could use the predicted values for each item to rank them.
This ranking criterion, denoted as Highest Predicted Rated item recommendation (HPR),
is inﬂuenced by the the Mean Absolute Error (MAE)1 between the predicted and the real
preferences of a user for an item. HPR opts for recommending the items that are more
likely to receive a higher rating. Notice that HPR shows poor performance for the classic
CF algorithms. However, it has very good results when it is used in combination with SVD.
The reason is that in the latter it is based only on the major trends of users.
As another method, we can sum the positive rates of the items in the neighborhood,
instead of just counting their frequency. This method is denoted as the Highest Sum of
Rates item recommendation (HSR). The top-N list consists of the N items with the highest
sum. The intuition behind HSR is that it takes into account both the frequency (as MF)
and the actual ratings, because it wants to favor items that appear most frequently in the
neighborhood and have the best ratings. Assume, for example, an item Ij that has just a
1MAE =
1
n
Pn
i=1 |fi −yi|: The Mean Absolute Error (MAE) is the average of the absolute errors
ei = fi −yi, where fi is the prediction and yi the true value.

192
Graph-Based Social Media Analysis
smaller frequency than an item Ik. If Ij is rated much higher than Ik, then HSR will prefer
it over Ik, whereas MF will favor Ik.
7.2.4
Inserting a Test User in the c-dimensional Space
Related work [SKKR00] has studied SVD on CF considering the test data as apriori
known. It is evident that for a user-based approach, the test data should be considered
as unknown in the c-dimensional space. Thus, a specialized insertion process should be
used. Given the current ratings of the test user u, we enter a pseudo-user vector in the
c-dimensional space using equation (7.2.4) [FDD+88]. In the current example,we insert U4
into the 2-dimensional space, as it is shown in Figure 7.2.4:
unew = u · Vm×c · S−1
c×c
(7.2.4)
−0.23
−0.89
1
4
1
0
−0.47
0.11
−0.28
−0.85
−0.47
−0.27
−0.69
0.45
0.11
0
0
0.25
unew
u
Vm×c
S−1
c×c
FIGURE 7.2.4: Example of: unew (inserted new user vector), u (user vector), Vm×c (two
right singular vectors of V), S−1
c×c (two singular values of inverse S).
In equation (7.2.4), unew denotes the mapped ratings of the test user u, whereas Vm×c
and S−1
c×c are matrices derived from the SVD. This unew vector should be added at the end
of the Un×c matrix, which is shown in Figure 7.2.3. Notice that the inserted vector values
of test user U4 are very similar to these of U2 after the insertion, as shown in Figure 7.2.5.
This is reasonable, because these two users have similar ratings, as shown in Figure 7.2.1a
and Figure 7.2.1b.
7.2.5
Other Factorization Methods
There are many methods to decompose a matrix, in order to deal with a high-dimensional
data set. Principal-Component Analysis or simply PCA is a data mining technique that
replaces the high-dimensional original data by its projection onto the most important axes.
It is a simple method, which is based on eigenvalues and eigenvectors of a matrix and it is
quite eﬀective.
Another useful decomposition method is UV decomposition. UV is an instance of SVD
decomposition and its philosophy is that the original matrix is actually the product of
two long “thin” matrices, U and V. UV ’s most often problem is overﬁtting. To address
this problem, we can extend UV with L2 regularization, which is also known as Tikhonov
regularization [Tik63]. That is, since the basic idea of the UV decomposition is to minimize
−0.61
0.28
−0.29
−0.95
−0.74
0.14
−0.23
−0.89
FIGURE 7.2.5: The new Un+1,c matrix containing the new user (unew) that we have added.

Matrix and Tensor Factorization with Recommender System Applications
193
an element-wise loss on the elements of the predicted/approximation matrix by optimizing
the square loss, we can extend it with L2 regularization terms. After the application of a
regularized optimization criterion the possible overﬁtting can be reduced.
Another widely known method in dimensionality reduction and data analysis is Non-
Negative Matrix Factorization (NMF). The non-negative matrix factorization, also known
as non-negative matrix approximation, is a group of algorithms in multivariate analysis and
linear algebra where a matrix A is factorized into (usually) two matrices U and V, with
the property that all three matrices have no negative elements. This non-negativity makes
the resulting matrices easier to inspect. Since the problem is not exactly solvable in general,
it is commonly approximated numerically [BBL+06].
Assume that ai, . . . , aN are N non-negative input vectors and we organize them as the
columns of non-negative data matrix A. Non-negative matrix factorization seeks a small set
of K non-negative representative vectors vi, . . . , vK that can be non-negatively combined
to approximate the input vectors ai:
A ≈U × V
(7.2.5)
and
ai ≈
K
X
k=1
uknvk,
1 ≤n ≤N,
(7.2.6)
where the combining coeﬃcients ukn are restricted to be non-negative [DS05]. There are
several ways in which the U and V may be found. The main equation we presented before
(see equation (7.2.5)) is the most popular method used to ﬁnd U and V matrices.
Another method used for the decomposition of a matrix is the CUR decomposition
[MD09], which presents good properties over SVD in some cases. As it is already described,
SVD is able to reduce dimensions without losing approximation accuracy. However, many
times in high-dimensional datasets, the produced SVD matrices tend to be very dense,
which makes their process a big challenge. In contrast, CUR decomposition confronts this
problem as it decomposes the original matrix into two sparse matrices C, R and only one
dense matrix U, whose size is quite small and doesn’t much aﬀect the time complexity of
the method. Another diﬀerence between SVD and CUR decompositions is that CUR gives
an exact decomposition no matter how large the rank of the original matrix is, whereas in
SVD the k largest singular values will be at least as many as the rank of the original matrix.
7.3
Higher Order Singular Value Decomposition
(HOSVD) on Tensors
HOSVD is a generalization of singular value decomposition and has been successfully ap-
plied in several areas. In this section, we summarize the HOSVD procedure, apply HOSVD
for recommendations in Social Tagging Systems (STSs), combine HOSVD with other meth-
ods and study the limitations of HOSVD.
7.3.1
From SVD to HOSVD
Formally, a tensor is a multi-dimensional matrix. A N-order tensor A is denoted
as A ∈RI1×···×IN , with elements ai1,...,iN . The high-order singular value decomposi-

194
Graph-Based Social Media Analysis
tion [LMV00] generalizes the SVD computation to tensors. To apply HOSVD on a 3-
order tensor A, three matrix unfolding operations are deﬁned as follows [LMV00]: A1 ∈
RI1×(I2I3), A2 ∈RI2×(I1I3), A3 ∈R(I3I1)×I2. A1, A2, A3 are called the mode-1, mode-2,
mode-3 matrix unfolding of A, respectively. Please note that we deﬁne as “matrix unfold-
ing” of a given tensor the matrix representations of that tensor in which all the column
vectors are stacked one after the other. The unfoldings of A in the three modes are illus-
trated in Figure 7.3.1.
I1
I2
I3
I1
I2
I3
I1
I2
I3
I1
I2
I3
I1
I2
I3
I1
I2
I3
A1
A2
A3
FIGURE 7.3.1: Visualization of the three unfoldings of a 3-order tensor.
In the following, we will present an example of tensor decomposition adopted from
[LMV00]:
Example 7.3.1. Deﬁne a tensor A ∈R3×2×3 by a1,1,1 = a1,1,2 = a2,1,1 = −a2,1,2 =
1, a2,1,3 = a3,1,1 = a3,1,3 = a1,2,1 = a1,2,2 = a2,2,1 = −a2,2,2 = 2, a2,2,3 = a3,2,1 =
a3,2,3 = 4, a1,1,3 = a3,1,2 = a1,2,3 = a3,2,2 = 0. The tensor and its mode-1 matrix
unfolding A1 ∈RI1×I2×I3 are illustrated in Figure 7.3.2.

Matrix and Tensor Factorization with Recommender System Applications
195
1
2
1
2
2
4
0
2
4
2
4
1
-1
-2
0
0
2
0
1 
1 
0 
2 
2 
0 
1 
-1 
2 
2 
-2 
4 
2 
0 
2 
4 
0 
4 
A1 = 
FIGURE 7.3.2: Visualization of tensor A ∈R3×2×3 and its mode-1 matrix unfolding.
Next, we deﬁne the mode-n product of a N-order tensor A ∈RI1×···×IN by a matrix
U ∈RJn×In, which is denoted as A ×n U. The result of the mode-n product is a (I1 × I2 ×
· · · × In−1 × Jn × In+1 × · · · × IN)-tensor, the entries of which are deﬁned as follows:
(A ×n U)i1i2...in−1jnin+1...iN =
X
in
ai1i2...in−1inin+1...iN ujn,in.
(7.3.1)
Since we focus on 3-order tensors, n ∈{1, 2, 3}, we use mode-1, mode-2, and mode-3 prod-
ucts. In terms of mode-n products, SVD on a regular two-dimensional matrix (i. e., 2-order
tensor), can be rewritten as follows [LMV00]:
F = S ×1 U(1) ×2 U(2),
(7.3.2)
where U(1) = [u(1)
1 u(1)
2
. . . u(1)
I1 ] is a unitary (I1 × I1)-matrix and U(2) = [u(2)
1 u(2)
2
. . . u(2)
I2 ]
is a unitary (I2 × I2)-matrix. Please note that a n × n matrix U is said to be unitary if its
column vectors form an orthonormal set in the complex inner product space Cn. That is,
UT U = In. Also, S is a (I1 × I2)-matrix with the properties of:
i. pseudo-diagonality: S = diag(σ1, σ2, . . . , σmin{I1,I2})
ii. ordering: σ1 ≥σ2 ≥· · · ≥σmin{I1,I2} ≥0.
By extending this form of SVD, HOSVD of a 3-order tensor A can be written as fol-
lows [LMV00]:
A = S ×1 U(1) ×2 U(2) ×3 U(3),
(7.3.3)
where U(1), U(2), and U(3) contain the orthonormal vectors (called the mode-1, mode-2 and
mode-3 singular vectors, respectively) spanning the column space of the A1, A2, A3 matrix
unfoldings. S is called core tensor and has the property of “all orthogonality.” Please note
that “all orthogonality” means that the diﬀerent “horizontal matrices” of S (the ﬁrst index
i1 is kept ﬁxed, while the two other indices, i2 and i3, are free) are mutually orthogonal
with respect to the scalar product of matrices (i. e., the sum of the products of the corre-
sponding entries vanishes); at the same time, the diﬀerent “frontal” matrices (i2 ﬁxed) and
the diﬀerent “vertical” matrices (i3 ﬁxed) should be mutually orthogonal as well. For more
information, see [LMV00]. This decomposition also refers to a general factorization model
known as Tucker decomposition [Tuc66].
In the following, we provide a solid description of the tensor reduction method with
an outline of the algorithm for the case of Social Tagging Systems, where we have three

196
Graph-Based Social Media Analysis
participatory entities (user, item, tag). In particular, we provide details on how HOSVD is
applied to tensors and how item/tag recommendation is performed, based on the detected
latent associations.
The tensor reduction approach initially constructs a tensor based on usage data triplets
{u, i, t} of user, item and tag. The motivation is to use all three objects that interact inside
a social tagging system. Consequently, we proceed to the unfolding of A, where we build
three new matrices. Then, we apply SVD in each new matrix. Finally, we build the core
tensor S and the resulting tensor ˆ
A. The six steps of the HOSVD approach are summarized
as follows:
• Step 1: The initial tensor A construction, which is based on usage data triplets (user,
item, tag).
• Step 2: The matrix unfoldings of tensor A, where we matricize the tensor in all three
modes, creating three new matrices (one for each mode).
• Step 3: The application of SVD in all three new matrices, where we keep the c-most
important singular values for each matrix.
• Step 4: The construction of the core tensor S, that reduces the dimensionality. (see
equation (7.3.2))
• Step 5: The construction of the ˆ
A tensor, that is an approximation of tensor A. (see
equation (7.3.3))
• Step 6: Based on the weights of the elements of the reconstructed tensor ˆ
A, we rec-
ommend item/tag to the target user u.
Steps 1 −5 build a model and can be performed oﬀ-line. The recommendation in Step 6
is performed on-line, i.e., each time we have to recommend a item/tag to a user, based on
the built model.
7.3.2
HOSVD for Recommendations in Social Tagging Systems
In this subsection, we elaborate on how HOSVD can be employed for computing recom-
mendations in STSs, and present an example on how one can recommend items according to
the detected latent associations. Although we illustrate only the recommendation of items,
once the approximation ˆ
A is computed, the recommendation of users or tags is straightfor-
ward [SNM10].
The ternary relation of users, items, and tags can be represented as a third-order ten-
sor A, such that tensor factorization techniques can be employed in order to exploit the
underlying latent semantic structure in A. While the idea of computing low rank tensor
approximations has already been used for many diﬀerent purposes [LMV00, SH05, SZL+05,
WA08, CWZ07, KS08], just recently it has been applied for the problem of recommenda-
tions in STS. The basic idea is to cast the recommendation problem as a third-order tensor
completion problem — completing the non-observed entries in A.
Formally, a social tagging system is deﬁned as a relational structure F := (U, I, T , Y) in
which:
• U, I, and T are disjoint non-empty ﬁnite sets, whose elements are called users, items,
and tags, respectively, and
• Y is the set of observed ternary relations between them, i. e., Y ⊆U × I × T , whose
elements are called tag assignments.

Matrix and Tensor Factorization with Recommender System Applications
197
• A post corresponds to the set of tag assignments of a user for a given item, i. e., a
triple (u, i, Tu,i) with u ∈U, i ∈I, and a non-empty set Tu,i := {t ∈T | (u, i, t) ∈Y}.
In the following we present several approaches for recommending in STS based on ten-
sor factorization. Y which represents the ternary relation of users, items and tags can be
depicted by the binary tensor A = (au,i,t) ∈R|U|×|I|×|T | where 1 indicates observed tag
assignments and 0 missing values, i. e.,
au,i,t :=
(
1,
(u, i, t) ∈Y
0,
else.
Now, we express the tensor decomposition as
ˆ
A := ˆC ×u ˆU ×i ˆI ×t ˆT,
(7.3.4)
where ˆU, ˆI, and ˆT are low-rank feature matrices representing a mode (i. e., user, items,
and tags, respectively) in terms of its small number of latent dimensions kU, kI, kT, and
ˆC ∈RkU×kI×kT is the core tensor representing interactions between the latent factors. The
model parameters
are represented by the quadruple ˆθ := (ˆC, ˆU,ˆI, ˆT) (see Figure 7.3.3).
The basic idea of the HOSVD algorithm is to minimize an element-wise loss on the elements
of ˆ
A by optimizing the square loss, i. e.,
arg min
ˆθ
X
(u,i,t)∈Y
(au,i,t −ˆau,i,t)2.
After the parameters are optimized, predictions can be done as follows:
ˆa(u, i, t) :=
kU
X
˜u=1
kI
X
˜i=1
kT
X
˜t=1
ˆc˜u,˜i,˜t · ˆuu,˜u ·ˆii,˜i · ˆtt,˜t,
(7.3.5)
where ˆU = [ˆuu,˜u]u=1,...,U
˜u=1,...,kU,ˆI = [ˆii,˜i]i=1,...,I
˜i=1,...,kI, ˆT = [ˆtt,˜t]t=1,...,T
˜t=1,...,kT and indices over the feature
dimension of a feature matrix are marked with a tilde, and elements of a feature matrix are
marked with a hat (i. e., ˆtt,˜t).
I
T
T
kT
kT
kI
kU
A^
C^
T^
U^
^I
U
kU
U
I
kI
=
FIGURE 7.3.3: Tensor decomposition in STS. Figure adapted from [RMNST09].

198
Graph-Based Social Media Analysis
Example 7.3.2. The HOSVD algorithm takes A as input and outputs the recon-
structed tensor ˆ
A. ˆ
A measures the strength of associations between users, items, and
tags. Each element of ˆ
A can be represented by a quadruplet {u, i, t, p}, where p measures
the likeliness that user u will tag item i with tag t. Therefore, items can be recommended
to u according to their weights associated with the {u, t} pair.
In this subsection, in order to illustrate how HOSVD for item recommendation works,
we apply HOSVD to a toy example. As illustrated in Figure 7.3.4, three users tagged three
diﬀerent items (web links). In Figure 7.3.4, the part of an arrow line (sequence of arrows
with the same annotation) between a user and an item represents that the user tagged the
corresponding item, and the part between an item and a tag indicates that the user tagged
this item with the corresponding tag. Thus, the annotated numbers on the arrow lines gives
the correspondence among the three types of objects. For example, user u1 tagged item i1
with tag “BMW,” denoted as t1. The remaining tags are “Jaguar,” denoted as t2, “CAT,”
denoted as t3.
1
1, 2
2
3
3
4
4
u1
u2
u3
t1
t2
t3
i1
i2
i3
http://www.cars.com
http://www.automobiles.com
http://www.animals.com
BMW
CAT
JAGUAR
FIGURE 7.3.4: Usage data of the running example.
From Figure 7.3.4, we can see that users u1 and u2 have common interests in cars, while
user u3 is interested in cats. A 3-order tensor A ∈R3×3×3, can be constructed from the
usage data. We use the co-occurrence frequency (denoted as weights) of each triplet user,
item, and tag as the elements of tensor A, which are given in Table 7.3.1. Note that all
associated weights are initialized to 1. Figure 7.3.5 presents the tensor construction of our
running example.
After performing the tensor reduction analysis, we can get the reconstructed tensor
of ˆ
A, which is presented in Table 7.3.2, whereas Figure 7.3.6 depicts the contents of ˆ
A
TABLE 7.3.1: Associations of the running example.
Arrow Line
User
Item
Tag
Weight
1
u1
i1
t1
1
2
u2
i1
t1
1
3
u2
i2
t2
1
4
u3
i3
t3
1

Matrix and Tensor Factorization with Recommender System Applications
199
items
tags
users
1
0
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
1
FIGURE 7.3.5: The tensor construction of our running example.
TABLE 7.3.2: Associations derived on the running example.
Arrow Line
User
Item
Tag
Weight
1
u1
i1
t1
0.72
2
u2
i1
t1
1.17
3
u2
i2
t2
0.72
4
u3
i3
t3
1
5
u1
i2
t2
0.44
graphically (the weights are omitted). As shown in Table 7.3.2 and Figure 7.3.6, the output
of the tensor reduction algorithm for the running example is interesting, because a new
association among these objects is revealed. The new association is between u1, i2, and t2.
This association is represented with the last (bold faced) row in Table 7.3.2 and with the
dashed arrow line in Figure 7.3.6).
If we have to recommend to u1 an item for tag t2, then there is no direct indication
for this task in the original tensor A. However, we see that in Table 7.3.2 the element of
ˆ
A associated with (u1, i2, r2) is 0.44, whereas for u1 there is no other element associating
other tags with i2. Thus, we recommend item i2 to user u1, who used tag t2. For the current
example, the resulting ˆ
A tensor is presented in Figure 7.3.7.
1
1, 2
2
3
3
4
4
u1
u2
u3
t1
t2
t3
i1
i2
i3
http://www.cars.com
http://www.automobiles.com
http://www.animals.com
BMW
CAT
JAGUAR
5
5
FIGURE 7.3.6: Illustration of the tensor reduction algorithm output for the running exam-
ple.

200
Graph-Based Social Media Analysis
items
tags
users
0.72
0
1.14
0
0
0
0
0
0
0
0
0
0
0
0
0
0.44
0.72
0
0
0
0
0
0
0
1
0
FIGURE 7.3.7: The resulting ˆ
A tensor for the running example.
The resulting recommendation is reasonable, because u1 is interested in cars rather than
cats. That is, the tensor reduction approach is able to capture the latent associations among
the multi-type data objects: user, items, and tags. The associations can then be used to
improve the item recommendation procedure.
7.3.3
Handling the Sparsity Problem
Sparsity is a severe problem in 3-dimensional data, and it can aﬀect the outcome of
SVD. To address this problem, instead of SVD we can apply kernel-SVD [CST04, CSS06]
in the three unfolded matrices. Kernel-SVD is the application of SVD in the Kernel-deﬁned
feature space. Smoothing with kernel SVD is also applied by Symeonidis et al. in [SNM10].
For each unfolding Ai (1 ≤i ≤3) we have to non-linearly map its contents to a higher
dimensional space using a mapping function φ. Therefore, from each Ai matrix we can
derive an Fi matrix, where each element axy of Ai is mapped to the corresponding element
fxy of Fi, i. e., fxy = φ(axy). Next, we can apply SVD and decompose each Fi as follows:
Fi = U(i)S(i)(V(i))
T .
(7.3.6)
The resulting U(i) matrices are then used to construct the core tensor.
Nevertheless, to avoid the explicit computation of Fi, all computations must be done in
the form of inner products. In particular, as we are interested to compute only the matrices
with the left-singular vectors, for each mode i we can deﬁne a matrix Bi as follows:
Bi = FiFT
i .
(7.3.7)
As Bi is computed using inner products from Fi, we can substitute the computation of
inner products with the results of a kernel function. This technique is called the “kernel
trick” [CST04] and avoids the explicit (and expensive) computation of Fi. As each U(i)
and V(i) are orthogonal and each S(i) is diagonal, it easily follows from equations (7.3.6)
and (7.3.7) that:
Bi = (U(i)S(i)(V(i))T )(U(i)S(i)(V(i))T )T = U(i)(S(i))2(V(i))T .
(7.3.8)
Therefore, each required U(i) matrix can be computed by diagonalizing each Bi matrix
(which is square) and taking its eigen-vectors.
Regarding the kernel function, in our experiments we use the Gaussian kernel K(x, y) =
e−||x−y||2
c
, which is commonly used in many applications of kernel SVD. As Gaussian Kernel
parameter c, we use the estimate for standard deviation in each matrix unfolding.

Matrix and Tensor Factorization with Recommender System Applications
201
7.3.4
Inserting New Users, Tags, or Items
As new users, tags, or items are being introduced to the system, the tensor ˆ
A, which
provides the recommendations, has to be updated. The most demanding operation is the
updating of the SVD of the corresponding mode in equations (7.3.6) and (7.3.8). As we would
like to avoid the costly batch re-computation of the corresponding SVD, we can consider
incremental solutions [Bur02, SKR02]. Depending on the size of the update (i. e., number of
new users, tags, or items), diﬀerent techniques have been followed in related research. For
small update sizes we can consider the folding-in technique [FDD+88, SKR02], whereas for
larger update sizes we can consider Incremental SVD techniques [Bur02]. Both techniques
are described next [SNM10].
Update by folding-in. Given a new user, we ﬁrst compute the new 1-mode matrix
unfolding A1. It is easy to see that the entries of the new user result to the appending of
new row in A1. This is exempliﬁed in Figure 7.3.8. Figure 7.3.8a shows the insertion of a
new user in the tensor of the current example. The new values are presented in the last
(fourth) row of each frontal slice of the tensor. Notice that to ease the presentation, the new
user’s tags and items are identical with those of user U2.
0
0
0
0
0
0
0
0
1
1
0
1
0
0
0
0
0
0
0
0
1
0
0
1
0
0
0
0
0
0
0
1
0
0
0
0
(a)
A1 =


1
0
0
0
0
0
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
0
0
0
0
1
1
0
0
0
1
0
0
0
0


(b)
FIGURE 7.3.8: Example of folding in a new user: a) the insertion of a new user into the
tensor, b) the new 1-mode unfolded matrix A1.
Let u denote the new row that is appended to A1. Figure 7.3.8b presents the new A1,
i. e., the 1-mode unfolded matrix, where it is shown that the contents of u (highlighted in
bold) have been appended as a new row on the end of A1.
Since A1 changed, we have to compute its SVD. To avoid batch SVD recomputation,
we can use the existing basis U(1)
c1 of left singular vectors, to project the row u onto the the

202
Graph-Based Social Media Analysis
reduced c1-dimensional space of users in the A1 matrix. This projection is called folding-in
and is computed by using the following equation (7.3.9) [FDD+88]:
unew = u · V(1)
c1 · (S(1)
c1 )−1.
(7.3.9)
In equation (7.3.9), unew denotes the mapped row, which will be appended to U(1)
c1 ,
whereas V(1)
c1 and (S(1)
c1 )−1 are the dimensionally reduced matrices derived when SVD was
originally applied to A1, i. e., before the insertion of the new user. In the current example,
the computation of unew is described in Figure 7.3.9.
−0.85
0
=
1
0
0
0
1
0
0
0
0
×
unew
u
×
−0.85
0
0
0
0
0
0
0
−0.53
0
0
0
0
0
0
0
0
1
×
0.62
0
0
1
V(1)
c1
(S(1)
c1 )−1
FIGURE 7.3.9: The result of folding-in for the current example.
The unew vector should be appended in the end of the U(1)
c1 matrix. For the current ex-
ample, appending should be done to the previously U(1)
c1 matrix. Notice that in the example,
unew is identical to the second column of the transpose of U(1)
c1 , as shown in Figure 7.3.8a.
The reason is that the new user has identical tags and items with user U2 and we mapped
them on the same space (recall that the folding-in technique maintains the same space
computed originally by SVD).
Finally, to update tensor ˆ
A, we have to perform the products given in equation (7.3.6).
Notice that only U(1)c1 has been modiﬁed in this equation. Thus, to optimize the inser-
tion of new users, as mode products are interchangeable, we can perform this product as
h
S ×2 U(2)
c2 ×3 U(3)
c3
i
×1U(1)
c1 , where the left factor (inside the brackets), which is unchanged,
can be pre-stored so as to avoid its re-computation. For the current example, the resulting
ˆ
A tensor is shown in Figure 7.3.10.
An analogous insertion procedure can be followed for the insertion of a new item or tag.
For a new item insertion, we have to apply equation (7.3.9) on the 2-mode matrix unfolding
of tensor A, while for a new tag we apply equation (7.3.9) on the 3-mode matrix unfolding
of tensor A.
Update by Incremental SVD. Folding-in incrementally updates SVD but the result-
ing model is not a perfect SVD model, because the space is not orthogonal [SKR02]. When
the update size is not big, loss of orthogonality may not be a severe problem in practice.
Nevertheless, for larger update sizes the loss of orthogonality may result in an inaccurate
SVD model. In this case, we need to incrementally update SVD so as to ensure orthogo-
nality. This can be attained in several ways. Next we describe the approach proposed by
Brand [Bur02].

Matrix and Tensor Factorization with Recommender System Applications
203
0
0
0
0
0
0
0
0.44
0.72
0.72
0
1.17
0
0
0
0
0
0
0
0
1.17
0
0
0.72
0
0
0
0
0
0
0
1
0
0
0
0
FIGURE 7.3.10: The resulting ˆ
A tensor of the running example after the insertion of the
new user.
Let Mp×q be a matrix, upon we which apply SVD and maintain the ﬁrst r singular
values, i.e.:
Mp×q = Up×rSr×rVT
r×q.
(7.3.10)
Assume that each column of matrix Cp×c contains the additional elements. Let L
△= U\C =
UT C be the projection of C onto the orthogonal basis of U. Let also H = (I −UUT )C =
C−UL be the component of C orthogonal to the subspace spanned by U (I is the identity
matrix). Finally, let J be an orthogonal basis of H and let K = J\H = JT H be the
projection of C onto the subspace orthogonal to U. Consider the following identity:
[U J]
"S
L
0
K
# "V
0
0
I
#T
=

U(I −UUT )C/K

"
S
UT C
0
K
# "V
0
0
I
#T
=

USVT C

= [M C] .
(7.3.11)
Like an SVD, the left and right matrixes in the product are unitary and orthogonal. The
middle matrix, denoted as Q, should be diagonal. To incrementally update the SVD, Q
must be diagonalized. If we apply SVD on Q we get:
Q = U′S′(V′)T .
(7.3.12)
Additionally, deﬁne U′′, S′′, V′′ as follows:
U′′ = [U J]U′, S′′ = S′, V′′ =
"V
0
0
I
#
V′.
(7.3.13)
Then, the updated SVD of matrix [M C] is:
[M C] = [USVT C] = U′′S′′(V′′)T .
(7.3.14)
This incremental update procedure takes O((p + q)r2 + pc2) time.
Returning to the application of incremental update for new users, items, or tags, as
described in the subsection on updating by folding in in the current section, in each case

204
Graph-Based Social Media Analysis
we obtain a result with a number of new rows that are appended at the end of the unfolded
matrix of the corresponding mode. Therefore, we need an incremental SVD procedure in
the case where we add new rows, whereas the aforementioned method works in the case
where we add new columns. In this case, we simply swap U for V and U′′ for V′′.
7.3.5
Other Scalable Factorization Models
The HOSVD approach has an important drawback. That is, its runtime complexity is
cubic in the size of the latent dimensions. This can be seen in equation (7.3.5), where three
nested sums have to be calculated just for predicting a single (user, item, tag)-triple. There
are several approaches to improve the eﬃciency of HOSVD [DM07, Tur07, KS08].
The limitation in runtime of HOSVD stems from its model which is the Tucker Decom-
position. In the following, we will discuss a second factorization model (i. e., PARAFAC)
that has been proposed for tag recommendation. We investigate its model assumptions,
complexity and its relation with HOSVD.
The underlying tensor factorization model of HOSVD is the Tucker Decomposition
(TD) [Tuc66]. As noted before, for tag recommendation, the model reads:
ˆ
A := ˆC ×u ˆU ×i ˆI ×t ˆT,
(7.3.15)
or equivalently
ˆau,i,t =
kU
X
˜u=1
kI
X
˜i=1
kT
X
˜t=1
ˆc˜u,˜i,˜t · ˆuu,˜u ·ˆii,˜i · ˆtt,˜t
(7.3.16)
The reason for the cubic complexity (i. e., O(k3) with k := min(kU, kI, kT)) of TD is the
core tensor.
The Parallel Factor Analysis (PARAFAC) [Har70] model or canonical decomposi-
tion [CC70] reduces the complexity of the TD model by assuming a diagonal core tensor.
c˜u,˜i,˜t
!=
(
1,
if ˜u = ˜i = ˜t
0,
else,
(7.3.17)
which allows us to rewrite the model equation:
ˆau,i,t =
k
X
f=1
ˆuu,f ·ˆii,f · ˆtt,f.
(7.3.18)
In contrast to TD, the model equation of PARAFAC can be computed in O(k). In total,
the model parameters ˆθ of the PARAFAC model are ˆU ∈R|U|×k,ˆI ∈R|I|×k, ˆT ∈R|T|×k.
The assumption of a diagonal core tensor is a restriction of the TD model.
A graphical representation of Tucker Decomposition (TD) and Parallel Factor Analysis
(PARAFAC) shown in Figure 7.3.11. It can be seen that any PARAFAC model can be
expressed by a TD model (with a diagonal core tensor).
Let M be the set of models that can be represented by a model class. In [Ren10] it is
shown that for the tag recommendation:
MPARAFAC ⊂MTD.
(7.3.19)
This means that any PARAFAC model can be expressed with a TD model, but there are

Matrix and Tensor Factorization with Recommender System Applications
205
T^
U^
I^
C^
U
I
T
(a) TD
T^
U^
I^
0
0
0
0
0
1
1
I
(b) PARAFAC
FIGURE 7.3.11: Relationship between Tucker Decomposition and Parallel Factor Analysis
(PARAFAC).
TD models that cannot be represented by a PARAFAC model. In [RST10, Ren10] it was
pointed out that this does not mean that TD is guaranteed to have a higher prediction
quality than PARAFAC. On the contrary, because all the model parameters are estimated
from limited data, restricting the expressiveness of a model can lead to higher prediction
quality if the restriction is in line with the true parameters.
7.4
A Real Geo-Social System-Based on HOSVD
This section presents a real-world recommender system for Location-Based Social Net-
works (LBSNs). Our GeoSocialRec website allows us to test, evaluate and compare diﬀerent
recommendation styles in an online setting, where the users of GeoSocialRec actually receive
recommendations during their check-in process.
The GeoSocialRec recommender system consists of several components. The system’s
architecture is illustrated in Figure 7.4.1, where three main sub-systems are described: (i)
the website, (ii) the Database Proﬁles and (iii) the Recommendation Engine. In the following
sections, we describe each subsystem of GeoSocialRec in detail.
7.4.1
GeoSocialRec Website
The GeoSocialRec system uses a website 2 to interact with the users. The website consists
of four subsystems: (i) the friend recommendation, (ii) the location recommendation, (iii)
the activity recommendation, and (iv) the check-in subsystem. The friend recommendation
subsystem is responsible for evaluating incoming data from the Recommendation Engine of
GeoSocialRec and providing updated friend recommendations. We provide friend, location,
and activity recommendations, where new and updated location and activity recommenda-
tions presented to the user as new check-ins are stored in the Database proﬁles. Finally, the
check-in subsystem is responsible for passing the data inserted by the users to the respective
Database proﬁles.
Figure 7.4.2 presents a scenario where the GeoSocialRec system recommends four pos-
sible friends to the target user. As shown, the ﬁrst table recommends 2 possible friends,
who are connected to him with 2-hop paths. The results are ordered based on the second
2http://delab.csd.auth.gr/geosocialrec

206
Graph-Based Social Media Analysis
FIGURE 7.4.1: Components of the Geo-social recommender system.
to last column of the table, which indicates the number of common friends that the target
user shares with each possible friend.
As also shown in Figure 7.4.2, Anastasia is the top recommendation because she shares
3 common friends with the target user. The common friends are then presented in the last
column of the table. The second table contains two users, who are connected to the target
user via 3-hop paths. The last column of the second table indicates the number of found
paths that connect the target user with the recommended friends. Manolis is now the top
recommendation, because he is connected to the target user via three 3-hop paths. It is
obvious that the second explanation style is more analytical and detailed, since users can
see, in a transparent way, the paths that connect them with the recommended friends.
Figure 7.4.3a shows a location recommendation, while Figure 7.4.3b depicts an activity
recommendation. As shown in Figure 7.4.3a, the target user can provide to the system
the activity she wants to do and the place she is (i.e., Bar in Athens). Then, the system
provides a map with bar places (i.e., place A, place B, place C, etc.) along with a table,
where these places are ranked based on the number of users’ check-ins and their average
rating. As shown in Figure 7.4.3a, the top recommended Bar is Mojo (i.e., place A), which
is visited 3 times (by the target user’s friends) and is rated highly (i.e., 5 stars). Regarding
the activity recommendation, as shown in Figure 7.4.3b, the user selects a nearby city (i.e.,
Thessaloniki) and the system provides activities that she could perform. In this case, the
top recommended activity is sightseeing at the White Tower of Thessaloniki, because it is
visited 14 times and has an average rating of 4.36.

Matrix and Tensor Factorization with Recommender System Applications
207
FIGURE 7.4.2: Friend recommendations provided by the GeoSocialRec system.
7.4.2
GeoSocialRec Database and Recommendation Engine
The database that supports the GeoSocialRec system is a MySQL(v.5.5.8) 3 database.
MySQL is an established Database Management System (DBMS), which is widely used in
on-line, dynamic, database driven websites.
The database proﬁle sub-system contains ﬁve proﬁles where data about the users, loca-
tions, activities and their corresponding ratings are stored. As shown in Figure 7.4.1, these
data are received by the Check-In proﬁle and along with the Friendship proﬁle, they provide
the input for the Recommendation Engine sub-system. Each table ﬁeld represents the re-
spective data that is collected by the Check-In proﬁle. User-id, Location-id and Activity-id
refer to speciﬁc ids given to users, locations, and activities respectively.
The recommendation engine is responsible for collecting the data from the database and
producing the recommendations, which will then be displayed on the website. As shown
in Figure 7.4.1, the recommendation engine constructs a friends-similarity matrix based on
FriendLink [PSM11] algorithm. The average geographical distances (in kilometers) between
users’ check-ins are used as link weights. To obtain the weights, we calculate the average
distance between all pairs of Points Of Interests (POIs) that two users have checked in.
The recommendation engine also produces a dynamically analyzed 3-order tensor, which is
ﬁrst constructed by the HOSVD algorithm and is then updated using incremental meth-
ods [Bra02, SKKR02], both of which are explained previously in Sections 7.2.4 and 7.3.4,
respectively.
3http://www.mysql.com

208
Graph-Based Social Media Analysis
(a)
(b)
FIGURE 7.4.3: Location and activity recommendations made by the Geo-Social recom-
mender system.
7.4.3
Experiments
In this section, we study the performance of the FriendLink [PSM11] algorithm and the
HOSVD method in terms of friend, location, and activity recommendations. To evaluate the
aforementioned recommendations we have chosen two real data sets. The ﬁrst one, denoted
as the GeoSocialRec data set, is extracted from the GeoSocialRec site 4. It consists of 102
users, 46 locations and 18 activities. The second data set, denoted as UCLAF [ZCZ+10],
consists of 164 users, 168 locations and 5 diﬀerent types of activities, including “Food
and Drink,” “Shopping,” “Movies and Shows,” “Sports and Exercise,” and “Tourism and
Amusement.”
4http://delab.csd.auth.gr/∼symeon

Matrix and Tensor Factorization with Recommender System Applications
209
FIGURE 7.4.4: Precision-recall diagram of HOSVD and FriendLink for activity, location
and friend recommendations on the GeoSocialRec data set.
The numbers c1, c2, and c3 of left singular vectors of matrices U (1), U (2), U (3) for
HOSVD, after appropriate tuning, are set to 25, 12, and 8 for the GeoSocialRec dataset,
and to 40, 35, and 5 for the UCLAF data set. Due to lack of space we do not present
experiments for the tuning of c1, c2, and c3 parameters. The core tensor dimensions are
ﬁxed, based on the aforementioned c1, c2, and c3 values.
We perform 4-fold cross validation and the default size of the training set is 75% (we
pick, for each user, 75% of his check-ins and friends randomly). The task of all three recom-
mendation types (i.e., friend, location, activity) is to predict the friends/locations/activities
of the 25% remaining user check-ins and friends, respectively. As performance measures we
use precision and recall, which are standard in such scenarios.
Next, we study the accuracy performance of HOSVD in terms of precision and recall.
This reveals the robustness of HOSVD in attaining high recall with minimal losses in terms
of precision. We examine the top-N ranked list, which is recommended to a test user, starting
from the top friend/location/activity. In this situation, the recall and precision vary as we
proceed with the examination of the top-N list. In Figure 7.4.4, we plot a precision versus
recall curve.
As it can be seen, the HOSVD approach presents high accuracy. The reason is that we
exploit altogether the information that concerns the three entities (friends, locations, and
activities) and thus, we are able to provide accurate location/activity recommendations.
Notice that activity recommendations are more accurate than location recommendations.
A possible explanation could be the fact that the number of locations is bigger than the
number of activities. That is, it is easier to accurately predict an activity than a location.
Notice that for the task of friend recommendation, the performance of Friendlink is not so
high. The main reason is data sparsity. In particular, the friendship network has an average
nodes’ degree equal to 2.7 and an average shortest distance between nodes of 4.7, which
means that the friendship network can not be considered a “small world” network and friend
recommendations can not be so accurate.
For the UCLAF data set, as shown in Figure 7.4.5, the HOSVD algorithm attains
analogous results. Notice that the recall for the activity recommendations, reaches 100%
because the total number of activities is 5. Moreover, notice that in this diagram, we do not
present results for the friend recommendation task, since there is no friendship network in
the corresponding UCLAF data set.

210
Graph-Based Social Media Analysis
FIGURE 7.4.5: Precision-recall diagram of HOSVD for activity and location recommenda-
tions on the UCLAF data set.
7.5
Conclusion
In this chapter, we described matrix and tensor factorization techniques in recommender
systems and social tagging systems, respectively. In addition, we presented a real-world rec-
ommender system for location-based social networks, which employs tensor decomposition
techniques. As shown, matrix and tensor decompositions are suitable for scenarios in which
the data is extremely large, very sparse, and too noisy, since the reduced representation of
the data can be interpreted as a de-noisiﬁed approximation of the “true” data.
Bibliography
[BBL+06]
M. W. Berry, M. Browne, A. N. Langville, V. P. Pauca, and R. J. Plemmons.
Algorithms and applications for approximate nonnegative matrix factorization.
In Computational Statistics and Data Analysis, pages 155–173, 2006.
[Bra02]
M. Brand. Incremental singular value decomposition of uncertain data with
missing values. In Proc. of the 7th European Conference on Computer Vision,
pages 707–720, Copenhagen, Denmark, 2002.
[Bur02]
R. Burke. Hybrid recommender systems: Survey and experiments. User Mod-
eling and User-Adapted Interaction, 12(4):331–370, 2002.
[CC70]
J. Carroll and J. Chang.
Analysis of individual diﬀerences in multidimen-
sional scaling via an n-way generalization of “Eckart-Young decomposition.”
Psychometrika, 35:283–319, 1970.
[CSS06]
T. Chin, K. Schindler, and D. Suter. Incremental kernel SVD for face recogni-
tion with image sets. In Proc. of the 7th International Conference on Automatic
Face and Gesture Recognition, pages 461–466, 2006.

Matrix and Tensor Factorization with Recommender System Applications
211
[CST04]
N. Cristianini and J. Shawe-Taylor. Kernel methods for pattern analysis. Cam-
bridge University Press, 2004.
[CWZ07]
S. Chen, F. Wang, and C. Zhang. Simultaneous heterogeneous data clustering
based on higher order relationships. In Proc. of the 7th IEEE International
Conference on Data Mining Workshops, pages 387–392, 2007.
[DDF+90]
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman.
Indexing by latent semantic analysis.
Journal of the American Society for
Information Science, 41(6):391–407, 1990.
[DM07]
P. Drineas and M. W. Mahoney. A randomized algorithm for a tensor-based
generalization of the SVD. Linear Algebra and Its Applications, 420(2–3):553–
571, 2007.
[DS05]
I. S. Dhillon and S. Sra. Generalized nonnegative matrix approximations with
Bregman divergences. In Proc. Advances in Neural Information Processing
Systems, pages 283–290, 2005.
[FDD+88]
G. W. Furnas, S. Deerwester, S. T. Dumais, T. K. Landauer, R. A. Harshman,
L. A. Streeter, and K. E. Lochbaum. Information retrieval using a singular
value decomposition model of latent semantic structure. In Proc. of the 11th
Annual International ACM SIGIR Conference on Research and Development
in Information Retrieval, pages 465–480, 1988.
[Har70]
R. A. Harshman. Foundations of the PARAFAC procedure: models and condi-
tions for an “explanatory” multimodal factor analysis. UCLA Working Papers
in Phonetics, 16:1–84, 1970.
[Kor08]
Y. Koren. Factorization meets the neighborhood: a multifaceted collaborative
ﬁltering model. In Proc. of the 14th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pages 426–434, 2008.
[Kor09]
Y. Koren. Collaborative ﬁltering with temporal dynamics. In Proc. of the 15th
ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, pages 447–456, 2009.
[KS08]
T. G. Kolda and J. Sun. Scalable tensor decompositions for multi-aspect data
mining. In Proc. of the 8th IEEE International Conference on Data Mining,
pages 363–372, 2008.
[LMV00]
L. D. Lathauwer, B. D. Moor, and J. Vandewalle.
A multilinear singular
value decomposition.
SIAM Journal on Matrix Analysis and Applications,
21(4):1253–1278, 2000.
[MD09]
M. W. Mahoney and P. Drineas. CUR matrix decompositions for improved
data analysis. Proceedings of the National Academy of Sciences, 106(3):697–
702, 2009.
[PSM11]
A. Papadimitriou, P. Symeonidis, and Y. Manolopoulos. Friendlink: Link pre-
diction in social networks via bounded local path traversal. In Proc. of the 3rd
Conference on Computational Aspects of Social Networks, 2011.
[Ren10]
S. Rendle. Context-Aware Ranking with Factorization Models. Springer, 1st
edition, November 2010.

212
Graph-Based Social Media Analysis
[RMNST09] S. Rendle, L. B. Marinho, A. Nanopoulos, and L. Schimdt-Thieme. Learning
optimal ranking with tensor factorization for tag recommendation. In Proc.
of the 15th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, pages 727–736, 2009.
[RST10]
S. Rendle and L. Schmidt-Thieme. Pairwise interaction tensor factorization
for personalized tag recommendation. In Proc. of the 3rd ACM International
Conference on Web Search and Data Mining, 2010.
[SH05]
A. Shashua and T. Hazan. Non-negative tensor factorization with applica-
tions to statistics and computer vision. In Proc. of the 22nd International
Conference on Machine Learning, pages 792–799, 2005.
[SKKR00]
B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Application of dimensionality
reduction in recommender system - a case study. In Proc. of the ACM SIGKDD
Workshop on Web Mining for E-Commerce - Challenges and Opportunities,
Boston, MA, 2000.
[SKKR02]
B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Incremental singular value
decomposition algorithms for highly scalable recommender systems. In Proc.
of the 5th International Conference on Computer and Information Technology,
pages 27–28, Dhaka, Bangladesh, 2002.
[SKR02]
B. Sarwar, J. Konstan, and J. Riedl. Incremental singular value decomposition
algorithms for highly scalable recommender systems. In Proc. International
Conference on Computer and Information Science, 2002.
[SM08]
R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization
using Markov chain Monte Carlo. In Proc. of the 25th International Conference
on Machine Learning, pages 880–887, 2008.
[SNM10]
P. Symeonidis, A. Nanopoulos, and Y. Manolopoulos. A uniﬁed framework
for providing recommendations in social tagging systems based on ternary
semantic analysis. IEEE Transactions on Knowledge and Data Engineering,
22(2), 2010.
[SNPM06]
P. Symeonidis, A. Nanopoulos, A. Papadopoulos, and Y. Manolopoulos. Col-
laborative ﬁltering based on users trends. In Proc. of the 30th Conference of
the German Classiﬁcation Society, 2006.
[Str06]
G. Strang. Linear Algebra and Its Applications. Brooks Cole, 2006.
[Sym07]
P. Symeonidis. Content-based dimensionality reduction for recommender sys-
tems. In Proc. of the 31st Conference of the German Classiﬁcation Society,
Freiburg, 2007.
[SZL+05]
J.-T. Sun, H.-J. Zeng, H. Liu, Y. Lu, and Z. Chen. CubeSVD: a novel approach
to personalized web search. In Proc. of the 14th International Conference on
World Wide Web, pages 382–390, 2005.
[Tik63]
A. Tikhonov. Solution of incorrectly formulated problems and the regulariza-
tion method. In Soviet Math. Doklady, volume 4, pages 1035–1038, 1963.
[Tuc66]
L. Tucker. Some mathematical notes on three-mode factor analysis. Psychome-
trika, pages 279–311, 1966.

Matrix and Tensor Factorization with Recommender System Applications
213
[Tur07]
P. Turney. Empirical evaluation of four tensor decomposition algorithms. Tech-
nical Report (NRC/ERB-1152), 2007.
[WA08]
H. Wang and N. Ahuja. A tensor approximation approach to dimensionality
reduction. International Journal of Computer Vision, 76:217–229, 2008.
[ZCZ+10]
V. Zheng, B. Cao, Y. Zheng, X. Xie, and Q. Yang. Collaborative ﬁltering
meets mobile recommendation: A user-centered approach. In Proc. of the 24th
AAAI Conference on Artiﬁcial Intelligence, pages 236–241, Atlanta, GA, 2010.


Chapter 8
Multimedia Social Search Based on
Hypergraph Learning
Constantine Kotropoulos
Aristotle University of Thessaloniki, Greece
8.1
Introduction ....................................................................
215
8.2
Hypergraphs ....................................................................
218
8.2.1
Uniform hypergraphs ..................................................
220
8.3
Game-Theoretic approaches to uniform hypergraph clustering ...............
223
8.4
Spectral clustering for arbitrary hypergraphs .................................
229
8.5
Ranking on hypergraphs .......................................................
238
8.5.1
Enforcing structural constraints .......................................
239
8.5.2
Learning hyperedge weights ...........................................
241
8.6
Applications ....................................................................
243
8.6.1
High-order web link analysis ..........................................
243
8.6.2
Hypergraph matching for object recognition ..........................
247
8.6.3
Music recommendation and personalized music tagging ..............
249
8.6.4
Simultaneous image tagging and geo-location prediction .............
251
8.6.5
Social image search exploiting joint visual-textual information .......
254
8.6.6
Annotation, classiﬁcation, and tourism recommendation driven by
probabilistic latent semantic analysis .................................
256
8.7
Big data: Randomized methods for matrix/hypermatrix decompositions .....
261
8.8
Conclusions .....................................................................
265
8.9
Acknowledgments ..............................................................
267
Bibliography ....................................................................
267
8.1
Introduction
Multimedia social search refers to a class of problems, such as tagging, retrieval, and
recommendation, where both multimedia content and social context (i.e., information dis-
tilled from the web that is correlated to the content) are exploited. Ranking is the heart
of multimedia social search. The motivation behind multimedia social search is that re-
trieval or recommendation based only on content yields frequently unsatisfactory results
due to the well known semantic gap. Thanks to online social sharing sites (e.g., Flickr
[http://www.ﬂickr.com], Lastfm [http://www.last.fm], etc.) multimedia come with addi-
tional metadata, including, ownership, tags, geo-location, etc. Such metadata oﬀer rich
complementary information worth exploiting. A new transductive learning framework has
recently emerged that addresses the aforementioned problems as ranking on hypergraphs by
215

216
Graph-Based Social Media Analysis
jointly analyzing the content and its associated context deﬁned by the metadata in a uniﬁed
manner.
Hypergraphs generalize the concept of graphs by allowing their edges, called hyperedges
hereafter, to connect more than two vertices. This way, the hypergraphs can capture more
complex relations, i.e., three-way or higher-order ones. Hypergraphs have been used in
various domains, such as, databases [Fag83], data mining [HKKM98], biology [KHT06],
or to model complex networks [ERV06], to mention a few, well in advance of their use
in multimedia social search. The roots of hypergraphs are traced back in mathematics
[BM73, Ber89]. The goal of this chapter is two-fold: (1) To provide a comprehensive, but
not superﬁcial, self-contained survey of the theory related to hypergraphs, promoting the
coherence of knowledge; (2) To describe sample applications in high-order web link analysis,
object recognition, music/image recommendation and tagging, image search, and tourism
recommendation. This way the chapter might be useful to senior undergraduate students,
graduate students, and Ph.D. candidates as well as scientists, engineers, and practitioners.
Starting with the basic deﬁnitions related to hypergraphs, given in Section 8.2, the
fundamentals of the most studied member of the family of hypergraphs, the so-called κ-
uniform hypergraph, are summarized in Section 8.2.1. Emphasis is given to the spectrum
of the κ-uniform hypergraphs. κ-uniform hypergraphs are important for two reasons. First,
they are described in terms of an adjacency tensor, a degree tensor, and a Laplacian ten-
sor. Accordingly, tensor decompositions can be used for detecting communities in the web,
as is demonstrated in Section 8.6.1. Hereafter, we shall refrain from using the term ten-
sor and shall prefer the more precise term hypermatrix. Moreover, feature matching, which
is strongly related to content-based image retrieval, can be cast as a uniform hypergraph
matching problem, as is demonstrated in Section 8.6.2. Second, uniform hypergraph clus-
tering can be cast into a non-cooperative multi-player game, where the notion of a cluster is
equivalent to a classical game-theoretic equilibrium concept, as is detailed in Section 8.3. The
Nash equilibria of the clustering game correspond to strict local maxima of a homogeneous
polynomial, a subject that goes back to the seminal Baum-Eagon theorem.
Section 8.4 elaborates spectral clustering for arbitrary hypergraphs. Spectral hypergraph
clustering extends the spectral clustering originally applied to graphs. In particular, a hy-
pergraph is transformed into a graph, whose edge weights are mapped from the weights of
the original hypergraph. Two transformations are studied, namely the click expansion and
the star expansion. The latter is shown to be closely related to the hypergraph normalized
cut criterion that generalizes the normalized cut method applied to graphs. This is tanta-
mount to the eigen-decomposition of a positive semideﬁnite matrix, the so-called hypergraph
Laplacian.
Hypergraphs were applied to exploit visual-duplicates for video re-ranking using a ran-
dom walk algorithm [TNW08]. A probabilistic hypergraph was proposed to describe both
the higher order grouping information and the aﬃnity relation among the vertices within
hyperedges for image retrieval [HLZM10]. Recently, Kapoor et al. [KSS13] proposed node
degree centrality metrics for weighted hypergraphs. Tramasco et al. evaluated various se-
mantic and structural hypotheses for academic team formation [TCR10]. Going beyond
collaborative ﬁltering that is heavily based on user ratings, hypergraphs were employed for
music recommendation [BTC+10]. Indeed, a hypergraph models the various objects (i.e.,
users, user groups, tags, tracks, albums, or artists) and the relations among them, such as
friendship relations, membership relations, listening relations, tagging relations on tracks,
tagging relations on albums, tagging relations on artists, track-album inclusion relations,
album-artist inclusion relations, and similarities between the tracks. Figure 8.1.1 shows var-
ious hyperedges, capturing information related to music recommendation. For example, e1
models a listening relation, which employs a user and a music recording; e2 and e3 capture
tagging relations, which embrace a user, a music recording, and a tag; e4 models content

Multimedia Social Search Based on Hypergraph Learning
217
FIGURE 8.1.1: Various hyperedges that model information useful for music recommenda-
tion.
similarity; and e5 captures the friends of a particular user. Section 8.5 elaborates hypergraph
ranking. In Section 8.5.1, it is shown that recommendation can be cast as an optimization
of an objective function made up by a smoothness constraint, which guarantees that ver-
tices with the same value in the ranking vector are strongly connected plus an ℓ2 norm
measuring the diﬀerence between the obtained ranking scores and the pre-given query vec-
tor. The former constraint is related to the spectral clustering of an arbitrary hypergraph,
while the latter one guarantees that the hypergraph vertices are ranked based on their
relevance to the given query. By properly structuring the query vector, recommendation,
tagging, or retrieval could also be addressed. Besides the construction of the query vector,
open questions in ranking on hypergraphs are how hyperedges are generated, how group
sparsity or other structural group penalties can be enforced, how the hyperedge weights
can be learnt. Hyperedge weight learning helps us to assess how the various subsets of ver-
tices aﬀect the recommendation or tagging quality for music recordings and images. The
associated constrained optimization problems are thoroughly studied in Sections 8.5.1 and
8.5.2 as well as in the applications to music recommendation and tagging (cf. Section 8.6.3),
image recommendation, retrieval, and tagging demonstrated in Sections 8.6.4-8.6.6.
Section 8.7 deals with the so-called Big Data case. Randomized matrix/hypermatrix
factorizations are surveyed, because they can cope eﬃciently with big data. Such techniques
complement the discussions on adaptive latent models, incremental spectral clustering, and
incremental tensor adaptation as well as their parallel and distributed implementations in
Chapter 11. Figure 8.1.2 depicts the dependencies among the various sections of the chapter.
Throughout the chapter, calligraphic uppercase letters are reserved for graphs, and hy-
pergraphs (e.g., G, H). Sets appear as blackboard bold uppercase letters (e.g., V), hyper-
matrices are denoted by boldface Euler script calligraphic letters (e.g., A), matrices are
indicated by uppercase boldface letters (e.g., A), and vectors are denoted by lowercase
boldface letters (e.g., a). The elements of all the aforementioned mathematical structures
are denoted by lowercase letters indexed by one or more indices. For example, the elements
of matrix A are denoted as ai1i2. Occasionally, the elements of A are indicated as A(i1, i2)
or [A]i1i2. In addition, ai is a shorthand notation for the i-th column of A, while a(j) refers
to the j-th row of A. R, Z, C denote the ﬁelds of real, integer, and complex numbers,
respectively, unless it is deﬁned otherwise.

218
Graph-Based Social Media Analysis
FIGURE 8.1.2: Roadmap of this chapter.
8.2
Hypergraphs
Let V = {v1, v2, . . . , vN} be a ﬁnite set of cardinality |V| = N. The elements vi, i =
1, 2, . . . , N are called vertices. A hypergraph on V, H(V, E), is formally deﬁned as a family
of nonempty subsets of V, called hyperedges, whose union yields V [Ber89]. To simplify
notation, let us refer to this collection by E = (e1, e2, . . . , eM). Let P(V) be the power set
of V, i.e., the set composed of all subsets of V. Its cardinality is 2N. It is evident that
E ⊆P(V). Moreover, the number of hyperedges could be gargantuan even for small N.
The aforementioned deﬁnition of hypergraph does not allow for repeated vertices within
a hyperedge (often called hyperloops). To include hyperloops, the deﬁnition has to be ex-
panded by employing the notion of multiset. A multiset is a generalization of a set in which
the members are allowed to appear more than once. Accordingly, a multi-hypergraph H is
a pair (V, E), where E is a set of multisets of V [PZ13].
Given two hypergraphs H = (V, E) and H′ = (V′, E′), if V′ ⊆V and E′ ⊆E, then
H′ is said to be a subgraph of H. A set of vertices S ⊆V is said to induce the subgraph
H[S] = (S, E ∩P(S)). Subgraphs formally describe communities in a hypergraph.
A vertex v ∈V and a hyperedge e ∈E are called incident, if v ∈e. Two vertices vi
and vk are connected by a hyperpath, if there is an alternating sequence of distinct vertices
and hyperedges v1, e1, v2, e2, . . . , ek−1, vk, such that {vi, vi+1} ⊆ei for 1 ≤i ≤k −1. A

Multimedia Social Search Based on Hypergraph Learning
219
v1
v2
v3
v4
v5
v6
e1
e2
e3
e4
FIGURE 8.2.1: A hypergraph.
hypergraph is connected, if there is a hyperpath for every pair of vertices [ZHS06, BTC+10].
A hypergraph may be drawn as a set of points representing the vertices. The order of vertices
incident to a hyperedge is irrelevant. Hyperedges are represented by closed curves enclosing
their elements, as shown in Figure 8.2.1.
A hypergraph can be deﬁned by its incidence matrix H ∈RN×M whose element h(v, e)
equals 1 if v ∈e and 0 otherwise. Strictly speaking, the aforementioned deﬁnition yields the
so-called unsigned incidence matrix in graph theory. The incidence matrix of the hypergraph
shown in Figure 8.2.1 is given by
H =
e1 e2 e3 e4
v1
v2
v3
v4
v5
v6


1
0
0
0
1
1
0
0
1
1
1
0
0
0
0
1
0
0
1
0
0
0
1
0


.
(8.2.1)
The dual of a hypergraph with incidence matrix H is also a hypergraph with incidence
matrix HT , where the superscript T denotes matrix transposition. Accordingly, the vertices
of the dual hypergraph correspond to the edges of the original hypergraph and its edges
e∗
i = {ej: vi ∈ej}. If two vertices vi and vk are adjacent in the original hypergraph, then
their corresponding edges e∗
i and e∗
k in the dual hypergraph are adjacent. If two edges ej and
el are adjacent in the original hypergraph, then they will correspond to adjacent vertices in
the dual hypergraph. The dual of the hypergraph in Figure 8.2.1 is shown in Figure 8.2.2.
The cardinality of e deﬁnes the degree of the hyperedge, i.e., δ(e) = |e|. Using the deﬁni-
tion of the incidence matrix, the degree of a hyperedge is given by
δ(e) =
X
v∈V
h(v, e).
(8.2.2)
Frequently, each hyperedge is assigned a positive real weight w(e). The hypergraph is then
called weighted and is denoted as H = (V, E, w). The degree of a vertex is deﬁned as:
δ(v) =
X
e∈E| v∈e
w(e) =
X
e∈E
h(v, e) w(e).
(8.2.3)

220
Graph-Based Social Media Analysis
e1
e2
e3
e4
e∗
3
e∗
2
e∗
4
e∗
1
e∗
5
e∗
6
FIGURE 8.2.2: The dual of the hypergraph in Figure 8.2.1.
In the following, we shall use the diagonal matrices De ∈RM×M and Dv ∈RN×N having
the hyperedge and vertex degrees in their main diagonal, respectively. Similarly, we deﬁne
the diagonal matrix W ∈RM×M containing the hyperedge weights in its main diagonal.
The adjacency matrix A ∈RN×N of a hypergraph is deﬁned as:
A = H W HT −Dv.
(8.2.4)
The rank of a hypergraph is deﬁned as maxe∈E δ(e), while the anti-rank is deﬁned as
mine∈E δ(e). If the rank of a hypergraph equals its anti-rank, then the hypergraph is called
uniform. When the degree of each hyperedge is constant, say κ, then the hypergraph is called
κ-uniform. In undirected κ-uniform hypergraphs, the ordering of vertices in a hyperedge does
not matter.
The degree of hyperedges κ is related to the order of the model underlying the objects
in κ-uniform hypergraphs. For example, let us elaborate the case when a set of patterns
in RD should be clustered into K groups. Assume that the patterns lie on K subspaces of
dimension ι. The patterns could be associated to the vertices of a κ-uniform hypergraph.
To cluster the set of vertices V into K groups, aﬃnity measures over more than ι vertices
should be considered. That is, given a hyperedge e ⊂V with degree δ(e) = κ > ι, a
subspace should be ﬁt to e, say, via singular value decomposition (SVD). The ﬁtting error ϑ
can be transformed into a hyperedge weight w(e) = exp(−ϑ2
σ2 ). Accordingly, the hyperedge
e captures the existence of a subspace with a “degree of conﬁdence” given by its weight
w(e). In practice, κ = ι + 1.
Hypergraph-based learning techniques are classiﬁed into two categories. The ﬁrst cate-
gory uses hypermatrices for clustering, such as the high-order extension of spectral methods
for graphs. Despite the fact that the methods resorting to hypermatrices are mathemati-
cally attractive, they are conﬁned to the κ-uniform hypergraphs. The second category deals
with arbitrary hypergraphs, which model mixed higher-order relationships. The latter tech-
niques ﬁrst approximate a hypergraph via a standard weighted graph and then resort to
graph-based clustering and semi-supervised learning.
8.2.1
Uniform hypergraphs
Spectral graph theory has been thoroughly studied and has found a plethora of applica-
tions in combinatorics, computer science, social sciences, operations research, and biology
[CD12]. Its aim is to establish connections of the graph intrinsic structures (e.g., connectiv-
ity, diameters) with the spectra (i.e., the eigenvalues) of various associated matrices, such as

Multimedia Social Search Based on Hypergraph Learning
221
the adjacency matrix, the incidence matrix, and the graph Laplacian [Chu97, PZ13]. Several
attempts have been made to deﬁne the spectra of κ-uniform hypergraphs through the eigen-
values of special nonnegative symmetric hypermatrices, such as the adjacency hypermatrix
or the Laplacian hypermatrix.
Formally, hypermatrices are coordinate representations of tensors, just as matrices are
coordinate representations of linear operators. A tensor is an element of a tensor product
of κ vector spaces. The latter can be interpreted as the space of multilinear functionals.
Tensors are represented as hypermatrices by choosing a basis for each vector space [Lim14].
Let us conﬁne ourselves to N-dimensional vector spaces Vl, l = 1, 2, . . . , κ. For each vector
space, the typical choice is the standard basis
n
ϵ[l]
1 , ϵ[l]
2 , . . . , ϵ[l]
N
o
, where ϵ[l]
i
∈RN has its
i-th element equal to 1 and all other elements equal to 0. Then:
A =
N
X
i1=1
N
X
i2=1
· · ·
N
X
iκ=1
ai1i2...iκ ϵ[l]
i1 ◦ϵ[l]
i2 ◦· · · ◦ϵ[l]
iκ,
(8.2.5)
where ◦denotes the (Segre) outer product among vectors [Lim14]. That is, all the informa-
tion about tensor A is captured by the coordinates ai1i2...iκ, which are gathered in a κ-order
N-dimensional hypermatrix A. Accordingly, a κ-order N-dimensional hypermatrix A is a
collection of N κ elements.
For notation simplicity, let us denote the set of hypergraph vertices as V = {1, 2, . . . , N},
so that we deal only with the indices il ∈V, l = 1, 2, . . . , κ. For a κ-uniform hypergraph
H of N vertices, the (normalized) adjacency hypermatrix A is the κ-order N-dimensional
hypermatrix with elements [CD12]:
ai1i2...iκ =
(
1
(κ−1)!
if {i1, i2, . . . , iκ} ∈E
0
otherwise.
(8.2.6)
If H is a multi-hypergraph, then the denominator in (8.2.6) has to be replaced by the
binomial coeﬃcient
 κ−1
ν1,ν2,...,νs

, where νt is the times the unique element iιt occurs in the
multiset {i2, i3, . . . , iκ}, t ≤(κ −1).
The hypermatrix A in (8.2.6) is super-symmetric, because the elements associated to the
same index sets are the same. That is, ai1i2...iκ = aiϖ(1)iϖ(2)...iϖ(κ) for all the permutations
of indices. A κ-order N-dimensional super-symmetric hypermatrix A uniquely deﬁnes a
homogenous polynomial of degree κ in N variables [CD12]:
FA(x) =
N
X
i1=1
N
X
i2=1
· · ·
N
X
iκ=1
ai1i2...iκ xi1xi2 · · · xiκ.
(8.2.7)
Equation (8.2.7) can be expressed as the scalar product of the hypermatrix A with the
κ-order N-dimensional hypermatrix X with elements xi1i2...iκ = xi1 · xi2 · · · xiκ [Lat97], i.e.,
FA(x) =< A, X >. More insight oﬀers the interpretation of (8.2.7) as:
FA(x) = κ
X
e∈E
x[e],
(8.2.8)
where x[e] is the monomial Q
il∈e xil. Indeed, there are κ! active monomials in (8.2.7)
weighted by
1
(κ−1)! giving rise to the factor κ appearing in (8.2.8).
Let Xi1 be the (κ −1) order N-dimensional hypermatrix with elements xi2i3...iκ =
xi2 · xi3 · · · xiκ. The inner product of hypermatrices A and Xi1 over the common indices

222
Graph-Based Social Media Analysis
i2, i3, . . . , iκ yields a vector in RN whose i-th element is:
(< A, Xi1 >)i =
N
X
i2=1
N
X
i3=1
· · ·
N
X
iκ=1
ai i2...iκ xi2i3...iκ.
(8.2.9)
A real number λ is called an eigenvalue of A, if there is a non-zero vector x =
(x1, x2, . . . , xN)T ∈RN, which is called an eigenvector, such that:
(< A, Xi1 >)i = λ xκ−1
i
,
i = 1, 2, . . . , N.
(8.2.10)
The pair (λ, x) satisfying (8.2.10) is called an H-eigenpair [PZ13]. The indices of the elements
of x correspond to the vertices of the hypergraph under consideration. The left-hand side
of (8.2.9) can be interpreted as:
(< A, Xi1 >)i =
X
e∈Ei
x[e],
(8.2.11)
where Ei = {e \ {i} | i ∈e ∈E} is the set of hyperedges obtained by removing the vertex i
from each hyperedge of H containing i.
Let λmax denote the eigenvalue with largest modulus in (8.2.10). Let us also deﬁne the
set:
S≥0 =
n
x ∈RN | PN
i=1 xk
i = 1 and xi ≥0 for i = 1, 2, . . . , N
o
.
(8.2.12)
We call support of x, supp(x), all the indices of the non-zero elements of x. The following
lemmata hold [CD12]:
Lemma 8.2.1
If v ∈S≥0 maximizes FA(x) on S≥0, then supp(v) induces some collection
of connected components of H.
Lemma 8.2.2
If H is a connected κ-uniform hypergraph, then it has a strictly positive
eigenpair (λ, v), where λ is the maximum value of FA(x) admitted at x = v on S≥0.
Lemma 8.2.3
Let z =
1
N
1
κ 1 ∈RN, where 1 is a vector of ones. FA(z) equals the average
vertex degree δ.
Lemma 8.2.4
If H is a connected κ-order N-dimensional uniform hypergraph, then the
real eigenvalue λ given by Lemma 8.2.2 is the only eigenvalue with a strictly positive eigen-
vector. If λ′ is any other eigenvalue of H, then |λ′| ≤λ.
Lemma 8.2.5
For any non-empty κ-order N-dimensional uniform hypergraph, λmax can
be chosen to be a positive real number. If H is connected, then a corresponding eigenvector
x can be chosen to be strictly positive.
Lemma 8.2.6
If H is a κ-order N-dimensional uniform hypergraph, λmax is bounded as
follows:
δ ≤λmax ≤∆,
(8.2.13)
where ∆is the maximum vertex degree in H.
A special case of hypergraphs are the δ-regular ones, which have vertices with the same
degree, i.e., δ(i) = δ, i = 1, 2, . . . , N. For δ-regular hypergraphs, λmax = δ [CQ14]. An
example of a regular hypergraph is the cicrulant hypergraph. A κ-order N-dimensional
hypergraph H is called circulant if e = {i1, i2, . . . , iκ} ∈E implies that ˜e = {i′
1, i′
2, . . . , i′
κ} ∈
E for i′
l = il + 1, l = 1, 2, . . . , κ [CQ14].

Multimedia Social Search Based on Hypergraph Learning
223
Lemma 8.2.7
If G is a subgraph of H, then λmax(G) ≤λmax(H).
For a κ-order N-dimensional uniform hypergraph, the following hypermatrices are de-
ﬁned in R
N × N × . . . × N
|
{z
}
κ
[CQ14]:
degree hypermatrix denoted by D, i.e., a diagonal hypermatrix having elements
di1i2...iκ =
(
δ(l)
if i1 = i2 = · · · = iκ = l, l = 1, 2, . . . N
0
otherwise.
(8.2.14)
Laplacian hypermatrix L = D −A,
unsigned Laplacian hypermatrix Q = D + A.
Like the adjacency hypermatrix, the Laplacian and unsigned Laplacian hypermatrices are
super-symmetric. In addition, the adjacency and unsigned Laplacian hypermatrices are
nonnegative. If κ is even, the Laplacian and the unsigned Laplacian hypermatrices are
positive semi-deﬁnite. All the aforementioned hypermatrices possess H-eigenvalues. The
smallest H-eigenvalue of the Laplacian hypermatrix is zero with corresponding eigenvector
1 ∈RN. Furthermore, if the uniform hypergraph is δ-regular, then the largest H-eigenvalue
of the Laplacian and the unsigned Laplacian hypermatrices are δ and 2δ, respectively. For a
circulant hypergraph, all the aforementioned hypermatrices are super-symmetric circulant
hypermatrices.
Let ||x||2 be the ℓ2 norm of vector x, RN
+ = {x = (x1, x2, . . . , xN)T ∈RN | xi ≥0, i =
1, 2, . . . , N} be the set of nonnegative N-dimensional vectors, and SN−1 be the standard
unit sphere in RN. The maximization of the homogenous polynomial FA(x) subject to the
constraint ||x||2
2 = 1 yields a Z-eigenpair (λ, v), where λ is real and positive, known as the
spectral radius of A, ϱ(A), satisfying [PZ13]:
λ
△= ϱ(A) =
max
x∈SN−1∩RN
+
FA(x)
(8.2.15)
and v =
argmax
x∈SN−1∩RN
+
FA(x). It can be shown that [PZ13]:
1
N
κ
2
N
X
i=1
δ(i) ≤λ ≤min

∆
√
N, κM

,
(8.2.16)
where ∆is the maximum vertex degree in H and M = |E| ≤
 N
κ

is the cardinality of the
set of hyperedges.
8.3
Game-Theoretic approaches to uniform hypergraph clustering
An interesting perspective to the hypergraph clustering (cf. Section 8.4) was proposed
in [BP13], extending the prior work to pairwise clustering [PP07]. Instead of partitioning
the input objects (i.e., hypergraph vertices), and hence obtaining the clusters, a rigorous
notion of a cluster was proposed. In particular, a cluster is deﬁned as a maximally coherent
subset of vertices, V′ ⊂V, which satisﬁes both an internal criterion (i.e., all vertices in

224
Graph-Based Social Media Analysis
V′ are highly similar to each other) and an external one (i.e., all elements outside V′ are
highly dissimilar to those within V′). Such formal deﬁnition of cluster was given in terms
of the classical equilibrium concept from evolutionary game theory. It has been shown that
there exists a one-to-one correspondence between the equilibria and the local solutions of
a linearly constrained polynomial optimization problem [BP13], following similar lines to
[PP07]. This implies that a powerful class of dynamical systems can be exploited to extract
the clusters. The just mentioned dynamical systems are based on the well-known Baum-
Eagon inequality [BE67], which generalizes the pairwise replicator dynamics [HS98] from
evolutionary game theory to higher-order interactions. In addition, there is no need to set
a priori the number of clusters.
A κ-order N-dimensional weighted uniform hypergraph H = (V, E, w) is cast into a
κ-player hypergraph clustering game G = (P, V, b) with a set of players P = {1, 2, . . . , κ},
set of pure strategies available to each player equal to the hypergraph vertex set V =
{v1, v2, . . . , vN}, and payoﬀfunction b(e), which assigns a utility to each hyperedge e =
{v1, v2, . . . , vκ} ∈E that is proportional to the similarity of the vertices e is incident to, i.e.,
[BP13]:
b(e) =
(
1
κ!w(e)
if e = {v1, v2, . . . , vκ} ∈E
0
otherwise.
(8.3.1)
It is seen that the hyperedges play the role of strategy proﬁles, which are ordered sets of
pure strategies (i.e., vertices) played by the diﬀerent players. In addition, it is assumed
that all players share the same set of pure strategies and payoﬀfunction. In contrast to
classical game theory, the players are not supposed to behave rationally or to have complete
knowledge of the game details. The game is played with an evolutionary setting, where κ
players are drawn at random from a large population and each player is assumed to play
a pre-assigned strategy. Let D =
n
x ∈RN | xi ≥0, PN
i=1 xi = 1
o
be the standard simplex.
Given a population x ∈D, xi represents the fraction of players that is programmed to select
vi ∈V from the vertices to be clustered.
If x[κ] = (x, x, . . . , x)
|
{z
}
κ
and ϵi ∈RN is the vector whose i-th element equals 1 and all
other elements equal 0, the following functions are deﬁned [BP13]:
u(x[κ])
=
X
{v1,v2,...,vκ}∈Vκ
b(v1, v2, . . . , vκ) x[e]
(8.3.2)
u(x[κ−1], ϵi)
=
X
{v1,v2,...,vκ−1}∈Vκ−1
b(v1, v2, . . . , vκ−1, i) x[eκ],
∀vi ∈V
(8.3.3)
and x[e] is the monomial Qκ
j=1 xvj and eκ = {v1, v2, . . . , vκ−1}. The expected payoﬀearned
by a player selecting vi ∈V in a population x ∈D is given by (8.3.3), which measures
the average similarity of vi with respect to the cluster. The expected payoﬀover the entire
population is given by (8.3.3), because
u(x[κ]) =
X
vi∈V
u(x[κ−1], ϵi) xvi.
(8.3.4)
The expected payoﬀ(8.3.3) can be treated as a measure of cluster internal coherence,
capturing the average similarity of the vertices forming the cluster.
Intuitively, an evolutionary process reaches a Nash equilibrium x ∈D, when every indi-
vidual in the population obtains the same expected payoﬀand no strategy can prevail upon
the other ones, i.e.:
u(x[κ−1], ϵi) ≤u(x[κ]) ∀vi ∈V.
(8.3.5)

Multimedia Social Search Based on Hypergraph Learning
225
A strategy x ∈D is an evolutionary stable strategy (ESS) if and only if for all y ∈D \ {x},
∃vi ∈eκ such that both conditions:
u

(y −x)[i+1], x[κ−1−i]
<
0
(8.3.6)
u

(y −x)[l+1], x[κ−1−l]
=
0, l = 0, 1, . . . , i −1,
(8.3.7)
are satisﬁed [BP13]. The indices of the nonzero elements of x correspond to the indices of
the vertices forming a cluster. That is, the support of x provides a measure of the degree
of membership of its elements.
An ESS of the hypergraph clustering game deﬁnes an ESS-cluster in H. If x ∈D is an
ESS-cluster of H with support supp(x), then [BP13]
u(x[κ−1], ϵi) = u(x[κ]) ∀i ∈supp(x).
(8.3.8)
The resulting ESS cluster is V′ = {vi | i ∈supp(x)}. Moreover, V′ is a two-cover of H. That
is, for any pair of vertices {vj, vl} ∈V′, there exists an edge e ∈E such that {vj, vl} ⊆e ⊂V′
[BP13].
The ESS-clusters also satisfy a property of external coherence. That is:
u(x[κ−1], ϵi) ≤u(x[κ]) ∀i ̸∈supp(x).
(8.3.9)
Equation (8.3.9) is a consequence of the fact that x ∈D is a Nash equilibrium with support
supp(x). Whenever one deviates from an ESS-cluster x ∈D, e.g., by adding an external
element to its support, the cluster similarity drops, provided that the deviation is not too
large.
Bulo and Pelillo [BP13] proved:
Theorem 8.3.1 Let H = (V, E, w) be a hypergraph clustering problem, G = (P, V, b) be the
corresponding clustering game, and f(x) be deﬁned as:
f(x) = u(x[κ]) =
X
e∈E
b(e) x[e].
(8.3.10)
Nash equilibria of G have one-to-one correspondence with the critical points of the maximiza-
tion of f(x) over D, i.e., they satisfy the ﬁrst-order necessary Karush-Kuhn-Tucker(KKT)
conditions [Kuh76] of this optimization problem. Moreover, the ESS-clusters of H have one-
to-one correspondence with the strict local maximizers of f(x) over D.
The extraction of the ESS-clusters can be cast into ﬁnding strict local maximizers of (8.3.10)
in D. The function f(x) in (8.3.10) is a homogeneous polynomial in the variables xi with
nonnegative coeﬃcients. Accordingly, it is a special case of the Baum-Eagon theorem [BE67].
In particular, we obtain:
∂f(x)
∂xi
= 1
κ u(x[κ−1], ϵi), i = 1, 2, . . . , N,
(8.3.11)
which implies
N
X
l=1
xl
∂f(x)
∂xl
= 1
k u(x[κ]).
(8.3.12)
Accordingly, the growth transformation that extracts an ESS-cluster takes the form:
xi(t + 1) =
xi(t) ∂f(x)
∂xi
|xi(t)
PN
l=1 xl(t) ∂f(x)
∂xl
|xi(t)
= xi(t) u(x[κ−1](t), ϵi)
u(x[κ](t))
, i = 1, 2, . . . , N.
(8.3.13)

226
Graph-Based Social Media Analysis
During the evolution of (8.3.13), better-than-average strategies, i.e., strategies satisfying
u(x[κ−1], ϵi) > u(x[κ]), will spread in the population, while the remaining ones will be
eliminated, establishing a Darwinian selection process. Accordingly, (8.3.13) generalizes the
classical formulation of natural selection process in a two-player evolutionary game theory,
known as replicator dynamics [HS98].
The population dynamics are initialized with the barycenter of the simplex D, i.e.,
x(0) = 1
N 1, where 1 ∈RN is a vector of ones, which deﬁnes a uniform distribution over the
set of vertices in V. By doing so, no particular vertex is favored. The growth transformation
(8.3.13) satisﬁes the invariant property supp(x(t)) ⊂supp(x(0)) for t > 0. If the numerator
in (8.3.13) is positive for all i ∈supp(x(0)), then supp(x(t)) = supp(x(0)) for all ﬁnite
t > 0. That is, the limit point of the trajectory xopt = limt→∞x(t) only asymptotically
satisﬁes supp(xopt) ⊂supp(x(0)), suggesting the need for thresholding the elements of x in
order to obtain the support of the corresponding ESS-cluster. Having found an ESS-cluster
V′ with dynamics (8.3.13), one removes its vertices from V and repeats the just described
procedure for the remaining vertices. That is, one cluster is extracted at a time.
A uniﬁed method for clustering from κ-ary aﬃnity relations can be found in [LLY10] that
is motivated by the intuitive observation that there may exist
 m
κ

possible κ-ary aﬃnity
relations for a cluster of m objects capturing the internal coherence conditions. Let B be
the weighted adjacency hypermatrix having elements:
b(e) =
(
w(e)
if e = {v1, v2, . . . , vκ} ∈E
0
otherwise.
(8.3.14)
For each edge e, there are κ! duplicate entries in B. For a subset of N ′ vertices, V′ ⊆V,
its hyperedge set is denoted as E′. If V′ is a cluster, then the majority of hyperedges in E′
should have large weights. This property can be captured by the sum of all entries in B
that are associated with hyperedges containing only vertices in V′ [LLY10]:
S(V′) =
X
{v1,v2,...,vκ}∈V′
b(v1, v2, . . . , vκ) =
X
{v1,v2,...,vκ}∈V
b(v1, v2, . . . , vκ) y[e],
(8.3.15)
where y[e] is the monomial Qκ
i=1 yvi with yvi = 1 if vi ∈V′ and zero otherwise. Since the
cardinality of V′ is N ′, (N ′)κ terms are added in (8.3.15) yielding an average
Sav(V′) =
1
(N ′)κ S(V′) =
X
{v1,v2,...,vκ}∈V
b(v1, v2, . . . , vκ) x[e],
(8.3.16)
where x[e] is the monomial Qκ
i=1 xvi with xvi =
yvi
N ′ if vi ∈V′ and zero otherwise. That is, the
ℓ1 norm of x ∈RN is 1. Accordingly, the clustering problem can be cast as maximization of
Sav(V′) [LLY10]. However, neither N ′ is known nor are known the N ′ objects to be chosen.
To relax this NP-hard problem, the i-th element of x, xi is allowed to vary in the continuous
range [0, ε], where the constant ε ≤1:
max f(x) =
X
{v1,v2,...,vκ}∈V
b(v1, v2, . . . , vκ) x[e]
s.t. x ∈Dε =
n
x ∈RN | 0 ≤xi ≤ε, PN
i=1 xi = 1
o
,
(8.3.17)
where the acronym s.t. stands for “subject to.” The constraint in (8.3.17) enables an intuitive
interpretation of xi as the probability for a cluster to contain the i-th object. Moreover,
such a constraint sparsiﬁes the solution. It is seen that the maximization of (8.3.10) in
the standard simplex is a special case of (8.3.17) for ε = 1. Setting ε < 1 implies that

Multimedia Social Search Based on Hypergraph Learning
227
the probability of choosing each vertex in the game has a known upper bound, which is
the prior, while ε = 1 represents a noninformative prior [LLY10]. In fact, ε oﬀers a tool
to control the least number of objects in a cluster. The problem (8.3.17) has many local
maxima. The large ones correspond to true clusters, while the small ones usually form
meaningless subsets. Let us elaborate the constrained optimization problem (8.3.17) by
inserting Lagrange multipliers ξ for the equality constraint PN
i=1 xi = 1, βi > 0 for the
inequality constraints xi ≥0, and γi for the inequality constraints xi ≤ε, i = 1, 2, . . . , N.
The resulting Lagrangian function is:
L(x, ξ, β, γ) = f(x) −ξ
 1T x −1

+
N
X
i=1
βi xi +
N
X
i=1
γi (ε −xi) .
(8.3.18)
Let eκ = {v1, v2, . . . , vκ−1}. The reward at vertex i is deﬁned as [LLY10]:
ri(x) =
X
eκ∈V
b(eκ, i)x[eκ].
(8.3.19)
It can be easily proved that the gradient of f(x) with respect to xi is proportional to ri(x),
i.e.:
∂f(x)
∂xi
= κ ri(x).
(8.3.20)
Any local maximizer xopt satisﬁes the KKT conditions. That is:
κ ri(xopt) −ξ + βi −γi
=
0, i = 1, 2, . . . , N
N
X
i=1
xopt,i βi
=
0
(8.3.21)
N
X
i=1
(ε −xopt,i) γi
=
0.
Since xopt,i, βi, and γi are all nonnegative for i = 1, 2, . . . , N, (8.3.21) can be rewritten as
[LLY10]:
ri(xopt)





≤ξ
κ
xopt,i = 0
= ξ
κ
0 < xopt,i < ε
≥ξ
κ
xopt,i = ε.
(8.3.22)
Indeed, if xopt,i > 0, then βi = 0 and if xopt,i < ε, then γi = 0, giving rise to equation in
the second line of (8.3.22). If xopt,i = ε, γi > 0 and βi = 0. Solving the equation in the
ﬁrst line of (8.3.21) for γi and replacing into γi ≥0 yields the inequality in the third line of
(8.3.22). Similarly if xopt,i = 0, then βi ≥0 and γi = 0. Solving the equation in the ﬁrst line
of (8.3.21) for βi and replacing into βi ≥0 yields the inequality in the ﬁrst line of (8.3.22).
It is seen that the vertices in V are divided into three disjoint subsets, namely V1(x) =
{vi | xi = 0}, V2(x) = {vi | xi ∈(0, ε)}, and V3(x) = {vi | xi = ε}. Let Vd(x) = V2(x) ∪
V3(x) be the set of non-zero components in x, while Vu(x) = V1(x) ∪V2(x) be the set of
components in x that are smaller than ε. For any x, if f(x) must increase, then the values
from some components in Vd(x) must decrease and the values of some components in Vu(x)
must increase [LLY10]. If x is a maximizer of (8.3.17), then ri(x) ≤rj(x), ∀vi ∈Vu(x) and
∀vj ∈Vd(x). If x is not a maximizer, then we should increase xi and decrease xj. This is

228
Graph-Based Social Media Analysis
achieved as follows [LLY10]. If eκ,κ−1 = {v1, v2, . . . , vκ−2}, deﬁne
rij(x)
=
X
eκ,κ−1
b(v1, v2, . . . , vκ−2, i, j) x[eκ,κ−1]
(8.3.23)
ζ
=
( min(xj, ε −xi)
if rij(x) ≤0
min(xj, ε −xi,
ri(x)−rj(x)
2(κ−1) rij(x))
otherwise
(8.3.24)
and update x according to
xl(t + 1) =





xl(t) + ζ
if l = i
xl(t) −ζ
if l = j
xl(t)
otherwise.
(8.3.25)
Multiple initializations (i.e., priors) are needed to obtain the signiﬁcant maxima of
(8.3.17), which correspond to true clusters in general. Informative priors can be eﬃciently
constructed from the vertices in E(v), which are connected with vertex v through a hyper-
edge, by means of Algorithm 8.3.1 [LLY10]. In Algorithm 8.3.1, [ 1
ε] represents the smallest
Algorithm 8.3.1: Construction of a prior x(0) containing vertex v.
Input: Hyperedge set E(v) and ε.
Output: A prior x(0).
1: Sort the hyperedges e ∈E(v) in descending order according to their weight w(e)
2: for e ∈|E(v)| do
3:
Add all vertices associated with the hyperedge e to L
4:
if |L| ≥[ 1
ε] then
5:
break
6:
end if
7: end for
8: for each vertex v ∈L do
9:
Set the corresponding component xv(0) =
1
|L|
10: end for
integer that is larger than or equal to 1
ε. Having an informative prior x(0), a local maximizer
of (8.3.17) can be obtained thanks to Algorithm 8.3.2 [LLY10].
As mentioned previously, the hyperedge degree δ(e) = κ is frequently chosen as model
order ι plus 1, i.e, the smallest possible. Even in this case, the number of possible hyperedges
in a κ-uniform N dimensional hypergraph,
 N
κ

, is too large for exhaustive listing. Most of
the existing techniques sample the set of hyperedges to construct sparse hypergraphs. In
addition, by limiting κ to the smallest number, the chance of identifying pure hyperedges,
i.e., those containing objects that are likely from the same cluster, is maximized [PCAS14].
Accordingly, computational feasibility is the underlying justiﬁcation for imposing the afore-
mentioned constraint in the hyperedge degree.
Purkait et al. investigated whether there was any beneﬁt to employing hyperedges of
higher degree, i.e., κ > ι + 1 for hypergraph clustering and whether one could sample
large hyperedges without signiﬁcant eﬀort [PCAS14]. Sampling strategies based on random
cluster models were proposed as a solution.
Shashua et al. [SZH06] cast the clustering problem with high-order relations into a
nonnegative factorization problem of the closest hyper-stochastic version of the input aﬃnity
hypermatrix, extending the work in [Gov05], where a pairwise similarity matrix was derived

Multimedia Social Search Based on Hypergraph Learning
229
Algorithm 8.3.2: Computation of a local maximizer xopt given an informative prior x(0).
Input: Weighted adjacency hypermatrix B and prior x(0).
Output: Local maximizer xopt.
1: t = 0
2: repeat
3:
Compute the reward ri(x(t)) for each vertex vi using (8.3.19)
4:
Compute V1(x(t)) = {vi | xi(t) = 0}, V2(x(t)) = {vi | xi(t) ∈(0, ε)}, V3(x(t)) =
{vi | xi(t) = ε}, Vd(x(t)) = V2(x(t)) ∪V3(x(t)), and Vu(x(t)) = V1(x(t)) ∪V2(x(t))
5:
Find the vertex vi ∈Vu(x(t)) with the largest reward and the vertex vj ∈Vd(x(t))
with the smallest reward.
6:
Compute ζ using (8.3.24)
7:
Compute x(t + 1) using (8.3.25)
8:
Set t ←t + 1
9: until x is a local maximizer
from the factorization of the input aﬃnity hypermatrix and was given as input to standard
graph spectral clustering techniques.
8.4
Spectral clustering for arbitrary hypergraphs
Given a similarity measure, clustering aims at organizing a set of objects into groups
(i.e., the clusters) so that the objects grouped together are similar and, at the same time,
dissimilar to other objects assigned to diﬀerent clusters. Object similarities are typically
expressed as pairwise relations. That is, the similarity relation between the objects forms
a weighted graph G(V, E, w). Various unsupervised and semi-supervised machine learning
techniques have been formulated as operations on this graph. For example, in spectral clus-
tering, the relation between the structural and the spectral properties have been analyzed
by matrix theoretic methods that are graph theoretic as well. Indeed, the Laplacian of
the graph is such a matrix that is used to study the structure of the graph [Chu97]. The
unnormalized Laplacian (also known as combinatorial Laplacian) is deﬁned as [Chu97]:
L = Dv −A,
(8.4.1)
where Dv is the diagonal matrix consisting of vertex degrees and A ∈R|V|×|V| is the
adjacency matrix with entry (u, v) equal to the weight of the edge (u, v) if they are connected
and 0 otherwise. The adjacency matrix can be expressed in terms of the incidence matrix
H ∈R|V|×|E| and the diagonal matrix of weights W as:
A = 1
2 H W HT .
(8.4.2)
Equation (8.4.1) can be rewritten as:
L = D
1
2v

I −D
−1
2
v
A D
−1
2
v

|
{z
}
˜L
D
1
2v ,
(8.4.3)

230
Graph-Based Social Media Analysis
where ˜L is the so-called normalized Laplacian. By substituting (8.4.2) into (8.4.1) and the
deﬁnition of the normalized Laplacian in (8.4.3), we obtain:
L
=
1
2
 2 Dv −H W HT 
(8.4.4)
˜L
=
I −1
2 D
−1
2
v
H W HT D
−1
2
v
.
(8.4.5)
The graph Laplacian is the discrete analog of the Laplace-Beltrami operator on compact
Riemannian manifolds [Chu97, ABB06]. The generalized eigenvalue problem involving the
graph Laplacian was used in the development of spectral clustering algorithms by relaxing
the graph partitioning into a continuous optimization problem [AKY99, SM00, NJW01].
However, in several applications, such as face clustering [ALZM+05], perceptual grouping
[Gov05], parametric motion segmentation [SZH06], image retrieval [HLZM10], and image
categorization [HLL+11], higher-order relations are shown to be more appropriate. To clus-
ter such high-order similarities, we may resort to spectral hypergraph clustering, where the
vertices are the objects to be clustered and the weighted hyperedges encode the high-order
similarities. In the following, we shall not assume that each hyperedge contains exactly κ
vertices, as in Section 8.3. Such arbitrary hypergraphs include the undirected, weighted
hypergraphs H(V, E, w), a superset containing the undirected, weighted graphs as a special
case.
It is worth noting that spectral hypergraph clustering has a long history in Very Large
Scale Integration (VLSI) [AK95, KAKS97, ZSC99, KK00]. There, vertices correspond to
circuit elements and hyperedges correspond to wiring that may connect more than two
elements. Finding the cuts with minimum cost allows one to partition the elements into
modules with minimum interconnections. The leading algorithm in VLSI design is based on
a two-phase multilevel approach [KK00]. In the ﬁrst phase, a hierarchy of hypergraphs is
constructed, where the hypergraph at each level is a coarser version of the hypergraph at the
previous one, according to some measure of homogeneity. In the second phase, starting from
a partition at the coarsest level, the algorithm proceeds downward in the hierarchy and each
level updates the partition obtained at the previous one in a greedy fashion. Recently, an
increasing interest in spectral hypergraph clustering has been observed in various domains,
notably computer vision, multimedia information retrieval, machine learning, etc.
In this section, spectral hypergraph clustering is addressed by resorting to graph-based
clustering methods. That is, to transform a hypergraph into a graph whose edge-weights
are mapped from the weights of the original hypergraph. Two approaches were proposed,
namely the clique expansion and the star expansion [ZSC99]. The clique expansion con-
structs a graph Gx(V, Ex, wx) from the hypergraph H(V, E, w) by replacing each hyper-
edge e = (v1, v2, . . . , vκ) ∈E with an edge for each pair of vertices in the hyperedge
Ex = {(vi, vj) | vi ∈e, vj ∈e, e ∈E}. The weight assigned to the resulting edge wx(vi, vj)
should minimize the diﬀerence between itself and the weight of each hyperedge e incident
to both vi and vj. Accordingly:
wx(vi, vj) =
X
eE| vi∈e, vj∈e
w(e).
(8.4.6)
The normalized Laplacian of Gx can be expressed as [ZSC99]:
˜Lx = I −C,
(8.4.7)
where C has elements
C(i, j) =



0
if ̸ ∃e ∈E such that vi ∈e and vj ∈e
wx(vi,vj)
√
δx(vi) √
δx(vj)
otherwise,
(8.4.8)

Multimedia Social Search Based on Hypergraph Learning
231
with:
δx(vi) =
X
e∈E
h(vi, e) (δ(e) −1) w(e).
(8.4.9)
The star expansion algorithm constructs a graph G∗(V∗, E∗, w∗) from the hypergraph
H(V, E, w) by introducing a new vertex for every hyperedge e ∈E. Thus, V∗= V ∪E.
The resulting edge set is E∗= {(v, e) | v ∈e, e ∈E}. Star expansion assigns the scaled
hyperedge weight to each corresponding graph edge:
w∗(v, e) = w(e)
δ(e) .
(8.4.10)
G∗is a bipartite graph with vertices corresponding to E on the one side and vertices corre-
sponding to V on the other, since there are no edges from V to V or from E to E [ABB06].
Let us assume that the vertex set V∗has been ordered such that all elements of V appear on
the top of the elements of E. The adjacency matrix of G∗having size (|V|+|E|)×(|V|+|E|)
can be written as [ABB06]:
A∗=


0|V|×|V|
H W∗
W∗HT
0|E|×|E|

,
(8.4.11)
where W∗is the properly scaled hyperedge weight matrix. The degrees of the vertices in
G∗are given by:
δ∗(v)
=
X
e∈E
h(v, e) w∗(v, e),
v ∈V
(8.4.12)
δ∗(e)
=
X
v∈V
h(v, e) w∗(v, e),
e ∈E.
(8.4.13)
Let Dv∗be the |V|×|V| diagonal matrix with elements in the main diagonal δ∗(v). Similarly
let De∗be the |E| × |E| diagonal matrix with elements in the main diagonal δ∗(e). The
normalized Laplacian of G∗is written in the form [ABB06]:
˜L∗
=
"
I|V|×|V|
0
0
I|E|×|E|
#
−

D
−1
2
v∗
0
0
D
−1
2
e∗

A∗

D
−1
2
v∗
0
0
D
−1
2
e∗


=
"
I|V|×|V|
−B
−BT
I|E|×|E|
#
,
(8.4.14)
where B ∈R|V|×|E| is:
B = D
−1
2
v∗H W∗D
−1
2
e∗.
(8.4.15)
Any eigenvector φ =

φT
v φT
e
T
of the normalized Laplacian ˜L∗with corresponding eigen-
value λ satisﬁes the identity [ABB06]:
B BT = (1 −λ)2φv.
(8.4.16)
That is, the |V| elements of the eigenvectors of the normalized Laplacian, which correspond
to the vertices of the hypergraph V, are the eigenvectors of the |V| × |V| matrix B BT . If
(8.4.10) is substituted into (8.4.12) and (8.4.13), we obtain:
δ∗(v)
=
X
e∈E
h(v, e) w(e)
δ(e) ,
v ∈V
(8.4.17)
δ∗(e)
=
w(e),
e ∈E.
(8.4.18)

232
Graph-Based Social Media Analysis
It is not diﬃcult to show that the (i, j) element of B BT is given by:
[B BT ]ij =
1
p
δ∗(vi)
p
δ∗(vj)
X
e∈E
h(vi, e) h(vj, e) w(e)
δ2(e).
(8.4.19)
If we substitute:
wc
∗= w(e) (δ(e) −1)
(8.4.20)
into (8.4.12) and (8.4.13), the vertex degree of the clique expansion Gx results [ABB06]:
δc
∗(v)
=
δx(v)
(8.4.21)
δc
∗(e)
=
w(e) δ(e) (1 −δ(e)) .
(8.4.22)
Accordingly, the equivalent bipartite graph matrix Bc
∗(Bc
∗)T for the clique expansion has
elements:
[Bc
∗(Bc
∗)T ]i,j =
X
e∈E
h(vi, e) h(vj, e) w(e)
δ(e) (1 −δ(e))
.
(8.4.23)
If instead of (8.4.6), we choose:
wc
x(vi, vj) =
1
δ(e) (δ(e) −1)
X
e∈E
h(vi, e) h(vj, e) w(e),
(8.4.24)
the vertex degree for star expansion G∗(8.4.17) emerges. The equivalent normalized graph
Laplacian of Gc
x, Cc
x, for star expansion has elements [ABB06]:
[Cc
x]ij =
1
p
δ∗(vi)
p
δ∗(vj)
X
e∈E
1
δ(e) (δ(e) −1) h(vi, e) h(vj, e) w(e).
(8.4.25)
For κ-uniform hypergraphs, all vertex degrees are equal to κ. In this case, the following
pairs of matrices coincide up to a multiplicative constant:
• C with elements (8.4.8) and Bc
∗(Bc
∗)T with elements (8.4.23),
• Cc
x with elements (8.4.25) and B BT with elements (8.4.19).
Thus, the eigenvectors of the normalized Laplacian Bc
∗(Bc
∗)T for the bipartite graph Gc
∗
are exactly the eigenvectors of the normalized Laplacian C of the clique expansion graph
Gx. Similarly, the eigenvectors of the clique matrix Cc
x are exactly the eigenvectors of the
normalized Laplacian for star expansion B BT .
Bolla deﬁned a Laplacian matrix for an unweighted hypergraph (i.e., W = I) in terms
of the vertex degree matrix Dv, the edge degree matrix De, and the incidence matrix
H, LBolla = Dv −H D−1
e
HT , and established a link between the spectral properties of
LBolla and the minimum cut of the hypergraph [Bol93]. It has been shown that LBolla is
equal to the unnormalized Laplacian of the associated clique expansion with weight matrix
WBolla = H D−1
e
HT [ABB06].
Rodriguez has disclosed the same results to those in [Bol93] by transforming the hy-
pergraph into a graph using the clique expansion [Rod03]. In particular, a weighted graph
GRodriguez(V, Ex, wr) has been constructed from the unweighted hypergraph H(V, E) by re-
placing each hyperedge by a clique. The weight of the edge is set to the number of edges
containing both vi and vj, i.e., wRodriguez(v1, v2) = | {e ∈E | vi ∈e, vj ∈e} |. The graph
Laplacian of GRodriguez was expressed as LRodriguez = Dr
v −H HT , where Dr
v is the vertex
degree of the graph GRodriguez. It has been shown that there is a relationship between the
spectral properties of LRodriguez and the minimum cut of the original hypergraph. Agarwal

Multimedia Social Search Based on Hypergraph Learning
233
et al. proposed the clique averaging method and reported better results than the clique
expansion method [ALZM+05].
A hypergraph normalized cut criterion has been proposed in [ZHS06] that generalizes
the well-known normalized cut method [SM00]. In particular, let (S, S) be a partitioning
of the vertices of V in H by a cut, where S ∪S = V. This can be achieved by removing
the hyperedges in the cut set Ec(S, S) =

e ∈E | e ∩S ̸= ∅, e ∩S ̸= ∅
	
, which yields the
disjoint sets S and S. The cost of the cut was deﬁned by the following volume [ZHS06]:
vol(S, S) =
X
e∈Ec(S,S)
w(e) |e ∩S| |e ∩S|
δ(e)
|
{z
}
ζ(e| S,S)
(8.4.26)
that is interpreted as the sum of weights of the hyperedges, which are cut. The volume of
cluster S was deﬁned as the sum of the degrees of the vertices in S:
vol(S) =
X
v∈S
δ(v).
(8.4.27)
The deﬁnition in (8.4.26) results from treating each hyperedge e as a clique (i.e., a fully
connected subgraph) and assigning the same weight w(e)
δ(e) to all subedges of the (virtual)
subgraph. When a hyperedge e is cut, |e ∩S| |e ∩S| subedges should be cut. Using (8.4.26)
and (8.4.27), the normalized cut criterion for partitioning V into (S, S) was set as:
ncut(S) = vol(S, S)

1
vol(S) +
1
vol(S)

(8.4.28)
and the optimal S, Sopt, was obtained as the solution of the optimization problem:
Sopt = argmin
∅̸=S⊂V
ncut(S),
(8.4.29)
which formally expresses the natural requirements of a partition, namely a dense connection
among the vertices in the same cluster and a sparse connection between two clusters. For a
simple graph, |e ∩S|=|e ∩S| = 1 and δ(e) = 2. Thus, the right-hand side of (8.4.26) reduces
to the ordinary graph normalized cut [SM00] up to a factor 1
2.
Another interpretation worth mentioning is associated with random walks [ZHS06] that
generalizes the natural random walk on ordinary graphs. Let us associate each hypergraph
with a natural random walk with the following transition rule: Given the current vertex
v ∈V, ﬁrst choose a hyperedge e among those incident with v with probability proportional
to w(e), and then choose a vertex v′ ∈e uniformly at random. The resulting transition
probability matrix P is given by:
P = D−1
v
H W D−1
e
HT .
(8.4.30)
Its elements are p(v, v′) = P
e∈E w(e) h(v,e)
δ(v)
h(v′,e)
δ(e) . The stationary distribution π of the
random walk satisﬁes the identity:
πT = πT P.
(8.4.31)
It can be veriﬁed that π(v) =
δ(v)
vol(V) or in matrix notation:
πT =
1
vol(V) δT
v ,
(8.4.32)

234
Graph-Based Social Media Analysis
where δv = vec(Dv) = HW1M with vec() being the operation that returns the elements in
the main diagonal of the diagonal matrix inside parentheses as a column vector of compatible
dimensions and 1M denoting the M × 1 vector of ones. Indeed if (8.4.32) and (8.4.30) are
substituted into (8.4.31), we obtain:
πT P
=
1
vol(V) δT
v D−1
v
| {z }
1T
N
H W D−1
e
HT =
1
vol(V) 1T
N H
| {z }
δ
T
e
W D−1
e
HT
=
1
vol(V) δT
e D−1
e
| {z }
1T
M
W HT =
1
vol(V) 1T
M W HT
=
1
vol(V) δT
v ,
(8.4.33)
where we used that δT
e = vec(De) = 1T
N H and the fact that the diagonal matrices W and
D−1
e
commute in multiplication. The hypergraph normalized cut (8.4.28) can be rewritten
as [ZHS06]:
ncut(S) = vol(S, S)
vol(V)


1
vol(S)
vol(V)
+
1
vol(S)
vol(V)

.
(8.4.34)
Moreover, it is seen that:
vol(S)
vol(V) =
X
v∈S
δ(v)
vol(V) =
X
v∈S
π(v)
(8.4.35)
and similarly:
vol(S, S)
vol(V)
=
X
v∈S
X
v′∈S
π(v) p(v, v′).
(8.4.36)
That is, the ratio vol(S,S)
vol(V) is the probability with which one sees a jump of the random walk
from S to S under the stationary distribution [ZHS06]. Accordingly, the hypergraph normal-
ized cut criterion can be interpreted as seeking a cut such that the probability with which
the random walk crosses diﬀerent clusters is as small as possible, while the probability with
which the random walk stays in the same cluster is as large as possible. This interpretation
is consistent with the random walk view of ordinary graphs [MS01].
The optimization problem (8.4.29) in NP-complete and it can be relaxed into a real-
valued optimization problem [ZHS06]:
argmin
f∈RN
1
2
X
e∈E
X
{v,v′}⊆e
w(e)
δ(e)
 
f(v)
p
δ(v)
−f(v)
p
δ(v)
!2
s.t. ||f||2
2 = 1 and P
v∈V f(v)
p
δ(v) = 0.
(8.4.37)
One may coin the N × N matrix:
˜L = I −Θ
(8.4.38)
as hypergraph Laplacian, where:
Θ = D
−1
2
v
H W D−1
e
HT D
−1
2
v
(8.4.39)

Multimedia Social Search Based on Hypergraph Learning
235
and I is the identity matrix of compatible dimensions. Indeed, for an ordinary graph (i.e.,
De = 2I), we obtain:
˜L
=
I −1
2 D
−1
2
v
H W HT
|
{z
}
Dv+A
D
−1
2
v
= I −1
2 D
−1
2
v
(Dv + A) D
−1
2
v
=
1
2

I −D
−1
2
v
AD
−1
2
v

,
(8.4.40)
which is the ordinary graph normalized Laplacian up to a factor 1
2. It is trivial to show
that the double sum in (8.4.37) is simply the quadratic form 2 f T ˜L f. For w(e) ≥0, the
quadratic form is nonnegative, accordingly, ˜L is positive semi-deﬁnite. It can be checked that
the smallest eigenvalue of ˜L is zero and its corresponding eigenvector is √δv = vec(D
1
2v ),
where the √is retained in the left-hand side for the easy of linear algebra manipulations
following:
˜L
p
δv
=
p
δv −Θ
p
δv =
p
δv −D
−1
2
v
H W D−1
e
HT D
−1
2
v
p
δv
|
{z
}
1N
=
p
δv −D
−1
2
v
H W D−1
e
HT 1N
| {z }
δe
=
p
δv −D
−1
2
v
H W D−1
e
δe
| {z }
1M
=
p
δv −D
−1
2
v
H W 1M
|
{z
}
δv
=
p
δv −D
−1
2
v
δv
| {z }
√δv
= 0.
(8.4.41)
The second constraint in (8.4.37) indicates that the solution should be orthogonal to the
eigenvector √δv. Accordingly, the solution of this optimization problem is an eigenvector φv
of ˜L associated with the second smallest eigenvalue (i.e., the smallest nonzero eigenvalue).
Consequently, the vertex set V is split into two disjoint clusters S = {v ∈V | φv(v) ≥0}
and S = {v ∈V | φv(v) < 0}.
The normalized hypergraph Laplacian ˜L in (8.4.38) is related with the Laplacian of a
graph with adjacency matrix (8.4.11) with W∗= W [ABB06]. The degree matrix for this
graph is given by the diagonal matrix:
DZhou =
"
Dv
0
0
W De
#
.
(8.4.42)
The normalized Laplacian for this bipartite graph is given by:
˜LZhou
=
"
I|V|×|V|
0
0
I|E|×|E|
#
−

D
−1
2
v
0
0
D
−1
2
e
W−1
2


"
0|V|×|V|
H W
W HT
0|E|×|E|
# 
D
−1
2
v
0
0
D
−1
2
e
W−1
2


=
"
I|V|×|V|
−B
−BT
I|E|×|E|
#
(8.4.43)
for B = D−1/2
v
H W
1
2 D
−1
2
e
. Any eigenvector φ =

φT
v φT
e
T
of the normalized Laplacian

236
Graph-Based Social Media Analysis
˜LZhou with corresponding eigenvalue λ′ satisﬁes (8.4.16), where B BT = Θ, i.e.:
Θ φv
=
(1 −λ′)2 φv or
(I −Θ)
| {z }
˜L
φv
=
(1 −(1 −λ′)2)
|
{z
}
λ
φv.
(8.4.44)
That is, the N = |V| elements of the eigenvectors of the normalized Laplacian ˜LZhou, which
correspond to the vertices of the hypergraph, are the eigenvectors of the N × N matrix
˜L = I −Θ. Since λ = λ′ (2 −λ′) ≥0, we infer that 0 ≤λ′ ≤2.
The spectral hypergraph clustering approach can be extended to K-way partitioning,
i.e., to ﬁnding the partition (V1, V2, . . . , VK) with ∪K
j=1Vj = V and Vj ∩V′
j = ∅for j ̸= j′
and 1 ≤j, j′ ≤K. Zhou et al. [ZHS06] proved the following theorem:
Theorem 8.4.1 Assume a hypergraph H = (V, E, w) with |V| = N. Denote λ1 ≤
λ2
≤
. . .
≤
λN
the eigenvalues of the hypergraph Laplacian ˜L. If ncutK(H)
=
min ncut(V1, V2, . . . , VK) is the cost of K-way partitioning, then ncutK(H) ≥PK
j=1 λj.
It is seen that the real-valued optimization problem derived from the relaxation is actually a
lower bound of the original combinatorial optimization problem. It is unclear how one may
employ multiple eigenvectors simultaneously in order to obtain a K-way partition [ZHS06].
Among the many heuristics proposed in the literature, the most popular works as follows
[NJW01].
1. Create matrix X = [φ1|φ2| · · · φK] where φj, j = 1, 2, . . . , K are the eigenvectors of
˜L associated to the K smallest eigenvalues.
2. Treat the row vectors of X as representations of the hypergraph vertices in the K-
dimensional Euclidean space. They are expected to be well separated.
3. Obtain a good partition by running K-means on them once.
The existence of the multiplier ζ(e | S, S) in (8.4.26) diﬀerentiates the normalized cut
on hypergraphs from that on ordinary graphs. As mentioned, this multiplier arises from the
projection of the hypergraph to a normal graph, where each hyperedge e is replaced by the
fully connected subgraph with vertices e to which the weight w(e)
δ(e) is assigned [ABB06]. The
cost (8.4.28) is the cut cost of the projected graph. ζ(e | S, S) can be factored as [PCAS14]:
ζ(e | S, S) = η (1 −η) δ(e),
(8.4.45)
where η = |e∩S|
δ(e) varies within the range:
1
δ(e) ≤η ≤δ(e) −1
δ(e)
.
(8.4.46)
In [PCAS14], it has been shown empirically that the cost of cutting a hyperedge e is highest,
if e is divided into equal halves (i.e., η = 0.5). Moreover, given the same η, ζ(e | S, S) is
always higher, when δ(e) = κ increases (i.e., for larger hyperedges), since the numerator
in ζ(e | S, S) increases quadratically with respect to δ(e), while the denominator increases
linearly with respect to δ(e). Hence, given two hyperedges of the same weight w(e) and
the same η, NCut will inherently favor preserving the larger hyperedge and cutting the
smaller hyperedge. Intuitively, the larger hyperedges convey more evidence on the existence
of a cluster than the smaller ones, even if the model is ﬁtted equally well in both cases. For
example, ﬁtting a circle (i.e., a third-order model) to points can be done by clustering either

Multimedia Social Search Based on Hypergraph Learning
237
a 4-uniform hypergraph or an 8-uniform hypergraph. However, 4 points cannot constrain
a circle, especially if they are spatially close. On the contrary, the 8-uniform hypergraph
yields a more accurate ﬁtting. The same applies to all the aforementioned approaches, which
although were designed to deal with high-order relations, they are reduced to standard
pairwise approaches [ABB06]. However, approximating the high-order relations in terms
of pairwise interactions can lead to a substantial information loss. Moreover, the intrinsic
bias toward larger hyperedges also aﬀects the clique averaging [ALZM+05] and the max
projection [OB12].
Another assumption frequently made is that clustering algorithms assign each object to
one class exclusively. Consequently, the clusters are not modeled, but they are obtained as
a by-product of the partition of objects into a predetermined number of classes. That is,
the resulting clusters are by deﬁnition disjoint sets. For example, in graph clustering into 2
classes, the goodness of the partitioning is inversely proportional to the cost of the cut that
separates the vertices, which is a function of the weights of those edges having vertices in
both clusters [PCAS14]. In several applications, overlapping clusters are more appropriate
[HG07]. To address the need for hypergraph soft clustering, the constraints imposed by crisp
partitions are relaxed so that soft boundaries between the clusters emerge.
Co-occurrence relations, such as co-citation and co-purchase relations, typically involve
more than two items. Consequently, they are represented by a hyperedge in a hypergraph.
As has been seen, the hyperedges are usually transformed into cliques of edges by clique
expansion, star expansion, or normalized hypergraph cut. Let us call the just-mentioned
transformations, vertex expansions. Another transformation, called hyperedge expansion has
been proposed in [PF12]. Hyperedge expansion operates on the dual hypergraph, i.e., the
hypergraph having as vertices the hyperedges of the original hypergraph. It is ﬁrst carried
out on the hyperedge level and the learning induced by the hyperedges is projected back to
the vertices through the adjacency matrix of the dual hypergraph. Formally, a multiclass
labeling on the hypergraph H(V, E, w) is deﬁned that associates each vertex v ∈V with
a single label y(v) ∈Y. For a hyperedge e ∈E, y(e) = {y(v) | v ∈e} is the set of labels
associated to e. When |y(e)| > 1, it is said that the hyperedge e is broken or violated by y
[PF12]. Hyperedge expansion ﬁnds a set of hyperedges of minimum weight that separates
the vertices of the hypergraph into two disjoint subsets by minimizing the cost function:
HE = min
y
X
e∈E |y(e)|>1
w(e),
(8.4.47)
i.e., the sum of the weights of broken hyperedges. The cost function (8.4.47) was proposed
a long time ago [Law73] and was revisited recently in [Fuk10]. However instead of dealing
with combinatorial algorithms that minimize (8.4.47), which are eﬃcient only when the
number of classes is small, a relaxation of (8.4.47) to a continuous optimization problem is
described next.
A directed graph ˆG is constructed by inserting two vertices e+ and e−for each hyper-
edge e in the original hypergraph H(V, E, w). The vertices in ˆG are twice as much as the
hyperedges of H. Accordingly, ˆG is associated to the dual hypergraph of H. For a pair of
overlapping hyperedges e1 and e2 in H, two directed edges (e−
1 , e+
2 ) and (e−
2 , e+
1 ) are included
in ˆG with weights w(e1) and w(e2), respectively, where w(·) is the weighting function in H.
The adjacency matrix of ˆG, A ∈R2M×2M, is deﬁned as:
A ˆG =
"
0
A∗W
W
0
#
,
(8.4.48)
where M = |E|, A∗∈RM×M is the adjacency matrix of the dual hypergraph of H (i.e.,

238
Graph-Based Social Media Analysis
the adjacency matrix of hyperedges) with elements:
A∗(i, j) =
(
1
if ei ∩ej ̸= ∅and ei ̸= ej
0
otherwise
(8.4.49)
and W ∈RM×M is the diagonal matrix of the hyperedge weights. Let us sort the rows and
columns of A ˆG in the following order [e−
1 , e−
2 , . . . e−
M, e+
1 , e+
2 , . . . e+
M]. The out-degree matrix
of ˆG is [PF12]:
D ˆG =
" ˆD
0
0
W
#
,
(8.4.50)
where ˆD = diag(1T
M W A∗) is a diagonal matrix of size M × M. Then, the unnormalized
out-degree Laplacian of ˆG is given by:
L ˆG = D ˆG −A ˆG =
"
ˆD
A∗W
−W
W
#
.
(8.4.51)
Let us elaborate a minimum cut in ˆG. That is, to seek for a set S, such that S+S = ˆV, where
ˆV =

e−
i , e+
i | ei ∈E
	
. Let also ˆE =

(e−
i , e+
j ), (e−
j , e+
i ) | ei ∩ej ̸= ∅, E ∋ei ̸= ej ∈E
	
. We
introduce the vector f ∈{
1
√
|S|, 0}2M with elements f(e) =
1
√
|S| and 0 otherwise. Such a
unit ℓ2 norm vector f minimizes the cost of the cut:
HEcut =
X
e1,e2∈ˆV
(e1,e2)∈ˆE
|S| (f(e1) −f(e2)) f(e1),
(8.4.52)
where the last term ensures that the edges from S to S are counted, whenever f(e1) =
1
√
|S|
and f(e2) = 0 [PF12]. If f admits real positive values, then the optimization problem
(8.4.52) is relaxed to the minimization of HEcut with respect to f subject to f T f = 1. The
solution of this optimization problem is [PF12]:
f T  2D ˆG −A ˆG

|
{z
}
D ˆ
G+L ˆ
G
= 2 λf T ,
(8.4.53)
i.e., f is a left eigenvector of the matrix in the left-hand side of (8.4.53), which is closely
related to L ˆG. Both the matrix in the left-hand side of (8.4.53) and L ˆG are non symmetric
matrices. Under certain conditions, it can be shown the eigenvalues of L ˆG are non-negative
real numbers and the left eigenvectors admit real values [PF12]. Suppose that the 2M × 1
left (real) eigenvectors ˆφ1, ˆφ2, . . . , ˆφK are associated to the K smallest eigenvalues of L ˆG. To
map the hyperedge embedding back to the vertices of the hypergraph H, one has to multiply
the N ×M incidence matrix of the hypergraph H with the M ×K matrix
h
ˆφ
−
1 |ˆφ
−
2 | · · · |ˆφ
−
K
i
,
where ˆφ
−
j is the vector formed by retaining the top M elements.
8.5
Ranking on hypergraphs
For both clustering and semi-supervised learning, the key factor is the cut functional
(e.g., (8.4.28) or (8.4.52)). The total variation on a hypergraph is introduced as the Lovasz

Multimedia Social Search Based on Hypergraph Learning
239
extension of the hypergraph cut [HSJR13]. In particular, a family of regularization func-
tionals was proposed, which interpolates between the total variation and the regularization
functionals enforcing smoother functions on the hypergraph, such as the quadratic form
Ω(f) = f T L f, where L is a proper Laplacian. It is shown that there exists a tighter relax-
ation of the normalized hypergraph cut. For both clustering and semi-supervised learning,
convex optimization problems are solved whose core is proximal mapping.
In the following, we shall refer to vertices via their indices.
Deﬁnition 1 [HSJR13] The total variation TVH : RN×1 →R on hypergraph H(V, E, w)
is deﬁned as:
TVH(f) =
X
e∈E
w(e)

max
i∈e fi −min
j∈e fj

=
X
e∈E
w(e) max
e∋i, j∈e |fi −fj|.
(8.5.1)
It is a generalization of the total variation on graphs TVG(f) = 1
2
PN
i=1
PN
j=1 wij |fi −fj|.
For nonnegative hyperedge weights w(e), (8.5.1) is a nonnegative combination of convex
functions, and thus is also a convex function.
Let ∇e : RN×1 →RN×N be the diﬀerence operator for the hyperedge e ∈E with
elements:
[∇e(f)]ij =
(
fi −fj
i ∈e and j ∈e
0
otherwise.
(8.5.2)
Then, the total variation (8.5.1) can be expressed as TVH(f) = P
e∈E w(e)∥∇e(f)∥∞, which
can be seen as inducing a group sparse structure on the gradient level. This interpretation
gives rise to a family of regularization functionals ΩH,p: RN×1 →R for a hypergraph
H(V, E, w) for p ≥1:
ΩH,p(f) =
X
e∈E
w(e)

max
i∈e fi −min
j∈e fj
p
.
(8.5.3)
Accordingly, TVH(f) is equivalent to ΩH,1(f). If H is a graph and p ≥1, ΩH,p(f) reduces
to a Laplacian regularization:
ΩG,p(f) =
N
X
i=1
N
X
j=1
wij |fi −fj|p.
(8.5.4)
It can be shown that:
ΩH,p(1N) =
X
e∈E:
e∩S̸=∅
e∩S̸=∅
w(e) = cut(S, S).
(8.5.5)
8.5.1
Enforcing structural constraints
Let H(V, E, w) denote a hypergraph with set of vertices V and set of hyperedges E to
which a weight function w: E →R is assigned. The vertex set V is frequently made by
concatenating sets of objects of diﬀerent type (e.g., images, users, social groups, geo-tags,
tags). Hereafter, each vertex subset is referred to as object group in order to avoid confusion
with social groups. In general, each object group contributes diﬀerently to the ranking
procedure. The diﬀerent impact of each object group can be taken into account by proper
regularization terms in the ranking procedure, as is detailed next.

240
Graph-Based Social Media Analysis
The vertices and hyperedges form a |V| × |E| incidence matrix with elements H(v, e) =
1, if v ∈e and 0 otherwise. Let Dv denote the vertex degree diagonal matrix of size
|V| × |V|, De be the hyperedge degree diagonal matrix of size |E| × |E|, and W represent
the |E| × |E| diagonal matrix of hyperedge weights. Then, ˜L = I −Θ is the positive semi-
deﬁnite hypergraph Laplacian (8.4.38) [ZHS06]. The elements of Θ deﬁned in (8.4.39),
Θ(u, v), can be interpreted as a measure of relatedness between the vertices u and v. A
real valued ranking vector f ∈R|V| that minimizes Ω(f) =
1
2f T ˜Lf yields a clustering of
the hypergraph, where all vertices with the same value in the ranking vector f are strongly
connected [ABB06]. By including the ℓ2 regularization norm between the ranking vector f
and a query vector y ∈R|V |, a recommendation problem was solved [BTC+10]. That is, the
function to be minimized is expressed as:
˜Ψ(f) = Ω(f) + ϑ ||f −y||2
2,
(8.5.6)
where ϑ is a regularization parameter. The ranking vector f ∗= argmin
f
˜Ψ(f) is [BTC+10]:
f ∗=
ϑ
1 + ϑ

I −
1
1 + ϑΘ
−1
y.
(8.5.7)
A Group Lasso regularizing term is more appropriate than the ℓ2 norm in this kind of
problem [YL06]. In [PK14c], the hypergraph vertices are split into R non-overlapping object
groups (images, users, social groups, geo-tags, tags) and diﬀerent weights γr, r = 1, 2, . . . , R
are assigned to each object group, yielding the following objective function to be minimized:
Ψ(f) = Ω(f) + ϑ
R
X
r=1
q
γr (f −y)T Kr(f −y).
(8.5.8)
In (8.5.8), ϑ is also a regularization parameter and Kr is the |V| × |V| diagonal matrix
with elements equal to 1 for the vertices, which belong to the r-th object group. The latter
minimization problem is expressed as:
f ∗= argmin
f
Ψ(f).
(8.5.9)
Let x = f −y. By introducing the auxiliary variable z = x, the right-hand side of (8.5.9) is
rewritten as:
argmin
x
1
2(x + y)T ˜L(x + y) + ϑ
R
X
r=1
p
γr zT Krz
s.t. z = x.
(8.5.10)
The solution of (8.5.10) is obtained by minimizing the augmented Lagrangian function:
L(x, z, λ) = 1
2(x + y)T ˜L(x + y) + ϑ
R
X
r=1
p
γrzT Krz
+λT (z −x) + µ
2 ∥z −x∥2
2,
(8.5.11)
where λ is the vector of the Lagrange multipliers, which is updated at each iteration and µ
is a parameter regularizing the violation of the constraint x = z. (8.5.11) can be solved by
the Alternating Directions Method [LLS11], as shown in Algorithm 8.5.1.
Solving for xt+1 in line 3 yields:
xt+1 = (˜L + µtI)−1(λt + µtzt −˜Ly).
(8.5.12)

Multimedia Social Search Based on Hypergraph Learning
241
Algorithm 8.5.1: Alternating Directions Method
1: Given xt, zt and λt.
2: Set tolerance ϵ and initialize µ0.
3: xt+1 ←argmin
x
L(x, zt, λt)
4: zt+1 ←argmin
z
L(xt+1, z, λt)
5: if ∥z −x∥2
2 > ϵ then
6:
λt+1 ←λt + µt(zt+1 −xt+1)
7:
µt+1 = min(1.1µt, 106)
8: else
9:
return xt+1, zt+1.
10:
f = xt+1 + y
11: end if
A careful look at (8.5.12) reveals that matrix inversion is not needed at each itera-
tion. Only one eigen-decomposition is needed. Indeed, let Qt = ˜L + µtI. Then, Q−1
t
=
1
µt−µt−1
h
I +
1
µt−µt−1 Qt−1
i−1
. Q0 = ˜L + µ0I is a symmetric matrix. Therefore, it is di-
agonalizable: Q0 = UΛ0UT , where UUT = UT U = I. It can be easily derived that
Q−1
1
= U

(µ1 −µ0)I + Λ0
−1 UT , and in general:
Q−1
t
= U

(µt −µ0)I + Λ0
−1 UT .
(8.5.13)
The minimization problem described in line 4 of Algorithm 8.5.1 is expressed as:
min
z µt
 ϑ
µt
R
X
r=1
√γr
p
zT Krz + 1
2∥z −(xt+1 −1
µt λt)∥2
2

.
(8.5.14)
By applying the soft-thresholding operator [QS10], we obtain:
zj =
ρj
||ρr||2
max

0, ||ρr||2 −ϑµt
1
√γr

,
(8.5.15)
where ρj = xt+1
j
−
1
µt λt
j, r is the object group where the j-th element belongs, and ρr
denotes the segment of ρ corresponding to the r-th object group.
8.5.2
Learning hyperedge weights
In Section 8.5.1, the hyperedges are assigned a ﬁxed weight by the user. Since the
aforementioned weights indicate the importance given to each relationship captured by a
hyperedge, a more suitable approach is to learn the weights by solving the optimization
problem [GWZ+13]:
argmin
f,w
˜Ψ′(f, w) = f T ˜Lf + ϑ1 ||f −y||2
2 + ϑ2||w||2
2
s.t. wT 1 = 1,
(8.5.16)
where w has elements like those in the main diagonal of matrix W. In (8.5.16), ϑ1 and
ϑ2 are positive regularization parameters. The objective function in (8.5.16) includes the
hypergraph smoothing, the discrepancy between the ranking vector f and the query vector

242
Graph-Based Social Media Analysis
y, and the hypergraph weight regularizer. An alternating optimization can be employed to
solve the optimization problem under study. That is, ﬁrst w is kept ﬁxed and we optimize
with respect to f and then the optimal f is ﬁxed and we optimize with respect to w in each
iteration.
When w is kept ﬁxed, the solution of (8.5.16) with respect to f is given by (8.5.7) for
ϑ = ϑ1. Next, we ﬁx f = f ∗and we solve the optimization problem:
argmin
w
f T ˜Lf + ϑ2||w||2
2
s.t. wT 1 = 1
(8.5.17)
for w. Having ignored the contribution of the vertex degree matrix Dv = diag(H w) having
as elements in its main diagonal those of the vector H w, the following solution is obtained
for the i-th weight [GWZ+13]:
wi = 1
|E| −f T ΓD−1
e ΓT f
2|E| ϑ2
+
f T ΓiΓT
i f
2ϑ2 De(i, i), i = 1, 2, . . . , |E|,
(8.5.18)
where Γ = D
−1
2
v
H and Γi is the i-th column of Γ.
An interesting probabilistic interpretation of the just described method [GWZ+13]. At
the optimal f and w, the posterior probability density function given the samples X and
the query vector y is maximized, i.e.:
{f ∗, w∗} = argmin
f,w
p(f, w|X, y).
(8.5.19)
If the data and the weights are conditionally independent and the constant term p(X, y) is
ignored, the conditional joint density p(f, w|X, y) is approximated by:
p(f, w|X, y) = p(y|X, f, w) p(f|X, w) p(w).
(8.5.20)
Let
p(y|X, f, w
=
p(y|X, f) = 1
Z1
exp
 
−||y −f||2
2
1
ϑ1
!
(8.5.21)
p(f|X, w)
=
1
Z2
exp

−f T ˜L f

(8.5.22)
p(w)
=
1
Z3
exp
 
−
||w −
1
|E|1||2
2
1
ϑ2
!
,
(8.5.23)
where Zi, i = 1, 2, 3 is a normalizing constant so that the integral of the associated prob-
ability density function1 is 1 and 1 is a vector of ones of size |E| × 1. The ﬁrst two terms
in (8.5.20) model the two assumptions that the ﬁnal ranking vector should not deviate too
much for the query vector and the conditional density function of the ranking vector should
be smooth on the hypergraph. The term (8.5.23) assigns a Gaussian density function on
the hyperedge weights instead of treating them as ﬁxed parameters.

Multimedia Social Search Based on Hypergraph Learning
243
8.6
Applications
To begin with, two applications employing uniform hypergraphs are described in Sec-
tions 8.6.1 and 8.6.2. The ﬁrst one deals with high-order link analysis and the second one
deals with object recognition. Next, four applications of ranking on arbitrary hypergraphs
are discussed. Section 8.6.3 deals with music recommendation and personalized music tag-
ging. Section 8.6.4 addresses simultaneous image tagging and geo-location prediction. Sec-
tion 8.6.5 is devoted to social image search exploiting joint visual-textual information. Fi-
nally, Section 8.6.6 describes annotation, classiﬁcation, and tourism recommendation driven
by latent semantic analysis. Although the comparisons against the state-of-the-art tech-
niques demonstrate the great potential of the methods described in Sections 8.6.3-8.6.6, the
main utility of the details disclosed is to enable the readers to implement and assess the
merits of these methods.
8.6.1
High-order web link analysis
The hyperlink structure between hypertexts (i.e., web pages) is the simplest case of
explicit, directed links between documents. They mimic the citations between the papers in
bibliometrics . There, the papers can be treated as vertices of a graph. Directed links (i.e.,
edges) exist whenever one paper cites another. Link analysis (e.g., PageRank [BP98], HITS
[Kle99]) decomposes a proper matrix, capturing the hyperlink structure. More precisely, let
A ∈RN×N be the adjacency matrix of the graph with elements:
Aij =
(
1,
if there exists an edge from vertex i to vertex j
0,
otherwise,
(8.6.1)
where N is the number of nodes. Let a(i) and aj denote the i-th row of A and its j-th
column, respectively. If 1 ∈RN is a vector of N ones, the row normalized adjacency matrix
˜A has elements ˜Aij =
1
1T a(i) . PageRank seeks for the eigenvector associated to the dominant
eigenvalue of the Google matrix, that is a convex combination of a stochastic variant of the
row normalized adjacency matrix ˜A and the matrix
1
N 1vT , where v ∈RN
+ is the so-called
personalization or teleportation vector [LM06]. Starting from a neighborhood, HITS seeks
for the dominant eigenvector of either AT A or AAT [LM06]. The former is known as the
authority matrix, while the latter is called the hub matrix. The aforementioned matrices are
closely related to the co-citation and co-reference matrices in bibliometrics [DHZS02].
Undirected links may be also established, capturing similarities between titles, abstracts,
and keywords or joint authorship. Accordingly, graphs with multiple link types (i.e., hyper-
graphs) emerge. For example, by exploiting both citation analysis and the just mentioned
implicit, derived, links one may obtain a full picture of publication impact. Another closely
related example is social network analysis. Social networks frequently include many link
types, capturing friendship relations, organizational relations, geographical ones, and so on.
High-order web link analysis[KBK05, DKK11] refers to the set of techniques used to analyze
such hypergraphs.
As previously mentioned, the hypergraphs are represented by hypermatrices. In partic-
ular, the adjacency hypermatrix can be formed by stacking the adjacency matrix for each
link type. Let us consider a hypergraph having N nodes capturing K link types. The hy-
pergraph can be represented by a 3rd order adjacency hypermatrix A of size N × N × K,
where A(i, j, k) is nonzero if vertex i is connected to vertex j by link type k. Let us denote

244
Graph-Based Social Media Analysis
by A(:, :, k) the k-th frontal slice of the adjacency hypermatrix A, hereafter. For example,
K could be 5 and the slices could be given the following interpretation in the context of
bibliometric data [DKK11]:
Abstract similarity. The ij element of the 1st slice A(i, j, 1) is a measure of the similarity
of abstracts for documents i and j (e.g., the cosine similarity between the abstracts).
Having found the term-abstract matrix T, say by means of the product of term fre-
quency and inverse document frequency, each column of T is normalized to unit norm,
yielding the matrix ˜T whose j-th column is
tj
||tj||2 , where || ||2 denotes the ℓ2 norm of
a vector. Then, A(:, :, 1) = TT T.
Title similarity. The ij element of the 2nd slice A(i, j, 2) is a measure of the similarity
of titles for documents i and j and can be computed in a similar way to abstract
similarity.
Keyword similarity. The ij element of the 3rd slice A(i, j, 3) is a measure of the similarity
between the keywords associated to documents i and j and can be computed in a
similar way to abstract similarity.
Author similarity. Let Q ∈RL×N be the author-document indicator matrix with element
Qli = 1 if author l, l = 1, 2, . . . , L, has authored document i, i = 1, 2, . . . , N and zero
otherwise. Let ˜Q be a properly normalized author-document matrix with elements
˜Qli =
1
√
1T qi . The denominator is simply the square root of the number of authors of
the i-th document. Then, A(:, :, 4) = ˜QT ˜Q.
Citations. The ﬁfth slice captures the citation information, i.e.,
A(i, j, 5) =
(
ξ,
if document i cites document j
0,
otherwise,
(8.6.2)
where ξ is an arbitrary number, e.g., ξ = 2 [KBK05, DKK11].
Figure 8.6.1 shows pictorially the frontal slices of the adjacency hypermatrix A. The just
described choices for the slices of the hypermatrix A are not unique. In any case, each slice
FIGURE 8.6.1: Frontal slices of the adjacency hypermatrix A.

Multimedia Social Search Based on Hypergraph Learning
245
can be treated as the adjacency matrix of a particular graph. All the graphs can be combined
into a hypergraph associated to the just described third-order hypermatrix, since all the
graphs refer to the same set of vertices. Moreover, the slices can be sparsiﬁed by retaining
only the elements that exceed a proper threshold, in order to facilitate computations.
The aforementioned discussion can easily be customized to accommodate any metadata
associated to images uploaded to Flickr or videos to YouTube. This is straightforward for
title, keyword, and author similarity. Other slices can be appended in order to capture user
comments or click-through data.
A straightforward procedure is to decompose the adjacency hypermatrix A of the
hypergraph. Various decompositions can be found in the literature [Lat97, CZPA09,
KB09, LPV13] The Canonical Decomposition (CANDECOMP)/Parallel Factor Analysis
(PARAFAC) or CP for short is a higher order analog of the SVD for matrices. The CP
decomposition generates feature vectors comprising all the linkages simultaneously for each
vertex of the hypergraph. If CP with R factors is applied to A, then A is approximated as:
A ≈
R
X
r=1
λr xr ◦yr ◦zr,
(8.6.3)
where ◦denotes the outer product of vectors. Each term λr xr ◦yr ◦zr is identiﬁed as a
rank-one hypermatrix (i.e., factor), that represents a “community” within the data. The
number of factors R in (8.6.3) is loosely related to the number of the communities in the
data. Figure 8.6.2 demonstrates graphically the CP decomposition (8.6.3). Let X ∈RN×R,
Y ∈RN×R, and Z ∈RK×R be the matrices formed by the column vectors xr, yr, and zr,
r = 1, 2, . . . , R.
The elements of the third-order hypermatrix A ∈RN×N×K can be reordered into the
following three mode-n matrices for n = 1, 2, 3:
A(1)(i, p) = A(i, j, k)
p = j + (k −1) N
A(2)(j, p) = A(i, j, k)
p = i + (k −1) N
(8.6.4)
A(3)(k, p) = A(i, j, k)
p = i + (j −1) N.
To derive the optimal approximation ˆA in (8.6.3), one has to minimize the squared Frobenius
norm of the approximation error ∥A −ˆA∥2
F , where ˆA is the 3rd order hypermatrix in the
right-hand side of (8.6.3). This can be achieved with the Alternating Least Squares(ALS)
algorithm [CC70, Har70, FBH03]. At each iteration, the ALS algorithm solves for one
component matrix, keeping the others ﬁxed. For example, to determine Z, keeping X and
Y ﬁxed, one has to solve for [DKK11]:
Z∗= argmin
Z
∥A(3) −Z (Y ⊙X)T ∥2
F ,
(8.6.5)
FIGURE 8.6.2: CP applied to the adjacency hypermatrix A.

246
Graph-Based Social Media Analysis
Algorithm 8.6.1: CP decomposition using the ALS algorithm.
Input: Hypermatrix A ∈RN×N×K, number of factors R > 0, maximum number of
iterations Titer, and stopping criterion ϵ > 0.
Output: Vector λ ∈RR and matrices X ∈RN×R, Y ∈RN×R, and Z ∈RK×R.
1: Initialize: iteration t = 0,
X = R principal eigenvectors of A(1)AT
(1) and
Y = R principal eigenvectors of A(2)AT
(2)
2: while not converged do
3:
Solve for Z: Z = A(3) (Y ⊙X)
 (YT Y) ∗(XT X)
−1
4:
Normalize the columns of Z to unit ℓ2 norm
5:
Solve for Y: Y = A(2) (Z ⊙X)
 (ZT Z) ∗(XT X)
−1
6:
Normalize the columns of Y to unit ℓ2 norm
7:
Solve for X: X = A(1) (Z ⊙Y)
 (ZT Z) ∗(YT Y)
−1
8:
Store the ℓ2 norms of the column vectors of X into λ
9:
Normalize the columns of X to unit ℓ2 norm
10:
Compute the approximation ˆA = PR
r=1 λr xr ◦yr ◦zr
11:
Check convergence conditions
t ≤Titer and ∥ˆA −A∥2
F < ϵ
12:
t ←t + 1.
13: end while
where λr have been absorbed into the ℓ2 norm of the column vectors of X and ⊙denotes
the Khatri-Rao product deﬁned as:
Y ⊙X = [y1 ⊗x1|y2 ⊗x2| . . . |yR ⊗xR] ,
(8.6.6)
with ⊗referring to the Kronecker product between two vectors. The solution of the least
squares problem (8.6.5) is:
Z = A(3)
h
(Y ⊙X)†iT
= A(3) (Y ⊙X)
 (YT Y) ∗(XT X)
−1 ,
(8.6.7)
where ∗denotes the Hadamard product between two matrices of the same size (i.e., element-
wise product) and † denotes the Moore-Penrose pseudo-inverse of a matrix. Let Ξ be the
square matrix of size R×R inside the big parentheses, i.e., Ξ = (YT Y)∗(XT X). Frequently,
Ξ† is used instead of Ξ−1 in (8.6.7). Assume the eigen-decomposition of Ξ, Ξ = U Σ UT ,
and Γ ≤R be the rank of Ξ. If Γ < R, Ξ is rank-deﬁcient (i.e., a singular matrix). In
this case, Ξ is replaced by its Γ-rank best approximation prior to matrix inversion. The
ALS algorithm for obtaining the CP decomposition is summarized in Algorithm 8.6.1. Let
us comment on the computational complexity of step 3: To obtain the Khatri-Rao product
O(N 2R) multiplications are needed, while matrix inversion inherits the cost of an SVD,
i.e., O(R3). Since R << N, the cost for any Khatri-Rao product is heavier than any matrix
inversion. The cumulative number of multiplications needed in steps 3-11 is:
R(2K + 3)N 2 + 2R(2R + K + 3))N + 3(η1 + η2 + 2)R3 + (2K + 3)R2 + 2(K + 2)R, (8.6.8)
where η1 = 4 and η2 = 22 [GL06]. The storage requirements become more prominent when
large matrices are employed. In the latter case, more sophisticated approaches are needed
[SPF14].

Multimedia Social Search Based on Hypergraph Learning
247
The rank-one factors reveal communities within the data. The largest entries in each
factor (xr, yr, zr) correspond to interlinked entries in the data [DKK11]. That is, top-
ranking vertices in xr are connected to top-ranking vertices in yr with top-ranking link
types in zr. If the top ranked scores in zr refer to link types k = 1, 2, 3, capturing symmetric
relations, then the documents associated to top ranked scores in xr′ and yr′ are expected
to be nearly identical and to form a community [DKK11]. If the top ranked score in zr′
refers to the 5th link type (i.e., citations), which is an asymmetric relation, the following
interpretation is given: the top ranked documents in xr cite the top ranked documents in yr.
The matrices X and Y provide latent representations for each document vertex, which
can be exploited to compute document similarities. Let Λ = diag(λ) be the diagonal matrix,
having the elements of λ in its main diagonal. The matrix Υ = 1
2XΛXT + 1
2YΛYT captures
the similarity between the documents in a way similar to the latent semantic analysis (LSA).
Moreover, having identiﬁed the documents containing a particular term or phrase in either
the title, abstract, or keywords, the centroids gX ∈RR and gY ∈RR are computed by
averaging the associated row vectors of the matrices X and Y. The vector υ = 1
2XgX +
1
2YgY captures the similarities between all documents and the centroids. It is useful to
reveal the documents related to the term under study. The method can be extended to
ﬁnding the most similar papers to those written by a speciﬁc author. It is worth noting that
the results heavily depend on the number of factors R used in the approximation (8.6.3)
[DKK11].
8.6.2
Hypergraph matching for object recognition
Optimization-based approaches to graph matching for object recognition and scene anal-
ysis date back to 80s [BB82, Ch. 11]. Combinatorial or mixed continuous/combinatorial op-
timization techniques for feature matching were exploited in [BBM05, LH05, ZD06]. Feature
matching is strongly related to content-based image retrieval, i.e., how one can establish
matches between pairs or triples of scale-invariant feature transform (SIFT) [Low04] de-
scriptors in two images. Formally, let N1 and N2 be the number of points (i.e., the output
of a Hessian-aﬃne keypoint detector) in image 1 and 2, respectively. Let X ∈{0, 1}N1×N2
denote an assignment matrix, such that xi1i2 equals 1 when point Pi1 of image 1 is matched
to point Pi2 of image 2 and 0 otherwise. Although a point from the ﬁrst image is matched
to exactly one point from the second image, two points from the second image could match
an arbitrary number of points in the ﬁrst image, if the constraint of a unit sum is imposed
to each column, i.e., PN1
i1=1 xi1i2 = 1. The matching between a pair of points (Pi1, Pj1) in
image 1 and a pair of points (Pi2, Pj2) in image 2 is formulated as maximization of a score
function:
score(X) =
N1
X
i1=1
N2
X
i2=1
N1
X
j1=1
N2
X
j2=1
ai1i2j1j2 xi1i2 xj1j2,
(8.6.9)
where ai1i2j1j2 is a potential corresponding to the aforementioned pairs of points. Whenever
there is a perfect match between the pair of points (Pi1, Pj1) and (Pi2, Pj2), xi1i2 and
xj1j2 will be 1 and the score(X) will be increased by a potential, admitting high values.
The graph matching problem (8.6.9) is an integer quadratic programming problem with no
known polynomial-time algorithm [DBKP11]. If the maximization is conﬁned to the set of
matrices such that ||X||F = √N2, i.e., when:
argmax
||X||F =√N2
score(X),
(8.6.10)
the classical Rayleigh quotient problem emerges, because score(X) can be rewritten as
˜xT ˜A˜x, where ˜x ∈RN1N2 is obtained by lexicographically ordering X and ˜A is a N1N2 ×

248
Graph-Based Social Media Analysis
N1N2 symmetric matrix. Search for correspondences in a framework that accommodates
both local geometric invariants and image descriptors was cast as a hypergraph matching
problem using third-order constraints instead of unary or pairwise ones in [DBKP11]. For
a triple of points {i1, j1, k1} from image 1 and a triple of points {i2, j2, k2} from image 2,
(8.6.9) was extended to [DBKP11]:
score(X)
=
N1
X
i1=1
N2
X
i2=1
N1
X
j1=1
N2
X
j2=1
N1
X
k1=1
N2
X
k2=1
ai1i2j1j2k1k2 xi1i2 xj1j2 xk1k2
=
A ×1 ˜x ×2 ˜x ×3 ˜x,
(8.6.11)
where A ∈RN1N2×N1N2×N1N2 is a 3rd-order N1N2 dimensional super-symmetric hyperma-
trix representing the aﬃnity between the triples of points. Since any triple of points deﬁnes
a triangle, one might choose as aﬃnity the cosine similarity between the vectors formed by
the sines of the three angles in the triangle of image 1 and the triangle in image 2 or the
normalized cross-correlation of intensity patterns inside properly normalized triangles in
images 1 and 2. The hypergraph matching problem (8.6.11) was formulated as maximiza-
tion of a multilinear objective function over all feature permutations. The maximization
of (8.6.11) is equivalent to computing the rank-1 approximation of the hypermatrix A. A
power iteration can easily be devised [DBKP11]. In the same framework, one may impose
the constraint for unit norm to the columns of X, merge several hypermatrices At capturing
potentials of diﬀerent orders, or impose unit ℓ1 norms to the columns of X [DBKP11].
In general, the hypergraph matching problem aims at ﬁnding a vertex-to-vertex mapping
between two hypergraphs, such that the overall discrepancy between the corresponding
matching hyperedges is minimized [ZS08]. The number of the vertices in the two hypergraphs
may not be equal. Accordingly, the matching problem aims at ﬁnding an optimal matching
sub-graph.
Formally, let H = (V, E) and H′ = (V′, E′) be two uniform hypergraphs, where the
degree of each hyperedge is κ. A matching between H and H′ is a vertex-to-vertex mapping
ϕ : V →V′. If e = {vi1, vi2, . . . , viκ} ∈E, then the vertex matching induces a hyperedge
matching as well. That is, ϕ(e) = {ϕ(vi1), ϕ(vi2), . . . , ϕ(viκ)} ∈E′.
The input to the hypergraph matching problem is a |V|κ × |V′|κ matrix S hav-
ing as elements the probability of match between pairs of hyperedges, i.e., S(e, e′) =
Prob(ϕ(e) = e′|H, H′). The output of the problem under study is the |V| × |V′| matrix
X having as elements the probability of match between pairs of vertices, i.e., X(v, v′) =
Prob(ϕ(v) = v′|H, H′). For a valid soft matching, X has to be doubly semi-stochastic, i.e.,
X ≥0, X 1 ≤1, and XT 1 ≤1, where 1 denotes the vector of ones of appropriate size.
The relationship between the input matrix S and the output matrix X can be expressed in
a compact manner, if the matches are assumed pairwise conditionally independent. If this
assumption holds, it can be proven that [ZS08]:
S = X ⊗X ⊗· · · ⊗X
|
{z
}
κ times
△= ⊗κX,
(8.6.12)
where ⊗denotes the Kronecker product. Accordingly, to ﬁnd X from S one needs to solve
the optimization problem:
argmin
X
dist(S, ⊗κX)
s.t. X ≥0, X 1 ≤1, and XT 1 ≤1,
(8.6.13)
for a proper distance function, e.g., the relative entropy error measure D(S|| ⊗κ X). The
complexity of the problem is greatly reduced, if S is marginalized, obtaining the matrix Y

Multimedia Social Search Based on Hypergraph Learning
249
of size |V| × |V′| having elements:
Y (v, v′) =
X
e|v∈e
e′|v′∈e′
See′.
(8.6.14)
The optimization problem (8.6.13) is equivalent to:
argmin
X
dist(Y, X)
s.t. X ≥0, X 1 ≤1, and XT 1 ≤1.
(8.6.15)
Although (8.6.15) is convex, it can be further simpliﬁed by setting 1T XT 1, which counts
the number of matches, to a ﬁxed value k and solving for:
X∗(k)
=
argmin
X≥0
dist(Y, X)
s.t. X 1 ≤1, XT 1 ≤1, and 1T XT 1 = k,
(8.6.16)
using a dual block update algorithm [ZS08]. Since X∗(k) is a convex function of k, this
solution is used for minimizing over 0 ≤k ≤min(|V|, |V′|).
8.6.3
Music recommendation and personalized music tagging
Music recommendation and personalized music tagging can be treated as ranking prob-
lems on arbitrary hypergraphs by enforcing structural constraints, as explained in Sec-
tion 8.5.1.
A dataset was created by collecting real data from Last.fm in [TKP13]. In particular,
to create the list of users, the 450 top artists were selected and their top 50 user fans where
concatenated in the user set. This user set was later reduced based on the track and tag
count of each user, yielding a ﬁnal set of 1389 users. To create the track set, the 500 top
played tracks for each user were concatenated in a list, from which 1765 unique tracks
were selected based on their popularity among the users. Finally, tagging relations were
collected for each user and the 1711 most frequent unique tags were retained. By using
Porter’s stemming algorithm [Por80] and calculating next the edit distance [RY98] between
the tag pairs, all synonyms have been removed from the tags vocabulary (i.e., pairs, such as
hardrock and hard rock or 90s and 1990s were merged). The ﬁnal size of all sets is described
in Table 8.6.1.
The vertex set of the hypergraph is deﬁned as V = Vu ∪Vug ∪Vta ∪Vtr. The incidence
matrix of the hypergraph H is formed by concatenating the 5 hyperedge sets indicated as
columns in Table 8.6.2. H has a size of 5867×146885 elements. In the following, the weights
of the hyperedge sets E(1), E(2), and E(4) are set equal to one.
TABLE 8.6.1: Music dataset objects, notations, and counts.
Objects
Notation
Count
Users
Vu
1389
Groups
Vug
10
Tracks
Vtr
1765
Tags
Vta
1711

250
Graph-Based Social Media Analysis
E(1) represents a pairwise friendship relation between users. The incidence matrix of the
hypergraph with vertices Vu and hyperedges E(1) has size 1389 × 13890.
E(2) represents a group of users. It contains all the vertices of the corresponding users,
as well as the vertices associated to the group object. The incidence matrix of the
hypergraph with vertices Vu ∪Vug and hyperedges E(2) has size 1399 × 13890.
E(3) contains a user and a music track, representing a user-track listening relation. The
hyperedge weight w(e(3)
ij ) is deﬁned as the number of times the particular user ui has
listened to the track trj, normalized as follows to eliminate the bias:
w(e(3)
ij )′ =
w(e(3)
ij )
qP|Vtr|
k=1 w(e(3)
ik )
qP|Vu|
l=1 w(e(3)
lj )
(8.6.17)
and further scaled as w(e(3)
ij )∗=
w(e(3)
ij )′
avej(w(e(3)
ij )′), where avej(w(e(3)′
ij )) is the average of
normalized weights of the particular user ui. The incidence matrix of the hypergraph
with vertices Vu ∪Vtr and hyperedges E(3) has size 3154 × 68774.
E(4) contains three vertices, a user, a tag, and a music track, capturing tagging relation.
The incidence matrix of the hypergraph with vertices Vu ∪Vta ∪Vtr and hyperedges
E(4) has size 4865 × 48566 elements.
E(5) captures pairwise audio-track similarities. Such similarities are computed as follows.
First, the 20 mel frequency cepstral coeﬃcients (MFCCs) were employed to encode the
timbral properties of the music signal. The MFCCs were calculated by using frames of
duration 23 ms with a hop size of 11.5 ms and a 42-band ﬁlter bank. Next, a Gaussian
mixture model (GMM) was created for each track with 30 components trained using
the Expectation-Maximization (EM) algorithm, as in [Pam04]. The distance between
two GMMs was computed by using the Earth Movers’ Distance[RTG00], yielding the
audio-track similarities. For a pair of tracks, tri and trj, the aforementioned distance
was set as weight of the associated hyperedge e(5)
ij , w(e(5)
ij ). The weights are then
properly normalized to eliminate any bias: w(e(5)
ij )′ =
w(e(5)
ij )
maxij(w(e(5)
ij )). The incidence
matrix of the hypergraph with vertices Vtr and hyperedges E(5) has size 1765 × 1765.
Frequently, only the K (e.g., K=10) most similar audio tracks to each track (i.e., its
K nearest neighbors) are retained, yielding a sparse incidence matrix, where each row
has only K nonzero entries.
Both music recommendation and personalized image tagging are cast as ranking on hyper-
graphs [TKP13, PK14a].
TABLE 8.6.2: The structure of the hypergraph incidence matrix H and its sub-matrices for
music recommendation and tagging.
E(1)
E(2)
E(3)
E(4)
E(5)
(Vu, E(1))
(Vu, E(2))
(Vu, E(3))
(Vu, E(4))
0
0
(Vug, E(2))
0
0
0
0
0
0
(Vta, E(4))
0
0
0
(Vtr, E(3))
(Vtr, E(4))
(Vtr, E(5))

Multimedia Social Search Based on Hypergraph Learning
251
Let y be the query vector of size 4875 × 1. For recommendation, the query vector y is
initialized by setting the entries corresponding to the target user and all objects (users, user
groups, tags, and tracks), which are associated to this user to 1. Having set the query vector
y, the ranking vector f ∗is given by either (8.5.7) or by solving (8.5.9). In the latter case,
the regularization parameter ϑ, and the weights γr for the group objects should also be set.
The ranking vector f ∗has the same size and structure as y. The values corresponding to
music tracks are used only with the top ranked tracks being recommended for the test user.
For music tagging, the query vector can be initialized by setting the entry corresponding
to the target user u and a certain track to (i.e., y(v) = 1, if v = u) and all other objects
connected to the speciﬁc user (groups, tags, and tracks) to Θ(u, v) deﬁned in (8.4.39), i.e.,
y(v) = Θ(u, v), if v ̸= u. The values corresponding to tags are considered for personalized
music tagging with the top ranked tags for a certain track, which was left out, being proposed
to the user.
8.6.4
Simultaneous image tagging and geo-location prediction
In this section, simultaneous image tagging and geo-location prediction is treated within
an arbitrary hypergraph framework as a ranking problem by enforcing structural con-
straints, as explained in Section 8.5.1.
An image dataset was collected from Flickr that contains both indoor and outdoor
medium sized photos of popular Greek landmarks, various city scenes, and landscapes
[PK14c]. Using FlickrApi (http://www.ﬂickr.com/services/api), a large set of “geo-tagged”
images was downloaded along with valuable information related to them (id, title, owner,
latitude, longitude, tags, image views). The dataset was ﬁltered with respect to image views
(i.e., the times the speciﬁc image has been seen in Flickr) and owner’s uploading statistics. It
was assumed that images with many views normally depict landmarks worth seeing. More-
over, owners (i.e., users) with many uploaded images were considered more trustworthy as
being more active, possessing more friends, and participating in more social groups. Then,
social information related to social groups was crawled and only the groups that had at least
5 users from the data set as members were kept. The speciﬁc cardinalities are summarized
in Table 8.6.3.
In order to form a proper set of tags, all the characters were converted to lower case,
while unreadable symbols and redundant information were removed. Spelling mistakes were
corrected and all morphological variations of the terms were merged to a single form, using
the Edit Distance [RY98]. After having removed the terms used with low frequency, a vocab-
ulary of unique words was created, along with their frequencies. The geo-tags were clustered
into 125 diﬀerent clusters using hierarchical clustering based on pairwise distances according
to the “Haversine formula” (http://www.movable-type.co.uk/scripts/latlong.html).
The vertex set is deﬁned as V = Vim ∪Vu ∪Vug ∪Vgeo ∪Vta. The incidence matrix of
the hypergraph H is formed by concatenating the 6 hyperedge sets indicated as columns
TABLE 8.6.3: Image dataset objects, notations, and counts.
Object
Notation
Count
Images
Vim
1292
Users
Vu
440
User Groups
Vug
1644
Geo-tags
Vgeo
125
Tags
Vta
2366

252
Graph-Based Social Media Analysis
TABLE 8.6.4: The structure of the hypergraph incidence matrix H and its sub-matrices for
image recommendation and geo-location prediction.
E(1)
E(2)
E(3)
E(4)
E(5)
E(6)
0
0
(Vim, E(3))
(Vim, E(4))
(Vim, E(5))
(Vim, E(6))
(Vu, E(1))
(Vu, E(2))
(Vu, E(3))
(Vu, E(4))
(Vu, E(5))
0
0
(Vug, E(2))
0
0
0
0
0
0
0
(Vgeo, E(4))
0
0
0
0
0
0
(Vta, E(5))
0
in Table 8.6.4. H has a size of 5867 × 30924 elements. In the following, the weights of the
hyperedge sets E(1)–E(5) are set equal to one.
E(1) represents a pairwise friendship relation between users. The incidence matrix of the
hypergraph with vertices Vu and hyperedges E(1) has size 440 × 2276.
E(2) represents a user group. It contains all the vertices of the corresponding users as well
as the ones corresponding to the user group. The incidence matrix of the hypergraph
with vertices Vu ∪Vug and hyperedges E(2) has size 2084 × 1644.
E(3) contains a user and an uploaded image, representing a user-image possession relation.
Each image has only one owner. The incidence matrix of the hypergraph with vertices
Vu ∪Vim and hyperedges E(3) has size 1732 × 1292.
E(4) captures a geo-location relation. This hyperedge set contains triplets of im, u and geo.
The incidence matrix of the hypergraph with vertices Vim ∪Vu ∪Vgeo and hyperedges
E(4) has size 1857 × 125 elements.
E(5) contains triplets of images, users, and tags. Each hyperedge represents a tagging re-
lation. The incidence matrix of the hypergraph with vertices Vim ∪Vu ∪Vta and
hyperedges E(5) has size 4098 × 19127 elements.
E(6) contains pairs of vertices, which represent two images. The weight w(e(6)
ij ) is set as
the normalized similarity between images i and j, i.e., w(e(6)
ij )′ =
w(e(6)
ij )
maxij(w(e(6)
ij )). Both
global and local features were used. Firstly, the 100 nearest neighbors to each image
were identiﬁed using the GIST descriptors [OT06] and they were reduced to the 5
most similar images to the reference image by using SIFT descriptors [Low04]. The
incidence matrix of the hypergraph with vertices Vim and hyperedges E(6) has size
1292 × 6460.
Both image tagging and geo-location prediction are cast as ranking on hypergraphs. That
is, we seek for f that minimizes either (8.5.6) or (8.5.9). The query vector y is initialized
by setting the entry corresponding to the test image im and its owner u to 1. The entries
of the query vector corresponding to the tags ta are set equal to Θ(im, ta) deﬁned in
(8.4.39). The entries of the query vector corresponding to ug and geo are set equal to
Θ(u, ug) and Θ(u, geo), respectively. The query vector y has a length of 5867 elements.
Having set the query vector y, the ranking vector f ∗is given by either (8.5.7) or by solving
(8.5.9). In the latter case, the regularization parameter ϑ, and the weights γr for the group
objects should also be set. The ranking vector f ∗has the same size and structure as y. The
values corresponding to tags are used for image tagging with the top ranked tags being
recommended for the test image. The values corresponding to geo are used for geo-location

Multimedia Social Search Based on Hypergraph Learning
253
prediction with only the 3 top ranked geo-locations (i.e., geo-clusters) being recommended
for the test image.
For evaluation purposes, a test set containing the 25% of the tags and a training set
containing the remaining 75% were deﬁned [PK14c]. During testing, the tags contained in
the test set were not included in the training procedure. That is, the associated elements
to the test image are set equal to zero in Θ and y. The relations between test images im
and geo-locations (i.e., geo-clusters geo) are set also to 0.
The averaged Recall-Precision and F1 measure were used as ﬁgures of merit. Precision
is deﬁned as the number of correctly recommended tags divided by the number of all rec-
ommended tags. Recall is deﬁned as the number of correctly recommended tags divided
by the number of all tags the user has actually set. The F1 measure is the weighted har-
monic mean of precision and recall, which measures the eﬀectiveness of recommendation
when treating precision and recall as equally important. The ranking obtained by (8.5.7) is
referred to as Image Tagging on Hypergraph (ITH), while that obtained by (8.5.9) is coined
as Query Group Sparse Optimization (QGSO). The geo-location prediction is referred to
as GPR. The results of the (ITH) are demonstrated in Figure 8.6.3, in which the aver-
aged Recall-Precision curves are plotted by averaging the Recall-Precision curves over 1186
images with at least 4 tags. To calculate the recall and precision, the 15 top ranked tags
are recommended to any test image. By enforcing group sparsity in the ranking problem,
the performance is improved signiﬁcantly, as shown in Figure 8.6.3. The weights for the 5
diﬀerent object groups (images, users, user groups, geo-tags, and tags) were set to 0.9, 0.9,
0.6, 0.2, and 0.2, respectively. This choice was made empirically. The typical values of µ0,
ϵ, and ϑ are 10−6, 10−8, and 2, respectively.
For the GPR, only the 3 top ranked elements are taken from the part of the f vector as-
sociated to the geo-locations. In Figure 8.6.4, the results for the GPR are presented. These
results are further compared with those obtained by exploiting geographical information
deduced from the tags. Greek geo-names were collected from the GeoNames geographi-
cal database (http://www.geo-names.org) along with their geo-coordinates. A geo-location
prediction was made for each image having tags matching the geo-names. The distance
between the image geo-coordinates, treated as ground truth, and the ones associated to
FIGURE 8.6.3: Averaged recall-precision curves for the ITH and the QGSO.

254
Graph-Based Social Media Analysis
FIGURE 8.6.4: Geo-location prediction rates (in %) for the compared methods. (See color
insert.)
the geo-name was computed by using the “Haversine formula.” Predictions whose distances
fall below a threshold of 500 m were considered as correct. Let us call the just described
naive approach GNPR. Figure 8.6.4 depicts the geo-location prediction rates achieved by
the ITH, the QGSO, and the GNPR.
8.6.5
Social image search exploiting joint visual-textual information
A method for social image search exploiting joint visual-textual information was pro-
posed in [PK14d]. It is depicted in Figure 8.6.5.
The term frequency-inverse document frequency (TF-IDF) [SWY75] is a numerical
statistic, quantifying the importance of a term in a document. It is the product of two
statistics, namely the term frequency and the inverse document frequency. The term fre-
quency weighs the terms more heavily which occur often in a speciﬁc document. On the
other hand, the inverse document frequency down-weighs the terms, which tend to appear
many times in several documents in the corpus. This way terms that are more common
FIGURE 8.6.5: Description of the proposed social image search approach.

Multimedia Social Search Based on Hypergraph Learning
255
than others are handled eﬀectively. By doing so, the terms that are truly representative of
a document are given higher weights.
Let each social image vi ∈Vim denote a vertex in the hypergraph with N = |Vim|. Each
visual-textual term generates a hyperedge. Regarding the visual image content, SIFT is
employed and SIFT descriptors are extracted from any image g. K-means is applied to the
SIFT descriptors of any image vi in order to quantize them to a predeﬁned number (e.g.,
200) of clusters represented by their mean vectors as codevectors. Accordingly, vi ∈Vim is
represented by the concatenation of the codevectors instead of the concatenation of SIFT
descriptors. K-means is applied to the set of the aforementioned quantized representations
in order to create the visual word vocabulary. The indices of the resulting codevectors are
treated as visual words zs ∈Zs = {zs1, zs2, . . . , zsR}, where R is the size of visual word
vocabulary. Let Bs ∈RN×R be the matrix having as elements the TF-IDF measurements,
Bs(i, j) = ϕij log2
N
Nj , where ϕij is the frequency of visual term j in image i, Nj is the
number of images that visual term j appears in, and N is the total number of images.
The textual information provided by the tags is captured by a bag-of-words represen-
tation as well. In order to form a proper text vocabulary, all characters are converted to
lower case, unreadable symbols are removed and redundant information is eliminated. Next,
a vocabulary of unique terms is generated along with their frequencies. Then, only the Q
terms with the highest frequency are kept. The tags assigned to a social image are treated as
a document annotating this image. Let zt ∈Zt = {zt1, zt2, . . . , ztQ} be the text vocabulary
and Bt ∈RN×Q be the document-term matrix. Any social image vi ∈Vim is represented
by a vector of size Q, having as elements the TF-IDF measurements obtained by taking
into account the text information as well.
Here, the binary incidence matrix H used in [GWZ+13] is replaced by the fuzzy incidence
matrix H =
h
˜Bs | ˜Bt
i
of size N × M, yielding a fuzzy hypergraph model, where M =
R + Q. The matrices ˜Bs and ˜Bt are obtained by a min-max normalization of Bs and Bt,
respectively, so that their elements admit values in [0, 1]. As can be seen, the incidence
matrix captures both the visual and the textual information, which is also inherited by
the hypergraph Laplacian and the matrix Θ (8.4.39), appearing in (8.5.7) as described in
Section 8.5.1. To assess the impact of the fuzzy hypergraph incidence matrix the diagonal
matrix W containing the hyperedge weights is set to I i.e., w(e) = 1, ∀e ∈E.
A query can be based on either tags or images. In the case of an image-based query, the
query vector y is initialized by setting the entry corresponding to the query image to 1. In
the case of a tag-based query, a simple tag-based search method is employed and the K top
images are returned from all the images that include the query tag in their corresponding
set of tags. Let Γ = {γ1, γ2, . . . , γK} ⊂Vim be the image set associated to this search. The
query vector y is initialized as follows:
y(v) =
(
1,
if v ∈Γ
0,
otherwise.
(8.6.18)
The ranking vector f ∗is derived by solving (8.5.7), as detailed in Section 8.5.1. It has the
same size and structure as y.
The averaged Recall-Precision and the F1 measure are used as ﬁgures of merit as in
Section 8.6.4. Let us denote the proposed method as fuzzy hypergraph learning (FHL) and
the one proposed in [GWZ+13] as HG-WE. The HG-WE was implemented by following
precisely the details in [GWZ+13].
In order to evaluate the proposed method, a dataset of 3291 images, depicting 11 popular
Greek sites (the old city of Rhodes, Santorini, the White Tower at Thessaloniki, Parthenon,
Delphi, Meteora, the ancient Olympia, Sounio, Mycenae, the Greek Parliament, and

256
Graph-Based Social Media Analysis
FIGURE 8.6.6: Averaged Recall-Precision curves for image-based queries.
FIGURE 8.6.7: Averaged F1 measure at several ranking positions for tag-based queries.
Epidaurus) was collected from Flickr. These Greek sites were used as query tags in the
evaluation procedure. Both textual and visual vocabularies were derived and the typical
values of R and Q were set to 3000 and 2000, respectively. Next, the dataset was further
enriched by including 4986 unseen test images collected also from Flickr. Experiments were
conducted for both image-based queries (K = 1) and tag-based queries (K = 100).
The FHL outperforms the HG-WE for both image-based and tag-based search, as shown
in Figures 8.6.6 and 8.6.7, respectively. As is demonstrated in Figure 8.6.6, a precision rate of
77% is achieved for 1% recall for image-based queries. For tag-based queries, the maximum
average F1 measure equals 0.7425. It is obtained at the ranking position 704. The complete
curve of the averaged F1 measure per ranking position is displayed in Figure 8.6.7. It is
worth mentioning that the FHL is much less computationally expensive than the HG-WE,
as only one least squares minimization problem should be solved to obtain f ∗in (8.5.7).
8.6.6
Annotation, classiﬁcation, and tourism recommendation driven
by probabilistic latent semantic analysis
In this section, we deal with image annotation, classiﬁcation, and tourism recommen-
dation [PK14b]. To make the discussion speciﬁc, we refer to a dataset of 50000 images

Multimedia Social Search Based on Hypergraph Learning
257
related to Greek sites that was collected from Flickr. The geo-tags (GPS coordinates) of
these images were clustered into 4660 clusters by means of hierarchical clustering applied to
distances computed using the “Haversine formula.” From these geo-clusters, only the 2000
most dense were considered as tourist places of interest (POIs), containing 45316 images.
For each geo-cluster, a document was created by concatenating the text information (e.g.,
title, tags) of all the constituent images. Next, text information related to 150000 images
was crawled in order to properly capture the context of the tourism application. All char-
acters were converted to lower case. Unreadable symbols and redundant information were
removed. Terms with frequency less than 100 were eliminated, yielding a vocabulary of 1901
terms.
Probabilistic latent semantic analysis (PLSA) performs a probabilistic mixture decom-
position, which associates an unobserved class variable to co-occurrences of terms and doc-
uments. By applying the PLSA to a term-document matrix, the relations between the terms
and the documents are captured by the probability distribution between the documents and
the generated topics as well as between the topics and the terms. The PLSA models each
term in a document as a sample from a mixture model [Hof99, BK14].
Let ta ∈Ta = {ta1, ta2, . . . , tak} be a vocabulary term and d ∈D = {d1, d2, . . . , dm}
denote a document. The joint probability model is deﬁned by the mixture:
P(ta, d)
=
P(d)P(ta|d)
(8.6.19)
P(ta|d)
=
X
za∈Za
P(ta|za) P(za|d),
where za ∈Za = {za1, za2, . . . , zan} is an unobserved class variable representing the topics.
As it is indicated in (8.6.19), the document speciﬁc term distribution P(ta|d) is a convex
combination of the n topic dependent distributions P(ta|za). The annotation procedure is
performed as follows:
1. PLSA is applied to a term-document matrix Ba ∈Rk×m. Here, the documents are
formed by concatenating any terms in the tags or the title of the images that belong
to a geo-cluster. Any document d ∈D is represented by a vector of size k, having as
elements the frequency of occurrence of each term in d.
2. For each document to be annotated, the most related topic is chosen, that with the
highest probability, i.e., z∗
a = argmax
za∈Za
P(za|d).
3. The k′ << k most related terms to z∗
a are identiﬁed by sorting P(ta|z∗
a) in decreasing
order of magnitude.
In particular, the term document matrix Ba is of size 1901 × 2000. Among the most
descriptive terms of a document, those providing geographical information are identiﬁed
using geogazetteers. Thus, a complete annotation model is built, which provides geographic
information in addition to semantic information.
The semantic annotation is complemented by visual annotation based on scene classiﬁ-
cation. SIFT and GIST descriptors are extracted from any image. Diﬀerent visual classes
c ∈C = {c1, c2, . . . , cp} have been deﬁned, capturing the diﬀerent themes pertaining the
image dataset. The objective is to propagate the class label along with the associated tags
to each image as visual annotation. To construct a proper visual word vocabulary, a small
image subset G = {g1, g2, . . . , gν}, made of images without occlusion or unwanted artifacts,
is manually extracted and annotated using the p class labels. K means is applied to the

258
Graph-Based Social Media Analysis
SIFT descriptors of any image in the controlled dataset G in order to quantize the de-
scriptors to a predeﬁned number (e.g., 200) of cluster mean vectors as codevectors. Any
image g ∈G is represented by the concatenation of the codevectors forming the vector
˜g. K-means is applied to the set of the aforementioned vectors ˜g in order to deﬁne the
visual word vocabulary. The indices of the resulting codevectors are treated as visual words
tv ∈Tv = {tv1, tv2, . . . , tvι}, where ι is the size of visual word vocabulary. Let Bv ∈Rι×ν be
the visual word-image matrix, having as columns the image representations built by mea-
suring the frequency of visual words the reduced representations are quantized into. Similar
to [BZM06], the PLSA is applied to Bv in order to calculate the conditional distributions
P(tv|zv) and P(zv|g), where zv ∈Zv = {zv1, zv2, . . . , zvl} are the visual latent topics. Hav-
ing learned the aforementioned conditional distributions from G, any test image gtest is
represented by the conditional distribution P(zv|gtest), obtained by running the M step of
the EM algorithm for P(zv|gtest) until convergence, keeping P(tv|zv) ﬁxed to those learned
during the training. Next, the ιG nearest neighbors of the GIST descriptor extracted from
any test image gtest are identiﬁed, using the K-nearest neighbor classiﬁer (KNN), which
employs the Euclidean distances between gtest and the GIST descriptor of any image in
the controlled subset g ∈G. Let GNN(gtest) be the set of nearest neighbors. GNN(gtest)
is further narrowed to ιGR << ιG training images by sorting the χ2 distances between
P(zv|gtest) and P(zv|g), retaining the images associated to the ιGR smallest distances. Let
˜GNN(gtest) be the resulting narrow set. Finally, the test image is assigned to the visual
class c being in majority within the narrow set ˜GNN(gtest).
A hypergraph is created to capture the multi-link relations among the vocabulary terms
ta, the geo-clusters d, and the topics za. The incidence matrix of the hypergraph H has
size 4251 × 6000 elements. It is formed by concatenating 2000 documents associated to
the geo-clusters, 350 topics za, and 1901 vocabulary terms ta. The vertex set is deﬁned as
V = D ∪Za ∪Ta.
1. For each document dj associated to a geo-cluster, a hyperedge e1 ∈E1 is inserted,
containing 1 in the j-th entry of D, 1 for the most related topic z∗
a ∈Za to dj, and 30
1s for the 30 most descriptive terms ta ∈Ta for z∗
a. The weight for this hyperedge is
w(e1) = P(z∗
a|dj).
2. To capture the geographical proximity, hyperedges e2 ∈E2 are created. For each dj
corresponding to a speciﬁc geo-cluster, one hyperedge e2 is inserted. It contains 1 to
the j-th entry of D associated to dj and 1 to the entries corresponding to geo-clusters
being at a geographical distance less than 150 km. The weight for this hyperedge is
set to 1.
3. In order to capture the visual similarity of the geo-clusters, the mean value of the GIST
descriptors of all the images belonging in a geo-cluster is computed as a codevector.
For each dj, one hyperedge e3 is inserted, having 1 to the j-th entry associated to
dj and 1 to the 10 nearest neighbor geo-clusters, identiﬁed by applying KNN to the
aforementioned codevectors. The hyperedge weight is set to 1.
The structure of the hypergraph incidence matrix is summarized in Table 8.6.5.
Let Θ be deﬁned as in (8.4.39) that can be found in Section 8.4. If d′
j is the geo-cluster
where the test image gtest belongs to with respect to its geo-tag, the query vector y ∈R|V |
has elements:
y(v) =
(
1,
if v = d′
j
Θ(d′
j, v),
otherwise,
(8.6.20)
where Θ(d′
j, v) is treated as a measure of relatedness between the vertices of the hypergraph.

Multimedia Social Search Based on Hypergraph Learning
259
TABLE 8.6.5: The hypergraph incidence matrix H for place of interest recommendation.
E1
E2
E3
D
(D, E1)
(D, E2)
(D, E3)
Za
(Za, E1)
0
0
Ta
(Ta, E1)
0
0
The best ranking vector f ∗∈R|V | is obtained by (8.5.7). The values corresponding to
the ﬁrst 2000 entries associated to geo-cluster documents are used as rankings for tourist
destination recommendation. The top ranked geo-cluster documents are recommended as
tourist POIs to the user, who has imported the test image gtest.
For evaluation purposes, a test set containing 205 images was randomly chosen and
excluded from the training set along with any text associated to these images. The PLSA
performance in semantic image annotation has been compared to that of the latent Dirichlet
allocation (LDA) [BNJ03] and the term frequency-inverse document frequency (TF-IDF)
[SWY75]. The average recall-precision curve is used as a ﬁgure of merit. As is shown in
Figure 8.6.8, PLSA outperforms both the LDA and the TF-IDF. An average precision of
90% at 10% recall is reported, using the PLSA. It is worth noting, that the PLSA is much
simpler than the LDA.
For visual image classiﬁcation, the same test set was used. Each test image was assigned
to one of 13 representative classes manually in order to create the ground truth. Visual
classiﬁcation accuracy is shown in Figure 8.6.9, when only the GIST descriptors were used
and when both the SIFT and the GIST descriptors were employed. Better results were
obtained by using both descriptors. Across the 205 test set images, the average accuracy of
content-based image classiﬁcation over 13 classes is 80%.
Two additional experiments were conducted to assess tourist POI recommendation.
First, only hyperedges e1 ∈E1 were taken into account in hypergraph creation. Second, all
the hyperedges were considered. In order to form the ground truth, relations were established
manually among the geo-clusters, taking into account the distance, common geographical
FIGURE 8.6.8: Recall-Precision curves for semantic image annotation by means of the
PLSA, the LDA, and the TF-IDF.

260
Graph-Based Social Media Analysis
FIGURE 8.6.9: Accuracy results of the visual image classiﬁcation.
FIGURE 8.6.10: Recall-precision curves for tourist POI recommendation.
entities (e.g., mainland, island) and leisure activities. For this, various tourist related web
sources were exploited, such as Trip Advisor (http://www.tripadvisor.com.gr/) and Travel
Muse (http://www.travelmuse.com/). The associated recall-precision curves are plotted in
Figure 8.6.10. As is clearly indicated, the results are increased when all the three types
of hyperedges are considered, including the visual similarity between the geo-clusters. An
average precision of 90% and 82% is reported at 1% and 10% recall, respectively.

Multimedia Social Search Based on Hypergraph Learning
261
8.7
Big data: Randomized methods for matrix/hypermatrix de-
compositions
From the applications described in Section 8.6, it has become evident that the nor-
malized hypergraph Laplacian matrix or the adjacency hypermatrix of the hypergraph can
admit stupendously big sizes. Classical linear algebra algorithms, such as the algorithms for
eigen-analysis, SVD, or hypermatrix factorizations are not always adapted to deal with the
aforementioned large-scale problems [HMT11]. Furthermore, one has to cope with missing
or inaccurate data that frequently arise from increasing matrix sizes. The noise in the data
and the propagation of rounding errors can become increasingly problematic. Nowadays,
data transfer is a crucial factor. Indeed, communication becomes a bottleneck, because of
the slow communication speed between diﬀerent layers in memory hierarchy, latency in
hard drives, inter-processor communications. Algorithms requiring few passes over the data
have been proven much faster in practice despite the more ﬂoating point operations that
are needed. Another practical necessity is the adaptation of algorithms to novel computer
processor architectures, such as graphics processing units.
Randomized algorithms provide a powerful tool for constructing approximate matrix
factorizations. They are simple, often faster, and perhaps surprisingly more robust than the
standard deterministic algorithms. Frequently, they produce factorizations that are accurate
to any speciﬁed tolerance above machine precision. Such algorithms are an example of the
large-scale applicable global operations.
Standard deterministic matrix decompositions include the pivoted QR factorization,
the eigenvalue decomposition, and the SVD [GL06]. The computational cost of the full
QR factorization or the full SVD of an N × M matrix to double-precision accuracy is
O(N M min{N, M}) ﬂops. Truncated versions of these factorizations are often used to
obtain a low-rank approximation of a given matrix A ∈RN×M of rank k, i.e.:
A ≈B C,
(8.7.1)
where B ∈RN×k and C ∈Rk×M. When k ≪N, the factorization (8.7.1) allows the matrix
to be stored inexpensively and to be multiplied rapidly with vectors or other matrices.
Furthermore, the factorization (8.7.1) can be used for data interpretation or for solving
least squares problems.
The most straightforward technique to obtain (8.7.1) is to compute the full SVD and to
truncate it afterwards. However, one should know the rank k. There are two alternatives to
estimate the rank: (i) to compute a partial QR decomposition; (ii) to apply a strong rank-
revealing QR factorization. The classical algorithm to compute a partial QR decomposition
is the Businger-Golub algorithm, which performs successive orthogonalization with pivoting
on the columns of the matrix. The procedure stops after l steps, when the Frobenius norm
of the remaining columns is less than a computational tolerance ϵ, i.e.:
A = Q R + E,
(8.7.2)
where Q ∈RN×l is a column orthonormal matrix, R ∈Rl×M is a weakly upper-triangular
matrix, and E ∈RN×M satisﬁes ||E||F ≤ϵ. The computational cost is O(N M l). The
number of steps l overestimates the rank k. The Gu-Eisenstat algorithm yields a strong-rank
revealing QR decomposition (8.7.2) at a computational cost O(kNM), where ℓ2 subordinate
matrix norm ||E||2 = maxx̸=0

||E x||2
||x||2

satisﬁes ||E||2 ≤
p
1 + 4 k (M −k) σk+1, where
σk+1 is the (k +1)-th largest singular value of E that constitutes the minimal error possible
in a rank-k approximation.

262
Graph-Based Social Media Analysis
To compute the low-rank approximation of A (8.7.1), the ﬁrst stage deals with the
construction of a low-dimensional subspace that captures the action of A. Formally, an
approximate basis of the range of A is computed. That is, a column orthonormal matrix
Q ∈RN×k (i.e., QT Q = I) is sought such that:
A ≈Q QT A.
(8.7.3)
The basis matrix Q should contain as few columns as possible (i.e., k ≤M). But the
primary objective is to yield an accurate approximation of A. The second stage aims at
restricting A to the subspace spanned by the columns of Q by constructing the small matrix
of size k × M B = QT A. Let B = ˜U Σ VT denote the SVD of B. To compute the column
orthonormal matrices U and V as well as a nonnegative diagonal matrix Σ, such that
A ≈U Σ VT , simply set U = Q ˜U, because A ≈Q B. When Q has few columns, this
procedure is eﬃcient, because the small matrix B can easily be constructed and the SVD
of B can be computed fast. In practice, the explicit construction of B can be avoided. It is
not even necessary to revisit A by means of the so-called single-pass algorithms [HMT11].
Similar manipulations yield eﬃcient algorithms for other standard factorizations, such as
the pivoted QR and the eigen-decomposition.
Random sampling methods help to execute eﬃciently the ﬁrst stage. That is, given
A ∈RN×M of rank k known in advance and an oversampling parameter p, a column
orthonormal matrix Q ∈RN×l is sought whose range approximates the range of A, where
l = k + p. This is achieved in rough terms by means of Algorithm 8.7.1.
Algorithm 8.7.1: Randomized range ﬁnder for the ﬁrst stage.
Input: Matrix A ∈RN×M, integer l.
Output: Column orthonormal matrix Q ∈RN×l whose range approximates the range of A.
1: Draw an M ×l random matrix Ωwhose elements are independent identically distributed
Gaussian random variables with zero mean and unit variance.
2: Form the N × l matrix Y = A Ω.
3: Construct an N × l matrix Q whose columns form an orthonormal basis for the range
of Y, e.g., by means of the QR factorization of Y = Q R.
The quality of the random number generator in Step 1 does not aﬀect the quality of Al-
gorithm 8.7.1. The structure of matrix A (i.e., whether it is sparse or structured) determines
the speed of matrix-vector products in the Step 2 of Algorithm 8.7.1, which constitutes its
computational bottleneck. The most important implementation issue is the basis calculation
in Step 3. The columns of the sample matrix Y are almost linearly dependent. This calls for
using stable methods to perform the orthonormalization. For example, the Gram-Schmidt
procedure, augmented with the double orthogonalization [Bj¨o94] has been proven reliable
in practice. The oversampling parameter p, which measures the discrepancy between l and
the actual rank k depends on three factors:
The matrix dimensions. Very large matrices may require more oversampling.
The singular spectrum. The more rapid the decay of the singular values, the less over-
sampling is needed.
The random test matrix. Gaussian matrices with very little oversampling (e.g., p=5
or p=10) are found suﬃcient in practice. The structured random matrices that will
be discussed next yield computational gains in certain settings at the expense of
substantial oversampling.

Multimedia Social Search Based on Hypergraph Learning
263
The basis in Step 3 of Algorithm 8.7.1 can be generated incrementally, starting with an
empty basis matrix Q(0), as follows [HMT11]:
1: for i = 1, 2, . . . do
2:
Draw an M × 1 Gaussian random vector ω(i) and set y(i) = A ω(i).
3:
Compute ˜q(i) =
 I −Q(i−1)(Q(i−1))T 
y(i).
4:
Normalize q(i) =
˜q(i)
||˜q(i)||2 , and form Q(i) =

Q(i−1)|q(i)
.
5: end for
For matrices whose singular values decay slowly, Algorithm 8.7.2 has been proven more
eﬃcient than Algorithm 8.7.1 [HMT11].
Algorithm 8.7.2: Randomized subspace iteration for the ﬁrst stage.
Input: Matrix A ∈RN×M, integers l and q.
Output: Column orthonormal matrix Q ∈RN×l whose range approximates the range of A.
1: Draw an M ×l random matrix Ωwhose elements are independent identically distributed
Gaussian random variables with zero mean and unit variance.
2: Form Y0 = A Ωand compute its QR factorization Y0 = Q0 R0.
3: for j = 1, 2, . . . , q do
4:
Form ˜Yj = AT Qj−1 and compute its QR factorization ˜Yj = ˜Qj ˜Rj.
5:
Form Yj = A ˜Qj and compute its QR factorization Yj = Qj Rj.
6: end for
7: Q = Qq.
For a dense matrix A ∈RN×M, it is possible to obtain its approximate rank-l factor-
ization can be computed in roughly O(N M log l) ﬂops, in contrast to the asymptotic cost
O(N M l) required by the earlier methods. The key idea of the fast technique is to use a
structured random matrix that allows us to compute the product AΩin O(N M log l) ﬂops
by employing a subsampled random Fourier transform(SRFT), yielding the M × l matrix:
Ω= M
l D F R,
(8.7.4)
where D is an M × M diagonal matrix, whose elements are independent random variables
uniformly distributed on the complex unit circle, F is the M × M unitary discrete Fourier
transform, whose elements are fpq =
1
√
M exp(−2 π (p−1) (q−1)
M
), and R is an M × l matrix
that samples l coordinates from M uniformly at random, i.e., its l columns are drawn
randomly without replacement from the columns of the M × M identity matrix. If Ωis
given by (8.7.4), the product Y = AΩcan be computed in O(N M l) ﬂops via a subsampled
Fast Fourier Transform [WLRT08].
In this chapter, we are interested in the eigenvalue decomposition of the normalized
hypergraph Laplacian ˜L ∈RN×N, which is a symmetric matrix. Moreover, the normalized
hypergraph Laplacian is a positive semideﬁnite. Accordingly, the discussion for the second
stage will be focused to the case of a positive semideﬁnite matrix A = ˜L. For positive
semideﬁnite matrices, the Nystr¨om method can be used to improve the quality of standard
factorizations. The Nystr¨om method builds a more sophisticated rank-k approximation, i.e.:
A
≈
(A Q)
 QT A Q
−1 (A Q)T
=
h
(A Q)
 QT A Q
−1
2 i h
(A Q)
 QT A Q
−1
2 iT
=
G GT ,
(8.7.5)

264
Graph-Based Social Media Analysis
where G is an approximate Cholesky factor of A with size N × k. To compute the factor
G numerically, ﬁrst form the matrices B1 = A Q and B2 = QT B1. Then, the positive
semideﬁnite matrix B2 is decomposed into its Cholesky factors, i.e., B2 = CT C. It can
easily be veriﬁed that:
B2
=
QT A Q
CT C
=
QT  G GT 
Q
=
 QT G
  GT Q

.
(8.7.6)
That is, C = GT Q. By pre-multiplying both sides of the previous equation by G, we
obtain:
G C = G GT
| {z }
A
Q = A Q
|{z}
B1
.
(8.7.7)
Accordingly, solve for G the linear system G C = B1, using a triangular solve, since C is
a triangular matrix. The second stage is summarized in Algorithm 8.7.3.
Algorithm 8.7.3: Eigen-decomposition via the Nystr¨om method for the second stage.
Input: Positive semideﬁnite matrix A ∈RN×N and a basis Q ∈RN×l satisfying (8.7.3).
Output: Approximate eigen-decomposition A ≈U Λ UT , where U is an orthonormal
matrix and Λ is a nonnegative diagonal matrix.
1: Form the matrices B1 = A Q and B2 = QT B1.
2: Perform a Cholesky factorization B2 = CT C.
3: Solve for G the linear system G C = B1, using a triangular solve.
4: Compute an eigen-decomposition of G = U Λ UT .
By merging Algorithms 8.7.1 and 8.7.3, the complete Algorithm 8.7.4 is obtained. For
Algorithm 8.7.4: Single pass algorithm.
Input: Positive semideﬁnite matrix A ∈RN×N.
Output: Approximate eigen-decomposition A ≈U Λ UT , where U is an N × l column
orthonormal matrix and Λ is an l × l nonnegative diagonal matrix.
1: Draw an N × l random matrix Ω.
2: Form the N × l matrix Y = A Ω.
3: Find an N × l matrix Q, such that Y ≈Q QT Y.
4: Solve for an l × l matrix G the linear system G
 QT Ω

= QT Y.
5: Compute an eigen-decomposition of G = ˜U Λ ˜UT .
6: Form U = Q ˜U.
a sparse or structured (e.g., Toeplitz) matrix A, the matrix-vector multiplications needed
in Step 2 of Algorithm 8.7.4 can be done much faster than O(N 2). It is not uncommon
that O(2N) ﬂops suﬃce. Let Tmult be the exact cost of matrix-vector multiplication. The
computational complexity of Algorithm 8.7.4 is 2lTmult+O(2k2N) [HMT11]. The interested
reader may refer to [HMT11] for a detailed performance analysis that provides theoretical
guarantees that the approximation error almost surely is less than a certain upper bound
as well as a thorough derivation of the computational cost paid in various situations.
Many recent works have focused on the design and analysis of algorithms that eﬃ-
ciently create small “sketches” of matrices and hypermatrices. Such sketches have been

Multimedia Social Search Based on Hypergraph Learning
265
used in eigen-decompositions [FKV98, AF07], semi deﬁnite programming solvers, and
matrix completion. Sketches of hypermatrices have been exploited in decompositions
[dKKV05, DM07, MD08]. Let I be the set of integers {1, 2, . . . , N} and ×dI = I × I × . . . × I
|
{z
}
d times
.
Given an order-d hypermatrix A ∈R×dI and an error tolerance ϵ ≥0, a hypermatrix sketch
˜A ∈R×dI of A is constructed such that:
||A −˜A||2 ≤ϵ ||A||2
(8.7.8)
and the number of non-zero entries in ˜A is minimized, where ||||2 denotes the spectral norm
of a hypermatrix. Algorithm 8.7.5 has been proposed that enjoys theoretical guarantees in
[NDT15]. The algorithm zeroes out “small” elements of the hypermatrix A, keeps “large”
elements of A, and randomly samples the remaining elements of A with probability that
depends on their magnitude.
Algorithm 8.7.5: Hypermatrix sparsiﬁcation.
Input: Order-d hypermatrix A and sampling parameter s.
Output: Order-d sketch ˜A of A.
1: for i1, i2, . . . id ∈I × I × . . . × I do
2:
if a2
i1,i2,...,id ≤ln2 N
N
d
2
||A||2
F
s
then
3:
˜ai1,i2,...,id = 0
4:
else
5:
if a2
i1,i2,...,id ≥||A||2
F
s
then
6:
˜ai1,i2,...,id = ai1,i2,...,id
7:
else
8:
˜ai1,i2,...,id =
(
ai1,i2,...,id
pi1,i2,...,id
with probability pi1,i2,...,id =
s a2
i1,i2,...id
||A||2
F
0
with probability 1 −pi1,i2,...,id
9:
end if
10:
end if
11: end for
This section is concluded with a straightforward extension of the work in [AF07] to
hypermatrices [Tso10] in conjunction with high-order SVD (HOSVD), summarized in Al-
gorithm 8.7.6.
8.8
Conclusions
In this chapter, hypergraphs have been studied. It has been shown that the hypergraphs
possess strong mathematical foundations and constitute a powerful concept, allowing us to
cast a number of multimedia social search problems as clustering or ranking problems.
Starting with the κ-uniform hypergraphs, their spectra have been reviewed. Evolutionary
stable strategies for clustering on κ-uniform hypergraphs have been shown to be closely
related to a growth transformation for the maximization of a homogeneous polynomial.
That is, such strategies constitute a special case of the Baum-Eagon theorem. Moreover,

266
Graph-Based Social Media Analysis
Algorithm 8.7.6: MACH-HOSVD.
Input: Order-d hypermatrix A, number of factors R1, R2, . . . , Rd, and probability p.
Output: Core hypermatrix G and matrices X(j), j = 1, 2, . . . , d made up of the Rj leading
left singular vectors of the j-th mode matrices of the sketch hypermatrix ˜A.
1: for i1, i2, . . . id ∈I × I × . . . × I do
2:
Toss a coin with probability p of keeping it
3:
if success then
4:
˜ai1,i2,...,id =
ai1,i2,...,id
p
5:
else
6:
˜ai1,i2,...,id = ai1,i2,...,id
7:
end if
8: end for
9: for j = 1, 2, . . . , d do
10:
Extract the leading Rj left singular vector of the j-th mode matrix ˜A(j) and store
them in X(j).
11: end for
12: Set G = ˜A ×d
j=1
 X(j)T .
high-order decomposition techniques have been applied to the adjacency hypermatrix of
κ-uniform hypergraphs, revealing their tight link to hypermatrix decompositions.
Next, spectral clustering for arbitrary hypergraphs has been studied. It has been shown
that the underlined techniques stem from various transformations of hypergraphs into
graphs, whose edge weights are expressed in terms of the hyperedge ones, notably the click
expansion and the star expansion. Emphasis has been given to the hypergraph normalized
cut criterion used more frequently in the literature. Its relationship with the star expansion
has been established. All the aforementioned transformations describe the hypergraph in
terms of the adjacency matrix of the induced graph and resort to the spectral analysis of the
(normalized) Laplacian of the induced graph. In addition to clustering, ranking on hyper-
graphs has been reviewed, because recommendation and tagging can be addressed in that
way. Proper optimization problems have been set in order to enforce structural constraints
on the weights and their solution has been derived.
Several applications have been demonstrated. Emphasis has been given to music/image
recommendation and tagging, geo-location prediction, and tourism recommendation.
Randomized algorithms for the factorization of big matrices and hypermatrices have
been surveyed and eﬃcient and fast algorithms have been outlined. The computational cost
of the various operations has been explicitly stated.
To sum up, this chapter has unveiled the potential of hypergraphs to capture multi-
modal high-order relations in social media. Other important aspects of social media, such
as temporal evolution, latent model adaptation, incremental spectral clustering, incremental
hypermatrix factorizations, and scalability issues are studied in Chapter 11, complement-
ing the topics presented in this chapter. There, parallel and distributed implementations of
latent models, spectral analysis, and hypermatrix decompositions are also discussed.

Multimedia Social Search Based on Hypergraph Learning
267
8.9
Acknowledgments
This research has been co-ﬁnanced by the European Union (European Regional De-
velopment Fund - ERDF) and Greek national funds through the Operation Program
“Competitiveness-Cooperation 2011” - Research Funding Program: SYN-10-1730-ATLAS.
Bibliography
[ABB06]
S. Agarwal, K. Branson, and S. Belongie. Higher order learning with graphs. In
Proc. 23rd International Conference on Machine Learning, pages 17–24, 2006.
[AF07]
D. Achlioptas and F. McSherry. Fast low-rank matrix approximations. Journal
of the ACM, 54(2), 2007.
[AK95]
C. J. Alpert and A. B. Kahng. Recent directions in netlist partitioning: A
survey. Integration, the VLSI Journal, 19(1-2):1–85, 1995.
[AKY99]
C. J. Alpert, A. B. Kahng, and S.-Z. Yao. Spectral partitioning with multiple
eigenvectors. Discrete Applied Mathematics, 90:3–26, 1999.
[ALZM+05] S. Agarwal, J. Lim, L. Zelnik-Manor, P. Peronal, D. Kriegman, and S. Belongie.
Beyond pairwise clustering. In Proc. IEEE Conference on Computer Vision
and Pattern Recognition, volume 2, pages 838–845, 2005.
[BB82]
D. H. Ballard and C. M. Brown. Computer Vision. Prentice-Hall, 1982.
[BBM05]
A. C. Berg, T. L. Berg, and J. Malik. Shape matching and object recognition
using low distortion correspondence. In Proc. IEEE Conference on Computer
Vision and Pattern Recognition, pages 26–33, 2005.
[BE67]
L. E. Baum and J. A. Eagon. An inequality with applications to statistical
estimation for probabilistic functions of Markov processes and to a model for
ecology. Bulletin American Mathematical Society, 73:360–363, 1967.
[Ber89]
C. Berge. Hypergraphs. North Holland, 1989.
[Bj¨o94]
A. Bj¨ork. Numerics of Gram-Schmidt orthogonalization. Linear Algebra Ap-
plications, 197-198:297–316, 1994.
[BK14]
N. Bassiou and C. Kotropoulos. On-line PLSA: Batch updating techniques
including out of vocabulary words. IEEE Transactions Neural on Networks
and Learning Systems, 25(11):1953–1966, 2014.
[BM73]
C. Berge and E. Minieka. Graphs and Hypergraphs. North Holland, 1973.
[BNJ03]
D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet Allocation. Journal
of Machine Learning Research, 3:993–1022, 2003.
[Bol93]
M. Bolla. Spectral Euclidean representations for clustering of hypergraphs.
Discrete Mathematics, 117(1-3):19–39, 1993.

268
Graph-Based Social Media Analysis
[BP98]
S. Brin and L. Page. The anatomy of a large-scale hypertextual Web search
engine. In Proc. 7th International World Wide Web Conference, pages 107–
117, 1998.
[BP13]
S. R. Bul`o and M. Pelillo. A game theoretic approach to hypergraph clustering.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(6):1312–
1327, 2013.
[BTC+10]
J. Bu, S. Tan, C. Chen, C. Wang, H. Wu, Z. Lijun, and X. He. Music rec-
ommendation by uniﬁed hypergraph: Combining social media information and
music content. In Proc. ACM Multimedia Conference, pages 391–400, 2010.
[BZM06]
A. Bosch, A. Zisserman, and X. Munoz. Scene classiﬁcation via PLSA. In
Proc. European Conference on Computer Vision, pages 517–530, 2006.
[CC70]
J. D. Carroll and J. J. Chang. Analysis of individual diﬀerences in multidimen-
sional scaling with an N-way generalization of “Eckart-Young” decomposition.
Psychometrika, 35:283–319, 1970.
[CD12]
J. Cooper and A. Dutle. Spectra of uniform hypergraphs. Linear Algebra and
Its Applications, 436(9):3268–3292, 2012.
[Chu97]
F. R. K. Chung. Spectral Graph Theory, volume 92. Regional Conference Series
in Mathematics American Mathematical Society, 1997.
[CQ14]
Z. Chen and L. Qi. Circulant tensors with applications to spectral hypergraph
theory and stochastic process. eprint arXiv:1312.2752v7 [Math.SP], pages 1–
26, 2014.
[CZPA09]
A. Cichocki, R. Zdunek, A. H. Pan, and S.-I. Amari. Nonnegative Matrix and
Tensor Factorizations. J. Wiley and Sons, Ltd., 2009.
[DBKP11]
O. Duchenne, F. Bach, I. Kweon, and J. Ponce.
A tensor-based algorithm
for high-order graph matching. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 33(12):2383–2395, 2011.
[DHZS02]
C. Ding, X. He, H. Zha, and H. Simon. PageRank, HITS, and a uniﬁed frame-
work for link analysis.
In Proc. 25th ACM SIGIR Conference Research &
Development on Information Retrieval, pages 353–354, 2002.
[DKK11]
D. M. Dunlavy, T. G. Kolda, and W. P. Kegelmeyer. Multilinear algebra for
analyzing data with multiple linkages. In J. Kepner and J. Gilbert, editors,
Graph Algorithms in the Language of Linear Algebra, pages 85–114. SIAM,
2011.
[dKKV05]
W. F. de la Vega, R. Kannan, M. Karpinski, and S. Vempala. Tensor decom-
position and approximation schemes for constraint satisfaction problems. In
Proc. 37th Annual ACM Symposium on Theory of Computing, pages 747–754,
2005.
[DM07]
P. Drineas and M. W. Mahoney. A randomized algorithm for a tensor-based
generalization of the singular value decomposition.
Linear Algebra and its
Applications, 420(2):553–571, 2007.
[ERV06]
E. Estrada and J. A. Rodriguez-Velazquez. Subgraph centrality and clustering
in complex hyper-networks. Physica A: Statistical Mechanics and its Applica-
tions, 364:581–594, 2006.

Multimedia Social Search Based on Hypergraph Learning
269
[Fag83]
R. Fagin.
Degrees of acyclicity for hypergraphs and relational database
schemes. Journal of the ACM, 30(3):514–550, 1983.
[FBH03]
N. M. Faber, R. Bro, and P. K. Hopke.
Recent developments in CANDE-
COMP/PARAFAC algorithms: A critical review. Chemometrics and Intelli-
gent Laboratory Systems, 65:119–137, 2003.
[FKV98]
A. Frieze, R. Kannan, and S. Vempala. Fast Monte Carlo algorithms for ﬁnding
low-rank approximations. In Proc. 39th Annual IEEE Symposium on Founda-
tions of Computer Science, pages 370–378, 1998.
[Fuk10]
T. Fukunaga. Computing minimum multiway cuts in hypergraphs from hy-
pertree packing. In F. Eisenbrand and F. B. Shepherd, editors, Integer Pro-
gramming and Combinatorial Optimization, volume LNCS 6080, pages 15–28.
Springer, 2010.
[GL06]
G. H. Golub and C. F. Van Loan. Matrix Computations. The Johns Hopkins
University Press, 3/e edition, 2006.
[Gov05]
V. M. Govindu. A tensor decomposition for geometric grouping and segmenta-
tion. In Proc. IEEE International Conference on Computer Vision and Pattern
Recognition, pages 1150–1157, 2005.
[GWZ+13]
Y. Gao, M. Wang, Z. Zha, J. Shen, X. Li, and X. Wu. Visual-textual joint
relevance learning for tag-based social image search. IEEE Transactions on
Image Processing, 22(1):363–376, 2013.
[Har70]
R. A. Harshman. Foundations of the PARAFAC procedure: Models and condi-
tions for an “explanatory” multimodal factor analysis. UCLA Working Papers
in Phonetics, 16:1–84, 1970.
[HG07]
K. Heller and Z. Ghahramani. A nonparametric Bayesian approach to model-
ing overlapping clusters. In Proc. International Conference on Artiﬁcial Intel-
ligence and Statistics, pages 187–194, 2007.
[HKKM98]
E.-H. Han, G. Karypis, V. Kumar, and B. Mobasher. Hypergraph based clus-
tering in high-dimensional datasets: A summary of results. IEEE Data Engi-
neering Bulletin, 21(1):15–22, 1998.
[HLL+11]
Y. Huang, Q. Liu, F. Lv, Y. Gong, and D. N. Metaxas. Unsupervised im-
age categorization by hypergraph partition. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 33(6):1266–1273, 2011.
[HLZM10]
Y. Huang, Q. Liu, S. Zhang, and D. Metaxas. Image retrieval via probabilistic
hypergraph ranking. In Proc. IEEE International Conference on Computer
Vision Pattern Recognition, pages 3376–3383, 2010.
[HMT11]
N. Halko, P. G. Martinsson, and J. A. Tropp. Finding structure with random-
ness: Probabilistic algorithms for constructing approximate matrix decompo-
sitions. SIAM Review, 53(2):217–288, 2011.
[Hof99]
T. Hofmann.
Probabilistic latent semantic indexing.
In Proc. 22nd Inter-
national ACM SIGIR Conference Research and Development in Information
Retrieval, pages 50–57, 1999.

270
Graph-Based Social Media Analysis
[HS98]
J. Hofbauer and K. Sigmund. Evolutionary Games and Population Dynamics.
Cambridge University Press, 1998.
[HSJR13]
M. Hein, S. Setzer, L. Jost, and S. S. Rangapuram. The total variation on
hypergraphs - learning on hypergraphs revisited. In C.J.C. Burges, L. Bottou,
M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural
Information Processing Systems, volume 26, pages 2427–2435, 2013.
[KAKS97]
G. Karypis, R. Aggarwal, V. Kumar, and S. Shekhar. Multilevel hypergraph
partitioning: application in VLSI domain. In Proc. 34th ACM Annual Design
Automation Conference, pages 526–529, 1997.
[KB09]
T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM
Review, 51:455–500, 2009.
[KBK05]
T. G. Kolda, B. W. Bader, and J. P. Kenny. Higher-order web link analysis
using multilinear algebra. In Proc. 5th IEEE International Conference on Data
Mining, pages 725–728, 2005.
[KHT06]
S. Klampt, U.-U. Haus, and F. Theis.
Hypergraphs and cellular networks.
PLOS Computational Biology, 5(5):e1000385, 2006.
[KK00]
G. Karypis and V. Kumar. Multilevel K-way hypergraph clustering. VLSI
Design, 11(3):285–300, 2000.
[Kle99]
J. M. Kleinberg. Authoritative sources in hyperlinked environment. Journal
of the ACM, 46:604–632, 1999.
[KSS13]
K. Kapoor, D. Sharma, and J. Srivastava. Weighted node degree centrality
for hypergraphs.
In Proc. 2nd IEEE Workshop on Network Science, pages
152–155, 2013.
[Kuh76]
H. W. Kuhn. Nonlinear programming: A historical view. SIAM-AMS Proceed-
ings, IX:1–26, 1976.
[Lat97]
L. De Lathawuer. Signal processing based on multilinear algebra. PhD thesis,
K. U. Leuven, Electrical Engineering Dept. (ESAT, 1997.
[Law73]
E. Lawler. Cutsets and partitions of hyperedges. Networks, 3(3):275–285, 1973.
[LH05]
M. Leordeanu and M. Hebert. A spectral technique for correspondence prob-
lems using pairwise constraints. In Proc. International Conference on Com-
puter Vision, pages 1482–1489, 2005.
[Lim14]
L.-H. Lim. Tensors and hypermatrices. In L. Hogben, editor, Handbook of
Linear Algebra, pages 15–1–15–30. Chapman & Hall/CRC, 2014.
[LLS11]
Z. Lin, R. Lui, and Z. Su. Linearized alternating direction method with adap-
tive penalty for low-rank representation. In Advances in Neural Information
Processing Systems, pages 612–620, 2011.
[LLY10]
H. Liu, L. J. Latecki, and S. Yan. Robust clustering as ensembles of aﬃnity
relations. In Advances in Neural Information Processing Systems, pages 1414–
1422, 2010.
[LM06]
A. N. Langville and C. D. Meyer. Google’s PageRank and Beyond. Princeton
University Press, 2006.

Multimedia Social Search Based on Hypergraph Learning
271
[Low04]
D. G. Lowe. Distinctive image features from scale-invariant keypoints. Inter-
national Journal Computer Vision, 60(2):91–110, 2004.
[LPV13]
H. Lu, K. Plataniotis, and A. N. Venetsanopoulos.
Multilinear Subspace
Learning: Dimensionality Reduction of Multidimensional Data. Chapman &
Hall/CRC Taylor and Francis Group, 2013.
[MD08]
M. W. Mahoney and P. Drineas. Tensor-CUR decompositions and data appli-
cations. SIAM Journal on Matrix Analysis and Applications, 30(2):957–987,
2008.
[MS01]
M. Meila and J. Shi. A random walk view of spectral segmentation. In Proc.
8th International Workshop on Artiﬁcial Intelligence and Statistics, 2001.
[NDT15]
N. Nguyen, P. Drineas, and T. Tran. Tensor sparsiﬁcation via a bound on the
spectral norm of random tensors. Information and Inference: A Journal of the
IMA, 2015.
[NJW01]
A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering: Analysis and
an algorithm. In Advances in Neural Information Processing Systems, pages
849–856, 2001.
[OB12]
P. Ochs and T. Brox. Higher order motion models and spectral clustering. In
Proc. IEEE Conference on Computer Vision and Pattern Recognition, pages
614–621, 2012.
[OT06]
A. Oliva and A. Torralba. Building the GIST of a scene: The role of global
image features in recognition. Progress in Brain Research, 155:23–36, 2006.
[Pam04]
E. Pampalk. A MATLAB toolbox to compute music similarity from audio.
In Proc. 5th International Conference on Music Information Retrieval, pages
254–257, 2004.
[PCAS14]
P. Purkait, T.-J. Chin, H. Ackermann, and D. Suter.
Clustering with hy-
pergraphs: The case for large hyperedges. In Proc. European Conference on
Computer Vision, 2014.
[PF12]
L. Pu and B. Faltings. Hypergraph learning with hyperedge expansion. In P. A.
Flach, T. De Bie, and N. Cristianini, editors, European Conference on Machine
Learning Principles and Practice of Knowledge Discovery in Databases, volume
LNCS 7523, Part I, pages 410–425. Springer, 2012.
[PK14a]
K. Pliakos and C. Kotropoulos. Personalized music tagging using ranking on
hypergraphs.
In Proc. 2014 International Symposium on Communications,
Control, and Signal Processing, pages 681–684, 2014.
[PK14b]
K. Pliakos and C. Kotropoulos. PLSA driven image annotation, classiﬁcation,
and tourism recommendation. In Proc. 2014 IEEE International Conference
on Image Processing, pages 3003–3007, 2014.
[PK14c]
K. Pliakos and C. Kotropoulos. Simultaneous image tagging and geo-location
prediction within hypergraph ranking framework. In Proc. 2014 IEEE Interna-
tional Conference on Audio, Speech, and Signal Processing, pages 6944–6948,
2014.

272
Graph-Based Social Media Analysis
[PK14d]
K. Pliakos and C. Kotropoulos. Social image search exploiting joint visual-
textual information within a fuzzy hypergraph framework. In Proc. IEEE 16th
International Workshop on Multimedia Signal Processing, 2014.
[Por80]
M. Porter. An algorithm for suﬃx stripping. Program, 14(3):130–137, 1980.
[PP07]
M. Pavan and M. Pelillo. Dominant sets and pairwise clustering. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, 29(1):167–172, 2007.
[PZ13]
K. J. Pearson and T. Zhang. On spectral hypergraph theory of the adjacency
tensor. Graphs and Combinatorics, pages 1–16, 2013.
[QS10]
Z. Qin and K. Scheinberg. Eﬃcient block-coordinate descent algorithms for
the group Lasso. Industrial Engineering, pages 1–21, 2010.
[Rod03]
J. Rodriguez. On the Laplacian spectrum and walk-regular hypergraphs. Lin-
ear and Multilinear Algebra, 51(3):285–297, 2003.
[RTG00]
Y. Rubner, C. Tomasi, and L. Guibas. The Earth Mover’s distance as a metric
for image retrieval. International Journal on Computer Vision, 40(2):99–121,
2000.
[RY98]
E. S. Ristad and P. Yianilos. Learning string-edit distance. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 20(5):522–532, 1998.
[SM00]
J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, 22(8):888–905, 2000.
[SPF14]
N. D. Sidiropoulos, E. E. Papalexakis, and C. Faloutsos. Parallel randomly
compressed cubes: A scalable distributed architecture for big tensor decompo-
sition. IEEE Signal Processing Magazine, 31(5):57–70, September 2014.
[SWY75]
G. Salton, A. Wong, and C. Yang. A vector space model for automatic indexing.
Communications of the ACM, 18(11):613–620, 1975.
[SZH06]
A. Shashua, R. Zass, and T. Hazan.
Multi-way clustering using super-
symmetric non-negative tensor factorization. In Proc. European Conference
on Computer Vision, volume 3954, pages 595–608, 2006.
[TCR10]
C. Tramasco, J.-P. Cointet, and C. Roth. Academic team formation as evolving
hypergraphs. Scientometrics, 85(3):721–740, 2010.
[TKP13]
A. Theodorids, C. Kotropoulos, and Y. Panagakis. Music recommendation
using hypergraphs and group sparsity. In Proc. 2013 IEEE International Con-
ference on Acoustics, Speech, and Signal Processing, pages 56–60, 2013.
[TNW08]
H. K. Tan, C. W. Ngo, and X. Wu. Modeling video hyperlinks with hypergraph
for web video reranking. In Proc. 2008 ACM Multimedia Conference, pages
659–662, 2008.
[Tso10]
C. E. Tsourakakis. MACH: Fast randomized tensor decompositions. In Proc.
SIAM Conference on Data Mining, pages 689–700, 2010.
[WLRT08]
F. Woolfe, E. Liberty, V. Rokhlin, and M. Tygert. A fast randomized algorithm
for the approximation of matrices. Applied Computational Harmonic Analysis,
25:335–366, 2008.

Multimedia Social Search Based on Hypergraph Learning
273
[YL06]
M. Yuan and Y. Lin. Model selection and estimation in regression with grouped
variables. Journal of the Royal Statistical Society: Series B (Statistical Method-
ology), 68(1):49–67, 2006.
[ZD06]
Y. Zheng and D. Doermann. Robust point matching for nonrigid shapes by pre-
serving local neighborhood structures. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 28(4):643–649, 2006.
[ZHS06]
D. Zhou, J. Huang, and B. Sch¨olkopf. Learning with hypergraphs: Clustering,
classiﬁcation, and embedding. In Advances in Neural Information Processing
Systems, pages 1601–1608, 2006.
[ZS08]
R. Zass and A. Shashua. Probabilistic graph and hypergraph matching. In
Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2008.
[ZSC99]
J. Zien, M. Schlag, and P. Chan. Multilevel spectral hypergraph partitioning
with arbitrary vertex sizes. IEEE Transactions on Computer-Aided Design of
Integrated Circuits and Systems, 18(9):1389–1399, 1999.


Chapter 9
Graph Signal Processing in Social Media
Sunil Narang
University of Southern California, U.S.A.
9.1
Motivation ......................................................................
275
9.2
Graph signal processing (GSP) ................................................
277
9.2.1
Basics of graph signal processing ......................................
277
9.2.2
Spectral representation of graph signals ..............................
279
9.2.3
Downsampling in graphs ..............................................
280
9.2.4
Graph wavelets and ﬁlterbanks ........................................
282
9.3
Applications ....................................................................
282
9.3.1
Information diﬀusion pattern analysis ................................
282
9.3.2
Interpolation in graphs ................................................
284
9.3.2.1
Movie recommendation system ..........................
286
9.4
Conclusions .....................................................................
289
Bibliography ....................................................................
289
9.1
Motivation
Social media has become an integral part of modern society. There are general social
networks with user bases larger than the population of most countries. There are niche sites
for virtually every special interest out there, for example, to share photos, videos, status
updates, sites for meeting new people, and sites to connect with old friends. The analysis of
these networks has become an intriguing topic both due to monetary and management per-
spective as well theoretical challenges. In this chapter, we summarize some of the interesting
problems in the analysis of social data, and the techniques used to solve them.
Mostly the social data have rich connections that can be naturally represented as graphs.
Consider, for example, the Facebook friendship network where individual users (as nodes)
are connected by friendship ties (as links). In some other applications social data can be rep-
resented as point-clouds of vectors and links are established between data sources based on
the distance between their feature-vectors. For example, Tang et al. [HJCTH12] use graph
representation of multimedia data for clustering and analysis. For brevity let’s just call the
datasets graphs. The links on the graph can be directed (as in Twitter follower/followee
relations) or undirected (bi-directional). Both the nodes and links can have a number of nu-
merical or categorical attributes. The problems of graph analysis can be roughly divided into
two parts: a) those analyzing graph structure (e.g., segmentation, ﬁnding nearest neighbors,
shortest distance between two nodes etc.) and b) those analyzing information available on
the graph (ﬁltering, denoising, missing value prediction, compression etc.). Graph analysis
of social data dates back to pre-internet era.
275

276
Graph-Based Social Media Analysis
FIGURE 9.1.1: Zachary Karate Club network. The ﬁlled and empty nodes represent diﬀerent
factions formed the club.
A classic example of social graph analysis is the Zachary Karate Club data [Zac77],
which was collected from the members of a university karate club by Wayne Zachary in
1977. Zachary used this data and an information ﬂow model of network conﬂict resolution
to explain the split-up of this group into two groups following disputes among the members.
His analysis could correctly predict the alliance of 33 of the 34 members of the group.
However, the modern day social data analysis problems are far more complex than that.
Below we discuss a few key problems in graph analysis which have a lot of well-known
applications.
Vector-Completion Problem: Vector-completion, or more generally matrix-completion,
problems often arise in the context of recommendation systems. The users (in set U) prefer-
ences (on a scale of 0 −N) for inventory items (in set I) are recorded in the form of a U ×I
matrix containing elements from ﬁnite set {0, 1, ..., N −1}. Here, each row is a partially
known preference vector of a user, which builds up as he or she views or accesses content.
The problem then is to ﬁll the remaining entries in the preference vector. The famous net-
ﬂix challenge [BLN07] is an example of the vector completion problem. The data can be
represented as a weighted bipartite graph in which users’ nodes U are connected to item
nodes I. We will discuss some solutions to this problem in the application section.
Information Diﬀusion Pattern Analysis: Social media has become a major platform
used by people to broadcast information. Given social media data, such as Twitter or blogs,
we are interested in analyzing the diﬀusion patterns of diﬀerent types of information (e.g.,
topics, emotions, sentiments etc.) within the same social network. One of the applications
is topic sentiment analysis [WWL+11], where the goal is to ﬁnd segmentation of graphs
along topic/sentiment boundaries in Twitter or a citation network. Another application is
emotion propagation [Kra12], which studies how emotional states can spread among people
without their awareness (known as emotional contagion).
Multimedia data analysis: Another application of graph analysis is in representing digital
multimedia content such as images and videos, where pixels can be connected with their
neighbors to form graphs [SKN+10, MEO11]. Having a graph representation of images (or
video) provides ﬂexibility of adjusting link weights of the image graph and we provide a
simple way to capture both directionality and intrinsic edge information of the image.
Graph Based Prediction: This problem is about predicting the value of a node based
network conﬁguration, and the value of other nearby nodes [GAO14]. This is useful for spam
ﬁltering, age or gender prediction, or identifying inﬂuential nodes.

Graph Signal Processing in Social Media
277
In most of these cases the data on the nodes such as emotions, preferences, or age can be
quantiﬁed and represented as discrete signals. Further, nodes connected to each other often
have some similarity between them, which can be used for denoising, ﬁltering, prediction
etc. For example, the gender prediction problem, there is a strong correlation between the
gender of a user and the gender of friends it is connected with. Thus, ﬁltering or predicting
the signal at a node w.r.t. the structure of the graph leads to eﬃcient representation (for
example sparse approximation) of the underlying signal. This interpretation has led to the
design of a new set of tools and techniques [SM14] that analyzes graph signals using the
extensions of tools and techniques in classical discrete signal processing (DSP) theory for
time signals and images. This area is now called graph signal processing (GSP).
The GSP techniques help understand the structure behind the observations, hence come
up with eﬃcient methods to process the information by using tools deﬁned on graphs. Major
challenges are posed by the size of the datasets in these problems (number of nodes and
links), making it diﬃcult to visualize, process, analyze, and act on the information available.
In distributed and cloud scenarios, the graph is stored on diﬀerent machines and the data-
exchanges between far-oﬀnodes can be expensive (bandwidth, latency, energy constraints
issues). Therefore, instead of operating on the original graph, it would be desirable to ﬁnd
and operate on smaller graphs with fewer nodes and data representing a smooth (more
generally, it could be any sparse approximation of the original data) approximation of the
original data. Moreover, such systems need to employ localized operations which could be
computed at each node by using data from a small neighborhood of nodes around it. In
this chapter, we describe GSP techniques which provide solutions to overcome both of the
above challenges.
The remainder of this chapter is as follows: in Section 9.2 we describe important concepts
of graph signal processing including spatial and spectral representation and vertex down-
sampling schemes. In Section 9.3, we study two applications of graph analysis: a blogger
network analysis to ﬁnd topic based segmentation of the network, and the design of a movie
recommendation system. Finally we conclude the chapter in Section 9.4 with a summary.
9.2
Graph signal processing (GSP)
Emerging data mining applications will have to operate on datasets deﬁned on graphs.
Examples of such datasets include online document networks, social networks, and knowl-
edge bases etc. The data on these graphs can be visualized as a ﬁnite collection of samples,
a graph-signal which can be deﬁned as the information attached to each node (scalar or
vector values mapped to the set of vertices/edges) of the graph. Two basic approaches have
been used for signal processing on graphs. The ﬁrst one uses the adjacency matrix of the
underlying graph as its basic building block (for example, see [SM14, SM13] and subsequent
extensions). The second approach adopts the graph Laplacian matrix as it’s fundamental
building block [NA12]. Both frameworks deﬁne several signal processing concepts similarly,
but the diﬀerence in their foundation leads to diﬀerent techniques for signal analysis and
processing.
9.2.1
Basics of graph signal processing
A graph signal is a real-valued scalar function f : V →R deﬁned on graph G =
(V, E) such that f(v) is the sample value of function at vertex v ∈V. The exten-
sion to complex or vector sample values f(v) is straightforward but is not considered in

278
Graph-Based Social Media Analysis
this work. On a ﬁnite graph, the graph-signal can be viewed as a sequence or a vector
f = [f(0), f(1), ..., f(N −1)]T , where the order of arrangement of the samples in the vector
is arbitrary and neighborhood (or nearness) information is provided separately by the adja-
cency matrix A. The matrix A is such that A(n, m) is the weight of the link(edge) between
node n and m (= 0 if not connected). The remaining portion of this chapter deals with
simple undirected graphs without self loops, which have a lot of interesting mathematical
properties. Often, the directed graph is also processed as undirected graph by symmetrizing
its links. Other work such as [SM14] implement analysis techniques directly on directed
graphs. There is also some work on the analysis of multilayered graph in the context of
social networks [OKH14] where diﬀerent types of relationships (edges) between nodes are
represented as diﬀerent layers.
The A matrix is symmetric for undirected graphs, with 0 at the diagonal. We deﬁne D
as the diagonal degree matrix where D(n, n) is the degree (sum of the weight of the edges
connected) of node n. The Laplacian matrix L is another fundamental matrix deﬁned on
graph, deﬁned as L = D −A. It is easy to see that L is a symmetric positive semi-deﬁnite
matrix which means its eigenvalues are always non-negative. Deﬁne symmetric normalized
adjacency matrix and symmetric normalized Laplacian matrix as A = D−1/2AD−1/2 and
L = D−1/2LD−1/2, respectively. A j-hop neighborhood Nj,n around node n is the set of
all nodes that are at most j edges apart from node n. It can be shown that:
Nj,n = {m : Aj(n, m) ̸= 0},
(9.2.1)
which is an easy way to compute the j-hop neighborhood. Denote < f1, f2 > as the inner
product between signals f1 and f2. Graph-signals can, for example, be a set of measured
values by sensor network nodes [WR06] or traﬃc measurement samples on the edges of
an Internet graph [CK03]1 or information about the actors in a social network. Further,
a graph based transform is deﬁned as a linear transform T : RN →RM in the N-node
graph-signal space, such that the operation at each node n is a linear combination of the
value of the sample at the node n and its j-hop neighborhood Nj,n for some j, i.e.,
y(m) =< tm f >= T(m, n)f(n) +
X
o∈Nj,n
T(m, o)f(o),
(9.2.2)
where tm is the mth row of the transform T. The transform is called undersampled, critically
sampled or oversampled depending upon the relation between input and output size (M <
N, M = N, M > N, respectively). In analogy to the 1-D regular case, we would sometimes
refer to graph-transforms as graph-ﬁlters and the elements T(n, m) for m = 1, 2, ...N as the
ﬁlter coeﬃcients at the nth node 2 A desirable feature of graph ﬁlters is spatial localization,
which typically means that the energy of each ﬁlter (i.e., each row) of the graph ﬁlter is
concentrated in a local region around a node. Let us deﬁne ∆2
k(tn) given as:
∆2
k(tn) =
1
||tn||2
X
l∈Nk,n
T(n, l)2
(9.2.3)
to be the fraction of energy of nth basis function (i.e., nth row tn), in the k-hop neighbor-
hood around node n. A graph transform is said to be strictly k-hop localized, or having a
1The graph in this case was chosen to be the line graph of the network graph. The line graph contains
edges of the original graph as nodes and the network load as the graph signal.
2Not every linear transform is a graph-transform, since graph-transforms, by deﬁnition, are deﬁned along
the edges in the graph. For example, ﬁlter-coeﬃcient T(n, m) can be non-zero only if nodes n and m are
connected, i.e., d(n, m) < ∞, and the magnitude of T(n, m) usually decreases with increasing distance
d(n, m).

Graph Signal Processing in Social Media
279
compact support in the spatial domain, if ∆2
k(tn) = 1 for all n = 1, 2, ..N. Note that spatial
localization can also be applied in a weaker sense in which ∆2
k(tn) is not exactly 1 but very
close to it for all n = 1, 2, ...N. In this tutorial, we will study graph-ﬁlters with compact sup-
port, which are scalable with the size of the graph. Examples of compact support transforms
are lifting transforms [JNS09, NO09] and the transforms proposed in [CK03] and [WR06].
Examples of non-compact transforms are diﬀusion wavelets [CM06] and spectral wavelets
[HVG11].
9.2.2
Spectral representation of graph signals
Intuitively, a signal is smooth or lowpass, if the diﬀerence of sample values between a
node and its neighbors is small. Thus, the smoothness of a graph signal depends on the un-
derlying graph topology, and is usually described in terms of eigenvalues and eigenvectors of
a fundamental symmetric graph matrix. The choice of this matrix diﬀers among researchers,
for example, Laplacian matrix is used in [HVG11, NA12], and adjacency matrix is used in
[SM14]. We use Laplacian matrix in this chapter. Let us denote spectral decomposition of
the graph G as the set of eigenvalues σ(G), and the corresponding eigenvectors uλ, λ ∈σ(G)
of the graph Laplacian matrix. Both the original Laplacian matrix L and the normalized
form L are symmetric positive semideﬁnite matrices, and therefore can be used for this
purpose. From the spectral projection theorem, there exists a real unitary matrix U which
diagonalizes L (or L), such that UT LU = Λ = diag{λi} is a non-negative diagonal matrix.
Let us use the symmetric normalized Laplacian matrix L, which is more closely related
to random walks in the graphs. This is because matrix L is similar to the random walk
Laplacian matrix Q = I −D−1A, and thus has identical set of eigenvalues (with identi-
cal algebraic multiplicities). In fact, the random The eigenvalues of L are in “normalized”
form, i.e., if λ ∈σ(G) then 0 ≤λ ≤2, and are thus consistent with the eigenvalues in the
stochastic processes. Refer to [Chu97] for details. This leads to an eigenvalue decomposition
of matrix L given as:
L = UΛUT =
N
X
i=1
λiuiuT
i ,
(9.2.4)
where the eigenvectors u1, u2, ..., uN, which are columns of U, form an orthogonal basis
in RN and {0 ≤λ1 ≤λ2... ≤λN} are corresponding eigenvalues. Thus, every graph-
signal f ∈RN can be decomposed into a linear combination of eigenvectors ui given as
f = PN
n=1 ¯f(n)un. It has been shown in [Chu97] that the eigenvectors of Laplacian matrix
provide a harmonic analysis of graph signals which gives a Fourier-like interpretation. The
eigenvectors act as the natural vibration modes of the graph, and the corresponding eigenval-
ues as the associated graph-frequencies. The mapping un →V associates the real numbers
un(i), i = {1, 2, ..., N}, with the vertices V of G. The numbers un(i) will be positive, nega-
tive or zero. The frequency interpretation of eigenvectors can thus be understood in terms
of number of zero-crossings (pair of connected nodes with diﬀerent signs) of eigenvector
un on the graph G. For any ﬁnite graph the eigenvectors with large eigenvalues have more
zero-crossings (hence high-frequency) than eigenvectors with small eigenvalues (see Figure
9.2.1 for example.). These results are related to “nodal domain theorems” and readers are
directed to [DGLS01] for more details. The spectrum σ(G) of a graph is deﬁned as the set of
eigen-values of its normalized Laplacian matrix, and it is always a subset of closed set [0, 2]
for any graph G. For the purpose of this chapter, an eigenvector uλ is either considered
to be a “lowpass” eigenvector if eigenvalue λ ≤1, or “highpass” eigenvector if λ > 1. The
graph Fourier transform (GFT), denoted as ¯f, is deﬁned in [HVG11] as the projections of

280
Graph-Based Social Media Analysis
(i) λ = 0.00
(ii) λ = 0.04
(iii)λ = 0.20
(iv)λ = 0.40
(v) λ = 1.20
λ = 1.49
(vi)
FIGURE 9.2.1: Stem plots of eigen vectors of Laplacian matrix L for an example graph. The
ﬁlled (empty) nodes contain positive (negative) value. The zero-crossings (edges between a
positive and a negative sample) of the eigenvectors usually increase with increasing eigen-
values λ. (See color insert.)
a signal f on the graph G onto the eigenvectors of G, i.e.,:
¯f(λ) =< uλ , f >=
N
X
i=1
f(i)uλ(i).
(9.2.5)
Note that GFT is an energy preserving transform. A signal is considered “lowpass” (or
“high-pass”) if the energy | ¯f(λ)|2 ≈0 for all λ > 1 (or for all λ ≤1). In case of eigenvalues
with multiplicity greater than 1 (say λ1 = λ2 = λ) the eigenvectors u1, u2 are unique up
to a unitary transformation in the eigenspace Vλ = Vλ1 = Vλ2. In this case, we can choose
λ1u1uT
1 + λ2u2uT
2 = λPλ where Pλ is the projection matrix for eigenspace Vλ. Note that
for all symmetric matrices, the dimension of eigenspace Vλ (geometric multiplicity) is equal
to the multiplicity of eigenvalue λ (algebraic multiplicity) and the spectral decomposition
in (9.2.4) can be written as:
L =
X
λ∈σ(G)
λ
X
λi=λ
uiuT
i =
X
λ∈σ(G)
λPλ.
(9.2.6)
The eigenspace projection matrices are idempotent and Pλ and Pγ are orthogonal if λ and
γ are distinct eigenvalues of the Laplacian matrix, i.e.,:
PλPγ = δ(λ −γ)Pλ,
(9.2.7)
where δ(λ) is the Kronecker delta function.
9.2.3
Downsampling in graphs
Downsampling is done to reduce the cost of storage and analysis of large graphs. A
downsampling operation on the graph G = (V, E) can be deﬁned as choosing a subset H
of vertex set V such that all samples of the graph signal f, corresponding to indices not

Graph Signal Processing in Social Media
281
regular signal
regular signal after DU by 2
graph signal
graph signal after DU by 2
(i)
(ii)
(iii)
(iv)
FIGURE 9.2.2: DU operation on a graph G. The empty circle nodes form the set L, whose
values are discarded and replaced by 0 after DU operation.
in H, are discarded. A subsequent upsampling operation projects the downsampled signal
back to original RN space by inserting zeros in place of discarded samples in Hc = L. A
comparison of downsampling a regular signal vs. a graph signal is shown in Figure 9.2.2.
Given such a set H we deﬁne a downsampling function βH ∈{−1, +1} given as:
βH(n) =
( 1
if n ∈H
−1
if n ∈L.
(9.2.8)
While several downsampling techniques have been proposed in literature, we only discuss
the ones where the goal is to reconstruct the signal from the downsampled upsampled (DU)
signal by interpolating values on L set using values in the H set. In general, downsampling
always leads to signal distortion. However, its been proven in [Pes08] that for a special class
of bandlimited signals in the Paley-Weiner space or PWω, there exist a set H such that the
values at the discarded nodes L can be reconstructed exactly without any information loss.
Furthermore, in the case of bipartite graphs, downsampling on one of the colored partitions
leads to an eﬀect analogous to frequency folding [NO11]. A bipartite graph G = (L, H, E)
is a graph whose vertices can be divided into two disjoint sets L and H, such that every
link connects a vertex in L to one in H. Bipartite graphs are also known as two-colorable
graphs since the vertices can be colored perfectly into two colors so that no two connected
vertices are of the same color. This gives the cut-oﬀfrequency and also suggests a natural
sampling strategy. The downsampled graph is constructed by reconnecting the downsampled
vertices H with an edge whose weight is equal to the number of their common neighbors
in the original graph. 3. It can also be seen that undirected trees are bipartite graphs
where nodes at alternative levels (nodes at even distance from root node and nodes at odd
distance from root node) form the two color sets. Therefore, the downsampling technique
proposed in [GNC10], and in [SO08] can be seen as a specialized case of downsampling on
bipartite graphs. In [EFAR13], the results from downsampling on bipartite graphs [NO11]
are generalized to circulant graphs. For one-step interpolation in arbitrary graphs, [Pes08]
gives a suﬃcient condition that the sampling set needs to satisfy for unique recovery. Using
3Note that the reduced graph is not bipartite. Therefore, iterative downsampling techniques require
additional steps to convert downsampled graph into one or more bipartite graphs. More explanation of the
iterative downsampling techniques can be found in [NA12, SFV13]

282
Graph-Based Social Media Analysis
this condition, a bound on the maximum bandwidth of all recoverable signals is given in
[NGO13].
9.2.4
Graph wavelets and ﬁlterbanks
Wavelet transforms have been widely used as a signal processing tool for a sparse repre-
sentation of signals. Wavelet based transforms split the sample space into an approximation
and a detail subspace. The approximation subspace contains a smoother version of the orig-
inal signal and the details of the signal are contained in the detail subspace. A discussion of
wavelets and wavelet transforms on regular signals can be found in standard textbooks such
as [VK95]. Here we only describe techniques used to extend classical wavelets to graphs.
While wavelet-based techniques would seem well suited to provide eﬃcient local analysis,
a major obstacle to their application to graphs is that these, unlike images, are not regu-
larly structured. Therefore properties of regular wavelets like locality and smoothness, do
not have an obvious extension in the graph case. Moreover, classical wavelet transforms
are critically sampled as they use local ﬁltering operations followed by downsampling. In
a graph, there is no obvious way to downsample nodes in a regular manner, since these
neighborhoods vary in size and orientation.
Recently, a variety of designs of the wavelets and wavelet transforms on graphs have been
proposed by researchers. These wavelet designs can be broadly classiﬁed into a) spatially
localized designs and b) spectral designs. The spatial designs are designed in the spatial
domain, i.e., directly on the nodes and their neighborhood, with spatial localization as
their focus. These include lifting wavelets [JNS09, SO08, NO09], wavelets on hierarchical
trees [GNC10], and wavelets for spatial traﬃc analysis [CK03]. The spectral designs, on the
other hand, are designed in the spectral domain of the graph and then translated to spatial
domains. Notable examples are: diﬀusion wavelets [CM06], spectral graph wavelets [HVG11],
graph-QMF and graph-Bior ﬁlterbanks [NA12, NA13]. Usually, these wavelet functions are
not the translates or dilates of a single mother wavelet function. However, we would like
to preserve as many properties of classical wavelets as possible, for example orthogonality,
sampling ratio, response to a constant (DC) signal, and reconstruction error. A comparison
of some of these designs can be found in [NA12].
9.3
Applications
9.3.1
Information diﬀusion pattern analysis
Social media has become a major platform for people to broadcast information. Given
social media data, such as Twitter or blogs, we are interested in analyzing whether diﬀerent
types of information (e.g., diﬀerent topics) have similar diﬀusion patterns within the same
social network. More speciﬁcally, we are interested in investigating combining topic models,
such as the Latent Dirichlet Allocation (LDA) model, with our wavelet algorithms for graph
analysis. The basic idea is as follows: for each topic generated by the LDA model, we run
the wavelet analysis on the citation network or retweet graphs of users (in which the value
of the node represents the relative frequency of the topics in the posts written by the user,
and the edges represent the frequency of citations or retweet) so that we can capture the
“high-pass” information of the graph associated with each topic, showing that there exist
“boundaries” in how a topic propagates (these boundaries are discovered where high-pass
information in the graph signal is signiﬁcant). We expect to use this approach at several

Graph Signal Processing in Social Media
283
resolutions to compare information diﬀusion patterns and potentially provide insights to
discoveries in social science.
Blogger Network Analysis Given large-scale linked unstructured data, such as a col-
lection of blog posts or a research literature archive, there are two fundamental problems
that have generated a lot of interest in the research community. One is to identify a set of
high-level topics covered by the documents in the collection using unstructured data only;
the other is to uncover and analyze the social network of the authors using the graph data
only. So far, these problems have been viewed as separate problems and considered inde-
pendently from each other. In this section, we argue that these two problems are in fact
interdependent and should be addressed together, and we propose a wavelet graph analysis
approach to solve the problem. The graph-based wavelet analysis is useful because it can re-
veal both spatial localization (spread) and spectral localization (uniformity of usages) of the
topics simultaneously on the graph. As an example we collect data from a political blogger
network. In these blogs we identify about 10000 keywords of interest and extract T = 10
topics by using the LDA model [BNJ03]. Each topic then corresponds to a distribution
vector of word-frequencies (fractions). Further, the distribution of topics, i.e., projections
of a word frequency vector used by a blogger onto the topic word frequency vectors, can be
represented as a signal on this graph. In Figure 9.3.1, the distribution of a topic is inter-
preted as a signal mapped to nodes of the graph and represented as a circle on each node
with the size of the circle being proportional to the magnitude of the signal at that node.
We demonstrate our analysis by choosing two example topics. The topics are represented
as graph-signals on the graph in Figures 9.3.1(i) and (iii) and we observe no discernible
diﬀerence in the input distributions of the two topics. We then apply a two-level lifting
wavelet ﬁlterbank [NO09] to these signals. In this design, the vertex set is ﬁrst partitioned
into sets of even and odd nodes V = O ∪E. The odd nodes compute their prediction
coeﬃcients using their own data and data from their even neighbors followed by even nodes
computing their update coeﬃcients using their own data and prediction coeﬃcient of their
neighboring odd nodes. The even nodes are reconnected for higher resolution lifting steps to
create a new downsampled graph and the above lifting steps are repeated. We observe that in
the spatial domain (graph) representation of the signal, the low-pass transform coeﬃcients of
topic2 have signiﬁcantly higher magnitudes than the low-pass coeﬃcients of topic1 especially
around the node 41 (compare Figures 9.3.1 ii and 9.3.1 iv). Since the low-pass coeﬃcients in
a wavelet-transform provide average approximation of original signal over a certain (in this
case a 2-hop) neighborhood, we surmise that topic2 is more uniformly distributed in a 2-hop
neighborhood around each blogger whereas the usage of topic1 ﬂuctuates in the neighborhood
of each node. Further the node 41 and its 2-hop neighborhood is a major source of topic2
generation. In the spectral domain representation of the graph-signals of the two topics
(Figure 9.3.2) we observe that the fraction of energy situated in lower (higher) spectral
modes of the graph is higher (lower) for topic2 than topic1 (compare Figures 9.3.2 a and
9.3.2(d)). According to the spectral interpretation of graph-signals, this implies that globally
topic2 is more uniformly distributed on the graph than topic1. Further, Figure 9.3.2 shows
the energy distribution of the graph signals reconstructed from only low-pass coeﬃcients in
the spectral domain. The spectral coeﬃcients αn are obtained by projecting topic-signals
onto nth eigenvector of the graph. The y-axis in all the plots in Figure 9.3.2 shows energy in
each coeﬃcient. In Figures 9.3.2(b) and 9.3.2(e), we observe that the signal corresponding to
topic2 has more energy concentrated at low frequencies (conﬁrmed by the higher magnitude
of low-frequency coeﬃcients for topic2). This is consistent with the facts, since the top
keywords corresponding to topic1 consist of speciﬁc names or nouns and hence likely to be
used sparingly and by small focused groups of bloggers (therefore non-uniform distributed)
and the top keywords for topic2 are relatively commonplace and likely to be used more or
less uniformly by all bloggers (hence uniformly distributed). Refer to Table 9.3.1.

284
Graph-Based Social Media Analysis
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Topic1 distribution on original graph
50
18
38
47
17
7
8
15
12
21
1
31
5
24
11
48
16
44
20
10
22
26
43
3
4
54
42
19
30
39
46
41
33
2
28
13
32
53
55
29
56
6
9
27
14
23
25
34
35
36
37
40
45
49
51
52
57
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Topic1 distribution on downsampled graph after lifting wavelet transform
17
38
47
50
7
21
18
1
48
31
16
10
5
22
8
15
12
24
11
44
20
26
43
3
4
54
42
19
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Topic2 distribution on original graph
50
41
8
38
18
3
5
1
56
28
7
47
31
10
29
20
30
11
17
44
54
5
24
46
22
21
39
26
42
48
12
19
16
53
4
55
43
33
9
27
2
32
13
6
1
14
23
25
34
35
36
37
40
45
49
51
52
57
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Topic2 distribution on downsampled graph after lifting wavelet transform
41
38
3
8
56
10
50
29
28
17
30
31
44
20
18
15
7
47
11
54
5
24
46
22
21
39
26
42
Low-pass nodes
High-pass nodes
41
(i)
(ii)
(iii)
(iv)
FIGURE 9.3.1: Graph representation of blogger network data of N = 59 blogger and M =
3500 blogs. (i) initial distribution of topic1 signal on the graph, (ii) transform coeﬃcients
after a two-level wavelet transform (blue-circles: low-pass nodes and red-circles: high-pass
nodes) similarly for topic2 (iii) initial distribution (signal) and (iv) transform coeﬃcients
after a two-level wavelet transform. (See color insert.)
TABLE 9.3.1: Top keywords corresponding to the example topics chosen for analysis.
Topic
Top Keywords
topic1
clinton, obam, huckab, mccain, delegat, poll, romney, prim,
sen, win, msnbc, memorabl, caucus, race, hill
topic2
peopl, year, govern, use, law, work, said, americ, iraq,
need, make, world, case, reason, right
9.3.2
Interpolation in graphs
An important area of research is the interpolation problem in graph structured data. This
arises in many guises, such as in semi-supervised learning of categorical data (see [KL02])
and missing value prediction such as matrix-completion problems [BLN07]. A common
theme in all these applications is that the goal is to predict the property of some nodes (class,
ranking or function), by interpolating the property values from a known set of nodes. The
accuracy of all linear and non-linear interpolation methods on graphs rely on the implicit

Graph Signal Processing in Social Media
285
FIGURE 9.3.2: Spectral representation of blogger network data: (a) topic1 signal (b) recon-
structed topic1 signal from only low-pass coeﬃcients of the lifting transform and (c) from
only high-pass coeﬃcients of lifting transform, (d) topic2 signal (e) reconstructed topic2
signal from only low-pass coeﬃcients and (f) from only high-pass coeﬃcients of lifting
transform.
assumption that nodes close to each other (in terms of the similarity captured by link-
weights in the graph) usually have similar signal values. For example, in an item-item
graph in a recommendation system, a typical user rates two similar items with similar
ratings. In the same way, when predicting the functions of unannotated proteins based
on a protein network, one relies on some notions of “closeness” or “distance” among the
nodes. In other words, the graph functions of interest are slowly-varying or “smooth” on
the graph. Smoothness is well studied in the classical signal processing domain, where it is
measured in terms of the frequency contents of a signal. For example, there are well known
sampling results for recovering smooth bandlimited signals from only a few of their samples
via interpolation.
We focus on the development of similar interpolation techniques for graph signals. The
main objective in designing an interpolation method is, of course, to achieve high accuracy.
A major challenge in achieving this goal is managing to do so with reasonable computa-
tional complexity. Graph based interpolation approaches can be broadly divided into two
categories: a) local methods and b) global methods. In local interpolation methods, such as
k-nearest neighbors (kNN) methods, the predicted value at an unknown node is computed
as a weighted combination of k-nearest known samples [CFS09]. Although the method is
computationally simple and eﬃcient, it does not capture global information, as well as the

286
Graph-Based Social Media Analysis
0.11789
0.75042
 
 
 kNN graph
Unrated
vertex
(a)
0.11789
1.1475
 
 
 complete graph
Unrated 
vertex
(b)
FIGURE 9.3.3: An instance of predicting ratings of an unknown movie node (in red) using
ratings of a known set of movie nodes (in blue), in MovieLens 100k dataset: (i) star graph
and (ii) alternative graph that contains the star graphs and all the links between movies in
the known set of movies. (See color insert.)
dependencies that exist between known samples. On the other hand, global methods, such
as [BN04, GS03], predict the value of all unknown nodes at once, by selecting as the so-
lution a function that matches the values at known nodes while satisfying certain global
“smoothness” conditions. Global methods are computationally more expensive but provide
more accurate results. However, it is not clear how to optimize the choice of objective
function. Although the graph signal processing method described below leads to a similar
least-squares based interpolation as in [BN04], the signal processing perspective allows us to
choose an optimal objective criterion, which will be shown to lead to better interpolation.
Another problem is the choice of graph to interpolate the data upon. The example in Fig-
ure 9.3.3 demonstrates the choice of either operating on a bare-bones star graph commonly
used in kNN [CFS09] prediction methods or an alternative graph that contains the star
graphs and all the links between movies in the known set of movies.
Interpolation can be deﬁned on both graphs, so it is unclear which of the two (or any
other graph) should be chosen. The approach described below takes inspiration from signal
processing techniques to formulate the partially known graph function as a downsampled-
upsampled (DU) signal. In the regular signal domain, the original signal is recovered from its
DU signal by applying a low-pass ﬁlter. Similarly, in the graph domain, we design low-pass
ﬁlters to recover the original graph signal from its DU signal (see Figure 9.3.4).
9.3.2.1
Movie recommendation system
Let us design an interpolation method for collaborative ﬁltering in recommendation
systems. The input in this problem is a partially observed user-item rating matrix R, such
that R(u, m) is the rating given by user u to the movie m. Based on this information, the
system predicts new user-item ratings. This problem is usually solved in two parts. During
training, we create a model for how the items or users are correlated with each other, either
by clustering them into latent dimensions as in [SM08] methods, or by explicitly ﬁnding the
correlation between them as described below. In testing or application, we take the known
ratings of a given users and then propagate to unknown items via interpolation. Speciﬁcally,
let’s choose the MovieLens 100k [MOV03] dataset containing 100k user-movie-rating triplets
from N = 943 users and M = 1682 movies. The ratings are integer values between 1 and 5.

Graph Signal Processing in Social Media
287
M
M
f(t) with F(ejω) = 0
for ω > π/M
-π/M
π/M
fdu(t)
f(t)
S
Sc
Sc
Sc
Smoothness of f(v)?
Optimal transform?
f(v)
f(v)
fS(v)
FIGURE 9.3.4: Graph interpolation problem represented as reconstruction of DU graph
signal.
0
100
200
300
400
500
600
700
1
1.5
2
2.5
3
Number of training samples
Cumulative RMSE
 
 
LS projection (K= K* + 10)
kNN (k = 30)
PMF
Proposed method (K = K*)
Proposed Method with bilateral weights
FIGURE 9.3.5: RMSE of diﬀerent prediction algorithms with the number of training samples
on MovieLens dataset. (See color insert.)

288
Graph-Based Social Media Analysis
We use the 5-fold cross validation data available in [MOV03], which consists of 5 disjoint
random sets of 20k triplets. At each iteration, one of these sets is used for testing and
remaining sets for training.
We ﬁrst compute a movie graph G0 from the training samples, by computing cosine
similarity [SKKR01] between every pair of movies. For each test user u, we deﬁne S to be
the set of movies with known ratings, and U to be the set of test movies. We compute the
subgraph Gu = (S ∪U, Eu), corresponding to subset S ∪U of nodes. We deﬁne DU signal
fu for u to be of size |U ∪S|, with fu(U) = 0 and fu(S) equal to known ratings. The goal
now is to predict the value of fu(U).
Subsequently, we compute interpolated signal ˆfu by using a method proposed in
[NGO13], with Gu and fu as inputs. This method is very similar to the least squares (LS)
interpolation of [BN04], except that here we operate on a normalized Laplacian matrix, and
choose K∗speciﬁed by in [NGO13]. To show that this K is a good choice, we also implement
the method in [BN04] with K = K∗+10 (i.e., with 10 additional eigenvectors), respectively.
Additionally, since the smoothness of a graph signal depends both on the signal values and
the underlying graph, we can modify the graph to adapt to the given signal so that the
signal is more band-limited on the simpliﬁed graph, thus leading to less interpolation error.
The simpliﬁcation makes sense in many cases such as in recommendation systems, where
the underlying graph is the result of observing average correlation over a set of training
users (multiple instances), and the signal corresponds to a single test user. This kind of
signal adaptive ﬁltering is achieved by replacing edge weights with bilateral ﬁlters [TM98].
The comparison of the performance of GSP method with two most popular algorithms
for collaborative ﬁltering: 1) kNN method [SKKR01] (with k = 30), and 2) probabilistic
matrix factorization (PMF) [SM08] with 10 latent features is shown in Figure 9.3.5. We
use root mean square error (RMSE) between predicted values and actual values to measure
performance. The predicted values less than 0 (more than 5) are set to 0 (5) before comput-
ing RMSE. Figure 9.5(a) plots the RMSE of prediction as a function of number of training
samples available. Observe that kNN method perform the worst of all method, and PMF
method performs the best. The GSP methods, both with or without bilateral weighting, are
very close to PMF method, with the interpolation with bilateral weights performing slightly
better. We also observed that choosing K = K∗+ 10 in LS method leads to poorer results.
The Figure 9.5(b) shows another RMSE plot where users are grouped by the number of
training samples, with x-axis showing those groups. We observe that both the LS method
and kNN method perform signiﬁcantly worse when the number of available training samples
are small. The eﬀect of applying bilateral weighting in our proposed methods is also most
visible here.
The PMF method predicts the ratings of all movies for all users simultaneously by fac-
torizing the whole N ×M rating matrix. It is based on an iterative update rule and requires
O(NMP) operations per iteration where P is the size of the latent space. Theoretically,
any change to the rating matrix would require the PMF system to be retrained on all
users. However, in our method, the process of computing the movie graph is decoupled
from the process of predicting ratings for a given user. Accommodating a few new ratings
in the systems is fast, as it only aﬀects a local portion of the movie graph. Once the movie
graph is ﬁxed, the proposed method allows us to predict the ratings of movies for each
user separately in O(K∗M 2) operations. Thus, assuming K∗≈P, the proposed method
is faster (i.e., M 2 < NM) than PMF, when ratings of items change frequently and the
recommendations need to be calculated only when a user requests them. The comparison
of computational complexity is given in Table 9.3.2. Further, it may be possible to reduce
complexity in our method by using simpliﬁed ﬁltering operations.

Graph Signal Processing in Social Media
289
TABLE 9.3.2: Computational Complexity of Interpolation Methods. M = #(movies), N =
#(users), R = #(latent factors), K = #(eigenvalues less than cut-oﬀ)
PMF
Graph
Signal
Interpola-
tion
Training
#(iterations) ×O(NMR)
O(NM 2)
Update (new user/movie
added)
#(iterations) ×O(NMR)
O(1)
Prediction (per user, per
movie)
O(R)
O(KM)
9.4
Conclusions
In this chapter, we learned about graph signal processing (GSP) techniques for analyzing
social media. We described important concepts of GSP, including ﬁltering, vertex sampling,
and interpolation. We described two applications of these techniques: one in analyzing blog-
ger network data and second in designing a movie recommendation system. The preliminary
research in GSP breaks ground for many new and interesting problems. Both a signal on
a graph with N vertices and a classical discrete-time signal with N samples can be viewed
as vectors in RN. However, there is a large gap in the knowledge between DSP techniques,
and the techniques designed for GSP. The future direction of GSP is to narrow this gap by
developing a more systematic understanding of fundamental concepts, such as translation,
modulation, sampling, and ﬁltering etc. for graphs. Graph based data is found in diverse
applications such as healthcare, social networks, intelligence, system biology, power grids,
and large scale simulations, which are full of semantically rich relationships. In the future,
GSP techniques will solve challenging problems in Big-Data using GSP ideas.
Bibliography
[BLN07]
J. Bennett, S. Lanning, and N. Netﬂix. The Netﬂix prize. In Proc. Knowledge
Discovery and Data Mining Cup and Workshop, 2007.
[BN04]
M. Belkin and P. Niyogi. Semi-supervised learning on Riemannian manifolds.
Machine Learning, 56(1-3):209–239, 2004.
[BNJ03]
D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. Journal
of Machine Learning Research, 3:993–1022, 2003.
[CFS09]
J. Chen, H. Fang, and Y. Saad. Fast approximate kNN graph construction
for high dimensional data via recursive Lanczos bisection. Journal of Machine
Learning Research, 10:1989–2012, 2009.
[Chu97]
F. R. K. Chung. Spectral Graph Theory. American Mathematical Society, 1997.
[CK03]
M. Crovella and E. Kolaczyk. Graph wavelets for spatial traﬃc analysis. In
Proc. IEEE INFOCOM, volume 3, pages 1848–1857, 2003.

290
Graph-Based Social Media Analysis
[CM06]
R. R. Coifman and M. Maggioni. Diﬀusion wavelets. Applied and Computa-
tional Harmonic Analysis, 21(1):53 – 94, 2006.
[DGLS01]
E. B. Davies, G. M. L. Gladwell, J. Leydold, and P. F. Stadler. Discrete nodal
domain theorems. Linear Algebra and its Applications, 336(1-3):51–60, 2001.
[EFAR13]
V. Ekambaram, G. Fanti, B. Ayazifar, and K. Ramchandran. Multiresolution
graph signal processing via circulant structures. In Proc. 2013 IEEE Digital
Signal Processing and Signal Processing Education Meeting (DSP/SPE), pages
112–117, 2013.
[GAO14]
A. Gadde, A. Anis, and A. Ortega. Active semi-supervised learning using sam-
pling theory for graph signals. In Proc. of the 20th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pages 492–501, 2014.
[GNC10]
M. Gavish, B. Nadler, and R. R. Coifman. Multiscale wavelets on trees, graphs
and high dimensional data: Theory and applications to semi supervised learn-
ing. In Proc. of the 27th International Conference on Machine Learning (ICML-
10), pages 367–374, 2010.
[GS03]
L. Grady and E. L. Schwartz. Anisotropic interpolation on graphs: The com-
binatorial Dirichlet problem. Technical report, Boston University, 2003.
[HJCTH12] M. Hasegawa-Johnson, S. M. Chu, H. Tang, and T. S. Huang. Partially super-
vised speaker clustering. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 34(5):959–971, 2012.
[HVG11]
D. K. Hammond, P. Vandergheynst, and R. Gribonval. Wavelets on graphs
via spectral graph theory.
Applied and Computational Harmonic Analysis,
30(2):129–150, 2011.
[JNS09]
M. Jansen, G. P. Nason, and B. W. Silverman. Multiscale methods for data on
graphs and irregular multidimensional situations. Journal of the Royal Statis-
tical Society. Series B (Statistical Methodology), 71(1):97–125, 2009.
[KL02]
R. Kondor and J. Laﬀerty.
Diﬀusion kernels on graphs and other discrete
structures. In Proc. International Conference on Machine Learning, pages 315–
322, 2002.
[Kra12]
A. D. Kramer. The spread of emotion via Facebook. In Proc. of the SIGCHI
Conference on Human Factors in Computing Systems, pages 767–770, 2012.
[MEO11]
E. Martinez-Enriquez and A. Ortega. Lifting transforms on graphs for video
coding.
In Proc. Data Compression Conference (DCC), 2011, pages 73–82,
2011.
[MOV03]
MovieLens dataset, as of 2003. http://www.grouplens.org/.
[NA12]
S. Narang and O. A. Perfect reconstruction two-channel wavelet ﬁlter-banks for
graph structured data. IEEE Transactions on Signal Processing, 60(6), 2012.
[NA13]
S. Narang and O. A. Compact support biorthogonal wavelet ﬁlterbanks for
arbitrary undirected graphs. IEEE Transactions on Signal Processing, 61(19),
2013.

Graph Signal Processing in Social Media
291
[NGO13]
S. K. Narang, A. Gadde, and A. Ortega.
Signal processing techniques for
interpolation in graph structured data. In Proc. IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP), pages 5445–5449, 2013.
[NO09]
S. K. Narang and A. Ortega.
Lifting based wavelet transforms on graphs.
Asia-Paciﬁc Sig. and Information Proc. Association, 2009, (APSIPA ASC’
09), pages 441–444, 2009.
[NO11]
S. Narang and A. Ortega.
Downsampling graphs using spectral theory.
In
Proc. IEEE International Conference on Acoustics, Speech and Signal Process-
ing (ICASSP), pages 4208–4211, 2011.
[OKH14]
B. Oselio, A. Kulesza, and A. Hero. Multi-layer graph analysis for dynamic so-
cial networks. IEEE Journal of Selected Topics in Signal Processing, 8(4):514–
523, 2014.
[Pes08]
I. Pesenson. Sampling in Paley-Wiener spaces on combinatorial graphs. Trans-
actions American Mathematical Society, 360(10):5603–5627, 2008.
[SFV13]
D. I. Shuman, M. J. Faraji, and P. Vandergheynst. A framework for multiscale
transforms on graphs. Computing Research Repository, 2013.
[SKKR01]
B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Item-based collaborative ﬁlter-
ing recommendation algorithms. In Proc. of the 10th International Conference
on World Wide Web, pages 285–295, 2001.
[SKN+10]
G. Shen, W. Kim, S. Narang, A. Ortega, J. Lee, and H. Wey. Edge-adaptive
transforms for eﬃcient depth map coding. In Proc. Picture Coding Symposium
(PCS), 2010, 2010.
[SM08]
R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. In Proc.
Advances in Neural Information Processing Systems, volume 20, pages 1257–
1264, 2008.
[SM13]
A. Sandryhaila and J. Moura. Discrete signal processing on graphs: Graph
ﬁlters. In Proc. IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pages 6163–6166, 2013.
[SM14]
A. Sandryhaila and J. Moura. Discrete signal processing on graphs: Frequency
analysis. IEEE Transactions on Signal Processing, 62(12):3042–3054, 2014.
[SO08]
G. Shen and A. Ortega. Optimized distributed 2D transforms for irregularly
sampled sensor network grids using wavelet lifting. In Proc. IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 2513–
2516, 2008.
[TM98]
C. Tomasi and R. Manduchi. Bilateral ﬁltering for gray and color images. In
Proc. 6th International Conference on Computer Vision, pages 839–846, 1998.
[VK95]
M. Vetterli and J. Kovaˇcevic. Wavelets and subband coding. Prentice-Hall, Inc.,
1995.
[WR06]
W. Wang and K. Ramchandran. Random multiresolution representations for
arbitrary sensor network graphs. In Proc. of the 5th International Conference
on Information Processing in Sensor Networks, pages 102–108, 2006.

292
Graph-Based Social Media Analysis
[WWL+11] X. Wang, F. Wei, X. Liu, M. Zhou, and M. Zhang.
Topic sentiment anal-
ysis in Twitter: A graph-based hashtag sentiment classiﬁcation approach. In
Pro. of the 20th ACM International Conference on Information and Knowledge
Management, pages 1031–1040, 2011.
[Zac77]
W. W. Zachary. An information ﬂow model for conﬂict and ﬁssion in small
groups. Journal of Anthropological Research, 1977.

Chapter 10
Big Data Analytics for Social Networks
Brian Baingana, Panagiotis Traganitis, Georgios Giannakis
University of Minnesota, U.S.A.
Gonzalo Mateos
University of Rochester, U.S.A.
10.1
Introduction ....................................................................
294
10.1.1
Signal processing for big data .........................................
294
10.1.2
Social network analytics problems ....................................
295
10.2
Visualizing and reducing dimension in social nets .............................
296
10.2.1
Kernel-based graph embedding ........................................
296
10.2.2
Centrality-constraints
.................................................
298
10.2.3
Numerical tests ........................................................
300
10.2.4
Visualization of dynamic social networks .............................
300
10.3
Inference and imputation on social graphs ....................................
303
10.3.1
Distributed anomaly detection for social graphs ......................
303
10.3.1.1
Anomaly detection via sparse plus low-rank
decomposition ............................................
303
10.3.1.2
In-network processing algorithm .........................
305
10.3.1.3
Numerical tests ..........................................
306
10.3.2
Prediction from partially-observed network processes ................
307
10.3.2.1
Semi-supervised prediction of network processes ........
308
10.3.2.2
Data-driven dictionary learning .........................
309
10.3.2.3
Numerical tests ..........................................
310
10.4
Unveiling communities in social networks .....................................
311
10.4.1
Big data spectral clustering ...........................................
312
10.4.1.1
Numerical tests ..........................................
315
10.4.2
Robust kernel PCA ....................................................
316
10.4.2.1
Numerical tests ..........................................
319
10.5
Topology tracking from information cascades .................................
319
10.5.1
Dynamic SEMs for tracking cascades .................................
321
10.5.1.1
Model and problem statement ...........................
322
10.5.1.2
Exponentially-weighted least-squares estimator .........
323
10.5.2
Topology tracking algorithm ..........................................
324
10.5.2.1
Accelerated convergence .................................
326
10.5.3
Real-Time operation ...................................................
327
10.5.3.1
Premature termination
..................................
327
10.5.3.2
Stochastic gradient descent iterations ...................
327
10.5.4
Experiments on real data ..............................................
328
10.6
Conclusion ......................................................................
330
10.7
Acknowledgments ..............................................................
330
Bibliography ....................................................................
330
293

294
Graph-Based Social Media Analysis
10.1
Introduction
The last few years have witnessed a dramatic upswing in the volume, variety, and acqui-
sition rate of data from disparate sources. Among other factors, this “data deluge” has been
fueled by the ubiquity of the web, pervasive deployment of low-cost sensors, cheaper storage
memory, and growing awareness that data is a key asset from which valuable insights can be
unlocked. Social networks lie at the forefront of this revolution, presenting ample opportuni-
ties to address several challenges associated with big data. Examples include micro-blogging
services (e.g., Twitter), web-based friendship networks (e.g., Facebook), and online product
reviews (e.g., Yelp).
Exploiting the immense big data opportunities comes at the cost of overcoming signiﬁ-
cant challenges. First, traditional analytics are ill-equipped to cope with the sheer volume of
data. Many distributed platforms (e.g., Hadoop/MapReduce and GraphLab) have emerged
to tame the scale of data. Nevertheless, only a subset of algorithms are readily implementable
on these platforms, and development of more versatile architectures is an active research
area.
In addition, most social data are high-dimensional, with many features. In order to
ﬁnesse the curse of dimensionality, parsimonious models must be devised for feature subset
selection. Since most big data are acquired sequentially as streaming inputs, batch learning
and optimization approaches are impractical. For example, securities traders interested in
capturing market sentiment from real-time tweets are driven by the need for split-second
buy/sell decisions. Furthermore, big data acquisition pipelines are plagued by measurement
inaccuracies, misses, and incompleteness due to, e.g., privacy concerns.
10.1.1
Signal processing for big data
Signal processing (SP) provides a principled framework within which big data chal-
lenges can be readily addressed [SGM14]. Advances in SP have formalized parsimonious
models that exploit key properties inherent to big data, e.g., sparsity, low-rank, and man-
ifold structures. For example, several contemporary models jointly capture low rank and
sparsity (see, e.g.,
[MMG13c]), subsuming several learning paradigms such as principal
component analysis (PCA) [HTF09], dictionary learning (DL) [OF97], and compressive
sampling (CS) [CW08].
Scaling to very large problem instances, while attaining real-time operation are notewor-
thy themes that have shaped the direction of contemporary research eﬀorts. The alternating
direction method of multipliers (ADMM) has enjoyed growing popularity in decentralized
learning and optimization algorithms, especially in settings where data resides over a net-
work of computing nodes [MBG10, BPC+11, MMG13a].
For streaming data, online learning has emerged as a powerful framework for real-time
analytics [MBPS10, KST11, MMG13b]. Popular online algorithms including online mirror
descent, and online gradient descent have been studied and deployed in practical social
network settings; see e.g., [SS11] for a comprehensive review. Figure 10.1.1 depicts various
themes for which SP and learning oﬀer a comprehensive framework.
In addition to decentralized computation, signiﬁcant improvements in running time have
been realized through random sampling and random projection algorithms [Mah11]. These
methods operate on a randomized sketch of an input data matrix by either: i) sampling
a small subset of rows of the matrix that are most informative (random sampling); or
ii) linearly combining a small number of rows of the matrix (random projection). Under

Big Data Analytics for Social Networks
295
Signal Processing
and Learning
for Big Data
Tasks
Dimensionality
reduction
Regression,
Classiﬁcation,
Clustering
Cleansing,
Imputation
Prediction,
Forecasting
Models and
Optimization
Parallel,
Decentralized
Time/Data
adaptive
Robust
Succint,
Sparse
Challenges
Cloud
storage
Real-time
constraints
Outliers,
Missing
values
Massive
scale
FIGURE 10.1.1: Signal processing in the context of Big Data analytics [SGM14].
reasonable conditions, these randomized approaches can in theory attain asymptotically
faster worst-case running times than deterministic alternatives.
10.1.2
Social network analytics problems
Initial social network studies were exploratory in nature and unveiled surprising prop-
erties common to large networks e.g., the existence of power laws, small-world properties,
and preferential attachment strategies for network growth [New10, EK10]. The exponential
growth of the web coupled with ground-breaking advances in ﬁelds as diverse as compu-
tational biology and epidemiology, have led to a dramatic change in the scope and size of
problems studied. A few examples include the ranking of web pages [BP98, CDK+99], the
discovery of causal interactions in gene regulatory networks [CBG13], and the prediction of
the spread of infectious diseases [VR07, Jac10].
Looking at network science through the lens of statistical learning, several contemporary
problems boil down to (non-)parametric regression, dimensionality reduction, clustering, or
(semi-)supervised classiﬁcation. “Work-horse” dimensionality reduction approaches such as
multidimensional scaling (MDS) have been advocated for network visualization tasks [KK89,
BP11, BG13]. Topology inference problems from network processes have been the focus
of several recent works [RLS10, BMG14]. Another line of research has concentrated on
virus (also information, buying patterns) propagation models over complex networks [Het00,
EK10]. Other interesting topics include community discovery [GN02, For10], prediction of
network processes from partial observations [Kol09, FRG14], and detection of anomalies in
social networks [MG12b]. This chapter is representative of a subset of problems, for which
the identiﬁed big data challenges are addressed by leveraging statistical learning advances.
Section 10.2 focuses on scalable visualization of social networks via nonlinear dimen-
sionality reduction. Inference and imputation of corrupted signals over social graphs is the
topic of Section 10.3. This is followed by Section 10.4, which presents algorithms for com-

296
Graph-Based Social Media Analysis
munity discovery in big social networks. Finally, Section 10.5 presents recent algorithms for
tracking topologies of dynamic social networks that facilitate diﬀusion of network processes.
Admittedly, these approaches represent a small fraction of the gamut of social network an-
alytics methods. Nevertheless, they are representative of a broader class of contemporary
tools that are relevant to big data analytics in social networks.
10.2
Visualizing and reducing dimension in social nets
Network visualization is often accomplished through graph embedding, which entails
mapping each node to a point in Euclidean space. Due to the data deluge spawned by modern
network-based phenomena, the rising complexity and sheer volume of networks present
new opportunities and challenges for graph embedding tools that capture global patterns
and use visual metaphors to convey meaningful structural information e.g., hierarchy, node
similarity, and natural communities [HL12].
Most traditional visualization algorithms trade oﬀthe clarity of structural characteristics
of the underlying network for aesthetic requirements like minimal edge crossing and ﬁxed
edge lengths; e.g., [KK89, PT98, DBD13]. Although eﬃcient for small graphs (hundreds of
nodes), embeddings for larger graphs generated by classical approaches are seldom struc-
turally informative. To this end, several approaches have been developed for embedding
graphs while preserving speciﬁc structural properties. Pioneering methods (e.g., [KK89])
have resorted to multidimensional scaling (MDS), which seeks a low-dimensional repre-
sentation of high-dimensional data so that pairwise dissimilarities are preserved through
Euclidean distances in the embedding [BG05, BG13]. In this case, the vertex dissimilar-
ity structure is preserved through pairwise distance metrics in the embedding. Spectral
embeddings whose coordinates consist of entries in the leading principal components of
the network adjacency matrix are advocated in [LWH03]. The structure-preserving em-
bedding algorithm solves a semideﬁnite program with linear topology constraints that em-
phasize reconstructability of the graph through neighborhood methods [SJ09]. Other vi-
sualization methods emphasize community structure [YLZ+13], while concentric layouts
emphasize node hierarchy by placing the highest ranked nodes at the center of the embed-
ding [AHDBV06, BP11, BG13].
Despite the rich history associated with graph drawing methods, development of visual-
ization techniques that eﬀectively capture hierarchical structure and other global patterns
remains a challenging and active area of research. This section showcases a recently de-
veloped kernel-based visualization approach that leverages local linear embedding (LLE),
a popular manifold learning technique [BG15]. Similar to recent works on graph embed-
ding, node importance is captured through centrality constraints [BP11, BG13]. In general,
centrality measures provide a means to assign a level of importance to each node in a
network [Sab66, Fre77]. For instance, betweenness centrality describes the extent to which
information is routed through a speciﬁc node by measuring the fraction of all shortest
paths traversing it; see e.g., [Kol09, p. 89]. Other measures include closeness and eigenvalue
centrality.
10.2.1
Kernel-based graph embedding
Consider a network represented by an undirected graph G = (V, E), where E denotes the
set of edges, and V the set of vertices with cardinality |V| = N. Suppose the structure of

Big Data Analytics for Social Networks
297
G is captured by its so-termed adjacency matrix A whose (i, j)-th entry (hereafter denoted
by aij) is zero only if edge (i, j) /∈E, otherwise it denotes the weight of (i, j). Given G
and a prescribed embedding dimension p (typically p ∈{2, 3}), the graph embedding task
is tantamount to searching for the set of p × 1 vectors X := {xi}N
i=1 which “eﬀectively”
capture the underlying local graph structure.
Suppose {yi ∈Rq}N
i=1 are data points sampled from a nonlinear manifold. LLE
seeks the low-dimensional vectors {xi ∈Rp}N
i=1 (p
≪
q) that preserve the local
neighborhood structure on the manifold. First, the neighborhoods {Ni}N
i=1 are con-
structed per datum by selecting the K-nearest neighbors of i, or setting Ni
:=
{yj ∈Rq : ∥yi −yj∥2 ≤ϵ, ϵ > 0, j = 1, . . . , N}. Assuming |Ni| = K, each point is then ﬁt
to a linear combination of its neighbors by solving the following constrained least-squares
(LS) optimization problem
arg min
 wi1,...,wiK
P
j∈Ni wij=1


yi −
X
j∈Ni
wijyj

2
2
i = 1, . . . , N,
(10.2.1)
where {wij}K
j=1 are the reconstruction weights for point i, while the constraint enforces
shift invariance. Setting wij = 0 for j /∈Ni, the ﬁnal step determines {xi ∈Rp}N
i=1 so that
reconstruction weights are preserved by solving:
arg min



x1,...,xN
PN
i=1 xi=0
1
N
PN
i=1 xixT
i =I



N
X
i=1

xi −
N
X
j=1
wijxj

2
2
.
(10.2.2)
The equality constraints are included to eliminate the trivial all-zero solution, and also to
eliminate shift and rotation ambiguities.
In order to tailor LLE for graph embedding where only weights {aij} are available, one
must contend with the general non-existence of high dimensional vectors {yi}N
i=1 deﬁned
per node. Fortunately, the optimization problem (10.2.1) can be cast in terms of the inner
products yT
i yj for all i, j ∈{1, . . . , N}. This brings to bear the merits of kernel methods
which entail computations on inner products of transformed feature vectors, φ(y), namely
kij(yi, yj) = φT (yi)φ(yj). However, this ﬂexibility comes at the challenge of selecting the
best kernel matrix K ∈RN×N where [K]ij := kij(yi, yj). A few choices of K include:
i. The doubly-centered dissimilarity matrix K = −(1/2)J∆(2)J, where [∆(2)]ij denotes
the squared geodesic distance between nodes i and j, or any other dissimilarity metric
on the graph, and J := I −N −111T denotes the centering operator (I is the identity
matrix and 1 is the all-one column vector) [BG05]. In this case, K is reminiscent of the
kernel adopted by classical MDS.
ii. The Penrose-Moore pseudoinverse of the graph Laplacian; that is K = L†, where L :=
A−D, A ∈{0, 1}N×N denotes the binary graph adjacency matrix, and D := diag(A1).
It turns out that L† admits an intuitive interpretation as a similarity matrix based on
random walk distances on graphs [FPRS07].
iii. Matrix K = AAT , where A ∈{0, 1}N×N, and [AAT ]ij counts the number of single-hop
neighbors shared by nodes i and j.
Neighborhood selection in traditional LLE entails O
 qN 2
complexity with q ≫. This

298
Graph-Based Social Media Analysis
bottleneck can be overcome by setting Ni to the single-hop neighbors per node. Let Yi :=
[φ(yi
1), . . . , φ(yi
di)] collect the “virtual” transformed vectors associated per Ni, where di
denotes the degree of node i. Letting wi := [wi1, . . . , widi]T , the constrained LS ﬁt (10.2.1)
can be written as:
wi =
arg min
{w: 1T w=1}
wT Kiw −2wT ki,
(10.2.3)
where Ki := YT
i Yi and ki := YT
i φ(yi) are submatrices of K indexed by elements of Ni.
Resorting to Lagrange multiplier theory, one can readily solve for wi in (10.2.3) in closed
form. Moreover for large-scale graph embedding, (10.2.3) can be easily parallelized over clus-
ters of computing nodes. Each subproblem entails O(d3
i ) complexity, which is manageable
because typically di ≪N.
The low-dimensional graph embedding can be evaluated from the reconstruction weights
via (10.2.2), by solving for:
arg min



X
PN
i=1 xi=0
1
N
PN
i=1 xixT
i =I



Tr

XT (I −W)T (I −W)X

,
(10.2.4)
where WT := [ ˜w1, . . . , ˜wN], ˜wij = wij if j ∈Ni otherwise ˜wij = 0, XT := [x1, . . . , xN], and
Tr(.) denotes matrix trace. The solution comprises the 2nd to the (p + 1)st least dominant
eigenvectors of (I −W)T (I −W). For large graphs, X can be eﬃciently computed via
orthogonal iterations which are amenable to decentralization, entailing O(pN 2) complexity;
see e.g., [KM08].
Although this approach preserves the local graph topology deﬁned by single-hop neigh-
bors, the spectral decomposition is generally not scalable for very large networks. Moreover
for large network visualization tasks, one is often more interested in conveying global prop-
erties such as node hierarchy. To this end, the next subsection modiﬁes the embedding step
to enforce centrality constraints.
10.2.2
Centrality-constraints
Large-scale settings call for emphasis on structural properties such as node hierarchy over
aesthetics. Centrality measures impose a hierarchical ordering among nodes by quantifying
the relative importance of nodes over their peers e.g., degree, closeness, and betweenness
centralities [Kol09]. As a result, graph embedding under centrality constraints yields very
informative network visualizations. Let C(G) := {ci}N
i=1 denote the set of centralities of G,
with ci representing the centrality measure of node i. The goal is to determine the embedding
{xi}N
i=1 that eﬀectively “preserves” the centrality ordering in C(G).
Modifying the ﬁnal step in (10.2.2) to incorporate centrality constraints yields the fol-
lowing optimization problem:
arg min
x1,...,xN
N
X
i=1

xi −
N
X
j=1
wijxj

2
2
s. t.
∥xi∥2
2 = f 2(ci), i = 1, . . . , N,
(10.2.5)
where f(ci) is a monotone decreasing function of ci ensuring that more central nodes are
placed closer to the center. The dropped 0-mean constraint can be compensated for by a
post-processing centering operation upon determination of {ˆxi}N
i=1.
Problem (10.2.5) is non-convex without global optimality guarantees. Fortunately, the

Big Data Analytics for Social Networks
299
problem decouples over vectors {xi}N
i=1 motivating a block coordinate descent (BCD) ap-
proach [Ber99]. The optimization variables can now be partitioned into N blocks with xi
corresponding to block i. Consequently, during iteration r one cycles through all blocks by
solving:
xr
i = arg min
x

x −
X
j<r
wijxr
j −
X
j>r
wijxr−1
j

2
2
s. t.
∥x∥2
2 = f 2(ci).
(10.2.6)
Letting vr
i := P
j<r wijxr
j + P
j>r wijxr−1
j
and λ ≥0 denote a Lagrange multiplier, the
solution:
xr
i = arg min
x
∥x −vr
i ∥2
2 + λ(∥x∥2
2 −f 2(ci))
(10.2.7)
is given by:
xr
i =
vr
i
1 + λ.
(10.2.8)
Upon substitution of (10.2.8) into the equality constraint in (10.2.6), one obtains the closed-
form per-iteration update:
xr
i =
(
vr
i
∥vr
i ∥2 f(ci),
if ∥vr
i ∥2 > 0
xr−1
i
,
otherwise.
(10.2.9)
If Xr denotes the embedding matrix after r BCD iterations, the operation X = (I −
N −111T )Xr centers {xr
i }N
i=1 to the origin in order to satisfy the shift invariance prop-
erty of the embedding. Algorithm 10.2.1 summarizes the steps outlined for the centrality-
constrained graph embedding scheme. The only inputs to the algorithm are the graph
topology G, the centrality measures, {ci}N
i=1, the graph embedding dimension p, and the
kernel matrix L. Note that Algorithm 10.2.1 scales well to big data settings since both steps
can be computed in parallel. Moreover, (10.2.3) entails O(d3
i ) complexity, with di ≪N, and
the dimensionality of (10.2.6) is p ∈{2, 3}.
Algorithm 10.2.1: Centrality-constrained graph embedding
1: Input: G, C(G), K, ϵ, p
2: for i = 1 . . . N (in parallel) do
3:
Set Ni to single-hop neighbors of i
4:
Extract Ki and ki from K
5:
Solve wi = arg min
w
wT Kiw −2wT ki s. t. 1T w = 1
6:
Set wij = 0 for j /∈Ni
7: end for
8: Initialize X0, r = 0
9: repeat
10:
r = r + 1
11:
for i = 1 . . . N (in parallel) do
12:
Compute xr
i according to (10.2.9)
13:
Xr(i, :) = (xr
i )T
14:
end for
15: until ∥Xr −Xr−1∥F ≤ϵ
16: X = (I −1
N 11T )Xr

300
Graph-Based Social Media Analysis
10.2.3
Numerical tests
This section presents numerical tests conducted on a synthetic small-world network gen-
erated by the Watts-Strogatz model [WS98] and Gnutella, a real-world ﬁle-sharing network.
Given the number of nodes N, average degree ¯d, and β ∈[0, 1], the Watts-Strogatz model
constructs a ¯d-regular ring lattice and rewires each edge with probability β. The synthetic
graph was generated with N = 2 × 103, ¯d = 4, and β = 0.3. Several centrality measures are
available with emphasis on diﬀerent importance criteria. For instance, closeness centrality
captures the extent to which a particular node is close to all other nodes in the network,
and it is commonly deﬁned as ci := 1/(P
j∈V dij), where dij denotes the geodesic distance
between nodes i and j. For the experiments, closeness centralities were transformed as
follows:
f(ci) =
 cmax −ci
cmax −cmin

,
(10.2.10)
where cmax := maxi ci, and cmin := mini ci ∀i = 1, . . . , N.
Figure 10.2.1 depicts visualizations of the Watts-Strogatz network obtained by setting
N = 2, 000. In Figure 10.2.1 (a), (b), and (c), centrality-constrained embeddings are plotted
with kernel matrices K1 = (−1/2)J∆(2)J, K2 = AAT , and K3 = L†, respectively. The
appeal for centrality-constrained embeddings is clear when compared with Figure 10.2.1(d),
which depicts a “centrality-agnostic” graph embedding using K1. Here, the ﬁnal weight
preservation step entailed spectral decomposition of (I −W)T (I −W), consistent with the
original LLE algorithm. It is clear that even for a moderately sized synthetic graph, little
meaningful information can be conveyed visually from the centrality-agnostic embedding.
For instance, it is not obvious how an analyst would discern which nodes are most accessible
to peers, or those whose removal would compromise the rate of information propagation
over the network.
Figure 10.2.2 depicts visualizations of snapshots of the Gnutella peer-to-peer ﬁle-sharing
network [LKF07]. Directed edges represent connections between hosts. The snapshots were
captured on Aug. 4, 2012 (N = 10, 876, |E| = 39, 994) and Aug. 24, 2012 (N = 26, 518,
|E| = 65, 369), respectively. The adjacency matrices were symmetrized to obtain undirected
versions of the network. In this case K = AAT and C was set to the node degrees. It is
clear from the network drawings that despite the dramatic growth in the number of edges
over a 20 day span, most new nodes had low degree, and are located far from the center.
10.2.4
Visualization of dynamic social networks
This section has so far focused on addressing issues concerning structurally-informative
visualization of large static networks. However, these issues are exacerbated in settings
involving dynamic networks whose topologies evolve over time [BW97, BW98, BC03,
MMBd05, MMBd06, LS08, FT08]. Typically, one is given a time series of graphs {Gt}T
t=1,
representing static snapshots indexed by time intervals t = 1, . . . , T. Although it is possible
to compute a sequence of static embeddings, this approach does not scale to big data where
graph snapshots may encode millions of nodes, and may be acquired in a streaming fashion.
Moreover, the sequence of embeddings must “respect” the viewer’s mental map, which
captures a sense of stability of the underlying structure of the network across time intervals.
Sequential embeddings with drastic changes for a signiﬁcant number of node positions vi-
olate a viewer’s expectations, rendering mental reconciliation of temporal network changes
diﬃcult. Pioneering works have advocated a Bayesian framework that facilitates modeling
dependencies between consecutive graph layouts [BW97, BW98]. A more recent visualiza-
tion framework advocates regularized graph layouts, which involve optimizing a cost function
augmented with a penalty that enforces stability across timeslots [XKI12].

Big Data Analytics for Social Networks
301
(a)
(b)
(c)
(d)
FIGURE 10.2.1: Graph embeddings for a Watts-Strogatz graph with N
=
2, 000:
(a) Centrality-constrained embedding with K1 = (−1/2)J∆(2)J; (b) Centrality-constrained
embedding with K2 = AAT ; (c) Centrality-constrained embedding with K3 = L†; and
(d) Centrality-agnostic embedding based on kernel matrix K1. The color bar maps node
colors to varying centrality values. (See color insert.)
Instead of generating a sequence of stable embeddings, a recent approach promotes static
displays that capture temporal variations using non-negative matrix factorization for dimen-
sionality reduction [MM13]. It tacitly assumes that the number of nodes remains ﬁxed but
the edge connections vary with time. Static visual plots of rank-one matrix factors capture
the temporal evolution of node importance. Despite numerous eﬀorts, most contemporary
approaches are not tailored for big networks with streaming (and possibly missing) inputs,
a setup of growing research activity.

302
Graph-Based Social Media Analysis
(a) Gnutella-04 (08/04/2012)
(b) Gnutella-24 (08/24/2012)
FIGURE 10.2.2: Visualization of two snapshots of the large-scale ﬁle-sharing network
Gnutella [LKF07] based on degree centrality and K = AAT . (See color insert.)

Big Data Analytics for Social Networks
303
10.3
Inference and imputation on social graphs
Inference over graphs is a rich subject, and includes the typical (non-) parametric re-
gression, classiﬁcation, and clustering tasks. Here, we will focus on anomaly detection,
interpolation (a.k.a. imputation), and extrapolation (a.k.a. prediction).
10.3.1
Distributed anomaly detection for social graphs
This section deals with graph (network) anomaly identiﬁcation. On the one hand, exist-
ing approaches have dealt with the pursuit of abnormal behavior exhibited by processes that
evolve over graphs, such as Internet traﬃc ﬂows [LCD04, ZGGR05, MMG13b, MMG13c,
MR13], or spatiotemporal energy consumption proﬁles [CLL+10, MG12a, MG13], to name
a couple. But also of great interest for spam, fraud and network intrusion detection is to
consider the structure of the underlying graphs, and determine whether e.g., nodes, edges,
or egonets are anomalous in the sense that they deviate from postulated network models; see
e.g., [NC03, SQCF05, EH07, MT08, TL11, AMF12, MAB13] for noteworthy contributions.
In the sequel, a novel approach to social graph anomaly detection is outlined, which is based
on contemporary low-rank plus sparse matrix decompositions. A distributed algorithm is
then developed leveraging the alternating-directions method of multipliers (ADMM); see
e.g., [BT99, BPC+11].
10.3.1.1
Anomaly detection via sparse plus low-rank decomposition
The idea here is to consider the egonets in a social graph (i.e., each node’s induced single-
hop subgraph), and for each of them evaluate structural features or graph invariants such as,
average degree, number of nodes, principal eigenvalue, and average clustering coeﬃcient, to
name a few. It is thus possible to collect all these structural quantities in a feature×egonet
matrix Y. Speciﬁcally, let the D × 1 vector yn := [y1,n, . . . , yD,n]T collect D diﬀerent graph
features, calculated for each egonet n ∈[1, N] in a graph of N vertices. Consider the D ×N
graph feature matrix Y := [y1, . . . , yN]. The d-th row yT (d) of Y is the networkwide
sequence corresponding to feature d, measured for all egonets in the graph. It may be useful
to explicitly account for missing data, which could arise because not all features can be
calculated for all egonets, or when a subsampling strategy is implemented to reduce the
computational complexity in forming Y. To this end, consider the set Ω⊆{1, . . . , D} ×
{1, . . . , N} of index pairs (d, n) deﬁning a sampling pattern (or mask) of the entries of
Y. Introducing the matrix sampling operator PΩ(·), which sets the entries of its matrix
argument not indexed by Ωto zero and leaves the rest unchanged, the (possibly) incomplete
matrix of egonet features in the presence of outliers can be modeled as:
PΩ(Y) = PΩ(X + O + E),
(10.3.1)
where X, O, and E denote the nominal feature matrix, the outliers (i.e., anomalies), and
small approximation errors, respectively. For nominal egonet features yd,n = xd,n + ed,n,
one has no anomaly; that is od,n = 0.
The model is inherently under-determined, since even for the (most favorable) case of
full data, i.e., Ω≡{1, . . . , D} × {1, . . . , N}, there are twice as many unknowns in X and O
as there is data in Y. Estimating X and O becomes even more challenging when data are
missing, since the number of unknowns remains the same, but the amount of data is reduced.
In any case, estimation of {X, O} from PΩ(Y) is an ill-posed problem unless one introduces
extra structural assumptions on the model components to reduce the eﬀective degrees of

304
Graph-Based Social Media Analysis
freedom. To this end, two cardinal properties of X and O will prove instrumental. First, a
key observation is that for “nominal” complex networks most of these features obey power
laws [FFF99, Kol09, EK10], and hence Y (or its entrywise logarithm) will be approximately
low rank. Second, anomalies (or outliers) only occur sporadically across egonets and features,
yielding a sparse matrix O.
An estimator matching nicely the speciﬁcations of the graph anomaly detection problem
stated, is the so-termed “robust” (stable) principal components pursuit (PCP, also known as
RPCA for robust principal component analysis) [ZLW+10, CLMW11, CSPW11, MG12b],
that will be outlined here for completeness. PCP seeks estimates { ˆX, ˆO} as the minimizers
of:
(P1)
min
{X,O} ∥PΩ(Y −X −O)∥2
F + λ∗∥X∥∗+ λ1 ∥O∥1 ,
where the ℓ1-norm ∥O∥1 := P
d,n |od,n| and the nuclear norm ∥X∥∗:= P
i σi(X) (σi(X)
denotes the i-th singular value of X) are utilized to promote sparsity in the number of
outliers (nonzero entries) in O, and the low rank of X, respectively. The nuclear and ℓ1-
norms are the closest convex surrogates to the rank and cardinality functions, which albeit
the most natural criteria they are in general NP-hard to optimize [CG84, Nat95]. The tuning
parameters λ1, λ∗≥0 control the tradeoﬀbetween ﬁtting error, rank, and sparsity level of
the solution. When an estimate ˆσ2
v of the noise variance is available, guidelines for selecting
λ∗and λ1 have been proposed in [ZLW+10].
The location of nonzero entries in ˆO reveals “anomalies” across both features and
egonets, while their amplitudes quantify the magnitude of deviation. Clearly, it does not
make sense to ﬂag outliers in data that has not been observed, namely for (d, n) /∈Ω. In
those cases (P1) yields ˆod,n = 0 since both the Frobenius and ℓ1-norms are separable across
the entries of their matrix arguments.
A numerical test on the arXiv General Relativity and Quantum Cosmology collaboration
social graph [Les11] is depicted in Figure 10.3.1. The graph has 5, 242 vertices (authors)
and 14, 496 edges (indicating that two authors collaborated on at least one paper), and
there is no missing data. The red egonet is ﬂagged as anomalous, and it is apparent that
edge density (number of collaborations) is markedly larger than e.g., the other highlighted
peers (purple, green and magenta egonets). Beyond egonets, it is also possible to devise
algorithms to unveil anomalous graphs at a macro level. To this end, the relevant graph
invariants (rows of Y) should be evaluated for the whole network using e.g., the scalable
graph minining package Pegasus [KTF09], and across networks (columns of Y) for all those
graphs of interest in the analysis.
Being convex (P1) is computationally appealing, and it has been shown to attain good
performance in theory and practice. For instance, in the absence of noise and when there is
no missing data, identiﬁability and exact recovery conditions were reported in [CLMW11]
and [CSPW11]. Even when data are missing, it is possible to recover the low-rank com-
ponent under some technical assumptions [CLMW11]. Theoretical performance guarantees
in the presence of noise are also available [ZLW+10]. Regarding batch centralized algo-
rithms, a PCP solver based on the accelerated proximal gradient method was put forth
in [LGW+11, MMG13c], while the ADMM was employed in [YY13, MMG13c]. For a sin-
gle but dynamic network, detection of structural changes in time can be naturally ac-
commodated if the feature vectors (now time-indexed columns of Y) are recalculated per
time slot, and processed on-the-ﬂy using online graph algorithms for streaming data; see
also [PPY13, MAB13, MMG13b, WTPP14].

Big Data Analytics for Social Networks
305
FIGURE 10.3.1: An anomalous (depicted in red) egonet for the arXiv General Relativity
and Quantum Cosmology collaboration social graph [Les11]. The red egonet is ﬂagged as
anomalous by the proposed low-rank plus sparse matrix decomposition method. (See color
insert.)
10.3.1.2
In-network processing algorithm
Increasingly-large graphs and computational challenges arising with big data motivate
well devising fully-distributed iterative algorithms for unveiling anomalies in social graphs.
In a nutshell, per iteration k = 1, 2, . . . nodes n (i.e., graph vertices) carry out simple
computational tasks locally, relying on their own local feature vectors yn. Subsequently,
local estimates are reﬁned after exchanging messages only with directly connected neighbors
in the vertex set Nn, which facilitates percolation of local information to the whole network.
The end goal is for each node to form local estimates xn[k] and on[k] that coincide with the
n-th columns of ˆX and ˆO as k →∞, where { ˆX, ˆO} is the solution of (P1) obtained when
all data PΩ(Y) are centrally available.
In its present form (P1) is not amenable for distributed implementation due to the
non-separable nuclear norm present in the cost function. If an upper bound rank( ˆX) ≤ρ
is a priori available, (P1)’s search space is eﬀectively reduced and one can factorize the
decision variable as X = PQT , where P and Q are D × ρ and N × ρ matrices, respectively.
Next, consider the following alternative characterization of the nuclear norm (see e.g. [SS05,
SRJ04, RR13]):
∥X∥∗:= min
{P,Q}
1
2
 ∥P∥2
F + ∥Q∥2
F

s. t.
X = PQT ,
(10.3.2)
where the optimization is over all possible bilinear factorizations of X, so that the number of
columns ρ of P and Q is also a variable. Leveraging (10.3.2), the following equivalent refor-
mulation of (P1) provides an important ﬁrst step towards obtaining a distributed algorithm
for graph anomaly detection
min
{P,Q,A}
N
X
n=1

∥PΩn(yn −Pqn −on)∥2 + λ∗
2N
 ∥P∥2
F + N∥qn∥2
+ λ1∥on∥1

,
(10.3.3)
which is non-convex due to the bilinear terms xn = Pqn, and where QT := [q1, . . . , qN].

306
Graph-Based Social Media Analysis
Adopting the separable Frobenius-norm regularization in (10.3.3) comes with no loss of
optimality relative to (P1), provided rank( ˆX) ≤ρ. By ﬁnding the global minimum of
(10.3.3) [which could have considerably less variables than (P1)], one can recover the optimal
solution of (P1). But since (10.3.3) is non-convex, it may have stationary points which need
not be globally optimum. Interestingly, as asserted in [MMG13a, Prop. 1] if a stationary
point {¯P, ¯Q, ¯O} of (10.3.3) satisﬁes ∥PΩ(Y −¯P ¯QT −¯O)∥< λ∗, then { ˆX := ¯P ¯QT , ˆO := ¯O}
is the globally optimal solution of (P1).
To decompose the cost in (10.3.3), in which summands inside the square brackets are
coupled through the global variable P, introduce auxiliary copies {Pn}N
n=1 representing
local estimates of P, one per node n. These local copies along with consensus constraints
yield the distributed estimator:
min
{Pn,qn,on}
N
X
n=1

∥PΩn(yn −Pnqn −on)∥2
+ λ∗
2N
 ∥Pn∥2
F + N∥qn∥2
+ λ1∥on∥1

(10.3.4)
s. t.
Pn = Pm, m linked with n ∈N,
which is equivalent to (10.3.3) provided the network topology graph is connected. Even
though consensus is a fortiori imposed within neighborhoods, it extends to the whole (con-
nected) network, and local estimates agree on the global solution of (10.3.3). Exploiting
the separable structure of (10.3.4), a general framework for in-network sparsity-regularized
rank minimization was put forth in [MMG13a], whereas a distributed algorithm for PCP
(D-PCP) can be found in [MG13]. Speciﬁcally, distributed iterations were obtained af-
ter adopting the ADMM, an iterative Lagrangian method well-suited for parallel process-
ing [BT99, BPC+11]. In a nutshell, local tasks per iteration k = 1, 2, . . . entail solving small
unconstrained quadratic programs to reﬁne the projections qn[k] on the nominal feature
subspace Pn[k], in addition to soft-thresholding operations to update the egonet anomaly
vectors on[k] per node; see [MG13] for further details. Per iteration, graph nodes exchange
their subspace estimates Pn[k] only with directly connected neighbors. This way the com-
munication overhead stays aﬀordable, and independent of the network size N.
When employed to solve non-convex problems such as (10.3.4), so far ADMM oﬀers no
convergence guarantees. However, there is ample experimental evidence in the literature
that supports empirical convergence of ADMM, especially when the non-convex problem
at hand exhibits “favorable” structure. For instance, (10.3.4) is a linearly constrained bi-
convex problem with potentially good convergence properties – extensive numerical tests
in [MG13, MMG13a] demonstrate that this is indeed the case. While establishing conver-
gence remains an open problem, one can still prove that upon convergence the distributed
iterations attain consensus and global optimality, oﬀering the desirable centralized perfor-
mance guarantees [MMG13a].
10.3.1.3
Numerical tests
A test social network of N = 25 nodes is generated as a realization of the random
geometric graph model, meaning nodes are randomly placed on the unit square and two
nodes communicate with each other if their Euclidean distance is less than a prescribed
communication range of 0.4; see Figure 10.3.2 (a). The number of egonet features is D = 20.
Entries of E are independent and identically distributed (i.i.d.), zero-mean, Gaussian with
variance σ2 = 10−3; i.e., ed,n ∼N(0, σ2). A simulated nominal egonet feature matrix with
rank r = 3 is generated from the bilinear factorization model X = WZT , where W and Z are
D×r and N ×r matrices with i.i.d. entries drawn from Gaussian distributions N(0, 100/D)

Big Data Analytics for Social Networks
307
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
0
100
200
300
400
500
600
10
3
10
2
10
1
10
0
10
1
10
2
D PCP
Centralized solver [28]
(a)
(b)
FIGURE 10.3.2: (a) A simulated small social network graph with N = 25 nodes. (b) Con-
vergence of the D-PCP algorithm for diﬀerent network sizes. D-PCP attains the same esti-
mation error as the centralized solver. (See color insert.)
and N(0, 100/N), respectively. Every entry of O is randomly drawn from the set {−1, 0, 1}
with Pr(od,n = −1) = Pr(od,n = 1) = 5×10−2. To simulate missing data, a sampling matrix
Ω∈{0, 1}D×N is generated with i.i.d. Bernoulli distributed entries od,n ∼Ber(0.7) (30%
missing data on average). Finally, measurements are generated as PΩ(Y) = Ω⊙(X+O+E)
[cf. (10.3.1)], and node n has available the n-th column y(n) of PΩ(Y).
To experimentally corroborate the convergence and optimality of the D-PCP algorithm
for graph anomaly detection, the distributed iterations are run and compared with the
centralized benchmark (P1), obtained using the solver in [YY13]. Parameters λ1 = 0.0141
and λ∗= 0.346 are chosen as suggested in [ZLW+10]. For both schemes, Figure 10.3.2b
shows the evolution of the global estimation errors eX[k] := ∥X[k]−X∥F /∥X∥F and eO[k] :=
∥O[k] −O∥F /∥O∥F . It is apparent that the D-PCP algorithm converges to the centralized
estimator, and as expected convergence slows down due to the delay associated with the
information ﬂow throughout the network. The test is also repeated for network sizes of
N = 15 and 35, to illustrate that the time till convergence scales gracefully as the network
size increases.
10.3.2
Prediction from partially-observed network processes
Understanding the inﬂuence of topology on network processes has relied on measuring
and monitoring the network itself. In practice however, gathering network-wide measure-
ments scales poorly with the network size and may thus be impractical for various networks
of interest. For instance, large social network surveys also pose a major logistic issue due
to, for example, limited availability of individuals included in the survey. Tools such as
tcpdump provide detailed packet-level information in Internet protocol (IP) networks, but
collecting all these data can demand excessive power and bandwidth. Moreover, errors due
to measurement and data-handling are more likely to emerge as the amount of data col-
lected increases. A similar challenge arises when capturing spatial and temporal structures
in big data. There, one may be forced to rely on partial (random) observations of the
data so that inference algorithms remain operational while coping with the data deluge.
With these motivating challenges in mind, this section surveys a recent joint topology- and
data-driven algorithm to enable network-wide prediction of dynamical processes based on
partial network observations, that is, measurements collected only at a subset of network
nodes [FRG14]. The known (graph-induced) network structure and historical data are lever-

308
Graph-Based Social Media Analysis
aged to design a dictionary for representing the network process. The novel approach draws
from semi-supervised learning to enable learning the dictionary with only partial network
observations. Once the dictionary is learned, network-wide prediction becomes possible via
a regularized LS estimate which exploits the parsimony encapsulated in the design of the
dictionary.
Consider an undirected weighted graph G(V, E), where V is the vertex set with cardi-
nality N = |V| and E is the edge set. The connectivity and edge strenghts of G are char-
acterized by the weighted adjacency matrix A ∈RN×N, where the entry ai,j := [A]i,j > 0
if nodes vi and vj are connected, and ai,j = 0 otherwise. At time instant t ∈N, corre-
sponding to each vertex vn ∈V there is a scalar variable xn,t ∈R, which represents the
network-wide dynamical process of interest. All node variables are collected in a single vec-
tor xt := [x1,t . . . xN,t]T ∈RN. To account for missing data, it is assumed that M < N
vertices are measured at any given time. For simplicity in exposition, the number of ob-
served vertices M is assumed ﬁxed. However, expressions and algorithms derived in the
subsequent sections can be readily modiﬁed to allow for time-varying M. Let Mt ∈RM×N
denote a binary measurement matrix with 0 −1 entries selecting the measured components
of xt. Each row of Mt corresponds to a vector of the canonical basis for RN, i.e., each row
has only one nonzero entry, which takes the value of 1, while all other entries are set to 0.
The M × 1 measurement vector at time t is modeled as:
yt = Mtxt + ϵt,
t = 1, 2, . . . ,
(10.3.5)
where ϵt is a random error term capturing measurement imperfections.
Recently, a network process prediction algorithm was put forth in [FRG14], where miss-
ing entries of xt are estimated from historical measurements in TM := {yt}T
t=1 by leveraging
the structural regularity of xt (induced by the underlying graph) through a semi-supervised
dictionary learning (DL) approach. Under the DL framework, data-driven dictionaries for
sparse signal representation are adopted as a versatile means of capturing parsimonious
signal structures; see e.g., [TF10] for a tutorial treatment. Propelled by the success of
compressive sampling (CS) [Don06], sparse signal modeling has led to major advances in
several machine learning, audio and image processing tasks [HTF09, TF10]. Motivated by
these ideas, it is postulated in [FRG14] that graph signals can be represented as a linear
combination xt = Bst of a few (≪Q) columns of an over-complete dictionary (basis) ma-
trix B := [b1, . . . , bQ] ∈RN×Q, where st ∈RQ is a sparse vector of expansion coeﬃcients.
Many signals including speech and natural images admit sparse representations even under
generic predeﬁned dictionaries, such as those based on the Fourier and the wavelet bases,
respectively [TF10]. Like audio and natural images, vertex variables can exhibit strong cor-
relations induced from the structure of the underlying graph. For instance, Internet traﬃc
volumes on two links are highly correlated if they both carry common end-to-end ﬂows,
as indicated by the corresponding routing matrix. DL schemes are attractive due to their
ﬂexibility, since they utilize training data to learn an appropriate over-complete basis cus-
tomized for the data at hand. However, the use of DL for modeling network data is well
motivated but so far relatively unexplored.
10.3.2.1
Semi-supervised prediction of network processes
Suppose for now that either a learnt, or, a suitable pre-speciﬁed dictionary B is available,
and consider predicting the process on the unobserved vertices. Data-driven learning of
dictionaries from historical data will be addressed in the ensuing subsection. To cope with
the absence of some entries of xt not present in yt, the idea here is to capitalize on the
topology of G. To that end, suppose wi,j represents a similarity weight between the time-
dependent variables associated with nodes vi and vj; e.g., the correlation between xi,t and

Big Data Analytics for Social Networks
309
xj,t. The topology of G, and thus the spatial correlation of the process, is captured by its
Laplacian matrix L := diag(A1N)−A. Given B, L and the measurements yt, contemporary
tools developed in the area of CS and semi-supervised learning can be used to form ˆxt, which
includes estimates for the missing N −M vertex observations [Don06, BNS06, HTF09].
Given a snapshot of incomplete measurements yt during the operational phase (where
a suitable basis B is available), the sparse basis expansion coeﬃcient vector st is estimated
as:
ˆst := arg min
st
∥yt −MtBst∥2
2 + λs∥st∥1 + λwsT
t BT LBst,
(10.3.6)
where λs and λw are tunable regularization parameters. The criterion in (10.3.6) consists
of a LS error between the observed and postulated network measurements, along with two
regularizers. The ℓ1-norm ∥st∥1 encourages sparsity in the coeﬃcient vector ˆst [Don06,
HTF09]. With xt := [x1,t, . . . , xN,t]T given by xt = Bst, the Laplacian regularization can
be explicitly written as sT
t BT LBst = (1/2) PN
i=1
PN
j=1 ai,j(xi,t −xj,t)2. It is thus apparent
that sT
t BT LBst encourages the vertex variables to be close if their corresponding weights are
large. Typically adopted for semi-supervised learning, such a regularization term encourages
Bst to lie on a smooth manifold approximated by G, which constrains how the measurements
relate to xt [BNS06, RBL+07]. It is also common to use normalized variants of the Laplacian
instead of L [Kol09, p. 46].
The cost in (10.3.6) is convex but non-smooth, and customized solvers developed for ℓ1-
norm regularized optimization can be employed here as well, e.g., [HTF09, p. 92]. Once ˆst is
available, an estimate of the full vector of network samples is readily obtained as ˆxt := Bˆst.
It is apparent that the quality of the imputation depends on the chosen B, and DL from
historical network data in TM is described next.
10.3.2.2
Data-driven dictionary learning
In its canonical form, DL seeks a (typically fat) dictionary B so that training data
TN := {xt}T
t=1 are well approximated as xt ≈Bst, t = 1, . . . , T, for some sparse vectors
st of expansion coeﬃcients [TF10]. Standard DL algorithms cannot, however, be directly
applied to learn B since they rely on the entire vector xt. To learn the dictionary in the
training phase using incomplete measurements TM instead of TN, the idea is to capitalize
on the structure in xt, of which G is an abstraction [FRG14]. To this end, one can adopt a
similar cost function as in the operational phase [cf. (10.3.6)], yielding the data-driven basis
and the corresponding sparse representation:
{ˆS, ˆB} :=
arg min
S,B:{∥bq∥2≤1}Q
q=1
T
X
t=1

∥yt −MtBst∥2
2+λs∥st∥1+λwsT
t BT LBst

,
(10.3.7)
where ˆS := [ˆS1, . . . , ˆST ] ∈RQ×T . The constraints {∥bq∥2 ≤1}Q
q=1 remove the scaling
ambiguity in the products Bst, and prevent the entries in B from growing unbounded.
Again, the combined regularization terms in (10.3.7) promote both sparsity in st through the
ℓ1-norm, and smoothness across the entries of Bst via the Laplacian L. The regularization
parameters λs and λw are typically cross-validated [HTF09, Ch. 7]. Although (10.3.7) is
non-convex, a BCD solver still guarantees convergence to a stationary point [BT99]. The
BCD updates involve solving for B and S in an alternating fashion, both doable eﬃciently
via convex programming [FRG14]. Alternatively, the online DL algorithm in [MBPS10]
oﬀers enhanced scalability by sequentially processing the data in TS. The training and
operational (prediction) phases are summarized in Figure 10.3.3, where Ct(B, s) denotes
the t-th summand from the cost in (10.3.7), and k = 1, 2, . . . indicate iterations of the BCD
solver employed during the training phase.

310
Graph-Based Social Media Analysis
min
wt Ct(B[k], st)
min
bq
1
T
t=1
Ct(B, st[k])
min
wt Ct( ˆB, st)
ˆxt = ˆBˆst
St[k]
ˆst
ˆB
B[k + 1]
yt, t > T
{yt}T
t=1
ˆxt
Training Phase
Operational Phase
FIGURE 10.3.3: Training and operational phases of the semi-supervised DL approach for
prediction of network processes that evolve over graphs [FRG14].
The explicit need for Laplacian regularization is apparent from (10.3.7). Indeed, if mea-
surements from a certain vertex are not present in TM, the corresponding row of B may
still be estimated with reasonable accuracy because of the third term in Ct(B, s). On top
of that, it is because of Laplacian regularization that the prediction performance degrades
gracefully as the number of missing entries in yt increases; see also Figure 10.3.4. It is worth
stressing that the time series {yt} need not be stationary or even contiguous in time. The
network-process prediction approach described so far can also be adapted to accommodate
time-varying network topologies, using a time-dependent Laplacian Lt. A word of caution
is due however, since drastic changes in either Lt or in the statistical properties of the
underlying process xt, will necessitate re-training B to attain satisfactory performance.
10.3.2.3
Numerical tests
Next, a numerical test on link count data from the Internet2 measurement archive [Int]
is outlined. Consider an IP network comprising N nodes and L links, carrying the traﬃc
of F origin-destination ﬂows (network connections). Let xl,t denote the traﬃc volume (in
bytes or packets) passing through link l ∈{1, . . . , L} over a ﬁxed interval of time (t, t+∆t).
Link counts across the entire network are collected in the vector xt ∈RL, e.g., using the
ubiquitous SNMP protocol. Since measured link counts are both unreliable and incomplete
due to hardware or software malfunctioning, jitter, and communication errors [ZRWQ09,
Rou10], they are expressed as noisy versions of a subset of S < L links
yt = Mtxt + ϵt,
t = 1, 2, . . . ,
where Mt is an S ×L selection matrix with 0-1 entries whose rows correspond to rows of the
identity matrix of size L, and ϵt is an S × 1 zero-mean noise term with constant variance
accounting for measurement and synchronization errors. Given yt the aim is to form an
estimate ˆxt of the full vector of link counts xt, which in this case deﬁnes the network state.
The data consists of link counts, sampled at 5 minute intervals, collected over several
weeks. For the purposes of comparison, the training phase consisted of 2, 000 time slots,
with a random subset of 50 links measured (out of L = 54 per time slot). Performance
of the learned dictionary is then assessed over the next T0 = 2, 000 time slots. Each test

Big Data Analytics for Social Networks
311
30
32
34
36
38
40
42
44
46
48
50
10
–1
10
0
S
Average NRE
Q = 100
Q = 80
Q = 60
Q = 54 DW
Entropy-penalized LS
FIGURE 10.3.4: Link-traﬃc cartography of Internet2 data [Int]. Comparison of NRE for
diﬀerent values of S [FRG14].
vector yt is constructed by randomly selecting S entries of the full link count vector xt. The
tuning parameters are chosen via cross-validation (λs = 0.1 and λw = 10−5). Figure 10.3.4
shows the normalized reconstruction error (NRE), evaluated as (LT0)−1 PT0
t=1 ∥yt −ˆxt∥2
for diﬀerent values of Q and S. For comparison, the prediction performance with a ﬁxed
diﬀusion wavelet matrix [CPR07] (instead of the data-trained dictionary), as well as that of
the entropy-penalized LS method [ZRLD05] is also shown. The latter approach solves a LS
problem augmented with a speciﬁc entropy-based regularizer, that encourages the traﬃc
volumes at the source/destination pairs to be stochastically independent. The DL-based
method markedly outperforms the competing approaches, especially for low values of S.
Furthermore, note how performance degrades gracefully as S decreases. Remarkably, the
predictions are close to the actual traﬃc even when using only 30 link counts during the
prediction phase.
10.4
Unveiling communities in social networks
Social networks generally exhibit community structure, which is characterized by the
existence of groups within which the edge density is relatively high compared to the edge
density between groups [GN02]. Communities are indicative of common functional roles or
similar node behavior e.g., co-workers on Facebook, or followers of a cause on Twitter.
The community detection task has attracted signiﬁcant attention from diﬀerent disci-
plines, with several algorithms developed to tackle it. Traditional approaches resort to graph
partitioning and data clustering algorithms such as k-means, hierarchical and spectral clus-
tering [For10]. Graph partitioning algorithms such as Min-Cut and Ratio-Cut divide the
graph into k parts of known size [For10]. However, these algorithms are not practical for
community detection because they are limited to settings where the number and size of

312
Graph-Based Social Media Analysis
communities are known. On the other hand, hierarchical clustering, which is sensitive to
selection of a similarity metric, is well-motivated because node similarity in social networks
can be succinctly deﬁned [HTF09].
Both k-means and spectral clustering are “workhorse” methods for (non)linearly sepa-
rable data clustering. Nevertheless, spectral clustering is more appealing for community
detection where the similarity graph is given. More recently, modularity methods have
emerged, which entail optimization of a graph-clustering quality function [GN02]. Fortu-
nato’s community detection survey catalogs most of the contemporary community detection
approaches [For10].
In general, most modern social networks are extremely large with millions of nodes and
edges, and attempts to eﬃciently unveil their communities need to cope with typical big
data challenges. To this end, a number of approaches based on parallelization, random sam-
pling, and random projections have been advocated for clustering big data. Among these,
parallelization is a relatively mature technology as tasks in k-means can be easily distributed
over a computing cluster. However, randomized algorithms can reduce the computational
load per node, and therefore require fewer nodes.
This section focuses on spectral clustering and its connections to the more general kernel
k-means. In light of the aforementioned big data challenges, a discussion of recent random
sampling extensions to such settings is given. It is also worth noting that the equivalence of
spectral clustering with kernel k-means can prove useful to reduction of the computational
load.
10.4.1
Big data spectral clustering
The emphasis of this subsection will be on big data spectral clustering. Although orig-
inally developed for data clustering, spectral clustering ﬁnds applications in community
detection as it exhibits good performance in arbitrary cluster conﬁgurations. Spectral clus-
tering exploits the properties of the similarity graph Laplacian L to group the vertices into
a prescribed number of k clusters. Let A ∈RN×N denote the (weighted) adjacency ma-
trix. The graph Laplacian is deﬁned as L := D −A, where D is a diagonal matrix with
[D]ii = PN
j=1[A]ij, and [A]ij denotes (i, j) entry of A. The key property of L is the equiva-
lence of algebraic multiplicity of the zero eigenvalue to the number of connected components.
The corresponding eigenvectors are the indicator vectors of each connected component. This
can be veriﬁed by considering an eigenvalue λ = 0 of L and its corresponding eigenvector
v ∈RN. Then:
Lv = 0 ⇒vT Lv = 0 = 1
2
N
X
i,j=1
[A]ij(vi −vj)2,
(10.4.1)
with vi denoting the i-th entry of v. As all terms of the sum must vanish and [A]ij ≥0,
the entries of v, for which [A]ij ̸= 0 must be equal. Thus v should have constant entries
corresponding to vertices of the connected component. In a network with k completely
separated clusters, the graph will have k connected components, and hence L will have k
zero eigenvalues. The corresponding eigenvectors suﬃce to reveal the clusters in the network.
This however is not the case in social networks where the graph can be connected with
communities linked to each other by a few edges. Thus, L will have a single all-ones (1 ∈RN)
eigenvector, and k eigenvalues close to zero. While the eigenvectors corresponding to these
k eigenvalues will not be indicator vectors, they can still be used to separate the clusters.
Spectral clustering algorithms ﬁnd the k smallest, non-zero, eigenvalues {λi}k
i=1 of L, and
their corresponding eigenvectors {vi ∈RN}k
i=1. With V := [v1, ..., vk], vertex i is mapped
to row i of V. This change of representation enhances the separability of clusters in the
graph, which can be recovered using simple algorithms such as k-means. The eigenvalue

Big Data Analytics for Social Networks
313
computation can be performed using eﬃcient methods such as the power iteration [GL12].
Algorithm 10.4.1 depicts the unnormalized spectral clustering algorithm. In addition to
the basic deﬁnition of L, certain spectral clustering approaches leverage normalized graph
Laplacians, which are equivalent to Min-Cut and Ratio-Cut [SM00, NJW+02].
Although the number of clusters is not necessarily known, one can deduce k by comparing
the magnitudes of the eigenvalues of L. Since eigenvalues corresponding to clusters are
close to zero, one can assess the value of k by ﬁnding the “jump” in the spectrum of the
eigenvalues.
Spectral clustering also has strong connections to kernel PCA [HTF09] and kernel k-
means (Algorithm 10.4.2). Kernel k-means [DGK04] extends the classic k-means algorithm,
and is able to cluster even non-linearly separable data. This is accomplished by mapping
each datum to a higher-dimensional space F, using a function φ : RD →F. The premise is
that a mapping exists to render the dataset linearly separable, and hence amenable to simple
and heuristic yet eﬀective algorithms such as k-means. Even if F is inﬁnite dimensional,
the Representer theorem [Wah90] guarantees that inner products between data points on
F suﬃce to perform clustering. Kernel k-means on a N-point dataset with k clusters aims
to minimize the following objective function:
D =
k
X
j=1
N
X
i=1
∥φ(xi) −µCj∥2
2,
(10.4.2)
where φ(xi) is the feature space representation of data point xi, and the centroid µCj :=
1
|Cj|
P
j∈Cj φ(xj) is the sample mean of the points in cluster Cj. Using the Representer
theorem the distance of each point in F from the centroid in (10.4.2) can be rewritten as:
∥φ(xi) −µC∥2
2 = [K]ii −2
|C|
X
j∈C
[K]ij +
1
|C|2
X
j,l∈C
[K]jl,
(10.4.3)
where K ∈RN×N is the Gramian of the kernel used, and [K]ij denotes the inner product
between φ(xi) and φ(xj). Consequently, minimization of (10.4.2) can be written as:
min
U,C tr(K) −tr(C1/2UT KUC1/2) ⇐⇒max
ˆU
tr( ˆUT K ˆU),
(10.4.4)
where U := [ui . . . uk] is a cluster membership matrix with ui ∈{0, 1}N, [ui]j =
1, if point j belongs to cluster i; the diagonal matrix C ∈Rk×k collects inverses of clus-
ter cardinalities, C = diag(
1
|C1|, ...,
1
|Ck|); and, ˆU := C1/2U. The optimization problem
in (10.4.4) is non-convex as ˆU is binary. By relaxing the binary constraint, and requiring
ˆUT ˆU = I, the problem can be recast as the following convex surrogate with a well-known
solution [GL12]:
max
ˆUT ˆU=I
tr( ˆUT K ˆU) =
k
X
i=1
λi,
(10.4.5)
where ˆU∗= VQ, {λi}k
i=1 are the largest eigenvalues of K, Q ∈Rk×k denotes an arbitrary
orthonormal matrix, and the columns of V ∈RN×k are formed with the k eigenvectors
corresponding to {λi}k
i=1. This is equivalent to ﬁnding the k trailing eigenvectors of I −K.
Due to the relaxation, the columns of ˆU∗most likely do not represent natural cluster-
ings, and as such post-processing is required. Similarly, Ratio-Cut and Min-Cut can be
converted to trace maximization problems [Lux07]. Furthermore, kernel PCA ﬁnds the k
largest eigenvectors of the centered Gramian ˜K.

314
Graph-Based Social Media Analysis
Algorithm 10.4.1: Unnormalized spectral clustering
Require: k, L ∈RN×N.
Ensure: Clustered vertices.
1: Compute the k smallest eigenvectors {vi}k
i=1 of L.
Let V := [v1, v2, ..., vk] ∈RN×k.
2: Let {xi ∈Rk}N
i=1 be the rows of V; xi corresponds to the i-th vertex.
3: Group {xi}N
i=1 into k clusters {Ci}k
i=1.
Algorithm 10.4.2: Kernel k-means
Require: k, K ∈RN×N, maximum number of iterations T
Ensure: Clustered points
1: Randomly assign points to clusters.
2: repeat
3:
for i = 1 to N do
4:
For point φ(xi) calculate closest centroid using equation 10.4.3
5:
end for
6:
Update point assignments; Assign each point to the cluster whose centroid is closest
7:
t ←t + 1; update iteration counter
8: until No changes in assignments or t > T
In large social networks, L is presumed to be sparse. In this case, methods such as
Arnoldi/Lanczos iterations [GL12] can be used to eﬃciently compute the trailing eigenspace
of L, as usually only matrix-vector products are required. Readily available packages that
tackle large-scale sparse eigenvalue problems exist [LSY98]. Distributed eigensolvers and
parallel versions of k-means [ZMH09] can also be used. Care should be taken when the
number of communities k is very large. While the trailing eigenvectors of L can be computed
eﬃciently, the ﬁnal clustering step would require clustering N k-dimensional vectors, which
can prove challenging even for distributed versions of k-means. Multiple approaches that
aim to tackle speciﬁcally large-scale spectral clustering tasks are available. These approaches
come in three major ﬂavors: Parallelization/distributed processing, random sampling and
random projections.
A useful overview of performing spectral clustering for sparse Laplacian matrices in
parallel is given in [CSB+11]. The parallelization can be performed using either MapRe-
duce [DG08] or MPI [Sni98]. Pre-processing of the data using k-means and random pro-
jection trees is investigated in [YHJ09]. In this method the preprocessing step reduces the
original N datapoints to M < N representatives and performs spectral clustering on these
M representatives, which results in a reduced graph which speeds up execution of the spec-
tral clustering algorithm. However, as usually only similiraties between datapoints are given
and not the datapoints themselves, algorithms such as kernel k-means or k-medoids, that
can work using only similarities have to be employed.
Random sampling and random projections of the data are advocated in [SI09]. The ran-
dom projection step involves projecting the data points to a lower dimensional space and
computing the similarity matrix from these lower-dimensional representations of the data
points. Again, as usually only the similarities are given and not the datapoints themselves,
this part of the algorithm cannot provide the needed computational time reduction. Af-
terwards, entries of the similarity matrix are randomly sampled and spectral clustering is
performed using this reduced similarity matrix. Nystr¨om’s method is proposed in [WLRB09]
and [FBCM04],
to form a low-rank similarity matrix, which is enabled by sampling the
original similarity matrix and using the similarities between the sampled and non-sampled

Big Data Analytics for Social Networks
315
points. Finding the eigenspace of the new low-rank similarity matrix is much more eﬃcient;
however, one should perform the sampling carefully, as it can drastically inﬂuence the ﬁnal
result. When the similarity matrix is sparse, the Nystr¨om eigenspace can be very similar to
the original eigenspace, leading to highly accurate clustering.
Random sampling of the similarity matrix is explored in [ST11], whereby entries of the
similarity matrix are sampled randomly, based on a budget constraint, and all other entries
are set to zero. This sparsiﬁes the similarity matrix and leads to faster computation of
the eigenvectors. Random sketching is promoted in [GKB13] and [LC10]. Entries of the
similarity matrix are randomly sketched using a random projection matrix to reduce the
size of the similarity matrix. This reduction allows for faster computation of the eigenspace.
Large-scale kernel k-means methods can also signiﬁcantly speed-up the clustering pro-
cess. Results in [DGK07] have shown that using kernel k-means instead of spectral clustering
can reduce the computation time required. Again care should be taken when using kernel
k-means. While the Laplacian matrix of a social network might be sparse, the similarity
matrix in general is not, possibly increasing the clustering time of kernel k-means (compared
to spectral clustering methods), especially in cases where the number of clusters k is small.
Methods to scale the kernel k-means algorithm are also available. Random sampling of the
kernel matrix is investigated in [CJHJ11], where the centroids (cluster representatives) are
forced to reside on the subspace spanned by those sampled points. Simulated tests demon-
strate that the resultant algorithm can tackle large datasets eﬀectively. Parallelization of
the kernel k-means algorithm is proposed in [EFKK14]. Here, low-dimensional embeddings
allow kernel k-means to be used in a distributed fashion using the MapReduce framework.
A more recent method, called Sketching and Validation (SkeVa [TSG15]) proposes taking
multiple sketches (random samples of M < N entries) of the similarity matrix, performing
kernel k-means to ﬁnd clusterings, and relies on diﬀerent sets of random samples to validate
these sketches by assigning a score to them. Afterwards, the sketch that yielded the highest
score is used to cluster the remaining data. This structured trial-and-error approach has
shown promising results with respect to clustering accuracy and reduced computational
time. Furthermore, as each of the sketching and validation runs is independent, this approach
admits easy parallelization, thereby combining random sketching approaches and distributed
computing, making it attractive for the task at hand. Simulated and real data tests indicate
that this algorithm can tackle large-scale datasets much faster than traditional kernel k-
means.
10.4.1.1
Numerical tests
Three methods are compared in this section with respect to clustering accuracy and
required time: Spectral clustering, kernel k-means, and the Sketching and Validation method
for kernel k-means (Kernel SkeVa) introduced in [TSG15]. Regarding spectral clustering,
the normalized version of L is used [NJW+02], and Lanczos iterations are employed to
evaluate the eigenspace of L. The kernel used for kernel k-means and kernel SkeVa is the
shortest path distance kernel K = (−1/2)JDJ, where D contains the shortest path distances
between every node pair in the graph and J = I −1
N 11T is the double centering operator,
while 1 denotes the all-ones vector. Figure 10.4.1 shows the community detection result
for the diﬀerent algorithms on the largest connected component of a Facebook egonet with
N = 744 vertices, and 30, 023 edges containing k = 5 communities [Les12]. Vertices of the
graph represent friends of a particular user, and edges between the vertices indicate whether
two people are friends with each other. All methods are able to distinguish the clearly deﬁned
communities in this graph. Kernel SkeVa misclassiﬁes some nodes when only 150 nodes are
sampled, but this is to be expected as not all nodes are sampled. Since the size of this
network is small all methods require similar amounts of time to perform the clustering (see

316
Graph-Based Social Media Analysis
TABLE 10.4.1: Clustering times for Facebook egonet.
Spectral
Clustering
Kernel
k-means
SkeVa
(150 samples)
SkeVa
(350 samples)
Time(s)
0.067
0.074
0.031
0.066
TABLE 10.4.2: Clustering times for arXiv General relativity collaboration network.
Spectral
Clustering
Kernel
k-means
SkeVa
(500 samples)
SkeVa
(1, 000 samples)
Time(s)
3.1
2.51
0.4
0.85
Table 10.4.1). Figure 10.4.2 shows the community detection result on the largest connected
component of an arXiv collaboration network (General Relativity) with N = 4, 158 vertices,
and 13, 422 edges [Les11]. Vertices represent paper authors and edges indicate whether two
people have co-authored a paper. It is assumed that k = 36 communities are present in
this graph. Similar to the Facebook network, all algorithms are able to recognize the tight
communities of this network, however the time required for kernel SkeVa is one order of
magnitude lower.
10.4.2
Robust kernel PCA
Kernel (K)PCA is a generalization of (linear) PCA, seeking principal components in
a feature space nonlinearly related to the input space, where the data in Tx live [SSM98].
KPCA has been shown eﬀective in performing nonlinear feature extraction for pattern recog-
nition [SSM98]. In addition, connections between KPCA and spectral clustering [HTF09, p.
548] motivate well the KPCA method outlined in this section, to robustly identify cohesive
subgroups (communities) from social network data.
Consider a nonlinear function φ : RD →H, that maps elements from the input space
RD to a feature space H of arbitrarily large — possibly inﬁnite — dimensionality. Given
transformed training data TH := {φ(yn)}N
n=1, the proposed approach to robust KPCA ﬁts
the model [cf. the low-rank subspace model in (10.3.3)]:
φ(yn) = m + Pqn + on + en,
n = 1, . . . , N,
(10.4.6)
where, again, on is an outlier vector, and m denotes the location (mean) vector in feature
space H. A natural criterion is (Φ := [φ(y1), . . . , φ(yN)] and 1T
N is the N × 1 row vector
of all ones):
min
m,P,Q,O ∥Φ −m1T
N −PQT −O∥2
F + λ∗
2 (∥P∥2
F + ∥Q∥2
F ) + λ2
N
X
n=1
∥on∥2,
(10.4.7)
where PN
n=1 ∥on∥2 is the so-termed group Lasso penalty [YL06]. It is a high-dimensional
extension of the ℓ1-norm, that encourages columnwise (vector) sparsity on the estimator of
O [cf. entrywise sparsity with the ℓ1-norm]. This way, one can declare whether the corre-
sponding training vector φ(y1) is an outlier or not. Except for the principal components’
matrix Q ∈RN×ρ, both the data and the unknowns in (10.4.7) are now vectors/matrices
of generally inﬁnite dimension. In principle, this challenges the optimization task since it
is impossible to store, or, perform updates of such quantities directly. For these reasons,
assuming zero-mean data φ(yn), or, the possibility of mean compensation for that matter,

Big Data Analytics for Social Networks
317
(a)
(b)
(c)
(d)
FIGURE 10.4.1: Community detection results for a Facebook egonet with N = 744 nodes
and k = 5 communities using: (a) Normalized spectral clustering; (b) Kernel k-means; (c)
Kernel SkeVa, where 150 nodes are sampled; and (d) Kernel SkeVa, where 350 nodes are
sampled. Diﬀerent shades of gray represent diﬀerent communities.
cannot be taken for granted here. Thus, it is important to explicitly consider the estimation
of m [which for instance, was not explicitly accounted for in (10.3.3)].
Interestingly, this hurdle can be overcome by endowing H with the structure of a re-
producing kernel Hilbert space (RKHS), where inner products between any two mem-
bers of H boil down to evaluations of the reproducing kernel KH : RD × RD →R,
i.e., ⟨φ(yi), φ(yj)⟩H = KH(yi, yj). Speciﬁcally, it is possible to form the kernel matrix
K := ΦT Φ ∈RN×N, without directly working with the vectors in H. This so-termed kernel
trick is the crux of most kernel methods in machine learning [HTF09], including kernel
PCA [SSM98]. The problem of selecting KH (and φ indirectly) will not be considered here.
Building on these ideas, it is shown in [MG12b] that natural alternating-minimization
(AM) iterations one can devise to optimize (10.4.7) can be kernelized, to solve (10.4.7) at
aﬀordable computational complexity and memory storage requirements that do not depend
on the dimensionality of H. Speciﬁcally, for k ≥1, [MG12b] shows that the sequence of
AM iterates obtained to solve (10.4.7) can be written as m(k) = Φµ(k), P(k) = ΦΠ(k),

318
Graph-Based Social Media Analysis
(a)
(b)
(c)
(d)
FIGURE 10.4.2: Community detection results for an arXiv collaboration network (General
Relativity) with N = 4, 158 nodes, and k = 36 communities using: (a) Normalized spectral
clustering; (b) Kernel k-means; (c) Kernel SkeVa where 500 nodes are sampled; and (d)
Kernel SkeVa where 1, 000 nodes are sampled. Diﬀerent shades of gray represent diﬀerent
communities.
and O(k) = ΦΩ(k). The quantities µ(k) ∈RN, Π(k) ∈RN×ρ, and Ω(k) ∈RN×N are then
recursively updated as in Algorithm 10.4.3, without the need of operating with vectors in
H.
In order to run the robust KPCA algorithm (tabulated as Algorithm 10.4.3), one does not
have to store or process the quantities m(k), U(k), and O(k). As per [MG12b, Prop. 4], the
iterations can be equivalently carried out by cycling through ﬁnite-dimensional ‘suﬃcient
statistics’ µ(k) →Π(k) →Q(k) →Ω(k). In other words, the iterations of the robust kernel
PCA algorithm are devoid of algebraic operations among vectors in H. Recall that the size
of matrix Q is independent of the dimensionality of H.
Because O(k) = ΦΩ(k) and upon convergence of the algorithm, the outlier vector norms
are computable in terms of K, i.e., [∥o1(∞)∥2
2, . . . , ∥oN(∞)∥2
2]T = diag[ΩT (∞)KΩ(∞)].
These are critical to identifying outlying vectors yn, since for those ∥on(∞)∥2 > 0. Moreover,
the principal component corresponding to any given new data point y is obtained through
the projection 1 = PT (∞)[φ(y) −m(∞)] = ΠT (∞)ΦT φ(x) −ΠT (∞)Kµ(∞), which is
again computable after N evaluations of the kernel function KH.

Big Data Analytics for Social Networks
319
Algorithm 10.4.3: : Robust KPCA solver
Initialize Ω(0) = 0N×N, Q(0) randomly, and form K = ΦT Φ.
for k = 1, 2, . . . do
Update µ(k) = [IN −Ω(k −1)]1N/N.
Form Φo(k) = IN −µ(k)1T
N −Ω(k −1).
Update Π(k) = Φo(k)Q(k −1)[QT (k −1)Q(k −1) + (λ∗/2)Iρ]−1.
Update Q(k) = ΦT
o (k)KΠ(k)[ΠT (k)KΠ(k) + (λ∗/2)Iρ]−1.
Form δn(k) = eN,n −µ(k) −Π(k)qn(k), n = 1, . . . , N
Form ∆(k) = diag

(δT
1 (k)Kδ1(k)−λ2
2 )+
δT
1 (k)Kδ1(k)
, . . . ,
(δT
N (k)KδN (k)−λ2
2 )+
δT
N (k)KδN (k)

.
Update Ω(k) = [IN −µ(k)1T
N −Π(k)QT (k)]∆(k).
end for
10.4.2.1
Numerical tests
Here robust KPCA is used to identify communities and outliers in a social network of
N = 115 college football teams, by capitalizing on the connection between KPCA and spec-
tral clustering [HTF09, p. 548]. Nodes in the network graph represent teams belonging to
eleven conferences (plus ﬁve independent teams), whereas (unweighted) edges joining pairs
of nodes indicate that both teams played against each other during the Fall 2000 Division I
season [GN02]. The kernel matrix used to run robust KPCA is K = ζIN + D−1/2AD−1/2,
where A and D denote the graph adjacency and degree matrices, respectively; while ζ > 0
is chosen to render K positive semi-deﬁnite. The choice of the normalized graph Laplacian
as kernel matrix is at the heart of the equivalence between KPCA and spectral cluster-
ing [HTF09]. The tuning parameters are chosen as λ2 = 1.297 so that ∥ˆO∥0 = 10, while
λ∗= 1, and ρ = 3. Figure 10.4.3 (a) shows the entries of K, where rows and columns are
permuted to reveal the clustering structure found by robust KPCA (after removing the
outliers); see also Figure 10.4.3 (b). The quality of the clustering is assessed through the
adjusted rand index (ARI) after excluding outliers [FKG11], which yielded the value 0.8967.
Four of the teams deemed as outliers are Connecticut, Central Florida, Navy, and Notre
Dame, which are indeed independent teams not belonging to any major conference. The
community structure of traditional powerhouse conferences such as Big Ten, Big 12, ACC,
Big East, and SEC was identiﬁed exactly.
10.5
Topology tracking from information cascades
It has been observed in many settings that information often spreads in cascades by
following implicit links between nodes in a network. Examples include the propagation
of viral news events between blogs, adoption of emerging fashion trends within an age
group, or acquisition of new buying habits by consumer groups. Consider the example of
a terrorist attack reported within minutes on mainstream news websites. An information
cascade may emerge because these websites’ readership includes bloggers who subsequently
write about the attack, inﬂuencing their own readers in turn to do the same. The underlying
dynamics for propagation of such information are remarkably similar to those governing the
rapid spread of infectious diseases within a population, leading to the so-termed contagions
[Rog95, EK10, BMG14]. In general, one is only able to observe the nodes of such networks
and the times when they got “infected” by a contagion, but not their link topology.

320
Graph-Based Social Media Analysis
(a)
(b)
FIGURE 10.4.3: (a) Entries of K after removing the outliers, where rows and columns are
permuted to reveal the clustering structure found by robust KPCA; and (b) Graph depic-
tion of the clustered network [MG12b]. Teams belonging to the same estimated conference
(cluster) are colored identically. The outliers are represented as diamond-shaped nodes.
(See color insert.)

Big Data Analytics for Social Networks
321
Knowledge of the network topology is crucial for several reasons. Viral web advertising
can be more eﬀectively achieved if a small set of inﬂuential initiators are identiﬁed through
the link structure. Furthermore, knowledge of the structure of hidden needle-sharing net-
works among communities of injecting drug users can aid formulation of policies for curbing
contagious diseases. Other examples include assessment of the reliability of heavily inter-
connected systems such as the power grid, or, estimating risk exposure among investment
banks in a highly interdependent global economy. In general, unveiling the network topology
can be used to predict the behavior of complex systems [Kol09, RLS10, BMG14].
Key to topology identiﬁcation from a cascade is the ease of observation of its evolution
over the unknown network. Indeed, this is tantamount to simply recording the times when
each node got infected by a cascade. Well-studied diﬀusion models based on epidemiological
studies have been put forth to identify an underlying network topology [VR07, EK10, Jac10].
Undoubtedly a challenging inference task, analysis of information cascades over modern
social networks leads to the fundamental big data challenges. Cascades typically propagate
over very large web-scale networks, and are acquired sequentially in inﬁnite streams. More
importantly, the underlying network topology is generally dynamic and varies over time.
Network inference from temporal traces of infection events has recently emerged as an
active area of research. According to the taxonomy in [Kol09, Ch. 7], this can be viewed as
a problem involving inference of association networks. Two other broad classes of network
topology identiﬁcation problems entail (individual) link prediction, or, tomographic infer-
ence. Several approaches postulate probabilistic models and rely on maximum likelihood
estimation (MLE) to infer static edge weights as pairwise transmission rates between nodes
[RBS11, ML13]. MLE-based stochastic gradient descent (SGD) iterations have been lever-
aged for inference of temporal diﬀusion networks [RLS10]. Most contemporary diﬀusion
models attribute node infection events to the network topology alone (endogenous factors),
and ignore exogenous factors such as non-topological information sources. Modeling causal
endogenous and exogenous factors is the mainstay of structural equation models (SEMs),
and the rest of this section will focus on such a general approach that was recently advocated
in [BMG14].
10.5.1
Dynamic SEMs for tracking cascades
Structural equation modeling refers to a family of statistical methods that model causal
relationships between interacting variables in a complex system; see e.g., [Kap09]. Their
appeal can be attributed to simplicity and the inherent ability to capture edge direction-
alities in graphs. They have been adopted in economics, psychometrics [Mut84], social sci-
ences [Gol72], and genetics [LdH08, CBG13], among others.
Reasoning that infection times depend on both topological (endogenous) and external
(exogenous) inﬂuences, a novel SEM-based scheme was proposed in [BMG14] for cascade
modeling. Topological inﬂuences are modeled as linear combinations of infection times of
other nodes in the network, whose weights correspond to entries in a time-varying asym-
metric adjacency matrix. External inﬂuences such as those due to on-site reporting in news
propagation contexts are useful for model identiﬁability, as they have been shown necessary
to resolve directional ambiguities [BBG13]. It is assumed that the network varies slowly
with time, facilitating adaptive parameter estimation by minimizing a sparsity-promoting
exponentially-weighted LS criterion. Furthermore, inherent sparse connectivity of social
networks is accounted for by ℓ1-norm regularization [CGH09, ABG10, KST11, AG11].

322
Graph-Based Social Media Analysis
FIGURE 10.5.1: Two cascades propagating over a dynamic directed 16-node network during
time intervals t = 1 and t = 2. Both cascades are initially observable at 4 nodes and they
propagate to include 4 extra nodes during t = 2. Changes to the network topology are
depicted by the new, thicker edges during t = 2.
10.5.1.1
Model and problem statement
Consider a dynamic N-node network observed over time intervals t = 1, . . . , T, repre-
sented by a graph whose temporal topology is encoded through a time-series of unknown,
time-indexed, and weighted adjacency matrices {At ∈RN×N}T
t=1. Per convention in net-
work studies, entry (i, j) of At (henceforth denoted by at
ij) is nonzero only if a directed
edge connects nodes i and j during the time interval t. The network topology is assumed
to remain ﬁxed per time interval t, but can change across intervals.
Over the course of the observation interval, many contagions propagate over the time-
varying network as illustrated in the 16-node directed network in Figure 10.5.1. Suppose a
ﬁxed number of contagions C is sampled, and the diﬀerence between infection time of node
i by contagion c and the earliest observation time is denoted by yt
ic ≥0. For uninfected
nodes at interval t, yt
ic is inﬁnite and will be set to a large positive value for practical
considerations. Assume that the susceptibility xic of node i to external (non-topological)
infection by contagion c is known and time invariant over the observation interval. In the
web context, xic can be set to the search engine rank of website i w.r.t. keywords associated
with c.
The model in [BMG14] postulates that yt
ic is linearly related to xic and the infection
times of its single-hop neighbors. Events adhering to this model of network-facilitated prop-
agation abound on the web where mention of e.g., a major baseball event by a blog will not
only depend on the times when similar blogs ﬁrst reported the event, but also the level of
interest of the blogger in baseball as a sport. Similarly, in epidemiological studies an indi-
vidual’s infection time by an infectious disease depends on both the infection times of her
immediate contacts as well as her immunity level to the disease. As a result yt
ic is modeled
according to the following linear dynamic structural equation model (SEM):
yt
ic =
X
j̸=i
at
ijyt
jc + bt
iixic + et
ic,
(10.5.1)
where bt
ii captures the time-varying level of inﬂuence of external sources, and et
ic accounts
for measurement errors and unmodeled dynamics. It follows from (10.5.1) that if at
ij ̸= 0,
then yt
ic is aﬀected by the value of yt
jc. Rewriting (10.5.1) for the entire network leads to
the vector model:
yt
c = Atyt
c + Btxc + et
c,
(10.5.2)

Big Data Analytics for Social Networks
323
where the N × 1 vector yt
c := [yt
1c, . . . , yt
Nc]T collects the node infection times by contagion
c during interval t, Bt := diag(bt
11, . . . , bt
NN); and likewise xc := [x1c, . . . , xNc]T and et
c :=
[et
1c, . . . , et
Nc]T . Collecting observations for all C contagions yields the dynamic matrix SEM:
Yt = AtYt + BtX + Et,
(10.5.3)
where Yt := [yt
1, . . . , yt
C], X := [x1, . . . , xC], and Et := [et
1, . . . , et
C] are all N × C matrices.
Given {Yt}T
t=1 and X, the goal is to track the underlying network topology {At}T
t=1, and the
eﬀect of external inﬂuences {Bt}T
t=1. In order to cope with constraints due to limited mea-
surement budgets, it is desirable that C ≪N. Unfortunately without further constraints,
this compromises the identiﬁability of (10.5.3). The approach outlined next overcomes this
limitation by leveraging the edge sparsity inherent to social networks.
10.5.1.2
Exponentially-weighted least-squares estimator
For the sake of exposition, consider the static setting with all {Yt}T
t=1 available. Lever-
aging the squared error cost leads to the batch problem:
{ ˆA, ˆB} = arg min
A,B
1
2
T
X
t=1
∥Yt −AYt −BX∥2
F + λ∥A∥1
s. t.
aii = 0, bij = 0, ∀i ̸= j,
(10.5.4)
where λ > 0 controls the sparsity level of ˆA. Reasonably assuming the absence of a self-
loop at node i leads to the constraint aii = 0, while having bij = 0, ∀i ̸= j, ensures that
ˆB is diagonal as in (10.5.2). Note that the estimator (10.5.4) tacitly assumes equal residual
variances since the infection times per cascade result from the same contagion over the
entire network.
In big data settings, measurements are more likely to be acquired sequentially over large
social networks (≥106 nodes), motivating online estimation algorithms with minimal stor-
age requirements. As a result, preference is given to recursive solvers facilitating sequential
topology inference. Incorporating a “forgetting factor” that assigns more weight to more
recent residuals then makes it possible to track slow temporal topological variations. Note
that the batch estimator (10.5.4) yields the single estimates { ˆA, ˆB} that best ﬁt the data
{Yt}T
t=1 and X over the entire measurement horizon t = 1, . . . , T, and as such (10.5.4)
neglects potential network variations across time intervals.
For t = 1, . . . , T, the sparsity-regularized exponentially-weighted LS estimator (EWLSE)
is given by:
{ ˆAt, ˆBt} = arg min
A,B
1
2
t
X
τ=1
βt−τ∥Yτ −AYτ −BX∥2
F + λt∥A∥1
s. t.
aii = 0, bij = 0, ∀i ̸= j,
(10.5.5)
where β ∈(0, 1] is the forgetting factor that forms estimates { ˆAt, ˆBt} using all measure-
ments acquired until time t. Whenever β < 1, past data are exponentially discarded thus en-
abling tracking of dynamic network topologies. The ﬁrst summand in the cost corresponds to
an exponentially-weighted moving average (EWMA) of the squared model residuals norms.
The EWMA can be seen as an average modulated by a sliding window of equivalent length
1/(1 −β), which clearly grows as β →1. In the so-termed inﬁnite-memory setting whereby
β = 1, (10.5.5) boils down to the batch estimator (10.5.4). Notice that λt is allowed to
vary with time in order to capture the generally changing edge sparsity level. In a linear
regression context, a related EWLSE was put forth in [ABG10] for adaptive estimation of
sparse signals; see also [KST11] for a projection-based adaptive algorithm.

324
Graph-Based Social Media Analysis
10.5.2
Topology tracking algorithm
Proximal gradient (PG) algorithms have been popularized for ℓ1-norm regularized linear
regression problems, through the class of iterative shrinkage-thresholding algorithms (ISTA);
see e.g., [DDM04] and [PB13] for a comprehensive tutorial treatment. The main advantage
of ISTA over oﬀ-the-shelf interior point methods is its computational simplicity. Iterations
boil down to matrix-vector multiplications involving the regression matrix, followed by a
soft-thresholding operation [HTF09, p. 93].
Introducing the optimization variable V := [A B], it follows that the gradient of f(V) :=
1
2
Pt
τ=1 βt−τ∥Yτ −AYτ −BX∥2
F is Lipschitz continuous, i.e., ∥∇f(V1) −∇f(V2)∥≤
Lf∥V1 −V2∥, ∀V1, V2 in the domain of f. The Lipschitz constant Lf is time varying, but
its dependence on t is kept implicit for notational convenience. Instead of directly optimizing
the cost in (10.5.5), PG algorithms minimize a sequence of overestimators evaluated at
judiciously chosen points.
Let k = 1, 2, . . . denote iterations and deﬁne g(V) := λt∥A∥1, PG algorithms iteratively
solve:
V[k] := arg min
V
Lf
2 ∥V −G(V[k −1])∥2
F + g(V)

,
(10.5.6)
where G(V[k −1]) := V[k −1] −(1/Lf)∇f(V[k −1]) corresponds to a gradient-descent
step taken from V[k −1], with step-size equal to 1/Lf. The optimization problem (10.5.6)
is known as the proximal operator of the function g/Lf evaluated at G(V[k −1]), and is
denoted as proxg/Lf (G(V[k−1])). Henceforth adopting the notation G[k−1] := G(V[k−1])
for convenience, the PG iterations can be compactly rewritten as V[k] = proxg/Lf (G[k−1]).
A key element to the success of PG algorithms stems from the possibility of eﬃciently
evaluating the proximal operator (cf. (10.5.6)). Specializing to (10.5.5), note that (10.5.6)
decomposes into:
A[k] := arg min
A
Lf
2 ∥A −GA[k −1]∥2
F + λt∥A∥1

= Sλt/Lf (GA[k −1])
(10.5.7)
B[k] := arg min
B

∥B −GB[k −1]∥2
F
	
= GB[k −1],
(10.5.8)
subject to the constraints in (10.5.5), which so far have been left implicit, and G := [GAGB].
Letting Sµ(M) with (i, j)-th entry given by sign(mij) max(|mij| −µ, 0) denote the soft-
thresholding operator, it follows that proxλt∥·∥1/Lf (·) = Sλt/Lf (·), e.g., [DDM04, HTF09].
Because there is no regularization on the matrix B, the corresponding update (10.5.8)
boils-down to a simple gradient-descent step.
What remains now is to obtain expressions for the gradient of f(V) with respect to A
and B, which are required to form the matrices GA and GB. To this end, note that by
incorporating the constraints aii = 0 and bij = 0, ∀j ̸= i, i = 1, . . . N, one can simplify the
expression of f(V) as:
f(V) := 1
2
t
X
τ=1
N
X
i=1
βt−τ∥(yτ
i )T −aT
−iYτ
−i −biixT
i ∥2
F ,
(10.5.9)
where (yτ
i )T and xT
i denote the i-th row of Yτ and X, respectively; while aT
−i denotes the
1 × (N −1) vector obtained by removing entry i from the i-th row of A, and likewise Yτ
−i
is the (N −1)×C matrix obtained by removing row i from Yτ. It is apparent from (10.5.9)
that f(V) is separable across the trimmed row vectors a⊤
−i, and the diagonal entries bii,

Big Data Analytics for Social Networks
325
i = 1, . . . , N. The sought gradients are:
∇a−if(V) = Σt
−ia−i + ¯Yt
−ixibii −σt
−i
(10.5.10)
∇biif(V) = aT
−i ¯Yt
−ixi + 1 −βt
1 −β bii∥xi∥2
2 −(¯yτ
i )T xi,
(10.5.11)
where (¯yt
i)T denotes the i-th row of ¯Yt := Pt
τ=1 βt−τYτ, and ¯Yt
−i := Pt
τ=1 βt−τYτ
−i.
Similarly, σt
−i := Pt
τ=1 βt−τYτ
−iyτ
i and Σt
−i is obtained by removing the i-th row and i-th
column from Σt := Pt
τ=1 βt−τYτ(Yτ)T . From (10.5.7)-(10.5.8) and (10.5.10)-(10.5.11), the
parallel ISTA iterations:
∇a−if[k] = Σt
−ia−i[k] + ¯Yt
−ixibii[k] −σt
−i
(10.5.12)
∇biif[k] = aT
−i[k] ¯Yt
−ixi + (1 −βt)
1 −β bii[k]∥xi∥2
2 −(¯yt
i)T xi
(10.5.13)
a−i[k + 1] = Sλt/Lf
 a−i[k] −(1/Lf)∇a−if[k]

(10.5.14)
bii[k + 1] = bii[k] −(1/Lf)∇biif[k]
(10.5.15)
are provably convergent to the globally optimal solution { ˆAt, ˆBt} of (10.5.5), as per the
general convergence results available for PG methods and ISTA in particular [DDM04,
PB13].
Computation of the gradients in (10.5.12)-(10.5.13) requires one matrix-vector muti-
plication by Σt
−i and one by ¯Yt
−i, in addition to three vector inner-products, plus a few
(negligibly complex) scalar and vector additions. Both the update of bii[k + 1] as well as
the soft-thresholding operation in (10.5.14) entail negligible computational complexity. Per
iteration, the actual rows of the adjacency matrix are obtained by zero-padding the updated
a−i[k], namely setting:
aT
i [k] = [a−i,1[k] . . . a−i,i−1[k] 0 a−i,i[k] . . . a−i,N[k]].
(10.5.16)
This way, the desired SEM parameter estimates at time t are given by
ˆAt
=
[a⊤
1 [k], . . . , a⊤
N[k]]T and ˆBt = diag(b11[k], . . . , bNN[k]), for k large enough so that conver-
gence has been attained.
Solving (10.5.5) over the entire time horizon t = 1, . . . , T. To track the dynamically-
evolving network topology, one can go ahead and solve (10.5.5) sequentially for each
t = 1, . . . , T as data arrive, using (10.5.12)-(10.5.15). Because the network is assumed to
vary slowly across time, it is convenient to warm-restart the ISTA iterations, that is, at
time t initialize {A[0], B[0]} with the solution { ˆAt−1, ˆBt−1}. This way, for smooth network
variations one expects convergence to be attained after few iterations.
To obtain the new SEM parameter estimates via (10.5.12)-(10.5.15), it suﬃces to update
(possibly) λt and the Lipschitz constant Lf, as well as the data-dependent EWMAs Σt,
and ¯Yt. Interestingly, the potential growing-memory problem in storing the entire history
of data {Yt}T
t=1 can be avoided by performing the recursive updates:
Σt = βΣt−1 + Yt(Yt)T ,
¯Yt = β ¯Yt−1 + Yt.
(10.5.17)
The complexity in evaluating the Gram matrix Yt(Yt)T dominates the per-iteration com-
putational cost of the algorithm. To circumvent the need of recomputing the Lipschitz con-
stant per time interval, the step-size 1/Lf in (10.5.14)-(10.5.15) can be selected by a line
search [PB13]. One choice is the backtracking step-size rule [BT09], for which convergence
to { ˆAt, ˆBt} can be established as well.
Algorithm 10.5.1 summarizes the steps outlined in this section for tracking the dynamic

326
Graph-Based Social Media Analysis
Algorithm 10.5.1: Pseudo real-time ISTA for topology tracking
Require: {Yt}T
t=1, X, β.
1: Initialize ˆA0 = 0N×N, ˆB0 = Σ0 = IN, ¯Y0 = 0N×C, λ0.
2: for t = 1, . . . , T do
3:
Update λt, Lf and Σt, ¯Yt via (10.5.17).
4:
Initialize A[0] = ˆAt−1, B[0] = ˆBt−1, and set k = 0.
5:
while not converged do
6:
for i = 1 . . . N (in parallel) do
7:
Compute Σt
−i and ¯Yt
−i.
8:
Form gradients at a−i[k] and bii[k] via (10.5.12)-(10.5.13).
9:
Update a−i[k + 1] via (10.5.14).
10:
Update bii[k + 1] via (10.5.15).
11:
Update ai[k + 1] via (10.5.16).
12:
end for
13:
k = k + 1.
14:
end while
15:
return
ˆAt = A[k], ˆBt = B[k].
16: end for
network topology, given temporal traces of infection events {Yt}T
t=1 and susceptibilities X.
It is termed pseudo real-time ISTA, since in principle one needs to run multiple (inner)
ISTA iterations till convergence per time interval t = 1, . . . , T. This will in turn incur
an associated delay, that may (or may not) be tolerable depending on the speciﬁc network
inference problem at hand. Nevertheless, numerical tests indicate that in practice 5-10 inner
iterations suﬃce for convergence; see [BMG14] for further details.
10.5.2.1
Accelerated convergence
For big data applications, ﬁrst-order methods such as ISTA are often the only admissible
option. Recently, several eﬀorts have led to improvement of the sublinear global rate of
convergence exhibited by PG algorithms while retaining their computational simplicity see
e.g., [Nes83, Nes05, BT09] and references therein.
The so-termed accelerated (A)PG algorithm has been shown to remarkably attain con-
vergence speedups in [Nes05]. APG algorithms generate the following sequence of iterates:
V[k] = arg min
V Q(V, U[k −1]) = proxg/Lf (G(U[k −1])),
where
U[k] := V[k −1] +
c[k −1] −1
c[k]

(V[k −1] −V[k −2])
(10.5.18)
c[k] = 1 +
p
4c2[k −1] + 1
2
.
(10.5.19)
The accelerated PG algorithm [a.k.a. fast (F)ISTA] utilizes a linear combination of the
previous two iterates {V[k−1], V[k−2]}. The iteration-dependent combination weights are
a function of the scalar sequence (10.5.19). FISTA aﬀords a (worst-case) convergence rate
guarantee of O(1/√ϵ) iterations to return an ϵ-optimal solution measured by its objective
value (ISTA instead aﬀords O(1/ϵ)) [BT09, Nes05]. With a few minor changes, (10.5.12)-
(10.5.15) can be modiﬁed to attain accelerated convergence (see [BMG14] for details). A
slight compromise to adopting FISTA is the increased memory cost for storing the two prior
estimates of A and B.

Big Data Analytics for Social Networks
327
0
200
400
600
800
1000
10
−4
10
−3
10
−2
10
−1
10
0
time
MSE
 
 
1 iteration
5 iterations
10 iterations
15 iterations
a)
0
100
200
300
400
500
600
700
800
900
1000
10
−4
10
−3
10
−2
10
−1
10
0
time
MSE
 
 
SGD
ISTA
FISTA
ADMM
b)
FIGURE 10.5.2: a) MSE (i.e., P
i,j(ˆat
ij −at
ij)2/N 2) performance of Algorithm 10.5.1 versus
time. For each t, (10.5.5) is solved “inexactly” for k = 1, 5, 10, and 15 inner iterations. It
is apparent that k = 5 iterations suﬃce to attain convergence to the minimizer of (10.5.5)
per t, especially after a short transient where the warm-restarts oﬀer increasingly better
initializations. b) MSE performance of real-time algorithms versus time. Real-time FISTA,
Algorithm 10.5.2 (SGD), as well as inexact versions of Algorithm 10.5.1 (ISTA) and the
ADMM solver in [BMG13] are compared. (See color insert.)
10.5.3
Real-Time operation
Under streaming big data settings, it may be impractical to run multiple inner (F)ISTA
iterations per time interval in the quest for convergence. In fact a high-quality answer
obtained slowly may not be as valuable as a medium-quality answer that is obtained quickly.
The remainder of this section focuses on strategies for online operation of the topology
tracking algorithms, namely: i) termination of inner iterations prematurely; and ii) pursuing
stochastic gradient iterations.
10.5.3.1
Premature termination
Consider a scenario where the underlying network processes are stationary (or piecewise
stationary with suﬃciently long coherence time). Premature termination is justiﬁed by the
fact that the solution of (10.5.5) for each t = 1, . . . , T does not need to be very accurate
since it is just an intermediate step in the outer loop matched to the time-instants of data
acquisition. In fact, it may be reasonable to run a single inner-iteration (so that k coincides
with the time index t).
For synthetically-generated data according to the setup described in [BMG14], Fig-
ure 10.5.2 shows the time evolution of the mean-square error (MSE) estimation performance
upon running FISTA. For each time interval t, (10.5.5) is solved “inexactly” after running
only k = 1, 5, 10 and 15 inner iterations. Note that realtime operation corresponds to k = 1.
10.5.3.2
Stochastic gradient descent iterations
Supposing β = 0 in (10.5.5), the resulting cost function can be expressed as ft(V) +
g(V), where V := [A B] and ft(V) := (1/2)∥Yt −AYt −BX∥2
F only accounts for data
acquired during time interval t. Solving the simpliﬁed optimization problem based only
on instantaneous data can be accomplished by following stochastic gradient descent (SGD)
iterations whose simplicity and tracking capabilities are well documented. Thus, one obtains

328
Graph-Based Social Media Analysis
Algorithm 10.5.2: SGD algorithm for topology tracking
Require: {Yt}T
t=1, X, η.
1: Initialize A[1] = 0N×N, B[1] = IN, λ1.
2: for t = 1, . . . , T do
3:
Update λt.
4:
for i = 1 . . . N (in parallel) do
5:
Form gradients at a−i[t] and bii[t] via (10.5.20)-(10.5.21).
6:
Update a−i[t + 1] via (10.5.22).
7:
Update bii[t + 1] via (10.5.23).
8:
Update ai[t + 1] via (10.5.16).
9:
end for
10:
return
ˆAt = A[t + 1], ˆBt = B[t + 1].
11: end for
the following updates:
∇a−ift[t] = Yt
−i
 (Yt
−i)T a−i[t] + xibii[t] −yt
i

(10.5.20)
∇biift[t] = aT
−i[t]Yt
−ixi + bii[t]∥xi∥2 −(yt
i)T xi
(10.5.21)
a−i[t + 1] = Sλt/η
 a−i[t] −η∇a−ift[t]

(10.5.22)
bii[t + 1] = bii[t] −η∇biift[t].
(10.5.23)
Compared to the parallel ISTA iterations in Algorithm 10.5.1 [cf. (10.5.12)-(10.5.14)], three
main diﬀerences are noteworthy: (i) iterations k are merged with the time intervals t of data
acquisition; (ii) the stochastic gradients ∇a−ift[t] and ∇biift[t] involve the (noisy) data
{Yt(Yt)T , Yt} instead of their time-averaged counterparts {Σt, ¯Yt}; and (iii) a generic
constant step-size η is utilized for the gradient descent steps.
The overall SGD algorithm is tabulated under Algorithm 10.5.2. Accelerated versions
could be developed as well, at the expense of a marginal increase in computational com-
plexity and doubling of memory requirements.
10.5.4
Experiments on real data
The tracking algorithms were tested on real cascade data obtained by monitoring
blog posts and news articles on the web between March 2011 and February 2012 (45
weeks) [LK14]. Popular textual phrases (a.k.a. memes) due to globally-popular topics dur-
ing this period were identiﬁed and the times when they were mentioned on the websites
were recorded as Unix timestamps (i.e., number of hours since midnight on January 1,
1970). In order to test the tracking algorithms, cascade traces related to two keywords were
extracted: i) “Kim Jong-un” the current leader of North Korea whose popularity rose after
the death of his father (and predecessor); and ii) “Reid Hoﬀman” the founder of LinkedIn.
Only signiﬁcant cascades that propagated to at least 7 websites were retained. This re-
sulted in N = 360 websites, C = 466 cascades, and T = 45 weeks for “Kim Jong-un”
memes. Similarly, N = 125, C = 85, and T = 41 weeks for “Reid Hoﬀman”.
In cases where website i made no mention of cascade c during interval t, yt
ic was set to
100tmax (i.e., a large number), where tmax denotes the largest timestamp in the dataset.
The entries of marix X typically capture prior knowledge about susceptibility of nodes to
contagions. In the web context, xic could be aptly set to the average search engine ranking
of website i on keywords pertaining to c. In the absence of such real data for the observation
interval, the entries of X were uniformly sampled over the interval [0, 0.01].

Big Data Analytics for Social Networks
329
t = 10
t = 40
0
5
10
15
20
25
30
35
40
45
0
1000
2000
3000
4000
5000
6000
7000
time (weeks)
total number of edges
Death of Kim Jong−il
Kim Jong−un appointed as vice
chairman of military commission
Kim Jong−un becomes ruler of N. Korea
FIGURE 10.5.3: (Top) Visualization of estimated networks from information cascades re-
lated to the topic “Kim Jong-un” at t = 10 and t = 40 weeks. (Bottom) Evolution of total
number of inferred edges [BMG14].
t = 5
t = 30
5
10
15
20
25
30
35
40
1500
2000
2500
3000
3500
4000
4500
5000
5500
time (weeks)
total number of edges
LinkedIn IPO
Zynga IPO
Groupon IPO
FIGURE 10.5.4: (Top) Visualization of estimated networks obtained by tracking “Reid
Hoﬀman” cascades at t = 5 and t = 30 weeks. (Bottom) Evolution of total number of
inferred edges [BMG14].
Experimental results. Algorithm 10.5.1 was run on both datasets with β = 0.9 and
λt = 100. Figure 10.5.3 (top) depicts drawings of the inferred network for Kim Jong-un at
t = 10 and t = 40 weeks. Speculation about the possible successor of the dying North Korean
ruler, Kim Jong-il, rose until his death on December 17, 2011 (week 38). He was succeeded
by Kim Jong-un on December 30, 2011 (week 40). The visualizations show an increasing
number of edges over the 45 weeks, illustrating the growing interest of international news
websites and blogs in the new ruler, about whom little was known in the ﬁrst 10 weeks.
Unfortunately, the observation horizon does not go beyond T = 45 weeks. A longer span of
data would have been useful to investigate the rate at which global news coverage on the
topic eventually subsided.
Figure 10.5.3 (bottom) depicts the time evolution of the total number of edges in the
inferred dynamic network. Of particular interest are the weeks during which: i) Kim Jong-
un was appointed as the vice chairman of the North Korean military commission; ii) Kim
Jong-il died; and iii) Kim Jong-un became the ruler of North Korea. These events were the
topics of many online news articles and political blogs, an observation that is reinforced by
the experimental results shown in the plot.
The results of running Algorithm 10.5.1 on the second dataset are shown in Figure 10.5.4.
Although Reid Hoﬀman was already popular in technology media coverage, his visibility
in popular news and blogs increased tremendously following the highly successful initial
public oﬀering (IPO) of LinkedIn on May 19, 2011. Toward the end of 2011, a number

330
Graph-Based Social Media Analysis
of other successful technology companies like Groupon and Zynga went public, possibly
stabilizing the amount of media coverage on Reid Hoﬀman. In fact, the drop in number
of edges towards week 41 could be attributed to the captivation of media attention by the
IPOs that occurred later in the year.
10.6
Conclusion
Big data analytics has risen to prominence within the last few years, and it remains a
very active research area for the foreseeable future. In parallel, computational analysis of
social networks has recently emerged as a versatile, cross-disciplinary ﬁeld. Interestingly, a
number of problems encountered in mining the web, learning and prediction of consumer
behavior, and the dynamics of the spread of infectious diseases, all lie at the intersection of
social networks, big data, and eﬃcient (online) optimization.
Towards addressing these big data problems, signal processing and machine learning oﬀer
a robust framework for advanced data analytics. A fair and totally balanced survey of all
pertinent issues and approaches is impossible within the scope of one chapter. Nevertheless,
this chapter presented several interesting problems of recent interest within the social media
community, namely: visualization of large social graphs, inference and imputation over social
networks, community discovery, and tracking dynamic network topologies from information
cascades. Eﬃcient algorithms scaling well under big data settings along with experimental
tests have been presented, and wherever possible, references to contemporary and prior
approaches have been highlighted.
10.7
Acknowledgments
The authors wish to thank the following friends, colleagues, and co-authors who con-
tributed to their joint publications from which the material of this chapter was extracted:
Drs. J. A. Bazerque, P. Forero, S.-J. Kim, M. Mardani, K. Rajawat, and K. Slavakis. The
work was supported in part by NSF grants 1343248, 1423316, 1442686, 1514056, and EA-
GER 1500713; as well as the MURI grant no. AFOSR FA9550-10-1-0567; and the NIH grant
no. 1R01GM10GM4975-01. This work has also been supported by the European Social Fund
and Greek national funds through the Operational Program “Education and Lifelong Learn-
ing” of the National Strategic Framework-Research Funding Program: Thalis-UoA-Secure
Wireless Nonlinear Communications at the Physica.
Bibliography
[ABG10]
D. Angelosante, J. A. Bazerque, and G. B. Giannakis. Online adaptive esti-
mation of sparse signals: where RLS meets the ℓ1-norm. IEEE Transactions
on Signal Processing, 58:3436–3447, 2010.

Big Data Analytics for Social Networks
331
[AG11]
D. Angelosante and G. B. Giannakis. Sparse graphical modeling of piecewise-
stationary time series.
In Proc. of International Conference on Acoustics,
Speech and Signal Processing, pages 1960–1963, 2011.
[AHDBV06] J. Alvarez-Hamelin, L. Dall’Asta, A. Barrat, and A. Vespignani. Large scale
networks ﬁngerprinting and visualization using the k-core decomposition. Ad-
vances in Neural Information Processing Systems, 18:41–50, 2006.
[AMF12]
L. Akoglu, M. McGlohon, and C. Faloutsos. OddBall: Spotting anomalies in
weighted graphs. In Proc. Paciﬁc Asia Knowledge Discovery and Data Mining,
pages 410–421, 2012.
[BBG13]
J. A. Bazerque, B. Baingana, and G. B. Giannakis. Identiﬁability of sparse
structural equation models for directed and cyclic networks. In Proc. of Global
Conference on Signal and Info. Processing, pages 839–842, 2013.
[BC03]
U. Brandes and S. R. Corman. Visual unrolling of network evolution and the
analysis of dynamic discourse. Information Visualization, 2(1):40–50, 2003.
[Ber99]
D. P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, 1999.
[BG05]
I. Borg and P. J. Groenen.
Modern Multidimensional Scaling: Theory and
Applications. Springer, 2005.
[BG13]
B. Baingana and G. B. Giannakis. Centrality-constrained graph embedding.
In Proc. of the 38th International Conference on Acoustics, Speech and Signal
Processing, pages 3113–3117, 2013.
[BG15]
B. Baingana and G. B. Giannakis. Kernel-based embeddings for large graphs
with centrality constraints. In Proc. of the 40th International Conference on
Acoustics, Speech and Signal Processing, 2015.
[BMG13]
B. Baingana, G. Mateos, and G. B. Giannakis. Dynamic structural equation
models for tracking topologies of social networks. In Proc. of 5th International
Workshop on Computational Advances in Multi-Sensor Adaptive Processing,
pages 292–295, 2013.
[BMG14]
B. Baingana, G. Mateos, and G. B. Giannakis. Proximal-gradient algorithms
for tracking cascades over social networks. IEEE Journal of Selected Topics
in Signal Processing, 8(4):563–575, 2014.
[BNS06]
M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A geomet-
ric framework for learning from labeled and unlabeled examples. Journal of
Machine Learning Research, 7:2399–2434, 2006.
[BP98]
S. Brin and L. Page. The anatomy of a large scale hypertextual web search
engine. Computer Networks and ISDN Systems, 30(1):107–117, 1998.
[BP11]
U. Brandes and C. Pich. More ﬂexible radial layout. Journal of Graph Algo-
rithms and Applications, 15:157–173, 2011.
[BPC+11]
S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimiza-
tion and statistical learning via the alternating direction method of multipliers.
Foundations and Trends in Machine Learning, 3:1–122, 2011.
[BT99]
D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computation:
Numerical Methods. Athena-Scientiﬁc, second edition, 1999.

332
Graph-Based Social Media Analysis
[BT09]
A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm
for linear inverse problems. SIAM Journal on Imaging Sciences, 2:183–202,
2009.
[BW97]
U. Brandes and D. Wagner. A Bayesian paradigm for dynamic graph layout. In
Proc. of the 5th International Symposium on Graph Drawing, pages 236–247,
1997.
[BW98]
U. Brandes and D. Wagner. Dynamic grid embedding with few bends and
changes. In Proc. of the 9th Annual International Symposium on Algorithms
and Computation, pages 89–98, 1998.
[CBG13]
X. Cai, J. A. Bazerque, and G. B. Giannakis. Inference of gene regulatory net-
works with sparse structural equation models exploiting genetic perturbations.
PLoS Computational Biology, 9(5):1–13, 2013.
[CDK+99]
S. Chakrabarti, B. E. Dom, S. R. Kumar, P. Raghavan, S. Rajagopalan,
A. Tomkins, D. Gibson, and J. Kleinberg. Mining the web’s link structure.
IEEE Computer, 32(8):60–67, 1999.
[CG84]
A. Chistov and D. Grigorev. Complexity of quantiﬁer elimination in the theory
of algebraically closed ﬁelds. In Math. Found. of Computer Science, volume
176 of Lecture Notes in Computer Science, pages 17–31. Springer, 1984.
[CGH09]
Y. Chen, Y. Gu, and A. O. Hero III. Sparse LMS for system identiﬁcation. In
Proc. of International Conference on Acoustics, Speech and Signal Processing,
pages 3125–3128, 2009.
[CJHJ11]
R. Chitta, R. Jin, T. C. Havens, and A. K. Jain.
Approximate kernel k-
means: Solution to large scale kernel clustering. In Proc. of the 17th ACM
International Conference on Knowledge discovery and data mining, pages 895–
903, 2011.
[CLL+10]
J. Chen, W. Li, A. Lau, J. Cao, and K. Eang. Automated load curve data
cleansing in power systems. IEEE Transactions on Smart Grid, 1(1):213–221,
2010.
[CLMW11]
E. J. Cand`es, X. Li, Y. Ma, and J. Wright.
Robust principal component
analysis? Journal of the ACM, 58(1):1–37, 2011.
[CPR07]
M. Coates, Y. Pointurier, and M. Rabbat. Compressed network monitoring for
IP and all-optical networks. In Proc. ACM Internet Measurement Conference,
pages 241–252, 2007.
[CSB+11]
W. Y. Chen, Y. Song, H. Bai, C. J. Lin, and E. Y. Chang. Parallel spectral
clustering in distributed systems. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 33(3):568–586, 2011.
[CSPW11]
V. Chandrasekaran, S. Sanghavi, P. R. Parrilo, and A. S. Willsky.
Rank-
sparsity incoherence for matrix decomposition. SIAM Journal on Optimiza-
tion, 21(2):572–596, 2011.
[CW08]
E. Cand`es and M. B. Wakin. An introduction to compressive sampling. IEEE
Signal Processing Magazine, 25(2):21–30, 2008.

Big Data Analytics for Social Networks
333
[DBD13]
U. Dogrusoz, M. E. Belviranli, and A. Dilek. Cise: A circular spring embedder
layout algorithm. IEEE Transactions on Visualization and Computer Graph-
ics, 19:953–966, 2013.
[DDM04]
I. Daubechies, M. Defrise, and C. D. Mol. An iterative thresholding algorithm
for linear inverse problems with a sparsity constraint. Communications on
Pure and Applied Mathematics, 57:1413–1457, 2004.
[DG08]
J. Dean and S. Ghemawat. MapReduce: Simpliﬁed data processing on large
clusters. Communications of the ACM, 51(1):107–113, 2008.
[DGK04]
I. S. Dhillon, Y. Guan, and B. Kulis.
Kernel k-means: Spectral clustering
and normalized cuts. In Proc. of the 10th ACM International Conference on
Knowledge Discovery and Data Mining, pages 551–556, 2004.
[DGK07]
I. S. Dhillon, Y. Guan, and B. Kulis. Weighted graph cuts without eigenvectors:
A multilevel approach. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 29(11):1944–1957, 2007.
[Don06]
D. L. Donoho. Compressed sensing. IEEE Transactions on Information The-
ory, 52(4):1289 –1306, 2006.
[EFKK14]
A. Elgohary, A. K. Farahat, M. S. Kamel, and F. Karray. Embed and conquer:
Scalable embeddings for kernel k-means on MapReduce. In SIAM Interna-
tional Conference on Data Mining, 2014.
[EH07]
W. Eberle and L. B. Holder. Discovering structural anomalies in graph-based
data. In Proc. of International Conference on Data Mining, pages 393–398,
2007.
[EK10]
D. Easley and J. Kleinberg. Networks, Crowds, and Markets: Reasoning About
a Highly Connected World. Cambridge University Press, 2010.
[FBCM04]
C. Fowlkes, S. Belongie, F. Chung, and J. Malik.
Spectral grouping using
the Nystrom method. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 26(2):214–225, 2004.
[FFF99]
M. Faloutsos, P. Faloutsos, and C. Faloutsos. On power-law relationships of the
Internet topology. In Proc. of Special Interest Group on Data Communications,
pages 251–262, 1999.
[FKG11]
P. Forero, V. Kekatos, and G. B. Giannakis. Outlier-aware robust clustering. In
Proc. of International Conference on Acoustics, Speech and Signal Processing,
pages 2244–2247, 2011.
[For10]
S. Fortunato. Community detection in graphs. Physics Reports, 486(3):75–174,
2010.
[FPRS07]
F. Fouss, A. Pirotte, J. M. Renders, and M. Saerens. Random-walk computa-
tion of similarities between nodes of a graph with application to collaborative
recommendation. IEEE Trans. on Knowledge and Data Engineering, 19:355–
369, 2007.
[Fre77]
L. C. Freeman. A set of measures of centrality based on betweenness. Sociom-
etry, 40:35–41, 1977.

334
Graph-Based Social Media Analysis
[FRG14]
P. Forero, K. Rajawat, and G. B. Giannakis. Prediction of partially observed
dynamical processes over networks via dictionary learning. IEEE Transactions
on Signal Processing, 62(13):3305–3320, 2014.
[FT08]
Y. Frishman and A. Tal. Online dynamic graph drawing. IEEE Transactions
on Visualization and Computer Graphics, 14(4):727–740, 2008.
[GKB13]
A. Gittens, P. Kambadur, and C. Boutsidis. Approximate spectral clustering
via randomized sketching. Computing Research Repository, 2013.
[GL12]
G. H. Golub and C. F. V. Loan. Matrix Computations, volume 3. JHU Press,
2012.
[GN02]
M. Girvan and M. E. J. Newman. Community structure in social and biological
networks. Proceedings of the National Academy of Sciences, 99(12):7821–7826,
2002.
[Gol72]
A. S. Goldberger. Structural equation methods in the social sciences. Econo-
metrica, 40:979–1001, 1972.
[Het00]
H. W. Hethcote.
The mathematics of infectious diseases.
SIAM Review,
42(4):599–653, 2000.
[HL12]
L. Harrison and A. Lu.
The future of security visualization: Lessons from
network visualization. IEEE Network, 26:6–11, 2012.
[HTF09]
T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learn-
ing. Springer, second edition, 2009.
[Int]
Internet2. Internet2 network ﬂow data. http://www.internet2.edu. Accessed:
2014.
[Jac10]
M. O. Jackson. Social and Economic Networks. Princeton University Press,
2010.
[Kap09]
D. Kaplan. Structural Equation Modeling: Foundations and Extensions. Sage
Publications, second edition, 2009.
[KK89]
T. Kamada and S. Kawai. An algorithm for drawing general undirected graphs.
Information Processing Letters, 31:7–15, 1989.
[KM08]
D. Kempe and S. McSherry. A decentralized algorithm for spectral analysis.
Journal of Computer and Systems Sciences, 74:70–83, 2008.
[Kol09]
E. D. Kolaczyk. Statistical Analysis of Network Data: Methods and Models.
Springer, 2009.
[KST11]
Y. Kopsinis, K. Slavakis, and S. Theodoridis. Online sparse system identiﬁca-
tion and signal reconstruction using projections onto weighted ℓ1 balls. IEEE
Transactions on Signal Processing, 59:936–952, 2011.
[KTF09]
U. Kang, C. E. Tsourakakis, and C. Faloutsos. PEGASUS: A peta-scale graph
mining system – implementation and observations. In Proc. of International
Conference on Data Mining, pages 229–238, 2009.
[LC10]
F. Lin and W. W. Cohen. Power iteration clustering. In Proc. of the 27th
International Conference on Machine Learning, pages 655–662, 2010.

Big Data Analytics for Social Networks
335
[LCD04]
A. Lakhina, M. Crovella, and C. Diot. Diagnosing network-wide traﬃc anoma-
lies. In Proc. of the 2004 Conference on Applications, Technologies, Architec-
tures, and Protocols for Computer Communications, pages 219–230, 2004.
[LdH08]
B. Liu, A. de la Fuente, and I. Hoeschele. Gene network inference via structural
equation modeling in genetical genomics experiments.
Genetics, 178:1763–
1776, 2008.
[Les11]
J. Leskovec. General relativity and quantum cosmology collaboration network.
Stanford Network Analysis Project, 2011.
[Les12]
J. Leskovec. Social circles: Facebook. Stanford Network Analysis Project, 2012.
[LGW+11]
Z. Lin, A. Ganesh, J. Wright, L. Wu, M. Chen, and Y. Ma.
Fast convex
optimization algorithms for exact recovery of a corrupted low-rank matrix.
UIUC Tech. Report UILU-ENG-09-2214, 2011.
[LK14]
J. Leskovec and A. Krevl. SNAP Datasets: Stanford large network dataset
collection. http://snap.stanford.edu/data, June 2014.
[LKF07]
J. Leskovec, J. Kleinberg, and C. Faloutsos. Graph evolution: Densiﬁcation
and shrinking diameters. ACM Transactions on Knowledge Discovery from
Data, 1(1), 2007.
[LS08]
L. Leydesdorﬀand T. Schank. Dynamic animations of journal maps: Indica-
tors of structural changes and interdisciplinary developments. Journal of the
American Society for Information Science and Technology, 59(11):1810–1818,
2008.
[LSY98]
R. B. Lehoucq, D. C. Sorensen, and C. Yang. ARPACK Users’ Guide: Solution
of Large-scale Eigenvalue Problems with Implicitly Restarted Arnoldi Methods,
volume 6. SIAM, 1998.
[Lux07]
U. V. Luxburg. A tutorial on spectral clustering. Statistics and Computing,
17(4):395–416, 2007.
[LWH03]
B. Luo, R. C. Wilson, and E. R. Hancock. Spectral embedding of graphs.
Pattern Recognition, 36:2213–2230, 2003.
[MAB13]
B. A. Miller, N. Arcolano, and N. T. Bliss. Eﬃcient anomaly detection in
dynamic, attributed graphs: Emerging phenomena and big data.
In Proc.
International Conference Intelligence and Security Informatics, pages 179–
184, 2013.
[Mah11]
M. W. Mahoney. Randomized algorithms for matrices and data. Foundations
and Trends in Machine Learning, 3(2):123–224, 2011.
[MBG10]
G. Mateos, J. A. Bazerque, and G. B. Giannakis. Distributed sparse linear
regression. IEEE Transactions on Signal Processing, 58(10):5262–5276, 2010.
[MBPS10]
J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online learning for matrix factor-
ization and sparse coding. Journal of Machine Learning Research, 11:19–60,
2010.
[MG12a]
G. Mateos and G. B. Giannakis. Robust nonparametric regression via sparsity
control with application to load curve data cleansing. IEEE Transactions on
Signal Processing, 60(4):1571–1584, 2012.

336
Graph-Based Social Media Analysis
[MG12b]
G. Mateos and G. B. Giannakis.
Robust PCA as bilinear decomposition
with outlier-sparsity regularization. IEEE Transactions on Signal Processing,
60(10):5176–5190, 2012.
[MG13]
G. Mateos and G. B. Giannakis. Load curve data cleansing and imputation
via sparsity and low rank. IEEE Transactions on Smart Grid, 4(4):2347–2355,
2013.
[ML13]
S. Meyers and J. Leskovec. On the convexity of latent social network inference.
In Proc. of Neural Information Processing Systems, pages 1741–1749, 2013.
[MM13]
S. Mankad and G. Michailidis.
Structural and functional discovery in dy-
namic networks with non-negative matrix factorization. Physical Review E,
88(4):042812, 2013.
[MMBd05]
J. Moody, D. McFarland, and S. Bender-deMoll. Dynamic network visualiza-
tion. American Journal of Sociology, 110(4):1206–1241, 2005.
[MMBd06]
J. Moody, D. McFarland, and S. Bender-deMoll. The art and science of dy-
namic network visualization. Journal of Social Structure, 7(2):1–38, 2006.
[MMG13a]
M. Mardani, G. Mateos, and G. B. Giannakis.
Decentralized sparsity-
regularized rank minimization: Algorithms and applications.
IEEE Trans-
actions on Signal Processing, 61:5374–5388, 2013.
[MMG13b]
M. Mardani, G. Mateos, and G. B. Giannakis.
Dynamic anomalography:
Tracking network anomalies via sparsity and low rank. IEEE Journal of Se-
lected Topics in Signal Processing, 7:50–66, 2013.
[MMG13c]
M. Mardani, G. Mateos, and G. B. Giannakis.
Recovery of low-rank plus
compressed sparse matrices with application to unveiling traﬃc anomalies.
IEEE Transactions on Information Theory, 59:5186–5205, 2013.
[MR13]
G. Mateos and K. Rajawat.
Dynamic network cartography.
IEEE Signal
Processing Magazine, 30(3):129–143, 2013.
[MT08]
H. D. K. Moonesinghe and P.-N. Tan. OutRank: A graph-based outlier de-
tection framework using random walks. International Journal on Artiﬁcial
Intelligence Tools, 17(1):1–18, 2008.
[Mut84]
B. Muth´en. A general structural equation model with dichotomous, ordered
categorical, and continuous latent variable indicators. Pyschometrika, 49:115–
132, 1984.
[Nat95]
B. K. Natarajan.
Sparse approximate solutions to linear systems.
SIAM
Journal on Computing, 24:227–234, 1995.
[NC03]
C. C. Noble and D. J. Cook. Graph-based anomaly detection. In Proc. of
Special Interest Group on Knowledge Discovery and Data Mining, pages 631–
636, 2003.
[Nes83]
Y. Nesterov. A method of solving a convex programming problem with con-
vergence rate O(1/k2). Soviet Mathematics Doklady, 27:372–376, 1983.
[Nes05]
Y. Nesterov.
Smooth minimization of nonsmooth functions.
Mathematical
Programming, 103:127–152, 2005.

Big Data Analytics for Social Networks
337
[New10]
M. E. J. Newman. Networks: An Introduction. Oxford University Press, 2010.
[NJW+02]
A. Y. Ng, M. I. Jordan, Y. Weiss, et al. On spectral clustering: Analysis and an
algorithm. In Advances in Neural Information Processing Systems, volume 2,
pages 849–856, 2002.
[OF97]
B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis
set: A strategy employed by v1? Vision Research, 37(23):3311–3325, 1997.
[PB13]
N. Parikh and S. Boyd.
Proximal algorithms.
Foundations and Trends in
Optimization, 1:123–231, 2013.
[PPY13]
Y. Park, C. E. Priebe, and A. Youssef. Anomaly detection in times series of
graphs using fusion of graph invariants. IEEE Journal of Selected Topics in
Signal Processing, 7(1):67–75, 2013.
[PT98]
A. Papakostas and I. Tollis. Algorithms for area-eﬃcient orthogonal drawings.
Computational Geometry: Theory and Applications, 9:83–110, 1998.
[RBL+07]
R. Raina, A. Battle, H. Lee, B. Packer, and A. Y. Ng. Self-taught learning:
Transfer learning from unlabeled data.
In Proc. of the 24th International
Conference on Machine learning, pages 759–766, 2007.
[RBS11]
M. G. Rodriguez, D. Balduzzi, and B. Scholkopf. Uncovering the temporal
dynamics of diﬀusion networks. In Proc. of 28th International Conference on
Machine Learning, 2011.
[RLS10]
M. G. Rodriguez, J. Leskovec, and B. Scholkopf. Structure and dynamics of
information pathways in online media.
In Proc. of 6th ACM International
Conference on Web Search and Data Mining, 2010.
[Rog95]
E. M. Rogers. Diﬀusion of Innovations. Free Press, fourth edition, 1995.
[Rou10]
M. Roughan. A case study of the accuracy of SNMP measurements. Journal
of Electrical and Computer Engineering, 2010.
[RR13]
B. Recht and C. Re. Parallel stochastic gradient algorithms for large-scale
matrix completion. Mathematical Programming Computation, 5(2):201–226,
2013.
[Sab66]
G. Sabidussi. The centrality index of a graph. Psychometrika, 31:581–683,
1966.
[SGM14]
K. Slavakis, G. B. Giannakis, and G. Mateos. Modeling and optimization for
big data analytics. IEEE Signal Processing Magazine, 31:18–31, 2014.
[SI09]
T. Sakai and A. Imiya. Fast spectral clustering with random projection and
sampling. In Proc. of the 6th International Conference on Machine Learning
and Data Mining in Pattern Recognition, pages 372–384, 2009.
[SJ09]
B. Shaw and T. Jebara. Structure preserving embedding. In Proc. of Inter-
national. Conference on Machine Learning, pages 937–944, 2009.
[SM00]
J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, 22(8):888–905, 2000.

338
Graph-Based Social Media Analysis
[Sni98]
M. Snir. MPI–the Complete Reference: The MPI Core, volume 1. MIT Press,
1998.
[SQCF05]
J. Sun, H. Qu, D. Chakrabarti, and C. Faloutsos. Neighborhood formation and
anomaly detection in bipartite graphs. In Proc. of International Conference
on Data Mining, 2005.
[SRJ04]
N. Srebro, J. Rennie, and T. S. Jaakkola. Maximum-margin matrix factor-
ization. In Proc. Advances in Neural Information Processing Systems, pages
1329–1336, 2004.
[SS05]
N. Srebro and A. Shraibman. Rank, trace-norm and max-norm. In Proc. of
Learning Theory, pages 545–560. Springer, 2005.
[SS11]
S. Shalev-Schwartz. Online learning and online convex optimization. Founda-
tions and Trends in Machine Learning, 4(2):107–194, 2011.
[SSM98]
B. Scholkopf, A. J. Smola, and K. R. Muller. Nonlinear component analysis
as a kernel eigenvalue problem. Neural Computation, 10:1299–1319, 1998.
[ST11]
O. Shamir and N. Tishby. Spectral clustering on a budget. In Intl. Conf. on
Artiﬁcial Intelligence and Statistics, pages 661–669, 2011.
[TF10]
I. Toˇsi´c and P. Frossard. Dictionary learning. IEEE Signal Processing Maga-
zine, 28:27–38, 2010.
[TL11]
H. Tong and C.-Y. Lin. Non-negative residual matrix factorization with ap-
plication to graph anomaly detection. In Proc. of SIAM Conference on Data
Mining, pages 143–153, 2011.
[TSG15]
P. Traganitis, K. Slavakis, and G. B. Giannakis. Big data spectral cluster-
ing via sketching and validation. IEEE Journal of Selected Topics in Signal
Processing, 2015.
[VR07]
F. Vega-Redondo.
Complex Social Networks.
Cambridge University Press,
2007.
[Wah90]
G. Wahba. Spline Models for Observational Data, volume 59. SIAM, 1990.
[WLRB09]
L. Wang, C. Leckie, K. Ramamohanarao, and J. Bezdek. Approximate spec-
tral clustering. In Proc. of the 13th Paciﬁc-Asia Conference on Knowledge
Discovery and Data Mining, pages 134–146, 2009.
[WS98]
D. J. Watts and S. H. Strogatz. Collective dynamics of small world networks.
Nature, 393(6684):440–442, 1998.
[WTPP14]
H. Wang, M. Tang, Y. Park, and C. E. Priebe. Locality statistics for anomaly
detection in time series of graphs. IEEE Transactions on Signal Processing,
62(3):703–717, 2014.
[XKI12]
K. S. Xu, M. Kliger, and A. O. H. III. A regularized graph layout framework
for dynamic network visualization. Data Mining and Knowledge Discovery,
27(1):84–116, 2012.
[YHJ09]
D. Yan, L. Huang, and M. I. Jordan. Fast approximate spectral clustering. In
Proc. of the 15th ACM International Conference on Knowledge Discovery and
Data Mining, pages 907–916, 2009.

Big Data Analytics for Social Networks
339
[YL06]
M. Yuan and Y. Lin. Model selection and estimation in regression with grouped
variables. Journal of the Royal Statistical Society: Series B, 68:49–67, 2006.
[YLZ+13]
J. Yang, Y. Liu, X. Zhang, X. Yuan, Y. Zhao, S. Barlowe, and S. Liu. Piwi:
Visually exploring graphs based on their community structure. IEEE Trans-
actions on Visualization and Computer Graphics, 19:1034–1047, 2013.
[YY13]
X. M. Yuan and J. Yang.
Sparse and low-rank matrix decomposition via
alternating direction method. Paciﬁc Journal of Optimization, 9(1):167–180,
2013.
[ZGGR05]
Y. Zhang, Z. Ge, A. Greenberg, and M. Roughan. Network anomography. In
Proc. of ACM Internet Measurement Conference, pages 30–30, 2005.
[ZLW+10]
Z. Zhou, X. Li, J. Wright, E. Cand`es, and Y. Ma. Stable principal component
pursuit. In Proc. of International Symposium on Information Theory, pages
1518–1522, 2010.
[ZMH09]
W. Zhao, H. Ma, and Q. He. Parallel k-means clustering based on MapReduce.
In Proc. of the 1st International Conference on Cloud Computing, pages 674–
679, 2009.
[ZRLD05]
Y. Zhang, M. Roughan, C. Lund, and D. L. Donoho. Estimating point-to-point
and point-to-multipoint traﬃc matrices: An information-theoretic approach.
IEEE/ACM Transactions on Networking, 13(5):947 – 960, Oct. 2005.
[ZRWQ09]
Y. Zhang, M. Roughan, W. Willinger, and L. Qiu.
Spatio-temporal com-
pressive sensing and Internet traﬃc matrices.
In Proc. of ACM SIGCOM
Conference on Data Commun., pages 267–278, 2009.


Chapter 11
Semantic Model Adaptation for Evolving
Big Social Data
Nikoletta Bassiou and Constantine Kotropoulos
Aristotle University of Thessaloniki, Greece
11.1
Introduction to Social Data Evolution .........................................
341
11.2
Latent Model Adaptation ......................................................
343
11.2.1
Incremental Latent Semantic Analysis ................................
343
11.2.2
Incremental Probabilistic Latent Semantic Analysis ..................
346
11.2.3
Incremental Latent Dirichlet Allocation ..............................
355
11.3
Incremental Spectral Clustering ...............................................
359
11.4
Tensor Model Adaptation ......................................................
362
11.4.1
Basic Tensor Concepts
................................................
362
11.4.2
Incremental Tensor Analysis ..........................................
363
11.5
Parallel and Distributed Approaches for Big Data Analysis ..................
368
11.5.1
Parallel Probabilistic Latent Semantic Analysis ......................
368
11.5.2
Parallel Latent Dirichlet Allocation ...................................
369
11.5.3
Parallel Spectral Clustering ...........................................
371
11.5.4
Distributed Tensor Decomposition ....................................
373
11.6
Applications to Evolving Social Data Analysis ................................
375
11.6.1
Incremental Label Propagation ........................................
375
11.6.2
Incremental Graph Clustering in Dynamic Social Networks ..........
376
11.7
Conclusions .....................................................................
379
Acknowledgment ...............................................................
381
Bibliography ....................................................................
381
11.1
Introduction to Social Data Evolution
Social data mainly comprise open source social network information from newswire and
social media. There are cases, however, where social data can stem from other sources of
network data, such as smart phones, proximity sensors, simulated data, surveys, commu-
nication networks, private company data, social science research, and databases [CDW13].
Social data exhibit three characteristics that play a key role in social network analysis. First,
social data are voluminous. Indeed, there are numerous on-line social networks that allow
their users to easily create and share content. For example, there are more than 1.39 bil-
lion monthly active Facebook users [FbS15] and around 288 million monthly active Twitter
users that generate 5800 tweets per second [Twi15], while 300 hours of video are uploaded
to YouTube every minute [You15]. Second, social data are multi-faceted. They range from
textual, image, audio, and video content to user metadata. They can be important in many
341

342
Graph-Based Social Media Analysis
ﬁelds of study besides computer science, such as sociology, politics, or marketing for exam-
ple. Third, the data are dynamic. Structural changes can occur at multiple time scales or
can be localized to a subset of users [LCSX11]. Consequently, social media data analysis
needs to handle the data volume as well as the number and the diversity of the data facets
in a dynamically evolving framework.
Graphs provide the most obvious and straightforward representation of a social network
with the set of nodes/vertices corresponding to social entities (e.g., users) and the set of
edges representing the associations among entities. The associations can be either explicit
(e.g., a friendship or relationship between users) or implicit (e.g., a tag inserted to a news
story). The connectivity between graph elements is usually encoded by an adjacency or
similarity matrix. Thus, social network analysis exploits matrix algebra operations, such
as Singular Value Decomposition (SVD) used in spectral clustering. Moreover, the Latent
Semantic Analysis (LSA) that resorts to the truncated SVD and its probabilistic counter-
part the Probabilistic Latent Semantic Analysis (PLSA), are employed to extract the most
important data aspects or latent data relationships. Additionally, diﬀerent types of edges
or entities in a social network are often represented by tensors (strictly speaking, the hy-
permatrices of Chapter 8). Tensors consist of the multi-dimensional extension of matrices,
and have arisen as the most natural way to encode n-way relationships between entities
simultaneously, including the temporal network dynamics.
A fundamental tool in the analysis of large complex networks is community detection.
In the graph representation setting, communities are groups of nodes that exhibit high con-
nectivity within a group and low connectivity across the groups [PKVS12]. In this sense,
community detection is usually treated as a graph clustering problem. Depending on the
deﬁnition of community and the methodological principle used, community detection meth-
ods are further classiﬁed into vertex/spectral clustering methods [PL05, Lux07], cohesive
subgraph discovery methods [PDFV05, XYFS07], community quality optimization methods
[SM00, New07, CZG09, KPSC10], divisive methods [FLG00, GN02, For10], and model-based
methods [RAK07, LHLC09]. In more detail, model-based methods consider either an under-
lying statistical model to divide the network into communities, or dynamic processes. Label
propagation is such a dynamic process that forms consensus on labels from the densely con-
nected groups of nodes and reveals communities built from nodes sharing the same labels
[PCW09].
Besides revealing the underlying structure in social networks, community detection
is largely exploited for higher-level inference and knowledge-discovery tasks, such as to
build recommender systems. Traditional recommender systems are mainly based on col-
laborative ﬁltering methods that can be either memory-based or model-based [DGR07].
Memory-based methods exploit the underlying similarity between users [JZM04] or items
[SKKR01, LSY03, DK04] for recommendations. That is, in user-based approaches, the rat-
ings in groups of similar users help to predict the ratings of active users, while in item-based
approaches, predictions are based on the information entailed in items similar to those cho-
sen by the user [MZL+11]. Thus, memory-based methods largely employ clustering in order
to build the groups of similar users and items, treating the recommendation problem as a
classiﬁcation problem. That is, an active user is classiﬁed in the group with the most similar
users to exploit the group users’ ratings in order to generate recommendations for him/her.
Common similarity measures include the Pearson correlation coeﬃcient [RIS+94] and the
norms between the rating vectors in a proper vector space [BHK98].
In contrast to memory-based methods, the model-based approaches build a compact
model from the observed user-item data in order to infer recommendations. The most rep-
resentative algorithms used for this goal include the LSA [DDF+90, SKKR00], the PLSA
[Hof04], the Latent Dirichlet Allocation (LDA) [BNJ03], the Bayesian clustering [BHK98],
the multiple multiplicative factor model [MZ04], the Markov Decision process [SBH02],

Semantic Model Adaptation for Evolving Big Social Data
343
the Bayesian hierarchical model [ZK07], and the ranking model [LY08]. To enhance their
performance, recommender systems also use tags, which constitute an additional source
of information. These systems are usually referred to as tag-based recommender systems
[IN14] and are largely built on tensor models for generating tags and item recommendations
[RST10, SNM10, LDv12, RD13].
Considering the dynamic nature of social networks and the large amount of information
they entail, all the aforementioned approaches need to be scalable and adaptive to the
evolutionary nature of on-line social networks. Thus, recent research eﬀorts mainly focus on
the ability of the social network analysis tools to eﬃciently capture the network dynamics,
while eﬀorts more targeted to the eﬃcient handling of large volumes of data (frequently met
under the term “big data”) also arise. This chapter summarizes the dynamic alternatives
of the corresponding state-of-the-art approaches. In Section 11.2, latent semantic model-
based methods, such as LSA, PLSA, and LDA, are discussed. In Section 11.3, incremental
spectral clustering is analyzed, while in Section 11.4 tensor model adaptation methods are
presented. In Section 11.5, methods that further optimize and adapt these state-of-the-art
approaches for handling big data are reviewed and parallel distributed implementations are
summarized. Representative applications, namely the incremental label propagation and
the incremental graph clustering, to evolving social data analysis are described in Section
11.6. Conclusions are drawn in Section 11.7.
11.2
Latent Model Adaptation
Latent variable models (LSA, PLSA and LDA) are widely used in semantic analysis
applications, since they successfully capture the underlying semantic relationships among
data objects. In social networks, latent variable models are widely used for revealing the
semantic relations between the social media textual data (i.e., word/document clustering)
and for modeling the underlying latent topics [WAB12]. They are also used in applications,
such as news recommendations and collaborative ﬁltering [DGR07, WYL+09], web-usage
mining [JZM04, XZMZ05, XYW+09] or co-citation and trends analysis [CH00, WYL+09].
They are useful for data ﬁltering by means of topic modeling [GLMY11] as well. In Sections
11.2.1-11.2.3, LSA, PLSA and LDA and their incremental counterparts are described within
a text document framework. However, they can also be used to capture relations between
other types of data, such as users, topics, tags, and ratings.
11.2.1
Incremental Latent Semantic Analysis
Latent Semantic Analysis (LSA) extends the vector space model where the dataset is
represented as a term-document matrix, by employing a reduced dimension representation
of the term-document relationship. This representation is built by means of the Partial
Singular Value Decomposition (PSVD) of the term-document matrix [ZS99]. Let A ∈RM×N
be the term-document matrix holding the co-occurrence of M terms in N documents. The
PSVD of A is given by the best rank-k approximation Ak = UkΣkVT
k , where Uk and Vk
are built by the ﬁrst k columns of U ∈RM×M and V ∈RN×N, holding the left and right
singular vectors of A, respectively. Σk is the k-th leading diagonal submatrix of Σ ∈RM×N,
having in its main diagonal the singular values of A.
In the case of a frequently updated term-document collection with the addition of new
documents and terms, LSA has to be also updated, reﬂecting these changes in the PSVD

344
Graph-Based Social Media Analysis
of the term-document matrix. The straightforward method of recomputing the SVD that
performs the PSVD of the augmented term-document matrix ˜A ∈R(M+M ′)×(N+N′) suﬀers
from time and memory constraints, especially for large data [BDO94, O’B94]. Instead of
recomputing the PSVD of the augmented term-document matrix from scratch, the LSA
updating methods resort to the existing PSVD in order to update the existing LSA model.
In more detail, folding-in, which is the simpler LSA updating method, handles new
documents and terms by projecting the corresponding new document vectors forming D ∈
RM×N′ and the new term vectors forming T ∈RM ′×N onto the existing k-dimensional
subspace. That is, folding-in new documents and terms is given by Dk = DT UkΣ−1 and
Tk = TVkΣ−1, respectively. The projection Dk ∈RN ′×k is folded-in to the existing PSVD
by appending it to the bottom of Vk, yielding the updated matrix ˜Vk ∈R(N+N′)×k.
Similarly, the projection Tk ∈RM ′×k is folded-in to the existing PSVD by appending
it to the bottom of Uk, yielding the updated matrix ˜Uk ∈R(M+M ′)×k. Folding-in new
documents has a computational complexity of O(2kMN ′), while folding-in new terms has
O(2kM ′N) complexity [BDO94].
SVD updating is a more complicated, but more robust, updating method than folding-in.
It estimates the PSVD of the augmented term-document matrix ˜A by updating the existing
PSVD of the original term-document matrix A [BDO94, O’B94]. The method exploits
the QR decomposition in the following three-step procedure. In all steps, ˜A denotes the
augmented term document matrix and ˆA is an intermediate matrix whose PSVD of order
k is denoted by ˆAk.
• Updating documents. Let us examine what happens when ˜A is the fat matrix obtained
by appending D ∈RM×N′ after A ∈RM×N, i.e. ˜A = [A|D]. Then:
˜A = [A|D] ≃[Ak|D] = [Uk|QD]
"
Σk
UT
k D
0
RD
#
|
{z
}
ˆ
A
"
VT
k
0
0
IN′
#
.
(11.2.1)
One can easily see that:
[Uk|QD] ˆA
"
VT
k
0
0
IN ′
#
= [UkΣkVT
k | UkUT
k D
|
{z
}
D−ˆ
D
+ QDRD
| {z }
ˆ
D
],
(11.2.2)
where ˆD = QD RD = (IM −Uk UT
k ) D ∈RM×N ′ is the projection of the columns of
D onto the left nullspace of A (i.e., the orthogonal complement of the column space
of A).
Let also the SVD of ˆA be:
ˆA = [ ˆUk| ˆUN ′]
"ˆΣk
0
0
ˆΣN ′
#
[ ˆVk| ˆVN′]T ,
(11.2.3)
where ˆUk, ˆVk ∈R(k+N′)×k and ˆΣk ∈Rk×k. The PSVD of order k of ˜A is ˆAk =
ˆUkΣk ˆVT
k . Then the PSVD of ˜A in k dimensions is given by [TS07, ZS99]:
˜Ak =

[Uk|QD] ˆUk

ˆΣk
 "
Vk
0
0
IN′
#
ˆVk
!T
.
(11.2.4)
This updating procedure has a complexity of O(k3 +(N +M)k2 +(N +M)kN ′ +N ′3)
[TS07].

Semantic Model Adaptation for Evolving Big Social Data
345
• Updating terms. When ˜A is the tall matrix obtained by appending T ∈RM ′×N below
A ∈RM×N, i.e.:
˜A =
"
A
T
#
≃
"
Ak
T
#
=
"
Uk
0
0
IM ′
# "
Σk
0
TVk
RT
T
#
|
{z
}
ˆ
A
[Vk|QT ]T .
(11.2.5)
One can easily identify that:
"
Uk
0
0
IM ′
#
ˆA [Vk|QT ]T =


UkΣkVT
k
IM ′TVkVT
k
|
{z
}
T−ˆTT
+ IM ′RT
T QT
T
|
{z
}
ˆTT

,
(11.2.6)
where ˆT = (IN −Vk VT
k ) TT = QT RT ∈RM ′×N is the projection of the row vectors
T onto the orthogonal complement of the row space of A (i.e., the nullspace of A).
Let also the SVD of ˆA be:
ˆA = [ ˆUk| ˆUM ′]
"ˆΣk
0
0
ˆΣM ′
#
[ ˆVk| ˆVM ′]T ,
(11.2.7)
where ˆUk, ˆVk ∈R(k+M ′)×k and ˆΣk ∈Rk×k. Then, the PSVD of ˜A in k dimensions
is given by [TS07, ZS99]:
˜Ak =
 "
Uk
0
0
IM ′
#
ˆUk
!
ˆΣk

[Vk|QT ] ˆVk
T
.
(11.2.8)
The computational complexity of this procedure is O(k3+(N +M)k2+(N +M)kM ′+
M ′3) [TS07].
• Updating weights. Let also ˜A = A + S GT where A ∈RM×N, S ∈RM×p and
G ∈RN×p. S can be treated as a selection matrix holding the p weights to be adjusted.
G is a matrix having as columns the diﬀerence between the old term weights and the
new term weights. If ˜S = QS RS = (IM −Uk UT
k ) S ∈RM×p and ˆG = QG RG =
(IN −Vk VT
k ) G ∈RN×p we obtain:
˜A ≃Ak + SGT = [Uk|QS]


"
Σk
0
0
0
#
+
"
UT
k S
RS
# "
VT
k G
RG
#T 

|
{z
}
ˆ
A
[Vk|QG]T .
(11.2.9)
Let also the SVD of ˆA be:
ˆA = [ ˆUk| ˆUp]
 "ˆΣk
0
0
ˆΣp
#!
[ ˆVk| ˆVp]T ,
(11.2.10)
where ˆUk, ˆVk ∈R(k+p)×k and ˆΣk ∈Rk×k. Then, the PSVD of ˜A in k dimensions is
given by [TS07, ZS99]:
˜Ak =

[Uk|QS] ˆUk

ˆΣk

[Vk|QG] ˆVk
T
.
(11.2.11)

346
Graph-Based Social Media Analysis
This updating procedure has a complexity of O
 k3 + (N + M)k2+
(N + M)kp + p3
[TS07].
The folding-in method and the SVD updating method are combined in an alternate hy-
brid LSA updating method, called folding-up. The method alternates repeatedly between
the folding-in and the SVD Updating in order update the existing LSA model. The com-
plexity of the process is of the same order as SVD Updating, but reduced by a factor that
is dependent on the number of iterations in which SVD updating is replaced by folding-in
[TS07].
A more recent approach, called Incremental LSI (ILSI), updates the existing LSA model
(Ak = UkΣkVT
k ) for a new term-document matrix A′ as follows [JNCJ08]:
1. Zero rows are appended to matrices Uk and Vk, resulting in U′ =
"
Uk
0M ′×k
#
and
V′ =
"
Vk
0N′×k
#
matrices, respectively.
2. The new central matrix Σ′ = U′T A′V′ is computed.
3. The SVD decomposition of Σ′ = ˆUk ˆΣk ˆVT
k is estimated.
4. The updated SVD matrices U′k = U′ ˆUk , V′k = V′ ˆVk and Σ′k = ˆΣk are estimated.
The total overhead complexity of incremental LSI over the traditional LSI lies in the com-
plexity of Step 2 and Step 4 of the algorithm, which is O(k(M + M ′)(N + N ′ + k) and
O(k2(M + M ′ + N + N ′) respectively, while Step 3 costs O(k3). [JNCJ08].
11.2.2
Incremental Probabilistic Latent Semantic Analysis
Probabilistic Latent Semantic Analysis (PLSA) is a statistical latent variable model
(or aspect model) [HP98], which associates an unobserved class variable to co-occurrence
data. For example, in a text processing application, the co-occurrence data may refer to a
training corpus X consisting of document-word pairs (d, w) collected from N documents
d ∈D = {d1, d2, . . . , dN} with a vocabulary of M words/terms w ∈W = {w1, w2, . . . , wM}
that were generated by topics z ∈Z = {z1, z2, . . . , zK}. Following the aspect model in-
dependence assumptions [MB88], all the pairs (d, w) are assumed to be independent and
identically distributed (the “bag of words” approach), and conditionally independent given
the respective latent class z [Hof01]. It also holds that |Z| ≪min (|D|, |W|), where | · |
stands for the cardinality of the corresponding set.
The data generation process can be better described by the following scheme [Hof01]:
1) select a document d with probability P(d); 2) pick a latent topic z for the docu-
ment with probability P(z|d); and 3) generate a term w with probability P(w|z). Ac-
cordingly, the joint distribution of a word w in a document d generated by a latent
topic z is given by P(d, w, z) = P(d)P(z|d)P(w|z) or after applying the Bayes rule by
P(d, w, z) = P(z)P(d|z)P(w|z) that resorts to an equivalent model which is perfectly sym-
metric in both entities documents and terms.
Asymmetric Formulation. The joint distribution of d and w is obtained by summing
over all possible realizations of z:
P(d, w) =
X
z∈Z
P(d, w, z) = P(d)
X
z∈Z
P(z|d)P(w|z)
|
{z
}
P (w|d)
.
(11.2.12)

Semantic Model Adaptation for Evolving Big Social Data
347
D
Z
W
P(di)
P(z|d)
P(w|z)
(a)
D
Z
W
P(z)
P(d|z)
P(w|z)
(b)
FIGURE 11.2.1: Graphical model representation of the aspect model in the (a) asymmetric
and (b) symmetric formulation.
As can be seen from (11.2.12), the document-speciﬁc term distribution P(w|d) is ob-
tained by a convex combination of the |Z| aspects/factors P(w|z). In order to determine
P(z|d) and P(w|z), the log-likelihood function of the training corpus X = {d, w} using
θ = {P(w|z), P(z|d)}:
L = log P(X|θ) =
X
d∈D
X
w∈W
n(d, w) log P(d, w)
(11.2.13)
has to be maximized with respect to all the aforementioned probabilities. That is, θML =
arg maxθ log P(X|θ). In (11.2.13), n(d, w) denotes the term-document frequency. The es-
timation of P(d) can be carried out independently resulting in P(d) =
n(d)
P
d′∈D n(d′). The
conditional probabilities P(z|d) and P(w|z) are estimated by means of the EM algorithm
[DLR77, Hof01], which alternates between the Expectation (E)-step:
ˆPML(z|d, w) =
P(w|z)P(z|d)
P
z′∈Z P(w|z′)P(z′|d)
(11.2.14)
and the Maximization (M)-step:
P(w|z)
=
P
d∈D n(d, w) ˆPML(z|d, w)
P
d∈D
P
w′∈W n(d, w′) ˆPML(z|d, w′)
(11.2.15)
P(z|d)
=
P
w∈W n(d, w) ˆPML(z|d, w)
n(d)
.
(11.2.16)
By alternating (11.2.14) with (11.2.15)-(11.2.16), a convergent procedure is obtained to a
local maximum of the log-likelihood.
Symmetric Formulation. Following similar lines to the asymmetric model, the joint
distribution of d and w is given by:
P(d, w) =
X
z∈Z
P(d, w, z) =
X
z∈Z
P(d|z)P(w|z)P(z).
(11.2.17)
and the corresponding log-likelihood function L of the training corpus X = {d, w} us-
ing θ = {P(w|z), P(d|z), P(z)} has to be maximized. Let R = P
d∈D
P
w∈W n(d, w). The
corresponding E-step and M-step are formulated as follows:
E-step:
ˆPML(z|d, w) =
P(z)P(d|z)P(w|z)
P
z′∈Z P(z′)P(d|z′)P(w|z′)
(11.2.18)

348
Graph-Based Social Media Analysis
M-step:
P(w|z)
=
P
d∈D n(d, w) ˆPML(z|d, w)
P
d∈D
P
w′∈W n(d, w′) ˆPML(z|d, w′)
(11.2.19)
P(d|z)
=
P
w∈W n(d, w) ˆPML(z|d, w)
P
d′∈D
P
w∈W n(d′, w) ˆPML(z|d′, w)
(11.2.20)
P(z)
=
P
d∈D
P
w∈W n(d, w) ˆPML(z|d, w)
R
.
(11.2.21)
The symmetric PLSA formulation can be rewritten in matrix notation as P =
UKSKVT
K, where UK is the |W| × |Z| matrix with jk element P(w|z), VK is the |D| × |Z|
matrix with ik element P(d|z), SK is the |Z| × |Z| diagonal matrix having as elements on
its main diagonal P(z), z ∈Z, and P is the |W| × |D| matrix with elements the proba-
bilities P(w, d). Such a decomposition looks like the partial singular value decomposition
employed within the LSA as described in Section 11.2.1. Despite the resemblance, it should
be stressed that the LSA and the PLSA solve diﬀerent optimization problems. Indeed, the
LSA minimizes the Frobenius norm between the original-term document matrix and its best
K-rank approximation, while the PLSA maximizes the likelihood function of multinomial
sampling. In other words, the PLSA minimizes the cross entropy (or Kullback-Leibler di-
vergence) between the model and the empirical distribution. The graphical representation
of the asymmetric and the symmetric formulations of the PLSA model is depicted in Figure
11.2.1. The time complexity of the standard PLSA is O(|Z|CW,D), where |Z| is the number
of latent topics and C is the number of total word-document co-occurrence, which is usually
less than |W||D|.
When new data are added to the initial data collection, the aforementioned PLSA model
has to be updated in order to reﬂect/assimilate these changes. Such a need emerges when
documents and/or terms are added or deleted in document clustering or topic-detection.
Various methods for updating the PLSA model exist that are frequently found in the liter-
ature with terms such as on-line, incremental, or folding-in.
The PLSA folding-in is the ﬁrst and simplest method for updating the PLSA model
when new documents (dnew ∈Dnew) are added in the initial document collection [Bra05,
BTHC06, GH99]. It is based on an incremental variant of the EM algorithm [NH98] that
recalculates only the probabilities of the topics given the new documents P(z|dnew) in the
M-step leaving the probabilities of the words given the topics P(w|z) unchanged. That is:
(E)-step:
ˆPML(z|dnew, w) =
P(w|z)P(z|dnew)
P
z′∈Z P(w|z′)P(z′|dnew).
(11.2.22)
(M)-step:
P(z|dnew) =
P
w′∈dnew n(dnew, w′) ˆPML(z|dnew, w′)
P
z′∈Z
P
w∈dnew n(dnew, w′) ˆPML(z′|dnew, w′)
.
(11.2.23)
Usually, a very small number of iterations is needed for the EM to converge. The time
complexity of the PLSA folding-in per EM iteration is O (|Z|CW,Dnew), where |Z| is the
number of latent topics and C is the number of total word-document co-occurrence, which
is usually less than |W||Dnew|.
In another incremental approach [WZWC08], the PLSA model is updated by means of
a modiﬁed EM scheme that is based on the Generalized Expectation Maximization [NH98]:

Semantic Model Adaptation for Evolving Big Social Data
349
(E)-step:
ˆPML(z|d, w)l =
P(w|z)lP(z|d)l
P
z′∈Z P(w|z′)lP(z′|d)l
.
(11.2.24)
(M)-step:
P(z|d)l+1 =
P
w∈W n(d,w) ˆ
PML(z|d,w)l+1
P
z′∈Z
P
w′∈dnew n(d,w′) ˆ
PML(z′|d,w′)l+1
(11.2.25)
P(w|z)l+1 =
P
d∈D n(d,w) ˆ
PML(z|d,w)l+1+α P (w|z)l
P
d∈D
P
w′∈dnew n(d,w′) ˆ
PML(z|d,w′)l+1+α P
w′∈W P (w′|z)l .
(11.2.26)
The subscripts l+1 and l denote new and old model parameters, respectively. The algorithm
exhibits a complexity of O(|Z|CW,Dnew) per EM iteration.
In another approach, an incremental version of EM updates the PLSA model param-
eters using subsets of training data that are selected sequentially and cyclically at each
iteration [XYW+09], while an on-line EM algorithm works on the weighted mean values of
the conditional probability P(z|dnew) = P (z)P (dnew|z)
P (dnew)
in order to update the PLSA model
parameters [XYW+11] .
In a more sophisticated PLSA updating method, called incremental PLSA, a batch of
new incoming documents is added and a batch of old documents is discarded in a document
scope moving window framework [CC08]. The PLSA folding-in is used to fold in new terms
and documents in four steps:
1. Discard old documents and terms. At each advance of the window, out-of-date
documents dout and the corresponding terms wout are discarded. The corresponding
PLSA parameters P(wout|z), P(dout|z), P(z|wout) and P(z|dout) are also removed and
the conditional probabilities for the remaining documents and terms are estimated by
renormalization.
2. Fold-in new documents. The new documents dnew are folded-in by means of the
PLSA folding-in for the asymmetric formulation using (11.2.22) and (11.2.23)).
3. Fold-in new terms. The new terms wnew present in the new documents dnew are
folded-in by means of the PLSA folding-in exploiting the symmetric formulation. That
is, P(dnew|z) is estimated as follows:
P(z|dnew, w) =
P(w|z) P(z|dnew)
P
z′∈Z P(w|z′) P(z′|dnew)
(11.2.27)
P(dnew|z) =
P
w∈dnew n(dnew, w) P(z|dnew, w)
P
d∈Dnew
P
w∈d n(d, w′) P(z′|d, w′).
(11.2.28)
The probability P(z|wnew) for new terms is estimated by the EM folding-in process:
(E)-step:
P(z|dnew, wnew) =
P(dnew|z) P(z|wnew)
P
z′∈Z P(dnew|z′) P(z′|wnew).
(11.2.29)
(M)-step:
P(z|wnew) =
P
d∈Dnew n(d, wnew) P(z|dnew, wnew)
P
d′∈Dnew n(d′, wnew)
.
(11.2.30)
The initial values of P(z|wnew) are set randomly and normalized.

350
Graph-Based Social Media Analysis
4. Update the PLSA parameters. The values P(wnew|z) that did not exist in the
previous window step are calculated by using (11.2.29) to estimate P(z|d, w). In a sim-
ilar manner, the values P(wold|z) are adjusted using (11.2.14) to estimate P(z|d, w).
The probability values are then normalized so that their sum is equal to 1:
P(w|z) =
P
d∈D∪Dnew n(d, w) P(z|d, w)
P
d′∈D∪Dnew
P
w′∈d′ n(d′, w′) P(z|d′, w′).
(11.2.31)
Finally, all the PLSA parameters are revised by applying the original PLSA algorithm
(11.2.14)-(11.2.16).
The
time
complexity
of
the
incremental
PLSA
algorithm
per
EM
iteration
is
O(|Z|C(|Dnew|+|D|),(|Wnew|+|W |)), where C(Dnew+D),(Wnew+W ) is the number of total word-
document co-occurrence, which is usually less than (|Wnew| + |W|)(|Dnew| + |D|), and |Z|
is the number of latent topics. The computational complexity is equal to the computational
complexity of the original PLSA algorithm executed in the augmented dataset. However,
the incremental PLSA needs less EM iterations to converge than the original PLSA.
Two adaptation paradigms for PLSA, namely the Quasi-Bayes (QB) PLSA for incre-
mental learning and the MAP PLSA for corrective training, are based on a Bayesian frame-
work [CW08]. The aforementioned framework uses a Dirichlet density kernel as a prior. The
MAP PLSA maximizes the posterior probability:
θMAP = arg max
θ
P(θ|X) = arg max
θ
log P(X|θ) + log g(θ),
(11.2.32)
where X are the adaptation data (new data) and g(θ) is the prior density of the model
parameters. The prior density of the model parameters, under the assumption that P(w|z)
and P(z|d) are independent, is expressed by:
g(θ) ∝
Y
z∈Z
" Y
w∈W
P(w|z)αwz−1 Y
d∈D
P(z|d)βzd−1
#
,
(11.2.33)
where φ = {αwz, βzd} are the hyperparameters of Dirichlet densities. Thus, the MAP-PLSA
model parameters are estimated by [CW08]:
PMAP (w|z) =
P
d∈D n(d,w) P (z|d,w)+(αwz−1)
P
w′∈W [P
d∈D n(d,w′) P (z|d,w′)+(αw′z−1)]
(11.2.34)
PMAP (z|d) =
P
w∈W n(d,w) P (z|d,w)+(βzd−1)
n(d)+P
z′∈Z(βz′d−1)
.
(11.2.35)
In MAP PLSA, no incremental learning mechanism is designed for continuously updat-
ing the model parameters with new words and topics, while fading away out-of-date words or
documents. Incremental learning is achieved by QB PLSA that estimates the model param-
eters by maximizing the posterior probability using the sequence of adaptation documents
for each epoch. Let χn = {X1, . . . , Xn}={(d(1), w(1)), . . . , (d(n), w(n))} be the adaptation
data for the n-th epoch. The QB PLSA estimate is determined by maximizing the posterior
probability:
θ(n)
QB
=
arg max
θ
P(θ|χn) = arg max
θ
log

P(Xn|θ) P(θ|χn−1)

∼=
arg max
θ
log
h
P(Xn|θ) g(θ|φ(n−1))
i
,
(11.2.36)
where φ(n−1) are the hyperparameters estimated on previously seen documents. At each

Semantic Model Adaptation for Evolving Big Social Data
351
epoch n, the previous block of documents Xn−1 is released and only the current block of
documents Xn and the accumulated statistics φ(n−1) are used for PLSA updating. Thus,
the QB PLSA parameters at the n-th epoch are estimated by:
P (n)
QB(w(n)|z) =
P
d∈D n(d(n),w(n)) P (n)(z|d(n),w(n))+(α(n−1)
wz
−1)
P
w′∈W [P
d∈D n(d,w′(n)) P (n)(z|d(n),w′(n))+(α(n−1)
w′z
−1)]
(11.2.37)
P (n)
QB(z|d(n)) =
P
w∈W n(d(n),w(n)) P (n)(z|d(n),w(n))+(β(n−1)
zd
−1)
n(d(n))+P
z′∈Z(β(n−1)
z′d
−1)
,
(11.2.38)
where the hyperparameters’ values at the n-th learning epoch are given by:
α(n)
wz
=
X
d∈Xn
n(d(n), w(n)) P (n)(z|d(n), w(n)) + α(n−1)
wz
(11.2.39)
β(n)
zd
=
X
w∈Xn
n(d(n), w(n)) P (n)(z|d(n), w(n)) + β(n−1)
zd
(11.2.40)
and the initial values are estimated on the initial data collection as follows:
α(0)
wz
=
1 +
X
d∈D
n(d, w) P(z|d, w)
(11.2.41)
β(0)
zd
=
1 +
X
w∈W
n(d, w) P (n)(z|d, w).
(11.2.42)
Given the |Z| latent variables, the number Nn of new documents at each learning epoch
n, the total number of all adaptation documents N ′ = P
n Nn, and CW,N ′ and CW,Nn the
numbers of non-zero entries in the corresponding word-by-document probability matrices,
which are far fewer than |W|N and |W|Nn, respectively, the computational complexity of
MAP PLSA and QB PLSA per EM iteration is respectively O(|Z|CW,N ′) and O(|Z|CW,Nn).
That is, the time complexity of MAP PLSA is proportionally increased by the number of
the observed events in the batch collection, while that of QB PLSA is only aﬀected by the
number of events added at the current epoch [CW08].
In the on-line Probabilistic Latent Semantic Analysis (oPLSA), the PLSA parameters
for both the asymmetric and the symmetric formulations are updated within the context of
a varying document stream [BK14]. In particular, a ﬁxed-size moving window is employed
over a document stream in order to incorporate new documents and at the same time to
discard old ones (i.e., documents that fall outside the scope of the window). In addition, the
oPLSA assimilates new words that had not been previously seen (out-of-vocabulary words)
and discards the words that exclusively appear in the documents to be thrown away. A
schematic representation of the moving window over the word-document matrix is depicted
in Figure 11.2.2.
The oPLSA method is formulated by taking into consideration the basic operations
between two successive EM iterations. That is, for w ∈W, d ∈D and z ∈Z, the E-step
at iteration l + 1 proceeding iteration l, when documents or words are neither removed nor
added, is given by:
ˆP(z|d, w)l+1 =
P(w|z)lP(z|d)l
P
z′∈Z P(w|z′)lP(z′|d)l
.
(11.2.43)
Let
P1(w|z)l+1 = P(w|z)l
X
d∈D
n(d, w)P(z|d)l
P
z′∈Z P(w|z′)lP(z′|d)l
.
(11.2.44)

352
Graph-Based Social Media Analysis
f
f
+
+
-
-
FIGURE 11.2.2: Schematic representation of (a) the initial term-document matrix, (b) the
deletion of old documents and the words that appear exclusively in them, and (c) the
insertion of new documents and their associated words in the word-document matrix as the
window advances.
After the substitution of equation (11.2.43) into equations (11.2.15) and (11.2.16), the M-
step equations are rewritten as:
P(w|z)l+1 =
P1(w|z)l+1
P
w′∈W P1(w′|z)l+1
,
(11.2.45)
P(z|d)l+1 = P2(z|d)l+1
n(d)
,
(11.2.46)
where
P2(z|d)l+1 = P(z|d)l
X
w∈W
n(d, w)P(w|z)l
P
z′∈Z P(w|z′)lP(z′|d)l
.
(11.2.47)
The oPLSA method is formulated by means of a fundamental operation, namely the
addition/deletion of a pivotal document to the document collection. A pivotal document
contains a single word appearing a number of times. Any document can be decomposed as a
union of pivotal documents. The oPLSA method updates the existing PLSA model param-
eters, which are produced from the original PLSA algorithm executed on the initial word
and document collections, for every advance of a window. That is, the PLSA model param-
eters at the (l + 1)-th position of the window are derived by the PLSA model parameters
at the l-th position of the window, obtained after EM convergence. The algorithm performs
three steps to update the existing PLSA model parameters between the l-th and (l + 1)-th
position of the window. These steps are analyzed next for the asymmetric formulation of
the PLSA model, while the corresponding steps for the symmetric formulation are derived
similarly [BK14].

Semantic Model Adaptation for Evolving Big Social Data
353
1. Discard old documents and their exclusive words.
A single document dout is discarded from the existing document collection Dl resulting
in D−
l
= Dl −{dout}. If the discarded document dout contains only a single exclusive
word wout, (i.e., a word that does not appear in any document in D−
l ), this word is also
discarded from Wl yielding the vocabulary of already seen words W −
l
= Wl −{wout}.
The corresponding PLSA model probabilities for dout, Pl(z|dout), and wout, Pl(wout|z),
are eliminated and the PLSA model parameters for the remaining documents d ∈D−
l
and words w ∈W −
l
are renormalized as follows:
P −(z|d)l =
P(z|d)l
P
z′∈Z P(z′|d)l
, z ∈Z, d ∈D−
l
(11.2.48)
P −(w|z)l =
P(w|z)l
P
w′∈W −
l P(w′|z)l
, z ∈Z, w ∈W −
l .
(11.2.49)
Clearly, (11.2.48) and (11.2.49) are still valid when more than one documents and words
are to be discarded.
2. Add a new word and document.
a. Let a new pivotal document din, which contains only a single word win appearing α
times, be inserted in the already seen document collection D−
l , yielding D+
l = D−
l +{din}.
The word, win, can be either a word from the vocabulary of already seen words W −
l
or
a new word (OOV word at this point) denoted as wOOV . In the latter case, the new
word is inserted in W −
l , expanding it into W +
l
= W −
l + {wOOV }, with the entries of the
augmented word-document matrix satisfying:
n(d, w)l+1 =





α ,
if w = win and d = din
n(d, w)l ,
if w ∈W −
l
and d ∈D−
l
0 ,
otherwise,
(11.2.50)
where n(d, w)l is the document-word matrix at the l-th position of the window.
b. For already seen documents d ∈D−
l , the latent-variable probability is initialized by
P +(z|d)l = P −(z|d)l.The corresponding probability P +(z|din)l for the new document
din is initialized by P +(z|din)l = P +(win|z)l, ∀z ∈Z, based on the assumption that the
assignment of the new document to the latent topics is driven by the single word win it
contains.
c. For already seen words w ∈W −
l , P +(w|z)l = P −(w|z)l. For an OOV word wOOV ,
the corresponding conditional probability P +(wOOV |z)l is initialized by the Good-Turing
estimate of the probability of unseen words [NH98]:
pGTl(wOOV ) =
n1
Rl+1
=
P
w∈W −
l :n(w)=1 1
P
d∈D+
l
P
w∈W +
l n(d, w)l+1
=
P
w∈W −
l :n(w)=1 1
Rl + a
,
(11.2.51)
where n(w) = P
d∈D−
l n(d, w)l. The probability given by (11.2.51) is uniformly dis-
tributed among the topics:
P +(wOOV |z)l =
1
|Z|pGTl(wOOV )
(11.2.52)
and the conditional probabilities of already seen words given the topics are renormalized
using:
P +(w|z)l =
 1 −P +(wOOV |z)l

P −(w|z)l
P
w∈W −
l P −(w|z)l
,
(11.2.53)
where w ∈W −
l , so that P
w∈W +
l P +(w|z)l = 1.

354
Graph-Based Social Media Analysis
3. Fold in the new word for pivotal documents. The PLSA model probabilities at
the window position l + 1 are estimated by updating the PLSA model probabilities at
the window position l. To achieve this, the computations between two successive EM
iterations, as described in (11.2.43)-(11.2.47), are taken into consideration [BK14]. Thus,
the conditional probability P +
1 (w|z)l+1 of the word w ∈W +
l
given the latent topic z ∈Z
is given by:
P +
1 (w|z)l+1 = P1(w|z)l+1 + n(din, w)l+1P +(w|z)lP +(z|din)l
P
z′∈Z P +(w|z′)lP +(z′|din)l
(11.2.54)
=























P1(w|z)l+1, if w ̸= win
P1(w|z)l+1 +
αP +(w|z)lP +(z|din)l
P
z′∈Z P +(w|z′)lP +(z′|din)l ,
if w = win and win ∈W −
l
αP +(w|zk)lP +(z|din)l
P
z′∈Z P +(w|z′)lP +(z′|din)l , if w = win and win = wOOV ,
where P1(w|z)l+1 is deﬁned in (11.2.44).
By normalizing P +
1 (w|z)l+1 as in (11.2.45), P +(w|z)l+1 is obtained:
P +(w|z)l+1 =
P +
1 (w|z)l+1
P
w′∈W +
l P +
1 (w′|z)l+1
=



P1(w|z)l+1
Al+1(win) ,
if w ̸= win
P +
1 (w|z)l+1
Al+1(win) ,
if w = win.
(11.2.55)
The denominator in eq.(11.2.55) is given by:
Al+1(win) =
(11.2.56)





P
w∈W −
l P1(w|z)l+1 + P +
1 (win|z)l+1 −P1(win|z)l+1, if win ∈W −
l
P
w∈W −
l P1(w|z)l+1 + P +
1 (win|z)l+1, if win = wOOV .
Similarly, the conditional probability P2(z|d)l+1 of the latent topic z ∈Z given the
document d ∈D+
l is given by:
P +
2 (z|d)l+1
=
P +(z|d)l
X
w∈W +
l
n(d, w)l+1P +(w|z)l
P
z′∈Z P +(w|z′)lP +(z′|d)l
=
=
( P2(z|d)l+1,
if d ∈D−
l
αP +(win|z)lP +(z|d)l
P
z′∈Z P +(win|z′)lP +(z′|d)l ,
if d = din,
(11.2.57)
where P2(z|d)l+1 is deﬁned in (11.2.47). When d = din it holds that:
P +
2 (z|din)l+1 =
(
P +
1 (win|z)l+1 −P1(win|z)l+1, if win ∈W −
l
P +
1 (win|z)l+1, if win = wOOV .
(11.2.58)
Finally, P +(z|d)l+1 is updated as follows:
P +(z|d)l+1 = P +
2 (z|d)l+1
n(d)
=
(
P2(z|d)l+1
n(d)
= P(z|d)l+1,
if d ∈D−
l
P +
2 (z|d)l+1
α
,
if d = din.
(11.2.59)

Semantic Model Adaptation for Evolving Big Social Data
355
In the case of an actual (no pivotal) document, the updating procedure has to be repeated as
many times as the number of the words in the document. Additionally, the insertion of more
than one document during the transition phase from the window at the l-th position to the
(l + 1)-th position can be assimilated by document-wise repeating the updating equations
of the PLSA parameters. Finally, after having absorbed all the documents in the transition
phase, the standard PLSA algorithm is applied for reﬁning the conditional probabilities
estimated by the updating equations during the aforementioned transition phase.
11.2.3
Incremental Latent Dirichlet Allocation
Latent Dirichlet Allocation (LDA) is a generative model of documents [BNJ03]. Indeed,
the documents in LDA are represented as random mixtures over the latent topics. To sim-
plify the discussion, we represent each document d as a vector of word indices:
d =

w(d)
1 , w(d)
2 , . . . , w(d)
|W (d)|
T
w(d)
i
∈{1, 2, . . . , M} ,
(11.2.60)
where M = |W| is the size of the overall vocabulary and |W (d)| is the number of words in d.
Let us assume a collection of |Z| = K topics. For each document d, we have a distribution
of topics z(d) with topic proportions θ(d)
k
such that PK
k=1 θ(d)
k
= 1, which gives a latent
description of the document in terms of its membership. To control complexity, one may
use a Dirichlet prior to limit the number of topics active in any particular document, i.e.:
p(θ(d)|α) = Dirichlet(θ(d)|α) = Γ(PK
k=1 αk)
QK
k=1 Γ(αk)
θα1−1
1
θα2−1
2
. . . θαK−1
K
(11.2.61)
where α = (α1, α2, . . . , αk)T has elements related to the number of topics and Γ(·) is the
Gamma function. The Dirichlet prior is the conjugate prior of the multinomial distribution.
A generative model for sampling a document d with |W (d)| word positions is as follows.
For each word position w(d)
i
, i = 1, 2, . . . , |W (d)|:
a) choose a topic:
z(d)
wi
∼
p(z(d)
wi |θ(d)) = Multinomial(θ(d))
=
PK
k=1 αk!
α1! α2! . . . αK!

θ(d)
1
α1 
θ(d)
2
α2
. . .

θ(d)
K
αK
,
(11.2.62)
b) choose a word:
w(d)
i
∼p

w(d)
i
|ϑ·|z(d)
wi

,
(11.2.63)
where ϑ·|z(d)
wi describes the distribution of words within the topic zw(d)
i .
The word probabilities are parameterized by a K × M matrix B with elements βij =
p(wj = 1|zi = 1). Given the parameters α, B, the joint distribution of a topic mixture θ, a
set of K topics z(d), and a set of |W (d)| words is given by:
p(θ, z(d), w(d)|α, B) = p(θ(d)|α)
|W (d)|
Y
i=1
p(z(d)
wi |θ(d)) p(w(d)
i
|z(d)
wi , βz
w(d)
i
),
(11.2.64)
where the last conditional argument refers to the zw(d)
i
row of B. Thus, the probability of

356
Graph-Based Social Media Analysis
observing the document d, denoted as a sequence of M words forming w is expressed as
[BNJ03]:
P(w|α, B) =
Z
Θ
p(θ|α)
 M
Y
m=1
zK
X
zm=1
P(zm, θ) P(wm|zm, B)
!
dθ.
(11.2.65)
Under this generative process, a corpus of documents can be analyzed with the LDA by
examining the posterior distribution of the topics B, the topic proportions θ, and the topic
assignments Z conditioned on the documents. This posterior cannot be computed directly
[BNJ03, HBB10]. It is usually approximated by using Markov Chain Monte Carlo (MCMC)
methods or variational inference.
Markov Chain Monte Carlo (MCMC) methods are sampling-based algorithms that at-
tempt to collect samples from the posterior to approximate it with an empirical distribution.
Let us assume symmetric Dirichlet priors for α (i.e., α1 = α2 = . . . = αk = α) and each
row of B (i.e., all elements of the row equal η). In the collapsed Gibbs sampling (CGS) the
variable θ is analytically integrated out of the model, and sampling of the topic assignments
zM is performed sequentially for each word wj as follows [GS04]:
P(zj|zM\j, wM) ∝
n(wj)
zj,M\j + η
n(·)
zj,M\j + Mη
n(dj)
zj,M\j + α
n(dj)
·,M\j + Kα
,
(11.2.66)
where zM\j = {z1, . . . , zj−1, zj+1, . . . zM}, M is the vocabulary size, K is the number of
topics, n(wj)
zj,M\j is the number of times word wj is assigned to topic zj, excluding the current
word, n(·)
zj,M\j is the total number of words assigned to topic zj, excluding the current word,
n(dj)
zj,M\j is the number of times a word in document dj is assigned to topic zj, excluding
the current word, and n(dj)
·,M\j is the total number of words in document dj, excluding the
current word.
Variational methods are a deterministic alternative to sampling-based algorithms.
Rather than approximating the posterior with samples, variational methods postulate a
parameterized family of distributions over the hidden structure and then ﬁnd the member
of this family that is closest to the posterior. Thus, the inference problem is transformed to
an optimization problem. That is, in batch Variational Bayesian Inference (VB), the true
posterior is approximated by a simpler distribution q(z, θ, B), which is indexed by a set of
free parameters [Att00, JGJS99]. These parameters are optimized to maximize the Evidence
Lower BOund (ELBO), which is equivalent to minimizing the Kullbak-Leibler Divergence
(KL) between q(z, θ, B) and the posterior p(z, θ, B|w, α, η) [HBB10]. For symmetric Dirich-
let priors, ELBO is deﬁned as:
log p(w|α, η) ≥L(w, φ, γ, λ) ≜Eq[log p(w, z, θ, B|α, η)] −Eq[log q(z, θ, B)],
(11.2.67)
where Eq is the expectation under q and choosing a fully factorized distribution of the form
[BNJ03, HBB10]:
q(z(d)) = φ(d)
w
q(θ(d)) = Dirichlet(θ(d)|γ(d))
q(βk) = Dirichlet(βz|λz),
(11.2.68)
where the posterior over the per-word topic assignments z is parameterized by φ, the pos-
terior over the per-document topic weights θ is parameterized by γ, and the posterior over
the topics β is parameterized by λ. L can be optimized using coordinate ascent over the
variational parameters φ, γ, λ [BNJ03, HBB10]:
φ(d)
wz ∝exp {Eq[log θ(d)
z ]} + exp {Eq[log β(d)
zw ]}
(11.2.69)

Semantic Model Adaptation for Evolving Big Social Data
357
γ(d)
z
= α +
X
w∈W
n(d, w) φ(d)
wz
(11.2.70)
λzw = η +
X
d∈D
n(d, w) φ(d)
wz.
(11.2.71)
The expectations in (11.2.69) are estimated by means of the digamma function Ψ(·), i.e.:
Eq[log θ(d)
z ] = Ψ(γ(d)
z ) −Ψ
 X
z′∈Z
γ(d)
z′
!
(11.2.72)
Eq[log β(d)
zw ] = Ψ(λzw) −Ψ
 X
w′∈W
λzw′
!
.
(11.2.73)
The updates in (11.2.71)-(11.2.73) are guaranteed to converge to a stationary point of the
ELBO. They can be partitioned into an (E)-step and (M)-step by analogy to the EM
algorithm [DLR77]. In the (E)-step, γ and φ are iteratively updated until convergence,
holding λ ﬁxed, and in the (M)-step, λ is updated given φ [HBB10].
Both classes of approximation methods (MCMC and VB) are eﬀective, but they present
signiﬁcant computational challenges in the face of massive datasets. Thus, developing
scalable approximate inference methods for topic models is an active area of research
[AWST09, NASW09, YMM09, YXQ09, HBB10]. In an online variational inference algo-
rithm, called Online VB for LDA or online LDA [HBB10], the parameters of the varia-
tional posterior over the topic distributions B, are approximated by maximizing the ELBO
L setting for λ. That is, having estimated the values γ(n(d), λ) and φ(n(d), λ) of the per-
document variational parameters γ(d) and φ(d), respectively, in the (E)-step of the classic
VB algorithm the ELBO maximizes:
L(w, φ, γ, λ) =
X
d∈D
ℓ(n(d), γ(n(d), λ), φ(n(d), λ), λ),
(11.2.74)
where ℓ(n(d), γ(n(d), λ), φ(n(d), λ), λ) denotes the contribution of the document d to the
ELBO and n(d) is the word count vector for document d [HBB10]. Thus, upon the obser-
vation of a document d indexed as di in the corpus having word counts n(d), an (E)-step is
performed to ﬁnd locally optimal values of γ(d) and φ(d), holding λ ﬁxed. Then, the optimal
value ˜λ of λ given φ(d) is estimated by assuming that the entire corpus consists of a single
document d repeated N times. That is:
λzw = η + N n(di, w) φ(di)
wz .
(11.2.75)
In the online case (i.e., N →∞), the empirical Bayes estimation of β emerges. Next, λ is
updated as a weighted average of its previous value and its optimal value ˜λ, where a control
function ρi ≜(τ0 + i)−κ with κ ∈(0.5, 1] and τ0 ≥0, is used for weight assignment. The
parameter κ controls the rate at which the old values of ˜λ fade away and τ0 slows down the
early iterations of the algorithm. The just described online LDA algorithm corresponds to
a stochastic natural gradient algorithm on the variational objective L [HBB10].
In order to reduce noise in the stochastic gradient estimation [BB08, LK09], the online
VB-LDA method is additionally formulated in a mini-batch framework, where multiple
observations per update are exploited. That is, in mini-batch online VB-LDA the document
corpus D is chunked in mini-batches B = {B1, . . . , BS} of equal size R = |BS| and ˜λ is
estimated upon the mini-batches as follows:
˜λ = ˜λzw = η + N
R
X
d∈Bs
n(d, w) φ(d)
wz,
(11.2.76)

358
Graph-Based Social Media Analysis
where Bs ∈B. In the special case R = N and κ = 0, the online VB-LDA degenerates to
batch VB-LDA algorithm. Another online adaptation of LDA resorts to batch collapsed
Gibbs sampling. The online algorithm, called o-LDA [BB07], is built on the incremental
LDA model [SLTS05]. In the incremental LDA algorithm, batch LDA (i.e., a batch Gibbs
sampler) initially runs on a small window of the incoming data stream. Then the topic of
each new word wr is sampled by conditioning on the words observed so far, instead of all
other words in the corpus.
The performance of o-LDA depends critically on the accuracy of the topics inferred
during the batch phase, since after the batch initialization, o-LDA applies (11.2.66) incre-
mentally for each new word wr, never resampling old topic variables. Thus, if the documents
used to initialize o-LDA are not representative of the full dataset, a poor inference may re-
sult. Also, because each topic variable is sampled by conditioning only on previous words
and topics, samples drawn with o-LDA are not distributed according to the true posterior
distribution P(zN|wN) [CSG09].
An alternative online MCMC approach revises the decisions about previous topic as-
signments, attempting to alleviate the shortcomings of the o-LDA. This approach, called
incremental LDA, periodically resamples the topic assignments for the previously analyzed
words by introducing the incremental Gibbs sampler [CSG09], an algorithm that rejuvenates
old topic assignments in light of new data. Unlike o-LDA, the incremental Gibbs sampler
does not have a batch initialization phase, but it samples topic variables of new words by
means of (11.2.66). That is, after each step r, the incremental Gibbs sampler resamples the
topics of some of the previous words, and the topic assignment zj of each index j in the
“rejuvenation sequence” R(r) is drawn from its conditional distribution as follows:
P(zj|zr\j, wr) ∝
n(wj)
zj,r\j + η
n(·)
zj,r\j + Mη
n(dj)
zj,r\j + α
n(dj)
·,r\j + Kα
,
(11.2.77)
where zr\j = {z1, . . . , zj−1, zj+1, . . . zr}, n(wj)
zj,r\j is the number of times word wj is assigned
to topic zj, n(·)
zj,r\j is the total number of words assigned to topic zj, n(dj)
zj,r\j is the number
of times a word in document dj is assigned to topic zj, and n(dj)
·,r\j is the total number of
words in document dj. All the counts are taken over the Gibbs sampler steps 1 through r,
excluding the word at position j itself (hence the subscripts r \ j).
Instead of frequently resampling the previous topic assignments, another solution is to
concurrently maintain multiple samples of zr, rejuvenating them less frequently. In this way,
the algorithm simultaneously explores several regions of the state space. In addition, it can
be used in a multi-processor environment, since it is simpler to parallelize multiple samples
dedicating each sample to a single machine than to parallelize operations on one sample.
An ensemble of independent samples from the incremental Gibbs sampler could be used to
approximate the posterior distribution P(zN|wN), but if the samples are not rejuvenated
often enough, they will not have the desired distribution. With this motivation, particle
ﬁlters can be used to perform importance weighting on a set of sequentially-generated
samples instead of the CGS [CSG09].
Besides the online methods, two approximate parallel CGS schemes for LDA with similar
predictive performance on held-out documents to the batch CGS have been developed for
large corpora [AWST09].

Semantic Model Adaptation for Evolving Big Social Data
359
11.3
Incremental Spectral Clustering
Spectral clustering has attracted much attention thanks to its solid theoretical foun-
dations in graph theory [Chu97] and its good performance. It has been applied in many
areas, such as word and document clustering [ZHD01, JX06, BK11], Web/blog clustering
[Din04, NXC+07], computer vision [Wei99, SM00, PZK04, CY05, MTP10], speech recogni-
tion [JB03], and classiﬁcation of biological data [PM05, SFSZ05]. Spectral clustering meth-
ods outperform commonly used methods, such as the k-means and the EM mixture models,
in handling complex situations with unknown cluster shapes.
Spectral clustering exploits the information inherent in the spectrum of the data aﬃnity
matrix in order to detect the structure of the data distributions [KTS11]. The aﬃnity matrix,
which holds all the similarity information between the data points, is analyzed in terms of
its eigenstructure expressed by the eigenvectors that correspond to the smallest eigenvalues
of the graph Laplacian. There is no need for the data to be represented as coordinates in
the Euclidean n-th dimensional space. Similarity can be expressed in terms of any measure
between the data points.
Spectral clustering methods vary depending on the use of the unnormalized graph Lapla-
cian [Lux07], the normalized Laplacian expressed as a random walk normalized Laplacian
[SM00], or the symmetric normalized Laplacian [NJW01]. However, all these methods resort
to traditional clustering algorithms, such as the k-means, for clustering the eigenvectors. By
examining spectral clustering in a graph partitioning framework, the methods vary depend-
ing on the disassociation measure used between groups, namely the normalized cut criterion
[SM00], the min-max cut criterion [Din04], or the ratio cut [WC89, HK92]. A comparison
on spectral clustering methods can be found in [VM03, Lux07].
In more detail, let G = G(E, V, W) be a weighted graph with node set V, edge set E,
and a similarity matrix W ∈Rn×n with elements wij that indicate the similarity between
nodes vi and vj. Commonly used similarity metrics between two data points include the
inner product of the feature vectors, wij = xT
i xj, the diagonally scaled Gaussian similarity,
wij = exp (−∥xi −xj∥2/σ2), or the aﬃnity matrix of the graph. The vector norm ∥· ∥is
usually the l2 norm, i.e., ∥x∥=
√
xT x, and σ is a scaling parameter that determines how
similarity depends on distance. That is, compared to the true scale of the problem, high σ
values make most points to appear similar, while low σ values reduce the similarity between
even close points. From the perspective of spectral clustering, graph G can be represented
through its Laplacian matrix [Chu97], which can be either [Lux07]:
• Unnormalized: L = D −W, or
• Normalized: Lsym = D−1/2 L D−1/2 = I −D−1/2 W D−1/2 (symmetric matrix) or
Lrw = D−1 L = I −D−1W (closely related to random walk),
where D = diag{d1, . . . , dn} is the degree matrix with elements di = Pn
j=1 wij in the main
diagonal.
In case of large datasets or dynamic data, the aforementioned spectral clustering methods
are ineﬃcient, since they cannot be applied in an incremental fashion. As a result, various
solutions have emerged that either simulate the change of eigensystem in order to avoid
re-computations in the presence of new data [NXC+10, DGC14] or extract representative
points to compress the dataset [VDL07, KTS11].
For methods simulating the change of eigensystem, an incremental spectral cluster-
ing method [NXC+07, NXC+10] updates the eigenvectors of the generalized eigenproblem
[SM00] by ﬁnding the derivatives on the eigenvalues/vectors with respect to perturbations

360
Graph-Based Social Media Analysis
in all the quantities involved. An iterative reﬁnement algorithm is given for the eigenvalues
and eigenvectors given a change in the edges or vertices of a graph. The resulting k small-
est eigenvectors are then clustered using the k-means clustering, while the eigenvectors are
recomputed after every R-th graph in the sequence [NXC+10].
In detail, the data dynamics are fed in the eignvalue system by means of the incidence
vector/matrix. An incidence matrix R is a matrix with elements the incident vectors rij(w).
An incident vector is deﬁned as rij(w) = √wuij, where uij is a column vector with only
two non-zero elements: i-th element equal to 1 and j-th element −1. In other words, an
incidence vector rij(w) assigns the similarity metric w between the two data points i and
j or the weight of edge (i, j). Accordingly the incidence matrix is another representation
of the similarity matrix [NXC+10]. Assuming L = R RT , whenever a change occurs in
the graph due to the perturbation of the edge weight between the data points i and j by
∆wij represented by the incidence vector rij(∆wij), the new graph Laplacian is given by
˜L = ˜R ˜RT , where ˜R = [R|rij(∆wij)]. Therefore, the increment of the Laplacian matrix due
to this modiﬁcation is given by [NXC+10]:
∆L = ˜L −L = ∆wij uij uT
ij
(11.3.1)
∆D = ∆wij diag{vij},
(11.3.2)
where vij is a column vector with zero elements, except i-th and j-th elements that equal
to 1.
Next, let us approximate the increments of the eigenvalues and the eigenvectors in the
spectral clustering due to the perturbation rij(∆wij). The generalized eigenvalue system of
the normalized cut is Lq = λDq, where (λ, q) is an eigenpair (i.e., the pair of a generalized
eigenvalue and its associated eigenvector). By taking into account the second and third
order error and using the existing eigenvector, the eigenvalue increment is approximated by
[NXC+10]:
∆λ
=
∆wij
a + b
1 + c + d,
(11.3.3)
where
a
=
(qi −qj)2 −λ (q2
i + q2
j )
b
=
(qi −qj) (∆qi −∆qj) −λ(qi ∆qi + qj ∆qj)
c
=
∆wij (q2
i + q2
j )
d
=
X
k∈Nij
qk dk ∆qk,
with qi and ∆qi denoting the i-th element of the generalized eigenvector q and its pertur-
bation ∆q. d is an approximation of qT D ∆q under the assumption that the impact of
similarity perturbation ∆wij lies within the spatial neighbourhood Nij. The latter is deﬁned
as Nij = {k|wik > τ or wjk > τ}, where τ is a predeﬁned threshold, which can become 0
for a sparse dataset. Let K = L −λ D and h = (∆λ D + λ ∆D −∆L) q, where ∆D and
∆L are given by (11.3.1) and (11.3.2), respectively. Assuming that ∆qk = 0 if k /∈Nij and
eliminating these entries from ∆q as well as the corresponding columns in K, let ∆qij and
KNij denote the elements survived in both ∆q and K. It has been proved that [NXC+10]:
∆qij = (KT
Nij KNij)−1 KT
Nij h.
(11.3.4)
The incremental spectral clustering method is summarized in Algorithm 11.3.1.

Semantic Model Adaptation for Evolving Big Social Data
361
The algorithm needs a constant running time to compute ∆λ and O( ¯N 2 n)+ O( ¯N 3)+
O( ¯N n)+ O( ¯N 2) to compute ∆q, where ¯N is the average size of the spatial neighborhood
of a node, O( ¯N 2 n) is needed to compute KT
Nij KNij in (11.3.4), O( ¯N 3) for the inversion
Algorithm 11.3.1: Incremental Spectral Clustering [NXC+07, NXC+10]
Input: Similarity matrix W, degree matrix D, Laplacian matrix L, k number of clusters,
and nsimChanges similarity changes.
1. Suppose at time t, the dataset grows large enough. Solve Lq = λDq using Algorithm
11.3.2 [SM00] for the eigenvectors with the smallest eigenvalues. This solution and the
matrices D and L serve as initialization for the following steps.
2. Every time a similarity change occurs
• Update the eigenvalues and eigenvectors using Algorithm 11.3.3.
• Update matrices L and D as described in (11.3.1) and (11.3.2), respectively.
3. If a data point is added or deleted, it is decomposed into a sequence of similarity
changes and Step 2 is repeatedly conducted.
4. After nsimChanges similarity changes occur, re-initialize the spectral clustering by
repeating Step 1 in order to eliminate any accumulated errors.
Algorithm 11.3.2: Normalized Spectral Clustering [SM00]
Input: Similarity matrix W, degree matrix D, Laplacian matrix L, and k number of clusters.
Output: Clusters A1, . . . , Ak with Ai = {j|vj ∈Ci} being the set of indices of vertices vj
assigned to i-th cluster.
1. Compute the ﬁrst k eigenvectors q1, . . . , qk of the generalized eigenproblem L q =
λ D q.
2. Let U ∈Rn×k be the matrix containing the vectors q1, . . . , qk as columns.
3. For i = 1, . . . , n, let ui ∈Rk be the vector corresponding to the i-th row of U.
4. Cluster the points (ui)i=1,...,n i = 1, 2, . . . , n with the k-means algorithm into clusters
C1, . . . , Ck.
Algorithm 11.3.3: Iterative reﬁnement of ∆λ and ∆q [NXC+10]
Input: ∆wij, number of iterations niter
Output: Reﬁned ∆λ and ∆q.
1. Set ∆q = 0.
2. Estimate ∆λ by (11.3.3), using the existing ∆q.
3. Estimate ∆q by (11.3.4), using the existing ∆λ.
4. Repeat Steps 2 and 3 until there is no signiﬁcant change in ∆λ and ∆q, or for at most
niter iterations.

362
Graph-Based Social Media Analysis
of O( ¯N n) for KT
Nij h, and O( ¯N 2) for ∆qij [NXC+10]. In Web applications, ¯N is usually
constant and small, so the running time of the incremental approach is O(n). The time
complexity can be tuned to some extent at the expense of accuracy adjusting ¯N by tuning
the threshold τ.
In the case of methods that extract representative points to compress the dataset, a self-
adaptive incremental spectral clustering method [VDL07] handles the addition of new data
by employing the spectral clustering algorithm [NJW01] on a few representatives for each
cluster. In more detail, the incremental spectral clustering method starts with an empty
dataset X and an empty aﬃnity matrix A. For each data point xi added to the dataset
X, the algorithm iteratively estimates a representative for each cluster, as the data point
that is most similar to all other points. If the cluster representative is too far away from
any point in the cluster with respect to a similarity threshold, the number of the clusters is
increased and a new clustering is performed. In this case, the entries of the aﬃnity matrix
that have been assigned to such a cluster are replaced by a single cluster representative,
while the original cluster contents are stored for future use in the computation of a new
cluster representative, if necessary. The aﬃnity matrix is thus shrunk to a smaller size.
The process continues iteratively for all the data points to be added. Usually, the clusters
determined are more than the actual ones, since the iterative method always increases the
number of clusters k. To alleviate this problem, the number of clusters k should be regularly
or randomly decreased by 1.
A more recent incremental spectral clustering approach, called Incremental Approximate
Spectral Clustering, which is viewed as an incremental eigenvalue solution for the spectral
clustering described in [NJW01], ﬁnds the approximate eigenvectors of a symmetric matrix
given a change [DGC14]. The method is based on the positive semi-deﬁnite shifted Laplacian
ˆL = 2I−Lsym = I+D−1/2WD−1/2. In the initialization step, the shifted Laplacian matrix
ˆL1 for the ﬁrst graph G1 is estimated, the largest k eigenvectors found are normalized,
and k-means clustering is applied to the rows of the matrix having as column vectors the
eigenvectors. Then, for every successive graph Gt+1, a rank-k eigendecomposition of ˆLt+1
is approximated with eigenupdate methods on the already known eigendecomposition of ˆLt
of graph Gt. The eigenupdate methods are based on the SVD updating, which is used for
latent semantic indexing [ZS99] (Section 11.2.1). These updates are eﬃcient as long as the
change between ˆLt+1 and ˆLt is small. However, as the number of iterations increases, they
suﬀer from cumulative errors. To alleviate this, a recomputation of the eigenvectors of ˆLt
is performed after a predeﬁned number of iterations.
11.4
Tensor Model Adaptation
11.4.1
Basic Tensor Concepts
Tensors (or more correctly hypermatrices) are multi-dimensional (multi-way) arrays that
are widely used to represent multi-dimensional data [LPV13]. Data in multiple tensors
can be modeled as tensor sequences or tensor streams. A sequence of N-th order tensors
X1, . . . , Xn where Xi ∈RI1×...×IN , (1 ≤i ≤n) is deﬁned as tensor sequence, when n is
ﬁxed and as a tensor stream, when n increases over time [STF06]. Tensor sequences can be
eﬃciently handled by means of oﬄine tensor analysis, while for tensor streams incremental
tensor analysis is employed [STPF08].

Semantic Model Adaptation for Evolving Big Social Data
363
11.4.2
Incremental Tensor Analysis
Oﬄine Tensor Analysis (OTA).
OTA for tensor sequences is a generalization of Principal Component Analysis (PCA)
for higher-order tensors. That is, it can be applied to general N-th order tensors instead
of simple vectors (ﬁrst-order tensors). Given a tensor sequence X1, . . . , Xn, where Xt|n
t=1 ∈
RI1×...×IN , OTA ﬁnds the projection matrices Uk|N
k=1 ∈RIk×rk and a sequence of core ten-
sors, Yt|n
t=1 ∈Rr1×...×rN such that the reconstruction error e = Pn
t=1 ∥Xt −Yt
QN
i=1 ×iUi∥
is minimized where ∥· ∥is the squared Frobenius norm. The core tensor can be viewed
as a low-dimensional summary of an input tensor, while the projection matrices represent
the transformation between the input tensor and the core tensor. Several solutions for ap-
proximating the projection matrices exist [LV00, KBK05, STPF08]. An iterative algorithm,
which projects and matricizes along each mode for every tensor sequence and then performs
PCA for ﬁnding the projection matrix for that mode, is depicted in Algorithm 11.4.1. An
example of OTA over n second-order tensors is presented in Figure 11.4.1.
Algorithm 11.4.1: Oﬄine Tensor Analysis (OTA)
Input: tensor sequence Xt|n
t=1 ∈RI1×...×IN , dimensionality rd|N
d=1 of the core tensors Yt|n
t=1.
Output: Core tensors Yt|n
t=1 ∈Rr1×...×rN and projection matrices Ud|N
d=1 ∈RId×rd.
1. Initialize projection matrix Ud|N
d=1 to be an Id × rd truncated identity matrix.
2. For each d ∈[1, N]
a. Initialiaze the covariance matrix Cd ∈RId×Id with zero values.
b. For every tensor Xt in sequence repeat Steps 2(b)i - 2(b)ii:
i. Construct Z by projecting into all but the d-th projection matrices, Z =
Xt
Q
j̸=d ×jUT
j .
ii. Update the covariance matrix for each mode of Z, Cd = Z(d) ZT
(d)
c. Set Ud as the matrix having columns the top rd eigenvectors of Cd.
3. For every tensor Xt in sequence estimate the core tensors Yt = Xt
QN
j=1 ×j UT
j .
FIGURE 11.4.1: OTA process for n second-order tensors (all matrices Ud are tall, i.e.,
Id > rd) [STPF08].

364
Graph-Based Social Media Analysis
The computational cost of OTA is:
O



N
X
d=1
n
N
X
i=1
i̸=d
iY
j=1
rj
N
Y
j=i
Ij + nId
N
Y
j=1
Ij + I3
d


≃O

n
N
Y
j=1
Ij
N
X
d=1
Id +
N
X
i=1
I3
d

,
(11.4.1)
where PN
i=1
i̸=d
Qi
j=1 rj
QN
j=i Ij is the cost of Step 2(b)i, Id
QN
j=1 Ij is the computational cost
of Step 2(b)ii and O(I3
d) the cost of Step 2c in Algorithm 11.4.1 [STPF08].
Although Algorithm 11.4.1 is eﬃcient for tensor sequences, it cannot be applied to dy-
namic environments (i.e., tensor streams), where new tensors are added to an initial tensor
sequence. The straightforward application of OTA to every newly incoming tensor is com-
putationally expensive. Therefore, Incremental Tensor Analysis (ITA) has been proposed
to work eﬃciently on tensor streams. Given a new tensor Xt ∈RI1×...×IN and the old
projection matrices, ITA ﬁnds the new projection matrices Uk|N
k=1 ∈RIk×rk and the core
tensors Yt, such that the reconstruction error:
et =
n
X
t=1
∥Xt −Yt
N
Y
i=1
×iUi∥
(11.4.2)
is minimized.
There are three variants of incremental tensor analysis: Dynamic Tensor Analysis
(DTA), Streaming Tensor Analysis (STA), and Window-Based Tensor Analysis (WTA).
In DTA and WTA, an incremental update of the covariance matrices is used, while in STA
an incremental update of projection matrices is employed. Additionally, in order to assim-
ilate changes over time DTA and STA work with a forgetting factor that fades away the
covariance matrices of earlier steps, while WTA uses a sliding window. The ITA methods
are presented in more detail next [STPF08]:
• Dynamic Tensor Analysis (DTA). Each mode of the tensor is processed at-
a-time and the covariance of d-th mode is updated by Cd ←λCd + X(d)XT
(d),
where X(d) ∈Id × R(Q
i̸=d Ii) is the mode-d matricizing of the tensor X. The up-
dated projection matrices are computed by diagonalization: Cd = UdSdUT
d , where
Ud is an orthogonal matrix and Sd is a diagonal matrix. The DTA process is illus-
trated in Figure 11.4.2, while the corresponding algorithm is outlined in Algorithm
11.4.2. The computational cost for updating the covariance matrix (Algorithm 11.4.2
- Step 2) is O
PN
i=1 Ii
QN
j=1 Ij

, while step 3 in Algorithm 11.4.2 requires N eigen-
decompositions for matrices of size Id × Id, d = 1, 2, . . . , N which costs O(I3
d) each
[STPF08]. Thus, the computational cost of DTA is O
PN
i=1(Ii + I2
i ) QN
j=1 Ij

.
• Streaming Tensor Analysis (STA). In the presence of a new tensor, STA aims
to smoothly adjust the projection matrices. The STA procedure is presented in Fig-
ure 11.4.3. For every mode d of the new tensor X, the matrix X(d) is derived by
unfolding, and the projection matrix Ud is adjusted by applying the Streaming Pat-
tern Discovery in Multiple Time-series (SPIRIT) algorithm over the columns of
X(d)[PSF05]. The STA algorithm is summarized in Algorithms 11.4.3 and 11.4.4.
Taking into account that the computational cost of Steps 2 and 3 of Algorithm 11.4.4
is O(r(4Ii + 1)) and O(2r2
i Ii), respectively, the computational complexity of STA is
≃O
PN
i (2r2
i Ii + 4riIi) QN
j=1
j̸=i
Ij

= O
PN
i (2r2
i + 4ri) QN
j=1 Ij

. This complexity
is smaller than that of DTA when ri ≪Ii.

Semantic Model Adaptation for Evolving Big Social Data
365
x
=
=
Construct Variance Matrix of 
Incermental Tensor
Reconstruct Variance Matrix
Diagonalize Variance 
 
Matrix
Update Variance Matrix
Matricize
Transpose
Matricize
X
X(d)
X(d)
X(d)
 X(d)
Ud
T
Ud
Cd
Cd
Sd
Sd
Ud
T
Ud
λ
T
T
Id
FIGURE 11.4.2: DTA process [STPF08].
Algorithm 11.4.2: Dynamic Tensor Analysis (DTA)
Input: new tensor X ∈RI1×...×IN , old projection matrices Ud|N
d=1 ∈RId×rd, old energy
matrices Sd|N
d=1 ∈Rrd×rd, output ranks ri|N
i=1, and forgetting factor λ.
Output: new projection matrices Ud|N
d=1 ∈RId×rd and core tensor Y ∈Rr1×...×rN .
– For each tensor mode d ∈[1, N] repeat:
1. Reconstruct the old covariance matrix Cd ←UdSdUT
d .
2. Update the old covariance matrix Cd ←λCd + X(d)XT
(d).
3. Set Ud as the matrix having the top rd eigenvectors of Cd as columns.
– Compute the core tensor Y = X QN
j=1 ×j UT
j .
ui
FIGURE 11.4.3: STA process [STPF08].

366
Graph-Based Social Media Analysis
Algorithm 11.4.3: Streaming Tensor Analysis (STA)
Input: new tensor X ∈RI1×...×IN , old projection matrices Ud|N
d=1 ∈RId×rd, old energy
matrices Sd|N
d=1 ∈Rrd×rd, output ranks ri|N
i=1, and forgetting factor λ.
Output: new projection matrices Ud|N
d=1 ∈RId×rd and core tensor Y ∈Rr1×...×rN .
– For each tensor mode d ∈[1, N] repeat:
1. Reconstruct matrix X(d) as mode-d unfolding of the new tensor X.
2. For each column vector x in X(d) apply the SPIRIT algorithm (Algorithm 11.4.4):
(Ud, Sd) ←TrackU(Ud, x, Sd, λ).
– Compute the core tensor Y = X QN
j=1 ×j UT
j .
Algorithm 11.4.4: SPIRIT Algorithm - TrackU
Input: projection matrix U ∈Rn×r, new vector x ∈Rn, energy matrix S ∈Rr×r, and
forgetting factor λ.
Output: projection matrix U and energy matrix S.
1. Initialize x′ = x and s = diag(S).
2. For each i ∈[1, r] repeat:
– Estimate the projection yi = uT
i x′i, where ui is the i-th column of U.
– Estimate the energy as the i-th eigenvalue: si ←λ si + y2
i .
– Estimate the error ei = x′i −yi ui.
– Update the estimate of the principal component ui ←ui + 1
si yi ei.
– Repeat with remainder of x′i+1 = x′i −yiui.
3. Orthogonalize ui by ﬁxing u1.
4. Set S to be the diagonal matrix having s in its main diagonal.
• Window-Based Tensor Analysis (WTA). Instead of using a forgetting factor,
as in DTA and STA, WTA handles time changes by means of a sliding window that
assigns the same weight to all time-stamps in the window. A tensor window D(n, W)
is deﬁned as a subset of a tensor stream ending at time n with size W. That is,
D(n, W) ∈RW ×I1×...×IN = {Xn−W +1, . . . , Xn}, where each Xi ∈RI1×...×IN . In order
to incrementally extract patterns from tensor streams, two variant WTA methods
exist. Namely, the Independent-Window Tensor Analysis (IWTA) and the Moving-
Window Tensor Analysis (MWTA) [STPF08].
Given a tensor window D ∈RW ×I1×...×IN , IWTA ﬁnds the projection matrices U0 ∈
RW ×r0 and Ui|N
i=1 ∈RIi×ri, such that the error e = ∥D −D QN
i=1 ×i (UT
i Ui)∥is
minimized [STPF08]. In that sense, each tensor window is treated independently of
each other and is summarized into a core tensor Y associated with projection matrices
Ui by Y = D QN
i=0 ×iUT
i . At every time-stamp t, a tensor window D(n, W) is formed
that includes the current tensor Dn and the W −1 previous versions. The projection
matrices Ui|N
i=0 are estimated by applying the alternating least squares method (i.e.,
the PARAFAC algorithm) [Tuc66]. Each projection matrix Ui|N
i=0 is initialized to be a
Ii × ri truncated identity matrix. An intuitive outline of IWTA is presented in Figure
11.4.4.
The MWTA relaxes the time independence assumption of ITWA and uses the
time-dependence structure on the tensor windows. In detail, given a tensor window

Semantic Model Adaptation for Evolving Big Social Data
367
Tensor 
stream
W
Tensor 
window
Tensor 
analysis
D(n-2,W)
D(n-1,W)
D(n,W)
Core n-2
Core n-1
Core n
n
FIGURE 11.4.4: IWTA process: The core tensors and projection matrices are estimated for
every tensor window separately [STPF08].
D(n, W) ∈RW ×I1×...×IN and the previous result D(n−1, W) ∈RW ×I1×...×IN , WTA
ﬁnds the new projection matrices U0 ∈RW ×r0 and Ui|N
i=1 ∈RIi×ri, such that the
error e = ∥D(n, W) −D(n, W) QN
i=1 ×i (UT
i Ui)∥is minimized. More speciﬁcally,
MWTA exploits the overlapping information of two consecutive tensor windows to
update the covariance matrices Ci|N
i=0 ∈RIi×Ii by the iterative procedure that fol-
lows [STPF08]:
– Covariance matrices Ci updates for modes 1 to N:
CT
d ←Cd −Dn−W ,(d) DT
n−W,(d) + Dn,(d) Dn,(d), where Dn−W,(d) and Dn,(d) is
the mode-d unfolding matrix of tensors Dn−W and Dn, respectively.
– Covariance matrix C0 update that corresponds to the time mode: The tensor win-
dow Dn−W , . . . , Dn is unfolded along the time mode giving the matrix [x|D|y],
where x and y are the vectorizations of the older tensor Dn−W and the new ten-
sor Dn, respectively (i.e., the transposed time mode of the associated tensors)
and D is the overlapping part between two tensor windows. Given that the old
covariance matrix for tensor window D(n −1, W) is:
Cold
0
= [x|D]T [x|D] =
"
xT x
xT D
DT x
DT D
#
,
(11.4.3)
the new covariance matrix Cnew
0
for tensor window D(n, W) can be incrementally
computed as:
Cnew
0
=
"
DT D
DT y
yT D
yT y
#
.
(11.4.4)
The upper left submatrix DT D in (11.4.4) is obtained from the lower right sub-
matrix Cold
0
in (11.4.3) without any computation and yT y is the inner product
of the new data. DT y involves both the new tensor and all other tensors in
the window. The update of the covariance matrix is not local to the added and
deleted tensor, but has a global eﬀect over the entire window.

368
Graph-Based Social Media Analysis
The algorithm of MWTA is summarized in Algorithm 11.4.5.
Algorithm 11.4.5: Moving Window Tensor Analysis (MWTA)
Input: new tensor Dn ∈RI1×...×IN , old tensor Dn−W ∈RI1×...×IN , old covariance matrices
Cd|N
d=1 ∈RId×Id.
Output: new covariance matrices Cd|N
d=1 ∈RId×Id, projection matrices Ud|N
d=1 ∈RId×rd
and core tensor Y ∈Rr1×...×rN .
1. Initialize each mode except mode 0 that corresponds to time. That is, for each d ∈
[1, N]:
• Estimate Dn−W,(d)(Dn,(d)) by mode-d unfolding of Dn−W (Dn).
• Update the old covariance matrix Cd ←Cd−Dn−W ,(d)DT
n−W,(d)+Dn,(d)DT
n,(d).
• Perform diagonalization Cd = UdΛdUT
d .
• Truncate Ud to the ﬁrst rd columns.
2. Apply the iterative algorithm with the new initialization starting from the time mode.
Ignoring the cost of diagonalization, the computational complexity of IWTA and MWTA
is O(W PN
i=1 Ii) and O(W PN
i=1 ri + PN
i=1 Ii ri + W r0), respectively. MWTA is faster
in practice, because it requires a smaller number of iterations to converge than IWTA
[STPF08].
11.5
Parallel and Distributed Approaches for Big Data Analysis
The incremental methods described in Sections 11.2-11.4 can eﬃciently handle the dy-
namic nature of the social data and the need for real time processing (i.e., online algorithms).
Additionally, they can deal with the big data volume, by incrementally processing the data
in subsets rather than as a whole at once. However, for big data, where processing speed,
memory, storage, and scalability are important issues, parallel and distributed approaches
can be even more eﬀective. In Sections 11.5.1-11.5.4, parallel approaches on latent variable
models, spectral clustering, and tensor models are presented.
11.5.1
Parallel Probabilistic Latent Semantic Analysis
The computationally intensive EM algorithm entailed in PLSA makes the use of PLSA
for large data collections rather impractical. Therefore, parallel implementations that dis-
tribute the algorithm across multiple processors with either shared or distributed memory
have been proposed to overcome this PLSA limitation.
The underlying idea of such a parallel PLSA method is to partition the co-occurrence
matrix into blocks to be queued and processed one-by-one by each available processor in a
round-robin way minimizing at the same time the overall processors’ idle time [HCZS08].
The block dividing algorithm splits the co-occurrence matrix along the dimension of the ob-
served variables (i.e., words and documents) [HCZS08], or along the dimension of the latent
variable (topics) [WAM09]. In the ﬁrst case, an additional pre-processing step is needed to
distribute the non-zero co-occurrence counts among blocks in order to achieve load balance

Semantic Model Adaptation for Evolving Big Social Data
369
during parallelization [HCZS08], while in the second case, when the latent variables are
evenly divisible to the number of processors, a load balance among processors is achieved
without any pre-processing step [WAM09]. Moreover, a more eﬀective formulation of the
EM algorithm that combines the expectation and the minimization step into a single step
is used [WAM09]. Among the various parallel implementations, multiple processors with
shared memory [HCZS08] and with distributed memory [WAM09] are considered, using
the OpenMP [CMD+00] and the Message Passing Interface (MPI) [TRG05] programming
model, respectively. In the case of distributed memory, the communication between proces-
sors is explicitly taken care of, while all the processors may have equal roles with all their
peers, or one processor is assigned the role of a master and all the others that of a slave. In
the ﬁrst case, all the processors communicate directly with their peers, while in the second
case the processors communicate only with the master processor. When the parallel method
is implemented with a master processor and k slave processors, the master processor is re-
sponsible for initializing and normalizing the entailed conditional probabilities of the PLSA
model, while the slave processors estimate the corresponding new conditional probabilities.
The slave processors remain idle during the execution of the initialization step in the master
processor, while the master processor is prevented from remaining idle when the slave pro-
cessors are active by being assigned a work similar to that of a slave processor to perform.
Transmissions among processors are blocked until the processors have the probabilities to
be sent/received estimated [WAM09].
Two more recent parallel implementations of PLSA, P 2LSA and P 2LSA+, are based
on the MapReduce model [DG08]. In P 2LSA, the E-step is performed by the Map function
and the M-step is performed by the Reduce function. The intermediate results computed
in the E-step have to be sent to the M-step, but exchanging large amounts of data between
the Map and the Reduce function increases the network load and the overall running time
[JGS+11]. The P 2LSA variant, P 2LSA+, alleviates this problem by performing the E-
step and the M-step simultaneously in the Map function, thus reducing the data transfers
between the EM steps [JGS+11].
11.5.2
Parallel Latent Dirichlet Allocation
The majority of the parallel implementations for LDA are built on the MCMC-based ap-
proximation rather on variational inference. Dirichlet Compound Multinomial LDA (DCM-
LDA) is one of the ﬁrst parallel LDA methods that performs Gibbs sampling on data subsets
distributed in diﬀerent processors. Each processor works independently without any com-
munication with the others, while a global clustering of the topics is performed at the end
of parallel processing in order to obtain a global solution [MM07].
In another approach, called Approximate Distributed LDA (AD-LDA), a local Gibbs
sampling iteration is performed in each processor followed by a global update using a reduce-
scatter operation [NASW07]. In more detail, the |D| documents are distributed among
p processors, with |D|/p documents in each processor. Similarly, the words w and the
corresponding topic assignments z are respectively partitioned into w = {w1, w2, . . . , wp}
and z = {z1, z2, . . . , zp} so that wp and zp exist only in processor p. The topic-document
counts n(dj)
zk
are likewise distributed, but each processor maintains its own copy of word-
topic counts n(wi)
zk|p and topic counts n(·)
zk|p. Each iteration of the algorithm is composed of
a Gibbs sampling step and a synchronization step. In the sampling step, each processor p
samples zM|p = {z1|p, . . . , zM|p}, where M is the vocabulary size, using the global topics of
the previous iteration. In the synchronization step, the local word-topic counts n(wi)
zk|p in each
processor are aggregated to produce a global set of word-topic counts n(wi)
zk
. This process

370
Graph-Based Social Media Analysis
is repeated until convergence or for a ﬁxed number of iterations [NASW07, WBSC09],
providing substantial memory and time savings.
The synchronous algorithm AD-LDA requires global synchronization at each iteration.
This is an important drawback of the method, since a global synchronization step may
not be always feasible due to processors unavailability, or diﬀerent processor speeds. In
an asynchronous distributed version of LDA (Async-LDA) that follows a similar two-step
process as AD-LDA, this drawback is tackled by means of a step where each processor
communicates with another random processor after the local Gibbs sampling step [ASW08].
That is, during each iteration of Async-LDA, the processors perform a full sweep of collapsed
Gibbs sampling over their local topic assignment variables zM|p for each word wj in a manner
analogous to (11.2.66):
P(zj|p|zM\j|p, wM|p) ∝
n(wj)
(zj,M\j)|p + n(wj)
(zj,M\j)|\p + η
n(·)
(zj,M\j)|p + n(·)
(zj,M\j)|\p + Mη

n(dj)
(zj,M\j)|p + α

(11.5.1)
where the notation n( )|p refers to the local counts in each processor p and n( )|\p denotes
the processor’s p belief for the corresponding counts of all the other processors with which
it has already communicated, excluding the processor’s p local counts. Thus, the sampling
in (11.5.1) is based on the processors “noisy view” of the global set of topics [ASW08].
In the communication step between two processors p and p′, two cases are handled: (a)
p and p′ communicate for the ﬁrst time and they simply exchange their local word-topics
counts n(·)
zk|p and n(·)
zk|p′ to be added to the corresponding global word-topic counts n(·)
zk|\p and
n(·)
zk|\p′ each processor monitors through communication with other processors; (b) p and p′
have communicated in a previous step, so besides exchanging and adding their local counts
they also remove from the global word-topic counts n(·)
zk|\p and n(·)
zk|\p′ the inﬂuence of the
other processor from their previous encounter. In the latter case, any over-inﬂuence among
processors that happen to communicate more frequently is prevented. When there are no
strict memory and bandwidth limitations, allowing each processor to cache the previous
counts of other processors and forward its individual cached counts to the other processors
it communicates with, the eﬃciency of Async-LDA can be signiﬁcantly improved [ASW08].
Two parallel implementations of the synchronous AD-LDA [NASW07], use either the
Message Passing Interface (MPI) [TRG05] or the MapReduce model [DG08], have been de-
scribed [WBSC09]. In the MPI-PLDA implementation, a worker (i.e., a thread or a process
that executes part of the parallel computing job), performs the necessary initializations, es-
timates the processor speciﬁc document and word-topic counts accumulated from local doc-
ument topic assignments in each processor and performs the Gibbs sampling, as described
in [NASW07]. At the end of each Gibbs sampling iteration, the procedure MPI AllReduce
is invoked in order to estimate the global word-topic counts and distribute them to all the
other workers. The worker sleeps until the MPI implementation ﬁnishes AllReduce and the
corresponding updated counts are stored in the worker’s buﬀer. The implementation also
has checkpoints testing for a machine failure.
The PLDA is implemented in the MapReduce framework by means of three proce-
dures (PLDA-MapperStart, PLDA-Map and PLDA-MapperFlush) in the mapping phase
and three procedures (PLDA-ReducerStart, PLDA-Reduce, and PLDA-ReducerFlush) in
the reducing phase. Gibbs sampling is performed in the mapping phase, while model and
topic assignment updates are performed in the reducing phase. Each map worker is assigned
a fraction of |D|/p documents in an input shard1 and loads a local copy of the word-topic
counts by calling PLDA-MapperStart. Then, it invokes a PLDA-Map for each local doc-
ument in order to update the corresponding topic assignments zj|p. After Gibbs sampling
1Shard is a local subset of input and output to a MapReduce process in the form of key-value pairs.

Semantic Model Adaptation for Evolving Big Social Data
371
on all documents in a shard is ﬁnished, the PLDA-MapperFlush is invoked to output the
updated local word-topic counts. In the reducing phase, two standard reducers are used: an
IdentityReducer that copies each zj|p produced by Gibbs sampling to the disk and a Vec-
torSummingReducer that outputs the aggregated word-topic counts by taking into account
the local counts. All map and all reduce workers run in parallel and they communicate only
in the shuﬄing phase [WBSC09].
PLDA+ consists an improved distributed algorithm of LDA based on PLDA. The
PLDA+ reduces inter-computer communication time by employing four interdependent
strategies (i.e., data placement, pipeline processing, word bundling, and priority-based
scheduling) [LZCS11]. Other LDA parallelization methods include parallel algorithms of
Gibbs sampling and variational inference for GPUs (Graphics Processing Units) [YXQ09],
and parallel implementations of the variational EM algorithm in a multiprocessor architec-
ture as well as in a distributed setting [NCL07].
11.5.3
Parallel Spectral Clustering
Spectral clustering described in Section 11.3 is eﬀective for ﬁnding clusters, but its
performance and its applicability to large scale datasets suﬀers from high computational
complexity. More precisely, spectral clustering exhibits a quadratic time and space complex-
ity, when constructing and storing the aﬃnity matrix that holds the pair-wise similarities
among the data instances. Time complexity becomes cubic when calculating the eigen-
decomposition of the Laplacian matrix, making the application of the spectral clustering
algorithm to big data diﬃcult. Several alternative methods have been proposed to address
these computational and memory diﬃculties.
One of the most commonly used approaches to handle the memory bottleneck is to spar-
sify the aﬃnity matrix and to use a sparse eigensolver for the decomposition of the resulting
sparse Laplacian matrix. Sparsiﬁcation is usually achieved by means of the t-nearest neigh-
bor approach that considers only the signiﬁcant relationships between the data instances,
or by the ϵ-neighborhood approach that zeroes out those elements of the aﬃnity matrix
that are below a pre-deﬁned threshold ϵ [CSBL11]. Both sparsiﬁcation schemes keep the
computational complexity high, since all the elements of the aﬃnity matrix still need to
be estimated. To alleviate this, some approaches consider zeroing out random entries in
the aﬃnity matrix, saving computational time at the expense of clustering performance
[AMS01]. More elaborate methods focus on reducing the computational cost of the eigen-
decomposition of the graph Laplacian by using the classical Nystr¨om approximation method
on a dense submatrix of the original aﬃnity matrix [FBCM04]. That is, data are randomly
sampled in order to obtain small-size eigenvectors that are then used to estimate an ap-
proximation of the eigenvectors of the original matrix.
Similar to the idea of reducing the original dataset, k-means clustering is ﬁrst applied
on the dataset resulting in a large cluster number and the data points that are close to
the centers (according to a pre-deﬁned distance threshold) are removed [SS08]. KASP and
RASP algorithms also attempt to perform fast approximate spectral clustering by collaps-
ing all data points into centroids obtained through k-means or random projection trees
respectively, and by applying eigen-decomposition only to the centroids [YXQ09].
Random projection is also used in order to reduce data dimensionality [SI09]. Early
stopping strategies could be applied to speed up eigen-decompositions [CGL+06, LYZ+07]
based on the observation that well-separated data points converge to the ﬁnal solution more
quickly. In Landmark-based spectral clustering, all data points are encoded by means of a
codebook constructed by selecting landmark points among the data points [CC11].
In a slightly diﬀerent method, the so-called Eﬃcient Spectral Clustering on Graphs
(ESCG) for large-scale graph data, the original graph is coarsened by generating supernodes

372
Graph-Based Social Media Analysis
linked to the nodes in the original graph and a bipartite structure is obtained which pre-
serves the links between original graph nodes and the new supernodes. The super nodes
are expected to behave as cluster indicators that guide the clustering of nodes in the origi-
nal graph. Thus, the supernode clustering and the regular node clustering mutually induce
each other. In this way, the clustering of the original graph can be solved by clustering the
bipartite graph [LWDH13].
The most common approach to sparsifying the original aﬃnity matrix with the t-nearest
neighbor approach together with the sparse eigen-solver decomposition of the Laplacian
matrix, has been recently formulated for parallel distributed processing under the MPI and
the MapReduce programming models [CSBL11]. In short:
• The sparse similarity matrix construction using t-nearest neighbors entails three steps:
a) distance computation among all the data points, b) the modiﬁcation of the sparse
matrix to a symmetric matrix, and c) the similarity computations using distances. All
three steps are implemented using MapReduce as follows:
a) Having n data points and a distributed environment with p processors, n/p rows
of the distance matrix are constructed at each node. In the map phase, interme-
diate keys/values are created so that every n/p data points have the same key. In
the reduce phase, these n/p data points, referred to as local points, are loaded to
the memory of a node and the distances between the points of the whole dataset
and the local points are estimated. n/p max heaps are also used to store a local
data points t-nearest neighbors.
b) To make the sparse distance matrix symmetric, the symmetric elements of the
matrix are assigned the same value. In the map phase, two key/value pairs are
generated for each non-zero element in the sparse distance matrix. The ﬁrst key
is the row ID of the element and the corresponding value is the column ID and
the distance, while the second key is the column ID and the corresponding value
is the row ID and the distance. In the reduction phase, elements having the
same key (that correspond to values in the same row of the distance matrix) are
collected. After symmetrization, each row contains at most 2t non-zero elements.
Since t ≪n, the resulting symmetric matrix is still sparse.
c) The Gaussian similarity between distances is estimated in a separate MapReduce
step that self-tunes the scaling parameter σ, which controls how rapidly the
similarity between two data points reduces with the distance between the data
points. The estimate of the scaling parameter for each data point with t-nearest
neighbors is usually deﬁned as the average of t distances or as the median value
of each row of the sparse similarity matrix. This estimate is performed in the
map phase, while each reduce function obtains a row and all parameters.
• Among the various parallel eigen-decompositions of the sparse similarity matrix, a
variant of the Lanczos/Arnoldi factorization ARPACK, called PARPACK, is consid-
ered [MS96, CSBL11]. The parallel implementation is based on the MPI programming
model, where each MPI node stores n/p rows of the graph Laplacian matrix. Similarly,
the eigen-vector matrix, estimated in the m-step of the Arnoldi factorization by calling
ARPACK, is split into p partitions with n/p rows. The major communication overhead
between the nodes resides to the estimate of the parallel sparse matrix-vector product
of the Arnoldi factorization, since each node should dispatch its matrix rows to the
other nodes. This task is performed by a gathering operation in MPI (MPI AllGather)
which is based on the recursive doubling algorithm [TRG05].
• The k eigen-vectors of the Laplacian matrix estimated by the parallel eigen-solver are

Semantic Model Adaptation for Evolving Big Social Data
373
distributedly stored, thus the corresponding normalized matrix can be computed in
parallel and stored on p local machines. Each row of the normalized matrix is treated
as a data point in the parallel k-means algorithm, which is implemented using MPI.
In more detail, for the parallel k-means algorithm the master node randomly chooses
a point (row of the normalized matrix) as the ﬁrst cluster center and it broadcasts it
to all the nodes. Then, each node identiﬁes the most orthogonal point to this center
by estimating the cosine similarity between its local points and the center. The p
minimal cosine similarities are collected and the most orthogonal point to the ﬁrst
center is selected as the second center. This procedure is iteratively executed in order
to produce k centers. After the deﬁnition of the k initial centers, each node assigns its
local data to clusters and estimates local squared error function for each cluster. The
master node receives the sum of all points in each cluster, calculates the new centers,
and broadcasts them to all the nodes implemented by a MPI AllReduce operation.
11.5.4
Distributed Tensor Decomposition
Assuming a three-way tensor X ∈RI1×I2×I3, the PARAFAC decomposition of the tensor
into R components is expressed by means of a triplet of matrices A ∈RI1×R, B ∈RI2×R,
and C ∈RI3×R together with normalization factor vector λ ∈RR×1. Alternating Least
Squares is the most popular algorithm for PARAFAC decomposition and entails three main
steps. Each one of these steps is a conditional update of one of the three factor matrices
given the other two followed by a normalization step of the columns of the updated matrix.
These conditional updates are expressed by the equations [KPHF12]:
A
=
X(1) (C ⊙B) (CT C ∗BT B)†
(11.5.2)
B
=
X(2) (C ⊙A) (CT C ∗AT A)†
(11.5.3)
C
=
X(3) (B ⊙A) (BT B ∗AT A)†
(11.5.4)
where ⊙is the Khatri-Rao product, † is the pseudo-inverse of the corresponding matrix,
∗is the Hadamard product, and X(1), X(2) and X(3) are the mode-1, mode-2 and mode-3
unfolding matrices of tensor X, respectively.
Large scale tensor decomposition, namely the PARAFAC decomposition, has been re-
cently formulated in a parallel distributed framework using the MapReduce programming
model. The new algorithm, called GIGATENSOR, exploits the sparseness of the real word
tensors and redesigns the tensor decomposition algorithm in a way that avoids the interme-
diate data explosion problem [KPHF12]. GIGATENSOR focuses on making the estimates
of the update rules in (11.5.2)-(11.5.4) more computationally eﬃcient. To achieve this, the
optimal ordering of computations is taken into consideration in order to minimize the num-
ber of ﬂoating point operations (ﬂops) needed. This ordering is deﬁned for the updating
rule of A as follows:
M1
=
X(1) (C ⊙B)
(11.5.5)
M2
=
(CT C ∗BT B)†
(11.5.6)
A
=
M1M2.
(11.5.7)
Similar equations hold for the orderings of B and C.
In order to deal with the intermediate data explosion problem entailed in the Khatri-Rao
product in (11.5.5), an equivalent algebraic computation is performed that decouples the

374
Graph-Based Social Media Analysis
two terms in the product [KPHF12]:
N1
=
X(1) ∗(1I1 ◦(C(:, r)T ) ⊗1T
I2))
(11.5.8)
N2
=
(bin(X(1)) ∗(1I1 ◦(1T
I3 ⊗C(:, r)T ))
(11.5.9)
M1
=
(N1 ∗N2) 1I2I3,
(11.5.10)
where 1 is a vector of ones with the size denoted in the subscript, ◦is the outer product,
⊗denotes the Kronecker product, and bin( ) is a function that converts any non-zero value
into 1, preserving sparsity. Moreover, to make the computations in (11.5.6) more eﬃcient,
CT C and BT B are computed separately as the sum of outer products of the rows, i.e.:
CT C =
I3
X
k=1
C(k, :)T ◦C(k, :)
BT B =
I2
X
k=1
B(k, :)T ◦B(k, :),
(11.5.11)
where C(k, :) and B(k, :) are the kth row of the matrix C and B, respectively. The ﬁnal
result is the obtained by performing the Hadamard product of the two resulting matrices
of (11.5.11).
The three steps described in (11.5.5-11.5.7) are implemented in the MapReduce frame-
work. To begin with, for (11.5.5) three diﬀerent MapReduce algorithms are formulated that
implement (11.5.8-11.5.10). In the mapping phase of the ﬁrst MapReduce algorithm, each
element of the mode-1 matrix X(1)(i, j) is mapped on ⌈j/I2⌉and each element of the factor
C(j, r) is mapped on j, such that tuples sharing the same key are shuﬄed to the same re-
ducer. In the reducing face, the reducer takes the tuples of C(j, r) and of the positive values
of X(1)(i, j), joins them for Hadamard product and emitsX(1)(i, j) C(j, r) for every i for
which the corresponding X(1)(i, j) is greater than zero. A similar task is performed in the
second MapReduce algorithm for (11.5.9). That is, in the mapping phase each element of
X(1)(i, j) is mapped on ⌈j/I2⌉and each element of the factor B(j, r) is mapped on j, while
in the reducing phase the reducer takes the tuples of B(j, r) and emits the corresponding
values of B(j, r) for every i for which the corresponding X(1)(i, j) is greater than zero.
The third algorithm combines the outputs of the ﬁrst and the second algorithms using the
Hadamard product, and sums up each row to get the ﬁnal result. In the mapping phase,
X(1)(i, j) C(j, r) and B(j, r) are mapped on i, such that tuples with the same i are shuﬄed
to the same reducer. In the reducing phase, the reducer takes X(1)(i, j) C(j, r) B(j, r) for
every j for which the corresponding X(1)(i, j) is greater than zero and emits the column-sum
PI2I3
j=1 X(1)(i, j) C(j, r) B(j, r).
The implementation of (11.5.6) in the MapReduce framework is performed by exploiting
(11.5.11). Each factor matrix C (B) is partitioned row-wise and is mapped on 0 so that
all the output is shuﬄed to the only reducer, which together with a combiner takes the
outer product PI3
k=1 C(k, :)T ◦C(k, :) (PI3
k=1 B(k, :)T ◦B(k, :)) and emits the corresponding
sum of these products. For the computation of the last step (11.5.7), only one MapReduce
job is required, since the distributed cache multiplication [KMF11] is used in order to
broadcast the second matrix (CT C∗BT B)† to all the mappers that process the ﬁrst matrix
X(1) (C ⊙B), performing then join in the ﬁrst matrix [KPHF12].

Semantic Model Adaptation for Evolving Big Social Data
375
11.6
Applications to Evolving Social Data Analysis
11.6.1
Incremental Label Propagation
Label propagation is widely used to reveal communities, i.e., local structure modules, in
large real-world networks. In the basic Label Propagation Algorithm (LPA), each node is
initialized with a unique label. At every iteration, each node is assigned the label shared by
most of its neighbors with ties broken uniformly at random [RAK07]. As the labels propagate
through the network, densely connected groups of nodes form a consensus on their labels.
At the end, the nodes having the same labels are grouped together as communities. Given
a simple undirected graph G = G(E, V) with V = {v1, . . . , v|V |} being the set of nodes, E
denoting the set of edges, and cv(t) representing the community label of each node v ∈V
at iteration t, the LPA is summarized as follows [RAK07]:
1. Initialize the labels at all nodes in the network. For a given node v, cv(0) = v. Set
t = 1.
2. Arrange the nodes in the network in a random order and set it to V.
3. For each v ∈V chosen in that speciﬁc order, let vi, i = 1, ..., k be its neighbors, and
cv(t) = f(v1(t), v2(t), . . . , vj(t), vj+1(t −1), . . . , vk(t −1)). Function f(·) returns the
label occurring with the highest frequency among the neighbors of v. Any ties are
broken uniformly. It is seen that the labels of the ﬁrst j neighbors of v have already
been updated, while the labels of the remaining k−j neighbors have not been updated
yet.
4. If every node has a label the same as the majority of its neighbors, then the algorithm
stops. Else, set t = t + 1 and go to 3.
Step 3 of the just described algorithm is essentially an asynchronous label updating. Syn-
chronous updating, where each node v at each iteration updates its label based on the labels
of its neighbors at the previous iteration, is usually avoided, since network subgraphs with
bi-partite or nearly bi-partite structure can lead to oscillations of the labels.
The LPA algorithm has been proven to work well for static networks. For evolving
networks, Incremental LPA (ILPA) is recommended that deals with the network changes
incrementally [PCW09]. That is, when a new node (edge) joins or an old node (edge) leaves
the network, the algorithm is executed locally instead of globally. To achieve this, the time
domain is discretized into time intervals of the same length. At each iteration, only the
nodes (edges) that changed at the previous iteration are considered. The algorithm is run
locally and iteratively until no labels can be changed. The ILPA is described as follows:
1. For each edge added at time interval t, the two nodes v1, v2, incident to the edge are
labeled as c(t)
v1 and c(t)
v2 , respectively. These two nodes are recorded as the new labeled
nodes.
2. All new labeled nodes and their neighbors are added to the local node calculation
sequence Vl in a random order.
3. The original LPA is sequentially applied to each node in Vl. Similarly, nodes whose
labels are changed will be recorded as new labeled nodes. If two or more diﬀerent
labels have the same number of neighbors, the most recent label is selected (i.e., the
label of the neighbor with greater t).

376
Graph-Based Social Media Analysis
4. Iterate Steps 2 and 3 until no labels are changed.
Each iteration of the LPA algorithm has a near-linear time complexity O(m) in the number
of edges. In the case of ILPA, when a new edge is added, the labels of the two vertices which
are adjacent to the edge will be updated. The vertices of the new-added edges and their
neighbor vertices are added into local random calculation sequence Vl and each iteration of
the ILPA algorithm has a near-linear time complexity O(ml) in the number of local edges
connected to the local vertices [PCW09].
11.6.2
Incremental Graph Clustering in Dynamic Social Networks
Graph clustering aims to partition the graph into several densely connected components
based on various criteria, such as vertex connectivity or neighborhood similarity [ZCX10].
That is, the existing graph clustering methods mainly focus on the topological structure
of a graph based on various criteria, such as normalized cut [SM00], modularity [NG04],
structural density [XYFS07], or stochastic ﬂows [SP09], so that each partition achieves a
cohesive internal structure. Contrary to these methods that ignore the vertex attributes,
graph summarization ignores the intra-cluster topological structures by exploiting the at-
tribute similarity so that nodes with the same attribute values are grouped into one partition
[YHP08].
Structural and attribute similarities are combined in SA-Cluster algorithm through a
uniﬁed distance measure, which is based on a neighborhood random walk model estimated
on the attribute augmented graph [ZCY09]. The attribute augmented graph is constructed
by adding to the original graph a set of attribute vertices and attribute edges that connect
attribute vertices sharing a common attribute value. Clustering is performed by following
the k-Medoids framework with the random walk distance as the vertex similarity measure.
The edge weights are iteratively adjusted to balance the importance between structural
and attribute similarities imposing the recalculation of the corresponding random walk
distances. The eﬃciency and scalability of the SA-Cluster algorithm is improved by the
incremental algorithm Inc-Cluster that incrementally updates the random walk distances
given the edge weight increments [ZCX10]. Both algorithms, SA-Cluster and its incremental
counterpart (Inc-Cluster), are described in detail, next.
Let G = (V, E, A) be an attributed graph with V set of vertices v, set of edges E and
A = {a1, . . . , am} be the set of m attributes associated with the vertices. A vertex v ∈V is
associated with an attribute vector [a1(v), . . . , am(v)], where ai(v) is the i-th attribute value
of vertex v. A vertex v ∈V is called a structure vertex and an edge (vi, vj) ∈E is called a
structure edge. The domain of attribute ai is denoted as Dom(ai) = {ai1, . . . , aini} having
size |Dom(ai)| = ni. An attribute augmented graph is denoted as Ga = (V ∪Va, E ∪Ea)
where Va = {vij}m, ni
i=1,j=1 is the set of attribute vertices and Ea ⊆V × Va is the set of
attribute edges. An attribute vertex vij ∈Va represents that attribute ai takes the j-th
value. An attribute edge (vi, vjk) ∈Ea, iﬀaj(vi) = ajk, i.e., vertex vi takes the value of ajk
on attribute aj. Examples of an attributed graph and an attribute augmented graph are
illustrated in Figure 11.6.1.
The uniﬁed neighborhood random walk distance matrix RA of length L is deﬁned as:
RA =
L
X
l=1
c (1 −c)l Pl
A,
(11.6.1)
where c ∈(0, 1) is the random walk restart probability and PA is the |V ∪Va| × |V ∪Va|
transition probability matrix of the attribute augmented graph Ga. The ﬁrst |V | rows
(columns) correspond to the structure vertices and the last |Va| rows (columns) correspond
to the attribute vertices.

Semantic Model Adaptation for Evolving Big Social Data
377
r1, Sports, 25-30 
r6, News, 35-40 
r7, News, 25-30 
r4, Sports, 35-40 
r5, Sports & 
      News, 40-45 
r2, Sports, 30-35 
r3, Sports, 25-30 
r8, News, 40-45 
r9, Sports, 30-35 
r8, Sports, 35-40 
r9, Sports, 25-30 
(a)
r1, Sports, 25-30 
r6, News, 35-40 
r7, News, 25-30 
r4, Sports, 35-40 
r5, Sports & 
      News, 40-45 
r2, Sports, 30-35 
r3, Sports, 25-30 
r8, News, 40-45 
r9, Sports, 30-35 
r8, Sports, 35-40 
r9, Sports, 25-30 
v11, Sports  
v12, News  
(b)
FIGURE 11.6.1: Graph examples for an article reader network with Two Attributes, “Topic”
and “Age” [ZCX10]: a) Attributed graph and b) Attribute augmented graph: two attribute
vertices v11 and v12 representing the topics “News” and “Sports” are added. Dashed lines
connect authors with corresponding topics to the two vertices respectively. Attribute vertices
and edges corresponding to the age attribute are omitted for the sake of clear presentation.
(See color insert.)
Given the |V | × |V | matrix PV1 holding the transition probabilities p(vi, vj) between
structure vertices vi and vj through a structure edge (vi, vj) ∈E, the |V | × |Va| matrix
A1 holding the transition probabilities p(vi, vjk) between structure and attribute vertices
through an attribute edge (vi, vjk) ∈Ea, the |Va| × |V | matrix B1 holding the transition
probabilities between attribute and structure vertices through an attribute edge (vik, vj) ∈
Ea, and O the |Va| × |Va| zero matrix corresponding to the transition probabilities between
attribute vertices p(vik, vjq), PA is partitioned as:
PA =
"
PV1
A1
B1
O
#
.
(11.6.2)
Assuming, without loss of generality, that a structure edge has a ﬁxed weight w0 and the
attribute edges corresponding to a1, . . . , am have edge weights w1, . . . , wm, respectively, the
corresponding transition probabilities are given by:
p(vi, vj)
=
(
w0
|N(vi)|w0+w1+...+wm ,
if (vi, vj) ∈E
0,
otherwise
(11.6.3)
p(vi, vjk)
=
(
wj
|N(vi)|w0+w1+...+wm ,
if (vi, vjk) ∈Ea
0,
otherwise,
(11.6.4)
p(vik, vj)
=
(
1
|N(vik)|,
if (vik, vj) ∈Ea
0,
otherwise,
(11.6.5)
where N(·) represents the set of structure vertices connected to the vertex within parenthe-
sis. The transition probability between attribute vertices is 0, since there is no edge between
attribute vertices, i.e.:
p(vik, vjq) = 0,
∀vik, vjq ∈Va.
(11.6.6)

378
Graph-Based Social Media Analysis
The SA-Cluster algorithm is based on the k-Medoids clustering framework. At the be-
ginning of the clustering process, the cluster centroids are initialized and the random walk
distance is estimated. Then, the following four steps are repeated until convergence.
1. Assign the vertices to their closest centroids.
2. Update the cluster centroids.
3. Adjust attribute edge weights {w1, . . . , wm}.
4. Re-calculate the random walk distance matrix RA.
Steps 3-4 are added on the top of the traditional k-Medoids (Steps 1-2). At each iteration,
the attribute edge weights {w1, . . . , wm} are automatically adjusted to reﬂect the clustering
tendencies of diﬀerent attributes. Accordingly, the transition probability matrix PA and
the neighborhood random walk distance matrix RA change. As a result, the random walk
distance matrix has to be re-calculated at each iteration due to the edge weight changes.
The incremental approach aims to reduce the number of random walk distance estimates
based on the observation that only the transition probabilities of the attribute edges and not
those of the structure edges are aﬀected by the attribute weight adjustments. This implies
that many elements in the random walk distance matrix remain unchanged. Thus, given the
original random walk distance matrix RA and the weight increments {∆w1, . . . , ∆wm}, the
incremental algorithm estimates the increment matrix ∆RA, and then the updated random
walk distance matrix RN,A = RA + ∆RA is obtained. Since RA is the weighted sum of a
series of matrices Pl
A, where Pl
A is the l-th power of the transition probability matrix PA,
l = 1, . . . , L, to compute ∆RA one needs to estimate ∆Pl
A for diﬀerent l values. Therefore,
the problem lies in computing the increment matrix ∆Pl
A given the original matrix Pl
A and
the edge weight increments {∆w1, . . . , ∆wm}.
Given that Pm
i=1 wi = m and w0 is ﬁxed, from (11.6.3)–(11.6.6) it is seen that only
the transition probabilities in the submatrix A1 are aﬀected by the attribute weight incre-
ments, leaving the other three submatrices unchanged. Thus, the increment of the transition
probability matrix ∆P1
A is denoted as:
∆P1
A =
"
0
∆A1
0
0
#
.
(11.6.7)
Let w′
j = wj + ∆wj be the new weight. The probability increment (11.6.4) is given by
[ZCX10]:
∆p(vi, vjk) = ∆wj p(vi, vjk).
(11.6.8)
Denoting by Aai the |V| × ni matrix of the transition probabilities from structure vertices
in V to attribute vertices corresponding to attribute ai, then the transition probabilities
submatrix can be written as A1 = [Aa1| . . . |Aan]. That is, the element Ai(p, q) represents
the transition probability from the p-th structure vertex vp ∈V to the q-th attribute value
aiq of ai. Thus, ∆A1 is equal to:
∆A1 = [∆w1 Aa1| . . . |∆wm Aam]
(11.6.9)
and the new transition probability matrix PN,A after the edge weight change is represented
as:
PN,A =
"
PV1
A1 + ∆A1
B1
0
#
=
"
PV1
AN,1
B1
0
#
.
(11.6.10)
The computation of ∆Pl
A is based on the representation of the original l-th power ma-
trix Pl = Pl−1 PA and the new matrix Pl
N,A = Pl−1
N,A PA given the weight increments

Semantic Model Adaptation for Evolving Big Social Data
379
{∆w1, . . . , ∆wm}. Thus, the l-th power transition probability matrix increment ∆Pl
A is
given by [ZCX10]:
∆Pl
A
=
Pl
N,A −Pl
A =
"
∆PVl
∆Al
∆Bl
∆Cl
#
(11.6.11)
=
"
∆PVl−1PV1 + ∆Al−1B1
PVl−1∆A1 + ∆PVl−1AN,1
∆PBl−1PV1 + ∆Cl−1B1
Bl−1∆A1 + ∆Bl−1AN,1
#
,
where ∆A1 is given by (11.6.9).
The steps of the incremental algorithm Inc-Cluster are summarized in Algorithm 11.6.1.
Algorithm 11.6.1: Inc-Cluster algorithm [ZCX10]
Input: Original matrices RA, PA, Al, and the attribute edge weight increments
{∆w1, . . . , ∆wm}.
Output: The new random walk distance matrix RN,A.
1. Estimate ∆PA1 according to Eq. (11.6.8).
2. Estimate the increment uniﬁed random walk distance matrix ∆RA = c (1 −c) ∆Pl
A.
3. For each l = 2, . . . , L:
a. Estimate ∆Pl
A according to Eq. (11.6.11).
b. Estimate ∆RA = ∆RA + c (1 −c)l ∆Pl
A.
4. Estimate RN,A = RA + ∆RA.
11.7
Conclusions
In this chapter, state-of-the-art methods for big social network analysis have been sur-
veyed and their corresponding dynamic alternatives for capturing the network dynamics
have been studied. Moreover, their optimized counterparts that eﬃciently handle large vol-
umes of data have also been outlined. These methods span latent model adaptation, tensor
model adaptation, and spectral clustering. Some representative applications entailing social
data analysis have also been described, namely incremental label propagation and incre-
mental graph clustering on an attribute augmented graph.
Latent models have the ability to reveal the latent information inherent in the data.
Three models have been studied, namely the LSA, the PLSA and the LDA. The incremental
LSA methods build on the incremental approaches of the PSVD decomposition (i.e., SVD
folding-in, SVD updating). PLSA methods include the PLSA folding-in, the incremental
PLSA, the QB PLSA and MAP PLSA, and the oPLSA. The PLSA folding-in exploits an
incremental variant of the EM algorithm. The incremental PLSA is based on the asymmetric
and the symmetric formulations of PLSA folding-in. A Bayesian PLSA framework with a
Dirichlet density kernel prior is assumed in the QB PLSA and the MAP PLSA methods. In
the oPLSA, the PLSA model parameters are updated by deriving from ﬁrst principles the
corresponding probability estimates between successive EM steps. The incremental PLSA

380
Graph-Based Social Media Analysis
and the oPLSA are formulated on a moving window framework allowing both addition and
deletion of new data (i.e., documents and words), while oPLSA additionally handles out-of-
vocabulary words in a systematic way. The LDA updating methods vary depending on the
optimization assumption or the inference assumption made in order to compute the posterior
distribution of the LDA generative model. The online LDA, that resorts to Variational
Bayesian Inference methods, is based on an online stochastic optimization that can also be
applied on mini-batches of data. Considering Markov Chain Monte Carlo based methods,
the o-LDA uses batch LDA (i.e., batch Gibbs sampler) to get initial estimates on an initial
data subset and then uses collapsed Gibbs sampling on previous data. o-LSA may result in
poor inference, when the initialization of LDA is not based on a representative subset (i.e.,
documents) of the full dataset. Skipping the batch initialization phase, incremental LDA
produces more robust estimates by entailing resampling by means of the incremental Gibbs
sampler.
Spectral clustering methods detect the structure of the data distributions by exploiting
the information inherent in the data relationships (expressed in terms of similarity). In an
evolving data framework, incremental spectral methods have been described that avoid re-
computations by simulating the change of eigensystem or by extracting representative points
to compress the dataset. An incremental spectral clustering method simulates the change
of eigensystem and updates the eigenvectors of the generalized eigenproblem by ﬁnding the
derivatives on the eigenvalues/vectors with respect to perturbations in all the quantities
involved. A self-adaptive incremental spectral clustering method is also built on an iterative
procedure that works on a few representative points for each cluster. This method pro-
vides the ability to increase the clusters number by applying a similarity threshold criterion
between the cluster representative and any points in the cluster. In Incremental Approxi-
mate Spectral Clustering, the positive semi-deﬁnite shifted Laplacian is approximated with
eigenupdate methods based on the SVD updating.
Tensor models can be eﬃciently used to represent the multi-faceted multi-dimensional
nature of social data. For evolving social data, the incremental tensor analysis framework
has been presented that entails dynamic, streaming, and window-based tensor analysis.
In dynamic tensor and window-based analyses, an incremental update of the covariance
matrices is used, while in streaming tensor analysis an incremental update of projection
matrices is employed by means of the SPIRIT algorithm. In dynamic tensor and streaming
tensor methods, a forgetting factor that fades away the variance matrices of earlier steps is
employed. This is not the case in the window-based tensor methods, which entail a sliding
window with the same weight to all timestamps in the window in order to handle time
changes.
All the aforementioned methods (i.e., latent variable models, spectral clustering, and
tensor models) and their incremental solutions can eﬃciently handle big data. However,
approaches other than the incremental ones that resort to optimization techniques of the
baseline algorithms have been formulated and implemented within a parallel distributed
framework may be more suitable and eﬃcient for processing big data, especially when
online-processing is not the main issue. Optimization techniques have mainly been pro-
posed for spectral clustering, including sparsiﬁcation of the aﬃnity matrix, approximate
solutions for eigen-decomposition, and data dimensionality reduction by means of cluster-
ing or random projection. Latent variable models and tensors can eﬃciently handle big data
by means of parallelization, where the baseline algorithms are formulated for paralllel pro-
gramming models (i.e., MPI, OpenMP and MapReduce) that are widely used in distributed
environments.
Finally, the incremental counterparts of the Label Propagation Algorithm (LPA), which
is widely used to reveal communities, and of the SA-Cluster algorithm, which exploits the
attribute augmented graph, have been outlined. Original LPA is a global algorithm that

Semantic Model Adaptation for Evolving Big Social Data
381
assigns to each node the label shared by most of its neighbors, while its incremental variant,
Incremental LPA, treats the problem locally by discretizing the time domain into equal sized
time intervals and taking into consideration only the nodes (edges) that change between
two time intervals. The SA-Cluster algorithm combines both structural and attribute graph
similarities by exploiting a neighborhood random walk model on the attribute augmented
graph. Its variant, Inc-Cluster, incrementally updates the random walk distances given the
edge weight increments by dividing the transition probability matrix into submatrices and
incrementally updates each one.
Acknowledgment
This research has been co-ﬁnanced by the European Union (European Social Fund -
ESF) and Greek national funds through the Operation Program “Education and Lifelong
Learning” of the National Strategic Reference Framework (NSRF) - Research Funding Pro-
gram: THALIS-UOA-ERASITECHNIS MIS 375435.
Bibliography
[AMS01]
D. Achlioptas, F. McSherry, and B. Sch¨olkopf. Sampling techniques for kernel
methods. In Proc. Advances in Neural Information Processing Systems, pages
335–342, 2001.
[ASW08]
A. Asuncion, P. Smyth, and M. Welling. Asynchronous distributed learning
of topic models. In Proc. Advances in Neural Information Processing Systems,
pages 81–88, 2008.
[Att00]
H. Attias. A variational Bayesian framework for graphical models. In Proc.
Advances in Neural Information Processing Systems, pages 209–215, 2000.
[AWST09]
A. Asuncion, M. Welling, P. Smyth, and Y.W. Teh. On smoothing and infer-
ence for topic models. In Proc. 25th Conference on Uncertainty in Artiﬁcial
Intelligence, pages 27–34, 2009.
[BB07]
A. Banerjee and S. Basu. Topic models over text streams: a study of batch and
online unsupervised learning. In Proc. 7th SIAM International Conference on
Data Mining, pages 431–436, 2007.
[BB08]
L. Bottou and O. Bousquet. The tradeoﬀs of large scale learning. In Proc.
Advances in Neural Information Processing Systems, pages 161–168, 2008.
[BDO94]
M. W. Berry, S. T. Dumais, and G. W. O’Brien.
Using linear algebra for
intelligent information retrieval. Technical Report UT-CS-94-270, 1994.
[BHK98]
J. S. Breese, D. Heckerman, and C. Kadie. Empirical analysis of predictive
algorithms for collaborative ﬁltering. In Proc. 14th Conference on Uncertainty
in Artiﬁcial Intelligence, pages 43–52, 1998.

382
Graph-Based Social Media Analysis
[BK11]
N. Bassiou and C. Kotropoulos. Long distance bigram models applied to word
clustering. Pattern Recognition, 44(1):145–158, 2011.
[BK14]
N. Bassiou and C. Kotropoulos.
Online PLSA: Batch updating techniques
including out-of-vocabulary words.
IEEE Transactions on Neural Networks
and Learning Systems, 25(11):1953–1966, 2014.
[BNJ03]
D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. Journal
of Machine Learning Research, 3(5):993–1022, 2003.
[Bra05]
T. Brants.
Test data likelihood for PLSA models.
Information Retrieval,
8(2):181–196, 2005.
[BTHC06]
T. H. Brants, I. Tsochantaridis, T. Hofmann, and F. R. Chen. Methods, ap-
paratus, and program products for performing incremental probabilistic latent
semantic analysis. Patent No. 20060112128, 2006.
[CC08]
T. C. Chou and M. C. Chen. Using incremental PLSI for threshold-resilient
online event analysis. IEEE Transactions on Knowledge and Data Engineering,
20(3):289–299, 2008.
[CC11]
X. Chen and D. Cai.
Large scale spectral clustering with landmark-based
representation. In Proc. 26th Conference on Artiﬁcial Intelligence, pages 313–
318, 2011.
[CDW13]
W. M. Campbell, C. K. Dagli, and C. J. Weinstein. Social network analysis
with content and graphs. Lincoln Laboratory Journal, 20(1):62–81, 2013.
[CGL+06]
B. Chen, B. Gao, T.-Y. Liu, Y.-F. Chen, and W.-Y. Ma. Fast spectral cluster-
ing of data using sequential matrix compression. In Proc. 7th European Conf.
Machine Learning (ECML’06), volume 4212 of Lecture Notes in Computer Sci-
ence, pages 590–597. Springer, 2006.
[CH00]
D. Cohn and T. Hofmann. The missing link – a probabilistic model of document
content and hypertext connectivity. In Proc. Advances in Neural Information
Processing Systems, 2000.
[Chu97]
F. R. K. Chung. Spectral Graph Theory, volume 92. American Mathematical
Society, 1997.
[CMD+00] R. Chandra, R. Menon, L. Dagum, D. Kohr, D. Maydan, and J. McDonald.
Parallel Programming in OpenMP. Morgan Kaufmann, 2000.
[CSBL11]
W. Y. Chen, Y. Song, H. Bai, and C. J. Lin. Parallel spectral clustering in
distributed systems.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 33(3):568–586, 2011.
[CSG09]
K. Canini, L. Shi, and T. Griﬃths.
Online inference of topics with latent
Dirichlet allocation. In Proc. Internatinal Conference on Artiﬁcial Intelligence
and Statistics, volume 5, pages 65–72, 2009.
[CW08]
J. T. Chien and M. S. Wu. Adaptive Bayesian latent semantic analysis. IEEE
Transactions on Audio, Speech, and Language Processing, 16(1):198–207, 2008.
[CY05]
H. Chang and D. Yeung. Robust path-based spectral clustering with application
to image segmentation. In Proc. International Conference on Computer Vision,
pages 278–285, 2005.

Semantic Model Adaptation for Evolving Big Social Data
383
[CZG09]
J. Chen, O.R. Zaiane, and R. Goebel. A visual data mining approach to ﬁnd
overlapping communities in networks. In Proc. International Conference on
Advances in Social Networks Analysis and Mining, pages 338–343, 2009.
[DDF+90]
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman.
Indexing by latent semantic analysis. Journal American Society Information
Science, 41(6):391–407, 1990.
[DG08]
J. Dean and S. Ghemawat. MapReduce: Simpliﬁed data processing on large
clusters. ACM Communications, 51(1):107–113, 2008.
[DGC14]
C. Dhanjal, R. Gaudel, and S. Cl´emencon. Eﬃcient eigen-updating for spectral
graph clustering. Neurocomputing, 131:440–452, 2014.
[DGR07]
A. S. Das, M. Datarand A. Garg, and S. Rajaram. Google news personalization:
Scalable online collaborative ﬁltering. In Proc. 16th International Conference
on World Wide Web, pages 271–280, 2007.
[Din04]
C. Ding. A tutorial on spectral clustering. In Proc. International Conference
Machine Learning, 2004.
[DK04]
M. Deshpande and G. Karypis.
Item-based top-n recommendation.
ACM
Transactions on Information Systems, 22(1):143–177, 2004.
[DLR77]
A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete
data via the EM algorithm (with discussion). Journal Royal Statistical Society,
Series B, 39:1–38, 1977.
[FBCM04]
C. Fowlkes, S. Belongie, F. Chung, and J. Malik.
Spectral grouping using
the Nystr¨om method. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 26(2):214–225, 2004.
[FbS15]
Facebook newsroom: Statistics. http://newsroom.fb.com/company-info/, 2015
(accessed March 27, 2015).
[FLG00]
G. W. Flake, S. Lawrence, and C. L. Giles. Eﬃcient identiﬁcation of web com-
munities. In Proc. 2000 ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, pages 150–160, 2000.
[For10]
S. Fortunato. Community detection in graphs. Physics Reports, 486:75–174,
2010.
[GH99]
D. Gildea and T. Hofmann. Topic-based language models using EM. In Proc.
6th European Conference on Speech Communication and Technology, pages
2167–2170, 1999.
[GLMY11] U. Gargi, W. Lu, V. Mirrokni, and S. Yoon. Large-scale community detection
on youtube for topic discovery and exploration.
In Proc. 5th International
AAAI Conference on Weblogs and Social Media, pages 486–489, 2011.
[GN02]
M. Girvan and M. E. J. Newman. Community structure in social and biological
networks. Proc. National Academy of Sciences of the United States of America,
99(12):7821–7826, 2002.
[GS04]
T. L. Griﬃths and M. Steyvers. Finding scientiﬁc topics. National Academy of
Sciences of the USA, 101(1):5228–5235, 2004.

384
Graph-Based Social Media Analysis
[HBB10]
M. D. Hoﬀman, D. M. Blei, and F. Bach. Online learning for latent Dirich-
let allocation. In Proc. Advances in Neural Information Processing Systems,
volume 23, pages 856–864, 2010.
[HCZS08]
C. Hong, Y. Chen, W. Zheng, and J. Shan. Parallelization and characterization
of probabilistic latent semantic analysis. In Proc. 37th International Conference
on Parallel Computing, pages 628–635, 2008.
[HK92]
L. Hagen and A. B. Kahng. New spectral methods for ratio cut partitioning
and clustering. IEEE Transactions on Computer-Aided Design, 11(9):1074–
1085, 1992.
[Hof01]
T. Hofmann. Unsupervised learning by probabilistic latent semantic analysis.
Machine Learning, 42(1-2):177–196, 2001.
[Hof04]
T. Hofmann. Latent semantic models for collaborative ﬁltering. ACM Trans-
actions on Information Systems, 22(1):89–115, 2004.
[HP98]
T. Hofmann and J. Puzicha. Unsupervised learning from dyadic data. Technical
Report TR-98-042, International Computer Science Institute, 1998.
[IN14]
N. Ifada and R. Nayak. Tensor-based item recommendation using probabilistic
ranking in social tagging systems. In Proc. 23rd International Conference on
World Wide Web companion, pages 805–810, 2014.
[JB03]
Michael I. Jordan and Francis R. Bach. Learning spectral clustering. In Proc.
Advances in Neural Information Processing Systems 16, 2003.
[JGJS99]
M. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to varia-
tional methods for graphical models. Machine Learning, 37:183–233, 1999.
[JGS+11]
Y. Jin, Y. Gao, Y. Shi, L. Shang, R. Wang, and Y. Yang. P2LSA and P2LSA+:
Two paralleled probabilistic latent semantic analysis algorithms based on the
MapReduce model. In Proc. 12th International Conference on Intelligent Data
Engineering and Automated Learning, volume 6936, pages 385–393, 2011.
[JNCJ08]
H. Jiang, T.N. Nguyen, I. Chen, and H. Jaygarl. Incremental latent seman-
tic indexing for automatic traceability link evolution management. In Proc.
IEEE/ACM International Conference on Automated Software Engineering,
pages 59–68, 2008.
[JX06]
X. Ji and W. Xu. Document clustering with prior knowledge. In Proc. 29th
International ACM SIGIR Conference on Research and Development in Infor-
mation Retrieval, pages 405–412, 2006.
[JZM04]
X. Jin, Y. Zhou, and B. Mobasher. Web usage mining based on probabilistic
latent semantic analysis. In Proc. 2004 ACM SIGKDD Conference Knowledge
Discovery and Data Mining (KDD’04), pages 197–205, 2004.
[KBK05]
T. G. Kolda, B. W. Bader, and J. P. Kenny. Higher-order web link analysis
using multi-linear algebra. In Proc. 5th IEEE International Conference on Data
Mining, pages 242–249, 2005.
[KMF11]
U. Kang, B. Meeder, and C. Faloutsos. Spectral analysis for billion-scale graphs:
Discoveries and implementation. In Proc. 15th Paciﬁc-Asia Conference, volume
6635 of Lecture Notes in Computer Science, pages 13–25. Springer, 2011.

Semantic Model Adaptation for Evolving Big Social Data
385
[KPHF12]
U. Kang, E. Papalexakis, A. Harpale, and C. Faloutsos. Gigatensor: scaling
tensor analysis up by 100 times - algorithms and discoveries. In Proc. 18th
ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, pages 316–324, 2012.
[KPSC10]
I. A. Kov´acs, R. Palotai, M. S. Szalay, and P. Csermeley. Community land-
scapes: An integrative approach to determine overlapping network module
hierarchy, identify key nodes and predict network dynamics.
PLoS ONE,
5(9):e12528, 2010.
[KTS11]
T. Kong, Y. Tian, and H. Shen. A fast incremental spectral clustering for large
data sets. In Proc. IEEE International Conference on Parallel and Distributed
Computing, Applications, and Technologies (PDCAT), pages 1–5, 2011.
[LCSX11]
Y.-R. Lin, K. S. Candan, H. Sundaram, and L .Xie.
SCENT: Scalable
compressed monitoring of evolving multi-relational social networks.
ACM
Transactions on Multimedia Computing, Communications, and Applications,
75(1):29:1–29:22, 2011.
[LDv12]
M. Leginus, P. Dolog, and V. ˇZemaitis. Improving tensor based recommenders
with clustering. In User Modeling, Adaptation, and Personalization, volume
LNCS 7379, pages 151–163. Springer, Berlin / Heidelberg, 2012.
[LHLC09]
I. X. Y. Leung, P. Hui, P. Lio, and J. Crowcroft. Towards real-time community
detection in large networks. Physical Review E, (066107), 2009.
[LK09]
P. Liang and D. Klein. Online EM for unsupervised models. In Proc. of Human
Language Technologies: The 2009 Annual Conference of the North American
Chapter of the Association for Computational Linguistics, pages 611–619, 2009.
[LPV13]
H. Lu, K. Plataniotis, and A. Venetsanopoulos. Multilinear Subspace Learning:
Dimensionality Reduction of Multidimensional Data. Taylor and Francis, 2013.
[LSY03]
G. Linden, B. Smith, and J. York. Amazon.com recommendations: Item-to-item
collaborative ﬁltering. IEEE Internet Computing, pages 76–80, 2003.
[Lux07]
U. Von Luxburg. A tutorial on spectral clustering. Statistics and Computing,
17(4):395–416, 2007.
[LV00]
L. D. Lathauwer and J. Vandewalle. On the best rank-1 and rank-(r1, r2, . . . , rn)
approximation of higher-order tensors. SIAM Journal on Matrix Analysis and
Applications, 21(4):1324–1342, 2000.
[LWDH13]
J. Liu, C. Wang, M. Danilevsky, and J. Han. Large-scale spectral clustering on
graphs. In Proc. 23rd International Joint Conference on Artiﬁcial Intelligence,
2013.
[LY08]
N. N. Liu and Q. Yang. Eigenrank: a ranking-oriented approach to collaborative
ﬁltering. In Proc. 31st ACM SIGIR Conference on Research and Development
in Information Retrieval, pages 83–90, 2008.
[LYZ+07]
T.-Y. Liu, H.-Y. Yang, X. Zheng, T. Qin, and W.-Y. Ma.
Fast large-scale
spectral clustering by sequential shrinkage optimization. In Proc. 29th European
Conf. IR (ECIR07), volume 4425 of Lecture Notes in Computer Science:, pages
319–330. Springer, 2007.

386
Graph-Based Social Media Analysis
[LZCS11]
Z. Liu, Y. Zhang, E. Y. Chang, and M. Sun. PLDA+: Parallel latent dirichlet
allocation with data placement and pipeline processing. ACM Transactions on
Intelligent Systems and Technology, 2(3):26:1–26:18, 2011.
[MB88]
G. McLachlan and K. E. Basford. Mixture Models, volume 84. Marcel Dekker
Inc., 1988.
[MM07]
D. Mimno and A. McCallum. Organizing the OCA: Learning faceted subjects
from a library of digital books.
In Proc. ACM/IEEE Joint Conference on
Digital Libraries, pages 376–385, 2007.
[MS96]
K. Maschhoﬀand D. Sorensen. A portable implementation of ARPACK for dis-
tributed memory parallel architectures. In Proc. Copper Mountain Conference
on Iterative Methods, 1996.
[MTP10]
A. Maronidis, A. Tefas, and I. Pitas. Frontal view recognition using spectral
clustering and subspace learning methods. In Proc. International Conference
Artiﬁcial Neural Networks, volume 6352 of Lecture Notes in Computer Sci-
ence: Proc. International Conference on Artiﬁcial Intelligence, pages 460–469.
Springer, 2010.
[MZ04]
B. Marlin and R. Zemel. The multiple multiplicative factor model for collabo-
rative ﬁltering. In Proc. 21st International Conference on Machine Learning,
volume 69, pages 576–583, 2004.
[MZL+11]
H. Ma, D. Zhou, C. Liu, M. R. Lyu, and I. King.
Recommender systems
with social regularization. In Proc. 4th ACM International Conference on Web
Search and Data Mining, pages 287–296, 2011.
[NASW07] D. Newman, A. Asuncion, P. Smyth, and M. Welling. Distributed inference for
latent Dirichlet allocation. In Proc. Advances in Neural Information Processing
Systems, pages 1081–1088, 2007.
[NASW09] R. Newman, A. Asuncion, P. Smyth, and M. Welling. Distributed algorithms
for topic models. Journal of Machine Learning Research, 10:1801–1828, 2009.
[NCL07]
R. Nallapati, W. Cohen, and J. Laﬀerty. Parallelized variational EM for latent
dirichlet allocation: An experimental evaluation of speed and scalability. In
Proc. 7th IEEE International Conference on Data Mining Workshops, pages
349–354, 2007.
[New07]
M. E. J. Newman. Finding community structure in networks using the eigen-
vectors of matrices. Physical Review E, (036104), 2007.
[NG04]
M. E. J. Newman and M. Girvan. Finding and evaluating community structure
in networks. Physical Review E, (026113), 2004.
[NH98]
R. M. Neal and G. E. Hinton.
A view of the EM algorithm that justiﬁes
incremental, sparse, and other variants. In M. I. Jordan, editor, Learning in
Graphical Models, pages 355–368. Kluwer Academic Publishers, 1998.
[NJW01]
A. Ng, M. Jordan, and Y. Weiss.
On spectral clustering: Analysis and an
algorithm. In Proc. Advances in Neural Information Processing Systems, pages
849–856, 2001.

Semantic Model Adaptation for Evolving Big Social Data
387
[NXC+07]
H. Ning, W. Xu, Y. Chi, Y. Gong, and T. Huang. Incremental spectral clus-
tering with application to monitoring of evolving blog communities. In Proc.
SIAM International Conference on Data Mining, pages 261–272, 2007.
[NXC+10]
H. Ning, W. Xu, Y. Chi, Y. Gong, and T. S. Huang. Incremental spectral clus-
tering by eﬃciently updating the eigen-system. Pattern Recognition, 43(1):113–
127, 2010.
[O’B94]
G. W. O’Brien. Information management tools for updating an SVD-encoded
indexing scheme. Technical Report UT-CS-94-258, 1994.
[PCW09]
S. Pang, C. Chen, and T. Wei. A realtime clique detection algorithm: Time-
based incremental label propagation.
In Proc. International Conference on
Intelligent Information Technology Application, volume 3, pages 459–462, 2009.
[PDFV05]
G. Palla, I. Derenyi, I. Farkas, and T. Vicsek.
Uncovering the overlapping
community structure of complex networks in nature and society.
Nature,
435(7043):814–818, 2005.
[PKVS12]
S. Papadopoulos, Y. Kompatsiaris, A. Vakali, and P. Spyridonos. Community
detection in social media performance and application considerations. Springer
Data Mining and Knowledge Discovery, 24:515–554, 2012.
[PL05]
P. Pons and M. Latapy. Computing communities in large networks using ran-
dom walks. In Proc. Computer and Information Sciences, volume LNCS 3733,
pages 284–293, 2005.
[PM05]
W. Pentney and M. Meila. Spectral clustering of biological sequence data. In
Proc. National Conference on Artiﬁcial Intelligence (AAAI), volume 2, pages
845–850, 2005.
[PSF05]
S. Papadimitriou, J. Sun, and C. Faloutsos. Streaming pattern discovery in
multiple time-series.
In Proc. 31st International Conference on Very Large
Data Bases, pages 697–708, 2005.
[PZK04]
J. Park, H. Zha, and R. Kasturi. Spectral clustering for robust motion seg-
mentation. In Proc. European Conference on Computer Vision, pages 390–401,
2004.
[RAK07]
U. N. Raghavan, R. Albert, and S. Kumara. Near linear time algorithm to
detect community structures in large-scale networks. Physical Review E, pages
036106+, 2007.
[RD13]
D. Rafailidis and P. Daras. The TFC model: Tensor factorization and tag clus-
tering for item recommendation in social tagging systems. IEEE Transactions
on Systems, Man and Cybernetics, Part A: Systems and Humans, 43(3):673–
688, 2013.
[RIS+94]
P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and J. Riedl. Grouplens: An
open architecture for collaborative ﬁltering of netnews. In Proc. 1994 ACM
Conference on Computer Supported Cooperative Work, pages 175–186, 1994.
[RST10]
S. Rendle and L. Schmidt-Thieme. Pairwise interaction tensor factorization for
personalized tag recommendation. In Proc. 3rd ACM International Conference
on Web Search and Data Mining, pages 81–90, 2010.

388
Graph-Based Social Media Analysis
[SBH02]
G. Shani, R. Brafman, and D. Heckerman. An MDP-based recommender sys-
tem. In Proc. 18th Conference on Uncertainty in Artiﬁcial Intelligence, vol-
ume 6, pages 1265–1295, 2002.
[SFSZ05]
N. Speer, H. Fr¨ohlich, C. Spieth, and A. Zell. Functional grouping of genes using
spectral clustering and gene ontology. In Proc. International Joint Conference
on Neural Networks, pages 298–303, 2005.
[SI09]
T. Sakai and A. Imiya. Fast spectral clustering with random projection and
sampling.
In Proc. 3rd International Conference on Machine Learning and
Data Mining, volume 5632 of Lecture Notes in Computer Science, pages 372–
384. Springer, 2009.
[SKKR00]
B. Sarwar, G. Karypis, J. Konstan, and J. Reidl. Application of dimensionality
reduction in recommender systems – a case study. In Proc. ACM-SIGKDD
Conference on Knowledge Discovery in Databases, pages 265–285, 2000.
[SKKR01]
B. Sarwar, G. Karypis, J. Konstan, and J. Reidl.
Item-based collaborative
ﬁltering recommendation algorithms. In Proc. 10th International Conference
on World Wide Web, 2001.
[SLTS05]
X. Song, C.-Y. Lin, B. L. Tseng, and M.-T. Sun. Modeling and predicting
personal information dissemination behavior.
In Proc. 11th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, pages 479–
488, 2005.
[SM00]
J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, 22(8):888–905, 2000.
[SNM10]
P. Symeonidis, A. Nanopoulos, and Y. Manolopoulos. A uniﬁed framework for
providing recommendations in social tagging systems based on ternary semantic
analysis. IEEE Transactions on Knowledge and Data Engineering, 22(2):179–
192, 2010.
[SP09]
V. Satuluri and S. Parthasarathy. Scalable graph clustering using stochastic
ﬂows: Applications to community discovery.
In Proc. 2009 ACM SIGKDD
Conference on Knowledge Discovery and Data Mining, pages 737–745, 2009.
[SS08]
H. Shinnou and M. Sasaki. Spectral clustering for a large data set by reducing
the similarity matrix size. In Proc. 6th International Conference on Language
Resources and Evaluation, pages 201–204, 2008.
[STF06]
J. Sun, D. Tao, and C. Faloutsos. Beyond streams and graphs: Dynamic tensor
analysis. In Proc. 12th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 374–383, 2006.
[STPF08]
J. Sun, D. Tao, S. Papadimitriou, and C. Faloutsos. Incremental tensor analysis:
Theory and applications.
ACM Transactions on Knowledge Discovery from
Data, 2(3):11:1–11:37, 2008.
[TRG05]
R. Thakur, R. Rabenseinfer, and W. Gropp. Improving the performance of
collective operations in MPICH. International Journal of High Performance
Computing Applications, 19(1):49–66, 2005.

Semantic Model Adaptation for Evolving Big Social Data
389
[TS07]
J. E. Tougas and R. J. Spiteri. Updating the partial singular value decomposi-
tion in latent semantic indexing. Computational Statistics and Data Analysis,
52:174–183, 2007.
[Tuc66]
L. R. Tucker. Some mathematical notes on three-mode factor analysis. Psy-
chometrika, 31:279–311, 1966.
[Twi15]
Twitter usage. https://about.twitter.com/company, 2015 (accessed March 27,
2015).
[VDL07]
C. Valgren, T. Duckett, and A. Lilienthal. Incremental spectral clustering and
its application to topological mapping. In Proc. IEEE International Conference
on Robotics and Automation, pages 4283–4288, 2007.
[VM03]
D. Verma and M. Meila. A comparison of spectral clustering algorithms. Tech-
nical report, University of Washington, 2003.
[WAB12]
Y. Wang, E. Agichtein, and M. Benzi. TM-LDA: eﬃcient online modeling of
latent topic transitions in social media. In Proc. ACM SIGKDD Conference on
Knowledge Discovery and Data Mining, pages 123–131, 2012.
[WAM09]
R. Wan, V. N. Ahn, and H. Mamitsuka. Eﬃcient probabilistic latent seman-
tic analysis through parallelization. In Proc. 5th Asia Information Retrieval
Symposium on Information Retrieval Technology, pages 432–443, 2009.
[WBSC09]
Y. Wang, H. Bai, M. Stanton, and E. Chang. PLDA: Parallel latent dirichlet
allocation for large-scale applications. In Proc. International Conference on
Algorithmic Aspects in Information and Management, volume 5564 of Lecture
Notes in Computer Science, pages 301–314. Springer, 2009.
[WC89]
Y. C. Wei and C. K. Cheng. Towards eﬃcient hierarchical designs by ratio cut
partitioning. In Proc. International Conference on Computer Aided Design,
pages 298–301, 1989.
[Wei99]
Y. Weiss. Segmentation using eigenvectors: A unifying view. In Proc. Interna-
tional Conference on Computer Vision, volume 2, pages 975 – 982, 1999.
[WYL+09] X. Wu, J. Yan, N. Liu, S. Yan, Y. Chen, and Z. Chen. Probabilistic latent
semantic user segmentation for behavioral targeted advertising. In Proc. 3rd
International Workshop on Data Mining and Audience Intelligence for Adver-
tising, pages 10–17, 2009.
[WZWC08] H. Wu, D. Zhang, Y. Wang, and X. Cheng. Incremental probabilistic latent
semantic analysis for automatic question recommendation. In Proc. ACM Con-
ference on Recommender Systems, pages 99–106, 2008.
[XYFS07]
X. Xu, N. Yuruk, Z. Feng, and T. A. J. Schweiger. Scan: a structural clustering
algorithm for networks. In Proc. 2007 ACM SIGKDD Conference on Knowledge
Discovery and Data Mining, pages 824–833, 2007.
[XYW+09] J. Xu, G. Ye, Y. Wang, G. Herman, B. Zhang, and J. Yang.
Incremental
EM for probabilistic latent semantic analysis on human action recognition.
In Proc. IEEE International Conference on Advanced Video and Signal Based
Surveillance, pages 55–60, 2009.

390
Graph-Based Social Media Analysis
[XYW+11] J. Xu, G. Ye, Y. Wang, W. Wang, and J. Yang. Online learning for PLSA-
based visual recognition. In R. Kimmel, R. Klette, and A. Sugimoto, editors,
Computer Vision – ACCV 2010, volume LNCS 6493, pages 95–108. Springer,
2011.
[XZMZ05]
G. Xu, Y. Zhang, J. Ma, and X. Zhou. Discovering user access pattern based
on probabilistic latent factor model. In Proc. 16th Australasian Database Con-
ference, volume 39, pages 27–35, 2005.
[YHP08]
Y.Tian, R. A. Hankins, and J. M. Patel. Eﬃcient aggregation for graph sum-
marization. In Proc. 2008 ACM SIGMOD International Conference on Man-
agement of Data, pages 567–580, 2008.
[YMM09]
L. Yao, D. Mimno, and A. McCallum. Eﬃcient methods for topic model infer-
ence on streaming document collections. In Proc. 15th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data Mining, pages 937–946,
2009.
[You15]
Youtube press: Statistics. https://www.youtube.com/yt/press/statistics.html,
2015 (accessed March 27, 2015).
[YXQ09]
F. Yan, N. Xu, and Y. Qi. Parallel inference for latent Dirichlet allocation
on graphics processing units. In Advances in Neural Information Processing
Systems 22, pages 2134–2142. 2009.
[ZCX10]
Y. Zhou, H. Cheng, and J. Xu. Clustering large attributed graphs: An eﬃ-
cient incremental approach. In Proc. IEEE International Conference on Data
Mining, pages 689–698, 2010.
[ZCY09]
Y. Zhou, H. Cheng, and J. X. Yu.
Graph clustering based on struc-
tural/attribute similarities. In Proc. Very Large Data Bases Endowment, pages
718–729, 2009.
[ZHD01]
H. Zha, X. He, and C. H. Q. Ding. Spectral relaxation for k-means clustering.
In Proc. Advances in Neural Information Processing Systems, pages 1057–1064,
2001.
[ZK07]
Y. Zhang and J. Koren. Eﬃcient Bayesian hierarchical user modeling for rec-
ommendation system. In Proc. 30th ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 47–54, 2007.
[ZS99]
H. Zha and H. D. Simon. On updating problems in latent semantic indexing.
SIAM Journal Scientiﬁc Computing, 21(2):782–791, 1999.

Chapter 12
Big Graph Storage, Processing and
Visualization
Jaroslav Pokorny
Charles University, Czech Republic
Vaclav Snasel
VSB Technical University, Czech Republic
12.1
Introduction ....................................................................
391
12.2
Basic Notions
..................................................................
393
12.3
Big Graph Data Storage .......................................................
395
12.3.1
DBMS Architectures ..................................................
395
12.3.2
Graph DBMSs .........................................................
397
12.3.3
Storing and indexing graph structures ................................
399
12.4
Graph Data Processing ........................................................
401
12.4.1
Querying graphs in relational DBMS .................................
402
12.4.2
Graph querying in Datalog ............................................
403
12.4.3
Query languages in graph DBMS .....................................
403
12.5
Graph Data Visualization ......................................................
405
12.5.1
Static graph visualization .............................................
406
E-learning log visualization and mining ........................................
406
12.5.2
Dynamic graph visualization ..........................................
408
Examples: Dynamics in co-authorship network ................................
409
12.6
Conclusions .....................................................................
409
Acknowledgment ...............................................................
411
Bibliography ....................................................................
411
12.1
Introduction
Graphs are the most common abstract structure encountered in computer science and
are widely used for abstract information representation. Any system that consists of discrete
states (or sites) and connections between them can be modeled by a graph.
Graph data processing is an important topic of research in the database area. Graph-
based logical models were studied in the context of object-oriented databases [Kim90] or
graph-oriented logical database models in the 1980s [KV84]. The research of graph databa-
ses was popular in the 1990s with database models like GOOD [GPVdBVG94], GraphDB
[G¨ut94] and graph query languages like GraphLog [CM90] and G [CMW87].
Graphs are ubiquitous in many areas of human activity. We can ﬁnd graph data in areas
like biology, software bug detection, information security, even in enterprise data, such as
hierarchies of products or bill of material, ﬁnancial data, and in a knowledge management
391

392
Graph-Based Social Media Analysis
is recent years. Other domains include the Web graph, social networks, and the Semantic
Web.
With new Web applications a lot of graph data are generated and new requirements on
their processing have emerged. For example, in the Web, Internet resources are represented
as a graph of triples, subject-predicate-object, according to a standard model for data inter-
change on the Web RDF http://www.w3.org/RDF/ (retrieved on 14.7.2014). Compared to
previous graph data, the approaches support of queries over structural properties of graphs
is needed. In the last decade, the challenges in graph data management were inﬂuenced by
the advent of large datasets called Big Data [SG14]. Large graphs (in the order of billions
of nodes, edges, and attributes) have become increasingly important in the world of Big
Data. Following the current terminology of Big Data, we will talk about Big Graphs in this
chapter.
Big Data has been a challenge for the relational database management systems (DBMS)
in the past. These graphs were stored in databases to allow for eﬃcient queries using declar-
ative query languages such as SQL. Traditional relational DBMSs (e.g., MySQL and Post-
greSQL) have long been used for this purpose, but with the growing number of nodes and
edges these DBMSs proved unsuitable for processing such data. This is in accordance with
the fact that graph data are more and more processed by data mining algorithms [AW10].
Most data mining algorithms do not operate directly in the Big Data. They just get the
data out, do whatever they need to do, and then store the results.
Commercial database products have to reﬂect these facts. Bigdata http://www.systap.
com/ (retrieved on 14.7.2014) DBMS, handling very Big Graphs, scaled to 50 billion edges on
a single machine and will scale to even larger graphs with its horizontally-scaled architecture.
Traditional graph algorithms assume the input graph ﬁts in the memory or disks of
a single machine. Because today’s graphs are measured in terabytes and heading toward
petabytes, with more than billions of nodes and edges, old assumptions about graphs have
to be changed. This chapter is going to be focused on modern methods of Big Graph data
storage and processing, its parallelization, compression and visualization. Special attention
is devoted to graph DBMSs (see, e.g., [SP12] or [RWWE13]). Graph databases are well
suited, e.g. for social networking analytics where relationships bind multiple entities across
a messy pattern that is hard to break into structures of traditional relational DBMS.
First it is necessary to introduce the notion of a graph database, i.e., to specify its basic
unit of information. Similarly as it is with XML or text databases, either one graph or a
collection of graphs can be considered as a graph database. Then as it is expected, a graph
DBMS is a DBMS optimized for managing graphs, i.e. highly-relational data. As it is usual
today, we will use (imprecisely) the concept of a database also in the meaning of a DBMS
itself in this chapter.
There are many types of graphs, and consequently, a lot of possibilities of how to set up
graph databases. Obviously, graphs can be represented as relations (tables) in a relational
DBMS and processed with SQL. This approach could be called graph-enabled databases.
However, the general purpose relational DBMS allows only a small opportunity for graph
speciﬁc optimizations, since it breaks down the graph structures into individual relations. A
more interesting approach produces native graph databases, which have capabilities to query
some structural properties of graphs. In a more dynamic environment, scalability, both in
data size and the number of users is required. Recently, NoSQL databases [Cat11, Cel13],
particularly their subcategory graph databases, have gained much attention due to their
advantages in scalability.
Section 12.2 introduces some basic notions concerning various types of graphs and op-
erations over them, especially to query graph data. In Section 12.3 we discuss possibilities
for storing graph data. We start with DBMS architectures in general, mainly NoSQL ones,
and those appropriate for storing Big Graph data. The rest of section is devoted to graph

Big Graph Storage, Processing and Visualization
393
English
German
Smith
Lang
Cohen
Wang
Vienna
Prague
Berlin
s
s s
s
m
m
m
b
b
b
FIGURE 12.2.1: Labeled Digraph.
DBMSs and their data structures used for storing and indexing graphs. Section 12.4 deals
with graph data processing, i.e. with possibilities to query graphs represented in diﬀerent
DBMSs. We consider relational DBMSs, Datalog implementations, and native graph data-
bases. Section 12.5 describes graph data visualization. Conclusions summarize the chapter.
12.2
Basic Notions
Formally, a graph G = (V, E) is an ordered pair of a set of vertices V = {vi} and a
set of edges E ⊂V × V. For more details see Chapter 2. Let vi and vj be two nodes from
V ; if {vi, vj} ∈E, then vi and vj are adjacents. In digraphs, for a node u ∈V, v1 ∈V
is a successor of u if (u, v1) ∈E. Similarly, v2 ∈V is a predecessor of u if (v2, u) ∈E.
Given a graph G, a path p is a sequence v0, v1, . . . , vk of nodes that are connected by edges
(vi, vi+1), i = 1, . . . , k −1. The node vi is a predecessor of the node vi+1. A cycle is a path
p with v0 = vk where k > 1. An acyclic digraph is a digraph with no cycles.
In many applications various properties associated with the edges and nodes are stored.
Often attributes (properties) are allowed, i.e., both nodes and edges may have attributes,
even a set of attributes. Attributes are expressed as couples (key, value). Sometimes both
edges and nodes are labeled. In the simplest case, each directed edge is labeled with a
symbol drawn from some ﬁnite alphabet Σ, hence E ⊆V × Σ × V. We talk about property
graphs in this case. If edge labels are numbers, we can talk about weighted graphs.
Figure 12.2.1 shows a labeled digraph describing relationships of types speaks (s), man-
ages (m), and is born in (b) in the domain containing entity types Language, Person, and
Town. The digraph represents a fragment of a graph database. The graph representing
friendship relationship on a social network like Facebook can be seen as an undirected
graph. Weighting is useful in transit networks like roads and streets, or in ﬂight networks.
The weight of an edge can represent a distance or duration. Then, e.g., a weight of a path
can be calculated as a sum of weights of their edges.
When multiple edges exist between the same nodes, we talk about multigraphs. An
extension to the standard graph concept that allows an edge to point to more than two
nodes is called hypergraph.
When a graph is considered as a database D, we have to deﬁne usual database operations
enabling its manipulation, i.e. inserting, deleting, or changing graph parts, and mainly
querying capabilities. Typical queries include subgraph matching (for a given graph ﬁnd
subgraphs of that match a given query pattern), reachability (can I get from u to v?),
shortest path (ﬁnd the quickest/shortest route from u to v), ﬁnding the sum/min/max

394
Graph-Based Social Media Analysis
aggregations over paths or subgraphs in a weighted graph. Other useful queries are ﬁnding
the immediate neighbors of a node or its out-neighbors (in-neighbors) in digraphs.
In databases consisting of a collection of graphs, queries such as supergraph matching
can be considered, e.g. queries like “Find graphs in D which are contained in the query
graph Q” are possible. A subgraph query retrieves all those graphs in the database that are
supergraphs of a Q.
Similarity subgraph matching is also useful. It deals with queries like “Find graphs
in D which have some components of the query graph Q”. Often similarity measure is
useful in cases of noise and inconsistency in data. Well-known graph edit distance has three
advantages: (1) it allows changes in both nodes and edges; (2) it reﬂects the topological
information of graphs; and (3) it is sometimes a metric that can be applied to any type of
graphs.
We will deﬁne formally some important query types on labeled graphs without associa-
tion to real query languages appearing in today’s graph databases. For formal query syntax
we use a syntactic variant of Datalog rules (see, e.g. [Woo12]). The fact that there is the
edge p between nodes u and v is expressed as (u, p, v). Let Σ be an alphabet.
A conjunctive query over Σ is an expression of the form:
Q(z1, . . . , zn) ←(x1, a1, y1), . . . , (xm, am, ym), m ≥1,
(12.2.1)
where m > 0, xi and yi are node variables or constants (1 ≤i ≤m), ai ∈Σ (1 ≤i ≤m),
and zi are either a xj or yj (1 ≤i ≤n, 1 ≤j ≤m), Q(z1, . . . , zn) is the head of the query;
the expression on the right of the arrow is its body. The symbol “,” between the triples
denotes conjunction. Given a database D the answer Q(D) to Q is a set of n-tuples of
nodes satisfying its body.
If the head is Q(), then the query is of type YES/NO.
Example 1:
Q(x) ←(x, speaks, German), (x, speaks, English), (x, is born in, Prague).
(12.2.2)
Using the labeled digraph from Figure 12.2.1, the associated database D contains facts
(Lang, speaks, German), (Cohen, speaks, English), (Lang, is born in, Prague), etc. Then
the answer to Q will be the set {Lang}.
A regular path query over Σ is an expression of the form:
Q(x, y) ←(x, r, y),
(12.2.3)
where x and y are node variables, r is a regular expression over Σ. We use | for disjunction,
· for concatenation, r∗for Kleene closure in regular expressions.
Given a database D the answer Q(D) to Q is a set of couples of nodes, such that there
is a path from x to y and the sequence of edge labels on this path satisﬁes r. When we
restrict to matching only simple paths we talk about a query with simple regular path over
Σ. A path is simple, if no node is repeated on it.
A conjunctive regular path query over Σ is an expression of the form:
Q(z1, . . . , zn) ←(x1, r1, y1), . . . , (xm, rm, ym), m ≥1,
(12.2.4)
where ri is regular expression over Σ and zi is a xj or yj.
Example 2:
We will add new relationship types has a nationality, lives in, and is located in to our

Big Graph Storage, Processing and Visualization
395
examples. They model various relationships between persons and towns, or between larger
territorial units in the case of the transitive relation lives in. The answer to
Q(x, y) ←(x, speaks, German), (x, speaks, English),
(x, has a nationality|((is born in|lives in) · located in∗), y)
(12.2.5)
contains pairs of persons x and places y, with x speaking German and English and being of
nationality x or being born in or live in some place which is connected to y by a sequence
of any number of located in relationships.
More theoretical considerations about graph queries can be found, e.g., in [FG00] and
[MW95].
12.3
Big Graph Data Storage
Today special attention is paid to storage and processing Big Data. In general, Big Data
are most often characterized by several V ’s which also pose problems for their storage and
processing:
• Volume - data scale in the range of TB to PB and more,
• Velocity - both how quickly data are being produced and how the data must be
processed to meet demand,
• Variety - data are in many format types - structured, unstructured, semi-structured,
text, media, etc.,
• Veracity - managing the reliability and predictability of inherently imprecise data.
In Big Graphs the ﬁrst two dimensions seem to be the most relevant. This rather sim-
pliﬁed and vague characterization does not consider explicitly structural relationship com-
plexity of graphs. This complexity of graphs includes a necessity to connect and correlate
relationships, hierarchies and multiple node relationships, which means in context of Big
Graphs also new challenges for data analytics, i.e. a development of data mining algorithms.
Thus, the volume means a lot of highly interconnected data in the case of Big Graphs. Con-
sequently, graph data models applied in graph databases are used more and more to address
the complexity problem, i.e. to provide analytical results from such complex data eﬃciently.
In Section 12.3.1 we describe some properties of relational and NoSQL DBMS architec-
tures important for graph management. Section 12.3.2 is devoted to native graph DBMSs.
Section 12.3.3 mentions some typical approaches to storing and indexing graph structures
in these DBMSs.
12.3.1
DBMS Architectures
Database approaches to graph management can use two types of architectures, relational
and NoSQL. Traditional relational DBMS is based on usage of SQL language and trans-
actional properties guaranteeing ACID properties. ACID stands for atomicity, consistency,
isolation, and durability and is fundamental to database transaction processing. One prob-
lem with the relational database model was that it was designed for character based data

396
Graph-Based Social Media Analysis
which could be modeled in terms of attributes and records translated into columns and rows
in a table. With a growing volume of data and an increasing the number of users, today’s
applications often require more scalability and performance. And it is precisely these two
aspects that are characteristic for a new category called NoSQL (Not Only SQL) databases.
Their providers claim their products outperform and out-scale relational DBMS.
In the broad sense, NoSQL represent more categories. For example, they include also
object-oriented and XML DBMSs. But only four approaches are considered as typical ones
in the literature about NoSQL databases: key-value stores, column oriented and document
stores, and obviously graph databases.
Since NoSQL databases are used in a non-reliable Internet environment, some important
functionalities known, e.g. from traditional relational DBMSs, are modiﬁed in the world of
NoSQL. For example,
• they have a simpliﬁed data model,
• database design is query driven,
• integrity constraints (IC) are not supported,
• there is no standard query language,
• unneeded complexity is reduced (simple API, simple get, put, and delete operations).
It is typical for distributed NoSQL databases that they are scalable [Pok13]. They use so
called horizontal scaling (also scale-out) in which data are distributed horizontally in the
network that means into groups of rows in the case of tabular data.
The high performance of NoSQL databases is often achieved by an in-memory process-
ing approach, which means that the data are stored in a computer’s memory to achieve
faster access. Thanks to considerable technological advances during the last 30 years, in-
memory databases have ﬁnally become available in commercial products. Various archi-
tectures are used for in-memory graph DBMSs. They have very diﬀerent design charac-
teristics. These databases often lack transactions, provide a single view of a graph, and
only support durability through a snapshot of the graph to disk. For example, Trinity
http://research.microsoft.com/en-us/projects/trinity/ (retrieved on 14.7.2014) is on a mem-
ory cloud, i.e., a globally addressable, in-memory key-value store over a cluster of machines.
Due to weakening ACID semantics, NoSQL provides little or no support for Online
Transaction Processing (OLTP) as it is required for most enterprise applications. Indeed,
CAP theorem [Bre00] has shown that a distributed computer system can only choose at
most two out of three properties: Consistency, Availability, and tolerance to Partitions.
Then, considering P in a network, NoSQL databases support A or C. In practice, A is the
most preferred and the strict consistency is relaxed to so-called eventual consistency. At a
high level, this means that if no new updates are made in the distributed storage system,
eventually all accesses will return the last updated values.
However, most graph DBMSs are ACID-compliant and disk-backed. For example, Titan
http://thinkaurelius.github.io/titan/ (retrieved on 14.7.2014) is a transactional DBMS that
can support thousands of concurrent users executing complex graph traversals. Titan is a
graph database layer which uses another database as a backend (NoSQL DBMS Cassandra
http://cassandra.apache.org/ (retrieved on 14.7.2014) or HBase http://hbase.apache.org/
(retrievedon14.7.2014)), i.e. it aﬀords the same transactional guarantees to the user as the
underlying data store does. In the case of Cassandra, it is eventual consistency. Moreover,
Titan can leverage both HBase and Cassandra’s scalable architecture.
Graph-Based transactions can be found in GraphBase http://graphbase.net/ (re-
trieved on 14.7.2014). Sparksee http://sparsity-technologies.com/\#sparksee (retrieved on
14.7.2014) (formerly known as DEX) supports ACID with only partial isolation.

Big Graph Storage, Processing and Visualization
397
Language
Person
Town
speaks
manages
is born in
FIGURE 12.3.1: Graph database schema.
12.3.2
Graph DBMSs
By the graph DBMSs we will understand native graph databases in this section. These
systems are mainly designed to provide eﬃcient graph traversal functions, have a special
implementation, often lack the support of declarative query interfaces, and do not use any
query optimization strategies. As we will see, the diversity of these approaches, however,
is huge. First, we mention data modeling used in native graph databases and then provide
some examples of their implementations.
Data models used in native graph databases
As one expects, graph database models concern, as is usual in the database area, data
structures for the database schema (graph database schema) and instances of this schema
(graph database), i.e. a collection of graphs or one graph. In this case both types of data
structures are modeled as graphs [AG08]. Figure 12.3.1 shows a graph database schema for
the graph database in Figure 12.2.1. It contains the necessary entity types and relationship
types.
An important part of each database schema is a set of ICs that enforce data consistency
of associated databases. Examples of IC for graph databases are “labels are unique names,”
“graph does not contain a loop,” etc. A graph database model usually describes simple
relationships between two entities. In more complex approaches ISA-hierarchies known from
classical conceptual modeling are possible. In practice, attributes of both entity types and
relationship types are allowed. For example, a Person entity type can have the attributes
Id, Name, and Position.
On the other hand, a graph database can be a collection of graphs or one graph without
an a priori deﬁned schema. Since we need to work with various graph types in applications,
it is important to know precisely which types of graphs are allowed in a graph DBMS. For
example, HyperGraphDB http://www.hypergraphdb.org/ (retrieved on 14.7.2014) serves
for storing hypergraphs.
In practice, a graph database model is described rather intuitively without any for-
mal fundamentals in most approaches. The terminology used is also very diverse and the
diﬀerence between conceptual and database views of data is mostly blurred.
Several commercial and open source native graph databases
The well-maintained and structured Website http://nosql-database.org/ (retrieved on
14.7.2014) included 14 graph DBMSs in 2011. The most known are Neo4j http://www.neo4j.
org/ (retrieved on 14.7.2014), InﬁniteGraph http://www.objectivity.com/inﬁnitegraph (re-

398
Graph-Based Social Media Analysis
trieved on 14.7.2014), Sparksee, Titan, the Web graph database InfoGrid http://infogrid.
org/trac/ (retrieved on 14.7.2014), HyperGraphDB, GraphBase, and Trinity.
Some products are intended for special graph applications. For example AllegroGraph
http://franz.com/agraph/ (retrieved on 14.7.2014) works with RDF graphs and thus sup-
ports reasoning and ontology modeling. BrightStarDB http://brightstardb.com/ (retrieved
on 14.7.2014), Bigdata and SparkleDB http://www.sparkledb.net/ (retrieved on 14.7.2014)
(formerly known as Meronymy) serve similar purposes. Other software tools are DBMSs
with restricted functionality. For example, WhiteDB http://whitedb.org/ (retrieved on
14.7.2014) is a lightweight NoSQL database library written in C, operating fully in
memory. Another, rather a hybrid solution, is represented by Virtuoso Universal Server
http://virtuoso.openlinksw.com/ (retrieved on 14.7.2014). Its functionality covers not only
processing RDF data, but also relations, XML, text, and others.
Some of these projects are more mature than others, but each of them is trying to solve
similar problems.
We describe some of graph DBMSs that are succesful in practice, in more detail.
Example 3: Neo4j
Neo4j is an open-source, highly scalable, robust (fully ACID compliant) native graph
database that stores data in a graph. It is implemented in Java and belongs to the older
NoSQL systems.
Neo4j stores data as nodes and relationships. Both nodes and relationships can hold
properties in a key-value form. Attribute values can be either a primitive or an array of one
primitive type. Nodes are often used to represent entities, but depending on the domain the
relationships may be used for that purpose as well. Both the nodes and edges have internal
unique identiﬁers that can be used for the data search. Nodes cannot refer to themselves
directly. The semantics can be expressed by adding directed relationships between nodes.
Neo4j is a centralized system that lacks the computational power of a distributed, parallel
system. It still suﬀers from scalability issues since it does not support partitioning still
necessary in a distributed environment.
Processing of graphs in Neo4j entails mostly random data access which can be unsuit-
able for Big Graphs. Graphs that cannot ﬁt into main memory may incur numerous disk
accesses that signiﬁcantly inﬂuences graph processing. Big Graphs similar to other Big Data
collections must be partitioned over multiple machines to achieve scalable processing.
Example 4: Sparksee
In Sparksee a graph is a labeled directed attributed multigraph, where edges can be
either directed or undirected. Both nodes and edges may have attributes. Sparksee also
introduces the notion of a virtual edge that connects nodes having the same value for a
given attribute. These edges are not materialized. A Sparksee graph is stored in a single
ﬁle; values and identiﬁers are mapped by mapping functions into B+-trees. Bitmaps are
used to store nodes and edges of a certain type.
The architecture of Sparksee includes the core, that manages and queries the graph
structures, then an API layer to provide an application programming interface, and the
higher layer applications, to extend the core capabilities and to visualize and browse the
results. To speed up the diﬀerent graph queries and other graph operations, Sparksee oﬀers
diﬀerent types of indexing: i) attributes. ii) unique attributes, iii) edges to index their
neighbors, and iv) indices on neighbors. Sparksee implements a number of graph algorithms,
e.g. shortest-path, depth-ﬁrst traversal, ﬁnding strong connected components of digraphs,
etc.

Big Graph Storage, Processing and Visualization
399
12.3.3
Storing and indexing graph structures
We have mentioned in Section 12.3.1 the possibility to store graph data in a relational
DBMS. However, graph databases are navigated mainly by the following relationships (e.g.
friendship in social networks). This kind of storage and navigation is not possible in rela-
tional DBMSs due to the rigid relational structures and the inability to follow connections
between the data in arbitrary way. On the other hand, there are attempts to provide in-
ternal support for graph data in relational DBMSs as well. Oracle’s Version 12c, released
in July 2013, includes the Oracle Spatial and Graph http://www.oracle.com/technetwork/
database/options/spatialandgraph/overview/index.html (retrieved on 14.7.2014), which en-
ables users to model and manipulate graph data in geographic information systems.
Similar to traditional relational DBMSs, some of the graph DBMS’s products are dis-
tributed databases, e.g. InﬁniteGraph. The distributed graph databases provide data per-
sistence by storing graphs in distributed ﬁle systems. For example InﬁniteGraph is built
on a highly scalable, distributed database architecture where both data and processing
are distributed across the network. A single graph database can be partitioned and dis-
tributed across multiple disk volumes and machines, with the ability to query data across
machine boundaries. Rather than in-memory graphs, this system supports eﬃcient traversal
of graphs across distributed data stores. The same database client program can access the
graph database locally or across a network in a native manner. Other graph databases, e.g.
Neo4j, now start to support running in distributed mode on clusters; they are not designed
to operate in a distribute environment.
The other example, Google’s internal graph processing platform Pregel [MAB+10], is
built on top of Hadoop http://hadoop.apache.org/(retrieved on 14.7.2014). Recall that
Hadoop the software framework with its Hadoop Distributed File System (HDFS) and
MapReduce framework provides batch jobs for processing the distributed nodes with mes-
sage passing. The application developer is isolated from details of distribution that does the
basic MapReduce programming model not optimal for graph processing because most graph
algorithms are iterative and traverse the graph in some way. The Pregel library divides a
graph into partitions, each consisting of a set of vertices and all of those vertices outgoing
edges.
Distributed graph databases require a partitioning graph data which is a non-trivial
problem. Optimal division of graphs requires ﬁnding the subgraphs of a graph. In practice,
however, the number of edges is too large to eﬃciently compute an optimal partition;
therefore most databases use random partitioning. In a dynamic environment, what looks
like a good distribution one moment, may no longer be optimal a few seconds later. This is
known to be an NP-complete problem in the general case.
Surfer [CWHY10] and GBASE [KTS+11] are examples of extensions for MapReduce
that are proposed to help to process graphs more eﬃciently. Surfer oﬀers a new primitive,
propagation, which is an iterative computational pattern that transfers information along
the edges from a node to its neighbors in the graph. GBASE uses a novel block compression
for graph storage and a MapReduce algorithm to support incidence matrix based queries
using the original adjacency matrix, without explicitly building the incidence matrix. More
advanced solutions are now Giraph http://giraph.apache.org/ (retrieved on 14.7.2014), orig-
inated as the open-source counterpart to Pregel, and GraphLab http://graphlab.org/ (re-
trieved on 14.7.2014) projects. The GraphLab framework uses the Message Passing Interface
http://en.wikipedia.org/wiki/Message Passing Interface (retrieved on 14.7.2014) model to
scale and run complex algorithms using data in HDFS.
An interesting and eﬀective technique for graph implementation is described in [BK14].
A ﬂat description of the particular graph records uses a column-oriented storage model,
bitmap columns enables fast access to parts of these graph records.

400
Graph-Based Social Media Analysis
For graph databases it is important to implement a set of relationships between nodes.
Native graph databases implement it with index-free adjacency (adjacency lists) where each
node has explicit references to its adjacent nodes, and does not need to use an index to ﬁnd
them. The adjacency list format is simple and might be good for answering out-neighbors
queries. In the case of only one relationship type, a digraph G = (V, E) can be represented
by the adjacency matrix A, were Aij = 1, if (i, j) ∈E and Aij = 0, otherwise. Unlike
adjacency lists, an adjacency matrix is preferred if the graph is dense.
There is also an incidence matrix whose each row corresponds to an edge, and it has
two non-zeros whose column Ids are the node Ids of the edge.
Since traversing neighbors (one of the most common operations) is too resource consum-
ing, adjacency matrix is not usually used for Big Graph databases. GBASE uses matrix-
vector multiplications on the adjacency and the incidence matrices for node-based and
edge-based queries.
Another data structure is used for implementing graph databases, i.e., bitmaps. For
example, in Sparksee a graph is represented through bitmap data structures and so-called
maps. Node adjacencies are represented by bitmaps to minimize the space needed in memory.
Bitmaps allow high compression rates. A map is an inverted index with key values associated
to bitmaps or data values.
A lot of graph databases use an approach similar to the one used in Neo4j, i.e. nodes,
relationships, relationship types, and attributes are stored in diﬀerent store ﬁles. Neo4j
actually stores adjacent relationships in doubly linked lists in the ﬁlesystem. Two separate
caches serve as eﬃcient block writes (a ﬁle system cache) and as eﬃcient access reads (the
object cache).
Graph querying is not an easy task. For example, to ﬁnd graphs that contain a given
graph pattern means to solve the subgraph isomorphism problem which has been proven to
be NP-complete. Deﬁning and computing the similarity between two graphs is also diﬃcult.
Concerning query processing in Big Graphs a node indexing is necessary. Then candidate
node sets obtained by index search serve to building subgraphs and constructing the query
answer.
T-trees [LC86] seem to be widely used for in-memory databases. The latter results
[LNT00] indicate that classical B-link trees outperform the T-tree if concurrency control is
enforced. This condition is usually fulﬁlled in today’s scalable systems.
Graph indexing is useful for graph pattern matching over a large collection of small
graphs. Graph indexing distinguishes from indexing, e.g. relational databases. Because of
queries on structural properties on the graph data some structure indexes are needed. Struc-
ture indexes can be based on reducing undirected graphs in two phases:
• performing depth-ﬁrst traversal to obtain a tree,
• encoding the tree into a string.
Another line of graph indexing addresses reachability queries in large digraphs. Reach-
ability queries correspond to recursive graph patterns, which are paths.
Often a path index is used in graph databases. GraphGrep index [SWG02] records all em-
beddings of paths (up to a maximal length) in the graph database. But a path is too simple
and structural information hidden in the graph is lost. More sophisticated approaches index
subgraphs [Sri11]. Authors of [YYH04] consider even a frequent structure-based approach
considering only frequent “small” subgraphs (fragments). Their gIndex is of 10 times smaller
size and achieves 3-10 times better performance in comparison with a typical path-based
method. An eﬃcient index, FG∗-index, is proposed in [CKN09].

Big Graph Storage, Processing and Visualization
401
12.4
Graph Data Processing
One possibility to handle graph data is through constructing complex graph oriented
programs based on direct calls to low level APIs. This style is very diﬃcult to optimize. Sim-
ilar to development of high-level languages for relational databases, there are approaches to
deﬁne algebra with a set of query operations and an SQL-like language allowing a recursion
with an engine that generates query plans. Therefore, these can be optimized and executed
eﬃciently. An example of such a direction is the last development of the Sparksee graph
database [MBDS14].
A lot of native graph databases have interfaces to some current open source software
products http://www.tinkerpop.com/ (retrieved on 14.7.2014) from the graph space.
• Blueprints is a property graph model interface with provided implementations. Data-
bases that implement the Blueprints interfaces automatically support Blueprints-
enabled applications.
• Frames exposes the elements of a Blueprints graph as Java objects. Instead of writing
software in terms of nodes and edges, with Frames, software is written in terms of
domain objects and their relationships to each other.
• Gremlin is a domain speciﬁc language for traversing property graphs. This language
has application in the areas of graph query, analysis, and manipulation.
• Rexster is a multi-faceted graph server that exposes any Blueprints graph through
several mechanisms with a general focus on REST. Remember that Web API REST
implements four basic methods denoted often as CRUD, i.e operation Create, Retrieve,
Update, and Delete.
• Furnace is a property graph algorithms package. It provides implementations for stan-
dard graph analysis algorithms that can be applied to property graphs in a meaningful
way.
• Pipes is a dataﬂow framework that enables the splitting, merging, ﬁltering, and trans-
formation of data from input to output. Computations are evaluated in a memory-
eﬃcient, lazy fashion.
For example, Titan DBMS provides native integration with the ﬁrst four graph tools. Big-
data DBMS supports Blueprints and Sesam.
Usually two types of graph databases are studied in the context of graph querying. The
ﬁrst type consists of very large graphs, such as the Web graph and social networks. Typical
querying tasks for such graph databases include ﬁnding the best connection between a given
set of query nodes and matching subgraphs. The second type of graph database collects a
large collection of small graphs, such as chemical compounds or those appearing in Customer
Relationship Management software and Workﬂow Management Systems. Typical queries for
this type of graph database include subgraph queries and similarity queries.
In Section 12.4.1 we describe possibilities of relational DBMSs for graph storage and
querying. Section 12.4.2 explains a Datalog approach to graph data processing. Section
12.4.3 is devoted to query languages implemented in native graph DBMSs.

402
Graph-Based Social Media Analysis
12.4.1
Querying graphs in relational DBMS
Since the 1999 version, the SQL standard has possibilities for graphs processing with
“recursive union” used in WITH RECURSIVE clause. Graphs represented by tables in
SQL database can be proceed by constructs of SQL. This was possible also earlier but
even simple traversal algorithms required costly self joins on the table and programming
in Embedded SQL. The WITH RECURSIVE clause allows formulating basic graph queries
which require ﬁxpoint computation, although not always in a simple way, but probably
with a more eﬀective implementation. The queries with WITH RECURSIVE work simply
on acyclic graphs. To recognize cycling during query evaluation needs to construct rather
tricky conditions in query expression.
Example 5:
Suppose a relational representation of a part of our graph database given by the entity
type persons and relationship type manages is described by the relation schema Persons(Id,
Name, Position, Manager Id). Consider the query “Find all managers of Wang (including
himself).” The table Managers will contain Wang’s managers from all levels of the hierarchy
given by relationships manages including the top manager.
WITH RECURSIVE Managers(Id, Name, Manager_Id) AS
(SELECT Id, Name, Manager_Id
FROM Persons
-
WHERE Name = ’Wang’
UNION ALL
SELECT P.Id, P.Name, P.Manager_Id
FROM Persons AS P
INNER JOIN
Managers AS M
ON M.Manager_Id = P.Id)
SELECT * FROM Managers
Materialization of such a recursively deﬁned view is given by the last SELECT. This query is
an ancestor or self query applicable in acyclic digraphs. The following two tables represent
relations Persons and Managers, respectively.
Persons
Id
Name
Position
Manager Id
1
Smith
director
NULL
2
Lang
manager
1
3
Cohen
manager
1
4
Wang
assistant
3
Managers
Id
Name
Manager Id
1
Smith
NULL
3
Cohen
1
4
Wang
3
The last SELECT returns a table of persons (given by their attributes, Id and Name)
who are the managers of Wang and Wang himself. Manager Id will contain Ids of their
direct managers.
However, expressiveness of WITH RECURSIVE clause is restricted in SQL. A recursive
part of a query must not contain clauses SELECT DISTINCT, GROUP BY, HAVING,
scalar aggregates, TOP, and OUTER JOIN.

Big Graph Storage, Processing and Visualization
403
12.4.2
Graph querying in Datalog
Datalog [AHV95] is an important rule-based language for inference using facts found in
databases. In classical Datalog a program P is a ﬁnite set of rules of the form:
A ←A1, . . . , Am
(12.4.1)
where Ai are predicates, or relation names with variables or constants as arguments. A is
a relation name with variables as arguments. Datalog can be extended with negation and
ICs. Datalog with negation usually allows a stratiﬁed negation, i.e., the conclusion of any
rule and a negated hypothesis of any rule are not mutually recursive. Facts like (u, p, v)
(see Section 12.2) correspond to p(u, v) in Datalog. In fact, they are rules without body, i.e.
p(u, v) ←. The main expressive advantage of Datalog is that it enables recursive queries.
This is extremely important for a lot of graph queries. On the other hand, for graph data
processing it is necessary to have an eﬃcient implantation at one’s disposal. The possibility
to implement Datalog in a relational environment tends to algorithms based on variants of
ﬁxpoint computation that is not too feasible. However, there are methods that transform
a Datalog program into an eﬃcient specialized implementation that, given any set of facts,
computes exactly the set of facts that can be inferred [LS03]. The authors of [LS06] used
these methods for the development and implementation of a new graph-oriented language
enabling them to formulate some variants of regular paths queries. Its optimized Datalog
implementation using the method from [LS03] introduced in [TGL10]. An added value of
such a language is that many queries can be written much more easily and clearly than
directly in Datalog.
Example 6:
The ancestor or self query introduced in Example 5 can be expressed in Datalog as
Manages(i, n, m)
←
Persons(i, n, p, m), n = Wang
Manages(i, n, m)
←
Manages(i1, n1, i), Persons(i, n, p, m)
Example 7:
Suppose a relation is part of containing facts that a location is a part of other location.
Evaluation of a regular paths query (one conjunct of the query in Example 2)
Q(x, y) ←(x, has a nationality|((is born in|lives in) · is located in∗), y)
can be represented by the following Datalog program:
asoc(x, y)
←
is born in(x, y)
asoc(x, y)
←
lives in(x, y)
is part of(x, y)
←
is located in(x, y)
is part of(x, y)
←
is located in(x, z), is part of(z, y)
Q(x, y)
←
has a nationality(x, y)
Q(x, y)
←
asoc(x, y)
Q(x, y)
←
asoc(x, y), is part of(z, y)
Compared to the WITH RECURSIVE clause in SQL, the capabilities of recursion in
Datalog are more expressive. SQL enables only linear recursion, i.e. each FROM has at
most one reference to a recursively deﬁned relation.
12.4.3
Query languages in graph DBMS
Graph DBMSs oﬀer a lot of graph languages (see, e.g., [AG08, Ang12]). Some of them
have syntax similar to SQL and enable a set of basic graph queries extended by selection

404
Graph-Based Social Media Analysis
conditions (e.g., Cypher Query Language of Neo4j). Some of them provide a more advanced
functionality like link analysis, social network analysis, pattern recognition, and keyword
search (e.g., Sparksee). We will show some details of Cypher query language and mention
brieﬂy the graph traversal language Gremlin http://github.com/tinkerpop/gremlin/wiki
(retrieved on 14.7.2014).
Example 8: Cypher Query Language
Cypher is a declarative graph query language that allows us to query and update the
graphs. Being a declarative language, Cypher focuses on the clarity of expressing what to
retrieve from a graph, not how to do it, in contrast to imperative languages like Java,
and scripting languages like Gremlin and the JRuby, which can also be used in the Neo4j.
Similar to relational DBMSs, a declarative approach to querying is not always an advantage.
It restricts the ability to optimize the order of traversals in a query.
The syntax of Cypher statements can be compared with the classic SQL. For example,
the SQL query
SELECT *
FROM Persons
WHERE Name = ’Lang’
has the following equivalent expression in Cypher:
START person=node:Person(Name = ’Lang’)
RETURN person
The START clause speciﬁes the starting point on the graph, from which the query is exe-
cuted. Thus, the role of this phrase is something between the FROM and WHERE clauses
of the SQL SELECT statement.
By the following expression we describe the query requiring the languages spoken by
Lang:
START person=node:Person(Name = ’Lang’)
MATCH person --> language
RETURN language.Name
where “-->” denotes directed edge. Other expression returns a hierarchy of places starting
with the one where Lang is born.
START person=node:Person(Name = ’Lang’)
MATCH person-[:is born_in]->[:located_in*]->place
RETURN place.Name
The notation principle used in Cypher reﬂects rewriting graph notation to linear ASCII
notation with the help of regular expressions.
Cypher supports traversal and neighborhood queries. Cypher commands can embrace
several parts, enabling
• data manipulations:
CREATE: creates nodes and edges,
DELETE: removes nodes, edges and attributes,
SET: set values of the attributes,
FOREACH: performs updating actions once per each element in a list.

Big Graph Storage, Processing and Visualization
405
• querying constructs:
START: starting points in the graph, obtained via index lookups or by element Ids,
MATCH: the graph pattern to match, bound to the starting points in START (it is
equivalent to the SQL JOIN clause),
WHERE: ﬁltering criteria,
RETURN: speciﬁes the answer to query (it is equivalent to the SQL SELECT clause),
ORDER BY: sorts the output,
WITH: divides a query into multiple, distinct parts. The WITH clause is used to pipe
the result from one query to the next one and to separate reading from updating of
the graph.
Thus the Cypher expressions not only allow for data searching, but also their insertion,
modiﬁcation, or deletion. Therefore, it is not only a parallel to an SQL SELECT statement,
but contains the UPDATE, INSERT, and DELETE statements as well.
In addition to Cypher, Neo4j also supports Gremlin and the Blueprints interface. Grem-
lin uses the concept of traversals to move from a starting location to (possibly multiple)
ending locations in a graph. Gremlin is an open-source Turing-complete programming lan-
guage based on the open source software Blueprints, that is based on traversals of a property
graph with a syntax taken from object-oriented systems and the C programming language
family. There is syntax for directed edges and more complex queries that looks more math-
ematical than SQL-like. Gremlin provides a command line prompt to insert/update/delete
operations on the nodes and edges. As a query language, it can perform complex graph
traversals compactly. To get the same query result, it uses much less code than using Java
API.
12.5
Graph Data Visualization
Information Visualization (InfoVis) research focuses on the use of techniques to help
people understand and analyze data. In particular, InfoVis considers how abstract data
(i.e., without correspondence to the physical world) can best be visually represented. In-
foVis, the study of transforming data, information, and knowledge into interactive visual
representations, is very important to users because it provides mental models of information.
The boom in Big Data analytics has triggered broad use of InfoVis in a variety of domains,
ranging from science to art [SM14]. In [LCWL14], Liu et al. present a comprehensive survey
and key insights into this fast-rising area.
In [SM14] the authors present an anthology of articles to foster the emerging convergence
of arts, humanities, and complex networks. This book covers a kaleidoscope of diﬀerent
approaches, ranging from vigorous humanistic inquiry and pure natural science to free
artistic expression.
Considering current exponential growth of InfoVis, this chapter cannot and does not
present an exhaustive account of all relevant aspects of this ﬁeld.
As an independent ﬁeld, graph visualization arose in the 1990s with the Symposium
on Graph Drawing, which was held in its 22nd edition in 2014. With increasing interest
in information visualization, alternative visual representations of graphs have also been
introduced, such as adjacency matrices.
Methods and tools for graph visualization are widely used in many applications, such
as social contacts [HCL05], co-authorship network [KHS+12], process mining [VDARS05],
trajectories on maps [SHH11], and electronic communications [SFMB12]. According to

406
Graph-Based Social Media Analysis
[VLKS+11], graphs can be classiﬁed into two categories: static and dynamic, based on
their time dependence.
Graph visualization methods compute a 2D/3D layout of the nodes and the edges,
mainly based on node-link diagrams [Tut63, War13]. They play a fundamental role in graph
visualization. Graph readability is aﬀected by quantitative measurements called aesthetic
criteria [EGHM10]. Thus, graph visualization generally deals with the ways of drawing
graphs according to a set of predeﬁned aesthetic criteria [Che06]. Diﬀerent graph drawing
algorithms may have their own aesthetic criteria to follow. As for aesthetic criteria, the
widely accepted rules for drawing a comprehensible graph can be summed up as follows:
• Reﬂect the inherent symmetry
• Uniform edge length
• Minimized edge crossings
• Avoidance of sharp angles between edges
• Clutter reduction
• Even distribution of the vertices in the available space
Some of these criteria can be mutually exclusive, and problems which aim to optimize
the criteria are often NP-hard. For example, a symmetrical graph may require a certain
number of edge crossings, even if they might be avoided. And uniform edge lengths may not
always produce the most appropriate results. Therefore, many graph drawing algorithms
are heuristics or meta-heuristics [KKS+14].
12.5.1
Static graph visualization
Graph visualization algorithms are categorized into the following approaches: force-
directed layouts, the use of dimension reduction in graph layout, and computational im-
provements including multi-level techniques [GFV13]. Methods developed speciﬁcally for
graph visualization often make use of node-attributes and are categorized based on whether
the attributes are used to introduce constraints to the layout, provide a clustered view, or
deﬁne an explicit representation in two-dimensional space.
Node-link diagrams have been the most used visual representation for graphs. Over
the last decade graph drawing visualization has emerged as an exciting research area that
addresses a signiﬁcant problem: how to make sense of the ever increasing amounts of re-
lational information that has become widely available. However, recent visualization work
indicates that researchers have gradually shifted their attention from ﬁnding new layout al-
gorithms [STT81, KK89, SCL+09, KHKS12, LCWL14] to studying the usability in various
applications.
E-learning log visualization and mining
E-learning is a method of education, which usually uses Learning Management Systems
and the internet environment to ensure the maintenance of courses and to support the
educational process. Moodle, one of such systems widely used, provides several statistical
tools to analyze students’ behavior in the system. However, none of these tools provides
visualization of relations between students and their clustering into groups based on their
similar behavior. In [SMDS14], the authors propose an approach for analysis of students’
behavior in the system based on their proﬁles and on the students’ proﬁle similarity. The

Big Graph Storage, Processing and Visualization
407
(a)
FIGURE 12.5.1: LMS Moodle log sequence visualization. (See color insert.)
approach uses process mining [VDARS05] for visualization of relations between students
and groups of students Figure 12.5.1.
[MM08] proposes an approach to graph layout through the use of space ﬁlling curves
which is very fast and guarantees that there will be no nodes that are colocated. The
resulting layout is also aesthetic, and satisﬁes several criteria for graph layout eﬀectiveness.
Burch et al. [BKH+11] conducted a user study to compare the readability of node-link
diagrams and space-ﬁlling representations. They found that space-ﬁlling results are more
space-eﬃcient but more diﬃcult to interpret.
In particular, orthogonal tree layouts signiﬁcantly outperform radial tree layouts for
some tasks, such as ﬁnding the least common ancestor of a set of marked leaf nodes. Yuan
et al. [YCHZ12] argued that a good layout cannot be achieved simply by using automatic al-
gorithms but need user inputs. Thus they proposed a framework that automatically stitches
and maintains the layouts of individual subgraphs submitted by multiple users.
Another hot reseach topic with regard to improving usability is clutter reduction. Among
all the solutions to reduce visual clutter, edge bundling is still the most popular one
[HCL05, CZQ+08, SHH11]. Recently, Selassie et al. [SHH11] proposed a bundling tech-
nique for directed graphs. At the same time, skeleton-based edge bundling was introduced
in [EHP+11]. They calculated the skeleton of edge distributions and used it to bundle the
edges. Other ways to reduce clutter include density estimation, node aggregation, and level-
of-detail rendering. Zinsmaier et al. [ZBDS12] presented an approach that combines these
techniques and achieves a better time performance than other state-of-art methods, while
generating appealing layouts.
The traditional matrix representation is suitable for visualizing dense graphs due to its
non-overlapping visual encoding of edges. However, it may be ineﬀective for sparse graphs.

408
Graph-Based Social Media Analysis
Recently, Dinkla et al. [DWvW12] designed compressed adjacency matrices, which aim to
visualize sparse graphs, such as gene regulatory networks. Similar to matrix representations,
PIWI [YLZ+13] uses vertex plots that show nodes as colored dots without overlap, to display
the neighborhood information of communities in a large graph.
Two-dimensional graph drawing, that is, graph drawing in the plane, has been widely
studied. While this is not yet the case for graph drawing in 3D, there is nevertheless a
growing body of research on this topic, motivated in part by advances in hardware for
three-dimensional graphics, by experimental evidence suggesting that displaying a graph in
three dimensions has some advantages over 2D displays [WF94, WF96, WM08], and by ap-
plications in information visualization [WF94, WM08], and software engineering [WHF93].
Not surprisingly, the mathematical literature is a source of results that can be regarded
as early contributions to graph drawing. It is natural to generalize from drawing graphs in
the plane to drawing graphs on other surfaces, such as the torus. Indeed, surface embeddings
are the object of a vast amount of research in topological graph theory, with entire books
devoted to the topic. We refer the interested reader to the book by Mohar and Thomassen
[MT01] as an example. Numerous drawing styles or conventions for 3D drawings have been
studied. These styles diﬀer from one another in the way they represent nodes and edges.
12.5.2
Dynamic graph visualization
In a static network, the properties of nodes, links, and mapping functions remain un-
changed over time. In a dynamic network, the number of nodes and links, the shape of the
mapping function, and perhaps other properties of the graph change over time. Dynamic
networks are time-varying networks. Visualization dynamics of the graph is very useful.
It helps to understand dynamics features of the graph. Evolution of graph and communi-
ties over time can help us understand social mechanisms behind the graph. Time–varying
changes leading to structural reorganization in a network, called evolution in some disci-
plines, is called emergence [Lew11].
To deﬁne a dynamic graph, we start from the deﬁnition of (static) graph (see section
12.2). Let Gi = (V, E) be a graph. Then, a dynamic graph is deﬁned as a sequence G =
(G1, G2, . . . , Gn) where Gi = (Vi, Ei) are static graphs and indices refer to a sequence of time
steps t := (t1, t2, . . . , tn). In a dynamic graph, time can be modeled as discrete, ordinal or
continuous. Dynamic graphs can be sampled based data on model. We also do not discern
between instants and intervals: whether Gi is a snapshot at instant ti or aggregates an
interval around ti.
While static graph visualizations are often divided into node-link and matrix represen-
tations, the representation of time was identiﬁed as the major distinguishing feature for
dynamic graph visualizations: either graphs are represented as animated diagrams or as
static charts based on a timeline. In [BBDW14], Beck et al. present a comprehensive survey
of dynamic graph visualization.
Animation is a natural way to illustrate changes over time since it can eﬀectively preserve
a mental map [BdM06]. Visualization of the dynamics is not an easy task. There are several
issues that have to be solved for correct visualization [YFDH01, BdM06, BBDW14].
The role of the mental map has been discussed since the ﬁrst works on dynamic graph
visualization and is probably their best evaluated aspect. While we brieﬂy summarize results
of related studies, Archambault and Purchase [AP13] review studies on the mental map in
much greater detail. They have shown that preserving a mental map does not help much in
gaining insights into animated dynamic graphs.
As a result, recent methods focus more on showing dynamic graphs statically [BKH+11,
TM12, LWW+13]. To encode the time dimension in a static way, a timeline and small
multiples are two popular choices.

Big Graph Storage, Processing and Visualization
409
Timeline-based approaches encode time as one axis and then draw and align the graph
at each time point on the timeline. Abello et al. [AAK+14] discuss the modeling and rep-
resentation of time for dynamic graphs in greater detail.
Based on small multiples, Hadlak et al. [HSS11] proposed in-situ visualization, which
allows users to interactively select multiple focused regions and choose suitable layouts for
the selected data. They argued that a single visualization technique may not be enough, due
to the complexity of large dynamic graphs. With their approach, a user can freely switch
between diﬀerent visualizations to adapt the analysis focus or the characteristics of regions
of interest.
Examples: Dynamics in co-authorship network
Experiments presented use a weighted co-authors network based on the DBLP data-
base [KHS+12]. Paper [RKHS13] presented an approach to the visualization of weighted
networks based on Sammon’s projection and linear approximation. It introduces a method
for visualizing dynamics of the social network. Results are illustrated by several 3D layout
snapshots of the co-authorship graph extracted from the DBLP database.
Figures 12.5.2a and 12.5.2h depict the state of the network in two consecutive months
11/2011 and 12/2011, respectively. Figures (12.5.2b, . . . , 12.5.2g) show dynamic graph G =
(G1, G2, G3, G4, G5, G6). Vertices where distance is over a selected threshold are interpolated
and these nodes are depicted in a diﬀerent color (for illustrative purposes only). Other
vertices in the interpolation layout are unchanged. Parts of the depicted layouts, highlighted
manually by the circle and square, show how the communities are formed. Interpolation is
very helpful in this case – the communities travel among the layout space and ﬁnally are
connected to the new vertices.
12.6
Conclusions
In this chapter we have described some methods and tools for storage and process-
ing graph data, and information visualization technologies particularly developed for this
purpose. The development of Big Data processing has led to the widespread use of these
technologies. Some of them are also appropriate for Big Graphs — a technology where data
analysis is the main application requirement.
In other words, the technology provides a support for scalability, analytical algorithms
integrated into query tools, etc. Transaction processing is probably not so actual in this
case.
However, some experiences with processing graph data show that a graph database
is not always the best choice; it depends on the exact types of queries that need to be
performed. For example in [HSS11], the authors compared SQL processing of some queries
in PostgreSQL DBMS and the same with Cypher in Neo4J. They reached better results
with PostgreSQL by decomposing the complex SQL query into multiple simple SQL queries.
By the way, this fact documents that query optimization in RDBMs has still reserves.
Sometimes the diﬀerences between graph-enabled and native DBMS are huge. Architects
of GraphBase DBMS argue that their product can be thousands of times faster than a
relational solution.
Traditionally, however, graph database implementations also have problems that limit
usefulness of some products:

410
Graph-Based Social Media Analysis
(a) November 2011 network lay-
out
(b) Interpolated layout 1
(c) Interpolated layout 2
(d) Interpolated layout 3
(e) Interpolated layout 4
(f) Interpolated layout 5
(g) Interpolated layout 6
(h) December 2011 network lay-
out
FIGURE 12.5.2: Layout linear interpolation between two consecutive months.
• Storing data in a graph can be slower than using data structures. This makes graph
databases less-suited to high-throughput and transaction-processing tasks.
• Graph databases can impose unnecessary structural complexity. Sometimes simpler
structures are easier to manage and provide better performance.
• They are diﬃcult to query. The tools available for working with a graph DBMS are
often poor.
We have seen that the diversity of graph data models and graph databases (or graph
data stores) is huge. Unfortunately, no standard benchmarks are at our disposal. Hence, to
compare these tools is diﬃcult.

Big Graph Storage, Processing and Visualization
411
Some of the traditional graph data problems, particularly for Big Graphs, e.g., dynamic
graph partitioning, workload balancing, integration of analytical methods with querying,
still remain a challenge for a future.
Acknowledgment
This chapter has been partially supported by the grant from the Grant Agency of the
Czech Republic – GACR No. P103/13/08195S.
Bibliography
[AAK+14]
D. Archambault, J. Abello, J. Kennedy, S. Kobourov, K.-L. Ma, S. Miksch,
C. Muelder, and A. C. Telea. Temporal multivariate networks. In Multi-
variate Network Visualization, pages 151–174. Springer, 2014.
[AG08]
R. Angles and C. Gutierrez.
Survey of graph database models.
ACM
Computing Surveys, 40(1):1, 2008.
[AHV95]
S. Abiteboul, R. Hull, and V. Vianu. Foundations of databases, volume 8.
Addison-Wesley Reading, 1995.
[Ang12]
R. Angles. A comparison of current graph database models. In Proc. IEEE
28th International Conference on Data Engineering Workshops, pages 171–
177, 2012.
[AP13]
D. Archambault and H. C. Purchase. The map in the mental map: Ex-
perimental results in dynamic graph drawing.
International Journal of
Human-Computer Studies, 71(11):1044–1055, 2013.
[AW10]
C. C. Aggarwal and H. Wang. Managing and mining graph data, volume 40.
Springer, 2010.
[BBDW14]
F. Beck, M. Burch, S. Diehl, and D. Weiskopf.
The state of the art in
visualizing dynamic graphs. In Proc. of the Eurographics Conference on
Visualization, 2014.
[BdM06]
S. Bender-deMoll and D. A. McFarland. The art and science of dynamic
network visualization. Journal of Social Structure, 7(2):1–38, 2006.
[BK14]
D. Bleco and Y. Kotidis. Graph analytics on massive collections of small
graphs. In Proc. of the 17th International Conference on Extending Data-
base Technology, pages 523–534, 2014.
[BKH+11]
M. Burch, N. Konevtsova, J. Heinrich, M. Hoeferlin, and D. Weiskopf. Eval-
uation of traditional, orthogonal, and radial tree diagrams by an eye track-
ing study. IEEE Transactions on Visualization and Computer Graphics,
17(12):2440–2448, 2011.

412
Graph-Based Social Media Analysis
[Bre00]
E. A. Brewer. Towards robust distributed systems. In Proc. Principles of
Distributed Computing, page 7, 2000.
[Cat11]
R. Cattell.
Scalable sql and nosql data stores.
ACM SIGMOD Record,
39(4):12–27, 2011.
[Cel13]
J. Celko. Joe Celkos Complete Guide to NoSQL: What Every SQL Profes-
sional Needs to Know about Non-Relational Databases. Newnes, 2013.
[Che06]
C. Chen. Information visualization: Beyond the horizon. Springer Science
& Business, 2006.
[CKN09]
J. Cheng, Y. Ke, and W. Ng. Eﬃcient query processing on graph databases.
ACM Transactions on Database Systems, 34(1):2, 2009.
[CM90]
M. P. Consens and A. O. Mendelzon. GraphLog: a visual formalism for
real life recursion. In Proc. of the 9th ACM SIGACT-SIGMOD-SIGART
symposium on Principles of database systems, pages 404–416, 1990.
[CMW87]
I. F. Cruz, A. O. Mendelzon, and P. T. Wood. A graphical query language
supporting recursion. In ACM SIGMOD Record, volume 16, pages 323–330,
1987.
[CWHY10]
R. Chen, X. Weng, B. He, and M. Yang. Large graph processing in the
cloud. In Proc. of the 2010 ACM SIGMOD International Conference on
Management of data, pages 1123–1126, 2010.
[CZQ+08]
W. Cui, H. Zhou, H. Qu, P. C. Wong, and X. Li. Geometry-based edge
clustering for graph visualization. IEEE Transactions on Visualization and
Computer Graphics, 14(6):1277–1284, 2008.
[DWvW12]
K. Dinkla, M. A. Westenberg, and J. J. van Wijk. Compressed adjacency
matrices: untangling gene regulatory networks. IEEE Transactions on Vi-
sualization and Computer Graphics, 18(12):2457–2466, 2012.
[EGHM10]
P. Eades, C. Gutwenger, S.-H. Hong, and P. Mutzel. Graph drawing al-
gorithms. In Algorithms and theory of computation handbook, pages 6–6.
Chapman & Hall/CRC, 2010.
[EHP+11]
O. Ersoy, C. Hurter, F. V. Paulovich, G. Cantareiro, and A. Telea. Skeleton-
based edge bundling for graph visualization. IEEE Transactions on Visu-
alization and Computer Graphics, 17(12):2364–2373, 2011.
[FG00]
S. Flesca and S. Greco. Querying graph databases. In Advances in Database
Technology, Lecture Notes in Computer Science, pages 510–524. Springer,
2000.
[GFV13]
H. Gibson, J. Faith, and P. Vickers. A survey of two-dimensional graph
layout techniques for information visualisation. Information Visualization,
12(3-4):324–357, 2013.
[GPVdBVG94] M. Gyssens, J. Paredaens, J. Van den Bussche, and D. Van Gucht.
A
graph-oriented object database model. IEEE Transactions on Knowledge
and Data Engineering, 6(4):572–586, 1994.

Big Graph Storage, Processing and Visualization
413
[G¨ut94]
R. H. G¨uting. Graphdb: Modeling and querying graphs in databases. In
Proceedings of 20th International Conference on Very Large Data Bases,
pages 297–308, 1994.
[HCL05]
J. Heer, S. K. Card, and J. A. Landay. Prefuse: a toolkit for interactive
information visualization. In Proc. of the SIGCHI Conference on Human
Factors in Computing Systems, pages 421–430, 2005.
[HSS11]
S. Hadlak, H. Schulz, and H. Schumann. In situ exploration of large dynamic
networks.
IEEE Transactions on Visualization and Computer Graphics,
17(12):2334–2343, 2011.
[KHKS12]
M. Khoury, Y. Hu, S. Krishnan, and C. Scheidegger. Drawing large graphs
by low-rank stress majorization. In Computer Graphics Forum, volume 31,
pages 975–984, 2012.
[KHS+12]
M. Kudˇelka, Z. Hor´ak, V. Sn´aˇsel, P. Kr¨omer, J. Platoˇs, and A. Abraham.
Social and swarm aspects of co-authorship network. Logic Journal of IGPL,
20(3):634–643, 2012.
[Kim90]
W. Kim. Introduction to object-oriented databases, volume 90. MIT Press,
1990.
[KK89]
T. Kamada and S. Kawai. An algorithm for drawing general undirected
graphs. Information Processing Letters, 31(1):7–15, 1989.
[KKS+14]
P. Kromer, M. Kudelka, V. Snael, M. Radvansky, and Z. Hor´ak. Computing
Sammon’s projection of social networks by diﬀerential evolution. In IEEE
28th International Conference on Advanced Information Networking and
Applications, pages 1001–1006, 2014.
[KTS+11]
U. Kang, H. Tong, J. Sun, C.-Y. Lin, and C. Faloutsos. Gbase: a scalable
and general graph management system. In Proc. of the 17th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, pages
1091–1099, 2011.
[KV84]
G. M. Kuper and M. Y. Vardi. A new approach to database logic. In Proc.
of the 3rd ACM SIGACT-SIGMOD symposium on Principles of database
systems, pages 86–96, 1984.
[LC86]
T. J. Lehman and M. J. Carey.
A study of index structures for main
memory database management systems. In Proc. of the 12th International
Conference on Very Large Data Bases, volume 294, pages 294–303, 1986.
[LCWL14]
S. Liu, W. Cui, Y. Wu, and M. Liu. A survey on information visualization:
recent advances and challenges. The Visual Computer, pages 1–21, 2014.
[Lew11]
T. G. Lewis. Network science: Theory and applications. John Wiley &
Sons, 2011.
[LNT00]
H. Lu, Y. Y. Ng, and Z. Tian. T-tree or b-tree: Main memory database index
structure revisited.
In Proc. of 11th Australasian Database Conference,
pages 65–73, 2000.

414
Graph-Based Social Media Analysis
[LS03]
Y. A. Liu and S. D. Stoller. From datalog rules to eﬃcient programs with
time and space guarantees. In Proc. of the 5th ACM SIGPLAN Interna-
tional Conference on Principles and Practice of Declarative Programming,
pages 172–183, 2003.
[LS06]
Y. A. Liu and S. D. Stoller. Querying complex graphs. In Practical Aspects
of Declarative Languages, pages 199–214. 2006.
[LWW+13]
S. Liu, Y. Wu, E. Wei, M. Liu, and Y. Liu. Storyﬂow: Tracking the evolution
of stories. IEEE Transactions on Visualization and Computer Graphics,
19(12):2436–2445, 2013.
[MAB+10]
G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, I. Horn, N. Leiser,
and G. Czajkowski. Pregel: a system for large-scale graph processing. In
Proc. of the 2010 ACM SIGMOD International Conference on Management
of Data, pages 135–146, 2010.
[MBDS14]
N. Martinez-Bazan and D. Dominguez-Sal.
Using semijoin programs to
solve traversal queries in graph databases. In Proc. of Workshop on Graph
Data Management Experiences and Systems, pages 1–6, 2014.
[MM08]
C. Muelder and K.-L. Ma. Rapid graph layout using space ﬁlling curves.
IEEE Transactions on Visualization and Computer Graphics, 14(6):1301–
1308, 2008.
[MT01]
B. Mohar and C. Thomassen. Graphs on Surfaces. Johns Hopkins Univer-
sity Press, 2001.
[MW95]
A. O. Mendelzon and P. T. Wood. Finding regular simple paths in graph
databases. SIAM Journal on Computing, 24(6):1235–1258, 1995.
[Pok13]
J. Pokorny. NoSQL databases: a step to database scalability in web envi-
ronment. International Journal of Web Information Systems, 9(1):69–82,
2013.
[RKHS13]
M. Radvansky, M. Kudelka, Z. Horak, and V. Snasel.
Visualization of
social network dynamics using Sammon’s projection. In Proc. of the IEEE
5th International Conference on Computational Aspects of Social Networks,
pages 56–61, 2013.
[RWWE13]
I. Robinson, J. Webber, J. Webber, and E. Eifrem.
Graph Databases.
O’Reilly Media, 2013.
[SCL+09]
L. Shi, N. Cao, S. Liu, W. Qian, L. Tan, G. Wang, J. Sun, and C.-Y. Lin.
HiMap: Adaptive visualization of large-scale online social networks. In Proc.
Visualization Symposium, PaciﬁcVis, pages 41–48, 2009.
[SFMB12]
M. Sedlmair, A. Frank, T. Munzner, and A. Butz. RelEx: Visualization
for actively changing overlay network speciﬁcations. IEEE Transactions on
Visualization and Computer Graphics, 18(12):2729–2738, 2012.
[SG14]
S. Sakr and M. Gaber. Large Scale and Big Data: Processing and Manage-
ment. CRC Press, 2014.
[SHH11]
D. Selassie, B. Heller, and J. Heer. Divided edge bundling for directional
network data. IEEE Transactions on Visualization and Computer Graphics,
17(12):2354–2363, 2011.

Big Graph Storage, Processing and Visualization
415
[SM14]
M. Schich and I. Meirelles.
Arts, humanities and complex networks.
Leonardo, 47(3):265–265, 2014.
[SMDS14]
K. Slaninov´a, J. Martinoviˇc, P. Dr´aˇzdilov´a, and V. Snaˇsel. From Moodle
log ﬁle to the students network. In Proc. International Joint Conference
SOCO13-CISIS13-ICEUTE13, pages 641–650, 2014.
[SP12]
S. Sakr and E. Pardede. Graph data management: techniques and applica-
tions. Information Science Reference, 2012.
[Sri11]
S. Srinivasa. Data, storage and index models for graph databases. In Graph
Data Management: Techniques and Applications, pages 47–70. IGI Global,
2011.
[STT81]
K. Sugiyama, S. Tagawa, and M. Toda. Methods for visual understanding
of hierarchical system structures. IEEE Transactions on Systems, Man and
Cybernetics, 11(2):109–125, 1981.
[SWG02]
D. Shasha, J. T. Wang, and R. Giugno. Algorithmics and applications of
tree and graph searching. In Proc. of the 21st ACM SIGMOD-SIGACT-
SIGART Symposium on Principles of Database Systems, pages 39–52, 2002.
[TGL10]
K. T. Tekle, M. Gorbovitski, and Y. A. Liu. Graph queries through datalog
optimizations. In Proc. of the 12th International ACM SIGPLAN Sympo-
sium on Principles and Practice of Declarative Programming, pages 25–34,
2010.
[TM12]
Y. Tanahashi and K.-L. Ma. Design considerations for optimizing storyline
visualizations. IEEE Transactions on Visualization and Computer Graph-
ics, 18(12):2679–2688, 2012.
[Tut63]
W. T. Tutte. How to draw a graph. Proceedings of the London Mathematical
Society, 13:743–768, 1963.
[VDARS05]
W. M. Van Der Aalst, H. A. Reijers, and M. Song. Discovering social net-
works from event logs. Computer Supported Cooperative Work, 14(6):549–
593, 2005.
[VLKS+11]
T. Von Landesberger, A. Kuijper, T. Schreck, J. Kohlhammer, J. J. van
Wijk, J.-D. Fekete, and D. W. Fellner.
Visual analysis of large graphs:
State-of-the-art and future research challenges. In Proc. Computer Graphics
Forum, volume 30, pages 1719–1749, 2011.
[War13]
C. Ware. Information visualization: perception for design. Elsevier, 2013.
[WF94]
C. Ware and G. Franck. Viewing a graph in a virtual reality display is
three times as good as a 2d diagram. In Proc. IEEE Symposium on Visual
Languages, pages 182–183, 1994.
[WF96]
C. Ware and G. Franck. Evaluating stereo and motion cues for visualizing
information nets in three dimensions.
ACM Transactions on Graphics,
15(2):121–140, 1996.
[WHF93]
C. Ware, D. Hui, and G. Franck. Visualizing object oriented software in
three dimensions. In Proc. of the 1993 Conference of the Centre for Ad-
vanced Studies on Collaborative Research: Software Engineering-Volume 1,
pages 612–620, 1993.

416
Graph-Based Social Media Analysis
[WM08]
C. Ware and P. Mitchell. Visualizing graphs in three dimensions. ACM
Transactions on Applied Perception, 5(1):2, 2008.
[Woo12]
P. T. Wood. Query languages for graph databases. ACM SIGMOD Record,
41(1):50–60, 2012.
[YCHZ12]
X. Yuan, L. Che, Y. Hu, and X. Zhang. Intelligent graph layout using many
users’ input. IEEE Transactions on Visualization and Computer Graphics,
18(12):2699–2708, 2012.
[YFDH01]
K.-P. Yee, D. Fisher, R. Dhamija, and M. Hearst. Animated exploration
of dynamic graphs with radial layout. In Proc. IEEE Symposium on Infor-
mation Visualization, pages 43–43, 2001.
[YLZ+13]
J. Yang, Y. Liu, X. Zhang, X. Yuan, Y. Zhao, S. Barlowe, and S. Liu.
Piwi: Visually exploring graphs based on their community structure. IEEE
Transactions on Visualization and Computer Graphics, 19(6):1034–1047,
2013.
[YYH04]
X. Yan, P. S. Yu, and J. Han. Graph indexing: a frequent structure-based
approach. In Proc. of the 2004 ACM SIGMOD International Conference
on Management of Data, pages 335–346, 2004.
[ZBDS12]
M. Zinsmaier, U. Brandes, O. Deussen, and H. Strobelt. Interactive level-
of-detail rendering of large graphs. IEEE Transactions on Visualization
and Computer Graphics, 18(12):2486–2495, 2012.

(a)
(b)
FIGURE 5.1.1: a) Recommendation network graph, b) YouTube video relation graph.
100
80
60
40
20
0
ITH
QGSO
GNPR
1st rank
Cumulative
GNPR
FIGURE 8.6.4: Geo-location prediction rates (in %) for the compared methods.

(i)
(ii)
(iii)
= 0.00
λ = 0.04
λ = 0.40
λ = 0.20
(iv)
(v)
(vi)
λ = 1.20
λ = 1.49
FIGURE 9.2.1: Stem plots of eigen vectors of Laplacian matrix L for an example graph. The
ﬁlled (empty) nodes contain positive (negative) value. The zero-crossings (edges between a
positive and a negative sample) of the eigenvectors usually increase with increasing eigen-
values λ
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
00
0.2
0.4
0.6
0.8
1
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
00
0.2
0.4
0.6
0.8
1
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
00
0.2
0.4
0.6
0.8
1
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
00
0.2
0.4
0.6
0.8
1
(i) Topic1 distribution on original graph
(ii) Topic1 distribution on downsampled graph after lifting wavelet transform
(iii) Topic2 distribution on original graph
(iv) Topic2 distribution on downsampled graph after lifting wavelet transform
20
33
55
57
7
24 44 52
14
25
49
45
16
53
2
18
31 4
13 34
6
8
38
47
12
36
54
27
50
23
41
28
37
15
35
40
26
39
19
3
46
42
56
22
17
9
1
5
21
11
10
43
48
51
32
29
30
20
16
54
22
17
21
11
26
1
5
10
43
48
50
38
47
12
41
19
3
42
15
8
31 4
18
7
24 44
20
33
55
57
7
24 44
52
14
25
49
45
16
53
2
18
31 4
13
34
6
8
38
54
27
22
17
26
21
9
11
50
39
19
1
5
47
12
23
41
36
28
3
37
46
10
43
48
42
15
35
56
51
32
29
30
40
20
7
24 44
18
31
54
22
17
21
26
38
47
50
39
11
10
5
41
3
46
28
15
42
56
8
29
30
Low-pass nodes
High-pass nodes
FIGURE 9.3.1: Graph representation of blogger network data of N = 59 blogger and M =
3500 blogs. (i) initial distribution of topic1 signal on the graph, (ii) transform coeﬃcients
after a two-level wavelet transform (blue-circles: low-pass nodes and red-circles: high-pass
nodes) similarly for topic2 (iii) initial distribution (signal) and (iv) transform coeﬃcients
after a two-level wavelet transform.

0.11789
0.75042
 kNN graph
Unrated
vertex
(a)
0.11789
1.1475
 complete graph
Unrated 
vertex
(b)
FIGURE 9.3.3: An instance of predicting ratings of an unknown movie node (in red) using
ratings of a known set of movie nodes (in blue), in MovieLens 100k dataset: (i) star graph
and (ii) alternative graph that contains the star graphs and all the links between movies in
the known set of movies.
0
100
200
300
400
500
600
700
1
1.5
2
2.5
3
2
1.5
1
0.5
0
Number of training samples
Cumulative RMSE
 
LS projection (K= K* + 10)
kNN (k = 30)
PMF
Proposed method (K = K*)
Proposed Method with bilateral weights
1-40
41-80
81-120
121-160 161-200 201-240 241-280 281-668
Number of training samples per movie
LS projection (K = K* + 10)
kNN (k = 30)
PMF
Proposed method (K = K*)
Proposed method with bilateral weights
FIGURE 9.3.5: RMSE of diﬀerent prediction algorithms with the number of training samples
on MovieLens dataset.

(a)
(b)
(c)
(d)
1.5
0.17
0.14
0.12
1.0
0.5
0.0
–0.5
–1.0
–1.5–1.5 –1.0 –0.5
0.0
0.5
1.0
1.5
1.5
0.17
0.14
0.12
1.0
0.5
0.0
–0.5
–1.0
–1.5–1.5 –1.0 –0.5
0.0
0.5
1.0
1.5
1.5
0.17
0.14
0.12
1.0
0.5
0.0
–0.5
–1.0
–1.5–1.5 –1.0 –0.5
0.0
0.5
1.0
1.5
1.5
0.17
0.14
0.12
1.0
0.5
0.0
–0.5
–1.0–1.5
–1.0
–0.5
0.0
0.5
1.5
FIGURE 10.2.1: Graph embeddings for a Watts-Strogatz graph with N
=
2, 000:
(a) Centrality-constrained embedding with K1 = (−1/2)J∆(2)J; (b) Centrality-constrained
embedding with K2 = AAT ; (c) Centrality-constrained embedding with K3 = L†; and
(d) Centrality-agnostic embedding based on kernel matrix K1. The color bar maps node
colors to varying centrality values.

(a) Gnutella-04 (08/04/2012)
1.0
0.5
0.0
–0.5
–1.0–1.0
–0.5
0.0
0.5
1.0
1.0
0.5
0.0
–0.5
–1.0–1.0
–0.5
0.0
0.5
1.0
FIGURE 10.2.2: Visualization of two snapshots of the large-scale ﬁle-sharing network
Gnutella [LKF07] based on degree centrality and K = AAT .

0.62
0.60
0.58
0.56
0.54
0.52
0.50
0.48
0.34
0.36
0.38
0.40
0.42
0.44
FIGURE 10.3.1: An anomalous (depicted in red) egonet for the arXiv General Relativity
and Quantum Cosmology collaboration social graph [Les11]. The red egonet is ﬂagged as
anomalous by the proposed low-rank plus sparse matrix decomposition method.
1
0.8
0.6
0.4
0.2
0
102
101
100
101
102
103
0
0.2
0.4
0.6
0.8
1
0
100
200
300
400
500
600
(a)
(b)
D PCP
Centralized solver [28]
FIGURE 10.3.2: (a) A simulated small social network graph with N = 25 nodes. (b) Con-
vergence of the D-PCP algorithm for diﬀerent network sizes. D-PCP attains the same esti-
mation error as the centralized solver.

(a)
(b)
10
20
30
40
50
60
70
80
90
100
10
20
30
40
50
60
70
80
90
100
Team index
FIGURE 10.4.3: (a) Entries of K after removing the outliers, where rows and columns are
permuted to reveal the clustering structure found by robust KPCA; and (b) Graph depic-
tion of the clustered network [MG12b]. Teams belonging to the same estimated conference
(cluster) are colored identically. The outliers are represented as diamond-shaped nodes.
100
10–1
10–2
10–3
10–4
0
200
400
600
800
1000
time
0
200
300
100
400
500
600
700
800
900
1000
time
MSE
100
10–1
10–2
10–3
10–4
MSE
1 iteration
5 iterations
10 iterations
15 iterations
SGD
ISTA
FISTA
ADMM
(a)
(b)
FIGURE 10.5.2: a) MSE (i.e., P
i,j(ˆat
ij −at
ij)2/N 2) performance of Algorithm 10.5.1 versus
time. For each t, (10.5.5) is solved “inexactly” for k = 1, 5, 10, and 15 inner iterations. It
is apparent that k = 5 iterations suﬃce to attain convergence to the minimizer of (10.5.5)
per t, especially after a short transient where the warm-restarts oﬀer increasingly better
initializations. b) MSE performance of real-time algorithms versus time. Real-time FISTA,
Algorithm 10.5.2 (SGD), as well as inexact versions of Algorithm 10.5.1 (ISTA) and the
ADMM solver in [BMG13] are compared.

r1, Sports, 25-30
r6, News, 35-40
r7, News, 25-30
r4, Sports, 35-40
r5, Sports & 
      News, 40-45
r2, Sports, 30-35
r3, Sports, 25-30
r8, News, 40-45
r9, Sports, 30-35
r8, Sports, 35-40
r9, Sports, 25-30
(a)
r1, Sports, 25-30
r6, News, 35-40
r7, News, 25-30
r4, Sports, 35-40
r5, Sports & 
      News, 40-45
r2, Sports, 30-35
r3, Sports, 25-30
r8, News, 40-45
r9, Sports, 30-35 
r8, Sports, 35-40 
r9, Sports, 25-30
v11, Sports
v12, News
(b)
FIGURE 11.6.1: Graph examples for an article reader network with Two Attributes, “Topic”
and “Age” [ZCX10]: a) Attributed graph and b) Attribute augmented graph: two attribute
vertices v11 and v12 representing the topics “News” and “Sports” are added. Dashed lines
connect authors with corresponding topics to the two vertices respectively. Attribute vertices
and edges corresponding to the age attribute are omitted for the sake of clear presentation.
FIGURE 12.5.1: LMS Moodle log sequence visualization.

K25480
w w w . c r c p r e s s . c o m
Focused on the mathematical foundations of social media analysis, Graph-
Based Social Media Analysis provides a comprehensive introduction to the 
use of graph analysis in the study of social and digital media. It addresses an 
important scientific and technological challenge, namely the confluence of graph 
analysis and network theory with linear algebra, digital media, machine learning, 
big data analysis, and signal processing. 
Supplying an overview of graph-based social media analysis, the book provides 
readers with a clear understanding of social media structure. It uses graph the­
ory, particularly the algebraic description and analysis of graphs, in social media 
studies.
The book emphasizes the big data aspects of social and digital media. It pres­
ents various approaches to storing vast amounts of data online and retrieving 
that data in real-time. It demystifies complex social media phenomena, such as 
information diffusion, marketing and recommendation systems in social media, 
and evolving systems. It also covers emerging trends, such as big data analysis 
and social media evolution.
Describing how to conduct proper analysis of the social and digital media mar­
kets, the book provides insights into processing, storing, and visualizing big so­
cial media data and social graphs. It includes coverage of graphs in social and 
digital media, graph and hyper-graph fundamentals, mathematical foundations 
coming from linear algebra, algebraic graph analysis, graph clustering, commu­
nity detection, graph matching, web search based on ranking, label propaga­
tion and diffusion in social media, graph-based pattern recognition and machine 
learning, and graph-based pattern classification and dimensionality reduction.
This book is an ideal reference for scientists and engineers working in social 
media and digital media production and distribution. It is also suitable for use as 
a textbook in undergraduate or graduate courses on digital media, social media, 
or social networks.
Computer Science & Engineering / Data Mining & Knowledge Discovery
Chapman & Hall/CRC 
Data Mining and Knowledge Discovery Series


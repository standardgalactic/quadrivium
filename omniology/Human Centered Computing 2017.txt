Qiaohong Zu
Bo Hu (Eds.)
 123
LNCS 10745
Third International Conference, HCC 2017
Kazan, Russia, August 7–9, 2017
Revised Selected Papers
Human Centered 
Computing
www.allitebooks.com

Lecture Notes in Computer Science
10745
Commenced Publication in 1973
Founding and Former Series Editors:
Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen
Editorial Board
David Hutchison
Lancaster University, Lancaster, UK
Takeo Kanade
Carnegie Mellon University, Pittsburgh, PA, USA
Josef Kittler
University of Surrey, Guildford, UK
Jon M. Kleinberg
Cornell University, Ithaca, NY, USA
Friedemann Mattern
ETH Zurich, Zurich, Switzerland
John C. Mitchell
Stanford University, Stanford, CA, USA
Moni Naor
Weizmann Institute of Science, Rehovot, Israel
C. Pandu Rangan
Indian Institute of Technology, Madras, India
Bernhard Steffen
TU Dortmund University, Dortmund, Germany
Demetri Terzopoulos
University of California, Los Angeles, CA, USA
Doug Tygar
University of California, Berkeley, CA, USA
Gerhard Weikum
Max Planck Institute for Informatics, Saarbrücken, Germany
www.allitebooks.com

More information about this series at http://www.springer.com/series/7409
www.allitebooks.com

Qiaohong Zu
• Bo Hu (Eds.)
Human Centered
Computing
Third International Conference, HCC 2017
Kazan, Russia, August 7–9, 2017
Revised Selected Papers
123
www.allitebooks.com

Editors
Qiaohong Zu
Wuhan University of Technology
Wuhan
China
Bo Hu
Fujitsu Laboratories of Europe Ltd.
Hayes
UK
ISSN 0302-9743
ISSN 1611-3349
(electronic)
Lecture Notes in Computer Science
ISBN 978-3-319-74520-6
ISBN 978-3-319-74521-3
(eBook)
https://doi.org/10.1007/978-3-319-74521-3
Library of Congress Control Number: 2017964220
LNCS Sublibrary: SL3 – Information Systems and Applications, incl. Internet/Web, and HCI
© Springer International Publishing AG 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the
material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now
known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors
give a warranty, express or implied, with respect to the material contained herein or for any errors or
omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland
www.allitebooks.com

Preface
In the past decade, a gradual yet steady paradigm shift was the refocusing of tech-
nologies from machine-oriented concepts, algorithms, and automats toward the max-
imization of human potentials and the fulﬁlment of human needs and human
well-being. Consequently, the development of computational power and computational
efﬁciency has been set in the context of effortless technology utilization by both
individuals and societies. With the recent advances in human-machine interfaces,
wireless and mobile network technologies, and data analytics, we ﬁnally set out on the
long journey toward making computer services truly human-centric, instead of limiting
human capacities to suit computer requirements. Instead of playing a prominent role in
the behavior of individuals and societies, machines and their very existence have
become ever more subtle: smart devices have been weaved into the everyday fabric;
analytic power has penetrated into all aspects of our daily activities from family to
work (and everything in between); and automation has begun to displace segments of
our routines. For an area as active as human-centered computing, it is not possible to
cover the entire thematic spectrum. Instead, the Human-Centered Computing
(HCC) conference aims to present a selection of examples of new approaches, methods,
and achievements that can underpin the aforementioned paradigm shift.
HCC 2017 was the third in the series, following successful events in Phnom Penh,
Cambodia (2014), and Columbo, Sri Lanka (2016). The HCC 2017 papers present a
balance between conceptual and empirical work, between design and evaluation
studies, and between theoretical and applied research.
All HCC 2017 submissions went through rigorous paper evaluation and selection
process. Each paper was peer-reviewed by the Program Committee and selected
reviewers and meta-reviewed by senior Program committee members. Based on these
recommendations, the program co-chairs made acceptance decisions and classiﬁed the
accepted papers into the following categories: full regular papers, short papers, and
position papers.
It has been another year of hard-working and selﬂess contribution. As the confer-
ence Organizing Committee, we are grateful to all members of the Technical Program
Committee — it was their hard work that enabled us to identify a set of high-quality
submissions reﬂecting the trends and interests of the paradigm shift. We would like to
extend our gratitude to the international Advisory Committee for its invaluable advice
and guidance. Finally, our special thanks also goes to the additional reviewers, student
volunteers and members of the local organization team, who are the key elements that
made HCC2017 a successful event.
August 2017
Max Talanov
Salvatore Distefano
Qiaohong Zu
Bo Hu
Vlada Kugurakova
www.allitebooks.com

Organization
Honorary Conference Chair
Airat Khasianov
Kazan Federal University, Russia
Conference Co-chairs
Max Talanov
ITIS, Kazan Federal University, Russia
Yong Tang
South China Normal University, China
Program Committee Co-chairs
Salvatore Distefano
Università degli Studi di Messina, Italy
Jing He
Victoria University, Australia
Organizing Committee
Chengzhou Fu
South China Normal University, China
Vlada Kugurakova
Kazan Federal University, Russia
Qiaohong Zu
Wuhan University of Technology, China
Publication Committee
Bo Hu
Fujitsu Labs of Europe, UK
Philip Moore
Birmingham City University, UK
Secretariat
Jizheng Wan
Coventry University, UK
www.allitebooks.com

Abstracts of Invited Keynotes
www.allitebooks.com

IoT+AI: Opportunities and Challenges
Huadong Ma
School of Computer Science, Beijing University of Posts
and Telecommunications, Beijing, China
mhd@bupt.edu.cn
The Internet of Things (IoT) can enable the interconnection and integration of the
physical word and the cyber space, and has been widely considered as the kernel
technology for sensing the urban environments and providing smart services further. At
the same time, the rapid development of Artiﬁcial Intelligence (AI) brings many
opportunities to IoT. In this talk, we ﬁrst introduce the challenges of urban sensing
networks. Combing AI theory, we discuss some researches on sensing, networking and
computing, and service in the IoT environment. Finally, we outline the prospects of IoT
development.
Biograph
Dr. Huadong Ma is a Chang Jiang Scholar Professor and Executive Dean, School of
Computer Science, Beijing University of Posts and Telecommunications (BUPT),
China. He is also Director of Beijing Key Lab of Intelligent Telecommunications
Software and Multimedia, BUPT. He is Chief Scientist of the project “Basic Research
on the Architecture of Internet of Things” supported by the National 973 Program of
China from 2010 to 2013. He received his PhD degree in Computer Science from the
Institute of Computing Technology, Chinese Academy of Science in 1995. From 1999
to 2000, he held a visiting position in the Department of Electrical Engineering and
Computer Science, The University of Michigan, Ann Arbor, USA. His current research
focuses on sensor networks and Internet of things, multimedia computing, and he has
published over 200 papers in journals (such as ACM/IEEE Transactions) or Confer-
ences (such as IEEE INFOCOM, ACM MM) and 4 books on these ﬁelds. As a
co-author, he got the best paper award in IEEE ICPADS2010 and the best student
paper award in IEEE ICME2016. He was awarded National Funds for Distinguished
Young Scientists in 2009. He serves for Chair of ACM SIGMOBILE China.
www.allitebooks.com

Machine Learning for Real-World:
Is Deep Learning Enough?
Adil Khan
Department of Computer Science, Innopolis University, Innopolis,
Respublika Tatarstan, Russian Federation
a.khan@innopolis.ru
The real world is unpredictable; it is full of noise and ﬁlled with novel scenarios.
Therefore, it is almost impossible to provide the machine learning models with a
complete representation of such a world at the time of training, forcing us to work with
an insufﬁcient picture of our world. That is why one of the biggest problems that the
ﬁeld of machine learning still faces today is the inability of the learned models to
generalize well to scenarios that are different from the ones seen at the time of training.
In this talk, we will explore this problem in four different areas: Human Activity
Recognition, Emotion Recognition in Text, User Authentication, and Medical Image
Analysis. We will see the results of applying deep learning models to these problems,
and discuss ways that may be used in addition to the use of such models to achieve
optimum performance.
Biograph
Dr. Adil Khan is an Associate Professor in the Department of Computer Science at
Innopolis University, Russia. He is also the head of Machine Learning & Knowledge
Representation Lab at Innopolis. He has received his Ph.D. in Computer Engineering
from Kyung Hee University, South Korea. He has over 11 years of experience in
academic research and teaching. His primary research interests include machine
learning, computer vision, data analytics, wearable computing, and context-aware
computing. His research work, comprising over 60 articles, is published in various
international conferences and journals. He is currently participating in several inter-
national research projects. He is an IEEE member and is also a reviewer for numerous
IEEE, ACM, Elsevier and other international journals.
www.allitebooks.com

Contents
An Improved MST Clustering Algorithm Based
on Membrane Computing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
Ping Gong and Xiyu Liu
A Method for Correcting the Leaning of AR Two-Dimensional
Codes Based on LSM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
Chen Miao, Shufen Liu, and Zhilin Yao
The Associated Algorithm of Target Batch Number Based
on Gaussian Mixture Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
Ning Liu, Shufen Liu, and Xinjia Zhang
Density Peaks Clustering Based on Improved RNA Genetic Algorithm . . . . .
28
Liyan Ren and Wenke Zang
Local Homogeneous Weighted Spiking Neural P Systems . . . . . . . . . . . . . .
34
Mengmeng Liu and Feng Qi
Weight-Improved K-Means-Based Consensus Clustering . . . . . . . . . . . . . . .
46
Yanhua Wang, Laisheng Xiang, and Xiyu Liu
A Novel Trace Clustering Technique Based on Constrained
Trace Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
Pan Wang, Wen’an Tan, Anqiong Tang, and Kai Hu
A Fast Local Image Descriptor Based on Patch Quantization . . . . . . . . . . . .
64
Tian Tian, Fan Yang, Kun Zheng, Hong Yao, and Qian Gao
A Method of Large - Scale Log Pattern Mining . . . . . . . . . . . . . . . . . . . . .
76
Lu Li, Yi Man, and Mo Chen
More Efficient Filtration Method for Big Data
Order-Preserving Matching. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
Wenchao Jiang, Dexi Lin, Sui Lin, Chuanjie Li, and Aobing Sun
Blind Image Quality Assessment via Analysis of GLCM . . . . . . . . . . . . . . .
95
Guanghui Yue, Chunping Hou, Tongtong Ma, and Yang Yang
An Improved Ranked K-medoids Clustering Algorithm Based
on a P System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
102
Bao Zhang, Laisheng Xiang, and Xiyu Liu

A Novel Inverse-Operation Based Group Undo/Redo Algorithm
for Feature-Based 3D Collaborative CAD Systems . . . . . . . . . . . . . . . . . . .
108
Yuan Cheng, Fazhi He, Xiao Lv, and Weiwei Cai
Two Steps Method Using Polarization to Resist Phase Noise
for Self-interference Cancelation in Full-Duplex . . . . . . . . . . . . . . . . . . . . .
118
Fangfang Liu, Xinyi Wang, Chunyan Feng, and Xiao Han
Sparse Linear Method Based Top-N Course Recommendation System
with Expert Knowledge and L0 Regularization . . . . . . . . . . . . . . . . . . . . . .
130
Jinjiao Lin, Haitao Pu, Yibin Li, and Jian Lian
The Research on the Container Truck Scheduling Based on Fuzzy
Control and Ant Colony Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
139
Meng Yu, Dawei Li, and Qiang Wang
Researches on the Analysis Framework of Application Layer
Communication Protocol Based on SQLite. . . . . . . . . . . . . . . . . . . . . . . . .
150
Wenyuan Xu, Hao Li, and Weifeng Xu
The Implementation of Growth Guidance Factor Diffusion via Octree
Spatial Structures for Neuronal Systems Simulation. . . . . . . . . . . . . . . . . . .
158
Almaz Sabitov, Fail Gafarov, Vlada Kugurakova, and Vitaly Abramov
Intelligent Perceptive QoS Based Centralized Admission Control
for Route Computing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
164
Xuejing Li, Yajuan Qin, Haohui Fu, Zhewei Zhang, and Xiaorong Lin
Research on the Shortest Path Problem Based on Improved
Genetic Algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
173
Baoliang Wang, Susu Yao, Kaining Lu, and Huizhen Zhao
Semantic Web Languages for Policy Enforcement
in the Internet of Things . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
183
Rustem Dautov and Salvatore Distefano
Network Traffic Prediction Based on Wavelet Transform
and Genetic Algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
189
Xuehui Zhao, Wanbo Zheng, Lei Ding, and Xingang Zhang
Research and Optimization of the Cluster Server Load Balancing
Technology Based on Centos 7. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
201
Lei Ding, Wanbo Zheng, Shufen Liu, and Zhian Han
Base Station Location Optimization Based on Genetic Algorithm
in CAD System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
208
Yanhua Wang, Laisheng Xiang, and Xiyu Liu
XII
Contents

A Study of Optimal Multi-server System Configuration with Variate
Deadlines and Rental Prices in Cloud Computing . . . . . . . . . . . . . . . . . . . .
215
Zhongfeng Kang and Bo Yang
A Composite Anomaly Detection Method for Identifying
Network Element Hitches. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
232
Duo Zhang, Yi Man, and Ligang Ren
D-SVM Fusion Clustering Algorithm Based on Indoor Location. . . . . . . . . .
245
Zhongliang Deng, Jiachen Fan, and Jichao Jiao
Research on Evaluation of Sensor Deviations During Flight . . . . . . . . . . . . .
252
Yuping Xiong, Shufen Liu, Sihua Gao, and Yemei Zhu
A Distributed Routing Algorithm for LEO Satellite Networks. . . . . . . . . . . .
258
Jundong Ding, Yong Zhang, Ruonan Li, and Liang Zhao
Optimization of Smart Home System Based on Wireless
Sensor Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
265
Xing Guo and Neng Hu
The Energy Efficiency Research of Traffic Offloading Mechanism
for Green Heterogeneous Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
274
Kaili Wu, Yifei Wei, Qiao Li, Da Guo, and Mei Song
A Game-Theoretic Approach for Joint Optimization of Sensing
and Access in Cognitive Cellular Heterogeneous Networks . . . . . . . . . . . . .
288
Changqing Pan, Ya’nan Xiao, Yinglei Teng, Weiqi Sun,
and Xiaoqi Qin
Energy Harvesting Relay Node Deployment for Network
Capacity Expansion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
301
Zhiqiang Zhang, Yifei Wei, Bo Gu, Xaojun Wang, and Mei Song
Parameters Optimization for KFKM Clustering Algorithm Based
on WiFi Indoor Positioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
311
Zhengying Hu, Lujuan Ma, Baoling Liu, and Zhi Zhang
Energy Harvesting Time Coefficient Analyze for Cognitive Radio
Sensor Network Using Game Theory. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
318
Mengyu Zhao, Yifei Wei, Qiao Li, Mei Song, and Ningning Liu
Traffic Paralysis Alarm System Based on Strong Associated Subnet . . . . . . .
330
Chen Yu, Shaohui Zhu, Hanhua Chen, Ruiguo Zhang, Jiehan Zhou,
and Hai Jin
Contents
XIII

Fault Recovery Algorithm Based on SDN Network. . . . . . . . . . . . . . . . . . .
342
Yi Zhang, Yifei Wei, Ruqin Huang, Bo Gu, Yue Ma,
and Mei Song
Sensor Location Verification Scheme Based on Range-Free
Localizations in WSNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
354
Chunyu Miao, Lina Chen, and Qingzhang Chen
Design and Development of Parametric System for Planetary Reducer. . . . . .
364
Yangpeng Chen, Menglun Tao, Dingfang Chen, Bo Li,
Yanfang Yang, and Boting Chen
A Dynamic Double Threshold Based Cooperative Spectrum Sensing
Strategy in Heterogeneous Cognitive Radio Networks . . . . . . . . . . . . . . . . .
376
Chongxiao Peng, Yifei Wei, Bo Gu, Ligang Ren, and Mei Song
A Distributed Self-adaption Cube Building Model Based on Query Log . . . .
382
Meina Song, Mingkun Li, Zhuohuan Li, and Haihong E.
Property-Based Network Discovery of IoT Nodes Using Bloom Filters . . . . .
394
Rustem Dautov, Salvatore Distefano, Oleg Senko, and Oleg Surnin
The Personality Analysis of Characters in Vernacular Novels
by SC-LIWC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
400
Yahui Yuan, Baobin Li, Dongdong Jiao, and Tingshao Zhu
Digging Deep Inside: An Extended Analysis of SCHOLAT
E-Learning Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
410
Aftab Akram, Chengzhou Fu, Yong Tang, Yuncheng Jiang,
and Kun Guo
Social Network Analysis of China Computer Federation
Co-author Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
422
Chengzhou Fu, Weiquan Zeng, Rui Ding, Chengjie Mao,
Chaobo He, and Guohua Chen
Detecting Postpartum Depression in Depressed People
by Speech Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
433
Jingying Wang, Xiaoyun Sui, Bin Hu, Jonathan Flint,
Shuotian Bai, Yuanbo Gao, Yang Zhou, and Tingshao Zhu
Research on Network Public Opinion Propagation Mechanism
Based on Sina Micro-blog . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
443
Weidong Huang, Qian Wang, and Yixuan Wang
XIV
Contents

Scholar Recommendation Model in Large Scale Academic Social
Networking Platform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
453
Ming Chen, Chunying Li, Jiwei Liu, Dejie Meng,
and Yong Tang
Research on Simulation for Fuel Consumption of UAV . . . . . . . . . . . . . . . .
465
Zongpu Jia, Yonghui Shi, Songyuan Gu, and Shufen Liu
Research on Meteorological Data Simulation . . . . . . . . . . . . . . . . . . . . . . .
472
Zhenglun Wu, Shufen Liu, and Tie Bao
Anti-data Mining on Group Privacy Information . . . . . . . . . . . . . . . . . . . . .
481
Fan Yang, Tian Tian, Hong Yao, Xiuyu Zhao,
Tinggang Zheng, and Min Ning
Big Data Analysis of Reviews on E-commerce Based on Hadoop . . . . . . . . .
492
Qiaohong Zu and Jiangming Wu
Analysis on Structural Vulnerability Under the Asymmetric Information . . . .
503
Mingshu He, Xiaojuan Wang, Jingwen You, and Zhen Wang
Combining Link and Content Correlation Learning for Cross-Modal
Retrieval in Social Multimedia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
516
Longtao Zhang, Fangfang Liu, and Zhimin Zeng
The Research on Touch Gestures Interaction Design for Personal
Portable Computer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
527
Qing Sheng, Ting Liu, Wenjun Hou, and Gengyi Wang
Research on Mobile User Dynamic Trust Model Based
on Mobile Agent System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
538
Weijin Jiang and Yuhui Xu
A Group Decision-Making Method Based on Evidence Theory
in Uncertain Dynamic Environment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
550
Weijin Jiang and Yuhui Xu
Analysis and Estimate the Effect of Knowledge on Software
Reliability Distribution. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
561
Chunhui Yang, Yan Gao, Xuedong Kong, Dingfang Chen,
Shengwu Xiong, and Jianwen Xiang
Development of Virtual Reality-Based Rock Climbing System . . . . . . . . . . .
571
Yiming Su, Dingfang Chen, Congxing Zheng, Sihan Wang,
Liwen Chang, and Jie Mei
Contents
XV

Kinematics and Simulation Analysis of a 3-DOF Mobile
Handling Robot Based ADAMS and MATLAB . . . . . . . . . . . . . . . . . . . . .
582
Jingbo Hu, Dingfang Chen, Lijie Li, Jie Mei, Qin Guo,
and Huafeng Shi
A New Type of 3D Printing Nozzle with PET Wire
as Raw Material . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
588
Yawei Hong, Shaobo Li, Shupei Wu, Tianhao Huang,
Guang Liu, Liwen Chang, and Yang Zhang
When Partitioning Works and When It Doesn’t: An Empirical
Study on Cache Way Partitioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
595
Hanfeng Qin
Track Maintenance Feedback Mechanism Based on Hadoop
Big Data Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
608
Yong Zhu, Jiawei Fan, Guangyue Liu, Mou Wang,
and Qian Wang
Crowdsourcing and Stigmergic Approaches for (Swarm) Intelligent
Transportation Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
616
Salvatore Distefano, Giovanni Merlino, Antonio Puliafito,
Davide Cerotti, and Rustem Dautov
Research on Visual Feedback Based on Natural Gesture . . . . . . . . . . . . . . .
627
Wenjun Hou, Shupeng Zhang, and Zhiyang Jiang
Information Security Technology and Application in Logistics
Traceability System of Aquatic Products Based on QR Code . . . . . . . . . . . .
638
Qiaohong Zu and Rui Chen
Quantified Self: Big Data Analysis Platform for Human Factor Efficacy
Evaluation of Complex System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
648
Chaoqiang Li, Wenjun Hou, Xiaoling Chen, and Hao Li
Application of Speech Recognition Technology in Logistics
Selection System. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
654
Tianzheng Fu and Bin Sun
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
661
XVI
Contents

An Improved MST Clustering Algorithm
Based on Membrane Computing
Ping Gong and Xiyu Liu(&)
School of Management Science and Engineering,
Shandong Normal University, Jinan 250014, Shandong, China
gongping_sd@163.com, sdxyliu@163.com
Abstract. MST clustering algorithm can detect data clusters with irregular
boundaries. For a weighted complete graph the feasible solutions to the MST
problem is non-unique. Membrane computing known for its characteristics of
distribution and maximal parallelism can properly reduce the complexity of
processing a MST of a graph. This paper combines MST clustering algorithm
and membrane computing by designing a speciﬁc P system. The designed P
system realizes the process of an improved MST clustering by collecting all
feasible solutions to the MST problem together preserving proper edges and
deleting redundant heavy edges. The improved MST clustering method efﬁ-
ciently enhances the quality of clustering and proved to be feasible through an
instance.
Keywords: MST clustering algorithm  Membrane computing
P system
1
Introduction
Clustering is the process of partitioning a set of data objects into subsets, making each
subset a cluster, such that objects in identical clusters are similar to each other, yet
dissimilar to objects in other clusters. Clustering has been widely used in many
applications such as business intelligence, image pattern recognition, Web search,
biology, and security [1].
There are many kinds of algorithms in the literature to solve clustering problems,
mainly including partitioning methods, hierarchical methods, density-based methods
and grid-based methods and so on. Among various kinds of clustering methods, the
minimum spanning tree (MST) clustering algorithm is known to be capable of
detecting clusters with irregular boundaries, because it does not assume a spherical
shaped clustering structure of the underlying data. MST constructing problems has
been investigated by researchers since 1926 which also makes it a relatively mature
algorithm. Many researches on this algorithm have promised close to linear time
complexity of construction cost [2, 3]. Standard MST clustering algorithms basically
include sorting the edges in constructed graph and removing the edges with heaviest
weight. Applying MST algorithm to solve clustering problems has been investigated,
but in practical applications, clustering results of MST algorithms are easy to be
affected by outliers [3].
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 1–12, 2018.
https://doi.org/10.1007/978-3-319-74521-3_1

Membrane computing is a new branch of natural computing, which have been
investigated and proved to be universal and efﬁcient. Membrane computing models
(named P system) are motivated by the parallel characteristic of biochemical reactions
taking place in a series of regions of a living cell. This computing theory is also
motivated by the mathematical convenience of this kind of parallelism. P systems owe
the characteristics of distribution and maximal parallelism [4]. Thus, membrane
computing approaches are more suitable applied to combinatorial problems, graph
theory, and ﬁnite state problems.
On account of the non-uniqueness of feasible solutions to the MST problem and the
lack of researches on MST clustering in membrane computing ﬁelds, this paper
combines membrane structures with MST clustering algorithm together. Make P sys-
tem an efﬁcient computing tool during the process of solving MST clustering problem
to reduce the time complexity. It is no doubt that combination will greatly improve the
quality and efﬁciency of ﬁnding the best clustering results.
2
Preliminary
2.1
P System
Membrane computing, introduced by PAUN in 1998, takes the living cell as multi-
hierarchical structural regions, which are surrounded by the so-called membranes.
There are three main investigated variants of P systems, cell-like, tissue-like, and
neural-like. Cell-like P System imitates the function and structure of the cells, and it
includes the membrane structure, rules and objects as basic elements. Cell-like
arrangements of membranes correspond to trees. And some P systems have been
proposed to solve computer science related problems, like NP problems, arithmetic
operations, matrix vector computation and image processing. P systems have also been
proved to be effective when being studied with clustering problems [6].
Membranes divide the whole system into different regions. The skin membrane is
the outermost membrane. A membrane is a basic membrane if there are no membranes
in it and a membrane is a non-elemental membrane otherwise. Rules and objects exist
in regions. Usually the objects are indicated by strings or characters. Rules are used to
process objects or membranes in corresponding region. The rules are executed
uncertainly and maximum concurrently. Cell-like P System can be further divided into
three types from according to kinds of rules: transition P system, P system with
communication rules and P system with active membranes [7]. The basic membrane
structure is shown in Fig. 1.
In general, a P system of degree m is a construct:
Y
¼ V; T; C; H; l; w1; w2; . . .; wm; ðR1; q1Þ; ðR2; q2Þ; . . .; ðRm; qmÞ
ð
Þ
Where:
1. V is an alphabet. Elements in it are called objects;
2. T  V is the output objects;
2
P. Gong and X. Liu

3. C  V −T is the catalyst. These catalysts neither change their numbers nor their
kinds in rules. But rules cannot be executed without these catalysts;
4. H = {1, 2, m} is the set of membrane labels.
5. µ is a membrane structure; each membrane has its label;
6. wiði ¼ 1; 2; mÞ is the objects in membrane i;
7. The basic rule is in the form of (u ! v), u is a string composed of objects in V and
v is a string in the form of v = v′ or v = v′d. v′ is a string over {here, out, in 2 V,
1  j  m}.
8. Ri is the set of the rules in region i. qi is the precedence relation which deﬁnes the
partial order relation over Ri. High priority rule is executed prior [8].
The rules are used in maximum parallel and uncertainly in each membrane when
calculating. So space of exponential growth can be generated in linear operation steps.
This is very helpful to solve the computationally hard problems within feasible time.
The P system will halt after some steps if no more rules can be executed and these
objects in output membrane is the ﬁnal result. The P system will not halt if rules
are always executed, then this calculation is invalid, and there is no result being
outputted [9].
2.2
MST Clustering Method
A tree is a simple structure for representing binary relationships, and any connected
component of a tree is called a sub-tree. A spanning tree is an acyclic connected
sub-tree of a graph G, which contains all the vertices from G. And the minimum
spanning tree (MST) is the one with the minimum weight. By representing the data set
in a graph, ﬁnding the corresponding minimum spanning tree, the structure charac-
teristics of data can be got to some extent.
Regions
Skin
Environment
Membranes
Elementary membrane
4
3
1
6
5
8
7
2
Fig. 1. The basic membrane structure
An Improved MST Clustering Algorithm Based on Membrane Computing
3

MST related theories have been widely used for data classiﬁcation in the ﬁeld of
pattern recognition and image processing for about forty years. As classical algorithms
rely on either the idea of grouping data around some ‘centers’ or the idea of separating
data points using some regular geometric curve like a hyper-plane, they generally do
not work well when the boundaries of the clusters are very complex. An MST is quite
invariant to detailed geometric changes in the boundaries of clusters. As long as the
relative distances between clusters do not change signiﬁcantly, the shape complexity of
a cluster has very little effect on the performance of our MST-based clustering algo-
rithms. And the process is quite simple. Remove the k−1 largest weighted edges from
the constructed minimum spanning tree, and k sub-trees obtained can just be regarded
as k clusters [10]. Two key advantages in representing a set of multi-dimensional data
as an MST are: 1. The simple structure of a tree facilitates efﬁcient implementations of
rigorous clustering algorithms, which otherwise are highly computationally challeng-
ing; and 2. As an MST-based clustering does not depend on detailed geometric shape
of a cluster, it can overcome many of the problems faced by classical clustering
algorithms.
Representing a set of multi-dimensional data points as a simple tree structure will
lose some of the inter-data relationship. But during the process of simplifying the data
set into a MST, essential intra-data information is remained in MST tree. Each cluster
corresponds to one sub-tree, which does not overlap the representing sub-tree of any
other clusters. And through MST representation, we can convert a multi-dimensional
clustering problem to a tree partitioning problem, just like ﬁnding a particular set of tree
edges and cutting them. And ﬁnding a globally optimal solution for a combinatorial
optimization problem is often possible.
3
A P System for Improved MST Clustering Method
3.1
The Improved MST Clustering Method
MST clustering methods attempt to represents a data set into a tree graph with the
minimum edge weights, then cut k−1 longest edges in it to form k clusters. In this
section, we propose a new method to ﬁnd a minimum spanning tree, and implement the
MST constructing rules in a membrane system. By applying the membrane structure as
a computing tool, we can ﬁnd all feasible solutions to an MST problem in the process.
And ﬁnally we optimize the process of cutting the longest k−1 edges to ﬁnd a better
clustering solution.
Firstly, we measure similarity between data points as distances of objects, and
express in a matrix W0
nn
W0
nn ¼
w0
11
w0
12
. . .
w0
1n
w0
21
w
0
22
. . .
w0
2n
. . .
. . .
. . .
. . .
w0
n1
w0
n2
. . .
w0
nn
2
664
3
775
4
P. Gong and X. Liu
www.allitebooks.com

Where w0
ij is the distance between point ai and aj. To facilitate calculation, W0
nn is
totally converted into corresponding integer matrix Wnn as follows:
Wnn ¼
w11
w12
. . .
w1n
w21
w22
. . .
w2n
. . .
. . .
. . .
. . .
wn1
wn2
. . .
wnn
2
664
3
775
Accordingly, every wij here means the weight of edge eij connecting vertex ai and aj.
The ﬁrst part of MST clustering process is to construct minimum spanning trees:
We set V as vertex set and E as edge set, and they both are empty sets initially; rank all
edges in ascending order; then add the shortest edge eij (with the smallest weight) into
the set E, and accordingly add the associated two vertex ai and aj into the set V; other
edges are added into set E gradually, which are supposed to be connected to one of the
existing vertices and with smallest weight in the remaining edges; after an edge is
added into E, we add the other vertex connected to it into V; while once there are two
identical vertices in V, it means that there is a circle in tree. And this edge will be
abandoned. Above steps are repeated until |V| = n and |E| = n−1(|V| is the number of
vertices in V; |E| is the number of edges in E).
For a complete graph, especially a graph with a mass of vertex, there are edges with
identical weights. Therefore, the minimum spanning tree of a given complete graph is
not unique and only if the shape of the minimum spanning tree is not unique. To ﬁnd
all feasible solutions, we are supposed to choose every different edge with the same
weights every time when constructing a MST. But obviously, such enumerating pro-
cess will take too much time.
One of the most advantages of membrane computing is its parallelism. Membrane
structure is hierarchical, and computing rules in regions evolve react and communicate
synchronously. For this reason, we apply this parallelism to constructing all minimum
spanning trees of a complete graph at a time by duplicating and generating new reaction
membranes with rules. When there are more than one edges can be chosen, we can
construct new computing regions, where we add every feasible new edge into the tree.
When the process of adding edges and vertices to corresponding sets ends, the next
step is to partition the produced MST. An edge with a large weight means that the two
vertices connected by it are distinct from each other. For this reason, we choose to
delete k−1 maximum value edges from the existed set E to form k clusters. Since there
are more than one MST feasible solutions collected, for these always appearing short
edges, we can conclude that they are correct elements of sub-tree which should be
preserved in partitions. We rearrange edges which used to belong to set E but now
appear in different membranes as parts of MST solutions with their frequency of
occurrences in descending order. Then we delete redundant edges one by one. For
edges with the same value, we preferentially preserve ones with higher frequency; for
edges with same frequency, we preferentially delete ones with heavier weights. To
make sure that every point is kept in set V, when an edge is to be deleted, the existence
of two adjacent vertices must be checked. Repeat this process until |E| = N−K.
An Improved MST Clustering Algorithm Based on Membrane Computing
5

The procedure of our method is as below:
3.2
Deﬁnition of the P System for Improved MST Clustering Method
P system deal with multi-sets of symbol objects in distributed and parallel manner. So
we design a speciﬁc P system for the improved MST clustering method to collect all
6
P. Gong and X. Liu

MST feasible solutions and partition the dataset. The designed P system is shown as
Fig. 2.
Y
¼ O; l; M0; M1; Mi0; R0; R1; Ri0; q
ð
Þ
Where:
O ¼ c11a1a2. . .anh1k
f
g speciﬁes the initial collection of objects in the P system;
l ¼ ½0½11 ½i0i00 speciﬁes the initial membrane structure of the P system;
M0 ¼ c11a1a2. . .an
f
g speciﬁes the initial objects in membrane 0;
M1 ¼ h1
f
g speciﬁes the initial object in membrane 1;
Mi0 ¼ k
f g speciﬁes that there is nothing in the output membrane i0 initially;
Rule in R0:
r1 ¼ fcijaiaj ! ciðj þ 1Þaiaj½Uwij
ij in1j1  i; j  ng [
fciðn þ 1Þ ! cði þ 1Þði þ 2Þj1  i; j  ng [ fcnðn þ 1Þ ! kg
Rules in R1:
r2 ¼ fh1Uwij
ij ! h1Uwij1
ij
j1  i; j  ng
r3 ¼ ffh1Ut
ij ! qiqjvivjPijh2Acjt ¼ 0; 1  i; j  ng
[ ffh1Ut
i1j1    Ut
isjs ! ½qi1qj1vi1vj1Pi1j1Ach21    ½qisqjsvisvjsPisjsAch21
jt ¼ 0; 1  i; j  ng
r4 ¼ ffh2qiUwis
is ! fh2qiUwis1
is
j1  i; s  ng [
ffh2qjUwsj
sj ! fh2qjUwsj1
sj
j1  s; j  ng
r5 ¼
fh2vivsUt
is ! fh2vivsjt ¼ 0; 1  i; s  n


0
1
0i
1θ
λ
11
1
2...
n
c a a
a
Fig. 2. The initial state of the designed P system
An Improved MST Clustering Algorithm Based on Membrane Computing
7

r6 ¼ ffh2viUt
is ! qsvivsxsPish2jt ¼ 0; 1  i; s  ng [
ffh2vjUt
sj ! qsvsvjxsPsjh2jt ¼ 0; 1  s; j  ng
r7 ¼ f½fh2
2vsxiPisAc1 ! ½qsvsPisAch21½Ach2f1j1  i; j  ng [
f½fh2
2vsxjPsjAc1 ! ½qsvsPsjAch21½Ach2f1j1  i; j  ng
r8 ¼ h2 ! d0
f
g
r9 ¼
qiviUij ! k d0j


r10 ¼
Pij ! ½gPiji0 d0j
n
o
r11 ¼ d0 ! d
f
g
r12 ¼ f ! d
f
g
r13 ¼ Ac ! k dj
f
g
r14 ¼ d ! d
f
g
Rules in Ri0:
r15 ¼ fgtPm1
i1j1Pm2
i2j2    Pmt
itjt ! gt1Pm11
i1j1
Pm21
i2j2
   Pmt1
itjt
j
t 2 N þ ; mi 2 N þ ; m1  m2      mt; 1  i; j  ng
r16 ¼ fgtmnPðm1Þ0
i1j1 Pðm2Þ0
i2j2
   Pðmn1Þ0
in1jn1 ! P
wi1j1
i1j1 P
wi2j2
i2j2    P
win1jn1
in1jn1 j
1  t; m 2 N þ ; m1  m2      mn1; 1  is; js  ng
r17 ¼ fP
wi1j1
i1j1 P
wi2j2
i2j2 . . .P
win1jn1
in1jn1 ! P
wi1j11
i1j1
P
wi2j21
i2j2
. . .P
win1jn11
in1jn1
j1  is; js  ng
r18 ¼ fP
ðwis1 js1 Þ0
i1j1
P
ðwis2 js2 Þ0
i2j2
. . .Pisjs. . .P
ðwisjsnk Þ0
in1jn1
!
g2PisjsP
ðwis1 js1 Þ01
i1j1
P
ðwis2 js2 Þ01
i2j2
. . .. . .P
ðwisjsnk Þ01
in1jn1
g
r19 ¼ fgnk
2
Pis1js1Pis2js2. . .Pisnk jsnk . . .P
ðwisjsnk Þ00
isn1jsn1 !
Pis1js1Pis2js2. . .Pisnk jsnk j1  is; js  ng
By counting the amount of objects to ﬁnd the shortest edges, the sorting procedure
is combined with the tree construction. Meanwhile the parallelism of computation
reduces the time complexity of ﬁnding all feasible solutions to OðnÞ.
3.3
Introduction of Computations
Rule r1 is executed ﬁrstly in the environment to calculate the direct distance between
any two points and send it to membrane1. After objects uwij
ij are sent into membrane1,
rule r2 and r3 are activated to identify the shortest edge from all edges and generate two
related points among these uwij
ij . If the number of edges with minimum weight of the
complete graph is greater than 1, rule r3 generates same new membranes (membrane1)
for every initial minimum edge to continue the next steps.
8
P. Gong and X. Liu

Rule r4 continually identiﬁes edges which are connected to the existed points and
calculates which the minimum one is. When ut
is ¼ 0 appears, it means the next edge
with minimum weight is ﬁnd out. Adding a minimum weight edge, two identical
objects vi may appear in same membrane. That means there is a circle in the minimum
spanning tree. Rule r5 are activated to dissolve the according uis to avoid forming a
circle. Then rule r6 generates relative objects to represent the edge and point.
If the number of current edges with minimum weight is greater than 1, rule r7
activates the current membrane to divide into two, with one containing all elements
including the new added edge but another with all elements except the new edge.
The number of object f is set as n−1, and as every edge is added, it reduces by 1.
When there is no f in membrane1, it means that n−1 edges have been added into the
graph. Object h2 generates object d0 without the constraint of f which catalyzes other
objects than Pij to dissolve themselves. It also promote Pij conveyed into membrane i0.
While if there are objects f but no other edges to be added, rule r12 r13 r14 dissolve the
current membrane and other objects.
The second computing stage of this structure happens in membrane i0. Rule r8 
r11 sends the chosen edges for a minimum spanning tree into membrane i0. In every
membrane1, executing rule r8  r11, there is a feasible solution to the minimum
spanning tree. As a consequence, there will be more than n–1 edges existing in
membrane sent to i0, including identical edges. Rule r15  r19 is executed in membrane
i0. The number of object g indicates the number of edges in membrane i0. We choose
the edge with maximum weights and highest frequency. Rule r18 executes until the
cardinal of g2 reducing to n−k (n−1−(k−1) = n−k), which means there are only n−k
different edges left. At this time, Rule r19 ﬁnds out the ﬁnal n−k edges and preserves
them in the result.
4
Instance Analysis
In order to verify whether the designed P system has better clustering effect, we
introduce a simple example in this section. In this test, 10 points need to be divided into
3 clusters, and data-points in same clusters are similar to each other but dissimilar to
data-points in other clusters. The example points are shown as Fig. 3.
The modiﬁed similarity matrix of this set is as below.
W1;10 ¼
0
1
1
8
10
13
10
10
17
25
1
0
2
5
5
10
9
13
20
26
1
2
0
5
9
8
5
5
10
16
8
5
5
0
2
1
2
10
13
13
10
5
9
2
0
5
8
20
25
25
13
10
8
1
5
0
1
9
10
8
10
9
5
2
8
1
0
4
5
5
10
13
5
10
20
9
4
0
1
5
17
20
10
13
25
10
5
1
0
2
25
26
16
13
25
8
5
5
2
0
2
666666666666664
3
777777777777775
An Improved MST Clustering Algorithm Based on Membrane Computing
9

The designed P system realizes the minimum spanning tree clustering algorithm
with its maximal parallelism and distributed manner. Now we give simple analysis of
how the P system executives in this example.
At ﬁrst, rule r1 in the environment sends objects uwij
ij ð1  i; j  10Þ into membrane
1 to represent distances between data-points. Among all these uwij
ij ð1  i; j  10Þ, rules
r2 r3 pick u12 u13 u46 u67 to start the construction, because they have the minimum
weight 1. Therefore there are four membrane1 in parallel to continue next constructing
steps, and each of them starts from different edges. For the membrane1 starting from
u12, the edge with next minimum weight is e13. Then rule r5 estimate whether adding
e13 will form a circle or not. If a circle is to form, this edge will be abandoned.
Otherwise, the edge is added. After the judgment, u13 evolves into q3v3x3P13
according to rule r6. Then the next object u23 is to be judged. Since there are v1v2v3
here, rule r5 dissolves u23, and the next edges with minimum weight 5–edges
e24 e25 e34 e27 e38 continue this process. As there are more than one objects to be added,
these previous objects in the membrane is copied 5 times and membrane 1 split into 5
identical membranes according to rule r7. For one of them, for example, the one
containing e25, the next minimum edge to be added is e54; and the one containing e34,
e46 with weight 1 is next to be added. These procedures will repeat until no f or uij is
left. Finally there are 26 minimum spanning trees constructed as shown in Fig. 4. And
according to rule r10, edges in these trees are sent into membrane i0. The membrane
structure containing all feasible solutions is shown as Fig. 4.
The ﬁnal partition step is executed in membrane i0 with rules r15  r19. In this
example, we can see edges in all minimum spanning tree feasible solutions including
e12 e13 e46 e78 e89 e9;10 e67 e45 e24 e25 e34 e38 e56 e37. They are sorted by rules in
descending order by their cardinal and retained the ﬁrst 9(n−1) edges only by rule r15
and r16. Rule r17 r18 and r19 pick the longest 2(k−1) edges – e78e24 and delete them by
their weights. Finally edges e12 e13 e46 e89 e9;10 e67 e45 are reserved in the graph. The
clustering result is fa1a2a3g,fa4a5a6a7g,fa8a9a10g.
0
1
2
3
4
5
6
7
0
1
2
3
4
5
6
7
Fig. 3. Instance data set (a1 = (1,2), a2 = (1,3), a3 = (2,2), a4 = (3,4), a5 = (2,5), a6 = (4,4),
a7 = (4,3), a8 = (4,1), a9 = (5,1), a10 = (6,2)).
10
P. Gong and X. Liu

5
Discussion and Future Works
In this paper, we construct a speciﬁc P system where all feasible solutions to the
minimum spanning tree problem of a complete graph can be calculated. Then by
integrating these feasible solutions and deleting some large weight edges the ﬁnal
partition result is formed in the output membrane. Through the example test, we can
see that the designed P system can effectively ﬁnd quality clustering result in a dis-
tributed and parallel manner. The future work is mainly focused on optimizing the
process of ﬁnding solutions with the purpose of reducing identical solutions and
computation complexity and making our system run more efﬁciently.
Acknowledgement. Project supported by National Natural Science, Foundation of China
(61170038, 61472231, 61640201), Jinan, City independent innovation plan project in College
and Universities, China (201401202), Ministry of education of Humanities and social science
research project, China (12YJA630152), Social Science Fund Project of Shandong Province,
China (11CGLJ22), outstanding youth scientist foundation project of Shandong Province, China
(ZR2011FM001).
References
1. Han, J., Pei, J., Kamber, M.: Data Mining: Concepts and Techniques. Elsevier (2011)
2. Fredman, M., Willard, D.: Trans-dichotomous algorithms for minimum spanning trees and
shortest paths. In: Proceedings of the 31st Annual IEEE Symposium on Foundations of
Computer Science, pp. 719–725 (1990)
3. Zhong, C., Malinen, M., Miao, D., et al.: A fast minimum spanning tree algorithm based on
k-means. Inf. Sci. 295, 1–17 (2015)
4. Zhong, C., Miao, D., Wang, R.: A graph-theoretical clustering method based on two rounds
of minimum spanning trees. Pattern Recogn. 43(3), 752–766 (2010)
5. Păun, G.: Computing with membranes. J. Comput. Syst. Sci. 61(1), 108–143 (2000)
12
13
25
56
64
67
78
89
910 '
p p p p p p p p p d
1
1
1
1
12
13
24
45
46
67
78
89
910 '
p p p p p p p p p d
12
13
25
24
46
67
78
89
910 '
p p p p p p p p p d
12
13
25
24
46
67
78
89
910
'
p p p p p p p p p
d
0
0i
01
λ
……
Fig. 4. conﬁguration after applying rule r9
An Improved MST Clustering Algorithm Based on Membrane Computing
11

6. Xue, J., Liu, X.: Lattice based communication P systems with applications in cluster
analysis. Soft. Comput. 18(7), 1425–1440 (2014)
7. Paun, G., Rozenberg, G., Salomaa, A.: The Oxford Handbook of Membrane Computing.
Oxford University Press Inc., New York (2010)
8. Sosík, P., Rodríguez-Patón, A.: Membrane computing and complexity theory: A charac-
terization of PSPACE. J. Comput. Syst. Sci. 73(1), 137–152 (2007)
9. Zhang, G.X., Pan, L.Q.: A survey of membrane computing as a new branch of natural
computing. Chin. J. Comput. 33, 208–214 (2010)
10. Grygorash, O., Zhou, Y., Jorgensen, Z.: Minimum spanning tree based clustering
algorithms. In: 2006 18th IEEE International Conference on Tools with Artiﬁcial
Intelligence (ICTAI 2006), pp. 73–81. IEEE (2006)
12
P. Gong and X. Liu

A Method for Correcting the Leaning of AR
Two-Dimensional Codes Based on LSM
Chen Miao, Shufen Liu, and Zhilin Yao
(✉)
Jilin University, Changchun, China
814274078@qq.com
Abstract. In this paper, it is inevitable that the AR two-dimensional codes
obtained by the mobile device will have a tendency towards the mobile augmented
reality application. A two-dimensional code tilts correction method based on least
square method is proposed. After binarization preprocessing, the target two-
dimensional code inclination angle is obtained by least-squares method. After
further correction by bilinear interpolation, satisfactory results can be achieved.
Restore the image. The experimental results show that the method can be quickly
and eﬀectively identify the target two-dimensional code, and then the corre‐
sponding three-dimensional model to enhance the reality display.
Keywords: Augmented reality · 2D code · Tilt correction
Least square method
1
Introduction
Augmented reality (AR) is with the development of virtual reality technology and
produce a new kind of computer application and the technology of human-computer
interaction [1]. It combines the computer-generated virtual environment with the real
scene of the user by means of photoelectric display technology, interactive technology,
multi-sensor technology, computer graphics and multimedia technology, so that the user
can conﬁrm from the sensory eﬀect that the virtual environment is an integral part of
the surrounding real scene. Unlike immersive virtual reality, augmented reality tech‐
nology is mainly based on the existing real world, to provide users with a new sensory
composite visual eﬀects, to expand the human cognitive and perceived ability of the
world. Augment reality technology not only has the strong sense of reality, modeling
on small workload, and more secure and reliable.
Mobile augmented reality applications require mobile device software and hardware,
using the device camera to capture real-world images, calculate the relevant information,
integration of virtual scenes, and ﬁnally output to the screen, projectors and other display
devices [4]. When the two-dimensional barcode image is acquired and decoded by the
camera of the mobile phone, the acquired two-dimensional code is inevitably inclined.
When the tilt angle exceeds a certain range, the decoded barcode can not be decoded
correctly. Therefore, it is necessary to tilt the two-dimensional code.
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 13–22, 2018.
https://doi.org/10.1007/978-3-319-74521-3_2

The traditional two-dimensional code correction algorithm: Hough transform,
Fourier transform [6]. The Fourier transform method uses the two-dimensional barcode
tilt angle corresponding to the azimuth angle with the largest density of the Fourier space.
The computational complexity is very high and is rarely used at present. Hough trans‐
form is the most commonly used method to detect the inclination angle. For the real-
time processing of mobile phones using mobile phones to enhance the real-time display
of 3D models may appear unpredictable error, this approach is ﬁrst recorded two-
dimensional code contour space coordinates, record the coordinates, the application of
the minimum. The linear regression of these points by LSM gives the slope of the line,
and the slope of the two-dimensional code is known. This can eﬀectively improve the
computation time.
2
Overview of 2D Code
As a new information storage and transmission technology, two-dimensional code can
be used to express text and image processing information on many languages, and has
the characteristics of high capacity, high density, strong error correction capability, fast
positioning and automatic encoding and decoding [3]. It can not only encode the scene
information associated with it to the landmarks, but also obtain the scene information
on the decoding landmarks, which is obviously diﬀerent from the traditional ARToolKit
marker points for locating and recognizing [7]. The common mark point has QR code,
like Fig. 1 shows. As a kind of two-dimensional code, QR code is based on the computer
image processing technology, combined coding principle and so on, and has the function
of automatic recognition read processing.
Fig. 1. Two kinds of mark points
2.1
QR Code Internal Structure
The QR code is a matrix-type two-dimensional code consisting of a square block
consisting of a coding region (consisting of version information, format information and
data and error correction code words) and function graphics (composed of a seek pattern,
a delimiter, and a correction pattern component) [5], the coding region is a region for
coding data or error correction code words, and the functional pattern refers to a speciﬁc
pattern in the symbol for symbol localization and feature recognition, an imaging pattern
located at three corners, It can help to determine the position, size and inclination of the
symbol. The symbol is surrounded by a blank area. Figure 2 shows the internal structure
14
C. Miao et al.

of the QR code. In the symbol, the dark module represents the binary “1”, the light
module represents the binary “0” [9].
Fig. 2. QR code structure
3
The Lean Correction Principle Based on LSM
According to the structure of the QR code, ﬁrstly locate the position mark and record
the space coordinates of its left boundary, construct the data set {X, Y}, where X, Y is
the input vector, used to store the spatial coordinates of the boundary point of the
contour [2].
After rotation, the left edge of the 2D barcode must be perpendicular to the X axis,
then:
min
∑n
i=1
(𝜔(xi, yi
) −𝜔(x, y))2
(1)
According to the data set {X, Y} constructed, it can be linear regression according
to f(x) = ωx + b, we can see that the regression error of sample points is:
ei = f(x) −Y = 𝜔x = b −Y
(2)
Substitute Eq. 1 into Eq. 2
min
n
∑
i=1
(𝜔(xi, yi
) −𝜔(x, y))2
= min
n
∑
i=1
(
ei −b −Y −
(
e −b −Y
))2
= min
∑n
i=1
(ei
)2
(3)
A Method for Correcting the Leaning of AR Two-Dimensional Codes
15

Therefore, only the data set {X, Y} regression. When the inaccuracy ei variance is
minimum, the parameter ω is the tilt vector. In this way, the optimization problem of
the image tilt angle is transformed into the process of regression of the data set {X, Y}
to identify the parameter ω.
Suppose there exists a univariate regression model f(x) = ax + b, and the regression
of the random variable Y on the independent variable is f(x). The univariate regression
model is:
y = ax + b + ε
ε ∼N(0, 𝜎2)
Here parameters a, b, on the independent variables X are independent.
Let the left boundary of the boundary contour of a two-dimensional code be inde‐
pendent of each other by N characteristic points [6]. The deviation between the estimated
value and the sample is:
𝛿i = yi −f(xi
) = yi −axi −b
By Eq. 3 we can see that when the deviation is the minimum angle is the tilt, the
value is:
min 𝛿2 = min
∑n−1
i=0
(yi −axi −b)2
(4)
According to KKT conditions to obtain the optimal a, b value, that is
a =
N ∑n−1
i=0 xiyi −∑n−1
i=0 xi
∑n−1
i=0 yi
N ∑n−1
i=0 x2
i −∑n−1
i=0 xi
∑n−1
i=0 xi
(5)
b =
∑n−1
i=0 x2
i
∑n−1
i=0 yi −∑n−1
i=0 xi
∑n−1
i=0 xiyi
N ∑n−1
i=0 x2
i −∑n−1
i=0 xi
∑n−1
i=0 xi
(6)
The value of a obtained here is the slope of the straight line after the left edge of the
ﬁtting, and the inclination angle of the two-dimensional code in the horizontal direction
can be obtained.
4
QR Code Correction Process
In order to realize the detection of QR code in mobile augmented reality, we need to
perform binary preprocessing of the obtained 2D code to make the later operation more
accurate. And then obtains the coordinates of the boundary points of the QR code, and
obtains the ﬁnal image through the distortion correction, the skew correction and the
bilinear interpolation mapping. Finally, the 3D model is obtained by decoding the 2D
code image, and output to the screen, projector and other display devices to enhance
Reality display. The ﬂowchart is shown below (Fig. 3):
16
C. Miao et al.

Fig. 3. Flowchart
4.1
Image Preprocessing
There are two kinds of formulas for converting the color image into grayscale image:
Y = 0.299R + 0.587G + 0.114B
(7)
Y = (R + G + B)∕3
(8)
Here, we use the formula Y = (R + G)/2 and ignore the B component of the two-
dimensional code image gray processing. The adaptive threshold segmentation algo‐
rithm is adopted in the binarization threshold processing. The basic idea of the algorithm
is that the average gray value of S pixels is calculated when traversing the whole image.
When the pixel value of a pixel is lower than this value, set to black, otherwise set to
white.
A Method for Correcting the Leaning of AR Two-Dimensional Codes
17

4.2
Barcode Location
After the bar code is binarized, the bar code needs to be positioned. According to the
structure of the QR code, we take the location of the QR code, record its spatial coor‐
dinates, and construct the data set {X, Y}, where X and Y are the input variables.
4.3
Distortion Correction
In practice, the camera in take of QR code image will have diﬀerent degree of distortion.
Any geometric distortion can be deﬁned by an equation that transforms the non-distorted
coordinate system (X, Y) into the distortion coordinate system (X
′, Y
′), which is gener‐
ally of the form:
{
x′ = h1(x, y)
y′ = h2(x, y)
(9)
Let f(x, y) be the original image without distortion and g(x, y) be the result of f(x, y)
distortion. The distortion process is known, and is defined by the functions h1(x, y) and
h2(x, y)
g(x′, y′) = f(x, y)
(10)
Equation (10) that should appear in the image in pixels (x, y) on the grey value due
to the distortion, and appear in the (x, y), the distortion problem might be solved by
mapping transformation. In the case of known g(x′, y′), h1(x, y) and h2(x, y), the resto‐
ration process is as follows:
1. Find the corresponding positions in g(x′, y′) for each point (x0, y0) in f(x, y): (M,
N) = [h1(x0, y0), h2(x0, y0)]. M and N are the coordinate values of the spatial points,
respectively. (M, N) does not coincide with any point in g(x′, y′), since M and N are
not necessarily integers.
2. Find the point (
x1′, y1′) nearest to (M, N) in g(x′, y′), let f(x0, y0) = g
(
x1′, y1′),
that is, give the grayscale values of g(
x1′, y1′) to f(x0, y0), according to this way
point by point until the entire image is ﬁnished, and the geometry is corrected.
4.4
Tilt Correction
After the distortion correction, we record the left border of the bar code, using (5,6) to
calculate the tilt angle 𝛼= 𝚊𝚛𝚌𝚝𝚊𝚗a; After the bar code is obtained, the rotation formula,
that is
[
xnew
ynew
]
=
[
cos a
sin a
−sin a cos a
][
xold
yold
]
(11)
18
C. Miao et al.

If the Eq. (11) is used directly, for a wide (W) * high (H) image, the rotation requires
4WH multiplication and 2WH addition, and the algorithm is highly computationally. In
fact, for a height of H for the two-dimensional code image, the vertical direction of the
projection maximum deviation is:
Ymax = H tan a
The deviations for the jth column and the ith row in the image are, respectively (Fig. 4).
Δyj =
(
j
Ymax + 1
)
× W
Δxi =
(
i
Xmax + 1
)
× H
Fig. 4. Spatial mapping of control points
4.5
Bilinear Interpolation
Some points that are not in the integer position may be generated during the image
rotation transformation. This requires an algorithm for gray-scale interpolation to
produce a smooth mapping that maintains continuity and connectivity. Interpolation
methods include the nearest neighbor interpolation, bilinear interpolation and high
order interpolation. The nearest neighbor interpolation is a simple interpolation
algorithm, but the nearest neighbor interpolation algorithm will produce a clear
image of the zigzag boundary. For the QR code with only black and white, bilinear
interpolation can be used to produce a satisfactory image restoration effect [8]. The
mathematical model is shown in Fig. 5:
A Method for Correcting the Leaning of AR Two-Dimensional Codes
19

Fig. 5. Mathematical model of bilinear interpolation
First, by ﬁrst-order linear interpolation:
f(x, 0) = f(0, 0) + x[f(1, 0) −f(0, 0)]
The same can be drawn:
f(x, 1) = f(0, 1) + x[f(1, 1) −f(0, 1)]
f(x, y) = f(x, 0) + y[f(x, 1) −f(x, 0)]
Merge the above three formulas:
f(x, y) = [f(1, 0) −f(0, 0)]x + [f(0, 1) −f(0, 0)]y
+ [f(1, 1) + f(0, 0) −f(0, 1) −f(1, 0)]xy + f(0, 0)
After this series of operations, the original two-dimensional code image into a more
standard image, as shown (Fig. 6):
Fig. 6. Compared before and after correction
20
C. Miao et al.

4.6
Download the 3D Model and Display It
As shown in Fig. 7, QR code content with the soldier URL address, we can easily
download from the server to the corresponding three-dimensional model, and through
the camera to enhance its reality display.
Fig. 7. Enhanced reality display
5
Conclusion and Outlook
This paper mainly focuses on the tilt correction method of 2-D codes in mobile AR. The
LSM eﬀectively improves the decoding time and decoding success rate. Start from the
server to download the three-dimensional virtual model loaded into memory. Then, the
model rendering output to the phone screen, and ultimately achieve the target QR code
to enhance the reality show. However, this method only supports the identiﬁcation of a
single QR code. The next step is to consider a method that supports multiple QR code
recognition and combine with the current excellent local feature point algorithm to
realize fast and eﬀective recognition of multiple QR codes.
References
1. Belimpasakis, P., Selonen, P., You, Y.: Bringing user-generated content from internet services
to mobile augmented reality clients. In: 2010 Cloud-Mobile Convergence for Virtual Reality
Workshop (CMCVR), pp. 14–17. IEEE (2010)
2. Trier, O.D., Jain, A.K.: Goal-directed evaluation of binarization methods. IEEE Trans PAMI
17(12), 1191–1201 (1995)
3. Chen, Y.-Y., Shi, P.-F.: Study and application of two-dimensional bar code. Tech. Meas.
Control 23(12), 17–19 (2006)
4. Schmalstieg, D., Wagner, D.: Experiences with handheld augmented reality. In: Proceedings
of Mixed and Augmented Reality, 6th IEEE and ACM International Symposium, pp. 3–18
(2007)
A Method for Correcting the Leaning of AR Two-Dimensional Codes
21

5. Liu, Y., Liu, M.-Y., Shang, Z.-H.: Multi-level thresholding method for fast response matrix
codes. Appl. Res. Comput. 8, 177–179 (2006)
6. Liang, Y.-H., Wang, Z.-Y.: Two-dimensional bar code tilt detection based on projection under
high noise conditions. Microcomput. Inf. 22, 232–234 (2006)
7. Azuma, R., Bailot, Y., Behringer, R., et al.: Recent advances in augmented reality. Comput.
Graph. Appl. 21(6), 34–47 (2001)
8. White, S., Feiner, S., Kopylec, J.: Virtual vouchers: prototyping a mobile augmented Reality
user interface for botanical species identiﬁcation. In: IEEE Symposium on 3DUserInterfaces
2006, 3DUI2006, pp. 119–126. IEEE (2006)
9. Pavlidis, T.: A new paper/computer interface: Two-dimensional symbologies. IEEE Comput.
Mag. 2, 145–151 (2000)
22
C. Miao et al.

The Associated Algorithm of Target Batch Number Based
on Gaussian Mixture Clustering
Ning Liu
(✉), Shufen Liu
(✉), and Xinjia Zhang
(✉)
Software College, Jilin University, Changchun, China
frank93@sina.cn, {liusf,zhxj}@jlu.edu.cn
Abstract. This article provides some questions about the large quantities of
target track information, unclear sensor batch number, etc. referring to target
fusion, and provide the clustering algorithm of Gaussian Mixture Distribution to
realize the division of all the batch numbers of target track and associated oper‐
ation. According to the processing to the Gaussian distribution of information by
maximum likelihood estimation and through continuous iterative and reﬁnement
of the clusters divided, the tracking information detected by all radar sensors of
each target batch number can be obtained precisely in the end. This algorithm
provides rapid and precise working conditions for target fusion.
Keywords: Gaussian mixture distribution · Associated operation
Maximum likelihood estimation · Clustering algorithm
1
Introduction
When the formation of ship combat synergistically, naval and radars in the ﬂeet will
report all the detected objectives about the main naval, which will create inﬂuences on
the warship equipment attacked precisely according to the information.
Not only this, the objectives detected by radars can only feedback its physical infor‐
mation and can not precisely judge the target batch number deﬁned by human. In the
current literature, radars’ objective fusion rarely have research, which aims at the two
sides. In order to simplify the target quantity, avoid unnecessary redundant target infor‐
mation. The associated algorithm of target batch number in this article will connect the
prototype cluster in the ensemble learning to solve the related problems between target
information and target batch number.
2
The Data Analysis
The command system of formation of ship receives periodically signals from diﬀerent
radars in formation. Here it is set as M pieces of sensors, with each sensor as m. It can
periodically make up instructions and send N pieces of tracking information processed
by itself and under the uniﬁed coordinate system. Here the ensemble of communication
of multiple batches of tracking information sent by No.m sensor is set as Dm and among
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 23–27, 2018.
https://doi.org/10.1007/978-3-319-74521-3_3

it, some batch of tracking information is dmn(PS: m = 1, 2,…… as the tracking infor‐
mation detected by NO.m sensor. n = 1, 2…… as the NO.n target detected).
3
Target Batch Correlation
Target batch correlation is the basis of target fusion. Exactly diﬀerentiating DS is the
precondition of fusion algorithm accuracy. In the war of formation of ship, because it
is very diﬃcult to recognize the target without exactly diﬀerentiating a large amount of
tracking information detected, the accuracy of the algorithm is very important.
Firstly deﬁne some basic knowledge. As it is mentioned earlier, target tracking
information includes a great amount of data, such as a sensor’ serial number, target
nationality, target type, longitude and latitude, height and a series of data. Here the
number of attributes is set as k and set dmn = {xmn1, xmn2,…, xmnk}, the NO.n target
tracking information detected by NO.m radar as a k-dimensional vector. xmnk is the value
of NO.k of a some batch tracking information (such as, the above target type’s value on
xijk is “the air”), while k is the dimension of the tracking information vector.
The result of batch correlation is a list, with a batch number on each line. Batch
numbers correspond to the tracking information collection under the target detected by
the related sensor. The collections can not be controlled strictly or intersected and the
parallel operation of these collections is D, which is all tracking information detected
by all radars. Here we call every subset as a “cluster”. Through this divide, each cluster
stands for tracking information collection of the same target detected by the radar sensor.
After the target batch correlates with tracking information, the list formed by all clusters
report to fusion center to perform blend operation.
It is formally said that D can be divided into h pieces of disjoint clusters {Ci|i = 1,
2…, h}, with Ci ∩ Ci’ ≠ ∅. Accordingly, each cluster’s marker is set as the target batch
number. We use λ = {λ1, λ2,…, λh} to represent tracking information clusters’ marker,
including the value of λh as the target batch number.
Above we introduce is some basic knowledge about association algorithm. The
association algorithm the article studies will be researched by connecting clusters under
unsupervised learning. We just need to divide the D collection formed by all tracking
information into multiple joint clusters. But we need to solve the problem of distance
calculation.
3.1
Gaussian Mixture Clustering Algorithm
We know that Gaussian distribution is ﬁxed by two parameters among it. The two
parameters are mean vector μ and covariance matrix G. We deﬁne Gaussian mixture
distribution ﬁrst:
PM(x) =
h
∑
i=1
αip(x|μi, G)
(3.1.1)
24
N. Liu et al.

Among it, the latter part p(x|μi, G) is the probability density of Gaussian distribution.
Suppose that all tracking information is produced by Gaussian mixture distribution, the
speciﬁc process is to ﬁx diﬀerent Gaussian mixture distribution probability density
according to α1, α2,…, αh. After ﬁx the probability density function, put diﬀerent tracking
information in diﬀerent Gaussian distributions. In here we can see that Gaussian mixture
distribution is the mixed composition of multiple Gaussian distribution. Each Gaussian
distribution’s probability is controlled by a parameter αh. The probability is how much
the the size is about some tracking information has been speciﬁcally related to one
Gaussian distribution.
It can be seen that three parameters αi, μi, G among it should be fixed if defining
Gaussian mixture distribution. Suppose that all tracking information has been
completed the correlation, how to fix the target batch number of some tracking
information, which means how to fix cluster marker of the tracking information
cluster some tracking information is in. Suppose that the tracking information cluster
of some tracking information dij is in is z. In Gaussian mixture distribution, zmn is the
Gaussian distribution some sample is in. The value range is 1~h. But we have no way
to know its specific value. According to Bayes’ theorem, posteriori distribution of
zij is
PM(zmn = h|dmn
) =
p(zmn = h) ⋅PM(dmn|zmn = h)
PM(dmn
)
=
αi ⋅p(dmn|μi, G)
∑h
l=1 αl ⋅p(dmn|μi, G)
(3.1.2)
Which means we have got the posteriori probability of each tracking information
cluster dmn is in, marked as γmnz. When we want to know the batch number of some target
tracking information, it is only to ﬁnd the max one the dmn corresponds to.
3.2
Maximum Likelihood Estimation
To all tracking information collection D, perform maximum likelihood estimation.
LL(D) = ln
( m
∏
j=1
PM(x)
)
=
m
∑
j=1
ln(
h
∑
i=1
αi ⋅p(dmn|μi, G))
(3.2.1)
Maximum likelihood estimation is to maximize the above formula. The derivative
is 0 with the maximum value. The partial derivatives of three parameters are 0 through
the formula 3.2.3.
Firstly, get out the partial derivative to μi through the formula 3.2.3, with the values
of derivatives as 0, and according to the formula 3.2.2, get out
μi =
∑γmnzdmn
∑γmnz
(3.2.2)
The Associated Algorithm of Target Batch Number
25

Similarly, through the derivation of G, get out of
G =
∑γmn
(dmn −μi
)(dmn −μi
)T
∑γmnz
(3.2.3)
Because αi is the related probability of target batch selected by tracking information,
the sum of α1, α2,…, αh should be 1, which all are greater than and equal to 0. According
to the lagrange formalism of maximum likelihood estimation, perform αi partial deriv‐
atives and get the values with the partial derivative as 0, get out
αi = 1
mn
∑
γmnz
(3.2.4)
After get every parameter’s calculation method and use iterative process of Gaussian
mixture clustering, which means getting posterior probability of each Gaussian distri‐
bution through current parameters and current tracking information. Then through the
formula, update three parameters till reach the maximum value of iterative or LL(D)
with a little improvement or no improvement, stop updating the parameters and ﬁnally
make sure the tracking information included among every tracking information. And
the corresponding marker of every tracking information is target batch. In this way, the
overview that ﬂow the target tracking information relates is shown in following ﬁgure.
26
N. Liu et al.

4
Justiﬁcation and Summary
We deﬁne a measure that evaluates the clarity of the tracking information to prove the
advantages of this algorithm. Davies-Bouldin index:
DB = 1
h
h
∑
i=1
max
j≠i
(
avg(Ci
) + avg(Cj
)
dcen
(μi, μj
)
)
Among them, dcen(…) is to calculate the distance between the center of two clusters,
avg(C) is the average distance of each trace information in the cluster C. So, we can see,
the denominator in the DB index is the distance between the two tracking information
clusters, the larger the denominator, the lower the similarity of the two clusters, and the
molecules represent the tracking information aggregation in two clusters. So, the smaller
the value of the DB index, the better the result of the algorithm. In a tracking information
data set of 11 ships, the results are obtained 0.32~0.33, it’s better than the general infor‐
mation.
The algorithm promoted in this article controls all the tracking information within
the k-dimension space, proceeds loop iteration and ﬁx the collection of tracking infor‐
mation detected by multi sensors each target corresponds to, which realizes the connec‐
tive work of precise target batch number and complete the work within the quadratic
time.
References
1. Wan, S.: Clustering of multisensor data fusion method. J. Syst. Eng. Theory Pract. (05) (2008)
2. Xinrui, Z., he, J.: Multi-sensor information fusion technology. Comput. Meas. Control (09)
(2007)
3. Hongfeng ailin, W., Single Showers: Research progress of foreign military information fusion
theory and application. Lightning Control (04) (2007)
4. Ma, J., Jixiang, S.: Information fusion model of neural network to describe. J. Avionics
Technol. (01) (1998)
5. Xin, W., Qidan, Z., Shuli, S.: Unfettered global optimal weighted measurement fusion
estimation. Comput. Eng. Appl. (24) (2010)
6. Hu, W., Shenyuan, Y.: The multi-sensor track correlation. Appl. Sci. Technol. (12) (2005)
The Associated Algorithm of Target Batch Number
27

Density Peaks Clustering Based on Improved
RNA Genetic Algorithm
Liyan Ren and Wenke Zang(&)
School of Management Science and Engineering, Shandong Normal University,
No. 88 East of Wenhua Road, Jinan 250014, Shandong, China
13395410670@163.com, zwker@163.com
Abstract. A density peaks clustering based on improved RNA genetic algo-
rithm (DPC-RNAGA) is proposed in this paper. To overcome the problems of
Clustering by fast search and ﬁnd of density peaks (referred to as DPC),
DPC-RNAGA uses exponential method to calculate the local density, In addi-
tion, improved RNA-GA was used to search the optimums of local density and
distance. So clustering centers can be determined easily. Numerical experiments
on synthetic and real-world datasets show that, DPC-RNAGA can achieve better
or comparable performance on the benchmark of clustering, adjusted rand index
(ARI), compared with K-means, DPC and Max_Min SD methods.
Keywords: RNA genetic algorithm  Clustering  Density peaks
Distance
1
Introduction
Clustering, a kind of unsupervised learning technique, aims at dividing a given pop-
ulation into groups or clusters with common characteristics, so that objects in the same
cluster are similar to one another and dissimilar to objects in any other clusters.
Clustering is widely used in exploratory pattern analysis, grouping, decision making,
and machine learning situations, including data mining, document retrieval, image
segmentation, and pattern classiﬁcation [1]. Many different clustering methods exist
including K-means clustering algorithm, Density-Based Spatial Clustering of Appli-
cations with Noise (DBSCAN), Afﬁnity Propagation (AP) and so on [2, 7, 9, 10].
Recently, a novel clustering algorithm was proposed by Alex Rodrguez and
Alessandro Laio [3], based on the assumption that clustering centers are surrounded by
the neighbors with lower local density and that they are at a relatively large distance
from any points with a higher local density. We refer to this algorithm as DPC (Density
Peaks Clustering) in this paper. It was demonstrated on several test cases that DPC can
efﬁciently ﬁnd the cluster centers (i.e., the density peaks) and assign the remaining
points to their appropriate clusters as well as detect outliers. Several researches have
been going on around this method [4]. However, DPC does have some shortcomings.
The clustering centers are only determined by the multiplication of the density and the
distance, which affects the selection of the best clustering centers, what’s more, the
calculation of the density is highly depending on the cutoff distance dc. That affects the
generalization of the algorithm.
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 28–33, 2018.
https://doi.org/10.1007/978-3-319-74521-3_4

In order to overcome these problems, we propose a novel DPC based on RNA
genetic algorithm (DPC-RNAGA). The method completes clustering by density and
distance analysis based on RNA-GA, which computes density with exponential method
to reduce the impact of cutoff distance and adopts RNA-GA to search the optimal
thresholds of the density and distance to determine the best clustering centers. The
benchmark of clustering, rand index, is taken as the ﬁtness function. To overcome the
excursion of search re-gion and accelerate convergence, a penalty factor was intro-
duced in the RNA-GA. Meanwhile, the selective crossover operators improve the
effectiveness and accuracy of the algorithm.
This paper is organized as follows: Sect. 2 brieﬂy describes the related works of the
improved algorithm which include density peaks clustering, the improvement of
RNA-GA. Section 3 introduces our novel clustering algorithm DPC-RNAGA and
gives a detailed analysis. Section 4 tests our proposed algorithm on several real-world
datasets and synthetic datasets, and compares its performance with K-means, DPC and
Max_Min SD on adjusted rand index (ARI). Section 5 draws some conclusions.
2
Related Works
The proposed DPC-RNAGA is based on DPC and RNA-GA. This section provides
brief reviews of DPC and RNA-GA.
2.1
Density Peaks Clustering
Rodriguez and Laio proposed an algorithm published in Science and has appealed to
many machine learning researchers. Its idea is that clustering centers are characterized
by a higher density than their neighbors and by a relatively large distance from points
with higher densities. This method utilizes two important quantities: One is the local
density of each point iqi and the other is the distance from points of higher densities di.
In the following, we will describe the calculation of qi and di in detail. Assume that the
data set is XNM ¼ v1; v2;    ; vN
½
T, where vi ¼ v1i; v2i;    ; vNi
½
T is the vector with
M attributes and N is the number of points. The distance matrix of the dataset needs to
be computed ﬁrstly.
Let d vi; vj


denote the Euclidean distance between the point vi and vj, The local
density of a point vi, denoted by qi, is deﬁned as:
qi ¼
X
j
v dðvi; vjÞ  dc


v x
ð Þ ¼
1 if x\0
0 if x  0
(
ð1Þ
Where dc is cutoff distance, qi is deﬁned as the number of points that are adjacent to
the point vi, dc is an adjustable parameter and the only variable in Formula (1). Here we
Density Peaks Clustering Based on Improved RNA Genetic Algorithm
29

use the other way to calculate the local density called exponential kernel function, it is
deﬁned as follows:
qi ¼
X
j6¼i
exp  dij
dc

2
 
!
ð2Þ
The computation of di is quite simple. The minimum distance between the point vi
and any other points with higher densities, denoted by di, is deﬁned as:
di ¼
minj:qj [ qi d vi; vj




if 9js:t:qj [ qi
maxj d vi; vj




if otherwise
(
ð3Þ
Only those points with relative high qi and high di are considered as clustering
centers. The points with high qi and high di are also called as peaks. After clustering
centers have been found, DPC assigns each remaining points to the same cluster as its
nearest neighbor with higher density.
2.2
The Improvement of RNA Genetic Algorithm
RNA encoding and decoding. In the RNA-GA, a section of chromosome is used to
encode one parameter and a chromosome is used to encode all the parameters.
Speciﬁcally, a chromosome represents the values of qi and di in this study.
Genetic operators and penalty factor. The tournament method is adopted as the
selection method in DPC-RNAGA [5]. However, in the special circumstance, It was
found that the search ﬁeld is biased after several experiments. So we propose the
penalty factor d to get rid of inferior individuals whose ﬁtness value stays the last d
percentage of all. If the initial population is Q1 ¼ q1; q2;    ; qm
f
g, the remaining ratio
of the parent generation is r, so the number of individuals remaining from the parents is
r  m.
Selective crossover operator is designed to enhance the ability in exploring the
searching space in this paper. According to the crossover probability pc, the two parents
exchange the bases between the two crossover points to produce two offspring
individuals.
In this paper, we adopted the ordinary mutation operators but with adaptive
probability. We divide the nucleotide bases into two parts: left and right parts. Cor-
respondingly, the adaptive probabilities pml (left) and pmh (right) are described as
follows:
pml ¼ a1 þ
b1
1 þ exp aa gg0
ð
Þ
½

pmh ¼ a1 þ
b1
1 þ exp aa gg0
ð
Þ
½

8
<
:
ð4Þ
30
L. Ren and W. Zang

Where a1 denotes the initial mutation probability of pml, b1 denotes the range of
transmutability. The parameter g is evolutionary generation, and g0 denotes the gen-
eration where the maximum of mutation probability is reached, and aa is the speed of
change.
Fitness function. In this paper, Rand index was adopted as the ﬁtness function of
RNA-GA. The Rand index is deﬁned as the ratio of the same data pairs from the two
clustering results and all the data pairs [6]. The value of rand index ranges from 0
through 1, and the value close to 1 shows that the clustering is better.
3
The Density Peaks Clustering Algorithm Based
on Improved RNA-GA
The density peaks clustering algorithm based on improved RNA-GA is an adaptive
way to select the best clustering centers without human intervention and reduce the
impact of cutoff distance on the calculation of local density. Firstly, calculate q and d of
each point and then generate m couples of parameters randomly to form the initial
population. After the execution of RNA-GA, we can get two thresholds. Secondly, we
can determine the clustering centers according to the thresholds. Thirdly, complete the
clustering by assigning remaining data points. Here we cluster the points to the same
cluster as the nearest center with bigger local density. Calculate the ﬁtness value when
the clustering process is ﬁnished. The algorithm stops until the ﬁtness value is stable or
the maximum of iteration is reached.
Assuming the size of dataset is n. The iterations of RNA-GA cost most time. The
complexity of each iteration is O n2
ð
Þ, the numbers of iteration is Gmax. The time
complexity of the algorithm is O mG max n2
ð
Þ, the space complexity of the algorithm is
due to the storage of the density and distance metrics. So the space complexity is
O n2
ð
Þ. Compared with several typical clustering algorithms, the complexity analysis of
the algorithm is shown as Table 1.
From Table 1, we can see that the time complexity of DPC-RNAGA is of the same
order as DPC. Although it is higher than K-means, it is still acceptable. And it doesn’t
affect the application of the algorithm.
Table 1. The comparative complexity analysis of algorithms
Algorithm
Time complexity Space complexity
DPC-RNAGA O mG max n2
ð
Þ
O n2
ð
Þ
DPC
O kn2
ð
Þ
O n2
ð
Þ
K-means
O ktn
ð
Þ
O n
ð Þ
Max_Min_SD O n3
ð
Þ
O n2
ð
Þ
Density Peaks Clustering Based on Improved RNA Genetic Algorithm
31

4
Experiments
To evaluate our proposed algorithm, clusters created by DPC-RNAGA are compared
with state of art clustering algorithms on synthetic datasets and real-word datasets. The
synthetic datasets are collected at http://cs.uef.ﬁ/sipu/datasets/ [8]. The details of the
datasets are shown in Tables 2 and 3 correspondingly.
To evaluate the performance of DPC-RNAGA, we used eight kinds of datasets and
compare with K-means, DPC and Max_Min SD. The dimensions of synthetic datasets
are small and the shape of clusterings is obvious. So we set that the size of the
population is 50, the maximum of the genetic iteration is 30, the penalty factor is 5.
However, for real-word datasets, the clusterings are mostly crossed and there are many
noises. So we set that the size of the population is 100, the maximum of the genetic
iteration is 30, the penalty factor is 1. The parameters of another three algorithms are
set according to the actual clusterings.
In our proposed algorithms, RI is used as ﬁtness value. Table 4 shows the RI got
from four kinds of clustering algorithms dealing with eight kinds of datasets. The RI is
more close to 1, the clustering result is better. The underlined RI shows the best RI for
the special datasets. The experiment result shows that our proposed algorithm performs
better than the other three methods.
The results show that four clustering methods perform well when the datasets is
small and there are few noise points, however, when noise point appears, our proposed
algorithm performs better than DPC; What’s more, because real-word datasets have
more noises and crosses, and the dataset is large, our proposed algorithm got better
clustering results than the other three algorithms.
Table 2. Synthetic datasets
Dataset
Objects Dimensions Classes
Aggregation
788
2
7
Spiral
312
2
3
D31
3100
2
31
ﬂame
240
2
2
Table 3. Real-world datasets
Dataset Objects Dimensions Classes
Iris
150
4
3
Wine
178
133
3
Glass
214
10
6
Seeds
210
7
3
Table 4. Comparison of RI benchmark for 4 clustering algorithms
Dataset
RNA-GA-DPC K-means DPC
Max_ Min_SD
Aggregation 0.9993
0.9234
0.9893 0.9434
Spiral
1.0000
0.5713
1.0000 0.5813
D31
0.9959
0.9931
0.9859 0.9934
Flame
1.0000
0.4524
1.0000 0.4829
Iris
0.8923
0.7342
0.8923 0.7931
Wine
0.7350
0.4219
0.6309 0.4341
Glass
0.8198
0.4512
0.7936 0.4621
Seeds
0.7908
0.6864
0.7343 0.7049
32
L. Ren and W. Zang

5
Conclusion
A clustering algorithm based on RNA genetic algorithm and ﬁnding density peaks was
proposed in this paper. This method computes the local density of data points with the
exponential kernel methods, which reduced the dependence on the parameter dc.
RNA-GA is used to search the optimal density and distance from other data point with
higher density, which makes the clustering algorithms more effective. Penalty factor is
added to the RNA-GA, which avoids the search region getting wrong way and
accelerates convergence. The power of DPC-RNAGA was tested on several synthetic
datasets and real-world datasets. The results demonstrate that DPC-RNAGA is pow-
erful in ﬁnding clustering centers and recognizing clusters regardless of their shape and
size. The comparison analysis of several clustering algorithms shows that our proposed
method can be ﬁt to many different datasets and perform better than the existing DPC,
K-means and Max_Min_SD methods.
Acknowledgement. This research is supported by Excellent Young Scholars Research Fund of
Shandong Normal University, China. It is also supported by Natural Science Foundation of China
(No. 61472231, No. 61640201, No. 61502283, No. 61402266). And in part by the Jinan Youth
Science and Technology Star Project under Grant 20120108, and in part by the soft science research
on national economy and social information of Shandong, China under Grant(2015EI013).
References
1. Mohebi, A., Aghabozorgi, S.R., Teh, Y.W., Herawan, T., Yahyapour, R.: Iterative big data
clustering algorithms: a review. Softw. Pract. Exper. 46(1), 107–129 (2016). https://doi.org/
10.1002/spe.2341
2. Gorunescu, F.: Data Mining –Concepts, Models and Techniques. ISRL, vol. 12. Springer,
Heidelberg (2011). https://doi.org/10.1007/978-3-642-19721-5
3. Rodriguez, A., Laio, A.: Clustering by fast search and ﬁnd of density peaks. Science 344
(6191), 1492–1496 (2014)
4. Sun, K., Geng, X., Ji, L.: Exemplar component analysis: a fast band selection method for
hyperspectral imagery. IEEE Geosci. Remote Sensing Lett. 12(5), 998–1002 (2015). https://
doi.org/10.1109/LGRS.2014.2372071
5. Zhu, Q., Ning, W., Li, Z., Zhu, Q., Ning, W., Li, Z.: Circular genetic operators based RNA
genetic algorithm for modeling proton exchange membrane fuel cells. Int. J. Hydrog. Energy
39(31), 17779–17790 (2014)
6. Dubes, R.C.: Cluster analysis and related issues. In: Handbook of Pattern Recognition &
Computer Vision (2015). 996 p
7. Chang, H., Yeung, D.: Robust path-based spectral clustering. Pattern Recogn. 41(1), 191–
203 (2008). https://doi.org/10.1016/j.patcog.2007.04.010
8. KBache, M.L.: Uci machine learning repository (2013). http://archive.ics.uci.edu/ml
9. Dubey, A.K., Gupta, U., Jain, S.: Analysis of k-means clustering approach on the breast
cancer wisconsin dataset. Int. J. Comput. Assist. Radiol. Surg. 11(11), 2033–2047 (2016).
https://doi.org/10.1007/s11548-016-1437-9
10. Bian, W., Tao, D.: Max-min distance analysis by using sequential SDP relaxation for
dimension reduction. IEEE Trans. Pattern Anal. Mach. Intell. 33(5), 1037–1050 (2011).
https://doi.org/10.1109/TPAMI.2010.189
Density Peaks Clustering Based on Improved RNA Genetic Algorithm
33

Local Homogeneous Weighted Spiking Neural P Systems
Mengmeng Liu and Feng Qi
(✉)
School of Management Science and Engineering, Shandong Normal University,
No. 88 East of Wenhua Road, Jinan 250014, Shandong, China
18353110110@163.com, qfsdnu@126.com
Abstract. Homogeneous Spiking Neural P Systems (HSN P systems, for short)
are a class of neural-like computing models in membrane computing, which are
inspired by neurons that they are “designed” by nature to have the same “set of
rules”, “working” in a uniform way to transform input into output. HSN P systems
can be converted to weighted homogeneous SNP systems. In this work, based on
the above two known systems, we consider a restricted variant of SN P systems
called local homogeneous weighted SN P systems (LHWSN P systems, for short),
where neurons in same module have the same set of rules. As a result, we prove
that such systems can achieve Turing completeness. Speciﬁcally, it is proved that
using only standard spiking rules is suﬃcient to compute and accept the family
of sets of Turing computable natural numbers, moreover local homogeneity
reduces the time required for the execution of the system.
Keywords: Spiking Neural P system · Weight · Local homogeneous
1
Introduction
Membrane computing [1] is one of the recent branches of natural computing, whose aim
is to construct powerful computing models and intelligent algorithms by abstracting
ideas from a single living cell and from complexes of cells. The obtained models are
distributed and parallel computing devices, usually called P systems. There are three
main classes of P systems investigated: cell-like P systems [1], tissue-like P systems [2]
and neural-like P systems [3]. Spiking Neural P systems (SN P systems, for short) are
a class of neural-like P systems, which are inspired by the way of biological neuron in
human brain processing information and communicating with each other by means of
electrical impulses (spikes) [3].
In the research of SN P system, the results mainly focus on three aspects: theory,
application and implement. Theoretical level is divided into three aspects: generating
sets of numbers [4–10], generating languages [11, 12], computing any Turing comput‐
able functions [13, 14]. In recent years, we have studied a variety of SNP variants and
analyzed their ability to generate sets of numbers [4, 6–8, 15, 16]. Particularly, Zeng
proposed the homogeneous SNP systems in 2009 [4], and proved that homogeneous
SNP systems have Turing universality in generating and accepting modes. In 2015, Song
proposed homogenous SNP Systems with Inhibitory Synapses (through inhibitory
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 34–45, 2018.
https://doi.org/10.1007/978-3-319-74521-3_5
www.allitebooks.com

synapses, spikes is converted to the anti-spikes) [16], and proved the systems have
Turing universality in the case of using only the spiking rules.
Inspired by various of SNPS, this article proposed a variant of SNP system based on
a new biological fact and study the ability of generating sets of numbers in two modes
of the new system, besides that, we analyze the time need to execute the system. The
biological fact is that although neurons are “designed” very similar, but there are still
some diﬀerences between diﬀerent neurons. According to functional division, neurons
can be divided into three categories: sensory neurons, connecting neurons, motor
neurons. In the nerve center, neuronal cell bodies having the same function gather
together to regulate a physiological activity of body (the deﬁnition of nerve center). That
is to say, there are some diﬀerences in the rules of the neuron, but the neurons of the
same rules will be gathered together to cooperate with each other to complete a certain
physiological function. We introduce this biological phenomenon into SN P systems
and the SNP systems have local homogeneity.
In LHWSN P systems, the rules used by neurons in the same function module are
identical, and the weights on the synapse can be positive integer and negative integer.
When the spike passes through the synapse with negative weights, the spike is changed
into the anti-spike and the number of the spike is doubled according to the absolute value
of the weight.
In this paper, we proved that the LHWSN P systems have Turing universality. First,
we conclude that the delay of rules can be removed but the systems still have Turing
universality. Second, we use negative weight to express inhibitory synapse so the
forgetting rule in neurons is removed. Third, the proposing of local homogeneous
reduces the total time needed to execute the system.
In the next section, we introduce prerequisites and the deﬁnition of LHWSN P
systems. In Sect. 3, we investigate the computation power of the system. Conclusions
and remarks are drawn in Sect. 4.
2
Local Homogeneous Weighted SN P Systems
Before we introduce LHWSN P systems, we recall some necessary prerequisites. It is
useful for readers to have some familiarity with basic elements of formal language
theory, e.g., from [17], as well as basic concepts and notions of SN P system and register
machine [18].
2.1
Background
Generally, an SN P system is composed of neurons, spikes, synapses, and rules (spiking
rules and forgetting rules). A neuron can send information to its neighboring neurons
by using the spiking rule. A predeﬁned number of spikes will be removed from the
neuron by using the forgetting rule, and thus they are removed out of the system. One
neuron in the system is speciﬁed as the output neuron, which can emit spikes to the
environment.
Local Homogeneous Weighted Spiking Neural P Systems
35

In the study of the computing power of SN P system, most of them are proved by
simulating calculation process of the register machine. A register machine is a construct
M =
(
m, H, l0, lh, R
)
, where m is the number of registers, H is the set of instruction labels,
l0 is the start label, lh is the halt label, and R is the set of instructions. The instructions
are of the following forms:
• li:
(
ADD(r), lj, lk
) (add 1 to the register r and then go to one of the instructions with
label lj and lk, non-deterministically chosen),
• li:(SUB(r), lj, lk
) (if register r is non-empty, then subtract 1 from it and go to the
instruction with label lj, otherwise go to the instruction with label lk),
• lh:HALT (halt the calculation of the register machine and regard the number in register
1 as the result of the register machine).
Register machine has two kinds of working modes: generating mode and accepting
mode. In generating mode, the register machine computes number n as follows: starting
M with all registers empty, the initial instruction with label l0 is applied, and the instruc‐
tions are applied as indicated by labels. When the register machine M proceeds to the
halt instruction, number n stored at that time in the ﬁrst register is said to be computed
by M. In accepting mode (using deterministic ADD instructions), a random number is
stored in the ﬁrst register (other registers are empty). If the computation starting from
the initial conﬁguration eventually halts, the number is said to be accepted by M.
2.2
System Description
In this section, we deﬁne the LHWSN P system. The deﬁnition is complete, but famil‐
iarity with the basic elements of homogeneous SN P systems, anti-spikes and SNP with
weights (e.g. from [4, 7, 19]) is helpful.
An LHWSN P system of degree m ≥1 is a construct of the form:
∏
=
(
O, σ1, σ2, … σm, syn, in, out
)
Where
• o = {a, ̄a} is the alphabet, where a is spike and ̄a is anti-spike;
• σ1, σ2, … σm are neurons of the form σi = (ni, R) with 1 ≤i ≤m where
(a) ni is a natural number representing the initial number of spikes in σi;
(b) R is set of rules in each neuron of the following forms:
1. E∕ac →a is the spiking rule, where E is the regular expression over {a}.
(c is integer and c ≥ 1);
2. as →λ is the forgetting rule, with the restriction that for any s ≥1 and any
spiking rule E∕ac →a;d, as ∉L(E), where L(E) is set of regular languages
associated with regular expression E and λ is the empty string;
3. aa →λ.
36
M. Liu and F. Qi

• syn ⊆{1, 2, ..., m} × {1, 2, ..., m} × Z (Z is integer) is set of synapses between neurons,
where i ≠ j, z ≠ 0 for each (i, j, z) ∈syn, and for each (i, j) ∈{1, 2, ..., m} × {1, 2, ..., m}
there is at most one synapse (i, j, z) in syn.
• In, out ∈{1, 2, ..., m} indicate the input and output neurons respectively.
In LHWSN P systems, the rules used by neurons in the same function module are
identical, but the rules of diﬀerent functional modules are diﬀerent. A neuron may
contain a number of spikes or anti-spikes, but only one of them. In LHWSN P systems,
spiking rules (E∕ac →a) can be applied in any neuron as follows: if neuron σi contains
k spikes a with ak ∈L(E) and k ≥c, the spiking rule E∕ac →a is enabled to be applied.
By using the rule, c spikes a are consumed, thus k −c spikes a remain in the neuron σi,
and one spike a is sent to all neurons σj such that (i, j, z) ∈syn. Rules of the form
as →λ, s ≥1 are forgetting rules with the restriction as ∉L(E) (that is to say, a neuron
cannot apply the spiking rules and forgetting rules at the same moment), where L(E) is
set of regular languages associated with regular expression E and λ is the empty string.
If neuron σi contains exactly s spikes, the forgetting rule as →λ can be applied, by which
s spikes can be removed from the neuron.
The set of synapses is denoted by (i, j, z). Speciﬁcally, neuron σi spikes at certain step
sending a spike to neuron σj along the synapse (i, j, z), if z > 0, neuron σj will receive z
spikes; if z < 0, neuron σj will receive −z anti-spikes. The result of a computation of the
system ∏ is deﬁned as the time internal of ﬁrst two spikes being emitted to the envi‐
ronment by the output neuron at steps t1, t2 in the form of number t2 −t1; we say that this
number is generated by ∏. The set of all numbers computed in this way by ∏ is denoted
by N2
(∏)(the subscript 2 indicates that we only consider the distance between the ﬁrst
two spikes of any computation.)
An LHWSN P system ∏ can also work in the accepting mode. A number n is intro‐
duced in a speciﬁed neuron in the form of f(n) spikes by reading spike train from the
environment through the input neuron. If the computation eventually halts, then number
n is said to be accepted by ∏. The set of numbers accepted by ∏ is denoted by Nacc
(∏)
(the subscript acc indicates the system works in the accepting mode).
We denote by NαLHWkSNPm, α ∈{2, acc} all sets of numbers generated or accepted
by LHWSN P systems of degree m. If the parameter m is not bounded, then it is replaced
with ∗. The absolute value of the weight is not more than K.
3
The Computing Power of LHWSN P Systems
3.1
Proof of Turing Universal
In this section, we proved that the LHWSN P systems have a universal computation in
the generating mode and the accepting mode. In the following proofs, a directed graph
is used to represent the structure of the system: the neurons are placed in the nodes of
the graph connecting with each other by the synapses; the output neuron has an outgoing
synapse, emitting spikes to the environment.
Local Homogeneous Weighted Spiking Neural P Systems
37

Theorem 1. N2LHW3SNP∗= NRE
It is enough to prove the inclusion NRE ⊆N2IHW3SNP *, for the converse inclusion
can be invoked from the Turing-Church thesis. In the following universality proof, a
LHWSN P system ∏ is constructed to simulate the computation of M.
Each register r of M is represented by a neuron σr, whose content corresponds to the
number stored in register r. Speciﬁcally, if the number stored in register r is n ≥ 0, then
neuron σr contains 5n spikes. In the initial conﬁguration, the number stored in each
register of M is 0, so that there is zero spike in neurons representing those registers. One
neuron σli is associated with each instruction labeled li of M. During a computation, a
neuron σli is spiked and start to simulate an instruction li:
(
ADD(r), lj, lk
) or
li:
(
SUB(r), lj, lk
)
,starting with neuron σli activated, operating neuron σr as requested by
ADD or SUB; then introducing spikes into neuron σlj or neuron σlk, which becomes active
in this way. When neuron σlh (associated with the label lh of the halting instruction of
M) is activated, a computation in M is completely simulated in ∏; the FIN module starts
to output the computation result.
The LHWSN P system ∏ is composed of three types of modules, ADD modules,
SUB modules, and a FIN modules, which are used to simulate the ADD, SUB, and HALT
instructions of M.
Module SUB shown in Fig. 1. The rules for neurons of SUB module are
a →a and a4(
a5)+∕a4 →a.
il
1
il
2
il
r
3
il
kl
jl
5
1
-
1
-
Fig. 1. The structure of SUB module
Assume at step t, the system starts to simulate a SUB instruction li of M. Having one
spike inside, neuron σli ﬁres at step t emitting a spike to neurons σl1
i and σr. The spike
arriving at neuron σr will be changed into an anti-spike due to the negative weight, and
there are the following two cases in neuron σr.
38
M. Liu and F. Qi

– Before receiving the anti-spike, if the number stored in register r is not zero, then
neuron σr contains at least 5n spikes. After the annihilation, the number of spikes in
neuron σr is decremented to 5(n −1) + 4, which simulates decreasing the number in
register r by one. With 5(n −1) + 4 spikes inside, neuron σr ﬁres at step t + 1 by using
the rule a4(
a5)+∕a4 →a and sends one spike to neuron σl2
i , σl3
i. The spike arriving at
neuron σl2
i will be changed into an anti-spike due to the negative weight. Meanwhile,
neuron σl2
i receives one spike from neurons σl1
i. The anti-spike in neuron σl2
i will be
annihilated by the spike from neuron σl1
i. So neuron σl2
i ends with no spike or anti-
spike inside and remains inactive. By receiving the spike, neuron σl3
i ﬁres at step
t + 2, and the neuron σlj will be ﬁred at step t + 3 to start to simulate instruction lj of
M. It is worth to point out that 5 spikes are sent back to neuron σr at step t + 3 from
neuron σl3
i, which represents that the number in register r is still n.
– Before receiving the anti-spike, if the number in register r is zero, then neuron σr has
no spikes. After receiving the anti-spike, neuron σr has one anti-spike. At step t + 1,
neuron σl1
i ﬁres sending one spike to neuron σl2
i. At step t + 2, neuron σl2
i sends a spike
to neuron σlk and neuron σr. In neuron σr, the anti-spike is annihilated, and it keeps
inactive. At step t + 3, neuron σlk receives a spike from neuron σl2
i and becomes active,
which starts to simulate instruction lk of M.
The simulation of SUB instruction is correct: system P starts from neuron σli and
ends in neuron σlj(if the number stored in register r is great than 0 and decreased by one),
or in neuron σlk(if the number stored in register r is 0).
Module 𝐀𝐃𝐃 shown in Fig. 2— simulating the ADD instruction li:(ADD(r), lj, lk
).
The neurons of ADD module are identical, and the rules contained in the neurons are
a →a , a2 →a, a3 →a, and a3∕a2 →a.
1
il
2
il
3
il
4
il
5
il
6
il
r
il
kl
jl
3
-1
  -1
2
Fig. 2. The structure of ADD module
Local Homogeneous Weighted Spiking Neural P Systems
39

The initial instruction l0 of M is an ADD instruction. Let us assume that at step t, an
instruction li:(ADD(r), lj, lk
) has to be simulated, with one spike in neuron σli and no
spike in any other neurons, except in those neurons associated with registers. Having
one spike inside, neuron σli ﬁres at step t sending one spike to neuron σl1
i, σl2
i and σr, and
three spikes to σl3
i respectively. Neuron σli sends one spike to neuron σr, which simulates
increasing the number in register r by one. With three spikes inside, neuron σl3
i non-
deterministically chooses one of the two spiking rules a3 →a and a3∕a2 →a to apply at
step t + 1, which will non-deterministically activate neuron σlk or σlj.
– In neuron σl3
i, if rule a3 →a is chosen to be used, then it sends one spike to neurons
σl5
i and σl6
i, but when the spike arrives at neuron σl6
i, it will be changed into an anti-
spike due to the negative weight between neurons σl3
i and σl6
i. Neuron σl5
i receives an
anti-spike from neuron σl2
i and a spike from neuron σl3
i. So neuron σl5
i ends with no
spike or anti-spike inside and remains inactive. At step t + 2, neuron σl4
i receives a
spike from neuron σl1
i and neuron σl6
i has an anti-spike. At step t + 3, neuron σl6
i receives
two spikes from neuron σl4
i. In neuron σl6
i, the anti-spike inside will be annihilated by
the spikes from neuron σl4
i. With one spike inside, neuron σl6
i ﬁres at step t + 3 by using
the rule a →a and sends one spike to neuron σlk. By receiving the spike, neuron σlk
ﬁres at step t + 4, simulating instruction lk of M.
– In neuron σl3
i, if rule a3∕a2 →a is chosen to use, then after the rule is applied, neuron
σl3
i can spike for the second times by using rule a →a. It indicates that neuron σl5
i
receives spike and σl6
i receives anti-spike in t + 2 and t + 3 two steps. The two anti-
spikes in neuron σl6
i can be annihilated by the two spikes from neuron σl4
i arriving at
step t + 3.With no spike or anti-spike inside, neuron σl6
i keeps inactive. In neuron
σl5
i, after annihilating the anti-spike inside, it has one spike and ﬁres at step t + 3,
sending one spike to neuron σlj. Neuron σlj ﬁres at step t + 4 to start to simulate
instruction lj of M.
Therefore, from ﬁring neuron σlj, the system adds one spike to neuron σr and non-
deterministically spikes one of neurons σlj and σlk; which correctly simulates the ADD
instruction li:
(
ADD(r), lj, lk
).
Module FIN shown in Fig. 3—outputting the result of computation. FIN module is
composed of neurons from Fig. 4.
40
M. Liu and F. Qi

2
2
2
2
2
2
-
4
1
-
hl
1
h
2
h
3
h
out
1
Fig. 3. The structure of FIN module
Fig. 4. The neuron of FIN module
Assume now that the computation in M halts (that is, the halting instruction is
reached), which means that neuron σlh in ∏ has two spikes and ﬁres. The evolution of
the numbers of spikes in the neurons during the computation of the FIN module is shown
in Table 1.
Table 1. The numbers of spikes in neurons of FIN module during the process outputting the
computational result.
Neuron
Step
t
t + 1
t + 2
t + 3
···
t + n
t + n+1 t + n+2 t + n+3
𝜎lh
1
0
0
0
···
0
0
0
0
𝜎1
5n
5n + 2
5(n−1) + 2
5(n−2) + 1
···
5 + 1
1
1
0
𝜎h1
0
2
0
0
···
0
0
0
0
𝜎h2
0
0
2
2
···
2
2
0
0
𝜎h3
0
0
2
0
···
0
0
2
0
𝜎out
0
0
0
2
···
0
0
0
2
According to the function of the above three modules, it is clear that the register
machine M can be correctly simulated by LHWSN P system ∏. Therefore, LHWSN P
systems can characterize NRE. It is also worth to mention that there are only standard
spiking rules, and no forgetting rule is used. Moreover, we can see that the maximum
number of rules in neurons is 4(in ADD module), so we greatly reduce the number of
rules in the neurons.
Local Homogeneous Weighted Spiking Neural P Systems
41

Theorem 2. NaccLHW7SNP∗= NRE
LHWSN P systems working in accepting mode can accept any set of Turing comput‐
able natural numbers. This result is achieved by simulating register machines working
in accepting mode. In the simulation, the FIN module is not necessary any more, but an
INPUT module is needed to receive spike train 10n-11 from the environment. The
number n of steps elapsed between the two spikes is the number checked to be accepted.
After two spikes are received, if the system ﬁnally halts, then number n is said to be
accepted.
Module INPUT is shown in Fig. 5. An LHWSN P system ∏′ working in the
accepting mode is constructed to simulate M. Each register r of M is associated with a
neuron σr in ∏′; the rules in neuron are a →a and a4(a5)+∕a4 →a. The evolution of the
numbers of spikes in the neurons during the computation of the INPUT module is shown
in Table 2.
2
-
2
-
2
-
7
in
1
d
2d
3d
4
d
0l
1
3
2
Fig. 5. The structure of INPUT module
Table 2. The numbers of spikes in the neurons of INPUT module during the process of reading
spike train 10n−11
Neuron
Step
t
t + 1
t + 2
t + 3
···
t + n
t + n+1 t + n+2 t + n+3
𝜎in
14
10
8
8
···
9
5
3
3
𝜎d1
0
7
7
7
···
7
14
10
10
𝜎d2
0
1
1
1
···
1
2
0
0
𝜎d3
0
1
1
1
···
1
2
0
0
𝜎d4
0
1
0
0
···
0
1
0
0
𝜎l0
0
0
0
0
···
0
0
1
0
𝜎1
0
0
5
5 × 2
···
5(n−1)
5n
5n
5n
42
M. Liu and F. Qi

During the process of reading spike train 10n−11, the evolution of the numbers of
spikes in the neurons of INPUT module is shown in Table 2.
In accepting module, ADD module is deterministic and its form is
(
li:
(
ADD(r), lj
)), which is indicted in Fig. 6. The neurons of this system contain rule
a →a.
il
r
jl
Fig. 6. The structure of deterministic ADD module
Module SUB remains unchanged (as shown in Fig. 1), module FIN is removed, with
neuron σlh remaining in the system, but without outgoing synapses. When neuron σlh
receives two spikes, it means that the computation of register machine M reaches
instruction σlh and stops. Having two spikes inside, neuron σlh spikes (a2 →a is enabled)
without emitting any spike out and the system ∏′ halts.
As explained above, it is obtained that any set of natural numbers accepted by deter‐
ministic register machines can be accepted by LHWSN P system ∏′.
3.2
Analysis of Results
Literature [20] proposed that spiking rules in neurons see the regular expressions as
criteria of spiking and determining whether a regular expression satisﬁed may be a NP
hard problem. This paper reduce the number of rules in neurons, greatly reducing the
time of determining whether a regular expression is satisﬁed and then reducing the time
of the whole module running.
We assume that the time to judge whether the spiking rules are satisﬁed is T1, and
whether the forgetting rules are satisﬁed is T2. Each step of the system needs to deter‐
mine whether the spiking and forgetting rules can be executed. In the following, We
will compare the time required for LHWSNPS and HWSNPS [4] in each module.
In the ADD module of INHWSPS, each neuron has 4 spiking rules, and the ADD
module takes 5 steps from start to ﬁnish, So the total time required for the ADD module
is 20T1; In ADD module of HWSNPS, each neuron has ﬁve spiking rules and two
forgetting rules, and four steps are needed to ﬁnish ADD module, so the total time is
20T1 + 8T2. Similarly, we can calculate the time consumed by two systems in the SUB
module, FIN module and INPUT module respectively. Comparison of execution time
of two systems can be seen in Table 3.
Local Homogeneous Weighted Spiking Neural P Systems
43

Table 3. The time comparison of two system
4
Final Remarks
In this work, inspired by the local homogeneity of biological neurons, we consider a
restricted variant of SN P system (LHWSN P systems). In this system, each neuron in
the same module has the same set of rules, which is having the local homogeneity. We
investigate the computing power of LHWSN P systems. It is obtained that such systems
only using standard spiking rules can achieve the Turing completeness. It reveals that
we not only maintain the homogeneity of the system, but also reduce the time needed
of the system.
Many problems of LHWSN P systems need further researches. It is worth to consider
that whether we can reduce the category of rules in the system without losing the univer‐
sality of LHWSN P systems. Also, the language generated by such systems is worthy
to be studied. Moreover, the next step, we will consider use this SNP system to solve
practical problems.
Acknowledgment. This work was supported by the Natural Science Foundation of China (No.
61502283). Natural Science Foundation of China (No. 61472231). Natural Science Foundation
of China (No. 61640201).
References
1. Păun, G.: Computing with membranes. J. Comput. Syst. Sci. 61(1), 108–143 (2000)
2. Freund, R., Păun, G., Pérez-Jiménez, M.J.: Tissue-like P systems with channel-states. Theor.
Comput. Sci. 330, 101–116 (2005)
3. Ionescu, M., Păun, G., Yokomori, T.: Spiking neural P systems. Fund. Inf. 71(2–3), 279–308
(2006)
4. Zeng, X., Zhang, X., Pan, L.: Homogeneous spiking neural P systems. Fundamenta
Informaticae 97(1–2), 275–294 (2009)
5. Ibarra, O.H., Păun, A., Rodríguez-Patón, A.: Sequential SNP systems based on min/max spike
number. Theor. Comput. Sci. 410(s30–s32), 2982–2991 (2009)
6. Cavaliere, M., Ibarra, O.H., Păun, G., et al.: Asynchronous spiking neural P systems. Theoret.
Comput. Sci. 410(24), 2352–2364 (2009)
7. Pan, L., Păun, G.: Spiking neural P systems with anti-spikes. Int. J. Comput. Commun. Control
4(3), 273–282 (2009)
8. Song, T., Pan, L., Păun, G.: Asynchronous spiking neural P systems with local
synchronization. Inf. Sci. 219, 197–207 (2013)
44
M. Liu and F. Qi

9. Zhang, X., Zeng, X., Pan, L.: Weighted spiking neural p systems with rules on synapses.
Fundamenta Informaticae 134(1–2), 201–218 (2014)
10. Song, T., Zou, Q., Liu, X., et al.: Asynchronous spiking neural P systems with rules on
synapses. Neurocomputing 151, 1439–1445 (2015)
11. Chen, H., Freund, R., Ionescu, M., et al.: On string languages generated by spiking neural P
systems. Fundamenta Informaticae 75(75), 141–162 (2007)
12. Zhang, X., Zeng, X., Pan, L.: On string languages generated by spiking neural P systems with
exhaustive use of rules. Nat. Comput. 7(4), 535–549 (2008)
13. Păun, A., Păun, G.: Small universal spiking neural P systems. BioSystems 90(1), 48–60
(2007)
14. Pan, L., Zeng, X.: Small universal spiking neural P systems working in exhaustive mode.
IEEE Trans. NanoBiosci. 10(2), 99–105 (2011)
15. Song, T., Shi, X., Jinbang, X.U.: Reversible spiking neural P systems. Front. Comput. Sci.
Sel. Publ. Chin. Univ. 7(3), 350–358 (2013)
16. Song, T., Wang, X.: Homogenous spiking neural P systems with inhibitory synapses. Neural
Process. Lett. 42(1), 199–214 (2014)
17. Rozenberg, G., Salomaa, A.: Handbook of Formal Languages: Beyond Words. Springer,
Heidelberg (1997). https://doi.org/10.1007/978-3-642-59126-6
18. Hopcroft, J.E., Motwani, R., Ullman, J.D.: Introduction to automata theory, languages, and
computation, 2nd edn. ACM Sigact News 5(8), 60–65 (2001)
19. Wang, J., Hoogeboom, H.J., Pan, L., et al.: spiking neural P systems with weights. Neural
Comput. 22(10), 2615–2646 (2010)
20. Leporati, A., Zandron, C., Ferretti, C., et al.: On the computational power of spiking neural
P systems. Int. J. Unconventional Comput. 5(5), 459–473 (2009)
Local Homogeneous Weighted Spiking Neural P Systems
45

Weight-Improved K-Means-Based Consensus Clustering
Yanhua Wang, Laisheng Xiang, and Xiyu Liu
(✉)
School of Management Science and Engineering, Shandong Normal University, Jinan, China
15554130027@163.com, xls3366@163.com, sdxyliu@163.com
Abstract. Many consensus clustering methods ensemble all the basic partition‐
ings (BPs) with the same weight and without considering their contribution to
consensus result. We use the Normalized Mutual Information (NMI) theory to
design weight for BPs that participate in the integration, which highlights the
contribution of the most diverse BPs. Then an eﬃcient approach K-means is used
for consensus clustering, which eﬀectively improves the eﬃciency of combina‐
torics learning. Experiment on UCI dataset iris demonstrates the eﬀective of the
proposed algorithm in terms of clustering quality.
Keywords: Consensus clustering · K-means · Basic partitionings
1
Introduction
There is no single clustering algorithm can performs best for all data sets [1], and can
discover all types of cluster shapes and structures [2]. Consensus clustering approached
are able to integrate multiple clustering solutions obtained from diﬀerent data sources
into a uniﬁed solution, and provide a more robust, stable and accurate ﬁnal result [3].
However, the previous research still has some limitations.
Firstly, high quality BPs are beneﬁcial to the performance of consensus clustering
yet the partitions with poor quality will lead to worse consensus result. But most studies
tend to integrate all BPs, and they do not ﬁlter poor BPs. Secondly, the diversity between
BPs also have great impact on consensus clustering, diverse BPs which means the BP
that has diﬀerent mutual information with other BPs will have diﬀerent contribution to
the consensus result. However, there are few references explore impact of the number
of BPs to consensus clustering neither did they take into account the diversity of BPs
into the integration process.
We proposed weight-improved K-means-based consensus clustering (WIKCC).
Firstly, we design weight for each BP participating in the integration. Specifically,
we generate multiple BPs and measure the quality of each BP using normalized Rand
index Rn [6], and sort the BPs in the increasing order of Rn, then we explore the
influence of the number of BPs on consensus clustering, based on the above explo‐
ration we can choose an appropriate number of better BPs for consensus clustering,
which can minimize the number of BPs in quality assurance. After that we construct
the co-occurrence matrix with the selected BPs, and calculate the similarity of two
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 46–52, 2018.
https://doi.org/10.1007/978-3-319-74521-3_6

BPs with Normalized Mutual Information (NMI) method [4] according to the co-
occurrence matrix. Then weight of each BP is designed according to NMI values
which reflect a single BP to overall BPs’ similarity. K-means-based method [2] make
special attention for its simplicity and high efficiency. So we transform consensus
clustering to K-means clustering.
2
Weight Design Based on the Normalized Mutual Information
Mutual information is used to measure the shared information of the two distributions.
We compute the NMI between two BPs, and the greater the value of NMI means the
lower diﬀerence, which will result in lower wi.
Given two BPs results πi with Ki clusters, πi =
{
C(i)
1 , C(i)
2 , … , C(i)
Ki
}
 and πj with Kj
clusters, πj =
{
C(j)
1 , C(j)
2 , … C(j)
Kj
}
 the mutual information between two results is deﬁned
as follows:
NMI(πi, πj
) =
2I1
(πi, πj
)
I2
(πi
) + I2
(πj
)
(1)
I1
(
πi, πj
)
=
∑
h
∑
l
|||C(i)
h ∩C(j)
l
|||
n
log
n|||C(i)
h ∩C(j)
l
|||
|||C(i)
h
||| |C(j)
l |
(2)
I2
(πi
) = −
∑
h
|C(i)
h |
n
log
|C(i)
h |
n
(3)
I2
(πj
) = −
∑
l
|C(j)
l |
n
log
|C(j)
l |
n
(4)
For a single BP the average mutual information can be deﬁned as:
H(𝜋i
) =
1
r −1
∑r
k=1,k≠i NMI(𝜋i, 𝜋k
), (i = 1, 2, … r)
(5)
Where h ∈{1, 2, … , ki
}, l ∈{1, 2, … , kj
} is one of the cluster result label of πi and
πj, |||C(i)
h
|||, |||C(j)
l
||| respectively represent the number of the data set belong to cluster C(i)
h  in πi
and C(j)
l  in πj, |||C(i)
h ∩C(j)
l
||| is the number of the dataset belong both to C(i)
h  and C(j)
l , r is the
number of the BPs.
The greater H(𝜋i
) indicate that cluster member 𝜋i share more information with other
cluster members. The weight is deﬁned as:
w,
m =
1
H(𝜋i
)
(6)
Weight-Improved K-Means-Based Consensus Clustering
47

The normalized form is deﬁned as:
wm =
w,
m
∑r
m=1 w,
m
(7)
The weight is bigger as the greater diversity between two base clustering.
3
The Weight-Improved K-Means-Based Consensus Clustering
In this section, we ﬁrst introduce the co-occurrence matrix which is used for records the
situation of sharing dataset between two BPs. Table 1 shows an example.
Table 1. The co-occurrence matrix
In Table 1, BPs: 𝜋∗ and 𝜋i contain k and ki clusters respectively, n(i)
KKi represents the
number 
of 
the 
objects 
that 
belongs 
to 
both CK 
and C(i)
Ki, 
then 
let
nk+ = ∑Ki
j=1 n(i)
kj , 1 ≤j ≤Ki, 1 ≤k ≤K, P(i)
kj = n(i)
kj ∕n, pk+ = nk+∕n, and p(i)
+j = n(i)
+j∕n. We
can obtain a normalized co-occurrence matrix (NCM), based on which we can compute
the centroid of the K-means clustering.
K-means algorithm cannot directly run on the co-occurrence matrix, so a binary data
set is introduced to represent the result of r BPs. The binary data set
X(b)
l
= {x(b)
l |1 ≤l ≤n} as follows:
x(b)
l
=
⟨
x(b)
l,1, … , x(b)
l,i , … , x(b)
l,r
⟩
, with
(8)
x(b)
l,i =
⟨
x(b)
l,i1, … , x(b)
l,ij, … x(b)
l,iKi
⟩
, and
(9)
x(b)
l,ij =
{ 1, if object l belongs to the cluster Cj in 𝜋i
0, otherwise
,
(10)
Where x(b)
l  is an n × ∑r
i=1 Ki binary data set matrix with |||x(b)
l,i
||| = 1.
48
Y. Wang et al.

We use the K-means algorithm to integrate the BPs, suppose r BPs are integrated to
a result 𝜋∗, mk represent the centroid of the Ck in 𝜋∗ as follows:
mk = ⟨mk,1, … , mk,i, … , mk,r
⟩, with
(11)
mk,i = ⟨mk,i1, … , mk,ij, … , mk,iKi
⟩,
(12)
The centroids of the K-means on Xb are represented as follows:
mk,i =
⟨
p(i)
k1
pk+
, … ,
p(i)
kj
pk+
, … ,
p(i)
kKi
pk+
⟩
, ∀k, i.
(13)
The centroids can be computed by the Co- occurrence matrix, and mk is a vector of
∑r
i=1 ki dimension. The element in the vector is computed by the number of shared data
set between current cluster and all of the clusters of BPs.
By using the co-occurrence matrix and the binary data set the consensus clustering
are transformed to the K-means clustering, that is:
max
∑r
i=1 wiU(𝜋, 𝜋∗) = min
∑k
k=1
∑
xl∈Ck f(x(b)
l , mk
)
(14)
As shown in Fig. 1. In BPs generation phase, classic partition clustering method K-
means is used, diﬀerent initial number of cluster k, to generate diversiﬁed BPs. In
consensus clustering phase, after generating the BPs and computing the weight for each
clustering member, we can obtain the weighted co-occurrence matrix, and then we can
Fig. 1. Algorithm WIKCC
Weight-Improved K-Means-Based Consensus Clustering
49

get the weighted binary dataset X(b)′, by running K-means on weighted binary dataset
X(b)′, we can get ﬁnal consensus result 𝜋∗.
4
Experimental Results
We present experiment on UCI dataset iris. The normalized Rand index (Rn) [6] is
adopted. Its value usually range between [0,1]. The higher value, indicate that the higher
quality of clustering. We demonstrate the cluster validity of WIKCC by comparing it
with two well-known consensus clustering algorithms the K-means-based algorithm
(KCC) [2], the hierarchical algorithm (HCC) [5].
4.1
Quality of BPs
We run K-means algorithm 100 times with the initialized number of clusters randomized
within [K, 
√
n] to generate 100 basic partitionings (BPs) for consensus clustering; K is
the true class of data set, n is the number of the instances, the squared Euclidean distance
is used for the distance function, the quality of each BPs is measured by Rn, the distri‐
bution of quality of BPs is shown as Fig. 2.
0
0.2
0.4
0.6
0.8
0
5
10
15
20
25
30
iris
BPs
Rn
Fig. 2. Clustering quality distribution of BPs
As can be seen in Fig. 2, the distribution of the clustering quality of the BPs show
that there is a large proportion of BPs with poor quality, but only quite a small proportion
of BPs with relatively high quality. This shows that the incorrect pre-speciﬁed number
of classes will lead to weak clustering result.
4.2
Exploration of Impact Factors
In order to In order to determine a suitable number of BPs for WIKCC, we explore the
inﬂuence of the number of BPs on consensus clustering. In the above experiment, r BPs
have generated, and r = 100. We randomly select a part of BPs to obtain the subset
∏r, with r = 10, 20, …, 90. For each r we do KCC [2] algorithm 100 times to get 100
50
Y. Wang et al.

consensus clustering result. The distribution of the quality of consensus clustering result
for diﬀerent subset is shown as Fig. 3.
10
20
30
40
50
60
70
80
90
0.4
0.5
0.6
0.7
0.8
0.9
The number of BPs
 iris
Rn
Fig. 3. Impact of the number of BPs to the consensus clustering
As shown in Fig. 3, when r < 50, the quality of the consensus result present increasing
trend with the increase of r, but when r > 50 the result ﬂuctuate in a mall range and
nearly tend to be stable, it imply that 50 may be the appropriate number of BPs for
WIKCC. Based on above exploration we chose the BPs with the quality of the top 50
BPs for WIKCC.
4.3
WIKCC versus Other Clustering Methods
We compare the WIKCC with KCC and HCC, we choose top 50 better BPs for each
method and run on the iris dataset for 10 times.
0
2
4
6
8
10
0.55
0.6
0.65
0.7
0.75
0.8
0.85
R
n
 
 
WIKCC
KCC
HCC
Fig. 4. WIKCC versus KCC and HCC
We can see in Fig. 4. The WIKCC shows signiﬁcantly higher than KCC, and outper‐
forms better than the HCC in term of the quality of consensus clustering. In addition,
comprising the Figs. 2 and 4, we can see that consensus clustering is much better than
almost all the basic clustering result obtained by K-means, this indicates that, the
consensus clustering method can ﬁnd the real cluster structure more accurately than a
single traditional clustering algorithm by integrating the commonality of many basic
Weight-Improved K-Means-Based Consensus Clustering
51

clustering results, so it can obtain a more stable and accurate clustering result by
ensemble multiple weak BPS.
5
Concluding Remarks
We explore the inﬂuence of the number of BPs on the consensus clustering and chose
appropriate number better BPs for WIKCC. The weight is designed by the NMI method
between two BPs based on co-occurrence matrix. Finally, the experiment on iris demon‐
strates that WIKCC outperforms the state-of-the-art well-known KCC and HCC algo‐
rithms in terms of clustering quality. In the future, we will explore the more other factors
that have inﬂuence on the performance of KCC, and we will consider more other factors
when designing the weights.
Acknowledgment. Projected supported by National Natural Science Foundation of China
(61472231, 61170038, 61502283, 61640201), Jinan City independent innovation plan project in
College and Universities, China (201401202), Ministry of education of Humanities and social
science research project, China (12YJA630152), Social Science Fund Project of Shandong
Province, China (11CGLJ22, 16BGLJ06).
References
1. Strehl, A., Ghosh, J.: Cluster ensembles—a knowledge reuse framework for combining
multiple partitions. J. Mach. Learn. Res. 3(12), 583–617 (2002)
2. Wu, J., Liu, H., Xiong, H., et al.: K-means-based consensus clustering: a uniﬁed view. IEEE
Trans. Knowl. Data Eng. 27(1), 155–169 (2015)
3. Yu, Z., Luo, P., You, J., et al.: Incremental semi-supervised clustering ensemble for high
dimensional data clustering. IEEE Trans. Knowl. Data Eng. 28(3), 701–714 (2016)
4. Vinh, N.X., Epps, J., Bailey, J.: Information theoretic measures for clusterings comparison: is
a correction for chance necessary? In: Proceedings of the 26th Annual International Conference
on Machine Learning, pp. 1073–1080. ACM (2009)
5. Fred, A.L.N., Jain, A.K.: Combining multiple clusterings using evidence accumulation. IEEE
Trans. Pattern Anal. Mach. Intell. 27(6), 835–850 (2005)
6. Tan, P.-N., Steinbach, M., Kumar, V.: Introduction to Data Mining. Addison-Wesley, Reading
(2005)
52
Y. Wang et al.

A Novel Trace Clustering Technique Based on Constrained
Trace Alignment
Pan Wang1(✉), Wen’an Tan1,2, Anqiong Tang2, and Kai Hu3
1 Nanjing University of Aeronautics and Astronautics, Nanjing, People’s Republic of China
pwang@nuaa.edu.cn
2 Shanghai Polytechnic University, Shanghai, People’s Republic of China
{watan,aqtang}@sspu.edu.cn
3 Beihang University, Beijing, People’s Republic of China
hukai@buaa.edu.cn
Abstract. Whenever traditional process discovery techniques are confronted
with complex and ﬂexible environments, equipping all the traces with just one
single model might lead to a spaghetti-like process description. Trace clustering
which splits the logs into clusters and applies discovery algorithm per cluster has
aﬃrmed to be a versatile solution for that. Nevertheless, most trace clustering
techniques are not precise enough due to the indiscriminate treatment on the
activities captured in traces. As a result, the impacts of some important activities
are reduced and some typical information may be distorted or even lost during
comparison. In this paper, we propose a novel trace clustering technique that
based on constrained traces alignment and then adapt two appropriate clustering
strategies into process mining perspective. And experiments on real-life event
logs show that our technique has compelling outperformance in terms of process
models complexity and comprehensibility.
Keywords: Constrained Trace Clustering · Trace clustering · Process mining
Business process management · Constrained Trace Alignment
1
Introduction
Process discovery is one of the most crucial process mining tasks that entails the
construction of process models from event logs of information systems [1]. The most
arduous challenge for process discovery is tackling the problem that discovery algo‐
rithms are unable to generate accurate and comprehensible process models out of event
logs stemming from highly ﬂexible environments.
Trace Clustering is an eﬃcient solution, which clusters the traces such that each
of the resulting clusters corresponds to coherent sets of cases that can each be
adequately represented by a process model [3]. Figure 1 shows the basic procedure for
trace clustering.
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 53–63, 2018.
https://doi.org/10.1007/978-3-319-74521-3_7

Fig. 1. Illustration of the basic procedure of trace clustering in process mining.
Nevertheless, most currently available trace clustering techniques are not precise
enough due to the indiscriminate treatment on the activities captured in traces. As a
result, the impacts of some important activities are reduced and some typical process
information may be distorted or even lost during comparison.
To address the drawback, this paper presents a novel similarity measurement based
on constrained traces alignment. First, some typical causal sequences that reﬂect the
“backbone” of process are identiﬁed. Then, these sequences are exploited as constraints
to guarantee the priority of important activities in traces. Subsequently, we suggest two
clustering strategies that agree with the process mining perspective. The agglomerative
hierarchical clustering (AHC) was selected for its embedded ﬂexibility on abstraction
level to provide us an overall insight into the complex process. And the spectral clus‐
tering has a good recommendation about the number of clusters corresponding to the
generic abstraction level.
In brief, this work contributes by proposing a novel constrained trace similarity
measurement to guarantee the priority of important process episodes and subsequently
adapting two appropriate clustering techniques into process mining perspective. In
addition, experiments on real-life logs prove the improvements achieved by our method
relative to six existing methods.
The rest of the paper is organized as follows: Sect. 2 provides a brief overview of
related works. Next, Sect. 3 introduces our novel constrained trace similarity measure‐
ment and the process-adaptive clustering strategies we selected. And Sect. 4 discusses
the experiment results. Finally, Sect. 5 draws conclusions and spells out directions for
future work.
54
P. Wang et al.

2
Related Work
The main distinction between trace clustering techniques is the clustering bias (distance/
similarity measures) they proposed. Existing approaches in literature can be classiﬁed
into two major categories.
2.1
Distance-Based Trace Clustering
2.1.1
Vector-Based Trace Clustering
Vector-based trace clustering approaches transform traces into a vector space. Then,
clustering can be achieved combining diﬀerent distance metrics in the vector space.
Greco et al. [8] were pioneers in study of clustering log traces within the process mining
domain. They make trace clusters through the vector space over the activities and their
transitions to discover expressive process models at the ﬁrst attempt. They also intro‐
duced a notion of disjunctive workﬂow schemas (DWS) for divisive trace clustering [5].
Song et al. [13] elaborated on constructing so-called proﬁles associated with multiple
trace perspectives as the feature vector.
2.1.2
Context-Aware Trace Clustering
Context-aware trace clustering approaches regard the entire trace as a whole sequence
which implies all the process context information. Then various string edit distance
metrics can be applied on it in conjunction with standard clustering techniques. In [2],
Bose and van der Aalst propose a generic edit distance which derives speciﬁc edit oper‐
ation costs so as to take into account the behavior in traces. The context-aware method
is further developed in [3], it leverages conserved patterns or subsequences as feature
sets to describe the characteristic of a certain trace.
2.2
Model-Based Trace Clustering
2.2.1
Sequence Clustering
Sequence clustering algorithm creates ﬁrst-order Markov chains for clusters cooperating
with the expectation-maximization (EM) algorithm to determine the assignment of a
certain sequence. It has been used to automatically group large protein datasets to search
for homologous gene sequences in bioinformatics. This technique was migrated into
trace clustering by [6].
2.2.2
Active Trace Clustering
Active trace clustering inherits the underlying idea of sequence clustering [15]. Therein
a trace is added to the current cluster if the model discovered from the cluster including
that trace satisﬁes the target threshold of ﬁtness. An optimal distribution of execution
traces over a given number of clusters is achieved whereby the combined accuracy of
the associated process models is maximized. In this way, the quality of process model
discovered is under control. More extension on it has been developed to support further
objectives in [7].
A Novel Trace Clustering Technique Based on Constrained Trace Alignment
55

3
Approach Design
Distance/similarity measurement and clustering strategy with its speciﬁc characteristics
are both important cluster-theoretical aspects. Therefore, we introduce our approach in
the two steps. The framework of the approach in this paper is depicted in Fig. 2.
Fig. 2. Framework of Constrained Trace Clustering.
3.1
Similarity Measurement Based on Constrained Traces Alignment
Just as noted by [10], “specifying an appropriate dissimilarity measure is far more
important than choice of clustering algorithm. This aspect of the problem is emphasized
less in the clustering literature since it depends on domain knowledge speciﬁcs.” There‐
fore, it is inevitable to reﬁne the distance/similarity metrics in trace clustering for
providing more appropriate information to the clustering algorithm.
We can perceive that identifying some signiﬁcant behaviors in traces will assist in
mining better sub-process models by clustering the traces based on those signiﬁcant
behaviors. However, due to the indiscriminate treatment on behaviors in traces and the
lack of domain knowledge, capturing them directly from event logs seems to be a diﬃcult
task.
Fortunately, the association rules in data mining shed light on this tough task.
Employing the association rules, we are able to reveal the “backbone” of process. Then,
some typical causal sequences that reﬂect the process backbone are identiﬁed and
56
P. Wang et al.

exploited as constraints to guarantee the priority of signiﬁcant behaviors during simi‐
larity comparison.
3.1.1
Dependency Measures to Reveal Process Backbone
Deﬁnition 1. (Dependency Measures) Let L be an event log over A. a and b are activities
that occur in L, i.e.; |σ| denotes the length of the trace.
||a >L b|| is the number of times a is directly followed by b in, i.e.,
||a >L b|| =
∑
σ∈L
L(σ) × |{1 ≤i ≤|σ| |σ(i) = a ∧σ(i + 1) = b}|
(1)
||a ⇒L b|| is the value of the dependency relation between a and b:
||a ⇒L b|| =
⎧
⎪
⎪
⎨
⎪
⎪⎩
||a >L b|| −||b >L a||
||a >L b|| + ||b >L a|| + 1 if a ≠b
||a >L a||
||a >L a|| + 1
if a = b
(2)
||a ⇒L b|| produces a value between –1 and 1. If ||a ⇒L b|| is close to 1, then there is a
strong positive dependency between a and b.
By setting μ, the threshold of ||a >L b||, we can ﬁlter out the infrequent items. And
when ||a ⇒L b|| meets certain thresholds υ, we speciﬁed that there is a connection between
a and b.
To illustrate the basic concepts, we use the following event log L:
L = [< a, d, e, f, g, i >16, < a, b, e, f, i >1, < a, c, e, f, h, i >1, < a, c, b, e, f, h, i >12,
< a, e, f, g, i >5, < d, d, a, d, e, f, g, i >1, < a, b, c, e, f, h, i >13, < a, d, d, d, e, f, g, i >1]
Figure 3(A) depicts the dependency graph corresponding to the threshold of μ = 2,
υ = 0.7. And Fig. 3(B) is another dependency graph with the threshold of μ = 5, υ = 0.85.
Obviously, the dependency graph does not show the routing logic but it reveals the
“backbone” of the process model. The derived dependency graph (denote as G) is then
used as a reference to reveal the general order of some typical activities.
Fig. 3. Dependency graphs according to the dependency measures.
A Novel Trace Clustering Technique Based on Constrained Trace Alignment
57

3.1.2
Constrained Similarity Measurement
Let Ai represents the set of activities that involved in trace σi (known as Alphabet) while
B represents the set of activities that involved in the “process backbone”. Ai ∩Aj ∩B
are the common activities between σi and σj with respect to the “process backbone”.
Referring to the dependency graph mentioned above, we can get the causal sequences
of them. For example, the involved common activities between traces
< d, d, a, d, e, f, g, i >, < a, d, d, d, e, f, g, i > in L and the dependency graph are
< a, d, e, f, g, i >, their constraints are shown as the black highlights in Fig. 4. We call
these common activities that attached with causal sequences as typical behaviors.
Fig. 4. Illustration of constraint instance.
These behaviors provide an approximation of the essence of the both comparing
traces in a global perspective. Next, we utilize them as constraint conditions to guarantee
the priority of typical behaviors between traces. In the following, the typical behaviors
between traces σi and σj concerning the process backbone are denoted as Ci,j.
Deﬁnition 2. (Constrained Similarity Measurement) σi and σj are two traces, the
constraints of them are Ci,j. Then, the similarity of σi and σj, Sim(σi, σj) is deﬁned as:
Sim(σi, σj) =
length(CLCS(σi, σj, Ci,j))
max(length(σi), length (σj))
(3)
The constrained measurement is relied on the constrained longest common
sequences (CLCS). The CLCS has already been applied in bioinformatics for the compu‐
tation of the homology of two biological sequences. For more details about CLCS, please
refer to [14].
3.2
Adapted Trace Clustering Strategies
Traditional trace clustering only adapts data-centric clustering algorithms. However, as
described in [3], the most important evaluation dimension for trace clustering is from a
process discovery perspective. This entails the compatibility with process features on
the adopted clustering strategies. Here, we suggest two apposite clustering strategies for
diﬀerent applications.
58
P. Wang et al.

3.2.1
Agglomerative Hierarchical Clustering
Thanks to the hierarchical characteristic of AHC algorithm, it pertinently agrees with
the continuum ranging from unstructured processes to structured processes. The bottom
of AHC means clusters corresponding to each trace whose processes mined are surely
structured while the top represents the only cluster that contains everything whose
process mined is usually unstructured when confronted with ﬂexible environment. Thus,
we are able to ascertain the applicable level as desired or traverse all the hierarchy
straightforward to gain an overall insight into the complex process.
The method we adopt is proposed by [4] termed as GuideTreeMiner. The Guide‐
TreeMiner uses AHC algorithm to build a guide tree (also known as dendrogram). Any
horizontal line spanning over the dendrogram corresponds to a practical clustering at a
speciﬁc abstraction level.
3.2.2
Spectral Clustering
Except for hierarchical clustering algorithms, most of the trace clustering require prede‐
ﬁned parameters for clustering such as the amount of clusters, the maximum cluster size
etc. The truth is the deﬁnition of these specialized parameters are far from easy for
general users due to the lack of domain knowledge. Actually, even for experts, it’s also
not a trivial work as well owing to the complexity and ﬂexibility of real-life process.
Against this background, the spectral clustering was selected as it provides a good
recommendation about the number of clusters [11] which can guide us to a generic
abstraction level.
It’s worth point out that the aﬃnity matrix always calculated as Gaussian kernel,
however, it doesn’t reﬂect the nature of processes. So, in this work, it is calculated as
the constrained similarity described in the previous section. Likewise, the laplacian is
often normalized, but duo to the robustness of constrained similarity measurement
against infrequency and the pursuit of stable clustering indication, we select the non-
normalized laplacian. As for the indication about the cluster number k, it can be recog‐
nized whereby the sudden drop in the eigenvalues. Actually, in many cases, the two
solutions can be used in union.
4
Experiments
4.1
Experiment Conﬁguration and Evaluation Criterion
We used the ProM1 framework which has been developed to support process mining
algorithms to perform the experiments. The data is from Dutch Financial Institute2. And
we adopted the HeuristicsMiner to derive the process model as it has the best capability
to deal with real-life event logs. The approaches to compare with are presented as
follows: DWS Mining [5], Trace Clustering [13], GuideTree Miner [2, 3], Sequence
Clustering [6] and ActiTraC [15].
1 http://www.processmining.org.
2 http://www.win.tue.nl/bpi/doku.php?id=2012:challenge.
A Novel Trace Clustering Technique Based on Constrained Trace Alignment
59

We evaluate the results with respect to their model complexity, as they are measured
by a comprehensive list of metrics reported in [12]:
1. |A| signiﬁes the number of arcs in the process model.
2. |N| signiﬁes the number of nodes in the process model.
3. |CN| = |A| −|N| + 1 signiﬁes the cyclomatic number of the process model.
4. CNC = |A|
|N| signiﬁes the coeﬃcient of connectivity of the process model.
5. Δ =
|A|
|N|⋅|N−1| signiﬁes the density of the process model.
4.2
Clustering Results
We made comparisons with diﬀerent number of clusters for three diﬀerent abstraction
levels. Here, level 1 represents the original trace set, i.e. there is only one cluster. Level
2 stands for 2/3/4 clusters while level 3 contains 5/6/7 clusters. We calculated |A| by
taking their corresponding nodes weighted average and the same as |N|.
The aggregated results are presented in Table 1. All the data has been depicted to
the radarplots in Fig. 5. We can see that all cluster techniques lead to models with lower
complexity than the original log ﬁle. However, the DWS, the ActiTraC and the Trace
Clustering approaches lead to clusters whose models have higher density values than
the unclustered one though they perform well in the other metrics. The smaller area and
more balanced capabilities shown in the radarplots from two abstraction levels proved
the eﬀectiveness of our constraints.
Table 1. The aggregated clustering results
Abstraction
level
Method
Cluster number
A
N
CNC
CN
∆
Level 1
Unclustered
1
141.0
36.0
3.917
106.0
0.112
Level 2
SC
3
101.3
35.7
2.838
65.6
0.082
DWS
4(Std.)
62.3
22.5
2.769
39.8
0.129
TC
3(1–3)
50.0
16.7
2.994
33.3
0.191
ATC
4(3-Std.)
21.5
12.0
1.792
9.5
0.163
GED
3
86.6
34.6
2.503
52.0
0.074
MRA
3
94.4
34.6
2.728
59.8
0.081
Co-TC
3
86.5
35.3
2.450
51.2
0.071
Level 3
SC
6
74.5
34.7
2.147
39.8
0.064
DWS
6(5-5-5-10)
41.7
19.0
2.195
22.7
0.122
ATC
7(6-Std.)
15.6
10.1
1.544
5.5
0.170
GED
6
76.5
33.6
2.277
42.9
0.070
MRA
6
82.3
33.9
2.428
48.4
0.074
Co-TC
6
73.5
33.1
2.221
39.4
0.069
60
P. Wang et al.

Fig. 5. Radarplots of diﬀerent trace clustering techniques.
Moreover, Fig. 6 depicts Constrained Trace Clustering at diﬀerent levels. With the
increasing number of clusters, there is only a little improvement in all aspects. Consid‐
ering the extra elaboration on more clusters, it is ineﬃcient and meaningless to set the
number of clusters to a higher value. This is consistent with the spectral clustering. Just
as the eigenvalues scatterplot shown in Fig. 7, only the ﬁrst three clusters are well sepa‐
rated. Therefore, the spectral clustering can guide us to correctly capture the right level
of abstraction by providing a good recommendation about the number of clusters instead
of the iterations on diﬀerent hierarchies.
Fig. 6. Constrained Trace Clustering from
diﬀerent abstraction levels.
Fig. 7. Similarity matrix eigenvalues.
In a nutshell, all these experiments conﬁrm the eﬀectiveness and eﬃciency of
Constrained Trace Clustering to deliver comprehensible process models from the ﬂex‐
ible and complex logs.
A Novel Trace Clustering Technique Based on Constrained Trace Alignment
61

5
Conclusions and Future Perspectives
In this paper, we contribute to trace clustering techniques by imposing constraints on
trace similarity/distance measurements to guarantee the priority of important activities
in traces. By this means, signiﬁcant process information is preserved as much as possible
such that more accurate trace similarity measurement will be obtained. Moreover, we
integrate two clustering strategies that agree with the process mining perspective to
cluster these traces.
There are still a number of challenging issues remain open for future research work.
Firstly, more reﬁned similarity/distance measurements that felicitously agree with the
process domain knowledge are encouraged. On the issues of processing sequence data,
we should learn from bioinformatics which has more mature applications on sequence
data mining. Another direction of research might be that of integrating advanced clus‐
tering strategies, as currently available techniques only adapt traditional data clustering
techniques which are data-centric instead of process-centric into process mining. More
generally, designing ad hoc clustering strategies usually leads to more suitable clustering
results. Finally, trace clustering techniques only take into account of sorting in the hori‐
zontal direction, it would be promising to combine with techniques that focus on
abstraction in the vertical direction, such as FuzzyMiner [9] which enforces cartography
to the process mining by means of clustering activities.
Acknowledgment. This work is supported in part by the National Natural Science Foundation
of China under Grant No. 61672022, Key Disciplines of Computer Science and Technology of
Shanghai Polytechnic University under Grant No. XXKZD1604, the Fundamental Research
Funds for the Central Universities and Foundation of Graduate Innovation of Shanghai
Polytechnic University, and Foundation of Graduate Innovation Center in NUAA under Grant
No. kj20161601.
References
1. Aalst, W.V.D.: Process Mining: Discovery, Conformance and Enhancement of Business
Processes. Springer, Heidelberg (2011)
2. Bose, R.P.J.C., van der Aalst, W.M.P.:: Context aware trace clustering: towards improving
process mining results. In: SIAM International Conference on Data Mining, pp. 401–412
(2009)
3. Bose, R.P.J.C., van der Aalst, W.M.P.: Trace clustering based on conserved patterns: towards
achieving better process models. In: Rinderle-Ma, S., Sadiq, S., Leymann, F. (eds.) BPM
2009. LNBIP, vol. 43, pp. 170–181. Springer, Heidelberg (2010). https://doi.org/
10.1007/978-3-642-12186-9_16
4. Bose, R.P.J.C., van der Aalst, W.M.P.: Trace alignment in process mining: opportunities for
process diagnostics. In: Hull, R., Mendling, J., Tai, S. (eds.) BPM 2010. LNCS, vol. 6336,
pp. 227–242. Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-15618-2_17
5. de Medeiros, A.K.A., Guzzo, A., Greco, G., van der Aalst, W.M.P., Weijters, A.J.M.M.,
van Dongen, B.F., Saccà, D.: Process mining based on clustering: a quest for precision. In:
ter Hofstede, A., Benatallah, B., Paik, H.-Y. (eds.) BPM 2007. LNCS, vol. 4928, pp. 17–29.
Springer, Heidelberg (2008). https://doi.org/10.1007/978-3-540-78238-4_4
62
P. Wang et al.

6. Ferreira, D.R.: Applied sequence clustering techniques for process mining. In: Handbook of
Research on Business Process Modeling, pp. 492–513 (2009)
7. García BañUelos, L., Dumas, M., La Rosa, M., De Weerdt, J., Ekanayake, C.C.: Controlled
automated discovery of collections of business process models. Inf. Syst. 46, 85–101 (2014)
8. Greco, G., Guzzo, A., Pontieri, L., Sacca, D.: Discovering expressive process models by
clustering log traces. IEEE Trans. Knowl. Data Eng. 18, 1010–1027 (2006)
9. Günther, C.W., van der Aalst, W.M.P.: Fuzzy mining – adaptive process simpliﬁcation based
on multi-perspective metrics. In: Alonso, G., Dadam, P., Rosemann, M. (eds.) BPM 2007.
LNCS, vol. 4714, pp. 328–343. Springer, Heidelberg (2007). https://doi.org/
10.1007/978-3-540-75183-0_24
10. Hastie, T., Friedman, J., Tibshirani, R.: The Elements of Statistical Learning, vol. 167.
Springer, New York (2009). https://doi.org/10.1007/978-0-387-21606-5
11. von Luxbur, U.: A tutorial on spectral clustering. Stat. Comput. 17, 395–416 (2007)
12. Reijers, H.A., Mendling, J.: A study into the factors that inﬂuence the understandability of
business process models. Trans. Sys. Man Cyber. Part A 41, 449–462 (2011)
13. Song, M., Günther, C.W., van der Aalst, W.M.P.: Trace clustering in process mining. In:
Ardagna, D., Mecella, M., Yang, J. (eds.) BPM 2008. LNBIP, vol. 17, pp. 109–120. Springer,
Heidelberg (2009). https://doi.org/10.1007/978-3-642-00328-8_11
14. Tsai, Y.T.: The constrained common subsequence problem. Inf. Process. Lett. 88, 173–176
(2003)
15. Weerdt, J.D., Broucke, S.V., Vanthienen, J., Baesens, B.: Active trace clustering for improved
process discovery. IEEE Trans. Knowl. Data Eng. 25, 2708–2720 (2013)
A Novel Trace Clustering Technique Based on Constrained Trace Alignment
63

A Fast Local Image Descriptor Based on Patch
Quantization
Tian Tian1(B), Fan Yang1, Kun Zheng2, Hong Yao1(B), and Qian Gao3
1 College of Computer Science, China University of Geosciences,
No. 388 Lumo Road, Wuhan 430074, China
{tiantian,yaohong}@cug.edu.cn
2 Faculty of Information Engineering, China University of Geosciences,
No. 388 Lumo Road, Wuhan 430074, China
3 School of Mathematics and Statistics,
Huazhong University of Science and Technology,
No. 1037 Luoyu Road, Wuhan 430074, China
Abstract. Representing local image patches is a key step in many appli-
cations of computer vision, while fast and eﬀective description methods
are always required by real-time image processing. Motivated by the
fact that quantization compresses information while preserving primary
structures, in this paper, we propose to use vector quantization (VQ) on
local patch descriptor building. Compared to conventional approaches
that compress ﬂoating-point features with VQ, we produce local inte-
ger descriptors very fast directly based on simple quantization meth-
ods. Experimental results on a publicly available dataset show that the
present method is eﬃcient both to build and to match. It achieves compa-
rable performance to some typical ﬂoating-point and binary descriptors
such as SIFT and BRIEF, oﬀering a novel solution to fast local image
representation except for bit test created in BRIEF.
Keywords: Local image descriptor · Vector quantization
Feature matching · Cumulative distribution function
1
Introduction
As one of the vital problems in image processing and understanding, image
representation is a core research of many computer vision applications, such
as image registration, image retrieval, and object classiﬁcation. This task can
be generally divided into two main steps: the extraction of feature points and
the description of these keypoints [9]. During the past decades, Scale Invariant
Feature Transform (SIFT) [11] proposed by Lowe was once a benchmark for
its excellent performance. And Speeded Up Robust Features (SURF) [2] which
exhibits much higher eﬃciency than SIFT has then become a de facto standard.
SURF addresses the issue of speed, but, since the SURF feature is still a
vector of ﬂoating point values, the storage and computation of features is still
expensive. Thus, a number of techniques are proposed from diﬀerent aspects to
reduce memory consumption and speed up matching of features. The ﬁrst direct
c
⃝Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 64–75, 2018.
https://doi.org/10.1007/978-3-319-74521-3_8
www.allitebooks.com

A Fast Local Image Descriptor Based on Patch Quantization
65
way is to work with short descriptors, which is obtained by applying dimension-
ality reduction on computed features. Principal Component Analysis (PCA) or
Linear Discriminant Embedding (LDE) [8] can be easily used to reduce descrip-
tor size without loss in recognition performance [13]. Another way to shorten a
descriptor is to quantize its ﬂoating-point coordinates into integers [20] or bina-
rize the feature with a few of bits. Quantization is a simple additional operation
yielding better memory gain and faster matching, and both CHoG [5] and PQ-
WGLOH [21] use vector quantization to build compact descriptors. Binarization
motivated by Locality Sensitive Hashing (LSH) [6] is usually achieved by using
hash functions to turn ﬂoating-point vectors into binary strings. Such implemen-
tation especially on SIFT and GIST [15] can be easily found in the literatures
of improving existing features [14,19].
Though diﬀerent approaches of dimensionality reduction are added, the con-
struction of original full descriptor is still indispensable. In other words, the
substantial amount of time-consuming computation has not be reduced yet.
With the development of mobile devices, algorithms with low computation com-
plexity are in great demand. Binary Robust Independent Elementary Features
(BRIEF) proposed by Calonder et al. [4] constructs the descriptor by directly
comparing pairwise pixel intensities to produce a binary string for an image
patch. Other outstanding features based on bit test include Oriented steered
BRIEF (ORB) [16], Binary Robust Invariant Scalable Keypoints (BRISK) [10],
Fast Retina Keypoint (FREAK) [1], etc.
Diﬀerent from BRIEF-like descriptors, Locally Uniform Comparison Image
Descriptor (LUCID) [22] attempts to describe an image patch from the view of
order permutation of pixel intensities. The employment of permutation brings in
a better resistance to monotonic brightness changes, meanwhile the linear-time
construction and integer representation of LUCID descriptor lead to a good time
eﬃciency. Local Image Permutation Interval Descriptor (LIPID) [18] improved
the robustness of LUCID by means of zone division. These descriptors create
fast and short patch representation in a diﬀerent way from bit test.
As we know, many descriptors are constructed based on gradient, histogram,
bit comparison and ﬁltering. Quantization is also utilized as a postprocessing
step to shorten a descriptor as we have mentioned above. But to our knowl-
edge, no one makes a direct use of it in descriptor building. In this paper, we
show that a local image patch can be described by virtue of patch quantization,
which produces a descriptor that is very fast both to build and to match. We
study two simple ways to implement the patch vector quantization and compare
the respective performance. The experimental results show that the proposed
method is suﬃcient to obtain good matching results, as long as invariance to
large in-plane rotations is not a requirement. Moreover, the quantization-based
method employs integer descriptors which avoid ﬂoating point computation, so
it is promising to be time eﬃcient. In the rest of this paper, Sect. 2 will describe
the overall methodology, experimental results and discussions are provided in
Sect. 3, and conclusions are drawn in Sect. 4.

66
T. Tian et al.
2
Methods
2.1
Image Patch Quantization
Quantization methods have been thoroughly studied in the ﬁeld of signal pro-
cessing, including the coding and compression of diﬀerent signals. Image quan-
tization represents the image with ﬁnite gray levels, whose idea is exactly the
same as we desire for image descriptors – represent the most primary informa-
tion with less data. Inspired by this, we attempt to use vector quantization on
image patches for descriptor construction. Quantizing an image patch refers to
the quantization of the gray scales. A scalar quantization can be understood as
a mapping of an input into a ﬁnite number of output. A straight forward way is
the uniform quantization, which divides the gray scales equally. It is the simplest
and fastest way of implementation, and the intensity mapping function for pixels
within a patch is as following:
f(p) =
(p(x, y) −minv) Nq
maxv −minv + 1

(1)
where
minv =
min
(x,y)∈P p(x, y),
maxv = max
(x,y)∈P p(x, y)
(2)
and p(x, y) is the pixel intensity in a patch P, Nq is the number of quantization
levels, and [·] refers to rounding down.
Though the above method is a fast and reasonable solution, it may be too
simple and crude for not taking intensity distribution into account. Another way
to quantize the patch lays emphasis on equal pixel quantities. This non-uniform
quantization divides the gray scales into disjoint intervals of equal probability by
virtue of the Cumulative distribution function (CDF) [3]. The original histogram
of an image patch counts the number of pixels whose intensities fall into each
gray level, which can be depicted as:
N(i) = N (p(x, y) = i)
(3)
and the CDF of it is calculated as:
CDF(i) =
i

j=minv
N(j)
(4)
where minv is deﬁned in Eq. (2). CDF counts the number of pixels whose inten-
sities are below each gray level, which provides us an expression for quantity
partition. For a CDF function, it is obvious that CDF(maxv) = Npatch (pixel
number of the whole patch), and CDF(minv) = N(minv) (number of pixels
with the smallest intensity). So the intensity mapping function can be written
in two forms:

A Fast Local Image Descriptor Based on Patch Quantization
67
f(p) =

(CDF(i) −CDF(minv)) Nq
CDF(maxv) −CDF(minv) + 1

(5)
=
(CDF(i) −N(minv)) Nq
Npatch −N(minv) + 1

(6)
Figure 1 illustrates how these two quantization methods work by plotting
histograms of an image patch and lines which indicate the dividing gray levels.
Figure 1a shows that the ﬁrst method sets uniform threshold levels within the
range of gray levels. Figure 1b and c describe the process of the second app-
roach. Figure 1b plots the CDF we use to get threshold levels which ensures an
equivalent number of pixels in each interval. The green horizontal lines in Fig. 1b
indicate the uniform partition on pixel quantities, and intersection points of these
lines and CDF function show the locations of thresholds (red lines). Figure 1c
shows these non-uniform thresholds which are obtained in Fig. 1b on gray scales.
Sum of pixels within each interval is approximately the same.
50
100
150
200
0
10
20
30
40
50
pixel intensity
pixel numbers
(a)
50
100
150
200
0
50
100
150
200
250
300
350
400
pixel intensity
cumulative pixel numbers
(b)
50
100
150
200
0
10
20
30
40
50
pixel intensity
pixel numbers
(c)
Fig. 1. Illustration of quantization approaches on image histograms. (a) Quantization
based on equal division on gray levels. (b) Cumulative distribution function and equal
division on pixel numbers. (c) Quantization based on equal pixel quantities. (Color
ﬁgure online)
2.2
Descriptor Construction
Once the intensity mapping function is given, we can obtain a matrix by mapping
all pixels into new values within the patch. Then by vectorizing it, we get the
feature vector as the patch descriptor. Our descriptors take the form of integers,
within the range of 0 to Nq, where Nq equals the number of quantization levels.
A diagram shown as Fig. 2 depicts the descriptor construction process.
The main parameters our descriptors may concern should be the quantization
levels and patch size. Too few levels will result in a loss of feature distinctiveness,
but an excess of levels will also overlook the intention of feature extraction.
As for the size of an image patch, a proper size is also demanded. Oversize
patches will no longer contribute to description eﬀectiveness but cause a waste on
computation power. Usually, parameters are always trade-oﬀs between precision
and recall, accuracy and eﬃciency. We provide some experiments on parameter

68
T. Tian et al.
Image patch
Descriptor
Calculate intensity 
mapping function
Map pixel 
intensities
Patch 
vectorization
Quantization
Fig. 2. The process of descriptor construction.
selection in Sect. 3.2, from which, some empirical parameters can be concluded
for practical uses.
We discover the descriptor quantized via CDF is actually a order-
permutation-based method, which should be invariant to monotonic illumination
changes [17]. As we know in combinatorics, permutation is a bijective mapping
of a ﬁnite set onto itself. It can be understood as a sequence involving every
element in the ﬁnite set which appears only once. We use X = (x1, x2, ..., xn)
to denote the vector of an image patch, and vectorization is arbitrary (e.g, row-
major) but ﬁxed for all patches. Let Z = (0, ..., 0, 1, ...1, ..., Nq −1, ..., Nq −1) be
a vector associated with X, where Z contains all the quantized values of each
element (pixel intensity) in X. Z is a ﬁxed vector and we deﬁne a bijective map-
ping or action π to denote the corresponding relationship between X and Z via
the mapping function in Eq. (6). From the CDF-based quantization process, we
know that this action basically concerns the orders of the elements rather than
the values. Hence, the descriptor D is a permutation on Z, where π−1 is applied
on Z. As we imagine, the descriptor based on patch quantization via CDF is
promising to be robust to monotonic changes.
3
Experiments
In this section, we compare our method with several competing descriptors, and
a real-world dataset that is publicly available is employed in the experiments.
Parameters of the quantization-based descriptors are discussed, and performance
evaluation including recognition rates and computation time is studied.
3.1
Experiment Setup
Our methods are tested on the widely-used datasets introduced by Mikolajczyk
and Schmid [13], as shown in Fig. 3. The dataset contains several image sequences,
each sequence includes six images of a same scene, with varying degrees of a same
image transformation. They are designed to test robustness to typical image dis-
turbances that often occurs in real-world scenarios. For all sequences, we con-
sider ﬁve image pairs by matching the ﬁrst image to the remaining ﬁve ones. Our
quantization-based methods are marked as Q1 (uniform quantization) and Q2
(using CDF). All the experiments are implemented by OpenCV 2.4.4 on a PC with

A Fast Local Image Descriptor Based on Patch Quantization
69
Fig. 3. Oxford dataset introduced by Mikolajczyk and Schmid.
2.93 GHz i7 CPU and Windows 7 Professional 64-bit OS, since OpenCV provides
a convenient implementation of most state-of-the-art features.
Although our descriptor can be computed on arbitrary kinds of keypoint
detector, we ﬁrst carry out the comparison on SURF points. And in the sit-
uation that considers scaling and rotation correction, we would rather employ
the AGAST [12] detector that BRISK feature uses for its fast speed. After key-
points are extracted from each images, 1000 points are selected from them based
on their rankings of Harris corner measure [7]. We evaluate the performance of
descriptors using the recognition rate as BRIEF does. As for the strategy to
determine what is a correct match, we choose the Nearest Neighbor Distance
Ratio (NNDR) [13]. This strategy is more strict than the Nearest Neighbor one,
and produce less but more accurate correspondences. The distance ratio is set
as 0.9 in all the experiments.
3.2
Parameter Selection
We ﬁrst test the impact of parameters in our methods in order to ﬁnd a group of
well-performed parameters. Figure 4 plots the variations of patch size and num-
ber of quantization levels when the other parameter is ﬁxed. As the patch size
increases, recognition rate of image pair matching is improving explicitly. The
curves increase as an exponential function and approach the upper limit when
the patch size is above 16. Hereafter, larger patch size will not contribute to an
obvious improvement on performance but bring in more computation require-
ment. Results of Quantization levels have the similar ﬁndings as the former
parameter. Recognition rate increases rapidly when the quantization level grows
under 8, but starts to level oﬀwith slight ﬂuctuations after that. To sum up,
we consider a patch of 16 × 16 which is quantized to 8 levels is a good choice of
empirical parameters. Therefore, we use this group of parameters in the latter
tests against other descriptors.

70
T. Tian et al.
0
10
20
30
40
50
10
20
30
40
50
60
70
80
90
100
patch size
recognition rates(%)
 
 
Q1
Q2
(a)
0
5
10
15
20
25
30
35
76
78
80
82
84
86
88
90
92
94
96
number of quantization levels
recognition rates(%)
 
 
Q1
Q2
(b)
Fig. 4. Impact of parameter variations in quantization-based descriptors. (a) Recog-
nition rate changes as patch size alters, number of quantization level is ﬁxed to 8.
(b) Recognition rate changes as number of quantization level alters, a 16 × 16 patch
size is selected. Experiments are carried out on image pair Wall 1 and 2.
3.3
Performance Evaluation
Because our original descriptor does not correct for scale and orientation, we
ﬁrst carry out a performance evaluation of several comparable but competing
description approaches. Chief among them are SURF and BRIEF descriptors
provided by OpenCV. The single-scale and orientation-ignored (U stands for
upright) SURF (SU-SURF) is also employed for a fair comparison. Moreover,
we supplement LIPID rather than LUCID as a representative of permutation-
based description methods since LIPID is more robust than LUCID. Note that
all methods involved in this comparison except SURF are scale and orientation
ignored, while SURF is included as a reference of rotation and scale invariant
feature. In addition, the SURF keypoints are detected for all descriptors as we
have explained above, and Boat sequence for zoom and rotation test is also
ignored here. Results of recognition rates are given by Fig. 5.
Our methods outperform most of the others on Wall, Leuven and Ubc images.
For the two sequences of image blur, our algorithms work very well on minor
distortions but degrade faster than BRIEF and SURF. SURF features which
are based on gradient truly suﬀer a lot from image blurring. As for the Graf-
ﬁti sequence, most involved descriptors cannot undergo such an extreme aﬃne
warping. Q1 and Q2 show obvious advantages on texture scene images and com-
pression artifacts. Q2 performs much better than Q1 on monotonic illumination
changes probably because Q2 descriptor inherits inherent robustness from order
permutation, and LIPID shows a talent on Leuven sequence because it is also
permutation-based. Overall, our quantization-based descriptors perform well on
many diﬀerent deformations, and Q2 based on non-uniform quantization shows
a better recognition accuracy than Q1.
With consideration of correcting orientation and scale for the descriptors,
we further compare several state-of-the-art invariant features in the following
experiment. SIFT and SURF are benchmarks of the scale and rotation invariant
features, while BRISK is a remarkable binary feature that handles the invariance

A Fast Local Image Descriptor Based on Patch Quantization
71
1|2
1|3
1|4
1|5
1|6
0
10
20
30
40
50
image pairs
recognition rates(%)
Graf−viewpoint
 
 
SURF
SU−SURF
BRIEF
LIPID
Q1
Q2
1|2
1|3
1|4
1|5
1|6
0
20
40
60
80
100
Wall−viewpoint
image pairs
recognition rates(%)
1|2
1|3
1|4
1|5
1|6
0
20
40
60
80
100
Leuven−light
image pairs
recognition rates(%)
1|2
1|3
1|4
1|5
1|6
0
20
40
60
80
100
Ubc−compression
image pairs
recognition rates(%)
1|2
1|3
1|4
1|5
1|6
0
20
40
60
80
Bikes−blur
image pairs
recognition rates(%)
1|2
1|3
1|4
1|5
1|6
0
20
40
60
80
Tree−blur
image pairs
recognition rates(%)
Fig. 5. Recognition rates of diﬀerent description methods
1|2
1|3
1|4
1|5
1|6
0
20
40
60
80
image pairs
recognition rates(%)
Graf−viewpoint
 
 
SIFT
SURF
BRISK
Q2
1|2
1|3
1|4
1|5
1|6
0
20
40
60
80
100
Wall−viewpoint
image pairs
recognition rates(%)
1|2
1|3
1|4
1|5
1|6
0
20
40
60
80
100
Leuven−light
image pairs
recognition rates(%)
1|2
1|3
1|4
1|5
1|6
0
20
40
60
80
100
Ubc−compression
image pairs
recognition rates(%)
1|2
1|3
1|4
1|5
1|6
0
20
40
60
80
Bikes−blur
image pairs
recognition rates(%)
1|2
1|3
1|4
1|5
1|6
0
20
40
60
80
100
Boat−zoomrotation
image pairs
recognition rates(%)
Fig. 6. Recognition rates of orientation and scale corrected methods

72
T. Tian et al.
problem well. On account of the overall better performance of CDF-based descrip-
tor in the previous experiment, only Q2 is tested during this part. All the features
participated in use their own keypoint detectors, and Q2 employs the multi-scale
AGAST detector that BRISK uses [10] for its best speed. Results of these four
features are shown in Fig. 6. From the rates, it is concluded that the quantization-
based descriptor outperforms SURF and BRISK in most cases. SIFT feature
shows better results than Q2 in viewpoint change sequence, but poorer ones than
the latter under light changes and compression artifacts. For the severe blurring
pairs, multi-scale AGAST detector has a poor repeatability, therefore Q2 performs
well on slight image blur pairs and degrades rapidly ever after. As for scale and
rotation changes, AGAST detector is inferior to SIFT detector on orientation con-
sistency, so the quantization-based feature shows a tendency that it outperforms
SIFT under small deformation while yields poorer results than SIFT as the defor-
mation increases.
0
50
100
150
200
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
NCC
Patches#
SURF
BRISK
Q2
Fig. 7. NCC values for patches from Wall image
We further evaluate the noise resistance of our descriptor. We add Gaus-
sian noise of zero mean and 0.01 variance into Wall 1 image, and extract fea-
ture vectors from the original image and the additive noise image respectively.
Approximate 200 keypoints are detected in the original image and descriptors
of these 200 patches are computed. SURF, BRISK and our method are involved
for evaluation, and Normalized Cross Correlation (NCC) is used to measure the
similarity of the two feature vectors before and after adding noise. As shown in
Fig. 7, the NCC value of our method is higher than BRISK but lower than SURF,
which shows that SURF is the most sensitive to image noises. As BRISK employs
Gaussian smooth before the descriptor building (pixel sampling and bit compar-
ison), it is more robust to Gaussian noise in our evaluation. The real matching
experiment shown in Fig. 8 further illustrates the noise resistance of the involved
features. The wall images added with diﬀerent degree of noises are matched to
itself, and the recognition rates are computed and plotted as the variance of
Gaussian noise varies. As the noise increases, BRISK shows the best resistance,
and performance of SURF has the most severe degradation. Our method retains

A Fast Local Image Descriptor Based on Patch Quantization
73
good performance under slight noise and begins to degrade gradually when noise
keeps increasing.
Finally we provide the time eﬃciency estimation of the proposed method.
Time complexity and CPU running time are included as the evaluation of time
analysis. The uniform quantization can be computed in linear time with no
doubt, and no extra space is need either. And the CDF-based quantization
requires an auxiliary array to store the CDF function, which is also O(n) in
time complexity. Merely, it operates two linear-time computations, one for the
CDF and the other for the mapping, hence it should be more time-consuming
than the uniform one. Table 1 lists the description and matching time of diﬀerent
descriptors on image pair Wall 1-2. For each image, top 1000 points are selected
according to Harris measure from SURF keypoints, and time shown in the table
is the average computation time per point. For a fair assessment, only descrip-
tion time is compared here and all the descriptors except SURF do not take
scale and rotation into account in the description process. It is seen from the
table that the Q1 quantization method shows a deﬁnite superiority on descrip-
tion time consumption. Though Q2 requires more time than the former, it shares
a similar rank with BRIEF. In general, our methods perform competitively well
as state-of-the-art features and require very little computation time.
0.001
0.002
0.003
0.004
0.005
0.01
0.02
0
10
20
30
40
50
60
70
80
90
100
recognition rates(%)
sigma
SURF
BRISK
Q2
Fig. 8. Recognition performance under diﬀerent degrees of Gaussian noises.
Table 1. Comparison of computational timings.
SURF SU-SURF BRIEF
LIPID Q1
Q2
Description time (ms) 1.99
1.43
0.30
1.04
0.12 0.31
Matching norm
L2
L2
Hamming L1
L1
L1
Matching time (ms)
0.13
0.37
0.35
0.28
0.12 0.11

74
T. Tian et al.
4
Conclusion
In this paper, we present a simple quantization-based description method for
local image patch. More detailed, we propose two diﬀerent ways to carry out
quantization based on uniform thresholds on either gray scales or pixel quan-
tities. Generally, descriptors using both quantization methods have very good
time eﬃciency, and perform well on texture scene, illumination changes, com-
pression distortions and image blurring. Speciﬁcally, the CDF-based descriptor
requires more computation time yet yields better results. We think it is a fea-
sible attempt to introduce quantization into fast descriptor building besides bit
sampling and comparison of BRIEF-like descriptors and intensity permutation
of LUCID-like descriptors. The quantization-based methods have similar advan-
tages as permutation-based descriptors and consume as little computation power
as binary ones. Our method can be applied as an alternative of SURF-like and
BRIEF-like descriptor under the circumstance of real-time requirement, lim-
ited computation power and minor image deformation. Future work may aim
at developing the robustness to undergo greater deformation and warping, and
binarized construction methods utilizing quantization is worthy to be discovered
for better storage eﬃciency.
Acknowledgement. This work is supported by the National Natural Science Foun-
dation of China under Grant 61672474 and 41701417, the Fundamental Research Funds
for the Central Universities - China University of Geosciences (Wuhan), the Provincial
Natural Science Foundation of Hubei Province of China under Grant 2016CFB278 and
2015CFB400, the China Postdoctoral Science Foundation under Grant 2016M602390,
and the Open Research Project of Hubei Key Laboratory of Intelligent Geo-Information
Processing (KLIGIP1608).
References
1. Alahi, A., Ortiz, R., Vandergheynst, P.: FREAK: fast retina keypoint. In: 2012
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 510–
517. IEEE (2012)
2. Bay, H., Tuytelaars, T., Van Gool, L.: SURF: speeded up robust features. In:
Leonardis, A., Bischof, H., Pinz, A. (eds.) ECCV 2006, Part I. LNCS, vol. 3951,
pp. 404–417. Springer, Heidelberg (2006). https://doi.org/10.1007/11744023 32
3. Bjornson, E., Ottersten, B.: Post-user-selection quantization and estimation of cor-
related frobenius and spectral channel norms. In: IEEE 19th International Sympo-
sium on Personal, Indoor and Mobile Radio Communications (PIMRC 2008), pp.
1–6. IEEE (2008)
4. Calonder, M., Lepetit, V., Strecha, C., Fua, P.: BRIEF: binary robust independent
elementary features. In: Daniilidis, K., Maragos, P., Paragios, N. (eds.) ECCV 2010,
Part IV. LNCS, vol. 6314, pp. 778–792. Springer, Heidelberg (2010). https://doi.
org/10.1007/978-3-642-15561-1 56
5. Chandrasekhar, V., Takacs, G., Chen, D.M., Tsai, S.S., Reznik, Y., Grzeszczuk,
R., Girod, B.: Compressed histogram of gradients: a low-bitrate descriptor. Int. J.
Comput. Vis. 96(3), 384–399 (2012)

A Fast Local Image Descriptor Based on Patch Quantization
75
6. Gionis, A., Indyk, P., Motwani, R., et al.: Similarity search in high dimensions via
hashing. In: VLDB, vol. 99, pp. 518–529 (1999)
7. Harris, C., Stephens, M.: A combined corner and edge detector. In: Alvey Vision
Conference, Manchester, UK, vol. 15, p. 50 (1988)
8. Hua, G., Brown, M., Winder, S.: Discriminant embedding for local image descrip-
tors. In: IEEE 11th International Conference on Computer Vision (ICCV 2007),
pp. 1–8. IEEE (2007)
9. Huang, Z., Kang, W., Wu, Q., Chen, X.: A new descriptor resistant to aﬃne
transformation and monotonic intensity change. Comput. Vis. Image Underst. 120,
117–125 (2014)
10. Leutenegger, S., Chli, M., Siegwart, R.Y.: BRISK: binary robust invariant scalable
keypoints. In: 2011 IEEE International Conference on Computer Vision (ICCV),
pp. 2548–2555. IEEE (2011)
11. Lowe, D.G.: Object recognition from local scale-invariant features. In: The pro-
ceedings of the seventh IEEE International Conference on Computer Vision, vol.
2, pp. 1150–1157. IEEE (1999)
12. Mair, E., Hager, G.D., Burschka, D., Suppa, M., Hirzinger, G.: Adaptive and
generic corner detection based on the accelerated segment test. In: Daniilidis, K.,
Maragos, P., Paragios, N. (eds.) ECCV 2010, Part II. LNCS, vol. 6312, pp. 183–196.
Springer, Heidelberg (2010). https://doi.org/10.1007/978-3-642-15552-9 14
13. Mikolajczyk, K., Schmid, C.: A performance evaluation of local descriptors. IEEE
Trans. Pattern Anal. Mach. Intell. 27(10), 1615–1630 (2005)
14. Ni, Z.S.: B-SIFT: a binary sift based local image feature descriptor. In: 2012 Fourth
International Conference on Digital Home, pp. 117–121. IEEE (2012)
15. Oliva, A., Torralba, A.: Modeling the shape of the scene: a holistic representation
of the spatial envelope. Int. J. Comput. Vis. 42(3), 145–175 (2001)
16. Rublee, E., Rabaud, V., Konolige, K., Bradski, G.: ORB: an eﬃcient alternative
to SIFT or SURF. In: 2011 IEEE International Conference on Computer Vision
(ICCV), pp. 2564–2571. IEEE (2011)
17. Singh, M., Parameswaran, V., Ramesh, V.: Order consistent change detection via
fast statistical signiﬁcance testing. In: IEEE Conference on Computer Vision and
Pattern Recognition (CVPR 2008), pp. 1–8. IEEE (2008)
18. Tian, T., Sethi, I., Ming, D., Zhang, Y., Ma, J.: LIPID: local image permutation
interval descriptor. In: 2013 12th International Conference on Machine Learning
and Applications (ICMLA), vol. 2, pp. 513–518. IEEE (2013)
19. Torralba, A., Fergus, R., Weiss, Y.: Small codes and large image databases for
recognition. In: IEEE Conference on Computer Vision and Pattern Recognition
(CVPR 2008), pp. 1–8. IEEE (2008)
20. Tuytelaars, T., Schmid, C.: Vector quantizing feature space with a regular lattice.
In: IEEE 11th International Conference on Computer Vision (ICCV 2007), pp.
1–8. IEEE (2007)
21. Wang, C., Duan, L.Y., Wang, Y., Gao, W.: PQ-WGLOH: a bit-rate scalable local
feature descriptor. In: 2012 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), pp. 941–944. IEEE (2012)
22. Ziegler, A., Christiansen, E., Kriegman, D., Belongie, S.J.: Locally uniform com-
parison image descriptor. In: Advances in Neural Information Processing Systems,
pp. 1–9 (2012)

A Method of Large - Scale Log Pattern Mining
Lu Li1(✉), Yi Man1, and Mo Chen2
1 School of Electronic Engineering, Beijing University of Posts and Telecommunications,
Beijing 100876, China
SallyLi0863@163.com
2 Institute of Network Technology, Beijing University of Posts and Telecommunications,
Beijing 100876, China
Abstract. With the development of the telecommunication network, more and
more devices are used in the network, which has been a burden for the network
operation and maintenance. At the same time, network devices generate large
amounts of log data every day, recording the activities of each device in detail.
As a result, the log can reﬂect the performance of network state, and sometimes,
we can predict the occurrence of network failure based on the log. However, since
the log has such features: big volume, multi-source heterogeneous and diﬃcult
to understand, people have not reasonably used it to analyze and predict network
failure. Therefore, we propose a method for structuring a large number of device
logs in the short term, and use the data generated from a real communication
device network to verify the eﬀect. Besides, we compare our method with the
traditional log parsers, such as regular expressions, LogSig, etc. to demonstrate
the eﬃcient processing performance and accurate pattern extraction analysis for
massive network device logs.
Keywords: Big data · Log parser · Telecommunication network equipment
Word2vec
1
Introduction
With the rapid development of telecommunications technology, the telecommunications
network is more complex and the network business is more diverse. At the same time,
the mining for a large amount data generated by network has also attracted the attention
of many people. Network devices log contains a lot of information, which can represent
the operating state and healthy degree of network. However, because of the volume and
characteristic of the log data, the researchers have not achieved remarkable results. For
example, all the equipment in an operator can produce about 2 TB log data in a province
in one day, and these log are written by seven diﬀerent vendors with diﬀerent formats
(Fig. 1).
Obviously, without the instructions and the guidance of the professionals, raw log
message produced by telecommunication network equipment, as shown in the following
example, is diﬃcult for the operator to understand the exact meaning of these logs, not
to mention using it to carry out further work.
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 76–84, 2018.
https://doi.org/10.1007/978-3-319-74521-3_9

At present, there are several methods to deal with log, and methods based on the
pattern are widely used in log analysis. In this method, one raw log message can be
divided into two parts: the constant part and the variable part. For telecommunication
network equipment log, the variable part contains a lot of valid information, such as the
location of the module that issued the log, the actions performed by the operator, and
the time of the log. However, when the volume of log increases to a certain extent, due
to the huge variable data, despite using this method, the results will take up a lot of
storage space.
Therefore, we investigated the various log preprocess methods and for the charac‐
teristics of the network device logs, we have taken certain ways based on the natural
language model, making complex log becomes more suitable for storing and mining. In
order to validate our method, we used it and several other typical log parser to compare
the result in the test set of diﬀerent kinds of logs and the real network device data set,
which proves the accuracy and eﬃciency of our method.
2
Log Parser
2.1
Parser Methods
There are three kinds of methods that are mainly used for log data parser.
2.1.1
Methods Based on Regular Expression
In the traditional log processing methods, regular expression is often used to extract a
speciﬁc ﬁeld. Many programming languages support the use of regular expressions for
string manipulation. It can develop the structured data to process the log, so that a large
number of non-structural or semi-structured information is discarded. And this kind of
method is not ﬂexible enough, basically for some speciﬁc log need to be processed.
2.1.2
Methods Based on Pattern
The log data is automatically generated by the program in the device, which is often
composed by constant strings and variable parameters, so the log data has obvious semi-
structured features. By generating and comparing the existing set of patterns, the words
in the log are divided into log template words and variable words, so that we can ﬁnd
the abnormal parameters in the data set (Fig. 2).
Fig. 1. Typical raw telecommunication device log data
A Method of Large - Scale Log Pattern Mining
77

2015-10-10 18:18:18 A  down
port 8080
2015-10-10 18:18:19 B  down
port 8081
2015-10-10 18:18:19 C  down
port 8082
2015-10-10 18:18:22 A  restart
username = D
password=E
2015-10-10 18:18:23 B  restart
username = F 
password=G
Parser
2015-10-10 18:18:18 pattern 1
2015-10-10 18:18:19 pattern 1
2015-10-10 18:18:19 pattern 1
2015-10-10 18:18:22  pattern 2
2015-10-10 18:18:23 pattern 2
pattern 1: *  down
port *
pattern 2: *  restart
username = *
password=*
Pattern list
Event list
Row Log Data
Fig. 2. The object of methods based on pattern
For example, in [5], the author proposes a STE method to judge whether it is a log
template word or a variable word based on the location and length of the word in the
log text, which can determine the log pattern.
This is the usual method for log analysis, and its speed is faster and performance is
better, but because the device log parameters varies and the data format is very irregular,
leading to the poor quality and large redundancy for the pattern set, which has a great
impact on the eﬀect of the data mining work.
2.1.3
Methods Based on Data Mining
At present, many studies apply the data mining algorithm to the log process, for example,
[6] applied the k-center clustering algorithm to analyze the ITS system log data to
analyze the internal structure of the ITS system. In [7], by using the clustering algorithm,
a log pattern recognition algorithm based on distributed platform is designed to improve
the speed and eﬃciency of log recognition.
However, at present there is not suitable data mining method for telecommunication
network device log analysis.
2.2
Three Typical Parsers
In order to verify the performance of our parser, we chose three typical parsers to
compare with ours. And these parser’s source code can be ﬁnd in the [1].
2.2.1
LKE
This method is proposed to parse free-form text log for anomaly detection in distributed
systems, and it is made up by the following steps: (1) remove the parameters according
78
L. Li et al.

to the established rules (2) measure raw log similarity by the weighted edit distance (3)
cluster similar raw log keys together (4) Log template generation [2].
2.2.2
IPLoM
This method is proposed for automatic event log analysis, which includes three step
hierarchical partitioning process: (1) Partition by log length. The ﬁrst step is to use the
token count heuristic to partition the log messages, because the log messages that have
the same line format are likely to have the same token length (2) Partition by token
position. By counting the words in the same position, the method will sort the log by
the count. (3) Partition by search for mapping. By searching for mapping relationships
between the set of unique tokens in two token positions, the log is divided into smaller
partition. (4) Log template generation [3].
2.2.3
LogSig
To understand and optimize system behaviors, LogSig is proposed to generate system
events from textual log messages. LogSig works in three steps: (1) Word pair generation.
Raw log data are converted to a set of word pairs to record both the word and its position.
(2) Log Clustering. Based on the word pairs, a potential value is calculated for each log
message to decide which cluster the log message potentially belongs to. (3) Log template
generation. In each cluster, the log messages are leveraged to generate a log template
[1, 4].
2.3
Our Parser
Telecommunication device log data often have the following characteristics: (1)
Complex parameters. Most of the parameters are numbers. (2) Short text. Most of logs
are short sentences or parameters list, and the sentence are irregular. The longest
sentence is no more than 30 words. (3) Diﬃcult to understand. Without professional
instruction book, it is diﬃcult to identify the meaning of the log.
According to the characteristics of the raw data, our analytical method has three
steps. First, remove all the punctuations, numbers and the words containing numbers.
Second, compute the Hash value of the processed text, to obtain unique Hash value of
each log text. Third, by comparing the hash values, the log is merged into the same log
pattern, and we can obtain the log pattern table and the simpliﬁed log event sequence.
Fourth, use the edit distance to merge the log patterns again and rewrite the log event
sequence.
2.4
Parser Practice
2.4.1
In the Five Kinds of Log Data Set
We use ﬁve diﬀerent log data, which come from diﬀerent log systems (BGL, HPC,
HDFS, Zookeeper and Proxiﬁer), each kind log data contains two thousand lines [1],
and we used our parser and three other parsers to compare the result. In the experiment,
we used the same environment and code language to come to the following results.
A Method of Large - Scale Log Pattern Mining
79

From the results we can see that for the small data set of log data, IPLoM algorithm
show a great advantage in speed, our algorithm is running faster than the other two
parsers. In terms of accuracy, our parser has an advantage in the analysis of certain logs
(Figs. 3 and 4).
0
50
100
150
200
250
300
350
400
450
LogSig
LKE
IPLoM 
myparser
zookeeper
proxiﬁer
HPC
BGL
SOSP
Fig. 3. The running time of four parsers
0
500
1000
1500
2000
2500
SOSP
BGL
HPC
proxiﬁer
zookeeper
LogSig
LKE
IPLoM 
myparser
Fig. 4. The accuracy of four parsers
80
L. Li et al.

2.4.2
In the Large-Scale Log Data Set
We still experimented with actual data set on, because the actual data set is very large,
we use thousandth log in one day, 750 M data, containing more than 7 million log data
(Table 1).
Table 1. The running time of four parsers on big data set
Parser name
Time(s)
LogSig
38581
LKE
–
IPLoM
376
My parser
7673
We listed the running time of each parser, where LKE was unable to perform log
parsing due to memory overﬂow, and we could see that IPLoM still had a great advantage
in speed, but when there was no standard regular expressions, its results contain a lot of
redundancy. Our parser will give less redundant and more accurate results within a
tolerable time range.
2.4.3
Summary
Comparing our performance with the other three classic parsers, we found that although
our parser was not as fast as IPLoM, it showed good adaptability and speed when dealing
with a large number of telecommunications device logs data. After that, we have carried
out further data mining to understand the data and ﬁnd the information in the log data.
3
Log Analysis
3.1
Background
3.1.1
Word Vector
To apply the machine learning method to the natural language processing ﬁeld, the most
basic problem is the representation of the language symbol. So far, the most commonly
used method for natural language processing is One-hot Representation, which means
that n words are n-dimensional vectors, each vector is 1 in a dimension and the other
dimension is zero. However, this method will cause the lexical gap problem, there is no
connection between words and words.
Therefore, the Distributed Representation method is proposed, that is, using low-
dimensional real vector to represent vocabulary. The biggest advantage of Distributed
Representation is that it can make meaning-related words relatively similar in distance.
At the same time, the word vector will show many special properties, as shown below
(Fig. 5).
Fig. 5. An application of word vector
A Method of Large - Scale Log Pattern Mining
81

3.1.2
Neural Network Language Model
Bengio Yoshua proposed a neural network algorithm using a three-layer neural network
to build the model, the purpose of the model is to predict the next word wt by former
n − 1 words. At the bottom are the former n − 1 words (Wt − n + 1 ~ Wt − 1), and C(w)
means the vector of the word W. The input layer of the network is to concatenate the
n-1 vectors to a (n − 1) * m dimensions vector, which is labeled x. The second layer of
the network is obtained directly by using d + Hx, where d is the oﬀset term, and the
initialization value is random, using tanh as the activation function (Fig. 6).
Fig. 6. Neural network structure proposed by Bengio Yoshua
The third layer of the network is represented by the node Yi, using the softmax
function to normalize the output value into probability, and the ﬁnal Y is calculated as:
Y = b + Wx + U * tanh(d + Hx)
U is the parameter from the hidden layer to the output layer, the majority of the model
compute operation is centered on the matrix multiplication of the U and hidden layers.
Finally, we use the stochastic gradient descent method to optimize the model, then we
can get word vector [8].
82
L. Li et al.

3.1.3
Word2vec
Word2vec is a tool launched by Google to calculate words vector, which has been gained
comprehensive attention because of its eﬃciency and convenience. It is based on the
neural network and the natural language model. By the relationship between words and
sentences, it can calculate a word vector for each word, and we can compare the word
similarity by the distance between the word vectors. Based on the principle of word2vec,
we have present a log mining method that can get literally similar logs or logs containing
a log of important links.
3.2
Use Word2vec in the Log Process
3.2.1
The Method
We use our parser to parse certain carrier’s seven-day telecommunication device log
data. (1) Firstly, we parse the log data into a log pattern set and a log event data table.
(2) Then, we list one-day log pattern number in order of their time sequence, as the
word2vec sentence, and we see each log pattern as word in word2vec. Through the
word2vec tool we calculate the word vector for each pattern. (3) Finally, we derive a
similar pattern set by comparing the Euclidean distance between the word vectors and
comparing the similarity between patterns.
3.2.2
The Result
We optimized the parameters of word2vec for structure log data. When using the Skip-
Gram model, the vector dimension is 50 dimensions and the window size is 5, we ﬁnd
that the word vector is more accurate when looking for similar data patterns. At the same
time, we choose the vectors whose cosine similarity is greater than 0.9 as a similar
pattern. Finally, we get a number of similar patterns, and these similar patterns is of
great signiﬁcance in the problem analysis (Table 2 and Fig. 7).
Table 2. The parameters of word2vec
CBOW
Size
Window
Negative
HS
Sample
Threads
Binary
Iter
0
50
5
0
1
1.00E−04
20
0
100
Fig. 7. The similar log pattern produced by word2vec
In the results, we can ﬁnd that some similar log patterns produced by word2vec have
literal similarity, which means that word2vec can help us to optimize the log parsing
and ﬁnd else log patterns need to be classiﬁed except which has the diﬀerent digital
parameters or whose edit distance is less than a certain value. At the same time, word2vec
A Method of Large - Scale Log Pattern Mining
83

can help us discover log patterns that are literally unrelated but have similar meanings
or links, which is of great importance to subsequent log analysis.
4
Conclusion
Our algorithm uses the neural network language model for the ﬁrst time to analyze the
telecommunication equipment log, and obtains the similarity pattern. At the same time,
we design the log analysis method to adapt to the log of the telecommunication equip‐
ment, and verify the eﬀectiveness of the method by experiment.
Through the experiment and the comparison of the results, we can ﬁnd that our parser
obtains a better analytical eﬀect for the telecommunications server log data in the toler‐
able time. Subsequent analysis, whether using word2vec for similar patterns discovery,
or the use of other data mining methods to explore, such as abnormal point recognition
and correlation analysis, can be based on our processing results for analysis.
However, our analytical methods are also deﬁcient, for example, we can ﬁnd patterns
that have similar characteristics in the order of occurrence, but how these patterns are
applied speciﬁcally to some telecommunications systems problem, such as system error
prediction, auto log analysis system without expert, we also need to continue exploring
and researching.
References
1. He, P., et al.: An evaluation study on log parsing and its use in log mining. In: IEEE/IFIP
International Conference on Dependable Systems and Networks. IEEE Computer Society, pp.
654–661 (2016)
2. Fu, Q., Lou, J., Wang, Y., Li, J.: Execution anomaly detection in distributed systems through
unstructured log analysis. In: Proceedings of International Conference on Data Mining, ICDM
2009 (2009)
3. Makanju, A., Zincir-Heywood, A., Milios, E.: Clustering event logs using iterative
partitioning. In: Proceedings of International Conference on Knowledge Discovery and Data
Mining, KDD 2009 (2009)
4. Tang, L., Li, T., Perng, C.: LogSig: generating system events from raw textual logs. In:
Proceedings of ACM International Conference on Information and Knowledge Management,
CIKM 2011, pp. 785–794 (2011)
5. Kimura, T., lshibashi, K., Mori, T., Shiomoto, K.: Spatio-temporal factorization of log data
for understanding network events. In: INFOCOM 2014 Proceedings. IEEE (2014)
6. Juneja, P., Kundra, D., Sureka, A.: Anvaya: an algorithm and case-study on improving the
goodness of software process models generated by mining event-log data in issue tracking
systems. Support. Care Cancer 6(6), 539–541 (2015)
7. Hamooni, H., Debnath, B., Xu, J., et al.: LogMine: fast pattern recognition for log analytics.
In: CIKM (2016)
8. Bengio, Y., et al.: A neural probabilistic language model. J. Mach. Learn. Res. 3(6), 113 (2003)
84
L. Li et al.

More Eﬃcient Filtration Method for Big Data
Order-Preserving Matching
Wenchao Jiang
(✉), Dexi Lin, Sui Lin, Chuanjie Li, and Aobing Sun
School of Computer Science and Technology, Guangdong University of Technology,
Guangzhou 510006, China
jiangwenchao@gdut.edu.cn
Abstract. Data matching and retrieval aims at ﬁnding out similar substrings with
the pattern P in the given data set T. This problem has wide applications in big
data analysis. A liberalized veriﬁcation rule is proposed ﬁrst, and then a similarity
computing based order preserving matching method is presented. Theory analysis
indicates our method runs in linear. Furthermore, the experimental results show
that our method can improve eﬀectively the precision ratio and the recall ratio.
More qualiﬁed matching results can be detected compared with the state of the
art of this problem.
Keywords: Pattern recognition · Order-preserving matching ·
Similarity retrieval
1
Introduction
Fast and accurate data matching and retrieval is one of the key problems in big data
applications such as video retrieval, stock analysis and prediction. Based on the context
and application scenarios, the data objections can be abstracted into a series of vectors
with diﬀerent properties. Furthermore, the diﬀerent properties can be integrated into a
number through reduction or conversion. Consequently, data matching problem will be
transformed into string or number matching problem which is one kind of well known
problems in pattern recognition. Given a set of numbers T of length n and a pattern P of
length m, both being numbers or strings over a ﬁnite alphabet ∑, the task of string
matching is to ﬁnd all the substrings u in T which have the same relative order as P, and
|u| = |P|. For example, let P = (10, 22, 15, 30, 20, 18, 27) and T = (22, 85, 79, 24, 42,
27, 62, 40, 32, 47, 69, 55, 25), then the relative order of P matches the substring u = (24,
42, 27, 62, 40, 32, 47) of T [1].
Several online [3–7] and one oﬄine solution [2] have been proposed for the string
matching problem. Kim et al. [3] and Kubica et al. [4] presented solutions based on the
Knuth-Morris-Pratt algorithm (KMP) [8], the KMP algorithm is mutated such that is
determines if the text contains substring with the same relative order as that of the pattern
using the order-borders table. Kim et al. [3] utilized the preﬁx representation method to
ﬁnd the rank of each number in the preﬁx, and this method was further optimized using
the nearest neighbor representation to overcome the overhead involved in computing
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 85–94, 2018.
https://doi.org/10.1007/978-3-319-74521-3_10

the rank function. Later, Cho et al. [5, 6] gave a sublinear solution based on the bad
character heuristic of the Boyer-Moore algorithm [9]. Almost at the same time,
Belazzougui et al. [7] derived an optimal sublinear solution. The state of the art for this
problem is the ﬁltration method [1] which is presented by Chhabra et al. All the veriﬁ‐
cation rules in previous researching demanded the values and the positions of the
numbers in both T and P must be coherent strictly. So, some actual matching results
may be discarded. Moreover, almost all the earlier researching were focused on the time
complexity analysis while ignored the precision ratio analysis and recall ratio analysis.
They didn’t research whether some actual matching results were missing despite the
algorithm became more and more fast.
In view of the above problem, a similarity computing based string or number order
preserving matching method is presented. Based on the preprocessing, i.e. binary trans‐
forming and ﬁltration, our method proposes a novel veriﬁcation rule which can guarantee
more candidate results can be found out. Then, a similarity computing based sorting
method is presented which can ensure all the matching results are listed according to
the similarities with the pattern P. Theory analysis indicates our method is sublinear.
The experimental results show that our method can improve eﬀectively the precision
ratio and the recall ratio compared with the newest method at present.
This paper is organized as follows. Section 2 presents our solution. Section 2.4
analyses our approach. The experimental results are given and discussed in Sect. 3.
Section 4 concludes this paper.
2
Our Solution
2.1
Problem Description and Motivation
The state of the art for order-preserving matching is the ﬁltration method [1] which was
presented by Chhabra et al. We call the ﬁltration method as MT.C in this paper. The
MT.C transform the original data T and the pattern P into binary string T’ and P’
according to formulation (1). Then searching for the substring with the same relative
order with P in T can be transformed into searching P’ in the analogously T’. In the
above example, P’ = 101001 and T’ = 100101001100. Each occurrence is a match
candidate which is veriﬁed following the numerical order of the positions in the original
pattern P.
ti =
{
1, ti−1 < ti
0, ti ≤ti+1
,
ti ∈T ;
pi =
{
1, pi−1 < pi
0, pi ≤pi+1
,
pi ∈P
(1)
The MT.C method made the matching process simpler and more eﬃcient than the
earlier solutions. However, some deﬁciencies could be found because of the too harsh
ﬁltration and veriﬁcation rules. The MT.C method required the order of the numbers is
strictly coherent according to the values and the corresponding positions in P and T. For
example, as shown in Fig. 1(a), the MT.C method can’t ﬁnd the diﬀerences between T
and P if the max-number increases or the mini-number decreases immensely in T.
Moreover, as shown in Fig. 1(b), the MT.C method can’t ﬁnd the diﬀerences either if
86
W. Jiang et al.

certain subsection data in T jumps suddenly while the variation trend maintains as a
whole. Furthermore, to ﬁnd out the matching substrings from the candidate strings which
were produced through ﬁltration algorithm, the numbers of P must be sorted, and the
veriﬁcation processing demands the positions and the size relations between T and P
must be coherent strictly. This rule would result in the looseness of some actual matching
substrings. As shown in Fig. 1(c), according to the MT.C method, the gray node x in T
can only change between dashed line a and dashed line b which represent the right
neighbor y and the left neighbor z respectively in the sorting of T. Once the gray node
x changes above the dashed line a or down the dashed line b, the MT.C method will
regard that T is not matching with P. Then the corresponding substring will be discarded
consequently. Actually, we can ﬁnd that this is not the case especially when |y-z| is small
enough. For example, when x changes to x’ which is slightly larger than y or changes
to x” which is slightly smaller than z, the MT.C method will discard the substring. But
we think the substring is still similar with P in most actual applications such as data
retrieval.
Fig. 1. Problem description
2.2
Our Solution
Problem deﬁnition of string or number matching in [1] is described as following: Two
strings u = u1u2…um and v = v1v2…vm of the same length over ∑ are called order-
isomorphic, written u ≈ v, if formulation (2) holds.
ui ≤uj ⇔vi ≤vj for 1 ≤i, j ≤m.
(2)
This rule demands all numbers in u and v are strictly coherent with both the sizes
and their positions. So, some actual matching results would be discarded as shown in
Fig. 1(c). To overcome this deﬁciency, a diﬀerent rule is presented as following. Two
strings u = u1u2…um and v = v1v2…vm of the same length over ∑. The numbers of
v = v1v2…vm are sorted ﬁrstly and the result is a sequential table r as formulation (3).
r = {vr[i]|vr[i] ≤vr[j] ,
1 ≤i < j ≤m}
(3)
More Eﬃcient Filtration Method for Big Data
87

||ur[i] −ur[i]−1|| ≤
||ur[i]+1 −ur[i]−1||
2
or
||ur[i]+1 −ur[i]|| ≤
||ur[i]+1 −ur[i]−1||
2
for vr[i]−1 ≤vr[i] ≤vr[i]+1 , 1 ≤i ≤m.
(4)
Two strings u and v are called order-isomorphic, written u ≈ v, if formulation (4)
holds. Apparently, multiple matching results would be found out according to formu‐
lation (4) once the pattern is given. But, the similar extent of the multiple matching
results are diﬀerent. To distinguish the similar level of multiple candidate results, a
similarity computing method should be given based on the optimized veriﬁcation
method. Considering the possible concussion range of some numbers in T which have
the same positions with relevant numbers in P. The similarity function f, as shown in
Fig. 2, between the candidate substring u and the pattern P is deﬁned as formulation (5).
f(u, P) =
∑
|g(ui) −g(pi)|,
1 ≤i ≤m.
(5)
Fig. 2. Data reduction
Where, ui and pi are the relevant numbers which have the same position order in the
candidate substring u and the pattern P. m is the length of u and P. g is the reduction
function, shown in formulation (6), which can reduce all the numbers in u and p into the
same zone [0,1]. Min(u) and Min (P) are separately the Min-values in u and P. Max(u)
and Max(P) are separately the Max-values in u and P.
ui −Min(u)∕Max(u) −Min(u) or pi −Min(P)∕Max(P) −Min(P)
(6)
As shown in Fig. 2, the similarity between the candidate substring u and the pattern
P can be computed easily as the sum of all the diﬀerences, represented by Diﬀ.i
(1 ≤ i ≤ n), between the relevant nodes of u and P according to formulation (5).
2.3
The Proposed Algorithm
As described above, our solution includes four steps: binary transformation, ﬁltration,
veriﬁcation and similarity computing. The binary transformation can be processed
according to [1], and the ﬁltration can be conducted using any exact string matching
algorithm. Supposing the binary transformation and ﬁltration have been ﬁnished, the
88
W. Jiang et al.

pseudo-code of our solution based on the optimization and the similarity computing can
be described as Algorithm 1 and Algorithm 2.
Algorithm  1: Computing the similarity
1)  Xt=AnticipationBinary (Tr);
2)
Xp=AnticipationBinary (P);
3)
For i=0 to length; 
4)
Sum+=Abs(Xt[i] - Xp[i]);
5)
Return Sum.
Algorithm 2: AnticipationBinary is reduction function
1)
intput A;  
2)  Max=A.Max;
3)  Min=A.Min;
4)  For i=0 to length
5)  If (Max==Min);
6)    B[i]=0;
7)  else
8)    B[i]=(A[i]-Min)/(Max-Min);
9)  Return B.
More Eﬃcient Filtration Method for Big Data
89

2.4
Algorithm Analysis
Our solution, represented as MS, include four steps: Binary Transformation (BT), ﬁltra‐
tion, veriﬁcation and Similarity Computing (SC). So, the time complexity of our solution
O(MS) can be represented as following.
O(MS) = O(BT) + O(ﬁltration ) + O(veriﬁcation) + O(SC)
Compared with [1], the main diﬀerences of our solution are the veriﬁcation and
similarity computing. Furthermore, the main modiﬁcation in veriﬁcation is a liberalized
veriﬁcation rule is implemented and more matching results can be detected. So, the time
complexity of veriﬁcation doesn’t be changed and only the O(SC) need be analyzed.
Supposing the numbers in P and T are integers and they are statistically independent
of each other and the distribution of numbers is discrete uniform. Supposing LT = n,
LP = m. In the worst case, the similarity computing process requires O(m(n-m)) simi‐
larity computing operations. In most cases, LT is usually very large while LP is usually
a constant small enough. So, O(m(n-m)) ≈ O(nm) on average which is equal with MT.C
method. So, we can conclude O(MS) = O(MT.C) which are all sublinear.
3
Experimental Results
Our experiments used linear string matching algorithm KMP [8] as the ﬁltration method.
The tests were run on single node of Tianhe-2 super computer with conﬁguration CPU
E5-2692 v2 12*2 2.20 GHz. All the algorithms were implemented in C# in the 64-bit
mode.
3.1
Eﬀectiveness
To explain the superiority of our method, two special data sets are generated based on
one basic data set. The basic data set was given as T = (10, 14, 12, 15, 13, 28, 36, 32,
24, 38, 26) and P = (10, 14, 12, 15, 13, 19, 21, 20, 17, 22, 18). The ﬁrst special data set
shown in Fig. 3(a) was generated by multiplying n (1 < n < 100) to the last six numbers
in T. The second special data set shown in Fig. 3(b) was generated by multiplying n
(1 < n < 100) to only the 10th number in T.
90
W. Jiang et al.

Fig. 3. Data set samples
From Fig. 3(a) and (b), we can ﬁnd that the data T and the pattern P become more
and more dissimilar with the increasing of n. But, the MT.C method couldn’t ﬁnd the
diﬀerences among them. Through computing the similarities between T and P according
to the technologies presented in Sect. 2, the variation trend of the similarity between T
and P with the increasing of n was shown in Fig. 4. According to the deﬁnition of
similarity shown as formulation (5), the more bigger the value of the similarity is, the
more dissimilar the substring is compared with P. We can ﬁnd that the similarity
increases with the increasing of n and becomes converging with the inﬁnite increasing
of n. The point of inﬂection means that the data set T couldn’t be considered similar
with P according to our method. At the same time, we can ﬁnd that the points of inﬂection
are diﬀerent with diﬀerent data sets. So there is no stable point of inﬂection for diﬀerent
data sets in our method. The appearance of the point of inﬂection depend on the data set
itself.
Fig. 4. Similarity convergence
More Eﬃcient Filtration Method for Big Data
91

Imitating the data generation method in [1], we generated two special data set and
one random data set. The random data set contains 100000000 random integers between
0 and 230. The lengths of patterns (LP) were picked as 10, 15 and 30. The experimental
results showed that our method can ﬁnd more matching substrings than the MT.C
method. For example, when the pattern length was picked as 10, the MT.C method can
only ﬁnd 15 results. However, our method can ﬁnd 135 results which were sorted
according to the similarities. The ﬁrst 10 most similar results were shown in Figs. 5, 6
and 7 separately when the length of pattern were picked as 10, 15 and 30. The red line
represents the pattern, and the blue lines represent the missing results using MT.C
method while are detected using our solution. The black lines represent the matching
results which can be detected using both MT.C method and our solution.
Fig. 5. The most similar 10 results (LP = 10)
Fig. 6. The most similar 10 results (LP = 15)
92
W. Jiang et al.

Fig. 7. The most similar 10 results (LP = 30)
3.2
Time Consuming
Furthermore, to compare the time consuming between our method and the MT.C
method. We enlarged the length of T from 10000000 to 49000000 gradually with an
increment of 1000000. Ten experiments were conducted and the average time
consuming was computed with the diﬀerent lengths of patterns such as 5, 30 and 50.
The statistical results were shown in Fig. 8. Because our method could ﬁnd more results
than the MT.C method, and more similarity computing operations were needed. So the
overall executing time appear a small amount of growth on equal conditions. But, the
executing time growth decreased quickly with the increasing of the length of the pattern.
Because the number of matching results decreased quickly with the increasing of the
length of the pattern.
Fig. 8. Comparison of average time consuming
More Eﬃcient Filtration Method for Big Data
93

4
Conclusion
Fast and accurate data matching and retrieval is a key problem in big data applications
such as video retrieval, stock analysis and bioinformatics etc. A similarity computing
based string or number order preserving matching method is presented. Theory analysis
indicates our method is sublinear. Furthermore, the experimental results show that our
method can improve eﬀectively the precision ratio and the recall ratio compared with
the newest method at present under the same conditions. Compared with former research
works for order-preserving matching problem, our solution liberalized the veriﬁcation
rules on certain extent and so more qualiﬁed matching results can be detected and found
out.
Acknowledgements. 
This work is supported by Guangdong Province Natural Science
Foundation (2016A030313703) and Guangdong Province science and technology program
(2015B010109001, 2015B010131001, 2016B030305002, 2016YFB10005000, 2017B09090
1005, 2017A070712016).
References
1. Chhabra, T., Tarhio, J.: A ﬁltration method for order-preserving matching. Inf. Process. Lett.
116(2), 71–74 (2016)
2. Chhabra, T., Külekci, M.O., Tarhio, J.: Alternative algorithms for order-preserving matching.
In: Prague Stringology Conference (2015)
3. Chhabra, T., Giaquinta, E., Tarhio, J.: Filtration algorithms for approximate order-preserving
matching. In: Iliopoulos, C., Puglisi, S., Yilmaz, E. (eds.) SPIRE 2015. LNCS, vol. 9309, pp.
177–187. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-23826-5_18
4. Cantone, D., Faro, S., Külekci, M.O.: An eﬃcient skip-search approach to the order-
preserving pattern matching problem. In: Prague Stringology Conference (2015)
5. Knuth, D.E., Morris, J.H., Pratt, V.R.: Fast pattern matching in strings. SIAM J. Comput.
6(2), 323–350 (1977)
6. Crochemore, M., Iliopoulos, C.S., Kociumaka, T., Kubica, M., Langiu, A., Pissis, Solon P.,
Radoszewski, J., Rytter, W., Waleń, T.: Order-preserving incomplete suﬃx trees and order-
preserving indexes. In: Kurland, O., Lewenstein, M., Porat, E. (eds.) SPIRE 2013. LNCS,
vol. 8214, pp. 84–95. Springer, Cham (2013). https://doi.org/10.1007/978-3-319-02432-5_13
7. Kim, J., Eades, P., Fleischer, R., et al.: Order-preserving matching. Theoret. Comput. Sci.
525(4), 68–79 (2013)
8. Boyer, R.S.: A fast string searching algorithm. Commun. ACM 20(10), 762–772 (1977)
9. Cho, S., Na, J.C., Park, K., Sim, J.S.: Fast order-preserving pattern matching. In: Widmayer,
P., Xu, Y., Zhu, B. (eds.) COCOA 2013. LNCS, vol. 8287, pp. 295–305. Springer, Cham
(2013). https://doi.org/10.1007/978-3-319-03780-6_26
10. Cho, S., Na, J.C., Park, K., et al.: A fast algorithm for order-preserving pattern matching. Inf.
Process. Lett. 115(2), 397–402 (2015)
11. Belazzougui, D., Pierrot, A., Raﬃnot, M., Vialette, S.: Single and multiple consecutive
permutation motif search. In: Cai, L., Cheng, S.-W., Lam, T.-W. (eds.) ISAAC 2013. LNCS,
vol. 
8283, 
pp. 
66–77. 
Springer, 
Heidelberg 
(2013). 
https://doi.org/
10.1007/978-3-642-45030-3_7
94
W. Jiang et al.

Blind Image Quality Assessment via Analysis of GLCM
Guanghui Yue
(✉), Chunping Hou, Tongtong Ma, and Yang Yang
School of Electrical and Information Engineering, Tianjin University, Tianjin, China
{yueguanghui,hcp,matongtong,yang_yang}@tju.edu.cn
Abstract. Blind image quality assessment (BIQA) assesses the perceptual
quality of the distorted image without any information about its original reference
image. Features, in consistent with human visual system (HVS), have been proved
eﬀective for BIQA. Motivated by this, we propose a novel general purpose BIQA
approach. Firstly, considering that HVS is sensitive to image texture and edge,
the image gradient and wavelet decomposition is computed. Secondly, taking the
direction sensitivity of HVS into account, the gray level co-occurrence matrixes
(GLCMs) are calculated in two directions at four scales on the computed feature
maps, i.e., gradient and wavelet decomposition maps, as well as the image itself.
Then, four features are extracted for each of GLCM matrix. Finally, a regression
model is established to map image features to subjective opinion scores. Extensive
experiments are conducted on LIVE II, TID2013 and CSIQ databases, and show
that the proposed method is superior to the state-of-the-art BIQA methods and
comparable to SSIM and PSNR.
Keywords: Blind image quality assessment (BIQA)
Gray level co-occurrence matrix (GLCM) · Human visual system (HVS)
Image structure
1
Introduction
At present, digital images, as the carrier of massive information, have greatly enriched
people’s life as well as drastically facilitated the communication among people [1, 2].
Yet image distortion remains a stubborn problem in image transmission system. There‐
fore, it is indispensable to establish eﬃcient methods for image quality assessment
(IQA).
Generally, IQA method can be split into two major categories: subjective and objec‐
tive assessment methods. Currently, objective IQA algorithm has been widely studied
because it is easy to implement and portability. Given the available information of the
pristine image, objective assessment method can be further classiﬁed into full-reference
IQA (FR-IQA), reduced-reference IQA (RR-IQA) and no-reference IQA (NR-IQA).
Since both FR-IQA and RR-IQA methods use information of the original reference
image, so they are limited to special situations. In this paper, we mainly focus on the
NR-IQA method.
At present, NR-IQA method can be broadly divided into two classes, i.e., training-
based opinion-aware metric and opinion-unaware metric. The former one requires a
training process to create a regression model for predicting image quality. For example,
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 95–101, 2018.
https://doi.org/10.1007/978-3-319-74521-3_11

Moorthy and Bovik [3] proposed a two-step framework that called BIQI. Speciﬁcally,
each distortion type was trained with a regression model. In such case, the distortion
type of image can be obtained through these models. Subsequently, the image statistical
properties are gradually applied into IQA and have been proved eﬀectively. For instance,
Saad et al. [4] provided a NR-IQA algorithm under the hypothesis that the statistics
features of discrete cosine transform (DCT) coeﬃcients change regularly along with
image quality. Although these methods have achieved meaningful performance, they
require training procedure. To tackle the problem, metrics, which don’t require human
opinion scores and any regression model, have been proposed. Xue et al. [5] used a set
of cluster centroid with quality label as a codebook to predict image quality, called QAC.
Natural image quality evaluator (NIQE) [6] established a completely blind BIQA metric
by ﬁtting the quality-aware features to a multivariate Gaussian (MVG) model. Although
the training process is not required, their performances need to be further improved. In
this paper, we propose a new blind image quality assessment method based on training.
It should be mentioned that the above methods mainly rely on mathematical statistics
method but without full consideration of the HVS characteristics. GLCM can eﬀectively
describe image feature by measuring statistical characteristic of image in multi-direction
and multi-scale. By considering characteristics of HVS and the variety of computing
method for GLCM, this paper presents a simple yet eﬀective BIQA metric. Figure 1
shows the pipeline of our method. It can be divided into the following three parts: calcu‐
lation of GLCM, feature extraction and image quality prediction.
Fig. 1. The pipeline of the proposed method
2
The Proposed Method
2.1
Feature Map Extraction
(1) Gradient map and wavelet transform
Given a color image, ﬁrstly, it is transformed into grayscale, which is denoted by
I(x, y). The direction templates in the horizontal and vertical directions are denoted by
Tx and Ty
Tx = [−1 0 1]
(1)
96
G. Yue et al.

Ty = T′
x
(2)
where ‘′’ denotes transpose.
Then, the gradient components in the horizontal and vertical directions, denoted by
Gx and Gy, are computed as:
Gx = Tx * I
(3)
Gy = Ty * I
(4)
where ‘*’ denotes convolution. Finally, the gradient map G is calculated as:
G =
|Gx| + |Gy|
2
(5)
Wavelet transform decomposes image into multi-scale and multi-direction. The
image is usually transformed along horizontal, vertical and diagonal directions. And
then the decomposition sub-graphs in those three directions are usually denoted by HL,
LH and HH, respectively [15]. In this paper, the wavelet decomposition scale is set to
1, which gets good results.
As image distortion always induces the structural degradation, we desire to evaluate
image quality by utilizing image structure information. Image gradient and decompo‐
sition sub-graphs are complementary to each other in representing rich image structure.
On the one hand, image gradient describes the global image structure while misses
orientation information. On the other hand, wavelet decomposition reﬂects image
features in diﬀerent orientation, while ignores global structure. Hence, their combination
ensures integrity of the image structure information.
(2) GLCM matrix calculation
Usually, image distortion brings about a signiﬁcant change of image statistic char‐
acteristics. GLCM can provide image statistic characteristics in diﬀerent directions and
at diﬀerent scales in spatial domain, so it can describe image characteristics from various
aspects. Based on this, in this paper, the GLCM matrixes of the above image structure
maps are calculated.
The GLCM is composed of the joint probability density between image gray tones.
There are three important parameters in GLCM: angle (θ), quantized gray tones (L) and
distance (d). Firstly, the image is quantized to L gray tones. Then, the probability of
occurrences of the pair of gray tones i and j in original image is expressed in P(i, j, d,
θ) (i = 1, 2, …, L, j = 1, 2, …, L). Each entry (i, j) is depart at a distance d in angle θ.
Finally, the GLCM can be denoted as [P(i, j, d, θ)]L×L, where P(i, j, d, θ) is the element
of [P(i, j, d, θ)]L×L in the i-th row and j-th column.
2.2
Feature Extraction
In [7], fourteen features were extracted from GLCM to represent image properties from
multiple perspectives. Currently, researchers usually used part of them in view of the
Blind Image Quality Assessment via Analysis of GLCM
97

redundancy among them [8]. In this paper, we employ four commonly used features,
namely contrast, energy, correlation, and homogeneity, to extract quality sensitive
features for IQA. Those four selected features involve local and global image charac‐
teristics. Among them, contrast and energy describe the overall characteristics of the
image. Speciﬁcally, contrast describes image deﬁnition. Energy reﬂects the image
distribution as well as roughness. On the contrary, correlation and homogeneity are local
image descriptors. Concretely, correlation illustrates the local correlation of image
grayscale. Homogeneity measures local change of image grayscale. Overall, the selected
features can reﬂect both local and global features of image, to a certain extent. Therefore,
they can be applied into IQA problem.
Although we have demonstrated the feasibility of GLCM in IQA problem, how to
choose the parameters, i.e., θ, L, d, is still a thorny problem. Research shows that HVS
is more sensitive to the horizontal and vertical image information than the oblique
direction [9]. Moreover, diﬀerent viewing distances produce various perception for
HVS. HVS focuses on outline of image at large viewing distance, while at small distance,
it will pay attention to image details [10]. And for GLCM, small scale in GLCM can
describe characteristics of ﬁne image structure, while large scale obtains characteristics
of rough image structure. Inspired by these, we extract GLCM in multi-direction and
multi-scale. Speciﬁcally, θ is set as 0° and 90° to highlight the sensitive direction of HVS,
the distance d is set as 1, 2, 4, and 8 for simulating the variation of viewing distance,
and L is set as 8. Since distortion also corrupts the brightness information, to avoid its
loss, we also extract the above features on distorted image. Overall, the GLCM for
gradient image, decomposed high-frequency sub-images (HL, LH and HH after one
scale wavelet decomposition) and distorted image is calculated in two directions (0° and
90°) at 4 scales, resulting in eight GLCM matrices for each calculated image. A total of
40 GLCM matrices are attained, followed by four features extraction for each GLCM
matrices.
2.3
Image Quality Assessment
After the feature extraction, the realization of image quality assessment is based on a
regression model. Speciﬁcally, the train samples is denoted as T = {(F1, D1), (F2, D2),
…, (Fi, Di), …, (Fm, Dm)}, where i is the index of the train images, Fi ∈ Rn represents
image feature vectors and Di denotes image opinion scores. The array T is trained to
learn a model. Then, the obtained regression model can be used to predict image quality.
Its mapping function can be abbreviated as Dt = model (Ft), where Ft is the feature vector
of the test image and Dt is the predicted quality score. In our metric, we employ support
vector regression (SVR) to evaluate image quality. The LIBSVM toolbox is utilized to
implement Epsilon-SVR with kernel of radial basis function [11].
98
G. Yue et al.

3
Experiment Results and Analysis
3.1
Experiment Setup
The proposed method is tested on three public databases: LIVE II [12], TID2013 [13]
and CSIQ [14] database. In LIVE II database, we test the proposed algorithm on all of
the ﬁve distortion types, i.e., JPEG2000 compression (JP2K), JPEG compression
(JPEG), white noise (WN), Gaussian blur (Gblur) and transmission errors in the JP2K
using Fast-fading Rayleigh channel model (FF). In TID2013 and CSIQ databases, four
distortion types are tested, namely JP2K, JPEG, WN and Gblur. Three general IQA
criteria, i.e., Spearman rank order correlation coeﬃcient (SROCC), Pearson linear
correlation coeﬃcient (PLCC) and root-mean-squared error (RMSE), are employed for
performance evaluation. A better performance means a value close to 1 for PLCC and
SROCC while a value close to 0 for RMSE.
In order to verify the eﬀectiveness of the proposed method, we select two public FR
algorithms (SSIM [15] and PSNR) and several mainstream NR methods (QAC [5], BIQI
[3], ILNIQE [14], GM-LOG [16] and YCLTYCbCr [17]) for comparison. For ROI-
BRISQUE, because the source code is not obtained, we directly use the experiment
results on LIVE II database provided in the original paper for comparison. Since the
proposed method is based on training, we divide the image set into two non-overlapping
image sets: training set and testing set. The training set contains 80% of the reference
images and corresponding distortion versions of them, and the testing set is comprised
by the residual images. After the random train-test split is repeated 1000 times, the
median performance is taken as the ﬁnal results.
3.2
Experiment Results
Table 1 shows the performance tested on the entire database. For better observation, the
top three performed algorithms are highlighted in bold. As we can see, the performance
of the proposed method always lies in top three. In fact, compared with those IQA
methods in Table 1, our method achieves the best performance in all three databases.
Table 1. SROCC, PLCC and RMSE (median value across 1000 train-test trials) of SSIM, PSNR,
QAC, BIQI, NIQE, ILNIQE, GM-LOG and YCLT-YCbCr on the overall database of LIVE II,
TID2013 and CSIQ respectively.
Database
Metric
SSIM
PSNR
QAC
BIQI
IL-NIQE
GM-
LOG
YCLT-
YCbCr
Pro.
LIVE II
PLCC
0.9397
0.9122
0.8755
0.8909
0. 9011 0.9539
0.9354
0.9581
SROCC
0.9244
0.8000
0.8803
0.8899
0. 8996 0.9503
0.9348
0.9524
RMSE
7.9001
14.0604
11.1677
10.5236
12.1186
8.1723
5.9386
6.6445
CSIQ
PLCC
0.9294
0. 9281
0.8645
0.9101
0.8991
0.9408
0.8980
0.9482
SROCC
0.9274
0. 8550
0.8338
0.8925
0.8854
0.9228
0.8869
0.9432
RMSE
0.1040
0.1483
0.1411
0.1198
0.1491
0.0950
0.1295
0.0890
TID2013
PLCC
0.8591
0.9149
0.8273
0.8048
0.9001
0.9439
0.8789
0.9512
SROCC
0.8291
0.9058
0.8188
0.7846
0.8714
0.9282
0.8690
0.9377
RMSE
0.7232
0.5908
0.7670
0.8254
0.6728
0.4629
0.9017
0.4293
Blind Image Quality Assessment via Analysis of GLCM
99

4
Conclusion
In this paper, we propose a simple yet eﬃcient blind IQA metric based on GLCM statistic
model of image structure. To verify the performance of our proposed method, we
conducted a set of experiments on LIVE II, CSIQ and TID2013 databases. We apply it
on the entire database, and the experimental results demonstrate that our predicted scores
is more accuracy than two public FR-IQA algorithms and the mainstream NR-IQA
methods. In summary, we can draw the conclusion that the proposed method obtains
excellent performance in BIQA.
Acknowledgement. This work was supported by the National Natural Science Foundation of
China under Grant 61471262, the International (Regional) Cooperation and Exchange under
Grant 61520106002.
References
1. Gu, K., Lin, W.S., Zhai, G.T., Yang, X.K.: No reference quality metric of contrast-distorted
images based on information maximization. IEEE Trans. Cybern. 47, 4559–4565 (2016)
2. Gu, K., Zhai, G.T., Yang, X.K., Zhang, W.J.: Using free energy principle for blind image
quality assessment. IEEE Trans. Multimed. 17(1), 50–63 (2015)
3. Moorthy, A.K., Bovik, A.C.: A two-step framework for constructing blind image quality
indices. IEEE Signal Process. Lett. 17(5), 513–516 (2010)
4. Saad, M.A., Bovik, A.C., Charrier, C.: Blind image quality assessment: a natural scene
statistics approach in the DCT domain. IEEE Trans. Image Process. 21(8), 3339–3352 (2012)
5. Xue, W.F., Zhang, L., Mou, X.Q.: Learning without human scores for blind image quality
assessment. In: Computer Vision and Pattern Recognition, pp. 995–1002 (2013)
6. Mittal, A., Soundararajan, R., Bovik, A.C.: Making a completely blind image quality
analyzer. IEEE Signal Process. Lett. 20(3), 209–212 (2013)
7. Haralick, R.M., Shanmugam, K., Dinstein, I.: Textural features for image classiﬁcation. IEEE
Trans. Syst. Man Cybern. smc-3(6), 610–621 (1973)
8. Gadelmawla, E.S.: A vision system for surface roughness characterization using the gray
level co-occurrence matrix. NDT E Int. 37(7), 577–588 (2004)
9. Lambrecht, C.J.V.D.B., Verscheure, O.: Perceptual quality measure using a spatiotemporal
model of the human visual system. In: Electronic Imaging: Science & Technology, pp. 450–
461. International Society for Optics and Photonics (1996)
10. Gu, K., Wang, S.Q., Yang, H., Lin, W.S.: Saliency guided quality assessment of screen
content images. IEEE Trans. Multimed. 18(6), 1 (2016)
11. Chang, C.C., Lin, C.J.: LIBSVM: a library for support vector machines. ACM Trans. Intell.
Syst. Technol. 2(3), 389–396 (2007). Article 27
12. Sheikh, H.R., Sabir, M.F., Bovik, A.C.: A statistical evaluation of recent full reference image
quality assessment algorithms. IEEE Trans. Image Process. 15(11), 3440–3451 (2006)
13. Ponomarenko, N., Lukin, V., Zelensky, A., Egiazarian, K., Carli, M., Battisti, F.: TID2008-
a database for evaluation of full-reference visual quality assessment metrics. Adv. Mod.
Radioelectron. 10(4), 30–45 (2009)
14. Larson, E.C., Chandler, D.M.: Categorical image quality (CSIQ) database (2010). http://
vision.okstate.edu/csiq
100
G. Yue et al.

15. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment: from error
visibility to structural similarity. IEEE Trans. Image Process. 13(4), 600–612 (2004)
16. Xue, W., Mou, X., Zhang, L., Bovik, A.C., Feng, X.: Blind image quality assessment using
joint statistics of gradient magnitude and Laplacian features. IEEE Trans. Image Process.
23(11), 4850–4862 (2014)
17. Wu, Q.B., Li, H.L., Meng, F.M., Ngan, K.N., Luo, B., Huang, C., Zeng, B.: Blind image
quality assessment based on multichannel feature fusion and label transfer. IEEE Trans.
Circuits Syst. Video Technol. 26(3), 425–440 (2016)
Blind Image Quality Assessment via Analysis of GLCM
101

An Improved Ranked K-medoids Clustering
Algorithm Based on a P System
Bao Zhang, Laisheng Xiang, and Xiyu Liu(&)
School of Management Science and Engineering, Shandong Normal University,
Jinan 250014, China
zhangbao_sup@163.com,sdxyliu@163.com
Abstract. In this paper an improved ranked K-medoids algorithm by a speciﬁc
cell-like P system is proposed which extends the application of membrane
computing. First, we use the maximum distance method to choose the initial
clustering medoids, maximum distance method which is based on the fact that
the farthest initial medoids were the least likely assigned in the same cluster.
And then, we realize this algorithm by a speciﬁc P system. P system is adequate
to solve clustering problem for its high parallelism and lower computational
time complexity. By computation of the designed system, one possible clus-
tering result is obtained in a non-deterministic and maximal parallel way.
Through example veriﬁcation, our algorithm can improve the quality of
clustering.
Keywords: Ranked K-medoids  Maximum distance  P system clustering
1
Introduction
Clustering is a rapidly developing area which contributes to research ﬁeld including
data mining, machine learning, spatial database technology, biology and marketing and
so on [1]. Clustering analysis is the process of dividing a set of objects into
none-overlapping subsets [1].
Up to now, many kinds of approaches of clustering has appeared, for instance
hierarchical [2], partitioning [3], density-based [4], model-based [5] and grid-based [6].
As a partitioning clustering algorithm, ranked K-medoids algorithm has strong
robustness and high accuracy in contrast to traditional K-medoids algorithm [2]. In this
algorithm K medoids are selected randomly, in this way, two or more medoids will be
assigned to one cluster easily. When this phenomenon appeared, one of them was left
behind, and others should be relocated.
As a new branch of natural computing, membrane computing is a cross-discipline
topic incorporating computer science, biology, artiﬁcial intelligence and so on. It has
the advantage of parallelism so it can lessen the time complexity and improve the
process speed of massive data sets [8, 9].
In this paper, a new version of ranked k-mediods algorithms is proposed in order to
escaping from local optimum. When select the initial medoids we use the method
called “Maximum Distance Method”, by this method we select the accuracy medoids at
the beginning. To realize this algorithm we designed a speciﬁc cell-like P system.
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 102–107, 2018.
https://doi.org/10.1007/978-3-319-74521-3_12

The rest of the paper is organized as follows. Section 2 presents a variant of the
ranked k-medoids algorithm. Section 3 design a speciﬁc P system to realize the
algorithm we proposed. Finally, Sect. 4 makes conclusions.
2
The Improved Ranked K-medoids Clustering Based
on Maximum Distance Method
2.1
Maximum Distance Method
In ranked K-medoids clustering the initial medoids are selected randomly, when more
than one medoids are assigned to the same cluster one of them is left and others
relocated. It wastes a lot of time, to solve this problem we use the method of maximum
distance.
Maximum distance method based the fact as follows: (1) objects who are far from
each other is less likely to assigned to the same cluster. Based on this fact, ﬁrstly we
calculated the distance between every two objects, and two objects who has the
maximum distance are chosen as the initial clustering center. In the remained (N-2)
sample points, select the object as the third cluster center when the product of the
distance to the ﬁrst two initial centers is maximum. The fourth cluster center is selected
like before, and so on,we can ﬁnd the k initial cluster centers.
2.2
Ranked K-medoids Based on Maximum Distance Method
In this paper, a new ranked K-medoids clustering algorithm based on maximum dis-
tance method is prosed
1. Calculate the similarities among pairs of objects based on the similarity metric.
2. Calculate R matrix by sorting the similarity values and store the indexes of similar
objects from the most similar to the least similar in sorted index matrix.
3. Select k medoids use the maximum distance method.
4. Select the group of the most similar objects to each medoid, using sorted index
matrix (The number of members of the group is determined by an input
parameter m).
5. Calculate the hostility values of every object in those groups
6. Choose object with the maximum hostility value as the new medoid.
7. Assign each object to the most similar medoid.
3
P System for the Improved Ranked K-medoids Algorithm
3.1
Construction of the Speciﬁc P System
P ¼ O; M0; M1; M2; . . .Mk; ch; c0; q
ð
Þ
An Improved Ranked K-medoids Clustering Algorithm
103

Where:
O ¼ a11; c1;0; h1; n1; a1; a2; . . .an; #0


ch ¼ ½
1½
2; . . .½
k½
c0
h
i
r0 ¼ w0;0; R0


w0;0 ¼ a11; c1;0; h1; n1; a1;a2; . . .an; b12
R0:
r1 ¼
aijaiaj ! ai j þ 1
ð
ÞaiajUxij
ij
1  i; j  n
j
n
o
[ ai n þ 1
ð
Þ ! a i þ 1
ð
Þ;1 1  i  n
j


[ an n þ 1
ð
Þ ! k


r2 ¼ Uw11
11 ! Dw11
11 Uw11
11
r3 ¼
bh
ijUwij
ij U
wiðj þ 1Þ
iðj þ 1Þ ! b
h þ wijwiðj þ 1Þ
iðj þ 1Þ
Uwij
ij U
wiðj þ 1Þ
iðj þ 1Þ
n
o
[
bh
iðn þ 1Þ ! biði þ 1Þ
n
o
0 \ i \ j  n
j
r4 ¼
bh
0
ij Dwij
ij U
wiðj þ 1Þ
iðj þ 1Þcp ! bh
iðj þ 1ÞUwij
ij D
wiðj þ 1Þ
iðj þ 1Þcp þ 1 h
0 \ 0; 0 \ i \ j \ n

n
o
[
cpDwij
ij aiaj ! Ai
ð
Þ1 Aj


2 p ¼ ðn  1Þ!
j
n
o
r5 ¼
gqUw11
11 Uw12
12    Uw1n
1n Uw21
21    Uwnn
nn !
gqwijUw11
11 Uw12
12    Uw1n
1n Uw21
21    Uwnn
nn


[
gqwijaj ! Aj


t 2\t  k
j
n
o
Fig. 1. Membrane structure
104
B. Zhang et al.

r6 ¼
Uxij
ij ci;q ! ci;q þ 1 1q þ 1
ij
	

in1 1q þ 1
ij
	

in2. . . 1q þ 1
ij
	

ink1q þ 1
ij
xij ¼ 0; 1  i; j  n; 0  q \ n

8
<
:
9
=
;
[
ci; n þ 1
ð
Þ ! c i þ 1
ð
Þ;0 1  i  n
j
n
o
[
cn n þ 1
ð
Þ ! e
n
o
r7 ¼
ci;qUxi1
i1 Uxi2
i2 . . .Uxin
in ! ci;qUxi11
i1
Uxi21
i2
. . .Uxin1
in
1  i  n; 0  q  n
j


r8 ¼
vtAtiaj1p
ijnp ! vtAti Gpj


intnp þ 1 1  i; j  n; 1  t  k; 1  p  m
j
n
o
r9 ¼
vk þ 1nm ! n1 g1
ð
Þin1 g1
ð
Þin2. . . g1
ð
Þink
n
o
r10 ¼
dkaiA1j1A2j2. . .AkjkU
xij1
ij1 U
xij2
ij2 . . .U
xijk
ijk
!
dk ai
ð Þinp A1j1


in1 A2j2


in2. . . Akjk


ink
 
!
ai
xijp ¼ 0; 1  j1; j2; . . .jk  n

8
>
>
<
>
>
:
9
>
>
=
>
>
;
r11 ¼
U
xij1
ij1 U
xij2
ij2 . . .U
xijk
ijk
! U
xij11
ij1
U
xij21
ij2
. . .U
xijk 1
ijk
xijh ¼ 0; 1  j1; j2; . . .jk  n

(
)
r12 ¼
dk ! b
ð Þin1 b
ð Þin2. . . b
ð Þink 1  i  n; 0  j  k
j
n
o
[ 1n ! k
f
g
ri ¼
wi;0; Ri


; 1  i  k Where:
wt;0 ¼ #0
Rt 1  t  k
ð
Þ :
r0
1 ¼
bu ! buai
ð
Þai 1  i  n; u  Otj [ ap 1  p  n
j



n
o
r0
2 ¼
bu ! bu
ð
Þinc0 u  Otj [ ap 1  p  n
j



n
o
[
1 ! k
f
g
r0
3 ¼
AtpG1j1G2j2. . .Gmjm1q1
pj11q2
pj2. . .1qm
pjm ! AtpG1j1G2j2. . .Gmjm1q1
pj11q2
pj2. . .1qm
pjm1Qp
p
1  j1; j2; . . .jm; p  n; 1  t  k
j
(
)
r0
4 ¼
giGijiG1j1G2j2. . .G i1
ð
Þj i1
ð
ÞG i þ 1
ð
Þj i þ 1
ð
Þ. . .
GmjmAtp1q1
jij11q2
jij2. . .1
q i1
ð
Þ
jij i1
ð
Þ1
q i þ 1
ð
Þ
jij i þ 1
ð
Þ. . .1qm
jijm1qp
jip !
giGijiG1j1G2j2. . .G i1
ð
Þj i1
ð
ÞG i þ 1
ð
Þj i þ 1
ð
Þ. . .
GmjmAtp1q1
jij11q2
jij2. . .1
q i1
ð
Þ
jij i1
ð
Þ1
q i þ 1
ð
Þ
jij i þ 1
ð
Þ. . .1qm
jijm1
Qji
ji
1  j1; j2; . . .jm; p  n; 1  i  m
j
8
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
:
9
>
>
>
>
>
>
>
=
>
>
>
>
>
>
>
;
r0
5 ¼
#hAtpGiji1Qp
p 1
Qji
ji
! #h þ Qji QpAtpGiji1Qp
p 1
Qji
ji
1  t  k; 1  ji; p  n; 1  i  m
j
(
)
r0
6 ¼
#h0AtpGiji ! #0GipAtjil h0 [ 0; 1  p; ji  n; 1  i  m; 1  t  k
j


[ #h0 ! #0m h0  0
j
f
g [ gi ! gi þ 1


[ gm þ 1 ! k


r0
7 ¼
1Qp
p 1
Qji
ji
! k 1  ji; p  n
j
n
o
r0
8 ¼
lim jG1j1G2j2. . .GmjmAtp1 ! baj1aj2. . .ajmAtpv1


in0 1  j1; j2. . .jm; p  n; 1  t  k
j
n
o
[
miAtp ! Atpd


out
	

:l j 1  i; j  n
j


q ¼
ri [ rj 1  i \ j  11
j


[
r0
i [ r0
j 1  i \ j  8
j
n
o
An Improved Ranked K-medoids Clustering Algorithm
105

The P system we construct include membrane M0, membrane Mt(1  t  k),
W0;0 is the initial multiset that include in the membrane M0, and a1; a2    an represent
the objects in a given data set. R0 represents the rules in membrane M0 (Fig. 1).
3.2
Experiment and Results
In order to give a better interpretation of our P system model for the improved ranked
K-medoids clustering, we take an example to simulate the procedure of the P system.
There are 12 points (Fig. 2):
The P system is supposed to distribute the points into 3 clusters with the given
parameter m of the value 3. Diagram 2 depicts the original state of the 13 points.
First, calculate the distance between every two points, then ﬁnd points (1, 1), (5, 7),
(8, 2) that satisfy maximum distance method. After that, the rules in three membranes
are executed at the same time and ﬁnd the ﬁnal clustering medoids by hostility values.
Finally, sent the objects to the nearest clusters.
Eventually, the clustering result was attained that the 13 points were classiﬁed into
3 clusters. And the clustering effect sketch is shown in Fig. 3.
Fig. 3. The ﬁnal clustering result
Fig. 2. The initial state of the points
106
B. Zhang et al.

4
Conclusions
Ranked K-medoids is a novel partitioning clustering algorithm, it propose a novel new
term named hostility value. The medoid is update in each iteration ﬁnally it will ﬁnd the
center of the clusters, objects will be assigned to the nearest cluster.
This paper present a new ranked K-medoids algorithm that based on the maximum
distance method, by this method we can ﬁnd the suitable initial medoids. By this mean
we don’t need to relocate the medoids that are assigned to the same cluster, and proﬁt
from characteristic great parallelism of P system, so that the time of iteration is shorten.
Acknowledgment. This work is supported by the Natural Science Foundation of China (Nos.
61170038, 61472231, 61402187, 61502535, and 61572523).
References
1. Han, J., Kambr, M.: Data mining concepts and techniques, Elsevier Inc., USA (2012). ch. 8
2. Razavi Zadegan, S.M., Mirzaie, M., Sadoughi, F.: Ranked k-medoids: a fast and accurate
rank-based partitioning algorithm for clustering large datasets. Knowl.-Based Syst. 39(2),
133–143 (2013)
3. Zhang, T.: BIRCH: an efﬁcient data clustering method for very large databases.
ACM SIGMOD Record 25(2), 103–114 (1996)
4. Kaufman, L., Rousseeuw, P.J.: Finding Groups in Data: An Introduction to Cluster Analysis
(1991)
5. Sander, J., Ester, M., Kriegel, H.P., et al.: Density-based clustering in spatial databases: the
algorithm GDBSCAN and its applications. Data Min. Knowl. Disc. 2(2), 169–194 (1998)
6. Fraley, C., Raftery, A.E.: Model-based clustering, discriminant analysis, and density
estimation. J. Am. Stat. Assoc. 97(458), 611–631 (2002)
7. Wang, W., Yang, J., Muntz, R.R.: STING: a statistical information grid approach to spatial
data mining. In: International Conference on Very Large Data Bases, pp. 186–195. Morgan
Kaufmann Publishers Inc. (1997)
8. Peng, H., Wang, J., Pérez-Jiménez, M.J., et al.: An unsupervised learning algorithm for
membrane computing. Inf. Sci. 304(C), 80–91 (2015)
9. Niu, Y., Xiao, J., Jiang, Y.: Time-free solution to 3-coloring problem using tissue P systems.
Chin. J. Electron. 25(3), 407–412 (2016)
An Improved Ranked K-medoids Clustering Algorithm
107

A Novel Inverse-Operation Based Group Undo/Redo
Algorithm for Feature-Based 3D Collaborative CAD
Systems
Yuan Cheng1, Fazhi He2(✉), Xiao Lv2, and Weiwei Cai2
1 School of Information Management, Wuhan University, Wuhan, China
2 School of Computer Science, Wuhan University, Wuhan, China
fzhe@whu.edu.cn
Abstract. Supporting group Undo/Redo with high performance in a 3D
designing environment still remains a challenge. The question is how to recover
the document state as if an operation is never executed. In this paper, we are going
to propose an inverse-operation based group Undo/Redo solution for feature-
based 3D collaborative CAD systems. It allows a 3D part to be manipulated
locally. We developed an ontology in the CAD domain so as to describe common
elements in typical feature-based CAD systems. By classifying features according
to how they aﬀect a volume, feature modeling operations are categorized into four
groups. We are able to create inverse operations for operations belonging to the
same category. The proposed methods are tested in a prototype system with case
study.
Keywords: Feature-based collaborative CAD · Group Undo/Redo
Inverse operation · Ontology
1
Introduction
Feature-based modeling approaches have played a relevant role for qualitative knowl‐
edge speciﬁcation and integration in collaborative designing since the 70s [1–3], and
feature-based CAD systems are currently considered the state-of-art technologies for
product modeling [4–7]. Undo/Redo is a standard and core function for nearly all human-
computer interaction applications. It can help to remove some erroneous operations and
enable a user to explore a new system by try-and-failure. Most importantly, undo/redo
is an essential tool to grant that a document is the result of designers’ intentions by
removing the eﬀects of undesired operations. An Undo/Redo mechanism should under‐
stand which operation is to be undone when a user issues an undo command by diﬀerent
interaction methods. Besides, the undo eﬀect demands that the result of a certain oper‐
ation is eliminated as if it has never been executed. This leaves us a question that how
the eﬀect of an undo target be removed from the 3D model. Our previous study inves‐
tigate both full-rerun and full checkpoint strategy[8, 9], although the correctness of our
group Undo/Redo algorithms can be promised, the performance is still not satisfying.
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 108–117, 2018.
https://doi.org/10.1007/978-3-319-74521-3_13

Treating Undo as an inverse operation is a prevailing strategy for collaborative
document and graphic editing. The inverse operation is executed on the current docu‐
ment directly. The document does not need to be rolled back and forth which is consid‐
ered to be more eﬃcient than the previously mentioned two undo strategies. This mech‐
anism depends largely on two conditions. One is that there should be quite a limited
number of primitive operations. For example, in collaborative document editing, there
are simply two primitive operations, say insert and delete, which are regarded as the
inverse operation of each other [10]. The other one is that an object from the document
should be easily located and distinguished from other objects. A feature-based CAD
application retains a large quantity of feature modeling operations. Generating an inverse
operation for every feature modeling operation still remains a challenge.
In this paper, we are going to propose an inverse-operation based group undo/redo
approach to serve the purpose of both intention preservation and high performance. The
basic idea is to generate inverse operations by building a domain ontology and classi‐
fying the modeling operations into quite a limited number of groups. An Inverse oper‐
ation is generated in a uniformed way for an operation category but not for any speciﬁc
operation. The remaining of this paper is organized as follows. Section 2 reviews the
related works from the perspective of current group undo methods and ontologies build
in the CAD domains. In Sect. 3, the ontology constructed in the CAD domain is intro‐
duced. Section 4 provides a detailed description of how to create inverse operations
diﬀerent feature groups. Section 5 includes the experiments to illustrate the our group
Undo/Redo approach based on the inverse-operation model. Conclusions and limitations
are given.
2
Related Works
Several undo models are proposed for multi-user Undo/Redo. Each model sets a frame‐
work for developing speciﬁc undo and redo algorithms. The script model [11] is one of
the earliest group Undo/Redo solutions. Regional undo was also applied in spreadsheets
[12] by allowing users to select a part of the spreadsheet and perform undo on the cells.
Treating an undo as a concurrent inverse operation is another popular solution for group
undo. For the ﬁrst time, Berlage introduced the selective undo model that adds the
reverse operation of the selected command to the current context [13]. In collaborative
textual editors, delete and insert can be the inverse operation for each other. DistEdit
[14] is the ﬁrst OT based selective undo solution. The undo target is transposed to the
end of the history buﬀer and the inverse command of the transposed operation is
executed. The adopted [15, 16] also considered an inverse as the concurrent operation
of operations generated after O. Neither DisEdit nor adOPTed supports the full featured
selective undo. Sun has proposed the framework of AnyUndo to support “Anytime,
Anywhere undo”. The proposed algorithms GOTO and COT both integrate do and
selective undo [17, 18]. Azurite [19, 20], provides general selective undo in a code editor.
Azurite uses the inverse model, by adding the inverse text editing operation to the end
of the history, and provides a variety of user interfaces to resolve conﬂicts. Logoot-Undo
[21] is the group undo algorithm supporting AnyUndo under the CRDT framework.
A Novel Inverse-Operation Based Group Undo/Redo Algorithm
109

The document is manipulated at the line-level. The eﬀects of that an inserted line is
undone is actually controlled by changing the visibility of the line. Conﬂicts between
two concurrent undos are therefore avoided because if an undo cannot ﬁnd a line given
its unique identiﬁer, it can therefore be discarded.
Applying the inverse model to graphical editors seems easier than to painting
systems. On one hand, in an object-oriented system, the inverse operation can be easily
generated by retrieving the designing history, and then executing some system deﬁned
operation by using the properties of the graphical object before the latest modiﬁcation
[13, 22]. In pixel-based drawing systems, such as Photoshop, it is the historical infor‐
mation of every pixel that needs to be recorded which is resource consuming. In Wang’s
solution for undo in bitmap systems [23], only pixels of the drawing area and connections
between operations are recorded. Aquamarine [24, 25] is a system developed on the
basis of Photoshop for selective undo using the script model. Another undo scheme
developed under the script model is the group Undo/Redo method for 3D collaborative
CAD systems developed by our previous research [25].
3
Ontology-Based Inverse Operation Generation
We build an ontology, called Part Feature Ontology (PFO), to describe the semantics of
a part from the perspectives of individual settings and parameter values. We also intend
to use this ontology to categorize features in a 3D collaborative CAD system. The classes
deﬁned in PFO are: modeling feature, topology info, B-rep Operation, Reference Info
and Sketch Info. Six relations are deﬁned, which are: is-a, has-topology-attribute, has-
reference-attribute, has-boolean-operation, as-dependency-attribute, and has-sketch.
We categorize features according to their eﬀects on the volume. The subclasses of
modeling feature include convex feature, concave feature, edge transition feature and
primitive feature.
3.1
Inverse Operation for Convex Features
The inverse operation of a convex feature creating operation is to remove any topological
entities that extrude outward the datum plane without considering the shape of the
feature. The construction process of the inverse operation is presented in Algorithm 1.
110
Y. Cheng et al.

Algorithm 1 The inverse operation for convex feature creating operations
Function Inverse(O)
1: Obtain the topological entities that belong to the feature created by O. Typically, these 
topological entities are protrusive above the original part model. This is accomplished by 
parsing the semantic descriptor OWL file FDO with OWL API parser so as to obtain the 
attribute TopologicalFaceSet;
2: For every topological face f from the TopologicalFaceSet, execute the delete(f) so as to 
remove the revealed faces from the part model;
3: Detect the broken area on the datum plane, create a face in the same shape ,and merge 
the face with the plane.
4: return ;
Figure 1 gives us an example. Three feature instances are added to the same base
block feature by diﬀerent modeling methods. O1 adds the convex feature by sweeping
the face fp along a curve. O2 adds the convex feature by rotating. O3 adds the feature by
extruding. The features are in diﬀerent forms. It is worth noting that, in some situations,
when deleting some revealed faces from the boundary, there leaves a broken area on the
datum plane. In this case, the plane should be repaired. As it is illustrated in Fig. 2, when
the revealed faces from the round protrusion are deleted, there leaves a circular broken
area on the datum plane where the protrusion located.
Fig. 1. Inverse operations for diﬀerent convex
feature instances
Fig. 2. An example of the broken area
A Novel Inverse-Operation Based Group Undo/Redo Algorithm
111

3.2
Inverse Operation for Concave Feature Creating Operations
The inverse operation of a concave feature creating operation is to re-ﬁll in any hollow
space it creates on the part model. Inspired by the deﬁnition of the bounding box widely
used in 3D modeling, we intend to ﬁnd the Minimum Wrapping Solid, noted as MWS
for brevity, for a hollow space of any form. An MWS is actually a solid with the smallest
volume which contains all the points from the depression feature. Then, by uniting the
MWS with the part model, the hollow space is re-ﬁlled. The formal deﬁnition for the
Minimum Wrapping Solid is given in the following.
In Fig. 3, we give several examples of the MWSs for diﬀerent types of depression
features with diﬀerent forms. O1 creates a cubic slot. O2 creates a round hole. O3 creates
a sink hole. For the sake of creating an MWS, geometry-related information, such as the
radius and length of a simple hole, the width, length and height of a block, is more critical
than topology-related information.
Fig. 3. Inverse operations for diﬀerent concave
feature instances
Fig. 4. Inverse operations for diﬀerent edge
transition feature instances
112
Y. Cheng et al.

Algorithm 2 The inverse operation for concave feature creating operations
Function Inverse(O)
1: Parse the FDO and extract the information, namely, the category the feature instance 
belongs to, the dimension parameters and the reference information.
2: The MWS for the concave feature is constructed. It is then transformed to the proper 
position by using the same reference information extracted in step 1;
3: Unite the MWS with the current part model so as to refill the hollow space that O
created;
4: Delete faces from the MWS that are extruding outward the part and repair the broken 
area;
5: return ;
3.3
Inverse Operation for Edge Transition Feature Creating Operations
We devise the inverse operations by considering how an edge transition is created. As
it is illustrated in Fig. 4, an edge transition is created in two diﬀerent situations: (1) On
the left side of the Fig., a concave edge is chosen for edge transition which appears to
diﬀerence a small volume from the part; (2) On the right side of the Fig., a convex edge
is chosen and the result appears to add a volume so the two orthogonal faces connect
smoothly. The procedure of creating inverse operations for diﬀerent edge transition
occasions is given in Algorithm 3.
Algorithm 3 The inverse operation for edge transition feature creating operations
Function Inverse(O)
1: Parse the FDO and extract the information, namely, the category the feature instance 
belongs to, the dimension parameters.
2: The MWS for the transition edge is constructed. It is then transformed to the proper 
position;
3: If O selects a convex edge for edge transition, the MWS is united with the part. if O 
selects a concave edge for edge transition, the MWS is subtracted from the part;
4: return ;
A Novel Inverse-Operation Based Group Undo/Redo Algorithm
113

4
Experiments
In this example, three geographically dispersed sites build and modify models with both
do and undo operations. How the boundary of the part evolves with the continuous
execution of both do and undo at each site is given in Fig. 5 and the expected eﬀect of
every operation is described in details in Table 1. Whenever an operation is performed,
the related hybrid feature descriptor is generated. Observed from Fig. 5, the collaboration
activity involves three stages. The operations issued in the ﬁrst stage are pure modeling
operations. The second stage contains concurrent do and undo. The third stage is a typical
scenario of concurrent undo. The emphasis will be laid on the group undo process.
Fig. 5. A typical scenario of the collaborative modeling process
When undo is issued by site1, its intention is to delete the round hole feature created
by O5 from site1. It is carried out immediately at site1. The undo command is sent to
other sites in the form of UNDO(⟨1, 1⟩, ⟨1, 3⟩). Being an instance of a concave feature,
the MWS for the round hole is created given the length of the hole and the radius of the
circular end-face. The MWS is then transformed given the locating information
containing in the reference information descriptor of FDO5. By uniting it with the part,
O5 is successfully undone. When undo1 is received by a remote site, O5 is then identiﬁed
with the assistance of the tuple ⟨1, 3⟩. The undo process is carried in the same process
with how the undo is performed at the local site site1.
Undo2 and undo3 are concurrently generated at site0 and site2. However, they are
not in a conﬂictive relation. Undo2’s intention is to delete the round protrusion created
by O3 from site1. It is carried out immediately at site0. The undo command is sent to
other sites in the form of UNDO(⟨0, 1⟩, ⟨1, 1⟩). The revealed faces from the round
protrusion are deleted. Then, the broken area left on the base feature is repaired. The
topology information are required are from FDO3. When undo2 is received by a remote
site, O3 is then picked out with the assistance of the tuple ⟨1,1⟩.
114
Y. Cheng et al.

Table 1. Operation descriptions
Feature modeling
operation
Operation eﬀect
O1
Create the base cylinder feature
O2
Create another cylinder with the same radius on top of the base feature
O3
Create a round protrusion across the base cylinder
O4
Create a round hole within the base cylinder
O5
Create a round hole within the round protrusion built by O3
O6
Create a round cylinder crossing the hole generated by O3. The end of
the cylinder is decorated by spiral line.
O7
A mirror operation is invoked to generate another three holes around
the round protrusion generated by O3. Since now the four holes are
symmetric with the center of the round protrusion. Then we can say O7
depends on O4.
The intention of undo3 is to delete the small round hole within the base cylinder
created by O4 from site1. It is carried out immediately at site2 and sent to other sites in
the form of UNDO(⟨2, 1⟩,⟨2, 1⟩). The MWS for this round hole is created given its
length and the radius. It is then transformed given the same reference information
acquired from FDO4. By uniting it with the part, O4 is successfully undone. O7 is undone
as well because of its dependency relation with O4.
5
Conclusions
In this paper, we propose a novel inverse-operation based group Undo/Redo algorithm
for feature-based 3D collaborative CAD systems. This work is the ﬁrst to apply the
inverse model to the domain of 3D environment. We built an ontology in the CAD
domain and developed classes with sub-classes to describe general entities in the feature-
based 3D CAD environment. Using these concepts and their relations, we generate a
hybrid descriptor to record the information of a feature. For a convex feature, we delete
the topological entities it creates and repair the broken area after its deletion. For a
concave feature of any shape, we re-ﬁll the hollow space by creating the minimum
wrapping solid and uniting it with the part. This group Undo/Redo solution gives an
opportunity to modify a part locally. Our future work is to deal with the attribute
dependency and attribute conﬂicts among features.
Acknowledgement. The work is supported by the National Natural Science Foundation of China
(NSFC Grant Nos. 61472289 and 61502353) and Youth Innovation Corps Fund of Humanities
and Social Sciences, Wuhan University. The authors would like to thank all the reviewers for their
constructive comments and suggestions.
A Novel Inverse-Operation Based Group Undo/Redo Algorithm
115

References
1. He, F., Han, S.: A method and tool for human-human interaction and instant collaboration in
CSCW-based CAD. Comput. Ind. 57(8), 740–751 (2006)
2. Jing, S., He, F., Han, S., et al.: A method for topological entity correspondence in a replicated
collaborative CAD system. Comput. Ind. 60(7), 467–475 (2009)
3. Li, X., He, F., Cai, X., et al.: CAD data exchange based on the recovery of feature modelling
procedure. Int. J. Comput. Integr. Manuf. 25(10), 874–887 (2012)
4. Li, X., He, F., Cai, X., et al.: A method for topological entity matching in the integration of
heterogeneous CAD systems. Integr. Comput. Aided Eng. 20(1), 15–30 (2013)
5. Zhang, D., He, F., Han, S., et al.: Quantitative optimization of interoperability during feature-
based data exchange. Integr. Comput. Aided Eng. 23(1), 31–50 (2016)
6. Wu, Y., He, F., Zhang, D., et al.: Service-oriented feature-based data exchange for cloud-
based design and manufacturing. IEEE Trans. Serv. Comput. PP(99), 1 (2015)
7. Wu, Y., He, F., Han, S.: Collaborative CAD synchronization based on a symmetric and
consistent modeling procedure. Symmetry 9(4), 59 (2017)
8. Cheng, Y., He, F., Cai, X., et al.: A group Undo/Redo method in 3D collaborative modeling
systems with performance evaluation. J. Netw. Comput. Appl. 36(6), 1512–1522 (2013)
9. Cheng, Y., He, F., Wu, Y., et al.: Meta-operation conﬂict resolution for human-human
interaction in collaborative feature-based CAD systems. Cluster Comput. 19(1), 237–253
(2016)
10. Ressel, M., Nitsche-Ruhland, D., Gunzenhauser, R.: An integrating, transformation-oriented
approach to concurrency control and undo in group editors. In: Proceeding of Conference on
Computer Supported Cooperative Work (CSCW), pp. 288–297 (1996)
11. Archer Jr., J.E., Conway, R., Schneider, F.B.: User recovery and reversal in interactive
systems. ACM Trans. Program. Lang. Syst. 6(1), 1–19 (1984)
12. Kawasaki, Y., Igarashi, T.: Regional undo for spreadsheets (Demo). In: 2004 Adjunct
Proceedings UIST, 2 pages (2004)
13. Berlage, T., Genau, A.A.: Framework for shared applications with a replicated architecture.
In: 1993 ACM UIST, Atlanta, GA, pp. 249–257 (1993)
14. Prakash, A., Knister, M.J.: A framework for undoing actions in collaborative systems. ACM
Trans. Comp.-Hum. Inter. 1(4), 295–330 (1994)
15. Ressel, M., Gunzenhauser, R.: Reducing the problems of group undo. In: ACM Conference
on Supporting Group Work (1999)
16. Ressel, M., Nitsche-Ruhland, D., Gunzenhauser, R.: An integrating, transformation oriented
approach to concurrency control and undo in group editors. In: Proceeding of ACM
conference on computer supported co-operative work, November 1996, pp. 288–297 (1996)
17. Sun, C.Z.: Undo as concurrent inverse in group editors. ACM Trans. Comput. Hum. Interact.
9(4), 309–361 (2002)
18. Sun, D., Sun, C.: Context-based operational transformation in distributed collaborative
editing systems. IEEE Trans. Parallel Distrib. Syst. 20(10), 1454–1470 (2009)
19. Yoon, Y., Koo, S., Myers, B.A.: Visualization of ﬁne-grained code change history. In: IEEE
VL/HCC 2013, pp. 119–126 (2013)
20. Yoon, Y., Myers, B.A.: Supporting selective undo in a code editor. In: ICSE 2015, Florence,
Italy (2015)
21. Weiss, S., Urso, P., Molli, P., et al.: Logoot-undo: distributed collaborative editing system
on P2P networks. IEEE Trans. Parallel Distrib. Syst. 21(8), 1162–1174 (2010)
22. Myers, B.A.: Scripting graphical applications by demonstration. In: SIGCHI 1998, pp. 534–
541 (1998)
116
Y. Cheng et al.

23. Wang, X.Y., Bu, J.J., Chen, C.: Achieving undo in bitmap-based collaborative graphics
editing systems. In: The 2002 ACM Conference on Computer Supported Cooperative Work
(2002)
24. Myers, BA., Lai, A., Le, T.M., et al.: Selective undo support for painting applications. In:
Human Factors in Computing Systems, pp. 4227–4236 (2015)
25. www.cs.cmu.edu/NatProg/aquamarine.html
A Novel Inverse-Operation Based Group Undo/Redo Algorithm
117

Two Steps Method Using Polarization to Resist
Phase Noise for Self-interference Cancelation
in Full-Duplex
Fangfang Liu1(B), Xinyi Wang1, Chunyan Feng1, and Xiao Han2
1 Beijing Laboratory of Advanced Information Networks,
Beijing University of Posts and Telecommunications, Beijing 100876, China
fliu@bupt.edu.cn
2 Network Technology Research Institution, China Unicom, Beijing, China
Abstract. Phase noise caused by the local oscillator aﬀects both the
cancelation of self-interference and the reception of the desired signal. In
this paper, a novel two steps method exploiting polarization is proposed
to resist the eﬀect of phase noise on the self-interference cancelation in
full-duplex. This method can convert the multiplicative noise to additive
noise with polarization signal processing in two steps, which can cancel
the eﬀect of phase noise on both the self-interference and desired signal.
It should be noted that this method needs no prior information of the
phase noise. Theory analysis and numerical simulations show that the
eﬀect of phase noise is reduced and the amount of cancelation is also
evaluated in condition of diﬀerent phase noise values.
Keywords: Full duplex · Self-interference cancelation · Phase noise
Polarization
1
Introduction
Full-duplex is a modality of communication that allows a node to transmit and
receive signal simultaneously at the same frequency band. It has higher spec-
tral eﬃciency, higher throughput, and lower transmission delay than the tradi-
tional half duplex system, which satisﬁes the requirements of 5G very well [1–5].
However, massive self-interference (SI) caused by the local transmitter coupling
into the local receiver is an inevitable problem to resolve. Self-interference trans-
mitted through a short path is 15–100 dB stronger than the desired signal [6].
Therefore the cancelation of self-interference is one of the key factors inﬂuencing
the full-duplex communication. Moreover, it has been demonstrated that the
phase noise is one of the bottleneck of the self-interference cancelation [7].
Currently, relevant scholars put forward many self-interference cancelation
methods at the receiver, which can be classiﬁed into three categories, antenna
This work is supported by Chinese National Nature Science Foundations (61501050)
and (61271177).
c
⃝Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 118–129, 2018.
https://doi.org/10.1007/978-3-319-74521-3_14

Two Steps Method Using Polarization to Resist Phase Noise
119
cancelation [2,5,9], radio frequency cancelation (RFC) [1,3,6,8], and digital can-
celation [10–13]. Antenna cancelation and radio frequency cancelation is on the
premise of ignoring the phase noise, then the self-interference can be suppressed
to 10–15 dB stronger than the desired signal. Digital cancelation utilizes one pilot
siganl to track the phase noise and uses compensation structure to improve the
amount of self-interference cancelation. The eﬀect of phase noise caused by sep-
arate local oscillator or the same local oscillator on self-interference cancelation
is analyzed [10]. Inter carrier interference (ICI) and common phase error (CPE)
caused by the phase noise is handled through estimating and compensating the
phase noise, the amount of cancelation can be improved 7–10 dB in [11,12]. The
closed-form solution between the amount of cancelation and 3 dB bandwidth of
phase noise is derived [13]. However, the existing methods dealing with phase
noise only focus on self-interference signal but ignore the eﬀect of phase noise
on the desired signal. In this paper, a novel polarization self-interference can-
celation method is proposed to cancel the eﬀect of phase noise both on the
self-interference signal and the desired signal without any prior information of
the phase noise.
The polarization state (PS) of the signal is a substantive character which can
be used to carry information such as time, frequency, space, and code character
[14]. Polarization modulation technology have been proposed, and the oblique
projection ﬁltering is used to distinguish the desired signal from the noise [15].
The PS of the signal is decided by the amplitude ratio and phase diﬀerence.
Taking advantage of the PS deﬁnition, the basic idea of this paper is that, phase
noise does not aﬀect the phase diﬀerence between the two branches but only
aﬀect the absolute phase, thus the phase noise has no eﬀect on the PS of the
signal. We have proved the theoretical feasibility in the previous work [16], and
we will further consider the eﬀect of phase noise on both self-interference and
desired signal in this paper.
In this paper, the PS of the signal can be used to cancel the eﬀect of the
phase noise on the self-interference and desired signals, since it is not aﬀected
by the phase noise. The algorithm can be divided into two steps by converting
the multiplicative noise to additive noise with polarization signal processing.
The ﬁrst step is converting the eﬀect of phase noise on the self-interference
signal to the desired signal and white noise, and using the reconstructed signal
feedback from transmitter to cancel the self-interference signal. The second step
is converting the eﬀect of phase noise on the desired signal to white noise, then
recovering the desired signal. Both of the two steps does not need the prior
information of the phase noise. This method uses the polarization matching to
receive the desired signal and improve the amount of self-interference cancelation.
Theory analysis and numerical simulation show the eﬀect of phase noise on self-
interference cancelation. The amount of cancelation can be improved by 0–10 dB
in condition of the diﬀerent phase noise values. This paper mainly analyzes the
eﬀect of phase noise on the self-interference cancelation, other parameters like
the nonlinearity of power ampliﬁer of transmitter and receiver, I/Q (in phase and

120
F. Liu et al.
quadrature components) imbalance, and the quantization noise of ADC (analog
to digital converter) are ignored.
The rest of the paper is organized as follows. The full-duplex communication
system model and signal model of polarization is presented in Sect. 2. In Sect. 3,
the self-interference cancellation of two steps against phase noise based on polar-
ization signal processing is proposed. Simulation results are presented in Sect. 4
to demonstrate the eﬀectiveness of the proposed scheme. Finally, conclusions are
captured in Sect. 5.
2
System and Signal Model
2.1
System Model
Full-duplex communication link is established between two nodes as shown in
Fig. 1. A signiﬁcant diﬀerence between the polarization full-duplex model and the
traditional full-duplex communication model is introducing a Polarization Con-
trol Module (PCM) at the transmitter. After Code Modulation Module (CMM),
the signal enters into the PCM. PCM is composed of Power Division Unit (PDU)
and Phase Shifting Unit (PSU), amplitude ratio can be controlled by PDU and
phase diﬀerence can be controlled by PSU. The signal can obtain a speciﬁc PS
after PCM, then enters into the D/A conversion module. The up-converted signal,
which is aﬀected by phase noise, enters into the orthogonal dual polarized antenna.
The orthogonal polarization antenna is adopted at receiver, thus the mixed sig-
nal is aﬀected by phase noise when down-converted at the receiver. The eﬀect of
phase noise will be analyzed in following signal model. After the self-interference
cancelation, the mixed signal enters into the Demodulation Module (DM).
Fig. 1. Full-duplex communication system model of polarization
2.2
Signal Model
Jones vector is used to express polarization state of the signal, the polarization
state of the desired signal and self-interference are Ps ∈C2×1 and PI ∈C2×1
respectively, and the desired signal St and self-interference It are
St = Pss(t) = [cos(εs), sin(εs)ejδs ]T s(t),
(1)

Two Steps Method Using Polarization to Resist Phase Noise
121
It = PIi(t) = [cos(εi) sin(εi)ejδi ]T i(t),
(2)
where εs and εi express the polarization angles, δs and δi express the phase angles
of the desired and self-interference signals respectively. s(t) and i(t) express the
desired signal and self-interference in time domains.
s(t) = a(t)ejwctejφst(t),
(3)
i(t) = b(t)ejwctejφit(t),
(4)
where a(t) and b(t) represent the amplitude of the desired and self-interference
signals respectively, and wc is the carrier frequency. φst(t) and φit(t) represent
the phase noise respectively, then the desired and self-interference signals in
polarization domain are
St =
 a(t) cos(εs)ej(wct+φst(t))
a(t) sin(εs)ej(wct+φst(t)+δs)

=

Ehsejwct
Evsej(wct+δs)

ejφst(t),
(5)
It =
 b(t) cos(εi)ej(wct+φit(t))
b(t) sin(εi)ej(wct+φit(t)+δi)

=

Ehiejwct
Eviej(wct+δi)

ejφit(t),
(6)
where Ehs = a(t) cos(εs) and Evs = a(t) sin(εs) represent the H and V branch of
the desired signal, Ehi = b(t) cos(εi) and Evi = b(t) sin(εi) represent the H and
V branch of self-interference, suppose the channel satisﬁes Gauss distribution
and the noise is additive white Gaussian noise N(t), and the mixed signal at the
receiver.
y(t) = Sr(t) + Ir(t) + N(t),
(7)
Sr(t) and Ir(t) represents the desired and the interference signals at the
receiver. After been down-converted, the equivalent signal at baseband can be
written as
yL(t) = Sr(t)ejφr(t) + Ir(t)ejφr(t) + NL(t),
(8)
where φr(t) represents the phase noise caused by local oscillator at the receiver,
NL(t) expresses the white noise at baseband. Then Eq. (8) can be written as
yL(t) =
 Ehi
Eviejδi

ej(φit(t)+φr(t))
+
 Ehs
Evsejδs

ej(φst(t)+φr(t)) +

NhL(t)
NvL(t)

,
(9)
where the NhL(t) and NvL(t) express the H and V branch of NL(t), and the two
branches satisﬁes Gaussian distribution of random variable, NhL(t) and NvL(t)
are independent and identical with mean 0 and variance is σ2
2.
3
Two Steps Method of Self-interference Cancelation
The signal enters into the orthogonal dual-polarization antenna through the
AWGN channel. Then the signal passed from the mixer down-converts to the

122
F. Liu et al.
base band, which is aﬀected by phase noise. The traditional algorithm is esti-
mation and compensation [11–13], thus the estimation error of phase noise will
aﬀect the cancelation of self-interference. The proposed elimination algorithm
in domain of polarization is shown in Fig. 2. This method can be achieved by
converting the multiplicative noise to additive noise with polarization signal pro-
cessing in two steps. The ﬁrst step is converting the eﬀect of phase noise on the
self-interference to the desired signal and the white noise using stokes vector pro-
cessing in polarization domain, then using the reconstructed signal to cancel the
self-interference signal. The second step is recovering the desired signal using the
same principle. Then the mixed signal enters into the Match Receiving module
(MR), at last it enters into the Demodulation Module (DM).
Fig. 2. The self-interference elimination algorithm
3.1
Eliminating the Eﬀect of Phase Noise on Self-interference
After the mixed signal being down-converted, the mixed signal is expressed as
Eq. (9), in which the

NhL(t) NvL(t)
T can be written as Eq. (10).

NhL(t)
NvL(t)

=

nhc + jnhs
nvc + jnvs

(10)
The nhc and nhs express the in-phase and quadrature components of NhL(t),
the nvc and nvs has the similar expression. In order to analyze conveniently, the
signal transformed through stokes, as expression (11).
S1 = |yLH(t)|2 −|yLV(t)|2
S2 = 2 |yLH(t)| |yLV(t)| cos(ϕyLH(t) −ϕyLV(t))
S3 = 2 |yLH(t)| |yLV(t)| sin(ϕyLH(t) −ϕyLV (t)),
(11)
where S1, S2, S3 express the three components of stokes. yLH(t) and yLV (t)
express the H and V branch of yL(t). Through (9) and (11), yL(t) expressed as

Two Steps Method Using Polarization to Resist Phase Noise
123
S1 = (E2
hi −E2
vi) + (E2
hs −E2
vs) + (n2
hc + n2
hs) −(n2
vc + n2
vs)
+ 2(EhiEhs cos(φst(t) −φit(t)) −EviEvs cos(φst(t) −φit(t) + (δs −δi)))
+2

Ehs cos(φst(t) −φit(t)) + Ehi
Ehs sin(φst(t) −φit(t))
T
·
 cos(φit(t) + φr(t)) sin(φit(t) + φr(t))
−sin(φit(t) + φr(t)) cos(φit(t) + φr(t))
 nhc
nhs

−2

Evs cos(δs + φst(t) −φit(t)) + Evi cos(δi)
Evs sin(δs + φst(t) −φit(t)) + Evi sin(δi)
T
·
 cos(φit(t) + φr(t)) sin(φit(t) + φr(t))
−sin(φit(t) + φr(t)) cos(φit(t) + φr(t))
  nvc
nvs

,
(12)
S2 = 2(EhsEvs cos(δs) + EhsEvi cos(φst(t) −φit(t) −δi)
+EhiEvs cos(φst(t) −φit(t) + δs) + EhiEvi cos(δi)) + nhcnvc + nhsnvs
+2
Evs cos(φst(t) −φit(t) + δs) + Evi cos(δi)
Evs sin(φst(t) −φit(t) + δs) + Evi sin(δi)
T
·

cos(φit(t) + φr(t)) sin(φit(t) + φr(t))
−sin(φit(t) + φr(t)) cos(φit(t) + φr(t))
 
nhc
nhs

+2

Ehs cos(φst(t) −φit(t)) + Ehi
Ehs sin(φst(t) −φit(t))
T
·
 cos(φit(t) + φr(t)) sin(φit(t) + φr(t))
−sin(φit(t) + φr(t)) cos(φit(t) + φr(t))
 nvc
nvs

,
(13)
S3 = 2(EhsEvs sin(−δs) + EhsEvi sin(φst(t) −φit(t) −δi)
−EhiEvs sin(φst(t) −φit(t) + δs) −EhiEvi sin(δi)) + nhsnvc −nhcnvs
+2
−Evs sin(φst(t) −φit(t) + δs) −Evi sin(δi)
Evs cos(φst(t) −φit(t) + δs) + Evi cos(δi)
T
·
 cos(φit(t) + φr(t)) sin(φit(t) + φr(t))
−sin(φit(t) + φr(t)) cos(φit(t) + φr(t))
 nhc
nhs

+2

Ehs sin(φst(t) −φit(t))
−Ehs cos(φst(t) −φit(t)) + Ehi
T
·
 cos(φit(t) + φr(t)) sin(φit(t) + φr(t))
−sin(φit(t) + φr(t)) cos(φit(t) + φr(t))
 nvc
nvs

,
(14)
while
nhc1
nhs1

=
 cos(φit(t) + φr(t)) sin(φit(t) + φr(t))
−sin(φit(t) + φr(t)) cos(φit(t) + φr(t))

·
nhc
nhs

,
(15)

nvc1
nvs1

=

cos(φit(t) + φr(t)) sin(φit(t) + φr(t))
−sin(φit(t) + φr(t)) cos(φit(t) + φr(t))

·

nvc
nvs

,
(16)
nhc1 nhs1
T and
nvc1 nvs1
T express the H and V branch of white noise
after the ﬁrst rotation. According to the rotation characteristics unitary matrix

124
F. Liu et al.
[14],
nhc1 nhs1
T and
nvc1 nvs1
T have the same distribution with
nhc nhs
T
and
nvc nvs
T , then the yL(t) can be calculated as
yL(t) =

Ehi
Eviejδi

+

Ehs
Evsejδs

ej(φst(t)−φit(t)) +
NhL1(t)
NvL1(t)

,
(17)
the NhL1(t) = nhc1 + jnhs1, NvL1(t) = nvc1 + jnvs1. Comparing Eqs. (9) and
(17), the phase noise of self-interference is converted to the desired signal and
white noise, and the distribution of white noise has not changed. In the ﬁrst
rotation, it is not simply multiply the e−j(φit(t)+φr(t)) both sides of the Eq. (9).
because the φit(t), φst(t), φr(t) is unknown. It just uses the character of phase
noise in frequency domain. Therefore it is unnecessary to estimate phase noise
in this paper, which is the strong aspect compared with the method in time and
frequency domain. Using the canceling signal ycl(t) from transmitter through
wired feedback, the residual signal is Eq. (18).
yLR(t) = yL(t) −ycl(t)
=

Ehi
Eviejδi

+

Ehs
Evsejδs

ej(φst(t)−φit(t)) +
NhL1(t)
NvL1(t)

−

Ehi
Eviejδi

=

Ehs
Evsejδs

ej(φst(t)−φit(t)) +

NhL1(t)
NvL1(t)

,
(18)
where the yLR(t) represents the residual signal cancelled by the ﬁrst step. From
Eq. (18), the self-interference is cancelled. However, the phase noise is converted
to the desired signal, which will be solved in B.
3.2
Eliminating the Eﬀect of Phase Noise on Desired Signal
Suppose:

nhc2
nhs2

=

cos(φst(t) −φit(t)) sin(φst(t) −φit(t))
−sin(φst(t) −φit(t)) cos(φst(t) −φit(t))

·

nhc1
nhs1

,
(19)
nvc2
nvs2

=
 cos(φst(t) −φit(t)) sin(φst(t) −φit(t))
−sin(φst(t) −φit(t)) cos(φst(t) −φit(t))

·
nvc1
nvs1

,
(20)
nhc2 nhs2
T and
nvc2 nvs2
T express the H and V branch of white noise
after the second rotation. Using the same method as the ﬁrst step, the Eq. (18)
can be written as Eq. (21).
yLR(t) =

Ehs
Evsejδs

+
NhL2(t)
NvL2(t)

,
(21)
In Eq. (21), the NhL2(t) = nhc2 + jnhs2, NvL2(t) = nvc2 + jnvs2. From the
Eq. (21), the eﬀect of phase noise on the desired signal is converted to white noise,

Two Steps Method Using Polarization to Resist Phase Noise
125
and the distribution of white noise has not changed. Then using the minimum
variance criterion, we can get the polarization state of the desired signal. Then
the polarization of the desired signal can be written as Eq. (22).
∧
Ps = [cos(εs) sin(εs)ejδs ]T ,
(22)
∧
Ps is the polarization of the desired signal estimated at the receiver. After the
(MR), the mixed signal changed as Eq. (23).
yLR1(t) =
∧
Ps1 ·yLR(t)
=

cos(εs)
sin(εs)e−jδs
T
·

Ehs
Evsejδs

+
NhL2(t)
NvL2(t)

= [cos(εs) sin(εs)e−jδs ]

Ehs
Evsejδs

+ [cos(εs) sin(εs)e−jδs ]

NhL2(t)
NvL2(t)

= s(t) + no (t) ,
(23)
In Eq. (23), the yLR1(t) is the mixed signal after MR,
∧
Ps1 is the Hermitian
Transpose of
∧
Ps, no (t) is the white noise after MR.
no (t) = [cos(εs) sin(εs)e−jδs ]
NhL2(t)
NvL2(t)

(24)
After the MR, the desired signal s(t) has been recovered. From the Eq. (23), the
phase noise caused by the local oscillator of transmitter and receiver is canceled.
The white noise is full polarization after the MR, thus the power of white noise
keeps only in half. In the meanwhile, the amount of self-interference cancelation
is calculated as Eq. (25).
ηSINR (dB) = 10log
SINRout
SINRin

,
(25)
ηSINR (dB) expresses the amount of self-interference cancelation. SINRin and
SINRout express the ratio between the desired signal and self-interference add
white noise before and after signal processing at the receiver. Then the Eq. (25)
can be written as Eq. (26).
ηSINR (dB) = 10log

2(1 + b2(t)
σ2 )

.
(26)
Comparing the Eq. (26) and the amount of self-interference cancelation in
[7,9,10,12], the phase noise caused by the local oscillator of transmitter and the
receiver is canceled, and the amount of self-interference cancelation is improved.
4
Simulation Scenarios, Results and Analysis
Co-simulation between MATLAB and Advanced Design System (ADS) is used
to verify the performance of self-interference cancelation method proposed in this

126
F. Liu et al.
paper. This part shows the eﬀect of phase noise on the signal and the process of
how to cancel the eﬀect of the phase noise intuitionally.
In this section, the ratio between H and V branch is set as 1, the phase
diﬀerence is set as 45 in degree. The variance of phase noise is set as 0.018 in
radian. The eﬀect of phase noise on the self-interference is shown in Figs. 3 and 4.
The spectrum of the self-interference without phase noise is shown in Fig. 3,
then Fig. 4 shows the spectrum of self-interference eﬀected by phase noise. By
comparing Figs. 3 and 4, we can obtain that the spectrum of self-interference
has been spreaded due to the phase noise. The center power value of the self-
interference decreased. The total power of the signal unchanged at the same time.
The eﬀect of phase noise on the desired signal is the same as the self-interference.
The waveform of the mixed signal at the receiver in time domain is shown in
Fig. 5(a). The self-interference is 90 dB higher than the desired signal. Because of
Fig. 3. The spectrum of self-interference without phase noise
Fig. 4. The spectrum of self-interference eﬀected by phase noise

Two Steps Method Using Polarization to Resist Phase Noise
127
the eﬀect of the self-interference, phase noise and white noise, the desired signal
has been distorted seriously. After the ﬁrst step cancelation, the self-interference
is canceled, whose phase noise is converted to the desired signal and white noise
as shown in Fig. 5(b). After cancelation by the second step, the phase noise of
the desired signal is converted to the white noise as shown in Fig. 5(c). After the
two steps, the eﬀect of phase noise on self-interference cancelation is canceled,
and the desired signal is recovered.
Fig. 5. Waveform of the mixed signal before cancelation (a), after the ﬁrst cancelation
(b), after the second cancelation (c)
0
0.005
0.01
0.015
0.02
0
20
40
60
80
100
120
140
The standard deviation of Phase Noise in radian
The cancelation of self−interference in dB
Cancelation in polarization domain in theory
Cancelaiton in polarization domain in ADS
Cancelation in traditioanl domain in theory L=30
Cancelation in traditioanl domain in theory L=40
Cancelaiton withouting resisting phase noise in theory
Fig. 6. The cancelation aﬀected by phase noise (Color ﬁgure online)
Figure 6 shows the eﬀect of phase noise on the self-interference cancelation.
In Fig. 6, the abscissa expresses the standard deviation of phase noise, ordi-
nate expresses the amount of cancelation in dB. The signal interference noise
ratio (SINR) is set as 90 dB. The blue (rhombus) line expresses the relationship
between the amount of self-interference cancelation and the standard deviation
of phase noise without the method of suppression [7], the amount of cancelation

128
F. Liu et al.
decreased as the power of phase noise increased. The green (rectangle) and pink
(pentagram) lines express the amount of cancelation aﬀected by phase noise after
traditional self-interference elimination algorithm [11,12]. Although the amount
of cancelation increased under the condition without suppression, it decreased
when the phase noise increases. The red (triangle) line expresses the amount of
self-interference cancelation using the method proposed in this paper. The black
(roundness) line expresses the amount of self-interference cancelation simulated
in ADS. These two lines suggest that by utilizing the cancelation algorithm pro-
posed in this paper, the amount of cancelation remained unchanged as the phase
noise increased. We can conclude that the eﬀect of phase noise on the cancelation
of self-interference has been canceled by using the two steps method algorithm
proposed in this paper.
5
Conclusion
Based on the unitary matrix rotation characteristics in polarization domain, the
method in this paper has canceled the eﬀect of phase noise on the self-interference
cancelation and recovering of the desired signal. This method can be achieved
by converting the multiplicative noise to additive noise with polarization signal
processing in two steps. The ﬁrst step is converting the eﬀect of phase noise on
the self-interference to the desired signal and the white noise using stokes vector
processing in polarization domain, then using the reconstructed signal to cancel
the self-interference signal. The second step is recovering the desired signal using
the same principle. Theory analysis and numerical simulation show that eﬀect
of phase noise on self-interference cancelation is canceled, and the amount of
cancelation is improved on the premise of recovering the desired signal.
References
1. Zhang, Z., Long, K., Vasilakos, A.V., Hanzo, L.: Full-duplex wireless communica-
tions: challenges, solutions, and future research directions. Proc. IEEE 104, 1369–
1409 (2016)
2. Everett, E., Sahai, A., Sabharwal, A.: Passive self-interference suppression for full-
duplex infrastructure nodes. IEEE Trans. Wireless Commun. (2013, in press)
3. Debaillie, B., van den Broek, D.-J., Lavn, C., van Liempd, B., Klumperink, E.A.M.,
Palacios, C., Craninckx, J., Nauta, B.: Analog/RF solutions enabling compact full-
duplex radios. IEEE J. Sel. Areas Commun. 32(9), 1662–1673 (2014)
4. Liu, G., Yu, F.R., Ji, H., Leung, V.C.M., Li, X.: In-band full-duplex relaying for
5G cellular networks with wireless virtualization. IEEE Netw. 29, 54–61 (2015)
5. Foroozanfard, E., Franek, O., Tatomirescu, A., Tsakalaki, E., de Carvalho, E.,
Pedersen, G.F.: Full-duplex MIMO system based on antenna cancellation tech-
nique. Electron. Lett. 50(16), 1116–1117 (2014)
6. Sahai, A., Patel, G., Sabwarwal, A.: Pushing the limits of full duplex: design and
real-time implementation. Rice University, Technical report REE1104 (2011)
7. Sahai, A., Patel, G., Dick, C., Sabharwal, A.: On the impact of phase noise on
active cancelation in wireless full-duplex. IEEE Trans. Veh. Technol. 62(9), 4494–
4510 (2013)

Two Steps Method Using Polarization to Resist Phase Noise
129
8. Wang, J., Zhao, H., Tang, Y.: A RF adaptive least mean square algorithm for
self-interference cancellation in co-frequency co-time full duplex systems. In: IEEE
Symposium Transactions on Communications (2014)
9. Duarte, M., Dick, C., Sabharwal, A.: Experiment-driven characterization of full-
duplex wireless systems. IEEE Trans. Wireless Commun. 11(12), 4296–4307 (2012)
10. Syrjala, V., Valkama, M., Anttila, L., Riihonen, T., Korpi, D.: Analysis of oscilla-
tor phase-noise eﬀects on self-interference cancellation in full-duplex OFDM radio
transceivers. IEEE Trans. Wireless Commun. 13(6), 2977–2990 (2014)
11. Syrjala, V., Yamamoto, K.: Self-interference cancellation in full-duplex radio
transceivers with oscillator phase noise. In: European Wireless (2014)
12. Ahmed, E., Eltawil, A.M., Sabharwal, A.: Self-interference cancellation with phase
noise induced ICI suppression for full-duplex systems. In: Signal Processing for
Communications Symposium, GLOBECOM (2013)
13. Shao, S., Quan, X., Shen, Y., Tang, Y.: Eﬀect on phase noise of digital self-
interference cancelation in wireless full duplex. In: IEEE International Conference
on Acoustic, Speech and Signal Processing (ICASSP) (2014)
14. Zhuang, Z., Xiao, S., Wang, X.: Radar Polarization Information Processing and
Application. National Defense Industry Press, Beijing (1999)
15. Cao, B., Zhang, Q.Y., Jin, L.: Polarization division multiple access with polariza-
tion modulation for LOS wireless communications. EURASIP J. Wireless Com-
mun. Network. (2011)
16. Liu, F., Jia, S., Guo, C., Feng, C.: Exploiting polarization to resist phase noise for
digital self-interference cancellation in full-duplex. In: IEEE International Confer-
ence on Communications (ICC) (2016)
17. Benedetto, S., Poggiolin, P.: Theory of polarization shift keying modulation. IEEE
Trans. Commun. 40(4), 708–721 (1992)

Sparse Linear Method Based Top-N Course
Recommendation System with Expert Knowledge
and L0 Regularization
Jinjiao Lin1,2, Haitao Pu3, Yibin Li1, and Jian Lian3(✉)
1 School of Control and Engineering, Shandong University, Jinan, China
2 Shandong University of Finance and Economics, Jinan, China
3 Shandong University of Science and Technology, Qingdao, China
lianjianlianjian@163.com
Abstract. In this paper, we propose an approach of course recommender system
for the subject of information management speciality in China. We collect the
data relative to the course enrollment for speciﬁc set of students. The sparse linear
method (SLIM) is introduced in our approach to generate the top-N recommen‐
dations of courses for students. Furthermore, the L0 regularization terms were
presented in our proposed optimization method based on the observation of the
entries in recommendation system matrix. Expert knowledge based comparing
experiments between state-of-the-art methods and our method are conducted to
evaluate the performance of our method. Experimental results show that our
proposed method outperforms state-of-the-art methods both in accuracy and eﬃ‐
ciency.
Keywords: Course recommender system · Sparse linear method
Expert knowledge
1
Introduction
The emergence and rapid development of Internet have greatly aﬀected the traditional
viewpoint on choosing courses by providing detailed course information. As the number
of courses conforming to the students’ has tremendously increased, the above-mentioned
problem has become how to determine the courses mostly suitable for the students
accurately and eﬃciently. A plethora of methods and algorithms [2, 3, 11, 15] for course
recommendation have been proposed to deal with this problem. Most of the methods
designed for recommendation system can be grouped into three categories, including
collaborative [1, 8], content-based [7, 14], and knowledge-based [5, 8, 17], which have
been applied in diﬀerent ﬁelds such as [4] proposed a collaborative ﬁltering embedded
with an artiﬁcial immune system to the course recommendation for college students.
The rating from professor was exploited as ground truth to examine the results.
Inspired by the idea form [4] and the optimization framework in [9], we propose a
sparse linear based method for top-N course recommendation with expert knowledge
as the ground truth. This method extracts the coeﬃcient matrix for the courses in the
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 130–138, 2018.
https://doi.org/10.1007/978-3-319-74521-3_15

recommender system from the student/course matrix by solving a regularized optimi‐
zation problem. The sparseness is exploited to represent the sparse characteristics of
recommendation coeﬃcient matrix. Sparse linear method (SLIM) [9] was proposed to
top-N recommender systems, which is rarely exploited in course recommender systems.
Due to the characteristics of course recommendation system in Chinese University, our
method focuses on the accuracy more than the eﬃciency. It is diﬀerent form the previ‐
ously proposed SLIM based methods [6, 9, 10, 18], which mainly addresses the real-
time applications of top-N recommender systems. The framework of our proposed
course recommender system is shown in Fig. 1.
Fig. 1. The framework of our proposed course recommender system
According to our observation about common recommendation system matrix, most
of the entries are assigned the same value (zero or one), and the gradients of neighboring
entries also hold the same value (zero or one). Therefore, the sparse counting strategy
of L0 regularization terms [16] were included into the optimization framework of SLIM.
The L0 terms can globally constrain the non-zero values of entries and the gradients in
the recommendation system matrix, which is the main contribution of our proposed
method. Diﬀerent from the previously proposed regularization terms (the L1 and L2
terms), the L0 term can maintain the subtle relationship between the entries in recom‐
mendation system matrix.
After the process of data gathering as shown in Fig. 1, comparing experiments
between state-of-the-art methods and our method are conducted. Consequently, both the
Sparse Linear Method Based Top-N Course Recommendation System
131

experimental results of state-of-the-art methods and our method are evaluated with the
course recommendations presented by seven experts with voting strategy.
The rest of the paper is organized as follows. In Sect. 2, we describe the details of
our proposed method. In Sect. 3 the dataset that we used in our experiments and the
experimental results are presented. In Sect. 4 the discussion and conclusion are given.
2
Our Method
2.1
The Formation of the Method
In the following content, tj and si are introduced to denote each course and each student
in course recommender system, respectively. The whole student-course taken will be
represented by a matrix A of size m × n, in which the entry is 1 or 0 (1 denotes that the
student has taken the course, 0 vice versa).
In this paper, we introduce a Sparse Linear Method (SLIM) to implement top-N
course recommendation. In this approach, the score of course recommendation on each
un-taken student/course item tj of a student si is computed as a sparse aggregation of
items that have been taken by si, which is shown in Eq. (1).
aij = aT
i wj
(1)
where ̄a is the initial course selection of a speciﬁc student and wj is the sparse vector of
aggregation coeﬃcients. The model of SLIM with matrix is represented as:
̄A = AW
(2)
Where overlineA is the initial value of student/course matrix, A denotes the latent
binary student-course item matrix, W denotes the n × n sparse matrix of aggregation
coeﬃcients, in which j −th column corresponds to wj as in Eq. (1), and each row of
C(ci) is the course recommendation scores on all courses for student si. The ﬁnal course
recommendation result of each student is completed through sorting the non-taken
courses in decreasing order, and the top-N courses in the sequences are recommended.
In our method, the initial student/course matrix is extracted from the learning
management system of a speciﬁc University in China. With the extracted student/course
matrix of size m × n, the sparse matrix W size of n × n in Eq. (2) is iteratively optimized
by alternate minimization method. Diﬀerent from the objective function previously
proposed in [9] shown in Eq. (3), our proposed method is shown in Eq. (4).
min
W≥0
1
2
‖‖A −AW‖‖
2
2 + 𝛽1
2
||W‖‖
2
F + 𝜆1‖‖A‖‖1
(3)
min
W≥0
1
2
‖‖A −AW‖‖
2
2 + 𝛽2
2
||W‖‖
2
F + 𝜆2‖‖A‖‖0 + 𝜇|∇A|0
(4)
Where ‖.‖F denotes the Frobenius norm for matrix, ‖W‖1 is the item-wise L1 norm,
‖W‖0 denotes the entry-wise L0 norm that stands for the number of entries with zero
132
J. Lin et al.

value. The data term ‖A −AW‖ is exploited to measure the diﬀerence between the
calculated model and the training dataset. The LF −norm, L1 −norm, and L0 −norm
are exploited to regularize the entries of the coeﬃcient matrix W, A, and ∇A, respectively.
The parameters 𝛽1, 𝛽2, 𝜆2, and 𝜇 are used to constrain the weights of regularization terms
in the objective functions.
In our proposed ﬁnal objective function, the LF norm is introduced to transfer the
optimization problem into elastic net problem [19], which prevents the potential over
ﬁtting. Moreover, the L1 norm in Eq. (3) is changed to L0 norm in our proposed objective
function. This novel norm L0 [12, 13, 16] is introduced to constrain the sparseness of
the A and ∇A.
Due to the independency of the columns in matrix W, the ﬁnal objective function in
Eq. (4) is decoupled into a set of objective functions as follows:
min
wj≥0,wj,j=0
1
2
‖‖‖aj −ajwj‖‖‖
2
2 + 𝛽2
2
|||wj ‖‖
2
F + 𝜆2‖‖‖aj‖‖‖0 + 𝜇|||∇aj|||0
(5)
where aj is the j-th column of matrix A, wj denotes j-th column of matrix W. As there
are two unknown variables in each Eq. (5), which is a typical ill-posed problem. Thus,
this problem need to be solved by alternate minimization method. In each iteration, one
of the two variables is ﬁxed and the other variable is optimized.
2.2
The Solver of Our Proposed Method
Subproblem1: computing  wj
The wj computation sub-problem is represented by the minimization of Eq. (6):
1
2
‖‖‖aj −ajwj‖‖‖
2
2 + 𝛽2
2
‖‖‖wj‖‖‖
2
F
(6)
Through eliminating the L0 terms in Eq. (5), the function Eq. (6) has a global
minimum, which can be computed by gradient descent. The analytical solution to
Eq. (6) is shown in Eq. (7):
wj = F−1
⎛
⎜
⎜
⎜⎝
F(aj
)
F(aj
) + 𝛽2
2
(F(𝜕x
)∗⋅F(𝜕x
) + F(𝜕y
)∗⋅F(𝜕y
))
⎞
⎟
⎟
⎟⎠
(7)
where F(.) and F−1(⋅) denotes the Fast Fourier Transform (FFT) and reverse FFT,
respectively .F()∗ is the complex conjugate of F(⋅).
Sub-problem 2: computing  aj  and  ∇aj
With the intermediate outcome of wj, the aj and ∇aj can be computed by Eq. (8):
Sparse Linear Method Based Top-N Course Recommendation System
133

1
2
‖‖‖aj −ajwj‖‖‖
2
2 + 𝜆2‖‖‖aj‖‖‖0 + 𝜇‖‖‖∇aj‖‖‖0
(8)
By introducing two auxiliary variables h and v corresponding to the column vector
aj and ∇aj. The sub-problem can be transformed into Eq. (9):
1
2
‖‖‖aj −ajwj‖‖‖
2
2 + 𝜆2‖‖‖aj −h‖‖‖
2
2 + 𝜇‖‖‖∇aj −v‖‖‖
2
2 + 𝜆(
‖h‖0 + ‖v‖0
)
(9)
To testify the performance of our proposed method, comparing experiments between
state-of-the-art methods and our method are carried out with gathered dataset and expert
knowledge. In the following section, the experiments are described in detail.
3
Experimental Results
3.1
Datasets
In order to testify the performance of our proposed method and implement the method
in practical scenarios, we gather the data from ﬁve classes of information management
specialty for the learning management system of our University. The data records of the
Table 1. The initial dataset from the ﬁve classes
No.
SPSS
CH
Eng
LA
PT
DB
CC
PE
C
Acc
CS
1
1
1
1
0
1
0
1
1
0
1
0
2
0
1
1
0
1
0
1
1
1
1
0
3
0
1
1
0
1
1
0
1
0
1
0
4
0
1
1
0
1
1
0
1
1
1
0
5
1
1
1
1
1
0
0
1
1
1
0
6
1
1
1
1
1
0
0
1
1
1
1
7
0
1
1
1
1
0
0
1
1
1
1
8
1
1
1
1
1
1
0
1
0
1
1
9
1
1
1
0
1
1
1
1
0
1
0
10
1
1
1
0
1
1
1
1
0
1
0
11
0
1
1
0
1
0
1
1
0
1
0
12
0
1
1
0
1
1
0
1
0
1
1
13
0
1
1
1
1
0
1
1
0
1
1
14
1
1
1
1
1
1
1
1
0
1
1
15
1
1
1
1
1
1
1
1
0
1
1
16
0
1
1
0
1
1
1
1
0
1
1
17
1
1
1
1
1
1
1
1
0
1
1
18
0
1
1
1
1
0
1
1
0
1
0
19
1
1
1
0
1
1
1
1
0
1
0
20
0
1
1
1
1
0
1
1
0
1
1
134
J. Lin et al.

courses and students were extracted from the Department of Management Information
System, Shandong University of Finance and Economics and the Department of Elec‐
tronic Engineering Information Technology at Shandong University of Sci&Tech. The
most important information of the courses and students is mainly about the grades
corresponding to the courses. All of the students from the information management
specialty are freshmen in our University. Most of them have taken the courses of the
ﬁrst year in their curriculum except three students have failed to go up to the next grade.
Thus, ﬁrstly we eliminate the records of the three students. Meanwhile, we collect the
knowledge including the programming skill that they have mastered through a ques‐
tionnaire. The courses that they have taken and the content that have grasped are
combined in the ﬁnal datset. A part of the dataset is shown in Table 1, where 1 denotes
that the si student has mastered the tj course, and 0 denotes the opposite.
After gathering the data of the students from the ﬁve classes, comparing experiments
between state-of-the-art methods and our method are conducted. We choose several
state-of-the-art methods including collaborative ﬁltering methods itermkNN, userkNN,
and the matrix factorization methods PureSVD.
3.2
Measurement
The knowledge from several experts on the courses in information management
specialty are adopted as ground truth in the experimental process. To measure the
performance of the comparing methods, we introduce the Hit Rate (HR) and the Average
Reciprocal Hit-Rank (ARHR) in the experiments, which are deﬁned as shown in
Eqs. (11) and (12).
HR =
#hits
#students
(11)
where #hits denotes the number of students whose course in the testing set is recom‐
mended by the expert, too. #students denotes the number of all students in the dataset.
ARHR =
1
#students
#hits
∑
i=1
1
pi
(12)
Where pi is the ordered recommendation list.
3.3
Experimental Results
In this section, the experimental results calculated from the practical dataset. Table 2
shows the experimental results of the comparing methods in top-N course recommen‐
dation.
Sparse Linear Method Based Top-N Course Recommendation System
135

Table 2. The performance of the comparing methods
Methods
HR1
ARHR1
HR2
ARHR2
HR3
ARHR3
HR4
ARHR4
HR5
ARHR5
itemkN
N
0.18
0.13
0.19
0.14
0.20
0.13
0.18
0.13
0.19
0.14
Itempro
b
0.21
0.15
0.19
0.16
0.21
0.14
0.19
0.12
0.17
0.13
PureSV
D
0.09
0.11
0.10
0.12
0.12
0.12
0.17
0.14
0.18
0.15
SLIM
0.24
0.16
0.17
0.18
0.24
0.19
0.16
0.14
0.17
0.15
ours
0.27
0.17
0.19
0.17
0.25
0.18
0.20
0.14
0.19
0.15
Where HRi, ARHRi denotes the performance for classi, respectively. The experi‐
mental results shown in Table 2 demonstrate that our proposed method outperforms
state-of-the-art methods in most of course recommendations both in the HR and ARHR.
It shows that the sparse regularization term based on the prior knowledge from the
observation in our method are suitable for solving the problem of course recommenda‐
tion.
In order to illustrate the performance of our proposed method according to the
number of courses and topics included in the experimental testing. It shows in Fig. 2
that a higher accuracy is obtained when the number of courses increases. Meanwhile,
the courses included in our experiments are divided into 32 diﬀerent topics, Fig. 3 shows
that the accuracy is also higher when there are more relative courses.
Fig. 2. Accuracy of our proposed recommendation system method due to the number of courses
136
J. Lin et al.

Fig. 3. Accuracy of our proposed recommendation system method due to the number of topics
4
Conclusion
In this paper, we propose an approach of course recommendation. In our method, the
SLIM was introduced and a novel L0 regularization term was exploited in SLIM. Mean‐
while, the alternate minimization strategy is exploited to optimize the outcome of our
method. To testify the performance of our method, comparing experiments on students
from ﬁve diﬀerent classes between state-of-the-art methods and our method are
conducted. The experimental results show that our method outperforms the other previ‐
ously proposed methods.
The proposed method was be mainly used to implement the course recommendation
for the Universities in China. However, it also can b exploited in other relative ﬁelds.
In the future, more applications of our approach would be investigated. Other future
work includes the modiﬁcation of the objective function in our method including the
other regularization terms and diﬀerent optimization strategy.
Acknowledgments. This work was ﬁnancially supported by the Teaching Reform Research
Project of Undergraduate Colleges and Universities of Shandong Province (2015M111,
2015M110, Z2016Z036) and the Teaching Reform Research Project of Shandong University of
Finance and Economics (2891470), Teaching Reform Research Project of Undergraduate
Colleges and Universities of Shandong Province (2015M136). SDUST Young Teachers Teaching
Talent Training Plan (BJRC20160509); SDUST Excellent Teaching Team Construction Plan;
Teaching research project of Shandong University of Science and Technology (JG201509 and
qx2013286); Shandong Province Science and Technology Major Project (No. 2015ZDX
X0801A02).
Sparse Linear Method Based Top-N Course Recommendation System
137

Disclosures. The authors declare no conﬂict of interest. The founding sponsors had no role in the
design of the study; in the collection, analyses, or interpretation of data; in the writing of the
manuscript, and in the decision to publish the results.
References
1. Ahn, H.: Utilizing popularity characteristics for product recommendation. Int. J. Electron.
Commer. 11(2), 59–80 (2006)
2. Albadarenah, A., Alsakran, J.: An automated recommender system for course selection. Int.
J. Adv. Comput. Sci. Appl. 7(3) (2016)
3. Aher, S.B., Lobo, L.M.R.J.: A comparative study of association rule algorithms for course
recommender system in e-learning. Int. J. Comput. Appl. 39(1), 48–52 (2012)
4. Chang, P., Lin, C., Chen, M.: A hybrid course recommendation system by integrating
collaborative ﬁltering and artiﬁcial immune systems. Algorithms 9(3), 47 (2016)
5. Chen, Y., Cheng, L., Chuang, C.: A group recommendation system with consideration of
interactions among group members. Expert Syst. Appl. 34(3), 2082–2090 (2008)
6. Christakopoulou, E., Karypis, G.: HOSLIM: higher-order sparse linear method for top-N
recommender systems. In: Tseng, V.S., Ho, T.B., Zhou, Z.-H., Chen, A.L.P., Kao, H.-Y.
(eds.) PAKDD 2014. LNCS (LNAI), vol. 8444, pp. 38–49. Springer, Cham (2014). https://
doi.org/10.1007/978-3-319-06605-9_4
7. Kim, E., Kim, M., Ryu, J.: Collaborative ﬁltering based on neural networks using similarity.
In: Wang, J., Liao, X.-F., Yi, Z. (eds.) ISNN 2005. LNCS, vol. 3498, pp. 355–360. Springer,
Heidelberg (2005). https://doi.org/10.1007/11427469_57
8. Mclaughlin, M., Herlocker, J.L.: A collaborative ﬁltering algorithm and evaluation metric
that accurately model the user experience, pp. 329–336 (2004)
9. Ning, X., Karypis, G.: SLIM: sparse linear methods for top-n recommender systems, pp. 497–
506 (2011)
10. Ning, X., Karypis, G.: Sparse linear methods with side information for top-n recommendations,
pp. 155–162 (2012)
11. Omahony, M.P., Smyth, B.: A recommender system for on-line course enrolment: an initial
study, pp. 133–136 (2007)
12. Pan, J., Hu, Z., Su, Z., Yang, M.: Deblurring text images via L0-regularized intensity and
gradient prior, pp. 2901–2908 (2014)
13. Pan, J., Lim, J., Su, Z., Yang, M.H.: L0-regularized object representation for visual tracking.
In: Proceedings of the British Machine Vision Conference. BMVA Press (2014)
14. Philip, S., Shola, P.B., John, A.O.: Application of content-based approach in research paper
recommendation system for a digital library. Int. J. Adv. Comput. Sci. Appl. 5(10) (2014)
15. Sunil, L., Saini, D.K.: Design of a recommender system for web based learning, pp. 363–368
(2013)
16. Xu, L., Lu, C., Xu, Y., Jia, J.: Image smoothing via L0 gradient minimization. In: International
Conference on Computer Graphics and Interactive Techniques, vol. 30, No. 6 (2011). 174
17. Yap, G., Tan, A., Pang, H.: Dynamically-optimized context in recommender systems,
pp. 265–272 (2005)
18. Zheng, Y., Mobasher, B., Burke, R.: CSLIM: contextual slim recommendation algorithms,
pp. 301–304 (2014)
19. Zou, H., Hastie, T.: Regularization and variable selection via the elastic net. J. Roy. Stat. Soc.
Ser. B-Stat. Methodol. 67(2), 301–332 (2005)
138
J. Lin et al.

The Research on the Container Truck Scheduling Based
on Fuzzy Control and Ant Colony Algorithm
Meng Yu, Dawei Li
(✉), and Qiang Wang
School of Logistics Engineering, Wuhan University of Technology, Wuhan, China
ymmona@126.com, 1054386503@qq.com
Abstract. This paper studies a Container Trucks (CTs) dynamic dispatching
strategy in which multiple tasks are matched with multiple CTs and proposes
Multi-Agent contract net protocols based on two-way negotiation mechanism. It
could assign optimal tasks to CTs through tendering, bidding and contracting of
tasks. Fuzzy set theory and method is adopted to couple multiple factors on the
tasks to assess to their dispatching emergency degree in the process. One of the
factors is the distance from the current position of CT to the loading/unloading
position. Through judging obstruction status by using related data of the GPRS
system, the steps of ant colony algorithm are designed to ﬁnd the predicted travel
distance of the optimal route. After fuzziﬁcation and defuzziﬁcation in MATLAB,
the decision query table of dispatching plan is obtained. Finally, a case study is
given to describe the scheduling scheme in detail.
Keywords: Container truck dynamic scheduling · Multi- agent system
Fuzzy control · Ant colony algorithm · Path optimization
1
Introduction
The loading/unloading equipment at most container terminals in China are mainly Quay
Cranes (QCs), Yard Cranes (YCs) and CTs. The scheduling system of CTs is well known
to possess complex logistics system characteristics for there are many factors eﬀecting
scheduling, such as time, space, resources, and uncertain factors. Optimize the sched‐
uling system is meaningful, which can improve the eﬃciency of loading and unloading.
The physical entity resource network and the control decision-making information
network were integrated into the modeling and optimization architecture using Harvard
architecture and Agent based computing [1, 2]. In [3], Relationship between transport
tasks and service of CTS has been taken as a contract net using the fuzzy set theory and
method. The dispatching model based on Contract Network Protocol (CNP) using bidir‐
ectional negotiation is provided and fuzzy reasoning process of dispatching decisions is
suggested. But the distance from the current position of CT to the loading/unloading
position is calculated by the established coordinate system. In the case of road conges‐
tion, CT should choose a smooth route, so the distance should be the length of the route.
Recently, the path planning problem is a hot research area [4, 5], the main algorithms
used are genetic algorithm, ant colony algorithm and so on.
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 139–149, 2018.
https://doi.org/10.1007/978-3-319-74521-3_16

This paper proposes Multi-Agent contract net protocols based on two-way negotia‐
tion mechanism. The fuzzy control system is designed to couple multiple factors on the
tasks to assess to their dispatching emergency degree. The steps of ant colony algorithm
considering obstruction status were designed to ﬁnd the optimal route. After fuzziﬁca‐
tion and defuzziﬁcation, the decision query table of dispatching plan is obtained.
2
MAS Framework of Container Terminal Scheduling
Container terminal consists of containers, ships, handling equipment (QC, YC, and CT)
communication equipment, berths, container yards and human resource, etc. Superior
and subordinate subsystems have command and obedience relationships, and parallel
subsystems have collaboration, consultation, competitive relationships. Figure 1 shows
the hierarchical structure of the whole system.
BA-Berth Assigning; QCS-Quay Cranes Scheduling; CTS-Container Trucks 
Scheduling; YCS-Yard Cranes Scheduling; YA- Yard Allocating 
Fig. 1. Framework of container terminal schedule system based on Multi-Agent
As we can see, there are three types of individual agents, namely Control Agent
(ﬁxed), Execution Agent (moving with wireless mobile terminal) and Operation
(Mobile) Agent (dynamically generated).
140
M. Yu et al.

Operation Agents are created dynamically and freely by the Central Processing
Agent throughout the whole system just like computer software, and they are assigned
by the Control Agent, executed by the Execution Agent.
3
Dynamic Dispatching Strategy
In the traditional static scheduling mode, the truck is unloaded at half of the distance. A
dynamic dispatching strategy in which multiple tasks are matched with multiple CTs is
adapted in this study, which can guarantee the truck in the idle state can be quickly put
into the operation point which needs trucks. So how to arrange a truck for the task should
be considered. In this paper, considering the load of communication and consultation
eﬃciency in system, Multi-Agent contract net protocols based on two-way negotiation
mechanism was adopted to achieve the best arrangement. Figure 2 shows the contract
net.
Fig. 2. Contract net
In the contract negotiation protocol Agent network, diﬀerent types of equipment in
collaboration between the task request and response with announcing, bidding, and
awarding in contract net. In evaluating tender, fuzzy set theory and method is adopted
to couple multiple factors on the tasks to assess to their dispatching emergency degree.
The higher evaluation of dispatching emergency is, the greater probability to select the
CT is.
The Research on the Container Truck Scheduling Based on Fuzzy Control
141

4
Fuzzy Set Theory and Method
The structure of fuzzy reasoning controller is shown in Fig. 3. Here are the two inputs,
namely, predicted travel distance, priority of handling operation. The output is
dispatching emergency degree. Dispatching decision knowledge set consists of n subset,
R1, R2, … Rn. Each subset contains some knowledge of dispatching decision, which is
described in a matrix form according to certain rules. The input information is given
according to bidding document and database of Agents. The meaning or interpretation
of the function module is based on the input linguistic variables to determine its value
corresponding fuzzy sets A1, A2; Eﬀect of fuzzy reasoning is to use and expertise subset
R1, R2,… Rn, and the information of synthesis module A to obtain comprehensive
information on the fuzzy relation composition operations in order to generate output
result. The result is also indicated by fuzzy sets. Linguistic matching function with the
fuzzy decision module is translate the result into output language, or the result of this
reasoning is converted to exact amount of that task dispatching emergency degree, to
which Agents can refer to bid.
Fig. 3. Modal of fuzzy assessment for distributing tasks
Heavy weights and the closed space on the dock make the truck more likely to be
involved in traﬃc crash which leads to road congestion. So the travel time and distance
from the current position of CT to the loading/unloading position cannot just calculated
according to the shortest route. Through judging obstruction status, the steps of ant
colony algorithm were designed to ﬁnd the optimal route. Then the predicted travel
distance and time can be calculated.
4.1
The Ant Colony Algorithm Design for the Optimal Route of Container Truck
Deﬁnition and Control of Congestion
Deﬁnition and Control of Congestion. Traﬃc ﬂow model is a kind of math equation
to express the correlation of traﬃc parameters like speed, density and ﬂux etc. Obstruc‐
tion status can be judged according to related data gathered by GPRS. Take the spur
track i → j for instance.
142
M. Yu et al.

uij = uijf
(
1 −kij∕kij0
)
qij = kijuij
uij:
The vehicle speed
uijf:
The free driving speed
kij:
The vehicle density
kij0: The jam density
qij:
The traﬃc ﬂux
Suppose the average minimum distance headway at the port of congestion density
as hijs, and average minimum time headway as hijt, then
kij0 = 1000∕hijs
qijm = 3600∕hijt; kijm = kij0∕2
uijm = qijm∕kijm
qijm: The maximum traﬃc capability
kijm: The corresponding density
uijm: Critical speed of the congestion
The judging procedure about the congestion for the spur track is as follows:
Step 1: If qij > qijm, then switch to step 3, otherwise, switch to step 2.
Step 2: If uij < uijf, then switch to step 3.
Step 3: The spur track is under congestion, and the jam knots are recorded in the
congestion table Ck.
Conﬁrming Principle of Allowed Knot Set allowedk. Considering the reality of dock,
the container is abstracted into some grids. The dynamic control over the change of ants
position comes to be true through modifying Relation Matrices by utilizing coordinate
to conﬁrm the direct pre-knot or sub-knot.
The allowed knot set allowedk equals n minus Ck and Pk at knot i for ant number k,
in which n is the total number of knots and Pk is the knots passed by ant k. There is no
repetition at the same knot for the consequent knots considering the traﬃc rules of
container terminals.
The Steps of Ant Colony Algorithm of Optimal Route for Container Trucks.
Step 1: Initialization. To set up a starting point and ending point and set the maximum
number of cycles Nmax. To initialize the control parameters α, β, Q (user-
deﬁned). At the initial, m ants are randomly put on n nodes and the amount of
information on each path is equal. The data gathered by GPRS for every spur
track are: uij, hijs, hijt, qij.
The Research on the Container Truck Scheduling Based on Fuzzy Control
143

Step 2: Put the initial knot of ant k into Pk to search till ﬁnd target knot. If does, end
the loop, and turn to step 3. If doesn’t ﬁnd the target knot, determine allowedk
and work out the transfer probability pk
ij, then adjust knot and keep searching.
The transfer probability from knot i to j of ant k can be deﬁned as:
pk
ij =
⎧
⎪
⎨
⎪⎩
𝜏𝛼
ij𝜂𝛽
ij
∑
s∈allowedk
𝜏𝛼
is𝜂𝛽
is
, s ∈allowedk
0 ,
else
Step 3: End a loop iteration when the last ant ﬁnd the target knot and record the length
Lk.
Step 4: Update the pheromone intensity
If the arrival time is between t and t + 1, the pheromone intensity is to be updated
according to the following formula:
𝜏ij(t + 1) = 𝜌𝜏ij(t) + Δ𝜏ij
Δ𝜏ij =
m
∑
k=1
Δ𝜏k
ij
Δ𝜏k
ij =
{ Q
Lk if e(i, j) ∈
Tk(t)
0
else
Q is a constant denoting the pheromone intensity. Lk is the gross route length covered
in repetition by the ant number k. ρ symbolizes the left-over elements of pheromone,
and ρ ought to be deﬁned as a number less than 1 in order to avoid the endless accumu‐
lating of pheromone.
Step 5: To compare all the routes passed by the ants and the minimum route length is
the optimal route. To output the result, the algorithm is to be ceased.
Case Analysis. The optimal route and the shortest route are shown in Figs. 4 and 5,
respectively. 
The speed of the spur tracks which are under the congestion is 10 km/h, and the speed
of other spur tracks are 25 km/h. The length of each grid in the map is 50 m, and the
oblique line is simpliﬁed into two straight segments. The travel time and distance
between the current position of CT to the loading/unloading position are calculated. The
distance and time of the optimal route is respectively 1.25 km, 0.05 h, and the shortest
route respectively 0.95 km, 0.06 h. The application of ant colony algorithm can help to
search out the optimal route while the route is under the congestion. The study of optimal
route is helpful in instructing the running of container trucks so as to avoid or relieve
traﬃc congestion.
144
M. Yu et al.

4.2
Fuzziﬁcation
Fuzziﬁcation of Distance. Table 1 shows container truck steer distance partition.
Table 1. Container truck steer distance partition
Class
1
2
3
4
5
6
7
8
R
(0, 100]
(100,
300]
(300,
500]
(500,
700]
(700,
900]
(900,
1200]
(1200,
1500]
>1500
: The starting point;  
 : The ending point;
 : The container; 
Fig. 4. Chart of the optimal route
Fig. 5. Chart of the shortest route
The Research on the Container Truck Scheduling Based on Fuzzy Control
145

R represents distance. According to the actual situation, the distance is segregated
into 8 grade: 1, 2, 3, 4, 5, 6, 7, 8. The fuzzy linguistic value is deﬁned as very near, near,
nearer, general, far, farther, very far, extremely far, and the corresponding fuzzy set is
N1, N2, N3, N, R3, R2, R1, VR. According to the distribution of discrete points, the
triangular membership function is adopted for its convenience.
Fuzziﬁcation of Priority of Handling Operation. Suppose P as the artiﬁcially deﬁned
priority, N as the number of arranged CTs, and L as the priority of handling operation.
L = P −N
The fuzzy linguistic value is deﬁned as unimportant, a little important, important,
very important and extremely important, and the corresponding fuzzy set is P1, P2, P3,
P4, VP.
Fuzziﬁcation of Dispatching Emergency Degree. Suppose S as dispatching emer‐
gency degree. The fuzzy linguistic value is deﬁned as very low, lower, low, general,
high, higher, very high, and the corresponding fuzzy set is D1, D2, D3, M, M2, M1,
VM.
Determination of Fuzzy Rules. Scheduling principle:
(1) Dispatcher’s assignment is preferential;
(2) The important task has priority;
(3) The close distance is preferential;
(4) The requirements of QCs are preferential.
The experience of terminal scheduling engineers and operators is summarized, so
that the fuzzy rules can maximally reﬂect the actual scheduling principle of terminal.
Table 2 shows the rule table for fuzzy control.
Table 2. Rule table for fuzzy control
S
N1
N2
N3
N
R3
R2
R1
VR
P1
D2
D2
D2
D2
D2
D1
D1
D1
P2
D3
D3
D3
D3
D2
D2
D1
D1
P3
M2
M
M
M
D3
D3
D2
D2
P4
M1
M2
M2
M2
M
M
D3
D2
VP
VM
M1
M1
M2
M2
M
M
D3
Fuzzy Inference and Defuzziﬁcation. Method of centroid is used in this system. Clear
outputs corresponding to each of the input values are calculated through MATLAB tool.
Table 3 shows the decision query table.
In practical work, the terminal management system can directly query the table for
the dispatching emergency degree.
146
M. Yu et al.

Table 3. Decision query table
S
1
2
3
4
5
6
7
8
1
3.22
3.22
3.22
3.22
3.22
3.18
3.04
2.7
2
3.5
3.5
3.5
3.48
3.43
3.39
3.27
3.1
3
3.9
3.9
3.81
3.65
3.59
3.53
3.43
3.35
4
4.13
4.07
4
3.92
3.87
3.72
3.56
3.44
5
4.38
4.34
4.19
4.08
4.03
3.85
3.78
3.52
6
4.54
4.51
4.39
4.31
4.14
4.01
3.81
3.59
7
4.83
4.68
4.48
4.42
4.2
4.13
3.91
3.69
Figure 6 shows the relationship between R, L and S in MATLAB.
Fig. 6. System output result schematic
The two axis represents the input of the system (L, R), and the vertical axis represents
the output (S). It can be seen from this ﬁgure that S increases with the decrease of L and
R. Space surface is smooth, illustrating the design of membership functions and fuzzy
rules are basically correct.
5
Case Analysis
There are four operations requested, shown in Table 4, and three idle CT, shown in
Table 5. After CT dispatching Agent announce the four tasks, three free CT Agents gain
task dispatching emergency degree by fuzzy reasoning controller, shown in Tables 6, 7
and 8.
The Research on the Container Truck Scheduling Based on Fuzzy Control
147

Table 4. The information of cooperative request for transporting
Task serial number
Request equipment
number
Task priority
Loading container
number
1
QC105
5
CHPU2306280
2
QC111
4
SNBU476253
3
YC203
4
U4688360
4
YC208
3
AM2U4166740
Table 5. The information of idle CTs
CT number
Operating state
CT301
idle
CT311
idle
CT312
idle
Table 6. Task dispatching emergency degree evaluated by agent of CT301
CT number
Request equipment
number
Task priority
Distance
Task dispatching
emergency degree
CT301
QC105
5
300
4.34
CT301
QC111
4
900
3.87
CT301
YC203
4
950
3.72
CT301
Y208
3
1250
3.43
Table 7. Task dispatching emergency degree evaluated by agent of CT311
CT number
Request equipment
number
Task priority
Distance
Task dispatching
emergency degree
CT311
QC105
5
850
4.03
CT311
QC111
4
850
3.87
CT311
YC203
4
950
3.72
CT311
YC208
3
650
3.65
Table 8. Task dispatching emergency degree evaluated by agent of CT312
CT number
Request equipment
number
Task priority
Distance
Task dispatching
emergency degree
CT312
QC105
5
700
4.08
CT312
YC208
3
100
3.9
CT312
YC203
4
700
3.87
CT312
QC111
4
1100
3.72
Three CT Agents bid to QC105 whose emergency degree is the highest. CT
dispatching Agent evaluates these bidders, shown in Table 9. As the table shown,
148
M. Yu et al.

evaluated dispatching emergency degree of CT301 is the highest (shown in Table 9),
CT dispatching Agent rewards Agent of CT301 and meanwhile refuses to application
of CT311 and CT312.
Table 9. Task dispatching emergency degree sort of QC105
Request equipment
number
CT number
Task important
degree
Distance
Task dispatching
emergency degree
QC105
CT301
5
300
4.34
QC105
CT312
5
700
4.08
QC105
CT311
5
850
4.03
Then CT311 and CT312 select tasks to bid, whose dispatching assessed value of
emergency is the highest among the remaining, namely QC111 and YC208. CT
dispatching Agent respectively issue orders to CT311 and CT312 after evaluating.
6
Conclusion
A model of container terminal scheduling system was established based on Multi-Agent
and how to dispatch Container Trucks (CTs) in dynamic dispatching strategy was
studied. Relationship between transport tasks and service of CTs has been taken as a
contract net using the fuzzy theory and method of optimum allocation of multiple tasks
to multiple CTs, and the bidirectional negotiation mechanism was adopted. It coupled
multiple factors on the tasks to get the assessment score of dispatching emergency.
Through judging obstruction status by using related data of the GPRS system, the steps
of ant colony algorithm were designed to calculate the distance of the optimal route.
Further research is necessary to study on the method considering more practical factors
before it can be applied in practice and the optimization of algorithm.
Acknowledgements. This paper has been funded by the National Natural Science Foundation
of China (No. 71672137 and No. 61503291).
References
1. Li, B., Li, W.: Container terminal logistics systems collaborative scheduling based on multi-
agent systems. Comput. Integr. Manuf. Syst. 17, 2502–2513 (2011)
2. Li, B., Li, W., Zhang, Y.: Agent-based modeling and simulation for vehicle dispatching at
container terminals. J. Syst. Simul. 20(19), 5158–5161 (2008)
3. Meng, Y., Shaomei, W.: Study on scheduling system based on multi-agent of container
terminal. In: Proceedings 2006 10th International Conference on Computer Supported
Cooperative Work in Design, Nanjing, China, pp. 579–584 (2006)
4. Li, G.R., Yang, D.B., Ren, D.W.: Path optimization algorithm of dynamic scheduling for
container truck. Jiaotong Yunshu Gongcheng Xuebao 12(3), 86–91 (2012)
5. Cao, Q., Zhao, F.: Port trucks route optimization based on GA-ACO. Syst. Eng. Theor. Pract.
33(7), 1820–1828 (2013)
The Research on the Container Truck Scheduling Based on Fuzzy Control
149

Researches on the Analysis Framework of Application
Layer Communication Protocol Based on SQLite
Wenyuan Xu1, Hao Li1, and Weifeng Xu2(✉)
1 China Shipbuilding Industry Systems Engineering Research Institute, Beijing, China
xwy0987@sina.com
2 School of Control and Computer Engineering, North China Electric Power University,
Hebei, China
weifengxu@163.com
Abstract. Since the concept of mobile Internet, big data and cloud computing
has been proposed, the format of the information undergoes a tremendous change,
information gradually appear problems in large quantities, the complexity and the
data form is not ﬁxed. This is not a small impact and challenge to the traditional
ﬁxed form protocol analysis. Dynamic processing and analysis of data is very
important for the data processing ﬂexibility, easy scalability and better fault
tolerance. Dynamic is mainly reﬂected in the dynamics of dynamic deﬁnition,
dynamic analysis and dynamic processing. In the dynamic analysis of the data,
you don’t need to set the speciﬁc format of the data in the program, only the
framework of dynamic analysis need to be constructed in the sending and
receiving programs, the programs can read the format of data automatically, and
get the content of data easily. This approach can make the program more suitable
for the current application in size and ﬂexibility than the traditional form of a
ﬁxed communication protocol analysis. Redundant structures does not need to
add into the programs, which is no longer processing data passively. Data trans‐
mission becomes more convenient, because the data format needn’t to be token
care when the administrator wants to transfer the data.
Keywords: SQLite · Communication protocol · Dynamic analysis
Message buﬀer
1
Instruction
At present, the format of messages transmitted between application processes on different
terminal systems is mostly defined in programs, and it can’t be changed when users
transmit messages with application processes, the flexibility of dynamically define message
format is lacked [1]. Once a user modified the message format, changes to the format of
messages in code layer by program developers would be required, and part of communica‐
tion procedure of application would need to be retested, this process will consume a lot of
manpower and material resources [5]. The format of the information undergoes a tremen‐
dous change, information gradually appear problems in large quantities, the complexity and
the data form is not fixed. This is not a small impact and challenge to the traditional fixed
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 150–157, 2018.
https://doi.org/10.1007/978-3-319-74521-3_17

form protocol analysis. Dynamic processing and analysis of data is very important for the
data processing flexibility, easy scalability and better fault tolerance.
The analysis framework of application layer communication protocol based on SQLite
[6], which is proposed in this paper, presents the format of message defined in the program
in the form of database table, users can configure the table flexibly according to needs, and
dynamically analyze it when in using. The whole framework contains protocol develop‐
ment process, dynamic object generation process, protocol analysis process, message
combination sending process, message receiving process, data stored procedure, commu‐
nication process. Dynamic is mainly reflected in the dynamic definition of communication
protocol, dynamic analysis and dynamic processing. In the dynamic analysis of the data, you
don’t need to set the specific format of the data in the program, only the framework of
dynamic analysis need to be constructed in the sending and receiving programs, the
programs can read the format of data automatically, and get the content of data easily. This
approach can make the program more suitable for the current application in size and flexi‐
bility than the traditional form of a fixed communication protocol analysis. Redundant
structures does not need to add into the programs, which is no longer processing data
passively. Data transmission becomes more convenient, because the data format needn’t to
be token care when the administrator wants to transfer the data.
In view of the SQLite memory database belongs to the lightweight database, with
superiorities such as less resource consumption, portability, security, reliability, zero
management costs and so on, we will use the SQLite memory data as the base carrier to
implement the message format storage and high-speed information buﬀer in this paper.
2
SQLite Database Technology
SQLite memory database is an embedded database engine. Specifically suitable for appro‐
priate data access on a variety of equipment with limited resources (such as mobile phones,
pads and other intelligent devices) [3, 7]. It follows the ACID relational database manage‐
ment system, its design goal is embedded, and now has been applied in many embedded
products, and its source code is abundant on the official website. SQLite memory database
takes up very few resources, the memory is only occupied in the order of 100 k bytes in the
embedded device [8]. It can be supported by mainstream desktop operating systems like
Windows/Linux/Unix and all the mobile operating system platforms, and can be combined
with plenty of programming languages, such as C#, C/C+, PHP, Java and ODBC inter‐
face, and it is faster than MySQL and PostgreSQL, which are two of the world famous open
source database management systems [4]. The SQLite supports the most of the SQL92
syntax, and allows developers to use SQL statements to manipulate data in the database, and
the SQLite is just a file, that doesn’t need to be installed or started the server processes as
databases like Oracle and MySQL. It has the following characteristics:
(1) Lightweight: The SQLite is a built-in database, all the database operations can be
completed by it with a dynamic library.
(2) Portability: The SQLite can be run in a variety of environments, we can see from
the source code provided by the oﬃcial website, not only the desktop side, but also
the mobile side is covered by the mainstream platform.
Researches on the Analysis Framework
151

(3) Security and Reliability: When a certain data need to be accessed by multiple
processes simultaneously, data in the database can be read but cannot be written at
the same time by these processes, which ensures the reliability of the data.
(4) Green software: Another feature of SQLite is green: its core engine doesn’t rely on
any third-party software, and installation does not be required. So a lot of trouble
can be saved at the time of deployment.
(5) Easy to manage: The various data information in SQLite involve graphics, tables
and other ﬁles are isolated from each other, to ensure the mutual interference would
not take place, and also facilitate the operations on the database.
3
Overall Structure of the Application Layer Communication
Protocol Analysis Framework
3.1
Architecture Design of Analysis Framework
Figure 1 shows the architecture design of the application layer communication protocol
analysis framework. In this framework, the deﬁnition between the sender and receiver
is very vague, so we do not deliberately distinguish the sender or receiver.
Fig. 1. Overall structure of the application layer communication protocol analysis framework
Firstly the framework provides the user with a data table that can be created and
modiﬁed, and this table stipulates the format in which the protocol is created. The
protocol can be analyzed by software after it has been created, that is, getting into the
protocol analysis process. During the protocol analysis process, the object of each
152
W. Xu et al.

attribute in the protocol is generated by starting the dynamic object generation process.
The message combination sending process should be started if the simulation software
needed to send data after the protocol has been analyzed.
During the message sending process, the UDP or DDS [2] interface in communica‐
tion process will not be called directly for data transmission, but starts the data stored
procedure ﬁrstly, stores the data to be sent into the memory database, and then throw a
speciﬁc message to the communication process, which initiates the send function after
receiving this message, retrieves the data to be sent from the memory database and sends
it to the destination. If a message sent by the external software is received by the
communication process of the simulation software, this message would not be analyzed
but the raw string should be stored in the memory database ﬁrstly after the UDP or DDS
communication interface receives the information, and then a speciﬁc message should
be thrown by protocol analysis process which retrieves the raw string data from the
memory database and initiates the protocol analysis function to complete the analysis
of the message after receiving it. In the analysis process, message is associated with the
speciﬁc agreement according to the information identiﬁcation, and ﬁnally completes the
analysis of information according to the protocol. The data which have been analyzed
would be stored in the memory database again after the analysis process, and then a
speciﬁc message should be sent to inform the business application layer to complete the
corresponding business functions, such as display, calculation and so on.
3.2
Communication Process Based on Framework
The basic communication process of the software based on framework implementation
has been shown in Fig. 2. As can be seen from the ﬁgure, both the data sending and
Fig. 2. Information sending and receiving process
Researches on the Analysis Framework
153

receiving is centered on SQLite memory database. Taking memory data as the center is
an another important feature of the framework proposed in this paper.
4
Analysis Method of Application Layer Communication Protocol
4.1
Protocol Conﬁguration Process
In the analysis method of application layer communication protocol, information is
conﬁgured according to the information format, and the received information is identi‐
ﬁed to accomplish the analysis of the received information and the formatted storage of
the information, and the analysis method has the function of calling the business appli‐
cation in real time through the service manage center. Users conﬁgured by the commu‐
nication protocol can conﬁgure through the conﬁguration ﬁles dynamically, the real-
time information and business application scheduling relationship users can also
conﬁgure dynamically through the conﬁguration ﬁle.
The purpose of conﬁguring the information protocol is that the information protocol
can be dynamically conﬁgured by the software developer. The framework can analyze
the received information according to the identity of the conﬁguration pair and the
message after the information protocol is conﬁgured in database. The conﬁguration of
the information protocol is conﬁgured by the developers of the simulation software. The
conﬁguration is as follows:
(1) Information index conﬁguration
Database name: Conﬁg.db
Database path: main_project_directory\Bin\Win32\Conﬁg
Database table: MessageDictionary
When conﬁguring the information protocol, the basic information in the information
index table should be conﬁgured at ﬁrst. As shown in Table 1, RecNo in the conﬁguration
table is the information number automatically generated by database; ID is the protocol
number (not repeatable); Name is the information protocol name; and ReMark is the
information protocol description.
(2) Information protocol conﬁguration
Database name: StructMessage.db
Database path: main_project_directory\Bin\Win32\Conﬁg
Database table: MessageDictionary
Table 1. An example of the protocol conﬁguration
ID
EName
Srartindex
Byte count
Type
1
InFoN
0
16
Char[]
2
InFoId
16
1
Char[]
3
ShipGauge
24
4
FLOAT
4
WindSpeek
28
4
FLOAT
5
WindCourse
32
4
DOUBLE
154
W. Xu et al.

Each information establishes a protocol conﬁguration table which is named by the
name of the information protocol (name of the Name item) in the MessageDictionary
table.
As shown in Table 1, RecNo in the conﬁguration table is the protocol ﬁeld number
automatically generated by database; ID is the ﬁeld number in the protocol (not repeat‐
able); EnName is the English name of ﬁeld; startIndex is the number of the ﬁrst byte of
the ﬁeld in the entire protocol, the byte of protocol is numbered from “0”; ByteCount is
a count of bytes occupied by the ﬁeld; Type is the ﬁeld type; Rule is an additional
processing instructions of the ﬁeld; and ReMark is the ﬁeld description.
4.2
Protocol Analysis Implementation
(1) Dynamic generation of the object:
ClassCreator::ClassCreator(constchar *cName, CreateClass cc)
{
static std::map<std::string, CreateClass> s_classMap;
pMap = &s_classMap;
map<std::string, CreateClass>::iterator it = pMap->(className);
if (it == pMap->end())
{(*pMap)[className] = cc;}
}
This code is a dynamic generation of the object, and the object needs to be generated
before dynamically analyzing in order to call the analysis process.
(2) Process of dynamic analysis
bool CStructMessage::ParseMessage(unsigned char* pBuf)
{
for(int i = 0; i < m_StructMessageVec. size(); i++)
{m_StructMessageVec[i]->SetValue(pBuf);}
return true;
}
Through using a for-loop on valid data in the database, this code analyzes each
segment of data into an available value, and combines this values into a valid string for
transmission in the next step.
5
Message Buﬀer Design
The whole system is divided into three modules by function in order to implement the
message buﬀer: ﬁrst, Socket communication module, the module is mainly used to
accept and transmit messages which require to be buﬀered, such as string, structure, etc.;
second, SQLite message access module, the module is to access messages, which need
to be buﬀered, in the SQLite database; third, the forwarding module of Windows
message mechanism, the module generates a message reminder in the process and then
distributes it; The overall structure is shown in Fig. 3.
Researches on the Analysis Framework
155

Fig. 3. Structure of the message buﬀer
In the implementation of the message buﬀer, communicating between programs is
needed, so the Socket network communication technique has been used. Messages are
transmitted through the sockets between the program A and the message buﬀer. Sockets
programming can be used in three ways, streaming socket (SOCK_STREAM), datagram
socket (SOCK_DGRAM), raw socket (SOCK_RAW); this program is designed based
on UDP socket programming uses streaming sockets. The following describes the
speciﬁc implementation of the program: The programing steps of the sender of program
A: 1, Load the socket library, and create the socket (WSAStartup()/socket()); 2, send
the connect request to the receiver of buﬀer (connect()); 3, communicate to the receiver
of the message buﬀer (send()/recv()); 4, close the socket, and close the loaded socket
library (closesocket()/WSACleanup()).
When the message has been received by the message buﬀer through the socket, it
will be stored in the SQLite database according to the type, the stored messages has
diﬀerent types, so the SQLite message storage operations are not same, too. Step 1: load/
release the Winsock library; Step 2: connect to the SQLite; Step 3: diﬀerent storage
operations are performed according to diﬀerent message types.
When the message has been stored in the SQLite database, the system will send
message reminder to diﬀerent windows based on the diﬀerent types of messages, which
involves the windows message mechanism and dialog window design with MFC, and
this will be introduced in next two sections. Messages will be processed by MFC window
after the window receives a message reminder, but due to the limitation of design, the
message processing in this experiment is simply simulated, which selects messages from
SQLite database and displays them to the MFC Edit controls.
6
Conclusions
The analysis framework of application layer communication protocol based on SQLite,
which is proposed in this paper, presents the format of message deﬁned in the program
156
W. Xu et al.

in the form of database table, users can conﬁgure the table ﬂexibly according to needs,
and dynamically analyze it when in using.
The framework that is proposed in this paper can make the program more suitable
for the current application in size and ﬂexibility than the traditional form of a ﬁxed
communication protocol analysis. Based on the framework we proposed, redundant
structures does not need to add into the programs, which is no longer processing data
passively. Data transmission becomes more convenient, because the data format needn’t
to be token care when the administrator wants to transfer the data.
However, this paper only uses one type of database, SQLite, and only implements
one kind of communication mode, UDP, so that the application scope of this framework
is still limited. In the future work, increasing the type of database and expanding the
communication model will be an important way to develop this framework.
References
1. Harrison, T., Pyarali, I.: An object behavioral pattern for demultiplexing and dispatching
handlers for asynchronous events. In: 4th Pattern Languages of Programming Conference in
Allerton Park, Illinois, 2–5 September 1997 (1997)
2. An, K., Gokhale, A.: A cloud-enabled coordination service for internet-scale OMG DDS
applications. In: Proceedings of the 8th ACM International Conference on Distributed Event-
Based Systems, New York (2014)
3. Hunt, P., Konar, M., Junqueira, F.P., Reed, B.: ZooKeeper: wait-free coordination for internet-
scalesystems. In: Proceedings of the 2010 USENIX Conference on USENIX Annual Technical
Conference, vol. 8, p. 11 (2010)
4. Li, M., Ye, F., Kim, M., Chen, H., Lei, H.: A scalable and elastic publish/subscribe service.
In: 2011 IEEE International Parallel and Distributed Processing Symposium (IPDPS),
pp. 1254–1265 (2011)
5. Mei, H., Shen, J.-R.: Progress of research on software architecture. J. Softw. 17(06), 1257–
1275 (2006)
6. Lin, M.: Design and Implementation of a Personal Address Book Management System Based
on SQLite. Jilin University (2015)
7. Tong, S.: Study on Application of Mobile Meter Reading Technology Based on SQLite. Jilin
University (2015)
8. Yang, X.: Design and Implementation of Beidou Navigation System Based on SQLite
Database. Lanzhou University (2015)
Researches on the Analysis Framework
157

The Implementation of Growth Guidance Factor Diﬀusion
via Octree Spatial Structures for Neuronal Systems
Simulation
Almaz Sabitov, Fail Gafarov, Vlada Kugurakova
(✉), and Vitaly Abramov
Kazan Federal University, Kazan, Russian Federation
lina211090@gmail.com, fgafarov1977@gmail.com,
vlada.kugurakova@gmail.com, ivitazour@gmail.com
Abstract. The BioDynaMo project was created in CERN OpenLab V and aims
to become a general platform for computer simulations for biological research.
Important development stage was the code modernization by changing the archi‐
tecture in a way to utilize multiple levels of parallelism oﬀered by todays hard‐
ware. Individual neurons are implemented as spherical (for the soma) and cylin‐
drical (for neurites) elements that have appropriate mechanical properties. The
extracellular space is discretized, and allows for the diﬀusion of extracellular
signaling molecules, as well as the physical interactions of the many developing
neurons. This paper describes methods of the real-time growing brain dynamics
simulation for BioDynaMo project, a specially the implementation of growth
guidance factor diﬀusion via octree spatial structures.
Keywords: Biodynamo · Growth guidance factor · Diﬀusion · Octree
Octree spatial structures · Neuronal systems simulation
1
Introduction
The BioDynaMo project [2] aims at a general platform for computer simulations for
biological research. It is collaboration between CERN, Newcastle university, Innopolis
university and Kazan state university where is all sides take possible part in the project
according their professional skills. The main idea of the project was to close the gap
between very specialized applications and highly scalable systems to give life scientists
access to the rapidly growing computational resource [1]. Since the scientiﬁc investi‐
gations require extensive computer resources, this platform should be executable on
hybrid cloud computing systems, allowing for the eﬃcient use of state-of-the-art
computing technology.
First development stage was the code modernization based by neurodevelopmental
simulation software Cortex3D [15] transforming the application from Java to C++ and
changing the architecture in a way to utilize multiple levels of parallelism oﬀered by
todays hardware.
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 158–163, 2018.
https://doi.org/10.1007/978-3-319-74521-3_18

In the current state of the project distributed teams working on ﬁrst iteration: devel‐
oping spatial organization layer [7] using Octree Encoding with simulation of neuronal
growing and extracellular diﬀusion and visualisation of these processes.
These tools designed for modeling the development of large realistic neural networks
like as the neocortex, in a physical 3D space. Neurons arise by the replication and
migration of precursors that mature into cells that can expand axons and dendrites. Indi‐
vidual neurons are represented through spherical (for the soma) and cylindrical (for
neurites) elements that have appropriate mechanical properties. The growth functions
of each neuron are encapsulated in set of predeﬁned modules that are automatically
distributed across its segments during growth. The extracellular space is also discretized,
and allows for the diﬀusion of extracellular signaling molecules, as well as the physical
interactions of all developing neurons.
Typical approaches for space discretization include logically structured grids, block
structured and overlapping grids, unstructured grids, and octrees. Structured (regular)
grids are relatively easy to implement, have low memory requirements but limited
adaptivity, whereas unstructured grids can conform to complex geometries and enable
non-uniform discretization.
Unstructured grids also have the overhead by explicitly storing element-node
connectivity information and in general being cache ineﬃcient because of random
access. Regular grids are easy to generate but can be quite expensive when the solution
of the PDE problem is highly localized, and localized solutions can be executed more
eﬃciently using unstructured grids. Octrees [12] oﬀer us a good balance between adap‐
tivity and good performance. They are more ﬂexible than regular grids, the overhead of
constructing element-to-node connectivity information is lower than in unstructured
grids, and they allow calculations without the use of matrix’s.
In this work we focus on reducing the time to build data octree structures, memory
overhead and time to perform numerical solution of diﬀusion equation using these data
structures.
2
Octree Data Structure
An octree is a tree data structure that is used for spatial decomposition in which every
node (octant) has maximum eight children (see, Fig. 1). They are analogous to binary
trees (maximum 2 children per node) in 1D and quadtrees (maximum 4 children per
node) in 2D. The only octant with no parents is the root, and an octant with no children
is called a leaf. An octant with one or more children is called an interior octant, octants
with the same parents called siblings. Complete octrees are octrees in which every inte‐
rior octant has exactly eight children. The depth of an octant from the root is referred to
as its level. Octrees can be eﬀectively used to partition cuboid regions, and these regions
are refereed to as the domain of the tree. Geometric modeling technique called Octree
Encoding ﬁrstly was presented in [10] at 1981.
The Implementation of Growth Guidance Factor Diﬀusion
159

Ci,j,k
Fig. 1. Left: One large cell neighboring 8 smaller cells. The ci represent the x components of the
intermediate concentration c deﬁned at the cell faces. Right: Zoom of one computational cell. The
concentration components are deﬁned on the cell faces, while the pressure is deﬁned at the center
of the cell.
3
Diﬀusion Simulation Using an Octree
Diﬀusible extracellular signaling molecules plays important role in developing neuronal
systems. They are are synthesized in by diﬀerent sources in neuronal tissue and reach
their target due to diﬀusion process. These signals can be further combined and modu‐
lated by processing of the molecule both directly at the cell surface and by the mecha‐
nisms of intracellular signaling which are activated [4]. Simulating the diﬀusion of
growth guidance factors in the 3D model space is a diﬃcult problem, and computation‐
ally expensive [9, 14]. To keep diﬀusion computationally tractable, the diﬀusion space
must be quantized at a resolution that somehow matches the precision required by the
cellular detection mechanisms, cellular density, etc. Various physical and biological
processes are modeled using parabolic equations. The ﬁnite element method is a popular
technique for solving parabolic partial diﬀerential equations (PDEs) numerically. Finite
element methods requires grid generation to generate function approximation spaces.
The concentration ci = ci(𝐫j−𝐫i, t) of growth guidance factor at the point rj in the
moment t depends on the concentration of growth guidance factor released by i-th cell
located at the point ri. The concentration obeys standard diﬀusion equation
𝜕ci
𝜕t −D2 ▵d ci + kci = Ji
(
𝐫j, t
)
.
(1)
Here D2 – the diﬀusion coeﬃcient, k is the degradation coeﬃcient, △d is the Laplace
operator in 3D space, and the quantity Ji(rj, t) means the source [6].
Figure 1 illustrates unrestricted octree data structure (see e.g. [12]) with a standard
grid arrangement [8]. This is convenient since interpolations are more diﬃcult with cell
centered data (see e.g. [13]). Coarsening is performed from the smaller cells to the larger
160
A. Sabitov et al.

cells, i.e. from the leaves to the root. This procedure shown by algorithmic pseudo-code
(see, Algorithm 1).
Data: Octree data structure.
Result: Diﬀusion process.
1 // Produce Substances Step, where J – produce coeﬃcient is unique for
diﬀerent substances.
2 for ∀Source do
3
Concentration ←+JSource
4
if Concentration > 1 then
5
Concentration ←1
6
end
7 end
8 // Diﬀusion Step, where D2 is the diﬀusion coeﬃcient.
9 for ∀Octree.Leaf do
10
Concentration ←+ 1
6D2 × (Neighbor −Current)
11 end
12 // Decay Step, where k is the degradation coeﬃcient.
13 for ∀Octree.Leaf do
14
Concentration ←×(1 −k)
15 end
Fig. 2. The dependence of diffusion step solving time on number of octree nodes: The red line –
regular grid, blue line – octree structure.
The Implementation of Growth Guidance Factor Diﬀusion
161

4
Results
We simulated the diﬀusion process with 100,000 nodes for a regular grid and using
octree spatial structures. Figure 2 demonstrates the time dependence of diﬀusion
problem solution from number of octree nodes. Comparing the results of these two
modes, one can observe the eﬃciency of using octree on large number of nodes. By
using octree spatial structures we get a signiﬁcant reduction in computation time.
In Fig. 3 shows the results of simulation as a contour plot of substance concentration
for system containing two sources. The the 2D snapshots are shown for 4 time moments.
The diﬀusion process is implemented realistically what can be seen in the Fig. 3.
Fig. 3. Visualisation of the diﬀusion process of two sources for 4 diﬀerent iterations a) 15, b) 26,
c) 41, d) 70.
5
Conclusion
In computational physics, there has been plenty of work using either multilevel grids or
adaptive mesh reﬁnement to improve the computational eﬃciency.
Quadtree or octree-based adaptive reﬁnement have also proposed in [3, 5, 11]. An
octree-based algorithm for diﬀusion process simulation, developed in this work, is used
for modeling growth and development in neural networks [15]. This method adaptively
subdivides the whole simulation volume into multiple subregions using an octree. Each
leaf node in the octree also holds a uniform subgrid which is the basic unit for simulation.
A novel node subdivision and merging scheme is developed to dynamically adjust the
octree during each iteration of the simulation.
Further development of the project involves the development of octree algorithms
to perform in the cloud using parallel computations. The next stage in the development
of diﬀusion of substances will be devoted to the realization of existing algorithms by
using methods of parallel calculations.
162
A. Sabitov et al.

Acknowledgements. This work was funded by the subsidy of the Russian Government to support
the Program of competitive growth of Kazan Federal University among world class academic
centers and universities.
References
1. BioDynaMo. Code Repository on GitHub. https://github.com/BioDynaMo/biodynamo/
2. Breitwieser, L., Bauer, R., Di Meglio, A., Johard, L., Kaiser, M., Manca, M., Mazzara, M.,
Rademakers, F., Talanov, M.: The biodynamo project: creating a platform for large-scale
reproducible biological simulations. In: CEUR Workshop Proceedings, vol. 1686 (2016)
3. Charlton, E.P., Powell, K.G.: An octree solution to conservation-laws over arbitrary regions
(oscar). In: 35th Aerospace Sciences Meeting and Exhibit (1997)
4. Chilton, J.K.: Molecular mechanisms of axon guidance. Dev. Biol. 292(1), 13–24 (2006)
5. Coirier, W.J.: An adaptively-reﬁned, cartesian, cell-based scheme for the euler and navier-
stokes equations (1994)
6. Crank, J.: The Mathematics of Diﬀusion, 2nd edn. Clarendon Press, Oxford (1975)
7. Dmitrenok, I., Drobnyy, V., Johard, L., Mazzara, M.: Evaluation of spatial trees for simulation
of biological tissue. CoRR, abs/1611.03358 (2016)
8. Harlow, F.H., Welch, J.E.: Numerical calculation of time-dependent viscous incompressible
ﬂow of ﬂuid with free surface. Phys. Fluids 8(12), 2182–2189 (1965)
9. Hentschel, H.G., van Ooyen, A.: Models of axon guidance and bundling during development.
Proc. Biol. Sci. 266(1434), 2231–2238 (1999)
10. Meagher, D.: Geometric modeling using octree encoding. Comput. Graph. Process. 19(2),
129–147 (1982)
11. Melton, J.E., Enomoto, F.Y., Berger, M.J.: 3D automatic cartesian grid generation for euler
ﬂows. In: 11th Computational Fluid Dynamics Conference 1993, pp. 959–969 (1993)
12. Samet, H.: The Design and Analysis of Spatial Data Structures. Addison-Wesley,
Massachusetts (1990)
13. Strain, J.: Fast tree-based redistancing for level set computations. J. Comput. Phys. 152(2),
664–686 (1999)
14. Suleymanov, Y., Gafarov, F., Khusnutdinov, N.: Modeling of interstitial branching of axonal
networks. J. Integr. Neurosci. 12(01), 103–116 (2013)
15. Zubler, F., Douglas, R.: A framework for modeling the growth and development of neurons
and networks. Front. Comput. Neurosci. 3, 25 (2009)
The Implementation of Growth Guidance Factor Diﬀusion
163

Intelligent Perceptive QoS Based Centralized Admission
Control for Route Computing
Xuejing Li
(✉), Yajuan Qin, Haohui Fu, Zhewei Zhang, and Xiaorong Lin
School of Electronic and Information Engineering, Beijing Jiaotong University,
Beijing 100044, China
15111045@bjtu.edu.cn
Abstract. For the diversity of applications and traﬃc, the requirement of Quality
of Service (QoS) is various. In order to improve utilization of resource, perform‐
ance of the network and experience of users, the QoS aware routing approaches
have been researched. As the Software Deﬁned Networking (SDN) emerging and
the machine learning developing, this paper proposes a mechanism based on ﬂow
classiﬁcation and routing strategy selection. This architecture is running as
modules on SDN controller. In Mininet platform, we conducted experiments on
POX controller to verify eﬀective of our design and test the performance. The
measuring results show that throughput and delay of the network is distinct while
using diﬀerent routing theory.
Keywords: Demand perception · Flow classiﬁcation · Route computing
Strategy controlling · Dynamic updating
1
Introduction
For the rapid development trend, high bandwidth and low delay requirements are major
challenges with the emergence of new techniques like mobile communication and 5G.
Since the network traﬃc is increasing due to a wider spectrum of new applications, it is
important to address the essential trade-oﬀ problem of appropriate allocation of restrict
network resources and relatively eﬃcient routing computing method. The QoS demands
required by diﬀerent applications are diversity, and there are various ﬂows existed in
same application. In addition, even in the similar types of applications there exists some
diﬀerent QoS demand ﬂows. For instance, real-time multimedia applications usually
require high QoS demands on bandwidth and delay but are tolerant to packet loss rate,
while the high deﬁnition video streaming and the online competitive gaming have high
data rate need and strict latency guarantee respectively. Furthermore, in the scenario of
multimedia delivered network, there are diverse speciﬁc types of ﬂows such as, video,
audio, imaging, etc.
Custom route of path decision should be made for each ﬂow according to its QoS
requirements and network conditions, which is conducive to the suitable assignment of
limited network resource. On the one hand, if every ﬂow selects the shortest path
greedily, it would bring about local congestion so that the subsequent coming packets
may be dropped. On the other hand, more packets could be accepted with balancing
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 164–172, 2018.
https://doi.org/10.1007/978-3-319-74521-3_19

network traﬃc although it may consume excessive resource and derive long propagation
delay. Therefore, the routing theory needs to be customized for some special ﬂows
according to the perception of QoS requirements and the detection of network conditions
in union, which is conducive to optimal QoS guarantee and network resource orches‐
tration [1, 2].
Moreover, there is no one algorithm can outperform all the remaining ones in every
case, while there are always some scenarios where an algorithm performs well.
However, the QoS aware theory makes the routing algorithm even more complex
considering with classifying various ﬂows to match dynamic and adaptive strategy
respectively. Hence, the routing strategy should be trade-oﬀ between the optimality and
simplicity without too much processing overhead or memory consumption due to its
high complexity. Compared to some steadily and inﬂexible strategy aiming to realize
better average performance, we present a novel mechanism to improve the network
performance and reduce the computing complexity to some extent.
Considering traﬃc distribution state, QoS requirements of ﬂows and dynamic avail‐
able bandwidth resource, the proposed strategy enhances the high utilization of the
overall resource at the cost of increasing resource consumed by individual ﬂows. And
to ﬁnd the minimum cost path, the evaluation results show that it alleviates the perform‐
ance degeneration and simultaneously improves the utilization of whole network
resource.
SDN architecture consists of three layers: forwarding layer, control layer and appli‐
cation layer, as shown in Fig. 1. Physical switch is the crucial element of the forwarding
layer, which takes on all the performance of forwarding data in accordance with the
forwarding rules in ﬂow tables provided by the controller. Besides, the centralized
control plane is core layer of the network which takes the responsibility of topology
discovery, traﬃc classiﬁcation, strategy formulation and network conﬁguration. There‐
fore, the network traﬃc is allocated to the paths operated directly by OpenFlow switches,
where the ﬂow-handling rules are installed by the central SDN controller [3].
Fig. 1. SDN architecture
Fig. 2. The construction of OpenFlow switch
The OpenFlow switch is composed by three components including ﬂow table, secure
channel and OpenFlow protocol as illustrated in Fig. 2. Flow table contains the ﬂow-
oriented forwarding rules issued by the relevant controller. Secure channel ensures the
Intelligent Perceptive QoS Based Centralized Admission Control
165

interaction between the switch and the controller. And OpenFlow protocol regulates
messages in secure channel to realize functions, like traﬃc statistics collection, access
control and network conﬁguration.
As the SDN architecture can provide global visibility of network resources and
inherent programmable interfaces, some explorations on the fusion of SDN and other
technology have been carried out. Although some concepts have been presented several
years ago, there are still many issues worth to be researching further, such as eﬃcient
self-organized routing mechanism, network resource management, QoS perception and
environment forecasting [4–6].
SDN is a promising concept with the powerful advantage to introduce new dimen‐
sions of ﬂexibility and adaptability in current communication networks. For instance,
the realization of QoS aware routing becomes possible in an agile and dynamic manner.
Although some routing algorithms have been well researched, they were not applied in
communication networks due to its high implementation complexity and realization
costs. We propose the QoS-aware resource management routing mechanism, which
seems to be promising to provide better network resource management, traﬃc control,
and application classiﬁcation [7].
SDN paradigm radically transforms network architecture and provides some char‐
acteristics of programmable, eﬃcient, ﬁne-grained, dynamic, accurate, global-viewed,
and superior computational capacity. All of these features prompt the development of
network with using advanced optimization algorithm.
Computational intensive machine learning algorithms integrate several online
routing algorithms for the real-time control of new connection requests. Depending on
the available resource and status of the traﬃc, the cognitive computing could classify
the ﬂows and make adaptive routing strategies. Since new variables and additional
constraints are emerging over time, the routing mechanism is supposed to utilize
machine learning techniques to select the best sequence of accommodative decisions,
while considering the anticipative future scenarios for improving the performance of
network. In this paper, the proposed dynamic adaptive QoS-aware routing mechanism
is constrained with available network resource and distinct ﬂow requirements upon SDN
platform [8, 9].
The rest of this paper is organized as follows. Section 2 proposes implementation of
the QoS-aware routing mechanism with algorithms in detail. Section 3 describes the
simulations. Section 4 analyzes the experimental results and evaluates the performance.
Finally, Sect. 5 outlines the future researches and makes a conclusion.
2
Implementation and Algorithm
Our QoS aware routing selection computing mechanism consists of ﬁve modules,
including Traﬃc Measurement Module, Flow Classiﬁcation Module, Topology Detec‐
tion Module, Routing Selection Module, and Conﬁguration Updating Module, which
are working collaboratively on SDN controller.
First, the module Traﬃc Measurement collects these information of ﬂow statistics
from the forwarding layer. Then, Flow Classiﬁcation module diﬀerentiates the QoS
166
X. Li et al.

requirements like bandwidth of real-time traﬃc based on analyzing the ﬂow statistics
information using data mining technique, such as K-means algorithm. Moreover,
Topology Detection module gathers all the up-to-date information of the link state from
switches, such as delay and bandwidth, with using traﬃc monitoring tools like sFlow
or NetFlow. Furthermore, Routing Selection module includes two parts. On the one part,
the smaller bandwidth demanding ﬂows match to the routing algorithm of Bandwidth
Reserved Shortest Path (BRSP). On the other part, the larger bandwidth demanding
ﬂows match to the routing algorithm of Load Balancing Considered Polling Path
(LBCPP). Finally, the Conﬁguration Update module updates the strategies by rotating
above processes if the degree of the variation exceeds the setting threshold.
2.1
Traﬃc Measurement Module
Applications supporting various multimedia data have speciﬁc QoS requirements on
bandwidth, delay and packet loss. And the module is responsible for collecting infor‐
mation about the traﬃc type. For instance, the applications like high deﬁnition video
streaming or online competitive gaming, have high demand of data rate and strict
demand of latency respectively.
2.2
Flow Classiﬁcation Module
This module holds the purpose to ﬁnd the reasonable and eﬀective ﬂow classiﬁcation
theory according to the perception of ﬂow’s QoS requirements and the detection of link
states from nodes. To trade oﬀ the complexity and the practicability of mechanism, the
module measures the bandwidth requirement of ﬂows, and calculates a critical value to
divide the ﬂows into two types.
In order to classify the ﬂows rationally, the clustering algorithm of machine learning
is employed in this process. The K-means classiﬁcation method make the previous traﬃc
ﬂows to fall into categories, such as mice prior ﬂows and inferior elephant ﬂows. When
new ﬂows reach the switches, the ﬂows will be matched to the suitable class and
forwarded along the path based on the adaptive theory.
It is inevitable that the status of ﬂows is changing ongoing. For instance, if the size
enlarges and the amount increases, the mechanism should reserve more bandwidth to
guarantee for the mice ﬂows admission. Therefore, the residual network resource
remained for the elephant ﬂows should be conﬁned to release more resource for the mice
ﬂows with priority.
2.3
Topology Detection Module
An OpenFlow-enabled network can be modeled as a network graph G = (S, E), where
S represents OpenFlow-enabled switches and E indicates the link edge between two
adjacent switches. Link state of edge is comprised of the link capacity, link delay, and
link available bandwidth.
Our mechanism attempts to avoid traﬃc congestion when incoming ﬂow is added
to the link. Cost of a link is calculated according to link capacity, link utilization, and
Intelligent Perceptive QoS Based Centralized Admission Control
167

required bandwidth of incoming ﬂow. This principle ensures that the link with lower
utilization rate has smaller cost value. When link utilization comes near the link capacity,
it will become expensive to occupy this link.
2.4
Routing Selection Module
The routing algorithms are committed to ﬁnd the most feasible path for meeting the
speciﬁc QoS requirements, which is diﬀerentiated depending on ﬂow types: bandwidth-
sensitive traﬃc, delay-sensitive traﬃc, and others type traﬃc. A feasible path can
provide suﬃcient resource for the demands. However, ﬂows of diverse requirements
adapt to diﬀerent optimal routing theory. Therefore, in the case of the QoS requirements
being perceived, the proposed algorithms would assign the ﬂow an optimal route which
is beneﬁcial to both the traﬃc quality and network performance. As explained following,
the module consists of two routing algorithms.
The routing algorithm of BRST is matched to the smaller bandwidth demand ﬂows,
which are delay sensitive, bandwidth constrained, and packet loss intolerance. Reserved
bandwidth-guarantee could provide small jitter and low packet loss for some special
ﬂow in some multimedia applications. We ﬁnd the path with the smallest delay for delay-
sensitive ﬂows. With consuming the reserved bandwidth, this theory could ﬁnd the
optimal route which can assure admission guarantee for the ﬂow with higher priority,
and compute the shortest paths for the smallest propagation delay. Considering the
demand of the actual network, we adopt the Bellman-Ford algorithm to realize our BRST
algorithm. If there is no feasible path, the other routing theory would be employed to
ﬁnd a path to ensure the delivery of the ﬂow temporarily. And the update module starts
to modify the corresponding information and settings immediately.
On the contrary, the routing LBCPP is matched to the larger bandwidth demand
ﬂows which is bandwidth sensitive, delay constrained, and with moderate packet loss
torment. The lager bandwidth is provided to improving the performance of network with
load balancing theory. The routing algorithm of our mechanism attempts to avoid traﬃc
congestion whether the states of the network or the features of traﬃc are changed. After
the bandwidth reserved previously, the module uses the residual network resource to
ﬁnd the available paths from source to destination. And the optimal paths with enough
bandwidth are ﬁgured out based on the delay constraint and packet loss. Since we
consider hop counts as the only impact of propagation delay, we set the hop constraint
not exceeding the 1.2 times value of that in the shortest path. We use a Top-K paths
selection algorithm thinking about available bandwidth, delay, and utilization rate to
decide the route by round robin method. Path load balancing is used for distributing
workload to reinforce network reliability and optimize network resource utilization.
2.5
Conﬁguration Updating Module
Due to the instability and the resource restriction of network environment, more atten‐
tions should be paid to the continuously emerging variation. For accommodating the
variance with regard to the distribution of the ﬂows and the condition of the network
over time, the routing theory should be dynamically adjusted the critical value of ﬂow
168
X. Li et al.

classiﬁcation and the amount of reserved bandwidth. Meanwhile, the ﬂow table installed
in the switches should be updated necessarily for eﬃcient network usage.
In the module, the update process would be triggered if the classiﬁcation metric or
the reserved bandwidth is out-of-date, which means that the variance is exceeding the
threshold of extent or time. If some ﬂows matching the ﬁrst routing algorithm cannot
ﬁnd feasible path using the reserved bandwidth, the ﬂow would use the other routing
theory to ensure forwarding temporarily. And the reserved bandwidth should be
increased to avoid packet losing with providing better QoS.
In addition, the traﬃc forwarding rules in ﬂow table are generated by the central
controller and installed to the corresponding OpenFlow-enabled switches. As the
network traﬃc and topology are unpredictable, the ﬂow table will be dynamically
updated to keep consistent with the employed strategy. On one hand, paths are timely
evaluated whether new ﬂow arrives or network changes. On the other hand, the global
statistical information is timely updated and the ﬂow table is set to delete overtime
entries. We employ suitable period and threshold of updating theory to trade oﬀ the
consistency of real-time state information and overhead of frequently updating commu‐
nication in the control plane.
3
Simulation and Setting
In order to evaluate the proposed architecture of SDN-based mechanism, it is imple‐
mented in POX platform and Mininet simulator to emulate the network topology. POX
is a software platform for rapid exploiting and prototyping of network controller devel‐
oped by Python, which could invoke scripts to operate corresponding modules to derive
the adaptive actions. Mininet provides a lightweight test bed for developing OpenFlow
applications to help users to create a realistic virtual network, including a collection of
hosts, Open vSwitches and network links on a single machine [10, 11].
And the whole simulation is running on an experimental computer with the operating
system of Ubuntu 14.04. Therefore, the computing performance is inﬂuenced by the
physical machine with essential parameters, such as Intel Core i5-4590 CPU @
3.30 GHz, and 4.00 GB RAM.
In practice, data is commonly delivered to destination through multi-hop path in the
complex topology. For the sake of simplifying the simulation, we conducted experi‐
ments in the topology graph showed in the Fig. 3. In the Topology, we set H1 and H2
Fig. 3. Topology graph of simulation
Intelligent Perceptive QoS Based Centralized Admission Control
169

as clients, while H3 and H4 as servers. Meanwhile, 9 Open vSwitches were built as the
OpenFlow-enabled switches. And the bandwidth of each link was given to 10 Mbps.
Under stationary condition, the ﬂow distribution and the network state are invariable
over time. We conduct experiments on the condition to test algorithms of our theory.
Under dynamic condition, the moment of ﬂows arrival processes could follow the distri‐
butions like Poisson. If the size of the ﬂows is given, the departure time of ﬂow related
to the transmission delay could be equivalent to the bandwidth demand which is set as
Normal distribution. This work conduct experiments on Mininet to evaluate our mech‐
anism and compare the results in custom dynamic condition, in which the UDP traﬃc
of 5 Mbps is emulated as background ﬂow from H1 to H3 by using the command of
iperf. Using single routing algorithm, the throughput and delay are measured respec‐
tively when sudden ﬂows are generated at some point.
4
Evaluation and Analysis
In the case of the application of real time multimedia, QoS metrics with respect to band‐
width, delay, delay jitter, and packet loss ratio are usually taken as major concerns of
receivers, while the senders care more about the traﬃc transmission in high load and
high randomness. For the network management, the network performance depends on
the metrics, such as average link utilization, resource consuming, packet loss ratio,
throughput, device energy eﬃciency, and average end-to-end delay. Therefore, the QoS
aware routing problem could be converted to the multiple objective programming, which
is to ﬁnd a minimum cost of object with satisfying some QoS constraints.
In this paper, we focus on two metrics namely throughput and delay. Under stationary
condition, intuitively, our classiﬁed synthesized routing QoS mechanism has larger
throughput than BRST and smaller delay than LBCPP. Under custom dynamic condition
with adding background ﬂows through the shortest path at the time of 50 s and removing
the ﬂows at the time of 110 s, we compare the throughput and delay between the two
unclassiﬁed single routings as shown in Figs. 4 and 5. When the background ﬂows inject
in the network, we could ﬁnd that the throughput with BRST collapsed obviously, while
the other algorithm of LBCPP shows a higher throughput at same time. Similarly, the
latency of the BRST increases more evidently than the other. Therefore, from H2 to H4,
the results show that the proposed routing mechanism could reduce the delay for prior
mice-ﬂows and increase the throughput for inferior elephant-ﬂows. In the future, we
will employ the other dynamic condition with injecting the background traﬃc subjected
to Poisson distribution or Normal distribution.
170
X. Li et al.

Fig. 4. Throughput comparison
Fig. 5. Delay comparison
5
Conclusion and Future Work
Based on the fusion of techniques, this work presents an intelligent perceptive QoS
mechanism for forwarding the diverse ﬂows with the adaptive routing. The architecture
consists of ﬁve modules which are collaborating together on SDN controller. First, it
ﬁnds the feasible classiﬁcation among ﬂows according to the requirements of the band‐
width. Second, for the delay sensitive ﬂows, the theory is made with bandwidth reser‐
vation for ﬁnding the shortest path in order to guarantee the delay demand and reduce
packet loss rate caused by propagation in network. Third, for the bandwidth sensitive
ﬂows, the multipath polling method of load balance considered multipath is applied to
improve the network performance. Finally, it updates an adjustable traﬃc classiﬁcation
threshold so as to keep optimal for both network performance and service quality.
In the future work, we would employ and exploit other computing algorithms to
implement the proposed mechanism. In the paper, the machine learning technology
named K-means clustering is used to classify the ﬂows by ﬁnding the most suitable
dividing metrics for the previous period. Since the ﬂow classiﬁcation named 2-means
may not be eﬃcient, other skills like reinforcement learning would be used to ﬁnd the
optimal adaptive dividing metrics with real-time feedback. Moreover, in the updating
module, the predicted method could be exploited to get the potential growing tendency
of traﬃc features for more appropriate classiﬁcation. In addition, the machine learning
could also be applied in the controller analysis, admission control, and packet security.
References
1. Vissicchio, S., Tilmans, O., Vanbever, L., et al.: Central control over distributed routing. In:
SIGCOMM 2015, 17–21 August, vol. 45, no. 5, pp. 43–56 (2015)
2. Han, L., Sun, S., Joo, B., et al.: QoS-aware routing mechanism in OpenFlow-enabled wireless
multimedia sensor networks. Int. J. Distributed Sensor Netw. 12(7), 9378120 (2016)
3. OpenFlow Speciﬁcation. Open Networking Foundation. https://www.opennetworking.org/
sdn-resources/onfspeciﬁcations/openﬂow
Intelligent Perceptive QoS Based Centralized Admission Control
171

4. Liu, J., Li, J., Shou, G., et al.: SDN based load balancing mechanism for elephant ﬂow in data
center networks. In: International Symposium on Wireless Personal Multimedia
Communications, pp. 486–490. IEEE (2014)
5. Craig, A., Nandy, B., Lambadaris, I., et al.: Load balancing for multicast traﬃc in SDN using
real-time link cost modiﬁcation. In: IEEE International Conference on Communications, pp.
5789–5795. IEEE (2015)
6. Li, J., Chang, X., Ren, Y., et al.: An eﬀective path load balancing mechanism based on SDN.
In: IEEE International Conference on Trust, Security and Privacy in Computing and
Communications, pp. 527–533. IEEE (2014)
7. Durner, R., Blenk, A., Kellerer, W.: Performance study of dynamic QoS management for
OpenFlow-enabled SDN switches. In: IEEE International Symposium on Quality of Service,
pp. 177–182. IEEE (2015)
8. Mizrahi, T., Saat, E., Moses, Y.: Timed consistent network updates in software-deﬁned
networks. IEEE/ACM Trans. Netw. 24, 1–14 (2016)
9. Abu Alsheikh, M., Lin, S., Niyato, D., et al.: Machine learning in wireless sensor networks:
algorithms, strategies, and applications. IEEE Commun. Surv. Tutor. 16(4), 1996–2018
(2014)
10. Mininet. http://mininet.org/
11. POX. http://www.noxrepo.org/pox/about-pox/
172
X. Li et al.

Research on the Shortest Path Problem Based on Improved
Genetic Algorithm
Baoliang Wang1(✉), Susu Yao2, Kaining Lu1, and Huizhen Zhao1
1 Information and Network Center, Tianjin University, Tianjin 300072, China
{wbl,knlu,tdee79}@tju.edu.cn
2 School of Electrical and Information Engineering, Tianjin University, Tianjin 300072, China
yaosusu@tju.edu.cn
Abstract. Aiming at the complex large-scale shortest path problem, a modiﬁed
genetic learning algorithm has been oﬀered. Firstly, in order to prevent premature
convergence of evolution population, a new ﬁtness function for the shortest path
is proposed. At the same time, to ensure the global search ability of the algorithm
a selection method of crossover probability is proposed. And to ensure the local
search ability of genetic algorithm a selection method of mutation probability is
proposed. The experimental results show that the modiﬁed genetic algorithm has
a better success rate compared with the traditional algorithm.
Keywords: Genetic algorithm · Shortest path · Fitness function
Crossover operator · Mutation operator
1
Introduction
The shortest path problem has been widely employed in many ﬁelds, such as intelligent
transportation [1], computer networks [2], robot path planning [3, 4] and so on. For the
shortest path problem, there are many deterministic algorithms, such as Dijkstra algo‐
rithm [5] and Ford algorithm [6]. However, in the practical application, the size of the
shortest path problem is expanding, the constraint condition of the shortest path problem
is increasing, and the complexity of the inﬂuencing factors is also increasing. The above
leads to the diﬃculty of solving the shortest path is also increasing. In the traditional
algorithm, it is diﬃcult to eﬀectively ﬁnd a perfect solution to quickly ﬁnd the optimal
solution or suboptimal solution. For the shortest path problem of large scale, the intel‐
ligent algorithm can satisfy the user’s time and precision conditions to obtain the optimal
or suboptimal solution.
Genetic algorithm (GA) has good adaptability, robustness and flexibility [7]. In
this paper, we use genetic algorithm to solve the shortest path problem. But, there
are some problems in genetic algorithm, such as premature convergence and weak
local search ability, low efficiency and low precision. These problems are caused by
the irrational design of population scale, the irrational design of fitness function and
the irrational selection of cross probability and mutation probability. In order to solve
the above problems, two kinds of fitness function transformation methods are
designed, and the crossover probability and mutation probability selection strategies
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 173–182, 2018.
https://doi.org/10.1007/978-3-319-74521-3_20

are optimized. The effectiveness of the algorithm to deal with the shortest path
problem is verified by simulation experiments.
2
Genetic Algorithm
The shortest path genetic algorithm consists of the following ﬁve parts: (1) the genetic
representation of the shortest path problem, (2) the encoding and decoding of the shortest
path problem, (3) designing an ﬁtness function according to the merits of each chromo‐
some, (4) designing the genetic operators which used to change the genetic structure of
oﬀspring, usually include selection, crossover, mutation and so on, (5) parame‐
ters setting, including population size, probability of applying genetic operator, etc. The
keys to solve the shortest path problem based on GA are the design of the ﬁtness function,
the setting of the crossover operator and the mutation operator.
Genetic algorithm [8, 9] is an algorithm to simulate the biological evolution process
to ﬁnd the optimal solution. Its main advantage lies in its good adaptability, robustness
and ﬂexibility. First, as a kind of intelligent search algorithm, the optimization process
of genetic algorithms do not have much mathematical requirements. The genetic algo‐
rithm is modeled according to the speciﬁc problem, and the solution is found by evolu‐
tion without having to consider the speciﬁc internal operation problem. So it has a good
adaptability. And it borrows the evolutionary idea of biogenetics can carry out global
search very eﬃciently. The computational complexity of genetic algorithm is small
Begin
 Initial generate 
population 
gen=0
Calculate per 
Individual 
fitness value
Didn't reach the specified 
Generation or precision
Get results
End
Execute selection 
operation
Execute crossing 
operation
Execute mutation 
operation
gen = gen+1
Fig. 1. Genetic algorithm ﬂow chart
174
B. Wang et al.

Compared with the traditional algorithm. At the same time, the genetic algorithm can
be hybridized with other related heuristic algorithms, which can be ﬂexible to deal with
practical problems.
The basic process of genetic algorithm as shown in Fig. 1, including initial popula‐
tion, crossover, mutation, selection and so on. Individuals evolve in the direction of
higher ﬁtness by crossover, mutation and selection operations to ﬁnd the optimal solution
or suboptimal solution [10].
3
Optimization Design of Genetic Algorithm
3.1
Genetic Representation
Coding is a key factor to improve the eﬃciency and success rate of genetic algorithms.
In this paper, we use the priority coding method to represent the shortest path problem.
The advantage of the priority coding method is that any encoding has a corresponding
path; most crossover operations are very easy to implement; there is no loop after
crossing.
Encoding: Randomly generate sequences to form the original chromosomes. The
length is the total number of points in the graph. According to the coding pseudo-code,
we can get the random sequence which named “chromosome”. The node ID in Fig. 2 is
a random sequence containing 11 points. The speciﬁc encoding pseudo code is as
Table 1.
Table 1. Priority encoding pseudo code
Encoding Pseudo Code:
for(i=1:to n)
chromosome[i]=i;
for(i=1 to n/2)
repeat
q=random(1,n);
p=random(1,n);
until p!=q;
swap(chromosome[p],chromosome[q]);
end
Research on the Shortest Path Problem Based on Improved Genetic Algorithm
175

Fig. 2. Priority decoding and encoding process
Decoding: The decoding process similar to the depth traversal algorithm. Take the
Fig. 2 as an example, ﬁrstly, according to the topology of the path, we can see that node
Table 2. Priority encoding pseudo code
Decoding Pseudo Code
Initialize i=1;l=1;path[l]=1;
while S_1 != 
temp_j=max(chromosome[j],j∈S_i);
if(chromosome[temp_j]!=0)
path[l]=temp_j;
l=l+1;
chromosome[temp_j]=0;
i=temp_j;
else
S_i=S_i\temp_j; // delete temp_j from S_i
chromosome[i]=0;
if(l<1)l=1;break;
i = chromosome[l];
end
176
B. Wang et al.

1’s son nodes are node 2, node 3 and node 4. In the node ID, We can get their value, ID
[2] = 1, ID [3] = 10, ID [4] = 3. Secondly, choose the maximum value from the son
nodes of node 1, and then change the path to 1-3. Thirdly, repeat the ﬁrst two steps until
ﬁnd the shortest path. So we can get path 1-3-6-5-8-11 based on the node ID. Decoding
method pseudo code is as Table 2.
3.2
Fitness Function
The ﬁtness function determines the evolution direction of the GA process. And it has a
great eﬀect on the convergence speed of a GA process. In the paper, our coding method
of the shortest path algorithm containing the designated points is the priority coding
method, so a chromosome can be successfully decoded corresponds to an actual path.
A path is a correct solution when the number of designated points in this path meet the
requirements. Therefore, we sets the ﬁtness function to the number of the designated
points in the path. The direction of evolution is the direction of the path that contains
the number of designated points must be more and more. The ﬁtness function does not
use the weight of the path as a reference, the above selection way may cause a larger
weight to be included in the found feasible solutions. For the above defects, the algorithm
is used to compare the many feasible solutions, and the optimal solution is chosen as
the global optimal solution.
3.2.1
Exponential Transformation
{
f
′(i) = e−𝛼f(i)
𝛼=
m√
t∕(favg + 𝜀), m = 1 + lg(T)
(1)
Where f(i) represents the ith chromosome’s original ﬁtness value, f
′(i) represents
the new ﬁtness value of f(i) to adapt to exponential transformation, 𝛼 is an exponential
transformation coeﬃcient which is a positive number of dynamic changes that gradually
increase as evolutionary generation increases, t is the current chromosome evolutionary
generation, T is the largest genetic generation, favg is the average ﬁtness which is the
average value of f(i), 𝜀 is a positive number which small enough.
According to the analysis of the ﬁtness function, we can see that f(i) is a non-negative
number. In the process of population evolution, favg in the early evolution is very large
and 𝛼 is small. With the evolution of the population, favg gradually reduced, and the
evolution of the current generation t is increased. According to this trend, the transfor‐
mation of ﬁtness function can be guaranteed 𝛼 gradually increase. Therefore, the ﬁtness
function is an adaptive dynamic adjustment function.
3.2.2
Logistic Curve Function Transformation Method
The expression of logistic curve function is y =
1
1 + ex. According to the logistics curve
in Fig. 3, it can be seen that the function range is between 0 and 1. At the same time, the
function value is between 0.5 and 1 when the value of x is less than 0, and the function
value is between the range of 0 and 0.5 when the value of x is larger than 0.
Research on the Shortest Path Problem Based on Improved Genetic Algorithm
177

logistic
Fig. 3. Logistic curve
As can be seen from Fig. 3 the logistic function gets a value of either 1 or a value of
0 when the value of x is outside the [−10, 10]. If the logistic function is used as ﬁtness
function directly, the ﬁtness value of many chromosomes will be close to 1 or 0, so that
majority of the chromosomes in the population are less competitive which will lead to
precocious phenomenon. In order to avoid the premature phenomenon, a new ﬁtness
function is designed for the logistics function.
f ∗(i) =
⎧
⎪
⎨
⎪⎩
1
1 + exp((f −favg)∕c) g ≥30%n
1
1 + exp(f −favg)
g < 30%n
(2)
where f ∗(i) is a new ﬁtness value of the ith chromosome obtained by logistic function
transformation, f(i) is the original ﬁtness value of the ith chromosome, favg is the average
value of the current population f(i), g indicates the number of chromosomes where the
diﬀerence outside range (−10,10), c is the order of magnitude of the maximum value of
|||f −favg|||.
The above two kinds of ﬁtness function to meet the general ﬁtness function necessary
conditions. At the same time, relative to other ﬁtness function, they have the following
properties: (1) the ﬁtness function is a nonnegative number, so the objective function
can be regarded directly as the original ﬁtness function, regardless of its positive or
negative; (2) the original ﬁtness function value is inversely proportional to the value of
the new ﬁtness function, that is, the greater the value of the original ﬁtness function the
smaller the value of the new ﬁtness function; (3) to ensure that most of the chromosomes
in the population are highly competitive, and the probability of premature appearance
is reduced.
3.3
Crossover Operator
In the genetic algorithm, the crossover operator has a great eﬀect on the global searching
ability and convergence ability. In this paper, we presents a new cross probability setting
method. Firstly, in order to prevent the destruction of the excellent individual gene in
the population, we make the excellent individual directly as the next generation of indi‐
viduals. In this paper, we assume that the ﬁrst 30% chromosomes in the population are
the excellent individuals. Secondly, in order to reduce invalid crossover, the remaining
178
B. Wang et al.

chromosomes adopt the non-equal probability pairing strategy, we use the correlation
function in Formula 3 to calculate the correlation between chromosomes. A higher
crossover probability is set for two chromosomes with uncorrelated correlations.
First of all, we selected a chromosome X that had not been operated by crossover,
and then calculated the correlation index between X and other non-overlapping chro‐
mosomes. According to the correlation index, the probability of crossover between the
selected chromosome and X was calculated. The roulette wheel method is chosen as the
selection operator. So, we can get the chromosomes that cross with X.
a. Correlation index function
r(X, Y) =
m
∑
j=1
xj ⊕yj, xj ⊕yj =
{ 0, xj = yj
1, xj ≠yj
(3)
Where Yi is a chromosome that has not been crossed, m is the total number of genes
in the two chromosome which has the larger number of genes, r(X, Yi) is indicated that
the number of genes is not the same between X and Yi, the greater the r(X, Yi), the smaller
correlation between X, and Yi, and the smaller the probability of invalid crossover oper‐
ation will be reduced.
b. The crossover probability of chromosome Yi selected to cross with chromosome X
p(Yi|X ) = 1
m
(
1 + 𝜆
r(X, Yi) −ravg
rmax −ravg
)
, i ∈[1, m]
(4)
Where 
ravg = 1
m
m∑
i=1
r(X, Yi), 
rmax = max{r(X, Yi), i = 1, 2, … , m},
rmin = min{r(X, Yi), i = 1, 2, … , m}, 𝜆 is a constant, and 0 ≤𝜆≤1.
3.4
Mutation Operator
The mutation operator is an auxiliary method, and it has an eﬀect on the local search
ability of genetic algorithm. Therefore, the algorithm perform a mutation on a small
fraction of the chromosomes. The Formula 5 is used to calculate the mutation probability
of each chromosome. We sorted the individuals from small to large by mutation prob‐
ability. The ﬁrst 15% individuals are selected to mutation, thus is, the mutation proba‐
bility set to 15%.
pm =
⎧
⎪
⎨
⎪⎩
pm1, f < favg
pm1 −(pm1 −pm2)k1(fmax −f)
fmax −favg
, f ≥favg
(5)
Research on the Shortest Path Problem Based on Improved Genetic Algorithm
179

Where pm1 = 0.1, pm2 = 0.001, k1 is a constant, and 0 < k1 < 1, fmax is the largest
ﬁtness value in the population, favg is the average ﬁtness value of population, f is the new
ﬁtness value of the chromosome.
4
Experimental Analysis
4.1
The Inﬂuence of the Crossover Operator
In order to verify the inﬂuence of the crossover operator, the improved genetic algorithm
using the new crossover operator is compared with the traditional genetic algorithm.
The results in Table 3 are the average of 100 tests. According to Table 3, it can be seen
that the new crossover operator can improve the accuracy of the genetic algorithm, and
can eﬀectively avoid the algorithm into the local optimal solution.
Table 3. Performance comparison of improved GA and traditional GA
Algorithm
Average convergence generations
Optimal solution ratio
Traditional GA 78
0.61
Improved GA
42
0.94
4.2
The Modiﬁed Genetic Algorithm Analysis
In order to verify the validity of the algorithm, the Dijstra algorithm is compared with
the improved genetic algorithm. In Table 5, the optimal solution and the suboptimal
solution represent the average of the success cases, and the time represents the average
time of the cases. The genetic algorithm parameter settings: population size of about
1000 which slightly ﬂuctuate for speciﬁc cases, mutation probability is 15%.
As can be seen from Tables 4 and 5, the genetic algorithm has a better success rate
than Dijkstra algorithm, and is more eﬀective under large-scale data. Although the
probability of genetic algorithm to get the optimal solution is 100% in theory. Because
it is diﬃcult to meet the individual diversity in the population in practical application,
the success rate cannot reach 100%.
Table 4. Dijkstra algorithm simulation results
Nodes
Number of designated
points
Number of success/number
of experiments
Traversal completion time
20
7
4/5
<1 ms
50
10
4/5
3 ms
100
15
4/6
31 ms
200
15
8/10
90 ms
300
20
7/10
320 ms
180
B. Wang et al.

Table 5. Genetic algorithm simulation results
Nodes
Designated
points
Success/all
Suboptimal solution/
optimal solution
Suboptimal solution time/
optimal solution time
20
7
5/5
1/2
11 ms/11 ms
50
10
5/5
1/3
52 ms/63 ms
100
15
6/6
3/23
325 ms/3 s
200
15
9/10
6/41
325 ms/36 s
300
20
9/10
21/83
42 s/203 s
500
30
8/10
56/127
167 s/361 s
From the experimental results, we can see that the genetic algorithm has a high
computational cost compared with the traditional algorithm. The improved genetic
algorithm is much more expensive than Dijkstra algorithm, especially the optimal solu‐
tion cost. For the above-mentioned problem, we can adopt pruning strategy. First, we
cut the size of the graph and reduce the size of the graph. And then, we can use the
genetic algorithm mentioned in this paper in the results of the pruning. So that we can
reduce the computational cost in the case of guaranteed algorithm search capabilities.
5
Concluding
The shortest path problem is an active research area. In this paper, we modify the ﬁtness
function of genetic algorithm, and then design a new selection method for crossover
probability and a new selection method for mutation probability. These improvements
make the genetic algorithm have a good success rate. At the same time, the algorithm
proposed in this paper can easily combine with the pruning algorithm. So that we can
eﬀectively reduce the computational complexity and guarantee the search ability of the
shortest path problem. However, this paper only considers the constraint condition of
designated points, the algorithm also has room for improvement.
References
1. Ko, Y.D., Jang, Y.J., Min, S.L.: The optimal economic design of the wireless powered
intelligent transportation system using genetic algorithm considering nonlinear cost function.
Comput. Ind. Eng. 89(C), 67–79 (2015)
2. Chitra, C., Subbaraj, P.: A nondominated sorting genetic algorithm solution for shortest path
routing problem in computer networks. Expert Syst. Appl. 39(1), 1518–1525 (2012)
3. Cheng, H.: Obstacle avoidance shortest path algorithm and its application. Electron. Des.
Eng. 16, 56–60 (2013). (in Chinese)
4. Qi, X., Liu, S.: Selection algorithm for QoS routing based on k-shortest paths. J. Jilin Univ.
(Eng. Technol. Ed.) 05, 526–530 (2005). (in Chinese)
5. Dijkstra, E.W.: A note on two problems in connexion with graphs. Numer. Math. 1(1), 269–
271 (1959)
6. Bellman, R.: On a routing problem. Q. Appl. Math. 16, 87–90 (1958)
7. Lin, L., Gen, M.: Priority-based genetic algorithm for shortest path routing problem in OSPF.
Stud. Comput. Intell. 187, 91–103 (2009)
Research on the Shortest Path Problem Based on Improved Genetic Algorithm
181

8. Holland, J.H.: Adaptation in Natural Artiﬁcial Systems, pp. 1–17. MIT Press, Cambridge
(1975)
9. Cao, Z.: The study on exhaust algorithm, search algorithm, dynamic design for 0–1 Knapsack
problem. Comput. Knowl. Technol. 5(12), 3193–3198 (2009). (in Chinese)
10. Srinivas, M., Patnaik, L.: Adaptive probabilities of crossover and mutation in genetic
algorithm. IEEE Trans. Syst. Man Cybern. 24(4), 656–666 (1994)
182
B. Wang et al.

Semantic Web Languages for Policy
Enforcement in the Internet of Things
Rustem Dautov1,2(B) and Salvatore Distefano1,3
1 Higher Institute of Information Technology and Information Systems (ITIS),
Kazan Federal University (KFU), Kazan, Russia
{rdautov,s distefano}@it.kfu.ru
2 South-East European Research Centre (SEERC), The University of Sheﬃeld,
International Faculty CITY College, Thessaloniki, Greece
3 University of Messina, Messina, Italy
sdistefano@unime.it
Abstract. To enable device compatibility, interoperability and integra-
tion in the Internet of Things (IoT), several ontological frameworks have
been developed, using the Semantic Web technologies – a common and
widely-adopted toolkit for addressing the heterogeneity issues in com-
plex IT systems. These ontologies aim to provide a common vocabulary
of terms to be universally adopted by the IoT community. Deﬁned using
the Web Ontology Language – a language underpinned by the Descrip-
tion Logics – these vocabularies, however, seem to neglect the automated
reasoning support, which comes along with this semantic approach to
model IoT environments. To bridge this gap, this paper builds upon the
existing work in the area of semantic modelling for the IoT, and proposes
utilising IoT ontologies to deﬁne and enforce policies, thus beneﬁting
from the built-in support for automated reasoning.
Keywords: Internet of Things · Semantic Web · Policy management
Web Ontology Language · Semantic Web Rule Language · Reasoning
1
Introduction
While challenges associated with timely processing of sensor data have been
relatively successfully tackled by the advances in networking and hardware tech-
nologies, the challenge of properly handling data representation and semantics of
IoT descriptions and sensor observations is still pressing. In the presence of mul-
tiple organisations for standardisation, as well as various hardware and software
vendors, overcoming the resulting heterogeneity remains one of the major con-
cerns for the IoT community. Moreover, apart from the syntactic heterogeneity
(i.e. heterogeneity in the data representation, such as, for example, diﬀerences in
data formats/encodings), we can also distinguish heterogeneity in the semantics
of the data [2]. For example, diﬀerent units of measurement and metric systems
are common causes for incompatibility and integrity problems.
c
⃝Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 183–188, 2018.
https://doi.org/10.1007/978-3-319-74521-3_21

184
R. Dautov and S. Distefano
As a potential workaround to the IoT heterogeneity problem and yet-to-
come standards, the research community started investigating potential ways
of creating semantic modelling languages, which would provide an overarching
modelling framework and bridge formerly-disjoint heterogeneous IoT systems.
Such modelling languages can be seen as common vocabularies of terms, which
are expected to be used by IoT practitioners to enable compatibility and interop-
erability when integrating IoT solutions. Simply put, two disjoint system would
be able to ‘communicate’ to each other by expressing their data and interfaces,
using a common suitable modelling language. A representative example of how
this lack of uniﬁed data representation has been addressed in the context of
the IoT is the Semantic Sensor Web – a promising combination of two research
areas, the Semantic Web and the Sensor Web [6]. Using the Semantic Web
technology stack to represent data in a uniform and homogeneous manner, it
provides enhanced meaning for sensor descriptions and observations so as to
facilitate data analysis and situation awareness [3]. One of the main outcomes
of this initiative was the Semantic Sensor Network (SSN) ontology – a thorough
modelling vocabulary, jointly developed by a wide group of researchers.
From this perspective, however, Semantic Web ontologies do not diﬀer much
from other modelling approaches (e.g. UML or XML) that provide a taxonomy of
terms and relationships, to be used as ‘building blocks’ when describing the IoT
domain. A major advantage of the Semantic Web languages, which is frequently
neglected in this context, is the support for automated formal analysis, under-
pinned by the built-in reasoning capabilities of the Web Ontology Language
(OWL), which is one of the key enabling technology for the Semantic Web [4].
By representing data in terms of OWL classes and properties, one can perform
reasoning tasks over this data and beneﬁt from an already existing, highly-
optimised and reliable analysis mechanism. In this light, this paper is trying to
tap into this idle potential for automated reasoning, and presents an approach
to policy management and enforcement in the IoT context, using existing IoT
ontologies and corresponding reasoning support. As it will be further described
below, the proposed approach is expected to beneﬁt from separation of concerns,
extensibility, human readability, as well as increased reliability, automation, and
opportunities for reuse.
2
Background: Semantic Web Languages
The Semantic Web [1] is the extension of the World Wide Web that enables peo-
ple to share content beyond the boundaries of applications and websites. This
is typically achieved through the inclusion of semantic content in web pages,
which thereby converts the existing Web, dominated by unstructured and semi-
structured documents, into a web of meaningful machine-readable information.
The Semantic Web is realised through the combination of certain key technolo-
gies [4], whereas the presented research speciﬁcally focuses on the Web Ontology
Language (OWL) and the Semantic Web Rule Languages (SWRL) as the two
potential ways of deﬁning and enforcing policies in the context of the IoT.

Semantic Web Languages for Policy Enforcement in the Internet of Things
185
OWL is a family of knowledge representation languages used to formally
deﬁne an ontology – “a formal, explicit speciﬁcation of a shared conceptuali-
sation” [7]. Typically, an ontology is seen as a combination of a terminology
component (i.e. TBox) and an assertion component (i.e. ABox), which are used
to describe two diﬀerent types of statements in ontologies. The TBox contains
deﬁnitions of classes and properties, whereas the ABox contains deﬁnitions of
instances of those classes. Together, the TBox and the ABox constitute the
knowledge base of an ontology. OWL provides advanced constructs to describe
resources on the Semantic Web. This way, it is possible to explicitly and formally
deﬁne knowledge (i.e. concepts, relations, properties, instances, etc.) and basic
rules in order to reason about this knowledge. OWL allows stating additional
constraints, such as cardinality, restrictions of values, or characteristics of prop-
erties such as transitivity. OWL languages are characterised by formal semantics
– they are based on Description Logics (DLs) and thus bring reasoning power
to the Semantic Web. SWRL extends OWL with even more expressiveness, as
it allows deﬁning rules in the form of implication between an antecedent (body)
and consequent (head). It means that whenever the conditions speciﬁed in the
body of a rule hold, then the conditions speciﬁed in the head must also hold.
To date, a number of IoT ontologies have been developed using OWL. More
speciﬁcally, IoT-Lite ontology1 is a lightweight instantiation of the SSN ontol-
ogy, actively developed by the World Wide Web Consortium. It describes the key
IoT concepts to allow interoperability and discovery of sensory data in heteroge-
neous IoT platforms. This ontology reduces the complexity of other IoT models
by describing only the main concepts of the IoT domain. This means that fol-
lowing the Semantic Web principles of linking and reusing existing ontologies
and datasets, it is possible to extend the core vocabulary with other relevant
concepts, deﬁned in other ontologies if/when needed. This way, ontology engi-
neers can simply import an existing, established, and trusted ontology, instead
of ‘re-inventing the wheel’.
3
Sample Scenario
The following use case scenario demonstrates the proposed idea of leveraging
the idle potential of OWL ontologies and focuses on a complex IoT system com-
posed of multiple sensing devices, deployed both indoors and outdoors. Some
of these devices are temperature sensors installed in rooms within a building.
It is assumed that whenever any of these temperature sensors indicates a value
exceeding a dangerous level of 50◦, the situation has to be classiﬁed as critical,
and thus needs taking reactive actions. A possible way to handle this scenario
would be to deﬁne explicit policies for every single temperature sensor within
the building. In the worst case, such policies would be either ‘hard-coded’ with
numerous if/then operators (i.e. any modiﬁcations would lead to the source
code recompilation), or deﬁned declaratively (i.e. stored in some kind of con-
ﬁguration ﬁle to be dynamically fetched by the analysis component). In both
cases, however, the resulting knowledge base would be saturated by the excessive
1 https://www.w3.org/Submission/iot-lite/.

186
R. Dautov and S. Distefano
number of hardly manageable and possibly conﬂicting policies. An alternative
solution is based on using the reasoning capabilities of OWL and SWRL to clas-
sify observed IoT data as instances of speciﬁc classes. More speciﬁcally, with this
use case we demonstrate three diﬀerent types of automated classiﬁcation:2
— Deﬁned classes: underpinned by the DLs, OWL allows creating so-called
deﬁned classes via a set of necessary and suﬃcient conditions. This means that
the reasoner will classify any entity with a required set of suﬃcient properties
as an instance of a speciﬁc class, even if this class membership was not deﬁned
explicitly. The deﬁned class if RoomDevice is demonstrated in Listing 1.1, which
should be read as “if sd is a sensing device and has a rectangular coverage area,
then sd is a device installed in a room”.
Listing 1.1. Deﬁned OWL class RoomDevice.
ssn:SensingDevice(?sd) AND iot:Rectangle(?r)
AND iot:hasCoverage(?d,?r) ≡
iot:RoomDevice(?sd)
— OWL subclasses: OWL also provides a simpler and more explicit way of
deﬁning classes and subclass relationships. It supports multiple and transitive
inheritance, and, as in many other programming languages, subclasses inherit
all the properties of their parent classes. The code snippet in Listing 1.2 contains
two deﬁnitions. The ﬁrst deﬁnition simply states that any temperature sensor
is a sensing device. The second deﬁnition illustrates the transitive inheritance
through a subclass hierarchy that states – in simple words – that if a device is
installed in a room, then it is automatically assumed to be installed in a building
as well, which in turn means it is an indoor device.
Listing 1.2. Deﬁning OWL subclass relationships.
iot:TemperatureSensor IS A ssn:SensingDevice
iot:RoomDevice IS A iot:BuildingDevice IS A iot:IndoorDevice
— SWRL classiﬁcation: SWRL allows deﬁning more expressive rules, as illus-
trated by Listing 1.3. In simple words, the code snippet reads that if there is
an indoor device id, indicating that its measured value has exceeded 50◦, the
current observation has to be classiﬁed as critical.
Listing 1.3. Deﬁning the class CriticalObservation using SWRL.
iot:IndoorDevice(?id) AND ssn:Observation(?o) AND dul:Value(?v)
AND iot:observes(?id,?o) AND ssn:hasValue(?o, ?v)
AND swrl:greaterThan(?v, 50) THEN
iot:CriticalObservation(?o)
2 The notations ssn, iot, and dul are established shortcuts for imported OWL ontolo-
gies, where corresponding concepts are deﬁned.
SSN ontology (ssn): http://purl.oclc.org/NET/ssnx/ssn.
IoT-Lite ontology (iot): http://purl.oclc.org/NET/UNIS/ﬁware/iot-lite DOLCE
Ontology (dul): http://loa.istc.cnr.it/ontologies/DOLCE-Lite.owl.

Semantic Web Languages for Policy Enforcement in the Internet of Things
187
Taking together all three deﬁnitions, we now assume that there is a temperature
sensor ts reporting a temperature level of 60◦in its covered rectangular area.
The automated reasoner will follow the steps below:
1. Since TemperatureSensor is a subclass of SensingDevice, ts is classiﬁed as
a SensingDevice (Listing 1.2).
2. Since ts is a SensingDevice and has a rectangular coverage area, it is clas-
siﬁed as a RoomDevice (Listing 1.1).
3. Since RoomDevice is a subclass of BuildingDevice, which is a subclass of
IndoorDevice, ts is classiﬁed as an instance of IndoorDevice (Listing 1.2).
4. Since the measured observation of an IndoorDevice ts is greater than 50◦,
this observation is classiﬁed as CriticalObservation (Listing 1.3).
This way, the reasoning engine is able to detect a critical situation by inferring
implicit facts from the limited explicit information. It is worth noting that using
generic rules for a wide range of devices, as in the example above, does not
aﬀect the ﬂexibility of the proposed approach and its ability to deﬁne ﬁne-
grained, targeted policies for individual devices. Apart from inheritance, OWL
also supports overwriting parent properties by subclasses. This means that it
is possible to enforce device-speciﬁc policies, which will overwrite the default
behaviour and apply only to those speciﬁc devices. This way, ﬂexible policy
enforcement at various granularity levels can be achieved.
4
Discussion and Conclusion
The presented approach discusses the idle potential of existing IoT ontologies
to be used in the context of policy enforcement mechanisms. Semantic Web is
underpinned by Description Logics, which oﬀer automated reasoning support.
This means that IoT engineers can use existing ontological classes and properties
to deﬁne policies, and, as a result, beneﬁt from the built-in policy enforcement
mechanisms. More speciﬁcally, the following beneﬁts can be identiﬁed [2]:
— Separation of concerns: desirably, a policy enforcement mechanism is expected
to (i) separate the knowledge base and policies from the actual enforcement of
these policies, and (ii) allow the deﬁnition of the knowledge base in a declarative,
loosely-coupled manner. The presented approach addresses these requirements
and enables policy modiﬁcations ‘on the ﬂy’ in a seamless, transparent manner
– that is, without recompiling, redeploying and restarting the whole software
system. In particular, to apply changes, it is only required to add corresponding
OWL/SWRL rules to the policy base. As opposed to the existing (potentially
hard-coded) approaches, the declarative approach is seen as a considerable ben-
eﬁt, which enables minimum interruptions caused by potential modiﬁcations.
— Human readability and ease of use: the Semantic Web research targets at
making information on the Web to be both human- and machine-readable, with
languages which are characterised by an easy-to-understand syntax, as well as
the visual editors for eﬀortless and straight-forward knowledge engineering. OWL

188
R. Dautov and S. Distefano
ontologies are known to be used in a wide range of scientiﬁc domains (for exam-
ple, see [5] for an overview of biomedical ontologies), which are not necessarily
closely connected to computer science, and allows even for non-professional pro-
grammers (i.e. domain specialists) to design policies.
— Extensibility: IoT systems may be composed of an extreme number of smart
devices spread over a large area (e.g. traﬃc sensors distributed across a city-
wide road network) and have the capacity to be easily extended (as modern
cities continue to grow in size, more and more sensors are being deployed to
support their associated traﬃc surveillance requirements). To address this scal-
ability issue, the proposed semantic approach, suing the declarative deﬁnition,
can extend the knowledge base to integrate newly-added devices in a seamless,
transparent, and non-blocking manner. The same applies to the reverse process
– once old services are retired and do not need to be considered anymore, the
corresponding policies can be seamlessly removed from the knowledge base, so
as not to overload the reasoning processes.
— Increase in reuse, automation and reliability: policy enforcement mechanisms
already exist in the form of automated reasoners for OWL/SWRL languages,
and the proposed approach aims to build on these capabilities. Since the reason-
ing process is automated and performed by an existing reasoning engine, it is
expected to be free from so-called ‘human factors’ and more reliable, assuming,
of course, the validity of ontologies and policies. Arguably, as the policy base
grows in size and complexity, its accurate and prompt maintenance becomes a
pressing concern so as to avoid potential policy conﬂicts.
References
1. Berners-Lee, T., Hendler, J., Lassila, O., et al.: The semantic web. Sci. Am. 284(5),
28–37 (2001)
2. Dautov, R., Kourtesis, D., Paraskakis, I., Stannett, M.: Addressing self-management
in cloud platforms: a semantic sensor web approach. In: Proceedings of the 2013
International Workshop on Hot topics in Cloud Services, pp. 11–18. ACM (2013)
3. Dautov, R., Paraskakis, I., Stannett, M.: Towards a framework for monitoring cloud
application platforms as sensor networks. Cluster Comput. 17(4), 1203–1213 (2014)
4. Hitzler, P., Kr¨otzsch, M., Rudolph, S.: Foundations of Semantic Web Technologies.
Chapman & Hall/CRC, Boca Raton (2009)
5. Rubin, D.L., Shah, N.H., Noy, N.F.: Biomedical ontologies: a functional perspective.
Brieﬁngs Bioinf. 9(1), 75–90 (2008)
6. Sheth, A., Henson, C., Sahoo, S.S.: Semantic sensor web. IEEE Internet Comput.
12(4), 78–83 (2008)
7. Studer, R., Benjamins, V.R., Fensel, D.: Knowledge engineering: Principles and
methods. Data Knowl. Eng. 25(1–2), 161–197 (1998)

Network Traﬃc Prediction Based on Wavelet Transform
and Genetic Algorithm
Xuehui Zhao, Wanbo Zheng
(✉), Lei Ding, and Xingang Zhang
College of Computer Science and Technology, Jilin University, Changchun, China
442014097@qq.com, zwb@jlu.edu.cn
Abstract. The traditional network traﬃc prediction is based on the establishment
of a linear model, which can’t describe the changes of network traﬃc accurately,
resulting in low prediction accuracy. This paper proposes a new model of network
traﬃc prediction based on wavelet transform and Genetic Algorithm. Firstly, after
a wavelet decomposition, network traﬃc is turned into many stable components.
Secondly, using BP neural network to predict each stable component, and opti‐
mizing neural network by genetic algorithm. Finally, all the prediction of compo‐
nents is combined to achieve highly-accurate traﬃc prediction. The experimental
results show that the model has a better predictive eﬀect.
Keywords: BP neural network · Wavelet transform · Network traﬃc prediction
Time series · Genetic algorithms
1
Introduction
Network traﬃc data is a kind of time-series data. The analysis of traﬃc data can help
people know the running status of the network, which can do it more reasonably in
bandwidth allocation, control ﬂow, routing control, and error control [1]. Thus, it is an
eﬀective way to improve QoS (Quality of Service).
Today’s network traﬃc presents some characteristics as nonlinear, abrupt and non-
stationary. The ﬁrst-generation network traﬃc prediction models, such as Markov
model, Poisson model, AR model and ARMA model, can’t describe the non-stationary
characteristics of traﬃc. Meanwhile the prediction accuracy is too low to be suitable for
the current network traﬃc prediction [1].
Later, a second-generation traﬃc prediction model, such as FARIMA, gray model,
neural network model, wavelet model, and support vector machine (SVM), which can
capture long-run and non-linear ﬂow characteristics [2]. But there are still some short‐
comings, for instance, FARIMA can’t describe the non-stationary characteristics of
traﬃc; neural network model requires more training samples, and the algorithm is
complex; gray model only applies in the occasions which original sequence changes not
fast, according to the exponential law.
In order to overcome the limitation of single model and describe the characteristics
of network data more accurately, some hybrid models have been proposed, such as gray
neural network model [3], wavelet combined with neural network model [4], wavelet
combined with time series model [5], and more complex combination of wavelet, neural
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 189–200, 2018.
https://doi.org/10.1007/978-3-319-74521-3_22

network and time series model [6] etc. In some cases, the complex combination of model
methods may not reduce the forecast error of component. Although the method improves
the accuracy of prediction, the complexity will increase at the same time. This model is
suitable for occasions that require higher prediction accuracy and less time.
In this paper, a traﬃc prediction model combining wavelet transform and neural
network is proposed and optimized by genetic algorithm. Firstly, process the traﬃc data
by the wavelet decomposition, then the time series data will be decomposed into rela‐
tively simple components, for smoothing the original signal. Secondly, the BP neural
network is used to predict the high frequency components and low frequency compo‐
nents respectively. Considering the shortcomings of slow convergence and local opti‐
mization, the genetic algorithm with good global search ability is used to optimize the
weights and thresholds. Finally, the component is reconstructed to obtain the ﬁnal
prediction result. Experiments show that wavelet decomposition can eﬀectively increase
the signal stability, and BP neural network for non-linear changes in traﬃc has a better
prediction eﬀect. Meanwhile, the introduction of genetic algorithms accelerates the BP
neural network convergence rate, and improve the prediction accuracy.
2
Technical Overview
2.1
Wavelet Decomposition and Reconstruction
Wavelet decomposition is proposed by Meyer and Mallat. Time series data are decom‐
posed into two parts: low frequency and high frequency coeﬃcient.
The decomposition of time series is achieved by Mallat algorithm, and the decom‐
position relationship is as follows:
{ aj + 1 = h0 ∗aj
dj + 1 = h1 ∗dj
j = 0, 1, …
(1)
In the formula, h0 represents a low-pass decomposition ﬁlter; h1 represents a high-
pass analysis ﬁlter; * represents a convolution operator; aj represents the low-frequency
coeﬃcients; dj represents frequency coeﬃcients. When j = 0, the original time series a0
through h0 and h1, after several decomposition, can be decomposed into the low
frequency and high frequency coeﬃcients of the original time series.
The wavelet reconstruction relations are as follows:
{ Aj = g0 ∗aj
Dj = g1 ∗dj
j = 0, 1, …
(2)
In the formula, g0 and g1 represents a low-pass reconstruction ﬁlter and a high-pass
reconstruction ﬁlter respectively; Aj represents a low-frequency component; Dj repre‐
sents a high-frequency component.
Thus, the relation between the original time series S and Aj, Dj is as follows:
S = Aj + Dj
(3)
190
X. Zhao et al.

2.2
BP Neural Network
BP neural network is a hierarchical feed-forward network, and the training algorithm is
back-propagation algorithm (referred to BP algorithm). This is a supervised learning
method, and the basic idea is the least-squares algorithm. Using root mean square error
(referred to RMSE) and gradient descent method to correct the network connection
weight. The purpose is to minimize the RMSE between the actual and the speciﬁed
output [9]. The principle is shown in Fig. 1.
Neur
ons
Back-propagation (learning algorithms)
—
+
Expected Output 
Vector 
(Instructor 
Signal)
Input layer
Hidden layer
Output layer
Signal ﬂow
Fig. 1. BP neural network
The number of hidden layer neurons can be determined by the formula
m =
√
l + n + a, and a is an integer between [1, 10].
2.3
Genetic Algorithm
The genetic algorithm (referred to GA) treats possible solutions in the problem space as
chromosome individuals in the population, and encodes each individual into a symbol
string. According to the ﬁtness function, the value is calculated and then evolved from
generation to generation. After simulating biological evolution, selecting, crossover,
mutation and so on, the optimal solution [5] is obtained.
This paper discusses the weight optimization, including the following steps:
1. population initialization
Individuals are encoded with real numbers, and their encoding lengths are the sum
of the number of network ownership values and the number of thresholds:
S = l × m + m × n + m + n
(4)
In the formula, S is the code length, and l, m, n are the number of neurons in the
input layer, hidden layer, and output layer respectively.
2. ﬁtness function
F = k(
n
∑
i=1
abs(yi −oi))
(5)
Network Traﬃc Prediction Based on Wavelet Transform
191

In the formula, n is the number of output node in the network output layer, k is the
coeﬃcient used to adjust the range of ﬁtness values, yi is the expected output of the ith
node of BP neural network, oi is the ith node prediction output.
3. selecting
We use the roulette method based on individual ﬁtness selection strategy.
Pi =
k
Fi
n∑
j=1
k
Fi
(6)
In the formula, Pi is the selection probability of each individual, k is coeﬃcient, n is
the number of individuals, Fi is the ﬁtness value of individual i.
4. crossover
In this paper, the real cross method, that is, the kth chromosome ak and the lth chro‐
mosome al in the j-bit cross operation method is as follows:
{ akj = akj(1 −b) + aljb
alj = alj(1 −b) + akjb
(7)
In the formula, b is a random number between [0,1].
5. mutation
aij =
{ aij + (aij −amax) × f(g) r ≥0.5
aij + (amin −aij) × f(g) r < 0.5
f(g) = r2(1 −g∕Gmax)
(8)
In the formula, aij is to the jth gene in the ith individual, amax is the upper bound of
the gene aij, amin is the lower bound of the gene aij, r2 is a random number, g is the current
number of iterations, Gmax is the maximum evolution number, r is a random number
between [0,1].
3
Design of Traﬃc Prediction Model Based on Wavelet
Decomposition and Genetic Algorithm
3.1
Model Ideas
The network traﬃc data is self-similar (single fractal) on a large time scale, while multi‐
fractals appears on a smaller scale [10]. The model in this paper ﬁrstly processes the
ﬂow data by wavelet decomposition, and the prediction model is composed of BP neural
network, as shown in Fig. 2. Genetic algorithm is introduced to optimize the weights
and thresholds of neural networks [11].
192
X. Zhao et al.

Raw 
traffic 
data
Wavelet 
decomposition
BP1Neural 
Networks
     BP2~6Neural 
Networks
After refactoring ,
A5
After refactoring ,
D1~D5
Genetic algorithm
opƟmizaƟon
output
opƟmizaƟon
Fig. 2. Schematic diagram of network traﬃc prediction model
To optimize the BP neural network by genetic algorithm, the speciﬁc process is
shown in Fig. 3:
Determine 
network 
topology 
structure
Initialize the 
weights .thres
hold length
of BP neural 
network 
Encoding the 
initial value 
by GA
Calculate the 
fitness value 
(BP neural 
network 
training error)
Selection, 
crossover and 
mutation
Calculate 
fitness value
Satisfies the 
fitness 
condition
N
Get the 
optimal 
weight 
threshold
Y
Calculation 
error, update 
weight and 
threshold
Satisfy the end 
conditions
N
Prediction 
Results
Y
Input data
Fig. 3. Flow of GA optimizing BP neural network
3.2
Model Steps
Step 1: Wavelet decomposition. The wavelet transform is performed on the input traﬃc
data by the formula (1) Mallet algorithm. By using the Wavelet Toolbox in
MATLAB, the decomposition level is L, the low-frequency and high-frequency
components with stationary features at time k are obtained, as {D1(k), D2(k)…
DL(k), AL(k)}. The original signal S = D1 + D2 + D3 +…DL + AL;
Step 2: Initialize BP1, BP2, BP3…BP(L + 1) neural network. They have three layers:
input layer has 24 neurons, single hidden layer, output layer has 12 neurons;
Step 3: Preprocessing AL(k), D1(k), D2(k)…DL(k), and the samples of each neural
network are constructed for training;
Step 4: Training neural network BP1 optimized by genetic algorithm. AL(k) is the input
of BP1 neural network, AL(k + T) at k + T time is the expected value of the
network. The training process is shown in Fig. 2;
Step 5: Training BP2, BP3…BP(L + 1) optimized by genetic algorithm. D1(k) is the
input of BP2 neural network, D1(k + T) is the expected value at time k + T,
D2(k) is the input of BP3, D2(k + T) is the expected value at time k + T, and
so on.
Step 6: Input the test data ﬂow, combined L + 1 component (high-frequency and low-
frequency components) obtained from neural network prediction, and
Network Traﬃc Prediction Based on Wavelet Transform
193

comparing the actual results. Among them, the error of component prediction
is not only positive but negative. After combination, part of the error can be
oﬀset.
4
Network Traﬃc Prediction Model Implementation
4.1
Wavelet Decomposition
In this paper, network traﬃc data comes from the CERNET backbone network center
in Northeast China, which is oﬀered by China Education and Research Network. Using
the Shenyang-Changchun 30 days of data (from April 31, 2016 to May 29, 2016), as
shown in Fig. 4. Network traﬃc is in units of GB, and selecting 2 h as the acquisition
time granularity, that is, 12 points a day, 30 days a total of 360 points.
Fig. 4. Graph of real traﬃc data
For the data ﬂow in Fig. 4, wavelet transform is performed using the formula (1) (2).
Selecting db4 as Wavelet base, decomposition scale L = 5, the high-frequency and low-
frequency component are {D1 (k), D2 (k), D3 (k), D4 (k), D5 (k), A5 (k)} and the original
signal S = D1 + D2 + D3 + D4 + D5 + A5, as the results shown in Fig. 5.
194
X. Zhao et al.

Fig. 5. Data signal after wavelet transform
4.2
Preprocessing of Traﬃc Data Sequence
In order to speed up the convergence rate of neural network and improve the prediction
accuracy, the normalized network ﬂow data after wavelet decomposition is processed
[12]. The formula is:
X
′ =
[
X −Xmin
Xmax −Xmin
]
(9)
The normalized data is within the range of [0,1], Xmax and Xmin are the maximum
and minimum values respectively in the network traﬃc data set, X’ is the normalized
value, X is the original value of the variable.
4.3
The Sample Construction of Neural Network
Xi,j represents the network traﬃc on the ith day at j time, Xi represents the network traﬃc
on the ith day. The network traﬃc data is X = {(X1,1, X1,2, …, X1,n), (X2,1, X2,2, …, X2,n),
…, (Xm,1, Xm,2,…, Xm,n), …}; The speciﬁed k learning samples is P = ((X1, X2,…, Xj);
(X2, X3,…, Xj+1);…; (Xk, Xk+1,…, Xk+j-1)); The corresponding k teaching samples
T = (Xj+1; Xj+2;…; Xj+k). The purpose of learning is to correct the weight, using the
error between the neural network output corresponding to the k learning samples P1, P2,
…, Pk and the corresponding teacher samples T1, T2,…, Tk, so that the output is close
Network Traﬃc Prediction Based on Wavelet Transform
195

to the expected teacher sample. Xi+j is a seasonal time series with j as the cycle, Ti is the
teacher sample, the relationship between the two is Ti = Xi+j.
In this paper, m = 30, n = 12 and n = 2, the input layer of network training uses 24
neurons, the ﬂow of 2d is mapped respectively; the output layer uses 12 neurons,
mapping the future 1d ﬂow.
4.4
Training Neural Networks
After several debugging experiments, BP1 neural network is determined as a
24 × 12 × 12 three-layer structure, and the excitation functions of input layer, hidden
layer and output layer are purelin; BP2 is a 24 × 16 × 12 three-layer structure, and the
excitation functions are tansig, tansig, purelin; BP3, BP4, BP5 is a 24 × 10 × 12 three-
layer structure, and the excitation functions are tansig, tansig, purelin; BP6 neural
network is a 24 × 16 × 12 three-layer structure, and excitation function, training function
is the same as BP3. The training functions of these neural networks are trainlm, the
maximum training times are 2000, the neural network target error is 1e−4, the learning
rate is set to 0.3, and the rest is the default value.
In this paper, GAOT is used to program the genetic algorithm. The ﬂow of the algo‐
rithm is shown in Fig. 3, and the objective function is performed according to the ﬁtness
function, the formula (5). Population size is set to 50, genetic algebra is 100, real coding,
mutation is the probability of 0.09, and the rest is the default value.
5
Traﬃc Prediction and Analysis of Results
The data of the ﬁrst 20d(days) were selected for training, and the data of the last 10d(the
last 120 points) were used for testing. The ﬂow time series of 30d is:
X =
{
t1 =
(
t1,1, t1,2, … , t1,12
)
;t2 =
(
t2,1, t2,2, … , t2,12
)
; … ;t30 =
(
t30,1, t30,2, … , t30,12
)}
;
The learning samples: P = {(t1, t2);(t2, t3);…;(t18, t19)}, and the corresponding
teaching samples: T = {t3, t4,…, t20}. The network is predicted after learning, using t19
(network traﬃc on day 19) and t20 (network traﬃc on day 20) to predict traﬃc on day
21, and using t20 and t21 to predict traﬃc on day 22, and so on.
Figures 6 and 7 show the results of the short-term prediction of the last 120 data 1
day in advance. Figure 6 is a comparison of the new model without genetic algorithm
(GA) optimization, and Fig. 7 is the comparison chart of new model after GA optimi‐
zation. The abscissa is the detection point and the ordinate is network traﬃc (GB).
196
X. Zhao et al.

Fig. 6. Unoptimized ﬁtting ﬁgure of 1d short-term prediction
Fig. 7. GA optimized ﬁtting ﬁgure of 1d short-term prediction
The prediction results of Fig. 6 shows that the new model has a higher prediction
accuracy and a better degree of ﬁt. Figure 7 shows that the new model is satisfactory.
To clarify the performance of the new model, the results of the model performance
parameters using GA or not are listed in Table 1.
Table 1. Parameters of model performance
Model
MSE
MAE
RMSE
R-square
New model without GA (1d)
3.3507
1.2670
1.8305
0.9027
New model with GA (1d)
2.6736
1.2583
1.6351
0.9680
Network Traﬃc Prediction Based on Wavelet Transform
197

MSE is the mean square error, MAE is the absolute error, RMSE is the root mean
square error, and R-square is the determination coeﬃcient.
From Table 1, we can see that the MSE of the new model is reduced by 20.21%,
comparing the new model without the optimization, and the prediction accuracy is
improved. Meanwhile, the coeﬃcient of determination of the new model is increased
by 7.23%, which improves the ﬁtting degree. This indicates that genetic algorithm is
feasible and eﬀective to optimize the new model.
Although the genetic algorithm needs time to run, it speeds up the neural network
convergence rate, and improve the prediction accuracy. The comparison of training
times and running time are shown in Table 2.
Table 2. Compare training times and running time
The name of
neural network
New model without GA optimization New model with GA optimization
Training times
Running time(s)
Training times
Running time(s)
BP1
45
22.37
35
20.56
BP2
10
9.57
5
16.97
BP3
13
9.13
10
10.22
BP4
10
8.32
8
14.17
BP5
22
15.13
10
12.71
BP6
12
7.37
6
10.56
The above results prove the validity of using genetic algorithm. The new model can
predict the traﬃc of t21 and t22 according to t19, t20 and so on. The prediction results are
shown in Fig. 8.
Fig. 8. GA optimized ﬁtting ﬁgure of 2d short-term prediction
Using new model with GA optimization to realize the short-term forecast 3 days in
advance, as shown in Fig. 9.
198
X. Zhao et al.

Fig. 9. GA optimized ﬁtting ﬁgure of 3d short-term prediction
The parameters of the new model performance with GA optimization are listed,
which is 2d and 3d predicted in advance, as shown in Table 3.
Table 3. Parameters of model performance
Model
MSE
MAE
RMSE
R-square
New model with GA (2d)
3.8539
1.5591
1.9631
0.8476
New model with GA (3d)
4.7562
1.6605
2.1809
0.8119
From Tables 2 and 3, we can see that the new model has higher accuracy and better
ﬁtting degree in short-term prediction. Although the ﬁtting of the 3d prediction curve of
Fig. 9 is not superior to Figs. 7 and 8, the trend of the ﬂow rate can be roughly described,
with little eﬀect.
6
Conclusion
The results of MATLAB simulation show that the network traﬃc prediction model based
on wavelet decomposition and genetic algorithm has good accuracy and it is an eﬀective
prediction model, which is mainly embodied in:
1. After wavelet decomposition, the original signal becomes more simple and has good
stability, which provides the stability for BP neural network prediction;
2. The traditional time series analysis method is diﬃcult to ensure high accuracy. For
non-linear traﬃc ﬂow, BP neural network prediction is better;
3. Genetic algorithm can improve the defects of BP neural networks, which can easily
fall into the local optimal solution and the convergence rate.
Network Traﬃc Prediction Based on Wavelet Transform
199

However, the model in this paper is rather complicated. The high-frequency and low-
frequency components obtained by wavelet decomposition are predicted by neural
network, and the computational cost of prediction is large, which aﬀects eﬃciency of
the model. Therefore, in the case of the high accuracy, to further improve the eﬃciency
of the model is the future research direction of this paper.
References
1. Wang, Z.X., Sun, Y.G., Chen, Z.Q., Yuan, Z.Z.: Study of predicting network traﬃc using
fuzzy neural networks. J. Chin. Inst. Commun. 3(26), 136–140 (2005)
2. Hou, J.L.: Prediction for network traﬃc based on Elman neural network. Comput. Simul.
28(7), 154 (2010)
3. Liu, X.W., Fang, X.M., Qin, Z.H., Ye, C., Xie, M.: A short-term forecasting algorithm for
network traﬃc based on chaos theory and SVM. J. Netw. Syst. Manag. 19, 427–447 (2011)
4. Cao, J.H., Liu, Y.: Network traﬃc prediction based on grey neural network integrated model.
Comput. Eng. Appl. 05, 155–157 (2008)
5. Chen, Z.W., Guo, Z.W.: Simulation realization of prediction model based on wavelet neural
network. Comput. Simul. 25(6), 147–150 (2008)
6. Zhang, H., Wang, X.: Modeling and forecasting for network traﬃc based on wavelet
decomposition. Appl. Res. Comput. 08, 3134–3136 (2012)
7. Li, D.D., Zhang, R.T., Wang, C.C., Xiao, D.P.: A new network traﬃc prediction model based
on ant colony algorithm in cognitive networks. ACTA ELECTRONICASINI -CA 10, 2245–
2250 (2011)
8. Bai, X.Y., Ye, X.M., Jiang, H.: Network traﬃc predicting based on wavelet transform and
autoregressive model. Comput. Sci. 07, 47–49 (2007)
9. Deng, K.L., Zhao, Z.Y.: Research and Simulation of stock price prediction mode based on
genetic algorithm BP neural network. Comput. Simul. 26(5), 317 (2009)
10. Boris, T., Nicolas, D.: Self-similar processes in communications networks. IEEE Trans. Inf.
Theory 44, 1713–1725 (1998)
11. Yao, M.H.: Application of improved genetic algorithm in optimizing BP neural networks
weights. Comput. Eng. Appl. 24, 49–54 (2013)
12. Dang, X.C., Hao, Z.J.: Prediction for network traﬃc based on modiﬁed Elman neural network.
J. Comput. Appl. 10, 2648–2652 (2010)
200
X. Zhao et al.

Research and Optimization of the Cluster
Server Load Balancing Technology
Based on Centos 7
Lei Ding(&), Wanbo Zheng, Shufen Liu, and Zhian Han
School of Computer Science and Technology, Jilin University,
No. 2699, Qianjin Avenue, Qianweinan District,
Changchun, Jilin, People’s Republic of China
dinglei0828@qq.com
Abstract. The paper is to observe Weighted Least-Connection Algorithm, a
default one of load balancing scheduling algorithms, and point out its deﬁ-
ciencies by establishing Linux Virtual System under Centos 7. Then we also
improve the factors that inﬂuence weight based on above. During the experi-
ment, not only is the throughput of system increased, but also the responding
time shortened. It optimizes the performance and improves the stability of the
whole system consequently in brief.
Keywords: Load balance  Scheduling  Cluster  WLC algorithm
Throughput rate
1
Preface
With present information on the Internet is growing at a geometric rate, parallel
architecture technique of distribute web cluster service is also growing more mature.
Most service vendors establish cluster system through Linux, which can effectively
settle many concurrent requests to balance load and ﬁx bugs in time [1]. This text
relates to the latest released version of Centos 7, which manages and assigns tasks by
using default load balance schedule of LVS and ip_vs module.
The management applications abroad at the earliest were obsoleted to settle with
unbalanced load among multi servers, because deployment of each machine was
verbose, such as Zeus [2]. After that, load balance production based on software and
hardware appeared, like Barracuda and Load Directors, which started early on load
technique issue and maintained leading position.
LVS was a free software project. It started and studied by Wen-song Zhang, a
doctor of National University of Defense Technology. It covered more than ten kinds
of load scheduling algorithms with IP load balance technique mainly. Therefore, LVS
came ﬁrst on the list of load management system for cluster as well as had great fame
home and abroad with characters of scalability, reliability and manageability [3, 4].
In the study of Weighted Least-Connect Scheduling, we thought that ratio between
client connections and own weight for every server could not measure load capacity
accurately. Therefore, we introduced extra factors such as CPU, memory, hard disk, etc.
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 201–207, 2018.
https://doi.org/10.1007/978-3-319-74521-3_23

Most web service involved internet I/O resource, which also reﬂected part of real load for
each server. We should consider all those factors together above and make a rational
division of upcoming task for load balancer to enhance performance of the whole system.
In next section, we introduce the traditional WLC algorithm and the improved version.
2
Load Balancing Scheduling Algorithm
There are three ways such as VS/NAT, VS/TUN and VS/DR to realize LVS under IP
load balancing technique. VS/DR could avoid load balancer being the system bottle-
neck. It could also reduce the complexity of conﬁguration sometimes. The following is
the VS/DR architecture diagram of this experiment, as shown in Fig. 1.
2.1
Traditional Weighted Least-Connection Scheduling Algorithm
More than ten traditional load balance-scheduling algorithms are built-in for Centos 7,
most of which use Weighted Least-Connection Scheduling as default [5]. The schedule
is the superset of the Least-Connection one. The server Si (i = 0, 1…, n) uses weight W
(Si) as its performance with default 1, uses C(Si) as the current number of connections
and use Csum as the RC(Si). The condition as shown in formula (1) indicates a new
task assigned to Sm next.
CðSmÞ=Csum
WðSmÞ
¼ minfCðSiÞ=Csum
WðSiÞ
g
ð1Þ
As Csum from each side is extremely similar and efﬁciency of multiplication
exceed division, then we could simplify it to formula (2) [6].
CðSmÞ  WðSiÞ [ CðSiÞ  WðSmÞ
ð2Þ
Fig. 1. VS/DR architecture
202
L. Ding et al.

WLC’s aim is to ﬁnd the server Si in server pool with the minimal value of C/W,
and then assign the new task to it. However, it is not reliable by just using the
connections and weight as we discussed above. Administrator with much experience
must change the value of weight manually during the whole procedure. Therefore, we
introduce and adopt the improved WLC algorithm in next section.
2.2
Improved WLC Algorithm
The parameters such as CPU, memory and disk of servers are introduced to estimate
their real load and update weight automatically in the improved version. L(Si) repre-
sents load seizure rate, P(Si) represents performance of hardware, F(Si) indicates
performance of network. P(Si) and F(Si) should be concerned together to explain whole
performance better. Usually, P(Si) which indicates for hardware would change little
with increasing pressure, so we could assume it as static variable and send it to load
balancer once. However, F(Si) dynamically change by throughput rate and TTLB
factors, it could be send back with L(Si) together. The calculate formula is shown as
below [7].
LðSiÞ ¼ k1  LcpuðSiÞ þ k2  LmemðSiÞ þ k3  LdiskðSiÞ
ð3Þ
PðSiÞ ¼ l1  PcpuðSiÞ þ l2  PmemðSiÞ þ l3  PdiskðSiÞ
ð4Þ
FðSiÞ ¼ c1  FttlbðSiÞ þ c2  FtrðSiÞ
ð5Þ
The k, l, c represent the percentage of each item, whose range in (0, 1) and sum
is 1. We could assign them speciﬁc values into the above formulas to reduce the
complexity and beneﬁt for control variables. Therefore, load weight of server could be
estimated as formula (6). W(Si) is proportional to L(Si) while it is in inverse propor-
tional to F(Si). Load balancer collects L(Si) and F(Si) of each server while new time
slice ΔT begins [8].
WðSiÞ ¼
LðSiÞ
PðSiÞ  FðSiÞ
ð6Þ
In order to improve fault tolerance and avoid server becoming overload during
ΔT while a new task comes, we put forward the concept of load redundancy to indicate
extra load that server could hold [9, 10]. Load redundancy represents the max extra
capacity within a slice of Dt as formula (7) [5].
RðSiÞ ¼ PðSiÞ  FðSiÞ  Dt
LðSiÞ
ð7Þ
Dt’s range is expected to be [5, 10] seconds. On the one hand, Dt to be small would
bring extra computation frequently, on the other hand, it would be too large to reﬂect
the redundancy timely. Load balancer also needs to set an initial minimal redundancy
threshold Rmin and a binary sort tree, only the R(Si) which is equal or greater than
Research and Optimization of the Cluster Server Load Balancing Technology
203

threshold could be added in that data structure. Every node of the tree has chance to
receive new task, but we optimally choose the largest value of R(Si), which means we
choose the smallest W(Si) with less load pressure [5, 12].
P(Si) and F(Si) could be estimated by software in next section. L(Si) has three
factors which change as time goes on, here are intuitive ways to get these values
respectively below.
(1) Real-time CPU utilization rate considers differences within two close moment from
the ﬁle “/proc/stat” on a Linux server, the content shows many kinds of time slices
whose unit are jifﬁes. Formula (8) shows totalTime for CPU.
totalTime ¼ user þ nice þ system þ idle þ iowait þ irq þ softirq þ stealstolen þ guest
ð8Þ
We set totalTime as T1, idle to I1. Then after a period of a very short time, we
calculate totalTime, idle again and set them to T2, I2 respectively. Finally, formula (9)
shows the CPU utilization rate.
LcpuðSiÞ ¼ ðT2  T1Þ  ðI2  I1Þ
T2  T1
ð9Þ
(2) The memory information of Centos 7 is stored in “/proc/meminfo”. Formula (10)
shows the free memory of Linux, and formula (11) shows the occupancy rate of
memory which MemTotal indicates the total memory size.
Mfree ¼ Memfree þ Buffers þ Cached
ð10Þ
LmemðSiÞ ¼ MemTotal  Mfree
MemTotal
ð11Þ
(3) We can install “iotop” software to check whole I/O occupancy rate under Linux.
The command “iotop-o” shows all processes/threads which include I/O operations.
We focus the column named “IO>” and assign the sum to Ldisk(Si).
LdiskðSiÞ ¼
X
n
tid¼1
IOðtidÞ
ð12Þ
The communication between load balancer and servers use TCP socket [11].
Server, a TCP client, sends message L(Si), R(Si) and F(Si) regularly but P(Si) only
once. Load balancer, a TCP server, receives values of each server and calculates new
W(Si) and R(Si). If R(Si) > Rmin, we put this server into the tree. If R(Si) < Rmin, then
we remove this server out of the tree [5]. Traversing the tree in order ensures the
smaller node we pick up can deal with a new task easier.
204
L. Ding et al.

2.3
Result of Experiment
We use various kinds of software such as “AIDA64”, “CPU-Z”, “Web Application
Stress Tool” to estimate P(Si) with scores and F(Si) like Figs. 2 and 3 [12]. The
experiment imitates the operations of login, add, refresh, and quit in sequence on the
bookmarking website we build before which was written by PHP.
Firstly, we assign k1 = 0.4, k2 = 0.3, k3 = 0.3, l1 = 0.5, l2 = 0.4, l3 = 0.1,
c1 = 0.4, c2 = 0.6 and Dt = 10 s. Then we collect average response time and
throughput with increasing number of requests. Figures 4 and 5 show the trend lines.
The improved algorithm when request quantity after 271 is superior to the tradi-
tional in Fig. 4. Frequent calculation and update operations need extra time in the early
period, but it shorts the response time with more rational strategy to assign tasks. The
throughput increases ﬁrst and decrease later in Fig. 5, the improved is superior the
traditional after about 267. It reaches maximum at about 410 because the system
achieves saturation gradually with increasing requests.
Fig. 3. Web application stress tool
Fig. 4. TTLB average response time
Fig. 5. Bytes received rate
Fig. 2. AIDA64
Research and Optimization of the Cluster Server Load Balancing Technology
205

Secondly, we discuss about the factors k, c in L(Si) and F(Si) with the same 800
requests respectively as Tables 1 and 2 below.
Table 1 shows changes of TTLB while providing k with different percent. k1 has
more inﬂuence than k2 and (1 −k1 −k2), so CPU may have the highest priority. It is
similar in Table 2 with c. Setting appropriate factors can improve the situation [13].
Boundary values could not generate reliable result.
Finally, according to these analyses above, the improved algorithm lifts about
15–18% of load balancing performance in cluster environment. It is beneﬁt for
improving stability and efﬁciency of the whole system.
3
Conclusion
This paper introduces the approach to enhance performance of cluster system and offers
better concurrency service after improving Weighted Least-Connection Scheduling.
Estimating weights reasonably and ﬁltrating statistical data for delivering task accel-
erate procedure of response smoothly. The improved one contributes to science
experiment and business such as neural network, big data analysis, e-commerce, virtual
host service, etc. However, there are still many problems to solve [1, 12], such as
understanding more inﬂuenced factors fully, improving speed, etc. We will break the
limitation and strong the algorithm with the development of technology in the future.
References
1. Semchedine, F., Bouallouche-Medjkoune, L., Aïssani, D.: Task assignment policies in
distributed server systems. J. Netw. Comput. Appl. 34(4), 1123–1130 (2011)
2. Cardellini, V., Colajanni, M., Yu, P.S.: Dynamic load balancing on web-server systems.
IEEE Internet Comput. 3(3), 28–39 (1999)
3. Zhang, W., Wu, T., Jin, S., Wu, Q.: Design and implementation of a virtual internet server.
J. Softw. (2001). (in Chinese)
4. Zhang, W.: Research and implementation of scalable network services. Comput. Eng. Sci.
(2001). (in Chinese)
Table 1. k factor
λ2
λ1   ttlb
0.1
0.3
0.6
0.8
0.1
1894.36
2017.12
2398.40
2658.32
0.3
2271.24
2475.67
2825.21
0.6
3093.45
3317.04
0.8
3717.72
Table 2. c factor
c1
F(Si)
0.2
1204.68
0.5
963.57
0.8
673.90
206
L. Ding et al.

5. Bryhni, H.: A comparison of load balancing techniques for scalable web servers. IEEE
Network 14(4), 58–64 (2001)
6. Penmatsa, S., Cronopoulos, A.T.: Dynamic multi user load balancing in distributed systems.
In: IEEE International Parallel and Distributed Processing Symposium (2007)
7. Wang, H.: Research on Adaptive Load Balancing Scheduling Strategy in Web Server
Cluster System. Jilin University, Changchun (2013). (in Chinese)
8. Lu, J., Chen, Z., Liu, A.: A dynamic regulation weight value scheme for LVS. J. Comput.
Eng. 14, 038 (2006). (in Chinese)
9. Wang, J., Pan, L., Li, X.: Dynamic feedback scheduling algorithm in LVS. J. Comput. Eng.
19, 014 (2005). (in Chinese)
10. Sheng, G.Q., Wu, S.J., Ping, M.X., Chan, W.D., Min, Z.: Design and realization of dynamic
load balancing based on Linux Virtual System. J. Comput. Res. Dev. 6, 002 (2004). (in
Chinese)
11. Chun, W.: Core Python Applications Programming. 3rd edn. Prentice Hall (2012)
12. Wang, C., Dong, L., Jia, L.: Load balance algorithm for web cluster system. J. Comput. Eng.
36(2), 102–104 (2010). (in Chinese)
13. Guo, C., Yan, P.: A dynamic load balancing algorithm for heterogeneous web server cluster.
Chin. J. Comput. 2, 004 (2005). (in Chinese)
Research and Optimization of the Cluster Server Load Balancing Technology
207

Base Station Location Optimization Based on Genetic
Algorithm in CAD System
Yanhua Wang
(✉), Laisheng Xiang, and Xiyu Liu
College of Management Science and Engineering, Shandong Normal University, Jinan, China
15554130027@163.com, xls3366@163.com, sdxyliu@163.com
Abstract. A good base station deployment plan can help network operators save
cost and increase total revenue signiﬁcantly under the premise of ensuring
network quality. But in the past, base station location planning is often manually
based on the engineer’s experience. It has a lower eﬃciency and very high error
rate. In this paper, a new method based on genetic algorithm is proposed to opti‐
mize base station location. In our work, a CAD system based Google Earth and
ACIS is designed to provide data for Genetic algorithm and display the location
of base station in the reconstructed terrain. This system which takes three-dimen‐
sional geographic coordinates as the input of the algorithm is advanced and
diﬀerent from the traditional method which only uses two-dimensional coordi‐
nates, that is, this three-dimensional system can better display the base station
location and take the height into consideration. The proposed method is based on
a mathematical model of base station location. Genetic Algorithm is used to ﬁnd
the solution of this model so that it can eﬀectively reduce the error rate of base
station location.
Keywords: Genetic Algorithm · Base station planning · Coverage · ACIS
Google earth · CAD system
1
Introduction
Excellent base station location plan for operators can help to get the best results in the
mobile communications market in the increasingly ﬁerce competition [1]. Early plan
and design of the base station is mainly by the network optimization engineer based on
experience and ﬁeld measurements to choose [3, 4]. Obviously, this method is not very
scientiﬁc. Over-reliance on subjective factors, led to the results often far from the actual
optimum conﬁguration. Telecommunication base station automatic programming CAD
system based Genetic Algorithm proposed in this paper is based on Windows platform.
The process of solve the problem of base station planning as follows. First, use
Microsoft visual studio, Xtreme Toolkit Pro, ACIS/HOOPS to build the development
environment. Second, using com technology import Google earth into the development
environment and accessing and transforming Google earth coordinates which include
longitude, latitude, and elevation. Third, use ACIS/HOOPS technology to reconstruct
Three-Dimensional Terrain according to the transformed coordinates. At the same time,
establish a multi-objective mathematical model about base station planning according
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 208–214, 2018.
https://doi.org/10.1007/978-3-319-74521-3_24

to geographic information and produce base station primary scheme using genetic algo‐
rithm. At last, it uses ACIS/HOOPS modeling techniques to display the primary scheme
on the reconstructed terrain 1. Structure of the process is shown in Fig. 1:
Development 
environment based on 
VC++ and ACIS
Embedding Google earth in 
development program using com 
technology
Establish a mathematical
model about communication base 
station planning
Accessing and transforming 
Google earth coordinates
Using the Genetic  Algorithm to 
solve the model
Using ACIS to refactor terrain 
Using ACIS to display 
solutions on the refactored 
terrain
Fig. 1. Flow chart of base station location optimization
2
Terrain Reconstruction by Google Earth and ACIS Technology
ACIS is a development platform based on C++ structure graphics system, Developers
can use these classes and functions construct a Three-Dimensional software system for
end users. Google Earth provides a set of COM components that can be embedded into
visual studio development platform by the COM technology [2]. Through the secondary
development of Google Earth, you can get the actual terrain’s height, latitude and longi‐
tude information that was shown in Fig. 2 which can be converted into a three-dimen‐
sional coordinate that is shown in Fig. 3 of the screen of the CAD system so that it can
simplify the data and facilitate the calculation when the transformed coordinates of the
screen as the input of algorithm. After running the optimization algorithm, the optimal
position of the output of the base station can be converted to the actual elevation, latitude
and longitude coordinates.
Fig. 2. The extracted actual coordinates
Fig. 3. The transformed screen coordinates
Base Station Location Optimization
209

The transformed screen coordinates can be used to reconstruct the terrain by ACIS
technology with api_face_spl_apprx, api_face_plane and api_loft_faces interface. The
reconstructed terrain was shown in the Fig. 4, the actual terrain was shown in the Fig. 5.
The reconstructed terrain was used to show the plan of base station location in Three-
dimensional 3.
Fig. 4. The actual terrain
Fig. 5. The reconstructed terrain
3
The Mathematical Model of Base Station Location
3.1
Coverage Rate Statement and Formulation
Coverage rate is calculated by using the number of demand points which are covered,
divided by the total number of the demand points of the terrain points [4]. To avoid a
repeat count of demand points, we just put the demand point included in the base station’s
coverage area which has the nearest distance from the demand point compared to other
base stations. To formulate coverage targets we deﬁned as follows: U is the set of all
demand points; j is the demand points which j ∈U, T is the set of all base stations; i is
the base station which i ∈T. dij is the distance between base station i and demand point
j, ri is coverage radius of base station i, the coverage area of base station i, denoted by
Ci, is deﬁned as the collection of demand points which is within the coverage area of i
and has the shortest distance from i among all the base stations. Formally stated, Ci is
given by:
Ci = {j ∈U|dij ≤ri ∧(dij ≤di′j, ∀i′ ∈T ∧i ≠i′)}
(1)
The ultimate goal of optimizing the coverage is achieve full coverage of all user
points by using the minimum number of base stations.
In the actual experiment, we defined fCT as a function of the base station coverage
goals. The total number of all demand points covered by all base stations is given
by ∑
i∈T Ci. The total number of all demand points deployed on the terrain is denoted
by |U|. When fCT = 1, all demand points in the terrain are fully covered. However, a
fCT = 0.95 is quite acceptable. Although a fCT = 1 is too costly to achieve, fCT is
maximized in the model. fCT is defined as follows:
210
Y. Wang et al.

MaxfCT =
∑
i∈T Ci
|U|
(2)
3.2
Cost Statement
Cost is a very important objective optimization goals, the investment of base station
planning is about two-thirds of the total investment of the whole network. Therefore, a
lower cost of base station planning can eﬀectively reduce the total network investment
so that it can enhance the competitiveness of the enterprise communication. The cost of
the base station planning is mainly from the built of base stations. In condition of each
base station has a ﬁxed cost, the number of base stations becomes the most important
optimization goals.
In order to prevent the waste of the base station, the number of base station we
calculated by divide the terrain area by the area of the base station. There are three
diﬀerent radii of base stations, so the number of base stations in the interval [20, 30]
inner. By experiment, when the number is 25, the cost is optimal.
4
Model Solving Based on Genetic Algorithm
Genetic algorithm is one of the four main branches of evolutionary computation. It is
also the main evolutionary algorithm developed rapidly in the last ten years. Genetic
algorithm with evolutionary strategy, evolutionary programming and genetic program‐
ming had been rapid development and gradually to integration, and formed a new
computational theory of simulated evolution [5, 6].
In the genetic algorithm, simulate the evolution of biological processes, chromo‐
somes or individuals of the population perform crossover and mutation operations [7].
The implementation of basic genetic manipulation need to use the selection, crossover
and mutation of the three types of genetic operators. Thus, the genetic algorithm is also
considered as a random search algorithm. But it is also a process through iterative opti‐
mization, with self-adapting characteristics.
In the process of solve the mathematic model by Genetic Algorithm, we use ﬂoating
point encoding to encod Xi = {xi1, xi2, xi3 ⋯, xi,3m−2, xi,3m−1, xi,3m}(m = |TS|) is the
encoding of i-th chromosome of GA algorithm (xi,3m−2, xi,3m−1, xi,3m) ∈Xi (k = 1 … m)
is the three-dimensional coordinates of k-th base station. F = fCT is the objective function
of entire system, so F(Xi) is the ﬁtness value of i-th chromosome, That is a measure of
the solution of deployment of m base stations in the planning region. The ﬁtness value
is closer to 1, indicating that the program closer to the optimal. Algorithm ﬂow chart is
shown in Fig. 6.
Base Station Location Optimization
211

Begin
IniƟal populaƟon
Calculate populaƟon ﬁtness, to 
determine whether
Meet preset requirements
SelecƟon operator
Crossover operator
MutaƟon operator
Calculate populaƟon ﬁtness, to 
determine whether
meet preset requirements
Whether up to the 
maximum IteraƟons
End
YES
YES
No
NO
NO
YES
Fig. 6. Genetic Algorithm ﬂow chart
In the CAD system, we design a dialog to run the Genetic Algorithm which is shown
in Fig. 7. The maximum Iterations input box is used to set the maximum number of
iterations. Color setting input box is used to set the color of base station. First we design
the maximum iterations is 100. The Genetic Algorithm didn’t up to the optimal result
and stopped at the 100th generation. The result which the base station’s color is green
is shown in Fig. 8. Second we design the maximum iterations is 1000. The Genetic
Algorithm up to the optimal result and stopped at the 523th generation. The result which
the base station’s color is red is shown in Fig. 8. The cycle is the signal of each base
station.
Fig. 7. Genetic Algorithm dialog
Fig. 8. The plan of base station location (Color
ﬁgure online)
The iterative process of the Genetic Algorithm is shown in Fig. 9, we use 25 base
station and iterated 300 times, the coverage rate is as shown in the Fig. 9.
212
Y. Wang et al.

Fig. 9. Iterative process of the Genetic Algorithm
5
Conclusions
In this paper, we proposed a new method based genetic algorithm to make base station
location plan. At this time, we developed a CAD system based Google Earth and ACIS
to provide data for Genetic Algorithm and demonstrate base station location plan in the
reconstructed terrain which is Three-dimensional.
The future work will continue research on improving traditional intelligent optimi‐
zation algorithm and implement it on a three-dimensional CAD system. Meanwhile, the
mathematic model of base station plan will be further design. The genetic algorithm will
be improved.
Acknowledgment. Projected supported by National Natural Science Foundation of China
(61472231, 61170038, 61502283, 61640201), Jinan City independent innovation plan project in
College and Universities, China (201401202), Ministry of education of Humanities and social
science research project, China (12YJA630152), Social Science Fund Project of Shandong
Province, China (11CGLJ22, 16BGLJ06).
References
1. Xie, Q., Liu, X., Yan, X.: Research on station location optimization CAD system based on the
cooperative mode. In: Zu, Q., Hu, B. (eds.) HCC 2016. LNCS, vol. 9567, pp. 930–935.
Springer, Cham (2016). https://doi.org/10.1007/978-3-319-31854-7_102
2. Xie, Q., Liu, X., Yan, X.: Base station location optimization based on the Google Earth and
ACIS. In: Zu, Q., Hu, B. (eds.) HCC 2016. LNCS, vol. 9567, pp. 487–496. Springer, Cham
(2016). https://doi.org/10.1007/978-3-319-31854-7_44
3. Ren, S., Li, X., Liu, X.: The 3D visual research of improved DEM data based on Google Earth
and ACIS. In: Zu, Q., Vargas-Vera, M., Hu, B. (eds.) ICPCA/SWS 2013. LNCS, vol. 8351,
pp. 497–507. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-09265-2_51
4. Tao, M., et al.: SA-PSO based optimizing reader deployment in large-scale RFID Systems. J.
Netw. Comput. Appl. 52, 90–100 (2015)
Base Station Location Optimization
213

5. Fogel, L.J., Owens, A.J., Walsh, M.J.: Artiﬁcial Intelligence Through Simulated Evolution.
John Wiley & Sons, New York (1966)
6. Schwefel, H.-P.: Numerical Optimization of Computer Models. John Wiley & Sons Inc.,
New York (1981)
7. Golberg, D.E.: Genetic Algorithms in Search, Optimization, and Machine Learning. Addison
Wesley, Boston (1989)
214
Y. Wang et al.

A Study of Optimal Multi-server System
Conﬁguration with Variate Deadlines
and Rental Prices in Cloud Computing
Zhongfeng Kang and Bo Yang(&)
School of Computer Science and Engineering, University of Electronic Science
and Technology of China, Chengdu 611731, Sichuan,
People’s Republic of China
kzhf921203@163.com, yangbo@uestc.edu.cn
Abstract. Cloud computing is becoming more and more popular and attracts
considerable attention. In the there-tire cloud environment, an important prob-
lem is to determine the optimal multi-server system conﬁguration so that the
proﬁt of the service provider can be maximized. In related work, the maximum
allowed waiting time of service is assumed to be a constant, and the rental price
is also assumed to be constant for all servers despite the fact that different
servers have different execution speeds. These assumptions may not be valid in
realistic cloud environments. In this paper, we propose an optimization model to
determine the optimal conﬁguration of the multi-server system. There are two
major differences of the proposed model with that of the existing work. First, the
maximum allowed waiting time is not a constant and may change with different
service requests. Second, the situation that the servers with different execution
speed may have different rental prices is taken into account. Experiments are
carried out to verify the performance of the proposed optimization model. The
results show that the proposed optimization model can help the service provider
gain more proﬁt than the existing work.
Keywords: Multi-server system  Proﬁt maximization
Maximum allowed waiting time  Response time  Rental price
1
Introduction
Cloud computing is becoming more and more popular and attracts considerable atten-
tion [5]. In a cloud computing environment, there are three tiers, i.e., infrastructure
providers, service providers, and consumers (see Fig. 1) [1, 2]. An infrastructure pro-
vider maintains the basic hardware and software resources. A service provider could rent
a certain scale of software and hardware resources from infrastructure providers and
provides services to consumers. Consumers can submit their service requests to the
service provider and pay them based on the quantity and the quality of the services. In
above cloud computing environment, the problem of optimal multi-server system
conﬁguration for proﬁt maximization is introduced as a signiﬁcant, new research topic in
computer science [3–5]. The conﬁguration of a multi-server system is characterized by
two basic features, i.e., the size of the multi-server system (the number of rented servers)
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 215–231, 2018.
https://doi.org/10.1007/978-3-319-74521-3_25

and the execution speed of the multi-server system (execution speed of the rented
servers) [3]. The key issue of the multi-server system conﬁguration problem is to
determine the optimal size and execution speed of the multi-server system such that the
service provider can gain the maximum proﬁt.
Like any other business, the proﬁt is the most important issue to a service provider.
The service provider’s proﬁt is determined by two parts, i.e., the gained revenue and
the corresponding cost, the revenue minus the cost is its proﬁt. For a service provider,
the revenue is the sum of the charge to the consumers, and the cost is the rental cost
paid to the infrastructure providers plus the electricity cost caused by energy con-
sumption. The charge to a consumer is related to the quantity and the quality of the
service, that is, if the quality of the service is guaranteed, the service is fully charged,
otherwise, if the quality of the service request is lower than the promised Quality of
Service (QoS), the service provider serve the service request for free as a penalty of low
service quality [3, 6, 7]. The electricity cost is linearly proportional to the number of the
servers and to the square of the server speed [8, 9].
Many existing works have been done on the problem of multi-server system
conﬁguration for proﬁt maximization in the literature [3–5]. Cao et al. [3] proposed a
single renting scheme to conﬁgure the multi-server system. Using this scheme, the
servers in the multi-server system are all long-term rented servers, so the system is lack
of elasticity and easily leads to resource waste and consumer loss. To overcome this
weakness, Mei et al. [4] proposed a combined renting scheme. Using combined server
renting scheme, the main computing capacity is provided by long-term rented servers,
and the rest is provided by short-term rented servers. In this service system, when a
consumer submits a service request, the service request will be ﬁrst put into a queue
and wait in the queue until it can be served by any server. But in order to satisfy the
QoS requirement, the waiting time of each service request in the queue must be limited
within a certain range, which is named the maximum allowed waiting time and is
determined by the service lever agreement (SLA). To guarantee the QoS, the service
request should be started no later than the maximum allowed waiting time. So, for a
service request in the queue, if its waiting time has reached the maximum allowed
waiting time, system temporarily rents a short-term server to provide service and the
short-term server is freed when the service is ﬁnished. But in Mei et al.’s study, the
maximum allowed waiting time for all service requests is deemed to be the same, that
is, the service requests in the multi-server system have equal maximum allowed waiting
Fig. 1. The three tiers cloud environment.
216
Z. Kang and B. Yang

times. This assumption may not be realistic. Therefore, this paper takes into account the
condition that different service requests may have different maximum allowed waiting
time.
In Mei et al.’s work [4], the servers with different execution speed is deemed to
have the same rental price. Thus the rental cost in their work is only related to the
number of the rented servers. But more realistically, the rental price of the server may
depend on the execution speed. Hence, the rental cost is not only related to the number
of rented servers but also the execution speed of the rented servers, i.e., the more (less,
respectively) number of the rented server is, the more (less, respectively) rental cost is,
also the faster (slower, respectively) execution speed of rented server is, the more (less,
respectively) rental cost is.
In this paper, the problem of optimal multi-server system conﬁguration for proﬁt
maximization is studied as an optimization model. In this optimization model, the
conditions that the maximum allowed waiting times may be variables and the rental
price of the server may be changed with the execution speed of the rented server are
taken into account. The contribution of the paper is summarized as follows.
The maximum allowed waiting times of different service requests are deemed to be
no longer a constant but random variables.
The condition that the rental price for different execution speed server may be
different is taken into consideration. And a rental price model is proposed.
An optimization model is proposed to solve the problem of optimal multi-server
system conﬁguration for proﬁt maximization.
Experimental studies are conducted to verify the performance of the proposed
optimization model. The results show that the proposed optimization model proposed
in this paper can result in more proﬁt than that of existing work.
The rest of the paper is organized as follows. Section 2 presents the relevant models
and rental scheme, including a three-tier cloud environment model, a multi-server
system model, an energy consumption model and a combined renting scheme. In
Sect. 3, an important probability used in this paper is calculated. Section 4 describes
the rental price model. Section 5 establishes an optimization model to solve the
problem of optimal multi-server system conﬁguration for proﬁt maximization.
Section 6 veriﬁes the performance of the proposed model through comparison with that
of existing model. Finally, Sect. 7 concludes the work.
2
The Cloud Environment, Models and Renting Scheme
This section introduces three relevant models, namely a three-tier cloud environment
model, a multi-server system model and an energy consumption model, and a com-
bined renting scheme.
2.1
Three-Tier Cloud Environment Model
In this paper, we study the multi-server system conﬁguration problem under three-tier
cloud computing environment, in which there are three tiers, i.e., infrastructure provi-
ders, service providers, and consumers [3, 4, 15], as shown in Fig. 1. The infrastructure
A Study of Optimal Multi-server System Conﬁguration
217

provider maintains the basic hardware and software resources. The service provider
rents a certain scale of software and hardware resources from infrastructure providers
and builds its service platform to provide services to consumers. Consumers submit their
service requests to the service provider and pay based on the quantity and the quality of
the services. The infrastructure provides two kinds of resources renting schemes, i.e.,
long-term renting and short-term renting, and the rental price per server per unit period
of time with long-term renting is much cheaper than that with short-term renting [4]. For
a service provider, the gained revenue is the sum of charge to consumers, and the cost
involved is the rental cost paid to the infrastructure providers plus the electricity cost
caused by energy consumption. Thus the proﬁt is generated from the gap between the
revenue and the cost [3, 5].
2.2
Multi-server System Model
The cloud service provider is considered to be a multi-server system with a service
request queue [17]. The multi-server system consists of m long-term rented identical
servers and it can extend by temporarily renting short-term servers from infrastructure
provider [4], therefore, the multi-server system may have two parts, i.e., long-term
rented servers (the long-term part) and possible short-term rented servers (the short-
term part). Each server in the multi-server system has an execution speed of s (unit:
billion instructions per second) [4]. The long-term part of the multi-server system can
be modeled by an M/M/m queuing system [5, 18].
This paper makes the following assumptions, which are adopted for the
multi-server system. These assumption are also use in related studies [3, 4].
(i)
Service requests arrive according to a Poisson process, with arrival rate k
(measured by the number of service requests arrived per second). It means that
the inter-arrival times are independent and identically distributed (i.i.d.) expo-
nential random variables (r.v.’s) with mean 1=k.
(ii)
The multi-server system maintains a queue with inﬁnite capacity.
(iii)
Different service requests may have different service sizes (measured by the
number of billion instructions), denoted by ri (i 2 {1,2,3,…}), which are i.i.d.
exponential r.v.’s with parameter l.
(iv)
Each service request can only be served by one server that may be long-term or
short-term rented.
(v)
The ﬁrst-come-ﬁrst-served (FCFS) queuing discipline is adopted.
Denote by N(t) the total number of service requests that arrives during the time
interval (0,t], with mean E½NðtÞ ¼ kt.
Denote by Ns(t) and Nl(t) the number of the service requests that served by the
short-term and long-term servers, respectively, during the time interval ð0; t.
An arrived service request is put into the queue and waits in the queue until it can
be handled by any available server. Then, the working process of the system can be
modeled as Fig. 2.
218
Z. Kang and B. Yang

According to assumption (iii), the mean of ri is r ¼ EðriÞ ¼ 1
l (unit: billion
instructions), and the execution times of the services on the multi-server system, xi ¼ ri
s
(i 2 {1,2,3,…}), are i.i.d. exponential r.v.’s, with mean x ¼ EðxiÞ ¼ r
s ¼ 1
ls (unit:
second).
The system service intensity means the average percentage of time that a server of
the multi-server system is busy. Denote by q the system service intensity, which can be
given by
q ¼ kr
ms ¼
k
mls ;
ð1Þ
According to [5, 10], q should be no more than 1, i.e., k
l  ms.
Denote by Wi the waiting time of service request i in the queue. The cumulative
distribution function (c.d.f.) of Wi, FWiðtÞ, can be derived from M/M/m queuing system
theory [11], which is
FWiðtÞ ¼ 1 
pm
1  q emdð1qÞt;
ð2Þ
where
pm ¼ ðmqÞm
m!
X
m1
k¼0
ðmqÞk
k!
þ
ðmqÞm
m!ð1  qÞ
"
#1
:
2.3
Energy Consumption Model
The cost of a service provider consists of two major parts, i.e., the rental cost paid to the
infrastructure provider and the electricity cost caused by energy consumption. The cost
of energy consumption is determined by the electricity price and the amount of energy
consumption.
Denote by w the price of unit energy (unit: cents per Watt). The power consumption
of modern processor can be divided into two parts, dynamic power Pd (unit: Watt) and
Fig. 2. The working process of the system.
A Study of Optimal Multi-server System Conﬁguration
219

static power P* (unit: Watt) [12]. The dynamic power model used in the paper is given
by Eq. (3),which is also used in [3–5].
Pd ¼ nsa
ð3Þ
When n ¼ 9:4192 and a ¼ 2:0, the value of power consumption is close to the value of
the Intel Pentium M processor [13]. Therefore, the power per unit period of time for a
busy server is
P ¼ Pd þ P:
ð4Þ
2.4
Combined Renting Scheme
The renting scheme combines long-term renting with short-term renting for the
multi-server system [4]. The main service capacity of the multi-server system is provided
by long-term rented servers, due to the low rental price; and the rest service capacity is
provided by short-term rented servers. Algorithm 1 shows the combined renting scheme.
Algorithm1. Combined renting scheme
1. A multi-server system with m servers and speed s is running and waiting for the events
as follows
2. A queue Q is initialized as empty
3. Event - A service request arrives
4. Put it at the end of queue Q and records its maximum allowed waiting time and the
timer starts counting its waiting time
5. End Event
6. Event - A server becomes idle
7.
Search if the queue Q is empty
8.
if true then
9.
Wait for a new service request
10.
else
11.
Take the first service request from queue Q and assign it to the idle server
12.
end if
13. end Event
14. Event - The maximum allowed waiting time of a request is achieved
15.
Take this request from queue Q and rent a temporary server to execute the request
16. End Event
17. Event - A service on the temporary server is completed
18.
Release the temporary server
19. End Event
Denote by Wm
i (unit: second) the maximum allowed waiting time of service request
i in the queue before it is served. When a consumer submits a service request i to the
system which is then put into the queue, the system records Wm
i and starts counting its
waiting time Wi. The requests are assigned and executed on the long-term rented servers
according FCFS discipline. Once Wi of service request i reaches Wm
i , a temporary
short-term server is rented from infrastructure provider to process the request [4]. The
time that spending on renting activity is very short, so this time is ignored in this paper.
220
Z. Kang and B. Yang

As the long-term part is modeled as an M/M/m queuing system, then the
multi-server system builds on the combined renting scheme can be modeled as an
M/M/m queuing system with impatient consumers [4]. Denote by peðWi\Wm
i Þ the
steady-state probability that Wi reaches Wm
i for service request i. Then, service request
i will be served on a temporary short-term rented server according to the probability
peðWi\Wm
i Þ, and will be served on the long-term part according to the probability
1  peðWi\Wm
i Þ. peðWi\Wm
i Þ can be calculated next section.
3
peðWi\Wm
i Þ Under Variate Maximum Allowed Waiting
Time
In related work [4, 5, 7], Wm
i for all i in the queue is assumed to be a constant. This
assumption may not be realistic, such as for a service request i, the maximum allowed
response time may be shorter than Wm
i , which will lead to lower QoS. So the condition
that the Wm
i may change with different service requests is considered.
In this paper, the QoS of service request i is reﬂected by the response time Ti (unit:
second). Denote by Tm
i (unit: second) the maximum allowed response time that service
request i can tolerate before it is completed. Tm
i is assumed to be linearly proportional
to the service size ri, namely Tm
i / ri, i.e., Tm
i ¼ d  ri, where d is a positive constant
and determined by the service-level agreement (SLA). It can be noted that the waiting
time plus the execution time is the response time, i.e., Ti ¼ Wi þ xi. Then
Wm
i ¼ Tm
i xi ¼ kri;
ð5Þ
where
k ¼ ds  1
s
:
ð6Þ
According to assumption (iii), the service sizes ri’s are i.i.d. exponential r.v.’s with
the parameter l, so Wm
i (i 2 {1,2,3,…}) are i.i.d. exponential r.v.’s with the parameter
l=k. The probability density function (p.d.f.) of Wm
i is
fWm
i ðtÞ ¼ l
k el
kt;
ð7Þ
and the mean of Wm
i is k=l, where k is given by Eq. (6).
According to [10], the probability of the event Wi\Wm
i is:
PrðWi\Wm
i Þ ¼
Z 1
0
FwiðtÞfWm
i ðtÞdt:
ð8Þ
A Study of Optimal Multi-server System Conﬁguration
221

Substitute (2) and (7) into (8), and after some manipulation, we have
PrðWi\Wm
i Þ ¼ 1 
pml
ð1  qÞ kmdð1  qÞ þ l
½
 :
ð9Þ
Therefore, the probability of complementary event of Wi\Wm
i , PrðWi\Wm
i Þ, is:
PrðWi\Wm
i Þ ¼ 1  PrðWi\Wm
i Þ ¼
pml
ð1  qÞ½kmdð1  qÞ þ l :
ð10Þ
In PrðWi\Wm
i Þ, all service requests, in spite of exceeding their maximum allowed
waiting times, will be waiting in the queue. However, in the combined rented scheme
multi-server system, the service request whose waiting time reaches its maximum
allowed waiting time will be removed out of the queue and assigned to a temporary
short-term server, which will reduce the waiting time of the following requests.
Therefore, it will reduce the probability that the waiting time reaches maximum
allowed waiting time of the following service requests. According to [11],
peðWi\Wm
i Þ is as:
peðWi\Wm
i Þ ¼ ð1  qÞ PrðWi\Wm
i Þ
1  q PrðWi\Wm
i Þ ;
ð11Þ
where q is given by (1), PrðWi\Wm
i Þ is given by (10).
It can be seen from (5), (6), (10) and (11) that peðWi\Wm
i Þ is affected by d.
Figure 3 illustrates the peðWi\Wm
i Þ with different d, where k ¼ 5:99, l ¼ 1, m ¼ 6
and s ¼ 1 [4].
4
Rental Price Model
In this section, a new rental price model is described. In related work [4], Mei et al.
deemed that the servers with different execution speed have the same rental price. Hence,
they assume all long-term servers have the same rental price b (unit: cents per second) for
0
5
10
15
20
25
30
35
40
45
50
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
d
pe(Wi<Wi
m)
The probability that waiting time Wi exceeding Wi
m
Fig. 3. The probability of Wi exceeding Wm
i .
222
Z. Kang and B. Yang

each server, and all short-term servers have the same rental price c (unit: cents per second)
for each server as well, where b\c. This assumption may not be realistic.
In practice, the rental price for a server normally changes with the execution speed
of the server, i.e., a server with higher execution speed normally has a higher rental
price. In this paper, this condition is taken into account. Denote by s0 (unit: billion
instructions per second) the baseline execution speed [3]. The rental price of a
long-term rented server with execution speed s0 is b0 (unit: cents per second), and the
rental price of a short-term rented server with execution speed s0 is c0 (unit: cents per
second), where b0\c0. The infrastructure provider maintains some kinds of servers
with different execution speed s. s can be either higher or lower than s0, but s 2 S,
where S is the set of the possible execution speed. The service provider can select any
kind of server with the speed limited in S to rent.
Assume that there are two relation expressions between execution speed and the rental
price for long-term rented server and short-term rented server, respectively, as follows:
b
b0
¼
s
s0
 s
ð12Þ
c
c0
¼
s
s0
 s
ð13Þ
Hence, the rental price of a long-term or short-term rented with the execution speed
s can get from (12) and (13), respectively, as follows:
b ¼ b0
s
s0
 s
¼ b0
ss
0
ss
ð14Þ
c ¼ c0
s
s0
 s
¼ c0
ss
0
ss
ð15Þ
Figure 4 shows the rental price b and c with different execution speed, where
s0 ¼ 1:0, b0 ¼ 1:5 and c0 ¼ 3:0. In Fig. 4, s ¼ 0:5 and s ¼ 1 for (a) and (b),
respectively.
(a)
(b) 
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
1
2
3
4
5
6
s
β and γ
τ =0.5
β
γ
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
1
2
3
4
5
6
7
8
s
β and γ
τ =1
β
γ
Fig. 4. Rental prices b and c with different execution speeds s.
A Study of Optimal Multi-server System Conﬁguration
223

5
Optimal Multi-server System Conﬁguration
To determine the optimal conﬁguration (m, s), the optimal number m and execution
speed s of the servers in the multi-server system, such that the service provider can gain
the maximum proﬁt, an optimization model will be established in this section.
Denote by pf(t) (unit: cents) and R(t) (unit: cents) the proﬁt and the revenue of the
multi-server system during the time interval ð0; t, respectively. Denote by Cl(t) and
Cs(t) the cost of the multi-server system on long-term servers and the short-term servers
respective during the time interval ð0; t, including the rental cost and the electricity
cost. Then the proﬁt of multi-server system during the time interval ð0; t can be
calculated as:
pf ðtÞ ¼ RðtÞ  CsðtÞ  ClðtÞ:
ð16Þ
The mean of pf(t) is
E½pf ðtÞ ¼ E½RðtÞ  E½CsðtÞ  E½ClðtÞ:
ð17Þ
According to [10], the follow proposition can be got.
Proposition 1. {Ns(t)} and {Nl(t)} are both Poisson processes having respective rate
kpeðWi\Wm
i Þ
and
k½1  peðWi\Wm
i Þ.
Furthermore,
the
two
processes
are
independent.
According to Proposition 1, the mean of Ns(t) is
E½NsðtÞ ¼ kpeðWi\Wm
i Þ  t;
ð18Þ
and the mean of Nl(t) is
E½NlðtÞ ¼ k½1  peðWi\Wm
i Þ  t:
ð19Þ
As the combined rented scheme can guarantee the QoS to all service requests, in
steady-state, the revenue during the time interval ð0; t can be represented as
RðtÞ ¼
XNðtÞ
i¼1 ari;
ð20Þ
where a is a positive constant, which indicates the price per billion instructions (unit:
cents per billion instructions).
According to [10], R(t) is a compound Poisson process, with the mean
E½RðtÞ ¼ akt  EðriÞ ¼ akt
l :
ð21Þ
224
Z. Kang and B. Yang

The cost on the short-term rented servers in steady-state, Cs(t), is calculated as:
CsðtÞ ¼
XNsðtÞ
i¼1
ri
s ðc þ wPÞ ¼ c þ wP
s
XNsðtÞ
i¼1 ri;
ð22Þ
where P is given by (4), c is given by (15), w is introduced in Sect. 2.3.
According to (18) and (21), we have
E½CsðtÞ ¼ c þ wP
s
 E
XNsðtÞ
i¼1 ri
h
i
¼ c þ wP
sl
kpeðWi\Wm
i Þ  t:
ð23Þ
The cost on the long-term rented servers in steady-state, Cl(t), is calculated as:
ClðtÞ ¼ mbt þ wplðtÞ;
ð24Þ
where
PlðtÞ ¼ m 
PNlðtÞ
i¼1 ri
ms
Pd þ Pt
 
!
;
ð25Þ
and b is given by (14). In Eq. (25), Pd is given by (3), and P* is introduced in Sect. 2.3.
According to (19), we have
E½
XNlðtÞ
i¼1 r ¼ ½1  peðWi\Wm
i Þkt  EðrÞ ¼ ½1  peðWi\Wm
i Þkt
l
;
ð26Þ
then
E½PlðtÞ ¼ m 
½1  peðWi\Wm
i Þkt
mls
Pd þ Pt


¼ m  f½1  peðWi\Wm
i ÞqPd þ Pgt;
ð27Þ
furthermore,
EðClðtÞÞ ¼ mbt þ w  E½PlðtÞ
¼ m  fb þ w½ð1  peðWi\Wm
i ÞÞqPd þ Pgt:
ð28Þ
Therefore, substitute (3), (21), (23) and (28) into (17), we have
E½pf ðtÞ ¼ akt
l  c þ cP
sl
kpeðWi\Wm
i Þt
 m  fb þ w½ð1  peðWi\Wm
i ÞÞqnsa þ Pgt:
ð29Þ
A Study of Optimal Multi-server System Conﬁguration
225

Then
E½pf ðtÞ
t
¼ ak
l  c þ cP
sl
kpeðWi\Wm
i Þ
 m  fb þ w½ð1  peðWi\Wm
i ÞÞqnsa þ Pg:
ð30Þ
The key issue of the multi-server system conﬁguration problem is to determine the
optimal values of m and s of the servers so that the service provider can get the
maximum proﬁt. As the proﬁt can calculated as follows:
pf ðm; sÞ ¼ ak
l 
c0ss1
ss
0l
þ cP
sl


 kpeðWi\Wm
i Þ
 m  fb0
ss
0
ss þ w½ð1  peðWi\Wm
i ÞÞqnsa þ Pg:
ð31Þ
The problem of optimal multi-server system conﬁguration for proﬁt maximization
can be established as the following optimization model:
Maximize pf ðm; sÞ
ð32Þ
Subject to
k
l  ms;
ð33Þ
m 2 f1; 2; 3; . . .g;
ð34Þ
s 2 S:
ð35Þ
Equations (33)–(35) are the constraints of the established optimization model.
Equation (33) indicates that the system service intensity is less than 1. Equation (34)
denotes that the number of servers should be positive integer. The available execution
speed levels of the server are limited in set S, which is indicated by Eq. (35). In the
following section, some experiments are studied to testify the performance of we
proposed optimization model.
6
Experimental Study
The proposed optimization model can be solved by any optimization algorithms, such
as Genetic Algorithm, Ant Colony Algorithm, Simulated Annealing Algorithm, etc. In
this paper, we use Genetic Algorithm to solve the optimization model and compare the
results with those obtained from the optimization model in [4].
In [4], Wm
i , b and c are all deemed to be constant. In order to increase the prac-
ticality of the proposed optimization model than [4], the conditions that the maximum
allowed waiting times of services are r.v.’s and the rental price changed with the
226
Z. Kang and B. Yang

execution speed are taken into account. In order to distinguish the proposed opti-
mization model and the compared model, the proposed model is named as new model
and the compared model is named as old model in this paper.
In order to use optimal algorithm to solve the new model and old model, it is
necessary to get a closed-form expression of pm. In this paper, we use the same
closed-form expression as [3, 4], which is P
m1
k¼0
ðmqÞk
k!
 emq. This expression is accurate
when m is not too small and s is not to large [14]. According to Stirling’s approxi-
mation of m!, i.e.,
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2pm
p
m
e
 m [16], one closed-form expression of pm is given by
pm ¼
1q
ð1qÞ ﬃﬃﬃﬃﬃﬃ
2pm
p
eq
eq
ð Þ
m þ 1. In the following, we solve new model and old model based on
above closed-form expression of pm.
6.1
Parameters Setting
In the compared old model, k ¼ 5:99, a ¼ 15, P ¼ 3, c ¼ 0:3, a ¼ 2:0, n ¼ 9:41292,
r ¼ 1, D ¼ 5, b ¼ 1:5, c ¼ 3 and S ¼ 0:2; 0:4; . . .; 2
f
g [4].
In the proposed new model, k, a, P*,n, a and S are sat as the same value with old
model. w ¼ 0:3, b0 ¼ 1:5, c0 ¼ 3, s0 ¼ 1, l ¼ 1, so that the mean of service size r in
our proposed new model equals with the mean service size r in Mei et al.’s work,
namely EðrÞ ¼ r ¼ 1, and set d ¼ 1
s þ 5, so that k ¼ 5 and EðWm
i Þ ¼ D ¼ 5.
Denote by (m*, s*) and pf
* the optimal conﬁguration and maximum proﬁt
respective to old model. Denote by (m, s) and pf the optimal conﬁguration and max-
imum proﬁt respective to new model. The proﬁt of conﬁguration (m*, s*) obtained in
new model is denoted by pf’.
6.2
Performance Comparison
Table 1 shows the obtained optimal conﬁguration and maximum proﬁt of old model. It
can be seen from Table 1, when the multi-server system rents 10 long-term servers with
execution speed 1 billion instructions per second, the service provider can gain max-
imum proﬁt 57.9124 cents interval a unit period time.
The QoS to services is guaranteed by the both models, so the revenue keeps same
constant in both models.
In the ﬁrst experiment, we explore the difference between conﬁguration (m, s) and
(m*, s*), and the difference between proﬁt pf and pf’, with different s0 when parameter
s ¼ 1. The results are showed in Table 2. And Fig. 5a shows the changing trend of pf
and pf’ with different s0 when s ¼ 1. We can see that the ﬁgure shows the increasing
trend of pf and pf’ when s0 is increasing from 0.2 to 2. That is because with the
Table 1. The optimal conﬁguration and maximum proﬁt obtained from old model.
m* s* pf*
6
1
57.9124
A Study of Optimal Multi-server System Conﬁguration
227

increment of s0, the rental prices b and c are both decreased. Hence the cost on
long-term servers and short-term servers are both decreased when the number and
execution speed of servers remain unchanged. In addition, the obtained proﬁt pf to the
new model proposed by this paper is always greater than the pf’, which indicates that
the new model can gain more proﬁt than old model by more reasonable conﬁguration.
It is can be seen from Table 2, the optimal conﬁguration obtained from new model is
10 servers with 0.6 execution speed, while that obtained from compared old model is 6
servers with 1.0 execution speed.
In the second experiment, the conﬁgurations (m, s) and (m*, s*), the proﬁts pf and
pf’ are compared, with different s0 when s ¼ 1:5, and the result is shown in Table 3.
Figure 5b shows the changing trends of pf and pf with different s0 when s ¼ 1:5. It can
Table 2. The difference between conﬁgurations (m, s) and (m*, s*), and the difference between
proﬁts pf and pf’, with different s0 when s ¼ 1.
s0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2
(m, s)
(10,0.6)
(10,0.6)
(10,0.6)
(10,0.6)
(10,0.6)
(10,0.6)
(10,0.6)
(10,0.6)
(10,0.6)
(10,0.6)
pf
23.1808
46.8233
54.7041
58.6445
61.0087
62.5849
63.7107
64.5551
65.2118
65.7372
(m*, s*)
(6,1.0)
(6,1.0)
(6,1.0)
(6,1.0)
(6,1.0)
(6,1.0)
(6,1.0)
(6,1.0)
(6,1.0)
(6,1.0)
pf’
19.9803
43.6800
51.5799
55.5299
57.8999
59.4798
60.6084
61.4548
62.1131
62.6398
(a) 
1
=
τ
(b) τ = 1.5
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
10
20
30
40
50
60
70
s0
profit
pf
pf ’
Fig. 5. The pf and pf’ with different s0.
Table 3. The difference between conﬁgurations (m, s) and (m*, s*), and the difference between
proﬁts pf and pf’, with different s0 when s ¼ 1:5.
s0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2
(m, s)
(30,0.2)
(15,0.4)
(15,0.4)
(15,0.4)
(15,0.4)
(10,0.6)
(10,0.6)
(10,0.6)
(10,0.6)
(10,0.6)
pf
11.8929
45.6617 56.4112 60.9118 63.2843 64.8931 66.0435 66.8462 67.4324 67.8758
(m*, s*)
(6,1.0)
(6,1.0)
(6,1.0)
(6,1.0)
(6,1.0)
(6,1.0)
(6,1.0)
(6,1.0)
(6,1.0)
(6,1.0)
pf’
−38.608 29.9072 46.9823 54.1312 57.8999 60.1681 61.6569 62.6957 63.4542 64.0281
228
Z. Kang and B. Yang

be seen that the changing trends of two curves are similar from Fig. 5a When s0 is 0.2,
the value of pf’ is a negative. Which indicates that the optimal conﬁguration gained
from old model isn’t much truthful, when s ¼ 1:5, and s0 = 0.2.
In the third experiment, we explore the difference between conﬁguration (m, s) and
(m*, s*), and the difference between proﬁt pf and pf’, with different s when s0 ¼ 0:6.
And the results are showed in Table 4. We can see that the conﬁguration (m, s) changes
from (6, 1) to (15, 0.4), when s changes from 0.5 to 1.5, then (m, s) remain unchanged
when s increases. Figure 6a shows the changing trends of proﬁt pf and pf’ with
different s when s0 ¼ 0:6. It is can be seen from the ﬁgure, the two curves show
different changing trends, i.e., the pf shows increased trend, but pf’ shows decreased
trend. That is because when s is greater or less than 0.6, b and c show different trend
with the increasing of s, i.e. b and c show decreased trend when s is greater than 0.6,
but show increased trend when s is greater than 0.6. Furthermore, pf and pf’ shows
different changing trends.
Table 5 shows the results of the difference between conﬁguration (m, s) and (m*, s*),
and the difference between proﬁt pf and pf’ are explored with a given s when s0 ¼ 1:2.
Figure 6b shows the changing trends of proﬁt pf and pf’ with different s when s0 ¼ 1:2.
From the ﬁgure, we can see that the proﬁt pf and pf’ show increasing trend when s is
increasing from 0.5 to 2.5. That is because with the increasing of s, b and c are both
decreased when s is not more than 1.0. It is shown in Table 4, s and s* are not more than
1, so b and c are both decreased with the increase of s in both conﬁgurations. Although
with the increase of the number of servers the rental cost shows increase trend, the
decrease trend is deeper with the increase of the execution speed of servers, so the rental
cost integrally shows increase trend. As a consequence, the proﬁt of the multi-server
system increases. In addition, the proﬁt obtained of our proposed new model is greater
than the proﬁt of compared old model. And with the increasing of s from 0.5 to 2.5, the
gap is more and more great. It is can be seen from Table 4, the optimal conﬁguration
obtained from this paper proposed new model is 10 servers with 0.6 execution speed
when s is 0.5 and 1, 15 servers with 0.4 execution speed when s is from 1.5 to 2.5, while
the counterpart obtained from compared old model is 6 servers with 1 execution speed.
In essence, the difference of conﬁguration leads to the gap of proﬁt.
Table 4. The difference between conﬁgurations (m, s) and (m*, s*), and the difference between
proﬁts pf and pf’, with different s when s0 ¼ 0:6.
s
0.5
1
1.5
2
2.5
(m, s)
(6,1)
(10,0.6) (15,0.4) (15,0.4) (15,0.4)
pf
55.1413 54.7041 56.4112 58.7676 61.1497
(m*, s*) (6,1.0)
(6,1.0)
(6,1.0)
(6,1.0)
(6,1.0)
pf’
55.1413 51.5799 46.9823 41.0467 33.3840
A Study of Optimal Multi-server System Conﬁguration
229

7
Conclusions
The problem of optimal multi-server system conﬁguration for proﬁt maximization in a
three-tier cloud environment is investigated in this paper. We propose an optimization
model to determine the optimal conﬁguration such that the service provider can gain
the maximum proﬁt. There are two major differences with existing work. First, the
maximum allowed waiting time is deemed random variable, which may change with
the different service request. Second, the condition that the rental price of different
execution speed may be different is taken into consideration. In addition, the perfor-
mance of our proposed optimization model is veriﬁed by many experiments. The
results of experiments show that the optimization model proposed in this paper can
help the service provider gain more proﬁt than existing work.
Acknowledgements. This work is supported by Sichuan Provincial Project of International
Scientiﬁc and Technical Exchange and Research Collaboration Programs (Project No. 2016H
H0023).
(a) 
0.6
0 =
s
 
 
 
 
 
 (b) 
1.2
0 =
s
0.5
1
1.5
2
2.5
30
35
40
45
50
55
60
65
70
profit
pf
pf ’
0.5
1
1.5
2
2.5
58
60
62
64
66
68
70
profit
pf
pf ’
Fig. 6. The pf and pf’ with different s.
Table 5. The difference between conﬁgurations (m, s) and (m*, s*) and the difference between
proﬁts pf and pf’ with different s when s0 ¼ 1:2.
s
0.5
1
1.5
2
2.5
(m, s)
(10,0.6) (10,0.6) (15,0.4) (15,0.4) (15,0.4)
pf
59.3206 62.5849 64.7123 66.6311 67.7390
(m*, s*) (6,1.0)
(6,1.0)
(6,1.0)
(6,1.0)
(6,1.0)
pf’
58.7258 59.4798 60.1681 60.7965 61.3701
230
Z. Kang and B. Yang

References
1. Lee, Y.C., Wang, C., Zomaya, A.Y., Zhou, B.B.: Proﬁt-driven service request scheduling in
clouds. In: Proceedings of the 10th IEEE/ACM International Conference on Cluster, Cloud
and Grid Computing, pp. 15–24 (2010)
2. Chen, J., Wang, C., Zhou, B.B., Sun, L., Lee, Y.C., Zomaya, A.Y.: Tradeoffs between proﬁt
and customer satisfaction for service provisioning in the cloud. In: Proceedings of the 20th
International Symposium on High Performance Distributed Computing, pp. 229–238. ACM
(2011)
3. Cao, J., Hwang, K., Li, K., Zomaya, A.Y.: Optimal multiserver conﬁguration for proﬁt
maximization in cloud computing. IEEE Trans. Parallel Distrib. Syst. 24(6), 1087–1096
(2013)
4. Mei, J., Li, K., Ouyang, A., Li, K.: A proﬁt maximization scheme with guaranteed quality of
service in cloud computing. IEEE Trans. Comput. 64(11), 3064–3078 (2015)
5. Li, K., Mei, J., Li, K.: A fund-constrained investment scheme for proﬁt maximization in
cloud computing. IEEE Trans. Serv. Comput. (2016). IEEE Early Access Articles
6. Ghamkhari, M., Mohsenian-Rad, H.: Energy and performance management of green data
centers: a proﬁt maximization approach. IEEE Trans. Smart Grid 4(2), 1017–1025 (2013)
7. Liu, Z., Wang, S., Sun, Q., Zou, H., Yang, F.: Cost-aware cloud service request scheduling
for SaaS providers. Comput. J. 57, 291–301 (2013)
8. de Langen, P., Juurlink, B.: Leakage-aware multiprocessor scheduling. J. Sig. Process. Syst.
57(1), 73–88 (2009)
9. Mei, J., Li, K., Hu, J., Yin, S., Sha, E.H.-M.: Energy-aware preemptive scheduling algorithm
for sporadic tasks on DVS platform. Microprocess. Microsyst. 37(1), 99–112 (2013)
10. Ross, S.M.: Introduction to Probability Models, 11th edn. Elsevier, London (2014)
11. Boots, N.K., Tijms, H.: An M/M/c queue with impatient customers. Top 7(2), 213–220
(1999)
12. Ding, Y., Qin, X., Liu, L., Wang, T.: Energy efﬁcient scheduling of virtual machines in
cloud with deadline constraint. Future Gener. Comput. Syst. 50, 62–74 (2015)
13. Enhanced Intel® SpeedStep® technology for the Intel® Pentium® M processor. White
Paper, Intel, March 2004
14. Li, K.: Optimal conﬁguration of a multicore server processor for managing the power and
performance tradeoff. J. Supercomput. 61(1), 189–214 (2012)
15. Chen, J., Wang, C., Zhou, B.B., Lee, Y.C., Zomaya, A.Y.: Tradeoffs between proﬁt and
customer satisfaction for service provisioning in the cloud (2011)
16. https://en.wikipedia.org/wiki/Stirling’s_approximation (2016)
17. Ghamkhari, M., Mohsenian-Rad, H.: Energy and performance management of green data
centers: a proﬁt maximization approach. IEEE Trans. Smart Grid 4(2), 1017–1025 (2013)
18. Li, K., Liu, C., Zomaya, A.Y.: A framework of price bidding conﬁgurations for resource
usage in cloud computing. IEEE Trans. Parallel Distrib. Syst. 27(8), 2168–2181 (2016)
A Study of Optimal Multi-server System Conﬁguration
231

A Composite Anomaly Detection Method
for Identifying Network Element Hitches
Duo Zhang1(&), Yi Man1, and Ligang Ren2
1 ICN & CAD Center, Beijing University of Posts and Telecommunications,
Beijing, China
zhangduobupt@163.com
2 China Unicom System Integration Limited Corporation, Beijing Branch,
Beijing, China
15611030998@wo.com.cn
Abstract. Based on time-series detection algorithm, this paper puts forward a
new analysis method for identify Network Element (NE) hitches. Aiming at
speciﬁc characteristics of the NE, this paper propose a model which consider
seasonal timing characteristics and impact of current data from recent data.
Considering of multi-dimensional characteristics of NE, a density-based dis-
covery algorithm is introduced into the modeling process. Experiments on the
actual data coming from operates demonstrate the effectiveness and accuracy of
the proposed methods.
Keywords: Big data  Abnormity detection  Network element management
1
Introduction
In the ﬁeld of network management, fault warning is a very advanced subject. First of
all, the traditional method to deal with faults is to remedy the situation after mal-
function, which is neither predictable nor effective. In this situation, operation and
maintenance work will faces many difﬁculties. As a direct manifestation of devices
status, the data which comes from network element can reﬂect the state of the device. In
addition, it can helps operators to analyze and make decisions more accurately.
The data structure and data mining are carried out to explore the abnormal data in
the network equipment, which can helps user to quickly identify the possible failure
and achieve the target of early warning. Nevertheless, there are great challenges analyst
will meet in operator’s network management environment, because of the diversity and
complexity of the data comes from network elements. It’s also worth mentioning that
the method to get structured log data has objective impact on the result of anomaly
detection. Consequently, a comprehensive anomaly identiﬁcation scheme for network
element log data is required for operation and maintenance work.
Taking into account the different needs of operator business analysis, we propose
diverse analytics solutions to match different business scenarios. In this paper, we
propose a time-series-based abnormity detection method for identify network element
hitches. Meanwhile, we propose a solution to meet the needs of high dimension
analysis. In the experimental phase, we use actual fault data provided by the operator to
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 232–244, 2018.
https://doi.org/10.1007/978-3-319-74521-3_26

evaluate the precision of our algorithm. The experiment results indicate that our
methods can ﬁnd abnormal state of network element efﬁciently and accurately.
2
Related Work
The current anomaly detection algorithms can be divided into four categories: statistical
method, clustering-based method, distance-based method and density-based method.
Meanwhile, density-based anomaly detection is a hot topic in this ﬁeld. Based on the
local density anomaly detection algorithm [1] proposed by Breunig in 2000. Following
researchers gradually improved and developed the algorithm to make it suitable for
different scenarios. Some researchers attempted to use inexpensive local statistics to
reduce the sensitivity of choosing parameter k [2]. In the paper of Lazarevic [3], Local
Outlier Factor (LOF) is used on multiple projections and combines the results for
improved detection qualities in high dimensions. [4, 5] presents effective methods to
measure the similarity between objects, which can increase the stability and precision
of outlier detection. But these algorithms is difﬁcult to support large-scale data pro-
cessing needs, because of the complexity of the network element’s log data. Alterna-
tively, considering of the time characteristics of log data, timing analysis is another
feasible way to detect anomalies. In the ﬁeld of time series analysis, moving average
model, linear regression model, polynomial regression model and exponential
smoothing are common algorithms for time series analysis. In statistics, a moving
average (MA) is a calculation to analyze data points by creating series of averages of
different subsets of the full data set [6]. The weighted moving average model (WMA) is
commonly used in the analysis of natural sciences and economics [7, 8]. Besides these,
linear regression was the ﬁrst type of regression analysis to be studied rigorously, and
to be used extensively in practical applications [9]. The polynomial regression model
which is advanced by Gergonne [10, 11] had been applied extensively in a lot of
professions. Exponential smoothing was ﬁrst suggested by Robert Goodell Brown in
1956 [12], and one of the commonly used model is known as “Brown’s simple
exponential smoothing” [13]. In exponential smoothing model, the current time can be
suspected by the past time, while the impact of recent time is stronger than remote time.
Another practical model to analysis time series is autoregressive integrated moving
average model (ARIMA) [14, 15], this model can predict the future value of a speciﬁed
time series on the basis of its past performance, but the basic ARIMA can’t retain the
seasonal characteristics of a time series.
All above models have their own advantages in time series analysis, and are
effective mediums for anomaly detection. In the process of analysis of network ele-
ment’s log data, both statistical method and density-based method can be reasonable
applied based on their properties.
A Composite Anomaly Detection Method
233

3
Anomaly Detection Based on Network Element Log Data
3.1
Anomaly Detection Solution Design
There are different concerns in the process of network element hitches detection. To ﬁt
different demands from network management analysts, we propose a composite
anomaly detection method to identify network element hitches. The anomaly detection
procedure is showed in Fig. 1.
If network management analysts would like to detect anomaly for a speciﬁc mode,
they can choose our time-series-based scheme. In the process of speciﬁc anomaly
detection, we combine qualitative analysis with quantitative analysis. On one hand,
generally determine the presence of abnormal period by creating ARIMA model. On
the other hand, use our comprehensive time series model to detect speciﬁc anomalies. If
network management analysts would like to contain the features of network element in
analysis process as much as possible, they may choose our LOF-based scheme.
Considering of high dimension input, we use principal component analysis (PCA) to
reduce the dimension and retain characteristics of raw data. In this scheme, analysts can
get global anomalies from log data.
3.2
Qualitative Analysis Based on ARIMA Model
In autoregressive integrated moving average model [15], the data sequence formed by
the prediction object over time is regarded as a random sequence. Once the model is
identiﬁed, it can be used to predict the future value from the past and present values of
the time series.
Fig. 1. Anomaly detection procedure
234
D. Zhang et al.

– The autoregressive integrated moving average model is deﬁned as
ARIMAðp; d; qÞ
ð1Þ
p denotes autoregressive lagged item, q denotes moving average lagged item, and d
denotes the order of difference. In our method we use ARIMA model to draw the curve
of speciﬁc mode which can help us ﬁnd suspected anomalies qualitatively.
3.3
Quantitative Analysis Based on Comprehensive Time Series Model
We can deﬁne current time t, the predicted value of our comprehensive time series
model Pt is comprehensive decided by the continuous time smoothing factor Ct and the
seasonal average factor St. The continuous time smoothing factor Ct represents the
impact of recent trends on current time. While the seasonal average factor takes into the
impact from the same period.
– The continuous time smoothing factor Ct is deﬁned as
Ct ¼ byt þ ð1  bÞCt1
ð2Þ
where yt is the measured value at time t and b denotes the weight coefﬁcient which can
measure the impact of recent trends on current time.
– The seasonal average factor St is deﬁned as
St ¼ Yt ¼ 1
n
X
n
i¼1
Yi
ð3Þ
Yi denotes the time units which are at the same period of Yt, n denotes the number
of time units we concerned about.
– The predicted value of comprehensive time series model Pt is deﬁned as
max
a
aCt þ ð1  aÞSt
½

s:t: a  0:
ð4Þ
In this manner, we can reduce the inﬂuence of single factor on predicting value,
also increase the tolerance of extremely value. Is easy to understand that Pt retains the
impact of recent trends and the relevance of data in the same period, which can predict
the value of speciﬁed mode’s time series at time t reasonably and effectively.
3.4
High Dimensionality Data Processing Scheme
Considering of multi-dimensional characteristics of NE, we introduce a LOF-based
algorithm to detect global outliers, which can encapsulate properties as node attributes.
In the actual network management scenario, traditional LOF-based algorithm can’t
solve the problem when the data volume is extremely huge. In other words, the
A Composite Anomaly Detection Method
235

structured log data from network element has a large variety of characteristics, the
traditional LOF algorithm will meet the performance bottleneck in computing process.
Therefore, the PCA method is used to reduce the dimensions of input data, while
preserving the feature information of multi dimension data.
We use Euclidean distance to measure the difference between network element’s
log data in different periods.
dðxðt1Þ  xðt2ÞÞ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
X
n
i¼1
ðxiðt1ÞÞ  xiðt2ÞÞ2
s
ð5Þ
The abnormal degree of log data is measured by local outlier factor (LOF) [1]. And
the local outlier factor (LOF) is deﬁned as
LOFkðpÞ ¼
P
o2NkðpÞ
lrdkðoÞ
lrdkðpÞ
NkðpÞ
j
j
ð6Þ
where NkðpÞ denotes the k-distance neighborhood of p. In other words, NkðpÞ contains
every element whose distance from p is not greater than the k-distance. The k-distance
of p is deﬁned as the distance dðp; oÞ between p and an object o such that there are at
least k objects m 2 D dðp; mÞ  dðp; oÞ, and there are at most k  1 objects
m 2 D dðp; mÞ\dðp; oÞ
The local reachability density of p is deﬁned as
lrdkðpÞ ¼
NkðpÞ
j
j
P
o2NkðpÞ
rdkðp; oÞ
ð7Þ
where the reachability distance of object p with respect to object o is deﬁned as
rdkðp; oÞ ¼ max½k  distance(oÞ; dðp; oÞ
ð8Þ
For a speciﬁed network element, the statistical distribution of log modes per hour
form snapshots which can reﬂect the status of the network element. According to the
deﬁnition of local outlier factor (LOF) [1], the higher the value of local outlier factor,
the more likely the object is an anomaly. Based on this, we abstract the status of
network element per hour as objects (denoted as xðtÞ) which can calculate their
abnormal degree by LOF. Getting the values of LOF for every xðtÞ, we can infer the
occurrence time of anomalies.
3.5
Scheme Characteristic Analysis
Our model is based on time series analysis, taking into account the comprehensive
effect of continuous temporal and seasonal timing. In our model, we use the opti-
mization algorithm to optimize the parameters, which can avoid the impact of extreme
value. It’s suitable for network management personnel who are familiar with speciﬁc
236
D. Zhang et al.

abnormal patterns of NE’s log data. Obviously, for network management analysts this
method is effective, accurate and targeted-oriented. And that is to say, this model can
effectively ﬁnd the anomaly of speciﬁc log mode.
The global abnormity detection of network element considering of multiple attri-
butes of network logs, which can retain the comprehensive information of network
element’s log data. But the existence of relevance and the mutual inﬂuence between
attributes also has effects on detect results. Therefore, it’s suitable for network man-
agement personnel have a global perception of abnormity. And that is to say, network
management personnel can choose to ﬁnd targeted anomaly and global abnormity
ﬂexibly according to different scenarios.
4
Experiments
4.1
The Analysis of Algorithm
To detect anomalies of the speciﬁc mode, we input the time series of a speciﬁc mode, yt
denote the frequency of the mode per hour. After input the time series, the compre-
hensive time series analysis algorithm will ﬁnd which time units are anomalies
(Tables 1 and 2).
Considering of high dimension input, we use LOF-based model to ﬁnd global
anomalies.
Table 1. The comprehensive time series analysis algorithm
A Composite Anomaly Detection Method
237

4.2
Log Data Structure
Word2Vec is used to distinguish the difference between logging modes, meanwhile to get
structured log data. We use the SQL aggregation statement to aggregate the data to get the
frequency of modes per hour. The result of aggregation operation is showed in Table 3.
As can be seen in Table 3, all log modes are coded by hash algorithm, the last
column indicates the frequency of modes per hour. In order to satisfy our analysis
conditions, we convert the data to the following format.
In Table 4, the frequency of diverse log modes can be abstracted as attributes of log
status in Network Element. This form is useful for network management to positioning
speciﬁc log mode. Moreover, it can be processed by dimensional reduction algorithms
when the dimension of attributes is extremely high.
4.3
ARIMA Analysis on Operator Actual Data
In this part, we use ARIMA-based model to qualitative analysis the time series of the
speciﬁc log mode. In order to facilitate the description we select one of the speciﬁc log
modes which network management analysts are interested in. The analysis method of
other log modes are just the same (Table 5).
Firstly, we select a speciﬁc log mode as input for ARIMA model. Secondly, we
generate the distribution curve of Autocorrelation Function (ACF) and Partial Auto-
correlation Function (PACF) of the log mode’s time series.
In Fig. 2, the Autocorrelation Function (ACF) trend decay and the Partial Auto-
correlation Function (PACF) truncate after 1 order. This speciﬁc mode satisfaction the
distribution of ARIMA(1,0,0). Therefore, we can create an ARIMA(1,0,0) model, and
draw the curves of raw data and the predicted data of ARIMA.
As can be seen in Fig. 3, ARIMA(1,0,0) model well captured the trend of the
time-series of the speciﬁc log mode. Moreover, we can ﬁnd that there are great dif-
ferences between the raw data and predicted data. So we can conclude that there are
suspected anomalies in 2016-8-17 and 2016-8-19.
Table 2. LOF-based anomaly detection algorithm
238
D. Zhang et al.

Table 3. Input data structure
8f6da0d89a8252e01944d8863786d52b
2016-08-16 00 1103
a631a85c804c2bdc2afc80c596b0e005
2016-08-16 00
724
661770dbcd0394f8db00ddd74352c007
2016-08-16 00
723
78e3e5d53db84e87a09a3996b09fdbb2
2016-08-16 00
719
f24f62eeb789199b9b2e467df3bl876b
2016-08-16 00
410
fd7997adb870d78fa830348b5514fc0d
2016-08-16 00
238
4586e5d1ace7df6ac23cce10ee8dfdb4
2016-08-16 00
143
ff4b2749ab7483d8f9a7a89d70e08c43
2016-08-16 00
143
478458696bcf7c72812ee5e623dedfda
2016-08-16 00
132
13e208b8db3b0c2f9d956c02253cab1aa 2016-08-16 00
132
f4094a94a79a121bcd8a10960fe34a61
2016-08-16 00
126
8424707d0420cadee4c97ef4af2ef9f4
2016-08-16 00
112
f20e457d2828982ee27a640362891d63
2016-08-16 00
89
C3f091f048a28894fea7d50660dd6942
2016-08-16 00
89
e008518e807af2bcc2356cf7ecd57b22
2016-08-16 00
89
004f4855ba92c8781a6b8c2014fd9c2
2016-08-16 00
60
75849e828d2b48945d6a6453485b59ac
2016-08-16 00
15
731c019558bf9ed8ff96dc892e9b5325
2016-08-16 00
15
9d256115355016cf657aa064a9f04862
2016-08-16 00
12
6dbe23d02c718ad2118aef94479dl32b
2016-08-16 00
11
1dd5ddd339089a28a3b1c41dabfa87eb
2016-08-16 00
11
1b78efaa73d320280808e0b361b206bd
2016-08-16 00
11
13b97cb2bc33bd94b82e2c6b9c637725
2016-08-16 00
8
609c79b375b6a32fa86a56f4136e5734
2016-08-16 00
8
a15aa8107c651ce08ce8375746717ed1
2016-08-16 00
8
Table 4. Input data transformation
A Composite Anomaly Detection Method
239

Table 5. The speciﬁed mode
Fig. 2. Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) of
speciﬁc log mode
Fig. 3. Timing distribution of anomaly detection
240
D. Zhang et al.

4.4
Comprehensive Time Series Analysis on Operator’s Actual Data
After get the presence of abnormal period, furthermore, we use our comprehensive time
series analysis method to quantitative ﬁnd the anomalies. In this part we compare our
algorithm with other common time-series-based algorithms. We choose the log data
from network elements which has occurred hitches in recent year. For SAEGW1304, the
speciﬁed time range is 2016-08-16 00:00 to 2016-08-27 23:00. For another network
element SZHSAEGW105BEr, the time range is 2016-09-02 00:00 to 2016-10-11 23:00.
Experimental results show that our Comprehensive Time Series method can
improve precision ratio to ﬁnd anomalies of network element, but recall ratio greatly.
Fig. 4. Algorithm efﬁciency evaluation for SAEGW1304 and SZHSAEGW105BEr
A Composite Anomaly Detection Method
241

4.5
High Dimensionality Data Processing Scheme to Find Global
Abnormity
In LOF-based anomaly detection process, the frequency of diverse log modes can be
abstracted as attributes of log status in Network Element. We use LOF to measure the
degree of outlier, meanwhile ﬁnd global anomalies of log data.
In Fig. 5, we can ﬁnd that 2016-8-17 00:00 and 2016-8-16 23:00 are the global
anomalies of log data, which caused the hitch of Network Element (SAEGW1304).
Obviously, this method considering of multiple attributes of network logs, which can
retain the comprehensive information of network element’s log data. But the existence
of relevance and the mutual inﬂuence between attributes also has effects on detect
results.
4.6
Actual Case Veriﬁcation
In order to evaluate the accuracy of our detection method, we select a real case
provided by the operator. Applying our detection method on log data from network
element (SAEGW1304), we can verify the effectiveness of our method.
As can be seen in Tables 6 and 7, Comprehensive Time Series method detect the
anomalies by 100% recall ratio. Moreover, Fig. 4 shows that the precision ratio for
detection is higher than other common algorithms. Therefore, we can verify the
effectiveness of our method detection the abnormal status from SAEGW1304’s log
data.
Fig. 5. High dimensionality anomaly detection
Table 6. The record of hitches of SAEGW1304
Event description
Associated log
time
NE label
Data description
A hitch occurred in
Power Supply Bureau’s
network environment
Guangzhou’s network
element SAEGW1304
August 16, 2016
22:00-August 17,
2016 14:00
SAEGW1304
The data to be analyzed
comes from
SAEGW1304’s log data
242
D. Zhang et al.

5
Conclusion
In the background of efﬁcient system operation and maintenance, we propose diverse
analytics solutions to match different business scenario. In our method, the frequency
characteristics of log data are abstracted as attributes of network elements’ attributes.
Applying data mining algorithms to this data, we detect the speciﬁc anomalies and
global anomalies to ﬁt different demands from network management analysts. The
result of our analysis method is veriﬁed by actual data, meanwhile is useful for
operators’ fault early warning system. It can be expected to ﬁnd more interesting and
results along these lines, since the log data of network element contains abundant
information.
References
1. Breunig, M.: LOF: identifying density-based local outliers. ACM SIGMOD Rec. 29(2),
93–104 (2000)
2. Kriegel, H.P., Schubert, E., Zimek, A.: LoOP:local outlier probabilities. In: ACM
Conference on Information and Knowledge Management, CIKM 2009, Hong Kong, China,
DBLP, pp. 1649–1652, November 2009
3. Lazarevic, A., Kumar, V.: Feature bagging for outlier detection. In: Eleventh ACM SIGKDD
International Conference on Knowledge Discovery in Data Mining, pp. 157–166. ACM
(2005)
4. Schubert, E., Wojdanowski, R., Zimek, A., et al.: On evaluation of outlier rankings and
outlier scores. In: Proceedings of the 2012 SIAM International Conference on Data Mining,
pp. 1047–1058 (2012)
5. Guan, H., Li, Q., Yan, Z., et al.: SLOF: identify density-based local outliers in big data. In:
Web Information System and Application Conference, pp. 61–66. IEEE (2015)
Table 7. Comprehensive Time Series method detection result
46dcld901e25a7ff0dl9c9354e76d7a33 FALSE 2016-08-17 04
46dcld901e25a7ff0dl9c9354e76d7a34 FALSE 2016-08-17 05
46dcld901e25a7ff0dl9c9354e76d7a35 FALSE 2016-08-17 06
46dcld901e25a7ff0dl9c9354e76d7a36 TRUE
2016-08-17 07
46dcld901e25a7ff0dl9c9354e76d7a37 TRUE
2016-08-17 08
46dcld901e25a7ff0dl9c9354e76d7a38 TRUE
2016-08-17 09
46dcld901e25a7ff0dl9c9354e76d7a39 TRUE
2016-08-17 10
46dcld901e25a7ff0dl9c9354e76d7a40 TRUE
2016-08-17 11
40dcld90le25a7ff0dl9c9354e76d7a41
TRUE
2016-08-17 12
46dcld901e25a7ff0dl9c9354e76d7a42 TRUE
2016-08-17 13
46dcld901e25a7ff0dl9c9354e76d7a43 TRUE
2016-08-17 14
46dcld901e25a7ff0dl9c9354e76d7a44 TRUE
2016-08-17 15
46dcld901e25a7ff0dl9c9354e76d7a45 FALSE 2016-08-17 16
46dcld901e25a7ff0dl9c9354e76d7a46 FALSE 2016-08-17 17
46dcld901e25a7ff0dl9c9354e76d7a47 FALSE 2016-08-17 18
A Composite Anomaly Detection Method
243

6. Booth, E.G., Mount, J.F., Viers, J.H.: Hydrologic variability of the cosumnes river
ﬂoodplain. San Francisco Estuary Watershed Sci. 4(2), 1–19 (2006)
7. Domańska, J., Domański, A., Czachórski, T.: Fluid ﬂow analysis of RED algorithm with
modiﬁed weighted moving average. In: Dudin, A., Klimenok, V., Tsarenkov, G., Dudin, S.
(eds.) BWWQT 2013. CCIS, vol. 356, pp. 50–58. Springer, Heidelberg (2013). https://doi.
org/10.1007/978-3-642-35980-4_7
8. Glabadanidis, P.: Market timing with moving averages. Int. Rev. Finance 15(3), 387–425
(2015)
9. Yan, X., Su, X., World Scientiﬁc: Linear Regression Analysis [Electronic Resource]: Theory
and Computing (2009)
10. Gergonne, J.D.: The application of the method of least squares to the interpolation of
sequences. Historia Mathematica 1(4), 439–447 (1974)
11. Stigler, S.M.: Gergonne’s 1815 paper on the design and analysis of polynomial regression
experiments. Historia Mathematica 1(4), 431–439 (1974)
12. Brown, R.G.: Exponential Smoothing for Predicting Demand, p. 15. Arthur D. Little Inc.,
Cambridge (1956)
13. Brown, R.G.: Smoothing, forecasting and prediction of discrete time series. J. R. Stat. Soc.
127(2) (1964)
14. Mills, T.C.: Time Series Techniques for Economists. Cambridge University Press,
Cambridge (1990)
15. Jenkins, G.M.: Autoregressive-Integrated Moving Average (ARIMA) Models. Encyclopedia
of Statistical Sciences. Wiley, New York (2004)
244
D. Zhang et al.

D-SVM Fusion Clustering Algorithm
Based on Indoor Location
Zhongliang Deng, Jiachen Fan
(✉), and Jichao Jiao
Beijing University of Posts and Telecommunications, Beijing, China
406908388@qq.com, {jiachenf,hwj1505}@bupt.edu.cn
Abstract. Traditional ﬁngerprint orientation clustering algorithms often use k
means clustering algorithm, but as a result of ﬁngerprint and objective factors of
volatile characteristics over time, k-means cannot adapt to change at any time in
ﬁngerprint, and cannot be generated adaptive clustering cluster number, cause the
matching accuracy is not high. This paper adopts a based on support vector
machine (SVM) and DBSCAN clustering algorithm, can generate continuously
adapt to changing the optimal hyperplane ﬁngerprint model, solved the ﬁngerprint
ﬂuctuating lead to the problem of matching result is bad, and can be automatically
generated in the process of matching classiﬁcation number of clusters, based on
statistical density characteristics of DBSCAN selection matching probability
model, to improve the positioning of the matching accuracy, reduced the amount
of time matching positioning, positioning accuracy can be up to 2.04 m in the
range of 57%, relative k-means 6.1 m increased by 52.3%, improve the positioning
accuracy.
Keywords: Location ﬁngerprint · Clustering algorithm · SVM
1
Introduction
With the rapid development of mobile Internet and mobile terminal equipment, indoor
positioning has become the front of data information technology research [1]. Under the
support of the world’s four major satellite navigation systems, outdoor location services
have been widely into people’s lives. While 80% of the daily time in indoor activities, with
the increasing number of large buildings, indoor location services in commercial applica‐
tions, public safety and other aspects of the application of the most broad prospects. In
indoor environments, satellite system signals cannot be used due to building occlusion and
multipath [1]. At present, location fingerprint location based on signal strength (RSSI) is
widely used in indoor positioning. Fingerprint localization makes use of the fingerprint
feature of multipath non line of sight caused by the complex indoor architecture, and
improves the positioning accuracy under the traditional positioning problem.
Indoor positioning method (AOA, Arrival, of angle of arrival and time of arrival
(Angle) location TOA, Time of Arrive) location, time diﬀerence of arrival (TDOA Time,
Diﬀerence of, TDOA/AOA Arrival) positioning hybrid positioning, based on the
received signal strength indicator (RSSI, Received Signal Strength Indication) location.
TOA, TDOA positioning requires high-precision hardware synchronization, AOA
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 245–251, 2018.
https://doi.org/10.1007/978-3-319-74521-3_27

positioning requires directional antenna, and in the NLOS environment aﬀected by
multipath serious. WiFi positioning widely used RSSI technology, generally based on
ﬁngerprint matching positioning. The indoor environment is complex, but the pattern
remained unchanged, the characteristics of the wireless signal formed in the speciﬁc
position (the number of signals, phase, intensity) showed a special high, as the only
“ﬁngerprint” to identify the location, then according to the ﬁngerprint matching algo‐
rithm to calculate the position.
Improve the accuracy of fingerprint matching can generally improve the stability of
RSSI fingerprint database, optimize the fingerprint database structure and improve the
clustering algorithm three aspects. Starting from clustering algorithm, the traditional K-
means.
Is based on the concept of data partitioning, can not be adaptive optimization
matching model, ignoring the data model should continue to change with the test vector
changes. Positioning accuracy is easily aﬀected by time weather and objective factors,
positioning accuracy is diﬃcult to further improve.
2
Matching Positioning Method for Position Fingerprint
This method is generally divided into oﬀ-line training and online matching stage. The
oﬀ-line training phase of the mobile device to acquire the RSSI sequence, AP sequence
signal intensity and location information for the RSSI signal vector and stored in the
ﬁngerprint database, the formation of space vector space according to a certain density
of ﬁngerprint acquisition point position information (Fig. 1).
Fig. 1. Single AP signal intensity distribution of National Grand Theater
3
Several Data Clustering Methods
When the amount of ﬁngerprint data is large, clustering algorithm is widely used to
optimize the structure of ﬁngerprint database. Generally based on the data mean division
246
Z. Deng et al.

method, density based density method, data model based machine learning methods.
This paper uses divisive hierarchical clustering algorithm, the entire data set as a cluster,
and then use the adaptive mesh split into multiple clusters in data modeling, the entire
data set model, using two diﬀerent clustering algorithms are fused to play two kinds of
localization algorithm advantages, makes the use of the smallest the data space to get
the location information of the most useful, to improve the positioning accuracy.
3.1
K-means Clustering Based on Data Mean
For the K nearest neighbor method, the K nearest neighbor points are given diﬀerent
weights according to certain rules:
⎧
⎪
⎪
⎨
⎪
⎪⎩
(̂x, ̂y) = 1
K
K∑
i=1
wi(xi, yi)
k∑
i=1
wi = 1
(1)
(1) We need to give K neighbor points diﬀerent weights to improve the system posi‐
tioning accuracy, learn from the classic signal positioning algorithm based on active
RFID system. The system selects some reference points and arrangement of refer‐
ence tags in the positioning area, the signal strength compared to the tag and refer‐
ence tag values, obtained several reference tags to be located closest to the label,
and then based on these coordinates weighted reference tags to estimate the coor‐
dinates positioning label. Using the weight design idea of the system, the weights
are redesigned.
(2) The size of the Euclidean distance D square reﬂects the weight change, the smaller
the D, the greater the weight, and directly depends on the current Euclidean distance
Di, enhanced the Euclidean distance Di weight ratio (Fig. 2).
Fig. 2. National Grand Theater K-means clustering results
D-SVM Fusion Clustering Algorithm
247

3.2
Density Based DBSCAN Algorithm
(1) (Eps neighborhood) given a data object P, P Eps neighborhood deﬁned as the core
of P, Eps radius of the D dimensional hypersphere sphere region, that is, Numbered lists
should use the “Numbered Item” style.
NEps(p) = {q ∈D|dist(p, q) ≤Eps}
(2)
Data point density and DBSCAN group connected to a cluster. The density of points
m/nV, M is the number of data points that N input falls in a small region around the
point and V is the area of the volume. The region is considered to be a hypersphere
radius, so the MinPts threshold density can be determined by specifying a parameter.
For given and MinPts values, DBSCAN ﬁnds a dense point input set and expands it
by merging adjacent dense regions. A point can be a dense (core) or non-dense point
(non core business point). Non-dense point is a boundary point dense region or noisy
pattern.
In the plane space data density space vector set as ﬁngerprint clustering based on
density clustering algorithm, the number of clusters is not need to set in advance, so this
method is suitable for clustering point distribution of unknown data sets clustering.
Based on density of DBSCAN is a widely used mesh algorithm, the super ball algorithm
number of data samples within the region to measure the regional density of the fast.
DBSCAN algorithm can discover clusters of arbitrary shape, and eﬀective identiﬁcation
of singular points. The discovery of DBSCAN cluster grid data and characteristics, and
each cluster according to the data of diﬀerent densities of diﬀerent probability markers,
matching the priority of such arrangement has probability sequence cluster set the
maximum probability in the process of matching, the matching accuracy is very good
optimization (Fig. 3).
Fig. 3. Grid partition result
248
Z. Deng et al.

3.3
SVM (Support Vector Machine) Clustering Based on Data Model
After using DBSCAN to divide the grid, then use SVM to build the data model to carry
on second levels of clustering. The training set is linear SVM, assuming that the size of
N training set consists of two parts, respectively for the class and, if that is marked, if,
if there is marked. Ultra ﬂat g (x) the training vector is correct classiﬁcation of the
training sample set is linearly separable
⎧
⎪
⎨
⎪⎩
g(x) = 𝜔x + b = 0;
𝜔xi + b ≥1(yi = 1);
𝜔xi + b ≤−1(yi = −1).
(4)
When the training set is nonlinear, the input feature space by a nonlinear function is
mapped to a higher dimensional space m, X ∈Rn →X ∈Rm these classes can be used
to classify a hyperplane and get discriminant function.
𝜔Φ(x) + b = 0
(5)
Discriminant function:
f(x) = sign[𝜔Φ(x) + b] ≥1 −𝜉i
(6)
In the formula C is the parameter, the objective function should look for the
maximum hyperplane as far as possible and the guarantee data point deviation quantity
is smallest, parameter C controls before two weights.
4
DBSCAN-SVM Clustering Algorithm
4.1
Research Method
A support vector machine based on combination of DBSCAN and indoor positioning
method, which is characterized by the largest cluster DBSCAN quickly ﬁnd the
advantage and advantage of support vector machine to solve the super plane model, to
achieve the desired eﬀect of clustering. The original data points using DBSCAN to select
all kinds of density density cluster and number based on the traditional local mesh
according to the substitution, the purpose of doing so can eliminate the singularity and
little area quickly, so as to optimize the positioning accuracy and matching speed.
4.2
Implementation Methods
The original data points using density based DBSCAN select various density clusters.
After ﬁnding the clusters of various shapes at the fastest speed, the singular points are
excluded and the probability density of each cluster is sorted. In the matching process,
the cluster with higher probability is preferred (Fig. 4).
D-SVM Fusion Clustering Algorithm
249

Fig. 4. DBSCAN ﬁrst layer clustering results
4.2.1
Cluster Combination
Divides the good class cluster according to the number to carry on 22 combinations.
Then the SVM classiﬁer is used to classify the 22 clusters, and the total number of
classiﬁcation models is N(N −1)∕2.
To ﬁnd the best hyperplane classiﬁcation, the next step is to solve a constrained
extreme value problem:
SVM optimal classiﬁcation surface problem can be described as:
{
min(1
2‖𝜔‖2)
s, t.yi(𝜔rT
i + b) ≥1
i = 1, 2, ⋯, n
(7)
By means of formula (6), we can solve an optimal hyperplane of classifying the data
of the two kinds according to the mathematical model. Before the N data samples
obtained by the cluster, with permutations and combinations into the SVM classiﬁer
training, N(N −1)∕2 hyperplane model DBSCAN (Fig. 5).
⎧
⎪
⎨
⎪⎩
min Θ(𝜔) = 1
2‖𝜔‖2 + C
N∑
i=1
𝜉i;
yi(𝜔Φ(xi) + b) ≥1 −𝜉i
(𝜉i ≥0, i = 1, 2, ⋯, N),
(8)
Fig. 5. Fingerprint database statistical positioning results and optimal classiﬁcation plane sketch
map
250
Z. Deng et al.

The penalty function can be described as the distance between the hyperplane model
and the measured point in order to avoid over ﬁtting the correction factor to the distance.
4.2.2
Location Process
The signal intensity received by the positioning point is input into the established model,
and the probability density method is used to select the maximum probability density
model. The ﬁrst step, based on the probability of the K regions obtained by classiﬁcation,
calculates the weights (Fig. 6).
Fig. 6. Error cumulative distribution function
5
Conclusion
This paper introduces a fusion clustering algorithm in indoor positioning applica‐
tions, used to solve the existing RSSI volatility, quickly find clusters. After testing,
can effectively increase the matching accuracy, reduce matching time, improve posi‐
tioning accuracy.
References
1. Deng, Z., Yu, Y., Yuan, X., et al.: Indoor positioning status and development trend study.
China Commun. 10, 50–63 (2013)
2. Zheng, R., Deng, Z., et al.: Analysis of positioning error in multipath environment. In: China
Satellite Navigation Academic Annual Meeting (2015)
3. Ning, J.J.: An adaptive SA-DBSCAN: of density clustering algorithm. Univ. Acad. Sci. 26(4),
530–538 (2009)
4. Zhou, J., Li, W., Jin, L., et al.: Design of indoor positioning system based on KNN-SVM
algorithm. J. Huazhong Univ. Sci. Technol. Nat. Sci. Ed. S1, 517–520 (2015)
5. Zhang, B., He, Z.: New method of SVM multi classiﬁcation based on support vector data
description. Comput. Appl. Res. 24, 46–48 (2007)
6. Li, S.: Application research on indoor location technology based on WiFi. Nanjing Normal
University (2014)
7. Zhou, F.: Design and implementation of positioning system based on WiFi technology. Beijing
University of Post and Telecommunication, p. 12 (2009)
D-SVM Fusion Clustering Algorithm
251

Research on Evaluation of Sensor Deviations During Flight
Yuping Xiong1, Shufen Liu2, Sihua Gao2(✉), and Yemei Zhu3
1 System Engineering Research Institute of China State, Shipbuilding Corporation, Beijing, China
2 College of Computer Science and Technology, Jilin University, Changchun, China
13756651430@163.com
3 Chinese Journal of Electronics, Beijing 100036, China
Abstract. Aircrafts have been widely used in recent years with the rapid devel‐
opments in economies. In consideration of aircraft security and the importance
of navigation information, numerous sensors have been utilized for aircraft
tracking. Deviations are easily produced by sensors and have signiﬁcant inﬂuence
on information accuracy. The existing literature on methods to monitor sensors
and evaluate their deviations is limited. In this paper, the information detected by
sensors is compared to the reference trajectory. Subsequently, a novel method to
calculate deviations and errors is proposed. To validate the reliability of the eval‐
uation results, the Chi Square test is integrated into the research method. A stat‐
istical method is also proposed to identify the existence of deviations between
two sensors.
Keywords: Sensors · Deviations · Chi Square test · Test statistics
1
Introduction
Aircrafts have been widely used in economies, and ﬂight safety has gained increased
importance and attention. Many types of sensors have been deployed to acquire and
monitor location information in diﬀerent domains, such as the military, medical care,
and environmental management. Multiple sensors are generally used to identify
geographical coordinates precisely. However, various environmental factors can cause
sensor deviation, which results in inaccurate data collection. These factors should be
considered when deploying sensor probes. Thus, the calculation and the analysis of
sensor deviations are crucial issues.
In most recent studies, sensors are considered as suitable tools to investigate aircraft
information. In [1], the airspeed sensors were used to reconstruct the approach-path
deviations through triangulation and the ﬂow sensors controlled the touchdown phase.
The authors in [2] demonstrated a trajectory recommendation framework that relied on
various sensors. The authors of [3] reported that trajectories were diﬃcult to predict the
precise location of thrown objects. A sensor system, which received data from distance
sensors, was responsible for detecting the deviations between the actual and predicted
values. Then an algorithm to measure the in-between points during ﬂights was proposed.
The authors of [4] used inertial sensors to gather optical ﬂow information and presented
a set of novel navigation techniques to estimate the ground velocity and altitude of
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 252–257, 2018.
https://doi.org/10.1007/978-3-319-74521-3_28

aircrafts. Two formulations were proposed to calculate aircraft velocity and altitude. A
novel method that used a ToF [5] camera to detect an aerial target was proposed in [6].
On the basis of a special requirement, the sky was considered as noise in the image and
the target could be recognized from a model of complete randomness, thus illustrating
the robustness of sensor detection with respect to the number of false alarms. In addition,
the authors conducted a study to analyze the detection performance. In [7], the accuracy
of sensors was noted. The authors proposed a linear covariance technique to predict the
accuracy of altitude determination systems for ﬂights. The authors also carried out a
study to analyse the performance of the detection. In [8], a radar sensor was placed onto
a UAV, which calculated the position deviations of aircrafts from a theoretical trajectory.
A SAR processor and navigation system determined the diﬀerence between theoretical
and actual values.
To the best of our knowledge, the literature on the evaluation of sensor deviations
is limited. In the present research, we build the reference trajectory and propose a novel
method to calculate and evaluate the deviations for each sensor. The feasibility of the
obtained data is evaluated through residual analysis using the Chi Square test. We also
propose a statistical method to determine whether the deviations exist between two
sensors or not.
The rest of this paper is organized as follows. Section 1 discusses the potential
application of sensors in the aviation industry and the importance of evaluation methods.
Section 2 presents the proposed calculation and evaluation methods. Section 3 presents
the statistical method for detecting the deviations for each sensor. Finally, the study is
concluded in Sect. 4.
2
Deviation Calculation and Evaluation
To obtain the deviation, we should build a reference trajectory ﬁrst. The reference data
need to be transformed according to the local sensor framework. If the total number of
points tracked by two sensors is larger than 50, then high-accuracy sensors are used to
plan the trajectory. The reference time is calculated as follows:
Timeref =
(Min(time1) + Max(time2))
2
(1)
The reference trajectory of a sensor is built through the least square ﬁtting of a poly‐
nomial. We sum up of the values in the set deviation. The sensor deviation is equal to
the average of values. In order to calculate the deviation between two sensors, the devi‐
ation for each sensor corresponding to the reference trajectory should be obtained. The
tracks of sensor Sx are recorded in the set 𝜉Sx. The deviation of sensor Sx can be deﬁned
as follows:
bSx|rt = 𝜉−
sx = Σsx
Nsx
(2)
Research on Evaluation of Sensor Deviations During Flight
253

where bsx|rt and ̄𝜉sx are the deviation between Sx and the reference trajectory. The sum of
values in the set 𝜉Sx is expressed as Σsx, and variable NSx is the number of the points
tracked by sensor Sx. The deviation between sensor s1 and s2 is deﬁned as follows:
bs1|s2 = bs2|rt −bs1|rt
(3)
The errors induced by the deviations could be obtained simultaneously by calculating
the standard deviations of a sensor as follows:
𝜎x =
√
1
Nx
Σ(𝜉x −̄𝜉x
)2
(4)
The variable 𝜎̄x denotes the average value of the standard deviations:
𝜎̄x =
𝜎x
√
Nx
(5)
The standard deviation for each sensor could be obtained easily. After acquiring the
standard deviations for sensors s1 and s2, the errors induced by the deviations between
these two sensors can be calculated as follows:
𝜎evaluate =
√
𝜎2
s1 + 𝜎2
s2
(6)
Subsequently, the methods to achieve the research objective are tested because the
sensor errors are obviously normally distributed. In this paper, residual analysis is ﬁrst
utilized to identify the feasibility of the data. Then, the residual error is tested by Chi
Square test. The residual error must ﬁrst satisfy the test above to obtain accurate data
and information. Otherwise, the deviation is discarded. Sensors have diﬀerent preci‐
sions. Thus, the diﬀerence for each sensor should be normalized. The normalized diﬀer‐
ence of sensor X is deﬁned as follows:
𝜉∗
x =
𝜉∗
x
𝜎′
x
(7)
where 𝜎′
x is the sample-standard-deviation. Normalization is applied on the two sets, and
the results are sorted and stored in the set 𝜎∗∗.
The normalized diﬀerences and their order are analyzed. The possible samples are
divided into discrete areas, which are used to calculate the expectation of samples. The
Chi Square test is conducted to identify the consistency of samples and expectation
distributions, that is:
𝜒2
e =
k∑
i=1
(ne −ns
)2
ne
(8)
254
Y. Xiong et al.

where k is the number of discrete areas and ne is the distribution of the expectation. The
distribution of samples is expressed by ns. Subsequently, X2
e is compared to the hypo‐
thetical threshold. If the threshold is not exceeded, the distributions of the samples are
considered consistent with the expected distributions. Otherwise, the residual error could
not be used further.
3
Surveillance of Deviations
Apart from evaluating the deviations, the results from calculations also can also help in
monitoring the state of sensors. In this study, a statistical method is employed to identify
whether the deviations exist between the two sensors. Two aspects are considered in
detecting deviations: the detection for a single sample and the detection for accumulated
samples. Both are described in the next sections.
3.1
Detection for a Single Sample
The detection for a single sample transforms the reference trajectory and deviation
calculation data, such as distance, orientation, and elevation, into the same coordinate
systems. Unlike the deviation estimates, the result of deviation calculation constructs a
test statistic that is used to identify the consistency of the deviation for the reference
track in a statistically signiﬁcant way. If the statistic is greater than the predeﬁned
threshold value, the probability of signiﬁcant deviation between the two sensors is
greater. Otherwise, the obvious deviation is not considered. Three steps are required to
detect a single sample: calculation of deviation, calculation of test statistics, and calcu‐
lation of examination. Each step is described in the following sub-sections.
3.1.1
Calculating the Deviation
For parameter p, the method to obtain the deviation is similar to the method used in
calculating sensor deviations. The diﬀerence between the data tracked from sensors and
the reference value is obtained by
𝜉p,s(i) = (ps(i) −̄p(i))
(9)
where 𝜉px,s(i) indicates the diﬀerence of sensor s at point i. ps is the parameter (e.g.,
distance, orientation, and elevation). The reference value of parameter p is expressed
by ̄p(i), which can be formulated as follows:
̄p(i) =
j=n
∑
j=0
cj
(tj −tref
)j
(10)
where tj is the measured time for point j and cj is the polynomial coeﬃcient calculated
by reference trajectory. The duration for constructing reference trajectory is expressed
by tref.
Research on Evaluation of Sensor Deviations During Flight
255

3.1.2
Calculating the Test Statistics
The Strudent method is utilized to monitor the deviations between two sensors. We
deﬁne a variable tp, as the result of monitoring in terms of parameter p, that is:
tp =
̄𝜉p,1 −̄𝜉p,2
√(n1 −1)𝜎2
p,1 + (n2 −1)𝜎2
p,2
√(n1 + n2 −2)n1n2
n1 + n2
(11)
As for parameter p, the existence of obvious deviation is considered if tp is larger
than Tn1. If tp is less than Tn2, the two sensors are considered to be working cooperatively
without deviation. Otherwise, the present study is unable to determine whether deviation
exists between the two sensors or not (i.e., in terms of parameter p). In this case, the
detection for accumulated samples is used to investigate the deviation between the two
sensors.
3.2
Detection for Accumulated Samples
The detected data, such as starts, stops, average diﬀerence are recorded. Then, the results
of the detection for each single sample are counted. For the two sensors, the detection
for accumulated samples can be deﬁned as follows:
tc =
Σp,1 −Σp,2
√(N1 −1)𝜎2
p,1 + (N2 −1)𝜎2
p,2
√(N1 + N2 −2)N1N2
N1 + N2
(12)
where tc is the detection result of the accumulated samples. The standard deviation is
express by 𝜎p,s. The variable Ns is the number of records for parameter p. The total
number of data, which determines the existence of deviation, is stored in variable k1.
Similarly, the total number of data without deviation is k2. If k1 ⩽2 × k2, the accumulated
result is considered ambiguous and the accumulated samples will be detected in further
tests.
The multiple data calculation method is used to monitor the deviation between two
sensors. We combine the results of tests, and then achieve the cumulative tests statistic.
The multiple data calculation method is deﬁned as follows:
Nm = n1 + n2
(13)
Am = w1a1 + w2a2
w1 + w2
(14)
𝜎m =
√
w1 + w2
(15)
wx = 1
𝜎2
x
(16)
256
Y. Xiong et al.

where ax is the average value of sensor x, and the variance is expressed by 𝜎x. nx are the
points of sensor x.
4
Conclusion
This paper discussed the importance of the track points during ﬂight and presented
several practical methods to analyze the accuracy of the data obtained by sensors. We
ﬁrst constructed the reference trajectory, and then transformed the tracked data into the
same coordinates system. The deviation between two sensors was evaluated, and then
the Chi Square method was used to investigate the correctness of the deviation evalua‐
tion. In addition, we proposed a statistical method to identify whether the deviation exists
between sensors or not. Future work may consider more realistic situations, more algo‐
rithms to calculate deviation, and more evaluation methods to determine the accuracy
of deviations.
References
1. Trittler, M., Rothermel, T., Fichter, W.: Autopilot for landing small ﬁxed-wing unmanned
aerial vehicles with optical sensors. J. Guidance Control Dyn. 39(9), 2011–2021 (2016)
2. Yeung, S., Madria, S.K., Linderman, M.: A trajectory recommendation system via optimizing
sensors utilization in airborne systems (demo paper). In: Claramunt, C., Schneider, M., Wong,
R.C.-W., Xiong, L., Loh, W.-K., Shahabi, C., Li, K.-J. (eds.) SSTD 2015. LNCS, vol. 9239,
pp. 508–513. Springer, Cham (2015). https://doi.org/10.1007/978-3-319-22363-6_31
3. Frank, T., Janoske, U., Schroedter, C.: Detection of position and orientation of ﬂying cylinder
shaped objects by distance sensors. In: 2011 IEEE International Conference on Mechatronics
(2011)
4. Rhudy, M.B., Gu, Y.: Unmanned aerial vehicle navigation using wide-ﬁeld optical ﬂow and
inertial sensors. J. Robot. 2015 (2015)
5. Hussmann, S., Huhn, P., Hermanski, A.: Systematic distance deviation error compensation for
a ToF-camera in the close-up range. In: I2MTC - International Instrumentation and
Measurement Technology Conference (2012)
6. Rodríguez-Jiménez, S., Burrus, N., Abderrahim, M.: A-contrario detection of aerial target
using a Time-of-Flight camera. In: Sensor Signal Processing for Defence, SSPD 2012 (2012)
7. Blomqvist, J., Fullmerb, R.: The inﬂuence of uncertainties of attitude sensors on attitude
determination accuracy by linear covariance analysis. The International Society for Optical
Engineering (2010)
8. Labowski, M., Kaniewski, P., Konatowski, S.: Estimation of ﬂight path deviations for sar radar
installed on UAV. Metrol. Meas. Syst. 23(3), 383–391 (2016)
Research on Evaluation of Sensor Deviations During Flight
257

A Distributed Routing Algorithm for LEO Satellite
Networks
Jundong Ding1, Yong Zhang1(✉), Ruonan Li1, and Liang Zhao2
1 Beijing Key Laboratory of Space-Ground Interconnection and Convergence, Beijing University
of Posts and Telecommunications (BUPT), Xitucheng Road No. 10, Beijing 100876, China
{dingjundong,yongzhang,liruonan}@bupt.edu.cn
2 China Electronic Appliance Corporation, Beijing, China
zhaol@ceac.com.cn
Abstract. Satellite networks support a wide range of service and provide global
coverage. Moreover, LEO satellites, which have global coverage potential, have
caused great attention in the ﬁeld of satellite communication. Due to satellites’
mobility, an eﬃcient and stable routing algorithm is necessary for transmission
in satellite networks. In this paper, a Distributed Multi-Path Routing algorithm
(DMPR) using Earth-ﬁxed satellite systems is proposed. DMPR adopts the Polar
orbit constellation in which each satellite has four ISLs (inter satellite link) with
the neighbor satellites. A traﬃc-light is set for every ISL to judge the blocking
while avoiding some busy path and reducing the delay.
Keywords: Low earth orbit · Polar orbit constellation
Distributed multipath routing · Traﬃc light
1
Introduction
Satellite networks are becoming important in the ﬁeld of communication, especially the
LEO satellites. Compared to the terrestrial networks, the LEO satellites have global
coverage potential. Besides, they can also oﬀer services with lower latency than the
geostationary satellites. Therefore, the LEO satellites communication has aroused great
interest in academia and industry.
The satellites’ topologies are dynamic with frequent link switching and interruption.
Since the satellite constellation is determined in advance, the topologies will be predict‐
able. These characteristics, diﬀerent from terrestrial networks, bring many diﬃculties
in routing design. Many schemes have been proposed from researchers since the 1990s
[1]. Topological control strategy can generally be adopted to shield the topology’s
dynamic. There are two topology control strategies: Virtual topology [2, 3] and virtual
node [4, 5]. Virtual node strategy in which use the concept of satellite logical location
form a global virtual networks. Each node in the network is virtual node, which is serv‐
iced by the nearest satellite.
Werner [6] introduces the DT-DVTR routing algorithm (Discrete-Time Dynamic
Virtual Topology Routing) in the LEO satellites networks. They divided the routing
strategy into two processes: ﬁrstly, they set a virtual topology for all successive time
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 258–264, 2018.
https://doi.org/10.1007/978-3-319-74521-3_29

intervals of the system period, providing instantaneous sets of alternative paths between
all source destination node pairs; secondly path sequences over a series of time intervals
are chosen from that according to certain optimization procedures. Korçak [7] proposed
a multistate virtual network (MSVN) topology, provided formal mathematical model
for it and discussed its contribution to the overall system availability. Then Lu et al. [8]
formalized and optimized the virtual topology based on virtual node strategy while
elaborating its important features.
There are many distributed routing algorithms [9, 10] based on virtual nodes. A
datagram routing algorithm is proposed for LEO satellite networks where routing deci‐
sions are made on per-packet basis. Henderson and Kartz [11] has proposed a distributed
routing algorithm in which they ﬂood a link state packet only as the routing radius for
a given satellite and then they choose the best path using Dijkstra algorithm.
This paper is organized as follows: Sect. 2 introduces the satellite network architec‐
ture that has the Earth-ﬁxed footprints. Section 3 proposes the new routing strategy.
Section 4 presents the simulation result of the new routing algorithm and in the end
Sect. 5 summarizes this paper.
2
Satellite Network Architecture
The satellite network adopts the Walker star [12] constellation which is composed of N
polar orbits (planes), each with M satellites at low distances from the Earth. The planes
are separated from each other with the same angular distance of 360°/(2 * N). They cross
each other only over the North and South poles. The satellites of each plane are separated
from each other with an angular distance of 360°/M. Due to the circular planes, the
satellites in the same plane have the same radius all the times. In this paper N is 6 and
M is 12.
Each satellite has four neighboring satellites: two in the same orbit and two in the
left and right planes. So each satellite has four transmission directions which means that
when deciding the next hop node each satellite has four choices.
In our analysis of virtual topology dynamics, we consider a satellite orbit with NSAT
satellites that have Earth-ﬁxed footprints, and serve for total of NFP footprint areas along
the orbit. The footprint means the ground area is serviced by satellites. Firstly, average
number of satellites per footprint area (Nsat
f ) is calculated as
Nsat
f
= NSAT∕NFP
(1)
In traditional VN (virtual node) concept, NSAT is equal to NFP, i.e., there exists a single
satellite for each footprint, namely Nsat
f
= 1. However, for the sake of increasing system
availability, this paper also considers the case where there is more than one satellite per
footprint. Target on improving the transmission success rate while simplifying the
handover process, we set Nsat
f  to 2 as shown in Fig. 1.
A Distributed Routing Algorithm for LEO Satellite Networks
259

Fig. 1. The satellite system and virtual network with Nsat
f
= 2
In the virtual networks, we use <n, m> (n = 1, 2, N; m = 1, 2, M/2) to represent the
virtual footprint. Each satellite will also be assigned a corresponding address as the basis
for the transmission. Taking into account the mobility of satellites, the satellites’ coor‐
dinates need to be updated periodically. Ignoring the impact of the Earth’s rotation, we
only update the value of m. There are M/2 footprints in each orbit, hence each satellite
has M/2 diﬀerent m values. The satellite updating period (Tsat
upd) can be updated as:
Tsat
upd = Tsat∕(M∕2) = 2 ⋅Tsat∕M
(2)
Where Tsat means the satellite movement period.
3
Routing Strategy
DMPR routing algorithm is a kind of distributed routing algorithm. Therefore, each
satellite node can independently determine their next hop node depending on ISL
congestion. For the satellites, a buﬀer queue for each ISL was set to temporarily store
packets sent to the next hop satellite through this ISL. A buﬀer traﬃc is also set to
represent the congestion condition for this ISL. To select the next hop node (or the
transmission direction) the satellite will consider the current satellite’s congestion status
of a buﬀer queue.
Figure 2(a) shows how to set the color of traffic light according congestion. In our
algorithm, there are only two types of traffic light color: “GREEN” and “RED”. First
of all, BQ(n) (n = 1, 2, 3, 4; meaning there are 4 ISL in every satellite) is defined as
the buffer queue of satellite at direction n. And the buffer queue occupancy rate BOQR
is defined as the rate of packets’ number in the queue to the length of the queue. So
BOQRn represents the occupancy rate of BQ(n). Then we define a threshold T to
distinguish the two status. When BOQRn grows beyond T, the queue is considered to
be congested and the traffic light turns to “RED”. There is a special situation: when
the satellite checks BQ(n), BOQRn is very close to but still below T, so the traffic
light remains “GREEN”. Just after checking, BOQRn is beyond T, but the satellite has
260
J. Ding et al.

to stay wrong status for Δt (checking interval). To ensure that BOQRn will not exceed
T during this checking interval, T should meet:·
T ⋅L ⋅APS ≥(I −O)max ⋅𝛥t
(3)
Where L represents the size of buﬀer queue, APS represents average packets size,
and (I-O) max denotes the maximum diﬀerence between input and output traﬃc rates.
When the satellite moves to high latitudes, the distance between them is closer than
the low latitudes as shown in Fig. 2(b). The distance between satellites in orbit L2 can
be described by: d = 𝜋× (R × sin𝛼+ h) ÷ N
(R means the Earth radius, h means the satellites height and N means the number of
orbits).
According to Shannon formula, the channel capacity can be described by:
Rd = B ⋅log(1 + S
N ) = B ⋅log(1 + PD ⋅hss
𝜎2
)
(4)
hss means channel gain and hss ~ E(d−α). It can be seen satellites’ channel capacity
in high latitudes is larger than low latitudes. So the buﬀer size of inter-ISL can be set
higher when satellites moves to high latitudes. This means when satellites serving for
diﬀerent footprint their inter-ISL buﬀer size change dynamically.
The routing algorithm begins with sending data from the terrestrial device to the
satellite. Each terrestrial device can send date to 2 diﬀerent satellites, because every
footprint is served by two satellites. When the current satellite determines the next hop
node, it will compare the footprint’s coordinate to decide the direction.
<nd, md> is the destination footprint coordinate and <ni, mi> as the current satel‐
lite’s footprint coordinate. The current satellite will choose at most two directions to
transfer data. The vertical and horizontal movements are described by
Fig. 2. Traﬃc light color setting scheme (Color ﬁgure online)
A Distributed Routing Algorithm for LEO Satellite Networks
261

dv =
⎧
⎪
⎨
⎪⎩
+1, upward
0,
no movemet
−1, downward
dh =
⎧
⎪
⎨
⎪⎩
+1, right
0,
no movement
−1, left
(5)
During a date receiving processing, ﬁrstly current satellite will check whether the
same data has been received. After checking, satellites can avoid transferring the same
data twice, so it can save source and improve the system’s performance. If not, it will
compare its own footprint coordinate with the destination coordinate to choose the
direction. For example, if mi > md, dv = +1 means the current satellite chooses upward
as the backup direction. If ni < nd, dh = +1 means the current satellite chooses right as
the backup direction (Table 1).
Table 1. Direction selection scheme
mi > md
mi = md
mi < md
ni < nd
ni = nd
ni > nd
dv = +1
dv = 0
dv = –1
dh = +1
dh = 0
dh = –1
After choosing two backup directions, the satellite will enhance direction by taking
congestion into account. If the backup ISL’s traﬃc light is “GREEN”, the satellite will
transfer data through this ISL. If the backup ISL’s traﬃc light is “RED”, the satellite
will discard this ISL.
4
Simulation and Evaluation
In this section, we mainly analyze the performance of satellite transmission delay and
success rate. In satellite communications, congestion and satellite failure are important
factors aﬀecting data transmission. In this experiment, the average one-hop ISL propa‐
gation delay is set to 14 ms. When the ISL is set to “GREEN”, the maximum queuing
delay is set to 10 ms. When the ISL is set to “RED”, maximum queuing delay is set to
Fig. 3. Simulation of transmission delay and success rate
262
J. Ding et al.

30 ms. In simulation, inter-orbit link’s buﬀer size is set dynamically. Satellites’ buﬀer
size (BQ(n)) is 20% smaller than higher latitudes, so does the threshold T.
Figure 3(a) shows the two routing algorithm’s latency increases with the proportion
of blocking ISL. DMPR’s latency is shorter than DT-DVTR’s obviously. When the link
is blocking, the satellite will wait which will increase queuing delay using DT-DVTR
routing. So the impact of blocking on the delay is small while DT-DVTR’s delay
increases rapidly with the increase in proportion.
In DMPR routing algorithm each satellite node has at most two choices when
selecting next node. Even if a node fails, it can also ﬁnd an alternative node for trans‐
mission which can mitigate the adverse eﬀects caused by node failures. So in the
Fig. 3(b), it shows the success rate of DMPR is higher than DT-DVTR’s especially when
the number of disabled satellite exceeds 5.
5
Conclusion
In this paper, we have proposed a new distributed routing algorithm for LEO satellite
networks. When choosing the next hop satellite node, the satellite will take congestion
into account. So our routing strategy has an advantage in reducing latency when encoun‐
tering congestion. Meanwhile, this routing algorithm can guarantee the success rate
when the node fails.
Acknowledgements. The author would like to thank to the reviewers for their detailed
suggestions. This work is supported by the National Natural Science Foundation of China under
Grant No. 61171097.
References
1. Taleb, T., Jamalipour, A., Kato, N., Nemoto, Y.: IP traﬃc load distribution in NGEO
broadband satellite networks – (invited paper). In: Yolum, Güngör, T., Gürgen, F., Özturan,
C. (eds.) ISCIS 2005. LNCS, vol. 3733, pp. 113–123. Springer, Heidelberg (2005). https://
doi.org/10.1007/11569596_14
2. Gounder, V.V., Prakash, R., Abu-Amara, H.: Routing in LEO-based satellite networks (1999)
3. Fischer, D., Basin, D., Engel, T.: Topology dynamics and routing for predictable mobile
networks. In: IEEE International Conference on Network Protocols, pp. 207–217. IEEE
Xplore (2008)
4. Mauger, R., Rosenberg, C.: QoS guarantees for multimedia services on a TDMA-based
satellite network. Commun. Mag. IEEE 35(7), 56–65 (1997)
5. Ekici, E., Akyildiz, I.F., Bender, M.D.: A distributed routing algorithm for datagram traﬃc
in LEO satellite networks. IEEE/ACM Trans. Netw. 9(2), 137–147 (2001)
6. Werner, M.: A dynamic routing concept for ATM-based satellite personal communication
networks. IEEE J. Sel. Areas Commun. 15(8), 1636–1648 (1997)
7. Korçak, Ö., Alagöz, F.: Virtual topology dynamics and handover mechanisms in Earth-ﬁxed
LEO satellite systems. Comput. Netw. 53(9), 1497–1511 (2009). The International Journal
of Computer & Telecommunications Networking
A Distributed Routing Algorithm for LEO Satellite Networks
263

8. Lu, Y., Sun, F., Zhao, Y.: Virtual topology for LEO satellite networks based on earth-ﬁxed
footprint mode. IEEE Commun. Lett. 17(2), 357–360 (2013)
9. Wang, K., Yi, K., et al.: Packet routing algorithm for polar orbit LEO satellite constellation
network. Sci. Chin. Inf. Sci. 49(1), 103–127 (2006)
10. Papapetrou, E., Pavlidou, F.N.: Distributed load-aware routing in LEO satellite networks. In:
2008 Global Telecommunications Conference, IEEE GLOBECOM, pp. 1–5. IEEE Xplore
(2008)
11. Henderson, T.R., Katz, R.H.: On distributed, geographic-based packet routing for LEO
satellite networks. In: 2000 Global Telecommunications Conference, IEEE GLOBECOM
2000, vol. 2, pp. 1119–1123. IEEE Xplore (2000)
12. Wang, J., Li, L., Zhou, M.: Topological dynamics characterization for LEO satellite networks.
Comput. Netw. 51(1), 43–53 (2007)
264
J. Ding et al.

Optimization of Smart Home System Based
on Wireless Sensor Network
Xing Guo
(✉) and Neng Hu
(✉)
School of Logistics Engineering, Wuhan University of Technology,
Wuhan 430063, People’s Republic of China
2633283313@qq.com 419443370@qq.com
Abstract. In this paper, smart home is the research object. It analysed the intel‐
ligent acquisition for monitoring data, put forward a kind of solution for remote
monitoring system based on wireless sensor network, and designed a set of perfect
intelligent remote monitoring system by using the mobile terminal platform. This
system can achieve a series of basic functions, including monitoring, automatic
adjustment, alarm and control for the home appliances, which can provide
comprehensive and reliable home environment information for the home users
and meanwhile achieve remote control for home appliances. Finally, the smart
home monitoring system was deployed and tested, the test results show that the
system can achieve the basic functions of the modern smart home and has the
unique advantages in performance, cost, versatility, scalability and other
aspects.
Keywords: Smart home · Wireless sensor · ZigBee · HTML5 · WebSocket
1
Introduction
With the rapid development of wireless sensor network, smart phone communication
and Internet technology, the remote wireless mobile home monitoring system will
become one of the mainstream smart home network [1]. The smart home system needs
a lot of information acquisition and transmission. Introducing the wireless sensor
network technology into the construction of smart home can build a “home appliance
network” with adaptive control function for smart home [2]. This network can not only
make the smart appliances conduct mutual coordination, but also be interacted with the
external network. Therefore, users can achieve the remote control of the smart appliances
through this network.
At present, the domestic and foreign major operators are integrating resources and
carrying out innovative business to occupy the smart home market [3–5]. Although the
smart home industry is developing relatively quickly, the present level of development
is uneven. It is faced with many problems like the lack of uniﬁed industry standards,
expensive price, complex operation and the leakage of information which restrict the
development of the smart home industry [6, 7]. In this research, the ﬁeld research was
conducted to ﬁnd out the deﬁciencies of the current smart home system. And the envi‐
ronmental parameters required for the monitoring system were determined through the
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 265–273, 2018.
https://doi.org/10.1007/978-3-319-74521-3_30

analysis of actual functional requirements. Combined with ZigBee wireless network
communication technology and the research of the intelligent collection for monitoring
data by sensors, a set of perfect intelligent remote monitoring system for smart home
was designed through the mobile terminal platform. And the debugging environment
was deployed to analyze and verify the research results.
2
Overall Design
The overall design is based on the research about the wireless sensor network tech‐
nology, wireless data communication technology, HTML5, WebSocket and other
related theoretical basis [8]. Combined with corporate ﬁeld research and actual func‐
tional requirements analysis, the overall design of the smart home monitoring system
based on the wireless sensor network was determined generally, as shown in Fig. 1.
Fig. 1. Overall design
The system collects the required information of the family environment by the related
sensors and controls the switch of the household electrical appliances by the controller
modules. In this system, the sensor modules and the controller modules are connected
to the ZigBee terminal equipment. The data transmission is based on the ZigBee wireless
network and the ZigBee center coordinator. The ZigBee center coordinator is connected
to the server through the serial ports. It can save the data collected from the sensors into
the server database or sent the data to the client directly. According to the implementation
process of the system, the overall scheme includes the front-end sensors and the control‐
lers module design, the ZigBee wireless sensor network design, the data transmission
module design, the server program design, the data interaction module design, and the
client application software design.
266
X. Guo and N. Hu

3
Wireless Sensor Network Design
3.1
ZigBee Wireless Network Design
The ZigBee wireless transmission module of this system used TI wireless RF chip called
CC2530F256 as the core chip. CC2530F256 is compatible with the ZigBee protocol
stack, providing a powerful and complete ZigBee solution. Because the transmission
distance is relatively short and the family housing area is limited, the system just needs
relatively small sensor nodes and a simple data transmission structure, which does not
need the router nodes to extend the network coverage area [9]. Meanwhile the position
and the number of the household appliances are easy to change, which will change the
network state. Therefore, under the premise of the communication requirements, consid‐
ering the cost of the equipment and the simple structure, this system used the star
topology to build the smart home wireless network. The star topology wireless network
consists of a ZigBee full-function central coordinator and several terminal functional
devices. And the data transmission is based on the principle of ZigBee wireless data
transmission.
In this paper, the DS18B20 temperature sensor module was selected as an example
to introduce the concrete design process of the data acquisition and transmission soft‐
ware. First, the system obtained the collected data from the terminal device. Then the
data was transmitted to the central coordinator. Finally, the coordinator sent the data to
the sever module through the serial ports. In order to observe the results of data acquis‐
ition expediently, the coordinator used LCD to display the collected data values. Mean‐
while the data in the terminal nodes and the coordinator node was sent to the computer
through the serial ports and shown by serial debugging assistant. In order to enhance the
system layout and installation convenience, meanwhile reducing energy consumption,
the data acquisition nodes all used the battery power [5, 10]. The node modules will go
into sleep mode in leisure time and be wake up when the system needs data acquisition
or transmission. The entire workﬂow is shown in Fig. 2.
Fig. 2. Temperature sensor node workﬂow
Optimization of Smart Home System
267

3.2
Development and Encryption of the Data Communication Protocol
The data communication protocol was designed to solve all kinds of problems of the
parameters transmission, ensure the integrity, reliability and security of the data, and
achieve the data transmission function between the modules. Based on the basic Modbus
communication protocol, the system used the same structure to deﬁne the diﬀerent
command codes. According to the concrete data and the speciﬁc functions, the system
customized a complete communication protocol, which can make the system more
standard and perfect. In order to make the communication protocol deﬁned by the system
possess scalability, the functional code ﬁelds were set in the message component unit,
and the functional code table was established to manage the functional codes deﬁned by
the system. The developers only need to add the functional codes in the code table when
they want to add new function in the protocol, which will not aﬀect the previous
communication protocol and functions.
In order to avoid the interference of the external equipment and ensure the conﬁ‐
dentiality and the security of the data in the process of network communication, the
security and encrypted transmission of the data in the smart home network are the
importance of the research. There are three basic encryption keys of the ZigBee network
data, including the main key, the link key and the network key. According to the actual
application, the key is generated by the network layer and the application layer. The
security keys can be shared between the layers, which can reduce the storage require‐
ment. The system used CC2530 chip as the ZigBee module of the wireless transceiver.
And the data encryption and decryption of CC2530 can be achieved through the copro‐
cessor supported by AES (Advanced Encryption Standard).
4
Design of the Monitoring System
4.1
Design of the Functional Modules
According to the demand analysis of the system, the smart home monitoring system can
be divided into ﬁve main functional modules, including real-time monitoring module,
security alarm module, scene mode, basic information management and user manage‐
ment. These ﬁve functional modules can not only achieve the real-time monitoring data
display, alarm for abnormality, linkage control and historical data display, but also
achieve the statistical processing, analysis and diagnosis of the massive monitoring data
to search potential eﬀective information intelligently. In the meantime, this system can
provide the mode selection, information management and scalability. The overall func‐
tional structure of the system is shown in Fig. 3.
268
X. Guo and N. Hu

Fig. 3. Overall system functional structure
4.2
Acquisition and Processing of the Server Data
The server used multithreading serial communication technology to obtain the induction
layer data. In the data communication process, a variety of serial operations were placed
in diﬀerent threads, and every thread was only responsible for the corresponding serial
operation. The main thread was responsible for the coordination and management of the
auxiliary threads, which can use the CPU in fast switching between various threads.
Therefore, the system can achieve the concurrent execution and multi-task mechanism
to reduce the occupancy rate of the CPU. This mechanism can improve the utilization
of the serial ports and eﬃciency of data transmission, which can greatly improve the
data request and processing ability of the server.
In order to solve the common problems of the database like operating frequently and
low query eﬃciency, the system used the cache mechanism to save the collected data.
The cache mechanism saved the data in the cache memory. Returning data from the
cache memory is quicker than querying database, so it can greatly improve the perform‐
ance of the server application. In the process of operation, the database will start data
cache dependency. When the server requests the latest data collected by the sensor, it
will write the data to the cache and update the cache. And setting the key index
“CacheKey” when the system writes cache can improve query execution eﬃciency of
the data in the cache. The concrete process of data storage adopting data cache mecha‐
nism is shown in Fig. 4.
Optimization of Smart Home System
269

Fig. 4. Flow chart of cache data storage
4.3
Information Interaction of the Client and Server
For the basic operations which don’t need high operation frequency and real-time
requirements, such as user login, obtaining the equipment information, querying the
historical data and other operations, the client used Ajax asynchronous loading tech‐
nology to invoke the Web Service from the server and obtain the basic information. In
order to make the system client and server can transmit data cross platform and cross
language, this system used lightweight data exchange format “Json” to achieve the data
exchange between them.
As for viewing and operating the data in real-time monitoring system, it needs client
and server keep persistent connection and constantly obtain the real-time updated data
from the sensor layer, which has high requirements for real-time communication, so this
system designed the WebSocket full duplex bidirectional communication technology to
achieve real-time monitoring. First, this system enabled a thread in the server to open
the WebSocket network service and used the delegation method to open or close the
WebSocket connection. Then it obtained the IP address of the machine and established
the monitoring ports. Finally, the system regarded the two values as the parameter values
of the WebSocket server address to transmit data. For the realization of the system client,
the system used the HTML user interface. After the WebSocket connection between the
client and the server, the two sides will be able to achieve continuous two-way commu‐
nication with a high utilization ratio.
5
Application Test and Veriﬁcation of the System
5.1
Communication Distance and Packet Loss Rate Test
In order to compare the performance of the ZigBee communication system in diﬀerent
environments, the test was divided into indoor test and outdoor test, which conducted a
number of tests in diﬀerent indoor rooms and outdoor open occasions to ensure the
accuracy of the results. In each distance test process, the ZigBee terminal node data
270
X. Guo and N. Hu

transmission frequency was set as 3 s/times, and a total of 500 data was sent to the
coordinator. The average value of the test results was shown in Table 1.
Table 1. Communication distance and data packet loss rate of the ZigBee module
Test
environment
Communication
distance(m)
Number of
terminal data
transmission
Number of
coordinator
data
reception
Number of
test
Average
packet loss
rate(%)
Indoor
15
500
500
10
0
20
500
500
10
0
25
500
500
10
0
30
500
498
10
0.4
Outdoor
30
500
500
10
0
50
500
496
10
0.8
70
500
473
10
5.4
100
500
447
10
10.6
As we can see from the test results, by using the system based on the ZigBee wireless
sensor network, when the communication didn’t meet interference in the outdoor envi‐
ronment, the packets loss rate was very low within a 50 m range. When the node distance
was more than 70 m, the packet loss rate will increase a little. While in indoor environ‐
ment, when the transmission distance was less than 25 m, the packet loss rate was about
0%. The results show that the data transmission of the designed ZigBee network has
high reliability, and it can meet the requirements of data transmission in the general
family environment.
5.2
WebSocket Real-Time Communication Veriﬁcation
In order to examine the communication eﬃciency and real-time of the WebSocket tech‐
nology which was used in the smart home monitoring system, the Ajax polling based
on HTTP protocol and the real-time communication framework based on the WebSocket
were set up. In this test, the data request and response information of two kinds of scheme
in the process of communication were analyzed and compared through the Chrome
browser network tools. Then the test case was designed by using the Jmeter pressure
test tools, which simulated the communication between the client and the server to
compare the network throughput and delay of two methods.
5.2.1
Network Throughput
In this two test schemes, the same data in test was used to ensure that the server sent to
the client in the same actual data size. The Jmeter test tool was used to get data size
through the Ajax polling and WebSocket, and then we can observe the changes of
network throughput in four cases. The results were shown in Fig. 5. As we can see from
this picture, with the increase of the load and the ﬂow, the network traﬃc caused by the
Ajax polling was very huge. The WebSocket scheme caused much smaller network
Optimization of Smart Home System
271

throughput than the traditional Ajax polling scheme, which has great performance
advantages.
Fig. 5. Network throughput comparison of Ajax polling and WebSocket
5.2.2
Network Delay
After the completion of the test, we entered into the “view result tree” of the Jmeter test
tool to count the connection time and loading time consumed in the communication
process and calculate the network delay time of this two communication schemes. In
order to ensure the accuracy of the test results, several experiments were carried out in
each test case. The data delay time was collected to take the average value of the results.
Network delay test results were shown in Table 2.
Table 2. Average delay time of the test results
Number of users
Real-time
application
scheme
Number of
messages
transmission
Total delay time
(ms)
Average delay
time (ms)
1
Ajax polling
1000
8025
8.02
WebSocket
1000
4832
4.83
100
Ajax polling
1000
29757
29.75
WebSocket
1000
8364
8.36
500
Ajax polling
1000
187074
187.07
WebSocket
1000
12639
12.63
1000
Ajax polling
1000
472283
472.28
WebSocket
1000
16482
16.48
According to the comparison of the above two kinds of real-time communication
schemes and analysis of the results, the real-time application based on WebSocket
communication scheme performs better than the current widely-used Ajax long polling
272
X. Guo and N. Hu

technology in the aspects of message delay time and network throughput. WebSocket
technology can decrease the network throughput greatly, and take advantage of the stable
long connection mode to reduce the communication delay time, which has a high eﬃ‐
ciency in the real-time monitoring system of the smart home.
6
Conclusion
This paper introduced the wireless sensors into the system, which greatly simpliﬁed the
structure of the system and improve ﬂexibility. It used the serial debugging assistant to
test the communication distance and the data packet loss rate of the wireless network
set up by ZigBee equipment, and completed the ZigBee communication module test and
WebSocket communication test in the smart home system. It veriﬁed the stability and
safety of the ZigBee wireless network. Then, the real-time communication framework
of Ajax polling and WebSocket were constructed respectively, and the network
throughput and the network delay of the two schemes were tested through Chrome
browser and Jmeter pressure test tool, which proved the advantages of WebSocket tech‐
nology in system real-time monitoring. Finally, the on-line overall test for the entire
smart home system was conducted through the diﬀerent mobile terminals. The system
worked well and met the design requirements, which achieved the basic expected func‐
tions of the smart home system.
Acknowledgment. This paper was supported by the project in the Hubei Science and Technology
Pillar Program (No. 2015BKA222).
References
1. Lai, W.: Intelligent home system design. Comput. Knowl. Technol. 08, 23–25 (2015)
2. Xie, Y., Pei, L., Wu, T.: Internet of Things in the application of intelligent home. Instrum.
Technol. 03, 46–49 (2015)
3. Zhao, Y.: Intelligent home system in the Internet of Things. Internet Things 05, 32–35 (2013)
4. Alam, M., Reaz, M., Ali, M.: A review of smart homes-past, present, and future. IEEE Trans.
Syst. Man Cybern. 42(6), 12–16 (2012)
5. Zhu, M., Li, N.: Development status and future analysis of intelligent home. Telev. Technol.
04, 82–85, +96 (2015)
6. Bitterman, N., Shach-Pinsly, D.: Smart home - a challenge for architects and designers.
Architect. Sci. Rev. 58, 266–274 (2015)
7. Wu, L., Wei, Y., Yajie, M.: Development status analysis of domestic and international
intelligent home market. Mod. Telecommun. Technol. 12, 71–74 (2014)
8. Zou, S.: Study on the key technology of intelligent home system based on wireless sensor
network. South China University of Technology (2013)
9. Nagpal, P.: Key actors in the mobile phone industry: the smart phone wars. Acad. Inf. Manag.
Sci. J. 17(1), 87 (2014)
10. Liu, L., Hu, B., Li, L.: Energy conservation algorithms for maintaining coverage and
connectivity in wireless sensor networks. IET Commun. 4(7), 786–800 (2010)
Optimization of Smart Home System
273

The Energy Eﬃciency Research of Traﬃc Oﬄoading
Mechanism for Green Heterogeneous Networks
Kaili Wu
(✉), Yifei Wei
(✉), Qiao Li, Da Guo, and Mei Song
School of Electronic Engineering, Beijing University of Posts and Telecommunications,
Beijing, China
{kaili,weiyifei,liqiao1989,guoda,songm}@bupt.edu.cn
Abstract. Opportunistic Network as a novel networking, taking advantage of
meeting opportunities of mobile nodes, completes the message transmission from
the source node to the destination node through the way of each hop. Mobility
model decides how the nodes move and is used to analyze network performance
with various protocols, such as routing protocols, data dissemination protocols,
etc. Currently many mobility models are proposed by researchers. To evaluate
these mobility models, an analysis method is proposed. So we propose a method
by analyzing mobile distance to assess node mobility models. This paper ﬁrstly
introduced the commonly used mobility models based on ONE simulation plat‐
form, and then a calculation method of node mobile distance is put forward. Next,
it was simulated and discussed that we have considered the nodes mobility
features by mobile distance. Finally, we use the node contact duration and node
inter-contact time as a validation, to make an evaluation for node mobility model.
Keywords: Energy harvesting · Green communications
Heterogeneous cellular networks · Traﬃc oﬄoading
1
Introduction
Recent years, the energy consumption problem of communication network gradually
becomes the research focus in the industry and academia [1, 2]. How to reduce the energy
consumption of communication network and improve the energy eﬃciency of commu‐
nication system have become a hot topic in the current ﬁeld of communication research.
Heterogeneous network is expected to play an important role in the future communica‐
tion network [3, 4]. In heterogeneous network, it can be introduced a number of diﬀerent
types of small site equipment, such as micro base station (Micro), Pico base station
(Pico), family base station(Femto) and wiﬁ access points, it can strengthen the overlying
cover of certain areas, improve network capacity. These small sites are often deployed
in the place which are closer with the users, can provide service for users with far less
power consumption than Macro. Despite the high network capacity, the dense SBSs also
require huge power supply, causing heavy burdens to both the network operators and
the power grid [5].
To deal with the cumbersome energy consumption, energy harvesting (EH) technology
can be introduced into HCNs. Specifically, the emerging EH-SBSs, which are equipped
with EH devices (like solar panels or wind turbines) exploit renewable energy as
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 274–287, 2018.
https://doi.org/10.1007/978-3-319-74521-3_31

supplementary or alternative power sources, have received great attentions from both
academia and industry [6]. How to fully utilize the harvested energy to maximize network
energy efficiency while satisfying the quality of service (QoS) requirements is a critical issue.
Traﬃc oﬄoading problem is one of the hot issues in heterogeneous network. [7, 8]
analyzes the relationship between the traﬃc oﬄoading and customer service quality, and
puts forward the traﬃc oﬄoading incentive mechanism based on reverse auction model,
can guarantee the quality of customer service, increase the amount of data oﬄoading of
the system. Literature [9] puts forward oﬄoading users which have high requirements
of service quality to femto to reduce the energy consumption of the macro, and analyzes
the relationship between the system energy eﬃciency and the distance between the
neighborhood. Literature [10] analyzes the system power consumption inﬂuence of
diﬀerent traﬃc density by diﬀerent amount of micro. But [9, 10] only consider the static
traﬃc loading scenario, without considering the dynamic changes of the business. [11]
considers the oﬄoading when in the wiﬁ enabled small cell case.
Although traﬃc oﬄoading technology has been extensively investigated in on-grid
cellular networks, the conventional oﬄoading methods cannot be applied when EH is
applied. Instead, traﬃc oﬄoading technology of green heterogeneous cellular networks
needs to be devised, the operation of each cell is optimized individually based on their
renewable energy supply. Diﬀerent from existing works, we focus on the design of traﬃc
oﬄoading for green HCNs with multiple SBSs powered by diverse energy sources. We
aim to maximize the network energy eﬃciency while satisfying the QoS requirement of
every user. To this end, users are dynamically oﬄoaded from the MBS to the SBSs,
based on the statistical information of traﬃc intensity and energy arrival rate, based on
which the optimal traﬃc oﬄoading amount are obtained.
The reminder of the paper is organized as follows. System model is introduced in
Sect. 2. Section 3 analyzes the power demand and supply for SBSs with EH devices.
Then, the energy eﬃciency maximization problem for the single-SBS case is studied in
Sect. 4. Simulation results are presented in Sect. 5, followed by the conclusion in Sect. 6.
2
System Model
In this section, the details of the HCN with diﬀerent energy supply are presented as
follows.
2.1
Network Model
With EH technology employed, a typical scenario of HCN is shown in Fig. 1, where
diﬀerent types of SBSs and wiﬁ access points are deployed in addition to the conven‐
tional MBS to enhance network capacity. Based on the energy source, SBSs can be
classiﬁed into two types: (1) HSBSs powered jointly by energy harvesting devices and
power grid; and (2) RSBSs powered solely by harvested renewable energy (like solar
and wind power). Wiﬁ access points are powered only by conventional energy. Denote
by NH and NR the number of HSBSs and RSBSs, respectively, denote by
BH = {1, 2, … , NH}, BR = {1, 2, … , NR} the set of HSBSs and RSBSs respectively, let
The Energy Eﬃciency Research of Traﬃc Oﬄoading Mechanism
275

B = {BH, BR} be the set of all SBSs. Denote by Wa the number of wiﬁ access points,
denote by Bw = {W1, W2, … , Wa} the set of wiﬁ access points. SBSn serves a circular
area with radius Ds,n, and the small cells are assumed to have no overlaps with each
other. The MBS is always active to guarantee the basic coverage, whereas SBSs and
wiﬁ access points can be dynamically activated for traﬃc oﬄoading or deactivated for
energy saving, depending on the traﬃc and energy status.
Fig. 1. Illustration of a HCN with diverse energy sources
2.2
Traﬃc Model
The user distribution in spatial domain is modeled as a nonhomogeneous Poisson Point
Process (PPP), whose density at time t is 𝜌0(t) in small cell n and 𝜌n(t) outside of all small
cells. As shown in Fig. 1, users located outside of the small cells can only be served by the
MBS, while users within small cells can be partly or fully offloaded to the corresponding
SBS or the wifi access points. Thus, users can be classified into three types based on the
serving BS, wifi access points and location: (1) Macro-Macro Users (MMUs), users which
are located outside of small cells and served by the MBS; (2) SBS-SBS Users (SSUs),
users located within small cells and offloaded to the SBS; (3) Macro-SBS Users (MSUs),
users located in small cells but served by the MBS. (4) WIFI-WIFI Users (WWUs), users
located within small cells and offloaded to the wifi access points.
As for spectrum resource, the bandwidths available to the MBS-tier and SBS-tier
are orthogonal to avoid cross-tier interference. Denote by Wm, Ws, and Ww the system
bandwidth available to the MBS, each SBS and wifi access points, respectively. At
the SBSn, the bandwidth actually utilized is denoted as wss,n ≤Ws, which is allo‐
cated to its SSUs equally for fairness. At the MBS, Wm is further divided into
different orthogonal portions: wmm for serving MMUs and wms,n for serving MSUs in
small cell n, where wmm + ∑NH+NR
n=1
wms,n ≤Wm. At the wifi access points, the band‐
width actually utilized is denoted as www,n ≤Ww.
276
K. Wu et al.

2.3
Wireless Communication Model
If user u is served by SBSn, its received SINR is given by [12]
𝛾ss,nu = PTswu
Ws
d
−𝛼s
nu hnu
(𝜃s + 1)𝜎2wu
(1)
where PTs is the transmit power level of the SBS, wu is the bandwidth allocated to user
u, dnu is the distance between user u and SBSn, 𝛼s is the path loss exponent of the SBS-
tier, hnu is an exponential random variable with unit mean reﬂecting the eﬀect of Rayleigh
fading, 𝜃s is the ratio of inter-cell interference to noise among SBSs, and 𝜎2 is the noise
power density. As each SBS allocates bandwidth equally to its associated users, the
achievable data rate of a generic user u is as follows:
rss,nu = wss,nu log2(1 + 𝛾ss,nu)
(2)
where wss,nu denotes the bandwidth allocated to each user from SBSn.
Similarly, if user u is served by the MBS as a MMU or MSU, its received SINR is
given by
𝛾m,u = PTmwu
Wm
d
−𝛼m
mu hmu
(𝜃m + 1)𝜎2wu
(3)
where PTm is the transmit power level of the MBS, dmu is the distance between user u
and the MBS, 𝛼m is the path loss exponent of the MBS-tier, 𝜃m is the interference to noise
ratio from other MBSs, and hmu reﬂects Rayleigh fading with the same probability
distribution as hnu.
Then, the achievable data rate of user u is given by
rmm,u = wmm,u log2(1 + 𝛾m,u)
for MMU
(4)
rms,nu = wms,nu log2(1 + 𝛾m,u)
for MSU
(5)
where wmm,u and wms,nu denote the bandwidth allocated to each user from MBS.
If user u is served by wiﬁ access points, its received SINR is given by
𝛾w,nu = Pwwu
Ww
d
−𝛼w
wu hwu
(𝜃w + 1)𝜎2wu
(6)
As each wiﬁ access point allocates bandwidth equally to its associated users, the
achievable data rate of a generic user u is as follows:
rw,nu = ww,nu log2(1 + 𝛾w,nu)
(7)
The Energy Eﬃciency Research of Traﬃc Oﬄoading Mechanism
277

3
Analysis of Power Supply and Demand
3.1
Green Power Consumption
BSs can work in either active mode or sleep mode, with diﬀerent power consumption
parameters. According to [13], the power consumption of a BS in active mode can be
modeled as:
PBS = PC + w
W 𝛽PT
(8)
where PC denotes static power consumption including the baseband processor, the
cooling system and etc., suppose the static power consumption of wiﬁ access points is
0, coeﬃcient 𝛽 is the inverse of power ampliﬁer eﬃciency factor, W is the available
system bandwidth and w is the bandwidth of utilized subcarriers. PT is the transmit power
level and is treated as a system parameter, while we control the RF power by adjusting
the utilized bandwidth w.
3.2
Green Energy Supply Model
In this work, we optimize the amount of offloaded traffic, the ON-OFF state, and the RF
power of each SBS at the large time scale, based on the stochastic information of traffic and
energy (i.e., user density and energy arrival rate). The optimization is conducted for each
period independently [13], and thus we can focus on the optimization for one period.
Two time scales are considered for the problem analysis. In the large time scale, we
divide the time into T periods (e.g., T = 24 and the length of each time period is 1 h),
and assume the average energy harvesting amount and user density remain static in each
time period, but may change over diﬀerent periods. During period t, the arrival of
renewable energy packets is modeled as Poisson process with rate 𝜆E,n(t) for SBSn, and
the distribution of user in small cell n follows PPP with density 𝜌n(t).
Discrete energy model is adopted to describe the process of energy harvesting, and
a unit of energy is denoted by E [14]. Denote by 𝜆E,n(t) the arrival rate of per unit energy
at SBSn and time t. The harvested energy is saved in its battery for future use. The
battery is considered to have suﬃcient capacity for realistic operation conditions, and
thus we assume no battery overﬂow happens. For RSBSs without grid power input, they
have to be shut down when the battery runs out. Consequently, the corresponding users
will be served by the upper-tier MBS for QoS guarantee. Note that handover procedure
is conducted when the RSBS is shut down or reactivated, causing additional power
consumption. For HSBSs, they can use the on-grid power when there is no green energy,
until renewable energy arrives.
The energy supply and consumption process of each SBS can be modeled by a queue,
where the queue length denotes the battery amount [15]. Based on the power consump‐
tion model of BSs (Eq. (8)), the equivalent service rate of per unit energy for SBSn is
given by
278
K. Wu et al.

UE,n(t) = 1
E(PCs,n +
wss,n(t)
Ws
𝛽s,nPTs,n)
(9)
where PTs,n, PCs,n and 𝛽s,n are the transmit power, constant power and power ampliﬁer
coeﬃcient of SBSn, respectively. UE is called energy consumption rate in the rest of this
paper for simplicity.
For a SBS with EH, the variation of battery can be modeled as a M/D/1 queue with
arrival rate 𝜆E and service rate UE given by Eq. (9). In what follows, we analyze the stable
status of the energy queue. For the M/D/1 queue, the embedded Markov chain method
is usually applied to analyze the stable status [16]. When 𝜆E∕UE ≥1, the queue is not
stable and the queue length goes to inﬁnity, which means that the harvested energy is
always suﬃcient. When 𝜆E∕UE < 1, the stationary probability distribution of L+ can be
derived by Pollaczek-Khinchin formula,
q0 = 1 −𝜆E
UE
(10)
q1 = (1 −𝜆E
UE
)(e
𝜆E
UE −1)
(11)
qL+ = (1 −𝜆E
UE
){e
𝜆E
UE
L+
+
L+−1
∑
k=1
e
k 𝜆E
UE (−1)
𝜆E
UE
−k
[
(k 𝜆E
UE
)L+−k
(L+ −k)! +
(k 𝜆E
UE
)b−L+−1
(b −L+ −1)!]} (L+ > 2)
(12)
Thus, the stationary probability distribution of the energy queue length (i.e., the
amount of available green energy) is derived.
3.3
Outage Probability Analysis
Service outage happens when the user’s achievable data rate is less than the requirement
RQ, we are interested in analyzing the outage probability constraint for SSUs, MMUs
MSUs, and WWUs respectively, based on which the power demand can be obtained.
Service outage constraint of SSUs can be written as
wss,nu𝜏ss,nu ≥RQ
(13)
where wss,nu is the bandwidth of SSU, 𝜏ss,nu denotes the spectrum eﬃciency of cell edge
users given by [13]:
𝜏ss,nu = log2(1 +
PTs,n
(𝜃s + 1)
𝛼s + 2
2𝜎2Ws
𝜂
D
𝛼s
n
)
(14)
the physical meaning of Eq. (13) is that the average data rate of the non-cell-edge users
(with spectrum eﬃciency above 𝜏ss,nu) should be no smaller than RQ.
The Energy Eﬃciency Research of Traﬃc Oﬄoading Mechanism
279

Then
wmm,u𝜏mm,u ≥RQ
(15)
𝜏mm,u = log2(1 +
PTm
(𝜃m + 1)
𝛼m + 2
2𝜎2Wm
𝜂
D
𝛼m
m
)
(16)
where wmm,u is the bandwidth of MMU, the SBSs are considered to be uniformly distrib‐
uted in the macro cell.
In addition, the area served by MBS except SBSn is
𝜋D2
m′ = 𝜋D2
m −
∑
s∈B
𝜋D2
s
(17)
Next, we consider an MSU. The outage probability constraint of the MSUs can be
written as
wms,u𝜏ms,u ≥RQ
(18)
where
𝜏ms,nu = log2(1 +
PTm
(𝜃m + 1)
𝛼m + 2
2𝜎2Wm
𝜂
D
𝛼m
ms,n
)
(19)
wms,u is the bandwidth of MSU, and Dms,u denotes the distance between the MBS and
SBSn.
The outage probability constraint of the WWUs is
ww,u𝜏w,u ≥RQ
(20)
where
𝜏w,u = log2(1 +
Pw
(𝜃w + 1)
𝛼w + 2
2𝜎2Ww
𝜂
D
𝛼w
w
)
(21)
4
Energy Eﬃciency Maximization for Single-SBS Case
In this section, we optimize the traﬃc oﬄoading for the single small cell case, where
the HSBS and RSBS are analyzed respectively.
4.1
Outage Probability Analysis
For the single-HSBS case, the network energy eﬃciency is
EE = TH
Psum
(22)
280
K. Wu et al.

TH is the network throughput [17], the network throughput is deﬁned as the total amount
of data that is successfully received by the users per unit time and area, and is measured
in bits/sec/m2, Psum is the total on-grid power consumption.
TH = IH( rmm
𝜋D2
m′
+
rss,n
𝜋D2
s,n
+
ra
ms,n
𝜋D2
ms,n
+
ra
w,n
𝜋D2
w,n
) + (1 −IH)( rmm
𝜋D2
m′
+
ro
ms,n
𝜋D2
ms,n
+
ro
w,n
𝜋D2
w,n
)
(23)
rmm = 𝜌m𝜋D2
m′rmm,u
(24)
rss = 𝜓1𝜌s𝜋D2
srss,u
(25)
rms = 𝜓2𝜌s𝜋D2
srms,u
(26)
rw = (1 −𝜓1 −𝜓2)𝜌s𝜋D2
srms,u
(27)
where IH is a 0–1 indicator denoting whether SBSn is active or not, rmm is the total data
rate of MMUs, rss is the total data rate of SSUs, ra
ms is the total data rate of MSUs when
the SBS is active, ro
ms is the total data rate of MSUs when the SBS is not active.ra
w,n is the
total data rate of WWUs when the SBS is active, ro
w,n is the total data rate of MMUs when
the SBS is not active. 𝜓1 is the traﬃc oﬄoading ratio of the SBS. 𝜓
2 is the traﬃc oﬄoading
ratio of the MBS.
Psum = PMBS + PHSBS + Pwiﬁ
(28)
where PMBS, PHSBS and Pwiﬁ denote the on-grid power consumptions of MBS, HSBS and
wiﬁ access points respectively. PMBS,PHSBS and Pwiﬁ can be derived based on Eq. (8).
While wmm is constrained by Eq. (12), wa
ms and wo
ms are the corresponding bandwidth
needed by the MSUs. Then we have
PMBS = PCm + 𝛽mPTm
Wm
(wmm + IHwa
ms + (1 −IH)wo
ms)
(29)
In addition, as the HSBS consumes on-grid power only when the battery is empty, we
have
PHSBS =
⎧
⎪
⎨
⎪⎩
IHq0(PCs + 𝛽sPTs
Ws
wss)
, 𝜆E < UE
0
, 𝜆E ≥UE
(30)
where q0 is the probability of empty energy queue, obtained from Eq. (10).
The Energy Eﬃciency Research of Traﬃc Oﬄoading Mechanism
281

Then
Pwiﬁ= 𝛽wPTw
Ww
www
(31)
Then, the network energy eﬃciency maximization problem of the single-HSBS case
can be formulated as follows:
P1:
max
IH,UE
EE
(32)
s.t.
rmm,u ≥RQ, rss,u ≥RQ, rms,u ≥RQ, rw,u ≥RQ
(33)
0 ≤wmm + IHwa
ms + (1 −IH)wo
ms ≤Wm
(34)
0 ≤wss ≤Ws
(35)
0 ≤www ≤Ww
(36)
where Eq. (33) guarantees the QoS, Eqs. (34), (35) and (36) are due to the bandwidth
limitations of MBS, HSBS, and wiﬁ access points, respectively. UE can be derived based
on the power consumption model of HSBS Eq. (9).
In practice, the HSBS usually provides higher energy eﬃciency compared with the
MBS, due to shorter transmission distance and lower path loss. As a result, more subcar‐
riers should be utilized to oﬄoad more users if the HSBS is active, Besides,
wmm + wo
ms > Wm happens when the MBS is overloaded, in which case the HSBS should
be activated to relieve the burden of the MBS. But the SBS can only service the amount
while wss < Ws.
4.2
Single-RSBS Case
Unlike HSBS, RSBS do not consume on-grid power. Whereas, the SSUs have to be
served by the MBS when the battery is empty, which causes handover and on-grid power
consumption. The average power consumption is given by
Psum = PMBS + PHO + Pwiﬁ
(37)
where PMBS is the power consumption of the MBS, and PHO reﬂects the additional power
consumed by SSU handover.
Denote by IR ∈{0, 1} the ON-OFF state of the RSBS. If RSBn is active, handover
happens in the following cases: (1) RSBn is shut down when the battery runs out; (2)
RSBn is reactivated when new energy arrives. According to the energy queueing model,
the ﬁrst case corresponds to the event when L+ transits from 1 to 0, with frequency of
282
K. Wu et al.

q1A21UE after the energy queue becomes stable. Due to the duality between the two
cases, the additional handover power consumption is given by
PHO = IR ⋅2q1A21UECHO =
⎧
⎪
⎨
⎪⎩
2IR(1 −𝜆E
UE
)(1 −e
−𝜆E
UE )UECHO , 𝜆E < UE
0
, 𝜆E ≥UE
(38)
Note that SBSn may be shut down due to energy shortage even when its state is set
as on, in which the MBS has to utilize more bandwidth to serve SSUn with additional
bandwidth. Based on Eq. (8), the average on-grid power consumption of the MBS is
given as follows:
PMBS = PCm + 𝛽mPTm
Wm
(wmm + IR((1 −q0)wa
ms + q0wo
ms) + (1 −IR)wo
ms)
(39)
where wmm is constrained by Eq. (15), wa
ms and wo
ms denote the bandwidth needed by the
MBS to serve MSUs when the RSBS is active and shut down, respectively.
The average on-grid power consumption of the wiﬁ access point is given as follows:
Pwiﬁ= 𝛽wPTw
Ww
www
(40)
Thus, the energy eﬃciency maximization problem can be formulated as follows.
P2:
max
IH,UE
EE
(41)
s.t.
rmm,u ≥RQ, rss,u ≥RQ, rms,u ≥RQ, rw,u ≥RQ
(42)
0 ≤wmm + wo
ms ≤Wm
(43)
0 ≤wss ≤Ws
(44)
0 ≤www ≤Ww
(45)
where the objective function is the network energy eﬃciency, Eq. (42) guarantees the
QoS, Eqs. (43), (44) and (45)account for the bandwidth limitation of MBS, RSBS and
wiﬁ access points, respectively.
The Energy Eﬃciency Research of Traﬃc Oﬄoading Mechanism
283

5
Energy Eﬃciency Maximization for Single-SBS Case
In this section, we evaluate the energy eﬃciency of the optimal solution for the single-
SBS case. The SBSs are micro BSs, and the main simulation parameters can be found
in Tables 1and 2. Solar power harvesting devices are equipped at RSBS and HSBS.
Table 1. Power model parameters
Transmit power PT(w)
Constant power PC(w)
Coeﬃcient 𝛽
Macro
20
130
4.4
Micro
2
56
2.6
Wiﬁ
0.025
0
1
Table 2. Simulation parameters
Parameter
Value
Parameter
Value
Dm
1000 m
Ds
100 m
Dms
900 m
Dw
10
𝛼m
3.5
𝛼s
4
𝛼w
5
RQ
300 kbps
𝜎2
−104 dBm/MHZ
𝜂
0.05
𝜃s
500
𝜃m
1000
𝜃w
300
Wm
20 MHz
Ws
5 MHz
Ww
1 MHz
5.1
Network Energy Eﬃciency of Single-SBS with Diﬀerent User Density
Figure 2 shows the network energy eﬃciency of no oﬄoading and oﬄoading diﬀerent
users with diﬀerent user density under given system parameters.
Simulation shows with the amount of oﬄoading traﬃc to wiﬁ access points
increasing, the energy eﬃciency is decreasing, regardless of the energy arrival amount.
Because SBSs are powered by conventional energy and green energy, wiﬁ access points
are powered only by conventional energy and the radio of output and power consumption
decreases quickly. So the following contents are not including wiﬁ case.
The red line and black line with no point show the no oﬄoading case. It can be seen
that energy eﬃciency generally increases with the increase of oﬄoading ratio and with
the increase of users, and oﬄoading’s is larger than no oﬄoading. Besides, in the actual
situation, wss > Ws happens when more users are oﬄoaded to the SBS, in which case
not all users can be oﬄoaded to the SBS for satisfying the QoS of every user, so we can
choose some users whose energy eﬃciency is maximized to oﬄoad to the SBS. For
RSBS, because of diﬀerence of user density, the energy eﬃciency’s variation trend is
diﬀerent. Figure 2(b) shows when the user density is large enough, energy eﬃciency is
maximized with the oﬄoading radio is not until 1.
284
K. Wu et al.

Fig. 2. The energy eﬃciency with diﬀerent oﬄoading ratio and diﬀerent user density of single
SBS. (Color ﬁgure online)
5.2
Network Energy Eﬃciency of Single-SBS with Diﬀerent Energy Arrival Rate
Figure 3 shows the network energy eﬃciency of no oﬄoading and oﬄoading diﬀerent
users with diﬀerent user density and diﬀerent energy arrival rate under the same system
parameters. Figure 3(a) shows the energy eﬃciency for the HSBS, which increases with
the increase of energy arrival rate in the small cell and with the increase of user density.
Figure 3(b) shows the energy eﬃciency for the RSBS. It can be seen the energy eﬃ‐
ciency’s variation trend is diﬀerent with the increase of energy arrival rate, when energy
arrival rate is small enough, the energy eﬃciency is maximized with the oﬄoading radio
not until 1.
The Energy Eﬃciency Research of Traﬃc Oﬄoading Mechanism
285

Fig. 3. The energy eﬃciency with diﬀerent oﬄoading ratio and diﬀerent energy arrival rate of
single SBS.
6
Conclusions and Future Work
In this paper, we have investigated the network energy eﬃciency through oﬄoading
traﬃc for green heterogeneous networks. The analytical results reﬂect maximized
energy eﬃciency with diﬀerent user density through traﬃc oﬄoading, we also analyze
oﬄoading case of diﬀerent energy arrival rate, so we can choose how much traﬃc should
be oﬄoaded to the SBS and wiﬁ access points with diﬀerent energy arrival rate. But we
do not consider the case that all SBSs coexist, which SBS should be activated and how
much traﬃc should be loaded, next we will achieve this with new limits.
Acknowledgment. This work was supported by the National Natural Science Foundation of
China. (No. 61372117), and the State Major Science and Technology Special Projects of China
under Grant 2016ZX03001017-004.
286
K. Wu et al.

References
1. Hasan, Z., Boostanimehr, H., Bhargava, V.K.: Green cellular networks: a survey, some
research issues and challenges. IEEE Commun. Surv. Tutor. 13(4), 524–540 (2011)
2. Correia, L.M., Zeller, D., Blume, O., et al.: Challenges and enabling technologies for energy
aware mobile radio networks. IEEE Commun. Mag. 48(11), 66–72 (2010)
3. Damnjanovic, A., Montojo, J., Wei, Y., et al.: A survey on 3GPP heterogeneous networks.
IEEE Wirel. Commun. 18(3), 10–21 (2011)
4. Zhang, N., Cheng, N., Gamage, A., Zhang, K., Mark, J.W., Shen, X.: Cloud assisted HetNets
toward 5G wireless networks. IEEE Commun. Mag. 53(6), 59–65 (2015)
5. Ismail, M., Zhuang, W., Serpedin, E., Qaraqe, K.: A survey on green mobile networking:
From the perspectives of network operatorsand mobile users. IEEE Commun. Surv. Tutor.
17(3), 1535–1556 (2014)
6. Andrews, J., et al.: What will 5G be? IEEE J. Sel. Areas Commun. 32(6), 1065–1082 (2013)
7. Zhuo, X., Gao, W., Cao, G., et al.: Win-Coupon: an incentive framework for 3G traﬃc
oﬄoading. In: IEEE International Conference on Network Protocols, pp. 206–215. IEEE
Computer Society (2011)
8. Zhuo, X., Gao, W., Cao, G., et al.: An incentive framework for cellular traﬃc oﬄoading.
IEEE Trans. Mob. Comput. 13(3), 541–555 (2014)
9. Usman, M., Vastberg, A., Edler, T.: Energy eﬃcient high capacity HETNET by oﬄoading
high QoS users through femto. In: IEEE International Conference on Networks, pp. 19–24.
IEEE Computer Society (2011)
10. Arshad, M.W., Vastberg, A., Edler, T.: Energy eﬃciency gains through traﬃc oﬄoading and
traﬃc expansion in joint macro pico deployment. In: 2012 IEEE Communications and
Networking Conference (WCNC), pp. 2203–2208. IEEE (2012)
11. Dhillon, H., Li, Y., Nuggehalli, P., Pi, Z., Andrews, J.: Fundamentals of heterogeneous
cellular networks with energy harvesting. IEEE Trans Wirel. Commun. 13(5), 2782–2797
(2014)
12. Zhang, S., Zhang, N., Zhou, S., et al.: Energy-aware traﬃc oﬄoading for green heterogeneous
networks. IEEE J. Sel. Areas Commun. 34(5), 1116–1129 (2016)
13. Zhang, X., Zheng, Z., Shen, Q., Liu, J., Shen, X., Xie, L.: Optimizing network sustainability
and eﬃciency in green cellular networks. IEEE Trans. Wirel. Commun. 13(2), 1129–1139
(2014)
14. Yang, J., Ulukus, S.: Optimal packet scheduling in an energy harvesting communication
systems. IEEE Trans. Wirel. Commun. 60(1), 220–230 (2012)
15. Nakagawa, K.: On the series expansion for the stationary probabilities of an M/D/1 queue. J.
Oper. Res. Soc. Jpn 48(2), 111–122 (2005)
16. Yu, P.S., Lee, J., Quek, T.Q.S., et al.: Traﬃc oﬄoading in heterogeneous networks with
energy harvesting personal cells - network throughput and energy eﬃciency. IEEE Trans.
Wirel. Commun. 15(2), 1146–1161 (2016)
17. Wei, Y., Ren, C., Song, M., Yu, R.: The oﬄoading model for green base stations in hybrid
energy networks with multiple objectives. Int. J. Commun Syst 29(11), 1805–1816 (2016)
The Energy Eﬃciency Research of Traﬃc Oﬄoading Mechanism
287

A Game-Theoretic Approach for Joint
Optimization of Sensing and Access
in Cognitive Cellular Heterogeneous Networks
Changqing Pan1, Ya’nan Xiao2, Yinglei Teng2(&), Weiqi Sun2,
and Xiaoqi Qin2
1 China Railway Test and Certiﬁcation Center (CRCC),
No.2 Daliushu Road, Xizhimen-wai, Beijing 100081, China
2 Beijing University of Posts and Telecommunications,
No.10 Xitucheng Road, Haidian District, Beijing 100876, China
lilytengtt@gmail.com
Abstract. Cellular heterogeneous networks comprising small-cells coexisting
with macro-cells have emerged as a promising solution to improve in-building
coverage and capacity of wireless networks. By assuming the scenario of cog-
nitive radio enabled cellular heterogeneous network (CCHN), where the small
base stations (SBSs) carry out spectrum sensing, severe cross-tier interference
are portable, however, the joint design of spectrum sensing and access poses
new technology challenges. Furthermore, the joint optimization encompasses
afﬂuent heterogeneous nodes and connections, which renders the centralized
paradigm messy for enormous signaling and great computation. In this paper,
we propose a decentralized approach by formulating the non-cooperative power
allocation game (NPAG) wherein each SBS competes for the maximization his
own opportunistic throughput by joint choice of the sensing factors and the
resource allocation strategy, involving interference and energy constraints.
Further, the iterative water-ﬁlling (IWF) algorithm is utilized to deal with the
non-convexity of the game. Simulation results show that the proposed game
theoretical formulations yield a considerable performance improvement for the
joint optimization problem.
Keywords: CCHN  Cognitive radio  Joint optimization  Game theory
1
Introduction
Due to the ability to bring about massive spatial reuse of frequency, the low-power,
short-range mini-base stations (BSs), referred to small-cells (femto-cell, pico-cell) are
increasingly important for improving network capacity. These small-cells coexist with
conventional macro-cell networks, giving rise to cellular heterogeneous networks
(CHNs). The structure of CHN causes interference problem to each layer. The inter-tier
(inner-tier) interference (ITI) from the Macro BSs (small-cell BSs) to the small-cell
users can be critical challenge to the CHN. Severely, the lower spectral efﬁciency
(SE) and higher interference often results in great loss in the capacity and coverage.
Thereby, it is necessary to improve SE and reduce interference.
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 288–300, 2018.
https://doi.org/10.1007/978-3-319-74521-3_32

Apparently, small-cells provide the ﬂexibility of network topology while cognitive
radio (CR) enables the agile usage of spectrum resource and helps small-cells avoid
major interference sources. The CHN will be favored if we combine small-cell and
cognitive radio technologies, which is the CR enabled Cellular Heterogeneous
Networks (CCHNs). However, challenges arise for how to implement CR in CCHNs.
(1) The duration of the spectrum-sensing in one slot becomes the tough dilemma. The
longer sensing time helps higher sensing accuracy (SA), however, it also causes less
time for the data-transmission period. (2) The sensing threshold affects SA and SE
inversely, i.e., the higher sensing threshold means more accurate sensing results, but
lower spectral efﬁciency, since access opportunities are likely to be missed. Hence, the
design of the sensing factors should consider the tradeoff between SA and SE, so that
the access availability should be optimized. Resource allocation strategy for reducing
interference is another important issue that must be addressed to increase SE.
Summarily, the mutual suppression and inﬂuence between sensing and access have
motivated us to investigate the joint optimization of spectrum-sensing conﬁguration
and resource allocation in CCHNs.
The idea of considering sensing and resource allocation jointly has been investi-
gated in conventional cognitive radio network (CRN). Several previous works [1, 2]
have already considered the tradeoff between sensing capabilities and throughput of
secondary users. Some researches solve the joint optimization problem using the
convex optimization theory, such like [1–3], and some formulate the joint problem
using the game theory, such like [4, 5]. There are still some other papers adopting smart
algorithm, to name some, greedy algorithm [6], evolution algorithm [7] to solve this
kind of problem.
Also, there are plenty of literatures put insights on the design of spectrum sensing
method or access strategies in CCHNs. The authors of [8] present one cycle
stationary-based spectrum sensing method, the design of which is based on the study of
detection probability (DP), ignoring the jointing research of resource allocation. In [9],
the authors provide spectrum sharing algorithm according to existing sensing results,
basing on the model of ALOHA, DSMA. However, the issue of joining optimization in
CCHNs has barely been addressed due to afﬂuent heterogeneous nodes and connec-
tions, the messy interference environment, etc.
Based on the aforementioned related work, in this paper, we propose a decentral-
ized approach by formulating the non-cooperative power allocation game (NPAG)
wherein each SBS competes against the others by choosing jointly the sensing factors
and the resource allocation strategy. Then, a pricing mechanism with constraints of
sensing accuracy and power consumption is introduced to reduce the interference
caused by the sensing errors. To deal with the non-convex issue, we compute the
optimal allocation strategy under given sensing time using iterative water-ﬁlling
(IWF) algorithm, then, select the optimal solution from multi groups of sensing factors.
Finally, the performance of our proposed algorithm is analyzed by making comparison
with average power allocation (APA) algorithm in system level simulation.
A Game-Theoretic Approach for Joint Optimization
289

2
System Model
2.1
Network Scenario
We consider a CCHN consisting one MBS and multiple cognitive FBSs, where
femto-cells coexist with macro-cell in overlay approach. Each FBS operates in the
closed access mode where only authorized UEs are allowed to access to it. The sets of
FBSs and MUEs are denoted by SF ¼ i; i ¼ 1; 2; . . .; NF
f
g and SM ¼ j; j ¼
f
1; 2; . . .; NMg. The femto-cells share same spectrum while they adopt the OFDMA as
the spectrum allocation strategy in each cell. Speciﬁcally, the total spectrum band is
divided into NC orthogonal sub-channels, marked as SC ¼ n; n ¼ 1; 2; . . .; NC
f
g. We
focus on block transmissions over channels which are assumed to be perfectly syn-
chronized. With a ﬁxed length T, the transmission frame of FBS i is divided in two
slots: sensing slot si and data slot as shown in Fig. 1. During sensing interval, FBSs
sense the environment, looking for the spectrum holes, whereas during the data slot
they possibly transmit over the licensed spectrum detected as available. The sensing
and transmission phases are described in 2.1 and 2.2 respectively.
The channel model follows Rayleigh fading and the large-scale path loss. The
additive complex white Gaussian noise is also assumed (with mean being zero and
variance being r2). The channel fading between FBS i and its authorized UEs on the nth
subcarrier is normally indicated as gi;n

2. Meanwhile, gM;j;n

2 represents the channel
fading between MBS and MUE j on the nth subcarrier, and gi;j;n

2 stands for that
between the FBS i and the MUE j. Besides, the channel gain between MBS and the
UEs in FBS i is denoted as
gM;i;n

2. In CCHNs, the interference brought by the
mistaken sensing between the BSs in the different tier is not considered when
restricting sensing accuracy. Therefore, we just analyze the inner-tier which is depicted
in Fig. 2.
The Spectrum Sensing Problem
Assume FBSs sense the licensed channels using energy detection in non-cooperative
way. In sensing phase, we deﬁne the event of MBS’s occupation as follows:
H1: The MBS takes use of the channel.
H0: The MBS is idle on the channel.
Fig. 1. Frame structure of the cognitive FBSs
290
C. Pan et al.

The performance of the energy detector scheme is measured in terms of the DP
Pd
i;nðsi; ei;nÞ and false-alarming probability (FP) P f
i;nðsi; ei;nÞ. DP and FP are the prob-
abilities detecting that the channel is busy under the case H1 and H0. Here, employing
the results of [1], we write Pd
i;nðsi; ei;nÞ and P f
i;nðsi; ei;nÞ as in (1)-(2) respectively, where
si denotes the sensing time of FBS i, and ei;n is the decision threshold of FBS i on
sub-channel n, and l is the sampling rate. Besides, Gn ¼ ð g1;n

2; g2;n

2; . . .; gNF;n

2ÞT
(superscript T means transpose operation). I means an identity matrix, and Diag Gn
ð
Þ is
a diagonal matrix formed by elements in Gn. Q(.) is the Q-function and ||.||1 of a matrix
denotes its entry wise 1-norm, which is the summation of absolute values of all the
elements.
Pd
i;nðsi; ei;nÞ ¼ Q
ei;n  r2NF þ Gn
k
k1




ﬃﬃﬃﬃﬃﬃ
lsi
p
r
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 r2I þ 2Diag Gn
ð
Þ
k
k1NC
p
 
!
ð1Þ
P f
i;nðsi; ei;nÞ ¼ Q
ei;n  r2NF


ﬃﬃﬃﬃﬃﬃ
lsi
p
r2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2NFNC
p


ð2Þ
Deriving from (1), P f
i;nðsi; ei;nÞ can be further expressed with respect to Pd
i;nðsi; ei;nÞ
as (3), where Rn ¼ r2I þ 2DiagðGnÞ.
P f
i;nðsi; Pd
i;nÞ ¼ Q
Q1 Pd
i;nðsi; ei;nÞ

	
r
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 Rn
k
k1NC
p
þ Gn
k
k1
ﬃﬃﬃﬃﬃﬃ
lsi
p
r2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2NFNC
p
0
@
1
A
ð3Þ
In order to secure the performance of MUEs, it is rational to set system thresholds
of the minimal DP and maximal FP. Note that if and only if the received power from
MBS on this channel is less than the spectrum sensing threshold ei;n, can the FBS uses
the channel n. Thereby, ei;n should be carefully gauged to improve the access oppor-
tunity to make sure that the MUEs do not get severe interference.
Fig. 2. The CCHN with inner-tier/cross-tier interference
A Game-Theoretic Approach for Joint Optimization
291

Pd
i;nðsi; ei;nÞ  Pd
th; n 2 SC; i 2 SF
ð4Þ
P f
i;nðsi; ei;nÞ  P f
th; n 2 SC; i 2 SF
ð5Þ
The Transmission Phase
Cognitive FBSs coexist in the network operating in a non-cooperative manner, and no
centralized authority is assumed to handle the network access for the FBSs. The
transmission strategy of each FBS i is then the power allocation vector pi ¼
pi;n

NC
n¼1
and the channel allocation vector bi ¼
bi;n

NC
n¼1 over the NC subcarriers (pi;n is the
power allocated over channel n, bi;n is the sub-channel allocation indicator that it equals
1when the FBS i is allocated with sub-channel n, and zero not).
Successful detection: The goal of each FBS is to optimize its opportunistic spectral
utilization of the licensed spectrum. According to this paradigm, each sub-channel n is
available for the transmission of FBS i if no MBS’s signal is detected over that
frequency band, which happens with probability 1  P f
i;nðsi; ei;nÞ. This motivates the
use of the aggregate opportunistic throughput as a measure of the spectrum efﬁciency
of each FBS i. Given the power allocation proﬁle p , pi
ð ÞNF
i¼1 of the FBSs, the detection
thresholds ei , ei;n

NC
n¼1, and sensing time si, the aggregate opportunistic throughput of
FBS i is deﬁned as
Riðsi; ei; pi; biÞ ¼ ð1  si
TÞ
X
NC
n¼1
PrðH0
nÞ 1  P f
i;nðsi; ei;nÞ

	
ri;nðpi;n; bi;nÞ
ð6Þ
where 1  si=Tðsi\TÞ is the portion of the frame duration available for opportunistic
transmissions, P f
i;nðsi; ei;nÞ is the FP deﬁned in (2), and ri;nðpi;n; bi;nÞ is the maximum
rate achievable of FBS i over channel n when no MBS’s signal is detected and the
power allocation of the FBSs is given. The maximum achievable rate ri;nðpi;n; bi;nÞ is
expressed as
ri;nðpi;n; bi;nÞ ¼ bi;n logð1 þ
pi;n gi;n

2
r2 þ P
j6¼i
pj;n gj;i;n

2Þ
ð7Þ
where r2 is the variance of the background noise over channel n at the receiver UE
associated to FBS i (assumed to be Gaussian zero-mean distributed).
As mentioned above, P f
i;n can be written as (3). According to [1], when
Pd
i;nðsi; ei;nÞ ¼ Pd
th, (6) can achieve maximal value. Thereby, (7) can be rewritten as the
following one:
292
C. Pan et al.

Riðsi; pi;biÞ ¼ ð1  si
TÞ
X
NC
n¼1
PrðH0
nÞ 1  P f
i;nðsiÞ

	
ri;n
ð8Þ
Here,
P f
i;nðsiÞ ¼ Q Q1 Pd
th


r
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 Rn
k
k1NC
p
þ Gn
k
k1
ﬃﬃﬃﬃﬃﬃ
lsi
p
r2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2NFNC
p
 
!
ð9Þ
Energy constraints: To control the total power cost by FBSs, we propose to impose
individual energy constraints. Individual constraints are provided at the level of each
FBS i to limit the energy constraint. Individual energy constraints:
si
X
NC
n¼1
ps
i;n þ T  si
ð
Þ
X
NC
n¼1
pi;n  Emax
i
ð10Þ
where ps
i;n is the sensing power of FBS i on channel n, and Emax
i
is the maximum energy
limitation allowed to be generated by FBS i.
Interference constraints: To control the interference radiated by the FBSs, we pro-
pose to impose interference constraints in the form of individual constraints, which is
imposed at the level of each FBS i to limit the average interference generated at the
MBS. Individual interference constraints:
X
NC
n¼1
pmiss
i;n
gM;i;n

2pi;n  Imax
i
ð11Þ
where Imax
i
is the maximum average interference allowed interference allowed to be
generated by the FBS i, and pmiss
i;n
¼ 1  Pd
th represents the probability of FBS i failing
to detect the presence of the MBS on the sub-channel n and thus generating interfer-
ence against the MBS.
3
Joint Spectrum Sensing and Access Optimization Strategy
via Game Theory
In Sect. 2, we have deﬁned the opportunistic throughput Ri and the individual con-
straints (10)-(11). In the process, we assume that FBSs aim to maximize their own
throughput non-cooperatively, with the constraints holding. Therefore, a non-
cooperative game theory can be introduced to build an optimization model if we
regard each FBS as a game player and Ri as the player’s utility. Hence, in this section,
we focus on the formulation of the jointing optimization of the spectrum sensing and
resource allocation of the FBSs within the framework of game theory. We propose next
A Game-Theoretic Approach for Joint Optimization
293

equilibrium problem: games with individual constraints, and then provide a uniﬁed
analysis of the game.
As mentioned above, each FBS is modeled as a game player who aims to maximize
its own opportunistic throughput Riðsi; pi;biÞ by choosing jointly a proper power
allocation strategy pi ¼
pi;n

NC
n¼1, carrier allocation strategy bi ¼
bi;n

NC
n¼1, and
sensing time si, subject to individual constraints (10).
In order to enforce the constraints (10)-(11) while keeping the optimization as
decentralized as possible, we propose the use of a pricing mechanism through a penalty
in the utility function of each player. To be simple, we set the price of the ith player is
denoted by ci. Assume price ci in (12) is given. Therefore, we have the formulation (12).
Player i’s optimization problem is to determine, for the given c, a tuple ðsi; pi;biÞ,
maximize
si;pi;bi
Uiðsi; p;biÞ ¼ Riðsi; p;biÞ  ci 
X
NC
n¼1
ðsips
i;n þ ðT  siÞpi;nÞ
 
!
ð12Þ
s.t.
si
X
NC
n¼1
ps
i;n þ T  si
ð
Þ
X
NC
n¼1
pi;n  Emax
i
ð13Þ
smin  si  T; \8n 2 SC
ð14Þ
X
NC
n¼1
pmiss
i;n
gM;i;n

2pi;n  Imax
i
ð15Þ
Here,
(14)
is
derived
from
(2),
(3),
(5).
smin ¼ maxðs1
min; s2
min; . . .; sNC
minÞ,
sn
min ¼
Q1 P f
th
ð
Þr2 ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2NFNC
p
Q1 Pd
th
ð
Þr
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 Rn
k
k1NC
p
ð
Þ
2
l Gn
k
k2
1
.
For notational simplicity, we will use /i , si; pi;bi
ð
Þ to denote the strategy tuple of
player i.
4
Solution Analysis
This section is devoted to the solution analysis of the game (12). We start our analysis
by introducing the deﬁnition of non-cooperative power allocation game (NPAG).
According to (12), we can determine the optimal sub-channel matrix b once the
network vector p is determined. In addition, to simplify the jointing problem, we
compute the optimal power allocation vector with different sensing time, and then
choose the optimal tuple ðsi; pi;biÞ as the optimal solution. Therefore, the jointing
problem of sensing time and resource allocation turns into a power allocation problem,
and the game becomes a NPAG. G ¼
SF; pi
f gi2SF; Ui
f
gi2SF


as follows:
294
C. Pan et al.

NPAG : max
pi
Uiðsi; p;biÞ ¼ max Riðsi; p;biÞ  ci
X
NC
n¼1
ðsips
i;n þ ðT  siÞpi;nÞ
 
!
 
!
ð16Þ
st.
pi 2 Pi
ð17Þ
Here,
Pi ¼
pij si
X
NC
n¼1
ps
i;n þ T  si
ð
Þ
X
NC
n¼1
pi;n  Emax
i
;
X
NC
n¼1
pmiss
i;n
gM;i;n

2pi;n  Imax
i
(
)
ð18Þ
We call a network power vector p a Nash equilibrium if for every i 2 SF,
Uiðsi; pi; pi; biÞ  Uiðsi; qi; pi; biÞ for all Pi 2 Pi. In other words, at a Nash equi-
librium, given the transmission power vectors of other FBSs, no FBS can improve its
utility level by changing its own transmission power vector unilaterally.
For the given sensing time si, the utility Uiðsi; p; biÞ is a convex function of pi and
hence the problem is a convex problem. Therefore we can apply the Karush - Kuhn-
Tucker (KKT) condition to get the solution.
The Lagrangian function of FBS i is given as follows:
Li p
ð Þ¼ki
Emax
i

si
X
NC
n¼1
ps
i;n þ T  si
ð
Þ
X
NC
n¼1
pi;n
 
!
 
!
þ Ui p
ð Þ þ li
Imax
i

X
NC
n¼1
pmiss
i;n
gM;i;n

2pi;n
 
!
ð19Þ
Then, in the NPAG, the best response of FBS i is given by
pi;n ¼
ð1  si
TÞ PrðH0
nÞ 1  P f
i;nðsiÞ

	
bi;n
ci 
T  si
ð
Þ  ki T  si
ð
Þ  lipmiss
i;n
gM;i;n

2

	
ln 2

r2 þ P
j6¼i
pj;n gj;i;n

2
gi;n

2
2
664
3
775
þ
ki
Emax
i

si
X
NC
n¼1
ps
i;n þ T  si
ð
Þ
X
NC
n¼1
pi;n
 
!
 
!
¼ 0; ki  0;
li
Imax
i

X
NC
n¼1
pmiss
i;n
gM;i;n

2pi;n
 
!
¼ 0; li  0:
ð20Þ
where ki and li are the Lagrangian multipliers for the maximum energy constraint (10)
and interference constraint (11). x½  þ ¼ x if x  0 and 0 otherwise.
Unfortunately, it is hard to ﬁnd the max Lagrangian multiplier to solve the pi;n due
to two multipliers. To get around the difﬁculty, we consider an iterative water-ﬁlling
algorithm (IWF) mentioned in [10]. The optimal response of game can be found by
computing the multipliers iteratively until they reach a certain degree of accuracy.
The details of IWF algorithm is listed in Algorithm 1.
A Game-Theoretic Approach for Joint Optimization
295

296
C. Pan et al.

The sub-channel allocation strategy bi;n can be achieved by the corresponding
values of pi;n. bi;n ¼ 1 when pi;n [ 0 , vice versa. Furthermore, optimal solution /i can
be achieved by making comparison between multiple sets of ðp;bÞ with the given si.
5
Simulation Results and Analysis
In this section, simulation results are demonstrated to illustrate theoretical ﬁndings.
After testing the convergence of the proposed power allocation algorithm, performance
results with respect to throughput, EE and utility function are present, with the average
power allocation (APA) algorithm as comparison. The system is set up according to the
following simulation parameters (Table 1):
Table1. Simulation parameters
Parameter
Value
Number of MBS NM
1
Number of FBS NF
3
Number of Channel NC
5
Interference Threshold Imax
i
0.1 w
Maximal Iteration Number kmax 60
Accuracy Requirement d
10−9
Sampling Rate l
8 MHz
Frame Length T
20 ms
DP Threshold Pd
th
0.9
FP Threshold P f
th
0.2
Energy Threshold Emax
i
2 mj
Sensing Power Ps
i;n
1 mw
0
10
20
30
40
50
60
0.03
0.035
0.04
0.045
0.05
0.055
iteration number
 transmission power/w
 
 
n=1 i=1
n=1 i=2
n=2 i=1
n=2 i=2
Fig. 3. Convergence of the proposed power allocation algorithm
A Game-Theoretic Approach for Joint Optimization
297

Figure 3 presents the convergence of the proposed power allocation, where n and i
denote the index of channel and FBS. It can be seen that the power of FBS on each
channel declines gradually, and then keeps smooth. Meanwhile, the system gains the
stable performance. In the proposed algorithm, the power converges within 10 times to
achieve d accuracy.
From Figs. 4, 5 and 6, we provide the comparison of two schemes to show the
effect of sensing time on femto-cells’ throughput, utility function and the energy
efﬁciency (EE). As depicted in Fig. 4, the throughput ﬁrstly increases with the growth
of si, and then decreases to the zero for the proposed IWF algorithm. That is because
the higher si which means the lower FP, improves the availability of spectrum resource
and gains more throughput by making more FBSs access into the idle channels.
However, when the sensing time reaches to some threshold, it makes little effect on FP
and throughput. Contrarily, the shorter of transmission time reduces the transmission
efﬁciency and degrade the system performance. The same trend can be found in utility
function as illustrated in Fig. 5. It is not difﬁcult to notice that the price factor ki will
execute self-regulation according to the total power, aiming to limit the power of each
FBS. In contrary to throughput, the utility function achieves a lower value due to price
mechanism.
In Fig. 6, curve of EE keeps the same trend as in Fig. 5. Furthermore, IWF
algorithm is better than APA algorithm. That is because, in context of guaranteeing
throughput, IWF algorithm strongly reduces the total value of power allocation.
Figure 7 shows the increasing trend of throughput with the growth of inner SNR.
Since the higher SNR brings higher power allocation, which improves throughput of
femto-cells. While, with the increasing SNR, the inner-tier interference also become
larger, which limits the improvement of throughput. Thereby, the throughput is no
longer varied by the inner SNR.
Fig. 4. The comparison of throughput ver-
sus sensing time
Fig. 5. The comparison of utility function
versus sensing time
298
C. Pan et al.

6
Conclusion
In this paper, the problem of optimal multichannel sensing (with respect to sensing time
and threshold) and resource allocation (with respect to channel and power) in CCHN is
explored by formulating a game and the FBSs act as the players to maximize his own
opportunistic throughput by choosing jointly the sensing and access conﬁgurations.
A pricing mechanism that penalizes the FBSs in violating the power constraints is also
provided. Finally, we deal with the non-convexity of the game by utilizing the iterative
water-ﬁlling (IWF) algorithm. Simulation results show the proposed IWF algorithm
gains better performance in both femto-cell throughput and EE than traditional APA
algorithm.
Acknowledgement. This work was supported in part by the National Natural Science Foun-
dation of China under Grant No. 61372117, 61302081, 61421061, and the 863 project.under
Grant NO.2014AA01A701.
References
1. Fan, R., Jiang, H., Guo, Q., Zhang, Z.: Joint optimal cooperative sensing and resource
allocation in multichannel cognitive radio networks. IEEE Trans. Veh. Technol. 60(2), 722–
729 (2011)
2. Janatian, N., Sun, S., Modarres-Hashemi, M.: Joint optimal spectrum sensing and power
allocation in CDMA-based cognitive radio networks. IEEE Trans. Veh. Technol. 64(9),
3990–3998 (2015)
3. Zhang, N., Liang, H., Cheng, N., Tang, Y., Mark, J.W., Shen, X.S.: Dynamic spectrum
access in multi-channel cognitive radio networks. IEEE J. Sel. Areas Commun. 32(11),
2053–2064 (2014)
4. Wang, F., Wang, J., Li, W.W.: Game-theoretic analysis of opportunistic spectrum sharing
with imperfect sensing. EURASIP J. Wirel. Commun. Netw. (2016)
Fig. 6. The comparison of EE versus sens-
ing time
Fig. 7. The comparison of throughput versus
inner SNR
A Game-Theoretic Approach for Joint Optimization
299

5. Dai, Z., Wang, Z., Wong, V.W.S.: An overlapping coalitional game for cooperative
spectrum sensing and access in cognitive radio networks. IEEE Trans. Veh. Technol. 65(10),
8400–8413 (2016)
6. Zhang, S., Wang, H., Zhang, X., Cao, Z.: Optimal spectrum access algorithm based on
POMDP in cognitive networks. AEU Int. J. Electron. Commun. 69(6), 943–949 (2015)
7. Jiang, C., Chen, Y., Gao, Y., Liu, K.J.R.: Joint spectrum sensing and access evolutionary
game in cognitive radio networks. IEEE Trans. Wirel. Commun. 12(5), 2470–2483 (2013)
8. Bose, S., Natarajan, B.: Reliable spectrum sensing for resource allocation of cognitive radio
based WiMAX femtocells. In: The 9th Annual IEEE Consumer Communications and
Networking Conference-Wireless Consumer Communication and Networking, pp. 889–893,
14–17 January 2012
9. Cheng, S.-M., Ao, W.C., Tseng, F.-M., Chen, K.-C.: Design and analysis of downlink
spectrum sharing in two-tier cognitive femto networks. IEEE Trans. Veh. Technol. 61(5),
2194–2207 (2012)
10. Yu, W., Ginis, G., Ciofﬁ, J.M.: Distributed multiuser power control for digital subscriber
lines. IEEE J. Sel. Areas Commun. 20, 1105–1115 (2002)
300
C. Pan et al.

Energy Harvesting Relay Node Deployment
for Network Capacity Expansion
Zhiqiang Zhang1(✉), Yifei Wei1(✉), Bo Gu1, Xaojun Wang2, and Mei Song1
1 School of Electronic Engineering, Beijing University of Posts and Telecommunications,
Beijing 100876, People’s Republic of China
{zhangzhiqiang,weiyifei,gubo,songm}@bupt.edu.cn
2 Dublin City University, Dublin, Ireland
xiaojun.wang@dcu.ie
Abstract. The increasing traﬃc demands in the past few years require further
expansion of cellular network capacity. Densely deployed base stations may cause
a large amount of energy consumption and lots of environmental problems.
Relaying is considered as a promising cost-eﬃcient solution for capacity expan‐
sion and energy saving due to the physical characteristics and low power require‐
ments of the relay nodes. In this paper, we study the energy harvesting relay node
deployment problem in a cellular network where the user distribution may change
with time. We propose a greedy relay nodes deployment algorithm to satisfy the
network capacity requirements in diﬀerent time and put forward an operation
optimization algorithm to make full use of renewable energy. Simulation results
show that the network capacity can be increased signiﬁcantly and the energy
consumption can be reduced dramatically by the proposed algorithms.
Keywords: Heterogeneous cellular network · Capacity expansion
Energy harvesting · Relay node deployment
1
Introduction
In the recent years, the rapid rise in the popularity of smart devices requires a fast and
ubiquitous wireless network, which makes it necessary to expand the original network
capacity. However, increasing the number of macro base stations (BSs) in the cellular
network signiﬁcantly reduces the gain because of the elevated interference. Heteroge‐
neous networks (HetNets) provide services across both macro and small cells to optimize
the mix of capabilities and have attracted much attention in the recent years [1, 2]. Relay
networks, as one kind of HetNets, is a promising solution of performance enhancement
and energy saving. Relay is one of the proposed technologies in LTE-Advanced
networks for coverage expansion and capacity enhancement [3]. Relay Nodes (RNs) are
connected to the core network with wireless backhaul through an evolved Node B (eNB)
and they usually have much smaller energy consumption than BSs.
Over the years, cellular networks have become an important source of energy
consumption, reducing the network energy consumption has beneﬁts in both economic
and environmental aspects, which encourages people to look forward alterative power
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 301–310, 2018.
https://doi.org/10.1007/978-3-319-74521-3_33

supply. The usage of renewable energy sources such as wind power and photo-voltaic
energy can signiﬁcantly reduce traditional energy consumption. Many kinds of renew‐
able energy sources have been studied for green networks as introduced in [4].
The improvements of network performance have been widely studied. In the paper
[5], micro base stations are deployed to increase the area spectral eﬃciency (ASE) value.
The results show that the ASE can be improved signiﬁcantly when the micro base
stations located at the edges of macro cells where the received signal is worst. The energy
eﬃciency problem of cellular networks is studied in [6], excessively increasing the
number of micro base stations may reduce the energy eﬃciency, and there exists an
optimum number of deployed BSs. The capital expenditure and the total operational
expenditure are considered as the base station costs in [7]. A hierarchical power manage‐
ment architecture is introduced in [8], the authors focus on eﬀectively exploit the green
capabilities of networks. A green base station oﬄoading model is proposed in [9], by
predicting the value of green energy collected and updating the residual energy of each
BS, the maximum number of users that each BS can oﬄoad can be calculated to achieve
diﬀerent network performance. The article [10] mainly focuses on the energy supply
aspects in the network and investigates how to optimize the using of on-grid energy to
reduce the carbon emission rate. A green base station optimization algorithm is proposed
in [11] to reduce the traditional power consumption, the transmission power of BSs can
be adjusted to improve network performance. The deployment of RNs has been used to
improve network performances in the recent years such as connectivity by [12] and
lifetime maximization by [13]. The article [14] considered ﬁnite-state Markov channels
in the relay-selection problem to increase spectral eﬃciency, mitigate error propagation,
and maximize the network lifetime. These studies usually focus on the user distribution
in peak periods to ensure the network performance in the worst case while ignoring the
diﬀerences of traﬃc demands in diﬀerent positions and time periods, which may cause
much resource waste in the low-load state of the network.
In this paper, a more practical condition is considered. We deploy RNs to improve
the network capacity where one BS has already been deployed and the changes of user
distributions among diﬀerent time is full considered. In addition, The RNs are equipped
with energy harvesting devices so that they can harvest ambient energy. RNs still need
to be connected to the on-grid power network in case that the renewable energy sources
are inadequate.
The remainder of the article is organized as follows. In Sect. 2, the system model is
introduced. In Sect. 3, the problem, constraints and objective function are deﬁned.
Section 4 describes the proposed algorithms for these problems. Section 5 gives the
simulation parameters and results. Section 6 concludes the paper.
2
System Model
Consider a wireless network in which all users are served by one BS b. The capacity of
this network needs to be expanded to satisfy the growing traﬃc demand by deploying
RNs. The deployed RN set is denoted by R. The users, denoted by U, always connect
302
Z. Zhang et al.

to the BS b or a RN r which can provide the highest signal strength to them. The signal-
to-interference-plus-noise ratio (SINR) of user u can be written as
Γu(b) =
Ptx(b)gu,b
∑
r∈R
Ptx(r)gu,r + 𝜎2
(1)
Γu(r) =
Ptx(r)gu,r
Ptx(b)gu,b +
∑
r′∈R,r′≠r
Ptx(r′)gu,r′ + 𝜎2
(2)
Γu(b) and Γu(r) denote the SINR of users connected to the BS b and the RN r, respec‐
tively. Ptx(b) and Ptx(r) denote the transmission power of BS b and RN r, respectively.
gu,b is the channel gain from BS b to user u and gu,r is the channel gain from RN r to user
u. Thermal noise is denoted by 𝜎2. For simplicity, the shadow fading and small size
decline are ignored, only the path loss is considered. Then the network capacity of user
u can be calculated from SINR as
Cu = Bu log2(1 + Γu)
(3)
Bu is the bandwidth assigned to user u in equal bandwidth scheduling. This sched‐
uling allows BSs to share the resources equally among users [15].
The power consumption of a BS consists of two parts. The ﬁrst part is the static
power consumed with no transmission, and the other part depends on the transmission
power [16]. It can be given by
P(b) = Pc(b) + 𝛼bPtx(b)
(4)
Pc(b) is the static power of each BS and 𝛼b scales the transmission power depending
on the load.
The power consumption model of each RN is similar, however, unlike previous
studies, we consider their energy harvesting abilities. With the help of energy harvesting
devices, the RNs can produce energy from renewable sources such as sunlight or wind.
However, renewable energy sources may be limited and the generated energy may not
be enough to totally power the RNs [17]. So we deﬁne 𝛽=
Phv
P(r)max
 as the energy
harvesting rate, where Phv and P(r)max denote the average harvesting power and the
maximum consumption power of each RN, respectively. The RNs can consume less on-
grid power as long as 𝛽> 0 and can work for a whole day without consuming any on-
grid power when 𝛽≥1. Thus the consumption power of each RN can be written as
P(r) =
{
0
𝛽≥1
Pc(r) + 𝛼rPtx(r) −Phv 0 ≤𝛽< 1
(5)
Pc(r) is the static power of each RN and 𝛼r scales the transmission power depending
on the load, the average harvesting power is denoted by Phv.
Energy Harvesting Relay Node Deployment for Network Capacity Expansion
303

It’s assumed that users connected to the network always have data to transmit with
full buﬀer, in addition, no power control algorithm is used. Therefore, BSs and RNs are
fully utilized, 𝛼b and 𝛼r are constant for all BSs and RNs.
3
Problem Deﬁnition
Deploying RNs can improve the network capacity to a higher level, but it also brings extra
energy consumption. By efficiently deploy RNs, we can minimize the extra energy
consumption while meeting the requirement of capacity. The problem can be formulated as
min ∑
r∈R
P(r)
s.t. ∑
u∈U
Cu(B ∪R) ≥𝜆⋅Co
(6)
Cu(B ∪R) denotes the capacity of user u when both BSs and RNs are deployed. Co
is the original network capacity without deploying any RNs and the multiplier 𝜆≥1 is
the desired increasing times over Co after deploying RNs.
4
Solutions
Aﬀected by user living and working conditions, the user distribution may always change
with time in a day [18]. The deployment of BSs generally aims at satisfying the capacity
demand in peak hours, while ignoring the resource wastes in the other hours. It has been
reasonable in the past few years for the expensive costs and deployment inconvenience
of traditional BSs. With the deployment of RNs, these problems matter less for low costs
and high ﬂexibilities of RNs. A whole day can be divided into several parts, each of
them corresponding to a unique user distribution. Then the RNs can be deployed in each
time slice to meet the capacity requirements.
There can be lots of candidate locations for RN deploying in this area and we need
to select some of them. Furthermore, with the rising number of deployed RNs, the
network performance may not always increase due to the interference eﬀect, especially
when the RNs are located close. The strong interference between each other even
decreases the network capacity. On the other hand, inﬂuenced by the user distribution,
capacity contributions from diﬀerently located RNs may vary dramatically. Therefore,
the RN deployment problem is actually a combinatorial problem and it quickly becomes
unsolvable when the number of candidate locations becomes large.
Thus, a greedy algorithm is proposed to select the relatively better locations of RNs.
In this algorithm, a RN will be deployed at the location which can provide the most
increase of network capacity. The RNs will be added iteratively until the capacity
requirement is satisﬁed.
304
Z. Zhang et al.

Algorithm 1. Greedy RN Deployment Algorithm
In Algorithm 1, the deployed RN set is denoted by R and the candidate RN set is
denoted by RCAND. The algorithm should be executed for every time slice. Then we can
get a RN deployment schedule which consists of N sets of RNs, N is the number of time
slices. One set of the RNs will work only during the corresponding time slice and will
be turned oﬀ during the other time slices. All the RNs can continuously harvest energy,
even in turn-oﬀ time.
Then we focus on the energy consumption problem. Long turn-oﬀ time may cause
energy waste due to the limited battery capacity. One set of RNs which supposed to
work only in time slice ti actually can also work in tj as long as they have harvested
enough energy. However, the user distribution of tj may be diﬀerent and there is already
one set of RNs deployed in tj. Two sets of RNs working together may decrease the
network capacity due to the interference. So we propose an optimization algorithm to
merge the two sets of RNs together while satisfying the capacity requirements of both
time slices. In this algorithm, the RNs deployed in two time slices are simply merged
together ﬁrstly, then the nearest two RNs are replaced by a new RS which located at the
middle position of them, which can eﬀectively avoid redundant deployment and reduce
the interference.
Energy Harvesting Relay Node Deployment for Network Capacity Expansion
305

Rti,tj denotes the merged RN set and will work in both time slices ti and tj. dist(rx, ry)
gives the distance between rx and ry, getMidRN(rx, ry) gives a new RN that located at the
middle position of rx and ry.
5
Simulation Results
The deployment area size is 3 km × 3 km and served by one BS. RNs need to be deployed
in this area to improve the capacity. More candidate locations can get more accurate
results but also brings extra calculation complexity. The number of candidate locations
is set to 1600 after a set of experiments. The other parameters used in simulations are
given in Table 1.
306
Z. Zhang et al.

Table 1. Simulation parameters
Parameters
Values
Target capacity multiple
1.5
Number of time slices
12
Bandwidth
2 GHz
Static power of BS
168 W
Transmission power of BS
39.8 W
Static power of RN
20 W
Transmission power of RN
1 W
BS to user path loss
128.1 + 37.6 * log10 (d(km))
RN to user path loss
37 + 30 * log10 (d(m))
Thermal noise
−174 dBm/Hz
Firstly, we investigate the impact of RN deployment on network capacity. As is
shown in Fig. 1, with the continuously deploying of RNs, the network capacity increases
signiﬁcantly at ﬁrst, then it slows down and eventually decreases due to the gradually
increasing interference. The network capacity can be inﬂuenced by interference among
RNs signiﬁcantly. The maximum capacity that can be reached by deploying RNs is
limited. Expanding capacity to the maximum needs a large number of RNs, which may
be ineﬃcient.
The deployed positions of RNs are as shown in Fig. 2. Most of them located far from
the BS due to the lower SINR there. In addition, their locations are obviously aﬀected
by the user distribution. When user distribution changes in diﬀerent time, the optimum
deployed positions of RNs will change accordingly.
0
5
10
15
20
25
30
35
40
45
50
Number of Deployed Relay Nodes
1
1.2
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
3
Normalized Network Capacity
Interference Considered
Interference Ignored
Fig. 1. Increased capacity by relay nodes
Energy Harvesting Relay Node Deployment for Network Capacity Expansion
307

0
500
1000
1500
2000
2500
3000
Position (m)
0
500
1000
1500
2000
2500
3000
Position (m)
Relay Nodes in Time i
Base Station
Relay Nodes in Time j
Fig. 2. Positions of deployed relay nodes
In the following experiments, we focus on the energy consumption problem. If a RN
only works in non-adjacent time slices, it can make full use of its renewable energy. So
we divide the time slices into many groups and each group consists of several non-
adjacent time slices. The time slices in the same group has same RN deployment. If the
number of time slice groups is m, then m sets of RNs will be deployed. When the energy
harvesting rate is above 1/m, our split combination solution will reach zero on-grid
energy. With a certain energy harvesting rate, more groups of time slices can reduce the
on-grid energy signiﬁcantly. Deployment in 4 groups of time slices can save about 74.5%
on-grid energy over only one group of time slices (Fig. 3).
1
2
3
4
Number of Time Slice Groups
0
0.5
1
1.5
2
2.5
On-grid Energy Consumption (J)
10 7
0.1
0.2
0.3
0.4
0.5
Energy Harvesting Rate
Fig. 3. On-grid energy consumption of deployed relay nodes
308
Z. Zhang et al.

6
Conclusions
In this paper, we propose a greedy RN deployment algorithm and an optimization algo‐
rithm to improve the network capacity eﬃciently. The energy harvesting ability of RNs
and user distribution changes are considered in the process of deployment. Imprudently
increasing the number of RNs may decrease the network capacity due to the interference.
The proposed deployment algorithm greedily selects relatively better positions from
candidate locations to deploy RNs until the required capacity is satisﬁed. Due to the
heuristic nature of the algorithm, the complexity of deployment problem is signiﬁcantly
reduced. Then the optimization algorithm merges diﬀerent sets of RNs according to the
energy harvesting ability to make full use of renewable energy. Zero on-grid energy
consumption can be achieved by this algorithm. The simulation results show that the
capacity demand can be satisﬁed eﬃciently and the on-grid energy consumption can be
signiﬁcantly reduced by the proposed algorithms.
Acknowledgments. This work was supported by the National Natural Science Foundation of
China (No. 61571059).
References
1. Damnjanovic, A., Montojo, J., Wei, Y., et al.: A survey on 3GPP heterogeneous networks.
IEEE Wirel. Commun. 18(3), 10–21 (2011)
2. Ghosh, A., Ratasuk, R.: Essentials of LTE and LTE-A. Cambridge University Press (2011)
3. Saleh, A.B., Redana, S., Hämäläinen, J., Raaf, B.: Comparison of relay and pico eNB
deployments in LTE-Advanced. In: IEEE 70th Vehicular Technology Conference (VTC
2009-Fall), USA, September 2009
4. Schmitt, G.: The green base station. In: International Conference on Telecommunication -
Energy Special Conference, VDE, pp. 1–6 (2009)
5. Richter, F., Fehske, A.J., Fettweis, G.P.: Energy eﬃciency aspects of base station deployment
strategies for cellular networks. In: Vehicular Technology Conference Fall, pp. 1–5. IEEE
Xplore (2009)
6. Coskun, C.C., Ayanoglu, E.: A greedy algorithm for energy-eﬃcient base station deployment
in heterogeneous networks. In: IEEE International Conference on Communications, pp. 7–
12. IEEE (2015)
7. Chen, Y., Zhang, S., Xu, S.: Characterizing energy eﬃciency and deployment eﬃciency
relations for green architecture design. In: IEEE International Conference on
Communications Workshops, pp. 1–5. IEEE (2010)
8. Wei, Y., Wang, X., Fialho, L., Bruschi, R., Ormond, O., Collier, M.: Hierarchical power
management architecture and optimum local control policy for energy eﬃcient networks. J.
Commun. Netw. 18(4), 540–550 (2016)
9. Wei, Y., Ren, C., Song, M., Yu, R.: The oﬄoading model for green base stations in hybrid
energy networks with multiple objectives. Int. J. Commun. Syst. 29(11), 1805–1816 (2016)
10. Han, T., Ansari, N.: Optimizing cell size for energy saving in cellular networks with hybrid
energy supplies. In: Global Communications Conference, pp. 5189–5193. IEEE (2012)
11. Pamuklu, T., Ersoy, C.: Optimization of renewable green base station deployment. In: Green
Computing and Communications, pp. 59–63. IEEE (2013)
Energy Harvesting Relay Node Deployment for Network Capacity Expansion
309

12. Cheng, X., Du, D.Z., Wang, L., et al.: Relay sensor placement in wireless sensor networks.
Wirel. Netw. 14(3), 347–355 (2008)
13. Hou, Y.T., Shi, Y., Sherali, H.D., et al.: Prolonging sensor network lifetime with energy
provisioning and relay node placement. In: 2005 Second IEEE Communications Society
Conference on Sensor and Ad Hoc Communications and Networks, IEEE SECON 2005, pp.
295–304. IEEE Xplore (2005)
14. Wei, Y., Richard Yu, F., Song, M.: Distributed optimal relay selection in wireless cooperative
networks with ﬁnite state Markov channels. IEEE Trans. Veh. Technol. 59(5), 2149–2158
(2010)
15. GPP, TR 36.814: Further advancements for E-UTRA physical layer aspects (Release 9).
Technical report, March 2010
16. Richter, F., Fehske, A.J., Fettweis, G.P.: Energy eﬃciency aspects of base station deployment
strategies for cellular networks. In: Vehicular Technology Conference Fall, pp. 1–5. IEEE
Xplore (2009)
17. Han, T., Ansari, N.: ICE: intelligent cell brEathing to optimize the utilization of green energy.
IEEE Commun. Lett. 16(12), 866–869 (2012)
18. Nousiainen, S., Kordybach, K.: User distribution and mobility model framework for cellular
network simulations. In: VTT Information Technology, pp. 518–522 (2002)
310
Z. Zhang et al.

Parameters Optimization for KFKM Clustering Algorithm
Based on WiFi Indoor Positioning
Zhengying Hu1(✉), Lujuan Ma2, Baoling Liu1, and Zhi Zhang1
1 State Key Laboratory of Networking and Switching Technology, Beijing University of Posts
and Telecommunications, Beijing 100876, People’s Republic of China
Zhengying_hu@163.com
2 Hisense Co., Ltd, Qingdao 266061, People’s Republic of China
Abstract. Kernel fuzzy K-means (KFKM) clustering algorithm is widely used
to manage the ﬁngerprint database for WiFi indoor positioning system to reduce
the computational complexity of the position matching process. In this paper, we
propose a novel WiFi positioning scheme based on KFKM algorithm, which can
achieve a better precision by further optimizing the parameters employed in
KFKM. Our proposed scheme consists of three steps. First, we choose an interval
of reference points (RP) to build the ﬁngerprint database. Then we decide an
appropriate number of clusters based on the structure characteristics of ﬁngerprint
database using sample density method. During the process of clustering, we opti‐
mize the kernel parameter by approximating actual kernel matrix to a hypothetical
ideal kernel matrix to improve the positioning precision. Through simulation
results, we show that compared with the existing KFKM algorithm, our proposed
scheme achieves 23.48% improvement in terms of positioning accuracy.
Keywords: WIFI positioning · KFKM · Parameters optimization
Fingerprint database · Clustering
1
Introduction
The basic idea of the location ﬁngerprinting method for WiFi positioning consists of
two steps. The ﬁrst step is to build a database that stores pre-recorded received signal
strength (RSS), or “ﬁngerprint”, from diﬀerent WiFi access points (AP) at diﬀerent
reference points (RP). Then, the second step is to estimate the target’s location by
matching real-time RSS with the oﬄine-constructed database (Fig. 1).
The main limitation of the matching process is the high implementation cost when
the database is too large to traverse. Hence, some improved clustering methods were
proposed to manage the database such as K-means and fuzzy K-means (FKM) algo‐
rithms. However, non-linear data is beyond K-means method’s processing power since
the objective function is non-diﬀerentiable. Motivated by this, in [1], the authors came
up with fuzzy K-means (FKM) method, which aim to solve the non-diﬀerentiability of
the objective function; recently in [2], Saadi et al. used an un-supervised machine
learning K-means clustering based on two LEDs and achieved great accuracy. In this
paper, we propose a new scheme to strengthen the performance of the WiFi positioning.
We ﬁrst amend the data acquisition process of the database; second, we use sample
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 311–317, 2018.
https://doi.org/10.1007/978-3-319-74521-3_34

density method to modify cluster amount to match with the characteristic of database;
last, we propose an ideal kernel matrix to determine the kernel parameter σ.
2
WiFi Indoor Positioning System Based on KFKM
WQNN is an eﬀective matching algorithm in WIFI positioning [3]. Suppose a WIFI
wireless network with L access points, denoted by {
AP1, AP2, ⋯APL
}. There are n RP
in total, denoted by 
{
fi1, fi2, ⋯fin
}T, i = 1, 2, ⋯, L. At on-line stage, the RSSI of the
target, denoted by s =
(
s1, s2, ⋯sL
)T, will match with the ﬁngerprint database by eval‐
uating the Euclidean distance between s and all ﬁngerprints. Choose Q (Q > 2) smallest
distance and average the Q corresponding positions, the coordinate (̂x, ̂y) is the estimated
location of the target’s positions, i.e.
(̂x, ̂y) =
Q
∑
i=1
1
Di + 𝜀
Q∑
i=1
1
Di + 𝜀
× (xi, yi)
(1)
Where (xi, yi) is the coordinate of the ith RP, set ε to 0.003 to prevent the “zero divisor”
condition. In order to evaluate the precision of positioning test, the root-mean-square-
error (RMSE) between the estimated position and the real position is proposed to
measure errors. What’s more, a RMSE drawn from one positioning test doesn’t represent
the performance of an algorithm. Hence, we collect RMSE from times of experiments
to evaluate the algorithm comprehensively. In general, cumulative probability curve of
RMSE is an intuitive representation for evaluation.
The Chemistry building in BNU was set to verify the optimizations of database
structure would improve localization accuracy. Considering storage capacity and calcu‐
lation complexity, set RP interval = 2 m and collecte RSS from 5 AP at all RP(black
points) in Fig. 2, we have the database of the on-site test.
Fig. 1. The schematic diagram of database
312
Z. Hu et al.

Fig. 2. Ichnography of on-site positioning test
3
Parameters Optimizations for Positioning System
In this section, we propose a series of parameter optimizations to the WiFi positioning
system, including the modiﬁcation of clusters amount k and kernel parameter σ of kernel
function, denoted by POKFKM algorithm.
Function_1: set the cluster amount k according to the structure of RP.
The amount of clusters k plays an essential role in the performance of KFKM clus‐
tering algorithm. The main idea of sample density method [4] is to analyze all RP in the
database to ﬁnd out the amount of density peaks that extrude among neighbors and far
from other peaks. ρi is the density of the ith RP, δi is the smallest distance between the
ith RP and another RP with higher density, indicated as follows:
𝜌i =
∑
j
𝜒(dij −dc), i ≠j, 𝛿i = minj:𝜌j>𝜌i(dij), i ≠j
(2)
Figure 3 shows the δ-ρ situation of the on-site test database. There are 7 peaks in the
upper right corner of the ﬁgure. Thus, k = 7 is the optimal cluster amount.
Fig. 3. Finding of density peaks to determine k value
Function_2: set the optimal kernel parameter σ to get better clustering result.
Gaussian kernel function is:
k(x, y) = exp(−‖x−y‖2
2𝜎2
)
(3)
Parameters Optimization for KFKM Clustering Algorithm
313

While ||·|| means the Euclidean distance of vectors. The experiential value of σ is
given by the reciprocal of sample hypersphere radius [5] or enumeration, none is optimal.
Therefore, the optimal σ is supposed to approximate the actual kernel matrix to the ideal
matrix [6]. K-means algorithm ﬁrstly divides the database into k clusters and gets a row
vector L with n dimensions. The element Li = k′, 1 ≤k′ ≤k, i = 1, 2, ⋯n means the
aﬃliation of corresponding sample.
314
Z. Hu et al.

With the purpose of making the samples within the same cluster as similar as possible
and samples from diﬀerent clusters as unlike as possible, the ideal kernel matrix should
meet the following condition:
k
′
ij =
{ 1, Li = Lj
0, Li ≠Lj
(4)
The kernel parameter that approximate the kernel matrix to the ideal one is optimal.
The similarity can be quantiﬁed by the diﬀerence between kij and k′
ij:
E(k, k′) =
n
∑
i=1
n
∑
j=1
(kij −k
′
ij)2
(5)
The solution to dE(k, k′)
d𝜎
= 0 (noted as σ0) minimizes E(k, k′).
4
On-site Indoor Experiment
In order to verify the performance of POKFKM algorithm, we conduct contrast tests
between (1). KFKM clustering; (2). POKFKM clustering. The Clustering result is shown
in Fig. 4. Cluster centers are denoted by crosses.
1)
KFKM Clustering circumstance
2)
POKFKM Clustering circumstance
Fig. 4. Clustering result of original and optimized Clustering methods
After clustering, instead of matching all reference points in the database, it is capable
to complete matching process by comparing real-time RSS with 7 cluster centers and 9
Parameters Optimization for KFKM Clustering Algorithm
315

RP (at most) in the nearest cluster for 16 times in total after POKFKM clustering.
Therefore, the computational load reduces by 74.6%.
After initialization, ﬁrst position the test position RSS to the nearest cluster center,
then traverse other RP within the cluster using WKNN, estimated position is the result
of these two steps. We also use cumulative probability curves of RMSE of 70 times of
on-site tests to illustrate the positioning accuracy of the three diﬀerent clustering results
above. The simulation result is illustrated in Fig. 5.
Fig. 5. KFKM, POKFKM algorithms’ positioning errors
It’s shown that POKFKM have better performance than KFKM clustering. The
average RMSE of POKFKM is −23.48% narrowed than KFKM, which proves that
POKFKM method has superior positioning precision than the KFKM algorithm. Mean‐
while, POKFKM has less standard deviation of RMSE, which stands for higher robust‐
ness. KFKM Clustering method has 56.9% probability to derive expected error, while
POKFCN method is more reliable with the probability of 83.3%. Hence, POKFKM
clustering method largely increased the precision and the reliability of WiFi positioning
system.
5
Conclusion
In this paper, three optimizations for WiFi indoor positioning system were proposed to
enhance WiFi positioning. Simulation result has shown that the optimization of kennel
function and cluster amount performed better than original KFKM algorithm. During
on-site positioning experiments in the chemistry building in BNU, reasonable deploy‐
ment of RP and AP made the database both compendious and integrate. POKFKM
316
Z. Hu et al.

algorithm was proved to be more reliable and robust and substantially increased the
eﬃciency of WIFI indoor positioning.
Acknowledgement. This work was supported by the National Natural Science foundation of
China (61421062) and Hisense Co., Ltd.
References
1. Suroso, D.J., Cherntanomwong, P., Sooraksa, P., et al.: Location ﬁngerprint technique using
fuzzy C-means clustering algorithm for indoor localization. In: TENCON 2011 IEEE Region
10th Conference, pp. 88–92. IEEE, Bali (2011)
2. Saadi, M., Ahmad, T., Zhao, Y., et al.: An LED based indoor localization system using k-
means clustering. In: 2016 15th IEEE International Conference on Machine Learning and
Applications (ICMLA), pp. 246–252. IEEE (2016)
3. Sadeghi, H., et al.: A weighted KNN epipolar geometry-based approach for vision-based
indoor localization using smartphone cameras. In: Sensor Array and Multichannel Signal
Processing Workshop (2014)
4. Rodriguez, A., Laio, A.: Clustering by fast search and ﬁnd of density peaks. Science 344(6191),
1492–1496 (2014)
5. Li, Z., Weida, Z.: Kernel clustering algorithm. Chin. J. Comput. 25(6), 587–590 (2002)
6. Yang, H.L., Huang, Z.G., Liu, J.W., et al.: WIFI ﬁngerprint localization based on Kernel Fuzzy
C-means clustering. J. Zhejiang Univ. 50, 1126–1133 (2016)
Parameters Optimization for KFKM Clustering Algorithm
317

Energy Harvesting Time Coefﬁcient Analyze
for Cognitive Radio Sensor Network
Using Game Theory
Mengyu Zhao(&), Yifei Wei, Qiao Li, Mei Song, and Ningning Liu
School of Electronic Engineering, Beijing University of Posts
and Telecommunications, Beijing 100876, People’s Republic of China
{mengyuzhao,weiyifei,liqiao1989,songm,
liunn}@bupt.edu.cn
Abstract. In this paper, the cognitive radio sensor node can harvest energy
from the radio frequency signal which is transmitted by the primary user. A time
switching protocol was used to divide cognitive users’ time into three phases:
spectrum sensing mode, energy harvesting mode and data transmission mode.
Therefore, the optimal energy harvesting mode time selection is a question to be
solved. We consider a non-cooperative game model in which cognitive users are
regarded as selﬁsh players aiming to maximize their own energy efﬁciency.
Then we prove the existence and uniqueness of Nash Equilibrium. A distributed
best response algorithm is used to obtain the Nash Equilibrium. The simulation
results prove that this algorithm can converge to the same equilibrium from
different initial values. At last, we analysis the inﬂuence of various system
parameters on the results of Nash Equilibrium and energy efﬁciency.
Keywords: Energy harvesting  Energy efﬁciency
Cognitive radio sensor network  Non-cooperative game
Best response algorithm
1
Introduction
In recent years, with the increase of network energy consumption and the development
of green communication, energy harvesting has drawn more and more attention.
Energy harvesting is a technology to keep self-sustaining and prolong network lifetime
by harvesting the ambient energy (such as wind energy, solar energy, heat, etc.).
Moreover, the sensor network with the capacity of harvesting energy from the ambient
radio-frequency (RF) signals has received extensive research. Energy harvesting
wireless sensor network (EHWSN) can greatly prolong the lifetime of the sensor nodes,
which lays foundation for the development of emerging technologies such as big data
and Internet of Things (IoTs) [1]. But it is worth to note that the sensor node can only
work on the unlicensed spectrum band. And the unlicensed spectrum band are being
more and more crowded, which limit the development of the sensor network.
Meanwhile, the survey shows the spectrum efﬁciency of licensed spectrum is rel-
atively low. So energy harvesting cognitive radio sensor network (EHCRSN) has been
put forward. In EHCRSN, the second user is the sensor node equipped with the ability
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 318–329, 2018.
https://doi.org/10.1007/978-3-319-74521-3_35

of spectrum sensing, which can also harvest the RF energy. But energy harvesting and
data transmission of the sensor user can’t be implemented at the same time. Therefore,
two practical receiver methods have been proposed in [2], namely the time switching
protocol (TS) and power splitting protocol (PS). In the former, the receiver harvest
energy in the ﬁrst period and then transmit the information in the remainder time. In PS
protocol, the receiver split the received signal power into two streams: energy har-
vesting and information transmission.
There have been several papers in the literature that focus on the TS and PS
protocol in the past years. In [3], the author research a decode-and-forward energy
harvesting relay cognitive network using TS protocol. However the scenario is too
specialized. [4] research the relay selection question using TS and PS protocol in
cooperative cognitive radio network. [5] investigates a distributed power splitting
architecture with the purpose of achieving the optimal sum-rate through selecting the
power splitting ratios. RF energy harvesting in DF relay network is studied in [6]. In
this paper, TS protocol, PS protocol and a hybrid TS-PS protocol are applied to
compare and analyze outage probabilities and throughput.
In the aspect of cognitive radio sensor network, the author study the power allo-
cation in cognitive sensor networks in [7]. However, no energy harvesting and TS
protocol are considered in it. [8] develop a framework and propose a low-complexity
algorithm in EHCRSN. [9] research the optimal mode selection question to maximize
the throughput of the sensor node in EHCRSN. But to the best of our knowledge, there
is seldom investigation of applying TS protocol in EHCRSN and using the best
response algorithm to solve it. The main contributions of this paper are summarized as
follows:
1. We propose a new cognitive radio sensor network architecture which discuss the
question of how to select the energy harvesting time to balance the energy con-
sumption and energy replenishment in EHCRSN.
2. We reply the time switching relaying (TSR) protocol into the cognitive users, which
divides the time T into three parts: spectrum sensing phase, energy harvesting phase
and data transmission phase. And we analysis the energy efﬁcient maximization
question through selecting the optimal energy harvesting coefﬁcient.
3. We formulate this problem as a non-cooperative game. Then we use the best
response algorithm to reach Nash Equilibrium and prove the good performance of
our algorithm. At last, we get the result that different parameters variations have the
different inﬂuence on the energy harvesting time and system utility.
The remainder of the paper is structured as follows. Section 2 describes the system
model of primary user and cognitive user. In Sect. 3 we formulate the maximization
question as a non-cooperative game and prove the existence and uniqueness of Nash
Equilibrium. Then best response algorithm is introduced in Sect. 4. Finally Sect. 5
presents the numerical results and Sect. 6 concludes this paper.
Energy Harvesting Time Coefﬁcient Analyze
319

2
System Model
We consider a cognitive radio sensor network with one primary user (PU) and N
cognitive users (CUs) as illustrated in Fig. 1. The cognitive users, also called secondary
users (SUs), are sensor node equipped with the ability of spectrum sensing. Assume
that each cognitive radio sensor user can harvest energy and is equipped with the
energy storage device. Denote N = {1,2,…,N} the set of CUs.
A. Primary user model
The primary user is work in the licensed band and shared it with cognitive users.
The PU has two modes: data transmission mode and idle mode, as shown in Fig. 2(a).
Because the PU’s state will always switch from transmission to idle, we consider the
sum of a transmission state duration and it’s adjacent idle state duration as an analytical
interval T. Assume that PU won’t always keep one state for a long time, which is the
same as actual situation.
PU is regarded as a RF energy provider when it is in the data transmission. And the
CUs can harvest the RF energy from the PU in this phase.
Fig. 1. The system model of the primary user and cognitive users
Fig. 2. The stages of primary user and cognitive users in time T
320
M. Zhao et al.

B. Cognitive user model
In Fig. 2(b), cognitive user has three states during the time T: spectrum sensing
phase, energy harvesting phase, data transmission phase. Among them, a is the coefﬁ-
cient of spectrum sensing time and divided T into sensing phase and non-sensing phase. b
is the fraction of energy harvesting time coefﬁcient. In the ﬁrst duration 1  a
ð
ÞT, CU
senses the PU’s state and gather the sensing result together cooperatively. In this paper,
we assume the coefﬁcient a is ﬁxed and power consumption Pa is a constant value.
In the ﬁrst duration 1  a
ð
ÞT, CU senses the PU spectrum usage information and
gather the sensing result together cooperatively. In this paper, we assume the coefﬁcient
a is ﬁxed and power consumption Pa is a constant value.
Then CU can harvest energy from the PU when the PU is in the transmission
model. We consider that the CU doesn’t have its own power supply and totally depends
on the RF energy harvesting. And all of the CUs work in half-duplex mode, thus they
can’t transmit data and harvest energy at the same time. In this duration abT, the signal
CUi received can be expressed as
ycui ¼
ﬃﬃﬃﬃﬃ
Ps
p
gix1 + na
i
ð1Þ
Where gi is the channel gain from PU to the CUi. Ps and x1 are the ﬁxed transmit
power and the transmission information of the PU. na
i is the additive Gaussian noise
with zero mean and r2
i; a variance. For simplicity, we ignore the inﬂuence of the
Gaussian noise. The energy CUi harvested during the duration abT can be expressed as
Qi ¼ g  abiT  Ps  gi
j j2
ð2Þ
where g is the energy harvesting efﬁciency. Assume the energy Qi will be used all to
the next data transmission and spectrum sensing. The transmission power of CUi is
assumed to be so small that the harvested energy is enough to use.
In the rest time ð1  biÞaT, CUi will transmit data using the harvested energy to the
other CUs or the PU. Since we know the energy consumption during the sensing period
is Pa 1  a
ð
ÞT. We can calculate the transmission power of CUi as:
Pi ¼ Qi  Pses 1  a
ð
ÞT
að1  bÞT
¼
1
1  bi
ðg  bi  Ps  gi
j j2 1  a
a
PsesÞ
ð3Þ
C. Maximization of energy efﬁciency
We assumed that the cognitive users are deployed in the around of the PU. and the
interference produced by other cognitive users is far greater than that from the PU. Thus
the SINR of one particular cognitive user is determined by the inﬂuence of transmission
power of other CUs [10]. We can write the signal-to-interference-plus-noise ratio
(SINR) of the ith CU as
Energy Harvesting Time Coefﬁcient Analyze
321

SINRi ¼
hii  pi
P
n
j¼1;j6¼i
hij  pj þ a2
c
ð4Þ
Where hij denote the channel gain between the CUi and CUj, a2
c is the Gaussian
noise introduced by the environment. We konw the SINRi is in relation to the energy
harvesting coefﬁcient vector bi ¼ b1; b2; . . .bN
½
 from the Eqs. (3) and (4).
To insure the data transmission of the PU would not be affected, the interference
power from CUs to PU must be limited in a tolerable range. Assume that the largest
transmission power of CUs is Pmax. Conversely, if the CUs wants to transmit infor-
mation normally, its transmission power must be larger than a ﬁxed value Pmin.
Therefore, we get the range of CUi’s transmission power is
Pmin  Pi  Pmax
ð5Þ
Where Pi is deﬁned in Eq. (3), and we can get the inverse solution of bi.
aPmin þ Psesð1  aÞ
aðPmin þ gPs gi
j j2Þ
 bi  aPmax þ Psesð1  aÞ
aðPmax þ gPs gi
j j2Þ
ð6Þ
In order to simplify the description, we deﬁne the minimum and maximum value of
b as bmin and bmax respectively.
On the basis of Shannon formula, we know the achievable transmission rate of CUi is
determined by the SINRi given by Eq. (4). Therefore the rate of CUi can be obtained by
ri ¼ B logð1 þ SINRiÞ
ð7Þ
Where B is the transmission spectrum bandwidth shared by PU and CUi. We deﬁne
energy efﬁciency as the achievable data rate per unit power consumption [11], which
can be formulated as
uiðbÞ¼
ri
Ptotal
¼ B logð1 þ SINRiÞ
Pses + Pi
ð8Þ
In this paper, we express the time selection problem with the goal to maximize the
energy efﬁciency, which is a globally optimal network-wide performance problem. In
actuality, this is a tradeoff of time selection between energy harvesting mode and data
transmission mode. We need to solve the following network utility maximization problem
max
b
X
N
i¼1
uiðbÞ
s:t: aPmin þ Psesð1  aÞ
aðPmin þ gPs gi
j j2Þ
 bi  aPmax þ Psesð1  aÞ
aðPmax þ gPs gi
j j2Þ
0  bi  1
ð9Þ
322
M. Zhao et al.

3
Non Cooperative Game and Nash Equilibrium
D. Non Cooperative Game
Game theory is a complex activity where players are contend with each other
observing a series of rules [12]. In this paper, due to the interference is related to all the
CUs’ power, the maximization problem of each cognitive user is linking together.
Make a assumption that all the CUs are selﬁsh and rational, which means they are only
interested in maximizing their own energy efﬁciency and have the common knowledge.
Thus, question (9) can be converted as follows:
max
bi
uiðbi; biÞ
s:t: bi 2 Si
ð10Þ
Where bi ¼ b1; . . .; bi1; bi þ 1; . . .bN

T stands for the energy harvesting coefﬁ-
cients of all CUs, except the ith one. What’s more, Si is the feasible set of the ith link’s
power splitting ratio. Next, we can model it as a non-cooperative game with N players,
which can be expressed as follows:
g ¼ N; Sig
f
i2N; uiðqi; qiÞ
f
gi2N


ð11Þ
• Player set N ¼ 1
f ; 2. . .; i; . . .Ng: the set of N cognitive radio sensor users.
• actions
Sig
f
: Each player i selects its energy harvesting coefﬁcient bi 2 Si to
maximize its energy efﬁcient. S ¼ QN
i¼1 Si refer to the strategy space of N players
and strategy vector s ¼ s1; s2; . . .; sN
f
g is a subset of S.
• Utility function
uiðqi; qiÞ
f
g: the utility of player i.
E. Existence of the Nash Equilibrium
Deﬁnition 1. A Nash Equilibrium (NE) exists in a non-cooperative game if and only if
an strategy vectors* satisfy
uiðs
i ; s
iÞ  uiðsi; s
iÞ 8i 2 N; 8si 2 Si
That is to say, a NE is a strategy vector with the property that any player won’t have
better utility by chancing its strategy. The following theorem tells us how to distinguish
whether a NE exists or not for a particular game.
Theorem 1. In the game g ¼ N; Sig
f
; uiðqi; qiÞ
f
g
h
i, a NE exists if it have the limited
players and the strategy sets Si are convex set, closed and bounded. Last the utility
functions uiðqi; qiÞ is continuous and quasi-concave for 8i 2 N.
Proposition 1. The formulated energy harvesting time selection game g exists at least
one NE.
Energy Harvesting Time Coefﬁcient Analyze
323

Proof. Firstly, we have mentioned that there are N players in the game totally, which
meets the ﬁrst condition in Theorem 1. Secondly, because the strategy sets of CUi are
segments in two dimensions. It is straightforward to know that Si is convex, closed and
bounded. Besides, the utility function uiðbÞ is continuous in b. Thus, we only need to
focus on the quasi concavity of uiðbÞ.
If f(x) is quasi-convex, –f(x) is quasi-concave. So as long as we proved uiðbÞ is
quasi-convex, we can get the result that this game possesses at least one NE.
Theorem 2. A continuous function f: R ! R is quasiconvex if and only if at least one
of the following conditions is satisﬁed.
• f is nondecreasing.
• f is nonincreasing.
• there exists a point c 2 dom f which satisfy for t  c (and t 2 dom f ), f is nonin-
creasing, and for t  c (and t 2 dom f ), f is nondecreasing.
Now we perform the ﬁrst-order derivative of the negative utility function,
fiðbÞ ¼ uiðbÞ ¼  B logð1 þ SINRiÞ
Pses + Pi
ð12Þ
Through algebraic simpliﬁcation, the derivative function can be expressed as
@fiðbÞ
@bi
¼  @uiðbÞ
@bi
¼  @pi
@bi
 @ui
@pi
¼

1
ð1  biÞ2 ðgPs gi
j j2
a
1  a PsesÞ  B  logðeÞ
hiiðpi þ PsesÞ
a2c þ P
n
j¼1
hijpj
 lnð1 þ
hiipi
a2c þ P
n
j¼1;j6¼i
hijpj
Þ
ðpi þ PsesÞ2
ð13Þ
In reality, we know the primary user’s transmission power is far bigger than the
sensing power of CUi. So the ﬁrst half of Eq. (13) gPs gi
j j2
a
1a Pses [ 0 is always
established and the derivative @pi
@bi [ 0 is true forever. The transmission power is
increasing with the increase of bi. When the value of bi is ba¼ 1a
a 
Pses
gPs gi
j j2, we have
piðbaÞ ¼ 0 and @fiðbÞ
@bi

b¼ba
\ 0 After analysis, we know there is only one constant bc
which satisfy @fiðbÞ
@bi

b¼bc
¼ 0. And fiðbÞ is ﬁrstly decreasing and then increasing with the
increasing of bi. Therefore, we can conclude that uiðbÞ is quasi-concave and there exist
at least one NE in the game.
F. Uniqueness for the NE
Deﬁnition 2. A game g is said to be supermodular if the following condition is
satisﬁed
324
M. Zhao et al.

@2ui
@si@sj
 0; 8i 6¼ j 2 N
ð14Þ
We have the conclusion that a NE is unique if the game is a super-modular game.
Therefore, if we only need to analyze the non-negativity of differential function of the
utility function. The twice differential function can be expressed as
@2uiðbÞ
@bi@bj
¼ B logðeÞ 
1
ð1  biÞ2ð1  bjÞ2 ðgPs gi
j j2
a
1  a PsesÞ2

hiihij p2
i  hii  psesða2
c þ
P
n
j¼1;j6¼i
hij  pjÞ
"
#
ðpi þ psesÞ2ða2
c þ
P
n
j¼1;j6¼i
hij  pjÞða2
c þ P
n
j¼1
hij  pjÞ
ð15Þ
According to the analysis, it is concluded that whether the game is supermodular is
depended on the part of p2
i  hii  psesða2
c þ
P
n
j¼1;j6¼i
hij  pjÞ. After simpliﬁcation, we
get the necessary and sufﬁcient condition when the above part is greater than 0.
SINRi  pses
pi
ð16Þ
Because the sensing power is far smaller than the transmission power of cognitive
users. As a result, the ratio of sensing power to the transmission power is a pretty small
value and the above inequality is reasonable naturally. Finally, we can conclude that
this game is a super-modular game and only exist one NE.
4
Best Response Algorithm
In this section, we will introduce a distributed best response algorithm which can
achieve the unique NE from any initial state. The aim of this algorithm is to maximize
the energy utility through all the users changing their energy harvesting coefﬁcient bi
one by one until a suitable termination condition is satisﬁed.
First, when t = 0, set each player an initial value bið0Þ from its domain and
compute their utility value uið0Þ. Then, each player will update its energy harvesting
time coefﬁcient biðtÞ according to Eq. (17) in a ﬁxed sequence while other players
biðtÞ remain unchanged.
b
i ¼ arg max
bi2Si uiðbi; biÞ
ð17Þ
Finally, the iterative process will stop until the following termination condition is
satisﬁed.
Energy Harvesting Time Coefﬁcient Analyze
325

X
N
i¼1
½biðt þ 1Þ  biðtÞ2  g
ð18Þ
Where g is a extremely small constant. The algorithm can be programmed as
follows.
5
Numerical Results
In this section, we will present the numerical results to illustrate and demonstrate the
superiority of the algorithm. We select eight cognitive users to analyze and the channel
gain from PU to the CUs are from interval [0.3,0.6]. The channel gain hii and hij
between CUs vary from interval [0.8,1.2] and [0.05,1.15] independently. The recent
research shows that the achieved power of RF harvester can achieve 5 mW under
1000 MHz. And we set the shared spectrum is 1000 MHz. The minimum and maxi-
mum transmission power of CUi are Pmin ¼ 0:1 mW and Pmax ¼ 5 mW. For the fol-
lowing examples if not mentioned otherwise, we set Ps = 20 mW, Pses = 0.2 lW,
a = 0.2, g = 0.5 and a2
c = 1 mW.
Figures 3 and 4 show that the best response algorithm can converge to a unique NE
since the initial value of CU1 choose different values. It is observed that the conver-
gence process of CU1’ energy harvesting coefﬁcient b1 and the utility when the initial
condition is set in different values. It can be seen from Fig. 4 that the utility of CU1 is
rising constantly until reaching the NE.
Figure 5 depicts the utility comparison between the initial condition and the ﬁnial
condition achieving the NE. It shows the result that each player’s ﬁnal utility is much
larger than its corresponding utility in initial condition, which illustrates the superiority
of our algorithm. Next, we will analysis the impacts of one parameter (e.g.: g, Pses and
N) change on the solution of NE and the utility value.
326
M. Zhao et al.

Figure 6 show the ﬁnial energy harvesting time coefﬁcient bi and energy efﬁciency
ui versus the energy harvesting efﬁciency g. It can be observed from Fig. 6(a) that bi
decreases as g increases, because a larger g means less harvesting time to achieve the
Fig. 4. Convergence process of u1
Fig. 3. Convergence process of b1
Fig. 5. Utility comparison between the initial condition and the ﬁnial condition
Fig. 6. The impact of energy harvesting efﬁciency on (a) energy harvesting time coefﬁcient and
(b) energy efﬁciency with N = 8, Pses = 0.2 lW and a = 0.2
Energy Harvesting Time Coefﬁcient Analyze
327

needed transmission power. And we can see from Fig. 6(b) that g has no effect on the
ﬁnial energy efﬁciency.
Figure 7 show the inﬂuence of CUi’s sensing power on energy harvesting time
coefﬁcient and energy efﬁciency. We can see that the energy harvesting time coefﬁcient
bi keep unchanged ﬁrst and then increasing with the growth of CUi’s sensing power.
Meanwhile, the energy efﬁciency keep unchanged ﬁrst and then decreasing. This is
because when the sensing power is less than a certain threshold, the change of sensing
power is so small in relation to the transmission power that has no effect on the NE. But
with the growing of sensing power and when it is bigger enough, the CUi need more
harvesting energy to support it and the energy efﬁciency decrease based on the Eq. (8).
At last, we will analysis to the ﬁnial total energy efﬁciency uall¼ P
N
i¼1
ui versus the
number of cognitive users N. Figure 8 replies that the total energy efﬁciency of the two
carve is almost the same when there are two players and then have opposite tendency.
And we can see from the blue curve that with the increasing of N, the total utility after
game increases ﬁrst and then decreases. Because when the user number is lower than a
certain value, the interference from other users is relatively small and the total utility
increase with increasing the number. However, with a further increase of N, the
interference become stronger, which causes the direct decreasing of the total utility.
Fig. 7. The impact of CUi’s sensing power on (a) energy harvesting time coefﬁcient and
(b) energy efﬁciency with N = 8, g = 0.5 and a = 0.2
Fig. 8. The total energy efﬁciency versus different number of players with Pses = 0.2 lW and
g = 0.5 (Color ﬁgure online)
328
M. Zhao et al.

6
Conclusion
In this paper, we have researched how to select the optimal energy harvesting time to
achieve the maximize energy efﬁciency using non-cooperative game. First of all we
proved the existence and uniqueness of Nash equilibrium, and then we got the con-
verging process to achieve the NE with the best response algorithm. The results
demonstrate that the best response algorithm can converge to the same equilibrium
from different initial values. Besides, non-cooperative game yields a good utility per-
formance to address this energy harvesting time coefﬁcient selection question. At last
we studied the effect of various system parameters on the results of Nash equilibrium.
Acknowledgment. This work was supported by the National Natural Science Foundation of
China (No. 61571059), and the State Major Science and Technology Special Projects of China
under Grant 2016ZX03001017-004.
References
1. Ku, M.L., Li, W., Chen, Y., et al.: Advances in energy harvesting communications: past,
present, and future challenges. IEEE Commun. Surv. Tutorials 18(2), 1384–1412 (2016)
2. Nasir, A.A., Zhou, X., Durrani, S., et al.: Relaying protocols for wireless energy harvesting
and information processing. IEEE Trans. Wirel. Commun. 12(7), 3622–3636 (2012)
3. Mousavifar, S.A., Liu, Y., Leung, C., et al.: Wireless Energy Harvesting and Spectrum
Sharing in Cognitive Radio, pp. 1–5 (2014)
4. He, J., Guo, S., Wang, F., et al.: Relay selection and outage analysis in cooperative cognitive
radio networks with energy harvesting. In: ICC 2016 IEEE International Conference on
Communications, pp. 1–6. IEEE (2016)
5. Chen, H., Li, Y., Jiang, Y., et al.: Distributed power splitting for SWIPT in relay interference
channels using game theory. IEEE Trans. Wirel. Commun. 14(1), 410–420 (2015)
6. Elmorshedy, L., Leung, C., Mousavifar, S.A.: RF energy harvesting in DF relay networks in
the presence of an interfering signal. In: ICC 2016 IEEE International Conference on
Communications, pp. 1–6. IEEE (2016)
7. Chai, B., Deng, R., Cheng, P., et al.: Energy-efﬁcient power allocation in cognitive sensor
networks: a game theoretic approach. In: Global Communications Conference, pp. 416–421.
IEEE (2012)
8. Zhang, D., Chen, Z., Awad, M.K., et al.: Utility-optimal resource management and
allocation algorithm for energy harvesting cognitive radio sensor networks. IEEE J. Sel.
Areas Commun. 34(12), 3552–3565 (2016)
9. Park, S., Heo, J., Kim, B., et al.: Optimal mode selection for cognitive radio sensor networks
with RF energy harvesting. In: IEEE, International Symposium on Personal Indoor and
Mobile Radio Communications, pp. 2155–2159. IEEE (2012)
10. Wei, Y., Ren, C., Song, M., Yu, R.: The ofﬂoading model for green base stations in hybrid
energy networks with multiple objectives. Int. J. Commun Syst. 29(11), 1805–1816 (2016)
11. Wei, Y., Wang, X., Fialho, L., Bruschi, R., Ormond, O., Collier, M.: Hierarchical power
management architecture and optimal local control policy for energy efﬁcient networks.
J. Commun. Netw. 18(4), 540–550 (2016)
12. Lasaulce, S., Debbah, M., Altman, E.: Methodologies for analyzing equilibria in wireless
games. Signal Proc. Mag. IEEE 26(5), 41–52 (2009)
Energy Harvesting Time Coefﬁcient Analyze
329

Traﬃc Paralysis Alarm System Based on Strong
Associated Subnet
Chen Yu1(✉), Shaohui Zhu1, Hanhua Chen1, Ruiguo Zhang2,
Jiehan Zhou3, and Hai Jin1
1 Cluster and Grid Computing Lab, Big Data Technology and System Lab,
Services Computing Technology and System Lab, School of Computer Science and Technology,
Huazhong University of Science and Technology, Wuhan 430074, China
{yuchen,hjin}@hust.edu.cn
2 Siemens Ltd., Beijing, China
ruiguo.zhang@simens.com
3 University of Oulu, Oulu, Finland
jiehan.zhou@oulu.fi
Abstract. Urban traﬃc congestion is a major problem for urban transportation
management all over the world. However, traditional research focuses only on
detection and description of urban traﬃc situations, which are not enough for
improving urban traﬃc conditions. In this paper, we distinguish two types of
traﬃc congestion: traﬃc paralysis and traﬃc jams. The former is the state that
traﬃc is almost stagnant in a large area and on many roads, and it will take a long
time before recovering the normal traﬃc ﬂow. In comparison, a traﬃc jam has
less negative eﬀect on traﬃc ﬂow and recovers easily. According to this, we
propose a traﬃc paralysis alarm system based on strong associated subnet to alert
traﬃc paralysis incidents. The system orients to city road network, mines asso‐
ciation rules between road segments, constructs the strong associated subnets and
detects traﬃc anomalies with ﬂoating car GPS data. We analyze two parameters
of our proposed system with a true dataset generated by over 2000 taxicabs in
Zhuhai and explain our system with a simulation experiment on VISSIM.
Keywords: Traﬃc paralysis · Association rule · Strong associated subnet
Alarm
1
Introduction
With the rapid development of urbanization and the remarkable improvement of living
quality in China, both the population and the number of private cars has increased
signiﬁcantly in cities. It is reported that by the end of 2013 the urban population of China
accounted for 53.73% of the total population, and urbanization is expected to reach more
than 60% by 2020. In addition, the number of private cars rose from 18.48 million in
2005 to 123.39 million in 2014, according to the data from the National Bureau of
Statistics of China.
Urbanization means the movement of a large part of the rural population to cities.
Urbanization improves the living standards of many people, promotes economic
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 330–341, 2018.
https://doi.org/10.1007/978-3-319-74521-3_36

development, and is more conducive to improved development of industry, education,
science and technology, culture, etc. But urbanization also brings with it huge chal‐
lenges, such as air pollution, public health, and traﬃc congestion. Because of its impact
on pollution and public health, traﬃc congestion is a key point in successful urbaniza‐
tion.
With the development of the Intelligent Transportation System (ITS), there has been
a lot of research on traﬃc congestion [1–3], but most of these studies are just to discrim‐
inate or describe the traﬃc situations. These studies are based on a two value state: traﬃc
is either free or congested. But there is a big diﬀerence between a traﬃc jam and traﬃc
paralysis. We should consider them as two diﬀerent states when we analyze traﬃc
situations in the real world.
Figure 1 is our developed traﬃc state transition diagram. Area A shows the traditional
traﬃc state transition when a traﬃc anomaly occurs, such as a car rear-end collision, it
will cause a traﬃc jam. Each traﬃc anomaly itself is random, which is why the current
research on urban traﬃc congestion issues remains at the level of identiﬁcation and
description of traﬃc situations, and makes it diﬃcult to carry out further research.
Therefore, we re-analyze the traﬃc state, add a paralysis state, and then get the new
traﬃc state transition diagram shown in Fig. 1. In the new traﬃc state transition diagram,
a transformation from a traﬃc jam to a traﬃc paralysis does not always occur, it depends
on the following three factors:
• Spatial-temporal Location: the time point and the location of the traﬃc jam occurred.
• Road Network Topology: the road network topology of the jammed road segment
and its surrounding road segments.
• Traﬃc Flow: the size of traﬃc ﬂow on the jammed road segment.
recover
unimpeded
jam
1
paralysis
2
recover
traĸc incident
spaƟal-temporal locaƟon, road network 
topology, traĸc ﬂow
A
Fig. 1. Traﬃc state transition diagram
These three factors determine whether a traﬃc jam can escalate to traﬃc paralysis,
and they provide the possibility for us to predict traﬃc paralysis.
We propose a traﬃc paralysis alarm system based on a strong associated subnet
which consists of three modules, namely, data preprocessing, strong associated subnet
mining, and traﬃc anomaly detection. The proposed system mainly depends on the
strong association between the road segments in a strongly associated subnet.
Traﬃc Paralysis Alarm System Based on Strong Associated Subnet
331

2
Related Work
The proposed system is mainly related to three issues: map-matching, association rule
mining, and traﬃc anomaly detection.
2.1
Map-Matching Problem
Irregularities in GPS positioning systems prevent the direct use of GPS data. Map-
matching algorithms correct the deviation of GPS data and matches GPS data to road
segments by which to render the actual trajectories of vehicles.
Typical geometric algorithms include point-point, point-line, and line-line algo‐
rithms. A point-point algorithm matches GPS data to the nearest point on surrounding
road segments. The accuracy of such algorithms is unpredictable.
In [4, 5], topological algorithms consider the connectivity of the road, adjacency
relationships, and other attributes (such as one-way roads) to match. Paper [5] gives a
C-measure based algorithm, which uses distance, angle, and connectivity to calculate
match reliability.
Advanced algorithms [6–9] generally use more complex computational models, have
greater accuracy, and can meet some special needs. The ST-Matching algorithm
proposed for GPS trajectories with a low-sampling-rate (e.g., one point every 2–5 min)
[6], takes into account the geometric and topological structures of road networks and
the temporal/speed constraints of the trajectories. Paper [7] proposes a Hidden Markov
Model-based algorithm which accounts for measurement noise and the structure of the
road network. In our paper, we use the algorithm proposed in [7] to correct GPS data.
2.2
Association Rule Mining
Association rule is one of the most active and widely used knowledge types in the ﬁeld
of data mining. Now there are many association rule mining algorithms.
Apriority is the first typical rule mining algorithm, and many other algorithms
based on the idea of apriority after that. Paper [10] points out that the early itera‐
tions of Apriority-based algorithms is the dominating factor for the overall mining
performance, and proposes an effective hash-based algorithm that can generate
smaller candidates. FP-Growth in [11] does not generate candidates and only
requires two scans of transaction database. It compresses the transaction informa‐
tion to the FP-tree in which the support degree of item-sets is in descending order.
There are many algorithms for diﬀerent scenarios on association rule mining in the
years of research and applications. For example, paper [12] presents an algorithm to
mine rules in databases which are partitioned among many computers that are distributed
over a wide area. Paper [13] summarizes the existing algorithms that address the issue
of mining association rules from data streams.
332
C. Yu et al.

2.3
Traﬃc Anomaly Detection
A traﬃc anomaly is anything blocking the normal traﬃc ﬂow, such as broke-down
vehicle, rear end collision, or other accident. There is research on detecting traﬃc
anomalies [1, 3]. A traﬃc-ﬂow-pattern based algorithm which focuses on traﬃc volume
and velocity on roads is proposed [1]. An anomaly detected by the algorithm is repre‐
sented by a subgraph of a road network where drivers’ routing behaviors signiﬁcantly
diﬀer from their typical patterns. An improved nonparametric regression algorithm
(INPRA) is presented [3]. Standard Deviation Algorithm is used to calculate and check
the standard deviation between prediction traﬃc data generated by INPRA and current
traﬃc data: if the standard deviation is larger than a predeﬁned threshold, the algorithm
will send out a signal for a possible incident.
3
Architecture and Design
3.1
System Overview
In this section, we will present the architecture and design of the proposed system. The
proposed system consists of three modules shown in Fig. 2: A. pre-processing module;
B. strong associated subnet mining module; C. traﬃc anomaly detection module.
·
Observed GPS Data 
Preprocess
Map-Matching
A
Digital Map Database
Road Split
GPS Point Trajectory Database
B
Road Segments Trajectory Database
GPS Point Trajectory -> 
Road Segments Trajectory 
TransformaƟon
SpaƟal-temporal 
PosiƟon 
GeneraƟon
Strong 
Associated 
Subnet 
Mining
AssociaƟon Rule
Timestamp Fill
Traĸc Anomaly 
detecƟon
Alarm
C
Fig. 2. Architecture of traﬃc paralysis alarm system
The module A mainly pre-processes digital map data, large text, and GPS data. First,
a digital map database is constructed by importing digital map data into a spatial database
and splitting each road into road segments. Second, the collected data must be
Traﬃc Paralysis Alarm System Based on Strong Associated Subnet
333

preprocessed because every text ﬁle of stored one-day taxi GPS data of Zhuhai city is
bigger than 11 GB, and they are unordered and redundant. Therefore, we need to remove
redundancy and sort by taxi ID and timestamp. Finally, deploying the map-matching
server and importing GPS point data and corrected GPS point data into spatial database
as GPS point trajectory database.
The main purpose of module B is to mine strong associated subnet. First, road
segment trajectory database is constructed by the ways of transferring GPS point trajec‐
tories as {p1, p2, …, pn} to road segments trajectories as {r1, r2, …, rm}, pi represents a
GPS point and ri represents a road segments. Then, the most frequent passed is selected
as seeds through top-k road segments and these seeds are expanded into strong associated
subnets.
Module C is responsible for detecting traﬃc anomalies occurred at strong associated
subnets. The travel speed of road segments in strong associated subnets is monitored
and checked: if there occurs an extended low-speed situation, the proposed system will
regard this phenomenon as a traﬃc anomaly and send out a signal.
3.2
Pre-processing Module
This module includes two parts: constructing a digital map database and map-matching
for setting up a GPS point trajectory database.
The process of constructing a digital map database can be summarized as the
following steps:
Step 1: Install PostGIS plug-in for the PostgreSQL object-relational database.
Step 2: Download digital map data from OpenSteetMap using QGIS tools.
Step 3: Import digital map data into spatial database using ogr2gor tools.
Step 4: Install PgRouting plug-in for the PostgreSQL and split road into road segments
with it.
There are three steps in setting up GPS point trajectory database module:
Step 1: Remove redundancy and sort by taxi ID and timestamp for these large text ﬁles.
Step 2: Modify source code of Open Source Routing Machine (OSRM) map-matching
server, then compile and deploy it.
Step 3: Map-matching for every taxi GPS point, then import observed and corrected
GPS data together into spatial database.
3.3
Strong Associated Subnet Mining Module
This module, combined with digital map database, expands the most frequent top-k road
segments into strong associated subnets. As summarized by the following steps.
Step 1: Load a corrected GPS point trajectory as pointTr = {p1, p2, …, pn}, for every
point pi of this trajectory, determine the road segment ri that point pi located at,
then we can gain the corresponding road segments trajectory of pointTr as
roadsegTr = {r1, r2, …, rm}. At the same time, update the number of trajectory
of ri.
334
C. Yu et al.

Step 2: Save road segment trajectories to road segments trajectory database.
Step 3: According to the number of trajectories of every road segment, select the most
frequent top-k road segments as seeds.
Step 4: Expand seeds into strong associated subnets. Finally, these strong associated
subnets are used as association rules to guide the alarm system.
3.4
Traﬃc Anomaly Detection Module
We monitor only these road segments that strong associated subnets contain, and regard
these road segments as interesting road segments. It can be summarized as three steps.
Step 1: Select these corrected GPS point trajectories which pass through interesting
road segments from GPS point trajectory database.
Step 2: Fill timestamp of every point of the selected trajectory according to the corre‐
sponding observed GPS point trajectory.
Step 3: For every interesting road segment, we have the trajectories passed through it.
This tells us how long every taxicab passes through it and the travel distance
of every taxicab. With this knowledge, we can calculate and monitor travel
speed of every interesting road segment, and check the successive low speed
situation.
4
Strong Associated Subnet Mining and Traﬃc Anomaly Detection
In this section, we will introduce two key algorithms of traﬃc paralysis alarm system
based on strong associated subnet: strong associated subnet mining algorithm and traﬃc
anomaly detection algorithm.
4.1
Strong Associated Subnet Mining Algorithm
We ﬁrstly deﬁne six operators used by strong associated subnet mining algorithm.
Preliminaries
Deﬁnition 1 (cntRoadseg): cntRoadseg(rx) is the number of the road segment trajectories
that contain road segment rx.
Deﬁnition 2 (cntShared): cntShared(rx, ry) represents the number of road segment
trajectories that contain both road segment rx and ry. It is the quantized form of the
association between road segment rx and ry. Under the same conditions, the bigger the
score of this operator, the stronger the association between road segment rx and ry.
Deﬁnition 3 (cntU): cntU(rx, ry) represents the sum of these road segment trajectories
that contain any one of road segment rx and ry.
Deﬁnition 4 (support): this operator is deﬁned as the following:
support(rx, ry
) = cntShared(rx, ry
)∕cntU(rx, ry
)
(1)
Traﬃc Paralysis Alarm System Based on Strong Associated Subnet
335

In the process of mining strong associated subnet, we check the possibility of road
segment ry which is not in a strong associated subnet becoming a part of strong associated
subnet. If road segment rx is adjacent to ry and is a part of strong associated subnet,
support(rx, ry) is the ﬁrst we check, it must satisfy the following:
support(rx, ry
) ≥𝜆
(2)
where λ is a predeﬁned threshold and it will be analyzed in Sect. 5.1.
Deﬁnition 5 (correlation): correlation(rx, ry) represents the correlation between road
segment rx and ry. This operator is deﬁned as the following:
correlation(rx, ry
) = cntShared(rx, ry
)∕2
× (1∕cntRoadseg(rx
) + 1∕cntRoadseg(ry
))
(3)
Deﬁnition 6 (cluster): The cluster degree of subnet V is the average of the correlation
among these adjacent road segments in subnet V. This operator is deﬁned as the
following:
Algorithm 1: cluster(r, V)
Output: the cluster deg ree of subnet V after r added 
into
1:
clus= 0.0;
2:
for rx in V loop
3:
listRx=loadAdjacent(rx, V);
4:
for ry in listRx loop
5:
clus+=correlation(rx,ry);
6:
end loop;
7:
end loop;
8:
clus /= 2;
9:
listR=loadAdjacent(r, V);
10:
for re in listR loop
11:
clus +=correlation(r, re);
12:
end loop;
13:
return clus /(V.size + 1);
In Algorithm 1, loadAdjacent(r, V) is used to load the adjacent road segments of r
in subnet V. cluster(r, V) is cluster degree of the assumed subnet which is made up of
subnet V and road segment r. The second thing we check in the process of mining strong
associated subnet is ensuring that cluster degree of strong associated subnet satisﬁes the
following:
cluster(r, V) ≥𝜇
(4)
where μ is the predeﬁned threshold and it will be analyzed in Sect. 5.1.
336
C. Yu et al.

Details of Algorithm
Strong associated subnet mining algorithm mainly consists of two steps. First, selecting
the most frequent top-k road segments as strong associated subnet seeds. Second,
checking if there exists a road segment in strong associated subnets that adjacent to one
or more road segments which are not in strong associated subnets and meet the thresholds
for expansion: if there exists one, we call this one as scalable road segment, and add
those meeting thresholds into strong associated subnets. This process is iterative until
there does not exist one meeting the requirements. More precisely, the algorithm process
is the following:
Algorithm 2: SASM(minSup, minCls, maxNetsize)
1:
generate seeds and insert them into table Subnets
2:
do
3:
r, netid=loadScalable();
4:
do
5:
ri =loadAdjacent(r, netid);
6:
if the size of netid >= maxNetsize then
7:
break;
8:
end if;
9:
if ri is already in netid then
10:
continue;
11:
end then;
12:
if support(r,ri) < minSup then
13:
continue;
14:
end then;
15:
if cluster(ri , netid)< minCls then
16:
continue;
17:
end then;
18:
insert ri into Subnets;
19:
i++;
20:
while(i <number of adjacent road segments of r)
21:
while(there is still a scalable road segment )
4.2
Traﬃc Anomaly Detection Algorithm
In this section, we use the ﬂow chart to present our successive low-speed based traﬃc
anomaly detection algorithm. The proposed algorithm takes 30 s as a calculation cycle
and computes average travel speed for every road segment in strong associated subnets
in every cycle. This algorithm runs forever once started. It iteratively computes travel
time and checks if there occurs a traﬃc anomaly.
Traﬃc Paralysis Alarm System Based on Strong Associated Subnet
337

5
Experiment and Analysis
In this section, we ﬁrst analyze two parameters, λ and μ, of strong associated subnet
mining algorithm. Then, a simulation experiment based on VISSIM is carried out for
explaining the theory of traﬃc paralysis alarm system based on strong associated subnet
(Fig. 3).
SequenƟally load a road segment in strong 
associated subnet, recorded as roadseg
sequenƟally load a trajectory which pass 
through roadseg in Ɵme secƟon tseg=[ts-
start*30, ts-end*30], recorded as tr
START
ﬁnd out the ﬁrst and last gps point located 
at roadseg in tseg,  compute speed and add 
result to v
is there sƟll a 
trajectory that pass 
through roadseg in tseg
avgV meet threshold
Record current Ɵmestamp as ts in second 
form, iniƟal variables: idx=0, start=1, 
end=0, v=0
compute average speed of roadseg in tseg,
recoreded as avgV
i++,
start++,
end++, 
v=0
Send 
out a 
ab-
normal 
signal
i<4
Y
N
N
Y
Y
N
Next iteratoraƟon
Fig. 3. A successive low-speed based traﬃc anomaly detection algorithm
5.1
Strong Associated Subnet Mining Algorithm Parameter Analysis
In the system testing process, we ﬁnd that the scores of these operators, support and
correlation, are low in most of the time, and the low score of operator correlation directly
causes the score of operator cluster to be also low. Therefore, we perform an analysis
based on the GPS data of Zhuhai, and get the conclusion that the smaller the sampling
rate is, the smaller these operators’ scores are.
The original GPS data of Zhuhai is sampled every 10 s. We manually dilute the
original data and get another three datasets with sampling intervals of 20 s, 40 s, and
80 s. For each of the most frequent top-200 road segments, calculating separately average
score of the operator support based on four datasets. The statistical distribution of the
338
C. Yu et al.

average score of operator support is shown in Fig. 4. Similarly, the statistical distribution
of the average score of operator correlation is shown in Fig. 5.
Fig. 4. Distribution of score of operator
support
Fig. 5. Distribution of score of operator
correlation
It is easy to observe from the above two ﬁgures that the sampling rate is higher, the
distribution peaks are at further to the right positions.
5.2
Simulation Experiment Analysis
The idea of our proposed system is that when a traﬃc anomaly is detected on a strong
associated subnet, alarming immediately for this subnet to reduce the traﬃc entering
this subnet, to avoid the transformation from traﬃc jam to traﬃc paralysis. We want to
discover whether reducing the traﬃc entering this subnet helps avoid the transformation.
We perform an experiment with VISSIM and the result can be concluded as: when a
traﬃc anomaly occurs on a strong associated subnet, reducing the traﬃc entering this
subnet can decrease the inﬂuence of the traﬃc anomaly.
We simulate a traﬃc incident occurring at the 300th s and resolved at the 1500th s
and collected a delay time dataset and a queue length dataset under diﬀerent traﬃc
volumes shown in Figs. 6 and 7.
Fig. 6. Delay time under diﬀerent traﬃc
volume
Fig. 7. Queue length under diﬀerent traﬃc
volume
Traﬃc Paralysis Alarm System Based on Strong Associated Subnet
339

It is obvious that the delay time and queue length increase remarkably as the traﬃc
volume rises. The delay time and queue length clearly show the traﬃc anomaly’s inﬂu‐
ence. Therefore, we can get the conclusion declared above: reducing the traﬃc entering
this subnet can decrease the inﬂuence of the traﬃc anomaly.
6
Conclusion and Future Work
In this paper, we propose a traﬃc paralysis alarm system based on strong associated
subnet. The system mainly designs and implements strong associated subnet mining
algorithm and traﬃc anomaly detection algorithm based on the taxicabs GPS data of
Zhuhai. The former is used to mining strong associated subnets with trajectories on road
network, and the latter calculates travel speed of interesting road segments periodically
and checks if there exists a successive low speed situation on interesting road segments:
if there exists one, the algorithm sends out a signal.
Two experiments are carried out in this paper. One examines the case of low score
of operators and concludes that the smaller the sampling rate is, the smaller these oper‐
ators’ scores are. The second experiment uses VISSIM to simulate traﬃc situations when
a traﬃc incident occurred. Under diﬀerent traﬃc volume, we analyze delay time and
queue length, concluding that reducing the traﬃc entering this subnet can decrease the
inﬂuence of the traﬃc anomaly.
In future work, the proposed system should be improved to solve real-time GPS data
stream, how we store, pre-process, and analyze the data stream may need further
research.
Acknowledgements. The work is partly supported by NSFC (No. 61472149), the Fundamental
Research Funds for the Central Universities (2015QN67) and the Wuhan Youth Science and
Technology Plan (2016070204010132).
References
1. Pan, B., Zheng, Y., Wilkie, D., Shahabi, C.: Crowd sensing of traﬃc anomalies based on
human mobility and social media. In: Proceedings of the 21st ACM International Conference
on Advances in Geographic Information Systems, pp. 334–343 (2013)
2. Chen, S., Wang, W., Zuylen, H.V.: Construct support vector machine ensemble to detect
traﬃc incident. Expert Syst. Appl. 36(8), 10976–10986 (2009)
3. Tang, S., Gao, H.: Traﬃc-incident detection-algorithm based on nonparametric regression.
IEEE Trans. Intell. Transp. Syst. 6(1), 38–42 (2005)
4. Velaga, N.R., Quddus, M.A., Bristow, A.L.: Developing an enhanced weight-based
topological map-matching algorithm for intelligent transport systems. Transp. Res. Part C
Emerg. Technol. 17(6), 672–683 (2009)
5. Xu, Y., Wang, Z.: Improvement and implement of map matching algorithm based on C-
measure. In: Proceedings of the 2010 2nd IEEE International Conference on Information
Management and Engineering, pp. 284–287 (2010)
340
C. Yu et al.

6. Lou, Y., Zhang, C., Zheng, Y., Xie, X., Wang, W.: Map-matching for low-sampling-rate GPS
trajectories. In: Proceedings of the 17th ACM International Symposium on Advances in
Geographic Information Systems, pp. 352–361 (2009)
7. Newson, P., Krumm, J.: Hidden Markov map matching through noise and sparseness. In:
Proceedings of the 17th ACM International Symposium on Advances in Geographic
Information Systems, pp. 336–343 (2009)
8. Yuan, J., Zheng, Y., Zhang, C., Xie, X., Sun, G.Z.: An interactive-voting based map matching
algorithm. In: Proceedings of the Eleventh International Conference on Mobile Data
Management, pp. 43–52 (2010)
9. Li, Y., Huang, Q., Kerber, M., Zhang, L., Guibas, L.: Large-scale joint map matching of GPS
traces. In: Proceedings of the 21st ACM International Conference on Advances in Geographic
Information Systems, pp. 214–223 (2013)
10. Park, J.S., Chen, M.S., Yu, P.S.: An eﬀective hash-based algorithm for mining association
rules. In: Proceedings of the 1995 ACM International Conference on Management of Data,
pp. 175–186 (1995)
11. Han, J., Fu, Y.: Discovery of multiple-level association rules from large databases. In:
Proceedings of the 21th International Conference on Very Large Data Bases, pp. 420–431
(1995)
12. Wolﬀ, R., Schuster, A.: Association rule mining in peer-to-peer systems. In: Proceedings of
the 3rd IEEE International Conference on Data Mining, pp. 363–370 (2003)
13. Nan, J., Le, G.: Research issues in data stream association rule mining. ACM SIGMOD
Record 35(1), 14–19 (2006)
Traﬃc Paralysis Alarm System Based on Strong Associated Subnet
341

Fault Recovery Algorithm Based on SDN Network
Yi Zhang1,2(✉), Yifei Wei1, Ruqin Huang2, Bo Gu1, Yue Ma1, and Mei Song1
1 School of Electronic Engineering, Beijing University of Posts and Telecommunications,
Beijing 100876, People’s Republic of China
{zzyy,weiyifei,gubo,mayue,songm}@bupt.edu.cn
2 Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China
15652964297@163.com
Abstract. Nowadays, diversiﬁed demand of diﬀerent users is becoming a focus
for improving network performance. Traditional network can’t meet the demand,
so we turn to Software-Deﬁned Network architecture, which realizes virtualiza‐
tion and abstraction of underlying hardware resource via separation of control
and data planes. First, we propose a virtual network mapping algorithm to allocate
link resources, using Ant Colony algorithm to ﬁnd the optimal solution. Then we
develop a virtual network fault recovery mechanism to satisfy the need of end
users with diﬀerent fault tolerance. The mechanism is achieved by the failure
recovery algorithm named NumMap Algorithm, which provide varied network
reliability levels for users of varied priority. By the end of paper, we conduct
simulation experiments to evaluate the algorithms with performance metrics such
as failure repairing ratio, success running ratio, and working link resource utiliza‐
tion. The results demonstrate the superiority of the proposed algorithm compared
with ResRemap and ResBackup algorithms.
Keywords: Virtual network · Software-Deﬁned Network · Mapping algorithm
1
Introduction
Network virtualization is a new technology which can solve the problem of the internet
and support the development of new technology. Important advantages of this technique
is diﬀerent network can share the underlying physical infrastructure. The virtual network
is above the physical layer. Virtual nodes and virtual links constitute a virtual network.
Network topology, the use of technology, the provision of services for diﬀerent virtual
networks are diﬀerent.
The purpose of network virtualization is to facilitate the conﬁguration and manage‐
ment and to facilitate the construction of new network technology. Resource manage‐
ment has become the key to achieve the advantages of the network virtualization tech‐
nology. The resource management in the network virtualized environment should
rationally design the management structure and the resource scheduling algorithm to
realize the eﬃcient sharing of the underlying physical network resources. In ensuring
the virtual network user resource request conditions, greatly improve the network
resource utilization.
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 342–353, 2018.
https://doi.org/10.1007/978-3-319-74521-3_37

The traditional network of network construction cannot meet the diverse needs of
diﬀerent business. Then SDN network is proposed, control and forwarding separation,
to achieve the underlying hardware abstraction. SDN is a new network architecture
proposed by Stanford University and is the main application architecture of this paper.
With network virtualization technology, each experimental user can customize a virtual
network and construct a diﬀerent topology for each virtual network. The underlying
physical network forms a network resource pool through node virtualization and link
virtualization and allocates resources for the virtual network to improve network
resource utilization.
Based on the exchange equipment, through the deﬁnition of a common standard
interface to achieve data plane and control plane separation, to facilitate the researchers
in the real network environment for technical research. The SDN forwarding plane is
separated from the control plane. The experimenter can access the underlying physical
device with a deﬁned common protocol and programmatically control the forwarding
path of the data in the network. Network virtualization technology can divide the phys‐
ical network into multiple virtual networks. Each virtual network can run diﬀerent
routing protocols and provide diﬀerent services.
The actual situation of the physical network often causes the failure due to natural
causes or malicious attacks and other non-natural, affecting the normal operation of the
user’s business. And the previous algorithm is to enhance the physical network resource
utilization, there is no study of network failure. Aiming at the reliability problem of virtual
network, there is a remapping mechanism of virtual node and virtual link, which not only
can improve the receiving rate of virtual network but also benefit the load balance of
network. Migrating a failed virtual link to a backup path by pre-building the backup path
is one of the methods. Another way, building a unified backup resource pool which can
dynamically allocate backup resources for virtual links, and improve the utilization of
physical resources. These algorithms use different mechanisms to improve the reliability
of virtual networks. The cost of reserving backup resources in advance is too high in the
first method. In the second method, a new path is found when the fault occurs, resulting in
a low failure rate. In this paper, we propose a fault recovery algorithm based on the number
of users for the virtual network mapping, and compare the network performance with the
link remapping algorithm and the backup path construction algorithm.
2
Mapping Model Base on SDN
2.1
Network Description
Virtual network requests are mapped to the underlying physical network that is made
up of diﬀerent devices. The management creates a virtual network mapping request
based on the needs of the user and sends the request to the physical layer. Finally, the
virtual network provider can provide the customized virtual network service to the user
(Table 1).
Fault Recovery Algorithm Based on SDN Network
343

Table 1. Parameters of the mapping models
Ns
Node set
Es
Link set
Nv
Virtual node set
Ev
Virtual link set
Nfwd(
ns
)
Physical node forwarding sources ns ∈Ns
Nfwd(nv
)
Virtual node forwarding sources nv ∈Nv
Nctrl(g)
Controller resources ns ∈g
Nctrl(
nv
)
The required control resources
B(ls
)
Bandwidth resources for each physical link ls ∈Es
B(lv
)
The required bandwidth resources for each virtual link lv ∈Ev
2.2
Mapping Description
Node mapping
In the SDN network, not only the physical node can meet the forwarding resource require‐
ments of the virtual node, but also the domain of the physical node must meet the control
resource requirements of the virtual node. In addition, the most classic mapping is used.
Virtual nodes and physical nodes are one-to-one mapping. The remaining resources of the
physical node should more than the required forwarding resources, and the remaining
control resources in the area should more than the required control resources.
Nfwd
r
(nj
s
) ≥Rfwd(ni
v
)oij = 1
(1a)
Nctrl
r
(gk
) ≥Rctrl(ni
v
)oij = 1, nj
s ∈gk
(1b)
∑
nj
s∈N′
s oij = 1
∀ni
v ∈Nv
(1c)
∑
ni
v∈Nv oij ≤1
∀nj
s ∈N′
s
(1d)
Link mapping
Find all nodes that meet the requirements, select one from the node that satisﬁes the
requirement as an endpoint. Use the Dijkstra algorithm to ﬁnd the shortest path to meet
the bandwidth. Finally, link mapping is implemented.
Br
(lm
s
) ≥R(li
v
), ∀lm
s ∈PhS,DPhS,D = M(li
v
)
(2a)
Br
(lm
s
) = B(lm
s
) −
∑
lm
s ∈M(li
v) R(li
v
)
(2b)
344
Y. Zhang et al.

2.3
Objective Function
In order to improve the efficiency of the use of physical network resources, that is, using as
little physical resources as possible to carry as many virtual networks. This paper takes S
(
Gs
)
as the objective function that is the residual resource value of underlying network resources.
S
(
Gs
)
= α
[∑
nj
s∈ns Nfwd
r
(
nj
s
)
+
∑
gk⊆G Nctrl
r
(
gk
)]
+ β
∑
Br
(
lm
s
)
(3)
Nfwd
r
(
nj
s
)
 represent the remaining forwarding resources of the node, Nctrl
r
(
gk
) is the
remaining control resource for the area, Br
(
lm
s
) A represents the remaining bandwidth
resources of the link, α, β is resource conversion weight between node CPU resource
and the link bandwidth. S
(
Gs
) denote the physical resources available, which is in direct
proportion to the number of virtual networks supported. Therefore, set the optimization
target to max{S
(
Gs
)}.
3
Virtual Network Algorithm
In this paper, the ant colony algorithm is used to solve the virtual network mapping
problem in the software deﬁned network model.
The Algorithm of Virtual Network
1.
Initialization(); 
2.
VN_Creation();
3.
For i=1 to N;
4.
{ Update_probability(); 
5.
For ant=1 to M 
6.
{ Node_Map();
7.
Link_Map();
8.
if(S(local_solution)>S(global_solution))
9.
global_solution=local_solution;
10. }
11. Update_info();
12. }
Fault Recovery Algorithm Based on SDN Network
345

According to the above algorithm, in each iteration process, select the network
resource surplus value of the largest feasible solution as the current cycle optimal solu‐
tion according to S(Gs
). Then update the pheromone of the virtual node to the physical
node according to the optimal solution. After N iterations, the M feasible solutions
generated by the initialization can eventually converge to the approximate global
optimal solution.
The pheromone update to the physical node nj
s is as follows:
𝜏ij(t) = 𝜌𝜏ij(t −1) + Δ
(4)
𝜌 is the persistence of pheromone, Δ is the increment of pheromone.
Δ =
1
S(pbest)𝜎
(5)
pbest is the optimal solution of this cycle, S
(
pbest) is the objective function value of
the optimal solution, 𝜎 is the inﬂuence factor of the optimal solution on pheromone.
4
Fault Recovery Mechanism
4.1
Fault Recovery Algorithm Based on User Number(NumMap)
This paper presents a new fault recovery mechanism based on the previous network
mapping model. The number of users is diﬀerent during diﬀerent period. So we can
implement the strategy according to the number of users. When the number of users is
low, there are more free link resources. We use the backup path construction algorithm
for most users to ensure the reliability of the network as far as possible. The remaining
users take the faulty link remapping algorithm when a failure occurs. When the number
of users is large, the free link resources are less. So leave backup path for a small number
of users. Most failed users are remapped. Most failed users should take the faulty link
remapping algorithm. For each time slice, we will update the strategy based on the
number of users. Change the proportion of pre-backup users to normal users. In this
paper, we believe that the failure process is subject to uniform distribution.
The free link resources vary with the number of users. A fault recovery algorithm
based on the number of users is proposed, that is, the traﬃc of the pre-backup users
aﬀected by the fault is migrated to the backup path. And the common user’s faulty link
is remapped. The algorithm can improve the fault repair rate and network resource
utilization rate.
Mapping rules
Node mapping
Nfwd
r
(nj
s
) ≥Rfwd(ni
v
)oij = 1
(6a)
Nctrl
r
(gk
) ≥Rctrl(ni
v
)oij = 1, nj
s ∈gk
(6b)
346
Y. Zhang et al.

∑
nj
s∈N
′
s oij = 1
∀ni
v ∈Nv
(6c)
∑
ni
v∈Nv oij ≤1
∀nj
s ∈N
′
s
(6d)
Link mapping
For low level users, just meet
Br
(
lm
s
)
≥R
(
li
v
)
, ∀lm
s ∈PhS,DPhS,D = M
(
li
v
)
(7a)
Br
(lm
s
) = B(lm
s
) −
∑
lm
s ∈M(li
v) R(li
v
)
(7b)
For pre-backup users, link mapping is not only required to meet bandwidth require‐
ments, but also to ensure that there is no overlap with the backup link.
Br
(lm
s
) ≥R(li
v
), ∀ln
s ∈Ph′
SD
(8a)
Ph′
SD ∩PhSD = NULL
(8b)
Input: lSD, Wb
(
lSD
)
Output: PSD
(1) Determine the level of the virtual network carried on the faulty link;
(2) If it is a high level user, judge the availability of the backup path PSD;
If it is a low level user, skip to (4);
(3) Determine if the backup path is available. If it is not available, set PSD to NULL.
Skip to 7);
(4) Remove the faulty link lSD;
(5) Using the shortest path algorithm to ﬁnd an alternative path P_SD. Alternative path
and fault path have the same endpoint. If it is not found, set P_SD to NULL and
skip to 7);
(6) Br
(
lm
s
)
= Br
(
lm
s
)
−Wb
(
lSD
), Update the remaining bandwidth resources of the link;
(7) If PSD is NULL, the virtual network recovery failed. Otherwise, return the recovery
path;
The algorithm is run one by one for all virtual networks which aﬀected by the failure,
and then the repair is complete.
4.2
ResBackup Algorithm
The res-backup algorithm is to ﬁnd all the backup paths in advance and enable the backup
path when the network fails. If the backup path is available, the backup path is returned.
If the backup path is not available, the virtual network recovery fails. This algorithm
has a high recovery rate, but the backup path takes up too much resource, resulting in
reduced network utilization and degraded network performance.
Fault Recovery Algorithm Based on SDN Network
347

S and D represent the two endpoints of the link; Wb
(
lSD
) indicates the bandwidth
occupied by the backup resource on the link; Gl is represented as a collection of backup
links. The algorithm is described as follows:
Input: lSD, Wb
(
lSD
), Gl
Output: PSD
(1) Remove the faulty link in Gs;
(2) Determine if the backup path is available. If it is not available, set PSD to NULL
and skip to 4);
(3) Br
(lm
s
) = Br
(lm
s
) −Wb
(lSD
), Update the remaining bandwidth resources of the link;
(4) If PSD is NULL, the virtual network recovery failed. Otherwise, return the recovery
path;
4.3
ResRemap Algorithm
The ResRemap algorithm is a timely repair when the link fails. When the network fails,
ﬁrst remove the fault link from the set, calculate whether to ﬁnd the link which has the
same endpoint with the faulty link. If it can be found, update the resource, restore the
path, and if it was not found, the virtual network recovery fails.
Input: lSD, Wb
(
lSD
)
, Gl
Output: PSD
(1) Remove the faulty link in Gs;
(2) Using the shortest path algorithm to ﬁnd an alternative path PSD. Alternative path
and fault path have the same endpoint. If it is not found, set PSD to NULL and skip
to 4);
(3) Br
(
lm
s
)
= Br
(
lm
s
)
−Wb
(
lSD
), Update the remaining bandwidth resources of the link;
(4) If PSD is NULL, the virtual network recovery failed. Otherwise, return the recovery
path;
5
Simulation Parameters
5.1
Simulation Parameters
In this paper, we use MATLAB to generate the underlying physical network and virtual
network request topology. 25 nodes are generated in the space of 200 * 200, and some
nodes are generated according to the random function, which are connected to each other
by 0.5 probability (Table 2).
348
Y. Zhang et al.

Table 2. Parameters conﬁguration of models
Parameter
Conﬁguration
Node control resources
a uniform distribution of [50–100]
Link bandwidth resources
a uniform distribution of [50–100]
Virtual network request
The time unit is a time window, and the intensity of the
Poisson process is 4
Virtual network survival time
The mean is the exponential distribution of 10 time
windows
Number of virtual network nodes
a uniform distribution of [2–10]
Required control resources
Each request is subject to uniform distribution [0–20]
Required bandwidth resources
Each request is subject to uniform distribution [0–20]
Link failure
Each request is subject to uniform distribution [0–50]
Node resource weight
Set to 1
link resource weight
Set to 1
Number of ants
70
Number of iterations
70
The persistence of pheromones (ρ)
0.7
The initial value of pheromone (τ)
5
0
20
40
60
80
100
120
140
160
180
200
0
20
40
60
80
100
120
140
160
180
200
Fig. 1. Virtual network topology
5.2
Evaluation Standard
In order to evaluate the eﬃciency and performance of the virtual network more conven‐
iently, we deﬁne a series of parameters as the standard to evaluate the advantages and
disadvantages of the virtual network mapping algorithm (Fig. 1).
Fault Recovery Algorithm Based on SDN Network
349

(1) Acceptance ratio
Acceptance ratio refers to how many requests are successfully mapped. It is an
important criterion for evaluating the performance of the algorithm. Nacreq is the number
of accepted requests. Nalreq is the number of all requests. Rac is the acceptance ratio.
Rac =
Nacreq
Nalreq
(9)
(2) Fault recovery rate
Nsf is the number of failures that were successfully repaired. Nalf is the number of
all failure. Rfr is the fault recovery rate
Rfr =
Nsf
Nalf
(10)
(3) Link resource utilization
𝜂lr represent the link resource utilization. Nnorm is the normal link resources. Ntotal
indicates the total resources of links.
𝜂lr = Nnorm
Ntotal
(11)
6
Simulation Results
6.1
Acceptance Ratio
When γ = 0.1(γ =
𝜆f
𝜆r
), the request strength is not high, because the faulty link remap‐
ping algorithm does not have a backup link, so there are more free network resources,
the virtual network has the highest success rate. The backup link of the ResBackup
algorithm takes up many link resources, so the network running the lowest success rate.
The NumMap algorithm backs up some users’ links and takes up part of the additional
link resources, so the Rac is between the two algorithms.
The user’s requests get more when γ = 0.5. The success rate of ResRemap algorithm
is still the highest. But the Rac decreased from 67% to 55%. And The ResBackup algo‐
rithm back up link in advance, taking up a lot of link resources, so the the change of
running success rate is not obvious for virtual network. The Rac of the virtual network
operation of the fault recovery algorithm is slightly lower, but still between the two
algorithms (Fig. 2).
350
Y. Zhang et al.

0
5
10
15
20
25
30
35
40
0.4
0.5
0.6
0.7
0.8
0.9
1
Acceptance ratio(γ=0.1)
Time
ResRemap
Nummap
ResBackup
0
5
10
15
20
25
30
35
40
0.4
0.5
0.6
0.7
0.8
0.9
1
Acceptance ratio(γ=0.5)
Time
ResRemap
Nummap
ResBackup
Fig. 2. Acceptance rate for three algorithms
6.2
Fault Recovery Rate
When γ = 0.1, the high-level user virtual network failure repair rate close to 100%. Low-
level user fault repair rate is basically stable at 77%. The Resbackup algorithm has the
highest repair rate, close to 100%. The fault repair rate of ResRemap algorithm is basi‐
cally stable at 76%. When γ = 0.5, the number of users increased signiﬁcantly, but the
fault repair rate of the the high-level users is still about 95%, the fault repair rate of low-
level users stabilized at 60% or more. The fault repair rate of ResBackup algorithm is
higher. Because the backup path algorithm has set up a backup path for a virtual network
that is successfully mapped before a link failure occurs (Fig. 3).
0
5
10
15
20
25
30
35
40
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Fault recovery rate(γ=0.1)
Time
ResBackup
Nummap(high-level)
ResRemap
Nummap(low-level)
0
5
10
15
20
25
30
35
40
0.4
0.5
0.6
0.7
0.8
0.9
1
Fault recovery rate(γ=0.5)
Time
ResBackup
Nummap(high-level)
ResRemap
Nummap(low-level)
Fig. 3. Fault recovery rate for three algorithms
6.3
Link Resource Utilization
The number of users is large when γ = 0.1.Because it was when the link malfunctioned
that ResRemap started to repair, there is no backup link to seize resources, so 𝜂lr is the
highest, reaching 50%. ResBackup takes up a lot of link resources, which backup link,
Fault Recovery Algorithm Based on SDN Network
351

so 𝜂lr is the lowest. Fault recovery algorithm reached 45%, based on user number,
backuped part of the user’s link according to the number of users, which link resource
rate is between the two algorithms above. In the case of γ = 0.1, link resource utilization
declined (Fig. 4).
0
5
10
15
20
25
30
35
40
0
0.1
0.2
0.3
0.4
0.5
Link resource utilization(γ=0.5)
Time
ResRemap
Nummap
ResBackup
0
5
10
15
20
25
30
35
40
0
0.1
0.2
0.3
0.4
0.5
Link resource utilization(γ=0.1)
Time
ResRemap
Nummap
ResBackup
Fig. 4. Link resource utilization for three algorithms
7
Conclusion
The virtualized network represented by SDN network can meet the diversiﬁed business
needs of diﬀerent users, support multiple routing protocols, protect the security of user
information, and promote the evolution of traditional Internet architecture to the next
generation architecture. In this research, based on the premise of single link failure,
which is the most prone to network in the network, based on Network reliability require‐
ments diversity from users, we designed a fault recovery algorithm based on number.
Finally, we proved the superiority of the algorithm from the aspects of virtual network
fault repair rate, successful operation rate of virtual network and utilization of work link
resource.
Acknowledgements. The authors would like to thank the reviewers for their detailed reviews
and constructive comments, which have helped improve the quality of this paper. This work is
supported by the National Natural Science Foundation of China under (Grant No. 61471055).
References
1. Karakus, M., Durresi, A.: A survey: control plane scalability issues and approaches in
Software-Deﬁned Networking (SDN). Comput. Netw. 112, 279–293 (2017)
2. Bardgett, J.A., Miguez, M.A., Wierzbowski, P.: Network management system generating
virtual network map and related methods. US2015043378, 12 February 2015
3. Tootoonchian, A., Ganjali, Y.: HyperFlow: a distributed control plane for OpenFlow. In:
Proceedings of the USENIX INM Workshop on WREN (2010)
4. Luo, Y., Chen, X., Wang, J.: A virtual network embedding algorithm for providing stronger
connectivity in the residual networks. J. Netw. 8(4), 779–786 (2013)
352
Y. Zhang et al.

5. Yeganeh, S.H., Ganjali, Y.: Kandoo: a framework for eﬃcient and scalable oﬄoading of
control applications. In: Proceedings of the ACM SIGCOMM Workshop on HotSDN, pp.
19–24 (2012)
6. Chai, R., Dong, X.Y., Ma, J., et al.: An optimal IASA load balancing scheme in heterogeneous
wireless networks. In: 6th International ICST Conference on Communications and
Networking in China, pp. 714–719 (2011)
7. Huang, T., Liu, J., Chen, J., Liu, Y.: A topology-cognitive algorithm framework for virtual
network embedding problem. China Commun. 4, 73–84 (2014)
8. Mijumbi, R., Serrat, J., Rubio-Loyola, J., et al.: Dynamic resource management in SDN-based
virtualized networks. In: 10th International Conference on Network and Service Management
(CNSM), 17–21 November 2014, Rio de Janeiro, Brazil, 1st International Workshop on
Management of SDN and NFV Systems (ManSDN/NFV), 21 November 2014, Rio de Janeiro,
Brazil, pp. 412–417 (2014)
9. Bays, L.R., Oliveira, R.R., Buriol, L.S., Barcellos, M., Gaspary, L.P.: A toolset for eﬃcient
privacy-oriented virtual network embedding and its instantiation on SDN/OpenFlow-based
substrates. Comput. Commun. 82, 13–27 (2016)
10. Khan, A., An, X., Iwashina, S.: Virtual network embedding for telco-grade network protection
and service availability. Comput. Commun. 84, 25–38 (2016)
11. Li, Y., Zhao, Y., Zhang, J., Yu, X., Chen, H., Zhu, R., Zhou, Q., Yu, C., Cui, R.: First ﬁeld
trial of Virtual Network Operator oriented network on demand (NoD) service provisioning
over software deﬁned multi-vendor OTN networks. Optical Fiber Technol. 33, 22–29 (2017)
12. Xing, C., Lan, J., Hu, Y.: Virtual network with security guarantee embedding algorithms. J.
Comput. 8(11), 2782–2788 (2013)
13. Arakawa, S.I., Minami, Y., Koizumi, Y., Miyamura, T., Shiomoto, K., Murata, M.: A
managed self-organization of virtual network topology controls in WDM-based optical
networks. J. Optical Commun. 32(4), 233–242 (2011)
14. Sun, G., Yu, H., Li, L., Anand, V., Di, H.: The framework and algorithms for the survivable
mapping of virtual network onto a substrate network. IETE Tech. Rev. 28(5), 381–391 (2011)
15. Heisswolf, J., Zaib, A., Weichslgartner, A., König, R., Wild, T., Teich, J., Herkersdorf, A.,
Becker, J.: Virtual networks – distributed communication resource management. ACM Trans.
Reconﬁgurable Technol. Syst. (TRETS) 6(2), 8 (2013)
Fault Recovery Algorithm Based on SDN Network
353

Sensor Location Veriﬁcation Scheme Based on Range-Free
Localizations in WSNs
Chunyu Miao1(✉), Lina Chen1, and Qingzhang Chen2
1 College of Xingzhi, Zhejiang Normal University, Jinhua, China
netmcy@zjnu.cn
2 College of Computer, Zhejiang University of Technology, Hangzhou, China
Abstract. Localization is a pivotal technology in wireless sensor networks and
location information of sensor nodes is essential to location-based applications.
In the beacon-based localization, the reliability of beacons’ location information
is critical to the quality of network service. In this paper, we study the inﬂuences
of drifting beacons the network localization. So according to this scenario
mentioned above, we propose a distributed and lightweight beacons locations
veriﬁcation algorithm based on neighborhood-similarity (BLVNS), which
utilizes similarity of the beacons’ neighborhood in diﬀerent time slot to recognize
drifting beacons. The whole algorithms can be applied to the static and dynamic
WSNs to improve the accuracy of range-free localization. Experiment results
show that our algorithms can recognize unreliable beacons with detection rate
higher than 90%.
Keywords: WSNs · Reliable localization · Drifting beacons
Range-free localization
1
Introduction
Wireless sensor networks (WSNs), which consist of a large number of sensor nodes,
have been widely used in military and human daily life, e.g. surveillance, environmental
monitoring system and medical health [1]. Localization is one of the most essential
research issues in WSNs because the sensed information without location is insigniﬁ‐
cant, in some scenarios such as environment monitoring, target tracking, and geograph‐
ical routing [2]. To acquire the locations of sensor nodes, we can either mount nodes
with GPS receiver or predeﬁne nodes’ positions manually in deployment. Because of
relatively high price and energy-extensive consumption, GPS receivers may not be
available for power-limited and low-price WSN, and the second method is not available
for large scale WSN. As a result, we always predeﬁne a small part of nodes’ locations
manually in deployment, which are called beacon nodes. And other nodes are called
unknown nodes. Take the location of beacons as reference, the normal sensor nodes can
estimate their locations using some certain localization algorithms. These localization
algorithms can be divided into Range-Based localization algorithms and Range-Free
localization algorithms [3]. The former assumes the distances between sensors and
beacons can be estimated by using diﬀerent measurements, such as TDoA, ToA, AoA
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 354–363, 2018.
https://doi.org/10.1007/978-3-319-74521-3_38

and RSSI [4–7]. These algorithms can usually provide higher positioning accuracy with
higher hardware cost. While the latter estimates location of normal nodes based on the
features of networks, such as hop counts [8], topology of network [9], etc.
In the study of localization of static WSN, we usually assume that beacons’ locations
are reliable. Nevertheless, for many WSN systems deployed in unstable environments,
all nodes may be moved unexpectedly and Beacons have hardware failures or be
captured to provide false locations. The drifting beacons are called unreliable beacons.
The locations of drifting normal nodes can be re-localized by re-invoking localization
algorithms. But when unreliable beacons’ locations are used in re-localize process, it
will seriously degrade the accuracy of re-localization and aﬀect the quality of network
service. So, recognizing and ﬁltering out these unreliable beacons becomes one of the
most important research issues in node localization of WSNs.
To solve these problems mentioned above, we proposed a distributed and lightweight
beacons locations veriﬁcation algorithm based on neighborhood-similarity, which use
the similarity of the beacons’ neighborhood in diﬀerent time slot to verify which beacons
are drifting.
2
Related Works
At present, the existing study of reliable localization in WSNs is divided into robust
localization algorithms [10] and unreliable beacons ﬁlter algorithms [11]. The former
is applicable in some scenes, where there exist the interference of ranging information
or small movement of beacons. The main idea of these algorithms is reducing the envi‐
ronment interference of localization. The latter algorithms verify beacons locations to
recognize and ﬁlter out the unreliable beacons. As a result, unreliable beacons ﬁlter
algorithms are more universal and can be used in localization algorithms to ensure
localization accuracy.
In [12], it proposed a point to point location veriﬁcation algorithm, which can be
applied to range-based algorithm. But the nodes are equipped with GPS receiver. DJ.
He et al. proposed a location veriﬁcation algorithm based on TOA to eliminate abnormal
range value, which can be applied to Range-Based localization algorithms [13]. Kuo
et al. proposed beacon movement detection (BMD) algorithm to detect the unexpected
movements of beacons [14]. The basic idea of BMD algorithm is to let the beacons
record the variances of RSSI measurement results between each other and report to a
calculation center to determine the moved beacons. Like all other centralized algorithms,
BMD algorithms will bring a heavy communication burden and need a sink node or a
computer with a strong ability to calculate, which is not ﬁt for WSN. There are also
some related works, which use the location of the hidden checked nodes to verify the
locations of beacons, which are also a centralized algorithm and needs involvement of
additional nodes [15]. In [16], by using rigid theory to exclude the abnormal location of
beacons, it can provide reliable localization results. However, the rigid theory relies on
high accuracy of range and the algorithm’s computation is too heavy. Ravi Garg et al.
eliminated the beacons, which provide a larger descent gradient during the localization,
to improve the credibility of localization. The algorithm did not consider the reference
Sensor Location Veriﬁcation Scheme
355

of the Locations of normal nodes so that it can use in beacon-sparse networks [17].
Yawen Wei et al. presented a location veriﬁcation probabilistic model based on the
mutual observation between neighbor nodes, which achieved a better result [18]. In [19],
it developed a distributed neighbor nodes score mechanism based on variation of RSSI
to identify the moved beacons. However, all of above methods can’t be suitable for
Range-Free algorithms and solve single problem.
3
Problem Statement
3.1
System Model and Assumptions
We assume that there are two types of nodes deployed in the network: beacons and
sensors. The beacons know their locations in advance. The sensors do not know their
locations, which are also called unknown nodes. But they can estimate locations by using
localization algorithms with beacons’ locations. All nodes are mounted with RSSI
transceiver. All nodes can be moved unexpectedly. Since drifting unknown nodes are
always existed, we need to re-localized the network.
Besides, we assume all nodes’ communication ranges have the same radius. Notice
that environmental interruptions and permutations exist, so that neighborhood observa‐
tion is not always symmetric. Our algorithm also can solve such asymmetry problem.
In the initialized localization, we assume that beacons are reliable. And each node can
participate in the algorithm calculation. During the calculation, nodes are static. In the
most cases, the drifting nodes are 10%–20% of the total nodes. Unreliable beacons’
percentage is not over that 50% of beacons [20]. For conveniently introducing algorithm,
our notations are introduced in Table 1.
Table 1. Notations
Si
Node i
Bi
Beacon i
ID(i)t
At t time, the set of node i’s neighbors
Nei(i, j)t
At t time,the common set of node i’s and node j’s neighbors
DisRank(i, j)t
At t time, the actual distance coeﬃcient between node i and node j
EstDisRank(i, j)t
At t time, the estimated distance coeﬃcient between node i and node j
EstDis(i, j)t
At t time, the estimated distance between node i and node j
IDSame(i)(t, t+1)
In the (t, t + 1), the change of ID(i)t and ID(i)t+1
neiv(i)t
At t time, a vector to store DisRank(i, j)t, j ∈ID(i)t
Relation(i, j)t
At t time, the relationship of between node i and node j
3.2
Unreliable Beacons Models
At ﬁrst, in this paper two kinds of unreliable beacons are deﬁned as fallows:
Drifting Beacons: In some application scenarios, after deployment of network and
completion of localization, all nodes may be moved unexpectedly. Among these drifting
356
C. Miao et al.

nodes, beacons are called drifting beacons. As an example, Fig. 1 shows a scenario where
beacon B2 is moved, whose broadcast location doesn’t match its actual location. So
during re-localization, if re-invoking localization algorithms utilize B2’s location, it can
degrade the accuracy of re-localization.
B1
B2(t1)
B2(t2)
B4
B5
B6
B7
B7'(t2)
S3(t1)
Beacon
Sensor
Drifted Beacon
Malicious Beacon
S1
S4
S2
S3(t2)
Fig. 1. Unreliable beacons models
3.3
Problem Model
Drifting beacons lead to degrade accuracy of re-localization. Before re-localizing the
network, we should recognize and ﬁlter out the unreliable beacons. We assume that only
a small apart of beacons are unreliable beacons, the number of which is deﬁned as h.
The set of unreliable beacons is Ad =
{
ak:k = 1 ⋯h, h << m
}. The set of beacons is Ad
and the number of beacons is m. A′
d is denoted as the set of unreliable beacons veriﬁed
by proposed veriﬁcation algorithm g(∙), so g(Ad) = A′
d. In this paper, we design a location
veriﬁcation algorithm g(∙) to make A′
d close to Ad, i.e.:
Min||Ad −A′
d||
4
DV-Hop Localization Algorithm and Hop Count Correction
In this section, we describe the DV-Hop localization algorithm and how to correct hop
count by using RSSI technology.
4.1
DV-Hop Localization Algorithm
Niculescu et al. [21] proposed the DV-Hop localization algorithm by utilizing distance
vector routing mechanism. It has three phases [22]:
In the ﬁrst phase, the network employ a typical distance vector routing mechanism.
Beacons ﬂood their locations throughout the network with the initial hop-count of 0.
Each node that relays the message increase the hop-count by one. After the ﬂooding
procedure, every node can obtain the minimum hop-count to each other.
In the second phase, after obtaining the locations and hop-count information to all
other beacons, each beacon estimates the average distance per hop. Bi calculates the
Sensor Location Veriﬁcation Scheme
357

average distance per hop, called Hi, using the formula (1). Where (xi, yi) is the coordinate
of Bi. hij is the hop-count from Bi to Bj. Then, Hi will also be ﬂooded to sensors near to Bi.
Hi =
∑
i≠j
√
(xi −xj)2 + (yi −yj)2
∑
i≠j hij
(1)
In the last phase, before conducting the self-localization, each sensor estimates the
distance to each beacon based on its hop-count and the H to this beacon. After obtaining
all the distance information, each sensor conducts the triangulation or maximum like‐
lihood estimation to estimate its own location.
4.2
Hop Count Correction
In this section, we utilize the RSSI technology to correct the hop count between nodes,
which reﬂects the distance proximity between nodes. In the recent study of DV-Hop
localization algorithm, many papers utilize the RSSI technology to correct the hop count
between nodes [22, 23]. In this paper, we use this idea to correct the hop count to help
nodes get the distance proximity with their neighbors.
The coverage area of WSNs is large and the environments of diﬀerent regions are
diﬀerent, but in some scenarios, the environment is relatively stable, such as the range
of node and its neighbors. Based on this assumption, we can utilize the RSSI technology
to correct the hop count.
At first, each node transmits the information of RSSI to its neighbors. So each node
gets the RSSI values of its neighbors. Secondly, each node normalize the RSSI values,
using the formula (2). Rssi(i, j)t represents the RSSI value between Si and Sj, and
min (Rssi(i, m)t
), m ∈ID(i)t is the minimum of these RSSI values. For RSSI value is form
–95 dbm to –55 dbm, the rssi is a constant and set as –50 dbm. It is convenient for the
proposed algorithms to calculate similarity. At last, each node gets the distance coeffi‐
cients with its neighbors. Besides, if Si can’t communicate with Sj, DisRank(i, j)t = 0.
DisRank(i, j)t =
Rssi(i, j)t −rssi
min(Rssi(i, m)t
) −rssi
, m ∈ID(i)t
(2)
5
BLVNS Algorithm
In this section, we describe beacons locations veriﬁcation algorithm based on neigh‐
borhood-similarity. The BLVNS uses similarity of the beacons’ neighborhood in the
diﬀerent time slot to recognize drifting beacons. The neighborhood reﬂects in two
aspects: the set of neighbors and the distances between the node and its neighbors.
Besides, BLVNS can eﬀectively minimize the inﬂuences of drifting neighbors.
358
C. Miao et al.

5.1
Neighborhood Relationship
(1) Set of neighbors
All nodes may be moved unexpectedly. When one node is moved, its neighbors are
changed. As an example, Fig. 2 shows a scenario where B2 is moved in the (t0, t1). At
time t0, its neighbors are S5, S4 and S6. But at time t1, its neighbor is only S5. Its neighbors
are changed obviously. We denote sensor Si’s change of neighbors by IDSame(i)(t,t+1) in
the (t, t + 1), using the formula (3).
IDSame(i)(t,t+1) =
||ID(i)t ∩ID(j)t||
||ID(i)t ∪ID(j)t||
(3)
B1
B2(t0)
B2(t1)
S1
S2
S3(t0)
S3(t1)
S4(t1)
S4(t0)
S5
S6(t0)
S6(t1)
Fig. 2. The process of drifting nodes
(2) Distances between neighbors
If a node is moved in small distance, its neighbors are not changed completely. But
the distances between node and these neighbors are likely changed. As Fig. 2 shows that
after B2 is moved, S5 is still its neighbor. However, the distance between them is changed.
Nodes hardly get the distances with their neighbors based on Range-Free localization
algorithms. So In the Sect. 4.1, we calculate DisRank(i, j)t to reﬂect distance proximity
between neighbors.
5.2
Similarity of Neighborhood Relationship
In the (t, t + 1), At time t, Si gets ID(i)t and DisRank(i, j)t, j ∈ID(i)t. At time t + 1, Si
gets ID(i)t+1 and DisRank(i, j)t+1, j ∈ID(i)t+1. IDSame(i)(t,t+1) is computed by the formula
(3). Neighborhood relationship of Si and Sj’s at time t is deﬁned as Relation(i, j)t. Rela‐
tion(i, j)t and Relation(i, j)t+1 are computed by
Relation(i, j)t =
⎧
⎪
⎨
⎪⎩
DisRank(i, j)t ⋅IDSame(j)(t,t+1)
j ∈(ID(i)t ∩ID(i)t+1
)
DisRank(i, j)t ⋅(1 −IDSame(i)(t,t+1)
) j ∈(ID(i)t∕ID(i)t+1
)
0
j ∈(ID(i)t+1∕ID(i)t
)
(4)
Sensor Location Veriﬁcation Scheme
359

Relation(i, j)t+1 =
{ DisRank(i, j)t ⋅IDSame(j)(t,t+1) j ∈ID(i)t+1
0
j ∈(ID(i)t∕ID(i)t+1
)
(5)
In the formula (4) and (5), to minimize the inﬂuences of drifting neighbors,
DisRank(i, j)t is multiplied by IDSame(j)(t,t+1). When Si is static but Sj is moved, the
change of DisRank(i, j)t and DisRank(i, j)t+1 is caused by Sj. Because of Sj’s movement,
IDSame(j)(t,t+1) is closed to 0, which makes Relation(i, j)t and Relation(i, j)t close to 0.
Besides, for j ∈
(
ID(i)t∕ID(i)t+1
), at time t + 1, Sj is not Si’s neighbor, so Si can’t get
Sj’s IDSame(j)(t,t+1). IDSame(j)(t,t+1) is replaced by 1 −IDSame(i)(t,t+1). If Si is moved,
1 −IDSame(i)(t,t+1) is closed to 1. If not, 1 −IDSame(i)(t,t+1) is closed to 0. These opera‐
tions can minimize the inﬂuences of drifting neighbors.
After Si calculates Relation(i, j)t and Relation(i, j)t+1 of each neighbor, We deﬁne
vectors RelationV(i)t and RelationV(i)t+1 to store these data. Finally, we use cosine law
to calculate the similarity of RelationV(i)t and RelationV(i)t+1 by
NeiSim(i) =
RelationV(i)t ⋅RelationV(i)t+1
‖‖RelationV(i)t‖‖ ⋅‖‖RelationV(i)t+1‖‖
(6)
6
Experiment and Analysis
In this section, ﬁrstly, the simulation results are presented to validate the performance
and robustness of our proposed algorithms. Then, the algorithms are applied in the
dynamic WSN to improve the accuracy of re-localization.
BLVNS is based on neighborhood mutual observation, so the average connectivity
degree of network is more than 15. The network conﬁguration of our simulation is set
as follows: 150 nodes, including 15 beacons and 135 sensors, are deployed randomly in
a 150 m × 150 m region. The transmission range of each nodes equals to 30 m. We use
the signal attenuation model to simulate the RSSI value between nodes, by using the
formula (7). Where pkl is the path dissipation function mattered with nodes’ distance.
dkl is the distance between sender Sk and receiver Sl. d0 is a reference distance and equals
to 1 m. np is an exponent of path loss. ε is an error coeﬃcient.
pkl =
(
p0 −10nplg
(dkl
d0
))
⋅(1 −𝜀)
(7)
We use success detection rate Rs and error rate Re to evaluate the detection perform‐
ance. The calculation of Rs and Re is given in (8) and (9), in which Bu is the set of
unreliable beacons, Bdu is the set of unreliable beacons detected by algorithms.
Rs = Num(Bu ∩Bdu)
Num(Bu)
(8)
360
C. Miao et al.

Re =
Num
(
(B −Bu) ∩Bdu
)
Num(Bu)
(9)
The goal to this experiment is to study how the number of drifting nodes impact the
performance and analyze the environmental adaptability of the veriﬁcation algorithms.
At ﬁrst, the measurement of RSSI does not exist any error, 𝜀= 0. And the thresholds
of BLANS and BLATM are 0.6 and 0.45 respectively. Figure 3 shows the performance
of the veriﬁcation algorithms with the drifting beacons. For example, when exponent of
path loss is 1.5, 2, and 2.5, respectively, Rs of two algorithms is maintained at about 95%
and Re is under the 20% with the drifting nodes increasing. So the inﬂuences of drifting
sensors are small. When the measurements of RSSI exist errors, we set 𝜀= 0,
𝜀∈(−0.05, 0.05) and 𝜀∈(−0.1, 0.1) respectively. Figure 4 shows the performances of
the veriﬁcation algorithms with measuring errors. Compare Fig. 4 with Fig. 3, we can
ﬁnd their performances are similar so that the inﬂuences of RSSI measuring errors are
small. The experiment shows that BLVNS is robust.
5
10
15
20
25
30
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
BLVNS Rs/Re
Number of drifting nodes
np=1.5 Rs
np=2    Rs
np=2.5 Rs
np=1.5 Re
np=2    Re
np=2.5 Re
Fig. 3. Performance of BLVNS with diﬀerent drifting nodes and environments
5
10
15
20
=0 Rs
(-0.05,0.05) Rs
(-0.1,0.1) Rs
(-0.05,0.05) Re
(-0.1,0.1) Re
=0 Re
25
30
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
BLVNS Rs/Re
Number of drifting nodes
Fig. 4. Performance of BLVNS with diﬀerent drifting nodes and measuring errors
Sensor Location Veriﬁcation Scheme
361

7
Conclusion
In this paper, we analyze the severe impacts of the unreliable beacons on Range-Free
localization algorithms, which reﬂects the importance of beacon location veriﬁcation.
To eliminate the inﬂuences of localization arise from these unreliable beacons, we
propose the algorithms BLVNS which can eﬃciently recognize and ﬁlter out the drifting
beacons. BLVNS can minimize the re-localization error and have strong anti-jamming
capacity in diﬀerent environments and networks. Future study will extend the location
veriﬁcation model to real-world experiments.
Acknowledgment. This work is supported by the National Natural Science Foundation of China
under Grant (No: 61502413; 61379023), and Supported by Zhejiang provincial Top Discipline
of Cyber Security at Zhejiang Normal University.
References
1. Akyildiz, F., Su, W., Sankarasubramaniam, Y., Cayirci, E.: Wireless sensor Networks: a
survey. Comput. Netw. 38(4), 393–422 (2002)
2. Jang, W.S., Lee, D.E., Choi, J.H.: Ad-hoc performance of wireless sensor network for large
scale civil and construction engineering applications. Autom. Constr. 26(10), 32–45 (2012)
3. Safa, H.: A novel localization algorithm for large scale wireless sensor networks. Comput.
Commun. 45, 32–46 (2014)
4. Gezici, S., Tian, Z., Giannakis, G.B., et al.: Localization via ultra-wideband radios: a look at
positioning aspects of future sensor networks. IEEE Sign. Process. Mag. 22(4), 70–84 (2005)
5. Patwari, N., Ash, J.N., Kyperountas, S., et al.: Locating the nodes: cooperative localization
in wireless sensor networks. IEEE Sign. Process. Mag. 22(4), 54–69 (2005)
6. Vempaty, A., Ozdemir, O., Agrawal, K., et al.: Localization in wireless sensor networks:
Byzantines and Mitigation techniques. IEEE Trans. Sign. Process. 61(6), 1495–1508 (2013)
7. Shao, H.J., Zhang, X.P., Wang, Z.: Eﬃcient closed-form algorithms for AOA based self-
localization of sensor nodes using auxiliary variables. IEEE Trans. Sign. Process. 62(10),
2580–2594 (2014)
8. Wang, Y., Wang, X., Wang, D., et al.: Range-free localization using expected hop progress
in wireless sensor networks. IEEE Trans. Parallel Distrib. Syst. 20(10), 1540–1552 (2009)
9. Zhou, Y., Xia, S., Ding, S., et al.: An improved APIT node self-localization algorithm in
WSN based on triangle-center scan. J. Comput. Res. Dev. 46(4), 566–574 (2009)
10. Zhong, S., Jadliwala, M., Upadhyaya, S., et al.: Towards a theory of robust localization against
malicious beacon nodes. In: INFOCOM, pp. 2065–2073 (2008)
11. Hwang, J., He, T., Kim, Y.: Detecting phantom nodes in wireless sensor networks. In:
INFOCOM, pp. 2391–2395 (2007)
12. Liu, D., Lee, M.C., Wu, D.: A node-to-node location veriﬁcation method. IEEE Trans. Ind.
Electron. 57(5), 1526–1537 (2010)
13. He, D., Cui, L., Huang, H., et al.: Design and veriﬁcation of enhanced secure localization
scheme in wireless sensor networks. IEEE Trans. Parallel Distrib. Syst. 20(7), 1050–1058
(2009)
14. Kuo, S.P., Kuo, H.J., Tseng, Y.C.: The beacon movement detection problem in wireless
sensor networks for localization applications. IEEE Trans. Mob. Comput. 8(10), 1326–1338
(2009)
362
C. Miao et al.

15. Yang, Z., Jian, L., Wu, C., et al.: Beyond triangle inequality: sifting noisy and outlier distance
measurements for localization. ACM Trans. Sens. Netw. (TOSN) 9(2), 26 (2013)
16. Yang, Z., Wu, C., Chen, T., et al.: Detecting outlier measurements based on graph rigidity
for wireless sensor network localization. IEEE Trans. Veh. Technol. 62(1), 374–383 (2013)
17. Garg, R., Varna, A.L., Wu, M.: An eﬃcient gradient descent approach to secure localization
in resource constrained wireless sensor networks. IEEE Trans. Inf. Forensics Secur. 7(2),
717–730 (2012)
18. Wei, Y., Guan, Y.: Lightweight location veriﬁcation algorithms for wireless sensor networks.
IEEE Trans. Parallel Distrib. Syst. 24(5), 938–950 (2013)
19. Xia, M., Sun, P., Wang, X., et al.: Distributed beacon drifting detection for localization in
unstable environments. Math. Prob. Eng. (2013)
20. Alfaro, J.G., Barbeau, M., Kranakis, E.: Secure localization of nodes in wireless sensor
networks with limited number of truth tellers. In: 2009 Seventh Annual Communication
Networks and Services Research Conference, CNSR 2009, pp. 86–93. IEEE (2009)
21. Niculescu, D., Nath, B.: DV based positioning in Ad Hoc networks. Netw. IEEE 22(1–4),
267–280 (2003)
22. Wu, J., Chen, H., Lou, W., et al.: Label-based DV-Hop localization against wormhole attacks
in wireless sensor networks. Pervasive Mob. Comput. 16, 22–35 (2014)
23. Guo, Z., Min, L., Li, H., Wu, W.: Improved DV-hop localization algorithm based on RSSI
value and hop correction. In: Wang, R., Xiao, F. (eds.) CWSN 2012. CCIS, vol. 334, pp. 97–
102. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-36252-1_10
Sensor Location Veriﬁcation Scheme
363

Design and Development of Parametric System
for Planetary Reducer
Yangpeng Chen, Menglun Tao, Dingfang Chen
(✉), Bo Li,
Yanfang Yang, and Boting Chen
Institute of Intelligent Manufacturing and Control, Wuhan University of Technology,
Heping Road No. 1040, Wuchang District, Wuhan 430063, Hubei, China
453194287@qq.com, taomenglun@whut.edu.cn, cadcs@126.com
Abstract. For planetary reducers, most components have certain characteristics
and structure. In order to obtain the 3-D models and engineering drawings of
similar components which have the same characteristics and structure only
diﬀerent in size, a digital design system of planetary reducer is developed by
parametric design method. The system is planned according to the 3-D parametric
design and structural analysis of planetary reducers and the parametric operation
module is also designed. In parametric operation module, 3-D models of main
components of the planetary reducer are taken as templates; the parameters of
template are selected and modiﬁed by using VB platform to recall controls of
SolidWorks. The 3-D models and engineering drawings of similar components
are output rapidly. The eﬃciency and correctness of planetary reducers are
improved by this system, which is also provided as a reference for designers and
enterprises to produce other serial products.
Keywords: Planetary reducer · Parametric design · VB · SolidWorks
1
Introduction
Most components of the mechanical product have certain characteristics and structure.
Applying the parametric design method, it is helpful to improve the design eﬃciency
and simplify the design work by using the established 3-D model library, and generate
3-D models of these components with same characteristics and structure only diﬀerent
in size [1].
SolidWorks is a kind of powerful 3-D modeling software [2]. And it is able to meet
the speciﬁc needs of enterprises in the integration of the secondary development and the
parametric design. Variables are used to replace ﬁxed parameters of components through
the parametric design in the physical modeling. The parametric design of similar compo‐
nents is completed through the modiﬁcation of variables.
Enterprises and designers always pay attention to design, development and manu‐
facture of planetary reducers. Especially for the aspects of standardization, serialization
and generalization of planetary reducer components, it has become a general trend to
achieve parametric design of reducer components [3]. The rapid development of the
computer technology and CAD technology provides software with realizable conditions
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 364–375, 2018.
https://doi.org/10.1007/978-3-319-74521-3_39

for parametric design [4]. To design the planetary reducers, the mathematical model and
product structure are kept the same while the parameters in size should be modiﬁed [5].
Therefore, the use of digital technology to achieve parametric design of planetary
reducers helps to improve eﬃciency and correctness of the serial design.
2
Realization of Digital Design System
2.1
Secondary Development Based on ActiveX Automation in SolidWorks
The service program (Server) based on ActiveX Automation technology is able to be
controlled and accessed by the client program (Client). In the study of secondary devel‐
opment of SolidWorks with Visual Basic 6.0, the service program is SolidWorks and
the client program is VB [6]. The language of program is used by the client program
(VB) to communicate directly with the service program (SolidWorks) and manipulate
functionality of the software.
In the development of the planetary reducer system, the relationship between the
service and client programs is shown in Fig. 1.
Parametric Design System of Planetary Reducer
（Client）
ActiveX Automation
SolidWorks2013（Sever）
Fig. 1. The relationship between the service and client programs
2.2
Planning and Realization Process of Digital Design System
The parametric design of main components of planetary reducers is completed in the
digital system planned in this paper, including overall design, 3-D modeling of main
components, automatic output of engineering drawings, automatic assembly and ﬁnite
element analysis of the reducer [7]. For design goals above, the digital design system is
planned as shown in Fig. 2.
Design and Development of Parametric System
365

Parametric Design System 
of Planetary Reducer
Overall design
General parts and 
standard parts
Automatic assembly
Finite element 
analysis
Parametric 
operation
Help
Fig. 2. Design of digital design system
In Fig. 2, the parametric operations (3-D modeling and engineering drawing), auto‐
matic assembly, and ﬁnite element analysis in the component parametric design are key
modules of the digital design system.
The block diagram of the planetary reducer system is shown in Fig. 3.
Enter  parameters of 
overall design
Parametric design of 
components
Selection of common and
standard components
Model of finite element
Update the model
Update the model of
components
Update the model of
assembly
Update the geometric 
model of finite element
Check of motion 
intervention 
Generate engineering
drawings
Update the model of 
finite element
Structural analysis
Y
N
Fig. 3. Flow chart of planetary reducer digital design system
2.3
Design of Digital System
The process of secondary development for SolidWorks combined with VB is completed
in this paper [8]. The parametric design of the two-stage NGW planetary reducer is taken
as an example to complete the design of the digital design system.
The two parts completed are introduced as follows: main interface of the design
system and parametric operation.
Figure 4 is main interface of the design system. Figures 5 and 6 are parametric oper‐
ation module. The parametric operation module is one of core modules in the design
system, including parametric 3-D modeling of main reducer components and automatic
output of engineering drawings [9].
366
Y. Chen et al.

Fig. 4. Main interface of the design system
Fig. 5. Parametric design of the ﬁrst stage gear
Design and Development of Parametric System
367

Fig. 6. Parametric design of the input shaft
Users are able to select ﬁrst and second transmission system through modifying
parameters of the gear to generate 3-D models and engineering drawings of new gears,
and the parametric design of components is achieved as shown in Fig. 5.
After completing the selection of the gear type, the remaining components are able
to be designed accordingly as shown in Fig. 6.
After modifying parameters, the new 3-D model and corresponding engineering
drawing will be automatically saved to a new folder, so that the product will be viewed
again and corresponding changes can be made by the user.
3
Parametric 3-D Modeling of Main Components
3.1
Parametric Modeling Method
At present, the following two methods are commonly used in parametric design [10]:
(1) The modeling of parameter modiﬁcation method: The corresponding parameters in
the 3-D model template are modiﬁed through invoking program to obtain new 3-D
models.
(2) The modeling of program method: The 3-D model is built by the program fully.
The parameter modiﬁcation method is used by this paper. Figure 7 is the ﬂow chart
of the parameter modiﬁcation method.
368
Y. Chen et al.

Start
Create a 3-D model of components in SolidWorks and save it
Establish the relationship between VB and SolidWorks
Get the parameterized design variables of model 
Call the API object and assign a new value to the variable
Save
Generate a new model of the component through VB program
Fig. 7. Flow chart of the parameter modiﬁcation method
3.2
Realization of Parametric Modeling Process Based on Characteristics
The feature-based components with same characteristics are able to be generated by
using the same “template model”. These components need not to be re-modeled when
the feature size changes, which is the meaning of the parametric modeling [11]. When
the model with a diﬀerent size is required, the “template model” is able to be used for
generating the 3-D model quickly as shown in Fig. 8.
Parameter library
Template library
Mechanism of 
parametric drive 
Characteristic parameters
the template of components
 Interface of the user
Generate components
User
Preview the components
Fig. 8. Parametric method of “template model and parameter driven”
Following the mechanism of the parametric feature-based “template model and
parameter driven”, users are allowed to modify size parameters of the component
template and obtain components that meet requirements of the design rapidly [12]. At
the same time, it can meet the needs of users for the personalized customization of
components and the establishment of non-standard components.
Design and Development of Parametric System
369

In general, the common steps for the parametric establishment of a 3-D model are
shown in Fig. 9.
Start
Analyze drawings and features of components
Extract the drive parameters
Create the parametric template of components
Make parametric  settings
Add custom properties of components
modify
Develop parametric  program 
End
Y
N
Fig. 9. Parametric modeling process based on template model
3.3
Example of Parametric Modeling
In this paper, 3-D models of main components of the two-stage NGW planetary
reducer – including input shaft, sun gear, planetary gear, ring gear, planetary carrier,
pin and output shaft are modeled.
Taking the output shaft of the reducer as an example, parametric modeling is
performed according to Fig. 9.
The 3-D model template of the output shaft is shown in Fig. 10.
The parametric model interface of the output shaft is set up with VB to develop
SolidWorks API function as shown in Fig. 11.
370
Y. Chen et al.

Fig. 11. Parametric modeling of output shaft
Fig. 10. 3-D model template of the output shaft
Design and Development of Parametric System
371

After modifying the parameters of the output shaft, according to the modiﬁcation
process, new parameters are transferred to corresponding parameter names of the output
shaft. Then corresponding parameters in the template of the output shaft are modiﬁed
and the modiﬁcation of size parameters and custom properties is completed. Finally, a
new 3-D model and corresponding engineering drawing of the output shaft can be
generated, and the parametric operation of components is achieved [13].
4
Automatical Output of Engineering Drawing
4.1
Overall Framework of Automatic Adjustment System
After building a 3-D model, it is convenient to generate 2-D engineering drawings. But
the parametric ratio of the pattern, position of view, size and notes are not processed
automatically. These problems will result in defects of chaotic layouts and uncoordi‐
nated proportions, so it is not suitable for guiding the production directly [14]. Therefore,
it is necessary to carry out automatic adjustments for engineering drawings from Solid‐
Works, so that output engineering drawings can meet the requirements of enterprise
standards and facilitate rapid productions.
Create models of components 
and assemblies
 Engineering drawings of the 
enterprise
Generate engineering 
drawings of components and 
assemblies
Gets coordinates of the 
position of view,center 
coordinates,coordinates of 
annotation 
Generate new 3-D models
National 
standard
Reality of 
enterprises 
New engineering 
drawings of 
components and 
assemblies
New properties of 
components
Common scale of the 
view 
Coordinates of the 
position of view
Coordinates of 
annotation
Open an existing 
engineering drawing
Attributes added 
automatically 
Adjust  the Scale of 
view
Adjust  the position 
of view
AdJust the position 
of  annotations
Save the new 
engineering drawing
Interface of VB 
Parametrically drive
Update
Fig. 12. The process for the automatic adjustment of engineering drawings
372
Y. Chen et al.

When the new 3-D model is generated, the functions of the adjustment of engineering
drawings are able to be achieved with VB [14]. The process for automatic adjustments
of engineering drawings is shown in Fig. 12.
4.2
Example for Automatical Output of Engineering Drawing
The steps for the automatical output of engineering drawings are following: (1) Establish
a template for the drawing view; (2) Establish a template of engineering drawings; (3)
Set custom properties for the template of engineering drawings; (4) Adjust the view for
the template of engineering drawings; (5) Save and manage the template of engineering
drawings.
According to the steps above, the planetary carrier of the reducer is taken as an
example to complete the automatical output of the engineering drawing. Figure 13 shows
the custom properties of the planetary carrier [15].
Fig. 13. The custom properties of the planetary carrier
Figure 14 is the dialog box of custom properties in SolidWorks after the custom
properties of the planetary carrier is assigned.
Fig. 14. The dialog box of the custom properties
Figure 15 is the result of the planetary carrier generated automatically.
Design and Development of Parametric System
373

Fig. 15. The result of the engineering drawing automatically generated
5
Conclusions
Based on the understanding of the structure and design steps of the planetary
reducer, the general framework of the digital design system for the planetary reducer
is planned by the parametric design method. The development of the parametric
operation module in the system is completed by using SolidWorks as the develop‐
ment platform and Visual Basic as the development tool, and the automatic output
of 3-D models and engineering drawings of the same planetary reducer is realized.
The system is designed to provide enterprises and designers with a way to get serial
products rapidly in order to improve the efficiency and quality of design and make
the product more standardized and versatile.
References
1. Hua, Z., Dingfang, C.: Application and secondary development of SolidWorks based on
Visual Basic. J. Hubei Univ. Technol. 4, 36–38 (2010)
2. Xiuzi, Y., Chaoxiang, C.: SolidWorks Advanced Course. Secondary Development and API.
Mechanical Industry Press, Beijing (2007)
3. Xinhua, Z.: Modularization and Serialization of Planetary Gear Reducer. Dalian University
of Technology (2014)
4. Fengqin, Z., Chun, Z.: Parametric programming of NGW planetary gear reducer. Mech.
Transm. 05, 36–38, 88 (2005)
5. Yanli, C.: Research on Parametric Design of CAD System for NGW-S Planetary Gear
Reducer. Liaoning Technical University (2005)
6. Weiliang, L., Yixiang, W.: Parameterized design of hydraulic cylinder based on secondary
development of SolidWorks. Mach. Manufact. Autom. 01, 74–77 (2017)
374
Y. Chen et al.

7. Yi, W.: Parametric Design and Application of Wind Power Converter. Dalian University of
Technology (2014)
8. Zhaobin, H., Lin, X., Peng, H.: Parameterization design of hydraulic cylinder based on VB
and Solidworks. Coal Mine Mach. 12, 254–256 (2014)
9. Junliang, Z.: Study on Three-Dimensional Optimization Design of Planetary Gear Reducer
of Shearer Cutting Part. Harbin University of Science and Technology (2007)
10. Zhigeng, L.: Study on Key Technology of Parametric Design of Bridge Crane Bridge. North
University of China (2007)
11. Bo, P., Yue, Y., Chunzhu, S.: Parameterization modeling of centrifugal impeller using
SolidWorks API. J. Eng. Graph. 05, 1–7 (2009)
12. Ke, W., Zongyan, W., Chunyue, L., Bing, Z., Zhenyu, C.: Template-based parametric design
system of reducer. Mech. Transm. 05, 50–52 (2014)
13. Chao, S., Yuxiang, L.: Secondary development of SolidWorks for parametric design based
on VB language. Manufact. Autom. 15, 137–140 (2013)
14. Yuhua, P., Zongyan, W.: Study on key technology of automatic adjustment of parametric
design drawings. Hoisting Transp. Mach. 06, 58–61 (2012)
15. Yuhua, P., Zhigeng, L., Zhili, H., Jingang, Y., Zongyan, W.: Three-dimensional parametric
design system of main girder of bridge crane. Mech. Eng. Autom. 06, 45–47 (2006)
Design and Development of Parametric System
375

A Dynamic Double Threshold Based Cooperative
Spectrum Sensing Strategy in Heterogeneous Cognitive
Radio Networks
Chongxiao Peng1(✉), Yifei Wei1(✉), Bo Gu1, Ligang Ren2, and Mei Song1
1 School of Electronic Engineering, Beijing University of Posts and Telecommunications,
Beijing 100876, People’s Republic of China
{pengchongxiao,weiyifei,gubo,songm}@bupt.edu.cn
2 China Unicom System Integration Limited Corporation Beijing Branch, Beijing, China
15611030998@wo.com.cn
Abstract. Cooperative Spectrum Sensing (CSS) is the critical component of the
cognitive radio technology, which could relieve the shortage of spectrum
recourses. The double threshold based CSS scheme was adopted to deal with the
unreliability when the original energy value locates around the traditional single
threshold. We utilize the dynamic double threshold based energy detection
scheme, in which it can adjust the double threshold properly to respond the trans‐
formation of channel condition. This proposed dynamic double threshold based
scheme could be veriﬁed by analyzing the formula and simulation, and it achieves
the better performance compared with classical double threshold based CSS
scheme.
Keywords: Cooperative spectrum sensing · Dynamic double threshold
Energy detection · Cognitive radio
1
Introduction
The cognitive radio technologies adopt strategy which shares the frequency spectrum
among the users, and the secondary user (SU) opportunistically occupy the spectrum
only if spectrum hole has been detected, which means the primary user (PU) doesn’t
occupy the spectrum band at a certain period of time [1]. Before using shared spectrum
among users, spectrum sensing is necessary to determine if there is a PU currently
occupying the licensed spectrum band [2].
In this paper, we adopt dynamic double threshold method which is set dynamically
according to the instantaneous SNR, so that further improve the performance of double
threshold energy detection based CSS scheme.
The rest of the paper is organized as following: Sect. 2 describes the proposed system
model. Section 3 demonstrates the single threshold energy detection based CSS scheme.
Section 4 proposes a dynamic double threshold energy detection based CSS scheme.
The simulation results and analysis are presented in Sect. 5 and the conclusion from the
paper is drawn in Sect. 6.
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 376–381, 2018.
https://doi.org/10.1007/978-3-319-74521-3_40

2
System Model
There are three phases in the sensing system: sensing, reporting and broadcast. The SUs
receive the signal transmitted from PU and transfer the signal into the form of energy
value [3], then make the local decision or remain the original energy value. The local
sensing results would be reported to the fusion center at reporting phase. This process
would require some bits of data as communication overhead [4]. The ﬁnally global
decision would be made in the fusion center, then all the SUs would get the ﬁnal results
whether PU is occupying the licensed spectrum band.
3
Single Threshold Based Cooperative Spectrum Sensing
3.1
Probability of Detection and False Alarm
We choose Pd and Pf to express the probability of detection and probability of false
alarm respectively. We only consider the channel with Additive White Gaussian Noise
in this article, so we can get the probability of detection and false alarm as following [5],
Pdi = P{Ei > 𝜆||H1
} = QM
(√
2𝛾i,
√
𝜆
)
(1)
Pﬁ= P{Ei > 𝜆||H0
} = Γ(M, 𝜆∕2)
Γ(M)
(2)
where QM(..) is the generalized Marcum Q function [6] and Γ(..) is upper incomplete
gamma function. H0 denotes that the PU is absent of licensed spectrum band, while H1
denotes that the PU is present of licensed spectrum band.
3.2
Fusion Decision
K out of N rule is belong to the hard fusion decision scheme, which can also be called
voting rule or majority rule [7]. The fusion center deal with the local sensing results
reported by each SU. Assuming that the number of SU processing spectrum sensing is
N. The fusion center makes the ﬁnal decision that PU is present of the licensed spectrum
band only if there are at least K SU indicating the licensed spectrum band being occupied
by PU. The ﬁnally global probability of detection and false alarm can be formulated as
following,
PdKoutN =
N
∑
k=K
( N
k
)
k
∏
i=1
Pdi
N
∏
j=k+1
(1 −Pdj
)
(3)
PfKoutN =
N
∑
k=K
( N
k
)
k
∏
i=1
Pﬁ
N
∏
j=k+1
(
1 −Pfj
)
(4)
A Dynamic Double Threshold Based Cooperative Spectrum
377

The Maximal Ratio Combining (MRC) [8] method adopts strategy that getting the
weighted summation, in which the normalized weight factor is set according to instan‐
taneous SNR. The method can be formulated as following,
PdMRC = P{EMRC > 𝜆MRC||H1
} = QM
(√
2𝛾MRC,
√
𝜆MRC
)
(5)
PfMRC = P{EMRC > 𝜆MRC||H0
} =
Γ(M, 𝜆MRC
/2)
Γ(M)
(6)
where 𝛾MRC =
N∑
i=1
𝛾i, EMRC =
N∑
i=1
wiEi, wi =
𝛾i
N∑
i=1
𝛾i
=
𝛾i
𝛾MRC
.
4
Dynamic Double Threshold Based Cooperative Spectrum
Sensing
For a given threshold λ, the dynamic threshold is formulated as following.
𝜆0i = wi × 𝜆
(7)
𝜆1i = (2 −wi) × 𝜆
(8)
wi =
𝛾i
max (𝛾1, 𝛾2, ⋯⋯, 𝛾N
)
(9)
The fusion center makes the global decision GD1 based on the local binary decision,
and makes the global decision GD2 based on the original energy value. Finally, we can
get the ﬁnal decision which is expressed as following.
FD =
{
0, GD1 + GD2 < 1
1, otherwise
(10)
GD1 =
{
0, L1 < K
1, otherwise
(11)
GD2 =
⎧
⎪
⎨
⎪⎩
0, EMRC =
G∑
j=1
wjEj ≤𝜆MRC
1, otherwise
(12)
where L1 is the number of reporting binary decision which indicating the present state
of the PU.
378
C. Peng et al.

The ﬁnal decision is made in the fusion center. We could compute the ﬁnal proba‐
bility of detection and false alarm which is formulated as following.
Pd = 1 −(1 −PdB
)
(
1 −PdMRC ×
G
∏
i=1
P1i
)
(13)
Pf = 1 −(1 −PfB
)
(
1 −PfMRC ×
G
∏
i=1
P0i
)
(14)
where PdB and PfB is computed by using K out of N rule, PdMRC and PfMRC is computed
by using MRC, G is the number of original energy value reported from SU, the proba‐
bility of energy located between double thresholds must be taken account, which is
expressed as following.
P0i =
Γ(M, 𝜆0i∕2)
Γ(M)
−
Γ(M, 𝜆1i∕2)
Γ(M)
(15)
P1i = QM
(√
2𝛾i,
√
𝜆0i
)
−QM
(√
2𝛾i,
√
𝜆1i
)
(16)
5
Simulation Results and Analysis
The simulation is carried out in the assumption that channel is along with AWGN. The
time bandwidth product M = 5. We would compare the dynamic double threshold based
Fig. 1. Detection probability curve for single threshold based CSS using K out of N
A Dynamic Double Threshold Based Cooperative Spectrum
379

CSS scheme with the ﬁxed double threshold scheme in which it is set as
λ0 = 0.8λ, λ1 = 1.2λ.
Figure 1 shows the relationship curve of probability of detection and false alarm
using single threshold based CSS scheme, in which the ﬁnal probability is computed by
using the K out of N rule. Where the SNR is randomly generated between (−15 ~ 10)
dB. We can ﬁnd that the more SUs sensing spectrum is, the higher the probability is.
Meanwhile the smaller the K is, the higher the probability is as expected. Figure 2 shows
the changing tendency of detection probability along with false alarm probability which
Fig. 2. Detection probability curve for single threshold based CSS using MRC
Fig. 3. Detection probability curve for dynamic double threshold based CSS
380
C. Peng et al.

utilizes the MRC method to complete the ﬁnal decision. Apparently to achieve higher
detection probability it requires the number of original energy value as many as possible.
Figure 3 shows the proposed dynamic double threshold based CSS scheme under
the condition (N = 10, K = 10), meanwhile compares with the scheme which ignores
the energy value between the double thresholds and the scheme which deals with the
original energy reported from the SUs further. The proposed scheme presents a better
performance due to computing with the energy value between the double thresholds and
adjusting the threshold according to the instantaneous SNR appropriately.
6
Conclusion
The proposed dynamic double threshold based CSS scheme could realize better perform‐
ance by using MRC to deal with the original energy value between the double thresholds
and utilizing K out of N rule to deal with the local binary decision, the critical factor is
dynamic threshold adjusted according to instantaneous SNR.
Acknowledgement. This work was supported by the National Natural Science Foundation of
China (No. 61571059).
References
1. Ghasemi, A., Sousa, E.S.: Opportunistic spectrum access in fading channels through
collaborative sensing. J. Commun. 2(2), 71–82 (2007)
2. Yifei, W., Yinglei, T., Li, W., Mei, S., Xiaojun, W.: QoS provisioning energy saving dynamic
access policy for overlay cognitive radio networks with hidden markov channels. China
commun. 10(12), 92–101 (2013)
3. Urkowitz, H.: Energy detection of unknown deterministic signals. Proc. IEEE 55(4), 523–531
(2005)
4. Vien, Q.T., et al.: A hybrid double-threshold based cooperative spectrum sensing over fading
channels. IEEE Trans. Wirel. Commun. 15(3), 1821–1834 (2015)
5. Digham, F.F., Alouini, M.S., Simon, M.K.: On the energy detection of unknown signals over
fading channels. IEEE Trans. Commun. 55(1), 21–24 (2007)
6. Nuttall, A.H.: Some integrals involving the Q-function. IEEE Trans. Inf. Theor. 21(1), 95–96
(1972)
7. Mustapha, I., et al.: A weighted hard combination scheme for cooperative spectrum sensing in
cognitive radio sensor networks. In: IEEE Malaysia International Conference on
Communications, pp. 12–17. IEEE (2015)
8. Teguig, D., Scheers, B., Nir, V.L.: Data fusion schemes for cooperative spectrum sensing in
cognitive radio networks. In: Communications and Information Systems Conference IEEE,
pp. 1–7 (2012)
A Dynamic Double Threshold Based Cooperative Spectrum
381

A Distributed Self-adaption Cube Building Model
Based on Query Log
Meina Song
(✉), Mingkun Li
(✉), Zhuohuan Li
(✉), and Haihong E.
(✉)
Beijing University of Posts and Telecommunications, Beijing, China
{mnsong,dangshazi,lizhuohuan,ehaihong}@bupt.edu.cn
Abstract. Among the diverse distributed query and analysis engine, Kylin have
gained wide adoption since its various strengths. By using Kylin, users can
interact with Hadoop data at sub-second latency. However, it still has some
disadvantages. One representative disadvantage is the exponential growth of
cuboids along with the growth of dimensions. In this paper, we optimize the
cuboid materialization strategy of Kylin by reducing the number of cuboids based
on the traditional OLAP optimization method. We optimize the strategy mainly
from two aspects. Firstly, we propose Lazy-Building strategy to delay the
construction of nonessential cuboid and shorten the time of cuboid initialization.
Secondly, we adopt Materialized View Self-adjusting Algorithm to eliminate the
cuboids which are not in use for a long period. Experimental results demonstrate
the eﬃcacy of the proposed Distributed Self-Adaption Cube Building Model.
Speciﬁcally, by using our model, cube initialization speed has increased by 28.5%
points and 65.8% points space are saved, comparing with the cube building model
of Kylin.
Keywords: Distributed OLAP · Distributed query processing system · Kylin
Query log · Materialization strategy
1
Introduction
In the era of big data, many modern companies produce huge amounts of data in their
service lines. These data are used to conduct report analysis based on OLAP analysis.
In order to conduct report analysis, companies need a system which can response to the
query of thousands of data analysts at the same time. That requires high scalability,
stability, accuracy and speed of the system. In fact, there doesn’t exist a widely-accepted
method in distributed OLAP ﬁeld. Many query engines can also conduct report analysis,
such as Presto [4], Impala [2], Spark SQL [14] or Elasticsearch [10], but they are more
emphasis on data query and analysis. As a matter of fact, Kylin [7] is the specialized
tool in Distributed OLAP ﬁeld which is used often.
Kylin is originally developed by eBay, and is now a project of the Apache Software
Foundation. It is designed to accelerate analysis on Hadoop and allow the use of SQL-
compatible tools. It also provides a SQL interface and supports multidimensional anal‐
ysis on Hadoop for extremely large datasets. Kylin can reach the scale of one million or
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 382–393, 2018.
https://doi.org/10.1007/978-3-319-74521-3_41

even millisecond OLAP analysis. So it is very frequently-used in the domestic IT
industry.
The idea of Kylin is not original. Many technologies in Kylin have been used to
accelerate analysis over the past 30 years. These technologies involve storing pre-calcu‐
lated results, generating each level’s cuboids with all possible combinations of dimen‐
sions, and calculating all metrics at diﬀerent levels. Essentially, Kylin extends the
methods of traditional OLAP ﬁeld to the distributed ﬁeld, generating Cube on Hadoop
ecology.
When data becomes bigger, the pre-calculation processing becomes impossible even
with powerful hardware. However, with the beneﬁt of Hadoop’s distributed computing
power, calculation jobs can leverage hundreds of thousands of nodes [9]. This allows
Kylin to perform these calculations in parallel and merge the ﬁnal result, thereby signif‐
icantly reducing the processing time.
Data cube [5] construction is the core of Kylin, it has two characteristics: one is the
exponential growth of cuboids [5] along with the growth of dimensions; the other is the
large amount of IO due to increased number of cuboids. The cube is usually very sparse,
the increase of sparse data will waste a lot of computing time and memory space.
A full n-dimensional data cube could contains 2n cuboids [5]. However, most of
cuboids are not used, because most of query requested by data analyst follow the normal
distribution. That’s a waste of IO and memory.
In this paper, we propose a self-adaption cube building model which adopts a method
called lazy-building cuboids and abandons useless cuboids based on query log. It can
reduce the cube construction time and cube size a lot to save IO and memory. The paper
is structured as follows. In Sect. 2, we present the background. In Sect. 3, we introduce
the design and implementation details of the self-adaption cube building model. In
Sect. 4, we focus on experimental evaluation. Finally, in Sect. 5, we discuss the Self-
Adaption Cube Building Model and give a summary of the paper.
2
Background
2.1
Cube Calculation Algorithm
There are several strategies of data cube materialization [11] to reduce the cost of aggre‐
gation calculation and increase the query processing eﬃciency including iceberg cube
calculation Algorithm [3], condensed cube calculation Algorithm [15], shell fragment
cube calculation Algorithm [13], approximate cube calculation Algorithm [17], and
time-series data stream cube calculation Algorithm [6]. They are all based on Partial
Materialization [16], which means that a data sub-cube is selected and pre-calculated
according to speciﬁc methods. Partial Materialization is a compromise between storage
space, cost of maintenance and query processing eﬃciency.
In the process of iceberg cube calculation, sub-cubes which are higher than the
minimum threshold are aggregated and materialized. Beyer proposed BUC algorithm
[12] for iceberg cube calculation, which is widely-accepted.
According to the order of cuboid calculation, the methodologies of aggregation
calculation can be divided into two categories: top-down and bottom-up.
A Distributed Self-adaption Cube Building Model Based on Query Log
383

1. Top-Down: Firstly, calculate the metric of the whole data cube, and then the recur‐
sive search is performed along each dimension. Secondly, check the conditions of
the iceberg, prune branches that do not meet the condition. The most typical algo‐
rithm is BUC algorithm, it perform best on sparse data cube.
2. Bottom-Up: Starting from the base cuboids, compute high level cuboid from the low
level cuboid in the search grid according the parents-children relationship. Typical
algorithms are Pipesort algorithm, pipehash algorithm, overlap algorithm and
Multiway aggregation algorithm [18].
However, Kylin doesn’t follow the principle of partial materialization. In order to
reduce unnecessary redundant calculation and shorten the cube construction time, Kylin
adopts a Method called By Layer Cubing, which is a distributed version of the Pipesort
algorithm, a kind of bottom-up algorithm [1].
2.2
By Layer Cubing
As its name indicates, a full cube is calculated by layer: N dimension, N-1 dimension,
N-2 dimension, until 0 dimension; Each layer’s calculation is based on it’s parent layer
(except the ﬁrst, which base on source data); So this algorithm need N rounds of
MapReduce running in sequence [8]; In the MapReduces, the key is the composite of
the dimensions, the value is the composite of the measures; When the mapper reads a
key-value pair, it calculates its possible child cuboids; For each child cuboid, remove 1
dimension from the key, and then output the new key and value to the reducer; The
reducer gets the values grouped by key; It aggregates the measures, and then output to
HDFS; One layer’s MR is ﬁnished; When all layers are ﬁnished, the cube is calculated.
The following Fig. 1 describes the workﬂow:
Fig. 1. By layer cubing. Each level of computation is a MapReduce task, and serial execution.
A N dimensional Cube needs N times MapReduce Job at least.
384
M. Song et al.

It has some disadvantage:
1. This algorithm causes too much shuﬄing to Hadoop; The mapper doesn’t do aggre‐
gation, all the records that having same dimension values in next layer will be omitted
to Hadoop, and then aggregated by combiner and reducer;
2. Many reads/writes on HDFS: each layer’s cubing need write its output to HDFS for
next layer MR to consume; In the end, Kylin need another round MR to convert
these output ﬁles to HBase HFile for bulk load; These jobs generates many inter‐
mediate ﬁles in HDFS;
All in all: the performance is not good, especially when the cube has many dimen‐
sions.
2.3
By Segment Cubing
In order to solve these shortcomings above, Kylin develops a new cube building algorism
called by segment cubing. The core idea is, each mapper calculates the feed data block
into a small cube segment (with all cuboids), and then output all key/values to reducer;
the reducer aggregates them into one big cube segment, ﬁnishing the cubing; Fig. 2
illustrates the ﬂow;
Fig. 2. By segment cubing.
Compared with By Layer Cubing, the By Segment Cubing has two main diﬀerences:
1. The mapper will do pre-aggregation, this will reduce the number of records that the
mapper output to Hadoop, and also reduce the number that reducer need to aggregate;
2. One round MR can calculate all cuboids;
Based on the work mentioned above, we take advantage of both two Algorithm, and
optimize cuboid materialization strategy.
A Distributed Self-adaption Cube Building Model Based on Query Log
385

3
Design and Implementation
In this section, we ﬁrst introduce the architecture of Self-adaption Cube Building Model
(SCBM) and the overall workﬂow. Then we explain cuboids Lazy-Building and the
cuboid spanning tree. Finally, we describe the implementation details of the Materialized
View Self-Adjusting Algorithm.
3.1
Architecture of Self-adaption Cube Building Model
The overall architecture of self-adaption cube building model is illustrated in following
Fig. 3.
Fig. 3. Architecture of self-adaption cube building model.
Self-adaption cube building model takes fact table [5] as input of the overall system,
usually the fact table is managed by distributed Data Warehouse Hive. We ﬁrst set the
parameters of the cube model, such as the ﬁled for analysis, the base cuboid level and
so on, then we build base cuboids in a mapper-reduce. After the construction of the base
cuboids, the system can support query request, Query execution engine [7] resolves the
query to ﬁnd the required cuboids. If the cuboid has been generated, the query will be
executed; if the cuboid is missing, the Lazy building module will be triggered to build
the cuboid using the method in Sect. 3.2. When the query result returns, the system
records the query log and waits for the adjustment of cube launched by Self-Adaption
module according to the Materialized View Self-Adjusting Algorithm explained in
Sect. 3.3. At the same time, the system maintains a dynamic cube spanning tree to store
the metadata of cuboids.
386
M. Song et al.

3.2
Cuboid Spanning Tree and Lazy-Buliding
Cuboid Spanning Tree. In original By Layer Cubing, Kylin calculates the cuboids
with Broad First Search (BFS) order, which causes a waste of memory. On the contrary,
Cuboid Spanning Tree generates cuboid with Depth First Search (DFS) order to reduce
the cuboids that need be cached in memory. This avoids unnecessary disk and network
I/O, and the resource Kylin occupied is highly reduced;
With the DFS order, the output of a mapper is fully sorted (except some special
cases), as the row key of cuboid is composed of cuboid ID and dimension values like
[Cuboid ID + dimension values], and inside a cuboid the rows are already sorted. Since
the outputs of mapper are already sorted, Shuﬄes sort would be more eﬃcient.
In addition, DFS order is vary suitable for cuboid’s lazy building. Cuboid spanning
tree also record the metadata of cuboids in every node in the tree, it provides the basis
for the selection of ancestor cuboids.
Lazy-Building. Lazy-Building is a basic concept of the model. In order to reduce the
number of cuboids, we adopt the strategy of generating on demand. At the same time,
we persist all cuboids on the low layer in the By-Layer cubing algorithm for higher speed
and lower computational complexity of Lazy-Buliding. For example: a cube has 4
dimensions: A, B, C, D; Each mapper has 1 million source records to process; The
column cardinality in the mapper is Card(A), Card(B), Card(C) and Card(D). The Lazy-
Buliding is demonstrated in Fig. 4.
Fig. 4. Lazy-building and ancestor cuboids selection.
A Distributed Self-adaption Cube Building Model Based on Query Log
387

1. User set a base-layer parameter in cube model info to control the scale of base
cuboids layer. If this parameter is not set, it will use default value log
(dimensions) + 1
2. Base cuboids building module import data from fact table and build the base cuboids
by Cube Build Engine in Kylin.
3. Update the Cuboid Spanning Tree and save the metadata.
4. Client launch a query select avg (measure i) from table group by C which hit the
missing cuboid [C]. Then, lazy building module receive request to build cuboid [C].
5. Lazy building module ﬁnd a cuboid generation path according to Ancestor Cuboids
Selection and build the missing cuboid to response the query as soon as possible.
6. Record the path and determine whether to build all the cuboid on the path at low
load according to Materialized View Self-Adjustment Algorithm.
Ancestor Cuboids Selection. When the needed cuboids is missing, we should select
an ancestor cuboid and a cuboid generation path. The basic principle is to choose the
ancestor cuboid whose measures are the least to aggregate, which means we can get the
minimum amount of computation and time to generate the missing cuboid. After that,
we need to ﬁnd a path P from ancestor cuboid to the missing cuboid in compliance with
the Minimum cardinality principle.
For example, in the Fig. 4. In order to generate the missing cuboid [C], we ﬁrstly
ﬁnd all the candidate cuboids [A B C] [A C D] [B C D]. Then, we compare the size of
the three candidate cuboids. Assuming [B C D] is selected, we generate [C] by aggregate
[B C D] on dimension B, D. The cube is enough to response the query. However, for
the sake of maintenance of cube according to By Layer Cubing, we need to ﬁnd a path
from [B C D] to [C].
When aggregating from parent to a child cuboid, assuming that from base cuboid [B
C D] to 1-dimension cuboid [C], There are two paths: [B C D] [B C] [C] and [B C D]
[C D] [C]. We assume Card(D) > Card(B) and the dimension A is independent with
other dimensions, after aggregation, the cuboid [BCD]’s size will be about 1/Card(D)
or 1/Card(B) of the size of base cuboid; So the output will be reduced to 1/Card(D) or
1/Card(B) of the original one in this step. So we choose the ﬁrst path, the records that
written from mapper to reducer can be reduced to 1/Card(D) of original size; The less
output to Hadoop, which means less I/O and computing and the model can attain better
performance.
3.3
Materialized View Self-Adjustment Algorithm
Self-adaption module adjusts the cube according to the Materialized View Self-
Adjusting Algorithm. This chapter proposes a query statistics method which takes ﬁxed
times of queries as a statistical period, and this method updates the corresponding query
statistics. This method adjusts materialized views set according to the threshold of elim‐
ination and generation, stabilizes the query eﬃciency, and minimizes the shake of mate‐
rialized view.
388
M. Song et al.

Query Statistics Method. A kind of Statistics Method for query.
Deﬁnition 1: Materialized view adjustment cycle Tn.
The materialized view adjustment cycle can be customized to a ﬁxed number of
queries, for example, every 100 queries for a materialized view adjustment cycle.
Deﬁnition 2: Average query statistics E
(
Tn
(
qi
)).
Since the actual query may change over time, the query set should also be adjusted
accordingly. For example, a query that has not been executed in a couple of cycles should
be removed from the query collection and the corresponding materialized view is
deleted.
After many queries, the query log will accumulate a certain amount of query records,
this paper presents a query statistical method based on the query log which described in
the following. If there is a query set Q =
{
q1, q2, … , qn
}, and the query log set L, scan
forward the log ﬁle from the ending of the log ﬁle and determine whether there is a query
qi in the Tn cycle, and update E(Tn
(qi
)) according to Eq. 1:
E
(
Tn
(
qi
))
=
{ 𝛼+ (1 −𝛼)E(T(n−1)
(qi
)) qi ∈L(Tn
)
(1 −𝛼)E
(
T(n−1)
(
qi
))
qi ∉L
(
Tn
)
(1)
In the formula, α is a weighted coeﬃcient, a constant; L(Tn
) is the query set in the
Tn cycle. By this method, we can monitor the change of the query set Q, which can greatly
reduce the shake of materialized view.
Materialized View Self-Adjustment Algorithm. The main steps of the materialized
view set adjustment with the query changes are listed as follows:
1. Prior to the adjustment, initialize materialized view set M = {base cuboids}, the
corresponding query task set to Q.
2. During the query, the query is written into the query log L, and the query counter is
accumulated
3. Set the threshold of elimination T and the threshold of generation S, update the
Average query statistics each life cycle Tn, and determine whether eliminate or
materialize corresponding views.
The pseudo code of Materialized View Self-Adjustment Algorithm is showed in
Algorithm 1.
A Distributed Self-adaption Cube Building Model Based on Query Log
389

In the above algorithm, from line 1 to line 8, it scans the query log in a statistical
period Tn, and update the query task set Q during the scanning. From line 9 to line 17, it
iterate around query in Q, and determine whether eliminate or materialize corresponding
views according to the comparison of threshold and the E
(
Tn
(
qi
)) calculated by
formula 2. Suppose query task set Q contains k different query, then the time complexity
of the algorithm is O(Tn + k).
4
Experimental Evaluation
4.1
Dataset
To test performance, we use the standard weather dataset from the China Meteorological
Data network. The dataset contains 4726499 weather records from China’s 2170 distinct
counties started from January 1, 2011 to January 1, 2017. The original dataset is too
complicated. In order to better conduct the experiment, we select eight dimensions:
Province, city, county, date, weather, wind direction, wind speed, air quality level and
two measures: Maximum temperature, Minimum temperature.
390
M. Song et al.

4.2
Evaluation Metrics
We use cube ﬁrst construction time, average query time and cube size as the evaluation
metrics of our proposed method.
Cube First Construction Time refers to the base cuboids building time for self-
adaption cube building model.
Average Query Time is deﬁned as the average query time during materialized view
adjustment cycle T1–Tn. The detailed calculation is listed in Eq. 2.
Average Query Time =
1
NUM(Q)
∑n
i=1
∑mTi
j=1 response time(qi)
(2)
Cube Size refers to the disk allocation that the whole cube takes up.
4.3
Experimental Results
We ﬁrst compare the metric of cube ﬁrst construction time. Because the parameter of
base-layer has a great impact on this metric, in order to reﬂect the average condition,
we use the default value log (dimensions) + 1. We test the model 5 times and the results
were aggregated to calculate averages which can avoid the impact of MapReduce failure.
Results can be seen from Table 1, the time consumption of the new model is reduced
by 28.5% (Fig. 5).
Table 1. Cube ﬁrst construction time
Test result
Test 1
Test 2
Test 3
Test 4
Test 5
Average
Origin Kylin cube
building model
92 min
83 min
86 min
104 min
91 min
91.2 min
Self-adaption cube
building model
64 min
61 min
83 min
57 min
61 min
65.2 min
Fig. 5. Average query time trends in T1–T30.
A Distributed Self-adaption Cube Building Model Based on Query Log
391

In query time, we set Materialized view adjustment cycle Tn 50 and test 30 cycles T1
–T30. We can observe that Cuboid hit rate and query response time signiﬁcantly increased
and improved along with the increase of query requests. Finally, the query eﬃciency of
the two models are almost on a par.
In cube size, we see that the curve that represents this metric tends to be stable after
vibration in prophase from Fig. 6. Finally, the spaces consumption of the proposed model
was reduced by 65.83%.
Fig. 6. Average cube size trends in T1–T30.
5
Conclusion
We have presented a Distributed Self-Adaption Cube Construction Model Based on
Query Log and applied it to a weather dataset to test its performance. Our model adopts
a special partial materialization strategy and it can automatically adjust the cuboid set
which is used in query request according to query log. Based on experimental results,
the proposed model can reduce the cube construction time and cube size to a great extent
at the expense of tiny query eﬃciency reduction. However, this model has good
performance only when the query distribution is relatively concentrated. So users can
choose either of the two models according to their practical business query scenario.
Overall, the proposed model is of great practical signiﬁcance in the application of BI
tools. In the next stage, we will optimize the base cuboids generation strategy to reduce
the query latency in prophase.
Acknowledgement. This work is supported by the National Key project of Scientiﬁc and
Technical Supporting Programs of China (Grant No. 2015BAH07F01); Engineering Research
Center of Information Networks, Ministry of Education.
392
M. Song et al.

References
1. Chen, Y., Dehne, F., Eavis, T., Rau-Chaplin, A.: Parallel ROLAP data cube construction on
shared-nothing multiprocessors. Distribut. Parallel Databases 15(3), 219–236 (2004)
2. Impala (2017). http://impala.apache.org/. Accessed 13 Apr 2017
3. Deshpande, P.M., Gupta, R., Gupta, A.: Distributed iceberg cubing over ordered dimensions,
March 2015. US Patent App. 14/658,542
4. Presto (2017). https://prestodb.io/. Accessed 13 Apr 2017
5. Gray, J., Chaudhuri, S., Bosworth, A., Layman, A., Reichart, D., Venkatrao, M., Pellow, F.,
Pirahesh, H.: Data cube: a relational aggregation operator generalizing group-by, cross-tab,
and sub-totals. Data Mining Knowl. Disc. 1(1), 29–53 (1997)
6. Kalisch, M., Michalak, M., Przystałka, P., Sikora, M., Wróbel, Ł.: Outlier detection and
elimination in stream data – an experimental approach. In: Flores, V., et al. (eds.) IJCRS 2016.
LNCS (LNAI), vol. 9920, pp. 416–426. Springer, Cham (2016). https://doi.org/
10.1007/978-3-319-47160-0_38
7. Kylin (2017). http://kylin.apache.org/. Accessed 13 Apr 2017
8. Lee, S., Kim, J., Moon, Y.-S., Lee, W.: Eﬃcient distributed parallel top-down computation
of ROLAP data cube using MapReduce. In: Cuzzocrea, A., Dayal, U. (eds.) DaWaK 2012.
LNCS, vol. 7448, pp. 168–179. Springer, Heidelberg (2012). https://doi.org/
10.1007/978-3-642-32584-7_14
9. Li, F., Ozsu, M.T., Chen, G., Ooi, B.C.: R-store: a scalable distributed system for supporting
real-time analytics. In: 2014 IEEE 30th International Conference on Data Engineering
(ICDE), pp. 40–51. IEEE (2014)
10. Elasticsearch (2017). https://www.elastic.co/products/elasticsearch. Accessed 13 Apr 2017
11. Nandi, A., Yu, C., Bohannon, P., Ramakrishnan, R.: Distributed cube materialization on
holistic measures. In: 2011 IEEE 27th International Conference on Data Engineering (ICDE),
pp. 183–194. IEEE (2011)
12. Shi, Y., Zhou, Y.: An improved apriori algorithm. In: Granular Computing (GrC), pp. 759–
762. IEEE (2010)
13. Silva, R.R., Hirata, C.M., de Castro Lima, J.: Computing big data cubes with hybrid memory.
J. Convergence Inf. Technol. 11(1), 13 (2016)
14. Spark SQL (2017). http://spark.apache.org/sql/. Accessed 13 Apr 2017
15. Wang, W., Feng, J., Lu, H., Yu, J.X.: Condensed cube: an eﬀective approach to reducing data
cube size. In: 18th International Conference on Data Engineering, Proceedings, pp. 155–165.
IEEE (2002)
16. Xia, Y., Luo, T.T., Zhang, X., Bae, H.Y.: A parallel adaptive partial materialization method
of data cube based on genetic algorithm (2016)
17. Yin, D., Gao, H., Zou, Z., Li, J., Cai, Z.: Approximate iceberg cube on heterogeneous
dimensions. In: Navathe, S.B., Wu, W., Shekhar, S., Du, X., Wang, X.S., Xiong, H. (eds.)
DASFAA 2016. LNCS, vol. 9643, pp. 82–97. Springer, Cham (2016). https://doi.org/
10.1007/978-3-319-32049-6_6
18. Zhao, Y., Deshpande, P.M., Naughton, J.F.: An array-based algorithm for simultaneous
multidimensional aggregates. ACM SIGMOD Rec. 26, 159–170 (1997)
A Distributed Self-adaption Cube Building Model Based on Query Log
393

Property-Based Network Discovery
of IoT Nodes Using Bloom Filters
Rustem Dautov1(B), Salvatore Distefano1,2, Oleg Senko3, and Oleg Surnin3
1 Higher Institute of Information Technology and Information Systems (ITIS),
Kazan Federal University (KFU), Kazan, Russia
{rdautov,s distefano}@it.kfu.ru
2 University of Messina, Messina, Italy
sdistefano@unime.it
3 Kazan, Russia
Abstract. As the number of IoT devices is exponentially growing, and
IoT networks are expanding in their size and complexity, timely device
discovery is becoming a pressing concern. The extreme (and constantly
growing) number of network nodes, dynamically connecting to and dis-
connecting from a network, renders existing routing techniques, such
as multicasting and broadcasting, unscalable, especially when using the
IPv6 128-bit addresses. To address this limitation, this paper discusses
the potential of implementing the IoT device discovery, based on device
properties, such as type, functionality, location, etc., and presents an
approach to enable property-based access to IoT nodes using Bloom ﬁl-
ters. The proposed approach demonstrates space- and network-eﬃcient
characteristics, as well as provides an opportunity to perform device dis-
covery at various granularity levels.
Keywords: Bloom ﬁlter · Internet of things · Edge computing
Device discovery
1
Introduction
In the IoT context, a user (or an application) using hundreds or thousands
of devices has to deal with considerably long addresses to uniquely identify
and refer to network devices. This exponential growth is expected to introduce
new challenges to traditional computer network protocols, such as, for example,
(i) eﬃcient access to a huge number of devices; (ii) security and privacy; (iii)
interoperability and standardisation; (iv) eﬃcient energy consumption. More-
over, given the extreme amounts of heterogeneous devices constituting the IoT
ecosystem, timely and accurate device discovery based on some speciﬁc parame-
ters such as device type, sensing/actuating capabilities, status, powering options
is also seen as a pressing and challenging issue. In this light, a potential way to
enable device discovery in the IoT, taking into account the size and complex-
ity of underlying networks, could be to include additional parameters in the
c
⃝Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 394–399, 2018.
https://doi.org/10.1007/978-3-319-74521-3_42

Property-Based Network Discovery of IoT Nodes using Bloom Filters
395
routing procedure with a goal to limit the search space. More speciﬁcally, a
potential solution would be able address IoT network devices not only through
their IP addresses, but also through a combination of device properties, such
as their type, location, sensing/actuating capabilities, available resources and so
on. From this perspective, an envisaged solution could implement some kind of
selective routing algorithm, which would facilitate time- and network-eﬃcient
device discovery in the IoT context. For instance, this will allow collecting infor-
mation from speciﬁc sensing devices (e.g. within a single building) and apply
actuation commands (e.g. turn on heating in rooms with low temperature) in a
selective manner. Similarly, remote device management and maintenance would
also become feasible, as users would be able to diagnose errors and patch required
devices with corresponding software updates (e.g. keeping a surveillance system
up to date with most recent system security updates).
Taking into considerations these desired features of a possible solution, this
paper presents an approach to facilitate property-based device search and discov-
ery in complex IoT networks using counting Bloom ﬁlters. As it will be explained
below, the proposed approach beneﬁts from the space-eﬃcient way of storing
information about devices and their properties, as well as fast calculation times
when deciding whether a matching device is present in the network. Moreover,
with property-based search using Bloom ﬁlters, it becomes possible to perform
device discovery at various granularity levels.
2
Background: Bloom Filters
A Bloom ﬁlter, originally deﬁned by Bloom in 1970 [1], is a space-eﬃcient prob-
abilistic data structure, representing a set S of m elements using an array of n
bits B = (B[1], ..., B[n]) initialised to 0. The ﬁlter uses a set of k independent
hash functions H = {h1, ..., hk} with a range {1, ..., n} uniformly mapping each
element of S to a random position over the B array. More speciﬁcally, for each
element s ∈S, the bits B[hi(s)] are set to 1 ∀i | 1 ≤i ≤k. A bit can be set to 1
multiple times either through diﬀerent hash functions for the same element s or
diﬀerent elements of S. As a result, an answer to the query ‘Is b ∈S?’ is true, if
all hi(b) are set to 1, otherwise (i.e. if at least one bit is 0), b is not in S. While
the Bloom ﬁlter has many advantages, such as fast access time and a relatively
small size (a few bytes per element at most), it may suﬀer from a possibility
of false positive results on membership checks. A false positive occurs when the
hashes from an element not in the Bloom ﬁlter overlap with a combination of
hashes from elements that are in the Bloom Filter.
Deleting elements from a Bloom ﬁlter cannot be done simply by changing
ones back to zeros, as a single bit may correspond to multiple elements. To
enable deletion of elements, the so-called counting Bloom ﬁlter uses an array
of n counters instead of bits. These counters are able to ‘track’ the number of
elements currently hashed to that location [3]. Deletions can be safely done by
decrementing the relevant counters. A standard Bloom ﬁlter can be derived from
a counting Bloom ﬁlter by setting all non-zero counters to 1.

396
R. Dautov et al.
Bloom ﬁlters were originally introduced to improve data management per-
formance, and quickly became popular in a variety of databases and storage
systems [1]. Then, they have been widely used in distributed systems [6] across
a wide range of application domains. In the recent years, Bloom ﬁlters experi-
enced an increased interest by the networking and security domains [2]. More
speciﬁcally, they are also widely applied in intrusion detection, virus and spam
detection, access control [4], and IP traceback [5].
3
Proposed Approach
The proposed approach is based on a two-step procedure. First, all the interme-
diate nodes in the network hierarchy are populated with information about edge
nodes. Second, once the network is populated, an IoT device can be discovered
using a corresponding query.
3.1
Populating the Network
An IoT device in a network hierarchy can be represented as a tuple D =
(ID, Prop), where ID is a unique identiﬁer of this device, and Prop is a set
of properties of this device, such as, for example, type (e.g. CCTV camera,
environmental sensor, smartphone, etc.), sensing capabilities (e.g. temperature,
pressure, noise, acceleration, etc.), power supply (e.g. solar panel, battery, power
cord, etc.), manufacturer, model, production date, and so on. Using suitable hash
functions, each set of device properties Prop is converted into a corresponding
Bloom ﬁlter array. Next, the network hierarchy is ‘populated’ by these newly-
created Bloom ﬁlters in a bottom-up manner – i.e. edge devices provide its
immediate subnet gateway with their Bloom ﬁlter representations, which are
summed up in a single Bloom ﬁlter by doing the bitwise OR operation. This
process is then iteratively repeated up until the very top of an IoT network hier-
archy. As a result, the top-level server’s Bloom ﬁlter eventually contains Bloom
ﬁlters of all individual edge nodes in its network.
3.2
Device Discovery
At the device discovery step, Bloom ﬁlters are used to represent correspond-
ing discovery queries. More speciﬁcally, each query is represented by a tuple
Q = (ID, Prop), where ID is a unique identiﬁer of this query, and Prop is a
set of device properties, which are expected to be discovered within the given
network. These properties can be seen as search parameters, as typically used in
traditional searching. Each query is then represented by a corresponding Bloom
ﬁlter, using the same hash functions. Device discovery can be seen as a reverse
process of populating the network hierarchy, executed in a top-down manner
starting from the very top of the network topology. By performing the bitwise
AND operation, the top-level server ﬁrst checks with its own Bloom ﬁlter whether

Property-Based Network Discovery of IoT Nodes using Bloom Filters
397
there is a device, matching query parameters, in its managed network. Accord-
ingly, if the evaluation is true, the query is sent down to lower-level network
gateways, which similarly check whether a suitable device is present in their
subnet. This process iterates in each subnet either (i) until reaching the very
bottom level and a suitable device is discovered, or (ii) until one of the interme-
diate network nodes replies that no matching device is present in its subnet.
3.3
Sample Scenario
The simpliﬁed use case scenario assumes each device (and its properties) is rep-
resented by a 6-bit Bloom ﬁlter. From left to right, these bits indicate whether a
device is: a camera (1), an environmental sensor (2), a smartphone (3), battery-
powered (4), powered by a solar panel (5), or powered by a cord (6). It is also
assumed that there are three subnets in the network, each containing three
devices (Fig. 1). The three network gateways contain combined Bloom ﬁlters of
their respective subnets, and the server contains the overall Bloom ﬁlter rep-
resentation of the network. The goal of this scenario is to discover a camera,
powered by a cord. Accordingly, the query is represented by the following Bloom
ﬁlter BF = (1, 0, 0, 0, 0, 1).
At the ﬁrst step, the server evaluates the query against its own Bloom ﬁlter,
and decides that there is indeed a matching device present somewhere down the
network. Next, the query is propagated down to three subnets. The respective
gateways start evaluating the query against their own Bloom ﬁlters. As it is seen
from the diagram, Subnet A contains only smartphones, and Subnet B contains
only environmental sensors. The query evaluation returns false, and, as a result,
the network call is not propagated down the ﬁrst two subnets. Gateway C, by
evaluating the query, understands that there is a matching device in its subnet
and sends the query to all three nodes. Two of these nodes are cameras, but
only one of them – Device C3 – is actually powered by a cord. By evaluating
Fig. 1. Property-based network device discovery using Bloom ﬁlters.

398
R. Dautov et al.
the incoming query, it realises that it matches query parameters, and replies
back with its ID and network location. The reply is then sent back to the server
through intermediate hops.
The presented property-based search for IoT devices enables ﬂexible, ﬁne-
grained discovery of IoT nodes. The more properties are speciﬁed in the query,
the more precise the search is and less matching devices are discovered. On
contrary, for a single speciﬁed property, the search space is expected to be wider,
since more devices might satisfy the search query parameter. For example, in the
simpliﬁed scenario above, searching for a solar panel-powered device will yield 6
results.
4
Discussing the Beneﬁts
This paper presented an approach for property-based network discovery of IoT
nodes using Bloom ﬁlters. Potential beneﬁts of the proposed solution can be
summarised as follows:
Flexible device discovery at diﬀerent levels of granularity: as opposed to the
traditional access to edge nodes in IoT environments, where IP addresses need
to be known in advance, the proposed approach enables searching for devices
based on their properties, such as type, sensing capabilities, powering options,
etc. This kind of property-based device discovery can be performed at various
levels of granularity – i.e. coarse-grained (e.g. discover any kind of camera within
a network) or ﬁne-grained (e.g. discover an outdoors CCTV camera with high
resolution, powered by a solar panel). This ﬂexibility has the potential to con-
tribute to creation of a wide range of IoT systems, where the network topology
is not static, but rather devices are constantly joining and leaving the network.
Moreover, the property-based search paves the way for interchangeable IoT archi-
tectures, in which individual elements are described in terms if their features and
functionalities. This way, one element can be substituted by a similar one based
on their shared properties, in a seamless, transparent manner.
Network eﬃciency: a Bloom ﬁlter (as suggested by its name itself) serves to
ﬁlter incoming search queries to avoid redundant broadcast calls through the
whole network. If an intermediate node understands that there is no matching
device within its subnet, it does not allow the query to go down that speciﬁc
subnet, thus (i) decreasing the amount of time needed to discover a device, and
(ii) minimising the amount of redundant network traﬃc and improving network
latency. Moreover, the query evaluation procedure – i.e. performing the bitwise
AND operation on two bit arrays – is a time-eﬃcient operation with minimum
impact on the overall device discovery process. In the presence of hundreds and
thousands of edge devices and intermediate network nodes, both network- and
time-eﬃciency is seen as considerable beneﬁts when discovering devices (even
taking into consideration the false positive rate).
Space eﬃciency: Bloom ﬁlters are space-eﬃcient data structures, requiring min-
imum amount of memory. Even when using counting Bloom ﬁlters, resulting

Property-Based Network Discovery of IoT Nodes using Bloom Filters
399
arrays typically do not exceed 4 MB of storage space – a highly-relevant feature
in the context of IoT environments, where individual nodes are not necessarily
equipped with mass storage facilities.
High accuracy: despite potential false positive results, which may occur during
the device discovery process at intermediate network nodes, the overall accuracy
is not aﬀected, since a ﬁnal decision whether a matching device is present in
the network or not is taken by edge devices themselves. Only if an edge device’s
Bloom ﬁlter matches an incoming query, a corresponding acknowledgement is
sent back to the server. Otherwise, it is assumed that no matching devices were
identiﬁed.
Scalability and extensibility: thanks to the ability to store large amounts of
hashed values and high calculation speed, a Bloom ﬁlter can be updated with
new elements with a minimum eﬀect on the overall performance. This is espe-
cially important in the context of IoT networks, which are already constituted
by millions of devices, and keep on growing in their size and complexity.
5
Conclusions
The presented solution enables ﬂexible property-based device discovery in the
context of complex IoT networks using Bloom ﬁlters. As opposed to the IPv6
routing, which requires 128 bits to encode an address, the proposed approach
beneﬁts from the space-eﬃcient way of representing and storing data using a
Bloom ﬁlter. This also contributes to decreased traﬃc and network latency, as
the device discovery duration depends on how narrow-focused a search query is
(i.e. the less devices matching the query, the less network traﬃc is generated).
The Bloom ﬁlter decides whether a device belongs to a subnet branch or not,
and can ‘cut oﬀ’ the entire branch before actually checking it. As a result, this
considerably reduces the amount of network traﬃc, especially when compared
to broadcast and multicast routing techniques.
References
1. Bloom, B.H.: Space/time trade-oﬀs in hash coding with allowable errors. Commun.
ACM 13(7), 422–426 (1970)
2. Broder, A., Mitzenmacher, M.: Network applications of Bloom ﬁlters: a survey. Int.
Math. 1(4), 485–509 (2004)
3. Fan, L., Cao, P., Almeida, J., Broder, A.Z.: Summary cache: a scalable wide-area
web cache sharing protocol. IEEE/ACM Trans. Netw. (TON) 8(3), 281–293 (2000)
4. Foley, S.N., Navarro-Arribas, G.: A Bloom ﬁlter based model for decentralized
authorization. Int. J. Intell. Syst. 28(6), 565–582 (2013)
5. Snoeren, A.C., Partridge, C., Sanchez, L.A., Jones, C.E., Tchakountio, F.,
Kent, S.T., Strayer, W.T.: Hash-based IP traceback. In: ACM SIGCOMM Com-
puter Communication Review, vol. 31, pp. 3–14. ACM (2001)
6. Tarkoma, S., Rothenberg, C.E., Lagerspetz, E., et al.: Theory and practice of Bloom
ﬁlters for distributed systems. IEEE Commun. Surv. Tutorials 14(1), 131–155 (2012)

The Personality Analysis of Characters in Vernacular
Novels by SC-LIWC
Yahui Yuan1, Baobin Li1(✉), Dongdong Jiao2, and Tingshao Zhu2
1 School of Computer and Control, University of Chinese Academy of Sciences,
Beijing 100190, China
libb@ucas.ac.cn
2 Institute of Psychology Chinese Academy of Sciences, Beijing 100101, China
tszhu@psych.ac.cn
Abstract. There are many researches on psychological text analysis, and it has
been proved that the words people use can reﬂect their emotional states. In this
paper, we introduce how to analyze the psychology of the characters in vernacular
novels automatically. First, we process the dialogs with word segmentation, and
analyze the segmented text with SC-LIWC. Then, a vector reﬂecting the
psychology of the character is obtained and we map it to the big ﬁve. Finally,
taking the dialogs of the Journey to the West as corpus, We have got the person‐
alities of four main characters which are veriﬁed to be same as some famous
comments of the Journey to the West, which shows that our work is eﬀective.
Keywords: LIWC · Vernacular · The Journey to the West · The big ﬁve
Text analysis
1
Introduction
The use of words in the text can reﬂect the individual’s psychological state and person‐
ality [1]. Linguistic Inquiry and Word Count is a tool which we can use to analyze text.
The way that the Linguistic Inquiry and Word Count (LIWC) program works is fairly
simple. Basically, it reads a given text and counts the percentage of words that reﬂect
diﬀerent emotions, thinking styles, social concerns, and even parts of speech.
To date, LIWC has been applied to many psychology research. It is often used to
examine suicide writings in order to characterize the quantitative linguistic features of
suicidal texts, in [2], the authors analyze texts compiled in Marilyn Monroe’s Fragments
using LIWC, in order to explore the contact between the use of diﬀerent linguistic cate‐
gories over the years and her suiside. The result is coincide with diﬀerent theories of
suicide. López-López et al. [3] analyzed the StackOverﬂow’s answers and questions to
explore the users’ personality traits. They found that the top reputed authors are more
extroverted than general users. Moreover, authors who got more votes express signiﬁ‐
cantly less negative emotions than those who got less votes. Markovikj et al. [4] explored
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 400–409, 2018.
https://doi.org/10.1007/978-3-319-74521-3_43

the modeling feasibility of user personality based on the features extracted from Face‐
book. In [5], they collected a sample of 363 participants, including their written self-
introductions and ﬁnal course performance, the result shows that course performance
could indeed be predicted by the word usage of linguistic categories.
LIWC for Traditional Chinese, TC-LIWC, is published with the authorization of
Pennebaker by Huang et al. (2012). After that, SC-LIWC for Simpliﬁed Chinese [6, 7]
is published on the basis of TC-LIWC, which lays the foundation for the following
research [10, 12].
Recently, some researchers are concerned about automatic personalistic prediction
using liwc. Personality is stable in a period of time, so a collected corpus from several
months is suitable for this research. Gao [12] selects 1766 participants, ﬁrst make them
ﬁll in a big ﬁve inventory for comparison, and then collect their weibo through the API
of Sina. 90% of the samples are trained using liwc and the rest act as test set. In the
training stage, they compute the Pearson’s coeﬃcient between the inventory and the
training results, then choose features which behave well in the training. At last, the
features are composed to predict personality of test set. They compute the Pearson’s
coeﬃcient as before, and the results are between 0.3~0.4. While the coeﬃcient between
self-rating and rating by observers is about 0.5, hence the method has prediction ability
to some extent.
We will seek method to predict the personality of characters in novels written in
vernacular. Vernacular is a written language with some artistic processing. It is easy to
read, but still have some features of ancient Chinese. Vernacular is generally used for
literacy, especially in the novels. Vernacular novels are very popular from the beginning
of Ming Dynasty. Three of the four famous Chinese novels were accomplished in Ming
Dynasty. After that, vernacular novels were more and more popular. There are many
excellent ancient books in China, which created numerous virtual characters, a book
named A Dream of Red Mansions only, contains hundreds of characters. We will pay
a lot of time to read books, look up in the library, to understand these ﬁgures and step
into the author’s inner world. If we can analyze the characters of the books automatically,
it will save us much time and help us follow the books.
In this study, we use LIWC to analyze the personality of the characters in vernacular
novels. The process of personality analysis of the characters in the vernacular novels is
shown in Fig. 1. The rest of this paper is organized as follows. Section 2 will discuss
the preprocessing work for the novel. And then in Sect. 3, we will split the dialogs
obtained from Sect. 2, which will be used to analysis the personality of the characters
and get the big ﬁve of characters in Sect. 4. Finally, we also present the personality
change of Sun Wukong before and after the three strikes of White Bone Demon.
The Personality Analysis of Characters in Vernacular Novels
401

Fig. 1. The ﬂow chart of automatic personality analysis of vernacular novels.
2
Data and Data Preprocessing
The Journey to the West is the ﬁrst ancient Chinese romantic novel. The book deeply
depicts the social reality of the time, mainly describes the origin of Tang priest, Sun
Wukong, Zhu Bajie, Sha monk, and together with the story of pilgrimage to the west.
After the spread of centuries, the Journey to the West has been translated into many
languages, and a number of relevant research monographs have been published, which
made a high evaluation of the novel. The Journey to the West is known as one of the
four famous Chinese classics. There are four main characters in the Journey to the
West. They are Sun Wukong, Sha monk, Zhu Bajie and their master, Tang priest.
James W. Pennebaker proved that words used in their daily lives could contains
important information of psychological information [8]. Especially, he proved that not
only nouns and verbs serve as markers of emotional state, social identity, and cognitive
styles, particles, serve as the glue that holds nouns and regular verbs together, can also
do the same things. That means we can study the particles instead of more complex
methods. On the basis of his work, we decide to use the dialogs of the characters to study
their personality. We select the dialogues and their inner monologues for each character
respectively, and put them into 5 diﬀerent ﬁles. There are a lot of descriptive verses in
the text, we should delete them because they would interface the process of participation,
and these verses are often said by other people, not the roles themselves, so we believe
that the descriptive verse have little inﬂuence on the personality. Notice that it shouldn’t
include quotes on the end of the sentence. But other interval will be retained for the next
step. At last we get four ﬁles.
3
Segmentation
In order to get the particles for analysis, we should ﬁrst do the text segmentation. As we
know, the current methods of Chinese word segmentation can be divided into three
kinds: the method based on lexicons, the method based on statistics and one based on
402
Y. Yuan et al.

semantics. Also there are methods mixing two or three of them in order to improve the
accurate. There have been many kinds of word segmentation systems for modern
Chinese. For example, LTP-CLOUD, NLPIR, jieba, and so on. These systems all have
good results in Chinese word segmentation. Among these tools, LTP and NLPIR are
systematic tools, while jieba only contains segmentation function. Moreover, LTP and
jieba are open source tools, but NLPIR is not.
Few studies are involved in ancient Chinese segmentation. First, there is little corpus
for ancient Chinese. As we all know, segmentation need a lot of corpus which has been
marked manual to improve the accurate, but no one have done the work for ancient
Chinese. Second, it seems that we cannot obtain any economic beneﬁts from it. However,
Hou, et al. have studied ancient Chinese segmentation [9], but the corpus is still very
small, which couldn’t be generalized easily.
Vernacular has the features of both ancient Chinese and modern Chinese. Therefore,
we can refer to the methods for modern Chinese segmentation.
The punctuation and function words are not changed much over the years. They are
also used in ancient Chinese. Besides, there are a lot of words that still exist in ancient
Chinese. In addition, the Chinese word segmentation methods are able to identify new
words according to statistical methods.
In order to simplify the segmentation procession, we make a simple test on the
segmentation of vernacular and ﬁnd, most words are segmented correctly by applying
the LTP directly. But there are still many mistakes; notional words are not distinguished
from others, idioms are segmented wrong, and there are other mistakes generated in the
algorithm. That is mainly because of the diﬀerence between vernacular and modern
Chinese. There are many words which are not used now, especially those appear only
several times in the text.
A simpler and more eﬃcient method we used to solve above questions is add a
manual dictionary to the LTP. The dictionary includes notional words and some idioms.
Notional words include place name, monster name, Tang priest and his three appren‐
tices’ name and nickname, gods’ name and their nickname, the particular items and some
words about the emperor and the dynasty.
• Place name. There are a lot of places made up by the author, such as the monster’s
cave, the god’s mountain, the monkey’s birthplace, and so on. Take these words into
dictionary will make sure they are segmented correctly.
• Monster’s name. Tang priest and his four apprentices meet many big monsters on
the Journey to the West, and each of them get a nickname, even some of small monster
under them get one, too. To split them correctly, we had better put them into the
dictionary.
• Four characters’ name and their nickname. Though only four people, each of them
have many nickname. Only Tang priest has more than 5 nicknames, for example,
priest, Tang priest, Xuanzang, elder, Tang elder, master, and so on. Especially they
often emerge in the dialogs. Therefore, split them from other words are important.
• Gods’ name and their nickname. When walking through the long way, the four meet
a lot of gods. Each of them have several nickname, especially Guanyin, his nicknames
even catch up with Tang priest.
The Personality Analysis of Characters in Vernacular Novels
403

• The particular items. There are many particular things in the Journey to the West,
such as kinds of weapons, treasures, and so on. Much of them are not usually used
in modesty Chinese.
• Some idioms. Idioms are ﬁxed phrases in the ancient Chinese.
• Some words about the emperor and the dynasty.
We select 1000 characters randomly from the segmentation results for each of
the four, five of our workmates check it respectively. When finished all, they vote
on the contradicted ones. Table 1 shows that the error rate is about 2%~3%, in other
words, the accurate rate is about 97%~98%. Though the punctuation is count into the
words, the error rate will not exceed 1.2 times of the existing data. As we can see,
this method is quite effective.
Table 1. The error rate of word segmentation.
Total
Wrong
Error Rate
Tang priest
850
27
0.0318
Sun Wukong
766
21
0.0274
Zhu Bajie
1048
25
0.0239
Sha monk
753
30
0.0398
In [12], five of six feature classes are properties of Weibo, so in our work, we
choose only the features of liwc. Due to the differences between ancient Chinese and
modern Chinese, liwc dictionary will have corresponding change, so will the features
selected based on liwc. To solve it, we get rid of the features which are not consis‐
tent with ancient Chinese. The rest features will serve as input of the model which
have been trained in [12]. In order to inspect the personality more intuitive, we adopt
a commonly used quantitative method in psychology—the big five personality traits
[11]. In the big five traits model, the user’s personality is abstracted into five dimen‐
sions, which are shown in Tables 2 and 3. The big five score of Tang priest and his
apprentice is shown in Table 4.
Table 2. The big ﬁve 1. Each of them has 6 facets.
Agreeableness
Conscientiousness
Extraversion
Trust
Competence
Warmth
Straightforwardness
Order
Gregariousness
Altruism
Dutifulness
Assertiveness
Compliance
Achivement striving
Activity
Modesty
Self-discipline
Excitement seeking
Tender mindedness
Deliberation
Positive emotion
404
Y. Yuan et al.

Table 3. The big ﬁve 2.
Openness to experience
Neuroticism
Fantasy
Anxiety
Aesthetics
Hostility
Feelings
Depression
Actions
Self-consciousness
Ideas
Impulsiveness
Values
Vulnerability to stress
Table 4. The big ﬁve of Tang priest and his three apprentice.
Agree.
Cons.
Extra.
Open.
Neur.
Tang priest
13.06
11.26
7.94
3.50
25.42
Sun Wukong
9.63
4.97
1.34
0.92
1.93
Zhu Bajie
9.25
5.02
18.54
15.08
18.27
Sha monk
15.24
6.54
14.15
1.10
26.31
4
Personality Analysis of Characters
4.1
Agreeableness
As an eminent monk from Tang dynasty, Tang priest is very kind and compassionate
[13]. On the Journey to the West, he tries to help others, though when he is in danger.
He is modest and subject to authority, such as emperor of Tang Dynasty, and all the
gods they meet on the Journey to the West. But sometimes he is egoistic, and often shifts
responsibility. His agreeableness is relatively high.
Sha monk is very careful and slavish since he was surrendered by Sun Wukong. He
has never been egoistic, doing his best to serve the master and help his brothers [14].
Tang priest and Sun Wukong all have deep trust on him. The agreeableness of him is
highest.
Sun Wukong is capable, but his master does not trust him. He has helped many
people, but that does not mean he is willing to sacriﬁce. If someone harms his interests,
he will not hesitate to teach him a lesson. Sun Wukong is also an arrogance role, never
understanding what modesty is. He is not gentle too. In conclude, his agreeableness is
lowest.
4.2
Conscientiousness
Tang priest has no ability to protect himself, and has no experience to deal with monsters.
He strictly abides by the doctrine, trying his best to protect it. And he has a strong will,
which makes him go to the west ﬁrmly to obtain Mahayana Buddhism [13]. All in all,
his conscientiousness is relatively high.
The Personality Analysis of Characters in Vernacular Novels
405

Though exiled from heaven just because knocked over a glass made of colored glaze,
Sha monk not only gets no angry, but also accepts the destiny to atone for his sin. In this
point, he is something like Tang priest [14]. Compared with his brothers, he seems plain,
but he is better than his brothers in human nature. So his conscientiousness is relatively
higher.
Sun Wukong is strongest among his brothers. He pursues the idea that the stronger
should hold the power, never yielding to authority [15]. Therefore, many gods serve him
as a servant. He tries his best to protect the master, not for any beneﬁts, but for returning
Tang priest’s salvation. He could not restrain his aggressive instincts, and that makes
Tang priest most unhappy. It is not strange that his conscientiousness is lower than his
brothers.
Zhu Bajie is similar with Sun Wukong in agreeableness and conscientiousness. There
is also diﬀerence: Zhu Bajie gets more score in tender mindedness, while Sun Wukong
gets more in altruism [16, 17]. Zhu Bajie obeys the laws. Sun Wukong is a king of
monkey before he follows Tang priest, so he has no knowledge of it. Thus in order, Zhu
Bajie gets more score. But he always declares to go back to Gao Village when he
encounters danger. Sun Wukong loves battle while Zhu Bajie loves women. In total,
they go head in head with each other in agreeableness and conscientiousness.
4.3
Extraversion
In the point of extraversion, Zhu Bajie gets ﬁrst without question. He is very lustful,
showing great enthusiasm for women. He loves to eat, too [18]. Each time when they
arrive to a new town, he is happiest because he can eat a lot. Zhu Bajie is outgoing
compared with other people.
Sha monk is upright and honest [19]. He talks little, but what he said is very useful.
When there is contradiction among the group, Sha monk is the one who tries to solve it.
Tang priest is kind and friendly when dealing with people, but not enthusiastic. He
prefers quiet than noisy. So his extraversion is low.
In Table 4, Sun Wukong is lowest in extraversion. Though his warmth is not as good
as his brothers, but the rest features should be better than others. The possible reason
may be the dictionary we use may not be so ﬁt with his dialogs.
4.4
Openness
Openness is an indicator of the level of intelligence. From Table 4 we can observe that
Zhu Bajie gets the highest score. Zhu Bajie is always being called “fool”, but that is not
the case [18]. He is fond of eating and sleeping, and good at ﬂattering in front of master,
so Tang priest trusts him very much. In general, he always makes the best decision for
himself.
As a leader of the group, Tang priest is well learned and behaved. But he is often
cheated by monsters and confused by Zhu Bajie, and getting rid of Sun Wukong several
times, who tries to protect him.
Sha monk actually is a servant of Tang priest. He is responsible for all triﬂes, but
never complaining about it [20].
406
Y. Yuan et al.

Sun Wukong is powerful and good at dealing with enemies, however, he is often
fooled by Zhu Bajie. So in fact, Zhu Bajie is the most intelligent person among the group.
4.5
Neuroticism
In the term of neuroticism, Tang priest and Sha monk are similar to each other. When
facing the danger, Tang priest is anxious and scared, and often bursts into tears [13].
Sha monk is puzzled, and ‘What should we do?’ is his pet phrase. Zhu Bajie helps Sun
Wukong a lot in ﬁghting, but once failed the ﬁrst thing he thinks of is escaping. Sun
Wukong never gives up, even if he was alone, he will ﬁghts until success.
5
The Personality Change of Sun Wukong
We also did a research on the personality change of Sun Wukong before and after the
three strikes of White Bone Demon with this model. The results are shown in Table 5.
Table 5. The big ﬁve of Sun Wukong before and after his beating the White Bone Demon for
three times.
Agree.
Cons.
Extra.
Open.
Neur.
Before
6.28
3.63
2.83
19.27
6.62
After
10.56
4.99
0.39
1.03
1.19
Beating the White Bone Demon for three times was a turning point of Sun Wukong.
The author explained the mind change of Sun Wukong by the words of Zhu Bajie.
Before that, Sun Wukong was very irritable, but under the constraint of Tang priest,
he changed gradually. He became no more impulsive at all. It is corresponding to the
decrease of neuroticism.
In the early stage, Sun Wukong despised the authority, refused to obey the discipline,
and showed a clear sense of rebellion. While later he was inﬂuenced by Tang priest, no
longer having a strong sense of resistance. As we can see in Table 5, his openness is
greatly changed before and after the three beats of White Bone Demon.
The agreeableness is higher after the three strikes. In the respect of getting along
with people, Sun Wukong changed obviously, especially to Bodhisattva, Buddha, and
some others who is venerable. He was more and more courteous and no longer as arro‐
gant as before.
In the early stage, the responsibility of Sun Wukong is not clear, he even tried killing
Tang priest and attacked the Bodhisattva at ﬁrst. However, he accepted his duty in the
late, becoming a qualiﬁed defender.
6
Conclusion
In this paper, we make a automatic personality analysis of characters in the Journey to
the West using LIWC, and map the feature vector into the big ﬁve to make it easy to
The Personality Analysis of Characters in Vernacular Novels
407

observe. We compare the results with many famous reviews, and compare the results
between characters, and compare the results back and forward. These comparisons all
show that automatic personality analysis of characters with LIWC is feasible.
References
1. Tausczik, Y.R., Pennebaker, J.W.: The psychological meaning of words: LIWC and
computerized text analysis methods. J. Lang. Soc. Psychol. 29, 24–54 (2010)
2. Fernándezcabana, M., Garcíacaballero, A., Alvespérez, M.T., Garcíagarcía, M.J., Mateos, R.:
Suicidal traits in Marilyn Monroe’s fragments: an LIWC analysis. Crisis 34, 124–130 (2013)
3. López-López, E., del Pilar Salas-Zárate, M., Almela, Á., Rodríguez-García, M.Á., Valencia-
García, R., Alor-Hernández, G.: LIWC-based sentiment analysis in Spanish product reviews.
In: Omatu, S., Bersini, H., Corchado, J.M., Rodríguez, S., Pawlewski, P., Bucciarelli, E. (eds.)
Distributed Computing and Artiﬁcial Intelligence, 11th International Conference. AISC, vol.
290, pp. 379–386. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-07593-8_44
4. Markovikj, D., Gievska, S., Kosinski, M., Stillwell, D.J.: Mining Facebook data for predictive
personality modeling. In: AAAI International Conference on Weblogs and Social Media
(2013)
5. Robinson, R.L., Navea, R., Ickes, W.: Predicting ﬁnal course performance from students’
written self-introductions: a LIWC analysis. J. Lang. Soc. Psychol. 32, 469–479 (2013)
6. Gao, R., Hao, B., Li, H., Gao, Y., Zhu, T.: Developing simpliﬁed Chinese psychological
linguistic analysis dictionary for microblog. In: Imamura, K., Usui, S., Shirao, T., Kasamatsu,
T., Schwabe, L., Zhong, N. (eds.) BHI 2013. LNCS (LNAI), vol. 8211, pp. 359–368. Springer,
Cham (2013). https://doi.org/10.1007/978-3-319-02753-1_36
7. Zhao, N., Jiao, D.D., Bai, S.T., Zhu, T.S.: Evaluating the Validity of Simpliﬁed Chinese
Version of LIWC in Detecting Psychological Expressions in Short Texts on Social Network
Services, vol. 11, p. e0157947 (2016)
8. Pennebaker, J.W., Mehl, M.R., Niederhoﬀer, K.G.: Psychological aspects of natural language
use: our words, our selves. Annu. Rev. Psychol. 54, 547–577 (2003)
9. Zeng, Y., Hou, H.: Research on the extraction of ancient texts. J. China Soc. Sci. Tech. Inf.
30, 132–135 (2008)
10. Gao, R., Hao, B.B., Li, L., Bai, S., Zhu, T.S.: The establishment of the software system of
Chinese language psychological analysis. In: Psychology and the Promotion of Innovation
Ability: the Sixteenth National Conference on Psychology, p. 3 (2013)
11. de Raad, B.: The Big Five Personality Factors: The psycholexical Approach to Personality,
pp. 309–311. Hogrefe & Huber Publishers, Ashland (2000)
12. Gao, R.: The Research and Application of the Psychological Analysis Technology of Weibo
Content, University of the Chinese Academy of Sciences, vol. Master. University of the
Chinese Academy of Sciences (2014)
13. Cao, B.J.: The reﬂection and critique of pure confucianism personality: a new comment of
Tang priest. Acad. J. Zhongzhou 4, 110–112 (1999)
14. Jiang, K., Huang., L.J.: Compromise and secularization: an image evolution of Sha monk. J.
Zibo Normal Coll. 4, 58–62 (2007)
15. Zhou, X.S.: The time spirit and cultural connotation of Sun Wukong. J. Southeast Univ. 8,
63–73 (2006). (Philosophy and Social Science)
16. Cao, B.: Secularized Comic Image and Civil Personae 1: A New Reading of Ba Jie in A
Journey to the West. J. Huaihai Inst. Technol. 5, 23–27 (2007). (Social Science Edition)
408
Y. Yuan et al.

17. Cao, B.: Secularized Comic Image and Civil Personae 2: A New Reading of Ba Jie in A
Journey to the West. J. Huaihai Inst. Technol. 5, 27–30 (2007). (Social Science Edition)
18. Liu, W.L., Qiu, H.C.: The Empiricism of Zhu Bajie. J. Keshan Teachers Coll. 2, 42–46 (2002)
19. Cai, S.: Discussion on Sha monk in “Journey to the West”. J. Educ. Inst. Jilin Province 30,
114–115 (2014)
20. Lu, L.: Industrious, duty, ﬁrm and persistent: a comment of Sha monk. South J. 11, 98–99,
110 (2014)
The Personality Analysis of Characters in Vernacular Novels
409

Digging Deep Inside: An Extended Analysis
of SCHOLAT E-Learning Data
Aftab Akram1,2, Chengzhou Fu1(✉), Yong Tang1, Yuncheng Jiang1, and Kun Guo3,4
1 School of Computer Science, South China Normal University, Guangzhou 510631,
Guangdong Province, China
aftabit39@gmail.com, fucz@m.scnu.edu.cn,
{ytang,ycjiang}@scnu.edu.cn
2 University of Education, Lahore 54000, Pakistan
3 College of Mathematics and Computer Sciences, Fuzhou University, Fuzhou 350116,
Fujian Province, China
gukn@fzu.edu.cn
4 Fujian Provincial Key Laboratory of Network Computing and Intelligent
Information Processing, Fuzhou University, Fuzhou 350116, Fujian Province, China
Abstract. More and more higher education institutions are adopting computer
based learning management system to boost learning of the students. Networking
and collaboration through social media platforms are vital realities. Learning is
not merely limited to class rooms, which is now independent of location and time.
Understanding how students learn in this realm is a mighty challenge for teaching
professionals. Fortunately, data is abundantly available through learning manage‐
ment systems and social media platforms. Analyzing this vast data could give an
insight into how learning is happening in these days. Data mining techniques are
vastly being used for this purpose. In this paper, we present a statistical analysis
of e-leaning data obtained from SCHOLAT, a scholar oriented social networking
system. The analysis aims at getting data oriented perspectives of learning, e.g.,
which factor to what extent impacts learning. The analysis revealed factors which
positively or negatively aﬀect learning achievement of the students, i.e., course
ﬁnal scores.
Keywords: Data mining · Academic social network
Learning management system · Statistical analysis
1
Introduction
Self-motivated, self-directed, self-paced and collaborative knowledge construction, this
is how learning can be described now and in coming future. Learning is not limited to
class rooms and to individuals. In fact, apart from a fraction, most of learning takes place
outside of class room settings and through collaboration [1]. Realizing this need, the
modern e-learning systems are built around ‘knowledge construction by collaboration’
principle. These e-learning systems include features like wikis, forums, discussion
boards, podcasts, etc., to facilitate collaborative and interactive ‘free’ learning.
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 410–421, 2018.
https://doi.org/10.1007/978-3-319-74521-3_44

Students learn more by exchanging knowledge with each other, participating in
meaningful interactions and learning discourses [2]. Those who are engaged in collab‐
orative and interactive learning activities, are more productive in terms of learning
achievements [3]. Engaging students in meaningful and productive social interactions
is a major concern of modern day course instructors, and the vertical social networking
platforms emerged to respond. These kinds of social networking systems are built to
meet demand of social interactions for a speciﬁc group of users with similar needs, for
example learners and scholars. Together with the appropriate e-learning technology, this
system can fulﬁll the demand of future learning.
The students who involve in on-line discussions and interactions can easily be
distracted [4]. This is not what the course instructor has desired, nor may the learners
themselves have known this. The course instructor should decide which dimension of
learning process is most appropriate to be supported by social interactions [5]. This is
not likely to happen until instructors analyze the course very closely. For a course
supplemented by collaborative activities, the breach between instructor’s intentions and
students’ response should be well known. By making this sure, is what guarantees a
sound learning process.
The e-learning and social networking systems gather vast data as a result of users’
interactions with the system, [6] termed it as gold mine of educational data. The data
mining techniques can be applied to this data to get an insight into the learning process.
Unlike, traditional face to face learning settings, where data is scarce and available only
at the end of session, the data gathered through e-learning and social networking systems
is abundant and is available beforehand. Therefore, course instructor can get useful
knowledge about the current learning process. The emergence of educational data
mining and learning analytics ﬁelds is response to the growing demand of analyzing the
vast data produced in e-learning and social networking systems.
SCHOLAT is a vertical social networking platform built speciﬁcally for learners and
scholars. SCHOALT through its course module also facilitates building blended learning
environment to promote ‘beyond classroom interactions’ between learners and course
instructors. In this paper, we present an analysis of data retrieved from SCHOLAT-
course module. We analyze this data to get an insight of the learning process and to
know how the students behave. The analysis will help us understand the various
dynamics of learning process and in future, will lay foundation for building more
sophisticated algorithms to understand learning in a ‘blended social collaborative’
learning environment.
The paper is organized in the following way: Sect. 2 describes a brief review of
related work, Sect. 3 provides an introduction of SCHOLAT, Sect. 4 presents data and
methods used in this analysis, results of analysis are presented in Sect. 4, the results are
discussed in Sect. 5 and ﬁnally Sects. 6 and 7 present future work and conclusion
respectively.
Digging Deep Inside: An Extended Analysis of SCHOLAT E-Learning Data
411

2
Literature Review
Numerous studies have been conducted to demonstrate the eﬀectiveness of collaborative
and interactive learning. In essence, these studies emphasized the very fact that future
learning is only by cooperative and collaborative means. The presence of modern age
social media networks and their popularity highly signiﬁes this fact. Although,
researchers have recognized it quite a while ago, but now it is indispensable. In this brief
review, we present some of the important studies.
Social constructivist is a learning paradigm advocating social and collaborative
learning. This learning paradigm emphasizes on learner-to-learner interactions, knowl‐
edge co-construction and sharing of contents [7, 8]. According to this theory, learning
process resides in social interactions. And foremost learning activities are group work
and collaboration.
The collaborative activities, in fact, are supplement face to face teaching [9]. The
computer based tools help to vanish spatial and temporal limitations, therefore
expanding the scope of cooperation beyond classrooms to make anywhere and anytime
learning possible [10]. Yu et al. asserted that collaborative learning helps in achieving
desirable learning outcomes [11]. Another study maintained that more collaborative
students achieve more in terms of their learning outcomes [12].
It is the instructor who decides the scope and limits of social interactions and collab‐
oration. The learners are facilitated by engaging beyond classroom activities and avail‐
ability of course related material outside classrooms [13]. A careful design and conduct
of learning activities ensure a sound learning process. The instructor can analyze vast
data generated by students’ use of computer based learning system to have a close look
at the learning process.
Data mining or knowledge discovery in databases (KDD) is set of techniques applied
to extract implicit and interesting pattern from large collection of data [14]. Most
observed data mining method are statistics, visualization, clustering, classiﬁcation,
association rule mining, sequential pattern mining, etc. [15]. Whereas, educational data
mining (EDM) is the application of data mining techniques to a speciﬁc dataset coming
from education environment to address important educational questions [21].
Data mining techniques have been used to improve e-learning process [16]. There
are various uses of data mining of educational data, for example, to explore, analyze and
visualize data, to ﬁnd out useful patterns, to discover how students learn, etc. [17–19].
Other studies indicate some more uses of data mining, for example, recommending
activities for students, getting feedback about learning process, evaluating course struc‐
ture, classifying learners according to their learning needs, discovering regular and
irregular patterns, adoption of computer based learning systems to user need, etc. [15].
In a previous study [20], a descriptive statistical and correlation analysis of
SCHOLAT e-learning data was presented. The analysis disclosed many interesting facts
about students’ behavior in online learning management system. The analysis revealed
higher descriptive statistics values of variables representing activities expecting to
contribute towards ﬁnal scores and moderate weak correlation between them. Lower
descriptive statistical value and weak correlation with ﬁnal scores was observed in case
412
A. Akram et al.

of other variables. The distributional shape were skewed and outliers were present in
the data.
However, previous analysis was carried out on a single class and only four variables
were used (3 independent and 1 dependent). In this paper, we present an extended and
diverse analysis of SCHOLAT e-learning data. We include four class in this analysis to
study behavior across diﬀerent students’ cohorts. Further, we use 11 variable (10 inde‐
pendent and 1 depended). These variables represent administrative and collaborative
use of learning management system. By use of statistical and correlation analysis tech‐
niques, the analysis reveals common trends of system usage among diﬀerent groups of
students. We use course ﬁnal scores as criteria of successful learning. We plan to use
this analysis as base work for developing machine learning models representing
students’ learning in blended social learning environment.
3
SCHOLAT-Past and Future
SCHOLAT is an emerging vertical social networking system designed and built specif‐
ically for scholars, learners and course instructors. It uses un-directed graphs to represent
social network structure. It is bi-lingual, i.e., supports English and Chinese. The main
goal of SCHOLAT is to enhance collaboration and social interactions focused around
scholarly and learning discourses among community of scholars. In addition to social
networking capabilities, SCHOLAT incorporates various modules to encourage collab‐
orative and interactive discussions, for example, chat, email, events, news post, etc.
Table 1 shows a comparison of SCHOLAT with other similar social networking system.
SCHOLAT deﬁnitely has an advantage over them.
Table 1. Comparison of SCHOLAT and other scholar networking systems
Function
SCHOLAT
ResearchGate
SocialScholar
eScience
Scholarmate
Data space
Yes
Yes
Yes
Yes
Yes
Academic
search
Yes
Yes
Yes
Yes
Yes
Team
Yes
Yes
Yes
Yes
Yes
Course
Yes
No
No
No
No
Email
Yes
No
No
No
No
Chat
Yes
No
No
No
No
Course module is a distinctive feature in SCHOLAT. The course module is built
around blended social learning concept which provides its users, e.g., instructors and
students, an opportunity to indulge in collaborative and interactive activities beyond
class room settings. It mainly supports administrative and collaborative activities. For
example, instructors can create online courses and classes, present course introduction
and teaching plans on course website, make announcements, assign tasks to students
like homework, upload course help materials, interact with students via online question
answer, etc. The students can get up to date information about course in progress, down‐
load course help materials, submit homework, ask questions online, etc. At the time this
Digging Deep Inside: An Extended Analysis of SCHOLAT E-Learning Data
413

study is being conducted, 32758 students have been enrolled in 1785 classes of 760
courses.
SCHOLAT-course is still developing. Being a part of SCHOLAT scholar social
networking system opens vast horizons for its future development. We plan to work in
dimensions of social collaborative learning. We plan to study the factors aﬀecting the
learning in collaborative social learning environment. This study is start of a journey
towards this destination.
In this study, we explore the statistically supported answers of four research ques‐
tions: (1) what is the general pattern of system usage among four classes, (2) course
administration and collaboration, what type of course support is mostly used by students,
(3) is the data is normally distributed or not?, (4) which variable(s) is (are) most corre‐
lated with the course ﬁnal scores.
Table 2. Variables in previous and current studies
Variables
Description
Student level data independent variables
Number of loginsa,b
The number of times a student has logged into
the system
Number of online questions askeda,c
The number of questions asked by a student on-
line
Number of on-line replyc
The number of replies to all questions posted
by a student
Number of homework submitted on timea,d
The number of homeworks submitted within
deadline
Number of homework submitted lated
The number of homeworks submitted after
deadline
Number of homework not hand ind
The number of due homeworks not submitted
by a student
Course level data independent variables
Total number of homeworkd
The number of homework assigned by
instructor
Number of hits course noticed
The number of times course notices were
opened
Number of downloads course resourcesd
The number of times course materials were
dowloaded by students
Number of commentsc
The number of comments posted. The
comments can be posted by students or other
users
Dependent variable
Course ﬁnal scorea
The score obtained by student in a formative
assesment written test
aVariables included in previous study [20].
bVariable for system Use.
cVariable for collaboration.
dVariable for course administration.
414
A. Akram et al.

4
Data and Method
In previous study [20], the analysis was performed on one class. In present study, we
include four classes. We also increase number of variables from four to eleven. These
four classes were selected from three diﬀerent courses, course ID165 (C Language
Programming) class ID279 taught in autumn 2014, course ID520 (Introduction to
Learning Sciences) class ID1149 taught in autumn 2015, and course ID206 (Software
Requirement) classes ID1563 and ID1729 taught in spring 2016. The number of students
enrolled in four classes was 216 out of which 188 students passed the course or took the
ﬁnal exam.
Table 2 shows the variable included in previous and current studies. All of these
variables are quantitative values. The variables are categorized in to two types: student
level data for which observations for each student was available, and course level data
for which observations was only available at course level. Ten out of eleven variables
are independent variable, one variable (course ﬁnal score) is dependent variables. These
variables were extracted from course records stored in SCHOLAT database. The course
ﬁnal scores were not present in the database, so the relevant course instructor was asked
to provide. The main cause of selection of these four classes was availability of course
ﬁnal scores.
Earlier, we used two types of descriptive statistical techniques: univariate and
bivariate [20]. The univariate technique was used to uncover the properties of individual
variables, whereas bivariate analysis was used to discover relationship between inde‐
pendent and dependent variables. We intend to use same statistical techniques in this
study. This study has an added advantage that results will be compared among four
diﬀerent classes.
The results of the analysis are presented in following three subsections.
4.1
Univariate Analysis
We perform univariate analysis on student level variables and dependent variable
(course ﬁnal score) as discussed in previous section. Therefore, total seven variables are
included in this episode of analysis. The results of analysis are shown in Table 3. Five
descriptive statistics measures have been calculated for each variable in each class.
These measures include average or mean, mode, median, standard deviation and skew‐
ness. Table 3 is divided into ﬁve compartments, where each compartment presenting
results for one type of descriptive statistical measure. The top row indicates the variables
included in the analysis. The left most column speciﬁes the class to which the variable
corresponds to. The top row is not repeated, however, left most column is repeated in
each sub section of the table. Each column (next to left most column) in the table corre‐
sponds to results of one variable, for example, column under number of logins presents
results of analysis for this variable only. So if reader wants to know what is mode of
number of homework submitted on time for class ID279, choose column ﬁrst for this
variable and then go to corresponding sub section showing mode of each variable. In
this sub section, choose the row indicating the desired class, the intersection of that
column and this row the required answer, i.e., 105.
Digging Deep Inside: An Extended Analysis of SCHOLAT E-Learning Data
415

Table 3. Descriptive statistics univariate mode
No. of
logins
No. of
question
No. of
reply
No. of
homework
on time
No. of
homework
late
No. of
homework
not hand-in
Final
score
Mean
ID279
213.34
7.94
6.38
89.65
0.03
2.96
86.94
ID1149
22.32
0.45
0.45
2.29
1.84
0.26
73.77
ID1563
664.53
0
0
5.32
3.32
1.17
82.20
ID1729
68.43
0
0
6.86
2.43
1.08
84.27
Mode
ID279
0
0
0
105
0
0
90
ID1149
16
0
0
2
1
0
73
ID1563
3
0
0
4
0
0
90
ID1729
39
0
0
9
1
0
90
Median
ID279
0
2
2
100
0
2
90
ID1149
19
0
0
2
2
0
73
ID1563
47
0
0
4
3
0
86
ID1729
41
0
0
8
1
0
86
Standard Deviation
ID279
1667.57
13.50
21.56
28.93
0.16
2.98
11.57
ID1149
10.59
1.41
1.34
1.13
1.21
0.44
6.50
ID1563
2695.07
0.00
0.00
3.78
3.13
3.15
10.29
ID1729
168.10
0.00
0.00
3.83
2.69
2.85
8.90
Skewness
ID279
8.88
3.15
7.66
−2.68
6.16
0.74
−5.60
ID1149
0.58
3.38
3.01
1.45
1.64
1.16
−0.91
ID1563
5.92
0
0
0.18
0.97
3.12
−0.76
ID1729
6.00
0
0
−0.21
1.14
3.13
−0.95
4.2
Bivariate Analysis
Table 4 shows the results of bivariate analysis performed on each of aforementioned
variables. We calculated correlation co-eﬃcient for each pair of one of the six inde‐
pendent student level variables and dependent variable. The structure of Table 4 is
similar to Table 3 described in previous subsection with top row describing the variables
and left most columns the relevant class. The right most columns under ﬁnal score
contain all ones since it indicates self-correlation.
416
A. Akram et al.

Table 4. Descriptive statistics bivariate mode
No. of
logins
No. of
question
No. of
reply
No. of
homework
on time
No. of
homework
late
No. of
homework
not hand-in
Final
score
Correlation Co-eﬃcient (r)
ID279
0.12
0.16
0.16
−0.12
−0.06
−0.35a
1.00
ID1149
0.44a
0.10
0.22
0.03
−0.39a
−0.08
1.00
ID1563
−0.25
0
0
0.41a
−0.06
−0.50a
1.00
ID1729
0.16
0
0
0.42a
−0.50a
−0.19
1.00
aSigniﬁcant at p-value < 0.05, 95% conﬁdence interval.
4.3
Course Level Variables
Table 5 shows the obtained ﬁgures for course level variables. We did not perform any
calculations on these variables like we did on student level variables, since these ﬁgures
are available only at course level. However, we present a discussion on its implications
in next section. The class ID1563 and ID1729 belong to same course, so in Table 5 the
ﬁgures for these two classes are presented jointly.
Table 5. Course level variables
ID279
ID1149
ID1563/ID1729
Total number of homework
105
8
13
Number of hits course notice
6041
553
54
Number of download course resources
5969
9375
2176
Number of comments
30
3
0
5
Discussion
In this section, we present discussion on results presented in previous section. The
discussion is focused on ﬁnding the data supported answers of research questions
presented in Sect. 3.
Table 3 presents the results of univariate analysis. Five types of statistical measures
have been presented. We discuss each of them in following text.
The number of logins shows the extent of system use by the students. The mean,
mode and median values given in Table 3 show diﬀerent usage behavior shown by four
classes. The classes ID279 and ID1563 have high average values, i.e., 213.34 and
664.53, whereas classes ID1149 and ID1729 show low usage average, i.e., 22.32 and
68.43. Mode is value mostly appearing in data [22], the classes ID1149 and ID1729 have
mode values, i.e., 16 and 39, the class ID1563 has mode 3 and class ID279 has mode 0.
Median is the value appearing in the center of score continuum. If the distribution is not
normal, median gives a decent idea where is the center of data. The median of class
ID279 is 0 (range: 0−14841) indicates highly unbalanced use of system, i.e., some
Digging Deep Inside: An Extended Analysis of SCHOLAT E-Learning Data
417

students using system not at all while some other students usage is very high. A similar
trend is observed in class ID1563, i.e., median 47 (range: 0−17053). Earlier high average
values for these two classes were observed. The other two classes show relatively
balanced usage, i.e., median is 19 (range: 5−46) for class ID1149 and median is 41
(range: 11−1059) for class ID1729. From above discussion it can concluded that classes
ID279 and ID1563 show high and unbalanced usage while classes ID1149 and ID1729
show low but balanced usage. We also note presence of outliers in classes ID279 and
ID1563. The high values of standard deviation also signiﬁes presence of varied usage
behavior. The classes ID279, ID1563 and ID1729 shows high variability in system
usage, i.e., 1667.57, 2695.07 and 168.10. The class ID1149 has relatively consistent
usage pattern and low value for standard deviation, i.e., 10.59.
The variables number of question and number of reply are indication of level of
collaboration, since asking questions and getting replies promotes collaboration and
interaction among students. The descriptive statistics shown in Table 3 indicates that
these activities are highly overlooked by students. The mean, mode, median and standard
deviation values for these two variables for classes ID1563 and ID1729 are zero. For
other two classes these values are very low. The data indicates very low collaboration
among the students.
The SCHOLAT course module has comprehensive facility for instructors to upload
homework and for students to submit homework. Submitting homework on time is
indication of good behavior whereas late submission or not submitting at all indicates
procrastinating behavior which is alarming for students’ learning. The three variables
number of homework on-time, number of homework late and number of homework not
hand-in are related to this key aspect of course administration. The statistics provided
in Table 3 rather provide a satisfactory view, as mean, mode and median values for all
classes indicates a healthy trend of submitting assignments on-time and to avoid procras‐
tination. For class ID279 the mean value is 85.4% of maximum values of 105. Whereas
for other classes ID1149, ID1563 and ID1729 the mean values are 28.6%, 41% and
52.3%, although lower than class ID279 but still satisfactory. The standard deviation
values for all four classes are not high indicating consistent behavior of submitting
homework timely. The mean, mode and median values for variables number of home‐
work late and number of homework not hand-in indicate that the students tend not to
procrastinate because it can lower their ﬁnal scores. In Table 3, mostly zero and low
values for mean, mode and median are observed for these variables. Further, low
standard deviation values indicate that avoiding procrastination is universal trend among
all four classes.
The ﬁnal score variable is indication of successful learning. The scores have
maximum value of 100. The mean, mode and median values in Table 3 indicate that
despite all odds students manage to get good scores. The low standard deviation speaks
of healthy trend of getting good scores after all among all classes.
However, the distribution of data is not normal and we mostly see skewed distribu‐
tions. The skewness is measure of how much a frequency distribution is asymmetric
[23]. A normal distribution has skewness values to zero, whereas positive or negative
non zero values indicates a positively or negatively skewed distribution. Table 3 indi‐
cates that most distribution are positively or negatively skewed. There are also outliers
418
A. Akram et al.

present in data which make distribution further skewed. The skewed distribution speaks
of extreme behavior by students, i.e., against a normal distribution in which an average
behavior is mostly seen.
Table 4 shows the results of bivariate correlation coeﬃcient analysis. The analysis
was done to ﬁnd out the strength of relation between each of independent variables in
each course and the ﬁnal scores. The values of correlation coeﬃcient r illustrates the
strength of positive or negative relationship, with two extreme values +1 or −1 (strong
positive or negative relation) and value of 0 for no relation at all [24]. The variables have
weak to moderate relationships. But very few signiﬁcant correlations (p-value < 0.05)
are observed. The course ID279 has only one signiﬁcant correlation, i.e., number of
homework not hand is negatively correlated with ﬁnal scores (r = −0.35). In course
ID1149, the number of logins has positive signiﬁcant correlation with ﬁnal scores
(r = 0.44) and number of homework late has negative signiﬁcant correlation (r = −0.39).
In courses ID1563, the number of homework on time has positive moderate signiﬁcant
correlation with ﬁnal scores (r = 0.41) and number of homework not hand-in has nega‐
tive moderate signiﬁcant correlation (r = −0.50). Finally, in course ID1729 the number
of homework on-time has positive moderate signiﬁcant correlation with ﬁnal scores
(r = 0.42) and number of homework late has negative moderate signiﬁcant correlation
(r = −0.50).
Next we examine the course level data. This is data either same for entire group or
ﬁgures were not available for individual students. For example, total number of home‐
work is same for all students, i.e., 105, 8 and 13 for classes ID279, ID1149 and ID1563/
ID1729 respectively. For other three variables, number of hits course notice, number of
downloads course resources and number of comments, the ﬁgures were not available
for individual students. However, these ﬁgures are helpful to ﬁnd out the extent of use
of beyond class room on-line services. Since, the ﬁgures are not available for each
student, we cannot use them to correlate to student’s achievement.
First, we look at variable number of hits course notice. The ﬁgures of this variable
for classes under study, i.e., ID279, ID1149 and ID1563/ID1729 are 6041, 553 and 54,
where number of notices issued by course instructors were 30, 18 and 1 respectively.
As such, we see more activity in class ID279 reading and staying in touch with course
instructor. Similarly, we can view number of downloads of course resources as another
indicator of on-line interactive activities. Table 5 indicates that in class ID279 students
downloaded course resources 5969 times, in ID1149 9375 times, and in ID1563/ID1729
2176 times, whereas 61 resources were uploaded in ID279, 63 in ID1149 and only 18
resources were uploaded in classes ID1563/ID1729. We see rather healthy trend that the
students are beneﬁted from this facility eﬀectively, which is indicative by the number
of downloads in each class.
We observe a disappointing activity in terms of comments posted by students. For
our classes, there are only 30 comments for ID279, 3 comments for ID1149 and no
comment for other two classes combine. These comments can be useful for course
instructors and other students willing to join the course.
Digging Deep Inside: An Extended Analysis of SCHOLAT E-Learning Data
419

6
Conclusion
The important ﬁndings of the above analysis can be summarized as follows:
• There is varied trend of system usage, i.e., in some classes students use system more
and in some do not.
• Students do not take much interest in on-line collaboration activities.
• Students try their best to submit their homework and avoid procrastination.
• In each class there are diﬀerent factors which are signiﬁcantly related to students’
learning success (course ﬁnal scores). We see both positive and negative correlations.
• The distributions are skewed, which shows above or below normal behavior.
7
Future Work
In future, we intend to extend this work for building machine learning algorithm to
predict students’ current learning and future achievement. This would enable course
instructors to take timely actions to avoid students’ failures. We also intend to increase
data collection ability of the system.
Acknowledgment. This work is supported by the National Nature Science Foundation of China
(Grant Nos. 61272066, 61272067, 61300104), the Applied Technology Research and
Development Foundation of Guangdong Province (2016B010124008), the Technology
Innovation Platform Project of Fujian Province (Grant Nos. 2009J1007, 2014H2005), the Fujian
Collaborative Innovation Center for Big Data Applications in Governments.
References
1. Chen, B., Breyer, T.: Investigating instructional strategies for using social media in formal
and informal learning. Int. Rev. Res. Open Distance Learn. 13(1), 87–104 (2012)
2. Hrastinski, S.: A theory of on-line learning as on-line participation. Comput. Educ. 52, 78–
82 (2009)
3. Junco, R., Helbergert, G., Loken, E.: The eﬀect of Twitter on college student engagement
and grades. J. Comput. Assist. Learn. 27, 119–132 (2011)
4. Hurt, N.E., Moss, G.S., Bradley, C.L., Larson, L.R., Lovelace, M.D., Prevost, L.B., et al.:
The ‘Facebook’ eﬀect: college students’ perceptions of online discussions in the age of social
networking. Int. J. Sch. Teach. Learn. 6(2), 2–14 (2012)
5. Fewkes, M., McCabe, M.: Facebook: learning tool or distraction? J. Digital Learn. Teacher
Educ. 28(3), 92–98 (2012)
6. Mostow, J., Beck, J., Cen, H., Cuneo, A., Gouvea, E., Heiner, C.: An educational data mining
tool to browse tutor-student interactions: Time will tell!. In: Proceedings of the Workshop on
Educational Data Mining, pp. 15–22, Pittsburgh, USA (2005)
7. Dewey, J.: Experience and education, New York: Collier, 1938/1963
8. Vygotsky, L.S.: Mind in society: the development of higher psychological processes. Harvard
University Press, Cambridge Mass (1978)
9. Cole, J.: Using Moodle. O’Reilly, Sebastopol (2005)
420
A. Akram et al.

10. Dawson, S.: A study of the relationship between student social networks and sense of
community. Educ. Technol. Soc. 11(3), 224–238 (2008)
11. Yu, A.Y., Tian, S.W., Vogel, D., Kwok, R.C.: Can learning be virtually boosted? an
investigation of on-line social networking impacts. Comput. Educ. 55, 1494–1503 (2010)
12. Junco, R.: Too much face and not enough book: the relationship between multiple indices of
Facebook use and academic performance. Comput. Hum. Behav. 28, 187–198 (2012)
13. Lamb, L., Johnson, L.: Bring back the joy: creative teaching, learning, and librarian-ship.
Teach. Librarian 38(2), 61–66 (2010)
14. Klosgen, W., Zyttow, J.: Handbook of Data Mining And Knowledge Discovery. Oxford
University Press, New York (2002)
15. Romero, C., Ventura, S., Garcia, E.: Data mining in course management systems: moodle
case study and tutorial. Comput. Educ. 51(1), 368–384 (2008)
16. Romero, C., Ventura, S.: Data Mining in E-Learning, Southampton. Wit Press, UK (2006)
17. Mazza, R., Milani, C.: Exploring usage analysis in learning systems: gaining insights from
visualization. In: Workshop on Usage Analysis in Learning Systems at 12th International
Conference on Artiﬁcial Intelligence in Education, pp. 1–6, New York, USA (2005)
18. Mor, E., Minguillon, J.: E-learning personalization based on itineraries and long term
navigational behavior. In: Proceedings of the 13th International World Wide Web
Conference, pp. 264–265 (2004)
19. Talavera, L., Gaudioso, E.: Mining students data to characterize similar behavior groups in
unstructured collaboration spaces. In: Workshop on Artiﬁcial Intelligence in CSCL, pp. 17–
23, Valencia, Spain (2004)
20. Akram, A., Chengzhou, F., Yong, T., Yuncheng, J., Xueqin, L.: Exposing the hidden to the
eyes: analysis of SCHOLAT E-learning data. In: Proceedings of the 2016 IEEE 20th
International Conference on Computer Supported Cooperative Work in Design, pp. 693–698
(2016)
21. Romero, C., Ventura, S.: Data mining in education. WIREs Data Min. Knowl. Disc. 3, 12–
27 (2013). https://doi.org/10.1002/widm.1075
22. Huck, S.W.: Reading Statistics and Research, vol. 2(28). Pearson Education, Boston (2012)
23. Huck, S.W.: Reading Statistics and Research, vol. 2(24). Pearson Education, Boston (2012)
24. Huck, S.W.: Reading Statistics and Research, vol. 3(49). Pearson Education, Boston (2012)
Digging Deep Inside: An Extended Analysis of SCHOLAT E-Learning Data
421

Social Network Analysis of China Computer Federation
Co-author Network
Chengzhou Fu1, Weiquan Zeng1,2, Rui Ding1, Chengjie Mao1(✉),
Chaobo He3, and Guohua Chen1
1 Computer Science, South China Normal University, Guangzhou, China
{fucz,zengwq,ding,chengh}@m.scnu.edu.cn, maochj@qq.com
2 Computer Science, Clemson University, Clemson, USA
3 School of Information Science and Technology,
Zhongkai University of Agriculture and Engineering, Guangzhou, China
hechaobo@foxmail.com
Abstract. We extract the data set from 2010 to 2014 of the China Computer
Federation (CCF (http://www.ccf.org.cn)) by the distributed web crawler system
and build the co-author network with R language. In this co-author network,
authors represent nodes and a pair of authors is connected by an edge if they have
co-authored at least one paper over the entire duration. We analyze this network
with social-centric and ego-centric methods to study the situation of co-author
network of CCF and visualize the analysis results. Social-centric measure reveals
that the co-author network density, the usage of key words, the average number
of authors per article, and conﬁrms that most authors in computer science ﬁeld
publish a very small number of papers but have higher collaborators than those
of other ﬁelds. Ego-centric analysis discovers betweenness centrality, closeness
centrality, and degree centrality, indicates that only a small percentage of the
authors locate in the center of the coauthor network. Finally, we pick out eight
key persons and point out the group teams where the key persons are respectively.
Based on these ﬁndings, we suggest that computer science ﬁeld should promote
wider collaboration, encourage more authors to publish their papers.
Keywords: Co-author network · Social-centric · Ego-centric · Centrality
Key person · Visualize
1
Introduction and Motivation
CCF aims at bringing scholars together in computer research. CCF was founded in 1962,
member of China Association of Science and Technology. It contains 13 kinds of journals,
namely Chinese Journal of Computers, Journal of Computer-Aided Design & Computer
Graphics, Journal of Computer Science & Technology, Journal of Computer Research and
Development, Journal of Software, Journal of Computer Applications, Computer Engi‐
neering and Applications, Computer Technology and Development, Computer Science,
Journal of Chinese Computer Systems, Journal of Frontiers of Computer Science and Tech‐
nology, Computer Engineering and Science, and Computer Engineering and Design.
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 422–432, 2018.
https://doi.org/10.1007/978-3-319-74521-3_45

Social network analysis (SNA) methods have been used to study co-author network
in various ﬁelds including physics, library and information service (LIS), biology, and
computer science. Scholars in LIS ﬁeld need strengthen communication with each other
after studying the co-author network of LIS with betweenness centrality, degree
centrality, and average degree features [1]. Zhu et al. [2] discovered the present situation
and pattern of Chinese-foreign cooperation, simultaneously oﬀered suggestions to the
international collaboration through studying the co-author network of information
system ﬁeld. Sun et al. [3] ﬁgured out the most inﬂuential author and paper in the co-
author network with network density, betweenness centrality methods. Shen et al. [4]
applied the vector space model into the identiﬁcation of scientiﬁc research teams within
the co-author network, and revealed the scientiﬁc research co-author relationship by
degree of similarity between author vectors. Du [5] extracted the co-author network over
the past 26 years of USIT (User Interface Software and Technology), which is generally
recognized as the top conferences in the Human-Computer Interaction ﬁeld, and
analyzed the co-author network with SNA methods. El Kharboutly and Gokhale [6]
revealed the collaborative pattern of co-author from the co-author network that extracted
over the entire history of the SEKE conference.
After retrieving and reading paper about social network analysis, we can deeply
understand the collaboration, communication among authors. Thus, we can oﬀer sugges‐
tions on how to improve and strengthen the collaboration and communication among
authors, but no one has investigated into CCF co-author network before. Therefore, it
is necessary for us to study the CCF co-author network. In this paper, the main of
contribution of our work is summarized as follows:
1. Design and implement the distributed crawler system; collect the data set from CCF
through the distributed crawler system; build the co-author network of CCF.
2. Discover the co-author network with social-centric and ego-centric. Social-centric
analyzes the major metrics of the data set and compares the metrics with other ﬁeld.
Ego-centric reveals the betweenness centrality, closeness centrality, degree
centrality of the co-author network.
3. According to the analysis of key words, we point out the research hotspots and
research trendy, ﬁnd out the key persons with the higher degree centrality, and gain
the group teams where the key persons are from the co-author network.
The rest of paper is organized as follows: Sect. 2 describes data collection and pre-
processing. Sections 3 and 4 discuss socio-centric and ego-centric analysis respectively.
Section 5 points out key person and the group team. Section 6 concludes the paper and
oﬀers directions for future work.
2
Data Collection and Pre-processing
We extract the data set from the last recent 5 years of CCF by the distributed crawler
system. Table 1 shows the major metrics of CCF co-author network. The distributed
crawler system consists of a master node and several slave nodes. Master node is the
core of the crawler system, and it is responsible for scheduling task, managing process
Social Network Analysis of China Computer Federation
423

and crawling log, while every slave node is a performer of the task. Furthermore, slave
node is a plug-in base on the Google browser Chrome, thus it can run on any computer
that has chrome and run independently with other programs [7]. Architecture of the
distributed crawler system is presented in the Fig. 1.
Table 1. Major metrics of CCF co-author network
Metric
Value
Metric
Value
Total number of authors
49453
Total number of key
words
70631
Average papers per author
1.35
Average authors per
papers
4.38
Average collaborators per
author
4.88
Average number of key
words per author
9.09
Average clustering
coeﬃcient
0.30
Density
0.0037
Fig. 1. Architecture of the distributed crawler system.
Since data obtained from the web by the crawler system are JSON (JavaScript Object
Notation) format, we parsed and formatted data into XML format by program. Further‐
more, in pre-processing we found that some (only 10–20 instances) authors share their
ﬁrst name and last name. We disambiguated between such authors by their emails,
assuming that authors who share a name but not email represent diﬀerent individuals.
After pre-processing, we build the CCF co-author network, which authors represent
nodes, the pairwise of authors is connected by an edge if they have co-author at least
424
C. Fu et al.

one paper. In addition, this analysis of CCF co-author network is an unweighted graph.
That is, although authors A and B have multiple co-authored relationship, there is only
one edge connect author A with author B. Figure 2 shows the overview of the co-author
network graph in CCF.
Fig. 2. The overview of the co-author network graph in CCF
3
Social-Centric Analysis
In this section, we discuss social-centric metrics that we computed overall nodes in the
CCF co-author network. We compare these metrics, shown in the Table 1, with those
of other ﬁelds.
3.1
Major Metrics of CCF Co-author Network
From Table 1, we know that total number of authors is 49453, and total number of key
words is 70631. Furthermore we ﬁgure out that on an average a CCF author writes 1.35
articles, collaborates with 4.88 authors. Comparing with other communities within and
beyond computer science, on an average a CCF author writes 1.35 articles is lower than
one in LIS (2.71), biology (6.4) and physics (5.1), otherwise, on a average a CCF author
collaborates with 4.88 authors is higher than one in LIS (1.57), biology (3.75) and
physics (2.53) [2, 8, 9]. These diﬀerences may arise due of the nature of LIS, biology
and physics. Additionally, on an average a CCF article has 4.38 authors, and on an
average a CCF author has 9.09 key words.
Figure 3, which shows the distribution of number of articles per author with less than
10 articles, reveals that a very large of percentage of CCF authors have less than 3
Social Network Analysis of China Computer Federation
425

articles. Authors with 40 or more articles are very rare, with 87 being the maximum. In
order to analyze the distribution of articles per author, we extract the authors data over
the entire data set where per author has 10 or more articles. Figure 4, which shows the
distribution of number articles per author who has 10 or more articles further conﬁrms
that author with 40 or more articles are rare.
Fig. 3. Distribution of number of articles per author with less than 10 articles.
Fig. 4. Distribution of number of articles per author with 10 or more articles.
3.2
Research Major Trend
Research hotspot is the trend in special ﬁeld. In CCF co-author data set, there are 70631
key words. In order to point out the research trend of computer science, all those key
words, that have been use more than 1000, are referred to as Main Key Words (MKWs).
Figure 5, which shows MKWs of CCF co-author, indicates that seven MKWs occupy
31.02%, which is a larger proportion than any other key words. Seven MKWs are data
426
C. Fu et al.

mining, support vector machine (SVM), clustering, wireless sensor network, genetic
algorithm (GA), segmentation, and cloud computing. Apparently, they are consistent
with big data, Internet of Things (IoT), image processing, community discovery and
cloud computing research major trend.
Fig. 5. Proportion of MKWs in CCF data set
3.3
Co-author Network Density
The network density (D) is deﬁned as the number of edges E to the number of possible
edges and is given by Eq. (1) [10]. The density of the CCF co-author network is 0.0037,
indicating an overall very sparsely connected network.
D =
2 ∗E
N(N −1)
(1)
4
Ego-Centric Analysis
In this section, we discuss ego-centric metrics of centrality which is a tool to understand
structure, function, and composition of the co-author network tie around the individual.
4.1
Betweenness Centrality
Betweenness centrality is used to answer the question of who controls knowledge ﬂows.
Betweenness centrality is deﬁned as the number of the shortest paths from all authors
that pass through the given author [11]. It is an indicator of an author’s centrality in a
Social Network Analysis of China Computer Federation
427

network or an author’s ability to control the knowledge ﬂows, resources and information
[12]. Betweenness centrality of author v is given by Eq. (2), where Pi,j is the total number
of geodesic linking author i and author j, and Pi,j(v) is all the geodesics linking author i
and author j which pass through author v. Authors with high betweenness centrality can
play a role of “bridge” or “middleman” in the co-author network, also the author with
high betweenness centrality can obtain resources, information and knowledge eﬃciently
from other authors in the co-author network [13]. Table 2 shows the top 10 authors with
betweenness centrality in CCF co-author network.
Cb(v) =
∑
i,j≠v
Pij(v)
Pij
(2)
Table 2. Top 10 authors with betweenness centrality in CCF co-author network.
Author alias
Betweenness
centrality
Author alias
Betweenness
centrality
28452
21029.19
25214
13763.72
9161
11182.73
4921
14594.92
6151
12469.83
20252
22731.28
5066
12623.50
30312
13731.08
28719
16091.17
851
13689.25
4.2
Closeness Centrality
Closeness centrality is used to answer the question of who has the shortest distance to
other authors. Closeness Centrality is deﬁned as the mean length of all shortest paths
from a node to other nodes in the network [14]. It is measured as the average of the
reciprocal distance of an author from others. Closeness centrality of an author v is given
by Eq. (3), where d(v, j) is the distance between authors v and j, while N is the total
number of authors where author v can reach in the co-author network. Closeness
centrality judges how important an author is. As we know, the higher closeness centrality
is, the more important the author is. Moreover, an author with higher closeness centrality
could access or obtain resources in the co-author network more eﬃciently than others
with lower closeness centrality [15]. Additionally, an author with higher closeness
centrality also indicates that an author can communicate eﬃciently than those with lower
closeness centrality [12]. Table 3 shows the top 10 authors with closeness centrality in
CCF co-author network.
Cc(v) =
∑N
n=1
1
d(v, j)
(3)
428
C. Fu et al.

Table 3. Top 10 authors with closeness centrality in CCF co-author network.
Author alias
Closeness centrality
Author alias
Closeness centrality
44551
4.887e−10
43639
4.769e−10
45050
4.768e−10
36614
4.761e−10
31895
4.753e−10
45238
4.752e−10
36626
4.740e−10
18639
4.740e−10
31865
4.728e−10
18951
4.722e−10
4.3
Degree Centrality
Degree centrality is used to answer the question of who knows the most authors. It is
measured as the tie of author with others. Degree centrality is deﬁned as the number of
links incident upon a node [16]. Degree centrality of author v is given by Eq. (4). Authors
with higher degree centrality represent they are more central to the co-author network.
Table 4 shows the top 10 authors with ten degree centrality in CCF co-author network.
Dd(v) = deg(v)
(4)
Table 4. Top 10 authors with degree centrality in CCF co-author network.
Author alias
Degree centrality
Author alias
Degree centrality
28452
73
13772
85
20252
71
851
58
14929
55
30312
52
28719
59
15748
53
20222
49
18169
47
5
Identifying Key Person and Group Team
Above the discussion, we try to discover key persons and group teams. According to
the value of centrality, we regard the author as a key person who is in the central position
of CCF co-author network. Moreover, we point out group team members who have a
certain count of collaboration with the key person.
5.1
Identifying Key Person
Actually, key person indicates that he or she has high quantity and quality of their
research work in coordination with other scholars and may be at the top ﬁelds currently.
Above the discussion, we know that an author with higher degree centrality is more
central in the co-author network and the author accesses or obtains resources and infor‐
mation more eﬃciently than other authors with lower degree centrality. Therefore, we
deﬁne an author as a key person that degree centrality is higher than 50 in the CCF co-
author network. Table 5, which shows the key persons with degree centrality and number
Social Network Analysis of China Computer Federation
429

of publishes in the CCF co-author network conﬁrms that key persons not only have
higher degree centrality, but also have higher number of publications.
Table 5. Key persons with degree centrality and number of publications in CCF co-author
network.
Author alias
Degree centrality
Number of publish
28452
73
85
13772
85
87
20252
71
51
851
58
47
14929
55
57
30312
52
42
28719
59
42
15748
53
56
5.2
Group Team Discovery
As usual, key person has a group team or laboratory as a support, furthermore, key person
is also the soul in the group team. Therefore, we point out group teams through collab‐
oration with key person. Firstly, group team appears as a cluster in the co-author network,
that is, a key person has much collaboration with others in the group team and key person
is in the center of group team co-author network. Secondly, those authors who have less
than 3 co-authored relationships with key person may represent collaborations across
institutions but not the member in the group team where key person is. Therefore, while
selecting the group team members, we ﬁlter those who have less than 3 co-authored with
the corresponding key person. Figure 6 shows eight key persons and their group team
clusters in the co-author network. The bigger sizes nodes stand for the key persons, other
Fig. 6. The overview of the co-author network in group teams
430
C. Fu et al.

sizes nodes represent other authors that have collaboration with key persons. From
Fig. 6, apparently, a large proportion of collaboration exists in the group team, but there
is rare collaboration across group teams. Base on the above ﬁnding, there will be fruitful
produce if authors collaborate frequently with others across group teams.
6
Conclusions and Future Work
In this paper, we describe the process of extracting the data set from last resent 5 years
CCF paper websites and building the co-author network of CCF. We analyze CCF co-
author network using social-centric and ego-centric social network analysis methods to
understand the pattern of author collaboration and communication. Otherwise, we
analyze the MKWs to conﬁrm the research trend in computer science ﬁeld. Moreover,
we identify the key person in the CCF co-author network through the authors’ degree
centrality. Finally, according to the collaboration with key person, we point out eight
group teams where eight key persons are respectively.
Our future research involves co-authorship order to understand how the pattern of
collaboration has been inﬂuenced. Moreover, we will deeply study group team research
interest by analyzing the key words of group team members.
Acknowledgement. This work is supported by the National Nature Science Foundation of China
(Grant No. 61272067), the Applied Technology Research and Development Foundation of
Guangdong Province (2016B010124008).
References
1. Rong, X., Tao, Q.: Social network analysis in LIS ﬁeld co-relation of empirical research. Libr.
World 01 (2010)
2. Zhu, Q., Fan, Z., Shi, W.: Information systems and foreign co-authored research network.
Inf. Stud. Theory Appl. 11 (2011)
3. Sun, N., Zhu, J., Cheng, H., Wu, Y.: Study co-author relationship based on social network
analysis papers. J. Suzhou Univ. 29(09) (2014)
4. Shen, G., Huang, S., Wang, D.: On the scientiﬁc research teams identiﬁcation method taking
co-authorship of collaboration as the source data. New Technol. Libr. Inf. Serv. 1, 57–62
(2013)
5. Du, R.: UistViz: 26 Years of UIST Coauthor Network Visualization. CMSC 734 (2013)
6. El Kharboutly, R., Gokhale, S.S.: Social analysis of the SEKE co-author network. SEKE
2015-092 (2015)
7. Yang, Z., Cai, Z., Chen, G., Tang, Y., Zhang, L.: Design and implementation of distributed
web crawler for open access journal. J. Front. Comput. Sci. Technol. 10 (2014)
8. Sarigol, E., Pﬁtzner, R., Scholtes, I., Garas, A., Schweitzer, F.: Predicting scientiﬁc success
based on coauthorship networks. arXiv:1402.7268 (2014)
9. Newman, M.E.J.: Coauthorship network and patterns of scientiﬁc collaboration. Proc. Natl.
Acad. Sci. U.S.A. 101(1), 5200–5205 (2001)
10. Abbasi, A., Cheung, K.S.K., Hossain, L.: Egocentric analysis of co-authorship network
structure, position and performance. Inf. Process. Manag. 45, 671–679 (2012)
11. Borgatti, S.P.: Centrality and network ﬂow. Soc. Netw. 27(1), 55–71 (2015)
Social Network Analysis of China Computer Federation
431

12. Freeman, L.C.: Centrality in social networks: conceptual clariﬁcations. Soc. Netw. 1(3), 215–
239 (1979)
13. Jamali, M., Abolhassani, H.: Diﬀerent aspects of social network analysis. In:
IEEE/WIC/ACM International Conference on Web Intelligence (2006)
14. Lu, H., Feng, Y.: A measure of authors’ centrality in co-authorship networks based on the
distribution of collaborative relationships. Scientometrics 81(2), 499–511 (2009)
15. Borgatti, S.P.: Centrality and network ﬂow. Soc. Netw. 27(1), 55071 (2005)
16. Wasseman, S., Faust, K.: Social Network Analysis: Methods and Applications, vol. 8.
Cambridge University Press, Cambridge (1994)
432
C. Fu et al.

Detecting Postpartum Depression in Depressed
People by Speech Features
Jingying Wang1,2, Xiaoyun Sui1, Bin Hu3, Jonathan Flint4(&),
Shuotian Bai5, Yuanbo Gao2, Yang Zhou1,2, and Tingshao Zhu1(&)
1 Institute of Psychology, Chinese Academy of Sciences, Beijing 100101, China
{wangjingying,tszhu}@psych.ac.cn, oswicer@163.com
2 University of Chinese Academy of Sciences, Beijing 100049, China
3 School of Information Science and Engineering,
Lanzhou University, Gansu 730000, China
bh@lzu.edu.cn
4 Department of Psychiatry and Biobehavioral Sciences,
UCLA David Geffen School of Medicine, Los Angeles, CA 90095, USA
jf@well.ox.ac.uk
5 School of Information Engineering,
Hubei University of Economics, Wuhan 430205, China
Abstract. Postpartum depression (PPD) is a depressive disorder with peripar-
tum onset, which brings heavy burden to individuals and their families. In this
paper, we propose to detect PPD in depressed people via voices. We used
openSMILE for feature extraction, selected Sequential Floating Forward
Selection (SFFS) algorithm for feature selection, tried different settings of fea-
tures, set 5-fold cross validation and applied Support Vector Machine (SVM) on
Weka for training and testing different models. The best predictive performance
among our models is 69%, which suggests that the speech features could be
used as a potential behavioral indicator for identifying PPD in depression. We
also found that a combined impact of features and content of questions con-
tribute to the prediction. After dimension reduction, the average value of
F-measure was increased 5.2%, and the precision of PPD was rose to 75%.
Comparing with demographic questions, the features of emotional induction
questions have better predictive effects.
Keywords: Postpartum depression  Depression  Speech features
Detecting  Classiﬁcation
1
Introduction
Postpartum depression (PPD) is a kind of depressive disorder with peripartum onset,
which can affect both genders after childbirth, and females usually suffer worse than
males [1]. It is a heavy burden to not only patients themselves, but also their spouses,
children and whole families [2]. The concept “PPD” was proposed by Pitt. B in 1968
[1], but there is still no agreement on its diagnostic criteria until now. PPD is one
subtype of depressive disorder. It is liable to cause misdiagnosis between PPD and
other subtypes of depressive disorder [3]. Accurate diagnosis is the critical for effective
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 433–442, 2018.
https://doi.org/10.1007/978-3-319-74521-3_46

intervention. In the case of inconsistent diagnosis, it is necessary to develop new
methods to aid PPD diagnosis.
Depressive disorder has a visible inﬂuence on patients’ emotions [4]. The inﬂuence of
emotion on people could reﬂect in their voices [5]. The study of Cannizzaro [6] shown that
depressed people had less verbal production and variations comparing with healthy
people. Sobin and Sackeim [7] summarized some speciﬁc speech features in depression,
including slow speech speed, increased pause duration, and so on. As a subtype of
depressive disorder, the inﬂuence of PPD on patients should reﬂect on their voices as well.
The diagnosis of PPD by using voice is feasible. First of all, speech-based diagnosis
did not disturb patients too much. The procedure can be set during interviewing.
Patients need not ﬁnish any complex or time-consuming tasks. Secondly, the check
method is easier to hide. Behavioral features are easy to become untrue, because people
are able to control their behaviors [7]. The method of speech check can avoid to
directly contact with patients, which is beneﬁt for data’ ecological validity.
We proposed to detect PPD from depressed patients by using speech features. The
purpose of this study is to investigate the effect of detecting PPD within depression via
speech features under the state of natural experiment. The voices we used were orig-
inated from the conversation between patients and doctors recorded during inter-
viewing. Patients’ voices were separated from these recordings, and divided into two
group: PPD and non-PPD. Speech features extracted by OpenSMILE, and 988 features
were extracted in all. We used all speech features and features after dimension
reduction to predict PPD, respectively. The predictive effects of speech features were
evaluated in the light of three indexes: precision, recall and F-measure.
2
Related Work
Voice is one way of emotional expression. Speech features have been found to be able
to identify different emotions. Nwe et al. [8] reported that classifying voice as different
emotions based on HMM (hidden markov model) had a higher accuracy rate (average
7.7%) than artiﬁcial judging, the average rate was up to 78%. Wu et al. [9] used
prosodic and spectral features to identify seven emotions, with the best precision as
91.6%. From the above studies, we know emotional can be predicted based on speech
features. It motivates us to identify mental illness like PPD using speech features. There
have been few studies about PPD patients’ voices. We think those studies about
phonetic changes of depression probably can be generalized to PPD.
The sounds of depressed patients have signiﬁcant changes because of their illness
[6]. The diagnostic speech features of depression in DSM-5 are described as slow,
volume sank, variation of tone lessen, pause duration increase [4] (P163). Experiments
revealed some speciﬁc vocal indicators in depression, such as speech speed slow down
[10], increased pause duration and times [10, 11], shortened duration of utterance [12],
longer initiative time latency [13]. Speech features in depression express with changes
of F0 such as the decreasing of bandwidth, amplitude, energy [10, 14], shrunken F0
range (DF0) [14], weakened intensity [15], variation of frequency spectrum like
shrunken second formant transition [12] and shrunken spectral energy distribution [16],
and so on.
434
J. Wang et al.

Reviewing recent ﬁndings, Cohen and Elvevåg [17] believed that computer-based
assessments of natural language has the potential for measuring speech disturbances in
people with severe mental illnesses. Some researchers attempted to predict depression
via patients’ speech. Mundt et al. [11] stated that the regression model consisted of F0,
pause duration, speech speed, speech duration, etc. could predict depression, and the
explanatory power of model to depression reaching 79.2%. Emerging evidence sug-
gested that speech features have a strong performance in predicting depression, which
obtained a RMSE of 10.17 well below the baseline of 14.12 [18]. The study of Cohn
et al. shown that the accuracy in detecting depression was 79% for vocal prosody [19].
In our study, we choose the depressed voices collected from natural circumstance to
improve ecological validity, differing from the controlled experimental environments in
the previous studies. In clinical diagnosis, it is more crucial and harder to make a
distinction between different mental illnesses than distinguish healthy people from
psychiatric patients. To improve the differential diagnosis among depressive spectrum
disorders, our detective aim is detecting PPD within depression.
3
Methods
3.1
Participants
In this study, patients’ voices were secondary data which acquired from CONVERGE
(China, Oxford and VCU Experimental Research on Genetic Epidemiology) project of
MDD which recorded during interviewing. Our analyses were based on a total of 740
depressed patients recruited from 58 provincial mental health centers and psychiatric
departments of general medical hospitals in 45 cities of 23 China provinces. All
patients were female. They were excluded if they had bipolar disorder, intelligence
deﬁciency or any type of psychosis. Patients were aged from 30 to 60, the mean age
(standard deviation) of them was 44.4 (8.9). More details of this research include
diagnosis and measures were described in [20].
3.2
Data Acquisition
Voices have been collected by recording pens during computer-based interviewing. All
interviewers were professional medical staffs, and trained on how to carry out the
interview by CONVERGE team for at least a week. The Interview (equal to the content
of recordings) includes assessment of demographic, family history, life events, psy-
chopathology (e.g. depression, anxiety, mania, psychosis, PPD) and psychosocial
functioning. The interview lasted on average two hours. The answers of patients in
interviews are the data what we want.
3.3
Data Preprocessing
The audios were recorded for auditing by trained editors who provided feedback on the
quality of the interviews at the beginning, thus noise control had not been planned ahead.
Since we only analyze the voices of patients’, data preprocess is required before usage.
There are three steps of data preprocessing before speech features extraction (see Fig. 1).
Detecting Postpartum Depression in Depressed People by Speech Features
435

The ﬁrst step was matching and exclusion. First of all, we exported 11875 MP3
ﬁles from CONVERGE database. We need to match recordings with the results of
psychological scales. We only left those patients who have both voice recording and
questionnaire results. After matching, a total of 4243 patients were remained. The next
step is exclusion. To make sure that enough recordings with enough length were used
in the following steps, the short recordings should be excluded. On the basis of our
experiences, we excluded those recordings which less than half an hour. Finally, a total
of 3964 patients remained in this step.
The second step was screening. There were all kinds of noises in these recordings.
We selected high-quality recordings to avoid the impact of noises on the predictive
results. We divided all recordings into different levels according to the certain evalu-
ation criteria (see Table 1). Finally, 774 recordings of level A were labelled, which
were used for the further process.
In the last and most important step, our mission was cutting and denoising. We
needed to separate the voices of patients from interviewers, and wiped out other noises.
We recruited a few workers to engage in this part of work. The requirements of this
work including: (1) the voice clips should be longer than 5 s; (2) the noises need to be
cut include but not limited to ring, telephone ring, click, voices of other people, and so
on. There were 740 patients remained after denoised, including 21459 voice clips. Each
voice clip is equal to one answer of one patient.
3.4
Feature Extraction, Selection and Data Analysis
Open SMILE [21] was used to extract speech features. A total of 988 speech features
were extracted. The procedure of feature extraction was as follows: ﬁrstly, in view of
the above related work, 26 basic speech features were extracted from recordings,
Fig. 1. The procedure of data preprocessing
Table 1. Evaluation criteria of voices in different levels
Level Evaluation criteria
A
Background is quiet
B
Background has light noises
C
Noisy, hard to be cut
D
The voice of patient is not cleara
aClear, means the voices are
distinguishable, the content could be easily
understood via hearing
436
J. Wang et al.

including intensity, loudness, zero cross rate, voicing probability, F0, F0 envelope,
eight linear spectral pair frequencies (LSP) and twelve Mel-frequency cepstral coefﬁ-
cients (MFCC). Secondly, to investigate the variability of voices, 26 features were
turned into their ﬁrst-order derivatives. Thirdly, we calculated 19 statistics of those
features mentioned above, such as mean, standard deviation, range, etc. At last, we
acquired 988 (= (26 + 26)  19) speech features.
It is expected that there are some irrelevant and redundant data will weaken the
prediction performance. Therefore, it is not a good idea to input all speech features for
prediction. Speech features should be selected before prediction. For choosing relevant
features and achieving dimension reduction of speech features, we used the Sequential
Floating Forward Selection (SFFS) algorithm.
Data analyses mainly includes classiﬁcation and correlation analysis. Patients were
divided into two groups in the light of whether they have been diagnosed as PPD or
not. The group labels were considered as golden standard in classiﬁcation: patients with
PPD were labeled 1, without PPD were labeled 0. As classiﬁcation, we implemented
SVM and 5-fold cross validation for training and testing different models. To ﬁgure out
whether there are salient relationships between the independent variables “number of
features” and “sample size” and the predictive effects, we used partial correlation
analyses to test them. In addition, paired-sample t-test was used in trying out the
impacts of dimension reduction and the content of question on the predictive effects.
4
Results
4.1
Prediction
The rate of PPD group and non-PPD group was kept 1:1 to ensure that sample size has
no obvious impact on predictive results. In order to directly observe the effect of
dimension reduction, we used all speech features and features after dimension reduc-
tion to predict PPD, respectively. The results are respectively shown in Tables 2 and 3.
We list the results of top ten best-performing questions, and order by the sample size
from small to large.
In Table 2, the classiﬁcation was based on 988 speech features. The best predictive
result of F-measure is 65%, which is the reply to one question of depression scale.
Observation of row 4–9, we found that different questions with same sample size had
different predictive powers, considering the speech features used in these ten questions
were the same. In addition, the predictive effects of demographic answers were com-
mon lower than the other questions.
In Table 3, the number of features were dramatically reduced after dimension
reduction. The best predictive result 69% of the selected features is PSY.3, which is the
reply to one question of psychosis scale. The average value of F-measure was increased
5.2%. By looking into row 4–9, we found that the differences of predictive effects of
different questions with same sample size decreased.
Detecting Postpartum Depression in Depressed People by Speech Features
437

PPD and non-PPD Confusion matrixes were shown in Table 4. The precision of
PPD was markedly improved after dimension reduction, reaching 75%. In contrast, the
precision of non-PPD had slightly decreased after dimension reduction.
Table 2. Results of classiﬁcation using 988 features
Sample size Question
Precision Recall F-measure
80
DEP.E24.F
0.65
0.65
0.65
90
PSY.4
0.57
0.57
0.57
120
PSY.3
0.62
0.62
0.61
120
DEP.E29
0.58
0.57
0.57
130
GAD.D64.D 0.61
0.61
0.61
130
DEP.E26
0.53
0.53
0.53
170
D2.B
0.46
0.46
0.46
170
D4.A
0.49
0.49
0.49
190
D6.A
0.55
0.55
0.55
220
D6
0.53
0.53
0.53
Average
0.559
0.558
0.557
In the second column, these abbreviations before the ﬁrst
point represent the corresponding questionnaire, the content
after the ﬁrst point represents the corresponding item (means
question) number in questionnaire (except scale
demographics, the content after letter “D” is the item
number). DEP, depression scale; PSY, psychosis scale;
GAS, scale of general anxiety disorder; D, demographics.
Table 3. Results of classiﬁcation after dimension reduction
Sample size Number of features Question
Precision Recall F-measure
80
12
DEP.E24.F
0.63
0.62
0.62
90
35
PSY.4
0.61
0.60
0.59
120
15
PSY.3
0.69
0.69
0.69
120
18
DEP.E29
0.65
0.64
0.64
130
28
GAD.D64.D 0.62
0.62
0.62
130
12
DEP.E26
0.62
0.62
0.62
170
13
D2.B
0.53
0.53
0.53
170
30
D4.A
0.54
0.54
0.53
190
9
D6.A
0.63
0.63
0.63
220
12
D6
0.63
0.62
0.62
Average
0.615
0.611
0.609
In the second column, these abbreviations before the ﬁrst point represent the
corresponding questionnaire, the content after the ﬁrst point represents the
corresponding item (means question) number in questionnaire (except scale
demographics, the content after letter “D” is the item number). DEP, depression
scale; PSY, psychosis scale; GAS, scale of general anxiety disorder; D,
demographics.
438
J. Wang et al.

4.2
Correlation Analysis and Signiﬁcance Test
Do independent variables sample size and the number of features have signiﬁcant rela-
tionships with predictive effects? To ﬁgure out it, partial correlation analyses were
applied in consideration of the impact of the other factor. The analyzed result between the
number of features and predictive indexes exhibited that there is no salient correlation
between them, after controlling the sample size (precision: r = −0. 461, p > .1; recall:
r = −0.422, p > .1; F-measure: r = −0.473, p > .1). The correlation analysis between
sample size and predictive indexes (see Fig. 2) shown that sample size had a signiﬁcant
negative correlation with three indexes which predicted by 988 features after controlling
the impact of the number of features (precision: r = −0. 697, p < .05; recall: r = −0.691,
p < .05; F-measure: r = −0.691, p < .05). However, there is no signiﬁcant correlation
between sample size and three indexes which calculated via reduced features.
To make it clear that if dimension reduction can evidently improve the predictive
effect or not, we compared the predictive results of total features with reduced features
by using paired-sample t test (Table 5 lists the means and standard deviations of
Table 4. Confusion matrixes of the most effective questions
Fig. 2. The impact of sample size on predictive indexes (dr, dimension reduction)
Detecting Postpartum Depression in Depressed People by Speech Features
439

predictive indexes). The results indicated that the predictive results improved signiﬁ-
cantly after dimension reduction (precision: t = −4.763, p < .01; recall: t = −4.31,
p < .01; F-measure: t = −4.061, p < .01). Further analysis, considering the impact of
question, the contrast of three pairs questions with same sample size was ran by
paired-sample t test. The results shown that the differences between different questions
with same sample size had saliently shrank after dimension reduction (120 sample size:
t = −1, p = .42; 170 sample size: t = 7, p < .05). The results of the sample size of 130
cannot test t value because their SD is zero. But their difference value’s change is the
largest after dimension reduction.
5
Discussion
The purpose of this study is to detect PPD in depressed patients via speech features. We
used openSMILE for feature extraction, selected SFFS for feature selection, tried
different numbers of features, set 5-fold cross validation for strengthening generaliz-
ability of model and applied SVM in Weka for training and testing different models.
The best performance of F-measure reaching 69%, comparing with the random pre-
dictive effect 50%, which suggests that voice could be used as a potential behavioral
indicator to identify depression disorder’ subtypes.
We speculate there may be important inﬂuences of the number of features, sample
size and the content of question on the predictive effect. Our results indicated that the
number of features has no signiﬁcant relationship with the prediction, but the predictive
effect is dramatically improved after dimension reduction. The number of features
among different questions are different after dimension reduction, so we think the
positive impact of dimension reduction on predictive results is a combined result of
number of features and content of question. It is unexpected that there is a negative
correlation between sample size and predictive effect. The probable cause is demo-
graphic questions lack of the ability of emotional induction, which results in the
undistinguishable neutral emotion in all patients’ voices. Just as it is shown in Fig. 2,
questions D2.B and D4.A make signiﬁcant contributions to obvious dents of curves.
Different question has different predictive effect. We can ﬁnd some clues by the
most effective questions in Tables 2 and 3. The most effective question is DEP.E24.F
before dimension reduction. This question asked patients to recall the state in their
severest depressive episode. The most effective question is PSY.3 after dimension
reduction. This question asks, “have you ever taken medicine for your nerves or the
way you were feeling or acting?”. All patients are recurrent depressive sufferers, they
Table 5. The means and standard deviations of predictive indexes
988 features
Reduced features
Precision Recall F-measure Precision Recall F-measure
Mean 55.90
55.89
55.79
61.50
61.10
60.90
SD
5.92
5.88
5.77
4.77
4.64
4.86
SD, standard deviation
440
J. Wang et al.

must have experiences on taking anti-depressants. Thus, it is a good question to induce
emotion, because the experiences about psychotropic medicine probably were negative
due to medicines’ side effects. In summary, the effect of emotional induction of
questions has an important inﬂuence on the predictive effect.
6
Conclusion
In this study, the best predictive performance of our speech-based models is F-measure
69%, which suggests that the speech features could be used as a potential behavioral
indicator to identify PPD in depressed patients. A combined impact of features and
question contribute to the improvement of predictive effect. After dimension reduction,
the average value of F-measure was increased 5.2%, and the precision of PPD was rose
to 75%. Compared with neutral demographic questions, the features of emotional
induced questions have better predictive effects.
Acknowledgments. This work was supported by the National Basic Research Program of China
(973 Program) (No. 2014CB744603), and Natural Science Foundation of Hubei Province
(2016CFB208).
References
1. Pitt, B.: ‘Atypical’ depression following childbirth. Br. J. Psychiatry 114(516), 1325–1335
(1968)
2. Burke, L.: The impact of maternal depression on familial relationships. Int. Rev. Psychiatry
15(3), 243–255 (2003)
3. American College of Obstetricians and Gynecologists. Committee on Obstetric Practice,
Committee opinion no. 453: Screening for depression during and after pregnancy. Obstet.
Gynecol. 115(2 Pt 1), 394–395 (2010)
4. Accounts Payable Association: Diagnostic and Statistical Manual of Mental Disorders
(DSM-5®). American Psychiatric Publishing (2013)
5. Kramer, E.: Elimination of verbal cues in judgments of emotion from voice. J. Abnorm. Soc.
Psychol. 68(4), 390–396 (1964)
6. Cannizzaro, M., Harel, B., Reilly, N., Chappell, P., Snyder, P.J.: Voice acoustical
measurement of the severity of major depression. Brain Cognit. 56(1), 30–35 (2004)
7. Sobin, C., Sackeim, H.A.: Psychomotor symptoms of depression. Am. J. Psychiatry 154(1),
4–17 (1997)
8. Nwe, T.L., Foo, S.W., De Silva, L.C.: Speech emotion recognition using hidden Markov
models. Speech Commun. 41(4), 603–623 (2003)
9. Wu, S., Falk, T.H., Chan, W.-Y.: Automatic speech emotion recognition using modulation
spectral features. Speech Commun. 53(5), 768–785 (2011)
10. Ellgring, H., Scherer, P.K.R.: Vocal indicators of mood change in depression. J. Nonverbal
Behav. 20(2), 83–110 (1996)
11. Mundt, J.C., Vogel, A.P., Feltner, D.E., Lenderking, W.R.: Vocal acoustic biomarkers of
depression severity and treatment response. Biol. Psychiatry 72(7), 580–587 (2012)
Detecting Postpartum Depression in Depressed People by Speech Features
441

12. Flint, A.J., Black, S.E., Campbell-Taylor, I., Gailey, G.F., Levinton, C.: Abnormal speech
articulation, psychomotor retardation, and subcortical dysfunction in major depression.
J. Psychiatr. Res. 27(3), 309–319 (1993)
13. Mandal, M.K., Srivastava, P., Singh, S.K.: Paralinguistic characteristics of speech in
schizophrenics and depressives. J. Psychiatr. Res. 24(2), 191–196 (1990)
14. Porritt, L.L., Zinser, M.C., Bachorowski, J.-A., Kaplan, P.S.: Depression diagnoses and
fundamental frequency-based acoustic cues in maternal infant-directed speech. Lang. Learn.
Dev. 10(1), 51–67 (2014)
15. Cohen, A.S., Kim, Y., Najolia, G.M.: Psychiatric symptom versus neurocognitive correlates
of diminished expressivity in schizophrenia and mood disorders. Schizophr. Res. 146(1–3),
249–253 (2013)
16. Tolkmitt, F., Helfrich, H., Standke, R., Scherer, K.R.: Vocal indicators of psychiatric
treatment effects in depressives and schizophrenics. J. Commun. Disord. 15(3), 209–222
(1982)
17. Cohen, A.S., Elvevåg, B.: Automated computerized analysis of speech in psychiatric
disorders. Curr. Opin. Psychiatry 27(3), 203–209 (2014)
18. Cummins, N., Joshi, J., Dhall, A., Sethu, V., Goecke, R., Epps, J.: Diagnosis of depression
by behavioural signals: a multimodal approach. In: Proceedings of the 3rd ACM
International Workshop on Audio/Visual Emotion Challenge, New York, NY, USA,
pp. 11–20 (2013)
19. Cohn, J.F., et al.: Detecting depression from facial actions and vocal prosody. In: 2009 3rd
International Conference on Affective Computing and Intelligent Interaction and Work-
shops, pp. 1–7 (2009)
20. Yang, F., et al.: Clinical features of and risk factors for major depression with history of
postpartum episodes in Han Chinese women: a retrospective study. J. Affect. Disord. 183,
339–346 (2015)
21. Eyben, F., Weninger, F., Gross, F., Schuller, B.: Recent developments in openSMILE, the
Munich open-source multimedia feature extractor. In: Proceedings of the 21st ACM
International Conference on Multimedia, New York, NY, USA, pp. 835–838 (2013)
442
J. Wang et al.

Research on Network Public Opinion
Propagation Mechanism Based
on Sina Micro-blog
Weidong Huang(&), Qian Wang, and Yixuan Wang
School of Management, Nanjing University of Posts and Telecommunications,
Nanjing, Jiangsu, China
huangwd@njupt.edu.cn
Abstract. In the era of micro-blog, network public opinion has become the
main expression of public opinion. Network public opinion can quickly form a
network of public opinion, and promote the dissemination of information ﬁs-
sion, with a strong interaction and effectiveness. In the case of “A girl suffered
attacks in a Yitel”, we construct network propagation diagram by ucinet soft-
ware and research the structural characteristics of the network public opinion
propagation network based on Sina micro-blog, and the whole structure of the
propagation network and the key nodes are measured. The results show that the
key nodes in public opinion network propagation has a high ability to spread, so
we can control the velocity of micro-blog public opinion through affecting these
key nodes.
Keywords: Micro-blog  Social network analysis  Key nodes
Network structure
1
Introduction
Because of the clustering characteristics of network aggregation, the propagation of
network public opinion is often sudden and explosive. With the rise of social networks,
social media has become a great impetus to the formation of public opinion. By the end
of 2015, the active users of Sina micro-blog have reached 236 million monthly, an
increase of 34%. As one of the most rapid development of new media in recent years,
Sina micro-blog becomes the main carrier of aggregation and outbreak of network
public opinion due to its dual attributes of social and media.
As a medium of propagation, each user of micro-blog is the publisher and com-
municator of information. After the netizens release information, this information only
needs to be forwarded through their huge fans group, and then, secondary forwarding
again by the fans group and it will form the immeasurable reading quantity and
inﬂuence. Micro-blog’s concern is based on similar preferences and habits, which
makes the propagation of micro-blog’s superposition and resonance effect signiﬁcant.
And based on the event of public opinion in micro-blog, it is of great practical sig-
niﬁcance for the new media to study the propagation mechanism of the network public
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 443–452, 2018.
https://doi.org/10.1007/978-3-319-74521-3_47

opinion in micro-blog, and it has very important reference signiﬁcance to the monitor
and guide of network public opinion.
2
Research and Design of the Propagation Mechanism
of Network Public Opinion Based on Sina Micro-blog
The information of Sina micro-blog spread mainly in the following ways: the release
function, @ function, forwarding function and comment function [1]. These four ways
can quickly and effectively release information, so that this information can spread in
the Sina micro-blog and can be concerned by more users. The propagation of
micro-blog’s network public opinion is a very complex process, and it is not a single,
linear propagation mode. However, based on the sub-public communication formed by
the users’ concern function, it develops into mass communication by the ﬁrst level
information receiver who forward information through forwarding function for the
secondary spread, and then, it turns into a multi-level propagation mode with the
traditional mass media platform. In this paper, we selected “A girl suffered attacks in a
Yitel” as a case, and we collected data and tried to build the propagation analysis
framework of micro-blog public opinion events to study the evolutionary mechanism of
network public opinion.
2.1
Event Description of “A Girl Suffered Attacks in a Yitel”
April 5, 2016, the victim whose account name on micro-blog is Wanwan in her Sina
micro-blog released a video of the attack in the hotel. Her unnerving story soon
triggered a nationwide rage online. Through micro-blog’s comment function, some
users hope that the party can release the details of the development process, some users
asked if the hotel would give a statement and how to deal with it after calling the
police, and there are users who want more people to see this vicious incident through
forwarding it to give a warning to the female friends. Due to the spread of fast and large
scale, the topic of #A girl suffered attacks in a Yitel# is quickly on the hot topic list of
Sina micro-blog. During this period, @Reporter_hong-tao Xue has been forwarded
Wanwan’s micro-blog to conduct a consultation on this issue and popularize relevant
legal knowledge. @Yiteland @Homeinns Co., Lt issued a statement through
micro-blog, and it said that they would to investigate this vicious incident and give an
account of the party and the public. @Safety Beijing, as ofﬁcial micro-blog of the
Beijing Municipal Public Security Bureau, has been tracking this matter.
2.2
Data Collection and Processing
Set the time segment of collection and retrieval from April 5 to April 6, 2016 through
the acquisition of Sina micro-blog source data. We collected Sina micro-blogs and its
forwarding data by the searching method of hot topic, a total of 1900 micro-blogs. We
teased out the micro-blog’s forwarding relationship through Sina micro-blog’s for-
warding rules and identiﬁed the Sina micro-blog theme users more than 500 people of
“A girl suffered attacks in a Yitel”. We processed and calculated raw data and
444
W. Huang et al.

propagation relationship between each node, and constructed the propagation network
matrix of micro-blog public opinion of this event, this network matrix was
anon-symmetric matrix of 502*502, and the data indicated the nodes’ frequency of
emergence in the sample data. Based on the matrix, using UCINET software for
visualization processing of the propagation network for this event (Fig. 1).
2.3
The Design of Analytic Framework
To analyze the relationship of propagation node of micro-blog public opinion, we need
to measure the structure of the propagation network that is composed by nodes. Social
network analysis can not only measure the relationship between nodes and other nodes,
but also make the researchers more intuitively grasp the behavior of nodes by visu-
alizing the interaction between nodes.
For the micro-blog public opinion events, “A girl suffered attacks in a Yitel”, we
designed the analytic framework, and it mainly included the overall network structure
measurement and key nodes measurement. In the parameters that measure the overall
network structure, the network density measures the compactness of network nodes
through calculating the total distribution of each line and the difference between it and
the complete graph; Clustering coefﬁcients are used to reﬂect the structure of propa-
gation network and the degree of aggregation between nodes.
And in measuring the key parameters of the key nodes, the structure hole index
gives us the ability of nodes to control the information received by other nodes through
calculating the effective size, global constraints and grades; Intermediate centrality
refers to frequency, and the frequency means that shortest path of other nodes are by
the way of this node, and it shows the ability of a node to act as a medium; Intermediate
centrality reﬂects the dependence of nodes on information ﬂow, whereas closeness
centrality is a measure of the point in a graph that is not subject to other controls
(Fig. 2).
Fig. 1. Network propagation diagram of “A girl suffered attacks in a Yitel” (Numbered Edition)
Research on Network Public Opinion Propagation Mechanism
445

3
Experimental Measurement and Analysis
3.1
Analysis of Key Nodes
In the propagation of micro-blog public opinion of this event, @Wanwan_2016, as a
party, released the ﬁrst micro-blog about encountering attack in Yitel and it soon swept
across this social media platform. The ofﬁcial micro-blog of Yitel released a statement
on it. At the same time, @Safety Beijing also reported on this incident, and it was the
ﬁrst government micro-blog to report the incident. In addition, @Reporter_hong-tao
Xue also played a more important role in the propagation and diffusion of micro-blog
public opinion. Using the key nodes analysis algorithm to form a visual description, as
shown in Fig. 3, the nodes in the Figure are signiﬁcantly expressed.
3.1.1
Analysis of Structural Holes
The value of the contribution to the network of the two related people who exist
structural holes between nodes can be accumulated. By measurement; the degree of
Fig. 2. Analytic framework of propagation network structure
Fig. 3. Key nodes diagram of “A girl suffered attacks in a Yitel”
446
W. Huang et al.

structural holes in the propagation network can be analyzed [2]. By using UCINET
software, we analyzed part nodes of structural holes in micro-blog public opinion
propagation network of “A girl suffered attacks in a Yitel”, and we sort out the effective
scale to get the Table 1. We funded that No. 1 “Wanwan_2016”, No. 2, “Ctrip”,
No. 3 “Safety Beijing”, No. 5, “Homeinns Co., Ltd” and No. 4 “and “Yitel” ranked the
top ﬁve. The value of effective scale reﬂects the position of nodes in the propagation
network, and the bigger the value is, the more core the position becomes [2]. In
addition to the effective scale, the limiting degree of these ﬁve nodes are relatively
small, they are less than 0.2, and it also reﬂects that the ﬁve nodes are not easy to be
controlled by other nodes and easier to access and spread public opinion.
3.1.2
Intermediary Centrality
Intermediate centrality is used to measure the ability of a node as a medium of prop-
agation, that is, it measures the ability of the node as a “bridge” in the process of
information propagation [3]. When a node appears on the shortest path between two
nodes, the more the nodes appears, the higher the intermediate centrality of the node is,
and this shows that more and more nodes have to spread information through it, and the
node is also known as the “bridge node”. ljk is the shortest path number from node j to
node k, ljkðxiÞ is the shortest path number that contains the node i on the shortest path
from node j to node k, N is the number of all nodes in a network. In a directed network,
the formula for calculating the intermediate centrality is shown in Eq. (1). The results
of intermediary centrality measurements of the propagation network are shown in
Table 2.
CBðxiÞ ¼
P
j\k
ljkðxiÞ=ljk
ðn  1Þðn  2Þ
ð1Þ
Table 1. Analysis of propagation network structural holes of “A girl suffered attacks in a Yitel”
(part)
Nodes
Degree Effective
scale
Efﬁciency Limiting
degree
Grade
degree
1 Wanwan_2016
199
152.078
0.764
0.176
0.904
2 Ctrip
100
76.185
0.762
0.166
0.795
3 Safety Beijing
87
64.389
0.740
0.196
0.810
5 Homeinns Co., Ltd
82
60.809
0.742
0.190
0.795
4 Yitel
77
56.717
0.737
0.198
0.787
6 Reporter
hong-taoXue
28
19.286
0.689
0.223
0.451
10 Police Wang
15
9.090
0.606
0.281
0.195
7 People outside the
city
15
8.825
0.588
0.306
0.273
27 Update step by step
11
5.583
0.508
0.327
0.065
Research on Network Public Opinion Propagation Mechanism
447

We can ﬁnd that, “Wanwan_2016”, “Ctrip”, “Safety Beijing”, “Homeinns Co.,
Ltd”, “Yitel” and “Reporter_ hong-tao Xue”, the value of intermediate centrality of the
six nodes is located in the top six of the data sample, it means that the six nodes play
the important roles in the propagation process of micro-blog’s public opinion and take
on the more important roles of the medium. They not only have a greater share of
public opinion information, but also a greater degree of impact on other nodes. It can be
seen from the measurement results of intermediate centrality, the party discloses the
vicious incident and triggers a large number of micro-blog users to forward the
information, and it leads to the information propagation. In addition, the government
micro-blog, the ofﬁcial micro-blog of media and other celebrities have a high inter-
mediary, while the intermediary centrality of the other nodes is generally low, which
also shows that the different types of nodes have different effects on the propagation of
public opinion information.
3.1.3
Closeness Centrality
In the information propagation process of Sina micro-blog, a user may contact with
many people in the network directly or indirectly. In this case, the user has a relatively
high closeness centrality, that is, the user can close to a large number of other users in
the network. It can be seen from the Table 3, the closeness centrality of Reporter_
hong-taoXue is highest in the propagation network of the micro-blog public opinion,
and it means that this node contact with many people in the network, and it also can be
found from the attributes of the node, the inﬂuence of the opinion leaders is more
strong in the development of public opinion events.
From the point of view of structural holes, intermediary centrality and closeness
centrality, we analyze the key nodes of the propagation network of micro-blog public
opinion event of “A girl suffered attacks in a Yitel”. From the view of calculated data,
“Wanwan_2016”, as the party node, Homeinns Co., Ltd and Yitel, as the involved
companies nodes, their effective scale are 152.078, 60.89 and 56.717, and their limiting
degree are less than 0.2. And the three nodes are the starting nodes of microblog
information in “A girl suffered attacks in a Yitel”, so external nodes do not affect them.
Table 2. Analysis of intermediary centrality of propagation network of “A girl suffered attacks
in a Yitel” (part)
User type
Node
Absolute intermediary
centrality
Relative intermediary
centrality
1 Party
Wanwan_2016
143458.422
57.498
2 Companies involved Ctrip
67290.070
26.970
3 Government
micro-blog
Safety Beijing
64621.414
25.900
5 Companies involved Homeinns Co., Ltd
51395.484
20.599
4 Companies involved Yitel
50695.699
20.319
6 Other celebrities
Reporter_ hong-taoXue
47793.961
19.156
27 Domestic consumer
Update step by step
7226.098
2.896
23 Domestic consumer
A glimpse of autumn
7142.234
2.863
448
W. Huang et al.

What’s more, the intermediary center degrees of “Safety Beijing”, “Reporter_
hong-taoXue” are 25.900 and 19.156 respectively, and they have relatively high
closeness centrality, so it can be concluded that the two nodes are “bridge” nodes in
network propagation of this micro-blog public opinion event, and they play the roles of
“opinion leaders”, and they play leading roles in the propagation of public opinion
information in this event and promote the transfer of public opinion information
through their own inﬂuence.
3.2
Overall Network Structure Measurement
3.2.1
Network Density and Distance Between Nodes
There is a direct correlation between the network density and the strength of the
relationship between nodes, and the value of the network density will decrease with the
increase of the number of nodes [4, 5]. The distance between nodes in the propagation
network is mainly used to measure the network structure characteristics, and the shorter
the distance between nodes in the propagation network is, it means that the connection
can be established between the nodes through a shorter path, and the relationship
between the nodes is close, and the propagation network has a strong cohesion and the
information in the network can spread rapidly.
According to the statistical results in Table 4, the network density of this public
opinion event is only 0.0102, the result shows that the network density is very small in
the propagation process of the public opinion events, the links between nodes are more
dispersed, and the exchange of information is not frequent. The distance between nodes
of the propagation network is 3.193, cohesion distance-based is 0.344, and fragmen-
tation distance-weighted is 0.656. This result shows that the propagation ability of the
Table 3. Analysis of closeness centrality of propagation network of “A girl suffered attacks in a
Yitel” (part)
Node
In farness Out farness In closeness Out closeness
6 Reporter_ hong-taoXue
973.000
974.000
51.387
51.335
1 Wanwan_2016
1027.000
991.000
48.685
50.454
3 Safety Beijing
1105.000
1297.000
46.249
38.551
28 Legal Evening News
1203.000
1210.000
41.563
41.322
2 Ctrip
1250.000
1250.000
40.000
40.000
5 Homeinns Co., Ltd
1268.000
1269.000
39.432
39.401
305 Happy xiaoliumang
1279.000
1288.000
39.093
38.820
23 A glimpse of autumn
1290.000
1258.000
38.760
39.746
Table 4. Measurement results of network density and distance between nodes of “A girl
suffered attacks in a Yitel”
Density
Standard
deviation
Average
distance
Distance-based
cohesion
Distance-weighted
fragmentation
0.0102
0.1018
3.193
0.344
0.656
Research on Network Public Opinion Propagation Mechanism
449

public opinion information in data samples is general and cohesion is not strong. The
propagation probabilities of distance from 1 to 4 are 1%, 26.1%, 25.4% and 47.4%, so,
the propagation ability of sub-public propagation is general, however, the relationship
links whose distance is 4 are nearly half, and it also shows that the mass communi-
cation has a huge effect in this event.
3.2.2
Clustering Coefﬁcient
The clustering coefﬁcient is mainly used to reﬂect the characteristics of the topology
structure of the propagation network and the aggregation degree of the relationships
among the nodes [6]. If a node j in the network has nj neighbor nodes, then there may
be a maximum of njðnj  1Þ=2 edges between the nj nodes. We deﬁne the ratio of
actual number of edges EðjÞ between nodes nj and the possible number of edges
njðnj  1Þ=2 as the clustering coefﬁcient of nodes CCðjÞ, that is:
CCðjÞ ¼ 2EðjÞ=½njðnj  1Þ
ð2Þ
The clustering coefﬁcient CC of the whole network is the average of the clustering
coefﬁcients of all nodes j:
CC ¼
P
N
j¼1
CCðjÞ
N
ð3Þ
The value of clustering coefﬁcient ranged from 0 to 1, the greater the number of
clusters is, the stronger the cohesive force of the whole propagation network is, and the
links between nodes are more closely. The measure results in Table 5 show that
efﬁcient clustering coefﬁcient is 0.1, in propagation network in the public opinion event
of “A girl suffered attacks in a Yitel”. The clustering coefﬁcient is too small, and it
means that there is a relatively low level of information communication between nodes.
At the same time, the network structure is relatively loose, the community structure and
the state of the internal substructure are not obvious, and the connection between the
nodes is relatively weak.
Table 5. Analysis of clustering coefﬁcient of propagation network of “A girl suffered attacks in
a Yitel” (part)
Node
Clustering coefﬁcient Node logarithm
1 Wanwan_2016
0.014
19503
2 Ctrip
0.031
4851
3 Safety Beijing
0.031
3655
4 Yitel
0.036
2850
5 Homeinns Co., Ltd
0.034
3240
6 Reporter_hong-taoXue 0.138
351
7 People outside the city 0.275
91
Weighted Overall graph clustering coefﬁcient: 0.100.
450
W. Huang et al.

In this paper, we use three indexes, the network density, the distance between nodes
and the clustering coefﬁcient [7–9], in order to analyze the overall network structure of
the micro-blog public opinion of “A girl suffered attacks in a Yitel”. According to
measure results, the network density is very small in the propagation network of the
public opinion events, it is only 0.0102, the links between nodes are more dispersed,
and the exchange of information is not frequent. The propagation ability of sub-public
communication is general; however, the relationship links whose distance is 4 are
nearly half, so it also shows that the mass communication has a huge effect in this
event. In spite of this, the cohesive force of propagation network is weak and lack of
communication between nodes and nodes, and the forwarding relationship does exist,
but it did not communicate with other nodes for effective communication after the
forwarding, making public opinion propagation effect cease to advance. For this
micro-blog public opinion event, information is more concentrated on the party and the
company involved, “Wanwan_2016” and “Yitel”, and they are the key communicators
of this event. For “Safety Beijing”, “Reporter_ hongtaoXue” and other key nodes, etc.,
they only have single forward relationship rather than mutual forwarding relationship,
and the connectivity between nodes is poor, so it is difﬁcult to form a small group.
4
Conclusion
In this paper, we measured the overall network structure that are composed of nodes and
the key nodes of the propagation network through statistics and arrangement on the
relationship of micro-blog public opinion propagation node. Through the analysis of the
measurement results, it is found that it is difﬁcult to form a small group because of the
lack of cohesion between the nodes of this event. Investigating its deep reason, the event
of “A girl suffered attacks in a Yitel” is a vicious incident caused by the individual party,
and in the information propagation of micro-blog public opinion, it is focused on the
hotel’s indifference to violence that complained by the party, companies involved also
issued a statement and it said they would investigate it to the end, in order to give the
party and the public an account. Although the forwarding quantity in the short term of
this incident is very high, and it also appeared the “bridge” node as the “opinion leader”,
and there is a lack of cohesion between the nodes of the propagation network, and there
is no small group, and the communication between the nodes is weak.
Therefore, the parties of public opinion events should actively enhance commu-
nication with other users, and it needs more media, celebrities and government
micro-blog to pay attention to public opinion events so as to promote the transfer of
public opinion information through their own inﬂuence. On the one hand, the parties
should strengthen the communication with other users and select the representative
users to forward their questions for each other after forwarding their own micro-blog. It
will not only form a small group based on forwarding, but also can enhance the
cohesion between the user nodes, so that public opinion information can spread rapidly.
On the other hand, we can develop opinion leaders to achieve the effect of guiding
public opinion. The “opinion leader” in Sina micro-blog refers to a kind of commu-
nicator that has appeal and inﬂuence in the propagation process of network public
opinion. This speech of information communicator is very easy to cause the recognition
Research on Network Public Opinion Propagation Mechanism
451

of other users, and the users use comment and forwarding function to make it spread in
the micro-blog, resulting in a greater impact on public opinion, which has a greater
impact on public opinion. Therefore, we need to have the opinion leaders to exert their
inﬂuence and spread positive energy, in order to achieve the purpose of infects and
affects other user groups.
Acknowledgment. Work described in this paper was funded by the National Natural Science
Foundation of China under Grant No. 71671093. The authors would like to thank other
researchers at the Nanjing University of Posts and Telecommunications.
References
1. Russo, T.C., Koesten, J.: Prestige centrality and learning: a social network analysis of an
online class. Commun. Educ. 54(3), 254–261 (2005)
2. Jinlou, Z., Junhui, C.: Analysis of micro-blog public opinion diffusion based on SNA: an
empirical study on April 20 Ya’an Earthquake in Sichuan. Manag. Rev. 01, 148–157 (2015)
3. Wei, K.: Analysis of the key nodes in public opinion spread during emergencies based on
social network theory—a case study of the 723 Wenzhou high-speed train collision. J. Public
Manag. 03, 101–111, 127–128 (2012)
4. Xiaojuan, H., Nan, J., Jinjin, X.: Study on micro-blog rumor based on social network
analysis—a case study on the micro-blog about food security. Intell. J. 08, 161–167 (2014)
5. Wei, J.C., Bu, B., Liang, L.: Estimating the diffusion models of crisis information in
micro-blog. J. Inform. 6(4), 600–610 (2012)
6. Lee, C.H.: Mining spatio-temporal
information
on microblogging streams
using a
density-based online clustering method. Expert Syst. Appl. 39(10), 9623–9641 (2012)
7. Nair, H.S., Manchanda, P., Bhatia, T.: Asymmetric social interactions in physician
prescription behavior: the role of opinion leaders. J. Mark. Res. 47(5), 883–895 (2010)
8. Xin, M.J., Wu, H.X., Niu, Z.H.: A quick emergency response model for micro-blog public
opinion crisis based on text sentiment intensity. J. Softw. 7(6), 1413–1420 (2012)
9. Xia, Z.Y., Yu, Q., Wang, L.: The public crisis management in micro-bloging environment:
take the case of dealing with governmental affairs via micro-blogs in China. Adv. Intell. Soft
Comput. 141, 627–633 (2012)
452
W. Huang et al.

Scholar Recommendation Model in Large Scale Academic
Social Networking Platform
Ming Chen1, Chunying Li2, Jiwei Liu1, Dejie Meng1, and Yong Tang1(✉)
1 School of Computer Science, South China Normal University, Guangzhou, China
mchencf@163.com, {cocoliu,2016022321,ytang}@m.scnu.edu.cn
2 School of Computer Science, Guangdong Polytechnic Normal University, Guangzhou, China
zqxylcy@163.com
Abstract. A scholar-recommended model based on community division is
established due to the characteristics of social intercourse of academic social
network. The model was developed by GraphChi, the single version of large-scale
graphic computing system which was launched by GraphLab, to ﬁnd the core
network in parallel on network topology map. In the established network, using
self-adaptive label transmission to create labels and then according to the number
of labels on the nodes to get ﬁnal results of community division. Calculation is
done within the community for expert recommendation services. The experiment
of data-set on academic social networking platform, SCHOLAT, suggests,
models not only can create community quickly, but also can gain good recom‐
mendation results by all the three personalized methods, i.e. Community Weight
Recommended (CWR), Community Random Recommended (CRR) and
Acquaintance Community Recommended (ACR).
Keywords: GraphChi · Community detection · Kernel sub-network
Label propagation · Scholars recommendation
1
Introduction
In recent years, with the advent of WeChat, micro-Bo, Facebook and other social
networking platform, large-scale social networks have developed rapidly. The greatest
charm of a social network is Social, and each social networking platform creates many
explicit or hidden circle of friends [1]. With the rapid development of intelligent termi‐
nals, a rapid growth of network data has been seen in social networking platform, so
people often encounter the problem of information overload when they ﬁnd interested
friends. Therefore, the provision of friends recommended in social networking platform
can help users more quickly and more accurately ﬁnd their potential friends. As a kind
of social platform, academic social networking platform also need a scholars recommend
system to help users brush selected interested scholars.
At present, the domestic and foreign scholars have proposed many friend recom‐
mendation algorithm to solve the mentioned above problems, which can be divided into
two categories. One is the friend recommendation algorithm based on the user’s existing
information. In the literature [2], a friend recommendation model is established by
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 453–464, 2018.
https://doi.org/10.1007/978-3-319-74521-3_48

obtaining user’s preferences and information that already exists on the social network
platform, and the model explores and recommends friends who may have intersects with
the user on the view of user’s existing information. The algorithm in [3] analyzes the
microblog content published by the user to discover the interest of the user, and then
according to the user’s interest to recommend it may be interested friends. The paper [4]
aimed at some friends recommend algorithms, but ignored the relationship between
individuals and timing factors, this paper proposed a method to model the temporal
behavior among users based on the extraction of the user’s existing information, so they
could found the neighbour collection that have the greatest inﬂuence on the current user,
and then merged the collection in-to a coordinated ltering recommendation algorithm
which based on probability matrix decomposition. In the literature [5], the non-topology
information were used to calculate the similarity among users, and then recommended
the potential friends who were similar to the target user according to the similarity
between users. Another friends recommendation algorithm was based on social network
topology modeling. In the literature [6], the separation degree of each node was calcu‐
lated by dividing the topological graph of the buddy relationship in the social network.
Based on the degree of separation, the algorithm divided the nodes with the same degree
of dissimilarity into the same community group, and then recommended nodes within
the same group reciprocally.
Considering the unique social nature of academic social networks [7–9], we propose
a scholars recommendation algorithm based on label community discovery, which is a
friend recommendation algorithm based on social network topology modeling. The
Label Propagation Algorithm (LPA) [10] proposed by Raghavan et al. in 2007 ﬁrstly,
which is a community discovery algorithm based on label propagation. LPA is relatively
simple. The algorithm ﬁrst assigns a unique label to all the nodes, and then ﬂushes the
labels of all the nodes until the convergence requirements are reached, and ﬁnally get
the non-overlapping community. In recent years, many scholars [11–13] have improved
the LPA algorithm from diﬀerent angles in view of its simple and high eﬃciency. For
the LPA can only be applied to the discovery of non-overlapping community problems,
Gregory proposed the Community Overlap Propagation Algorithm (COPRA) [14] algo‐
rithm in 2009, which was applicable to overlapping communities. Concerning the
problem that community detection algorithm based on label propagation in complex
networks has a pre-parameter limit in the real network and redundant labels, LI Chunyin
proposed the Adaptive Label Propagation Algorithm (ALPA) [15]. The algorithm used
the adaptive threshold to eliminate the unreasonable label in the iterative process, and
ﬁnally classiﬁed the node with the same label into a community. ALPA algorithm has
achieved good results, especially in the academic social network. But it is not ideal when
dealing with social networks with large user population and complex user relationships,
because the time complexity of the algorithm is relatively high in this case. To solve
this problem, this paper proposes Community Detection Based on GraphChi (CDBG)
algorithm which is a community discovery scholars recommendation algorithm based
on GraphChi system. CDBG uses the characteristics of GraphChi system to achieve
ALPA algorithm in parallel, and then uses the community clustering results to achieve
friend recommendations. This algorithm improves the response speed of friends recom‐
mendation.
454
M. Chen et al.

2
Recommended Approach
The main idea of the model is to explore the community according to the topology
diagram of the user relationship in the social network, and then recommend a friend in
the community. Finding out core networks and then building the community by tag
propagation is a better-performing and easy-to-implement approach. However, it is very
time-consuming to ﬁnd the core network in the massive graph data, which makes the
recommendation system unable to meet the user’s psychological response time. There‐
fore, it is very important to improve the search strategy of the core network. The strategy
of this paper that to use the concurrent mechanism of Graphchi system to split the
topography of the buddy relationship, and then ﬁnd out the core network from each part.
Each local topology to ﬁnd out the core network. As the number of local nodes is much
smaller than the whole, it improves the speed of ﬁnding the core network, thus improving
the response time of the whole recommendation system.
2.1
GraphChi
The Graphchi system is a stand-alone version of the large-scale graphing system intro‐
duced by GraphLab Labs. Although it is not based on the popular distributed architec‐
ture, the eﬃciency of this system is very high, and it is not inferior to the distributed
computing system [16] in dealing with large-scale data. The ingenious design allows
Graphchi to eﬃciently process large-scale map data. In order to speed up the eﬃciency
of the system, Graphchi load the data into memory. This approach is similar to other
large scale computing systems. Since the memory is very limited in stand-alone envi‐
ronment, it is impossible to load all the data into memory when it encounters large graph
data. So Graphchi ﬁrst cut into a number of small map, and then load the sub-graph data
into the memory for each calculation. Loading each subgraph one after the other can
complete an iteration. After several rounds of iterations, you can complete the task of
graph calculation. The above process is the main idea of Parallel Sliding Windows
(PSW).
The running process of PSW has completed the update of the sub graph data. Each
round diagram update process includes three main steps: the loading of sub-graph data,
the concurrent implementation of node update function, write back the updated sub-map
data. When Graphchi completes the three sequential steps mentioned above, it completes
the update of a subgraph. When all the P intervals are updated, a round of iteration of
the entire graph is completed.
As can be seen from the above algorithmic ﬂow, the key of Craphchi to handle
massive amounts of data in a single-machine environment is the data is sliced and then
stored in memory. Because only need to read a piece of information at a time, so long
as the external memory space is large enough to be able to adjust the value of P to make
the memory to meet the computing requirements. Graphchi uses a vertex-centric
programming model in which adjacent vertices pass messages between edges. Devel‐
opers only need to consider a single vertex update function, and Graphchi framework
to solve speciﬁc details such as vertex-parallel.
Scholar Recommendation Model
455

2.2
Community Detection Based on GraphChi (CDBG)
2.2.1
Establish Core Network
Deﬁnition 1: Complete graph. The topological graph of social network user relations
is represented by graph G = {V, E}, where V represents the set of user nodes in the
topology graph. E is the edge set of the topology graph, which represents a collection
of friend relationships among users. If graph Gs is a subset of graph G and any pair of
diﬀerent vertices in Gs have exactly edges connected, then Gs is called a complete graph.
Deﬁnition 2: Core network. In graph G = {V, E}, assume that A and B are any two
nodes in V that are not tagged. If A and B are the maximum degree and unmarked nodes
of each other, then the edges A to B are used to ﬁnd the complete graph for the initial
edge. If Gs ∈ G and there is no any complete graph Gt ∈ G making Gs ∈ Gt, then Gs is a
core network of graph G.
Deﬁnition 3: Community consolidation. There are two sets of Gi and Gj representing
two diﬀerent communities. Gi is a subset of Gj that represents the community Gj contains
the community Gi. The behavior of deleting the community Gi is referred to as merge
communities Gj and Gi.
The core network is the core unit of the community, and the nodes in the same core
network must be in the same community. Because this algorithm is based on the core
network of relationships to build the community, so the algorithm must ﬁrst look for the
core network. This is the initialization phase of the algorithm. Speciﬁc steps are as
follows.
1. Assign 0 to all nodes in V;
2. Select a node Ui with value of 0 in the data fragment, then ﬁnd the node Uj with the
largest degree and value of 0 in the adjacent node of the node Ui;
3. If the value of node Ui is 0 and Ui is the adjacency node with the largest degree of
Uj, then choose e (Ui, Uj) as the initial edge to ﬁnd the core network according to
Deﬁnition 2;
4. If the condition of step 3 is satisﬁed, then the node number with larger degree in Ui
and Uj is regarded as the core network number and assigned to all the nodes in the
core network found in step 3;
5. If the condition in Step 3 is not true, go back to Step 2 and select another node.
6. Repeat steps 2–5 until there is no change in the core network, and the process of
ﬁnding the core network is stopped.
According to the above-mentioned rules, the network topology diagram shown in
Fig. 1 is taken as an example to illustrate the process of establishing the core network.
According to the above process, a total of 2 core networks are found in the topology
map, which are the core networks of 4 and 7: M4 = (3, 4, 2, 5), M7 = (6, 7, 8). Finally,
the core network assigned to all the nodes, that is, 4 is assigned to the node 3, 4, 2, 5,
and the 7 is assigned to the node 6, 7, 8. In this algorithm, the node’s value is the label
of the node. Figure 1 shows the topology which has established the core network.
456
M. Chen et al.

Fig. 1. After the establishment of the core network
2.2.2
Label Update
When the core network is established, each node in the network is assigned a unique
value. The value on the node is the label of the node, which is the original label. The
value is also the number of the core network to which it belongs. The core network is
the inﬂuential circle of friends in the community, and the core of the topological rela‐
tionship of the community is the core network. Therefore, the algorithm spreads the
label with the core network as the center. In order to reduce the redundant tags and avoid
the excessive spread of tags and improve the stability of the algorithm, the algorithm
automatically removes some labels with less weight in the process of label propagation,
so as to self-adaptively propagate the notes. Speciﬁc rules are as follows.
1. The weight of each node in the core network is set to 1, which corresponds to the
original label of the node;
2. In each iteration, the label of any node v in V is updated with the following rules:
(a) The nodes v check the labels of it’s neighbors one by one. If v node and its
adjacent node va have the same label L1, update the label in v: L1 = Wv + (W1/d).
Where Wv is the weight of the label L1 in v, W1 is the weight of the label L1 in
the adjacent node, and d is the degree of v. If the adjacent node va has the label
L2 but the node v does not have the label, then label v as L2 = W2/d. Where W2
is the weight of label L2 in va;
(b) Assume that the number of labels is C. If C is greater than 1, the label with
weight less than 1/C is deleted. If all the label values satisfy the condition, the
label with the largest weight is retained and the remaining labels are deleted. If
the weight of the largest have more than one label, you can keep a random;
Scholar Recommendation Model
457

(c) Normalize the labels in node v, and ﬁnally make the sum of the weights in each
node equals to 1.
3. Repeat steps 2–4 until all nodes have at least one label;
4. Traverse all nodes and classify nodes with the same label as the same community.
If a node has more than one label, it is categorized into multiple communities;
5. Community consolidation based on Deﬁnition 3.
According to the above rules, now take the Fig. 1 as an example to update the label.
Eventually generated two communities, namely C1 = {1, 2, 3, 4, 5}, C2 = {6, 7, 8}. The
results of community detection are shown in Fig. 2.
Fig. 2. Results of community detection
2.3
Scholars Recommend
In a social network, users in the same community tend to have similar interests or areas
of work, and users in the same community are more likely to accept it. Therefore, the
recommendations in this paper are carried out in the divided communities. The CDBG
algorithm is used here to classify communities. After the community division, this model
uses three methods to make personalized recommendation, namely: Community Weight
Recommendation (CWR), Acquaintance Community Recommendation (ACR) and
Community Random Recommendation (CRR).
After the establishment of the scholar community using the CDBG algorithm, each
scholar may belong to several diﬀerent communities. In the CWR recommendation
mode, all the scholars who are in the same community as the target scholar A and not
the friends of A are sorted by weight from high to low. Then the former N highest weight
scholars recommended to the scholar A, which is a CWR recommendation. This recom‐
mendation is suitable for recommending scholars to Ph.D. and professors because we
assume that highly educated user groups are more likely to accept inﬂuential scholars
in the same community. The ACR recommended method selects the community with
the least number of nodes among the communities to which the target scholar A belongs,
and ranks the users of the community in descending order of weight, and then recom‐
mends the ﬁrst N highest weight scholars to the scholar A. This recommendation is
recommended for scholarships for Masters and below, as these users have limited
communication and academic skills and are more willing to accept scholars in acquaint‐
ance communities. For example, a college counselor is more willing to accept the college
458
M. Chen et al.

clerk and other college counselors in her acquaintance community than a well-known
scholar in a ﬁeld. CRR recommended method is the most simple. The CRR randomly
recommends nodes that are in the same community as Target A and whose weights are
higher than the speciﬁed value. This approach is suitable for those who have just joined
the academic social networking platform and friends and team information is scarce. It
is a cold start recommended way. Take the CWR method as an example, the process of
Scholar Recommendation is illustrated below:
Scholar Recommendation Model
459

3
Experimental Results
In this part, we ﬁrst introduce the data set used in the experiment, and then explain the
evaluation method of the experiment. Finally, we give the experimental results of CWR,
CRR and ACR, and analyze the experimental results.
3.1
Description of Dataset
The data set used in the experiment is the friend relationship data set of the academic
social network platform (SCHOLAT) on October 12, 2016. The data set records the
relationship between scholars and friends on the social network platform. After
denoising the data set, there are 5168 users whose information is disclosed and 22284
user relations.
3.2
Evaluation Method
In the experiment, we used four general indicators [17] to evaluate the recommended
results, namely: Precision, Recall, F-measure and MAP. These four methods are the
standard to measure the accuracy of the recommendation system, and reﬂect the accu‐
racy of the recommendation system to the speciﬁed users, where Precision is the accu‐
racy of the recommended system recommendations, and it can be deﬁned as follows:
P = 1
T
∑
u
Nt
L
(1)
Where T is the number of experimental test samples, and Nt is the number of recom‐
mended objects the user likes in each recommendation, and L is the recommended list
length in a recommendation.
Recall expresses the possibility that the user’s favorite object is recommended by
the recommended system, and it can be deﬁned as follows:
R = 1
T
∑
u
Nt
Au
(2)
Where Au represents the total number of objects in the test set that are accepted by
the recommended user.
F-measure is the weighted harmonic average of Precision and Recall, and when the
value is high, the test result is more eﬀective. It can be deﬁned as:
F1 =
(a2 + 1)P ∗R
a2(P + R)
(3)
When a = 1, the formula is the most common F1-measure, and it deﬁned as:
460
M. Chen et al.

F1 = 2 ∗P ∗R
P + R
(4)
The ﬁnal evaluation criterion is MAP, which reﬂects the average accuracy of the
recommended system and is the probability of recommending an object to be accepted.
it can be deﬁned as:
MAP = 1
T
T
∑
j=1
( L
∑
k=1
p(Ljk
)
mj
)
(5)
Where p(Ljk) represents the ratio of the number of users preferred in the ﬁrst K
recommended objects in the jth recommendation to k. j represents the number of objects
that the user likes in the jth recommendation. L represents the length of the recommended
list, and we get L = 10 and L = 5.
3.3
Result Analysis
In the data set described above, we use the community recommendation model proposed
in this paper to generate a total of 442 communities, with the largest community having
1986 user nodes and the smallest community having 3 nodes. The total time it takes to
generate the community is 1.508 s. In order to verify the validity of this model and
compare the advantages and disadvantages of CWR, CRR and ACR recommendations,
we calculate the Precision, Recall, F-measure and MAP by questionnaire survey. These
questionnaires are divided into two categories, one with a recommended length of 10
and another of 5; Table 1 gives the experimental results for L = 10 and L = 5. From
Table 1, we can see that no matter which kind of recommendation method, the recom‐
mended accuracy rate is above 55%, and the average accuracy MAP can be maintained
above 55%. Additionally, when the ACR recommended method is adopted, the accuracy
can reach 77.68% and the MAP can reach 81.63% under the recommended length L = 10.
In this way, when the recommended length L = 5, the accuracy is 84%, MAP is 85.84%.
This is a very good result, and at the same time, it proves the eﬀectiveness of the proposed
model in this paper.
Table 1. Comparison of experimental results.
Description
L = 10
Precision
Recall
F1-measure
MAP
CWR
0.5503
0.4825
0.5141
0.5634
CRR
0.6335
0.4994
0.5585
0.6228
ACR
0.7768
0.5007
0.6089
0.8163
Description
L = 5
Precision
Recall
F1-measure
MAP
CWR
0.5234
0.4670
0.4935
0.5709
CRR
0.6021
0.4969
0.5445
0.6359
ACR
0.8467
0.5093
0.6360
0.8584
Scholar Recommendation Model
461

By comparing the results in Table 1, it can be seen that the CWR recommendation
method has the worst performance among the three methods. There are two reasons for
this situation. First of all, in the scholar network social network platform, the weight of
a scholar is not exactly the same as the scholar has great inﬂuence, because some scholars
often use the scholar network platform for teaching, thus accumulating a large number
of students friends to increase their weight, but this weight does not aﬀect the choice of
other people; In addition, a highly qualiﬁed scholar does not guarantee that he is well
known by other scholars, nor can he guarantee that the direction of his research is to
accept the recommendation of users interested in. And the majority of users of social
networking platform is a master’s degree, so the user’s academic social and research
areas are relatively limited. So the CWR method is more suitable to recommend scholars
to doctoral and professors and other highly educated users. As can be clearly seen in
Table 1, the ACR method is most eﬀective. The reason for this result is that there is a
high possibility that users have a connection in acquaintance communities, so the user
is more inclined to accept a scholar in the community. In addition, by comparing the
two results of L = 10 and L = 5, it can be seen that there is no signiﬁcant diﬀerence in
the eﬀect of CRR between the two lengths. However, CWR perform better at L = 5, but
ACR at L = 10 gives better results. This just veriﬁes the above analysis.
Fig. 3. Results of community detection
Figure 3 shows the change in the recommended results with the length of the recom‐
mended list. As can be seen from the ﬁgure, Recall and F1-measure are more sensitive
to the recommended list length, while Precision and MAP are less aﬀected by the
recommended length. In addition, it can be seen from the diagram that the ACR method
can achieve better results under diﬀerent recommended lengths. It can be seen that the
use of ACR recommended in many cases can get the best results. But the ACR method
462
M. Chen et al.

obviously has his shortcomings. The acquaintance community recommendation can
improve the acceptance of the recommendation object by the user, but such recommen‐
dation lacks novelty and unexpectedness, and the coverage rate is low, so that the user’s
satisfaction can not be guaranteed. WRR also has these deﬁciencies. CRR is not the best,
but its randomness is high, and can provide enough freshness and accident, and have a
higher coverage rate. In summary, if the user is a doctor or professor, you can choose
CRR and CWR mode. If you want to recommend scholars to the MS and the following
user groups, the best choice is ACR.
4
Conclusion and Discussion
In this paper, we propose a community-based recommendation model for large-scale
social networks. The model ﬁrstly uses the graphchi framework to divide the community
and obtains the core network of the network structure. Then, the core network is used
as the core to transmit the tags. The nodes are updated by using the synchronous adaptive
label propagation mode. Eventually forming multiple communities and recommending
scholars in these communities. In the stage of recommending buddies, this paper
provides three methods, which are community weight recommended (CWR),
community random recommended (CRR) and acquaintance community recommended
(ACR). Through the analysis of the experiment, we can see that the overall eﬀect of
CRR is the best, and the eﬀect of ACR is the second.
The lack of the CRR approach in the recommended scholarly stage is precisely the
advantage of the ACR. In the next work, we intend to study how to combine the two
recommended ways to further improve the eﬀectiveness of recommendations.
Acknowledgment. 
This work is supported by the Applied Technology Research and
Development Foundation of Guangdong Province (No. 2016B010124008), the Science and
Technology 
Planning 
Project 
of 
Guangdong 
Province 
(Nos. 
2016A030303058,
2015B010109003).
References
1. Zhang, Z., Li, Q.: Questionholic: hot topic discovery and trend analysis in community
question answering systems. Expert Syst. Appl. 38, 6848–6855 (2011)
2. Colace, F., Santo, M.D., Greco, L., Moscato, V., Picariello, A.: A collaborative user-centered
framework for recommending items in online social networks. Comput. Hum. Behav. 51,
694–704 (2015)
3. Armentano, M.G., Godoy, D., Amandi, A.A.: Followee recommendation based on text
analysis of micro-blogging activity. Inf. Syst. 38, 1116–1127 (2013)
4. Sun, G.F., Wu, L., Liu, Q., Zhu, C., Chen, E.H.: Recommendations based on collaborative
ﬁltering by exploiting sequential behaviors. J. Softw. 24, 2721–2733 (2013)
5. Debnath, S., Ganguly, N., Mitra, P.: Feature weighting in content based recommendation
system using social network analysis. In: International World Wide Web Conference, pp.
1041–1042 (2008)
Scholar Recommendation Model
463

6. Silva, N.B., Tsang, I.R., Cavalcanti, G.D.C., Tsang, I.J.: A graph-based friend
recommendation system using genetic algorithm. In: Evolutionary Computation, pp. 1–7
(2010)
7. Gruzd, A., Staves, K., Wilk, A.: Connected scholars: examining the role of social media in
research practices of faculty using the UTAUT model. Comput. Hum. Behav. 28, 2340–2350
(2012)
8. Nández, G., Borrego, Á.: Use of social networks for academic purposes: a case study.
Electron. Libr. 31, 781–791 (2013)
9. Gu, F., Widén-Wulﬀ, G.: Scholarly communication and possible changes in the context of
social media: a ﬁnnish case study. Electron. Libr. 29, 762–776 (2011)
10. Raghavan, U.N., Albert, R., Kumara, S.: Near linear time algorithm to detect community
structures in large-scale networks. Phys. Rev. E Stat. Nonlinear Soft Matter Phys. 76(3 Pt 2),
036106 (2007)
11. Liu, X., Murata, T.: Advanced modularity-specialized label propagation algorithm for
detecting communities in networks. Physica A Stat. Mech. Appl. 389, 1493–1500 (2010)
12. Cordasco, G., Gargano, L.: Community detection via semi-synchronous label propagation
algorithms, pp. 1–8 (2011)
13. Xie, J., Szymanski, B.K.: Community detection using a neighborhood strength driven label
propagation algorithm. In: IEEE NSW, pp. 188–195 (2011)
14. Gregory, S.: Finding overlapping communities in networks by label propagation. New J. Phys.
12, 2011–2024 (2009)
15. Li, C., Huang, Y., Tang, Z., Tang, Y., Zhao, J.: Adaptive label propagation algorithm to detect
overlapping community in complex networks. Int. J. Future Gener. Commun. Netw. 9, 317–
326 (2016)
16. Kyrola, A., Blelloch, G., Guestrin, C.: GraphChi: large-scale graph computation on just a PC.
In: Usenix Conference on Operating Systems Design and Implementation, pp. 31–46 (2012)
17. Lichtenwalter, R.N., Lussier, J.T., Chawla, N.V.: New perspectives and methods in link
prediction. In: ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, Washington, DC, USA, July, pp. 243–252 (2010)
464
M. Chen et al.

Research on Simulation for Fuel Consumption of UAV
Zongpu Jia1, Yonghui Shi2, Songyuan Gu3(✉), and Shufen Liu3
1 College of Computer Science and Technology, Henan Polytechnic University, Jiaozuo, China
jiazongpu@126.com
2 China Shipbuilding Information Technology Co., Ltd., Beijing, China
shiyh@csit.net.cn
3 College of Computer Science and Technology, Jilin University, Changchun, China
gusongyuan614@163.com, liusf@mail.jlu.edu.cn
Abstract. To enhance the sense of reality of Unmanned Aerial Vehicle (UAV)
simulation, factors inﬂuencing fuel consumption of UAV are analyzed in this
paper in detail. On the basis of these factors, a fuel consumption model of UAV
is presented, and ﬁnally fuel consumption simulation in UAV simulation control
system could be implemented employing the presented model.
Keywords: Unmanned Aerial Vehicle · Fuel consumption · Simulation model
Energy balance
1
Introduction
Technologies concerning Unmanned Aerial Vehicle (UAV) areas developed signiﬁ‐
cantly in recent years, and UAV simulation appears great theoretical and applied impor‐
tance [1]. The instrument monitoring areas display the ﬂight status of UAV during
missions in real-time, the areas contain a series of data, e.g. fuel consumption, propeller
speed, propeller temperature, link state, etc. The series of data is of great references to
estimate whether the UAV is in normal ﬂight state. As important one of instrument
monitoring areas, fuel gauge displays the fuel consumption level of UAV in real time,
which is extremely valuable for ﬂight status monitoring and ﬂight mission planning [2].
A fuel consumption model of UAV on the basis of detailed analyzed inﬂuencing factors
in Sect. 2 is presented in Sect. 3. Fuel consumption simulation of instrument monitoring
areas in UAV simulation control system could be implemented employing the presented
model.
2
Inﬂuencing Factors Analysis of Fuel Consumption of UAV
2.1
Fundamental Aerodynamic Parameters
Aerodynamic parameters contain lift coeﬃcient, drag coeﬃcient, lift-drag ratio, etc.
Fundamental aerodynamic parameters of UAVs with diﬀerent types or diﬀerent
performance would vary as temperatures and speeds diﬀer. However, the correlations
among those fundamental aerodynamic parameters are complex and could hardly be
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 465–471, 2018.
https://doi.org/10.1007/978-3-319-74521-3_49

described in functions, and thus are measured by wind tunnel test or test ﬂight, which
are shown in Figs. 1 and 2.
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
0
2
4
6
8
10
12
14
16
18
20
X-Aircraft elevation
Y-Lift coefficient
Fig. 1. Relation curve of aircraft elevation and lift coeﬃcient
0
0.2
0.4
0.6
0.8
1
1.2
0
0.2
0.4
0.6
0.8
1
1.2
X-Mach number
Y-Drag coefficient
Fig. 2. Scatter plot of Mach number and drag coeﬃcient
2.2
Engine Performance Parameters
The performance of engine is linear with respect to the fuel consumption; the fuel
consumption of engine is determined by UAV type, speed and altitude. One of the most
important indexes is the Thrust speciﬁc fuel consumption (TSFC) [3]. The TSFC of
466
Z. Jia et al.

engine, which is diﬀerent from fundamental aerodynamic parameters, is a function of
speed, altitude and thrust.
2.3
Flight Trajectory
The ﬂight trajectory of UAV is shown on Head up display (HUD) [4], which includes
speed, altitude, pitch angle, roll angle, deﬂection angle, acceleration, course, etc. These
indexes are all factors inﬂuencing fuel consumption.
2.3.1
Speed
The speed to be considered here is the level ﬂight speed of UAV. There are two speed
units in aviation ﬁeld: the commonly used km/h, and the Mach number. Speed relates
directly to fuel consumption of UAV.
2.3.2
Pitch Angle
The fuselage of a UAV would be at a certain angle to horizontal while the UAV is
ascending or is descending. The UAV is nosing up during ascent, and the angle which
is designated positive is deﬁned as elevation angle; on the contrary, the UAV is nosing
down during descent, and the angle which is designated negative is deﬁned as depression
angle. When ascending, greater thrust is required to overcome the gravity and to generate
upward acceleration, as a consequence, the fuel consumption would increase; when
descending, the thrust reduces, the fuel consumption decreases accordingly [5, 6].
2.3.3
Roll Angle and Deﬂection Angle
The UAV would subject to centrifugal force, gravity and drag, and would tilt at an angle
when it makes a turn. The tilt angle is deﬁned as roll angle. Turning radius and turning
rate of an aircraft are determined by the roll angle. The deﬂection angle is the course
change caused by the nose of a UAV when turning. More thrust is needed to maintain
-6
-4
-2
0
2
4
6
-80
-60
-40
-20
0
20
40
60
80
X—Delection angle
Y—Roll angle
Fig. 3. Scatter plot of roll angle and deﬂection angle
Research on Simulation for Fuel Consumption of UAV
467

circling when the UAV makes a turn. Figure 3 shows a series of roll angles and deﬂection
angles sampled in a turning process.
2.3.4
Acceleration
The acceleration of an aircraft consists of horizontal acceleration and vertical acceler‐
ation. Figure 4 shows scatters of vertical acceleration and fuel consumption sampled
during UAV missions.
X—Vertical acceleration
Y—Fuel consumption
Fig. 4. Scatter plot of vertical acceleration and fuel consumption
2.4
Weight
The weight of a UAV is the sum of fuselage weight and load weight. The heavier the
UAV weighs, the greater thrust is needed, accordingly the fuel consumption increases.
When in a UAV mission, the UAV weight would reduce as the fuel consumption
increases. In fuel consumption calculation, therefore, the ﬂight process should be divided
into several stages, weight deduction in each stage is neglected, and thus simpliﬁed
calculation is proceeded minimizing calculation error in accordance with aforemen‐
tioned assumption.
2.5
Atmospheric Parameters
Atmospheric parameters mainly include temperature, density, pressure, etc. These
parameters are evenly distributed on horizontal surface, but are with considerable varia‐
tion on vertical surface. Temperature and pressure are main factors inﬂuencing fuel
consumption; standard atmosphere would be utilized to execute simulation in aviation
ﬁeld.
468
Z. Jia et al.

3
Computational Model for Fuel Consumption
Fundamental aerodynamic parameters are complex variables in factors inﬂuencing fuel
consumption discussed in Sect. 2: ﬁrstly, aerodynamic parameters diﬀer as UAV types
are diﬀerent; secondly, the correlations between diﬀerent aerodynamic parameters could
hardly be described in functions. Hence, aerodynamic parameters won’t be considered
in fuel consumption modeling in this paper.
Because of the linear relationships between engine performance parameters and
TSFC (relative with speed v, altitude h and thrust F), engine performance parameters
could be used to model fuel consumption.
Flight trajectory parameters relate most directly to the fuel consumption, speed, alti‐
tude, pitch angle, roll angle, etc. could be all used to model fuel consumption, and
therefore we further involve these parameters into the fuel consumption modeling.
Since the accurate acquisition and uncomplicated calculation of the fuselage weight
and the load weight of a UAV, we also involve the total weight of a UAV as one of
parameters into fuel consumption modeling.
Normally, atmospheric parameters have certain inﬂuence on fuel consumption.
However, due to the little critical diﬀerence of the altitude in level ﬂight, atmospheric
parameters have little relationship to fuel consumption modeling; hence atmospheric
factor won’t be considered in fuel consumption model establishment.
The following assumptions are presented based on principle of energy balance before
fuel consumption modeling:
1. The weight change caused by fuel reduction in UAV mission doesn’t aﬀect the fuel
consumption calculation;
2. The accelerated motion of UAV is deemed as uniformly accelerated motion, i.e. the
variations of speed and altitude are linear;
3. Energy generated from fuel consumption is all for UAV motion, ignoring energy
supply for other components at the same time. In fact, energy for other components
is provided by battery in UAV;
4. A complete UAV mission includes 5 stages, i.e. taxiing, climbing, cruising, landing
and taxiing. Though the fuel consumption in each stage is diﬀerent, since taxiing,
climbing and landing make up small proportions of time throughout the mission,
only fuel consumption in cruising would be taken into account when modeling.
Model description of fuel consumption of UAV on vertical surface employing the
principle of energy balance is shown in Fig. 5.
Let altitude be h, speed be v, acceleration of gravity be g, the UAV mass be m1, the
load mass be m2, thrust generated from engine be T, aerodynamic drag be f, lift be L,
pitch be α, roll angle be γ, and abscissa and ordinate of the UAV be x and y. Then the
following equation set could be set up
m × dv
dt = T × cos 𝛼−f −
(
m1 + m2
)
g × sin 𝛾
(1)
Research on Simulation for Fuel Consumption of UAV
469

m × v × d𝛾
dt = T × sin 𝛼+ L −(m1 + m2
)g × cos 𝛾
(2)
dy
dt = v × sin 𝛾
(3)
dx
dt = v × cos 𝛾
(4)
The energy height of the UAV is following
E = h + 1
2g × v2
(5)
The total energy equals to the product of the energy height and the UAV mass, thus
the state model based on principle of energy balance could be obtained as follows
⎧
⎪
⎪
⎨
⎪
⎪⎩
E = h + 1
2gv2
E = v × (T × cos 𝛼−f)
mg
x = v × cos 𝛾
(6)
The formula of fuel consumption Mf is as follows
dMf = CePdt
(7)
where Ce is the TSFC of UAV; P is the rated thrust of engine.
Therefore, the formula of fuel consumption and energy height is as follows
dE
dMf = dE∕dt
dMf∕dt = dE∕dt
CeP
(8)
Motion equation
Mechanical energy
Flight trajectory
Engine performance 
parameters
Flight fuel consumption
Fig. 5. UAV fuel consumption model on vertical surface
470
Z. Jia et al.

where the TSFC of engines varies according to aircraft types: the TSFC of an airliner is
generally 0.1–0.5; the TSFC of a ﬁghter is generally 0.7–1.5; and the TSFC of the UAV
in this paper is about 0.1. Fuel consumption of a UAV in cruising stage could be derived
from formula 3.6 and formula 3.8.
4
Conclusion
Factors inﬂuencing fuel consumption of UAV are analyzed in detail above all in this
paper, and then on the basis of these factors, a fuel consumption model of UAV is
presented. Employing the presented model, fuel consumption simulation in UAV simu‐
lation control system could be implemented.
References
1. Huang, K.M., Zhang, M.Y., Wang, T.: The design of UAV simulation model system based on
component modeling technology. Ship Electron. Eng. 12, 86–89 (2015)
2. He, Y.C., Liu, K., Shen, X.Y., et al.: Simulation study of aircraft fuel consumption estimates
model. Comput. Simul. 32(5), 33–36 (2015)
3. Shirley, C.M., Schetz, J.A., Kapania, R.K., et al.: Tradeoﬀs of wing weight and lift/drag in
design of medium-range transport aircraft. J. Aircraft 51(3), 904–912 (2014)
4. Perez-Rodriguez, D., Maza, I., Caballero, F., et al.: A ground control station for a multi-UAV
surveillance system: design and validation in ﬁeld experiments. J. Intell. Robot. Syst. 69(1–4),
119–130 (2013)
5. Zhang, Y.X.: Research on Mission Flight Path Planning and Optimization Method for Fixed-
wing UAV. Zhejiang University (2016)
6. Xu, M.X., Zhu, X.P., Zhou, Z., et al.: Exploring an eﬀective method of thrust allocation for
solar-powered UAV with multiple propellers. J. Northwest. Polytechnical Univ. 4, 505–510
(2013)
Research on Simulation for Fuel Consumption of UAV
471

Research on Meteorological Data Simulation
Zhenglun Wu, Shufen Liu, and Tie Bao(&)
Jilin University, Changchun, China
497564655@qq.com
Abstract. This paper proceeded with simulating the meteorological data. Based
on the meteorological data that has been collected, this paper puts forward the
simulation method where the meteorological data can be classiﬁed into discrete
data and continuous data which can be simulated respectively. By establishing the
mathematical model of meteorological variables and the employment of the rele-
vant knowledge, the simulation of meteorological data can be achieved. The
simulated relationship among meteorological data accord with the relationship
among the real data. In other words, the simulated result approach the real mete-
orological data to a certain extent. Moreover, this result guarantees the application
of the unmanned aerial vehicle in logistics, disaster relief, medical care etc.
Keywords: Meteorological data  Simulation  Mathematical model
Discrete
In UAV simulation training, the weather is a very important factor, Changes in weather
will greatly affect the operation and ﬂight of UAV in simulation environment. In order to
improve simulation level of the simulation training system, the simulation of meteoro-
logical data needs to be studied. The objective of Ref. [1] was to develop and validate a
simulation model of the evaporation rate of a Class A evaporimeter pan. Daily weather
data and three climatic variables for four cities in China were gathered, investigated and
analyzed, and the sensitivity of climatic variables on building energy consumption was
discussed [2]. An experimental embankment was constructed, and the ﬁrst objective is to
investigate the inﬂuence of climatic changes on the soil response such as changes in water
content and temperature [3]. The main contribution of Ref. [4] is an analysis of
requirements for a wind power infeed model used in a power system simulator from a
meteorological viewpoint. The WARMF model was applied to the Catawba River
watershed of North and South Carolina to simulate ﬂow and water quality in rivers and a
series of 11 reservoirs [5]. The data generated in Ref. [6], can be directly applied to the
EIA prediction model and serve for EIA. Edgar et al. [7] analyze the performance of the
physically based snow model SNOWPACK to calculate the snow cover evolution with
input data commonly available from automatic weather stations. A system for observing
meteorological data based on a wireless sensor network is designed to fulﬁll the business
requirements for meteorological data observation in unattended areas [8]. This paper puts
forward the simulation method where the meteorological data can be classiﬁed into
discrete data and continuous data which can be simulated respectively. This paper pro-
ceeded with simulating the meteorological data. Based on the meteorological data that
has been collected. Task deducing and simulated training are conducted by coordinating
with the comprehensive task control system and the result is satisfactory.
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 472–480, 2018.
https://doi.org/10.1007/978-3-319-74521-3_50

1
Meteorological Data Collection and Sorting
Under the condition that the way of establishing statistic model has been determined,
appropriate meteorological data can be collected according to the types of meteoro-
logical data which is required to simulate. Table 1 shows the daily meteorological data
of one of the districts in Changchun.
2
Initial Meteorological Data Analysis
Meteorological data variable mainly consists of six types which are meteorology,
temperature, relative humidity, air pressure, wind power, wind direction. According to
the meteorological knowledge, initial analysis will be conducted on the types of
variable as well as the value range. Atmospheric phenomenon generally includes the
sky condition, rainfall, snowfall which are usually described as ﬁne, rain, fog, snow.
Meteorology names physical quantity that measures the degree of the hot and cold as
Table 1. The daily meteorological data of one of the districts in Changchun
Time
Weather
condition
Temperature
(°C)
Relative
humidity
Air pressure
(hpa)
Wind
force
Wind
direction
8 am
Fine
21
56
991
2
South
9 am
Fine
23
47
991
1
South
10 am Fine
25
41
991
2
Southwest
11 am Fine
25
41
991
2
West
12 am Fine
27
39
990
0
North
13 pm Fine
27
37
990
2
Northwest
14 pm Fine
28
35
989
1
West
15 pm cloudy
27
38
989
1
Northwest
16 pm cloudy
28
39
989
0
North
17 pm Fine
25
45
989
0
North
18 pm Fine
21
50
989
0
North
19 pm Fine
19
64
989
0
North
20 pm Fine
18
73
989
0
North
21 pm Fine
17
76
989
0
North
22 pm Fine
16
79
990
0
North
2 pm
Fine
15
83
990
0
North
24 pm Fine
15
86
990
0
North
1 am
Fine
14
88
990
0
North
2 am
Fine
14
88
990
0
North
3 am
Fine
14
87
990
2
West
4 am
Fine
14
88
991
1
South
5 am
Fine
13
87
991
1
East
6 am
Fine
13
91
991
0
North
7 am
Fine
16
69
991
1
South
Research on Meteorological Data Simulation
473

air temperature, international standard temperature unit of measurement is Celsius.
Relative humidity is the speciﬁc value of absolute humidity and saturated absolute
humidity under the same air temperature, it is also a percentage. Air pressure is the
hydrostatic pressure on a speciﬁc point which is generated by air. The resource of the
air pressure is gravity of the atmosphere, known as atmospheric pressure on unit
square. The measurement unit is hpa. Meteorology deﬁnes the direction that the wind
blows from as wind direction which normally falls in to sixteen directions. Wind speed
is usually represented as the level of wind. Wind level is determined by the degree of
effect that the wind generates on the ground objects. In meteorology, wind power can
be classiﬁed in to thirteen levels. Two kinds of variables can be concluded based on the
initial analysis of these variables. The ﬁrst type of variable includes weather condition,
wind power and wind direction, their value ranges belong to set value which are
deﬁned as discrete variables. The second type of variable includes temperature, relative
humidity and air pressure. They are numeric variables which are deﬁned as continuous
variables.
3
Simulation of the Discrete Data
Markov process is such a kind of process: In the process of x(t), every transfer of the
state is related to the state of the previous moment but not the past state. In other words,
there is no after effect during the state transfer process, hence, such a state transfer
process is called Markov process. Markov chain is a discrete Markov process in the
time discretization state.
Markov chain is a sequence of random variables like X1, X2, X3, X4. The range of
these variables called state space is the set of all their possible values and the value of
Xn represents the state in time n. If Xn + 1 is a function of Xn with respect to the
conditional probability distribution of the past state, then
PðXn þ 1 ¼ xjX0; X1; X2; . . .; XnÞ = P(Xn þ 1 = xjXnÞ
This identical equation represents the characteristic of the Markov chain. In the
Markov chain, n-step transition probability has the following characteristic:
pðnÞ
ij
¼
X
k2I
pðlÞ
ik pðnlÞ
kj
This equation is called Norman-Kolmogorov Equation which solves the relation-
ship between n-step transition probability and one-step transition probability and its
matrix format is: Pðk þ mÞðnÞ ¼ PðkÞðnÞPðmÞðn þ kÞ.
Then we take an example of the weather condition to illustrate the process of the
data generation. Firstly, we assume that weather condition at time a + 1 is only cor-
related to the weather condition at time a but has nothing to do with the previous
weather condition. This process is consistent with Markov process, then Markov Chain
can be used to calculate probability transition.
474
Z. Wu et al.

3.1
Generate Initial Value
Statistics of the Meteorological data can help ﬁgure out the frequency of each kind of
weather condition and calculate the probability of each kind of weather condition, the
results are shown in Table 2.
Divide the value range of the random number into corresponding interval according
to the probability and then stochastic number p is generated. Then, the interval of the
stochastic number p can be estimated while the initial value of the meteorology can also
be determined. For example, stochastic number range [1–100] can be divided into ﬁne
[1,69], partly cloudy [70,86], cloudy [87,93], overcast [93,97], rainy [98,100]. The
generated stochastic number 19 suggests that the initial value of the weather condition
is ﬁne.
3.2
Calculate Probability Transition Matrix
Then statistics of the meteorology data also contributes to obtaining the transition
frequency and the matrix F.
F
fclear to clear
fclear to partly
fclear to cloudy
fclear to overcast
fclear to rainy
fpartly to clear
fpartly to partly
fpartly to cloudy
fpartly to overcast
fpartly to rainy
fcloudy to clear
fcloudy to partly
fcloudy to cloudy
fcloudy to overcast
fcloudy to rainy
fovercast to clear
fovercast to partly
fovercast to cloudy
fovercast to overcast
fovercast to rainy
frainy to clear
frainy to partly
frainy to cloudy
frainy to overcast
frainy to rainy
2
6666664
3
7777775
¼
74
6
3
0
0
3
14
1
1
1
0
4
3
0
1
0
0
2
3
0
0
0
2
0
1
2
6666664
3
7777775
Then, transition probability matrix p can be generated according to the matrix F.
P ¼
0:89
0:07
0:04
0
0
0:15
0:7
0:05
0:05
0:05
0
0:5
0:375
0
0:125
0
0
0:4
0:6
0
0
0
0:66
0
0:34
2
66664
3
77775
By employing Norman-Kolmogorov Equation, we can calculate n-step transition
probability matrix Pð1Þ; Pð2Þ; . . .; Pð23Þ.
Table 2. The frequency and probability of each kind of weather condition table
Weather condition Fine
Partly cloudy Cloudy Overcast Rainy
Frequency
83
20
7
5
3
Probability
69.1% 16.6%
5.8%
4.1%
2.7%
Research on Meteorological Data Simulation
475

3.3
Generate Data from Transition Probability
The way of generating the weather condition at time n is similar to the way of gen-
erating the initial value of the weather condition. Divide the value range of the
stochastic number into corresponding intervals according to the transition probability
of the initial value in matrix PðnÞ, and then stochastic number p and its interval can be
obtained. Hence, the value can be determined.
4
Continuous Data Simulation
Linear regression analysis is the common method of establishing meteorology appli-
cation model. The relationship exists among many of the meteorology variables through
relevant meteorology knowledge. Moreover, linear regression analysis is thoroughly
complete from model establishment to authentication method theory. Meteorological
data can be forecast and analyzed by employing this model.
For continuous variables, existing meteorological data can be used to conduct
multiple linear regression analysis and establish the mathematical relationship among
meteorological data. Substitute the generated variables into the equation in order to
generate continuous variables. The following temperature example is to introduce the
process of generating continuous variables by employing the multiple linear regression
analysis. By screening out the factors, choose time and weather condition as the
dependent variables in the regression analysis of temperature, the scatter diagram of
time and temperature is shown as Fig. 1.
From the above Fig. 1, temperature is in a steadily increasing trend from 2 pm to
2 am; However, temperature is decreasing from 3 am to 1 pm. Thus, it is necessary to
conduct multiple linear regression analysis on these two time periods.
Fig. 1. Temperature and time scatter ﬁgure
476
Z. Wu et al.

We need to endow meteorological value with numerical meanings in order to
quantify the factors, the result is shown as Table 3.
The following explanation is the process of establishing regression model of
temperature from 2 pm to 2 am:
y ¼ b0 þ b1x1i þ b2x2i þ ei ;
i ¼ 1; 2; 3 . . . n
n is the number of value
ð
Þ
where ei is mutually independent and follow normal distribution N(0, ơ2).
The 65 times records can be written as:
y1 ¼ b0 þ b1x11 þ b2x12 þ e1
y2 ¼ b0 þ b1x21 þ b2x22 þ e2
. . .. . .
yn ¼ b0 þ b1xn1 þ b2xn2 þ en
8
>
>
<
>
>
:
9
>
>
=
>
>
;
If transfer this equation set into matrix, then: Y = XB + Ɛ
Where matrix:
Y ¼
y1
y2



y65
8
>
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
>
:
9
>
>
>
>
>
>
>
>
>
>
>
=
>
>
>
>
>
>
>
>
>
>
>
;
¼
28
28



15
8
>
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
>
:
9
>
>
>
>
>
>
>
>
>
>
>
=
>
>
>
>
>
>
>
>
>
>
>
;
X ¼
1
x11
x12
1
x21
x22









1
x65
1
x65
2
8
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
:
9
>
>
>
>
>
>
>
>
=
>
>
>
>
>
>
>
>
;
¼
1
14
1
1
15
1









1
26
1
8
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
:
9
>
>
>
>
>
>
>
>
=
>
>
>
>
>
>
>
>
;
B ¼
b0
b1
b2
8
>
>
<
>
>
:
9
>
>
=
>
>
;
e ¼
e0
e1



e65
8
>
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
>
:
9
>
>
>
>
>
>
>
>
>
>
>
=
>
>
>
>
>
>
>
>
>
>
>
;
For the equations above, we can use the least square method to ﬁnd the solutions.
The fundamental principle of the least square method is to ﬁnd b0, b1, … bn which can
minimize the residual sum of square between observation and regression value. It is
equivalent to ﬁnd the solutions of the equations:
@
@^b
Y  X^b

0
Y  X^b


¼ 0
@
@^b
Y0Y  ^b0X0Y  Y0X^b þ ^b0X0X^b


¼ 0
@
@^b
Y0Y  2Y0X^b þ ^b0X0X^b


¼ 0
 X0Y þ X0X^b ¼ 0
X0Y ¼ X0X^b
^b ¼ X0X
ð
Þ1X0Y
We can obtain the estimations b0, b1, b2 of b0, b1, b2 which are 42.9, −1.08, −0.85.
Therefore, the regression equation of temperature is: y = 42.9 −1.08x1 −0.85x2.
Then, it is required to proceed with the test of signiﬁcance of the regression equation.
Table 3. Meteorogical quantify table
Weather condition Fine Partly cloudy Cloudy Overcast Rainy
Value
1
2
3
4
5
Research on Meteorological Data Simulation
477

4.1
Certainty Coefﬁcient of the Equation
The certainty coefﬁcient of the equation represents the explanation degree of inde-
pendent variables against dependent variables, the certainty coefﬁcients of the regres-
sion equation are shown as Table 4:
From the diagram, it can be known that the coefﬁcient of R is 0.905, the coefﬁcient
of R2 is 0.819, the coefﬁcient of adjusted R2 is 0.813, we have to preferentially consider
adjusting the coefﬁcient of R2. Since 0.813 is bigger than 0.05 and approaches 1, the
explanation degree of the independent variables against dependent variables is high.
4.2
Signiﬁcance Test of the Linear Relationship of the Regression
Equation
Variance analysis is a way of decomposing the sum of squares of deviations and its
degree of freedom and examining the linear relationship between independent and
dependent variables by employing statistical magnitude F. The variance analysis is
shown as Table 5:
Assume:
H0 : b1 ¼ b2 ¼ 0
H1 : b1; b2 at least one nonzero


We can identify signiﬁcance level a = 0.05 and ﬁnd the rejection region. We ﬁnd
F0.05(2, 60) = 3.15 by referring to the table,
F ¼ 150:536 [ F0:05 2; 60
ð
Þ [ F0:05 2; 62
ð
Þ
By rejecting H0 and assuming to accept H1, we can believe that the linear rela-
tionship between independent variables and dependent variables is signiﬁcant.
Table 4. The deterministic coefﬁcient of the regression equation table
Item
R
R2
Adjusted R2
Value 0.905 0.819 0.813
Table 5. The variance analysis table
Source of variance Sum of square Degree of
freedom
F value
Regression
1037.914
2
140.536
Residual value
228.947
62
Total
1266.862
64
478
Z. Wu et al.

4.3
Signiﬁcance Test of the Regression Coefﬁcient of the Regression
Equations
The purpose of signiﬁcance test is to identify the signiﬁcant linear relationship between
independent variable and dependent variable by testing the signiﬁcant difference
between regression coefﬁcient and 0.
t-test on regression coefﬁcient:
Given level of signiﬁcance a = 0.05, calculate the value of the test statistic t:
t ¼
Estimation of regression coefficients
The standard deviation of the regression coefficients
t1 = −16.698, t2 = −3.477.
ta/2(n−m−1) = ta/2(62) = 1.999, |t1| > ta/2(62), |t2| > ta/2(62). Reject null hypothe-
sis, then linear relationship between independent variable and explained variable is
signiﬁcant which should be in the equation.
4.4
Introduce Normal Distribution Error
The result generated from the formula is obviously the ideal value. Meteorological data
has only several sets of values since ﬁxed independent value will also contribute to
ﬁxed dependent variable. In order to make the data more closed to the real value,
random error of normal distribution would be introduced once the value of variable has
been generated. Box-Muller is a widely used calculation method which is normally
distributed. It is efﬁcient and simple in calculating. In Box-Muller method, two evenly
distributed random numbers n1, n2 whose value range is (0,1) are required:
N ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 lnðn1Þ
p
cosð2pn2Þ:
Generate a normally distributed random number N (0,1). A normally distributed
random number whose mean value is a and standard error is sd can be obtained by
doing the following transformation: X = a + (N * sd).
5
Conclusion
This paper mainly focuses on the simulation method of the meteorological data. These
meteorological data can be classiﬁed into discrete variable and continuous variable
according to their own characteristics and respectively simulated. This paper begins by
collecting and analyzing the data. Then, Markov model is introduced to generate
discrete data. Finally, multiple regression model is required to e analyze both generated
variables and processing continuous variables. This paper provides a reliable data
simulation method for the application of UAV in logistics, disaster relief, medical care.
Research on Meteorological Data Simulation
479

References
1. Molina Martínez, J.M., Martínez, A.V., González-Real, M.M., Baille, A.: A simulation model
for predicting hourly pan evaporation from meteorological data. J. Hydrol. 318(1), 250–261
(2006)
2. Gao, Q., Liu, J., Yang, L.: Sensitivity studies on elements of meteorological data for building
energy simulation in China. In: International Building Performance Simulation Association,
pp. 217–222 (2007)
3. Cui, Y.J., Gao, Y.B., Ferber, V.: Simulating the water content and temperature changes in an
experimental embankment using meteorological data. Eng. Geol. 114(3), 456–471 (2010)
4. Brose, N.: Speciﬁcation of meteorological data requirements for a wind power infeed model
used in power system simulator. In: 12th International Conference on Environment and
Electrical Engineering, pp. 140–144 (2013)
5. Herr, J.W., Vijayaraghavan, K., Knipping, E.: Comparison of measured and MM5 modeled
meteorology data for simulating ﬂow in a mountain watershed. J. Am. Water Resour. Assoc.
46(6), 1255–1263 (2010)
6. Wang, Q., Li, S., Ding, F., Zhao, X.: Simulation of high-altitude meteorological data used to
environment impact assessment by MM5 model. Procedia Environ. Sci. 2, 1713–1716 (2010)
7. Edgar, S., Christoph, M., Charles, F., Michael, L.: Evaluation of modelled snow depth and
snow water equivalent at three contrasting sites in Switzerland using SNOWPACK simulations
driven by different meteorological data input. Cold Reg. Sci. Technol. 99, 27–37 (2014)
8. Reza, A., Zhiliang, Z., Shuang, Z.: Design and simulation of a meteorological data monitoring
system based on a wireless sensor. Int. J. Online Eng. 12(5), 27–32 (2016)
480
Z. Wu et al.

Anti-data Mining on Group Privacy Information
Fan Yang1, Tian Tian1, Hong Yao1, Xiuyu Zhao1(✉),
Tinggang Zheng1, and Min Ning2
1 Computer School, China University of Geosciences, Wuhan 430074, China
planesail@163.com, {tiantian,yaohong}@cug.edu.cn,
2829936010@qq.com, 2289067224@qq.com
2 Founder International Software (Beijing) Co., Ltd., Beijing 100080, China
ningmin@founder.com
Abstract. In the big data era, privacy preserving is a vital security challenge for
data mining. Common object of privacy preserving is personal privacy, which
should be kept unrevealed while data mining on group information. However, for
a few sensitive groups, such as suﬀering from some particular disease, engaging
in some special occupation or having some peculiar hobby, even if every personal
data is processed for privacy preserving, group speciﬁcity can be still exposed.
Therefore, we propose the concept and method of anti-data mining on group
privacy information. By adding, swapping data according to our rules, the minable
characteristic and group speciﬁcity of original data is destroyed and eliminated
to prevent group privacy from data mining.
Keywords: Anti-data mining · Group privacy information
Privacy preserving · Group speciﬁcity
1
Introduction
The concept of “big data” has appeared in the ﬁeld of physics, biology, environment
ecology, ﬁnance and communication for several years. In 2011, the well-known
consulting company Mckinsey predicted that “the era of big data” was coming for the
ﬁrst time [1]. Big data is a term for data sets that are so large or complex that traditional
data processing application software is inadequate to deal with them [2]. The lifecycle
of big data includes data extraction and integration, data analysis and interpretation,
among which data analysis is the core [3]. Diﬀerent from conventional data, big data
possesses the features of “4V”, which are Volume, Velocity, Variety and Value [4].
Consequently, the common ways of data analysis aren’t suitable for big data anymore.
Data mining is deﬁned as the procedure of extracting or excavating useful knowledge
from vast data stored in database [5]. Pattern and feature contained in big data is so
valuable that almost all industries, such as enterprises, telecom operators and govern‐
ments, are engaged in data mining. Sometimes, science and technology is a double-
edged sword. Data mining on big data brings forth both mass valuable information and
huge privacy leakage risk. The data mining algorithms are bound to collect abundant
users’ data for a long term to conclude the behavioral habits behind [6].
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 481–491, 2018.
https://doi.org/10.1007/978-3-319-74521-3_51

The concept of privacy preserving data mining (for short, PPDM) was ﬁrstly
presented in [7] to resolve the conﬂict between the precise excavation of knowledge rule
and the privacy protection of original information. Lindell indicated the need to protect
data privacy while ensuring data accuracy [8]. Evﬁmeievski used randomization to
establish an accurate data mining model for aggregated data [9]. In 2004, Vaidya intro‐
duced how data mining should be changed to accommodate the attacks of privacy advo‐
cates [10]. To solve the problem of constructing aggregated data without accurate data,
Zhang Nan describes the related technologies such as reconstruction [11]. By 2005,
Vaidya has proposed a relatively systematic idea and solution in [12], including a variety
of corresponding models. Lindell focused on the decision tree algorithm, especially the
ID3 algorithm [13]. Most recently, Saranya conducted extensive surveys of diﬀerent
PPDM algorithms and analyzed the representative technologies [14]. In addition, since
PPDM is a highly integrated cross-cutting topic, there have been signiﬁcant progress
for PPDM in statistics, machine learning, etc. [15–17]
Common object of PPDM is personal privacy, which should be kept unrevealed
while data mining on group information. But for a few sensitive groups, such as suﬀering
from some particular disease, engaging in some special occupation or having some
peculiar hobby, even if every personal data is processed for privacy preserving, group
speciﬁcity can be still exposed. Group privacy refers to the private information shared
inside the group, but unwilling to reveal to ones outside the group. Anti-data mining
indicates destroying the minable specialty of raw data and invalidating data mining to
protect the covert information contained in big data.
The rest of this paper is formed as following. Section 2 lists the technological clas‐
siﬁcation of PPDM, compares existing corporate privacy and anti-data mining to ours.
Section 3 explains our concrete algorithms. Section 4 conducts a series of experiments
to state the validity of above algorithms. Section 5 draws a conclusion.
2
Background and Related Work
2.1
Classiﬁcation of PPDM
According to the mainstream technologies of the raw data transformation, PPDM can
be classiﬁed into ﬁve dimensions [18]:
(i) Data distribution
(ii) Data modiﬁcation
(iii) Data mining algorithms
(iv) Data or rule hiding
(v) Privacy preservation
The ﬁrst dimension of data distribution can be classiﬁed as centralized data and
distributed data scenarios [19]. Distributed data scenarios can also be divided into hori‐
zontal data distribution [20–22] and vertical data distribution [23–25]. Generally, the
raw data needs to be modiﬁed before releasing to the public for privacy. The data
modiﬁcation of second dimension is divided into perturbation [26, 27], blocking [28],
aggregation/merging, swapping [29] and sampling [30]. The third dimension refers to
482
F. Yang et al.

the data mining algorithms, among which relatively important ones are decision tree
inducers [31], association rule mining [32, 33], clustering algorithms [34], rough sets
[35] and Bayesian networks [36]. The fourth dimension refers to whether raw data or
aggregated data should be hidden. To achieve higher utility and protect privacy, selective
modiﬁcation is essential. The last dimension of privacy preservation to modify data
includes heuristic-based techniques [37], cryptography-based technique [38], recon‐
struction-based techniques [39] and so on.
2.2
Group Privacy and Anti-data Mining
In China, a piece of news raised wide concern that 275 HIV-infected patients from 30
provinces declared the reception of fraud calls for the leak of authoritative AIDS data‐
base in July 2016 [40]. For this special group, any sensitive item revealed from the
database may label members as HIV or AIDS, probably resulting in discrimination,
losing job, even suicide. Even individual data is processed for privacy preservation,
message from group members still carries the group speciﬁcity. Once the mass fragments
are excavated and pieced together as integrated information, the group privacy will be
exposed in the end. Consequently, protecting the individual privacy of group is far from
enough. For this reason, we propose the protection on group privacy information.
The traditional PPDM means learning the group pattern and assuring the individual
privacy secret by limited data mining [41], while our anti-data mining prevents the group
speciﬁcity from data mining to protect group privacy. “Corporate privacy” mentioned
in [42] protects data from distributed sources by secure multiparty computation instead
of the trusted third party, while we centrally pre-process the raw data to hold back data
mining on group privacy. Diﬀerent from the anti-data mining in [43, 44], where noise
data is added to personal information, our method is applied to group attributes.
3
Anti-data Mining on Group Privacy
3.1
Scenario Description
We present anti-data mining on group privacy to solve the dilemma from the following
scenario. There are many active social networks based on common hobby, belief or
anything else. For example, there exists a network community among which are all AIDS
patients, meeting the social need for this special group. But this community is quite
vigilant against outside for the real world still discriminates them. Existing data mining
tools may collect the google searching records to ﬁnd out the keyword (AIDS) of this
community. Here comes the requirement that protects the common feature of this group.
Our work pre-processes the searching records so that both the needs of acquiring infor‐
mation and protecting group privacy are met.
Anti-data Mining on Group Privacy Information
483

3.2
Method Feasibility
Group speciﬁcity distinguishes group members from other random members, implying
the existing of a special group and the common privacy. Once we lessen or eliminate
the speciﬁcity, the group privacy can be protected. On the premise of not obstructing
the regular communication with outside, we pre-process the data issued by group
memberships. The material methods are adding and swapping according to some rules
so that it can’t be clustered as before. At last, the transformational searching records lose
their original group speciﬁcity completely. The more approximate are the results to
searching records of random user, the greater eﬀect does we get.
Our model is the agency of searching engine between group and outside Internet,
preprocessing the searching requests from group individuals and returning the results
back by converse progress. The preprocessing aims to convert the searching records
with obvious group speciﬁcity into so random ones that data mining on such records
can’t get the group characteristics about AIDS. Meanwhile, the eﬀect of information
retrieval remains insusceptible, i.e., the converse process can ﬁlter the rough searching
results into the interested information about AIDS for group individuals from seemingly
random result pool.
3.3
Relevant Deﬁnition
Clustering is a division of data into groups of similar objects. Each group, called cluster,
consists of objects that are similar between themselves and dissimilar to objects of other
groups [45]. The similarity is measured by the distances between the described objects.
As searching record is the main study object of our thesis, complex text clustering need
calculation of nonmetric similarity function. So, we introduce Jaccard index. Given two
sets, A and B, the Jaccard index is deﬁned as the ratio of intersection and union about
A and B.
Deﬁnition 1. Point of Searching Record. Point of searching record refers to the set of
searching keywords that the individual of special group submits in Internet, denoted as
PSR as below. There are n searching records in PSR, denoted as ri.
PSR =
{
r1, r2, ⋯, rn
}
(1)
Deﬁnition 2. Point Similarity. Point similarity of two PSRs are deﬁned as the Jaccard
index of two sets. For set A and B,
PS(A, B) = |A ∩B|
|A ∪B| =
|A ∩B|
|A| + |B| −|A ∩B|
(2)
Deﬁnition 3. Clustering Score. Clustering score indicates the proportion of special
individuals in clustering result vs the random distribution. Suppose that n is the number
of target records in cluster after clustering, s is the size of result cluster, ra is the ratio of
total target record in total searching records. We use the following equation to normalize
the clustering score into the interval of [−1,1].
484
F. Yang et al.

CS =
⎧
⎪
⎪
⎨
⎪
⎪⎩
n
s −ra
ra
whenn
s ≺ra
n
s −ra
1 −ra
whenn
s ≥ra
(3)
Deﬁnition 4. Average Clustering Score. Average clustering score indicates the average
of absolute value sum about all result clusters. Assume that there are m ﬁnal clusters,
then ACS is deﬁned as below.
ACS =
||CS1|| + ||CS2|| + ⋯+ ||CSm||
m
(4)
3.4
Algorithm Description
Diﬀerent to the common data mining, anti-data mining makes the result of clustering
unapparent by variation process. Our method follows the general approach of data
mining except the process of variation, indicating the core of group privacy preservation.
By diﬀerent clustering algorithms, patients of same kind tend to gather together into one
or a few clusters unequally. We grade the clustering result, rank and vary it, after which
it’s graded again to judge whether the mixing is uniform. If not, we loop to vary and
cluster again, or else output the result. In ideal condition, the target patients spread evenly
in all result clusters, indicating the data mining on group privacy is invalid.
3.4.1
Clustering
We use clustering algorithm to testify the eﬀect of anti-data mining. The distance calcu‐
lation of two points is the proportion of common entries to total entries in the clustering
procedure as the Jaccard index in Eq. (2). The central point is a virtual and auxiliary one
containing the most frequently used entries, whose length is the average of all points
inside a cluster. This parameter is used frequently in the following steps.
3.4.2
Grading
Grading refers to the hidden extent of target expressed by the uniformity degree distrib‐
uted in every cluster. Based on the expectation distribution of targets in the cluster,
calculate the deviation coeﬃcient to the expectation. The calculation follows Eq. (3),
where the result CS falls into the interval of [−1,1]. The value is closer to 1, the better
the clustering eﬀect is. The result of 1 means all records in the cluster are target. The
value is closer to −1, the worse the clustering eﬀect is. The result of −1 means no records
in the cluster are target. But for anti-data mining, the perfect result is 0, meaning ﬁnal
target after clustering approximates to the random distribution.
Anti-data Mining on Group Privacy Information
485

3.4.3
Variation
Before variation, rank the clusters in descending order according to the above grades.
The variation consists of two situations: adding and swapping. In the former adding,
match a pair clusters of the correspondent high and low grades every time. Then,
randomly select a keyword of central point from the other to add into one of its own real
records. If the keyword number of the added record is more than the maximum allowable
value, then replace the newly added one. By adding uncorrelated noise to change the
clustering characteristics of original records, the records belonging to one or a few clus‐
ters intensively can be randomly scattered into clusters as many as possible. In the latter
swapping, exchange a keyword pair from two neighboring clusters every time, where
the two keywords both come from the central point sets of each cluster. By reducing the
occurrence of high-frequency words, the clustering results can be altered.
4
Experiments
4.1
Construction of Experimental Data
At the beginning, two dictionaries are built artiﬁcially. One includes 100 entries about
AIDS keywords, such as HIV, homosexuality, incubation period, sex and so on, named
as target dictionary. The other includes 400 entries selected from everyday vocabulary
randomly, named as common dictionary. Next, we should construct ten original clusters,
where the ﬁrst cluster is all about AIDS and the other nine are non-AIDS. The AIDS
cluster contains 1000 records, where each record consists of one to twenty entries picked
from the target dictionary. Other nine original clusters are built as above from the
common dictionary.
4.2
Disposal of Anti Data Mining
Using k-means algorithm on 10000 records from above ten original clusters, in which
k (k = 5) indicates the number of result clusters.
Table 1. Clustering condition before variation (adding only).
Generation
Cluster 1
Cluster 2
Cluster 3
Cluster 4
Cluster 5
ACS
0
4806/308
693/0
2612/0
1197/0
692/692
0.871827
0
5396/332
2915/0
680/0
341/0
668/668
0.8769458
0
4707/310
2615/0
306/0
1682/0
690/690
0.8682812
0
5033/1000
1043/0
1978/0
623/0
1323/0
0.8219308
0
4795/326
2043/0
1519/0
969/0
674/674
0.864025
0
4867/1000
2777/0
387/0
682/0
1287/0
0.8234368
0
5850/1000
509/0
327/0
2228/0
1086/0
0.8157644
0
5120/1000
2328/0
1582/0
301/0
669/0
0.8211806
In Experiment 1, we gather statistics of running generation, target distribution in 5
result clusters and Average Clustering Score on eight independent tests, given the ending
486
F. Yang et al.

condition of ACS ≤ 0.1. In this experiment, we test our method on two variation ways:
adding only vs both adding and swapping. Tables 1 and 2 list the clustering conditions
before and after variation (adding only). Figures 1 and 2 illustrate the target distributions
in clusters before and after variation (adding only).
Table 2. Clustering condition after variation (adding only).
Generation
Cluster 1
Cluster 2
Cluster 3
Cluster 4
Cluster 5
ACS
7
2517/265
1749/175
2771/315
1401/105
1562/140
0.075076
3
2986/318
1297/100
2770/287
1569/143
1378/152
0.0680522
5
4252/446
1256/118
2479/269
1489/113
524/54
0.0639792
5
3069/334
3038/320
1918/173
1088/101
887/72
0.0747442
5
3414/378
2009/209
1387/142
405/26
2785/245
0.0994694
6
2833/287
2239/105
1277/112
1163/93
2488/303
0.0866714
5
4045/459
2507/243
821/68
1848/157
779/73
0.086152
6
4027/347
2825/274
1592/184
780/101
776/94
0.0483918
Fig. 1. Distribution of target before variation
Fig. 2. Distribution of target after variation
Comparing Tables 1 and 2, before variation, the clustering gathers the target records
into one or two clusters intensively and the ACS approximates 0.9 indicating the distri‐
bution is uneven. After variation, every cluster has target records and the ACS approx‐
imates 0.1. As our experimental data includes 1000 target records and 9000 normal
records, 0.1 implies so random distribution that data mining is without eﬀect on group
privacy. We choose the data of last line in Tables 1 and 2, showing the distributions in
histograms of Figs. 1 and 2. From these two ﬁgures, we ﬁnd before variation, the sizes
between clusters are quite diﬀerent. While after variation, every cluster is more similar
in scale than before and the target records have uniform distributions.
Table 3 lists the clustering condition after variation (both adding and swapping). For
this variation includes adding and swapping, the eﬀect is more obvious than Table 2 in
that the ACS and running generations are less. In particular, adding only is suitable for
the relative centralization of target data in early variation, while adding and swapping
is appropriate for the relative uniformity of target data in late variation.
Anti-data Mining on Group Privacy Information
487

Table 3. Clustering conditions after variation (Adding and Swapping).
Generation
Cluster 1
Cluster 2
Cluster 3
Cluster 4
Cluster 5
ACS
3
3548/365
567/48
3270/346
1019/94
1596/147
0.0639128
5
2764/251
2239/207
3458/382
1003/118
536/42
0.0830066
6
4099/392
826/95
2046/206
1531/192
1498/115
0.0643302
5
2053/195
2436/241
3241/379
1178/99
1092/86
0.0903424
6
2402/238
2268/232
2507/286
1927/167
896/77
0.060269
3
2684/307
2355/261
1618/130
2290/206
1053/96
0.082661
4
2773/289
2129/187
2370/212
1608/190
1120/122
0.0523848
5
3432/391
2837/294
1499/153
1341/73
891/89
0.095712
In Experiment 2, we gather statistics of running generation, target distribution in 5
result clusters and Average Clustering Score on eight independent tests, given the ending
condition of running generation = 10. In this experiment, we test our method on two
variation ways: adding only vs both adding and swapping. Table 4 lists the ACS of
adding only vs adding and swapping. Comparing the two columns, we can ﬁnd adding
and swapping is better than adding only for ACS is much less in average.
Table 4. ACS of adding only vs adding and swapping (generation = 10).
ACS (adding only)
ACS (adding and swapping)
0.118620
0.071738
0.176082
0.058726
0.062426
0.084542
0.115515
0.028413
0.070112
0.102586
0.057862
0.052701
0.628860
0.107852
0.132182
0.055899
Figure 3 illustrates the two scatter diagrams of Experiment 1 and 2. According to
the two variables: running generation (x-axis) and ACS (y-axis), we get two curves of
linear regression. In common, two experiments ﬁt a power function with a negative
exponent. In early variation, y decreases rapidly with the increase of x and the rate of
decrease slows down continuely later. In comparison, the tendency of decline tends to
be gentle when generation >=5 for adding only, while the generation threshold is 4 for
adding and swapping. Given the same running generation, adding and swapping gains
smaller ACS. The derivative is larger than adding only proving the blue curve decline
to nearly horizontal more quickly than the red one. In conclusion, adding and swapping
is better than adding only in variation eﬀect.
488
F. Yang et al.

Fig. 3. Scatter diagrams of Experiment 1 and 2
5
Conclusion
This paper puts forward the concept and realization of anti-data mining on group privacy
information. Transforming the clustering process by adding and swapping, the group
speciﬁcity is altered and the data mining becomes ineﬀective. The validity of our idea
is testiﬁed by a series of experiments.
In the era of big data, anti-data mining is of practical signiﬁcance for privacy
preserving and data security. We will research further on this theme in the near future.
Acknowledgement. The project was supported by the National Natural Science Foundation of
China under Grant 61502440 and the Open Research Project of The Hubei Key Laboratory of
Intelligent Geo-Information Processing under Grant KLIGIP1610.
References
1. Brown, B., Chui, M., Manyika, J.: Are you ready for the era of “big data”? http://
www.mckinsey.com/business-functions/strategy-and-corporate-ﬁnance/our-insights/are-
you-ready-for-the-era-of-big-data
2. https://en.wikipedia.org/wiki/Big_data
3. Meng, X., Ci, X.: Big data management: concepts, techniques and challenges. J. Comput.
Res. Dev. 50(1), 146–169 (2013)
4. Mauro, A.D., Greco, M., Grimaldi, M.: A formal deﬁnition of big data based on its essential
features. Libr. Rev. 65(3), 122–135 (2016)
5. Han, J., Kamber, M.: Data Mining: Concepts and Techniques, 2nd edn. Morgan Kaufmann
Publishers, San Francisco (2006)
6. Zetlin, M.: The Latest Privacy Invasion: Retailer Tracking (2012)
7. Agrawal, R., Srikant, R.: Privacy preserving data mining. In: Proceedings of SIGMOD 2000,
New York, pp. 439–450. ACM (2000)
Anti-data Mining on Group Privacy Information
489

8. Lindell, P.: Privacy preserving data mining. J. Cryptol. 15(3), 177–206 (2002)
9. Evﬁmievski, A., Gehrke, J., Srikant, R.: Limiting privacy breaches in privacy preserving data
mining. In: PODS, pp. 211–222 (2004)
10. Vaidya, J., Clifton, C.: Privacy-preserving data mining: why, how, and when. IEEE Secur.
Priv. Mag. 2(6), 19–27 (2004)
11. Zhang, N.: Privacy-preserving data mining. Texas A&M University, pp. 439–450 (2006)
12. Vaidya, J., Zhu, Y.M., Clifton, C.W.: Privacy Preserving Data Mining. Advances in
Information Security. Springer, New York (2005)
13. Lindell, P.: Privacy Preserving Data Mining. Springer, New York (2006)
14. Saranya, K., Premalatha, K., Rajasekar, S.S.: A survey on privacy preserving data mining.
In: International Conference on Electronics and Communication Systems, pp. 1740–1744.
IEEE (2015)
15. Aggarwal, C.C., Yu, P.S.: A general survey of privacy-preserving data mining models and
algorithms. In: Aggarwal, C.C., Yu, P.S. (eds.) Privacy-Preserving Data Mining. ADBS, vol.
34, pp. 11–52. Springer, Boston (2008). https://doi.org/10.1007/978-0-387-70992-5_2
16. Matwin, S.: Privacy-preserving data mining techniques: survey and challenges. In: Custers,
B., Calders, T., Schermer, B., Zarsky, T. (eds.) Discrimination and Privacy in the Information
Society. SAPERE, vol. 3, pp. 209–221. Springer, Heidelberg (2013). https://doi.org/
10.1007/978-3-642-30487-3_11
17. Xu, L., Jiang, C., Wang, J., et al.: Information security in big data: privacy and data mining.
Access IEEE 2, 1149–1176 (2014)
18. Verykios, V.S., Bertino, E., Fovino, I.N., et al.: State-of-the-art in privacy preserving data
mining. ACM Sigmod Rec. 33(1), 50–57 (2004)
19. Li, F., Ma, J., Li, J.H.: Distributed anonymous data perturbation method for privacy-
preserving data mining. J. Zhejiang Univ. Sci. A 10(7), 952–963 (2009)
20. Kantarcioglu, M., Clifton, C.: Privacy-preserving distributed mining of association rules on
horizontally partitioned data. IEEE Trans. Knowl. Data Eng. 16(9), 1026–1037 (2004)
21. Inan, A., Sayg, Y., Savas, E., et al.: Privacy preserving clustering on horizontally partitioned
data. Data Knowl. Eng. 63(3), 646–666 (2007)
22. Ouda, M.A., Salem, S.A., Ali, I.A., et al.: Privacy-preserving data mining (PPDM) method
for horizontally partitioned data. Int. J. Comput. Sci. 9(5) (2012)
23. Dwork, C., Nissim, K.: Privacy-preserving datamining on vertically partitioned databases.
In: Franklin, M. (ed.) CRYPTO 2004. LNCS, vol. 3152, pp. 528–544. Springer, Heidelberg
(2004). https://doi.org/10.1007/978-3-540-28628-8_32
24. Muthulakshmi, N.V., Sandhya Rani, K.: Privacy preserving association rule mining in
vertically partitioned databases. Int. J. Comput. Appl. 39(13), 29–35 (2012)
25. Malik, M.B., Ghazi, M.A., Ali, R.: Privacy preserving data mining techniques: current
scenario and future prospects. In: Third International Conference on Computer and
Communication Technology, pp. 26–32. IEEE Computer Society (2012)
26. Liu, K., Kargupta, H., Ryan, J.: Random projection-based multiplicative data perturbation
for privacy preserving distributed data mining. IEEE Trans. Knowl. Data Eng. 18(1), 92–106
(2006)
27. Kargupta, H., Datta, S., Wang, Q., et al.: On the privacy preserving properties of random data
perturbation techniques. In: IEEE International Conference on Data Mining, p. 99. IEEE
Computer Society (2003)
28. Sugumar, R., Jayakumar, C., Rengarajan, A.: An eﬃcient blocking algorithm for privacy
preserving data mining. J. Comput. (2011)
490
F. Yang et al.

29. Fienberg, S.E., McIntyre, J.: Data swapping: variations on a theme by Dalenius and Reiss.
In: Domingo-Ferrer, J., Torra, V. (eds.) PSD 2004. LNCS, vol. 3050, pp. 14–29. Springer,
Heidelberg (2004). https://doi.org/10.1007/978-3-540-25955-8_2
30. Li, G., Wang, Y.: Privacy-preserving data mining based on sample selection and singular
value decomposition. In: International Conference on Internet Computing & Information
Services, pp. 298–301. IEEE (2011)
31. Fang, W.W., Yang, B.R., Yang, J., et al.: Decision-tree model research based on privacy-
preserving. Pattern Recogn. Artif. Intell. 23(6), 776–780 (2010)
32. Oliveira, S.R.M., Zaïane, O.R., Saygin, Y.: Secure association rule sharing. In: Dai, H.,
Srikant, R., Zhang, C. (eds.) PAKDD 2004. LNCS (LNAI), vol. 3056, pp. 74–85. Springer,
Heidelberg (2004). https://doi.org/10.1007/978-3-540-24775-3_10
33. Agrawal, R., Srikant, R.: Fast algorithms for mining association rules in large databases. In:
International Conference on Very Large Data Bases, pp. 487–499. Morgan Kaufmann
Publishers Inc. (2000)
34. Kumar, P., Varma, K.I., Sureka, A.: Fuzzy based clustering algorithm for privacy preserving
data mining. Int. J. Bus. Inf. Syst. 7(1), 27–40 (2011)
35. Pawlak, Z.: Rough Sets: Theoretical Aspects of Reasoning about Data. Kluwer Academic
Publishers, Dordrecht (1991). https://doi.org/10.1007/978-94-011-3534-4
36. Iqbal, K., Asghar, S., Fong, S.: A PPDM model using Bayesian Network for hiding sensitive
XML Association Rules. In: IEEE International Conference on Digital Information
Management, ICDIM 2011, Melbourne, Australia, September, pp. 30–35. DBLP (2011)
37. Ferguson, D., Likhachev, M., Stentz, A.: A guide to heuristic-based path planning. Comput.
Knowl. Technol. (2005)
38. Pinkas, B.: Cryptographic techniques for privacy-preserving data mining. ACM Sigkdd
Explor. Newsl. 4(2), 12–19 (2002)
39. Agrawal, R., Srikant, R.: Privacy-Preserving Data Mining. Foundations and Advances in Data
Mining, pp. 36–54. Springer, Berlin Heidelberg (2005)
40. http://news.china.com/domestic/945/20160718/23079010.html. [DB/OL]
41. Fang, B., Jia, Y., Aiping, L.I., et al.: Privacy preservation in big data: a survey. Big Data Res.
(2016)
42. Brugger, S.T., Kelley, M., Sumikawa, K., et al.: Deﬁning privacy for data mining. In: National
Science Foundation Workshop on Next Generation Data Mining, pp. 126–133 (2002)
43. Chen, T.S., Chen, J., Kao, Y.H., et al.: A novel anti-data mining technique based on
hierarchical anti-clustering (HAC). In: Eighth International Conference on Intelligent
Systems Design and Applications, pp. 426–430. IEEE Computer Society (2008)
44. Chen, T.S., Chen, J., Kao, Y.H.: A novel hybrid protection technique of privacy-preserving
data mining and anti-data mining. Inf. Technol. J. 9(3), 500–505 (2010)
45. Berkhin, P.: A survey of clustering data mining techniques. Grouping Multidimension. Data
43(1), 25–71 (2006)
Anti-data Mining on Group Privacy Information
491

Big Data Analysis of Reviews on E-commerce
Based on Hadoop
Qiaohong Zu
(✉) and Jiangming Wu
(✉)
School of Logistics Engineering, Wuhan University of Technology,
Wuhan 430063, People’s Republic of China
zuqiaohong@foxmail.com, 1752398990@qq.com
Abstract. With the increasing popularity of online shopping, it has brought with
its massive online consumers and the growth of merchandise information data.
In order to deal with the demand for big data processing, building an analysis
system of e-commerce reviews base on Hadoop software framework. The reviews
of Internet commodity are chosen to be the samples of study. Choosing Navie
Bayesian classiﬁcation to analyze the attributed values are discrete. The classiﬁ‐
cation algorithms in accordance with MapReduce parallel computing theory
designed and run on Hadoop platform. Constructing the Naive Bayesian senti‐
ment classiﬁer, and make the classiﬁers on the Hadoop platform to achieve
commodity reviews mining job. Result shows that it can improve the eﬃciency
of the commodity reviews analysis by using the Hadoop distributed platform.
Keywords: Hadoop · MapReduce · Big data · Emotion tendency
1
Introduction
According to Chinese Online Shopping Market Research Report in 2015 [1], which
CNNIC published in 2016, China’s online sales continue to maintain the high growth
rate. With online shopping becoming more and more popular, it has also brought the
explosion of commodity review texts, produced huge amounts of data information, so
the demand for the analysis of the Internet commodity reviews is higher eﬃciency. For
automatic text sentiment analysis, traditional single machine has some limitations. This
project takes advantages of the MapReduce programming model of distributed
computing, based on Hadoop, compared with single machine, it has more CPU kernel
number and bigger RAMs. Under the lots of data reviews text information, with Hadoop
distributed computing framework in the review of text information has important signif‐
icance.
2
Analysis of Review Texts and the Research of Hadoop
2.1
Analysis of Review Texts
The text emotion analysis is also called opinion mining, it refers to the machine learning,
statistics, natural language processing and other techniques to automatically extract,
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 492–502, 2018.
https://doi.org/10.1007/978-3-319-74521-3_52

deﬁne or describe a content containing emotional information, achieves to make clas‐
siﬁcation judgments of the emotional tendencies of overall text or of some subjective
[2, 3]. In order to judge the emotional tendencies of the reviews. Naive Bayesian is
chosen to be classiﬁcation method. Naive Bayesian classiﬁcation model has the
following advantages: (1) algorithm theory is simple, high accuracy; (2) classiﬁcation
time is shorter; (3) support for diﬀerent kinds of data types, the algorithm is more stable.
Bayesian theorem is actually applied as follows, suppose a purchase experiment E,
X is a description of N attributes value of a customer, such as age, gender, annual income
and other statistical values, H represents the purchase hypothesis, H = 0 means the
purchase, H = 1 means no purchase. Which P(H | X) represents the probability that H
occurs when condition X is known, and P(H | X) is the posterior probability. P(H) is the
probability of occurrence of event and P(X) is the probability of occurrence of event X,
they are called the prior probability (unconditional probability) and P(X | H). The three
probability values can be obtained from the historical data, the probability of user
purchasing a product need to be predicted, i.e. the probability of event P(H | X).
According to the Bayesian theorem, the equation is available.
P(H | X) = P(X | H)P(H)
P(X)
(1)
The above pilot knowledge can be used to illustrate a simple Bayesian classiﬁcation
problem. Here is a training set of samples D, where each row of data represents a training
tuple and the tuple class attribute value label, each tuple with an n-dimensional attribute
vector represents X = {a1, a2, … an−1, an,}, {a1, a2, … an−1, an,} represents the meas‐
ured value of A1, A2, … An−1, An corresponding to the n characteristic attributes. Also
assume there are m class variables C1, C2, … Cm−1, Cm.
Assuming tuple vector X is known, classiﬁer needs to predict class Ci of X belongs
to, in fact, obtain the maximum posteriori probability of P(Ci | X) and classiﬁcation
prediction result for test set X can be obtained from Eq. (2). It is the value of Ci when
P(Ci | X)P(Ci) get the maximum value
class(X) = arg max{P(X | Ci)P(Ci)} = arg max
{
P(Ci)
n
∏
k=1
P(xk | Ci)
}
(2)
2.2
Research of Hadoop
Hadoop distributed software framework is designed to solve the problem of large data
computing, through distributed data storage and distributed computing. Therefore,
Hadoop’s basic platform is the core structure of HDFS distributed storage system and
the distributed data processing model and execution environment of MapReduce [4].
HDFS can automatically divide large ﬁles into many parts, and then upload these
parts to the computer node of the same Hadoop cluster. Users only need to log in to the
HDFS root directory to view all the shared ﬁles in the system, rather than knowing which
Big Data Analysis of Reviews on E-commerce Based on Hadoop
493

computer is the part of the ﬁle belong to. At the same time large-scale ﬁle block storage
is the base of achieve mass data parallel computing. HDFS distributed ﬁle system has
the advantages of detecting data node failure, setting ﬁle copy, high availability and low
operating costs and so on. MapReduce parallel computing model needs to implement
Map and Reduce two functions, with the way of “divide and rule” to achieve the idea
of distributed parallel computing [5]. Map function part of the MapReduce calculation
program is mapped to the part of the data stored on the machine to calculate the Reduce
function phase based on the actual data situation. Reduce function phase based on the
actual data situation, generate a number of Reduce tasks on the Map phase of the data
protocol, processing.
3
Design and Development of Review Texts Analysis System
3.1
Architecture Design of Review Texts Analysis System
In this study, a analysis system of e-commerce review texts is designed, under distributed
storage of Hadoop platform and framework of parallel computing, which is based on
the needs of businessmen’s automatic acquisition of product reviews, secure storage and
emotional analysis. Based on the B/S architecture WEB management system, the system
is divided into Hadoop distributed cluster product review data analysis module, data
storage module and data display module.
Data analysis module is the core module of the whole system. Its main job is divided
into three parts: ﬁrst part is preprocessing and distributed storage the date of the product
sales and review. Second part is the implementation of Mapreduce parallel computing
framework analysis mining commodity review data information, the processed data
transferred to the database server. Third part is the data migration work. Soop is chosen
to be the cross-platform data migration tool, it is good at dealing with transporting the
data of HDFS distributed storage system to the database server.
3.2
Design of Review Texts Analysis
According to the design requirements of analysis system of e-commerce reviews,
emotional tendency classiﬁer based on Naive Bayesian classiﬁcation algorithm is real‐
ized under Hadoop platform. The system architecture of the naive Bayesian aﬀective
classiﬁer is shown in Fig. 1, which includes classiﬁer a learning stage and a classiﬁcation
stage.
According to diﬀerent kinds of products, reviews of the commodity may show a big
diﬀerence. Therefore, combining the corpus to build the product ontology library for
the corresponding product.
3.2.1
Construction of Review Texts’ Lexicon
Major brands of chocolate is selected as the object of research, chocolate brands contains
Dove, Ferrero and so on. The corresponding product page of the product reviews are
randomly selected to analyze, based on frequency of the word frequency statistics,
494
Q. Zu and J. Wu

according to analytic hierarchy process to determine the weight of the Review elements,
constructing review elements and weight table, the results are shown in Table 1.
Table 1. Review elements and weight table
Review elements
Weighted value (N)
Product
9
Price
8
Logistics
7
Public praise
6
Pack
5
Promotion
4
Other
3
At the same time, it has been observed that there are many elements of reviews are
not directly described, but are described by other words, taking into account the lack of
elements. Therefore, it is necessary to establish a classiﬁcation table for the review
elements, which can be classiﬁed into the corresponding review elements when
matching the words of the classiﬁcation table. Table 2 shows the review elements.
Start
E-commerce 
review  training 
set
NB classification  
training
NB classifier
End
Ttext
preprocessing
Characteristics 
and weight 
selection
NB classify
Start
End
Classification 
result
Feature
lexicon
Learning stage
Classification stage
Fig. 1. System architecture of the naive Bayesian aﬀective classiﬁer
Big Data Analysis of Reviews on E-commerce Based on Hadoop
495

Table 2. Review element classiﬁcation table
Review elements
Related words
Product
Product, taste, taste, delicious, Ø
Price
Cheap, aﬀordable, cost-eﬀective, expensive
Logistics
Speed, express
Public praise
Genuine, fake, come again, just so so
Pack
Packaging, grade, exquisite
Promotion
Promotions, discount
Other
Attitude, service, reply
According to the review element words contained in each review text, the weights
of the review elements are calculated according to the weight table of the review factor
words. The calculation is shown in Eq. 3:
Pi =
Ni
n∑
i=1
Ni
(3)
In Eq. 3, Ni represents the weight setting of the i-th review element in the review,
n∑
i=1
Ni represents the sum of the weight values of all the review elements analyzed in the
review, and Pi is the weight proportion that represents the i-th review element in the
reviewary of this process.
The eﬀective factors in the review mainly include emotional words, negative words
and turning words. Combined with Chinese Emotional Vocabulary Ontology Library
[6], according to the review to improve the emotional word annotation, it is shown in
Table 3.
Table 3. Emotional poles vocabulary table
Emotion tendency
Words
Marking
Neutral
In general, okay, gray
0
Commendatory
Good, like, authentic, beautiful
1
Derogatory
Garbage, silent, bad, poor
2
In judging the emotional tendencies of the reviewary document, it is often determined
not only by the emotional polarity category of the emotional vocabulary in the text, but
rather by the common use of the words used in the emotional vocabulary [7, 8]. So
negative vocabulary is shown in Table 4. Turning vocabulary marked as Table 5.
496
Q. Zu and J. Wu

Table 4. Negative vocabulary table
Whether it contain negative
words?
Vocabulary
Marking
Yes
No, not, inferior to,
incompatible, will not
1
No
Ø
0
Table 5. Turning vocabulary table
Whether it contains a turning
words?
Vocabulary
Marking
Yes
But, well, however, but, yet,
still, just
1
No
Ø
0
In a commodity review sentence, although all the review elements are not evaluated,
there are often include two or more concerns, positive or negative. If all for the same
emotional tendencies, the emotional polarity of the sentence is easy to judge. But if there
are two kinds of emotional tendencies, the general approach is to take a simple weighting
approach to evaluate the weight of the elements, but this approach tends to have an
erroneous eﬀect on the results [9]. Therefore, during the emotional analysis of weighted
calculation, a turning word before the review element, the weight of the previous clause
can be weaken or strengthen the weight of themselves. When there are three or more
review elements, the weakening of the previous term will increase the number of calcu‐
lations, combined with Table 1 to evaluate the elements of the word and the weight table,
using Eq. 4 to calculate the inﬂuence of the turning point on the weighting elements of
the review element.
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪⎩
Pi =
Ni
n∑
i=1
Ni
(1 + 𝜃)
𝜃= 1 −
Ni
n∑
i=1
Ni
𝜃∈(0, 1)
(4)
3.2.2
Naive Bayesian Classiﬁcation
It can be known that the Naive Bayesian classiﬁcation algorithm only needs to calculate
the maximum value of P(X | Ci)P(Ci) for the text emotion classiﬁcation, from the Eq. 2,
and can predict the emotional tendency of the data to be classiﬁed. The classiﬁed tuples
data is called X and the attribute value of X is obtained by text preprocessing, feature
and weight selection, P(Ci) is called the prior probability of class Ci, and P(X | Ci) is
called the probability of vector X under class Ci. The probability values of P(Ci) and
Big Data Analysis of Reviews on E-commerce Based on Hadoop
497

P(X | Ci) are unknown, the prior probabilities of class Ci, which can be obtained from
training set data, Naive Bayesian algorithm is to calculate the prior probability and
conditional probability value, Fig. 2 shows the algorithm ﬂow.
Start
Calculate the prior probabilities 
of class C
Calculate the conditional 
probability of class Ci under each 
attribute
Returns the label of class Ci of
the largest probability value
End
Fig. 2. Bayesian classiﬁcation algorithm ﬂow
After calculating the priori probability, calculating probability P(X | Ci) of the tuple
data X in class Ci. It can be known that the attributes of X are independent of each other,
from the Naive Bayesian assumption, and P(X | Ci) is obtained by multiplying the
conditional probability of each attribute value under class Ci.
According to diﬀerent types of reviews, using the algorithm to deal with, various
categories of P(X | Ci) probability value can be obtained. By comparing the size of
P(X | Ci)P(Ci), it can predicted the label of current data tuple X class. When a review
text has multiple review elements, the emotional tendency of whole sentence can be
determined by comparing P(X | Ci)P(Ci) ∗wi (wi represents the weight value) with the
weighted value.
3.3
MapReduce Parallel Computing Module
MapReduce parallel computing module is that the calculation of the classiﬁcation stage
is integrated into the Hadoop platform, to achieve the emotional classiﬁcation of parallel
processing. In general among the Hadoop clusters in MapReduce parallel computing,
Map phase is responsible for the main data processing work, Reduce phase is primarily
responsible for data protocol operations, such as statistical operations, maximum fetched
498
Q. Zu and J. Wu

operation. And training set of naive Bayesian emotional classiﬁer construction stage is
artiﬁcially annotated, and the amount of data is not large. The massive training set of
data may also cause over-ﬁtting problems. So the structure of classiﬁer and classiﬁcation
stage are processed in the Map phase, classiﬁed statistic results is processed by the
Reduce phase. The MapReduce parallel computing ﬂow is shown in Fig. 3.
E-commerce 
comment 
text
slice
slice
slice
Classification 
algorithm
Classification 
algorithm
Statistics, 
combined 
classification 
results
Statistics, 
combined 
classification 
results
Output
result
Output
result
Map stage
Reduce stage
Classification 
algorithm
Fig. 3. MapReduce parallel computing ﬂow
4
Achievement of Review Texts Analysis System Based on Hadoop
4.1
Achievement and Test of Review Texts Analysis System
4.1.1
Data Selection and Evaluation Standard of Experiment
Hadoop distributed cluster has a total of six servers, and these six servers to run as virtual
machines build on the two desktop computers through VMware software.
The experimental data set is a commodity review text of the chocolate brands that
is collected on diﬀerent platforms. Training set and test set are constructed by artiﬁcial
annotation method. Before the data is marked out, the eﬀective data is selected, and the
data set is marked according to the lexical ontology database which has been constructed.
Finally, Constructed one hundred compliment comments, ﬁfty derogatory comments,
thirty neutral comment data sets, according to 1:1 ratio of ninety assigned to the training
set and test set.
Big Data Analysis of Reviews on E-commerce Based on Hadoop
499

In the review of the eﬀectiveness of the classiﬁcation of commodity reviews, the
following review indicators are often used: the precision is recorded as P; the recall rate
is denoted by R; the calculation of the next two indicators is described below (Table 6).
Table 6. Product review test set classiﬁcation example table
Category
The number of reviews of “good
reviews”
The number of reviews of “bad
reviews”
The number of reviews
judged to be “good
reviews”
a
b
The number of reviews
judged to be “bad reviews”
c
d
The precision ratio P is the proportion of the correct value of the classiﬁcation a in
the classiﬁcation result a and b. Then the precision P calculation equation as shown in
Eq. 5:
P =
a
a + b
(5)
The recall rate R is the correct value of the classiﬁcation a in the test set attributable
to “praise” the proportion of a and c. The recall rate R is calculated as shown in Eq. 6:
R =
a
a + c
(6)
4.1.2
Design and Result Analysis of Experiment
In order to verify the classiﬁcation accuracy of the emotion classiﬁer and the classiﬁ‐
cation eﬃciency under the Hadoop platform, following experiment was designed: ninety
test data was chosen to be the study object, the emotion classiﬁcation of ninety test set
data was analyzed, verify the accuracy of classiﬁcation analysis. The experiment runs
the program on the Hadoop platform to classify the review texts test set data. The result
of experimental classiﬁcation result is shown in Fig. 4.
Fig. 4. Result of experimental emotional classiﬁcation results
Figure 4, from left to right are the product brand code, electricity business platform
coding, date, review category code, commodity category code, and the statistics of
commodity, medium and bad reviews. The recall rate P of the experiment is zero point
eight six two and the recall rate R is zero point eight eight. The experimental results
500
Q. Zu and J. Wu

show that the accuracy rate of the classiﬁer is more than eighty-ﬁve percent, which means
the system is good at predicting the emotional tendency of the review texts.
After analyzing the output result, it is found that the classiﬁcation result is more
accurate when the review text has only one review factor or only include emotional
word. When the review text has multiple review factors, the classiﬁcation result is prone
to appear classiﬁcation error, which leads to the decrease of classiﬁcation accuracy.
5
Conclusion
Under the background of online shopping normalization and big data age, the e-
commerce reviews are choosen to be the object of study, to analyze the review texts
features, and designs the system of information retrieval based on Hadoop data
processing technology. Extraction, analysis of additional business information from a
large number of unstructured reviews text data, providing the important reference basis
for merchants to adjust production and sales strategy. Although the work of the corre‐
sponding research is completed, getting a better analysis of the results, but there are
some deﬁciencies, following aspects need to be improved:
(1) Only the naive Bayesian classiﬁcation algorithm is adopted. Although it is better
to classify the commodity in the text, it can not get all the attribute characteristics
of the text when it is large.
(2) Hadoop platform is good at dealing with massive data, due to limited hardware,
only in the virtual machine to achieve a real cluster structures, its eﬃciency in large-
scale data can not be eﬀectively tested.
Acknowledgment. This paper was supported by the project in the Hubei Science and Technology
Pillar Program (No. 2015BKA222).
References
1. CNNIC. Chinese Online Shopping Market Research Report of 2015 [EB/OL] (2016). http://
www.cnnic.net.cn/hlwfzyj/hlwxzbg/dzswbg/201606/t20160622_54248.htm
2. Liu, B.: Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language
Technologies, vol. 5, no. 1, pp. 1–167 (2012)
3. Mao, C., Hu, B., Wang, M., Moore, P.: Learning from neighborhood for classiﬁcation with
local distribution characteristics. In: 2015 International Joint Conference on Neural Networks
(IJCNN), pp. 1–8. IEEE, July 2015
4. Welcome to Apache Hadoop! [EB/OL] (2015). www.hadoop.apache.org
5. Dittrich, J., Quiané-Ruiz, J.A.: Eﬃcient big data processing in Hadoop MapReduce. Proc.
VLDB Endowment 5(12), 2014–2015 (2012)
6. Xu, L., Lin, F., Yu, P., et al.: The structure of emotional vocabulary ontology. J. Intell. 27(2),
180–185 (2008)
7. Zhao, P., Zhao, H., Tao, X., et al.: Based on the semantic model of TriPos Chinese subjective
and objective analysis method. Res. Comput. Appl. 29(9), 3285–3288 (2012)
Big Data Analysis of Reviews on E-commerce Based on Hadoop
501

8. Chen, J., Hu, B., Moore, P., Zhang, X., Ma, X.: Electroencephalogram-based emotion
assessment system using ontology and data mining techniques. Appl. Soft Comput. 30, 663–
674 (2015)
9. Xie, S., Liu, B., Wang, T.: Apply the semantic relation to construct the emotional dictionary
automatically. J. Natl. Univ. Defense Technol. 36(3), 111–115 (2014)
502
Q. Zu and J. Wu

Analysis on Structural Vulnerability Under
the Asymmetric Information
Mingshu He1,2(✉), Xiaojuan Wang1,2, Jingwen You1,2, and Zhen Wang1,2
1 Electronic Engineering Institute, Beijing University of Posts and Telecommunications,
Beijing 100876, China
hms_bupt@163.com
2 Electronic Engineering Institute of PLA, Heifei 230000, China
Abstract. Network attack can invalidate the connectivity of the resource
network topology composed of routers, switches and other resources. This type
of structural vulnerability which is caused by increasing scale of nodes in the
network is a hotspot of current researches. In order to integrate the random attack
and the targeted attack, we proposed an asymmetric information attack model
which is closer to reality. In this attack model, we use the attack range and the
node detection degree to adjust the attack mode and these two parameters extend
the attack mode more than the random attack and the targeted attack. In this paper,
we apply our attack model to attack BA network, ER network and Router network
under diﬀerent parameters. Then we ﬁnd the random attack is better than other
attack modes with nonzero node detection degree in ER network. And BA
network is fragile to nonzero node detection degree attack mode. In addition, we
also notice that although the distribution of Router network and BA network both
satisfy the power law distribution, they show diﬀerent structural vulnerability.
The random attack has a better eﬀect than the asymmetric information attack with
nonzero node detection degree and attack range. Router network has the same
structural vulnerability with ER network, which means Router network also has
randomness.
Keywords: Structural vulnerability · Asymmetric information
Random attack · Targeted attacks
1
Introduction
With the rapid development of the information society, some information systems such
as the distributed network equipment, computers, databases and the application software
has attracted particular interests, they have higher resources sharing speed and stronger
coordinated ability but meanwhile the structure may be complex. To analyze the inﬂu‐
ence of resources topology to communication, we need to detect the basic network
topology. We are capable of sending data packets to any place through a computer
terminal. By changing the survival time of the data packet we can ﬁnd IP of routers.
This process can be realized by the computer tool “Traceroute” [1]. Then we can use
this tool to combine up a great deal of node’s routing tracking paths to detect the whole
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 503–515, 2018.
https://doi.org/10.1007/978-3-319-74521-3_53

network topology [2]. In addition, we can refer to the routing list information stored in
routers to complete the whole network topology. The so-called routing list information
is the connected relationship with other routing in the network. Rather than studying the
routing of the network, we would like to pay more attention to the connectivity and other
properties of the network. The present researches on the complex information system
mostly concentrated on the connectivity of the network.
In an ideal world, network topology is connected. Meanwhile the number of network
attack rises, such as Trojan, botnets, computer viruses, worms, denial of service attacks,
web page tampering, domain name hijacking and so on. These can lead to nodes or links
failed. Single node or link which is removed from the network can cause damage to the
original connected network and result in communication failure. The robust of the network
is that the network still has strong connectivity after deleting several nodes or links. On the
contrary, if the network is no longer connected due to the emergence of a large number of
isolated nodes, we consider the network to be vulnerable. The vulnerability caused by the
change of network topology is called the structural vulnerability. Considering that the
influence of the failure of nodes and links to the network is equal, in this paper, we just talk
about the failure of nodes. There are two main types of existing way of attack: Random
attack [3, 4] and Targeted attack [5, 6]. Random attack randomly selects the nodes in the
network to attack and this attack way has strong randomness. Targeted attack is ranging the
nodes in network by the nodes centrality and select the most important nodes to delete.
Many researches [7–9] compared two attack methods but they did not point out the rela‐
tionship between them. We think that these two attack forms can use a common attack
model to combine together. How to put forward a common attack model to combine two
attack methods is our research focus.
We also studied the researches about the structural vulnerability. Albert [10] found
that, in the random network, when removed node number exceeds a threshold, the network
will become fragmented and lose the network connectivity. And in scale-free networks
when the threshold phenomenon disappears, deleted node number and largest number of
nodes in the connected subgraph synchronously reduce in proportion. Li [11] proposed an
attack method based on the Maximal Vertex Coverage and conducted a lot of experiment
simulations to prove that different networks have different network structural vulnerability
in MVC attack forms. Li [12] used Percolation Theory to analyze network reliability. The
failure of network can be regarded as a percolation process. Effective nodes are corre‐
sponded to occupied nodes in percolation process and failure nodes are corresponded to
blank nodes in percolation process. Through the network simulation analysis he got the
lifetime of the network nodes, and when the scale of the nodes is larger, the network has a
longer life. Ye [13] reconstructed the network to relatively small scale-free networks in
order to increase the robustness of the network. Tanizawa [14] described random attack and
targeted attack as a series of waves, so as to put forward a series of anti-destroying ability
optimization schemes. But most researches focus on comparing network structural vulner‐
ability under a specific attack mode, and assume that the network is known, ignoring the
fact that the attacker may not get the whole information of the network, and the differ‐
ences between the defenders and attackers. In the process of actual attack, the detected
network and real network are different due to some factors such as technology and
resources. This reflects the information asymmetry between the attackers and defenders.
504
M. He et al.

In order to acquire a better understanding of the structural vulnerability, we need to verify
the simulation results from the aspects of theory. But the study on network structural
vulnerability theory has only a few number of researches. Karrer [15] completed the theo‐
retical derivation of percolation theory in sparse network. The percolation process simula‐
tion is a process of information transmission. This provided theoretical basis for the spread
of disease and network attack. Newman [16] analyzed the max connected component in
random network and the result showed max connected component represent the network
connection. Di [17] combined assortativity and percolation theory together, and he thought
if the independence of the nodes in the network is added, the robustness of the network can
effectively increase. This part of the theoretical research, however, cannot completely fit the
simulation results, and sometimes it is not feasible in large-scale networks.
Accordingly, in view of the information differences between attacker and defender, we
put forward an asymmetric information model in this article. Based on the model we
proposed, we simulate in different networks and find that BA networks show vulnerability
under the asymmetric information of nonzero degree of node detection. From simulations
on the Routing network, the degree distribution of BA network and Routing network meets
the power law distribution, but Routing network shows the different structural vulnera‐
bility to BA network and the same to ER network. This suggests that with the node scale
of the network increasing the real network will show some randomness.
The chapters are arranged as follow. Section 2 introduces the random attack and the
targeted attack, and puts forward the asymmetric information attack model, to model the
network attack. Section 3 uses the proposed attack model to measure the network struc‐
tural vulnerability. By attacking BA network we observe the correlation between coeffi‐
cients in the model and different performances under different attack forms. After that, we
verify the Routing network and BA network both meet the power law distribution and we
do simulations on the Router network and compare the results between other networks.
Section 4 summarizes the research achievements of this paper.
2
Attack Mode
The targets of the network attack are generally the topology of network resouces such as
routes and switches, which can be abstracted as the graph G(N, E). Here N represents the
nodes in the network while E represents the edges. Let adjacency matrix A stand for the
connectivity of the network. A is an N*N matrix. If node i and node j are connected, aij = 1,
else aij = 0. When the nodes are under the attack caused by system vulnerability or other
reasons and fail in the topology of controlled resources, a disturbance occurs and the adja‐
cency matrix A will change into AN−k = A−Ak. Here, k is the proportion of removed
nodes. The analysis of structural vulnerability is equivalent to analyze the connectivity
difference between A and AN−k. We introduce R to represent the index of evaluating the
attack performance and R has many different definitions. In this paper, we choose the
largest connected branch as the major measuring object and use the local redundancy of
specific node as a supplement. Concrete expressions will be shown in Sect. 3.
Analysis on Structural Vulnerability
505

2.1
Random Attack and Targeted Attack
In the network under a random attack, nodes are deleted and the value of R would be
changed. To compare different indexes side by side, we normalize R to R*. The boundary
values of R* can be defined as follow.
R∗
min[k] = {min[R∗( 1
N )], min[R∗( 2
N )], … , min[R∗(1)]}
(1)
R∗max[k] = {max[R∗( 1
N )], max[R∗( 2
N )], … , max[R∗(1)]},
(2)
where k in R*[k] represents the proportion of removed nodes. We can get mean values
denoted as,
R∗
avg[k] = {E[R∗( 1
N )], E[R∗( 2
N )], … , E[R∗(1)]}
(3)
It can be confirmed that finding a node set to minimize the R*[k] is a NP hard problem
[18], and the random attack fits the mean value of R*[k]. In the case of a specific network
topology, we can get the probability of each node being selected when the nodes attacked
are randomly chosen, which can be denoted as,
PN =
{ 1
N , 1
N , 1
N , … , 1
N
}
(4)
This kind of attack doesn’t need too much algorithm complexity, and can finish the
choosing process quickly. But the attack effect is not ideal in some networks, this will be
reflected in the following simulations.
Targeted attack is to delete the nodes according to node centrality. That means the
attacker has to detect the whole network first. This attack method has high algorithm
complexity and needs to compare each node in the network. The node centrality [19, 20]
includes degree centrality, closeness centrality and betweenness centrality, etc. In this
paper, we apply the degree centrality. That is to say, the node with a higher degree value
has a greater priority to be deleted. Targeted attack has different effects in different
networks. The first step is to sort the nodes according to the degree value in the detected
network.
Centrality{n1, n2, ⋯, nN}
n1 ≥n2 ≥⋯≥nN
(5)
At this time, to all nodes in the network, the probability distribution of being chosen is
P{1,0,0,…,0}. Targeted attacks only select one node as the target at a time which ensures
the accuracy of the attack. We can assume that the targeted attack fits the minimum value
of R. We will do simulations and further explanations in Sect. 3.
506
M. He et al.

2.2
Asymmetric Information Attack
In the process of the network attack, the attacker cannot get the entire topology informa‐
tion of the network. The amount of information he gets depends on his resource, tech‐
nology and so on. The adjacency matrix the attacker detects A′ must has a perturbation
denoted as A′ = A + A∼. A~ is the error caused by the limitation of attacker’s technology
or resources. The attacker evaluates the node importance and deletes the nodes based on the
information he has got. At this time the effect to the detected network will reflect on the
real network. This whole process is called the asymmetric information attack. The attacker
cannot get the whole information of the network and the defender cannot know the choice
of the attacker or what he knows about the network. Therefore we should model this process.
To attack network G(N, E) with nodes N{n1, n2, n3 … nk}, we should sort nodes by
centrality to confirm the attack performance. In this paper, we apply the degree centrality
as the importance of nodes. And we assume the attacker is a rational person and has
restricted resources. That is to say, in the attack to a specific network, he cannot know the
entire topology of the network and will choose a certain number of nodes to delete. Based
on that, we put forward two parameters as follow.
(1) Attack range H. First we calculate the degree of all nodes and sorting them. H
represents the proportion of the detected nodes. We deﬁne H = i/n. And i represents
the number of detected nodes, n represents the number of all nodes in the network.
(2) Detection degree of node F. When attacking network, A and A′ are diﬀerent. There
must be error between them. We use the percentage to represent detection degree
of node F. F is the ratio of the detected network information to the whole network
topology information, ranging from 0 to 1.
With restricted resource, F and H must belong to 0 to 1. It is easy to know that if we
want to get higher F and H, we must spend more resource. So we define the resource cost
coefficient as follow.
C = H × F
C ∈[0, 1]
(6)
It can be seen that when C is higher, the algorithm is more complex and the time cost
is more. So we can know for the random attack C = 0 and for the targeted attack C = 1.
Assuming that the attacker spends the same resource on each deleted nodes denoted as G.
x is the number of deleted nodes. And we can get that the optimal attack plan must meet
P{Max[ΔCon]|Min[G(x −i) + R(i)]}
(7)
where R(i) = G′(i + C × i) is the resource spent under the asymmetric information
attack. △Con is the eﬀect to the connectivity of the network.
2.3
Attack Model
From the definition of the asymmetric information attack, we can find that it includes both
the random attack and the targeted attack. As shown in Fig. 1, light red nodes represent the
undetected nodes and red nodes are the detected nodes with high degree under the certain
Analysis on Structural Vulnerability
507

F. We can see that when F is not big enough, the attacker cannot find all nodes with big
degree. The lack of the amount of the information attackers acquires leads to inaccurate
attacks.
Fig. 1. Asymmetric information attack graph (Color ﬁgure online)
From Table 1, we can further understand the relationship between parameter and attack
mode. When F = 0, the attack mode is random attack. At this time H becomes meaning‐
less. When F = 1, the mode turns into the targeted attack. At this time, H is actually the
number of the deleted nodes. When F ∈ (0, 1), it is the asymmetric information attack. The
probability of selecting important nodes depends on F. Attack range H corresponds to the
number of selected nodes. It can be observed that bigger H can make up for F. This
phenomenon will be showed and analyzed in later simulation.
Table 1. The mapping relationship between parameter and attack mode
H
F
Attack mode
0
0
Random attack
1
1
Targeted attack
(0, 1)
(0, 1)
Asymmetric information attack
According to attacker’s resource we can get H and F, and the number of deleted nodes
x. To prevent nodes with big degree from repeatedly showing up in the sample, we module
the whole simulation process as the non-return unequal probability sampling. The whole
attack process is defined as follow:
Step 1: Based on H, we calculate the number of sample nodes i, which meets i = H×N
Step 2: Calculate all nodes’ degree in the network and sort them.
Step 3: Respectively give all the nodes in the network the sample rate. For the node
with the biggest degree, F is the sample rate. Other nodes’ sample rates are
calculated as follow,
508
M. He et al.

Nj = (1 −F)j−1F
(8)
Step 4: Select one node according to the sample rate. Then compare the number of the
sample nodes with i. If smaller, go to Step 2, else go next.
Step 5: When x ≤ i just delete x nodes of the sample nodes. When x > i ﬁrst delete the
sample nodes, then delete x−i nodes randomly.
In the real world, when the attacker launches an attack against a network, the attacker
cannot get the whole information of the network and the defender cannot know the
choice of the attacker or what he knows about the network. Supposed that the attacker
is rational and his resource is limited, his best choice is to attack the most important
nodes in the network. The importance of the node depends on two aspect, the attacker’s
knowledge of the network and the index of the importance. For the ﬁrst aspect, we
proposed two parameters, the attack arrange H and the detection degree of node F, which
represents the proportion of the detected nodes and the ratio of the detected network
information to the whole network topology information respectively. Or simply, H
shows how many nodes have been detected and F shows, for the detected nodes, how
much the attacker knows. As for the second aspect, the indicator of the importance is
usually the centrality of the nodes. In this paper, we use the degree value, and it is obvious
that for a single node, more degree value means it is connected to more nodes. And that
means if the node is attacked, there will be a greater eﬀect. Thus, we say the more the
degree value of the node is, the more important the node is.
3
Structural Vulnerability Measurement and Impact Analysis
3.1
Attack Performance Evaluation Indicators
R measures the connectivity of the network. The common measurement of the connec‐
tivity of the network is the number of the nodes that belongs to the largest connected
component. The redundancy of the network is also used to measure the network, espe‐
cially the robustness of the network. R has two deﬁnitions explained as follow. Rm is
the number of nodes in the largest connected branch
Rm = {nj|iﬀ∃ni ∈RmΛeij = 1}
(9)
The largest connected branch is the carrier of the network traﬃc. The number of
nodes in the largest connected branch measures the connectivity of the whole network
after the network is attacked, and is also an important index to judge whether the network
is failed.
Rv is the local redundancy of the speciﬁc node in the network. It can show the
robustness of the node against the attack. It is also a measurement of connectivity. It is
deﬁned as follow.
Rv = 1
|S|
∑
j∈V(Γ2
v)
min{d(v), d(j)}
(10)
Analysis on Structural Vulnerability
509

Node redundancy rate refers to the ratio of the number of the paths a node goes
through to arrive to its “neighbors’ neighbors” to the number of the paths in the complete
graph. Γv represents the set of neighbor nodes. Γ2
v is the set of nodes’ “neighbors’ neigh‐
bors”. |S| is the complete graph consisting of the nodes, Γv and Γ2
v.
3.2
Structural Vulnerability Analysis on BA Network
In order to test diﬀerent performances of the network under various attack modes, we
use BA network for simulations. BA network is a sparse network with 5000 nodes and
5000 edges. In simulations, we use Spyder as the simulation software and python as the
simulation platform. Besides we use networkx module to call the speciﬁc function to
generate BA networks. During the experiments we assume that the attacker prepares to
delete 100 nodes. Then we adjust the value of H and F and observe the change of
parameters in the network, and based on the simulation results we analyze the diﬀerent
properties of the network vulnerability under diﬀerent H and F. In order to guarantee
the experimental accuracy, we do each experiment for 50 times, then average them as
the ﬁnal results and plot curves.
Table 2. Asymmetric information attack algorithm
510
M. He et al.

Table 2 is the algorithm for using the number of the nodes belong to the largest
connected component to measure the connectivity of the network after the network
attacked.
Fig. 2. The change of Rm under diﬀerent H and F in BA network
Analysis on Structural Vulnerability
511

First we simulate the attack on the BA network (Fig. 2). The attack range H is
respectively 0.001, 0.002 and 0.005, and the detection degree of node is respectively
0.1, 0.5 and 0.9. In ﬁgures, the horizontal axis represents the number of the deleted nodes
and the vertical axis is the number of nodes in the largest connected branch. Based on
the simulation results it can be found that:
(1) As a whole, no matter what strategy the attack process is based on, after deleting
100 nodes, the curve tends to be gentle and the network shows its robustness. That
is to say, BA network will not be directly completely broken after deleting a certain
number of nodes. However, at this time, the network may not be able to commu‐
nicate. Therefore, to the attacker, it is not necessary to delete all 100 nodes to destroy
the network.
(2) Comparing 2-a, 2-b, 2-c we can ﬁnd that when H and F are big enough, the curves
go down rapidly, and then go to a plain. At this time the network is failed. When
H = 0.005 F = 0.9, deleting 20 nodes is enough to make the network failed. It is
obvious that these 20 nodes are important to the whole network connectivity. The
attacker is a rational person with limited resources, as a result he will choose the
most important nodes to attack.
(3) For the supplement eﬀect (H and F), it can be proved from the aspect of algorithm.
From 2-a, 2-b, 2-c, when H is ﬁxed, F just need to reach a certain value to make
network failed quickly. As in ﬁgures, curves (F = 0.5 F = 0.9) are close to each
other. From 2-d, 2-e, 2-f when F is ﬁxed, H reaches a certain value to make curves
close to each other. We use the resource cost coeﬃcient C to evaluate the attack
performance. Obviously when C is smaller, the attack cost is lower and corre‐
spondingly, the protect cost is lower. How to balance the relationship between the
cost and the eﬀect is a big issue. We can also ﬁnd that the BA network is a multi‐
center network. After deleting several center nodes, the network tends to be broken.
3.3
Compared with Other Indicators
Figure 3 is the change of redundancy of the speciﬁc node (the node is same as 3.2). From
this ﬁgure we can ﬁnd that the speciﬁc node’s local redundancy falls much. It indicates
that the attacker deletes many neighbor nodes or ‘neighbor’s neighbor’ nodes of the
speciﬁc node. Eventually, the lines all become to ﬂatten, which shows certain robustness.
Fig. 3. The change of redundancy in BA network
512
M. He et al.

Comparing Rm with Rv, we can ﬁnd that the two indicators can both be used to
measure the connectivity of the network, and can both reﬂect the robustness of the
network. The diﬀerence between the two indicators is that they indicate the properties
of the network from diﬀerent aspects. Rv is used to analyze single node’s connectivity
and Rm is used to measure the whole network’s connectivity. The connectivity of single
node and the whole network are diﬀerent but relevant. When there is a need for evalu‐
ating important nodes, we will select the Rv to be the indicator. When we need to evaluate
the robustness of the network structure, we select the Rm. From the analysis of BA
network, the eﬀect of the nonrandom attack (F > 0 H > 0) is better than random attack
(F = 0 H = 0). When F and H are bigger, the attack eﬀect is more obvious. And in the
beginning of the attack, the connectivity of the network reduces quickly. When the
network shows robust and the curves become gentle, the eﬀect of the random attack and
the nonrandom attack (F > 0 H > 0) are tend to be the same.
3.4
Simulations on Router Network
From analysis of Sect. 3 we can get the structural vulnerability of BA network. We can
compare real network with BA network to get more information about the structural
vulnerability. Router network has 192244 nodes and 609066 edges.
Fig. 4. Simulations on Router network. (a) the degree distribution of Router network (b) the
change of Rm during the attack of Router network
Figure 4a is the degree distribution of Router network. And x axis is degree; y axis
is the number of nodes. Degree and number of nodes meet linear relationship. So the
degree distribution of Router network and BA network obey power law distribution.
From Fig. 4b we can see after deleting 1000 nodes, the network becomes to be failed.
The curve at the top is the asymmetric information attack which has H = 0.0003 and
F = 0.9, and the random attack (F = 0 H = 0) is under the curve (H = 0.0003 F = 0.9).
So the eﬀect of the random attack is better than the asymmetric information attack. We
can conclude that the eﬀect of the asymmetric information attack in BA network is
opposite to that in Router network. But the degree distribution of the two networks both
Analysis on Structural Vulnerability
513

obey the power law distribution. So the structural vulnerability does not only depend on
the degree distribution. In order to fully understand the reason of this phenomenon, we
do simulations on ER network which has diﬀerent structural vulnerability with BA
network.
ER network has 5000 nodes and 12500 edges. Corresponding parameters m, n are
5000 and 0.0005. Here we attack ER network (Fig. 5) we can ﬁnd that.
(1) From Fig. 5a random attack (H = 0 F = 0) is at the bottom which states that the
eﬀect of random attack on ER network is best. And the bigger F is, the worse the
eﬀect is.
(2) In Fig. 5b three curves almost coincide. It indicates that when the H is less than a
certain value F has no inﬂuence. This is because the eﬀect of the asymmetric infor‐
mation attack (F > 0) is tiny in ER network.
Fig. 5. The change of Rm under diﬀerent H and F in ER network
The result shows that ER network is robust against the attack (F > 0) and the random
attack has the best eﬀect on ER network. ER network is a random network. The nodes
of the network have similar degree value, and the status of each node is the same. If we
use the asymmetric information attack (F > 0), can not improve the attack eﬀect not
only, rise instead counteractive. Compared with BA network the curves of attack (F > 0)
are above the random attack so the targeted attack cannot ﬁt Rmin[k].
Thus, the eﬀect of the asymmetric information attack on ER network is opposite to
that on BA network but is same with that on Router network. Although Router network
shares the same degree distribution with BA network, its structural vulnerability is more
close to ER network. This shows that the real network has certain randomness.
4
Conclusion
In this article, through the analysis of the random attack and the targeted attack, we
proposed an asymmetric information attack model, and the random attack and the
targeted attack are special cases of the attack model. Using node degree as the indicator,
514
M. He et al.

we do simulations on the ER, BA and Routing network, and analyze the contact among
the random attack, the targeted attack and the asymmetric information attack. Based on
the results, we reveal diﬀerent structural vulnerability of diﬀerent networks. Simulation
shows that BA network is fragile to the asymmetric information attack (F > 0). And ER
network shows robust to the attack (F > 0). Because the scale of the real network is large
and the real network has randomness, it is robust to the asymmetric information attack
(F > 0) when the attack uses degree centrality to measure nodes’ importance.
References
1. Burch, H., Cheswick, B.: Computer 32(4), 97 (1999)
2. Guo, R.R., Song, R.S.: Comput. Eng. Appl. 38(10), 162 (2002)
3. Trajanovski, S., Martín-Hernández, J., Winterbach, W., Mieghem, P.V.: J. Complex Netw.
1(1), 44–62 (2013)
4. Scellato, S., Leontiadis, I., Mascolo, C., Basu, P., Zafer, M.: Proceedings of INFOCOM,
Shanghai, China, pp. 1–5. IEEE (2011)
5. Huang, X., Gao, J., Buldyrev, S.V., Havlin, S., Stanley, H.E.: Phys. Rev. E 83, 065101 (2011)
6. Wang, J., Jiang, C., Qian, J.: J. Netw. Comput. Appl. 40(7), 97 (2014)
7. Byungjoon, M., Do, Y.S., Kyu-Min, L., Goh, K.-I.: Phys. Rev. E 89(4), 042811 (2014)
8. Pin-Yu, C., Shin-Ming, C.: Phys. Rev. E 91(2), 19 (2015)
9. Xie, F., Cheng, S.Q., Chen, D.Q., Zhang, G.Q.: J. Tsinghua Univ. (Sci. Technol.) 10, 1252
(2011)
10. Albert, R., Jeong, H., Barabási, A., Albert, R.: Nature 406(6794), 378 (2000)
11. Li, R.H., Yu, J.X., Huang, X., Hong, C., Ze, C.S.: Inf. Sci. 278(10), 685 (2014)
12. Li, D., Zhang, Q., Zio, E., Havlin, S., Kang, R.: Reliab. Eng. Syst. Saf. 142, 556 (2015)
13. Ye, B., Jia, J.-J., Zuo, K.-W., Ma, X.P.: Int. J. Mod. Phys. C 26(4), 1550040 (2015)
14. Tanizawa, T., Paul, G., Cohen, R., Havlin, S., Stanley, H.E.: Phys. Rev. E Stat. Nonlinear
Soft Matter Phys. 71(4), 047101 (2005)
15. Karrer, B., Newman, M.E.J., Zdeborová, L.: Phys. Rev. Lett. 113(20), 208702 (2014)
16. Newman, M.E.J.: Phys. Rev. E 76(4), 70 (2007)
17. Zhou, D., Stanley, H.E., D’Agostino, G., Scala, A.: Phys. Rev. E 86(6), 066103 (2012)
18. Dinh, T.N., Xuan, Y., Thai, M.T., Pardalos, P.M., Znati, T.: IEEE/ACM Trans. Networking
20(2), 609 (2011)
19. Ronald, S.B.: Am. J. Sociol. 110(2), 349 (2004)
20. Fu, L.D.: Ph.D. dissertation (Xidian University) (2012). (in Chinese)
21. Dueñas-Osorio, L., Craig, J.I., Goodno, B.J., Bostrom, A.: J. Infrastruct. Syst. 13(3), 185
(2007)
Analysis on Structural Vulnerability
515

Combining Link and Content Correlation
Learning for Cross-Modal Retrieval
in Social Multimedia
Longtao Zhang(&), Fangfang Liu, and Zhimin Zeng
Beijing Laboratory of Advanced Information Networks,
Beijing University of Posts and Telecommunications, Beijing, China
{zhanglongtao,ﬂiu,zengzm}@bupt.edu.cn
Abstract. With the rapid growth of multimedia data, cross-modal retrieval has
received great attention. Generally, learning semantics correlation is the primary
solution for eliminating heterogeneous gap between modalities. Existing
approaches usually focus on modeling cross-modal correlation and category
correlation, which can’t capture semantic correlation thoroughly for social
multimedia data. In fact, the diverse link information is complementary to
provide rich hints for semantic correlation. In this paper, we propose a novel
cross-modal correlation learning approach based on subspace learning by taking
heterogeneous
social
link
and
content
information
into
account.
Both
intra-modal and inter-modal correlation are simultaneously considered through
explicitly modeling link information. Additionally, those correlations are
incorporated into ﬁnal representation, which further improve the performance of
cross modal retrieval effectively. Experimental results demonstrate that the
proposed approach performs better comparing with several state-of-the-art
cross-modal correlation learning approaches.
Keywords: Cross-modal retrieval  Correlation learning  Social multimedia
Heterogeneous networks
1
Introduction
With the rapid development of multimedia technology, there has been a massive
explosion of multimedia data on social media websites, which makes the traditional
social media show the trend of multimedia [1]. In face of large amounts of complex
social multimedia data, retrieving valuable information is of great signiﬁcance. Con-
sequently, cross-modal retrieval attracts considerable attention, in which users can
input any modalities data at hand to query relevant information of other modalities.
Different from traditional single-modal retrieval, it is more comprehensive and can
meet the increasing user demands. Generally, learning semantic correlation is the main
solution for eliminating heterogeneous gap between modalities for cross-modal
retrieval. Nevertheless, these data do not exist in isolation in social multimedia, which
makes correlation learning more challenging. On the one hand, different modalities of
multimedia data are usually in coexistence. For example, in image sharing website,
users usually share images accompanied by some text to illustrate. On the other hand,
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 516–526, 2018.
https://doi.org/10.1007/978-3-319-74521-3_54

these multimedia data are closely associated with the multi social factors, such as user,
group and location information, which connect different social components into
heterogeneous social multimedia networks.
Although, several efforts have been paid to correlation learning. Most of existing
methods focus on modeling cross-modal correlation and category correlation, such as
canonical correlation analysis [2], cross-modal factor analysis [3] and semantic corre-
lation matching [4]. However, those methods fail to model the correlation thoroughly for
social multimedia data. Much social correlation between two objects is not exploited
adequately. For instance, image and text may be connected by the same user, which
indicates they have some semantic correlation. In fact, the complex heterogeneous link
structure is complementary to provide the rich hints for semantic correlation and can be
exploited to bridge the heterogeneous gap to some extent. Therefore, both link and
content information are should be captured to improve the retrieval performance.
In this paper, we propose a novel correlation learning approach based on subspace
learning. It jointly considers both heterogeneous link and multimedia content, which is
ignored by previous works. In this learning framework, ﬁrstly, multiple social links are
transformed into both intra-modal and inter-modal correlation via heterogeneous net-
works. Then heterogeneous modalities are projected into a uniﬁed subspace according
to those correlation so that the similarity between different modalities can be measured
through projection matrices. The proposed approach is experimentally evaluated better
than other prevailing approaches on a cross-modal dataset.
Along this line of research, there are two main contributions of proposed approach.
On one hand, instead of treating each semantic link equally, we design a weight
learning approach keeping link structure consistence with content information. On the
other hand, we propose incorporating heterogeneous link structure and content infor-
mation into the uniﬁed feature representation which are not only helpful to bridge the
heterogeneous gap, but also robust to noise.
The remainder of this paper is organized as follows. The related work on corre-
lation learning for cross-modal retrieval is reviewed in Sect. 2. Then we present the
proposed combining link and content correlation learning approach in Sect. 3. In
Sect. 4, experiments are shown to verify the effectiveness of proposed method. Finally,
we conclude this paper in Sect. 5.
2
Related Works
It is of considerable challenge to learning semantic correlation of heterogeneous
modalities. Generally speaking, existing efforts are roughly divided into two aspects:
the one is mapping the different features into a uniﬁed feature space based on subspace
learning so that similarity between heterogeneous modalities can be computed, and the
other is modeling the probability that learns a set of shared latent topics for different
modalities.
For subspace learning, there are many representative approaches like canonical
correlation analysis, deep canonical correlation analysis [5], and partial least square [6].
Those methods usually maximize the correlation between two different modalities and
learn the joint feature representation. While most of them do not take high-level
Combining Link and Content Correlation Learning
517

semantic correlation into account. To consider label information, Rasiwasia et al.
propose combining subspace and semantic modeling to improve retrieval accuracy.
Huang and Peng [7] present exploiting ﬁne-grained correlation by taking entity level
into account to construct high-level concepts. Moreover, further study jointly model
labeled information and unlabeled information with graph regularization in a
semi-supervised learning framework [8]. As for probabilistic model, Jia et al. [9]
proposed a probabilistic model seen as Markov random ﬁeld of topic models, which
establish correlation among modalities based on their similarity. Han and Thomas [10]
proposed matching images and sounds with tags which treated as shared latent vari-
ables by combining Latent Dirichlet Allocation and Correspondence Latent Dirichlet
Allocation model. However, these methods often take strict assumptions that there exist
the same topic proportions or pairwise topic correspondences between different
modalities, which are not satisﬁed for social multimedia data.
Generally, there is an important principle for correlation leaning. It is the both
inter-modal and intra-modal correlation that should be preserved [11]. That is, if two
objects are closely related in original space, they should be closed to each other in
latent subspace. The coexistence information is usually taken as intermodal correlation.
The intra-modal correlation is usually provided by high-level category information. In
this paper, we transform the rich link structure into the intra-modal and inter-modal
correlation, which can further improve the performance of semantic correlation.
3
Combining Link and Content Correlation Learning
3.1
Overview of Proposed Framework
To begin with, the problem formulation is deﬁnited in this section. Given a set of training
dataset D ¼ X; Z
f
g, in which X ¼ x1; x2; . . .; xn
f
g denotes images, Z ¼ z1; z2; . . .; zn
f
g
denotes texts. While X ¼ x1; x2; . . .; xn
f
g 2 Rd1n and Z ¼ z1; z2; . . .; zn
f
g 2 Rd2n
represent the image and text feature matrix respectively. In this case, the number of
training samples is n, and the feature dimensionalities of image and text are d1 and d2
respectively. The main goal of correlation learning is to learn two projection matrices
U 2 Rd1r and V 2 Rd2r for image and text domain, so that the similarity between
projected objects SimðUTxi; VTzjÞ can be computed. In other words, the semantic cor-
relation is built among heterogeneous modalities in same dimensional subspace, so that
cross-modal retrieval can be performed.
Next, an effective and concise learning framework of proposed approach is intro-
duced brieﬂy as illustrated in Fig. 1. This approach mainly consists of two phases. For
the ﬁrst step, the heterogeneous social multimedia network is treated as a kind of
heterogeneous information network [12], in which the link-based similarity could be
measured by meta-path. After that, heterogeneous network is transformed into multiple
homogeneous networks and bipartite networks. Therefore, the link-based similarity
relationship is encoded into intra-modal and intermodal correlation. For the second
step, it learns projection matrices according to those correlation for each modal based
on subspace learning. To state conveniently, we take two modalities for example in this
518
L. Zhang et al.

framework, while it can easily extend to multi-modalities case. Based on this frame-
work, the details are depicted for each step as follows.
3.2
Learning Link-Based Correlation
As mentioned above, similarity relationship should be preserved to learning semantic
correlation. Nevertheless, existing study [13] has shown that content-based similarity
sometimes may not reliable to determine the similarity between two objects. Thus, only
content-based correlation may lead to unsatisfying results for correlation learning. Of
course, the link-based correlation is also uncertain and contingency. Intuitively, link
and content are complementary to each other, so combining them will achieve more
robust performance. In this paper, we embed link-based correlation into content-based
correlation to obtain more effective yet efﬁcient feature representation.
It is reasonable to assume that semantic relationship exist between two objects, if
there exist link explicitly. Given a heterogeneous network, there are many semantic
links to connect two objects, which are deﬁned as meta-paths. Different meta-paths
imply different semantic meanings. For example, if two images are uploaded by the
same user, they are connected via “image
!
upload1
user
!
upload image” path, thus it can be
assumed that the two images are partially similar through common user. Besides, there
are many other meta-paths, such as “image !
favor1
user !
favor image” path, “image
!
upload1
user !
contact user
!
upload image”
path,
“image !
locate location
!
locate1
image”
path,
and
“image
!
upload1
user !
belong group
!
belong1
user
!
upload image” path. The link-based similarity
can be computed based on meta-path like Pathsim [14] which similarity measure
formula is deﬁned as follows,
sPðxi; xjÞ ¼
2 
pxi!xj : pxi!xj 2 P




pxi!xi : pxi!xi 2 P
f
g
j
j þ
pxj!xj : pxj!xj 2 P




ð1Þ
Heterogeneous Social Mulmedia 
Network
Mulple Homogeneous 
Networks and Biparte Networks
Uniﬁed Space of Diﬀerent 
Modalies
Step1
learning link-based 
correlaon
Step2
learning cross-modal 
correlaon
Text-text network
Image-text network
Image-image network
Fig. 1. The framework of the proposed method.
Combining Link and Content Correlation Learning
519

where xi and xj are the same type, P is a meta-path, Pxi!xj

 is the number of path
instances between xi and xj, Pxi!xi
j
j is that between xi and xi, and Pxj!xj

 is that
between xj and xj.
Different meta-paths reﬂect different aspects of image or text similarity. Instead of
treating each meta-path equally, we design a novel algorithm to learn different weights.
The latent hypothesis is the link-based similarity sðxi; xjÞ is consistence with
content-based similarity cðxi; xjÞ. The set of meta-paths are denoted as p ¼
p1; p2; . . .; pm
½

and
the
weights
of
different
meta-paths
are
denoted
as
w ¼ w1; w2; . . .; wm
½
. We minimize the following objective function to learn different
weights,
LðwÞ ¼
X
n
i¼1
X
n
j¼1
cðxi; xjÞ 
X
m
d¼1
wdspdðxi; xjÞ
 
!2
þ a w
k k2
2
ð2Þ
where the ﬁrst component ensures that link-based similarity is consistence with
content-based similarity, and the second term is L2 regulation. It is worth paying
attention that these similarities are normalized in [0, 1]. The above objective unction
can solved by many off-the-shelf methods, such as Newton method or stochastic
gradient decent method.
After having all the weights for each meta-path, the link-based similarity can be
computed by combining all the similarities that based on different meta-paths.
Therefore, link-based correlation can be modeled by three weight matrices, of which
each element wij is expressed as below,
wij ¼
P
m
d¼1
wdspdði; jÞ
i and j are of the same type
cij
otherwise
8
<
:
ð3Þ
where the cij denotes whether the link exist or not. In this paper, if there exist meta-path
“image
!
coexist1
text” between i and j, we set cij ¼ 1, otherwise we set cij ¼ 0. Fur-
thermore, multi-modal fusion is performed to get a common wij for image and text
domain in this research.
3.3
Learning Cross-Modal Correlation
So far, we have encoded the complex link information into both inter-modal and
intra-modal correlation. Our ultimate target is realizing cross-modal retrieval, so we
embed link-based correlation into content-based uniﬁed representation to learn
semantic correlation in the common subspace.
Essentially, similar objects should have similar feature representation. Then, the
higher of the correlation between two objects, the distance between them in the joint
subspace should be more close to each other. As mentioned above, to obtain better
semantic correlation and more robust representation, both intra-modal and inter-modal
520
L. Zhang et al.

similarity relationship should be preserved when learning the common subspace.
Hence, the loss function is formed by Frobenius norm as follows,
min
U;V
P
i2X;j2Z
wij UTxi  VTzj

2
F
þ k
P
i;j2X or Z
wij
UTxi  UTxj

2
F þ VTzi  VTzj

2
F


ð4Þ
where the ﬁrst term is the distance between different modalities in the projected space
and the second term is that between same modality, k is the parameter for balancing the
inter-modal and intra-modal correlation.
However, there are no obvious semantic meanings in the above projected space. In
fact, the semantic label information can offer some useful guide information. Motived
by this, we also add the label consistent term into above loss function. So our ultimate
objective function is deﬁned as follows,
min
U;V
X
i2X;j2Z
wij UTxi  VTzj

2
F
þ k
X
i;j2X or Z
wij
UTxi  UTxj

2
F þ VTzi  VTzj

2
F


þ b
UTX  YX

2
F þ VTZ  YZ

2
F


þ l
U
k k2;1 þ V
k k2;1


ð5Þ
here, YX and YZ are the initial label matrices on image and text domain respectively.
:k k2;1 denote l2;1-norm, which is used to sparse feature selection on projection matrices.
b and l are the balancing parameters.
To solve the objective function in Eq. (5), we ﬁrstly simplify it. Inspired by
Laplacian regularizer, we may arrive at a corresponding compact form as in Eq. (6).
min
U;V UTX  VTZ

2
F þ k trðFðD  WÞFTÞ
þ b
UTX  YX

2
F þ VTZ  YZ

2
F


þ l
U
k k2;1 þ V
k k2;1


ð6Þ
where tr() denote the trace of matrix, D is a diagonal matrix with dði; iÞ ¼ P
j
wij, and
F ¼ XTU; YTV

	
. Note that the parameter k have been changed to 2k. While it does
not affect the result, so we still use k for convenience.
The above objective function can be optimized by an iterative algorithm, which
solves for one variable while keeping the others ﬁxed. Hence, the solution of U and V
can be expressed in each iteration by Eqs. (7) and (8),
U ¼
1 þ b
ð
ÞXXT þ lRu þ kX D  W
ð
ÞXT

	1 bXYT
X þ XZTV

	
ð7Þ
V ¼
1 þ b
ð
ÞZZT þ lRv þ kZ D  W
ð
ÞZT

	1 bZYT
Z þ ZXTU

	
ð8Þ
Combining Link and Content Correlation Learning
521

Where Ru is a diagonal matrix [15], each element is deﬁned as in Eq. (9), Rv has
analogous representation with Ru as in Eq. (10).
ruði; iÞ ¼
1
2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
P
j
Uij

2
2 þ e
r
ð9Þ
rvði; iÞ ¼
1
2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
P
j
Vij

2
2 þ e
r
ð10Þ
in which e is a smoothing term, and we set it to be a small constant value to avoid the
denominator being zero.
Once projection matrices for heterogeneous low-level features are learnt, all the
data are readily projected into uniﬁed feature space. To perform cross-modal retrieval,
the similarity between the query and the other modalities can be computed directly.
4
Experiments
In this section, experiments are conducted to verify effectiveness of our method and
investigate the performance by comparing with several state-of-the-art methods.
4.1
Dataset
We perform the experiments for cross-modal retrieval based on the large-scale
NUS-WIDE dataset [16]. Each image is associated with some tags, which can be
regarded as text information. The dataset contains almost 270,000 images with 5,018
unique tags collected from Flickr website. We ﬁrstly ﬁnd 10 largest classes and crawl
the list of users who upload or favor a given image according to the image ID. Then,
we choose the users owing more than 10 images and obtain 6487 unique users
including 1418 authors and 5069 favorite users. What’s more, we also crawl the groups
and contacts information for each author.
In the learning link-based correlation stage, in order to catch the link-based
intra-modal correlation as much as possible, we chose 5 kinds of meta-paths for images
and
texts
as
mentioned
above.
While,
we
only
consider
one
meta-path
“image
!
coexist1
text” as inter-modal correlation. As for low-level feature, we take
500-dimensional SIFT feature vectors for images and 1000-dimensional tag feature
vectors for texts. Besides, we random select 80% of the image-text pairs used for
training and 20% for testing to conduct experiments.
4.2
Comparison Methods and Evaluation Metrics
To verify the effectiveness, we compare our method with several state-of-the-art cor-
relation learning methods including both unsupervised and supervised setting.
522
L. Zhang et al.

(1) Canonical Correlation Analysis (CCA): A traditional unsupervised method
maximizes the correlation in the common subspace to learn the correlation
matching between two heterogeneous features. Note that there are no obvious
semantic meanings in the latent subspace.
(2) Semantic Matching (SM): This approach is a supervised methods, which learns
the correlation though transform the heterogeneous feature to the common
semantic subspace. Thus the similarity between two objects can be computed
according to the probability belonging to the same class.
(3) Semantic Correlation Matching (SCM): This approach is also a supervised
method, which combines CCA and SM. Firstly, it learns cross-modal correlation
through maximize the correlated subspaces, and then logistic regression is per-
formed to get the probability of each object belonging to all the classes. Both
correlation analysis and semantic abstract are considered in this method.
In addition, since the dataset is a multi-labeled dataset, we use mean average
precision (MAP) to evaluate the retrieval performance which is a rank-based evaluation
metric and widely used to multi-label classiﬁcation. Moreover, precision-recall
(PR) curves are also plotted to investigate the performance of different approaches.
4.3
Experimental Result
In this section, we compare the proposed combining link and content correlation
(CLCC) approach with several prevailing approaches. Figure 2 shows the MAP per-
formance of different approaches on two different cross-modal retrieval task, i.e. image
to text retrieval task and text to image retrieval task. As can be seen that the method of
SCM achieves better performance than CCA and SM which both consider correlation
matching and semantic matching. It is also clear that our approach signiﬁcantly out-
performs other approaches, demonstrating the effectiveness of proposed approach. It is
reasonable because we not only exploit the rich link structure to improve correlation,
but also consider semantic projection in the subspace during the training stage. Note
that different tasks may have different performance for an approach. While our
approach obtains better performance for both retrieval tasks comparing with its several
counterparts.
Additionally, further analysis of the results is presented in Fig. 3 in terms of
precision-recall curves. Obviously, SCM obtains higher precision than CCA and CM at
all levels of recall for text to image retrieval task and almost levels of recall for image to
text retrieval task. The same insight can be acquired about the performance improve-
ments. Comparing the PR curves of CLCC with others, we can see that the precision of
our method is higher than others at all levels of recall for both forms of cross-modal
retrieval.
In short, we can conclude that the semantic correlation can be exploited fully by
combining link and content information for cross-modal retrieval.
Combining Link and Content Correlation Learning
523

(a)
(b)
Fig. 2. MAP performance of (a) image to text and (b) text to image retrieval task for each
category.
Fig. 3. Precision recall curve of (a) image to text and (b) text to image retrieval task.
524
L. Zhang et al.

5
Conclusion
In this paper, we examined the problem of correlation learning for cross-modal retrieval
in heterogeneous social multimedia networks. We have proposed an effective learning
approach by combining link and content information into together to improve the
performance of semantic correlation in a uniﬁed projection subspace. Different from
traditional approaches, we integrate rich link structure to obtain accurate and robust
representation. Furthermore, there are obvious semantic meanings in the uniﬁed space
through embedding semantic label information. Extensive experiments have veriﬁed
the better performance of proposed approach comparing with several prevailing
approaches. In the future, we intend to exploit nonlinear projection to obtain uniﬁed
feature representation.
Acknowledgments. This work is supported by Chinese National Nature Science Foundations
(61501050).
References
1. Sang, J., Xu, C., Jain, R.: Social multimedia ming: from special to general. In: IEEE
International Symposium on Multimedia, pp. 481–485 (2016)
2. Hardoon, D.R., Szedmak, S., Shawe-Taylor, J.: Canonical correlation analysis: an overview
with application to learning methods. Neural Compute. 16(12), 2639–2664 (2004)
3. Li, D., Dimitrova, N., Li, M., Sethi, I.: Multimedia content processing through cross-modal
association. In: Proceedings of ACM International Conference on Multimedia, pp. 604–611
(2003)
4. Rasiwasia, N., Pereira, J.C., Coviello, E., Doyle, G., Lanckriet, G., Levy, R., Vasconcelos,
N.: A new approach to cross-modal multimedia retrieval. In: Proceedings of the ACM
International Conference on Multimedia, pp. 251–260 (2010)
5. Andrew, G., Arora, R., Bilmes, J., Livescu, K.: Deep canonical correlation analysis. In:
Proceedings of the 30th International Conference on Machine Learning, pp. 1247–1255
(2013)
6. Rosipal, R., Krämer, N.: Overview and Recent Advances in Partial Least Squares. In:
Saunders, C., Grobelnik, M., Gunn, S., Shawe-Taylor, J. (eds.) SLSFS 2005. LNCS, vol.
3940, pp. 34–51. Springer, Heidelberg (2006). https://doi.org/10.1007/11752790_2
7. Huang, L., Peng, Y.: cross-media retrieval by exploiting ﬁne-grained correlation at entity
level. Neurocomputing 236, 123–133 (2017)
8. Peng, Y., Zhai, X., Zhao, Y., Huang, X.: Semi-supervised cross-media feature learning with
uniﬁed patch graph regularization. IEEE Trans. Circuits Syst. Video Technol. 26(3), 583–596
(2016)
9. Jia, Y., Salzmann, M., Darrell, T.: Learning cross-modality similarity for multinomial data.
In: Proceedings of the 11th International Conference on Computer Vision, pp. 2407–2414
(2011)
10. Han, X., Thomas, S.: Toward artiﬁcial synesthesia: linking images and sounds via words. In:
NIPS Workshop on Machine Learning for Next Generation Computer Vision Challenges
(2010)
11. Wang, S., Huang, Q.: Research on heterogeneous media analytics: a brief introduction.
J. Integr. Technol. 4(2), March 2015
Combining Link and Content Correlation Learning
525

12. Sun, Y., Han, J.: Mining heterogeneous information networks: a structural analysis
approach. SIGKDD Explor. 14(2), 20–28 (2012)
13. Jin, X., Luo, J., Yu, J., Wang, G., Joshi, D., Han, J.: Reinforced similarity integration in
image-rich information networks. IEEE Trans. Knowl. Data Eng. 25(2), 448–460 (2013)
14. Sun, Y., Han, J., Yan, X., Yu, P., Wu, T.: Pathsim: meta path-based top-k similarity search
in heterogeneous information networks. VLDB 4(11), 992–1003 (2011)
15. Wang, K., He, R., Wang, W., Wang, L., Tan, T.: Learning coupled feature spaces for
cross-modal matching. In: IEEE International Conference on Computer Vision, pp. 2088–
2095 (2013)
16. Chua, T., Tang, J., Hong, R., Li, H., Luo, Z., Zheng, Y.: NUS-WIDE: a real-world web
image database from National University of Singapore. In: Proceedings of ACM
International Conference Image Video Retrieval, pp. 1–9 (2009)
526
L. Zhang et al.

The Research on Touch Gestures Interaction
Design for Personal Portable Computer
Qing Sheng1,2, Ting Liu1,2, Wenjun Hou1,2(&), and Gengyi Wang1,2
1 School of Digital Media and Design Art, Beijing University of Posts
and Telecommunications, Beijing, China
25389640@qq.com, hou1505@163.com
2 Network System and Network Culture Key Laboratory of Beijing,
Beijing, China
Abstract. This paper studies the gestures interaction of personal portable
computer with user deﬁned methods on different operating objects. According to
the data analysis on experimental result, we developed a set of gestures for
personal portable computer which can meet different users’ needs. In accordance
with touch gestures, we can divide touch gestures into three categories: basic
gestures, symbol gestures and combination gestures. Based on this gestures
database, developer and designer can design touch gestures interaction which
match the product features quickly and in a reasonable way.
Keywords: Touch gestures  Interaction design  Personal portable computer
1
Related Researches
Touch screen technology has been applied to mobile phones for 16 years and the
exploration of touch gesture interaction has always been the focus of researchers.
User-Defined Gestures for Surface Computing [1] written by Jacob O. Wobbrock
etc. deﬁned the commonly used gestures and basic gestures of desktop large screen,
deeply discussed the inﬂuence of human cognitive behavior on hand gesture interac-
tion. Based on user deﬁned design method in intuitive interaction domain, foreign
scholars designed Interactive behaviors on touch screen mobile phone, somatosensory
operation device. Besides, there are also many scholars study on the pain point of the
touch screen interaction [2–4], such as “click low precision”, “fat ﬁnger” and so on.
Cedric Foucault et al designed a two-handed interaction model “Spad” [5] to enhance
the productivity of tablet, this interactive model adopts non master hand to activate the
functional mode and use a tablet application “Spad” interactive control mode cooperate
with context components in “Keynote” to complete the same task. By comparing the
operations conﬁrmed that “Spad” completes the task more efﬁciently without
increasing the complexity. But “Spad” requires that all tasks be divided into four
groups. Each of groups contains three buttons that brings functional limitations.
To draw a conclusion, researchers have proposed their own design methods and
solutions to the pain points of mobile phone touch screen gesture interaction. However,
there is no research on the design of large screen personal portable computer to provide
a uniﬁed guidance for personal portable computer touch screen gesture interaction.
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 527–537, 2018.
https://doi.org/10.1007/978-3-319-74521-3_55

2
Experimental Study
2.1
Experimental Subject
We invited 12 college students to participate in the experiment, all of them were
graduate students, among them, 5 were female, and the others are male. Their age
ranged from 20–25 7 formal experiment, there were no long time intense hand
movements. All of them had used touch screen equipment in the past and are familiar
with the touch operation. The subjects had 3 min to know the page layout and basic
operations before task.
2.2
Experimental Task
The experiment presented three kinds of common operation tasks, which were the basic
operation, the shape operation and the text operation while different target tasks had
different pre conditions. In order to facilitate the experiment, the extracted tasks were
divided into three usage scenarios. In order to avoid the contextual effects caused by
different tasks, we arranged the sequence of the task process. This also could avoid
interrupting some continuous operation tasks, such as Copy and Paste task (Table 1).
2.3
Experimental Equipment
The device used in the experiment were iPad Pro (12.9 in.), the operating system
platform was IOS10.0, having smart keyboard.
2.4
Experimental Process
Subjects had 3 min to get familiar with the layout of the user interface. In the formal
experiment. The task interface in the scene has prompt message which didn’t contain
the contents of the gestures. Subjects needed to deﬁne gestures themselves to complete
the corresponding task. In the experiment, sounding thinking was adopted, the subjects
need to inform the main reason of some gestures. After each task, the subjects were
asked to score the subjective perceptions of gestures, including the matching degree
between the gestures and target task, the accessibility of operations etc.
2.5
Data Treating
564 motion data were recorded in the experiment. Based on the data, we summarized
the frequency, the predictability, the consistency and the subjective evaluation of the
gestures.
2.5.1
Frequency
Select the most frequent gestures as standard of experiment task and remove the
gestures in poor consistency, we achieved the target collection of gesture. The corre-
sponding gestures and their frequency are shown in below tables (Tables 2, 3 and 4).
528
Q. Sheng et al.

Table 1. Experimental task operation
The Research on Touch Gestures Interaction Design
529

Table 2. Basic operation gestures
Task
Gestures
Operating mode
Frequency
Building a new default
format slide
Double-click the blank area
4
Specified location building a new
default format slide
Double-click the interval area
5
Move the slide
Press and hold to drag
10
Switch to the slide show view
Five fingers grasping
4
On one piece
underscore
9
The next
On the cross
9
The slide start playing from the
current position
Double-click
5
Exit the slides
Double-click
5
Delete the slides
Hold and slip out of the screen
5
Back out
Counterclockwise
7
Reform
Clockwise
7
Zoom in slide on this page
Expand
11
Zoom out slides on this page
Shrink
11
530
Q. Sheng et al.

Due to the current page layout will be trimmed by the more frequent task like copy,
cut and paste, the page layout framework will have an impact on the process of user
deﬁned gestures, therefore, the ﬁnal data of these three tasks are eliminated in the data
statistics.
2.5.2
Predictability
Through gestures set and the frequency of each gesture, the Predictability of the ges-
tures set can be calculated:
G ¼ P
s2S PS
j
j
P
j j
 100%0
In the formula: G stands for Predictability, P represents the total number of actions
deﬁned by the user in task, Ps represents the number set of “S”. s is a subset of S, the
predictable sequence of tasks is shown in the following ﬁgure (Fig. 1).
Table 3. Shape operation gestures
Pre
condition
Task
Gestures
Operating mode
Frequency
-
Access edit text
state
Double-click
7
Delete
Hold and slip out
the screen
7
Multiple
choice
Rotating
Compatible with
three kinds of
rotation
7
Zoom in shape
Two fingers
extended
8
Reduce the shape
Two fingers
pinched
8
Copy
Two fingers drag
into the clipboard
-
Multiple/
all
Cut
One finger into
the clipboard
-
Paste
One finger drag
the clipboard
-
The Research on Touch Gestures Interaction Design
531

Table 4. Text operation gestures
Cursor
calibration
Choice
Task
Gesture
Operating mode
Frequency
delete
Hold and slip out the
screen
5
Zoom in the
words
Two fingers extended
7
Zoom out
text
Two fingers pinched
7
Alignment
Drag and drop two points
to target alignment
direction
5
Copy
Two fingers drag into the
clipboard
-
Cut
One finger drag into
clipboard
-
Paste
One finger drag out of
clipboard
-
Fig. 1. Predictable sequence of gestures for each task
532
Q. Sheng et al.

2.5.3
Uniformity
After calculating the predictable nature of gesture set, the consistency of gesture set can
be calculated.
A ¼
P
r2R
P
PiPr
Pi
j
j
Pr
j
j

2
R
j j
 100%
In this formula: A indicates consistency score, R represents the total number of
actions for the target task deﬁned by the user, r is a character for the target mission R, P
represents the total number of actions for a task R, Pi is a subset of Pr that represents
the number of an actions (Fig. 2).
2.5.4
User’s Subjective Evaluation
From the following table, we can see the matching degree and accessibility scores.
Creating a new slide, moving the slide, switching the slide, deleting the slide, slide
zoom, activating text edition, shape zoom got higher scores, indicating the task
operations in the gesture set are easily understood and implemented by the user at the
cognitive level. While switching the slide and playing the slides are easy to implement,
but the matching degree is low. Through the interviews we know that this is because
the subjects afraid to conﬂict with the operating system in the process of deﬁning the
gesture, so that the results were also affected (Fig. 3).
Fig. 2. Sequence of gestures for each task
The Research on Touch Gestures Interaction Design
533

3
Personal Portable Device Resture Ttype
By summarizing the experimental data further, generally, the gestures can be used in
personal portable computer are divided into: basic gestures, symbol gestures, combined
gestures.
3.1
Basic Gestures
Basic gestures including: click, press, swipe, drag, zoom, rotates, complete the func-
tional operation of common application, such gestures are versatile (Table 5).
3.2
Basic Gestures
Symbolic gestures follow the user’s daily life and graphical interface experience,
continue to use the book written symbols or icon that from graphical user interface to
the touch screen interface. For example, most users use the counterclockwise circle
shape to represent revoking, the symbol is similar to the revoke icon in the graphical
interface, and counterclockwise circle also gives the user a psychological hint of time
reversal. In this way, you can extend the other gesture deﬁnitions of other functions,
such as text bold, tilted, underlined function can be expressed through the gesture to
draw the graphics that similar to the icon, for example, using “B”, “I”, “U” to
manipulate text attributes. Symbolic gestures rely on the user’s experience in the daily
life and graphical interface experience, the symbol that can be used is limited, and its
scalability is relatively low (Fig. 4).
Fig. 3. User’s subjective evaluation of each task
534
Q. Sheng et al.

Table 5. Symbolic gestures
Main screen
In list
Thumbnail
Detailed view
Control
Text editing
Picture 
editing
Shape 
editing
Form
Click
Open
Adjust 
the 
corresponding 
item
Adjust the corresponding item
Stop the 
sliding item
Active control 
function
Move cursor 
position
Select
Double click
Scale / Switch 
view
Intelligent 
phrase 
selection
Select
Enter input status
Triple click
Select whole 
segment
Long press
Active 
application 
editing
Corresponding items get into edit 
status
Activate hidden functions
Activate hidden functions
Single finger 
drag
Mobile 
shortcut 
location
Move project location 
/Delete
Delete
Move cursor
position like 
fisheye
Delete
Move position/Delete
Double finger 
drag
copy
copy
Horizontal 
sliding
Switch main 
screen 
Transfer 
delete
Switch to 
another 
sibling page
Horizontal 
movement
Switch to 
another 
Operation 
control
Vertical slip
Open 
Notification 
bar
toolbar
Vertical moving to certain 
page
Vertical 
moving Switch 
to another 
sibling page
Operation 
control
Four finger 
slip
Switching application
View the background running APP
Zoom
Return to 
previous level
Two finger 
retraction Three 
or four finger 
switch view
Scale
Five finger 
zoom
Close the application back to the main screen
Rotate
Rotate the 
picture
Rotate
The Research on Touch Gestures Interaction Design
535

3.3
Combination Gestures
Combination gestures refer to the simultaneous completion of gestures through two
hands or the completion of two different types of gestures in one hand, including three
types of gestures: hands combination of homogeneous anisotropy gestures, hands
combination of heterogeneous gestures, and one hand heterogeneous combination
gestures.
3.3.1
Hands Combination of Homogeneous Anisotropy Gestures
Hands input requires the user to interact with hands of a high degree of coordination,
coherence, avoiding the challenge of “Draw a circle with one hand, and draw a square
with the other”. On the other hand, the input of both hands requires a high degree of
concentration of the user, if the task is too complicated that the cognitive burden in the
implementation of the task will be increased. So in most cases, the hands interactive
gestures are mostly through the hands to complete the anisotropy homogeneous gesture
operation, such as the “rotation” “text center alignment” “text on both sides of the
alignment” etc.
3.3.2
Hands Heterogeneous Combinations
Hands heterogeneous combinations refer to the completion of static gestures by one
hand, the completion of dynamic gestures by the other hand. Guiard believes that the
operating mechanism of both hands is based on the co-operation and asymmetry of
both hands [6]. The movement of the non-conventional hand is more granular than that
of the conventional hand. Therefore, the non-conventional hand can be used to activate
the operation mode of some function and the master hand can be used to operation or
do some ﬁne adjustment. In the experiment, users complete the multi-election operation
through the “one ﬁnger long press, one ﬁnger click” or “one ﬁnger long press, one
ﬁnger drag”, complete the “shape rotation” operation through “one ﬁnger hold the
shape, one ﬁnger rotate the shape”. Such gestures increase the user’s ability to activate
the gesture memory of the functional modiﬁers for non-master controls, but can
improve the operational performance of the expert user.
3.3.3
One Hand Heterogeneous Combination
One hand heterogeneous combination refers to a group of different combinations of
gestures that completed by one hand, such as the continuous touch gestures that Google
created for Android device. In Google’s gestures, users are allowed to draw the letters
“g” and circle the target search term continuous, or cover the target search with the
combination shape of “g” and “o”, you can search for the target vocabulary quickly.
Fig. 4. Symbolic gesture manipulation
536
Q. Sheng et al.

4
Conclusion
At present, as a “productivity tool”, large-screen personal portable computer has not yet
formed a complete and systematic touch-screen operation model in the people’s
understanding. We applied the method put forward by Wobbrock and others in the
personal portable computer gestures interactive design, summarized a set of gesture set
that for different operators of the personal portable computer. We also divided the
personal portable computer gestures are into three categories of basic gestures, symbol
gestures, and combined gestures in accordance with the gesture operation model, and
summed up the specialty of three types of gestures. Based on this gesture set, it is
convenient for the personal portable computer designers to build gestures that conform
to the user’s mental model, and giving the user a more natural and intuitive interaction
experience.
References
1. Wobbrock, O., Morris, M.R., Wilson, A.D.: User-deﬁned gestures for surface computing. In:
Proceedings of CHI, no. 9, p. 183 (2009)
2. Wobbrock, O.: Maximizing the guessability of symbolic input. In: CHI Extended Abstracts,
no. 5, pp. 69–72 (2005)
3. Ruiz, J., Li, Y., Lank, E.: User-deﬁned motion gestures for mobile interaction. In: Proceedings
of the SIGCHI Conference on Human Factors in Computing Systems, Vancouver. ACM
(2011)
4. Abadi, H.G.N., Peng, L.Y., Zadeh, A.M.H.: Guessability study on considering cultural values
in gesture design for different user interfaces. In: International Proceedings of Economics
Development & Research, no. 4, pp. 76–80 (2012)
5. Foucault, C.: Spad:a bimanual interaction technique for productivity applications on
multi-touch tablets. In: CHI, no. 4, pp. 1879–1884 (2014)
6. Guiard, Y.: Asymmetric division of labor in human skilled bimanual action: The kinematic
chain as a model. J. Motor Behav. 19(4), 486–517 (1987)
The Research on Touch Gestures Interaction Design
537

Research on Mobile User Dynamic Trust
Model Based on Mobile Agent System
Weijin Jiang1,2,3 and Yuhui Xu1(&)
1 School of Computer and Information Engineering,
Hunan University of Commerce, Changsha 410205, China
nudtjwj@163.com, 363168449@qq.com
2 Electronic Information Engineering Department, Changsha Normal University,
Changsha 410100, China
3 School of Computer Science and Technology,
Wuhan University of Technology, Wuhan, China
Abstract. For mobile Internet people’s personalized service needs, how to
move from the vast number of mobile information in real time, accurate access
to mobile users really interested in the content. In order to obtain more accurate
mobile user’s preferences to meet the requirements of personalized services, this
paper propose a new mobile user’s preference prediction method based on trust
and link prediction by analyzing the mobile user behavior. Firstly, this paper
propose a method to calculate the trust of mobile users by analyzing the
behavior of mobile users; Then according to the similarity of the mobile user’s
trust and the mobile user’s score, the approximate neighbor of the mobile user is
selected; we use the link prediction method to calculate the correlation between
mobile users and mobile network services and determine mobile network ser-
vices that needed predict; Finally, we use this method to predict the user’s
preference. The research shows that the prediction accuracy of this method is
better than traditional method of Collaborative Filtering recommendation, which
solves the sparsity problem to some extent.
Keywords: User’s personality feature  Mobile agent system
Collaborative Filtering (CF)  Trustworthiness  Mobile internet
1
Introduction
Due to the rapid development of mobile communication technology and intelligent
mobile devices, mobile phones have become one of the main platforms for people to
obtain information. With the development of multimedia technology and mobile
information loading, mobile transmission capability, mobile e-commerce and mobile
Internet Service Mall is also more people to use, but the huge mobile network service
information for mobile users has brought serious overload problems of mobile infor-
mation. Therefore, how to get the real interest of mobile users in real-time and accuracy
from mobile information has become a difﬁcult problem to be solved in personalized
mobile network service [1, 2]. Among the many methods for predicting user prefer-
ences, the collaborative ﬁltering algorithm is the most classic and the most widely used.
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 538–549, 2018.
https://doi.org/10.1007/978-3-319-74521-3_56

However, when user preference prediction is carried out by using the traditional col-
laborative ﬁltering method, the prediction accuracy of the user preference is relatively
low due to the sparsity problem. At the same time, due to the limited input and output
capability of mobile phones and the characteristics of mobile user’s real-time access to
information, mobile network users have higher demand on mobile users’ forecasting
accuracy [3]. Traditional user preference forecasting methods are not suitable for
personalized mobile network service system.
The basic idea of collaborative ﬁltering is to ﬁnd the nearest neighbor of the target
user by calculating the similarity between the users and then to predict the interest
preference of the target user by the nearest interest preference of the neighbor. How-
ever, with the expansion of data size, data sparsity problems gradually appear, it leads
to algorithm efﬁciency and accuracy reducing. Some scholars mitigate this problem by
integrating trust relationships between users into traditional collaborative ﬁltering. In
literature [4], the trust relationship between mobile users is calculated by the mobile
user’s explicit score. However, in the mobile communication network, because the
mobile phone’s input and output capability is limited, the explicit evaluation infor-
mation between mobile users is relatively small, Generally, we use the implicit cal-
culation method to obtain the trust relationship among mobile users. In literature [2],
for the characteristics of mobile communication networks, the trust relationship
between mobile users is introduced into the traditional collaborative ﬁltering, and the
trust relationship between mobile users is analyzed by analyzing the communication
behavior between mobile users and gives a linear calculation method to calculate the
trust of mobile users. In literature [5], it describes an e-mail-based social network
computing method, which analyzes the communication behavior between users to build
a social network, and through experiments to prove that based on the logarithm
function to calculate the user trust relationship is more accurate than that through the
linear function [6].
In addition, some researchers solve the sparsity problem from another aspect. In
literature [7], it propose a collaborative ﬁltering improvement method based on link
prediction, which builds a network model that connects users and project nodes and by
analyzing the relation between the user and the project can effectively reduce the
sparsity problem existing in the traditional collaborative ﬁltering. In the traditional link
prediction method, node only represents the user/project, edge only represents the
relationship between the user/project, and the literature [7] improved the traditional link
prediction methods, nodes in the network represent users and items, and edges rep-
resent relationships between the user and the project. Although the literature [7]
reduces the sparsity problem of collaborative ﬁltering through link prediction, it does
not consider the impact of trust on mobile user preference prediction for the charac-
teristics of mobile communication network.
In this paper, we analyze the mobile user behavior to calculate the trust degree
between mobile users and select the approximate neighbor of the target user by
combining with the similarity of the mobile user’s preference; Then, the improved link
prediction method is used to calculate the correlation information between the mobile
user and the mobile network service, and the mobile network service which the target
user is most likely to use is determined according to the calculated correlation; Finally,
Research on Mobile User Dynamic Trust Model
539

based on the above research, a mobile user preference prediction method based on trust
and link prediction is proposed.
2
Mobile User Preference Prediction Model
In order to improve the accuracy of mobile user preference forecasting, this paper
introduces the trust degree between mobile users and the correlation between mobile
user-mobile network services into the mobile user preference prediction model [8].
2.1
Data Model
There are a large number of user communication and interactive information in the
mobile communication network, mainly including the communication records such as
voice communication, short message, ﬂying letter and some evaluation information of
the mobile network service by the mobile users. In this paper, we build the data model
based on these data, including four data set [8]:
(1) Mobile user Set: U ¼ fuiji 2 ½1; Ng, in this collection N is the number of mobile
subscribers
(2) Mobile Web Services Set [9]: S ¼ fsjjj 2 1; M
½
g, The mobile network service
includes basic communication service and value-added service, such as down-
loading software, browsing web page, e-commerce, receiving and sending mail,
customizing service. M denotes the number of mobile network services.
(3) Mobile subscriber service network [10]: The evaluation behavior of the mobile
user in the set U to the mobile network service in the set S is denoted by Gus
< Vus, Eus >, Vus represents the set of mobile users and the set of all mobile
network services that need to be processed, the edge e\u; s [ 2 Eus denotes the
mobile network service s used by the mobile user u. For the weights w < u, s >
between the edges, it is the rating of the mobile user u for the mobile network
service s. The mobile users ‘rating of mobile network services is extracted from
the feedback, evaluation and other records of the mobile services used by the
mobile users, and combined with the mobile user’s usage of the related mobile
services.
(4) Mobile users trust network [11]: The trust relationship between mobile users in
the set U is expressed by G uu < Vuu, Euu >, Which Vuu denotes a set of all mobile
communication network users, e\ui; uj [ 2 Euu, indicating that there is a trust
relationship between the mobile users ui and uj . For the weight w < ui, uj >
between the edge and the edge, it represents the trust degree of the mobile user uii
to the mobile user uj. The trust degree between the mobile users is obtained by
analyzing the communication behavior among the mobile users. If there are more
connections between mobile users ui and uj, the trust between mobile users ui and
uj is high, and on the contrary, is relatively low.
540
W. Jiang and Y. Xu

2.2
Mobile User Trust Calculation
Combining the characteristics of mobile communication behavior and the transitivity of
trust, this paper deﬁnes direct trust and indirect trust as follows:
Deﬁnition 1 (direct trust): If there is communication between mobile users, and trust is
greater than the set threshold k, then there is a direct trust relationship between mobile
users.
Deﬁnition 2 (indirect trust): If there is no direct trust relationship between mobile
users, but there is a common trust neighbor, then there is an indirect trust relationship
between users.
The calculation procedure of trust among mobile users in mobile communication
network is as follows:
(1) Preprocessing mobile user communication behavior, deleting incomplete data and
noise data;
(2) According to the communication behavior between mobile users, the initial
mobile user trust network Guu1 < V, E > is constructed, that is, if there is a call or
text message between the mobile user ui and uj, ui and uj are connected. Because
the degree of trust between users is asymmetric, so Guu1 < V, E > is a directed
weighted graph [11].
(3) Calculate the direct trust between mobile users: With the increase of the amount of
trafﬁc, the trust degree between the mobile users tends to be stable with the
increase of the amount of trafﬁc, which is consistent with the theory of marginal
utility decreasing. Therefore, this paper uses logarithmic function to express the
relationship between the them. For example, the mobile user ui, in his commu-
nication record is mainly for text messages and call information statistics. First,
the total number of statistical messages p and the number of messages puj cor-
responding to the mobile user uj serving as the receiving party are counted, and
then the total time q of the user ui is counted, the total number of calls r and the
mobile user uj serving as the called party and the total time quj, the total number of
calls ruj. Deﬁne T (ui, uj) as the trust of ui for mobile user uj, deﬁne T muj; m


as ui
or uj trust in a communication behavior. In it, muj 2 fpuj; quj; rujg, and
m 2 fp; q; rg, so T muj; m


can be expressed as
T muj; m


¼ 2 1  1= ln muj


1  1=ln m ¼ mj




2  1= ln muj  1=ln m  muj


ð1Þ
(4) The direct trust between mobile users can be expressed as
T u; uj


¼ T puj; p


þ T quj; q


þ T ruj; r


3
ð2Þ
So, In Guu1 < V, E >, The weight between e < ui, uj > is T (ui, uj);
Research on Mobile User Dynamic Trust Model
541

(5) Each user node in U repeat (3) to form Guu2 < V, E >;
(6) Calculate the indirect trust between mobile users. In the Mole Trust algorithm
proposed in document [8], we introduce the attenuation factor into the trust cal-
culation, and think that the trust degree is declining in the transmission process.
According to Deﬁnition 2, when calculating the indirect trust degree, because
there may be multiple co-trust neighbors between the two mobile user nodes, the
indirect trust degree can be calculated by the method of trust synthesis [12]. Let
the conﬁdence factor in the transmission process is D, taking the mobile user
nodes ui and uk as an example, if the node uj is the common trust neighbor, the
indirect trust degree of ui through uj and node uk can be expressed as
Tj ui; uk
ð
Þ ¼ D  Min T ui; uj


; T uj; uk




ð3Þ
For each of the mobile users uii and uk, the trust neighbor uj calculates Tj (ui, uk), and
ﬁnally obtains the indirect trust degree of ui and uk. The formula can be expressed as
T ui; uk
ð
Þ ¼
XL
j Tj ui; uk
ð
Þ=L
ð4Þ
Where L represents the total number of mobile users ui and uk common trust
neighbors.
(7) For each pair of mobile user nodes satisfying Deﬁnition 2 for Guu2 < V, E >
carried out (5), forming a complete mobile user trust network Guu3 < V, E >.
2.3
User - Service Dependency Calculations Based on Link Forecasting
Traditional collaborative ﬁltering mainly predicts user preferences by calculating the
similarity between users or between projects, and less consideration of the relationship
between users and projects. In document [7], the data model established by link pre-
diction shows the relationship between the two, and the improved link prediction
method is given. Jaccard’s Coefﬁcient is one of the commonly used methods in link
prediction [7], but this method does not apply to the calculation of user-service rele-
vance in link forecasting model, so this paper redeﬁnes it and gives the user-service
correlation calculation formula
corr u; s
ð
Þ ¼ C u
ð Þ \ C sð Þ
C u
ð Þ [ C sð Þ


ð5Þ
where C(u) represents the set of neighbor nodes of node u, C(s) represents the set of
neighbor nodes of node s and C sð Þ ¼ S
c 2 C sð Þ C c
ð Þ.
2.4
Mobile User Similarity Calculation
In this paper, the Pearson correlation coefﬁcient is used to calculate the similarity
between mobile users [13]. The formula is expressed as
542
W. Jiang and Y. Xu

simðui; ujÞ ¼
P
s2Suiuj rui;srui
ð
Þ ruj;sruj
ð
Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
P
s2suiuj
rui;srui
ð
Þ
2P
s2suiuj
ruj;sruj
ð
Þ
2
q
;
Suiuj

  t  min Sui
j
j; Suj




0;
Suiuj

\t  min Sui
j
j; Suj




8
>
<
>
:
ð6Þ
Where Sui and Suj represent the scoring service set of mobile users ui and uj
respectively, Suiuj represents the service set, rui;s and ruj;s of the common score of
mobile user ui and uj, respectively represent the score of mobile user ui and uj to the
service, rui represents the average score of the mobile user ui for the rating service.
2.5
Mobile User Preference Prediction
In order to improve the prediction result of mobile user preference [8, 11], this paper
improves the traditional collaborative ﬁltering method because of the low accuracy and
sparseness of traditional collaborative ﬁltering technology. In the approximate neighbor
search target user, fusion as approximate neighbor weights will move the similarity
between trust and mobile user preferences between users; and then through the link
prediction method to determine the mobile network services for mobile users are most
likely to use; ﬁnally improved by preference prediction formula to predict the mobile
user preferences. Speciﬁc methods are as follows:
(1) Determine the approximate neighbor set of the mobile user
In the process of mobile user preference prediction, this paper not only considers
the impact of similarity between mobile user preferences on forecasting results, but also
considers the impact of mobile users’ trust on mobile user preferences and the har-
monic neighbor is used to select the approximate neighbor set of the target user.
Reconciliation weights are determined by the degree of similarity of scoring and the
degree of trust of the user. When the trust between mobile users is relatively large (for
example, mobile users ui and mobile users uj is a good friend), the trust of the weight
value is relatively large; When the trust between mobile users is relatively small (for
example, mobile users ui and mobile users uj is strangers), the similarity of the weight
value is relatively large. So the formula for the harmonic weight is expressed as
Wðui; ujÞ ¼
a1sim ui; uj


þ b1T ui; uj


; T ui; uj


 k
a2sim ui; uj


þ b2T ui; uj


; T ui; uj


\k

ð7Þ
Where W (ui, uj) represents the harmonic weight of the mobile user ui and the
mobile user uj; sim(ui, uj) represents the similarity degree of the mobile user ui and the
mobile user uj; T(ui, uj) represents the trust of the mobile user ui and the mobile user uj,
a1, a2, b1, b2 are the harmonic factors, and a1 + b1 = 1, a2 + b2 = 1, k is the trust
threshold.
When determining the approximate neighbor set of mobile users, the approximate
neighbor of t% is selected as the approximate neighbor set of the target user. When
t = 0, the approximate neighbor set is empty set, t = 100, then the approximate
neighbor set includes all the mobile users in the data set, so we cannot get the desired
Research on Mobile User Dynamic Trust Model
543

result when the value of t is too small or too large. In this paper, according to a number
of experimental results to select a reasonable value of t.
(2) Identify the target mobile network service set that needs to be predicted
According to literature [14], the higher the correlation between the mobile user and
the mobile network service, the greater the possibility that the mobile user will use the
corresponding mobile network service in the future. Therefore, when determining the
target mobile network service set that needs to be predicted, selecting the relevance of
the former h% of the mobile network services as the target user is most likely to use a
collection of mobile network services.
(3) Predict mobile user preferences
Combining the harmonic weight with the traditional preference prediction formula,
the weighted average forecast formula [15] is
rui;s ¼ rui þ k
X
uj 2 Nui
s 2 Sui
W ui; s; uj


 ruj;s  ruj




ð8Þ
Where Nui is the nearest neighbor node set for mobile user ui, Sui is the set of
mobile network services needed for mobile user ui, k is a normalization factor
k ¼ 1= P
uj 2 Nui
s 2 Sui
6W ui; s; uj


:
3
Experiment and Analysis
3.1
Introduction to Data Sets
In order to verify the feasibility and validity of the proposed method in the mobile
network service environment, the experimental data were validated by two data sets.
One is the MIT data set based on mobile communication behavior provided by Mas-
sachusetts Institute of Technology (MIT); One is publicly available for movie score
data sets Filmtipset. The disadvantage of the MIT dataset is that the number of mobile
users is relatively small. The number of mobile network services is relatively small.
Therefore, in order to further verify the validity of the method, the ﬁlmtipset dataset is
used to verify. Although the Filmtipset data set is not for the mobile network to collect,
but the data set contains a user score for the project as well as the user’s social
relationship, so you can verify the validity of the method in a certain extent
(1) The MIT data set [16] is public data provided by the Massachusetts Institute of
Technology media lab set, the data set is a collection of 94 mobile users, mobile
user behavior information from September 2004 to June 2005 a total of 9 months,
including mobile communication and mobile Internet services using mobile users,
mobile context information related to the user’s friends and relations.
(2) Filmtipset [17] is Switzerland’s largest ﬁlm recommendation community, which
has more than 80000 registered users and more than twenty million movie rating
544
W. Jiang and Y. Xu

data. The Filmtipset community not only provides the relevant scoring informa-
tion based on user records, but also includes the social relations between users. In
this paper, we randomly selected 500 of the users and these users to make the
evaluation of the 1000 ﬁlms for the experiment.
Since the MIT data set does not include the user’s evaluation of the mobile network
service, the data set is processed as follows before the experiment: By using the
behavior of the mobile service for all users in the nine months, the user’s preference for
the mobile service is sorted in the order of time, and the user’s preference information
is quantized in the form of scores (1 to 10).
During the experiment, the two data sets are processed in the same way, each data
set is divided into ﬁve groups randomly, each group is divided into two parts, one is the
training set, the other is the test set, which training set accounts for 80% of each group
of data, test set accounted for 20%.
3.2
Evaluation Standard
In this paper, Mean Absolute Error (MAE) [3] and F measure [18] were used as the
evaluation criteria.
The MAE measures the accuracy of the forecast by calculating the deviation
between the predicted user score and the actual user score. The smaller the MAE value,
the higher the accuracy of the prediction. Assume that the predicted score set is rep-
resented as {p1, p2, …, pn}, The corresponding actual score set is expressed as {q 1, q2,
…, qn}, MAE is deﬁned as
MAE ¼
Xn
i¼1 pi  qi
j
j=n
ð9Þ
The F indicator is a comprehensive assessment of the accuracy and recall rate, The
higher the F index is, the higher the accuracy is, In this paper, according to the
experimental results evaluation criteria for the precision and recall rate is redeﬁned as
follows:
P u
ð Þ ¼ u number of trusted users
ð
Þ= u the number of contacts
ð
Þ
ð10Þ
R u
ð Þ ¼ u number of trusted users
ð
Þ= u the number of good friends
ð
Þ
ð11Þ
among them u 2 U, F index:
F u
ð Þ ¼ ð2  P u
ð Þ  R u
ð ÞÞ=ðP u
ð Þ þ R u
ð ÞÞ
ð12Þ
3.3
Analysis of Experimental Results
(1) Comparison of the accuracy of trust and trust threshold selection
This experiment is carried out on the MIT data set, this paper mainly compares the
accuracy of the trust degree calculated by the logarithm function and the degree of trust
Research on Mobile User Dynamic Trust Model
545

based on the linear function. The range of the indirect trust attenuation factor D is set to
[0.5, 1.0), the step size is 0.05, according to the experimental results, the D is set to
0.55. The threshold value of the conﬁdence threshold is set to [0.1,0.9] and the step
length is 0.1. The experimental results are shown in Fig. 1. You can see from Fig. 1,
based on the logarithmic function of trust calculation method is superior to linear
function calculation method based on trust, this is because between user preference and
trafﬁc with diminishing marginal utility theory, the logarithmic function can better
reﬂect the relationship between them.
You can see from Fig. 1, the F index is less than 0.6 in a general increasing trend,
in the case of more than 0.6 overall decreasing trend, this is because when a value is
small, too many users trust, when is large, remove too much trust users, both caused a
decline in trust the degree of accuracy. The results show that when the threshold of trust
is 0.6, the F index is larger. In this paper, the trust threshold is set to 0.6.
(2) Harmonic weighting parameter selection experiment
This paper makes use of genetic algorithm in Matlab (Genetic Algorithm GA)
toolbox respectively on the two datasets, the parameter a1 and b1 and a2 and beta b2
tuning experiments. The ﬁtness function of genetic algorithm is deﬁned as follows:
nMRE ¼
Xn
i¼1
pi  qi
j
j
qi


=n
ð13Þ
In this paper, the range of the harmonic eights a1 and a2 in the Eq. (7) is set to a1 2
(0,1) and a2 2 (0,1), while b1 = 1–a1 and b2 = 1–a2. The optimal parameter values
obtained by genetic algorithm optimization are shown in Table 1.
From the experimental results, we can see that in determining the mobile user
approximate neighbors, if the trust is greater than a given threshold, the trust plays a
major role. If the degree of trust is less than a given threshold, the score similarity plays
a major role. This is because people are always more trusted about the more familiar
people, and their preferences are more likely to be affected by the surrounding people
(such as family, friends, etc.), and for relatively unfamiliar people, their preferences are
primarily inﬂuenced by user preferences that are similar to their ratings. When the trust
is greater than a given threshold, mobile users are more likely to be family members,
friends, etc., and trust is less than a given threshold, the mobile users are more likely to
be strangers [19, 20].
Table 1. Tuning parameter values
Parameter name MIT data set Filmtipset data set
a1
0.42
0.38
b1
0.58
0.62
a2
0.71
0.67
b2
0.29
0.33
546
W. Jiang and Y. Xu

(3) Preference prediction experiment based on public data set
In this study, the following 3 methods were used to predict the user’s preference in
the two data sets: Tradition Collaborative Filtering, TCF; Preference Prediction based
on UserTrust, PPUT (i.e., only the use of (1) and (3) in Sect. 2.5, the method of using
link prediction) and Preference Prediction based on User Trust and Link Prediction,
PPUTLP (The method proposed in this paper). Experimental results shown in Figs. 2
and 3, according to a number of experimental results, select t = 25.
As can be seen from Figs. 2 and 3:
(1) For the MIT dataset, the PPUTLP method has the highest accuracy when t = 15;
for the Filmtipset dataset, when t = 25, the PPUTLP method has the highest
accuracy. This is because when the value of T is too small, the user’s data is
deleted too much. When the value of T is too large, it contains too much noise
data. In these two cases, the accuracy of prediction is reduced. In addition, for
different data sets, the proportion of the approximate neighbors is not the same,
we need to select the size of the approximate neighbor data set according to the
speciﬁc circumstances of the experimental data set.
(2) For the two data sets, compared with the TCF and PPUT methods, the PPUTLP
method has a lower MAE value in the case of different neighbors. This is because
the user’s preference is not only related to the similarity of the score, but also by
the surrounding people (such as family, friends, etc.), The TCF method does not
consider the trust relationship between users when determining the user’s
approximate neighbors, but only considers the similarity of the scores. PPUT
method in determining the approximate neighbor users, not only consider the
similarity score effect on user preference, also consider the surrounding personnel
(such as family, friends etc.) impact on user preferences, so the prediction
accuracy of PPUT method is better than that of TCF method. PPUTLP method on
the basis of PPUT method, ﬁrst through the link prediction to select the user may
use the project, to a certain extent, ease the sparsity problem, so PPUTLP method
is better than the PPUT prediction method.
(3) On the MIT dataset, the PPUTLP method has an average improvement of 8.4% on
the MAE compared to the TCF method. Compared with the PPUT method, the
PPUTLP method has an average improvement of 3.12% on the MAE [21]. On the
Filmtipset dataset, the PPUTLP method has an average improvement of 4.03% on
the MAE compared to the TCF method. Compared with the PPUT method, the
PPUTLP method has an average improvement of 1.94% on the MAE. The pre-
diction accuracy of MIT data set is generally larger than increase based on the
Filmtipset data set based on this is because of the unique characteristics of mobile
network, according to the communication behavior of mobile users to obtain more
reliable than the Internet in social relations, so the trust relationship between users
get more credible, the prediction accuracy is better user preference.
Research on Mobile User Dynamic Trust Model
547

4
Conclusion
Based on the characteristics of mobile communication network, this paper proposes a
mobile user preference prediction method based on trust and link prediction. By
analyzing the potential social relations in mobile communication networks, the trust
between mobile users is calculated, and the relevance information between mobile
users and mobile network services is explored by using link forecasting technology,
ﬁnally, the mobile user preference is predicted by the mobile user rating similarity. The
experimental results verify the feasibility of the proposed method in mobile user
preference prediction. The method proposed in this paper can obtain more accurate
mobile user preferences, so it can provide mobile users with accurate personalized
mobile network services.
In the course of this study, we do not consider the privacy and security of mobile
users. In the follow-up study, we will consider how to accurately predict the preference
of mobile users while ensuring the privacy of users.
Acknowledgments. This work was supported by the National Natural Science Foundation of
China (61472136; 61772196), the Hunan Provincial Focus Social Science Fund (2016ZBB006)
and Hunan Provincial Social Science Achievement Review Committee results appraisal identi-
ﬁcation project (Xiang social assessment 2016JD05).
References
1. Ricci, F.: Mobile recommender systems. J. Inf. Technol. Tourism 12(3), 205–231 (2011)
2. Wuhan, H., Xiangwu, M., Licai, W.: Collaborative ﬁltering algorithm based on user
socialization relation mining in mobile communication network. J. Electron. Inf. Technol. 33
(12), 3002–3007 (2011)
3. Jiang, W., Zhang, L., Wang, P.: Research on grid resource scheduling algorithm based on
MAS cooperative bidding game. Sci. China F 52(8), 1302–1320 (2009)
4. Wang, H.M., Yin, G., Xie, B., et al.: Research on network-based large-scale collaborative
development and evolution of trustworthy software. Sci. Sin. Inf. 44, 1–19 (2014)
Fig. 1. Comparison of
the accuracy of trust based
on the MIT data set
Fig. 2. Comparison results
based on MIT datasets
Fig. 3. Comparison results
based on the Filmtipset dataset
548
W. Jiang and Y. Xu

5. Fengling, X., Xiangwu, M., Licai, W.: Collaborative ﬁltering recommendation algorithm
based on mobile user context similarity. J. Electron. Inf. Technol. 33(11), 2785–2789 (2011)
6. Ding, Y., Wang, H., Shi, P., et al.: Trusted cloud service. Chin. J. Comput. 38(1), 133–149
(2015)
7. Jiang, W., Yusheng, X., Guo, H., Zhang, L.: Dynamic trust calculation model and credit
management mechanism of online transaction. Sci. China F Inf. Sci. 44(9), 1084–1101
(2014). https://doi.org/10.1360/N112013-00202
8. Zhang, S.B., Xu, C.X.: Study on the trust evaluation approach based on cloud model. Chin.
J. Comput. 36(2), 422–431 (2013)
9. Jiang, W.J., Zhong, L., Zhang, L.M., Shi, D.J.: Dynamic cooperative multi-agent model of
complex system based-on sequential action’ logic. Chin. J. Comput. 36(5), 115–1124 (2013)
10. Lim, S.L., Finkelstein, A.: StakeRare: using social networks and collaborative ﬁltering for
large-scale requirements elicitation. IEEE Trans. Softw. Eng. 38(3), 707–735 (2012)
11. Xu, J., Si, G.N., Yang, J.F., et al.: An internetware dependable entity model and trust
measurement based on evaluation. Sci. Sin. Inform. 43, 108–125 (2013)
12. Yuxiang, W., Xiuquan, Q., Xiaofeng, L.: Research on the mechanism of mobile social
service selection based on context. J. Comput. Sci. 33(11), 2126–2135 (2010)
13. Weijin, J.: Dynamic Modeling and Quantiﬁcation Trust More Research Methods Agent.
Science Press, Beijin (2014). 6
14. Wang, J., Li, S.-J., Yang, S., Jin, H., Yu, W.: A new transfer learning model for
cross-domain recommendation. Chin. J. Comput. 40(33), 1–15 (2017). Online publication
number
15. Zhang, W.-L., Guo, B., Shen, Y., et al.: Computation ofﬂoading on intelligent mobile
terminal. Chin. J. Comput. 39(5), 1021–1038 (2016)
16. Eagle, N., Pentland, A., Lazer, D.: Inferring friendship network structure by using mobile
phone data. Proc. Natl. Acad. Sci. 106(36), 15274–15278 (2009)
17. Fernando, D., Chavarriaga, J., Pedro, G., et al.: Movie recommendations based in explicit
and implicit features extracted from the Filmtipset dataset. In: Proceedings of the Workshop
on Context-Aware Movie Recommendation, Barcelona, Spain, pp. 45–52 (2010)
18. Xiong, C.Q., Ouyang, Y., Mei, Q.: Argumentation model based on certainty-factor and
algorithms of argument evaluation. J. Softw. 25(6), 1225–1238 (2014)
19. Huang, D.J., Arasan, V.: On measuring email-based social network trust. In: Proceedings of
the Global Telecommunications Conference (GLOBECOM), Miami, FL, pp. 1–5 (2010)
20. Xiuquan, Q., Chun, Y., Xiaofeng, L.: A trustworthiness method based on user context in
social network service. J. Comput. Sci. 34(12), 2403–2413 (2011)
21. Benchettara, N., Kanawati, R., Rouveirol, C.: Supervised machine learning applied to link
prediction in bipartite social networks. In: International Conference on Advances in Social
Networks Analysis and Mining, Odense, pp. 326–330 (2010)
Research on Mobile User Dynamic Trust Model
549

A Group Decision-Making Method Based
on Evidence Theory in Uncertain
Dynamic Environment
Weijin Jiang1,2,3(&) and Yuhui Xu1
1 School of Computer and Information Engineering,
Hunan University of Commerce, Changsha 410205, China
nudtjwj@163.com
2 Electronic Information Engineering Department,
Changsha Normal University, Changsha 410100, China
3 School of Computer Science and Technology,
Wuhan University of Technology, Wuhan, China
Abstract. A debate model based on evidence theory is proposed to solve the
problem of group decision-making under uncertain conditions. First, the
framework of the debate system is constructed. The internal structure of the
argument is composed of preconditions and conclusions. There is not only an
attack and a support relationship between the arguments, but also to support or
oppose such attacks and support relationships. Then we introduce the evidence
theory to describe the uncertainty of the argument, apply the evidence mapping
method to the uncertainty process of the debate process, and realize the
numerical calculation of the reliability of the argument. Finally, a simulation
example is given to demonstrate the effectiveness of the model.
Keywords: Evidence theory  Uncertainty  Argumentation model
Evidence mapping
1
Introduction
In the dynamic environment, the information obtained by the decision-making body is
usually incomplete and inconsistent, so there must be conﬂict and disagreement among
the subjects, and the debate is an effective way to resolve the conﬂict [1, 2] and has
become the ﬁeld of artiﬁcial intelligence research Hot issues. The abstract debate
framework proposed by Dung [3] is a large-scale debate model, after which many
scholars have extended the abstract debate framework, such as the rule-based debate
framework [4], the bipolar debate framework [5], the expansion of bipolar debate
Framework, etc. [6].
The above model is mainly applied to the reasoning of the debate under certainty,
and the actual debate process there is uncertainty. In recent years, scholars at home and
abroad have studied the debate model under uncertainty conditions, such as the
preference-based debate framework proposed by Amgoud and Vesic [7], Tang et al. [8]
proposed a debate based on Dempster-Shafer theory, Xiong et al. [9, 10] based on the
credibility of the debate model and evidence theory based on the debate model.
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 550–560, 2018.
https://doi.org/10.1007/978-3-319-74521-3_57

The above model achieves the uncertainty description of the debate model in different
ways, but it is not comprehensive enough to construct the relationship between argu-
ments in the debate system.
This paper expands the description of the relationship between the arguments by
referring to the model of the literature [11] in the description of the internal structure of
the argument. There is not only a mutual attack and support relationship between the
arguments, but also to support such attacks and support relationships. Against the
theory of evidence is used to analyze the reasoning process of uncertainty under the
condition of uncertainty, and the quantitative description of the reliability of the
argument is realized.
2
Group Discussion Framework
Group discussion is carried out through dialogue among speakers, and the arguments
(views) generated during the dialogue process constitute the content of the whole study.
The research framework is a formal description of the research arguments, including
the description of the internal structure of the argument and the interrelationship
between the arguments. Based on the literature [11], this paper proposes an extended
research model, which includes not only the attack and support relationship between
the arguments, but also the support or opposition to such attacks and support
relationships.
Deﬁnition 1. [11] The statement is a description of things, it can be objective data,
subjective judgments, factual phenomena, theoretical knowledge, etc., constitute the
basic constituent units of the argument, recorded as h.
Deﬁnition 2. [11] Reasoning argument is a tuple of At ¼ H; h
ð
Þ; Among them, H ¼
h1; h2;    ; hn
f
g is a statement, and satisfy: (1) H is consistent, namely 69hi; hj 2
H; hi  :hj; (2) Logically, H infers the h, denoted as H ) h. The H is called the
premise of the reasoning argument, the h is called the reasoning argument, and the set
of all arguments is denoted as At.
Deﬁnition 3. [11] The deﬁnition is provided with two theories of At ¼ H1; h1
ð
Þ,
Bt ¼ H2; h2
ð
Þ: (1) if 9h 2 H2; h1  :h, At attack Bt, denoted by At ! Bt or (At,Bt)−;
(2) if 9h 2 H2; h1  h; At support Bt, denoted as At ⊸Bt or (At, Bt)+. The relationship
between attack and support relationship referred to as an argument-the argument
relation, denoted as rll ¼ At; Bt
ð
Þ; as shown in Fig. 1. All the arguments-the set of
arguments are recorded as ℛll.
Deﬁnition 4. The rule argument is a two tuple Ag ¼ H; rll
ð
Þ; where H is a set of
statements, rll is an argument-argument relationship, and satisﬁes: (1) H is compatible,
(2) Logically H inference out of rll; denoted as H ) rll. The set of all rules argument is
denoted as At.
Deﬁnition 5. A rule Cg ¼ H1; r1
ð
Þ; argument-argument relationship r2 ¼ At; Bt
ð
Þ:
(1) if r1  :r2; Cg against r2; denoted as Cg!r2 or ðCg; r2Þ; (2) if r1r2; Cg support
r2, denoted as Cg(r2 or ðCg; r2Þ þ . The support and opposition of the argument to
A Group Decision-Making Method Based on Evidence Theory
551

argument-argument relation is called argument rule relation, denoted as rlg ¼ ðCg; r2Þ.
The set of all arguments-rule relations is denoted by ℛlg. The four types of
argument-rule relations are shown in Fig. 2.
Figure 2(a) indicates that the Cg is against At attacks on Bt; Fig. 2(b) indicates that
the Cg supports At attacks on Bt; Fig. 2(c) indicates that Cg is opposed to At support for
Bt; Fig. 2(d) indicates that Cg supports At support for Bt.
Deﬁnition 6. The debate-based research framework is a two-tuple AG ¼ ðA; RÞ;
where A ¼ At [ Ag is the argument set and R ¼ Rll [ Rlg is the relation set.
3
Debate Model Based on Evidence Theory
In a dynamic environment, the relationship between the argument and its arguments is
uncertain. The evidence theory is an effective method to express and deal with the
uncertainty reasoning. This paper analyzes the argument and the relationship of the
research model based on the evidence theory.
A
B
A
B
Fig. 1. Diagram of relations between reasoning arguments
A
B
C
A
B
C
A
B
C
A
B
C
a
b
c
d
Fig. 2. Diagram of relations between arguments and rules
552
W. Jiang and Y. Xu

3.1
The Uncertainty Representation of the Argument Premise
Deﬁnition 7. [12] For a verdict, the complete set of all possible answers is H ¼
h1; h2;    ; hn
f
g; and the elements in H are mutually exclusive, then H is the recog-
nition frame. The power set of H is denoted by 2H.
Deﬁnition 8. [13] For any proposition A22H; the deﬁnition of mapping m : 2H! 0; 1
½

satisﬁes the following conditions:
(1) m ;
ð Þ ¼ 0
(2)
P
AH
m A
ð Þ ¼ 1
Then we called m for H on the basic belief assignment (basic belief assignment,
BBA).
Deﬁnition 9. [14] Assuming that the two independent evidence m1 and m2 under
frame H are identiﬁed, m ¼ m1m2 is deﬁned as
m A
ð Þ ¼
P
Ai \ Bj¼A
m1 Ai
ð
Þm2 Bj
ð Þ
1K
;
A 6¼ ;
0;
A ¼ ;
8
<
:
ð1Þ
according to the Dempster combination rule, where K ¼
P
Ai \ Bj¼;
m1 Ai
ð
Þm2 Bj


is the
degree of conﬂict between the evidence m1 and m2.
Deﬁnition 10. The truth of a statement h as a question to be judged, construct a
statement recognition framework, recorded as Hh ¼ True; False
f
g, where True rep-
resents the statement is true, False on behalf of the statement is false.
Deﬁnition 11. For the statement recognition frame Hh ¼ True; False
f
g, the reliability
assignment function mh : 2Hh! 0; 1
½
 deﬁned on 2Hh, where mh True
ð
Þ ¼ ah is the
reliability of the statement that the statement is true according to the current situation,
mh False
ð
Þ ¼ bh indicates that the statement is false, mh Hh
ð
Þ ¼ ch ¼ 1  a  b is the
conﬁdence of the statement unknown reliability, the conﬁdence vector of statement h is
denoted as w h
ð Þ ¼ ah; bh; ch
ð
Þ.
Deﬁnition 12. The authenticity of a certain premise H ¼ h1; h2;    ; hn
f
g as a pending
decision, the premise of the framework to identify the framework, denoted as
HH ¼ True; False
f
g.
The reliability vector of the premise H is denoted as w H
ð Þ ¼ aH; bH; cH
ð
Þ. When
the current H is the combination of multiple statements, that is H ¼ h1^    ^hn; then
w H
ð Þ ¼ w hi
ð Þ; where i satisﬁes: ahi ¼
min
j2 1;;n
f
g ahj


; that is, the premise of the relia-
bility of the value of the vector from the lowest credibility of the statement. When the
current H is the recursion of multiple statements, that is H ¼ h1_    _hn; then w H
ð Þ ¼
w hi
ð Þ; where i satisﬁes: ahi ¼
max
j2 1;;n
f
g ahj


; that is the prerequisite reliability vector is
determined by the highest degree of conﬁdence.
A Group Decision-Making Method Based on Evidence Theory
553

3.2
The Reasoning of Uncertainty in Argument Conclusion
3.2.1
Reasoning Rule Representation Based on Evidence Mapping
Deﬁnition 13. [10, 15] Evidence mapping refers to the mapping of the prerequisite
framework HH to the conclusion recognition framework Hh, which describes the
reasoning relationship between the elements in HH and the elements in Hh. Evidence
mapping from HH to Hh is expressed as C : HH!22Hh½0;1. The evidence mapping for
each element in HH is expressed as
C hHi
ð
Þ ¼
hhi1; f hHi!hhi1
ð
Þ
ð
Þ;    ;
hhim; f hHi!hhim
ð
Þ
ð
Þ


ð2Þ
where hHi is the prerequisite element, hhij is the element of the conclusion, and
f hHi!hhij


is the rule strength of hHi supporting hhij.
Make HHi ¼ Sm
j¼1 hhij


a prerequisite for hHi to infer all the conclusion of the
collection.
Equation (2) satisﬁes the following condition:
(1) hhij 6¼ ; j ¼ 1; 2;    ; m
(2) 0\f hHi!hhij


	 1
(3) P
m
j¼1
f hHi!hhij


¼ 1
3.2.2
The Calculation of the Conclusion of Reasoning
Under the condition of uncertainty, for the reasoning argument At ¼ H; h
ð
Þ, the rule
relations between the prerequisite H and the conclusion h include the following:
H!h; H! :h; :H!h; :H! :h.
The
corresponding
regular
strength
is:
Hh ¼ f H!h
ð
Þ; H:h ¼ f H! :h
ð
Þ; :Hh ¼ f :H!h
ð
Þ; :H:h ¼ f :H! :h
ð
Þ:
Deﬁnition 14. [10] For the reasoning At ¼ H; h
ð
Þ, we deﬁne the mapping matrix
R At
ð
Þ ¼ rij


33 from the premise H to the conclusion h, and the row headings are H,
:H and the complete set HH; the column headings are h, :h and the complete set Hh.
The speciﬁc form of R At
ð
Þ is
R At
ð
Þ ¼
Hh
H:h
1  r11  r12
:Hh
:H:h
1  r21  r22
r31
r32
1  r31  r32
2
4
3
5
among which,
r31 ¼
r11 þ r21
ð
Þ=2;
r11  r21 6¼ 0
0;
r11  r21¼ 0

ð3Þ
554
W. Jiang and Y. Xu

r32 ¼
r12 þ r22
ð
Þ=2;
r12  r22 6¼ 0
0;
r12  r22 ¼ 0

ð4Þ
According to the reliability value vector w H
ð Þ and the mapping matrix R At
ð
Þ of the
premise H, the reliability value vector
w h
ð Þ ¼ w H
ð Þ  R At
ð
Þ
ð5Þ
of the conclusion h can be obtained.
Assuming that there are multiple prerequisites pointing to the same conclusion h,
we can fuse the multiple conﬁdence values of h according to the Dempster combination
rule to get w h
ð Þ ¼ w h
ð Þ1w h
ð Þ2    w h
ð Þk.
3.2.3
The Calculation of the Conclusions of the Rules
For the rule argument Ag ¼ H; rll
ð
Þ; where rll¼ At; Bt
ð
Þ; At ¼ H1; h1
ð
Þ; Bt ¼ H2; h2
ð
Þ;
we can see the argument-argument relation rll, whose essence corresponds to the
mapping matrix R At
ð
Þ of At. Therefore, the relationship between the premise H and the
conclusion rll is the relationship between H and R At
ð
Þ, including the following:
H ! f H1 ! h1
ð
Þ. The corresponding regular intensity is recorded as H H1h1
ð
Þ,
H H1:h1
ð
Þ,
:H H1h1
ð
Þ,
:H H1:h1
ð
Þ,
H :H1h1
ð
Þ,
H :H1:h1
ð
Þ,
:H :H1h1
ð
Þ,
:H :H1:h1
ð
Þ.
Because the relationship between the premise H and the conclusion rll is more
complex, it is divided into two categories (the ﬁrst four categories, the second category
for the last four) were calculated and then synthesized.
Deﬁnition 15. For the rule argument Ag ¼ H; rll
ð
Þ, the mapping matrix deﬁned by the
prerequisites H to R At
ð
Þ, the ﬁrst row R At
ð
Þ 1
ð Þ, is R1 ¼ rij


33, and the row headings
are H, :H and the complete set HH; the column heading is the rule strength deduced by
H1, followed by H1h1, H1:h1 and complete works HH1. The speciﬁc form of R1 is
R1 ¼
H H1h1
ð
Þ
H H1:h1
ð
Þ
1  r11  r12
:H H1h1
ð
Þ
:H H1:h1
ð
Þ
1  r21  r22
r31
r32
1  r31  r32
2
4
3
5
where r31 and r32 are calculated by the same formula (3) and (4).
According to the reliability value vector w H
ð Þ and the mapping matrix R1 of the
premise H, the ﬁrst behavior
R At
ð
Þ 1
ð Þ
H ¼ w H
ð Þ  R1
ð6Þ
of the mapping matrix R At
ð
Þ can be obtained.
A Group Decision-Making Method Based on Evidence Theory
555

Deﬁnition 16. Similarly, the mapping matrix deﬁned by the second row R At
ð
Þ 2
ð Þ of the
prerequisites H to R At
ð
Þ is R2 ¼ rij


33. The row headings are H, :H and the com-
plete set of HH; the column heading is the intensity of the rules deduced by :H1,
followed by :H1h1, :H1:h1 and the complete set H:H1. The speciﬁc form of R2 is
R2 ¼
H :H1h1
ð
Þ
H :H1:h1
ð
Þ
1  r11  r12
:H :H1h1
ð
Þ
:H :H1:h1
ð
Þ
1  r21  r22
r31
r32
1  r31  r32
2
4
3
5
where r31 and r32 are calculated as follows (3) and (4). The second row of the mapping
matrix R At
ð
Þ can be obtained
R At
ð
Þ 2
ð Þ
H ¼ w H
ð Þ  R2
ð7Þ
Thus, under the inﬂuence of H, the original mapping matrix R At
ð
Þ is updated to:
(1) Update the ﬁrst row of R At
ð
Þ according to the Dempster combination rule:
R At
ð
Þ 1
ð Þ
new¼ R At
ð
Þ 1
ð Þ
H R At
ð
Þð1Þ;
(2) Similarly, the second line of R At
ð
Þ is updated as R At
ð
Þ 2
ð Þ
new¼ R At
ð
Þ 2
ð Þ
H R At
ð
Þð2Þ;
(3) The third line of R At
ð
Þ is calculated by the same formula (3) and (4).
3.3
Declaring the Update of the Reliability
In the course of the study, when there is a new reasoning argument A2, if there is an
argument-argument relation A2; A1
ð
Þ, then A2 will change the conﬁdence value of a
statement h1i in the premise H1 of A1, thus changing the reliability of the conclusion h1
of A1. As the study continues, the relation A3; A2
ð
Þ is generated, then A3 changes the
conﬁdence value of a statement h2j in the premise H2 of A2, thus changing the con-
ﬁdence value of the conclusion h2 of A2 (that is, h1i), and the ﬁnal conclusion of the
reliability of A1 change h1. In addition, when a new rule argument A4 is generated, if
there is an argument-rule relation A4; A2; A1
ð
Þ
ð
Þ, then A4 changes the mapping matrix
R21 of A2 to A1, thus changing the conﬁdence value of A2’s conclusion h2 (that is, h1i),
and eventually changing A1’s conclusion h1’s reliability value. Thus, for each new
argument, the correspondence value of the corresponding statement on the entire
argument chain will change.
(1) When adding a reasoning argument, the algorithm described in the framework of
the reliability update is as follows:
Step1: Generate a reasoning argument At ¼ HA; hA
ð
Þ, set the initial conﬁdence
value
of
each
statement
hAi
in
the
preamble
HA ¼ hA1; hA2;    ; hAn
f
g
to
w hAi
ð
Þ0¼ ahAi; bhAi; chAi

0, calculate the reliability value w HA
ð
Þ0 of HA, and generate
the inference rule mapping matrix RA from HA to hA.
Step2: Calculate the conﬁdence value w hA
ð
Þ ¼ w HA
ð
Þ  RA of hA, and if hA is the
target statement for the whole debate, the stated reliability value is updated. If hA
556
W. Jiang and Y. Xu

belongs to the premise of an argument Bt ¼ HB; hB
ð
Þ, that is hA ¼ hBj2HB, then hA
updates the reliability value of hBj to w hBj


new¼ w hBj

0w hA
ð
Þ, the reliability value
w HB
ð
Þnew of HB is updated, and the reliability value w hB
ð
Þ ¼ w HB
ð
ÞnewRB of hB is
calculated.
If hB is still a prerequisite for an argument, follow the steps 2 above to proceed with
the update until the entire statement of the debate is updated.
(2) When adding a rule to the argument, the discussion within the framework of the
reliability value update algorithm is as follows:
Step1: Produce a rule argument for Cg ¼ HC; rC
ð
Þ, where the premise is
HC ¼ hC1; hC2;    ; hCn
f
g, the relationship is rC ¼ At; Bt
ð
Þ. The initial reliability value
of each statement hCi is set to w hCi
ð
Þ0¼ ahCi; bhCi; chCi

0, the reliability value w HC
ð
Þ of
HC is calculated, the initial mapping matrix of At is R At
ð
Þ0, and the updated
post-mapping matrix is R At
ð
Þnew.
Step2: Calculate the update reliability value w hA
ð
Þnew¼ w HA
ð
Þ  R At
ð
Þnew of hA, and
then hA updates the reliability value of hBj to w hBj


new¼ w hBj

0w hA
ð
Þnew, and then
update the reliability value of hB.
3.4
The Acceptability of the Statement
If the conﬁdence vector of statement h is w h
ð Þ ¼ ah; bh; ch
ð
Þ, it is possible to determine
whether it is acceptable according the following three rules (Table 1):
Rule1: If ah [ a0, statement h is acceptable.
Rule2: If bh\b0, statement h is acceptable.
Table 1. Arguments of the discussion
Argument Argument
premise
Argument
conclusion
Prerequisites initial
reliability values
Reasoning rule
strength
A1
h11 ^ h12
h
w(h11) = (0.7,0.2,0.1)
w(h12) = (0.85,0.1,0.05)
H1 ! h(0.9)
¬H1 ! ¬h(0.8)
A2
h21 ^ h22 ^ h23
h11
w(h21) = (0.8,0,0.2)
w(h22) = (0.9,0.1,0)
w(h23) = (0.7,0.1,0.2)
H2 ! h11(0.9)
A3
h31 ^ h32
h23
w(h31) = (0.8,0.1,0.1)
w(h32) = (0.7,0,0.3)
H3 ! ¬h23(0.8)
A4
h41 ^ (h42 _ h43) h12
w(h41) = (0.6,0.1,0.3)
w(h42) = (0.8,0.2,0)
w(h43) = (0.7,0.1,0.2)
H4 ! ¬h12(0.8)
A5
h51 ^ h52
r(A4,A1)
w(h51) = (0.9,0,0.1)
w(h52) = (0.8,0.2,0)
H5 ! (H4 ! h12)
(0.5)
A Group Decision-Making Method Based on Evidence Theory
557

Time node 1 produces argument A1, the reliability value w(H1)1 = w(h11) of the
premise H1, the rule mapping matrix R11 ¼
0:9
0
0:1
0
0:8
0:2
0
0
1
2
4
3
5, the conﬁdence value
w h
ð Þ1¼ w H1
ð
Þ1R11 = (0.63,0.16,0.21) of the conclusion h.
Time node 2 produces argument A2, the reliability value of w(H2)2 = w(h23) of the
premise H2, and the conﬁdence value w h11
ð
Þ2¼ w H2
ð
Þ2R21 of the conclusion h11. The
reliability value of h11 is updated to w h11
ð
Þ2
new¼w h11
ð
Þ2w h11
ð
Þ = (0.8730,0.0847,
0.0423). In this case, the reliability value of H1 is w(H1)2 = w(h12), the reliability value
of h is updated to w h
ð Þ2¼ w H1
ð
Þ2R11 = (0.7650,0.0800,0.1550). It can be seen that
the reliability of A1 conclusion h is improved by support of A2 for A1.
Time node 3 produces argument A3, A3 reduces the credibility of A2 premise, and
thus reduces the credibility of h11, the ﬁnal h of the reliability value is updated to
w h
ð Þ3 = (0.7267,0.1027,0.1706).
The time node 4 produces the argument A4, w h12
ð
Þ4
new¼ w h12
ð
Þ4w h12
ð
Þ =
(0.7466,0.2095,0.0439), the conﬁdence degree of h12, is reduced by A4, and the reli-
ability value of h is updated to w h
ð Þ4 = (0.6720,0.1676,0.1605).
The time node 5 produces the argument A5, A5 against the attack of A4 to A1, and
the ﬁrst row of the mapping matrix R41 of A4 is updated to Rð1Þ
41new ¼ Rð1Þ
41HRð1Þ
41 =
(0.1176 0.7059 0.1765). Thus the reliability value of h is updated to w h
ð Þ5 =
(0.7018,0.1442,0.1540).
Assuming that the acceptability of the statement is discriminated by rule 1, if the
threshold is a0 ¼ 0:7, then h is acceptable at the end of the study.
In the whole study, the update process of the presentation reliability value is shown
in Table 2.
Table 2. The renewal process of the reliability values of statements
Statement Time 1
Time 2
Time 3
Time 4
Time 5
h
(0.63,0.16,
0.21)
(0.7650,0.0800,
0.1550)
(0.7267,0.1027,
0.1706)
(0.6720,0.1676,
0.1605)
(0.7018,0.1442,
0.1540)
h11
(0.70,0.20,
0.10)
(0.8730,0.0847,
0.0423)
(0.8074,0.1284,
0.0642)
(0.8074,0.1284,
0.0642)
(0.8074,0.1284,
0.0642)
h12
(0.85,0.10,
0.05)
(0.85,0.10,0.05) (0.85,0.10,0.05) (0.7466,0.2095,
0.0439)
(0.7797,0.1803,
0.0400)
h21
–
(0.80,0,0.20)
(0.80,0,0.20)
(0.80,0,0.20)
(0.80,0,0.20)
h22
–
(0.90,0.10,0)
(0.90,0.10,0)
(0.90,0.10,0)
(0.90,0.10,0)
h23
–
(0.7,0.1,0.2)
(0.4565,0.4130,
0.1304)
(0.4565,0.4130,
0.1304)
(0.4565,0.4130,
0.1304)
h31
–
–
(0.8,0.1,0.1)
(0.8,0.1,0.1)
(0.8,0.1,0.1)
h32
–
–
(0.7,0,0.3)
(0.7,0,0.3)
(0.7,0,0.3)
(continued)
558
W. Jiang and Y. Xu

4
Conclusion
In this paper, the debate model based on evidence theory is constructed for the problem
of group decision-making under uncertain conditions. First, a more comprehensive
framework of the debate system is deﬁned, which deﬁnes the relationship between the
internal structure of the argument and the argument, which not only includes attacks
and support relationships, but also allows for support or opposition to such attacks and
support relationships. Then we introduce the theory of evidence to describe the
uncertainty of the argument, apply the evidence mapping method to the uncertainty
process of the debate process, and realize the transmission and renewal of the reliability
of the argument.
Acknowledgments. This work was supported by the National Natural Science Foundation of
China (61472136; 61772196), the Hunan Provincial Focus Social Science Fund (2016ZBB006)
and Hunan Provincial Social Science Achievement Review Committee results appraisal identi-
ﬁcation project (Xiang social assessment 2016JD05).
References
1. Amgoud, L., Prade, H.: Using arguments for making and explaining decisions. Artif. Intell.
173(3–4), 413–436 (2009)
2. Chen, Y., Xiang, D., Zhao, Y.: Argumentation-based group discussion modeling and
decision approach. Syst. Eng. Theory Pract. 33(7), 1633–1639 (2013)
3. Dung, P.M.: On the acceptability of arguments and its fundamental role in nonmonotonic
reasoning, logic programming and n-person games. Artif. Intell. 77(2), 321–357 (1995)
4. Caminada, M., Amgoud, L.: On the evaluation of argumentation formalisms. Artif. Intell.
171(5-6), 286–310 (2007)
5. Cayrol, C., Lagasquie-Schiex, M.C.: Bipolarity in argumentation graphs: towards a better
understanding. Int. J. Approximate Reasoning 54(7), 876–899 (2013)
6. Chen, J.L., Wang, C.C., Chen, C.: Extended bipolar argumentation model. J. Softw. 23(6),
1444–1457 (2012)
7. Amgoud, L., Vesic, S.: A new approach for preference-based argumentation frameworks.
Ann. Math. Artif. Intell. 63(2), 149–183 (2011)
8. Tang, Y., Hang, C.W., Parsons, S., et al.: Towards argumentation with symbolic
dempster-shafer evidence. In: Proceedings of the Computational Models of Argument
(COMMA 2012), Vienna, Austria, pp. 462–469. IOS Press (2012)
Table 2. (continued)
Statement Time 1
Time 2
Time 3
Time 4
Time 5
h41
–
–
–
(0.6,0.1,0.3)
(0.6,0.1,0.3)
h42
–
–
–
(0.8,0.2,0)
(0.8,0.2,0)
h43
–
–
–
(0.7,0.1,0.2)
(0.7,0.1,0.2)
h51
–
–
–
–
(0.9,0,0.1)
h52
–
–
–
–
(0.8,0.2,0)
A Group Decision-Making Method Based on Evidence Theory
559

9. Xiong, C.Q., Ouyang, Y., Mei, Q.: Argumentation model based on certainty-factor and
algorithms of argument evaluation. J. Softw. 25(6), 1225–1238 (2014)
10. Xiong, C.Q., Zhan, Y.F., Chen, S.B.: An argumentation model based on evidence theory. In:
The 10th International Conference on Computer Science and Education (ICCSE 2015),
Cambridge University, UK, pp. 451–454. IEEE Press (2015)
11. Xiong, C.Q., Li, D.H.: Model of argumentation. J. Softw. 20(8), 2181–2190 (2009)
12. Zhang, X., Mu, L.H.: Evidence combination rule based on local conﬂict elimination. Syst.
Eng. Electron. 38(7), 1594–1599 (2016)
13. Han, D.Q., Yang, Y., Han, C.Z.: Advances in DS evidence theory and related discussions.
Control Decis. 29(1), 1–11 (2014)
14. Shafer, G.: A Mathematical Theory of Evidence. Princeton University Press, Princeton
(1976)
15. Liu, W., Hughes, J.G., McTear, M.F.: Representing heuristic knowledge in DS theory. In:
Proceedings of the Eighth International Conference on Uncertainty in Artiﬁcial Intelligence,
pp. 182–190. Morgan Kaufmann Publishers Inc. (1992)
560
W. Jiang and Y. Xu

Analysis and Estimate the Eﬀect of Knowledge on Software
Reliability Distribution
Chunhui Yang1,2(✉), Yan Gao2,3(✉), Xuedong Kong2, Dingfang Chen1,
Shengwu Xiong1, and Jianwen Xiang1
1 Wuhan University of Technology, Wuhan, China
yangch@ceprei.com, cadcs@126.com, {xiongsw,jwxiang}@whut.edu.cn
2 Software Center, CEPREI, Guangzhou, China
kongxd@ceprei.com
3 Key Laboratory for Performance and Reliability Testing of Foundational Software
and Hardware, MIIT, Guangzhou, China
thegoldfishwang@163.com
Abstract. Software knowledge plays an important role in software testing and
software reliability model. This paper proposes that software knowledge aﬀects
the software reliability distribution signiﬁcantly based on the theoretical analysis
on the Weibull distribution of defect density, and proof that the software knowl‐
edge amount mainly aﬀects from the scale parameter c of Weibull distribution,
while c can be expressed as a quantitative expression of software knowledge
amount. In this paper, engineering experiment is carried out to verify the proposed
conclusion, which shows that more knowledge testers have, the smaller the scale
factor c of Weibull distribution becomes. Furthermore, according to the degree
of the software knowledge, the trend of the problems found in testing can be
predicted, so as to evaluate the reliability of the software.
Keywords: Software knowledge · Software test · Reliability model
Weibull distribution
1
Introduction
Software reliability is the possibility of software runs successfully according to the
design requirement under a given time interval and required environment condition
[1–3]. While software product is released, the reliability and latent fault of software
product can be predicted and estimated through the software reliability theory, which
includes two reasons: (1) It can be used as the objective statement of product quality.
(2) It can be used for the resource planning in software maintenance phase.
Software reliability model plays an important role in software reliability theory,
which includes two types as static model and dynamic model. In static model [4, 5], the
coeﬃcient is estimated from previous data of software products, while relative software
product can be the supplement observation for the total project. Static model did not
consider the time factor, but actually is a quality model. Dynamic model [6–8] is mainly
used in the software development phase and software reliability testing phase The
Rayleigh model is a typical model used in the development phase, while exponential
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 561–570, 2018.
https://doi.org/10.1007/978-3-319-74521-3_58

growth model and other reliability growth model is used in the test phase. All dynamic
reliability models can be denoted as a function of time or its logic in the development
phase.
Knowledge can provide guidance for software testing and many studies have been
carried out. Xu [9] indicated that the cause of software errors is the errors of knowledge
used and its usage in the software system, and proposed software testing method based
on the knowledge by practice analysis, which can make up the inadequacy of testing
mothed adequacy and suitability eﬀectively. Vijaykumar [10] proposed the concept of
Reference Ontology of Software Testing by identifying and reuse of the ontology in the
software testing process, which can gives semantics to a large number of software testing
information, so as to support the knowledge management in the software testing process.
However, the research of the eﬀect of Software knowledge on software reliability is
very few. How to describe the relationship between knowledge and software reliability,
so as to achieve high quality of software testing with knowledge, and improve software
quality and reliability, scientiﬁc and quantitative analysis is necessary. Therefore, this
paper estimates and is focus on the relationship between the software knowledge and
software reliability.
The remainder of this paper is structured as follows. In Sect. 2, Weibull distribution
is brieﬂy summarized. The relationship between the software knowledge and software
reliability are derived in detail, and a formalization expression of the relationship are
described brieﬂy in Sect. 3. Engineering Experiments results and analysis are reported
in Sect. 4 and Sect. 5 concludes this paper.
2
Software Rellability Distribution
Weibull distribution [11, 12] is used for reliability analysis in various engineering ﬁelds
for many years, which from bearing fatigue life of the deep groove ball to the tube fault
and river ﬂood. Weibull distribution is one of the three famous extreme value distribu‐
tions. The signiﬁcant feature of Weibull distribution is that the probability density grad‐
ually tends to zero. Figure 1 shows the Weibull probability density curves with diﬀerent
shape parameter m values.
The Cumulative Distribution Function(CDF) and Probability Density Func‐
tion(PDF) of Weibull distribution can be formulated as:
CDF:
F(t) = 1 −e−(t∕c)m
(1)
PDF:
f(t) = m
t
( t
c
)m
e−(t∕c)m
(2)
where m denotes the shape parameter, c denotes scale parameter, t denotes time.
Rayleigh model is special case of the Weibull distributions which shape parameter
m is equal to 2. The CDF and PDF are expressed as:
CDF:
F(t) = 1 −e−(t∕c)2
(3)
562
C. Yang et al.

PDF:
f(t) = 2
t
( t
c
)2
e−(t∕c)2
(4)
Exponential model is another special case of the Weibull distributions which shape
parameter m is equal to 1. Exponential model is suitable for statistical process that
monotonous decline to progressive values. The CDF and PDF are expressed as:
CDF:
F(t) = 1 −e−(t∕c) = 1 −e−𝜆t
(5)
PDF:
f(t) = 1
ce−(t∕c) = 𝜆e−𝜆t
(6)
In engineering practice, the above formulations need to be multiplied by the total
defect number or total defect cumulative probability K, where K is an estimated param‐
eter for deriving speciﬁc model from the dataset.
3
Analysis of Knowledge and Rellability
3.1
Relationship Derivation
Observed from the engineering practice, the experience and knowledge of testing team
can aﬀect the testing eﬃciency and problems found. Generally, high level testing team
can ﬁnd problem more quickly and the problems found are subject to certain mathe‐
matical distribution.
Two Weibull functions are used in the application of software reliability usually that,
Rayleigh distribution is used to describe the defect distribution in development phase,
exponential distribution is used to describe the fault distribution in system test phase or
after product delivery. Due to the signiﬁcant correlation between the knowledge of
testing team and testing eﬃciency from engineering practice, this paper analysis the
quantitative relationship of knowledge and fault distribution by Weibull function.
Fig. 1. Several typical curves of Weibull distribution
Analysis and Estimate the Eﬀect of Knowledge on Software Reliability Distribution
563

This paper takes the system testing phase for example, where the shape parameter
m is equal to 1. There are three hypotheses as follow.
Hypothesis 1: The knowledge quantity of testing team A and B are SA and SB corre‐
spondingly.
Hypothesis 2: Testing team A and B test the same software product respectively, while
they can ﬁnd out all defects through learning and experience accumu‐
lation.
Hypothesis 3: The fault distribution of software product is subject to Weibull distri‐
bution in software reliability ﬁeld.
Assuming the fault set X = {x1, x2, x3, …} is the sum of all kind fault modes found
at t moment.
X = K ⋅f(t) = K
c e−(t∕c)
(7)
where K denotes the total defect number, c denotes scale parameter.
As observed from the engineering practice, the experience and knowledge of testing
team can aﬀect the test eﬃciency and the found problems. Therefore, the knowledge
quantity of testing team is positively related to the amount of defects can be found.
K = K(S) ∝S
(8)
By plugging Formula (8) to Formula (7), the following formula is obtained as:
X = K(S)
c
e−(t∕c)
(9)
which means that the defect number X found at moment t is related to knowledge quantity
S and scale factor c. X can be expressed by S and c as:
X = X(S, c, t)
(10)
Assuming the fault probability distribution of testing team A and B are f1
(t, c1
) and
f2
(t, c2
) correspondingly:
X1 = K(SA) ⋅f1(t) = K(SA)
c1
e−(t∕c1)
(11)
X2 = K(SB) ⋅f2(t) = K(SB)
c2
e−(t∕c2)
(12)
Subtract Formula (11) and (12) after logarithmic, the following formula is obtained
as:
564
C. Yang et al.

ln X1 −ln X2 = ln K(SA) −ln K(SB) −ln c1 + ln c2 −t ∗c1 −c2
c1c2
(13)
Formula (13) can be rewritten as:
Δ ln X = Δ ln K −Δ ln c −t ∗c1 −c2
c1c2
(14)
High-level testing team can ﬁnd problem faster and more, which means that if SA >
SB, c1 < c2. For any moment t as a constant, team A ﬁnds more faults and more eﬃcient
than team B. Team A reaches the testing goal faster than team B at the end of the test
timing.
Because Δ ln K is a constant, the relationship of Δ ln X and |Δ ln c| can be shown in
Fig. 2.
Fig. 2. Relationship between defect number X and scale parameter c
From Fig. 2, Δ ln X is proportional to |Δ ln c|. Due to c1 < c2, so that X is inversely
proportional to c. Morever, Formula (13) indicates that X is proportional to the knowl‐
edge quantity S testing team have, therefore, the knowledge quantity S testing team have
is inversely proportional to c, and the knowledge quantity S required is proportional to c.
Therefore, software knowledge mainly aﬀects the scale parameter c in Weibull
distribution. The smaller c is, the more knowledge testers have, the less knowledge they
need to obtain; the larger c is, the less knowledge testers have, the more knowledge they
need to obtain. c can be expressed as a quantitative expression of knowledge quantity
needed.
c = 𝜆g(D, AD, R, AR, H, I)
(15)
where D denotes the concept set of software knowledge, AD denotes the attribute set of
software knowledge, R denotes the relationship and rules of knowledge, AR denotes
attribute set of knowledge relationship, H denotes he level of knowledge concept, I
denotes instance set, 𝜆 denotes coeﬃcient. Function g() denotes the measurement of the
knowledge quantity needed in software development or testing, which is proportional
to the scale parameter c.
Analysis and Estimate the Eﬀect of Knowledge on Software Reliability Distribution
565

3.2
The Eﬀect of Knowledge Analysis
This subsection analyses the eﬀect of knowledge on size parameter c from development
phrase and testing phrase respectively.
For the software development phase, assuming that the software development and
testing activities are ongoing, the eﬀect of knowledge on the software development
phase is shown in Fig. 3.
Fig. 3. The eﬀect of knowledge on development phase
The knowledge quantity required by software testers has direct eﬀect on the test. If
the tester’s understanding of the software system is more comprehensive and profound,
the less knowledge quantity required, the smaller scale parameter c is, which makes
software fault detection faster and software reliability can be improved eﬀectively. On
the contrary, if the knowledge and experience of tester is weak, the more knowledge
quantity required and the larger c is, which makes fault detection slower.
Fig. 4. The eﬀect of knowledge on the testing phase
566
C. Yang et al.

Similarly, the eﬀect of knowledge on the software testing phase is shown in Fig. 4.
The less of knowledge quantity tester requires, the smaller scale parameter c is, which
makes software fault found timely and eﬀectively. On the contrary, if the knowledge
and experience of tester is weak, the more knowledge quantity required, the larger scale
parameter c is, which makes fault detection slower.
4
Experiments
In this section, software engineering testing of a certain Linux operation system is carried
out, in order to verify the proposed conclusion of the relationship between knowledge
and software reliability.
There are total 10 round testing designed, the test strategies are shown as follow:
(1) Each round is independent.
(2) For each round, the found problems and fault modes are concluded as knowledge.
(3) For each round, test cases are added based on the increased problems and fault
modes of the previous round.
(4) For each round, test case distribution is improved based on the fault mode distri‐
bution of all previous rounds.
The test results of 10 rounds are shown in Table 1, and the analysis is shown in
Fig. 5. Firstly, tester can ﬁnd more faults by learning the software product more as the
testing going on. The knowledge of tester can be enriched after analyzing these defects
and faults. Secondly, the number of test problems and fault modes are increased each
round, while the increasing speed decreases and fault mode tends to 35 ﬁnally, which
means that the knowledge of tester has been improved.
Table 1. The test results of 10 rounds
Round
Result
Fault mode num
Test case num
Test problem
num
Fault mode added
1
20
240
92
/
2
23
328
97
3
3
26
368
108
3
4
28
392
114
2
5
29
421
123
1
6
32
450
140
3
7
34
489
145
2
8
35
520
151
1
9
35
531
156
0
10
35
543
170
0
Analysis and Estimate the Eﬀect of Knowledge on Software Reliability Distribution
567

Fig. 5. Analysis of the test results
According to Formula (7), K and c of Weibull cumulative defect distribution are
estimated with the fault mode number (20, 23, 26, 28, 29) of the ﬁrst ﬁve test rounds.
X = K ⋅f(t) = K
c e−(t∕c)
(16)
The simulation result by Matlab tool is shown in Fig. 6, which K = 27.943, c = 0.911.
Fig. 6. The ﬁtting distribution curve of the ﬁrst 5 rounds
Similarly, according to Formula (7), K and c of Weibull cumulative defect distribu‐
tion are estimated with the fault mode number (32, 34, 35, 35, 35) of the last ﬁve test
rounds.
The simulation result by Matlab tool is shown in Fig. 7, which K = 34.841, c = 0.404.
568
C. Yang et al.

Fig. 7. The ﬁtting distribution curve of the last 5 rounds
By plugging the above two c into the probability density function of Formula (7),
two probability density curves are obtained as shown in Fig. 8.
Fig. 8. The probability density curves of diﬀerent c
With the knowledge increasing of tester, more defects are found, and smaller the
scale parameter c is, which means that the knowledge mainly aﬀect the scale parameter
c of software reliability distribution.
5
Conclusion
On the basis of the theoretical analysis and derivation on the Weibull distribution of
defect density, this paper gives the conclusion that the knowledge qualiﬁcation varies
inversely to scale parameter c of Weibull distribution via the analyses of the quantitative
relation between the tester knowledge level and defect distribution. Accordingly, the
proposed relationship can help to analysis the eﬀect of software test team on the product
reliability curve, so as to predict the trend of ﬁnding problems along with software
reliability, and determine the end time of the test process.
Analysis and Estimate the Eﬀect of Knowledge on Software Reliability Distribution
569

References
1. Lyu, M.R.: Handbook of Software Reliability Engineering. McGraw Hill and IEEE Computer
Society Press, New York (1996)
2. Bansal, A., Pundir, S.A.: Review on approaches and models proposed for software reliability
testing. Int. J. Comput. Commun. Technol. 4(2), 7–9 (2013)
3. Xavier, J., Macêdo, A., Matias, R., et al.: A survey on research in software reliability
engineering in the last decade. In: Proceedings of the 29th Annual ACM Symposium on
Applied Computing, pp. 1190–1191. ACM (2014)
4. Duran, J.W., Wiorkowski, J.J.: Capture-recapture sampling for estimating software error
content. IEEE Trans. Softw. Eng. 1, 147–148 (1981)
5. Nathan, I.: A deteministric model to predict “error-free” status of complex software
development. In: Workshop on Quantitative Software Models for Software Reliability,
Complexity and Cost: An Assessment of the State of the Art
6. Musa, J.: Operational proﬁles in software-reliability engineering. IEEE Softw. 10(2), 14–32
(1993)
7. Littlewood, B., Verrall, J.L.: Likelihood function of a debugging model for computer software
reliability. IEEE Trans. Reliab. 30(2), 145–148 (1981)
8. Goel, A.L., Okumotu, K.: Time-dependent error detection rate model for software reliability
and other performance measures. IEEE Trans. Reliab. 28(3), 206–211 (1979)
9. Xu, R.: The testing method based on software knowledge. J. Wuhan Univ. (Nat. Sci. Edn.)
46(1), 61–62 (2000)
10. de Santiago Jr., V.A., Vijaykumar, N.L.: Generating model-based test cases from natural
language requirements for space application software. Softw. Qual. J. 20(1), 77–143 (2012)
11. Kan, S.H.: Metrics and models in software quality engineering (2003)
12. Covert, R.P., Philip, G.C.: An EOQ model for items with Weibull distribution deterioration.
AIIE Trans. 5(4), 323–326 (1973)
570
C. Yang et al.

Development of Virtual Reality-Based
Rock Climbing System
Yiming Su, Dingfang Chen
(✉), Congxing Zheng,
Sihan Wang, Liwen Chang, and Jie Mei
Institute of Intelligent Manufacturing and Control,
Wuhan University of Technology, Wuhan, China
953477859@qq.com
Abstract. In this paper, development of virtual reality-based rock climbing
system is demonstrated. As virtual reality possesses immersive, synchronous and
interactive characteristics, the VR technology has been widely used in many
engineering areas. Through the combination of multi-path variable amplitude
rock climbing machine and the virtual reality technology, the users can do the
rock climbing sports in room environment with immersive experience. The virtual
rock climbing scene is developed based on the Unity3D engine to achieve the
interaction among the user, the rock climbing machine and the virtual reality
environment. The system utilizes virtual reality technologies such as HTC Vive,
Leap Motion and Unity3D game engine in an attempt to simulate an immersive
rock climbing experience. The rock climbing machine is designed that capable
of changing the path or amplitude automatically, which overcomes the defects of
indoor climbing machine with ﬁxed route. With the VR technology, it greatly
improves the indoor climbing experience.
Keywords: Virtual reality · Unity3D · Leap Motion · Rock climbing
1
Introduction
Virtual reality (VR) typically refers to computer technologies that use VR headsets to
generate the realistic images, sounds and other sensations, through which users can interact
with virtual objects or stimuli that are modeled from the real world. VR environments have
been used extensively in a variety of fields, such as cinema and entertainment, healthcare
and clinical therapies, engineering, education and training and so on which offers users
numerous advantages and benefits such as immersive, interactive and cost-efficient expe‐
riences [1]. The study of VR technology possesses an interdisciplinary characteristic.
Through the advancement of interface technologies, VR will eventually become widely
popular, changing our lifestyle and making our work easier [2–4].
There are many commercial indoor rock climbing machines available in the market
[5–7]. The Treadwall series indoor climbing machine, released at 2012, is one of the
most widely used climbing machine, which is the ﬁrst generation of the track format
climbing machine. However, its climbing path is single and repeated which lacks
immersive sense. Currently, there are more and more research and development work
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 571–581, 2018.
https://doi.org/10.1007/978-3-319-74521-3_59

using VR techniques to construct virtual environments combined with mechatronic for
diﬀerent physical education applications. Widerun is the ﬁrst fully interactive bike
trainer speciﬁcally designed to deliver engaging ﬁtness sessions through VR headsets
and external screens, delivering a responsive, immersive, biking experience with
unlimited virtual 3D worlds featuring games and bike tracks.
In this research, the VR environment was developed to connect to the rock climbing
machine and provide a nearly real experience [8, 9]. HTC Vive was utilized as the VR
helmet, Leap Motion was used to recognize user’s hands and Vive Tracker to track the
position and orientation of the rock climbing machine. To satisfy the function of immer‐
sive rock climbing system, a suitable development engine is necessary, which should
work well with HTC Vive, Leap Motion and Vive Tracker with accessory hardware
devices. Unity3D engine is chosen as it can meet all above needs at the same time [10,
11]. The system combines rock climbing ﬁtness and social networking, to provide users
with immersive rock climbing experience, and give users a scientiﬁc feedback report of
health.
2
Development
The developed VR system consists of following components, which include the user
control interface, 3D geometric model of the entire environment and the correspondence
of the virtual and real.
The overall VR system architecture is illustrated in Fig. 1. A number of software
tools as well as hardware interfaces are used to develop the VR environment.
Fig. 1. Overall system architecture
2.1
The Control Interface and the Program Design
The Control Interface consists of four main interfaces, which are Log in Interface, Mode
Selection Interface, Setting Interface and Mountains Selection Interface. Each main
interface is built in diﬀerent scene using UGUI [12] (Fig. 2).
Control interface is the windows for information exchanging between user and the
system. User can interact with it according to his/her needs. Each main interface contains
one or more sub-interfaces. All of them form the application interface level that imple‐
ments the system function operation.
572
Y. Su et al.

So as to accomplish the login function, we create a database with two sheets. They
are user base information sheet: users: id, username, avatar and user authorization infor‐
mation sheet: user_auths: id, user_id, identity_type, identiﬁer, credential. When the user
sends the mailbox/username/phone number and password request to log in, we ﬁrst
determine the type the check the credential. For example, someone use username
“xiaoming” to login, part of the code is shown below:
SELECT * FROM user_auths WHERE identity_type =‘username’ and identi‐
ﬁer=‘xiaoming’
It will return user_id when password match with credential.
When user is in the scene “SelectMount” and select a mount, the program will load
the ‘LoadingScene’ at ﬁrst, then start the Coroutine ‘BeginLoading’ on its Start function.
Part of the code is shown below:
IEnumerator BeginLoading () 
{ 
asyn = SceneManager.LoadSceneAsync(MountName);
yield return asyn; 
} 
The progress of “asyn” will be shown in “LoadingScene” as shown in the Fig. 3.
Fig. 2. Four main interfaces of the Control Interface
Development of Virtual Reality-Based Rock Climbing System
573

Fig. 3. Loading progress of a mountain scene
The Setting Interface, which consists of video settings and audio settings, enable
user to change the eﬀect of mountain scene and the sound of system (Fig. 4).
(a) video settings
(b) audio settings
Fig. 4. Two sub-interface of the Setting Interface
The video settings allow user to set the resolution, the anti-aliasing mode, the ﬁltering
mode or shader to tell system how to render. The audio settings are for adjusting the
volume and quality of all sounds in the entire climbing system, giving the user the most
comfortable sound experience.
2.2
Geometric Model
2.2.1
Construction of Large-Scale Outdoor Rock Climbing Scene Modeling
The virtual scene model is the 3D data foundation of the virtual reality system [13]. The
ﬁdelity of the model and the data precision are the key factors to the successful construc‐
tion of the virtual reality system. In this paper, the construction process of large-scale
outdoor rock climbing scene and the key technology of large-scale virtual scene for VR
application are systematically studied. Finally, the construction of large-scale immersive
rock climbing scene is completed. The building process is shown in Fig. 5.
Step 1: Select the real outdoor climbing peaks for parametric analysis. Study the corre‐
sponding terrain appearance to provide data and graphics support for the estab‐
lishment of virtual mountain model.
574
Y. Su et al.

Step 2: Using the Terrain tool in Unity3D to draw the rough model of the mountain,
and then build the detailed model to shape it to be similar with the real mountain
appearance.
Step 3: Map the model and improve the mapping quality. Add the sky, sunlight, trees
and other natural elements to increase the reality degree and improve the
immersive experience.
Step 4: To determine whether the virtual rock climbing scene is similar to the real peak
scene. If there is a big gap, it is necessary to continue to optimize the model to
develop the immersion sense. If it meets the requirements, the scene is supposed
to be completed.
Fig. 5. Large-scale outdoor rock climbing scene modeling process
Fig. 6. UV mapping principle
2.2.2
Development of Immersive Scene
For a three-dimensional model, there are two most important coordinate systems, one
of which is the position of the vertex (X, Y, Z) and the other for the UV coordinates [14].
As shown in Fig. 6, the UV texture map deﬁnes the location information in the map,
which is associated with the 3D model and the location of the model surface texture.
The UV texture mapping technology is diﬀerent from the traditional plane projection
Development of Virtual Reality-Based Rock Climbing System
575

map. Each point of the image is accurately matched to the surface of the model object,
and the gap between the two adjacent points is processed by the smooth interpolation
of the image.
In the 3ds MAX 3D modeling software, polygon modeling technology is utilized to
build the rock model, as shown in Fig. 7(a). The rock texture map is obtained by
contrasting it to the real rock appearance. Through the UV mapping technology, the rock
model map is rendered to derive the ﬁnal model, which is shown as Fig. 7(b).
(a) Original rock model
(b) Rock model with UV map
Fig. 7. Rock model built by polygon modeling technology and UV map method
(a) Original large-scale terrain           (b) Optimized large-scale terrain
(C) Close-range mountain
Fig. 8. Mountain modeling
The brush tool of the terrain editor is utilized to build the mountain model. By refer‐
ring the model to the actual mountain surface of the Roraima, the mountains, valleys
and plains in the real peaks as shown in Fig. 8(a) are carved out. By using Photoshop
software to deal with the color map and the normal map of the environment, the corre‐
sponding textures are obtained. Then the terrain editor tool is adopted to select the
576
Y. Su et al.

diﬀerent materials on the terrain mapping. The model eﬀect is shown in Fig. 8(b). The
close-range mountain is shown in Fig. 8(c).
2.2.3
LOD Technology
In the process of running virtual reality system, the hardware system has to do real-time
rendering, which puts forward higher requirements to complex scene optimization. LOD
technology can guarantee the visual eﬀect. With the virtual point of view and the target
model object distance data changed, the system shows models of diﬀerent levels. LOD
follows the principle that rough model is drawing for long range and detailed one is for
close range. The LOD principle is designed as shown in Fig. 9.
Fig. 9. LOD schematic
a
LOD 0
b
LOD1
c
LOD2
d
LOD3
Fig. 10. LOD rendering models
Through the establishment of diﬀerent level of detail complex models and drawing
corresponding levels, it can realize the optimization of computing resources on the basis
of ensuring visual eﬀects. The system is based on Unity3d engine. By applying LOD
Group components and add diﬀerent level models, optimal resource adjustment is
Development of Virtual Reality-Based Rock Climbing System
577

achieved. Taking the tree model as an example, the LOD level detail rendering technique
is designed as shown in Fig. 10.
2.3
Correspondence of the Virtual and Real
2.3.1
Correspondence of the Climbing Surface
When the machine’s amplitude changes, the position and rotation of the tracker will
change at the same time. Since the rotation center or axis of the virtual mountain surface
or the rock climbing surface is diﬀerent, it is necessary to transform the position and
rotation of the tracker then set it as the virtual mountain surface’s ones in order to super‐
impose the virtual mountain surface on the rock climbing surface after the machine’s
amplitude changed.
A script, named Accessory.cs, is attached to the virtual mountain surface in order to
ﬁx the relative position and rotation of Vive Tracker and the rock climbing machine.
Part of the code is shown below [15]:
//Get current Tracker pose
Vector3 tracker_position = 
SteamVR_Controller.Input(deviceIndex).transform.pos + 
GameObject.Find("LMHeadMountedRig").transform.position; 
Quaternion tracker_rotation = 
SteamVR_Controller.Input(deviceIndex).transform.rot * 
GameObject.Find("LMHeadMountedRig").transform.rotation; 
//Transform current Tracker pose to Accessory pose 
this.transform.rotation = tracker_rotation * delta_rotation; 
this.transform.position = tracker_position + (tracker_rotation * 
delta_rotation) * delta_displacement; 
2.3.2
Correspondence of the Climbing Holds
In order to match the hold in the virtual system, it is necessary to convert the plane
position data to the position data of the hold relative to the starting point. We ﬁrstly
obtain the relative position of the tracker and the zeroth hold on the zeroth plate, and
thus deduce the position where the virtual hold should be, and then use this as the starting
point for the virtual holds. In the virtual scene, we create an empty GameObject as the
plane where all holds are placed, named HoldsGroup. We load each hold in accordance
with the distance, converted from the path ﬁle, of the hold relative to the starting point,
and make it perpendicular to the HoldsGroup plane. Finally, we superimpose the
578
Y. Su et al.

HoldsGroup plane on the virtual mountain surface so that the virtual hold can coincide
with the real hold.
Besides, we acquire the running speed of the rock climbing machine by using encoder
during the operation of it, and send it to PC via Serial Port. When the machine is running,
the rock climbing surface will rotate and the real holds will continuously move down.
We move up the Room in the VR environment at the speed received from machine so
as to match the virtual hold with the real hold through.
3
Interactive Device
3.1
Leap Motion
Leap Motion is use as hands and ﬁngers tracking device [16]. In order to use Leap Motion
normally, we need to install Leap Motion Orion software and download and import the
UNITY CORE ASSETS package to the Unity3D “IndoorClimbing” project. Then we
drag a LMHeadMountedRig into the scene from the path: LeapMotion/Prefabs. The
LMHeadMountedRig prefab uses the camera location provided by Unity to place the
LeapHandController at the correct position in the virtual world. The coordinates in the
tracking data are then transformed from Leap space to Unity space relative to the position
and orientation of the LeapHandController game object.
We import the UI Input module as well in order to control the standard Unity UI
widgets by naked hands. The primary component of the UI input module is the LeapE‐
ventSystem prefab. The LeapInputModule script component of this prefab implements
a Unity Input Module that uses tracking data to allow the user to manipulate standard
UI controls with their hands.
3.2
Vive Tracker
Vive Tracker have many modes to communicate with PC, the mode we used is that track
moving objects using a wireless interface in VR, with the accessory (refer to the rock
climbing machine) passing data to a PC via ‘FATFS-SDIO-USB’ [15]. The dongle is
used to transfer tracking data from the Vive Tracker to a PC, but the accessory transfers
data to/from a PC directly for a speciﬁc purpose based on our design that transfer the
Fig. 11. The communication mode
Development of Virtual Reality-Based Rock Climbing System
579

path ﬁle to the rock climbing machine’s controller. The communication mode is shown
in Fig. 11.
3.3
The Rock Climbing Machine
We have designed a rock climbing machine that is compatible with the immersive rock
climbing system. The developed virtual reality-based machine enables interaction with the
immersive rock climbing environment. There are four functional modules that were
designed to achieve the experience of the indoor climbing as similar as outdoor climbing
by superimposing virtual mountain surface on the rock climbing surface (Fig. 12).
Fig. 12. The virtual mountain surface superimposed on the rock climbing surface
The overall drive module controls the rock climbing machine to work through human
body weight as well as hydraulic resistance, to achieve the dynamic balance of people
climbing process. The path change module is used to achieve the path change of the rock
climbing process during climbing. We design the variable amplitude module to change the
slope of climbing machine through the worm gear mechanism in order to simulate the
different slope of the rock in the real outdoor rock climbing. The security module is used
to reduce the risk of the user during the climbing process and the failure rate of the machine.
4
Conclusion
Virtual Reality Technology as Emerging Technology has a huge development prospects.
Climbing movement as a new movement is also increasingly favored by the masses. The
immersive rock climbing system not only effectively solves the problem of rock climbing
site, but also puts forward a good solution to the problem of repeated path of indoor
climbing machine and dull experience. The system combines rock climbing machine with
virtual reality technology to achieve the experience of indoor rock climbing more intense,
more authenticity. The system make the climbing movement, whose risk factor is large,
into a safe indoor movement, and broaden the range of people who can enjoy the rock
580
Y. Su et al.

climbing. Users can stay at home to enjoy the beauty of the world’s major famous climbing
mountains via this system. This project is of great significance to the development of rock
climbing and the combination of virtual reality technology and sports [17].
References
1. Chen, D.: Colorful Virtual Reality World. China Water & Power Press, Beijing (2015)
2. Yu, X.: Virtual Reality Technology Basic Tutorial. Tsinghua University Press, Beijing (2015)
3. Shen, Y.: Virtual Reality Technology. Tsinghua University Press, Beijing (2009)
4. Wang, H.: Virtual Reality: Leading the Future Human-Computer Interaction Revolution.
China Machine Press, Beijing (2016)
5. Magiera, A., Roczniok, R.: The climbing preferences of advanced rock climbers. Hum. Mov.
14(3), 254–264 (2013)
6. Hinch, T.: “It’s a place to climb”: place meanings of indoor rock climbing facilities. Leisure/
loisir 38(3–4), 271–293 (2014)
7. Udrea, P.E.: Rock climbing exercising machine. CA, CA 2794590 A1 (2014)
8. Zulj, S., Seketa, G., Dzaja, D., Celic, L., Magjarevic, R.: Virtual reality system for assisted
exercising using WBAN. In: IFMBE Proceedings of 6th European Conference of the
International Federation for Medical and Biological Engineering, vol. 45, pp. 719–722 (2014)
9. Zhang, L., Liu, Q.: Application of simulation and virtual reality to physical education and
athletic training. In: Pan, Z., Cheok, A.D., Müller, W., Chang, M., Zhang, M. (eds.)
Transactions on Edutainment VII. LNCS, vol. 7145, pp. 24–33. Springer, Heidelberg (2012).
https://doi.org/10.1007/978-3-642-29050-3_3
10. Jerald, J., Giokaris, P., Woodall, D., Hartbolt, A., Chandak, A., Kuntz, S.: Developing virtual
reality applications with unity. In: Virtual Reality, pp. 1–3. IEEE (2014)
11. Linowes, J.: Unity Virtual Reality Projects. Kybernetes, vol. 29, no. 3 (2015)
12. Jackson, S.: Unity3D UI Essentials (2015)
13. Mooney, T.: 3ds Max Speed Modeling for Games. Packt Publishing, Birmingham (2012)
14. Xudong, Q.: Research on 3D Model Codec. (Doctoral dissertation, University of Science and
Technology of China)
15. HTC Corporation: HTC Vive Tracker Developer Guideline (2017)
16. Nandy, A.: Leap Motion for Developers. Apress, New York (2016)
17. Chang, L., Chen, D.: The development of immersive rock climbing system based on virtual
reality technology. In: National Conference on Intelligent Manufacturing. Chinese
Association of Artiﬁcial Intelligence (2016)
Development of Virtual Reality-Based Rock Climbing System
581

Kinematics and Simulation Analysis of a 3-DOF Mobile
Handling Robot Based ADAMS and MATLAB
Jingbo Hu, Dingfang Chen
(✉), Lijie Li, Jie Mei, Qin Guo, and Huafeng Shi
School of Logistic Engineering, Wuhan University of Technology, Heping Road No.1040,
Wuchang District, Wuhan 430063, Hubei, China
cadcs@126.com
Abstract. A 3 Degree-Of-Freedom (DOF) mobile handling robot model is
designed by SolidWorks package. The link coordinate system and kinematics
equation is established by the D-H parameter method. The kinematics and inverse
kinematics is solved and it oﬀers theoretical foundation. And then, the kinematics
simulation analysis of the robot is carried by using ADAMS–multi-body dynamic
simulation software. Finally, the trajectory planning is carried out by MATLAB.
It establishes the foundation for the structure design, dynamic analysis and control
system of the robot.
Keywords: D-H parameter method · Handling robot · Kinematic
Adam simulation · Coordinate system
1
Introduction
The handling robot made in KUKA, ABB and so on has been used in the production
process of assembly, welding, handling and etc. [1]. With the robot technology gradually
becoming mature, handling robots have presented an important support in the ﬁeld of
industrial production [2]. In microelectronics manufacturing, welding, packaging and
many other areas, handling robots have been widely used [3]. At present, the handling
robot has been widely used in the national economy in all important aspects [4]. But,
there are less robots which combines motion and handling. In fact, robots combining
motion and handling are necessary and can play important role in many ﬁelds.
In this paper, a mobile handling robot model is established. And in the end the
displacement of the joints is analyzed, which provides the basis for the debugging of
handling robot, saving the on-site debugging time and protecting the handling robot in
an extent [5].
2
Kinematics Models of 3-DOF Mobile Handling Robot
The robot designed in this paper is a 3 DOF mobile handling robot. Three manipulators
are rotating joints and the rotation axes are parallel to each other. As shown in Fig. 1,
the front mobile platform (1) is equipped with a servo for the steering of the mobile
platform, and the servo implements the forward and backward commands of the
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 582–587, 2018.
https://doi.org/10.1007/978-3-319-74521-3_60

platform. The joints (7), (8), (9) are individually driven by a servo to achieve steering.
All servos are controlled by the controller (3). The end part of the arm (6) can be provided
with diﬀerent clamping mechanisms depending on the work object. In the actual process,
through the mobile platform, the object can be held and moved from the target point 1
to the target point 2.
Fig. 1. Three - dimensional simplified model of mobile handling robot. 1-Mobile platform, 2-
Additional weight, 3-Controller, 4-Big arm, 5-Middle arm, 6-End arm, 7-Joint 3, 8-Joint 2, 9-Joint 1
As the robot actually works, the movement and handling are not carried out at the
same time. Therefore, the model can be simpliﬁed in the analysis of the robot’s handling
characteristics. The mobile part of the robot can be neglected temperately, and only
manipulator is analyzed. According to the D-H parameter method, the coordinate system
of the links in the manipulator is built. The coordinate system of each joint is established
at the end of the link. Each coordinate system draws only two axes in the Fig. 2.
Fig. 2. Link coordinator system of the mobile handling robot
According to the link coordinate system established in Fig. 2, the corresponding
D-H parameter table can be obtained, as shown in Table 1.
Kinematics and Simulation Analysis of a 3-DOF Mobile Handling Robot
583

Table 1. D-H parameter table of the mobile handling robot
Joint (i)
ai(mm)
𝛼i(°)
di(mm)
𝜃i(°)
1
–90
0
𝜃1
2
a2
0
d2
𝜃2
3
a1a3
0
d3
𝜃3
According to above table, the posture transformation matrix is obtained as:
0
3T = 0
1T1
2T2
3T =
⎡
⎢
⎢
⎢⎣
nx ox ax px
ny oy ay py
nz oz az pz
0 0 0 1
⎤
⎥
⎥
⎥⎦
(1)
In the analysis, the geometric and joint variables of links are supposed to be known,
and the position and posture of the robot terminal actuator relative to the reference
coordinate system can be solved [6]. The inverse kinematics means that the position and
posture of the robot terminal actuator are provided to derive the joint parameters. In
order to control the robot arm, the inverse kinematics solution is the basis of the robot
movement [7]. It is because that in the practically work, the variables of all joints are
determined according to the position of target point.
For a given position and posture, the transformation matrix of robot is deduced as
formula (1), The values of the joint variable 𝜃1, 𝜃2, 𝜃3 can be solved by using corre‐
sponding inverse transformation matrix multiply Eq. (1), as shown in Table 2:
Table 2. The algebraic solution of each joint
𝜃i(°)
The algebraic solution
𝜃1
arctan
(
ax∕ay
)
𝜃2
arctan
((
axc𝜃1 −azs𝜃1
)
∕
(
azc𝜃1 + axs𝜃1
))
𝜃3
arccos (c𝜃2
(nxc𝜃1 −nzs𝜃1
) −s𝜃2
(nzc𝜃1 + nxs𝜃1
))
3
Kinematics Analyses
Virtual Prototyping Technology is a digital design method based on virtual prototyping,
which can shorten product development cycles, improve product quality [8]. A three-
dimensional simpliﬁed model of the robot is imported into an ADAMS virtual prototype
for simulation [9]. Adam simulation is carried out to get the displacement curve of the
marker points relative to the earth, as shown in Figs. 3, 4, 5 and 6
584
H. Jingbo et al.

Fig. 3. The marker point displacement curve of joint 1
Fig. 4. The marker point displacement curve of joint 2
Fig. 5. The marker point displacement curve of joint 3
As it can be seen from Fig. 3, the displacement of the mark point 1 does not change
because the joint shaft 1 is connected to the seat (i.e., the base), so that the displacement
does not change. From Figs. 4, 5 and 6, it is found that the displacement of the marker
points in the Z direction does not change and there is no movement in the Z direction.
In the X, Y direction, the displacement curve approximates in the form of a sin function.
The results of the above ADAMS simulation are in accordance with the actual situation,
which veriﬁes the rationality of the designed structure. The kinematics analysis and
research provide the theoretical and simulation basis for the subsequent product design.
Kinematics and Simulation Analysis of a 3-DOF Mobile Handling Robot
585

4
Trajectory Planning
The trajectory planning is to map each joint variable to a smooth time function [10],
which includes Cartesian space planning and joint space planning. The Cartesian space
planning is to use the Cartesian coordinate points’ sequences of the end eﬀector position
to constitute a trajectory, which may reach the singularity. But the situation does not
appear in the joint space planning. In this paper, the trajectory planning of robot is carried
out by using the joint space planning method (Fig. 7).
Fig. 7. Space model of robot
As can be seen from Fig. 8, the kinematic curve in the x, y direction of the terminal
manipulator can be seen as a trigonometric function, and the kinematic curve in the z
direction can be considered basically no change, which is the same as the previous Adam
simulation results. If the image is furtherly solved, the correlation graph of velocity and
acceleration can be obtained. It can be seen that the trajectory is in accordance with the
operational requirements of the mobile manipulator robot, indicating that the path plan‐
ning method is reasonable.
Fig. 6. The marker point displacement curve of terminal link 3
586
H. Jingbo et al.

(a) Displacement curve              (b) Three-dimensional space trajectory
Fig. 8. Terminal manipulator
5
Conclusion
In this paper, the displacement curves of the three joints and the end point of the robot
are obtained, and the rationality of the structural design is veriﬁed, which provides the
theoretical and simulation basis for the next step of the robot design. The two-dimen‐
sional and three-dimensional displacement curves of the end of the manipulator further
validate the rationality of the mechanism and the correctness of the Adam simulation.
It establishes the foundation for the structure design, dynamic analysis and control
system of the robot.
References
1. Miao, D.: Design and Dynamic Performance Analysis of Heavy Load Handling Robot. Hefei
University of Technology (2014)
2. Li, R.: Development strategy of industrial robot industrialization in China. Aviat. Manuf.
Technol. 9, 32–37 (2010)
3. Li, C., Yang, Z., Cai, T.: A motion planning of moving robot based on ADAMS simulation.
Mech. Transm. 9, 28–31, 37 (2016)
4. Song, D.: Application of industrial robot in manufacturing industry. In: China Automotive
Industry and Equipment Manufacturing Industry Development Forum (2008)
5. Han, X., Li, C., Yu, X., Zhao G.: Modeling simulation of arc welding robot based on ADAMS/
view. Weld. J. 4, 69–72, 116 (2013)
6. Cai, Z., Xie, B.: Robotics, 3rd edn. Tsinghua University Press, Beijing
7. Ding, L., Li, E., Tan, M., Wang, Y.: Design and kinematics analysis of ﬁve-DOF moving
robot system. J. Huazhong Univ. Sci. Technol. S1, 19–22 (2015). (Natural Science Edition)
8. Chen, Z., Dong, Y.: Characteristics and Examples of MSC Adams Simulation of Multi-Body
Dynamics. China Water Resources and Hydropower Press, Beijing (2012)
9. Ma, R., Hao, S., Zheng, W.: Research on joint simulation of manipulator based on MATLAB
and ADAMS. J. Mech. Des. Manuf. 4, 93–95 (2010)
10. Wang, Y., Yu, X., Li, N., Zhu, W.: Kinematical analysis of ROBONOVA-1 robot. J. Xihua
Univ. 03, 6–9, 22 (2009). (Natural Science Edition)
Kinematics and Simulation Analysis of a 3-DOF Mobile Handling Robot
587

A New Type of 3D Printing Nozzle with PET Wire
as Raw Material
Yawei Hong, Shaobo Li
(✉), Shupei Wu, Tianhao Huang, Guang Liu,
Liwen Chang, and Yang Zhang
Yujiatou Campus, Wuhan University of Technology, Wuhan, Hubei, China
1985296054@qq.com
Abstract. In this paper, the wasted PET bottles were analyzed. With the experi‐
ment analysis and the related literature, we determine the feasibility of the wasted
PET plastic bottles as 3D printing materials. We design and introduced the prin‐
ciple of the 3D print nozzle of using the positive and negative screw to extrude
material. The feasibility of the design is demonstrated by three - dimensional
modeling, physical experiment analysis and ANSYS simulation analysis.
Keywords: FDM technology · PET material · Positive and negative screw
The modiﬁcation of material
1
Introduction
The “No.1” plastic bottle which is made of the PET material accounts for more than half
of the plastic bottle market. The output is huge, but its recovery rate is low [1]. The PET
material can be used as an excellent printing supplies, because of its strong adhesion
between the layers, good mobility, easy carbonation and other advantages. However,
due to the crystallization rate of the PET material is slow, the PET material can’t meet
the requirements of 3D printing technology rapid prototyping.
In recent years, FDM technology is the most used 3D printing technology, many
open source desktop 3D printers are mostly using this program [2]. FDM printing
supplies are mostly ABS and PLA [3], supplies are expensive, and ABS in the printing
process will release toxic gases. The use of ABS and PLA for FDM printing has some
shortcomings. For example, it is easy to break the wire, poor extrusion molded product
and product strength is not enough and the surface accuracy is poor. To improve the
printing performance, Kannan [4] added iron powder to the ABS, the addition of surface
active agent material made of iron powder or ABS composite. You Shu studied the eﬀect
of 3D printing conditions on the mechanical properties of PLA plastics [5].
In this paper, we ﬁrst studied the modiﬁcation of materials. We have designed a
nucleating agent addition device, positive and negative threaded screw extrusion device,
heating and cooling system. The feasibility of the design is demonstrated by three-
dimensional modeling, physical experiment analysis and ANSYS simulation analysis.
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 588–594, 2018.
https://doi.org/10.1007/978-3-319-74521-3_61

2
The Study on Modiﬁcation of Materials
The experiment two types of mainstream PET plastic bottles on the market were DSC
experiments, the experimental results shown in Figs. 1 and 2. From the DSC experi‐
mental data obtained, wasted PET material Tm is 240 °C–255 °C, the diﬀerence between
pure PET plastic bottle is not much, and it can be used for secondary processing.
Fig. 1. Nongfushanquan
Fig. 2. Yibao
It is helpful to increase the crystallization rate and improve the mechanical properties
of the ﬁnished product by adding nucleating agent to PET during the melting process
[6]. The results showed that the use of inorganic nucleating agent talc powder to modify
the PET material, when the nucleating agent to add the quality ratio of 5% [7], the
crystallization eﬀect is better, the printing eﬀect is the best.
A New Type of 3D Printing Nozzle with PET Wire as Raw Material
589

3
Nozzle Mechanical Structure
3.1
Overview of the Overall Structure
The design of new nozzle structure includes nucleating agent addition device, positive
and negative threaded screw extrusion device, heating and cooling system. Figure 3 is
a graph model for explosion.
Fig. 3. Model explosion diagram
Fig. 4. Schematic diagram of heat dissipation
The nucleating agent adding device comprises a feeding screw rod and a material
cylinder. It can achieve the purpose of accurately and stably adding nucleating agent.
The extruding device is composed of a mixing cylinder and a positive and negative
threaded screw extrusion device.
The heating device is arranged outside the mixing cylinder and is heated by a heating
rod, and the temperature is accurately controlled by a temperature sensor. The heat
dissipation system adopts the principle of air pump blowing. The heat dissipation eﬀect
is better, the energy consumption is lower, and the noise is smaller. A schematic diagram
of the heat sink is shown in Fig. 4.
3.1.1
Nucleating Agent Adding Device
Because the amount of nucleating agent is small, and the amount of added material is
larger, it needs to be accurately controlled. In this paper, the screw feeding mechanism
is used for feeding, the nucleating agent is stored in the material bed and sent to the feed
bed continuously through an external device to ensure that the nucleating agent can be
continuously transported to the pet cylinder. The feed screw of the nucleating agent is
driven by a micro reduction motor, and the rotation speed of the motor is controlled
accurately through the PWM wave, so that the nucleating agent in the screw thread gap
can be accurately and stably added to the print nozzle.
3.1.2
Positive and Negative Thread Extrusion Device
Device section view as shown in Fig. 6 and screw thread. Both ends of the thread is the
thread, the thread and the inner wall of the sleeve is tightly attached at both ends of a
590
Y. Hong et al.

thread feeding extrusion eﬀect. The middle part is anti thread, and the screw thread and
the inner wall of the sleeve are buﬀered by a certain clearance between the threads and
the inner wall of the sleeve. PET sheet material into the above material inlet, PET mate‐
rial into the molten state, it will melt the screw thread of PET material is transported
downwards and the formation of certain anti back ﬂow in the thread, and after the
nucleating agent is fully mixed, melted PET material is down from the nozzle at the
lower end of the screw extrusion molding.
3.1.3
Heating and Cooling System
The heating device is arranged around the nozzle and is heated by the mixing device,
and the inner part is connected with a heating rod and a temperature sensor. The heating
rod when heating block temperature increases when the temperature is higher than the
preset temperature, the heating rod stops working. When the temperature is lower than
the preset temperature, the heating rod starts working, heating block temperature rise.
By this dynamic adjustment process, the internal temperature of the screw extruder is
maintained at about 250 °C by controlling the extrusion of the screw.
The heat transfer is installed on the nozzle frame to dissipate heat at the nozzle, so
as to ensure that the pet material can be crystallized and cooled in time when the nozzle
is extruded.
3.2
Experiments and Simulation Analysis
3.2.1
Positive and Negative Screw Experiment Analysis
First of all, the simulation results show that the positive and reverse directions screw
can mix up the PET material with Polymer Nucleators. The experimental process is
shown in Fig. 5. The screw can squeeze the molten PET smoothly. In addition, it is
veriﬁed that the anti-thread can achieve the purpose of mixing.
Fig. 5. Test simulation process
A New Type of 3D Printing Nozzle with PET Wire as Raw Material
591

Figure 6 is the positive and negative screw in kind. Figure 7 shows the nozzle’s
working process.
Fig. 6. The positive and negative screw
Fig. 7. The nozzle’s working process
By analyzing the experiment results, We can obtain that:
(1) The rate of discharging of the molten PET material depends on the screw’s speed.
The faster the screw’s speed is, the faster the extrusion rate of molten PET material
is.
(2) The molten PET material ﬂows simultaneously in the cartridge and can be well
blended with the nucleating agent.
(3) The ﬂow velocity is relatively stable and PET can extrude smoothly at the nozzle.
3.2.2
Simulation Analysis of Heating and Cooling System
We conduct thermal analysis of the nozzle’s heating and cooling modules by ANSYS
Workbench [8]. Nozzle, heating block, mixing cylinder’s material is brass. Their thermal
conductivity is 45 W/(m*K). Screw’s material is stainless steel. Its thermal conductivity
is 14.6 W/(m*K). We know that the melting temperature of pet is between 245–255 °C.
The optimum heating temperature of heating rod is obtained by thermal analysis. Nozzle
surface’s convective heat transfer coeﬃcient is set to 50 W/(m2*K). The heating block
is set to 35 W/(m2*K). Mixing cylinder is set to 20 W/(m2*K). By setting diﬀerent
heating temperatures for analysis. When the temperature of the heating rod is set to
270 °C, the internal temperature of the mixing cylinder reaches 250 °C. Through the
heat pump nozzle, the temperature dropped to 230 °C. This is consistent with the actual
temperature requirement for pet melting during actual printing.
Figure 8 is the temperature distribution nephogram, Fig. 9 is the picture of pressure
nephogram.
592
Y. Hong et al.

Fig. 8. Temperature distribution nephogram
Fig. 9. Pressure nephogram
4
Conclusion
The nozzle we designed completes the modiﬁcation and extrusion molding of waste
PET material. The whole process from the plastic bottle to the actual model can be
realized. PET material does not produce toxic substances during printing which is more
environmentally friendly Compared to ABS. However the melting range is too narrow.
Suitable temperature is an important indicator to measure the printing eﬀect. How to
control the printing temperature accurately is a problem we need to solve. Due to the
complex internal mechanical structure of the nozzle, the vibration amplitude of the
nozzle has a great inﬂuence on the model printing accuracy. It is the important direction
of mechanical debugging to make the printing more smoothly. This design has solved
the current printer can’t use PET material as 3D printing raw materials. Waste PET
material is cheap, the cost of printing is reduced by about 90% Compared to ABS and
PLA materials which provides a new opportunity for the development and wide appli‐
cation of 3D printing technology [9].
References
1. Tang, G., Hu, B., Kang, Z.: Current situation and problems of waste plastics recycling. Renew.
Resour. Recycl. Econ. 6(1) (2013)
2. Guo, K.: Three-dimensional printing equipment and data processing software a number of key
technology research. Huazhong University of Science and Technology (2008)
3. Melnikova, R., Ehrmann, A., Finsterbusch, K.: 3D printing of textile-based structures by Fused
Deposition Modelling (FDM) with diﬀerent polymer materials. Mater. Sci. Eng. 62(1), 12–18
(2012)
4. Kannan, S., Senthilkumaran, D., Elangovan, K.: Development of composite materials by rapid
prototyping technology using FDM method. In: International Conference on Current Trends
in Engineering and Technology, vol. 13, pp. 281–284 (2013)
5. You, S., Hu, Y., Wei, Q.: Eﬀects of 3D printing conditions on the mechanical properties of
degradable polylactic acid. Plastics 29(3), 91–94 (2015)
A New Type of 3D Printing Nozzle with PET Wire as Raw Material
593

6. Legras, R., Dekoninck, J.M., Vanzieleghem, A., Mercier, J.P., Nield, E.: Crystallization of
poly (ethylene terephthalate) induced by organic salts: model compound study of the
mechanism of action of the nucleating agent. Polymer 27, 109–117 (1986)
7. Ailan, Z.: Application of nucleating agents in crystallization of PET. Polyester Ind. 05, 1–5
(2011)
8. Gao, J.: FDM rapid prototyping machine temperature ﬁeld and stress ﬁeld numerical simulation
9. Sun, J., Tong, Z., Ying, Z.: Analysis of the market development prospect and application of
3D printing technology. Wuhan Univ. Technol. 6(1), 1672–3918 (2014)
594
Y. Hong et al.

When Partitioning Works and When It Doesn’t:
An Empirical Study on Cache Way Partitioning
Hanfeng Qin
(✉)
School of Computer Science and Technology, Huazhong University of Science and Technology,
Wuhan 430074, China
hanfengtsin@hust.edu.cn
Abstract. Virtualization and cloud computing technologies enable modern data
centers to consolidate various services and applications with the prevalent multi‐
core processor to improve resource utilization. However, service consolidation
has the risk of degraded quality-of-service (QoS) due to uncontrolled contention
for the shared last-level caches (LLC). Cache partitioning techniques are prom‐
ising to improve resource utilization as well as guarantee QoS. As the capacity
of LLC in data center is ever growing and there have been some practical cache
partitioning techniques implemented in production system. Although the parti‐
tioning schemes have been explored extensively, how to make eﬀective use of
partitioning is still an important problem in data center and not well understood.
Given the varying cache conﬁgurations, the complex workload mixes of diverse
memory characteristics, and the diﬀerent overheads of partitioning algorithms,
we do not always gain performance improvement with cache partitioning. In this
paper, we are seeking to explore when partitioning. We investigate the impact of
cache conﬁgurations, memory characteristic of program, and partitioning varia‐
tion to the performance gain under partitioning. We also identify several inter‐
esting ﬁndings and implications which help us in future cache system design and
optimization for cloud data centers.
Keywords: Cache partitioning · Memory architecture · Empirical study
1
Introduction
The poor resource utilization in data centers increases the total cost of ownership (TCO)
of IT service. For example, the average utilization achieves only around 6% to 12% in
Google’s data center [10]. Recent eﬀorts in virtualization and cloud computing tech‐
nologies are promising to improve resource utilization by consolidating many services
on the same server in data centers. However, co-locating more applications has the risk
of degraded quality-of-service (QoS) due to uncontrolled contention for shared
resources, primarily the last-level caches (LLC) [18, 19]. Researchers leverage cache
partitioning techniques [17] to address the arbitrary access to the shared LLC. As an
important performance isolation mechanism, cache partitioning has been proposed to
restrict the available amount of shared cache lines that an applications can access when
it co-runs with other workloads. Depending on the optimization target, generally, a cache
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 595–607, 2018.
https://doi.org/10.1007/978-3-319-74521-3_62

partitioning technique consists of a line allocation policy and a partitioning-enable
mechanism, which determine the amount of lines a program receives, and enforce such
allocation to be actually executed, respectively. Prior works explored many cache parti‐
tioning schemes implemented by hardware [8, 14, 15, 19], software [9, 20], or co-
designed hardware and software [4, 6].
As the capacity of last-level cache (LLC) in modern x86 processors is ever growing,
there have been some practical cache partitioning technologies implemented in produc‐
tion system and markets [4, 6]. It is important to understand how to make eﬀective use
of cache partitioning techniques in data center. Despite that cache partitioning techni‐
ques have been extensively explored, given the varying cache conﬁgurations, the
complex workload mixes of diverse memory characteristics, and the diﬀerent overheads
of partitioning algorithms how to use cache partitioning eﬀectively is still a problem and
not well understood. We carry an experimental case study that we consolidate various
quad-core workload mixes to co-run on the same multicore server that supports cache
partitioning, and ﬁnd that cache partitioning does not always work as an expected
winner. Partitioning vanishes its expected performance gains for some workloads, even
worse, results in unexpected performance degradation.
We are very interested in the unexpected behavior of cache partitioning which has
not ever been studied in prior work. In this paper, we are seeking to explore when parti‐
tioning works and when it does not, and how to make eﬀective use of it. With an empir‐
ical study on a commonly used way-partitioning technique, we perform a thorough
evaluation with the SPEC CPU2006 suite. We group the total 29 programs into 11
subsets based on memory intensity analysis and way sensitive analysis, and construct
77 workloads with various memory characteristics in terms of memory access intensity
and associative way sensitivity. We investigate the impact to partitioning performance
of many possible factors spanning from cache conﬁgurations, memory characteristic of
program, and partitioning variations to the performance gain under partitioning. We also
identify several interesting ﬁndings and implications which help us in future cache
system design and optimization for cloud data centers.
We highlight our key contributions here:
• We identify an important problem on how to make eﬀective use of cache partitioning
technology which has not ever been well understood by prior studies. To seek answers
to this problem, we perform a detailed empirical study on a commonly used way-
partitioning technology for a variety of workload mixes with diﬀerent memory access
intensity and associative way sensitivity, and study the impact of cache conﬁgura‐
tions, memory characteristics of program, and the partitioning variation to the
performance gain under partitioning quantitatively.
• We identity several interesting ﬁndings and implications which help us in future
cache system design and optimization for cloud data centers. (i) There is a close
correlation between cache conﬁgurations and the performance gains from cache
partitioning where cache partitioning does not work in caches of small capacity.
Increasing the set number can improve the performance gains of cache partitioning.
Moreover, it is not beneﬁcial to design high-associative caches in data center because
they do not make cache partitioning work better. (ii) Consolidating services in data
centers should take the memory characteristics of programs into account. It degrades
596
H. Qin

performance when cache partitioning is applied to programs that have less similarity
in memory access intensity of high memory intensity. Co-scheduling programs of
heterogeneous cache way sensitivity can make eﬀective use of cache partitioning.
(iii) The overhead of dynamic partitioning adjustment impacts the performance gains
of cache partitioning. Design a partitioning algorithm with a steady partitioning size
from less frequently adjustment can improve performance.
2
Motivation
Cache partitioning is an important performance isolation mechanism in use of guaran‐
teeing QoS. Table 1 summarizes the commonly used partitioning schemes during the
past research eﬀorts. Hardware-based implementations require architecture modiﬁca‐
tion and are performance eﬃcient. Software-based schemes are based on page coloring
theory that leverage dynamic page allocation in OS to enforce pages to be scattered in
contiguous caches. Co-HW/SW-based solutions combine the ﬂexibility and eﬃciency
of both software and hardware with low-level architecture extensions and a group of
programming control routines.
Table 1. Cache partitioning technologies
Implementation
Partitioning schemes
Hardware
Way-partitioning [8, 14, 15, 19]
Software
Page coloring [9, 20]
Co-HW/SW
Intel CAT [6]
As the capacity of last-level cache (LLC) in modern x86 processors is ever growing.
It is important to understand how to make eﬀective use of cache partitioning techniques
in data center. For example, Intel releases Xeon E7-8893 v4 processor equipped with
an LLC of 60 MB, and IBM Power 8 associates a larger LLC of 96 MB. It is important
to understand how to make eﬀective use of cache partitioning techniques in data center.
Although has been proposed for a decade around, way-partitioning has still been an
active baseline in cache partitioning research [1, 12, 19], even in complex workload in
context of cloud computing [3] and warehouse computer [7]. In practical production
system and market, Intel releases a cache allocation techniques (CAT) [4, 6] in Haswell
SKUs processor, which is also based on way-partitioning.
Although cache partitioning techniques have been extensively explored, how to use
cache partitioning eﬀectively is still a problem and not well understood. We carry an
experiment and ﬁnd that cache partitioning does not always work as an expected winner.
We consolidate various quad-core workload mixes to co-run on the same multicore
server that supports CAT, and measure their performance under the baseline LRU and
a CAT-based partitioning policy, respectively. Figure 1 reports the results under the
baseline system and CAT-based partitioning. We ﬁnd that cache partitioning has diverse
impact on performance for diﬀerent workload mixes. For some workloads cache parti‐
tioning outperforms LRU indeed as expected. But for some workloads, the expected
beneﬁts of partitioning vanish. Even worse, it result in unexpected performance
When Partitioning Works and When It Doesn’t
597

degradation for some workloads under partitioning. We are very interested in seeking
the answer to this unexpected behavior which has not ever been reported in prior work.
In this paper, we are motivated to explore when partitioning works and when it does not,
and how to make eﬀective use of it.
Fig. 1. Performance comparison for various workload mixes under the baseline LRU and the
Intel CAT-based partitioning replacement policy
3
Experimental Methodology
3.1
Simulator
We use an event-driven ZSim [16] to model our baseline multicore system. ZSim is
a fast x86 micro-architecture simulator based on Intel Pin [11]. We closely model an
Intel Sandy-bridge processor, of which configuration parameters are detailed
Table 2. Simulation conﬁgurations of baseline multicore system
Components
Parameters
Processor
4-core, 2.6 GHz, Out-of-Order, 4-issue width, 168-entry ROB,
64-entry load queue, 32-entry store queue
L1 Cache
32 KB, split instruction/data cache, 4-way associative,
64-byte block size, 4-cycle latency, LRU replacement
L2 Cache
256 KB, uniﬁed cache, 8-way associative, 64-byte block size,
8-cycle latency, LRU replacement
L3 Cache
4 MB, uniﬁed cache, 16-way associative, 64-byte block size,
28-cycle latency, LRU replacement
Memory
1 channel, 8 ranks, 8 banks, 64-bit bandwidth,
1333 MHz bus frequency, 64-entry read queue, 64-entry write queue,
open page management, 1 KB row buﬀer,
tRCD = 15 ns, tCL = 15 ns, tRP = 15 ns, tWTR = 7.5 ns,
tWR = 15 ns, BL/2 = 4, FR-FCFS request scheduler
598
H. Qin

presented in Table 2. Each core has its private split 32 KB 4-way associative L1
instruction and data cache, and a unified 256 KB 8-way associative L2 cache. A
unified 4 MB 16-way associative L3 cache (LLC) is shared by all cores. The default
LLC replacement policy in our baseline system is LRU. We choose Utility Cache
Partitioning (UCP) [14] as way-partitioning candidate, and implement an UCP-based
replacement policy, labeled as WayPart in Sect. 4. We configure UCP with a per-
core 256-line 16-way associative UMON circuits, and enforce the Lookahead algo‐
rithm to dynamically resize the partitions every 5000 cycles, similar as [14].
3.2
Workloads
The SPEC CPU2006 [5] suite is used to perform evaluation. We compile each program
using GCC 6.2.1 with an optimization ﬂag of -O3, and feed them with a single reference
input. A representative slice of 500 million instructions for each program is identiﬁed
with PinPoint [13]. To characterize the memory behavior, we perform both way sensi‐
tivity analysis and memory intensity analysis for the total 29 programs, respectively.
Fig. 2. Sensitivity of performance in terms of IPC to cache ways
Way Sensitivity Analysis. We observe both the varied performance in terms of
instructions per cycles (IPC) as we change the available way number a program can
access. We classify these programs into four categories: Way insensitive (WI) programs
do not experience performance improvement as cache way increases. Way sensitive
(WS) programs have a positive correlation between performance improvement and
increased ways. Way ﬁt (WF) programs also improve performance as cache way
When Partitioning Works and When It Doesn’t
599

increases, but would not increase any more at some point even more ways are assigned.
Way step (WT) programs improve performance as cache way increases, and they show
an increased steady phase. Figure 2 depicts the correlation curves of performance and
cache ways for typical workload within these categories.
Memory Access Intensive Analysis. We also study the memory access intensity in
terms of misses per thousand instructions (MPKI) and classify programs into four cate‐
gories: High intensive (HI) programs have MPKI larger than 30, thus generate very high
memory pressure. Medium intensive (MI) programs have MPKI larger than 10 but less
than 30. Low intensive (LI) programs have MPKI larger than 1 but less than 10. Non-
intensive (NI) programs have lower MPKI than 1 and thus they do not contend for the
memory bandwidth.
Combining both memory intensity and way sensitivity, we group the total 29
programs into 11 subsets as shown in Table 3. We perform evaluation with a total of 77
quad-workloads mixed with programs from these categories by varying memory inten‐
sity and way sensitivity.
Table 3. Memory characteristics of SPEC CPU2006 in a 4 MB shared LLC
#
Memory intensity
Way sensitivity
Benchmarks
G01
HI
WT
429.mcf, 473.astar
G02
HI
WF
470.lbm
G03
MI
WT
437.leslie3d, 482.sphinx3, 459.GemsFDTD
G04
MI
WS
450.soplex, 471.omnetpp, 483.xalancbmk
G05
MI
WI
462.libquantum, 433.milc
G06
LI
WT
434.zeusmp, 445.gobmk, 436.cactusADM
G07
LI
WF
447.dealII
G08
NI
WT
400.perlbench, 435.gromacs
G09
NI
WF
458.sjeng, 403.gcc, 444.namd, 481.wrf,
465.tonto, 453.povray, 416.gamess
G10
NI
WS
401.bzip2, 456.hmmer, 464.h264ref
G11
NI
WI
410.bwaves, 454.calculix
3.3
Simulation Control
We leverage a most common simulation control mechanism used in the past researches
in cache memories [8, 14, 16]. We have caches warmed up with the subsequent 1 billion
instructions, and have each program detailed executed at least 500 million instruction.
If some program ﬁnishes earlier, they continue to run to contend for shared cache with
other co-running programs. We only report performance for the ﬁrst 500 million instruc‐
tions interval. Performance are measured with weighted speedup [2] calculated as
Weighted Speedup =
∑IPCshared
i
IPCalone
i
,
600
H. Qin

where both the IPCshared
i
 and IPCalone
i
 are performances of the i-th program when it co-
runs with other programs, and that when it runs alone, respectively.
4
Empirical Studies
In this section, we report the results of our empirical studies. We are restricted to present
the evaluation to a subset of these workloads due to page limit instead of the total 77
workload mixes, nevertheless, the results are also applied to the remaining workloads.
Specially, we perform evaluations on workload mixes of medium memory intensity and
varying way sensitivity as shown in Table 4 in studying the impact of cache conﬁgura‐
tion, cache way sensitivity, and partitioning variations to performance. We use workload
mixes of way step sensitivity and varying memory intensity as shown in Table 5 to
investigate the impact of memory intensity to performance.
Table 4. Workload mixes of medium memory intensity and varying way sensitivity
Category
Workload mixes
Category
Workload mixes
4WS
(450,471,483,471)
4WT
(437,482,459,437)
4WI
(462,433,462,433)
3WS + 1WI
(450,471,483,462)
3WS + 1WT
(450,471,483,459)
2WS + 2WI
(471,483,462,433)
2WS + 1WI + 1WT (483,450,433,459)
2WS + 2WT
(471,483,437,482)
1WS + 1WI + 2WT (450,462,437,482)
1WS + 3WI
(450,462,433,462)
1WS + 2WI + 1WT (471,462,433,459)
1WS + 3WT
(471,437,482,459)
Table 5. Workload mixes of way step sensitivity and varying memory intensity
Category
Workload mixes
Category
Workload mixes
1HI + 3LI
(473,434,445,436) 1HI + 1MI + 1LI + 1NI (473,459,436,435)
1HI + 3MI
(429,437,482,459) 1HI + 1MI + 2LI
(473,437,434,445)
2HI + 2MI
(429,473,437,482) 1HI + 1MI + 2NI
(429,482,435,400)
2HI + 2LI
(429,473,445,436) 1HI + 2MI + 1LI
(429,482,459,434)
2HI + 2NI
(429,473,435,400) 1HI + 2MI + 1NI
(473,437,482,400)
3HI + 1MI
(429,473,429,482) 2HI + 1MI + 1LI
(429,473,459,434)
3HI + 1LI
(429,473,429,436) 2HI + 1MI + 1NI
(429,473,437,435)
3HI + 1NI
(429,473,429,435) 2HI + 1LI + 1NI
(429,473,445,400)
1HI + 3NI
(429,400,435,400)
4.1
Impact of Cache Conﬁguration
Cache Set Number. Figure 3 reports the normalized performances across a group of
cache size from 4 MB to 16 MB by increasing the number of cache sets while keeping
a ﬁxed cache way number. We can see that the set number has a direct impact of
performance gains of way-partitioning. To our surprised, the baseline LRU outperforms
way-partitioning for most workloads (7 out of 12) with a small cache of 4 MB. As we
When Partitioning Works and When It Doesn’t
601

increase the set number, way-partitioning improves these workloads gradually. When
the cache size is increase to 16 MB, besides 2 workloads which have the similar
performance under both partitioning and LRU, way-partitioning wins LRU for 10 out
of the total 12 workloads, including the 7 workloads which has poor performance in
small caches of 4 MB. Way-partitioning achieves a normalized performance improve‐
ment to LRU at a geometric mean of 11.5% in the large cache of 16 MB.
Fig. 3. Normalized performance of way-partitioning to LRU in LLC of the same way numbers
but diﬀerent set numbers
Fig. 4. Normalized performance of way-partitioning to LRU in LLC of the same set numbers
but diﬀerent way numbers
Cache Way Number. We set a ﬁxed cache size of 4 MB but varying the associativity
to 16-way and 32-way, respectively. Figure 4 reports the normalized performances of
way-partitioning to LRU with an associativity of 16-way and 32-way. Considering that
the workloads used in this evaluation are picked in terms of cache way sensitivity, we
expect to see performance improvement for workload mixes containing more cache way
sensitive programs under a high associative cache. However, we hardly observe any
performance improvement for all workload mixes, only by a geometric mean of 1%, as
we increase the cache associativity. Compared with the impact of increasing the cache
602
H. Qin

set number, the performance improvement from increasing cache way number is tiny
(1% v.s. 11.5%).
The diﬀerent impact of cache set number and cache way number attributes to the
amount of conﬂict misses. As the cache set number increases, the address stream is
scattered among more cache sets, consequently, decreases the conﬂict misses within a
cache set. However, increasing the cache way number decreases capacity misses but
does not prevent conﬂict misses. Excessive conﬂict misses diminish the performance
gains form cache partitioning.
4.2
Impact of Memory Characteristics
Memory Access Intensity. We investigate the impact of memory access intensity with
the mixed workloads of way step sensitivity and various memory intensity shown in
Table 5. The normalized performance of cache partitioning to LRU are presented in
Fig. 5. We observe an unexpected performance degradation for almost all of the work‐
loads with cache partitioning. Compared with LRU, the performance loss in way-parti‐
tioning reaches by a geometric mean of 17% and at most by 38%. Obviously, for this
group of workload mixes, cache partitioning does not work at all. Further, we ﬁnd that
there is a negative correlation of performance loss with the similarity in memory access
intensity of the mixed workloads. It is highly likely to suﬀer performance degradation
when programs that have less similarity are co-scheduled. For example, the workloads
labeled as 1HI3LI and 3HI1LI both contain programs of high memory intensive and
low memory intensive. The large diversity in memory access intensity, consequently,
makes them suﬀer signiﬁcant performance loss. In contrast, the performance of work‐
loads labeled as 3HI1MI and 3MI1HI has little impact because they have smaller
diversity in memory access intensity. This negative correlation can be explained with
the extra misses from partitioning interference. We compares the cache misses in LRU
and way-partitioning for each program in the workload mixes of 3HI1LI as shown in
Table 6. We can see with partitioning, the misses in 429.mcf decreases but the misses
for the remaining programs increase. The misses in 436.cactusADM increases by
20x. Due to partitioning interference, extra misses are pushed to the co-scheduled part‐
ners.
Fig. 5. Normalized performance of way-partitioning to LRU for workload mixes of cache way
step sensitivity programs under diﬀerent memory intensities
When Partitioning Works and When It Doesn’t
603

Table 6. Cache misses comparison of workload mixes with high diverse memory access intensity
under LRU and way-partitioning
Cache misses
473.astar
429.mcf
473.astar
436.cacutsADM
LRU
45.38
68.27
45.38
5.04
WayPart
48.29
64.60
48.28
102.74
Associative Way Sensitivity. We evaluate the impact of associative way sensitivity
by enforce mixed workloads of ﬁxed medium memory intensive while varying asso‐
ciative way sensitivities. Figure 3(a) shows the normalized performance of way-parti‐
tioning to that of LRU. Firstly, we observe a similar performance in a 4 MB shared LLC
on average under either LRU or way-partitioning. Secondly, we observe that way-parti‐
tioning outperforms LRU for some workloads as a result of eﬀective use of the allocated
ways. For example, for workload mixes labeled as 1WS3WI, an extra performance of
3% to 4% can be gained with cache partitioning. Thirdly, we observe that workload mixes
that beneﬁt from way-partitioning contain at least one way insensitive program, which
is necessary but not suﬃcient. Way insensitive programs do not beneﬁt from the extra
lines received. For example, 462.libquantum accesses cache lines with a streaming
pattern and does not see any performance gains on receiving more ways. Consequently,
cache partitioning preserve less ways for these programs, typically only 1 way. The
remaining ways can be devoted to those highly utilize caches. Thirdly, we ﬁnd perform‐
ance is highly dependent on co-runners when way insensitive program are scheduled
with others. Workload mix labeled as 1WS1WI2WT has a performance lost due to the
excessive line contention from programs of way step sensitivity.
4.3
Impact of Partitioning Variation
To correlate performance with programs that beneﬁt from cache partitioning, we review
the dynamic number of cache blocks each program receives during co-running.
Figure 6 presents the number of blocks allocated by the Lookahead algorithm in UCP
for a workload mix labeled as 1WS3WT consists of 450.soplex, 437.leslie3d,
482.sphinx3, and 459.GemsFDTD. Firstly, we observe a negative correlation of
partitioning size variation with cache size. In a 4 MB LLC, the number of allocated
blocks varies frequently, which results a frequent partitioning adjustment, consequently,
the overhead of partitioning increases. With the increment of cache set to 8 MB and
16 MB, the variation comes down gradually. Secondly, we see that more performance
improves as more steady the partition is. It implicates that frequent partition variation
does not improve performance due to extra overhead of partitioning adjustment.
604
H. Qin

Fig. 6. Cache blocks received by each program in a medium memory intensive workload mixes
consisting of one way sensitive and three way step sensitive programs under the Lookahead
partitioning algorithm
4.4
Implications
We summarize our ﬁndings and their implications to future cache system design or
eﬀective cache resource exploration for cloud data centers as follows.
• Cache partitioning does not work in caches of small capacity. Increasing capacity by
increasing the set number instead of the way number can improve the performance
gains of cache partitioning. Considering the design complication, high overhead and
energy of associative lookup, it is not beneﬁcial to design high-associative caches in
data center because they do not make cache partitioning work better.
• Consolidating services in data centers should take the memory characteristics of
programs into account since they have direct impact on performance gains of cache
partitioning. It degrades performance when cache partitioning is applied to programs
that have less similarity in memory access intensity of high memory intensity. Co-
scheduling programs of heterogeneous cache way sensitivity can make eﬀective use
of cache partitioning.
• The overhead of dynamic partitioning adjustment impacts the performance gains of
cache partitioning. Design a partitioning algorithm with a steady partitioning size
from less frequently adjustment can improve performance.
5
Conclusion
Service consolidation is promising to improve the poor resource utilization in cloud data
centers but at a risk of suffering performance due to uncontrolled access to shared last-
level cache. Although cache partitioning schemes have been exploited extensively, how
to make effect use of cache partitioning is still not well understood. In this paper, we are
seeking to explore when partitioning works and when it does not with an empirical study
on a commonly used way-partitioning policy for a variety of workloads. We investigate
the impact of cache configuration, memory characteristic of program, and partitioning
variation to the performance gain under partitioning. We identify several interesting find‐
ings which help us in future cache system design and optimization for cloud data centers.
When Partitioning Works and When It Doesn’t
605

References
1. Cook, H., Moreto, M., Bird, S., Dao, K., Patterson, D.A., Asanovic, K.: A hardware evaluation
of cache partitioning to improve utilization and energy-eﬃciency while preserving
responsiveness. In: Proceedings of ISCA, pp. 308–319 (2013)
2. Eyerman, S., Eeckhout, L.: System-level performance metrics for multiprogram workloads.
IEEE Micro 28(3), 42–53 (2008)
3. Ferdman, M., Adileh, A., Kocberber, O., Volos, S., Alisafaee, M., Jevdjic, D., Kaynak, C.,
Popescu, A.D., Ailamaki, A., Falsaﬁ, B.: Clearing the clouds: a study of emerging scale-out
workloads on modern hardware. In: Proceedings of ASPLOS, pp. 37–48 (2012)
4. Funaro, L., Ben-Yehuda, O.A., Schuster, A.: Ginseng: market-driven LLC allocation. In:
Proceedings of ATC, pp. 295–308 (2016)
5. Henning, J.L.: SPEC CPU2006 benchmark descriptions. ACM SIGARCH Comput. Archit.
News 34(4), 1–17 (2006)
6. Herdrich, A., Verplanke, E., Autee, P., Illikkal, R., Gianos, C., Singhal, R., Iyer, R.: Cache
QoS: from concept to reality in the Intel® Xeon® processor e5-2600 v3 product family. In:
Proceedings of HPCA, pp. 657–668 (2016)
7. Kanev, S., Darago, J.P., Hazelwood, K., Ranganathan, P., Moseley, T., Wei, G.Y., Brooks,
D.: Proﬁling a warehouse-scale computer. In: Proceedings of ISCA, pp. 158–169 (2015)
8. Kasture, H., Sanchez, D.: Ubik: eﬃcient cache sharing with strict QoS for latency-critical
workloads. In: Proceedings of ASPLOS, pp. 729–742 (2014)
9. Lin, J., Lu, Q., Ding, X., Zhang, Z., Zhang, X., Sadayappan, P.: Gaining insights into multicore
cache partitioning: bridging the gap between simulation and real systems. In: Proceedings of
HPCA, pp. 367–378 (2008)
10. Lo, D., Cheng, L., Govindaraju, R., Barroso, L.A., Kozyrakis, C.: Towards energy propor-
tionality for large-scale latency-critical workloads. In: Proceedings of ISCA, pp. 301–312
(2014)
11. Luk, C., Cohn, R., Muth, R., Patil, H., Klauser, A., Lowney, G., Wallace, S., Reddi, V.J.,
Hazelwood, K.: Pin: building customized program analysis tools with dynamic
instrumentation. In: Proceedings of PLDI, pp. 190–200 (2005)
12. Pan, A., Pai, V.S.: Imbalanced cache partitioning for balanced data-parallel programs. In:
Proceedings of MICRO, pp. 297–309 (2013)
13. Patil, H., Cohn, R., Charney, M., Kapoor, R., Sun, A., Karunanidhi, A.: PinPointing
representative portions of large Intel® Itanium® programs with dynamic instrumentation. In:
Proceedings of MICRO, pp. 81–92 (2004)
14. Qureshi, M.K., Patt, Y.N.: Utility-based cache partitioning: a low-overhead, high
performance, runtime mechanism to partition shared caches. In: Proceedings of MICRO, pp.
423–432 (2006)
15. Sanchez, D., Kozyrakis, C.: Vantage: scalable and eﬃcient ﬁne-grain cache partitioning. In:
Proceedings of ISCA, pp. 57–68 (2011)
16. Sanchez, D., Kozyrakis, C.: ZSim: fast and accurate microarchitectural simulation of
thousand-core systems. In: Proceedings of ISCA, pp. 475–486 (2013)
17. Suh, G.E., Rudolph, L., Devadas, S.: Dynamic partitioning of shared cache memory. J.
Supercomput. 28(1), 7–26 (2004)
18. Tang, L., Mars, J., Vachharajani, N., Hundt, R., Soﬀa, M.L.: The impact of memory subsystem
resource sharing on datacenter applications. In: Proceedings of ISCA, pp. 283–294. ACM
(2011)
606
H. Qin

19. Yang, H., Breslow, A., Mars, J., Tang, L.: Bubble-Flux: precise online QoS management for
increased utilization in warehouse scale computers. In: Proceedings of ISCA, pp. 607–618
(2013)
20. Ye, Y., West, R., Cheng, Z., Li, Y.: COLORIS: a dynamic cache partitioning system using
page coloring. In: Proceedings of PACT, pp. 381–392 (2014)
When Partitioning Works and When It Doesn’t
607

Track Maintenance Feedback Mechanism
Based on Hadoop Big Data Analysis
Yong Zhu
(✉), Jiawei Fan
(✉), Guangyue Liu
(✉), Mou Wang
(✉), and Qian Wang
(✉)
Intelligent Manufacturing and Control Institute, Wuhan University of Technology, Wuhan, China
zhuyong2016@gmail.com, 1228915795@qq.com, 491534654@qq.com,
137078388@qq.com, 775316545@qq.com
Abstract. With the rapid development of economy and the people’s growing
material needs, increased frequency and intensity of railway transportation, the
requirement of increasing the railway maintenance, security is becoming more
and more attention. The current routine of daily maintenance is done mainly by
manual and large rail inspection vehicles. The maintenance method is of high
strength, low eﬃciency, high risk and low maintenance accuracy. Based on the
above background, the project team has designed an eﬃcient track inspection
machine based on the collaborative working method of the mother-machine. The
railway maintenance and data collection is achieved through the collaborative
work of the mother-machine. In this case, the mother machine detects and collects
the data, the sub-machine repairs and collects the data, the upper machine imple‐
ments the coordination, the big data processing and the feedback system. Data
collected by a railway big data, to take advantage of these data, the team set up
big data processing system based on hadoop, adopting clustering analysis, inte‐
grated analysis and time prediction analysis method, experience about defect
distribution map, so as to optimize the workings of a composite aircraft, constantly
improve the maintenance system based on composite aircraft performance. The
design of the project team is based on the system of the railway maintenance
system, which is intelligent and timely. Can be automated and dehumanized,
realize railway maintenance, and can improve the eﬃciency of railway mainte‐
nance system and reduce cost.
Keywords: Railway maintenance · Zipper · Big data analysis
Intelligent system · Feedback optimization
1
The Background Under the Time of Big Data
Recent years, China’s railway development is rapid as China’s economy continues to
rise. According to the data of the Ministry of Railways Statistics Center, China’s railway
operating mileage of 91,000 km, while the annual passenger traﬃc volume up to 167609
million passengers, the total amount of 364.27 million tons of cargo sent. China’s
railway system is now six times a large area of acceleration transformation project, the
introduction and development of high-speed railway speed of 350 km/h or more.
What’s more, the safety factor of the railway is highly demanded with the continuous
improvement of the railway. The daily maintenance of the track is mainly on the track
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 608–615, 2018.
https://doi.org/10.1007/978-3-319-74521-3_63

irregularity, orbital defect, bolt looseness and other issues detection and maintenance.
However, because of the wide distribution of China’s railway and large the number of
long tracks, China’s railway sector mainly rely on artiﬁcial or semi-automated machi‐
nery inspection method to detect and repair rail damage. Under this background, the
project team designed an eﬃcient rail maintenance machine which based on the coherent
work mode of the parent machine. At the same time, the machine can also achieve the
rail detection and maintenance of automation and intelligence. The most important is,
people could establish a big data processing system on the hadoop based on the process
of detection and maintenance of this machine.
Although the current equipment has data collection and data feedback, but it did not
achieve real-time interaction, the two part are distributed with a certain lag. The data
can not be timely fed back to the data processing system through the host computer after
the collection. And the data which has been dealt by the data processing terminal could
not be sent back to the work of equipment in time, so it could not met the requirement
of immediate maintenance.
Through the collection of big data on the railway, the Hadoop for data processing
and integration, and by the hadoop system for information feedback, the equipment
could examines the defects of the relevant data more accurately according the relevant
data. Besides, people could start a directional work through the integration of informa‐
tion and the types of defects and multiple locations. So the ﬁxed section of the railway
can be detected, and the eﬃciency can be improved.
2
Data Detection and Acquisition
From the background of railway’s development, we can see that there are two kinds of
information on the existing railway tracks. Among them, irregularity refers to the orbital
geometry, size and spatial position of the deviation. In the broadest sense, the position
of the center line of the linear track, the height of the track, deviates; the curve center
curve deviation; curvature, high, gauge value, slope changes in the size of the deviation,
collectively referred to as the track is not smooth. Track surface defects include cracks
of rail surface, abrasions, blocks, and the surface appears dark or black lines and so on.
Fig. 1. Track maintenance machine
Track Maintenance Feedback Mechanism
609

This information, needs to work through the way composite machine to work to collect.
Track maintenance machine is as shown in Fig. 1 [1].
The acquisition process is divided into three parts. The ﬁrst is the image, instrument
and other information of data acquisition. Then these directly collected information,
through the processing of data conversion, into the corresponding digital information.
Finally, the data ﬁnished, and through digital-analog conversion, the digital information
combined with positioning information into a digital group, and through the FPI bus-4G
communication networks to the host computer. The concrete data of detection and
acquisition methods are as follows:
2.1
Uneven Information Detection
In order to obtain the information of irregular, we are in accordance with the category
of irregular, diﬀerent types of track irregularity using diﬀerent instruments for testing.
Orbit irregularities can be divided into vertical orbit irregularities, transverse orbit irreg‐
ularities and complex orbit irregularities (Fig. 2).
Fig. 2. All sorts of schematic schematics
In order to obtain the above non-smooth information, we use the parent machine in
the detection of the parent machine for data collection. For diﬀerent types of irregular‐
ities, we use diﬀerent data collection. At work, each testing hardware works simultane‐
ously. Using displacement sensors to measure displacement variables, the actual gauge
values are added to the standard gauge. Using gyroscope to get the machine’s side rolling
Angle, the high value is calculated through trigonometric relation. The lateral acceler‐
ation of the vehicle is measured by the transverse acceleration sensor, and the integral
is obtained by the horizontal displacement deviation. By using the vertical acceleration
sensor, the vertical acceleration of the body is measured, and the integral obtains the
vertical displacement oﬀset, and the calculation is high and low [2].
2.2
Defect Information Detection
Track surface defects include the cracks of rail surface, abrasions, blocks, and the surface
appears dark or black lines and so on. The causes of defects can be divided into two
610
Y. Zhu et al.

categories: One is the limitation of the manufacturing process, the defects of the steel
rail during the forging process; The other is generated by the high intensity and high
density fatigue wear of the locomotive and rail.
As a result, we use machine vision technology to detect defects caused by this insta‐
bility. By CCD camera track surface image acquisition in the ﬁrst place, again by FPGA
combined with single chip microcomputer to feature extraction, image through image
processing the ﬁnal defect type and location. Then, according to the image pixel infor‐
mation, you can coordinate the defect [3].
2.3
Location Information Detection
Location information is mainly through GPS and encoder to achieve the absolute posi‐
tioning of geographic information and relative positioning. The GPS absolute posi‐
tioning is mainly the approximate range of the framing machine, the detected uneven
information and defect information. Coding counter relative positioning is mainly based
on the initial operation of the sub-machine positioning information and rail along the
direction of information to achieve the relative positioning of detection information.
Finally the use both of them to achieve the location of the defect location.
After obtaining the above three kinds of information, through the FPGA to achieve
the information of digital signal conversion, and through the FPI bus-4G network
communications to the host computer, to achieve data collection and further processing.
3
The Detection Data Processing and Analysis
After the data collection is the corresponding data processing, in this project, the most
important is the analysis of these data analysis and data feedback after this, you can
simplify the construction process of maintenance machinery in the railway track,
improve the eﬃciency of rail maintenance and management.
For the railway track data can be divided into structured data and unstructured data
two categories. Unstructured data refer to the data tracked by the rail maintenance
machine in this project, the most important of which is the generating data after the
image processing, and the processing of this part of the data is the key to the project.
3.1
Big Data Characteristics
As we collect and maintain data on railway maintenance, we accumulate large data
resources. For these big data, we need to analyze it to get the distribution of the railway
service. The big data has the following characteristics:
(1) The amount of data
Rail traﬃc is a wide range of distribution, long distance, daily life is essential a data
entity, so in order to collect data from the railway track, traditional data-processing
software can’t store or process such a huge amount of data, requiring large data to
meet the project’s requirements.
Track Maintenance Feedback Mechanism
611

(2) Data type diversiﬁcation
With the deepening of the project, the railway situation is intricate, so big data
diversity the data type characteristics of the project to meet the early stage of the
data processing.
(3) The rapid spread of data
In this project, it is hoped that the processing of the data on the railway situation
will be sent to the staﬀ’s hand-held equipment in order to respond quickly to the
abnormal state of the rail.
(4) Low the data density
Big data in the process of processing may need to deal with most of the nonsigni‐
ﬁcant data, and ultimately will reﬂect it contains that part of the high value of the
data or results.
3.2
Big Data Processing Technology Based on Hadoop
The data of the railway track is a lot of data, so the transmission and storage of these
data are key problems to be solved. In the data transmission, data compression can
eﬀectively reduce the amount of network data transmission and improve the storage
eﬃciency. We use the fault transient process of ascension based format signal compres‐
sion and reconstruction algorithm of real-time data, using linear integer transform
biorthogonal wavelet ﬁlter combination Huﬀman encoding method of track detection
of real-time data compression and decompression. Then, we need to decompress the
data after the data arrived in the monitoring center, it needs appropriate computing and
storage platform. In the data storage, because the orbital data on the real-time require‐
ments are not very high, so the amount of data that can be detected can be stored using
Hadoop’s HDFS storage system. To meet the hadoop of the big data technology to the
number of processing, we can continuously optimize the railway track detection system
according to the data feedback mechanism, such as after long-term detection, we can
determine the frequency of diﬀerent location defects, so we can set a diﬀerent detection
frequency, to meet the detection and eﬃcient management.
4
The Data Analysis and Optimization
In the sub-machine work together for some time, it will inevitably produce a large
number of railway maintenance data, the analysis and optimization of the massive data
have certain inﬂuence on the distribution and proportion of the work on the railway.
Therefore, we are for the detection and collection of data, data processing, the analysis,
to obtain big data based on the railway maintenance proﬁle, that is, “track spectrum.”
Through the data analysis of the track spectrum, we ﬁnd the frequency distribution rule
of the daily maintenance work, so as to optimize the working scheduling of the coop‐
erating sub-machine, realize the principle of the twenty-eight in the course of railway
daily maintenance, improve the eﬃciency of overhaul and reduce the cost of mainte‐
nance.
Data monitoring and analysis platform as shown in Fig. 3.
612
Y. Zhu et al.

Fig. 3. Monitoring platform
In order to analyse the data, we use two methods of clustering analysis and integration
analysis on the basis of Hadoop, and carry out the comprehensive evaluation and analysis
of the data by the two types of data and the state of track maintenance. Maintenance of
the real state and its forecast analysis, and targeted adjustment of the proportional
number of sub-machine maintenance and distribution of the scope of work. In the
following, two analytical methods will be brieﬂy introduced:
4.1
Clustering Analysis of Prediction Intensity
The clustering analysis is divided into two parts: the test set and the training set. The
detailed analysis is as follows, Fig. 4.
Fig. 4. Clustering analysis
Track Maintenance Feedback Mechanism
613

Through the prediction of strength, we can get predictive state distribution of railway
track defects and irregularities and bolts loosening, based on Hadoop, so as to make
corresponding repair and distribution rules for the occurrence rate of defects of diﬀerent
frequency in diﬀerent regions, and further to achieve the principle of twenty-eight, to
improve the eﬃciency of railway railways daily maintenance, reduce the cost of railway
railways daily maintenance.
4.2
Integration Analysis
Integration analysis is also an eﬀective way to solve the problem of “big p small n”. so
it is necessary to study the integration and analysis of diﬀerent data sets in the era of big
data [4].
Due to scattered data clustering analysis, analysis of incoherent, overall analysis is
not comprehensive, we adopted the combination of integration analysis, for railway
track maintenance and repair of large data analysis, realize the whole track along the
integrity of big data analysis.
4.3
Comprehensive Analysis
Based on the results of the above two analyzes, we can obtain the comprehensive distri‐
bution of railway track defects, irregularities, bolts and other information, and predict
the occurrence rate and time of occurrence of railway rails. For diﬀerent regions,
diﬀerent intensity and the frequency of maintenance planning, improve the eﬃciency
of maintenance sub-machine, reduce the corresponding cost, in order to achieve the
railway track maintenance operations really “twenty-eighty principle.” Maintenance
machine feedback workﬂow is shown as Fig. 5.
Fig. 5. Clustering analysis
614
Y. Zhu et al.

5
Summary
Today, the main way of railway maintenance is still labour, it exists high duration,
diﬃcult and other defects. The current machinery can only be automated, can not be
intelligent, can not meet today’s needs.
This project designed a highly eﬃcient intelligent rail maintenance machine, and
based on this design based on the hadoop for big data processing system, and its collec‐
tion, processing, analysis and feedback maintenance machine to detect the massive data,
the project has the following innovations:
(1) Mechanical structure design adaptive strong, can be automatically detected main‐
tenance operations
(2) Modular, integrated, systematic design can be adapted to a variety of complex
operating requirements
(3) Design of the data and instructions two-wire transmission function, to achieve the
machine’s online detection and status monitoring, and real-time access to the state
of the rail changes
(4) Designed data on-line analysis and big data processing platform to achieve the
analysis of the state of the railway maintenance and sub-machine work mechanism
feedback, shorten costs and improve eﬃciency
The project team uses cloud computing as a platform for heterogeneous and diver‐
siﬁed data storage and analysis and the platform after the operation of the massive data
based on the maintenance of state maintenance, system feedback optimization, isolated
information system interoperability Support, and become a candidate after integration.
This work has a low cost, good system scalability (unlimited storage capacity), high
reliability, parallel analysis and so on, will become one important system of intelligent
way of railway maintenance in the future.
References
1. Chen, C., Kong, J., et al.: Modern Mechanical Designer Manual. Mechanical Industry Press,
Beijing (2014)
2. Wang, Y., Yu, Z., Bia, B., Xu, X., Zhu, L.: Study on crack identiﬁcation algorithm of metro
tunnel based on image processing. J. Instrum. 07, 1489–1496 (2014)
3. Wang, Y.: Study on key technology of big data processing ﬂow based on Hadoop. Inf. Technol.
09, 143–146, 151 (2014)
4. Ma, S., Wang, X., Fang, K.: Integration analysis of big data. J. Stat. Res. 11, 3–11 (2015)
5. Cao, Y.: Hadoop Performance Optimization in Big Data Environment. Dalian Maritime
University (2013)
6. Tang, D.: Hadoop-based aﬃne propagation big data clustering analysis method. Comput. Eng.
Appl. 04, 29–34 (2015)
Track Maintenance Feedback Mechanism
615

Crowdsourcing and Stigmergic Approaches
for (Swarm) Intelligent Transportation Systems
Salvatore Distefano1,2(B), Giovanni Merlino1, Antonio Puliaﬁto1,
Davide Cerotti3, and Rustem Dautov2
1 Universit`a degli Studi di Messina, Messina, Italy
{sdistefano,gmerlino,apuliafito}@unime.it
2 Social and Urban Computing Group, Kazan Federal University, Kazan, Russia
{s distefano,rdautov}@it.kfu.ru
3 Politecnico di Milano, Milano, Italy
davide.cerotti@polimi.it
Abstract. In the last decades, the impact of Information and Commu-
nication Technologies (ICT) on transportation systems radically changed
them, identifying in the Intelligent Transportation Systems (ITS) a new
research area. A problem often addressed in ITS is vehicle routing, for
which plenty of solutions have been already deﬁned in literature. Vehicle
routing problems are usually NP hard, therefore these are mainly heuris-
tic solutions. A requirement for them is to be deployed and run in naviga-
tion systems, ready to react to sudden changes in a (quasi) real-time way.
Hence, to reduce the latency is still an open issue, not only depending
on the complexity of the solution but also on other parameters, such as
the traﬃc update latency in traﬃc-aware vehicle routing. A way to solve
them is by exploiting distributed, collaborative approaches, establishing
a proper collaboration platform and algorithms able to use it. Mobile
Crowdsensing, on the one hand, and collective and swarm intelligence
approaches, on the other, can ﬁll this gap. This paper is a ﬁrst attempt
in this direction, aiming at deﬁning a new class of (swarm) the Intelli-
gent Transportation Systems (SITS), on top of a crowdsourcing-based
infrastructure.
Keywords: MANETs · Mobile crowdsensing · Stigmergy
Traﬃc engineering · ITS
1
Introduction and Motivations
With an ever growing availability of embedded, mostly personal and mobile com-
puting devices for everyday tasks, there is an almost limitless potential for tap-
ping onboard resources, especially sensing-related ones, as well as corresponding
compute nodes to be exploited for locally executable tasks. Mobile CrowdSens-
ing (MCS) comprises by deﬁnition a category of applications where individuals
carrying sensor-hosting embedded computers (e.g. smartphones) get collectively
c
⃝Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 616–626, 2018.
https://doi.org/10.1007/978-3-319-74521-3_64

Crowdsourcing and Stigmergic Approaches
617
engaged in information gathering and sharing eﬀorts in order to analyze and
georeference events which may be interesting for individuals and communities
alike. MCS is already establishing itself as a trendy paradigm, but most eﬀorts
go into the direction of easing participatory (e.g. manned) patterns. Apart from
privacy and security issues, where anonymization and sandboxing respectively
are key countermeasures, as most engagement chances should be meant to be
opportunistic to let MCS be truly widespread, inexpensive and wholly disrup-
tive as a paradigm. In this context problems lie foremost in enabling unassisted
deployments, as well as accommodating for peer-oriented communication and
distributed self-organization, mostly due to real-world constraints, e.g. intermit-
tently (WAN-)disconnected operation.
One of the main advantages of MCS is the possibility to conduct sample col-
lection, data mining, etc., without accounting for the corresponding experiments
in advance, just leveraging natural daily life patterns arising from human activ-
ities as they happen and leave behind breadcrumbs in form of samplings ready
to be collected. Aim of this kind of enablement then is putting this power at the
ﬁngertips of developers or would-be entrepreneurs, ready to kickstart whichever
eﬀort in next to no time. In particular self-provisioning and autonomous cooper-
ation are needed to avoid long setup times for experiments, disruptions beyond
careful planning and sizing, as well as aiding coders in developing less custom
logic.
Most existing typical MCS applications currently feature a common, simpli-
ﬁed architecture, made up of two main components, one running on the embed-
ded device in order to collect and disseminate measurements, and a second one
as backend hosted on e.g. the Cloud for data mining, analytics and other busi-
ness intelligence according to the requirements of the application at hand. Each
application gets designed mostly from scratch with no common ground despite
every implementation tackling, of necessity, overlapping challenges in schedul-
ing, sampling, and communication duties, among others. A few drawbacks of
such siloed pattern deserve to be pointed out as severe hindrances despite the
promise of the underlying paradigm:
– wasted development eﬀorts, due to mostly ground-up coding every time,
including OS and platform-dependent adaptations
– unoptimized runtime, as multiple applications would execute on the same
nodes without taking into account such conﬁguration, possibly duplicating
sensing or processing activities on resource-constrained devices, thus also lim-
iting scalability of the platform
– no exploitation of proximity or density in topologies, by e.g. cooperation
across nodes.
In particular this last point is crucial, as any kind of high-density scenario, espe-
cially if with real-time constraints, e.g. Intelligent Transportation Systems (ITS)
as we will see in the following, needs a smart approach to proactively take advan-
tage of proximal nodes and crowded areas instead of crumbling under the weight
of such scale. Indeed, such a strategy could translate either in (self-)throttling
redundant devices or even letting node aggregates preprocess data and shape

618
S. Distefano et al.
traﬃc accordingly by e.g. network coding strategies, otherwise leaving the back-
end (and the network itself) prone to scalability issues over huge population
scenarios.
Given the paradigm, i.e. MCS, and forthcoming use cases, with speciﬁc regard
to ITS where mobility is really going to match crowds at scale, we conceived a
design pattern. Such a scheme may lend resilience when faced with big swarms,
while at the same time helping DevOps with their coding and deployment strate-
gies. More speciﬁcally, collaborative approaches, collective and swarm intelligent
ones could be good metaheuristics than can provide eﬀective solutions to ITS
problems such as vehicle routing, usually NP-hard. The collaboration of multiple
agents is the best solution available in the ITS scenario to reduce latency and
be eﬀective in addressing the routing problem also when traﬃc conditions must
be considered. This perfectly matches with crowdsourcing based paradigms as
MCS, mainly aiming at supporting applications able to exploit the collective
intelligence and their emerging properties in problem solving. In this paper, we
propose to address the routing problem adopting a swarm intelligence approach
able to take into account the road traﬃc conditions. It mainly consists of an ant
colony optimization (ACO) algorithm implemented and deployed on mobiles
constituting an MCS-contributed infrastructure, able to interact each other fol-
lowing an opportunistic patterns. We therefore framed our approach in the class
of Swarm ITS (SITS).
In the following, we are going to ﬁrst lay out an overview of ITS systems, then
discussing MCS solutions for them. After that, we focus on swarm intelligence,
stigmergy and ACO, coming up with SITS. This way, we deﬁne our traﬃc-
aware vehicle routing solution based on a modiﬁed version of an ACO. This
SITS solution is thus evaluated by a simulator which preliminary results are
discussed. Finally some remarks and hints for future work close the paper.
2
Intelligent Transportation Systems
Intelligent transportation systems (ITS) are the coherent combination of
advanced systems, and services which aim at providing as a whole innovative
solutions related to typically metropolitan and regional mobility based on mul-
tiple modes of transport, by means of traﬃc management, as well as enabling
users belonging to whichever category to be up-to-date, thoroughly informed
and aware of any (transient or structural) issues, in order to make safer, more
coordinated, and ‘smarter’ use of transport networks. A directive by the Euro-
pean Union Commission deﬁnes ITS [1] as “systems in which information and
communication technologies are applied in the ﬁeld of road transport, includ-
ing infrastructure, vehicles and users, and in traﬃc management and mobility
management, as well as for interfaces with other modes of transport”.
From the ICT perspective, we may envision ITS embracing any advanced
solution for transport engineering that integrates live data and other feedback
from a number of heterogeneous sources, such as parking guidance and infor-
mation systems. In particular, eﬀorts related to ITS seem naturally poised to

Crowdsourcing and Stigmergic Approaches
619
have as target high-population density areas considering a consistent orientation
of such networks towards multimodal systems of transportation, be those either
personal vehicles, shared vectors such as buses or trains.
ITS naturally spans a wide range of technologies, in particular ICT ones,
starting from basic management systems such as navigation ones, possibly to
be augmented in the future by systems where artiﬁcial “co-drivers” may assist
humans during their duties [2]. Yet, there are many other examples of instances
of subsystems prone to be enhanced through ICT, e.g., traﬃc signal control
systems, which may leverage some kind of system-optimal routing algorithm for
road networks as well, such as game-theory based ones [3]. Moreover, from a
technological viewpoint, any delay in information dissemination for vehicle-to-
vehicle communication networks [4], so called VANETs [5], considering a traﬃc-
dense conﬁguration as the relevant scenario, can be identiﬁed as one of the
main challenges to be overcome for any coordination system to really work as
expected. Some authors [6] have leveraged Deep Learning to predict traﬃc ﬂows
by dealing with Big Data sources. Such problems were also analyzed by model-
based solutions: for instance in [7] a stochastic (hazard-based) model to evaluate
the impact of a reliability-safety tradeoﬀon the travel-time is proposed.
3
A Crowdsourcing Infrastructure for ITS
Typical MCS applications mainly implement a client-server interaction pattern
where a service provider oﬀers MCS-based services to end users, leveraging con-
tributor willingness to provide their physical (sensing) resources [8]. Data are
therefore collected and processed by (backend and frontend) servers to carry out
aggregate analytics and feed back relevant results to end users.
Starting from the lowest level, through heuristics and algorithm design, local
analytics may provide a category of functions, among which simple ones are
interpolation, extrapolation and outlier ﬁltering, which may enhance a standard
MCS application. This aspect is summarized and depicted in Fig. 1, where the
main diﬀerences against the traditional MCS approach are highlighted in red.
Indeed, the diﬀerentiation is related to an opportunistic, collaborative approach,
where nodes may interact one another to aid local computations and perform
distributed optimizations on a small/medium scale. This way, end users may
leverage an MCS application, server interaction issues notwithstanding, by just
exploiting cooperation among nodes.
These designs may be quite application-speciﬁc, e.g., diﬀerent crowdsensing
applications would coexist, each bringing independently operating local analyt-
ics, yet still possibly accessing the same readings or applying comparable infer-
ence strategies. Anyway, local analytics provide data about a relatively conﬁned
area. There are applications with a diﬀerent set of requirements, where some
kind of aggregate analytics is needed, to be run at the backend. The main task
in this case is extracting patterns from huge sets of sensor-provided data, origi-
nating from large populations of mobile devices. Patterns may highlight features
of certain physical (or social) environments of interest, also helping in building
models about observed phenomena, a way to improve in forecasting.

620
S. Distefano et al.
Fig. 1. The MCS reference scenario. (Color ﬁgure online)
Identifying patterns from large datasets means resorting to data mining,
which calls for one of two approaches, according to either the size of incoming
data or the limits imposed by applications on delays. In the ﬁrst case measure-
ments are preliminary stored in a database, to apply mining algorithms against
whole datasets and detect patterns. When the input stream is continuous and
overwhelming in terms of storage, or even when applications would require fast
pattern recognition techniques, “streaming” algorithms for mining may be the
only viable solutions to identify patterns from streams in ﬂight, independently
from subsequent treatment such as long-term storage strategies. Data mining
algorithms usually require domain-speciﬁc expertise.
4
Swarm Intelligent Transportation Systems
Over such a set of (dynamic) meshes, we propose a stigmergic approach for
cooperation and optimization. Let’s ﬁrst tackle swarm optimization alone.
Ant Colony Optimization. Ant colony optimization (ACO) [9] is a relatively
recent metaheuristic based on the behavior of ants seeking a path between their
colony and a source of food. In nature wandering ants have exhibited in this
sense a provable capability to discover optimal paths. The collective intelligence
of the swarm relies on ants exchanging information indirectly via the environ-
ment (the so-called stigmergy). While traveling to search for food, ants lay down
pheromones on their way back to the nest (i.e. home colony) only when sources

Crowdsourcing and Stigmergic Approaches
621
of food are found. As other colony members step into pheromone trails, they
tend to stick to the beaten path accordingly. Moreover, the trace gets reinforced
as individuals follow the same trail, leaving pheromone of their own, in turn
resulting increasingly attractive for other ants. For any complex problem which
can be reduced to a search for optimal paths, ACO may work as a probabilistic
solver, by emulating such naturally occurring behavior. The probability pk
ij for
an artiﬁcial ant k, placed in vertex i, to move toward node j is deﬁned as follows:
pk
ij =
τ α
ij · ηβ
ij

l∈Nk
i (τ α
il · ηβ
il)
(1)
where τij corresponds to the quantity of pheromones laid over arc aij, ηij to
a-priori attractiveness of the move, computed by some heuristic embedding the
cost of choosing arc aij along the path that leads to the destination, and Nik is
the set of neighbours in node i for ant k, i.e., the nodes available for the ant to
move in. Coeﬃcients α and β are global parameters for the algorithm. According
to typical ACO variants, ants bring food back home (i.e. nest) after being done
with their movement. Denoting Tk as the tour of ant k, Ck is deﬁned as the
length of Tk, and used to specify the amount of pheromones to be placed by ant
k on each arc on the trail that led to the food source:
Δτ k
ij =

1
Ck if arc (i, j) belongs to T k
0
otherwise

(2)
τij = τij +
m

k=1
Δτ k
ij.
(3)
At the end of a round, after each ant has completed a move, the extent of
pheromones laid over each arc gets reduced (e.g. evaporates), according to:
τij = (1 −ρ)τij
(4)
where ρ is a global parameter as well. Such simpliﬁed form of the ACO is deﬁned
an “ant system” (AS). In Fig. 2 the pseudocode for an AS of size n is listed.
ACO algorithms yield their best performance when some form of local search
algorithm is employed.
Modiﬁed ACO for Traﬃc-Aware Route Planning. To adapt ACOs to
MCS applications, we propose here a novel version of an ACO, MoCSACO,
extending the algorithm of Fig. 2 into the one of Fig. 3, where an ant corresponds
to a (physical) mobile device, i.e., a real-world agent.
We are also redeﬁning the general objective of ﬁnding the shortest path
on a (weighted) graph in terms of reusing common, state-of-the-art and readily
available heuristics for path discovery. The A* search algorithm is such a solution,
and leaves us free to conﬁne stigmergy to weighting only, e.g. the admissible
heuristic function in case of A*, where each arc has a cost (e.g., weight) deﬁned

622
S. Distefano et al.
Fig. 2. The behavior of ant system in pseudocode.
Fig. 3. The behavior of modiﬁed ant system in pseudocode.

Crowdsourcing and Stigmergic Approaches
623
by a certain metric. In its turn, weight directly correlates to the quantity of
pheromones, as in:
wij = γ · τij
(5)
where wij is the weight of the arc, γ is a constant of proportionality and τij
represents the amount of pheromones placed on arc aij.
In order to make the a-priori cost (i.e., of choosing an arc along the path
towards destination) explicit, we ﬁrst deﬁne:
ci
j→d = cij + cjd
(6)
the distance (i.e., cost) cd
ij from node i towards destination d along node j as
the sum of that between i and j, cij, and that from j to destination, cjd. Then
we specify the cost over a certain arc aij:
cij = hij/wij
(7)
as the ratio between a certain property we want to use as metric, hij, and its
weight, wij.
Given all the above, the value of the a-priori gain, ηi
j→d, for a certain choice
leading to destination d is computed according to the following formula:
ηi
j→d = δ/ci
j→d
(8)
where the relationship is inversely proportional with respect to the (weighted)
distance, and δ is just a constant of proportionality.
Following this pattern, one more ﬁx, also applicable to the standard ACO
variant, would consist in relaxing the requirement that agents travel back home
after ﬁnding food, in its stead by leveraging the communication bus for near-
instant swarm-wide dissemination of pheromone trails.
Thus, a newly deﬁned probability pd
ij for any artiﬁcial ant, placed in vertex i,
to move toward node j, according to destination d, is deﬁned as follows:
pi
j→d =
τ α
ij · ηβ
i
j→d

l∈Nk
i (τ α
il · ηβ
i
l→d)
(9)
where τij corresponds to the quantity of pheromones laid over arc aij, ηij to
a-priori attractiveness of the choice, computed by some heuristic embedding the
cost of choosing arc aij along the path that leads to the destination, and Nik
is the set of neighbors in node i for ant k, i.e., the admissible transitions for
the ant.
The amount of pheromone to be deposited depends on a metric for posterior
costs, in general with no relation to h as deﬁned in Eq. 7. Even in this case,
pheromone gets updated as deﬁned in Eq. 3.
As choices are unpredictable and there cannot be a notion of rounds for real-
world agents, pheromone laid over each arc evaporates, still according to Eq. 4,
but on a time basis, by setting timers.

624
S. Distefano et al.
Even in this case, far for the solution from losing generality, on the contrary
the modiﬁcation expands the scope of applicability, by relaxing constraints over
the scenario.
5
Preliminary Evaluation
To evaluate our proposed technique, we examined the traﬃc of an urban area,
near down-town the city of Messina. Arcs weights represent the length lij of each
road segment such that, using Eqs. 6 and 7 we can compute for each destination d
the cost to take the arc (i, j) in the path towards node d without considering the
road traﬃc. Providing these values as a metric to deﬁne the heuristic function
of the A* search algorithm, we can ﬁnd the shortest path length, which can be
very diﬀerent from the optimal path when the traﬃc is considered. We then
implemented a simulator of the overall system. The evaluation of the model will
provide a diﬀerent metric to the heuristic function of the A* search algorithm
that takes in account also the traﬃc.
In the evaluation through the simulator we considered two scenarios where
either we take into account the pheromone value or not. According to Eq. 9 this
can be obtained by setting either α = −1 or α = 0. According to the results
thus obtained, in both cases the ﬂows of traﬃc concentrate on the neighbors
of the speciﬁc destination and are directed towards it, thus conﬁrming that
the messages go in the right directions. In the case not taking into account
pheromones we can observe a strong ﬂow from node 1 to node 3 which is justiﬁed
by the presence of a source in node 1. Moreover, the preferred paths used to
reach node 3 are clearly visible. However, as expected, the pheromone eﬀect
Fig. 4. Frequency distribution of the aggregated traﬃc obtained by simulation of the
solutions with (Pher) and without pheromone (No Pher).

Crowdsourcing and Stigmergic Approaches
625
is to redistribute the traﬃc. In such case the ﬂow on congested arcs decreases
indeed, while underexploited arc one increase.
To evaluate the eﬀectiveness of the proposed approach in distributing the
overall traﬃc on the graph, we investigated the distribution of the aggregated
traﬃc (i.e. related to all destinations) for all the arcs. Since the traﬃc is evaluated
as a rate of an exponential process, we can compute the aggregated one just as
the sum of the traﬃcs of each destination.
The resulting frequency or probability mass functions of the traﬃc with and
without pheromone are shown in Fig. 4. It can be observed that the pheromone
improves the distribution of the overall traﬃc intensities. Indeed, without
pheromone, several arcs present a high-intensity traﬃc, especially around the
values 0.2 and 0.55. The distribution thus obtained appears to be bimodal. When
the pheromone is considered, the traﬃc results more evenly distributed, with a
peak around the value 0.6. From these values, we can argue that the pheromone
allows the overall traﬃc to be more evenly distributed than when the pheromone
is not considered.
6
Conclusions
People, crowds and critical masses are becoming more and more powerful, not
only from an abstract point of view related to the opinion they express, but
also in more physical terms, due to their work potential. Indeed, volunteer and
crowd-based approaches are spreading like wild-ﬁre across diﬀerent disciplines
and sciences. Example are crowdsourcing, crowdfunding, geocomputing and vol-
unteer geographic information, just to name a few in diﬀerent areas. A very fertile
ground for new approaches and technologies is computer science and engineering,
where volunteer and crowd-based ones have gained solid roots as in the cases of
crowdsourcing, crowdsearching, crowdsensing. In particular, Mobile Crowdsens-
ing is a very promising approach for involving people, citizen, crowds into sensing
campaigns according to participatory and/or opportunistic schemes. Although
the Mobile Crowdsensing paradigm is quickly rising interests and funds, there
is still untapped potential, as well as unexplored solutions this paradigm may
empower.
This paper tries to partially ﬁll this gap by ﬁrst deﬁning a scenario and identi-
fying some speciﬁc features for a novel opportunistic, distributed, self-organizing
approach, applied to a speciﬁc class of MCS application, tackling local optimiza-
tion problems, in the ITS domain. The solution proposed adapts and extends
an ant colony optimization algorithm to a problem of pathﬁnding and graph
traversal according to a given distance metric. This way a new class of intelli-
gent transportation systems is identiﬁed: the swarm intelligent transportation
systems - SITS - one.

626
S. Distefano et al.
References
1. European Parliament: Directive 2010/40/EU, Brussels (2010)
2. Da Lio, M., Biral, F., Bertolazzi, E., Galvani, M., Bosetti, P., Windridge, D., Saroldi,
A., Tango, F.: Artiﬁcial co-drivers as a universal enabling technology for future intel-
ligent vehicles and transportation systems. IEEE Trans. Int. Transp. Syst. 16(1),
244–263 (2015)
3. Groot, N., De Schutter, B., Hellendoorn, H.: Toward system-optimal routing in
traﬃc networks: a reverse stackelberg game approach. IEEE Trans. Int. Transp.
Syst. 16(1), 29–40 (2015)
4. Du, L., Dao, H.: Information dissemination delay in vehicle-to-vehicle communi-
cation networks in a traﬃc stream. IEEE Trans. Int. Transp. Syst. 16(1), 66–80
(2015)
5. Lee, E., Lee, E.-K., Gerla, M., Oh, S.: Vehicular cloud networking: architecture and
design principles. IEEE Commun. Magaz. 52(2), 148–155 (2014)
6. Lv, Y., Duan, Y., Kang, W., Li, Z., Wang, F.-Y.: Traﬃc ﬂow prediction with big
data: a deep learning approach. IEEE Trans. Int. Transp. Syst. 16, 1–9 (2014)
7. Hamdar, S., Talebpour, A., Dong, J.: Travel time reliability versus safety: a stochas-
tic hazard-based modeling approach. IEEE Trans. Int. Transp. Syst. 16(1), 264–273
(2015)
8. Ganti, R., Ye, F., Lei, H.: Mobile crowdsensing: current state and future challenges.
IEEE Commun. Magaz. 49(11), 32–39 (2011)
9. Dorigo, M., St¨utzle, T.: Ant Colony Optimization. Bradford Company, Scituate
(2004)

Research on Visual Feedback Based on Natural Gesture
Wenjun Hou1,2, Shupeng Zhang1(✉), and Zhiyang Jiang1
1 School of Digital Media and Design Art, Beijing University of Posts and Telecommunications,
Beijing 100876, China
hwj1505@bupt.edu.cn, super_bupt@126.com
2 Network System and Network Culture Key Laboratory of Beijing, Beijing 100876, China
Abstract. Objective To study some problems of visual feedback on user cogni‐
tion and satisfaction in virtual and real interaction with inconsistent input and
output. Method Based on natural gestures, experiments were designed to study
user’s perception of diﬀerent visual feedback and satisfaction in virtual grasping,
with the Ergo LAB physiological auxiliary testing equipment. Conclusion In the
virtual and real interaction visual feedback, the feedback form of visual expres‐
sion is superior to the performance of movement, and the color is the main form.
The feedback form of movement performance is stronger in physiological stim‐
ulation, deformation is the most feedback way of human cognition, which can
increase the sense of immersion. In the interaction involving a variety of visual
feedback, it is suggested giving priority to the classiﬁcation of the same form of
expression.
Keywords: Virtual grasping · Virtual and real interaction · Visual feedback
Variable analysis · Physiological index
1
Introduction
Virtual and real interaction is a new and emerging form of human-computer interaction,
highlighting the virtual reality and augmented reality. With the help of powerful
computing and graphics capabilities of computers, it achieves more intelligent under‐
standing of human order and enables more input modes [1].
Various input modes expand the interaction between virtual and real worlds. In the
virtual and real interaction, the most reasonable and eﬃcient interaction should be
natural gestures. It allows users to abandon the external devices and interact with the
virtual scene.
There are plenty forms of feedback during virtual and real interaction, such as vision,
hearing, touch, space conversion etc. In virtual and real interaction, the content is mainly
based on the three-dimensional scene and model objects, and the input and output of
information are diﬀerent.
The input of visual information accounts for more than 80% of the total intake of
information, and can provide interactive feedback immediately, which has a positive
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 627–637, 2018.
https://doi.org/10.1007/978-3-319-74521-3_65

eﬀect on reducing the user’s cognitive load. Since the feedback comes from the percep‐
tion of vision [2], environment and input gestures, the visual feedback plays an important
role in virtual and real interaction.
2
Related Work
In view of the current research, the study of visual feedback among the virtual and real
interaction was divided into the following three directions: hardware development and
extensions, hardware algorithm based on recognition accuracy and natural gestures
design.
Mores Prachuabrued et al. explored the visual feedback when ﬁngers penetrating
virtual objects during virtual grasping and evaluated the performance (penetration, ﬁnger
release time, accuracy) of several common visual feedbacks. Results showed that
recommend visual feedback is color change [3]. And another experiment showed the
combination of touch and visual feedback is optimal [4].
Faieza Abdul Aziz et al. studied visual feedback mechanism, they found out that
users achieved higher eﬃciency in ﬁnishing assigned tasks with visual feedback. In
addition, the results showed that color changes are more eﬀective than text prompts [5].
Based on previous studies, YingKai designed visual feedback for virtual hand and
grasping objects. She ﬁnally conclued:1. overall performance was better than the local
performance. 2. visual change of objects was more eﬀective than the change of virtual
hand, 3. red color obtained the best feedback [6].
Zhang Wei et al. presented that static color recognition eﬃciency in virtual envi‐
ronment is far higher than in the real world, and suggested using dynamic [7] visual
feedback in the virtual scene.
Previous research had laid a certain foundation for related ﬁelds, but it also exposed
some shortcomings. In the virtual and real interaction visual feedback, it is not enough
only to study the changes of static visual feedback, and in the course of experiment, the
level of the user’s operation is not accurately regulated, so it is necessary to use the
auxiliary information of other channels to authenticate of conclusions under single visual
feedback.
In order to improve the research content in this ﬁeld, on the basis of previous research,
we conducted an experimental study on natural grasping gestures in the virtual reality
interaction with respect to visual feedback. This paper mainly studied the diﬀerence and
validity of visual feedback form of the target object. The feedback form variables include
object color, transparency, ﬂashing, highlight, rotation, shaking, scale, and deformation.
Besides, auxiliary electromyography and heart rate test were applied for evaluation.
Experimental results were analyzed.
3
Experiment Design
First, we deﬁne natural gestures. Natural gestures refer to gestures that do not add any
markings, do not wear any auxiliary equipment, and use bare hand to interact with
628
W. Hou et al.

objects directly. According to Wu and his co-author [8], a gesture includes Three stages,
concluding the following Fig. 1:
Fig. 1. Three stages of the gesture input, OOR means the device “out of range”
We deﬁned the natural grasping gesture; sub-action can be divided into “encounter
object” and “pick up the object”. We introduced the way of visual feedback.
So how do you judge the validity of diﬀerent visual feedback types in virtual reality
interactions? In fact, it is the in process of the grab gesture from “encounter object” to
“pick up the object”. From the “encounter object”, to see the visual feedback, and then
quickly “pick up the object”, the moment of seeing the visual feedback will diﬀer
because of the diﬀerent kinds of visual feedback; this diﬀerence can represent the
diﬀerence of feedback ways, and then we can analyze its eﬀectiveness. On this basis,
we construct the experimental platform, and do the experimental study.
3.1
Design Experiment
The experiment used Unity 5.4 as the platform, C# as the programming language, and
Leap Motion camera to input the gesture coordinates and use grasping gestures to
perform experimental operations and record data.
At the same time, the Ergo LAB physiological detection equipment was used to
detect the EMG and heart rate in the experiment. The physiological indexes of the
subjects were recorded under diﬀerent visual feedback conditions (Fig. 2). The EMG
detector (sEMG) electrode was pasted at the extensor carpi radials brevis muscle of the
right arm [9], which was the main muscle of the activity in the operation of the grasping
gesture.
Research on Visual Feedback Based on Natural Gesture
629

Fig. 2. Based on leap motion system and Ergo LAB, visual feedback - cognitive experiment
In this study, we use controlled-variable single factor inner-group comparison
experiment and inter-group comparison experiment. Inner-group comparison experi‐
ments included the use of Leap Motion to achieve the virtual grab of the ball without
feedback and in eight control variables of feedback, a total of nine groups. Inter-group
comparison experiment included grabbing empty and grabbing solid table tennis, a total
of two groups.
3.2
Control Arguments
Object color, transparency change - For the feedback form in this study, the independent
variable of object color change was red, the self-variable of object transparency change
was alpha equal to 0.4 concluding from the literature [6].
Object ﬂashing - For the feedback form in this study, the independent variable of
object blink control was the red and transparent alternation of the object in this paper [7].
Object highlighting – The paper [10] suggested that in the three-dimensional virtual
scene, the use of highlighted form to interact with object was desirable. So, we designed
highlighting color selection red, display on the edge of the small ball.
Rotation, shaking, and scaling of objects – The author [11] used scaling, translation
and rotation operations in interactive operations of 3D models in virtual reality, so we
chose these three ways in the experiment.
Object deformation - When the virtual ﬁnger is grasping, the places where ﬁngers
and small ball collide will have deformation [12], which control the deformation of the
radius and the intensity in the appropriate range, in order to achieve the deformation of
the feedback form. The interface was shown in Fig. 3.
630
W. Hou et al.

Fig. 3. Visual feedback experimental interface overall (Color ﬁgure online)
3.3
Experiment Process
After pre-tests and screening, 20 participants were selected: 10 boys and 10 girls, aged
from 20 to 25 (mean age 23.4), were all right handed and familiar with computer oper‐
ation. All participants were in good health and no symptoms of physical fatigue such as
massive exercise before the test. All participants were willing to accept the experiment
voluntarily.
Before the start of experiment, the Ergo LAB device will be ﬁxed, and the heart
index detector (HRV) was clamped on the left index ﬁnger (HRV) of the subject, wipe
the right side of the arm extensor parts with alcohol and ﬁxed the electromyography
(sEMG).
The experiment will begin after the detection of heart rate and myoelectric physio‐
logical signal remaining stable. The experiment was divided into 2 groups and 11
experiments. Each process requires 3 s of rest after the participant completes each
grasping, each procedure include 5 times of this process. After each procedure, let the
participant take a short break while saving Ergo LAB physiological data. And so on,
followed by the implementation of 11 experimental process, the data will automatically
be saved in the local ﬁle. After the experiment was completed, the subjects were graded
on the satisfaction of several visual feedback methods, and they were graded from -2,
-1, 1, and 2 by Likert 5 scale.
3.4
Experiment Data
The experimental data included response time ΔT, satisfaction S, mean electromyog‐
raphy YAverage, mean muscle contraction YVariance and mean heart rate AVHR.
Research on Visual Feedback Based on Natural Gesture
631

Reaction time ΔT = T1−T2, which T1 was the time when the subject encounters the
ball to the release of the ball, T2 was the time when the user begin performing the
grasping gesture to the release. Therefore, ΔT could be expressed as the user to see the
feedback to the beginning of the implementation of the time diﬀerence, that is, the reac‐
tion time, as the form of feedback eﬀectiveness of the important indicators. Satisfaction
S was the satisfaction of the user after the completion of the experiment, the evaluation
of the eight variables of the satisfaction score, respectively, -2, -1, 0, 1, 2, post-data
analysis was standardized, all returned to 0 to -1 score.
The physiological signal data were sorted out in the Ergo LAB analysis software,
and the fragments from the signal equalization change to the signal equalization change
were selected and processed for analysis. Analysis of sEMG signal according to the
literature [13] selected these three data indicators commonly used in time domain and
frequency domain analysis. Of this, YAverage represented the average level of the
physiological signal amplitude of the segment, YVariance represented the amplitude
variance of the physiological signal of the fragment. They all obey to the zero mean
Gaussian distribution, proportional to the degree of muscle contraction; AVHR repre‐
sents the mean heart rate of the segment.
4
Experiment Design
4.1
Inner-Group Analysis
After 20 person-times of experiments, a large amount of statistical data was obtained.
After the invalid data was eliminated, the data were consolidated and imported into the
SPSS software for analysis and processing. The ﬁrst test of the ΔT and S data obey
normal distribution, results all meet. Descriptive statistics are then given in Table 1
below.
Table 1. Descriptive statistics of variable response time and satisfaction
Cases
The average
of ΔT
The average
of S
Standard
deviation
Color-feedback
20
.646497
1.30
.1102680
Transparency -feedback
20
.635995
0.250
.1800684
Flashing-feedback
20
.708847
−0.20
.1707118
Highlight-feedback
20
.615964
1.40
.1706103
Rotation-feedback
20
.763828
−0.65
.1851743
Shake-feedback
20
.756642
−0.95
.1803534
Scale-feedback
20
.775068
−0.55
.2113159
Deformation-feedback
20
.738598
0.35
.1518762
None-feedback
20
.960270
.2135176
Eﬀective case number
20
632
W. Hou et al.

It can be seen from the chart that the ﬁrst ΔT without feedback was much larger than
that of the other eight feedback modes, indicating that the operational eﬃciency value
in the form of visual feedback was improved and the visual feedback mode is reasonable
and necessary;
The ΔT of the highlight-feedback was the smallest, and the user satisfactions were
also the highest. The ΔT of these four modes of feedback such as rotation-feedback,
shake-feedback, scale-feedback, and deformation-feedback were too large, and
customer satisfaction were relatively low, the initial description of these the form of
feedback was inappropriate.
In order to test the rationality of the ΔT and S data results, ΔT and S were tested for
paired samples T, and the results showed that, in addition to the transparent and deformed
feedback methods, the other six feedback patterns had a signiﬁcant correlation in the
relevance of the sample sig <0.05, showing a high degree of correlation, which was also
consistent with the description of the statistical feedback time is small and high satis‐
faction Feedback time and satisfaction with low consistency.
In the process of data analysis, we found that the results of diﬀerent variables showed
a certain consistency, such as color, transparency, highlight the feedback method are
better. And rotation, scaling, shaking, deformation of these feedback methods was rela‐
tively poor, but the results were also close to the guess may also be a certain degree of
relevance. Therefore, cluster analysis was performed and the clustering results were
shown in Fig. 4.
Fig. 4. Systematic clustering result pedigree
Research on Visual Feedback Based on Natural Gesture
633

As you can see, shake-feedback, scale-feedback, deformation-feedback, and rota‐
tion-feedback are automatically clustered together, and the ﬂashing-feedback, transpar‐
ency-feedback, highlight-feedback, and color-feedback came together. The eight vari‐
ables could be classiﬁed into two types, summed up the visual display and movement
performance.
4.2
Inter-group Analysis
Considering that single visual input information could be deceptive, it was necessary to
continue to analyze and validate from the perspective of physiological indicators, refer
to Figs. 5 and 6.
Fig. 5. Diagram of sEMG and HRV signals
Fig. 6. sEMG in diﬀerent experimental grasping processes, empty-grasping, grasping table-
tennis, none-feedback and highlight-feedback
The data of the physiological indexes after the errata was sorted out, and the mean
value of the case variables was output. At the same time, the satisfaction score was
normalized and the overall data results were presented in Table 2.
From the overall analysis, the physiological index data of diﬀerent feedback forms
was diﬀerent, we could guess that there existence a certain psychological model between
the visual and tactile perception.
634
W. Hou et al.

Table 2. Experimental data cross table
∆T
S
YAverage
YVariance
AVHR
Empty-grasping
3.7294
1.5835
71.4375
Grasping table-tennis
3.7106
1.3006
70.4
Color-feedback
0.5194
0.825
3.7456
1.6041
73.1875
transparency-feedback
0.5851
0.5625
3.8839
3.2312
75.0667
Flashing-feedback
0.6705
0.45
3.6828
2.0161
76.7143
Highlight-feedback
0.563
0.85
3.9522
1.5965
75.3571
Rotation-feedback
0.7157
0.3375
3.8833
1.995
71.1875
Shake-feedback
0.7333
0.2625
3.9244
1.4994
75.1429
Scale-feedback
0.7153
0.3625
4.0394
1.4412
71.4667
Deformation-feedback
0.7078
0.5875
4.6644
3.8787
71.7143
None-feedback
0.8869
3.665
1.8639
66.2143
The average EMG and reaction time of visual feedback was positively related to the
reaction time, the greater the average EMG value is higher, but the eﬀectiveness of the
feedback form is small;
The heart rate associated with visual feedback, visual feedback was more intense,
the heart rate is higher, but the eﬀectiveness is not high, the use of context information,
such as presentation, warning eﬀect, interaction eﬀects etc.;
The muscle fatigue and cognition, more familiar and more acceptable form of feed‐
back of the lower degree of muscle fatigue, exercise performance was better than the
intuitive, people were more likely to perceive, but the eﬀect was not good.
5
Conclusion
Through inner-group analysis and inter-group analysis, the data were analyzed by hori‐
zontal and vertical, respectively, summarized as follows:
(1) In general, the feedback form of visual expression was superior to the performance
of movement;
(2) The more obvious the physiological index, the strongest the stimulus. The color
feedback and highlight feedback performance were the best, and also had the
highest subjective satisfaction;
(3) The ﬂashing feedback was the most intense stimulus, but it was not applicable in
interactive feedback, and subjective satisfaction was low;
(4) The deformation feedback was the most consistent cognitive approach to grasp
gestures, and was recommended in the virtual scene collision detection;
(5) The scale feedback was not the best in cognition, but it could reduce the user’s
operation fatigue and reduce the user’s learning process in interactive operation;
In summary, the form of visual feedback was suggested as follows:
In the virtual and real interaction, it was best to use the visual expression of visual
feedback form, mainly color, local performance and overall performance should be
considered;
Research on Visual Feedback Based on Natural Gesture
635

In scene content design, the dynamic visual feedback was more easily perceived than
the static;
In the multimodal input and output model, visual feedback of motor form was better
and more in line with human cognition;
In the multi-channel information input and output mode, the visual feedback of motor
expression form is better and more consistent with human cognition;
When there were a lot of visual feedback forms in the interaction of virtual and real
interaction, priority should be given to using a consistent and consistent feedback
method to select the correct form of visual expression and the performance of movement;
6
Summary and Future Work
In this paper, we studied the eﬀects of diﬀerent visual feedback forms on cognitive and
user subjective satisfaction under the natural gesture. At the same time, we used phys‐
iological detection equipment to detect the physiological characteristics of the experi‐
ment process and concluded that visual feedback aﬀects user reaction and user experi‐
ence to some extent, and found that there was a relationship between physical charac‐
teristics and visual feedback.
In this paper, the study of the type of visual feedback was still at the initial stage.
The variable level of scaling and deformation feedback was derived from the actual
project, the selected deformation parameters needed further study. In addition, visual
feedback had more dimensions and angles of variable types that need further study.
References
1. Zhao, Q.: Virtual reality review. Sci. China Ser. F Inf. Sci. 39(1), 2–46 (2009)
2. Zhao, Z.: Virtual reality based on human-computer interaction. J. Wuhan Univ. Sci. Technol.
(2007)
3. Prachyabrued, M., Borst, C.W.: Visual feedback for virtual grasping. In: IEEE Symposium
on 3D User Interfaces (3DUI) (2014)
4. Prachyabrued, M., Borst, C.W.: Virtual grasp release method and evaluation. Int. J. Hum
Comput Stud. 70(11), 828–848 (2012)
5. Prachyabrued, M., Borst, C.W.: Visual interpenetration tradeoﬀs in whole-hand virtual
grasping. In: Proceedings of IEEE 3DUI, pp. 39–42 (2012)
6. Ying, K.: Visual feedback for natural interaction study. Beijing University of Posts and
Telecommunications (2016)
7. Meng, F.-X., Zhang, W.: Way-ﬁnding during a ﬁre emergency: an experimental study in a
virtual environment, vol. 57(6), June (2014). Taylor & Francis, Oxon. ISSN:0014-0139
8. Wu, M., Shen, C., Ryall, C., Balakrishnan, R.: Gesture registration, relaxation, and reuse for
multipoint direct-touch surfaces. In: Proceedings of the First IEEE International Workshop
on Horizontal Interactive Human-Computer Systems (TABLETOP 06), pp. 183–190 (2006)
9. Lin, Y., Zhang, F.: Carpi brevis tendon twin countershaft one case. Chin. J. Clin. Anat. 23(6),
582 (2005)
10. Chen, D.: Interactive real-time rendering of three-dimensional scene style of lead color.
Nanjing University (2016)
636
W. Hou et al.

11. Li, S.-S.: Interactive 3D model of operational studies. Dalian University of Technology
(2009)
12. Hou, W., Guo, Y., Li, T.-M., et al.: Research on three - dimensional object deformation
technology for natural interaction
13. Wang, J.: Research progress on signal analysis and its application of sEMG. J. Phys. Educ.
20(4), 56–60 (2000)
Research on Visual Feedback Based on Natural Gesture
637

Information Security Technology and Application
in Logistics Traceability System of Aquatic Products
Based on QR Code
Qiaohong Zu and Rui Chen
(✉)
School of Logistics Engineering, Wuhan University of Technology,
Wuhan 430063, People’s Republic of China
780225537@qq.com
Abstract. In view of the security hidden danger of the business data in the
logistics traceability system of aquatic product, the design of the information
security strategy is carried out from the two angles, which are data collection and
transmission. Firstly, the aquatic traceability code is designed. Then the data
encryption algorithm is studied, and the data security transmission of information
security protection system is built. In the intelligent collection of traceability data,
QR Code is used to express the logistics trace data, then uses the improved data
encryption algorithm to encryption storage about QR Code. In the transmission
of traceability data, the HTTPS protocol is used to construct the client-server
transmission encryption channel to ensure the integrity of the data. Finally those
information security technologies about Logistics traceability data are used in a
logistics traceability system of aquatic product, and the eﬀectiveness of RC4-RSA
hybrid encryption algorithm is veriﬁed. And the integrated applications enhance
the system’s information security.
Keywords: Logistics traceability system of aquatic product · Data security
Data encryption technology · RC4-RSA hybrid algorithm
1
Introduction
In the logistics system, the importance of information security is increasing gradually
[1]. Those technologies of information security, two-dimensional information encryp‐
tion based on random shift [2], encoding encryption key of GPS location information
[3], secure data aggregation based on homomorphic encryption scheme [4], encryption
the key of AES algorithm with ECC [5], sustained data protection mechanism based on
virtual storage technology and others have been born on. For logistics traceability system
of aquatic products, the data of aquaculture, processing, transportation and sales are
stored in the bar codes in the form of encoding. With the development of network tech‐
nology, the encryption algorithm must be upgraded and the data storage and backup
must be strengthen, to ensure the information security during the logistics traceability
process.
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 638–647, 2018.
https://doi.org/10.1007/978-3-319-74521-3_66

2
Risks Analysis on Information Security of Aquatic Logistics
Traceability System
The logistics traceability system of aquatic products is a distributed processing envi‐
ronment which is a multi-module, multi-role, multi-service information system. With
the increasing of logistics business, the security risks increase. Analyzing the hidden
security about the logistics traceability system of aquatic products from four layer of the
acquisition control, data, application and user, the security problems of logistics business
data mainly exist in the two aspects of data acquisition and transmission.
In order to improve the data security of the aquatic products traceability system, and
ensure the integrity and availability of information, the author put forward a solution
from the point of technology security combining with the hidden data security in the
collection and transmission process.
In the logistics traceability system of aquatic products, data collection is mainly
based on bar code identiﬁcation technology, wireless data transmission and wireless tag
(RFID) technology [6]. Among them, the bar code technology is more widely used in
logistics system than RFID technology because of its low using cost. The two-dimen‐
sional code-QR Code is widely used in the current aquatic logistics traceability system.
Therefore, this paper only focuses on the encryption of QR Code as a representative
research object in the data collection section. Through the encryption to QR Code, avoid
the direct exposure of information carried by the two-dimensional code.
3
Research on Two-Dimensional Encoding and Encryption About
Traceability Data of Aquatic Logistics
3.1
Logistics Data Collection Code Generation Encryption
3.1.1
QR Code Encoding Encryption Design
(1) QR Code encryption location selection
Through the analysis on the encoding process of the QR Code, data can be encrypted
at all levels before the ﬁnal information is constructed, as shown in the Fig. 1.
Analysis on diﬀerent positions, encryption after the data encoding, which is related
to the complex process of coding generation, cannot guarantee that the original structure
is not aﬀect by the encrypted data. The poor ﬂexibility aﬀect the generation of the two-
dimensional code. Wrong coding function cannot be guaranteed. So, the paper takes the
encryption at position 1 to prevent information from being tampered with criminals.
(2) Encryption algorithm selection
Both the security of the information and the complexity of encoding and decoding
should be took into account in a QR Code encryption strategy. Table 1 compares several
common cryptographic algorithms in some important reference dimensions.
Information Security Technology and Application in Logistics Traceability System
639

Table 1. Comparison of common encryption algorithms
Encryption
algorithm
Category
Operation speed
Security
Resource
consumption
DES
Symmetric
Faster
Low
Middle
AES
Symmetric
Fast
High
Low
RC4
Symmetric
(Streaming
encryption)
l0 times faster
than DES
High
Low
RSA
Asymmetry
Slow
High
High
DSA
Asymmetry
Slow
High
Digital signatures
only
Symmetric encryption algorithms suit for a large amount of data encryption because
of its low complexity of decryption and fast encryption speed, but they also have key
shortcomings. Asymmetric algorithms have the feature for the high security, simple
public key public key management, but relatively slow encryption speed.
QR Code using in the aquatic products logistics traceability, information read a large
amount of encrypted data more, speed and security of encryption to protect data, using
symmetric encryption algorithm and asymmetric encryption algorithm combination.
Through the comparison of common algorithms in 3–1, this paper uses RC4 algorithm
and RSA algorithm for QR Code hybrid, the original plaintext data is encrypted by RC4
algorithm, improve the speed of encryption and decryption of the RC4 algorithm, key
is encrypted using the RSA algorithm, to ensure the security of the key, solve the problem
of key distribution and management, further to improve the security of encryption.
(3) Encryption scheme for QR Code
Use RC4 and RSA algorithms in the encoding link of QR Code, to encrypt the data.
The QR Code encryption process is shown in Fig. 2.
Fig. 1. QR Code multiple encryption
640
Q. Zu and R. Chen

Fig. 2. QR Code encryption process
The QR Code decoding process is a reverse process of encoding, decryption process
of the decryption algorithm is shown in Fig. 3.
Fig. 3. QR Code decryption process
Through the mixed encryption with RC4 and RSA algorithm, the speed and security
are improved while the security requirements of QR Code encryption and decryption
are ensured. When QR Code is used in the aquatic traceability system, the aquatic enter‐
prise uses the RSA algorithm to generate the public key and the private key, in which
the public key can be made public, and the private key is reserved inside the enterprise
for encryption. In the QR Code generating process, according to the encrypted data,
enterprise distribute RC4 key or randomly generate, guarantee diﬀerent QR Code using
diﬀerent encryption keys, dynamically generated QR Code encryption, and improve
system security. The logistics staﬀ read or write information by professional equipment
with QR Code decryption algorithm, input the information to aquatic product traceability
system, and data acquisition and encryption to achieve traceability to logistics supply
chain, enhance the security of information.
3.1.2
Chaos Improvement of RC4 Algorithm
The QR Code encryption program for RC4 algorithm encryption is to take the XOR
(exclusive OR) operation, once the sub-password is repeated, it is easy to be cracked.
In addition, the randomness and ergodicity of the algorithm is not good enough, and it
Information Security Technology and Application in Logistics Traceability System
641

is a low-diﬀusion state. Therefore, the RC4 algorithm has some weak keys and is
vulnerable to attack.
In order to improve the security of RC4 algorithm in QR code encryption, the
Logistic chaotic map is used to improve the randomness of pseudo-random sub-code
generation, which make a better diﬀusion eﬀect and reduce the occurrence of weak key.
The so-called chaos is the disorder and random phenomenon in a deterministic system,
the chaotic sequence generated by the chaotic system has unpredictability [7].
Logistic one-dimensional mapping which is widely used is a chaos mapping of
mathematical form, the mathematical expression is:
Xi+1 = μXi
(1 −Xi
)
(2.1)
The ranges of values for the initialization parameter μ and X0 are:
0 < 𝜇< 4, 0 < X0 < 1.
When 3.5 < 𝜇< 4 the system is in a discrete state, that is, reaching the chaotic state.
The value Xi have the characteristics of non-convergence and non-periodic, the value
can traverse the whole interval of (0, 1], the sequence generated outside the interval will
converge to a speciﬁc value. The closer the μ is to 4, the better the proliferation.
After a large number of experiments, the parameters used in this scheme are:
X0 = 0.8755, μ = 3.99919. The chaotic state reached at this time is shown in Fig. 4.
Fig. 4. The chaotic mapping when X0 = 0.8755, 𝜇= 3.99919
After the chaotic map, the RC4 algorithm is improved as follows:
①Calculate the chaotic value based on the setting initial value of Logistic map.
y = μX0
(
1 −X0
)
(2.2)
②Generate the random key sequence by substituting the chaotic value and iteration in
the RC4 key scheduling algorithm.
642
Q. Zu and R. Chen

y=u*x(1-x);
x=y;
j=(j+S[i]+T[i]+ y*256)mod256;
swap(s[i],s[j]);
③Recursive execution after adding chaotic maps during the process of generation and
replacement of Pseudo-random sub-code.
i=(i+1) mod 256;
y=u*x(1-x);
x=y;
j=(j+S[i]+y*256)mod 256;
swap(S[i],S[j]);
t=(S[i]+S[j])mod 256;
key[r]=S[t];
Do the XOR operation for the sequence generated by the third step with the plaintext,
until the algorithm encryption step is completed. And the randomness of the random
code generation is greatly improved after using the Logistic mapping.
The encryption security of QR Code information is improved by RC4 based on
chaotic mapping and RSA algorithm, which are used to encrypt and decrypt the QR
Code. It can eﬀectively prevent the forgery and tampering of logistics business data and
ensure the security of data in data acquisition and generation rooting.
4
Integrated Application of Aquatic Logistics Traceability Data
Security Technology
In this example, the aquatic logistics traceability system is based on the background of
an aquatic product processing enterprises in Hubei Province, and it is constructed
according to the actual needs of the enterprises. The two-dimensional bar code is used
to collect the information intelligently and realize the resource sharing of aquatic prod‐
ucts. In the event of aquatic products quality problems, a quick inquiry to trace the
relevant breeding distribution information can found where the problem is through the
information contained in the two-dimensional code.
Information Security Technology and Application in Logistics Traceability System
643

From the logistics business data security considerations, use QR Code data encryp‐
tion to protect the security of data in the supply chain information transmission, and the
HTTPS protocol to ensure the safety of data transmission channel.
4.1
Application of QR Code Encryption Technology in Aquatic Logistics
Traceability System
4.1.1
Coding Design for QR Code in Aquatic Traceability
The core of aquatic logistics traceability is the construction of unique traceability code.
Through the traceability coding rules, QR Code is used as label carrier to cover aquatic
raw materials, breeding, processing and distribution process.
Combine the aquatic enterprises, product categories, production and breeding infor‐
mation organically to construct the only aquatic traceability code by using sub-rules.
The aquaculture tracing a total of 24, is as shown in Fig. 5.
Fig. 5. Aquatic traceability code 24 bit structure
A unique identiﬁcation of aquatic traceability code must be constructed, in order to
truly achieve the retrospective query of aquatic logistics, processing, distribution and
other information.
4.1.2
Encryption Implementation of Aquatic Traceability QR Code
QR Code stores the processing and distribution information of aquatic products. If the
information is directly exposed to the two-dimensional code, it is easy to cause the
information to be tampered with, so the QR Code information needs to be encrypted
when printing. According to the distribution processing part of the trace code to generate
encrypted QR Code.
Through the processing and distribution tracking code “123401020300216090
800087”, the processing phase of the relevant information can be queried. According to
the pre-assigned RC4 algorithm key “txttest” click to encrypted QR code can be gener‐
ated by clicking to the pre-assigned RC4 algorithm key “txttest”. During the QR Code
encryption process, the aquaculture enterprise retains the RSA unique private key for
encrypting the RC4 key to make an intermediate key. The staﬀ will posted the generated
QR Code on the corresponding aquatic packaging to make user-friendly query.
When the aquatic products transported to the processing site, the staﬀ get the aquatic
product details through scanning the QR Code by the built-in decryption program PDA/
mobile terminal. The built-in the RSA public key of aquatic enterprise is opening in the
handheld terminal program, to convenient for staﬀ or consumers to scan and decrypt.
Using the RSA public key in the decryption program to decrypt the RC4 algorithm
644
Q. Zu and R. Chen

intermediate key formed in the encryption process, and according to the decrypted RC4
key to obtain the original of the processing information. Using of ordinary scanning
software without decryption procedures and RSA public key, can just access to a mean‐
ingless garbled.
The eﬀect before and after the QR Code encryption is shown in Fig. 6.
Fig. 6. QR Code encryption eﬀect diagram
4.2
Result Analysis
4.2.1
Stochastic Analysis of the Improved RC4 Algorithm
For the improvement of RC4 algorithm in the QR Code generation encryption process,
this paper adopts frequency test and run length test to verify the random performance
of the improved algorithm.
The measure of randomness is P-Value, and statistic X obeyed χ ^ 2 (n) distribution,
signiﬁcance level α ε [0.001, 0.01]. When P-Value ≥ α, then the sequence is random,
which take α = 0.01.
(1) Frequency Test
The frequency test is to test the proportion of 1 and 0 in the entire random sequence,
and whether the two are close.
The frequency test formulas are:
P −Value = erfc
(
x
√
2
)
, x = ||Sn||∕
√
n
(3.1)
erfc(x) =
2
√
π ∫
∞
x
e−u2dμ
(3.2)
erfc(x) is the complementary error function, and Sn is the result of the addition of the
random sequence to −1 and 1. By running the RC4 algorithm in MATLAB to obtain
the random sequence, and then conversion and addition to get Sn, the value of Sn into
Information Security Technology and Application in Logistics Traceability System
645

the formula, get P −Value, as shown in Fig. 7. It can be seen from the ﬁgure
P −Value > 0.01, in line with the requirements of the ideal random sequence.
Fig. 7. Frequency test
Fig. 8. Run test
(2) Run Test
Run test is a coherence test, the run is a two-part total, in the RC4 randomness test
it is the uninterrupted sub-sequence constituted by the same bit sequence. The purpose
of the run test is to determine whether the number of runs of 0 and 1 is consistent with
the random sequence (Fig. 8).
The run test formula is:
P −Value = erfc(x), x =
||Vn(obs) −2nπ(1 −π)||
2
√
2nπ(1 −π)
(3.3)
Vn(obs) represents the sum of all 0 and 1 values in the random sequence of the RC4
algorithm, and n is its length. Replace the relevant parameters in MATLAB, to calculate
P −Value. As can be seen from Figs. 3 and 4, P −Value > 0.01.
Through the frequency test and run test, the P −Value obtained is greater than the
NIST speciﬁed 0.01, in line with the requirements of the ideal random sequence.
5
Conclusion
Under the background of supply chain logistics system, this paper analyzes the hidden
dangers of aquatic logistics traceability system in business data, and constructs data
acquisition and coding encryption. For the RC4 algorithm based on chaotic mapping is
used in QR code encryption process, the author made the randomness test, and the test
results show that the randomness is in accordance with NIST.
Through the QR Code encryption technology, using diﬀerent RC4 key, the dynamic
encryption of aquatic supply chain traceability can be achieved, fully guarantee the
uniqueness of aquatic logistics data in the whole supply chain system, and avoid the
forgery of others, improve the safety of aquatic system information from the root.
646
Q. Zu and R. Chen

But the study only QR Code as a representative of the logistics information carrier
research, Future research can also expand the data acquisition information carrier object,
from the grid intrusion detection, trusted computing point of view, more adequate
protection of the logistics system information security.
Acknowledgment. This paper was supported by the project in the Hubei Science and Technology
Pillar Program (No. 2015BKA222).
References
1. Li, J.: The security challenge of network information reﬂected by “Prism” and its strategic
thinking. Inf. Theory Practice 37(4), 48–52 (2014)
2. Cheng, D.S., Wu, K.K., Liu, Z.Y.: Image encryption algorithm based on chaotic pseudorandom
match shift. Inf. Commun. 8, 7–9 (2016)
3. Prasad, R.P., Sudha, K.R., Sanyasi Naidu, P.: Information system with interlacing, key
dependent permutation and rotation. Int. J. Comput. Netw. Secur. 2(5), 86 (2010)
4. Souﬁene, B.O., Abdullah, A.B., Abdelbasset, T.: Conﬁdentiality and integrity for data
aggregation in WSN using homomorphic encryption. Wirel. Pers. Commun. 80(2), 867–889
(2015)
5. Liao, C.Z., Xu, J.W.: Research on AES and ECC mixed encryption algorithm. Softw. Guide
15(11), 63–64 (2016)
6. Zhao, Q., Peng, H., Hu, B., Liu, Q., Liu, L., Qi, Y., Li, L.: Improving individual identiﬁcation
in security check with an EEG based biometric solution. In: Yao, Y., Sun, R., Poggio, T., Liu,
J., Zhong, N., Huang, J. (eds.) BI 2010. LNCS (LNAI), vol. 6334, pp. 145–155. Springer,
Heidelberg (2010). https://doi.org/10.1007/978-3-642-15314-3_14
7. Niu, C., Yang, Y.J., Mao, X.Q.: Protection mechanism of continuous data based on virtual
storage technology. Comput. Eng. Des. 34(4), 1207–1211 (2013)
Information Security Technology and Application in Logistics Traceability System
647

Quantiﬁed Self: Big Data Analysis Platform for Human
Factor Eﬃcacy Evaluation of Complex System
Chaoqiang Li1, Wenjun Hou2,3(✉), Xiaoling Chen2, and Hao Li2
1 China Electronic Science Research Institute, Beijing, China
2 School of Digital Media and Design Arts, Beijing University of Post and Telecommunications,
Beijing  100876,  China
hou1505@163.com
3 Beijing Key Laboratory of Network and Network Culture, Beijing University of Post
and Telecommunications, Beijing  100876,  China
Abstract. By analyzing the factors aﬀecting the airborne mission system, this
paper applied the method of Quantiﬁed Self to the evaluation of human eﬀec‐
tiveness in the military airborne mission system. According to the depth of inter‐
action between people and information, we divide the information circumstance
into four aspects including individual, equipment, network and environment.
Then we construct a complete individual Quantiﬁed Self information interaction
system by collecting physiological data, cognitive data, behavioral data and envi‐
ronmental data. Finally, the functional architecture and composition of the ergo‐
nomic evaluation platform are given in combination with the airborne mission
system.
Keywords: Quantiﬁed Self · Complex information system · Big data
1
Introduction
With the development of information technology such as the internet of things, cloud
computing, mobile Internet, “Natural Interaction” and “Intelligent Decision” have
become important concepts in various information system. Developing of the informa‐
tion technology also lead to the explosive growth of data, which has had a profound
eﬀect and even gradually change the original knowledge production mode and cognitive
framework. Big data analysis has become an integral method in many ﬁelds, and one of
the big data analysis trend is Quantiﬁed Self (Fig. 1).
The airborne cabin is an important campaign environment in high-tech war. The
working space of the cabin is airtight and narrow, and the operation environment is
complex. Noise, vibration, electromagnetic ﬁled will aﬀect the working condition and
operation ability of operators to varying degrees, which will aﬀect the combat eﬀec‐
tiveness. So, the main problems we have to face are:
(1) Large amount of data: With the development of sensors, the amount of data will
become larger, the types of data will become more diverse and the update speed of
data will become faster.
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 648–653, 2018.
https://doi.org/10.1007/978-3-319-74521-3_67

Fig. 1. Conception of future combat cabin
(2) High complexity of tasks: There are many kinds of tasks, such as searching,
tracking, and monitoring. The information capacity of these tasks interface is enor‐
mous, and the structural relationship of these tasks is complicated, and these tasks
have the characteristic of uncertain and diﬃcult to predict, which is easy to cause
the disorder of cognitive and aﬀect the judgement of the operator.
(3) Change of the working mode: The working mode of the operators is mainly
converted from operation to surveillance and decision making, which results in a
sharp increase in imbalances between the human cognitive and the interface infor‐
mation encoding [2].
The information warfare requires both commanders and operators to perform tasks
accurately and in real-time. It is a key link for improving the individual performance
and the team decision-making ability to establish a real-time and high-precision state
awareness model for combatants, which can detect the cognitive state of the commanders
and soldier combatant states in real time and adjust them in a timely manner. At the
same time, the information warfare also requires more dominant, autonomy and intel‐
ligent. Building a decision support system that provides operators with inferences about
current and upcoming behavior and assigning tasks autonomously between operators
and system.
2
Related Work
Quantifying self, as one of the big data analysis methods, can discover valuable infor‐
mation and implicit relationships in human-computer-environment system by analyzing
various data associated with human activities, which can help to improve current states.
The emergence and development of the concept of “quantifying self” was only a few
years ago and mainly in the ﬁeld of health. It is used to track and record individual
behaviors such as sports, sleep, diet, mood by using multiple device such as computers,
portable sensors and smart phone. Then the collected data were used to discover valuable
information in ﬁtness, daily physiology and diseases treatment which can be used to
improve the people’s living condition. With the development of micro sensors and
Quantiﬁed Self
649

intelligent mobile technology, quantifying self will become an important method for
ergonomics research, information visualization and human-computer interaction
research.
At present, western countries led by the United States are in absolute leading position
in the research ﬁeld of complex systems. The key technology program of United States
Department of Defense not only lists human-computer interaction interfaces as an
important part of software technology development, but also specially adds human-
computer system interface which is juxtaposed with software technology. A new project
of the DARPA hopes to improve the cognitive ability by changing the information
display mode, which can improve operators’ eﬃciency, and the cognitive ability is
reﬂected by the cognitive enhancement. The US Navy Research institute used eye
tracking technology to solve problems in the ship pilot train [6]. The following work
had been done:
(1) The researchers organized a large number of experienced crew and ordinary
students to simulate operations such as navigation, berthing and emergency avoid‐
ance. And the researchers collected eye movements, behavioral data and some
electrophysiological data from them throughout the missions.
(2) The researchers identiﬁed diﬀerent eye movements patterns between experienced
crew and ordinary students by detailed analyzing above data, and established a
quantitative model of eye movements pattern.
(3) The model is applied to training students. By monitoring the eye movement data
of students, the system can automatically calculate whether the students deviate
from the correct mode of operation and give timely feedback according to the
model.
At present, the system has been applied to the simulation training of American Navy
drivers, and has achieved good results. The study of human eﬃcacy has entered a new
stage in China, too. Quantitative research, as an important method to study human
eﬃciency and user experience, has been carried out in many ﬁelds such as ships, tanks,
aircraft driving and civil aviation control. User experience is also attached great impor‐
tance in the development of civil information systems such as the Internet. A large
number of quantitative research has been carried out. For example, Wen-jun Hou and
Xiao-yu Gao of the Beijing University of Posts and Telecommunications studied a
method based on measurement of pupil satisfaction. [4] This paper analyzed the rela‐
tionship between the pupil size and the user satisfaction in usability tests. And they
discovered that there is a linear signiﬁcant negative correlation between the user satis‐
faction and user’s pupil diameter standard deviation rate, and build a model to describe
this relationship.
3
Classiﬁcation of Quantiﬁed Self Data
According to the depth of interaction between people and information, we divide the
information circumstance into four aspects including individual, equipment, network
and environment [3]. Then we construct a complete individual Quantiﬁed Self
650
C. Li et al.

information interaction system by collecting physiological data, cognitive data, behav‐
ioral data and environmental data [3].
The physiological data include height, weight, blood oxygen, blood pressure, heart
rate, etc. These data reﬂect the health condition of the operator. The harsh cabin work
environment, limited space, noise, vibration, electromagnetic ﬁelds have a serious
impact on physical and mental health of operators. The narrow space makes the operator
stay in a ﬁxed position for a long time, which will lead to blood in the legs, causing
swelling, stiﬀness and discomfort. Operators not only have to face complex and diverse
information, but also perform complex tasks, which can easily lead to fatigue. According
to the corresponding physiological indexes, work intensity are reasonably arranged so
as to achieve the best operation safety and performance.
The cognitive load caused by the mental energy consumption in the process of infor‐
mation processing has become an important indicator of the reliability evaluation of
human-computer interaction system. Many researchers have realized that the cognitive
load is a very serious threat for job performance and operational safety. Facing the large
screen display and the global visual interface of beyond sight distance, in order to obtain
the situation of the whole system, the operator must enter the interface of diﬀerent func‐
tions through the interface management task to master all the information needed. When
cognitive load is overloaded, the operator cannot successfully complete the task due to
the insuﬃcient supply of cognitive resources, which leads to decreased accuracy,
prolonged reaction time and eve accidents. However, the eﬀective control of cognitive
load does not mean to blindly reduce the cognitive load. Because when the cognitive
load is too low, especially when in a monotonous boring situation, the operator is likely
to become unresponsive and lead to more errors.
The environmental data, such as temperature and humidity, will have a great impact
on the operators. For example, if under insuﬃcient illumination condition, operators
need repeated identiﬁcation. The vision continues to decline, cause eye fatigue and even
systemic fatigue. Human beings have diﬀerent behavioral responses in human-computer
interaction in diﬀerent environments. The environment aﬀects not only people’s
working ability, and also the performance and reliability of the machine. On the other
hand, people and machines also aﬀect the state of the environment. Therefore, the envi‐
ronmental factors are no longer excluded as a passive interference factor outside the
system, but as a positive active factor into the system, and become an important part of
the system research.
Behavior is the process of conscious decision making. Behavior is inﬂuenced by
many factors, such as personal habits, emotions, brain state, and so on. Under long-term
training, the operators’ proﬁciency will gradually increase along with the eﬃciency and
speed. During the process, they form individual operation habits combine with their
inherent personalities. It’s necessary to meet the common habits of people to improve
the speed and also need to compatible with the diﬀerences of habits to reduce the diﬃ‐
culty of operation. At the same time, the high eﬃciency behavior pattern is certainly
need to be promoted, and ineﬃcient behavior patterns should be avoided by the operator.
Quantiﬁed Self
651

4
The Ergonomic Evaluation Platform
We build up a scientiﬁc assessment of environmental ergonomics based on large data
mining, machine learning and artiﬁcial intelligence technology to solve the problems of
small data in small world. Through the ergonomic evaluation platform, we apply the
methods of Quantiﬁed Self to collect operators’ overall data which is based on objective
data and supplemented by subjective data. Through the machine learning, a compre‐
hensive multidimensional assessment model of the complex system is proposed with
the use of continuous accumulation of data and the mediated by working load (physio‐
logical load and cognitive load) (Fig. 2).
Fig. 2. Ergonomic evaluation platform
The system combines eye movement with motion capture and physiological synchro‐
nization technology. It uses wearable data acquisition technology and wireless data
transmission technology to let operators can completely free from any interference when
collecting data. Human-environment synchronization platform is capable of: Recording,
tracking and analyzing real-time psychological and physiological changes of users, such
as ECG, skin electricity and other indicators; analyzing people’s attention, cognitive
load, fatigue; Synchronizing real-time records involved human behaviors, equipment
running information and the environmental information, such as temperature and
humidity.
The system collects and analyzes the causal relationship between the changes of
human-computer-environment interaction, and realizes real-time simultaneous
recording and analysis of human-environment data. Therefore, it accomplishes multi‐
dimensional data visualization analysis. At the same time, for the eﬀective data extrac‐
tion and visualization presentation, we improve the human-computer interaction inter‐
face and system process to build an eﬀective monitoring and evaluation methods.
According to the diﬀerent design for the information display interface of the aircraft
system, the indicators such as the physiology and eye movement of the operator are
analyzed. Meanwhile, the time, the number of occasions and the path of the operation
task are explored and analyzed. We found out the good and bad of the operation interface
and tried to improve the interface to become a standard of intelligent human-computer
interface.
652
C. Li et al.

5
Conclusion
Quantiﬁed Self combining with big data technology can be applied to the evaluation of
complex information system, which greatly promotes the development of China’s mili‐
tary strategy. It not only can improve the airborne information system interface and the
operation eﬃciency but also provides more possibilities in real-time monitoring, combat
multi-distributed data acquisition and other aspects. Furthermore, suﬃcient and compre‐
hensive data contribute to display interface design, ﬂexible layout and adaptive recon‐
ﬁguration. The system makes an eﬀort on establishing a real-time and high-precision
model of combat personnel state perception based on physiological data and neural
calculation real-time analysis. Moreover, it increases the accuracy of automatic online
analysis based on deep learning training.
References
1. What will Royal Navy warships look like in 2050? Jonathan Beale Defense correspondent,
BBC News, 31 August 2015
2. Jing, L.: Encoding information of human-computer interface for equilibrium of time pressure.
J. Comput. Aided Des. Comput. Graphics 25(7), 1022 (2013)
3. Wang, H.: In the information age, human-computer interaction design of complex system are
very important. China Aviation News (2016)
4. Hou, W.: Customer satisfaction evaluation model based on pupil size changes. Space Med.
Med. Eng. 05, 001 (2013)
Quantiﬁed Self
653

Application of Speech Recognition Technology in Logistics
Selection System
Tianzheng Fu1(✉) and Bin Sun2
1 Wuhan Second High School, Wuhan  400010,  China
22836099@qq.com
2 Zhen Kun Hang Industrial Supermarket, Shanghai  201716,  China
Abstract. Speech recognition technology is designed to be used in the logistics
picking system. Picking personnel start picking receive voice command from
system, he feedbacks “check code” by voice, ﬁnish man-machine dialogue,
choose terminal equipment to capture the “checksum” for identiﬁcation. After
successful veriﬁcation, complete the sorting operation. When speech recognition
technology is used in logistics picking system, walking time can be reduced, the
work ﬂow will simpliﬁed and the data transmission accuracy and picking eﬃ‐
ciency will be improved, it is conducive to promote logistics service level,
enhance the economic eﬃciency of logistics enterprises and customer satisfac‐
tion.
Keywords: Speech recognition technology · Logistics picking system
Check code · Picking eﬃciency
1
Introduction
Order picking based on product barcode is widely adopted in automatic logistics and
warehouse management systems. However, barcode order picking presents several
disadvantages when the order number increases signiﬁcantly. Firstly, as the number of
order increases, system capacity does not scale-up easily, leading to delay in order
delivery. Secondly, currently barcode order picking systems may still rely heavily on
human labors. As a result, the entire system becomes error-prone and inconsistent in
system eﬃciency. In the present paper, we propose a voice recognition based order
picking system. It is our contention that the introduction of voice recognition and voice
based control system can signiﬁcantly reduce the physical activity at the order picking
production line. This proposed approach, therefore, is applicable in labor intensive and
high throughput retail warehouse/distribution and logistics centers, in particular, those
that are not equipped with automatic and semiautomatic systems.
2
Speed Recognition Technology
Automatic Speech Recognition (ASR) refers to a whole raft of technologies that aim to
collect and process human speech and digitise such data into computer understandable
© Springer International Publishing AG 2018
Q. Zu and B. Hu (Eds.): HCC 2017, LNCS 10745, pp. 654–659, 2018.
https://doi.org/10.1007/978-3-319-74521-3_68

and actable data and/or instructions. Based on retrained models, computers should be
able to performed the entire process without or with very limited human intervention.
Upon receiving human speech signal, ASR systems process the data based on
features such as ambient/background signals, timbre, pitch, etc. Figure 1 depicts the key
components of an ASR system. An ASR system normally consists of 5 modules, prepro‐
cessing, feature extraction, model training, model scoring and selection, and post-
processing. The workﬂow can be largely divided into model training and model appli‐
cation.
(1) Preprocessing module: Speech signal includes ﬁltering, A/D conversion, ﬁltering,
demonising, speech enhancement, signal smoothing, end-point detection, etc.
(2) Feature extraction: After preprocessing, speech signals are subject to time-
frequency analysis, Cepstral analysis and wavelet transformation. This is to extract
from the signal data such features as: timbre, language, and voice contents.
(3) Model training: Features extracted by the previous module are then fed into training
algorithms to obtain language models. Model training normally is an iterative
process wherein features are evaluated, re-processed and optimised so as to
construct models that can reﬂect all features of the input data. Trained models are
stored in a model library.
(4) Model scoring and selection: When ASR system is in application mode, features
extracted from speech signal are utilised to identify and retrieve the best model
from the model library. A predeﬁned scoring scheme can guide the selection of best
matching models.
(5) Post-processing: Natural languages are ambiguous. In order to improve the system
performance, it may be necessary to introduce linguistic and semantic analysis to
the output of selected speech signal model.
Fig. 1. Chipset for speech recognition
3
Chipset for Speech Recognition
The envisaged use case of the proposed system is that the speech recognition system
should be a portable device that the order picking staﬀ can easily carry along without
hindering their normal work routines. Such a scenario derives a plethora of requirements
on speech processing speed and accuracy, device portability, device energy consumption
proﬁle. In the present paper, we focused on local dialects and opt for non-speciﬁc speech
recognition chip. We also tuned the model library based on the core user population,
ambient and environmental noise, and typical use cases of the distribution centre where
the system will be evaluated and put into production.
Application of Speech Recognition Technology in Logistics
655

Currently, widely used non-speciﬁc speed recognition chips include ASR M08,
LD3320 and LingYang 61A. Such chips do not require pre-recording of speech signals
for initialisation and validation and are suitable for our envisaged use case [1].
LD3320 speech recognition module is a specialised processor with external circuits
including AD, DA converters, microphone interface, audio output interface, and parallel
and serial interface. It beneﬁts from small physical size and low power consumption,
lending itself to mobile applications. LD3320 embeds speech recognition module trained
based on a large amount of speech data. It provides an oﬀ-the-shelf solution for many
use cases. LD3320 provides a versatile of external control and interfaces, including
dynamic editing of the recognised keywords. LD3320, therefore, facilitates further
development of speech recognition functionalities and customisation against speciﬁc
application context [2].
End users of the proposed speech recognition system are order picking staﬀ in labour
intensive manual warehouse/distribution centres. The speech recognition system is
expected to be worn by the users while they are moving between aisles, staging areas,
order picking lines and storage spaces. It, therefore, should be light weight, low energy
consumption, easy to carry and highly responsive [3]. Meanwhile, as the staﬀ turnover
rate is very high in such a working environment, the speech recognition module should
be non-speciﬁc [4].
LD3320 presents the following key features:
(1) Integrated Flash and RAM. This eliminates the needs for external storage and thus
reduce the complexity and cost of the system and overall energy consumption
proﬁle.
(2) Embedded speech recognition models. The on chip models are already suitable for
many generic application scenarios. It therefore presents a low learning curve for
adoption.
(3) Parameter tuning on distance and sensitivity.
(4) No requirement on prior training and recording.
(5) Dynamic keyword editing allows easy extension and adaption to new scenarios.
(6) High accuracy rate (95%) against a list of as many as 50 keywords.
(7) Integrated A/D, D/A convertors, Integrated ampliﬁer circuit and a 550 mW speaker
interface for playback; parallel and serial interface for further development.
(8) Can easily switch between sleep and activate states to reduce energy consumption.
(9) Working power supply of 3.3 V. LD3320 can be powered by three AA batteries to
meet the power supply needs for portable systems.
LD3320 ICRoute is based on keyword recognition. Figure 2 illustrates key compo‐
nents of LD3320.
LD3320 collects speech signal through the integrated microphone (MIC). The signal
is then subject to spectrum analysis, feature extraction and feature engineering so as to
be ready for keyword extraction. A trained speech recognition model will then be applied
to the processed signal, outcome of which are candidate words which will be compared
against a predeﬁned list provided to the system. During the ﬁnal step, the system will
compute a score for each candidate keywords and output the one(s) with the highest
656
T. Fu and B. Sun

score(s). Human intervention is possible by dynamic keyword editing through Micro
Control Unit (MCU).
LD3320 is equipped with two speech signal collection and recognition modes:
(1) Fixed Interval based: Based on a predeﬁned interval (e.g. 3 s), LD3320 collects and
records all speech signal data during the given time window. At the end of a time
window, data collection is put on hold and keyword detection is only performed on
signals collected within the time window.
(2) Continuous analysis: LD3320 leverages a voice activity detection technology
(VAD) to identify the beginning and end of human speech out of environmental
and ambient sound. All speech signals between these two points are collected, upon
which further processes are applied.
Due to the ambiguity caused by multiple matchings, when processing input speech
signals, LD3320 considers any matching as interim results and does not output the
identiﬁed keywords until the input speech signal stops (i.e. the end point of human
speech is identiﬁed or a predeﬁned time interval ends). When a signal ending condition
is reached, LD3320 deems the input signal complete and thus the optimal keyword
matching results are returned. For instance, the keyword list may contains “201” and
“2017”. When processing speech signal, when “1” is detected, the best matching
keyword is “201” with “2017” as a candidate with high probability. LD3320 will proceed
as follows:
(1) if input signal ends, output “201”;
(2) if input signal continues and “7” is detected, recomputes “2017” as the best match.
In the end, if not speech signal after “7”, “2017” is deemed best matching keywords
and returned to end users.
4
Application of Speech Recognition in Order Picking
Speech recognition enabled order picking proceeds as follows:
(1) order picking staﬀ initiates the device by instructing “start operation”. Once
receiving the initiation instruction, the portable device starts to “read” out the region
Fig. 2. The working principle of speech chip
Application of Speech Recognition Technology in Logistics
657

number, aisle number, and shelf number of the ﬁrst order. It also instruct the order
picking staﬀ to read out bar code of the picked items.
(2) When the item is located, order picking staﬀ reads the barcode. The portable
terminal compares recognised barcode with the one in the picking order. If matches,
portable terminal will pronounce quantity of item for the order; otherwise, portable
terminal will read out location information again.
(3) When the current order is ﬁnished, order picking staﬀ instruct the device with “task
complete”. The portable device changes the status of the current order and moves
to the next one in the order queue. If the order queue is empty, the portable terminal
pronounces “operation complete”.
Barcode comparison in step two acts as a key checking point. This is to avoid poten‐
tial cost introduced by human errors. If the barcode does not match, order picking staﬀ
will be remaindered the correct location information or provided means to manually
double-check the order details.
The envisaged order picking scenario limits the keyword search space to reduce
system complexity. As shown above, interactions between human order picking staﬀ
and portable terminal are restricted to the following keywords: “start operation”,
“repeat”, “task complete” and digits based item barcodes. Apart from barcodes, the other
instruction speech patterns can be preloaded, tuned and stored locally in the portable
terminal to enable oﬄine non-speciﬁc speech signal recognition. Terminal and server
communication is only needed for handshaking, system initialisation, and downloading
order details. Downlink and uplink bandwidth can be kept to minimum and thus improve
system performance by reducing network latency.
5
Evaluation and Discussion
A preliminary evaluation and comparative study has been carried out. Figure 3 demon‐
strates the diﬀerence between conventional and speech recognition based order picking
workﬂows.
Fig. 3. The diﬀerence between conventional and speech recognition based order picking
workﬂows
It is evident that our proposed solution can signiﬁcantly reduce physical activities
that are inevitable in conventional approach based on barcode or RF scanning. Reduced
physical activity leads to improved work eﬃciency and reduced human errors due to
fatigue and negligence. Speech recognition also simpliﬁed the overall workﬂow from
658
T. Fu and B. Sun

original six steps to three by integrating and removing such activities as operating scan‐
ners. The reduced steps also contributed to cost reduction through paperless work space,
per order time reduction, and reduced error-rate.
The speech recognition based terminal optimises human system interface. The
underlying system eﬃciency needs to be based on route optimisation and shelving and
stock optimisation which are beyond the scope of this paper [5].
6
Conclusion
The application of speech recognition in warehouse and logistics is beneﬁcial in the
following aspects:
(1) Signiﬁcantly reduce travel distance of order picking staﬀ
(2) Simplify the order picking workﬂow
(3) Reduce errors due to human factors
(4) Increase eﬃciency of order data broadcasting
Jointly, the above lead to increased eﬃciency of order picking and thus reduced
overall cost.
References
1. Qingxiu, M., Zhengyun, R.: Voice picking system based on indoor positioning technology of
RFID. Artif. Intell. 18, 50–53 (2015)
2. Qin, Y., Yong, L.V.: Speech Signal Processing and Recognition, pp. 7–9. National Defend
Industry Press, Beijing (2015)
3. Jinping, L.: Simulation analysis of picking path and storage allocation strategy in distribution
center. Simul. Intell. 04, 310–315 (2015)
4. Zhao, Q., Hu, B., Shi, Y., Li, Y., Moore, P., Sun, M., Peng, H.: Automatic identiﬁcation and
removal of ocular artifacts in EEG—improved adaptive predictor ﬁltering for portable
applications. IEEE Trans. Nanobiosci. 13(2), 109–117 (2014)
5. Qingguo, Z., Lihua, H., Rongrong, W., et al.: The application of speech picking technology in
drug storage work. Chin. Pharm. 27, 501–503 (2014)
Application of Speech Recognition Technology in Logistics
659

Author Index
Abramov, Vitaly
158
Akram, Aftab
410
Bai, Shuotian
433
Bao, Tie
472
Cai, Weiwei
108
Cerotti, Davide
616
Chang, Liwen
571, 588
Chen, Boting
364
Chen, Dingfang
364, 561, 571, 582
Chen, Guohua
422
Chen, Hanhua
330
Chen, Lina
354
Chen, Ming
453
Chen, Mo
76
Chen, Qingzhang
354
Chen, Rui
638
Chen, Xiaoling
648
Chen, Yangpeng
364
Cheng, Yuan
108
Dautov, Rustem
183, 394, 616
Deng, Zhongliang
245
Ding, Jundong
258
Ding, Lei
189, 201
Ding, Rui
422
Distefano, Salvatore
183, 394, 616
E., Haihong
382
Fan, Jiachen
245
Fan, Jiawei
608
Feng, Chunyan
118
Flint, Jonathan
433
Fu, Chengzhou
410, 422
Fu, Haohui
164
Fu, Tianzheng
654
Gafarov, Fail
158
Gao, Qian
64
Gao, Sihua
252
Gao, Yan
561
Gao, Yuanbo
433
Gong, Ping
1
Gu, Bo
301, 342, 376
Gu, Songyuan
465
Guo, Da
274
Guo, Kun
410
Guo, Qin
582
Guo, Xing
265
Han, Xiao
118
Han, Zhian
201
He, Chaobo
422
He, Fazhi
108
He, Mingshu
503
Hong, Yawei
588
Hou, Chunping
95
Hou, Wenjun
527, 627, 648
Hu, Bin
433
Hu, Jingbo
582
Hu, Kai
53
Hu, Neng
265
Hu, Zhengying
311
Huang, Ruqin
342
Huang, Tianhao
588
Huang, Weidong
443
Jia, Zongpu
465
Jiang, Weijin
538, 550
Jiang, Wenchao
85
Jiang, Yuncheng
410
Jiang, Zhiyang
627
Jiao, Dongdong
400
Jiao, Jichao
245
Jin, Hai
330
Kang, Zhongfeng
215
Kong, Xuedong
561
Kugurakova, Vlada
158
Li, Baobin
400
Li, Bo
364
Li, Chaoqiang
648
Li, Chuanjie
85

Li, Chunying
453
Li, Dawei
139
Li, Hao
150, 648
Li, Lijie
582
Li, Lu
76
Li, Mingkun
382
Li, Qiao
274, 318
Li, Ruonan
258
Li, Shaobo
588
Li, Xuejing
164
Li, Yibin
130
Li, Zhuohuan
382
Lian, Jian
130
Lin, Dexi
85
Lin, Jinjiao
130
Lin, Sui
85
Lin, Xiaorong
164
Liu, Baoling
311
Liu, Fangfang
118, 516
Liu, Guangyue
588, 608
Liu, Jiwei
453
Liu, Mengmeng
34
Liu, Ningning
23, 318
Liu, Shufen
13, 23, 201, 252, 465, 472
Liu, Ting
527
Liu, Xiyu
1, 46, 102, 208
Lu, Kaining
173
Lv, Xiao
108
Ma, Lujuan
311
Ma, Tongtong
95
Ma, Yue
342
Man, Yi
76, 232
Mao, Chengjie
422
Mei, Jie
571, 582
Meng, Dejie
453
Merlino, Giovanni
616
Miao, Chen
13
Miao, Chunyu
354
Ning, Min
481
Pan, Changqing
288
Peng, Chongxiao
376
Pu, Haitao
130
Puliaﬁto, Antonio
616
Qi, Feng
34
Qin, Hanfeng
595
Qin, Xiaoqi
288
Qin, Yajuan
164
Ren, Ligang
232, 376
Ren, Liyan
28
Sabitov, Almaz
158
Senko, Oleg
394
Sheng, Qing
527
Shi, Huafeng
582
Shi, Yonghui
465
Song, Meina
274, 301, 318, 342, 376, 382
Su, Yiming
571
Sui, Xiaoyun
433
Sun, Aobing
85
Sun, Bin
654
Sun, Weiqi
288
Surnin, Oleg
394
Tan, Wen’an
53
Tang, Anqiong
53
Tang, Yong
410, 453
Tao, Menglun
364
Teng, Yinglei
288
Tian, Tian
64, 481
Wang, Baoliang
173
Wang, Gengyi
527
Wang, Jingying
433
Wang, Mou
608
Wang, Pan
53
Wang, Qian
443, 608
Wang, Qiang
139
Wang, Sihan
571
Wang, Xiaojuan
301, 503
Wang, Xinyi
118
Wang, Yanhua
46, 208
Wang, Yixuan
443
Wang, Zhen
503
Wei, Yifei
274, 301, 318, 342, 376
Wu, Jiangming
492
Wu, Kaili
274
Wu, Shupei
588
Wu, Zhenglun
472
662
Author Index

Xiang, Jianwen
561
Xiang, Laisheng
46, 102, 208
Xiao, Ya’nan
288
Xiong, Shengwu
561
Xiong, Yuping
252
Xu, Weifeng
150
Xu, Wenyuan
150
Xu, Yuhui
538, 550
Yang, Bo
215
Yang, Chunhui
561
Yang, Fan
64, 481
Yang, Yanfang
364
Yang, Yang
95
Yao, Hong
64, 481
Yao, Susu
173
Yao, Zhilin
13
You, Jingwen
503
Yu, Chen
330
Yu, Meng
139
Yuan, Yahui
400
Yue, Guanghui
95
Zang, Wenke
28
Zeng, Weiquan
422
Zeng, Zhimin
516
Zhang, Bao
102
Zhang, Duo
232
Zhang, Longtao
516
Zhang, Ruiguo
330
Zhang, Shupeng
627
Zhang, Xingang
189
Zhang, Xinjia
23
Zhang, Yang
588
Zhang, Yi
342
Zhang, Yong
258
Zhang, Zhewei
164
Zhang, Zhiqiang
301, 311
Zhao, Huizhen
173
Zhao, Liang
258
Zhao, Mengyu
318
Zhao, Xiuyu
481
Zhao, Xuehui
189
Zheng, Congxing
571
Zheng, Kun
64
Zheng, Tinggang
481
Zheng, Wanbo
189, 201
Zhou, Jiehan
330
Zhou, Yang
433
Zhu, Shaohui
330
Zhu, Tingshao
400, 433
Zhu, Yemei
252
Zhu, Yong
608
Zu, Qiaohong
492, 638
Author Index
663



This page intentionally left blank

LOGIC IN COMPUTER SCIENCE
Modelling and Reasoning about Systems


LOGIC IN COMPUTER SCIENCE
Modelling and Reasoning about Systems
MICHAEL HUTH
Department of Computing
Imperial College London, United Kingdom
MARK RYAN
School of Computer Science
University of Birmingham, United Kingdom

CAMBRIDGE UNIVERSITY PRESS
Cambridge, New York, Melbourne, Madrid, Cape Town, Singapore, São Paulo
Cambridge University Press
The Edinburgh Building, Cambridge CB2 8RU, UK
First published in print format
ISBN-13
978-0-521-54310-1
ISBN-13
978-0-511-26401-6
© Cambridge University Press 2004
2004
Information on this title: www.cambridge.org/9780521543101
This publication is in copyright. Subject to statutory exception and to the provision of
relevant collective licensing agreements, no reproduction of any part may take place
without the written permission of Cambridge University Press.
ISBN-10
0-511-26401-1
ISBN-10
0-521-54310-X
Cambridge University Press has no responsibility for the persistence or accuracy of urls
for external or third-party internet websites referred to in this publication, and does not
guarantee that any content on such websites is, or will remain, accurate or appropriate.
Published in the United States of America by Cambridge University Press, New York
www.cambridge.org
paperback
eBook (EBL)
eBook (EBL)
paperback

Contents
Foreword to the ﬁrst edition
page ix
Preface to the second edition
xi
Acknowledgements
xiii
1
Propositional logic
1
1.1
Declarative sentences
2
1.2
Natural deduction
5
1.2.1
Rules for natural deduction
6
1.2.2
Derived rules
23
1.2.3
Natural deduction in summary
26
1.2.4
Provable equivalence
29
1.2.5
An aside: proof by contradiction
29
1.3
Propositional logic as a formal language
31
1.4
Semantics of propositional logic
36
1.4.1
The meaning of logical connectives
36
1.4.2
Mathematical induction
40
1.4.3
Soundness of propositional logic
45
1.4.4
Completeness of propositional logic
49
1.5
Normal forms
53
1.5.1
Semantic equivalence, satisﬁability and validity
54
1.5.2
Conjunctive normal forms and validity
58
1.5.3
Horn clauses and satisﬁability
65
1.6
SAT solvers
68
1.6.1
A linear solver
69
1.6.2
A cubic solver
72
1.7
Exercises
78
1.8
Bibliographic notes
91
2
Predicate logic
93
2.1
The need for a richer language
93
v

vi
Contents
2.2
Predicate logic as a formal language
98
2.2.1
Terms
99
2.2.2
Formulas
100
2.2.3
Free and bound variables
102
2.2.4
Substitution
104
2.3
Proof theory of predicate logic
107
2.3.1
Natural deduction rules
107
2.3.2
Quantiﬁer equivalences
117
2.4
Semantics of predicate logic
122
2.4.1
Models
123
2.4.2
Semantic entailment
129
2.4.3
The semantics of equality
130
2.5
Undecidability of predicate logic
131
2.6
Expressiveness of predicate logic
136
2.6.1
Existential second-order logic
139
2.6.2
Universal second-order logic
140
2.7
Micromodels of software
141
2.7.1
State machines
142
2.7.2
Alma – re-visited
146
2.7.3
A software micromodel
148
2.8
Exercises
157
2.9
Bibliographic notes
170
3
Veriﬁcation by model checking
172
3.1
Motivation for veriﬁcation
172
3.2
Linear-time temporal logic
175
3.2.1
Syntax of LTL
175
3.2.2
Semantics of LTL
178
3.2.3
Practical patterns of speciﬁcations
183
3.2.4
Important equivalences between LTL formulas
184
3.2.5
Adequate sets of connectives for LTL
186
3.3
Model checking: systems, tools, properties
187
3.3.1
Example: mutual exclusion
187
3.3.2
The NuSMV model checker
191
3.3.3
Running NuSMV
194
3.3.4
Mutual exclusion revisited
195
3.3.5
The ferryman
199
3.3.6
The alternating bit protocol
203
3.4
Branching-time logic
207
3.4.1
Syntax of CTL
208

Contents
vii
3.4.2
Semantics of CTL
211
3.4.3
Practical patterns of speciﬁcations
215
3.4.4
Important equivalences between CTL formulas
215
3.4.5
Adequate sets of CTL connectives
216
3.5
CTL* and the expressive powers of LTL and CTL
217
3.5.1
Boolean combinations of temporal formulas in CTL
220
3.5.2
Past operators in LTL
221
3.6
Model-checking algorithms
221
3.6.1
The CTL model-checking algorithm
222
3.6.2
CTL model checking with fairness
230
3.6.3
The LTL model-checking algorithm
232
3.7
The ﬁxed-point characterisation of CTL
238
3.7.1
Monotone functions
240
3.7.2
The correctness of SATEG
242
3.7.3
The correctness of SATEU
243
3.8
Exercises
245
3.9
Bibliographic notes
254
4
Program veriﬁcation
256
4.1
Why should we specify and verify code?
257
4.2
A framework for software veriﬁcation
258
4.2.1
A core programming language
259
4.2.2
Hoare triples
262
4.2.3
Partial and total correctness
265
4.2.4
Program variables and logical variables
268
4.3
Proof calculus for partial correctness
269
4.3.1
Proof rules
269
4.3.2
Proof tableaux
273
4.3.3
A case study: minimal-sum section
287
4.4
Proof calculus for total correctness
292
4.5
Programming by contract
296
4.6
Exercises
299
4.7
Bibliographic notes
304
5
Modal logics and agents
306
5.1
Modes of truth
306
5.2
Basic modal logic
307
5.2.1
Syntax
307
5.2.2
Semantics
308
5.3
Logic engineering
316
5.3.1
The stock of valid formulas
317

viii
Contents
5.3.2
Important properties of the accessibility relation
320
5.3.3
Correspondence theory
322
5.3.4
Some modal logics
326
5.4
Natural deduction
328
5.5
Reasoning about knowledge in a multi-agent system
331
5.5.1
Some examples
332
5.5.2
The modal logic KT45n
335
5.5.3
Natural deduction for KT45n
339
5.5.4
Formalising the examples
342
5.6
Exercises
350
5.7
Bibliographic notes
356
6
Binary decision diagrams
358
6.1
Representing boolean functions
358
6.1.1
Propositional formulas and truth tables
359
6.1.2
Binary decision diagrams
361
6.1.3
Ordered BDDs
366
6.2
Algorithms for reduced OBDDs
372
6.2.1
The algorithm reduce
372
6.2.2
The algorithm apply
373
6.2.3
The algorithm restrict
377
6.2.4
The algorithm exists
377
6.2.5
Assessment of OBDDs
380
6.3
Symbolic model checking
382
6.3.1
Representing subsets of the set of states
383
6.3.2
Representing the transition relation
385
6.3.3
Implementing the functions pre∃and pre∀
387
6.3.4
Synthesising OBDDs
387
6.4
A relational mu-calculus
390
6.4.1
Syntax and semantics
390
6.4.2
Coding CTL models and speciﬁcations
393
6.5
Exercises
398
6.6
Bibliographic notes
413
Bibliography
414
Index
418

Foreword to the first edition
by
Edmund M. Clarke
FORE Systems Professor of Computer Science
Carnegie Mellon University
Pittsburgh, PA
Formal methods have ﬁnally come of age! Speciﬁcation languages, theorem
provers, and model checkers are beginning to be used routinely in industry.
Mathematical logic is basic to all of these techniques. Until now textbooks
on logic for computer scientists have not kept pace with the development
of tools for hardware and software speciﬁcation and veriﬁcation. For exam-
ple, in spite of the success of model checking in verifying sequential circuit
designs and communication protocols, until now I did not know of a sin-
gle text, suitable for undergraduate and beginning graduate students, that
attempts to explain how this technique works. As a result, this material is
rarely taught to computer scientists and electrical engineers who will need to
use it as part of their jobs in the near future. Instead, engineers avoid using
formal methods in situations where the methods would be of genuine beneﬁt
or complain that the concepts and notation used by the tools are compli-
cated and unnatural. This is unfortunate since the underlying mathematics
is generally quite simple, certainly no more diﬃcult than the concepts from
mathematical analysis that every calculus student is expected to learn.
Logic in Computer Science by Huth and Ryan is an exceptional book.
I was amazed when I looked through it for the ﬁrst time. In addition to
propositional and predicate logic, it has a particularly thorough treatment
of temporal logic and model checking. In fact, the book is quite remarkable
in how much of this material it is able to cover: linear and branching time
temporal logic, explicit state model checking, fairness, the basic ﬁxpoint
ix

x
Foreword to the first edition
theorems for computation tree logic (CTL), even binary decision diagrams
and symbolic model checking. Moreover, this material is presented at a level
that is accessible to undergraduate and beginning graduate students. Nu-
merous problems and examples are provided to help students master the
material in the book. Since both Huth and Ryan are active researchers in
logics of programs and program veriﬁcation, they write with considerable
authority.
In summary, the material in this book is up-to-date, practical, and ele-
gantly presented. The book is a wonderful example of what a modern text
on logic for computer science should be like. I recommend it to the reader
with greatest enthusiasm and predict that the book will be an enormous
success.
(This foreword is re-printed in the second edition with its author’s permis-
sion.)

Preface to the second edition
Our motivation for (re)writing this book
One of the leitmotifs of writing the ﬁrst edition of our book was the obser-
vation that most logics used in the design, speciﬁcation and veriﬁcation of
computer systems fundamentally deal with a satisfaction relation
M ⊨φ
where M is some sort of situation or model of a system, and φ is a speciﬁ-
cation, a formula of that logic, expressing what should be true in situation
M. At the heart of this set-up is that one can often specify and implement
algorithms for computing ⊨. We developed this theme for propositional,
ﬁrst-order, temporal, modal, and program logics. Based on the encourag-
ing feedback received from ﬁve continents we are pleased to hereby present
the second edition of this text which means to preserve and improve on the
original intent of the ﬁrst edition.
What’s new and what’s gone
Chapter 1 now discusses the design, correctness, and complexity of a SAT
solver (a marking algorithm similar to St˚almarck’s method [SS90]) for full
propositional logic.
Chapter 2 now contains basic results from model theory (Compactness
Theorem and L¨owenheim–Skolem Theorem); a section on the transitive clo-
sure and the expressiveness of existential and universal second-order logic;
and a section on the use of the object modelling language Alloy and its anal-
yser for specifying and exploring under-speciﬁed ﬁrst-order logic models with
respect to properties written in ﬁrst-order logic with transitive closure. The
Alloy language is executable which makes such exploration interactive and
formal.
xi

xii
Preface to the second edition
Chapter 3 has been completely restructured. It now begins with a discus-
sion of linear-time temporal logic; features the open-source NuSMV model-
checking tool throughout; and includes a discussion on planning problems,
more material on the expressiveness of temporal logics, and new modelling
examples.
Chapter 4 contains more material on total correctness proofs and a new
section on the programming-by-contract paradigm of verifying program cor-
rectness.
Chapters 5 and 6 have also been revised, with many small alterations and
corrections.
The interdependence of chapters and prerequisites
The book requires that students know the basics of elementary arithmetic
and naive set theoretic concepts and notation. The core material of Chap-
ter 1 (everything except Sections 1.4.3 to 1.6.2) is essential for all of the
chapters that follow. Other than that, only Chapter 6 depends on Chapter 3
and a basic understanding of the static scoping rules covered in Chapter 2 –
although one may easily cover Sections 6.1 and 6.2 without having done
Chapter 3 at all. Roughly, the interdependence diagram of chapters is
1
4
2
5
3
6
WWW page
This book is supported by a Web page, which contains a list of errata;
text ﬁles for all the program code; ancillary technical material and links;
all the ﬁgures; an interactive tutor based on multiple-choice questions;
and details of how instructors can obtain the solutions to exercises in
this book which are marked with a ∗. The URL for the book’s page
is www.cs.bham.ac.uk/research/lics/. See also www.cambridge.org/
052154310x

Acknowledgements
Many people have, directly or indirectly, assisted us in writing this book.
David Schmidt kindly provided serveral exercises for Chapter 4. Krysia
Broda has pointed out some typographical errors and she and the other
authors of [BEKV94] have allowed us to use some exercises from that book.
We have also borrowed exercises or examples from [Hod77] and [FHMV95].
Susan Eisenbach provided a ﬁrst description of the Package Dependency
System that we model in Alloy in Chapter 2. Daniel Jackson make very
helpful comments on versions of that section. Zena Matilde Ariola, Josh
Hodas, Jan Komorowski, Sergey Kotov, Scott A. Smolka and Steve Vickers
have corresponded with us about this text; their comments are appreciated.
Matt Dwyer and John Hatcliﬀmade useful comments on drafts of Chap-
ter 3. Kevin Lucas provided insightful comments on the content of Chapter
6, and notiﬁed us of numerous typographical errors in several drafts of the
book. Achim Jung read several chapters and gave useful feedback.
Additionally, a number of people read and provided useful comments on
several chapters, including Moti Ben-Ari, Graham Clark, Christian Haack,
Anthony Hook, Roberto Segala, Alan Sexton and Allen Stoughton. Numer-
ous students at Kansas State University and the University of Birmingham
have given us feedback of various kinds, which has inﬂuenced our choice and
presentation of the topics. We acknowledge Paul Taylor’s LATEX package for
proof boxes. About half a dozen anonymous referees made critical, but con-
structive, comments which helped to improve this text in various ways. In
spite of these contributions, there may still be errors in the book, and we
alone must take responsibility for those.
Added for second edition
Many people have helped improve this text by pointing out typos and
making other useful comments after the publication date. Among them,
xiii

xiv
Acknowledgements
we mention Wolfgang Ahrendt, Yasuhiro Ajiro, Torben Amtoft, Stephan
Andrei, Bernhard Beckert, Jonathan Brown, James Caldwell, Ruchira Datta,
Amy Felty, Dimitar Guelev, Hirotsugu Kakugawa, Kamran Kashef, Markus
Kr¨otzsch, Jagun Kwon, Ranko Lazic, David Makinson, Alexander Miczo,
Aart Middeldorp, Robert Morelli, Prakash Panangaden, Aileen Paraguya,
Frank Pfenning, Shekhar Pradhan, Koichi Takahashi, Kazunori Ueda,
Hiroshi Watanabe, Fuzhi Wang and Reinhard Wilhelm.

1
Propositional logic
The aim of logic in computer science is to develop languages to model the
situations we encounter as computer science professionals, in such a way
that we can reason about them formally. Reasoning about situations means
constructing arguments about them; we want to do this formally, so that
the arguments are valid and can be defended rigorously, or executed on a
machine.
Consider the following argument:
Example 1.1 If the train arrives late and there are no taxis at the station,
then John is late for his meeting. John is not late for his meeting. The train
did arrive late. Therefore, there were taxis at the station.
Intuitively, the argument is valid, since if we put the ﬁrst sentence and
the third sentence together, they tell us that if there are no taxis, then John
will be late. The second sentence tells us that he was not late, so it must be
the case that there were taxis.
Much of this book will be concerned with arguments that have this struc-
ture, namely, that consist of a number of sentences followed by the word
‘therefore’ and then another sentence. The argument is valid if the sentence
after the ‘therefore’ logically follows from the sentences before it. Exactly
what we mean by ‘follows from’ is the subject of this chapter and the next
one.
Consider another example:
Example 1.2 If it is raining and Jane does not have her umbrella with her,
then she will get wet. Jane is not wet. It is raining. Therefore, Jane has her
umbrella with her.
This is also a valid argument. Closer examination reveals that it actually
has the same structure as the argument of the previous example! All we have
1

2
1 Propositional logic
done is substituted some sentence fragments for others:
Example 1.1
Example 1.2
the train is late
it is raining
there are taxis at the station
Jane has her umbrella with her
John is late for his meeting
Jane gets wet.
The argument in each example could be stated without talking about trains
and rain, as follows:
If p and not q, then r. Not r. p. Therefore, q.
In developing logics, we are not concerned with what the sentences really
mean, but only in their logical structure. Of course, when we apply such
reasoning, as done above, such meaning will be of great interest.
1.1 Declarative sentences
In order to make arguments rigorous, we need to develop a language in which
we can express sentences in such a way that brings out their logical structure.
The language we begin with is the language of propositional logic. It is based
on propositions, or declarative sentences which one can, in principle, argue
as being true or false. Examples of declarative sentences are:
(1)
The sum of the numbers 3 and 5 equals 8.
(2)
Jane reacted violently to Jack’s accusations.
(3)
Every even natural number >2 is the sum of two prime numbers.
(4)
All Martians like pepperoni on their pizza.
(5)
Albert Camus ´etait un ´ecrivain fran¸cais.
(6)
Die W¨urde des Menschen ist unantastbar.
These sentences are all declarative, because they are in principle capable of
being declared ‘true’, or ‘false’. Sentence (1) can be tested by appealing to
basic facts about arithmetic (and by tacitly assuming an Arabic, decimal
representation of natural numbers). Sentence (2) is a bit more problematic.
In order to give it a truth value, we need to know who Jane and Jack are
and perhaps to have a reliable account from someone who witnessed the
situation described. In principle, e.g., if we had been at the scene, we feel
that we would have been able to detect Jane’s violent reaction, provided
that it indeed occurred in that way. Sentence (3), known as Goldbach’s
conjecture, seems straightforward on the face of it. Clearly, a fact about
all even numbers >2 is either true or false. But to this day nobody knows
whether sentence (3) expresses a truth or not. It is even not clear whether
this could be shown by some ﬁnite means, even if it were true. However, in

1.1 Declarative sentences
3
this text we will be content with sentences as soon as they can, in principle,
attain some truth value regardless of whether this truth value reﬂects the
actual state of aﬀairs suggested by the sentence in question. Sentence (4)
seems a bit silly, although we could say that if Martians exist and eat pizza,
then all of them will either like pepperoni on it or not. (We have to introduce
predicate logic in Chapter 2 to see that this sentence is also declarative if no
Martians exist; it is then true.) Again, for the purposes of this text sentence
(4) will do. Et alors, qu’est-ce qu’on pense des phrases (5) et (6)? Sentences
(5) and (6) are ﬁne if you happen to read French and German a bit. Thus,
declarative statements can be made in any natural, or artiﬁcial, language.
The kind of sentences we won’t consider here are non-declarative ones,
like
r Could you please pass me the salt?
r Ready, steady, go!
r May fortune come your way.
Primarily, we are interested in precise declarative sentences, or statements
about the behaviour of computer systems, or programs. Not only do we
want to specify such statements but we also want to check whether a given
program, or system, fulﬁls a speciﬁcation at hand. Thus, we need to develop
a calculus of reasoning which allows us to draw conclusions from given as-
sumptions, like initialised variables, which are reliable in the sense that they
preserve truth: if all our assumptions are true, then our conclusion ought to
be true as well. A much more diﬃcult question is whether, given any true
property of a computer program, we can ﬁnd an argument in our calculus
that has this property as its conclusion. The declarative sentence (3) above
might illuminate the problematic aspect of such questions in the context of
number theory.
The logics we intend to design are symbolic in nature. We translate a cer-
tain suﬃciently large subset of all English declarative sentences into strings
of symbols. This gives us a compressed but still complete encoding of declar-
ative sentences and allows us to concentrate on the mere mechanics of our
argumentation. This is important since speciﬁcations of systems or software
are sequences of such declarative sentences. It further opens up the possibil-
ity of automatic manipulation of such speciﬁcations, a job that computers
just love to do1. Our strategy is to consider certain declarative sentences as
1 There is a certain, slightly bitter, circularity in such endeavours: in proving that a certain
computer program P satisﬁes a given property, we might let some other computer program Q try
to ﬁnd a proof that P satisﬁes the property; but who guarantees us that Q satisﬁes the property
of producing only correct proofs? We seem to run into an inﬁnite regress.

4
1 Propositional logic
being atomic, or indecomposable, like the sentence
‘The number 5 is even.’
We assign certain distinct symbols p, q, r, . . ., or sometimes p1, p2, p3, . . . to
each of these atomic sentences and we can then code up more complex
sentences in a compositional way. For example, given the atomic sentences
p:
‘I won the lottery last week.’
q:
‘I purchased a lottery ticket.’
r:
‘I won last week’s sweepstakes.’
we can form more complex sentences according to the rules below:
¬:
The negation of p is denoted by ¬p and expresses ‘I did not win the lottery
last week,’ or equivalently ‘It is not true that I won the lottery last week.’
∨:
Given p and r we may wish to state that at least one of them is true: ‘I won the
lottery last week, or I won last week’s sweepstakes;’ we denote this declarative
sentence by p ∨r and call it the disjunction of p and r2.
∧:
Dually, the formula p ∧r denotes the rather fortunate conjunction of p and r:
‘Last week I won the lottery and the sweepstakes.’
→:
Last, but deﬁnitely not least, the sentence ‘If I won the lottery last week,
then I purchased a lottery ticket.’ expresses an implication between p and q,
suggesting that q is a logical consequence of p. We write p →q for that3. We
call p the assumption of p →q and q its conclusion.
Of course, we are entitled to use these rules of constructing propositions
repeatedly. For example, we are now in a position to form the proposition
p ∧q →¬r ∨q
which means that ‘if p and q then not r or q’. You might have noticed a
potential ambiguity in this reading. One could have argued that this sentence
has the structure ‘p is the case and if q then . . . ’ A computer would require
the insertion of brackets, as in
(p ∧q) →((¬r) ∨q)
2 Its meaning should not be confused with the often implicit meaning of or in natural language
discourse as either . . . or. In this text or always means at least one of them and should not be
confounded with exclusive or which states that exactly one of the two statements holds.
3 The natural language meaning of ‘if . . . then . . . ’ often implicitly assumes a causal role of
the assumption somehow enabling its conclusion. The logical meaning of implication is a bit
diﬀerent, though, in the sense that it states the preservation of truth which might happen
without any causal relationship. For example, ‘If all birds can ﬂy, then Bob Dole was never
president of the United States of America.’ is a true statement, but there is no known causal
connection between the ﬂying skills of penguins and eﬀective campaigning.

1.2 Natural deduction
5
to disambiguate this assertion. However, we humans get annoyed by a pro-
liferation of such brackets which is why we adopt certain conventions about
the binding priorities of these symbols.
Convention 1.3 ¬ binds more tightly than ∨and ∧, and the latter two
bind more tightly than →. Implication →is right-associative: expressions of
the form p →q →r denote p →(q →r).
1.2 Natural deduction
How do we go about constructing a calculus for reasoning about proposi-
tions, so that we can establish the validity of Examples 1.1 and 1.2? Clearly,
we would like to have a set of rules each of which allows us to draw a con-
clusion given a certain arrangement of premises.
In natural deduction, we have such a collection of proof rules. They al-
low us to infer formulas from other formulas. By applying these rules in
succession, we may infer a conclusion from a set of premises.
Let’s see how this works. Suppose we have a set of formulas4 φ1, φ2,
φ3, . . . , φn, which we will call premises, and another formula, ψ, which we
will call a conclusion. By applying proof rules to the premises, we hope
to get some more formulas, and by applying more proof rules to those, to
eventually obtain the conclusion. This intention we denote by
φ1, φ2, . . . , φn ⊢ψ.
This expression is called a sequent; it is valid if a proof for it can be found.
The sequent for Examples 1.1 and 1.2 is p ∧¬q →r, ¬r, p ⊢q. Construct-
ing such a proof is a creative exercise, a bit like programming. It is not
necessarily obvious which rules to apply, and in what order, to obtain the
desired conclusion. Additionally, our proof rules should be carefully chosen;
otherwise, we might be able to ‘prove’ invalid patterns of argumentation. For
4 It is traditional in logic to use Greek letters. Lower-case letters are used to stand for formulas
and upper-case letters are used for sets of formulas. Here are some of the more commonly used
Greek letters, together with their pronunciation:
Lower-case
φ
phi
ψ
psi
χ
chi
η
eta
α
alpha
β
beta
γ
gamma
Upper-case
Φ
Phi
Ψ
Psi
Γ
Gamma
∆
Delta

6
1 Propositional logic
example, we expect that we won’t be able to show the sequent p, q ⊢p ∧¬q.
For example, if p stands for ‘Gold is a metal.’ and q for ‘Silver is a metal,’
then knowing these two facts should not allow us to infer that ‘Gold is a
metal whereas silver isn’t.’
Let’s now look at our proof rules. We present about ﬁfteen of them in
total; we will go through them in turn and then summarise at the end of
this section.
1.2.1 Rules for natural deduction
The rules for conjunction
Our ﬁrst rule is called the rule for conjunc-
tion (∧): and-introduction. It allows us to conclude φ ∧ψ, given that we
have already concluded φ and ψ separately. We write this rule as
φ
ψ
φ ∧ψ
∧i.
Above the line are the two premises of the rule. Below the line goes the
conclusion. (It might not yet be the ﬁnal conclusion of our argument;
we might have to apply more rules to get there.) To the right of the line,
we write the name of the rule; ∧i is read ‘and-introduction’. Notice that we
have introduced a ∧(in the conclusion) where there was none before (in the
premises).
For each of the connectives, there is one or more rules to introduce it and
one or more rules to eliminate it. The rules for and-elimination are these
two:
φ ∧ψ
φ
∧e1
φ ∧ψ
ψ
∧e2.
(1.1)
The rule ∧e1 says: if you have a proof of φ ∧ψ, then by applying this rule
you can get a proof of φ. The rule ∧e2 says the same thing, but allows
you to conclude ψ instead. Observe the dependences of these rules: in the
ﬁrst rule of (1.1), the conclusion φ has to match the ﬁrst conjunct of the
premise, whereas the exact nature of the second conjunct ψ is irrelevant.
In the second rule it is just the other way around: the conclusion ψ has to
match the second conjunct ψ and φ can be any formula. It is important
to engage in this kind of pattern matching before the application of proof
rules.
Example 1.4 Let’s use these rules to prove that p ∧q, r |−q ∧r is valid.
We start by writing down the premises; then we leave a gap and write the

1.2 Natural deduction
7
conclusion:
p ∧q
r
q ∧r
The task of constructing the proof is to ﬁll the gap between the premises
and the conclusion by applying a suitable sequence of proof rules. In this
case, we apply ∧e2 to the ﬁrst premise, giving us q. Then we apply ∧i to this
q and to the second premise, r, giving us q ∧r. That’s it! We also usually
number all the lines, and write in the justiﬁcation for each line, producing
this:
1
p ∧q
premise
2
r
premise
3
q
∧e2 1
4
q ∧r
∧i 3, 2
Demonstrate to yourself that you’ve understood this by trying to show on
your own that (p ∧q) ∧r, s ∧t |−q ∧s is valid. Notice that the φ and ψ can
be instantiated not just to atomic sentences, like p and q in the example we
just gave, but also to compound sentences. Thus, from (p ∧q) ∧r we can
deduce p ∧q by applying ∧e1, instantiating φ to p ∧q and ψ to r.
If we applied these proof rules literally, then the proof above would actu-
ally be a tree with root q ∧r and leaves p ∧q and r, like this:
p ∧q
∧e2
q
r
∧i
q ∧r
However, we ﬂattened this tree into a linear presentation which necessitates
the use of pointers as seen in lines 3 and 4 above. These pointers allow
us to recreate the actual proof tree. Throughout this text, we will use the
ﬂattened version of presenting proofs. That way you have to concentrate only
on ﬁnding a proof, not on how to ﬁt a growing tree onto a sheet of paper.
If a sequent is valid, there may be many diﬀerent ways of proving it. So if
you compare your solution to these exercises with those of others, they need
not coincide. The important thing to realise, though, is that any putative
proof can be checked for correctness by checking each individual line, starting
at the top, for the valid application of its proof rule.

8
1 Propositional logic
The rules of double negation
Intuitively, there is no diﬀerence be-
tween a formula φ and its double negation ¬¬φ, which expresses no more
and nothing less than φ itself. The sentence
‘It is not true that it does not rain.’
is just a more contrived way of saying
‘It rains.’
Conversely, knowing ‘It rains,’ we are free to state this fact in this more
complicated manner if we wish. Thus, we obtain rules of elimination and
introduction for double negation:
¬¬φ
φ
¬¬e
φ
¬¬φ
¬¬i.
(There are rules for single negation on its own, too, which we will see later.)
Example 1.5 The proof of the sequent p, ¬¬(q ∧r) ⊢¬¬p ∧r below uses
most of the proof rules discussed so far:
1
p
premise
2
¬¬(q ∧r)
premise
3
¬¬p
¬¬i 1
4
q ∧r
¬¬e 2
5
r
∧e2 4
6
¬¬p ∧r
∧i 3, 5
Example 1.6 We now prove the sequent (p ∧q) ∧r, s ∧t |−q ∧s which
you were invited to prove by yourself in the last section. Please compare
the proof below with your solution:
1
(p ∧q) ∧r
premise
2
s ∧t
premise
3
p ∧q
∧e1 1
4
q
∧e2 3
5
s
∧e1 2
6
q ∧s
∧i 4, 5

1.2 Natural deduction
9
The rule for eliminating implication
There is one rule to introduce
→and one to eliminate it. The latter is one of the best known rules of
propositional logic and is often referred to by its Latin name modus ponens.
We will usually call it by its modern name, implies-elimination (sometimes
also referred to as arrow-elimination). This rule states that, given φ and
knowing that φ implies ψ, we may rightfully conclude ψ. In our calculus, we
write this as
φ
φ →ψ
ψ
→e.
Let us justify this rule by spelling out instances of some declarative sen-
tences p and q. Suppose that
p : It rained.
p →q : If it rained, then the street is wet.
so q is just ‘The street is wet.’ Now, if we know that it rained and if we
know that the street is wet in the case that it rained, then we may combine
these two pieces of information to conclude that the street is indeed wet.
Thus, the justiﬁcation of the →e rule is a mere application of common sense.
Another example from programming is:
p : The value of the program’s input is an integer.
p →q : If the program’s input is an integer, then the program outputs
a boolean.
Again, we may put all this together to conclude that our program outputs
a boolean value if supplied with an integer input. However, it is important
to realise that the presence of p is absolutely essential for the inference
to happen. For example, our program might well satisfy p →q, but if it
doesn’t satisfy p – e.g. if its input is a surname – then we will not be able to
derive q.
As we saw before, the formal parameters φ and the ψ for →e can be
instantiated to any sentence, including compound ones:
1
¬p ∧q
premise
2
¬p ∧q →r ∨¬p
premise
3
r ∨¬p
→e 2, 1

10
1 Propositional logic
Of course, we may use any of these rules as often as we wish. For example,
given p, p →q and p →(q →r), we may infer r:
1
p →(q →r)
premise
2
p →q
premise
3
p
premise
4
q →r
→e 1, 3
5
q
→e 2, 3
6
r
→e 4, 5
Before turning to implies-introduction, let’s look at a hybrid rule which
has the Latin name modus tollens. It is like the →e rule in that it eliminates
an implication. Suppose that p →q and ¬q are the case. Then, if p holds
we can use →e to conclude that q holds. Thus, we then have that q and ¬q
hold, which is impossible. Therefore, we may infer that p must be false. But
this can only mean that ¬p is true. We summarise this reasoning into the
rule modus tollens, or MT for short:5
φ →ψ
¬ψ
¬φ
MT.
Again, let us see an example of this rule in the natural language setting:
‘If Abraham Lincoln was Ethiopian, then he was African. Abraham
Lincoln was not African; therefore he was not Ethiopian.’
Example 1.7 In the following proof of
p →(q →r), p, ¬r ⊢¬q
we use several of the rules introduced so far:
1
p →(q →r)
premise
2
p
premise
3
¬r
premise
4
q →r
→e 1, 2
5
¬q
MT 4, 3
5 We will be able to derive this rule from other ones later on, but we introduce it here because it
allows us already to do some pretty slick proofs. You may think of this rule as one on a higher
level insofar as it does not mention the lower-level rules upon which it depends.

1.2 Natural deduction
11
Examples 1.8 Here are two example proofs which combine the rule MT
with either ¬¬e or ¬¬i:
1
¬p →q
premise
2
¬q
premise
3
¬¬p
MT 1, 2
4
p
¬¬e 3
proves that the sequent ¬p →q, ¬q ⊢p is valid; and
1
p →¬q
premise
2
q
premise
3
¬¬q
¬¬i 2
4
¬p
MT 1, 3
shows the validity of the sequent p →¬q, q ⊢¬p.
Note that the order of applying double negation rules and MT is diﬀerent
in these examples; this order is driven by the structure of the particular
sequent whose validity one is trying to show.
The rule implies introduction
The rule MT made it possible for us to
show that p →q, ¬q ⊢¬p is valid. But the validity of the sequent p →q ⊢
¬q →¬p seems just as plausible. That sequent is, in a certain sense, saying
the same thing. Yet, so far we have no rule which builds implications that
do not already occur as premises in our proofs. The mechanics of such a rule
are more involved than what we have seen so far. So let us proceed with
care. Let us suppose that p →q is the case. If we temporarily assume that
¬q holds, we can use MT to infer ¬p. Thus, assuming p →q we can show
that ¬q implies ¬p; but the latter we express symbolically as ¬q →¬p. To
summarise, we have found an argumentation for p →q ⊢¬q →¬p:
1
p →q
premise
2
¬q
assumption
3
¬p
MT 1, 2
4
¬q →¬p
→i 2−3
The box in this proof serves to demarcate the scope of the temporary as-
sumption ¬q. What we are saying is: let’s make the assumption of ¬q. To

12
1 Propositional logic
do this, we open a box and put ¬q at the top. Then we continue applying
other rules as normal, for example to obtain ¬p. But this still depends on
the assumption of ¬q, so it goes inside the box. Finally, we are ready to
apply →i. It allows us to conclude ¬q →¬p, but that conclusion no longer
depends on the assumption ¬q. Compare this with saying that ‘If you are
French, then you are European.’ The truth of this sentence does not depend
on whether anybody is French or not. Therefore, we write the conclusion
¬q →¬p outside the box.
This works also as one would expect if we think of p →q as a type of a
procedure. For example, p could say that the procedure expects an integer
value x as input and q might say that the procedure returns a boolean value
y as output. The validity of p →q amounts now to an assume-guarantee
assertion: if the input is an integer, then the output is a boolean. This
assertion can be true about a procedure while that same procedure could
compute strange things or crash in the case that the input is not an in-
teger. Showing p →q using the rule →i is now called type checking, an
important topic in the construction of compilers for typed programming
languages.
We thus formulate the rule →i as follows:
φ
...
ψ
φ →ψ
→i.
It says: in order to prove φ →ψ, make a temporary assumption of φ and then
prove ψ. In your proof of ψ, you can use φ and any of the other formulas
such as premises and provisional conclusions that you have made so far.
Proofs may nest boxes or open new boxes after old ones have been closed.
There are rules about which formulas can be used at which points in the
proof. Generally, we can only use a formula φ in a proof at a given point if
that formula occurs prior to that point and if no box which encloses that
occurrence of φ has been closed already.
The line immediately following a closed box has to match the pattern
of the conclusion of the rule that uses the box. For implies-introduction,
this means that we have to continue after the box with φ →ψ, where φ
was the ﬁrst and ψ the last formula of that box. We will encounter two
more proof rules involving proof boxes and they will require similar pattern
matching.

1.2 Natural deduction
13
Example 1.9 Here is another example of a proof using →i:
1
¬q →¬p
premise
2
p
assumption
3
¬¬p
¬¬i 2
4
¬¬q
MT 1, 3
5
p →¬¬q
→i 2−4
which veriﬁes the validity of the sequent ¬q →¬p ⊢p →¬¬q. Notice that
we could apply the rule MT to formulas occurring in or above the box: at
line 4, no box has been closed that would enclose line 1 or 3.
At this point it is instructive to consider the one-line argument
1
p
premise
which demonstrates p ⊢p. The rule →i (with conclusion φ →ψ) does not
prohibit the possibility that φ and ψ coincide. They could both be instanti-
ated to p. Therefore we may extend the proof above to
1
p
assumption
2
p →p
→i 1 −1
We write ⊢p →p to express that the argumentation for p →p does not
depend on any premises at all.
Deﬁnition 1.10 Logical formulas φ with valid sequent ⊢φ are theorems.
Example 1.11 Here is an example of a theorem whose proof utilises most
of the rules introduced so far:
1
q →r
assumption
2
¬q →¬p
assumption
3
p
assumption
4
¬¬p
¬¬i 3
5
¬¬q
MT 2, 4
6
q
¬¬e 5
7
r
→e 1, 6
8
p →r
→i 3−7
9
(¬q →¬p) →(p →r)
→i 2−8
10
(q →r) →((¬q →¬p) →(p →r))
→i 1−9

14
1 Propositional logic
q →r
→
→
→
¬q →¬p
r
p
Figure 1.1. Part of the structure of the formula (q →r) →((¬q →¬p) →
(p →r)) to show how it determines the proof structure.
Therefore the sequent
⊢(q →r) →((¬q →¬p) →(p →r)) is valid,
showing that (q →r) →((¬q →¬p) →(p →r)) is another theorem.
Remark 1.12 Indeed, this example indicates that we may transform any
proof of φ1, φ2, . . . , φn ⊢ψ in such a way into a proof of the theorem
⊢φ1 →(φ2 →(φ3 →(· · · →(φn →ψ) . . . )))
by ‘augmenting’ the previous proof with n lines of the rule →i applied to
φn, φn−1,. . . , φ1 in that order.
The nested boxes in the proof of Example 1.11 reveal a pattern of using
elimination rules ﬁrst, to deconstruct assumptions we have made, and then
introduction rules to construct our ﬁnal conclusion. More diﬃcult proofs
may involve several such phases.
Let us dwell on this important topic for a while. How did we come up
with the proof above? Parts of it are determined by the structure of the for-
mulas we have, while other parts require us to be creative. Consider the log-
ical structure of (q →r) →((¬q →¬p) →(p →r)) schematically depicted
in Figure 1.1. The formula is overall an implication since →is the root of
the tree in Figure 1.1. But the only way to build an implication is by means

1.2 Natural deduction
15
of the rule →i. Thus, we need to state the assumption of that implication
as such (line 1) and have to show its conclusion (line 9). If we managed
to do that, then we know how to end the proof in line 10. In fact, as we
already remarked, this is the only way we could have ended it. So essentially
lines 1, 9 and 10 are completely determined by the structure of the formula;
further, we have reduced the problem to ﬁlling the gaps in between lines 1
and 9. But again, the formula in line 9 is an implication, so we have only
one way of showing it: assuming its premise in line 2 and trying to show
its conclusion in line 8; as before, line 9 is obtained by →i. The formula
p →r in line 8 is yet another implication. Therefore, we have to assume p in
line 3 and hope to show r in line 7, then →i produces the desired result in
line 8.
The remaining question now is this: how can we show r, using the three
assumptions in lines 1–3? This, and only this, is the creative part of this
proof. We see the implication q →r in line 1 and know how to get r (using
→e) if only we had q. So how could we get q? Well, lines 2 and 3 almost look
like a pattern for the MT rule, which would give us ¬¬q in line 5; the latter
is quickly changed to q in line 6 via ¬¬e. However, the pattern for MT does
not match right away, since it requires ¬¬p instead of p. But this is easily
accomplished via ¬¬i in line 4.
The moral of this discussion is that the logical structure of the formula
to be shown tells you a lot about the structure of a possible proof and
it is deﬁnitely worth your while to exploit that information in trying to
prove sequents. Before ending this section on the rules for implication,
let’s look at some more examples (this time also involving the rules for
conjunction).
Example 1.13 Using the rule ∧i, we can prove the validity of the sequent
p ∧q →r ⊢p →(q →r):
1
p ∧q →r
premise
2
p
assumption
3
q
assumption
4
p ∧q
∧i 2, 3
5
r
→e 1, 4
6
q →r
→i 3−5
7
p →(q →r)
→i 2−6

16
1 Propositional logic
Example 1.14 Using the two elimination rules ∧e1 and ∧e2, we can show
that the ‘converse’ of the sequent above is valid, too:
1
p →(q →r)
premise
2
p ∧q
assumption
3
p
∧e1 2
4
q
∧e2 2
5
q →r
→e 1, 3
6
r
→e 5, 4
7
p ∧q →r
→i 2−6
The validity of p →(q →r) ⊢p ∧q →r and p ∧q →r ⊢p →(q →r)
means that these two formulas are equivalent in the sense that we can prove
one from the other. We denote this by
p ∧q →r ⊣⊢p →(q →r).
Since there can be only one formula to the right of ⊢, we observe that each
instance of ⊣⊢can only relate two formulas to each other.
Example 1.15 Here is an example of a proof that uses introduction and
elimination rules for conjunction; it shows the validity of the sequent p →
q ⊢p ∧r →q ∧r:
1
p →q
premise
2
p ∧r
assumption
3
p
∧e1 2
4
r
∧e2 2
5
q
→e 1, 3
6
q ∧r
∧i 5, 4
7
p ∧r →q ∧r
→i 2−6
The rules for disjunction
The rules for disjunction are diﬀerent in spirit
from those for conjunction. The case for conjunction was concise and clear:
proofs of φ ∧ψ are essentially nothing but a concatenation of a proof of φ and
a proof of ψ, plus an additional line invoking ∧i. In the case of disjunctions,
however, it turns out that the introduction of disjunctions is by far easier
to grasp than their elimination. So we begin with the rules ∨i1 and ∨i2.
From the premise φ we can infer that ‘φ or ψ’ holds, for we already know

1.2 Natural deduction
17
that φ holds. Note that this inference is valid for any choice of ψ. By the
same token, we may conclude ‘φ or ψ’ if we already have ψ. Similarly, that
inference works for any choice of φ. Thus, we arrive at the proof rules
φ
φ ∨ψ
∨i1
ψ
φ ∨ψ
∨i2.
So if p stands for ‘Agassi won a gold medal in 1996.’ and q denotes the
sentence ‘Agassi won Wimbledon in 1996.’ then p ∨q is the case because p
is true, regardless of the fact that q is false. Naturally, the constructed dis-
junction depends upon the assumptions needed in establishing its respective
disjunct p or q.
Now let’s consider or-elimination. How can we use a formula of the form
φ ∨ψ in a proof? Again, our guiding principle is to disassemble assumptions
into their basic constituents so that the latter may be used in our argumen-
tation such that they render our desired conclusion. Let us imagine that we
want to show some proposition χ by assuming φ ∨ψ. Since we don’t know
which of φ and ψ is true, we have to give two separate proofs which we need
to combine into one argument:
1.
First, we assume φ is true and have to come up with a proof of χ.
2.
Next, we assume ψ is true and need to give a proof of χ as well.
3.
Given these two proofs, we can infer χ from the truth of φ ∨ψ, since our case
analysis above is exhaustive.
Therefore, we write the rule ∨e as follows:
φ ∨ψ
φ
...
χ
ψ
...
χ
χ
∨e.
It is saying that: if φ ∨ψ is true and – no matter whether we assume φ or
we assume ψ – we can get a proof of χ, then we are entitled to deduce χ
anyway. Let’s look at a proof that p ∨q ⊢q ∨p is valid:
1
p ∨q
premise
2
p
assumption
3
q ∨p
∨i2 2
4
q
assumption
5
q ∨p
∨i1 4
6
q ∨p
∨e 1, 2−3, 4−5

18
1 Propositional logic
Here are some points you need to remember about applying the ∨e rule.
r For it to be a sound argument we have to make sure that the conclusions in each
of the two cases (the χ in the rule) are actually the same formula.
r The work done by the rule ∨e is the combining of the arguments of the two cases
into one.
r In each case you may not use the temporary assumption of the other case, unless
it is something that has already been shown before those case boxes began.
r The invocation of rule ∨e in line 6 lists three things: the line in which the
disjunction appears (1), and the location of the two boxes for the two cases (2–3
and 4–5).
If we use φ ∨ψ in an argument where it occurs only as an assumption or
a premise, then we are missing a certain amount of information: we know
φ, or ψ, but we don’t know which one of the two it is. Thus, we have
to make a solid case for each of the two possibilities φ or ψ; this resem-
bles the behaviour of a CASE or IF statement found in most programming
languages.
Example 1.16 Here is a more complex example illustrating these points.
We prove that the sequent q →r ⊢p ∨q →p ∨r is valid:
1
q →r
premise
2
p ∨q
assumption
3
p
assumption
4
p ∨r
∨i1 3
5
q
assumption
6
r
→e 1, 5
7
p ∨r
∨i2 6
8
p ∨r
∨e 2, 3−4, 5−7
9
p ∨q →p ∨r
→i 2−8
Note that the propositions in lines 4, 7 and 8 coincide, so the application of
∨e is legitimate.
We give some more example proofs which use the rules ∨e, ∨i1 and ∨i2.
Example 1.17 Proving the validity of the sequent (p ∨q) ∨r ⊢p ∨(q ∨r)
is surprisingly long and seemingly complex. But this is to be expected, since

1.2 Natural deduction
19
the elimination rules break (p ∨q) ∨r up into its atomic constituents p, q
and r, whereas the introduction rules then built up the formula p ∨(q ∨r).
1
(p ∨q) ∨r
premise
2
(p ∨q)
assumption
3
p
assumption
4
p ∨(q ∨r)
∨i1 3
5
q
assumption
6
q ∨r
∨i1 5
7
p ∨(q ∨r)
∨i2 6
8
p ∨(q ∨r)
∨e 2, 3−4, 5−7
9
r
assumption
10
q ∨r
∨i2 9
11
p ∨(q ∨r)
∨i2 10
12
p ∨(q ∨r)
∨e 1, 2−8, 9−11
Example 1.18 From boolean algebra, or circuit theory, you may know that
disjunctions distribute over conjunctions. We are now able to prove this in
natural deduction. The following proof:
1
p ∧(q ∨r)
premise
2
p
∧e1 1
3
q ∨r
∧e2 1
4
q
assumption
5
p ∧q
∧i 2, 4
6
(p ∧q) ∨(p ∧r)
∨i1 5
7
r
assumption
8
p ∧r
∧i 2, 7
9
(p ∧q) ∨(p ∧r)
∨i2 8
10
(p ∧q) ∨(p ∧r)
∨e 3, 4−6, 7−9
veriﬁes the validity of the sequent p ∧(q ∨r) ⊢(p ∧q) ∨(p ∧r) and you
are encouraged to show the validity of the ‘converse’ (p ∧q) ∨(p ∧r) ⊢p ∧
(q ∨r) yourself.

20
1 Propositional logic
A ﬁnal rule is required in order to allow us to conclude a box with a for-
mula which has already appeared earlier in the proof. Consider the sequent
⊢p →(q →p), whose validity may be proved as follows:
1
p
assumption
2
q
assumption
3
p
copy 1
4
q →p
→i 2−3
5
p →(q →p)
→i 1−4
The rule ‘copy’ allows us to repeat something that we know already. We need
to do this in this example, because the rule →i requires that we end the inner
box with p. The copy rule entitles us to copy formulas that appeared before,
unless they depend on temporary assumptions whose box has already been
closed. Though a little inelegant, this additional rule is a small price to pay
for the freedom of being able to use premises, or any other ‘visible’ formulas,
more than once.
The rules for negation
We have seen the rules ¬¬i and ¬¬e, but we
haven’t seen any rules that introduce or eliminate single negations. These
rules involve the notion of contradiction. This detour is to be expected since
our reasoning is concerned about the inference, and therefore the preserva-
tion, of truth. Hence, there cannot be a direct way of inferring ¬φ, given
φ.
Deﬁnition 1.19 Contradictions are expressions of the form φ ∧¬φ or ¬φ ∧
φ, where φ is any formula.
Examples of such contradictions are r ∧¬r, (p →q) ∧¬(p →q) and ¬(r ∨
s →q) ∧(r ∨s →q). Contradictions are a very important notion in logic.
As far as truth is concerned, they are all equivalent; that means we should
be able to prove the validity of
¬(r ∨s →q) ∧(r ∨s →q) ⊣⊢(p →q) ∧¬(p →q)
(1.2)
since both sides are contradictions. We’ll be able to prove this later, when
we have introduced the rules for negation.
Indeed, it’s not just that contradictions can be derived from contradic-
tions; actually, any formula can be derived from a contradiction. This can be

1.2 Natural deduction
21
confusing when you ﬁrst encounter it; why should we endorse the argument
p ∧¬p ⊢q, where
p : The moon is made of green cheese.
q : I like pepperoni on my pizza.
considering that our taste in pizza doesn’t have anything to do with the
constitution of the moon? On the face of it, such an endorsement may seem
absurd. Nevertheless, natural deduction does have this feature that any for-
mula can be derived from a contradiction and therefore it makes this argu-
ment valid. The reason it takes this stance is that ⊢tells us all the things
we may infer, provided that we can assume the formulas to the left of it.
This process does not care whether such premises make any sense. This has
at least the advantage that we can match ⊢to checks based on semantic
intuitions which we formalise later by using truth tables: if all the premises
compute to ‘true’, then the conclusion must compute ‘true’ as well. In partic-
ular, this is not a constraint in the case that one of the premises is (always)
false.
The fact that ⊥can prove anything is encoded in our calculus by the
proof rule bottom-elimination:
⊥
φ
⊥e.
The fact that ⊥itself represents a contradiction is encoded by the proof rule
not-elimination:
φ
¬φ
⊥
¬e.
Example 1.20 We apply these rules to show that ¬p ∨q |−p →q is
valid:
1
¬p ∨q
2
¬p
premise
3
p
assumption
4
⊥
¬e 3, 2
5
q
⊥e 4
6
p →q
→i 3−5
q
premise
p
assumption
q
copy 2
p →q
→i 3−4
7
p →q
∨e 1, 2−6

22
1 Propositional logic
Notice how, in this example, the proof boxes for ∨e are drawn side by side
instead of on top of each other. It doesn’t matter which way you do it.
What about introducing negations? Well, suppose we make an assumption
which gets us into a contradictory state of aﬀairs, i.e. gets us ⊥. Then our
assumption cannot be true; so it must be false. This intuition is the basis
for the proof rule ¬i:
φ
...
⊥
¬φ
¬i.
Example 1.21 We put these rules in action, demonstrating that the se-
quent p →q, p →¬q ⊢¬p is valid:
1
p →q
premise
2
p →¬q
premise
3
p
assumption
4
q
→e 1, 3
5
¬q
→e 2, 3
6
⊥
¬e 4, 5
7
¬p
¬i 3−6
Lines 3–6 contain all the work of the ¬i rule. Here is a second example,
showing the validity of a sequent, p →¬p ⊢¬p, with a contradictory formula
as sole premise:
1
p →¬p
premise
2
p
assumption
3
¬p
→e 1, 2
4
⊥
¬e 2, 3
5
¬p
¬i 2−4
Example 1.22 We prove that the sequent p →(q →r), p, ¬r |−¬q is valid,

1.2 Natural deduction
23
without using the MT rule:
1
p →(q →r)
premise
2
p
premise
3
¬r
premise
4
q
assumption
5
q →r
→e 1, 2
6
r
→e 5, 4
7
⊥
¬e 6, 3
8
¬q
¬i 4−7
Example 1.23 Finally, we return to the argument of Examples 1.1 and 1.2,
which can be coded up by the sequent p ∧¬q →r, ¬r, p |−q whose validity
we now prove:
1
p ∧¬q →r
premise
2
¬r
premise
3
p
premise
4
¬q
assumption
5
p ∧¬q
∧i 3, 4
6
r
→e 1, 5
7
⊥
¬e 6, 2
8
¬¬q
¬i 4−7
9
q
¬¬e 8
1.2.2 Derived rules
When describing the proof rule modus tollens (MT), we mentioned that it
is not a primitive rule of natural deduction, but can be derived from some
of the other rules. Here is the derivation of
φ →ψ
¬ψ
¬φ
MT

24
1 Propositional logic
from →e, ¬e and ¬i:
1
φ →ψ
premise
2
¬ψ
premise
3
φ
assumption
4
ψ
→e 1, 3
5
⊥
¬e 4, 2
6
¬φ
¬i 3−5
We could now go back through the proofs in this chapter and replace applica-
tions of MT by this combination of →e, ¬e and ¬i. However, it is convenient
to think of MT as a shorthand (or a macro).
The same holds for the rule
φ
¬¬φ
¬¬i.
It can be derived from the rules ¬i and ¬e, as follows:
1
φ
premise
2
¬φ
assumption
3
⊥
¬e 1, 2
4
¬¬φ
¬i 2−3
There are (unboundedly) many such derived rules which we could write
down. However, there is no point in making our calculus fat and unwieldy;
and some purists would say that we should stick to a minimum set of rules,
all of which are independent of each other. We don’t take such a purist view.
Indeed, the two derived rules we now introduce are extremely useful. You will
ﬁnd that they crop up frequently when doing exercises in natural deduction,
so it is worth giving them names as derived rules. In the case of the second
one, its derivation from the primitive proof rules is not very obvious.
The ﬁrst one has the Latin name reductio ad absurdum. It means ‘reduc-
tion to absurdity’ and we will simply call it proof by contradiction (PBC
for short). The rule says: if from ¬φ we obtain a contradiction, then we are
entitled to deduce φ:
¬φ
...
⊥
φ
PBC.

1.2 Natural deduction
25
This rule looks rather similar to ¬i, except that the negation is in a diﬀerent
place. This is the clue to how to derive PBC from our basic proof rules.
Suppose we have a proof of ⊥from ¬φ. By →i, we can transform this into
a proof of ¬φ →⊥and proceed as follows:
1
¬φ →⊥
given
2
¬φ
assumption
3
⊥
→e 1, 2
4
¬¬φ
¬i 2−3
5
φ
¬¬e 4
This shows that PBC can be derived from →i, ¬i, →e and ¬¬e.
The ﬁnal derived rule we consider in this section is arguably the most
useful to use in proofs, because its derivation is rather long and complicated,
so its usage often saves time and eﬀort. It also has a Latin name, tertium
non datur; the English name is the law of the excluded middle, or LEM for
short. It simply says that φ ∨¬φ is true: whatever φ is, it must be either true
or false; in the latter case, ¬φ is true. There is no third possibility (hence
excluded middle): the sequent ⊢φ ∨¬φ is valid. Its validity is implicit, for
example, whenever you write an if-statement in a programming language:
‘if B {C1} else {C2}’ relies on the fact that B ∨¬B is always true (and
that B and ¬B can never be true at the same time). Here is a proof in
natural deduction that derives the law of the excluded middle from basic
proof rules:
1
¬(φ ∨¬φ)
assumption
2
φ
assumption
3
φ ∨¬φ
∨i1 2
4
⊥
¬e 3, 1
5
¬φ
¬i 2−4
6
φ ∨¬φ
∨i2 5
7
⊥
¬e 6, 1
8
¬¬(φ ∨¬φ)
¬i 1−7
9
φ ∨¬φ
¬¬e 8

26
1 Propositional logic
Example 1.24 Using LEM, we show that p →q ⊢¬p ∨q is valid:
1
p →q
premise
2
¬p ∨p
LEM
3
¬p
assumption
4
¬p ∨q
∨i1 3
5
p
assumption
6
q
→e 1, 5
7
¬p ∨q
∨i2 6
8
¬p ∨q
∨e 2, 3−4, 5−7
It can be diﬃcult to decide which instance of LEM would beneﬁt the progress
of a proof. Can you re-do the example above with q ∨¬q as LEM?
1.2.3 Natural deduction in summary
The proof rules for natural deduction are summarised in Figure 1.2. The
explanation of the rules we have given so far in this chapter is declarative;
we have presented each rule and justiﬁed it in terms of our intuition about
the logical connectives. However, when you try to use the rules yourself,
you’ll ﬁnd yourself looking for a more procedural interpretation; what does
a rule do and how do you use it? For example,
r ∧i says: to prove φ ∧ψ, you must ﬁrst prove φ and ψ separately and then use
the rule ∧i.
r ∧e1 says: to prove φ, try proving φ ∧ψ and then use the rule ∧e1. Actually,
this doesn’t sound like very good advice because probably proving φ ∧ψ will
be harder than proving φ alone. However, you might ﬁnd that you already have
φ ∧ψ lying around, so that’s when this rule is useful. Compare this with the
example sequent in Example 1.15.
r ∨i1 says: to prove φ ∨ψ, try proving φ. Again, in general it is harder to prove
φ than it is to prove φ ∨ψ, so this will usually be useful only if you’ve already
managed to prove φ. For example, if you want to prove q |−p ∨q, you certainly
won’t be able simply to use the rule ∨i1, but ∨i2 will work.
r ∨e has an excellent procedural interpretation. It says: if you have φ ∨ψ, and you
want to prove some χ, then try to prove χ from φ and from ψ in turn. (In those
subproofs, of course you can use the other prevailing premises as well.)
r Similarly, →i says, if you want to prove φ →ψ, try proving ψ from φ (and the
other prevailing premises).
r ¬i says: to prove ¬φ, prove ⊥from φ (and the other prevailing premises).

1.2 Natural deduction
27
The basic rules of natural deduction:
introduction
elimination
∧
φ
ψ
φ ∧ψ
∧i
φ ∧ψ
φ
∧e1
φ ∧ψ
ψ
∧e2
∨
φ
φ ∨ψ
∨i1
ψ
φ ∨ψ
∨i2
φ ∨ψ
φ
...
χ
ψ
...
χ
χ
∨e
→
φ
...
ψ
φ →ψ
→i
φ
φ →ψ
ψ
→e
¬
φ
...
⊥
¬φ
¬i
φ
¬φ
⊥
¬e
⊥
(no introduction rule for ⊥)
⊥
φ
⊥e
¬¬
¬¬φ
φ
¬¬e
Some useful derived rules:
φ →ψ
¬ψ
¬φ
MT
φ
¬¬φ
¬¬i
¬φ
...
⊥
φ
PBC
φ ∨¬φ
LEM
Figure 1.2. Natural deduction rules for propositional logic.

28
1 Propositional logic
At any stage of a proof, it is permitted to introduce any formula as as-
sumption, by choosing a proof rule that opens a box. As we saw, natural
deduction employs boxes to control the scope of assumptions. When an as-
sumption is introduced, a box is opened. Discharging assumptions is achieved
by closing a box according to the pattern of its particular proof rule. It’s
useful to make assumptions by opening boxes. But don’t forget you have to
close them in the manner prescribed by their proof rule.
OK, but how do we actually go about constructing a proof?
Given a sequent, you write its premises at the top of your page and
its conclusion at the bottom. Now, you’re trying to ﬁll in the gap,
which involves working simultaneously on the premises (to bring them to-
wards the conclusion) and on the conclusion (to massage it towards the
premises).
Look ﬁrst at the conclusion. If it is of the form φ →ψ, then apply6 the
rule →i. This means drawing a box with φ at the top and ψ at the bottom.
So your proof, which started out like this:
...
premises
...
φ →ψ
now looks like this:
...
premises
...
φ
assumption
ψ
φ →ψ
→i
You still have to ﬁnd a way of ﬁlling in the gap between the φ and the ψ.
But you now have an extra formula to work with and you have simpliﬁed
the conclusion you are trying to reach.
6 Except in situations such as p →(q →¬r), p ⊢q →¬r where →e produces a simpler proof.

1.2 Natural deduction
29
The proof rule ¬i is very similar to →i and has the same beneﬁcial eﬀect
on your proof attempt. It gives you an extra premise to work with and
simpliﬁes your conclusion.
At any stage of a proof, several rules are likely to be applicable. Before
applying any of them, list the applicable ones and think about which one
is likely to improve the situation for your proof. You’ll ﬁnd that →i and ¬i
most often improve it, so always use them whenever you can. There is no
easy recipe for when to use the other rules; often you have to make judicious
choices.
1.2.4 Provable equivalence
Deﬁnition 1.25 Let φ and ψ be formulas of propositional logic. We say
that φ and ψ are provably equivalent iﬀ(we write ‘iﬀ’ for ‘if, and only
if’ in the sequel) the sequents φ ⊢ψ and ψ ⊢φ are valid; that is, there
is a proof of ψ from φ and another one going the other way around.
As seen earlier, we denote that φ and ψ are provably equivalent by
φ ⊣⊢ψ.
Note that, by Remark 1.12, we could just as well have deﬁned φ ⊣⊢ψ to
mean that the sequent ⊢(φ →ψ) ∧(ψ →φ) is valid; it deﬁnes the same
concept. Examples of provably equivalent formulas are
¬(p ∧q) ⊣⊢¬q ∨¬p
¬(p ∨q) ⊣⊢¬q ∧¬p
p →q ⊣⊢¬q →¬p
p →q ⊣⊢¬p ∨q
p ∧q →p ⊣⊢r ∨¬r
p ∧q →r ⊣⊢p →(q →r).
The
reader
should
prove
all
of
these
six
equivalences
in
natural
deduction.
1.2.5 An aside: proof by contradiction
Sometimes we can’t prove something directly in the sense of taking apart
given assumptions and reasoning with their constituents in a constructive
way. Indeed, the proof system of natural deduction, summarised in Fig-
ure 1.2, speciﬁcally allows for indirect proofs that lack a constructive quality:
for example, the rule
¬φ
...
⊥
φ
PBC

30
1 Propositional logic
allows us to prove φ by showing that ¬φ leads to a contradiction. Although
‘classical logicians’ argue that this is valid, logicians of another kind, called
‘intuitionistic logicians,’ argue that to prove φ you should do it directly,
rather than by arguing merely that ¬φ is impossible. The two other rules
on which classical and intuitionistic logicians disagree are
φ ∨¬φ
LEM
¬¬φ
φ
¬¬e.
Intuitionistic logicians argue that, to show φ ∨¬φ, you have to show φ,
or ¬φ. If neither of these can be shown, then the putative truth of the
disjunction has no justiﬁcation. Intuitionists reject ¬¬e since we have already
used this rule to prove LEM and PBC from rules which the intuitionists do
accept. In the exercises, you are asked to show why the intuitionists also
reject PBC.
Let
us
look
at
a
proof
that
shows
up
this
diﬀerence,
involv-
ing
real
numbers.
Real
numbers
are
ﬂoating
point
numbers
like
23.54721, only some of them might actually be inﬁnitely long such as
23.138592748500123950734 . . ., with no periodic behaviour after the deci-
mal point.
Given a positive real number a and a natural (whole) number b, we can
calculate ab: it is just a times itself, b times, so 22 = 2 · 2 = 4, 23 = 2 · 2 · 2 =
8 and so on. When b is a real number, we can also deﬁne ab, as follows.
We say that a0 def
= 1 and, for a non-zero rational number k/n, where n ̸= 0,
we let ak/n def
=
n√
ak where
n√x is the real number y such that yn = x. From
real analysis one knows that any real number b can be approximated by a
sequence of rational numbers k0/n0, k1/n1, . . . Then we deﬁne ab to be the
real number approximated by the sequence ak0/n0, ak1/n1, . . . (In calculus,
one can show that this ‘limit’ ab is unique and independent of the choice of
approximating sequence.) Also, one calls a real number irrational if it can’t
be written in the form k/n for some integers k and n ̸= 0. In the exercises
you will be asked to ﬁnd a semi-formal proof showing that
√
2 is irrational.
We now present a proof of a fact about real numbers in the informal style
used by mathematicians (this proof can be formalised as a natural deduction
proof in the logic presented in Chapter 2). The fact we prove is:
Theorem 1.26 There exist irrational numbers a and b such that ab is ra-
tional.
Proof: We choose b to be
√
2 and proceed by a case analysis. Either bb is
irrational, or it is not. (Thus, our proof uses ∨e on an instance of LEM.)

1.3 Propositional logic as a formal language
31
(i)
Assume that bb is rational. Then this proof is easy since we can choose irra-
tional numbers a and b to be
√
2 and see that ab is just bb which was assumed
to be rational.
(ii)
Assume that bb is irrational. Then we change our strategy slightly and choose
a to be
√
2
√
2. Clearly, a is irrational by the assumption of case (ii). But we
know that b is irrational (this was known by the ancient Greeks; see the proof
outline in the exercises). So a and b are both irrational numbers and
ab = (
√
2
√
2)
√
2
=
√
2
(
√
2·
√
2) = (
√
2)
2 = 2
is rational, where we used the law (xy)z = x(y·z).
Since the two cases above are exhaustive (either bb is irrational, or it isn’t)
we have proven the theorem.
2
This proof is perfectly legitimate and mathematicians use arguments like
that all the time. The exhaustive nature of the case analysis above rests on
the use of the rule LEM, which we use to prove that either b is rational or it
is not. Yet, there is something puzzling about it. Surely, we have secured the
fact that there are irrational numbers a and b such that ab is rational, but
are we in a position to specify an actual pair of such numbers satisfying this
theorem? More precisely, which of the pairs (a, b) above fulﬁls the assertion
of the theorem, the pair (
√
2,
√
2), or the pair (
√
2
√
2,
√
2)? Our proof tells
us nothing about which of them is the right choice; it just says that at least
one of them works.
Thus, the intuitionists favour a calculus containing the introduction and
elimination rules shown in Figure 1.2 and excluding the rule ¬¬e and the
derived rules. Intuitionistic logic turns out to have some specialised applica-
tions in computer science, such as modelling type-inference systems used in
compilers or the staged execution of program code; but in this text we stick
to the full so-called classical logic which includes all the rules.
1.3 Propositional logic as a formal language
In the previous section we learned about propositional atoms and how they
can be used to build more complex logical formulas. We were deliberately
informal about that, for our main focus was on trying to understand the
precise mechanics of the natural deduction rules. However, it should have
been clear that the rules we stated are valid for any formulas we can form, as
long as they match the pattern required by the respective rule. For example,

32
1 Propositional logic
the application of the proof rule →e in
1
p →q
premise
2
p
premise
3
q
→e 1, 2
is equally valid if we substitute p with p ∨¬r and q with r →p:
1
p ∨¬r →(r →p)
premise
2
p ∨¬r
premise
3
r →p
→e 1, 2
This is why we expressed such rules as schemes with Greek symbols stand-
ing for generic formulas. Yet, it is time that we make precise the notion of
‘any formula we may form.’ Because this text concerns various logics, we will
introduce in (1.3) an easy formalism for specifying well-formed formulas. In
general, we need an unbounded supply of propositional atoms p, q, r, . . ., or
p1, p2, p3, . . . You should not be too worried about the need for inﬁnitely
many such symbols. Although we may only need ﬁnitely many of these
propositions to describe a property of a computer program successfully, we
cannot specify how many such atomic propositions we will need in any con-
crete situation, so having inﬁnitely many symbols at our disposal is a cheap
way out. This can be compared with the potentially inﬁnite nature of En-
glish: the number of grammatically correct English sentences is inﬁnite, but
ﬁnitely many such sentences will do in whatever situation you might be in
(writing a book, attending a lecture, listening to the radio, having a dinner
date, . . . ).
Formulas in our propositional logic should certainly be strings over the
alphabet {p, q, r, . . . } ∪{p1, p2, p3, . . . } ∪{¬, ∧, ∨, →, (, )}. This is a trivial
observation and as such is not good enough for what we are trying to capture.
For example, the string (¬)() ∨pq →is a word over that alphabet, yet, it
does not seem to make a lot of sense as far as propositional logic is concerned.
So what we have to deﬁne are those strings which we want to call formulas.
We call such formulas well-formed.
Deﬁnition 1.27 The well-formed formulas of propositional logic are those
which we obtain by using the construction rules below, and only those,
ﬁnitely many times:

1.3 Propositional logic as a formal language
33
atom: Every propositional atom p, q, r, . . . and p1, p2, p3, . . . is a well-
formed formula.
¬: If φ is a well-formed formula, then so is (¬φ).
∧: If φ and ψ are well-formed formulas, then so is (φ ∧ψ).
∨: If φ and ψ are well-formed formulas, then so is (φ ∨ψ).
→: If φ and ψ are well-formed formulas, then so is (φ →ψ).
It is most crucial to realize that this deﬁnition is the one a computer would
expect and that we did not make use of the binding priorities agreed upon
in the previous section.
Convention. In this section we act as if we are a rigorous computer and
we call formulas well-formed iﬀthey can be deduced to be so using the
deﬁnition above.
Further, note that the condition ‘and only those’ in the deﬁnition above
rules out the possibility of any other means of establishing that formulas are
well-formed. Inductive deﬁnitions, like the one of well-formed propositional
logic formulas above, are so frequent that they are often given by a deﬁning
grammar in Backus Naur form (BNF). In that form, the above deﬁnition
reads more compactly as
φ ::= p | (¬φ) | (φ ∧φ) | (φ ∨φ) | (φ →φ)
(1.3)
where p stands for any atomic proposition and each occurrence of φ to the
right of ::= stands for any already constructed formula.
So how can we show that a string is a well-formed formula? For example,
how do we answer this for φ being
(((¬p) ∧q) →(p ∧(q ∨(¬r)))) ?
(1.4)
Such reasoning is greatly facilitated by the fact that the grammar in (1.3)
satisﬁes the inversion principle, which means that we can invert the process
of building formulas: although the grammar rules allow for ﬁve diﬀerent ways
of constructing more complex formulas – the ﬁve clauses in (1.3) – there is
always a unique clause which was used last. For the formula above, this
last operation was an application of the ﬁfth clause, for φ is an implication
with the assumption ((¬p) ∧q) and conclusion (p ∧(q ∨(¬r))). By applying
the inversion principle to the assumption, we see that it is a conjunction of
(¬p) and q. The former has been constructed using the second clause and
is well-formed since p is well-formed by the ﬁrst clause in (1.3). The latter
is well-formed for the same reason. Similarly, we can apply the inversion

34
1 Propositional logic
→
∧
¬
p
q
∧
p
∨
q
¬
r
Figure 1.3. A parse tree representing a well-formed formula.
principle to the conclusion (p ∧(q ∨(¬r))), inferring that it is indeed well-
formed. In summary, the formula in (1.4) is well-formed.
For us humans, dealing with brackets is a tedious task. The reason
we need them is that formulas really have a tree-like structure, although
we prefer to represent them in a linear way. In Figure 1.3 you can see the
parse tree7 of the well-formed formula φ in (1.4). Note how brackets become
unnecessary in this parse tree since the paths and the branching structure
of this tree remove any possible ambiguity in interpreting φ. In representing
φ as a linear string, the branching structure of the tree is retained by the
insertion of brackets as done in the deﬁnition of well-formed formulas.
So how would you go about showing that a string of symbols ψ is not well-
formed? At ﬁrst sight, this is a bit trickier since we somehow have to make
sure that ψ could not have been obtained by any sequence of construction
rules. Let us look at the formula (¬)() ∨pq →from above. We can decide
this matter by being very observant. The string (¬)() ∨pq →contains ¬)
and ¬ cannot be the rightmost symbol of a well-formed formula (check all
the rules to verify this claim!); but the only time we can put a ‘)’ to the right
of something is if that something is a well-formed formula (again, check all
the rules to see that this is so). Thus, (¬)() ∨pq →is not well-formed.
Probably the easiest way to verify whether some formula φ is well-formed
is by trying to draw its parse tree. In this way, you can verify that the
7 We will use this name without explaining it any further and are conﬁdent that you will under-
stand its meaning through the examples.

1.3 Propositional logic as a formal language
35
formula in (1.4) is well-formed. In Figure 1.3 we see that its parse tree has
→as its root, expressing that the formula is, at its top level, an implication.
Using the grammar clause for implication, it suﬃces to show that the left
and right subtrees of this root node are well-formed. That is, we proceed in
a top-down fashion and, in this case, successfully. Note that the parse trees
of well-formed formulas have either an atom as root (and then this is all
there is in the tree), or the root contains ¬, ∨, ∧or →. In the case of ¬
there is only one subtree coming out of the root. In the cases ∧, ∨or →we
must have two subtrees, each of which must behave as just described; this
is another example of an inductive deﬁnition.
Thinking in terms of trees will help you understand standard notions
in logic, for example, the concept of a subformula. Given the well-formed
formula φ above, its subformulas are just the ones that correspond to the
subtrees of its parse tree in Figure 1.3. So we can list all its leaves p, q
(occurring twice), and r, then (¬p) and ((¬p) ∧q) on the left subtree of →
and (¬r), (q ∨(¬r)) and ((p ∧(q ∨(¬p)))) on the right subtree of →. The
whole tree is a subtree of itself as well. So we can list all nine subformulas
of φ as
p
q
r
(¬p)
((¬p) ∧q)
(¬r)
(q ∨(¬r))
((p ∧(q ∨(¬r))))
(((¬p) ∧q) →(p ∧(q ∨(¬r)))).
Let us consider the tree in Figure 1.4. Why does it represent a well-formed
formula? All its leaves are propositional atoms (p twice, q and r), all branch-
ing nodes are logical connectives (¬ twice, ∧, ∨and →) and the numbers
of subtrees are correct in all those cases (one subtree for a ¬ node and two
subtrees for all other non-leaf nodes). How do we obtain the linear represen-
tation of this formula? If we ignore brackets, then we are seeking nothing but
the in-order representation of this tree as a list8. The resulting well-formed
formula is ((¬(p ∨(q →(¬p)))) ∧r).
8 The other common ways of ﬂattening trees to lists are preordering and postordering. See any
text on binary trees as data structures for further details.

36
1 Propositional logic
∧
r
¬
∨
p
→
q
¬
p
Figure 1.4. Given: a tree; wanted: its linear representation as a logical
formula.
The tree in Figure 1.21 on page 82, however, does not represent a well-
formed formula for two reasons. First, the leaf ∧(and a similar argument
applies to the leaf ¬), the left subtree of the node →, is not a propositional
atom. This could be ﬁxed by saying that we decided to leave the left and
right subtree of that node unspeciﬁed and that we are willing to provide
those now. However, the second reason is fatal. The p node is not a leaf
since it has a subtree, the node ¬. This cannot make sense if we think of
the entire tree as some logical formula. So this tree does not represent a
well-formed logical formula.
1.4 Semantics of propositional logic
1.4.1 The meaning of logical connectives
In the second section of this chapter, we developed a calculus of reasoning
which could verify that sequents of the form φ1, φ2, . . . , φn ⊢ψ are valid,
which means: from the premises φ1, φ2, . . . , φn, we may conclude ψ.
In this section we give another account of this relationship between the
premises φ1, φ2, . . . , φn and the conclusion ψ. To contrast with the sequent

1.4 Semantics of propositional logic
37
above, we deﬁne a new relationship, written
φ1, φ2, . . . , φn ⊨ψ.
This account is based on looking at the ‘truth values’ of the atomic formu-
las in the premises and the conclusion; and at how the logical connectives
manipulate these truth values. What is the truth value of a declarative sen-
tence, like sentence (3) ‘Every even natural number > 2 is the sum of two
prime numbers’? Well, declarative sentences express a fact about the real
world, the physical world we live in, or more abstract ones such as computer
models, or our thoughts and feelings. Such factual statements either match
reality (they are true), or they don’t (they are false).
If we combine declarative sentences p and q with a logical connective, say
∧, then the truth value of p ∧q is determined by three things: the truth value
of p, the truth value of q and the meaning of ∧. The meaning of ∧is captured
by the observation that p ∧q is true iﬀp and q are both true; otherwise p ∧q
is false. Thus, as far as ∧is concerned, it needs only to know whether p and
q are true, it does not need to know what p and q are actually saying about
the world out there. This is also the case for all the other logical connectives
and is the reason why we can compute the truth value of a formula just by
knowing the truth values of the atomic propositions occurring in it.
Deﬁnition 1.28 1.
The set of truth values contains two elements T and F, where
T represents ‘true’ and F represents ‘false’.
2.
A valuation or model of a formula φ is an assignment of each propositional atom
in φ to a truth value.
Example 1.29 The map which assigns T to q and F to p is a valuation for
p ∨¬q. Please list the remaining three valuations for this formula.
We can think of the meaning of ∧as a function of two arguments; each
argument is a truth value and the result is again such a truth value. We
specify this function in a table, called the truth table for conjunction, which
you can see in Figure 1.5. In the ﬁrst column, labelled φ, we list all possible
φ
ψ
φ ∧ψ
T
T
T
T
F
F
F
T
F
F
F
F
Figure 1.5. The truth table for conjunction, the logical connective ∧.

38
1 Propositional logic
φ
ψ
φ ∧ψ
T
T
T
T
F
F
F
T
F
F
F
F
φ
ψ
φ ∨ψ
T
T
T
T
F
T
F
T
T
F
F
F
φ
ψ
φ →ψ
T
T
T
T
F
F
F
T
T
F
F
T
φ
¬φ
T
F
F
T
⊤
T
⊥
F
Figure 1.6. The truth tables for all the logical connectives discussed so far.
truth values of φ. Actually we list them twice since we also have to deal
with another formula ψ, so the possible number of combinations of truth
values for φ and ψ equals 2 · 2 = 4. Notice that the four pairs of φ and ψ
values in the ﬁrst two columns really exhaust all those possibilities (TT, TF,
FT and FF). In the third column, we list the result of φ ∧ψ according to the
truth values of φ and ψ. So in the ﬁrst line, where φ and ψ have value T,
the result is T again. In all other lines, the result is F since at least one of
the propositions φ or ψ has value F.
In Figure 1.6 you ﬁnd the truth tables for all logical connectives of propo-
sitional logic. Note that ¬ turns T into F and vice versa. Disjunction is the
mirror image of conjunction if we swap T and F, namely, a disjunction re-
turns F iﬀboth arguments are equal to F, otherwise (= at least one of the
arguments equals T) it returns T. The behaviour of implication is not quite
as intuitive. Think of the meaning of →as checking whether truth is being
preserved. Clearly, this is not the case when we have T →F, since we infer
something that is false from something that is true. So the second entry
in the column φ →ψ equals F. On the other hand, T →T obviously pre-
serves truth, but so do the cases F →T and F →F, because there is no truth
to be preserved in the ﬁrst place as the assumption of the implication is
false.
If you feel slightly uncomfortable with the semantics (= the meaning)
of →, then it might be good to think of φ →ψ as an abbreviation of the
formula ¬φ ∨ψ as far as meaning is concerned; these two formulas are very
diﬀerent syntactically and natural deduction treats them diﬀerently as well.
But using the truth tables for ¬ and ∨you can check that φ →ψ evaluates

1.4 Semantics of propositional logic
39
to T iﬀ¬φ ∨ψ does so. This means that φ →ψ and ¬φ ∨ψ are semantically
equivalent; more on that in Section 1.5.
Given a formula φ which contains the propositional atoms p1, p2, . . . , pn,
we can construct a truth table for φ, at least in principle. The caveat is that
this truth table has 2n many lines, each line listing a possible combination
of truth values for p1, p2, . . . , pn; and for large n this task is impossible to
complete. Our aim is thus to compute the value of φ for each of these 2n
cases for moderately small values of n. Let us consider the example φ in
Figure 1.3. It involves three propositional atoms (n = 3) so we have 23 = 8
cases to consider.
We illustrate how things go for one particular case, namely for the val-
uation in which q evaluates to F; and p and r evaluate to T. What does
¬p ∧q →p ∧(q ∨¬r) evaluate to? Well, the beauty of our semantics is that
it is compositional. If we know the meaning of the subformulas ¬p ∧q and
p ∧(q ∨¬r), then we just have to look up the appropriate line of the →
truth table to ﬁnd the value of φ, for φ is an implication of these two sub-
formulas. Therefore, we can do the calculation by traversing the parse tree
of φ in a bottom-up fashion. We know what its leaves evaluate to since we
stated what the atoms p, q and r evaluated to. Because the meaning of p is
T, we see that ¬p computes to F. Now q is assumed to represent F and the
conjunction of F and F is F. Thus, the left subtree of the node →evaluates
to F. As for the right subtree of →, r stands for T so ¬r computes to F and q
means F, so the disjunction of F and F is still F. We have to take that result,
F, and compute its conjunction with the meaning of p which is T. Since the
conjunction of T and F is F, we get F as the meaning of the right subtree
of →. Finally, to evaluate the meaning of φ, we compute F →F which is T.
Figure 1.7 shows how the truth values propagate upwards to reach the root
whose associated truth value is the truth value of φ given the meanings of
p, q and r above.
It should now be quite clear how to build a truth table for more com-
plex formulas. Figure 1.8 contains a truth table for the formula (p →¬q) →
(q ∨¬p). To be more precise, the ﬁrst two columns list all possible combina-
tions of values for p and q. The next two columns compute the corresponding
values for ¬p and ¬q. Using these four columns, we may compute the column
for p →¬q and q ∨¬p. To do so we think of the ﬁrst and fourth columns
as the data for the →truth table and compute the column of p →¬q ac-
cordingly. For example, in the ﬁrst line p is T and ¬q is F so the entry for
p →¬q is T →F = F by deﬁnition of the meaning of →. In this fashion, we
can ﬁll out the rest of the ﬁfth column. Column 6 works similarly, only we
now need to look up the truth table for ∨with columns 2 and 3 as input.

40
1 Propositional logic
p
q
p
r
T
T
F
T
F
F
F
F
¬
F
∧
∨
q
T
F
∧
¬
→
Figure 1.7. The evaluation of a logical formula under a given valuation.
p
q
¬p
¬q
p →¬q
q ∨¬p
(p →¬q) →(q ∨¬p)
T
T
F
F
F
T
T
T
F
F
T
T
F
F
F
T
T
F
T
T
T
F
F
T
T
T
T
T
Figure 1.8. An example of a truth table for a more complex logical formula.
Finally, column 7 results from applying the truth table of →to columns 5
and 6.
1.4.2 Mathematical induction
Here is a little anecdote about the German mathematician Gauss who, as a
pupil at age 8, did not pay attention in class (can you imagine?), with the
result that his teacher made him sum up all natural numbers from 1 to 100.
The story has it that Gauss came up with the correct answer 5050 within
seconds, which infuriated his teacher. How did Gauss do it? Well, possibly
he knew that
1 + 2 + 3 + 4 + · · · + n = n · (n + 1)
2
(1.5)

1.4 Semantics of propositional logic
41
for all natural numbers n.9 Thus, taking n = 100, Gauss could easily calcu-
late:
1 + 2 + 3 + 4 + · · · + 100 = 100 · 101
2
= 5050.
Mathematical induction allows us to prove equations, such as the one
in (1.5), for arbitrary n. More generally, it allows us to show that every
natural number satisﬁes a certain property. Suppose we have a property M
which we think is true of all natural numbers. We write M(5) to say that
the property is true of 5, etc. Suppose that we know the following two things
about the property M:
1.
Base case: The natural number 1 has property M, i.e. we have a proof of
M(1).
2.
Inductive step: If n is a natural number which we assume to have property
M(n), then we can show that n + 1 has property M(n + 1); i.e. we have a proof
of M(n) →M(n + 1).
Deﬁnition 1.30 The principle of mathematical induction says that, on the
grounds of these two pieces of information above, every natural number n
has property M(n). The assumption of M(n) in the inductive step is called
the induction hypothesis.
Why does this principle make sense? Well, take any natural number k.
If k equals 1, then k has property M(1) using the base case and so we are
done. Otherwise, we can use the inductive step, applied to n = 1, to infer
that 2 = 1 + 1 has property M(2). We can do that using →e, for we know
that 1 has the property in question. Now we use that same inductive step on
n = 2 to infer that 3 has property M(3) and we repeat this until we reach
n = k (see Figure 1.9). Therefore, we should have no objections about using
the principle of mathematical induction for natural numbers.
Returning to Gauss’ example we claim that the sum 1 + 2 + 3 + 4 + · · · +
n equals n · (n + 1)/2 for all natural numbers n.
Theorem 1.31 The sum 1 + 2 + 3 + 4 + · · · + n equals n · (n + 1)/2 for all
natural numbers n.
9 There is another way of ﬁnding the sum 1 + 2 + · · · + 100, which works like this: write the
sum backwards, as 100 + 99 + · · · + 1. Now add the forwards and backwards versions, obtaining
101 + 101 + · · · + 101 (100 times), which is 10100. Since we added the sum to itself, we now
divide by two to get the answer 5050. Gauss probably used this method; but the method of
mathematical induction that we explore in this section is much more powerful and can be
applied in a wide variety of situations.

42
1 Propositional logic
We prove M(1)
...
...
1
2
3
...
n + 1
n
We prove M(2) using M(1) and M(1) →M(2)
We prove M(3) using M(2) and M(2) →M(3)
We prove M(n) using M(n −1) and M(n −1) →M(n)
We prove M(n + 1) using M(n) and M(n) →M(n + 1)
Figure 1.9. How the principle of mathematical induction works. By
proving just two facts, M(1) and M(n) →M(n + 1) for a formal (and
unconstrained) parameter n, we are able to deduce M(k) for each natural
number k.
Proof: We use mathematical induction. In order to reveal the ﬁne structure
of our proof we write LHSn for the expression 1 + 2 + 3 + 4 + · · · + n and
RHSn for n · (n + 1)/2. Thus, we need to show LHSn = RHSn for all n ≥1.
Base case: If n equals 1, then LHS1 is just 1 (there is only one summand),
which happens to equal RHS1 = 1 · (1 + 1)/2.
Inductive step: Let us assume that LHSn = RHSn. Recall that this as-
sumption is called the induction hypothesis; it is the driving force of
our argument. We need to show LHSn+1 = RHSn+1, i.e. that the longer
sum 1 + 2 + 3 + 4 + · · · + (n + 1) equals (n + 1) · ((n + 1) + 1)/2. The key
observation is that the sum 1 + 2 + 3 + 4 + · · · + (n + 1) is nothing but
the sum (1 + 2 + 3 + 4 + · · · + n) + (n + 1) of two summands, where the
ﬁrst one is the sum of our induction hypothesis. The latter says that
1 + 2 + 3 + 4 + · · · + n equals n · (n + 1)/2, and we are certainly entitled
to substitute equals for equals in our reasoning. Thus, we compute
LHSn+1
= 1 + 2 + 3 + 4 + · · · + (n + 1)
= LHSn + (n + 1) regrouping the sum

1.4 Semantics of propositional logic
43
= RHSn + (n + 1) by our induction hypothesis
= n·(n+1)
2
+ (n + 1)
= n·(n+1)
2
+ 2·(n+1)
2
arithmetic
= (n+2)·(n+1)
2
arithmetic
= ((n+1)+1)·(n+1)
2
arithmetic
= RHSn+1.
Since we successfully showed the base case and the inductive step, we can
use mathematical induction to infer that all natural numbers n have the
property stated in the theorem above.
2
Actually, there are numerous variations of this principle. For example, we
can think of a version in which the base case is n = 0, which would then
cover all natural numbers including 0. Some statements hold only for all
natural numbers, say, greater than 3. So you would have to deal with a
base case 4, but keep the version of the inductive step (see the exercises for
such an example). The use of mathematical induction typically suceeds on
properties M(n) that involve inductive deﬁnitions (e.g. the deﬁnition of kl
with l ≥0). Sentence (3) on page 2 suggests there may be true properties
M(n) for which mathematical induction won’t work.
Course-of-values induction.
There is a variant of mathematical induction
in which the induction hypothesis for proving M(n + 1) is not just M(n), but
the conjunction M(1) ∧M(2) ∧· · · ∧M(n). In that variant, called course-
of-values induction, there doesn’t have to be an explicit base case at all –
everything can be done in the inductive step.
How can this work without a base case? The answer is that the base
case is implicitly included in the inductive step. Consider the case n = 3:
the inductive-step instance is M(1) ∧M(2) ∧M(3) →M(4). Now consider
n = 1: the inductive-step instance is M(1) →M(2). What about the case
when n equals 0? In this case, there are zero formulas on the left of the →,
so we have to prove M(1) from nothing at all. The inductive-step instance
is simply the obligation to show M(1). You might ﬁnd it useful to modify
Figure 1.9 for course-of-values induction.
Having said that the base case is implicit in course-of-values induction,
it frequently turns out that it still demands special attention when you get
inside trying to prove the inductive case. We will see precisely this in the
two applications of course-of-values induction in the following pages.

44
1 Propositional logic
¬
∧
→
→
q
¬
p
∨
r
q
p
Figure 1.10. A parse tree with height 5.
In computer science, we often deal with ﬁnite structures of some kind, data
structures, programs, ﬁles etc. Often we need to show that every instance of
such a structure has a certain property. For example, the well-formed for-
mulas of Deﬁnition 1.27 have the property that the number of ‘(’ brackets
in a particular formula equals its number of ‘)’ brackets. We can use mathe-
matical induction on the domain of natural numbers to prove this. In order
to succeed, we somehow need to connect well-formed formulas to natural
numbers.
Deﬁnition 1.32 Given a well-formed formula φ, we deﬁne its height to be
1 plus the length of the longest path of its parse tree.
For example, consider the well-formed formulas in Figures 1.3, 1.4
and 1.10. Their heights are 5, 6 and 5, respectively. In Figure 1.3, the
longest path goes from →to ∧to ∨to ¬ to r, a path of length 4, so
the height is 4 + 1 = 5. Note that the height of atoms is 1 + 0 = 1. Since
every well-formed formula has ﬁnite height, we can show statements about
all well-formed formulas by mathematical induction on their height. This
trick is most often called structural induction, an important reasoning tech-
nique in computer science. Using the notion of the height of a parse tree,
we realise that structural induction is just a special case of course-of-values
induction.

1.4 Semantics of propositional logic
45
Theorem 1.33 For every well-formed propositional logic formula, the num-
ber of left brackets is equal to the number of right brackets.
Proof: We proceed by course-of-values induction on the height of well-
formed formulas φ. Let M(n) mean ‘All formulas of height n have the same
number of left and right brackets.’ We assume M(k) for each k < n and try
to prove M(n). Take a formula φ of height n.
r Base case: Then n = 1. This means that φ is just a propositional atom. So there
are no left or right brackets, 0 equals 0.
r Course-of-values inductive step: Then n > 1 and so the root of the parse tree
of φ must be ¬, →, ∨or ∧, for φ is well-formed. We assume that it is →, the other
three cases are argued in a similar way. Then φ equals (φ1 →φ2) for some well-
formed formulas φ1 and φ2 (of course, they are just the left, respectively right,
linear representations of the root’s two subtrees). It is clear that the heights
of φ1 and φ2 are strictly smaller than n. Using the induction hypothesis, we
therefore conclude that φ1 has the same number of left and right brackets and
that the same is true for φ2. But in (φ1 →φ2) we added just two more brackets,
one ‘(’ and one ‘)’. Thus, the number of occurrences of ‘(’ and ‘)’ in φ is the
same.
2
The formula (p →(q ∧¬r)) illustrates why we could not prove the above
directly with mathematical induction on the height of formulas. While this
formula has height 4, its two subtrees have heights 1 and 3, respectively.
Thus, an induction hypothesis for height 3 would have worked for the right
subtree but failed for the left subtree.
1.4.3 Soundness of propositional logic
The natural deduction rules make it possible for us to develop rigorous
threads of argumentation, in the course of which we arrive at a conclusion
ψ assuming certain other propositions φ1, φ2, . . . , φn. In that case, we said
that the sequent φ1, φ2, . . . , φn ⊢ψ is valid. Do we have any evidence that
these rules are all correct in the sense that valid sequents all ‘preserve truth’
computed by our truth-table semantics?
Given a proof of φ1, φ2, . . . , φn ⊢ψ, is it conceivable that there is a valu-
ation in which ψ above is false although all propositions φ1, φ2, . . . , φn are
true? Fortunately, this is not the case and in this subsection we demonstrate
why this is so. Let us suppose that some proof in our natural deduction cal-
culus has established that the sequent φ1, φ2, . . . , φn ⊢ψ is valid. We need
to show: for all valuations in which all propositions φ1, φ2, . . . , φn evaluate
to T, ψ evaluates to T as well.

46
1 Propositional logic
Deﬁnition 1.34 If, for all valuations in which all φ1, φ2, . . . , φn evaluate to
T, ψ evaluates to T as well, we say that
φ1, φ2, . . . , φn ⊨ψ
holds and call ⊨the semantic entailment relation.
Let us look at some examples of this notion.
1.
Does p ∧q ⊨p hold? Well, we have to inspect all assignments of truth values to
p and q; there are four of these. Whenever such an assignment computes T for
p ∧q we need to make sure that p is true as well. But p ∧q computes T only if
p and q are true, so p ∧q ⊨p is indeed the case.
2.
What about the relationship p ∨q ⊨p? There are three assignments for which
p ∨q computes T, so p would have to be true for all of these. However, if we
assign T to q and F to p, then p ∨q computes T, but p is false. Thus, p ∨q ⊨p
does not hold.
3.
What if we modify the above to ¬q, p ∨q ⊨p? Notice that we have to be con-
cerned only about valuations in which ¬q and p ∨q evaluate to T. This forces q
to be false, which in turn forces p to be true. Hence ¬q, p ∨q ⊨p is the case.
4.
Note that p ⊨q ∨¬q holds, despite the fact that no atomic proposition on the
right of ⊨occurs on the left of ⊨.
From the discussion above we realize that a soundness argument has to show:
if φ1, φ2, . . . , φn ⊢ψ is valid, then φ1, φ2, . . . , φn ⊨ψ holds.
Theorem 1.35 (Soundness) Let φ1, φ2, . . . , φn and ψ be propositional
logic formulas. If φ1, φ2, . . . , φn ⊢ψ is valid, then φ1, φ2, . . . , φn ⊨ψ holds.
Proof:
Since φ1, φ2, . . . , φn ⊢ψ is valid we know there is a proof of ψ
from the premises φ1, φ2, . . . , φn. We now do a pretty slick thing, namely,
we reason by mathematical induction on the length of this proof! The length
of a proof is just the number of lines it involves. So let us be perfectly
clear about what it is we mean to show. We intend to show the assertion
M(k):
‘For all sequents φ1, φ2, . . . , φn ⊢ψ (n ≥0) which have a proof of
length k, it is the case that φ1, φ2, . . . , φn ⊨ψ holds.’
by course-of-values induction on the natural number k. This idea requires

1.4 Semantics of propositional logic
47
some work, though. The sequent p ∧q →r ⊢p →(q →r) has a proof
1
p ∧q →r
premise
2
p
assumption
3
q
assumption
4
p ∧q
∧i 2, 3
5
r
→e 1, 4
6
q →r
→i 3−5
7
p →(q →r)
→i 2−6
but if we remove the last line or several of the last lines, we no longer
have a proof as the outermost box does not get closed. We get a complete
proof, though, by removing the last line and re-writing the assumption of
the outermost box as a premise:
1
p ∧q →r
premise
2
p
premise
3
q
assumption
4
p ∧q
∧i 2, 3
5
r
→e 1, 4
6
q →r
→i 3−5
This is a proof of the sequent p ∧q →r, p ⊢p →r. The induction hypothesis
then ensures that p ∧q →r, p ⊨p →r holds. But then we can also reason
that p ∧q →r ⊨p →(q →r) holds as well – why?
Let’s proceed with our proof by induction. We assume M(k′) for each
k′ < k and we try to prove M(k).
Base case: a one-line proof.
If the proof has length 1 (k = 1), then it must
be of the form
1
φ
premise
since all other rules involve more than one line. This is the case when n = 1
and φ1 and ψ equal φ, i.e. we are dealing with the sequent φ ⊢φ. Of course,
since φ evaluates to T so does φ. Thus, φ ⊨φ holds as claimed.

48
1 Propositional logic
Course-of-values inductive step:
Let us assume that the proof of the se-
quent φ1, φ2, . . . , φn ⊢ψ has length k and that the statement we want to
prove is true for all numbers less than k. Our proof has the following struc-
ture:
1
φ1 premise
2
φ2 premise
...
n
φn premise
...
k
ψ justiﬁcation
There are two things we don’t know at this point. First, what is happening
in between those dots? Second, what was the last rule applied, i.e. what is
the justiﬁcation of the last line? The ﬁrst uncertainty is of no concern; this
is where mathematical induction demonstrates its power. The second lack
of knowledge is where all the work sits. In this generality, there is simply no
way of knowing which rule was applied last, so we need to consider all such
rules in turn.
1.
Let us suppose that this last rule is ∧i. Then we know that ψ is of the form
ψ1 ∧ψ2 and the justiﬁcation in line k refers to two lines further up which have
ψ1, respectively ψ2, as their conclusions. Suppose that these lines are k1 and k2.
Since k1 and k2 are smaller than k, we see that there exist proofs of the sequents
φ1, φ2, . . . , φn ⊢ψ1 and φ1, φ2, . . . , φn ⊢ψ2 with length less than k – just take
the ﬁrst k1, respectively k2, lines of our original proof. Using the induction
hypothesis, we conclude that φ1, φ2, . . . , φn ⊨ψ1 and φ1, φ2, . . . , φn ⊨ψ2 holds.
But these two relations imply that φ1, φ2, . . . , φn ⊨ψ1 ∧ψ2 holds as well – why?
2.
If ψ has been shown using the rule ∨e, then we must have proved, as-
sumed or given as a premise some formula η1 ∨η2 in some line k′ with
k′ < k, which was referred to via ∨e in the justiﬁcation of line k. Thus,
we have a shorter proof of the sequent φ1, φ2, . . . , φn ⊢η1 ∨η2 within that
proof, obtained by turning all assumptions of boxes that are open at
line k′ into premises. In a similar way we obtain proofs of the sequents
φ1, φ2, . . . , φn, η1 ⊢ψ and φ1, φ2, . . . , φn, η2 ⊢ψ from the case analysis of ∨e.
By our induction hypothesis, we conclude that the relations φ1, φ2, . . . , φn ⊨
η1 ∨η2, φ1, φ2, . . . , φn, η1 ⊨ψ and φ1, φ2, . . . , φn, η2 ⊨ψ hold. But together
these three relations then force that φ1, φ2, . . . , φn ⊨ψ holds as well –
why?
3.
You can guess by now that the rest of the argument checks each possible proof
rule in turn and ultimately boils down to verifying that our natural deduction

1.4 Semantics of propositional logic
49
rules behave semantically in the same way as their corresponding truth tables
evaluate. We leave the details as an exercise.
2
The soundness of propositional logic is useful in ensuring the non-existence of
a proof for a given sequent. Let’s say you try to prove that φ1, φ2, . . . , φ2 ⊢ψ
is valid, but that your best eﬀorts won’t succeed. How could you be sure that
no such proof can be found? After all, it might just be that you can’t ﬁnd
a proof even though there is one. It suﬃces to ﬁnd a valuation in which φi
evaluate to T whereas ψ evaluates to F. Then, by deﬁnition of ⊨, we don’t
have φ1, φ2, . . . , φ2 ⊨ψ. Using soundness, this means that φ1, φ2, . . . , φ2 ⊢ψ
cannot be valid. Therefore, this sequent does not have a proof. You will
practice this method in the exercises.
1.4.4 Completeness of propositional logic
In this subsection, we hope to convince you that the natural deduction rules
of propositional logic are complete: whenever φ1, φ2, . . . , φn ⊨ψ holds, then
there exists a natural deduction proof for the sequent φ1, φ2, . . . , φn ⊢ψ.
Combined with the soundness result of the previous subsection, we then
obtain
φ1, φ2, . . . , φn ⊢ψ is valid iﬀφ1, φ2, . . . , φn ⊨ψ holds.
This gives you a certain freedom regarding which method you prefer to
use. Often it is much easier to show one of these two relationships (al-
though neither of the two is universally better, or easier, to establish).
The ﬁrst method involves a proof search, upon which the logic program-
ming paradigm is based. The second method typically forces you to com-
pute a truth table which is exponential in the size of occurring proposi-
tional atoms. Both methods are intractable in general but particular in-
stances of formulas often respond diﬀerently to treatment under these two
methods.
The remainder of this section is concerned with an argument saying that
if φ1, φ2, . . . , φn ⊨ψ holds, then φ1, φ2, . . . , φn ⊢ψ is valid. Assuming that
φ1, φ2, . . . , φn ⊨ψ holds, the argument proceeds in three steps:
Step 1: We show that ⊨φ1 →(φ2 →(φ3 →(. . . (φn →ψ) . . . ))) holds.
Step 2: We show that ⊢φ1 →(φ2 →(φ3 →(. . . (φn →ψ) . . . ))) is valid.
Step 3: Finally, we show that φ1, φ2, . . . , φn ⊢ψ is valid.
The ﬁrst and third steps are quite easy; all the real work is done in the
second one.

50
1 Propositional logic
→
→
→
→
→
F
F
F
F
F
ψ
F
T
T
T
T
T
φn
φn−1
φ3
φ2
φ1
Figure 1.11. The only way this parse tree can evaluate to F. We repre-
sent parse trees for φ1, φ2, . . . , φn as triangles as their internal structure
does not concern us here.
Step 1:
Deﬁnition 1.36 A formula of propositional logic φ is called a tautology iﬀ
it evaluates to T under all its valuations, i.e. iﬀ⊨φ.
Supposing that φ1, φ2, . . . , φn ⊨ψ holds, let us verify that φ1 →(φ2 →
(φ3 →(. . . (φn →ψ) . . . ))) is indeed a tautology. Since the latter formula is
a nested implication, it can evaluate to F only if all φ1, φ2,. . .,φn evaluate to T
and ψ evaluates to F; see its parse tree in Figure 1.11. But this contradicts the
fact that φ1, φ2, . . . , φn ⊨ψ holds. Thus, ⊨φ1 →(φ2 →(φ3 →(. . . (φn →
ψ) . . . ))) holds.
Step 2:
Theorem 1.37 If ⊨η holds, then ⊢η is valid. In other words, if η is a
tautology, then η is a theorem.
This step is the hard one. Assume that ⊨η holds. Given that η contains
n distinct propositional atoms p1, p2, . . . , pn we know that η evaluates to T
for all 2n lines in its truth table. (Each line lists a valuation of η.) How can
we use this information to construct a proof for η? In some cases this can
be done quite easily by taking a very good look at the concrete structure of
η. But here we somehow have to come up with a uniform way of building
such a proof. The key insight is to ‘encode’ each line in the truth table of η

1.4 Semantics of propositional logic
51
as a sequent. Then we construct proofs for these 2n sequents and assemble
them into a proof of η.
Proposition 1.38 Let φ be a formula such that p1, p2, . . . , pn are its only
propositional atoms. Let l be any line number in φ’s truth table. For all
1 ≤i ≤n let ˆpi be pi if the entry in line l of pi is T, otherwise ˆpi is ¬pi.
Then we have
1.
ˆp1, ˆp2, . . . , ˆpn ⊢φ is provable if the entry for φ in line l is T
2.
ˆp1, ˆp2, . . . , ˆpn ⊢¬φ is provable if the entry for φ in line l is F
Proof: This proof is done by structural induction on the formula φ, that
is, mathematical induction on the height of the parse tree of φ.
1.
If φ is a propositional atom p, we need to show that p ⊢p and ¬p ⊢¬p. These
have one-line proofs.
2.
If φ is of the form ¬φ1 we again have two cases to consider. First, assume that φ
evaluates to T. In this case φ1 evaluates to F. Note that φ1 has the same atomic
propositions as φ. We may use the induction hypothesis on φ1 to conclude that
ˆp1, ˆp2, . . . , ˆpn ⊢¬φ1; but ¬φ1 is just φ, so we are done.
Second, if φ evaluates to F, then φ1 evaluates to T and we get ˆp1, ˆp2, . . . , ˆpn ⊢φ1
by induction. Using the rule ¬¬i, we may extend the proof of ˆp1, ˆp2, . . . , ˆpn ⊢φ1
to one for ˆp1, ˆp2, . . . , ˆpn ⊢¬¬φ1; but ¬¬φ1 is just ¬φ, so again we are done.
The remaining cases all deal with two subformulas: φ equals φ1 ◦φ2, where
◦is →, ∧or ∨. In all these cases let q1, . . . , ql be the propositional
atoms of φ1 and r1, . . . , rk be the propositional atoms of φ2. Then we cer-
tainly have {q1, . . . , ql} ∪{r1, . . . , rk} = {p1, . . . , pn}. Therefore, whenever
ˆq1, . . . , ˆql ⊢ψ1 and ˆr1, . . . , ˆrk ⊢ψ2 are valid so is ˆp1, . . . , ˆpn ⊢ψ1 ∧ψ2 using
the rule ∧i. In this way, we can use our induction hypothesis and only owe
proofs that the conjunctions we conclude allow us to prove the desired con-
clusion for φ or ¬φ as the case may be.
3.
To wit, let φ be φ1 →φ2. If φ evaluates to F, then we know that φ1 evaluates
to T and φ2 to F. Using our induction hypothesis, we have ˆq1, . . . , ˆql ⊢φ1
and ˆr1, . . . , ˆrk ⊢¬φ2, so
ˆp1, . . . , ˆpn ⊢φ1 ∧¬φ2 follows. We need to show
ˆp1, . . . , ˆpn ⊢¬(φ1 →φ2); but using ˆp1, . . . , ˆpn ⊢φ1 ∧¬φ2, this amounts to
proving the sequent φ1 ∧¬φ2 ⊢¬(φ1 →φ2), which we leave as an exercise.
If φ evaluates to T, then we have three cases. First, if φ1 evaluates to F and
φ2 to F, then we get, by our induction hypothesis, that ˆq1, . . . , ˆql ⊢¬φ1 and
ˆr1, . . . , ˆrk ⊢¬φ2, so ˆp1, . . . , ˆpn ⊢¬φ1 ∧¬φ2 follows. Again, we need only to
show the sequent ¬φ1 ∧¬φ2 ⊢φ1 →φ2, which we leave as an exercise. Second,
if φ1 evaluates to F and φ2 to T, we use our induction hypothesis to arrive at

52
1 Propositional logic
ˆp1, . . . , ˆpn ⊢¬φ1 ∧φ2 and have to prove ¬φ1 ∧φ2 ⊢φ1 →φ2, which we leave as
an exercise. Third, if φ1 and φ2 evaluate to T, we arrive at ˆp1, . . . , ˆpn ⊢φ1 ∧φ2,
using our induction hypothesis, and need to prove φ1 ∧φ2 ⊢φ1 →φ2, which
we leave as an exercise as well.
4.
If φ is of the form φ1 ∧φ2, we are again dealing with four cases in total. First, if
φ1 and φ2 evaluate to T, we get ˆq1, . . . , ˆql ⊢φ1 and ˆr1, . . . , ˆrk ⊢φ2 by our induc-
tion hypothesis, so ˆp1, . . . , ˆpn ⊢φ1 ∧φ2 follows. Second, if φ1 evaluates to F and
φ2 to T, then we get ˆp1, . . . , ˆpn ⊢¬φ1 ∧φ2 using our induction hypothesis and
the rule ∧i as above and we need to prove ¬φ1 ∧φ2 ⊢¬(φ1 ∧φ2), which we leave
as an exercise. Third, if φ1 and φ2 evaluate to F, then our induction hypothesis
and the rule ∧i let us infer that ˆp1, . . . , ˆpn ⊢¬φ1 ∧¬φ2; so we are left with prov-
ing ¬φ1 ∧¬φ2 ⊢¬(φ1 ∧φ2), which we leave as an exercise. Fourth, if φ1 evalu-
ates to T and φ2 to F, we obtain ˆp1, . . . , ˆpn ⊢φ1 ∧¬φ2 by our induction hypoth-
esis and we have to show φ1 ∧¬φ2 ⊢¬(φ1 ∧φ2), which we leave as an exercise.
5.
Finally, if φ is a disjunction φ1 ∨φ2, we again have four cases. First, if φ1 and φ2
evaluate to F, then our induction hypothesis and the rule ∧i give us ˆp1, . . . , ˆpn ⊢
¬φ1 ∧¬φ2 and we have to show ¬φ1 ∧¬φ2 ⊢¬(φ1 ∨φ2), which we leave as an
exercise. Second, if φ1 and φ2 evaluate to T, then we obtain ˆp1, . . . , ˆpn ⊢φ1 ∧φ2,
by our induction hypothesis, and we need a proof for φ1 ∧φ2 ⊢φ1 ∨φ2, which
we leave as an exercise. Third, if φ1 evaluates to F and φ2 to T, then we arrive
at ˆp1, . . . , ˆpn ⊢¬φ1 ∧φ2, using our induction hypothesis, and need to establish
¬φ1 ∧φ2 ⊢φ1 ∨φ2, which we leave as an exercise. Fourth, if φ1 evaluates to T
and φ2 to F, then ˆp1, . . . , ˆpn ⊢φ1 ∧¬φ2 results from our induction hypothesis
and all we need is a proof for φ1 ∧¬φ2 ⊢φ1 ∨φ2, which we leave as an
exercise.
2
We apply this technique to the formula ⊨φ1 →(φ2 →(φ3 →(. . . (φn →
ψ) . . . ))). Since it is a tautology it evaluates to T in all 2n lines of its truth
table; thus, the proposition above gives us 2n many proofs of ˆp1, ˆp2, . . . , ˆpn ⊢
η, one for each of the cases that ˆpi is pi or ¬pi. Our job now is to assemble
all these proofs into a single proof for η which does not use any premises.
We illustrate how to do this for an example, the tautology p ∧q →p.
The formula p ∧q →p has two propositional atoms p and q. By the propo-
sition above, we are guaranteed to have a proof for each of the four sequents
p, q ⊢p ∧q →p
¬p, q ⊢p ∧q →p
p, ¬q ⊢p ∧q →p
¬p, ¬q ⊢p ∧q →p.
Ultimately, we want to prove p ∧q →p by appealing to the four proofs of
the sequents above. Thus, we somehow need to get rid of the premises on

1.5 Normal forms
53
the left-hand sides of these four sequents. This is the place where we rely on
the law of the excluded middle which states r ∨¬r, for any r. We use LEM
for all propositional atoms (here p and q) and then we separately assume all
the four cases, by using ∨e. That way we can invoke all four proofs of the
sequents above and use the rule ∨e repeatedly until we have got rid of all our
premises. We spell out the combination of these four phases schematically:
1
p ∨¬p
LEM
2
p
ass
3
q ∨¬q
LEM
4
q
ass
5
...
...
6
7
p ∧q →p
¬q
ass
...
...
p ∧q →p
8
p ∧q →p
∨e
¬p
ass
q ∨¬q
LEM
q
ass
...
...
p ∧q →p
¬q
ass
...
...
p ∧q →p
p ∧q →p
∨e
9
p ∧q →p
∨e
As soon as you understand how this particular example works, you will
also realise that it will work for an arbitrary tautology with n distinct atoms.
Of course, it seems ridiculous to prove p ∧q →p using a proof that is this
long. But remember that this illustrates a uniform method that constructs
a proof for every tautology η, no matter how complicated it is.
Step 3:
Finally, we need to ﬁnd a proof for φ1, φ2, . . . , φn ⊢ψ. Take the
proof for ⊢φ1 →(φ2 →(φ3 →(. . . (φn →ψ) . . . ))) given by step 2 and aug-
ment its proof by introducing φ1, φ2, . . . , φn as premises. Then apply →e n
times on each of these premises (starting with φ1, continuing with φ2 etc.).
Thus, we arrive at the conclusion ψ which gives us a proof for the sequent
φ1, φ2, . . . , φn ⊢ψ.
Corollary 1.39 (Soundness and Completeness) Let
φ1, φ2, . . . , φn, ψ
be formulas of propositional logic. Then φ1, φ2, . . . , φn ⊨ψ is holds iﬀthe
sequent φ1, φ2, . . . , φn ⊢ψ is valid.
1.5 Normal forms
In the last section, we showed that our proof system for propositional logic is
sound and complete for the truth-table semantics of formulas in Figure 1.6.

54
1 Propositional logic
Soundness means that whatever we prove is going to be a true fact, based on
the truth-table semantics. In the exercises, we apply this to show that a se-
quent does not have a proof: simply show that φ1, φ2, . . . , φ2 does not seman-
tically entail ψ; then soundness implies that the sequent φ1, φ2, . . . , φ2 ⊢ψ
does not have a proof. Completeness comprised a much more powerful state-
ment: no matter what (semantically) valid sequents there are, they all have
syntactic proofs in the proof system of natural deduction. This tight cor-
respondence allows us to freely switch between working with the notion of
proofs (⊢) and that of semantic entailment (⊨).
Using natural deduction to decide the validity of instances of ⊢is only
one of many possibilities. In Exercise 1.2.6 we sketch a non-linear, tree-like,
notion of proofs for sequents. Likewise, checking an instance of ⊨by apply-
ing Deﬁnition 1.34 literally is only one of many ways of deciding whether
φ1, φ2, . . . , φn ⊨ψ holds. We now investigate various alternatives for deciding
φ1, φ2, . . . , φn ⊨ψ which are based on transforming these formulas syntac-
tically into ‘equivalent’ ones upon which we can then settle the matter by
purely syntactic or algorithmic means. This requires that we ﬁrst clarify
what exactly we mean by equivalent formulas.
1.5.1 Semantic equivalence, satisfiability and validity
Two formulas φ and ψ are said to be equivalent if they have the same
‘meaning.’ This suggestion is vague and needs to be reﬁned. For example,
p →q and ¬p ∨q have the same truth table; all four combinations of T and F
for p and q return the same result. ’Coincidence of truth tables’ is not good
enough for what we have in mind, for what about the formulas p ∧q →p
and r ∨¬r? At ﬁrst glance, they have little in common, having diﬀerent
atomic formulas and diﬀerent connectives. Moreover, the truth table for
p ∧q →p is four lines long, whereas the one for r ∨¬r consists of only two
lines. However, both formulas are always true. This suggests that we deﬁne
the equivalence of formulas φ and ψ via ⊨: if φ semantically entails ψ and
vice versa, then these formulas should be the same as far as our truth-table
semantics is concerned.
Deﬁnition 1.40 Let φ and ψ be formulas of propositional logic. We say
that φ and ψ are semantically equivalent iﬀφ ⊨ψ and ψ ⊨φ hold. In that
case we write φ ≡ψ. Further, we call φ valid if ⊨φ holds.
Note that we could also have deﬁned φ ≡ψ to mean that ⊨(φ →ψ) ∧
(ψ →φ) holds; it amounts to the same concept. Indeed, because of soundness
and completeness, semantic equivalence is identical to provable equivalence

1.5 Normal forms
55
(Deﬁnition 1.25). Examples of equivalent formulas are
p →q ≡¬q →¬p
p →q ≡¬p ∨q
p ∧q →p ≡r ∨¬r
p ∧q →r ≡p →(q →r).
Recall that a formula η is called a tautology if ⊨η holds, so the tautologies
are exactly the valid formulas. The following lemma says that any decision
procedure for tautologies is in fact a decision procedure for the validity of
sequents as well.
Lemma 1.41 Given formulas φ1, φ2, . . . , φn and ψ of propositional logic,
φ1, φ2, . . . , φn ⊨ψ holds iﬀ⊨φ1 →(φ2 →(φ3 →· · · →(φn →ψ))) holds.
Proof: First, suppose that ⊨φ1 →(φ2 →(φ3 →· · · →(φn →ψ))) holds.
If φ1, φ2, . . . , φn are all true under some valuation, then ψ has to be true
as well for that same valuation. Otherwise,
⊨φ1 →(φ2 →(φ3 →· · · →
(φn →ψ))) would not hold (compare this with Figure 1.11). Second, if
φ1, φ2, . . . , φn ⊨ψ holds, we have already shown that ⊨φ1 →(φ2 →(φ3 →
· · · →(φn →ψ))) follows in step 1 of our completeness proof.
2
For our current purposes, we want to transform formulas into ones which
don’t contain →at all and the occurrences of ∧and ∨are conﬁned to
separate layers such that validity checks are easy. This is being done by
1.
using the equivalence φ →ψ ≡¬φ ∨ψ to remove all occurrences of →from a
formula and
2.
by specifying an algorithm that takes a formula without any →into a normal
form (still without →) for which checking validity is easy.
Naturally, we have to specify which forms of formulas we think of as being
‘normal.’ Again, there are many such notions, but in this text we study only
two important ones.
Deﬁnition 1.42 A literal L is either an atom p or the negation of an atom
¬p. A formula C is in conjunctive normal form (CNF) if it is a conjunction
of clauses, where each clause D is a disjunction of literals:
L ::= p | ¬p
D ::= L | L ∨D
(1.6)
C ::= D | D ∧C.

56
1 Propositional logic
Examples of formulas in conjunctive normal form are
(i)
(¬q ∨p ∨r) ∧(¬p ∨r) ∧q
(ii)
(p ∨r) ∧(¬p ∨r) ∧(p ∨¬r).
In the ﬁrst case, there are three clauses of type D: ¬q ∨p ∨r, ¬p ∨r, and q –
which is a literal promoted to a clause by the ﬁrst rule of clauses in (1.6).
Notice how we made implicit use of the associativity laws for ∧and ∨,
saying that φ ∨(ψ ∨η) ≡(φ ∨ψ) ∨η and φ ∧(ψ ∧η) ≡(φ ∧ψ) ∧η, since
we omitted some parentheses. The formula (¬(q ∨p) ∨r) ∧(q ∨r) is not in
CNF since q ∨p is not a literal.
Why do we care at all about formulas φ in CNF? One of the reasons
for their usefulness is that they allow easy checks of validity which other-
wise take times exponential in the number of atoms. For example, consider
the formula in CNF from above: (¬q ∨p ∨r) ∧(¬p ∨r) ∧q. The semantic
entailment ⊨(¬q ∨p ∨r) ∧(¬p ∨r) ∧q holds iﬀall three relations
⊨¬q ∨p ∨r
⊨¬p ∨r
⊨q
hold, by the semantics of ∧. But since all of these formulas are disjunctions
of literals, or literals, we can settle the matter as follows.
Lemma 1.43 A disjunction of literals L1 ∨L2 ∨· · · ∨Lm is valid iﬀthere
are 1 ≤i, j ≤m such that Li is ¬Lj.
Proof: If Li equals ¬Lj, then L1 ∨L2 ∨· · · ∨Lm evaluates to T for all
valuations. For example, the disjunct p ∨q ∨r ∨¬q can never be made false.
To see that the converse holds as well, assume that no literal Lk has a
matching negation in L1 ∨L2 ∨· · · ∨Lm. Then, for each k with 1 ≤k ≤n,
we assign F to Lk, if Lk is an atom; or T, if Lk is the negation of an atom.
For example, the disjunct ¬q ∨p ∨r can be made false by assigning F to p
and r and T to q.
2
Hence, we have an easy and fast check for the validity of ⊨φ, provided
that φ is in CNF; inspect all conjuncts ψk of φ and search for atoms in ψk
such that ψk also contains their negation. If such a match is found for all
conjuncts, we have ⊨φ. Otherwise (= some conjunct contains no pair Li and
¬Li), φ is not valid by the lemma above. Thus, the formula (¬q ∨p ∨r) ∧
(¬p ∨r) ∧q above is not valid. Note that the matching literal has to be found
in the same conjunct ψk. Since there is no free lunch in this universe, we can
expect that the computation of a formula φ′ in CNF, which is equivalent to
a given formula φ, is a costly worst-case operation.
Before we study how to compute equivalent conjunctive normal forms, we
introduce another semantic concept closely related to that of validity.

1.5 Normal forms
57
Deﬁnition 1.44 Given a formula φ in propositional logic, we say that φ is
satisﬁable if it has a valuation in which is evaluates to T.
For example, the formula p ∨q →p is satisﬁable since it computes T if we
assign T to p. Clearly, p ∨q →p is not valid. Thus, satisﬁability is a weaker
concept since every valid formula is by deﬁnition also satisﬁable but not vice
versa. However, these two notions are just mirror images of each other, the
mirror being negation.
Proposition 1.45 Let φ be a formula of propositional logic. Then φ is sat-
isﬁable iﬀ¬φ is not valid.
Proof: First, assume that φ is satisﬁable. By deﬁnition, there exists a
valuation of φ in which φ evaluates to T; but that means that ¬φ evaluates
to F for that same valuation. Thus, ¬φ cannot be valid.
Second, assume that ¬φ is not valid. Then there must be a valuation
of ¬φ in which ¬φ evaluates to F. Thus, φ evaluates to T and is there-
fore satisﬁable. (Note that the valuations of φ are exactly the valuations of
¬φ.)
2
This result is extremely useful since it essentially says that we need provide
a decision procedure for only one of these concepts. For example, let’s say
that we have a procedure P for deciding whether any φ is valid. We obtain a
decision procedure for satisﬁability simply by asking P whether ¬φ is valid.
If it is, φ is not satisﬁable; otherwise φ is satisﬁable. Similarly, we may
transform any decision procedure for satisﬁability into one for validity. We
will encounter both kinds of procedures in this text.
There is one scenario in which computing an equivalent formula in CNF
is really easy; namely, when someone else has already done the work of
writing down a full truth table for φ. For example, take the truth table
of (p →¬q) →(q ∨¬p) in Figure 1.8 (page 40). For each line where (p →
¬q) →(q ∨¬p) computes F we now construct a disjunction of literals. Since
there is only one such line, we have only one conjunct ψ1. That conjunct
is now obtained by a disjunction of literals, where we include literals ¬p
and q. Note that the literals are just the syntactic opposites of the truth
values in that line: here p is T and q is F. The resulting formula in CNF
is thus ¬p ∨q which is readily seen to be in CNF and to be equivalent to
(p →¬q) →(q ∨¬p).
Why does this always work for any formula φ? Well, the constructed
formula will be false iﬀat least one of its conjuncts ψi will be false. This
means that all the disjuncts in such a ψi must be F. Using the de Morgan

58
1 Propositional logic
rule ¬φ1 ∨¬φ2 ∨· · · ∨¬φn ≡¬(φ1 ∧φ2 ∧· · · ∧φn), we infer that the con-
junction of the syntactic opposites of those literals must be true. Thus, φ
and the constructed formula have the same truth table.
Consider another example, in which φ is given by the truth table:
p
q
r
φ
T
T
T
T
T
T
F
F
T
F
T
T
T
F
F
T
F
T
T
F
F
T
F
F
F
F
T
F
F
F
F
T
Note that this table is really just a speciﬁcation of φ; it does not tell us what
φ looks like syntactically, but it does tells us how it ought to ‘behave.’ Since
this truth table has four entries which compute F, we construct four con-
juncts ψi (1 ≤i ≤4). We read the ψi oﬀthat table by listing the disjunction
of all atoms, where we negate those atoms which are true in those lines:
ψ1
def
= ¬p ∨¬q ∨r (line 2)
ψ2
def
= p ∨¬q ∨¬r (line 5)
ψ3
def
= p ∨¬q ∨r
etc
ψ4
def
= p ∨q ∨¬r.
The resulting φ in CNF is therefore
(¬p ∨¬q ∨r) ∧(p ∨¬q ∨¬r) ∧(p ∨¬q ∨r) ∧(p ∨q ∨¬r).
If we don’t have a full truth table at our disposal, but do know the structure
of φ, then we would like to compute a version of φ in CNF. It should be
clear by now that a full truth table of φ and an equivalent formula in
CNF are pretty much the same thing as far as questions about validity are
concerned – although the formula in CNF may be much more compact.
1.5.2 Conjunctive normal forms and validity
We have already seen the beneﬁts of conjunctive normal forms in that they
allow for a fast and easy syntactic test of validity. Therefore, one wonders
whether any formula can be transformed into an equivalent formula in CNF.
We now develop an algorithm achieving just that. Note that, by Deﬁni-
tion 1.40, a formula is valid iﬀany of its equivalent formulas is valid. We
reduce the problem of determining whether any φ is valid to the problem
of computing an equivalent ψ ≡φ such that ψ is in CNF and checking, via
Lemma 1.43, whether ψ is valid.

1.5 Normal forms
59
Before we sketch such a procedure, we make some general remarks about
its possibilities and its realisability constraints. First of all, there could be
more or less eﬃcient ways of computing such normal forms. But even more
so, there could be many possible correct outputs, for ψ1 ≡φ and ψ2 ≡φ
do not generally imply that ψ1 is the same as ψ2, even if ψ1 and ψ2 are in
CNF. For example, take φ
def
= p, ψ1
def
= p and ψ2
def
= p ∧(p ∨q); then convince
yourself that φ ≡ψ2 holds. Having this ambiguity of equivalent conjunctive
normal forms, the computation of a CNF for φ with minimal ‘cost’ (where
‘cost’ could for example be the number of conjuncts, or the height of φ’s
parse tree) becomes a very important practical problem, an issue persued in
Chapter 6. Right now, we are content with stating a deterministic algorithm
which always computes the same output CNF for a given input φ.
This algorithm, called CNF, should satisfy the following requirements:
(1)
CNF terminates for all formulas of propositional logic as input;
(2)
for each such input, CNF outputs an equivalent formula; and
(3)
all output computed by CNF is in CNF.
If a call of CNF with a formula φ of propositional logic as input terminates,
which is enforced by (1), then (2) ensures that ψ ≡φ holds for the output
ψ. Thus, (3) guarantees that ψ is an equivalent CNF of φ. So φ is valid iﬀ
ψ is valid; and checking the latter is easy relative to the length of ψ.
What kind of strategy should CNF employ? It will have to function
correctly for all, i.e. inﬁnitely many, formulas of propositional logic. This
strongly suggests to write a procedure that computes a CNF by structural
induction on the formula φ. For example, if φ is of the form φ1 ∧φ2, we
may simply compute conjunctive normal forms ηi for φi (i = 1, 2), where-
upon η1 ∧η2 is a conjunctive normal form which is equivalent to φ provided
that ηi ≡φi (i = 1, 2). This strategy also suggests to use proof by structural
induction on φ to prove that CNF meets the requirements (1–3) stated above.
Given a formula φ as input, we ﬁrst do some preprocessing. Initially, we
translate away all implications in φ by replacing all subformulas of the form
ψ →η by ¬ψ ∨η. This is done by a procedure called IMPL FREE. Note that
this procedure has to be recursive, for there might be implications in ψ or
η as well.
The application of IMPL FREE might introduce double negations into the
output formula. More importantly, negations whose scopes are non-atomic
formulas might still be present. For example, the formula p ∧¬(p ∧q) has
such a negation with p ∧q as its scope. Essentially, the question is whether
one can eﬃciently compute a CNF for ¬φ from a CNF for φ. Since nobody
seems to know the answer, we circumvent the question by translating ¬φ

60
1 Propositional logic
into an equivalent formula that contains only negations of atoms. Formulas
which only negate atoms are said to be in negation normal form (NNF). We
spell out such a procedure, NNF, in detail later on. The key to its speciﬁcation
for implication-free formulas lies in the de Morgan rules. The second phase
of the preprocessing, therefore, calls NNF with the implication-free output of
IMPL FREE to obtain an equivalent formula in NNF.
After all this preprocessing, we obtain a formula φ′ which is the result of
the call NNF (IMPL FREE (φ)). Note that φ′ ≡φ since both algorithms only
transform formulas into equivalent ones. Since φ′ contains no occurrences
of →and since only atoms in φ′ are negated, we may program CNF by an
analysis of only three cases: literals, conjunctions and disjunctions.
r If φ is a literal, it is by deﬁnition in CNF and so CNF outputs φ.
r If φ equals φ1 ∧φ2, we call CNF recursively on each φi to get the respective output
ηi and return the CNF η1 ∧η2 as output for input φ.
r If φ equals φ1 ∨φ2, we again call CNF recursively on each φi to get the respective
output ηi; but this time we must not simply return η1 ∨η2 since that formula is
certainly not in CNF, unless η1 and η2 happen to be literals.
So how can we complete the program in the last case? Well, we may resort
to the distributivity laws, which entitle us to translate any disjunction of
conjunctions into a conjunction of disjunctions. However, for this to result in
a CNF, we need to make certain that those disjunctions generated contain
only literals. We apply a strategy for using distributivity based on matching
patterns in φ1 ∨φ2. This results in an independent algorithm called DISTR
which will do all that work for us. Thus, we simply call DISTR with the pair
(η1, η2) as input and pass along its result.
Assuming that we already have written code for IMPL FREE, NNF and
DISTR, we may now write pseudo code for CNF:
function CNF (φ):
/* precondition: φ implication free and in NNF */
/* postcondition: CNF (φ) computes an equivalent CNF for φ */
begin function
case
φ is a literal: return φ
φ is φ1 ∧φ2 : return CNF (φ1) ∧CNF (φ2)
φ is φ1 ∨φ2 : return DISTR (CNF (φ1), CNF (φ2))
end case
end function

1.5 Normal forms
61
Notice how the calling of DISTR is done with the computed conjunctive nor-
mal forms of φ1 and φ2. The routine DISTR has η1 and η2 as input parameters
and does a case analysis on whether these inputs are conjunctions. What
should DISTR do if none of its input formulas is such a conjunction? Well,
since we are calling DISTR for inputs η1 and η2 which are in CNF, this can
only mean that η1 and η2 are literals, or disjunctions of literals. Thus, η1 ∨η2
is in CNF.
Otherwise, at least one of the formulas η1 and η2 is a conjunction. Since
one conjunction suﬃces for simplifying the problem, we have to decide which
conjunct we want to transform if both formulas are conjunctions. That way
we maintain that our algorithm CNF is deterministic. So let us suppose that
η1 is of the form η11 ∧η12. Then the distributive law says that η1 ∨η2 ≡
(η11 ∨η2) ∧(η12 ∨η2). Since all participating formulas η11, η12 and η2 are
in CNF, we may call DISTR again for the pairs (η11, η2) and (η12, η2), and
then simply form their conjunction. This is the key insight for writing the
function DISTR.
The case when η2 is a conjunction is symmetric and the structure of
the recursive call of DISTR is then dictated by the equivalence η1 ∨η2 ≡
(η1 ∨η21) ∧(η1 ∨η22), where η2 = η21 ∧η22:
function DISTR (η1, η2):
/* precondition: η1 and η2 are in CNF */
/* postcondition: DISTR (η1, η2) computes a CNF for η1 ∨η2 */
begin function
case
η1 is η11 ∧η12 : return DISTR (η11, η2) ∧DISTR (η12, η2)
η2 is η21 ∧η22 : return DISTR (η1, η21) ∧DISTR (η1, η22)
otherwise (= no conjunctions): return η1 ∨η2
end case
end function
Notice how the three clauses are exhausting all possibilities. Furthermore,
the ﬁrst and second cases overlap if η1 and η2 are both conjunctions. It
is then our understanding that this code will inspect the clauses of a case
statement from the top to the bottom clause. Thus, the ﬁrst clause would
apply.
Having speciﬁed the routines CNF and DISTR, this leaves us with the
task of writing the functions IMPL FREE and NNF. We delegate the design

62
1 Propositional logic
of IMPL FREE to the exercises. The function NNF has to transform any
implication-free formula into an equivalent one in negation normal form.
Four examples of formulas in NNF are
p
¬p
¬p ∧(p ∧q)
¬p ∧(p →q),
although we won’t have to deal with a formula of the last kind since →
won’t occur. Examples of formulas which are not in NNF are ¬¬p and
¬(p ∧q).
Again, we program NNF recursively by a case analysis over the structure of
the input formula φ. The last two examples already suggest a solution for two
of these clauses. In order to compute a NNF of ¬¬φ, we simply compute
a NNF of φ. This is a sound strategy since φ and ¬¬φ are semantically
equivalent. If φ equals ¬(φ1 ∧φ2), we use the de Morgan rule ¬(φ1 ∧φ2) ≡
¬φ1 ∨¬φ2 as a recipe for how NNF should call itself recursively in that case.
Dually, the case of φ being ¬(φ1 ∨φ2) appeals to the other de Morgan rule
¬(φ1 ∨φ2) ≡¬φ1 ∧¬φ2 and, if φ is a conjunction or disjunction, we simply
let NNF pass control to those subformulas. Clearly, all literals are in NNF.
The resulting code for NNF is thus
function NNF (φ):
/* precondition: φ is implication free */
/* postcondition: NNF (φ) computes a NNF for φ */
begin function
case
φ is a literal: return φ
φ is ¬¬φ1 : return NNF (φ1)
φ is φ1 ∧φ2 : return NNF (φ1) ∧NNF (φ2)
φ is φ1 ∨φ2 : return NNF (φ1) ∨NNF (φ2)
φ is ¬(φ1 ∧φ2): return NNF (¬φ1) ∨NNF (¬φ2)
φ is ¬(φ1 ∨φ2): return NNF (¬φ1) ∧NNF (¬φ2)
end case
end function
Notice that these cases are exhaustive due to the algorithm’s precondition.
Given any formula φ of propositional logic, we may now convert it into an

1.5 Normal forms
63
equivalent CNF by calling CNF (NNF (IMPL FREE (φ))). In the exercises, you
are asked to show that
r all four algorithms terminate on input meeting their preconditions,
r the result of CNF (NNF (IMPL FREE (φ))) is in CNF and
r that result is semantically equivalent to φ.
We will return to the important issue of formally proving the correctness of
programs in Chapter 4.
Let us now illustrate the programs coded above on some concrete exam-
ples. We begin by computing CNF (NNF (IMPL FREE (¬p ∧q →p ∧(r →q)))).
We show almost all details of this computation and you should compare this
with how you would expect the code above to behave. First, we compute
IMPL FREE (φ):
IMPL FREE (φ) = ¬IMPL FREE (¬p ∧q) ∨IMPL FREE (p ∧(r →q))
= ¬((IMPL FREE ¬p) ∧(IMPL FREE q)) ∨IMPL FREE (p ∧(r →q))
= ¬((¬p) ∧IMPL FREE q) ∨IMPL FREE (p ∧(r →q))
= ¬(¬p ∧q) ∨IMPL FREE (p ∧(r →q))
= ¬(¬p ∧q) ∨((IMPL FREE p) ∧IMPL FREE (r →q))
= ¬(¬p ∧q) ∨(p ∧IMPL FREE (r →q))
= ¬(¬p ∧q) ∨(p ∧(¬(IMPL FREE r) ∨(IMPL FREE q)))
= ¬(¬p ∧q) ∨(p ∧(¬r ∨(IMPL FREE q)))
= ¬(¬p ∧q) ∨(p ∧(¬r ∨q)).
Second, we compute NNF (IMPL FREE φ):
NNF (IMPL FREE φ) = NNF (¬(¬p ∧q)) ∨NNF (p ∧(¬r ∨q))
= NNF (¬(¬p) ∨¬q) ∨NNF (p ∧(¬r ∨q))
= (NNF (¬¬p)) ∨(NNF (¬q)) ∨NNF (p ∧(¬r ∨q))
= (p ∨(NNF (¬q))) ∨NNF (p ∧(¬r ∨q))
= (p ∨¬q) ∨NNF (p ∧(¬r ∨q))
= (p ∨¬q) ∨((NNF p) ∧(NNF (¬r ∨q)))
= (p ∨¬q) ∨(p ∧(NNF (¬r ∨q)))
= (p ∨¬q) ∨(p ∧((NNF (¬r)) ∨(NNF q)))
= (p ∨¬q) ∨(p ∧(¬r ∨(NNF q)))
= (p ∨¬q) ∨(p ∧(¬r ∨q)).

64
1 Propositional logic
Third, we ﬁnish it oﬀwith
CNF (NNF (IMPL FREE φ)) = CNF ((p ∨¬q) ∨(p ∧(¬r ∨q)))
= DISTR (CNF (p ∨¬q), CNF (p ∧(¬r ∨q)))
= DISTR (p ∨¬q, CNF (p ∧(¬r ∨q)))
= DISTR (p ∨¬q, p ∧(¬r ∨q))
= DISTR (p ∨¬q, p) ∧DISTR (p ∨¬q, ¬r ∨q)
= (p ∨¬q ∨p) ∧DISTR (p ∨¬q, ¬r ∨q)
= (p ∨¬q ∨p) ∧(p ∨¬q ∨¬r ∨q) .
The formula (p ∨¬q ∨p) ∧(p ∨¬q ∨¬r ∨q) is thus the result of the call
CNF (NNF (IMPL FREE φ)) and is in conjunctive normal form and equivalent to
φ. Note that it is satisﬁable (choose p to be true) but not valid (choose p to be
false and q to be true); it is also equivalent to the simpler conjunctive normal
form p ∨¬q. Observe that our algorithm does not do such optimisations so
one would need a separate optimiser running on the output. Alternatively,
one might change the code of our functions to allow for such optimisations
‘on the ﬂy,’ a computational overhead which could prove to be counter-
productive.
You should realise that we omitted several computation steps in the sub-
calls CNF (p ∨¬q) and CNF (p ∧(¬r ∨q)). They return their input as a result
since the input is already in conjunctive normal form.
As a second example, consider φ
def
= r →(s →(t ∧s →r)). We compute
IMPL FREE (φ) = ¬(IMPL FREE r) ∨IMPL FREE (s →(t ∧s →r))
= ¬r ∨IMPL FREE (s →(t ∧s →r))
= ¬r ∨(¬(IMPL FREE s) ∨IMPL FREE (t ∧s →r))
= ¬r ∨(¬s ∨IMPL FREE (t ∧s →r))
= ¬r ∨(¬s ∨(¬(IMPL FREE (t ∧s)) ∨IMPL FREE r))
= ¬r ∨(¬s ∨(¬((IMPL FREE t) ∧(IMPL FREE s)) ∨IMPL FREE r))
= ¬r ∨(¬s ∨(¬(t ∧(IMPL FREE s)) ∨(IMPL FREE r)))
= ¬r ∨(¬s ∨(¬(t ∧s)) ∨(IMPL FREE r))
= ¬r ∨(¬s ∨(¬(t ∧s)) ∨r)

1.5 Normal forms
65
NNF (IMPL FREE φ) = NNF (¬r ∨(¬s ∨¬(t ∧s) ∨r))
= (NNF ¬r) ∨NNF (¬s ∨¬(t ∧s) ∨r)
= ¬r ∨NNF (¬s ∨¬(t ∧s) ∨r)
= ¬r ∨(NNF (¬s) ∨NNF (¬(t ∧s) ∨r))
= ¬r ∨(¬s ∨NNF (¬(t ∧s) ∨r))
= ¬r ∨(¬s ∨(NNF (¬(t ∧s)) ∨NNF r))
= ¬r ∨(¬s ∨(NNF (¬t ∨¬s)) ∨NNF r)
= ¬r ∨(¬s ∨((NNF (¬t) ∨NNF (¬s)) ∨NNF r))
= ¬r ∨(¬s ∨((¬t ∨NNF (¬s)) ∨NNF r))
= ¬r ∨(¬s ∨((¬t ∨¬s) ∨NNF r))
= ¬r ∨(¬s ∨((¬t ∨¬s) ∨r))
where the latter is already in CNF and valid as r has a matching ¬r.
1.5.3 Horn clauses and satisfiability
We have already commented on the computational price we pay for trans-
forming a propositional logic formula into an equivalent CNF. The latter
class of formulas has an easy syntactic check for validity, but its test for
satisﬁability is very hard in general. Fortunately, there are practically im-
portant subclasses of formulas which have much more eﬃcient ways of de-
ciding their satisﬁability. One such example is the class of Horn formu-
las; the name ‘Horn’ is derived from the logician A. Horn’s last name.
We shortly deﬁne them and give an algorithm for checking their satisﬁ-
ability.
Recall that the logical constants ⊥(‘bottom’) and ⊤(‘top’) denote an
unsatisﬁable formula, respectively, a tautology.
Deﬁnition 1.46 A Horn formula is a formula φ of propositional logic if it
can be generated as an instance of H in this grammar:
P ::= ⊥| ⊤| p
A ::= P | P ∧A
C ::= A →P
H ::= C | C ∧H.
(1.7)
We call each instance of C a Horn clause.

66
1 Propositional logic
Horn formulas are conjunctions of Horn clauses. A Horn clause is an impli-
cation whose assumption A is a conjunction of propositions of type P and
whose conclusion is also of type P. Examples of Horn formulas are
(p ∧q ∧s →p) ∧(q ∧r →p) ∧(p ∧s →s)
(p ∧q ∧s →⊥) ∧(q ∧r →p) ∧(⊤→s)
(p2 ∧p3 ∧p5 →p13) ∧(⊤→p5) ∧(p5 ∧p11 →⊥).
Examples of formulas which are not Horn formulas are
(p ∧q ∧s →¬p) ∧(q ∧r →p) ∧(p ∧s →s)
(p ∧q ∧s →⊥) ∧(¬q ∧r →p) ∧(⊤→s)
(p2 ∧p3 ∧p5 →p13 ∧p27) ∧(⊤→p5) ∧(p5 ∧p11 →⊥)
(p2 ∧p3 ∧p5 →p13 ∧p27) ∧(⊤→p5) ∧(p5 ∧p11 ∨⊥).
The ﬁrst formula is not a Horn formula since ¬p, the conclusion of the
implication of the ﬁrst conjunct, is not of type P. The second formula does
not qualify since the premise of the implication of the second conjunct,
¬q ∧r, is not a conjunction of atoms, ⊥, or ⊤. The third formula is not a
Horn formula since the conclusion of the implication of the ﬁrst conjunct,
p13 ∧p27, is not of type P. The fourth formula clearly is not a Horn formula
since it is not a conjunction of implications.
The algorithm we propose for deciding the satisﬁability of a Horn for-
mula φ maintains a list of all occurrences of type P in φ and proceeds like
this:
1.
It marks ⊤if it occurs in that list.
2.
If there is a conjunct P1 ∧P2 ∧· · · ∧Pki →P ′ of φ such that all Pj with 1 ≤j ≤
ki are marked, mark P ′ as well and go to 2. Otherwise (= there is no conjunct
P1 ∧P2 ∧· · · ∧Pki →P ′ such that all Pj are marked) go to 3.
3.
If ⊥is marked, print out ‘The Horn formula φ is unsatisﬁable.’ and stop. Oth-
erwise, go to 4.
4.
Print out ‘The Horn formula φ is satisﬁable.’ and stop.
In these instructions, the markings of formulas are shared by all other oc-
currences of these formulas in the Horn formula. For example, once we
mark p2 because of one of the criteria above, then all other occurrences
of p2 are marked as well. We use pseudo code to specify this algorithm
formally:

1.5 Normal forms
67
function HORN (φ):
/* precondition: φ is a Horn formula */
/* postcondition: HORN (φ) decides the satisﬁability for φ */
begin function
mark all occurrences of ⊤in φ;
while there is a conjunct P1 ∧P2 ∧· · · ∧Pki →P ′ of φ
such that all Pj are marked but P ′ isn’t do
mark P ′
end while
if ⊥is marked then return ‘unsatisﬁable’ else return ‘satisﬁable’
end function
We need to make sure that this algorithm terminates on all Horn formulas
φ as input and that its output (= its decision) is always correct.
Theorem 1.47 The algorithm HORN is correct for the satisﬁability decision
problem of Horn formulas and has no more than n + 1 cycles in its while-
statement if n is the number of atoms in φ. In particular, HORN always
terminates on correct input.
Proof: Let us ﬁrst consider the question of program termination. Notice
that entering the body of the while-statement has the eﬀect of marking an
unmarked P which is not ⊤. Since this marking applies to all occurrences
of P in φ, the while-statement can have at most one more cycle than there
are atoms in φ.
Since we guaranteed termination, it suﬃces to show that the answers
given by the algorithm HORN are always correct. To that end, it helps to
reveal the functional role of those markings. Essentially, marking a P means
that that P has got to be true if the formula φ is ever going to be satisﬁable.
We use mathematical induction to show that
‘All marked P are true for all valuations in which φ evaluates to T.’ (1.8)
holds after any number of executions of the body of the while-statement
above. The base case, zero executions, is when the while-statement has not
yet been entered but we already and only marked all occurrences of ⊤. Since
⊤must be true in all valuations, (1.8) follows.
In the inductive step, we assume that (1.8) holds after k cycles of the
while-statement. Then we need to show that same assertion for all marked
P after k + 1 cycles. If we enter the (k + 1)th cycle, the condition of the
while-statement is certainly true. Thus, there exists a conjunct P1 ∧P2 ∧
· · · ∧Pki →P ′ of φ such that all Pj are marked. Let v be any valuation

68
1 Propositional logic
in which φ is true. By our induction hypothesis, we know that all Pj and
therefore P1 ∧P2 ∧· · · ∧Pki have to be true in v as well. The conjunct P1 ∧
P2 ∧· · · ∧Pki →P ′ of φ has be to true in v, too, from which we infer that
P ′ has to be true in v.
By mathematical induction, we therefore secured that (1.8) holds no mat-
ter how many cycles that while-statement went through.
Finally, we need to make sure that the if-statement above always renders
correct replies. First, if ⊥is marked, then there has to be some conjunct
P1 ∧P2 ∧· · · ∧Pki →⊥of φ such that all Pi are marked as well. By (1.8)
that conjunct of φ evaluates to T →F = F whenever φ is true. As this is
impossible the reply ‘unsatisﬁable’ is correct. Second, if ⊥is not marked, we
simply assign T to all marked atoms and F to all unmarked atoms and use
proof by contradiction to show that φ has to be true with respect to that
valuation.
If φ is not true under that valuation, it must make one of its principal
conjuncts P1 ∧P2 ∧· · · ∧Pki →P ′ false. By the semantics of implication
this can only mean that all Pj are true and P ′ is false. By the deﬁnition of our
valuation, we then infer that all Pj are marked, so P1 ∧P2 ∧· · · ∧Pki →P ′
is a conjunct of φ that would have been dealt with in one of the cycles of
the while-statement and so P ′ is marked, too. Since ⊥is not marked, P ′ has
to be ⊤or some atom q. In any event, the conjunct is then true by (1.8), a
contradiction
2
Note that the proof by contradiction employed in the last proof was not
really needed. It just made the argument seem more natural to us. The
literature is full of such examples where one uses proof by contradiction
more out of psychological than proof-theoretical necessity.
1.6 SAT solvers
The marking algorithm for Horn formulas computes marks as constraints
on all valuations that can make a formule true. By (1.8), all marked atoms
have to be true for any such valuation. We can extend this idea to general
formulas φ by computing constraints saying which subformulas of φ require
a certain truth value for all valuations that make φ true:
‘All marked subformulas evaluate to their mark value
for all valuations in which φ evaluates to T.’
(1.9)
In that way, marking atomic formulas generalizes to marking subformu-
las; and ‘true’ marks generalize into ‘true’ and ‘false’ marks. At the same

1.6 SAT solvers
69
time, (1.9) serves as a guide for designing an algorithm and as an invariant
for proving its correctness.
1.6.1 A linear solver
We will execute this marking algorithm on the parse tree of formulas, except
that we will translate formulas into the adequate fragment
φ ::= p | (¬φ) | (φ ∧φ)
(1.10)
and then share common subformulas of the resulting parse tree, making the
tree into a directed, acyclic graph (DAG). The inductively deﬁned transla-
tion
T(p) = p
T(¬φ) = ¬T(φ)
T(φ1 ∧φ2) = T(φ1) ∧T(φ2)
T(φ1 ∨φ2) = ¬(¬T(φ1) ∧¬T(φ2))
T(φ1 →φ2) = ¬(T(φ1) ∧¬T(φ2))
transforms formulas generated by (1.3) into formulas generated by (1.10)
such that φ and T(φ) are semantically equivalent and have the same propo-
sitional atoms. Therefore, φ is satisﬁable iﬀT(φ) is satisﬁable; and the set
of valuations for which φ is true equals the set of valuations for which T(φ)
is true. The latter ensures that the diagnostics of a SAT solver, applied to
T(φ), is meaningful for the original formula φ. In the exercises, you are asked
to prove these claims.
Example 1.48 For the formula φ being p ∧¬(q ∨¬p) we compute T(φ) =
p ∧¬¬(¬q ∧¬¬p). The parse tree and DAG of T(φ) are depicted in Fig-
ure 1.12.
Any valuation that makes p ∧¬¬(¬q ∧¬¬p) true has to assign T to the
topmost ∧-node in its DAG of Figure 1.12. But that forces the mark T on
the p-node and the topmost ¬-node. In the same manner, we arrive at a
complete set of constraints in Figure 1.13, where the time stamps ‘1:’ etc
indicate the order in which we applied our intuitive reasoning about these
constraints; this order is generally not unique.
The formal set of rules for forcing new constraints from old ones is depicted
in Figure 1.14. A small circle indicates any node (¬, ∧or atom). The force
laws for negation, ¬t and ¬f, indicate that a truth constraint on a ¬-node
forces its dual value at its sub-node and vice versa. The law ∧te propagates
a T constraint on a ∧-node to its two sub-nodes; dually, ∧ti forces a T mark
on a ∧-node if both its children have that mark. The laws ∧ﬂand ∧fr force a
F constraint on a ∧-node if any of its sub-nodes has a F value. The laws ∧ﬂl

70
1 Propositional logic
∧
¬
¬
¬
¬
¬
q
p
¬
¬
¬
q
∧
¬
¬
∧
p
∧
p
Figure 1.12. Parse tree (left) and directed acyclic graph (right) of the
formula from Example 1.48. The p-node is shared on the right.
∧
p
¬
¬
¬
¬
¬
q
∧
1: T
2: T
2: T
3: F
4: T
3: F
4: T
5: T
6: F
Figure 1.13. A witness to the satisfiability of the formula represented
by this DAG.
and ∧frr are more complex: if an ∧-node has a F constraint and one of its
sub-nodes has a T constraint, then the other sub-node obtains a F-constraint.
Please check that all constraints depicted in Figure 1.13 are derivable from
these rules. The fact that each node in a DAG obtained a forced marking
does not yet show that this is a witness to the satisﬁability of the formula

1.6 SAT solvers
71
T
F
¬
T
F
T
T
F
F
F
F
F
T
F
F
T
F
T
∧
∧
∧
∧
¬t:
¬f:
∧ﬂ:
∧fr:
∧ﬂl:
∧frr:
false conjuncts
force false conjunction
false conjunction and true conjunct
¬
∧
forcing laws for negation
force false conjunction
T
true conjunctions force true conjunction
T
∧
T
∧te:
∧ti:
true conjunction forces true conjuncts
Figure 1.14. Rules for flow of constraints in a formula’s DAG. Small
circles indicate arbitrary nodes (¬, ∧or atom). Note that the rules ∧ﬂl,
∧frr and ∧ti require that the source constraints of both =⇒are present.
represented by this DAG. A post-processing phase takes the marks for all
atoms and re-computes marks of all other nodes in a bottom-up manner, as
done in Section 1.4 on parse trees. Only if the resulting marks match the
ones we computed have we found a witness. Please verify that this is the
case in Figure 1.13.
We can apply SAT solvers to checking whether sequents are valid. For
example, the sequent p ∧q →r ⊢p →q →r is valid iﬀ(p ∧q →r) →p →
q →r is a theorem (why?) iﬀφ = ¬((p ∧q →r) →p →q →r) is not satis-
ﬁable. The DAG of T(φ) is depicted in Figure 1.15. The annotations “1” etc
indicate which nodes represent which sub-formulas. Notice that such DAGs
may be constructed by applying the translation clauses for T to sub-formulas
in a bottom-up manner – sharing equal subgraphs were applicable.
The ﬁndings of our SAT solver can be seen in Figure 1.16. The solver
concludes that the indicated node requires the marks T and F for (1.9) to be
met. Such contradictory constraints therefore imply that all formulas T(φ)
whose DAG equals that of this ﬁgure are not satisﬁable. In particular, all

72
1 Propositional logic
p
q
r
¬
∧
∧
∧
¬
¬
¬
∧
¬
¬
∧
¬
¬
= ”3” →”2”
“5” = entire formula
“4”
“3” = p ∧q →r
“2” = p →”1”
“1” = q →r
“2”
“3”
“1”
“4”
“5”
Figure 1.15. The DAG for the translation of ¬((p ∧q →r) →p →q →
r). Labels ‘‘1’’ etc indicate which nodes represent what subformulas.
such φ are unsatisﬁable. This SAT solver has a linear running time in the
size of the DAG for T(φ). Since that size is a linear function of the length
of φ – the translation T causes only a linear blow-up – our SAT solver has
a linear running time in the length of the formula. This linearity came with
a price: our linear solver fails for all formulas of the form ¬(φ1 ∧φ2).
1.6.2 A cubic solver
When we applied our linear SAT solver, we saw two possible outcomes:
we either detected contradictory constraints, meaning that no formula rep-
resented by the DAG is satisﬁable (e.g. Fig. 1.16); or we managed to force
consistent constraints on all nodes, in which case all formulas represented by
this DAG are satisﬁable with those constraints as a witness (e.g. Fig. 1.13).
Unfortunately, there is a third possibility: all forced constraints are consis-
tent with each other, but not all nodes are constrained! We already remarked
that this occurs for formulas of the form ¬(φ1 ∧φ2).

1.6 SAT solvers
73
p
q
r
¬
∧
∧
∧
¬
¬
¬
∧
¬
¬
∧
¬
¬
1: T
2: F
3: T
4: T
4: T
5: F
6: T
5: F
7: T
8: F
9: T
11: F
10: T
10: T
7: T
its conjunction parent
– a contradiction
and ∧frr force F
its children and
∧ti force T
Figure 1.16. The forcing rules, applied to the DAG of Figure 1.15,
detect contradictory constraints at the indicated node – implying that
the initial constraint ‘1:T’ cannot be realized. Thus, formulas represented
by this DAG are not satisfiable.
Recall that checking validity of formulas in CNF is very easy. We already
hinted at the fact that checking satisﬁability of formulas in CNF is hard. To
illustrate, consider the formula
((p ∨(q ∨r)) ∧((p ∨¬q) ∧((q ∨¬r) ∧((r ∨¬p) ∧(¬p ∨(¬q ∨¬r))))))
(1.11)
in CNF – based on Example 4.2, page 77, in [Pap94]. Intuitively, this formula
should not be satisﬁable. The ﬁrst and last clause in (1.11) ‘say’ that at least
one of p, q, and r are false and true (respectively). The remaining three
clauses, in their conjunction, ‘say’ that p, q, and r all have the same truth
value. This cannot be satisﬁable, and a good SAT solver should discover
this without any user intervention. Unfortunately, our linear SAT solver can
neither detect inconsistent constraints nor compute constraints for all nodes.
Figure 1.17 depicts the DAG for T(φ), where φ is as in (1.11); and reveals

74
1 Propositional logic
p
q
r
¬
¬
¬
¬
¬
¬
∧
¬
¬
∧
¬
∧
¬
¬
∧
¬
∧
¬
∧
¬
∧
¬
∧
∧
∧
5: T
6: F
3: T
4: F
5: T
6: F
3: F
4: T
3: T
2: T
1: T
2: T
∧
5: F
4: T
Figure 1.17. The DAG for the translation of the formula in (1.11). It
has a ∧-spine of length 4 as it is a conjunction of five clauses. Its linear
analysis gets stuck: all forced constraints are consistent with each other
but several nodes, including all atoms, are unconstrained.
that our SAT solver got stuck: no inconsistent constraints were found and
not all nodes obtained constraints; in particular, no atom received a mark!
So how can we improve this analysis? Well, we can mimic the role of LEM
to improve the precision of our SAT solver. For the DAG with marks as in
Figure 1.17, pick any node n that is not yet marked. Then test node n by
making two independent computations:
1.
determine which temporary marks are forced by adding to the marks in Fig-
ure 1.17 the T mark only to n; and
2.
determine which temporary marks are forced by adding, again to the marks in
Figure 1.17, the F mark only to n.

1.6 SAT solvers
75
p
q
r
¬
¬
¬
¬
¬
¬
∧
¬
¬
∧
¬
∧
¬
¬
∧
¬
∧
¬
∧
¬
∧
¬
∧
∧
∧
5: T
6: F
3: T
4: F
5: T
6: F
3: F
4: T
3: T
2: T
1: T
2: T
∧
5: F
4: T
a:T
b:F
c:T
f:T
g:F
i:F
h:T
b:F
c:T
b:F
c:T
c:T
constraints
temporary T mark
at test node;
explore consequences
e:F
g:F
d:F
at conjunction
contradictory
Figure 1.18. Marking an unmarked node with T and exploring what
new constraints would follow from this. The analysis shows that this
test marking causes contradictory constraints. We use lowercase letters
‘a:’ etc to denote temporary marks.
If both runs ﬁnd contradictory constraints, the algorithm stops and re-
ports that T(φ) is unsatisﬁable. Otherwise, all nodes that received the same
mark in both of these runs receive that very mark as a permanent one; that
is, we update the mark state of Figure 1.17 with all such shared marks.
We test any further unmarked nodes in the same manner until we either
ﬁnd contradictory permanent marks, a complete witness to satisﬁability (all
nodes have consistent marks), or we have tested all currently unmarked
nodes in this manner without detecting any shared marks. Only in the lat-
ter case does the analysis terminate without knowing whether the formulas
represented by that DAG are satisﬁable.

76
1 Propositional logic
Example 1.49 We revisit our stuck analysis of Figure 1.17. We test a ¬-
node and explore the consequences of setting that ¬-node’s mark to T; Fig-
ure 1.18 shows the result of that analysis. Dually, Figure 1.19 tests the
consequences of setting that ¬-node’s mark to F. Since both runs reveal a
contradiction, the algorithm terminates, ruling that the formula in (1.11) is
not satisﬁable.
In the exercises, you are asked to show that the speciﬁcation of our cubic
SAT solver is sound. Its running time is indeed cubic in the size of the
DAG (and the length of original formula). One factor stems from the linear
SAT solver used in each test run. A second factor is introduced since each
unmarked node has to be tested. The third factor is needed since each new
permanent mark causes all unmarked nodes to be tested again.
p
q
r
¬
¬
¬
¬
¬
¬
∧
¬
¬
∧
¬
∧
¬
¬
∧
¬
∧
¬
∧
¬
∧
¬
∧
∧
∧
5: T
6: F
3: T
4: F
5: T
6: F
3: F
4: T
3: T
2: T
1: T
2: T
∧
5: F
4: T
temporary F mark
at test node;
explore consequences
a: F
c:F
d:T
d:T
e:F
f:T
g:F
e:F
g:F
e:F
contradictory
constraints
at conjunction
b:T
c:F
e:F
f:T
c:F
Figure 1.19. Marking the same unmarked node with F and exploring
what new constraints would follow from this. The analysis shows that
this test marking also causes contradictory constraints.

1.6 SAT solvers
77
∧
¬
∧
∧
¬
∧
∧
¬
¬
∧
p
q
r
¬
1: T
2: F
testing this node
with T renders
justifying to mark
it with F permanently
a contradiction
analysis gets stuck right away
Figure 1.20. Testing the indicated node with T causes contradictory
constraints, so we may mark that node with F permanently. However,
our algorithm does not seem to be able to decide satisfiability of this
DAG even with that optimization.
We deliberately under-speciﬁed our cubic SAT solver, but any implemen-
tation or optimization decisions need to secure soundness of the analysis.
All replies of the form
1.
‘The input formula is not satisﬁable’ and
2.
‘The input formula is satisﬁable under the following valuation . . . ’
have to be correct. The third form of reply ‘Sorry, I could not ﬁgure this one
out.’ is correct by deﬁnition. :-) We brieﬂy discuss two sound modiﬁcations
to the algorithm that introduce some overhead, but may cause the algorithm
to decide many more instances. Consider the state of a DAG right after we
have explored consequences of a temporary mark on a test node.
1.
If that state – permanent plus temporary markings – contains contradictory
constraints, we can erase all temporary marks and mark the test node perma-
nently with the dual mark of its test. That is, if marking node n with v resulted
in a contradiction, it will get a permanent mark v, where T = F and F = T;
otherwise
2.
if that state managed to mark all nodes with consistent constraints, we report
these markings as a witness of satisﬁability and terminate the algorithm.
If none of these cases apply, we proceed as speciﬁed: promote shared marks
of the two test runs to permanent ones, if applicable.
Example 1.50 To see how one of these optimizations may make a diﬀer-
ence, consider the DAG in Figure 1.20. If we test the indicated node with

78
1 Propositional logic
T, contradictory constraints arise. Since any witness of satisﬁability has to
assign some value to that node, we infer that it cannot be T. Thus, we may
permanently assign mark F to that node. For this DAG, such an optimiza-
tion does not seem to help. No test of an unmarked node detects a shared
mark or a shared contradiction. Our cubic SAT solver fails for this DAG.
1.7 Exercises
Exercises 1.1
1. Use ¬, →, ∧and ∨to express the following declarative sentences in propositional
logic; in each case state what your respective propositional atoms p, q, etc. mean:
(a)
*
If the sun shines today, then it won’t shine tomorrow.
(b) Robert was jealous of Yvonne, or he was not in a good mood.
(c) If the barometer falls, then either it will rain or it will snow.
(d)
*
If a request occurs, then either it will eventually be acknowledged, or the
requesting process won’t ever be able to make progress.
(e) Cancer will not be cured unless its cause is determined and a new drug for
cancer is found.
(f) If interest rates go up, share prices go down.
(g) If Smith has installed central heating, then he has sold his car or he has not
paid his mortgage.
(h)
*
Today it will rain or shine, but not both.
(i)
*
If Dick met Jane yesterday, they had a cup of coﬀee together, or they took
a walk in the park.
(j) No shoes, no shirt, no service.
(k) My sister wants a black and white cat.
2. The formulas of propositional logic below implicitly assume the binding priorities
of the logical connectives put forward in Convention 1.3. Make sure that you fully
understand those conventions by reinserting as many brackets as possible. For
example, given p ∧q →r, change it to (p ∧q) →r since ∧binds more tightly
than →.
(a)
*
¬p ∧q →r
(b) (p →q) ∧¬(r ∨p →q)
(c)
*
(p →q) →(r →s ∨t)
(d) p ∨(¬q →p ∧r)
(e)
*
p ∨q →¬p ∧r
(f) p ∨p →¬q
(g)
*
Why is the expression p ∨q ∧r problematic?
Exercises 1.2
1. Prove the validity of the following sequents:
(a) (p ∧q) ∧r, s ∧t ⊢q ∧s

1.7 Exercises
79
(b) p ∧q ⊢q ∧p
(c)
*
(p ∧q) ∧r ⊢p ∧(q ∧r)
(d) p →(p →q), p ⊢q
(e)
*
q →(p →r), ¬r, q ⊢¬p
(f)
*
⊢(p ∧q) →p
(g) p ⊢q →(p ∧q)
(h)
*
p ⊢(p →q) →q
(i)
*
(p →r) ∧(q →r) ⊢p ∧q →r
(j)
*
q →r ⊢(p →q) →(p →r)
(k) p →(q →r), p →q ⊢p →r
(l)
*
p →q, r →s ⊢p ∨r →q ∨s
(m) p ∨q ⊢r →(p ∨q) ∧r
(n)
*
(p ∨(q →p)) ∧q ⊢p
(o)
*
p →q, r →s ⊢p ∧r →q ∧s
(p) p →q ⊢((p ∧q) →p) ∧(p →(p ∧q))
(q) ⊢q →(p →(p →(q →p)))
(r)
*
p →q ∧r ⊢(p →q) ∧(p →r)
(s) (p →q) ∧(p →r) ⊢p →q ∧r
(t) ⊢(p →q) →((r →s) →(p ∧r →q ∧s)); here you might be able to ‘recycle’
and augment a proof from a previous exercise.
(u) p →q ⊢¬q →¬p
(v)
*
p ∨(p ∧q) ⊢p
(w) r, p →(r →q) ⊢p →(q ∧r)
(x)
*
p →(q ∨r), q →s, r →s ⊢p →s
(y)
*
(p ∧q) ∨(p ∧r) ⊢p ∧(q ∨r).
2. For the sequents below, show which ones are valid and which ones aren’t:
(a)
*
¬p →¬q ⊢q →p
(b)
*
¬p ∨¬q ⊢¬(p ∧q)
(c)
*
¬p, p ∨q ⊢q
(d)
*
p ∨q, ¬q ∨r ⊢p ∨r
(e)
*
p →(q ∨r), ¬q, ¬r ⊢¬p without using the MT rule
(f)
*
¬p ∧¬q ⊢¬(p ∨q)
(g)
*
p ∧¬p ⊢¬(r →q) ∧(r →q)
(h) p →q, s →t ⊢p ∨s →q ∧t
(i)
*
¬(¬p ∨q) ⊢p.
3. Prove the validity of the sequents below:
(a) ¬p →p ⊢p
(b) ¬p ⊢p →q
(c) p ∨q, ¬q ⊢p
(d)
*
⊢¬p →(p →(p →q))
(e) ¬(p →q) ⊢q →p
(f) p →q ⊢¬p ∨q
(g) ⊢¬p ∨q →(p →q)

80
1 Propositional logic
(h) p →(q ∨r), ¬q, ¬r |−¬p
(i) (c ∧n) →t, h ∧¬s, h ∧¬(s ∨c) →p |−(n ∧¬t) →p
(j) the two sequents implict in (1.2) on page 20
(k) q |−(p ∧q) ∨(¬p ∧q) using LEM
(l) ¬(p ∧q) |−¬p ∨¬q
(m) p ∧q →r |−(p →r) ∨(q →r)
(n)
*
p ∧q ⊢¬(¬p ∨¬q)
(o) ¬(¬p ∨¬q) ⊢p ∧q
(p) p →q ⊢¬p ∨q possibly without using LEM?
(q)
*
⊢(p →q) ∨(q →r) using LEM
(r) p →q, ¬p →r, ¬q →¬r ⊢q
(s) p →q, r →¬t, q →r ⊢p →¬t
(t) (p →q) →r, s →¬p, t, ¬s ∧t →q ⊢r
(u) (s →p) ∨(t →q) ⊢(s →q) ∨(t →p)
(v) (p ∧q) →r, r →s, q ∧¬s ⊢¬p.
4. Explain why intuitionistic logicians also reject the proof rule PBC.
5. Prove the following theorems of propositional logic:
(a)
*
((p →q) →q) →((q →p) →p)
(b) Given a proof for the sequent of the previous item, do you now have a quick
argument for ((q →p) →p) →((p →q) →q)?
(c) ((p →q) ∧(q →p)) →((p ∨q) →(p ∧q))
(d)
*
(p →q) →((¬p →q) →q).
6. Natural deduction is not the only possible formal framework for proofs in propo-
sitional logic. As an abbreviation, we write Γ to denote any ﬁnite sequence of
formulas φ1, φ2, . . . , φn (n ≥0). Thus, any sequent may be written as Γ ⊢ψ for
an appropriate, possibly empty, Γ. In this exercise we propose a diﬀerent notion
of proof, which states rules for transforming valid sequents into valid sequents.
For example, if we have already a proof for the sequent Γ, φ ⊢ψ, then we ob-
tain a proof of the sequent Γ ⊢φ →ψ by augmenting this very proof with one
application of the rule →i. The new approach expresses this as an inference rule
between sequents:
Γ, φ ⊢ψ
Γ ⊢φ →ψ →i.
The rule ‘assumption’ is written as
φ ⊢φ assumption
i.e. the premise is empty. Such rules are called axioms.
(a) Express all remaining proof rules of Figure 1.2 in such a form. (Hint: some
of your rules may have more than one premise.)
(b) Explain why proofs of Γ ⊢ψ in this new system have a tree-like structure
with Γ ⊢ψ as root.
(c) Prove p ∨(p ∧q) ⊢p in your new proof system.

1.7 Exercises
81
7. Show that
√
2 cannot be a rational number. Proceed by proof by contradiction:
assume that
√
2 is a fraction k/l with integers k and l ̸= 0. On squaring both sides
we get 2 = k2/l2, or equivalently 2l2 = k2. We may assume that any common 2
factors of k and l have been cancelled. Can you now argue that 2l2 has a diﬀerent
number of 2 factors from k2? Why would that be a contradiction and to what?
8. There is an alternative approach to treating negation. One could simply ban the
operator ¬ from propositional logic and think of φ →⊥as ‘being’ ¬φ. Naturally,
such a logic cannot rely on the natural deduction rules for negation. Which of
the rules ¬i, ¬e, ¬¬e and ¬¬i can you simulate with the remaining proof rules
by letting ¬φ be φ →⊥?
9. Let us introduce a new connective φ ↔ψ which should abbreviate (φ →ψ) ∧
(ψ →φ). Design introduction and elimination rules for ↔and show that they
are derived rules if φ ↔ψ is interpreted as (φ →ψ) ∧(ψ →φ).
Exercises 1.3
In order to facilitate reading these exercises we assume below the usual
conventions about binding priorities agreed upon in Convention 1.3.
1. Given the following formulas, draw their corresponding parse tree:
(a) p
(b)
*
p ∧q
(c) p ∧¬q →¬p
(d)
*
p ∧(¬q →¬p)
(e) p →(¬q ∨(q →p))
(f)
*
¬((¬q ∧(p →r)) ∧(r →q))
(g) ¬p ∨(p →q)
(h) (p ∧q) →(¬r ∨(q →r))
(i) ((s ∨(¬p)) →(¬p))
(j) (s ∨((¬p) →(¬p)))
(k) (((s →(r ∨l)) ∨((¬q) ∧r)) →((¬(p →s)) →r))
(l) (p →q) ∧(¬r →(q ∨(¬p ∧r))).
2. For each formula below, list all its subformulas:
(a)
*
p →(¬p ∨(¬¬q →(p ∧q)))
(b) (s →r ∨l) ∨(¬q ∧r) →(¬(p →s) →r)
(c) (p →q) ∧(¬r →(q ∨(¬p ∧r))).
3. Draw the parse tree of a formula φ of propositional logic which is
(a)
*
a negation of an implication
(b) a disjunction whose disjuncts are both conjunctions
(c)
*
a conjunction of conjunctions.
4. For each formula below, draw its parse tree and list all subformulas:
(a)
*
¬(s →(¬(p →(q ∨¬s))))
(b) ((p →¬q) ∨(p ∧r) →s) ∨¬r.

82
1 Propositional logic
∧
→
¬
∧
p
¬
¬
Figure 1.21. A tree that represents an ill-formed formula.
5.
*
For the parse tree in Figure 1.22 ﬁnd the logical formula it represents.
6. For the trees below, ﬁnd their linear representations and check whether they
correspond to well-formed formulas:
(a) the tree in Figure 1.10 on page 44
(b) the tree in Figure 1.23.
7.
*
Draw a parse tree that represents an ill-formed formula such that
(a) one can extend it by adding one or several subtrees to obtain a tree that
represents a well-formed formula;
(b) it is inherently ill-formed; i.e. any extension of it could not correspond to a
well-formed formula.
8. Determine, by trying to draw parse trees, which of the following formulas are
well-formed:
(a) p ∧¬(p ∨¬q) →(r →s)
(b) p ∧¬(p ∨q ∧s) →(r →s)
(c) p ∧¬(p ∨∧s) →(r →s).
Among the ill-formed formulas above which ones, and in how many ways, could
you ‘ﬁx’ by the insertion of brackets only?
Exercises 1.4
1.
*
Construct the truth table for ¬p ∨q and verify that it coincides with the one for
p →q. (By ‘coincide’ we mean that the respective columns of T and F values are
the same.)
2. Compute the complete truth table of the formula
(a)
*
((p →q) →p) →p
(b) represented by the parse tree in Figure 1.3 on page 34

1.7 Exercises
83
¬
→
¬
r
∨
p
∧
q
¬
p
Figure 1.22. A parse tree of a negated implication.

84
1 Propositional logic
¬
→
¬
∧
→
q
p
∨
q
r
p
Figure 1.23. Another parse tree of a negated implication.
(c)
*
p ∨(¬(q ∧(r →q)))
(d) (p ∧q) →(p ∨q)
(e) ((p →¬q) →¬p) →q
(f) (p →q) ∨(p →¬q)
(g) ((p →q) →p) →p
(h) ((p ∨q) →r) →((p →r) ∨(q →r))
(i) (p →q) →(¬p →¬q).
3. Given a valuation and a parsetree of a formula, compute the truth value of the
formula for that valuation (as done in a bottom-up fashion in Figure 1.7 on
page 40) with the parse tree in
(a)
*
Figure 1.10 on page 44 and the valuation in which q and r evaluate to T and
p to F;
(b) Figure 1.4 on page 36 and the valuation in which q evaluates to T and p and
r evaluate to F;
(c) Figure 1.23 where we let p be T, q be F and r be T; and
(d) Figure 1.23 where we let p be F, q be T and r be F.
4. Compute the truth value on the formula’s parse tree, or specify the corresponding
line of a truth table where
(a)
*
p evaluates to F, q to T and the formula is p →(¬q ∨(q →p))
(b)
*
the formula is ¬((¬q ∧(p →r)) ∧(r →q)), p evaluates to F, q to T and r
evaluates to T.

1.7 Exercises
85
5.
*
A formula is valid iﬀit computes T for all its valuations; it is satisﬁable iﬀit
computes T for at least one of its valuations. Is the formula of the parse tree in
Figure 1.10 on page 44 valid? Is it satisﬁable?
6. Let ∗be a new logical connective such that p ∗q does not hold iﬀp and q are
either both false or both true.
(a) Write down the truth table for p ∗q.
(b) Write down the truth table for (p ∗p) ∗(q ∗q).
(c) Does the table in (b) coincide with a table in Figure 1.6 (page 38)? If so,
which one?
(d) Do you know ∗already as a logic gate in circuit design? If so, what is it
called?
7. These exercises let you practice proofs using mathematical induction. Make sure
that you state your base case and inductive step clearly. You should also indicate
where you apply the induction hypothesis.
(a) Prove that
(2 · 1 −1) + (2 · 2 −1) + (2 · 3 −1) + · · · + (2 · n −1) = n2
by mathematical induction on n ≥1.
(b) Let k and l be natural numbers. We say that k is divisible by l if there
exists a natural number p such that k = p · l. For example, 15 is divisible by
3 because 15 = 5 · 3. Use mathematical induction to show that 11n −4n is
divisible by 7 for all natural numbers n ≥1.
(c)
*
Use mathematical induction to show that
12 + 22 + 32 + · · · + n2 = n · (n + 1) · (2n + 1)
6
for all natural numbers n ≥1.
(d)
*
Prove that 2n ≥n + 12 for all natural numbers n ≥4. Here the base case is
n = 4. Is the statement true for any n < 4?
(e) Suppose a post oﬃce sells only 2c| and 3c| stamps. Show that any postage of
2c|, or over, can be paid for using only these stamps. Hint: use mathematical
induction on n, where nc| is the postage. In the inductive step consider two
possibilities: ﬁrst, nc| can be paid for using only 2c| stamps. Second, paying
nc| requires the use of at least one 3c| stamp.
(f) Prove that for every preﬁx of a well-formed propositional logic formula the
number of left brackets is greater or equal to the number of right brackets.
8.
*
The Fibonacci numbers are most useful in modelling the growth of populations.
We deﬁne them by F1
def
= 1, F2
def
= 1 and Fn+1
def
= Fn + Fn−1 for all n ≥2. So
F3
def
= F1 + F2 = 1 + 1 = 2 etc. Show the assertion ‘F3n is even.’ by mathemat-
ical induction on n ≥1. Note that this assertion is saying that the sequence
F3, F6, F9, . . . consists of even numbers only.

86
1 Propositional logic
9. Consider the function rank, deﬁned by
rank(p)
def
= 1
rank(¬φ)
def
= 1 + rank(φ)
rank(φ ◦ψ)
def
= 1 + max(rank(φ), rank(ψ))
where p is any atom, ◦∈{→, ∨, ∧} and max(n, m) is n if n ≥m and m other-
wise. Recall the concept of the height of a formula (Deﬁnition 1.32 on page 44).
Use mathematical induction on the height of φ to show that rank(φ) is nothing
but the height of φ for all formulas φ of propositional logic.
10.
*
Here is an example of why we need to secure the base case for mathematical
induction. Consider the assertion
‘The number n2 + 5n + 1 is even for all n ≥1.’
(a) Prove the inductive step of that assertion.
(b) Show that the base case fails to hold.
(c) Conclude that the assertion is false.
(d) Use mathematical induction to show that n2 + 5n + 1 is odd for all n ≥1.
11. For the soundness proof of Theorem 1.35 on page 46,
(a) explain why we could not use mathematical induction but had to resort to
course-of-values induction;
(b) give justiﬁcations for all inferences that were annotated with ‘why?’ and
(c) complete the case analysis ranging over the ﬁnal proof rule applied; inspect
the summary of natural deduction rules in Figure 1.2 on page 27 to see which
cases are still missing. Do you need to include derived rules?
12. Show that the following sequents are not valid by ﬁnding a valuation in which
the truth values of the formulas to the left of ⊢are T and the truth value of
the formula to the right of ⊢is F.
(a) ¬p ∨(q →p) ⊢¬p ∧q
(b) ¬r →(p ∨q), r ∧¬q ⊢r →q
(c)
*
p →(q →r) ⊢p →(r →q)
(d) ¬p, p ∨q ⊢¬q
(e) p →(¬q ∨r), ¬r ⊢¬q →¬p.
13. For each of the following invalid sequents, give examples of natural language
declarative sentences for the atoms p, q and r such that the premises are true,
but the conclusion false.
(a)
*
p ∨q ⊢p ∧q
(b)
*
¬p →¬q ⊢¬q →¬p
(c) p →q ⊢p ∨q
(d) p →(q ∨r) ⊢(p →q) ∧(p →r).
14. Find a formula of propositional logic φ which contains only the atoms p, q
and r and which is true only when p and q are false, or when ¬q ∧(p ∨r) is
true.

1.7 Exercises
87
15. Use mathematical induction on n to prove the theorem ((φ1 ∧(φ2 ∧(· · · ∧
φn) . . . ) →ψ) →(φ1 →(φ2 →(. . . (φn →ψ) . . . )))).
16. Prove the validity of the following sequents needed to secure the completeness
result for propositional logic:
(a) φ1 ∧¬φ2 ⊢¬(φ1 →φ2)
(b) ¬φ1 ∧¬φ2 ⊢φ1 →φ2
(c) ¬φ1 ∧φ2 ⊢φ1 →φ2
(d) φ1 ∧φ2 ⊢φ1 →φ2
(e) ¬φ1 ∧φ2 ⊢¬(φ1 ∧φ2)
(f) ¬φ1 ∧¬φ2 ⊢¬(φ1 ∧φ2)
(g) φ1 ∧¬φ2 ⊢¬(φ1 ∧φ2)
(h) ¬φ1 ∧¬φ2 ⊢¬(φ1 ∨φ2)
(i) φ1 ∧φ2 ⊢φ1 ∨φ2
(j) ¬φ1 ∧φ2 ⊢φ1 ∨φ2
(k) φ1 ∧¬φ2 ⊢φ1 ∨φ2.
17. Does ⊨φ hold for the φ below? Please justify your answer.
(a) (p →q) ∨(q →r)
(b)
*
((q →(p ∨(q →p))) ∨¬(p →q)) →p.
Exercises 1.5
1. Show that a formula φ is valid iﬀ⊤≡φ, where ⊤is an abbreviation for an
instance p ∨¬p of LEM.
2. Which of these formulas are semantically equivalent to p →(q ∨r)?
(a) q ∨(¬p ∨r)
(b)
*
q ∧¬r →p
(c) p ∧¬r →q
(d)
*
¬q ∧¬r →¬p.
3. An adequate set of connectives for propositional logic is a set such that for every
formula of propositional logic there is an equivalent formula with only connectives
from that set. For example, the set {¬, ∨} is adequate for propositional logic,
because any occurrence of ∧and →can be removed by using the equivalences
φ →ψ ≡¬φ ∨ψ and φ ∧ψ ≡¬(¬φ ∨¬ψ).
(a) Show that {¬, ∧}, {¬, →} and {→, ⊥} are adequate sets of connectives for
propositional logic. (In the latter case, we are treating ⊥as a nullary con-
nective.)
(b) Show that, if C ⊆{¬, ∧, ∨, →, ⊥} is adequate for propositional logic, then
¬ ∈C or ⊥∈C. (Hint: suppose C contains neither ¬ nor ⊥and consider
the truth value of a formula φ, formed by using only the connectives in C,
for a valuation in which every atom is assigned T.)
(c) Is {↔, ¬} adequate? Prove your answer.
4. Use soundness or completeness to show that a sequent φ1, φ2, . . . , φn ⊢ψ has a
proof iﬀφ1 →φ2 →. . . φn →ψ is a tautology.

88
1 Propositional logic
5. Show that the relation ≡is
(a) reﬂexive: φ ≡φ holds for all φ
(b) symmetric: φ ≡ψ implies ψ ≡φ and
(c) transitive: φ ≡ψ and ψ ≡η imply φ ≡η.
6. Show that, with respect to ≡,
(a) ∧and ∨are idempotent:
i. φ ∧φ ≡φ
ii. φ ∨φ ≡φ
(b) ∧and ∨are commutative:
i. φ ∧ψ ≡ψ ∧φ
ii. φ ∨ψ ≡ψ ∨φ
(c) ∧and ∨are associative:
i. φ ∧(ψ ∧η) ≡(φ ∧ψ) ∧η
ii. φ ∨(ψ ∨η) ≡(φ ∨ψ) ∨η
(d) ∧and ∨are absorptive:
i.
*
φ ∧(φ ∨η) ≡φ
ii. φ ∨(φ ∧η) ≡φ
(e) ∧and ∨are distributive:
i. φ ∧(ψ ∨η) ≡(φ ∧ψ) ∨(φ ∧η)
ii.
*
φ ∨(ψ ∧η) ≡(φ ∨ψ) ∧(φ ∨η)
(f) ≡allows for double negation: φ ≡¬¬φ and
(g) ∧and ∨satisﬁes the de Morgan rules:
i. ¬(φ ∧ψ) ≡¬φ ∨¬ψ
ii.
*
¬(φ ∨ψ) ≡¬φ ∧¬ψ.
7. Construct a formula in CNF based on each of the following truth tables:
(a)
*
p
q
φ1
T
T
F
F
T
F
T
F
F
F
F
T
(b)
*
p
q
r
φ2
T
T
T
T
T
T
F
F
T
F
T
F
F
T
T
T
T
F
F
F
F
T
F
F
F
F
T
T
F
F
F
F

1.7 Exercises
89
(c)
r
s
q
φ3
T
T
T
F
T
T
F
T
T
F
T
F
F
T
T
F
T
F
F
T
F
T
F
F
F
F
T
F
F
F
F
T
8.
*
Write a recursive function IMPL FREE which requires a (parse tree of a) proposi-
tional formula as input and produces an equivalent implication-free formula as
output. How many clauses does your case statement need? Recall Deﬁnition 1.27
on page 32.
9.
*
Compute CNF (NNF (IMPL FREE ¬(p →(¬(q ∧(¬p →q)))))).
10. Use structural induction on the grammar of formulas in CNF to show that the
‘otherwise’ case in calls to DISTR applies iﬀboth η1 and η2 are of type D in (1.6)
on page 55.
11. Use mathematical induction on the height of φ to show that the call
CNF (NNF (IMPL FREE φ)) returns, up to associativity, φ if the latter is already
in CNF.
12. Why do the functions CNF and DISTR preserve NNF and why is this important?
13. For the call CNF (NNF (IMPL FREE (φ))) on a formula φ of propositional logic,
explain why
(a) its output is always a formula in CNF
(b) its output is semantically equivalent to φ
(c) that call always terminates.
14. Show that all the algorithms presented in Section 1.5.2 terminate on any input
meeting their precondition. Can you formalise some of your arguments? Note
that algorithms might not call themselves again on formulas with smaller height.
E.g. the call of CNF (φ1 ∨φ2) results in a call DISTR (CNF(φ1), CNF(φ2)), where
CNF(φi) may have greater height than φi. Why is this not a problem?
15. Apply algorithm HORN from page 66 to each of these Horn formulas:
(a)
*
(p ∧q ∧w →⊥) ∧(t →⊥) ∧(r →p) ∧(⊤→r) ∧(⊤→q) ∧(u →
s) ∧(⊤→u)
(b) (p ∧q ∧w →⊥) ∧(t →⊥) ∧(r →p) ∧(⊤→r) ∧(⊤→q) ∧(r ∧u →
w) ∧(u →s) ∧(⊤→u)
(c) (p ∧q ∧s →p) ∧(q ∧r →p) ∧(p ∧s →s)
(d) (p ∧q ∧s →⊥) ∧(q ∧r →p) ∧(⊤→s)
(e) (p5 →p11) ∧(p2 ∧p3 ∧p5 →p13) ∧(⊤→p5) ∧(p5 ∧p11 →⊥)
(f) (⊤→q) ∧(⊤→s) ∧(w →⊥) ∧(p ∧q ∧s →⊥) ∧(v →s) ∧(⊤→
r) ∧(r →p)

90
1 Propositional logic
(g)
*
(⊤→q) ∧(⊤→s) ∧(w →⊥) ∧(p ∧q ∧s →v) ∧(v →s) ∧(⊤→
r) ∧(r →p).
16. Explain why the algorithm HORN fails to work correctly if we change the concept
of Horn formulas by extending the clause for P on page 65 to P ::= ⊥| ⊤|
p | ¬p?
17. What can you say about the CNF of Horn formulas. More precisely, can you
specify syntactic criteria for a CNF that ensure that there is an equivalent Horn
formula? Can you describe informally programs which would translate from one
form of representation into another?
Exercises 1.6
1. Use mathematical induction to show that, for all φ of (1.3) on page 33,
(a) T(φ) can be generated by (1.10) on page 69,
(b) T(φ) has the same set of valuations as φ, and
(c) the set of valuations in which φ is true equals the set of valuations in which
T(φ) is true.
2.
*
Show that all rules of Figure 1.14 (page 71) are sound: if all current marks
satisfy the invariant (1.9) from page 68, then this invariant still holds if the
derived constraint of that rule becomes an additional mark.
3. In Figure 1.16 on page 73 we detected a contradiction which secured the validity
of the sequent p ∧q →r ⊢p →q →r. Use the same method with the linear SAT
solver to show that the sequent ⊢(p →q) ∨(r →p) is valid. (This is interest-
ing since we proved this validity in natural deduction with a judicious choice
of the proof rule LEM; and the linear SAT solver does not employ any case
analysis.)
4.
*
Consider the sequent p ∨q, p →r ⊢r. Determine a DAG which is not satisﬁable
iﬀthis sequent is valid. Tag the DAG’s root node with ‘1: T,’ apply the forcing
laws to it, and extract a witness to the DAG’s satisﬁability. Explain in what
sense this witness serves as an explanation for the fact that p ∨q, p →r ⊢r is
not valid.
5. Explain in what sense the SAT solving technique, as presented in this chapter,
can be used to check whether formulas are tautologies.
6. For φ from (1.10), can one reverse engineer φ from the DAG of T(φ)?
7. Consider a modiﬁcation of our method which initially tags a DAG’s root node
with ‘1: F.’ In that case,
(a) are the forcing laws still sound? If so, state the invariant.
(b) what can we say about the formula(s) a DAG represents if
i. we detect contradictory constraints?
ii. we compute consistent forced constraints for each node?
8. Given an arbitrary Horn formula φ, compare our linear SAT solver – applied
to T(φ) –
to the marking algorithm – applied to φ. Discuss similarities and
diﬀerences of these approaches.

1.8 Bibliographic notes
91
9. Consider Figure 1.20 on page 77. Verify that
(a) its test produces contradictory constraints
(b) its cubic analysis does not decide satisﬁability, regardless of whether the
two optimizations we described are present.
10. Verify that the DAG of Figure 1.17 (page 74) is indeed the one obtained for
T(φ), where φ is the formula in (1.11) on page 73.
11.
*
An implementor may be concerned with the possibility that the answers to the
cubic SAT solver may depend on a particular order in which we test unmarked
nodes or use the rules in Figure 1.14. Give a semi-formal argument for why the
analysis results don’t depend on such an order.
12. Find a formula φ such that our cubic SAT solver cannot decide the satisﬁability
of T(φ).
13. Advanced Project: Write a complete implementation of the cubic SAT solver
described in Section 1.6.2. It should read formulas from the keyboard or a ﬁle;
should assume right-associativity of ∨, ∧, and →(respectively); compute the
DAG of T(φ); perform the cubic SAT solver next. Think also about including
appropriate user output, diagnostics, and optimizations.
14. Show that our cubic SAT solver speciﬁed in this section
(a) terminates on all syntactically correct input;
(b) satisﬁes the invariant (1.9) after the ﬁrst permanent marking;
(c) preserves (1.9) for all permanent markings it makes;
(d) computes only correct satisﬁability witnesses;
(e) computes only correct ‘not satisﬁable’ replies; and
(f) remains to be correct under the two modiﬁcations described on page 77 for
handling results of a node’s two test runs.
1.8 Bibliographic notes
Logic has a long history stretching back at least 2000 years, but the truth-
value semantics of propositional logic presented in this and every logic text-
book today was invented only about 160 years ago, by G. Boole [Boo54].
Boole used the symbols + and · for disjunction and conjunction.
Natural deduction was invented by G. Gentzen [Gen69], and further de-
veloped by D. Prawitz [Pra65]. Other proof systems existed before then, no-
tably axiomatic systems which present a small number of axioms together
with the rule modus ponens (which we call →e). Proof systems often present
as small a number of axioms as possible; and only for an adequate set of con-
nectives such as →and ¬. This makes them hard to use in practice. Gentzen
improved the situation by inventing the idea of working with assumptions
(used by the rules →i, ¬i and ∨e) and by treating all the connectives sepa-
rately.

92
1 Propositional logic
Our linear and cubic SAT solvers are variants of St˚almarck’s method
[SS90], a SAT solver which is patented in Sweden and in the United States
of America.
Further historical remarks, and also pointers to other contemporary books
about propositional and predicate logic, can be found in the bibliographic
remarks at the end of Chapter 2. For an introduction to algorithms and data
structures see e.g. [Wei98].

2
Predicate logic
2.1 The need for a richer language
In the ﬁrst chapter, we developed propositional logic by examining it from
three diﬀerent angles: its proof theory (the natural deduction calculus), its
syntax (the tree-like nature of formulas) and its semantics (what these for-
mulas actually mean). From the outset, this enterprise was guided by the
study of declarative sentences, statements about the world which can, for
every valuation or model, be given a truth value.
We begin this second chapter by pointing out the limitations of propo-
sitional logic with respect to encoding declarative sentences. Propositional
logic dealt quite satisfactorily with sentence components like not, and, or
and if . . . then, but the logical aspects of natural and artiﬁcial languages
are much richer than that. What can we do with modiﬁers like there exists
. . . , all . . . , among . . . and only . . . ? Here, propositional logic shows clear
limitations and the desire to express more subtle declarative sentences led
to the design of predicate logic, which is also called ﬁrst-order logic.
Let us consider the declarative sentence
Every student is younger than some instructor.
(2.1)
In propositional logic, we could identify this assertion with a propositional
atom p. However, that fails to reﬂect the ﬁner logical structure of this sen-
tence. What is this statement about? Well, it is about being a student, being
an instructor and being younger than somebody else. These are all proper-
ties of some sort, so we would like to have a mechanism for expressing them
together with their logical relationships and dependences.
We now use predicates for that purpose. For example, we could write
S(andy) to denote that Andy is a student and I(paul) to say that Paul is an
instructor. Likewise, Y (andy, paul) could mean that Andy is younger than
93

94
2 Predicate logic
Paul. The symbols S, I and Y are called predicates. Of course, we have to
be clear about their meaning. The predicate Y could have meant that the
second person is younger than the ﬁrst one, so we need to specify exactly
what these symbols refer to.
Having such predicates at our disposal, we still need to formalise those
parts of the sentence above which speak of every and some. Obviously, this
sentence refers to the individuals that make up some academic community
(left implicit by the sentence), like Kansas State University or the University
of Birmingham, and it says that for each student among them there is an
instructor among them such that the student is younger than the instructor.
These predicates are not yet enough to allow us to express the sentence
in (2.1). We don’t really want to write down all instances of S(·) where · is
replaced by every student’s name in turn. Similarly, when trying to codify
a sentence having to do with the execution of a program, it would be rather
laborious to have to write down every state of the computer. Therefore,
we employ the concept of a variable. Variables are written u, v, w, x, y, z, . . .
or x1, y3, u5, . . . and can be thought of as place holders for concrete values
(like a student, or a program state). Using variables, we can now specify the
meanings of S, I and Y more formally:
S(x) :
x is a student
I(x) :
x is an instructor
Y (x, y) :
x is younger than y.
Note that the names of the variables are not important, provided that we
use them consistently. We can state the intended meaning of I by writing
I(y) :
y is an instructor
or, equivalently, by writing
I(z) :
z is an instructor.
Variables are mere place holders for objects. The availability of variables is
still not suﬃcient for capturing the essence of the example sentence above.
We need to convey the meaning of ‘Every student x is younger than some
instructor y.’ This is where we need to introduce quantiﬁers ∀(read: ‘for
all’) and ∃(read: ‘there exists’ or ‘for some’) which always come attached
to a variable, as in ∀x (‘for all x’) or in ∃z (‘there exists z’, or ‘there is some
z’). Now we can write the example sentence in an entirely symbolic way as
∀x (S(x) →(∃y (I(y) ∧Y (x, y)))).

2.1 The need for a richer language
95
Actually, this encoding is rather a paraphrase of the original sentence. In
our example, the re-translation results in
For every x, if x is a student, then there is some y which is an
instructor such that x is younger than y.
Diﬀerent predicates can have a diﬀerent number of arguments. The predi-
cates S and I have just one (they are called unary predicates), but predicate
Y requires two arguments (it is called a binary predicate). Predicates with
any ﬁnite number of arguments are possible in predicate logic.
Another example is the sentence
Not all birds can ﬂy.
For that we choose the predicates B and F which have one argument ex-
pressing
B(x) :
x is a bird
F(x) :
x can ﬂy.
The sentence ‘Not all birds can ﬂy’ can now be coded as
¬(∀x (B(x) →F(x)))
saying: ‘It is not the case that all things which are birds can ﬂy.’ Alterna-
tively, we could code this as
∃x (B(x) ∧¬F(x))
meaning: ‘There is some x which is a bird and cannot ﬂy.’ Note that the
ﬁrst version is closer to the linguistic structure of the sentence above. These
two formulas should evaluate to T in the world we currently live in since, for
example, penguins are birds which cannot ﬂy. Shortly, we address how such
formulas can be given their meaning in general. We will also explain why
formulas like the two above are indeed equivalent semantically.
Coding up complex facts expressed in English sentences as logical formulas
in predicate logic is important – e.g. in software design with UML or in
formal speciﬁcation of safety-critical systems – and much more care must be
taken than in the case of propositional logic. However, once this translation
has been accomplished our main objective is to reason symbolically (⊢) or
semantically (⊨) about the information expressed in those formulas.
In Section 2.3, we extend our natural deduction calculus of propositional
logic so that it covers logical formulas of predicate logic as well. In this way
we are able to prove the validity of sequents φ1, φ2, . . . , φn ⊢ψ in a similar
way to that in the ﬁrst chapter.

96
2 Predicate logic
In Section 2.4, we generalize the valuations of Chapter 1 to a proper
notion of models, real or artiﬁcial worlds in which formulas of predicate
logic can be true or false, which allows us to deﬁne semantic entailment
φ1, φ2, . . . , φn ⊨ψ.
The latter expresses that, given any such model in which all φ1, φ2, . . . , φn
hold, it is the case that ψ holds in that model as well. In that case, one
also says that ψ is semantically entailed by φ1, φ2, . . . , φn. Although this
deﬁnition of semantic entailment closely matches the one for propositional
logic in Deﬁnition 1.34, the process of evaluating a predicate formula diﬀers
from the computation of truth values for propositional logic in the treatment
of predicates (and functions). We discuss it in detail in Section 2.4.
It is outside the scope of this book to show that the natural deduction
calculus for predicate logic is sound and complete with respect to semantic
entailment; but it is indeed the case that
φ1, φ2, . . . , φn ⊢ψ
iﬀ
φ1, φ2, . . . , φn ⊨ψ
for formulas of the predicate calculus. The ﬁrst proof of this was done by
the mathematician K. G¨odel.
What kind of reasoning must predicate logic be able to support? To get
a feel for that, let us consider the following argument:
No books are gaseous. Dictionaries are books. Therefore, no dictio-
nary is gaseous.
The predicates we choose are
B(x) :
x is a book
G(x) :
x is gaseous
D(x) :
x is a dictionary.
Evidently, we need to build a proof theory and semantics that allow us to
derive the validity and semantic entailment, respectively, of
¬∃x (B(x) ∧G(x)), ∀x (D(x) →B(x)) ⊢¬∃x (D(x) ∧G(x))
¬∃x (B(x) ∧G(x)), ∀x (D(x) →B(x)) ⊨¬∃x (D(x) ∧G(x)).
Verify that these sequents express the argument above in a symbolic form.
Predicate logic extends propositional logic not only with quantiﬁers but
with one more concept, that of function symbols. Consider the declarative
sentence
Every child is younger than its mother.

2.1 The need for a richer language
97
Using predicates, we could express this sentence as
∀x ∀y (C(x) ∧M(y, x) →Y (x, y))
where C(x) means that x is a child, M(x, y) means that x is y’s mother
and Y (x, y) means that x is younger than y. (Note that we actually used
M(y, x) (y is x’s mother), not M(x, y).) As we have coded it, the sentence
says that, for all children x and any mother y of theirs, x is younger than y.
It is not very elegant to say ‘any of x’s mothers’, since we know that every
individual has one and only one mother1. The inelegance of coding ‘mother’
as a predicate is even more apparent if we consider the sentence
Andy and Paul have the same maternal grandmother.
which, using ‘variables’ a and p for Andy and Paul and a binary predicate
M for mother as before, becomes
∀x ∀y ∀u ∀v (M(x, y) ∧M(y, a) ∧M(u, v) ∧M(v, p) →x = u).
This formula says that, if y and v are Andy’s and Paul’s mothers, respec-
tively, and x and u are their mothers (i.e. Andy’s and Paul’s maternal grand-
mothers, respectively), then x and u are the same person. Notice that we
used a special predicate in predicate logic, equality; it is a binary predicate,
i.e. it takes two arguments, and is written =. Unlike other predicates, it is
usually written in between its arguments rather than before them; that is,
we write x = y instead of = (x, y) to say that x and y are equal.
The function symbols of predicate logic give us a way of avoiding this
ugly encoding, for they allow us to represent y’s mother in a more direct
way. Instead of writing M(x, y) to mean that x is y’s mother, we simply
write m(y) to mean y’s mother. The symbol m is a function symbol: it takes
one argument and returns the mother of that argument. Using m, the two
sentences above have simpler encodings than they had using M:
∀x (C(x) →Y (x, m(x)))
now expresses that every child is younger than its mother. Note that we
need only one variable rather than two. Representing that Andy and Paul
have the same maternal grandmother is even simpler; it is written
m(m(a)) = m(m(p))
quite directly saying that Andy’s maternal grandmother is the same person
as Paul’s maternal grandmother.
1 We assume that we are talking about genetic mothers, not adopted mothers, step mothers etc.

98
2 Predicate logic
One can always do without function symbols, by using a predicate symbol
instead. However, it is usually neater to use function symbols whenever pos-
sible, because we get more compact encodings. However, function symbols
can be used only in situations in which we want to denote a single object.
Above, we rely on the fact that every individual has a uniquely deﬁned
mother, so that we can talk about x’s mother without risking any ambigu-
ity (for example, if x had no mother, or two mothers). For this reason, we
cannot have a function symbol b(·) for ‘brother’. It might not make sense to
talk about x’s brother, for x might not have any brothers, or he might have
several. ‘Brother’ must be coded as a binary predicate.
To exemplify this point further, if Mary has several brothers, then the
claim that ‘Ann likes Mary’s brother’ is ambiguous. It might be that Ann
likes one of Mary’s brothers, which we would write as
∃x (B(x, m) ∧L(a, x))
where B and L mean ‘is brother of’ and ‘likes,’ and a and m mean Ann and
Mary. This sentence says that there exists an x which is a brother of Mary
and is liked by Ann. Alternatively, if Ann likes all of Mary’s brothers, we
write it as
∀x (B(x, m) →L(a, x))
saying that any x which is a brother of Mary is liked by Ann. Predicates
should be used if a ‘function’ such as ‘your youngest brother’ does not always
have a value.
Diﬀerent function symbols may take diﬀerent numbers of arguments.
Functions may take zero arguments and are then called constants: a and
p above are constants for Andy and Paul, respectively. In a domain involv-
ing students and the grades they get in diﬀerent courses, one might have
the binary function symbol g(·, ·) taking two arguments: g(x, y) refers to the
grade obtained by student x in course y.
2.2 Predicate logic as a formal language
The discussion of the preceding section was intended to give an impression
of how we code up sentences as formulas of predicate logic. In this section,
we will be more precise about it, giving syntactic rules for the formation
of predicate logic formulas. Because of the power of predicate logic, the
language is much more complex than that of propositional logic.
The ﬁrst thing to note is that there are two sorts of things involved in
a predicate logic formula. The ﬁrst sort denotes the objects that we are

2.2 Predicate logic as a formal language
99
talking about: individuals such as a and p (referring to Andy and Paul) are
examples, as are variables such as x and v. Function symbols also allow us
to refer to objects: thus, m(a) and g(x, y) are also objects. Expressions in
predicate logic which denote objects are called terms.
The other sort of things in predicate logic denotes truth values; expres-
sions of this kind are formulas: Y (x, m(x)) is a formula, though x and m(x)
are terms.
A predicate vocabulary consists of three sets: a set of predicate symbols
P, a set of function symbols F and a set of constant symbols C. Each pred-
icate symbol and each function symbol comes with an arity, the number of
arguments it expects. In fact, constants can be thought of as functions which
don’t take any arguments (and we even drop the argument brackets) – there-
fore, constants live in the set F together with the ‘true’ functions which do
take arguments. From now on, we will drop the set C, since it is convenient to
do so, and stipulate that constants are 0-arity, so-called nullary, functions.
2.2.1 Terms
The terms of our language are made up of variables, constant symbols
and functions applied to those. Functions may be nested, as in m(m(x))
or g(m(a), c): the grade obtained by Andy’s mother in the course c.
Deﬁnition 2.1 Terms are deﬁned as follows.
r Any variable is a term.
r If c ∈F is a nullary function, then c is a term.
r If t1, t2, . . . , tn are terms and f ∈F has arity n > 0, then f(t1, t2, . . . , tn) is a
term.
r Nothing else is a term.
In Backus Naur form we may write
t ::= x | c | f(t, . . . , t)
where x ranges over a set of variables var, c over nullary function symbols
in F, and f over those elements of F with arity n > 0.
It is important to note that
r the ﬁrst building blocks of terms are constants (nullary functions) and variables;
r more complex terms are built from function symbols using as many previously
built terms as required by such function symbols; and
r the notion of terms is dependent on the set F. If you change it, you change the
set of terms.

100
2 Predicate logic
Example 2.2 Suppose n, f and g are function symbols, respectively
nullary, unary and binary. Then g(f(n), n) and f(g(n, f(n))) are terms, but
g(n) and f(f(n), n) are not (they violate the arities). Suppose 0, 1, . . . are
nullary, s is unary, and +, −, and ∗are binary. Then ∗(−(2, +(s(x), y)), x)
is a term, whose parse tree is illustrated in Figure 2.14 (page 159). Usually,
the binary symbols are written inﬁx rather than preﬁx; thus, the term is
usually written (2 −(s(x) + y)) ∗x.
2.2.2 Formulas
The choice of sets P and F for predicate and function symbols, respectively,
is driven by what we intend to describe. For example, if we work on a
database representing relations between our kin we might want to consider
P = {M, F, S, D}, referring to being male, being female, being a son of . . .
and being a daughter of . . . . Naturally, F and M are unary predicates (they
take one argument) whereas D and S are binary (taking two). Similarly, we
may deﬁne F = {mother-of, father-of}.
We already know what the terms over F are. Given that knowledge, we
can now proceed to deﬁne the formulas of predicate logic.
Deﬁnition 2.3 We deﬁne the set of formulas over (F, P) inductively, using
the already deﬁned set of terms over F:
r If P ∈P is a predicate symbol of arity n ≥1, and if t1, t2, . . . , tn are terms over
F, then P(t1, t2, . . . , tn) is a formula.
r If φ is a formula, then so is (¬φ).
r If φ and ψ are formulas, then so are (φ ∧ψ), (φ ∨ψ) and (φ →ψ).
r If φ is a formula and x is a variable, then (∀x φ) and (∃x φ) are formulas.
r Nothing else is a formula.
Note how the arguments given to predicates are always terms. This can also
be seen in the Backus Naur form (BNF) for predicate logic:
φ ::= P(t1, t2, . . . , tn) | (¬φ) | (φ ∧φ) | (φ ∨φ) | (φ →φ) | (∀x φ) | (∃x φ)
(2.2)
where P ∈P is a predicate symbol of arity n ≥1, ti are terms over F and x
is a variable. Recall that each occurrence of φ on the right-hand side of the
::= stands for any formula already constructed by these rules. (What role
could predicate symbols of arity 0 play?)

2.2 Predicate logic as a formal language
101
∀x
∧
→
S
P
Q
x
y
x
x
Figure 2.1. A parse tree of a predicate logic formula.
Convention 2.4 For convenience, we retain the usual binding priorities
agreed upon in Convention 1.3 and add that ∀y and ∃y bind like ¬. Thus,
the order is:
r ¬, ∀y and ∃y bind most tightly;
r then ∨and ∧;
r then →, which is right-associative.
We also often omit brackets around quantiﬁers, provided that doing so in-
troduces no ambiguities.
Predicate logic formulas can be represented by parse trees. For example,
the parse tree in Figure 2.1 represents the formula ∀x ((P(x) →Q(x)) ∧
S(x, y)).
Example 2.5 Consider translating the sentence
Every son of my father is my brother.
into predicate logic. As before, the design choice is whether we represent
‘father’ as a predicate or as a function symbol.
1.
As a predicate. We choose a constant m for ‘me’ or ‘I,’ so m is a term, and we
choose further {S, F, B} as the set of predicates with meanings

102
2 Predicate logic
S(x, y) :
x is a son of y
F(x, y) :
x is the father of y
B(x, y) :
x is a brother of y.
Then the symbolic encoding of the sentence above is
∀x ∀y (F(x, m) ∧S(y, x) →B(y, m))
(2.3)
saying: ‘For all x and all y, if x is a father of m and if y is a son of x, then y is
a brother of m.’
2.
As a function. We keep m, S and B as above and write f for the function which,
given an argument, returns the corresponding father. Note that this works only
because fathers are unique and always deﬁned, so f really is a function as
opposed to a mere relation.
The symbolic encoding of the sentence above is now
∀x (S(x, f(m)) →B(x, m))
(2.4)
meaning: ‘For all x, if x is a son of the father of m, then x is a brother of m;’
it is less complex because it involves only one quantiﬁer.
Formal speciﬁcations require domain-speciﬁc knowledge. Domain-experts
often don’t make some of this knowledge explicit, so a speciﬁer may miss
important constraints for a model or implementation. For example, the spec-
iﬁcation in (2.3) and (2.4) may seem right, but what about the case when
the values of x and m are equal? If the domain of kinship is not common
knowledge, then a speciﬁer may not realize that a man cannot be his own
brother. Thus, (2.3) and (2.4) are not completely correct!
2.2.3 Free and bound variables
The introduction of variables and quantiﬁers allows us to express the notions
of all . . . and some . . . Intuitively, to verify that ∀x Q(x) is true amounts
to replacing x by any of its possible values and checking that Q holds for
each one of them. There are two important and diﬀerent senses in which such
formulas can be ‘true.’ First, if we give concrete meanings to all predicate and
function symbols involved we have a model and can check whether a formula
is true for this particular model. For example, if a formula encodes a required
behaviour of a hardware circuit, then we would want to know whether it is
true for the model of the circuit. Second, one sometimes would like to ensure
that certain formulas are true for all models. Consider P(c) ∧∀y(P(y) →
Q(y)) →Q(c) for a constant c; clearly, this formula should be true no matter
what model we are looking at. It is this second kind of truth which is the
primary focus of Section 2.3.

2.2 Predicate logic as a formal language
103
Unfortunately, things are more complicated if we want to deﬁne formally
what it means for a formula to be true in a given model. Ideally, we seek a
deﬁnition that we could use to write a computer program verifying that a
formula holds in a given model. To begin with, we need to understand that
variables occur in diﬀerent ways. Consider the formula
∀x ((P(x) →Q(x)) ∧S(x, y)).
We draw its parse tree in the same way as for propositional formulas, but
with two additional sorts of nodes:
r The quantiﬁers ∀x and ∃y form nodes and have, like negation, just one subtree.
r Predicate expressions, which are generally of the form P(t1, t2, . . . , tn), have the
symbol P as a node, but now P has n many subtrees, namely the parse trees of
the terms t1, t2, . . . , tn.
So in our particular case above we arrive at the parse tree in Figure 2.1.
You can see that variables occur at two diﬀerent sorts of places. First, they
appear next to quantiﬁers ∀and ∃in nodes like ∀x and ∃z; such nodes always
have one subtree, subsuming their scope to which the respective quantiﬁer
applies.
The other sort of occurrence of variables is leaf nodes containing variables.
If variables are leaf nodes, then they stand for values that still have to be
made concrete. There are two principal such occurrences:
1.
In our example in Figure 2.1, we have three leaf nodes x. If we walk up the
tree beginning at any one of these x leaves, we run into the quantiﬁer ∀x. This
means that those occurrences of x are actually bound to ∀x so they represent,
or stand for, any possible value of x.
2.
In walking upwards, the only quantiﬁer that the leaf node y runs into is ∀x but
that x has nothing to do with y; x and y are diﬀerent place holders. So y is free
in this formula. This means that its value has to be speciﬁed by some additional
information, for example, the contents of a location in memory.
Deﬁnition 2.6 Let φ be a formula in predicate logic. An occurrence of x
in φ is free in φ if it is a leaf node in the parse tree of φ such that there
is no path upwards from that node x to a node ∀x or ∃x. Otherwise, that
occurrence of x is called bound. For ∀x φ, or ∃x φ, we say that φ – minus
any of φ’s subformulas ∃x ψ, or ∀x ψ – is the scope of ∀x, respectively ∃x.
Thus, if x occurs in φ, then it is bound if, and only if, it is in the scope of
some ∃x or some ∀x; otherwise it is free. In terms of parse trees, the scope
of a quantiﬁer is just its subtree, minus any subtrees which re-introduce a

104
2 Predicate logic
→
∀x
∨
∧
¬
Q
P
Q
P
y
x
x
x
free
free
bound
bound
Figure 2.2. A parse tree of a predicate logic formula illustrating free
and bound occurrences of variables.
quantiﬁer for x; e.g. the scope of ∀x in ∀x (P(x) →∃x Q(x)) is P(x). It is
quite possible, and common, that a variable is bound and free in a formula.
Consider the formula
(∀x (P(x) ∧Q(x))) →(¬P(x) ∨Q(y))
and its parse tree in Figure 2.2. The two x leaves in the subtree of ∀x are
bound since they are in the scope of ∀x, but the leaf x in the right subtree of
→is free since it is not in the scope of any quantiﬁer ∀x or ∃x. Note, however,
that a single leaf either is under the scope of a quantiﬁer, or it isn’t. Hence
individual occurrences of variables are either free or bound, never both at
the same time.
2.2.4 Substitution
Variables are place holders so we must have some means of replacing them
with more concrete information. On the syntactic side, we often need to
replace a leaf node x by the parse tree of an entire term t. Recall from the
deﬁnition of formulas that any replacement of x may only be a term; it
could not be a predicate expression, or a more complex formula, for x serves
as a term to a predicate symbol one step higher up in the parse tree (see
Deﬁnition 2.1 and the grammar in (2.2)). In substituting t for x we have to

2.2 Predicate logic as a formal language
105
leave untouched the bound leaves x since they are in the scope of some ∃x
or ∀x, i.e. they stand for some unspeciﬁed or all values respectively.
Deﬁnition 2.7 Given a variable x, a term t and a formula φ we deﬁne φ[t/x]
to be the formula obtained by replacing each free occurrence of variable x
in φ with t.
Substitutions are easily understood by looking at some examples. Let f be a
function symbol with two arguments and φ the formula with the parse tree
in Figure 2.1. Then f(x, y) is a term and φ[f(x, y)/x] is just φ again. This
is true because all occurrences of x are bound in φ, so none of them gets
substituted.
Now consider φ to be the formula with the parse tree in Figure 2.2. Here
we have one free occurrence of x in φ, so we substitute the parse tree of
f(x, y) for that free leaf node x and obtain the parse tree in Figure 2.3.
Note that the bound x leaves are unaﬀected by this operation. You can see
that the process of substitution is straightforward, but requires that it be
applied only to the free occurrences of the variable to be substituted.
A word on notation: in writing φ[t/x], we really mean this to be the
formula obtained by performing the operation [t/x] on φ. Strictly speaking,
the chain of symbols φ[t/x] is not a logical formula, but its result will be a
formula, provided that φ was one in the ﬁrst place.
x replaced by the term f(x, y)
x
y
f
P
¬
∨
Q
y
→
∀x
∧
P
Q
x
x
Figure 2.3. A parse tree of a formula resulting from substitution.

106
2 Predicate logic
Unfortunately, substitutions can give rise to undesired side eﬀects. In
performing a substitution φ[t/x], the term t may contain a variable y, where
free occurrences of x in φ are under the scope of ∃y or ∀y in φ. By carrying
out this substitution φ[t/x], the value y, which might have been ﬁxed by a
concrete context, gets caught in the scope of ∃y or ∀y. This binding capture
overrides the context speciﬁcation of the concrete value of y, for it will now
stand for ‘some unspeciﬁed’ or ‘all ,’ respectively. Such undesired variable
captures are to be avoided at all costs.
Deﬁnition 2.8 Given a term t, a variable x and a formula φ, we say that
t is free for x in φ if no free x leaf in φ occurs in the scope of ∀y or ∃y for
any variable y occurring in t.
This deﬁnition is maybe hard to swallow. Let us think of it in terms of
parse trees. Given the parse tree of φ and the parse tree of t, we can perform
the substitution [t/x] on φ to obtain the formula φ[t/x]. The latter has a
parse tree where all free x leaves of the parse tree of φ are replaced by the
parse tree of t. What ‘t is free for x in φ’ means is that the variable leaves of
the parse tree of t won’t become bound if placed into the bigger parse tree
of φ[t/x]. For example, if we consider x, t and φ in Figure 2.3, then t is free
for x in φ since the new leaf variables x and y of t are not under the scope
of any quantiﬁers involving x or y.
Example 2.9 Consider the φ with parse tree in Figure 2.4 and let t be
f(y, y). All two occurrences of x in φ are free. The leftmost occurrence of
x could be substituted since it is not in the scope of any quantiﬁer, but
substituting the rightmost x leaf introduces a new variable y in t which
becomes bound by ∀y. Therefore, f(y, y) is not free for x in φ.
What if there are no free occurrences of x in φ? Inspecting the deﬁnition
of ‘t is free for x in φ,’ we see that every term t is free for x in φ in that
case, since no free variable x of φ is below some quantiﬁer in the parse tree
of φ. So the problematic situation of variable capture in performing φ[t/x]
cannot occur. Of course, in that case φ[t/x] is just φ again.
It might be helpful to compare ‘t is free for x in φ’ with a precondition of
calling a procedure for substitution. If you are asked to compute φ[t/x] in
your exercises or exams, then that is what you should do; but any reasonable
implementation of substitution used in a theorem prover would have to check
whether t is free for x in φ and, if not, rename some variables with fresh
ones to avoid the undesirable capture of variables.

2.3 Proof theory of predicate logic
107
the term f(y, y) is
not free for x in
this formula
∀y
x
→
P
x
y
S
Q
∧
Figure 2.4. A parse tree for which a substitution has dire consequences.
2.3 Proof theory of predicate logic
2.3.1 Natural deduction rules
Proofs in the natural deduction calculus for predicate logic are similar to
those for propositional logic in Chapter 1, except that we have new proof
rules for dealing with the quantiﬁers and with the equality symbol. Strictly
speaking, we are overloading the previously established proof rules for the
propositional connectives ∧, ∨etc. That simply means that any proof rule
of Chapter 1 is still valid for logical formulas of predicate logic (we origi-
nally deﬁned those rules for logical formulas of propositional logic). As in
the natural deduction calculus for propositional logic, the additional rules
for the quantiﬁers and equality will come in two ﬂavours: introduction and
elimination rules.
The proof rules for equality
First, let us state the proof rules for
equality. Here equality does not mean syntactic, or intensional, equality,
but equality in terms of computation results. In either of these senses, any
term t has to be equal to itself. This is expressed by the introduction rule
for equality:
t = t
=i
(2.5)
which is an axiom (as it does not depend on any premises). Notice that it

108
2 Predicate logic
may be invoked only if t is a term, our language doesn’t permit us to talk
about equality between formulas.
This rule is quite evidently sound, but it is not very useful on its own.
What we need is a principle that allows us to substitute equals for equals
repeatedly. For example, suppose that y ∗(w + 2) equals y ∗w + y ∗2; then
it certainly must be the case that z ≥y ∗(w + 2) implies z ≥y ∗w + y ∗2
and vice versa. We may now express this substitution principle as the rule
=e:
t1 = t2
φ[t1/x]
φ[t2/x]
=e.
Note that t1 and t2 have to be free for x in φ, whenever we want to apply
the rule =e; this is an example of a side condition of a proof rule.
Convention 2.10 Throughout this section, when we write a substitution
in the form φ[t/x], we implicitly assume that t is free for x in φ; for, as we
saw in the last section, a substitution doesn’t make sense otherwise.
We obtain proof
1
(x + 1) = (1 + x)
premise
2
(x + 1 > 1) →(x + 1 > 0)
premise
3
(1 + x > 1) →(1 + x > 0)
=e 1, 2
establishing the validity of the sequent
x + 1 = 1 + x, (x + 1 > 1) →(x + 1 > 0) ⊢(1 + x) > 1 →(1 + x) > 0.
In this particular proof t1 is (x + 1), t2 is (1 + x) and φ is (x > 1) →
(x > 0). We used the name =e since it reﬂects what this rule is doing to
data: it eliminates the equality in t1 = t2 by replacing all t1 in φ[t1/x]
with t2. This is a sound substitution principle, since the assumption that
t1 equals t2 guarantees that the logical meanings of φ[t1/x] and φ[t2/x]
match.
The principle of substitution, in the guise of the rule =e, is quite powerful.
Together with the rule =i, it allows us to show the sequents
t1 = t2 ⊢t2 = t1
(2.6)
t1 = t2, t2 = t3 ⊢t1 = t3.
(2.7)

2.3 Proof theory of predicate logic
109
A proof for (2.6) is:
1
t1 = t2
premise
2
t1 = t1
=i
3
t2 = t1
=e 1, 2
where φ is x = t1. A proof for (2.7) is:
1
t2 = t3
premise
2
t1 = t2
premise
3
t1 = t3
=e 1, 2
where φ is t1 = x, so in line 2 we have φ[t2/x] and in line 3 we obtain φ[t3/x],
as given by the rule =e applied to lines 1 and 2. Notice how we applied the
scheme =e with several diﬀerent instantiations.
Our discussion of the rules =i and =e has shown that they force equality
to be reﬂexive (2.5), symmetric (2.6) and transitive (2.7). These are minimal
and necessary requirements for any sane concept of (extensional) equality.
We leave the topic of equality for now to move on to the proof rules for
quantiﬁers.
The proof rules for universal quantification
The rule for eliminat-
ing ∀is the following:
∀x φ
φ[t/x]
∀x e.
It says: If ∀x φ is true, then you could replace the x in φ by any term t
(given, as usual, the side condition that t be free for x in φ) and conclude
that φ[t/x] is true as well. The intuitive soundness of this rule is self-evident.
Recall that φ[t/x] is obtained by replacing all free occurrences of x in φ
by t. You may think of the term t as a more concrete instance of x. Since φ
is assumed to be true for all x, that should also be the case for any term t.
Example 2.11 To see the necessity of the proviso that t be free for x in
φ, consider the case that φ is ∃y (x < y) and the term to be substituted
for x is y. Let’s suppose we are reasoning about numbers with the usual
‘smaller than’ relation. The statement ∀x φ then says that for all numbers
n there is some bigger number m, which is indeed true of integers or real
numbers. However, φ[y/x] is the formula ∃y (y < y) saying that there is a
number which is bigger than itself. This is wrong; and we must not allow a
proof rule which derives semantically wrong things from semantically valid

110
2 Predicate logic
ones. Clearly, what went wrong was that y became bound in the process of
substitution; y is not free for x in φ. Thus, in going from ∀x φ to φ[t/x],
we have to enforce the side condition that t be free for x in φ: use a fresh
variable for y to change φ to, say, ∃z (x < z) and then apply [y/x] to that
formula, rendering ∃z (y < z).
The rule ∀x i is a bit more complicated. It employs a proof box similar
to those we have already seen in natural deduction for propositional logic,
but this time the box is to stipulate the scope of the ‘dummy variable’ x0
rather than the scope of an assumption. The rule ∀x i is written
x0
...
φ[x0/x]
∀x φ
∀x i.
It says: If, starting with a ‘fresh’ variable x0, you are able to prove some
formula φ[x0/x] with x0 in it, then (because x0 is fresh) you can derive
∀x φ. The important point is that x0 is a new variable which doesn’t occur
anywhere outside its box; we think of it as an arbitrary term. Since we
assumed nothing about this x0, anything would work in its place; hence the
conclusion ∀x φ.
It takes a while to understand this rule, since it seems to be going from
the particular case of φ to the general case ∀x φ. The side condition, that
x0 does not occur outside the box, is what allows us to get away with
this.
To understand this, think of the following analogy. If you want to prove
to someone that you can, say, split a tennis ball in your hand by squashing
it, you might say ‘OK, give me a tennis ball and I’ll split it.’ So we give you
one and you do it. But how can we be sure that you could split any tennis
ball in this way? Of course, we can’t give you all of them, so how could we
be sure that you could split any one? Well, we assume that the one you did
split was an arbitrary, or ‘random,’ one, i.e. that it wasn’t special in any
way – like a ball which you may have ‘prepared’ beforehand; and that is
enough to convince us that you could split any tennis ball. Our rule says
that if you can prove φ about an x0 that isn’t special in any way, then you
could prove it for any x whatsoever.
To put it another way, the step from φ to ∀x φ is legitimate only if we have
arrived at φ in such a way that none of its assumptions contain x as a free
variable. Any assumption which has a free occurrence of x puts constraints

2.3 Proof theory of predicate logic
111
on such an x. For example, the assumption bird(x) conﬁnes x to the realm
of birds and anything we can prove about x using this formula will have
to be a statement restricted to birds and not about anything else we might
have had in mind.
It is time we looked at an example of these proof rules at work. Here is a
proof of the sequent ∀x (P(x) →Q(x)), ∀x P(x) ⊢∀x Q(x):
1
∀x (P(x) →Q(x))
premise
2
∀x P(x)
premise
x0
3
P(x0) →Q(x0)
∀x e 1
4
P(x0)
∀x e 2
5
Q(x0)
→e 3, 4
6
∀x Q(x)
∀x i 3−5
The structure of this proof is guided by the fact that the conclusion is
a ∀formula. To arrive at this, we will need an application of ∀x i, so we
set up the box controlling the scope of x0. The rest is now mechanical:
we prove ∀x Q(x) by proving Q(x0); but the latter we can prove as soon as
we can prove P(x0) and P(x0) →Q(x0), which themselves are instances of
the premises (obtained by ∀e with the term x0). Note that we wrote the
name of the dummy variable to the left of the ﬁrst proof line in its scope
box.
Here is a simpler example which uses only ∀x e: we show the validity of
the sequent P(t), ∀x (P(x) →¬Q(x)) ⊢¬Q(t) for any term t:
1
P(t)
premise
2
∀x (P(x) →¬Q(x))
premise
3
P(t) →¬Q(t)
∀x e 2
4
¬Q(t)
→e 3, 1
Note that we invoked ∀x e with the same instance t as in the assumption
P(t). If we had invoked ∀x e with y, say, and obtained P(y) →¬Q(y), then
that would have been valid, but it would not have been helpful in the case
that y was diﬀerent from t. Thus, ∀x e is really a scheme of rules, one for
each term t (free for x in φ), and we should make our choice on the basis of
consistent pattern matching. Further, note that we have rules ∀x i and ∀x e
for each variable x. In particular, there are rules ∀y i, ∀y e and so on. We

112
2 Predicate logic
will write ∀i and ∀e when we speak about such rules without concern for the
actual quantiﬁer variable.
Notice also that, although the square brackets representing substitution
appear in the rules ∀i and ∀e, they do not appear when we use those rules.
The reason for this is that we actually carry out the substitution that is asked
for. In the rules, the expression φ[t/x] means: ‘φ, but with free occurrences
of x replaced by t.’ Thus, if φ is P(x, y) →Q(y, z) and the rule refers to
φ[a/y], we carry out the substitution and write P(x, a) →Q(a, z) in the
proof.
A helpful way of understanding the universal quantiﬁer rules is to com-
pare the rules for ∀with those for ∧. The rules for ∀are in some sense
generalisations of those for ∧; whereas ∧has just two conjuncts, ∀acts like
it conjoins lots of formulas (one for each substitution instance of its vari-
able). Thus, whereas ∧i has two premises, ∀x i has a premise φ[x0/x] for
each possible ‘value’ of x0. Similarly, where and-elimination allows you to
deduce from φ ∧ψ whichever of φ and ψ you like, forall-elimination allows
you to deduce φ[t/x] from ∀x φ, for whichever t you (and the side condition)
like. To say the same thing another way: think of ∀x i as saying: to prove
∀x φ, you have to prove φ[x0/x] for every possible value x0; while ∧i says
that to prove φ1 ∧φ2 you have to prove φi for every i = 1, 2.
The proof rules for existential quantification
The analogy between
∀and ∧extends also to ∃and ∨; and you could even try to guess the rules
for ∃by starting from the rules for ∨and applying the same ideas as those
that related ∧to ∀. For example, we saw that the rules for or-introduction
were a sort of dual of those for and-elimination; to emphasise this point, we
could write them as
φ1 ∧φ2
φk
∧ek
φk
φ1 ∨φ2
∨ik
where k can be chosen to be either 1 or 2. Therefore, given the form of
forall-elimination, we can infer that exists-introduction must be simply
φ[t/x]
∃xφ
∃x i.
Indeed, this is correct: it simply says that we can deduce ∃x φ whenever we
have φ[t/x] for some term t (naturally, we impose the side condition that t
be free for x in φ).
In the rule ∃i, we see that the formula φ[t/x] contains, from a compu-
tational point of view, more information than ∃x φ. The latter merely says

2.3 Proof theory of predicate logic
113
that φ holds for some, unspeciﬁed, value of x; whereas φ[t/x] has a witness
t at its disposal. Recall that the square-bracket notation asks us actually to
carry out the substitution. However, the notation φ[t/x] is somewhat mis-
leading since it suggests not only the right witness t but also the formula
φ itself. For example, consider the situation in which t equals y such that
φ[y/x] is y = y. Then you can check for yourself that φ could be a number
of things, like x = x or x = y. Thus, ∃x φ will depend on which of these φ
you were thinking of.
Extending the analogy between ∃and ∨, the rule ∨e leads us to the
following formulation of ∃e:
∃x φ
x0 φ[x0/x]
...
χ
χ
∃e.
Like ∨e, it involves a case analysis. The reasoning goes: We know ∃x φ is
true, so φ is true for at least one ‘value’ of x. So we do a case analysis over
all those possible values, writing x0 as a generic value representing them
all. If assuming φ[x0/x] allows us to prove some χ which doesn’t mention
x0, then this χ must be true whichever x0 makes φ[x0/x] true. And that’s
precisely what the rule ∃e allows us to deduce. Of course, we impose the
side condition that x0 can’t occur outside its box (therefore, in particular,
it cannot occur in χ). The box is controlling two things: the scope of x0 and
also the scope of the assumption φ[x0/x].
Just as ∨e says that to use φ1 ∨φ2, you have to be prepared for either of
the φi, so ∃e says that to use ∃x φ you have to be prepared for any possible
φ[x0/x]. Another way of thinking about ∃e goes like this: If you know ∃x φ
and you can derive some χ from φ[x0/x], i.e. by giving a name to the thing
you know exists, then you can derive χ even without giving that thing a
name (provided that χ does not refer to the name x0).
The rule ∃x e is also similar to ∨e in the sense that both of them are
elimination rules which don’t have to conclude a subformula of the formula
they are about to eliminate. Please verify that all other elimination rules
introduced so far have this subformula property.2 This property is computa-
tionally very pleasant, for it allows us to narrow down the search space for
a proof dramatically. Unfortunately, ∃x e, like its cousin ∨e, is not of that
computationally benign kind.
2 For ∀x e we perform a substitution [t/x], but it preserves the logical structure of φ.

114
2 Predicate logic
Let us practice these rules on a couple of examples. Certainly, we should
be able to prove the validity of the sequent ∀x φ ⊢∃x φ. The proof
1
∀x φ
premise
2
φ[x/x]
∀x e 1
3
∃x φ
∃x i 2
demonstrates that, where we chose t to be x with respect to both ∀x e and
to ∃x i (and note that x is free for x in φ and that φ[x/x] is simply φ again).
Proving the validity of the sequent ∀x (P(x) →Q(x)),
∃x P(x)
⊢
∃x Q(x) is more complicated:
1
∀x (P(x) →Q(x))
premise
2
∃x P(x)
premise
x0
3
P(x0)
assumption
4
P(x0) →Q(x0)
∀x e 1
5
Q(x0)
→e 4, 3
6
∃x Q(x)
∃x i 5
7
∃x Q(x)
∃x e 2, 3−6
The motivation for introducing the box in line 3 of this proof is the existential
quantiﬁer in the premise ∃x P(x) which has to be eliminated. Notice that
the ∃in the conclusion has to be introduced within the box and observe the
nesting of these two steps. The formula ∃x Q(x) in line 6 is the instantiation
of χ in the rule ∃e and does not contain an occurrence of x0, so it is allowed
to leave the box to line 7. The almost identical ‘proof’
1
∀x (P(x) →Q(x))
premise
2
∃x P(x)
premise
x0
3
P(x0)
assumption
4
P(x0) →Q(x0)
∀x e 1
5
Q(x0)
→e 4, 3
6
Q(x0)
∃x e 2, 3−5
7
∃x Q(x)
∃x i 6
is illegal! Line 6 allows the fresh parameter x0 to escape the scope of the
box which declares it. This is not permissible and we will see on page 116 an
example where such illicit use of proof rules results in unsound arguments.

2.3 Proof theory of predicate logic
115
A sequent with a slightly more complex proof is
∀x (Q(x) →R(x)), ∃x (P(x) ∧Q(x)) ⊢∃x (P(x) ∧R(x))
and could model some argument such as
If all quakers are reformists and if there is a protestant who is also
a quaker, then there must be a protestant who is also a reformist.
One possible proof strategy is to assume P(x0) ∧Q(x0), get the instance
Q(x0) →R(x0) from ∀x (Q(x) →R(x)) and use ∧e2 to get our hands on
Q(x0), which gives us R(x0) via →e . . . :
1
∀x (Q(x) →R(x))
premise
2
∃x (P(x) ∧Q(x))
premise
x0
3
P(x0) ∧Q(x0)
assumption
4
Q(x0) →R(x0)
∀x e 1
5
Q(x0)
∧e2 3
6
R(x0)
→e 4, 5
7
P(x0)
∧e1 3
8
P(x0) ∧R(x0)
∧i 7, 6
9
∃x (P(x) ∧R(x))
∃x i 8
10
∃x (P(x) ∧R(x))
∃x e 2, 3−9
Note the strategy of this proof: We list the two premises. The second premise
is of use here only if we apply ∃x e to it. This sets up the proof box in
lines 3−9 as well as the fresh parameter name x0. Since we want to prove
∃x (P(x) ∧R(x)), this formula has to be the last one in the box (our goal)
and the rest involves ∀x e and ∃x i.
The rules ∀i and ∃e both have the side condition that the dummy variable
cannot occur outside the box in the rule. Of course, these rules may still be
nested, by choosing another fresh name (e.g. y0) for the dummy variable. For
example, consider the sequent ∃x P(x), ∀x ∀y (P(x) →Q(y)) ⊢∀y Q(y).
(Look how strong the second premise is, by the way: given any x, y, if P(x),
then Q(y). This means that, if there is any object with the property P, then
all objects shall have the property Q.) Its proof goes as follows: We take an
arbitrary y0 and prove Q(y0); this we do by observing that, since some x

116
2 Predicate logic
satisﬁes P, so by the second premise any y satisﬁes Q:
1
∃x P(x)
premise
2
∀x∀y (P(x) →Q(y))
premise
y0
3
x0
4
P(x0)
assumption
5
∀y (P(x0) →Q(y))
∀x e 2
6
P(x0) →Q(y0)
∀y e 5
7
Q(y0)
→e 6, 4
8
Q(y0)
∃x e 1, 4−7
9
∀y Q(y)
∀y i 3−8
There is no special reason for picking x0 as a name for the dummy variable
we use for ∀x and ∃x and y0 as a name for ∀y and ∃y. We do this only
because it makes it easier for us humans. Again, study the strategy of this
proof. We ultimately have to show a ∀y formula which requires us to use
∀y i, i.e. we need to open up a proof box (lines 3−8) whose subgoal is to
prove a generic instance Q(y0). Within that box we want to make use of the
premise ∃x P(x) which results in the proof box set-up of lines 4−7. Notice
that, in line 8, we may well move Q(y0) out of the box controlled by x0.
We have repeatedly emphasised the point that the dummy variables in
the rules ∃e and ∀i must not occur outside their boxes. Here is an example
which shows how things would go wrong if we didn’t have this side condi-
tion. Consider the invalid sequent ∃x P(x), ∀x (P(x) →Q(x)) ⊢∀y Q(y).
(Compare it with the previous sequent; the second premise is now much
weaker, allowing us to conclude Q only for those objects for which we know
P.) Here is an alleged ‘proof’ of its validity:
1
∃x P(x)
premise
2
∀x (P(x) →Q(x))
premise
x0
3
x0
4
P(x0)
assumption
5
P(x0) →Q(x0)
∀x e 2
6
Q(x0)
→e 5, 4
7
Q(x0)
∃x e 1, 4−6
8
∀y Q(y)
∀y i 3−7

2.3 Proof theory of predicate logic
117
The last step introducing ∀y is not the bad one; that step is ﬁne. The bad
one is the second from last one, concluding Q(x0) by ∃x e and violating the
side condition that x0 may not leave the scope of its box. You can try a few
other ways of ‘proving’ this sequent, but none of them should work (assuming
that our proof system is sound with respect to semantic entailment, which
we deﬁne in the next section). Without this side condition, we would also
be able to prove that ‘all x satisfy the property P as soon as one of them
does so,’ a semantic disaster of biblical proportions!
2.3.2 Quantifier equivalences
We have already hinted at semantic equivalences between certain forms of
quantiﬁcation. Now we want to provide formal proofs for some of the most
commonly used quantiﬁer equivalences. Quite a few of them involve several
quantiﬁcations over more than just one variable. Thus, this topic is also
good practice for using the proof rules for quantiﬁers in a nested fashion.
For example, the formula ∀x ∀y φ should be equivalent to ∀y ∀x φ since
both say that φ should hold for all values of x and y. What about (∀x φ) ∧
(∀x ψ) versus ∀x (φ ∧ψ)? A moment’s thought reveals that they should have
the same meaning as well. But what if the second conjunct does not start
with ∀x? So what if we are looking at (∀x φ) ∧ψ in general and want to
compare it with ∀x (φ ∧ψ)? Here we need to be careful, since x might be
free in ψ and would then become bound in the formula ∀x (φ ∧ψ).
Example 2.12 We may specify ‘Not all birds can ﬂy.’ as ¬∀x (B(x) →
F(x)) or as ∃x (B(x) ∧¬F(x)). The former formal speciﬁcation is closer
to the structure of the English speciﬁcation, but the latter is logically equiv-
alent to the former. Quantiﬁer equivalences help us in establishing that
speciﬁcations that ‘look’ diﬀerent are really saying the same thing.
Here are some quantiﬁer equivalences which you should become familiar
with. As in Chapter 1, we write φ1 ⊣⊢φ2 as an abbreviation for the validity
of φ1 ⊢φ2 and φ2 ⊢φ1.
Theorem 2.13 Let φ and ψ be formulas of predicate logic. Then we have
the following equivalences:
1.
(a) ¬∀x φ ⊣⊢∃x ¬φ
(b) ¬∃x φ ⊣⊢∀x ¬φ.
2.
Assuming that x is not free in ψ:

118
2 Predicate logic
(a) ∀x φ ∧ψ ⊣⊢∀x (φ ∧ψ)3
(b) ∀x φ ∨ψ ⊣⊢∀x (φ ∨ψ)
(c) ∃x φ ∧ψ ⊣⊢∃x (φ ∧ψ)
(d) ∃x φ ∨ψ ⊣⊢∃x (φ ∨ψ)
(e) ∀x (ψ →φ) ⊣⊢ψ →∀x φ
(f) ∃x (φ →ψ) ⊣⊢∀x φ →ψ
(g) ∀x (φ →ψ) ⊣⊢∃x φ →ψ
(h) ∃x (ψ →φ) ⊣⊢ψ →∃x φ.
3.
(a) ∀x φ ∧∀x ψ ⊣⊢∀x (φ ∧ψ)
(b) ∃x φ ∨∃x ψ ⊣⊢∃x (φ ∨ψ).
4.
(a) ∀x ∀y φ ⊣⊢∀y ∀x φ
(b) ∃x ∃y φ ⊣⊢∃y ∃x φ.
PROOF: We will prove most of these sequents; the proofs for the remaining
ones are straightforward adaptations and are left as exercises. Recall that
we sometimes write ⊥to denote any contradiction.
1.
(a) We will lead up to this by proving the validity of two simpler sequents
ﬁrst: ¬(p1 ∧p2) ⊢¬p1 ∨¬p2 and then ¬∀x P(x) ⊢∃x ¬P(x). The reason for
proving the ﬁrst of these is to illustrate the close relationship between ∧and
∨on the one hand and ∀and ∃on the other – think of a model with just two
elements 1 and 2 such that pi (i = 1, 2) stands for P(x) evaluated at i. The
idea is that proving this propositional sequent should give us inspiration for
proving the second one of predicate logic. The reason for proving the latter
sequent is that it is a special case (in which φ equals P(x)) of the one we are
really after, so again it should be simpler while providing some inspiration.
So, let’s go.
1
¬(p1 ∧p2)
premise
2
¬(¬p1 ∨¬p2)
assumption
3
¬p1
assumption
4
¬p1 ∨¬p2
∨i1 3
5
⊥
¬e 4, 2
6
p1
PBC 3−5
¬p2
assumption
¬p1 ∨¬p2
∨i2 3
⊥
¬e 4, 2
p2
PBC 3−5
7
p1 ∧p2
∧i 6, 6
8
⊥
¬e 7, 1
9
¬p1 ∨¬p2
PBC 2−8
3 Remember that ∀x φ ∧ψ is implicitly bracketed as (∀x φ) ∧ψ, by virtue of the binding priorities.

2.3 Proof theory of predicate logic
119
You have seen this sort of proof before, in Chapter 1. It is an example of
something which requires proof by contradiction, or ¬¬e, or LEM (meaning
that it simply cannot be proved in the reduced natural deduction system
which discards these three rules) – in fact, the proof above used the rule
PBC three times.
Now we prove the validity of ¬∀x P(x) ⊢∃x ¬P(x) similarly, except that
where the rules for ∧and ∨were used we now use those for ∀and ∃:
1
¬∀x P(x)
premise
2
¬∃x ¬P(x)
assumption
x0
3
4
¬P(x0)
assumption
5
∃x¬P(x)
∃x i 4
6
⊥
¬e 5, 2
7
P(x0)
PBC 4−6
8
∀x P(x)
∀x i 3−7
9
⊥
¬e 8, 1
10
∃x ¬P(x)
PBC 2−9
You will really beneﬁt by spending time understanding the way this proof
mimics the one above it. This insight is very useful for constructing predicate
logic proofs: you ﬁrst construct a similar propositional proof and then mimic
it.
Next we prove that ¬∀x φ ⊢∃x ¬φ is valid:
1
¬∀x φ
premise
2
¬∃x ¬φ
assumption
x0
3
4
¬φ[x0/x]
assumption
5
∃x¬φ
∃x i 4
6
⊥
¬e 5, 2
7
φ[x0/x]
PBC 4−6
8
∀x φ
∀x i 3−7
9
⊥
¬e 8, 1
10
∃x ¬φ
PBC 2−9

120
2 Predicate logic
Proving that the reverse ∃x ¬φ ⊢¬∀x φ is valid is more straightforward,
for it does not involve proof by contradiction, ¬¬e, or LEM. Unlike its
converse, it has a constructive proof which the intuitionists do accept. We
could again prove the corresponding propositional sequent, but we leave
that as an exercise.
1
∃x ¬φ
assumption
2
∀x φ
assumption
x0
3
4
¬φ[x0/x]
assumption
5
φ[x0/x]
∀x e 2
6
⊥
¬e 5, 4
7
⊥
∃x e 1, 3−6
8
¬∀x φ
¬i 2−7
2.
(a) Validity of ∀x φ ∧ψ ⊢∀x (φ ∧ψ) can be proved thus:
1
(∀x φ) ∧ψ
premise
2
∀x φ
∧e1 1
3
ψ
∧e2 1
x0
4
5
φ[x0/x]
∀x e 2
6
φ[x0/x] ∧ψ
∧i 5, 3
7
(φ ∧ψ)[x0/x]
identical to 6, since x not free in ψ
8
∀x (φ ∧ψ)
∀x i 4−7
The argument for the reverse validity can go like this:
1
∀x (φ ∧ψ)
premise
x0
2
3
(φ ∧ψ)[x0/x]
∀x e 1
4
φ[x0/x] ∧ψ
identical to 3, since x not free in ψ
5
ψ
∧e2 3
6
φ[x0/x]
∧e1 3
7
∀x φ
∀x i 2−6
8
(∀x φ) ∧ψ
∧i 7, 5

2.3 Proof theory of predicate logic
121
Notice that the use of ∧i in the last line is permissible, because ψ was obtained
for any instantiation of the formula in line 1; although a formal tool for proof
support may complain about such practice.
3.
(b) The sequent (∃x φ) ∨(∃x ψ) ⊢∃x (φ ∨ψ) is proved valid using the rule
∨e; so we have two principal cases, each of which requires the rule
∃x i:
1
(∃x φ) ∨(∃x ψ)
premise
2
∃x φ
x0
3
φ[x0/x]
4
φ[x0/x]∨ψ[x0/x]
5
(φ ∨ψ)[x0/x]
6
∃x (φ ∨ψ)
7
∃x (φ ∨ψ)
∃x ψ
assumpt.
x0
ψ[x0/x]
assumpt.
φ[x0/x]∨ψ[x0/x]
∨i 3
(φ ∨ψ)[x0/x]
identical
∃x (φ ∨ψ)
∃x i 5
∃x (φ ∨ψ)
∃x e 2, 3−6
8
∃x (φ ∨ψ)
∨e 1, 2−7
The converse sequent has ∃x (φ ∨ψ) as premise, so its proof has to use ∃x e
as its last rule; for that rule, we need φ ∨ψ as a temporary assumption and
need to conclude (∃x φ) ∨(∃x ψ) from those data; of course, the assumption
φ ∨ψ requires the usual case analysis:
1
∃x (φ ∨ψ)
premise
x0
2
(φ ∨ψ)[x0/x]
assumption
3
φ[x0/x] ∨ψ[x0/x]
identical
4
φ[x0/x]
5
∃x φ
6
∃x φ ∨∃x ψ
ψ[x0/x]
assumption
∃x ψ
∃x i 4
∃x φ ∨∃x ψ
∨i 5
7
∃x φ ∨∃x ψ
∨e 3, 4−6
8
∃x φ ∨∃x ψ
∃x e 1, 2−7
4.
(b) Given the premise ∃x ∃y φ, we have to nest ∃x e and ∃y e to conclude ∃y ∃x φ.
Of course, we have to obey the format of these elimination rules as done
below:

122
2 Predicate logic
1
∃x ∃y φ
premise
x0
2
(∃y φ)[x0/x]
assumption
3
∃y (φ[x0/x])
identical, since x, y diﬀerent variables
y0
4
φ[x0/x][y0/y]
assumption
5
φ[y0/y][x0/x]
identical, since x, y, x0, y0 diﬀerent variables
6
∃x φ[y0/y]
∀x i 5
7
∃y ∃x φ
∀y i 6
8
∃y ∃x φ
∃y e3, 4−7
9
∃y ∃x φ
∃x e1, 2−8
The validity of the converse sequent is proved in the same way by swapping
the roles of x and y.
2
2.4 Semantics of predicate logic
Having seen how natural deduction of propositional logic can be extended
to predicate logic, let’s now look at how the semantics of predicate logic
works. Just like in the propositional case, the semantics should provide a
separate, but ultimately equivalent, characterisation of the logic. By ‘sepa-
rate,’ we mean that the meaning of the connectives is deﬁned in a diﬀerent
way; in proof theory, they were deﬁned by proof rules providing an oper-
ative explanation. In semantics, we expect something like truth tables. By
‘equivalent,’ we mean that we should be able to prove soundness and com-
pleteness, as we did for propositional logic – although a fully ﬂedged proof
of soundness and completeness for predicate logic is beyond the scope of this
book.
Before we begin describing the semantics of predicate logic, let us look
more closely at the real diﬀerence between a semantic and a proof-theoretic
account. In proof theory, the basic object which is constructed is a proof.
Let us write Γ as a shorthand for lists of formulas φ1, φ2, . . . , φn. Thus, to
show that Γ ⊢ψ is valid, we need to provide a proof of ψ from Γ. Yet,
how can we show that ψ is not a consequence of Γ? Intuitively, this is
harder; how can you possibly show that there is no proof of something?
You would have to consider every ‘candidate’ proof and show it is not one.
Thus, proof theory gives a ‘positive’ characterisation of the logic; it pro-
vides convincing evidence for assertions like ‘Γ ⊢ψ is valid,’ but it is not
very useful for establishing evidence for assertions of the form ‘Γ ⊢φ is not
valid.’

2.4 Semantics of predicate logic
123
Semantics, on the other hand, works in the opposite way. To show that ψ
is not a consequence of Γ is the ‘easy’ bit: ﬁnd a model in which all φi are
true, but ψ isn’t. Showing that ψ is a consequence of Γ, on the other hand,
is harder in principle. For propositional logic, you need to show that every
valuation (an assignment of truth values to all atoms involved) that makes
all φi true also makes ψ true. If there is a small number of valuations, this
is not so bad. However, when we look at predicate logic, we will ﬁnd that
there are inﬁnitely many valuations, called models from hereon, to consider.
Thus, in semantics we have a ‘negative’ characterisation of the logic. We ﬁnd
establishing assertions of the form ‘Γ ̸⊨ψ’ (ψ is not a semantic entailment of
all formulas in Γ) easier than establishing ‘Γ ⊨ψ’ (ψ is a semantic entailment
of Γ), for in the former case we need only talk about one model, whereas in
the latter we potentially have to talk about inﬁnitely many.
All this goes to show that it is important to study both proof theory and
semantics. For example, if you are trying to show that ψ is not a consequence
of Γ and you have a hard time doing that, you might want to change your
strategy for a while by trying to prove the validity of Γ ⊢ψ. If you ﬁnd a
proof, you know for sure that ψ is a consequence of Γ. If you can’t ﬁnd a
proof, then your attempts at proving it often provide insights which lead
you to the construction of a counter example. The fact that proof theory
and semantics for predicate logic are equivalent is amazing, but it does not
stop them having separate roles in logic, each meriting close study.
2.4.1 Models
Recall how we evaluated formulas in propositional logic. For example, the
formula (p ∨¬q) →(q →p) is evaluated by computing a truth value (T or
F) for it, based on a given valuation (assumed truth values for p and q).
This activity is essentially the construction of one line in the truth table of
(p ∨¬q) →(q →p). How can we evaluate formulas in predicate logic, e.g.
∀x ∃y ((P(x) ∨¬Q(y)) →(Q(x) →P(y)))
which ‘enriches’ the formula of propositional logic above? Could we simply
assume truth values for P(x), Q(y), Q(x) and P(y) and compute a truth
value as before? Not quite, since we have to reﬂect the meaning of the
quantiﬁers ∀x and ∃y, their dependences and the actual parameters of P
and Q – a formula ∀x ∃y R(x, y) generally means something else other than
∃y ∀x R(x, y); why? The problem is that variables are place holders for any,
or some, unspeciﬁed concrete values. Such values can be of almost any kind:
students, birds, numbers, data structures, programs and so on.

124
2 Predicate logic
Thus, if we encounter a formula ∃y ψ, we try to ﬁnd some instance of
y (some concrete value) such that ψ holds for that particular instance of
y. If this succeeds (i.e. there is such a value of y for which ψ holds), then
∃y ψ evaluates to T; otherwise (i.e. there is no concrete value of y which
realises ψ) it returns F. Dually, evaluating ∀x ψ amounts to showing that
ψ evaluates to T for all possible values of x; if this is successful, we know
that ∀x ψ evaluates to T; otherwise (i.e. there is some value of x such that
ψ computes F) it returns F. Of course, such evaluations of formulas require
a ﬁxed universe of concrete values, the things we are, so to speak, talking
about. Thus, the truth value of a formula in predicate logic depends on, and
varies with, the actual choice of values and the meaning of the predicate and
function symbols involved.
If variables can take on only ﬁnitely many values, we can write a program
that evaluates formulas in a compositional way. If the root node of φ is ∧,
∨, →or ¬, we can compute the truth value of φ by using the truth table of
the respective logical connective and by computing the truth values of the
subtree(s) of that root, as discussed in Chapter 1. If the root is a quantiﬁer,
we have sketched above how to proceed. This leaves us with the case of the
root node being a predicate symbol P (in propositional logic this was an
atom and we were done already). Such a predicate requires n arguments
which have to be terms t1, t2, . . . , tn. Therefore, we need to be able to assign
truth values to formulas of the form P(t1, t2, . . . , tn).
For formulas P(t1, t2, . . . , tn), there is more going on than in the case of
propositional logic. For n = 2, the predicate P could stand for something
like ‘the number computed by t1 is less than, or equal to, the number com-
puted by t2.’ Therefore, we cannot just assign truth values to P directly
without knowing the meaning of terms. We require a model of all function
and predicate symbols involved. For example, terms could denote real num-
bers and P could denote the relation ‘less than or equal to’ on the set of real
numbers.
Deﬁnition 2.14 Let F be a set of function symbols and P a set of predicate
symbols, each symbol with a ﬁxed number of required arguments. A model
M of the pair (F, P) consists of the following set of data:
1.
A non-empty set A, the universe of concrete values;
2.
for each nullary function symbol f ∈F, a concrete element f M of A
3.
for each f ∈F with arity n > 0, a concrete function f M : An →A from An, the
set of n-tuples over A, to A; and
4.
for each P ∈P with arity n > 0, a subset P M ⊆An of n-tuples over A.

2.4 Semantics of predicate logic
125
The distinction between f and fM and between P and P M is most im-
portant. The symbols f and P are just that: symbols, whereas fM and
P M denote a concrete function (or element) and relation in a model M,
respectively.
Example 2.15 Let F
def
= {i} and P
def
= {R, F}; where i is a constant, F a
predicate symbol with one argument and R a predicate symbol with two
arguments. A model M contains a set of concrete elements A – which may be
a set of states of a computer program. The interpretations iM, RM, and F M
may then be a designated initial state, a state transition relation, and a set
of ﬁnal (accepting) states, respectively. For example, let A
def
= {a, b, c}, iM def
=
a, RM def
= {(a, a), (a, b), (a, c), (b, c), (c, c)}, and F M def
= {b, c}. We informally
check some formulas of predicate logic for this model:
1.
The formula
∃y R(i, y)
says that there is a transition from the initial state to some state; this is true
in our model, as there are transitions from the initial state a to a, b, and c.
2.
The formula
¬F(i)
states that the initial state is not a ﬁnal, accepting state. This is true in our
model as b and c are the only ﬁnal states and a is the intitial one.
3.
The formula
∀x∀y∀z (R(x, y) ∧R(x, z) →y = z)
makes use of the equality predicate and states that the transition relation is
deterministic: all transitions from any state can go to at most one state (there
may be no transitions from a state as well). This is false in our model since
state a has transitions to b and c.
4.
The formula
∀x∃y R(x, y)
states that the model is free of states that deadlock: all states have a transition
to some state. This is true in our model: a can move to a, b or c; and b and c
can move to c.
Example 2.16 Let F
def
= {e, ·} and P
def
= {≤}, where e is a constant, · is a
function of two arguments and ≤is a predicate in need of two arguments as
well. Again, we write · and ≤in inﬁx notation as in (t1 · t2) ≤(t · t).

126
2 Predicate logic
The model M we have in mind has as set A all binary strings, ﬁnite
words over the alphabet {0, 1}, including the empty string denoted by ϵ. The
interpretation eM of e is just the empty word ϵ. The interpretation ·M of · is
the concatenation of words. For example, 0110 ·M 1110 equals 01101110. In
general, if a1a2 . . . ak and b1b2 . . . bn are such words with ai, bj ∈{0, 1}, then
a1a2 . . . ak ·M b1b2 . . . bn equals a1a2 . . . akb1b2 . . . bn. Finally, we interpret ≤
as the preﬁx ordering of words. We say that s1 is a preﬁx of s2 if there is
a binary word s3 such that s1 ·M s3 equals s2. For example, 011 is a preﬁx
of 011001 and of 011, but 010 is neither. Thus, ≤M is the set {(s1, s2) |
s1 is a preﬁx of s2}. Here are again some informal model checks:
1.
In our model, the formula
∀x ((x ≤x · e) ∧(x · e ≤x))
says that every word is a preﬁx of itself concatenated with the empty word and
conversely. Clearly, this holds in our model, for s ·M ϵ is just s and every word
is a preﬁx of itself.
2.
In our model, the formula
∃y ∀x (y ≤x)
says that there exists a word s that is a preﬁx of every other word. This is true,
for we may chose ϵ as such a word (there is no other choice in this case).
3.
In our model, the formula
∀x ∃y (y ≤x)
says that every word has a preﬁx. This is clearly the case and there are in
general multiple choices for y, which are dependent on x.
4.
In our model, the formula ∀x ∀y ∀z ((x ≤y) →(x · z ≤y · z)) says that when-
ever a word s1 is a preﬁx of s2, then s1s has to be a preﬁx of s2s for every word
s. This is clearly not the case. For example, take s1 as 01, s2 as 011 and s to
be 0.
5.
In our model, the formula
¬∃x ∀y ((x ≤y) →(y ≤x))
says that there is no word s such that whenever s is a preﬁx of some other word
s1, it is the case that s1 is a preﬁx of s as well. This is true since there cannot
be such an s. Assume, for the sake of argument, that there were such a word s.
Then s is clearly a preﬁx of s0, but s0 cannot be a preﬁx of s since s0 contains
one more bit than s.
It is crucial to realise that the notion of a model is extremely liberal and
open-ended. All it takes is to choose a non-empty set A, whose elements

2.4 Semantics of predicate logic
127
model real-world objects, and a set of concrete functions and relations, one
for each function, respectively predicate, symbol. The only mild requirement
imposed on all of this is that the concrete functions and relations on A have
the same number of arguments as their syntactic counterparts.
However, you, as a designer or implementor of such a model, have the
responsibility of choosing your model wisely. Your model should be a suf-
ﬁciently accurate picture of whatever it is you want to model, but at the
same time it should abstract away (= ignore) aspects of the world which are
irrelevant from the perspective of your task at hand.
For example, if you build a database of family relationships, then it would
be foolish to interpret father-of(x, y) by something like ‘x is the daughter
of y.’ By the same token, you probably would not want to have a predicate
for ‘is taller than,’ since your focus in this model is merely on relationships
deﬁned by birth. Of course, there are circumstances in which you may want
to add additional features to your database.
Given a model M for a pair (F, P) of function and predicate symbols,
we are now almost in a position to formally compute a truth value for all
formulas in predicate logic which involve only function and predicate sym-
bols from (F, P). There is still one thing, though, that we need to discuss.
Given a formula ∀x φ or ∃x φ, we intend to check whether φ holds for all,
respectively some, value a in our model. While this is intuitive, we have no
way of expressing this in our syntax: the formula φ usually has x as a free
variable; φ[a/x] is well-intended, but ill-formed since φ[a/x] is not a logical
formula, for a is not a term but an element of our model.
Therefore we are forced to interpret formulas relative to an environment.
You may think of environments in a variety of ways. Essentially, they are
look-up tables for all variables; such a table l associates with every variable
x a value l(x) of the model. So you can also say that environments are
functions l: var →A from the set of variables var to the universe of values
A of the underlying model. Given such a look-up table, we can assign truth
values to all formulas. However, for some of these computations we need
updated look-up tables.
Deﬁnition 2.17 A look-up table or environment for a universe A of con-
crete values is a function l: var →A from the set of variables var to A. For
such an l, we denote by l[x →a] the look-up table which maps x to a and
any other variable y to l(y).
Finally, we are able to give a semantics to formulas of predicate logic. For
propositional logic, we did this by computing a truth value. Clearly, it suﬃces
to know in which cases this value is T.

128
2 Predicate logic
Deﬁnition 2.18 Given a model M for a pair (F, P) and given an environ-
ment l, we deﬁne the satisfaction relation M ⊨l φ for each logical formula
φ over the pair (F, P) and look-up table l by structural induction on φ. If
M ⊨l φ holds, we say that φ computes to T in the model M with respect to
the environment l.
P:
If φ is of the form P(t1, t2, . . . , tn), then we interpret the terms t1, t2, . . . , tn in
our set A by replacing all variables with their values according to l. In this way
we compute concrete values a1, a2, . . . , an of A for each of these terms, where
we interpret any function symbol f ∈F by f M. Now M ⊨l P(t1, t2, . . . , tn)
holds iﬀ(a1, a2, . . . , an) is in the set P M.
∀x:
The relation M ⊨l ∀x ψ holds iﬀM ⊨l[x→a] ψ holds for all a ∈A.
∃x:
Dually, M ⊨l ∃x ψ holds iﬀM ⊨l[x→a] ψ holds for some a ∈A.
¬:
The relation M ⊨l ¬ψ holds iﬀit is not the case that M ⊨l ψ holds.
∨:
The relation M ⊨l ψ1 ∨ψ2 holds iﬀM ⊨l ψ1 or M ⊨l ψ2 holds.
∧:
The relation M ⊨l ψ1 ∧ψ2 holds iﬀM ⊨l ψ1 and M ⊨l ψ2 hold.
→:
The relation M ⊨l ψ1 →ψ2 holds iﬀM ⊨l ψ2 holds whenever M ⊨l ψ1 holds.
We sometimes write M ̸⊨l φ to denote that M ⊨l φ does not hold.
There is a straightforward inductive argument on the height of the parse
tree of a formula which says that M ⊨l φ holds iﬀM ⊨l′ φ holds, whenever
l and l′ are two environments which are identical on the set of free variables
of φ. In particular, if φ has no free variables at all, we then call φ a sentence;
we conclude that M ⊨l φ holds, or does not hold, regardless of the choice of
l. Thus, for sentences φ we often elide l and write M ⊨φ since the choice of
an environment l is then irrelevant.
Example 2.19 Let us illustrate the deﬁnitions above by means of an-
other simple example. Let F
def
= {alma} and P
def
= {loves} where alma is a
constant and loves a predicate with two arguments. The model M we
choose here consists of the privacy-respecting set A
def
= {a, b, c}, the constant
function almaM def
= a and the predicate lovesM def
= {(a, a), (b, a), (c, a)}, which
has two arguments as required. We want to check whether the model M
satisﬁes
None of Alma’s lovers’ lovers love her.
First, we need to express the, morally worrying, sentence in predicate logic.
Here is such an encoding (as we already discussed, diﬀerent but logically
equivalent encodings are possible):
∀x ∀y (loves(x, alma) ∧loves(y, x) →¬loves(y, alma)) .
(2.8)

2.4 Semantics of predicate logic
129
Does the model M satisfy this formula? Well, it does not; for we may choose
a for x and b for y. Since (a, a) is in the set lovesM and (b, a) is in the
set lovesM, we would need that the latter does not hold since it is the
interpretation of loves(y, alma); this cannot be.
And what changes if we modify M to M′, where we keep A and almaM,
but redeﬁne the interpretation of loves as lovesM′ def
= {(b, a), (c, b)}? Well,
now there is exactly one lover of Alma’s lovers, namely c; but c is not one
of Alma’s lovers. Thus, the formula in (2.8) holds in the model M′.
2.4.2 Semantic entailment
In propositional logic, the semantic entailment φ1, φ2, . . . , φn ⊨ψ holds iﬀ:
whenever all φ1, φ2, . . . , φn evaluate to T, the formula ψ evaluates to T as well.
How can we deﬁne such a notion for formulas in predicate logic, considering
that M ⊨l φ is indexed with an environment?
Deﬁnition 2.20 Let Γ be a (possibly inﬁnite) set of formulas in predicate
logic and ψ a formula of predicate logic.
1.
Semantic entailment Γ ⊨ψ holds iﬀfor all models M and look-up tables l,
whenever M ⊨l φ holds for all φ ∈Γ, then M ⊨l ψ holds as well.
2.
Formula ψ is satisﬁable iﬀthere is some model M and some environment l such
that M ⊨l ψ holds.
3.
Formula ψ is valid iﬀM ⊨l ψ holds for all models M and environments l in
which we can check ψ.
4.
The set Γ is consistent or satisﬁable iﬀthere is a model M and a look-up table
l such that M ⊨l φ holds for all φ ∈Γ.
In predicate logic, the symbol ⊨is overloaded: it denotes model checks ‘M ⊨
φ’ and semantic entailment ‘φ1, φ2, . . . , φn ⊨ψ.’ Computationally, each of
these notions means trouble. First, establishing M ⊨φ will cause problems,
if done on a machine, as soon as the universe of values A of M is inﬁnite.
In that case, checking the sentence ∀x ψ, where x is free in ψ, amounts to
verifying M ⊨[x→a] ψ for inﬁnitely many elements a.
Second, and much more seriously, in trying to verify that φ1, φ2, . . . , φn ⊨
ψ holds, we have to check things out for all possible models, all models which
are equipped with the right structure (i.e. they have functions and predicates
with the matching number of arguments). This task is impossible to perform
mechanically. This should be contrasted to the situation in propositional
logic, where the computation of the truth tables for the propositions involved
was the basis for computing this relationship successfully.

130
2 Predicate logic
However, we can sometimes reason that certain semantic entailments are
valid. We do this by providing an argument that does not depend on the
actual model at hand. Of course, this works only for a very limited number
of cases. The most prominent ones are the quantiﬁer equivalences which we
already encountered in the section on natural deduction. Let us look at a
couple of examples of semantic entailment.
Example 2.21 The justiﬁcation of the semantic entailment
∀x (P(x) →Q(x)) ⊨∀x P(x) →∀x Q(x)
is as follows. Let M be a model satisfying ∀x (P(x) →Q(x)). We need to
show that M satisﬁes ∀x P(x) →∀x Q(x) as well. On inspecting the deﬁni-
tion of M ⊨ψ1 →ψ2, we see that we are done if not every element of our
model satisﬁes P. Otherwise, every element does satisfy P. But since M
satisﬁes ∀x (P(x) →Q(x)), the latter fact forces every element of our model
to satisfy Q as well. By combining these two cases (i.e. either all elements of
M satisfy P, or not) we have shown that M satisﬁes ∀x P(x) →∀x Q(x).
What about the converse of the above? Is
∀x P(x) →∀x Q(x) ⊨∀x (P(x) →Q(x))
valid as well? Hardly! Suppose that M′ is a model satisfying ∀x P(x) →
∀x Q(x). If A′ is its underlying set and P M′ and QM′ are the corresponding
interpretations of P and Q, then M′ ⊨∀x P(x) →∀x Q(x) simply says that,
if P M′ equals A′, then QM′ must equal A′ as well. However, if P M′ does not
equal A′, then this implication is vacuously true (remember that F →· = T
no matter what · actually is). In this case we do not get any additional
constraints on our model M′. After these observations, it is now easy to
construct a counter-example model. Let A′ def
= {a, b}, P M′ def
= {a} and QM′ def
=
{b}. Then M′ ⊨∀x P(x) →∀x Q(x) holds, but M′ ⊨∀x (P(x) →Q(x)) does
not.
2.4.3 The semantics of equality
We have already pointed out the open-ended nature of the semantics of
predicate logic. Given a predicate logic over a set of function symbols F and
a set of predicate symbols P, we need only a non-empty set A equipped with
concrete functions or elements fM (for f ∈F) and concrete predicates P M
(for P ∈P) in A which have the right arities agreed upon in our speciﬁcation.
Of course, we also stressed that most models have natural interpretations of

2.5 Undecidability of predicate logic
131
functions and predicates, but central notions like that of semantic entailment
(φ1, φ2, . . . , φn ⊨ψ) really depend on all possible models, even the ones that
don’t seem to make any sense.
Apparently there is no way out of this peculiarity. For example, where
would you draw the line between a model that makes sense and one that
doesn’t? And would any such choice, or set of criteria, not be subjective? Such
constraints could also forbid a modiﬁcation of your model if this alteration
were caused by a slight adjustment of the problem domain you intended to
model. You see that there are a lot of good reasons for maintaining such a
liberal stance towards the notion of models in predicate logic.
However, there is one famous exception. Often one presents predicate logic
such that there is always a special predicate = available to denote equality
(recall Section 2.3.1); it has two arguments and t1 = t2 has the intended
meaning that the terms t1 and t2 compute the same thing. We discussed its
proof rule in natural deduction already in Section 2.3.1.
Semantically, one recognises the special role of equality by imposing on
an interpretation function =M to be actual equality on the set A of M.
Thus, (a, b) is in the set =M iﬀa and b are the same elements in the set A.
For example, given A
def
= {a, b, c}, the interpretation =M of equality is forced
to be {(a, a), (b, b), (c, c)}. Hence the semantics of equality is easy, for it is
always modelled extensionally.
2.5 Undecidability of predicate logic
We continue our introduction to predicate logic with some negative results.
Given a formula φ in propositional logic we can, at least in principle, de-
termine whether ⊨φ holds: if φ has n propositional atoms, then the truth
table of φ contains 2n lines; and ⊨φ holds if, and only if, the column for φ
(of length 2n) contains only T entries.
The bad news is that such a mechanical procedure, working for all for-
mulas φ, cannot be provided in predicate logic. We will give a formal proof
of this negative result, though we rely on an informal (yet intuitive) notion
of computability.
The problem of determining whether a predicate logic formula is valid is
known as a decision problem. A solution to a decision problem is a program
(written in Java, C, or any other common language) that takes problem
instances as input and always terminates, producing a correct ‘yes’ or ‘no’
output. In the case of the decision problem for predicate logic, the input to
the program is an arbitrary formula φ of predicate logic and the program

132
2 Predicate logic
is correct if it produces ‘yes’ whenever the input formula is valid and ‘no’
whenever it is not. Note that the program which solves a decision problem
must terminate for all well-formed input: a program which goes on thinking
about it for ever is not allowed. The decision problem at hand is this:
Validity in predicate logic.
Given a logical formula φ in predicate logic, does
⊨φ hold, yes or no?
We now show that this problem is not solvable; we cannot write a correct
C or Java program that works for all φ. It is important to be clear about
exactly what we are stating. Naturally, there are some φ which can easily be
seen to be valid; and others which can easily be seen to be invalid. However,
there are also some φ for which it is not easy. Every φ can, in principle, be
discovered to be valid or not, if you are prepared to work arbitrarily hard at
it; but there is no uniform mechanical procedure for determining whether φ
is valid which will work for all φ.
We prove this by a well-known technique called problem reduction. That
is, we take some other problem, of which we already know that it is not
solvable, and we then show that the solvability of our problem entails the
solvability of the other one. This is a beautiful application of the proof rules
¬i and ¬e, since we can then infer that our own problem cannot be solvable
as well.
The problem that is known not to be solvable, the Post correspondence
problem, is interesting in its own right and, upon ﬁrst reﬂection, does not
seem to have a lot to do with predicate logic.
The Post correspondence problem.
Given a ﬁnite sequence of pairs
(s1, t1), (s2, t2), . . . , (sk, tk) such that all si and ti are binary strings of pos-
itive length, is there a sequence of indices i1, i2, . . . , in with n ≥1 such that
the concatenation of strings si1si2 . . . sin equals ti1ti2 . . . tin?
Here is an instance of the problem which we can solve successfully: the
concrete correspondence problem instance C is given by a sequence of three
pairs C
def
= ((1, 101), (10, 00), (011, 11)) so
s1
def
= 1
s2
def
= 10
s3
def
= 011
t1
def
= 101
t2
def
= 00
t3
def
= 11.
A solution to the problem is the sequence of indices (1, 3, 2, 3) since s1s3s2s3
and t1t3t2t3 both equal 101110011. Maybe you think that this problem must
surely be solvable; but remember that a computational solution would have

2.5 Undecidability of predicate logic
133
to be a program that solves all such problem instances. Things get a bit
tougher already if we look at this (solvable) problem:
s1
def
= 001
s2
def
= 01
s3
def
= 01
s4
def
= 10
t1
def
= 0
t2
def
= 011
t3
def
= 101
t4
def
= 001
which you are invited to solve by hand, or by writing a program for this
speciﬁc instance.
Note that the same number can occur in the sequence of indices, as hap-
pened in the ﬁrst example in which 3 occurs twice. This means that the
search space we are dealing with is inﬁnite, which should give us some indi-
cation that the problem is unsolvable. However, we do not formally prove it
in this book. The proof of the following theorem is due to the mathematician
A. Church.
Theorem 2.22 The decision problem of validity in predicate logic is unde-
cidable: no program exists which, given any φ, decides whether ⊨φ.
PROOF: As said before, we pretend that validity is decidable for predicate
logic and thereby solve the (insoluble) Post correspondence problem. Given
a correspondence problem instance C:
s1 s2 . . . sk
t1 t2 . . . tk
we need to be able to construct, within ﬁnite space and time and uniformly
so for all instances, some formula φ of predicate logic such that ⊨φ holds
iﬀthe correspondence problem instance C above has a solution.
As function symbols, we choose a constant e and two function symbols
f0 and f1 each of which requires one argument. We think of e as the empty
string, or word, and f0 and f1 symbolically stand for concatenation with 0,
respectively 1. So if b1b2 . . . bl is a binary string of bits, we can code that up
as the term fbl(fbl−1 . . . (fb2(fb1(e))) . . . ). Note that this coding spells that
word backwards. To facilitate reading those formulas, we abbreviate terms
like fbl(fbl−1 . . . (fb2(fb1(t))) . . . ) by fb1b2...bl(t).
We also require a predicate symbol P which expects two arguments.
The intended meaning of P(s, t) is that there is some sequence of indices
(i1, i2, . . . , im) such that s is the term representing si1si2 . . . sim and t rep-
resents ti1ti2 . . . tim. Thus, s constructs a string using the same sequence of
indices as does t; only s uses the si whereas t uses the ti.

134
2 Predicate logic
Our sentence φ has the coarse structure φ1 ∧φ2 →φ3 where we set
φ1
def
=
k
i=1
P(fsi(e), fti(e))
φ2
def
= ∀v ∀w

P(v, w) →
k
i=1
P(fsi(v), fti(w))

φ3
def
= ∃z P(z, z) .
Our claim is ⊨φ holds iﬀthe Post correspondence problem C has a solution.
First, let us assume that ⊨φ holds. Our strategy is to ﬁnd a model for
φ which tells us there is a solution to the correspondence problem C simply
by inspecting what it means for φ to satisfy that particular model. The
universe of concrete values A of that model is the set of all ﬁnite, binary
strings (including the empty string denoted by ϵ).
The interpretation eM of the constant e is just that empty string ϵ. The
interpretation of f0 is the unary function fM
0
which appends a 0 to a given
string, fM
0 (s)
def
= s0; similarly, fM
1 (s)
def
= s1 appends a 1 to a given string.
The interpretation of P on M is just what we expect it to be:
P M def
= {(s, t) | there is a sequence of indices (i1, i2, . . . , im) such that
s equals si1si2 . . . sim and t equals ti1ti2 . . . tim}
where s and t are binary strings and the si and ti are the data of the
correspondence problem C. A pair of strings (s, t) lies in P M iﬀ, using the
same sequence of indices (i1, i2, . . . , im), s is built using the corresponding
si and t is built using the respective ti.
Since ⊨φ holds we infer that M ⊨φ holds, too. We claim that M ⊨
φ2 holds as well, which says that whenever the pair (s, t) is in P M, then
the pair (s si, t ti) is also in P M for i = 1, 2, . . . , k (you can verify that is
says this by inspecting the deﬁnition of P M). Now (s, t) ∈P M implies that
there is some sequence (i1, i2, . . . , im) such that s equals si1si2 . . . sim and t
equals ti1ti2 . . . tim. We simply choose the new sequence (i1, i2, . . . , im, i) and
observe that s si equals si1si2 . . . simsi and t ti equals ti1ti2 . . . timti and so
M ⊨φ2 holds as claimed. (Why does M ⊨φ1 hold?)
Since M ⊨φ1 ∧φ2 →φ3 and M ⊨φ1 ∧φ2 hold, it follows that M ⊨φ3
holds as well. By deﬁnition of φ3 and P M, this tells us there is a solution
to C.
Conversely, let us assume that the Post correspondence problem C has
some solution, namely the sequence of indices (i1, i2, . . . , in). Now we have to
show that, if M′ is any model having a constant eM′, two unary functions,

2.5 Undecidability of predicate logic
135
fM′
0
and fM′
1
, and a binary predicate P M′, then that model has to satisfy
φ. Notice that the root of the parse tree of φ is an implication, so this is
the crucial clause for the deﬁnition of M′ ⊨φ. By that very deﬁnition, we
are already done if M′ ̸⊨φ1, or if M′ ̸⊨φ2. The harder part is therefore the
one where M′ ⊨φ1 ∧φ2, for in that case we need to verify M′ ⊨φ3 as well.
The way we proceed here is by interpreting ﬁnite, binary strings in the
domain of values A′ of the model M′. This is not unlike the coding of an
interpreter for one programming language in another. The interpretation is
done by a function interpret which is deﬁned inductively on the data structure
of ﬁnite, binary strings:
interpret(ϵ)
def
= eM′
interpret(s0)
def
= fM′
0
(interpret(s))
interpret(s1)
def
= fM′
1
(interpret(s)) .
Note that interpret(s) is deﬁned inductively on the length of s. This interpre-
tation is, like the coding above, backwards; for example, the string 0100110
gets interpreted as fM′
0
(fM′
1
(fM′
1
(fM′
0
(fM′
0
(fM′
1
(fM′
0
(eM′))))))). Note that
interpret(b1b2 . . . bl) = fM′
bl (fM′
bl−1(. . . (fb1(eM′) . . . ))) is just the meaning of
fs(e) in A′, where s equals b1b2 . . . bl. Using that and the fact that M′ ⊨φ1,
we conclude that (interpret(si), interpret(ti)) ∈P M′ for i = 1, 2, . . . , k. Sim-
ilarly, since M′ ⊨φ2, we know that for all (s, t) ∈P M′ we have that
(interpret(ssi), interpret(tti)) ∈P M′ for i = 1, 2, . . . , k. Using these two facts,
starting with (s, t) = (si1, ti1), we repeatedly use the latter observation to
obtain
(interpret(si1si2 . . . sin), interpret(ti1ti2 . . . tin)) ∈P M′.
(2.9)
Since si1si2 . . . sin and ti1ti2 . . . tin together form a solution of C, they are
equal; and therefore interpret(si1si2 . . . sin) and interpret(ti1ti2 . . . tin) are the
same elements in A′, for interpreting the same thing gets you the same result.
Hence (2.9) veriﬁes ∃z P(z, z) in M′ and thus M′ ⊨φ3.
2
There are two more negative results which we now get quite easily. Recall
that a formula φ is satisﬁable if there is some model M and some environ-
ment l such that M ⊨l φ holds. This property is not to be taken for granted;
the formula ∃x (P(x) ∧¬P(x)) is clearly unsatisﬁable. More interesting is
the observation that φ is unsatisﬁable if, and only if, ¬φ is valid, i.e. holds
in all models. This is an immediate consequence of the deﬁnitional clause
M ⊨l ¬φ for negation. Since we can’t compute validity, it follows that we
cannot compute satisﬁability either.

136
2 Predicate logic
The other undecidability result comes from the soundness and complete-
ness of predicate logic which, in special form for sentences, reads as
⊨φ iﬀ
⊢φ
(2.10)
which we do not prove in this text. Since we can’t decide validity, we cannot
decide provability either, on the basis of (2.10). One might reﬂect on that
last negative result a bit. It means bad news if one wants to implement
perfect theorem provers which can mechanically produce a proof of a given
formula, or refute it. It means good news, though, if we like the thought
that machines still need a little bit of human help. Creativity seems to have
limits if we leave it to machines alone.
2.6 Expressiveness of predicate logic
Predicate logic is much more expressive than propositional logic, having
predicate and function symbols, as well as quantiﬁers. This expressivess
comes at the cost of making validity, satisﬁability and provability undecid-
able. The good news, though, is that checking formulas on models is practi-
cal; SQL queries over relational databases or XQueries over XML documents
are examples of this in practice.
Software models, design standards, and execution models of hardware or
programs often are described in terms of directed graphs. Such models M
are interpretations of a two-argument predicate symbol R over a concrete
set A of ‘states.’
Example 2.23 Given a set of states A = {s0, s1, s2, s3}, let RM be the
set {(s0, s1), (s1, s0), (s1, s1), (s1, s2), (s2, s0), (s3, s0), (s3, s2)}. We may de-
pict this model as a directed graph in Figure 2.5, where an edge (a transi-
tion) leads from a node s to a node s′ iﬀ(s, s′) ∈RM. In that case, we often
denote this as s →s′.
The validation of many applications requires to show that a ‘bad’ state
cannot be reached from a ‘good’ state. What ‘good’ and ‘bad’ mean will
depend on the context. For example, a good state may be one in which an
integer expression, say x ∗(y −1), evaluates to a value that serves as a safe
index into an array a of length 10. A bad state would then be one in which
this integer expression evaluates to an unsafe value, say 11, causing an ‘out-
of-bounds exception.’ In its essence, deciding whether from a good state one
can reach a bad state is the reachability problem in directed graphs.

2.6 Expressiveness of predicate logic
137
s1
s0
s3
s2
Figure 2.5. A directed graph, which is a model M for a predicate sym-
bol R with two arguments. A pair of nodes (n, n′) is in the interpretation
RM of R iff there is a transition (an edge) from node n to node n′ in
that graph.
Reachability: Given nodes n and n′ in a directed graph, is there a ﬁnite
path of transitions from n to n′?
In Figure 2.5, state s2 is reachable from state s0, e.g. through the path
s0 →s1 →s2. By convention, every state reaches itself by a path of length
0. State s3, however, is not reachable from s0; only states s0, s1, and s2
are reachable from s0. Given the evident importance of this concept, can
we express reachability in predicate logic – which is, after all, so expressive
that it is undecidable? To put this question more precisely: can we ﬁnd a
predicate-logic formula φ with u and v as its only free variables and R as
its only predicate symbol (of arity 2) such that φ holds in directed graphs
iﬀthere is a path in that graph from the node associated to u to the node
associated to v? For example, we might try to write:
u = v ∨∃x(R(u, x) ∧R(x, v)) ∨∃x1∃x2(R(u, x1) ∧R(x1, x2) ∧R(x2, v)) ∨. . .
This is inﬁnite, so it’s not a well-formed formula. The question is: can we
ﬁnd a well-formed formula with the same meaning?
Surprisingly, this is not the case. To show this we need to record an im-
portant consequence of the completeness of natural deduction for predicate
logic.
Theorem 2.24 (Compactness Theorem) Let Γ be a set of sentences of
predicate logic. If all ﬁnite subsets of Γ are satisﬁable, then so is Γ.
PROOF:
We use proof by contradiction: Assume that Γ is not satisﬁable.
Then the semantic entailment Γ ⊨⊥holds as there is no model in which
all φ ∈Γ are true. By completeness, this means that the sequent Γ ⊢⊥
is valid. (Note that this uses a slightly more general notion of sequent in
which we may have inﬁnitely many premises at our disposal. Soundness and

138
2 Predicate logic
completeness remain true for that reading.) Thus, this sequent has a proof
in natural deduction; this proof – being a ﬁnite piece of text – can use
only ﬁnitely many premises ∆from Γ. But then ∆⊢⊥is valid, too, and
so ∆⊨⊥follows by soundness. But the latter contradicts the fact that all
ﬁnite subsets of Γ are consistent.
2
From this theorem one may derive a number of useful techniques. We men-
tion a technique for ensuring the existence of models of inﬁnite size.
Theorem 2.25 (L¨owenheim-Skolem Theorem) Let ψ be a sentence of
predicate logic such for any natural number n ≥1 there is a model of ψ with
at least n elements. Then ψ has a model with inﬁnitely many elements.
PROOF:
The formula φn
def
= ∃x1∃x2 . . . ∃xn

1≤i<j≤n ¬(xi = xj) speciﬁes
that there are at least n elements. Consider the set of sentences Γ
def
=
{ψ} ∪{φn | n ≥1} and let ∆be any if its ﬁnite subsets. Let k ≥1 be such
that n ≤k for all n with φn ∈∆. Since the latter set is ﬁnite, such a k has to
exist. By assumption, {ψ, φk} is satisﬁable; but φk →φn is valid for all n ≤k
(why?). Therefore, ∆is satisﬁable as well. The compactness theorem then
implies that Γ is satisﬁable by some model M; in particular, M ⊨ψ holds.
Since M satisﬁes φn for all n ≥1, it cannot have ﬁnitely many elements. 2
We can now show that reachability is not expressible in predicate logic.
Theorem 2.26 Reachability is not expressible in predicate logic: there is
no predicate-logic formula φ with u and v as its only free variables and R as
its only predicate symbol (of arity 2) such that φ holds in directed graphs
iﬀthere is a path in that graph from the node associated to u to the node
associated to v.
PROOF:
Suppose there is a formula φ expressing the existence of a path
from the node associated to u to the node associated to v. Let c and c′ be
constants. Let φn be the formula expressing that there is a path of length n
from c to c′: we deﬁne φ0 as c = c′, φ1 as R(c, c′) and, for n > 1,
φn
def
= ∃x1 . . . ∃xn−1(R(c, x1) ∧R(x1, x2) ∧· · · ∧R(xn−1, c′)).
Let ∆= {¬φi | i ≥0} ∪{φ[c/u][c′/v]}. All formulas in ∆are sentences and
∆is unsatisﬁable, since the ‘conjunction’ of all sentences in ∆says that
there is no path of length 0, no path of length 1, etc. from the node denoted
by c to the node denoted by c′, but there is a ﬁnite path from c to c′ as
φ[c/u][c′/v] is true.

2.6 Expressiveness of predicate logic
139
However, every ﬁnite subset of ∆is satisﬁable since there are paths of any
ﬁnite length. Therefore, by the Compactness Theorem, ∆itself is satisﬁable.
This is a contradiction. Therefore, there cannot be such a formula φ.
2
2.6.1 Existential second-order logic
If predicate logic cannot express reachability in graphs, then what can, and
at what cost? We seek an extension of predicate logic that can specify such
important properties, rather than inventing an entirely new syntax, seman-
tics and proof theory from scratch. This can be realized by applying quan-
tiﬁers not only to variables, but also to predicate symbols. For a predicate
symbol P with n ≥1 arguments, consider formulas of the form
∃P φ
(2.11)
where φ is a formula of predicate logic in which P occurs. Formulas of that
form are the ones of existential second-order logic. An example of arity 2 is
∃P ∀x∀y∀z (C1 ∧C2 ∧C3 ∧C4)
(2.12)
where each Ci is a Horn clause4
C1
def
= P(x, x)
C2
def
= P(x, y) ∧P(y, z) →P(x, z)
C3
def
= P(u, v) →⊥
C4
def
= R(x, y) →P(x, y).
If we think of R and P as two transition relations on a set of states, then
C4 says that any R-edge is also a P-edge, C1 states that P is reﬂexive, C2
speciﬁes that P is transitive, and C3 ensures that there is no P-path from
the node associated to u to the node associated to v.
Given a model M with interpretations for all function and predicate sym-
bols of φ in (2.11), except P, let MT be that same model augmented with
an interpretation T ⊆A × A of P, i.e. P MT = T. For any look-up table l,
the semantics of ∃P φ is then
M ⊨l ∃P φ
iﬀ
for some T ⊆A × A, MT ⊨l φ.
(2.13)
4 Meaning, a Horn clause after all atomic subformulas are replaced with propositional atoms.

140
2 Predicate logic
Example 2.27 Let ∃P φ be the formula in (2.12) and consider the model
M of Example 2.23 and Figure 2.5. Let l be a look-up table with l(u) = s0
and l(v) = s3. Does M ⊨l ∃P φ hold? For that, we need an interpretation
T ⊆A × A of P such that MT ⊨l ∀x∀y∀x (C1 ∧C2 ∧C3 ∧C4) holds. That
is, we need to ﬁnd a reﬂexive and transitive relation T ⊆A × A that con-
tains RM but not (s0, s3). Please verify that T
def
= {(s, s′) ∈A × A | s′ ̸= s3}
∪{(s3, s3)} is such a T. Therefore, M ⊨l ∃P φ holds.
In the exercises you are asked to show that the formula in (2.12) holds in
a directed graph iﬀthere isn’t a ﬁnite path from node l(u) to node l(v) in
that graph. Therefore, this formula speciﬁes unreachability.
2.6.2 Universal second-order logic
Of course, we can negate (2.12) and obtain
∀P ∃x∃y∃z (¬C1 ∨¬C2 ∨¬C3 ∨¬C4)
(2.14)
by relying on the familiar de Morgan laws. This is a formula of universal
second-order logic. This formula expresses reachability.
Theorem 2.28 Let M = (A, RM) be any model. Then the formula
in (2.14) holds under look-up table l in M iﬀl(v) is R-reachable from l(u)
in M.
PROOF:
1.
First, assume that MT ⊨l ∃x∃y∃z (¬C1 ∨¬C2 ∨¬C3 ∨¬C4) holds for all inter-
pretations T of P. Then it also holds for the interpretation which is the re-
ﬂexive, transitive closure of RM. But for that T, MT ⊨l ∃x∃y∃z (¬C1 ∨¬C2 ∨
¬C3 ∨¬C4) can hold only if MT ⊨l ¬C3 holds, as all other clauses Ci (i ̸= 3)
are false. But this means that MT ⊨l P(u, v) has to hold. So (l(u), l(v)) ∈T
follows, meaning that there is a ﬁnite path from l(u) to l(v).
2.
Conversely, let l(v) be R-reachable from l(u) in M.
– For any interpretation T of P which is not reﬂexive, not transitive or does
not contain RM the relation MT ⊨l ∃x∃y∃z (¬C1 ∨¬C2 ∨¬C3 ∨¬C4) holds,
since T makes one of the clauses ¬C1, ¬C2 or ¬C4 true.
– The other possibility is that T be a reﬂexive, transitive relation containing
RM. Then T contains the reﬂexive, transitive closure of RM. But (l(u), l(v)) is
in that closure by assumption. Therefore, ¬C3 is made true in the interpreta-
tion T under look-up table l, and so MT ⊨l ∃x∃y∃z (¬C1 ∨¬C2 ∨¬C3 ∨¬C4)
holds.

2.7 Micromodels of software
141
In summary, MT ⊨l ∃x∃y∃z (¬C1 ∨¬C2 ∨¬C3 ∨¬C4) holds for all inter-
pretations T ⊆A × A. Therefore, M ⊨l ∀P ∃x∃y∃z (¬C1 ∨¬C2 ∨¬C3 ∨¬C4)
holds.
2
It is beyond the scope of this text to show that reachability can also be
expressed in existential second-order logic, but this is indeed the case. It is
an important open problem to determine whether existential second-order
logic is closed under negation, i.e. whether for all such formulas ∃P φ there
is a formula ∃Q ψ of existential second-order logic such that the latter is
semantically equivalent to the negation of the former.
If we allow existential and universal quantiﬁers to apply to predicate sym-
bols in the same formula, we arrive at fully-ﬂedged second-order logic, e.g.
∃P∀Q (∀x∀y (Q(x, y) →Q(y, x)) →∀u∀v (Q(u, v) →P(u, v))).
(2.15)
We have ∃P∀Q (∀x∀y (Q(x, y) →Q(y, x)) →∀u∀v (Q(u, v) →P(u, v))) iﬀ
there is some T such that for all U we have (MT )U ⊨∀x∀y (Q(x, y) →
Q(y, x)) →∀u∀v (Q(u, v) →P(u, v)), the latter being a model check in ﬁrst-
order logic.
If one wants to quantify over relations of relations, one gets third-order
logic etc. Higher-order logics require great care in their design. Typical re-
sults such as completeness and compactness may quickly fail to hold. Even
worse, a naive higher-order logic may be inconsistent at the meta-level. Re-
lated problems were discovered in naive set theory, e.g. in the attempt to
deﬁne the ‘set’ A that contains as elements those sets X that do not contain
themselves as an element:
A
def
= {X | X ̸∈X}.
(2.16)
We won’t study higher-order logics in this text, but remark that many the-
orem provers or deductive frameworks rely on higher-order logical frame-
works.
2.7 Micromodels of software
Two of the central concepts developed so far are
r model checking: given a formula φ of predicate logic and a matching model M
determine whether M ⊨φ holds; and
r semantic entailment: given a set of formulas Γ of predicate logic, is Γ ⊨φ valid?

142
2 Predicate logic
How can we put these concepts to use in the modelling and reasoning about
software? In the case of semantic entailment, Γ should contain all the re-
quirements we impose on a software design and φ may be a property we
think should hold in any implementation that meets the requirements Γ.
Semantic entailment therefore matches well with software speciﬁcation and
validation; alas, it is undecidable in general. Since model checking is de-
cidable, why not put all the requirements into a model M and then check
M ⊨φ? The diﬃculty with this approach is that, by comitting to a particu-
lar model M, we are comitting to a lot of detail which doesn’t form part of
the requirements. Typically, the model instantiates a number of parameters
which were left free in the requirements. From this point of view, semantic
entailment is better, because it allows a variety of models with a variety of
diﬀerent values for those parameters.
We seek to combine semantic entailment and model checking in a way
which attempts to give us the advantages of both. We will extract from
the requirements a relatively small number of small models, and check that
they satisfy the property φ to be proved. This satisfaction checking has the
tractability of model checking, while the fact that we range over a set of mod-
els (albeit a small one) allows us to consider diﬀerent values of parameters
which are not set in the requirements.
This approach is implemented in a tool called Alloy, due to D. Jackson.
The models we consider are what he calls ‘micromodels’ of software.
2.7.1 State machines
We illlustrate this approach by revisiting Example 2.15 from page 125. Its
models are state machines with F = {i} and P = {R, F}, where i is a con-
stant, F a predicate symbol with one argument and R a predicate symbol
with two arguments. A (concrete) model M contains a set of concrete el-
ements A – which may be a set of states of a computer program. The in-
terpretations iM ∈A, RM ∈A × A, and F M ⊆A are understood to be a
designated initial state, a state transition relation, and a set of ﬁnal (ac-
cepting) states, respectively. Model M is concrete since there is nothing left
un-speciﬁed and all checks M ⊨φ have deﬁnite answers: they either hold or
they don’t.
In practice not all functional or other requirements of a software sys-
tem are known in advance, and they are likely to change during its life-
cycle. For example, we may not know how many states there will be; and
some transitions may be mandatory whereas others may be optional in an
implementation. Conceptually, we seek a description M of all compliant

2.7 Micromodels of software
143
implementations Mi (i ∈I) of some software system. Given some matching
property ψ, we then want to know
r (assertion checking) whether ψ holds in all implementations Mi ∈M; or
r (consistency checking) whether ψ holds in some implementation Mi ∈M.
For example, let M be the set of all concrete models of state machines, as
above. A possible assertion check ψ is ‘Final states are never initial states.’
An example of a consistency check ψ is ‘There are state machines that
contain a non-ﬁnal but deadlocked state.’
As remarked earlier, if M were the set of all state machines, then checking
properties would risk being undecidable, and would at least be intractable.
If M consists of a single model, then checking properties would be decidable;
but a single model is not general enough. It would comit us to instantiating
several parameters which are not part of the requirements of a state machine,
such as its size and detailed construction. A better idea is to ﬁx a ﬁnite bound
on the size of models, and check whether all models of that size that satisfy
the requirements also satisfy the property under consideration.
r If we get a positive answer, we are somewhat conﬁdent that the property holds
in all models. In this case, the answer is not conclusive, because there could be
a larger model which fails the property, but nevertheless a positive answer gives
us some conﬁdence.
r If we get a negative answer, then we have found a model in M which violates
the property. In that case, we have a conclusive answer, and can inspect the
model in question.
D. Jackson’s small scope hypothesis states that negative answers tend to
occur in small models already, boosting the conﬁdence we may have in a
positive answer. Here is how one could write the requirements for M for
state machines in Alloy:
sig State {}
sig StateMachine {
A : set State,
i : A,
F : set A,
R : A -> A
}
The model speciﬁes two signatures. Signature State is simple in that it has
no internal structure, denoted by {}. Although the states of real systems may

144
2 Predicate logic
well have internal structure, our Alloy declaration abstracts it away. The
second signature StateMachine has internal, composite structure, saying
that every state machine has a set of states A, an initial state i from A, a set
of ﬁnal states F from A, and a transition relation R of type A -> A. If we read
-> as the cartesian product ×, we see that this internal structure is simply
the structural information needed for models of Example 2.15 (page 125).
Concrete models of state machines are instances of signature StateMachine.
It is useful to think of signatures as sets whose elements are the instances of
that signature. Elements possess all the structure declared in their signature.
Given these signatures, we can code and check an assertion:
assert FinalNotInitial {
all M : StateMachine | no M.i & M.F
} check FinalNotIntial for 3 but 1 StateMachine
declares an assertion named FinalNotInitial whose body speciﬁes that
for all models M of type StateMachine the property no M.i & M.F is true.
Read & for set intersection and no S (‘there is no S’) for ‘set S is empty.’
Alloy identiﬁes elements a with singleton sets {a}, so this set intersection
is well typed. The relational dot operator . enables access to the internal
components of a state machine: M.i is the initial state of M and M.F is its set
of ﬁnal states etc. Therefore, the expression no M.i & M.F states ‘No initial
state of M is also a ﬁnal state of M.’ Finally, the check directive informs the
analyzer of Alloy that it should try to ﬁnd a counterexample of the assertion
FinalNotInitial with at most three elements for every signature, except
for StateMachine which should have at most one.
The results of Alloy’s assertion check are shown in Figure 2.7. This visual-
ization has been customized to decorate initial and ﬁnal states with respec-
tive labels i and F. The transition relation is shown as a labeled graph and
there is only one transition (from State 0 back to State 0) in this exam-
ple. Please verify that this is a counterexample to the claim of the assertion
FinalNotInitial within the speciﬁed scopes. Alloy’s GUI lets you search
for additional witnesses (here: counterexamples), if they exist.
Similarly, we can check a property of state machines for consistency with
our model. Alloy uses the keyword fun for consistency checks. e.g.
fun AGuidedSimulation(M : StateMachine, s : M.A) {
no s.(M.R)
not s in M.F
# M.A = 3
} run AGiudedSimulation for 3 but 1 StateMachine

2.7 Micromodels of software
145
module AboutStateMachines
sig State {}
-- simple states
sig StateMachine { -- composite state machines
A : set State,
-- set of states of a state machine
i : A,
-- initial state of a state machine
F : set A,
-- set of final states of a state machine
R : A -> A
-- transition relation of a state machine
}
-- Claim that final states are never initial: false.
assert FinalNotInitial {
all M : StateMachine | no M.i & M.F
} check FinalNotInitial for 3 but 1 StateMachine
-- Is there a three-state machine with a non-final deadlock? True.
fun AGuidedSimulation(M : StateMachine, s : M.A) {
no s.(M.R)
not s in M.F
# M.A = 3
} run AGuidedSimulation for 3 but 1 StateMachine
Figure 2.6. The complete Alloy module for models of state machines,
with one assertion and one consistency check. The lexeme -- enables
comments on the same line.
State_0
R
State_1
(F)
State_2
(i, F)
Figure 2.7. Alloy’s analyzer finds a state machine model (with one
transition only) within the specified scope such that the assertion
FinalNotInitial is false: the initial state State 2 is also final.
This consistency check is named AGuidedSimulation and followed by an
ordered ﬁnite list of parameter/type pairs; the ﬁrst parameter is M of type
StateMachine, the second one is s of type M.A – i.e. s is a state of M. The
body of a consistency check is a ﬁnite list of constraints (here three), which
are conjoined implicitly. In this case, we want to ﬁnd a model with instances
of the parameters M and s such that s is a non-ﬁnal state of M, the second
constraint not s in M.F plus the type information s : M.A; and there is
no transition out of s, the ﬁrst constraint no s.(M.R).
The latter requires further explanation. The keyword no denotes ‘there
is no;’ here it is applied to the set s.(M.R), expressing that there are no

146
2 Predicate logic
State_0
State_2
(i)
R
State_1
R
Figure 2.8. Alloy’s analyzer finds a state machine model within the
specified scope such that the consistency check AGuidedSimulation is
true: there is a non-final deadlocked state, here State 2.
elements in s.(M.R). Since M.R is the transition relation of M, we need to
understand how s.(M.R) constructs a set. Well, s is an element of M.A and
M.R has type M.A -> M.A. Therefore, we may form the set of all elements s’
such that there is a M.R-transition from s to s’; this is the set s.(M.R). The
third constraint states that M has exactly three states: in Alloy,
# S = k
declares that the set S has exactly k elements.
The
run
directive
instructs
to
check
the
consistency
of
AGuidedSimulation for at most one state machine and at most three
states; the constraint analyzer of Alloy returns the witness (here: an exam-
ple) of Figure 2.8. Please check that this witness satisﬁes all constraints of
the consistency check and that it is within the speciﬁed scopes.
The complete model of state machines with these two checks is depicted in
Figure 2.6. The keyword plus name module AboutStateMachines identify
this under-speciﬁed model M, rightly suggesting that Alloy is a modular
speciﬁcation and analysis platform.
2.7.2 Alma – re-visited
Recall Example 2.19 from page 128. Its model had three elements and did
not satisfy the formula in (2.8). We can now write a module in Alloy which
checks whether all smaller models have to satisfy (2.8). The code is given in
Figure 2.9. It names the module AboutAlma and deﬁnes a simple signature of
type Person. Then it declares a signature SoapOpera which has a cast – a
set of type Person – a designated cast member alma, and a relation loves
of type cast -> cast. We check the assertion OfLovers in a scope of at
most two persons and at most one soap opera. The body of that assertion
is the typed version of (2.8) and deserves a closer look:
1.
Expressions of the form all x : T | F state that formula F is true for all
instances x of type T. So the assertion states that with S {...} is true for all
soap operas S.

2.7 Micromodels of software
147
module AboutAlma
sig Person {}
sig SoapOpera {
cast : set Person,
alma : cast,
loves : cast -> cast
}
assert OfLovers {
all S : SoapOpera |
with S {
all x, y : cast |
alma in x.loves && x in y.loves => not alma in y.loves
}
}
check OfLovers for 2 but 1 SoapOpera
Figure 2.9. In this module, the analysis of OfLovers checks whether
there is a model of ≤2 persons and ≤1 soap operas for which the
query in (2.8), page 128, is false.
Person_1
(cast, alma)
loves
Person_0
Figure 2.10. Alloy’s analyzer finds a counterexample to the formula in
(2.8): Alma is the only cast member and loves herself.
2.
The expression with S {...} is a convenient notation that allows us to write
loves and cast instead of the needed S.loves and S.cast (respectively) within
its curly brackets.
3.
Its body ... states that for all x, and y in the cast of S, if alma is loved by x
and x is loved by y, then – the symbol => expresses implication – alma is not
loved by y.
Alloy’s analysis ﬁnds a counterexample to this assertion, shown in Fig-
ure 2.10. It is a counterexample since alma is her own lover, and therefore
also one of her lover’s lovers’. Apparently, we have underspeciﬁed our model:
we implicitly made the domain-speciﬁc assumption that self-love makes for

148
2 Predicate logic
Person_1
(cast)
Person_0
(cast, alma)
loves
Person_2
(cast)
loves
loves
Figure 2.11. Alloy’s analyzer finds a counterexample to the formula in
(2.8) that meets the constraint of NoSelfLove with three cast members.
The bidirectional arrow indicates that Person 1 loves Person 2 and vice
versa.
a poor script of jealousy and intrigue, but did not rule out self-love in our
Alloy module. To remedy this, we can add a fact to the module; facts may
have names and restrict the set of possible models: assertions and consis-
tency checks are conducted only over concrete models that satisfy all facts
of the module. Adding the declaration
fact NoSelfLove {
all S : SoapOpera, p : S.cast | not p in p.(S.loves)
}
to the module AboutAlma enforces that no member of any soap-opera cast
loves him or herself. We re-check the assertion and the analyzer informs us
that no solution was found. This suggests that our model from Example 2.19
is indeed a minimal one in the presence of that domain assumption. If we
retain that fact, but change the occurrence of 2 in the check directive to 3,
we get a counterexample, depicted in Figure 2.11. Can you see why it is a
counterexample?
2.7.3 A software micromodel
So far we used Alloy to generate instances of models of ﬁrst-order logic that
satisfy certain constraints expressed as formulas of ﬁrst-order logic. Now we
apply Alloy and its constraint analyzer to a more serious task: we model a
software system. The intended beneﬁts provided by a system model are
1.
it captures formally static and dynamic system structure and behaviour;
2.
it can verify consistency of the constrained design space;

2.7 Micromodels of software
149
3.
it is executable, so it allows guided simulations through a potentially very com-
plex design space; and
4.
it can boost our conﬁdence into the correctness of claims about static and
dynamic aspects of all its compliant implementations.
Moreover, formal models attached to software products can be seen as a
reliability contract; a promise that the software implements the structure and
behaviour of the model and is expected to meet all of the assertions certiﬁed
therein. (However, this may not be very useful for extremely under-speciﬁed
models.)
We will model a software package dependency system. This system is used
when software packages are installed or upgraded. The system checks to see
if prerequisites in the form of libraries or other packages are present. The
requirements on a software package dependency system are not straightfor-
ward. As most computer users know, the upgrading process can go wrong
in various ways. For example, upgrading a package can involve replacing
shared libraries with newer versions. But other packages which rely on the
older versions of the shared libraries may then cease to work.
Software package dependency systems are used in several computer sys-
tems, such as Red Hat Linux, .NET’s Global Assembly Cache and others.
Users often have to guess how technical questions get resolved within the de-
pendency system. To the best of our knowledge, there is no publicly available
formal and executable model of any particular dependency system to which
application programmers could turn if they had such non-trivial technical
questions about its inner workings.
In our model, applications are built out of components. Components oﬀer
services to other components. A service can be a number of things. Typically,
a service is a method (a modular piece of program code), a ﬁeld entry, or a
type – e.g. the type of a class in an object-oriented programming language.
Components typically require the import of services from other components.
Technically speaking, such import services resolve all un-resolved references
within that component, making the component linkable. A component also
has a name and may have a special service, called ‘main.’
We model components as a signature in Alloy:
sig Component {
name: Name,
-- name of the component
main: option Service, -- component may have a ‘main’ service
export: set Service,
-- services the component exports
import: set Service,
-- services the component imports
version: Number
-- version number of the component
}{ no import & export }

150
2 Predicate logic
The signatures Service and Name won’t require any composite structure for
our modelling purposes. The signature Number will get an ordering later on.
A component is an instance of Component and therefore has a name, a set of
services export it oﬀers to other components, and a set import of services
it needs to import from other components. Last but not least, a component
has a version number. Observe the role of the modiﬁers set and option
above.
A declaration i : set S means that i is a subset of set S; but a declara-
tion i : option S means that i is a subset of S with at most one element.
Thus, option enables us to model an element that may (non-empty, sin-
gleton set) or may not (empty set) be present; a very useful ability indeed.
Finally, a declaration i : S states that i is a subset of S containing ex-
actly one element; this really speciﬁes a scalar/element of type S since Alloy
identiﬁes elements a with sets {a}.
We can constrain all instances of a signature with C by adding { C } to
its signature declaration. We did this for the signature Component, where C
is the constraint no import & export, stating that, in all components, the
intersection (&) of import and export is empty (no).
A Package Dependency System (PDS) consists of a set of components:
sig PDS {
components : set Component
...
}{ components.import in components.export }
and other structure that we specify later on. The primary concern in a PDS
is that its set of components be coherent: at all times, all imports of all of its
components can be serviced within that PDS. This requirement is enforced
for all instances of PDS by adding the constraint components.import in
components.export to its signature. Here components is a set of compo-
nents and Alloy deﬁnes the meaning of components.import as the union of
all sets c.import, where c is an element of components. Therefore the re-
quirement states that, for all c in components, all of c’s needed services can
be provided by some component in components as well. This is exactly the
integrity constraint we need for the set of components of a PDS. Observe that
this requirement does not specify which component provides which service,
which would be an unacceptable imposition on implementation freedom.
Given this integrity constraint we can already model the installation
(adding) or removal of a component in a PDS, without having speciﬁed the
remaining structure of a PDS. This is possible since, in the context of these
operations, we may abstract a PDS into its set of components. We model

2.7 Micromodels of software
151
the addition of a component to a PDS as a parametrized fun-statement with
name AddComponent and three parameters
fun AddComponent(P, P’: PDS, c: Component) {
not c in P.components
P’.components = P.components + c
} run AddComponent for 3
where P is intended to be the PDS prior to the execution of that operation,
P’ models the PDS after that execution, and c models the component that is
to be added. This intent interprets the parametric constraint AddComponent
as an operation leading from one ‘state’ to another (obtained by removing
c from the PDS P). The body of AddComponent states two constraints, con-
joined implicitly. Thus, this operation applies only if the component c is not
already in the set of components of the PDS (not c in P.components; an
example of a precondition) and if the PDS adds only c and does not lose
any other components (P’.components = P.components + c; an example
of a postcondition).
To get a feel for the complexities and vexations of designing software sys-
tems, consider our conscious or implicit decision to enforce that all instances
of PDS have a coherent set of components. This sounds like a very good idea,
but what if a ‘real’ and faulty PDS ever gets to a state in which it is inco-
herent? We would then be prevented from adding components that may re-
store its coherence! Therefore, the aspects of our model do not include issues
such as repair – which may indeed by an important software management
aspect.
The speciﬁcation for the removal of a component is very similar to the
one for AddComponent:
fun RemoveComponent(P, P’: PDS, c: Component) {
c in P.components
P’.components = P.components - c
} run RemoveComponent for 3
except that the precondition now insists that c be in the set of components
of the PDS prior to the removal; and the postcondition speciﬁes that the
PDS lost component c but did not add or lose any other components. The
expression S - T denotes exactly those ‘elements’ of S that are not in T.
It remains to complete the signature for PDS. Three additions are
made.
1.
A relation schedule assigns to each PDS component and any of its import
services a component in that PDS that provides that service.

152
2 Predicate logic
fact SoundPDSs {
all P : PDS |
with P {
all c : components, s : Service |
--1
let c’ = c.schedule[s] {
(some c’ iff s in c.import) && (some c’ => s in c’.export)
}
all c : components | c.requires = c.schedule[Service]
--2
}
}
Figure 2.12. A fact that constrains the state and schedulers of all PDSs.
2.
Derived from schedule we obtain a relation requires between components of
the PDS that expresses the dependencies between these components based on
the schedule.
3.
Finally, we add constraints that ensure the integrity and correct handling of
schedule and requires for all instances of PDS.
The complete signature of PDS is
sig PDS {
components : set Component,
schedule
: components -> Service ->? components,
requires
: components -> components
}
For any P : PDS, the expression P.schedule denotes a relation of type
P.components -> Service ->? P.components. The ? is a multiplicity con-
straint, saying that each component of the PDS and each service get related
to at most one component. This will ensure that the scheduler is deter-
ministic and that it may not schedule anything – e.g. when the service is
not needed by the component in the ﬁrst argument. In Alloy there are also
multiplicity markings ! for ‘exactly one’ and + for ‘one or more.’ The ab-
sence of such markings means ‘zero or more.’ For example, the declaration
of requires uses that default reading.
We use a fact-statement to constrain even further the structure and
behaviour of all PDSs, depicted in Figure 2.12. The fact named SoundPDSs
quantiﬁes the constraints over all instances of PDSs (all P : PDS | ...)
and uses with P {...} to avoid the use of navigation expressions of the
form P.e. The body of that fact lists two constraints --1 and --2:

2.7 Micromodels of software
153
--1 states two constraints within a let-expression of the form let x
= E {...}. Such a let-expression declares all free occurrences of x in
{...} to be equal to E. Note that [] is a version of the dot operator
. with lower binding priority, so c.schedule[s] is syntactic sugar for
s.(c.schedule).
r In the ﬁrst constraint, component c and a service s have another component c’
scheduled (some c’ is true iﬀset c’ is non-empty) if and only if s is actually in
the import set of c. Only needed services are scheduled!
r In the second constraint, if c’ is scheduled to provide service s for c, then s is
in the export set of c’ – we can only schedule components that can provide the
scheduled services!
--2 deﬁnes requires in terms of schedule: a component c requires all those
components that are scheduled to provide some service for c.
Our complete Alloy model for PDSs is shown in Figure 2.13. Using Al-
loy’s constraint analyzer we validate that all our fun-statements, notably
the operations of removing and adding components to a PDS, are logically
consistent for this design.
The assertion AddingIsFunctionalForPDSs claims that the execution of
the operation which adds a component to a PDS renders a unique result
PDS. Alloy’s analyzer ﬁnds a counterexample to this claim, where P has
no components, so nothing is scheduled or required; and P’ and P’’ have
Component 2 as only component, added to P, so this component is required
and scheduled in those PDSs.
Since P’ and P’’ seem to be equal, how can this be a counterexample?
Well, we ran the analysis in scope 3, so PDS = {PDS 0, PDS 1, PDS 2} and
Alloy chose PDS 0 as P, PDS 1 as P’, and PDS 2 as P’’. Since the set PDS
contains three elements, Alloy ‘thinks’ that they are all diﬀerent from each
other. This is the interpretation of equality enforced by predicate logic. Ob-
viously, what is needed here is a
structural equality of types: we want to
ensure that the addition of a component results into a PDS with unique
structure. A fun-statement can be used to specify structural equality:
fun StructurallyEqual(P, P’ : PDS) {
P.components = P’.components
P.schedule = P’.schedule
P.requires = P’.requires
} run StructurallyEqual for 2
We then simply replace the expression P’ = P’’ in AdditionIsFunctional
with the expression StructurallyEqual(P’,P’’), increase the scope for

154
2 Predicate logic
module PDS
open std/ord
-- opens specification template for linear order
sig Component {
name: Name,
main: option Service,
export: set Service,
import: set Service,
version: Number
}{ no import & export }
sig PDS {
components: set Component,
schedule: components -> Service ->? components,
requires: components -> components
}{ components.import in components.export }
fact SoundPDSs {
all P : PDS |
with P {
all c : components, s : Service |
--1
let c’ = c.schedule[s] {
(some c’ iff s in c.import) && (some c’ => s in c’.export) }
all c : components | c.requires = c.schedule[Service]
}
--2
}
sig Name, Number, Service {}
fun AddComponent(P, P’: PDS, c: Component) {
not c in P.components
P’.components = P.components + c
} run AddComponent for 3 but 2 PDS
fun RemoveComponent(P, P’: PDS, c : Component) {
c in P.components
P’.components = P.components - c
} run RemoveComponent for 3 but 2 PDS
fun HighestVersionPolicy(P: PDS) {
with P {
all s : Service, c : components, c’ : c.schedule[s],
c’’ : components - c’ {
s in c’’.export && c’’.name = c’.name =>
c’’.version in c’.version.^(Ord[Number].prev) } }
} run HighestVersionPolicy for 3 but 1 PDS
fun AGuidedSimulation(P,P’,P’’ : PDS, c1, c2 : Component) {
AddComponent(P,P’,c1)
RemoveComponent(P,P’’,c2)
HighestVersionPolicy(P)
HighestVersionPolicy(P’)
HighestVersionPolicy(P’’)
} run AGuidedSimulation for 3
assert AddingIsFunctionalForPDSs {
all P, P’, P’’: PDS, c: Component
{
AddComponent(P,P’,c) &&
AddComponent(P,P’’,c) => P’ = P’’ }
} check AddingIsFunctionalForPDSs for 3
Figure 2.13. Our Alloy model of the PDS.

2.7 Micromodels of software
155
that assertion to 7, re-built the model, and re-analyze that assertion.
Perhaps surprisingly, we ﬁnd as counterexample a PDS 0 with two com-
ponents Component 0 and Component 1 such that Component 0.import =
{ Service 2 }
and
Component 1.import = { Service 1 }.
Since
Service 2 is contained in Component 2.export, we have two struc-
turally diﬀerent legitimate post states which are obtained by adding
Component 2 but which diﬀer in their scheduler. In P’ we have the same
scheduling instances as in PDS 0. Yet P’’ schedules Component 2 to
provide service Service 2 for Component 0; and Component 0 still provides
Service 1 to Component 1. This analysis reveals that the addition of
components creates opportunities to reschedule services, for better (e.g.
optimizations) or for worse (e.g. security breaches).
The utility of a micromodel of software resides perhaps more in the ability
to explore it through guided simulations, as opposed to verifying some of
its properties with absolute certainty. We demonstrate this by generating
a simulation that shows the removal and the addition of a component to a
PDS such that the scheduler always schedules components with the highest
version number possible in all PDSs. Therefore we know that such a schedul-
ing policy is consistent for these two operations; it is by no means the only
such policy and is not guaranteed to ensure that applications won’t break
when using scheduled services. The fun-statement
fun HighestVersionPolicy(P: PDS) {
with P {
all s : Service, c : components, c’ : c.schedule[s],
c’’ : components - c’ {
s in c’’.export && c’’.name = c’.name =>
c’’.version in c’.version.^(Ord[Number].prev)
}
}
} run HighestVersionPolicy for 3 but 1 PDS
speciﬁes that, among those suppliers with identical name, the scheduler
chooses one with the highest available version number. The expression
c’.version.^(Ord[Number].prev)
needs explaining: c’.version is the version number of c’, an element of
type Number. The symbol ^ can be applied to a binary relation r : T -> T
such that ^r has again type T -> T and denotes the transitive closure of r.
In this case, T equals Number and r equals Ord[Number].prev.

156
2 Predicate logic
But what shall me make of the latter expression? It assumes that the mod-
ule contains a statement open std/ord which opens the signature speciﬁca-
tions from another module in ﬁle ord.als of the library std. That module
contains a signature named Ord which has a type variable as a parameter; it
is polymorphic. The expression Ord[Number] instantiates that type variable
with the type Number, and then invokes the prev relation of that signa-
ture with that type, where prev is constrained in std/ord to be a linear
order. The net eﬀect is that we create a linear order on Number such that
n.prev is the previous element of n with respect to that order. Therefore,
n.^prev lists all elements that are smaller than n in that order. Please reread
the body of that fun-statement to convince yourself that it states what is
intended.
Since fun-statements can be invoked with instances of their parameters,
we can write the desired simulation based on HighestVersionPolicy:
fun AGuidedSimulation(P,P’,P’’ : PDS, c1, c2 : Component) {
AddComponent(P,P’,c1)
RemoveComponent(P,P’’,c2)
HighestVersionPolicy(P)
HighestVersionPolicy(P’) HighestVersionPolicy(P’’)
} run AGuidedSimulation for 3
Alloy’s analyzer generates a scenario for this simulation, which amounts
to two diﬀerent operation snapshots originating in P such that all three
participating PDSs schedule according to HighestVersionPolicy. Can you
spot why we had to work with two components c1 and c2?
We conclude this case study by pointing out limitations of Alloy and its
analyzer. In order to be able to use a SAT solver for propositional logic
as an analysis engine, we can only check or run formulas of existential or
universal second-order logic in the bodies of assertions or in the bodies of
fun-statements (if they are wrapped in existential quantiﬁers for all param-
eters). For example, we cannot even check whether there is an instance of
AddComponent such that for the resulting PDS a certain scheduling policy is
impossible. For less explicit reasons it also seems unlikely that we can check
in Alloy that every coherent set of components is realizable as P.components
for some PDS P. This deﬁciency is due to the inherent complexity of such
problems and theorem provers may have to be used if such properties need
to be guaranteed. On the other hand, the expressiveness of Alloy allows for
the rapid prototyping of models and the exploration of simulations and pos-
sible counterexamples which should enhance once understanding of a design
and so improve that design’s reliability.

2.8 Exercises
157
2.8 Exercises
Exercises 2.1
1.
*
Use the predicates
A(x, y) :
x admires y
B(x, y) :
x attended y
P(x) :
x is a professor
S(x) :
x is a student
L(x) :
x is a lecture
and the nullary function symbol (constant)
m :
Mary
to translate the following into predicate logic:
(a) Mary admires every professor.
(The answer is not ∀x A(m, P(x)).)
(b) Some professor admires Mary.
(c) Mary admires herself.
(d) No student attended every lecture.
(e) No lecture was attended by every student.
(f) No lecture was attended by any student.
2. Use the predicate speciﬁcations
B(x, y) :
x beats y
F(x) :
x is an (American) football team
Q(x, y) :
x is quarterback of y
L(x, y) :
x loses to y
and the constant symbols
c :
Wildcats
j :
Jayhawks
to translate the following into predicate logic.
(a) Every football team has a quarterback.
(b) If the Jayhawks beat the Wildcats, then the Jayhawks do not lose to every
football team.
(c) The Wildcats beat some team, which beat the Jayhawks.
3.
*
Find appropriate predicates and their speciﬁcation to translate the following
into predicate logic:
(a) All red things are in the box.
(b) Only red things are in the box.
(c) No animal is both a cat and a dog.
(d) Every prize was won by a boy.
(e) A boy won every prize.

158
2 Predicate logic
4. Let F(x, y) mean that x is the father of y; M(x, y) denotes x is the mother of y.
Similarly, H(x, y), S(x, y), and B(x, y) say that x is the husband/sister/brother
of y, respectively. You may also use constants to denote individuals, like ‘Ed’
and ‘Patsy.’ However, you are not allowed to use any predicate symbols other
than the above to translate the following sentences into predicate logic:
(a) Everybody has a mother.
(b) Everybody has a father and a mother.
(c) Whoever has a mother has a father.
(d) Ed is a grandfather.
(e) All fathers are parents.
(f) All husbands are spouses.
(g) No uncle is an aunt.
(h) All brothers are siblings.
(i) Nobody’s grandmother is anybody’s father.
(j) Ed and Patsy are husband and wife.
(k) Carl is Monique’s brother-in-law.
5. The following sentences are taken from the RFC3157 Internet Taskforce Docu-
ment ‘Securely Available Credentials – Requirements.’ Specify each sentence in
predicate logic, deﬁning predicate symbols as appropriate:
(a) An attacker can persuade a server that a successful login has occurred, even
if it hasn’t.
(b) An attacker can overwrite someone else’s credentials on the server.
(c) All users enter passwords instead of names.
(d) Credential transfer both to and from a device MUST be supported.
(e) Credentials MUST NOT be forced by the protocol to be present in cleartext
at any device other than the end user’s.
(f) The protocol MUST support a range of cryptographic algorithms, includ-
ing syymetric and asymmetric algorithms, hash algorithms, and MAC algo-
rithms.
(g) Credentials MUST only be downloadable following user authentication or
else only downloadable in a format that requires completion of user authen-
tication for deciphering.
(h) Diﬀerent end user devices MAY be used to download, upload, or manage the
same set of credentials.
Exercises 2.2
1. Let F be {d, f, g}, where d is a constant, f a function symbol with two arguments
and g a function symbol with three arguments.
(a) Which of the following strings are terms over F? Draw the parse tree of those
strings which are indeed terms:
i. g(d, d)
ii.
*
f(x, g(y, z), d)

2.8 Exercises
159
∗
−
x
2
+
s
y
x
Figure 2.14. A parse tree representing an arithmetic term.
iii.
*
g(x, f(y, z), d)
iv. g(x, h(y, z), d)
v. f(f(g(d, x), f(g(d, x), y, g(y, d)), g(d, d)), g(f(d, d, x), d), z)
(b) The length of a term over F is the length of its string representation, where we
count all commas and parentheses. For example, the length of f(x, g(y, z), z)
is 13. List all variable-free terms over F of length less than 10.
(c)
*
The height of a term over F is deﬁned as 1 plus the length of the longest
path in its parse tree, as in Deﬁnition 1.32. List all variable-free terms over
F of height less than 4.
2. Draw the parse tree of the term (2 −s(x)) + (y ∗x), considering that −, +, and
∗are used in inﬁx in this term. Compare your solution with the parse tree in
Figure 2.14.
3. Which of the following strings are formulas in predicate logic? Specify a reason
for failure for strings which aren’t, draw parse trees of all strings which are.
(a)
*
Let m be a constant, f a function symbol with one argument and S and B
two predicate symbols, each with two arguments:
i. S(m, x)
ii. B(m, f(m))
iii. f(m)
iv. B(B(m, x), y)
v. S(B(m), z)
vi. (B(x, y) →(∃z S(z, y)))
vii. (S(x, y) →S(y, f(f(x))))
viii. (B(x) →B(B(x))).
(b) Let c and d be constants, f a function symbol with one argument, g a function
symbol with two arguments and h a function symbol with three arguments.
Further, P and Q are predicate symbols with three arguments:

160
2 Predicate logic
i. ∀x P(f(d), h(g(c, x), d, y))
ii. ∀x P(f(d), h(P(x, y), d, y))
iii. ∀x Q(g(h(x, f(d), x), g(x, x)), h(x, x, x), c)
iv. ∃z (Q(z, z, z) →P(z))
v. ∀x ∀y (g(x, y) →P(x, y, x))
vi. Q(c, d, c).
4. Let φ be ∃x (P(y, z) ∧(∀y (¬Q(y, x) ∨P(y, z)))), where P and Q are predicate
symbols with two arguments.
(a)
*
Draw the parse tree of φ.
(b)
*
Identify all bound and free variable leaves in φ.
(c) Is there a variable in φ which has free and bound occurrences?
(d)
*
Consider the terms w (w is a variable), f(x) and g(y, z), where f and g are
function symbols with arity 1 and 2, respectively.
i. Compute φ[w/x], φ[w/y], φ[f(x)/y] and φ[g(y, z)/z].
ii. Which of w, f(x) and g(y, z) are free for x in φ?
iii. Which of w, f(x) and g(y, z) are free for y in φ?
(e) What is the scope of ∃x in φ?
(f)
*
Suppose that we change φ to ∃x (P(y, z) ∧(∀x (¬Q(x, x) ∨P(x, z)))). What
is the scope of ∃x now?
5. (a) Let P be a predicate symbol with arity 3. Draw the parse tree of ψ
def
=
¬(∀x ((∃y P(x, y, z)) ∧(∀z P(x, y, z)))).
(b) Indicate the free and bound variables in that parse tree.
(c) List all variables which occur free and bound therein.
(d) Compute ψ[t/x], ψ[t/y] and ψ[t/z], where t
def
= g(f(g(y, y)), y). Is t free for x
in ψ; free for y in ψ; free for z in ψ?
6. Rename the variables for φ in Example 2.9 (page 106) such that the resulting
formula ψ has the same meaning as φ, but f(y, y) is free for x in ψ.
Exercises 2.3
1. Prove the validity of the following sequents using, among others, the rules =i
and =e. Make sure that you indicate for each application of =e what the rule
instances φ, t1 and t2 are.
(a) (y = 0) ∧(y = x) ⊢0 = x
(b) t1 = t2 ⊢(t + t2) = (t + t1)
(c) (x = 0) ∨((x + x) > 0) ⊢(y = (x + x)) →((y > 0) ∨(y = (0 + x))).
2. Recall that we use = to express the equality of elements in our models. Consider
the formula ∃x ∃y (¬(x = y) ∧(∀z ((z = x) ∨(z = y)))). Can you say, in plain
English, what this formula speciﬁes?
3. Try to write down a sentence of predicate logic which intuitively holds in a
model iﬀthe model has (respectively)
(a)
*
exactly three distinct elements
(b) at most three distinct elements
(c)
*
only ﬁnitely many distinct elements.

2.8 Exercises
161
What ‘limitation’ of predicate logic causes problems in ﬁnding such a sentence
for the last item?
4. (a) Find a (propositional) proof for φ →(q1 ∧q2) |−(φ →q1) ∧(φ →q2).
(b) Find a (predicate) proof for φ →∀x Q(x) |−∀x (φ →Q(x)), provided that
x is not free in φ.
(Hint: whenever you used ∧rules in the (propositional) proof of the previous
item, use ∀rules in the (predicate) proof.)
(c) Find a proof for ∀x (P(x) →Q(x)) |−∀x P(x) →∀x Q(x).
(Hint: try (p1 →q1) ∧(p2 →q2) |−p1 ∧p2 →q1 ∧q2 ﬁrst.)
5. Find a propositional logic sequent that corresponds to ∃x ¬φ ⊢¬∀x φ. Prove it.
6. Provide proofs for the following sequents:
(a) ∀x P(x) ⊢∀y P(y); using ∀x P(x) as a premise, your proof needs to end with
an application of ∀i which requires the formula P(y0).
(b) ∀x (P(x) →Q(x)) ⊢(∀x ¬Q(x)) →(∀x ¬P(x))
(c) ∀x (P(x) →¬Q(x)) ⊢¬(∃x (P(x) ∧Q(x))).
7. The sequents below look a bit tedious, but in proving their validity you make
sure that you really understand how to nest the proof rules:
(a)
*
∀x ∀y P(x, y) |−∀u ∀v P(u, v)
(b) ∃x ∃y F(x, y) |−∃u ∃v F(u, v)
(c)
*
∃x ∀y P(x, y) |−∀y ∃x P(x, y).
8. In this exercise, whenever you use a proof rule for quantiﬁers, you should men-
tion how its side condition (if applicable) is satisﬁed.
(a) Prove 2(b-h) of Theorem 2.13 from page 117.
(b) Prove one direction of 1(b) of Theorem 2.13: ¬∃x φ ⊢∀x ¬φ.
(c) Prove 3(a) of Theorem 2.13: (∀x φ) ∧(∀x ψ) ⊣⊢∀x (φ ∧ψ); recall that you
have to do two separate proofs.
(d) Prove both directions of 4(a) of Theorem 2.13: ∀x ∀y φ ⊣⊢∀y ∀x φ.
9. Prove the validity of the following sequents in predicate logic, where F, G, P,
and Q have arity 1, and S has arity 0 (a ‘propositional atom’):
(a)
*
∃x (S →Q(x)) |−S →∃x Q(x)
(b) S →∃x Q(x) |−∃x (S →Q(x))
(c) ∃x P(x) →S |−∀x (P(x) →S)
(d)
*
∀x P(x) →S |−∃x (P(x) →S)
(e) ∀x (P(x) ∨Q(x)) |−∀x P(x) ∨∃x Q(x)
(f) ∀x ∃y (P(x) ∨Q(y)) |−∃y ∀x (P(x) ∨Q(y))
(g) ∀x (¬P(x) ∧Q(x)) ⊢∀x (P(x) →Q(x))
(h) ∀x (P(x) ∧Q(x)) ⊢∀x (P(x) →Q(x))
(i) ∃x (¬P(x) ∧¬Q(x)) ⊢∃x (¬(P(x) ∧Q(x)))
(j) ∃x (¬P(x) ∨Q(x)) ⊢∃x (¬(P(x) ∧¬Q(x)))
(k)
*
∀x (P(x) ∧Q(x)) |−∀x P(x) ∧∀x Q(x).
(l)
*
∀x P(x) ∨∀x Q(x) |−∀x (P(x) ∨Q(x)).
(m)
*
∃x (P(x) ∧Q(x)) |−∃x P(x) ∧∃x Q(x).
(n)
*
∃x F(x) ∨∃x G(x) |−∃x (F(x) ∨G(x)).
(o) ∀x ∀y (S(y) →F(x)) |−∃yS(y) →∀x F(x).

162
2 Predicate logic
(p)
*
¬∀x ¬P(x) |−∃x P(x).
(q)
*
∀x ¬P(x) |−¬∃x P(x).
(r)
*
¬∃x P(x) |−∀x ¬P(x).
10. Just like natural deduction proofs for propositional logic, certain things that
look easy can be hard to prove for predicate logic. Typically, these involve the
¬¬e rule. The patterns are the same as in propositional logic:
(a) Proving that p ∨q |−¬(¬p ∧¬q) is valid is quite easy. Try it.
(b) Show that ∃x P(x) |−¬∀x ¬P(x) is valid.
(c) Proving that ¬(¬p ∧¬q) |−p ∨q is valid is hard; you have to try to prove
¬¬(p ∨q) ﬁrst and then use the ¬¬e rule. Do it.
(d) Re-express the sequent from the previous item such that p and q are unary
predicates and both formulas are universally quantiﬁed. Prove its validity.
11. The proofs of the sequents below combine the proof rules for equality and
quantiﬁers. We write φ ↔ψ as an abbreviation for (φ →ψ) ∧(ψ →φ). Find
proofs for
(a)
*
P(b) |−∀x (x = b →P(x))
(b) P(b), ∀x∀y (P(x) ∧P(y) →x = y) |−∀x (P(x) ↔x = b)
(c)
*
∃x ∃y (H(x, y) ∨H(y, x)), ¬∃x H(x, x) |−∃x∃y ¬(x = y)
(d) ∀x (P(x) ↔x = b) |−P(b) ∧∀x∀y (P(x) ∧P(y) →x = y).
12.
*
Prove the validity of S →∀x Q(x) |−∀x (S →Q(x)), where S has arity 0 (a
‘propositional atom’).
13. By natural deduction, show the validity of
(a)
*
∀x P(a, x, x), ∀x ∀y ∀z (P(x, y, z) →P(f(x), y, f(z)))
|−P(f(a), a, f(a))
(b)
*
∀x P(a, x, x), ∀x ∀y ∀z (P(x, y, z) →P(f(x), y, f(z)))
|−∃z P(f(a), z, f(f(a)))
(c)
*
∀y Q(b, y), ∀x ∀y (Q(x, y) →Q(s(x), s(y)))
|−∃z (Q(b, z) ∧Q(z, s(s(b))))
(d) ∀x ∀y ∀z (S(x, y) ∧S(y, z) →S(x, z)), ∀x ¬S(x, x)
⊢∀x ∀y (S(x, y) →¬S(y, x))
(e) ∀x (P(x) ∨Q(x)), ∃x ¬Q(x), ∀x (R(x) →¬P(x)) ⊢∃x ¬R(x)
(f) ∀x (P(x) →(Q(x) ∨R(x))), ¬∃x (P(x) ∧R(x)) ⊢∀x (P(x) →Q(x))
(g) ∃x ∃y (S(x, y) ∨S(y, x)) ⊢∃x ∃y S(x, y)
(h) ∃x (P(x) ∧Q(x)), ∀y (P(x) →R(x)) ⊢∃x (R(x) ∧Q(x)).
14. Translate the following argument into a sequent in predicate logic using a suit-
able set of predicate symbols:
If there are any tax payers, then all politicians are tax payers.
If there are any philanthropists, then all tax payers are philan-
thropists. So, if there are any tax-paying philanthropists, then
all politicians are philanthropists.
Now come up with a proof of that sequent in predicate logic.

2.8 Exercises
163
15. Discuss in what sense the equivalences of Theorem 2.13 (page 117) form the
basis of an algorithm which, given φ, pushes quantiﬁers to the top of the for-
mula’s parse tree. If the result is ψ, what can you say about commonalities and
diﬀerences between φ and ψ?
Exercises 2.4
1.
*
Consider the formula φ
def
= ∀x ∀y Q(g(x, y), g(y, y), z), where Q and g have arity
3 and 2, respectively. Find two models M and M′ with respective environments
l and l′ such that M ⊨l φ but M′ ̸⊨l′ φ.
2. Consider the sentence φ
def
= ∀x ∃y ∃z (P(x, y) ∧P(z, y) ∧(P(x, z) →P(z, x))).
Which of the following models satisﬁes φ?
(a) The model M consists of the set of natural numbers with P M def
= {(m, n) |
m < n}.
(b) The model M′ consists of the set of natural numbers with P M′ def
= {(m, 2 ∗
m) | m natural number}.
(c) The model M′′ consists of the set of natural numbers with P M′′ def
= {(m, n) |
m < n + 1}.
3. Let P be a predicate with two arguments. Find a model which satisﬁes the
sentence ∀x ¬P(x, x); also ﬁnd one which doesn’t.
4. Consider the sentence ∀x(∃yP(x, y) ∧(∃zP(z, x) →∀yP(x, y))). Please simu-
late the evaluation of this sentence in a model and look-up table of your choice,
focusing on how the initial look-up table l grows and shrinks like a stack when
you evaluate its subformulas according to the deﬁnition of the satisfaction
relation.
5. Let φ be the sentence ∀x ∀y ∃z (R(x, y) →R(y, z)), where R is a predicate sym-
bol of two arguments.
(a)
*
Let A
def
= {a, b, c, d} and RM def
= {(b, c), (b, b), (b, a)}. Do we have M ⊨φ? Jus-
tify your answer, whatever it is.
(b)
*
Let A′ def
= {a, b, c} and RM′ def
= {(b, c), (a, b), (c, b)}. Do we have M′ ⊨φ? Jus-
tify your answer, whatever it is.
6.
*
Consider the three sentences
φ1
def
= ∀x P(x, x)
φ2
def
= ∀x ∀y (P(x, y) →P(y, x))
φ3
def
= ∀x ∀y ∀z ((P(x, y) ∧P(y, z) →P(x, z)))
which express that the binary predicate P is reﬂexive, symmetric and transitive,
respectively. Show that none of these sentences is semantically entailed by the
other ones by choosing for each pair of sentences above a model which satisﬁes
these two, but not the third sentence – essentially, you are asked to ﬁnd three
binary relations, each satisfying just two of these properties.

164
2 Predicate logic
7. Show the semantic entailment ∀x ¬φ ⊨¬∃x φ; for that you have to take any
model which satisﬁes ∀x ¬φ and you have to reason why this model must also
satisfy ¬∃x φ. You should do this in a similar way to the examples in Sec-
tion 2.4.2.
8.
*
Show the semantic entailment ∀x P(x) ∨∀x Q(x) ⊨∀x (P(x) ∨Q(x)).
9. Let φ and ψ and η be sentences of predicate logic.
(a) If ψ is semantically entailed by φ, is it necessarily the case that ψ is not
semantically entailed by ¬φ?
(b)
*
If ψ is semantically entailed by φ ∧η, is it necessarily the case that ψ is
semantically entailed by φ and semantically entailed by η?
(c) If ψ is semantically entailed by φ or by η, is it necessarily the case that ψ
is semantically entailed by φ ∨η?
(d) Explain why ψ is semantically entailed by φ iﬀφ →ψ is valid.
10. Is ∀x (P(x) ∨Q(x)) ⊨∀x P(x) ∨∀x Q(x) a semantic entailment? Justify your
answer.
11. For each set of formulas below show that they are consistent:
(a) ∀x ¬S(x, x), ∃x P(x), ∀x ∃y S(x, y), ∀x (P(x) →∃y S(y, x))
(b)
*
∀x ¬S(x, x), ∀x ∃y S(x, y),
∀x ∀y ∀z ((S(x, y) ∧S(y, z)) →S(x, z))
(c) (∀x (P(x) ∨Q(x))) →∃y R(y), ∀x (R(x) →Q(x)), ∃y (¬Q(y) ∧P(y))
(d)
*
∃x S(x, x), ∀x ∀y (S(x, y) →(x = y)).
12. For each of the formulas of predicate logic below, either ﬁnd a model which
does not satisfy it, or prove it is valid:
(a) (∀x ∀y (S(x, y) →S(y, x))) →(∀x ¬S(x, x))
(b)
*
∃y ((∀x P(x)) →P(y))
(c) (∀x (P(x) →∃y Q(y))) →(∀x ∃y (P(x) →Q(y)))
(d) (∀x ∃y (P(x) →Q(y))) →(∀x (P(x) →∃y Q(y)))
(e) ∀x ∀y (S(x, y) →(∃z (S(x, z) ∧S(z, y))))
(f) (∀x ∀y (S(x, y) →(x = y))) →(∀z ¬S(z, z))
(g)
*
(∀x ∃y (S(x, y) ∧((S(x, y) ∧S(y, x)) →(x = y)))) →
(¬∃z ∀w (S(z, w))).
(h) ∀x ∀y ((P(x) →P(y)) ∧(P(y) →P(x)))
(i) (∀x ((P(x) →Q(x)) ∧(Q(x) →P(x)))) →((∀x P(x)) →(∀x Q(x)))
(j) ((∀x P(x)) →(∀x Q(x))) →(∀x ((P(x) →Q(x)) ∧(Q(x) →P(x))))
(k) Diﬃcult: (∀x ∃y (P(x) →Q(y))) →(∃y ∀x (P(x) →Q(y))).
Exercises 2.5
1. Assuming that our proof calculus for predicate logic is sound (see exercise 3
below), show that the validity of the following sequents cannot be proved by
ﬁnding for each sequent a model such that all formulas to the left of ⊢evaluate
to T and the sole formula to the right of ⊢evaluates to F (explain why this
guarantees the non-existence of a proof):

2.8 Exercises
165
(a) ∀x (P(x) ∨Q(x)) ⊢∀x P(x) ∨∀x Q(x)
(b)
*
∀x (P(x) →R(x)), ∀x (Q(x) →R(x)) ⊢∃x (P(x) ∧Q(x))
(c) (∀x P(x)) →L ⊢∀x (P(x) →L), where L has arity 0
(d)
*
∀x ∃y S(x, y) ⊢∃y ∀x S(x, y)
(e) ∃x P(x), ∃y Q(y) ⊢∃z (P(z) ∧Q(z)).
(f)
*
∃x (¬P(x) ∧Q(x)) ⊢∀x (P(x) →Q(x))
(g)
*
∃x (¬P(x) ∨¬Q(x)) ⊢∀x (P(x) ∨Q(x)).
2. Assuming that ⊢is sound and complete for ⊨in ﬁrst-order logic, explain in detail
why the undecidability of ⊨implies that satisﬁability, validity, and provability
are all undecidable for that logic.
3. To show the soundness of our natural deduction rules for predicate logic, it
intuitively suﬃces to show that the conclusion of a proof rule is true provided
that all its premises are true. What additional complication arises due to the
presence of variables and quantiﬁers? Can you precisely formalise the necessary
induction hypothesis for proving soundness?
Exercises 2.6
1. In Example 2.23, page 136, does M ⊨l ∃P φ hold if l satisﬁes
(a)
*
l(u) = s3 and l(v) = s1;
(b) l(u) = s1 and l(v) = s3?
Justify your answers.
2. Prove that M ⊨l ∃P ∀x∀y∀z (C1 ∧C2 ∧C3 ∧C4) holds iﬀstate l(v) is not reach-
able from state l(u) in the model M, where the Ci are the ones of (2.12) on
page 139.
3. Does Theorem 2.26 from page 138 apply or remain valid if we allow φ to contain
function symbols of any ﬁnite arity?
4.
*
In the directed graph of Figure 2.5 from page 137, how many paths are there
that witness the reachability of node s3 from s2?
5. Let P and R be predicate symbols of arity 2. Write formulas of existential second-
order logic of the form ∃P ψ that hold in all models of the form M = (A, RM)
iﬀ
(a)
*
R contains a reﬂexive and symmetric relation;
(b) R contains an equivalence relation
(c) there is an R-path that visits each node of the graph exactly once – such a
path is called Hamiltonian
(d) R can be extended to an equivalence relation: there is some equivalence
relation T with RM ⊆T
(e)
*
the relation ‘there is an R-path of length 2’ is transitive.
6.
*
Show informally that (2.16) on page 141 gives rise to Russell’s paradox: A has
to be, and cannot be, an element of A.
7. The second item in the proof of Theorem 2.28 (page 140) relies on the fact
that if a binary relation R is contained in a reﬂexive, transitive relation T of

166
2 Predicate logic
the same type, then T also contains the reﬂexive, transitive closure of R. Prove
this.
8. For the model of Example 2.23 and Figure 2.5 (page 137), determine which model
checks hold and justify your answer:
(a)
*
∃P (∀x∀y P(x, y) →¬P(y, x)) ∧(∀u∀v R(u, v) →P(v, u));
(b) ∀P (∃x∃y∃z P(x, y) ∧P(y, z) ∧¬P(x, z)) →(∀u∀v R(u, v) →P(u, v)); and
(c) ∀P (∀x ¬P(x, x)) ∨(∀u∀v R(u, v) →P(u, v)).
9. Express the following statements about a binary relation R in predicate
logic, universal second-order logic, or existential second-order logic – if at all
possible:
(a) All symmetric, transitive relations either don’t contain R or are equivalence
relations.
(b)
*
All nodes are on at least one R-cycle.
(c) There is a smallest relation containing R which is symmetric.
(d) There is a smallest relation containing R which is reﬂexive.
(e)
*
The relation R is a maximal equivalence relation: R is an equivalence relation;
and there is no relation contained in R that is an equivalence relation.
Exercises 2.7
1. (a)
*
Explain why the model of Figure 2.11 (page 148) is a counterexample to
OfLovers in the presence of the fact NoSelfLove.
(b) Can you identify the set {a, b, c} from Example 2.19 (page 128) with the
model of Figure 2.11 such that these two models are structurally the same?
Justify your answer.
(c)
*
Explain informally why no model with less than three elements can sat-
isfy (2.8) from page 128 and the fact NoSelfLove.
2. Use the following fragment of an Alloy module
module AboutGraphs
sig Element {}
sig Graph {
nodes : set Element,
edges : nodes -> nodes
}
for these modelling tasks:
(a) Recall Exercise 6 from page 163 and its three sentences, where P(x, y) spec-
iﬁes that there is an edge from x to y. For each sentence, write a consistency
check that attempts to generate a model of a graph in which that sentence
is false, but the other two are true. Analyze it within Alloy. What it the
smallest scope, if any, in which the analyzer ﬁnds a model for this?

2.8 Exercises
167
(b)
*
(Recall that the expression # S = n speciﬁes that set S has n elements.)
Use Alloy to generate a graph with seven nodes such that each node can
reach exactly ﬁve nodes on ﬁnite paths (not necessarily the same ﬁve
nodes).
(c) A cycle of length n is a set of n nodes and a path through each of them,
beginning and ending with the same node. Generate a cycle of length 4.
3. An undirected graph has a set of nodes and a set of edges, except that every
edge connects two nodes without any sense of direction.
(a) Adjust the Alloy module from the previous item – e.g. by adding an appro-
priate fact – to ‘simulate’ undirected graphs.
(b) Write some consistency and assertion checks and analyze them to boost the
conﬁdence you may have in your Alloy module of undirected graphs.
4.
*
A colorable graph consists of a set of nodes, a binary symmetric relation (the
edges) between nodes and a function that assigns to each node a color. This
function is subject to the constraint that no nodes have the same color if they
are related by an edge.
(a) Write a signature AboutColoredGraphs for this structure and these con-
straints.
(b) Write a fun-statement that generates a graph whose nodes are colored by
two colors only. Such a graph is 2-colorable.
(c) For eack k = 3, 4 write a fun-statement that generates a graph whose nodes
are colored by k colors such that all k colors are being used. Such a graph is
k-colorable.
(d) Test these three functions in a module.
(e) Try to write a fun-statement that generates a graph that is 3-colorable but
deﬁnitely not 2-colorable. What does Alloy’s model builder report? Consider
the formula obtained from that fun-statement’s body by existentially quan-
tifying that body with all its parameters. Determine whether is belongs to
predicate logic, existential or universal second-order logic.
5.
*
A Kripke model is a state machine with a non-empty set of initial states init, a
mapping prop from states to atomic properties (specifying which properties are
true at which states), a state transition relation next, and a set of ﬁnal states
final (states that don’t have a next state). With a module KripkeModel:
(a) Write a signature StateMachine and some basic facts that reﬂect this struc-
ture and these constraints.
(b) Write a fun-statement Reaches which takes a state machine as ﬁrst parame-
ter and a set of states as a second parameter such that the second parameter
denotes the ﬁrst parameter’s set of states reachable from any initial state.
Note: Given the type declaration r : T -> T, the expression *r has type T
-> T as well and denotes the reﬂexive, transitive closure of r.
(c) Write these fun-statements and check their consistency:
i. DeadlockFree(m: StateMachine), among the reachable states of m only
the final ones can deadlock;

168
2 Predicate logic
State_0
prop: Prop_1
next
State_2
prop: Prop_0
next
State_1
Figure 2.15. A snapshot of a non-deterministic state machine in which
no non-final state deadlocks and where states that satisfy the same
properties are identical.
ii. Deterministic(m: StateMachine), at all reachable states of m the state
transition relation is deterministic: each state has at most one outgoing
transition;
iii. Reachability(m: StateMachine, p: Prop), some state which has
property p can be reached in m; and
iv. Liveness(m: StateMachine, p: Prop),
no
matter
which
state
m
reaches, it can – from that state – reach a state in which p holds.
(d) i. Write an assertion Implies which says that whenever a state machine
satisﬁes Liveness for a property then it also satisﬁes Reachability for
that property.
ii. Analyze that assertion in a scope of your choice. What conclusions can you
draw from the analysis’ ﬁndings?
(e) Write an assertion Converse which states that Reachability of a property
implies its Liveness. Analyze it in a scope of 3. What do you conclude, based
on the analysis’ result?
(f) Write a fun-statement that, when analyzed, generates a statemachine with
two propositions and three states such that it satisﬁes the statement of the
sentence in the caption of Figure 2.15.
6.
*
Groups are the bread and butter of cryptography and group operations are ap-
plied in the silent background when you use PUTTY, Secure Socket Layers etc.
A group is a tuple (G, ⋆, 1), where ⋆: G × G →G is a function and 1 ∈G such
that
G1 for every x ∈G there is some y ∈G such that x ⋆y = y ⋆x = 1 (any such y
is called an inverse of x);
G2 for all x, y, z ∈G, we have x ⋆(y ⋆z) = (x ⋆y) ⋆z; and
G3 for all x ∈G, we have x ⋆1 = 1 ⋆x = x.
(a) Specify a signature for groups that realizes this functionality and its con-
straints.
(b) Write a fun-statement AGroup that generates a group with three elements.
(c) Write an assertion Inverse saying that inverse elements are unique. Check
it in the scope of 5. Report your ﬁndings. What would the small scope hy-
pothesis suggest?

2.8 Exercises
169
(d)
i. Write an assertion Commutative saying that all groups are commutative.
A group is commutative iﬀx ⋆y = y ⋆x for all its elements x and y.
ii. Check the assertion Commutative in scope 5 and report your ﬁndings.
What would the small scope hypothesis suggest?
iii. Re-check assertion Commutative in scope 6 and record how long the tool
takes to ﬁnd a solution. What lesson(s) do you learn from this?
(e) For the functions and assertions above, is it safe to restrict the scope for
groups to 1? And how does one do this in Alloy?
7. In Alloy, one can extend a signature. For example, we may declare
sig Program extends PDS {
m : components
-- initial main of PDS
}
This declares instances of Program to be of type PDS, but to also possess a
designated component named m. Observe how the occurrence of components
in m : components refers to the set of components of a program, viewed as a
PDS5. In this exercise, you are asked to modify the Alloy module of Figure 2.13
on page 154.
(a) Include a signature Program as above. Add a fact stating that all programs’
designated component has a main method; and for all programs, their set
of components is the reﬂexive, transitive closure of their relation requires
applied to the designated component m. Alloy uses *r to denote the reﬂexive,
transitive closure of relation r.
(b) Write a guided simulation that, if consistent, produces a model with three
PDSs, exactly one of them being a program. The program has four compo-
nents – including the designated m – all of which schedule services from the
remaining three components. Use Alloy’s analyzer to detemine whether your
simulation is consistent and compliant with the speciﬁcation given in this
item.
(c) Let’s say that a component of a program is garbage for that program if
no service reachable from the main service of m via requires schedules that
component. Explain whether, and if so how, the constraints of AddComponent
and RemoveComponent already enforce the presence of ‘garbage collection’ if
the instances of P and P’ are constrained to be programs.
8. Recall our discussion of existential and universal second-order logic from Sec-
tion 2.6. Then study the structure of the fun-statements and assertions in Fig-
ure 2.13 on page 154. As you may know, Alloy analyzes such statements by de-
riving from them a formula for which it tries to ﬁnd a model within the speciﬁed
scope: the negation of the body of an assertion; or the body of a fun-statement,
existentially quantiﬁed with all its parameters. For each of these derived formulas,
5 In most object-oriented languages, e.g. Java, extends creates a new type. In Alloy 2.0 and 2.1, it
creates a subset of a type and not a new type as such, where the subset has additional structure
and may need to satisfy additional constraints.

170
2 Predicate logic
determine whether they can be expressed in ﬁrst-order logic, existential second-
order logic or universal second-order logic.
9. Recalling the comment on page 142 that Alloy combines model checking M ⊨φ
and validity checking Γ ⊨φ, can you discuss to what extent this is so?
2.9 Bibliographic notes
Many design decisions have been taken in the development of predicate
logic in the form known today. The Greeks and the medievals had systems
in which many of the examples and exercises in this book could be rep-
resented, but nothing that we would recognise as predicate logic emerged
until the work of Gottlob Frege in 1879, printed in [Fre03]. An account of
the contributions of the many other people involved in the development of
logic can be found in the ﬁrst few pages of W. Hodges’ chapter in [Hod83].
There are many books covering classical logic and its use in computer sci-
ence; we give a few incomplete pointers to the literature. The books [SA91],
[vD89] and [Gal87] cover more theoretical applications than those in this
book, including type theory, logic programming, algebraic speciﬁcation and
term-rewriting systems. An approach focusing on automatic theorem prov-
ing is taken by [Fit96]. Books which study the mathematical aspects of
predicate logic in greater detail, such as completeness of the proof systems
and incompleteness of ﬁrst-order arithmetic, include [Ham78] and [Hod83].
Most of these books present other proof systems besides natural deduction
such as axiomatic systems and tableau systems. Although natural deduction
has the advantages of elegance and simplicity over axiomatic methods, there
are few expositions of it in logic books aimed at a computer science audi-
ence. One exception to this is the book [BEKV94], which is the ﬁrst one to
present the rules for quantiﬁers in the form we used here. A natural deduc-
tion theorem prover called Jape has been developed, in which one can vary
the set of available rules and specify new ones6.
A standard reference for computability theory is [BJ80]. A proof for the
undecidability of the Post correspondence problem can be found in the text
book [Tay98]. The second instance of a Post correspondence problem is taken
from [Sch92]. A text on the fundamentals of databases systems is [EN94].
The discussion of Section 2.6 is largely based on the text [Pap94] which
we highly recommend if you mean to ﬁnd out more about the intimate
connections between logic and computational complexity.
6 www.comlab.ox.ac.uk/oucl/users/bernard.sufrin/jape.html

2.9 Bibliographic notes
171
The source code of all complete Alloy modules from this chapter (work-
ing under Alloy 2.0 and 2.1) as well as source code compliant with Alloy
3.0 are available under ‘ancillary material’ at the book’s website. The PDS
model grew out of a coursework set in the Fall 2002 for C475 Software En-
gineering Environments, co-taught by Susan Eisenbach and the ﬁrst author;
a published model customized for the .NET global assembly cache will
appeared in [EJC03]. The modelling language Alloy and its constraint
analyzer [JSS01] have been developed by D. Jackson and his Software
Design Group at the Laboratory for Computer Science at the Massachusetts
Institute of Technology. The tool has a dedicated repository website at
alloy.mit.edu.
More information on typed higher-order logics and their use in the
modelling and verifying of programming frameworks can be found on F.
Pfenning’s course homepage7 on Computation and Deduction.
7 www-2.cs.cmu.edu/~fp/courses/comp-ded/

3
Verification by model checking
3.1 Motivation for verification
There is a great advantage in being able to verify the correctness of computer
systems, whether they are hardware, software, or a combination. This is most
obvious in the case of safety-critical systems, but also applies to those that
are commercially critical, such as mass-produced chips, mission critical, etc.
Formal veriﬁcation methods have quite recently become usable by industry
and there is a growing demand for professionals able to apply them. In this
chapter, and the next one, we examine two applications of logics to the
question of verifying the correctness of computer systems, or programs.
Formal veriﬁcation techniques can be thought of as comprising three
parts:
r a framework for modelling systems, typically a description language of some sort;
r a speciﬁcation language for describing the properties to be veriﬁed;
r a veriﬁcation method to establish whether the description of a system satisﬁes
the speciﬁcation.
Approaches to veriﬁcation can be classiﬁed according to the following
criteria:
Proof-based vs. model-based. In a proof-based approach, the system
description is a set of formulas Γ (in a suitable logic) and the speciﬁcation
is another formula φ. The veriﬁcation method consists of trying to ﬁnd
a proof that Γ |−φ. This typically requires guidance and expertise from
the user.
In a model-based approach, the system is represented by a model M for
an appropriate logic. The speciﬁcation is again represented by a formula
φ and the veriﬁcation method consists of computing whether a model
M satisﬁes φ (written M ⊨φ). This computation is usually automatic
for ﬁnite models.
172

3.1 Motivation for verification
173
In Chapters 1 and 2, we could see that logical proof systems are often
sound and complete, meaning that Γ |−φ (provability) holds if, and only
if, Γ ⊨φ (semantic entailment) holds, where the latter is deﬁned as fol-
lows: for all models M, if for all ψ ∈Γ we have M ⊨ψ, then M ⊨φ.
Thus, we see that the model-based approach is potentially simpler than
the proof-based approach, for it is based on a single model M rather
than a possibly inﬁnite class of them.
Degree of automation. Approaches
diﬀer
on
how
automatic
the
method is; the extremes are fully automatic and fully manual. Many
of the computer-assisted techniques are somewhere in the middle.
Full- vs. property-veriﬁcation. The speciﬁcation may describe a sin-
gle property of the system, or it may describe its full behaviour. The
latter is typically expensive to verify.
Intended domain of application, which may be hardware or software;
sequential or concurrent; reactive or terminating; etc. A reactive system
is one which reacts to its environment and is not meant to terminate
(e.g., operating systems, embedded systems and computer hardware).
Pre- vs. post-development. Veriﬁcation is of greater advantage if in-
troduced early in the course of system development, because errors
caught earlier in the production cycle are less costly to rectify. (It is
alleged that Intel lost millions of dollars by releasing their Pentium chip
with the FDIV error.)
This chapter concerns a veriﬁcation method called model checking. In
terms of the above classiﬁcation, model checking is an automatic, model-
based, property-veriﬁcation approach. It is intended to be used for concur-
rent, reactive systems and originated as a post-development methodology.
Concurrency bugs are among the most diﬃcult to ﬁnd by testing (the activ-
ity of running several simulations of important scenarios), since they tend to
be non-reproducible or not covered by test cases, so it is well worth having
a veriﬁcation technique that can help one to ﬁnd them.
The Alloy system described in Chapter 2 is also an automatic, model-
based, property-veriﬁcation approach. The way models are used is slightly
diﬀerent, however. Alloy ﬁnds models which form counterexamples to asser-
tions made by the user. Model checking starts with a model described by
the user, and discovers whether hypotheses asserted by the user are valid
on the model. If they are not, it can produce counterexamples, consisting of
execution traces. Another diﬀerence between Alloy and model checking is
that model checking (unlike Alloy) focuses explicitly on temporal properties
and the temporal evolution of systems.

174
3 Verification by model checking
By contrast, Chapter 4 describes a very diﬀerent veriﬁcation technique
which in terms of the above classiﬁcation is a proof-based, computer-assisted,
property-veriﬁcation approach. It is intended to be used for programs which
we expect to terminate and produce a result.
Model checking is based on temporal logic. The idea of temporal logic is
that a formula is not statically true or false in a model, as it is in propo-
sitional and predicate logic. Instead, the models of temporal logic contain
several states and a formula can be true in some states and false in others.
Thus, the static notion of truth is replaced by a dynamic one, in which the
formulas may change their truth values as the system evolves from state
to state. In model checking, the models M are transition systems and the
properties φ are formulas in temporal logic. To verify that a system satisﬁes
a property, we must do three things:
r model the system using the description language of a model checker, arriving at
a model M;
r code the property using the speciﬁcation language of the model checker, resulting
in a temporal logic formula φ;
r Run the model checker with inputs M and φ.
The model checker outputs the answer ‘yes’ if M ⊨φ and ‘no’ otherwise; in
the latter case, most model checkers also produce a trace of system behaviour
which causes this failure. This automatic generation of such ‘counter traces’
is an important tool in the design and debugging of systems.
Since model checking is a model-based approach, in terms of the classiﬁca-
tion given earlier, it follows that in this chapter, unlike in the previous two,
we will not be concerned with semantic entailment (Γ ⊨φ), or with proof
theory (Γ ⊢φ), such as the development of a natural deduction calculus for
temporal logic. We will work solely with the notion of satisfaction, i.e. the
satisfaction relation between a model and a formula (M ⊨φ).
There is a whole zoo of temporal logics that people have proposed and
used for various things. The abundance of such formalisms may be organised
by classifying them according to their particular view of ‘time.’ Linear-
time logics think of time as a set of paths, where a path is a sequence of
time instances. Branching-time logics represent time as a tree, rooted at the
present moment and branching out into the future. Branching time appears
to make the non-deterministic nature of the future more explicit. Another
quality of time is whether we think of it as being continuous or discrete.
The former would be suggested if we study an analogue computer, the latter
might be preferred for a synchronous network.

3.2 Linear-time temporal logic
175
Temporal logics have a dynamic aspect to them, since the truth of a
formula is not ﬁxed in a model, as it is in predicate or propositional logic,
but depends on the time-point inside the model. In this chapter, we study
a logic where time is linear, called Linear-time Temporal Logic (LTL), and
another where time is branching, namely Computation Tree Logic (CTL).
These logics have proven to be extremely fruitful in verifying hardware and
communication protocols; and people are beginning to apply them to the
veriﬁcation of software. Model checking is the process of computing an answer
to the question of whether M, s ⊨φ holds, where φ is a formula of one of
these logics, M is an appropriate model of the system under consideration,
s is a state of that model and ⊨is the underlying satisfaction relation.
Models like M should not be confused with an actual physical system.
Models are abstractions that omit lots of real features of a physical system,
which are irrelevant to the checking of φ. This is similar to the abstractions
that one does in calculus or mechanics. There we talk about straight lines,
perfect circles, or an experiment without friction. These abstractions are
very powerful, for they allow us to focus on the essentials of our particular
concern.
3.2 Linear-time temporal logic
Linear-time temporal logic, or LTL for short, is a temporal logic, with con-
nectives that allow us to refer to the future. It models time as a sequence of
states, extending inﬁnitely into the future. This sequence of states is some-
times called a computation path, or simply a path. In general, the future is
not determined, so we consider several paths, representing diﬀerent possible
futures, any one of which might be the ‘actual’ path that is realised.
We work with a ﬁxed set Atoms of atomic formulas (such as p, q, r, . . . , or
p1, p2, . . . ). These atoms stand for atomic facts which may hold of a system,
like ‘Printer Q5 is busy,’ or ‘Process 3259 is suspended,’ or ‘The content of
register R1 is the integer value 6.’ The choice of atomic descriptions obvi-
ously depends on our particular interest in a system at hand.
3.2.1 Syntax of LTL
Deﬁnition 3.1 Linear-time temporal logic (LTL) has the following syntax
given in Backus Naur form:
φ ::= ⊤| ⊥| p | (¬φ) | (φ ∧φ) | (φ ∨φ) | (φ →φ)
| (X φ) | (F φ) | (G φ) | (φ U φ) | (φ W φ) | (φ R φ)
(3.1)
where p is any propositional atom from some set Atoms.

176
3 Verification by model checking
→
r
∨
F
p
G
q
¬
U
p
Figure 3.1. The parse tree of (F (p →G r) ∨((¬q) U p)).
Thus, the symbols ⊤and ⊥are LTL formulas, as are all atoms from Atoms;
and ¬φ is an LTL formula if φ is one, etc. The connectives X, F, G, U, R,
and W are called temporal connectives. X means ‘neXt state,’ F means ‘some
Future state,’ and G means ‘all future states (Globally).’ The next three, U,
R and W are called ‘Until,’ ‘Release’ and ‘Weak-until’ respectively. We will
look at the precise meaning of all these connectives in the next section; for
now, we concentrate on their syntax.
Here are some examples of LTL formulas:
r (((F p) ∧(G q)) →(p W r))
r (F (p →(G r)) ∨((¬q) U p)), the parse tree of this formula is illustrated in
Figure 3.1.
r (p W (q W r))
r ((G (F p)) →(F (q ∨s))).
It’s boring to write all those brackets, and makes the formulas hard to read.
Many of them can be omitted without introducing ambiguities; for example,
(p →(F q)) could be written p →F q without ambiguity. Others, however,
are required to resolve ambiguities. In order to omit some of those, we assume
similar binding priorities for the LTL connectives to those we assumed for
propositional and predicate logic.

3.2 Linear-time temporal logic
177
q
p
G
r
F
→
∨
p
U
¬
Figure 3.2. The parse tree of F p →G r ∨¬q U p, assuming binding pri-
orities of Convention 3.2.
Convention 3.2 The unary connectives (consisting of ¬ and the temporal
connectives X, F and G) bind most tightly. Next in the order come U, R
and W; then come ∧and ∨; and after that comes →.
These binding priorities allow us to drop some brackets without introduc-
ing ambiguity. The examples above can be written:
r F p ∧G q →p W r
r F (p →G r) ∨¬q U p
r p W (q W r)
r G F p →F (q ∨s).
The brackets we retained were in order to override the priorities of Conven-
tion 3.2, or to disambiguate cases which the convention does not resolve.
For example, with no brackets at all, the second formula would become
F p →G r ∨¬q U p, corresponding to the parse tree of Figure 3.2, which is
quite diﬀerent.
The following are not well-formed formulas:
r U r – since U is binary, not unary
r p G q – since G is unary, not binary.

178
3 Verification by model checking
Deﬁnition 3.3 A subformula of an LTL formula φ is any formula ψ whose
parse tree is a subtree of φ’s parse tree.
The subformulas of p W (q U r), e.g., are p, q, r, q U r and p W (q U r).
3.2.2 Semantics of LTL
The kinds of systems we are interested in verifying using LTL may be
modelled as transition systems. A transition system models a system by
means of states (static structure) and transitions (dynamic structure). More
formally:
Deﬁnition 3.4 A transition system M = (S, →, L) is a set of states S
endowed with a transition relation
→(a binary relation on S), such
that every s ∈S has some s′ ∈S with s →s′, and a labelling function
L: S →P(Atoms).
Transition systems are also simply called models in this chapter. So a model
has a collection of states S, a relation →, saying how the system can move
from state to state, and, associated with each state s, one has the set of
atomic propositions L(s) which are true at that particular state. We write
P(Atoms) for the power set of Atoms, a collection of atomic descriptions.
For example, the power set of {p, q} is {∅, {p}, {q}, {p, q}}. A good way of
thinking about L is that it is just an assignment of truth values to all the
propositional atoms, as it was the case for propositional logic (we called
that a valuation). The diﬀerence now is that we have more than one state,
so this assignment depends on which state s the system is in: L(s) contains
all atoms which are true in state s.
We may conveniently express all the information about a (ﬁnite) tran-
sition system M using directed graphs whose nodes (which we call states)
contain all propositional atoms that are true in that state. For example, if
our system has only three states s0, s1 and s2; if the only possible transi-
tions between states are s0 →s1, s0 →s2, s1 →s0, s1 →s2 and s2 →s2;
and if L(s0) = {p, q}, L(s1) = {q, r} and L(s2) = {r}, then we can condense
all this information into Figure 3.3. We prefer to present models by means
of such pictures whenever that is feasible.
The requirement in Deﬁnition 3.4 that for every s ∈S there is at least
one s′ ∈S such that s →s′ means that no state of the system can ‘dead-
lock.’ This is a technical convenience, and in fact it does not represent any
real restriction on the systems we can model. If a system did deadlock, we
could always add an extra state sd representing deadlock, together with new

3.2 Linear-time temporal logic
179
s0
p, q
s1
q, r
s2
r
Figure 3.3. A concise representation of a transition system M =
(S, →,L) as a directed graph. We label state s with l iff l ∈L(s).
s1
s3
s0
s2
s4
s1
s3
s0
s2
s4
sd
Figure 3.4. On the left, we have a system with a state s4 that does not
have any further transitions. On the right, we expand that system with a
‘deadlock’ state sd such that no state can deadlock; of course, it is then
our understanding that reaching the ‘deadlock’ state sd corresponds to
deadlock in the original system.
transitions s →sd for each s which was a deadlock in the old system, as well
as sd →sd. See Figure 3.4 for such an example.
Deﬁnition 3.5 A path in a model M = (S, →, L) is an inﬁnite sequence of
states s1, s2, s3, . . . in S such that, for each i ≥1, si →si+1. We write the
path as s1 →s2 →. . . .
Consider the path π = s1 →s2 →. . . . It represents a possible future of
our system: ﬁrst it is in state s1, then it is in state s2, and so on. We write
πi for the suﬃx starting at si, e.g., π3 is s3 →s4 →. . . .

180
3 Verification by model checking
p, q
r
r
r
q, r
p, q
q, r
s0
s2
s2
s2
s0
s1
s1
r
s2
r
s2
Figure 3.5. Unwinding the system of Figure 3.3 as an infinite tree of
all computation paths beginning in a particular state.
It is useful to visualise all possible computation paths from a given state
s by unwinding the transition system to obtain an inﬁnite computation tree.
For example, if we unwind the state graph of Figure 3.3 for the designated
starting state s0, then we get the inﬁnite tree in Figure 3.5. The execu-
tion paths of a model M are explicitly represented in the tree obtained by
unwinding the model.
Deﬁnition 3.6 Let M = (S, →, L) be a model and π = s1 →. . . be a path
in M. Whether π satisﬁes an LTL formula is deﬁned by the satisfaction
relation ⊨as follows:
1.
π ⊨⊤
2.
π ̸⊨⊥
3.
π ⊨p iﬀp ∈L(s1)
4.
π ⊨¬φ iﬀπ ̸⊨φ
5.
π ⊨φ1 ∧φ2 iﬀπ ⊨φ1 and π ⊨φ2
6.
π ⊨φ1 ∨φ2 iﬀπ ⊨φ1 or π ⊨φ2
7.
π ⊨φ1 →φ2 iﬀπ ⊨φ2 whenever π ⊨φ1
8.
π ⊨X φ iﬀπ2 ⊨φ
9.
π ⊨G φ iﬀ, for all i ≥1, πi ⊨φ

3.2 Linear-time temporal logic
181
s0
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10



p
q
. . .
Figure 3.6. An illustration of the meaning of Until in the semantics of
LTL. Suppose p is satisfied at (and only at) s3, s4, s5, s6, s7, s8 and q is
satisfied at (and only at) s9. Only the states s3 to s9 each satisfy p U q
along the path shown.
10.
π ⊨F φ iﬀthere is some i ≥1 such that πi ⊨φ
11.
π ⊨φ U ψ iﬀthere is some i ≥1 such that πi ⊨ψ and for all j = 1, . . . , i −1
we have πj ⊨φ
12.
π ⊨φ W ψ iﬀeither there is some i ≥1 such that πi ⊨ψ and for all j =
1, . . . , i −1 we have πj ⊨φ; or for all k ≥1 we have πk ⊨φ
13.
π ⊨φ R ψ iﬀeither there is some i ≥1 such that πi ⊨φ and for all j = 1, . . . , i
we have πj ⊨ψ, or for all k ≥1 we have πk ⊨ψ.
Clauses 1 and 2 reﬂect the facts that ⊤is always true, and ⊥is always false.
Clauses 3–7 are similar to the corresponding clauses we saw in propositional
logic. Clause 8 removes the ﬁrst state from the path, in order to create a
path starting at the ‘next’ (second) state.
Notice that clause 3 means that atoms are evaluated in the ﬁrst state along
the path in consideration. However, that doesn’t mean that all the atoms
occuring in an LTL formula refer to the ﬁrst state of the path; if they are in
the scope of a temporal connective, e.g., in G (p →X q), then the calculation
of satisfaction involves taking suﬃces of the path in consideration, and the
atoms refer to the ﬁrst state of those suﬃces.
Let’s now look at clauses 11–13, which deal with the binary temporal
connectives. U, which stands for ‘Until,’ is the most commonly encountered
one of these. The formula φ1 U φ2 holds on a path if it is the case that φ1
holds continuously until φ2 holds. Moreover, φ1 U φ2 actually demands that
φ2 does hold in some future state. See Figure 3.6 for illustration: each of the
states s3 to s9 satisﬁes p U q along the path shown, but s0 to s2 don’t.
The other binary connectives are W, standing for ‘Weak-until,’ and R,
standing for ‘Release.’ Weak-until is just like U, except that φ W ψ does not
require that ψ is eventually satisﬁed along the path in question, which is
required by φ U ψ. Release R is the dual of U; that is, φ R ψ is equivalent to
¬(¬φ U ¬ψ). It is called ‘Release’ because clause 11 determines that ψ must
remain true up to and including the moment when φ becomes true (if there
is one); φ ‘releases’ ψ. R and W are actually quite similar; the diﬀerences
are that they swap the roles of φ and ψ, and the clause for W has an i −1

182
3 Verification by model checking
where R has i. Since they are similar, why do we need both? We don’t; they
are interdeﬁnable, as we will see later. However, it’s useful to have both. R
is useful because it is the dual of U, while W is useful because it is a weak
form of U.
Note that neither the strong version (U) or the weak version (W) of until
says anything about what happens after the until has been realised. This
is in contrast with some of the readings of ‘until’ in natural language. For
example, in the sentence ‘I smoked until I was 22’ it is not only expressed
that the person referred to continually smoked up until he or she was 22
years old, but we also would interpret such a sentence as saying that this
person gave up smoking from that point onwards. This is diﬀerent from the
semantics of until in temporal logic. We could express the sentence about
smoking by combining U with other connectives; for example, by asserting
that it was once true that s U (t ∧G ¬s), where s represents ‘I smoke’ and
t represents ‘I am 22.’
Remark 3.7 Notice that, in clauses 9–13 above, the future includes the
present. This means that, when we say ‘in all future states,’ we are including
the present state as a future state. It is a matter of convention whether we
do this, or not. As an exercise, you may consider developing a version of
LTL in which the future excludes the present. A consequence of adopting
the convention that the future shall include the present is that the formulas
G p →p, p →q U p and p →F p are true in every state of every model.
So far we have deﬁned a satisfaction relation between paths and LTL for-
mulas. However, to verify systems, we would like to say that a model as
a whole satisﬁes an LTL formula. This is deﬁned to hold whenever every
possible execution path of the model satisﬁes the formula.
Deﬁnition 3.8 Suppose M = (S, →, L) is a model, s ∈S, and φ an LTL
formula. We write M, s ⊨φ if, for every execution path π of M starting at
s, we have π ⊨φ.
If M is clear from the context, we may abbreviate M, s ⊨φ by s ⊨φ.
It should be clear that we have outlined the formal foundations of a pro-
cedure that, given φ, M and s, can check whether M, s ⊨φ holds. Later
in this chapter, we will examine algorithms which implement this calcula-
tion. Let us now look at some example checks for the system in Figures 3.3
and 3.5.
1.
M, s0 ⊨p ∧q holds since the atomic symbols p and q are contained in the node
of s0: π ⊨p ∧q for every path π beginning in s0.

3.2 Linear-time temporal logic
183
2.
M, s0 ⊨¬r holds since the atomic symbol r is not contained in node s0.
3.
M, s0 ⊨⊤holds by deﬁnition.
4.
M, s0 ⊨X r holds since all paths from s0 have either s1 or s2 as their next
state, and each of those states satisﬁes r.
5.
M, s0 ⊨X (q ∧r) does not hold since we have the rightmost computation path
s0 →s2 →s2 →s2 →. . . in Figure 3.5, whose second node s2 contains r, but
not q.
6.
M, s0 ⊨G ¬(p ∧r) holds since all computation paths beginning in s0 satisfy
G ¬(p ∧r), i.e. they satisfy ¬(p ∧r) in each state along the path. Notice that
G φ holds in a state if, and only if, φ holds in all states reachable from the
given state.
7.
For similar reasons, M, s2 ⊨G r holds (note the s2 instead of s0).
8.
For any state s of M, we have M, s ⊨F (¬q ∧r) →F G r. This says that if
any path π beginning in s gets to a state satisfying (¬q ∧r), then the path
π satisﬁes F G r. Indeed this is true, since if the path has a state satisfying
(¬q ∧r) then (since that state must be s2) the path does satisfy F G r. Notice
what F G r says about a path: eventually, you have continuously r.
9.
The formula G F p expresses that p occurs along the path in question inﬁnitely
often. Intuitively, it’s saying: no matter how far along the path you go (that’s
the G part) you will ﬁnd you still have a p in front of you (that’s the F part).
For example, the path s0 →s1 →s0 →s1 →. . . satisﬁes G F p. But the path
s0 →s2 →s2 →s2 →. . . doesn’t.
10.
In our model, if a path from s0 has inﬁnitely many ps on it then it must be the
path s0 →s1 →s0 →s1 →. . . , and in that case it also has inﬁnitely many rs
on it. So, M, s0 ⊨G F p →G F r. But it is not the case the other way around!
It is not the case that M, s0 ⊨G F r →G F p, because we can ﬁnd a path from
s0 which has inﬁnitely many rs but only one p.
3.2.3 Practical patterns of specifications
What kind of practically relevant properties can we check with formulas of
LTL? We list a few of the common patterns. Suppose atomic descriptions
include some words such as busy and requested. We may require some of
the following properties of real systems:
r It is impossible to get to a state where started holds, but ready does not hold:
G¬(started ∧¬ready)
The negation of this formula expresses that it is possible to get to such a state,
but this is only so if interpreted on paths (π ⊨φ). We cannot assert such a
possibility if interpreted on states (s ⊨φ) since we cannot express the existence
of paths; for that interpretation, the negation of the formula above asserts that
all paths will eventually get to such a state.

184
3 Verification by model checking
r For any state, if a request (of some resource) occurs, then it will eventually be
acknowledged:
G (requested →F acknowledged).
r A certain process is enabled inﬁnitely often on every computation path:
G F enabled.
r Whatever happens, a certain process will eventually be permanently deadlocked:
F G deadlock.
r If the process is enabled inﬁnitely often, then it runs inﬁnitely often.
G F enabled →G F running.
r An upwards travelling lift at the second ﬂoor does not change its direction when
it has passengers wishing to go to the ﬁfth ﬂoor:
G (floor2 ∧directionup ∧ButtonPressed5 →(directionup U floor5))
Here, our atomic descriptions are boolean expressions built from system vari-
ables, e.g., floor2.
There are some things which are not possible to say in LTL, however. One
big class of such things are statements which assert the existence of a path,
such as these ones:
r From any state it is possible to get to a restart state (i.e., there is a path from
all states to a state satisfying restart).
r The lift can remain idle on the third ﬂoor with its doors closed (i.e., from the
state in which it is on the third ﬂoor, there is a path along which it stays there).
LTL can’t express these because it cannot directly assert the existence of
paths. In Section 3.4, we look at Computation Tree Logic (CTL) which has
operators for quantifying over paths, and can express these properties.
3.2.4 Important equivalences between LTL formulas
Deﬁnition 3.9 We say that two LTL formulas φ and ψ are semantically
equivalent, or simply equivalent, writing φ ≡ψ, if for all models M and all
paths π in M: π ⊨φ iﬀπ ⊨ψ.
The equivalence of φ and ψ means that φ and ψ are semantically inter-
changeable. If φ is a subformula of some bigger formula χ, and ψ ≡φ, then
we can make the substitution of ψ for φ in χ without changing the meaning
of χ. In propositional logic, we saw that ∧and ∨are duals of each other,
meaning that if you push a ¬ past a ∧, it becomes a ∨, and vice versa:
¬(φ ∧ψ) ≡¬φ ∨¬ψ
¬(φ ∨ψ) ≡¬φ ∧¬ψ.
(Because ∧and ∨are binary, pushing a negation downwards in the parse
tree past one of them also has the eﬀect of duplicating that negation.)

3.2 Linear-time temporal logic
185
Similarly, F and G are duals of each other, and X is dual with itself:
¬G φ ≡F ¬φ
¬F φ ≡G ¬φ
¬X φ ≡X ¬φ.
Also U and R are duals of each other:
¬(φ U ψ) ≡¬φ R ¬ψ
¬(φ R ψ) ≡¬φ U ¬ψ.
We should give formal proofs of these equivalences. But they are easy, so we
leave them as an exercise to the reader. ‘Morally’ there ought to be a dual
for W, and you can invent one if you like. Work out what it might mean,
and then pick a symbol based on the ﬁrst letter of the meaning. However, it
might not be very useful.
It’s also the case that F distributes over ∨and G over ∧, i.e.,
F (φ ∨ψ) ≡F φ ∨F ψ
G (φ ∧ψ) ≡G φ ∧G ψ.
Compare this with the quantiﬁer equivalences in Section 2.3.2. But F does
not distribute over ∧. What this means is that there is a model with a
path which distinguishes F (φ ∧ψ) and F φ ∧F ψ, for some φ, ψ. Take the
path s0 →s1 →s0 →s1 →. . . from the system of Figure 3.3, for example;
it satisﬁes F p ∧F r but it doesn’t satisfy F (p ∧r).
Here are two more equivalences in LTL:
F φ ≡⊤U φ
G φ ≡⊥R φ.
The ﬁrst one exploits the fact that the clause for Until states two things:
the second formula φ must become true; and until then, the ﬁrst formula ⊤
must hold. So, if we put ‘no constraint’ for the ﬁrst formula, it boils down
to asking that the second formula holds, which is what F asks. (The formula
⊤represent ‘no constraint.’ If you ask me to bring it about that ⊤holds,
I need do nothing, it enforces no constraint. In the same sense, ⊥is ‘every
constraint.’ If you ask me to bring it about that ⊥holds, I’ll have to meet
every constraint there is, which is impossible.)
The second formula, that G φ ≡⊥R φ, can be obtained from the ﬁrst by
putting a ¬ in front of each side, and applying the duality rules. Another
more intuitive way of seeing this is to recall the meaning of ‘release:’ ⊥
releases φ, but ⊥will never be true, so φ doesn’t get released.
Another pair of equivalences relates the strong and weak versions of Until,
U and W. Strong until may be seen as weak until plus the constraint that
the eventuality must actually occur:
φ U ψ ≡φ W ψ ∧F ψ .
(3.2)

186
3 Verification by model checking
To prove equivalence (3.2), suppose ﬁrst that a path satisﬁes φ U ψ. Then,
from clause 11, we have i ≥1 such that πi ⊨ψ and for all j = 1, . . . , i −1
we have πj ⊨φ. From clause 12, this proves φ W ψ, and from clause 10 it
proves F ψ. Thus for all paths π, if π ⊨φ U ψ then π ⊨φ W ψ ∧F ψ. As an
exercise, the reader can prove it the other way around.
Writing W in terms of U is also possible: W is like U but also allows the
possibility of the eventuality never occurring:
φ W ψ ≡φ U ψ ∨G φ.
(3.3)
Inspection of clauses 12 and 13 reveals that R and W are rather similar. The
diﬀerences are that they swap the roles of their arguments φ and ψ; and the
clause for W has an i −1 where R has i. Therefore, it is not surprising that
they are expressible in terms of each other, as follows:
φ W ψ ≡ψ R (φ ∨ψ)
(3.4)
φ R ψ ≡ψ W (φ ∧ψ).
(3.5)
3.2.5 Adequate sets of connectives for LTL
Recall that φ ≡ψ holds iﬀany path in any transition system which sat-
isﬁes φ also satisﬁes ψ, and vice versa. As in propositional logic, there is
some redundancy among the connectives. For example, in Chapter 1 we saw
that the set {⊥, ∧, ¬} forms an adequate set of connectives, since the other
connectives ∨, →, ⊤, etc., can be written in terms of those three.
Small adequate sets of connectives also exist in LTL. Here is a summary
of the situation.
r X is completely orthogonal to the other connectives. That is to say, its presence
doesn’t help in deﬁning any of the other ones in terms of each other. Moreover,
X cannot be derived from any combination of the others.
r Each of the sets {U, X}, {R, X}, {W, X} is adequate. To see this, we note that
– R and W may be deﬁned from U, by the duality φ R ψ ≡¬(¬φ U ¬ψ) and
equivalence (3.4) followed by the duality, respectively.
– U and W may be deﬁned from R, by the duality φ U ψ ≡¬(¬φ R ¬ψ) and
equivalence (3.4), respectively.
– R and U may be deﬁned from W, by equivalence (3.5) and the duality φ U
ψ ≡¬(¬φ R ¬ψ) followed by equivalence (3.5).
Sometimes it is useful to look at adequate sets of connectives which do not
rely on the availability of negation. That’s because it is often convenient to
assume formulas are written in negation-normal form, where all the negation
symbols are applied to propositional atoms (i.e., they are near the leaves

3.3 Model checking: systems, tools, properties
187
of the parse tree). In this case, these sets are adequate for the fragment
without X, and no strict subset is: {U, R}, {U, W}, {U, G}, {R, F}, {W, F}.
But {R, G} and {W, G} are not adequate. Note that one cannot deﬁne G
with {U,F}, and one cannot deﬁne F with {R,G} or {W,G}.
We ﬁnally state and prove a useful equivalence about U.
Theorem 3.10 The equivalence φ U ψ ≡¬(¬ψ U (¬φ ∧¬ψ)) ∧F ψ holds
for all LTL formulas φ and ψ.
PROOF: Take any path s0 →s1 →s2 →. . . in any model.
First, suppose s0 ⊨φ U ψ holds. Let n be the smallest number such that
sn ⊨ψ; such a number has to exist since s0 ⊨φ U ψ; then, for each k < n,
sk ⊨φ. We immediately have s0 ⊨F ψ, so it remains to show s0 ⊨¬(¬ψ U
(¬φ ∧¬ψ)), which, if we expand, means:
(∗) for each i > 0, if si ⊨¬φ ∧¬ψ, then there is some j < i with sj ⊨ψ.
Take any i > 0 with si ⊨¬φ ∧¬ψ; i > n, so we can take j
def
= n and have
sj ⊨ψ.
Conversely, suppose s0 ⊨¬(¬ψ U (¬φ ∧¬ψ)) ∧F ψ holds; we prove s0 ⊨φ U
ψ. Since s0 ⊨F ψ, we have a minimal n as before. We show that, for any
i < n, si ⊨φ. Suppose si ⊨¬φ; since n is minimal, we know si ⊨¬ψ, so
by (∗) there is some j < i < n with sj ⊨ψ, contradicting the minimality
of n.
2
3.3 Model checking: systems, tools, properties
3.3.1 Example: mutual exclusion
Let us now look at a larger example of veriﬁcation using LTL, having to do
with mutual exclusion. When concurrent processes share a resource (such as
a ﬁle on a disk or a database entry), it may be necessary to ensure that they
do not have access to it at the same time. Several processes simultaneously
editing the same ﬁle would not be desirable.
We therefore identify certain critical sections of each process’ code and
arrange that only one process can be in its critical section at a time. The
critical section should include all the access to the shared resource (though it
should be as small as possible so that no unnecessary exclusion takes place).
The problem we are faced with is to ﬁnd a protocol for determining which
process is allowed to enter its critical section at which time. Once we have
found one which we think works, we verify our solution by checking that it
has some expected properties, such as the following ones:
Safety: Only one process is in its critical section at any time.

188
3 Verification by model checking
n1n2
t1n2
c1n2
t1t2
n1t2
n1c2
t1c2
c1t2
s0
s1
s2
s4
s3
s5
s6
s7
Figure 3.7. A first-attempt model for mutual exclusion.
This safety property is not enough, since a protocol which permanently
excluded every process from its critical section would be safe, but not very
useful. Therefore, we should also require:
Liveness: Whenever any process requests to enter its critical section, it
will eventually be permitted to do so.
Non-blocking: A process can always request to enter its critical section.
Some rather crude protocols might work on the basis that they cycle through
the processes, making each one in turn enter its critical section. Since it
might be naturally the case that some of them request access to the shared
resource more often than others, we should make sure our protocol has the
property:
No strict sequencing: Processes need not enter their critical section in
strict sequence.
The first modelling attempt
We will model two processes, each of
which is in its non-critical state (n), or trying to enter its critical state (t),
or in its critical state (c). Each individual process undergoes transitions in
the cycle n →t →c →n →. . . , but the two processes interleave with each
other. Consider the protocol given by the transition system M in Figure 3.7.
(As usual, we write p1p2 . . . pm in a node s to denote that p1, p2, . . . , pm
are the only propositional atoms true at s.) The two processes start oﬀin
their non-critical sections (global state s0). State s0 is the only initial state,
indicated by the incoming edge with no source. Either of them may now

3.3 Model checking: systems, tools, properties
189
move to its trying state, but only one of them can ever make a transition at
a time (asynchronous interleaving). At each step, an (unspeciﬁed) scheduler
determines which process may run. So there is a transition arrow from s0 to
s1 and s5. From s1 (i.e., process 1 trying, process 2 non-critical) again two
things can happen: either process 1 moves again (we go to s2), or process 2
moves (we go to s3). Notice that not every process can move in every state.
For example, process 1 cannot move in state s7, since it cannot go into its
critical section until process 2 comes out of its critical section.
We would like to check the four properties by ﬁrst describing them as
temporal logic formulas. Unfortunately, they are not all expressible as LTL
formulas. Let us look at them case-by-case.
Safety: This is expressible in LTL, as G ¬(c1 ∧c2). Clearly, G ¬(c1 ∧c2)
is satisﬁed in the initial state (indeed, in every state).
Liveness: This is also expressible: G (t1 →F c1). However, it is not sat-
isﬁed by the initial state, for we can ﬁnd a path starting at the
initial state along which there is a state, namely s1, in which t1 is
true but from there along the path c1 is false. The path in question
is s0 →s1 →s3 →s7 →s1 →s3 →s7 . . . on which c1 is always false.
Non-blocking: Let’s just consider process 1. We would like to express the
property as: for every state satisfying n1, there is a successor satisfying
t1. Unfortunately, this existence quantiﬁer on paths (‘there is a successor
satisfying. . . ’) cannot be expressed in LTL. It can be expressed in the
logic CTL, which we will turn to in the next section (for the impatient,
see page 215).
No strict sequencing: We might consider expressing this as saying: there
is a path with two distinct states satisfying c1 such that no state in
between them has that property. However, we cannot express ‘there
exists a path,’ so let us consider the complement formula instead. The
complement says that all paths having a c1 period which ends can-
not have a further c1 state until a c2 state occurs. We write this as:
G (c1 →c1 W (¬c1 ∧¬c1 W c2)). This says that anytime we get into a
c1 state, either that condition persists indeﬁnitely, or it ends with a non-
c1 state and in that case there is no further c1 state unless and until we
obtain a c2 state.
This formula is false, as exempliﬁed by the path s0 →s5 →s3 →s4 →
s5 →s3 →s4 . . . . Therefore the original condition expressing that strict
sequencing need not occur, is true.
Before further considering the mutual exclusion example, some comments
about expressing properties in LTL are appropriate. Notice that in the

190
3 Verification by model checking
no-strict-sequencing property, we overcame the problem of not being able to
express the existence of paths by instead expressing the complement prop-
erty, which of course talks about all paths. Then we can perform our check,
and simply reverse the answer; if the complement property is false, we de-
clare our property to be true, and vice versa.
Why was that tactic not available to us to express the non-blocking prop-
erty? The reason is that it says: every path to a n1 state may be continued
by a one-step path to a t1 state. The presence of both universal and exis-
tential quantiﬁers is the problem. In the no-strict-sequencing property, we
had only an existential quantiﬁer; thus, taking the complement property
turned it into a universal path quantiﬁer, which can be expressed in LTL.
But where we have alternating quantiﬁers, taking the complement property
doesn’t help in general.
Let’s go back to the mutual exclusion example. The reason liveness failed
in our ﬁrst attempt at modelling mutual exclusion is that non-determinism
means it might continually favour one process over another. The problem is
that the state s3 does not distinguish between which of the processes ﬁrst
went into its trying state. We can solve this by splitting s3 into two states.
The second modelling attempt
The two states s3 and s9 in Figure 3.8
both correspond to the state s3 in our ﬁrst modelling attempt. They both
record that the two processes are in their trying states, but in s3 it is im-
plicitly recorded that it is process 1’s turn, whereas in s9 it is process 2’s
turn. Note that states s3 and s9 both have the labelling t1t2; the deﬁnition of
transition systems does not preclude this. We can think of there being some
other, hidden, variables which are not part of the initial labelling, which
distinguish s3 and s9.
Remark 3.11 The four properties of safety, liveness, non-blocking and no-
strict-sequencing are satisﬁed by the model in Figure 3.8. (Since the non-
blocking property has not yet been written in temporal logic, we can only
check it informally.)
In this second modelling attempt, our transition system is still slightly
over-simpliﬁed, because we are assuming that it will move to a diﬀerent
state on every tick of the clock (there are no transitions to the same state).
We may wish to model that a process can stay in its critical state for several
ticks, but if we include an arrow from s4, or s7, to itself, we will again violate
liveness. This problem will be solved later in this chapter when we consider
‘fairness constraints’ (Section 3.6.2).

3.3 Model checking: systems, tools, properties
191
s0 n1n2
s1
s2
s4
s3
t1n2
c1n2
c1t2
s5
n1c2
s7
n1t2
t1c2
t1t2
t1t2
s9
s6
Figure 3.8. A second-attempt model for mutual exclusion. There are
now two states representing t1t2, namely s3 and s9.
3.3.2 The NuSMV model checker
So far, this chapter has been quite theoretical; and the sections after this
one continue in this vein. However, one of the exciting things about model
checking is that it is also a practical subject, for there are several eﬃcient
implementations which can check large systems in realistic time. In this
section, we look at the NuSMV model-checking system. NuSMV stands for
‘New Symbolic Model Veriﬁer.’ NuSMV is an Open Source product, is ac-
tively supported and has a substantial user community. For details on how
to obtain it, see the bibliographic notes at the end of the chapter.
NuSMV (sometimes called simply SMV) provides a language for describ-
ing the models we have been drawing as diagrams and it directly checks the
validity of LTL (and also CTL) formulas on those models. SMV takes as
input a text consisting of a program describing a model and some speciﬁca-
tions (temporal logic formulas). It produces as output either the word ‘true’
if the speciﬁcations hold, or a trace showing why the speciﬁcation is false
for the model represented by our program.
SMV programs consist of one or more modules. As in the programming
language C, or Java, one of the modules must be called main. Modules can
declare variables and assign to them. Assignments usually give the initial
value of a variable and its next value as an expression in terms of the current
values of variables. This expression can be non-deterministic (denoted by
several expressions in braces, or no assignment at all). Non-determinism is
used to model the environment and for abstraction.

192
3 Verification by model checking
The following input to SMV:
MODULE main
VAR
request : boolean;
status : {ready,busy};
ASSIGN
init(status) := ready;
next(status) := case
request : busy;
1 : {ready,busy};
esac;
LTLSPEC
G(request -> F status=busy)
consists of a program and a speciﬁcation. The program has two variables,
request of type boolean and status of enumeration type {ready, busy}:
0 denotes ‘false’ and 1 represents ‘true.’ The initial and subsequent values
of variable request are not determined within this program; this conserva-
tively models that these values are determined by an external environment.
This under-speciﬁcation of request implies that the value of variable status
is partially determined: initially, it is ready; and it becomes busy whenever
request is true. If request is false, the next value of status is not deter-
mined.
Note that the case 1: signiﬁes the default case, and that case statements
are evaluated from the top down: if several expressions to the left of a ‘:’ are
true, then the command corresponding to the ﬁrst, top-most true expression
will be executed. The program therefore denotes the transition system shown
in Figure 3.9; there are four states, each one corresponding to a possible value
of the two binary variables. Note that we wrote ‘busy’ as a shorthand for
‘status=busy’ and ‘req’ for ‘request is true.’
It takes a while to get used to the syntax of SMV and its meaning. Since
variable request functions as a genuine environment in this model, the
program and the transition system are non-deterministic: i.e., the ‘next
state’ is not uniquely deﬁned. Any state transition based on the behaviour
of status comes in a pair: to a successor state where request is false, or
true, respectively. For example, the state ‘¬req, busy’ has four states it can
move to (itself and three others).
LTL speciﬁcations are introduced by the keyword LTLSPEC and are sim-
ply LTL formulas. Notice that SMV uses &, |, -> and ! for ∧, ∨, →and
¬, respectively, since they are available on standard keyboards. We may

3.3 Model checking: systems, tools, properties
193
req
ready
busy
req
¬req
busy
ready
¬req
Figure 3.9. The model corresponding to the SMV program in the text.
easily verify that the speciﬁcation of our module main holds of the model in
Figure 3.9.
Modules in SMV
SMV supports breaking a system description into sev-
eral modules, to aid readability and to verify interaction properties. A mod-
ule is instantiated when a variable having that module name as its type is
declared. This deﬁnes a set of variables, one for each one declared in the
module description. In the example below, which is one of the ones dis-
tributed with SMV, a counter which repeatedly counts from 000 through to
111 is described by three single-bit counters. The module counter cell is
instantiated three times, with the names bit0, bit1 and bit2. The counter
module has one formal parameter, carry in, which is given the actual value
1 in bit0, and bit0.carry out in the instance bit1. Hence, the carry in of
module bit1 is the carry out of module bit0. Note that we use the period
‘.’ in m.v to access the variable v in module m. This notation is also used by
Alloy (see Chapter 2) and a host of programming languages to access ﬁelds
in record structures, or methods in objects. The keyword DEFINE is used
to assign the expression value & carry in to the symbol carry out (such
deﬁnitions are just a means for referring to the current value of a certain
expression).
MODULE main
VAR
bit0 : counter_cell(1);
bit1 : counter_cell(bit0.carry_out);
bit2 : counter_cell(bit1.carry_out);
LTLSPEC
G F bit2.carry_out

194
3 Verification by model checking
MODULE counter_cell(carry_in)
VAR
value : boolean;
ASSIGN
init(value) := 0;
next(value) := (value + carry_in) mod 2;
DEFINE
carry_out := value & carry_in;
The eﬀect of the DEFINE statement could have been obtained by declaring
a new variable and assigning its value thus:
VAR
carry_out : boolean;
ASSIGN
carry_out := value & carry_in;
Notice that, in this assignment, the current value of the variable is assigned.
Deﬁned symbols are usually preferable to variables, since they don’t increase
the state space by declaring new variables. However, they cannot be assigned
non-deterministically since they refer only to another expression.
Synchronous and asynchronous composition
By default, modules
in SMV are composed synchronously: this means that there is a global clock
and, each time it ticks, each of the modules executes in parallel. By use of
the process keyword, it is possible to compose the modules asynchronously.
In that case, they run at diﬀerent ‘speeds,’ interleaving arbitrarily. At each
tick of the clock, one of them is non-deterministically chosen and executed
for one cycle. Asynchronous interleaving composition is useful for describing
communication protocols, asynchronous circuits and other systems whose
actions are not synchronised to a global clock.
The bit counter above is synchronous, whereas the examples below of
mutual exclusion and the alternating bit protocol are asynchronous.
3.3.3 Running NuSMV
The normal use of NuSMV is to run it in batch mode, from a Unix shell or
command prompt in Windows. The command line
NuSMV counter3.smv

3.3 Model checking: systems, tools, properties
195
will analyse the code in the ﬁle counter3.smv and report on the speciﬁca-
tions it contains. One can also run NuSMV interactively. In that case, the
command line
NuSMV -int counter3.smv
enters NuSMV’s command-line interpreter. From there, there is a variety
of commands you can use which allow you to compile the description and
run the speciﬁcation checks, as well as inspect partial results and set various
parameters. See the NuSMV user manual for more details.
NuSMV also supports bounded model checking, invoked by the command-
line option -bmc. Bounded model checking looks for counterexamples in
order of size, starting with counterexamples of length 1, then 2, etc., up
to a given threshold (10 by default). Note that bounded model checking
is incomplete: failure to ﬁnd a counterexample does not mean that there
is none, but only that there is none of length up to the threshold. For
related reasons, this incompleteness features also in Alloy and its constraint
analyzer. Thus, while a negative answer can be relied on (if NuSMV ﬁnds a
counterexample, it is valid), a positive one cannot. References on bounded
model checking can be found in the bibliographic notes on page 254. Later
on, we use bounded model checking to prove the optimality of a scheduler.
3.3.4 Mutual exclusion revisited
Figure 3.10 gives the SMV code for a mutual exclusion protocol. This code
consists of two modules, main and prc. The module main has the variable
turn, which determines whose turn it is to enter the critical section if both
are trying to enter (recall the discussion about the states s3 and s9 in Sec-
tion 3.3.1).
The module main also has two instantiations of prc. In each of these
instantiations, st is the status of a process (saying whether it is in its critical
section, or not, or trying) and other-st is the status of the other process
(notice how this is passed as a parameter in the third and fourth lines of
main).
The value of st evolves in the way described in a previous section: when
it is n, it may stay as n or move to t. When it is t, if the other one is n, it will
go straight to c, but if the other one is t, it will check whose turn it is before
going to c. Then, when it is c, it may move back to n. Each instantiation of
prc gives the turn to the other one when it gets to its critical section.
An important feature of SMV is that we can restrict its search tree to
execution paths along which an arbitrary boolean formula about the state

196
3 Verification by model checking
MODULE main
VAR
pr1: process prc(pr2.st, turn, 0);
pr2: process prc(pr1.st, turn, 1);
turn: boolean;
ASSIGN
init(turn) := 0;
-- safety
LTLSPEC
G!((pr1.st = c) & (pr2.st = c))
-- liveness
LTLSPEC
G((pr1.st = t) -> F (pr1.st = c))
LTLSPEC
G((pr2.st = t) -> F (pr2.st = c))
-- ‘negation’ of strict sequencing (desired to be false)
LTLSPEC G(pr1.st=c -> ( G pr1.st=c | (pr1.st=c U
(!pr1.st=c & G !pr1.st=c | ((!pr1.st=c) U pr2.st=c)))))
MODULE prc(other-st, turn, myturn)
VAR
st: {n, t, c};
ASSIGN
init(st) := n;
next(st) :=
case
(st = n)
: {t,n};
(st = t) & (other-st = n)
: c;
(st = t) & (other-st = t) & (turn = myturn): c;
(st = c)
: {c,n};
1
: st;
esac;
next(turn) :=
case
turn = myturn & st = c : !turn;
1
: turn;
esac;
FAIRNESS running
FAIRNESS
!(st = c)
Figure 3.10. SMV code for mutual exclusion. Because W is not sup-
ported by SMV, we had to make use of equivalence (3.3) to write the
no-strict-sequencing formula as an equivalent but longer formula in-
volving U.

3.3 Model checking: systems, tools, properties
197
φ is true inﬁnitely often. Because this is often used to model fair access to
resources, it is called a fairness constraint and introduced by the keyword
FAIRNESS. Thus, the occurrence of FAIRNESS φ means that SMV, when
checking a speciﬁcation ψ, will ignore any path along which φ is not satisﬁed
inﬁnitely often.
In the module prc, we restrict model checks to computation paths along
which st is inﬁnitely often not equal to c. This is because our code allows
the process to stay in its critical section as long as it likes. Thus, there
is another opportunity for liveness to fail: if process 2 stays in its critical
section forever, process 1 will never be able to enter. Again, we ought not
to take this kind of violation into account, since it is patently unfair if a
process is allowed to stay in its critical section for ever. We are looking for
more subtle violations of the speciﬁcations, if there are any. To avoid the
one above, we stipulate the fairness constraint !(st=c).
If the module in question has been declared with the process keyword,
then at each time point SMV will non-deterministically decide whether or
not to select it for execution, as explained earlier. We may wish to ignore
paths in which a module is starved of processor time. The reserved word
running can be used instead of a formula in a fairness constraint: writing
FAIRNESS running restricts attention to execution paths along which the
module in which it appears is selected for execution inﬁnitely often.
In prc, we restrict ourselves to such paths, since, without this restriction,
it would be easy to violate the liveness constraint if an instance of prc were
never selected for execution. We assume the scheduler is fair; this assumption
is codiﬁed by two FAIRNESS clauses. We return to the issue of fairness, and
the question of how our model-checking algorithm copes with it, in the next
section.
Please run this program in NuSMV to see which speciﬁcations hold for
it.
The transition system corresponding to this program is shown in
Figure 3.11. Each state shows the values of the variables; for example, ct1
is the state in which process 1 and 2 are critical and trying, respectively,
and turn=1. The labels on the transitions show which process was selected
for execution. In general, each state has several transitions, some in which
process 1 moves and others in which process 2 moves.
This model is a bit diﬀerent from the previous model given for mutual
exclusion in Figure 3.8, for these two reasons:
r Because the boolean variable turn has been explicitly introduced to distinguish
between states s3 and s9 of Figure 3.8, we now distinguish between certain states

198
3 Verification by model checking
cn0
tn0
tc0
tt0
nn0
ct0
1,2
2
1
1
1
2
1,2
1
2
2
2
2
1
1
2
1
1,2
1
1,2
1,2
1
2
2
2
1
2
1
1
1
2
1
2
2
nn1
tn1
cn1
ct1
nt1
tt1
nc1
tc1
1,2
2
nc0
nt0
1,2
2
1
1,2
1,2
1
1,2
Figure 3.11. The transition system corresponding to the SMV code
in Figure 3.10. The labels on the transitions denote the process which
makes the move. The label 1, 2 means that either process could make
that move.

3.3 Model checking: systems, tools, properties
199
(for example, ct0 and ct1) which were identical before. However, these states
are not distinguished if you look just at the transitions from them. Therefore,
they satisfy the same LTL formulas which don’t mention turn. Those states are
distinguished only by the way they can arise.
r We have eliminated an over-simpliﬁcation made in the model of Figure 3.8. Recall
that we assumed the system would move to a diﬀerent state on every tick of the
clock (there were no transitions from a state to itself). In Figure 3.11, we allow
transitions from each state to itself, representing that a process was chosen for
execution and did some private computation, but did not move in or out of its
critical section. Of course, by doing this we have introduced paths in which one
process gets stuck in its critical section, whence the need to invoke a fairness
constraint to eliminate such paths.
3.3.5 The ferryman
You may recall the puzzle of a ferryman, goat, cabbage, and wolf all on one
side of a river. The ferryman can cross the river with at most one passenger
in his boat. There is a behavioural conﬂict between:
1.
the goat and the cabbage; and
2.
the goat and the wolf;
if they are on the same river bank but the ferryman crosses the river or stays
on the other bank.
Can the ferryman transport all goods to the other side, without any con-
ﬂicts occurring? This is a planning problem, but it can be solved by model
checking. We describe a transition system in which the states represent which
goods are at which side of the river. Then we ask if the goal state is reach-
able from the initial state: Is there a path from the initial state such that it
has a state along it at which all the goods are on the other side, and during
the transitions to that state the goods are never left in an unsafe, conﬂicting
situation?
We model all possible behaviour (including that which results in conﬂicts)
as a NuSMV program (Figure 3.12). The location of each agent is modelled
as a boolean variable: 0 denotes that the agent is on the initial bank, and
1 the destination bank. Thus, ferryman = 0 means that the ferryman is
on the initial bank, ferryman = 1 that he is on the destination bank, and
similarly for the variables goat, cabbage and wolf.
The variable carry takes a value indicating whether the goat, cabbage,
wolf or nothing is carried by the ferryman. The deﬁnition of next(carry)
works as follows. It is non-deterministic, but the set from which a value is
non-deterministically chosen is determined by the values of ferryman, goat,

MODULE main
VAR
ferryman : boolean;
goat
: boolean;
cabbage
: boolean;
wolf
: boolean;
carry
: {g,c,w,0};
ASSIGN
init(ferryman) := 0; init(goat)
:= 0;
init(cabbage)
:= 0; init(wolf)
:= 0;
init(carry)
:= 0;
next(ferryman) := 0,1;
next(carry) := case
ferryman=goat : g;
1
: 0;
esac union
case
ferryman=cabbage : c;
1
: 0;
esac union
case
ferryman=wolf : w;
1
: 0;
esac union 0;
next(goat) := case
ferryman=goat
& next(carry)=g : next(ferryman);
1
: goat;
esac;
next(cabbage) := case
ferryman=cabbage & next(carry)=c : next(ferryman);
1
: cabbage;
esac;
next(wolf) := case
ferryman=wolf & next(carry)=w : next(ferryman);
1
: wolf;
esac;
LTLSPEC !((
(goat=cabbage | goat=wolf) -> goat=ferryman)
U (cabbage & goat & wolf & ferryman))
Figure 3.12. NuSMV code for the ferryman planning problem.

3.3 Model checking: systems, tools, properties
201
etc., and always includes 0. If ferryman = goat (i.e., they are on the same
side) then g is a member of the set from which next(carry) is chosen. The
situation for cabbage and wolf is similar. Thus, if ferryman = goat = wolf ̸=
cabbage then that set is {g, w, 0}. The next value assigned to ferryman is
non-deterministic: he can choose to cross or not to cross the river. But the
next values of goat, cabbage and wolf are deterministic, since whether they
are carried or not is determined by the ferryman’s choice, represented by the
non-deterministic assignment to carry; these values follow the same pattern.
Note how the boolean guards refer to state bits at the next state. The
SMV compiler does a dependency analysis and rejects circular dependencies
on next values. (The dependency analysis is rather pessimistic: sometimes
NuSMV complains of circularity even in situations when it could be resolved.
The original CMU-SMV is more liberal in this respect.)
Running NuSMV
We seek a path satisfying φ U ψ, where ψ asserts the
ﬁnal goal state, and φ expresses the safety condition (if the goat is with
the cabbage or the wolf, then the ferryman is there, too, to prevent any
untoward behaviour). Thus, we assert that all paths satisfy ¬(φ U ψ), i.e.,
no path satisﬁes φ U ψ. We hope this is not the case, and NuSMV will give
us an example path which does satisfy φ U ψ. Indeed, running NuSMV gives
us the path of Figure 3.13, which represents a solution to the puzzle.
The beginning of the generated path represents the usual solution to this
puzzle: the ferryman takes the goat ﬁrst, then goes back for the cabbage. To
avoid leaving the goat and the cabbage together, he takes the goat back, and
picks up the wolf. Now the wolf and the cabbage are on the destination side,
and he goes back again to get the goat. This brings us to State 1.9, where
the ferryman appears to take a well-earned break. But the path continues.
States 1.10 to 1.15 show that he takes his charges back to the original side
of the bank; ﬁrst the cabbage, then the wolf, then the goat. Unfortunately
it appears that the ferryman’s clever plan up to state 1.9 is now spoiled,
because the goat meets an unhappy end in state 1.11.
What went wrong? Nothing, actually. NuSMV has given us an inﬁnite
path, which loops around the 15 illustrated states. Along the inﬁnite path,
the ferryman repeatedly takes his goods across (safely), and then back again
(unsafely). This path does indeed satisfy the speciﬁcation φ U ψ, which as-
serts the safety of the forward journey but says nothing about what happens
after that. In other words, the path is correct; it satisﬁes φ U ψ (with ψ oc-
curring at state 8). What happens along the path after that has no bearing
on φ U ψ.

202
3 Verification by model checking
acws-0116% nusmv
ferryman.smv
*** This is NuSMV 2.1.2 (compiled 2002-11-22 12:00:00)
*** For more information of NuSMV see <http://nusmv.irst.itc.it>
*** or email to <nusmv-users@irst.itc.it>.
*** Please report bugs to <nusmv-users@irst.itc.it>.
-- specification !(((goat = cabbage | goat = wolf) -> goat = ferryman)
U (((cabbage & goat) & wolf) & ferryman)) is false
-- as demonstrated by the following execution sequence
-- loop starts here --
-> State 1.1 <-
ferryman = 0
-> State 1.8 <-
goat = 0
ferryman = 1
cabbage = 0
goat = 1
wolf = 0
carry = g
carry = 0
-> State 1.9 <-
-> State 1.2 <-
-> State 1.10 <-
ferryman = 1
ferryman = 0
goat = 1
cabbage = 0
carry = g
carry = c
-> State 1.3 <-
-> State 1.11 <-
ferryman = 0
ferryman = 1
carry = 0
carry = 0
-> State 1.4 <-
-> State 1.12 <-
ferryman = 1
ferryman = 0
cabbage = 1
wolf = 0
carry = c
carry = w
-> State 1.5 <-
-> State 1.13 <-
ferryman = 0
ferryman = 1
goat = 0
carry = 0
carry = g
-> State 1.14 <-
-> State 1.6 <-
ferryman = 0
ferryman = 1
goat = 0
wolf = 1
carry = g
carry = w
-> State 1.15 <-
-> State 1.7 <-
carry = 0
ferryman = 0
carry = 0
Figure 3.13. A solution path to the ferryman puzzle. It is unnecessar-
ily long. Using bounded model checking will refine it into an optimal
solution.
Invoking bounded model checking will produce the shortest possible path
to violate the property; in this case, it is states 1.1 to 1.8 of the illus-
trated path. It is the shortest, optimal solution to our planning problem
since the model check NuSMV -bmc -bmc_length 7 ferryman.smv shows
that the LTL formula holds in that model, meaning that no solution with
fewer than seven transitions is possible.

3.3 Model checking: systems, tools, properties
203
One might wish to verify whether there is a solution which involves three
journeys for the goat. This can be done by altering the LTL formula. In-
stead of seeking a path satisfying φ U ψ, where φ equals (goat = cabbage ∨
goat = wolf) →goat = ferryman and ψ equals cabbage ∧goat ∧wolf ∧
ferryman, we now seek a path satisfying (φ U ψ) ∧G (goat →G goat). The
last bit says that once the goat has crossed, he remains across; otherwise,
the goat makes at least three trips. NuSMV veriﬁes that the negation of this
formula is true, conﬁrming that there is no such solution.
3.3.6 The alternating bit protocol
The alternating bit protocol (ABP) is a protocol for transmitting messages
along a ‘lossy line,’ i.e., a line which may lose or duplicate messages. The
protocol guarantees that, providing the line doesn’t lose inﬁnitely many mes-
sages, communication between the sender and the receiver will be successful.
(We allow the line to lose or duplicate messages, but it may not corrupt mes-
sages; however, there is no way of guaranteeing successful transmission along
a line which can corrupt.)
The ABP works as follows. There are four entities, or agents: the sender,
the receiver, the message channel and the acknowledgement channel. The
sender transmits the ﬁrst part of the message together with the ‘control’
bit 0. If, and when, the receiver receives a message with the control bit 0,
it sends 0 along the acknowledgement channel. When the sender receives
this acknowledgement, it sends the next packet with the control bit 1. If
and when the receiver receives this, it acknowledges by sending a 1 on the
acknowledgement channel. By alternating the control bit, both receiver and
sender can guard against duplicating messages and losing messages (i.e.,
they ignore messages that have the unexpected control bit).
If the sender doesn’t get the expected acknowledgement, it continually re-
sends the message, until the acknowledgement arrives. If the receiver doesn’t
get a message with the expected control bit, it continually resends the pre-
vious acknowledgement.
Fairness is also important for the ABP. It comes in because, although
we want to model the fact that the channel can lose messages, we want to
assume that, if we send a message often enough, eventually it will arrive.
In other words, the channel cannot lose an inﬁnite sequence of messages. If
we did not make this assumption, then the channels could lose all messages
and, in that case, the ABP would not work.
Let us see this in the concrete setting of SMV. We may assume that
the text to be sent is divided up into single-bit messages, which are sent

204
3 Verification by model checking
MODULE sender(ack)
VAR
st
: {sending,sent};
message1 : boolean;
message2 : boolean;
ASSIGN
init(st) := sending;
next(st) := case
ack = message2 & !(st=sent) : sent;
1
: sending;
esac;
next(message1) :=
case
st = sent : {0,1};
1
: message1;
esac;
next(message2) :=
case
st = sent : !message2;
1
: message2;
esac;
FAIRNESS running
LTLSPEC G F st=sent
Figure 3.14. The ABP sender in SMV.
sequentially. The variable message1 is the current bit of the message be-
ing sent, whereas message2 is the control bit. The deﬁnition of the mod-
ule sender is given in Figure 3.14. This module spends most of its time in
st=sending, going only brieﬂy to st=sent when it receives an acknowledge-
ment corresponding to the control bit of the message it has been sending.
The variables message1 and message2 represent the actual data being sent
and the control bit, respectively. On successful transmission, the module ob-
tains a new message to send and returns to st=sending. The new message1
is obtained non-deterministically (i.e., from the environment); message2 al-
ternates in value. We impose FAIRNESS running, i.e., the sender must be
selected to run inﬁnitely often. The LTLSPEC tests that we can always suc-
ceed in sending the current message. The module receiver is programmed
in a similar way, in Figure 3.15.
We also need to describe the two channels, in Figure 3.16. The acknowl-
edgement channel is an instance of the one-bit channel one-bit-chan below.
Its lossy character is speciﬁed by the assignment to forget. The value of

3.3 Model checking: systems, tools, properties
205
MODULE receiver(message1,message2)
VAR
st
: {receiving,received};
ack
: boolean;
expected : boolean;
ASSIGN
init(st) := receiving;
next(st) := case
message2=expected & !(st=received) : received;
1
: receiving;
esac;
next(ack) :=
case
st = received : message2;
1
: ack;
esac;
next(expected) :=
case
st = received : !expected;
1
: expected;
esac;
FAIRNESS running
LTLSPEC G F st=received
Figure 3.15. The ABP receiver in SMV.
input should be transmitted to output, unless forget is true. The two-bit
channel two-bit-chan, used to send messages, is similar. Again, the non-
deterministic variable forget determines whether the current bit is lost or
not. Either both parts of the message get through, or neither of them does
(the channel is assumed not to corrupt messages).
The channels have fairness constraint which are intended to model the fact
that, although channels can lose messages, we assume that they inﬁnitely
often transmit the message correctly. (If this were not the case, then we
could ﬁnd an uninteresting violation of the liveness property, for example a
path along which all messages from a certain time onwards get lost.)
It is interesting to note that the fairness constraint ‘inﬁnitely often
!forget’ is not suﬃcient to prove the desired properties, for although it
forces the channel to transmit inﬁnitely often, it doesn’t prevent it from
(say) dropping all the 0 bits and transmitting all the 1 bits. That is why
we use the stronger fairness constraints shown. Some systems allow fairness

206
3 Verification by model checking
MODULE one-bit-chan(input)
VAR
output : boolean;
forget : boolean;
ASSIGN
next(output) := case
forget : output;
1:
input;
esac;
FAIRNESS running
FAIRNESS input & !forget
FAIRNESS !input & !forget
MODULE two-bit-chan(input1,input2)
VAR
forget : boolean;
output1 : boolean;
output2 : boolean;
ASSIGN
next(output1) := case
forget : output1;
1:
input1;
esac;
next(output2) := case
forget : output2;
1:
input2;
esac;
FAIRNESS running
FAIRNESS input1 & !forget
FAIRNESS !input1 & !forget
FAIRNESS input2 & !forget
FAIRNESS !input2 & !forget
Figure 3.16. The two modules for the two ABP channels in SMV.
contraints of the form ‘inﬁnitely often p implies inﬁnitely often q’, which
would be more satisfactory here, but is not allowed by SMV.
Finally, we tie it all together with the module main (Figure 3.17). Its role
is to connect together the components of the system, and giving them initial
values of their parameters. Since the ﬁrst control bit is 0, we also initialise
the receiver to expect a 0. The receiver should start oﬀby sending 1 as its

3.4 Branching-time logic
207
MODULE main
VAR
s : process sender(ack_chan.output);
r : process receiver(msg_chan.output1,msg_chan.output2);
msg_chan : process two-bit-chan(s.message1,s.message2);
ack_chan : process one-bit-chan(r.ack);
ASSIGN
init(s.message2) := 0;
init(r.expected) := 0;
init(r.ack)
:= 1;
init(msg_chan.output2) := 1;
init(ack_chan.output) := 1;
LTLSPEC
G (s.st=sent & s.message1=1 -> msg_chan.output1=1)
Figure 3.17. The main ABP module.
acknowledgement, so that sender does not think that its very ﬁrst message
is being acknowledged before anything has happened. For the same reason,
the output of the channels is initialised to 1.
The speciﬁcations for ABP.
Our SMV program satisﬁes the following spec-
iﬁcations:
Safety: If the message bit 1 has been sent and the correct acknowledge-
ment has been returned, then a 1 was indeed received by the receiver:
G (S.st=sent & S.message1=1 -> msg chan.output1=1).
Liveness: Messages get through eventually. Thus, for any state there is
inevitably a future state in which the current message has got through. In
the module sender, we speciﬁed G F st=sent. (This speciﬁcation could
equivalently have been written in the main module, as G F S.st=sent.)
Similarly, acknowledgements get through eventually. In the module
receiver, we write G F st=received.
3.4 Branching-time logic
In our analysis of LTL (linear-time temporal logic) in the preceding sections,
we noted that LTL formulas are evaluated on paths. We deﬁned that a state
of a system satisﬁes an LTL formula if all paths from the given state satisfy
it. Thus, LTL implicitly quantiﬁes universally over paths. Therefore, prop-
erties which assert the existence of a path cannot be expressed in LTL. This
problem can partly be alleviated by considering the negation of the property
in question, and interpreting the result accordingly. To check whether there

208
3 Verification by model checking
exists a path from s satisfying the LTL formula φ, we check whether all paths
satisfy ¬φ; a positive answer to this is a negative answer to our original ques-
tion, and vice versa. We used this approach when analysing the ferryman
puzzle in the previous section. However, as already noted, properties which
mix universal and existential path quantiﬁers cannot in general be model
checked using this approach, because the complement formula still has a mix.
Branching-time logics solve this problem by allowing us to quantify ex-
plicitly over paths. We will examine a logic known as Computation Tree
Logic, or CTL. In CTL, as well as the temporal operators U, F, G and X of
LTL we also have quantiﬁers A and E which express ‘all paths’ and ‘exists
a path’, respectively. For example, we can write:
r There is a reachable state satisfying q: this is written EF q.
r From all reachable states satisfying p, it is possible to maintain p continuously
until reaching a state satisfying q: this is written AG (p →E[p U q]).
r Whenever a state satisfying p is reached, the system can exhibit q continuously
forevermore: AG (p →EG q).
r There is a reachable state from which all reachable states satisfy p: EF AG p.
3.4.1 Syntax of CTL
Computation Tree Logic, or CTL for short, is a branching-time logic, mean-
ing that its model of time is a tree-like structure in which the future is not
determined; there are diﬀerent paths in the future, any one of which might
be the ‘actual’ path that is realised.
As before, we work with a ﬁxed set of atomic formulas/descriptions (such
as p, q, r, . . . , or p1, p2, . . . ).
Deﬁnition 3.12 We deﬁne CTL formulas inductively via a Backus Naur
form as done for LTL:
φ ::= ⊥| ⊤| p | (¬φ) | (φ ∧φ) | (φ ∨φ) | (φ →φ) | AX φ | EX φ |
AF φ | EF φ | AG φ | EG φ | A[φ U φ] | E[φ U φ]
where p ranges over a set of atomic formulas.
Notice that each of the CTL temporal connectives is a pair of symbols.
The ﬁrst of the pair is one of A and E. A means ‘along All paths’ (inevitably)
and E means ‘along at least (there Exists) one path’ (possibly). The second
one of the pair is X, F, G, or U, meaning ‘neXt state,’ ‘some Future state,’
‘all future states (Globally)’ and Until, respectively. The pair of symbols
in E[φ1 U φ2], for example, is EU. In CTL, pairs of symbols like EU are

3.4 Branching-time logic
209
indivisible. Notice that AU and EU are binary. The symbols X, F, G and
U cannot occur without being preceded by an A or an E; similarly, every A
or E must have one of X, F, G and U to accompany it.
Usually weak-until (W) and release (R) are not included in CTL, but they
are derivable (see Section 3.4.5).
Convention 3.13 We assume similar binding priorities for the CTL con-
nectives to what we did for propositional and predicate logic. The unary
connectives (consisting of ¬ and the temporal connectives AG, EG, AF, EF,
AX and EX) bind most tightly. Next in the order come ∧and ∨; and after
that come →, AU and EU .
Naturally, we can use brackets in order to override these priorities. Let
us see some examples of well-formed CTL formulas and some examples
which are not well-formed, in order to understand the syntax. Suppose
that p, q and r are atomic formulas. The following are well-formed CTL
formulas:
r AG (q →EG r), note that this is not the same as AG q →EG r, for according to
Convention 3.13, the latter formula means (AG q) →(EG r)
r EF E[r U q]
r A[p U EF r]
r EF EG p →AF r, again, note that this binds as (EF EG p) →AF r, not
EF (EG p →AF r) or EF EG (p →AF r)
r A[p1 U A[p2 U p3]]
r E[A[p1 U p2] U p3]
r AG (p →A[p U (¬p ∧A[¬p U q])]).
It is worth spending some time seeing how the syntax rules allow us to
construct each of these. The following are not well-formed formulas:
r EF G r
r A¬G ¬p
r F [r U q]
r EF (r U q)
r AEF r
r A[(r U q) ∧(p U r)].
It is especially worth understanding why the syntax rules don’t allow us to
construct these. For example, take EF (r U q). The problem with this string
is that U can occur only when paired with an A or an E. The E we have is
paired with the F. To make this into a well-formed CTL formula, we would
have to write EF E[r U q] or EF A[r U q].

210
3 Verification by model checking
AU
EU
AX
¬
¬
EX
p
p
∧
q
p
Figure 3.18. The parse tree of a CTL formula without infix notation.
Notice that we use square brackets after the A or E, when the paired
operator is a U. There is no strong reason for this; you could use ordinary
round brackets instead. However, it often helps one to read the formula
(because we can more easily spot where the corresponding close bracket is).
Another reason for using the square brackets is that SMV insists on it.
The reason A[(r U q) ∧(p U r)] is not a well-formed formula is that the
syntax does not allow us to put a boolean connective (like ∧) directly inside
A[ ] or E[ ]. Occurrences of A or E must be followed by one of G, F, X or U;
when they are followed by U, it must be in the form A[φ U ψ]. Now, the φ
and the ψ may contain ∧, since they are arbitrary formulas; so A[(p ∧q) U
(¬r →q)] is a well-formed formula.
Observe that AU and EU are binary connectives which mix inﬁx and
preﬁx notation. In pure inﬁx, we would write φ1 AU φ2, whereas in pure
preﬁx we would write AU(φ1, φ2).
As with any formal language, and as we did in the previous two chapters,
it is useful to draw parse trees for well-formed formulas. The parse tree for
A[AX ¬p U E[EX (p ∧q) U ¬p]] is shown in Figure 3.18.
Deﬁnition 3.14 A subformula of a CTL formula φ is any formula ψ whose
parse tree is a subtree of φ’s parse tree.

3.4 Branching-time logic
211
3.4.2 Semantics of computation tree logic
CTL formulas are interpreted over transition systems (Deﬁnition 3.4). Let
M = (S, →, L) be such a model, s ∈S and φ a CTL formula. The deﬁnition
of whether M, s ⊨φ holds is recursive on the structure of φ, and can be
roughly understood as follows:
r If φ is atomic, satisfaction is determined by L.
r If the top-level connective of φ (i.e., the connective occurring top-most in the
parse tree of φ) is a boolean connective (∧, ∨, ¬, ⊤etc.) then the satisfaction
question is answered by the usual truth-table deﬁnition and further recursion
down φ.
r If the top level connective is an operator beginning A, then satisfaction holds if
all paths from s satisfy the ‘LTL formula’ resulting from removing the A symbol.
r Similarly, if the top level connective begins with E, then satisfaction holds if
some path from s satisfy the ‘LTL formula’ resulting from removing the E.
In the last two cases, the result of removing A or E is not strictly an LTL
formula, for it may contain further As or Es below. However, these will be
dealt with by the recursion.
The formal deﬁnition of M, s ⊨φ is a bit more verbose:
Deﬁnition 3.15 Let M = (S, →, L) be a model for CTL, s in S, φ a CTL
formula. The relation M, s ⊨φ is deﬁned by structural induction on φ:
1.
M, s ⊨⊤and M, s ̸⊨⊥
2.
M, s ⊨p iﬀp ∈L(s)
3.
M, s ⊨¬φ iﬀM, s ̸⊨φ
4.
M, s ⊨φ1 ∧φ2 iﬀM, s ⊨φ1 and M, s ⊨φ2
5.
M, s ⊨φ1 ∨φ2 iﬀM, s ⊨φ1 or M, s ⊨φ2
6.
M, s ⊨φ1 →φ2 iﬀM, s ̸⊨φ1 or M, s ⊨φ2.
7.
M, s ⊨AX φ iﬀfor all s1 such that s →s1 we have M, s1 ⊨φ. Thus, AX says:
‘in every next state.’
8.
M, s ⊨EX φ iﬀfor some s1 such that s →s1 we have M, s1 ⊨φ. Thus, EX
says: ‘in some next state.’ E is dual to A – in exactly the same way that ∃is
dual to ∀in predicate logic.
9.
M, s ⊨AG φ holds iﬀfor all paths s1 →s2 →s3 →. . ., where s1 equals s, and
all si along the path, we have M, si ⊨φ. Mnemonically: for All computation
paths beginning in s the property φ holds Globally. Note that ‘along the path’
includes the path’s initial state s.
10.
M, s ⊨EG φ holds iﬀthere is a path s1 →s2 →s3 →. . ., where s1 equals s,
and for all si along the path, we have M, si ⊨φ. Mnemonically: there Exists
a path beginning in s such that φ holds Globally along the path.

212
3 Verification by model checking
φ
Figure 3.19. A system whose starting state satisfies EF φ.
11.
M, s ⊨AF φ holds iﬀfor all paths s1 →s2 →. . ., where s1 equals s, there is
some si such that M, si ⊨φ. Mnemonically: for All computation paths begin-
ning in s there will be some Future state where φ holds.
12.
M, s ⊨EF φ holds iﬀthere is a path s1 →s2 →s3 →. . ., where s1 equals s,
and for some si along the path, we have M, si ⊨φ. Mnemonically: there Exists
a computation path beginning in s such that φ holds in some Future state;
13.
M, s ⊨A[φ1 U φ2] holds iﬀfor all paths s1 →s2 →s3 →. . ., where s1 equals
s, that path satisﬁes φ1 U φ2, i.e., there is some si along the path, such that
M, si ⊨φ2, and, for each j < i, we have M, sj ⊨φ1. Mnemonically: All com-
putation paths beginning in s satisfy that φ1 Until φ2 holds on it.
14.
M, s ⊨E[φ1 U φ2] holds iﬀthere is a path s1 →s2 →s3 →. . ., where s1 equals
s, and that path satisﬁes φ1 U φ2 as speciﬁed in 13. Mnemonically: there Exists
a computation path beginning in s such that φ1 Until φ2 holds on it.
Clauses 9–14 above refer to computation paths in models. It is there-
fore useful to visualise all possible computation paths from a given state
s by unwinding the transition system to obtain an inﬁnite computation
tree, whence ‘computation tree logic.’ The diagrams in Figures 3.19–3.22
show schematically systems whose starting states satisfy the formulas EF φ,
EG φ, AG φ and AF φ, respectively. Of course, we could add more φ to any
of these diagrams and still preserve the satisfaction – although there is noth-
ing to add for AG . The diagrams illustrate a ‘least’ way of satisfying the
formulas.

3.4 Branching-time logic
213
φ
φ
φ
Figure 3.20. A system whose starting state satisfies EG φ.
φ
φ
φ
φ
φ
φ
φ
φ
φ
φ
Figure 3.21. A system whose starting state satisfies AG φ.
Recall the transition system of Figure 3.3 for the designated starting state
s0, and the inﬁnite tree illustrated in Figure 3.5. Let us now look at some
example checks for this system.
1.
M, s0 ⊨p ∧q holds since the atomic symbols p and q are contained in the node
of s0.
2.
M, s0 ⊨¬r holds since the atomic symbol r is not contained in node s0.

214
3 Verification by model checking
φ
φ
φ
φ
φ
Figure 3.22. A system whose starting state satisfies AF φ.
3.
M, s0 ⊨⊤holds by deﬁnition.
4.
M, s0 ⊨EX (q ∧r) holds since we have the leftmost computation path s0 →
s1 →s0 →s1 →. . . in Figure 3.5, whose second node s1 contains q and r.
5.
M, s0 ⊨¬AX (q ∧r) holds since we have the rightmost computation path s0 →
s2 →s2 →s2 →. . . in Figure 3.5, whose second node s2 only contains r, but
not q.
6.
M, s0 ⊨¬EF (p ∧r) holds since there is no computation path beginning in s0
such that we could reach a state where p ∧r would hold. This is so because
there is simply no state whatsoever in this system where p and r hold at the
same time.
7.
M, s2 ⊨EG r holds since there is a computation path s2 →s2 →s2 →. . .
beginning in s2 such that r holds in all future states of that path; this is
the only computation path beginning at s2 and so M, s2 ⊨AG r holds as well.
8.
M, s0 ⊨AF r holds since, for all computation paths beginning in s0, the system
reaches a state (s1 or s2) such that r holds.
9.
M, s0 ⊨E[(p ∧q) U r] holds since we have the rightmost computation path
s0 →s2 →s2 →s2 →. . . in Figure 3.5, whose second node s2 (i = 1) satisﬁes
r, but all previous nodes (only j = 0, i.e., node s0) satisfy p ∧q.
10.
M, s0 ⊨A[p U r] holds since p holds at s0 and r holds in any possible successor
state of s0, so p U r is true for all computation paths beginning in s0 (so we
may choose i = 1 independently of the path).
11.
M, s0 ⊨AG (p ∨q ∨r →EF EG r) holds since in all states reachable from s0
and satisfying p ∨q ∨r (all states in this case) the system can reach a state
satisfying EG r (in this case state s2).

3.4 Branching-time logic
215
3.4.3 Practical patterns of specifications
It’s useful to look at some typical examples of formulas, and compare the sit-
uation with LTL (Section 3.2.3). Suppose atomic descriptions include some
words such as busy and requested.
r It is possible to get to a state where started holds, but ready doesn’t:
EF (started ∧¬ready). To express impossibility, we simply negate the formula.
r For any state, if a request (of some resource) occurs, then it will eventually be
acknowledged:
AG (requested →AF acknowledged).
r The property that if the process is enabled inﬁnitely often, then it runs in-
ﬁnitely often, is not expressible in CTL. In particular, it is not expressed by
AG AF enabled →AG AF running, or indeed any other insertion of A or E into
the corresponding LTL formula. The CTL formula just given expresses that if
every path has inﬁnitely often enabled, then every path is inﬁnitely often taken;
this is much weaker than asserting that every path which has inﬁnitely often
enabled is inﬁnitely often taken.
r A certain process is enabled inﬁnitely often on every computation path:
AG (AF enabled).
r Whatever happens, a certain process will eventually be permanently deadlocked:
AF (AG deadlock).
r From any state it is possible to get to a restart state:
AG (EF restart).
r An upwards travelling lift at the second ﬂoor does not change its direction when
it has passengers wishing to go to the ﬁfth ﬂoor:
AG (floor2 ∧directionup ∧ButtonPressed5 →A[directionup U floor5])
Here, our atomic descriptions are boolean expressions built from system vari-
ables, e.g., floor2.
r The lift can remain idle on the third ﬂoor with its doors closed:
AG (floor3 ∧idle ∧doorclosed →EG (floor3 ∧idle ∧doorclosed)).
r A process can always request to enter its critical section. Recall that this was
not expressible in LTL. Using the propositions of Figure 3.8, this may be written
AG (n1 →EX t1) in CTL.
r Processes need not enter their critical section in strict sequence. This was also
not expressible in LTL, though we expressed its negation. CTL allows us to
express it directly: EF (c1 ∧E[c1 U (¬c1 ∧E[¬c2 U c1])]).
3.4.4 Important equivalences between CTL formulas
Deﬁnition 3.16 Two CTL formulas φ and ψ are said to be semantically
equivalent if any state in any model which satisﬁes one of them also satisﬁes
the other; we denote this by φ ≡ψ.

216
3 Verification by model checking
We have already noticed that A is a universal quantiﬁer on paths and E
is the corresponding existential quantiﬁer. Moreover, G and F are also uni-
versal and existential quantiﬁers, ranging over the states along a particular
path. In view of these facts, it is not surprising to ﬁnd that de Morgan rules
exist:
¬AF φ ≡EG ¬φ
¬EF φ ≡AG ¬φ
(3.6)
¬AX φ ≡EX ¬φ.
We also have the equivalences
AF φ ≡A[⊤U φ]
EF φ ≡E[⊤U φ]
which are similar to the corresponding equivalences in LTL.
3.4.5 Adequate sets of CTL connectives
As in propositional logic and in LTL, there is some redundancy among the
CTL connectives. For example, the connective AX can be written ¬EX ¬;
and AG, AF, EG and EF can be written in terms of AU and EU as follows:
ﬁrst, write AG φ as ¬EF ¬φ and EG φ as ¬AF ¬φ, using (3.6), and then use
AF φ ≡A[⊤U φ] and EF φ ≡E[⊤U φ]. Therefore AU, EU and EX form
an adequate set of temporal connectives.
Also EG, EU, and EX form an adequate set, for we have the equivalence
A[φ U ψ] ≡¬(E[¬ψ U (¬φ ∧¬ψ)] ∨EG ¬ψ)
(3.7)
which can be proved as follows:
A[φ U ψ] ≡A[¬(¬ψ U (¬φ ∧¬ψ)) ∧F ψ]
≡¬E¬[¬(¬ψ U (¬φ ∧¬ψ)) ∧F ψ]
≡¬E[(¬ψ U (¬φ ∧¬ψ)) ∨G ¬ψ]
≡¬(E[¬ψ U (¬φ ∧¬ψ)] ∨EG ¬ψ).
The ﬁrst line is by Theorem 3.10, and the remainder by elementary manipu-
lation. (This proof involves intermediate formulas which violate the syntactic
formation rules of CTL; however, it is valid in the logic CTL* introduced in
the next section.) More generally, we have:
Theorem 3.17 A set of temporal connectives in CTL is adequate if, and
only if, it contains at least one of {AX , EX }, at least one of {EG , AF , AU }
and EU .

3.5 CTL* and the expressive powers of LTL and CTL
217
This theorem is proved in a paper referenced in the bibliographic notes
at the end of the chapter. The connective EU plays a special role in that
theorem because neither weak-until W nor release R are primitive in CTL
(Deﬁnition 3.12). The temporal connectives AR, ER, AW and EW are all
deﬁnable in CTL:
r A[φ R ψ] = ¬E[¬φ U ¬ψ]
r E[φ R ψ] = ¬A[¬φ U ¬ψ]
r A[φ W ψ] = A[ψ R (φ ∨ψ)], and then use the ﬁrst equation above
r E[φ W ψ] = E[ψ R (φ ∨ψ)], and then use the second one.
These deﬁnitions are justiﬁed by LTL equivalences in Sections 3.2.4
and 3.2.5. Some other noteworthy equivalences in CTL are the following:
AG φ ≡φ ∧AX AG φ
EG φ ≡φ ∧EX EG φ
AF φ ≡φ ∨AX AF φ
EF φ ≡φ ∨EX EF φ
A[φ U ψ] ≡ψ ∨(φ ∧AX A[φ U ψ])
E[φ U ψ] ≡ψ ∨(φ ∧EX E[φ U ψ]).
For example, the intuition for the third one is the following: in order to have
AF φ in a particular state, φ must be true at some point along each path
from that state. To achieve this, we either have φ true now, in the current
state; or we postpone it, in which case we must have AF φ in each of the next
states. Notice how this equivalence appears to deﬁne AF in terms of AX
and AF itself, an apparently circular deﬁnition. In fact, these equivalences
can be used to deﬁne the six connectives on the left in terms of AX and
EX , in a non-circular way. This is called the ﬁxed-point characterisation of
CTL; it is the mathematical foundation for the model-checking algorithm
developed in Section 3.6.1; and we return to it later (Section 3.7).
3.5 CTL* and the expressive powers of LTL and CTL
CTL allows explicit quantiﬁcation over paths, and in this respect it is more
expressive than LTL, as we have seen. However, it does not allow one to
select a range of paths by describing them with a formula, as LTL does.
In that respect, LTL is more expressive. For example, in LTL we can say
‘all paths which have a p along them also have a q along them,’ by writing
F p →F q. It is not possible to write this in CTL because of the constraint
that every F has an associated A or E. The formula AF p →AF q means

218
3 Verification by model checking
something quite diﬀerent: it says ‘if all paths have a p along them, then
all paths have a q along them.’ One might write AG (p →AF q), which is
closer, since it says that every way of extending every path to a p eventually
meets a q, but that is still not capturing the meaning of F p →F q.
CTL* is a logic which combines the expressive powers of LTL and CTL,
by dropping the CTL constraint that every temporal operator (X, U, F, G)
has to be associated with a unique path quantiﬁer (A, E). It allows us to
write formulas such as
r A[(p U r) ∨(q U r)]: along all paths, either p is true until r, or q is true until r.
r A[X p ∨X X p]: along all paths, p is true in the next state, or the next but one.
r E[G F p]: there is a path along which p is inﬁnitely often true.
These formulas are not equivalent to, respectively, A[(p ∨q) U r)], AX p ∨
AX AX p and EG EF p. It turns out that the ﬁrst of them can be written
as a (rather long) CTL formula. The second and third do not have a CTL
equivalent.
The syntax of CTL* involves two classes of formulas:
r state formulas, which are evaluated in states:
φ ::= ⊤| p | (¬φ) | (φ ∧φ) | A[α] | E[α]
where p is any atomic formula and α any path formula; and
r path formulas, which are evaluated along paths:
α ::= φ | (¬α) | (α ∧α) | (α U α) | (G α) | (F α) | (X α)
where φ is any state formula. This is an example of an inductive deﬁnition
which is mutually recursive: the deﬁnition of each class depends upon the
deﬁnition of the other, with base cases p and ⊤.
LTL and CTL as subsets of CTL*
Although the syntax of LTL does
not include A and E, the semantic viewpoint of LTL is that we consider
all paths. Therefore, the LTL formula α is equivalent to the CTL* formula
A[α]. Thus, LTL can be viewed as a subset of CTL*.
CTL is also a subset of CTL*, since it is the fragment of CTL* in which
we restrict the form of path formulas to
α ::= (φ U φ) | (G φ) | (F φ) | (X φ)
Figure 3.23 shows the relationship among the expressive powers of CTL,
LTL and CTL*. Here are some examples of formulas in each of the subsets

3.5 CTL* and the expressive powers of LTL and CTL
219
LTL
ψ1
ψ2
ψ3
ψ4
CTL
CTL*
Figure 3.23. The expressive powers of CTL, LTL and CTL*.
shown:
In CTL but not in LTL: ψ1
def
= AG EF p. This expresses: wherever we
have got to, we can always get to a state in which p is true. This is
also useful, e.g., in ﬁnding deadlocks in protocols.
The proof that AG EF p is not expressible in LTL is as follows. Let φ be
an LTL formula such that A[φ] is allegedly equivalent to AG EF p. Since
M, s ⊨AG EF p in the left-hand diagram below, we have M, s ⊨A[φ].
Now let M′ be as shown in the right-hand diagram. The paths from s
in M′ are a subset of those from s in M, so we have M′, s ⊨A[φ]. Yet,
it is not the case that M′, s ⊨AG EF p; a contradiction.
¬p
¬p
p
s
s
t
In CTL*, but neither in CTL nor in LTL: ψ4
def
= E[G F p],
saying
that
there is a path with inﬁnitely many p.
The proof that this is not expressible in CTL is quite complex and may
be found in the papers co-authored by E. A. Emerson with others, given
in the references. (Why is it not expressible in LTL?)
In LTL but not in CTL: ψ3
def
= A[G F p →F q], saying that if there are in-
ﬁnitely many p along the path, then there is an occurrence of q. This
is an interesting thing to be able to say; for example, many fairness
constraints are of the form ‘inﬁnitely often requested implies eventually
acknowledged’.
In LTL and CTL: ψ2
def
= AG (p →AF q) in CTL, or G (p →F q) in LTL:
any p is eventually followed by a q.
Remark 3.18 We just saw that some (but not all) LTL formulas can be
converted into CTL formulas by adding an A to each temporal operator. For

220
3 Verification by model checking
a positive example, the LTL formula G (p →F q) is equivalent to the CTL
formula AG (p →AF q). We discuss two more negative examples:
r F G p and AF AG p are not equivalent, since F G p is satisﬁed, whereas AF AG p
is not satisﬁed, in the model
p
¬p
p
In fact, AF AG p is strictly stronger than F G p.
r While the LTL formulas X F p and F X p are equivalent, and they are equivalent
to the CTL formula AX AF p, they are not equivalent to AF AX p. The latter
is strictly stronger, and has quite a strange meaning (try working it out).
Remark 3.19 There is a considerable literature comparing linear-time and
branching-time logics. The question of which one is ‘better’ has been debated
for about 20 years. We have seen that they have incomparable expressive
powers. CTL* is more expressive than either of them, but is computationally
much more expensive (as will be seen in Section 3.6). The choice between
LTL and CTL depends on the application at hand, and on personal prefer-
ence. LTL lacks CTL’s ability to quantify over paths, and CTL lacks LTL’s
ﬁner-grained ability to describe individual paths. To many people, LTL ap-
pears to be more straightforward to use; as noted above, CTL formulas like
AF AX p seem hard to understand.
3.5.1 Boolean combinations of temporal formulas in CTL
Compared with CTL*, the syntax of CTL is restricted in two ways: it does
not allow boolean combinations of path formulas and it does not allow nest-
ing of the path modalities X, F and G. Indeed, we have already seen exam-
ples of the inexpressibility in CTL of nesting of path modalities, namely the
formulas ψ3 and ψ4 above.
In this section, we see that the ﬁrst of these restrictions is only apparent;
we can ﬁnd equivalents in CTL for formulas having boolean combinations
of path formulas. The idea is to translate any CTL formula having boolean
combinations of path formulas into a CTL formula that doesn’t. For exam-
ple, we may see that E[F p ∧F q] ≡EF [p ∧EF q] ∨EF [q ∧EF p] since, if
we have F p ∧F q along any path, then either the p must come before the q,
or the other way around, corresponding to the two disjuncts on the right.
(If the p and q occur simultaneously, then both disjuncts are true.)

3.6 Model-checking algorithms
221
Since U is like F (only with the extra complication of its ﬁrst argument),
we ﬁnd the following equivalence:
E[(p1 U q1) ∧(p2 U q2)] ≡E[(p1 ∧p2) U (q1 ∧E[p2 U q2])]
∨E[(p1 ∧p2) U (q2 ∧E[p1 U q1])].
And from the CTL equivalence A[p U q] ≡¬(E[¬q U (¬p ∧¬q)] ∨EG ¬q)
(see Theorem 3.10) we can obtain E[¬(p U q)]
≡
E[¬q U (¬p ∧¬q)] ∨
EG ¬q. Other identities we need in this translation include E[¬X p]
≡
EX ¬p.
3.5.2 Past operators in LTL
The temporal operators X, U, F, etc. which we have seen so far refer to the
future. Sometimes we want to encode properties that refer to the past, such
as: ‘whenever q occurs, then there was some p in the past.’ To do this, we
may add the operators Y, S, O, H. They stand for yesterday, since, once, and
historically, and are the past analogues of X, U, F, G, respectively. Thus,
the example formula may be written G (q →O p).
NuSMV supports past operators in LTL. One could also add past opera-
tors to CTL (AY, ES, etc.) but NuSMV does not support them.
Somewhat counter-intuitively, past operators do not increase the expres-
sive power of LTL. That is to say, every LTL formula with past operators
can be written equivalently without them. The example formula above can
be written ¬p W q, or equivalently ¬(¬q U (p ∧¬q)) if one wants to avoid
W. This result is surprising, because it seems that being able to talk about
the past as well as the future allows more expressivity than talking about
the future alone. However, recall that LTL equivalence is quite crude: it says
that the two formulas are satisﬁed by exactly the same set of paths. The
past operators allow us to travel backwards along the path, but only to reach
points we could have reached by travelling forwards from its beginning. In
contrast, adding past operators to CTL does increase its expressive power,
because they can allow us to examine states not forward-reachable from the
present one.
3.6 Model-checking algorithms
The semantic deﬁnitions for LTL and CTL presented in Sections 3.2 and 3.4
allow us to test whether the initial states of a given system satisfy an LTL or
CTL formula. This is the basic model-checking question. In general, inter-
esting transition systems will have a huge number of states and the formula

222
3 Verification by model checking
we are interested in checking may be quite long. It is therefore well worth
trying to ﬁnd eﬃcient algorithms.
Although LTL is generally preferred by speciﬁers, as already noted, we
start with CTL model checking because its algorithm is simpler.
3.6.1 The CTL model-checking algorithm
Humans may ﬁnd it easier to do model checks on the unwindings of models
into inﬁnite trees, given a designated initial state, for then all possible paths
are plainly visible. However, if we think of implementing a model checker
on a computer, we certainly cannot unwind transition systems into inﬁ-
nite trees. We need to do checks on ﬁnite data structures. For this reason,
we now have to develop new insights into the semantics of CTL. Such a
deeper understanding will provide the basis for an eﬃcient algorithm which,
given M, s ∈S and φ, computes whether M, s ⊨φ holds. In the case that
φ is not satisﬁed, such an algorithm can be augmented to produce an ac-
tual path (= run) of the system demonstrating that M cannot satisfy φ.
That way, we may debug a system by trying to ﬁx what enables runs which
refute φ.
There are various ways in which one could consider
M, s0
?
⊨φ
as a computational problem. For example, one could have the model M, the
formula φ and a state s0 as input; one would then expect a reply of the form
‘yes’ (M, s0 ⊨φ holds), or ‘no’ (M, s0 ⊨φ does not hold). Alternatively, the
inputs could be just M and φ, where the output would be all states s of the
model M which satisfy φ.
It turns out that it is easier to provide an algorithm for solving the second
of these two problems. This automatically gives us a solution to the ﬁrst one,
since we can simply check whether s0 is an element of the output set.
The labelling algorithm
We present an algorithm which, given a model
and a CTL formula, outputs the set of states of the model that satisfy the
formula. The algorithm does not need to be able to handle every CTL con-
nective explicitly, since we have already seen that the connectives ⊥, ¬ and
∧form an adequate set as far as the propositional connectives are concerned;
and AF , EU and EX form an adequate set of temporal connectives. Given
an arbitrary CTL formula φ, we would simply pre-process φ in order to write
it in an equivalent form in terms of the adequate set of connectives, and then

3.6 Model-checking algorithms
223
Repeat. . .
AFψ1
AFψ1
AFψ1
AFψ1
AFψ1
AFψ1
AFψ1
. . . until no change.
Figure 3.24. The iteration step of the procedure for labelling states with
subformulas of the form AF ψ1.
call the model-checking algorithm. Here is the algorithm:
INPUT: a CTL model M = (S, →, L) and a CTL formula φ.
OUTPUT: the set of states of M which satisfy φ.
First, change φ to the output of TRANSLATE (φ), i.e., we write φ in terms
of the connectives AF, EU, EX, ∧, ¬ and ⊥using the equivalences given
earlier in the chapter. Next, label the states of M with the subformulas of φ
that are satisﬁed there, starting with the smallest subformulas and working
outwards towards φ.
Suppose ψ is a subformula of φ and states satisfying all the immediate
subformulas of ψ have already been labelled. We determine by a case analysis
which states to label with ψ. If ψ is
r ⊥: then no states are labelled with ⊥.
r p: then label s with p if p ∈L(s).
r ψ1 ∧ψ2: label s with ψ1 ∧ψ2 if s is already labelled both with ψ1 and with ψ2.
r ¬ψ1: label s with ¬ψ1 if s is not already labelled with ψ1.
r AF ψ1:
– If any state s is labelled with ψ1, label it with AF ψ1.
– Repeat: label any state with AF ψ1 if all successor states are labelled with
AF ψ1, until there is no change. This step is illustrated in Figure 3.24.
r E[ψ1 U ψ2]:
– If any state s is labelled with ψ2, label it with E[ψ1 U ψ2].
– Repeat: label any state with E[ψ1 U ψ2] if it is labelled with ψ1 and at least
one of its successors is labelled with E[ψ1 U ψ2], until there is no change. This
step is illustrated in Figure 3.25.
r EX ψ1: label any state with EX ψ1 if one of its successors is labelled with ψ1.

224
3 Verification by model checking
ψ1
ψ1
E[ψ1 U ψ2]
E[ψ1 U ψ2]
E[ψ1 U ψ2]
Figure 3.25. The iteration step of the procedure for labelling states with
subformulas of the form E[ψ1 U ψ2].
Having performed the labelling for all the subformulas of φ (including φ
itself), we output the states which are labelled φ.
The complexity of this algorithm is O(f · V · (V + E)), where f is the
number of connectives in the formula, V is the number of states and E is
the number of transitions; the algorithm is linear in the size of the formula
and quadratic in the size of the model.
Handling EG directly
Instead of using a minimal adequate set of con-
nectives, it would have been possible to write similar routines for the other
connectives. Indeed, this would probably be more eﬃcient. The connectives
AG and EG require a slightly diﬀerent approach from that for the others,
however. Here is the algorithm to deal with EG ψ1 directly:
r EG ψ1:
– Label all the states with EG ψ1.
– If any state s is not labelled with ψ1, delete the label EG ψ1.
– Repeat: delete the label EG ψ1 from any state if none of its successors is
labelled with EG ψ1; until there is no change.
Here, we label all the states with the subformula EG ψ1 and then whittle
down this labelled set, instead of building it up from nothing as we did in
the case for EU. Actually, there is no real diﬀerence between this procedure
for EG ψ and what you would do if you translated it into ¬AF ¬ψ as far as
the ﬁnal result is concerned.
A variant which is more efficient
We can improve the eﬃciency of
our labelling algorithm by using a cleverer way of handling EG. Instead of
using EX, EU and AF as the adequate set, we use EX, EU and EG instead.
For EX and EU we do as before (but take care to search the model by

3.6 Model-checking algorithms
225
states satisfying ψ
⊨EG ψ
SCC
SCC
SCC
Figure 3.26. A better way of handling EG.
backwards breadth-ﬁrst search, for this ensures that we won’t have to pass
over any node twice). For the EG ψ case:
r Restrict the graph to states satisfying ψ, i.e., delete all other states and their
transitions;
r Find the maximal strongly connected components (SCCs); these are maximal
regions of the state space in which every state is linked with (= has a ﬁnite path
to) every other one in that region.
r Use backwards breadth-ﬁrst search on the restricted graph to ﬁnd any state that
can reach an SCC; see Figure 3.26.
The complexity of this algorithm is O(f · (V + E)), i.e., linear both in the
size of the model and in the size of the formula.
Example 3.20 We applied the basic algorithm to our second model of mu-
tual exclusion with the formula E[¬c2 U c1]; see Figure 3.27. The algorithm
labels all states which satisfy c1 during phase 1 with E[¬c2 U c1]. This labels
s2 and s4. During phase 2, it labels all states which do not satisfy c2 and
have a successor state that is already labelled. This labels states s1 and s3.
During phase 3, we label s0 because it does not satisfy c2 and has a succes-
sor state (s1) which is already labelled. Thereafter, the algorithm terminates
because no additional states get labelled: all unlabelled states either satisfy
c2, or must pass through such a state to reach a labelled state.
The pseudo-code of the CTL model-checking algorithm
We
present the pseudo-code for the basic labelling algorithm. The main function
SAT (for ‘satisﬁes’) takes as input a CTL formula. The program SAT expects
a parse tree of some CTL formula constructed by means of the grammar in
Deﬁnition 3.12. This expectation reﬂects an important precondition on the
correctness of the algorithm SAT. For example, the program simply would
not know what to do with an input of the form X (⊤∧EF p3), since this is
not a CTL formula.

226
3 Verification by model checking
s5
s0
0: t1n2
0: c1n2
0: t1t2
0: c1t2
2: E[¬c2 U c1]
s3
s1
s2
s6
s9
s4
s7
1: E[¬c2 U c1]
1: E[¬c2 U c1]
2: E[¬c2 U c1]
3: E[¬c2 U c1]
0: n1n2
0: n1t2
0: t1t2
0: t1c2
0: n1c2
Figure 3.27. An example run of the labelling algorithm in our second
model of mutual exclusion applied to the formula E[¬c2 U c1].
The pseudo-code we write for SAT looks a bit like fragments of C or
Java code; we use functions with a keyword return that indicates which
result the function should return. We will also use natural language to
indicate the case analysis over the root node of the parse tree of φ. The
declaration local var declares some fresh variables local to the current in-
stance of the procedure in question, whereas repeat until executes the
command which follows it repeatedly, until the condition becomes true. Ad-
ditionally, we employ suggestive notation for the operations on sets, like
intersection, set complement and so forth. In reality we would need an ab-
stract data type, together with implementations of these operations, but for
now we are interested only in the mechanism in principle of the algorithm
for SAT; any (correct and eﬃcient) implementation of sets would do and
we study such an implementation in Chapter 6. We assume that SAT has
access to all the relevant parts of the model: S, →and L. In particular,
we ignore the fact that SAT would require a description of M as input as
well. We simply assume that SAT operates directly on any such given model.
Note how SAT translates φ into an equivalent formula of the adequate set
chosen.

3.6 Model-checking algorithms
227
function SAT (φ)
/* determines the set of states satisfying φ */
begin
case
φ is ⊤: return S
φ is ⊥: return ∅
φ is atomic: return {s ∈S | φ ∈L(s)}
φ is ¬φ1 : return S −SAT (φ1)
φ is φ1 ∧φ2 : return SAT (φ1) ∩SAT (φ2)
φ is φ1 ∨φ2 : return SAT (φ1) ∪SAT (φ2)
φ is φ1 →φ2 : return SAT (¬φ1 ∨φ2)
φ is AX φ1 : return SAT (¬EX ¬φ1)
φ is EX φ1 : return SATEX(φ1)
φ is A[φ1 U φ2] : return SAT(¬(E[¬φ2 U (¬φ1 ∧¬φ2)] ∨EG ¬φ2))
φ is E[φ1 U φ2] : return SATEU(φ1, φ2)
φ is EF φ1 : return SAT (E(⊤U φ1))
φ is EG φ1 : return SAT(¬AF ¬φ1)
φ is AF φ1 : return SATAF (φ1)
φ is AG φ1 : return SAT (¬EF ¬φ1)
end case
end function
Figure 3.28. The function SAT. It takes a CTL formula as input and
returns the set of states satisfying the formula. It calls the functions
SATEX, SATEU and SATAF, respectively, if EX , EU or AF is the root of the
input’s parse tree.
The algorithm is presented in Figure 3.28 and its subfunctions in Fig-
ures 3.29–3.31. They use program variables X, Y , V and W which are sets
of states. The program for SAT handles the easy cases directly and passes
more complicated cases on to special procedures, which in turn might call
SAT recursively on subexpressions. These special procedures rely on imple-
mentations of the functions
pre∃(Y ) = {s ∈S | exists s′, (s →s′ and s′ ∈Y )}
pre∀(Y ) = {s ∈S | for all s′, (s →s′ implies s′ ∈Y )}.
‘Pre’ denotes travelling backwards along the transition relation. Both func-
tions compute a pre-image of a set of states. The function pre∃(instrumental
in SATEX and SATEU) takes a subset Y of states and returns the set of states
which can make a transition into Y . The function pre∀, used in SATAF, takes

228
3 Verification by model checking
function SATEX (φ)
/* determines the set of states satisfying EX φ */
local var X, Y
begin
X := SAT (φ);
Y := pre∃(X);
return Y
end
Figure 3.29. The function SATEX. It computes the states satisfying φ by
calling SAT. Then, it looks backwards along →to find the states satisfying
EX φ.
function SATAF (φ)
/* determines the set of states satisfying AF φ */
local var X, Y
begin
X := S;
Y := SAT (φ);
repeat until X = Y
begin
X := Y ;
Y := Y ∪pre∀(Y )
end
return Y
end
Figure 3.30. The function SATAF. It computes the states satisfying φ by
calling SAT. Then, it accumulates states satisfying AF φ in the manner
described in the labelling algorithm.
a set Y and returns the set of states which make transitions only into Y .
Observe that pre∀can be expressed in terms of complementation and pre∃,
as follows:
pre∀(Y ) = S −pre∃(S −Y )
(3.8)
where we write S −Y for the set of all s ∈S which are not in Y .
The correctness of this pseudocode and the model checking algorithm is
discussed in Section 3.7.

3.6 Model-checking algorithms
229
function SATEU (φ, ψ)
/* determines the set of states satisfying E[φ U ψ] */
local var W, X, Y
begin
W := SAT (φ);
X := S;
Y := SAT (ψ);
repeat until X = Y
begin
X := Y ;
Y := Y ∪(W ∩pre∃(Y ))
end
return Y
end
Figure 3.31. The function SATEU. It computes the states satisfying φ by
calling SAT. Then, it accumulates states satisfying E[φ U ψ] in the manner
described in the labelling algorithm.
The ‘state explosion’ problem
Although the labelling algorithm (with
the clever way of handling EG) is linear in the size of the model, unfortu-
nately the size of the model is itself more often than not exponential in the
number of variables and the number of components of the system which
execute in parallel. This means that, for example, adding a boolean variable
to your program will double the complexity of verifying a property of it.
The tendency of state spaces to become very large is known as the state
explosion problem. A lot of research has gone into ﬁnding ways of overcoming
it, including the use of:
r Eﬃcient data structures, called ordered binary decision diagrams (OBDDs),
which represent sets of states instead of individual states. We study these in
Chapter 6 in detail. SMV is implemented using OBDDs.
r Abstraction: one may interpret a model abstractly, uniformly or for a speciﬁc
property.
r Partial order reduction: for asynchronous systems, several interleavings of com-
ponent traces may be equivalent as far as satisfaction of the formula to be checked
is concerned. This can often substantially reduce the size of the model-checking
problem.
r Induction: model-checking systems with (e.g.) large numbers of identical, or sim-
ilar, components can often be implemented by ‘induction’ on this number.

230
3 Verification by model checking
r Composition: break the veriﬁcation problem down into several simpler veriﬁca-
tion problems.
The last four issues are beyond the scope of this book, but references may
be found at the end of this chapter.
3.6.2 CTL model checking with fairness
The veriﬁcation of M, s0 ⊨φ might fail because the model M may contain
behaviour which is unrealistic, or guaranteed not to occur in the actual sys-
tem being analysed. For example, in the mutual exclusion case, we expressed
that the process prc can stay in its critical section (st=c) as long as it needs.
We modelled this by the non-deterministic assignment
next(st) :=
case
...
(st = c)
: {c,n};
...
esac;
However, if we really allow process 2 to stay in its critical section as
long as it likes, then we have a path which violates the liveness constraint
AG (t1 →AF c1), since, if process 2 stays forever in its critical section, t1
can be true without c1 ever becoming true.
We would like to ignore this path, i.e., we would like to assume that the
process can stay in its critical section as long as it needs, but will eventually
exit from its critical section after some ﬁnite time.
In LTL, we could handle this by verifying a formula like FG¬c2 →φ,
where φ is the formula we actually want to verify. This whole formula asserts
that all paths which satisfy inﬁnitely often ¬c2 also satisfy φ. However,
we cannot do this in CTL because we cannot write formulas of the form
FG¬c2 →φ in CTL. The logic CTL is not expressive enough to allow us
to pick out the ‘fair’ paths, i.e., those in which process 2 always eventually
leaves its critical section.
It is for that reason that SMV allows us to impose fairness constraints
on top of the transition system it describes. These assumptions state that
a given formula is true inﬁnitely often along every computation path. We
call such paths fair computation paths. The presence of fairness constraints
means that, when evaluating the truth of CTL formulas in speciﬁcations,
the connectives A and E range only over fair paths.

3.6 Model-checking algorithms
231
We therefore impose the fairness constraint that !st=c be true inﬁnitely
often. This means that, whatever state the process is in, there will be a
state in the future in which it is not in its critical section. Similar fairness
constraints were used for the Alternating Bit Protocol.
Fairness constraints of the form (where φ is a state formula)
Property φ is true inﬁnitely often
are known as simple fairness constraints. Other types include those of the
form
If φ is true inﬁnitely often, then ψ is also true inﬁnitely often.
SMV can deal only with simple fairness constraints; but how does it do
that? To answer that, we now explain how we may adapt our model-checking
algorithm so that A and E are assumed to range only over fair computation
paths.
Deﬁnition 3.21 Let C
def
= {ψ1, ψ2, . . . , ψn} be a set of n fairness constraints.
A computation path s0 →s1 →. . . is fair with respect to these fairness
constraints iﬀfor each i there are inﬁnitely many j such that sj ⊨ψi, that
is, each ψi is true inﬁnitely often along the path. Let us write AC and EC
for the operators A and E restricted to fair paths.
For example, M, s0 ⊨ACG φ iﬀφ is true in every state along all fair paths;
and similarly for ACF, ACU, etc. Notice that these operators explicitly de-
pend on the chosen set C of fairness constraints. We already know that ECU,
ECG and ECX form an adequate set; this can be shown in the same man-
ner as was done for the temporal connectives without fairness constraints
(Section 3.4.4). We also have that
EC[φ U ψ] ≡E[φ U (ψ ∧ECG ⊤)]
ECX φ ≡EX (φ ∧ECG ⊤).
To see this, observe that a computation path is fair iﬀany suﬃx of it is
fair. Therefore, we need only provide an algorithm for ECG φ. It is similar
to Algorithm 2 for EG, given earlier in this chapter:
r Restrict the graph to states satisfying φ; of the resulting graph, we want to know
from which states there is a fair path.
r Find the maximal strongly connected components (SCCs) of the restricted graph;
r Remove an SCC if, for some ψi, it does not contain a state satisfying ψi. The
resulting SCCs are the fair SCCs. Any state of the restricted graph that can
reach one has a fair path from it.

232
3 Verification by model checking
states satisfying φ
⊨EfGφ
ψ2
ψ1
ψ3
fair SCC
fair SCC
Figure 3.32. Computing the states satisfying ECG φ. A state satisfies
ECG φ iff, in the graph resulting from the restriction to states satisfying
φ, the state has a fair path from it. A fair path is one which leads to an
SCC with a cycle passing through at least one state that satisfies each
fairness constraint; in the example, C equals {ψ1, ψ2, ψ3}.
r Use backwards breadth-ﬁrst search to ﬁnd the states on the restricted graph that
can reach a fair SCC.
See Figure 3.32. The complexity of this algorithm is O(n · f · (V + E)), i.e.,
still linear in the size of the model and formula.
It should be noted that writing fairness conditions using SMV’s FAIR-
NESS keyword is necessary only for CTL model checking. In the case of LTL,
we can assert the fairness condition as part of the formula to be checked.
For example, if we wish to check the LTL formula ψ under the assumption
that φ is inﬁnitely often true, we check G F φ →ψ. This means: all paths
satisfying inﬁnitely often φ also satisfy ψ. It is not possible to express this
in CTL. In particular, any way of adding As or Es to G F φ →ψ will result
in a formula with a diﬀerent meaning from the intended one. For example,
AG AF φ →ψ means that if all paths are fair then ψ holds, rather than what
was intended: ψ holds along all paths which are fair.
3.6.3 The LTL model-checking algorithm
The algorithm presented in the sections above for CTL model checking
is quite intuitive: given a system and a CTL formula, it labels states of
the system with the subformulas of the formula which are satisﬁed there.
The state-labelling approach is appropriate because subformulas of the for-
mula may be evaluated in states of the system. This is not the case for
LTL: subformulas of the formula must be evaluated not in states but along
paths of the system. Therefore, LTL model checking has to adopt a diﬀerent
strategy.
There are several algorithms for LTL model checking described in the
literature. Although they diﬀer in detail, nearly all of them adopt the same

3.6 Model-checking algorithms
233
basic strategy. We explain that strategy ﬁrst; then, we describe some algo-
rithms in more detail.
The basic strategy
Let M = (S, →, L) be a model, s ∈S, and φ an LTL
formula. We determine whether M, s ⊨φ, i.e., whether φ is satisﬁed along
all paths of M starting at s. Almost all LTL model checking algorithms
proceed along the following three steps.
1.
Construct an automaton, also known as a tableau, for the formula ¬φ. The
automaton for ψ is called Aψ. Thus, we construct A¬φ. The automaton has a
notion of accepting a trace. A trace is a sequence of valuations of the proposi-
tional atoms. From a path, we can abstract its trace. The construction has the
property that for all paths π: π ⊨ψ iﬀthe trace of π is accepted by Aψ. In other
words, the automaton Aψ encodes precisely the traces which satisfy ψ.
Thus, the automaton A¬φ which we construct for ¬φ has the property that it
encodes all the traces satisfying ¬φ; i.e., all the traces which do not satisfy φ.
2.
Combine the automaton A¬φ with the model M of the system. The combina-
tion operation results in a transition system whose paths are both paths of the
automaton and paths of the system.
3.
Discover whether there is any path from a state derived from s in the combined
transition system. Such a path, if there is one, can be interpreted as a path in
M beginning at s which does not satisfy φ.
If there was no such path, then output: ‘Yes, M, s ⊨φ.’ Otherwise, if there is
such a path, output ‘No, M, s ̸⊨φ.’ In the latter case, the counterexample can
be extracted from the path found.
Let us consider an example. The system is described by the SMV program
and its model M, shown in Figure 3.33. We consider the formula ¬(a U b).
Since it is not the case that all paths of M satisfy the formula (for example,
the path q3, q2, q2 . . . does not satisfy it) we expect the model check to
fail.
In accordance with Step 1, we construct an automaton AaUb which char-
acterises precisely the traces which satisfy a U b. (We use the fact that
¬¬(a U b) is equivalent to a U b.) Such an automaton is shown in Figure
3.34. We will look at how to construct it later; for now, we just try to un-
derstand how and why it works.
A trace t is accepted by an automaton like the one of Figure 3.34 if there
exists a path π through the automaton such that:
r π starts in an initial state (i.e. one containing φ);
r it respects the transition relation of the automaton;
r t is the trace of π; matches the corresponding state of π;

234
3 Verification by model checking
init(a) := 1;
init(b) := 0;
next(a) := case
!a : 0;
b
: 1;
1
: {0,1};
esac;
next(b) := case
a & next(a) : !b;
!a : 1;
1
: {0,1};
esac;
q3
q1
q2
ab
q4
ab
ab
ab
Figure 3.33. An SMV program and its model M.
a b φ
q4
a b φ
a b φ
q1
q′
3
a b φ
q3
a b φ
q2
Figure 3.34. Automaton accepting precisely traces satisfying φ
def
= a U b.
The transitions with no arrows can be taken in either direction. The
acceptance condition is that the path of the automaton cannot loop
indefinitely through q3.
r the path respects a certain ‘accepting condition.’ For the automaton of Fig-
ure 3.34, the accepting condition is that the path should not end q3, q3, q3 . . . ,
indeﬁnitely.
For example, suppose t is a b, a b, a b, a b, a b, a b, a b, a b, . . . , eventually re-
peating forevermore the state a b. Then we choose the path q3, q3, q3, q4, q4,
q1, q′
3, q′
3 . . . . We start in q3 because the ﬁrst state is a b and it is an initial

3.6 Model-checking algorithms
235
state. The next states we choose just follow the valuation of the states of
π. For example, at q1 the next valuation is a b and the transitions allow us
to choose q3 or q′
3. We choose q′
3, and loop there forevermore. This path
meets the conditions, and therefore the trace t is accepted. Observe that the
deﬁnition states ‘there exists a path.’ In the example above, there are also
paths which don’t meet the conditions:
r Any path beginning q3, q′
3, . . . doesn’t meet the condition that we have to respect
the transition relation.
r The path q3, q3, q3, q4, q4, q1, q3, q3 . . . doesn’t meet the condition that we must
not end on a loop of q3.
These paths need not bother us, because it is suﬃcient to ﬁnd one which
does meet the conditions in order to declare that π is accepted.
Why does the automaton of Figure 3.34 work as intended? To understand
it, observe that it has enough states to distinguish the values of the propo-
sitions – that is, a state for each of the valuations {a b, a b, a b, a b}, and in
fact two states for the valuation a b. One state for each of {a b, a b, a b} is
intuitively enough, because those valuations determine whether a U b holds.
But a U b could be false or true in a b, so we have to consider the two cases.
The presence of φ
def
= a U b in a state indicates that either we are still ex-
pecting φ to become true, or we have just obtained it. Whereas φ indicates
we no longer expect φ, and have not just obtained it. The transitions of the
automaton are such that the only way out of q3 is to obtain b, i.e., to move to
q2 or q4. Apart from that, the transitions are liberal, allowing any path to be
followed; each of q1, q2, q3 can transition to any valuation, and so can q3, q′
3
taken together, provided we are careful to choose the right one to enter.
The acceptance condition, which allows any path except one looping indeﬁ-
nitely on q3, guarantees that the promise of a U b to deliver b is eventually
fulﬁlled.
Using this automaton AaUb, we proceed to Step 2. To combine the au-
tomaton AaUb with the model of the system M shown in Figure 3.33, it is
convenient ﬁrst to redraw M with two versions of q3; see Figure 3.35(left).
It is an equivalent system; all ways into q3 now non-deterministically choose
q3 or q′
3, and which ever one we choose leads to the same successors. But it
allows us to superimpose it on AaUb and select the transitions common to
both, obtaining the combined system of Figure 3.35(right).
Step 3 now asks whether there is a path from q of the combined automa-
ton. As can be seen, there are two kinds of path in the combined system:
q3, (q4, q3, )∗q2, q2 . . . , and q3, q4, (q3, q4, )∗q′
3, q1, q2, q2, . . .
where (q3, q4)∗
denotes either the empty string or q3, q4 or q3, q4, q3, q4 etc. Thus, according

236
3 Verification by model checking
a b
a b
a b
q1
a b
a b
q2
a b φ
a b φ
a b φ
q1
a b φ
a b φ
q2
q3
q′
3
q3
q′
3
q4
q4
Figure 3.35. Left: the system M of Figure 3.33, redrawn with an ex-
panded state space; right: the expanded M and AaUb combined.
to Step 3, and as we expected, ¬(a U b) is not satisﬁed in all paths of the
original system M.
Constructing the automaton
Let us look in more detail at how the
automaton is constructed. Given an LTL formula φ, we wish to construct
an automaton Aφ such that Aφ accepts precisely those runs on which φ
holds. We assume that φ contains only the temporal connectives U and X;
recall that the other temporal connectives can be written in terms of these
two.
Deﬁne the closure C(φ) of formula φ as the set of subformulas of φ
and their complements, identifying ¬¬ψ and ψ. For example, C(a U b) =
{a, b, ¬a, ¬b, a U b, ¬(a U b)}. The states of Aφ, denoted by q, q′ etc., are
the maximal subsets of C(φ) which satisfy the following conditions:
r For all (non-negated) ψ ∈C(φ), either ψ ∈q or ¬ψ ∈q, but not both.
r ψ1 ∨ψ2 ∈q holds iﬀψ1 ∈q or ψ2 ∈q, whenever ψ1 ∨ψ2 ∈C(φ).
r Conditions for other boolean combinations are similar.
r If ψ1 U ψ2 ∈q, then ψ2 ∈q or ψ1 ∈q.
r If ¬(ψ1 U ψ2) ∈q, then ¬ψ2 ∈q.
Intuitively, these conditions imply that the states of Aφ are capable of saying
which subformulas of φ are true.

3.6 Model-checking algorithms
237
The initial states of Aφ are those states containing φ. For transition rela-
tion δ of Aφ we have (q, q′) ∈δ iﬀall of the following conditions hold:
r if X ψ ∈q then ψ ∈q′;
r if ¬X ψ ∈q then ¬ψ ∈q′;
r If ψ1 U ψ2 ∈q and ψ2 /∈q then ψ1 U ψ2 ∈q′;
r If ¬(ψ1 U ψ2) ∈q and ψ1 ∈q then ¬(ψ1 U ψ2) ∈q′.
These last two conditions are justiﬁed by the recursion laws
ψ1 U ψ2 = ψ2 ∨(ψ1 ∧X (ψ1 U ψ2))
¬(ψ1 U ψ2) = ¬ψ2 ∧(¬ψ1 ∨X ¬(ψ1 U ψ2)) .
In particular, they ensure that whenever some state contains ψ1 U ψ2, sub-
sequent states contain ψ1 for as long as they do not contain ψ2.
As we have deﬁned Aφ so far, not all paths through Aφ satisfy φ. We use
additional acceptance conditions to guarantee the ‘eventualities’ ψ promised
by the formula ψ1 U ψ2, namely that Aφ cannot stay for ever in states satis-
fying ψ1 without ever obtaining ψ2. Recall that, for the automaton of Figure
3.34 for a U b, we stipulated the acceptance condition that the path through
the automaton should not end q3, q3, . . . .
The acceptance conditions of Aφ are deﬁned so that they ensure that
every state containing some formula χ U ψ will eventually be followed by
some state containing ψ. Let χ1 U ψ1, . . . , χk U ψk be all subformulas of
this form in C(φ). We stipulate the following acceptance condition: a run
is accepted if, for every i such that 1 ≤i ≤k, the run has inﬁnitely many
states satisfying ¬(χi U ψi) ∨ψi. To understand why this condition has the
desired eﬀect, imagine the circumstances in which it is false. Suppose we
have a run having only ﬁnitely many states satisfying ¬(χi U ψi) ∨ψi. Let
us advance through all those ﬁnitely many states, taking the suﬃx of the run
none of whose states satisﬁes ¬(χi U ψi) ∨ψi, i.e., all of whose states satisfy
(χi U ψi) ∧¬ψi. That is precisely the sort of run we want to eliminate.
If we carry out this construction on a U b, we obtain the automaton shown
in Figure 3.34. Another example is shown in Figure 3.36, for the formula
(p U q) ∨(¬p U q). Since that formula has two U subformulas, there are two
sets speciﬁed in the acceptance condition, namely, the states satisfying p U q
and the states satisfying ¬p U q.
How LTL model checking is implemented in NuSMV
In the sec-
tions above, we described an algorithm for LTL model checking. Given an
LTL formula φ and a system M and a state s of M, we may check whether
M, s ⊨φ holds by constructing the automaton A¬φ, combining it with M,

238
3 Verification by model checking
¬(p U q),
¬(¬p U q),
¬p, ¬q, ¬φ
¬(p U q),
¬(¬p U q),
p U q,
¬p U q,
¬p, q, φ
p U q,
¬(¬p U q),
p, ¬q, φ
p U q,
¬p U q,
p, q, φ
¬(p U q),
¬p U q,
¬p, ¬q, φ
q1
q2
q3
q4
q5
q6
p, ¬q, ¬φ
Figure 3.36. Automaton accepting precisely traces satisfying φ
def
= (p U
q) ∨(¬p U q). The transitions with no arrows can be taken in either direc-
tion. The acceptance condition asserts that every run must pass infinitely
often through the set {q1, q3, q4, q5, q6}, and also the set {q1, q2, q3, q5, q6}.
and checking whether there is a path of the resulting system which satisﬁes
the acceptance condition of A¬φ.
It is possible to implement the check for such a path in terms of CTL
model checking, and this is in fact what NuSMV does. The combined system
M × A¬φ is represented as the system to be model checked in NuSMV,
and the formula to be checked is simply EG ⊤. Thus, we ask the question:
does the combined system have a path. The acceptance conditions of A¬φ
are represented as implicit fairness conditions for the CTL model-checking
procedure. Explicitly, this amounts to asserting ‘FAIRNESS ¬(χ U ψ) ∨ψ’
for each formula χ U ψ occurring in C(φ).
3.7 The fixed-point characterisation of CTL
On page 227, we presented an algorithm which, given a CTL formula φ and
a model M = (S, →, L), computes the set of states s ∈S satisfying φ. We
write this set as [[φ]]. The algorithm works recursively on the structure of
φ. For formulas φ of height 1 (⊥, ⊤or p), [[φ]] is computed directly. Other

3.7 The fixed-point characterisation of CTL
239
formulas are composed of smaller subformulas combined by a connective of
CTL. For example, if φ is ψ1 ∨ψ2, then the algorithm computes the sets
[[ψ1]] and [[ψ2]] and combines them in a certain way (in this case, by taking
the union) in order to obtain [[ψ1 ∨ψ2]].
The more interesting cases arise when we deal with a formula such as
EX ψ, involving a temporal operator. The algorithm computes the set [[ψ]]
and then computes the set of all states which have a transition to a state in
[[ψ]]. This is in accord with the semantics of EX ψ: M, s ⊨EX ψ iﬀthere is
a state s′ with s →s′ and M, s′ ⊨ψ.
For most of these logical operators, we may easily continue this discussion
to see that the algorithms work just as expected. However, the cases EU,
AF and EG (where we needed to iterate a certain labelling policy until it
stabilised) are not so obvious to reason about. The topic of this section is to
develop the semantic insights into these operators that allow us to provide a
complete proof for their termination and correctness. Inspecting the pseudo-
code in Figure 3.28, we see that most of these clauses just do the obvious
and correct thing according to the semantics of CTL. For example, try out
what SAT does when you call it with φ1 →φ2.
Our aim in this section is to prove the termination and correctness
of SATAF and SATEU. In fact, we will also write a procedure SATEG and
prove its termination and correctness1. The procedure SATEG is given in
Figure 3.37 and is based on the intuitions given in Section 3.6.1: note how
deleting the label if none of the successor states is labelled is coded as
intersecting the labelled set with the set of states which have a labelled
successor.
The semantics of EG φ says that s0 ⊨EG φ holds iﬀthere exists a com-
putation path s0 →s1 →s2 →. . . such that si ⊨φ holds for all i ≥0. We
could instead express it as follows: EG φ holds if φ holds and EG φ holds
in one of the successor states to the current state. This suggests the equiv-
alence EG φ ≡φ ∧EX EG φ which can easily be proved from the semantic
deﬁnitions of the connectives.
Observing that [[EX ψ]] = pre∃([[ψ]]) we see that the equivalence above
can be written as [[EG φ]] = [[φ]] ∩pre∃([[EG φ]]). This does not look like a
very promising way of calculating EG φ, because we need to know EG φ in
order to work out the right-hand side. Fortunately, there is a way around
this apparent circularity, known as computing ﬁxed points, and that is the
subject of this section.
1 Section 3.6.1 handles EG φ by translating it into ¬AF ¬φ, but we already noted in Section 3.6.1
that EG could be handled directly.

240
3 Verification by model checking
function SATEG (φ)
/* determines the set of states satisfying EG φ */
local var X, Y
begin
Y := SAT (φ);
X := ∅;
repeat until X = Y
begin
X := Y ;
Y := Y ∩pre∃(Y )
end
return Y
end
Figure 3.37. The pseudo-code for SATEG.
3.7.1 Monotone functions
Deﬁnition 3.22 Let S be a set of states and F : P(S) →P(S) a function
on the power set of S.
1.
We say that F is monotone iﬀX ⊆Y implies F(X) ⊆F(Y ) for all subsets X
and Y of S.
2.
A subset X of S is called a ﬁxed point of F iﬀF(X) = X.
For an example, let S
def
= {s0, s1} and F(Y )
def
= Y ∪{s0} for all subsets Y
of S. Since Y ⊆Y ′ implies Y ∪{s0} ⊆Y ′ ∪{s0}, we see that F is monotone.
The ﬁxed points of F are all subsets of S containing s0. Thus, F has two
ﬁxed points, the sets {s0} and {s0, s1}. Notice that F has a least (= {s0})
and a greatest (= {s0, s1}) ﬁxed point.
An example of a function G: P(S) →P(S), which is not monotone, is
given by
G(Y )
def
= if Y = {s0} then {s1} else {s0}.
So G maps {s0} to {s1} and all other sets to {s0}. The function G is
not monotone since {s0} ⊆{s0, s1} but G({s0}) = {s1} is not a subset of
G({s0, s1}) = {s0}. Note that G has no ﬁxed points whatsoever.
The reasons for exploring monotone functions on P(S) in the context of
proving the correctness of SAT are:
1.
that monotone functions always have a least and a greatest ﬁxed point;
2.
that the meanings of EG, AF and EU can be expressed via greatest, respectively
least, ﬁxed points of monotone functions on P(S);

3.7 The fixed-point characterisation of CTL
241
3.
that these ﬁxed-points can be easily computed, and;
4.
that the procedures SATEU and SATAF code up such ﬁxed-point computations,
and are correct by item 2.
Notation 3.23 F i(X) means
F(F(. . . F



i times
(X) . . . ))
Thus, the function F i is just ‘F applied i many times.’
For example, for the function F(Y )
def
= Y ∪{s0}, we obtain F 2(Y ) =
F(F(Y )) = (Y ∪{s0}) ∪{s0} = Y ∪{s0} = F(Y ). In this case, F 2 = F and
therefore F i = F for all i ≥1. It is not always the case that the sequence of
functions (F 1, F 2, F 3, . . . ) stabilises in such a way. For example, this won’t
happen for the function G deﬁned above (see Exercise 1(d) on page 253).
The following fact is a special case of a fundamental insight, often referred
to as the Knaster–Tarski Theorem.
Theorem 3.24 Let S be a set {s0, s1, . . . , sn} with n + 1 elements. If
F : P(S) →P(S) is a monotone function, then F n+1(∅) is the least ﬁxed
point of F and F n+1(S) is the greatest ﬁxed point of F.
PROOF: Since ∅⊆F(∅), we get F(∅) ⊆F(F(∅)), i.e., F 1(∅) ⊆F 2(∅), for F
is monotone. We can now use mathematical induction to show that
F 1(∅) ⊆F 2(∅) ⊆F 3(∅) ⊆. . . ⊆F i(∅)
for all i ≥1. In particular, taking i
def
= n + 1, we claim that one of the expres-
sions F k(∅) above is already a ﬁxed point of F. Otherwise, F 1(∅) needs to
contain at least one element (for then ∅̸= F(∅)). By the same token, F 2(∅)
needs to have at least two elements since it must be bigger than F 1(∅). Con-
tinuing this argument, we see that F n+2(∅) would have to contain at least
n + 2 many elements. The latter is impossible since S has only n + 1 ele-
ments. Therefore, F(F k(∅)) = F k(∅) for some 0 ≤k ≤n + 1, which readily
implies that F n+1(∅) is a ﬁxed point of F as well.
Now suppose that X is another ﬁxed point of F. We need to show that
F n+1(∅) is a subset of X; but, since ∅⊆X, we conclude F(∅) ⊆F(X) =
X, for F is monotone and X a ﬁxed point of F. By induction, we obtain
F i(∅) ⊆X for all i ≥0. So, for i
def
= n + 1, we get F n+1(∅) ⊆X.
The proof of the statements about the greatest ﬁxed point is dual to the
one above. Simply replace ⊆by ⊇, ∅by S and ‘bigger’ by ‘smaller.’
2

242
3 Verification by model checking
This theorem about the existence of least and greatest ﬁxed points of
monotone functions F : P(S) →P(S) not only asserted the existence of
such ﬁxed points; it also provided a recipe for computing them, and cor-
rectly so. For example, in computing the least ﬁxed point of F, all we have
to do is apply F to the empty set ∅and keep applying F to the result un-
til the latter becomes invariant under the application of F. The theorem
above further ensures that this process is guaranteed to terminate. More-
over, we can specify an upper bound n + 1 to the worst-case number of
iterations necessary for reaching this ﬁxed point, assuming that S has n + 1
elements.
3.7.2 The correctness of SATEG
We saw at the end of the last section that [[EG φ]] = [[φ]] ∩pre∃([[EG φ]]). This
implies that EG φ is a ﬁxed point of the function F(X) = [[φ]] ∩pre∃(X). In
fact, F is monotone, EG φ is its greatest ﬁxed point and therefore EG φ can
be computed using Theorem 3.24.
Theorem 3.25 Let F be as deﬁned above and let S have n + 1 elements.
Then F is monotone, [[EG φ]] is the greatest ﬁxed point of F, and [[EG φ]] =
F n+1(S).
PROOF:
1.
In order to show that F is monotone, we take any two subsets X and Y of S
such that X ⊆Y and we need to show that F(X) is a subset of F(Y ). Given s0
such that there is some s1 ∈X with s0 →s1, we certainly have s0 →s1, where
s1 ∈Y , for X is a subset of Y . Thus, we showed pre∃(X) ⊆pre∃(Y ) from which
we readily conclude that F(X) = [[φ]] ∩pre∃(X) ⊆[[φ]] ∩pre∃(Y ) = F(Y ).
2.
We have already seen that [[EG φ]] is a ﬁxed point of F. To show that it is the
greatest ﬁxed point, it suﬃces to show here that any set X with F(X) = X has
to be contained in [[EG φ]]. So let s0 be an element of such a ﬁxed point X. We
need to show that s0 is in [[EG φ]] as well. For that we use the fact that
s0 ∈X = F(X) = [[φ]] ∩pre∃(X)
to infer that s0 ∈[[φ]] and s0 →s1 for some s1 ∈X; but, since s1 is in X,
we may apply that same argument to s1 ∈X = F(X) = [[φ]] ∩pre∃(X) and we
get s1 ∈[[φ]] and s1 →s2 for some s2 ∈X. By mathematical induction, we can
therefore construct an inﬁnite path s0 →s1 →· · · →sn →sn+1 →. . . such that
si ∈[[φ]] for all i ≥0. By the deﬁnition of [[EG φ]], this entails s0 ∈[[EG φ]].
3.
The last item is now immediately accessible from the previous one and Theo-
rem 3.24.
2

3.7 The fixed-point characterisation of CTL
243
Now we can see that the procedure SATEG is correctly coded and termi-
nates. First, note that the line Y := Y ∩pre∃(Y ) in the procedure SATEG
(Figure 3.37) could be changed to Y := SAT(φ) ∩pre∃(Y ) without changing
the eﬀect of the procedure. To see this, note that the ﬁrst time round the
loop, Y is SAT(φ); and in subsequent loops, Y ⊆SAT(φ), so it doesn’t matter
whether we intersect with Y or SAT(φ)2. With the change, it is clear that
SATEG is calculating the greatest ﬁxed point of F; therefore its correctness
follows from Theorem 3.25.
3.7.3 The correctness of SATEU
Proving the correctness of SATEU is similar. We start by noting the equiv-
alence E[φ U ψ] ≡ψ ∨(φ ∧EX E[φ U ψ]) and we write it as [[E[φ U ψ]]] =
[[ψ]] ∪([[φ]] ∩pre∃[[E[φ U ψ]]]). That tells us that [[E[φ U ψ]]] is a ﬁxed point
of the function G(X) = [[ψ]] ∪([[φ]] ∩pre∃(X)). As before, we can prove that
this function is monotone. It turns out that [[E[φ U ψ]]] is its least ﬁxed
point and that the function SATEU is actually computing it in the manner of
Theorem 3.24.
Theorem 3.26 Let G be deﬁned as above and let S have n + 1 elements.
Then G is monotone, [[E(φ U ψ)]] is the least ﬁxed point of G, and we have
[[E(φ U ψ)]] = Gn+1(∅).
2 If you are sceptical, try computing the values Y0, Y1, Y2, . . . , where Yi represents the value of Y
after i iterations round the loop. The program before the change computes as follows:
Y0 = SAT(φ)
Y1 = Y0 ∩pre∃(Y0)
Y2 = Y1 ∩pre∃(Y1)
= Y0 ∩pre∃(Y0) ∩pre∃(Y0 ∩pre∃(Y0))
= Y0 ∩pre∃(Y0 ∩pre∃(Y0)).
The last of these equalities follows from the monotonicity of pre∃.
Y3 = Y2 ∩pre∃(Y2)
= Y0 ∩pre∃(Y0 ∩pre∃(Y0)) ∩pre∃(Y0 ∩pre∃(Y0 ∩pre∃(Y0)))
= Y0 ∩pre∃(Y0 ∩pre∃(Y0 ∩pre∃(Y0))).
Again the last one follows by monotonicity. Now look at what the program does after the change:
Y0 = SAT(φ)
Y1 = SAT(φ) ∩pre∃(Y0)
= Y0 ∩pre∃(Y0)
Y2 = Y0 ∩pre∃(Y1)
Y3 = Y0 ∩pre∃(Y1)
= Y0 ∩pre∃(Y0 ∩pre∃(Y0)).
A formal proof would follow by induction on i.

244
3 Verification by model checking
PROOF:
1.
Again, we need to show that X ⊆Y implies G(X) ⊆G(Y ); but that is essen-
tially the same argument as for F, since the function which sends X to pre∃(X)
is monotone and all that G now does is to perform the intersection and union
of that set with constant sets [[φ]] and [[ψ]].
2.
If S has n + 1 elements, then the least ﬁxed point of G equals Gn+1(∅) by
Theorem 3.24. Therefore it suﬃces to show that this set equals [[E(φ U ψ)]].
Simply observe what kind of states we obtain by iterating G on the empty set
∅: G1(∅) = [[ψ]] ∪([[φ]] ∩pre∃([[∅]])) = [[ψ]] ∪([[φ]] ∩∅) = [[ψ]] ∪∅= [[ψ]], which are
all states s0 ∈[[E(φ U ψ)]], where we chose i = 0 according to the deﬁnition of
Until. Now,
G2(∅) = [[ψ]] ∪([[φ]] ∩pre∃(G1(∅)))
tells us that the elements of G2(∅) are all those s0 ∈[[E(φ U ψ)]] where we chose
i ≤1. By mathematical induction, we see that Gk+1(∅) is the set of all states
s0 for which we chose i ≤k to secure s0 ∈[[E(φ U ψ)]]. Since this holds for all
k, we see that [[E(φ U ψ)]] is nothing but the union of all sets Gk+1(∅) with
k ≥0; but, since Gn+1(∅) is a ﬁxed point of G, we see that this union is just
Gn+1(∅).
2
The correctness of the coding of SATEU follows similarly to that of
SATEG. We change the line Y := Y ∪(W ∩pre∃(Y )) into Y := SAT(ψ) ∪
(W ∩pre∃(Y )) and observe that this does not change the result of the pro-
cedure, because the ﬁrst time round the loop, Y is SAT(ψ); and, since Y is
always increasing, it makes no diﬀerence whether we perform a union with
Y or with SAT(ψ). Having made that change, it is then clear that SATEU is
just computing the least ﬁxed point of G using Theorem 3.24.
We illustrate these results about the functions F
and G above
through an example. Consider the system in Figure 3.38. We begin
by computing the set [[EF p]]. By the deﬁnition of EF
this is just
[[E(⊤U p)]]. So we have φ1
def
= ⊤and φ2
def
= p. From Figure 3.38, we ob-
tain [[p]] = {s3} and of course [[⊤]] = S. Thus, the function G above
equals G(X) = {s3} ∪pre∃(X). Since [[E(⊤U p)]] equals the least ﬁxed
point of G, we need to iterate G on ∅until this process stabilises.
First, G1(∅) = {s3} ∪pre∃(∅) = {s3}. Second, G2(∅) = G(G1(∅)) = {s3} ∪
pre∃({s3}) = {s1, s3}. Third, G3(∅) = G(G2(∅)) = {s3} ∪pre∃({s1, s3}) =
{s0, s1, s2, s3}. Fourth, G4(∅) = G(G3(∅)) = {s3} ∪pre∃({s0, s1, s2, s3}) =
{s0, s1, s2, s3}. Therefore, {s0, s1, s2, s3} is the least ﬁxed point of G,
which equals [[E(⊤U p)]] by Theorem 3.20. But then [[E(⊤U p)]] =
[[EF p]].

3.8 Exercises
245
s0
s1
s4
q
q
p
s3
s2
Figure 3.38. A system for which we compute invariants.
The other example we study is the computation of the set [[EG q]]. By
Theorem 3.25, that set is the greatest ﬁxed point of the function F above,
where φ
def
= q. From Figure 3.38 we see that [[q]] = {s0, s4} and so F(X) =
[[q]] ∩pre∃(X) = {s0, s4} ∩pre∃(X). Since [[EG q]] equals the greatest ﬁxed
point of F, we need to iterate F on S until this process stabilises. First,
F 1(S) = {s0, s4} ∩pre∃(S) = {s0, s4} ∩S since every s has some s′ with s →
s′. Thus, F 1(S) = {s0, s4}.
Second, F 2(S) = F(F 1(S)) = {s0, s4} ∩pre∃({s0, s4}) = {s0, s4}. There-
fore, {s0, s4} is the greatest ﬁxed point of F, which equals [[EG q]] by Theo-
rem 3.25.
3.8 Exercises
Exercises 3.1
1. Read Section 2.7 in case you have not yet done so and classify Alloy and its
constraint analyser according to the classiﬁcation criteria for formal methods
proposed on page 172.
2. Visit and browse the websites3 and4 to ﬁnd formal methods that interest you for
whatever reason. Then classify them according to the criteria from page 172.
Exercises 3.2
1. Draw parse trees for the LTL formulas:
(a) F p ∧G q →p W r
(b) F (p →G r) ∨¬q U p
(c) p W (q W r)
(d) G F p →F (q ∨s)
3 www.afm.sbu.ac.uk
4 www.cs.indiana.edu/formal-methods-education/

246
3 Verification by model checking
q3
q1
q2
ab
q4
ab
ab
ab
Figure 3.39. A model M.
2. Consider the system of Figure 3.39. For each of the formulas φ:
(a) G a
(b) a U b
(c) a U X (a ∧¬b)
(d) X ¬b ∧G (¬a ∨¬b)
(e) X (a ∧b) ∧F (¬a ∧¬b)
(i) Find a path from the initial state q3 which satisﬁes φ.
(ii) Determine whether M, q3 ⊨φ.
3. Working from the clauses of Deﬁnition 3.1 (page 175), prove the equivalences:
φ U ψ ≡φ W ψ ∧F ψ
φ W ψ ≡φ U ψ ∨G φ
φ W ψ ≡ψ R (φ ∨ψ)
φ R ψ ≡ψ W (φ ∧ψ) .
4. Prove that φ U ψ ≡ψ R (φ ∨ψ) ∧F ψ.
5. List all subformulas of the LTL formula ¬p U (F r ∨G ¬q →q W ¬r).
6. ‘Morally’ there ought to be a dual for W. Work out what it might mean, and
then pick a symbol based on the ﬁrst letter of the meaning.
7. Prove that for all paths π of all models, π ⊨φ W ψ ∧F ψ implies π ⊨φ U ψ.
That is, prove the remaining half of equivalence (3.2) on page 185.
8. Recall the algorithm NNF on page 62 which computes the negation normal form
of propositional logic formulas. Extend this algorithm to LTL: you need to add
program clauses for the additional connectives X, F, G and U, R and W; these
clauses have to animate the semantic equivalences that we presented in this
section.

3.8 Exercises
247
Exercises 3.3
1. Consider the model in Figure 3.9 (page 193).
(a)
*
Verify that G(req -> F busy) holds in all initial states.
(b) Does ¬(req U ¬busy) hold in all initial states of that model?
(c) NuSMV has the capability of referring to the next value of a declared vari-
able v by writing next(v). Consider the model obtained from Figure 3.9
by removing the self-loop on state !req & busy. Use the NuSMV feature
next(...) to code that modiﬁed model as an NuSMV program with the
speciﬁcation G(req -> F busy). Then run it.
2. Verify Remark 3.11 from page 190.
3.
*
Draw the transition system described by the ABP program.
Remarks: There are 28 reachable states of the ABP program. (Looking at the
program, you can see that the state is described by nine boolean variables, namely
S.st, S.message1, S.message2, R.st, R.ack, R.expected, msg chan.output1,
msg chan.output2 and ﬁnally ack chan.output. Therefore, there are 29 = 512
states in total. However, only 28 of them can be reached from the initial state
by following a ﬁnite path.)
If you abstract away from the contents of the message (e.g., by setting
S.message1 and msg chan.output1 to be constant 0), then there are only 12
reachable states. This is what you are asked to draw.
Exercises 3.4
1. Write the parse trees for the following CTL formulas:
(a)
*
EG r
(b)
*
AG (q →EG r)
(c)
*
A[p U EF r]
(d)
*
EF EG p →AF r, recall Convention 3.13
(e) A[p U A[q U r]]
(f) E[A[p U q] U r]
(g) AG (p →A[p U (¬p ∧A[¬p U q])]).
2. Explain why the following are not well-formed CTL formulas:
(a)
*
F G r
(b) X X r
(c) A¬G ¬p
(d) F [r U q]
(e) EX X r
(f)
*
AEF r
(g)
*
AF [(r U q) ∧(p U r)].
3. State which of the strings below are well-formed CTL formulas. For those which
are well-formed, draw the parse tree. For those which are not well-formed,
explain why not.

248
3 Verification by model checking
r
p, q
q, r
s3
p, t, r
s1
s2
s0
Figure 3.40. A model with four states.
(a) ¬(¬p) ∨(r ∧s)
(b) X q
(c)
*
¬AX q
(d) p U (AX ⊥)
(e)
*
E[(AX q) U (¬(¬p) ∨(⊤∧s))]
(f)
*
(F r) ∧(AG q)
(g) ¬(AG q) ∨(EG q).
4.
*
List all subformulas of the formula AG (p →A[p U (¬p ∧A[¬p U q])]).
5. Does E[req U ¬busy] hold in all initial states of the model in Figure 3.9 on
page 193?
6. Consider the system M in Figure 3.40.
(a) Beginning from state s0, unwind this system into an inﬁnite tree, and draw
all computation paths up to length 4 (= the ﬁrst four layers of that tree).
(b) Determine whether M, s0 ⊨φ and M, s2 ⊨φ hold and justify your answer,
where φ is the LTL or CTL formula:
(i)
*
¬p →r
(ii) F t
(iii)
*
¬EG r
(iv) E (t U q)
(v) F q
(vi) EF q
(vii) EG r
(viii) G (r ∨q).
7. Let M = (S, →, L) be any model for CTL and let [[φ]] denote the set of all s ∈S
such that M, s ⊨φ. Prove the following set identities by inspecting the clauses
of Deﬁnition 3.15 from page 211.
(a)
*
[[⊤]] = S,
(b) [[⊥]] = ∅

3.8 Exercises
249
r
p, q
q, r
p, t
s2
s3
s1
s0
Figure 3.41. Another model with four states.
(c) [[¬φ]] = S −[[φ]],
(d) [[φ1 ∧φ2]] = [[φ1]] ∩[[φ2]]
(e) [[φ1 ∨φ2]] = [[φ1]] ∪[[φ2]]
(f)
*
[[φ1 →φ2]] = (S −[[φ1]]) ∪[[φ2]]
(g)
*
[[AX φ]] = S −[[EX ¬φ]]
(h) [[A(φ2 U φ2)]] = [[¬(E(¬φ1 U (¬φ1 ∧¬φ2)) ∨EG ¬φ2)]].
8. Consider the model M in Figure 3.41. Check whether M, s0 ⊨φ and M, s2 ⊨φ
hold for the CTL formulas φ:
(a) AF q
(b) AG (EF (p ∨r))
(c) EX (EX r)
(d) AG (AF q).
9.
*
The meaning of the temporal operators F, G and U in LTL and AU, EU, AG,
EG, AF and EF in CTL was deﬁned to be such that ‘the present includes the
future.’ For example, EF p is true for a state if p is true for that state already.
Often one would like corresponding operators such that the future excludes the
present. Use suitable connectives of the grammar on page 208 to deﬁne such
(six) modiﬁed connectives as derived operators in CTL.
10. Which of the following pairs of CTL formulas are equivalent? For those which
are not, exhibit a model of one of the pair which is not a model of the
other:
(a) EF φ and EG φ
(b)
*
EF φ ∨EF ψ and EF (φ ∨ψ)
(c)
*
AF φ ∨AF ψ and AF (φ ∨ψ)
(d) AF ¬φ and ¬EG φ
(e)
*
EF ¬φ and ¬AF φ
(f) A[φ1 U A[φ2 U φ3]] and A[A[φ1 U φ2] U φ3], hint: it might make it simpler
if you think ﬁrst about models that have just one path
(g) ⊤and AG φ →EG φ
(h)
*
⊤and EG φ →AG φ.
11. Find operators to replace the ?, to make the following equivalences:

250
3 Verification by model checking
(a)
*
AG (φ ∧ψ) ≡AG φ ? AG ψ
(b) EF ¬φ ≡¬??φ
12. State explicitly the meaning of the temporal connectives AR etc., as deﬁned on
page 217.
13. Prove the equivalences (3.6) on page 216.
14.
*
Write pseudo-code for a recursive function TRANSLATE which takes as input
an arbitrary CTL formula φ and returns as output an equivalent CTL formula
ψ whose only operators are among the set {⊥, ¬, ∧, AF , EU , EX }.
Exercises 3.5
1. Express the following properties in CTL and LTL whenever possible. If neither
is possible, try to express the property in CTL*:
(a)
*
Whenever p is followed by q (after ﬁnitely many steps), then the system
enters an ‘interval’ in which no r occurs until t.
(b) Event p precedes s and t on all computation paths. (You may ﬁnd it easier
to code the negation of that speciﬁcation ﬁrst.)
(c) After p, q is never true. (Where this constraint is meant to apply on all
computation paths.)
(d) Between the events q and r, event p is never true.
(e) Transitions to states satisfying p occur at most twice.
(f)
*
Property p is true for every second state along a path.
2. Explain in detail why the LTL and CTL formulas for the practical speciﬁcation
patterns of pages 183 and 215 capture the stated ‘informal’ properties expressed
in plain English.
3. Consider the set of LTL/CTL formulas F = {F p →F q, AF p →AF q, AG (p →
AF q)}.
(a) Is there a model such that all formulas hold in it?
(b) For each φ ∈F, is there a model such that φ is the only formula in F satisﬁed
in that model?
(c) Find a model in which no formula of F holds.
4. Consider the CTL formula AG (p →AF (s ∧AX (AF t))). Explain what exactly
it expresses in terms of the order of occurrence of events p, s and t.
5. Extend the algorithm NNF from page 62 which computes the negation normal
form of propositional logic formulas to CTL*. Since CTL* is deﬁned in terms
of two syntactic categories (state formulas and path formulas), this requires two
separate versions of NNF which call each other in a way that is reﬂected by the
syntax of CTL* given on page 218.
6. Find a transition system which distinguishes the following pairs of CTL* formu-
las, i.e., show that they are not equivalent:
(a) AF G p and AF AG p
(b)
*
AG F p and AG EF p
(c) A[(p U r) ∨(q U r)] and A[(p ∨q) U r)]

3.8 Exercises
251
(d)
*
A[X p ∨X X p] and AX p ∨AX AX p
(e) E[G F p] and EG EF p.
7. The translation from CTL with boolean combinations of path formulas to plain
CTL introduced in Section 3.5.1 is not complete. Invent CTL equivalents for:
(a)
*
E[F p ∧(q U r)]
(b)
*
E[F p ∧G q].
In this way, we have dealt with all formulas of the form E[φ ∧ψ]. Formulas of the
form E[φ ∨ψ] can be rewritten as E[φ] ∨E[ψ] and A[φ] can be written ¬E[¬φ].
Use this translation to write the following in CTL:
(c) E[(p U q) ∧F p]
(d)
*
A[(p U q) ∧G p]
(e)
*
A[F p →F q].
8. The aim of this exercise is to demonstrate the expansion given for AW at the
end of the last section, i.e., A[p W q] ≡¬E[¬q U ¬(p ∨q)].
(a) Show that the following LTL formulas are valid (i.e., true in any state of any
model):
(i) ¬q U (¬p ∧¬q) →¬G p
(ii) G ¬q ∧F ¬p →¬q U (¬p ∧¬q).
(b) Expand ¬((p U q) ∨G p) using de Morgan rules and the LTL equivalence
¬(φ U ψ) ≡(¬ψ U (¬φ ∧¬ψ)) ∨¬F ψ.
(c) Using your expansion and the facts (i) and (ii) above, show ¬((p U q) ∨
G p) ≡¬q U ¬(p ∧q) and hence show that the desired expansion of AW
above is correct.
Exercises 3.6
1.
*
Verify φ1 to φ4 for the transition system given in Figure 3.11 on page 198. Which
of them require the fairness constraints of the SMV program in Figure 3.10?
2. Try to write a CTL formula that enforces non-blocking and no-strict-sequencing
at the same time, for the SMV program in Figure 3.10 (page 196).
3.
*
Apply the labelling algorithm to check the formulas φ1, φ2, φ3 and φ4 of the
mutual exclusion model in Figure 3.7 (page 188).
4. Apply the labelling algorithm to check the formulas φ1, φ2, φ3 and φ4 of the
mutual exclusion model in Figure 3.8 (page 191).
5. Prove that (3.8) on page 228 holds in all models. Does your proof require that
for every state s there is some state s′ with s →s′?
6. Inspecting the deﬁnition of the labelling algorithm, explain what happens if you
perform it on the formula p ∧¬p (in any state, in any model).
7. Modify the pseudo-code for SAT on page 227 by writing a special procedure for
AG ψ1, without rewriting it in terms of other formulas5.
5 Question: will your routine be more like the routine for AF, or more like that for EG on page 224?
Why?

252
3 Verification by model checking
8.
*
Write the pseudo-code for SATEG, based on the description in terms of deleting
labels given in Section 3.6.1.
9.
*
For mutual exclusion, draw a transition system which forces the two processes
to enter their critical section in strict sequence and show that φ4 is false of its
initial state.
10. Use the deﬁnition of ⊨between states and CTL formulas to explain why s ⊨
AG AF φ means that φ is true inﬁnitely often along every path starting at s.
11.
*
Show that a CTL formula φ is true on inﬁnitely many states of a computa-
tion path s0 →s1 →s2 →. . . iﬀfor all n ≥0 there is some m ≥n such that
sm ⊨φ.
12. Run the NuSMV system on some examples. Try commenting out, or deleting,
some of the fairness constraints, if applicable, and see the counter examples
NuSMV generates. NuSMV is very easy to run.
13. In the one-bit channel, there are two fairness constraints. We could have written
this as a single one, inserting ‘&’ between running and the long formula, or we
could have separated the long formula into two and made it into a total of three
fairness constraints.
In general, what is the diﬀerence between the single fairness constraint φ1 ∧φ2 ∧
· · · ∧φn and the n fairness constraints φ1, φ2, . . . , φn? Write an SMV program
with a fairness constraint a & b which is not equivalent to the two fairness
constraints a and b. (You can actually do it in four lines of SMV.)
14. Explain the construction of formula φ4, used to express that the processes need
not enter their critical section in strict sequence. Does it rely on the fact that
the safety property φ1 holds?
15.
*
Compute the ECG ⊤labels for Figure 3.11, given the fairness constraints of the
code in Figure 3.10 on page 196.
Exercises 3.7
1. Consider the functions
H1, H2, H3 : P({1, 2, 3, 4, 5, 6, 7, 8, 9, 10}) →P({1, 2, 3, 4, 5, 6, 7, 8, 9, 10})
deﬁned by
H1(Y )
def
= Y −{1, 4, 7}
H2(Y )
def
= {2, 5, 9} −Y
H3(Y )
def
= {1, 2, 3, 4, 5} ∩({2, 4, 8} ∪Y )
for all Y ⊆{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}.
(a)
*
Which of these functions are monotone; which ones aren’t? Justify your an-
swer in each case.
(b)
*
Compute the least and greatest ﬁxed points of H3 using the iterations Hi
3
with i = 1, 2, . . . and Theorem 3.24.

3.8 Exercises
253
q
p
q
q
Figure 3.42. Another system for which we compute invariants.
(c) Does H2 have any ﬁxed points?
(d) Recall G: P({s0, s1}) →P({s0, s1}) with
G(Y )
def
= if Y = {s0} then {s1} else {s0} .
Use mathematical induction to show that Gi equals G for all odd numbers
i ≥1. What does Gi look like for even numbers i?
2.
*
Let A and B be two subsets of S and let F : P(S) →P(S) be a monotone
function. Show that:
(a) F1 : P(S) →P(S) with F1(Y )
def
= A ∩F(Y ) is monotone;
(b) F2 : P(S) →P(S) with F2(Y )
def
= A ∪(B ∩F(Y )) is monotone.
3. Use Theorems 3.25 and 3.26 to compute the following sets (the underlying model
is in Figure 3.42):
(a) [[EF p]]
(b) [[EG q]].
4. Using the function F(X) = [[φ]] ∪pre∀(X) prove that [[AF φ]] is the least ﬁxed
point of F. Hence argue that the procedure SATAF is correct and terminates.
5.
*
One may also compute AG φ directly as a ﬁxed point. Consider the function
H : P(S) →P(S) with H(X) = [[φ]] ∩pre∀(X). Show that H is monotone and
that [[AG φ]] is the greatest ﬁxed point of H. Use that insight to write a procedure
SATAG.
6. Similarly, one may compute A[φ1 U φ2] directly as a ﬁxed point, using
K : P(S) →P(S), where K(X) = [[φ2]] ∪([[φ1]] ∩pre∀(X)). Show that K is
monotone and that [[A[φ1 U φ2]]] is the least ﬁxed point of K. Use that insight
to write a procedure SATAU. Can you use that routine to handle all calls of the
form AF φ as well?
7. Prove that [[A[φ1 U φ2]]] = [[φ2 ∨(φ1 ∧AX (A[φ1 U φ2]))]].
8. Prove that [[AG φ]] = [[φ ∧AX (AG φ)]].
9. Show that the repeat-statements in the code for SATEU and SATEG always termi-
nate. Use this fact to reason informally that the main program SAT terminates
for all valid CTL formulas φ. Note that some subclauses, like the one for AU,
call SAT recursively and with a more complex formula. Why does this not aﬀect
termination?

254
3 Verification by model checking
3.9 Bibliographic notes
Temporal logic was invented by the philosopher A. Prior in the 1960s; his
logic was similar to what we now call LTL. The ﬁrst use of temporal logic for
reasoning about concurrent programs was by A. Pnueli [Pnu81]. The logic
CTL was invented by E. Clarke and E. A. Emerson (during the early 1980s);
and CTL* was invented by E. A. Emerson and J. Halpern (in 1986) to unify
CTL and LTL.
CTL model checking was invented by E. Clarke and E. A. Emerson [CE81]
and by J. Quielle and J. Sifakis [QS81]. The technique we described for LTL
model checking was invented by M. Vardi and P. Wolper [VW84]. Surveys
of some of these ideas can be found in [CGL93] and [CGP99]. The theorem
about adequate sets of CTL connectives is proved in [Mar01].
The original SMV system was written by K. McMillan [McM93] and is
available with source code from Carnegie Mellon University6. NuSMV7 is a
reimplementation, developed in Trento by A. Cimatti, and M. Roveri and is
aimed at being customisable and extensible. Extensive documentation about
NuSMV can be found at that site. NuSMV supports essentially the same
system description language as CMU SMV, but it has an improved user in-
terface and a greater variety of algorithms. For example, whereas CMU SMV
checks only CTL speciﬁcation, NuSMV supports LTL and CTL. NuSMV im-
plements bounded model checking [BCCZ99]. Cadence SMV8 is an entirely
new model checker focused on compositional systems and abstraction as
ways of addressing the state explosion problem. It was also developed by
K. McMillan and its description language resembles but much extends the
original SMV.
A website which gathers frequently used speciﬁcation patterns in various
frameworks (such as CTL, LTL and regular expressions) is maintained by
M. Dwyer, G. Avrunin, J. Corbett and L. Dillon9.
Current research in model checking includes attempts to exploit abstrac-
tions, symmetries and compositionality [CGL94, Lon83, Dam96] in order to
reduce the impact of the state explosion problem.
The model checker Spin, which is geared towards asynchronous systems
and is based on the temporal logic LTL, can be found at the Spin website10. A
model checker called FDR2 based on the process algebra CSP is available11.
6 www.cs.cmu.edu/~modelcheck/
7 nusmv.irst.itc.it
8 www-cad.eecs.berkeley.edu/~kenmcmil/
9 patterns.projects.cis.ksu.edu/
10 netlib.bell-labs.com/netlib/spin/whatispin.html
11 www.fsel.com.fdr2 download.html

3.9 Bibliographic notes
255
The Edinburgh Concurrency Workbench12 and the Concurrency Workbench
of North Carolina13 are similar software tools for the design and analysis of
concurrent systems. An example of a customisable and extensible modular
model checking frameworks for the veriﬁcation of concurrent software is
Bogor14.
There are many textbooks about veriﬁcation of reactive systems; we men-
tion [MP91, MP95, Ros97, Hol90]. The SMV code contained in this chapter
can be downloaded from www.cs.bham.ac.uk/research/lics/.
12 www.dcs.ed.ac.uk/home/cwb
13 www.cs.sunysb.edu/~cwb
14 http://bogor.projects.cis.ksu.edu/

4
Program verification
The methods of the previous chapter are suitable for verifying systems of
communicating processes, where control is the main issue, but there are no
complex data. We relied on the fact that those (abstracted) systems are
in a ﬁnite state. These assumptions are not valid for sequential programs
running on a single processor, the topic of this chapter. In those cases, the
programs may manipulate non-trivial data and – once we admit variables of
type integer, list, or tree – we are in the domain of machines with inﬁnite
state space.
In terms of the classiﬁcation of veriﬁcation methods given at the beginning
of the last chapter, the methods of this chapter are
Proof-based. We do not exhaustively check every state that the system
can get in to, as one does with model checking; this would be impossi-
ble, given that program variables can have inﬁnitely many interacting
values. Instead, we construct a proof that the system satisﬁes the prop-
erty at hand, using a proof calculus. This is analogous to the situation
in Chapter 2, where using a suitable proof calculus avoided the prob-
lem of having to check inﬁnitely many models of a set of predicate logic
formulas in order to establish the validity of a sequent.
Semi-automatic. Although many of the steps involved in proving that
a program satisﬁes its speciﬁcation are mechanical, there are some steps
that involve some intelligence and that cannot be carried out algorith-
mically by a computer. As we will see, there are often good heuristics
to help the programmer complete these tasks. This contrasts with the
situation of the last chapter, which was fully automatic.
Property-oriented. Just like in the previous chapter, we verify proper-
ties of a program rather than a full speciﬁcation of its behaviour.
256

4.1 Why should we specify and verify code?
257
Application domain. The domain of application in this chapter is se-
quential transformational programs. ‘Sequential’ means that we assume
the program runs on a single processor and that there are no concur-
rency issues. ‘Transformational’ means that the program takes an input
and, after some computation, is expected to terminate with an output.
For example, methods of objects in Java are often programmed in this
style. This contrasts with the previous chapter which focuses on reactive
systems that are not intended to terminate and that react continually
with their environment.
Pre/post-development. The techniques of this chapter should be used
during the coding process for small fragments of program that perform
an identiﬁable (and hence, speciﬁable) task and hence should be used
during the development process in order to avoid functional bugs.
4.1 Why should we specify and verify code?
The task of specifying and verifying code is often perceived as an unwel-
come addition to the programmer’s job and a dispensable one. Arguments
in favour of veriﬁcation include the following:
r Documentation: The speciﬁcation of a program is an important component
in its documentation and the process of documenting a program may raise or
resolve important issues. The logical structure of the formal speciﬁcation, written
as a formula in a suitable logic, typically serves as a guiding principle in trying
to write an implementation in which it holds.
r Time-to-market: Debugging big systems during the testing phase is costly and
time-consuming and local ‘ﬁxes’ often introduce new bugs at other places. Ex-
perience has shown that verifying programs with respect to formal speciﬁcations
can signiﬁcantly cut down the duration of software development and maintenance
by eliminating most errors in the planning phase and helping in the clariﬁcation
of the roles and structural aspects of system components.
r Refactoring: Properly speciﬁed and veriﬁed software is easier to reuse, since
we have a clear speciﬁcation of what it is meant to do.
r Certiﬁcation audits: Safety-critical computer systems – such as the control
of cooling systems in nuclear power stations, or cockpits of modern aircrafts –
demand that their software be speciﬁed and veriﬁed with as much rigour and
formality as possible. Other programs may be commercially critical, such as ac-
countancy software used by banks, and they should be delivered with a warranty:
a guarantee for correct performance within proper use. The proof that a program
meets its speciﬁcations is indeed such a warranty.

258
4 Program verification
The degree to which the software industry accepts the beneﬁts of proper
veriﬁcation of code depends on the perceived extra cost of producing it and
the perceived beneﬁts of having it. As veriﬁcation technology improves, the
costs are declining; and as the complexity of software and the extent to which
society depends on it increase, the beneﬁts are becoming more important.
Thus, we can expect that the importance of veriﬁcation to industry will
continue to increase over the next decades. Microsoft’s emergent technology
A# combines program veriﬁcation, testing, and model-checking techniques
in an integrated in-house development environment.
Currently, many companies struggle with a legacy of ancient code with-
out proper documentation which has to be adapted to new hardware and
network environments, as well as ever-changing requirements. Often, the
original programmers who might still remember what certain pieces of code
are for have moved, or died. Software systems now often have a longer
life-expectancy than humans, which necessitates a durable, transparent and
portable design and implementation process; the year-2000 problem was just
one such example. Software veriﬁcation provides some of this.
4.2 A framework for software verification
Suppose you are working for a software company and your task is to write
programs which are meant to solve sophisticated problems, or computations.
Typically, such a project involves an outside customer – a utility company,
for example – who has written up an informal description, in plain English,
of the real-world task that is at hand. In this case, it could be the devel-
opment and maintenance of a database of electricity accounts with all the
possible applications of that – automated billing, customer service etc. Since
the informality of such descriptions may cause ambiguities which eventually
could result in serious and expensive design ﬂaws, it is desirable to condense
all the requirements of such a project into formal speciﬁcations. These formal
speciﬁcations are usually symbolic encodings of real-world constraints into
some sort of logic. Thus, a framework for producing the software could be:
r Convert the informal description R of requirements for an application domain
into an ‘equivalent’ formula φR of some symbolic logic;
r Write a program P which is meant to realise φR in the programming environment
supplied by your company, or wanted by the particular customer;
r Prove that the program P satisﬁes the formula φR.
This scheme is quite crude – for example, constraints may be actual design
decisions for interfaces and data types, or the speciﬁcation may ‘evolve’

4.2 A framework for software verification
259
and may partly be ‘unknown’ in big projects – but it serves well as a ﬁrst
approximation to trying to deﬁne good programming methodology. Several
variations of such a sequence of activities are conceivable. For example,
you, as a programmer, might have been given only the formula φR, so you
might have little if any insight into the real-world problem which you are
supposed to solve. Technically, this poses no problem, but often it is handy
to have both informal and formal descriptions available. Moreover, crafting
the informal requirements R is often a mutual process between the client
and the programmer, whereby the attempt at formalising R can uncover
ambiguities or undesired consequences and hence lead to revisions of R.
This ‘going back and forth’ between the realms of informal and formal
speciﬁcations is necessary since it is impossible to ‘verify’ whether an infor-
mal requirement R is equivalent to a formal description φR. The meaning
of R as a piece of natural language is grounded in common sense and gen-
eral knowledge about the real-world domain and often based on heuristics
or quantitative reasoning. The meaning of a logic formula φR, on the other
hand, is deﬁned in a precise mathematical, qualitative and compositional
way by structural induction on the parse tree of φR – the ﬁrst three chap-
ters contain examples of this.
Thus, the process of ﬁnding a suitable formalisation φR of R requires
the utmost care; otherwise it is always possible that φR speciﬁes behaviour
which is diﬀerent from the one described in R. To make matters worse, the
requirements R are often inconsistent; customers usually have a fairly vague
conception of what exactly a program should do for them. Thus, producing
a clear and coherent description R of the requirements for an application do-
main is already a crucial step in successful programming; this phase ideally is
undertaken by customers and project managers around a table, or in a video
conference, talking to each other. We address this ﬁrst item only implicitly
in this text, but you should certainly be aware of its importance in practice.
The next phase of the software development framework involves construct-
ing the program P and after that the last task is to verify that P satisﬁes φR.
Here again, our framework is oversimplifying what goes on in practice, since
often proving that P satisﬁes its speciﬁcation φR goes hand-in-hand with
inventing a suitable P. This correspondence between proving and program-
ming can be stated quite precisely, but that is beyond the scope of this book.
4.2.1 A core programming language
The programming language which we set out to study here is the typical
core language of most imperative programming languages. Modulo trivial

260
4 Program verification
syntactic variations, it is a subset of Pascal, C, C++ and Java. Our lan-
guage consists of assignments to integer- and boolean-valued variables, if-
statements, while-statements and sequential compositions. Everything that
can be computed by large languages like C and Java can also be computed
by our language, though perhaps not as conveniently, because it does not
have any objects, procedures, threads or recursive data structures. While
this makes it seem unrealistic compared with fully blown commercial lan-
guages, it allows us to focus our discussion on the process of formal program
veriﬁcation. The features missing from our language could be implemented
on top of it; that is the justiﬁcation for saying that they do not add to the
power of the language, but only to the convenience of using it. Verifying
programs using those features would require non-trivial extensions of the
proof calculus we present here. In particular, dynamic scoping of variables
presents hard problems for program-veriﬁcation methods, but this is beyond
the scope of this book.
Our core language has three syntactic domains: integer expressions,
boolean expressions and commands – the latter we consider to be our
programs. Integer expressions are built in the familiar way from variables
x, y, z, . . . , numerals 0, 1, 2, . . . , −1, −2, . . . and basic operations like addition
(+) and multiplication (∗). For example,
5
x
4 + (x −3)
x + (x ∗(y −(5 + z)))
are all valid integer expressions. Our grammar for generating integer expres-
sions is
E ::=
n | x | (−E) | (E + E) | (E −E) | (E ∗E)
(4.1)
where n is any numeral in {. . . , −2, −1, 0, 1, 2, . . . } and x is any variable.
Note that we write multiplication in ‘mathematics’ as 2 · 3, whereas our
core language writes 2 ∗3 instead.
Convention 4.1 In the grammar above, negation −binds more tightly
than multiplication ∗, which binds more tightly than subtraction −and
addition +.
Since if-statements and while-statements contain conditions in them, we
also need a syntactic domain B of boolean expressions. The grammar in

4.2 A framework for software verification
261
Backus Naur form
B ::= true | false | (!B) | (B & B)
| (B || B) | (E < E)
(4.2)
uses ! for the negation, & for conjunction and || for disjunction of
boolean expressions. This grammar may be freely expanded by operators
which are deﬁnable in terms of the above. For example, the test for equal-
ity1 E1 == E2 may be expressed via !(E1 < E2) & !(E2 < E1). We gener-
ally make use of shorthand notation whenever this is convenient. We also
write (E1 != E2) to abbreviate !(E1 == E2). We will also assume the usual
binding priorities for logical operators stated in Convention 1.3 on page 5.
Boolean expressions are built on top of integer expressions since the last
clause of (4.2) mentions integer expressions.
Having integer and boolean expressions at hand, we can now deﬁne the
syntactic domain of commands. Since commands are built from simpler com-
mands using assignments and the control structures, you may think of com-
mands as the actual programs. We choose as grammar for commands
C
::=
x = E | C; C | if B {C} else {C} | while B {C}
(4.3)
where the braces { and } are to mark the extent of the blocks of code in the
if-statement and the while-statement, as in languages such as C and Java.
They can be omitted if the blocks consist of a single statement. The intuitive
meaning of the programming constructs is the following:
1.
The atomic command x = E is the usual assignment statement; it evaluates
the integer expression E in the current state of the store and then overwrites
the current value stored in x with the result of that evaluation.
2.
The compound command C1; C2 is the sequential composition of the commands
C1 and C2. It begins by executing C1 in the current state of the store. If that
execution terminates, then it executes C2 in the storage state resulting from the
execution of C1. Otherwise – if the execution of C1 does not terminate – the
run of C1; C2 also does not terminate. Sequential composition is an example of
a control structure since it implements a certain policy of ﬂow of control in a
computation.
1 In common with languages like C and Java, we use a single equals sign = to mean assignment
and a double sign == to mean equality. Earlier languages like Pascal used := for assignment and
simple = for equality; it is a great pity that C and its successors did not keep this convention.
The reason that = is a bad symbol for assignment is that assignment is not symmetric: if we
interpret x = y as the assignment, then x becomes y which is not the same thing as y becoming
x; yet, x = y and y = x are the same thing if we mean equality. The two dots in := helped
remind the reader that this is an asymmetric assignment operation rather than a symmetric
assertion of equality. However, the notation = for assignment is now commonplace, so we will
use it.

262
4 Program verification
3.
Another control structure is if B {C1} else {C2}. It ﬁrst evaluates the boolean
expression B in the current state of the store; if that result is true, then C1 is
executed; if B evaluated to false, then C2 is executed.
4.
The third control construct while B {C} allows us to write statements which
are executed repeatedly. Its meaning is that:
a the boolean expression B is evaluated in the current state of the store;
b if B evaluates to false, then the command terminates,
c otherwise, the command C will be executed. If that execution terminates,
then we resume at step (a) with a re-evaluation of B as the updated state
of the store may have changed its value.
The point of the while-statement is that it repeatedly executes the command
C as long as B evaluates to true. If B never becomes false, or if one of the
executions of C does not terminate, then the while-statement will not termi-
nate. While-statements are the only real source of non-termination in our core
programming language.
Example 4.2 The factorial n! of a natural number n is deﬁned induc-
tively by
0!
def
= 1
(4.4)
(n + 1)!
def
= (n + 1) · n!
For example, unwinding this deﬁnition for n being 4, we get 4!
def
= 4 · 3! =
· · · = 4 · 3 · 2 · 1 · 0! = 24. The following program Fac1:
y = 1;
z = 0;
while (z != x) {
z = z + 1;
y = y * z;
}
is intended to compute the factorial2 of x and to store the result in y. We
will prove that Fac1 really does this later in the chapter.
4.2.2 Hoare triples
Program fragments generated by (4.3) commence running in a ‘state’ of the
machine. After doing some computation, they might terminate. If they do,
then the result is another, usually diﬀerent, state. Since our programming
2 Please note the diﬀerence between the formula x! = y, saying that the factorial of x is equal to
y, and the piece of code x != y which says that x is not equal to y.

4.2 A framework for software verification
263
language does not have any procedures or local variables, the ‘state’ of the
machine can be represented simply as a vector of values of all the variables
used in the program.
What syntax should we use for φR, the formal speciﬁcations of require-
ments for such programs? Because we are interested in the output of the
program, the language should allow us to talk about the variables in the
state after the program has executed, using operators like = to express
equality and < for less than. You should be aware of the overloading of
=. In code, it represents an assignment instruction; in logical formulas, it
stands for equality, which we write == within program code.
For example, if the informal requirement R says that we should
Compute a number y whose square is less than the input x.
then an appropriate speciﬁcation may be y · y < x. But what if the input x
is −4? There is no number whose square is less than a negative number, so
it is not possible to write the program in a way that it will work with all
possible inputs. If we go back to the client and say this, he or she is quite
likely to respond by saying that the requirement is only that the program
work for positive numbers; i.e., he or she revises the informal requirement
so that it now says
If the input x is a positive number, compute a number whose square
is less than x.
This means we need to be able to talk not just about the state after the
program executes, but also about the state before it executes. The assertions
we make will therefore be triples, typically looking like

φ

P

ψ

(4.5)
which (roughly) means:
If the program P is run in a state that satisﬁes φ, then the state
resulting from P’s execution will satisfy ψ.
The speciﬁcation of the program P, to calculate a number whose square is
less than x, now looks like this:

x > 0

P

y · y < x

.
(4.6)
It means that, if we run P in a state such that x > 0, then the resulting
state will be such that y · y < x. It does not tell us what happens if we run
P in a state in which x ≤0, the client required nothing for non-positive
values of x. Thus, the programmer is free to do what he or she wants in that
case. A program which produces ‘garbage’ in the case that x ≤0 satisﬁes
the speciﬁcation, as long as it works correctly for x > 0.

264
4 Program verification
Let us make these notions more precise.
Deﬁnition 4.3 1.
The form

φ

P

ψ

of our speciﬁcation is called a Hoare
triple, after the computer scientist C. A. R. Hoare.
2.
In (4.5), the formula φ is called the precondition of P and ψ is called the
postcondition.
3.
A store or state of core programs is a function l that assigns to each variable
x an integer l(x).
4.
For a formula φ of predicate logic with function symbols −(unary), +, −, and ∗
(binary); and a binary predicate symbols < and =, we say that a state l satisﬁes
φ or l is a φ-state – written l ⊨φ – iﬀM ⊨l φ from page 128 holds, where l
is viewed as a look-up table and the model M has as set A all integers and
interprets the function and predicate symbols in their standard manner.
5.
For Hoare triples in (4.5), we demand that quantiﬁers in φ and ψ only bind
variables that do not occur in the program P.
Example 4.4 For any state l for which l(x) = −2, l(y) = 5, and l(z) = −1,
the relation
1.
l ⊨¬(x + y < z) holds since x + y evaluates to −2 + 5 = 3, z evaluates to l(z) =
−1, and 3 is not strictly less than −1;
2.
l ⊨y −x ∗z < z does not hold, since the lefthand expression evaluates to 5 −
(−2) · (−1) = 3 which is not strictly less than l(z) = −1;
3.
l ⊨∀u (y < u →y ∗z < u ∗z) does not hold; for u being 7, l ⊨y < u holds, but
l ⊨y ∗z < u ∗z does not.
Often, we do not want to put any constraints on the initial state; we
simply wish to say that, no matter what state we start the program in, the
resulting state should satisfy ψ. In that case the precondition can be set to
⊤, which is – as in previous chapters – a formula which is true in any state.
Note that the triple in (4.6) does not specify a unique program P, or
a unique behaviour. For example, the program which simply does y = 0;
satisﬁes the speciﬁcation – since 0 · 0 is less than any positive number – as
does the program
y = 0;
while (y * y < x) {
y = y + 1;
}
y = y - 1;
This program ﬁnds the greatest y whose square is less than x; the while-
statement overshoots a bit, but then we ﬁx it after the while-statement.3
3 We could avoid this inelegance by using the repeat construct of exercise 3 on page 299.

4.2 A framework for software verification
265
Note that these two programs have diﬀerent behaviour. For example, if x is
22, the ﬁrst one will compute y = 0 and the second will render y = 4; but
both of them satisfy the speciﬁcation.
Our agenda, then, is to develop a notion of proof which allows us to
prove that a program P satisﬁes the speciﬁcation given by a precondition
φ and a postcondition ψ in (4.5). Recall that we developed proof calculi
for propositional and predicate logic where such proofs could be accom-
plished by investigating the structure of the formula one wanted to prove.
For example, for proving an implication φ →ψ one had to assume φ and
manage to show ψ; then the proof could be ﬁnished with the proof rule for
implies-introduction. The proof calculi which we are about to develop follow
similar lines. Yet, they are diﬀerent from the logics we previously studied
since they prove triples which are built from two diﬀerent sorts of things:
logical formulas φ and ψ versus a piece of code P. Our proof calculi have to
address each of these appropriately. Nonetheless, we retain proof strategies
which are compositional, but now in the structure of P. Note that this is
an important advantage in the veriﬁcation of big projects, where code is
built from a multitude of modules such that the correctness of certain parts
will depend on the correctness of certain others. Thus, your code might
call subroutines which other members of your project are about to code,
but you can already check the correctness of your code by assuming that
the subroutines meet their own speciﬁcations. We will explore this topic in
Section 4.5.
4.2.3 Partial and total correctness
Our explanation of when the triple

φ

P

ψ

holds was rather informal. In
particular, it did not say what we should conclude if P does not terminate.
In fact there are two ways of handling this situation. Partial correctness
means that we do not require the program to terminate, whereas in total
correctness we insist upon its termination.
Deﬁnition 4.5 (Partial correctness) We say that the triple

φ

P

ψ

is satisﬁed under partial correctness if, for all states which satisfy φ, the
state resulting from P’s execution satisﬁes the postcondition ψ, provided
that P actually terminates. In this case, the relation ⊨par

φ

P

ψ

holds.
We call ⊨par the satisfaction relation for partial correctness.
Thus, we insist on ψ being true of the resulting state only if the program P
has terminated on an input satisfying φ. Partial correctness is rather a weak
requirement, since any program which does not terminate at all satisﬁes its

266
4 Program verification
speciﬁcation. In particular, the program
while true { x = 0; }
– which endlessly ‘loops’ and never terminates – satisﬁes all speciﬁcations,
since partial correctness only says what must happen if the program termi-
nates.
Total correctness, on the other hand, requires that the program terminates
in order for it to satisfy a speciﬁcation.
Deﬁnition 4.6 (Total correctness) We say that the triple

φ

P

ψ

is
satisﬁed under total correctness if, for all states in which P is executed which
satisfy the precondition φ, P is guaranteed to terminate and the resulting
state satisﬁes the postcondition ψ. In this case, we say that ⊨tot

φ

P

ψ

holds and call ⊨tot the satisfaction relation of total correctness.
A program which ‘loops’ forever on all input does not satisfy any spec-
iﬁcation under total correctness. Clearly, total correctness is more useful
than partial correctness, so the reader may wonder why partial correctness
is introduced at all. Proving total correctness usually beneﬁts from prov-
ing partial correctness ﬁrst and then proving termination. So, although our
primary interest is in proving total correctness, it often happens that we
have to or may wish to split this into separate proofs of partial correctness
and of termination. Most of this chapter is devoted to the proof of partial
correctness, though we return to the issue of termination in Section 4.4.
Before we delve into the issue of crafting sound and complete proof calculi
for partial and total correctness, let us brieﬂy give examples of typical sorts
of speciﬁcations which we would like to be able to prove.
Examples 4.7
1.
Let Succ be the program
a = x + 1;
if (a - 1 == 0) {
y = 1;
} else {
y = a;
}
The program Succ satisﬁes the speciﬁcation

⊤

Succ

y = (x + 1)

under par-
tial and total correctness, so if we think of x as input and y as output, then
Succ computes the successor function. Note that this code is far from optimal.

4.2 A framework for software verification
267
In fact, it is a rather roundabout way of implementing the successor function.
Despite this non-optimality, our proof rules need to be able to prove this pro-
gram behaviour.
2.
The program Fac1 from Example 4.2 terminates only if x is initially non-
negative – why? Let us look at what properties of Fac1 we expect to be able to
prove.
We should be able to prove that ⊨tot

x ≥0

Fac1

y = x!

holds. It states
that, provided x ≥0, Fac1 terminates with the result y = x!. However, the
stronger statement that ⊨tot

⊤

Fac1

y = x!

holds should not be provable,
because Fac1 does not terminate for negative values of x.
For partial correctness, both statements ⊨par

x ≥0

Fac1

y = x!

and
⊨par

⊤

Fac1

y = x!

should be provable since they hold.
Deﬁnition 4.8 1.
If the partial correctness of triples

φ

P

ψ

can be proved
in the partial-correctness calculus we develop in this chapter, we say that the
sequent ⊢par

φ

P

ψ

is valid.
2.
Similarly, if it can be proved in the total-correctness calculus to be developed
in this chapter, we say that the sequent ⊢tot

φ

P

ψ

is valid.
Thus, ⊨par

φ

P

ψ

holds if P is partially correct, while the validity of
⊢par

φ

P

ψ

means that P can be proved to be partially-correct by our
calculus. The ﬁrst one means it is actually correct, while the second one
means it is provably correct according to our calculus.
If our calculus is any good, then the relation ⊢par should be contained in
⊨par! More precisely, we will say that our calculus is sound if, whenever it
tells us something can be proved, that thing is indeed true. Thus, it is sound
if it doesn’t tell us that false things can be proved. Formally, we write that
⊢par is sound if
⊨par

φ

P

ψ

holds whenever ⊢par

φ

P

ψ

is valid
for all φ, ψ and P; and, similarly, ⊢tot is sound if
⊨tot

φ

P

ψ

holds whenever ⊢tot

φ

P

ψ

is valid
for all φ, ψ and P. We say that a calculus is complete if it is able to prove
everything that is true. Formally, ⊢par is complete if
⊢par

φ

P

ψ

is valid whenever ⊨par

φ

P

ψ

holds
for all φ, ψ and P; and similarly for ⊢tot being complete.
In Chapters 1 and 2, we said that soundness is relatively easy to show,
since typically the soundness of individual proof rules can be established
independently of the others. Completeness, on the other hand, is harder to

268
4 Program verification
show since it depends on the entire set of proof rules cooperating together.
The same situation holds for the program logic we introduce in this chapter.
Establishing its soundness is simply a matter of considering each rule in
turn – done in exercise 3 on page 303 – whereas establishing its (relative)
completeness is harder and beyond the scope of this book.
4.2.4 Program variables and logical variables
The variables which we have seen so far in the programs that we verify
are called program variables. They can also appear in the preconditions and
postconditions of speciﬁcations. Sometimes, in order to formulate speciﬁca-
tions, we need to use other variables which do not appear in programs.
Examples 4.9
1.
Another version of the factorial program might have been Fac2:
y = 1;
while (x != 0) {
y = y * x;
x = x - 1;
}
Unlike the previous version, it ‘consumes’ the input x. Nevertheless, it cor-
rectly calculates the factorial of x and stores the value in y; and we would
like to express that as a Hoare triple. However, it is not a good idea to write

x ≥0

Fac2

y = x!

because, if the program terminates, then x will be 0 and
y will be the factorial of the initial value of x.
We need a way of remembering the initial value of x, to cope with the fact
that it is modiﬁed by the program. Logical variables achieve just that: in the
speciﬁcation

x = x0 ∧x ≥0

Fac2

y = x0!

the x0 is a logical variable and
we read it as being universally quantiﬁed in the precondition. Therefore, this
speciﬁcation reads: for all integers x0, if x equals x0, x ≥0 and we run the
program such that it terminates, then the resulting state will satisfy y equals
x0!. This works since x0 cannot be modiﬁed by Fac2 as x0 does not occur in
Fac2.
2.
Consider the program Sum:
z = 0;
while (x > 0) {
z = z + x;
x = x - 1;
}
This program adds up the ﬁrst x integers and stores the result in z.
Thus,

x = 3

Sum

z = 6

,

x = 8

Sum

z = 36

etc. We know from The-
orem 1.31 on page 41 that 1 + 2 + · · · + x = x(x + 1)/2 for all x ≥0, so

4.3 Proof calculus for partial correctness
269
we would like to express, as a Hoare triple, that the value of z upon
termination is x0(x0 + 1)/2 where x0 is the initial value of x. Thus, we write

x = x0 ∧x ≥0

Sum

z = x0(x0 + 1)/2

.
Variables like x0 in these examples are called logical variables, because they
occur only in the logical formulas that constitute the precondition and post-
condition; they do not occur in the code to be veriﬁed. The state of the
system gives a value to each program variable, but not for the logical vari-
ables. Logical variables take a similar role to the dummy variables of the
rules for ∀i and ∃e in Chapter 2.
Deﬁnition 4.10 For a Hoare triple

φ

P

ψ

, its set of logical variables
are those variables that are free in φ or ψ; and don’t occur in P.
4.3 Proof calculus for partial correctness
The proof calculus which we now present goes back to R. Floyd and C.
A. R. Hoare. In the next subsection, we specify proof rules for each of the
grammar clauses for commands. We could go on to use these proof rules
directly, but it turns out to be more convenient to present them in a diﬀerent
form, suitable for the construction of proofs known as proof tableaux. This
is what we do in the subsection following the next one.
4.3.1 Proof rules
The proof rules for our calculus are given in Figure 4.1. They should be
interpreted as rules that allow us to pass from simple assertions of the form

φ

P

ψ

to more complex ones. The rule for assignment is an axiom as
it has no premises. This allows us to construct some triples out of noth-
ing, to get the proof going. Complete proofs are trees, see page 274 for an
example.
Composition.
Given speciﬁcations for the program fragments C1 and C2,
say

φ

C1

η

and

η

C2

ψ

,
where the postcondition of C1 is also the precondition of C2, the proof
rule for sequential composition shown in Figure 4.1 allows us to derive a
speciﬁcation for C1; C2, namely

φ

C1; C2

ψ

.

270
4 Program verification

φ

C1

η


η

C2

ψ


φ

C1; C2

ψ

Composition

ψ[E/x]

x = E

ψ
 Assignment

φ ∧B

C1

ψ


φ ∧¬B

C2

ψ


φ

if B {C1} else {C2}

ψ

If-statement

ψ ∧B

C

ψ


ψ

while B {C}

ψ ∧¬B
 Partial-while
⊢AR φ′ →φ

φ

C

ψ

⊢AR ψ →ψ′

φ′
C

ψ′
Implied
Figure 4.1. Proof rules for partial correctness of Hoare triples.
Thus, if we know that C1 takes φ-states to η-states and C2 takes η-states
to ψ-states, then running C1 and C2 in that sequence will take φ-states to
ψ-states.
Using the proof rules of Figure 4.1 in program veriﬁcation, we have to
read them bottom-up: e.g. in order to prove

φ

C1; C2

ψ

, we need to ﬁnd
an appropriate η and prove

φ

C1

η

and

η

C2

ψ

. If C1; C2 runs on
input satisfying φ and we need to show that the store satisﬁes ψ after its
execution, then we hope to show this by splitting the problem into two. After
the execution of C1, we have a store satisfying η which, considered as input
for C2, should result in an output satisfying ψ. We call η a midcondition.
Assignment.
The rule for assignment has no premises and is therefore an
axiom of our logic. It tells us that, if we wish to show that ψ holds in the state
after the assignment x = E, we must show that ψ[E/x] holds before the
assignment; ψ[E/x] denotes the formula obtained by taking ψ and replacing
all free occurrences of x with E as deﬁned on page 105. We read the stroke
as ‘in place of;’ thus, ψ[E/x] is ψ with E in place of x. Several explanations
may be required to understand this rule.
r At ﬁrst sight, it looks as if the rule has been stated in reverse; one might expect
that, if ψ holds in a state in which we perform the assignment x = E, then surely

4.3 Proof calculus for partial correctness
271
ψ[E/x] holds in the resulting state, i.e. we just replace x by E. This is wrong. It
is true that the assignment x = E replaces the value of x in the starting state
by E, but that does not mean that we replace occurrences of x in a condition on
the starting state by E.
For example, let ψ be x = 6 and E be 5. Then

ψ

x = 5

ψ[x/E]

does not
hold: given a state in which x equals 6, the execution of x = 5 results in a
state in which x equals 5. But ψ[x/E] is the formula 5 = 6 which holds in no
state.
The right way to understand the Assignment rule is to think about what you
would have to prove about the initial state in order to prove that ψ holds in
the resulting state. Since ψ will – in general – be saying something about the
value of x, whatever it says about that value must have been true of E, since
in the resulting state the value of x is E. Thus, ψ with E in place of x – which
says whatever ψ says about x but applied to E – must be true in the initial
state.
r The axiom

ψ[E/x]

x = E

ψ

is best applied backwards than forwards in the
veriﬁcation process. That is to say, if we know ψ and we wish to ﬁnd φ such
that

φ

x = E

ψ

, it is easy: we simply set φ to be ψ[E/x]; but, if we know
φ and we want to ﬁnd ψ such that

φ

x = E

ψ

, there is no easy way of
getting a suitable ψ. This backwards characteristic of the assignment and the
composition rule will be important when we look at how to construct proofs;
we will work from the end of a program to its beginning.
r If we apply this axiom in this backwards fashion, then it is completely
mechanical to apply. It just involves doing a substitution. That means we could
get a computer to do it for us. Unfortunately, that is not true for all the rules;
application of the rule for while-statements, for example, requires ingenuity.
Therefore a computer can at best assist us in performing a proof by carrying
out the mechanical steps, such as application of the assignment axiom, while
leaving the steps that involve ingenuity to the programmer.
r Observe that, in computing ψ[E/x] from ψ, we replace all the free occurrences of
x in ψ. Note that there cannot be problems caused by bound occurrences, as seen
in Example 2.9 on page 106, provided that preconditions and postconditions quan-
tify over logical variables only. For obvious reasons, this is recommended practice.
Examples 4.11
1.
Suppose P is the program x = 2. The following are instances of axiom
Assignment:
a

2 = 2

P

x = 2

b

2 = 4

P

x = 4

c

2 = y

P

x = y

d

2 > 0

P

x > 0

.

272
4 Program verification
These are all correct statements. Reading them backwards, we see that they
say:
a If you want to prove x = 2 after the assignment x = 2, then we must be able
to prove that 2 = 2 before it. Of course, 2 is equal to 2, so proving it shouldn’t
present a problem.
b If you wanted to prove that x = 4 after the assignment, the only way in which
it would work is if 2 = 4; however, unfortunately it is not. More generally,

⊥

x = E

ψ

holds for any E and ψ – why?
c If you want to prove x = y after the assignment, you will need to prove that
2 = y before it.
d To prove x > 0, we’d better have 2 > 0 prior to the execution of P.
2.
Suppose P is x = x + 1. By choosing various postconditions, we obtain the fol-
lowing instances of the assignment axiom:
a

x + 1 = 2

P

x = 2

b

x + 1 = y

P

x = y

c

x + 1 + 5 = y

P

x + 5 = y

d

x + 1 > 0 ∧y > 0

P

x > 0 ∧y > 0

.
Note that the precondition obtained by performing the substitution can often be
simpliﬁed. The proof rule for implications below will allow such simpliﬁcations
which are needed to make preconditions appreciable by human consumers.
If-statements.
The proof rule for if-statements allows us to prove a triple
of the form

φ

if B {C1} else {C2}

ψ

by decomposing it into two triples, subgoals corresponding to the cases of
B evaluating to true and to false. Typically, the precondition φ will not tell
us anything about the value of the boolean expression B, so we have to
consider both cases. If B is true in the state we start in, then C1 is executed
and hence C1 will have to translate φ states to ψ states; alternatively, if
B is false, then C2 will be executed and will have to do that job. Thus,
we have to prove that

φ ∧B

C1

ψ

and

φ ∧¬B

C2

ψ

. Note that the
preconditions are augmented by the knowledge that B is true and false,
respectively. This additional information is often crucial for completing the
respective subproofs.
While-statements.
The rule for while-statements given in Figure 4.1 is ar-
guably the most complicated one. The reason is that the while-statement
is the most complicated construct in our language. It is the only command
that ‘loops,’ i.e. executes the same piece of code several times. Also, unlike
as the for-statement in languages like Java we cannot generally predict how

4.3 Proof calculus for partial correctness
273
many times while-statements will ‘loop’ around, or even whether they will
terminate at all.
The key ingredient in the proof rule for Partial-while is the ‘invariant’ ψ.
In general, the body C of the command while (B) {C} changes the values
of the variables it manipulates; but the invariant expresses a relationship
between those values which is preserved by any execution of C. In the proof
rule, ψ expresses this invariant; the rule’s premise,

ψ ∧B

C

ψ

, states
that, if ψ and B are true before we execute C, and C terminates, then ψ
will be true after it. The conclusion of Partial-while states that, no matter
how many times the body C is executed, if ψ is true initially and the while-
statement terminates, then ψ will be true at the end. Moreover, since the
while-statement has terminated, B will be false.
Implied.
One ﬁnal rule is required in our calculus: the rule Implied of Figure
4.1. It tells us that, if we have proved

φ

P

ψ

and we have a formula φ′
which implies φ and another one ψ′ which is implied by ψ, then we should
also be allowed to prove that

φ′
P

ψ′
. A sequent ⊢ARφ →φ′ is valid iﬀ
there is a proof of φ′ in the natural deduction calculus for predicate logic,
where φ and standard laws of arithmetic – e.g. ∀x (x = x + 0) – are premises.
Note that the rule Implied allows the precondition to be strengthened (thus,
we assume more than we need to), while the postcondition is weakened (i.e.
we conclude less than we are entitled to). If we tried to do it the other way
around, weakening the precondition or strengthening the postcondition, then
we would conclude things which are incorrect – see exercise 9(a) on page 300.
The rule Implied acts as a link between program logic and a suitable
extension of predicate logic. It allows us to import proofs in predicate logic
enlarged with the basic facts of arithmetic, which are required for reasoning
about integer expressions, into the proofs in program logic.
4.3.2 Proof tableaux
The proof rules presented in Figure 4.1 are not in a form which is easy
to use in examples. To illustrate this point, we present an example of a
proof in Figure 4.2; it is a proof of the triple

⊤

Fac1

y = x!

where Fac1
is the factorial program given in Example 4.2. This proof abbreviates rule
names; and drops the bars and names for Assignment as well as sequents
for ⊢AR in all applications of the Implied rule. We have not yet presented
enough information for the reader to complete such a proof on her own,
but she can at least use the proof rules in Figure 4.1 to check whether all
rule instances of that proof are permissible, i.e. match the required pat-
tern.

274
4 Program verification

1 = 1
y = 1
y = 1
i

⊤
y = 1
y = 1

y = 1 ∧0 = 0
z = 0
y = 1 ∧z = 0
i

y = 1
z = 0
y = 1 ∧z = 0
c

⊤
y = 1; z = 0
y = 1 ∧z = 0

y · (z + 1) = (z + 1)!
z = z+1
y · z = z!
i

y = z! ∧z ̸= x
z = z+1
y · z = z!

y · z = z!
y = y*z
y = z!
c

y = z! ∧z ̸= x
z = z+1; y = y*z
y = z!
w

y = z!
while (z
!=
x) {z = z+1; y = y*z}
y = z! ∧z = x
i

y = 1 ∧z = 0
while (z
!=
x) {z = z+1; y = y*z}
y = x!
c

⊤
y = 1; z = 0; while (z
!=
x) {z = z+1; y = y*z}
y = x!
Figure 4.2. A partial-correctness proof for Fac1 in tree form.

4.3 Proof calculus for partial correctness
275
It should be clear that proofs in this form are unwieldy to work with.
They will tend to be very wide and a lot of information is copied from one
line to the next. Proving properties of programs which are longer than Fac1
would be very diﬃcult in this style. In Chapters 1, 2 and 5 we abandon
representation of proofs as trees for similar reasons. The rule for sequential
composition suggests a more convenient way of presenting proofs in pro-
gram logic, called proof tableaux. We can think of any program of our core
programming language as a sequence
C1;
C2;
·
·
·
Cn
where none of the commands Ci is a composition of smaller programs, i.e. all
of the Ci above are either assignments, if-statements or while-statements. Of
course, we allow the if-statements and while-statements to have embedded
compositions.
Let P stand for the program C1; C2; . . . ; Cn−1; Cn. Suppose that we want
to show the validity of ⊢par

φ0

P

φn

for a precondition φ0 and a postcon-
dition φn. Then, we may split this problem into smaller ones by trying to
ﬁnd formulas φj (0 < j < n) and prove the validity of ⊢par

φi

Ci+1

φi+1

for i = 0, 1, . . . , n −1. This suggests that we should design a proof calcu-
lus which presents a proof of ⊢par

φ0

P

ψn

by interleaving formulas with
code as in

φ0

C1;
φ1

justification
C2;
·
·
· 
φn−1

justification
Cn;
φn

justification

276
4 Program verification
Against each formula, we write a justiﬁcation, whose nature will be clariﬁed
shortly. Proof tableaux thus consist of the program code interleaved with
formulas, which we call midconditions, that should hold at the point they
are written.
Each of the transitions

φi

Ci+1

φi+1

will appeal to one of the rules of Figure 4.1, depending on whether Ci+1 is
an assignment, an if-statement or a while-statement. Note that this notation
for proofs makes the proof rule for composition in Figure 4.1 implicit.
How should the intermediate formulas φi be found? In principle, it seems
as though one could start from φ0 and, using C1, obtain φ1 and continue
working downwards. However, because the assignment rule works backwards,
it turns out that it is more convenient to start with φn and work upwards,
using Cn to obtain φn−1 etc.
Deﬁnition 4.12 The process of obtaining φi from Ci+1 and φi+1 is called
computing the weakest precondition of Ci+1, given the postcondition φi+1.
That is to say, we are looking for the logically weakest formula whose truth
at the beginning of the execution of Ci+1 is enough to guarantee φi+14.
The construction of a proof tableau for

φ

C1; . . . ; Cn

ψ

typically con-
sists of starting with the postcondition ψ and pushing it upwards through
Cn, then Cn−1, . . . , until a formula φ′ emerges at the top. Ideally, the formula
φ′ represents the weakest precondition which guarantees that the ψ will hold
if the composed program C1; C2; . . . ; Cn−1; Cn is executed and terminates.
The weakest precondition φ′ is then checked to see whether it follows from
the given precondition φ. Thus, we appeal to the Implied rule of Figure 4.1.
Before a discussion of how to ﬁnd invariants for while-statement, we now
look at the assignment and the if-statement to see how the weakest precon-
dition is calculated for each one.
Assignment.
The assignment axiom is easily adapted to work for proof
tableaux. We write it thus:
4 φ is weaker than ψ means that φ is implied by ψ in predicate logic enlarged with the basic
facts about arithmetic: the sequent ⊢AR ψ →φ is valid. We want the weakest formula, because
we want to impose as few constraints as possible on the preceding code. In some cases, espe-
cially those involving while-statements, it might not be possible to extract the logically weakest
formula. We just need one which is suﬃciently weak to allow us to complete the proof at hand.

4.3 Proof calculus for partial correctness
277

ψ[E/x]

x = E

ψ

Assignment
The justiﬁcation is written against the ψ, since, once the proof has been con-
structed, we want to read it in a forwards direction. The construction itself
proceeds in a backwards direction, because that is the way the assignment
axiom facilitates.
Implied.
In tableau form, the Implied rule allows us to write one formula φ2
directly underneath another one φ1 with no code in between, provided that
φ1 implies φ2 in that the sequent ⊢AR φ1 →φ2 is valid. Thus, the Implied
rule acts as an interface between predicate logic with arithmetic and program
logic. This is a surprising and crucial insight. Our proof calculus for partial
correctness is a hybrid system which interfaces with another proof calculus
via the Implied proof rule only.
When we appeal to the Implied rule, we will usually not explicitly write
out the proof of the implication in predicate logic, for this chapter focuses
on the program logic. Mostly, the implications we typically encounter will
be easy to verify.
The Implied rule is often used to simplify formulas that are generated by
applications of the other rules. It is also used when the weakest precondition
φ′ emerges by pushing the postcondition upwards through the whole pro-
gram. We use the Implied rule to show that the given precondition implies
the weakest precondition. Let’s look at some examples of this.
Examples 4.13
1.
We show that ⊢par

y = 5

x = y + 1

x = 6

is valid:

y = 5


y + 1 = 6

Implied
x = y + 1

x = 6

Assignment
The proof is constructed from the bottom upwards. We start with

x = 6

and, using the assignment axiom, we push it upwards through x = y + 1. This
means substituting y + 1 for all occurrences of x, resulting in

y + 1 = 6

. Now,
we compare this with the given precondition

y = 5

. The given precondition
and the arithmetic fact 5 + 1 = 6 imply it, so we have ﬁnished the proof.

278
4 Program verification
Although the proof is constructed bottom-up, its justiﬁcations make sense
when read top-down: the second line is implied by the ﬁrst and the fourth
follows from the second by the intervening assignment.
2.
We prove the validity of ⊢par

y < 3

y = y + 1

y < 4

:

y < 3


y + 1 < 4

Implied
y = y + 1;

y < 4

Assignment
Notice that Implied always refers to the immediately preceding line. As already
remarked, proofs in program logic generally combine two logical levels: the ﬁrst
level is directly concerned with proof rules for programming constructs such as
the assignment statement; the second level is ordinary entailment familiar to
us from Chapters 1 and 2 plus facts from arithmetic – here that y < 3 implies
y + 1 < 3 + 1 = 4.
We may use ordinary logical and arithmetic implications to change a certain
condition φ to any condition φ′ which is implied by φ for reasons which have
nothing to do with the given code. In the example above, φ was y < 3 and the
implied formula φ′ was then y + 1 < 4. The validity of ⊢AR (y < 3) →(y + 1 <
4) is rooted in general facts about integers and the relation < deﬁned on them.
Completely formal proofs would require separate proofs attached to all instances
of the rule Implied. As already said, we won’t do that here as this chapter focuses
on aspects of proofs which deal directly with code.
3.
For the sequential composition of assignment statements
z = x;
z = z + y;
u = z;
our goal is to show that u stores the sum of x and y after this sequence of
assignments terminates. Let us write P for the code above. Thus, we mean to
prove ⊢par

⊤

P

u = x + y

.
We construct the proof by starting with the postcondition u = x + y and
pushing it up through the assignments, in reverse order, using the assignment
rule.
– Pushing it up through u = z involves replacing all occurrences of u by z,
resulting in z = x + y. We thus have the proof fragment

z = x + y

u = z;

u = x + y

Assignment
– Pushing z = x + y upwards through z = z + y involves replacing z by z + y,
resulting in z + y = x + y.

4.3 Proof calculus for partial correctness
279
– Pushing that upwards through z = x involves replacing z by x, resulting in
x + y = x + y. The proof fragment now looks like this:

x + y = x + y

z = x;

z + y = x + y

Assignment
z = z + y;

z = x + y

Assignment
u = z;

u = x + y

Assignment
The weakest precondition that thus emerges is x + y = x + y; we have to check
that this follows from the given precondition ⊤. This means checking that any
state that satisﬁes ⊤also satisﬁes x + y = x + y. Well, ⊤is satisﬁed in all states,
but so is x + y = x + y, so the sequent ⊢AR ⊤→(x + y = x + y) is valid.
The ﬁnal completed proof therefore looks like this:

⊤


x + y = x + y

Implied
z = x;

z + y = x + y

Assignment
z = z + y;

z = x + y

Assignment
u = z;

u = x + y

Assignment
and we can now read it from the top down.
The application of the axiom Assignment requires some care. We describe
two pitfalls which the unwary may fall into, if the rule is not applied correctly.
r Consider the example ‘proof’

x + 1 = x + 1

x = x + 1;

x = x + 1

Assignment
which uses the rule for assignment incorrectly. Pattern matching with the assign-
ment axiom means that ψ has to be x = x + 1, the expression E is x + 1 and
ψ[E/x] is x + 1 = x + 1. However, ψ[E/x] is obtained by replacing all occur-
rences of x in ψ by E, thus, ψ[E/x] would have to be equal to x + 1 = x + 1 + 1.
Therefore, the corrected proof

280
4 Program verification

x + 1 = x + 1 + 1

x = x + 1;

x = x + 1

Assignment
shows that ⊢par

x + 1 = x + 1 + 1

x = x + 1

x = x + 1

is valid.
As an aside, this corrected proof is not very useful. The triple says that, if
x + 1 = (x + 1) + 1 holds in a state and the assignment x = x + 1 is executed
and terminates, then the resulting state satisﬁes x = x + 1; but, since the precon-
dition x + 1 = x + 1 + 1 can never be true, this triple tells us nothing informative
about the assignment.
r Another way of using the proof rule for assignment incorrectly is by allowing ad-
ditional assignments to happen in between ψ[E/x] and x = E, as in the ‘proof’

x + 2 = y + 1

y = y + 1000001;
x = x + 2;

x = y + 1

Assignment
This is not a correct application of the assignment rule, since an additional
assignment happens in line 2 right before the actual assignment to which the
inference in line 4 applies. This additional assignment makes this reasoning un-
sound: line 2 overwrites the current value in y to which the equation in line 1
is referring. Clearly, x + 2 = y + 1 won’t be true any longer. Therefore, we are
allowed to use the proof rule for assignment only if there is no additional code
between the precondition ψ[E/x] and the assignment x = E.
If-statements.
We now consider how to push a postcondition upwards
through an if-statement. Suppose we are given a condition ψ and a pro-
gram fragment if (B) {C1} else {C2}. We wish to calculate the weakest
φ such that

φ

if (B) {C1} else {C2}

ψ

.
This φ may be calculated as follows.
1.
Push ψ upwards through C1; let’s call the result φ1. (Note that, since C1 may
be a sequence of other commands, this will involve appealing to other rules. If
C1 contains another if-statement, then this step will involve a ‘recursive call’
to the rule for if-statements.)
2.
Similarly, push ψ upwards through C2; call the result φ2.
3.
Set φ to be (B →φ1) ∧(¬B →φ2).
Example 4.14 Let us see this proof rule at work on the non-optimal code
for Succ given earlier in the chapter. Here is the code again:

4.3 Proof calculus for partial correctness
281
a = x + 1;
if (a - 1 == 0)
{
y = 1;
} else {
y = a;
}
We want to show that ⊢par

⊤

Succ

y = x+1

is valid. Note that this
program is the sequential composition of an assignment and an if-statement.
Thus, we need to obtain a suitable midcondition to put between the if-
statement and the assignment.
We push the postcondition y = x + 1 upwards through the two branches
of the if-statement, obtaining
r φ1 is 1 = x + 1;
r φ2 is a = x + 1;
and obtain the midcondition (a −1 = 0 →1 = x + 1) ∧(¬(a −1 = 0) →
a = x + 1) by appealing to a slightly diﬀerent version of the rule
If-statement: 
φ1

C1

ψ


φ2

C2

ψ


(B →φ1) ∧(¬B →φ2)

if B {C1} else {C2}

ψ
 If-Statement
(4.7)
However, this rule can be derived using the proof rules discussed so far; see
exercise 9(c) on page 301. The partial proof now looks like this:
(⊤)
(?)
?
a = x + 1;
((a −1 = 0 →1 = x + 1) ∧(¬(a −1 = 0) →a = x + 1))
?
if (a - 1 == 0) {
(1 = x + 1)
If-Statement
y = 1;
(y = x + 1)
Assignment
} else {
(a = x + 1)
If-Statement
y = a;
(y = x + 1)
Assignment
}
(y = x + 1)
If-Statement
Continuing this example, we push the long formula above the if-statement
through the assignment, to obtain
(x + 1 −1 = 0 →1 = x + 1) ∧(¬(x + 1 −1 = 0) →x + 1 = x + 1)
(4.8)

282
4 Program verification
We need to show that this is implied by the given precondition ⊤, i.e. that
it is true in any state. Indeed, simplifying (4.8) gives
(x = 0 →1 = x + 1) ∧(¬(x = 0) →x + 1 = x + 1)
and both these conjuncts, and therefore their conjunction, are clearly valid
implications. The above proof now is completed as:

⊤


(x + 1 −1 = 0 →1 = x + 1) ∧(¬(x + 1 −1 = 0) →x + 1 = x + 1)

Implied
a = x + 1;

(a −1 = 0 →1 = x + 1) ∧(¬(a −1 = 0) →a = x + 1)

Assignment
if (a - 1 == 0) {

1 = x + 1

If-Statement
y = 1;

y = x + 1

Assignment
} else {

a = x + 1

If-Statement
y = a;

y = x + 1

Assignment
} 
y = x + 1

If-Statement
While-statements.
Recall that the proof rule for partial correctness of
while-statements was presented in the following form in Figure 4.1 – here
we have written η instead of ψ:

η ∧B

C

η


η

while B {C}

η ∧¬B
 Partial-while.
(4.9)
Before we look at how Partial-while will be represented in proof tableaux,
let us look in more detail at the ideas behind this proof rule. The formula η is
chosen to be an invariant of the body C of the while-statement: provided the
boolean guard B is true, if η is true before we start C, and C terminates,
then it is also true at the end. This is what the premise

η ∧B

C

η

expresses.
Now suppose the while-statement executes a terminating run from a state
that satisﬁes η; and that the premise of (4.9) holds.
r If B is false as soon as we embark on the while-statement, then we do not execute
C at all. Nothing has happened to change the truth value of η, so we end the
while-statement with η ∧¬B.

4.3 Proof calculus for partial correctness
283
r If B is true when we embark on the while-statement, we execute C. By the
premise of the rule in (4.9), we know η is true at the end of C.
– if B is now false, we stop with η ∧¬B.
– if B is true, we execute C again; η is again re-established. No matter how
many times we execute C in this way, η is re-established at the end of each
execution of C. The while-statement terminates if, and only if, B is false after
some ﬁnite (zero including) number of executions of C, in which case we have
η ∧¬B.
This argument shows that Partial-while is sound with respect to the sat-
isfaction relation for partial correctness, in the sense that anything we prove
using it is indeed true. However, as it stands it allows us to prove only things
of the form

η

while (B) {C}

η ∧¬B

, i.e. triples in which the postcon-
dition is the same as the precondition conjoined with ¬B. Suppose that we
are required to prove

φ

while (B) {C}

ψ

(4.10)
for some φ and ψ which are not related in that way. How can we use
Partial-while in a situation like this?
The answer is that we must discover a suitable η, such that
1.
⊢AR φ →η,
2.
⊢AR η ∧¬B →ψ and
3.
⊢par

η

while (B) {C}

η ∧¬B

are all valid, where the latter is shown by means of Partial-while. Then,
Implied infers that (4.10) is a valid partial-correctness triple.
The crucial thing, then, is the discovery of a suitable invariant η. It is a
necessary step in order to use the proof rule Partial-while and in general it
requires intelligence and ingenuity. This contrasts markedly with the case of
the proof rules for if-statements and assignments, which are purely mechan-
ical in nature: their usage is just a matter of symbol-pushing and does not
require any deeper insight.
Discovery of a suitable invariant requires careful thought about what the
while-statement is really doing. Indeed the eminent computer scientist, the
late E. Dijkstra, said that to understand a while-statement is tantamount
to knowing what its invariant is with respect to given preconditions and
postconditions for that while-statement.
This is because a suitable invariant can be interpreted as saying that the
intended computation performed by the while-statement is correct up to
the current step of the execution. It then follows that, when the execution

284
4 Program verification
terminates, the entire computation is correct. Let us formalize invariants
and then study how to discover them.
Deﬁnition 4.15 An invariant of the while-statement while (B) {C} is a
formula η such that ⊨par

η ∧B

C

η

holds; i.e. for all states l, if η and B
are true in l and C is executed from state l and terminates, then η is again
true in the resulting state.
Note that η does not have to be true continuously during the execution of
C; in general, it will not be. All we require is that, if it is true before C is
executed, then it is true (if and) when C terminates.
For any given while-statement there are several invariants. For example,
⊤is an invariant for any while-statement; so is ⊥, since the premise of the
implication ‘if ⊥∧B is true, then . . . ’ is false, so that implication is true.
The formula ¬B is also an invariant of while (B) do {C}; but most of
these invariants are useless to us, because we are looking for an invariant
η for which the sequents ⊢AR φ →η and ⊢AR η ∧¬B →ψ, are valid, where
φ and ψ are the preconditions and postconditions of the while-statement.
Usually, this will single out just one of all the possible invariants – up to
logical equivalence.
A useful invariant expresses a relationship between the variables manip-
ulated by the body of the while-statement which is preserved by the exe-
cution of the body, even though the values of the variables themselves may
change. The invariant can often be found by constructing a trace of the
while-statement in action.
Example 4.16 Consider the program Fac1 from page 262, annotated with
location labels for our discussion:
y = 1;
z = 0;
l1:
while (z != x) {
z = z + 1;
y = y * z;
l2:
}
Suppose program execution begins in a store in which x equals 6. When the
program ﬂow ﬁrst encounters the while-statement at location l1, z equals
0 and y equals 1, so the condition z ̸= x is true and the body is executed.
Thereafter at location l2, z equals 1 and y equals 1 and the boolean guard
is still true, so the body is executed again. Continuing in this way, we obtain

4.3 Proof calculus for partial correctness
285
the following trace:
after iteration
z at l1
y at l1
B at l1
0
0
1
true
1
1
1
true
2
2
2
true
3
3
6
true
4
4
24
true
5
5
120
true
6
6
720
false
The program execution stops when the boolean guard becomes false.
The invariant of this example is easy to see: it is ‘y = z!’. Every time
we complete an execution of the body of the while-statement, this fact is
true, even though the values of y and z have been changed. Moreover, this
invariant has the needed properties. It is
r weak enough to be implied by the precondition of the while-statement, which
we will shortly discover to be y = 1 ∧z = 0 based on the initial assignments and
their precondition 0!
def
= 1,
r but also strong enough that, together with the negation of the boolean guard, it
implies the postcondition ‘y = x!’.
That is to say, the sequents
⊢AR (y = 1 ∧z = 0) →(y = z!) and ⊢AR (y = z! ∧x = z) →(y = x!)
(4.11)
are valid.
As in this example, a suitable invariant is often discovered by looking at
the logical structure of the postcondition. A complete proof of the factorial
example in tree form, using this invariant, was given in Figure 4.2.
How should we use the while-rule in proof tableaux? We need to think
about how to push an arbitrary postcondition ψ upwards through a while-
statement to meet the precondition φ. The steps are:
1.
Guess a formula η which you hope is a suitable invariant.
2.
Try to prove that ⊢AR η ∧¬B →ψ and ⊢AR φ →η are valid, where B is the
boolean guard of the while-statement. If both proofs succeed, go to 3. Otherwise
(if at least one proof fails), go back to 1.
3.
Push η upwards through the body C of the while-statement; this involves ap-
plying other rules dictated by the form of C. Let us name the formula that
emerges η′.

286
4 Program verification
4.
Try to prove that ⊢AR η ∧B →η′ is valid; this proves that η is indeed an in-
variant. If you succeed, go to 5. Otherwise, go back to 1.
5.
Now write η above the while-statement and write φ above that η, annotating
that η with an instance of Implied based on the successful proof of the validity
of ⊢AR φ →η in 2. Mission accomplished!
Example 4.17 We continue the example of the factorial. The partial proof
obtained by pushing y = x! upwards through the while-statement – thus
checking the hypothesis that y = z! is an invariant – is as follows:
y = 1;
z = 0;

y = z!

?
while (z != x) {

y = z! ∧z ̸= x

Invariant Hyp. ∧guard

y · (z + 1) = (z + 1)!

Implied
z = z + 1;

y · z = z!

Assignment
y = y * z;

y = z!

Assignment
} 
y = x!

?
Whether y = z! is a suitable invariant depends on three things:
r The ability to prove that it is indeed an invariant, i.e. that y = z! implies y · (z +
1) = (z + 1)!. This is the case, since we just multiply each side of y = z! by z + 1
and appeal to the inductive deﬁnition of (z + 1)! in Example 4.2.
r The ability to prove that η is strong enough that it and the negation of the
boolean guard together imply the postcondition; this is also the case, for y = z!
and x = z imply y = x!.
r The ability to prove that η is weak enough to be established by the code leading
up to the while-statement. This is what we prove by continuing to push the result
upwards through the code preceding the while-statement.
Continuing, then: pushing y = z! through z = 0 results in y = 0! and push-
ing that through y = 1 renders 1 = 0!. The latter holds in all states as 0! is

4.3 Proof calculus for partial correctness
287
deﬁned to be 1, so it is implied by ⊤; our completed proof is:

⊤


1 = 0!

Implied
y = 1;

y = 0!

Assignment
z = 0;

y = z!

Assignment
while (z != x) {

y = z! ∧z ̸= x

Invariant Hyp. ∧guard

y · (z + 1) = (z + 1)!

Implied
z = z + 1;

y · z = z!

Assignment
y = y * z;

y = z!

Assignment
} 
y = z! ∧¬(z ̸= x)

Partial-while

y = x!

Implied
4.3.3 A case study: minimal-sum section
We practice the proof rule for while-statements once again by verifying a
program which computes the minimal-sum section of an array of integers.
For that, let us extend our core programming language with arrays of inte-
gers5. For example, we may declare an array
int a[n];
whose name is a and whose ﬁelds are accessed by a[0], a[1], . . . , a[n-1],
where n is some constant. Generally, we allow any integer expression E to
compute the ﬁeld index, as in a[E]. It is the programmer’s responsibility to
make sure that the value computed by E is always within the array bounds.
Deﬁnition 4.18 Let a[0], . . . , a[n −1] be the integer values of an array a.
A section of a is a continuous piece a[i], . . . , a[j], where 0 ≤i ≤j < n. We
5 We only read from arrays in the program Min Sum which follows. Writing to arrays introduces
additional problems because an array element can have several syntactically diﬀerent names and
this has to be taken into account by the calculus.

288
4 Program verification
write Si,j for the sum of that section: a[i] + a[i + 1] + · · · + a[j]. A minimal-
sum section is a section a[i], . . . , a[j] of a such that the sum Si,j is less than
or equal to the sum Si′,j′ of any other section a[i′], . . . , a[j′] of a.
Example 4.19 Let us illustrate these concepts on the example integer array
[−1, 3, 15, −6, 4, −5]. Both [3, 15, −6] and [−6] are sections, but [3, −6, 4]
isn’t since 15 is missing. A minimal-sum section for this particular array is
[−6, 4, −5] with sum −7; it is the only minimal-sum section in this case.
In general, minimal-sum sections need not be unique. For example, the
array [1, −1, 3, −1, 1] has two minimal-sum sections [1, −1] and [−1, 1] with
minimal sum 0.
The task at hand is to
r write a program Min Sum, written in our core programming language extended
with integer arrays, which computes the sum of a minimal-sum section of a given
array;
r make the informal requirement of this problem, given in the previous item, into
a formal speciﬁcation about the behaviour of Min Sum;
r use our proof calculus for partial correctness to show that Min Sum satisﬁes those
formal speciﬁcations provided that it terminates.
There is an obvious program to do the job: we could list all the possible
sections of a given array, then traverse that list to compute the sum of
each section and keep the recent minimal sum in a storage location. For the
example array [−1, 3, −2], this results in the list
[−1], [−1, 3], [−1, 3, −2], [3], [3, −2], [−2]
and we see that only the last section [−2] produces the minimal sum −2.
This idea can easily be coded in our core programming language, but it
has a serious drawback: the number of sections of a given array of size n is
proportional to the square of n; if we also have to sum all those, then our task
has worst-case time complexity of the order n · n2 = n3. Computationally,
this is an expensive price to pay, so we should inspect the problem more
closely in order to see whether we can do better.
Can we compute the minimal sum over all sections in time proportional
to n, by passing through the array just once? Intuitively, this seems diﬃcult,
since if we store just the minimal sum seen so far as we pass through the
array, we may miss the opportunity of some large negative numbers later on
because of some large positive numbers we encounter en route. For example,

4.3 Proof calculus for partial correctness
289
suppose the array is
[−8, 3, −65, 20, 45, −100, −8, 17, −4, −14].
Should we settle for −8 + 3 −65, or should we try to take advantage of the
−100 – remembering that we can pass through the array only once? In this
case, the whole array is a section that gives us the smallest sum, but it
is diﬃcult to see how a program which passes through the array just once
could detect this.
The solution is to store two values during the pass: the minimal sum seen
so far (s in the program below) and also the minimal sum seen so far of
all sections which end at the current point in the array (t below). Here is a
program that is intended to do this:
k = 1;
t = a[0];
s = a[0];
while (k != n) {
t = min(t + a[k], a[k]);
s = min(s,t);
k = k + 1;
}
where min is a function which computes the minimum of its two arguments
as speciﬁed in exercise 10 on page 301. The variable k proceeds through
the range of indexes of the array and t stores the minimal sum of sections
that end at a[k] – whenever the control ﬂow of the program is about to
evaluate the boolean expression of its while-statement. As each new value is
examined, we can either add it to the current minimal sum, or decide that a
lower minimal sum can be obtained by starting a new section. The variable
s stores the minimal sum seen so far; it is computed as the minimum we
have seen so far in the last step, or the minimal sum of sections that end at
the current point.
As you can see, it not intuitively clear that this program is correct, war-
ranting the use of our partial-correctness calculus to prove its correctness.
Testing the program with a few examples is not suﬃcient to ﬁnd all mis-
takes, however, and the reader would rightly not be convinced that this
program really does compute the minimal-sum section in all cases. So let
us try to use the partial-correctness calculus introduced in this chapter to
prove it.

290
4 Program verification
We formalise our requirement of the program as two speciﬁcations6, writ-
ten as Hoare triples.
S1.

⊤

Min Sum

∀i, j (0 ≤i ≤j < n →s ≤Si,j)

.
It says that, after the program terminates, s is less than or equal to, the
sum of any section of the array. Note that i and j are logical variables
in that they don’t occur as program variables.
S2.

⊤

Min Sum

∃i, j (0 ≤i ≤j < n ∧s = Si,j)

,
which says that there is a section whose sum is s.
If there is a section whose sum is s and no section has a sum less than s,
then s is the sum of a minimal-sum section: the ‘conjunction’ of S1 and S2
give us the property we want.
Let us ﬁrst prove S1. This begins with seeking a suitable invariant. As
always, the following characteristics of invariants are a useful guide:
r Invariants express the fact that the computation performed so far by the while-
statement is correct.
r Invariants typically have the same form as the desired postcondition of the while-
statement.
r Invariants express relationships between the variables manipulated by the while-
statement which are re-established each time the body of the while-statement is
executed.
A suitable invariant in this case appears to be
Inv1(s, k)
def
= ∀i, j (0 ≤i ≤j < k →s ≤Si,j)
(4.12)
since it says that s is less than, or equal to, the minimal sum observed up
to the current stage of the computation, represented by k. Note that it has
the same form as the desired postcondition: we replaced the n by k, since
the ﬁnal value of k is n. Notice that i and j are quantiﬁed in the formula,
because they are logical variables; k is a program variable. This justiﬁes the
notation Inv1(s, k) which highlights that the formula has only the program
variables s and k as free variables and is similar to the use of fun-statements
in Alloy in Chapter 2.
If we start work on producing a proof tableau with this invariant, we
will soon ﬁnd that it is not strong enough to do the job. Intuitively, this is
because it ignores the value of t, which stores the minimal sum of all sections
ending just before a[k], which is crucial in the idea behind the program. A
suitable invariant expressing that t is correct up to the current point of the
6 The notation ∀i, j abbreviates ∀i∀j, and similarly for ∃i, j.

4.3 Proof calculus for partial correctness
291
(⊤)
(Inv1(a[0], 1) ∧Inv2(a[0], 1))
Implied
k = 1;
(Inv1(a[0], k) ∧Inv2(a[0], k))
Assignment
t = a[0];
(Inv1(a[0], k) ∧Inv2(t, k))
Assignment
s = a[0];
(Inv1(s, k) ∧Inv2(t, k))
Assignment
while (k != n) {
(Inv1(s, k) ∧Inv2(t, k) ∧k ̸= n)
Invariant Hyp. ∧guard
(Inv1(min(s, min(t + a[k], a[k])), k + 1)
∧Inv2(min(t + a[k], a[k]), k + 1))
Implied (Lemma 4.20)
t = min(t + a[k], a[k]);
(Inv1(min(s, t), k + 1) ∧Inv2(t, k + 1))
Assignment
s = min(s,t);
(Inv1(s, k + 1) ∧Inv2(t, k + 1))
Assignment
k = k + 1;
(Inv1(s, k) ∧Inv2(t, k))
Assignment
}
(Inv1(s, k) ∧Inv2(t, k) ∧¬¬(k = n))
Partial-while
(Inv1(s, n))
Implied
Figure 4.3. Tableau proof for specification S1 of Min Sum.
computation is
Inv2(t, k)
def
= ∀i (0 ≤i < k →t ≤Si,k−1)
(4.13)
saying that t is not greater than the sum of any section ending in a[k −1].
Our invariant is the conjunction of these formulas, namely
Inv1(s, k) ∧Inv2(t, k).
(4.14)
The completed proof tableau of S1 for Min Sum is given in Figure 4.3. The
tableau is constructed by
r Proving that the candidate invariant (4.14) is indeed an invariant. This involves
pushing it upwards through the body of the while-statement and showing that
what emerges follows from the invariant and the boolean guard. This non-trivial
implication is shown in the proof of Lemma 4.20.
r Proving that the invariant, together with the negation of the boolean guard, is
strong enough to prove the desired postcondition. This is the last implication of
the proof tableau.

292
4 Program verification
r Proving that the invariant is established by the code before the while-statement.
We simply push it upwards through the three initial assignments and check that
the resulting formula is implied by the precondition of the speciﬁcation, here ⊤.
As so often the case, in constructing the tableau, we ﬁnd that two formulas
meet; and we have to prove that the ﬁrst one implies the second one. Some-
times this is easy and we can just note the implication in the tableau. For
example, we readily see that ⊤implies Inv1(a[0], 1) ∧Inv2(a[0], 1): k being
1 forces i and j to be zero in order that the assumptions in Inv1(a[0], k)
and Inv2(a[0], k) be true. But this means that their conclusions are true as
well. However, the proof obligation that the invariant hypothesis imply the
precondition computed within the body of the while-statement reveals the
complexity and ingenuity of this program and its justiﬁcation needs to be
taken oﬀ-line:
Lemma 4.20 Let s and t be any integers, n the length of the array a,
and k an index of that array in the range of 0 < k < n. Then Inv1(s, k) ∧
Inv2(t, k) ∧k ̸= n implies
1.
Inv1(min(s, min(t + a[k], a[k])), k + 1) as well as
2.
Inv2(min(t + a[k], a[k]), k + 1).
PROOF:
1.
Take any i with 0 ≤i < k + 1; we will prove that min(t + a[k], a[k]) ≤Si,k. If
i < k, then Si,k = Si,k−1 + a[k], so what we have to prove is min(t + a[k], a[k]) ≤
Si,k−1 + a[k]; but we know t ≤Si,k−1, so the result follows by adding a[k] to
each side. Otherwise, i = k, Si,k = a[k] and the result follows.
2.
Take any i and j with 0 ≤i ≤j < k + 1; we prove that min(s, t + a[k], a[k]) ≤
Si,j. If i ≤j < k, then the result is immediate. Otherwise, i ≤j = k and the
result follows from part 1 of the lemma.
2
4.4 Proof calculus for total correctness
In the preceding section, we developed a calculus for proving partial correct-
ness of triples

φ

P

ψ

. In that setting, proofs come with a disclaimer: only
if the program P terminates an execution does a proof of ⊢par

φ

P

ψ

tell
us anything about that execution. Partial correctness does not tell us any-
thing if P ‘loops’ indeﬁnitely. In this section, we extend our proof calculus
for partial correctness so that it also proves that programs terminate. In the
previous section, we already pointed out that only the syntactic construct
while B {C} could be responsible for non-termination.

4.4 Proof calculus for total correctness
293
Therefore, the proof calculus for total correctness is the same as
for partial correctness for all the rules except the rule for while-
statements.
A proof of total correctness for a while-statement will consist of two parts:
the proof of partial correctness and a proof that the given while-statement
terminates. Usually, it is a good idea to prove partial correctness ﬁrst since
this often provides helpful insights for a termination proof. However, some
programs require termination proofs as premises for establishing partial cor-
rectness, as can be seen in exercise 1(d) on page 303.
The proof of termination usually has the following form. We identify an
integer expression whose value can be shown to decrease every time we
execute the body of the while-statement in question, but which is always
non-negative. If we can ﬁnd an expression with these properties, it follows
that the while-statement must terminate; because the expression can only
be decremented a ﬁnite number of times before it becomes 0. That is because
there is only a ﬁnite number of integer values between 0 and the initial value
of the expression.
Such integer expressions are called variants. As an example, for the pro-
gram Fac1 of Example 4.2, a suitable variant is x −z. The value of this
expression is decremented every time the body of the while-statement is
executed. When it is 0, the while-statement terminates.
We can codify this intuition in the following rule for total correctness
which replaces the rule for the while statement:

η ∧B ∧0 ≤E = E0

C

η ∧0 ≤E < E0


η ∧0 ≤E

while B {C}

η ∧¬B

Total-while.
(4.15)
In this rule, E is the expression whose value decreases with each execution
of the body C. This is coded by saying that, if its value equals that of the
logical variable E0 before the execution of C, then it is strictly less than E0
after it – yet still it remains non-negative. As before, η is the invariant.
We use the rule Total-while in tableaux similarly to how we use Partial-
while, but note that the body of the rule C must now be shown to satisfy

η ∧B ∧0 ≤E = E0

C

η ∧0 ≤E < E0

.
When we push η ∧0 ≤E < E0 upwards through the body, we have to prove
that what emerges from the top is implied by η ∧B ∧0 ≤E = E0; and
the weakest precondition for the entire while-statement, which gets writ-
ten above that while-statement, is η ∧0 ≤E.

294
4 Program verification
Let us illustrate this rule by proving that ⊢tot

x ≥0

Fac1

y = x!

is
valid, where Fac1 is given in Example 4.2, as follows:
y = 1;
z = 0;
while (x != z) {
z = z + 1;
y = y * z;
}
As already mentioned, x −z is a suitable variant. The invariant (y = z!) of
the partial correctness proof is retained. We obtain the following complete
proof for total correctness:
(x ≥0)
(1 = 0! ∧0 ≤x −0)
Implied
y = 1;
(y = 0! ∧0 ≤x −0)
Assignment
z = 0;
(y = z! ∧0 ≤x −z)
Assignment
while (x != z) {
(y = z! ∧x ̸= z ∧0 ≤x −z = E0)
Invariant Hyp. ∧guard
(y · (z + 1) = (z + 1)! ∧0 ≤x −(z + 1) < E0)
Implied
z = z + 1;
(y · z = z! ∧0 ≤x −z < E0)
Assignment
y = y * z;
(y = z! ∧0 ≤x −z < E0)
Assignment
}
(y = z! ∧x = z)
Total-while
(y = x!)
Implied
and so ⊢tot

x ≥0

Fac1

y = x!

is valid. Two comments are in order:
r Notice that the precondition x ≥0 is crucial in securing the fact that 0 ≤x −z
holds right before the while-statements gets executed: it implies the precondition
1 = 0! ∧0 ≤x −0 computed by our proof. In fact, observe that Fac1 does not
terminate if x is negative initially.
r The application of Implied within the body of the while-statement is valid, but
it makes vital use of the fact that the boolean guard is true. This is an exam-
ple of a while-statement whose boolean guard is needed in reasoning about the
correctness of every iteration of that while-statement.

4.4 Proof calculus for total correctness
295
One may wonder whether there is a program that, given a while-statement
and a precondition as input, decides whether that while-statement termi-
nates on all runs whose initial states satisfy that precondition. One can prove
that there cannot be such a program. This suggests that the automatic ex-
traction of useful termination expressions E cannot be realized either. Like
most other such universal problems discussed in this text, the wish to com-
pletely mechanise such decision or extraction procedures cannot be realised.
Hence, ﬁnding a working variant E is a creative activity which requires skill,
intuition and practice.
Let us consider an example program, Collatz, that conveys the challenge
one may face in ﬁnding suitable termination variants E:
c = x;
while (c != 1) {
if (c % 2 == 0) { c = c / 2; }
else { c = 3*c + 1; }
}
This program records the initial value of x in c and then iterates an if-
statement until, and if, the value of c equals 1. The if-statement tests
whether c is even – divisible by 2 – if so, c stores its current value divided
by 2; if not, c stores ‘three times its current value plus 1.’ The expression
c / 2 denotes integer division, so 11 / 2 renders 5 as does 10 / 2.
To get a feel for this algorithm, consider an execution trace in which the
value of x is 5: the value of c evolves as 5 16 8 4 2 1. For another example,
if the value of x is initially 172, the evolution of c is
172 86 43 130 65 196 98 49 148 74 37 112 56 28 14 7 22
11 34 17 52 26 13 40 20 10 5 16 8 4 2 1
This execution requires 32 iterations of the while-statement to reach a ter-
minating state in which the value of c equals 1. Notice how this trace reaches
5, from where on the continuation is as if 5 were the initial value of x.
For the initial value 123456789 of x we abstract the evolution of c with +
(its value increases in the else-branch) and −(its value decreases in the
if-branch):
+ - - - - - - + - - - + - + - - + - + - + - + - + - + - - + - - -
- + - - - - + - - + - - + - - + - + - - - + - + - - - - - + - - +
- + - - + - - - - + - - - - - - + - - + - + - - + - + - + - - + -
+ - + - + - - + - - - + - + - + - - + - + - - + - + - + - + - + -
+ - - - + - + - + - + - - - - + - - + - - + - - - - + - - - + - +
- + - - - - - + - - - -

296
4 Program verification
This requires 177 iterations of the while-statement to reach a terminating
state. Although it is re-assuring that some program runs terminate, the
irregular pattern of + and −above make it seem very hard, if not impossible,
to come up with a variant that proves the termination of Collatz on all
executions in which the initial value of x is positive.
Finally, let’s consider a really big integer:
32498723462509735034567279652376420563047563456356347563\\
96598734085384756074086560785607840745067340563457640875\\
62984573756306537856405634056245634578692825623542135761\\
9519765129854122965424895465956457
where \\ denotes concatenation of digits. Although this is a very large num-
ber indeed, our program Collatz requires only 4940 iterations to terminate.
Unfortunately, nobody knows a suitable variant for this program that could
prove the validity of ⊢tot

0 < x

Collatz

⊤

. Observe how the use of ⊤as
a postcondition emphasizes that this Hoare triple is merely concerned about
program termination as such. Ironically, there is also no known initial value
of x greater than 0 for which Collatz doesn’t terminate. In fact, things are
even subtler than they may appear: if we replace 3*c + 1 in Collatz with a
diﬀerent such linear expression in c, the program may not terminate despite
meeting the precondition 0 < x; see exercise 6 on page 303.
4.5 Programming by contract
For a valid sequent ⊢tot

φ

P

ψ

, the triple

φ

P

ψ

may be seen as a
contract between a supplier and a consumer of a program P. The supplier
insists that consumers run P only on initial state satisﬁes φ. In that case,
the supplier promises the consumer that the ﬁnal state of that run satisﬁes
ψ. For a valid ⊢par

φ

P

ψ

, the latter guarantee applies only when a run
terminates.
For imperative programming, the validation of Hoare triples can be in-
terpreted as the validation of contracts for method or procedure calls. For
example, our program fragment Fac1 may be the ... in the method body
int factorial (x: int) { ... return y; }
The code for this method can be annotated with its contractual assumptions
and guarantees. These annotations can be checked oﬀ-line by humans, during
compile-time or even at run-time in languages such as Eiﬀel. A possible
format for such contracts for the method factorial is given in Figure 4.4.

4.5 Programming by contract
297
method name:
factorial
input:
x ofType int
assumes:
0 <= x
guarantees:
y = x!
output:
ofType int
modifies only:
y
Figure 4.4. A contract for the method factorial.
The keyword assumes states all preconditions, the keyword guarantees lists
all postconditions. The keyword modifies only speciﬁes which program
variables may change their value during an execution of this method.
Let us see why such contracts are useful. Suppose that your boss tells
you to write a method that computes
n
k

– read ‘n choose k’ – a notion of
combinatorics where 1/
49
6

is your change of getting all six lottery numbers
right out of 49 numbers total. Your boss also tells you that
n
k

=
n!
k! · (n −k)!
(4.16)
holds. The method factorial and its contract (Figure 4.4) is at your dis-
posal. Using (4.16) you can quickly compute some values, such as
5
2

=
5!/(2! · 3!) = 10,
10
0

= 1, and
49
6

= 13983816. You then write a method
choose that makes calls to the method factorial, e.g. you may write
int choose(n : int, k : int) {
return factorial(n) / (factorial(k) * factorial (n - k));
}
This method body consists of a return-statement only which makes three
calls to method factorial and then computes the result according to (4.16).
So far so good. But programming by contract is not just about writing
programs, it is also about writing the contracts for such programs! The
static information about choose – e.g. its name – are quickly ﬁlled into that
contract. But what about the preconditions (assumes) and postconditions
(guarantees)?
At the very least, you must state preconditions that ensure that all
method calls within this method’s body satisfy their preconditions. In this
case, we only call factorial whose precondition is that its input value be
non-negative. Therefore, we require that n, k, and n −k be non-negative.
The latter says that n is not smaller than k.
What about the postconditions of choose? Since the method body de-
clared no local variables, we use result to denote the return value of this

298
4 Program verification
method. The postcondition then states that result equals
n
k

– assuming
that you boss’ equation (4.16) is correct for your preconditions 0 ≤k, 0 ≤n,
and k ≤n. The contract for choose is therefore
method name:
choose
input:
n ofType int, k ofType int
assumes:
0 <= k, 0 <= n, k <= n
guarantees:
result = ‘n choose k’
output:
ofType int
modifies only local variables
From this we learn that programming by contract uses contracts
1.
as assume-guarantee abstract interfaces to methods;
2.
to specify their method’s header information, output type, when calls to its
method are ‘legal,’ what variables that method modiﬁes, and what its output
satisﬁes on all ‘legal’ calls;
3.
to enable us to prove the validity of a contract C for method m by ensuring that
all method calls within m’s body meet the preconditions of these methods and
using that all such calls then meet their respective postconditions.
Programming by contract therefore gives rise to program validation by
contract. One proves the ‘Hoare triple’

assume

method

guarantee

very
much in the style developed in this chapter, except that for all method
invocations within that body we can assume that their Hoare triples are
correct.
Example 4.21 We have already used program validation by contract in our
veriﬁcation of the program that computes the minimal sum for all sections
of an array in Figure 4.3 on page 291. Let us focus on the proof fragment
(Inv1(min(s, min(t + a[k], a[k])), k + 1) ∧Inv2(min(t + a[k], a[k]), k + 1))
Implied (Lemma 4.20)
t = min(t + a[k], a[k]);
(Inv1(min(s, t), k + 1) ∧Inv2(t, k + 1))
Assignment
s = min(s,t);
(Inv1(s, k + 1) ∧Inv2(t, k + 1))
Assignment
Its last line serves as the postcondition which gets pushed through the as-
signment s = min(s,t). But min(s,t) is a method call whose guarantees
are speciﬁed as ‘result equals min(s, t),’ where min(s, t) is a mathematical
notation for the smaller of the numbers s and t. Thus, the rule Assignment
does not substitute the syntax of the method invocation min(s,t) for all
occurrences of s in Inv1(s, k + 1) ∧Inv2(t, k + 1), but changes all such s to
the guarantee min(s, t) of the method call min(s,t) – program validation

4.6 Exercises
299
by contract in action! A similar comment applies for the assignment t =
min(t + a[k], a[k]).
Program validation by contract has to be used wisely to avoid circular
reasoning. If each method is a node in a graph, let’s draw an edge from
method n to method m iﬀwithin the body of n there is a call to method m.
For program validation by contract to be sound, we require that there be
no cycles in this method-dependency graph.
4.6 Exercises
Exercises 4.1
1.
*
If you already have written computer programs yourself, assemble for each pro-
gramming language you used a list of features of its software development envi-
ronment (compiler, editor, linker, run-time environment etc) that may improve
the likelihood that your programs work correctly. Try to rate the eﬀectiveness of
each such feature.
2. Repeat the previous exercise by listing and rating features that may decrease
the likelihood of procuding correct and reliable programs.
Exercises 4.2
1.
*
In what circumstances would if (B) {C1} else {C2} fail to terminate?
2.
*
A familiar command missing from our language is the for-statement. It may be
used to sum the elements in an array, for example, by programming as follows:
s = 0;
for (i = 0; i <= max; i = i+1) {
s = s + a[i];
}
After performing the initial assignment s = 0, this executes i = 0 ﬁrst, then
executes the body s = s + a[i] and the incrementation i = i + 1 continually
until i <= max becomes false. Explain how for (C1; B; C2) {C3} can be deﬁned
as a derived program in our core language.
3. Suppose that you need a language construct repeat {C} until (B) which re-
peats C until B becomes true, i.e.
i. executes C in the current state of the store;
ii. evaluates B in the resulting state of the store;
iii. if B is false, the program resumes with (i); otherwise, the program
repeat {C} until (B) terminates.
This construct sometimes allows more elegant code than a corresponding while-
statement.

300
4 Program verification
(a) Deﬁne repeat C until B as a derived expression using our core language.
(b) Can one deﬁne every repeat expression in our core language extended with
for-statements? (You might need the empty command skip which does noth-
ing.)
Exercises 4.3
1. For any store l as in Example 4.4 (page 264), determine which of the relations
below hold; justify your answers:
(a)
*
l ⊨(x + y < z) →¬(x ∗y = z)
(b) l ⊨∀u (u < y) ∨(u ∗z < y ∗z)
(c)
*
l ⊨x + y −z < x ∗y ∗z.
2.
*
For any φ, ψ and P explain why ⊨par

φ

P

ψ

holds whenever the relation
⊨tot

φ

P

ψ

holds.
3. Let the relation P ⊢l ; l′ hold iﬀP’s execution in store l terminates, resulting
in store l′. Use this formal judgment P ⊢l ; l′ along with the relation l ⊨φ to
deﬁne ⊨par and ⊨tot symbolically.
4. Another reason for proving partial correctness in isolation is that some program
fragments have the form while (true) {C}. Give useful examples of such pro-
gram fragments in application programming.
5.
*
Use the proof rule for assignment and logical implication as appropriate to show
the validity of
(a) ⊢par

x > 0

y = x + 1

y > 1

(b) ⊢par

⊤

y = x; y = x + x + y

y = 3 · x

(c) ⊢par

x > 1

a = 1; y = x; y = y - a

y > 0 ∧x > y

.
6.
*
Write down a program P such that
(a)

⊤

P

y = x + 2

(b)

⊤

P

z > x + y + 4

holds under partial correctness; then prove that this is so.
7. For all instances of Implied in the proof on page 274, specify their corresponding
⊢AR sequents.
8. There is a safe way of relaxing the format of the proof rule for assignment: as
long as no variable occurring in E gets updated in between the assertion ψ[E/x]
and the assignment x = E we may conclude ψ right after this assignment. Ex-
plain why such a proof rule is sound.
9. (a) Show, by means of an example, that the ‘reversed’ version of the rule Implied
⊢AR φ →φ′

φ

C

ψ

⊢AR ψ′ →ψ

φ′
C

ψ′
Implied Reversed
is unsound for partial correctness.
(b) Explain why the modiﬁed rule If-Statement in (4.7) is sound with respect
to the partial and total satisfaction relation.

4.6 Exercises
301
(c)
*
Show that any instance of the modiﬁed rule If-Statement in a proof can
be replaced by an instance of the original If-statement and instances of the
rule Implied. Is the converse true as well?
10.
*
Prove the validity of the sequent ⊢par

⊤

P

z = min(x, y)

, where min(x, y) is
the smallest number of x and y – e.g. min(7, 3) = 3 – and the code of P is given
by
if (x > y) {
z = y;
} else {
z = x;
}
11. For each of the speciﬁcations below, write code for P and prove the partial
correctness of the speciﬁed input/output behaviour:
(a)
*

⊤

P

z = max(w, x, y)

, where max(w, x, y) denotes the largest of w, x
and y.
(b)
*

⊤

P

((x = 5) →(y = 3)) ∧((x = 3) →(y = −1))

.
12. Prove the validity of the sequent ⊢par

⊤

Succ

y = x + 1

without using the
modiﬁed proof rule for if-statements.
13.
*
Show that ⊢par

x ≥0

Copy1

x = y

is valid, where Copy1 denotes the code
a = x;
y = 0;
while (a != 0) {
y = y + 1;
a = a - 1;
}
14.
*
Show that ⊢par

y ≥0

Multi1

z = x · y

is valid, where Multi1 is:
a = 0;
z = 0;
while (a != y) {
z = z + x;
a = a + 1;
}
15. Show that ⊢par

y = y0 ∧y ≥0

Multi2

z = x · y0

is valid, where Multi2 is:
z = 0;
while (y != 0) {
z = z + x;
y = y - 1;
}
16. Show that ⊢par

x ≥0

Copy2

x = y

is valid, where Copy2 is:
y = 0;
while (y != x) {
y = y + 1;
}

302
4 Program verification
17. The program Div is supposed to compute the dividend of integers x by y; this
is deﬁned to be the unique integer d such that there exists some integer r – the
remainder – with r < y and x = d · y + r. For example, if x = 15 and y = 6,
then d = 2 because 15 = 2 · 6 + 3, where r = 3 < 6. Let Div be given by:
r = x;
d = 0;
while (r >= y) {
r = r - y;
d = d + 1;
}
Show that ⊢par

¬(y = 0)

Div

(x = d · y + r) ∧(r < y)

is valid.
18.
*
Show that ⊢par

x ≥0

Downfac

y = x!

is valid7, where Downfac is:
a = x;
y = 1;
while (a > 0) {
y = y * a;
a = a - 1;
}
19. Why can, or can’t, you prove the validity of ⊢par

⊤

Copy1

x = y

?
20. Let all while-statements while (B) {C} in P be annotated with invariant
candidates η at the and of their bodies, and η ∧B at the beginning of their
body.
(a) Explain how a proof of ⊢par

φ

P

ψ

can be automatically reduced to show-
ing the validity of some ⊢AR ψ1 ∧· · · ∧ψn.
(b) Identify such a sequent ⊢AR ψ1 ∧· · · ∧ψn for the proof in Example 4.17 on
page 287.
21. Given n = 5 test the correctness of Min Sum on the arrays below:
(a)
*
[−3, 1, −2, 1, −8]
(b) [1, 45, −1, 23, −1]
(c)
*
[−1, −2, −3, −4, 1097].
22. If we swap the ﬁrst and second assignment in the while-statement of Min Sum,
so that it ﬁrst assigns to s and then to t, is the program still correct? Justify
your answer.
23.
*
Prove the partial correctness of S2 for Min Sum.
24. The program Min Sum does not reveal where a minimal-sum section may be
found in an input array. Adapt Min Sum to achieve that. Can you do this with
a single pass through the array?
25. Consider the proof rule

φ

C

ψ1


φ

C

ψ2


φ

C

ψ1 ∧ψ2

Conj
7 You may have to strengthen your invariant.

4.6 Exercises
303
for Hoare triples.
(a) Show that this proof rule is sound for ⊨par.
(b) Derive this proof rule from the ones on page 270.
(c) Explain how this rule, or its derived version, is used to establish the overall
correctness of Min Sum.
26. The maximal-sum problem is to compute the maximal sum of all sections on
an array.
(a) Adapt the program from page 289 so that it computes the maximal sum of
these sections.
(b) Prove the partial correctess of your modiﬁed program.
(c) Which aspects of the correctness proof given in Figure 4.3 (page 291) can
be ‘re-used?’
Exercises 4.4
1. Prove the validity of the following total-correctness sequents:
(a)
*
⊢tot

x ≥0

Copy1

x = y

(b)
*
⊢tot

y ≥0

Multi1

z = x · y

(c) ⊢tot

(y = y0) ∧(y ≥0)

Multi2

z = x · y0

(d)
*
⊢tot

x ≥0

Downfac

y = x!

(e)
*
⊢tot

x ≥0

Copy2

x = y

, does your invariant have an active part in secur-
ing correctness?
(f) ⊢tot

¬(y = 0)

Div

(x = d · y + r) ∧(r < y)

.
2. Prove total correctness of S1 and S2 for Min Sum.
3. Prove that ⊢par is sound for ⊨par. Just like in Section 1.4.3, it suﬃces to assume
that the premises of proof rules are instances of ⊨par. Then, you need to prove
that their respective conclusion must be an instance of ⊨par as well.
4. Prove that ⊢tot is sound for ⊨tot.
5. Implement program Collatz in a programming language of your choice such
that the value of x is the program’s input and the ﬁnal value of c its output.
Test your program on a range of inputs. Which is the biggest integer for which
your program terminates without raising an exception or dumping the core?
6. A function over integers f : I →I is aﬃne iﬀthere are integers a and b such that
f(x) = a · x + b for all x ∈I. The else-branch of the program Collatz assigns to
c the value f(c), where f is an aﬃne function with a = 3 and b = 1.
(a) Write an parameterized implementation of Collatz in which you can initially
specify the values of a and b either statically or through keyboard input such
that the else-branch assigns to c the value of f(c).
(b) Determine for which pairs (a, b) ∈I × I the set Pos
def
= {x ∈I | 0 < x} is in-
variant under the aﬃne function f(x) = a · x + b: for all x ∈Pos, f(x) ∈Pos.
(c)
*
Find an aﬃne function that leaves Pos invariant, but not the set Odd
def
= {x ∈
I | ∃y ∈I: x = 2 · y + 1}, such that there is an input drawn from Pos whose

304
4 Program verification
execution with the modiﬁed Collatz program eventually enters a cycle, and
therefore does not terminate.
Exercises 4.5
1. Consider methods of the form boolean certify V(c : Certificate) which
return true iﬀthe certiﬁcate c is judged valid by the veriﬁer V, a class in which
method certify V resides.
(a)
*
Discuss how programming by contract can be used to delegate the judgment
of a certiﬁcate to another veriﬁer.
(b)
*
What potential problems do you see in this context if the resulting method-
dependency graph is circular?
2.
*
Consider the method
boolean withdraw(amount: int) {
if (amount < 0 && isGood(amount))
{ balance = balance - amount;
return true;
} else { return false; }
}
named withdraw which attempts to withdraw amount from an integer ﬁeld
balance of the class within which method withdraw lives. This method makes
use of another method isGood which returns true iﬀthe value of balance is
greater or equal to the value of amount.
(a) Write a contract for method isGood.
(b) Use that contract to show the validity of the contract for withdraw:
method name:
withdraw
input:
amount of Type int
assumes:
0 <= balance
guarantees:
0 <= balance
output:
of Type boolean
modifies only:
balance
Notice that the precondition and postcondition of this contract are the same
and refer to a ﬁeld of the method’s object. Upon validation, this contract
establishes that all calls to withdraw leave (the ‘object invariant’) 0 <=
balance invariant.
4.7 Bibliographic notes
An early exposition of the program logics for partial and total correctness of
programs written in an imperative while-language can be found in [Hoa69].
The text [Dij76] contains a formal treatment of weakest preconditions.

4.7 Bibliographic notes
305
Backhouse’s book [Bac86] describes program logic and weakest precondi-
tions and also contains numerous examples and exercises. Other books giv-
ing more complete expositions of program veriﬁcation than we can in this
chapter are [AO91, Fra92]; they also extend the basic core language to in-
clude features such as procedures and parallelism. The issue of writing to
arrays and the problem of array cell aliasing are described in [Fra92]. The
original article describing the minimal-sum section problem is [Gri82]. A
gentle introduction to the mathematical foundations of functional program-
ming is [Tur91]. Some web sites deal with software liability and possible
standards for intellectual property rights applied to computer programs8 9.
Text books on systematic programming language design by uniform exten-
sions of the core language we presented at the beginning of this chapter are
[Ten91, Sch94]. A text on functional programming on the freely available
language Standard ML of New Jersey is [Pau91].
8 www.opensource.org
9 www.sims.berkeley.edu/~pam/papers.html

5
Modal logics and agents
5.1 Modes of truth
In propositional or predicate logic, formulas are either true, or false, in any
model. Propositional logic and predicate logic do not allow for any further
possibilities. From many points of view, however, this is inadequate. In nat-
ural language, for example, we often distinguish between various ‘modes’ of
truth, such as necessarily true, known to be true, believed to be true and true
in the future. For example, we would say that, although the sentence
George W. Bush is president of the United States of America.
is currently true, it will not be true at some point in the future. Equally, the
sentence
There are nine planets in the solar system.
while true, and maybe true for ever in the future, is not necessarily true, in
the sense that it could have been a diﬀerent number. However, the sentence
The cube root of 27 is 3.
as well as being true is also necessarily true and true in the future. It does
not enjoy all modes of truth, however. It may not be known to be true by
some people (children, for example); it may not be believed by others (if
they are mistaken).
In computer science, it is often useful to reason about modes of truth. In
Chapter 3, we studied the logic CTL in which we could distinguish not only
between truth at diﬀerent points in the future, but also between diﬀerent
futures. Temporal logic is thus a special case of modal logic. The modalities
of CTL allow us to express a host of computational behaviour of systems.
Modalities are also extremely useful in modelling other domains of com-
puter science. In artiﬁcial intelligence, for example, scenarios with several
306

5.2 Basic modal logic
307
interacting agents are developed. Each agent may have diﬀerent knowledge
about the environment and also about the knowledge of other agents. In this
chapter, we will look in depth at modal logics applied to reasoning about
knowledge.
Modal logic adds unary connectives to express one, or more, of these
diﬀerent modes of truth. The simplest modal logics just deal with one con-
cept – such as knowledge, necessity, or time. More sophisticated modal logics
have connectives for expressing several modes of truth in the same logic; we
will see some of these towards the end of this chapter.
We take a logic engineering approach in this chapter, in which we address
the following question: given a particular mode of truth, how may we develop
a logic capable of expressing and formalising that concept? To answer this
question, we need to decide what properties the logic should have and what
examples of reasoning it should be able to express. Our main case study will
be the logic of knowledge in a multi-agent system. But ﬁrst, we look at the
syntax and semantics of basic modal logic.
5.2 Basic modal logic
5.2.1 Syntax
The language of basic modal logic is that of propositional logic with two
extra connectives, 2 and 3. Like negation (¬), they are unary connectives
as they apply themselves to a single formula only. As done in Chapters 1
and 3, we write p, q, r, p3 . . . to denote atomic formulas.
Deﬁnition 5.1 The formulas of basic modal logic φ are deﬁned by the
following Backus Naur form (BNF):
φ ::= ⊥| ⊤| p | (¬φ) | (φ ∧φ) | (φ ∨φ) | (φ →φ) | (φ ↔φ) | (2φ) | (3φ)
(5.1)
where p is any atomic formula.
Example formulas of basic modal logic are (p ∧3(p →2¬r)) and 2((3q ∧
¬r) →2p), having the parse trees shown in Figure 5.1. The following strings
are not formulas, because they cannot be constructed using the grammar
in (5.1): (p2 →q) and (p →3(q 3 r)).
Convention 5.2 As done in Chapter 1, we assume that the unary connec-
tives (¬, 2 and 3) bind most closely, followed by ∧and ∨and then followed
by →and ↔.

308
5 Modal logics and agents
∧
p
3
→
p
2
¬
r
2
→
2
¬
3
∧
r
p
q
Figure 5.1. Parse trees for (p ∧3(p →2¬r)) and 2((3q ∧¬r) →2p).
This convention allows us to remove many sets of brackets, retaining them
only to avoid ambiguity, or to override these binding priorities. For example,
2((3q ∧¬r) →2p) can be written 2(3q ∧¬r →2p). We cannot omit the
remaining brackets, however, for 23q ∧¬r →2p has quite a diﬀerent parse
tree (see Figure 5.2) from the one in Figure 5.1.
In basic modal logic, 2 and 3 are read ‘box’ and ‘diamond,’ but, when
we apply modal logics to express various modes of truth, we may read them
appropriately. For example, in the logic that studies necessity and possibility,
2 is read ‘necessarily’ and 3 ‘possibly;’ in the logic of agent Q’s knowledge,
2 is read ‘agent Q knows’ and 3 is read ‘it is consistent with agent Q’s
knowledge that,’ or more colloquially, ‘for all Q knows.’ We will see why
these readings are appropriate later in the chapter.
5.2.2 Semantics
For a formula of propositional logic, a model is simply an assignment of
truth values to each of the atomic formulas present in that formula – we
called such models valuation in Chapter 1. However, this notion of model is
inadequate for modal logic, since we want to distinguish between diﬀerent
modes, or degrees, of truth.

5.2 Basic modal logic
309
→
2
¬
∧
p
r
q
3
2
Figure 5.2. The parse tree for 23q ∧¬r →2p.
Deﬁnition 5.3 A model M of basic modal logic is speciﬁed by three
things:
1.
A set W, whose elements are called worlds;
2.
A relation R on W (R ⊆W × W), called the accessibility relation;
3.
A function L : W →P(Atoms), called the labelling function.
We write R(x, y) to denote that (x, y) is in R.
These models are often called Kripke models, in honour of S. Kripke who
invented them and worked extensively in modal logic in the 1950s and 1960s.
Intuitively, w ∈W stands for a possible world and R(w, w′) means that w′
is a world accessible from world w. The actual nature of that relationship
depends on what we intend to model. Although the deﬁnition of models
looks quite complicated, we can use an easy graphical notation to depict
ﬁnite models. We illustrate the graphical notation by an example. Suppose
W equals {x1, x2, x3, x4, x5, x6} and the relation R is given as follows:
r R(x1, x2), R(x1, x3), R(x2, x2), R(x2, x3), R(x3, x2), R(x4, x5), R(x5, x4),
R(x5, x6); and no other pairs are related by R.
Suppose further that the labelling function behaves as follows:
x
x1 x2
x3
x4 x5 x6
L(x) {q} {p, q} {p} {q} ∅{p}

310
5 Modal logics and agents
Then, the Kripke model is illustrated in Figure 5.3. The set W is drawn as
a set of circles, with arrows between them showing the relation R. Within
each circle is the value of the labelling function in that world. If you have
read Chapter 3, then you might have noticed that Kripke structures are also
the models for CTL, where W is S, the set of states; R is →, the relation
of state transitions; and L is the labelling function.
Deﬁnition 5.4 Let M = (W, R, L) be a model of basic modal logic. Sup-
pose x ∈W and φ is a formula of (5.1). We will deﬁne when formula φ is true
in the world x. This is done via a satisfaction relation x ⊩φ by structural
induction on φ:
x ⊩⊤
x ̸⊩⊥
x ⊩p iﬀp ∈L(x)
x ⊩¬φ iﬀx ̸⊩φ
x ⊩φ ∧ψ
iﬀx ⊩φ and x ⊩ψ
x ⊩φ ∨ψ
iﬀx ⊩φ , or x ⊩ψ
x ⊩φ →ψ
iﬀx ⊩ψ , whenever we have x ⊩φ
x ⊩φ ↔ψ
iﬀ(x ⊩φ iﬀx ⊩ψ)
x ⊩2ψ
iﬀ, for each y ∈W with R(x, y), we have y ⊩ψ
x ⊩3ψ
iﬀthere is a y ∈W such that R(x, y) and y ⊩ψ.
When x ⊩φ holds, we say ‘x satisﬁes φ,’ or ‘φ is true in world x.’ We write
M, x ⊩φ if we want to stress that x ⊩φ holds in the model M.
The ﬁrst two clauses just express the fact that ⊤is always true, while ⊥is
always false. Next, we see that L(x) is the set of all the atomic formulas that
are true at x. The clauses for the boolean connectives (¬, ∧, ∨, →and ↔)
should also be straightforward: they mean that we apply the usual truth-
table semantics of these connectives in the current world x. The interesting
cases are those for 2 and 3. For 2φ to be true at x, we require that φ be
true in all the worlds accessible by R from x. For 3φ, it is required that
there is at least one accessible world in which φ is true. Thus, 2 and 3
are a bit like the quantiﬁers ∀and ∃of predicate logic, except that they do
not take variables as arguments. This fact makes them conceptually much
simpler than quantiﬁers. The modal operators 2 and 3 are also rather like
AX and EX in CTL – see Section 3.4.1. Note that the meaning of φ1 ↔φ2
coincides with that of (φ1 →φ2) ∧(φ2 →φ1); we call it ‘if and only if.’
Deﬁnition 5.5 A model M = (W, R, L) of basic modal logic is said to sat-
isfy a formula if every state in the model satisﬁes it. Thus, we write M ⊨φ
iﬀ, for each x ∈W, x ⊩φ.

5.2 Basic modal logic
311
q
p
q
p, q
x1
x2
x3
x4
x5
x6
p
Figure 5.3. A Kripke model.
Examples 5.6 Consider the Kripke model of Figure 5.3. We have:
r x1 ⊩q, since q ∈L(x1).
r x1 ⊩3q, for there is a world accessible from x1 (namely, x2) which satisﬁes q.
In mathematical notation: R(x1, x2) and x2 ⊩q.
r x1 ̸⊩2q, however. This is because x1 ⊩2q says that all worlds accessible from
x1 (i.e. x2 and x3) satisfy q; but x3 does not.
r x5 ̸⊩2p and x5 ̸⊩2q. Moreover, x5 ̸⊩2p ∨2q. However, x5 ⊩2(p ∨q).
To see these facts, note that the worlds accessible from x5 are x4 and x6. Since
x4 ̸⊩p, we have x5 ̸⊩2p; and since x6 ̸⊩q, we have x5 ̸⊩2q. Therefore, we get
that x5 ̸⊩2p ∨2q. However, x5 ⊩2(p ∨q) holds because, in each of x4 and x6,
we ﬁnd p or q.
r The worlds which satisfy 2p →p are x2, x3, x4, x5 and x6; for x2, x3 and x6
this is so since they already satisfy p; for x4 this is true since it does not satisfy
2p – we have R(x4, x5) and x5 does not satisfy p; a similar reason applies to x5.
As for x1, it cannot satisfy 2p →p since it satisﬁes 2p but not p itself.
Worlds like x6 that have no world accessible to them deserve special attention
in modal logic. Observe that x6 ̸⊩3φ, no matter what φ is, because 3φ
says ‘there is an accessible world which satisﬁes φ.’ In particular, ‘there is
an accessible world,’ which in the case of x6 there is not. Even when φ is
⊤, we have x6 ̸⊩3⊤. So, although ⊤is satisﬁed in every world, 3⊤is not
necessarily. In fact, x ⊩3⊤holds iﬀx has at least one accessible world.
A dual situation exists for the satisfaction of 2φ in worlds with no accessi-
ble world. No matter what φ is, we ﬁnd that x6 ⊩2φ holds. That is because
x6 ⊩2φ says that φ is true in all worlds accessible from x6. There are no
such worlds, so φ is vacuously true in all of them: there is simply nothing
to check. This reading of ‘for all accessible worlds’ may seem surprising, but
it secures the de Morgan rules for the box and diamond modalities shown

312
5 Modal logics and agents
→
φ
φ
3
2
Figure 5.4. The parse tree of the formula scheme φ →23φ.
below. Even 2⊥is true in x6. If you wanted to convince someone that 2⊥
was not true in x6, you’d have to show that there is a world accessible from
x6 in which ⊥is not true; but you can’t do this, for there are no worlds
accessible from x6. So again, although ⊥is false in every world, 2⊥might
not be false. In fact, x ⊩2⊥holds iﬀx has no accessible worlds.
Formulas and formula schemes
The grammar in (5.1) speciﬁes ex-
actly the formulas of basic modal logic, given a set of atomic formulas. For
example, p →23p is such a formula. It is sometimes useful to talk about
a whole family of formulas which have the same ‘shape;’ these are called
formula schemes. For example, φ →23φ is a formula scheme. Any formula
which has the shape of a certain formula scheme is called an instance of the
scheme. For example,
r p →23p
r q →23q
r (p ∧3q) →23(p ∧3q)
are all instances of the scheme φ →23φ. An example of a formula scheme
of propositional logic is φ ∧ψ →ψ. We may think of a formula scheme as
an under-speciﬁed parse tree, where certain portions of the tree still need to
be supplied – e.g. the tree of φ →23φ is found in Figure 5.4.

5.2 Basic modal logic
313
Semantically, a scheme can be thought of as the conjunction of all its
instances – since there are generally inﬁnitely many such instances, this
cannot be carried out syntactically! We say that a world/model satisﬁes a
scheme if it satisﬁes all its instances. Note that an instance being satisﬁed
in a Kripke model does not imply that the whole scheme is satisﬁed. For
example, we may have a Kripke model in which all worlds satisfy ¬p ∨q,
but at least one world does not satisfy ¬q ∨p; the scheme ¬φ ∨ψ is not
satisﬁed.
Equivalences between modal formulas
Deﬁnition 5.7 1.
We say that a set of formulas Γ of basic modal logic seman-
tically entails a formula ψ of basic modal logic if, in any world x of any model
M = (W, R, L), we have x ⊩ψ whenever x ⊩φ for all φ ∈Γ. In that case, we
say that Γ ⊨ψ holds.
2.
We say that φ and ψ are semantically equivalent if φ ⊨ψ and ψ ⊨φ hold. We
denote this by φ ≡ψ.
Note that φ ≡ψ holds iﬀany world in any model which satisﬁes one
of them also satisﬁes the other. The deﬁnition of semantic equivalence is
based on semantic entailment in the same way as the corresponding one for
formulas of propositional logic. However, the underlying notion of semantic
entailment for modal logic is quite diﬀerent, as we will see shortly.
Any equivalence in propositional logic is also an equivalence in modal
logic. Indeed, if we take any equivalence in propositional logic and substi-
tute the atoms uniformly for any modal logic formula, the result is also
an equivalence in modal logic. For example, take the equivalent formulas
p →¬q and ¬(p ∧q) and now perform the substitution
p
→
2p ∧(q →p)
q
→
r →3(q ∨p).
The result of this substitution is the pair of formulas
2p ∧(q →p) →¬(r →3(q ∨p))
(5.2)
¬((2p ∧(q →p)) ∧(r →3(q ∨p)))
which are equivalent as formulas of basic modal logic.
We have already noticed that 2 is a universal quantiﬁer on accessible
worlds and 3 is the corresponding existential quantiﬁer. In view of these
facts, it is not surprising to ﬁnd that de Morgan rules apply for 2 and 3:
¬2φ ≡3¬φ and ¬3φ ≡2¬φ.

314
5 Modal logics and agents
Moreover, 2 distributes over ∧and 3 distributes over ∨:
2(φ ∧ψ) ≡2φ ∧2ψ and 3(φ ∨ψ) ≡3φ ∨3ψ.
These equivalences correspond closely to the quantiﬁer equivalences dis-
cussed in Section 2.3.2. It is also not surprising to ﬁnd that 2 does not
distribute over ∨and 3 does not distribute over ∧, i.e. we do not have equiv-
alences between 2(φ ∨ψ) and 2φ ∨2ψ, or between 3(φ ∧ψ) and 3φ ∧3ψ.
For example, in the fourth item of Example 5.6 we had x5 ⊩2(p ∨q) and
x5 ̸⊩2p ∨2q.
Note that 2⊤is equivalent to ⊤, but not to 3⊤, as we saw earlier.
Similarly, 3⊥≡⊥but they are not equivalent to 2⊥.
Another equivalence is 3⊤≡2p →3p. For suppose x ⊩3⊤– i.e. x has
an accessible world, say y – and suppose x ⊩2p; then y ⊩p, so x ⊩3p.
Conversely, suppose x ⊩2p →3p; we must show it satisﬁes 3⊤. Let us
distinguish between the cases x ⊩2p and x ̸⊩2p; in the former, we get
x ⊩3p from x ⊩2p →3p and so x must have an accessible world; and in
the latter, x must again have an accessible world in order to avoid satisfying
2p. Either way, x has an accessible world, i.e. satisﬁes 3⊤. Naturally, this
argument works for any formula φ, not just an atom p.
Valid formulas
Deﬁnition 5.8 A formula φ of basic modal logic is said to be valid if it is
true in every world of every model, i.e. iﬀ⊨φ holds.
Any propositional tautology is a valid formula and so is any substitution
instance of it. A substitution instance of a formula is the result of uniformly
substituting the atoms of the formula by other formulas as done in (5.2).
For example, since p ∨¬p is a tautology, performing the substitution p →
2p ∧(q →p) gives us a valid formula (2p ∧(q →p)) ∨¬(2p ∧(q →p)).
As we may expect from equivalences above, these formulas are valid:
¬2φ ↔3¬φ
2(φ ∧ψ) ↔2φ ∧2ψ
(5.3)
3(φ ∨ψ) ↔3φ ∨3ψ.
To prove that the ﬁrst of these is valid, we reason as follows. Suppose x is
a world in a model M = (W, R, L). We want to show x ⊩¬2φ ↔3¬φ, i.e.
that x ⊩¬2φ iﬀx ⊩3¬φ. Well, using Deﬁnition 5.4,

5.2 Basic modal logic
315
b
c
d
a
e
p, q
p, q
q
p
Figure 5.5. Another Kripke model.
x ⊩¬2φ
iﬀit isn’t the case that x ⊩2φ
iﬀit isn’t the case that, for all y such that R(x, y), y ⊩φ
iﬀthere is some y such that R(x, y) and not y ⊩φ
iﬀthere is some y such that R(x, y) and y ⊩¬φ
iﬀx ⊩3¬φ.
Proofs that the other two are valid are similarly routine and left as exercises.
Another important formula which can be seen to be valid is the following:
2(φ →ψ) ∧2φ →2ψ.
It is sometimes written in the equivalent, but slightly less intuitive, form
2(φ →ψ) →(2φ →2ψ). This formula scheme is called K in most books
about modal logic, honouring the logician S. Kripke who, as we mentioned
earlier, invented the so-called ‘possible worlds semantics’ of Deﬁnition 5.4.
To see that K is valid, again suppose we have some world x in some
model M = (W, R, L). We have to show that x ⊩2(φ →ψ) ∧2φ →2ψ.
Again referring to Deﬁnition 5.4, we assume that x ⊩2(φ →ψ) ∧2φ and
try to prove that x ⊩2ψ:
x ⊩2(φ →ψ) ∧2φ
iﬀx ⊩2(φ →ψ) and x ⊩2φ
iﬀfor all y with R(x, y), we have y ⊩φ →ψ and y ⊩φ
implies that, for all y with R(x, y), we have y ⊩ψ
iﬀx ⊩2ψ.
There aren’t any other interesting valid formulas in basic modal logic. Later,
we will see additional valid formulas in extended modal logics of interest.

316
5 Modal logics and agents
5.3 Logic engineering
Having looked at the framework for basic modal logic, we turn now to how
one may formalise the diﬀerent modes of truth discussed at the beginning
of this chapter. The basic framework is quite general and can be reﬁned
in various ways to give us the properties appropriate for the intended ap-
plications. Logic engineering is the subject of engineering logics to ﬁt new
applications. It is potentially a very broad subject, drawing on all branches
of logic, computer science and mathematics. In this chapter, however, we
are restricting ourselves to the particular engineering of modal logics.
We will consider how to re-engineer basic modal logic to ﬁt the following
readings of 2φ:
r It is necessarily true that φ
r It will always be true that φ
r It ought to be that φ
r Agent Q believes that φ
r Agent Q knows that φ
r After any execution of program P, φ holds.
As modal logic automatically gives us the connective 3, which is equivalent
to ¬2¬, we can ﬁnd out what the corresponding readings of 3 in our system
will be. For example, ‘it is not necessarily true that not φ’ means that it is
possibly true that φ. You could work this out in steps:
It is not necessarily true that φ
= it is possible that not φ.
Therefore,
It is not necessarily true that not φ
= it is possible that not not φ
= it is possible that φ.
Let us work this out with the reading ‘agent Q knows φ’ for 2φ. Then, 3φ
is read as
agent Q does not know not φ
= as far as Q’s knowledge is concerned, φ could be the case
= φ is consistent with what agent Q knows
= for all agent Q knows, φ.
The readings for 3 for the other modes are given in Table 5.6.

5.3 Logic engineering
317
Table 5.6. The readings of 3 corresponding to each reading of 2.
2φ
3φ
It is necessarily true that φ
It is possibly true that φ
It will always be true that φ
Sometime in the future φ
It ought to be that φ
It is permitted to be that φ
Agent Q believes that φ
φ is consistent with Q’s beliefs
Agent Q knows that φ
For all Q knows, φ
After any execution of program P, φ holds
After some execution of P, φ holds
5.3.1 The stock of valid formulas
We saw in the last section some valid formulas of basic modal logic, such
as instances of the axiom scheme K: 2(φ →ψ) →(2φ →2ψ) and of the
schemes in (5.3). Many other formulas, such as
r 2p →p
r 2p →22p
r ¬2p →2¬2p
r 3⊤
are not valid. For example, for each one of these, there is a world in the
Kripke model of Figure 5.3 which does not satisfy the formula. The world
x1 satisﬁes 2p, but it does not satisfy p, so it does not satisfy 2p →p. If we
add R(x2, x1) to our model, then x1 still satisﬁes 2p but does not satisfy
22p. Thus, x1 fails to satisfy 2p →22p. If we change L(x4) to {p, q}, then
x4 does not satisfy ¬2p →2¬2p, because it satisﬁes ¬2p, but it does not
satisfy 2¬2p – the path R(x4, x5)R(x5, x4) serves as a counter example.
Finally, x6 does not satisfy 3⊤, for this formula states that there is an
accessible world satisfying ⊤, which is not the case.
If we are to build a logic capturing the concept of necessity, however, we
must surely have that 2p →p is valid; for anything which is necessarily true
is also simply true. Similarly, we would expect 2p →p to be valid in the
case that 2p means ‘agent Q knows p,’ for anything which is known must
also be true. We cannot know something which is false. We can, however,
believe falsehoods, so in the case of a logic of belief, we would not expect
2p →p to be valid.
Part of the job of logic engineering is to determine what formula schemes
should be valid and to craft the logic in such a way that precisely those ones
are valid.
Table 5.7 shows six interesting readings for 2 and eight formula schemes.
For each reading and each formula scheme, we decide whether we should
expect the scheme to be valid. Notice that we should only put a tick if the

318
5 Modal logics and agents
2φ
2φ →φ
2φ →22φ
3φ →23φ
3⊤
2φ →3φ
2φ ∨2¬φ
2(φ →ψ) ∧2φ →2ψ
3φ ∧3ψ →3(φ ∧ψ)
It is necessarily true that φ
√
√
√
√
√
×
√
×
It will always be true that φ
×
√
×
×
×
×
√
×
It ought to be that φ
×
×
×
√
√
×
√
×
Agent Q believes that φ
×
√
√
√
√
×
√
×
Agent Q knows that φ
√
√
√
√
√
×
√
×
After any execut’n of prgrm P, φ holds
×
×
×
×
×
×
√
×
Table 5.7. Which formula schemes should hold for these readings of 2?
formula should be valid for all cases of φ and ψ. If it could be valid for some
cases, but not for others, we put a cross.
There are many points worth noting about Table 5.7. First, observe that
it is rather debatable whether to put a tick, or a cross, in some of the cells.
We need to be precise about the concept of truth we are trying to formalise,
in order to resolve any ambiguity.
Necessity. When we ask ourselves whether 2φ →22φ and 3φ →23φ
should be valid, it seems to depend on what notion of necessity we are
referring to. These formulas are valid if that which is necessary is nec-
essarily necessary. If we are dealing with physical necessity, then this
amounts to: are the laws of the universe themselves physically neces-
sary, i.e. do they entail that they should be the laws of the universe?
The answer seems to be no. However, if we meant logical necessity, it
seems that we should give the answer yes, for the laws of logic are meant
to be those assertions whose truth cannot be denied. The row is ﬁlled
on the understanding that we mean logical necessity.
Always in the future. We must be precise about whether or not the
future includes the present; this is precisely what the formula 2φ →
φ states. It is a matter of convention whether the future includes the
present, or not. In Chapter 3, we saw that CTL adopts the convention
that it does. For variety, therefore, let us assume that the future does not
include the present in this row of the table. That means that 2φ →φ
fails. What about 3⊤? It says that there is a future world in which ⊤
is true. In particular, then, there is a future world, i.e. time has no end.
Whether we regard this as true or not depends on exactly what notion
of ‘the future’ we are trying to model. We assumed the validity of 3⊤

5.3 Logic engineering
319
in Chapter 3 on CTL since this resulted in an easier presentation of our
model-checking algorithms, but we might choose to model it otherwise,
as in Table 5.7.
Ought. In this case the formulas 2φ →22φ and 3φ →23φ state that
the moral codes we adopt are themselves forced upon us by morality.
This seems not to be the case; for example, we may believe that ‘It
ought to be the case that we wear a seat-belt,’ but this does not compel
us to believe that ‘It ought to be the case that we ought to wear a seat-
belt.’ However, anything which ought to be so should be permitted to
be so; therefore, 2φ →3φ.
Belief. To decide whether 3⊤, let us express it as ¬2⊥, for this is seman-
tically equivalent. It says that agent Q does not believe any contradic-
tions. Here we must be precise about whether we are modelling human
beings, with all their foibles and often plainly contradictory beliefs, or
whether we are modelling idealised agents that are logically omniscient –
i.e. capable of working out the logical consequences of their beliefs. We
opt to model the latter concept. The same issue arises when we consider,
for example, 3φ →23φ, which – when we rewrite it as ¬2ψ →2¬2ψ –
says that, if agent Q doesn’t believe something, then he believes that he
doesn’t believe it. Validity of the formula 2φ ∨2¬φ would mean that Q
has an opinion on every matter; we suppose this is unlikely. What about
3φ ∧3ψ →3(φ ∧ψ)? Let us rewrite it as ¬3(φ ∧ψ) →¬(3φ ∧3ψ),
i.e. 2(¬φ ∨¬ψ) →(2¬φ ∨2¬ψ) or – if we subsume the negations into
the φ and ψ – the formula 2(φ ∨ψ) →(2φ ∨2ψ). This seems not to
be valid, for agent Q may be in a situation in which she or he believes
that there is a key in the red box, or in the green box, without believing
that it is in the red box and also without believing that it is in the green
box.
Knowledge. It seems to diﬀer from belief only in respect of the ﬁrst for-
mula in Table 5.7; while agent Q can have false beliefs, he can only know
that which is true. In the case of knowledge, the formulas 2φ →22φ
and ¬2ψ →2¬2ψ are called positive introspection and negative intro-
spection, respectively, since they state that the agent can introspect upon
her knowledge; if she knows something, she knows that she knows it; and
if she does not know something, she again knows that she doesn’t know
it. Clearly, this represents idealised knowledge, since most humans – with
all their hang-ups and infelicities – do not satisfy these properties. The
formula scheme K is sometimes referred to as logical omniscience in the
logic of knowledge, since it says that the agent’s knowledge is closed
under logical consequence. This means that the agent knows all the

320
5 Modal logics and agents
consequences of anything he knows, which is unfortunately (or fortu-
nately?) true only for idealised agents, not humans.
Execution of programs. Not many of our formulas seem to hold in this
case. The scheme 2φ →22φ says that running the program twice is the
same as running it once, which is plainly wrong in the case of a program
which deducts money from your bank account. The formula 3⊤says
that there is an execution of the program which terminates; this is false
for some programs.
The formula schemes 3⊤and 2φ →3φ were seen to be equivalent in the
preceding section and, indeed, we see that they get the same pattern of ticks
and crosses. We can also show that 2φ →φ entails 3⊤– i.e. (2φ →φ) →
3⊤is valid – so whenever the former gets a tick, so should the latter. This
is indeed the case, as you can verify in Table 5.7.
5.3.2 Important properties of the accessibility relation
So far, we have been engineering logics at the level of deciding what formulas
should be valid for the various readings of 2. We can also engineer logics
at the level of Kripke models. For each of our six readings of 2, there is a
corresponding reading of the accessibility relation R which will then suggest
that R enjoys certain properties such as reﬂexivity or transitivity.
Let us start with necessity. The clauses
x ⊩2ψ
iﬀfor each y ∈W with R(x, y) we have y ⊩ψ
x ⊩3ψ
iﬀthere is a y ∈W such that R(x, y) and y ⊩ψ
from Deﬁnition 5.4 tell us that φ is necessarily true at x if φ is true in all
worlds y accessible from x in a certain way; but accessible in what way?
Intuitively, necessarily φ is true if φ is true in all possible worlds; so R(x, y)
should be interpreted as meaning that y is a possible world according to the
information in x.
In the case of knowledge, we think of R(x, y) as saying: y could be the
actual world according to agent Q’s knowledge at x. In other words, if the
actual world is x, then agent Q – who is not omniscient – cannot rule out
the possibility of it being y. If we plug this deﬁnition into the clause above
for x ⊩2φ, we ﬁnd that agent Q knows φ iﬀφ is true in all the worlds that,
for all he knows, could be the actual world. The meaning of R for each of
the six readings of 2 is shown in Table 5.8.
Recall that a given binary relation R may be:
r reﬂexive: if, for every x ∈W, we have R(x, x);
r symmetric: if, for every x, y ∈W, we have R(x, y) implies R(y, x);

5.3 Logic engineering
321
Table 5.8. For each reading of 2, the meaning of R is given.
2φ
R(x, y)
It is necessarily true that φ
y is possible world according to the in-
formation at x
It will always be true that φ
y is a future world of x
It ought to be that φ
y is an acceptable world according to
the information at x
Agent Q believes that φ
y could be the actual world according
to Q’s beliefs at x
Agent Q knows that φ
y could be the actual world according
to Q’s knowledge at x
After any execution of P, φ holds
y is a possible resulting state after ex-
ecution of P at x
r serial: if, for every x there is a y such that R(x, y);
r transitive: if, for every x, y, z ∈W, we have R(x, y) and R(y, z) imply R(x, z);
r Euclidean: if, for every x, y, z ∈W with R(x, y) and R(x, z), we have R(y, z);
r functional: if, for each x there is a unique y such that R(x, y);
r linear: if, for every x, y, z ∈W, we have that R(x, y) and R(x, z) together imply
that R(y, z), or y equals z, or R(z, y);
r total: if for every x, y ∈W we have R(x, y) or R(y, x); and
r an equivalence relation: if it is reﬂexive, symmetric and transitive.
Now, let us consider this question: according to the various readings of
R, which of these properties do we expect R to have?
Example 5.9 If 2φ means ‘agent Q knows φ,’ then R(x, y) means y could
be the actual world according to Q’s knowledge at x.
r Should R be reﬂexive? This would say: x could be the actual world according
to Q’s knowledge at x. In other words, Q cannot know that things are diﬀerent
from how they really are – i.e., Q cannot have false knowledge. This is a desirable
property for R to have. Moreover, it seems to rest on the same intuition – i.e. the
impossibility of false knowledge – as the validity of the formula 2φ →φ. Indeed,
the validity of this formula and the property of reﬂexivity are closely related, as
we see later on.
r Should R be transitive? It would say: if y is possible according to Q’s knowledge
at x and z is possible according to her knowledge at y, then z is possible according
to her knowledge at x.
Well, this seems to be true. For suppose it was not true, i.e. at x she knew
something preventing z from being the real world. Then, she would know she
knew this thing at x; therefore, she would know something at y which prevented
z from being the real world; which contradicts our premise.

322
5 Modal logics and agents
In this argument, we relied on positive introspection, i.e. the formula 2φ →22φ.
Again, we will shortly see that there is a close correspondence between R being
transitive and the validity of this formula.
5.3.3 Correspondence theory
We saw in the preceding section that there appeared to be a correspondence
between the validity of 2φ →φ and the property that the accessibility re-
lation R is reﬂexive. The connection between them is that both relied on
the intuition that anything which is known by an agent is true. Moreover,
there also seemed to be a correspondence between 2φ →22φ and R being
transitive; they both seem to assert the property of positive introspection,
i.e. that which is known is known to be known.
In this section, we will see that there is a precise mathematical relation-
ship between these formulas and properties of R. Indeed, to every formula
scheme there corresponds a property of R. From the point of view of logic
engineering, it is important to see this relationship, because it helps one to
understand the logic being studied. For example, if you believe that a cer-
tain formula scheme should be accepted in the system of modal logic you are
engineering, then it is well worth looking at the corresponding property of
R and checking that this property makes sense for the application, too. Al-
ternatively, the meaning of some formulas may seem diﬃcult to understand,
so looking at their corresponding properties of R can help.
To state the relationship between formula schemes and their correspond-
ing properties, we need the notion of a (modal) frame.
Deﬁnition 5.10 A frame F = (W, R) is a set W of worlds and a binary
relation R on W.
A frame is like a Kripke model (Deﬁnition 5.3), except that it has no la-
belling function. From any model we can extract a frame, by just forgetting
about the labelling function; for example, Figure 5.9 shows the frame ex-
tracted from the Kripke model of Figure 5.3. A frame is just a set of worlds
and an accessibility relationship between them. It has no information about
what atomic formulas are true at the various worlds. However, it is useful to
say sometimes that the frame, as a whole, satisﬁes a formula. This is deﬁned
as follows.
Deﬁnition 5.11 A frame F = (W, R) satisﬁes a formula of basic modal
logic φ if, for each labelling function L : W →P(Atoms) and each w ∈W,

5.3 Logic engineering
323
x1
x2
x3
x4
x5
x6
Figure 5.9. The frame of the model in Figure 5.3.
x4
x5
x6
Figure 5.10. Another frame.
the relation M, w ⊩φ holds, where M = (W, R, L) – recall the deﬁnition of
M, w ⊩φ on page 310. In that case, we say that F ⊨φ holds.
One can show that, if a frame satisﬁes a formula, then it also satisﬁes
every substitution instance of that formula. Conversely, if a frame satisﬁes
an instance of a formula scheme, it satisﬁes the whole scheme. This con-
trasts markedly with models. For example, the model of Figure 5.3 satisﬁes
p ∨3p ∨33p, but doesn’t satisfy every instance of φ ∨3φ ∨33φ; for ex-
ample, x6 does not satisfy q ∨3q ∨33q. Since frames don’t contain any
information about the truth or falsity of propositional atoms, they can’t
distinguish between diﬀerent atoms; so, if a frame satisﬁes a formula, it also
satisﬁes the formula scheme obtained by substituting its atoms p, q, . . . by
φ, ψ, . . .
Examples 5.12 Consider the frame F in Figure 5.10.
1.
F satisﬁes the formula 2p →p. To see this, we have to consider any labelling
function of the frame – there are eight such labelling functions, since p could
be true or false in each of the three worlds – and show that each world satisﬁes
the formula for each labelling. Rather than really doing this literally, let us

324
5 Modal logics and agents
x4
x5
x6
p
p
Figure 5.11. A model.
give a generic argument: let x be any world. Suppose that x ⊩2p; we want to
show x ⊩p. We know that R(x, x) because each x is accessible from itself in the
diagram; so, it follows from the clause for 2 in Deﬁnition 5.4 that x ⊩p.
2.
Therefore, our frame F satisﬁes any formula of this shape, i.e. it satisﬁes the
formula scheme 2φ →φ.
3.
The frame does not satisfy the formula 2p →22p. For suppose we take the
labelling of Figure 5.11; then x4 ⊩2p, but x4 ̸⊩22p.
If you think about why the frame of Figure 5.10 satisﬁed 2p →p and why
it did not satisfy 2p →22p, you will probably guess the following:
Theorem 5.13 Let F = (W, R) be a frame.
1.
The following statements are equivalent:
– R is reﬂexive;
– F satisﬁes 2φ →φ;
– F satisﬁes 2p →p;
2.
The following statements are equivalent:
– R is transitive;
– F satisﬁes 2φ →22φ;
– F satisﬁes 2p →22p.
PROOF: Each item 1 and 2 requires us to prove three things: (a) that, if R
has the property, then the frame satisﬁes the formula scheme; and (b) that,
if the frame satisﬁes the formula scheme, it satisﬁes the instance of it; and
(c) that, if the frame satisﬁes a formula instance, then R has the property.
1.
(a) Suppose R is reﬂexive. Let L be a labelling function, so now M = (W, R, L)
is a model of basic modal logic. We need to show M ⊨2φ →φ. That means
we need to show x ⊩2φ →φ for any x ∈W, so pick any x. Use the clause
for implication in Deﬁnition 5.4. Suppose x ⊩2φ; since R(x, x), it immediately
follows from the clause for 2 in Deﬁnition 5.4 that x ⊩p. Therefore, we have
shown x ⊩2φ →φ.
(b) We just set φ to be p.

5.3 Logic engineering
325
Table 5.12. Properties of R corresponding to some formulas.
name
formula scheme
property of R
T
2φ →φ
reﬂexive
B
φ →23φ
symmetric
D
2φ →3φ
serial
4
2φ →22φ
transitive
5
3φ →23φ
Euclidean
2φ ↔3φ
functional
2(φ ∧2φ →ψ) ∨2(ψ ∧2ψ →φ)
linear
(c) Suppose the frame satisﬁes 2p →p. Take any x; we’re going to show R(x, x).
Take a labelling function L such that p ̸∈L(x) and p ∈L(y) for all worlds y
except x. Proof by contradiction: Assume we don’t have R(x, x). Then, x ⊩2p,
since all the worlds accessible from x satisfy p – this is because all the worlds
except x satisfy p; but since F satisﬁes 2p →p, it follows that x ⊩2p →p;
therefore, putting x ⊩2p and x ⊩2p →p together, we get x ⊩p. This is a
contradiction to the assumption that we don’t have R(x, x), since we said that
p ̸∈L(x). So we must have R(x, x) in our frame!
2.
(a) Suppose R is transitive. Let L be a labelling function and M = (W, R, L). We
need to show M ⊩2φ →22φ. That means we need to show x ⊩2φ →22φ
for any x ∈W. Suppose x ⊩2φ; we need to show x ⊩22φ. That is, using the
clause for 2 in Deﬁnition 5.4, that any y such that R(x, y) satisﬁes 2φ; that is,
for any y, z with R(x, y) and R(y, z), we have z ⊩φ.
Well, suppose we did have y and z with R(x, y) and R(y, z). By the fact that R
is transitive, we obtain R(x, z). But we’re supposing that x ⊩2φ, so from the
meaning of 2 we get z ⊩φ, which is what we needed to prove.
(b) Again, just set φ to be p.
(c) Suppose the frame satisﬁes 2p →22p. Take any x, y and z with R(x, y)
and R(y, z); we are going to show R(x, z).
Deﬁne a labelling function L such that p ̸∈L(z) and p ∈L(w) for all worlds
w except z. Suppose we don’t have R(x, z); then x ⊩2p, since w ⊩p for all
w ̸= z. Using the axiom 2p →22p, it follows that x ⊩22p. So y ⊩2p holds
since R(x, y). The latter and R(y, z) then render z ⊩p, a contradiction. Thus,
we must have R(x, z).
2
This picture is completed in Table 5.12, which shows, for a collection of
formulas, the corresponding property of R. What this table means mathe-
matically is the following:
Theorem 5.14 A frame F = (W, R) satisﬁes a formula scheme in Table
5.12 iﬀR has the corresponding property in that table.
The names of the formulas in the left-hand column are historical, but have
stuck and are still used widely in books.

326
5 Modal logics and agents
5.3.4 Some modal logics
The logic engineering approach of this section encourages us to design logics
by picking and choosing a set L of formula schemes, according to the ap-
plication at hand. Some examples of formula schemes that we may wish to
consider for a given application are those in Tables 5.7 and 5.12.
Deﬁnition 5.15 Let L be a set of formula schemes of modal logic and
Γ ∪{ψ} a set of formulas of basic modal logic.
1.
The set Γ is closed under substitution instances iﬀwhenever φ ∈Γ, then any
substitution instance of φ is also in Γ.
2.
Let Lc be the smallest set containing all instances of L.
3.
Γ semantically entails ψ in L iﬀΓ ∪Lc semantically entails ψ in basic modal
logic. In that case, we say that Γ ⊨L ψ holds.
Thus, we have Γ ⊨L ψ if every Kripke model and every world x satisfying
Γ ∪Lc therein also satisﬁes ψ. Note that for L = ∅this deﬁnition is consistent
with the one of Deﬁnition 5.7, since we then have Γ ∪Lc = Γ. For logic
engineering, we require that L be
r closed under substitution instances; otherwise, we won’t be able to characterize
Lc in terms of properties of the accessibility relation; and
r consistent in that there is a frame F such that F ⊨φ holds for all φ ∈L; oth-
erwise, Γ ⊨L ψ holds for all Γ and ψ! In most applications of logic engineering,
consistency is easy to establish.
We now study a few important modal logics that extend basic modal logic
with a consistent set of formula schemes L.
The modal logic K
The weakest modal logic doesn’t have any chosen
formula schemes, like those of Tables 5.7 and 5.12. So L = ∅and this modal
logic is called K as it satisﬁes all instances of the formula scheme K; modal
logics with this property are called normal and all modal logics we study in
this text are normal.
The modal logic KT45
A well-known modal logic is KT45 – also called
S5 in the technical literature – where L = {T, 4, 5} with T, 4 and 5 from
Table 5.12. This logic is used to reason about knowledge; 2φ means that
the agent Q knows φ. Table 5.12 tell us, respectively, that
T. Truth: the agent Q knows only true things.
4. Positive introspection: if the agent Q knows something, then she knows
that she knows it.
5. Negative introspection: if the agent Q doesn’t know something, then
she knows that she doesn’t know it.

5.3 Logic engineering
327
In this application, the formula scheme K means logical omniscience: the
agent’s knowledge is closed under logical consequence. Note that these prop-
erties represent idealisations of knowledge. Human knowledge has none of
these properties! Even computer agents may not have them all. There are
several attempts in the literature to deﬁne logics of knowledge that are more
realistic, but we will not consider them here.
The semantics of the logic KT45 must consider only relations R which
are: reﬂexive (T), transitive (4) and Euclidean (5).
Fact 5.16 A relation is reﬂexive, transitive and Euclidean iﬀit is reﬂexive,
transitive and symmetric, i.e. if it is an equivalence relation.
KT45 is simpler than K in the sense that it has few essentially diﬀerent ways
of composing modalities.
Theorem 5.17 Any sequence of modal operators and negations in KT45
is equivalent to one of the following: −, 2, 3, ¬, ¬2 and ¬3, where −
indicates the absence of any negation or modality.
The modal logic KT4
The modal logic KT4, that is L equals {T, 4},
is also called S4 in the literature. Correspondence theory tells us that its
models are precisely the Kripke models M = (W, R, L), where R is reﬂexive
and transitive. Such structures are often very useful in computer science. For
example, if φ stands for the type of a piece of code – φ could be int × int →
bool, indicating some code which expects a pair of integers as input and
outputs a boolean value – then 2φ could stand for residual code of type φ.
Thus, in the current world x this code would not have to be executed, but
could be saved (= residualised) for execution at a later computation stage.
The formula scheme 2φ →φ, the axiom T, then means that code may be
executed right away, whereas the formula scheme 2φ →22φ, the axiom 4,
allows that residual code remain residual, i.e. we can repeatedly postpone its
execution in future computation stages. Such type systems have important
applications in the specialisation and partial evaluation of code. We refer
the interested reader to the bibliographic notes at the end of the chapter.
Theorem 5.18 Any sequence of modal operators and negations in KT4 is
equivalent to one of the following: −, 2, 3, 23, 32, 232, 323, ¬, ¬2,
¬3, ¬23, ¬32, ¬232 and ¬323.
Intuitionistic propositional logic
In Chapter 1, we gave a natural de-
duction system for propositional logic which was sound and complete with

328
5 Modal logics and agents
respect to semantic entailment based on truth tables. We also pointed out
that the proof rules PBC, LEM and ¬¬e are questionable in certain com-
putational situations. If we disallow their usage in natural deduction proofs,
we obtain a logic, called intuitionistic propositional logic, together with its
own proof theory. So far so good; but it is less clear what sort of semantics
one could have for such a logic – again with soundness and completeness in
mind. This is where certain models of KT4 will do the job quite nicely. Recall
that correspondence theory implies that a model M = (W, R, L) of KT4 is
such that R is reﬂexive and transitive. The only additional requirement we
impose on a model for intuitionistic propositional logic is that its labelling
function L be monotone in R: R(x, y) implies that L(x) is a subset of L(y).
This models that the truth of atomic positive formulas persist throughout
the worlds that are reachable from a given world.
Deﬁnition 5.19 A model of intuitionistic propositional logic is a model
M = (W, R, L) of KT4 such that R(x, y) always implies L(x) ⊆L(y). Given
a propositional logic formula as in (1.3), we deﬁne x ⊩φ as in Deﬁnition 5.4
exception for the clauses →and ¬. For φ1 →φ2 we deﬁne x ⊩φ1 →φ2 iﬀ
for all y with R(x, y) we have y ⊩φ2 whenever we have y ⊩φ1. For ¬φ we
deﬁne x ⊩¬φ iﬀfor all y with R(x, y) we have y ̸⊩φ.
As an example, consider the model W = {x, y} with accessibility relation
R = {(x, x), (x, y), (y, y)}, which is indeed reﬂexive and transitive. For a la-
belling function L with L(x) = ∅and L(y) = {p}, we claim that x ̸⊩p ∨¬p.
(Recall that p ∨¬p is an instance of LEM which we proved in Chapter 1 with
the full natural deduction calculus.) We do not have x ⊩p, for p is not in
the set L(x) which is empty. Thus, Deﬁnition 5.4 for the case ∨implies that
x ⊩p ∨¬p can hold only if x ⊩¬p holds. But x ⊩¬p simply does not hold,
since there is a world y with R(x, y) such that y ⊩p holds, for p ∈L(y). The
availability of possible worlds in the models of KT4 together with a ‘modal
interpretation’ of →and ¬ breaks down the validity of the theorem LEM in
classical logic.
One can now deﬁne semantic entailment in the same manner as for modal
logics. Then, one can prove soundness and completeness of the reduced nat-
ural deduction system with respect to this semantic entailment, but those
proofs are beyond the scope of this book.
5.4 Natural deduction
Verifying semantic entailment Γ ⊨L ψ by appealing to its deﬁnition directly
would be rather diﬃcult. We would have to consider every Kripke model

5.4 Natural deduction
329
that satisﬁes all formulas of Γ and every world in it. Fortunately, we have a
much more usable approach, which is an extension, respectively adaptation,
of the systems of natural deduction met in Chapters 1 and 2. Recall that
we presented natural deduction proofs as linear representations of proof
trees which may involve proof boxes which control the scope of assumptions,
or quantiﬁers. The proof boxes have formulas and/or other boxes inside
them. There are rules which dictate how to construct proofs. Boxes open
with an assumption; when a box is closed – in accordance with a rule –
we say that its assumption is discharged. Formulas may be repeated and
brought into boxes, but may not be brought out of boxes. Every formula
must have some justiﬁcation to its right: a justiﬁcation can be the name
of a rule, or the word ‘assumption,’ or an instance of the proof rule copy;
see e.g. page 13.
Natural deduction works in a very similar way for modal logic. The main
diﬀerence is that we introduce a new kind of proof box, to be drawn with
dashed lines. This is required for the rules for the connective 2. The dashed
proof box has a completely diﬀerent role from the solid one. As we saw
in Chapter 1, going into a solid proof box means making an assumption.
Going into a dashed box means reasoning in an arbitrary accessible world.
If at any point in a proof we have 2φ, we could open a dashed box and put
φ in it. Then, we could work on this φ, to obtain, for example, ψ. Now we
could come out of the dashed box and, since we have shown ψ in an arbi-
trary accessible world, we may deduce 2ψ in the world outside the dashed
box.
Thus, the rules for bringing formulas into dashed boxes and taking for-
mulas out of them are the following:
r Wherever 2φ occurs in a proof, φ may be put into a subsequent dashed box.
r Wherever ψ occurs at the end of a dashed box, 2ψ may be put after that dashed
box.
We have thus added two rules, 2 introduction and 2 elimination:
...
φ
2φ
2i
2φ
...
φ
...
2e

330
5 Modal logics and agents
In modal logic, natural deduction proofs contain both solid and dashed
boxes, nested in any way. Note that there are no explicit rules for 3, which
must be written ¬2¬ in proofs.
The extra rules for KT45
The rules 2i and 2e are suﬃcient for cap-
turing semantic entailment of the modal logic K. Stronger modal logics, e.g.
KT45, require extra rules if one wants to capture their semantic entailment
via proofs. In the case of KT45, this extra strength is expressed by rule
schemes for the axioms T, 4 and 5:
2φ
φ
T
2φ
22φ
4
¬2φ
2¬2φ
5
An equivalent alternative to the rules 4 and 5 would be to stipulate relax-
ations of the rules about moving formulas in and out of dashed boxes. Since
rule 4 allows us to double-up boxes, we could instead think of it as allowing
us to move formulas beginning with 2 into dashed boxes. Similarly, axiom
5 has the eﬀect of allowing us to move formulas beginning with ¬2 into
dashed boxes. Since 5 is a scheme and since φ and ¬¬φ are equivalent in ba-
sic modal logic, we could write ¬φ instead of φ throughout without changing
the expressive power and meaning of that axiom.
Deﬁnition 5.20 Let L be a set of formula schemes. We say that Γ ⊢L ψ is
valid if ψ has a proof in the natural deduction system for basic modal logic
extended with the axioms from L and premises from Γ.
Examples 5.21 We show that the following sequents are valid:
1.
|−K 2p ∧2q →2(p ∧q).
1
2p ∧2q
assumption
2
2p
∧e1 1
3
2q
∧e2 1
4
p
2e 2
5
q
2e 3
6
p ∧q
∧i 4, 5
7
2(p ∧q)
2i 4−6
8
2p ∧2q →2(p ∧q)
→i 1−7

5.5 Reasoning about knowledge in a multi-agent system
331
2.
|−KT45 p →23p.
1
p
assumption
2
2¬p
assumption
3
¬p
T 2
4
⊥
¬e 1, 3
5
¬2¬p
¬i 2−4
6
2¬2¬p
axiom 5 on line 5
7
p →2¬2¬p
→i 1−6
3.
|−KT45 232p →2p.
1
2¬2¬2p
assumption
2
¬2¬2p
2e 1
3
¬2p
assumption
4
2¬2p
axiom 5 on line 3
5
⊥
¬e 4, 2
6
¬¬2p
¬i 3−5
7
2p
¬¬e 6
8
p
T 7
9
2p
2i 2−8
10
2¬2¬2p →2p
→i 1−9
5.5 Reasoning about knowledge in
a multi-agent system
In a multi-agent system, diﬀerent agents have diﬀerent knowledge of the
world. An agent may need to reason about its own knowledge about the
world; it may also need to reason about what other agents know about
the world. For example, in a bargaining situation, the seller of a car must
consider what a buyer knows about the car’s value. The buyer must also
consider what the seller knows about what the buyer knows about that
value and so on.
Reasoning about knowledge refers to the idea that agents in a group take
into account not only the facts of the world, but also the knowledge of other
agents in the group. Applications of this idea include: games, economics,

332
5 Modal logics and agents
cryptography and protocols. It is not very easy for humans to follow the
thread of such nested sentences as
Dean doesn’t know whether Nixon knows that Dean knows that
Nixon knows that McCord burgled O’Brien’s oﬃce at Watergate.
However, computer agents are better than humans in this respect.
5.5.1 Some examples
We start with some classic examples about reasoning in a multi-agent envi-
ronment. Then, in the next section, we engineer a modal logic which allows
for a formal representation of these examples via sequents and which solves
them by proving them in a natural deduction system.
The wise-men puzzle
There are three wise men. It’s common knowl-
edge – known by everyone and known to be known by everyone, etc. – that
there are three red hats and two white hats. The king puts a hat on each
of the wise men in such a way that they are not able to see their own hat,
and asks each one in turn whether they know the colour of the hat on their
head. Suppose the ﬁrst man says he does not know; then the second says he
does not know either.
It follows that the third man must be able to say that he knows the colour
of his hat. Why is this? What colour has the third man’s hat?
To answer these questions, let us enumerate the seven possibilities which
exist: they are
R R R
R R W
R W R
R W W
W R R
W R W
W W R
where, for example, R W W refers to the situation that the ﬁrst, second
and third men have red, white and white hats, respectively. The eighth
possibility, W W W, is ruled out as there are only two white hats.
Now let’s think of it from the second and third men’s point of view.
When they hear the ﬁrst man speak, they can rule out the possibility of
the true situation being R W W, because if it were this situation, then the
ﬁrst man, seeing that the others were wearing white hats and knowing that
there are only two white hats, would have concluded that his hat must be
red. As he said that he did not know, the true situation cannot be R W W.
Notice that the second and third men must be intelligent in order to perform

5.5 Reasoning about knowledge in a multi-agent system
333
this reasoning; and they must know that the ﬁrst man is intelligent and
truthful as well. In the puzzle, we assume the truthfulness and intelligence
and perceptiveness of the men are common knowledge – known by everyone
and known to be known by everyone, etc.
When the third man hears the second man speak, he can rule out the
possibility of the true situation being W R W, for similar reasons: if it were
that, the second man would have said that he knew his hat was red, but
he did not say this. Moreover, the third man can also rule out the situation
R R W when he hears the second man’s answer, for this reason: if the second
man had seen that the ﬁrst was wearing red and the third white, he would
have known that it must be R W W or R R W; but he would have known
from the ﬁrst man’s answer that it couldn’t be R W W, so he would have
concluded it was R R W and that he was wearing a red hat; but he did not
draw this conclusion, so, reasons the third man, it cannot be R R W.
Having heard the ﬁrst and second men speak, the third man has elimi-
nated R W W, W R W and R R W; leaving only R R R, R W R, W R R and
W W R. In all of these he is wearing a red hat, so he concludes that he must
be wearing a red hat.
Notice that the men learn a lot from hearing the other men speak. We
emphasise again the importance of the assumption that they tell the truth
about their state of knowledge and are perceptive and intelligent enough to
come to correct conclusions. Indeed, it is not enough that the three men
are truthful, perceptive and intelligent; they must be known to be so by the
others and, in later examples, this fact must also be known etc. Therefore,
we assume that all this is common knowledge.
The muddy-children puzzle
This is one of the many variations on the
wise-men puzzle; a diﬀerence is that the questions are asked in parallel rather
than sequentially. There is a large group of children playing in the garden –
their perceptiveness, truthfulness and intelligence being common knowledge,
it goes without saying. A certain number of children, say k ≥1, get mud on
their foreheads. Each child can see the mud on others, but not on his own
forehead. If k > 1, then each child can see another with mud on its forehead,
so each one knows that at least one in the group is muddy. Consider these
two scenarios:
Scenario 1. The father repeatedly asks the question ‘Does any of you
know whether you have mud on your own forehead?’ The ﬁrst time they
all answer ‘no;’ but, unlike in the wise-men example, they don’t learn
anything by hearing the others answer ‘no,’ so they go on answering ‘no’
to the father’s repeated questions.

334
5 Modal logics and agents
Scenario 2. The father ﬁrst announces that at least one of them is
muddy – which is something they know already; and then, as before,
he repeatedly asks them ‘Does any of you know whether you have mud
on your own forehead?’ The ﬁrst time they all answer ‘no.’ Indeed, they
go on answering ‘no’ to the ﬁrst k −1 repetitions of that same question;
but at the kth those with muddy foreheads are able to answer ‘yes.’
At ﬁrst sight, it seems rather puzzling that the two scenarios are diﬀerent,
given that the only diﬀerence in the events leading up to them is that in the
second one the father announces something that they already know. It would
be wrong, however, to conclude that the children learn nothing from this
announcement. Although everyone knows the content of the announcement,
the father’s saying it makes it common knowledge among them, so now
they all know that everyone else knows it, etc. This is the crucial diﬀerence
between the two scenarios.
To understand scenario 2, consider a few cases of k.
k = 1, i.e. just one child has mud. That child is immediately able to
answer ‘yes,’ since she has heard the father and doesn’t see any other
child with mud.
k = 2, say only the children Ramon and Candy have mud. Everyone
answers ‘no’ the ﬁrst time. Now Ramon thinks: since Candy answered
‘no’ the ﬁrst time, she must see someone with mud. Well, the only person
I can see with mud is Candy, so if she can see someone else it must be me.
So Ramon answers ‘yes’ the second time. Candy reasons symmetrically
about Ramon and also answers ‘yes’ the second time round.
k = 3, say only the children Alice, Bob, and Charlie have mud. Everyone
answers ‘no’ the ﬁrst two times. But now Alice thinks: if it was just
Bob and Charlie with mud, they would have answered ‘yes’ the second
time; making the argument for k = 2 above. So there must be a third
person with mud; since I can see only Bob and Charlie having mud,
the third person must be me. So Alice answers ‘yes’ the third time. For
symmetrical reasons, so do Bob and Charlie.
And similarly for other cases of k.
To see that it was not common knowledge before the father’s announce-
ment that one of the children was muddy, consider again k = 2, with Ramon
and Candy. Of course, Ramon and Candy both know someone is muddy –
they see each other; but, for example, Ramon doesn’t know that Candy
knows that someone is dirty. For all Ramon knows, Candy might be the
only dirty one and therefore not be able to see a dirty child.

5.5 Reasoning about knowledge in a multi-agent system
335
5.5.2 The modal logic KT45n
We now generalise the modal logic KT45 given in Section 5.3.4. Instead of
having just one 2, it will have many, one for each agent i from a ﬁxed set
A = {1, 2, . . . , n} of agents. We write those modal connectives as Ki (for
each agent i ∈A); the K is to emphasise the application to knowledge. We
assume a collection p, q, r, . . . of atomic formulas. The formula Ki p means
that agent i knows p; so, for example, K1 p ∧K1¬K2K1 p means that agent 1
knows p, but knows that agent 2 doesn’t know he knows it.
We also have the modal connectives EG, where G is any subset of A. The
formula EG p means everyone in the group G knows p. If G = {1, 2, 3, . . . , n},
then EG p is equivalent to K1 p ∧K2 p ∧· · · ∧Kn p. We assume similar bind-
ing priorities to those put forward on page 307.
Convention 5.22 The binding priorities of KT45n are the ones of basic
modal logic, if we think of each modality Ki, EG and CG as ‘being’ 2.
One might think that φ could not be more widely known than everyone
knowing it, but this is not the case. It could be, for example, that everyone
knows φ, but they might not know that they all know it. If φ is supposed
to be a secret, it might be that you and your friend both know it, but your
friend does not know that you know it and you don’t know that your friend
knows it. Thus, EGEG φ is a state of knowledge even greater than EG φ and
EGEGEG φ is greater still. We say that φ is common knowledge among G,
written CG φ, if everyone knows φ and everyone knows that everyone knows
it; and everyone knows that; and knows that etc. So we may think of CG φ
as an inﬁnite conjunction
EG φ ∧EGEG φ ∧EGEGEG φ ∧. . . .
However, since our logics only have ﬁnite conjunctions, we cannot reduce
CG to something which is already in the logic. We have to express the
inﬁnite aspect of CG via its semantics and retain it as an additional modal
connective. Finally, DG φ means the knowledge of φ is distributed among
the group G; although no-one in G may know it, they would be able to
work it out if they put their heads together and combined the information
distributed among them.
Deﬁnition 5.23 A formula φ in the multi-modal logic of KT45n is deﬁned
by the following grammar:
φ ::= ⊥| ⊤| p | (¬φ) | (φ ∧φ) | (φ ∨φ) | (φ →φ) | (φ ↔φ) |
(Ki φ) | (EG φ) | (CG φ) | (DG φ)

336
5 Modal logics and agents
q
q
p, q
x1
x2
x4
x5
p
x6
p
x3
R1
R1, R2
R1, R3
R1, R2
R3
Figure 5.13. A KT45n model for n = 3.
where p is any atomic formula, i ∈A and G ⊆A. We simply write E, C and
D without subscripts if we refer to EA, CA and DA.
Compare this deﬁnition with Deﬁnition 5.1. Instead of 2, we have several
modalities Ki and we also have EG, CG and DG for each G ⊆A. Actually,
all of these connectives will shortly be seen to be ‘box-like’ rather than
‘diamond-like’, in the sense that they distribute over ∧rather than over ∨–
compare this to the discussion of equivalences on page 308. The ‘diamond-
like’ correspondents of these connectives are not explicitly in the language,
but may of course be obtained using negations, i.e. ¬Ki¬, ¬CG¬ etc.
Deﬁnition 5.24 A model M = (W, (Ri)i∈A, L) of the multi-modal logic
KT45n with the set A of n agents is speciﬁed by three things:
1.
a set W of possible worlds;
2.
for each i ∈A, an equivalence relation Ri on W (Ri ⊆W × W), called the
accessibility relations; and
3.
a labelling function L : W →P(Atoms).
Compare this with Deﬁnition 5.3. The diﬀerence is that, instead of just one
accessibility relation, we now have a family, one for each agent in A; and we
assume the accessibility relations are equivalence relations.
We exploit these properties of Ri in the graphical illustrations of Kripke
models for KT45n. For example, a model of KT453 with set of worlds
{x1, x2, x3, x4, x5, x6} is shown in Figure 5.13. The links between the worlds
have to be labelled with the name of the accessibility relation, since we have
several relations. For example, x1 and x2 are related by R1, whereas x4 and

5.5 Reasoning about knowledge in a multi-agent system
337
x5 are related both by R1 and by R2. We simplify by no longer requiring ar-
rows on the links. This is because we know that the relations are symmetric,
so the links are bi-directional. Moreover, the relations are also reﬂexive, so
there should be loops like the one on x4 in Figure 5.11 in all the worlds and
for all of the relations. We can simply omit these from the diagram, since we
don’t need to distinguish between worlds which are self-related and those
which are not.
Deﬁnition 5.25 Take a model M = (W, (Ri)i∈A, L) of KT45n and a world
x ∈W. We deﬁne when φ is true in x via a satisfaction relation x ⊩φ by
induction on φ:
x ⊩p iﬀp ∈L(x)
x ⊩¬φ iﬀx ̸⊩φ
x ⊩φ ∧ψ
iﬀx ⊩φ and x ⊩ψ
x ⊩φ ∨ψ
iﬀx ⊩φ or x ⊩ψ
x ⊩φ →ψ
iﬀx ⊩ψ whenever we have x ⊩φ
x ⊩Ki ψ
iﬀ, for each y ∈W, Ri(x, y) implies y ⊩ψ
x ⊩EG ψ
iﬀ, for each i ∈G, x ⊩Ki ψ
x ⊩CG ψ
iﬀ, for each k ≥1, we have x ⊩Ek
Gψ,
where Ek
G means EGEG . . . EG – k times
x ⊩DG ψ
iﬀ, for each y ∈W, we have y ⊩ψ,
whenever Ri(x, y) for all i ∈G.
Again, we write M, x ⊩φ if we want to emphasise the model M.
Compare this with Deﬁnition 5.4. The cases for the boolean connectives
are the same as for basic modal logic. Each Ki behaves like a 2, but refers to
its own accessibility relation Ri. As already stated, there are no equivalents
of 3, but we can recover them as ¬Ki¬. The connective EG is deﬁned in
terms of the Ki and CG is deﬁned in terms of EG.
Many of the results we had for basic modal logic with a single accessi-
bility relation also hold in this more general setting of several accessibility
relations. Summarising,
r a frame F for KT45n (W, (Ri)i∈A) for the modal logic KT45n is a set W of
worlds and, for each i ∈A, an equivalence relation Ri on W.
r a frame F = (W, (Ri)i∈A) for KT45n is said to satisfy φ if, for each labelling
function L: W →P(Atoms) and each w ∈W, we have M, w ⊩φ holds, where
M = (W, (Ri)i∈A, L). In that case, we say that F ⊨φ holds.
The following theorem is useful for answering questions about formu-
las involving E and C. Let M = (W, (Ri)i∈A, L) be a model for KT45n

338
5 Modal logics and agents
and x, y ∈W. We say that y is G-reachable in k steps from x if there are
w1, w2, . . . , wk−1 ∈W and i1, i2, . . . , ik in G such that
x Ri1 w1 Ri2 w2 . . . Rik−1 wk−1 Rik y
meaning Ri1(x, w1), Ri2(w1, w2), . . . , Rik(wk, y). We also say that y is G-
reachable from x if there is some k such that it is G-reachable in k steps.
Theorem 5.26
1.
x ⊩Ek
Gφ iﬀ, for all y that are G-reachable from x in k steps, we have y ⊩φ.
2.
x ⊩CG φ iﬀ, for all y that are G-reachable from x, we have y ⊩φ.
PROOF:
1.
First, suppose y ⊩φ for all y G-reachable from x in k steps. We will prove
that x ⊩Ek
Gφ holds. It is suﬃcient to show that x ⊩Ki1Ki2 . . . Kik φ for any
i1, i2, . . . , ik ∈G. Take any i1, i2, . . . , ik ∈G and any w1, w2,. . . , wk−1 and y
such that there is a path of the form x Ri1 w1 Ri2 w2 . . . Rik−1 wk−1 Rik y. Since
y is G-reachable from x in k steps, we have y ⊩φ by our assumption, so x ⊩
Ki1Ki2 . . . Kik φ as required.
Conversely, suppose x ⊩Ek
Gφ holds and y is G-reachable from x in k steps. We
must show that y ⊩φ holds. Take i1, i2, . . . , ik by G-reachability; since x ⊩Ek
Gφ
implies x ⊩Ki1Ki2 . . . Kik φ, we have y ⊩φ.
2.
This argument is similar.
Some valid formulas in KT45n
The formula K holds for the connec-
tives Ki, EG, CG and DG, i.e. we have the corresponding formula schemes
Ki φ ∧Ki (φ →ψ) →Ki ψ
EG φ ∧EG (φ →ψ) →EG ψ
CG φ ∧CG (φ →ψ) →CG ψ
DG φ ∧DG (φ →ψ) →DG ψ.
This means that these diﬀerent ‘levels’ of knowledge are closed under log-
ical consequence. For example, if certain facts are common knowledge and
some other fact follows logically from them, then that fact is also common
knowledge.
Observe that E, C and D are ‘box-like’ connectives, in the sense that
they quantify universally over certain accessibility relations. That is to say,
we may deﬁne the relations REG, RDG and RCG in terms of the relations
Ri, as follows:
REG(x, y)
iﬀ
Ri(x, y)
for some i ∈G
RDG(x, y)
iﬀ
Ri(x, y)
for all i ∈G
RCG(x, y)
iﬀ
Rk
EG(x, y)
for each k ≥1.

5.5 Reasoning about knowledge in a multi-agent system
339
It follows from this that EG, DG and CG satisfy the K formula with respect
to the accessibility relations REG, RDG and RCG, respectively.
What about other valid formulas? Since we have stipulated that the rela-
tions Ri are equivalence relations, it follows from the multi-modal analogues
of Theorem 5.13 and Table 5.12 that the following formulas are valid in
KT45n for each agent i:
Ki φ →KiKi φ
positive introspection
¬Ki φ →Ki¬Ki φ
negative introspection
Ki φ →φ
truth.
These formulas also hold for DG, since RDG is also an equivalence rela-
tion, but these don’t automatically generalise for EG and CG. For example,
EG φ →EGEG φ is not valid; if it were valid, it would imply that common
knowledge was nothing more than knowledge by everybody. The scheme
¬EG φ →EG¬EG φ is also not valid. The failure of these formulas to be
valid can be traced to the fact that REG is not necessarily an equivalence
relation, even though each Ri is an equivalence relation. However, REG is
reﬂexive, so EG φ →φ is valid, provided that G ̸= ∅. If G = ∅, then EG φ
holds vacuously, even if φ is false.
Since RCG is an equivalence relation, the formulas T, 4 and 5 above do
hold for CG, although the third one still requires the condition that G ̸= ∅.
5.5.3 Natural deduction for KT45n
The proof system for KT45 is easily extended to KT45n; but for simplicity,
we omit reference to the connective D.
1.
The dashed boxes now come in diﬀerent ‘ﬂavours’ for diﬀerent modal connec-
tives; we’ll indicate the modality in the top left corner of the dashed box.
2.
The axioms T, 4 and 5 can be used for any Ki, whereas axioms 4 and 5 can be
used for CG, but not for EG – recall the discussion in Section 5.5.2.
3.
In the rule CE, we may deduce Ek
Gφ from CG φ for any k; or we could go
directly to Ki1 . . . Kik φ for any agents i1, . . . , ik∈G by using the rule CK.
Strictly speaking, these rules are a whole set of such rules, one for each choice
of k and i1, . . . , ik, but we refer to all of them as CE and CK respectively.
4.
Applying rule EKi, we may deduce Ki φ from EG φ for any i ∈G. From

i∈G Ki φ we may deduce EG φ by virtue of rule KE. Note that the proof
rule EKi is like a generalised and-elimination rule, whereas KE behaves like
an and-introduction rule.
The proof rules for KT45n are summarised in Figure 5.14. As before, we
can think of the rules K4 and K5 and C4 and C5 as relaxations of the

340
5 Modal logics and agents
Ki
...
φ
Kiφ
Kii
EG
...
φ
EGφ
EGi
CG
...
φ
CGφ
CGi
Kiφ
Ki
...
φ
...
Kie
EGφ
EG
...
φ
...
EGe
CGφ
CG
...
φ
...
CGe
Ki φ for each i ∈G
EG φ
KE
EG φ
i ∈G
Ki φ
EKi
CG φ
EG . . . EG φ
CE
CG φ
ij ∈G
Ki1 . . . Kik φ
CK
CG φ
CGCG φ
C4
¬CG φ
CG¬CG φ
C5
Ki φ
φ
KT
Ki φ
KiKi φ
K4
¬Ki φ
Ki¬Ki φ
K5
Figure 5.14. Natural deduction rules for KT45n.
rules about moving formulas in and out of dashed proof boxes. Since rule
K4 allows us to double-up Ki, we could instead think of it as allowing us
to move formulas beginning with Ki into Ki-dashed boxes. Similarly, rule
C5 has the eﬀect of allowing us to move formulas beginning with ¬CG into
CG-dashed boxes.
An intuitive way of thinking about the dashed boxes is that formulas in
them are known to the agent in question. When you open a Ki-dashed box,
you are considering what agent i knows. It’s quite intuitive that an ordinary
formula φ cannot be brought into such a dashed box, because the mere truth
of φ does not mean that agent i knows it. In particular, you can’t use the
rule ¬i if one of the premises of the rule is outside the dashed box you’re
working in.

5.5 Reasoning about knowledge in a multi-agent system
341
1
C(p ∨q)
premise
2
K1(K2 p ∨K2 ¬p)
premise
3
K1¬K2 q
premise
4
K1K2 (p ∨q)
CK 1
K1
5
K2 (p ∨q)
K1e 4
6
K2 p ∨K2 ¬p
K1e 2
7
¬K2 q
K1e 3
8
K2 p
assumption
9
p
axiom T 8
10
11
12
13
14
K2 ¬p
assumption
K2
¬p
K2e 8
p ∨q
K2e 5
q
prop 9, 10
K2 q
K2i 9−11
⊥
¬e 12, 7
p
⊥e 13
15
p
∨e 6, 8−14, 8−14
16
K1 p
K1i 5−15
Figure 5.15. A proof of C(p ∨q), K1(K2p ∨K2 ¬p), K1¬K2 q |−K1 p.
Observe the power of C φ in the premises: we can bring φ into any dashed
box by the application of the rules CK and Kie, no matter how deeply nested
boxes are. The rule Ek φ, on the other hand, ensures that φ can be brought
into any dashed box with nesting ≤k. Compare this with Theorem 5.26.
Example 5.27 We show that the sequent1 C(p ∨q), K1(K2p ∨K2 ¬p),
K1¬K2 q |−K1 p is valid in the modal logic KT45n. That means: if it is com-
mon knowledge that p ∨q; and agent 1 knows that agent 2 knows whether
p is the case and also knows that agent 2 doesn’t know that q is true; then
agent 1 knows that p is true. See Figure 5.15 for a proof. In line 12, we
derived q from ¬p and p ∨q. Rather than show the full derivation in propo-
sitional logic, which is not the focus here, we summarise by writing ‘prop’
as the justiﬁcation for an inference in propositional logic.
1 In this section we simply write |−for |−KT45n, unless indicated otherwise.

342
5 Modal logics and agents
5.5.4 Formalising the examples
Now that we have set up the modal logic KT45n, we can turn our attention to
the question of how to represent the wise-men and muddy-children puzzles in
this logic. Unfortunately, in spite of its sophistication, our logic is too simple
to capture all the nuances of those examples. Although it has connectives
for representing diﬀerent items of knowledge held by diﬀerent agents, it
does not have any temporal aspect, so it cannot directly capture the way
in which the agents’ knowledge changes as time proceeds. We will overcome
this limitation by considering several ‘snapshots’ during which time is ﬁxed.
The wise-men puzzle
Recall that there are three wise men; and it’s
common knowledge that there are three red hats and two white hats. The
king puts a hat on each of the wise men and asks them sequentially whether
they know the colour of the hat on their head – they are unable to see their
own hat. We suppose the ﬁrst man says he does not know; then the second
says he does not know. We want to prove that, whatever the distribution of
hats, the third man now knows his hat is red.
Let pi mean that man i has a red hat; so ¬pi means that man i has a
white hat. Let Γ be the set of formulas
{C(p1 ∨p2 ∨p3),
C(p1 →K2 p1), C(¬p1 →K2 ¬p1),
C(p1 →K3 p1), C(¬p1 →K3 ¬p1),
C(p2 →K1 p2), C(¬p2 →K1 ¬p2),
C(p2 →K3 p2), C(¬p2 →K3 ¬p2),
C(p3 →K1 p3), C(¬p3 →K1 ¬p3),
C(p3 →K2 p3), C(¬p3 →K2 ¬p3)}.
This corresponds to the initial set-up: it is common knowledge that one of
the hats must be red and that each man can see the colour of the other
men’s hats.
The announcement that the ﬁrst man doesn’t know the colour of his hat
amounts to the formula
C(¬K1 p1 ∧¬K1 ¬p1)
and similarly for the second man.
A naive attempt at formalising the wise-men problem might go something
like this: we simply prove
Γ, C(¬K1 p1 ∧¬K1 ¬p1), C(¬K2 p2 ∧¬K2 ¬p2) |−K3 p3

5.5 Reasoning about knowledge in a multi-agent system
343
i.e. if Γ is true and the announcements are made, then the third man knows
his hat is red. However, this fails to capture the fact that time passes between
the announcements. The fact that C¬K1 p1 is true after the ﬁrst announce-
ment does not mean it is true after some subsequent announcement. For
example, if someone announces p1, then Cp1 becomes true.
The reason that this formalisation is incorrect is that, although knowledge
accrues with time, lack of knowledge does not accrue with time. If I know φ,
then (assuming that φ doesn’t change) I will know it at the next time-point;
but if I do not know φ, it may be that I do know it at the next time point,
since I may acquire more knowledge.
To formalise the wise-men problem correctly, we need to break it into two
entailments, one corresponding to each announcement. When the ﬁrst man
announces he does not know the colour of his hat, a certain positive formula
φ becomes common knowledge. Our informal reasoning explained that all
men could then rule out the state RWW which, given p1 ∨p2 ∨p3, led them
to the common knowledge of p2 ∨p3. Thus, φ is just p2 ∨p3 and we need to
prove the entailment
Entailment 1. Γ, C(¬K1 p1 ∧¬K1 ¬p1) |−C(p2 ∨p3).
A proof of this sequent can be found in Figure 5.16.
Since p2 ∨p3 is a positive formula, it persists with time and can be used
in conjunction with the second announcement to prove the desired conclu-
sion:
Entailment 2. Γ, C(p2 ∨p3), C(¬K2 p2, ∧¬K2 ¬p2) |−K3 p3.
This method requires some careful thought: given an announcement of
negative information such as a man declaring that he does not know what
the colour of his hat is, we need to work out what positive-knowledge formula
can be derived from this and such new knowledge has to be suﬃcient to make
even more progress towards solving the puzzle in the next round.
Routine proof segments like those in lines 11–16 of Figure 5.16 may be
abbreviated into one step as long as all participating proof rules are recorded.
The resulting shorter representation can be seen in Figure 5.17.
In Figure 5.16, notice that the premises in lines 2 and 5 are not used.
The premises in lines 2 and 3 stand for any such formula for a given value
of i and j, provided i ̸= j; this explains the inference made in line 8. In
Figure 5.18, again notice that the premises in lines 1 and 5 are not used.
Observe also that axiom T in conjunction with CK allows us to infer φ
from any Cφ, although we had to split this up into two separate steps in
lines 16 and 17. Practical implementations would probably allow for hybrid
rules which condense such reasoning into one step.

344
5 Modal logics and agents
1
C(p1 ∨p2 ∨p3)
premise
2
C(pi →Kj pi)
premise, (i ̸= j)
3
C(¬pi →Kj ¬pi)
premise, (i ̸= j)
4
C¬K1 p1
premise
5
C¬K1 ¬p1
premise
C
6
7
¬p2 ∧¬p3
assumption
8
¬p2 →K1 ¬p2
Ce 3 (i, j) = (2, 1)
9
¬p3 →K1 ¬p3
Ce 3 (i, j) = (3, 1)
10
K1 ¬p2 ∧K1 ¬p3
prop 7, 8, 9
11
K1 ¬p2
∧e1 10
12
K1 ¬p3
∧e2 10
K1
13
14
¬p2
K1e 11
15
¬p3
K1e 12
16
¬p2 ∧¬p3
∧i 14, 15
17
p1 ∨p2 ∨p3
Ce 1
18
p1
prop 16, 17
19
K1 p1
K1i 13−18
20
¬K1 p1
Ce 4
21
⊥
¬e 19, 20
22
¬(¬p2 ∧¬p3)
¬i 7−21
23
p2 ∨p3
prop 22
24
C(p2 ∨p3)
Ci 6−23
Figure 5.16. Proof of the sequent ‘Entailment 1’ for the wise-men puzzle.
The muddy-children puzzle
Suppose there are n children. Let pi mean
that the ith child has mud on its forehead. We consider Scenario 2, in which
the father announces that one of the children is muddy. Similarly to the case
for the wise men, it is common knowledge that each child can see the other
children, so it knows whether the others have mud, or not. Thus, for example,

5.5 Reasoning about knowledge in a multi-agent system
345
1
C(p1 ∨p2 ∨p3)
premise
2
C(pi →Kj pi)
premise, (i ̸= j)
3
C(¬pi →Kj ¬pi)
premise, (i ̸= j)
4
C¬K1 p1
premise
5
C¬K1 ¬p1
premise
C
6
7
¬p2 ∧¬p3
assumption
8
¬p2 →K1 ¬p2
Ce 3 (i, j) = (2, 1)
9
¬p3 →K1 ¬p3
Ce 3 (i, j) = (3, 1)
10
K1 ¬p2 ∧K1 ¬p3
prop 7, 8, 9
K1
11
12
¬p2 ∧¬p3
∧e1, K1e, ∧i
13
p1 ∨p2 ∨p3
Ce 1
14
p1
prop 12, 13
15
K1 p1
K1i 11−14
16
¬K1 p1
Ce 4
17
⊥
¬e 15, 16
18
¬(¬p2 ∧¬p3)
¬i 7−17
19
p2 ∨p3
prop 18
20
C(p2 ∨p3)
Ci 6−19
Figure 5.17. A more compact representation of the proof in Figure 5.16.
we have that C(p1 →K2 p1), which says that it is common knowledge that,
if child 1 is muddy, then child 2 knows this and also C(¬p1 →K2 ¬p1). Let
Γ be the collection of formulas:
C(p1 ∨p2 ∨· · · ∨pn)

i̸=j
C(pi →Kj pi)

i̸=j
C(¬pi →Kj ¬pi).

346
5 Modal logics and agents
1
C(p1 ∨p2 ∨p3)
premise
2
C(pi →Kj pi)
premise, (i ̸= j)
3
C(¬pi →Kj ¬pi)
premise, (i ̸= j)
4
C¬K2 p2
premise
5
C¬K2 ¬p2
premise
6
C(p2 ∨p3)
premise
K3
7
8
¬p3
assumption
9
¬p3 →K2 ¬p3
CK 3 (i, j) = (3, 2)
10
K2 ¬p3
→e 9, 8
K2
11
12
¬p3
K2e 10
13
p2 ∨p3
Ce 6
14
p2
prop 12, 13
15
K2 p2
K2i 11−14
16
Ki ¬K2 p2
CK 4, for each i
17
¬K2 p2
KT 16
18
⊥
¬e 15, 17
19
p3
PBC 8−18
20
K3 p3
K3i 7−19
Figure 5.18. Proof of the sequent ‘Entailment 2’ for the wise-men puzzle.
Note that 
i̸=j ψ(i,j) is a shorthand for the ﬁnite conjunction of all formulas
ψ(i,j), where i is diﬀerent from j. Let G be any set of children. We will
require formulas of the form
αG
def
=

i∈G
pi ∧

i̸∈G
¬pi.
The formula αG states that it is precisely the children in G that have muddy
foreheads.

5.5 Reasoning about knowledge in a multi-agent system
347
1
¬p1 ∧¬p2 ∧· · · ∧pi ∧· · · ∧¬pn
α{i}
2
C(p1 ∨· · · ∨pn)
in Γ
3
¬pj
∧e 1, for each j ̸= i
4
¬pj →Ki ¬pj
in Γ, for each j ̸= i
5
Ki ¬pj
→e 4, 3, for each j ̸= i
6
Ki (p1 ∨· · · ∨pn)
CK 2
Ki
7
8
p1 ∨· · · ∨pn
Ki e 6
9
¬pj
Ki e 5, for each j ̸= i
10
pi
prop 9, 8
11
Ki pi
Ki i
Figure 5.19. Proof of the sequent ‘Entailment 1’ for the muddy-children
puzzle.
Suppose now that k = 1, i.e. that one child has mud on its forehead. We
would like to show that that child knows that it is the one. We prove the
following entailment.
Entailment 1. Γ, α{i} |−Ki pi.
This says that, if the actual situation is one in which only one child
called i has mud, then that child will know it. Our proof follows exactly
the same lines as the intuition: i sees that no other children have mud,
but knows that at least one has mud, so knows it must be itself who has
a muddy forehead. The proof is given in Figure 5.19.
Note that the comment ‘for each j ̸= i’ means that we supply this argu-
ment for any such j. Thus, we can form the conjunction of all these inferences
which we left implicit in the inference on line 10.
What if there is more than one child with mud? In this case, the children
all announce in the ﬁrst parallel round that they do not know whether they
are muddy or not, corresponding to the formula
A
def
= C(¬K1 p1 ∧¬K1 ¬p1) ∧· · · ∧C(¬Kn pn ∧¬Kn ¬pn).
We saw in the wise-men example that it is dangerous to put the announce-
ment A alongside the premises Γ, because the truth of A, which has negative
claims about the children’s knowledge, cannot be guaranteed to persist with

348
5 Modal logics and agents
time. So we seek some positive formula which represents what the children
learn upon hearing the announcement. As in the wise-men example, this for-
mula is implicit in the informal reasoning about the muddy children given
in Section 5.5.1: if it is common knowledge that there are at least k muddy
children, then, after an announcement of the form A, it will be common
knowledge that there are at least k + 1 muddy children.
Therefore, after the ﬁrst announcement A, the set of premises is
Γ,

1≤i≤n
C¬α{i}.
This is Γ together with the common knowledge that the set of muddy chil-
dren is not a singleton set.
After the second announcement A, the set of premises becomes
Γ,

1≤i≤n
C¬α{i},

i̸=j
C¬α{i,j}
which we may write as
Γ,

|G|≤2
C¬αG.
Please try carefully to understand the notation:
αG
the set of muddy children is precisely the set G
¬αG
the set of muddy children is some other set than G

|G|≤k
¬αG
the set of muddy children is of size greater than k.
The entailment corresponding to the second round is:
Γ, C(

|G|≤2
¬αG), αH |−

i∈H
Ki pi,
where |H| = 3 .
The entailment corresponding to the kth round is:
Entailment 2. Γ, C(
|G|≤k ¬αG), αH |−
i∈H Ki pi, where |H| = k + 1.
Please try carefully to understand what this sequent is saying. ‘If all
the things in Γ are true and if it is common knowledge that the set of
muddy children is not of size less than or equal to k and if actually it is
of size k + 1, then each of those k + 1 children can deduce that they are
muddy.’ Notice how this ﬁts with our intuitive account given earlier in
this text.

5.5 Reasoning about knowledge in a multi-agent system
349
1
αH
premise
2
C¬αG
premise as |G| ≤k
3
pj
∧e 1, for each j ∈G
4
¬pk
∧e 1, for each k ̸∈H
5
pj →Ki pj
in Γ for each j ∈G
6
Ki pj
→e 5, 4, for each j ∈G
7
¬pk →Ki ¬pk
in Γ for each k ̸∈H
8
Ki ¬pk
→e 7, 4, for each k ̸∈H
9
Ki ¬αG
CK 2
Ki
10
11
pj
Ki e 6 (j ∈G)
12
¬pk
Ki e 8 (k ̸∈H)
13
¬pi
assumption
14
αG
∧i 11, 12, 13
15
¬αG
Ki e 9
16
⊥
¬e 14, 15
17
¬¬pi
¬i 13−16
18
pi
¬¬e 17
Ki pi
19
Ki i 10−18
Figure 5.20. The proof of Γ, C(¬αG), αH |−Ki pi, used to prove ‘En-
tailment 2’ for the muddy-children puzzle.
To prove Entailment 2, take any i ∈H. It is suﬃcient to prove that
Γ, C(

|G|≤k
¬αG), αH |−Ki pi
is valid, as the repeated use of ∧i over all values of i gives us a proof of
Entailment 2. Let G be H −{i}; the proof that Γ, C(¬αG), αH |−Ki pi
is valid is given in Figure 5.20. Please study this proof in every detail
and understand how it is just following the steps taken in the informal
proof in Section 5.5.1.

350
5 Modal logics and agents
The line 14 of the proof in Figure 5.20 applies several instances of ∧i in
sequence and is a legitimate step since the formulas in lines 11–13 had been
shown ‘for each’ element in the respective set.
5.6 Exercises
Exercises 5.1
1. Think about the highly distributed computing environments of today with their
dynamic communication and network topology. Come up with several kinds of
modes of truth pertaining to statements made about such environments.
2. Let M be a model of ﬁrst-order logic and let φ range over formulas of ﬁrst-order
logic. Discuss in what sense statements of the form ‘Formula φ is true in model
M.’ express a mode of truth.
Exercises 5.2
1. Consider the Kripke model M depicted in Figure 5.5.
(a) For each of the following, determine whether it holds:
i. a ⊩p
ii. a ⊩2¬q
iii.
*
a ⊩q
iv.
*
a ⊩22q
v. a ⊩3p
vi.
*
a ⊩23¬q
vii. c ⊩3⊤
viii. d ⊩3⊤
ix. d ⊩22q
x.
*
c ⊩2⊥
xi. b ⊩2⊥
xii. a ⊩33(p ∧q) ∧3⊤.
(b) Find for each of the following a world which satisﬁes it:
i. 2¬p ∧22¬p
ii. 3q ∧¬2q
iii.
*
3p ∨3q
iv.
*
3(p ∨3q)
v. 2p ∨2¬p
vi. 2(p ∨¬p).
(c) For each formula of the previous item, ﬁnd a world which does not satisfy
the formula.
2. Find a Kripke model M and a formula scheme which is not satisﬁed in M, but
which has true instances in M.

5.6 Exercises
351
3. Consider the Kripke model M = (W, R, L) where W = {a, b, c, d, e}; R =
{(a, c), (a, e), (b, a), (b, c), (d, e), (e, a)}; and L(a) = {p}, L(b) = {p, q}, L(c) =
{p, q}, L(d) = {q} and L(e) = ∅.
(a) Draw a graph for M.
(b) Investigate which of the formulas in exercise 1(b) on page 350 have a world
which satisﬁes it.
4. (a) Think about what you have to do to decide whether p →23q is true in a
model.
(b)
*
Find a model in which it is true and one in which it is false.
5. For each of the following pairs of formulas, can you ﬁnd a model and a world in
it which distinguishes them, i.e. makes one of them true and one false? In that
case, you are showing that they do not entail each other. If you cannot, it might
mean that the formulas are equivalent. Justify your answer.
(a) 2p and 22p
(b) 2¬p and ¬3p
(c) 2(p ∧q) and 2p ∧2q
(d)
*
3(p ∧q) and 3p ∧3q
(e) 2(p ∨q) and 2p ∨2q
(f)
*
3(p ∨q) and 3p ∨3q
(g) 2(p →q) and 2p →2q
(h) 3⊤and ⊤
(i) 2⊤and ⊤
(j) 3⊥and ⊥.
6. Show that the following formulas of basic modal logic are valid:
(a)
*
2(φ ∧ψ) ↔(2φ ∧2ψ)
(b) 3(φ ∨ψ) ↔(3φ ∨3ψ)
(c)
*
2⊤↔⊤
(d) 3⊥↔⊥
(e) 3⊤→(2φ →3φ)
7. Inspect Deﬁnition 5.4. We said that we deﬁned x ⊩φ by structural induction on
φ. Is this really correct? Note the implicit deﬁnition of a second relation x ̸⊩φ.
Why is this deﬁnition still correct and in what sense does it still rely on structural
induction?
Exercises 5.3
1. For which of the readings of 2 in Table 5.7 are the formulas below valid?
(a)
*
(φ →2φ) →(φ →3φ)
(b) (2φ →(φ ∧22φ ∧3φ)) →((2φ →(φ ∧22φ)) ∧(3φ →23φ)).
2. Dynamic logic: Let P range over the programs of our core language in Chapter 4.
Consider a modal logic whose modal operators are ⟨P⟩and [P] for all such
programs P. Evaluate such formulas in stores l as in Deﬁnition 4.3 (page 264).

352
5 Modal logics and agents
The relation l ⊨⟨P⟩φ holds iﬀprogram P has some execution beginning in
store l and terminating in a store satisfying φ.
(a)
*
Given that ¬⟨P⟩¬ equals [P], spell out the meaning of [P].
(b) Say that φ is valid iﬀit holds in all suitable stores l. State the total cor-
rectness of a Hoare triple as a validity problem in this modal logic.
3. For all binary relations R below, determine which of the properties reﬂexive
through to total from page 320 apply to R where R(x, y) means that
(a)
*
x is strictly less than y, where x and y range over all natural numbers n ≥1
(b) x divides y, where x and y range over integers – e.g. 5 divides 15, whereas
7 does not
(c) x is a brother of y
(d)
*
there exist positive real numbers a and b such that x equals a · y + b, where
x and y range over real numbers.
4.
*
Prove the Fact 5.16.
5. Prove the informal claim made in item 2 of Example 5.12 by structural induction
on formulas in (5.1).
6. Prove Theorem 5.17. Use mathematical induction on the length of the sequence
of negations and modal operators. Note that this requires a case analysis over
the topmost operator other than a negation, or a modality.
7. Prove Theorem 5.14, but for the case in which R is reﬂexive, or transitive.
8. Find a Kripke model in which all worlds satisfy ¬p ∨q, but at least one world
does not satisfy ¬q ∨p; i.e. show that the scheme ¬φ ∨ψ is not satisﬁed.
9. Below you ﬁnd a list of sequents Γ ⊢φ in propositional logic. Find out whether
you can prove them without the use of the rules PBC, LEM and ¬¬e. If you
cannot succeed, then try to construct a model M = (W, R, L) for intuitionistic
propositional logic such that one of its worlds satisﬁes all formulas in Γ, but
does not satisfy φ. Assuming soundness, this would guarantee that the sequent
in question does not have a proof in intuitionistic propositional logic.
(a)
*
⊢(p →q) ∨(q →r)
(b) The proof rule MT: p →q, ¬q ⊢¬p
(c) ¬p ∨q ⊢p →q
(d) p →q ⊢¬p ∨q
(e) The proof rule ¬¬e: ¬¬p ⊢p
(f)
*
The proof rule ¬¬i: p ⊢¬¬p.
10. Prove that the natural deduction rules for propositional logic without the rules
¬¬e, LEM and PBC are sound for the possible world semantics of intuitionis-
tic propositional logic. Why does this show that the excluded rules cannot be
implemented using the remaining ones?
11. Interpreting 2φ as ‘agent Q believes φ,’ explain the meaning of the following
formula schemes:
(a) 2φ →3φ
(b)
*
2φ ∨2¬φ
(c) 2(φ →ψ) ∧2φ →2ψ.

5.6 Exercises
353
12. In the second row of Table 5.7, we adopted the convention that the future
excludes the present. Which formula schemes would be satisﬁed in that row if
instead we adopted the more common convention that the future includes the
present?
13. Consider the properties in Table 5.12. Which ones should we accept if we read
2 as
(a)
*
knowledge
(b) belief
(c)
*
‘always in the future?’
14. Find a frame which is reﬂexive, transitive, but not symmetric. Show that your
frame does not satisfy the formula p →23p, by providing a suitable labelling
function and choosing a world which refutes p →23p. Can you ﬁnd a labelling
function and world which does satisfy p →23p in your frame?
15. Give two examples of frames which are Euclidean – i.e. their accessibility rela-
tion is Euclidean – and two which are not. Explain intuitively why 3p →23p
holds on the ﬁrst two, but not on the latter two.
16. For each of the following formulas, ﬁnd the property of R which corresponds to
it.
(a) φ →2φ
(b)
*
2⊥
(c)
*
32φ →23φ.
17.
*
Find a formula whose corresponding property is density: for all x, z ∈W such
that R(x, z), there exists y ∈W such that R(x, y) and R(y, z).
18. The modal logic KD45 is used to model belief; see Table 5.12 for the axiom
schemes D, 4, and 5.
(a) Explain how it diﬀers from KT45.
(b) Show that ⊨KD45 2p →3p is valid. What is the signiﬁcance of this, in terms
of knowledge and belief?
(c) Explain why the condition of seriality is relevant to belief.
19. Recall Deﬁnition 5.7. How would you deﬁne ≡L for a modal logic L?
Exercises 5.4
1. Find natural deduction proofs for the following sequents over the basic modal
logic K.
(a)
*
⊢K 2(p →q) |−2p →2q
(b) ⊢K 2(p →q) |−3p →3q
(c)
*
⊢K|−2(p →q) ∧2(q →r) →2(p →r)
(d) ⊢K 2(p ∧q) |−2p ∧2q
(e) ⊢K|−3⊤→(2p →3p)
(f)
*
⊢K 3(p →q) |−2p →3q
(g) ⊢K 3(p ∨q) |−3p ∨3q.

354
5 Modal logics and agents
2. Find natural deduction proofs for the following, in modal logic KT45.
(a) p →23p
(b) 23p ↔3p
(c)
*
32p ↔2p
(d) 2(2p →2q) ∨2(2q →2p)
(e) 2(3p →q) ↔2(p →2q).
3. Study the proofs you gave for the previous exercise to see whether any of
these formula schemes could be valid in basic modal logic. Inspect where and
how these proofs used the axioms T, 4 and 5 to see whether you can ﬁnd a
counter example, i.e. a Kripke model and a world which does not satisfy the
formula.
4. Provide a sketch of an argument which shows that the natural deduction rules
for basic modal logic are sound with respect to the semantics x ⊩φ over Kripke
structures.
Exercises 5.5
1. This exercise is about the wise-men puzzle. Justify your answers.
(a) Each man is asked the question ‘Do you know the colour of your hat?’
Suppose that the ﬁrst man says ‘no,’ but the second one says ‘yes.’ Given
this information together with the common knowledge, can we infer the
colour of his hat?
(b) Can we predict whether the third man will now answer ‘yes’ or ‘no?’
(c) What would be the situation if the third man were blind? What about the
ﬁrst man?
2. This exercise is about the muddy-children puzzle. Suppose k = 4, say children
a, b, c and d have mud on their foreheads. Explain why, before the father’s
announcement, it is not common knowledge that someone is dirty.
3. Write formulas for the following:
(a) Agent 1 knows that p.
(b) Agent 1 knows that p or q.
(c)
*
Agent 1 knows p or agent 1 knows q.
(d) Agent 1 knows whether p.
(e) Agent 1 doesn’t know whether p or q.
(f) Agent 1 knows whether agent 2 knows p.
(g)
*
Agent 1 knows whether agent 2 knows whether p.
(h) No-one knows p.
(i) Not everyone knows whether p.
(j) Anyone who knows p knows q.
(k)
*
Some people know p but don’t know q.
(l) Everyone knows someone who knows p.
4. Determine which of the following hold in the Kripke model of Figure 5.13 and
justify your answer:

5.6 Exercises
355
(a) x1 ⊩K1 p
(b) x3 ⊩K1 (p ∨q)
(c) x1 ⊩K2 q
(d)
*
x3 ⊩E(p ∨q)
(e) x1 ⊩Cq
(f) x1 ⊩D{1,3}p
(g) x1 ⊩D{1,2}p
(h) x6 ⊩E¬q
(i)
*
x6 ⊩C¬q
(j) x6 ⊩C{3}¬q.
5. For each of the following formulas, show that it is not valid by ﬁnding a Kripke
model with a world not satisfying the formula:
(a) EG φ →EGEG φ
(b) ¬EG φ →EG¬EG φ.
Explain why these two Kripke models show that the union of equivalence rela-
tions is not necessarily an equivalence relation.
6.
*
Explain why CG φ →CGCG φ and ¬CG φ →CG¬CG φ are valid.
7. Prove the second part of Theorem 5.26.
8. Recall Section 3.7. Can you specify a monotone function over the power set
of possible worlds which computes the set of worlds satisfying CG φ? Is this a
least, or a greatest, ﬁxed point?
9. Use the natural deduction rules for propositional logic to justify the proof steps
below which are only annotated with ‘prop.’
(a) Line 11 in Figure 5.15.
(b) Lines 10, 18 and 23 of the proof in Figure 5.16. Of course this requires three
separate proofs.
(c) Line 14 of the proof in Figure 5.18.
(d) Line 10 of the proof in Figure 5.19.
10. Using the natural deduction rules for KT45n, prove the validity of
(a) Ki (p ∧q) ↔Ki p ∧Ki q
(b) C(p ∧q) ↔Cp ∧Cq
(c)
*
Ki Cp ↔Cp
(d) C Ki p ↔Cp
(e)
*
¬φ →Ki¬Ki φ.
Explain what this formula means in terms of knowledge. Do you believe it?
(f) ¬φ →K1K2¬K2K1 φ
(g)
*
¬K1¬K1φ ↔K1 φ.
11. Do a natural deduction proof for a simpler version of the wise-men problem:
There are two wise men; as usual, they can see each other’s hats but not their
own. It is common knowledge that there’s only one white hat available and two
red ones. So at least one of the men is wearing a red one. Man 1 informs the
second that he doesn’t know which hat he is wearing. Man 2 says, ‘Aha, then
I must be wearing a red hat.’

356
5 Modal logics and agents
(a) Justify man 2’s conclusion informally.
(b) Let p1, p2 respectively, mean man 1, 2 respectively, is wearing a red hat.
So ¬p1, ¬p2 mean they (respectively) are wearing a white one. Informally
justify each of the following premises in terms of the description of the
problem:
i. K2K1 (p1 ∨p2)
ii. K2(¬p2 →K1 ¬p2)
iii. K2¬K1 p1.
(c) Using natural deduction, prove from these premises that K2 p2.
(d) Show that the third premise was essential, by exhibiting a model/world
which satisﬁes the ﬁrst two, but not the conclusion.
(e) Now is it easy to answer questions like ‘If man 2 were blind would he still be
able to tell?’ and ‘if man 1 were blind, would man 2 still be able to tell?’?
12. Recall our informal discussion on positive-knowledge formulas and negative-
knowledge formulas. Give formal deﬁnitions of these notions.
5.7 Bibliographic notes
The ﬁrst systematic approaches to modal logic were made by C. I. Lewis
in the 1950s. The possible-worlds approach, which greatly simpliﬁed modal
logic and is now almost synonymous with it, was invented by S. Kripke.
Books devoted to modal logic include [Che80, Gol87, Pop94], where exten-
sive references to the literature may be found. All these books discuss the
soundness and completeness of proof calculi for modal logics. They also in-
vestigate which modal logics have the ﬁnite-model property: if a sequent
does not have a proof, there is a ﬁnite model which demonstrates that. Not
all modal logics enjoy this property, which is important for decidability.
Intuitionistic propositional logic has the ﬁnite-model property; an anima-
tion which generates such ﬁnite models (called PORGI) is available from
A. Stoughton’s website2.
The idea of using modal logic to reason about knowledge is due to J.
Hintikka. A great deal of work on applying modal logic to multi-agent sys-
tems has been done in [FHMV95] and [MvdH95] and other work by those
authors. Many examples in this chapter are taken from this literature (some
of them are attributed to other people there), though our treatment of them
is original.
The natural deduction proof system for modal logic presented in this
chapter is based on ideas in [Fit93].
2 www.cis.ksu.edu/~allen/porgi.html

5.7 Bibliographic notes
357
An application of the modal logic KT4 (more precisely, its fragment with-
out negation) as a type system for staged computation in a functional pro-
gramming language can be found in [DP96].
We should stress that our framework was deliberately ‘classical;’ the thesis
[Sim94] is a good source for discussions of intuitionistic modal logics; it also
contains a gentle introduction to basic ﬁrst-order modal logic.

6
Binary decision diagrams
6.1 Representing boolean functions
Boolean functions are an important descriptive formalism for many hard-
ware and software systems, such as synchronous and asynchronous circuits,
reactive systems and ﬁnite-state programs. Representing those systems in a
computer in order to reason about them requires an eﬃcient representation
for boolean functions. We look at such a representation in this chapter and
describe in detail how the systems discussed in Chapter 3 can be veriﬁed
using the representation.
Deﬁnition 6.1 A boolean variable x is a variable ranging over the values
0 and 1. We write x1, x2, . . . and x, y, z, . . . to denote boolean variables. We
deﬁne the following functions on the set {0, 1}:
r 0
def
= 1 and 1
def
= 0;
r x · y
def
= 1 if x and y have value 1; otherwise x · y
def
= 0;
r x + y
def
= 0 if x and y have value 0; otherwise x + y
def
= 1;
r x ⊕y
def
= 1 if exactly one of x and y equals 1.
A boolean function f of n arguments is a function from {0, 1}n to {0, 1}.
We write f(x1, x2, . . . , xn), or f(V ), to indicate that a syntactic representa-
tion of f depends on the boolean variables in V only.
Note that ·, + and ⊕are boolean functions with two arguments, whereas
¯ is a boolean function that takes one argument. The binary functions ·,
+ and ⊕are written in inﬁx notation instead of preﬁx; i.e. we write x + y
instead of +(x, y), etc.
358

6.1 Representing boolean functions
359
Example 6.2 In terms of the four functions above, we can deﬁne other
boolean functions such as
(1)
f(x, y)
def
= x · (y + x)
(2)
g(x, y)
def
= x · y + (1 ⊕x)
(3)
h(x, y, z)
def
= x + y · (x ⊕y)
(4)
k()
def
= 1 ⊕(0 · 1).
6.1.1 Propositional formulas and truth tables
Truth tables and propositional formulas are two diﬀerent representations of
boolean functions. In propositional formulas, ∧denotes ·, ∨denotes +, ¬
denotes ¯ and ⊤and ⊥denote 1 and 0, respectively.
Boolean functions are represented by truth tables in the obvious way; for
example, the function f(x, y)
def
= x + y is represented by the truth table on
the left:
x
y
f(x, y)
1
1
0
0
1
0
1
0
0
0
0
1
p
q
φ
T
T
F
F
T
F
T
F
F
F
F
T
On the right, we show the same truth table using the notation of Chapter 1; a
formula having this truth table is ¬(p ∨q). In this chapter, we may mix these
two notational systems of boolean formulas and formulas of propositional
logic whenever it is convenient. You should be able to translate expressions
easily from one notation to the other and vice versa.
As representations of boolean functions, propositional formulas and truth
tables have diﬀerent advantages and disadvantages. Truth tables are very
space-ineﬃcient: if one wanted to model the functionality of a sequential
circuit by a boolean function of 100 variables (a small chip component would
easily require this many variables), then the truth table would require 2100
(which is more than 1030) lines. Alas, there is not enough storage space
(whether paper or particle) in the universe to record the information of
2100 diﬀerent bit vectors of length 100. Although they are space ineﬃcient,
operations on truth tables are simple. Once you have computed a truth table,
it is easy to see whether the boolean function represented is satisﬁable: you
just look to see if there is a 1 in the last column of the table.
Comparing whether two truth tables represent the same boolean function
also seems easy: assuming the two tables are presented with the same order

360
6 Binary decision diagrams
of valuations, we simply check that they are identical. Although these opera-
tions seem simple, however, they are computationally intractable because of
the fact that the number of lines in the truth table is exponential in the num-
ber of variables. Checking satisﬁability of a function with n atoms requires
of the order of 2n operations if the function is represented as a truth table.
We conclude that checking satisﬁability and equivalence is highly ineﬃcient
with the truth-table representation.
Representation of boolean functions by propositional formulas is slightly
better. Propositional formulas often provide a wonderfully compact and eﬃ-
cient presentation of boolean functions. A formula with 100 variables might
only be about 200–300 characters long. However, deciding whether an arbi-
trary propositional formula is satisﬁable is a famous problem in computer
science: no eﬃcient algorithms for this task are known, and it is strongly
suspected that there aren’t any. Similarly, deciding whether two arbitrary
propositional formulas f and g denote the same boolean function is sus-
pected to be exponentially expensive.
It is straightforward to see how to perform the boolean operations ·, +, ⊕
and ¯ on these two representations. In the case of truth tables, they involve
applying the operation to each line; for example, given truth tables for f and
g over the same set of variables (and in the same order), the truth table for
f ⊕g is obtained by applying ⊕to the truth value of f and g in each line. If
f and g do not have the same set of arguments, it is easy to pad them out
by adding further arguments. In the case of representation by propositional
formulas, the operations ·, ⊕, etc., are simply syntactic manipulations. For
example, given formulas φ and ψ representing the functions f and g, the
formulas representing f · g and f ⊕g are, respectively, φ ∧ψ and (φ ∧¬ψ) ∨
(¬φ ∧ψ).
We could also consider representing boolean functions by various sub-
classes of propositional formulas, such as conjunctive and disjunctive normal
forms. In the case of disjunctive normal form (DNF, in which a formula is a
disjunction of conjunctions of literals), the representation is sometimes com-
pact, but in the worst cases it can be very lengthy. Checking satisﬁability is a
straightforward operation, however, because it is suﬃcient to ﬁnd a disjunct
which does not have two complementary literals. Unfortunately, there is not
a similar way of checking validity. Performing + on two formulas in DNF
simply involves inserting ∨between them. Performing · is more complicated;
we cannot simply insert ∧between the two formulas, because the result will
not in general be in DNF, so we have to perform lengthy applications of
the distributivity rule φ ∧(ψ1 ∨ψ2) ≡(φ ∧ψ1) ∨(φ ∧ψ1). Computing the
negation of a DNF formula is also expensive. The DNF formula φ may be

6.1 Representing boolean functions
361
Representation of
test for
boolean operations
boolean functions
compact?
satisf’ty
validity
·
+
¯
Prop. formulas
often
hard
hard
easy
easy
easy
Formulas in DNF
sometimes
easy
hard
hard
easy
hard
Formulas in CNF
sometimes
hard
easy
easy
hard
hard
Ordered truth tables
never
hard
hard
hard
hard
hard
Reduced OBDDs
often
easy
easy
medium
medium
easy
Figure 6.1. Comparing efficiency of five representations of boolean formulas.
y
y
0
1
0
0
x
Figure 6.2. An example of a binary decision tree.
quite short, whereas the length of the disjunctive normal form of ¬φ can be
exponential in the length of φ.
The situation for representation in conjunctive normal form is the dual. A
summary of these remarks is contained in Figure 6.1 (for now, please ignore
the last row).
6.1.2 Binary decision diagrams
Binary decision diagrams (BDDs) are another way of representing boolean
functions. A certain class of such diagrams will provide the implementational
framework for our symbolic model-checking algorithm. Binary decision di-
agrams were ﬁrst considered in a simpler form called binary decision trees.
These are trees whose non-terminal nodes are labelled with boolean vari-
ables x, y, z, . . . and whose terminal nodes are labelled with either 0 or 1.
Each non-terminal node has two edges, one dashed line and one solid line.
In Figure 6.2 you can see such a binary decision tree with two layers of
variables x and y.
Deﬁnition 6.3 Let T be a ﬁnite binary decision tree. Then T determines
a unique boolean function of the variables in non-terminal nodes, in the
following way. Given an assignment of 0s and 1s to the boolean variables

362
6 Binary decision diagrams
1
0
y
x
y
1
0
y
x
Figure 6.3. (a) Sharing the terminal nodes of the binary decision tree
in Figure 6.2; (b) further optimisation by removing a redundant decision
point.
occurring in T, we start at the root of T and take the dashed line when-
ever the value of the variable at the current node is 0; otherwise, we travel
along the solid line. The function value is the value of the terminal node we
reach.
For example, the binary decision tree of Figure 6.2 represents a boolean
function f(x, y). To ﬁnd f(0, 1), start at the root of the tree. Since the value
of x is 0 we follow the dashed line out of the node labelled x and arrive
at the leftmost node labelled y. Since y’s value is 1, we follow the solid
line out of that y-node and arrive at the leftmost terminal node labelled
0. Thus, f(0, 1) equals 0. In computing f(0, 0), we similarly travel down
the tree, but now following two dashed lines to obtain 1 as a result. You
can see that the two other possibilities result in reaching the remaining
two terminal nodes labelled 0. Thus, this binary decision tree computes the
function f(x, y)
def
= x + y.
Binary decision trees are quite close to the representation of boolean func-
tions as truth tables as far as their sizes are concerned. If the root of a binary
decision tree is an x-node then it has two subtrees (one for the value of x
being 0 and another one for x having value 1). So if f depends on n boolean
variables, the corresponding binary decision tree will have at least 2n+1 −1
nodes (see exercise 5 on page 399). Since f’s truth table has 2n lines, we
see that decision trees as such are not a more compact representation of
boolean functions. However, binary decision trees often contain some redun-
dancy which we can exploit.
Since 0 and 1 are the only terminal nodes of binary decision trees, we can
optimise the representation by having pointers to just one copy of 0 and
one copy of 1. For example, the binary decision tree in Figure 6.2 can be
optimised in this way and the resulting structure is depicted in Figure 6.3(a).
Note that we saved storage space for two redundant terminal 0-nodes, but
that we still have as many edges (pointers) as before.

6.1 Representing boolean functions
363
x
x
1
0
z
y
y
y
y
Figure 6.4. A BDD with duplicated subBDDs.
A second optimisation we can do is to remove unnecessary decision points
in the tree. In Figure 6.3(a), the right-hand y is unnecessary, because we go
to the same place whether it is 0 or 1. Therefore the structure could be
further reduced, to the one shown on the right, (b).
All these structures are examples of binary decision diagrams (BDDs).
They are more general than binary decision trees; the sharing of the leaves
means they are not trees. As a third optimisation, we also allow subBDDs to
be shared. A subBDD is the part of a BDD occurring below a given node. For
example, in the BDD of Figure 6.4, the two inner y-nodes perform the same
role, because the subBDDs below them have the same structure. Therefore,
one of them could be removed, resulting in the BDD in Figure 6.5(a). Indeed,
the left-most y-node could also be merged with the middle one; then the
x-node above both of them would become redundant. Removing it would
result in the BDD on the right of Figure 6.5.
To summarise, we encountered three diﬀerent ways of reducing a BDD to
a more compact form:
C1. Removal of duplicate terminals. If a BDD contains more than one
terminal 0-node, then we redirect all edges which point to such a 0-node to
just one of them. We proceed in the same way with terminal nodes labelled
with 1.
C2. Removal of redundant tests. If both outgoing edges of a node n
point to the same node m, then we eliminate that node n, sending all its
incoming edges to m.
C3. Removal of duplicate non-terminals. If two distinct nodes n and
m in the BDD are the roots of structurally identical subBDDs, then we

364
6 Binary decision diagrams
x
1
0
z
y
y
y
x
0
1
z
y
y
x
Figure 6.5. The BDD of Figure 6.4: (a) after removal of one of the
duplicate y-nodes; (b) after removal of another duplicate y-node and
then a redundant x-decision point.
eliminate one of them, say m, and redirect all its incoming edges to the
other one.
Note that C1 is a special case of C3. In order to deﬁne BDDs precisely,
we need a few auxiliary notions.
Deﬁnition 6.4 A directed graph is a set G and a binary relation →on G:
→⊆G × G. A cycle in a directed graph is a ﬁnite path in that graph that
begins and ends at the same node, i.e. a path of the form v1 →v2 →· · · →
vn →v1. A directed acyclic graph (dag) is a directed graph that does not
have any cycles. A node of a dag is initial if there are no edges pointing to
that node. A node is called terminal if there are no edges out of that node.
The directed graph in Figure 3.3 on page 179 has cycles, for example
the cycle s0 →s1 →s0, and is not a dag. If we interpret the links in BDDs
(whether solid or dashed) as always going in a downwards direction, then
the BDDs of this chapter are also directed graphs. They are also acyclic and
have a unique initial node. The optimisations C1–C3 preserve the property
of being a dag; and fully reduced BDDs have precisely two terminal nodes.
We now formally deﬁne BDDs as certain kinds of dags:
Deﬁnition 6.5 A binary decision diagram (BDD) is a ﬁnite dag with
a unique initial node, where all terminal nodes are labelled with 0 or
1 and all non-terminal nodes are labelled with a boolean variable. Each

6.1 Representing boolean functions
365
0
1
0
x
Figure 6.6. The BDDs (a) B0, representing the constant 0 boolean
function; similarly, the BDD B1 has only one node 1 and represents
the constant 1 boolean function; and (b) Bx, representing the boolean
variable x.
non-terminal node has exactly two edges from that node to others: one la-
belled 0 and one labelled 1 (we represent them as a dashed line and a solid
line, respectively).
A BDD is said to be reduced if none of the optimisations C1–C3 can be
applied (i.e. no more reductions are possible).
All the decision structures we have seen in this chapter (Figures 6.2–6.5)
are BDDs, as are the constant functions B0 and B1, and the function Bx
from Figure 6.6. If B is a BDD where V = {x1, x2, . . . , xn} is the set of labels
of non-terminal nodes, then B determines a boolean function f(V ) in the
same way as binary decision trees (see Deﬁnition 6.3): given an assignment
of 0s and 1s to the variables in V , we compute the value of f by starting
with the unique initial node. If its variable has value 0, we follow the dashed
line; otherwise we take the solid line. We continue for each node until we
reach a terminal node. Since the BDD is ﬁnite by deﬁnition, we eventually
reach a terminal node which is labelled with 0 or 1. That label is the result
of f for that particular assignment of truth values.
The deﬁnition of a BDD does not prohibit that a boolean variable occur
more than once on a path in the dag. For example, consider the BDD in
Figure 6.7.
Such a representation is wasteful, however. The solid link from the left-
most x to the 1-terminal is never taken, for example, because one can only
get to that x-node when x has value 0.
Thanks to the reductions C1–C3, BDDs can often be quite compact rep-
resentations of boolean functions. Let us consider how to check satisﬁability
and perform the boolean operations on functions represented as BDDs. A
BDD represents a satisﬁable function if a 1-terminal node is reachable from
the root along a consistent path in a BDD which represents it. A consistent
path is one which, for every variable, has only dashed lines or only solid lines
leaving nodes labelled by that variable. (In other words, we cannot assign

366
6 Binary decision diagrams
x
y
z
x
y
x
0
1
Figure 6.7. A BDD where some boolean variables occur more than
once on an evaluation path.
a variable the values 0 and 1 simultaneously.) Checking validity is similar,
but we check that no 0-terminal is reachable by a consistent path.
The operations · and + can be performed by ‘surgery’ on the component
BDDs. Given BDDs Bf and Bg representing boolean functions f and g, a
BDD representing f · g can be obtained by taking the BDD f and replacing
all its 1-terminals by Bg. To see why this is so, consider how to get to a
1-terminal in the resulting BDD. You have to satisfy the requirements for
getting to a 1 imposed by both of the BDDs. Similarly, a BDD for f + g
can be obtained by replacing all 0 terminals of Bf by Bg. Note that these
operations are likely to generate BDDs with multiple occurrences of variables
along a path. Later, in Section 6.2, we will see deﬁnitions of + and · on BDDs
that don’t have this undesirable eﬀect.
The complementation operation ¯ is also possible: a BDD representing f
can be obtained by replacing all 0-terminals in Bf by 1-terminals and vice
versa. Figure 6.8 shows the complement of the BDD in Figure 6.2.
6.1.3 Ordered BDDs
We have seen that the representation of boolean functions by BDDs is often
compact, thanks to the sharing of information aﬀorded by the reductions
C1–C3. However, BDDs with multiple occurrences of a boolean variable
along a path seem rather ineﬃcient. Moreover, there seems no easy way to
test for equivalence of BDDs. For example, the BDDs of Figures 6.7 and 6.9
represent the same boolean function (the reader should check this). Neither
of them can be optimised further by applying the rules C1–C3. However,

6.1 Representing boolean functions
367
x
y
y
0
1
1
1
Figure 6.8. The complement of the BDD in Figure 6.2.
y
0
1
x
y
z
Figure 6.9. A BDD representing the same function as the BDD of
Figure 6.7, but having the variable ordering [x, y, z].
testing whether they denote the same boolean function seems to involve as
much computational eﬀort as computing the entire truth table for f(x, y, z).
We can improve matters by imposing an ordering on the variables occur-
ring along any path. We then adhere to that same ordering for all the BDDs
we manipulate.
Deﬁnition 6.6 Let [x1, . . . , xn] be an ordered list of variables without du-
plications and let B be a BDD all of whose variables occur somewhere in
the list. We say that B has the ordering [x1, . . . , xn] if all variable labels of
B occur in that list and, for every occurrence of xi followed by xj along any
path in B, we have i < j.
An ordered BDD (OBDD) is a BDD which has an ordering for some list
of variables.
Note that the BDDs of Figures 6.3(a,b) and 6.9 are ordered (with ordering
[x, y]). We don’t insist that every variable in the list is used in the paths.
Thus, the OBDDs of Figures 6.3 and 6.9 have the ordering [x, y, z] and so

368
6 Binary decision diagrams
0
1
z
x
y
y
x
Figure 6.10. A BDD which does not have an ordering of variables.
does any list having x, y and z in it in that order, such as [u, x, y, v, z, w] and
[x, u, y, z]. Even the BDDs B0 and B1 in Figure 6.6 are OBDDs, a suitable
ordering list being the empty list (there are no variables), or indeed any list.
The BDD Bx of Figure 6.6(b) is also an OBDD, with any list containing x
as its ordering.
The BDD of Figure 6.7 is not ordered. To see why this is so, consider the
path taken if the values of x and y are 0. We begin with the root, an x-
node, and reach a y-node and then an x-node again. Thus, no matter what
list arrangement we choose (remembering that no double occurrences are
allowed), this path violates the ordering condition. Another example of a
BDD that is not ordered can be seen in Figure 6.10. In that case, we cannot
ﬁnd an order since the path for (x, y, z) ⇒(0, 0, 0) – meaning that x, y and z
are assigned 0 – shows that y needs to occur before x in such a list, whereas
the path for (x, y, z) ⇒(1, 1, 1) demands that x be before y.
It follows from the deﬁnition of OBDDs that one cannot have multiple
occurrences of any variable along a path.
When operations are performed on two OBDDs, we usually require that
they have compatible variable orderings. The orderings of B1 and B2 are
said to be compatible if there are no variables x and y such that x comes
before y in the ordering of B1 and y comes before x in the ordering of B2.
This commitment to an ordering gives us a unique representation of boolean
functions as OBDDs. For example, the BDDs in Figures 6.8 and 6.9 have
compatible variable orderings.
Theorem 6.7 The reduced OBDD representing a given function f is
unique. That is to say, let B and B′ be two reduced OBDDs with

6.1 Representing boolean functions
369
compatible variable orderings. If B and B represent the same boolean func-
tion, then they have identical structure.
In other words, with OBDDs we cannot get a situation like the one en-
countered earlier, in which we have two distinct reduced BDDs which repre-
sent the same function, provided that the orderings are compatible. It follows
that checking equivalence of OBDDs is immediate. Checking whether two
OBDDs (having compatible orderings) represent the same function is simply
a matter of checking whether they have the same structure1.
A useful consequence of the theorem above is that, if we apply the reduc-
tions C1–C3 to an OBDD until no further reductions are possible, then we
are guaranteed that the result is always the same reduced OBDD. The order
in which we applied the reductions does not matter. We therefore say that
OBDDs have a canonical form, namely their unique reduced OBDD. Most
other representations (conjunctive normal forms, etc.) do not have canonical
forms.
The algorithms for · and + for BDDs, presented in Section 6.1.2, won’t
work for OBDDs as they may introduce multiple occurrences of the same
variable on a path. We will soon develop more sophisticated algorithms
for these operations on OBDDs, which exploit the compatible ordering of
variables in paths.
OBDDs allow compact representations of certain classes of boolean func-
tions which only have exponential representations in other systems, such as
truth tables and conjunctive normal forms. As an example consider the even
parity function feven(x1, x2, . . . , xn) which is deﬁned to be 1 if there is an
even number of variables xi with value 1; otherwise, it is deﬁned to be 0.
Its representation as an OBDD requires only 2n + 1 nodes. Its OBDD for
n = 4 and the ordering [x1, x2, x3, x4] can be found in Figure 6.11.
The impact of the chosen variable ordering
The size of the OBDD
representing the parity functions is independent of the chosen variable or-
dering. This is because the parity functions are themselves independent of
the order of variables: swapping the values of any two variables does not
change the value of the function; such functions are called symmetric.
However, in general the chosen variable ordering makes a signiﬁcant dif-
ference to the size of the OBDD representing a given function. Consider
the boolean function (x1 + x2) · (x3 + x4) · · · · · (x2n−1 + x2n); it corresponds
to a propositional formula in conjunctive normal form. If we choose the
1 In an implementation this will amount to checking whether two pointers are equal.

370
6 Binary decision diagrams
1
0
x1
x2
x3
x4
x3
x2
x4
Figure 6.11. An OBDD for the even parity function for four bits.
‘natural’ ordering [x1, x2, x3, x4, . . . ], then we can represent this function as
an OBDD with 2n + 2 nodes. Figure 6.12 shows the resulting OBDD for
n = 3. Unfortunately, if we choose instead the ordering
[x1, x3, . . . , x2n−1, x2, x4, . . . , x2n]
the resulting OBDD requires 2n+1 nodes; the OBDD for n = 3 can be seen
in Figure 6.13.
The sensitivity of the size of an OBDD to the particular variable order-
ing is a price we pay for all the advantages that OBDDs have over BDDs.
Although ﬁnding the optimal ordering is itself a computationally expensive
problem, there are good heuristics which will usually produce a fairly good
ordering. Later on we return to this issue in discussions of applications.
The importance of canonical representation
The importance of
having a canonical form for OBDDs in conjunction with an eﬃcient test for
deciding whether two reduced OBDDs are isomorphic cannot be overesti-
mated. It allows us to perform the following tests:
Absence of redundant variables. If the value of the boolean function
f(x1, x2, . . . , xn) does not depend on the value of xi, then any reduced
OBDD which represents f does not contain any xi-node.
Test for semantic equivalence. If two functions f(x1, x2, . . . , xn) and
g(x1, x2, . . . , xn) are represented by OBDDs Bf, respectively Bg, with a
compatible ordering of variables, then we can eﬃciently decide whether f
and g are semantically equivalent. We reduce Bf and Bg (if necessary); f

6.1 Representing boolean functions
371
0
1
x1
x6
x5
x3
x4
x2
Figure 6.12. The OBDD for (x1 + x2) · (x3 + x4) · (x5 + x6) with vari-
able ordering [x1, x2, x3, x4, x5, x6].
x1
x3
x3
x5
x5
x5
x2
x2
x2
x4
x4
1
x6
0
x2
x5
Figure 6.13. Changing the ordering may have dramatic effects on the
size of an OBDD: the OBDD for (x1 + x2) · (x3 + x4) · (x5 + x6) with
variable ordering [x1, x3, x5, x2, x4, x6].

372
6 Binary decision diagrams
and g denote the same boolean functions if, and only if, the reduced OBDDs
have identical structure.
Test for validity. We can test a function f(x1, x2, . . . , xn) for validity (i.e.
f always computes 1) in the following way. Compute a reduced OBDD for
f. Then f is valid if, and only if, its reduced OBDD is B1.
Test for implication. We can test whether f(x1, x2, . . . , xn) implies g(x1,
x2, . . . , xn) (i.e. whenever f computes 1, then so does g) by computing the
reduced OBDD for f · g. This is B0 iﬀthe implication holds.
Test for satisﬁability. We can test a function f(x1, x2, . . . , xn) for satis-
ﬁability (f computes 1 for at least one assignment of 0 and 1 values to its
variables). The function f is satisﬁable iﬀits reduced OBDD is not B0.
6.2 Algorithms for reduced OBDDs
6.2.1 The algorithm reduce
The reductions C1–C3 are at the core of any serious use of OBDDs, for
whenever we construct a BDD we will want to convert it to its reduced form.
In this section, we describe an algorithm reduce which does this eﬃciently
for ordered BDDs.
If the ordering of B is [x1, x2, . . . , xl], then B has at most l + 1 layers. The
algorithm reduce now traverses B layer by layer in a bottom-up fashion,
beginning with the terminal nodes. In traversing B, it assigns an integer
label id(n) to each node n of B, in such a way that the subOBDDs with
root nodes n and m denote the same boolean function if, and only if, id(n)
equals id(m).
Since reduce starts with the layer of terminal nodes, it assigns the ﬁrst
label (say #0) to the ﬁrst 0-node it encounters. All other terminal 0-nodes
denote the same function as the ﬁrst 0-node and therefore get the same label
(compare with reduction C1). Similarly, the 1-nodes all get the next label,
say #1.
Now let us inductively assume that reduce has already assigned integer
labels to all nodes of a layer > i (i.e. all terminal nodes and xj-nodes with
j > i). We describe how nodes of layer i (i.e. xi-nodes) are being handled.
Deﬁnition 6.8 Given a non-terminal node n in a BDD, we deﬁne lo(n) to
be the node pointed to via the dashed line from n. Dually, hi(n) is the node
pointed to via the solid line from n.
Let us describe how the labelling is done. Given an xi-node n, there are
three ways in which it may get its label:

6.2 Algorithms for reduced OBDDs
373
0
1
#0
#1
0
1
0
1
x3
x3
x2
x2
x1
#0
#1
#0
#1
#2
#2
#3
#2
#4
=⇒
x3
x2
x1
#2
#3
#4
Reduce
Figure 6.14. An example execution of the algorithm reduce.
r If the label id(lo(n)) is the same as id(hi(n)), then we set id(n) to be that label.
That is because the boolean function represented at n is the same function as the
one represented at lo(n) and hi(n). In other words, node n performs a redundant
test and can be eliminated by reduction C2.
r If there is another node m such that n and m have the same variable xi, and
id(lo(n)) = id(lo(m)) and id(hi(n)) = id(hi(m)), then we set id(n) to be id(m).
This is because the nodes n and m compute the same boolean function (compare
with reduction C3).
r Otherwise, we set id(n) to the next unused integer label.
Note that only the last case creates a new label. Consider the OBDD
in left side of Figure 6.14; each node has an integer label obtained in the
manner just described. The algorithm reduce then ﬁnishes by redirecting
edges bottom-up as outlined in C1–C3. The resulting reduced OBDD is in
right of Figure 6.14. Since there are eﬃcient bottom-up traversal algorithms
for dags, reduce is an eﬃcient operation in the number of nodes of an
OBDD.
6.2.2 The algorithm apply
Another procedure at the heart of OBDDs is the algorithm apply. It is
used to implement operations on boolean functions such as +, · , ⊕and
complementation (via f ⊕1). Given OBDDs Bf and Bg for boolean formulas
f and g, the call apply (op, Bf, Bg) computes the reduced OBDD of the
boolean formula f op g, where op denotes any function from {0, 1} × {0, 1}
to {0, 1}.

374
6 Binary decision diagrams
The intuition behind the apply algorithm is fairly simple. The algorithm
operates recursively on the structure of the two OBDDs:
1.
let v be the variable highest in the ordering (=leftmost in the list) which occurs
in Bf or Bg.
2.
split the problem into two subproblems for v being 0 and v being 1 and solve
recursively;
3.
at the leaves, apply the boolean operation op directly.
The result will usually have to be reduced to make it into an OBDD. Some
reduction can be done ‘on the ﬂy’ in step 2, by avoiding the creation of a new
node if both branches are equal (in which case return the common result),
or if an equivalent node already exists (in which case, use it).
Let us make all this more precise and detailed.
Deﬁnition 6.9 Let f be a boolean formula and x a variable.
1.
We denote by f[0/x] the boolean formula obtained by replacing all occurrences
of x in f by 0. The formula f[1/x] is deﬁned similarly. The expressions f[0/x]
and f[1/x] are called restrictions of f.
2.
We say that two boolean formulas f and g are semantically equivalent if they
represent the same boolean function (with respect to the boolean variables that
they depend upon). In that case, we write f ≡g.
For example, if f(x, y)
def
= x · (y + x), then f[0/x](x, y) equals 0 · (y + 0),
which is semantically equivalent to 0. Similarly, f[1/y](x, y) is x · (1 + x),
which is semantically equivalent to x.
Restrictions allow us to perform recursion on boolean formulas, by decom-
posing boolean formulas into simpler ones. For example, if x is a variable in
f, then f is equivalent to x · f[0/x] + x · f[1/x]. To see this, consider the case
x = 0; the expression computes to f[0/x]. When x = 1 it yields f[1/x]. This
observation is known as the Shannon expansion, although it can already be
found in G. Boole’s book ‘The Laws of Thought’ from 1854.
Lemma 6.10 (Shannon expansion) For all boolean formulas f and all
boolean variables x (even those not occurring in f) we have
f ≡x · f[0/x] + x · f[1/x].
(6.1)
The function apply is based on the Shannon expansion for f op g:
f op g = xi · (f[0/xi] op g[0/xi]) + xi · (f[1/xi] op g[1/xi]).
(6.2)
This is used as a control structure of apply which proceeds from the roots

6.2 Algorithms for reduced OBDDs
375
0
1
0
1
x4
x3
x1
R5
R6
R4
R2
R1
R3
+
S1
S3
S4
S5
S2
x4
x2
x1
x3
Figure 6.15. An example of two arguments for a call apply (+, Bf, Bg).
of Bf and Bg downwards to construct nodes of the OBDD Bf op g. Let rf be
the root node of Bf and rg the root node of Bg.
1.
If both rf and rg are terminal nodes with labels lf and lg, respectively (recall
that terminal labels are either 0 or 1), then we compute the value lf op lg and
let the resulting OBDD be B0 if that value is 0 and B1 otherwise.
2.
In the remaining cases, at least one of the root nodes is a non-terminal. Suppose
that both root nodes are xi-nodes. Then we create an xi-node n with a dashed
line to apply (op, lo(rf), lo(rg)) and a solid line to apply (op, hi(rf), hi(rg)), i.e.
we call apply recursively on the basis of (6.2).
3.
If rf is an xi-node, but rg is a terminal node or an xj-node with j > i,
then we know that there is no xi-node in Bg because the two OBDDs have
a compatible ordering of boolean variables. Thus, g is independent of xi
(g ≡g[0/xi] ≡g[1/xi]). Therefore, we create an xi-node n with a dashed line
to apply (op, lo(rf), rg) and a solid line to apply (op, hi(rf), rg).
4.
The case in which rg is a non-terminal, but rf is a terminal or an xj-node with
j > i, is handled symmetrically to case 3.
The result of this procedure might not be reduced; therefore apply ﬁnishes
by calling the function reduce on the OBDD it constructed. An example of
apply (where op is +) can be seen in Figures 6.15–6.17. Figure 6.16 shows
the recursive descent control structure of apply and Figure 6.17 shows the
ﬁnal result. In this example, the result of apply (+, Bf, Bg) is Bf.
Figure 6.16 shows that numerous calls to apply occur several times with
the same arguments. Eﬃciency could be gained if these were evaluated only

376
6 Binary decision diagrams
(R1, S1)
x1
x2
x3
(R3, S3)
(R2, S3)
(R3, S2)
x4
x3
(R5, S4)
(R6, S5)
(R4, S3)
(R6, S3)
(R4, S3)
(R4, S3)
x4
(R5, S4)
(R6, S5)
(R6, S5)
x4
(R6, S5)
x4
(R5, S4)
(R6, S4)
(R6, S5)
Figure 6.16. The recursive call structure of apply for the example in
Figure 6.15 (without memoisation).
0
1
x4
x3
x2
x1
Figure 6.17. The result of apply (+, Bf, Bg), where Bf and Bg are given
in Figure 6.15.

6.2 Algorithms for reduced OBDDs
377
the ﬁrst time and the result remembered for future calls. This program-
ming technique is known as memoisation. As well as being more eﬃcient,
it has the advantage that the resulting OBDD requires less reduction. (In
this example, using memoisation eliminates the need for the ﬁnal call to
reduce altogether.) Without memoisation, apply is exponential in the size
of its arguments, since each non-leaf call generates a further two calls. With
memoisation, the number of calls to apply is bounded by 2 · |Bf| · |Bg|, where
|B| is the size of the BDD. This is a worst-time complexity; the actual per-
formance is often much better than this.
6.2.3 The algorithm restrict
Given an OBDD Bf representing a boolean formula f, we need an algo-
rithm restrict such that the call restrict(0, x, Bf) computes the reduced
OBDD representing f[0/x] using the same variable ordering as Bf. The al-
gorithm for restrict(0, x, Bf) works as follows. For each node n labelled
with x, incoming edges are redirected to lo(n) and n is removed. Then we
call reduce on the resulting OBDD. The call restrict (1, x, Bf) proceeds
similarly, only we now redirect incoming edges to hi(n).
6.2.4 The algorithm exists
A boolean function can be thought of as putting a constraint on the values
of its argument variables. For example, the function x + (y · z) evaluates to 1
only if x is 1; or y is 0 and z is 1 – this is a constraint on x, y, and z.
It is useful to be able to express the relaxation of the constraint on a subset
of the variables concerned. To allow this, we write ∃x. f for the boolean
function f with the constraint on x relaxed. Formally, ∃x. f is deﬁned as
f[0/x] + f[1/x]; that is, ∃x. f is true if f could be made true by putting x
to 0 or to 1. Given that ∃x. f
def
= f[0/x] + f[1/x] the exists algorithm can
be implemented in terms of the algorithms apply and restrict as
apply (+, restrict (0, x, Bf), restrict (1, x, Bf)) .
(6.3)
Consider, for example, the OBDD Bf for the function f
def
= x1 · y1 + x2 ·
y2 + x3 · y3, shown in Figure 6.19. Figure 6.20 shows restrict(0, x3, Bf)
and restrict(1, x3, Bf) and the result of applying + to them. (In this case
the apply function happens to return its second argument.)
We can improve the eﬃciency of this algorithm. Consider what happens
during the apply stage of (6.3). In that case, the apply algorithm works on
two BDDs which are identical all the way down to the level of the x-nodes;

378
6 Binary decision diagrams
1
0
x
x
x
z
y
Figure 6.18. An example of a BDD which is not a read-1-BDD.
1
0
x1
x2
x3
y1
y2
y3
Figure 6.19. A BDD Bf to illustrate the exists algorithm.
therefore the returned BDD also has that structure down to the x-nodes.
At the x-nodes, the two argument BDDs diﬀer, so the apply algorithm
will compute the apply of + to these two subBDDs and return that as the
subBDD of the result. This is illustrated in Figure 6.20. Therefore, we can
compute the OBDD for ∃x. f by taking the OBDD for f and replacing each
node labelled with x by the result of calling apply on + and its two branches.
This can easily be generalised to a sequence of exists operations. We
write ∃ˆx. f to mean ∃x1.∃x2. . . . ∃xn. f, where ˆx denotes (x1, x2, . . . , xn).

6.2 Algorithms for reduced OBDDs
379
1
0
x1
x2
y1
y2
y3
1
0
1
0
x1
x2
y1
y2
x1
x2
y1
y2
y3
Figure 6.20. restrict(0, x3, Bf) and restrict(1, x3, Bf) and the result
of applying + to them.
1
0
x1
x2
x3
y1
y2
y3
1
0
x1
x2
y1
y2
y3
1
0
x1
y1
y2
y3
∃x3
⇒
∃x2
⇒
Figure 6.21. OBDDs for f, ∃x3. f and ∃x2.∃x3. f.
The OBDD for this boolean function is obtained from the OBDD for f by
replacing every node labelled with an xi by the + of its two branches.
Figure 6.21 shows the computation of ∃x3. f and ∃x2.∃x3. f (which is
semantically equivalent to x1 · y1 + y2 + y3) in this way.
The boolean quantiﬁer ∀is the dual of ∃:
∀x.f
def
= f[0/x] · f[1/x]
asserting that f could be made false by putting x to 0 or to 1.
The translation of boolean formulas into OBDDs using the algorithms of
this section is summarised in Figure 6.22.

380
6 Binary decision diagrams
Boolean formula f
Representing OBDD Bf
0
B0 (Fig. 6.6)
1
B1 (Fig. 6.6)
x
Bx (Fig. 6.6)
f
swap the 0- and 1-nodes in Bf
f + g
apply (+, Bf, Bg)
f · g
apply (· , Bf, Bg)
f ⊕g
apply (⊕, Bf, Bg)
f[1/x]
restrict (1, x, Bf)
f[0/x]
restrict (0, x, Bf)
∃x.f
apply (+, Bf[0/x], Bf[1/x])
∀x.f
apply (· , Bf[0/x], Bf[1/x])
Figure 6.22. Translating boolean formulas f to OBDDs Bf, given a
fixed, global ordering on boolean variables.
Algorithm Input OBDD(s)
Output OBDD
Time-complexity
reduce
B
reduced B
O(|B| · log |B|)
apply
Bf, Bg (reduced) Bf op g (reduced)
O(|Bf| · |Bg|)
restrict Bf (reduced)
Bf[0/x] or Bf[1/x] (reduced) O(|Bf| · log |Bf|)
∃
Bf (reduced)
B∃x1.∃x2....∃xn.f (reduced)
NP-complete
Figure 6.23. Upper bounds in terms of the input OBDD(s) for the
worst-case running times of our algorithms needed in our implementa-
tion of boolean formulas.
6.2.5 Assessment of OBDDs
Time complexities for computing OBDDs
We can measure the com-
plexity of the algorithms of the preceding section by giving upper bounds
for the running time in terms of the sizes of the input OBDDs. The table
in Figure 6.23 summarises these upper bounds (some of those upper bounds
may require more sophisticated versions of the algorithms than the versions
presented in this chapter). All the operations except nested boolean quantiﬁ-
cation are practically eﬃcient in the size of the participating OBDDs. Thus,
modelling very large systems with this approach will work if the OBDDs

6.2 Algorithms for reduced OBDDs
381
which represent the systems don’t grow too large too fast. If we can some-
how control the size of OBDDs, e.g. by using good heuristics for the choice
of variable ordering, then these operations are computationally feasible. It
has already been shown that OBDDs modelling certain classes of systems
and networks don’t grow excessively.
The expensive computational operations are the nested boolean quantiﬁ-
cations ∃z1. . . . ∃zn.f and ∀z1. . . . ∀zn.f. By exercise 1 on page 406, the com-
putation of the OBDD for ∃z1. . . . ∃zn.f, given the OBDD for f, is an NP-
complete problem2; thus, it is unlikely that there exists an algorithm with
a feasible worst-time complexity. This is not to say that boolean functions
modelling practical systems may not have eﬃcient nested boolean quan-
tiﬁcations. The performance of our algorithms can be improved by using
further optimisation techniques, such as parallelisation.
Note that the operations apply, restrict, etc. are only eﬃcient in the
size of the input OBDDs. So if a function f does not have a compact repre-
sentation as an OBDD, then computing with its OBDD will not be eﬃcient.
There are such nasty functions; indeed, one of them is integer multiplication.
Let bn−1bn−2 . . . b0 and an−1an−2 . . . a0 be two n-bit integers, where bn−1 and
an−1 are the most signiﬁcant bits and b0 and a0 are the least signiﬁcant bits.
The multiplication of these two integers results in a 2n-bit integer. Thus, we
may think of multiplication as 2n many boolean functions fi in 2n variables
(n bits for input b and n bits for input a), where fi denotes the ith output
bit of the multiplication. The following negative result, due to R. E. Bryant,
shows that OBDDs cannot be used for implementing integer multiplication.
Theorem 6.11 Any OBDD representation of fn−1 has at least a number
of vertices proportional to 1.09n, i.e. its size is exponential in n.
Extensions and variations of OBDDs
There are many variations and
extensions to the OBDD data structure. Many of them can implement cer-
tain operations more eﬃciently than their OBDD counterparts, but it seems
that none of them perform as well as OBDDs overall. In particular, one fea-
ture which many of the variations lack is the canonical form; therefore they
lack an eﬃcient algorithm for deciding when two objects denote the same
boolean function.
One kind of variation allows non-terminal nodes to be labelled with bi-
nary operators as well as boolean variables. Parity OBDDs are like OBDDs
in that there is an ordering on variables and every variable may occur at
2 Another NP-complete problem is to decide the satisﬁability of formulas of propositional logic.

382
6 Binary decision diagrams
most once on a path; but some non-terminal nodes may be labelled with ⊕,
the exclusive-or operation. The meaning is that the function represented by
that node is the exclusive-or of the boolean functions determined by its chil-
dren. Parity OBDDs have similar algorithms for apply, restrict, etc. with
the same performance, but they do not have a canonical form. Checking for
equivalence cannot be done in constant time. There is, however, a cubic algo-
rithm for determining equivalence; and there are also eﬃcient probabilistic
tests. Another variation of OBDDs allows complementation nodes, with the
obvious meaning. Again, the main disadvantage is the lack of canonical form.
One can also allow non-terminal nodes to be unlabelled and to branch
to more than two children. This can then be understood either as non-
deterministic branching, or as probabilistic branching: throw a pair of dice
to determine where to continue the path. Such methods may compute wrong
results; one then aims at repeating the test to keep the (probabilistic)
error as small as desired. This method of repeating probabilistic tests is
called probabilistic ampliﬁcation. Unfortunately, the satisﬁability problem
for probabilistic branching OBDDs is NP-complete. On a good note, prob-
abilistic branching OBDDs can verify integer multiplication.
The development of extensions or variations of OBDDS which are cus-
tomised to certain classes of boolean functions is an important area of on-
going research.
6.3 Symbolic model checking
The use of BDDs in model checking resulted in a signiﬁcant breakthrough in
veriﬁcation in the early 1990s, because they have allowed systems with much
larger state spaces to be veriﬁed. In this section, we describe in detail how
the model-checking algorithm presented in Chapter 3 can be implemented
using OBDDs as the basic data structure.
The pseudo-code presented in Figure 3.28 on page 227 takes as input a
CTL formula φ and returns the set of states of the given model which satisfy
φ. Inspection of the code shows that the algorithm consists of manipulating
intermediate sets of states. We show in this section how the model and the
intermediate sets of states can be stored as OBDDs; and how the operations
required in that pseudo-code can be implemented in terms of the operations
on OBDDs which we have seen in this chapter.
We start by showing how sets of states are represented with OBDDs,
together with some of the operations required. Then, we extend that to
the representation of the transition system; and ﬁnally, we show how the
remainder of the required operations is implemented.

6.3 Symbolic model checking
383
Model checking using OBDDs is called symbolic model checking. The term
emphasises that individual states are not represented; rather, sets of states
are represented symbolically, namely, those which satisfy the formula being
checked.
6.3.1 Representing subsets of the set of states
Let S be a ﬁnite set (we forget for the moment that it is a set of states). The
task is to represent the various subsets of S as OBDDs. Since OBDDs encode
boolean functions, we need somehow to code the elements of S as boolean
values. The way to do this in general is to assign to each element s ∈S a
unique vector of boolean values (v1, v2, . . . , vn), each vi ∈{0, 1}. Then, we
represent a subset T by the boolean function fT which maps (v1, v2, . . . , vn)
onto 1 if s ∈T and maps it onto 0 otherwise.
There are 2n boolean vectors (v1, v2, . . . , vn) of length n. Therefore, n
should be chosen such that 2n−1 < |S| ≤2n, where |S| is the number of
elements in S. If |S| is not an exact power of 2, there will be some vec-
tors which do not correspond to any element of S; they are just ignored.
The function fT : {0, 1}n →{0, 1} which tells us, for each s, represented by
(v1, v2, . . . , vn), whether it is in the set T or not, is called the characteristic
function of T.
In the case that S is the set of states of a transition system M = (S, →, L)
(see Deﬁnition 3.4), there is a natural way of choosing the representation
of S as boolean vectors. The labelling function L : S →P(Atoms) (where
P(Atoms) is the set of subsets of Atoms) gives us the encoding. We assume
a ﬁxed ordering on the set Atoms, say x1, x2, . . . , xn, and then represent
s ∈S by the vector (v1, v2, . . . , vn), where, for each i, vi equals 1 if xi ∈
L(s) and vi is 0 otherwise. In order to guarantee that each s has a unique
representation as a boolean vector, we require that, for all s1, s2 ∈S, L(s1) =
L(s2) implies s1 = s2. If this is not the case, perhaps because 2|Atoms| < |S|,
we can add extra atomic propositions in order to make enough distinctions
(Cf. introduction of the turn variable for mutual exclusion in Section 3.3.4.)
From now on, we refer to a state s ∈S by its representing boolean vector
(v1, v2, . . . , vn), where vi is 1 if xi ∈L(s) and 0 otherwise. As an OBDD,
this state is represented by the OBDD of the boolean function l1 · l2 · · · · · ln,
where li is xi if xi ∈L(s) and xi otherwise. The set of states {s1, s2, . . . , sm}
is represented by the OBDD of the boolean function
(l11 · l12 · · · · · l1n) + (l21 · l22 · · · · · l2n) + · · · + (lm1 · lm2 · · · · · lmn)
where li1 · li2 · · · · · lin represents state si.

384
6 Binary decision diagrams
s2
x1
s0
x2
s1
Figure 6.24. A simple CTL model (Example 6.12).
set of
representation by
representation by
states
boolean values
boolean function
∅
0
{s0}
(1, 0)
x1 · x2
{s1}
(0, 1)
x1 · x2
{s2}
(0, 0)
x1 · x2
{s0, s1}
(1, 0), (0, 1)
x1 · x2 + x1 · x2
{s0, s2}
(1, 0), (0, 0)
x1 · x2 + x1 · x2
{s1, s2}
(0, 1), (0, 0)
x1 · x2 + x1 · x2
S
(1, 0), (0, 1), (0, 0)
x1 · x2 + x1 · x2 + x1 · x2
Figure 6.25. Representation of subsets of states of the model of Figure 6.24.
The key point which makes this representation interesting is that the
OBDD representing a set of states may be quite small.
Example 6.12 Consider the CTL model in Figure 6.24, given by:
S
def
= {s0, s1, s2}
→
def
= {(s0, s1), (s1, s2), (s2, s0), (s2, s2)}
L(s0)
def
= {x1}
L(s1)
def
= {x2}
L(s2)
def
= ∅.
Note that it has the property that, for all states s1 and s2, L(s1) = L(s2)
implies s1 = s2, i.e. a state is determined entirely by the atomic formulas
true in it. Sets of states may be represented by boolean values and by boolean
formulas with the ordering [x1, x2], as shown in Figure 6.25.
Notice that the vector (1, 1) and the corresponding function x1 · x2 are
unused. Therefore, we are free to include it in the representation of a subset

6.3 Symbolic model checking
385
x2
0
1
x1
0
1
x1
x2
x2
Figure 6.26. Two OBDDs for the set {s0, s1} (Example 6.12).
of S or not; so we may choose to include it or not in order to optimise the
size of the OBDD. For example, the subset {s0, s1} is better represented
by the boolean function x1 + x2, since its OBDD is smaller than that for
x1 · x2 + x1 · x2 (Figure 6.26).
In order to justify the claim that the representation of subsets of S as
OBDDs will be suitable for the algorithm presented in Section 3.6.1, we need
to look at how the operations on subsets which are used in that algorithm
can be implemented in terms of the operations we have deﬁned on OBDDs.
The operations in that algorithm are:
r Intersection, union and complementation of subsets. It is clear that these are
represented by the boolean functions ·, + and ¯ respectively. The implementation
via OBDDs of · and + uses the apply algorithm (Section 6.2.2).
r The functions
pre∃(X) = {s ∈S | exists s′, (s →s′ and s′ ∈X)}
(6.4)
pre∀(X) = {s | for all s′, (s →s′ implies s′ ∈X)}.
The function pre∃(instrumental in SATEX and SATEU) takes a subset X of states
and returns the set of states which can make a transition into X. The function
pre∀, used in SATAF, takes a set X and returns the set of states which can make
a transition only into X. In order to see how these are implemented in terms of
OBDDs, we need ﬁrst to look at how the transition relation itself is represented.
6.3.2 Representing the transition relation
The transition relation →of a model M = (S, →, L) is a subset of S × S.
We have already seen that subsets of a given ﬁnite set may be represented
as OBDDs by considering the characteristic function of a binary encoding.
Just like in the case of subsets of S, the binary encoding is naturally given
by the labelling function L. Since →is a subset of S × S, we need two copies
of the boolean vectors. Thus, the link s →s′ is represented by the pair of

386
6 Binary decision diagrams
x1
x2
x′
1
x′
2
→
0
0
0
0
1
0
0
0
1
0
0
0
1
0
1
0
0
1
1
0
0
1
0
0
1
0
1
0
1
0
0
1
1
0
0
0
1
1
1
0
1
0
0
0
0
1
0
0
1
1
1
0
1
0
0
1
0
1
1
0
1
1
0
0
0
1
1
0
1
0
1
1
1
0
0
1
1
1
1
0
x1
x′
1
x2
x′
2
→
0
0
0
0
1
0
0
0
1
0
0
0
1
0
1
0
0
1
1
0
0
1
0
0
1
0
1
0
1
0
0
1
1
0
0
0
1
1
1
0
1
0
0
0
0
1
0
0
1
1
1
0
1
0
0
1
0
1
1
0
1
1
0
0
0
1
1
0
1
0
1
1
1
0
0
1
1
1
1
0
Figure 6.27. The truth table for the transition relation of Figure 6.24
(see Example 6.13). The left version shows the ordering of variables
[x1, x2, x′
1, x′
2], while the right one orders the variables [x1, x′
1, x2, x′
2] (the
rows are ordered lexicographically).
boolean vectors ((v1, v2, . . . , vn), (v′
1, v′
2, . . . , v′
n)), where vi is 1 if pi ∈L(s)
and 0 otherwise; and similarly, v′
i is 1 if pi ∈L(s′) and 0 otherwise. As an
OBDD, the link is represented by the OBDD for the boolean function
(l1 · l2 · · · · · ln) · (l′
1 · l′
2 · · · · · l′
n)
and a set of links (for example, the entire relation →) is the OBDD for the
+ of such formulas.
Example 6.13 To compute the OBDD for the transition relation of Fig-
ure 6.24, we ﬁrst show it as a truth table (Figure 6.27 (left)). Each 1 in
the ﬁnal column corresponds to a link in the transition relation and each 0
corresponds to the absence of a link. The boolean function is obtained by
taking the disjunction of the rows having 1 in the last column and is
f→def
= x1 · x2 · x′
1 · x′
2 + x1 · x2 · x′
1 · x′
2 + x1 · x2 · x′
1 · x′
2 + x1 · x2 · x′
1 · x′
2.
(6.5)
It turns out that it is usually more eﬃcient to interleave unprimed and
primed variables in the OBDD variable ordering for →. We therefore use

6.3 Symbolic model checking
387
0
1
x2
x2
x′
2
x′
2
x1
x′
1
x′
1
Figure 6.28. An OBDD for the transition relation of Example 6.13.
[x1, x′
1, x2, x′
2] rather than [x1, x2, x′
1, x′
2]. Figure 6.27 (right) shows the truth
table redrawn with the interleaved ordering of the columns and the rows
reordered lexicographically. The resulting OBDD is shown in Figure 6.28.
6.3.3 Implementing the functions pre∃and pre∀
It remains to show how an OBDD for pre∃(X) and pre∀(X) can be com-
puted, given OBDDs BX for X and B→for the transition relation →. First
we observe that pre∀can be expressed in terms of complementation and
pre∃, as follows: pre∀(X) = S −pre∃(S −X), where we write S −Y for the
set of all s ∈S which are not in Y . Therefore, we need only explain how to
compute the OBDD for pre∃(X) in terms of BX and B→. Now (6.4) suggests
that one should proceed as follows:
1.
Rename the variables in BX to their primed versions; call the resulting OBDD
BX′.
2.
Compute the OBDD for exists(ˆx′, apply(·, B→, BX′)) using the apply and
exists algorithms (Sections 6.2.2 and 6.2.4).
6.3.4 Synthesising OBDDs
The method used in Example 6.13 for producing an OBDD for the transi-
tion relation was to compute ﬁrst the truth table and then an OBDD which
might not be in its fully reduced form; hence the need for a ﬁnal call to

388
6 Binary decision diagrams
the reduce function. However, this procedure would be unacceptable if ap-
plied to realistically sized systems with a large number of variables, for the
truth table’s size is exponential in the number of boolean variables. The
key idea and attraction of applying OBDDs to ﬁnite systems is therefore to
take a system description in a language such as SMV and to synthesise the
OBDD directly, without having to go via intermediate representations (such
as binary decision trees or truth tables) which are exponential in size.
SMV allows us to deﬁne the next value of a variable in terms of the
current values of variables (see the examples of code in Section 3.3.2)3. This
can be compiled into a set of boolean functions fi, one for each variable xi,
which deﬁne the next value of xi in terms of the current values of all the
variables. In order to cope with non-deterministic assignment (such as the
assignment to status in the example on page 192), we extend the set of
variables by adding unconstrained variables which model the input. Each x′
i
is a deterministic function of this enlarged set of variables; thus, x′
i ↔fi,
where f ↔g = 1 if, and only if, f and g compute the same values, i.e. it is
a shorthand for f ⊕g.
The boolean function representing the transition relation is therefore of
the form

1≤i≤n
x′
i ↔fi,
(6.6)
where 
1≤i≤n gi is a shorthand for g1 · g2 · . . . · gn. Note that the  ranges
only over the non-input variables. So, if u is an input variable, the boolean
function does not contain any u′ ↔fu.
Figure 6.22 showed how the reduced OBDD could be computed from the
parse tree of such a boolean function. Thus, it is possible to compile SMV
programs into OBDDs such that their speciﬁcations can be executed accord-
ing to the pseudo-code of the function SAT, now interpreted over OBDDs.
On page 396 we will see that this OBDD implementation can be extended
to simple fairness constraints.
Modelling sequential circuits
As a further application of OBDDs to
veriﬁcation, we show how OBDDs representing circuits may be synthesised.
Synchronous circuits.
Suppose that we have a design of a sequential circuit
such as the one in Figure 6.29. This is a synchronous circuit (meaning that
3 SMV also allows next values to be deﬁned in terms of next values, i.e. the keyword next to appear
in expressions on the right-hand side of :=. This is useful for describing synchronisations, for
example, but we ignore that feature here.

6.3 Symbolic model checking
389
x2
x1
Figure 6.29. A simple synchronous circuit with two registers.
all the state variables are updated synchronously in parallel) whose func-
tionality can be described by saying what the values of the registers x1 and
x2 in the next state of the circuit are. The function f→coding the possible
next states of the circuits is
(x′
1 ↔x1) · (x′
2 ↔x1 ⊕x2).
(6.7)
This may now be translated into an OBDD by the methods summarised in
Figure 6.22.
Asynchronous circuits.
The symbolic encoding of synchronous circuits is
in its logical structure very similar to the encoding of f→for CTL models;
compare the codings in (6.7) and (6.6). In asynchronous circuits, or processes
in SMV, the logical structure of f→changes. As before, we can construct
functions fi which code the possible next state in the local component, or
the SMV process, i. For asynchronous systems, there are two principal ways
of composing these functions into global system behaviour:
r In a simultaneous model, a global transition is one in which any number of
components may make their local transition. This is modelled as
f →def
=
n

i=1
((x′
i ↔fi) + (x′
i ↔xi)) .
(6.8)
r In an interleaving model, exactly one local component makes a local transition;

390
6 Binary decision diagrams
all other local components remain in their local state:
f →def
=
n

i=1

(x′
i ↔fi) ·

j̸=i
(x′
j ↔xj)

.
(6.9)
Observe the duality in these approaches: the simultaneous model has an
outer product, whereas the interleaving model has an outer sum. The latter,
if used in ∃ˆx′.f (‘for some next state’), can be optimised since sums distribute
over existential quantiﬁcation; in Chapter 2 this was the equivalence ∃x.(φ ∨
ψ) ≡∃x.φ ∨∃x.ψ. Thus, global states reachable in one step are the ‘union’
of all the states reachable in one step in the local components; compare the
formulas in (6.8) and (6.9) with (6.6).
6.4 A relational mu-calculus
We saw in Section 3.7 that evaluating the set of states satisfying a CTL for-
mula in a model may involve the computation of a ﬁxed point of an operator.
For example, [[EF φ]] is the least ﬁxed point of the operator F : P(S) →P(S)
given by F(X) = [[φ]] ∪pre∃(X).
In this section, we introduce a syntax for referring to ﬁxed points in the
context of boolean formulas. Fixed-point invariants frequently occur in all
sorts of applications (for example, the common-knowledge operator CG in
Chapter 5), so it makes sense to have an intermediate language for express-
ing such invariants syntactically. This language also provides a formalism
for describing interactions and dependences of such invariants. We will see
shortly that symbolic model checking in the presence of simple fairness con-
straints exhibits such more complex relationships between invariants.
6.4.1 Syntax and semantics
Deﬁnition 6.14 The formulas of the relational mu-calculus are given by
the grammar
v ::= x | Z
f ::= 0 | 1 | v | f | f1 + f2 | f1 · f2 | f1 ⊕f2 |
∃x.f | ∀x.f | µZ.f | νZ.f | f[ˆx := ˆx′]
(6.10)
where x and Z are boolean variables, and ˆx is a tuple of variables. In the
formulas µZ.f and νZ.f, any occurrence of Z in f is required to fall within an
even number of complementation symbols ¯; such an f is said to be formally
monotone in Z. (In exercise 7 on page 410 we consider what happens if we
do not require formal monotonicity.)

6.4 A relational mu-calculus
391
Convention 6.15 The binding priorities for the grammar in (6.10) are that
¯, and [ˆx := ˆx′] have the highest priority; followed by ∃x and ∀y; then µZ
and νZ; followed by · . The operators + and ⊕have the lowest binding
priority.
The symbols µ and ν are called least ﬁxed-point and greatest ﬁxed-point
operators, respectively. In the formula µZ.f, the interesting case is that in
which f contains an occurrence of Z. In that case, f can be thought of as
a function, taking Z to f. The formula µZ.f is intended to mean the least
ﬁxed point of that function. Similarly, νZ.f is the greatest ﬁxed point of the
function. We will see how this is done in the semantics.
The formula f[ˆx := ˆx′] expresses an explicit substitution which forces f
to be evaluated using the values of x′
i rather than xi. (Recall that the primed
variables refer to the next state.) Thus, this syntactic form is not a meta-
operation denoting a substitution, but an explicit syntactic form in its own
right. The substitution will be made on the semantic side, not the syntactic
side. This diﬀerence will become clear when we present the semantics of ⊨.
A valuation ρ for f is an assignment of values 0 or 1 to all variables v.
We deﬁne a satisfaction relation ρ ⊨f inductively over the structure of such
formulas f, given a valuation ρ.
Deﬁnition 6.16 Let ρ be a valuation and v a variable. We write ρ(v) for
the value of v assigned by ρ. We deﬁne ρ[v →0] to be the updated valuation
which assigns 0 to v and ρ(w) to all other variables w. Dually, ρ[v →1]
assigns 1 to v and ρ(w) to all other variables w.
For example, if ρ is the valuation represented by (x, y, Z) ⇒(1, 0, 1) –
meaning that ρ(x) = 1, ρ(y) = 0, ρ(Z) = 1 and ρ(v) = 0 for all other vari-
ables v – then ρ[x →0] is represented by (x, y, Z) ⇒(0, 0, 1), whereas
ρ[Z →0] is (x, y, Z) ⇒(1, 0, 0). The assumption that valuations assign val-
ues to all variables is rather mathematical, but avoids some complications
which have to be addressed in implementations (see exercise 3 on page 409).
Updated valuations allow us to deﬁne the satisfaction relation for all for-
mulas without ﬁxed points:
Deﬁnition 6.17 We deﬁne a satisfaction relation ρ ⊨f for formulas f with-
out ﬁxed-point subformulas with respect to a valuation ρ by structural in-
duction:
r ρ ̸⊨0
r ρ ⊨1
r ρ ⊨v iﬀρ(v) equals 1

392
6 Binary decision diagrams
r ρ ⊨f iﬀρ ̸⊨f
r ρ ⊨f + g iﬀρ ⊨f or ρ ⊨g
r ρ ⊨f · g iﬀρ ⊨f and ρ ⊨g
r ρ ⊨f ⊕g iﬀρ ⊨(f · g + f · g)
r ρ ⊨∃x.f iﬀρ[x →0] ⊨f or ρ[x →1] ⊨f
r ρ ⊨∀x.f iﬀρ[x →0] ⊨f and ρ[x →1] ⊨f
r ρ ⊨f[ˆx := ˆx′] iﬀρ[ˆx := ˆx′] ⊨f,
where ρ[ˆx := ˆx′] is the valuation which assigns the same values as ρ, but for
each xi it assigns ρ(x′
i).
The semantics of boolean quantiﬁcation closely resembles the one for the
quantiﬁers of predicate logic. The crucial diﬀerence, however, is that boolean
formulas are only interpreted over the ﬁxed universe of values {0, 1}, whereas
predicate formulas may take on values in all sorts of ﬁnite or inﬁnite models.
Example 6.18 Let ρ be such that ρ(x′
1) equals 0 and ρ(x′
2) is 1. We evaluate
ρ ⊨(x1 + x2)[ˆx := ˆx′] which holds iﬀρ[ˆx := ˆx′] ⊨(x1 + x2). Thus, we need
ρ[ˆx := ˆx′] ⊨x1 or ρ[ˆx := ˆx′] ⊨x2 to be the case. Now, ρ[ˆx := ˆx′] ⊨x1 cannot
be, for this would mean that ρ(x′
1) equals 1. Since ρ[ˆx := ˆx′] ⊨x2 would
imply that ρ[ˆx := ˆx′] ̸⊨x2, we infer that ρ[ˆx := ˆx′] ̸⊨x2 because ρ(x′
2) equals
1. In summary, we demonstrated that ρ ̸⊨(x1 + x2)[ˆx := ˆx′].
We now extend the deﬁnition of ⊨to the ﬁxed-point operators µ and ν.
Their semantics will have to reﬂect their meaning as least, respectively great-
est, ﬁxed-point operators. We deﬁne the semantics of µZ.f via its syntactic
approximants which unfold the meaning of µZ.f:
µ0Z.f
def
= 0
µm+1Z.f
def
= f[µmZ.f/Z]
(m ≥0).
(6.11)
The unfolding is achieved by a meta-operation [g/Z] which, when applied
to a formula f, replaces all free occurrences of Z in f with g. Thus, we view
µZ as a binding construct similar to the quantiﬁers ∀x and ∃x, and [g/Z]
is similar to the substitution [t/x] in predicate logic. For example, (x1 +
∃x2.(Z · x2))[x1/Z] is the formula x1 + ∃x2.(x1 · x2), whereas ((µZ.x1 + Z) ·
(x1 + ∃x2.(Z · x2)))[x1/Z] equals (µZ.x1 + Z) · (x1 + ∃x2.(x1 · x2)). See ex-
ercise 3 on page 409 for a formal account of this meta-operation.
With these approximants we can deﬁne:
ρ ⊨µZ.f iﬀ(ρ ⊨µmZ.f for some m ≥0).
(6.12)

6.4 A relational mu-calculus
393
Thus, to determine whether µZ.f is true with respect to a valuation ρ,
we have to ﬁnd some m ≥0 such that ρ ⊨µmZ.f holds. A sensible strategy
is to try to prove this for the smallest such m possible, if indeed such an
m can be found. For example, in attempting to show ρ ⊨µZ.Z, we try
ρ ⊨µ0Z.Z, which fails since the latter formula is just 0. Now, µ1Z.Z is
deﬁned to be Z[µ0Z.Z/Z] which is just µ0Z.Z again.
We can now use
mathematical induction on m ≥0 to show that µmZ.Z equals µ0Z.Z for all
m ≥0. By (6.12), this implies ρ ̸⊨µZ.Z.
The semantics for νZ.f is similar. First, let us deﬁne a family of approx-
imants ν0Z.f, ν1Z.f, . . . by
ν0Z.f
def
= 1
νm+1Z.f
def
= f[νmZ.f/Z]
(m ≥0).
(6.13)
Note that this deﬁnition only diﬀers from the one for µmZ.f in that the
ﬁrst approximant is deﬁned to be 1 instead of 0.
Recall how the greatest ﬁxed point for EG φ requires that φ holds on all
states of some path. Such invariant behaviour cannot be expressed with a
condition such as in (6.12), but is adequately deﬁned by demanding that
ρ ⊨νZ.f iﬀ(ρ ⊨νmZ.f for all m ≥0).
(6.14)
A dual reasoning to the above shows that ρ ⊨νZ.Z holds, regardless of the
nature of ρ.
One informal way of understanding the deﬁnitions in (6.12) and (6.14) is
that ρ ⊨µZ.f is false until, and if, it is proven to hold; whereas ρ ⊨νZ.f is
true until, and if, it is proven to be false. The temporal aspect is encoded
by the unfolding of the recursion in (6.11), or in (6.13).
To prove that this recursive way of specifying ρ ⊨f actually is well de-
ﬁned, one has to consider more general forms of induction which keep track
not only of the height of f’s parse tree, but also of the number of syntactic
approximants µmZ.g and νnZ.h, their ‘degree’ (in this case, m and n), as
well as their ‘alternation’ (the body of a ﬁxed point may contain a free oc-
currence of a variable for a recursion higher up in the parse tree). This can
be done, though we won’t discuss the details here.
6.4.2 Coding CTL models and specifications
Given a CTL model M = (S, →, L), the µ and ν operators permit us to
translate any CTL formula φ into a formula, fφ, of the relational mu-calculus
such that fφ represents the set of states s ∈S with s ⊨φ. Since we already
saw how to represent subsets of states as such formulas, we can then capture

394
6 Binary decision diagrams
the model-checking problem
M, I
?
⊨φ
(6.15)
of whether all initial states s ∈I satisfy φ, in purely symbolic form: we
answer in the aﬃrmative if fI · f
φ is unsatisﬁable, where fI is the charac-
teristic function of I ⊆S. Otherwise, the logical structure of fI · f
φ may be
exploited to extract debugging information for correcting the model M in
order to make (6.15) true.
Recall how we can represent the transition relation →as a boolean for-
mula f→(see Section 6.3.2). As before, we assume that states are coded as
bit vectors (v1, v2, . . . , vn) and so the free boolean variables of all functions
fφ are subsumed by the vector ˆx. The coding of the CTL formula φ as a
function fφ in the relational mu-calculus is now given inductively as follows:
fx def
= x
for variables x
f⊥def
= 0
f¬φ def
= fφ
fφ∧ψ def
= fφ · fψ
fEX φ def
= ∃ˆx′. (f→· fφ[ˆx := ˆx′]).
The clause for EX deserves explanation. The variables xi refer to the
current state, whereas x′
i refer to the next state. The semantics of CTL says
that s ⊨EX φ if, and only if, there is some s′ with s →s′ and s′ ⊨φ. The
boolean formula encodes this deﬁnition, computing 1 precisely when this is
the case. If ˆx models the current state s, then ˆx′ models a possible successor
state if f→, a function in (ˆx, ˆx′), holds. We use the nested boolean quantiﬁer
∃ˆx′ in order to say ‘there is some successor state.’ Observe also the desired
eﬀect of [ˆx := ˆx′] performed on fφ, thereby ‘forcing’ φ to be true at some
next state4.
The clause for EF is more complicated and involves the µ operator. Recall
the equivalence
EF φ ≡φ ∨EX EF φ.
(6.16)
4 Exercise 6 on page 409 should give you a feel for how the semantics of f[ˆx := ˆx′] does not inter-
fere with potential ∃ˆx′ or ∀ˆx′ quantiﬁers within f. For example, to evaluate ρ ⊨(∃ˆx′.f)[ˆx := ˆx′],
we evaluate ρ[ˆx := ˆx′] ⊨∃ˆx′.f, which is true if we can ﬁnd some values (v1, v2, . . . , vn) ∈{0, 1}n
such that ρ[ˆx := ˆx′][x′
1 
→v1][x′
2 
→v2] . . . [x′
n 
→vn] ⊨f is true. Observe that the resulting en-
vironment binds all x′
i to vi, but for all other values it binds them according to ρ[ˆx := ˆx′]; since
the latter binds xi to ρ(x′
i) which is the ‘old’ value of x′
i, this is exactly what we desire in order
to prevent a clash of variable names with the intended semantics.
Recall that an OBDD implementation synthesises formulas in a bottom-up fashion, so a reduced
OBDD for ∃ˆx′.f will not contain any x′
i nodes as its function does not depend on those variables.
Thus, OBDDs also avoid such name clash problems.

6.4 A relational mu-calculus
395
Therefore, fEF φ has to be equivalent to fφ + fEX EF φ which in turn is equiv-
alent to fφ + ∃ˆx′. (f→· fEF φ[ˆx := ˆx′]). Now, since EF involves computing
the least ﬁxed point of the operator derived from the Equivalence (6.16), we
obtain
fEF φ def
= µZ. (fφ + ∃ˆx′. (f→· Z[ˆx := ˆx′])).
(6.17)
Note that the substitution Z[ˆx := ˆx′] means that the boolean function Z
should be made to depend on the x′
i variables, rather than the xi variables.
This is because the evaluation of ρ ⊨Z[ˆx := ˆx′] results in ρ[ˆx := ˆx′] ⊨Z,
where the latter valuation satisﬁes ρ[ˆx := ˆx′](xi) = ρ(x′
i). Then, we use the
modiﬁed valuation ρ[ˆx := ˆx′] to evaluate Z.
Since EF φ is equivalent to E[⊤U φ], we can generalise our coding of EF φ
accordingly:
fE[φUψ] def
= µZ. (fψ + fφ · ∃ˆx′. (f→· Z[ˆx := ˆx′])).
(6.18)
The coding of AF is similar to the one for EF in (6.17), except that ‘for
some’ (boolean quantiﬁcation ∃ˆx′) gets replaced by ‘for all’ (boolean quantiﬁ-
cation ∀ˆx′) and the ‘conjunction’ f→· Z[ˆx := ˆx′] turns into the ‘implication’
f→+ Z[ˆx := ˆx′]:
fAF φ def
= µZ. (fφ + ∀ˆx′. (f→+ Z[ˆx := ˆx′])).
(6.19)
Notice how the semantics of µZ.f in (6.12) reﬂects the intended meaning
of the AF connective. The mth approximant of fAF φ, which we write as
fAF φ
m
, represents those states where all paths reach a φ-state within m steps.
This leaves us with coding EG, for then we have provided such a coding
for an adequate fragment of CTL (recall Theorem 3.17 on page 216). Because
EG involves computing greatest ﬁxed points, we make use of the ν operator:
fEG φ def
= νZ. (fφ · ∃ˆx′. (f→· Z[ˆx := ˆx′])).
(6.20)
Observe that this does follow the logical structure of the semantics of
EG: we need to show φ in the present state and then we have to ﬁnd some
successor state satisfying EG φ. The crucial point is that this obligation
never ceases; this is exactly what we ensured in (6.14).
Let us see these codings in action on the model of Figure 6.24. We
want to perform a symbolic model check of the formula EX (x1 ∨¬x2).
You should verify, using e.g. the labelling algorithm from Chapter 3, that
[[EX (x1 ∨¬x2)]] = {s1, s2}. Our claim is that this set is computed symbol-
ically by the resulting formula fEX (x1∨¬x2). First, we compute the formula

396
6 Binary decision diagrams
f→which represents the transition relation →:
f→= (x′
1 ↔x1 · x2 · u) · (x′
2 ↔x1)
where u is an input variable used to model the non-determinism (compare
the form (6.6) for the transition relation in Section 6.3.4). Thus, we obtain
fEX (x1∨¬x2) = ∃x′
1.∃x′
2.(f→· fx1∨¬x2[ˆx := ˆx′])
= ∃x′
1.∃x′
2.((x′
1 ↔x1 · x2 · u) · (x′
2 ↔x1) · (x′
1 + x′
2)).
To see whether s0 satisﬁes EX (x1 ∨¬x2), we evaluate ρ0 ⊨fEX (x1∨¬x2),
where ρ0(x1) = 1 and ρ0(x2) = 0 (the value of ρ0(u) does not matter). We
ﬁnd that this does not hold, whence s0 ̸⊨EX (x1 ∨¬x2). Likewise, we verify
s1 ⊨EX (x1 ∨¬x2) by showing ρ1 ⊨fEX (x1∨¬x2); and s2 ⊨EX (x1 ∨¬x2) by
showing ρ2 ⊨fEX (x1∨¬x2), where ρi is the valuation representing state si.
As a second example, we compute fAF (¬x1∧¬x2) for the model in
Figure 6.24. First, note that all three5 states satisfy AF (¬x1 ∧¬x2), if we
apply the labelling algorithm to the explicit model. Let us verify that the
symbolic encoding matches this result. By (6.19), we have that fAF (¬x1∧¬x2)
equals
µZ.

(x1 · x2) + ∀x′
1.∀x′
2.(x′
1 ↔x1 · x2 · u) · (x′
2 ↔x1) · Z[ˆx := ˆx′]
	
. (6.21)
By (6.12), we have ρ ⊨fAF (¬x1∧¬x2) iﬀρ ⊨fAF (¬x1∧¬x2)
m
for some m ≥0.
Clearly, we have ρ ̸⊨fAF (¬x1∧¬x2)
0
. Now, fAF (¬x1∧¬x2)
1
equals
((x1 · x2) + ∀x′
1.∀x′
2.(x′
1 ↔x1 · x2 · u) · (x′
2 ↔x1) · Z[ˆx := ˆx′])[0/Z].
Since [0/Z] is a meta-operation, the latter formula is just
(x1 · x2) + ∀x′
1.∀x′
2.(x′
1 ↔x1 · x2 · u) · (x′
2 ↔x1) · 0[ˆx := ˆx′].
Thus, we need to evaluate the disjunction (x1 · x2) + ∀x′
1.∀x′
2.(x′
1 ↔x1 · x2 ·
u) · (x′
2 ↔x1) · 0[ˆx := ˆx′] at ρ. In particular, if ρ(x1) = 0 and ρ(x2) = 0,
then ρ ⊨x1 · x2 and so ρ ⊨(x1 · x2) + ∀x′
1.∀x′
2.(x′
1 ↔x1 · x2 · u) · (x′
2 ↔x1) ·
0[ˆx := ˆx′]. Thus, s2 ⊨AF (¬x1 ∧¬x2) holds.
Similar reasoning establishes that the formula in (6.21) renders a correct
coding for the remaining two states as well, which you are invited to verify
as an exercise.
Symbolic model checking with fairness
In Chapter 3, we sketched
how SMV could use fairness assumptions which were not expressible entirely
5 Since we have added the variable u, there are actually six states; they all satisfy the formula.

6.4 A relational mu-calculus
397
within CTL and its semantics. The addition of fairness could be achieved
by restricting the ordinary CTL semantics to fair computation paths, or fair
states. Formally, we were given a set C = {ψ1, ψ2, . . . , ψk} of CTL formulas,
called the fairness constraints, and we wanted to check whether s ⊨φ holds
for a CTL formula φ and all initial states s, with the additional fairness
constraints in C. Since ⊥, ¬, ∧, EX, EU and EG form an adequate set of
connectives for CTL, we may restrict this discussion to only these operators.
Clearly, the propositional connectives won’t change their meaning with the
addition of fairness constraints. Therefore, it suﬃces to provide symbolic
codings for the fair connectives ECX, ECU and ECG from Chapter 3. The
key is to represent the set of fair states symbolically as a boolean formula
fair deﬁned as
fair
def
= fECG⊤
(6.22)
which uses the (yet to be deﬁned) function fECG φ with ⊤as an instance.
Assuming that the coding of fECG φ is correct, we see that fair computes 1
in a state s if, and only if, there is a fair path with respect to C that begins
in s. We say that such an s is a fair state.
As for ECX, note that s ⊨ECXφ if, and only if, there is some next state s′
with s →s′ and s′ ⊨φ such that s′ is a fair state. This immediately renders
fECXφ def
= ∃ˆx′.(f→· (fφ · fair)[ˆx := ˆx′]).
(6.23)
Similarly, we obtain
fEC[φ1Uφ2] def
= µZ. (fφ2 · fair + fφ1 · ∃ˆx′. (f→· Z[ˆx := ˆx′])).
(6.24)
This leaves us with the task of coding fECG φ. It is this last connective
which reveals the complexity of fairness checks at work. Because the coding
of fECG φ is rather complex, we proceed in steps. It is convenient to have the
EX and EU functionality also at the level of boolean formulas directly. For
example, if f is a boolean function in ˆx, then checkEX (f) codes the boolean
formula which computes 1 for those vectors ˆx which have a next state ˆx′ for
which f computes 1:
checkEX (f)
def
= ∃ˆx′.(f→· f[ˆx := ˆx′]).
(6.25)
Thus, fECXφ equals checkEX (fφ · fair). We proceed in the same way for
functions f and g in n arguments ˆx to obtain checkEU (f, g) which computes

398
6 Binary decision diagrams
1 at ˆx if there is a path that realises the f U g pattern:
checkEU (f, g)
def
= µY.g + (f · checkEX(Y )).
(6.26)
With this in place, we can code fECGφ quite easily:
fECGφ def
= νZ.fφ ·
k

i=1
checkEX (checkEU (fφ, Z · fψi) · fair).
(6.27)
Note that this coding has a least ﬁxed point (checkEU) in the body of a
greatest ﬁxed point. This is computationally rather involved since the call of
checkEU contains Z, the recursion variable of the outer greatest ﬁxed point,
as a free variable; thus these recursions are nested and inter-dependent;
the recursions ‘alternate.’ Observe how this coding operates: to have a fair
path from ˆx on which φ holds globally, we need φ to hold at ˆx; and for
all fairness constraints ψi there has to be a next state ˆx′, where the whole
property is true again (enforced by the free Z) and each fairness constraint
is realised eventually on that path. The recursion in Z constantly reiterates
this reasoning, so if this function computes 1, then there is a path on which
φ holds globally and where each ψi is true inﬁnitely often.
6.5 Exercises
Exercises 6.1
1. Write down the truth tables for the boolean formulas in Example 6.2 on page 359.
In your table, you may use 0 and 1, or F and T, whatever you prefer. What truth
value does the boolean formula of item (4) on page 359 compute?
2. ⊕is the exclusive-or: x ⊕y
def
= 1 if the values of x and y are diﬀerent; otherwise,
x ⊕y
def
= 0. Express this in propositional logic, i.e. ﬁnd a formula φ having the
same truth table as ⊕.
3.
*
Write down a boolean formula f(x, y) in terms of ·, +, ¯, 0 and 1, such that f
has the same truth table as p →q.
4. Write down a BNF for the syntax of boolean formulas based on the operations
in Deﬁnition 6.1.
Exercises 6.2
1.
*
Suppose we swap all dashed and solid lines in the binary decision tree of Fig-
ure 6.2. Write out the truth table of the resulting binary decision tree and ﬁnd
a formula for it.

6.5 Exercises
399
2.
*
Consider the following truth table:
p
q
r
φ
T
T
T
T
T
T
F
F
T
F
T
F
T
F
F
F
F
T
T
T
F
T
F
F
F
F
T
T
F
F
F
F
Write down a binary decision tree which represents the boolean function speciﬁed
in this truth table.
3. Construct a binary decision tree for the boolean function speciﬁed in Figure 6.2,
but now the root should be a y-node and its two successors should be x-nodes.
4. Consider the following boolean function given by its truth table:
x
y
z
f(x, y, z)
1
1
1
0
1
1
0
1
1
0
1
0
1
0
0
1
0
1
1
0
0
1
0
0
0
0
1
0
0
0
0
1
(a) Construct a binary decision tree for f(x, y, z) such that the root is an x-node
followed by y- and then z-nodes.
(b) Construct another binary decision tree for f(x, y, z), but now let its root be
a z-node followed by y- and then x-nodes.
5. Let T be a binary decision tree for a boolean function f(x1, x2, . . . , xn) of n
boolean variables. Suppose that every variable occurs exactly once as one travels
down on any path of the tree T. Use mathematical induction to show that T has
2n+1 −1 nodes.
Exercises 6.3
1.
*
Explain why all reductions C1–C3 (page 363) on a BDD B result in BDDs which
still represent the same function as B.
2. Consider the BDD in Figure 6.7.
(a)
*
Specify the truth table for the boolean function f(x, y, z) represented by
this BDD.

400
6 Binary decision diagrams
(b) Find a BDD for that function which does not have multiple occurrences of
variables along any path.
3. Let f be the function represented by the BDD of Figure 6.3(b). Using also the
BDDs B0, B1 and Bx illustrated in Figure 6.6, ﬁnd BDDs representing
(a) f · x
(b) x + f
(c) f · 0
(d) f · 1.
Exercises 6.4
1. Figure 6.9 (page 367) shows a BDD with ordering [x, y, z].
(a)
*
Find an equivalent reduced BDD with ordering [z, y, x]. (Hint: ﬁnd ﬁrst the
decision tree with the ordering [z, y, x], and then reduce it using C1–C3.)
(b) Carry out the same construction process for the variable ordering [y, z, x].
Does the reduced BDD have more or fewer nodes than the ones for the
orderings [x, y, z] and [z, y, x]?
2. Consider the BDDs in Figures 6.4–6.10. Determine which of them are OBDDs.
If you ﬁnd an OBDD, you need to specify a list of its boolean variables without
double occurrences which demonstrates that ordering.
3. Consider the following boolean formulas. Compute their unique reduced OBDDs
with respect to the ordering [x, y, z]. It is advisable to ﬁrst compute a binary
decision tree and then to perform the removal of redundancies.
(a) f(x, y)
def
= x · y
(b)
*
f(x, y)
def
= x + y
(c) f(x, y)
def
= x ⊕y
(d)
*
f(x, y, z)
def
= (x ⊕y) · (x + z).
4. Recall the derived connective φ ↔ψ from Chapter 1 saying that for all valuations
φ is true if, and only if, ψ is true.
(a) Deﬁne this operator for boolean formulas using the basic operations ·, +, ⊕
and ¯ from Deﬁnition 6.1.
(b) Draw a reduced OBDD for the formula g(x, y)
def
= x ↔y using the ordering
[y, x].
5. Consider the even parity function introduced at the end of the last section.
(a) Deﬁne the odd parity function fodd(x1, x2, . . . , xn).
(b) Draw an OBDD for the odd parity function for n = 5 and the ordering
[x3, x5, x1, x4, x2]. Would the overall structure of this OBDD change if you
changed the ordering?
(c) Show that feven(x1, x2, . . . , xn) and fodd(x1, x2, . . . , xn) denote the same
boolean function.
6. Use Theorem 6.7 (page 368) to show that, if the reductions C1–C3 are applied
until no more reduction is possible, the result is independent of the order in
which they were applied.

6.5 Exercises
401
Exercises 6.5
1. Given the boolean formula f(x1, x2, x3)
def
= x1 · (x2 + x3), compute its reduced
OBDD for the following orderings:
(a) [x1, x2, x3]
(b) [x3, x1, x2]
(c) [x3, x2, x1].
2. Compute the reduced OBDD for f(x, y, z) = x · (z + z) + y · x in any ordering
you like. Is there a z-node in that reduced OBDD?
3. Consider the boolean formula f(x, y, z)
def
= (x + y + z) · (x + y + z) · (x + y). For
the variable orderings below, compute the (unique) reduced OBDD Bf of f with
respect to that ordering. It is best to write down the binary decision tree for that
ordering and then to apply all possible reductions.
(a) [x, y, z].
(b) [y, x, z].
(c) [z, x, y].
(d) Find an ordering of variables for which the resulting reduced OBDD Bf has a
minimal number of edges; i.e. there is no ordering for which the corresponding
Bf has fewer edges. (How many possible orderings for x, y and z are there?)
4. Given the truth table
x
y
z
f(x, y, z)
1
1
1
0
1
1
0
1
1
0
1
1
1
0
0
0
0
1
1
0
0
1
0
1
0
0
1
0
0
0
0
1
compute the reduced OBDD with respect to the following ordering of variables:
(a) [x, y, z]
(b) [z, y, x]
(c) [y, z, x]
(d) [x, z, y].
5. Given the ordering [p, q, r], compute the reduced BDDs for p ∧(q ∨r) and (p ∧
q) ∨(p ∧r) and explain why they are identical.
6.
*
Consider the BDD in Figure 6.11 (page 370).
(a) Construct its truth table.
(b) Compute its conjunctive normal form.
(c) Compare the length of that normal form with the size of the BDD. What is
your assessment?

402
6 Binary decision diagrams
Exercises 6.6
1. Perform the execution of reduce on the following OBDDs:
(a) The binary decision tree for
i. x ⊕y
ii. x · y
iii. x + y
iv. x ↔y.
(b) The OBDD in Figure 6.2 (page 361).
(c)
*
The OBDD in Figure 6.4 (page 363).
Exercises 6.7
1. Recall the Shannon expansion in (6.1) on page 374. Suppose that x does not
occur in f at all. Why does (6.1) still hold?
2. Let f(x, y, z)
def
= y + z · x + z · y + y · x be a boolean formula. Compute f’s
Shannon expansion with respect to
(a) x
(b) y
(c) z.
3. Show that boolean formulas f and g are semantically equivalent if, and only if,
the boolean formula (f + g) · (f + g) computes 1 for all possible assignments of
0s and 1s to their variables.
4. We may use the Shannon expansion to deﬁne formally how BDDs determine
boolean functions. Let B be a BDD. It is intuitively clear that B determines
a unique boolean function. Formally, we compute a function fn inductively
(bottom-up) for all nodes n of B:
– If n is a terminal node labelled 0, then fn is the constant 0 function.
– Dually, if n is a terminal 1-node, then fn is the constant 1 function.
– If n is a non-terminal node labelled x, then we already have deﬁned the
boolean functions flo(n) and fhi(n) and set fn to be x · flo(n) + x · fhi(n).
If i is the initial node of B, then fi is the boolean function represented by
B. Observe that we could apply this deﬁnition as a symbolic evaluation of B
resulting in a boolean formula. For example, the BDD of Figure 6.3(b) renders
x · (y · 1 + y · 0) + x · 0. Compute the boolean formulas obtained in this way for
the following BDDs:
(a) the BDD in Figure 6.5(b) (page 364)
(b) the BDDs in Figure 6.6 (page 365)
(c) the BDD in Figure 6.11 (page 370).
5.
*
Consider a ternary (= takes three arguments) boolean connective f →(g, h)
which is equivalent to g when f is true; otherwise, it is equivalent to h.
(a) Deﬁne this connective using any of the operators +, ·, ⊕or ¯.
(b) Recall exercise 4. Use the ternary operator above to write fn as an expres-
sion of flo(n), fhi(n) and its label x.

6.5 Exercises
403
0
1
z
x
y
y
z
y
z
x
0
1
Figure 6.30. The reduced OBDDs Bf and Bg (see exercises).
(c) Use mathematical induction (on what?) to prove that, if the root of fn is
an x-node, then fn is independent of any y which comes before x in an
assumed variable ordering.
6. Explain why apply (op, Bf, Bg), where Bf and Bg have compatible ordering,
produces an OBDD with an ordering compatible with that of Bf and Bg.
7. Explain why the four cases of the control structure for apply are exhaustive,
i.e. there are no other possible cases in its execution.
8. Consider the reduced OBDDs Bf and Bg in Figure 6.30. Recall that, in order
to compute the reduced OBDD for f op g, you need to
– construct the tree showing the recursive descent of apply (op, Bf, Bg) as
done in Figure 6.16;
– use that tree to simulate apply (op, Bf, Bg); and
– reduce, if necessary, the resulting OBDD.
Perform these steps on the OBDDs of Figure 6.30 for the operation ‘op’ being
(a) +
(b) ⊕
(c) ·
9. Let Bf be the OBDD in Figure 6.11 (page 370). Compute apply (⊕, Bf, B1) and
reduce the resulting OBDD. If you did everything correctly, then this OBDD
should be isomorphic to the one obtained from swapping 0- and 1-nodes in
Figure 6.11.
10.
*
Consider the OBDD Bc in Figure 6.31 which represents the ‘don’t care’ condi-
tions for comparing the boolean functions f and g represented in Figure 6.30.
This means that we want to compare whether f and g are equal for all values
of variables except those for which c is true (i.e. we ‘don’t care’ when c is true).
(a) Show that the boolean formula (f ⊕g) + c is valid (always computes 1)
if, and only if, f and g are equivalent on all values for which c evaluates
to 0.

404
6 Binary decision diagrams
0
1
z
y
y
x
Figure 6.31. The reduced OBDD Bc representing the ‘don’t care’ con-
ditions for the equivalence test of the OBDDs in Figure 6.30.
(b) Proceed in three steps as in exercise 8 on page 403 to compute the reduced
OBDD for (f ⊕g) + c from the OBDDs for f, g and c. Which call to apply
needs to be ﬁrst?
11. We say that v ∈{0, 1} is a (left)-controlling value for the operation op, if either
v op x = 1 or v op x = 0 for all values of x. We say that v is a controlling value
if it is a left- and right-controlling value.
(a) Deﬁne the notion of a right-controlling value.
(b) Give examples of operations with controlling values.
(c) Describe informally how apply can be optimised when op has a controlling
value.
(d) Could one still do some optimisation if op had only a left- or right-controlling
value?
12. We showed that the worst-time complexity of apply is O(|Bf| · |Bg|). Show that
this upper bound is hard, i.e. it cannot be improved:
(a) Consider the functions f(x1, x2, . . . , x2n+2m)
def
= x1 · xn+m+1 + · · · + xn ·
x2n+m and g(x1, x2, . . . , x2n+2m)
def
= xn+1 · x2n+m+1 + · · · + xn+m · x2n+2m
which are in sum-of-product form. Compute the sum-of-product form of
f + g.
(b) Choose the ordering [x1, x2, . . . , x2n+2m] and argue that the OBDDs Bf
and Bg have 2n+1 and 2m+1 edges, respectively.
(c) Use the result from part (a) to conclude that Bf+g has 2n+m+1 edges, i.e.
0.5 · |Bf| · |Bg|.
Exercises 6.8
1. Let f be the reduced OBDD represented in Figure 6.5(b) (page 364). Compute
the reduced OBDD for the restrictions:
(a) f[0/x]
(b)
*
f[1/x]

6.5 Exercises
405
(c) f[1/y]
(d)
*
f[0/z].
2.
*
Suppose that we intend to modify the algorithm restrict so that it is capable
of computing reduced OBDDs for a general composition f[g/x].
(a) Generalise Equation (6.1) to reﬂect the intuitive meaning of the operation
[g/x].
(b) What fact about OBDDs causes problems for computing this composition
directly?
(c) How can we compute this composition given the algorithms discussed so far?
3. We deﬁne read-1-BDDs as BDDs B where each boolean variable occurs at most
once on any evaluation path of B. In particular, read-1-BDDs need not possess
an ordering on their boolean variables. Clearly, every OBDD is a read-1-BDD;
but not every read-1-BDD is an OBDD (see Figure 6.10). In Figure 6.18 we see
a BDD which is not a read-1-BDD; the path for (x, y, z) ⇒(1, 0, 1) ‘reads’ the
value of x twice.
Critically assess the implementation of boolean formulas via OBDDs to see which
implementation details could be carried out for read-1-BDDs as well. Which
implementation aspects would be problematic?
4. (For those who have had a course on ﬁnite automata.) Every boolean function
f in n arguments can be viewed as a subset Lf of {0, 1}n; deﬁned to be the
set of all those bit vectors (v1, v2, . . . , vn) for which f computes 1. Since this
is a ﬁnite set, Lf is a regular language and has therefore a deterministic ﬁnite
automaton with a minimal number of states which accepts Lf. Can you match
some of the OBDD operations with those known for ﬁnite automata? How close
is the correspondence? (You may have to consider non-reduced OBDDs.)
5. (a) Show that every boolean function in n arguments can be represented as a
boolean formula of the grammar
f ::= 0 | x | f | f1 + f2.
(b) Why does this also imply that every such function can be represented by a
reduced OBDD in any variable ordering?
6. Use mathematical induction on n to prove that there are exactly 2(2n) many
diﬀerent boolean functions in n arguments.
Exercises 6.9
1. Use the exists algorithm to compute the OBDDs for
(a) ∃x3.f, given the OBDD for f in Figure 6.11 (page 370)
(b) ∀y.g, given the OBDD for g in Figure 6.9 (page 367)
(c) ∃x2.∃x3.x1 · y1 + x2 · y2 + x3 · y3.
2. Let f be a boolean function depending on n variables.
(a) Show:

406
6 Binary decision diagrams
i. The formula ∃x.f depends on all those variables that f depends upon,
except x.
ii. If f computes to 1 with respect to a valuation ρ, then ∃x.f computes 1
with respect to the same valuation.
iii. If ∃x.f computes to 1 with respect to a valuation ρ, then there is a valuation
ρ′ for f which agrees with ρ for all variables other than x such that f
computes to 1 under ρ′.
(b) Can the statements above be shown for the function value 0?
3. Let φ be a boolean formula.
(a)
*
Show that φ is satisﬁable if, and only if, ∃x.φ is satisﬁable.
(b) Show that φ is valid if, and only if, ∀x.φ is valid.
(c) Generalise the two facts above to nested quantiﬁcations ∃ˆx and ∀ˆx. (Use
induction on the number of quantiﬁed variables.)
4. Show that ∀ˆx.f and ∃ˆx.f are semantically equivalent. Use induction on the
number of arguments in the vector ˆx.
Exercises 6.10
(For those who know about complexity classes.)
1. Show that 3SAT can be reduced to nested existential boolean quantiﬁcation.
Given an instance of 3SAT, we may think of it as a boolean formula f in product-
of-sums form g1 · g2 · · · · · gn, where each gi is of the form (l1 + l2 + l3) with each
lj being a boolean variable or its complementation. For example, f could be
(x + y + z) · (x5 + x + x7) · (x2 + z + x) · (x4 + x2 + x4).
(a) Show that you can represent each function gi with an OBDD of no more
than three non-terminals, independently of the chosen ordering.
(b) Introduce n new boolean variables z1, z2, . . . , zn. We write 
1≤i≤n fi for
the expression f1 + f2 + · · · + fn and 
1≤i≤n fi for f1 · f2 · · · · · fn. Consider
the boolean formula h, deﬁned as

1≤i≤n

gi · zi ·

1≤j<i
zj

.
(6.28)
Choose any ordering of variables whose list begins as in [z1, z2, . . . , zn, . . . ].
Draw the OBDD for h (draw only the root nodes for gi).
(c) Argue that the OBDD above has at most 4n non-terminal nodes.
(d) Show that f is satisﬁable if, and only if, the OBDD for ∃z1.∃z2. . . . .∃zn.h is
not equal to B1.
(e) Explain why the last item shows a reduction of 3SAT to nested existential
quantiﬁcation.
2. Show that the problem of ﬁnding an optimal ordering for representing boolean
functions as OBDDs is in coNP.

6.5 Exercises
407
x1
x2
x2
x1
s0
s3
s1
s2
x2
s1
s2
x1
s0
Figure 6.32. (a) A CTL model with four states. (b) A CTL model with
three states.
3. Recall that ∃x.f is deﬁned as f[1/x] + f[0/x]. Since we have eﬃcient algorithms
for restriction and +, we obtain hereby an eﬃcient algorithm for ∃z1. . . . ∃zn.f.
Thus, P equals NP! What is wrong with this argument?
Exercises 6.11
1.
*
Consider the CTL model in Figure 6.32(a). Using the ordering [x1, x2], draw the
OBDD for the subsets {s0, s1} and {s0, s2}.
2. Consider the CTL model in Figure 6.32(b). Because the number of states is not
an exact power of 2, there are more than one OBDDs representing any given
set of states. Using again the ordering [x1, x2], draw all possible OBDDs for the
subsets {s0, s1} and {s0, s2}.
Exercises 6.12
1. Consider the CTL model in Figure 6.32(a).
(a) Work out the truth table for the transition relation, ordering the columns
[x1, x′
1, x2, x′
2]. There should be as many 1s in the ﬁnal column as there are
arrows in the transition relation. There is no freedom in the representation
in this case, since the number of states is an exact power of 2.
(b) Draw the OBDD for this transition relation, using the variable ordering
[x1, x′
1, x2, x′
2].
2. Apply the algorithm of Section 3.6.1, but now interpreted over OBDDs in the
ordering [x1, x2], to compute the set of states of the CTL model in Figure 6.32(b)
which satisfy
(a) AG (x1 ∨¬x2)
(b) E[x2 U x1].
Show the OBDDs which are computed along the way.
3. Explain why exists(ˆx′, apply(·, B→, BX′)) faithfully implements the meaning
of pre∃(X).

408
6 Binary decision diagrams
x1
x2
x3
Figure 6.33. A synchronous circuit for a modulo 8 counter.
Exercises 6.13
1. (a) Simulate the evolution of the circuit in Figure 6.29 (page 389) with initial
state 01. What do you think that it computes?
(b) Write down the explicit CTL model (S, →, L) for this circuit.
2. Consider the sequential synchronous circuit in Figure 6.33.
(a) Construct the functions fi for i = 1, 2, 3.
(b) Code the function f →.
(c) Recall from Chapter 2 that (∃x.φ) ∧ψ is semantically equivalent to ∃x.(φ ∧
ψ) if x is not free in ψ.
i. Why is this also true in our setting of boolean formulas?
ii. Apply this law to push the ∃quantiﬁcations in f →as far inwards as possi-
ble. This is an often useful optimisation in checking synchronous circuits.
3. Consider the boolean formula for the 2-bit comparator:
f(x1, x2, y1, y2)
def
= (x1 ↔y1) · (x2 ↔y2).
(a) Draw its OBDD for the ordering [x1, y1, x2, y2].
(b) Draw its OBDD for the ordering [x1, x2, y1, y2] and compare that with the
one above.
4. (a) Can you use (6.6) from page 388 to code the transition relation →of the
model in Figure 6.24 on page 384?
(b) Can you do it with equation (6.9) from page 390?
(c) With equation (6.8) from page 389?

6.5 Exercises
409
Exercises 6.14
1. Let ρ be the valuation for which (x, y, z) ⇒(0, 1, 1). Compute whether ρ ⊨f
holds for the following boolean formulas:
(a) x · (y + z · (y ⊕x))
(b) ∃x.(y · (x + z + y) + x · y)
(c) ∀x.(y · (x + z + y) + x · y)
(d) ∃z.(x · z + ∀x.((y + (x + x) · z)))
(e)
*
∀x.(y + z).
2.
*
Use (6.14) from page 393 and the deﬁnition of the satisfaction relation for for-
mulas of the relational mu-calculus to prove ρ ⊨νZ.Z for all valuations ρ. In
this case, f equals Z and you need to show (6.14) by mathematical induction on
m ≥0.
3. An implementation which decides ⊨and ̸⊨for the relational mu-calculus ob-
viously cannot represent valuations which assign semantic values 0 or 1 to all,
i.e. inﬁnitely many variables. Thus, it makes sense to consider ⊨as a relation
between pairs (ρ, f), where ρ only assigns semantic values to all free variables
of f.
(a) Assume that νZ and µZ, ∃x, ∀x, and [ˆx := ˆx′] are binding constructs similar
to the quantiﬁers in predicate logic. Deﬁne formally the set of free variables
for a formula f of the relational mu-calculus. (Hint: You should deﬁne this
by structural induction on f. Also, which variables get bound in f[ˆx := ˆx′]?)
(b) Recall the notion of t being free for x in φ which we discussed in Section 2.2.4
Deﬁne what ‘g is free for Z in f’ should mean and ﬁnd an example, where g
is not free for Z in f.
(c) Explain informally why we can decide whether ρ ⊨f holds, provided that ρ
assigns values 0 or 1 to all free variables of f. Explain why this answer will
be independent of what ρ does to variables which are bound in f. Why is
this relevant for an implementation framework?
4. Let ρ be the valuation for which (x, x′, y, y′) ⇒(0, 1, 1, 1). Determine whether ρ ⊨
f holds for the following formulas f (recall that we write f ↔g as an abbreviation
for f ⊕g, meaning that f computes 1 iﬀg computes 1):
(a) ∃x.(x′ ↔(y + y′ · x))
(b) ∀x.(x′ ↔(y + y′ · x))
(c) ∃x′.(x′ ↔(y + y′ · x))
(d) ∀x′.(x′ ↔(y + y′ · x)).
5. Let ρ be a valuation with ρ(x′
1) = 1 and ρ(x′
2) = 0. Determine whether ρ ⊨f
holds for the following:
(a) x1[ˆx := ˆx′]
(b) (x1 + x2)[ˆx := ˆx′]
(c) (x1 · x2)[ˆx := ˆx′].
6. Evaluate ρ ⊨(∃x1.(x1 + x2))[ˆx := ˆx′] and explain how the valuation ρ changes
in that process. In particular, [ˆx := ˆx′] replaces xi by x′
i, but why does this not
interfere with the binding quantiﬁer ∃x1?

410
6 Binary decision diagrams
7. (a) How would you deﬁne the notion of semantic entailment for the relational
mu-calculus?
(b) Deﬁne formally when two formulas of the relational mu-calculus are seman-
tically equivalent.
Exercises 6.15
1. Using the model of Figure 6.24 (page 384), determine whether ρ ⊨f EX (x1∨¬x2)
holds, where ρ is
(a) (x1, x2) ⇒(1, 0)
(b) (x1, x2) ⇒(0, 1)
(c) (x1, x2) ⇒(0, 0).
2. Let S be {s0, s1}, with s0 →s0, s0 →s1 and s1 →s0 as possible transitions
and L(s0) = {x1} and L(s1) = ∅. Compute the boolean function f EX (EX ¬x1).
3. Equations (6.17) (page 395), (6.19) and (6.20) deﬁne f EF φ, f AF φ and f EG φ.
Write down a similar equation to deﬁne f AG φ.
4. Deﬁne a direct coding f AU φ by modifying (6.18) appropriately.
5. Mimic the example checks on page 396 for the connective AU: consider the
model of Figure 6.24 (page 384). Since [[E[(x1 ∨x2) U (¬x1 ∧¬x2)]]] equals the
entire state set {s0, s1, s2}, your coding of f E[x1∨x2U¬x1∧¬x2] is correct if it
computes 1 for all bit vectors diﬀerent from (1, 1).
(a) Verify that your coding is indeed correct.
(b) Find a boolean formula without ﬁxed points which is semantically equiva-
lent to f E[(x1∨x2)U(¬x1∧¬x2)].
6. (a) Use (6.20) on page 395 to compute f EG ¬x1 for the model in Figure 6.24.
(b) Show that f EG ¬x1 faithfully models the set of all states which satisfy
EG ¬x1.
7. In the grammar (6.10) for the relational mu-calculus on page 390, it was stated
that, in the formulas µZ.f and νZ.f, any occurrence of Z in f is required to
fall within an even number of complementation symbols ¯. What happens if we
drop this requirement?
(a) Consider the expression µZ.Z. We already saw that our relation ρ is total in
the sense that either ρ ⊨f or ρ ̸⊨f holds for all choices of valuations ρ and
relational mu-calculus formulas f. But formulas like µZ.Z are not formally
monotone. Let ρ be any valuation. Use mutual mathematical induction to
show:
i. ρ ̸⊨µmZ.Z for all even numbers m ≥0
ii. ρ ⊨µmZ.Z for all odd numbers m ≥1
Infer from these two items that ρ ⊨µZ.Z holds according to (6.12).
(b) Consider any environment ρ. Use mathematical induction on m (and maybe
an analysis on ρ) to show:
If
ρ ⊨µmZ.(x1 + x2 · Z)
for
some
m ≥0,
then
ρ ⊨
µkZ.(x1 + x2 · Z) for all k ≥m.

6.5 Exercises
411
(c) In general, if f is formally monotone in Z then ρ ⊨µmZ.f implies
ρ ⊨µm+1Z.f. Can you state a similar property for the greatest ﬁxed-point
operator ν?
8. Given the CTL model for the circuit in Figure 6.29 (page 389):
(a)
*
code the function f EX (x1∧¬x2)
(b) code the function f AG (AF ¬x1∧¬x2)
(c)
*
ﬁnd a boolean formula without any ﬁxed points which is semantically equiv-
alent to f AG (AF ¬x1∧¬x2).
9. Consider the sequential synchronous circuit in Figure 6.33 (page 408). Evaluate
ρ ⊨f EX x2, where ρ equals
(a) (x1, x2, x3) ⇒(1, 0, 1)
(b) (x1, x2, x3) ⇒(0, 1, 0).
10. Prove
Theorem 6.19 Given a coding for a ﬁnite CTL model, let φ be a CTL formula
from an adequate fragment. Then [[φ]] corresponds to the set of valuations ρ such
that ρ ⊨f φ.
by structural induction on φ. You may ﬁrst want to show that the evaluation of
ρ ⊨f φ depends only on the values ρ(xi), i.e. it does not matter what ρ assigns
to x′
i or Z.
11. Argue that Theorem 6.19 above remains valid for arbitrary CTL formulas as
long as we translate formulas φ which are not in the adequate fragment into
semantically equivalent formulas ψ in that fragment and deﬁne f φ to be f ψ.
12. Derive the formula f AF (¬x1∧x2) for the model in Figure 6.32(b) on page 407
and evaluate it for the valuation corresponding to state s2 to determine whether
s2 ⊨AF (¬x1 ∧x2) holds.
13. Repeat the last exercise with f E[x1∨¬x2Ux1].
14. Recall the way the two labelling algorithms operate in Chapter 3. Does our
symbolic coding mimic either or both of them, or neither?
Exercises 6.16
1. Consider the equations in (6.22) and (6.27). The former deﬁnes fair in terms of
f ECG⊤, whereas the latter deﬁnes f ECG φ for general φ. Why is this unproblem-
atic, i.e. non-circular?
2. Given a ﬁxed CTL model M = (S, →, L), we saw how to code formulas f φ
representing the set of states s ∈S with s ⊨φ, φ being a CTL formula of an
adequate fragment.
(a) Assume the coding without consideration of simple fairness constraints. Use
structural induction on the CTL formula φ to show that
i. the free variables of f φ are among ˆx, where the latter is the vector of
boolean variables which code states s ∈S;
ii. all ﬁxed-point subformulas of f φ are formally monotone.

412
6 Binary decision diagrams
(b) Show these two assertions if f φ also encodes simple fairness constraints.
3. Consider the pseudo-code for the function SAT on page 227. We now want to
modify it so that the resulting output is not a set, or an OBDD, but a formula
of the relational mu-calculus; thus, we complete the table in Figure 6.22 on
page 380 to give formulas of the relational mu-calculus. For example, the output
for ⊤would be 1 and the output for EU ψ would be a recursive call to SAT
informed by (6.18). Do you have a need for a separate function which handles
least or greatest ﬁxed points?
4. (a) Write pseudo-code for a function SATrel mu which takes as input a formula
of the relational mu-calculus, f, and synthesises an OBDD Bf, represent-
ing f. Assume that there are no ﬁxed-point subexpressions of f such that
their recursive body contains a recursion variable of an outwards ﬁxed point.
Thus, the formula in (6.27) is not allowed. The ﬁxed-point operators µ and
ν require separate subfunctions which iterate the ﬁxed-point meaning in-
formed by (6.12), respectively (6.14). Some of your clauses may need further
comment. E.g. how do you handle the constructor [ˆx := ˆx′]?
(b) Explain what goes wrong if the input to your code is the formula in (6.27).
5. If f is a formula with a vector of n free boolean variables ˆx, then the iteration of
µZ.f, whether as OBDD implementation, or as in (6.12), may require up to 2n
recursive unfoldings to compute its meaning. Clearly, this is unacceptable. Given
the symbolic encoding of a CTL model M = (S, →, L) and a set I ⊆S of initial
states, we seek a formula that represents all states which are reachable from I on
some ﬁnite computation path in M. Using the extended Until operator in (6.26),
we may express this as checkEU (f I, ⊤), where f I is the characteristic function
of I. We can ‘speed up’ this iterative process with a technique called ‘iterative
squaring’:
µY.(f →+ ∃ˆw.(Y [ˆx′ := ˆw] · Y [ˆx := ˆw])).
(6.29)
Note that this formula depends on the same boolean variables as f →, i.e. the
pair (ˆx, ˆx′). Explain informally:
If we apply (6.12) m times to the formula in (6.29), then this
has the same semantic ‘eﬀect’ as applying this rule 2m times to
checkEU (f →, ⊤).
Thus, one may ﬁrst compute the set of states reachable from any initial state
and then restrict model checking to those states. Note that this reduction does
not alter the semantics of s ⊨φ for initial states s, so it is a sound technique;
it sometimes improves, other times worsens, the performance of symbolic model
checks.

6.6 Bibliographic notes
413
6.6 Bibliographic notes
Ordered binary decision diagrams are due to R. E. Bryant [Bry86]. Binary
decision diagrams were introduced by C. Y. Lee [Lee59] and S. B. Akers
[Ake78]. For a nice survey of these ideas see [Bry92]. For the limitations
of OBDDs as models for integer multiplication as well as interesting con-
nections to VLSI design see [Bry91]. A general introduction to the topic of
computational complexity and its tight connections to logic can be found
in [Pap94]. The modal mu-calculus was invented by D. Kozen [Koz83]; for
more on that logic and its application to speciﬁcations and veriﬁcation see
[Bra91].
The use of BDDs in model checking was proposed by the team of au-
thors J. R. Burch, E. M. Clarke, K. L. McMillan, D. L. Dill and J. Hwang
[BCM+90, CGL93, McM93].

Bibliography
Ake78. S. B. Akers. Binary decision diagrams. IEEE Transactions on
Computers, C-27(6):509–516, 1978.
AO91. K. R. Apt and E.-R. Olderog. Veriﬁcation of Sequential and
Concurrent Programs. Springer-Verlag, 1991.
Bac86. R. C. Backhouse. Program Construction and Veriﬁcation. Prentice
Hall, 1986.
BCCZ99. A. Biere, A. Cimatti, E. Clarke, and Y. Zhu. Symbolic model checking
without BDDs. In Proceedings of Tools and Algorithms for the
Analysis and Construction of Systems (TACAS’99), volume 1579 of
Lecture Notes in Computer Science, pages 193–207, 1999.
BCM+90. J. R. Burch, J. M. Clarke, K. L. McMillan, D. L. Dill, and J. Hwang.
Symbolic model checking: 1020 states and beyond. In IEEE Symposium
on Logic in Computer Science. IEEE Computer Society Press, 1990.
BEKV94. K. Broda, S. Eisenbach, H. Khoshnevisan, and S. Vickers. Reasoned
Programming. Prentice Hall, 1994.
BJ80. G. Boolos and R. Jeﬀrey. Computability and Logic. Cambridge
University Press, 2nd edition, 1980.
Boo54. G. Boole. An Investigation of the Laws of Thought. Dover, New York,
1854.
Bra91. J. C. Bradﬁeld. Verifying Temporal Properties of Systems. Birkh¨auser,
Boston, 1991.
Bry86. R. E. Bryant. Graph-based algorithms for boolean function
manipulation. IEEE Transactions on Compilers, C-35(8), 1986.
Bry91. R. E. Bryant. On the Complexity of VLSI Implementations and Graph
Representations of Boolean Functions with Applications to Integer
Multiplication. IEEE Transactions on Computers, 40(2):205–213,
February 1991.
Bry92. R. E. Bryant. Symbolic Boolean Manipulation with Ordered
Binary-decision Diagrams. ACM Computing Surveys, 24(3):293–318,
September 1992.
CE81. E. M. Clarke and E. A. Emerson. Synthesis of synchronization
skeletons for branching time temporal logic. In D. Kozen, editor,
Logic of Programs Workshop, number 131 in LNCS. Springer Verlag,
1981.
414

Bibliography
415
CGL93. E. Clarke, O. Grumberg, and D. Long. Veriﬁcation tools for
ﬁnite-state concurrent systems. In A Decade of Concurrency, number
803 in Lecture Notes in Computer Science, pages 124–175. Springer
Verlag, 1993.
CGL94. E. M. Clarke, O. Grumberg, and D. E. Long. Model checking and
Abstraction. ACM Transactions on Programming Languages and
Systems, 16(5):1512–1542, September 1994.
CGP99. E. M. Clarke, O. Grumberg, and D. A. Peled. Model Checking. MIT
Press, 1999.
Che80. B. F. Chellas. Modal Logic – an Introduction. Cambridge University
Press, 1980.
Dam96. D. R. Dams. Abstract Interpretation and Partition Reﬁnement for
Model Checking. PhD thesis, Institute for Programming Research and
Algorithmics. Eindhoven University of Technology, July 1996.
Dij76. E. W. Dijkstra. A Discipline of Programming. Prentice Hall, 1976.
DP96. R. Davies and F. Pfenning. A Modal Analysis of Staged Computation.
In 23rd Annual ACM Symposium on Principles of Programming
Languages. ACM Press, January 1996.
EJC03. S. Eisenbach, V. Jurisic, and C. Sadler. Modeling the evolution of
.NET programs. In IFIP International Conference on Formal Methods
for Open Distributed Systems, LNCS. Springer Verlag, 2003.
EN94. R. Elmasri and S. B. Navathe. Fundamentals of Database Systems.
Benjamin/Cummings, 1994.
FHMV95. R. Fagin, J. Y. Halpern, Y. Moses, and M. Y. Vardi. Reasoning about
Knowledge. MIT Press, Cambridge, 1995.
Fit93. M. Fitting. Basic modal logic. In D. Gabbay, C. Hogger, and
J. Robinson, editors, Handbook of Logic in Artiﬁcial Intelligence and
Logic Programming, volume 1. Oxford University Press, 1993.
Fit96. M. Fitting. First-Order Logic and Automated Theorem Proving.
Springer, 2nd edition, 1996.
Fra92. N. Francez. Program Veriﬁcation. Addison-Wesley, 1992.
Fre03. G. Frege. Grundgesetze der Arithmetik, begriﬀsschriftlich abgeleitet.
1903. Volumes I and II (Jena).
Gal87. J. H. Gallier. Logic for Computer Science. John Wiley, 1987.
Gen69. G. Gentzen. Investigations into logical deduction. In M. E. Szabo,
editor, The Collected Papers of Gerhard Gentzen, chapter 3, pages
68–129. North-Holland Publishing Company, 1969.
Gol87. R. Goldblatt. Logics of Time and Computation. CSLI Lecture Notes,
1987.
Gri82. D. Gries. A note on a standard strategy for developing loop invariants
and loops. Science of Computer Programming, 2:207–214, 1982.
Ham78. A. G. Hamilton. Logic for Mathematicians. Cambridge University
Press, 1978.
Hoa69. C. A. R. Hoare. An axiomatic basis for computer programming.
Communications of the ACM, 12:576–580, 1969.
Hod77. W. Hodges. Logic. Penguin Books, 1977.
Hod83. W. Hodges. Elementary predicate logic. In D. Gabbay and
F. Guenthner, editors, Handbook of Philosophical Logic, volume 1.
Dordrecht: D. Reidel, 1983.

416
Bibliography
Hol90. G. Holzmann. Design and Validation of Computer Protocols. Prentice
Hall, 1990.
JSS01. D. Jackson, I. Shlyakhter, and M. Sridharan. A Micromodularity
Mechanism. In Proceedings of the ACM SIGSOFT Conference on the
Foundations of Software Engineering/European Software Engineering
Conference (FSE/ESEC’01), September 2001.
Koz83. D. Kozen. Results on the propositional mu-calculus. Theoretical
Computer Science, 27:333–354, 1983.
Lee59. C. Y. Lee. Representation of switching circuits by binary-decision
programs. Bell System Technical Journal, 38:985–999, 1959.
Lon83. D. E. Long. Model Checking, Abstraction, and Compositional
Veriﬁcation. PhD thesis, School of Computer Science, Carnegie Mellon
University, July 1983.
Mar01. A. Martin. Adequate sets of temporal connectives in CTL. Electronic
Notes in Theoretical Computer Science 52(1), 2001.
McM93. K. L. McMillan. Symbolic Model Checking. Kluwer Academic
Publishers, 1993.
MP91. Z. Manna and A. Pnueli. The Temporal Logic of Reactive and
Concurrent Systems: Speciﬁcation. Springer-Verlag, 1991.
MP95. Z. Manna and A. Pnueli. Temporal Veriﬁcation of Reactive Systems:
Safety. Springer-Verlag, 1995.
MvdH95. J.-J. Ch. Meyer and W. van der Hoek. Epistemic Logic for AI and
Computer Science, volume 41 of Cambridge Tracts in Theoretical
Computer Science. Cambridge University Press, 1995.
Pap94. C. H. Papadimitriou. Computational Complexity. Addison Wesley,
1994.
Pau91. L.C. Paulson. ML for the Working Programmer. Cambridge University
Press, 1991.
Pnu81. A. Pnueli. A temporal logic of programs. Theoretical Computer
Science, 13:45–60, 1981.
Pop94. S. Popkorn. First Steps in Modal Logic. Cambridge University Press,
1994.
Pra65. D. Prawitz. Natural Deduction: A Proof-Theoretical Study. Almqvist &
Wiksell, 1965.
QS81. J. P. Quielle and J. Sifakis. Speciﬁcation and veriﬁcation of concurrent
systems in CESAR. In Proceedings of the Fifth International
Symposium on Programming, 1981.
Ros97. A. W. Roscoe. The Theory and Practice of Concurrency. Prentice
Hall, 1997.
SA91. V. Sperschneider and G. Antoniou. Logic, A Foundation for Computer
Science. Addison Wesley, 1991.
Sch92. U. Schoening. Logik f¨ur Informatiker. B. I. Wissenschaftsverlag, 1992.
Sch94. D. A. Schmidt. The Structure of Typed Programming Languages.
Foundations of Computing. The MIT Press, 1994.
Sim94. A. K. Simpson. The Proof Theory and Semantics of Intuitionistic
Modal Logic. PhD thesis, The University of Edinburgh, Department of
Computer Science, 1994.
SS90. G. St˚almarck and M. S˚aﬂund. Modeling and verifying systems and
software in propositional logic. In B. K. Daniels, editor, Safety of
Computer Control Systems (SAFECOMP’90), pages 31–36. Pergamon
Press, 1990.

Bibliography
417
Tay98. R. G. Taylor. Models of Computation and Formal Languages. Oxford
University Press, 1998.
Ten91. R. D. Tennent. Semantics of Programming Languages. Prentice Hall,
1991.
Tur91. R. Turner. Constructive Foundations for Functional Languages.
McGraw Hill, 1991.
vD89. D. van Dalen. Logic and Structure. Universitext. Springer-Verlag, 3rd
edition, 1989.
VW84. M. Y. Vardi and Pierre Wolper. Automata-theoretic techniques for
modal logics of programs. In Proc. 16th ACM Symposium on Theory
of Computing, pages 446–456, 1984.
Wei98. M. A. Weiss. Data Structures and Problem Solving Using Java.
Addison-Wesley, 1998.

Index
ABP, 203
acknowledgement channel, 203
alternating the control bit, 203
fairness, 203
main SMV program, 207
absorption laws, 88
abstract data type
sets, 226
abstraction, 175, 229, 247
and non-determinism, 191
accessibility relation, 309, 320, 336
adequate set of connectives
for CTL, 216, 222, 231, 397
for LTL, 186
for propositional logic, 69, 87, 91
agent, 307, 319, 327
algebraic speciﬁcation, 170
algorithm
deterministic, 59
algorithm apply, 373
complexity, 380
control structure, 374
recursive descent, 375
algorithm CNF, 59
algorithm reduce, 372
complexity, 380
algorithm restrict, 377
complexity, 380
algorithm reduce
example execution, 373
Alloy
[], 153
fun-statement, 155
with, 146
assertion, 144
check directive, 144
consistency check, 144
constrain signature, 150
counterexample, 144
dot operator, 144
extending signature, 169
implication, 147
let-expression, 153
limitations, 156
modiﬁers, 150
module, 146
opening module, 156
polymorphism, 156
postcondition, 151
precondition, 151
reﬂexive, transitive closure, 169
run directive, 146
signature, 143
instance, 144
small scope hypothesis, 143, 168
transitive closure, 155
universal quantiﬁcation, 146
alternating bit protocol, 203
always in the future, 318
and-elimination, 6, 339
and-introduction, 6, 339
application domain, 173, 257
approach
model-based, 173
proof-based, 173
approximants
µmZ.f, 392
νmZ.f, 393
arity, 99
array, 299
bounds, 136, 287
ﬁeld, 287
of integers, 287
section, 287
artiﬁcial intelligence, 306
artiﬁcial language, 93
assertion checking, 143
assignment, 191
initial, 292
non-deterministic, 205, 230
program notation, 261
statement, 261
associativity laws, 56, 88
assumption, 4
discharging, 28, 329
temporary, 11, 121
418

Index
419
asynchronous
circuit, 358
interleaving, 189
atomic formula, 335
of modal logic, 307
of predicate logic
meaning, 124
axiom
5, 331
T, 341
4, 327, 330, 339
5, 330, 339
T, 327, 330, 339, 343
for assignment, 270
for equality, 107
instance, 271
Backus Naur form (BNF), 33
backwards breadth-ﬁrst search, 225,
232
base case, 41, 42, 86
basic modal logic, 307
BDD, 364
hi(n), 372
lo(n), 372
as boolean function, 365
complement, 367
consistent path, 365
edge, 361
examples, 365
has an ordering, 367
layer of variables, 361
line
dashed, 362, 365
solid, 362, 365
ordered, 367
read-1, 405
reduced, 365
removal of duplicate non-terminals, 363
removal of duplicate terminals, 363
removal of redundant tests, 363
satisﬁable, 365
subBDD, 363
which is not a read-1-BDD, 378
which is not an OBDD, 368
with duplicated subBDDs, 363
belief, 319
binary decision diagram, 364
binary decision tree, 361
redundancies in, 362
binding priorities, 176, 209
for basic modal logic, 307
for integer expressions, 260
for KT45n, 335
for predicate logic, 101
for propositional logic, 5
for relational mu-calculus, 391
bit, 133
control, 203
least signiﬁcant, 381
most signiﬁcant, 381
one-bit channel, 205
two-bit channel, 205
blocks of code, 261
Boole, G., 91, 374
boolean algebra, 19
boolean connective, 210, 310
boolean existential quantiﬁcation, 377
boolean expression, 260, 272
boolean forall quantiﬁcation, 379
boolean formula
independent of a variable, 375
semantically equivalent, 374
truth table, 359
boolean function
‘don’t care’ conditions, 403
as a binary decision tree, 361
symbolic representation, 358
boolean guard, 282
boolean variable, 358
bottom-elimination, 21
bottom-introduction (see ‘not-elimination’), 21
box-elimination, 329
box-introduction, 329
branching-time logic, 174
case
overlap, 61
case analysis, 61, 62, 113
case-statement, 18, 192
characteristic function, 383
Church, A., 133
circuit
2-bit comparator, 408
asynchronous, 194, 389
sequential, 359
synchronous, 194, 358, 388, 411
circular deﬁnition, 217
Clarke, E., 254
classical logic, 30, 328
client, 259
clock tick, 190
CNF, 55
code
speciﬁcation, 257
veriﬁcation, 257
coding
AF , 395
EF , 394
EG , 395
EU , 395
EX , 394
examples of symbolic evaluation, 395
fair EG , 398
fair EU , 397
fair EX , 397
set of fair states, 397
command, 261
atomic, 261
compound, 261
common knowledge, 332, 335
as invariant, 390

420
Index
communicating processes, 256
communication protocol, 194
Compactness Theorem, 137
completeness
of natural deduction for predicate logic, 96
of natural deduction for propositional logic,
54
complexity
exponential, 229
of apply, 404
of brute force minimal-sum section
algorithm, 288
of fairness, 397
of labelling algorithm, 224, 225
of labelling EGC, 232
composition
sequential, 278
synchronous, 194
compositional semantics, 39
compositionality
in model checking, 230
computability, 131
computation
intractable, 49
computation path, 180, 212
fair, 231
computation trace, 285
computation tree logic, 175
computation tree logic, 306, 318
computational behaviour, 306
computer program, 103
concatenation, 126, 132
conclusion, 4, 273
concurrency, 257
conjunct, 56
conjunction, 4, 291
inﬁnite, 335
connective
adequate set, 226
unary, 177, 209
consistency, 308, 316
consistency checking, 143
constant symbol, 157
constraints
inconsistent, 74
SAT solver, 69
contradiction, 20, 118, 319, 325
control structure, 261, 262
controlling value, 404
copy rule, 20, 329
core programming language, 259, 287
correspondence theory, 325
counter example, 123, 130, 317, 354
counter trace, 174
critical section, 187
CTL, 175, 254, 306, 318
as a subset of CTL*, 218
expressive power, 220
modalities, 306
with boolean combinations of path
formulas, 220, 251
CTL connectives
fair, 397
CTL formula
square brackets, 210
CTL*, 254
DAG, 70
dag, 364
dashed box
ﬂavour, 339
data structure, 123
de Morgan laws, 57, 216, 251
for modalities, 313
deadlock, 178, 184, 215, 219
debugging systems, 222
debugging systems, 257
decision problem, 131
of validity in predicate logic, 133
decision procedure, 55
declarative explanation, 26
declarative sentence, 2, 93
truth value, 37
default case, 192
deﬁnition
inductive, 33
description
informal, 258
language, 172, 174
Dijkstra, E., 283
directed graph, 136, 178, 364
acyclic, 69, 364
cycle, 364
disjunction, 4
of literals, 55, 57
distributivity laws
of box modality, 314
of F connective, 185
of propositional logic, 19, 60, 88
dividend, 302
domain assumption, 148
double negation-elimination, 352
double negation-introduction, 352
elimination rule, 6, 107
Emerson, E. A., 254
encoding, 128
entailment
in program logics, 278
environment
and non-determinism, 191
for concurrent programs, 173
for predicate logic formulas, 127
equality, 263
intentional, 107
program notation, 261
structural, 153
symbol, 107
equivalence relation, 321, 327, 339
equivalent formulas
of basic modal logic, 314
of CTL, 215–217

Index
421
of KT4, 327
of KT45, 327
of LTL, 184
of predicate logic, 117
of propositional logic, 16
of relational mu-calculus, 410
exclusive-or, 382, 398
existential quantiﬁer, 216
existential second-order logic, 139, 156
exists-elimination, 113
exists-introduction, 112
factorial
of a natural number, 262
program, 262, 284
fairness
nested ﬁxed points, 398
symbolic model checking, 396
fairness constraint, 190, 197
simple, 231, 252
FAIRNESS running, 204
Fibonacci numbers, 85
ﬁeld index, 287
ﬁnite automata, 405
ﬁnite data structure, 222
ﬁrst-order logic, 93
ﬁxed point, 240
greatest, 240, 241
least, 240, 241
semantics for CTL, 217, 238
ﬂow of control, 261
Floyd, R., 269
for-statement, 299
forall-elimination, 109
forall-introduction, 110
formal
path, 218
formula
atomic, 175
height, 44, 86
Horn, 65
ill-formed, 177
immediate subformula, 223
of basic modal logic, 314
of CTL, 208
atomic, 208
ill-formed, 209
well-formed, 209
of LTL
valid, 251
of predicate logic, 100
of propositional logic, 33, 50
well-formed, 32, 33, 44
of relational mu-calculus, 390
positive, 328, 343, 348
scheme, 312, 317
K, 315
in propositional logic, 312
instance, 312
subformula, 35
frame, 322
free for x in φ, 106, 109
Frege, G., 170
function
in predicate logic, 124
monotone, 240
a non-example, 240
nullary, 99
recursive, 250
SAT, 225, 227
termination, 253
SATaf, 228
SATag, 253
SATeg, 252
SATeu, 229
SATex, 228
symbol, 96, 98, 157
binary, 98
translate, 250
function pre∀(X), 227
function pre∃(X), 227, 385
function pre∀(X), 385
function SAT
correctness, 240
future
excludes the present, 249, 353
includes the present, 182, 249, 353
whether it includes the present, 318
G-reachable, 338
in k steps, 338
G¨odel, K., 96
Gentzen, G., 91
Global Assembly Cache, 149
grammar, 33
clause, 269
guided simulation, 155
Halpern, J., 254
higher-order logic, 141
Hoare triple, 264
Hoare, C. A. R., 264, 269
Hodges, W., 170
Horn clause, 65, 139
hybrid rule, 343
if-statement, 280
implementation
compliant, 143
implication, 4
logical, 278
implies-elimination, 9
implies-introduction, 12
in-order representation, 35
inconsistency, 259
index, 132
induction
course-of-values, 43
hypothesis, 41, 42
in model checking, 229
mathematical, 40
inductive step, 41

422
Index
inﬁx notation, 125, 210
information
negative, 343
input parameter, 61
integer
expression, 260
integer label, 372
integer multiplication, 381
interface between logics, 277
interleaving
formulas with code, 275
transitions, 188, 194
introduction rules, 6, 107
introspection
negative, 319, 326
positive, 319, 326
intuitionistic logic, 30, 120, 327
invariants, 273
discovering, 283
SAT solver, 69
iterative squaring, 412
Jape, 170
justiﬁcation, 276, 277, 329
Knaster-Tarski Theorem, 241
knowledge
common, 333
distributed, 335
domain-speciﬁc, 102
false, 321
formula
positive, 343
idealised, 319, 327
in a multi-agent system, 307
modality, 335
of agent, 307, 319
Kozen, D., 413
Kripke model, 167, 309
as a counter example, 354
for KT45n, 336
Kripke, S., 309, 315
L¨owenheim-Skolem Theorem, 138
label
adding, 223
deleting, 224
labelling
AF, 223
EG, 224
EGC, 231
EU, 223
EX, 223
labelling algorithm, 222
labelling function
coding subsets, 383
for Kripke model, 309
for LTL model, 178
frame does not have one, 322
language construct, 299
law of excluded middle, 25
LEM
instance, 328
linear-time logic, 174
linear-time temporal logic, 175
Linear-time temporal logic (LTL), 175
literal, 55, 62
liveness, 190, 197
property, 188, 189, 207, 230
logic engineering, 307, 316
logic programming, 49, 170
logical level, 278
logical variables
of Hoare triple, 269
look-up table, 127, 140
up-dated, 127
LTL, 175
LTLSPEC, 192, 204
machine state, 263
McMillan, K., 254
memoisation
of computed OBDDs, 377
midcondition, 270, 276
minimal-sum section, 288
minimal-sum-section problem, 305
modal connective
CG, 335
Ki, 335
modal logic, 306
K, 326
KT4, 327
KT45, 326, 327
normal, 326
S4, 327
S5, 326
modality, 308
diamond, 308
path, 220
model
for propositional logic, 37
of KT45n, 336
of basic modal logic, 309, 324
of CTL, 310
pictorial representation, 248, 249
of intuitionistic propositional logic, 328
of KT45, 337
of KT45n, 337
of LTL
pictorial representation, 179
of predicate logic, 96, 124
under-speciﬁed, 143
model checker, 174
model checking, 141, 173, 175, 256
algorithm, 217, 225, 231, 318
debugging, 394
example, 182, 213
with fairness constraints, 230
model-based veriﬁcation, 172, 174
module, 265
modulo 8 counter, 408
modus ponens, 9

Index
423
modus tollens, 10, 352
muddy-children puzzle, 342, 344
multiplicity constraint, 152
Mutex model
pictorial representation, 188
mutual exclusion, 187
natural deduction
extension to predicate logic, 95
for modal logic, 332
for temporal logic, 174
inventor, 91
natural deduction rules
for basic modal logic, 329
for KT45n, 340, 355
for predicate logic, 107
for propositional logic, 27
necessity
logical, 308, 318
physical, 318
negation, 4
negation-elimination (see
‘bottom-elimination’), 21
negation-introduction, 22
nested boolean quantiﬁcation, 394
network
synchronous, 174
no strict sequencing, 188, 189, 215
node
initial, 364
leaf, 103
non-terminal, 361
shared, 70
terminal, 362, 364
non-blocking protocol, 188, 189, 215
non-determinism, 190
non-termination, 262
normal form, 53, 55
conjunctive, 55, 360
disjunctive, 360
negation, 60
CTL*, 250
LTL, 186, 246
product-of-sums, 406
sum-of-products, 404
not-elimination, 21
not-introduction, 22
OBDD, 367
absence of redundant variables,
370
canonical form, 369
complementation, 385
deﬁnition, 367
extensions, 381
for pre∃(X), 387
for pre∀(X), 387
integer multiplication, 381
intersection, 385
limitations, 381
memoisation, 377
nested boolean quantiﬁcation, 380
of a transition relation, 386
of an even parity function, 370
of the odd parity function, 400
optimal ordering, 406
reduced, 368
unique representation, 368
reduced one for logical ‘iﬀ’, 400
representing subsets, 383
running time of algorithms
upper bounds, 380
sensitivity of size, 370
synthesis of boolean formula, 380
test
for implication, 372
for satisﬁability, 372
for semantic equivalence, 370
for validity, 372
union, 385
variations, 381
odd parity function, 400
omniscience
logical, 319
or-elimination, 17
or-introduction, 16
overloading
of ⊨, 129
of proof rules, 107
parity function
even, 369
as OBDD, 370
parity OBDD, 381
parse tree
for a predicate logic formula, 103
of a term, 159
of a basic modal logic formula, 307
of a CTL formula, 210
of propositional logic formula, 34
root, 35
subtree, 35
underspeciﬁed, 312
partial correctness, 265
partial order reduction, 229
pattern
checkEU (f, g), 398
checkEX (f), 397
pattern matching, 6, 111, 279
place holder, 94
possibility, 316
logical, 308
possible world
semantics, 315
Post correspondence problem, 132
postcondition, 151
in program logic, 264
Prawitz, D., 91
precondition, 151
in program logic, 264
weakest, 276
of algorithm, 63

424
Index
predicate, 93
binary, 95
number of arguments, 95
unary, 95
predicate logic, 93
consistent set of formulas, 129
satisﬁability, 129
semantic entailment, 129
validity, 129
preﬁx, 126
notation, 210
ordering, 126
premise, 270
Prior, A., 254
problem
instance, 132
reduction, 132
procedural interpretation, 26
process
concurrent, 187
instantiation, 206
processor, 257
program
behaviour, 264
bug, 257
code, 276
construct, 261
correctness, 63, 67, 225
derived, 299
diverging, 266
documentation, 257
environment, 258
ﬁnite-state, 358
fragment, 262
logic, 275
methodology, 259
procedures, 263
sequential, 256
termination, 67, 89, 242, 265
variable, 227, 268
veriﬁcation, 270
formal, 260
program execution, 316, 319
programming by contract, 296
Eiﬀel, 296
programming language
imperative, 259
proof
box
for →i, 11
for forall-introduction, 110
for modal logic, 329
opening, 28
side by side, 22
by contradiction, 24
calculus, 256, 260
construction, 269
constructive, 120
dashed box, 329, 340
fragment, 278
indirect, 29
of correctness, 239
of termination, 266
partial, 281
partial correctness, 269, 300
search, 49
solid box, 329
strategy, 115, 265
subproof, 272
tableaux, 269
theory, 93, 122, 174
total correctness, 292
proof rules, 5
for implication, 273
for assignment, 269
for conjunction, 6
for disjunction, 16
for double negation, 8
for equality, 108
for existential quantiﬁcation, 112
for if-statements, 272, 280
modiﬁed, 281
for implication, 12, 277
for KT45n, 339
for negation, 20
for quantiﬁers, 112
for sequential composition, 269, 275
for universal quantiﬁcation, 109
for while-statements, 272, 282, 287
schema, 111
subformula property, 113
proof tableaux
complete, 291
proof-based veriﬁcation, 172, 256
proposition, 2
propositional logic, 93
protocol, 187, 188
provability
undecidability of predicate logic, 136
quantiﬁer, 310, 313
equivalences, 185
in predicate logic, 94
binding priorities, 101
equivalences, 130
meaning, 123
Quielle, J., 254
reachability, 136, 137
reasoning
about knowledge, 326, 331
constructive, 29
in an arbitrary accessible world, 329
informal, 343
quantitative, 259
unsound, 280
record
ﬁeld, 193
recursion
mutual, 218
recursive call, 280
reductio ad absurdum, 24, 119
reduction to absurdity, 24
reﬂexive, transitive closure, 167

Index
425
regular language, 405
relation
binary, 178
Euclidean, 321, 327
functional, 321
linear, 321
reﬂexive, 140, 320, 324
as formula, 109
serial, 320, 353
symmetric, 320
as formula, 109
total, 321
transition, 178
transitive, 140, 320, 324
as formula, 109
relational mu-calculus
ﬁxed-point operators, 392
requirement
informal, 258, 263, 288
requirements, 142
restriction, 374
right-associative, 5
root of a parse tree, 135
rule
derived, 23
hybrid, 10
Russell’s paradox, 165
safety property, 187, 189, 207
SAT solver
cubic, 76
forcing rules, 71
permanent marks, 75
temporary marks, 74
satisfaction
in a frame, 322
in a frame for KT45n, 337
satisfaction relation
for relational mu-calculus, 391
for basic modal logic, 310
for KT45, 337
for LTL, 180
for partial correctness, 265
for predicate logic, 128
for relational mu-calculus, 391
for total correctness, 266
satisﬁability, 360
3SAT, 406
deciding, 65
of a propositional logic formula,
85
undecidability of predicate logic,
135
SCC
fair, 232
scheduler
fair, 197
scope
of a dummy variable, 117
of a variable, 103, 113
of an assumption, 28, 113, 329
search space, 113, 133
second-order logic, 141
semantic entailment
for predicate logic, 141
for basic modal logic, 313
for KT4, 328
for normal modal logics, 326
for predicate logic, 96
for propositional logic, 46
for relational mu-calculus, 410
semantic equivalence, 39
semantics
of µZ.f, 392
of νZ.f, 393
of basic modal logic, 310
of boolean quantiﬁcation, 392
of CTL, 211
of EG, 239
of equality, 131
of predicate logic, 122
of propositional logic, 38
of relational mu-calculus, 391
of Until, 181
sentence
atomic, 4
components, 93
declarative, 93
in predicate logic, 128
sequent, 5
invalid, 116
Shannon expansion, 374
side condition, 108, 110
Sifakis, J., 254
small scope hypothesis, 143
SMV, 254
main program for ABP, 207
module, 193
receiver, 205
sender, 204
for channel, 206
instantiation, 193
process, 389
program
example, 192
for Mutex, 195
speciﬁcation, 192
software
life-cycle, 142
micromodel, 142
reliability, 149
requirements, 142
speciﬁcation, 142
validation, 142
soundness
of forall-elimination, 109
of natural deduction
basic modal logic, 354
predicate logic, 96, 122
propositional logic, 45
of program logics, 267
of proof rule for while-statements,
282
of the substitution principle, 108

426
Index
speciﬁcation
for ABP, 207
formal, 259
informal, 259
language, 172
of a predicate, 157
patterns, 254
practical pattern, 183, 215
truth table, 58
speciﬁcations, 191
Spin, 254
state
critical, 188
explosion, 229
explosion problem, 254
fair, 397
ﬁnal, 142
formula, 218
global, 188
graph, 180
initial, 142, 189, 222, 247, 252, 264
non-critical, 188
of a system, 269
of core program, 264
reachable, 247
resulting, 263, 299
space, 229
splitting states, 190
transition, 142
trying, 188
state machine, 142
storage
location, 288
state, 261
store
of core program, 264
string, 247, 307
binary, 126, 132
empty, 126
strongly connected component, 225
structural equality, 153
structural induction, 44, 51
subformula, 178
substitution
in predicate logic, 105
instance, 323
instance of tautology, 314
principle, 108
symbolic model checking, 383
syntactic
domain, 260, 261
syntax
of basic modal logic, 307
of boolean expressions, 261
of boolean formulas, 398
of CTL, 208
of CTL*, 218
of KT45n, 335
of LTL, 175
of predicate logic, 100
of propositional logic, 33
of relational mu-calculus, 390
of terms, 99
system
asynchronous, 254
interleaving model, 389
simultaneous model, 389
axiomatic, 91
commercially critical, 172, 257
component, 206
concurrent, 173
debugging, 174
description, 193
design, 174
development, 173
elevator, 184, 215
ﬁnite-state, 256
hybrid, 277
inﬁnite-state, 256
mission-critical, 172
multi-agent, 331
physical, 175
reactive, 173, 257, 358
safety-critical, 172, 257
transition, 174
veriﬁcation, 256
tautology, 50
temporal connective
AF, 212
AG, 211
AU, 212
AX, 211
EF, 212
EG, 211
EU, 212
EX, 211
temporal connectives, 176
temporal logic, 174, 306
term, 99
interpretation, 128
term-rewriting system, 170
termination
Collatz 3n + 1, 295
proof, 266
tertium non datur, 25
theorem, 13
prover, 106, 136
proving, 170
time
continuous, 174
discrete, 174
top
marking, 66
total correctness, 265, 266
transition relation, 178
for SMV programs, 388
transition system, 174
of ABP program, 247
of Mutex code, 198
of SMV program, 192
unwinding, 180, 212, 222

Index
427
translation
English into predicate logic, 95, 101
tree
inﬁnite, 180, 212
truth
dynamic, 174
mode, 306, 308
of knowledge, 326
static, 174
value
for predicate logic, 127
for propositional logic, 3
truth table
for conjunction, 37
truth tables, 38
type, 12, 327
checking, 12
theory, 170
unary connective, 307
undecidability
of provability, 136
of satisﬁability, 135
of validity in predicate logic, 133
universal quantiﬁcation, 268
universal quantiﬁer, 216
universal second-order logic, 140, 156
universe of concrete values, 124
unreachability, 140
unsound sequent, 164
Until
in natural language, 182
negating, 187
updated valuation, 391
valid sequent
of modal logic, 330
partial correctness, 267
total correctness, 267
validity
in basic modal logic, 314
in KT45n, 339
in propositional logic, 85
undecidability in predicate logic, 133
valuation
for propositional logic, 37
in predicate logic, 123
in relational mu-calculus, 391
value
initial, 206, 268, 269
Vardi, M., 254
variable, 94, 260
boolean, 229, 247, 358
bound, 103
capture, 106
dummy, 110
free, 103
local, 263
logical, 268, 290
variable ordering
compatible, 368
list, 367
variant, 293
veriﬁcation
full, 173
method, 172
of communication protocols, 175
of hardware, 175
of software, 175
of systems, 256
post-development, 173, 257
pre-development, 173, 257
process, 271
program, 270
property, 173
property-oriented, 256
semi-automatic, 256
techniques, 172
weakest precondition, 276
while-statement, 261, 262
body, 273, 282, 286
non-termination, 292
wise-men puzzle, 342
Wolper, P., 254
word
empty, 126
world
accessible, 309
possible, 309, 336
year-2000 problem, 258


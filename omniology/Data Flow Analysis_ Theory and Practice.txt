
CRC Press is an imprint of the
Taylor & Francis Group, an informa business
Boca Raton   London   New York
Uday P. Khedker
Amitabha Sanyal
Bageshri Karkare
Theory and Practice
Data
Flow
Analysis
© 2009 by Taylor & Francis Group, LLC

CRC Press
Taylor & Francis Group
6000 Broken Sound Parkway NW, Suite 300
Boca Raton, FL 33487-2742
© 2009 by Taylor & Francis Group, LLC 
CRC Press is an imprint of Taylor & Francis Group, an Informa business
No claim to original U.S. Government works
Printed in the United States of America on acid-free paper
10 9 8 7 6 5 4 3 2 1
International Standard Book Number-13: 978-0-8493-2880-0 (Hardcover)
This book contains information obtained from authentic and highly regarded sources. Reasonable
efforts have been made to publish reliable data and information, but the author and publisher can-
not assume responsibility for the validity of all materials or the consequences of their use. The 
authors and publishers have attempted to trace the copyright holders of all material reproduced 
in this publication and apologize to copyright holders if permission to publish in this form has not 
been obtained. If any copyright material has not been acknowledged please write and let us know so
we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced,
transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or 
hereafter invented, including photocopying, microfilming, and recording, or in any information 
storage or retrieval system, without written permission from the publishers.
For permission to photocopy or use material electronically from this work, please access www.copy-
right.com (http://www.copyright.com/) or contact the Copyright Clearance Center, Inc. (CCC), 222
Rosewood Drive, Danvers, MA 01923, 978-750-8400. CCC is a not-for-profit organization that pro-
vides licenses and registration for a variety of users. For organizations that have been granted a 
photocopy license by the CCC, a separate system of payment has been arranged.
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and 
are used only for identification and explanation without intent to infringe.
Library of Congress Cataloging-in-Publication Data
Khedker, Uday.
Data flow analysis : theory and practice / Uday Khedker, Amitabha Sanyal, 
Bageshri Karkare.
p. cm.
Includes bibliographical references and index.
ISBN 978-0-8493-2880-0 (hardcover : alk. paper)
1. Compilers (Computer programs) 2. Data flow computing. 3. Software 
engineering. 4. Computer software--Verification. I. Sanyal, Amitabha. II. Karkare, 
Bageshri. III. Title.
QA76.76.C65K54 2009
004’.35--dc22
2009002056
Visit the Taylor & Francis Web site at
http://www.taylorandfrancis.com
and the CRC Press Web site at
http://www.crcpress.com
© 2009 by Taylor & Francis Group, LLC

© 2009 by Taylor & Francis Group, LLC

© 2009 by Taylor & Francis Group, LLC

Preface
Data ﬂow analysis is a classical static analysis technique that has been used to dis-
cover useful properties of programs being analyzed. It has found many useful ap-
plications ranging from compiler optimizations to software engineering to software
veriﬁcation. Modern compilers use this technique to produce code that maximize
performance. In software engineering, it is used to re-engineer or reverse engineer
programs. Finally, data ﬂow analysis based techniques are used in software veriﬁca-
tion to prove the soundness of programs with respect to properties of interest.
This book provides a detailed treatment of data ﬂow analysis. Although we explain
it in the context of compiler optimizations, the concepts are general enough to be
used for other applications. This is possible because we use a general model of data
ﬂow equations to represent the speciﬁcation of data ﬂow analysis. These data ﬂow
equations are deﬁned in terms of constant and dependent Gen and Kill components.
For classical bit vector frameworks, the constant Gen and Kill suﬃce; dependent
parts are required for frameworks like constant propagation, points-to analysis etc.
Such a modeling explicates the inter-dependence of data ﬂow values and leads to
an orthogonal generality that models ﬂow functions in terms of a rather small set
of constituent functions called entity functions. On the one hand, modeling ﬂow
functions in terms of entity functions allows us to deﬁne information ﬂow paths that
explain empirical observations for a large class of data ﬂow frameworks and facilitate
tight complexity bounds on solution procedures for data ﬂow equations. On the
other hand, this modeling also allows reasoning about the feasibility of constructing
summary ﬂow functions.
The book is organized in three parts: The ﬁrst part deals with the speciﬁcation of
data ﬂow frameworks and the solution process at the intraprocedural level. This part
presents the lattice theoretic modeling of data ﬂow frameworks apart from the gen-
eralizations of constant and dependent parts in ﬂow functions and entity functions
as constituents of ﬂow functions. It shows how these generalizations lead to tight
complexity bounds. This part also presents a large number of data ﬂow frameworks.
The diversity of these analyses is an evidence of the wide applicability of the gener-
alizations presented. The ﬁnal chapter of the ﬁrst part presents SSA representation of
programs. This is interesting because it builds an additional layer of abstraction over
the control ﬂow graph representation of programs and directly relates the deﬁnition
points and the use points of data. This increases the eﬃciency with which a class of
optimizations can be performed.
The second part of the book presents interprocedural data ﬂow analysis. As a
matter of choice, we avoid methods that are speciﬁc to a particular application or
v
© 2009 by Taylor & Francis Group, LLC

vi
a particular data ﬂow framework and instead, focus on generic approaches. The
ﬁrst approach is a functional approach that constructs context independent summary
ﬂow functions of procedures. These ﬂow functions are used at the call points to
incorporate the eﬀects of procedure calls. The second approach is a value-based
approach that computes distinct values for distinct calling contexts; this is achieved
by augmenting the data ﬂow values with context information.
The third part of the book describes the implementation of a generic data ﬂow
analyzer for bit vector frameworks in GCC and shows how it can be instantiated to a
given framework.
This book is an outcome of our notes for the course CS618: Program Analysis
which is a graduate course at the Department of Computer Science and Engineering,
IIT Bombay. The slides used in the course and the source of the generic data ﬂow
analyzer gdfa are available at the web page of the book:
http://www.cse.iitb.ac.in/˜uday/dfaBook-web
As errors are discovered, we will upload an errata on the above web page. Any
additional material that we ﬁnd relevant to a course based on this book will also be
made available on the same web page.
Many people have gone through the earlier versions of this manuscript. The reg-
istrants of CS618 were our captive audience for testing our examples—some ex-
amples tested their patience in the examinations of CS618. The following students
of CS618 pointed out errors to us: Abhishek Shrivastav, Amitraj Singh Chouhan,
Dhritiman Das, Harshada Gune, Md.
Naseerunddin, Nilesh Padariya, Prashima
Sharma, and Pushpraj Agrawal. Among others, Jaishri Waghmare, Prashant Singh
Rawat, Sameera Deshpande, Santosh Sonawane, and Seema Ravandale read some
chapters and gave valuable comments. Seema extended gdfa to include support for
reaching deﬁnitions analysis. Sameera’s help in preparing the ﬁrst draft of the index
is gratefully acknowledged.
Finally, this book would not have been possible without the patience and constant
encouragement of our families. They have gracefully tolerated our mental, if not
physical, absence, relieving us from a sense of guilt. We express a deep sense of
gratitude for their support.
Uday P. Khedker, Amitabha Sanyal, and Bageshri Karkare
© 2009 by Taylor & Francis Group, LLC

vii
To my mother Rajani and the memory of my father Prabhakar Khedker
Uday Khedker
To my parents Arunojjwal and Prakriti Sanyal
Amitabha Sanyal
© 2009 by Taylor & Francis Group, LLC

© 2009 by Taylor & Francis Group, LLC

Contents
Preface
v
1
An Introduction to Data Flow Analysis
1
1.1
A Motivating Example
. . . . . . . . . . . . . . . . . . . . . . .
1
1.1.1
Optimizing for Heap Memory . . . . . . . . . . . . . . .
1
1.1.2
Computing Liveness . . . . . . . . . . . . . . . . . . . .
4
1.1.3
Computing Aliases . . . . . . . . . . . . . . . . . . . . .
9
1.1.4
Performing Optimization . . . . . . . . . . . . . . . . . .
10
1.1.5
General Observations . . . . . . . . . . . . . . . . . . . .
10
1.2
Program Analysis: The Larger Perspective . . . . . . . . . . . . .
12
1.3
Characteristics of Data Flow Analysis
. . . . . . . . . . . . . . .
16
1.4
Summary and Concluding Remarks
. . . . . . . . . . . . . . . .
18
1.5
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . .
19
I
Intraprocedural Data Flow Analysis
21
2
Classical Bit Vector Data Flow Analysis
23
2.1
Basic Concepts and Notations . . . . . . . . . . . . . . . . . . . .
23
2.2
Discovering Local Data Flow Information
. . . . . . . . . . . . .
24
2.3
Discovering Global Properties of Variables . . . . . . . . . . . . .
26
2.3.1
Live Variables Analysis
. . . . . . . . . . . . . . . . . .
26
2.3.2
Dead Variables Analysis . . . . . . . . . . . . . . . . . .
29
2.3.3
Reaching Deﬁnitions Analysis . . . . . . . . . . . . . . .
29
2.3.4
Reaching Deﬁnitions for Copy Propagation . . . . . . . .
32
2.4
Discovering Global Properties of Expressions
. . . . . . . . . . .
33
2.4.1
Available Expressions Analysis
. . . . . . . . . . . . . .
33
2.4.2
Partially Available Expressions Analysis
. . . . . . . . .
36
2.4.3
Anticipable Expressions Analysis . . . . . . . . . . . . .
37
2.4.4
Classical Partial Redundancy Elimination . . . . . . . . .
39
2.4.5
Lazy Code Motion . . . . . . . . . . . . . . . . . . . . .
49
2.5
Combined May-Must Analyses
. . . . . . . . . . . . . . . . . . .
53
2.6
Summary and Concluding Remarks
. . . . . . . . . . . . . . . .
56
2.7
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . .
57
ix
© 2009 by Taylor & Francis Group, LLC

x
3
Theoretical Abstractions in Data Flow Analysis
59
3.1
Graph Properties Relevant to Data Flow Analysis
. . . . . . . . .
59
3.2
Data Flow Framework . . . . . . . . . . . . . . . . . . . . . . . .
63
3.2.1
Modeling Data Flow Values Using Lattices . . . . . . . .
64
3.2.2
Modeling Flow Functions
. . . . . . . . . . . . . . . . .
71
3.2.3
Data Flow Frameworks . . . . . . . . . . . . . . . . . . .
72
3.3
Data Flow Assignments . . . . . . . . . . . . . . . . . . . . . . .
74
3.3.1
Meet Over Paths Assignment . . . . . . . . . . . . . . . .
75
3.3.2
Fixed Point Assignment
. . . . . . . . . . . . . . . . . .
76
3.3.3
Existence of Fixed Point Assignment
. . . . . . . . . . .
77
3.4
Computing Data Flow Assignments
. . . . . . . . . . . . . . . .
79
3.4.1
Computing MFP Assignment
. . . . . . . . . . . . . . .
79
3.4.2
Comparing MFP and MOP Assignments . . . . . . . . .
81
3.4.3
Undecidability of MOP Assignment Computation
. . . .
83
3.5
Complexity of Data Flow Analysis for Rapid Frameworks . . . . .
85
3.5.1
Properties of Data Flow Frameworks . . . . . . . . . . . .
86
3.5.2
Complexity for General CFGs . . . . . . . . . . . . . . .
90
3.5.3
Complexity in Special Cases . . . . . . . . . . . . . . . .
97
3.6
Summary and Concluding Remarks
. . . . . . . . . . . . . . . .
99
3.7
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . .
100
4
General Data Flow Frameworks
101
4.1
Non-Separable Flow Functions
. . . . . . . . . . . . . . . . . . .
101
4.2
Discovering Properties of Variables . . . . . . . . . . . . . . . . .
103
4.2.1
Faint Variables Analysis . . . . . . . . . . . . . . . . . .
103
4.2.2
Possibly Uninitialized Variables Analysis . . . . . . . . .
106
4.2.3
Constant Propagation . . . . . . . . . . . . . . . . . . . .
108
4.2.4
Variants of Constant Propagation . . . . . . . . . . . . . .
115
4.3
Discovering Properties of Pointers
. . . . . . . . . . . . . . . . .
119
4.3.1
Points-To Analysis of Stack and Static Data . . . . . . . .
119
4.3.2
Alias Analysis of Stack and Static Data . . . . . . . . . .
129
4.3.3
Formulating Data Flow Equations for Alias Analysis . . .
132
4.4
Liveness Analysis of Heap Data
. . . . . . . . . . . . . . . . . .
135
4.4.1
Access Expressions and Access Paths . . . . . . . . . . .
137
4.4.2
Liveness of Access Paths . . . . . . . . . . . . . . . . . .
138
4.4.3
Representing Sets of Access Paths by Access Graphs . . .
141
4.4.4
Data Flow Analysis for Explicit Liveness . . . . . . . . .
146
4.4.5
The Motivating Example Revisited . . . . . . . . . . . . .
151
4.5
Modeling Entity Dependence . . . . . . . . . . . . . . . . . . . .
152
4.5.1
Primitive Entity Functions . . . . . . . . . . . . . . . . .
153
4.5.2
Composite Entity Functions
. . . . . . . . . . . . . . . .
155
4.6
Summary and Concluding Remarks
. . . . . . . . . . . . . . . .
156
4.7
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . .
156
© 2009 by Taylor & Francis Group, LLC

xi
5
Complexity of Iterative Data Flow Analysis
159
5.1
Generic Flow Functions and Data Flow Equations . . . . . . . . .
159
5.2
Generic Round-Robin Iterative Algorithm
. . . . . . . . . . . . .
162
5.3
Complexity of Round-Robin Iterative Algorithm . . . . . . . . . .
164
5.3.1
Identifying the Core Work Using Work List . . . . . . . .
165
5.3.2
Information Flow Paths in Bit Vector Frameworks . . . . .
171
5.3.3
Deﬁning Complexity Using Information Flow Paths
. . .
173
5.3.4
Information Flow Paths in Fast Frameworks . . . . . . . .
175
5.3.5
Information Flow Paths in Non-separable Frameworks . .
179
5.4
Summary and Concluding Remarks
. . . . . . . . . . . . . . . .
184
5.5
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . .
184
6
Single Static Assignment Form as Intermediate Representation
185
6.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
185
6.1.1
An Overview of SSA . . . . . . . . . . . . . . . . . . . .
186
6.1.2
Beneﬁts of SSA Representation
. . . . . . . . . . . . . .
188
6.2
Construction of SSA Form Programs . . . . . . . . . . . . . . . .
189
6.2.1
Dominance Frontier
. . . . . . . . . . . . . . . . . . . .
191
6.2.2
Placement of φ-instructions . . . . . . . . . . . . . . . . .
194
6.2.3
Renaming of Variables . . . . . . . . . . . . . . . . . . .
196
6.2.4
Correctness of the Algorithm . . . . . . . . . . . . . . . .
198
6.3
Destruction of SSA
. . . . . . . . . . . . . . . . . . . . . . . . .
207
6.3.1
An Algorithm for SSA Destruction
. . . . . . . . . . . .
209
6.3.2
SSA Destruction and Register Allocation
. . . . . . . . .
216
6.4
Summary and Concluding Remarks
. . . . . . . . . . . . . . . .
227
6.5
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . .
228
II
Interprocedural Data Flow Analysis
231
7
Introduction to Interprocedural Data Flow Analysis
233
7.1
A Motivating Example
. . . . . . . . . . . . . . . . . . . . . . .
233
7.2
Program Representations for Interprocedural Analysis . . . . . . .
234
7.3
Modeling Interprocedural Data Flow Analysis
. . . . . . . . . . .
236
7.3.1
Summary Flow Functions
. . . . . . . . . . . . . . . . .
236
7.3.2
Inherited and Synthesized Data Flow Information . . . . .
237
7.3.3
Approaches to Interprocedural Data Flow Analysis . . . .
238
7.4
Compromising Precision for Scalability
. . . . . . . . . . . . . .
239
7.4.1
Flow and Context Insensitivity . . . . . . . . . . . . . . .
240
7.4.2
Side Eﬀects Analysis . . . . . . . . . . . . . . . . . . . .
244
7.5
Language Features Inﬂuencing Interprocedural Analysis
. . . . .
244
7.6
Common Variants of Interprocedural Data Flow Analysis
. . . . .
246
7.6.1
Intraprocedural Analysis with Conservative Interprocedu-
ral Approximation
. . . . . . . . . . . . . . . . . . . . .
246
7.6.2
Intraprocedural Analysis with Side Eﬀects Computation
.
248
7.6.3
Whole Program Analysis . . . . . . . . . . . . . . . . . .
253
© 2009 by Taylor & Francis Group, LLC

xii
7.7
An Aside on Interprocedural Optimizations
. . . . . . . . . . . .
254
7.8
Summary and Concluding Remarks
. . . . . . . . . . . . . . . .
256
7.9
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . .
256
8
Functional Approach to Interprocedural Data Flow Analysis
259
8.1
Side Eﬀects Analysis of Procedure Calls
. . . . . . . . . . . . . .
259
8.1.1
Computing Flow Sensitive Side Eﬀects
. . . . . . . . . .
261
8.1.2
Computing Flow Insensitive Side Eﬀects
. . . . . . . . .
263
8.2
Handling the Eﬀects of Parameters
. . . . . . . . . . . . . . . . .
266
8.2.1
Deﬁning Aliasing of Parameters . . . . . . . . . . . . . .
267
8.2.2
Formulating Alias Analysis of Parameters . . . . . . . . .
268
8.2.3
Augmenting Data Flow Analyses Using Parameter Aliases
271
8.2.4
Eﬃcient Parameter Alias Analysis . . . . . . . . . . . . .
273
8.3
Whole Program Analysis
. . . . . . . . . . . . . . . . . . . . . .
274
8.3.1
Lattice of Flow Functions
. . . . . . . . . . . . . . . . .
274
8.3.2
Reducing Function Compositions and Conﬂuences . . . .
275
8.3.3
Constructing Summary Flow Functions . . . . . . . . . .
278
8.3.4
Computing Data Flow Information . . . . . . . . . . . . .
282
8.3.5
Enumerating Summary Flow Functions . . . . . . . . . .
285
8.4
Summary and Concluding Remarks
. . . . . . . . . . . . . . . .
290
8.5
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . .
291
9
Value-Based Approach to Interprocedural Data Flow Analysis
293
9.1
Program Model for Value-Based Approaches to InterproceduralData
Flow Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . .
293
9.2
Interprocedural Analysis Using Restricted Contexts
. . . . . . . .
296
9.3
Interprocedural Analysis Using Unrestricted Contexts . . . . . . .
301
9.3.1
Using Call Strings to Represent Unrestricted Contexts
. .
302
9.3.2
Issues in Termination of Call String Construction . . . . .
305
9.4
Bounding Unrestricted Contexts Using Data Flow Values
. . . . .
311
9.4.1
Call String Invariants . . . . . . . . . . . . . . . . . . . .
311
9.4.2
Value-Based Termination of Call String Construction . . .
317
9.5
The Motivating Example Revisited
. . . . . . . . . . . . . . . . .
324
9.6
Summary and Concluding Remarks
. . . . . . . . . . . . . . . .
326
9.7
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . .
328
III
Implementing Data Flow Analysis
331
10 Implementing Data Flow Analysis in GCC
333
10.1
Specifying a Data Flow Analysis
. . . . . . . . . . . . . . . . . .
333
10.1.1
Registering a Pass With the Pass Manager in GCC
. . . .
334
10.1.2
Specifying Available Expressions Analysis
. . . . . . . .
336
10.1.3
Specifying Other Bit Vector Data Flow Analyses . . . . .
338
10.2
An Example of Data Flow Analysis . . . . . . . . . . . . . . . . .
340
10.2.1
Executing the Data Flow Analyzer . . . . . . . . . . . . .
341
© 2009 by Taylor & Francis Group, LLC

xiii
10.2.2
Examining the Gimple Version of CFG
. . . . . . . . . .
342
10.2.3
Examining the Result of Data Flow Analysis
. . . . . . .
346
10.3
Implementing the Generic Data Flow Analyzer gdfa . . . . . . . .
352
10.3.1
Speciﬁcation Primitives . . . . . . . . . . . . . . . . . . .
352
10.3.2
Interface with GCC . . . . . . . . . . . . . . . . . . . . .
354
10.3.3
The Preparatory Pass . . . . . . . . . . . . . . . . . . . .
358
10.3.4
Local Data Flow Analysis
. . . . . . . . . . . . . . . . .
358
10.3.5
Global Data Flow Analysis . . . . . . . . . . . . . . . . .
360
10.4
Extending the Generic Data Flow Analyzer gdfa . . . . . . . . . .
363
A An Introduction to GCC
365
A.1
About GCC
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
365
A.2
Building GCC . . . . . . . . . . . . . . . . . . . . . . . . . . . .
366
A.3
Further Readings in GCC
. . . . . . . . . . . . . . . . . . . . . .
368
References
371
© 2009 by Taylor & Francis Group, LLC

© 2009 by Taylor & Francis Group, LLC

1
An Introduction to Data Flow Analysis
Data ﬂow analysis is a process of deriving information about the run time behaviour
of a program.
This chapter introduces the basic concepts of data ﬂow analysis through a contem-
porary optimization. Then we describe common properties of program analyses at
an abstract level and instantiate them for data ﬂow analysis.
1.1
A Motivating Example
We present a data ﬂow analysis for optimizing heap memory usage in programs to
free heap cells as soon as possible. Formal details of the analysis are postponed to
Section 4.4. In this section we perform the required analysis and explain the issues
involved intuitively. The result of intraprocedural data ﬂow analysis of this program
using the formal theory is presented in Section 4.4.5 whereas Section 9.5 presents
the result of interprocedural data ﬂow analysis.
1.1.1
Optimizing for Heap Memory
Figure 1.1(b) provides a program to traverse a tree in depth ﬁrst order. The data
structure used for representing the input tree is illustrated in Figure 1.1(a). Func-
tion dfTraverse recursively descends down a tree node and prints node numbers
while unwinding from recursion. Figure 1.1(c) provides its control ﬂow graph. The
nodes in this graph represent statements and the edges represent control transfers be-
tween the statements. Observe that the while loop, which is a compound statement,
has been translated in terms of a conditional branch (out edges of block n2) and an
unconditional branch (out edge of block n5).
For simplicity of descriptions, we assume that reading a pointer is equivalent to
reading the data pointed to by the pointer. Further, when we say that a given data
object is read, we mean that some pointer which points to the data object is read;
when a data object is not read, no pointer which points to the data object is read.
Figure 1.2 provides the execution trace of dfTraverse on the input tree in Fig-
ure 1.1(a). It is clear from the trace that the data object pointed to by pointer succ
is last read in block n4. Thus it is desirable that the heap memory allocated for this
1
© 2009 by Taylor & Francis Group, LLC

2
Data Flow Analysis: Theory and Practice
1
2
3
4
5
6
7
8
o1
1
o2
2
o3
3
o4
4
child
sib
sib
o5
5
o6
6
child
sib
o7
7
o8
8
child
sib
(a) An example tree and its data structure representation. Each object contains a pointer to its
ﬁrst child. Other children are siblings of the ﬁrst child.
0.
void main()
1.
{
Tree *tree;
2.
tree = createTree();
3.
dfTraverse(tree);
4.
}
5.
void dfTraverse(Tree *n)
6.
{
Tree *succ, *next;
7.
succ = n->child;
8.
while (succ != NULL);
9.
{
dfTraverse(succ);
10.
next = succ->sib;
free(succ);
11.
succ = next;
12.
}
13.
printf("%d\n",n->num);
14.
}
n1 succ = n->child n1
n2 if (succ != NULL) n2
n3 dfTraverse(succ) n3
n4 next = succ->sib n4
free(succ) n5
n5 succ = next n5
printf("%d\n",n->num)
n6
T
F
(b) A tree traversal program
(c) CFG of dfTraverse
FIGURE 1.1
An example of heap memory optimization. Various nodes of tree are freed as shown
in the gray boxes as soon as their traversal is over.
data object be reclaimed as soon as possible and added to the free pool for a possible
subsequent allocation. The statement which performs the suggested deallocation has
been shown in gray box and is not part of the original program. Observe that this
deallocation cannot be performed through garbage collection because all these data
objects are reachable from the root variable tree in the program.
This particular instance of optimization can be summarized as follows:
Pointer variable succ is not live at the entry of n5 and is not aliased to
any live pointer. Hence the data can be deallocated at the entry of n5.
The properties of liveness and aliasing of pointers are deﬁned as:
Liveness of a pointer. A pointer is live at a program point u if the address that it
holds at u is read along some path starting at u.
Aliasing of pointers. Two pointers are aliased to each other at a program point u if
they hold the same address in some execution instance of u.
© 2009 by Taylor & Francis Group, LLC

An Introduction to Data Flow Analysis
3
n1:o1,o2:n->child
n2:o2:succ
n3:o2:succ
n1:o2,o5:n->child
n2:o5:succ
n3:o5:succ
n1:o5,NULL:n->child
n2:NULL:succ
n6:o5:n
n4:o5,o6:succ->sib
n5:o6:next
n2:o6:succ
n3:o6:succ
n1:o6,NULL:n->child
n2:NULL:succ
n6:o6:n
n4:o6,NULL:succ->sib
n5:NULL:next
n2:NULL:succ
n6:o2:n
n4:o2,o3:succ->sib
n5:o3:next
n2:o3:succ
n3:o3:succ
n1:o3,NULL:n->child
n2:NULL:succ
n6:o3:n
n4:o3,o4:succ->sib
n5:o4:next
n2:o4:succ
n3:o4:succ
n1:o4,o7:n->child
n2:o7:succ
n3:o7:succ
n1:o7,NULL:n->child
n2:NULL:succ
n6:o7:n
n4:o7:succ->sib
n5:o8:next
n2:o8:succ
n3:o8:succ
n1:o8,NULL:n->child
n2:NULL:succ
n6:o8:n
n4:o8:succ->sib
n5:NULL:next
n2:NULL:succ
n6:o4:n
n4:o4:succ->sib
n5:NULL:next
n2:NULL:succ
FIGURE 1.2
Execution trace of function dfTraverse on the input tree in Figure 1.1(a). Each
entry is of the form x : y : z where y is the list of objects read using the pointer se-
quence z in block x. Entries with gray background correspond to the last use of the
ﬁrst object in the list. Nested activations have been shown by nested indentations.
The ﬁnal data ﬂow information which enables this optimization has been provided
in Figure 1.4 (Section 1.1.4).
The liveness and alias analyses required for performing optimization such as above
use the concept of an access path which is a sequence of pointers representing a
path in the memory. The ﬁrst pointer in the sequence is a local or global variable
whereas all subsequent pointers are ﬁeld members of structures. In our example,
when succ points to object o2, objects o3, o4, o5, o6 can be accessed using access
paths succ
sib, succ
sib
sib, succ
child, and succ
child
sib; we say that objects
o3, o4, o5, and o6 are targets of access paths succ
sib, succ
sib
sib, succ
child,
and succ
child
sib respectively.
For the purpose of this chapter, we do not distinguish between access paths beyond
two levels of pointer indirections. Access paths with three or more pointers are
summarized by suﬃxing a  after the ﬁrst two pointers. Thus succ
sib
sib and
succ
sib
child are both represented by succ
sib
. A more precise and formal
method of summarization of access paths using graphs is presented in Section 4.4.3.
Our analyses extend the concept of liveness and aliasing of pointer variables to
liveness and aliasing of access paths.
© 2009 by Taylor & Francis Group, LLC

4
Data Flow Analysis: Theory and Practice
1.1.2
Computing Liveness
The liveness information at a program point is represented by a set of live access
paths where liveness of an access path is deﬁned as follows:
Liveness of access paths. An access path ρ is live at a program point u if the targets
of all preﬁxes of ρ are read along some control ﬂow path starting at u.
Clearly, liveness sets are preﬁx-closed. For notational convenience, we retain only
those access paths which are not preﬁxes of other access paths.
Since liveness information at u represents possible uses beyond u, it is computed
from the liveness information at the successors of u. For an access path to be live
at u, it is suﬃcient that it is live at any successor of u. Hence the set of live access
paths at u is a union of the corresponding sets at successors of u. In our example, the
liveness set at the exit of n2 is computed by taking a union of the sets of live access
paths at the entries of n6 and n3.
The sets of live access paths are computed by successive reﬁnements starting from
a conservative initial value of ∅. The initial value chosen is ∅because it is the identity
of union operation. We choose an iterative traversal over the CFG for each step of
reﬁnement. Since liveness at a program point depends on the successor points, we
traverse the CFG against the direction of control ﬂow. For our example, this implies
the following order of computing liveness sets: n6, n5, n4, n3, n2, and n1. This
method is called the round-robin iterative method of performing data ﬂow analysis.
We will use this method in the rest of the book to present our examples. Sections 3.4,
3.5, and 5.2 deﬁne this method formally and analyze its complexity.
Modelling Interprocedural Eﬀects
The data ﬂow information within a function is inﬂuenced by interprocedural eﬀects
arising out of function calls. In particular, the data ﬂow information in function f is
inﬂuenced by the caller functions of f as well as by the functions called by f. If the
interprocedural eﬀects are ignored during intraprocedural analysis, it could lead to
incorrect results. This can be avoided by either performing interprocedural analysis
or by approximating the interprocedural eﬀects.
Figure 1.3 models the above situations for our example program. Figure 1.3(a)
illustrates the situation when the interprocedural eﬀects are ignored: The call state-
ment in block n3 is modeled as reading merely the actual parameter succ. Further
it is assumed that no access path rooted at the formal parameter n is live at the exit
of dfTraverse. Figure 1.3(b) shows a safe approximation of liveness for handling
interprocedural eﬀects: In block n3, it is assumed that any access path rooted at the
actual parameter succ becomes live due to the call made in n3. Similarly, it is as-
sumed that any path rooted at the formal parameter n is live at end of dfTraverse
because it may be accessed in a caller’s body using the actual parameter.
Figure 1.3(c) shows how the function dfTraverse can be represented to facili-
tate interprocedural analysis. It models function calls by splitting them into a call
node and a return node and by adding an edge from the call node to the start of the
© 2009 by Taylor & Francis Group, LLC

An Introduction to Data Flow Analysis
5
n1 succ=n->child n1
n2 if (succ!=NULL) n2
n3
Read succ
n3
n4 next=succ->sib n4
n5 succ=next n5
printf(”%d\n”,n->num)
Assume that n
 is dead
n6
T
F
n1 succ=n->child n1
n2 if (succ!=NULL) n2
n3
Read succ->
n3
n4 next=succ->sib n4
n5 succ=next n5
printf(”%d\n”,n->num)
Assume that n
 is live
n6
T
F
n1 succ=n->child n1
n2 if (succ!=NULL) n2
n3
Call n=succ
Return succ=n
n3
n4 next=succ->sib n4
n5 succ=next n5
printf(”%d\n”,n->num)
n6
T
F
(a) Intraprocedural analysis
ignoring
interprocedu-
ral eﬀects.
(b) Intraprocedural analysis
with interprocedural ap-
proximation.
(c) Interprocedural analysis.
Function main has not
been shown.
FIGURE 1.3
Modelling interprocedural eﬀects in liveness analysis for the program in Figure 1.1.
called procedure and an edge from the end of the called procedure to the return node.
A call node maps the actual parameters to the formal parameters. During liveness
analysis, the call node in block n3 transfers the liveness of the formal parameter n
in the callee’s body (dfTraverse) to the liveness of the actual parameter succ in
the caller’s body (also dfTraverse). In our example, the callee does not return any
value. However, since the parameter of dfTraverse is a pointer variable, the return
node in block n3 transfers the liveness of the actual parameter succ in the caller’s
body to the liveness of the formal parameter n in the callee’s body.
For simplicity of exposition, we ﬁrst show the liveness analysis for simple in-
traprocedural analysis (modeled in Figure 1.3(a)). Then we show the eﬀect of incor-
porating the interprocedural approximation (modeled in Figure 1.3(b)). Finally we
show a simple interprocedural liveness analysis (modeled in Figure 1.3(c)).
In the later part of the book, a solution of the simple intraprocedural liveness anal-
ysis of our example program as well as intraprocedural liveness analysis with inter-
procedural summarization has been presented in Section 4.4.5. Common variants of
interprocedural data ﬂow analysis are later introduced in Section 7.6 and Section 9.5
presents interprocedural liveness analysis of our example.
Simple Intraprocedural Liveness Analysis
As described before, simple intraprocedural analysis disregards the interprocedural
eﬀects completely. Thus it is assumed that no access path is live at the end of the
procedure. Liveness information at the end of the ﬁrst iteration is:
© 2009 by Taylor & Francis Group, LLC

6
Data Flow Analysis: Theory and Practice
Block
Liveness at
Exit
Liveness at
Entry
Remark
n6
∅
{n}
Liveness of n is generated.
n5
∅
{next}
Liveness of next is generated.
n4
{next}
{succ
sib}
Liveness of next is killed.
Liveness of succ
sib is generated.
n3
{succ
sib}
{succ
sib}
Liveness of succ is generated.
Liveness of succ
sib is propagated.
n2
{n,succ
sib}
{n,succ
sib}
Liveness is propagated.
n1
{n,succ
sib}
{n
child
}
Liveness of succ
sib is transferred
to n and is summarized.
Liveness computation in block n1 illustrates the process of transferring liveness
from one access path to the other access paths. The target objects of succ at the exit
of n1 are target objects of n
child at the entry of n1. Hence the live access path
succ
sib from the exit of n1 is transferred to the entry of n1 as n
child
sib which is
then summarized to n
child
; this also subsumes the unchanging live access path
n. The process of transfer is described as follows:
If access path a
σ is live after an assignment a = b, then σ is trans-
ferred to b and the access path b
σ becomes live before the assignment.
Data ﬂow information converges in the third iteration as shown below. In the
second iteration, liveness information {n,succ
sib} at the entry of n2 is propagated
to the exit of n5 along the back edge. The assignment in n5 does not aﬀect n, but
the access paths succ
sib cease to be live before n5 due to the assignment to succ
and the liveness of succ
sib is transferred as the liveness of next
sib before the
assignment. Computing liveness in n4 involves transfer followed by summarization.
Block
Liveness in iteration 2
Liveness in iteration 3
At Exit
At Entry
At Exit
At Entry
n6
∅
{n}
∅
{n}
n5
{n,succ
sib}
{n,next
sib}
{n,succ
sib
} {n,next
sib}
n4
{n,next
sib}
{n,succ
sib
} {n,next
sib}
{n,succ
sib
}
n3
{n,succ
sib
} {n,succ
sib
} {n,succ
sib
} {n,succ
sib
}
n2
{n,succ
sib
} {n,succ
sib
} {n,succ
sib
} {n,succ
sib
}
n1
{n,succ
sib
} {n
child
}
{n,succ
sib
} {n
child
}
Intraprocedural Analysis with Interprocedural Approximation
Interprocedural approximation assumes that n
 is live at the end of dfTraverse
and succ
 is live just before the recursive call. Due to this approximation, the
analysis terminates in two iterations.
© 2009 by Taylor & Francis Group, LLC

An Introduction to Data Flow Analysis
7
Block
Liveness in iteration 2
Liveness in iteration 3
At Exit
At Entry
At Exit
At Entry
n6
{n
}
{n
}
{n
}
{n
}
n5
∅
{next}
{n
,succ
} {n
,next
}
n4
{next}
{succ
sib}
{n
,next
} {n
,succ
}
n3
{succ
sib}
{succ
}
{n
,succ
}{n
,succ
}
n2
{n
,succ
} {n
,succ
} {n
,succ
} {n
,succ
}
n1
{n
,succ
} {n
}
{n
,succ
} {n
}
Interprocedural Analysis
For interprocedural analysis, we split block n3 into a call block n3.call and a return
block n3.ret and compute liveness at the entries and exits of these blocks. The initial
value is ∅. No access path is live after the call to dfTraverse in function main. The
liveness information after ﬁrst two iterations is:
Block Liveness in iteration 2
Liveness in iteration 3
At Exit
At Entry
At Exit
At Entry
n6
∅
{n}
{n
sib}
{n
sib}
n5
∅
{next}
{n,succ}
{n,next}
n4
{next}
{succ
sib} {n,next}
{n,succ
sib}
n3.ret
{succ
sib} {n
sib}
{n,succ
sib}
{n
sib}
n3.call
∅
∅
{n
child}
{succ
child}
n2
{n}
{n,succ}
{n
sib,succ
child} {n
sib,succ
child}
n1
{n,succ}
{n
child}
{n
sib,succ
child} {n
sib,n
child
}
In the second iteration, {n
sib} is propagated from the entry of n3.ret to the exit of
n6 and {n
child} is propagated from the entry of n1 to the exit of n3.call. Further, the
transfer in block n1 causes summarization in the second iteration.
Block
At Exit
At Entry
Iteration 3
n6
{n
sib}
{n
sib}
n5
{n
sib,succ
child}
{n
sib,next
child}
n4
{n
sib,next
child}
{n
sib,succ
sib
}
n3.ret {n
sib,succ
sib
}
{n
sib
}
n3.call {n
sib,n
child
}
{succ
sib,succ
child
}
n2
{n
sib,succ
sib,succ
child
}
{n
sib,succ
sib,succ
child
}
n1
{n
sib,succ
sib,succ
child
}
{n
sib,n
child
}
Iteration 4
n6
{n
sib
}
{n
sib
}
n5
{n
sib,succ
sib,succ
child
}
{n
sib,next
sib,next
child
}
n4
{n
sib,next
sib,next
child
}
{n
sib,succ
sib
}
n3.ret {n
sib,succ
sib
}
{n
sib
}
n3.call {n
sib,n
child
}
{succ
sib,succ
child
}
n2
{n
sib
,succ
sib,
succ
child
}
{n
sib
,succ
sib,
succ
child
}
n1
{n
sib
,succ
sib, succ
child
} {n
}
© 2009 by Taylor & Francis Group, LLC

8
Data Flow Analysis: Theory and Practice
Block
At Exit
At Entry
Iteration 5
n6
{n
sib
}
{n
sib
}
n5
{n
sib
,succ
sib,
succ
child
}
{n
sib
,next
sib,
next
child
}
n4
{n
sib
,next
sib,next
child
}
{n
sib
,succ
sib
}
n3.ret {n
sib
,succ
sib
}
{n
sib
}
n3.call {n
}
{succ
}
n2
{n
sib
,succ
}
{n
sib
,succ
}
n1
{n
sib
,succ
}
{n
}
Iteration 6
n6
{n
sib
}
{n
sib
}
n5
{ n
sib
,succ
}
{ n
sib
,next
}
n4
{ n
sib
,next
}
{ n
sib
,succ
sib
}
n3.ret { n
sib
,succ
sib
}
{n
sib
}
n3.call {n
}
{succ
}
n2
{ n
sib
,succ
}
{ n
sib
,succ
}
n1
{ n
sib
,succ
}
{n
}
It can be veriﬁed that the seventh iteration results in the same liveness at each pro-
gram point indicating convergence.
A Comparison of Liveness Computed by Three Methods
We reproduce below the liveness information computed by the three methods.
Program
Point
Intraprocedural Analysis
Interprocedural Analysis
Simple
Interprocedural
Approximation
n6
Exit
∅
∅
{n
sib
}
Entry
{n}
{n
}
{n
sib
}
n5
Exit
{n, succ
sib
}
{n
, succ
}
{ n
sib
, succ
}
Entry
{n, next
sib}
{n
, next
}
{ n
sib
, next
}
n4
Exit
{n, next
sib}
{n
, next
}
{ n
sib
, next
}
Entry
{n, succ
sib
}
{n
, succ
}
{ n
sib
,succ
sib
}
n3
Exit
{n, succ
sib
}
{n
, succ
}
{ n
sib
,succ
sib
}
Entry
{n, succ
sib
}
{n
, succ
}
{succ
}
n2
Exit
{n, succ
sib
}
{n
, succ
}
{ n
sib
, succ
}
Entry
{n, succ
sib
}
{n
, succ
}
{ n
sib
, succ
}
n1
Exit
{n, succ
sib
}
{n
, succ
}
{ n
sib
, succ
}
Entry
{n
child
}
{n
}
{n
}
It is easy to see that the simple intraprocedural analysis fails to record some access
paths as live. For example, access path n
sib is live at the end of dfTraverse. This
is because the procedure traverses the next sibling of n after traversing n. However,
the simple intraprocedural analysis concludes that it is not live. When interprocedu-
ral summarization is included, it records n
sib as live at the end of the procedure but
© 2009 by Taylor & Francis Group, LLC

An Introduction to Data Flow Analysis
9
it also marks n
child as live. The interprocedural analysis correctly recognizes that
only n
sib is live at the end of the procedure.
1.1.3
Computing Aliases
Computing alias information is simpler compared to liveness for this example be-
cause there are no interprocedural eﬀects. This is because unlike liveness which is
a property of an access path, aliasing at a program point is a relation between two
access paths that are visible at that program point. Since there are no global vari-
ables, and no assignments to formal parameter in our example, aliases created in
dfTraverse are restricted to a single activation.
Aliasing of access paths. Access path ρ1 and ρ2 are aliased to each other at a pro-
gram point u, denoted ρ1  ρ2, if their targets are same at u along some control
ﬂow path reaching u.
Aliasing information at a program point is represented using a set of alias pairs
ρ1  ρ2. Since an alias holds at a program point u if it holds along some predecessor
of u, we use union to combine sets of alias pairs and use its identity (∅) as the initial
value. Unlike liveness analysis, aliasing information at a program point u depends
on the aliases at predecessors of u. Hence we traverse control ﬂow graph along
the control ﬂow for faster convergence of successive reﬁnements. This implies the
following order: n1, n2, n3, n4, n5, and n6.
The aliases at the end of ﬁrst iteration are as shown below:
Block
Aliases at Entry
Aliases at Exit
Remark
n1
∅
{succ  n
child}
Generation
n2
{succ  n
child}
{succ  n
child}
Propagation
n3
{succ  n
child}
{succ  n
child}
Propagation
n4
{succ  n
child}
{succ  n
child,
next  succ
sib
next  n
child
}
Propagation, transfer
and summarization
n5
{succ  n
child,
next  succ
sib
next  n
child
}
{succ  next,
succ  n
child
,
next  n
child
}
Generation,
killing and
transfer
n6
{succ  n
child}
{succ  n
child}
Propagation
Observe the eﬀect of assignment next = succ->sib in block n4 on the aliases
at the entry of n4. Since succ is aliased to n
child, next gets aliased to n
child
.
This is analogous to the transfer in liveness. In n5, since assignment succ = next
modiﬁes succ, alias succ  n
child ceases to hold at the exit of n6. Two new aliases
succ  next and succ  n
child
 are created.
The second iteration causes aliases from the exit of n5 to be propagated to the entry
of n2 and some more aliases to be generated as a consequence of transfer. Since succ
is aliased to n
child
 in block n4, next gets aliased to n
child
.
© 2009 by Taylor & Francis Group, LLC

10
Data Flow Analysis: Theory and Practice
Block
Aliases at Entry
Aliases at Exit
n1
∅
{succ  n
child}
n2
{succ  next,succ  n
child

next  n
child
}
{succ  next,succ  n
child
,
next  n
child
}
n3
{succ  next,succ  n
child
,
next  n
child
}
{succ  next,succ  n
child
,
next  n
child
}
n4
{succ  next,succ  n
child
,
next  n
child
}
{succ  n
child
,
next  succ
sib,
next  n
child
}
n5
{succ  n
child
,next  succ
sib,
next  n
child
}
{succ  next,succ  n
child
,
next  n
child
}
n6
{succ  next,succ  n
child
,
next  n
child
}
{succ  next,succ  n
child
,
next  n
child
}
It can be veriﬁed that the third iteration does not compute any new aliases.
1.1.4
Performing Optimization
Figure 1.4 summarizes the ﬁnal data ﬂow information which enables the desired
optimization. Access path succ is not live at the exit of n4 and the entry of n5.
Further at these points none of the access paths that it is aliased to are live. Thus
the object pointed to by it can be freed. Although next is not live in blocks n2, n3,
and n4, it is aliased to a live access path and hence its target cannot be freed. An
alternative place for deallocating succ is block n6. The diﬀerence between the two
deallocations is that the former will be performed after a call to dfTraverse is over
while the latter will be performed just before the end of a call.
1.1.5
General Observations
At the entry of n5, access path succ is not live. It is aliased to n
child which is not
live either. If function main is modiﬁed to access tree->child after the call to
dfTraverse as shown below, then n
child will be live at the exit of n6.
0.
void main()
1.
{
Tree *tree;
2.
tree = createTree();
3.
printEdges(tree);
printf("%d\n",tree->child->num);
4.
}
Since liveness of n
child is not aﬀected by the assignment in n5, it will be live at
the entry of n5 too. Thus, with this change, succ cannot be freed. Interestingly, this
change accesses only object o2 outside of function dfTraverse but prohibits freeing
any object in dfTraverse. This is because the same statements in dfTraverse are
used to access all objects and unless the code is rewritten to access o2 and other
objects diﬀerently, selective freeing is not feasible.
© 2009 by Taylor & Francis Group, LLC

An Introduction to Data Flow Analysis
11
Program
Point
Interprocedural Liveness
Aliases
n1 Entry {n
}
∅
Exit
{n
sib
,succ
}
{succ  n
child}
n2 Entry {n
sib
,succ
}
{succ  next,succ  n
child
,
next  n
child
}
Exit
{n
sib
,succ
}
{succ  next,succ  n
child
,
next  n
child
}
n3 Entry {succ
}
{succ  next,succ  n
child
,
next  n
child
}
Exit
{n
sib
,succ
sib
}
{succ  next,succ  n
child
,
next  n
child
}
n4 Entry {n
sib
,succ
sib
}
{succ  next,succ  n
child
,
next  n
child
}
Exit
{n
sib
,next
}
{succ  n
child
,next  succ
sib,
next  n
child
}
n5 Entry {n
sib
,next
}
{succ  n
child
,next  succ
sib,
next  n
child
}
Exit
{n
sib
,succ
}
{succ  next,succ  n
child
,
next  n
child
}
n6 Entry {n
sib
}
{succ  next,succ  n
child
,
next  n
child
}
Exit
{n
sib
}
{succ  next,succ  n
child
,
next  n
child
}
FIGURE 1.4
Liveness and alias information in function dfTraverse.
This brings out the concept of safety of data ﬂow analysis and the conservative
approximations which are used to achieve safety. Since liveness is used to prohibit
freeing of objects, it is safer to include spurious access paths as live. Missing a live
access path could lead to incorrect optimization. Data ﬂow information is required to
represent all possible executions on all possible inputs. Hence the concept of approx-
imation depends on the intended use of the data ﬂow information. Approximations
performed by data ﬂow analysis can be characterized by the following two proper-
ties: exhaustiveness and safety.
Data ﬂow information is exhaustive if it does not
miss any optimization opportunity; it is safe if it does not enable optimizations that
do not preserve program semantics. In the context of liveness analysis, exclusion of
an access path that is actually live is an approximation towards exhaustiveness be-
cause it facilitates freeing a larger number of objects; however, this may be unsafe.
In contrast, inclusion of an access path that is not live is an approximation towards
safety because it prohibits freeing objects thereby preserving program semantics.
The goal of data ﬂow analysis is to compute the most exhaustive safe information.
© 2009 by Taylor & Francis Group, LLC

12
Data Flow Analysis: Theory and Practice
The interprocedural analysis performed by us is context insensitive because it does
not distinguish between diﬀerent calling contexts. In our original example, n be-
comes live at the exit of dfTraverse in those activations of dfTraverse that are
invoked through the recursive call. It is not live at the end of the outermost activation
of dfTraverse made through main. A context sensitive interprocedural analysis
can make this distinction. However, exploiting this distinction requires rewriting
the code in a non-trivial manner. Otherwise, the data ﬂow information reaching at
a program point along diﬀerent contexts will have to be merged. This highlights
the limitation of transformations performed statically. In any case, merging the in-
formation discovered by context sensitive analysis generally results in more precise
information than the information computed by context insensitive analysis.
The alias analysis performed by us is ﬂow sensitive because it propagates aliases
along the control ﬂow. A ﬂow insensitive alias analysis disregards the control ﬂow
and assumes that the aliases discovered hold at all program points. Such an anal-
ysis visits each block only once and accumulates the aliases discovered, no aliases
can be killed. For our example, the ﬂow insensitive aliases are: succ  n
child
,
succ  next, next  succ
sib, and next  n
child
. This alias information pro-
hibits freeing the target of succ at the entry of n5 because it is aliased to next which
is live at that point.
We have summarized the access paths n, n
child, n
child
sib, n
child
child,
n
child
sib
sib, ... by n
child
. It is clear that some kind of summarization is
essential because statically it is not possible to know how many such access paths
need to be created by analysis. However for precision, the process of summarization
should keep as many access paths distinct in the summary information as is possible.
Further, these summaries have to be constructed automatically by data ﬂow analysis.
Ensuring convergence on safe summaries requires creating suitable representation
for data ﬂow information and devising appropriate operations on the chosen repre-
sentation. In the case of stack and static data, building summaries is simpler because
the mapping between names and addresses does not change during the lifetime of a
name and hence names can be directly used to represent data. Section 4.4.3 shows
how access paths for heap data can be summarized using graphs.
1.2
Program Analysis: The Larger Perspective
Program analyses cover a large spectrum of motivations, basic principles, and meth-
ods. Diﬀerent approaches to program analysis diﬀer in details but at a conceptual
level, almost all program analyses are characterized by some common properties.
Although these properties are abstract, they provide useful insights about a particu-
lar analysis. A deeper understanding of the analysis would require exploring many
more analysis-speciﬁc details.
© 2009 by Taylor & Francis Group, LLC

An Introduction to Data Flow Analysis
13
Applications of Analysis
The uses of information derived by program analyses can be broadly classiﬁed as:
• Determining the validity of a program. An analysis may be used to validate
programs with regard to some desired properties (viz. type correctness).
• Understanding the behaviour of a program. An analysis may discover useful
properties of programs required for debugging, maintenance, veriﬁcation, or
testing etc. Abstract interpretation, slicing, ripple analysis, test data generation
etc. are the common examples of such analyses.
• Transforming a program. Most analyses enable useful transformations to be
performed on programs. Traditionally, the term program analysis has been
used for the analyses that facilitate transforming a program within the same
given representation. These transformations may be aimed at optimizing the
program for space, time, or power consumption. Note that analyses such as
lexical and syntax analyses transform a program representation into another
representation and are not included in the class of program analyses.
• Enabling program execution. Program analysis can also be used for determin-
ing the operations implied by a program so that the program can be executed
(viz. dynamic type inferencing).
Approaches to Program Analysis
Some of the common paradigms of program analysis are:
• Inference Systems consisting of a set of axioms and inductive and composi-
tional deﬁnitions constituting rules of inference.
In such systems, the properties are inferred by repeatedly discovering the
premises that are satisﬁed by the program components of interest and by in-
voking appropriate rules of inference. Note that there is no algorithm that
suggests appropriate choice of rules; it is left to the creativity of the user of
such a system. As a consequence, such systems may not be decidable.
Typically, the inference systems are converted to constraint based system (de-
scribed below) and constraint resolution algorithms are used for inference.
• Constraint Resolution Systems consisting of a constraint store and a logic for
solving constraints.
In such systems, a program component constrains the semantic properties.
These constraints are expressed in form of inequalities and the semantics prop-
erties are derived by ﬁnding a solution which satisﬁes all the constraints.
Often these constraints take advantage of the temporal or spatial structures of
data and operations by grouping the related constraints together. Traditionally
they have been unconditional, and are called ﬂow-based constraints because
they have been solved by traversals over trees or general graphs. Grouping of
© 2009 by Taylor & Francis Group, LLC

14
Data Flow Analysis: Theory and Practice
structured constraints often leads to replacing groups of related inequalities by
equations. Structured constraints often lead to more eﬃcient analyses, both in
terms of time as well as space.
• Model Checking requires creating suitable abstractions of programs as mod-
els and the desired properties are expressed in terms of boolean formulae. A
model checking algorithm then discovers the states in the mode that satisfy the
given formulae.
• Abstract Interpretations use abstraction functions to map the concrete seman-
tics values to abstract semantics, perform the computations on the abstract se-
mantics, and use concretization functions to map the abstract semantics back to
the concrete semantics. The theory of abstract interpretation provides mecha-
nisms to show the soundness of the abstraction functions. The most interesting
aspect of this approach is that the algorithms for performing analysis emerge
from the construction of abstraction functions.
This is unlike inference systems, constraints resolution systems, and model
checking, where the speciﬁcations of analysis are generally based on intu-
itions of semantics instead of being derived formally from concrete seman-
tics. Hence these three approaches require separate algorithms that perform
the speciﬁed analyses.
Other approaches like those involving denotational semantics or logic are relatively
less common.
In general an analysis can be expressed in any of the above approaches.
Time of Performing Analysis
An analysis performed before the execution of a program is termed static analysis,
whereas an analysis performed during the execution of a program (in an interleaved
fashion) is termed dynamic analysis. Thus an interpreter can perform static analy-
sis (by analyzing a program just before execution) as well as dynamic analysis (by
analyzing the program during execution). A compiler, however, can perform static
analysis only; for dynamic analysis, a compiler must embed extra code in the com-
piled program as a part of run time support.
In principle, the choice between static and dynamics analysis is governed by the
availability of information on which the analysis depends, the amount of precision
required and the permissible run time overheads.
An analysis which depends on run time information is inherently dynamic. For
example, if type annotations can be omitted in a language and type associations
could change at run time, types can be discovered only at run time. This requires
dynamic type inferencing. If some amount of imprecision can be tolerated (viz. if
precise types are not expected but it is only expected to constrain the set of possible
types by ruling out some types before execution), it may be possible to perform
an approximate static analysis for an otherwise inherently dynamic analysis. This
obviates dynamic analysis only if a compromise on the precision of information is
© 2009 by Taylor & Francis Group, LLC

An Introduction to Data Flow Analysis
15
acceptable; otherwise it requires a subsequent dynamic analysis. In any case, it
reduces the amount of dynamic analysis and hence, run time overheads.
If run time overheads are a matter of concern, dynamic analyses should be either
avoided or preceded by corresponding (approximate) static analyses. This often is
the case and it should not come as a surprise that, in practice a majority of analyses
performed by language processors are indeed static. Besides, many dynamic analy-
ses have a static counterpart. For instance, many languages require array bounds to
be checked at run time; optimizing compilers can minimize these checks by a static
array bound checking optimization.
Scope of Analysis
Programs can be viewed as hierarchical constructions consisting of structures and
sub-structures. Program analyses try to discover information about a program struc-
ture by correlating the information discovered for constituent sub-structures. As
such, an analysis may be conﬁned to a small sub-structure like an expression, a
statement, or to larger sub-structure like a group of statements or function/procedure
blocks, or to still larger structures like modules or entire programs. The nature of
analysis for the structures and the sub-structures may be diﬀerent. The sub-structures
that belong to the same structure are analyzed independently. Analysis of a structure
and its sub-structure may be interleaved or may be non-overlapping (and cascaded);
in either case, the larger structure can be analyzed only after their constituent sub-
structures. For example, the liveness analysis performed in Section 1.1 requires anal-
ysis of basic blocks to discover their eﬀects.
Flow Sensitivity of Analysis
If the information discovered by an analysis at a program point depends on the con-
trol ﬂow paths involving the program point and could vary from one program point
to another, then the analysis is ﬂow sensitivity. Otherwise, it is ﬂow insensitive. Type
inferencing in C is ﬂow insensitive whereas that in Ruby is ﬂow sensitive. In general,
ﬂow insensitivity is a compromise on precision for achieving eﬃciency.
Context Sensitivity of Analysis
If the information discovered by an interprocedural analysis for a function could vary
from one calling context of the function to another, then the analysis is context sen-
sitive. A context insensitive analysis does not distinguish between diﬀerent calling
contexts and computes the same information for all calling contexts of a function.
Context insensitivity is also a compromise on precision for achieving eﬃciency.
Granularity of Performing Analysis
An exhaustive analysis derives information starting from scratch whereas an incre-
mental analysis updates the previously derived information to incorporate the eﬀect
of some changes in the programs. These changes may be caused by transforma-
tions (typically for optimization) or by user edits (typically in programming environ-
© 2009 by Taylor & Francis Group, LLC

16
Data Flow Analysis: Theory and Practice
ments). In general, an incremental analysis must be preceded by at least one instance
of the corresponding exhaustive analysis.
Program Representations Used for Analysis
An analysis is typically performed on an intermediate representation of the program.
Though the theoretical discussions of many analyses are in terms of the source code
(viz. in the case of parallelization), in practice these analyses are performed on a
suitable internal representation.
These internal representations diﬀer in their “shapes”: They may be either linear
data structures (viz. a sequence of quadruples), hierarchical data structures (viz.
abstract syntax trees), or general non-linear structures (viz. graphs). The graphs may
capture linear abstractions of control ﬂow (as in CFGs) or hierarchical abstractions
of control ﬂow (as in call graphs).
Single Static Assignment (SSA) form is an interesting representation that does not
belong to the above category. SSA form is used for optimization rather than analysis.
As a matter of fact, it can be viewed as the result of a diﬀerent kind of data ﬂow
analysis that explicates the data ﬂow information in a CFG.
Representations of Information
Most common representations of information are sets. The elements of these sets
may be of states of a model that satisfy given formulae, or program entities that
satisfy the given constraints, or facts that hold at a given program point, or trees or
graphs representing types. In many cases these elements may be pairs of program
entities and the representations of their properties.
Most analyses require these sets to be ﬁnite. Some form of summarization may be
required if these sets are not ﬁnite. Further the representations of individual proper-
ties must also be bounded.
1.3
Characteristics of Data Flow Analysis
Data ﬂow analysis statically computes information about the ﬂow of data (i.e., uses
and deﬁnitions of data) for each program point in the program being analyzed. This
information is required to be a safe approximation of the desired properties of the
run time behaviour of the program during each possible execution of that program
point on all possible inputs.
Data ﬂow analysis is a special case of program analysis and is characterized by
the following:
• Applications. Data ﬂow analysis can be used for
© 2009 by Taylor & Francis Group, LLC

An Introduction to Data Flow Analysis
17
– Determining the semantic validity of a program (viz. type correctness
based on inferencing, prohibiting the use of uninitialized variables etc.)
– Understanding the behaviour of a program for debugging, maintenance,
veriﬁcation, or testing.
– Transforming a program. This is the classical application of data ﬂow
analysis and data ﬂow analysis was originally conceived in this context.
• Approach of Program Analysis. Data ﬂow analysis uses constraint resolution
systems based on equalities. These constraints are often unconditional. The
constraints are called Data Flow Equations.
• Time. Data ﬂow analysis is mostly static analysis. The Just-In-Time (JIT)
compilation and dynamic slicing etc. involve dynamic data ﬂow analysis.
• Scope. Data ﬂow analysis may be performed at almost all levels of scope in
a program. Traditionally the following terms have been associated with data
ﬂow analysis for diﬀerent scopes in the domain of imperative languages:
– Across statements but conﬁned to a maximal sequence of statements with
no control transfer other than fall through (i.e., within a basic block):
Local Data Flow Analysis.
– Across basic blocks but conﬁned to a function/procedure: Global (in-
traprocedural) Data Flow Analysis.
– Across functions/procedures: Interprocedural Data Flow Analysis.
It is also common to use the term local data ﬂow analysis for analysis of a
single statement and global data ﬂow analysis for analysis across statements
in a function/procedure. Eﬀectively, the basic blocks for such analyses consist
of a single statement.
• Flow Sensitivity. Data ﬂow analysis is almost always ﬂow sensitive in that it
computes point-speciﬁc information. In some cases like alias analysis, ﬂow
insensitive analyses are also common.
• Context Sensitivity. Interprocedural data ﬂow analysis can be context sensi-
tive as well as context insensitive. In general, fully context sensitive analysis
is very ineﬃcient and most practical algorithms employ a limited amount of
context sensitivity. Context insensitive data ﬂow analysis is also very common.
• Granularity. Data ﬂow analysis can have exhaustive as well as incremental
versions. Incremental versions of data ﬂow analysis are conceptually more
diﬃcult compared to exhaustive data ﬂow analysis.
• Program Representations. The possible internal representations for data ﬂow
analysis are abstract syntax trees (ASTs), directed acyclic graphs (DAGs), con-
trol ﬂow graphs (CFGs), program ﬂow graphs (PFGs), call multigraphs (CGs),
© 2009 by Taylor & Francis Group, LLC

18
Data Flow Analysis: Theory and Practice
program dependence graphs (PDGs), static single assignment (SSA) forms
etc. The most common representations for global data ﬂow analysis are CFGs,
PFGs, SSA, and PDGs whereas interprocedural data ﬂow analyses use a com-
bination of CGs (and CFGs or PFGs). Though ASTs can and have been used
for data ﬂow analysis, they are not common since they do not exhibit control
ﬂow explicitly.
In this book, we restrict ourselves to CFGs and supergraphs created by con-
necting CFGs of diﬀerent procedures.
• Representation of Data Flow Information. The most common representations
are sets of program entities such as variables or expressions satisfying the
given property. These sets are implemented using bit vectors. Some analy-
ses use sets of pairs of entities and their properties. For example, constant
propagation stores a constantness value for each expression. Some other form
of representations such as access paths require summarization.
1.4
Summary and Concluding Remarks
Data ﬂow analysis is a technique of discovering useful information from programs
without executing them. This information can be put to a variety of uses. Data ﬂow
analysis was conceived in the context of optimization performed by compilers and
to date this remains its most dominant application.
Data ﬂow analysis constructs a static summary of the information that represents
run time behaviour of a program. Precision of this information depends on the for-
mulation of analysis in terms of the representation of information, rules of sum-
marization, and the algorithms used to compute the information. This chapter has
presented a contemporary optimization that demonstrates the importance of these
aspects of data ﬂow analysis. We use access paths as a unit of data ﬂow information
and summarization is based on treating all access paths beyond two ﬁeld names as
identical.
Our formulation of liveness analysis uses sets of access paths as data ﬂow infor-
mation; at a given program point, the data ﬂow information depends on the compu-
tations that occur after the program point in some execution path. The eﬀect of a
statement on the incoming data ﬂow information is incorporated by applying a ﬂow
function. In the case of alias analysis, the data ﬂow information is a set of pairs of
access paths; at a given program point this information depends on the computations
that precede the program point in some execution path. In either case, the data ﬂow
information along diﬀerent paths is combined by taking a union of the sets.
We have also seen that data ﬂow analysis can be restricted to a single procedure
by ignoring function calls or can be performed across procedure boundaries. In the
latter situation, the calling context of a procedure inﬂuences data ﬂow information
© 2009 by Taylor & Francis Group, LLC

An Introduction to Data Flow Analysis
19
and for precision, such an analysis should be context sensitive.
This book builds on the above theme in the following manner:
• Part I presents analysis formulations at the intraprocedural level. This part
describes a large number of data ﬂow problems ranging from the classical
problems to contemporary problems. It also presents generalizations underly-
ing these problems. In particular, it presents the lattice theoretic modeling of
data ﬂow frameworks apart from the generalizations of constant and dependent
parts in ﬂow functions and entity functions as constituents of ﬂow functions.
It shows how these generalizations lead to tight complexity bounds.
The ﬁnal chapter of the ﬁrst part presents SSA representation of programs
which builds an additional layer of abstraction over the control ﬂow graph
representation of programs and directly relates the deﬁnition points and the
use points of data.
• Part II shows how an intraprocedural formulation can be used for interpro-
cedural analysis. The main theme of this part is that the two are orthogonal
and hence we avoid methods that are speciﬁc to a particular application or
a particular data ﬂow framework. This part presents two generic approaches.
The ﬁrst approach is a functional approach that constructs context independent
summary ﬂow functions of procedures. These ﬂow functions are used at the
call points to incorporate the eﬀects of procedure calls. The second approach
is a value-based approach that computes distinct values for distinct calling
contexts; this is achieved by augmenting the data ﬂow values with context
information.
• Part III describes the implementation of a GCC based generic data ﬂow an-
alyzer for bot vectors and shows how particular data ﬂow analyses can be
implemented by writing simple speciﬁcations.
1.5
Bibliographic Notes
Most texts on compilers discuss data ﬂow analysis in varying lengths [3, 10, 40, 75,
76, 105]. Some of them discuss details [3, 10, 76]. An advanced treatment of data
ﬂow analysis can be found in the books by Hecht [44], Muchnick and Jones [77],
and F. Nielson, H. R. Nielson and Hankin [80].
Historically, the practice of data ﬂow analysis precedes the theory. Hecht [44]
reports that the round-robin method of performing data ﬂow analysis can be traced
back to Vyssotsky and Wegner [101]. It was an attempt to discover uses of variables
that were potentially uninitialized in a Bell Laboratories 7090 Fortran II compiler.
This was the ﬁrst variant of an analysis that later came to be known as reaching deﬁ-
nitions analysis. We describe this analysis in Chapter 2. A more powerful variant of
this analysis considers transitive eﬀects of assignments and is described in Chapter 4.
© 2009 by Taylor & Francis Group, LLC

20
Data Flow Analysis: Theory and Practice
The problem of early deallocation of heap memory is an important optimization
and has been attempted in many diﬀerent ways. The fact that there is ample scope
for performing such an optimization has been well established [1, 90, 91, 89, 52].
Some approaches to this optimization attempt to allocate objects on stack when pos-
sible [73, 81, 15, 16, 23]. This ensures that the memory is automatically deallocated
when activation records are popped oﬀthe control stack.
Among earliest data ﬂow analyses, Kennedy [55] presented liveness analysis for
scalar variables and since then it has been discussed thoroughly in the literature.
Liveness of heap data was ﬁrst approximated by Agesen, Detlefs and Moss [1] by
performing liveness of root variables on the stack that point to heap data. A more
precise liveness analysis for heap cells was formulated recently by Khedker, Sanyal
and Karkare [62].
The concept of aliasing was ﬁrst studied in the context of interprocedural analysis
for discovering the side eﬀects of function calls. Cooper [25] introduced aliasing in
the context formal parameters. Later aliasing of pointers was studied in details. We
list references in the bibliographic notes of Chapter 4.
Cocke [24], Ullman [100], Allen [4, 5], and Kennedy [55, 56] were the earliest
researchers in intraprocedural data ﬂow analysis. The most inﬂuential work in in-
traprocedural analysis is the classical work by Kildall [63] and Kam and Ullman [49].
Spillman [94], Allen [6], Barth [13] and Banning [12] were the earliest researchers
to study interprocedural data ﬂow analysis. This was motivated by the side eﬀect
analysis. The most inﬂuential work on interprocedural data ﬂow analysis is the clas-
sical work by Sharir and Pnueli [93].
© 2009 by Taylor & Francis Group, LLC

Part I
Intraprocedural Data Flow
Analysis
21
© 2009 by Taylor & Francis Group, LLC

© 2009 by Taylor & Francis Group, LLC

2
Classical Bit Vector Data Flow Analysis
Data ﬂow analysis originated with what was later termed as “bit vector” data ﬂow
frameworks. The term “bit vector” arises from the fact that not only can the data
ﬂow information be represented using bit vectors, it can also be computed using bit
vector operations alone. There are data ﬂow former for which although the data ﬂow
information can be represented using bit vectors, computing it requires additional
operations. We make this notion more precise in the chapter summary with the help
of the examples presented in the chapter.
2.1
Basic Concepts and Notations
Data ﬂow analysis views computation of data through expressions and transition of
data through assignments to variables. Properties of programs are deﬁned in terms
of properties of program entities such as expressions, variables, and deﬁnitions ap-
pearing in a program. In this chapter, we restrict expressions to primitive expressions
involving a single operator. Variables are restricted to scalar variables and deﬁnitions
are restricted to assignments made to scalar variables. Data ﬂow analyses of other
program entities such as composite expressions, array variables, pointer variables,
statement numbers etc. have also been devised; we present some of them in later
chapters.
For a given program entity such as an expression, data ﬂow analysis of a program
involves the following two steps (a) discovering the eﬀect of individual statements
on the expression, and (b) relating these eﬀects across statements in the program.
For reasons of eﬃciency, both these steps are often carried over a basic block instead
of a single statement. A basic block is a maximal group of consecutive statements
that are always executed together with a strictly sequential control ﬂow between
them. Step (a) is called local data ﬂow analysis and is performed for a basic block
only once. Step (b) constitutes global data ﬂow analysis∗and may require repeated
traversals over basic blocks in a CFG. Since global analysis correlates local proper-
ties, combining local analysis of several statements together and performing global
∗Observe that the term global data ﬂow analysis is restricted to data ﬂow analysis of a single procedure.
23
© 2009 by Taylor & Francis Group, LLC

24
Data Flow Analysis: Theory and Practice
analysis over the resulting basic blocks rather than individual statements implies
lesser work for global analysis.
Relating the eﬀects across basic blocks involves propagating data ﬂow informa-
tion from a basic block to another along the direction of control ﬂow or against it.
Propagation along the direction of control ﬂow constitutes a forward ﬂow whereas
propagation against the direction of control ﬂow constitutes a backward ﬂow. As
observed in Sections 1.1.2 and 1.1.3, liveness analysis involves backward ﬂows and
alias analysis involves forward ﬂows.
Global data ﬂow information is associated with the entry and exit points of a basic
block. For block n these points are denoted by Entry(n) and Exit(n); they represent
the possible states of the program just before the execution of the ﬁrst statement and
the just after the execution of the last statement in block n. Data ﬂow information
associated with them is usually denoted by Inn and Outn. For bit vector frameworks,
the local data ﬂow information is usually expressed in terms of Genn and Killn. Genn
denotes the data ﬂow information which is generated within block n whereas Killn
denotes the data ﬂow information which becomes invalid in block n.
The relationship between local and global data ﬂow information for a block (i.e.,
Genn, Killn, Inn, and Outn) and between global data ﬂow information across dif-
ferent blocks is captured by a system of linear simultaneous equations called data
ﬂow equations. In general, these equations have multiple solutions. This makes it
important to choose the initial values of Inn and Outn carefully.
Edges in CFGs denote the predecessor and successor relationships: If there is an
edge n1 →n2, then n1 is a predecessor of n2 and n2 is a successor of n1. Observe
that this is diﬀerent from the notions of ancestors and descendants which are the
transitive closures of predecessors and successors respectively. Predecessors and
successors of a block n are denoted by pred(n) and succ(n) respectively.
We assume that the CFG has two distinguished unique nodes: Start which has no
predecessor and End which has no successor. If such nodes do not exist, dummy
nodes can be added without aﬀecting the program semantics. It is further assumed
that every basic block n is reachable from the Start block and that the End block is
reachable from n. We use the terms nodes and blocks interchangeably.
2.2
Discovering Local Data Flow Information
The manner in which the eﬀect of a statement is modeled varies from one analysis
to another. In any case, there is a common pattern of generation of data ﬂow infor-
mation or invalidation of data ﬂow information. In this chapter we are interested in
the following entities and operations related to data ﬂow analysis:
© 2009 by Taylor & Francis Group, LLC

Classical Bit Vector Data Flow Analysis
25
Entity
Operations
Variable x ∈Var
Reading the value of x
Modifying the value of x
Expression e ∈Expr
Computing e
Modifying an operand of e
Deﬁnition di : x = e,
Occurrence of di
Any deﬁnition of x
di ∈Defs, x ∈Var,
e ∈Expr
Reading the value of a variable is also termed as the use of the variable. A variable
may be used or an expression may be computed (a) in the right hand side of an
assignment statement, (b) in a condition for altering ﬂow of control, (c) as an actual
parameter in a function call, or (d) as a return value from a function. All other
operations in the above table involve an assignment statement to a relevant variable.
Note that reading a value of a variable from input can be safely considered as an
assignment statement assigning an unknown value to the variable.
The set Genn and Killn are computed from the operations described above. It is
easy to see that the operation in one column nulliﬁes the eﬀect of the operation in
the other column. From that viewpoint, the operation in one column is an inverse of
the operation in the other column. Computing Genn and Killn requires identifying
operations that are exposed in the direction of analysis i.e., are not followed by the
inverse operation in the direction of analysis. For forward problems, we are inter-
ested in the operations that are downwards exposed and for the backward problems
we are interested in the operations that are upwards exposed. This is illustrated by
the following example.
Example 2.1
Consider an assignment statement x = x+1.
In this statement, the use of
variable x and the computation of expression x + 1 are upwards exposed be-
cause they are not preceded by a modiﬁcation of the value of x. They are not
downwards exposed because they are followed by a modiﬁcation of the value
of x. As a contrasting example, the use of x and computation of x+1 are both
upwards and downwards exposed in an assignment y = x+1 if x and y do not
have the same address (i.e., they are not aliased).
Traditionally, the deﬁnitions of Genn and Killn have not been symmetric with re-
spect to the chosen operation. In particular, the operations which contribute to Genn
are required to be downwards exposed for forward ﬂows and upwards exposed for
backward ﬂows. The operations which contribute to Killn may be preceded or fol-
lowed by their inverses. We explain this asymmetry later in the speciﬁc contexts of
the data ﬂow problems presented in this chapter.
Local property computation isolates global analysis from the intermediate repre-
sentation (IR) in that it is the former which needs to examine the IR statements. In
practice, IRs in real compilers are very complicated since they need to store a lot of
information about each statement across diﬀerent phases of a compiler. Hence local
property computations are tedious and error-prone. Global data ﬂow analyzers are
relatively much simpler and cleaner.
© 2009 by Taylor & Francis Group, LLC

26
Data Flow Analysis: Theory and Practice
2.3
Discovering Global Properties of Variables
In this section, we describe two analyses involving variables: Live Variables Analysis
and Reaching Deﬁnitions Analysis. Although we have listed a deﬁnition as a separate
entity, here we club its analysis with those of variables.
2.3.1
Live Variables Analysis
Section 1.1.2 has introduced liveness analysis for heap data. Liveness analysis for
scalar variables essentially involves determining whether a variable is used in future
and is relatively much simpler because it does not have to consider pointer derefer-
encing.
DEFINITION 2.1
A variable x ∈Var is live at a program point u if some
path from u to End contains a use of x which is not preceded by its deﬁnition.
The data ﬂow equations which deﬁne live variables analysis are:
Inn = (Outn −Killn) ∪Genn
(2.1)
Outn =

BI
n is End block

s∈succ(n)
Ins otherwise
(2.2)
where Inn, Outn, Genn, Killn, and BI are sets of variables.
Liveness at Exit(End) is represented by BI. This is required because diﬀerent
categories of variables have to be treated diﬀerently. Local variables are not live
at Exit(End) whereas liveness of the return value, global variables, and parameters
passed by reference depends on the calling contexts. If there is no interprocedural
analysis, all variables other than local variables are assumed to be live. We assume
that all our analyses in Part I are restricted to local entities only. Thus we will deﬁne
BI for local entities only. Under the assumption of parameter passing by value as in
C, this also allows us to ignore function calls completely.
Observe the use of ∪in Equation (2.2). It essentially means that the liveness
information at Exit(n) is a superset of the liveness information at Entry(s) where s
is a successor of n. This is consistent with the “any path” nature of the deﬁnition
of liveness: Subsequent use along a single path is suﬃcient to make a variable live.
Further, since data ﬂow information at a node depends on the successor nodes, this
is a backward data ﬂow problem.
Genn contains the variables whose liveness is generated within n. Clearly, these
variables have upwards exposed uses in n. Killn contains the variables whose live-
ness is killed in n. These are the variables which appear on the left hand side of
an assignment anywhere in n. Observe that Genn and Killn need not be mutually
exclusive.
© 2009 by Taylor & Francis Group, LLC

Classical Bit Vector Data Flow Analysis
27
n1
b1: b = 4;
a1: a = b +c;
d1: d = a ∗b;
n1
n2 b2: b = a −c; n2
n3 c1: c = b+c; n3
n4 c2: c = a∗b;
f(a−b);
n4
n5 d2: d = a+b; n5
n6 f(b+c); n6
n7 g(a +b); n7
n8 h(a −c);
f(b+c); n8
Var = {a,b,c,d}
Defs = {a1,b1,b2,c1,c2,d1,d2}
Expr = {a ∗b,a +b,a −b,a−c,b+c}
FIGURE 2.1
Program for illustrating bit vector data ﬂow frameworks.
Example 2.2
In Figure 2.1, variable c is contained in both Genn3 and Killn3.
In general, assuming that variable x is live at Exit(n), there are four possibilities
with four distinct semantics:
Case
Local Information
Eﬀect on Liveness
1
x  Genn
x  Killn
Liveness of x is unaﬀected in block n
2
x ∈Genn
x  Killn
Liveness of x is generated in block n
3
x  Genn
x ∈Killn
Liveness of x is killed in block n
4
x ∈Genn
x ∈Killn
Liveness of x is unaﬀected in block
n in spite of x being modiﬁed in n.
Variable x is live at Entry(n) in cases 1, 2, and 4 but the reason for its liveness is
diﬀerent in each case. In particular, case 4 captures the fact that the liveness of x is
killed in n but is re-generated within n. The reason why this needs to be distinguished
from case 1 and case 2 is that in some instances, it is important to know whether the
value of a variable is modiﬁed in a block or not.
Example 2.3
We provide a trace of liveness analysis for the program ﬂow graph in Fig-
ure 2.1. Since this analysis involves backward ﬂows, we prefer to traverse the
© 2009 by Taylor & Francis Group, LLC

28
Data Flow Analysis: Theory and Practice
CFG in the reverse postorder. We use ∅as the initialization and assume that
all variables are local implying that BI is ∅.
Local
Global Information
Block Information Iteration # 1 Iteration # 2
Genn
Killn
Outn
Inn
Outn
Inn
n8
{a,b,c}
∅
∅
{a,b,c}
∅
{a,b,c}
n7
{a,b}
∅
{a,b,c} {a,b,c} {a,b,c} {a,b,c}
n6
{b,c}
∅
{a,b,c} {a,b,c} {a,b,c} {a,b,c}
n5
{a,b}
{d}
{a,b,c} {a,b,c} {a,b,c} {a,b,c}
n4
{a,b}
{c}
{a,b,c} {a,b} {a,b,c} {a,b}
n3
{b,c}
{c}
{a,b,c} {a,b,c} {a,b,c} {a,b,c}
n2
{a,c}
{b}
{a,b,c} {a,c} {a,b,c} {a,c}
n1
{c}
{a,b,d} {a,b,c}
{c}
{a,b,c}
{c}
Observe that the data ﬂow values computed in the second iteration are
identical to the values computed in the ﬁrst iteration indicating convergence.
We leave it to the reader to verify that the ﬁnal result would be same even
if the graph is traversed in postorder; the only diﬀerence is that it will take
many more iterations.
Observe that the result would be diﬀerent if we had used the universal set
(in this case {a,b,c,d}) as the initialization. Then, d would have been live at
Exit(n7) whereas d is not used anywhere in the program.
For brevity, we will show only new values computed in an iteration in subsequent
examples—if a value is same as in the previous iteration, we will not show it explic-
itly. Hence we will not show the data ﬂow values in the last iteration.
Two major applications of liveness analysis are in register allocation and dead
code elimination. If a variable x is live at a program point, the current value of x is
likely to be used along some execution path and hence x is a potential candidate for
being allocated a register. On the other hand, if x is not live, the register allocated to
x can be allocated to some other variable without the need of storing the value of x
in memory. If x is not live at a exit of an assignment of x, then this assignment can
be safely deleted.† For example, in Figure 2.1, variable d is not live anywhere. Thus
all assignments of d can be safely eliminated.
In some cases deleting such assignments can have a transitive eﬀect because the
variables used in the right hand side of such an assignment may cease to be live.
Instead of repeating the sequence of liveness analysis and dead code elimination,
it is possible to discover such transitive eﬀects through a single data ﬂow analysis
before dead code elimination is performed. This analysis is called faint variables
analysis and will be presented in Chapter 4. Note that such an analysis cannot be
restricted to a single variable at a time because the liveness of variables occurring on
†Deletion of code which is unreachable is also called dead code elimination but we will restrict dead code
elimination to deletion of assignments to values which have no further use.
© 2009 by Taylor & Francis Group, LLC

Classical Bit Vector Data Flow Analysis
29
the right hand side of an assignment now also depends on the liveness of the variable
on the left hand side. Such analyses are not bit vector analyses in spite of the fact
that some of them use bit vector representation for data ﬂow information. This is
because the eﬀect of basic blocks in these analyses are not expressible in terms of
constant functions deﬁned using Gen and Kill due to inter-dependence of various
entities. Such frameworks are called non-separable. We describe them in Chapter 4.
For a given variable x, liveness analysis discovers a set of liveness paths. Each
liveness path is a sequence of blocks (b1,b2,...,bk) which is a preﬁx of some poten-
tial execution path starting at b1 such that:
• bk contains an upwards exposed use of x, and
• b1 is either Start or contains an assignment to x, and
• no other block on the path contains an assignment to x.
Example 2.4
Some liveness paths for variable c in our example program are: (n4,n7,n8),
(n3,n5,n6,n7,n8), (n3,n5,n6,n5,n6,n7,n8), and (n1,n2,n8).
2.3.2
Dead Variables Analysis
A variable is dead (i.e., not live) if it is dead along all paths. If we wish to perform
dead variables analysis instead of live variables analysis, the interpretation of Inn and
Outn changes: If a variable is contained in Inn or Outn, it is dead instead of being
live. This requires the following changes:
• The deﬁnitions of Genn and Killn will change. Genn will now contain all
variables whose values are modiﬁed in the block such that the modiﬁcations
are upwards exposed (i.e., are not preceded by a use of the variable). Killn
will contain variables which are used anywhere regardless of what precedes
or follows the uses. Observe that this is diﬀerent from merely swapping Genn
and Killn of liveness analysis.
• We will have to use ∩rather than ∪for merging information.
• We will have to use the universal set as initialization rather than empty set.
Similarly, BI will now have a diﬀerent set of variables.
2.3.3
Reaching Deﬁnitions Analysis
A deﬁnition of a variable x is a statement which assigns a value to x. For the purpose
of analysis, a unique label is associated with each assignment and these labels are
used to represent the deﬁnitions. As a consequence, diﬀerent occurrences of the same
assignment become diﬀerent deﬁnitions. This is diﬀerent from uses of variables or
© 2009 by Taylor & Francis Group, LLC

30
Data Flow Analysis: Theory and Practice
computation of expressions—labels are not associated with them and hence lexically
same computations are not treated as diﬀerent entities for analysis.
DEFINITION 2.2
A deﬁnition di ∈Defs of a variable x ∈Var reaches a
program point u if di occurs on some path from Start to u and is not followed
by any other deﬁnition of x on this path.
The data ﬂow equations which deﬁne the required analysis are:
Inn =

BI
n is Start block

p∈pred(n)
Out p otherwise
(2.3)
Outn = (Inn −Killn) ∪Genn
(2.4)
where Inn, Outn, Genn, Killn, and BI are sets of deﬁnitions. Observe the use of ∪to
capture the “any path” nature of data ﬂow. This is similar to liveness analysis except
that now the data ﬂow is forward rather than backward.
For every local variables x, it is assumed that a ﬁctitious deﬁnition x = undef
reaches Entry(Start). This is required for the optimization of copy propagation (de-
scribed in Section 2.3.4). If deﬁnition x = undef reaches a use of x, it suggests a
potential use before deﬁnition. Whether this happens at run time depends on the
actual results of conditions along the path taken to reach the program point.
Genn contains downwards exposed deﬁnitions in n whereas Killn contains all def-
initions of all variables modiﬁed in n. Thus Genn ⊆Killn for reaching deﬁnitions
analysis.
Example 2.5
The labels of assignments in the program in Figure 2.1 consist of variable
names and an instance number. We use them to represent the deﬁnitions
in the programs. Deﬁnitions a0, b0, c0, and d0 represent the special deﬁni-
tions a = undef, b = undef, c = undef, and d = undef respectively. Since the
conﬂuence operation is ∪, the initial value at each program point is ∅.
The result of performing reaching deﬁnitions analysis has been shown in
Figure 2.2. The deﬁnitions which reach Exit(n6) and Exit(n7) in ﬁrst iteration
have to be propagated to Entry(n5) and Entry(n3) respectively requiring an
additional iteration.
Reaching deﬁnitions analysis is used for constructing use-def and def-use chains
which connect deﬁnitions to their uses as illustrated in the following example. These
chains facilitate several optimizing transformations.
Example 2.6
Figure 2.3 shows the use-def and def-use chains of variables a and c in our
example program. For simplicity, we have not shown the chains for other
© 2009 by Taylor & Francis Group, LLC

Classical Bit Vector Data Flow Analysis
31
Local
Information
Global Information
Block
Iteration # 1
Changed values
in iteration # 2
Genn
Killn
Inn
Outn
Inn
Outn
n1
{a1,
b1,
d1}
{a0,a1,
b0,b1,b2,
d0,d1,d2}
{a0,b0,c0,d0} {a1,b1,c0,d1}
n2 {b2} {b0,b1,b2} {a1,b1,c0,d1} {a1,b2,c0,d1}
n3 {c1} {c0,c1,c2} {a1,b1,c0,d1} {a1,b1,c1,d1} {a1,b1,c0,
c1,c2,d1,d2}
{a1,b1,
c1,d1,d2}
n4 {c2} {c0,c1,c2} {a1,b1,c1,d1} {a1,b1,c2,d2}
{a1,b1,
c1,d1,d2}
{a1,b1,
c2,d1,d2}
n5 {d2} {d0,d1,d2} {a1,b1,c1,d1} {a1,b1,c1,d2}
{a1,b1,
c1,d1,d2}
n6
∅
∅
{a1,b1,c1,d2} {a1,b1,c1,d2}
n7
∅
∅
{a1,b1,c1,
c2,d1,d2}
{a1,b1,c1,
c2,d1,d2}
n8
∅
∅
{a1,b1,b2,c0,
c1,c2,d1,d2}
{a1,b1,b2,c0,
c1,c2,d1,d2}
FIGURE 2.2
Reaching deﬁnitions analysis for Example 2.5.
variables. Observe that the deﬁnition c0 reaches some uses of c. This suggests
a potential use before any assigning meaningful value. This, in turn, makes
variable b potentially undeﬁned.
Transitive eﬀects of undeﬁned variables are captured by possibly uninitialized
variables analysis. Similar to faint variables analysis which captures transitive eﬀect
of dead variables, possibly uninitialized variables analysis is also non-separable—
whether a variable is possibly undeﬁned may depend on whether other variables are
possibly undeﬁned.
For deﬁnition xi of variable x, reaching deﬁnitions analysis discovers a set of
deﬁnition reaching paths. This path is a sequence of blocks (b1,b2,...,bk) which
is a preﬁx of some potential execution path starting at b1 such that:
• b1 contains the deﬁnition xi
• bk is either End or contains a deﬁnition of x
• no other block in the path contains a deﬁnition of x.
Example 2.7
Some deﬁnition reaching paths for variable c in our example program are:
(n4,n7,n3), (n3,n5,n6,n5,n6,n7,n8), and (n3,n5,n6,n7,n3).
© 2009 by Taylor & Francis Group, LLC

32
Data Flow Analysis: Theory and Practice
n1
c0: c = undef;
b1: b = 4;
a1: a = b+c;
d1: d = a∗b;
n1
n2 b2: b = b−c; n2
n3 c1: c = b+c; n3
n4
c2: c = a∗b;
f(a−b);
n4
n5 d2: d = a+b; n5
n6 f(b+c); n6
n7 g(a+b); n7
n8
h(b−c);
f(b+c); n8
FIGURE 2.3
Def-use chains of variables a and c in our example program.
2.3.4
Reaching Deﬁnitions for Copy Propagation
Another application of reaching deﬁnitions analysis is in performing copy propaga-
tion. A deﬁnition of the form x = y is called a copy because it merely copies the
value of y to x. When such a deﬁnition reaches a use of x, and no other deﬁnition of
x reaches that use then the use of x can be replaced by y.
Example 2.8
Copy b = 4 in block n1 in our example program is the only deﬁnition which
reached the uses of b in blocks n3, n4, n5, n6 and n7. Thus all these uses can
be replaced by constant 4.
In the above example, the right hand side value is constant. When they are vari-
ables, as in x = y, replacing the uses of x by y requires an additional check that the
value of y has not been modiﬁed along the path from the copy to the use. We can
deﬁne a variant of reaching deﬁnitions analysis to accomplish this. The main dif-
ference between this variant and the analysis presented in Section 2.3.3 is that we
restrict the deﬁnitions to copies and a deﬁnition x = y is contained in
• Genn if it is downwards exposed in n in the sense of not being followed by a
© 2009 by Taylor & Francis Group, LLC

Classical Bit Vector Data Flow Analysis
33
deﬁnition of x or y, and in
• Killb if n contains a deﬁnition of x or y.
With these changes, we can now perform reaching deﬁnitions analysis. If one deﬁ-
nition reaches a use, we can perform copy propagation.
Note that this optimization does not improve the program on its own but it has the
potential of creating dead code: When copy propagation is performed using x = y, it
is possible that all uses of x are replaced by y thus making x dead after the assign-
ment. Thus this assignment can be safely deleted.
We leave it for the reader to deﬁne a variant of copy propagation analysis using
intersection rather than union.
2.4
Discovering Global Properties of Expressions
In this section we present analyses for eliminating redundant computations of ex-
pressions. Our ﬁrst analysis involves replacing an expression by its precomputed
value. The remaining analyses facilitate code movement which involve advancing
computation an expression to earlier points in control ﬂow paths.
2.4.1
Available Expressions Analysis
Given a program point u, this analysis discovers the expressions whose results at u
are same as the their previously computed values regardless of the execution path
taken to reach u.
DEFINITION 2.3
An expression e ∈Expr is available at a program point
u if all paths from Start to u contain a computation of e which is not followed
by an assignment to any of its operands.
The data ﬂow equations which deﬁne available expressions analysis are:
Inn =

BI
n is Start block

p∈pred(n)
Out p otherwise
(2.5)
Outn = (Inn −Killn) ∪Genn
(2.6)
where Inn, Outn, Genn, Killn, and BI are sets of expressions. Observe the use of
∩to capture the “all paths” nature of data ﬂow. This is diﬀerent from liveness and
reaching deﬁnitions analyses. However, similar to reaching deﬁnitions analysis, the
direction of data ﬂow is forward.
© 2009 by Taylor & Francis Group, LLC

34
Data Flow Analysis: Theory and Practice
BI assumes that expressions involving local variables are not available at entry of
Start since the local variables come into existence with function invocations.‡ Genn
contains downwards exposed expressions in n whereas Killn contains all expressions
whose operands are modiﬁed in n.
The availability information is useful in an optimization called common subex-
pression elimination in which computation of an expression is marked as redundant
if the expression is available at that point. Let the set of expressions whose upwards
exposed computations exist in block n be denoted by AntGenn§. Let Redundantn
denote expressions which can be eliminated in block n. Then,
Redundantn = AntGenn ∩Inn
(2.7)
Values of the previous computations are stored in a temporary variable and the re-
dundant computations are replaced by that temporary variable. Most production
compilers such as gcc perform common subexpression elimination.
Example 2.9
The program in Figure 2.1 contains expressions (a∗b), (a +b), (a−b), (a−c),
and (b +c). We represent the set of expression by a bit vector; the position a
bit indicates the expression which it represents as shown below.
a∗b
a +b
a −b
a−c
b+c
Bit string 11111 represents the set {a ∗b,a +b,a−b,a−c,b+c} whereas bit
string 00000 represents ∅.
The result of available expressions analysis has
been shown below. Since this is an all paths analysis, the initial value at each
‡There could be exceptions to this in languages which allocate activation records in static area instead of
stack e.g., FORTRAN IV.
§AntGen is the Gen set for Anticipability analysis described in Section 2.4.3. Here we use a diﬀerent
name to avoid confusion with Gen of the current analysis.
© 2009 by Taylor & Francis Group, LLC

Classical Bit Vector Data Flow Analysis
35
program point is the universal set (i.e., 11111).
Global Information
Block
Local Information
Iteration # 1 Changed values
in iteration # 2
Genn
Killn AntGenn
Inn
Outn
Inn
Outn
Redundantn
n1
10001 11111
00000
00000 10001
00000
n2
00010 11101
00010
10001 00010
00000
n3
00000 00011
00001
10001 10000 10000
00000
n4
10100 00011
10100
10000 10100
10000
n5
01000 00000
01000
10000 11000
00000
n6
00001 00000
00001
11000 11001
00000
n7
01000 00000
01000
10000 11000
00000
n8
00011 00000
00011
00000 00011
00000
Expression (a ∗b) in n4 is redundant. Its value can be stored in a temporary
variable say t0. Then the assignment d = a ∗b in n1 can be replaced by d = t0
and the assignment c = a ∗b in n4 can be replaced by c = t0.
If we had used 00000 as the initial value, expression (a ∗b) would not have
been available anywhere in the loops except at Exit(n4). Thus we would have
missed the opportunity of eliminating the computation of (a∗b) in n4.
For a given expression e, available expressions analysis discovers a set of avail-
ability paths. Each availability path is a sequence of blocks (b1,b2,...,bk) which is
a preﬁx of some potential execution path starting at b1 such that:
• b1 contains a downwards exposed computation of e,
• bk is either End or contains a computation of e, or an assignment to some
operand of e,
• no block in the path contains a computation of e, or an assignment to any
operand of e, and
• every path ending on bk is an availability path for e.
Note that because of the last condition, we cannot talk about an availability path in
isolation from other paths ending on a node—we must talk about a group of avail-
ability paths.
In terms of availability paths, common subexpression elimination in block n in-
volves storing the value of redundant expression in a temporary at the start of every
availability path terminating at n and replacing the computation of the expression in
n by the temporary.
Example 2.10
Some availability paths for expression (a ∗b) in our example program are:
(n1,n3,n4), (n1,n3,n5,n6,n7,n3,n4), and (n4,n7,n3,n4).
© 2009 by Taylor & Francis Group, LLC

36
Data Flow Analysis: Theory and Practice
n1 c = a ∗b n1
n2 c = a∗b n2
n3 a = a∗b n3
n1
t = a ∗b
c = t
n1
n2 t = a ∗b n2
n3 a = t ∗b n3
(a) Partial Redundancy
(b) Eliminating Partial Redundancy
FIGURE 2.4
Partial availability and partial redundancy.
2.4.2
Partially Available Expressions Analysis
An important variant of available expressions analysis relaxes the condition that an
expression should be available along all paths—it is suﬃcient if the expression is
available along some path.
If a block contains an upwards exposed computation of an expression and the
expression is available at the entry of the block, then the upwards exposed compu-
tation is totally redundant. If the expression is partially available at the entry of the
block, then the upwards exposed computation is partially redundant as illustrated in
Figure 2.4. This information is used in partial redundancy elimination described in
Section 2.4.4.
We need to make a simple change in available expressions analysis to discover
partially available expressions: Data ﬂow information should be merged using ∪
instead of ∩. This also means that the initial value is ∅instead of the universal set.
Partially redundant computations in block n are deﬁned by
ParRedundn = AntGenn ∩Inn
(2.8)
where AntGenn denotes the set of expressions whose upwards exposed computations
exist in block n.
Example 2.11
The result of partially available expressions analysis on our example program
has been shown below. Since the conﬂuence operation is ∪, the initial value
of Ini and Outi for all i is 00000.
© 2009 by Taylor & Francis Group, LLC

Classical Bit Vector Data Flow Analysis
37
Global Information
Block
Local Information
Iteration # 1 Changed values
in iteration # 2
Genn
Killn AntGenn
Inn
Outn
Inn
Outn
ParRedundn
n1
10001 11111
00000
00000 10001
00000
n2
00010 11101
00010
10001 00010
00000
n3
00000 00011
00001
10001 10000 11101
11100
00001
n4
10100 00011
10100
10000 10100 11100
11100
10100
n5
01000 00000
01000
10000 11000 11101
11101
01000
n6
00001 00000
00001
11000 11001 11101
11101
00001
n7
01000 00000
01000
11101 11101
01000
n8
00011 00000
00011
11111 11111
00011
Observe that for every n, ParRedundn ⊇Redundantn suggesting partial redun-
dancies subsume total redundancies. Also note that in our program, there are
many partial redundancies which are not total.
The paths discovered by partial available expressions analysis are a special case of
the availability paths discovered by available expressions analysis: The last condition
in the deﬁnition of availability paths does not apply to partial availability paths. Thus
unlike availability paths, we can talk about individual partial availability paths.
2.4.3
Anticipable Expressions Analysis
Common subexpression elimination explained in Section 2.4.1 involves “in-place”
transformation. As observed in the beginning of Section 2.4, some transformations
involve inserting expressions at program points where they were not computed in
the original program. Preserving the semantics of programs requires ensuring that a
computation should not be inserted in a path along which the computation was not
performed in the original program.
Example 2.12
Consider our running example of Figure 2.1. It is easy to see that expression
(a +b) is invariant in both the loops and it is desirable to move it out of the
loops and place it at Exit(n1). However, the control ﬂow path n1 →n2 →n8
does not have any computation of the expression. Hence inserting the expres-
sion at Exit(n1) is not safe.
The decision such as above can be arrived at by performing anticipable expres-
sions analysis (also called very busy expressions analysis).
DEFINITION 2.4
An expression e ∈Expr is anticipable at a program
© 2009 by Taylor & Francis Group, LLC

38
Data Flow Analysis: Theory and Practice
Global Information
Block
Local
Information
Iteration # 1 Changed values
in iteration # 2
Genn
Killn
Outn
Inn
Outn
Inn
n8
00011 00000 00000 00011
n7
01000 00000 00011 01011 00001 01001
n6
00001 00000 01011 01011 01001 01001
n5
01000 00000 01011 01011 01001 01001
n4
10100 00011 01011 11100 01001 11100
n3
00001 00011 01000 01001 01000 01001
n2
00010 11101 00011 00010
n1
00000 11111 00000 00000
FIGURE 2.5
Anticipable expressions analysis for Example 2.13.
point u if every path from u to End contains a computation of e which is not
preceded by an assignment to any operand of e.
The data ﬂow equations which deﬁne anticipable expressions analysis are:
Inn = (Outn −Killn) ∪Genn
(2.9)
Outn =

BI
n is End block

s∈succ(n)
Ins otherwise
(2.10)
where Inn, Outn, Genn, Killn, and BI are sets of expressions. Similar to available
expressions analysis, these equations use ∩to capture the “all paths” nature of data
ﬂow. However, the data ﬂow is backward similar to live variables analysis.
BI assumes that the expressions involving local variables are not anticipated at
Exit(End). Genn contains upwards exposed expressions in n whereas Killn contains
all expressions whose operands are modiﬁed in n.
Example 2.13
The result of anticipable expressions analysis on our example program has
been shown in Figure 2.5. Since the conﬂuence operation is ∩, the initial
value of Ini and Outi for all i is 11111.
For a given expression e, anticipable expressions analysis discovers a set of antic-
ipability paths. Each anticipability path is a sequence of blocks (b1,b2,...,bk) which
is a preﬁx of some potential execution path starting at b1 such that:
• bk contains an upwards exposed computation of e,
© 2009 by Taylor & Francis Group, LLC

Classical Bit Vector Data Flow Analysis
39
v
a∗b
a∗b
a∗b
a∗b
a∗b
a∗b
a∗b
Liveness
Anticipability
Availability
Partial Availability
FIGURE 2.6
Data ﬂow paths discovered by data ﬂow analysis (shown by double lines).
• b1 is either Start or contains a computation of e, or an assignment to some
operand of e,
• no block in the path contains a computation of e, or an assignment to any
operand of e, and
• every path starting at b1 is an anticipability path.
Similar to availability paths, we talk about a group of anticipability paths rather than
a single anticipability path.
Example 2.14
Some anticipability paths for expression (a + b) in our example program are:
(n5,n6,n5), (n5,n6,n7), (n3,n4,n7), and (n3,n5). Note that (a + b) is not antici-
pable at Exit(n1).
2.4.4
Classical Partial Redundancy Elimination
This section presents the classical approach to partial redundancy elimination (PRE)
which involves a bidirectional formulation of data ﬂows. This section also describes
its limitations and shows how they are overcome by some of its variants.
The basic principle of PRE has been illustrated in Figure 2.4. It can be viewed
as an instance of code hoisting along a hosting path. This hoisting subsumes loop
invariant movement and common subexpression elimination.
Example 2.15
In Figure 2.4 the hoisting path is (n2,n3); path (n1,n3) is an availability path.
In Figure 2.7, expression b∗c is loop invariant and is partially available due
the availability path along the back edge. This is a special case of partial
redundancy and can be eliminated along the hoisting path (n1,n2).
© 2009 by Taylor & Francis Group, LLC

40
Data Flow Analysis: Theory and Practice
n1 a = b∗c
n1
n2 a = b ∗c n2
not available
available
n1 t = b ∗c n1
n2 a = t n2
(a) Original program
(b) Optimized program
FIGURE 2.7
Loop invariant is a special case of PRE.
Hoisting Path of an Expression
Informally, the safety and desirability of hoisting an expression are deﬁned as fol-
lows: An expression can be safely hoisted to a program point u if it is anticipable at
u. It should be hoisted to ancestors of u if it is partially available at u.
For an expression e, a hoisting path is a maximal sequence of blocks (b1,b2,...,bk)
which is a preﬁx of a potential execution path starting at b1 such that:
• bk contains an upwards exposed computation of e,
• e is anticipable and partially available at Entry(bi) and Exit(bi) of each block
bi (other than b1 and bk), and at Entry(bk),
• e is not available at Exit(b1), or can be hoisted to Entry(b1), and
• no block in the path contains a computation of e, or an assignment to any
operand of e.
A key design idea in deﬁning a hoisting path is that an expression is hoisted to
Entry(n) only if it can be hoisted out of n into its predecessors. This means that if an
expression has to be inserted at the start of a hoisting path, it is inserted at the exit of
the ﬁrst block rather than at its entry. The conditions for hoisting an expression into
and out of a block are deﬁned as follows:
• Safety of hoisting to Exit(n).
An expression e should be hoisted to Exit(n) only if
(S.1) it can be hoisted to Entry(s) for every successor s of n.
This is captured by the equation:
Outn =

BI
n is End block

s∈succ(n)
Ins otherwise
(2.11)
© 2009 by Taylor & Francis Group, LLC

Classical Bit Vector Data Flow Analysis
41
• Safety of hoisting to Entry(n).
An expression e should be hoisted to Entry(n) only if
(S.2) n contains an upwards exposed computation of e, or
(S.3) e can be hoisted to Exit(n) and n does not contain an assignment to any
operand of e.
Condition S.2 is satisﬁed by Genn of Anticipability analysis which is denoted
by AntGenn to distinguish it from Genn of other analyses. Condition S.3 is
satisﬁed by the term (Outn −Killn).¶ Thus the safety of placement at Entry(n)
is captured by the term
Inn ⊆(AntGenn ∪(Outn −Killn))
(2.12)
• Desirability of hoisting.
By design, an expression e should be hoisted to Entry(n) only if it can be
hoisted out of it into a predecessor of n. If it can be hoisted into some prede-
cessor but not all predecessors then safety requires that one evaluation of the
expression should be made in n and then it is not proﬁtable to hoist it into any
predecessor.
Further, if it is not partially available, hoisting it does not eliminate any partial
redundancy. Hence an expression e should be hoisted to Entry(n) only if
(D.1) e is partially available at Entry(n), and
(D.2) for each predecessor p of n,
(D.2.a) e can be hoisted to Exit(p), or
(D.2.b) e is available at Exit(p) (and hence need not be inserted at Exit(n)).
Condition D.1 is captured by the term
Inn ⊆PavInn
(2.13)
Condition D.2 is captured by the term
Inn ⊆

p∈pred(n)

Out p ∪AvOut p

(2.14)
Combining Conditions (2.12), (2.13), and (2.14) results in Equation (2.15) below
which deﬁnes Inn. Outn is deﬁned by Equation (2.11).
¶Note that Killn is same for all analyses involving expressions: Available expressions analysis, partially
available expressions analysis, anticipable expressions analysis, and PRE.
© 2009 by Taylor & Francis Group, LLC

42
Data Flow Analysis: Theory and Practice
Global Information
Block
Local
information
Constant
information
Iteration # 1
Changes in
iteration # 2
Changes in
iteration # 3
Genn
Killn PavInn AvOutn Outn
Inn
Outn
Inn
Outn
Inn
n8 00011 00000 11111 00011 00000 00011
00001
n7 01000 00000 11101 11000 00011 01001 00001
n6 00001 00000 11101 11001 01001 01001
01000
n5 01000 00000 11101 11000 01001 01001
01000
n4 10100 00011 11100 10100 01001 11100
11000
n3 00001 00011 11101 10000 01000 01001
00001
n2 00010 11101 10001 00010 00011 00000
00001
n1 00000 11111 00000 10001 00000 00000
FIGURE 2.8
Partial redundancy elimination.
Inn = PavInn ∩(AntGenn ∪(Outn −Killn))∩

p∈pred(n)

Out p ∪AvOut p

(2.15)
Observe that if we drop the desirability terms from Equations (2.11) and (2.15),
they reduce to the anticipability equations (Equations 2.9 and 2.10).
Example 2.16
We illustrate the conditions deﬁning hosting criteria with the help of expres-
sion (a+b) in our running example of Figure 2.1. Since this expression is not
computed along path (n1,n2,n8), it is not anticipable at the exit of n1. Hence
inserting it at the exit of n1 violates safety. However, it is anticipable at the
exit of n3 and inserting it there is safe. Feasibility condition S.2 for (a + b)
is satisﬁed by block n7 and n5 whereas condition S.3 is satisﬁed by block n4.
Condition D.1 is satisﬁed by blocks n4, n5, n6, and n7. Condition D.2.a is
satisﬁed by n3 whereas Condition D.2.b is satisﬁed by n7.
Example 2.17
The computation of PRE data ﬂow properties of our running example is shown
in Figure 2.8. Since the conﬂuence operation is ∩, the initial value of Ini and
Outi for all i is 11111.
Figure 2.9 shows the hoisting paths in our example. Observe that there
is no hoisting path for expression (a ∗b) since it is totally redundant and
© 2009 by Taylor & Francis Group, LLC

Classical Bit Vector Data Flow Analysis
43
n1
b1: b = 4;
a1: a = b +c;
d1: d = a ∗b;
n1
n2 b2: b = a −c; n2
n3 c1: c = b+c ;
n3
n4 c2: c = a∗b ;
f(a−b);
n4
n5 d2: d = a+b ; n5
n6 f( b+c ); n6
n7 g( a+b ); n7
n8 h(a −c);
f( b+c ); n8
FIGURE 2.9
Hoisting paths in PRE of the running example.
need not be inserted anywhere. For expression (a+b) there are three hoisting
paths: (n3,n4,n7), (n3,n5) and (n5,n6,n7). Since the last path also happens to
be an availability path, there is no need to insert the expression in n5. Ex-
pression (b+c) has the following hoisting paths: (n2,n8), (n6,n7,n8), (n6,n7,n3),
(n4,n7,n8), (n4,n7,n3), and (n5,n6). Observe that there is no hoisting path for
expressions (a −b) and (a−c).
Also observe the need of the third iteration for suppressing the hoisting of
expressions (a −c), and (b+c). The initial values of the bits corresponding to
these expressions is 1 in In/Out values. Expression (a−c) cannot be hoisted
out of the outer loop because it is neither partially available anywhere in
the loop nor is it invariant in the loop due to assignment to c.
Thus the
bit corresponding to this expression becomes 0 in Inn3 in the ﬁrst iteration.
The fact that it cannot be placed at Exit(n7) because of this reason, can be
discovered only in the second iteration when its bit in Outn7 becomes 0. Its
hoisting out of n8 is suppressed in the third iteration when its bit in Inn8
becomes 0 in the third iteration.
Expression (b + c) is not anticipated at Exit(n3) and hence its bit in Outn3
becomes 0 in the ﬁrst iteration. Setting the corresponding bit in Inn5 to 0
requires the second iteration. Its placement at Exit(n6) is suppressed in the
third iteration.
© 2009 by Taylor & Francis Group, LLC

44
Data Flow Analysis: Theory and Practice
Transformation Using Hoisting Path
Having identiﬁed hoisting paths, and complementary availability paths for an ex-
pression e, the following transformations need to be performed by creating a new
temporary variable t:
• At the Start of a Hoisting or an Availability Path.
Insert an assignment t = e, just before the computation of e. Replace the orig-
inal computation of e by t at the start of an availability path.
Note that there is no need to detect an availability path explicitly. All down-
wards exposed computations of e can be safely assumed to start an availability
path. Thus the main task is to identify the start of that hoistability path where
the expression has to be inserted. The necessary conditions for block n to start
a hoistability path are:
– It should be possible to hoist the expression to Exit(n), and
– It should not be possible to hoist the expression at Entry(n), or some
operand of the expression should be modiﬁed in n.
These conditions are captured by the following:
Insertn = Outn ∩(¬Inn ∪Killn)
(2.16)
• At the End of a Hoisting Path.
Replace the original computation of e by t.
Identifying this is easy: It should be possible to hoist e to Entry(n) and there
should be an upwards exposed computation of e in n. These conditions are
captured by the following:
Replacen = Inn ∩AntGenn
(2.17)
Example 2.18
In our running example, the data ﬂow information which enables the trans-
formation is:
Local
Global Information
Block
Information
Iteration # 3
AntGenn Killn
Inn
Outn
Replacen Insertn
n1
00000
11111 00000 00000
00000
00000
n2
00010
11101 00000 00001
00000
00001
n3
00001
00011 00001 01000
00001
01000
n4
10100
00011 11000 01001
10000
00001
n5
01000
00000 01000 01001
01000
00001
n6
00001
00000 01001 01000
00001
00000
n7
01000
00000 01001 00001
01000
00000
n8
00011
00000 00001 00000
00001
00000
© 2009 by Taylor & Francis Group, LLC

Classical Bit Vector Data Flow Analysis
45
n1
b1: b = 4;
t2 = b +c;
a1: a = t2;
t0 = a ∗b;
d1: d = t0;
n1
n2
b2: b = c;
f(a −c);
t2 = b +c;
n2
n3 c1: c = t2
t1 = a +b; n3
n4
c2: c = t0;
f(a−b);
t2 = b +c;
n4
n5 d2: d = t1;
t2 = b +c; n5
n6 f(t2);
n6
n7 g(t1); n7
n8 h(a −c);
f(t2);
n8
FIGURE 2.10
Optimized program after PRE.
Figure 2.10 shows the optimized program after performing PRE.
An important property of this transformation is that on any path in the program,
the number of computations in the optimized program is guaranteed to not exceed
the number of computations in the original program.
Limitations of Partial Redundancy Elimination
PRE combines many ﬂows: Partial availability is a forward ﬂow with union as the
conﬂuence, total availability is a forward ﬂow with intersection as the conﬂuence,
and anticipability is backward ﬂow with intersection as the conﬂuence. Combin-
ing these ﬂows results in conservative approximations. Thus in some cases, partial
redundancies cannot be eliminated; in some cases, elimination causes some undesir-
able side eﬀects; and in most cases, eﬃciency of performing analysis is a matter of
concern.
Example 2.19
We illustrate the above limitations with our running example.
• Inability to eliminate all partial redundancies.
© 2009 by Taylor & Francis Group, LLC

46
Data Flow Analysis: Theory and Practice
It is clear from the optimized program in Figure 2.10 that expression
(a+b) has been moved out of the inner loop but cannot be moved out of
the outer loop. Similarly, expression (a−c) in n8 and expression (a −b)
in n4 are not eliminated in spite of being partially redundant.
• Increase in lifetimes of values of expressions, and hence increase in register
pressure.
Expression (b+c) is merely hoisted from block n6 to n5 without reducing
the number of computations of (b + c) in that path. Such redundant
hoisting increases register pressure since the result of (b + c) must be
kept in a register for a longer duration.
• Concern about eﬃciency of performing PRE.
Inn/Outn computation for PRE requires three iterations. For liveness
analysis this computation converged in one iteration whereas for all
other analyses discussed in this chapter, it converged in two iterations.
PRE is blocked by a combination of data ﬂows in the presence of the following
two structures in CFGs: Critical edges, and critical nodes. A critical edge is an edge
that runs from a fork node (i.e., a node with more than one successor) to a join node
(i.e., a node with more than one predecessor). A critical node is a fork node which
has multiple paths reaching it.
Figure 2.11 illustrates the eﬀect of critical edges and nodes on hoisting. Edge
n1 →n2 in Figure 2.11(a) is a critical edge whereas node n2 in Figure 2.11(b) is a
critical node. In each case, expression e is a possible candidate for hoisting from
Entry(n2) to Exit(n1) but is not anticipated at Exit(n1). In the case of a critical edge,
e is partially available at Entry(n2) due to another predecessor of n2 whereas in the
case of a critical node, e is partially available at Entry(n2) due to n1.
Observe that if e were available at Exit(n1), the critical edge or critical node would
not have any adverse eﬀect because there would be no need of hoisting e out of n2;
it would be totally redundant in n2. Alternatively, if e were anticipated at Exit(n1),
then e would be hoisted out of n2—in the case of critical edge, it would be placed in
n1 and in the case of critical node, it would be hoisted further out of n1.
Example 2.20
Edges n1 →n3, n3 →n5, n6 →n5, n6 →n7, and n7 →n8 in our running example
are critical edges. Nodes n3, n6, and n7 are critical nodes. Edge n1 →n3 blocks
hoisting expression (a + b) from n3 to n1, n3 →n5, blocks hoisting expression
(b+c) from n5 to n3, and n7 →n8, blocks hoisting expression (a−c) from n8 to
n7. Critical node n3 blocks hoisting expression (a−b) from n4 to n3.
© 2009 by Taylor & Francis Group, LLC

Classical Bit Vector Data Flow Analysis
47
n1 a = 5 n1
e ∈PavOut
e  AntIn
n2 a = 5 n2
e  AntOut
e  AvOut
e ∈AntIn
e ∈PavIn
n1 a = 5 n1
...
...
e  AntIn
n2 a = 5 n2
e  AvIn
e ∈PavIn
e  AvOut
e ∈PavOut
e ∈PavIn
e ∈AntIn
(a) Edge n1 →n2 is a critical edge
(b) Block n1 is a critical node
FIGURE 2.11
Critical edges and critical nodes block PRE. Expression e cannot be hoisted out of
n2 into the exit of n1.
Handling Critical Edges
A careful examination of the eﬀect of critical edges reveals that this limitation arises
due to the fact that the data ﬂow value represented by Inn plays a dual role: It captures
the property of safety of placement (Constraint 2.12) as well as the desirability of
placement (Constraints 2.13 and 2.14).
In Figure 2.11(a), the bit corresponding to e becomes 0 in Outn1 due to safety
constraint (e  AntIn of the successor on the left). This makes the corresponding bit
0 in Inn2 due to desirability constraint which in turn make the corresponding bit 0 in
Out of the right predecessor of n2. If a new node n12 is inserted along edge n1 →n2
in Figure 2.11, the repercussions of the desirability constraint are restricted to Inn12
since it does not have any predecessor other than n1. Further, since e ∈AntOutn12
even if e  AntOutn1, it becomes possible to hoist e out of n2 into the newly created
node n12. Note that this hoisting is not redundant because e is not partially available
in n12.
Example 2.21
Figure 2.12 shows PRE after splitting critical edges in our running example.
This allows hoisting (b +c) out of the inner loop and (a +b) out of the outer
loop. Besides, (a −c) is hoisted out of n8. Note that this has no eﬀect on the
placement of loop invariant expression (a−b).
Edge splitting has a pleasant side eﬀect of increasing the eﬃciency of analysis.
Intuitively, an all-path analysis can be seen as optimistically assuming bits to be 1
in the CFG and then resetting them to 0 due to the inﬂuence of corresponding bits
at neighbouring program point. Thus analysis involves propagating 0 in the graph
along arbitrarily long paths. The corresponding view for any-path analyses assumes
the bits to be 0 initially and then propagates 1 in the graph. Edge splitting prunes
this propagation for PRE because it prohibits the repercussions of the desirability
© 2009 by Taylor & Francis Group, LLC

48
Data Flow Analysis: Theory and Practice
n1
b1: b = 4;
t2 = b +c;
a1: a = t2;
t0 = a ∗b;
d1: d = t0;
n1
n2
b2: b = c;
t3 = a −c;
f(t3);
t2 = b +c;
n2
n13 t1 = a +b; n13
n3 c1: c = t2 n3
n4
c2: c = t0;
f(a−b);
t2 = b +c;
n4
n35 t2 = b +c; n35
n5 d2: d = t1; n5
n6 f(t2);
n6
n7 g(t1); n7
n78 t3 = a −c; n78
n8
h(t3);
f(t2);
n8
FIGURE 2.12
PRE after splitting critical edges. Among the new blocks, we have retained only
non-empty blocks.
constraints: Propagation of 0 from Outn1 to Inn2 is truncated at Inn12: Outn12 cannot
become 0 even if Inn12 becomes 0 and hence Inn2 remains 1.
A variant of edge-splitting is edge-placement which essentially achieves the same
eﬀect except that instead of splitting critical edges a-priori, the approach is to change
data ﬂow analysis to discover the edges along which expressions should be placed.
Then the required edges are split and expressions placed in the new node. Thus this
can be seen as edge-splitting on demand.
Handling Critical Nodes
Edge splitting does not help in the case of critical nodes even if we decide to split
the out edges of critical nodes regardless of whether these edges are critical or not.
If we split edge n1 →n2 in Figure 2.11(b), it would be possible to hoist e from n2
into the new node but it will continue to be partially redundant. What is required is a
transformation which will enable hoisting e out of n1 to those ancestors m of n1 such
© 2009 by Taylor & Francis Group, LLC

Classical Bit Vector Data Flow Analysis
49
that e  PavOutm.
A transformation which achieves this involves duplicating the critical node, and
along with it some other nodes such that in one copy of these nodes, the expression
is available whereas in the other copy, the expression is not available. The region
in which the expression is available does not need hoisting since the expression be-
comes totally redundant. The region in which the expression is not available can be
optimized by edge-splitting.
Example 2.22
Figure 2.13 shows code duplication involving a critical node which blocks
hoisting. The basic idea is to identify code motion preventing (CMP) region
which is a set of nodes characterized by the following:
n ∈CMP(e) ⇔e ∈(PantOutn ∩PavOutn ∩PantInn ∩PavInn)
For our running example,
CMP(a −b) = {n3,n35,n5,n6,n7}
A critical node is that node in CMP where the expression is not anticipated
along one set of out edges and is anticipated along the other set of edges. It
is this node which blocks the hoisting of expressions into the region. In our
case n3 is the critical node.
The transformation involves duplicating each CMP region such that for one
copy the expression is available and for the other copy it is not available. This
involves retaining the availability edge in one copy and not in the other. In our
example, the expression is available in the nodes with dashed labels through
edge n4 →n
7. Note that the other copy does not have the corresponding edge.
There are two copies of the critical node and since their out edges are re-
tained, the out edges along which the expression is anticipated become critical
edges because these edges go to a unique node out of CMP region. Splitting
these edges facilitates hoisting into a new successor of the critical node.
2.4.5
Lazy Code Motion
This section presents an alternative approach to PRE which minimizes lifetimes by
separating safety and desirability constraints. It allows placement of expressions
at the entry of a block and incorporates the desirability through separate analyses.
These analyses employ a stronger notion of desirability to minimize the lifetimes of
temporary variables. Unlike the classical formulation of PRE, all analyses involved
in this approach are unidirectional.
This approach is called lazy code motion because it performs as little code motion
as possible suppressing it where it does not result in proﬁtable placement. The main
steps of this approach are:
© 2009 by Taylor & Francis Group, LLC

50
Data Flow Analysis: Theory and Practice
n1 b+c; n1
n13 b+c; n13
n2 b +c; n2
n3 b+c; n3
n35 b+c; n35
n5 b+c; n5
n6 b+c;
n6
n7 b+c; n7
n
3 b+c; n3
n
35 b+c; n35
n
5 b+c; n5
n
6 b+c;
n6
n
7 b+c; n7
n34 t4 = a −b; n34
n4 f(t4); n4
n78 b+c; n78
n8 b+c; n8
FIGURE 2.13
PRE after duplicating a code motion preventing region rooted at a critical node (n3).
Duplicate copies have dashed labels. Additional edge-splitting is required for the
technique to work.
1. Splitting critical edges. Observe that in classical PRE, edge-splitting only en-
hances the eﬀectiveness of redundancy elimination but is not required for its
correctness. However, it is crucial for the correctness of this approach.
2. Discovering a region of safe placement of expressions.
This involves anticipability analysis (Section 2.4.3) for discovering hoisting
paths where the expressions could be placed anywhere to make the original
computations redundant. A safe region of placement for an expression e is the
set of program points where the expression is anticipable. Equations (2.10)
and (2.9) are used to discover the region of safe placement.
3. Discovering entry points of region of safe placements.
Entry points of a region of safe placement are the points in the region where
the expression can be inserted in order to make the original computations in
the region totally redundant.
Entry points are the earliest points and form the smallest such set where ex-
pressions can be placed. These points are discovered by combining the results
© 2009 by Taylor & Francis Group, LLC

Classical Bit Vector Data Flow Analysis
51
of availability analysis (Section 2.4.1) with the result of anticipability analy-
sis. Placing expressions at earliest points amounts to hoisting them from their
original points of computation.
We use the preﬁx Ant and Av to denote data ﬂow values in anticipability and
availability. Let EarliestInn and EarliestOutn denote the entry points. Then,
EarliestInn = AntInn ∩


p∈pred(n)
¬

AvOut p ∪AntOut p


(2.18)
EarliestOutn = (AntOutn ∪AvGenn)∩Killn
(2.19)
Availability is computed using Equations (2.5) and (2.6).
Edge splitting ensures that AntOut of all predecessors of a node is identical.
Thus the earliest points are
• Entry(n) of block n where it is safe to insert the expression, cannot be
hoisted into any predecessor, and is not available along any predecessor.
• Exit(n) of block n that contains a downwards exposed computation of the
expression such that it cannot be hoisted to Entry(n) due the presence of
an assignment to some operand of the expression.
Note that it is possible that both Exit(n) and Entry(n) are earliest points for
some expression e. This happens when e is anticipable at Exit(n) and n con-
tains both downwards and upwards exposed computations of e and an assign-
ment to an operand of e.
4. Discovering the latest points of region of safe placements.
In order to minimize the lifetimes of temporary variables, the expressions
placed at earliest points can be sunk to later points along the control ﬂow in
the region of safe placements. This analysis is an all-paths forward analysis:
SinkInn = EarliestInn ∪
(2.20)

∅
n is Start block

p∈pred(n)
(SinkOut p −AvGenp) otherwise
SinkOutn = EarliestOutn ∪(SinkInn −AntGenn)
(2.21)
Sinking begins at the earliest points of placements and discovered path along
which the expressions can be sunk. The latest placement points of expressions
are the end points of these paths and are deﬁned by:
© 2009 by Taylor & Francis Group, LLC

52
Data Flow Analysis: Theory and Practice
LatestInn = SinkInn ∩AntGenn
(2.22)
LatestOutn = SinkOutn ∩
(2.23)
AvGenn ∪


s∈succ(n)
¬SinkIns


The above equations use AntGenn and AvGenn to ensure that sinking is appli-
cable only to new placements—an original computation in a block cannot be
sunk.
5. Discovering those expressions whose values need not be preserved in tempo-
rary variables.
When the expressions are sunk to their latest points, some computations might
have only a local use within a block. Such computations need not be preserved
in a temporary variable. Discovering the variables whose values need not be
preserved is a simple variation of deadness analysis.
NoUseInn = EarliestInn ∪NoUseOutn
(2.24)
NoUseOutn =

s∈succ(n)
(EarliestIns ∪(NoUseIns −AntGens))
(2.25)
6. Inserting assignments to temporary variables at insertion points and replacing
original expressions by temporary variables.
The values of expressions should be stored in temporary variables at the lat-
est computation points provided the values have some use in future. This is
identiﬁed by
InsertInn = LatestInn −NoUseInn
(2.26)
InsertOutn = LatestOutn −NoUseOutn
(2.27)
The original computations which should be replaced by temporary variables
are deﬁned by the following:
ReplaceInn = AntGenn −(LatestInn ∩NoUseInn)
(2.28)
ReplaceOutn = AvGenn −(LatestOutn ∩NoUseOutn)
(2.29)
Example 2.23
Consider our running example after edge splitting: Edge n1 →n3 in Figure 2.1
is split to create node n13, edge n3 →n5 is split to create node n35, and edge
© 2009 by Taylor & Francis Group, LLC

Classical Bit Vector Data Flow Analysis
53
Availability
Anticipability
Earliest Placement
Block
Kill
AvGen AvIn AvOut AntGen AntIn AntOut EarliestIn EarliestOut
n1 11111 10001 00000 10001 00000 00000 00000
00000
10001
n2 11101 00010 10001 00010 00010 00010 00011
00010
00001
n13 00000 00000 10001 10001 00000 01001 01001
01000
00000
n3 00011 00000 10000 10000 00001 01001 01000
00000
00000
n35 00000 00000 10000 10000 00000 01001 01001
00001
00000
n4 00011 10100 10000 10100 10100 11100 01001
00100
00001
n5 00000 01000 10000 11000 01000 01001 01001
00000
00000
n6 00000 00001 11000 11001 00001 01001 01001
00000
00000
n7 00000 01000 10000 11000 01000 01001 00001
00000
00000
n78 00000 00000 11000 11000 00000 00011 00011
00010
00000
n8 00000 00011 00000 00011 00011 00011 00000
00000
00000
FIGURE 2.14
Early placement points for lazy code motion.
n7 →n8 is split to create node n78. The early placement points are shown
in Figure 2.14. As shown in Figure 2.15, the earliest placement points also
happen to be the latest points for this particular example. This is because of
the early placement opportunities created by edge splitting. The optimized
program after lazy code motion is identical to that shown in Figure 2.12.
Example 2.24
If we do not split critical edges in our running example, lazy code motion
replaces all occurrence of expressions (a−c) and (a+b) by temporaries. How-
ever, the value of (a −c) is stored in its temporary only in n2 and hence it is
not available along the paths reaching n8 from n7. The value of (a +b) is not
stored in its temporary anywhere.
2.5
Combined May-Must Analyses
Classical PRE requires both total availability and partial availability analysis. Such a
need is not uncommon and often both any-path and all-paths variants of information
are required. The all-path variant of data ﬂow information is also called must in-
formation. Analogously, the any-path variant of data ﬂow information is called may
© 2009 by Taylor & Francis Group, LLC

54
Data Flow Analysis: Theory and Practice
Block SinkIn SinkOut LatestIn LatestOut NoUseIn NoUseOut
n1
00000
10001
00000
10001
11101
01100
n2
00010
00001
00010
00001
11101
11100
n13
01000
01000
00000
01000
00100
00100
n3
00000
00000
00000
00000
00100
00100
n35
00001
00001
00000
00001
00100
00100
n4
00100
00001
00100
00001
00101
00100
n5
00000
00000
00000
00000
00100
00100
n6
00000
00000
00000
00000
00100
00100
n7
00000
00000
00000
00000
00100
00100
n78
00010
00010
00000
00010
11100
11100
n8
00000
00000
00000
00000
11111
11111
FIGURE 2.15
Latest placement points for lazy code motion.
information. It is possible to deﬁne a single analysis which discovers both may and
must information. We explain this with the help of availability analysis.
Deﬁning may-must availability analysis requires us to deﬁne four possible values
which can be associated an expression at any program point. For an expression e,
the value unknown at a program point u indicates that suﬃcient information is not
available at u; the value must indicates that e is available along all paths reaching u;
the value may indicates that e is available along some but not along all paths reaching
u; and the value no indicates that e is not available along any path reaching u. We
view them as degrees of certainty. We deﬁne a new conﬂuence operation which
combines the degree of certainties of a given expression e as shown in Figure 2.16.
These values can be represented using 2 bits. If we represent unknown by 11,
must by 10, no by 01, and may by 00, then  can be implemented using simple
bitwise AND. An alternative representation is to swap the bit strings for unknown
and may and use bitwise OR for .
The data ﬂow information is deﬁned in terms of sets of pairs e,de where de is
the degree of certainty of expression e. The local data ﬂow information is deﬁned as
follows:
Killn = {e,d | e ∈(AvGenn ∪AvKilln),d ∈{may,must,no,unknown}}
Genn = {e,must | e ∈AvGenn}∪{e,no | e ∈AvKilln}
where AvGenn and AvKilln represent Genn and Killn for availability (or partial avail-
ability) analysis.
Observe that when an expression e is in AvGenn or AvKilln, it belongs to both
Genn as well as Killn. This is because the local eﬀect of block n may change the
degree of certainty of e. Eﬀectively, the pairs are neither removed nor added to in Inn
and Outn—only the degrees of certainties change. In other words, these sets have the
© 2009 by Taylor & Francis Group, LLC

Classical Bit Vector Data Flow Analysis
55

e,unknown e,must e,no e,may
e,unknown e,unknown e,must e,no e,may
e,must
e,must
e,must e,may e,may
e,no
e,no
e,may
e,no e,may
e,may
e,may
e,may e,may e,may
FIGURE 2.16
Conﬂuence operation for combined may and must analysis.
same size at each program point. This is diﬀerent from other bit vector frameworks
which we have seen in this chapter,
Since the un-availability of an expression e is reﬂected by recording its degree of
certainty as no instead of removing it from the set, Killn does not imply that e ceases
to be available; it captures the fact that the data ﬂow information of e is killed.
The data ﬂow equations are deﬁned in the usual manner. The conﬂuence  deﬁned
over pairs e,de is lifted to the sets by applying it to pairs of the same expression.
Inn =

{e,no | e ∈Expr} n is Start
p∈pred(n)Out p
otherwise
(2.30)
Outn = (Inn −Killn)∪Genn
(2.31)
Example 2.25
For brevity, we represent the sets of pairs e,de in terms of vectors of de such
that there is a positional correspondence between e and de. We retain the
order of expressions as described in Example 2.9 except that now there are
two bits for every expression instead of a single bit. The boundary informa-
tion BI is no,no,no,no,no and the initial value of Inn and Outn for all n
is the tuple unknown,unknown,unknown,unknown,unknown. With our ﬁrst
choice of representation, these values are represented by 01,01,01,01,01 and
11,11,11,11,11 respectively.
The data ﬂow values are presented in Figure 2.17. Note that this informa-
tion is same as availability and partial availability information computed in
Example 2.9 and 2.11 except that partial availability includes total availability
whereas may and must availabilities are mutually exclusive.
An eﬃcient implementation of the computation of Outn is as follows:
Outn = {e, f n(e,X) | e ∈Expr}
(2.32)
where f n(e,X) represents the local eﬀect of a block on the availability of expression
e. The actual implementation of f n in terms of bit vector operations depends on
© 2009 by Taylor & Francis Group, LLC

56
Data Flow Analysis: Theory and Practice
Iteration #1
Iteration #2
Block
Inn
Outn
Inn
Outn
n1 01,01,01,01,01 10,01,01,01,10
n2 10,01,01,01,10 01,01,01,10,01
n3 10,01,01,01,10 10,01,01,01,01 10,00,00,01,00 10,00,00,01,01
n4 10,01,01,01,01 10,01,10,01,01 10,00,01,01,01 10,00,10,01,01
n5 10,01,01,01,01 10,10,01,01,01 10,00,00,01,01 10,10,00,01,01
n6 10,10,01,01,01 10,10,01,01,10 10,10,00,01,01 10,10,00,01,10
n7 10,00,00,01,00 10,10,00,01,00
n8 00,10,00,00,00 00,10,00,10,10
FIGURE 2.17
Combined may and must availability analysis.
the choice of representation for the degrees of certainty. Assuming that we use the
representation unknown ≡11, must ≡10, no ≡01, and may ≡00, and use bitwise
AND as , f n can be implemented as follows:
f n(e,X) = Ae + Be ·de
(2.33)
where e,de ∈X, “+” denotes bitwise OR and “·” denotes bitwise AND. The values
of Ae and Be are governed by local information:
Local Information of e
Ae
Be
e ∈AvGenn
e ∈Killn
10
00
e ∈AvGenn
e  Killn
10
00
e  AvGenn
e ∈Killn
01
00
e  AvGenn
e  Killn
00
11
2.6
Summary and Concluding Remarks
It is clear from the data ﬂow frameworks presented in this chapter that data ﬂow
equations have a common form which can be customized for each analysis. The
customization of this common form involves specifying the direction of ﬂow, the
conﬂuence operation, and the ﬂow functions which are deﬁned in terms of Genn and
Killn components.
All ﬂow functions in this chapter can be implemented using the bitwise operations
AND and OR (or set operations ∩and ∪). There are two important points associated
with this observation:
© 2009 by Taylor & Francis Group, LLC

Classical Bit Vector Data Flow Analysis
57
• Killn used in the operation X −Killn is a constant value. Thus set complement
(or bitwise NOT) is applied only to constant value. This computation can be
performed once and the desired operation can be applied during the data ﬂow
analysis by X ∩¬Killn.
• Genn and Killn do not depend on Inn and Outn and are purely local eﬀects.
Since Genn and Killn are constant values, Inn and Outn can be computed un-
conditionally without examining the operands.
In summary, in bit vector frameworks, the data ﬂow information can be represented
and computed using aggregate operations on bits; there is no need to examine the
bits individually. Although the data ﬂow value of an entity in common bit vector
frameworks is a boolean value and hence can be represented by a single bit, this is
neither necessary nor suﬃcient for a framework to qualify as a bit vector framework.
For example, the combined may-must availability analysis described in Section 2.5
requires two bits but is a bit vector framework. Chapter 4 presents faint variables
analysis in which data ﬂow value is boolean and hence can be represented using a
single bit. However, it is not a bit vector framework.
Subsequent chapters relax both these constraints and describe frameworks which
capture more powerful semantics.
2.7
Bibliographic Notes
Bit vector frameworks are some of the oldest data ﬂow problems. Among the ini-
tial works that introduced most common bit vector problems, Cocke [24] and Ull-
man [100] described available expressions analysis and its use in common subex-
pression elimination, Allen [4, 5] presented reaching deﬁnitions analysis, and live
variables analysis was described by Kennedy [55, 56]. Partial redundancy elimi-
nation was introduced by Morel and Renvoise [74]. Bodik, Gupta and Soﬀa [17]
discuss a combination of must and may availability and its use in complete removal
of redundancies. Knoop, R¨uthing and Steﬀen [65] introduced lazy code motion. Al-
most every book on compiler construction discusses bit vector data ﬂow frameworks.
A detailed treatment can be found in the advanced texts on compilers such as Aho,
Lam, Sethi, and Ullman [3], Appel [10], or Muchnick [76] or in the books devoted
to static analysis such as by Hecht [44], Muchnick and Jones [77], and F. Nielson,
H. R. Nielson and Hankin [80]. The ﬁrst formal deﬁnition of bit vector frameworks
was provided by Khedker and Dhamdhere [60].
© 2009 by Taylor & Francis Group, LLC

© 2009 by Taylor & Francis Group, LLC

3
Theoretical Abstractions in Data Flow
Analysis
The study of several examples of data ﬂow problems suggests that they share similar
features in terms of their speciﬁcations, their formulations as data ﬂow equations,
and their solution methods. In this chapter, we describe a general framework so that
most of the data ﬂow problems that we have seen earlier can be viewed as instances
of this framework. Doing so yields two important beneﬁts.
The ﬁrst beneﬁt is that which results from any generalization. When a data ﬂow
problem is shown to be an instance of the framework, it also suggests a solution
method whose properties are apparent. We do not have to separately prove the cor-
rectness or estimate the complexity of the solution method.
The second beneﬁt is that the generalization leads to the design of data ﬂow an-
alyzer generators, much in the way that lexer generators and parser generators have
emerged from the study of formal languages. Instead of implementing each data ﬂow
analyzer separately, a general solution method that is parametrized with respect to
the speciﬁc details of any analysis is implemented. When the speciﬁcs of a data ﬂow
analysis are supplied to this solution method, it yields a data ﬂow analyzer for the
particular analysis. This results in a rapid method of implementing data ﬂow analyz-
ers. Further, the reliability of the generated analyzers is related to the reliability of
the generator. As the generator becomes more reliable through usage, the generated
analyzers are likely to become more reliable than hand-coded analyzers.
This chapter deals with unidirectional data ﬂow problems; generalizations for han-
dling bidirectional data ﬂow problems have been presented in Chapter 5. Further,
although our descriptions are in terms of forward unidirectional problems, they are
uniformly applicable to backward data ﬂow problems. For such problems, the prop-
agation of data ﬂow information begins from the End node of the CFG instead of
the Start node and computation of the data ﬂow value at a node is in terms of its
successors instead of its predecessors.
3.1
Graph Properties Relevant to Data Flow Analysis
Programs and their properties are often represented by directed or undirected graphs.
A path in a directed graph is a sequence of nodes (n0,n1,...,nk) such that there is an
59
© 2009 by Taylor & Francis Group, LLC

60
Data Flow Analysis: Theory and Practice
Input: A CFG G with N nodes.
Output: A DFST T for G and an array rpo[1..N] representing a reverse postorder
listing of nodes in the graph.
Algorithm:
0
function dfstMain()
1
{
i = N
2
make root(G) the root of T
3
dfst(root(G))
4
}
5
function dfst(currnode)
6
{
mark currnode
7
while there are unmarked successors of currnode do
8
{
let child be an unmarked successor of currnode in
9
{
add the edge (currnode →child) to T
10
dfst(child)
11
}
12
}
13
rpo[currnode] = i
14
i = i−1
15
}
FIGURE 3.1
An algorithm to compute a depth ﬁrst spanning tree.
edge between any two consecutive nodes in the sequence. An edge between nodes
n and m is denoted as n →m. A non-null path whose starting and ending nodes are
n and m is denoted as n
+→m, and the corresponding unrestricted path as n ∗→m,
i.e., we denote the path from n to n with no edges between them as n ∗→n. An edge
connecting n to m in an undirected graph is denoted as n — m. The corresponding
unrestricted path and non-null paths are denoted as n ∗— m and n
+— m. The length
of the path n0,n1,...,nk is k.
Recall that data ﬂow analysis models programs in terms of CFGs which have been
described in Section 2.1. As described in Chapter 2, data ﬂow equations are deﬁned
by associating variables Inn and Outn with every node n in the CFG. The variables
are related through data ﬂow equations. In the examples presented in Chapter 2, the
equations were solved using a round-robin iterative algorithm which traversed the
CFG in a ﬁxed order.
In this section, we present some properties of CFGs that are relevant to round-
robin iterative data ﬂow analysis. Since we restrict ourselves to CFGs, these proper-
ties are deﬁned for connected directed graphs with a unique Start node.
A spanning tree of a directed graph G is a connected subgraph of G that includes
all nodes of G and is a tree. The root of a spanning tree is the same as the Start node
of the graph. A depth ﬁrst spanning tree (DFST) of G is a spanning tree rooted at
Start that is constructed by the algorithm in Figure 3.1.
© 2009 by Taylor & Francis Group, LLC

Theoretical Abstractions in Data Flow Analysis
61
DEFINITION 3.1
Given a graph G and its DFST T, the edges of G can
be categorized as follows:
• Tree edges are the edges that are in T.
• Backward edges are the edges from a node to one of its tree ancestors in
T. A loop from a node to itself is also classiﬁed as a backward edge.
• Forward edges are the edges not in T that connect a node to one of its
tree descendants.
• Cross edges are the edges connecting nodes that are not related by the
ancestor-descendant relation in the tree.
The classiﬁcation of edges allows us to deﬁne the following order of traversal over
CFGs.
DEFINITION 3.2
Given a graph G and its DFST, consider the sub-
graph G obtained by eliminating the back edges of G. A reverse postorder is a
topological sort of the nodes of G.
The algorithm shown in Figure 3.1 computes a reverse postorder listing of the
input graph in the array rpo. The position of the node n in this listing is rpo[n]. The
nodes of the example graph of Figure 3.2 have been numbered in reverse postorder,
i.e., rpo[i] is i for all nodes.
As we shall see later, the process of equation solving converges faster for for-
ward data ﬂow problems when the round-robiniterative algorithm traverses the CFGs
graphs in reverse postorder. For backward data ﬂow problems, the preferred order of
traversal is a postorder traversal.
OBSERVATION 3.1 Let G be a graph and T be a DFST of G. Then,
1. An edge x →y of G is a back edge iﬀrpo[x] ≥rpo[y].
2. Every cycle of G contains at least one back edge.
Back edges are important for unidirectional data ﬂow problems since they prop-
agate data ﬂow information in a direction which is opposite to the chosen direction
of graph traversal. Therefore, they may add to the number of iterations required for
convergence of the analyses.
DEFINITION 3.3
Let G be a graph and T be a DFST of G. The loop
connectedness
(more often called depth) of G with respect to T, denoted as
d(G,T), is the largest number of back edges in any acyclic path in G.
The depth of a graph could be diﬀerent for diﬀerent DFSTs. There is a special
class of graphs called reducible graphs for which the choice of DFST does not matter
because every DFST identiﬁes exactly the same set of back edges.
© 2009 by Taylor & Francis Group, LLC

62
Data Flow Analysis: Theory and Practice
1
2
6
3
4
5
7
8
1
2
6
3
4
5
7
8
1
2
6
8
7
3
4
5
(a) G and one of its DFSTs. Tree edges in G are shown
by double lines, back edges by single black lines,
the forward edge by a gray line, and cross edges by
dashed lines.
(b) Dominator tree of G.
Given an edge x →y,
x = idom(y).
FIGURE 3.2
A graph, its depth ﬁrst spanning tree and its dominator tree.
DEFINITION 3.4
A graph G is reducible
if and only if it does not
contain the forbidden subgraph shown in Figure 3.3 on the next page.
The forbidden subgraph is characterized by presence of a cycle that has two dis-
tinct entry points for paths from a node that does not appear in the cycle. The com-
mon control constructs in programs result in reducible control ﬂow graphs. However,
a compiler inserts gotos liberally in a program being compiled. The CFG of such a
program could become irreducible after optimizations.
DEFINITION 3.5
Let n and m be nodes in the CFG. The node n is said
to dominate m, denoted n  m, if every path from Start to m passes through n.
Dominance is, by deﬁnition, reﬂexive. It is also transitive. Figure 3.2(b) shows
the dominator tree for our example graph.
We now prove an important result that relates dominance and reducibility.
LEMMA 3.1
A graph G is reducible iﬀthe head of every back edge in G dominates its tail.
PROOF
If part: We show that if G is not reducible then there is a back
edge in G whose head does not dominate its tail. Indeed if G is irreducible
then it must contain the forbidden subgraph shown in Figure 3.3. Without
any loss of generality, let us consider b →c to be a back edge. Then there is
a path from Start to b through a which does not pass through c.
© 2009 by Taylor & Francis Group, LLC

Theoretical Abstractions in Data Flow Analysis
63
a
b
c
FIGURE 3.3
The forbidden subgraph for reducibility.
Only if part: We now show that if there is a back edge in G whose head does
not dominate its tail, G is irreducible. Assume that b →c is such a back edge.
Since c does not dominate b, there is a path from Start to b which bypasses c.
Further, there is also a path from Start to c which bypasses b, or else b would
have been visited before c in any depth ﬁrst traversal and b →c would not
have been a back edge. Thus G contains a forbidden subgraph with Start, b
and c as the constituent nodes.
The graph in Figure 3.2(a) is reducible. Some examples of edges whose addition
could make it irreducible are: 1 →7, 1 →3, and 1 →5.
3.2
Data Flow Framework
As we have said earlier, given a data ﬂow problem, we associate data ﬂow variables
with entry and exit points of each basic block. The data ﬂow variables are related
through equations which are then solved to get data ﬂow values at the program points.
To obtain a solution of these equations, each data ﬂow variable is initialized with a
value, and the equations are iterated over till the value of each data ﬂow variable
converges.
Recall that in the case of available expressions analysis, the value of a data ﬂow
variable during an iteration is a subset of the value in the preceding iteration. In gen-
eral there is an order between the values that a data ﬂow variable takes in successive
iterations during the solution process. In fact, one can impose an order on the entire
space of data ﬂow values. The order is related to the notion of approximation of
data ﬂow values that we discussed in Section 1.1.5 and is also important in reason-
ing about the termination of the solution procedure. Therefore the ﬁrst step in the
generalization of data ﬂow problems and their solutions is to formalize this notion of
order in the space of data ﬂow values. A general way to express an order between
objects is to embed them in a mathematical structure called a lattice.
The analyses studied in the previous chapter also illustrated the eﬀect of a basic
© 2009 by Taylor & Francis Group, LLC

64
Data Flow Analysis: Theory and Practice
block on data ﬂow values and the manner in which data ﬂow values arriving along
diﬀerent paths are merged. Our generalization includes both these aspects of data
ﬂow analysis. The transformations eﬀected by basic blocks on data ﬂow values are
called ﬂow functions. The essential properties of ﬂow functions and merge operations
are identiﬁed as part of the generalization.
A data ﬂow framework is an algebraic structure consisting of a set of data ﬂow
values, a set of ﬂow functions and a merge operator.
3.2.1
Modeling Data Flow Values Using Lattices
Systematic computation of data ﬂow values requires that the concept of approxi-
mations of data ﬂow values and the operation of merging data ﬂow values should
satisfy certain properties. In this section we provide a lattice theoretic basis of these
properties.
Partially Ordered Sets
The relation of partial order, deﬁned below, captures the notion of approximations
amongst data ﬂow values.
DEFINITION 3.6
A partial order  on a set S is a relation over S ×S
that is
1. Reﬂexive. For all elements x ∈S : x x.
2. Transitive. For all elements x,y,z ∈S : xy and yz implies xz.
3. Anti-symmetric. For all elements x,y ∈S : xy and y x implies x = y.
A partially ordered set (abbreviated as poset), denoted by (S, ), is a set S
with a partial order .
We shall read xy as “x is weaker than y”. If xy and x  y, we shall say that
“x is strictly weaker than y”, and denote this as x  y. If xy (x  y), we shall also
equivalently write y x (y  x) and read it as “y is stronger than (strictly stronger
than) x”. The posets that we shall deal with will often have an element which is
weaker than any other element in the poset. Such an element, if it exists, is called
the least element and denoted as ⊥. The greatest element, deﬁned similarly, will be
denoted as  .
Example 3.1
The poset of data ﬂow values in live variables analysis is shown in Figure 3.4
on the facing page. Here, the set of all data ﬂow values, denoted by Llv, is
2Var, where Var denotes the set of variables in a program. The partial order
is: For xi and xj in Llv, xi lvxj iﬀxi ⊇xj. The greatest element of this poset
is ∅and the least element is Var.
© 2009 by Taylor & Francis Group, LLC

Theoretical Abstractions in Data Flow Analysis
65
∅
{a}
{b}
{c}
{d}
{a,b}
{a,c}
{a,d}
{b,c}
{b,d}
{c,d}
{a,b,c}
{a,b,d}
{a,c,d}
{b,c,d}
{a,b,c,d}
FIGURE 3.4
Llv as a partially ordered set.
In the representation of the poset as a directed graph, xi  xj, if there is a directed
path from xj to xi. Since paths of length 0 are also possible, for every element xi,
xi  xi. Such representations of posets are called Hasse diagrams∗.
Example 3.2
As a dual example, consider the poset of data ﬂow values for available expres-
sions analysis. If we denote the set of all expressions occurring in the program
as Expr, then the set of data ﬂow values, Lav, is 2Expr, the set of all subsets
of Expr. Consider the partial order av deﬁned as: for all xi and xj in Lav,
xi avxj iﬀxi ⊆xj. The least element of this poset is the empty set ∅and the
top element is Expr.
In the context of data ﬂow analysis, the relation  can be interpreted as “a con-
servative (safe) approximation of”. If xy, then, in any context, the data ﬂow value
x can be used in place of y for optimization without aﬀecting the correctness of the
optimized program. As an example, consider the use of liveness analysis for either
dead code elimination (Section 2.3.1) or freeing memory objects (Section 1.1.5). If
y is the set of variables that are actually live, then performing an optimization on the
basis of a set x that is larger than y will not make the optimization unsafe. It is for
this reason that the  relation in the case of live variables analysis is ⊇. Similarly, for
optimizations which are based on available expressions analysis like common subex-
pression or partial redundancy elimination, an optimization performed at a program
∗Traditionally, Hasse diagrams are undirected graphs with the implicit assumption that xi xj if xj is
drawn at a higher level in the diagram than xi. We have, instead, chosen to make the graph directed with
the hope that this lends to clarity.
© 2009 by Taylor & Francis Group, LLC

66
Data Flow Analysis: Theory and Practice
point on the basis of a set of expressions y can safely be replaced by one based on a
subset x of y. Therefore the partial order in the case of available expressions analysis
is ⊆.
Conversely, xy can be interpreted as follows: In any context, the data ﬂow value
x provides more opportunities for optimization than y, or, using the terminology
introduced in Chapter 1, x is more exhaustive than y. This may be at the cost of safety,
i.e., the optimization based on x may result in a program that has a behavior diﬀerent
from the original program. We would like the data ﬂow values resulting from our
analyses to be safe and yet provide maximum opportunities for optimization.
DEFINITION 3.7
Let (L, ) be a poset and let S ⊆L. An element x ∈L
is an upper bound of S iﬀfor all y ∈S , y x. Similarly, an element x ∈L is a
lower bound of S iﬀfor all y ∈S , xy.
In the graphical representation of a poset, x is an upper bound of S iﬀthere are
paths from x to each element of S . Similarly, x is an lower bound of S iﬀthere
are paths from each element of S to x. Also note that the deﬁnition above does
not require the upper bound of a set to be in the set itself. As an example, let S =
{{a,b},{b,c}} in Figure 3.4. Then none of the upper or lower bounds of S are in S .
DEFINITION 3.8
The least upper bound (lub) of a set S is an element
x such that (i) x is an upper bound of S , and (ii) for all other upper bounds y
of S , xy. The greatest lower bound (glb) of a set is an element x such that
(i) x is a lower bound of S , and (ii) for all other lower bounds y of S , y x.
Referring once again to Figure 3.4 on the previous page, {a,b}, {a}, {b} are all
upper bounds of the set {{a,b,c},{a,b,d}}. However the lub of this set is {a,b}.
The lub of a set S is also called the join of S and is denoted as
S . The glb of a
set S is also called the meet of S and is denoted as
S .
can also be used as an
inﬁx operator; x !y denotes the lub of the two elements x and y. The lub (glb) of a
set, if it exists, is unique. It can be veriﬁed that the join (and meet) operator has the
following properties:
1. Idempotence. ∀x ∈S : x x = x.
2. Commutativity. ∀x,y ∈S : xy = y x.
3. Associativity. ∀x,y,z ∈S : (xy)z = x(yz).
In the context of data ﬂow analysis, the meet operator is used to merge data ﬂow
values along diﬀerent paths and reaching a join node of the underlying CFG. The
result of the meet operation is the most exhaustive safe approximation of data ﬂow
values along each of the paths.
© 2009 by Taylor & Francis Group, LLC

Theoretical Abstractions in Data Flow Analysis
67
OBSERVATION 3.2 Let L be a poset and S be a subset of L whose glb exists. Let
x ∈L. If xy for each y ∈S , then x
S . This is just a restatement of the fact that
any lower bound of a poset is weaker than the glb of the poset.
It is important to mention that the posets that represent data ﬂow values may be
inﬁnite. However, since each data ﬂow value is a ﬁnite quantity, the posets are count-
able. Since we want to present algorithms that search for solutions of equations in
posets which may be countably inﬁnite, we have to impose additional constraints on
these posets to ensure termination of the algorithms.
DEFINITION 3.9
A chain S is a subset of a poset which is totally
ordered, i.e., ∀x,y ∈S : xy or y x.
A descending chain is a sequence of
elements {x1, x2,...} from a poset such that i ≤j implies xi  xj.
DEFINITION 3.10
A descending chain {x1, x2,...} stabilizes eventually iﬀ
∃n,∀m > n : xm = xn.
DEFINITION 3.11
A poset satisﬁes the descending chain condition iﬀ
every descending chain in the poset stabilizes eventually.
The importance of the descending chain condition is that it allows us to extend the
guarantee of existence of meets to countably inﬁnite sets. Let S = {x1, x2, x3,...} be
such a set. Then the values
k
i=1xi, k = 1,2,..., form a chain. Because of the descending
chain condition, there is an m such that for any n > m,
m
i=1xi =
n
i=1xi. Then
m
i=1xi is the
glb of S .
Analogous to the descending chain condition, we can also deﬁne the ascending
chain condition. However, since data ﬂow analysis uses the meet operator for con-
ﬂuence, the result of merging is a lower bound of the data ﬂow values being merged.
Hence we are interested in the descending chain condition rather than the ascending
chain condition. In the rest of the chapter we restrict the discussions to posets that
satisfy the descending chain condition.
Lattices and Complete Lattices
During data ﬂow analysis, we have to merge sets of data ﬂow values. Therefore it is
important to ensure that the meet of such sets exists.
DEFINITION 3.12
A poset (L, ) is a lattice, iﬀ, for each non-empty
ﬁnite subset S of L, both
S and
S are in L. L is a complete lattice, iﬀ, for
each subset S of L, both
S and
S are in L.
© 2009 by Taylor & Francis Group, LLC

68
Data Flow Analysis: Theory and Practice
The condition that every non-empty ﬁnite subset must have a glb and a lub in L is
equivalent to the condition that for any pair of elements x and y, both x!y and xy
should be in L. For the lattice L to be complete, even ∅and inﬁnite subsets of L must
have a glb and lub in L.
Example 3.3
The posets (Llv, lv) and (Lav, av) are complete lattices. An example of a
lattice which is not complete is the set of natural numbers N = {0,1,2,3,...},
ordered by ≤. In fact, any inﬁnite set in this lattice does not have a lub. This
set can be converted to a complete lattice by adding the element ∞with the
property that for any x ∈N, x ≤∞.
For a poset L, the conditions (i)
S ∈L for every subset S of L and (ii)
S ∈L
for every subset S of L are equivalent. Thus for a poset to be a complete lattice,
it is enough to require one of the two conditions to hold, the other is automatically
satisﬁed. To see this, assume that the glb of every subset of L exists in L. We have to
show that for an arbitrary S ⊆L, the lub of S exists. Consider the set B of all upper
bounds of S . Since every element of S is a lower bound of B, from Observation 3.2
B is an upper bound of S . In particular, it is the least upper bound of S .
If L is a complete lattice, then we denote the top element of the lattice,
L, by  .
Similarly, the bottom element of the lattice,
L is denoted by ⊥. Since every subset
of L must have a glb and a lub, ∅must also have a glb and a lub. It turns out that
∅
is  and
∅is ⊥. To see this, consider the deﬁnition of a lower bound of S : x is a
lower bound of S iﬀ∀y ∈S : xy. When S is ∅, every element of L is vacuously a
lower bound of S . The greatest among them,  , is the glb of ∅. For similar reasons
⊥is the same as
∅. Observe that ∅cannot be a complete lattice; the smallest poset
which is complete must contain at least one element which can serve as both  and
⊥.
Very often we shall consider tuples of values, each component of the tuple coming
from a complete lattice. In such a case, the tuples themselves also form a complete
lattice.
DEFINITION 3.13
Let Li, 1 ≤i ≤m be complete lattices with the partial
order  i and meet  i. Then the cross-product L = L1 × L2 ×...× Lm is also a
complete lattice with the partial order:
x1, x2,..., xmy1,y2,...,ym iﬀxi i yi for all i, 1 ≤i ≤m
and the induced meet
x1, x2,..., xmy1,y2,...,ym = x1  1y1, x2 2y2, ..., xn nyn
The Lis are called the components of the product lattice L.
© 2009 by Taylor & Francis Group, LLC

Theoretical Abstractions in Data Flow Analysis
69
x  y,must
x  y,no
x  y,may
FIGURE 3.5
A meet semilattice for may-must alias analysis.
Meet Semilattices
Although the data ﬂow values of most of the analyses can be modeled as a complete
lattice, there are analyses whose data ﬂow values cannot be modeled even as a lattice.
Hence we need a structure which is less restrictive than a lattice.
Example 3.4
As an example of a data ﬂow analysis problem which cannot be modeled
naturally as a lattice, consider the combined may-must alias analysis problem.
This is a similar to the combined may-must analysis described in Section 2.5.
Assume that a program has just two pointer variables x and y. The data ﬂow
values for this problem can be modeled as a pair (x  y,d), where d is one of
the three values must, may and no. The data ﬂow value (x  y,must) at a
program point p indicates that x and y are aliased along all paths reaching p,
(x  y,may) indicates that x and y are aliased along some paths and not along
all paths reaching p, and (x  y,no) indicates that x and y are not aliased
along any path reaching p. The poset of these data ﬂow values is shown in
Figure 3.5.
In particular, we shall consider posets in which subsets have a glb but drop the
requirement that they have a lub as well. Thus these lattices may not have a  
element. The poset in Figure 3.5 is an example of a meet semilattice.
DEFINITION 3.14
A poset (L, ) is a meet semilattice, iﬀ, for each
non-empty ﬁnite subset S of L,
S is in L.
We are interested in meet semilattices that satisfy the descending chain condition.
Further, some of the algorithms that we discuss (algorithm in Figure 3.9, for ex-
ample) assume the existence of the greatest element  . In general, it is possible to
modify these algorithms to avoid using  (algorithm in Figure 3.15). However, it is
often convenient to use an element outside of the meet semilattice and give it the sta-
tus of the  element. As an example, the algorithm used for may-must availability
analysis in Section 2.5 required a  and hence a ﬁctitious value unknown was added
to the meet semilattice. Adding a new value requires us to deﬁne all ﬂow functions
for the value. We shall assume that for all functions f, f( ) =  . This extension pre-
serves monotonicity of functions. Adding a  element to a meet semilattice results
© 2009 by Taylor & Francis Group, LLC

70
Data Flow Analysis: Theory and Practice
Meet Semilattices (M)
Meet Semilattices
with ⊥element (Mb)
Meet Semilattices
satisfying dcc (Md)
Join Semilattices (J)
Join Semilattices
with  element (Jt)
Join Semilattices
satisfying acc (Ja)
Lattices (L)
Bounded lattices (B)
Complete lattices (C)
Complete lattices
with dcc and acc (Cda)
• dcc = descending chain condition
• acc = ascending chain condition
L = M ∩J
B = Mb ∩Jt
Cda = Md ∩Ja
Md ⊆Mb ⊆M
Ja ⊆Jt ⊆J
Cda ⊆C ⊆B ⊆L
FIGURE 3.6
Relationships between diﬀerent types of posets. The posets are assumed to be count-
able.
in a bounded lattice, i.e., a lattice with  and ⊥elements. Note that a bounded lattice
need not be complete because arbitrary subsets may not have a lub or glb.
Example 3.5
Consider the poset (A,⊆) of all ﬁnite subsets of the set of integers I. Since
every element of A is a subset of I, the poset (A∪{I},⊆) is a bounded lattice
with I and ∅as  and ⊥. However, it is not a complete lattice because the
join (∪) of arbitrary subsets of A∪{I} may not exist in A∪{I}. For example,
the union of all sets that do not contain a given number (say 1) does not exist
in A∪{I}.
It is possible to deﬁne a join semilattice much in the same way as a meet semilat-
tice. Figure 3.6 illustrates the relationships between diﬀerent kinds of posets.
© 2009 by Taylor & Francis Group, LLC

Theoretical Abstractions in Data Flow Analysis
71
3.2.2
Modeling Flow Functions
Recall that the data ﬂow equations for reaching deﬁnitions analysis are:
Inn =

BI
n is Start block

p∈pred(n)
Out p otherwise
(3.1)
Outn = (Inn −Killn) ∪Genn
(3.2)
where Inn and Outn are data ﬂow variables, whose values are being deﬁned by the
data ﬂow equations and Genn and Killn are constants whose values depend on the
contents of node n. BI is the information that is available at the Start block of the
CFG.
For unidirectional problems, having two sets of variables Inn and Outn is not
essential—it just increases the readability of the equations. To avoid proliferation
of variables in the ensuing discussion, we substitute for Out in the equations for In,
and get
Inn =

BI
n is Start block

p∈pred(n)
(Inp −Kill p) ∪Genp otherwise
(3.3)
Expressing (Inp −Kill p) ∪Genp as the application of a ﬂow function fp on Inp,
we have:
Inn =

BI
n is Start block

p∈pred(n)
fp(Inp) otherwise
(3.4)
We generalize Equations (3.4) so that the set of equations for any data ﬂow analy-
sis can be seen as an instance of the general set of equations shown below:
Inn =

BI
n is Start block
p∈pred(n) fp(Inp) otherwise
(3.5)
In Equation (3.5),
is the meet operator used to merge data ﬂow information
along diﬀerent paths. If the set of data ﬂow values is L, then fn : L '→L represents the
transformation of the data ﬂow values that reach the basic block n by the statements
in n. These functions are called ﬂow functions. Two important and related properties
of ﬂow functions are monotonicity and distributivity.
DEFINITION 3.15
A function f : L '→L is called monotonic iﬀ
∀x,y ∈L : xy ⇒f(x) f(y)
© 2009 by Taylor & Francis Group, LLC

72
Data Flow Analysis: Theory and Practice
Monotonicity implies that the ﬂow functions are well-behaved in the sense that
they preserve the order of approximations.
DEFINITION 3.16
A function f : L '→L is called distributive iﬀ
∀x,y ∈L :
f(xy) = f(x) f(y)
OBSERVATION 3.3 If f is monotonic then f(xy) f(x) f(y).
OBSERVATION 3.4 Every distributive function is also monotonic. If xy, then
x = xy and distributivity gives f(x) = f(x) f(y). This implies f(x) f(y).
Distributivity is a stronger condition than monotonicity. A distributive function
not only preserves the order of approximations but also guarantees that merging in-
formation before function application does not result in any loss of precision.
In our generalization we shall assume that the set of ﬂow functions F has the
following properties:
1. The identity function id ∈F. This is the ﬂow function for the empty block of
statements.
2. If f ∈F and g ∈F, then f ◦g ∈F. Composing the ﬂow functions transforma-
tions of two basic blocks results in a ﬂow function.
3. The functions in F are monotonic.
4. For every x ∈L, there is a ﬁnite set of ﬂow functions {f1, f2,... fm} such that
x =
1≤i≤m fi(BI). This condition arises from the fact that solution procedures
can only compute data ﬂow values which are expressible as a ﬁnite meet of
ﬂow functions applied to BI. This condition can be seen either as a minimality
condition on the set of data ﬂow values or as a suﬃciency condition on the set
of ﬂow functions.
The above four conditions characterize the set of admissible functions for data ﬂow
analysis.
3.2.3
Data Flow Frameworks
Having discussed lattice theoretic modeling of data ﬂow values and the admissible
ﬂow functions, we now combine the two to present a generalization called data ﬂow
frameworks.
DEFINITION 3.17
A data ﬂow framework is a tuple (LG,  G,FG), where
G is a symbol standing for a unspeciﬁed CFG, and :
© 2009 by Taylor & Francis Group, LLC

Theoretical Abstractions in Data Flow Analysis
73
• LG is a description of a meet semilattice that represents the data ﬂow
values relevant to the problem.
LG must satisfy the descending chain
condition.
•
G is a description of the meet operator of the semilattice.
G is, of
course, derivable from LG.
• FG is a description of the set of admissible ﬂow functions from LG to
LG. Each ﬂow function has an associated direction which could be along
the control ﬂow in the unspeciﬁed CFG G or against it.
Forward ﬂow functions indicate ﬂow of information along the ﬂow of control: The
data ﬂow information associated with a node is inﬂuenced by its predecessors. Back-
ward ﬂow functions indicate ﬂow of information against the ﬂow of control: The
data ﬂow information at a node is inﬂuenced by its successors. In unidirectional data
ﬂow frameworks, all functions have the same direction; bidirectional frameworks
have a combination of ﬂow functions in both directions.
Since we assume that the set of admissible functions are monotonic, we call the
framework a monotone data ﬂow framework.
If the admissible functions are dis-
tributive, we call the framework a distributive data ﬂow framework.
Example 3.6
As an example of a monotone data ﬂow framework, consider available ex-
pressions analysis. In this framework LG is 2Expr, where Expr is the set of
all expressions occurring in G,
G is ∩, and FG consists of functions f such
that f(X) = (X −Kill)∪Gen for arbitrary subsets Kill and Gen of Expr. When
Kill = Gen = ∅, f is the identity function.
OBSERVATION 3.5 Bit vector frameworks are distributive, i.e., if the ﬂow func-
tions f : L '→L of a framework can be expressed as f(x) = (x−Kill)∪Gen where
Kill,Gen ∈L, then
∀x,y ∈L : f(xy) = f(x) f(y)
It follows that bit vector frameworks are also monotonic.
DEFINITION 3.18
An instance of a data ﬂow framework is an instantia-
tion of the framework to a particular CFG. It is a pair G, MG where
• G = Nodes,Edges is an instance of G. This yields concrete values LG,
G and FG for LG,
G and FG.
• MG is a mapping from blocks in G to FG.
© 2009 by Taylor & Francis Group, LLC

74
Data Flow Analysis: Theory and Practice
Example 3.7
An instance of the available expression analysis is a pair consisting of a con-
crete CFG G and a mapping function MG. A basic block consisting of a single
statement a = b ∗c would be mapped to the following function by MG.
f(X) = X −Expra ∪{b ∗c}
where Expra is the set of all expressions in G that have a as an operand.
Example 3.8
As a more interesting example, consider the may alias problem for a CFG G.
The goal here is to ﬁnd at each program point the set of pointer variables
whose values are the same, i.e., they point to the same location. The result of
this analysis is used to sharpen the eﬀect of optimizations in the presence of
pointers. As an example, the fact that a and b are not may aliased ensures that
the assignment ∗b = 5 does not kill the expression ∗a+c. Thus the expression
∗a+c can be discovered as a common sub-expression.
• The meet semilattice LG consists of sets of pairs e1  e2, where e1 and
e2 are pointer expressions. The data ﬂow value at a program point p
containing this pair indicates a possible aliasing of the expressions e1
and e2 at p.
• Since a larger set of may aliases represent safer approximation by dis-
abling more optimization opportunities, the partial order is: X GY iﬀ
X ⊇Y. Thus
G is ∪.
• Apart from the identity function, FG consists of functions f such that
f(X) = X−Kill(X)∪Gen(X). Notice that unlike available expressions anal-
ysis the Kill and Gen sets are dependent on X.
Consider a basic block consisting of a single assignment statement ∗x = y.
Kill(X) consists of the set of pairs in X, one of whose components has ∗x as a
preﬁx.† Gen(X) consists of all pairs (∗e1  e2) such that e1  x and e2  y in
X.
3.3
Data Flow Assignments
Given an instance of a data ﬂow framework, the desired data ﬂow information is
represented by the values of data ﬂow variables Inn for every node n. We deﬁne
†A more precise deﬁnition of Kill(X) would include all those pairs in X, one of whose components has a
preﬁx that is must aliased to ∗x.
© 2009 by Taylor & Francis Group, LLC

Theoretical Abstractions in Data Flow Analysis
75
f1
f2
f3
f4
f5
d1
d2
d1 d2
d1 d2
(a) Example CFG
(b) Merging information at a join node
FIGURE 3.7
Example to illustrate MOP assignment value and ﬁxed point assignment.
a data ﬂow assignment (or simply assignment) as a mapping from each data ﬂow
variable Inn to a data ﬂow value.
3.3.1
Meet Over Paths Assignment
Let paths(p) denote the set of paths from Start to p. Given a path ρ ∈paths(p)
consisting of basic blocks (n1,n2...ni), let fρ denote the composition of functions
corresponding to the blocks in ρ, i.e., fρ = fni−1 ◦...◦f2 ◦f1. If ρ is a path (n) con-
sisting of a single block, fρ is the identity function.
DEFINITION 3.19
An assignment represented by the values of data ﬂow
variables Inn is safe iﬀ
∀n ∈Nodes : Inn 
ρ∈paths(n) fρ(BI)
(3.6)
Observe that the informal deﬁnitions of analyses (2.1), (2.2) and (2.3) in Chapter 2
have been given in terms of paths from Start to p.
DEFINITION 3.20
A Meet Over Paths assignment, denoted MOP, is the
maximum safe assignment.
∀n ∈Nodes : MOPn =
ρ∈paths(n) fρ(BI)
(3.7)
The existence of a MOP assignment follows from the closure and monotonicity
properties of ﬂow functions and the descending chain condition of the lattice of data
ﬂow values. A safe assignment is an approximation of the MOP assignment.
© 2009 by Taylor & Francis Group, LLC

76
Data Flow Analysis: Theory and Practice
3.3.2
Fixed Point Assignment
Observe that the deﬁnition of the MOP assignment as the desired data ﬂow informa-
tion is a path-based deﬁnition whereas the data ﬂow equations such as (3.5) form an
edge-based speciﬁcation: Data ﬂow information of a node is computed from the data
ﬂow information at the predecessors.
Example 3.9
Consider Figure 3.7(a). The data ﬂow information at the beginning of node
5 can be characterized by the following equations.
In1 = BI
In2 = f1(In1)
In3 = f1(In1) f3(In3)
In4 = f2(In2) f3(In3)
In5 = f4(In4)
Unfolding the right hand side of In5 partially, we get:
f4(f2(f1(BI))  (f3(f1(BI)  f3(In3))))
(3.8)
The expression, represented as a tree in Figure 3.8(a), gives an idea of
the nature of the solution of the equations. The solution computed by data
ﬂow equations at p consider all paths to p starting from the Start block and
computes the data ﬂow information along all these paths. However it merges
the information at join nodes as shown in part (b) of Figure 3.7 on the previous
page. The data ﬂow information d1 and d2 is merged at the join node and
the merged information d1d2 is propagated along all edges beyond the join
node.
In contrast, the computation of MOP assignment does not involve merging
values at intermediate points as shown in part (b) of Figure 3.8 on the facing
page.
As we shall see, merging is important for the existence of an algorithm for obtain-
ing a solution. However it can also imply a potential loss of information.
To investigate whether the system of equations described by (3.5) have a solution,
we ﬁrst convert it into a single equation. The equations are of the form:
In1 = f1(In1,...,InN)
In2 = f2(In1,...,InN)
...
InN = f N(In1,...,InN)
where Ini ∈Li. Let the product lattice L1 × L2 ×...LN be denoted by −→
L . Observe the
diﬀerence between fi and fi. fi ∈F : Li '→Li is a ﬂow function, whereas fi : −→
L '→Li
© 2009 by Taylor & Francis Group, LLC

Theoretical Abstractions in Data Flow Analysis
77
f4
∩
f2
f3
f1
∩
BI
f1
f3
BI
In3
∩
f4
f4
f4
f2
f3
f3
f1
f1
f3
BI
BI
f1
BI
f4
f3
f3
f3
f1
BI
...
(a) Expression tree for MFP
(b) Expression tree for MOP
FIGURE 3.8
Unfoldings of In5.
is formed by composing ﬂow functions and the meet operator. The system of simul-
taneous equations can be rewritten as the single equation
−→
In = −→f ( −→
In)
(3.9)
where −→
In ∈−→
L and −→f : −→
L '→−→
L is deﬁned as
−→f ( −→
In) =

f1( −→
In),f2( −→
In),...f N( −→
In)

A solution of Equation (3.9) represents the data ﬂow information computed by solv-
ing data ﬂow equations.
DEFINITION 3.21
A ﬁxed point of a function f : L '→L is a value v ∈L
that satisﬁes f (v) = v.
A ﬁxed point assignment is a solution of the data ﬂow equations represented by
(3.9). For a ﬁxed point assignment FP, we denote the value of variable Inn by FPn.
The maximum ﬁxed point assignment is a ﬁxed point assignment MFP such that for
any ﬁxed point assignment FP,
∀n ∈Nodes : FPnMFPn
3.3.3
Existence of Fixed Point Assignment
The set of all ﬁxed points of f is denoted by ﬁx(f). We are interested in the existence
and structure of ﬁx(−→f ) where −→f is the function used for deﬁning Equation (3.9). We
© 2009 by Taylor & Francis Group, LLC

78
Data Flow Analysis: Theory and Practice
require −→f to be monotonic; this in turn depends on the monotonicity of the ﬂow
functions in the data ﬂow framework.
The desired properties of ﬁx(−→f ) follow from the Knaster-Tarski ﬁxed point theo-
rem which we present below in a general setting.
DEFINITION 3.22
Consider a monotonic function f : L '→L. A value
v ∈L is a reductive point of f iﬀf (v)  v. A value v is an extensive point of f
iﬀf (v)  v.
The set of all reductive points of a function is denoted as red(f) and the set of all
extensive points of a function is denoted as ext(f).
THEOREM 3.1 (Knaster-Tarski ﬁxed point theorem)
Let f : L '→L be a monotonic function on a complete lattice L. Then
1.
red(f) ∈ﬁx(f) and
ﬁx(f) =
red(f).
2.
ext(f ) ∈ﬁx(f ) and
ﬁx(f) =
ext(f ).
3. ﬁx(f ) is a complete lattice.
PROOF
1. Let
red(f ) be l. We ﬁrst prove that l is a ﬁxed point, i.e., f (l) = l. To
show f (l)l, consider any element x ∈red(f). Since l x, f(l) f (x) be-
cause of monotonicity of f . Further, since x ∈red(f ), f (x) x. Therefore
f(l) x. Since x was an arbitrary element in red(f), f (l)l by Observa-
tion 3.2.
We now show l f (l). Interestingly, this can be derived from f (l)l.
Because of monotonicity, f (f(l)) f (l). Thus f (l) is a reductive point of
red(f). Since l is
red(f), we have l f (l).
Since ﬁx(f) ⊆red(f ),
red(f) is a lower bound of ﬁx(f). Further, since
red(f) ∈ﬁx(f),
red(f) =
ﬁx(f ).
2. Similar to 1.
3. Consider any arbitrary subset Y of ﬁx(f). It is enough to show that
Y
exists in ﬁx(f ). Let X = {x | x
Y, x ∈L}. Since L is a complete lattice, it
is easy to see that X is a complete lattice with
Y as the top element and
the bottom of L as the bottom element of X. Now consider a restriction
of f to X called f . f  is a monotonic function on the complete lattice
X. Clearly ﬁx(f ) ⊆ﬁx(f). Further, ﬁx(f ) ⊆X. Thus every ﬁxed point of
f  is weaker than
Y. Since ﬁx(f ) ⊆ﬁx(f),
Y is contained in ﬁx(f).
© 2009 by Taylor & Francis Group, LLC

Theoretical Abstractions in Data Flow Analysis
79
Input: An instance (G, MG) of a monotone data ﬂow framework (LG,  G, FG). The
function to which MG maps a node n is denoted as fn. The Start node is
numbered 0. The rest of the nodes are arbitrarily ordered from 1 to N −1.
Output: Ink,0 ≤k ≤N −1 giving the output of the data ﬂow analysis for each node.
Algorithm:
0
function dfaMain()
1
{
In0 = BI
2
for all j, j  0 do In j =  
3
change = true
4
while change do
5
{
change = false
6
for j = 1 to N −1 do
7
{
temp =
p∈pred(j) fp(Inp)
8
if temp  In j then
9
{
In j = temp
10
change = true
11
}
12
}
13
}
14
}
FIGURE 3.9
Round-robin iterative algorithm for computing MFP assignment for frameworks
with a complete lattice.
3.4
Computing Data Flow Assignments
Given a complete lattice and a monotonic function deﬁning data ﬂow equations
(which, in our case, is −→f ), the Knaster-Tarski ﬁxed point theorem guarantees ex-
istence of ﬁxed points. In this section we present an algorithm for computing the
MFP assignment and show the computability of MFP assignment and undecidabil-
ity of MOP assignment.
3.4.1
Computing MFP Assignment
Figure 3.9 provides an algorithm to solve the data ﬂow equations. The iterations of
lines 7-12 can be indexed using a pair (i, j), where i, starting with 1, is the iteration
number of the while loop, and j is the iteration number (the index) of the for loop.
Given an iteration (i, j), we shall denote the next iteration in lexicographical ordering
as N(i, j) and the value of Inm before the (i, j)th iteration as Inm(i,j).
© 2009 by Taylor & Francis Group, LLC

80
Data Flow Analysis: Theory and Practice
LEMMA 3.2
The algorithm shown in Figure 3.9 terminates.
PROOF
We shall ﬁrst show that the value of a data ﬂow variable decreases
across successive iterations. In other words, for all m
Inm(i,j)  InmN(i,j)
(3.10)
This must be true for m = 0 for all (i, j) as its value remains constant at BI.
For other values of m, we show (3.10) by induction on the iteration count (i, j).
Basis: True, because the value of InmN(1,1) is  .
Inductive Step: Assume as the inductive hypothesis that Inm(i,j) InmN(i,j) for
all m. From monotonicity, it follows that
∀m ∈Nodes : fm

Inm(i,j)
 fm

InmN(i,j)
(3.11)
We have to show that
∀m ∈Nodes : InmN(i,j)  InmN(N(i,j))
(3.12)
The second component of N(i, j) gives the block number whose data ﬂow
variable is examined on line 8 in N(i, j)th iteration.
We shall denote this
block number as l. If this is not the same as m, or, if the value of Inm is the
same since it was last examined, there is nothing to be proven. Otherwise, by
lines 7 and 9 of the algorithm, the proof obligation (3.12) reduces to
p∈pred(l) fp

Inp(i,j)

p∈pred(l) fp

Inp(N(i,j))
(3.13)
The inductive step then follows from (3.11) and Observation 3.2.
The termination of the algorithm follows directly from (3.10) and the de-
scending chain condition.
We next show that algorithm (3.9) computes the MFP assignment of the associ-
ated data ﬂow equations.
LEMMA 3.3
The algorithm in Figure 3.9 computes the MFP assignment of the data ﬂow
equations represented by (3.5).
PROOF
The convergence of the algorithm implies that the values of
In found by the algorithm form a ﬁxed point assignment of the equations
represented by (3.5). We have to prove that it is the maximum ﬁxed point
by showing that for any other ﬁxed point assignment FP, FPm  Inm for every
node m. We do this by showing that the value of Inm computed at each step
© 2009 by Taylor & Francis Group, LLC

Theoretical Abstractions in Data Flow Analysis
81
(i, j) of the algorithm is stronger than FPm. This is true of FP0 and In0 since
the value of FP0 is BI and so is the value of In0 in each step of the algorithm.
We therefore prove the lemma for values of m other than 0. The proof is by
induction on (i, j).
Basis: Follows from the fact that Inm(1,1) =  
Inductive step: We have to show that FPm InmN(i,j). Since FP is a ﬁxed
point assignment of Equation (3.5), FPm =
p∈pred(m) fp(FP p). Further, from
line 7 of the algorithm, InmN(i,j) =
p∈pred(m) fp

Inp(i,j)
. Therefore we have to
show that
p∈pred(m) fp(FP p) 
p∈pred(m) fp

Inp(i,j)
(3.14)
This once again follows from the induction hypothesis, monotonicity of the
ﬂow functions and Observation 3.2.
3.4.2
Comparing MFP and MOP Assignments
In this section we show that the MFP assignment computed by the algorithm in Fig-
ure 3.9 is weaker than the MOP assignment. We also show examples of frameworks
in which the MFP is strictly weaker than the MOP assignment.
LEMMA 3.4
When the algorithm in Figure 3.9 terminates, ∀m ∈Nodes, Inm MOPm.
PROOF
Let pathsl(m) denote the set of all paths of length l from Start to
m. We want to show by induction on l that Inm 
ρ∈pathsl(m) fρ(BI).
Basis: l = 1. In this case the path being considered has the single node
Start. The lemma holds because In0, which represents the data ﬂow value at
the beginning of Start is held constant at BI.
Inductive step: We have to show that
Inm 
ρ∈pathsl(m) fρ(BI)
(3.15)
From Equation (3.5), we also have
Inm =
p∈pred(m) fp(Inp)
and as the induction hypothesis, we can assume that for all p ∈pred(m),
Inp 
ρ∈pathsl−1(p) fρ(BI)
© 2009 by Taylor & Francis Group, LLC

82
Data Flow Analysis: Theory and Practice
x = z
y = w
∗x = y
p1
p2
p3
FIGURE 3.10
CFG illustrating the non-distributivity of may alias framework.
Monotonicity of ﬂow functions gives
fp(Inp)  fp

ρ∈pathsl−1(p) fρ(BI)

And from Observation 3.3,
fp(Inp) 
ρ∈pathsl−1(p) fp

fρ(BI)

Since ρ is a path of length l−1 and p is a predecessor of m, the composition
fp ◦fρ corresponds to a path of length l reaching m and
fp(Inp) 
ρ∈pathsl(m) fρ(BI)
Therefore
Inm =
p∈pred(m) fp(Inp)
ρ∈pathsl(m) fρ(BI)  MOPm
We now show that for some data ﬂow frameworks, the MFP assignment is strictly
weaker than the MOP assignment.
Example 3.10
Consider a CFG fragment shown in Figure 3.10 as an instance of the may
alias analysis framework. The data ﬂow values at p1 and p2 are {x  z} and
{y  w}. While computing the MFP assignment, these data ﬂow values will be
merged to obtain the value {x  z,y  w} at the input of the block containing
the assignment ∗x = y. The ﬂow function of this assignment will add to this
value the cross product of all aliases of ∗x and all aliases of y. The data ﬂow
value at p3 is thus {x  z,y  w,∗x  y,∗x  w,∗z  y,∗z  w}.
The MOP assignment on the other hand ﬁnds the eﬀect of the assignment
∗x = y on the incoming data ﬂow values {x  z} and {y  w} separately. This
© 2009 by Taylor & Francis Group, LLC

Theoretical Abstractions in Data Flow Analysis
83
yields the sets {x  z,∗x  y,∗z  y} and {∗x  y,∗x  w,y  w}. The value at p3
is a union of the two sets, and this is clearly stronger than the corresponding
MFP value.
The MFP value includes an alias ∗z  w which is not possible
along any execution path.
In Chapter 4 we will see other examples of data ﬂow frameworks for which the
MFP assignment is strictly weaker than MOP assignment. We now show that the
MFP and the MOP assignments coincide for distributive frameworks.
LEMMA 3.5
For distributive frameworks, ∀m ∈Nodes, Inm = MOPm.
PROOF
We replay the proof of Lemma 3.4 with  substituted by = in
(3.15). As the induction hypothesis, we can assume that for all p ∈pred(m),
Inp =
ρ∈pathsl−1(p) fρ(BI)
Applying fp to both sides of the equation, we have:
fp(Inp) = fp

ρ∈pathsl−1(p) fρ(BI)

Because fp is distributive, we have
fp(Inp) =
ρ∈pathsl−1(p) fp

fρ(BI)

which simpliﬁes to
fp(Inp) =
ρ∈pathsl(m) fρ(BI)
Therefore
Inm =
p∈pred(m) fp(Inp) =
ρ∈pathsl(m) fρ(BI)
3.4.3
Undecidability of MOP Assignment Computation
We have seen that if a framework is not distributive, then the algorithm shown in
Figure 3.9 on page 79 may produce a solution which is strictly weaker than the MOP
value. Thus it is interesting to investigate whether there exists an algorithm which
can compute the MOP of an arbitrary monotone data ﬂow framework. We now show
that the problem of ﬁnding the MOP value of a monotone data ﬂow framework is
© 2009 by Taylor & Francis Group, LLC

84
Data Flow Analysis: Theory and Practice
undecidable. To do this we reduce an instance of an undecidable problem called
the Modiﬁed Post’s Correspondence Problem (MPCP) to an instance of a monotone
data ﬂow framework. MPCP is a decision problem deﬁned as follows:
DEFINITION 3.23
Given lists A = [a1,a2,...,ak] and B = [b1,b2,...,bk],
where ai and bi are strings of 0’s and 1’s, is there an index list [1,i1,i2,...,ir]
such that a1ai1ai2 ...air = b1bi1bi2 ...bir?
In the above deﬁnition, juxtaposition of strings denotes their concatenation. Given
an instance I of MPCP, we convert it into an instance of a monotone data ﬂow frame-
work as follows:
• The meet semilattice L of data ﬂow values consists of lists of integers between
1 and k. These play the role of index lists. The semilattice also includes ⊥and
the special element $ indicating that the instance of MPCP has no solution.
• The relation  I is deﬁned as x  I y iﬀx = y or x = ⊥.
• The set of ﬂow functions F is formed by composing the following functions:
1. The identity function id.
2. A class of functions fi,1 ≤i ≤k, such that:
fi(α) =

⊥,
α is ⊥
$,
α is $
α#i otherwise
α#i extends the index list α by adding the integer i at the end.
3. A function g such that
g(α) =

⊥, the index list α is a solution of the
MPCP instance I
$ otherwise
• BI is the singleton list containing 1.
• The CFG is shown in Figure 3.11.
LEMMA 3.6
The data ﬂow framework deﬁned above is monotone.
PROOF
Obvious.
© 2009 by Taylor & Francis Group, LLC

Theoretical Abstractions in Data Flow Analysis
85
id
BI = [1]
Start
id
g
p
id
q
f1
f2
...
fk
FIGURE 3.11
CFG for showing undecidability of MOP computation.
THEOREM 3.1
The problem of ﬁnding the MOP assignment for any monotone data ﬂow
framework is undecidable.
PROOF
Given an MPCP instance I, we deﬁne an instance of a monotone
data ﬂow framework using the above construction. Each path to the program
point p generates a distinct index list as the data ﬂow value. Conversely,
for each possible index list there is a path to p that generates the list. The
function g checks whether each of these lists is a possible solution of the MPCP
instance. Therefore, the MOP value at the program point q is $ iﬀthere is a
solution to the MPCP instance, and ⊥otherwise. If an algorithm to compute
the MOP assignment existed, we could use it to ﬁnd a solution of the MPCP
instance I. However, since MPCP is known to be undecidable, the problem
of ﬁnding MOP for any monotone data ﬂow framework is also undecidable.
3.5
Complexity of Data Flow Analysis for Rapid Frameworks
Recall that the MFP algorithm presented in Figure 3.9 on page 79 does not assume
an a-priori order in which nodes of the input CFG are visited during an iteration. In
order to estimate the complexity of data ﬂow analysis, we now consider a special-
ization of the MFP algorithm in which the nodes of the CFG are visited in reverse
postorder. We also consider special properties of data ﬂow frameworks that make
the algorithm amenable to complexity analysis.
© 2009 by Taylor & Francis Group, LLC

86
Data Flow Analysis: Theory and Practice
3.5.1
Properties of Data Flow Frameworks
Section 3.2.3 presented monotonicity and distributivity properties of data ﬂow frame-
works. They are related to the convergence of the MFP algorithm and character-
ize the data ﬂow assignment computed by the MFP algorithm. In this section, we
present properties of data ﬂow frameworks based on algorithmic complexity.
DEFINITION 3.24
A monotone data ﬂow framework is k-bounded iﬀ
∃k ≥1 s.t. ∀f ∈F,∀x ∈L : f 0(x) f 1(x) f 2(x) ··· =
k−1
i=0 f i(x)
(3.16)
where f 0 is the identity function and f j+1 = f ◦f j.
The unbounded expression f 0(x) f 1(x) f 2(x) ··· represents the glb of the
data ﬂow value computed in all possible traversals over a loop and is called the loop
closure of f. Since we require L to satisfy the descending chain condition, all loop
closures are bounded. For a framework to be k-bounded, the loop closures must be
bounded by a constant k.
A 2-bounded framework is called a fast framework. It can be shown that a frame-
work is fast iﬀ
∀f ∈F,∀x ∈L : f 2(x)  x  f(x)
(3.17)
Intuitively, a single traversal over a loop is suﬃcient for computing loop closure.
LEMMA 3.7
Bit vector frameworks are fast.
PROOF
Recall that the ﬂow functions in bit vector frameworks can be
expressed as f(x) = (x−Kill)∪Gen where Kill,Gen ∈L. For such functions,
f 2(x) = f ((x−Kill)∪Gen)
= (((x−Kill)∪Gen)−Kill) ∪Gen
= (x−Kill) ∪(Gen −Kill) ∪Gen
= (x−Kill) ∪Gen
= f(x)
This implies f 2(x)  x  f(x).
There is an important subclass of fast frameworks called rapid frameworks in
which traversing the loop independently of the value at the entry of the loop is suﬃ-
cient for computing the ﬁnal value at the entry of the loop.
© 2009 by Taylor & Francis Group, LLC

Theoretical Abstractions in Data Flow Analysis
87
a
Start
h
f
ρ2
g
ρ1
FIGURE 3.12
The signiﬁcance of the rapidity condition.
DEFINITION 3.25
A data ﬂow framework is rapid, iﬀ
(∀f,g ∈F) (∀x, BI ∈L) : f(g(BI))  g(BI) f(x) x
(3.18)
The condition is signiﬁcant for paths which include loops. Figure 3.12 shows an
example of such a path whose initial segment ρ1 is from the Start node to the header
h of a loop. The second segment ρ2 is from h back to h along the looping path. The
ﬂow functions corresponding to the two segments are g and f. The result of f(g(BI))
represents the data ﬂow value at h along the path ρ1ρ2. The rapidity condition says
that this is safely approximated by combining the data ﬂow value generated along ρ1
and the value obtained by traversing the loop with any data ﬂow x available at h. The
important point is that the data value g(BI) may take several iterations to reach h from
Start because of the presence of back edges in ρ1. However, the rapidity condition
ensures that it is enough to traverse the loop with a data ﬂow value that has reached
h earlier, say, through a back edge free path from Start to h.
We now state and prove a condition that is equivalent to Condition (3.18).
LEMMA 3.8
The rapid condition (3.18) is equivalent to:
(∀f ∈F) (∀x, y ∈L) : f(y)  y f(x) x
(3.19)
PROOF
It is easy to derive (3.18) from (3.19). If (3.19) holds for any y,
in particular it holds for values expressible as g(BI) for any choice of g and BI.
To derive (3.19) from (3.18), we show that for arbitrary f, y and x,
f(y)  y f(x) x
Since any value y can be expressed as
k
i=0gi(BI), our proof obligation becomes:
f(
k
i=0gi(BI)) 
k
i=0gi(BI)  f(x)  x
© 2009 by Taylor & Francis Group, LLC

88
Data Flow Analysis: Theory and Practice
 
v1
⊥
• Let f( ) =  , f(v1) = ⊥, and f(⊥) = ⊥.
• f is fast because ∀x ∈L,∀i ≥2 : f i(x)  f(x) x
• f is not rapid because, f(v1)  v1  f( )
(a) Lattice L
(b) Flow function
FIGURE 3.13
A fast function need not be rapid.
Because of monotonicity, this is the same as
k
i=0f(gi(BI)) 
k
i=0gi(BI)  f(x)  x
(3.20)
Because of (3.18), f(gi(BI))  gi(BI)  f(x)  x holds for each i. Therefore (3.20)
holds because of Observation 3.2.
A consequence of this condition is that it is not necessary for an algorithm to
traverse a loop twice. If x is taken as the value at h before iterating over the loop,
then setting y to f(x) we have
f(f(x))  f(x) f(x) x
 f(x) x
We have just shown that every rapid framework is fast. To show that fastness does
not necessarily imply rapidity, it is suﬃcient to construct a framework with a ﬂow
function that is fast but is not rapid. Figure 3.13 deﬁnes such a function.
For complete lattices, the rapid condition can also be stated as
∀z ∈L,∀f ∈F :
f(z)  z f( )
(3.21)
This is just an instance of (3.19). Observe this condition also has the same meaning:
A loop can be analyzed independently of the incoming information.
We have already shown that bit vector frameworks are fast (Lemma 3.7). Now we
show that they are also rapid.
LEMMA 3.9
Bit vector frameworks are rapid.
PROOF
We ﬁrst consider frameworks in which the  relation is ⊆. For
such frameworks, Condition (3.18) reduces to
(∀f,g ∈F) (∀x, y ∈L) : f(g(y)) ⊇g(y)∩f(x)∩x
© 2009 by Taylor & Francis Group, LLC

Theoretical Abstractions in Data Flow Analysis
89
Bounded frameworks
k-Bounded frameworks
Fast frameworks
Rapid frameworks
Bit vector frameworks
FIGURE 3.14
Monotone data ﬂow frameworks with lattices containing  and satisfying the de-
scending chain condition.
Let g(y) = z. Then our proof obligation becomes
(∀f ∈F) (∀x, z ∈L) : f(z) ⊇z∩f(x)∩x
The right hand side can be reduced to
z ∩f(x) ∩x = z ∩((x−Kill)∪Gen) ∩x
= z ∩(x∩(¬Kill)∪Gen) ∩x
= (z∩(¬Kill)∩x) ∪(Gen ∩z∩x)
⊆(z∩(¬Kill)) ∪(Gen)
⊆(z−Kill) ∪Gen
⊆f(z)
Rapidity of frameworks in which  relation is ⊇, can be shown similarly.
Given arbitrary x ∈L and arbitrary f ∈F, we have seen that for fast frameworks,
f 2(x) f(x) x whereas for bit vector frameworks, f 2(x) = f(x). We now show that
the rapid frameworks satisfy the condition f 2(x) f(x). This condition is stronger
than the condition for fast frameworks but weaker that the condition for bit vector
frameworks.
LEMMA 3.10
If f ∈F is a ﬂow function in a rapid framework and the underlying lattice is
complete, then ∀z ∈L : f 2(z) f(z).
PROOF
Instantiating g to f, BI to z, and x to  in the rapid condi-
tion (3.18), we have
© 2009 by Taylor & Francis Group, LLC

90
Data Flow Analysis: Theory and Practice
Input: An instance (G, MG) of a distributive data ﬂow framework (LG, G, FG).
The function to which MG maps a node n is denoted as fn. The nodes are
numbered from 1 to N −1 in reverse postorder.
Output: Ink, 0 ≤k ≤N −1 giving the output of the data ﬂow analysis for each node.
Algorithm:
0
function dfaMain()
1
{
In0 = BI
2
for j = 1 to N −1 do In j =
p∈pred(j)∧p<j fp(Inp)
3
change = true
4
while change do
5
{
change = false
6
for j = 1 to N −1 do
7
{
temp =
p∈pred(j) fp(Inp)
8
if temp  In j then
9
{
In j = temp
10
change = true
11
}
12
}
13
}
14
}
FIGURE 3.15
An eﬃcient and more general version of the MFP algorithm.
f 2(z)  f(z) f( )
 f(z )  f(z)
The second step follows from Observation 3.3 proving the lemma.
Observe that conditions f 2(x)  f(x) and f 2(x) = f(x) on rapid and bit vector
frameworks respectively are only necessary conditions and are not suﬃcient. For
the function f deﬁned in Figure 3.13 on page 88, ∀x ∈L : f 2(x) = f(x) and yet the
framework is neither rapid nor bit vector. Figure 3.14 shows the relationship between
various frameworks.
3.5.2
Complexity for General CFGs
The complexity analysis in this section is restricted to rapid frameworks that are dis-
tributive. The modiﬁed algorithm is shown in Figure 3.15. Note that the data ﬂow
variables are also initialized diﬀerently. Such an initialization obviates the need of
© 2009 by Taylor & Francis Group, LLC

Theoretical Abstractions in Data Flow Analysis
91
the  element and allows handling frameworks with meet semilattices. This initial-
ization has the eﬀect of assigning to each node except Start with  and propagating
the initial values by assuming f( ) =  .
We count the number of iterations of the algorithm as follows. The initialization
of all data ﬂow variables in the for loop in line 2 is counted as the ﬁrst iteration.
Following this, each iteration of the while loop is counted separately.
To prove the main complexity result, we need a couple of auxiliary results. The
ﬁrst result characterizes the data ﬂow value at any program point after a given number
of iterations in terms of paths containing a speciﬁed number of back edges. When
the number of back edges in paths also needs to be denoted, we extend the notation
paths(j) to pathsk(j) to denote the set of paths containing at most k −1 back edges.
LEMMA 3.11
After k iterations of the algorithm, the data ﬂow value at the entry of block
j is given by In j =
ρ∈pathsk(j) fρ(BI).
PROOF
The proof is by induction on the number of iterations k.
• Basis: k = 1. This step corresponds to line 2 of the algorithm when we
traverse only back edge free paths. To prove this case, we do an inner
induction on the visiting order of nodes. The variable j is used to denote
the position of the nodes in this order.
– Basis: j = 0. The node that is numbered 0 in the visiting order
is Start. The relevant path in this case is (Start). Thus we have
In0 = BI = f(Start)(BI).
– Inductive step: Recall that the nodes are visited in reverse postorder.
Assume that the lemma holds for all nodes whose position in reverse
postorder is less than j. Observe that back edge free paths from
Start to j consist of back edge free paths from Start to p, where
p < j, followed by the forward edge (p, j). Thus:
In j =
p∈pred(j)∧p<j fp(Inp)
=
p∈pred(j)∧p<j fp(
ρ∈paths1(p) fρ(BI))
=
p∈pred(j)∧p<j
ρ∈paths1(p) fp(fρ(BI))
=
ρ∈paths1(j) fρ(BI)
• Inductive step: Assume that the lemma holds for k −1 iterations. We
once again do an inner induction on the visiting order of nodes.
– Basis: Trivial.
© 2009 by Taylor & Francis Group, LLC

92
Data Flow Analysis: Theory and Practice
– Inductive step: Assume that the lemma holds for k iterations for
those nodes whose number in reverse postorder is less than j. Then:
In j =
p∈pred(j) fp(Inp)
=

p∈pred(j)∧p<j fp(Inp)


p∈pred(j)∧p≥j fp(Inp)

=

p∈pred(j)∧p<j fp

ρ∈pathsk(p) fρ(BI)


p∈pred(j)∧p≥j fp

ρ∈pathsk−1(p) fρ(BI)

{using induction hypothesis}
=

(p∈pred(j)∧p<j) (ρ∈pathsk(p)) fp(fρ(BI))


(p∈pred(j)∧p≥j) (ρ∈pathsk−1(p)) fp(fρ(BI))

{distributivity}
=
ρ∈pathsk(j) fρ(BI)
The last step is justiﬁed because a path from Start to j with k back
edges could either be made up of (i) a path with k back edges from
Start to p, where p < j, followed by a traversal along the forward
edge p →j, or (ii) a path from Start to p with k −1 back edges,
where p ≥j, followed by a traversal along the back edge p →j.
Hence the lemma.
Since pathsk−1(j) ⊂pathsk(j), the data ﬂow value at any node decreases with
increasing number of iterations. This is similar to the algorithm shown in Figure 3.9,
and is crucial for the termination of the algorithm. Note that the algorithms have this
property because of the choice of the initial values.
The second result relates the termination of the algorithm to the data ﬂow values
at program points.
LEMMA 3.12
The algorithm terminates within k iterations iﬀfor each block j, each path ρ
in paths(j) and any boundary value BI, there exists a ﬁnite set of paths ρ1,...ρr
from pathsk−1(j) such that
fρ(BI) 
1≤i≤r fρi(BI)
(3.22)
PROOF
If part: Let BI be an arbitrary boundary value. Assume that
© 2009 by Taylor & Francis Group, LLC

Theoretical Abstractions in Data Flow Analysis
93
Condition (3.22) is satisﬁed after k iterations. Since pathsk−1(j) ⊆pathsk(j),
ρ∈pathsk(j) fρ(BI) 
ρ∈pathsk−1(j) fρ(BI)
(3.23)
Further, following Condition (3.22), for each path ρ ∈pathsk(j) we have a ﬁnite
set of paths ρ1,...ρr from pathsk−1(j) such that fρ(BI)
1≤i≤r fρi(BI). Therefore
fρ(BI)
ρ∈pathsk−1(j) fρ(BI). Considering all paths in pathsk(j) we have:
ρ∈pathsk(j) fρ(BI) 
ρ∈pathsk−1(j) fρ(BI)
(3.24)
Combining (3.23) and (3.24), we have:
ρ∈pathsk(j) fρ(BI) =
ρ∈pathsk−1(j) fρ(BI)
Therefore the data ﬂow values at the end of iterations k −1 and k coincide at
every program point and the algorithm terminates.
Only if part: Suppose the algorithm halts after m iterations, where m ≤k.
From Lemma 3.11, the data ﬂow information at any node j after m iterations
is
ρ∈pathsm(j) fρ(BI). Further, since the data ﬂow framework is assumed to be
distributive, the algorithm computes the MOP solution. Thus
In j =
ρ∈paths(j) fρ(BI) =
ρ∈pathsm(j) fρ(BI)
Therefore, for an arbitrary path ρ ∈paths(j),
fρ(BI) 
ρ∈pathsm(j) fρ(BI)
We now show that there is a ﬁnite set S of paths in pathsm(j) such that
ρ∈pathsm(j) fρ(BI) =
ρ∈S fρ(BI). Enumerate the paths in pathsm(j) as ρ1,ρ2 ..., and
let xi =
1≤n≤i fρn(BI). It is clear that the xi’s form a chain. Therefore, from
the descending chain condition, there is a number i such that for all i > i,
xi = xi. Let S be {ρ1,ρ2,...ρi}. We then have:
fρ(BI) 
ρ∈pathsm(j) fρ(BI) =
ρ∈S fρ(BI)
Now we prove the main complexity result captured by Theorem 3.2. Note that the
theorem asserts a property of data ﬂow frameworks and not of particular instances of
© 2009 by Taylor & Francis Group, LLC

94
Data Flow Analysis: Theory and Practice
a
i1
a
a
ia = ib
a
a ia+1
a
a
ir
a
i1
a
a
ia = ib
a
a ia+1
a
a
ir
ρ2
ρ3
ρ4
ρ1
(a) A representative path of interest
(b) Relevant path segments
FIGURE 3.16
Paths of interest in the CFG.
the framework. It says that if the framework satisﬁes the rapid condition, the algo-
rithm will terminate for every instance of the framework within an instance-related
bound. Conversely, if the algorithm terminates for every instance of the framework
within the speciﬁed bound, the framework is rapid. The theorem, however, does not
say anything about the precision of the speciﬁed bound.
THEOREM 3.2
Let (G, MG) be an arbitrary instance of a distributive data ﬂow framework
(L, ,F). Assume that the traversal of G is based on the DFST T. Then the
rapid condition is both necessary and suﬃcient for the algorithm in Figure 3.15
to terminate within d(G,T)+3 iterations.
PROOF
If part: Following Lemma 3.12, it is enough to show that for an
arbitrary program point j and a path ρ ∈paths(j), there exists a set of paths
S = {ρ1,ρ2,...,ρm} ⊆pathsd(G,T)+2(j) and
fρ(BI) 
ρ∈S fρ(BI)
We shall prove the above by induction on the number of back edges l in the
path ρ.
Basis: l ≤d(G,T)+1. In this case ρ itself is in pathsd(G,T)+2(j) and S = {ρ}.
Inductive Step: l > d(G,T)+1. Since the number of back edges in ρ exceeds
the depth d(G,T), ρ has a cycle. Let us enumerate the program points that
constitute ρ as (i0,i1,...,ia,...,ib,...,ir), where ia is the last point in the path
that is the same as a later point ib in the path. This situation is illustrated
in Figure 3.16. We now identify the following subpaths of ρ in the graph:
© 2009 by Taylor & Francis Group, LLC

Theoretical Abstractions in Data Flow Analysis
95
• The path ρ1 = (i1,...ia) contains at least one back edge. This is because
the path (ia+1,...,ir) is cycle free and contains at most d(G,T) edges, and
even assuming the edge ia →ia+1 to be a back edge, the number of back
edges in (ia,...,ir) is at most d(G,T)+1.
• The path ρ2 = (ia,...ib) constitutes a cycle and therefore must contain
at least one back edge.
• The path ρ3 = (ib,...ir) is an acyclic path and therefore contains at most
d(G,T) back edges.
• Let ρ4 be a back edge free path from i0 to ia. Such a path can always
be found by following tree edges from i0 to ia.
Using the rapid condition, fρ(BI) can be rewritten as:
fρ(BI) = fρ3(fρ2(fρ1(BI)))
 fρ3

fρ1(BI)  fρ2(x) x

for any x. Instantiating x to fρ4(BI), we have
fρ(BI)  fρ3

fρ1(BI)  fρ2(fρ4(BI))  fρ4(BI)

which, because of distributivity, gives
fρ(BI)  fρ3(fρ1(BI))  fρ3(fρ2(fρ4(BI)))  fρ3(fρ4(BI))
Recall that the original path ρ1ρ2ρ3 had l back edges. We observe that:
• The path ρ1ρ3 has at most l−1 back edges since the path ρ2 = (ia,...ib)
has at least one back edge.
• The path ρ4ρ2ρ3 has at most l−1 back edges since ρ1, which had at least
one back edge, has been replaced by ρ4 which has none.
• The path ρ4ρ3 has less than l−1 back edges.
Thus the induction hypothesis applies and there exists sets S 1, S 2 and S 3, all
of them subsets of pathsd(G,T)+2(j), such that
fρ(BI) 
σ∈S 1
fσ(BI)
σ∈S 2
fσ(BI)
σ∈S 3
fσ(BI)
Thus the required set S is S 1 ∪S 2 ∪S 3.
Only if part: Assume that condition (3.18) is violated for a data ﬂow frame-
work, i.e.,
(∃f,g ∈F)(∃x, BI ∈L) : f(g(BI)) . g(BI) f(x)(x)
Using the above f, g, x and BI, we have to create an instance of the framework
for which the algorithm takes more than d(G,T)+1 iterations to terminate.
© 2009 by Taylor & Francis Group, LLC

96
Data Flow Analysis: Theory and Practice
id
1
h1
2
h2
3
...
hn
n+1
f
n+2
id
1
h1
2
h2
3
...
hn
n+1
id
n+2
id
n+3
id
n+6
bot
n+4
g
n+5
f
n+7
(a) f(f(x)) . f(x) x
(b) f(f(x)) f(x) x
FIGURE 3.17
Instances that require more than d(G,T) iterations to converge.
Because of the conditions on the values in the data ﬂow lattice and the
admissible ﬂow functions (Section 3.2.2), we can assume that x =
1≤i≤n hi(BI).
There are two cases to consider. If f(f(x))/ f(x) x then consider the CFG
of part (a) of Figure 3.17. The tree edges are drawn with double lines, back
edges are single lines, cross edges are dashed lines, and forward edges are gray
lines. The depth of the graph for the indicated DFST is 0.‡ The data ﬂow
values Inn+2 in the ﬁrst three iterations are: x, x f(x), and x f(x) f(f(x))
respectively. Clearly, the algorithm will not terminate within 3 iterations.
If f(f(x)) f(x) x, then we consider the instance in part (b) of Figure 3.17.
The function bot in node n+4 is the constant function ∀x ∈L : bot(x) = ⊥. The
depth of the graph in this case is 2. Figure 3.18 shows the data ﬂow values
at program points of interest in the ﬁrst four iterations. In the ﬁfth iteration,
the data ﬂow value at n+3 is x f(x)g(⊥) f(g(⊥)).
Under the assumed
condition, this value is diﬀerent from the value at n+3 in the fourth iteration.
Therefore the algorithm takes at least six iterations to terminate.
Example 3.11
Figure 3.19 provides an instance of a framework that requires d(G,T)+3 iter-
ations. We leave it for the reader to verify that the framework is distributive
and rapid. As usual, tree edges have been shown in double lines and back
‡Note that this is because we are not distinguishing between In and Out properties.
© 2009 by Taylor & Francis Group, LLC

Theoretical Abstractions in Data Flow Analysis
97
Node i
Ini in various iterations
#1
#2
#3
#4
n+2
x
x
x f(x)g(⊥)
x f(x)g(⊥)
n+3
x
xg(⊥)
x f(x)g(⊥)
x f(x)g(⊥)
n+4
x
xg(⊥)
x f(x)g(⊥)
x f(x)g(⊥)
n+5
⊥
⊥
⊥
⊥
n+6
x
x f(x)
x f(x) f(f(x))g(⊥)
= x f(x)g(⊥)
x f(x)g(⊥) f(g(⊥))
n+7
x
x f(x)
x f(x)g(⊥)
x f(x)g(⊥) f(g(⊥))
FIGURE 3.18
First four iterations for the instance in Figure 3.17 on the facing page.
edges are shown in single lines. The value of d(G,T) is 1. The lattice does not
have a  element but the graph is reducible.
As shown in the following table, the data ﬂow values converge in the third
iteration—one more iteration is required to detect convergence. This is inde-
pendent of the BI value because of the presence of function h1. We leave it for
the reader to verify that if L contains  , three iterations are suﬃcient.
Variables
Values in each iteration
Iteration 1 Iteration 2 Iteration 3 Iteration 4
In1
v1
v1
v1
v1
In2
v1
v1
⊥
⊥
In3
v0
⊥
⊥
⊥
In4
v0
⊥
⊥
⊥
An non-rapid fast framework has been illustrated in Figure 5.9 on page 178.
3.5.3
Complexity in Special Cases
First we consider the situation when the CFG is reducible. The modiﬁed statement
of Theorem (3.2) for reducible CFGs is as follows.
THEOREM 3.3
Let (G, MG) be an arbitrary instance of a distributive data ﬂow framework
(L, ,F) such that G is reducible. Assume that the traversal of G is based on
the DFST T. Then the rapid condition is both necessary and suﬃcient for the
algorithm in Figure 3.15 to terminate within d(G,T)+2 iterations.
© 2009 by Taylor & Francis Group, LLC

98
Data Flow Analysis: Theory and Practice
1 h1 1
2 h0 2
3 h1 3
4
f
4
v1
v0
v2
⊥
Function f
v1
v0
v2
⊥
v1
v0
v2
⊥
Function h0 and h1
∀x ∈L : h0(x) = v0
∀x ∈L : h1(x) = v1
(a) CFG
(b) L
(c) Flow Functions
FIGURE 3.19
A instance of a distributive rapid framework that requires d(G,T)+3 iterations.
PROOF
We replay the earlier proof with the expression d(G,T) + x uni-
formly replaced by d(G,T)+(x −1). The only change is in the portion of the
proof that asserts the suﬃciency of the rapid condition (the if part).
The basis of the if part remains identical. We consider the changes required
to prove the inductive case. Reducibility imposes some restrictions on the
structure of the path ρ. We consider the following two cases:
• The edge ia →ia+1 is not a back edge. Since the path from ia+1 to ir is
acyclic, it can have at most d(G,T) back edges. Thus the only way in
which the path ρ1ρ2ρ3 can have d(G,T)+1 back edges is to have at least
one back edge in ρ1. As in the earlier proof, this path can be replaced
by a back edge free path and the induction hypothesis applied.
• The edge ia →ia+1 is a back edge. Due to reducibility, ia+1 must dom-
inate ia and the path segment ρ1 must also pass through ia+1. We can
then divide the path ρ into path segments as illustrated in Figure 3.20 on
the facing page. Due to the back edge ia →ia+1, the path ρ is ρ1ρ2ρ2ρ3.
Since the path from ia+1 to ir is acyclic, the path ρ2ρ3 can have at most
d back edges. The data ﬂow value along path ρ is:
fρ(BI) = fρ3

fρ2

fρ2

fρ1 (BI)

 fρ3

fρ2

fρ1 (BI)

since f 2(x) f(x) from Lemma 3.10.
Observe that ρ has been replaced by the path ρ1ρ2ρ3 which excludes the
back edge ia →ia+1 and thus contains one back edge less. Hence, the
induction hypothesis applies to the path ρ1ρ2ρ3 and the result follows.
© 2009 by Taylor & Francis Group, LLC

Theoretical Abstractions in Data Flow Analysis
99
a
i1
a
a
a
a
ia = ib
ia+1
ib+1
ir
a
i1
a
a
a
a
ib+1
ia = ib
ia+1
ir
ρ2
ρ3
ρ1
(a) ia →ia+1 is a back edge
and graph is reducible.
(b) Relevant path segments
FIGURE 3.20
Paths of interest in the CFG.
Now we consider the special case when a  element exists in the meet semilattice.
The bit vector data ﬂow problems fall in this category. In this case, the algorithm in
Figure 3.15 can be made more eﬃcient by initializing In for each node except Start
to the value  . This is identical to the initialization on line 2 of Figure 3.9. Thus the
two algorithms become similar except for the order of traversal. Only the iterations of
the while loop are counted—the work done during the initialization step is ignored.
This is reasonable because, unlike the algorithm in Figure 3.15, no attempt is being
made to propagate the initial values in this step. We merely state the theorem and
point to the source of the proof in bibliographic notes.
THEOREM 3.4
Consider an instance (G, MG) of a distributive data ﬂow framework (L, ,F),
where L has a  element. The rapid condition is both necessary and suﬃcient
for the algorithm in Figure 3.15 with the modiﬁcation mentioned above to
terminate within d(G,T) + 2 iterations. T is the DFST used for deciding the
order of traversal of G.
3.6
Summary and Concluding Remarks
In this chapter, we have presented generalizations of data ﬂow frameworks based on
mathematical abstractions and have presented lattice theoretic modelling of data ﬂow
© 2009 by Taylor & Francis Group, LLC

100
Data Flow Analysis: Theory and Practice
frameworks. The generalizations include data ﬂow values, operations to manipulate
them, algorithms to compute the data ﬂow information and the characteristics of the
computed data ﬂow information. All these generalizations are uniformly applicable
to all unidirectional monotone data ﬂow frameworks but are not directly applicable
to bidirectional frameworks. Even among unidirectional frameworks, the generaliza-
tions related to complexity of data ﬂow analysis are applicable to a limited class of
frameworks, leaving out some important frameworks that have arisen in practical sit-
uations. In the subsequent chapters, we consider some of these data ﬂow frameworks
and then present a diﬀerent view of data ﬂow analysis to uniformly characterize the
complexity of a larger class of data ﬂow frameworks including bidirectional frame-
works.
3.7
Bibliographic Notes
Of the graph theoretic concepts introduced early in the chapter, discussion on DFST
can be found in the texts by Aho, Hopcroft and Ullman [2] and Cormen, Rivest,
Leiserson and Stein [27]. Reducibility was introduced by Allen [4] and is further
discussed by Hecht and Ullman in [45, 46]. Dominance was introduced by Lowry
and Medlock [70]. Lengauer and Tarjan [68] present an algorithm that can be used
to compute dominators eﬃciently. The text by Davey and Priestley [29] is a good
introduction to lattice theory. The presentation of Tarski’s ﬁxed point theorem is
from Tarski’s original paper [99].
The initial attempt to model data ﬂow values in terms of meet semilattices was by
Kildall [63]. Kam and Ullman [49] introduced reverse postorder for visiting nodes
in the CFG and also introduces the rapidity condition which guarantees convergence
within d(G,T) + 3 iterations. This work has given rise to the folklore that iterative
data ﬂow analysis is fast for many data ﬂow frameworks. Much of Section 3.5.2 is
based on this paper.
The papers so far dealt with distributive frameworks. Kam and Ullman [50]
showed that for montonic frameworks, which are less restrictive then distributive
frameworks, a round-robin iterative algorithm computes the MFP solution. How-
ever, for a monotonic framework that is not distributive, the MFP solution may be
diﬀerent from the MOP solution. They also showed the undecidability of the problem
of ﬁnding MOP solution of an arbitrary monotonic data ﬂow problem. Monotonicity
of ﬂow functions was also discussed by Graham and Wegman [37] where they in-
troduced the concept of fast frameworks. A detailed treatment of these concepts can
also be found in the book by Hecht [44]. Marlowe and Ryder [71] review properties
of diﬀerent data ﬂow frameworks in lattice theoretic settings.
© 2009 by Taylor & Francis Group, LLC

4
General Data Flow Frameworks
In bit vector frameworks the data ﬂow information of diﬀerent entities is independent
of each other. However, there are many situations in which the data ﬂow information
of an entity could depend on the data ﬂow information of some other entity. For
example, the concept of transfer of liveness was described in Section 1.1.2 as follows:
If access path x
σ is live after an assignment x = y, then σ is trans-
ferred to x and the access path y
σ becomes live before the assignment.
Here, the liveness of access path x
σ depends on the liveness of access path y
σ.
Capturing such interdependences requires a more general kind of ﬂow functions and
the frameworks involving such ﬂow functions are called non-separable.
4.1
Non-Separable Flow Functions
This section deﬁnes the non-separability of ﬂow functions, shows how it can be
modeled in terms of Gen and Kill eﬀects, and describes the limitations it imposes on
the nature of basic blocks that can be constructed for performing data ﬂow analysis.
Recall that a data ﬂow framework (LG,  G,FG) is deﬁned in terms of an unspec-
iﬁed CFG G. For convenience, we drop the subscript G where not required. We
assume that the entities occurring in G that are of interest to us for a given analysis
are contained in a set Σ = {α,β,γ,...,ω}. A given analysis discovers some proper-
ties of interest for a speciﬁc kind of entities e.g., expressions, variables, deﬁnitions,
etc. Thus for any given analysis, all entities are of the same type. The lattice L is
a product Lα ×Lβ ×···×Lω where Lα is the component lattice containing the data
ﬂow values of entity α. In general, all Ls are same. Data ﬂow value x ∈L is a tuple
x α,x β,...,x ω.
The motivation behind modeling non-separability explicitly arises from the obser-
vation that an element in L is not atomic—it consists of a tuple of separate data ﬂow
values for each entity. Thus it is natural to ask if instead of viewing ﬂow functions as
atomic, they can also be modeled in terms of functions that compute data ﬂow values
of smaller granularities. This view allows us to explicate the dependence of the data
ﬂow value of an entity on the data ﬂow values of the other entities. This leads to rich
insights that are useful in deﬁning tight complexity bounds as well as the feasibility
conditions for systematic reduction of ﬂow function compositions.
101
© 2009 by Taylor & Francis Group, LLC

102
Data Flow Analysis: Theory and Practice
DEFINITION 4.1
A ﬂow function f : L '→L is separable iﬀit is a tu-
ple f α, f β,··· , f ω of component functions f : L '→L. If f
is of the form
L '→L, then f is non-separable.
A component function f α computes the data ﬂow value of entity α. Similar to
the ﬂow function, we use basic block as a subscript of the component function when
required.
As the name suggests, separability is based on independence of data ﬂow proper-
ties of entities for which data ﬂow analysis is being performed. In order to model
non-separable ﬂow functions in terms of Gen and Kill components, instead of deﬁn-
ing constant Genn and Killn, we deﬁne them as Genn : L '→L and Killn : L '→L by
allowing dependent parts also:
Genn(x) = ConstGenn ∪DepGenn(x)
(4.1)
Killn(x) = ConstKilln ∪DepKilln(x)
(4.2)
The ﬂow function fn is deﬁned as:
fn(x) = (x−Killn(x))∪Genn(x)
(4.3)
In bit vector frameworks, the dependent parts are absent resulting in constant Gen
and Kill components. Rapid and fast frameworks require that the ﬂow functions
are separable, so that the rapidity condition (3.18) and fastness condition (3.17) are
satisﬁed. In these and other separable frameworks, dependent parts may exist due
to a possibility of dependence among data ﬂow values of the same entity at diﬀerent
program points. In non-separable frameworks, the dependence can be of two types:
The data ﬂow value of a given entity may depend on the data ﬂow value of the
same entity or on data ﬂow value of some other entity. Dependence captured by
DepGen on the data ﬂow value of the same entity must necessarily be a non-identity
dependence because identity dependence is implicitly deﬁned by ensuring that both
Gen and Kill have no eﬀect on the entity. The dependence on other entities may be
identity or non-identity dependence. Unlike identity dependence on the same entity,
identity dependence on other entities must be explicitly deﬁned. We model these
dependences in Section 4.5.
The presence of dependent parts in Gen and Kill makes it diﬃcult to summa-
rize the eﬀect of multiple statements in a ﬂow function. Hence, basic blocks for
non-separable analyses consist of single statements. However, multiple consecutive
statements which do not have any data dependence between them can still be com-
bined into a basic block subject to the usual control ﬂow restriction. If two consec-
utive statements can be executed in any order without aﬀecting program semantics,
then they can be grouped into the same basic block for data ﬂow analysis of non-
separable ﬂows. Further, a conditional or unconditional jump need not always be a
separate block. If it is included in a block, it must be the last statement of the block.
The statements relevant to data ﬂow analysis are divided in the following cate-
gories: (a) assignment statements x = e where x ∈Var, e ∈Expr, (b) input statements
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
103
read(x) which assign a new value to x, (c) use statements use(x) which model uses
of x for condition checking, printing and parameter passing etc., and (d) other state-
ments. Since we restrict ourselves to intraprocedural analysis in this part, we assume
that there are no function calls. Eﬀectively, Var contains local variables only. Print
statements and evaluation of branching condition etc. are modeled in terms of use
statements.
4.2
Discovering Properties of Variables
In this section we present analyses to discover whether a given scalar variable is
dead, or possibly undeﬁned, or has a constant value.
4.2.1
Faint Variables Analysis
As discussed towards the end of Section 2.3.1, liveness analysis does not take into
account interdependence of variables. This section describes a data ﬂow analysis
which takes into account such interdependence and discovers the transitive closure
of deadness of a variable which is the complement of liveness.
DEFINITION 4.2
A variable x ∈Var is faint at a program point u if
along every path from u to End, it is either not used before being deﬁned or is
used to deﬁne a faint variable.
Clearly, this is a backward data ﬂow problem. However, unlike liveness analysis
this is an all-paths analysis. Hence the conﬂuence operation is ∩. The lattice is
(2Var,⊆) and  is Var. The initial value of Inn and Outn for all n is Var.
Inn = fn(Outn)
(4.4)
Outn =

BI
n is End

s∈succ(n)
Ins otherwise
(4.5)
All local variables are dead at the end of a procedure and BI = Var.
The constant and dependent parts of Genn(x) component are deﬁned as follows.
A variable x becomes faint before every assignment to it. There is no other way in
which a variable that is live after a statement, could become faint before the state-
ment.
ConstGenn =

{x} n is assignment x = e, x  Opd(e)
{x} n is read(x)
∅
otherwise
DepGenn(x) = ∅
© 2009 by Taylor & Francis Group, LLC

104
Data Flow Analysis: Theory and Practice
n1
d = 0; n1
n2 if d ≥3; n2
n3 if d ≥2; n3
n4 a = b; n4
n5 if d ≥1; n5
n6 b = c; n6n7 read (c); n7
n8 d = d +1; n8
n9 print a; n9
true
false
true
false
true
false
FIGURE 4.1
Program for illustrating faint variables analysis and possibly uninitialized variables
analysis.
A variable x could cease to become faint before an assignment statement if it appears
on the right hand side and the left hand side variable is faint. Alternatively, it could
cease to become faint because of a use statement. The former represents the transitive
eﬀect of left hand side variable not being faint and is captured by the dependent part
DepKilln(x) as follows:
ConstKilln =
{x} n is use(x)
∅
otherwise
DepKilln(x) =

Opd(e)∩Var n is assignment x = e, x  x
∅
otherwise
where Opd(e) denotes the operands of expression e.
Example 4.1
The result of performing faint variables analysis for the program in Figure 4.1
has been shown in Figure 4.2. Since a is used in block n9, it is not faint. As
a consequence, variables b and c cease to be faint. Discovering these facts
requires two additional iterations and propagating it against the back edge
requires the fourth iteration.
If n9 did not contain a use of a, the variables a, b, and c would have been
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
105
Node
Iteration #1
Changes in
Iteration #2
Changes in
Iteration #3
Changes in
Iteration #4
Outn
Inn
Outn
Inn
Outn
Inn
Outn
Inn
n9
{a,b,c,d} {b,c,d}
n8
{a,b,c,d} {a,b,c,d} {b,c}
{b,c}
{c}
{c}
∅
∅
n7
{a,b,c,d} {a,b,c,d} {b,c}
{b,c}
{c}
{c}
∅
n6
{a,b,c,d} {a,b,c,d} {b,c}
{b,c}
{c}
∅
∅
n5
{a,b,c,d} {a,b,c} {b,c}
{b,c}
∅
∅
n4
{a,b,c,d} {a,b,c,d} {b,c}
{c}
{c}
∅
∅
n3
{a,b,c}
{a,b,c}
{c}
{c}
∅
∅
n2
{b,c}
{b,c}
{c}
{c}
∅
∅
∅
n1
{b,c}
{b,c,d}
{c}
{c,d}
∅
{d}
FIGURE 4.2
Performing faint variables analysis of program in Figure 4.1.
discovered to be faint. Liveness analysis would conclude that b and c are live
regardless of the use of a in block n9.
It is interesting to explore the distributivity, rapidity, and fastness properties of
faint variables analysis. Since DepGenn(x) is ∅, fn can be rewritten as:
fn(x) = (x−Killn(x))∪Genn(x)
= (x−(ConstKilln ∪DepKilln(x))) ∪(ConstGenn ∪DepGenn(x))
= ((x−ConstKilln)∪ConstGenn) ∪(x−DepKilln(x))
Clearly the constant part of fn is similar to ﬂow functions in bit vector frameworks
and hence is distributive, rapid and fast. Thus, in order to investigate whether these
properties hold for fn, it is suﬃcient to explore them for (x−DepKilln(x)).
LEMMA 4.1
Faint variables analysis is distributive.
PROOF
It is suﬃcient to prove that ∀x1,x2 ∈L,∀fn ∈F :
(x1 ∩x2)−DepKilln(x1 ∩x2) = (x1 −DepKilln(x1)) ∩(x2 −DepKilln(x2))
From the deﬁnition of DepKilln(x), there are two cases to consider. First we
consider the case when n is an assignment statement x = e and x  x1 ∩x2.
Assume that x is neither in x1 nor in x2.
(x1 ∩x2)−DepKilln(x1 ∩x2) = (x1 ∩x2)−(Opd(e)∩Var)
= (x1 −(Opd(e)∩Var)) ∩(x2 −(Opd(e)∩Var))
= (x1 −DepKilln(x1)) ∩(x2 −DepKilln(x2))
© 2009 by Taylor & Francis Group, LLC

106
Data Flow Analysis: Theory and Practice
n1 x = y n1
n2 read(a) n2
n3 a = b n3
n4 b = c n4
n5 x = y n5
Let f = fn2 ◦fn3 ◦fn4 and let x be Var. f i(x) represents
the set of faint variables at the entry of n2 in iteration
number i in postorder traversal over the graph.
x = Var
f(x) = Var−{a}
f 2(x) = Var−{a,b}
∀i ≥3 :
f i(x) = Var−{a,b,c}
x∩f(x)∩f 2(x)∩...  x∩f(x)
FIGURE 4.3
Faint variables analysis is not fast.
If x  x1 but x ∈x2, DepKilln(x2) is ∅and the proof obligation follows due to ∩
even if Opd ∩Var is not removed from x2.
In other situations, DepKilln(x) is ∅and the lemma trivially follows.
Figure 4.3 contains an instance of faint variables analysis to show that it is neither
rapid nor fast. It is easy to generalize the example to show that faint variables analysis
is not k-bounded. It is bounded by height of the lattice which turns out to be |Var|
and depends on the particular instance.
4.2.2
Possibly Uninitialized Variables Analysis
Section 2.3.3 described reaching deﬁnitions analysis which is primarily motivated
by construction of def-use chains. If we use BI to include deﬁnitions x = undef for
all x ∈var, reaching deﬁnitions analysis also discovers the program points where
these deﬁnitions reach suggesting the possibility of a use before a variable is initial-
ized. However, the transitive eﬀect of such deﬁnitions is not handled by reaching
deﬁnitions analysis. We present an analysis which handles these eﬀects. Further, un-
like reaching deﬁnitions analysis, this analysis is aimed at discovering only whether
a given variable is possibly uninitialized—It does not collect the deﬁnitions of the
variable. This simpliﬁes the analysis and makes it very eﬃcient.
DEFINITION 4.3
A variable x ∈Var is possibly uninitialized at a program
point u if there exists a path from Start to u along which either no deﬁnition of
the variable has been encountered or the deﬁnition uses a possibly uninitialized
variable on the right hand side of the assignment.
Clearly this is a forward data ﬂow problem and uses ∪as the conﬂuence operation.
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
107
The lattice is (2Var,⊇) and  is ∅. The initial value at each node is ∅.
Inn =

BI
n is Start

p∈pred(n)
Out p otherwise
(4.6)
Outn = fn(Inn)
(4.7)
Since every local variable is uninitialized at Entry(Start), BI = Var.
An interesting aspect of this analysis is that the possibility of a variable being
uninitialized is generated only at Entry(Start) and no other program point. Hence
ConstGenn is ∅. The transitive eﬀect of an uninitialized variable appearing on the
right hand side of an assignment is captured by DepGenn(x).
ConstGenn = ∅
DepGenn(x) =
{x} n is assignment x = e, Opd(e)∩x  ∅}
∅
otherwise
A variable ceases to be uninitialized if its value is read from input or a constant
value is assigned to it. The transitive eﬀect of such initializations is captured by
DepKilln(x).
ConstKilln =

{x} n is assignment x = e, Opd(e) ⊆Const
{x} n is read(x)
∅
otherwise
DepKilln(x) =
{x} n is assignment x = e, Opd(e)∩x = ∅}
∅
otherwise
Example 4.2
For the program in Figure 4.1, the result of possibly uninitialized analysis is:
Inn1 = {a,b,c,d}, Outn4 = {b,c}, Outn6 = {a,c}, Outn6 = {a,b}. All other Inn and
Outn are {a,b,c}.
LEMMA 4.2
Possibly uninitialized analysis is distributive.
PROOF
It is suﬃcient to show that ∀x1,x2 ∈L,∀fn ∈F :
((x1 ∪x2)−DepKilln(x1 ∪x2)) ∪Genn(x1 ∪x2) =
(x1 −DepKilln(x1)) ∪(x2 −DepKilln(x2)) ∪Genn(x1) ∪Genn(x2)
Further, it is suﬃcient to consider only the assignment statement x = e.
Consider the following three cases:
• Opd(e)∩x1 = ∅and Opd(e)∩x2 = ∅. Thus Opd(e)∩(x1 ∪x2) = ∅.
© 2009 by Taylor & Francis Group, LLC

108
Data Flow Analysis: Theory and Practice
In this case.
DepKilln(x1 ∪x1)
= DepKilln(x1) = DepKilln(x2) = {x}
DepGenn(x1 ∪x1) = DepGenn(x1) = DepGenn(x2) = ∅
Hence the proof obligation is satisﬁed.
• Opd(e)∩x1  ∅and Opd(e)∩x2  ∅. Thus Opd(e)∩(x1 ∪x2)  ∅.
In this case.
DepKilln(x1 ∪x1)
= DepKilln(x1) = DepKilln(x2) = ∅
DepGenn(x1 ∪x1) = DepGenn(x1) = DepGenn(x2) = {x}
Hence the proof obligation is satisﬁed.
• Opd(e)∩x1  ∅and Opd(e)∩x2 = ∅. Thus Opd(e)∩(x1 ∪x2)  ∅.
In this case.
DepKilln(x1 ∪x1) = DepKilln(x1) = ∅
, DepKilln(x2) = {x}
DepGenn(x1 ∪x1) = DepGenn(x1) = {x} , DepKilln(x2) = ∅
In this case also, the proof obligation is satisﬁed.
• Opd(e)∩x1 = ∅and Opd(e)∩x2  ∅. Thus Opd(e)∩(x1 ∪x2)  ∅.
This case is similar to the above case.
This framework is not fast, and hence is not rapid. We leave it for the reader to
construct suitable examples.
4.2.3
Constant Propagation
If it can be asserted at compile time that a given expression would compute a ﬁxed
known value at a given program point in every execution of the program, the expres-
sion computation can be replaced by the known constant value. This can then be
propagated further as the value of the variable to which the result of the expression
is assigned. This can help in identifying if other expressions that involve the variable
compute a constant value.
For simplicity, we restrict our discussion to integer constants.
DEFINITION 4.4
A variable x ∈Var has a constant value c ∈Const at
a program point u if for every path reaching u along which a deﬁnition of x
reaches u, the value of x is c.
Note that this deﬁnition assumes that the program is correct in the sense that no
execution path uses a variable before deﬁning it and if the CFG contains a path
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
109
n1
read (e); n1
n2 a = 7;b = 2; f = e;
if (f > 0)
n2
n3
a = 2;
if (f ≥e+2) n3
n4 b = c+1;
if (b ≥7) n4
n6 if (f ≥e+1) n6
n5
f = f +1; n5
n7 c = d ∗a; n7
n8 d = a +b; n8
n9 d = a +1;
f = f +1
n9
n10 e = a +b; n10
false
true
false
false
true
false
true
true
FIGURE 4.4
Program for illustrating constant propagation.
reaching u that does not have any deﬁnition of x, such a path can be ignored so long
as at least one path containing a deﬁnition of x reaches u.
Example 4.3
We use the program in Figure 4.4 as a running example for constant propa-
gation. We have included branching conditions and have labeled out edges of
branch nodes with the branch outcomes to emphasize the above assumption
about the correctness of program in terms of use and deﬁnitions of variables.
Observe that, if we ignore the branching conditions, our basic blocks consist of
single statements except n2 and n9 which contains multiple statements because
they are independent of each other. Within the loop, the uses of following
variables can be replaced by their statically known values: a = 2, c = 6, and
d = 3. Further, b = 7 in block n4. This results in the branching condition in
block n4 being true making block n5 unreachable.
Given a variable x and a program point u, apart from associating integer constants
© 2009 by Taylor & Francis Group, LLC

110
Data Flow Analysis: Theory and Practice
undef ( )
−∞... −1
0
1 ...
∞
nonconst (⊥)
 
undef −∞... −1
0
1 ...
∞
nonconst (⊥)
(a) Assuming that every use is
preceded by a deﬁnition
(b) Combining detection of possibly
uninitialized variables
FIGURE 4.5
L for constant propagation.
with x at u, this analysis associates two additional values: undef to indicate that no
deﬁnition of x has been seen along any path reaching u, and nonconst to indicate
that x can have diﬀerent values at u along diﬀerent paths reaching u. The component
lattice L for a variable is shown in Figure 4.5(a).
Observe that the structure of the lattice is governed by the choice of ignoring
those control ﬂow paths along which no deﬁnition of the variable has been seen. The
assumption here is that the program is correct and such paths are not executed in
any run of the program or an independent analysis to discover possibly uninitialized
variables is being performed.
An alternative policy is to combine the possibly uninitialized variables analysis
along with constant propagation. This would require declaring a variable to be
nonconst at a join point if it has a constant value along a path but is undeﬁned
along some other path reaching the program point. This is fair under the assump-
tion that all paths are potential execution paths so the value of the variable is known
along some paths and is not known along some other paths. This results in a meet
semilattice as illustrated in Figure 4.5(b). In this lattice  is an artiﬁcial element and
is required for initialization. The ﬂow functions will have to be suitably extended to
include this value.
Such an analysis will discover fewer constants in the program and is more conser-
vative compared to the analysis that excludes those paths that do not contain a def-
inition of the variable under consideration. Hence practically, this policy is usually
not adopted. In this book, we restrict ourselves to the common policy of assuming
that the program is correct in the sense that every use of a variable is preceded by its
deﬁnition.
Classical Constant Propagation Using Def-Use Chains
Constant propagation can be performed using def-use chains as described below:
1. Create a work list Wl consisting of deﬁnitions of the form xi : x = ci occurring
in the program, where x ∈Var and ci ∈Const. The read(x) statement should
be treated as a deﬁnition x = nonconst and must be inserted in the work list.
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
111
n1
read (e); n1
n2 a = 7;b = 2; f = e;
if (f > 0)
n2
n3
a = 2;
if (f ≥e+2) n3
n4 b = c+1;
if (b ≥7) n4
n6 if (f ≥e+1) n6
n5
f = f +1; n5
n7 c = d ∗a; n7
n8 d = a +b; n8
n9 d = a +1;
f = f +1
n9
n10 e = a +b; n10
false
true
false
false
true
false
true
true
FIGURE 4.6
Def-use chains of variables a, b, c, and d for constant propagation.
Repeat the following step until Wl becomes empty.
2. Remove a deﬁnition xi : x = ci from Wl. Perform the following steps for each
def-use chain of xi.
(a) Traverse the def-use chain to locate the use of x reachable by the chain.
(b) Let the value of the use be denoted by x. If the use of x has not been
replaced by any value, then x is  .
(c) Replace the use of x by xci. This then becomes the value of x.
(d) Evaluate the expression in which the use of x occurs. If the result is a
constant value and the expression appears in the right hand side of an
assignment, replace the expression by the constant value and add the
deﬁnition to Wl. If the result is nonconst, then add the deﬁnition to Wl
without replacing the expression.
© 2009 by Taylor & Francis Group, LLC

112
Data Flow Analysis: Theory and Practice
eval(e,x) where Opd(e)∩Var  ∅
Notation: d1 = val(e1,x),d2 = val(e2,x) where {e1,e2} ⊆(Var∪Const)
e ≡(e1 bop e2)
e ≡(uop e1)
e ≡e1
Any other
d2 =  d2 = ⊥d2 ∈Const
expression
d1 =  
 
⊥
 
 
 
d1 = ⊥
⊥
⊥
⊥
⊥
⊥
⊥
d1 ∈Const
 
⊥
d1 bop d2
uop d1
Not
Applicable
FIGURE 4.7
Evaluating constantness of expressions for constant propagation.
Example 4.4
The def-use chains for our running example are shown in Figure 4.6. Initially,
the work list contains the assignments to a and b in blocks n2 and n7. When
we traverse the def-use chains of the deﬁnition in block n3, d is discovered to
be 3 in block n9. This is added to the work list and causes c to become 6
in block n7. This cause b to become 7 in block n4. Since this is a compile
time evaluation, it is valid for every possible execution of n4 and block n5
is never executed. Interestingly, compile time analysis concludes that b can
have diﬀerent values in n8 and n10 and hence is ⊥. For n8, this is conservative
imprecision since the execution never reaches n8 after b becomes 7 in n4.
Data Flow Analysis for Constant Propagation
Observe that the speciﬁcation of constant propagation in terms of def-use chains has
a highly operational ﬂavor. Data ﬂow equations provide a declarative mechanism of
deﬁning a program analysis and reduce the work to ﬁxed point computation.
Data ﬂow analysis for constant propagation uses an overall lattice L that is a prod-
uct of L. For convenience of deﬁning ﬂow functions, we represent an element in L
by sets of pairs x,dx where x ∈Var and dx ∈L.
This is a forward data ﬂow analysis. The data ﬂow equations are:
Inn =

BI
n is Start
p∈pred(n)Out p otherwise
(4.8)
Outn = fn(Inn)
(4.9)
BI contains pairs x,  for all variables x ∈Var. The conﬂuence operation  on
elements in L is deﬁned in terms of  by applying it to pairs of the same variable:
∀x1,x2 ∈L, x1 x2 = {z,dxdy | z,dx ∈x1,z,dy ∈x2, x ∈Var}
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
113
ConstGenn =

{x,eval(e, )} n is assignment x = e,Opd(e) ⊆Const
{x,⊥}
n is read(x)
∅
otherwise
DepGenn(x) =
{x,eval(e,x)} n is assignment x = e,Opd(e)∩Var  ∅
∅
otherwise
ConstKilln = ∅
DepKilln(x) =

{x,d} n is assignment x = e, x,d ∈x
{x,d} n is read(x), x,d ∈x
∅
otherwise
Function eval is deﬁned in Figure 4.7. It uses val(e,x) to denote the value of a
simple expression (consisting of a variable or a constant) in the context of the given
data ﬂow information x:
val(e,x) =
c if e is c ∈Const
d if e is x ∈Var, x,d ∈x
Example 4.5
The computation of data ﬂow values for our running example of Figure 4.4
has been shown in Figure 4.8. For brevity, we represent the data ﬂow infor-
mation as a vector da,db,dc,dd,de where dx represents the constantness value
of variable x. BI is  , , , , . The initial value of Ini and Outi for all i is
 =  , , , , .
Observe that this analysis requires four traversals over the control ﬂow
graph in reverse postorder.
In the ﬁrst iteration, d is discovered to be 3
in block n9.
Thus, c is discovered to be 6 block n7 in the third iteration.
This makes b a constant with value 7 at Exit(n3) in the fourth iteration. At
Entry(n2), b is 2 along the path from n1 and 7 along the path from n6. Observe
the non-separability of constant propagation: The constantness of b depends
on the constantness of a through c and d.
Also note that b is ⊥in n8 due to the eﬀect of n4 in spite of the fact that
control never reaches n8 after execution n4.
Properties of Constant Propagation Data Flow Framework
In this section we show that Constant Propagation framework is monotonic but non-
distributive.
THEOREM 4.1
Constant Propagation framework is monotonic.
© 2009 by Taylor & Francis Group, LLC

114
Data Flow Analysis: Theory and Practice
Iteration #1
Changes in
Changes in
Changes in
iteration #2
iteration #3
iteration #4
Inn1
 , , , , , 
Outn1  , , , ,⊥, 
Inn2
 , , , ,⊥,⊥
Outn2 7,2, , ,⊥,⊥
Inn3
7,2, , ,⊥,⊥⊥,2, ,3,⊥,⊥⊥,2,6,3,⊥,⊥⊥,⊥,6,3,⊥,⊥
Outn3 2,2, , ,⊥,⊥2,2, ,3,⊥,⊥2,2,6,3,⊥,⊥2,⊥,6,3,⊥,⊥
Inn4
2,2, , ,⊥,⊥2,2, ,3,⊥,⊥2,2,6,3,⊥,⊥2,⊥,6,3,⊥,⊥
Outn4 2, , , ,⊥,⊥2, , ,3,⊥,⊥2,7,6,3,⊥,⊥
Inn5
2, , , ,⊥,⊥2, , ,3,⊥,⊥2,7,6,3,⊥,⊥
Outn5 2, , , ,⊥,⊥2, , ,3,⊥,⊥2,7,6,3,⊥,⊥
Inn6
2,2, , ,⊥,⊥2,2, ,3,⊥,⊥2,2,6,3,⊥,⊥2,⊥,6,3,⊥,⊥
Outn6 2,2, , ,⊥,⊥2,2, ,3,⊥,⊥2,2,6,3,⊥,⊥2,⊥,6,3,⊥,⊥
Inn7
2,2, , ,⊥,⊥2,2, ,3,⊥,⊥2,⊥,6,3,⊥,⊥
Outn7 2,2, , ,⊥,⊥2,2,6,3,⊥,⊥2,⊥,6,3,⊥,⊥
Inn8
2,2, , ,⊥,⊥2,2, ,3,⊥,⊥2,2,6,3,⊥,⊥2,⊥,6,3,⊥,⊥
Outn8 2,2, ,4,⊥,⊥2,2, ,4,⊥,⊥2,2,6,4,⊥,⊥2,⊥,6,⊥,⊥,⊥
Inn9
2,2, ,4,⊥,⊥2,2,6,⊥,⊥,⊥2,⊥,6,⊥,⊥,⊥
Outn9 2,2, ,3,⊥,⊥2,2,6,3,⊥,⊥2,⊥,6,3,⊥,⊥
Inn10
⊥,2, , ,⊥,⊥⊥,2, ,3,⊥,⊥⊥,⊥,6,3,⊥,⊥
Outn10 ⊥,2, , ,⊥,⊥⊥,2, ,3,⊥,⊥⊥,⊥,6,3,⊥,⊥
FIGURE 4.8
Constant propagation data ﬂow analysis for the running example in Figure 4.4.
PROOF
Showing monotonicity of fn(x) requires showing that (x−DepKilln(x))
and DepGenn(x) are monotonic.
DepKilln(x) is {x,dx} for assignment x = e or read(x). In all other cases it is
∅. Since it does not depend on x,
∀x1  x2 ∈L : (x1 −DepKilln(x1))  (x2 −DepKilln(x2))
Showing monotonicity of DepGenn(x) reduces to showing
∀e ∈Expr,∀x1,x2 ∈L : x1  x2 ⇒eval(e,x1)  eval(e,x2)
Function eval(e,x) examines the data ﬂow values of the operands of e. From
its deﬁnition in Figure 4.7, it is easy to see that the data ﬂow value computed
by eval(e,x) preserves the partial order.
THEOREM 4.2
Constant Propagation framework is non-distributive.
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
115
PROOF
Using the arguments similar to those in Theorem 4.1, it can be
shown in terms of eval().
∃e ∈Expr, ∃x1,x2 ∈L : eval(e,x1 x2)  eval(e,x1)  eval(e,x2)
This is demonstrated by expression (a + b) in block n10 in the program in
Figure 4.4 for x1 = 7,2,−,−,− and x2 = 2,7,−,−,− where “−” indicates the
values which do not matter.
Presence of non-distributivity shows the limits of static analysis: Unless all paths
are traversed independently, which may require exponential amount of work, a static
analysis is likely to miss out on useful information even if the information is indepen-
dent of program execution. This happens because of sharing of information across
distinct paths as shown by the following example.
Example 4.6
Only two execution paths reach n10: (n1, n2, n7), and (n1, n2, n3, n6, n8, n9,
n3, n6, n7, n9, n3, n4, n10). The values of a, b, and e at Exit(n10) along the ﬁrst
path are 7, 2, and 9 respectively whereas along the second path they are 2,
7, and 9. Static summary of constantness information should conclude that
a and b are ⊥and e is 9. However, due to non-distributivity, our analysis
concludes that all the three variables are ⊥. Eﬀectively, the ﬂow function in
n10 uses all possible combinations of a and b including those across diﬀerent
paths: a = 7 and b = 2 resulting in e = 9; a = 2 and b = 7 resulting in e = 9; a = 2
and b = 2 resulting in e = 4; a = 7 and b = 7 resulting in e = 14. Observe that
the last two combinations are infeasible because there is no execution path
reaching n10 along which a and b can both be 2 or both be 7. Fortunately,
the imprecision caused by non-distributivity is safe because a ⊥variable does
not enable any transformation.
Constant propagation is not fast, and hence is not rapid. We leave it for the reader
to construct suitable examples.
4.2.4
Variants of Constant Propagation
Constant propagation is a very useful analysis in practice. It improves the eﬃciency
of programs by advancing some computations to compile time. It facilitates many
other optimizations such as elimination of dead code (i.e., assignments which deﬁne
variables which are not used later) as well as unreachable code. The latter simpli-
ﬁes control ﬂow and may reduce branch delays on pipelined architectures. It can
help in strength reduction and may enable many loop optimizations that require loop
iterations bounds to be known at compile time.
It is not surprising that many variants of constant propagation have been proposed.
The formulation which we have presented in the preceding sections is called full
© 2009 by Taylor & Francis Group, LLC

116
Data Flow Analysis: Theory and Practice
constant propagation to distinguish it from other variants of constant propagation
which restrict the analysis in some ways.
Conditional Constant Propagation
As observed in Examples 4.3, 4.4, and 4.5, the value b = 7 in n4 causes the control
ﬂow to leave the loop. Block n5 is never executed and the value 7 does not reach n8
resulting in both b and d being constant in n8. Conditional constant propagation can
discover this by evaluating the branching conditions appearing on execution paths.
In order to achieve the above, we create a lattice {reachable,notReachable} with
the partial order notReachable  reachable. Let L be the product lattice of L. We
create a new product lattice Lc = {reachable,notReachable}× L. Values in Lc are
pairs status,x where status is either reachable or notReachable and x is the con-
stantness information as discovered in the unconditional constant propagation. The
conﬂuence operation c of values in Lc ignores the values which are not reachable
and is as deﬁned below.
status1,x1 c status2,x2
status2 = reachable status2 = notReachable
status1 = reachable
reachable,x1 x2
reachable,x1
status1 = notReachable
reachable,x2
notReachable, 
Reachability status is determined by evaluating branching conditions using func-
tion evalCond(m,x) which computes true, false, or undeﬁned depending upon the
following: If basic block m contains a condition at the end and data ﬂow informa-
tion x contains constant values for all variables required to evaluate the condition,
then evalCond(m,x) is the result of the condition. Otherwise, evalCond(m,x) is
undeﬁned. Propagation of data ﬂow information along the out edge associated with
the outcome is ensured by using an edge ﬂow function.
An alternative to such a data ﬂow analysis is to simply delete the edge that will
not be executed instead of qualifying data ﬂow information with reachable and
notReachable values. However, this may not be possible if branch outcome is likely
to be inﬂuenced by calling contexts. In particular, when context sensitive interpro-
cedural data ﬂow analysis is performed a branch outcome may be diﬀerent in diﬀer-
ent contexts and deletion of an edge may not be possible. Further, the abstraction
of conditional propagation along edges is a powerful mechanism that can compute
more precise data ﬂow information for analyses such as null pointer analysis: For this
analysis, a condition that checks for the nullity of a pointer can propagate diﬀerent
outcomes along the two out edges of a condition.
The propagation function for an edge m →n, is deﬁned as follows:
gm→n(status,x) =

notReachable,  evalCond(m,x)  undeﬁned and
evalCond(m,x)  label(m →n)
status,x
otherwise
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
117
Iteration #1
Changes in
Changes in
iteration #2
iteration #3
Inn1
R, , , , , , 
Outn1 R, , , , ,⊥, 
Inn2
R, , , , ,⊥,⊥
Outn2 R,7,2, , ,⊥,⊥
Inn3
R,7,2, , ,⊥,⊥ R,⊥,2, ,3,⊥,⊥
R,⊥,2,6,3,⊥,⊥
Outn3 R,2,2, , ,⊥,⊥ R,2,2, ,3,⊥,⊥
R,2,2,6,3,⊥,⊥
Inn4
R,2,2, , ,⊥,⊥ R,2,2, ,3,⊥,⊥
R,2,2,6,3,⊥,⊥
Outn4 R,2, , , ,⊥,⊥ R,2, , ,3,⊥,⊥
R,2,7,6,3,⊥,⊥
Inn5
R,2, , , ,⊥,⊥ R,2, , ,3,⊥,⊥
R,2,7,6,3,⊥,⊥
Outn5 R,2, , , ,⊥,⊥ R,2, , ,3,⊥,⊥
R,2,7,6,3,⊥,⊥
Inn6
R,2,2, , ,⊥,⊥ R,2,2, ,3,⊥,⊥ N, =  , , , , , 
Outn6 R,2,2, , ,⊥,⊥ R,2,2, ,3,⊥,⊥ N, =  , , , , , 
Inn7
R,2,2, , ,⊥,⊥ R,2,2, ,3,⊥,⊥
R,2,2,6,3,⊥,⊥
Outn7 R,2,2, , ,⊥,⊥ R,2,2,6,3,⊥,⊥
R,2,2,6,3,⊥,⊥
Inn8
R,2,2, , ,⊥,⊥ R,2,2, ,3,⊥,⊥
R,2,2,6,3,⊥,⊥
Outn8 R,2,2, ,4,⊥,⊥ R,2,2, ,4,⊥,⊥
R,2,2,6,4,⊥,⊥
Inn9
R,2,2, ,4,⊥,⊥ R,2,2,6,⊥,⊥,⊥
R,2,⊥,6,⊥,⊥,⊥
Outn9 R,2,2, ,3,⊥,⊥ R,2,2,6,3,⊥,⊥
R,2,⊥,6,3,⊥,⊥
Inn10
R,⊥,2, , ,⊥,⊥ R,⊥,2, ,3,⊥,⊥
R,⊥,⊥,6,3,⊥,⊥
Outn10 R,⊥,2, , ,⊥,⊥ R,⊥,2, ,3,⊥,⊥
R,⊥,⊥,6,3,⊥,⊥
FIGURE 4.9
Conditional constant propagation for the running example in Figure 4.4.
The data ﬂow equations remain much the same except that now they must honor
the reachability status as shown below.
Inn =

reachable,BI
n is Start
C
p∈pred(n)gp→n(Out p) otherwise
Outn =
reachable, fn(x)
Inn = reachable,x
notReachable,  otherwise
In the beginning, only the Start block is assumed to be reachable and the initial
value associated with all other program points is notReachable, . This is required
for computing the MFP solution. If we use the initial value reachable, , the
analysis will converge on a ﬁxed point that may not be maximum. The result would
be imprecise but safe.
© 2009 by Taylor & Francis Group, LLC

118
Data Flow Analysis: Theory and Practice
Example 4.7
Figure 4.9 provides the data ﬂow values for conditional constant propagation
in our running example. Since the data ﬂow information is not propagated
from n4 to n5, b remains constant in the loop and analysis converges in three
iterations rather than four.
It is easy to see that conditional constant propagation can discover more precise
information than unconditional constant propagation. It is guaranteed to be at least
as good, if not better.
Copy Constant Propagation
Copy constant propagation limits the expressions appearing on the right hand side of
an assignment to simple variables or constants. Such statements have been called
copies in Section 2.3.4 to describe copy propagation using reaching deﬁnitions.
There are two fundamental diﬀerences between the analysis presented here and the
copy propagation described in Section 2.3.4: (a) the analysis presented here allows
replacement of variables by constants only whereas the earlier analysis allowed re-
placement of variables by other variables also, and (b) the analysis presented here
takes care of transitive eﬀects of replacements whereas the earlier analysis does not
do so.
Copy constant propagation does not generate new constants based on the values of
variables. Hence it is guaranteed to compute only a ﬁnite number of constants. Thus
the component lattice L is ﬁnite. The ﬂow function does not evaluate any expres-
sion involving a variable. Thus the deﬁnitions of ConstKilln and DepKilln(x) do not
change. ConstGenn and DepGenn(x) change in the following manner. DepGenn(x)
is restricted to copy assignments and DepGenn(x) computes ⊥value for non-copy
assignments. The new deﬁnitions are:
ConstGenn =

{v,eval(e, )} n is assignment v = e,Opd(e) ⊆Const
{v,⊥}
n is read(v) or a non-copy assignment to v
∅
otherwise
DepGenn(x) =
{v,d} n is assignment v = w, w,d ∈x
∅
otherwise
Observe that the expression evaluation in the above deﬁnition is restricted to constant
operands only.
Full constant propagation is non-distributive due to the use of function eval(e,x).
All other terms involved in deﬁning fn are distributive. Copy constant propagation is
distributive because it does not involve eval(e,x). However, due to non-separability,
the framework remains non-fast.
Since expressions are not evaluated, this analysis ﬁnds fewer constants and is lim-
ited in scope compared to the full constant propagation.
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
119
Linear Constant Propagation
A slightly more general formulation than copy constant propagation allows expres-
sions to appear on the right hand side but these expressions may contain at most a
single variable. This requires a restricted version of eval. Since this analysis com-
putes new constants, the lattice L is inﬁnite, unlike copy constant propagation. How-
ever, similar to copy constant propagation, linear constant propagation is distributive
because the number of variables in the right hand side is restricted to one. However,
due to non-separability, the framework remains non-fast.
4.3
Discovering Properties of Pointers
Pointers allow indirect modiﬁcation of data thereby making it diﬃcult to discover
useful information from programs. They reduce the eﬀectiveness of program analy-
sis tools. This is because, in the absence of precise analysis of pointer manipulations,
program analysis must conservatively assume that any data object could be modiﬁed
by any pointer. Practically, this can be mitigated somewhat by using type infor-
mation and by conﬁning the conservative assumptions within variables of the same
type. However, if information about the possible manipulations performed through
pointers is available, it can enhance the precision of other analyses.
This section presents pointer analyses for stack and static data. These analyses
capture relationships between pointers and other variables or pointers. This is diﬀer-
ent from other analyses which we have seen because the domain of data ﬂow values
of an entity did not involve other entities.
Our model of pointer manipulations is based on C except that we do not take into
account pointer arithmetic. Since the size of stack and static data is ﬁxed, pointers
can point to only a ﬁxed set of locations that are known at compile time. We as-
sume that ﬁeld references of a structure are ﬂattened out into a new pointer name:
A reference like x.f occurring in a statement can be modeled as a new pointer xf
to which the pointer x points to. Further, a null assignment to a pointer x is treated
as assigning address 0 to x. Thus assignment x = null is treated as x = &zero where
zero is a special symbol whose address is 0.
4.3.1
Points-To Analysis of Stack and Static Data
This analysis establishes points-to relation between pointer variables and memory
locations under the assumption that the program is type correct in terms of pointer
manipulations.
DEFINITION 4.5
A pointer variable x points to variable y at a program
point u, denoted x y, if it holds the address of variable y at u.
© 2009 by Taylor & Francis Group, LLC

120
Data Flow Analysis: Theory and Practice
Points-to relation is neither reﬂexive (because x x may not hold), nor symmetric
(because x y  y x), nor transitive (because x y,y z  x z).
We assume that the left hand side of a pointer assignment is either a pointer vari-
able x or a pointer indirection ∗x. The right hand side could be either an address
expression &x, a pointer variable x, or a pointer indirection ∗x.
The pointers which are likely to be modiﬁed by a pointer assignment are called
left locations of the assignment. The addresses which may be assigned to the left
locations are called the right locations of the assignment. Let x be the set repre-
senting the points-to relations that hold just before assignment statement n. The left
and the right locations of n which depend on x are denoted by DepLeftLn(x) and
DepRightLn(x). The left and right locations which depend solely on the local eﬀect
of n are denoted by ConstLeftLn and ConstRightLn.
Consider an assignment statement lhsn = rhsn. The left and the right locations of
n are deﬁned as follows:
Left Locations
Right Locations
lhsn ConstLeftLn DepLeftLn(x) rhsn ConstRightLn
DepRightLn(x)
x
{x}
∅
x
∅
{y | (x y) ∈x}
∗x
∅
{y | (x y) ∈x} ∗x
∅
{z | {x y,y z} ⊆x}
&x
{x}
∅
Points-to relation between the left and the right locations is established in terms of
new points-to pairs which are generated and the points-to pairs which cease to hold
due to the eﬀect of a basic block.
ConstGenn = {x y | x ∈ConstLeftLn,y ∈ConstRightLn}
DepGenn(x) = {x y | (x ∈ConstLeftLn,y ∈DepRightLn(x)), or
(x ∈DepLeftLn(x),y ∈ConstRightLn), or
(x ∈DepLeftLn(x),y ∈DepRightLn(x))}
ConstKilln = {x y | x ∈ConstLeftLn}
DepKilln(x) = {x y | x ∈DepLeftLn(x)}
DepKilln(x) depends on DepLeftLn(x) which involves pointer indirection on the left
hand side of a pointer assignment. Thus it captures the indirect eﬀect of an assign-
ment due to pointer indirection and hence the choice of x is critical for ensuring
conservative approximation on the safer side. We explain this below.
DEFINITION 4.6
If a pointer z is modiﬁed by a pointer assignment
regardless of the execution path taken to reach the assignment, then such a
modiﬁcation is called a strong update of z. If z may be modiﬁed by the assign-
ment when the execution reaches along some path and may not be modiﬁed
when it reaches along some other path, such a modiﬁcation of z is called a
weak update of z.
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
121
An assignment z = w causes a strong update of z. Contrast this with the assignment
∗x = w such that x z holds along some path reaching the assignment. If the execution
follows this path, then the assignment modiﬁes z, otherwise it does not modify z. If
x z holds along every path, then z is modiﬁed by the assignment in every execution.
In order to capture the indirect eﬀect of such an assignment, there is a need to make
a distinction between the points-to relations which cause weak updates from those
which cause strong updates. The former is called may points-to relation while the
latter is called must points-to relation.
DEFINITION 4.7
If pointer x holds the address of variable y at program
point u along some path from Start to u, then x y at u under may points-to
relation. If x holds the address of y along every path from Start to u, then x y
at u under must points-to relation.
It is easy to see that a may points-to relation is weaker than a must points-to
relation: If x must point to y at u then x may point to y at u but not vice-versa.
Since may points-to relations must not miss any points-to pair which may hold at
a program point, only the pairs aﬀected by a strong update must be removed. Thus,
for computing MayOutn, DepKilln must depend on MustInn. Since must points-to
relations should include a points-to pair only if it is guaranteed to hold, all pairs
aﬀected by weak update must be removed. Thus, for computing MustOutn, DepKilln
must depend on MayInn. We explain this in Example 4.9.
The data ﬂow equations for points-to analysis are:
MayInn =

BI
n is Start

p∈pred(n)
MayOutn otherwise
(4.10)
MayOutn = fn(MayInn,MustInn)
(4.11)
MustInn =

BI
n is Start

p∈pred(n)
MustOutn otherwise
(4.12)
MustOutn = fn(MustInn,MayInn)
(4.13)
where ﬂow function fn is deﬁned as follows:
fn(x1,x2) = (x1 −Killn(x2))∪Genn(x1)
(4.14)
Observe the use of diﬀerent sets as arguments to Genn(x) and Killn(x). The Genn(x)
and Killn(x) are deﬁned in the usual manner:
Genn(x) = ConstGenn ∪DepGenn(x)
Killn(x) = ConstKilln ∪DepKilln(x)
In the intraprocedural context, BI is ∅for both may and must point-to because no
pointer points to any variable at Start.
© 2009 by Taylor & Francis Group, LLC

122
Data Flow Analysis: Theory and Practice
n1 b = &d; n1
n2 c = b; n2
n3 a = &b; n3
n4 ∗a = a; n4
n5 a = &c; n5
n6 a = ∗a; n6
n7 ∗b = c; n7
• Var = {a,b,c,d}
U = { a a, a b, a c, a d,
b a, b b, b d, b d,
c a, c b, c c, c d,
d a, d b, d c, d d }
• Lmay = 2U,⊇,  may = ∅,⊥may = U
• Lmust = La ×Lb ×Lc ×Ld
We show the component lattice L a:
{a a,a b,a c,a d}
{a a}
{a b}
{a c}
{a d}
∅
FIGURE 4.10
Example program for points-to analysis.
Figure 4.10 illustrates the lattices for may and must points-to analysis. Observe
that the  value for must points-to in Figure 4.10 is {a a,a b,a c,a d}. It is easy
to see to that this is a value that cannot naturally occur in any instance of must points-
to analysis because a pointer can pointer to at most one location in must points-to
analysis. This is an example of an artiﬁcial value added to a meet semilattice for
convenience. Since the descending chain condition is satisﬁed, the resulting lattice
is a complete lattice. By contrast, the lattice for may points-to analysis is a naturally
complete lattice and its  element can actually occur during may points-to analysis.
Technically, the lattice for must points-to analysis is a tuple of values from com-
ponent lattice. For example, given the lattice in Figure 4.10, if a points to c, b does
not point to any location, c points to d, and d points to b, then the must points-to
information should be represented as {a b},∅,{c d},{d b}. However, for compat-
ibility with may points-to analysis, we treat it as a ﬂattened set rather than as a vector
of sets for each pointer variable. Thus, we represent the same data ﬂow information
by {a b,c d,d b}.
Example 4.8
Consider the example program in Figure 4.10. The computation of may and
must points-to relations has been shown below. The  for may is ∅whereas
that for must is the universal set U of points-to pairs. The computation of
may and must proceeds in an interleaved fashion.
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
123
Iteration #1
Changes in
Iteration #2
Changes in
Iteration #3
MayInn1
∅
MustInn1
∅
MayOutn1
{b d}
MustOutn1
{b d}
MayInn2
{b d}
{a b,a d,b b,b d,c d}
{a b,a d,b b,
b d,c b,c d}
MustInn2
{b d}
∅
MayOutn2
{b d,c d}
{a b,a d,b b,b d,c b,c d}
MustOutn2
{b d,c d}
∅
MayInn3
{b d,c d}
{a b,a d,b b,b d,c b,c d}
MustInn3
{b d,c d}
∅
MayOutn3
{a b,b d,c d}
{a b,b b,b d,c b,c d}
MustOutn3
{a b,b d,c d}
{a b}
MayInn4
{a b,b d,c d}
{a b,b b,b d,c b,c d}
MustInn4
{a b,b d,c d}
{a b}
MayOutn4
{a b,b b,c d}
{a b,b b,c b,c d}
MustOutn4
{a b,b b,c d}
{a b,b b}
MayInn5
{b d,c d}
{a b,a d,b b,b d,c b,c d}
MustInn5
{b d,c d}
∅
MayOutn5
{a c,b d,c d}
{a c,b b,b d,c b,c d}
MustOutn5
{a c,b d,c d}
{a c}
MayInn6
{a b,a c,b b,
b d,c d}
{a b,a c,b b,b d,c b,c d}
MustInn6
{c d}
∅
MayOutn6
{a b,a d,b b,
b d,c d}
{a b,a d,b b,b d,c b,c d}
MustOutn6
{c d}
∅
MayInn7
{a b,a d,b b,
b d,c d}
{a b,a d,b b,b d,c b,c d}
MustInn7
{c d}
∅
MayOutn7
{a b,a d,b b,
b d,c d,d d}
{a b,a d,b b,b d,c b,c d,
d b,d d}
MustOutn7
{c d}
∅
Since (a b) ∈MayInn4, assignment ∗a = a generates (b b) ∈MayOutn4. Fur-
ther, since (a b) ∈MustInn4, this assignment causes a strong update of b caus-
ing the removal of b d from MayInn4. The third iteration is required c b from
MayOutn2 to MayInn2.
Example 4.9
Consider the program ﬂow graph in Figure 4.11 on the next page which illus-
© 2009 by Taylor & Francis Group, LLC

124
Data Flow Analysis: Theory and Practice
n1 a = &b n1
n2 c = &a n2n3 e = &d n3
n4
∗c = e
n4
n5
∗c = e
n4
• a b in block 5 along path 1,3,4,5 but not along path
1,2,4,5.
• Required: a b ∈MayInn5 and a b  MustInn5
• If DepKilln4 for MayOutn4 is deﬁned in terms of
MayInn4 then a b  MayOutn4 because a is in
DepLeftLn4(MayInn4)
• If DepKilln4 for MustOutn4 is deﬁned in terms of
MustInn4 then a b  MustOutn4 because a is in
DepLeftLn4(MustInn4)
FIGURE 4.11
Inverse dependence of may and must points-to relations for Kill.
trates that the dependence between may and must points-to relations for Kill
is not only mutual, but is also inverse. In block n5, the relation a b holds
along the path (n1,n3,n4,n5) but not along the path (n1,n2,n4,n5). This is be-
cause along the latter path, c a and the assignment in n4 modiﬁes a. Since
c a ∈MayInn4 and c a  MustInn4, a is a left location in may points-to rela-
tion but not in must points-to relation. Thus if MayInn4 is used for deﬁning
DepKilln4 for computing MayOutn4, a b will not exist in MayOutn4 which is
incorrect. Similarly, if MustInn4 is used for deﬁning DepKilln4 for computing
MustOutn4, a b will exist in MustOutn4 which is incorrect.
If may and must analyses are performed independently, then
MayOutn = fn(MayInn,∅)
MustOutn = fn(MustInn,U)
In other words, in the absence of must points-to information, no points-to pair can
be killed by indirect eﬀect of an assignment since no strong update is known. In the
absence of may points-to information, every points-to pair must be assumed to be
killed by indirect eﬀect of an assignment since no weak update is known.
Observe that unlike any other ﬂow function, the ﬂow function for points-to anal-
ysis given by Equation (4.14) is a binary function rather than a unary function. It
has been deﬁned so to capture the inverse dependence of may and must informa-
tion through the DepKilln part. The overall lattice for the data ﬂow Equations (4.11)
through (4.13) is a product lattice of the lattices for may and must points-to relations
and the ﬂow function is a unary function for the values in this overall lattice. The  
element of the overall lattice is the pair ∅,U whereas ⊥is U,∅.
Given a constant must points-to information, it is easy to see that the ﬂow func-
tions in may points-to analysis are monotonic. This is because the DepKilln(x) com-
ponent becomes constant and given a larger x, DepGenn(x) computes a larger set of
points-to pairs. Since must points-to analysis has also been deﬁned using the same
components, given a constant may points-to information, the ﬂow functions of must
points-to analysis are also monotonic.
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
125
n1 ∗x = y n1
n2 x = &z n2n3 y = &w n3
n4 ∗x = y n4
n1 ∗x = y n1
n2 b = &c
c = &d n2n3 b = &e
e = &d n3
n4 a = ∗b n4
(a) Example for may points-to analysis
(b) Example for must points-to analysis
FIGURE 4.12
Non-distributivity of points-to analysis.
Example 4.10
Figure 4.12 shows the non-distributivity of points-to analysis using the ﬂow
function associated with node n4.
Consider the example for may points-to analysis. Assuming that the must
points-to information is constant, non-distributivity of may points-to analysis
depends on DepGenn(x). Let x1 be the may points-to information along the
edge n2 →n4 and let x2 be the may points-to information along the edge
n3 →n4. Then x1 = {x y}, x2 = {y w} and:
DepGenn(x1 ∪x2) = {x y,y w,z w}
DepGenn(x1) = {x y}
DepGenn(x2) = {y w}
DepGenn(x1 ∪x2) ⊃DepGenn(x1) ∪DepGenn(x2)
Consider the example for must points-to analysis under similar situations. In
this case x1 = {b c,c d}, x2 = {b e,e d} and:
DepGenn(x1 ∩x2) = ∅
DepGenn(x1) = {a d}
DepGenn(x2) = {a d}
DepGenn(x1 ∩x2) ⊂DepGenn(x1) ∩DepGenn(x2)
We leave it to the reader to construct examples to show that the data ﬂow frame-
work of points-to analysis is not fast.
Points-To Analysis with Degree of Certainty
Instead of computing separate may and must points-to sets, a points-to pair x y
can be qualiﬁed with degrees of certainties may and must and can be denoted xmyy
and xmuy. This reduces computation of may and must points-to sets to a single
© 2009 by Taylor & Francis Group, LLC

126
Data Flow Analysis: Theory and Practice
( )
unknown
must
no
may
(⊥)
( )
x uny
xmuy
x noy
xmyy
(⊥)
(a) Degree of certainty
(b) Points-to relation between x and y
FIGURE 4.13
Lattices for points-to analysis with degree of certainty.
analysis unlike MayIn/MayOut and MustIn/MustOut. In order to deﬁne data ﬂow
analysis, we add two more degrees of certainty: x noy indicates that x does not point
to y and x uny indicates that nothing is known about the points-to relation between x
and y.∗This results in the component lattices shown in Figure 4.13. The conﬂuence
operations used in deﬁning the data ﬂow analysis are induced by these lattices and
are left implicit in the description of the analysis.
The left and right locations are now qualiﬁed with degrees of certainty. However,
values unknown and no are irrelevant in the context of a pointer assignment. The
left locations are deﬁned as follows:
lhsn ConstLeftLn
DepLeftLn(x)
x
{x,must}
∅
∗x
∅
{y,d | (x d y) ∈x,d ∈{may,must}}
The right locations are deﬁned as follows:
rhsn ConstRightLn
DepRightLn(x)
&x
{x,must}
∅
x
∅
{y,d | (x d y) ∈x,d ∈{may,must}}
∗x
∅
{z,d1 d2 | {x d1 y,y d2 z} ⊆x,{d1,d2} ⊆{may,must}}
When the left hand side is variable x, all points-to pairs with x as the source are
removed. If the right hand side is an address expression, new must and no points-
∗no and unknown need not be represented explicitly. x noy can be represented by ensuring that xmuy
or x
myy is not present in the set enumerating the points-to relation. For x uny, it is suﬃcient to record
whether the data ﬂow values associated with a node have been computed or not. While combining the
data ﬂow information from predecessors, if the values have not been computed for a predecessor m, it can
be ignored in the merge operation; this has the eﬀect of assuming that the data ﬂow information associated
with m is  . This has been achieved on line 2 of the algorithm presented in Figure 3.15 on page 90 by
excluding the predecessors along a back edge during the initialization.
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
127
to pairs are generated purely due to local eﬀect regardless of the existing points-to
relations. ConstGenn is deﬁned only in the context of assignments such as x = &y
whereas ConstKilln is deﬁned only when the left hand side is a variable such as x.
ConstGenn = {xmuy | x,must ∈ConstLeftLn,y,must ∈ConstRightLn} ∪
{x noz | x,must ∈ConstLeftLn,ConstRightLn  ∅,
z,d  ConstRightLn}
ConstKilln = {x d y | x,must ∈ConstLeftLn}
In other situations ConstLeftLn ∩ConstRightLn = ∅. For these situations let
Leftn(x)
= ConstLeftLn ∪DepLeftLn(x)
Rightn(x) = ConstRightLn ∪DepRightLn(x)
The dependent information that is generated and killed by a pointer assignment is
deﬁned as follows:
DepGenn(x) = {x d y | x,dl∈Leftn(x),y,dr∈Rightn(x),d = dl dr}∪
{xmyy | x,may ∈Leftn(x), xmuy ∈x}∪
{x noy | x,must ∈Leftn(x),y,d  Rightn(x)}
The ﬁrst term in the deﬁnition of DepGenn(x) is the result of a combination of the
left and right hand sides. The second term lowers the degree of certainty of xmuy in
x to x
myy due to a possible modiﬁcation of x by the assignment. The third term is a
replacement of points-to pairs killed by the assignment.
DepKilln(x) = {x d y | x,must ∈DepLeftLn(x)}∪
{xmuy | x,may ∈DepLeftLn(x)}
The ﬁrst term in DepKilln(x) represents the guaranteed modiﬁcation of x by the
pointer assignment n. The second term removes xmuy so that it can be replaced
by the generated pair xmyy.
The ﬁnal data ﬂow equations are:
Inn =

BI
n is Start
p∈pred(n)Out p otherwise
Outn = fn(Inn) = (Inn −Killn(Inn))∪Genn(Inn)
where BI = {x noy | x is a pointer variable and y is any variable }.
Example 4.11
We show the computation of points-to pairs qualiﬁed with the degree of cer-
tainty for the example program in Figure 4.10 on page 122. For simplicity, we
© 2009 by Taylor & Francis Group, LLC

128
Data Flow Analysis: Theory and Practice
omit the pairs representing no except for Inn where we list the BI. Since we
perform round-robin analysis and traverse the graph in reverse postorder, the
pairs representing unknown are required only in the ﬁrst iteration and only
for Outn6. We leave them also implicit. Observe that now may and must are
mutually exclusive and the resulting information is more precise.
Iteration #1
Changes in
Iteration #2
Changes in
Iteration #3
Inn1
{x noy | x,y ∈{a,b,c,d}}
Outn1
{bmud}
Inn2
{bmud}
{amyb,amyd,bmyb,bmyd,
cmyd}
{amyb,amyd,bmyb,
bmyd,cmyb,cmyd}
Outn2
{bmud,cmud}
{amyb,amyd,bmyb,bmyd,
cmyb,cmyd}
Inn3
{bmud,cmud}
{amyb,amyd,bmyb,bmyd,
cmyb,cmyd}
Outn3
{amub,bmud,cmud}
{amub,b
myb,b
myd,c
myb,
cmyd}
Inn4
{amub,bmud,cmud}
{amub,bmyb,bmyd,cmyb,
cmyd}
Outn4
{amub,bmub,cmud}
{amub,bmub,cmyb,cmyd}
Inn5
{bmud,cmud}
{amyb,amyd,bmyb,bmyd,
cmyb,cmyd}
Outn5
{amuc,bmud,cmud}
{amuc,bmyb,bmyd,cmyb,
cmyd}
Inn6
{amyb,amyc,bmyb,bmyd,
cmud}
{amyb,amyc,bmyb,bmyd,
cmyb,cmyd}
Outn6
{amyb,amyd,bmyb,bmyd,
cmud}
{amyb,amyd,bmyb,bmyd,
cmyb,cmyd}
Inn7
{amyb,amyd,bmyb,bmyd,
cmud}
{amyb,amyd,bmyb,bmyd,
cmyb,cmyd}
Outn7
{a
myb,a
myd,b
myb,b
myd,
cmud,d myd}
{a
myb,a
myd,b
myb,b
myd,
cmyb,cmyd,d myb,d myd}
The analysis still requires three iterations.
Example 4.12
We illustrate non-distributivity of points-to analysis with the degree of cer-
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
129
tainty by enumerating MOP and MFP assignments for the example in part (a)
of Figure 4.12 on page 125.
MOP Assignment MFP Assignment
Outn2 {xmuz}
{xmuz}
Outn3 {ymuw}
{ymuw}
Outn4 {xmyz,ymyw}
{xmyz,ymyw,zmyw}
4.3.2
Alias Analysis of Stack and Static Data
An alternative way of representing information about pointers is to use the relation
of aliasing. Aliasing is deﬁned between pointer expressions which may use derefer-
encing operations, such as x, ∗x, ∗∗x etc.
DEFINITION 4.8
A pointer expression e1 is aliased to pointer expression
e2 at program point u, denoted e1  e2, if the expressions e1 and e2 evaluate to
the same address at u.
A Comparison of Points-to and Alias Relations
Similar to points-to relation, an alias pair e1  e2 that holds along all paths reaching
u is a must alias; if it holds along some paths then it is a may alias. The lattices of
may and must aliases are similar to the lattices for may and must points-to relations
illustrated in Figure 4.10.
Aliasing is diﬀerent from points-to relation in the following sense. Although it is
possible to create points-to pairs between pointer expressions such as (∗x) (∗∗y),
the points-to analysis represents the same information by a pair z w, where by con-
struction, z and w are both variable names such that z is the target of x and w is the
target of ∗∗y. This is possible since points-to analysis is deﬁned in terms of loca-
tions that are compile time constants whereas aliasing is a relation deﬁned in terms
of address expressions that can be evaluated only at run time. This information can-
not be represented as an alias by using w because an alias does not relate a pointer
expression to the address it holds but relates pointer expressions that hold the same
address and the target of the two pointer expressions is left implicit. Hence, alias
pair ∗x  ∗∗y needs to be stored.
An alternative way of comparing points-to relations and alias relations is to view
them in terms of a memory graph in which edges represent points-to pairs. Alias
pairs represent paths that reach the same node in the graph. As a consequence,
unlike points-to relation, alias relation is both symmetric and reﬂexive. must aliases
are transitive and may aliases are not transitive.
© 2009 by Taylor & Francis Group, LLC

130
Data Flow Analysis: Theory and Practice
a = b
d = &c
∗a = c
a
b
c
d
a
a
&a
&b
&c
&d
l1
l2
(a) Sequence of assignments
(b) Resulting memory graph
FIGURE 4.14
The need of link aliases in computing node aliases.
The diﬀerence that aliasing involves pointer expressions whereas points-to rela-
tions involves names of variables implies that in points-to relations, an edge in the
memory graph is always represented by a single points-to pair. In the presence of
cycles in a data structure, there are an inﬁnite number of paths reaching a node. Thus
alias analysis may derive inﬁnite aliases. To see this, consider the assignments se-
quence a = &b followed by b = &b creating a self loop around b. Thus we have an
alias a  ∗b. However, since ∗b, ∗∗b, ∗∗∗b etc. all point to b, we have all possible
aliases a  ∗∗b, a  ∗∗∗b, b  ∗b, ∗b  ∗∗b, ∗∗b  ∗b, etc. In points-to analysis,
the same information is represented by two pairs a b and b b.
Due to the presence aliases resulting from pointer indirections, it becomes impor-
tant to distinguish between node and link aliases which are deﬁned below.
DEFINITION 4.9
Pointer expressions e1 and e2 are node aliases
if
their r-values are same but l-values are diﬀerent; they are link aliases if their
l-values are also same. An assignment a = b creates a node alias a  b and
link aliases ∗a  ∗b, ∗∗a  ∗∗b, etc.
In terms of paths in memory graph, link aliases relate paths that have a non-empty
common suﬃx whereas node aliases relate paths with disjoint non-empty suﬃxes.
In order to deﬁne node aliases for an assignment a = &b, we also introduce a
ﬁctitious pointer expression &b which is assumed to have a unique l-value; its r-value
is b. By deﬁnition, ∗&b = b. Assignment a = &b, results in a node alias a  &b. If
this is not done, we will have to capture the eﬀect of the assignment by alias pair
∗a  b which is not a node alias but a link alias.
Link aliases can be computed from node aliases and we restrict our analysis to
node aliases only. However, it is necessary to identify link aliases at intermediate
stages as explained in the following example. In the rest of this section, we reserve
the notation e1  e2 to node aliases only; where link aliases are required, they are
explicitly deﬁned in terms of node aliases.
Example 4.13
Consider the assignment sequence in Figure 4.14. The assignment ∗a = c cre-
ates the link l2 in the memory graph thereby creating the node aliases ∗a  ∗d,
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
131
s
e1
s
e2
s
e3
s
e1
s
e2
s
e3
(a) Memory graph before
the assignment s = e2
(b) Memory graph after the
assignment s = e2
FIGURE 4.15
Direct and indirect node aliases generated as a result of a pointer assignment.
∗b  ∗d and ∗b  c. In order to discover these node aliases, we need to use the
fact that ∗b is a link alias of ∗a (sharing the link l2) and ∗d is a link alias of c
(sharing the link l1).
In the general situation, given an assignment lhsn = rhsn we say that all link aliases
of lhsn get node-aliased to all node and link aliases of rhsn that are not modiﬁed by
the assignment. Unlike points-to analysis, alias analysis is signiﬁcantly inﬂuenced
by the choice of representation of the alias information. When alias relation is repre-
sented in the form of pairs, the node aliases computed by relating appropriate aliases
of lhsn and rhsn, the resulting aliases are direct aliases. However, due to possible
indirections of aliases of lhsn and rhsn, indirect node aliases are also created as ex-
plained in the following example.
Example 4.14
Consider the memory graphs in Figure 4.15. As a result of the assignment
s = e2, direct aliases s  e2 and ∗e1  e2 are created. However, node aliases
∗s  e3 and ∗∗e1  e3 must also be identiﬁed. These are examples of indirect
node aliases.
Computing indirect aliases can be avoided by representing alias relations using
graphs rather than pairs but the graph representation results in imprecision due to
transitivity: When graph representation of two alias pairs x  y and y  z are merged
at a join point, their targets are represented by the same node in the graph resulting in
a spurious alias x  z. This makes the may alias information transitive even though
the may alias relation is not transitive.
Points-to analysis does not have any of the above problems because it is restricted
to stack locations and there is a one-to-one mapping between the points-to pairs and
the edges in the memory graph. A comparison of points-to relations and alias rela-
tions for all possible assignments in our language has been provided in Figure 4.16.
It is easy to see that points-to information is much more compact than alias informa-
tion. On the ﬂip side, using points-to information would require traversing paths in
the memory graph; alias information explicates these paths in the pointer expressions
used in the alias information.
© 2009 by Taylor & Francis Group, LLC

132
Data Flow Analysis: Theory and Practice
Statement
Memory
Points-to
Aliases
x = &y
x
y
Before
x
y
After
Existing
New
x y
Existing
New Direct
x&y
x = y
x
y
z
Before
x
y
z
After
Existing y z
New
x z
Existing
∗yz
New Direct
xy
New Indirect
∗xz
x = ∗y
x
y
z
u
Before
After
x
y
z
u
Existing y z
z u
New
x u
Existing
∗y z
∗z u
∗∗y u
New Direct
x ∗y
x z
New Indirect
∗x u
∗x = &y
Before
x
y
z
After
x
y
z
Existing x z
New
z y
Existing
∗xz
New Direct
∗x&y
∗zy
∗x = y
Before
x
y
z
u
After
x
y
z
u
Existing x u
y z
New
u z
Existing
∗xu
∗yz
New Direct
∗xy
yu
New Indirect
∗uz
∗∗xz
∗x = ∗y
x
y
z
u
v
Before
After
x
y
z
u
v
Existing
x v
y z
z u
New
v u
Existing
∗xv
∗yz
∗zu
∗∗yu
New Direct
∗x∗y
∗xz
vz
v∗y
New Indirect ∗∗xu
∗vu
FIGURE 4.16
A comparison of points-to and alias relations.
4.3.3
Formulating Data Flow Equations for Alias Analysis
In order to facilitate creation and detection of link aliases, we deﬁne a preﬁx relation
on pointer expressions as follows:
e1 
k e2 ⇔e2 ≡(∗)ke1
where (∗)k denotes k occurrences of the pointer indirection operator ∗. With this
notation x
1 ∗x, x
2 ∗∗x, and ∗x
1 ∗∗x. Observe that &b 
1 b. We also use the &
operator with the following semantics:
&e =
 &x e is a pointer variable x
e1
otherwise, where e1 
1 e
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
133
Given a set of node aliases x, we identify all aliases of a pointer expression e as the
maximum ﬁxed point of the equation:
Aliases(e,x) =
{e1 | e1  e ∈x}
e = &x, x ∈Var
{e1 | e1  e ∈x} ∪{∗e1 | e1 ∈Aliases(&e,x)} otherwise
In the presence of cycles in data structures, Aliases(e,x) could be inﬁnite; this would
require employing suitable summarization mechanism. We shall see one such mech-
anism in the context of heap data analysis.
Now we identify the right and left pointer expressions of a pointer assignment for
computing alias relations. Consider a pointer assignment lhsn = rhsn. The deﬁni-
tions of ConstLeftLn and ConstRightLn given below are easy to follow. DepLeftLn(x)
represents the set of all link aliases of lhsn. They are computed from all link and node
aliases of &lhsn. DepRightLn(x) represents all node and link aliases of rhsn.
ConstLeftLn = {lhsn}
ConstRightLn =

∅
lhsn 
 rhsn
{rhsn} otherwise
DepLeftLn(x) =
∅
lhsn is &x, x ∈Var
{∗e | e ∈Aliases(&lhsn,x)} otherwise
DepRightLn(x) =
∅
lhsn is &x, x ∈Var
Aliases(rhsn,x) otherwise
Observe that we can use only those right pointer expressions that are not modiﬁed
by the assignment. The pointer expressions that are modiﬁed by the assignment are
the pointer expressions that have a preﬁx that is must link aliased to lhsn.
Modn(x1,x2) = {e | e1 
i e, i ≥0, e1  e2 ∈x2, e2 ∈({lhsn}∪DepLeftLn(x1))}
Similar to the inverse dependence of may and must points-to relations for Kill, if x1
is the set of may aliases, then x2 is the set of must aliases and vice-versa.
In the case of points-to analysis, Modn is not required because target of the re-
sulting points-to pair is referred to by a variable name rather than through rhsn.
However, in the case of alias analysis, the pointer expression rhsn is used in the gen-
erated aliases and if the resulting pointer expression is link aliased to lhsn before the
assignment, its target changes due to the assignment. Thus it should not participate
in the generation of new alias pairs.
Now we deﬁne the ﬂow functions for alias analysis. The generated alias pairs are
deﬁned by:
Genn(x1,x2) = ConstGenn ∪DepGennD (x1,x2)∪DepGennI (x1,x2)
where DepGennD (x1,x2) represents the direct aliases and DepGennI (x1,x2) repre-
© 2009 by Taylor & Francis Group, LLC

134
Data Flow Analysis: Theory and Practice
sents indirect aliases and are deﬁned as follows:
ConstGenn = {e1  e2 | e1 ∈ConstLeftLn,e2 ∈ConstRightLn}
DepGennD (x1,x2) = {e1  e2 | e2  Modn(x1,x2), and
(e1 ∈ConstLeftLn,e2 ∈DepRightLn(x1)), or
(e1 ∈DepLeftLn(x1),e2 ∈ConstRightLn), or
(e1 ∈DepLeftLn(x1),e2 ∈DepRightLn(x1))}
DepGennI (x1,x2) = {(∗)ke1  e2 | e2  Modn(x1,x2),(∗)krhsn  e2 ∈x1, k > 0,
e1 ∈(ConstLeftLn ∪DepLeftLn(x1))}
The aliases killed by the assignment are deﬁned by
Killn(x1,x2) = ConstKilln ∪DepKilln(x1,x2)
where
ConstKilln = {e1  e2 | lhsn 
k e1, k ≥0}
DepKilln(x1,x2) = {e1  e2 | e1  e2 ∈x2, e3 
k e1, k ≥0, e3 ∈DepLeftLn(x1)}
The top level data ﬂow equations for alias analysis are identical to that of points-to
analysis; the ﬂow function fn is slightly diﬀerent.
MayInn =

BI
n is Start

p∈pred(n)
MayOut p otherwise
(4.15)
MayOutn = fn(MayInn,MustInn)
(4.16)
MustInn =

BI
n is Start

p∈pred(n)
MustOut p otherwise
(4.17)
MustOutn = fn(MustInn,MayInn)
(4.18)
where ﬂow function fn is deﬁned as follows:
fn(x1,x2) = (x1 −Killn(x1,x2)) ∪Genn(x1,x2)
(4.19)
In the intraprocedural context, BI is ∅because no aliases exist at Start.
Example 4.15
Recall that the program in Figure 4.10 on page 122 results in a cycle in the
data structure because the assignment ∗a = a in node 4 creates the points-to
pair b b in both may and must points-to analysis. This results in an inﬁnite
number of aliases when Aliases(b,x) is computed. Hence we perform may alias
analysis for a simpliﬁed version provided in Figure 4.17 on the facing page.
The initialization and BI for may alias analysis is ∅. For simplicity, we assume
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
135
n1 b = &d; n1
n2 c = b; n2
n3 a = &b; n3n4 a = &c; n4
n5 a = ∗a; n5
FIGURE 4.17
Example program for alias analysis.
that the must alias information is ∅at each program point; this causes fewer
aliases to be killed and hence is a safe approximation for may alias analysis.
Node
Iteration #1
Changes in Iteration #2
Inn
Outn
Inn
Outn
n2 {b  &d}
{b  &d,c  b,c  &d}
{b  &d,c  &d,
c  b,a  c,a  d}
n3 {b  &d,c  b,c  &d}
{b  &d,c  b,c  &d,
a  &b,∗a  c,∗a  &d}
n4 {b  &d,c  b,c  &d}
{b  &d,c  b,c  &d,
a  &c,∗a  c,∗a  &d}
n5
{b  &d,c  b,c  &d,
a  &c,a  &b,∗a  c,
∗a  &d}
{b  &d,c  b,c  &d,
a  c,a  d}
Observe that the pairs ∗a  c and ∗a  &d in Outn3 and Outn4 are indirect
aliases. All other aliases are direct aliases.
Similar to points-to analysis, alias analysis is neither fast nor distributive. Ex-
ample 3.10 in Chapter 3 (Figure 3.10) showed the non-distributivity of may alias
analysis. We leave it for the reader to construct examples to demonstrate the non-
distributivity of must alias analysis and non-fastness of may and must alias analysis.
4.4
Liveness Analysis of Heap Data
The data ﬂow analyses described earlier referred to data objects resident in the stack
or the static area. In this section, we describe an analysis for data objects residing
© 2009 by Taylor & Francis Group, LLC

136
Data Flow Analysis: Theory and Practice
x->succ = y->rptr->lptr
z->lptr = y->rptr
...
p: ...
...
if (u == z->lptr->lptr)
...
x
y
z
m1
m2
m3
m4
m
rptr
lptr
lptr
succ
FIGURE 4.18
An example to motivate liveness analysis. The path consisting of thick edges is
explicitly live at p.
on the heap. An optimization that requires this analysis was described in Chapter 1.
The key idea was to identify heap objects that would not be used in the future, even
if they were reachable. Such objects can be freed and the memory space occupied by
them can be reused. This optimization brings down the overall memory requirement
of the program. If the run time support of the language includes a garbage collector,
then the garbage collector can be expected to collect more garbage per collection.
Further, if the collector is a copying collector, then the collection itself will be faster
since copying collectors process live data only.
To identify the nature of the analysis required for this purpose, consider the ex-
ample shown in Figure 4.18. The declared variables x, y and z are local or global
pointers and accordingly reside in the stack or the static area. We call these root
variables. The objects pointed to by these variables are on the heap. In this analysis
we ignore non-pointer variables. Though our language resembles C, we assume that
the programs being analyzed do not make use of the & (address of) operator. Thus
root variables cannot point to other root variables. We view the heap at a program
point as a directed graph called memory graph.
The root variables form the entry
nodes of the memory graph. Other nodes in the graph correspond to objects on the
heap and edges correspond to pointers. The out-edges of entry nodes are labeled by
root variable names while out-edges of other nodes are labeled by ﬁeld names. The
edges in the memory graph are called links.
Example 4.16
Figure 4.18 shows the memory graph at the program point p. If we can dis-
cover that the links m4 →m and m1 →m are never used in any execution
path starting from p, then we can free the object m at p by inserting the
statements z->lptr->lptr=NULL and x->succ =
NULL. Here, by usage of a
link we mean either dereferencing it to access an object or testing it for com-
parison. In the example shown, the statement z->lptr->lptr=NULL cannot
be inserted because the link m4 →m is subsequently used by the condition
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
137
if (u == z->lptr->rptr) for comparison.
Thus the object m cannot be
freed at p.
In this section, we consider the analysis that discovers whether a link is live i.e.,
whether it will be used in the sense described above.
4.4.1
Access Expressions and Access Paths
A program accesses data through expressions which have l-values. Such expressions
are called access expressions. They can be scalar variables such as x, or may in-
volve an array access such as a[2 ∗i], or can be reference expressions such as ∗x or
y →rptr →lptr. Since we are concerned with analysis of heap-resident data, from
now on we shall limit our attention to reference expressions. These are the expres-
sions that are primarily used to access the heap. In Figure 4.18, the access expression
y →rptr →lptr refers to the heap data denoted as m.
In order to discover liveness and other properties of heap, we need a way of naming
links in the memory graph. We do this using access paths. An access path ρx is a root
variable name followed by a sequence of zero or more ﬁeld names and is denoted
by x
f1
f2
···
fk. Since an access path represents a path in a memory graph,
it can be used for naming links and nodes. An access path consisting of just a root
variable name is called a simple access path; it represents a path from a root variable
to the object pointed to by it. In the context of C, one could think of this as the path
followed to access an object using an access expression such as ∗x. E denotes an
empty access path.
The last ﬁeld name in an access path ρ is called its frontier and is denoted by
frontier(ρ). The frontier of a simple access path is the root variable name. The
access path corresponding to the sequence of names in ρ excluding only its frontier
is called its base and is denoted by base(ρ). The base of a simple access path is
the empty access path E. The object reached by traversing an access path ρ is called
the target of the access path and is denoted by target(ρ). When we use an access
path ρ to refer to a link in a memory graph, it denotes the last link in ρ i.e., the link
corresponding to frontier(ρ).
Example 4.17
Consider the access path ρz = z lptr lptr at program point p. target(ρz) de-
notes the node m and frontier(ρz) denotes the link m4 →m. As we have said
earlier, access paths are also used to denote links in memory graph. The link
denoted by ρz is also m4 →m. base(ρz) is the access path z m3
m4.
In the rest of the section, α will denote an access expression, ρ will denote an ac-
cess path and σ will denote a (possibly empty) sequence of ﬁeld names separated by
. Let the access expression αx be x →f1 →f2 ... →fn. Then, the corresponding
access path ρx is x
f1
f2 ...
fn. When the root variable name is not required, we
drop the subscripts from αx and ρx.
© 2009 by Taylor & Francis Group, LLC

138
Data Flow Analysis: Theory and Practice
We assume that our method does a context insensitive interprocedural analysis.
To simplify the description of analysis we assume that the conditions that alter ﬂow
of control are made up only of simple variables. If not, the oﬀending reference
expression is assigned to a fresh simple variable before the condition and is replaced
by the fresh variable in the condition.
The statements that we handle fall in one of the following categories:
• Function Calls. These are statements x = f(αy,αz,...) where the functions
involve access expressions in arguments. The variable x can be a reference or
a non-reference variable.
• Assignment Statements. These are assignments to references and are denoted
by αx = αy. Only these statements can modify the structure of the heap.
• Use Statements. These statements use heap references to access heap data but
do not modify heap references. For the purpose of analysis, these statements
are abstracted as lists of expressions αy.d where αy is an access expression and
d is a non-reference.
• Return Statement of the type return αx involving reference variable x.
• Other Statements. These statements include all statements which do not re-
fer to the heap. We ignore these statements since they do not inﬂuence heap
reference analysis.
As is customary in static analysis, when we talk about execution paths, we shall
refer to a trace of the program that ignores the evaluation of condition checks. For
simplicity of exposition, we present the analyses assuming that the program to be
analyzed does not create cycles in the heap during execution.
4.4.2
Liveness of Access Paths
A link l is live at a program point p if it is used in some control ﬂow path starting
from p. As noted earlier, l may be used in two diﬀerent ways; it may be dereferenced
to access an object or tested for comparison. Figure 4.18(b) shows links that are live
before program point p by thick arrows. For a link l to be live, there must be at least
one access path from some root variable to l such that every link in this path is live.
This is the path that is actually traversed while using l.
Since the freeing of nodes is through access paths, we need to express the notion
of liveness of links in terms of access paths. An access path is deﬁned to be live at p
if the link corresponding to its frontier is live along some path starting at p.
We limit ourselves to a subset of live access paths, whose liveness can be deter-
mined without taking into account the aliases created before p. These access paths
are live solely because of the execution of the program beyond p. We call access
paths which are live in this sense as explicitly live access paths. An interesting prop-
erty of explicitly live access paths is that they form the minimal set covering every
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
139
live link. In this section, we further restrict ourselves to the computation of explicit
liveness.
Example 4.18
The access paths z, z lptr, z lptr lptr and y rptr lptr are all live at p.
All these paths except y rptr lptr are also explicitly live. The access path
y rptr lptr is live because of the alias created before p. Also note that if an
access path is explicitly live, so are all its preﬁxes.
Example 4.19
We illustrate the issues in determining explicit liveness of access paths by
considering the assignment x.r.n = y.n.n.
• Killed Access Paths. Since the assignment modiﬁes frontier(x r n), any
access path which is live after the assignment and has x r n as preﬁx
will cease to be live before the assignment. Access paths that are live
after the assignment and not killed by it are live before the assignment
also.
• Directly Generated Access Paths. All preﬁxes of x r and y n are explicitly
live before the assignment due to the local eﬀect of the assignment.
• Transferred Access Paths. If x r n σ is live after the assignment, then
y n n σ will be live before the assignment. For example, if x r n n
is live after the assignment, then y n n n will be live before the as-
signment. The sequence of ﬁeld names σ is viewed as being transferred
from x r n to y n n.
We now deﬁne liveness by generalizing the above observations. We use the no-
tation ρx
∗to enumerate all access paths which have ρx as a preﬁx. The summary
liveness information for a set S of access paths is deﬁned as follows:
summary(S ) =

ρ∈S
{ρ ∗}
Further, the set of all global variables is denoted by Globals and the set of formal
parameters of the function being analyzed is denoted by Params.
DEFINITION 4.10
The set of explicitly live access paths at a program
point p, denoted by livenessp is deﬁned as follows.
livenessp =

ψ∈paths(p)
(pathLivenessψ
p)
© 2009 by Taylor & Francis Group, LLC

140
Data Flow Analysis: Theory and Practice
0.
w = x
1.
while (x->data < max)
2.
{
3.
x = x->rptr
4.
}
5.
y = x->lptr
6.
z = malloc(...)
7.
y = y->lptr
8.
z->sum = x->lptr->data
+ y->data
Heap
Stack
z
x
w
ma
y
mk
mb
rptr
mc
rptr
md
rptr
me
lptr
mf
lptr
mg
lptr
mh
lptr
mi
lptr
mj
lptr
ml
lptr
mm
lptr
FIGURE 4.19
An example program and possible memory graphs before line 6. Depending on
whether the while loop is iterated 0, 1, 2, or 3 times, x will point to ma, mb, mc, or
md. Accordingly y will point to mi, mf , mg, or me.
where, ψ ∈paths(p) is a control ﬂow path p to Start and pathLivenessψ
p denotes
the liveness at p along ψ and is deﬁned as follows. If p is not program exit
then let the statement which follows it be denoted by s and the program point
immediately following s be denoted by p. Then,
pathLivenessψ
p =

∅
p = Exit(main)
summary(Globals)
p = Exit(f), f  main
statementLivenesss(pathLivenessψ
p) otherwise
where the ﬂow function for s is deﬁned as follows:
statementLivenesss(X) = (X −LKills) ∪LDirect s ∪LTransfer s(X)
LKills denotes the sets of access paths which cease to be live before statement
s, LDirect s denotes the set of access paths which become live due to local eﬀect
of s and LTransfer s(X) denotes the set of access paths which become live before
s due to transfer of liveness from live access paths after s. They are deﬁned
in Figure 4.20.
Observe that the deﬁnitions of LKills, LDirect s, and LTransfer s ensure that the
livenessp is preﬁx-closed.
When we view the above deﬁnition in terms of the constant and dependent parts
of ﬂow functions as deﬁned in Section 4.1, it is clear that LKills represents DepKills
and ConstKills is ∅. Liveness information is generated by LDirect s which represents
ConstGens and LTransfer s which represents DepGens.
Example 4.20
In Figure 4.19, it cannot be statically determined which link is represented by
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
141
Statement s LKills
LDirect s
LTransfer s(X)
αx = αy
{ρx
∗} preﬁxes(base(ρx)) ∪
ρy
σ | ρx
σ ∈X}
preﬁxes(base(ρy))
αx = f(αy) {ρx
∗} preﬁxes(base(ρx)) ∪
∅
preﬁxes(base(ρy)) ∪
summary({ρy}∪Globals)
αx = new
{ρx
∗} preﬁxes(base(ρx))
∅
αx = null
{ρx
∗} preﬁxes(base(ρx))
∅
Use αy.d
∅
preﬁxes(ρy)
∅
return αy
∅
preﬁxes(base(ρy)) ∪
∅
summary({ρy})
other
∅
∅
∅
FIGURE 4.20
Deﬁning ﬂow functions for liveness. Globals denotes the set of global references
and Params denotes the set of formal parameters. For simplicity, we have shown a
single access expression on the RHS.
access expression x.lptr at line 5. Depending upon the number of iterations of
the while loop, it may be any of the links represented by thick arrows. Thus at
line 0, we have to assume that all access paths {x lptr lptr, x rptr lptr lptr,
x rptr rptr lptr lptr, . . . } are explicitly live.
4.4.3
Representing Sets of Access Paths by Access Graphs
In the presence of loops, the set of access paths may be inﬁnite and the lengths of
access paths may be unbounded. If the algorithm for analysis tries to compute sets
of access paths explicitly, termination cannot be guaranteed. We solve this problem
by representing a set of access paths by a graph of bounded size. The structure that
we use for the representation is called an access graph.
An access graph, denoted by Gv, is a directed graph n0,N,E representing a set
of access paths starting from a root variable v.† N is the set of nodes, n0 ∈NF is
the entry node with no in-edges and E is the set of edges. Every path in the graph
represents an access path. The empty graph EG has no nodes or edges and does not
accept any access path.
The entry node of an access graph is labeled with the name of the root variable
while the non-entry nodes are labeled with a unique label created as follows: If a ﬁeld
name f is referenced in basic block b, we create an access graph node with a label
f,b,i where i is the instance number used for distinguishing multiple occurrences
of the ﬁeld name f in block b. Note that this implies that the nodes with the same
†Where the root variable name is not required, we drop the subscript v from Gv.
© 2009 by Taylor & Francis Group, LLC

142
Data Flow Analysis: Theory and Practice
1 x = x.r 1
2 x = x.r 2
Live access paths at entry of block 1: {x, x r, x r r}
Corresponding access graph: G2
x
x
r1
r2
1 x = x.r 1
Live access paths at entry of block 1:
{x, x r, x r r, x r r r, ...}
Corresponding access graph: G1
x
x
r1
FIGURE 4.21
Approximations in access graphs.
label are treated as being identical. Often, i is 0 and in such a case we denote the
label f,b,0 by fb for brevity.
A node in the access graph represents one or more links in the memory graph.
Additionally, during analysis, it represents a state of access graph construction (ex-
plained in Section 4.4.3). An edge fn →gm in an access graph at program point p
indicates that a link corresponding to ﬁeld f dereferenced in block n may be used
to dereference a link corresponding to ﬁeld g in block m on some path starting at p.
This has been used in Section 4.4.4 to argue that the size of access graphs in practical
programs is small.
Pictorially, the entry node of an access graph is indicated by an incoming double
arrow.
Summarization
Recall that a link is live at a program point p if it is used along some control ﬂow
path from p to Start. Since diﬀerent access paths may be live along diﬀerent control
ﬂow paths and there may be inﬁnitely many control ﬂow paths in the case of a loop
following p, there may be inﬁnitely many access paths which are live at p. Hence, the
lengths of access paths will be unbounded. In such a case summarization is required.
Summarization is achieved by merging appropriate nodes in access graphs, re-
taining all in and out edges of merged nodes. We explain merging with the help of
Figure 4.21:
• Node r1 in access graph G1
x indicates references of n at diﬀerent execution
instances of the same program point. Every time this program point is visited
during analysis, the same state is reached in that the pattern of references after
r1 is repeated. Thus all occurrences of r1 are merged into a single state. This
creates a cycle which captures the repeating pattern of references.
• In G2
x, nodes r1 and r2 indicate referencing n at diﬀerent program points. Since
the references made after these program points may be diﬀerent, r1 and r2 are
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
143
not merged.
Summarization captures the pattern of heap traversal in the most straightforward
way. Traversing a path in the heap requires the presence of reference assignments
αx = αy such that ρx is a proper preﬁx of ρy. Assignments in Figure 4.21 are examples
of such assignments. The structure of the ﬂow of control between such assignments
in a program determines the pattern of heap traversal. Summarization captures this
pattern without the need of control ﬂow analysis and the resulting structure is re-
ﬂected in the access graphs as can be seen in Figure 4.21. More examples of the
resemblance of program structure and access graph structure can be seen in the ac-
cess graphs in Figure 4.24.
Operations on Access Graphs
Section 4.4.2 deﬁned liveness by applying certain operations on access paths. In
this subsection we deﬁne the corresponding operations on access graphs. Unless
speciﬁed otherwise, the binary operations are applied only to access graphs having
same root variable. The auxiliary operations and associated notations are:
• root(ρ) denotes the root variable of access path ρ, while root(G) denotes the
root variable of access graph G.
• ﬁeld(n) for a node n denotes the ﬁeld name component of the label of n.
• makeGraph(ρ) constructs access graphs correspondingto ρ. It uses the current
basic block number and the ﬁeld names to create appropriate labels for nodes.
The instance number depends on the number of occurrences of a ﬁeld name in
the block. makeGraph(ρ ∗) creates an access graph for ρ and connects the
ﬁnal node of the access graph to a special node n called summary node. In
addition, there is a self loop over n. Both the new edges are assumed to have
all ﬁeld names as labels.
• lastNode(G) returns the last node of a linear graph G constructed from a given
access path ρ.
• cleanUp(G) deletes the nodes which are not reachable from the entry node.
• CN(G,G,S ) computes the set of nodes of G which correspond to the nodes of
G speciﬁed in the set S . To compute CN(G,G,S ), we deﬁne ACN(G,G), the
set of pairs of all corresponding nodes. Let G ≡n0,N,E and G ≡n
0,N,E.
A node n in access graph G corresponds to a node n in access graph G if there
exists an access path ρ which is represented by a path from n0 to n in G and a
path from n
0 to n in G.
© 2009 by Taylor & Francis Group, LLC

144
Data Flow Analysis: Theory and Practice
Program
Access Graphs
Remainder
Graphs
1 x = x.l 1
2 y = x.r.d 2
g1
x
g2
x
r2
g3
x
l1
g4
x
l1
r2
g5
x
l1
r2
g6
x
l1
r2
rg1
r2
rg2
l1
r2
Union
Path Removal
Factorization
Extension
g3 1 g4 = g4
g2 1 g4 = g5
g5 1 g4 = g5
g5 1 g6 = g6
g6 2 x l = g2
g5 2 x = EG
g4 2 x r = g4
g4 2 x l = g1
g2/(g1,{x}) = {rg1}
g5/(g1,{x}) = {rg1,rg2}
g5/(g2,{r2}) = {RG}
g4/(g2,{r2}) = ∅
(g3,{l1})#{rg1} = g4
(g3,{x,l1})#{rg1,rg2} = g6
(g2,{r2})#{RG} = g2
(g2,{r2})#∅= EG
FIGURE 4.22
Examples of operations on access graphs.
Formally, ACN(G,G) is the least solution of the following equation:
ACN(G,G) =

∅
root(G)  root(G)
{n0,n
0}∪{n j,n
j |
otherwise
ﬁeld(n j) = ﬁeld(n
j),
ni →n j ∈E,n
i →n
j ∈E,
ni,n
i ∈ACN(G,G)}
CN(G,G,S ) = {n | n,n ∈ACN(G,G), n ∈S }
Note that ﬁeld(n j) = ﬁeld(n
j) would hold even when n j or n
j is the summary node
n.
Let G ≡n0,N,E and G ≡n0,N,E be access graphs (having the same entry
node). G and G are equal if N = N and E = E.
The main operations of interest are deﬁned below and are illustrated in Figure 4.22.
1. Union ( 1 ). G 1 G combines access graphs G and G such that any access
path contained in G or G is contained in the resulting graph.
G 1 G = n0,N ∪N,E ∪E
The operation N ∪N treats the nodes with the same label as identical. Because
of associativity, 1 can be generalized to arbitrary number of arguments in an
obvious manner.
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
145
2. Path Removal (2). The operation G 2ρ removes those access paths in G which
have ρ as a preﬁx.
G 2ρ =

G
ρ = E or root(ρ)  root(G)
EG
ρ is a simple access path
cleanUp(n0,N,E −Edel)
otherwise
where
Edel = {ni →n j | ni →n j ∈E,ni ∈CN(G,GB,{lastNode(GB)}),
{ni →f n j |ﬁeld(n j) = frontier(ρ),GB = makeGraph(base(ρ)),
{ni →f n j |uniqueAccessPath?(G,ni)}
uniqueAccessPath?(G, n) returns true if in G, all paths from the entry node to
node n represent the same access path. Note that path removal is conservative
in that some paths having ρ as preﬁx may not be removed. Since an access
graph edge may be contained in more than one access path, we have to ensure
that access paths which do not have ρ as preﬁx are not erroneously deleted.
3. Factorization (/). Recall that the LTransfer term in Deﬁnition 4.10 requires
extracting suﬃxes of access paths and attaching them to some other access
paths. The corresponding operations on access graphs are performed using
factorization and extension. Given a node m ∈(N −{n0}) of an access graph
G, the Remainder Graph of G at m is the subgraph of G rooted at m and is
denoted by RG(G, m). If m does not have any outgoing edges, then the result
is the empty remainder graph RG. Let M be a subset of the nodes of G and
M be the set of corresponding nodes in G. Then, G/(G, M) computes the set
of remainder graphs of the successors of nodes in M.
G/(G, M) = {RG(G, n j) | ni →n j ∈E,ni ∈CN(G,G, M)}
(4.20)
A remainder graph is similar to an access graph except that (a) its entry node
does not correspond to a root variable but to a ﬁeld name and (b) the entry
node can have incoming edges.
4. Extension. Extending an empty access graph EG results in the empty access
graph EG. For non-empty graphs, this operation is deﬁned as follows.
(a) Extension with a remainder graph (·). Let M be a subset of the nodes of
G and R ≡n, NR, ER be a remainder graph. Then, (G, M)·R appends
the suﬃxes in R to the access paths ending on nodes in M.
(G, M)·RG = G
(G, M)·R =

n0,N ∪NR,E ∪ER ∪{ni →n | ni ∈M}

(4.21)
© 2009 by Taylor & Francis Group, LLC

146
Data Flow Analysis: Theory and Practice
Operation
Access Graphs
Access Paths
Union
G3 = G1 1 G2
P(G3, M3) ⊇P(G1, M1)∪P(G2, M2)
Path Removal G2 = G1 2 ρ
P(G2, M2) ⊇P(G1, M1) −
{ρ σ | ρ σ ∈P(G1, M1)}
Factorization
S = G1/(G2, M) P(S, Ms) =
{σ | ρ
σ ∈P(G1, M1), ρ ∈P(G2, M)}
Extension
G2 = (G1, M)#S P(G2, M2) ⊇P(G1, M1) ∪
{ρ σ | ρ ∈P(G1, M), σ ∈P(S, Ms)}
FIGURE 4.23
Safety of access graph operations. P(G, M) is the set of paths in graph G terminating
on nodes in M. For graph Gi, Mi is the set of all nodes in Gi. S is the set of remainder
graphs and P(S, Ms) is the set of all paths in all remainder graphs in S .
(b) Extension with a set of remainder graphs (#). Let S be a set of remainder
graphs. Then, G#S extends access graph G with every graph in S .
(G, M)#∅= EG
(G, M)#S =

R∈S
(G, M)·R
(4.22)
Safety of Access Graph Operations
Since access graphs are not exact representations of sets of access paths, the safety of
approximations needs to be deﬁned explicitly. The constraints deﬁned in Figure 4.23
capture safety in the context of liveness in the following sense: Every access path
which can possibly be live should be retained by each operation. Since the comple-
ment of liveness is used to free heap data by nullifying links, this ensures that no live
access path is considered for nulliﬁcation.
4.4.4
Data Flow Analysis for Explicit Liveness
For a given root variable v, ELInv(i) and ELOutv(i) denote the access graphs repre-
senting explicitly live access paths at the entry and exit of basic block i. We use EG
as the initial value for ELInv(i)/ELOutv(i).
ELInv(i) = (ELOutv(i)2 ELKillPathv(i)) 1 ELGenv(i)
(4.23)
ELOutv(i) =

makeGraph(v ∗) i = Start, v ∈Globals
EG
i = Start, v  Globals

s∈succ(i)
ELInv(s)
otherwise
(4.24)
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
147
where
ELGenv(i) = LDirectv(i) 1 LTransferv(i)
The term LDirectv(i) represents the ConstGeni component for variable v whereas
LTransferv(i) represents the DepGeni component for v.
Liveness information is
killed using path removal which is implemented by deleting an edge in an access
graph. In our case, this edge is frontier(ρx) where ρx denotes the access path rep-
resenting the access expression appearing on the left hand side of an assignment.
Hence ELKillPathv(i) represents ConstGeni. This is unlike LKills (Deﬁnition 4.10 on
page 139) which represents DepKills rather than ConstKills. This is because LKills is
not a ﬁxed set but depends on the liveness information that holds after statement s.
The deﬁnitions of ELKillPathv(i), LDirectv(i), and LTransferv(i) depend on state-
ment i as follows:
1. Assignment statement αx = αy. Apart from deﬁning the desired terms for x
and y, we also need to deﬁne them for any other variable z. In the follow-
ing equations, Gx and Gy denote makeGraph(ρx) and makeGraph(ρy) re-
spectively, whereas Mx denotes lastNode(makeGraph(ρx)) and My denotes
lastNode(makeGraph(ρy)).
LDirectx(i)=makeGraph(base(ρx))
LDirecty(i)=
EG
αy is New ... or null
makeGraph(base(ρy)) otherwise
LDirectz(i)=EG,for any variable z other than x and y
LTransfery(i)=

EG
αy is New or null
(Gy, My)#
otherwise
(ELOutx(i)/(Gx, Mx))
(4.25)
LTransferz(i)=EG, for any variable z other than y
ELKillPathx(i)=ρx
ELKillPathz(i)=E, for any variable z other than x
As stated earlier, the path removal operation deletes an edge only if it is con-
tained in a unique path. Thus fewer paths may be killed than desired. This is
a safe approximation. Another approximation which is also safe is that only
the paths rooted at x are killed. Since assignment to αx changes the link repre-
sented by frontier(ρx), for precision, any path which is guaranteed to contain
the link represented by frontier(ρx) should also be killed. Such paths can be
discovered through must-alias analysis.
2. Function call αx = f(αy). We conservatively assume that a function call may
make any access path rooted at y or any global reference variable live. Thus
© 2009 by Taylor & Francis Group, LLC

148
Data Flow Analysis: Theory and Practice
Statement i
ELOut(i)
ELIn(i)
7
x
l7
y
z
6
x
l7
y
z
x
l7
y
l6
z
5
x
l7
y
l6
z
x
l7
y
l6
4
x
l7
y
l6
x
l7
l4
l6
3
x
r3
l7
l4
l6
x
r3
l7
l4
l6
2
x
r3
l7
l4
l6
x
r3
l7
l4
l6
1
x
r3
l7
l4
l6
x
r3
l7
l4
l6
FIGURE 4.24
Explicit liveness for the program in Figure 4.19 on page 140 under the assumption
that all variables are local variables.
this version of our analysis is context insensitive.
LDirectx(i)=makeGraph(base(ρx))
LDirecty(i)=makeGraph(base(ρy)) 1 makeGraph(ρy
∗)
LDirectz(i)=
makeGraph(z ∗) if z is a global variable
EG
otherwise
LTransferz(i)=EG, for all variables z
ELKillPathx(i)=ρx
ELKillPathz(i)=E, for any variable z other than x
3. Return Statement return αx.
LDirectx(i)=preﬁxes(base(ρx))∪makeGraph(ρx
∗)
LDirectz(i)=
makeGraph(z ∗) if z is a global variable
EG
otherwise
LTransferz(i)=EG, for any variable z
ELKillPathz(i)=E, for any variable z
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
149
4. Use Statements
LDirectx(i) =

makeGraph(ρx) for every αx.d used in i
LDirectz(i) = EG for any variable z other than x
LTransferz(i) = EG, for every variable z
ELKillPathz(i) = E, for every variable z
Example 4.21
Figure 4.24 lists explicit liveness information at diﬀerent points of the program
in Figure 4.19 on page 140 under the assumption that all variables are local
variables.
Observe that computing liveness using Equations (4.23) and (4.24) results in an
MFP solution of data ﬂow analysis whereas Deﬁnition 4.10 speciﬁes an MOP so-
lution of data ﬂow analysis. Since the ﬂow functions are non-distributive, the two
solutions may be diﬀerent.
Convergence of Explicit Liveness Analysis
We now show the termination of explicit liveness analysis using the properties of
access graph operations. In particular, we show that the ﬂow functions are monotonic
and the data ﬂow values form a ﬁnite complete lattice.
For a program there are a ﬁnite number of basic blocks, a ﬁnite number of ﬁelds
for any root variable, and a ﬁnite number of ﬁeld names in any access expression.
Hence the number of access graphs for a program is ﬁnite. Further, the number of
nodes and hence the size of each access graph, is bounded by the number of labels
which can be created for a program.
Access graphs for a variable x form a complete lattice with a partial order G
induced by 1 . Note that 1 is commutative, idempotent, and associative. Let
G = x,NF,NI,E and G = x,N
F,N
I,E where subscripts F and I distinguish be-
tween the ﬁnal and intermediate nodes. The partial order G is deﬁned as
G G G ⇔

N
F ⊆NF

∧

N
I ⊆(NF ∪NI)

∧E ⊆E
Clearly, G G G implies that G contains all access paths of G. We extend G to a
set of access graphs as follows:
S 1 S S 2 ⇔∀G2 ∈S 2,∃G1 ∈S 1 s.t. G1 G G2
It is easy to verify that G is reﬂexive, transitive, and antisymmetric. For a given
variable x, the access graph EG forms the  element of the lattice while the ⊥element
is a greatest lower bound of all access graphs.
The partial order over access graphs and their sets can be carried over unaltered
to remainder graphs (RG) and their sets (RS), with the added condition that RG is
incomparable to any other non empty remainder graph.
© 2009 by Taylor & Francis Group, LLC

150
Data Flow Analysis: Theory and Practice
Operation
Monotonicity
Union
G1 G G
1 ∧G2 G G
2
⇒G1 1 G2 G G
1 1 G
2
Path Removal
G1 G G2
⇒G1 2ρ G G2 2ρ
Factorization
G1 G G2
⇒G1/(G, M) RS G2/(G, M)
Extension
RS1 RS RS2 ∧G1 G G2 ∧M1 ⊆M2
⇒(G1, M1)#RS1 G (G2, M2)#RS2
Link-Alias Closure
G1 G G
1 ∧G2 G G
2
⇒LnG(G1,G2,gx,gy) S LnG(G
1,G
2,gx,gy)
FIGURE 4.25
Monotonicity of access graph operations.
Access graph operations are monotonic as described in Figure 4.25. Path removal
is monotonic in the ﬁrst argument but not in the second argument. Similarly fac-
torization is monotonic in the ﬁrst argument but not in the second and the third ar-
gument. However, we show that in each context where they are used, the resulting
functions are monotonic:
1. Path removal is used only for an assignment αx = αy. It is used in liveness
analysis and its second argument is ρx which is constant for any assignment
statement αx = αy. Thus the resulting ﬂow functions are monotonic.
2. Factorization is used during liveness analysis. It is used for the ﬂow function
corresponding to an assignment αx = αy. In this context, its second and third
arguments are makeGraph(ρx) and lastNode(makeGraph(ρx)). Both these
are constants for a given assignment statement αx = αy. Thus, the resulting
ﬂow functions are monotonic.
Thus we conclude that all ﬂow functions are monotonic. Since lattices are ﬁnite,
termination of explicit liveness analysis follows.
Eﬃciency of Explicit Liveness Analysis
This section discusses the issues which inﬂuence the eﬃciency of performing explicit
liveness analysis.
The data ﬂow frameworks deﬁned in this paper are not separable [59] because the
data ﬂow information of a variable depends on the data ﬂow information of other
variables. Thus the number of iterations over control ﬂow graph is not bounded
by the depth of the graph [3, 44, 59] but would also depend on the number of root
variables that depend on each other.
The amount of work done in each iteration is not ﬁxed but depends on the size
of access graphs. Of all operations performed in an iteration, only CFN(G,G) is
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
151
costly. In practice, the access graphs are quite small because of the following reason:
Recall that edges in access graphs capture dependence of a reference made at one
program point on some other reference made at another point (Section 4.4.3). In
real programs, traversals involving long dependences are performed using iterative
constructs in the program. In such situations, the length of the chain of dependences
is limited by the process of summarization because summarization treats nodes with
the same label as being identical. Thus, in real programs chains of such dependences,
and hence the access graphs, are quite small in size. Hence the complexities of access
graph operations is not a matter of concern.
4.4.5
The Motivating Example Revisited
For our motivating example in Section 1.1, we had performed liveness analysis of
heap data intuitively. The liveness information was represented using access paths
which were summarized by combining all ﬁeld names beyond the second ﬁeld by a
summary ﬁeld “”. We now present the result of liveness analysis of the program in
Figure 1.1 on page 2 in terms of access graphs.
Intraprocedural Analysis by Ignoring the Interprocedural Eﬀects
In this case we treat a function call as a statement that reads its actual parameters and
assume that BI is EG.
Node
Outn
Inn
n6
EG
n
n5
n
next
sib
n
next
sib
n4
n
next
sib
n
succ
sib
n3
n
succ
sib
n
succ
sib
n2
n
succ
sib
n
succ
sib
n1
n
succ
sib
n
child
sib
When we compare these results with the corresponding liveness information com-
puted in Section 1.1.2, we observe that the above access graphs do not include access
paths such as succ
child
sib or succ
sib
child whereas they are included in the
liveness information computed in Section 1.1.2. This diﬀerence arises because of
the diﬀerence between the summarization of access paths using access graphs and
the summarization by restricting the lengths of access paths.
Intraprocedural Analysis with Conservative Interprocedural Approximation
As described earlier, the eﬀect of the function call in our example can be incorporated
conservatively by assuming that every access path rooted at n is live at the exit of
© 2009 by Taylor & Francis Group, LLC

152
Data Flow Analysis: Theory and Practice
dfTraverse and that every access path rooted at succ is live at the entry of n3
due to the call. We use the special summary node n deﬁned for access graph to
denote any ﬁeld name. Thus we assume that the function call creates the access
graph
succ
n
and BI is
n
n
. With these assumptions, the data
ﬂow information after ﬁrst iteration is:
Node
Iteration #1
Outn
Inn
n6
n
n
n
n
n5
EG
EG
n4
EG
EG
n3
EG
succ
n
n2
n
n
succ
n
n
n
succ
n
n1
n
n
succ
n
n
n
If there is an edge n →n, then n cannot have any other out edge because all its
successors are consumed by n. The data ﬂow values after second iteration are:
Node
Changes in Iteration #2
Outn
Inn
n6
n5
n
n
succ
n
n
n
next
n
n4
n
n
next
n
n
n
succ
sib
n
n3
n
n
succ
sib
n
n2
n1
There are no further changes. Observe that the values of Inn4 and Outn3 are more
precise than those in Section 1.1.2. This is because unlike the earlier summarization,
access graphs do not restrict the length of access paths to two.
Interprocedural analysis of this example is presented in Section 9.5.
4.5
Modeling Entity Dependence
Recall that a component function f α : L '→L computes the data ﬂow value of entity
α. The domain of f α is not atomic reﬂecting the fact that the data ﬂow value of α
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
153
x α, x β, ..., x ω 
f
x α, x β, ..., x ω 
x α, x β, ..., x ω 
f β
x α,x β, ..., x ω 
x α, x β, ..., x ω 
f
α→β
x α,x β, ..., x ω 
(a) Overall function
(b) Component function
(c) pef
FIGURE 4.26
Deﬁning overall ﬂow function in terms of component and primitive entity functions.
could depend on the data ﬂow values of other entities also. Thus even f α need not
be atomic. For some frameworks, it can be deﬁned in terms of simpler functions that
use the value of an entity to compute the value of another entity.
4.5.1
Primitive Entity Functions
We deﬁne primitive entity functions (abbreviated as pef) as the functions that com-
pute the data ﬂow value of an entity α from the data ﬂow value of some entity β. We
denote such a pef by f
β→α: Lβ '→Lα. The component function f α
u→v is deﬁned as:
f α(x α) =
β ∈Σf
β→αx β
(4.26)
Figure 4.26 illustrates how an overall ﬂow function f can be a deﬁned in terms of
component functions f β, and a pefs f
α→β. x = x α, x β, ..., x ω  is the input data
ﬂow value and x = x α, x β, ..., x ω  is the output data ﬂow value.
Modeling component functions in terms of pefs is interesting because it allows
the component functions to be deﬁned in terms of a very small set of pefs. We
explain this by distinguishing between general unspeciﬁed functions and speciﬁc
known functions. Our notation f denotes a general unspeciﬁed function. When
we wish to denote speciﬁc known functions computing speciﬁc values, we use the
notation φ. Unlike the subscript of f which denotes a program point or an edge, the
subscript of φ distinguishes it from other speciﬁc functions. A couple of common
special functions are:
∀x ∈L : φid(x) = x
∀x ∈L : φz(x) = z
There are two special values of φz that are used very frequently: They are φ and
φ⊥. The speciﬁc functions that can be used for component functions and pefs are
© 2009 by Taylor & Francis Group, LLC

154
Data Flow Analysis: Theory and Practice
denoted by φ that are deﬁned as follows:
∀x ∈L : φz(x) = z
∀x ∈L : φid(x) = x
∀x ∈L,∀m,n ∈Const : φm,n(x) = m×x+n
φz are constant functions. They include φ and φ⊥also. Some other examples of
constant functions are: pefs corresponding to constant value assignments such as
a = 2 in constant propagation, pefs corresponding to constant address assignments
such as a = &b in point-to analysis etc. The latter is possible because the address of
each named variable is a compile time constant.‡
φid is an identity pef. Note that the domain of φid could be L α and the range
could be L β. Yet, it is an identity function because the component lattices L α and
L β are identical in terms of values and structure. In separable frameworks, for every
identity pef, α = β. Examples of φid with α  β are functions corresponding to copy
statements such as a = b in non-separable frameworks like possibly uninitialized
variable analysis, constant propagation or points-to analysis.
Together, φz and φid cover all bit vector frameworks, all fast frameworks, all non-
separable frameworks in which the data ﬂow values can be represented by bit vec-
tors (e.g., faint variables analysis, possibly uninitialized variables analysis), and copy
constant propagation. They also cover a restricted points-to analysis if the right hand
side does not involve indirection. The last pef φm,n is included to cover linear con-
stant propagation. It is easy to see that all these pefs are distributive and are closed
under composition. The frameworks whose component functions can be deﬁned us-
ing the above pefs are called primary frameworks.
If an entity β does not inﬂuence α, then f
β→α = φ . A separable framework is a
special case of non-separable frameworks such that
α  β ⇒∀x β ∈L β, f
β→α 
x β
=  
This reduces f α from L '→Lα to L α '→L α.
Example 4.22
Given an assignment a = b∗c, some examples of component functions for some
data ﬂow frameworks are as follows:
• Available expressions analysis:
f b∗c = φ ; for any expression e that
involves a, f e = φ⊥; and for an expression e that does not involve a,
f e = φid.
‡As is customary, addresses deﬁned in terms of ﬁxed oﬀsets from frame pointers in activations records
are considered compile time constants even if the actual address depends on run time.
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
155
• Live variables analysis: f a = φ⊥; f b = f c = φ ; and for any variable x
other than a, b, and c, f x = φid.
• Faint variables analysis f a = φ⊥; f b = φ
b
id φ
a
id; f c = φ
c
id φ
a
id; and for
any variable x other than a, b, and c, f x = φid.
For an assignment a = 2 in constant propagation, f a = φ2; for an assignment
a = b, f a = φ
b
id; and for an assignment a = b+2, f a = φ
b
1,2. For any variable x
other than a, f x = φid.
Example 4.23
Consider the ﬂow functions in explicit liveness analysis of heap data.
An
access graph consists of edges between nodes. Since the set of nodes that may
occur in any access graph is ﬁxed for an instance of explicit liveness analysis,
the set of possible edges is also ﬁxed. Thus we deﬁne the following pefs for an
edge: φ⊥adds an edge to the given graph, φ removes an edge from the given
graph whereas φid copies an edge. Thus the ﬂow functions of liveness analysis
deﬁned in Section 4.4.4 can be formulated in terms of these three pefs.
4.5.2
Composite Entity Functions
The component functions of full constant propagation and points-to analysis cannot
be deﬁned in terms of pefs. Such frameworks are not primary frameworks.
Flow functions in full constant propagation evaluate an arithmetic expression and
if we wish to deﬁne component functions in terms of simpler functions, we will have
to use the functions of the form L ×L '→L. Such functions are neither distributive
nor closed under composition. In points-to analysis a right hand side could involve an
indirection like ∗x. In such a situation computing right locations requires collecting
points-to information of all z that x could point to. Contrast this with the right hand
side x; in this case, the right locations consist of only the points-to information of x.
Thus the required function has the form L '→L.
The component functions that cannot be deﬁned in terms of primitive entity func-
tions are deﬁned in terms of composite entity functions (abbreviated as cef) where
cefs themselves are deﬁned as combinations of pefs. For example, addition of two
variables in full constant propagation is represented by the composite entity function
φ
α, β
+
: L ×L '→L deﬁned below:
φ
α, β
+
= φ
α
id +φ
β
id
Indirection in points-to analysis is deﬁned in terms of composite entity function
φ
α
∗: L '→L deﬁned below:
φ
α
∗=
α
β
φ
β
id
© 2009 by Taylor & Francis Group, LLC

156
Data Flow Analysis: Theory and Practice
Example 4.24
For an assignment x = y+z in constant propagation, f x = φ
y,z
+
and for every
variable w other than x, f w = φid.
For an assignment x = ∗y in points-to
analysis, f x = φ
y
∗. Observe that modeling assignment ∗x = y does not require
a special function because we deﬁne f z = φid for every z such that x z.
4.6
Summary and Concluding Remarks
In this chapter we have extended the Gen-Kill model of bit vector frameworks to
general frameworks. The largest class of practical problems that can be described
using this extended model are non-separable frameworks. In principle, separable
frameworks can also have dependent parts and this model captures such frameworks
also. However, the focus of this chapter has been on non-separable frameworks
because we are not aware of a practical separable framework that is not a bit vector
framework.
The extended Gen-Kill model can be seen as a uniform speciﬁcation model for
semantics captured by an analysis. This is useful because it allows ﬂow functions
to be decomposed in similar parts so that ﬂow functions of diﬀerent frameworks
can be compared and contrasted. This facilitates modeling ﬂow functions at a ﬁner
granularity in terms of primitive and composite entity functions. Surprisingly, a very
small set of pefs is suﬃcient to model ﬂow functions in most frameworks despite
the diversity of the data ﬂow information. As shown in Section 4.5, four pefs are
enough to model almost all frameworks except full constant propagation and points-
to analysis in which addresses of pointers are taken. This should be contrasted with
the conventional modeling where ﬂow functions remain at a much higher level of
abstraction f : L '→L and no attempt is made to examine their constituents. Two
signiﬁcant beneﬁts of modeling ﬂow functions in terms of pefs are that
• it becomes possible to devise tight complexity bounds for round-robin iterative
analysis of a large class of data ﬂow frameworks. We do so in Chapter 5.
• it becomes possible to devise feasibility conditions for systematic reduction of
ﬂow function compositions.
4.7
Bibliographic Notes
The term separability was coined by Khedker and Dhamdhere [60]. Separable frame-
works were called “decomposable” by Sharir and Pnueli [93] whereas Rosen [84]
had called them “factorizable”.
© 2009 by Taylor & Francis Group, LLC

General Data Flow Frameworks
157
Constant propagation was described by Kildall [63] and it has been widely stud-
ied in literature. Some important works include conditional constant propagation by
Wegman and Zadeck [102] and complexity study of many variants of constant prop-
agation by M¨uller-Olm and R¨uthing [78]. Strongly live variables analysis, which is a
dual of faint variables analysis can be found in the text by F. Nielson, H. R. Nielson
and Hankin [80].
There is a plethora of literature on pointer analysis. Unlike our presentation which
tries to present a clean model of pointer analysis independently of other concerns,
most of the works on pointer analysis have almost always given a much higher pref-
erence to practical concerns such as eﬃciency and eﬀectiveness in interprocedural
settings. Thus many combinations of ﬂow sensitivity and context sensitivity have
been explored. Even among ﬂow insensitive approaches, two separate categories of
equality-based and subset-based methods have been studied. Equality-based method
assumes that if a can point to b and c, then b can point to everything that c can point
to and vice-versa. Subset-based does not unify the points-to sets of b and c. Equality-
based approach was pioneered by Steensgaard [97] whereas subset-based approach
was pioneered by Andersen [9]. Fahndrich, Foster, Su and Aiken [35] presented
an Andersen style of context insensitive pointer analysis which was followed up by
Steensgaard style of context sensitive pointer analysis [36]. Andersen style context
sensitive pointer analysis was reported by Whaley and Lam [104]. Among other in-
ﬂuential works on pointer analysis, Landi and Ryder [66, 67] have presented ﬂow
sensitive pointer analysis which is also context sensitive in non-recursive parts of
programs. The work done by Choi, Burke and Carini [21, 48] belongs to the same
category. The only pointer analysis that is ﬂow sensitive and also context sensi-
tive for recursive programs is by Emami, Ghiya and Hendren [34]. Our version of
points-to analysis is based on its reformulation by Kanade, Khedker and Sanyal [51].
An excellent discussion of the state of art of pointer analysis has been presented by
Hind [47].
Liveness analysis of heap data using access graphs is a recent work by Khed-
ker, Sanyal and A. Karkare [62]. We have only presented explicit liveness analysis.
Actual nulliﬁcation requires some other analyses such as alias analysis, availability
analysis, anticipability analysis, and nullability analysis [62].
The earlier attempt at discovering the liveness of heap was by Agesen, Detlefs and
Moss [1] and was restricted to the liveness of root variables. Our approach of heap
data analysis can be seen as some kind of shape analysis [88, 106] which is a general
method of creating suitable abstractions (called Shape Graphs) of heap memory with
respect to the relevant properties. Program execution is then modeled as operations
on shape graphs. However, it is not clear how shape analysis can be directly used
for discovering future properties like liveness that require analysis against control
ﬂow. Shaham, Yahav, Kolodner and Sagiv [92] have devised a restricted version of
liveness of heap data using shape analysis.
The concept of modeling ﬂow functions in terms of primitive entity functions has
been proposed by B. Karkare [53].
© 2009 by Taylor & Francis Group, LLC

© 2009 by Taylor & Francis Group, LLC

5
Complexity of Iterative Data Flow Analysis
The round-robin iterative method of MFP computation presented in Chapter 3, was
described in terms of forward data ﬂow problems. It is a general method that can be
used with suitable changes for separable and non-separable, forward and backward,
unidirectional and bidirectional frameworks. We have already used the method in
working out examples of various frameworks in Chapters 1, 2, and 4. However, its
complexity was deﬁned only for rapid frameworks (Chapter 3).
In this chapter we present a generic version of round-robin method and deﬁne a
tight complexity bound for general monotone data ﬂow frameworks. We also intro-
duce work list based iterative algorithm which computes data ﬂow information in
a demand driven fashion. This algorithm forms the basis of formalizing the exact
amount of work that a data ﬂow analysis algorithm needs to perform.
5.1
Generic Flow Functions and Data Flow Equations
For simplicity of exposition, deﬁnitions of ﬂow functions and data ﬂow equations in
Chapter 3 were restricted to forward unidirectional frameworks. They are applicable
to backward unidirectional frameworks with a simple substitution of Inn by Outn
and pred(n) by succ(n). In either case, the following variations are possible and are
equivalent in terms of the data ﬂow information that is computed:
• Data ﬂow equations can be deﬁned in terms of Inn or Outn. It is not necessary
to deﬁne both Inn and Outn. In other words, given a neighbour m of n (i.e.,
a successor for backward problems and a predecessor for forward problems),
Inn can be computed from Inm. Similarly, Outn can be computed from Outm.
• The ﬂow functions can be associated with nodes or edges. Thus the following
two deﬁnitions of Inn are equivalent:
Inn =
p∈pred(n) fp(Inp)
Inn =
p∈pred(n) fp→n(Inp)
The above variations are possible because the data ﬂow information in unidirectional
data ﬂows depends on either the predecessors or successors but not on both. The
159
© 2009 by Taylor & Francis Group, LLC

160
Data Flow Analysis: Theory and Practice
n
m
Inn
Outn
Inm
Outm
−→
fn
−→
f n→m
Forward Flows
−→
fm
Inn
Outn
Inm
Outm
←−fn
←−fn→m Backward Flows
←−fm
FIGURE 5.1
Associating ﬂow functions with nodes and edges separately.
classical formulation of PRE (Section 2.4.4) does not meet these restrictions because
data ﬂow information associated with a node depends on both successors as well as
predecessors. In particular, in classical PRE,
• Inn is computed from Outn and Outm where m ∈pred(n), and
• Outn is computed from Ins where s ∈succ(n).
Such dependencies can be modeled by associating ﬂow functions with nodes and
edges separately as illustrated in Figure 5.1.
−→
f denotes a forward ﬂow function
whereas ←−f denotes a backward ﬂow function. The subscripts used in ﬂow function
notation distinguish node ﬂow functions from edge ﬂow functions. Deﬁning sep-
arate node and edge ﬂow functions requires explicating Inn and Outn rather than
leaving one of them implicit. This allows modeling the known ﬂows as illustrated
in Figure 5.2 by composing the node and edge ﬂow functions appropriately. For
forward unidirectional data ﬂows, the forward ﬂow functions associated with edges
are identity functions φid and the backward node and edge ﬂow functions are φ .
Analogous remarks hold for backward unidirectional data ﬂows. Figure 5.3 shows
ﬂow functions in forward, backward and bidirectional bit vector frameworks.
When separate ﬂow functions are associated with nodes and edges, the generic
data ﬂow equations can be written as shown below.
Inn =

BIStart ←−fn(Outn)
n = Start

m∈pred(n)
−→
f m→n(Outm)

←−fn(Outn)
otherwise
(5.1)
Outn =

BIEnd  −→
fn(Inn)
n = End

m∈succ(n)
←−fm→n(Inm)

 −→
fn(Inn)
otherwise
(5.2)
These equations compute the MFP solution of an instance of a data ﬂow frame-
work. They can be written at an abstract level in terms of program points rather
© 2009 by Taylor & Francis Group, LLC

Complexity of Iterative Data Flow Analysis
161
Forward
Backward
Bidirectional
Bidirectional
j
j
j
j
j
i
j
k
l
m
j
j
j
j
j
i
j
k
l
m
j
j
j
j
j
i
j
k
l
m
j
j
j
j
j
i
j
k
l
m
−→
f k→l ◦−→
fk ◦−→
fi→k
←−fi→k ◦←−fk ◦←−fk→l
−→
fi→k ◦←−fk→j
←−fk→m ◦−→
f k→l
FIGURE 5.2
Representing diﬀerent kinds of ﬂows by composing node and edge ﬂow functions.
than basic blocks as follows: Let Points denote the set of all program points in a
given CFG and xv ∈L denote the data ﬂow information associated with program
point v ∈Points. Let neighbours(v) denote the set of program points adjacent to v.
Then,
xv = Initialv 

u∈neighbours(v) fu→v(xu)

(5.3)
where Initialv is deﬁned as
Initialv =

BIStart v = Entry(Start)
BIEnd
v = Exit(End)
 
otherwise
fu→v is a forward/backward node/edge ﬂow function depending upon u and v as
described below:
u
v
fu→v
Entry(n)
Exit(n)
−→
f n
Exit(n)
Entry(n)
←−fn
Exit(m) Entry(n),m ∈pred(n) −→
fm→n
Entry(m) Exit(n),m ∈succ(n) ←−fn→m
This generalization can be viewed as replacing basic blocks by their entry and exit
points with conceptual edges between them. The direction of these edges indicates
the direction in which ﬂow functions are applied. A given edge u →v represents
a node ﬂow function if u and v are the two end-points of the same basic block;
otherwise it represents an edge ﬂow function.
In Section 3.3.1 we have deﬁned paths(n) as the paths starting from Start reach-
ing basic block n. We generalize this notion to deﬁne paths(u) as the set of paths
in the underlying undirected graph. These paths begin either at Start or End and
© 2009 by Taylor & Francis Group, LLC

162
Data Flow Analysis: Theory and Practice
fu→v
Data ﬂow framework
u = Entry(n)
u = Exit(n)
u = Entry(n)
u = Exit(n)
v = Exit(n)
v = Entry(n)
v = Exit(m)
v = Entry(m)
m ∈pred(n)
m ∈succ(n)
Reaching Deﬁnitions
−→
fn
φ 
φid
φ 
Live Variables
φ 
←−fn
φ 
φid
PRE
φ 
←−fn
φid
φid
FIGURE 5.3
Generic ﬂow functions in forward, backward, and bidirectional bit vector frame-
works.
reach program point u. We deﬁne the path function fρ for every path in paths(u) as
composition of generic ﬂow functions along the conceptual edges in ρ.
In unidirectional forward frameworks, the MOP solution at node n is deﬁned in
terms of all paths in paths(n). We deﬁne MOP solution at a program point u using
the generalized deﬁnition of paths(u) and generalized path function as follows:
MOPu =
ρ∈paths(u) fρ(BIρ)
(5.4)
where BIρ is BIStart if ρ begins at Start, BIEnd otherwise.
5.2
Generic Round-Robin Iterative Algorithm
A round-robin iterative algorithm for computing MFP assignment for forward data
ﬂow problems was described in Figure 3.9. Its version presented in Figure 3.15 uses
reverse postorder traversal over the graph. This makes it eﬃcient for forward data
ﬂow problems. Both the versions compute the data ﬂow information at entry points
of all blocks. We refer to the former version as RR (Round-Robin) and the latter
version as rpoRR (Reverse PostOrder Round-Robin).
We now introduce further generalizations in terms of program points and the order
of their traversal which can be chosen according to the data ﬂow problem. We use
the term stoRR (Speciﬁed Traversal Order Round-Robin) to refer to our algorithm.
It is presented in Figure 5.4. For simplicity we assume the presence of the  element
in the lattice, unlike rpoRR. If the lattice does not contain a  element, we replace
the initialization on Line 5 by
xu = Initialu 

j∈neighbours(i),j<i fj→i(xj)

(5.5)
© 2009 by Taylor & Francis Group, LLC

Complexity of Iterative Data Flow Analysis
163
Input: An instance (G, MG) of a monotone data ﬂow framework (LG,  G, FG). Ad-
jacent program points i, j are mapped to fi→j by MG. Program points are
numbered from 0...N −1 according to the chosen order of graph traversal.
Output: xi,∀i giving the output of the data ﬂow analysis for each program point.
Algorithm:
0
function stoRRMain()
1
{
for all i = 0 to N −1 do
2
{
if i = Start then Initiali = BIStart
3
else if i = End then Initiali = BIEnd
4
else Initiali =  
5
xi =  
6
}
7
change = true
8
while change do
9
{
change = false
10
for all i = 0 to N −1 do
11
{
temp = Initiali 
j∈neighbours(i) fj→i(xj)
12
if temp  xi then
13
{
xi = temp
14
change = true
15
}
16
}
17
}
18
}
FIGURE 5.4
Round-robin algorithm for computing MFP assignment at each program point.
The preferred order of traversal depends on the ﬂow functions in the data ﬂow
problem. For example, in forward problems, all node and edge ﬂow functions are
forward functions, hence reverse postorder is the most eﬃcient order of traversal. In
backward problems postorder traversal is preferable. The original bidirectional for-
mulation of PRE contains three types of ﬂow functions: Forward edge ﬂow functions,
backward edge ﬂow functions and backward node ﬂow functions. Thus a sequence
of consecutive backward ﬂow functions can be composed but a sequence of consec-
utive forward ﬂow functions cannot be composed. Hence postorder traversal is the
most eﬃcient traversal.
Complexity of round-robin method is deﬁned in context of the chosen order of
graph traversal. In Chapter 3, depth of the CFG was used to deﬁne the complexity
bound of round-robin method: The number of iterations required for MFP computa-
tion was shown to be 2 + d for forward bit vector frameworks and 3 + d for forward
rapid frameworks, assuming a reverse postorder traversal. For other frameworks,
© 2009 by Taylor & Francis Group, LLC

164
Data Flow Analysis: Theory and Practice
1 a = 2; b = 1;
c = 3; d = 3; 1
2 a = b +1; 2
3 b = c−2; 3
4 c = d; 4
5 d = d +3; 5
6 d = d +3; 6
Iterations
1
2
3
4
5
Out1 2,1,3,3 2,1,3,3
2,1,3,3
2,1,3,3
2,1,3,3
In2 2,1,3,3 2,1,3,⊥ 2,1,⊥,⊥ 2,⊥,⊥,⊥ ⊥,⊥,⊥,⊥
Out2 2,1,3,3 2,1,3,⊥ 2,1,⊥,⊥ ⊥,⊥,⊥,⊥ ⊥,⊥,⊥,⊥
Out3 2,1,3,3 2,1,3,⊥ 2,⊥,⊥,⊥ ⊥,⊥,⊥,⊥ ⊥,⊥,⊥,⊥
Out4 2,1,3,3 2,1,⊥,⊥ 2,⊥,⊥,⊥ ⊥,⊥,⊥,⊥ ⊥,⊥,⊥,⊥
Out5 2,1,3,6 2,1,⊥,⊥ 2,⊥,⊥,⊥ ⊥,⊥,⊥,⊥ ⊥,⊥,⊥,⊥
Data ﬂow values of variables a,b,c,d are shown as
xa,xb,xc,xd. Initial values are  , , , .
(a) A CFG with
(b) Data ﬂow values in round-robin method
d = 1
FIGURE 5.5
Complexity of round-robin algorithm for constant propagation cannot be deﬁned
using depth of CFG.
depth of CFG is not suﬃcient to deﬁne complexity bounds for round-robin method.
Example 5.1
Consider the CFG in Figure 5.5(a). Statements in node 1 do not have any data
dependence between them hence for simplicity we have combined them into
a single block. Depth of this CFG is 1. Round-robin algorithm for constant
propagation on this graph converges in 6 iterations. Part (b) of the ﬁgure
shows the values at some program points each iteration. The last iteration
is not shown since it is required only for detection of ﬁxed point. It is not
possible to explain the number of iterations in terms of d.
5.3
Complexity of Round-Robin Iterative Algorithm
When stoRR algorithm is used for performing data ﬂow analysis for a given instance
of a framework, xu values are initialized to  if  exists in the lattice of the frame-
work. If the lattice does not contain  , then xu values are initialized to a suitably
high value in the lattice using Equation (5.5). As the algorithm executes, the data
© 2009 by Taylor & Francis Group, LLC

Complexity of Iterative Data Flow Analysis
165
ﬂow values gradually change towards ⊥. The number of iterations required by the
algorithm depends on the number of data ﬂow value changes that can be accommo-
dated in a single iteration. In this section we investigate the order of changes in data
ﬂow values and their impact on the number of iterations of stoRR algorithm.
Two main steps in our treatment of complexity analysis of stoRR algorithm are:
• Formalizing the notion of order of dependence of data ﬂow values at diﬀerent
program points in a CFG.
• Devising a measure of how closely the order of traversal speciﬁed to the stoRR
algorithm follows the order of dependences of data ﬂow values.
For the ﬁrst step, we present an algorithm that directly follows the dependence of
data ﬂow values. We show that this algorithm computes the same solution as the
stoRR. This allows us to deﬁne the minimum work that any algorithm of data ﬂow
analysis must perform. Based on the observations made in the algorithm, we capture
the order of dependence of data ﬂow values at diﬀerent program points by deﬁning
the concept of an information ﬂow path. For a given order of traversal, it then be-
comes possible to quantify how close the order is to the order of the dependence of
data ﬂow values.
5.3.1
Identifying the Core Work Using Work List
In this section we describe an iterative algorithm called the work list algorithm which
follows the order of data ﬂow value changes, and hence is typically more eﬃcient
than round-robin method. However, it has an additional overhead of managing the
work list. It follows the order of changes by restricting the computation of data ﬂow
values to paths along which changes in data ﬂow values take place. This is diﬀer-
ent from round-robin method where a single change in the data ﬂow information at
a program point triggers another iteration which traverses all program points indis-
criminately.
Figure 5.6 shows a work list based algorithm for performing data ﬂow analysis
using generic ﬂow functions. The organization of the work list inﬂuences the eﬃ-
ciency of the algorithm signiﬁcantly; it can be increased by incorporating heuristics
such as insertion of program points in a preferred order of traversal.
Lines 1 to 7 in the algorithm initialize the data ﬂow values at each program point
to a value that is computed independently of the other program points. It is assumed
that the lattice contains a  element; if it does not, then the assignment on line
5 must be modiﬁed to restrict computation of fu→v to only those neighbours of v
that have already been visited. Initialization of the work list involves adding the
program points with non- data ﬂow values to the work list; a  does not inﬂuence
any value. From these program points, data ﬂow information is propagated to their
neighbouring program point which in turn are added to the work list if their data ﬂow
values change.
In stoRR algorithm, the data ﬂow value at a program point is recomputed in each
iteration (line 11, Figure 5.4). This accumulates the eﬀect of all neighbours of a
© 2009 by Taylor & Francis Group, LLC

166
Data Flow Analysis: Theory and Practice
Input: An instance (G, MG) of a monotone data ﬂow framework (LG,  G, FG). Ad-
jacent program points u,v are mapped to fu→v by MG.
Output: xu,∀u giving the output of the data ﬂow analysis for each program point.
Algorithm:
0
function worklist dfaMain()
1
{
for all u ∈Points,
2
{
if u = Start then Initialu = BIStart
3
else if u = End then Initialu = BIEnd
4
else Initialu =  
5
xu = Initialu 

v∈neighbours(u) fv→u( )

6
if xu   then add u to worklist
7
}
8
while worklist is not empty do
9
{
Remove the ﬁrst program point u from worklist
10
for all v ∈neighbours(u) do
11
{
temp = xv  fu→v(xu)
12
if temp  xv then
13
{
xv =temp
14
Add v to worklist
15
}
16
}
17
}
18
}
FIGURE 5.6
Work list algorithm for computing MFP assignment at each program point.
program point u on the data ﬂow value xu. By contrast, in a work list algorithm,
a change in a value xu is propagated to all its neighbours by reﬁning their values.
Reﬁnement implied merging the old value at that point with the new value obtained
from a single neighbour. Because of this diﬀerence between the two algorithms, we
need to explicitly show that they compute the same assignment of data ﬂow values.
We do so by showing three important results:
• A work list algorithm terminates.
• When a work list algorithm terminates, the resulting data ﬂow values constitute
a ﬁxed point assignment.
• Finally we show that the resulting ﬁxed point assignment is actually the max-
imum ﬁxed point assignment.
Since we know that stoRR algorithm also computes the MFP assignment, and that
© 2009 by Taylor & Francis Group, LLC

Complexity of Iterative Data Flow Analysis
167
the MFP assignment is unique, it follows that the two algorithms compute identical
assignment.
For proving the properties of the work list algorithm, we deﬁne the notion of a
step of the algorithm as follows. Step 1 refers to the execution of the for loop (lines
1 to 7). Each subsequent step corresponds to the reﬁnement of some xu on lines 11,
12, and 13. Each step i uses the values from step i−1; observe that the value used
may have been computed in some earlier step. It follows that the values used in step
1 must be values from step 0; since the value used in step 1 is  , we say that x0
u =  .
LEMMA 5.1
The work list algorithm terminates.
PROOF
Consider some step i in the algorithm. If step i computes xu,
then due to reﬁnement, xi
u  xi−1
u . If xu has not been modiﬁed in this step,
then xi
u = xi−1
u
and u is not put on the work list. However, if xu is modiﬁed and
u is put on the work list, then xi
u  xi−1
u . Thus the modiﬁcations in the value of
xu follow a strictly descending chain. Since all strictly descending chains are
ﬁnite, each program point can be inserted in the worklist a ﬁnite number of
times. Eventually the worklist becomes empty and the algorithm terminates.
Now we prove that on termination, the work list algorithm computes a ﬁxed point
assignment.
LEMMA 5.2
Let the work list algorithm terminate in n steps. Then,
∀u ∈Points : xn
u  Initialu 

v∈neighbours(u) fv→u
xn
v

PROOF
From Lemma 5.1
∀u ∈Points,∀i ≥1 : xi
u  xi−1
i
⇒
∀u ∈Points,∀i ≥1 : xi
u  Initialu
(because ∀u ∈Points : x1
u  Initialu)
Consider an arbitrary program point u and the last step m in which the value
of xu was computed. By the deﬁnition of reﬁnement, we have
xm
u = xm−1
u
 fv→u(xm−1
v
)
(5.6)
⇒
xm
u  fv→u(xm−1
v
)
© 2009 by Taylor & Francis Group, LLC

168
Data Flow Analysis: Theory and Practice
Since this is the last computation of xu, the eﬀect of changes in other neigh-
bours v of u has been incorporated by executing (5.6) for some m ≤m ≤n.
Hence,
xn
u  Initialu 

v∈neighbours(u) fv→u(xn−1
v
)

(5.7)
The algorithm terminates when no program point is added to the work list.
Thus,
∀v ∈Points : xn
v = xn−1
v
Substituting the above in (5.7) results in,
xn
u  Initialu 
v∈neighbours(u) fv→u(xn
v)
LEMMA 5.3
Let the work list algorithm terminate in n steps. Then,
∀u ∈Points : xn
u  Initialu 

v∈neighbours(u) fv→u
xn
v

PROOF
We prove this by induction on the number of steps.
1. Basis: In step 1, we compute
∀u ∈Points : x1
u = Initialu 

v∈neighbours(u) fv→u
x0
v =  
⇒
∀u ∈Points : x1
u  Initialu 

v∈neighbours(u) fv→u
x0
v =  
2. Inductive step: Assume that for some step i
∀u ∈Points : xi
u  Initialu 

v∈neighbours(u) fv→u
xi−1
v

Consider an arbitrary program point u and step i+1. If xu is not modiﬁed
in step i+1, xi+1
u
= xi
u and by the inductive hypothesis, it trivially follows
that,
xi+1
u
 Initialu 

v∈neighbours(u) fv→u
xi
v

© 2009 by Taylor & Francis Group, LLC

Complexity of Iterative Data Flow Analysis
169
Thus the interesting case that needs to be proved is when xu is modiﬁed
in step i+1. By the deﬁnition of reﬁnement,
xi+1
u
= xi
u  fv→u
xi
v

Substituting for xi
u from the inductive hypothesis
xi+1
u
 Initialu 

v∈neighbours(u) fv→u
xi−1
v

 fv→u
xi
v

If the value of every neighbour v was modiﬁed in some step j < i, then
xi
v = xi−1
v
and
xi+1
u
 Initialu 

v∈neighbours(u) fv→u
xi
v

and the lemma holds. For the other possibility, let there be a neighbour
v whose value was modiﬁed in step i. Then,
xi+1
u
 Initialu 

v∈neighbours(u) fv→u
xi−1
v

 fv→u
xi
v
We rewrite the meet to separate the term for v
xi+1
u
 Initialu 

v∈neighbours(u),
vv
fv→u
xi−1
v

 fv→u
xi−1
v 
 fv→u
xi
v
However,
fv→u
xi
v  fv→u
xi−1
v 
(because xi
v  xi−1
v )
For all other v, the values in i−1 and i are same. Hence,
xi+1
u
 Initialu 

v∈neighbours(u),
vv
fv→u
xi
v

 fv→u
xi
v
 Initialu 

v∈neighbours(u) fv→u
xi
v

Hence the lemma follows.
LEMMA 5.4
The work list algorithm computes a solution of Equation (5.3).
PROOF
Let the work list become empty after n steps. From Lemma (5.2),
we know that
∀u ∈Points : xn
u  Initialu 

v∈neighbours(u) fv→u
xn
v

© 2009 by Taylor & Francis Group, LLC

170
Data Flow Analysis: Theory and Practice
and from Lemma (5.3)
∀u ∈Points : xn
u  Initialu 

v∈neighbours(u) fv→u
xn
v

Hence it follows that,
∀u ∈Points : xn
u = Initialu 

v∈neighbours(u) fv→u
xn
v

LEMMA 5.5
The work list algorithm computes MFP assignment of Equation (5.3).
PROOF
Consider an arbitrary solution FP of Equation (5.3). Clearly,
∀u ∈Points : FPu = Initialu 

v∈neighbours(u) fv→u
FPv

Let the work list algorithm terminate after n steps. We need to prove that
∀u ∈Points : FPu  xn
u
We prove this by induction on step number in the work list algorithm.
1. Basis: From the deﬁnition of step 1,
∀u ∈Points : x1
u = Initialu 

v∈neighbours(u) fv→u
 
Since ∀v ∈Points : FPv   , it follows that
∀u,v ∈Points :

v∈neighbours(u) fv→u
FPv



v∈neighbours(u) fv→u
 
Since Initialv is constant,
∀u ∈Points : FPu  x1
u
2. Inductive Step: Assume the inductive hypothesis
∀u ∈Points : FPu  xi
u
Consider an arbitrary program point u. If xu is not modiﬁed in step
i+1 then the inductive step trivially follows. Thus we have to show the
inductive step when xu is modiﬁed in step i+1. From the deﬁnition of
a ﬁxed point,
∀u ∈Points, FPu  fv→u
FPv

∀v ∈neighbours(u)
© 2009 by Taylor & Francis Group, LLC

Complexity of Iterative Data Flow Analysis
171
By the inductive hypothesis, FPv  xi
v and hence
∀u ∈Points : FPu  fv→u
xi
v

∀v ∈neighbours(u)
However, from inductive hypothesis we also have
∀u ∈Points : FPu  xi
u
Combining the two,
∀u ∈Points : FPu  xi
u  fv→u
xi
v

∀v ∈neighbours(u)
From the deﬁnition of reﬁnement,
xi+1
u
= xi
u  fv→u(xi
v)
Hence it follows that
∀u ∈Points : FPu  xi+1
u
Since the assignment computed by the work list algorithm is a ﬁxed point and
it contains every possible ﬁxed point FP, it must be the MFP.
5.3.2
Information Flow Paths in Bit Vector Frameworks
For simplicity of exposition we begin our discussion with bit vector frameworks in
which the data ﬂow values of all entities are independent.
Recall that Σ = {α,β,...,ω} denotes the set of program entities whose data ﬂow
information is computed during data ﬂow analysis. Since bit vector frameworks are
separable, ﬂow of information for each entity can be examined independently. Hence
the discussion in this section refers to a single entity say α and its lattice L. The
iterative algorithms deﬁned in Figures 5.4 and 5.6 compute data ﬂow information of
all entities simultaneously.
Since L = { ,⊥} in bit vector frameworks, only the following three monotonic
ﬂow functions are possible: φ , φ⊥, and φid (Section 4.5). The data ﬂow analysis of
bit vector framework involves initializing data ﬂow values to  and then propagating
the ⊥value in the graph. The ⊥values are generated as a result of local analysis
and are propagated to other program points during global analysis. We say that data
ﬂow information is generated at a program point if the information results from
application of a constant function other than φ ; in bit vector frameworks a data ﬂow
information is generated φ⊥. The point of generation, called origin of information
ﬂow is deﬁned as follows.
DEFINITION 5.1
A program point v is an origin of data ﬂow informa-
tion for entity α if any of the following conditions is satisﬁed:
© 2009 by Taylor & Francis Group, LLC

172
Data Flow Analysis: Theory and Practice
1. v is Entry(Start) and x α
v = ⊥in BIStart.
2. v is Exit(End) and x α
v = ⊥in BIEnd.
3. If there exists a pair of adjacent program points u,v such that for some
entity α, fu→v = φ⊥.
DEFINITION 5.2
An information ﬂow path for an entity α in a bit vector
framework is deﬁned as a maximal acyclic sequence of adjacent program points
p0, p1,...pm such that p0 is an origin of data ﬂow information for α, and every
ﬂow function fpi→pi+1 is φid.
An information ﬂow path represents a single thread of changes in the values of an
entity in the program. In general, when there is a change in the data ﬂow at a program
point u, the ﬂow of information terminates at u if the change at u does not cause a
change in the data ﬂow value of any neighbour v of u. In bit vector frameworks,
data ﬂow value of an entity at a program point can change only once. Since an ifp
propagates a ⊥value, no more changes in data ﬂow value are possible at any program
point already present in the ifp. Hence, ifps in bit vector frameworks are acyclic.
Information ﬂow paths diﬀer from paths in paths(u) in many ways: the paths in
paths(u) always start from Start or End, ifps may start from any program point.
Further, a path in paths(u) ends on u, whereas an ifps is not deﬁned for a give pro-
gram point. Paths in paths(u) may be cyclic, whereas ifps in bit vector frameworks
are always acyclic.
For brevity, we denote Entry(n) and Exit(n) by In and On respectively when de-
picting an information ﬂow path. In Figure 5.2(c), the data ﬂow indicated by the
dashed line takes place along the subpath (Oi →Ik →O j) of an ifp, while the data
ﬂow in Figure 5.2(d) takes place along the subpath (Il →Ok →Im) of an ifp. Fig-
ure 5.7 shows an information ﬂow path in partial redundancy elimination for our
example program. In this example, data ﬂow information at Exit(n6) is 0 as a re-
sult of assignment to c in n4. The ifp responsible for propagating information from
Entry(n4) to Exit(n6) is (In4 →On3 →In5 →On6) and is shown by a sequence of gray
dashed arrows in the ﬁgure.
The information ﬂow from p0 to pm is realized through the path ﬂow function fρ
of ρ which is a composition of ﬂow functions of all edges in ρ:
fρ = fpm−1→pm ◦fpm−2→pm−1 ◦···◦fp1→p2 ◦fp0→p1
(5.8)
Using the path ﬂow function the data ﬂow information reaching pm from p0 can be
computed. In bit vector frameworks, the path ﬂow function of an ifp is an identity
function.
Information Flow Paths and the Work List Algorithm
Observe that the information ﬂow paths in bit vector frameworks correspond to the
paths traced by the generic work list based algorithm given in Figure 5.6. Program
© 2009 by Taylor & Francis Group, LLC

Complexity of Iterative Data Flow Analysis
173
n1
b1: b = 4;
a1: a = b +c;
d1: d = a ∗b;
n1
n2 b2: b = a −c; n2
n3 c1: c = b+c; n3
n4 c2: c = a∗b;
f(a−b);
n4
n5 d2: d = a+b; n5
n6 f(b+c); n6
n7 g(a +b); n7
n8 h(a −c);
f(b+c); n8
The ifp (In4 →On3 →In5 →On6) (shown in dashed arrows) is responsible for sup-
pressing hoisting of expression b +c at Exit(n6).
FIGURE 5.7
An information ﬂow path in PRE example.
points during initialization are essentially the origins of information ﬂow for some
entity. However, since work list algorithm operates on data ﬂow values of all entities
simultaneously, the paths traced by work list algorithms may correspond to multiple
ifps each referring to a diﬀerent entity. Further, if a program point is added to the
head of the work list, ifps for an entity are traversed independently; if it is added to
the rear, ifps of an entity may be traversed in an interleaved fashion.
5.3.3
Deﬁning Complexity Using Information Flow Paths
We now deﬁne the complexity of stoRR algorithm by relating each iteration of the
algorithm to the fragment of an ifp that it can cover. Note that we consider the
iterations of the while loop only; the initialization is not counted in the number of
iterations unless the initialization is performed using Equation (5.5).
The discussion in this section is general and is not restricted to bit vector frame-
works because it relies on the occurrence of program points in ifps. Later when we
deﬁne ifps for fast frameworks and non-separable frameworks, the ifps are extended
to qualify the program points with additional information. It is done only to identify
© 2009 by Taylor & Francis Group, LLC

174
Data Flow Analysis: Theory and Practice
the relevant sequences of program points and eventually the complexity is deﬁned in
terms of the sequence of program points only.
Consider an edge pi →pi+1 in an ifp. In the stoRR algorithm, the order of visiting
pi and pi+1 depends upon the chosen order of graph traversal and is ﬁxed throughout
the analysis. This has the following consequences:
• Let pi be visited before pi+1 in the order of traversal. In this case, the data
ﬂow value at pi is computed ﬁrst, and it is available during computation of the
value at pi+1 in the same iteration. Hence, propagation of information from pi
to pi+1 takes place in the same iteration in which the value at pi is computed.
• Let pi+1 be visited before pi in the order of traversal. In this case, the data
ﬂow value at pi+1 is computed ﬁrst. Hence, it must use the old value at pi.
The new value at pi is computed in the same iteration, but can only be used
for computing the value at pi+1 in a subsequent iteration. This implies that
propagating information from pi to pi+1 requires an additional iteration.
DEFINITION 5.3
Traversal of adjacent program point pi and pi+1 in
an information ﬂow path ρ is called conforming if pi occurs before pi+1 in the
chosen order of traversal. Otherwise, it is a non-conforming traversal.
Conforming traversals do not contribute additional iterations in the stoRR algo-
rithm whereas each non-conforming traversal requires one extra iteration.
DEFINITION 5.4
Width of an information ﬂow path ρ with respect to a
given order of traversal is deﬁned as the number of non-conforming traversals
in ρ.
We denote the width of an ifp ρ by width(ρ). Width is a measure of the number of
iterations required by stoRR algorithm to propagate information along ρ.
Example 5.2
In Figure 5.7, width of ifp (In4 →On3 →In5 →On6) is 2 since edge traversals
On3 →In5 and In5 →On6 are non-conforming traversals as the CFG nodes are
visited in postorder traversal.
DEFINITION 5.5
A span is a maximal sequence of conforming edge
traversals in an ifp.
Spans are separated by a non-conforming edge traversal and vice-versa. Thus
two successive non-conforming edge traversals have a null span between them. An
information ﬂow path may begin and/or end with a null span.
© 2009 by Taylor & Francis Group, LLC

Complexity of Iterative Data Flow Analysis
175
The information along a span can be propagated in a single traversal over the
graph. This traversal is same as the traversal of the preceding non-conforming edge.
DEFINITION 5.6
Width of a CFG for an instance of data ﬂow frame-
work is deﬁned with respect to a given order of traversal as the maximum
width of any ifp for the given instance.
THEOREM 5.1
If the width of a CFG for an instance of a bit vector framework is w then the
round-robin iterative method stoRR converges in w+1 iterations.
PROOF
The information ﬂow can be initiated only after data ﬂow values
at all origins are computed. The stoRR algorithm achieves this in the ﬁrst iter-
ation after initialization. The same iteration propagates the information along
a non-null span (if any) at the beginning of each ifp. Every non-conforming
edge traversal and the span following it requires an additional iteration. Thus,
w+1 iterations are suﬃcient along the ifps that determine width of the CFG.
Though the stoRR algorithm converges in w + 1 iterations, in practice we do not
know the width of a ﬂow graph and the method terminates after discovering that
there is no further change. Thus, practically, w+2 iterations are required.
The main advantage of using the notion of width is that it is uniformly applicable
to general data ﬂow frameworks including bidirectional and non-separable frame-
works. Further, it is deﬁned in terms of a speciﬁed order and hence explains the
diﬀerence in the number of iterations when the order of traversal is changed.
Example 5.3
The depth of the program in Figure 5.7 for PRE is 1 whereas its width is 2.
Hence the round-robin method requires at most 4 iterations to converge.
For unidirectional data ﬂow problems, if the direction of graph traversal is same
as the direction of the data ﬂows, the width of a graph reduces to its depth. However,
depth is applicable to unidirectional data ﬂow problems only.
5.3.4
Information Flow Paths in Fast Frameworks
Fast frameworks are separable 2-bounded frameworks. However, they are more gen-
eral than bit vector frameworks in that they allow more than two elements in a com-
ponent lattice, and also allow ﬂow functions that compute incomparable values. The
former requires generalizing the deﬁnition of origin while the latter requires gener-
© 2009 by Taylor & Francis Group, LLC

176
Data Flow Analysis: Theory and Practice
alizing the value associated with a program point in an ifp.
In fast frameworks, the data ﬂow value at a program point changes due to one of
the following reasons: (a) Result of application of a ﬂow function, or (b) Merging
incomparable values from neighbours. In bit vector frameworks, the latter situation
never arises because the component lattice does not contain incomparable values. In
order to handle fast frameworks, the deﬁnition of information ﬂow paths must be
extended to incorporate merging of information. Also, in bit vector frameworks, an
information ﬂow path propagates the same data ﬂow value (⊥) from an origin to all
possible program points. In fast frameworks, a value at a program point may undergo
more than one change due to non-identity non-constant functions and merging.
First we extend the deﬁnition of origin to allow the program point to be qualiﬁed
with the generated data ﬂow value.
DEFINITION 5.7
A pair v,x α
v  is an origin of information ﬂow for
entity α if any of the following conditions is satisﬁed:
1. v is Entry(Start) and x α
v   in BIStart.
2. v is Exit(End) and x α
v   in BIEnd.
3. If there exists a pair of adjacent program points u,v such that for some
entity α, f α
u→v is a constant pef φz computing the value z   .
Apart from recording the data ﬂow value, handling the merging of data ﬂow values
intermediate program points requires the following extensions:
• Merging may involve a data ﬂow value generated by some other ifp traversed
earlier. To remember the values computed by a diﬀerent ifp, we deﬁne an ifp
with respect to a given assignment A : Points '→L ∪{undef}. Aα
u denotes value
of α at program point u in assignment A. Initial assignment is ∀u ∈Points,Aα
u =
 if the lattice contains a  element; it is ∀u ∈Points,Aα
u = undef otherwise.
• We need to deﬁne a function latest() to extract the latest data ﬂow value of α
at u when examining an ifp ρ.
An ifp for a fast framework is deﬁned as follows.
DEFINITION 5.8
Given an assignment A : Points '→L ∪{undef}, an in-
formation ﬂow path ρ for an entity α in a fast framework is deﬁned as a max-
imal acyclic sequence of tuples p0,x0,p1,x1,...,pm,xm such that p0,x0 is
an origin of information ﬂow for α, and given pi,xi, its successor pi+1,xi+1
is deﬁned as follows:
1. pi, pi+1 are adjacent program points,
2. Let ρ be the preﬁx of ρ containing i tuples. Then
xi+1 = fpi→pi+1(xi) ⊕latest(pi+1,ρ)
© 2009 by Taylor & Francis Group, LLC

Complexity of Iterative Data Flow Analysis
177
1 h1 1
2 h0 2
3 h1 3
4
f
4
v1
v0
v2
⊥
Function f
v1
v0
v2
⊥
v1
v0
v2
⊥
Function h0 and h1
∀x ∈L : h0(x) = v0
∀x ∈L : h1(x) = v1
(a) CFG
(b) L
(c) Flow Functions
FIGURE 5.8
An instance of a distributive non-bit vector rapid framework reproduced from Fig-
ure 3.19. This instance requires d(G,T) + 3 iterations of a round-robin algorithm
with reverse post order traversal.
where
(a) f α
pi→pi+1 is a non-constant function.
(b) latest(u,ρ) returns value x α
j if p j is the last occurrence of u in ρ;
if ρ does not contain u, then latest(u,ρ) returns Aα
u.
(c) x⊕x  =
x
if x  = undef
xx  otherwise
The acyclicity condition prohibits the same pair pi,xi from occurring multiple
times in the ifp; a program point may appear multiple times in the ifp.
Unlike bit vector frameworks, the path ﬂow function fρ for an ifp ρ need not
be φid. An assignment A used to deﬁne an ifp must be a valid assignment for a
correct representation of information ﬂows in a program. If it is arbitrarily chosen,
the resulting complexity measures could be incorrect. For the ﬁrst ifp traversed,
A : ∀u ∈Points,xu =  is valid if  exists in the lattice of the framework; otherwise
it must be A : ∀u ∈Points,xu = undef. A must get updated by each subsequent ifp.
DEFINITION 5.9
Given an assignment A : Points '→L ∪{undef}, and an
ifp ρ, the resulting assignment A is ∀u ∈Points,A
u =latest(u,ρ) where latest(u,ρ)
is deﬁned in Deﬁnition 5.8.
Example 5.4
Consider the instance of a data ﬂow framework shown in Figure 5.8 which
has been reproduced from Example 3.11 on page 96. For simplicity, we have
shown only the non-identity node ﬂow functions and assume that there is
© 2009 by Taylor & Francis Group, LLC

178
Data Flow Analysis: Theory and Practice
1 h 1
2
g
2
3 h0 3
4
f
44
5
f
55
 
v1
v0
v2
v3
⊥
Function f
Function g
 
v1
v0
v2
v3
⊥
 
v1
v0
v2
v3
⊥
 
v1
v0
v2
v3
⊥
 
v1
v0
v2
v3
⊥
Function h0
Function h 
∀x ∈L : h0(x) = v0
∀x ∈L : h (x) =  
(a) CFG
(b) L
(c) Flow Functions
FIGURE 5.9
An instance of a distributive non-rapid fast framework that requires d(G,T)+4 iter-
ations of a round-robin algorithm with reverse post order traversal.
a single unspeciﬁed entity.
All edge ﬂow functions are φid.
Let the given
assignment be A : ∀u ∈Points,xu = undef. The constant function h0 produces
data ﬂow value v0 for entity en. Hence Exit(2),v0 is an origin of information
ﬂow. An ifp originating at Exit(2),v0 is
(O2,v0 →I3,v0 →O3,v0 →I4,v0 →O4,v1 →
I3,⊥ →O3,⊥ →I4,⊥ →O4,⊥ →I2,⊥)
The round-robin algorithm requires 4 iterations to converge with a reverse
post ﬁrst order traversal. The data ﬂow value at Exit(3) is v0 in the ﬁrst
iteration. In the second iteration, it changes to ⊥as a result of merging the
data value of Exit(4) and Exit(2). The third iteration is required to propagate
this value to Entry(2) and the ﬁnal iteration is required to detect convergence.
The depth of the CFG in example in Figure 5.8 is 1. The required number
of iterations can be explained in term of width. Width of the above ifp is 2
due to the non-conforming edges O4 →I3 and O4 →I2.
Example 5.5
Consider the instance of a data ﬂow framework shown in Figure 5.9. We leave
it for the reader to verify that is a distributive non-rapid fast framework. All
edge ﬂow functions are φid. Constant function h0 produces data ﬂow value
v0. With the initialization  at all program points, the round-robin algorithm
converges in 5 iterations with a reverse post order traversal. The data ﬂow
value at Entry(4) changes from v0 to v3 to ⊥in the ﬁrst three iterations. The
© 2009 by Taylor & Francis Group, LLC

Complexity of Iterative Data Flow Analysis
179
fourth iteration is required to propagate this change to Entry(2) and the ﬁfth
iteration is required to detect the ﬁxed point.
The depth of the CFG is 1. The number of iterations can be explained by
the following ifp whose origin is Exit(3),v0.
(O3,v0 →I4,v0 →O4,v0 →I5,v0 →O5,v1 →I2,v1 →
O2,v1 →I4,v3 →O4,v3 →I5,v3 →O5,v3 →I2,v3 →
O2,⊥ →I4,⊥ →O4,⊥ →I5,⊥ →O5,⊥ →I2,⊥)
The width of this ifp is 3 due to three occurrences of non-conforming edge
O5 →I2; observe that the data ﬂow values associated with the multiple occur-
rence of program points are diﬀerent.
5.3.5
Information Flow Paths in Non-separable Frameworks
Recall that in bit vector frameworks, only one change is possible in the data ﬂow
value of a given entity α at a given program point u. Further, the value of α at u
is inﬂuenced only by the value of α at a neighbouring program point v; some other
entity β cannot inﬂuence the value of α. In fast frameworks, the data ﬂow value of α
at u could change multiple times. Hence information ﬂow paths for fast frameworks
are deﬁned in terms of a given assignment of values and a program point is qualiﬁed
with the data ﬂow value. Besides, they are also deﬁned for a given entity due to the
independence of entities.
In non-separable frameworks the possible changes in data ﬂow values are still
more general. A data ﬂow value of an entity α at a program point u can be inﬂu-
enced by the data ﬂow value of some other entity β at a neighbouring program point
v. Similar to fast frameworks, data ﬂow value of an entity could change multiple
times. Thus multiple interdependent information ﬂows are simultaneously possible
at a given program point.
Example 5.6
Consider the CFG in Figure 5.10 on the next page. In constant propagation
framework, the value of variable c in node 4 is inﬂuenced by the value of a
computed in node 2 (via deﬁnition of b in node 5) as well as by the value of b
computed in 3. We cover these inﬂuences in separate information ﬂow paths.
Also, the value of a generated in node 2 is propagated to the entry and exit
points of nodes 4,5,6. This propagation is covered by a separate ifp.
We continue to deﬁne an information ﬂow path for a single thread of information
ﬂow. Since an ifp is deﬁned in terms of a given assignment, using the data ﬂow value
of an entity at a the program point where multiple ifps intersect, allows us to handle
interdependence of information ﬂows.
We use the concepts and notations from Section 4.5 that models the component
ﬂow functions in non-separable frameworks in terms of primitive and composite
© 2009 by Taylor & Francis Group, LLC

180
Data Flow Analysis: Theory and Practice
1
a = b
1
2 a = 2 23 b = 3 3
4
c = b
4
5
b = a
5
6
b = a
6
FIGURE 5.10
A CFG to illustrate information ﬂow paths in copy constant propagation.
entity functions. We extend the notation by using program points u and v and edges
between them as subscripts of a function. A component function f α that computes
the data ﬂow value of an entity α at program point v from the values of other entities
at a neighbouring program point u is denoted by f α
u→v. If it can be deﬁned in terms
of primitive entity functions (pefs):
f α
u→v(xu) =
β ∈Σf
β→α
u→v

x β
u

(5.9)
where Σ is the set of entities, and f
β→α
u→v is the pef that computes the data ﬂow value
of α at program point v from the value of β at program point u.
Since we need to handle changes across diﬀerent entities, we extend the notion of
information ﬂow to qualify a program point with the entity also.
DEFINITION 5.10
A tuple

v,z α
v ,α

is an origin of information ﬂow
for entity α if any of the following conditions is satisﬁed:
1. v is Entry(Start) and x α
v   in BIStart.
2. v is Exit(End) and x α
v   in BIEnd.
3. If there exists a pair of adjacent program points u,v such that for some
entity α ∈Σ, pef f
β→α
u→v is a constant pef φz computing the value z   
for every β ∈Σ.
Observe that any other pef cannot originate the ﬂow of information. Similarly, a
composite entity function (cef) also cannot originate the ﬂow of information.
At each point in an ifp, we record the entity denoted en whose data ﬂow value
is modiﬁed at that point as a result of the application of a non-constant component
© 2009 by Taylor & Francis Group, LLC

Complexity of Iterative Data Flow Analysis
181
function, and use it to identify the candidate entity at the subsequent point. Changes
in values due to merging of information are computed using the latest() function as
discussed in the context of fast frameworks.
DEFINITION 5.11
Given an assignment A : Points '→L ∪{undef}, and
an origin p0,x α
0 ,α of information ﬂow for some entity α, an information
ﬂow path ρ is deﬁned as a maximal acyclic sequence of tuples
(p0,x0,α,p1,x1,en1,...,pm,xm,enm)
where ∀β  α ∈Σ,x β
0 = Aβ
0 and given pi,xi,eni, its successor pi+1,xi+1,eni+1
is deﬁned as follows:
1. pi, pi+1 are adjacent program points,
2. Let ρ be the preﬁx of ρ containing i tuples. Select a β such that eni
inﬂuences β through a non-constant pef or a cef. Then,
eni+1 = β
x γ
i+1 =

f β
pi→pi+1(xi) ⊕latest(pi+1,ρ,β)
γ = β
latest(pi+1,ρ,γ)
otherwise
where
(a) latest(u,ρ,β) returns value x β
j if p j is the last occurrence of u in ρ;
if ρ does not contain u, then latest(u,ρ,β) returns A β
u .
(b) x⊕x  =
x
x  = undef
xx  otherwise
When the changes in data ﬂow values are not required explicitly, we denote an ifp
by a sequence of program points p0, p1,..., pn. In the presence of cycles, a program
point q contained in a cycle may appear multiple times in an ifp. The condition of
acyclicity in the deﬁnition of ifp implies that a tuple u,xu,enu cannot appear twice
in an ifp, although a program point u may appear multiple times.
Example 5.7
Consider the instance of copy constant propagation for the CFG in Figure 5.10
on the facing page. Figure 5.11 on the next page shows some information ﬂow
paths for this instance. Changes in data ﬂow values due to application of non-
constant component functions are shown by adding edges from x α
u →x β
v for
each edge u,xu,α →v,xv,β in the ifp. Thick arrows indicate the traversal
along the back edge 5 →4. We leave identiﬁcation of the ifps beginning at
node 3 as an exercise.
© 2009 by Taylor & Francis Group, LLC

182
Data Flow Analysis: Theory and Practice
AI1 =

 , , 

AO1 =

 , , 

AI2 =

 , , 

AO2 =

 , , 

AI3 =

 , , 

AO3 =

 , , 

AI4 =

 , , 

AO4 =

 , , 

AI5 =

 , , 

AO5 =

 , , 

AI6 =

 , , 

AO6 =

 , , 


O2,

2, , 

,a

,

I4,

2, , 

,a

,

O4,

2, , 

,a

,

I5,

2, , 

,a

,

O5,

2, , 

,a

,

I6,

2, , 

,a

,

O6,

2, , 

,a

AI1 =

 , , 

AO1 =

 , , 

AI2 =

 , , 

AO2 =

2, , 

AI3 =

 , , 

AO3 =

 , , 

AI4 =

2, , 

AO4 =

2, , 

AI5 =

2, , 

AO5 =

2, , 

AI6 =

2, , 

AO6 =

2, , 


O2,

2, , 

,a

,

I4,

2, , 

,a

,

O4,

2, , 

,a

,

I5,

2, , 

,a

,

O5,

2,2, 

, b

,

I4,

2,2, 

, b

,

O4,

2,2, 

, b

,

I5,

2,2, 

, b

Assignment A
ifp ρ w.r.t. A
Assignment A
ifp ρ1 w.r.t. A
resulting from A,ρ
AI1 =

 , , 

AO1 =

 , , 

AI2 =

 , , 

AO2 =

2, , 

AI3 =

 , , 

AO3 =

 , , 

AI4 =

2,2, 

AO4 =

2,2, 

AI5 =

2,2, 

AO5 =

2,2, 

AI6 =

2, , 

AO6 =

2, , 


O2,

2, , 

,a

,

I4,

2, , 

,a

,

O4,

2, , 

,a

,

I5,

2, , 

,a

,

O5,

2,2, 

, b

,

I4,

2,2, 

, b

,

O4,

2,2,2

, c

,

I5,

2,2,2

, c

,

O5,

2,2,2

, c

,

I4,

2,2,2

, c

AI1 =

 , , 

AO1 =

 , , 

AI2 =

 , , 

AO2 =

2, , 

AI3 =

 , , 

AO3 =

 , , 

AI4 = 2,2,2
AO4 = 2,2,2
AI5 = 2,2,2
AO5 = 2,2,2
AI6 =

2, , 

AO6 =

2, , 


O3,

 ,2, 

,b

,

I4,

2,⊥, 

,b

,

O4,

2,2,⊥

, c

,

I5,

2,2,⊥

, c

,

O5,

2,2,⊥

, c

,

I4,

2,⊥,⊥

,c

,
Assignment A
ifp ρ2 w.r.t. A
Assignment A
ifp ρ3 w.r.t. A
resulting from A,ρ1
resulting from A,ρ2
Data ﬂow value xu is

x a
u ,x b
u ,x c
u

for variables a,b,c.
FIGURE 5.11
Some information ﬂow paths in copy constant propagation for CFG in Figure 5.10
© 2009 by Taylor & Francis Group, LLC

Complexity of Iterative Data Flow Analysis
183
AI1 =

 , , , 

AO1 =

 , , , 

AI2 =

 , , , 

AO2 =

 , , , 

AI3 =

 , , , 

AO3 =

 , , , 

AI4 =

 , , , 

AO4 =

 , , , 

AI5 =

 , , , 

AO5 =

 , , , 

AI6 =

 , , , 

AO6 =

 , , , 


O1,

 , , ,3

, d

,

I2,

 , , ,3

, d

,

O2,

 , , ,3

, d

,

I3,

 , , ,3

, d

,

O3,

 , , ,3

, d

,

I4,

 , , ,3

, d

,

O4,

 , , ,3

, d

,

I5,

 , , ,3

, d

,

O5,

 , , ,6

, d

,

I2,

 , , ,⊥

,d

,

O2,

 , , ,⊥

,d

,

I3,

 , , ,⊥

,d

,

O3,

 , , ,⊥

,d

,

I4,

 , , ,⊥

,d

,

O4,

 , ,⊥,⊥

,c

,

I5,

 , ,⊥,⊥

,c

,

O5,

 , ,⊥,⊥

,c

,

I2,

 , ,⊥,⊥

,c

,

O2,

 , ,⊥,⊥

,c

,

I3,

 , ,⊥,⊥

,c

,

O3,

 ,⊥,⊥,⊥

,b

,

I4,

 ,⊥,⊥,⊥

,b

,

O4,

 ,⊥,⊥,⊥

,b

,

I5,

 ,⊥,⊥,⊥

,b

,

O5,

 ,⊥,⊥,⊥

,b

,

I2,

 ,⊥,⊥,⊥

,b

,

O2,

⊥,⊥,⊥,⊥

,a

,

I3,

⊥,⊥,⊥,⊥

,a

,

O3,

⊥,⊥,⊥,⊥

,a

,

I4,

⊥,⊥,⊥,⊥

,a

,

O4,

⊥,⊥,⊥,⊥

,a

,

I5,

⊥,⊥,⊥,⊥

,a

,

O5,

⊥,⊥,⊥,⊥

,a

,

I2,

⊥,⊥,⊥,⊥

,a

(a) Given assignment A
(b) An ifp ρ w.r.t. A. width(ρ) = 4
FIGURE 5.12
A width deﬁning ifp in constant propagation problem in Figure 5.5 on page 164.
Example 5.8
Recall that round-robin method requires 6 iterations for Constant Propagation
example for CFG with d = 1 in Figure 5.5 on page 164. This can be explained
using the ifp shown in Figure 5.12.
In this ifp, the non-conforming edge
Exit(5) →Entry(2) appears 4 times, which makes width of this ifp 4.
© 2009 by Taylor & Francis Group, LLC

184
Data Flow Analysis: Theory and Practice
5.4
Summary and Concluding Remarks
This chapter is the culmination of generalizations across a large class of data ﬂow
frameworks. The ﬁrst generalization was to deﬁne bit vector frameworks in terms
of data ﬂow equations using Gen-Kill components. A subsequent generalization
extended the Gen-Kill components to general frameworks. The next step provided a
uniform model of ﬂow functions in terms of its constituent pefs.
This chapter has shown that such a modeling allows a clean extension of com-
plexity measures for bit vector frameworks to the complexity measures for general
frameworks. In particular, the underlying theme of information ﬂow paths and the
concept of width which governs the number of iterations of round-robin iterative
analysis remains same. The only change is that the concept of the constituent points
in an information ﬂow path gets extended progressively with a transition from bit
vector framework to fast frameworks and then to non-separable frameworks.
5.5
Bibliographic Notes
For a long time, the complexity measures in most of the classical literature were
restricted to unidirectional data ﬂow problems. This has also been reﬂected in Chap-
ter 3 where the discussion is limited to unidirectional ﬂows. Complexity of bidirec-
tional problems like PRE [74] was ﬁrst explained by Khedker and Dhamdhere [60]
which also introduced the notion of information ﬂow path in context of bit vec-
tor frameworks. This formed a generalized theory of bit vector data ﬂow analy-
ses [60, 59, 30] which provided a uniform treatment to unidirectional as well as bidi-
rectional data ﬂow frameworks. However it was limited to bit vector frameworks.
This limitation was removed by the work by B. Karkare [53] which forms the basis
of our discussion in this chapter.
We have restricted ourselves to iterative methods of data ﬂow analysis. This is
because both round-robin and work list variants of iterative data ﬂow analysis are
general methods and can be used for all data ﬂow frameworks. For bit vector frame-
works, a much larger class of methods exists. Among them, elimination methods
use the structural properties of CFGs and have been widely studied. The pioneer-
ing works in elimination methods of data ﬂow analysis are by Allen and Cocke [7],
Graham and Wegman [37] and Tarjan [98]. Ryder and Paull [86] describe these
methods in details. A much wider range of solution methods have been described by
Hecht [44] and Kennedy [57].
© 2009 by Taylor & Francis Group, LLC

6
Single Static Assignment Form as
Intermediate Representation
In this chapter we present an intermediate form of programs called single static as-
signment (SSA) form that is useful for many optimizations. Because of the sparse-
ness of def-use chains in the representation, optimizations based on SSA form can
be performed eﬃciently.
6.1
Introduction
The result of many data ﬂow analyses can be represented by superimposing struc-
tures called def-use or use-def chains on the CFG of a program. As mentioned in
Section 2.3.3, a def-use chain associates with each deﬁnition a list of statements that
are reached by the deﬁnition and contain uses of the variable being deﬁned. Sim-
ilarly, a use-def chain associates with each use of a variable, a list of statements
containing deﬁnitions of the variable that reach the use. Def-use chains can be com-
puted by extending liveness analysis. In this extension, the data ﬂow information is a
set of tuples (x,n) where x ∈Var and n is a basic block, where it is assumed that each
statement forms a basic block by itself. The CFG is traversed backwards as in live-
ness analysis. The use of a variable x in a statement at n generates the tuple (x,n). If
a statement at n contains a deﬁnition of the variable x, then, for each (x,n) in Outn,
n is chained to n. (x,n) is subsequently killed by the statement at n. Similarly,
use-def chains can be found by a minor modiﬁcation of reaching deﬁnitions analy-
sis. Optimizations like dead code elimination make use of def-use chains whereas
constant propagation and loop-invariant detection make use of use-def chains. Fig-
ure 6.1 shows an example program and its CFG on which the def-use chains have
been superimposed. A def-use chain is concretely represented by a set of def-use
edges connecting the deﬁnition with its uses.
Def-use chains are used to propagate data ﬂow information. A def-use edge may
bypass a path through a number of control ﬂow edges and directly connect a deﬁ-
nition with its use. Clearly, the time taken for performing an optimization based on
def-use or use-def chains will depend on the number of def-use edges in the graph.
Any optimization over the example program shown in Figure 6.1 will have to repeat-
edly iterate over the 12 chains, propagating a data ﬂow value from a deﬁnition to its
185
© 2009 by Taylor & Francis Group, LLC

186
Data Flow Analysis: Theory and Practice
switch(machineId)
{
case1:
st = initState1;
break;
case2:
st = initState2;
break;
case3:
st = initState3;
}
while (1)
{
sym = getsym();
if(isAlpha(sym))
st = next[st,sym];
elseif(sym == ’\n’)
{ printf("%d\n", st);
nextline();
}
else
{ printf("%d\n", st);
break;
}
}
switch(machineId)
st = initState1
st = initState2
st = initState3
sym = getsym()
if(isAlpha(sym))
st = next[st,sym]
elseif(sym == ’\ n’)
printf(”%d\n”, st)
nextline()
printf(”%d\n”, st)
st = initState
FIGURE 6.1
Example of def-use chains.
corresponding uses in each iteration. The number of def-use edges tend to proliferate
when each of several deﬁnitions of a variable reach several uses of the same variable
through a join node in the CFG. As an example, m deﬁnitions reaching each of n
uses result in m×n def-use chains.
6.1.1
An Overview of SSA
A program in SSA form reduces the number of def-use (or use-def) chains by intro-
ducing a separate variable version for each deﬁnition of the same variable reaching
a join node. Thus st1, st2, st3 and st4 are four diﬀerent versions of the same vari-
able st. Each version corresponds to a deﬁnition of state. The values carried by the
four versions are transferred to a new version, st5, at a join node. This is done using
a notational mechanism called a φ-instruction. A φ-instruction is a special kind of
assignment whose right hand side consists of a φ-function applied to the incoming
variable versions (st1, st2, st3 and st4 for the example), and the left hand side consists
of the new version (st5). The variable st5 reaches each of several uses in the original
program. These uses are also modiﬁed to receive their values from st5. Thus there
are m def-use chains, one for each sti reaching the φ-instruction, and n def-use chains
corresponding to the deﬁnition involving the φ-instruction reaching each of n uses,
© 2009 by Taylor & Francis Group, LLC

Single Static Assignment Form as Intermediate Representation
187
switch(machineId)
st1 = initState1
st2 = initState2
st3 = initState3
st5 = φ(st1,st2,st3,st4)
sym = getsym()
if(isAlpha(sym))
st4 = next[st5,sym]
elseif(sym == ’\ n’)
printf(“%d\n”, st5)
nextline()
printf(“%d\n”, st5)
st = initState
FIGURE 6.2
The earlier example in SSA form.
making up a total of m+n chains. Figure 6.2 shows the earlier program in SSA form.
The reduction in the number of def-use chains can be clearly observed.
The variables involved in φ-instructions are called φ-variables. The variables on
the right hand side of a φ-instruction are called the arguments of the φ-instruction and
the variable on the left hand side is called the result. Since the transformation to SSA
form includes insertion of φ-instructions, it is important to describe the semantics
of the φ-instructions. Consider a basic block with k predecessors. Then the block
could have several φ-instructions, all placed at the beginning of the block. These are
denoted as:
y1 = φ(x11, x12,..., x1k)
y2 = φ(x21, x12,..., x2k)
...
yn = φ(xn1, xn2,..., xnk)
During execution, if the block containing these instructions is reached along prede-
cessor edge j, then the eﬀect of these instructions is that of simultaneously executing
the assignment statements y1 = x1 j, y2 = x2 j, ..., yn = xn j along the edge from the jth
© 2009 by Taylor & Francis Group, LLC

188
Data Flow Analysis: Theory and Practice
a = b
a = b
a = b
y1 = φ(x11,..., x1k)
...
yn = φ(xn1,..., xnk)
a = b
a = b
a = b
y1 = x12||
...
x12||
yn = xn2
y1 = x11||
...
x11||
yn = xn1
y1 = x1k||
...
x1k||
yn = xnk
y1 = x1k||
yn = xnk
FIGURE 6.3
Semantics of φ-instruction.
predecessor block to the block containing the φ-instructions. This is shown in Fig-
ure 6.3. A simultaneous execution of y1 = e1 and y2 = e2, denoted y1 = e1 || y1 = e2,
ﬁrst evaluates the expressions e1 and e2 and then assigns the resulting values to y1
and y2 respectively. As we shall see, the semantics becomes important when we
transform the program into and out of SSA form.
6.1.2
Beneﬁts of SSA Representation
Transformation of a program to SSA form results in a sparser representation of def-
use chains. The beneﬁt that results due to this sparsity is an improvement in time to
perform the optimization. To see this, consider a generic work list based algorithm
that uses def-use edges. Such an algorithm will propagate data ﬂow values from
the deﬁnition of a variable to its uses. Therefore, we can associate data ﬂow values
with the deﬁnition end and the use end of each def-use edge. At any point of time,
the work list will hold def-use edges for which the data ﬂow value has yet to be
propagated from the deﬁnition to the use. After this is done, the propagated value is
used to compute the value of the deﬁnition that depends on this use and, provided
this is a new value, all def-use edges which have this deﬁnition as the argument are
put on the work list.
The algorithm takes time proportional to the product of the total number of def-use
edges and the number of times each edge can be inserted in the work list. The number
of times each def-use edge can be put on the work list is the same as the maximum
number of changes in the data ﬂow value, and this is the same as the height of the data
ﬂow lattice. Thus, if we ﬁx the data ﬂow lattice, the time required for the analysis
depends on the number of def-use edges in the program representation. As we have
argued earlier, the number of def-use edges in a program in SSA form is smaller than
the program from which it was constructed, thus reducing the time required for the
analysis.
The second beneﬁt is that certain analyses or optimizations become easier due
© 2009 by Taylor & Francis Group, LLC

Single Static Assignment Form as Intermediate Representation
189
to the nature of the SSA form itself. In a SSA form program, there is exactly one
deﬁnition reaching each use of a variable. To see a use of this property, consider
a method for detection of induction variables in a program. Figure 6.4 shows a
program along with its SSA form. The def-use edges from the deﬁnitions to the
uses of variables are shown explicitly. To discover that i is an induction variable
of the original program, we note that statements i3 = φ(i1,i2) and i2 = i3 +2 form a
strongly connected region (SCR) involving (versions of) the variable i in the SSA
form program.
The initial value of the variable is supplied by the statement i1 = 0
and the statements constituting the SCR increase i by a constant in each iteration.
In addition, since the SCR passes through the φ-instruction, i is identiﬁed as an
induction variable of the outer loop. This information is not readily available in the
original program with def-use chains. By a similar reasoning, j is detected to be
an induction variable of the inner loop of the original program. Its increment, i, is
detected to be a loop invariant of the inner loop because the deﬁnition of i2 reaching
the statement j2 = j3 +i2 in the SSA form program is outside the SCR formed by the
statements j3 = φ(j1, j2) and j2 = j3 +i2.
A larger example of use of SSA form will be presented later in the chapter when
we discuss a method for register allocation that exploits the special properties of SSA
form programs.
6.2
Construction of SSA Form Programs
As in reaching deﬁnitions analysis (Section 2.3.3), we assume that the node Start
contains an assignment of the special value undef to every variable. Thus along any
path in the CFG from Start to the use of a variable, there is at least one deﬁnition of
the variable reaching the use. Programs which satisfy this property are called strict
programs.
As mentioned earlier, the φ-instructions should be inserted where more than one
deﬁnition coming along diﬀerent paths converge. We ﬁrst formalize the notion of
converging paths.
DEFINITION 6.1
Let ρ1 = (n1,n2,...,o) and ρ2 = (m1,m2,...,o) be non-
null paths. ρ1 and ρ2 are said to converge, if:
1. The start nodes of ρ1 and ρ2 are diﬀerent, i.e., n1  m1.
2. The two paths are disjoint except for the node o.
Note that the common node o could occur in more than one position in the two
paths. An interesting example of converging paths for the CFG in Figure 6.5 is
(n1,n5,n7) and (n7,n9,n10,n7). If a variable is deﬁned in nodes 1 and 7 of the CFG,
© 2009 by Taylor & Francis Group, LLC

190
Data Flow Analysis: Theory and Practice
0
i = 0;
1
while (...)
2
{
i = i + 2;
3
j = i;
4
while (...)
5
j = j + i;
6
}
i1 = 0
i3 = φ(i1,i2)
i2 = i3 +2
j1 = i2
j3 = φ(j1, j2)
j2 = j3 +i2
i1 = 0
FIGURE 6.4
Detecting induction variables using SSA form.
then there must be a φ-instruction for this variable at the entry of 7. The exam-
ple shows why the end node is allowed to occur in more than one position in the
paths—the converging paths may include loops∗. The pair of paths (n5,n7,n8,n10)
and (n6,n7,n9,n10) is an example of paths that are non-converging.
We now specify the properties of a valid transformation of a program to SSA form.
The algorithm that we describe later will be proved to be correct with respect to this
speciﬁcation.
DEFINITION 6.2
The transformation of a program to another is a valid
SSA-transformation, if the following two conditions are satisﬁed:
1. Correctness of form: Each variable mentioned in the transformed program
must have exactly one deﬁnition.
2. Semantic invariance: Consider an execution path leading to a use of a
variable x in the original program and a corresponding execution path
leading to the variable version xi in the program in SSA form. Then,
under the execution semantics of φ-instructions described earlier, the
two variables x and xi must have the same value.
Unless stated otherwise, by the phrase ‘a program in SSA form’ we shall mean a
program that has been obtained by a valid SSA-transformation of a strict program.
A program in SSA form is minimal, if it results from a transformation satisfying
the properties listed above and has a minimum number of φ-instructions. A program
in SSA form is pruned, if it has the added restriction that a φ-instruction is inserted
only if the result variable of the instruction is used later along some path.
∗Observe that (n1,n5,n7) and (n6,n7,n9,n10,n7) are also converging paths by the deﬁnition. This is clearly
not necessary since their role is subsumed by the pair of paths (n1,n5,n7) and (n6,n7).
© 2009 by Taylor & Francis Group, LLC

Single Static Assignment Form as Intermediate Representation
191
Entry x = ... Entry
n1 x = ... 1
n2 x = ... 2
n3 x = ... 4
n4 x = ... 4
n6 x = ... 6
n5 x = ... 5
n7 x = ... 7
n8 x = ... 8
n9 x = ... 9
n10 x = ... 10
Exit x = ... Exit
Entry x = ... Entry
n1 x = ... 1
n2 x = ... 2
n3 x = ... 4
n4 x = ... 4
n6 x = ... 6
n5 x = ... 5
n7 x = ... 7
n8 x = ... 8
n9 x = ... 9
n10 x = ... 10
Exit x = ... Exit
(a)
(b)
FIGURE 6.5
(a)A CFG and (b) its dominator tree.
To distinguish between the predecessor and successor relation in the CFG and the
same relation in the dominator tree, we use the terms ancestor and descendant in the
latter case. An immediate descendant will be called a child.
6.2.1
Dominance Frontier
The key idea behind insertion of φ-instructions is that of dominance frontier. To
develop this idea, we ﬁrst deﬁne the concept of dominance in a graph. Recall the
deﬁnition of dominance from Section 3.1.
DEFINITION 6.3
Let n and m be nodes in the CFG. The node n is said
to dominate m, denoted n  m, if every path from Start to m passes through n.
We also need a notion of dominance that is not reﬂexive.
DEFINITION 6.4
If nm and n  m, then we say that n strictly dominates
m and denote this as n5m. Further, the closest strict dominator of a node n
© 2009 by Taylor & Francis Group, LLC

192
Data Flow Analysis: Theory and Practice
n1
x =
n1
n2 x =
n2n3 x =
n3
n4 x =
n4
n5 x =
n5
n6 x =
n6
n8 x =
n8
n7 x =
n7
n9 x =
n9
n10 x =
n10
n11 x =
n11
FIGURE 6.6
Idea behind φ insertion through dominance frontier.
is called the immediate dominator of n and is denoted as idom(n).
We use the notation n 5/ m to mean n does not strictly dominate m. Figure 6.5 shows
a CFG and its dominator tree in which the edges represent immediate dominance.
We shall sometimes consider dominance to be a relation between program points
instead of nodes.
OBSERVATION 6.1 If the nodes n and m both dominate a node o then either nm
or mn.
Consider Figure 6.6 in which the node n1 contains a deﬁnition of the variable x.
The node n6 is dominated by n1 and so are all the shaded nodes in the ﬁgure. Each
shaded node will have the property of a single deﬁnition of x reaching it and will thus
not require a φ-instruction for x. Now consider the node n9 which is an immediate
successor of n6 and is not dominated by n1. This node needs a φ-instruction because,
apart from the deﬁnition in n1, some other deﬁnition, possibly the one that is assumed
to initialize the value of x to undef at Start, will reach n9. Nodes such as n9, and
n10 are said to be in the dominance frontier of n1 and need a φ-instruction for the
variable x. We shall now formalize this idea.
A straightforward translation of the idea represented by Figure 6.6 gives a ﬁrst
deﬁnition of dominance frontier. The dominance frontier of a node n, denoted df (n),
is given as
df (n) = {m | ∃p ∈pred(m), (n p and n 5/ m)}
© 2009 by Taylor & Francis Group, LLC

Single Static Assignment Form as Intermediate Representation
193
By this deﬁnition, a loop header will be included in its own dominance fron-
tier. This is reasonable since a variable in the loop header may have two reaching
deﬁnitions—one from inside the loop, the other from outside. As an example, if n1
is a loop header in Figure 6.6, a program point in n1 before the deﬁnition of x will
have more than one reaching deﬁnition—one reaching from outside the loop and the
other from the deﬁnition in n1 itself. In such a situation there will be a φ-instruction
at the beginning of n1.
A direct implementation of the above deﬁnition will ﬁnd df (n) by considering
each node m dominated by n and checking whether it has an immediate successor
in the CFG that is not strictly dominated by n. The problem with this approach is
that it ﬁnds the dominance frontier of each node independently of the dominance
frontier of other nodes. A more eﬃcient algorithm that exploits the relation between
dominance frontiers of diﬀerent nodes is based on the following observations:
1. Consider Figure 6.6 as an example. Nodes that are immediate successors of
n1 and not strictly dominated by n1 are in df (n1). An example of such a node
is n2. We call such nodes as dfbase(n1) as these nodes are included in what can
be considered as the base step of an inductive deﬁnition for df .
dfbase(n) = {m ∈succ(n) | n 5/ m}
2. We shall now relate the dominance frontier of n1 in Figure 6.6 to the domi-
nance frontier of its children. Consider n5 as an example of a child of n1. The
node n9, which is in df (n5), is also in df (n1). However, n8, which is also in
df (n5) is not in df (n1). The reason is that while n5 does not dominate n8, its
immediate dominator n1 dominates n8. We call this component of df as dfind,
the inductive step of the deﬁnition of df .
dfind(n) =

m∈children(n)
{p ∈df (m) | idom(m) 5/ p}
Combining the two:
df (n) = dfbase(n)∪dfind(n)
We can reformulate dfbase and dfind so that they use the easily checkable  relation
instead of 5/ . If m is a successor of n then the condition n5m is exactly the same as
n = idom(m). Thus
dfbase(n) = {m ∈succ(n) | n  idom(m)}
Similarly, if m is a child of n and p is in df (m), then the condition n5 p is exactly
the same as n = idom(p). To see this, we ﬁrst observe that any strict dominator of p
is also a strict dominator of m. Assume to the contrary that o is a strict dominator of
p and either o is the same as m or o is unrelated to m in the dominance relationship.
In the ﬁrst case o cannot dominate p, because p is in df (o). In the second, if o is
© 2009 by Taylor & Francis Group, LLC

194
Data Flow Analysis: Theory and Practice
Input: A CFG with the dominance frontier for each node.
Output: The dominance frontier of each node n in the CFG computed in a variable
DFn.
Algorithm:
0
for each n in a bottom up traversal of the dominator tree do
1
{
DFn = ∅
2
for each m ∈succ(n) do
/* Calculate dfbase */
3
if idom(m)  n then DFn = DFn ∪{m};
4
for each m ∈children(n) do
/* Calculate dfind */
5
for each p ∈DFm do
6
if idom(p)  n then DFn = DFn ∪{p};
7
}
FIGURE 6.7
The algorithm for dominance frontier.
unrelated to m, o cannot dominate p since there is an alternate path from Start to p
through m which does not pass through o. Thus we have a contradiction.
Now since n is the closest ancestor of m that strictly dominates p, we must have
n = idom(p). Thus we can rewrite dfind as
dfind(n) =

m∈children(n)
{p ∈df (m) | n  idom(p)}
The algorithm in Figure 6.7 computes the dominance frontier using the formula-
tion presented above. The table in Figure 6.8 gives dfbase and dfind for the nodes in
the CFG in Figure 6.5.
Let E and N be the number of edges and nodes in the CFG. To calculate dfbase,
the algorithm clearly visits each edge once, so its complexity is O(E). Let |df(n)|
denote the size of dominance frontier of the node n. Then the complexity of the part
that calculates dfind is bounded by O(Σn|df (n)|). This is O(N2) for arbitrary CFGs,
which gives an overall complexity of O(E + N2). However, it can be shown that for
CFGs programs composed of assignments, if-then-else and while-dos, |df (n)| is a
constant. For such CFGs, both O(Σn|df (n)|) and E are O(N). Thus the complexity of
the algorithm is also O(N).
6.2.2
Placement of φ-instructions
The algorithm for placing φ-instructions is shown in Figure 6.9. It considers each
variable in turn and maintains a work list for nodes that are yet to be examined.
For every variable it starts by inserting the nodes that contain an assignment to the
variable in the work list. The dominance frontier of each node in the work list is
examined. φ-instructions are inserted in the nodes forming the dominance frontier,
and these nodes are in turn inserted in the work list.
The algorithm, maintains the following variables.
© 2009 by Taylor & Francis Group, LLC

Single Static Assignment Form as Intermediate Representation
195
Node
Exit
10
9
8
7
6
5
4
3
2
1
Entry
dfbase
∅
7
8,10
10
∅
7,2
7,1
6
6
∅
∅
∅
dfind
∅
∅
∅
∅
7
∅
∅
∅
∅
7,2
7,1
∅
FIGURE 6.8
dfbase and dfind for the CFG in Figure 6.5 on page 191.
• inWorklist : If inWorklistn is x, it means that the node n has been inserted in
the work list in connection with the variable x.
• inserted : If inserted n is x, it means that a φ-instruction has been inserted in
node n for the variable x.
• assign : assign x is the set of nodes containing an assignment to the variable x
in the original program.
It is possible for the following situation to arise: A node n has been put in the work
list in connection with a variable but a φ-instruction for the variable has not yet been
inserted in n. This could happen, for instance, when the node being examined is a
loop header containing an assignment to a variable. Thus insertedm and inWorklistm
could have diﬀerent values when entering the body of the for loop in line 13 and
therefore checking the condition inWorklistm  x in line 16 is not redundant.
For the CFG in Figure 6.5 on page 191, a φ-instruction for the variable x is inserted
at node 1 since 1 contains a deﬁnition of x and is in its own dominance frontier.
Similarly a φ-instruction is inserted in 6 which is in df (4) and 2 which is in df (6).
φ-instructions are also inserted in 7 and 10.
From a single node, the notion of dominance frontier can be generalized to a set
of nodes in the following way:
df (S ) =

x∈S
df (x)
It is easy to see that df is monotonic, i.e., S 1 ⊆S 2 implies df (S 1) ⊆df (S 2).
If S x is the set of nodes containing assignments to the variable x, then the φ-
instructions placed by the φ-placement algorithm is given by the iterated dominance
frontier of S x denoted as idf +(S x). This is deﬁned as the limit of the increasing
sequence idf i(S ):
idf 1(S ) = df (S )
(6.1)
idf i+1(S ) = df (S ∪idf i(S ))
(6.2)
Let Aorig(n) and Atrans(n) represent the number of assignments in node n in the
original and the transformed program. Observe that nodes are put in the work list
O(Atrans) number of times, and for each node n that has been put in the work list
O(|df (n)|) nodes are examined. Let avgcost represent this work averaged over all the
assignments in the transformed program. Thus
© 2009 by Taylor & Francis Group, LLC

196
Data Flow Analysis: Theory and Practice
Input: A CFG with the dominance frontier for each node.
Output: The CFG with the φ-instructions inserted but without variable renaming.
Algorithm:
0
worklist = ∅
1
for each node n do
2
{
inserted n = x0
/* x0 should not occur in the program */
3
inWorklistn = x0
4
}
5
for each variable x do
6
for each n ∈assign(x) do
7
{
inWorklistn = x
8
worklist = worklist ∪{n}
9
}
10
while worklist  ∅do
11
{
remove a node n from worklist
12
for each m ∈dfn do
13
if inserted m  x then
14
{
place a φ-instruction for x at m
15
inserted m = x
16
if inWorklistm  x then
17
{
inWorklistm = x
18
worklist = worklist ∪{m}
19
}
20
}
21
}
FIGURE 6.9
The algorithm for φ placement.
avgcost =

n
(Atrans(n)×|df(n)|)/Σn(Atrans(n))
Then the cost for computation of the iterated dominance frontier is avgcost ×
Σn(Atrans(n)). However, for CFGs consisting of assignments, if-then-else and while-
dos, avgcost is a constant and the complexity reduces to O(Σn(Atrans(n))).
6.2.3
Renaming of Variables
The algorithm for renaming is given in Figure 6.11. In order to generate new versions
of the variable, the algorithm maintains a counter for each variable. To rename the
use of a variable in an assignment, the algorithm needs to keep track of the deﬁnition
that reaches the use. The algorithm maintains a array of stacks (called stacks ), one
for each variable for this purpose. As the algorithm traverses the program, the version
of x that reaches a program point is given by the value of top(stacks x). In addition,
to rename the arguments of a φ-function, we need to know the predecessor number
© 2009 by Taylor & Francis Group, LLC

Single Static Assignment Form as Intermediate Representation
197
n1
y = 2
x = 3
n1
n2
x = x+1
n2 n3
x = x+6
n3
n4
x = φ(x, x)
z = y∗x
n4
1
1
1
2
n1
y1 = 2
x1 = 3
n1
n2
x2 = x1 +1
n2n3
x4 = x1 +6
n3
n4
x3 = φ(x2, x4)
z1 = y1 ∗x3
n4
1
1
1
2
FIGURE 6.10
CFG before and after renaming variables.
of a node with respect to its successor†. This is given by predNumber(m,n), where
n is a predecessor of m.
Consider a call to rename(n). If a variable x is used by an ordinary assignment,
it is renamed to the version xi given by top(stacks x). The deﬁnition of a variable
y, whether deﬁned by a ordinary assignment or a φ-instruction, is renamed to a new
version yj. The new version number j is inserted in the stack for y. The call to
rename(n) also renames the arguments of the φ-function in each successor m of
n. The reason why this is done during a call to rename(n) is the following. To
rewrite the ith argument of a φ-function, we need to know the variable version whose
deﬁnition reaches the end of the ith predecessor. If x is the current variable being
renamed, it is at this point of time that we know that top(stacks x) is the version of
x reaching the end of predNumber(m,n). This information is used for renaming.
Thus the uses on the right hand side of a φ-instruction and an ordinary assignment
are renamed during diﬀerent calls to rename.
Example 6.1
We illustrate the algorithm for renaming variables through the example in
Figure 6.10.
The labels on the edges number the predecessors of a node.
Thus n2 and n3 are the ﬁrst and the second predecessors of n4.
• The algorithm does a reverse postorder traversal of the dominator tree
starting with node n1. The variables on the left hand side of the assign-
ments are renamed to y1 and x1. Since none of n1’s successors contain
a φ-instruction, the children of n1 are processed next. The values of
top(stacks x) and top(stacksy) are both 1 at this time.
†The predecessors of a node are assumed to be ordered.
© 2009 by Taylor & Francis Group, LLC

198
Data Flow Analysis: Theory and Practice
• Assume that n2 is the node that is selected next. The variable x on
the right hand side of n2 is renamed to the variable version whose def-
inition reaches this use. This is indicated by stacks x as being x1. The
variable on the left hand side is changed to a new variable version x2
and top(stacks x) is changed to 2. Since n2 has a successor which has a
φ-instruction, the ﬁrst variable on the right hand side of this assignment
is renamed to x2. After n2 is processed, stacks x is popped.
• Since n4 is also a child of n1, assume it is picked next. The left hand side
of the φ-instruction is renamed to a new variable x3. Since the values
of top(stacks x) and top(stacksy) are 3 and 1, the assignment following
the φ-instruction is renamed to z1 = y1 ∗x3. The stacks for x and z are
popped.
• Finally, the block n3 is rewritten as shown in the ﬁgure. Since n4 is a
successor of n3 and this has a φ-instruction, the second argument of the
φ-instruction is renamed to x4, the version of x reaching this program
point.
Let Mtrans(n) denote the number of mentions (uses and deﬁnitions) of variables in
the block n of the transformed program. Then the algorithm is linear in total number
of mentions of variables in the entire transformed program, i.e., the complexity is
O(Σn(Mtrans(n))).
6.2.4
Correctness of the Algorithm
We now show that the algorithm to calculate the dominance frontier, the φ-placement
algorithm and the renaming algorithm together satisfy the speciﬁcation of a valid
transformation to SSA form. To do this we ﬁrst need to prove the following important
property regarding placement of φ-instructions in the transformed program: If two
non-null paths which begin with the deﬁnitions of diﬀerent versions of the same
variable converge at a node n, then there is a φ-instruction for the variable at n. Note
that the deﬁnitions at the beginning of the converging paths could themselves involve
φ-instructions.
DEFINITION 6.5
Given a set of nodes S , the join of S is deﬁned as:
join(S ) = {n | ∃converging paths m1
+→n and m2
+→n, m1,m2 ∈S }
(6.3)
The iterated join of a set of nodes S , denoted ij+(S ), is deﬁned as the limit
of the increasing sequence iji(S ):
ij1(S ) = join(S )
(6.4)
iji+1(S ) = join(S ∪iji(S ))
(6.5)
© 2009 by Taylor & Francis Group, LLC

Single Static Assignment Form as Intermediate Representation
199
Input: A CFG with φ-instruction inserted.
Output: The same CFG with variables renamed.
Algorithm:
0
for each variable x do
1
{
counterx = 0; stacks x = emptyStack
2
}
3
rename(Start)
4
5
function rename(n)
6
{
for each assignment a in n do
7
{
if a is an ordinary assignment then
8
for each variable x in RHS(a) do
9
replace x by xi where i = top(stacks x)
10
let y be LHS(a) in
11
{
i = countery
12
replace y by new variable yi in y = e
13
push i onto stacksy
14
countery = i+1
15
}
16
}
17
for each m ∈succ(n) do
18
{
j = predNumber(m,n)
19
for each φ-instruction a in m do
20
replace j-th operand x in RHS(a) by xi, where i = top(stacks x)
21
}
22
for each m ∈children(n) do rename(n)
23
for each assignment a in n do
24
pop(stacksz), where z is the original variable of LHS(a)
25
}
FIGURE 6.11
Algorithm for renaming.
The property regarding placement of φ-instructions can now be recast as follows:
If S x is the set of deﬁnition involving a variable x in the original program, then there
must be a φ-instruction for x in every node in ij +(S x). To prove this, we need a result
relating the end node of a non-null path with the iterated dominance frontier of the
start node of the path.
LEMMA 6.1
Consider a path ρ : n
+→m. We can ﬁnd a node n on ρ such that n ∈{n} ∪
idf +({n}) and n dominates m. Further, if n does not dominate each node in ρ,
n ∈idf +({n}).
© 2009 by Taylor & Francis Group, LLC

200
Data Flow Analysis: Theory and Practice
n
m
n
o
ρ
1
ρ
1
q
m
= n = 0 n = n = 0
o
ρ
1
q
(a)
(b)
FIGURE 6.12
Figure illustrating Lemma 6.2.
PROOF
Clearly, if n dominates each node in ρ, then n is the same as
n. Now assume that there are nodes in ρ that are not dominated by n. Let
the path ρ be (n = n0,n1,...,nk = m). Since n does not dominate all nodes in ρ,
there will be some nodes in ρ that are in idf +({n}). Let n j be the node with
the largest index j that is in idf +({n}). Claim that n j is the required n.
Suppose n j does not dominate m. Then consider the closest node ni on ρ
that is not dominated by n j. All nodes in between n j and ni+1 are dominated
by n j. Thus ni ∈df ({n j}) and since n j ∈idf +({n}), ni ∈idf +({n}). Thus n j is not
the node with the largest index that is in idf +({n}).
The second lemma shows that a one step join of two nodes is contained in the
union of their iterated dominance frontier.
LEMMA 6.2
Let n and m be two distinct nodes in the CFG. Then join({n,m}) ⊆idf +({n})∪
idf +({m}).
PROOF
Let o ∈join({n,m}). Then there are non-null paths ρ1 : n
+→o and
ρ2 : m
+→o converging at o. From Lemma 6.1, there is a node n on ρ1 and a
node m on ρ2 such that both n and m dominate o. We prove the lemma by
case analysis:
1. n is also on ρ2: The general situation is illustrated by Figure 6.12(a)
where ρ1 is a concatenation of the paths ρ
1 and ρ
1 .
Of course, one
of the path segments ρ
1 and ρ
1 could be null. From the deﬁnition of
convergence, n is the same as o. If n does not dominate all nodes in ρ1
© 2009 by Taylor & Francis Group, LLC

Single Static Assignment Form as Intermediate Representation
201
then Lemma 6.1 gives us n ∈idf +({n}) and we are through. If n dominates
all the nodes in ρ1 then the situation is illustrated by Figure 6.12(b)
obtained by considering ρ
1 to be a null path. Clearly n and n are the
same and n ∈df ({n}). Therefore n ∈idf +({n}).
2. m is also on ρ1: The reasoning for this case is similar to the previous
case.
3. n is not on ρ2 and m is not on ρ1: We shall show that this is not possible.
Since n and m both dominate o, from Observation 6.1, either n m or
m n. The condition m n along with n o implies that every path
from m to o has n on it. In particular, n is on ρ2. Since this is not
the case, the only possibility is n m. By a symmetrical reasoning, we
also have m n. This gives n = m contradicting the initial assumption
that n is not on ρ2.
It is easy to generalize Lemma 6.2 to any ﬁnite set of nodes.
COROLLARY 6.1
For a set of nodes S , join(S ) ⊆idf +(S ).
PROOF
Induction on the number of nodes in S and use of Lemma 6.2.
We now show that dominance frontier is contained in joins.
LEMMA 6.3
Let S be a set of CFG nodes that contains the Start node. Then df (S ) ⊆
join(S ).
PROOF
Let n ∈S and m ∈df ({n}). Then there is a path ρ1 from n to m in
which all the nodes till the predecessor of m are dominated by n. Of course,
m could be the same as n. Further, since m ∈df ({n}), there is a path ρ2 from
Start to m which does not pass through any node in ρ1 except m. Since the
two paths converge at m, m is in join(S ).
We ﬁnally show that iterated dominance frontier computes the same set that is
speciﬁed by iterated joins.
© 2009 by Taylor & Francis Group, LLC

202
Data Flow Analysis: Theory and Practice
LEMMA 6.4
Let S be a set of nodes in a CFG that contains the Start node. Then
ij+(S ) = idf +(S )
PROOF
We ﬁrst prove
ij+(S ) ⊆idf +(S )
by an induction on the iteration index in the deﬁnition of ij +. Speciﬁcally, we
show that for all k,
ijk(S ) ⊆idf +(S )
Then, since ij+(S ) = ijk(S ) for some ﬁnite k, we shall have shown the con-
tainment in the limit.
Basis: Follows from Corollary 6.1.
ij(S ) = join(S ) ⊆idf +(S )
Inductive step:
ij k(S ) = ij(S ∪ijk(S ))
⊆ij(S ∪idf +(S ))
(induction hypothesis, monotonicity of ij)
= join(S ∪idf +(S ))
(deﬁnition of ij)
⊂idf +(S ∪idf +(S ))
(Corollary 6.1)
= idf +(S )
(deﬁnition of idf )
The proof of idf +(S ) ⊆ij+(S ) is very similar.
Let S x represent the set of nodes which contain a deﬁnition of x. By our assump-
tion, Start ∈S x. Therefore ij+(S x) = idf +(S x) for any variable x in the program.
We next prove the ﬁrst condition in the speciﬁcation of valid SSA-transformation
is satisﬁed by the algorithm.
LEMMA 6.5
Each variable in the SSA form program is assigned exactly once.
PROOF
After renaming the deﬁnition of a variable x in the original
program, counterx is incremented before renaming the next deﬁnition.
So
there is at most one assignment to a variable xi. Thus we have to show that
for each variable xi which has a use occurrence in the SSA form program,
there is at least one assignment to xi.
© 2009 by Taylor & Francis Group, LLC

Single Static Assignment Form as Intermediate Representation
203
When the renaming algorithm was renaming the use occurrence of x to xi,
the value of top(stacks x) must have been i. Since top(stacks x) is set to a value
i only after renaming a deﬁnition of x to xi, there is at least one deﬁnition of
xi.
For an assignment statement a, let the notations before(a) and after(a) denote
program points just before and after a. Similarly, if n is a block then after(n) will
denote a program point just after the last statement in n.
Finally we show that the SSA-transformation algorithm maintains semantic invari-
ance. We show that the value of a variable x at a statement in the original program
is the same as the renamed variable at the same statement in the SSA form program.
This requires us to know what the renamed variable at diﬀerent program points are.
The version of x at the program point p in the transformed program is denoted as
version(x, p). This is the version that corresponds to the value of top(stacks x) when
the renaming algorithm is at the program point p during its traversal of the CFG.
Clearly, the following relations hold:
1. If the statement a1 is followed by the statement a2 in a block, then
version(x,after(a1)) = version(x,before(a2))
2. If a is the last statement in a block n, then
version(x,after(a)) = version(x,after(n))
LEMMA 6.6
Let x be a variable and n →m be an edge in the CFG such that m does not
have a φ-instruction for x. Then
version(x,after(n)) = version(x,after(idom(m)))
PROOF
If n = idom(m), there is nothing to be proven. So assume that
n  idom(m). Since n dominates a predecessor of m (namely itself) and does
not strictly dominate m, m ∈df (n). Further, from Lemma 6.4, since m does
not have a φ-instruction for x, n does not have a deﬁnition for x.
Observe that idom(m) strictly dominates n; otherwise there would be a path
from Start to m through n which bypasses idom(m). Consider the node o that
is closest to m, strictly dominates m and deﬁnes x. Then
version(x,after(n)) = version(x,after(idom(m))) = version(x,after(o))
Given a variable x and a control ﬂow path ρ from Start to a program point p, let
val(x,ρ) denote the value of x at program point p when execution takes place along
ρ.
© 2009 by Taylor & Francis Group, LLC

204
Data Flow Analysis: Theory and Practice
n a1 : y = e n
m
a : y = e
a :
m
n a
1 : yj = e n
m
a : y = e
a :
m
n a
1 : yj = e n
m
a : xi = φ(...)
a : y = e
m
(a)
(b)
(c)
FIGURE 6.13
Figure illustrating Lemma 6.7. (a) represents the original program and (b) and (c)
represent the transformed program.
LEMMA 6.7
Consider a path ρ in the original program from Start to an assignment state-
ment a. Let ρ and a be the corresponding path and statement in the SSA-
transformed program. Then, for any variable x,
val(x,ρ) = val(version(x,before(a)),ρ)
PROOF
The proof is by induction on number of statements in the path
ρ. In the proof, a will denote the last statement of ρ and a and ρ will denote
the corresponding statement and path in the transformed program. Further
ρ−{a} will denote the path obtained by deleting the last statement a from ρ.
Basis: Consider the path ρ from Start to the ﬁrst statement a of the successor
node of Start. Clearly for any variable x,
val(x, ρ) = undef
= val(x0, ρ)
= val(version(x, before(a)), ρ)
Inductive step: Now assume that the lemma holds for all paths of length
k −1 or less. Consider a path ρ of length k. We consider the following cases.
1. a is not the ﬁrst statement of the containing block m. Consider the state-
ments before a and a as shown below. e has been obtained from e by
renaming each variable y in e to version(y, before(xi = e))
x = e
xi = e
a :
a :
By the induction hypothesis, for any variable y,
val(y, ρ−{x = e}) = val(version(y, before(xi = e)),ρ −{xi = e})
© 2009 by Taylor & Francis Group, LLC

Single Static Assignment Form as Intermediate Representation
205
Thus from the induction hypothesis and the semantics of the assignment
statement,
val(x, ρ) = val(version(x, before(a)),ρ)
Variables other than x remain unchanged and the statement of the
lemma holds for them because of the induction hypothesis.
2. a is the ﬁrst statement of the containing block m. Assume that the control
ﬂows along the edge n →m.
This situation is shown in Figure 6.13.
The original program is shown in part (a) and the two cases of the
transformed program are shown in parts (b) and (c). Let the paths to
the end of node n be denoted as ρ1 and ρ
1. We ﬁrst show that for any
variable x, val(x, ρ1) = val(version(x, after(n)), ρ
1).
Consider the last
statement of n denoted as a1 and the corresponding statement in the
transformed program a
1. Because of the induction hypothesis, we have
for any variable y,
val(y, ρ1 −{a1}) = val(version(y, before(a
1)), ρ
1 −{a
1})
Once again, using the induction hypothesis and the semantics of assign-
ment, we have:
val(x, ρ1) = val(version(x, after(a
1)), ρ
1)
and therefore
val(x, ρ1) = val(version(x, after(n)), ρ
1)
We now have to show that the values of any variable x and its renamed
version version(x, before(a)) match. For this consider two subcases:
(a) m does not have a φ-instruction for x. Let the path to the imme-
diate dominator of m be ρ
1 . Then:
val(x,ρ) = val(x, ρ1)
= val(version(x, after(n)), ρ
1)
= val(version(x, after(idom(m))), ρ
1 )
(Lemma 6.6)
= val(version(x, before(a)), ρ)
(Renaming algorithm, line 22)
(b) If m has a φ-instruction for x, then:
val(x, ρ) = val(x, ρ1)
= val(version(x, after(n)), ρ
1)
= val(version(x, before(a)), ρ
1)
(Renaming algorithm, lines 18-20)
= val(xi, ρ)
(Semantics of φ-instruction)
= val(version(x, before(a)), ρ)
(Renaming algorithm, line 8-9)
© 2009 by Taylor & Francis Group, LLC

206
Data Flow Analysis: Theory and Practice
The following theorem ties the previous results into an statement of correctness of
the entire algorithm.
THEOREM 6.1
The algorithms for φ-placement and renaming together constitute a valid SSA-
transformation.
PROOF
Follows from Lemmas 6.5 and 6.7.
We ﬁnally prove a property about programs in SSA form that will be used in
later sections. Let the program point associated with the deﬁnition of a variable
x be represented as def(x). This is the point just before the statement that has a
deﬁnition of x, where the deﬁning statement may also be a φ-instruction. Program
points associated with the uses of a variable x are denoted as use(x), and are deﬁned
as follows:
DEFINITION 6.6
A program point p is in use(x) iﬀ
1. The statement just after p is an ordinary assignment (not a φ-instruction)
and x occurs on the right hand side of the assignment.
2. p is after(n), n is the ith predecessor of a block m and m contains a
φ-function with x as the ith argument.
We shall use the term use(x) to refer to any of the points denoted by it.
LEMMA 6.8
(SSA dominance property) Consider the SSA transformation of a strict pro-
gram. For any variable x in the transformed program, def(x) use(x).
PROOF
Because of the semantic invariance property of an SSA transfor-
mation, the SSA of a strict program is also strict. Now assume that there is a
use of a variable that is not dominated by its deﬁnition. By Lemma 6.5, the
variable has a single deﬁnition. If this deﬁnition does not dominate the use,
then the SSA form program is not strict, a contradiction.
The program in SSA form must be ﬁnally converted into executable code. How-
ever no real processor has instructions that can directly capture the semantics of
φ-instructions. Therefore the φ-instructions have to be replaced by code fragments
inserted at appropriate places. The elimination of φ-instructions from a program
in SSA form is called SSA destruction. The intermediate form of the program that
© 2009 by Taylor & Francis Group, LLC

Single Static Assignment Form as Intermediate Representation
207
n1
y = 5
x1 = 3
n1
n2
x2 = y
n2
n3
x3 = φ(x1, x2)
z = x3
n1
n1
y = 5
x1 = 3
n1
n2
x2 = y
n2
n3
x3 = φ(x1,y)
z = x3
n1
(a)
(b)
n1
x = 5
x = 3
n1
n2
x2 = y
n2
n3
z = x
n3
n1
y = 5
x1 = 3
x3 = x1
n1
n2
x3 = y
n2
n3
z = x3
n3
(c)
(d)
FIGURE 6.14
(a) A program in CSSA form. (b) The same program in TSSA form after copy
propagation. (c) Eliminating φ assignments by merging variable versions results in
an incorrect program. (d) A correct program obtained by inserting copy statements.
emerges as the result of applying the SSA construction algorithm discussed earlier
is called canonical SSA (CSSA). This is to distinguish it from the SSA form after
optimizations called transformed SSA (TSSA).
6.3
Destruction of SSA
Before embarking on the issues related to SSA destruction, we deﬁne live ranges
for SSA form programs. Recall that the last use of the ith argument of a φ-function
© 2009 by Taylor & Francis Group, LLC

208
Data Flow Analysis: Theory and Practice
in a block n is considered to be at the end of the ith predecessor block of n. This
means that this argument, say xi, is not live at the entry of the block containing the
φ-instruction. In contrast, the result of a φ-instruction is live at the entry of the block
containing the φ-instruction. This follows from the semantics of φ-instruction which
places a copy statement ... = xi on the edge from the ith predecessor to n.
DEFINITION 6.7
The live range of a variable is its def-use chain. It
includes all the program points between the deﬁnition and each of its uses.
Two live ranges interfere if there is a program point that is common to both
the live ranges.
Because of the single deﬁnition property of SSA form programs, the deﬁnition
associates a live range with a variable. In contrast, for non-SSA form programs, the
live range is deﬁned as the maximal union of intersecting def-use chains‡. Since
def-use chains for SSA form programs do not intersect, the simple deﬁnition given
above suﬃces.
A naive method to convert a SSA form program into an executable form may sim-
ply merge diﬀerent versions of the same variable into one. While merging variable
versions may be a trivial matter for a CSSA—it simply means going back to the orig-
inal program—it can aﬀect correctness in the case of TSSAs. Figure 6.14(a) shows
an example of a program in CSSA form. Part (b) shows copy-propagation applied to
the program. Notice that the deﬁnitions of both y and x1 interfere at the end of the
predecessor block n1. This has important consequences for SSA destruction. Part
(c) shows the result of replacing y, x1 and x3 by a single variable x and eliminating
the φ-instruction. The resulting program is incorrect. However, in keeping with the
semantics of the φ-instruction, we can insert copy statements at the end of blocks n1
and n2. This is shown in part (d). While this is correct for this example, we shall
show later that removing φ-instructions by inserting copy statements may still result
in incorrect programs. Besides, the copy at the end of n1 is obviously redundant.
The subtleties involved in SSA destruction through insertion of copy statements are
illustrated through two well-known problems called the lost-copy problem and the
swap problem.
The lost copy problem is illustrated in Figure 6.15 on the next page. The original
program and its SSA form are shown in Figures 6.15, parts (a) and (b). The program
after copy propagation and dead-code elimination is shown in part (c). Finally, part
(d) shows the program after insertion of copy statements. The resulting program is
incorrect because it prints the value of x in the last iteration instead of the penultimate
iteration.
The reason for the incorrectness is a departure from the semantics of φ-instructions.
This requires us to insert the copy x3 = x2 on the back edge from node n2 to itself.
This edge is a critical edge. An edge n →m is a critical edge if n has more than one
successor and m has more than one predecessor. What we have done is to hoist the
‡Also called a web.
© 2009 by Taylor & Francis Group, LLC

Single Static Assignment Form as Intermediate Representation
209
n1 x = 1 n1
n2
y = x
x = x+1
x3
2
n2
n3 print(y) n3
n1 x1 = 1 n1
n2
x3 = φ(x1, x2)
y = x3
x2 = x3 +1
n2
n3 print(y) n3
n1 x1 = 1 n1
n2
x3 = φ(x1, x2)
x2 = x3 +1
y
x3
n2
n3 print(x3) n3
n1
x1 = 1
x3 = x1 n1
n2
x3 = φ(x3, x3)
x2 = x3 +1
x3 = x2
n2
n3 print(x3) n3
(a)
(b)
(c)
(d)
FIGURE 6.15
The lost copy problem.
copy statement across the critical edge. As a result this copy interferes with the live
range of x3 (shown using the dotted arrow).
The swap problem: The swap problem is illustrated in Figure 6.16 on the follow-
ing page. In this case also the problem arises because the process of SSA destruction
does not follow the semantics of φ-instructions. The program in Figure 6.16(c) is
correct because of the implied translation of the φ-instruction to the simultaneous
assignment x3 = y3 || y3 = x3. The actual translation, however, replaces the simulta-
neous assignment by a sequence of assignments resulting in a dependence between
them.
6.3.1
An Algorithm for SSA Destruction
Since the algorithm will require us to talk about variables which are related through
φ-instructions, we introduce the following deﬁnitions.
DEFINITION 6.8
A pair of variables are φ-related if they occur in the
same φ-instruction.
The idea of variables related through φ-instructions, which we have been infor-
mally calling variable versions, is captured through φ-congruence.
DEFINITION 6.9
For a SSA variable x, φ-congruence(x) is the least set
deﬁned by the following two rules:
1. if y and x are φ-related, then y is in φ-congruence(x).
2. if y and z are φ-related and z is in φ-congruence(x) then y is in φ-
congruence(x).
© 2009 by Taylor & Francis Group, LLC

210
Data Flow Analysis: Theory and Practice
n1
x = 1
y = 2 n1
n2
tmp = x
x = y
y = tmp
n2
n3 print(x,y) n3
n1
x1 = 1
y1 = 2 n1
n2
x3 = φ(x1, x2)
y3 = φ(y1,y2)
tmp = x3
x2 = y3
y2 = tmp
n2
n3 print(x3,y3) n3
n1
x1 = 1
y1 = 2 n1
n2
x3 = φ(x1,y3)
y3 = φ(y1, x3)
y
x3
n2
n3 print(x3,y3) n3
n1
x1 = 1
y1 = 2
x3 = x1
y3 = y1
n1
n2
x3 = φ(x3, x3)
y3 = φ(y3,y3)
x3 = y3
y3 = x3
n2
n3 print(x3,y3) n3
(a)
(b)
(c)
(d)
FIGURE 6.16
(a) The original program (b) After conversion to SSA form (c) After copy propaga-
tion (d) Destruction of the SSA form program through copying results in an incorrect
program.
In other words, variables in φ-congruence(x) are directly or transitively connected
to x through φ-instructions. Further, if y is in φ-congruence(x) then x is also in φ-
congruence(y), and we say that x and y are in the same φ-congruence class. The
notion of φ-congruence class is very similar to the notion of live range (or web) for
programs which are not in SSA form.
For a program in CSSA form, all variables that are in the same φ-congruence class
can be replaced by a common variable and the φ-instruction can be eliminated. Our
objective now is to modify TSSA programs so that φ-instructions can be eliminated
in the same way as CSSA programs, i.e., by renaming φ-congruence variables to the
same name.
The reason why SSA-destruction through merging of φ-congruent variables poses
problems in the case of TSSA programs can be better explained through live ranges.
Observe in part (a) of Figure 6.17 on the next page that the φ-congruent variables
x1 and y interfere with each other and thus cannot be replaced by the same variable.
Replacing both the variables by a single variable eﬀectively kills the earlier deﬁni-
tion. So a key idea might be to make the φ-congruent variables non-interfering by
inserting copy statements. As shown in part (b) of the ﬁgure, this has been achieved
for the example by inserting the copy statement x
1 = x1 in block n1, x
2 = y in block
n2 and x3 = x
3 in n3. Further, the φ-instruction has been rewritten to refer to the new
variables x
1, x
2 and x
3. Since the live ranges of these φ-variables are non-interfering,
they can be renamed to a single variable x and the φ-instruction can be eliminated.
The result is shown in Figure 6.17(c).
A further reﬁnement of the idea is to minimize the number of copy instructions by
© 2009 by Taylor & Francis Group, LLC

Single Static Assignment Form as Intermediate Representation
211
n1
y = 5
x1 = 3
n1
n2
x2 = y
n2
n3
x3 = φ(x1,y)
z = x3
n3
n1
y = 5
x1 = 3
x
1 = x1
n1
n2
x
2 = y
n2
n3
x
3 = φ(x
1, x
2)
x3 = x
3
z = x3
n3
(a)
(b)
n1
y = 5
x1 = 3
x = x1
n1
n2
x = y
n2
n3
x3 = x
z = x3 n3
n1
y = 5
x1 = 3
n1
n2
x
2 = y
n2
n3
x3 = φ(x1, x
2)
z = x3
n3
(c)
(d)
FIGURE 6.17
(a) Interference of Live ranges of the φ variables x1 and y. The dashed lines represent
live ranges of variables. (b) Breaking the interference through copy statements. The
new φ variables x
1 and x
2 do not interfere. (c) Eliminating φ assignments now results
in a correct program. (d) The copy statement for x1 is redundant.
introducing the copy statement x
2 = y only. This also makes the live ranges of the
φ-variables x1, x
2 and x3 non-interfering. The result of this minimization is shown
in Figure 6.17(d). Observe that inserting a copy statement x
1 = x1 at the end of block
n1 instead of x
2 = y does not break the interference between the live ranges of the
variables x
1 and y which are now φ-congruent.
The basis for the decision that the insertion of a single copy statement x
2 = y is
enough is as follows. First notice that the only interference that has to be broken is
between x1 and y; x3 does not interfere with these variables. Now y is live at the
exit of n1. Therefore insertion of a copy statement x
1 = x1 at the end of n1 is useless
since the new φ-congruent variables x
1 and y will still continue to interfere. However
insertion of the statement x
2 = y creates the φ-congruent variables x1 and x
2 which
© 2009 by Taylor & Francis Group, LLC

212
Data Flow Analysis: Theory and Practice
liveout(n1) = {x1, x3}
liveout(n2) = {x2}
liveout(n3) = {x3}
n1
x3 = 5
x1 = 3
n1
x2 = y
n2
x2 = y
n2n3
x2 = y
n3
n0 x0 = φ(x1, x2, x3) n0
x1
x3
x2
liveout(n1) = {x1, x2}
liveout(n2) = {x2, x4}
n3 x3 = φ(..., x4) n3
n1
x1 = φ(..., x3)
x2 = y
n1
n2
x4 = y n2
n0 x0 = φ(x1, x2) n0
x1
x2
(a)
(b)
FIGURE 6.18
(a) Example to illustrate the condition when both φ-congruent(xi) ∩liveout(n j) = φ
and φ-congruent(xj) ∩liveout(ni) = φ.
Live ranges and liveout sets are shown.
(b) Example illustrating the condition φ-congruent(xi) ∩liveout(n j)  ∅and φ-
congruent(xj)∩liveout(ni)  ∅.
do not interfere with each other.
The algorithm systematically creates φ-congruent classes that are non-interfering.
Initially φ-congruent(x) is set to {x} for every variable x. It considers in sequence all
the φ-instructions. For each pair of operands xi and xj mentioned in a φ-instruction,
it checks whether φ-congruent(xi) interferes with φ-congruent(xj). If it does not,
φ-congruent(xi) and φ-congruent(xj) are merged into the same φ-congruent class.
Otherwise copy statements are inserted to change the variables in the φ-instruction
itself so that the φ-congruence classes of the new variables are non-interfering. The
method for choosing the copy statement to be inserted is described below. In the
description below, n is the block that contains the φ-instruction, ni refers to the ith
predecessor of n, and xi and xj are the interfering variables. We ﬁrst consider the
case when both xi and xj are arguments of the φ-instruction:
1. φ-congruent(xi) ∩liveout(n j)  ∅and φ-congruent(xj) ∩liveout(ni) = ∅: This
situation is similar to Figure 6.17 with y playing the role of xi and x1 playing
the role of xj. In this case, a copy statement x
i = xi is needed at the end of ni.
xi is marked to record this fact.
2. φ-congruent(xj) ∩liveout(ni)  ∅and φ-congruent(xi) ∩liveout(n j) = ∅: This
is similar to the previous situation and the variable xj is marked.
3. φ-congruent(xi) ∩liveout(n j) = ∅and φ-congruent(xj) ∩liveout(ni) = ∅: This
situation is as shown in Figure 6.18(a), where x2 and x3 play the roles of xi
© 2009 by Taylor & Francis Group, LLC

Single Static Assignment Form as Intermediate Representation
213
xi = φ(..., xj) xi
xj
xi = φ(..., xj) xi xj
xj
xi = φ(..., xj) xi
xj
xi = φ(..., xj) xi xj
xj
(a)
(b)
(c)
(d)
FIGURE 6.19
Interference between argument and result variables of a φ-instruction.
and xj. In this case, either a copy statement x
2 = x2 at the end of n2 or a
copy statement x
3 = x
3 at the end of n3 will break the interference. The better
choice is to insert x
2 = x2 as it will also break the interference between x3 and
x1. Since we cannot know this till we examine the pairs x1 and x2, we defer
the insertion of the copy statement till we have examined all the pairs.
4. φ-congruent(xi) ∩liveout(n j)  ∅and φ-congruent(xj) ∩liveout(ni)  ∅: This
situation is represented by Figure 6.18(b) with x1 and x2 playing the roles of xi
and xj. Note that x4 is in φ-congruence(x1). In this situation copy statements
are needed for both xi and xj, so both the variables are marked.
When one of the interfering variables, say xi, is the result and the other variable
xj is an argument of the φ-instruction, the situation is slightly more complex. The
program point for inserting the copy statement involving the result variable is just
after the φ-instruction. Consider the block which has the φ-instruction. As shown in
Figure 6.19(a)–(d), there are four cases:
1. φ-congruent(xi)∩liveout(n)  ∅and φ-congruent(xj)∩livein(n) = ∅: We have
to insert the copy statement xi = x
i just after the φ-instruction. The result
variable of the φ-instruction is changed to x
i.
2. φ-congruent(xi) ∩liveout(n) = ∅and φ-congruent(xj) ∩livein(n)  ∅: As we
shall see later when we re-examine the swap problem, this situation occurs
when x j is also the result of a subsequent φ-instruction. This requires the copy
statement x
j = xj. The argument variable xj is changed to x
j.
3. φ-congruent(xi)∩liveout(n j) = ∅and φ-congruent(xj)∩liveout(ni) = ∅: Here
we can insert either a copy statement for xi or for xj. As explained earlier, the
choice is deferred.
4. φ-congruent(xi)∩liveout(n j)  ∅and φ-congruent(xj)∩liveout(ni)  ∅: In this
situation copy statements are needed for both xi and xj.
The variables for which copy statements are to be inserted are added to a marked or
deferred list as before. After all variables have been considered, we obtain two lists—
a list of variables which have been marked and for which we need copy statements
© 2009 by Taylor & Francis Group, LLC

214
Data Flow Analysis: Theory and Practice
Input: A CFG of a TSSA program.
Output: The corresponding program with the φ-instructions eliminated.
Algorithm:
0
Initialize the φ-congruent class of each φ variable x to {x}.
1
for each φ-instruction I do
2
{
Initialize the marked and deferred lists to the empty list.
3
for each pair xi and xj of argument variables in I do
4
if xi and xj interfere, then proceed according to the four cases
described in Section 6.3.1.
5
for the result variable xi and each argument variable xj do
6
if xi and xj interfere, then proceed according to the four cases
described in Section 6.3.1.
7
if a variable x is in the marked list, then remove all pairs which have x as
one of the components from the deferred list.
8
while there are elements in the deferred list do
9
{
Select the variable x that appears maximum number of times in the
deferred list.
10
Insert x in the marked list.
11
Remove all pairs which have x as one of the components from the
deferred list
12
}
13
for each element x in the marked list do
14
{
Insert the copy statement x = x at the appropriate program
point.
15
Update I to contain x instead of x.
16
Modify the interference graph to reﬂect this change.
17
}
18
Put all the variables of I in the same congruence class.
19
}
20
Eliminate all φ-instructions.
FIGURE 6.20
An algorithm for SSA destruction.
and the other a list of pair of variables, the choice from which has been deferred. We
now choose a variable which appears the largest number of times in the deferred list
and enter it in the marked list. All pairs in which it appears are removed from the
deferred list. This is repeated till the deferred list is empty.
The last step consists of taking each variable from the marked list, inserting a copy
statement for the variable, updating the φ-instruction, and updating the live ranges of
the old and the new variables. The φ-instructions which now contain variables in the
same φ-congruent class are now eliminated. The algorithm for breaking interference
through insertion of copy statements is shown in Figure 6.20.
We now explain how the algorithm works on the lost-copy problem and the swap
© 2009 by Taylor & Francis Group, LLC

Single Static Assignment Form as Intermediate Representation
215
n1 x1 = 1 n1
n2
x3 = φ(x1, x2)
x2 = x3 +1
n2
n3 print(x3) n3
x3
x2
n1 x1 = 1 n1
n2
x
3 = φ(x1, x2)
x3 = x
3
x2 = x3 +1
n2
n3 print(x3) n3
(a)
(b)
n1
x1 = 1
y1 = 1 n1
n2
x3 = φ(x1,y3)
y3 = φ(y1, x3)
n2
n3 print(x3,y3) n3
x3,y3
n1
x1 = 1
y1 = 1 n1
n2
x
3 = φ(x1,y
3)
y3 = φ(y1, x3)
x3 = x
3
y
3 = y3
n2
n3 print(x3) n3
y3
x3
n1
x1 = 1
y1 = 1 n1
n2
x
3 = φ(x1,y
3)
y3 = φ(y1, x
3 )
x3 = x
3
y
3 = y3
x
3 = x3
n2
n3 print(x3,y3) n3
(c)
(d)
(e)
FIGURE 6.21
Illustrating the eﬀect of the algorithm on the lost-copy (Figures (a) and (b)) and the
swap problem (Figures (c), (d) and (e)).
problem. Consider the SSA corresponding to the lost-copy problem shown in part
(a) of Figure 6.21. The live ranges of x2 and x3 interfere. While x3 is in liveout(n2),
x2 is not in livein(n2). Therefore, as shown in Figure 6.21(b), a copy x3 = x
3 inserted
after the φ-instruction breaks the interference.
The SSA form of the program illustrating the swap problem is shown in Fig-
ure 6.21(c). The live ranges of both x3 and y3 span the entire block n2. Now consider
the ﬁrst assignment. Since x3 is in liveout(n2) a copy is needed for x3. Similarly,
since y3 is in livein(n2), a copy is needed for y3. Now there is no interference be-
tween the variables of the ﬁrst assignment. Considering the second assignment in
Figure 6.21(d), we see that the live ranges of y3 and x3 interfere. However, neither
y3 is in liveout(n2) nor x3 is in livein(n2). Therefore a copy statement for either x3
or y3 can be inserted. We choose x3 and the result is shown in Figure 6.21(e).
© 2009 by Taylor & Francis Group, LLC

216
Data Flow Analysis: Theory and Practice
6.3.2
SSA Destruction and Register Allocation
In the traditional sequence of events during compilation, the program in SSA form is
destructed before register allocation takes place. However, as we shall explain later,
the SSA form program has properties that are useful for register allocation through
graph coloring. After register allocation is done, SSA destruction can be viewed as
a form of coalescing registers.
Overview
The idea behind register allocation through graph coloring is as follows. The main
data structure used is a graph called interference graph. An interference graph has a
node for every live range in the program. Since, for programs in SSA form, each live
range corresponds to a variable, we can also associate the nodes of the interference
graph with variables. An edge is drawn between live ranges if they interfere, i.e., they
range over common program points. In such a situation, the variables corresponding
to the live ranges cannot be allocated the same register. Thus the problem of register
allocation reduces to one of coloring the interference graph with a number of colors
equal to the number of available registers so that no two adjacent nodes have the
same color.
The chromatic number of a graph is the minimum number of colors required to
color a graph as described above. If the chromatic number of a graph is larger than
the number of available registers, then an attempt is made to reduce the interference
by spilling, i.e., inserting stores after deﬁnitions and loads before uses. This eﬀec-
tively replaces a long live range by a number of shorter live ranges. The reduction in
interference has the possible consequence of bringing down the chromatic number.
As shown in Figure 6.22, a typical register allocator that uses graph coloring re-
peats the following steps till the graph is colored.
1. Constructing or updating the interference graph.
2. Coalescing live ranges. If the live range of x ends with a copy statement to
y, then the live ranges for x and y can be combined by replacing subsequent
references to the uses of y in the live range by x and eliminating the copy. As
this will change the interference graph, it has to be updated.
3. Attempt to color nodes. If this requires a node to be spilled, the interference
graph has to be updated. So we go back to step 1.
There are two problems with this approach. While coalescing eliminates copy
statements, it might also result in a graph with a larger chromatic number. Moreover,
since every spill may not reduce the chromatic number of the graph, the interference
graph may have to be constructed several times. This can be costly.
Now consider register allocation for a program in SSA form. We shall show that
the interference graph of a SSA form program is a special kind of graph called
chordal graph. The chromatic number of such a graph is the same as the size of
the largest clique. Moreover, the largest clique in a SSA form program is equal to
© 2009 by Taylor & Francis Group, LLC

Single Static Assignment Form as Intermediate Representation
217
build/update
interference
graph
build / update
interference
graph
build/update
interference
graph
coalesce
build/update
interference
graph
attempt to
color
build/update
interference
graph
spill
(a)
build/update
interference
graph
spill
build/update
interference
graph
color
build/update
interference
graph
coalesce
(b)
FIGURE 6.22
(a) Traditional register allocation. (b) Register allocation for SSA form programs.
the maximum number of variables live at a program point. So we can spill variables
till the largest number of variables live at any program point equals the given number
of registers. The interference graph of the resulting program is now guaranteed to be
colorable with colors equal to the available number of registers. In fact, for the SSA
form program, there is also an eﬃcient algorithm to ﬁnd the coloring.
While variables are replaced by registers after coloring, the φ-instructions are still
present. As mentioned earlier, SSA destruction is a form of coalescing. For instance,
assume that the φ-instructions in a basic block are:
R1 = φ(R1,R3)
R2 = φ(R2,R4)
For both the φ-instructions, the ﬁrst operand is the same as the result and therefore
no transfer of values need take place. An attempt is made to recolor R3 to R1 and R4
to R2. If this succeeds, then the φ-instructions can simply be eliminated. Otherwise
copy instructions must be inserted.
The overall scheme for register allocation for SSA form programs is shown in
Figure 6.22(b). Note that the process is not iterative. More importantly, all the above
steps can be carried without constructing the interference graph.
© 2009 by Taylor & Francis Group, LLC

218
Data Flow Analysis: Theory and Practice
Spilling
We now show certain properties of interference graphs of SSA form programs which
makes it easy to determine whether enough variables have been spilled so as to make
the interference graph colorable.
LEMMA 6.9
Let a variable x be live at a program point p. Then def(x)  p.
PROOF
Assume to the contrary that def(x) p. Since x is live at p, there
is a path from p to some use of x, use(x). Then def(x)use(x) contradicting
the SSA dominance property (Lemma 6.8).
LEMMA 6.10
If x and y interfere either def(x)def(y) or def(y)def(x).
PROOF
Since x and y interfere, there is a program point p where they are
both live. From Lemma 6.9, both def(x) and def(y) dominate p. The result
then follows from Observation 6.1.
LEMMA 6.11
Assume def(x)def(y). Then x and y interfere if and only if then x is live at
def(y).
PROOF
The if part is obvious. For the only if part observe that under
the condition def(x)def(y) if x is not live at def(y) then there is no point p
such that x is live at p and there is a path from def(y) to p. It follows that x
and y cannot interfere, leading to a contradiction.
LEMMA 6.12
Let x — y and y — z be edges in the interference graph G of a SSA form
program. Further, assume that x — z is not an edge in G. If def(x)def(y),
then def(y)def(z).
PROOF
Since x and y interfere and def(x)def(y), x must be live at
def(y). Further, since y and z interfere, either def(y)def(z) or def(z)def(y).
If def(z)def(y), then z must also be live at def(y) and x and z interfere. This
is a contradiction and we must have def(y)def(z).
© 2009 by Taylor & Francis Group, LLC

Single Static Assignment Form as Intermediate Representation
219
x = ...
y = ...
z = ...
... = x
z = ...
... = y
... = z
FIGURE 6.23
An example to show that Lemma 6.14 does not hold for programs not in SSA form.
LEMMA 6.13
Let G be the interference graph of a program in SSA form and let C ⊆G
be a clique whose vertex set is {x1,..., xn}. Then there is a permutation π of
{1,...,n} such that def(xπ(1)),..., def(xπ(n)).
PROOF
For i, j ∈{1,...n}, xi and xj interfere. Therefore from Lemma 6.10,
either def(xi)def(xj) or def(xj)def(xi). π can be obtained by sorting {x1,..., xn}
on .
LEMMA 6.14
Let G be the interference graph of a program in SSA form and let C ⊆G be a
induced subgraph with vertex set {x1,..., xn}. C is a clique if and only if there
is a program point where x1,..., xn are all live.
PROOF
The if part is trivial. Let C be a clique. By Lemma 6.13, there
is a permutation π of {1,...,n} such that def(xπ(1)) ... def(xπ(n)). Therefore,
from Lemma 6.11, x1,..., xn are all live just after def(xπ(n)).
The above lemma does not hold for programs which are not in SSA form. Con-
sider, for example, Figure 6.23. The live ranges of x, y and z form a clique in the
interference graph. However, there is no program point where all three variables are
live.
DEFINITION 6.10
A chordal graph is a graph which does not have any
induced cycle of length more than three.
© 2009 by Taylor & Francis Group, LLC

220
Data Flow Analysis: Theory and Practice
3
1
2
4
6
7
5
FIGURE 6.24
Example of a chordal graph.
Figure 6.24 gives an example of a chordal graph.
An interesting property of a chordal graph is its chromatic number is the same
as the size of its largest clique. This can be seen in the example graph where the
maximum clique size is three which is also the minimum number of colors required
to color the graph. Therefore, if we can show that the interference graph of a pro-
gram in SSA form is chordal, then, by Lemma 6.14, its chromatic number will be
determined by the largest liveness set at any point in the program.
LEMMA 6.15
Let G be an interference graph of a program in SSA form. Then G is chordal.
PROOF
Assume to the contrary that it is not. Then there will be at least
one induced cycle C = x1, x2,..., xn, x1 with n ≥4. Now consider the sequence
x1, x2,..., xn. Clearly, we do not have an edge xi — x j, such that j > i+1, or C
would not be a cycle.
Since x1 and x2 interfere, either def(x1)def(x2) or def(x2)def(x1) by
Lemma 6.10.
Assume without loss of generality, def(x1)def(x2).
Since
x2 — x3 is an edge and x1 — x3 is not an edge, by Lemma 6.12, def(x2)def(x3).
Using this idea, we can show by induction that there is a chain of dominance
def(x1)def(x2) ... def(xn).
Now since x1 and xn interfere, there is a program point p where both x1
and xn are live. Further, by Lemma 6.9, def(xn) dominates p. Because of the
chain of dominances each def(xi) dominates p.
Consider a xi, where i is not 1 or n. Since def(xi) dominates p and does
not dominate def(x1), there is at least one path from def(xi) to p that does
not have a deﬁnition of x1. Thus x1 is live at def(xi). This means there is an
interference edge between 1 and i, leading to a contradiction.
The Spilling Algorithm
The chromatic number of an SSA form program is the same as the maximum number
of variables that are live at any point in the program. Hence we should ensure through
© 2009 by Taylor & Francis Group, LLC

Single Static Assignment Form as Intermediate Representation
221
spilling that the size of the set of live variables at any program point is no larger than
the available number of registers. This is done without constructing the interference
graph.
For every variable x we assume that there is a memory location x. These memory
locations are not in contention for registers. A spill is an assignment x = x. A reload
of a variable is an assignment x = x. For every basic block, the spilling algorithm
decides:
1. The variables that get into registers at the entry of the basic block.
2. For every assignment statement, the variables that have to be spilled so that
the operands on the right hand side of the assignment can be accommodated
into registers, by reloading if necessary.
The spilling decision at a program point is based on the nearest distance at which a
variable is subsequently used. We call this the next use of the variable and is captured
through a function called nextuse. The next variable to be spilled at a program point
is the one whose next use is the farthest. This is with the expectation that a free
register can be found for the variable by the time the next use is reached.
For a basic block n and a variable x, the function nextuse is deﬁned as follows:
nextuse(n, x) =

∞
x is not live
0
x is used in n
1+
min
n∈succ(n)nextuse(n, x)
Assume that the number of available registers is k. At the entry of each block,
we consider only the variables that are live and select k variables with the lowest
nextuse. These are the variables to be held in registers at the entry of the block.
Similarly, for an assignment p : x = op(y1,...yi), if any of the variables y1, ..., yi
have to be brought into a register, the variable z with the highest value of nextuse(p,z)
value is spilled. Since the assignment to x takes place after the computation of
op(y1,...yi), to ﬁnd a register for x, we spill the variable z with the highest value
of minp∈succ(p) nextuse(p,z).
Let us assume that based on the above consideration, we have decided to assign
registers to the set of variables I at the beginning of a basic block n. Consider any
predecessor n of n. If O is the set of variables that have been decided to be kept in
registers at the exit of the predecessor, we have to reload the variables in I −O at the
edge connecting n and n.
Reloads introduce deﬁnitions that were not present in the original program. As a
result of reloads, the program may not be in SSA form. However, for the PEO based
coloring that we discuss later to be applicable, the program must be brought back to
SSA form. We explain with an example how this is done.
Assume that in Figure 6.25(a), the variable x1 had to be reloaded in block n3 so
that the program is no longer in SSA form. We have to bring the program back to
SSA form and rewrite the uses of x1. We start by calculating the iterated dominance
frontier of the node n3 which contains the reload. Next, the variable x1 in n3 is
© 2009 by Taylor & Francis Group, LLC

222
Data Flow Analysis: Theory and Practice
n1
x1 = ...
n1
n2
x1 = ... n2
n3
x1 = x1 n3
n4
x1 = ... n4
n5
x1 = ... n5
n6 x4 = φ(x3, x1) n6
n1
x1 = ...
n1
n2
x1 = ... n2
n3
x5 = x1 n3
n4 x7 = φ(x1, x5) n4
n5 x6 = φ(x1, x7) n5
n6 x4 = φ(x3, x6) n6
(a)
(b)
FIGURE 6.25
Example to illustrate SSA reconstruction.
renamed to a new variable x5 to bring back the single deﬁnition property of SSA
form. We now rewrite the use occurrence of each variable to the deﬁnition reaching
it. In the process φ-instructions are inserted wherever necessary.
To start with, we have to decide how to rewrite the use of x1 in n6. We observe that
the predecessor n5 of n6 is in the iterated dominance frontier of n3. Since this does not
have a φ-instruction for x1, we have to insert one. The result of this φ-instruction, x6,
is the deﬁnition reaching x1 and thus will replace x1. Now we recursively try to ﬁnd
the deﬁnitions reaching the ﬁrst and second argument of the inserted φ-instruction
at n5. The deﬁnition reaching the ﬁrst argument comes from x1 at n1. The search
for the deﬁnition reaching the second argument results in the insertion of another
φ-instruction at n4. The result of this φ instruction, x7, reaches the second argument
of the φ function at n5. The arguments of the φ function at n4 are similarly found to
be x1 and x5.
Coloring
We now describe properties of a chordal graph due to which it can be colored eﬃ-
ciently.
DEFINITION 6.11
A node in a graph G is called simplicial if its neigh-
boring nodes induce a clique in G.
© 2009 by Taylor & Francis Group, LLC

Single Static Assignment Form as Intermediate Representation
223
DEFINITION 6.12
A perfect elimination order (PEO) of a graph G is an
order based on elimination of the nodes of G as follows: At each step eliminate
a simplicial node in the remaining graph.
It can be veriﬁed that 1,2,4,3,6,5,7 is a PEO for the graph in Figure 6.24.
Let the maximum size of a clique in a graph be k. Consider the following proce-
dure to color the graph with k colors using a PEO ordering: Starting with the empty
graph, at each step we color and add a node, say n, in reverse order of PEO. While
adding n we are assured that its neighbors form a clique of size at most k−1. Thus a
color can always be found for n.
A graph is chordal if and only if it admits of a PEO ordering. If a graph has a
PEO, not only is its colorability equal to the maximum size over all cliques of the
graph, there is a polynomial time algorithm to obtain the coloring.
In the context of programs in SSA form, the following result gives a PEO of
interference graphs and thus forms the basis of the coloring algorithm.
LEMMA 6.16
Let G be the interference graph of a program in SSA form.
Consider an
ordering of the nodes of the graph in which a node v is included only if all the
nodes whose deﬁnitions are dominated by the deﬁnition of v have been already
added to the ordering. Then the ordering is a PEO.
PROOF
We have to show that v is simplicial at the point when it is
included in the ordering. Consider two nodes u and x both of which interfere
with v. Then we have to show that u and x interfere with each other.
Since the nodes v and u interfere, following Lemma 6.10 we have either
def(v)def(u) or def(u)def(v). Since all deﬁnitions that are dominated by u
have already been added to the ordering and eliminated from the interference
graph, it must be the case that def(u)def(v). Therefore u is live at def(v).
For similar reasons x is also live at def(u). Thus u and x interfere.
The function colorNode uses the dominator based PEO to color the interference
graph and is shown in Figure 6.26. The function is initially called with Start. It
processes a node in the dominator tree before processing its children, and within a
node it processes the statements in sequence. This ensures that the corresponding in-
terference graph is colored in reverse PEO order. Observe that this happens without
actually constructing the interference graph.
Coalescing by Recoloring
At the end of the coloring phase, the residency of variables in registers is as follows:
1. Some variables which do not participate in any φ-instruction could be assigned
registers. If such a variable is live across a join node, it is held in a register
© 2009 by Taylor & Francis Group, LLC

224
Data Flow Analysis: Theory and Practice
Input: A node n of the CFG of the SSA form program, the dominator tree of the
CFG and the set of live variables at the entry of n. The function color returns the
color of a already colored node.
Output: A coloring of the variables in n.
Algorithm:
0
function colorNode(n)
1
{
allocated = ∅
2
for each variable x in livein(n) do
3
{
allocated = allocated ∪color(x)
4
for each statement s in n in sequence do
5
for each variable y used in s do
6
{
if the last use of y is in s then
7
allocated = allocated −{color(y)}
8
let x be the variable deﬁned in s and c be an unallocated color
in
9
{
color(x) = c
10
allocated = allocated −{c}
11
}
12
}
13
}
14
for each child m of node n do colorNode(m)
15
}
FIGURE 6.26
Algorithm for coloring the interference graph.
along all paths reaching the join node. This situation is diﬀerent for a φ vari-
able which could be held in a register along one of the paths reaching the join
point.
2. If the result of a φ-instruction is in a register, then it is ensured that the argu-
ments of the φ-instruction are also in registers. Since the number of registers
available for allocation to φ-variables is the same along all paths to a join node,
the result of the coloring algorithm can always be altered to satisfy this condi-
tion. Similarly, if the result of a φ-instruction is a memory location, then the
arguments of the φ-instruction are also made to reside memory locations.
The destruction of φ-instructions is viewed as a form of coalescing. Let alloc be
an assignment of variables to registers and consider the φ-instruction
alloc(y) = φ(alloc(x1),...,alloc(xi),...,alloc(xn))
Destruction of this φ-instruction is the transfer of values from the registers alloc(xi)
to alloc(y) through register copies. If alloc(y) is the same as alloc(xi), then no trans-
fer of value needs take place. Otherwise, a copy statement has to be issued to transfer
the value from alloc(xi) to alloc(y). The problem is to color the interference graph
© 2009 by Taylor & Francis Group, LLC

Single Static Assignment Form as Intermediate Representation
225
of a program so as to minimize the transfer cost. This problem is called the SSA-
coalescing problem. We shall now deﬁne it formally.
SSA coalescing
Call a pair of variables φ-assigned, if one of them is an argument and the other the
result of a φ-instruction. Assume that we have a function c which associates a cost
with every pair of edges that are φ-assigned. This cost takes into account (i) the cost
of transferring a value x to y, assuming the variables are in separate registers, and (ii)
the frequency of execution of the basic block which has the φ-instruction containing
x and y. Given a coloring alloc, we deﬁne the cost of the coloring for the φ-assigned
pair (x,y) as
cost alloc(x,y) =
0
alloc(x) = alloc(y)
c(x,y) otherwise
And the cost of the coloring for the entire program P as:
cost alloc(P) =

(x,y)∈P, φ-assigned(x,y)
cost alloc(x,y)
DEFINITION 6.13
Given a program in SSA form and its interfer-
ence graph, the S S A-coalescing problem is to ﬁnd a coloring alloc for which
cost alloc(P) is minimum.
Since this problem is NP-hard, we now present a heuristic for solving the problem.
The idea is that we take the output of the coloring algorithm described before and
modify the coloring so as to minimize the cost of transfer of values.
To start with, the algorithm forms groups of variables. Each group consists of max-
imal number of φ-congruence variables that are non-interfering. Interfering variables
cannot be given the same color. Deﬁne the cost of each such group g as the cost of
all the φ-related edges between variables in the group g, i.e.,
cost group (g) =

x,y∈g, φ-assigned(x,y)
cost alloc(x,y)
Clearly, a successful coloring of a costlier group will yield more beneﬁts in trans-
fer costs. Therefore, the groups are sorted by decreasing cost and entered into a
priority queue for recoloring in this order.
Now the groups in the priority queue are attempted to be recolored. We take each
color c in turn and attempt a recoloring with c. Not all nodes in the group G can
be colored with c. As a result, the recoloring attempt results in several subgroups
g1,g2,...,gn such that:
1. Each variable in each subgroup can be recolored to c.
2. Each gi forms a φ-congruence class.
© 2009 by Taylor & Francis Group, LLC

226
Data Flow Analysis: Theory and Practice
The subgroup gi with maximum value of costgroup(gi) is the candidate subgroup sgc
for the color c. This is done for all colors and the ﬁnal decision is to chose c with the
maximum sgc. The corresponding subgroup’s color is ﬁxed at c and never changed
thereafter. This ensures the termination of the algorithm. A new group G −sgc is
formed and entered in the priority queue at an appropriate place depending on its
cost. This process is repeated till the priority queue is empty.
To recolor a node with the color c, the algorithm checks that none of its neighbors
have the color c. If this is true then the recoloring attempt is successful. Otherwise
the algorithm recursively attempts to recolor the oﬀending neighbor with a color
diﬀerent from c. The recoloring attempt fails if the color of the node is already ﬁxed
to a color that is diﬀerent from the color for which the recoloring attempt is being
made, or the node cannot be colored because of a lack of color.
It might appear that the recoloring step requires construction of the interference
graph to, for example, determine non-interfering φ-congruence groups. However,
this is not the case. Assume that we want to decide whether x interferes with y. We
ﬁrst determine whether def(x) and def(y) are related by a dominance relationship. If
they are not, then by Lemma 6.10 they do not interfere. On the other hand, if there
is a dominance relationship and def(x)def(y), for example, then by Lemma 6.11,
x and y interfere if x is live at def(y).
Register Copies
The last step in the method is to arrange for transfer of value for φ-congruent vari-
ables that could not be colored with the same color. To take into account that φ-
instructions within the same basic block are to be simultaneously executed, we con-
sider all the φ-instructions in the basic block together. As an example consider the
φ-instructions
R1 = φ(...,R2,...)
R2 = φ(...,R2,...)
R3 = φ(...,R5,...)
R4 = φ(...,R3,...)
R5 = φ(...,R4,...)
In the example, we limit ourselves to the registers at one of the argument positions.
We can represent this transfer of value through a graph shown in Figure 6.27. While
in the example, we have restricted ourselves to the registers at one of the argument
positions, the graph has to be extended to other argument positions. The resulting
graph is called the register transfer graph. Now we generate instructions to eﬀect
the value transfers suggested by the register transfer graph. Each step is repeated as
many times as possible.
1. If there is a edge Ri →R j in the graph such that R j does not have any out
edges, then a copy statement R j = Ri is issued. This is illustrated by the edge
R2 →R1 in the example, for which a copy statement R1 = R2 has to be issued.
© 2009 by Taylor & Francis Group, LLC

Single Static Assignment Form as Intermediate Representation
227
R2
R1
R5
R3
R4
FIGURE 6.27
Graph indicating transfer of values between registers.
2. Now the register transfer graph will consist of one or more cycles. The cycles
of length 1 like R2 are eliminated.
3. The cyclic transfers values indicated by loops of length more than one like
R3,R4,R5 can be eﬀected in more than one way:
(a) Transfer using a free register as a temporary. For the example, assuming
R0 is a free register, the instructions generated are:
R0 = R3
R3 = R4
R4 = R5
R5 = R0
(b) If there is no free register, then pairwise swap operations can be used. For
the example, the transfer can be eﬀected through the following swaps:
swap R3 R4
swap R4 R5
swap R5 R3
If the underlying machine does not directly support a swap operation, it
may be simulated through xor operations.
6.4
Summary and Concluding Remarks
In this chapter we described a useful intermediate representation of programs called
the SSA form. In this representation every variable has exactly one deﬁnition, and
this deﬁnition dominates each use of the variable. The number of def-use chains
© 2009 by Taylor & Francis Group, LLC

228
Data Flow Analysis: Theory and Practice
in SSA form programs is much smaller than corresponding programs not in SSA
form. As a consequence, optimizations performed on SSA form programs are faster.
Apart from the sparseness of def-use chains, a program in SSA form also has other
interesting properties that could be used in various applications. An example that
was presented is register allocation.
The transformation of a program to SSA form involves ﬁnding program points
where φ-functions are to be inserted. These points are identiﬁed by iterated domi-
nance frontiers. After φ-functions are inserted, variables are renamed to satisfy the
single deﬁnition property. Both these steps can be done eﬃciently. The transforma-
tion of programs to their SSA form can be thought of as being the result of some
form of data ﬂow analysis. Destruction of SSA form programs is based on creating
φ-congruence variables that are also non-interfering. This is through insertion of
copy statements. The φ-congruence variables are then renamed to the same variable
and the φ-instruction is removed.
We also presented register allocation as a way of destructing SSA form programs.
Register allocation of SSA form programs through graph coloring is convenient be-
cause the interference graphs of such programs have properties that enable us to
(a) determine how much spilling is required so that the interference graph becomes
colorable, and (b) obtain a coloring. Removal of φ-instructions is through register
coalescing. Interestingly, all these steps can be done without actually constructing
the interference graph.
SSA-based optimizations are more diﬃcult when the entity involved in the opti-
mization is not a variable, as in the redundancy elimination optimizations. The prob-
lem is that the expressions representing redundant computations may not be lexically
the same; they may have diﬀerent versions of a variable. Detecting these occurrences
and eliminating the redundant ones by exploiting the sparseness of def-use chains is
not straightforward.
6.5
Bibliographic Notes
The earliest papers on SSA form are by Rosen, Wegman and Zadeck [85] and Alpern,
Wegman and Zadeck [8]. The ﬁrst comprehensive method for construction of SSA
form programs is by Cytron, Ferrante, Rosen, Wegman, and Zadeck [28].
The
method described in this chapter is based on this paper. A later paper by Sreed-
har and Gao [95] gives a linear time algorithm for placing φ-instructions using a
data structure called DJ-graphs. Both methods involve ﬁnding the dominator tree of
a program. Lengauer and Tarjan [68] give a fast algorithm for ﬁnding dominators
in a graph. The methods above construct minimal SSA. Choi, Cytron and Ferrante
[22] present a method to create programs in pruned SSA form and Briggs, Cooper,
Harvey, and Simpson [18] describe construction of semi-pruned SSA.
While Cytron, Ferrante, Rosen, Wegman, and Zadeck [28] discuss destruction of
© 2009 by Taylor & Francis Group, LLC

Single Static Assignment Form as Intermediate Representation
229
SSA, the method that they suggest has shortcomings. The method discussed here is
based on the work by Sreedhar, Dz-Ching Ju, Gillies and Santhanam [96]. Briggs,
Cooper, Harvey, and Simpson [18] discuss SSA-destruction by placing copy state-
ments along edges. The method for SSA destruction by register allocation is by
Hack [41] and by Hack, Grund, and Goos [42].
Many applications of SSA form can be found in literature. Rosen, Wegman and
Zadeck [85] describe a method to eliminate redundant computations among expres-
sions that may not be lexically identical. Alpern, Wegman and Zadeck [8] describe
how to conservatively detect equality of variables in a program. Kennedy, Chan,
Liu, Lo, Tu, and Chow [58] use SSA for partial redundancy elimination and Weg-
man and Zadeck [103] for conditional constant propagation. As mentioned earlier,
Hack, Grund, and Goos [42] perform register allocation over SSA form programs
while Knobe and Sarkar [64] use a variation of SSA form for parallelization.
Appel [11] describes the similarity between SSA programs and functional pro-
grams written in continuation passing style and Dhamdhere, Rosen and Zadeck [31]
point out the diﬃculties in using SSA form for partial redundancy elimination.
© 2009 by Taylor & Francis Group, LLC

© 2009 by Taylor & Francis Group, LLC

Part II
Interprocedural Data Flow
Analysis
231
© 2009 by Taylor & Francis Group, LLC

© 2009 by Taylor & Francis Group, LLC

7
Introduction to Interprocedural Data Flow
Analysis
The intraprocedural optimizations that we have discussed so far have ignored the ef-
fect of a call under the assumption that a safe approximation of the eﬀect of a call
can be incorporated without inspecting the called procedures. This was illustrated in
Section 1.1.2. A possible improvement of using interprocedural data ﬂow informa-
tion by analyzing the called procedures was also demonstrated in the same section.
In this chapter we evolve the basic concepts of the latter.
7.1
A Motivating Example
We use the program in Figure 7.1 as a running example in this chapter. We perform
constant propagation and dead code elimination over this program and introduce
common variants of interprocedural analyses. Figure 7.1(a) shows our program.
From the viewpoint of interprocedural analysis, its simplifying features are that it
is non-recursive and contains global variables only.
The optimized program after performing interprocedural constant propagation is
shown in part (b). Modiﬁed statements are shown in gray background. Constant
propagation replaces uses of variables by their known values and potentially creates
dead code. The statements shown in gray background in part (c) are the assignments
that become dead code and can be deleted. Observe that when procedure p is called
from procedure q, the value of variable d is 14. However, p is also called from main
and the value of d in that call is not known. Hence we cannot conclude that d is
constant in procedure p. Also observe that when procedure p is called the second
time, since the values of b and d are known to be 2 and 14 respectively, the condition
on line 17 is true and the assignment on line 18 is executed. Since a is assigned 1 in
procedure q, the value of c becomes 3 and remains 3 in expression a +c on line 12.
Our analysis does not perform conditional constant propagation and fails to discover
that the value of c is 3. However, it discovers the value of a in expression a + c on
line 12 to be 2 due to the assignment in line 25.
233
© 2009 by Taylor & Francis Group, LLC

234
Data Flow Analysis: Theory and Practice
0. int a,b,c,d;
1.
2. void main()
3. {
a = 5;
4.
b = 3;
5.
c = 7;
6.
read(d);
7.
p();
8.
a = a+2;
9.
print(c+d);
10.
d = a*b;
11.
q();
12.
print(a+c);
13. }
14.
15. void p()
16. {
b = 2;
17.
if (b<d)
18.
c = a+b;
19.
print(c+d);
20. }
21.
22. void q()
23. {
a = 1;
24.
p();
25.
a = a*b;
26. }
0. int a,b,c,d;
1.
2. void main()
3. {
a = 5;
4.
b = 3;
5.
c = 7;
6.
read(d);
7.
p();
8.
a = 7;
9.
print(7+d);
10.
d = 14;
11.
q();
12.
print(2+c);
13. }
14.
15. void p()
16. {
b = 2;
17.
if (2<d)
18.
c = a+2;
19.
print(c+d);
20. }
21.
22. void q()
23. {
a = 1;
24.
p();
25.
a = 2;
26. }
0. int a,b,c,d;
1.
2. void main()
3. {
a = 5;
4.
b = 3;
5.
c = 7;
6.
read(d);
7.
p();
8.
a = 7;
9.
print(7+d);
10.
d = 14;
11.
q();
12.
print(2+c);
13. }
14.
15. void p()
16. {
b = 2;
17.
if (2<d)
18.
c = a+2;
19.
print(c+d);
20. }
21.
22. void q()
23. {
a = 1;
24.
p();
25.
a = 2;
26. }
(a) Original program
(b) Discovered constants
(c) Discovered dead code
FIGURE 7.1
An example program with interprocedural constant propagation and subsequent in-
terprocedural dead code elimination. For simplicity, we assume built-in operations
to read and print data.
7.2
Program Representations for Interprocedural Analysis
Figure 7.2 shows two intermediate representations of our example. A call multigraph
is a directed graph which captures the caller-callee relationships in a program. Nodes
in a call multigraph represent procedures whereas edges represent procedure calls
and are labeled by the call sites. Since each call to a procedure is represented by a
distinct edge, a call multigraph contains parallel edges when a procedure contains
multiple calls to some procedure. Recursion in a program would cause cycles in the
call multigraph. The call multigraph for our program does not contain parallel edges
or cycles.
© 2009 by Taylor & Francis Group, LLC

Introduction to Interprocedural Data Flow Analysis
235
main
p
q
c1
c2
c3
Supergraph
Call multigraph
a = 5;b = 3
c = 7;read d
Startmain
Call p
C1
Call p
R1
n1
a = a+2
print c+d n1
n2
d = a ∗b n2
Call q
C2
Call q
R2
print a +c
Endmain
b = 2
if (b < d)
Startp
n3 c = a+b n3
print c+d
Endp
T
F
a = 1
Startq
Call p
C3
Call p
R3
a = a∗b
Endq
FIGURE 7.2
Common intermediate representations for interprocedural data ﬂow analysis.
The second intermediate representation is also a directed graph called a super-
graph which connects CFGs of callers and callees by edges indicating interprocedu-
ral control transfers.
A simpler version of supergraph was introduced in Chapter 1.
As illustrated in Figure 1.3 on page 5, it represented a call by a single basic block.
Now we split a call site ci into a call node Ci and the corresponding return node
Ri. A call to procedure r at call site ci is represented by an edge from Ci to Startr.
The corresponding return from procedure r is represented by an edge from Endr to
Ri. These edges are interprocedural edges. The edges in the individual CFG are in-
traprocedural edges. The supergraph in Figure 7.2 shows the interprocedural edges
by dashed lines and intraprocedural edges by solid lines. The program entry and exit
is denoted by Startmain and Endmain.
A supergraph and the corresponding call multigraph are related to each other by
a simple graph transformation. If every procedure in a supergraph of a program is
represented by a single node by combining all blocks of a procedure and all return
edges are removed, a supergraph reduces to the call multigraph of the program.
Observe that blocks Startmain, n1, and Startp in our supergraph contain multiple
statements in spite of the fact that for constant propagation a basic block consists of
a single statement. However, it is possible to combine assignment statements into a
single block when they do not have data dependence between them; we have done
so for convenience.
© 2009 by Taylor & Francis Group, LLC

236
Data Flow Analysis: Theory and Practice
7.3
Modeling Interprocedural Data Flow Analysis
In this section, we develop an abstract view of interprocedural data ﬂow analysis with
the goal of evolving basic concepts; details are postponed to subsequent chapters.
7.3.1
Summary Flow Functions
A simple view of interprocedural analysis is to model a procedure call as a basic
block and represent the eﬀect of the called procedure by a summary ﬂow function.
Since it needs to represent the eﬀect of all calls to the procedure it represents, a
summary ﬂow function must be context independent and must be parametrized so
that the data ﬂow information from the calling context can be incorporated.
A summary ﬂow function fr : L '→L for procedure r can be modeled in the usual
manner in terms of Gen and Kill components as shown below:
fr(x) = (x−Killr(x)) ∪Genr(x)
= (x−(ConstKillr ∪DepKillr(x))) ∪(ConstGenr ∪DepGenr(x))
Note that this merely models the function fr; whether fr is actually constructed by
identifying ConstKill, DepKill, ConstGen, and DepGen is an independent matter
and is discussed in Section 7.3.3. Chapter 8 discusses how it is constructed; we
introduce some intuitions related to it in Section 7.6.
Although the notions of Genr and Killr for a procedure r are similar to the notions
of Geni and Killi for a basic block i, there are some diﬀerences arising from the
fact that the execution of a procedure may involve control transfers whereas a basic
block involves a strictly sequential execution. Thus we need to distinguish between
may and must properties. For example, when performing liveness analysis, Killr
must ensure that a variable is modiﬁed along all paths in r. This is represented by
MustKillr which is diﬀerent from MayKillr; the latter says that a variable is modiﬁed
along some path but not necessarily all. For available expressions analysis, Killr
should be MayKillr rather than MustKillr.
We now describe the summary ﬂow functions for constant propagation and live-
ness analysis of our example program. Consider the instance of constant propagation
framework involving our example program. Let x ∈L be the tuple xa,xb,xc,xd rep-
resenting the constantness information of the four variables in our example program.
Thus,xa,xb,xc, andxd are values in the component lattice L for constant propagation
(Figure 4.5 on page 110).
From the supergraph in Figure 7.2, it is clear that the data ﬂow values of a and d
remain unaﬀected by procedure p since it does not modify them. Further, variable b
is always 2 at the end of procedure p regardless of the ﬂow of execution. The data
ﬂow value of variable c depends on result of the condition in block Startp. If the
execution follows edge Startp →n3, the data ﬂow value of c becomes xa +2. The
alternative execution path involving edge Startp →Endp does not modify c. Static
© 2009 by Taylor & Francis Group, LLC

Introduction to Interprocedural Data Flow Analysis
237
summarization of the two possibilities results inxc (xa +2). Thus, the ﬂow function
that summarizes the eﬀect of procedures p is:
fp(xa,xb,xc,xd) = xa,2,xc (xa +2),xd
To see the ﬂow function in terms of Gen and Kill, observe that the data ﬂow infor-
mation x = xa,xb,xc,xd is merely a convenient notation for the set representation
x =
 
a,xa,b,xb,c,xc,d,xd
!
. Thus, the Gen and Kill components of fp are:
ConstGenp = {b,2}
ConstKill p = ∅
DepGenp(x) = {c,xc (xa +2)}
DepKill p(x) = {b,xb,c,xc}
Since procedure q calls procedure p, the deﬁnition of fq depends on the deﬁnition
of fp. In particular, procedure q assigns 1 to a and then passes on the resulting data
ﬂow information 1,xb,xc,xd to fp. The resulting intermediate ﬂow function deﬁnes
the data ﬂow at R3 in terms of the assumed input value x = xa,xb,xc,xd available at
Startq. When the ﬂow function of block Endq is composed with it, we get
fq(xa,xb,xc,xd) = 2,2,xc 3,xd
For live variables analysis, Var = {a,b,c,d} and L is 2Var. We leave it for the reader
to verify that the ﬂow functions for procedures p and q are:
fp(x) = (x−{b}) ∪{a,c,d}
fq(x) = (x−{a,b}) ∪{c,d}
where x ⊆{a,b,c,d}.
7.3.2
Inherited and Synthesized Data Flow Information
For a given call to procedure r in the body of procedure s, let x be the data ﬂow infor-
mation reaching the call point. Then, x represents the data ﬂow information inherited
by procedure r from the call site in s and fr(x) represents the data ﬂow information
synthesized by r at the call site in s. This is illustrated in Figure 7.3. The inherited
data ﬂow information is context sensitive. The synthesized data ﬂow information
has a context insensitive component represented by ConstGenp and a context sensi-
tive component represented by DepGenp(x) and x−

ConstKill p ∪DepKill p(x)

. The
ﬁnal data ﬂow information at a program point u in procedure r is inﬂuenced by
• interprocedural data ﬂow information inherited by r from all calls to r,
• interprocedural data ﬂow information synthesized by calls appearing on the
paths from Startr to u for forward ﬂows and from u to Endr for backward
ﬂows, and
© 2009 by Taylor & Francis Group, LLC

238
Data Flow Analysis: Theory and Practice
Startr
Endr
Starts
Ends
Ci
Ri
ci
Startt
Endt
C j
R j
cj
x
x
x
x
y
y
y
y
Data Flow Information
x
Inherited by r from
call site ci in s
y
Inherited by r from
call site cj in t
x
Synthesized by r in
s at call site ci
y
Synthesized by r in
t at call site cj
FIGURE 7.3
Inherited and synthesized data ﬂow information.
• intraprocedural data ﬂow information along the paths from Startr to u for for-
ward ﬂows and from u to Endr for backward ﬂows.
In Part I of the book, the interprocedural data ﬂow information was approximated
as follows: The inherited data ﬂow information was approximated by a conserva-
tive value of BI and the synthesized data ﬂow information was approximated by
using ﬁxed conservative values for Gen(x) and Kill(x). These approximations were
independent of calls and were same for all calls to all procedures in the program.
Interprocedural data ﬂow analysis tries to replace the above approximations by more
precise values.
For constant propagation in our example, procedure p has a call from main and
a call from q. The context insensitive synthesized data ﬂow information of p is
{b,2}.
It inherits x = 5,3,7,⊥ from its call in main.
Since we wish to sep-
arate the data ﬂow information associated with diﬀerent variables, we view x as
{a,5,b,3,c,7,d,⊥}. The context sensitive synthesized data ﬂow information
for this call is {a,5,c,7,d,⊥}. This is the data ﬂow information associated
with block R1 in the caller procedure main. The data ﬂow information inherited
by p from its call in q is 1,2,7,14. The corresponding context sensitive synthe-
sized data ﬂow information associated with block R3 in the caller procedure q is
{a,1,c,⊥,d,14}.
7.3.3
Approaches to Interprocedural Data Flow Analysis
Various methods of interprocedural data ﬂow analysis can be divided into two broad
categories: functional approach or a value-based approach.
A functional approach to interprocedural analysis consists of two steps: In the ﬁrst
step, the summary ﬂow functions that represent the eﬀects of a call are computed.
These functions are context independent and are parametrized. In the second step,
inherited data ﬂow information of a procedure is computed from its calling contexts.
Then, the body of the procedure is analyzed and the summary ﬂow functions cor-
responding to the callee are used to compute the synthesized data ﬂow information.
© 2009 by Taylor & Francis Group, LLC

Introduction to Interprocedural Data Flow Analysis
239
Observe that using the summary functions does not require traversing the body of
the caller procedures represented by the functions. In practice, computation of sum-
mary ﬂow functions is possible only for a limited class of frameworks. In particular,
it is easy for separable frameworks. In non-separable frameworks, it may not be
possible to automatically construct summary ﬂow functions unless the lattice is ﬁ-
nite and ﬂow functions are distributive. This is because constructing summary ﬂow
functions requires reducing expressions involving function compositions and inter-
sections. Whether a systematic method of reductions can be devised or not depends
on the nature of the ﬂow functions and data ﬂow values.
A value-based approach avoids computing summary ﬂow functions. Instead, it
directly computes data ﬂow values by traversing a program during analysis. In par-
ticular, when it encounters a procedure call, the inherited data ﬂow information is
propagated to the callee and the method starts examining the callee’s body. At the
end of the analysis of the callee’s body, synthesized data ﬂow information is propa-
gated back to the caller and the analysis of caller’s body is resumed. This approach
requires traversing a procedure repeatedly for diﬀerent calling contexts. Conceptu-
ally, this approach is simpler than functional approach except that it may have to
distinguish between a large number of contexts.
Both these approaches inherently handle recursion so long as the frameworks in-
volve ﬁnite lattices. Although our example program in this chapter is non-recursive,
subsequent chapters present these approaches for recursive programs.
7.4
Compromising Precision for Scalability
Recall that the scope of intraprocedural data ﬂow analysis is restricted to individual
procedures. By contrast, interprocedural data ﬂow analysis needs to examine entire
programs. Although this increases the precision of data ﬂow information, practically
interprocedural analysis could be very ineﬃcient both in terms of space as well as
time. Since real life applications often contain hundreds or thousands of procedures,
a supergraph is many times larger than a single CFG. Hence eﬃciency and scalability
issues assume much more signiﬁcance in interprocedural data ﬂow analysis than
in intraprocedural data ﬂow analysis. Most approaches that achieve eﬃciency and
scalability, compromise on precision in one way or the other. Two common tradeoﬀs
that enhance eﬃciency and scalability are:
• Not distinguishing between actual and spurious control ﬂow paths.
This manifests itself in the form of ﬂow or context insensitivity.
• Restricting the inﬂuences between caller and callees.
This results in side eﬀects analysis instead of whole program analysis.
In this section we explore these tradeoﬀs and explain how they aﬀect the precision
of interprocedural data ﬂow analysis. Empirical investigations have revealed that
© 2009 by Taylor & Francis Group, LLC

240
Data Flow Analysis: Theory and Practice
0 f0 0
1 f1 1
2 f2 2 3 f3 3
i fi i
m fm m
Start
0 f0 01 f1 12 f2 23 f3 3 ...
i fi i ...
m fm m
End
(a) CFG
(b) Flow insensitive analysis
FIGURE 7.4
Modeling ﬂow insensitive analysis.
these tradeoﬀs enhance the eﬃciency of analysis signiﬁcantly. The resulting loss of
precision has been found to be tolerable in many cases but not all.
7.4.1
Flow and Context Insensitivity
Recall that the MOP value associated with a program point u is the glb of data ﬂow
information computed along all paths reaching u (Deﬁnition 3.20). Let P(u) denote
the set of paths used for computing data ﬂow information at u. If P(u) ⊇paths(u),
then a data ﬂow value computed along all paths in P(u) is weaker than MOPu and
hence is safe. Precision of the data ﬂow value computed by traversing paths in P(u)
depends on how close P(u) is to paths(u). The larger the number of spurious paths
in P(u), the more imprecise the computed data ﬂow value is likely to be.
As observed in Section 3.4.3, computing the MOP assignment for arbitrary mono-
tone frameworks is undecidable. Thus the algorithms that need to cover all potential
paths can at best compute the MFP solution (Section 3.4.2). This involves merging
data ﬂow information at shared program points in paths(u). If the ﬂow functions
are non-distributive, this has the eﬀect of creating combinations of data ﬂow values
across paths (Example 4.6). This can be seen as traversing some paths that are not
present in paths(u). This source of imprecision shows the limit of static analysis and
hence is accepted as inevitable.
We now describe two features called ﬂow and context insensitivity that a method
can employ as a matter of choice for achieving eﬃciency. They are orthogonal but
are similar in the sense that both of them relate to spurious paths; they are diﬀerent
in the nature of paths they consider.
© 2009 by Taylor & Francis Group, LLC

Introduction to Interprocedural Data Flow Analysis
241
Flow insensitivity
As mentioned in Section 1.2, ﬂow insensitive analysis disregards the ﬂow of control
by implicitly assuming that the block can be executed in all possible orders. This is
achieved by accumulating the eﬀect of each block in the same data ﬂow value and
the resulting value is a safe approximation of data ﬂow information at each point.
For convenience, let the blocks in a procedure be numbered from 0 to m in any
arbitrary order. Then, ﬂow insensitive analysis computes x ∈L as deﬁned below:
x =
m
i=0
fi(BI)
(7.1)
where BI ∈L is the boundary information. This is illustrated in Figure 7.4.
Intuitively, the operation of function composition employed in the usual ﬂow sen-
sitive data ﬂow analysis is replaced by the operation of function conﬂuence; the
latter is commutative while the former is not. Thus just a single visit to each block in
any arbitrary order approximates all possible orders between blocks. Section 8.1.2
shows that the value x computed by Equation (7.1) is a safe approximation of the
corresponding ﬂow sensitive data ﬂow information at each program point.
In the case of ﬂow functions with dependent parts, the above model of ﬂow in-
sensitive computation needs to be modiﬁed slightly. This is because the dependent
component of fi could depend on a value computed by some fj and since the state-
ments are assumed to be executed in an arbitrary order, this dependence must be
taken care of. For example, consider ﬂow insensitive may points-to analysis for the
pointer assignments in Figure 7.5(a). Block n3 generates a points-to pair b d and
since we assume that n2 could be potentially executed after n1 or n3, our analysis
should discover the points-to pairs a c and a d. Following the strategy of Fig-
ure 7.4(b) we would get the ﬂow graph in Figure 7.5(b) and it will not compute the
desired points-to pairs. A simple way of modeling ﬂow insensitive analysis in such a
situation is to extend the graph by adding edges from n1 and n3 to n2 as shown in Fig-
ure 7.5(c). Observe that these are data ﬂow dependences captured by the primitive
entity functions and the composite entity functions described in Section 4.5. They
are diﬀerent from the dependences of values of variables at run time which may or
may not create dependences of data ﬂow values.
In practice, instead of creating such ﬂow graphs, the required dependences are
remembered in a global data structure. Points-to analysis constructs a graph that
contains points-to edges as well as constraints that result in points-to edges. Thus,
edge b c is added while processing n1. When n2 is processed, edge a
∗b is also
remembered apart from adding the edge a c. Whenever new points-to information
for b becomes available, an appropriate points-to edge is added to the graph.
We now introduce the issues that arise when we wish to construct a ﬂow insensi-
tive summary ﬂow function instead of computing a ﬂow insensitive data ﬂow value.
These issues are handled in details in Chapter 8. We consider the following two cases
in constructing ﬂow insensitive summary functions:
• When the ﬂow functions do not have dependent parts.
© 2009 by Taylor & Francis Group, LLC

242
Data Flow Analysis: Theory and Practice
n1 :
b = &c;
...
n2 :
a = b;
...
n3 :
b = &d;
Start
End
n1
n2
n3
Start
End
n1
n2
n3
(a) Pointer assignment
(b) Default modeling
(c) Required modeling
FIGURE 7.5
Modeling ﬂow insensitive analysis in presence of dependent parts in ﬂow functions.
Edges n1 →n2 and n3 →n2 represent the fact that DepGenn2(x) depends on the data
ﬂow information computed at n1 and n3.
If ﬂow functions have only constant parts (as in liveness analysis), then the
summary ﬂow function can be constructed by computing the values of the
constant parts. In particular, for constructing side eﬀect function of procedure
r for liveness analysis, we need to compute only ConstKillr and ConstGenr
sets.
The ConstKill set for live variables should include only those variables that
are guaranteed to be modiﬁed within the called procedure regardless of the
order of execution of basic blocks. This is represented by ﬂow insensitive
MustKill set which is computed using Equation (7.1) by intersecting the Kill
sets of the individual basic blocks in the procedure. This should be contrasted
with ConstGen computation which must record every variable that becomes
live locally within the called procedure regardless of the control ﬂow. This is
represented by ﬂow insensitive MayUse set which is computed using Equa-
tion (7.1) by taking a union of the Gen sets of individual basic blocks in the
procedure. Then,
fr(x) = (x−MustKillr) ∪MayUser
where x ∈L. Both these approaches are demonstrated for our example program
in Section 7.6 although their detailed formal deﬁnitions are provided later in
Chapter 8.
• When the ﬂow functions have dependent parts.
In this case, merely combining the Gen and Kill sets does not work. Instead,
we will have to replace BI ∈L by a symbolic value that represents the data
ﬂow value in the calling context and parametrizes the summary ﬂow function.
For simplicity, we describe this for liveness analysis. A symbolic value for
liveness analysis could be the following set:
BI =
 
a,xa | a ∈Gvar
!
© 2009 by Taylor & Francis Group, LLC

Introduction to Interprocedural Data Flow Analysis
243
where xa is a symbolic value that will be replaced by a concrete value true or
false from the calling context. The ﬂow function fi to be used in Equation (7.1)
will also have to be re-written as:
fi(x) = x−Removei
 ∪Addi
Removei = {a,xa | a ∈Killi,a  Geni}
Addi = {a,true | a ∈Geni} ∪
{a,false | a ∈Killi,a  Geni}
a,true ∈x indicates that variable a is live and a,false ∈x indicates that
variable a is not live; exactly one of them is in x by construction. The conﬂu-
ence operation over the sets of pairs is deﬁned as follows for liveness analysis:
x ∪y = {a,xa +ya | a,xa ∈x,a,ya ∈y}
where + denote the boolean OR operation.
In the presence of dependent parts in ﬂow functions, it may not always be
possible to construct summary ﬂow functions. In Chapter 8 we characterize
the class of frameworks for which summary ﬂow functions can be directly
constructed. For others, either it is not possible to construct summary ﬂow
functions or some adhoc mechanism may have to be employed. For exam-
ple, it is not possible to construct ﬂow sensitive summary ﬂow functions for
points-to analysis. However, ﬂow insensitive summary ﬂow functions can be
constructed by building a points-to graph which has been explained before.
An application of such a summary ﬂow function requires traversing the graph.
In our example program, assuming that it is known that the value of b is 2 after
every call to procedure p, a ﬂow insensitive analysis of procedure q would conclude
that a could be both 1 and 2 and hence is not constant in q. This is a safe conclusion
when only gross information instead of ﬁne grained point-speciﬁc information about
q is desired.
In general, ﬂow insensitive analysis is not common at the intraprocedural level.
Context insensitivity
A calling context is represented by the snapshot of the control stack at run time.
During program analysis, it is determined by the sequence of unﬁnished calls in a
path in the supergraph.
As explained in Chapter 1, context insensitive analysis does not distinguish be-
tween diﬀerent calling contexts. Instead, the inherited data ﬂow information from all
contexts is merged and the resulting synthesized data ﬂow information is propagated
to all calling contexts indiscriminately. This implies traversing interprocedurally in-
valid paths—paths in which calls and returns do not match. In essence, there is no
distinction between the interprocedural and intraprocedural edges in a supergraph.
In our program, procedure p inherits 5,3,7,⊥ from its call in the main and
1,2,7,14 from its call in procedure q. The merged value is ⊥,⊥,7,⊥ and the
© 2009 by Taylor & Francis Group, LLC

244
Data Flow Analysis: Theory and Practice
resulting synthesized value ⊥,2,7,⊥ is propagated back to both the callers of p.
As a consequence, such an analysis fails to discover the fact that a is constant with
value 5 at the entry of block n1. Eﬀectively, this is a consequence of propagating
the value a = 1 from Startq to n1. Although there is a path from Startq to n1 in the
supergraph, it does not represent matching calls and returns: Data ﬂow information
computed along the path from Startq to Endp should be propagated to R3 and not to
R1 because the last call in this path represents a call to p from q and not from main.
Context sensitive analysis excludes such paths and restricts P(u) to interprocedurally
valid paths.
The issue of context sensitivity does not arise at the intraprocedural level.
7.4.2
Side Eﬀects Analysis
Interprocedural analysis requires incorporating the mutual inﬂuence of callers and
callees on each other. This requires computing both inherited and synthesized part
of data ﬂow information. We call such an analysis, a whole program analysis. This
should be contrasted with the situation when only callee’s inﬂuence on callers is
computed. This is achieved by computing the synthesized part of interprocedural
data ﬂow information; the inherited part is approximated by a ﬁxed value for each
procedure. Traditionally, such analyses have been called side eﬀects analyses.
A side eﬀects analysis can also have some variations depending upon whether only
the context insensitive side eﬀects are computed or the context sensitive side eﬀects
are also computed. For a given procedure p, the context insensitive side eﬀects are
represented by ConstGenp while the context sensitive side eﬀects are represented
by DepGenp(x) and x−(ConstKill p∪DepKill p(x)). The former is much simpler but
less useful compared to the latter.
For a given procedure call, side eﬀect analysis restricts the scope of optimization
to the caller whereas whole program analysis facilitates optimization in both caller
and callee. For example, if interprocedural live variables analysis is performed using
side eﬀects, it is possible to decide whether a value in a register should be preserved
across a procedure call. The transformation resulting from this decision is restricted
to a caller’s body. However, if whole program analysis is performed, it may be
possible to assign the same register to a variable both within a caller and its callee.
7.5
Language Features Inﬂuencing Interprocedural Analysis
Interprocedural data ﬂow analysis is inﬂuenced by language features that support
high level abstractions related to procedure calls.
In this chapter, we have deliberately used a non-recursive program to introduce in-
terprocedural data ﬂow analysis. In the presence of recursion, functional approaches
require ﬁxed point computation to construct summary ﬂow functions. Convergence
© 2009 by Taylor & Francis Group, LLC

Introduction to Interprocedural Data Flow Analysis
245
of this computation needs to be established by examining the ﬂow functions and data
ﬂow values in the framework. Since the value-based approaches have to explicitly
remember contexts, a mechanism of summarizing the contexts needs to be devised.
For the frameworks with ﬁnite lattices, it is possible to bound the number of contexts
by a ﬁnite number without compromising on precision. However, the number of con-
texts remains very large. Thus recursion aﬀects both the feasibility and the eﬃciency
of interprocedural data ﬂow analysis signiﬁcantly. Many practical value-based ap-
proaches perform context insensitive analysis in the recursive portions of programs.
However, it is possible to perform context sensitive interprocedural analysis in the
presence of recursions. We present such methods in Chapters 8 and 9.
The other simplifying feature of our program was that it did not involve parameters
and local variables. In practice, parameterless procedures are rare and it is important
to handle the parameter passing mechanism because computation of inherited data
ﬂow information requires transferring the data ﬂow information of actual parameters
to that of the corresponding formal parameters. For this purpose, the call by value
parameter passing mechanism can be modeled by simple assignments whereas call
by reference parameter passing mechanism should be modeled by pointer assign-
ments. Further, distinction should be made between global variables and local vari-
ables for inherited and synthesized data ﬂow information. Unless local variables are
involved in the actual parameters of a procedure call, synthesized data ﬂow informa-
tion should not be computed for local entities nor should their data ﬂow information
be propagated as a part of the inherited data ﬂow information of the callee. Recall
that the motivating example of heap data analysis presented in Section 1.1 contains
local pointer variables that are passed as actual parameters. Section 9.5 performs
interprocedural liveness analysis for that example and describes how transfer of data
ﬂow information between actual and formal parameters can be modeled.
Further, in the presence of parameter passing by reference, depending upon the
actual parameters a particular call may create aliasing between formal parameters
or between formal parameters and global variables within the callee’s body. This
may aﬀect the correctness or precision of the data ﬂow information discovered. Sec-
tion 8.2 shows how such aliasing can be discovered.
Some languages support local functions. This inﬂuences interprocedural analysis
in the following ways: (a) The possible call structure in a program is governed by
the scope rules of the language that restrict the visibility of local procedures. (b) The
notion of global variables must now be replaced by the notion of non-local variables
that depend on the scope of a procedure.
Function pointers and subtyping mechanism resulting in dynamic dispatch of func-
tion calls hide the identity of the called procedures at compile time making the static
call structure imprecise. Exception handling mechanisms of a language have a simi-
lar eﬀect. Interprocedural data ﬂow analyses are restricted to single threads, similar
to intraprocedural data ﬂow analysis. Use of library functions imply that the entire
source is not available to an interprocedural analyzer and a summary of their eﬀects
must be provided explicitly.
© 2009 by Taylor & Francis Group, LLC

246
Data Flow Analysis: Theory and Practice
7.6
Common Variants of Interprocedural Data Flow Analysis
We introduce the following common variants using our running example.
• Intraprocedural analysis with conservative approximation. We use conser-
vative approximation of inherited and synthesized data ﬂow information for
handling procedure calls.
• Intraprocedural analysis with side eﬀects. We compute ﬂow sensitive as well
as ﬂow insensitive side eﬀects and represent them by context independent ﬂow
functions.
• Whole program analysis. We perform context sensitive as well as context
insensitive analysis.
In each case, the data ﬂow information in the caller procedures is computed in ﬂow
sensitive manner and the data ﬂow value associated each program point is computed
separately. In the case of ﬂow insensitive side eﬀects, only the eﬀect of a call is ﬂow
insensitive—the data ﬂow values computed in the caller’s body are ﬂow sensitive.
Although ﬂow insensitive analysis of all procedures has also been used in practice,
it computes a single summary data ﬂow value per procedure which is usually very
imprecise. For example, a ﬂow insensitive constant propagation of our program
computes the data ﬂow value ⊥,⊥,⊥,⊥ for procedures main and q and, ⊥,2,⊥,⊥
for procedure p. This value is same regardless of the variant. Flow sensitive version
of these variants compute data ﬂow values with varying degrees of precision.
7.6.1
Intraprocedural Analysis with Conservative Interprocedural Ap-
proximation
Intraprocedural analysis with conservative interprocedural approximation involves
using safe values for inherited and synthesized data ﬂow information. This approach
was introduced in Section 1.1 analysis of heap data.
The inherited data ﬂow information for constant propagation is represented by
BImain = 0,0,0,0 and BIp = BIq = ⊥,⊥,⊥,⊥. This distinction arises from the fact
that all our variables are global variables which are initialized to 0; however, their
values cannot be assumed to be known when other procedures are invoked. For local
variables, the value in BI is  but our program does not have local variables. For live
variables analysis, BImain = ∅because no variable is live at the end of the program.
However, all global variables should be conservatively assumed to be live at the end
of other procedures, hence BIp = BIq = {a,b,c,d}.
The synthesized data ﬂow information for constant propagation is conservatively
represented by ⊥,⊥,⊥,⊥ under the assumption that a function call could modify
all variables. For live variables analysis, the synthesized data ﬂow information is
{a,b,c,d} because it is conservatively assumed that all global variables are live at the
© 2009 by Taylor & Francis Group, LLC

Introduction to Interprocedural Data Flow Analysis
247
Block and
associated
data ﬂow
value
Intraprocedural
analysis with
conservative
interprocedural
approximation
Side Eﬀects Analysis
Whole Program Analysis
Flow sensitivity of
synthesized information
Context sensitivity of
inherited information
Insensitive
Sensitive
Insensitive
Sensitive
Startm
In
0,0,0,0
0,0,0,0
0,0,0,0
0,0,0,0
0,0,0,0
Out
5,3,7,⊥
5,3,7,⊥
5,3,7,⊥
5,3,7,⊥
5,3,7,⊥
C1
In,Out
5,3,7,⊥
5,3,7,⊥
5,3,7,⊥
5,3,7,⊥
5,3,7,⊥
R1
In,Out
⊥,⊥,⊥,⊥
5,⊥,⊥,⊥
5,2,7,⊥
⊥,2,⊥,⊥
5,2,7,⊥
n1
In
⊥,⊥,⊥,⊥
5,⊥,⊥,⊥
5,2,7,⊥
⊥,2,⊥,⊥
5,2,7,⊥
Out
⊥,⊥,⊥,⊥
7,⊥,⊥,⊥
7,2,7,⊥
⊥,2,⊥,⊥
7,2,7,⊥
n2
In
⊥,⊥,⊥,⊥
7,⊥,⊥,⊥
7,2,7,⊥
⊥,2,⊥,⊥
7,2,7,⊥
Out
⊥,⊥,⊥,⊥
7,⊥,⊥,⊥ 7,2,7,14 ⊥,2,⊥,⊥
7,2,7,14
C2
In,Out
⊥,⊥,⊥,⊥
7,⊥,⊥,⊥ 7,2,7,14 ⊥,2,⊥,⊥
7,2,7,14
R2
In,Out
⊥,⊥,⊥,⊥
⊥,⊥,⊥,⊥ 2,2,⊥,14 ⊥,2,⊥,⊥
2,2,⊥,14
Endm
In,Out
⊥,⊥,⊥,⊥
⊥,⊥,⊥,⊥ 2,2,⊥,14 ⊥,2,⊥,⊥
2,2,⊥,14
Startp
In
⊥,⊥,⊥,⊥
⊥,⊥,⊥,⊥ ⊥,⊥,⊥,⊥ ⊥,⊥,⊥,⊥
⊥,⊥,7,⊥
Out
⊥,2,⊥,⊥
⊥,2,⊥,⊥ ⊥,2,⊥,⊥ ⊥,2,⊥,⊥
⊥,2,7,⊥
n3
In
⊥,2,⊥,⊥
⊥,2,⊥,⊥ ⊥,2,⊥,⊥ ⊥,2,⊥,⊥
⊥,2,7,⊥
Out
⊥,2,⊥,⊥
⊥,2,⊥,⊥ ⊥,2,⊥,⊥ ⊥,2,⊥,⊥
⊥,2,⊥,⊥
Endp
In,Out
⊥,2,⊥,⊥
⊥,2,⊥,⊥ ⊥,2,⊥,⊥ ⊥,2,⊥,⊥
⊥,2,⊥,⊥
Startq
In
⊥,⊥,⊥,⊥
⊥,⊥,⊥,⊥ ⊥,⊥,⊥,⊥ ⊥,2,⊥,⊥
7,2,7,14
Out
1,⊥,⊥,⊥
1,⊥,⊥,⊥ 1,⊥,⊥,⊥
1,2,⊥,⊥
1,2,7,14
C3
In,Out
1,⊥,⊥,⊥
1,⊥,⊥,⊥ 1,⊥,⊥,⊥
1,2,⊥,⊥
1,2,7,14
R3
In,Out
⊥,⊥,⊥,⊥
1,⊥,⊥,⊥
1,2,⊥,⊥
⊥,2,⊥,⊥
1,2,⊥,14
Endq
In
⊥,⊥,⊥,⊥
1,⊥,⊥,⊥
1,2,⊥,⊥
⊥,2,⊥,⊥
1,2,⊥,14
Out
⊥,⊥,⊥,⊥
⊥,⊥,⊥,⊥ 2,2,⊥,⊥
⊥,2,⊥,⊥
2,2,⊥,14
FIGURE 7.6
Constant propagation in our example program using ﬂow sensitive version of com-
mon variants of interprocedural data ﬂow analysis.
entry of p and q. This does not contradict the assumption made for constant propa-
gation because although a global variable may be modiﬁed in the callee procedure,
it cannot be guaranteed to be modiﬁed along all paths before being used. These
assumptions are safe because they cannot enable incorrect optimizations.
From Figure 7.6, it is easy to see that intraprocedural analysis limits the scope
of constant propagation to a single procedure and disables it across function calls.
As illustrated in Figure 7.7 on the following page, only variable b in blocks Startp
and n3 is replaced by its value which happens to be 2. The result of performing
liveness analysis on the program obtained after constant propagation is shown in
Figure 7.7 on the next page. Our analysis concludes that all left hand side variables
in the assignments are live after the assignments. Thus this variant of analysis fails
to enable dead code elimination in our example program.
© 2009 by Taylor & Francis Group, LLC

248
Data Flow Analysis: Theory and Practice
a = 5;b = 3
c = 7;read d
Startmain
Call p
C1
Call p
R1
n1
a = a +2
print c+d n1
n2
d = a ∗b n2
Call q
C2
Call q
R2
print a +c
Endmain
b = 2
if ( 2 < d)
Startp
n3 c = a+ 2
n3
print c+d
Endp
T
F
a = 1
Startq
Call p
C3
Call p
R3
a = a∗b
Endq
Block n Inn
Outn
Startmain ∅
{a,b,c,d}
C1
{a,b,c,d} {a,b,c,d}
R1
{a,b,c,d} {a,b,c,d}
n1
{a,b,c,d} {a,b,c}
n2
{a,b,c}
{a,b,c,d}
C2
{a,b,c,d} {a,b,c,d}
R2
{a,c}
{a,c}
Endmain {a,c}
∅
Startp
{a,c,d}
{a,b,c,d}
n3
{a,b,d}
{a,b,c,d}
Endp
{a,b,c,d} {a,b,c,d}
Startq
{b,c,d}
{a,b,c,d}
C3
{a,b,c,d} {a,b,c,d}
R3
{a,b,c,d} {a,b,c,d}
Endq
{a,b,c,d} {a,b,c,d}
FIGURE 7.7
Intraprocedural liveness analysis after intraprocedural constant propagation. Propa-
gated constants are shown in circles.
7.6.2
Intraprocedural Analysis with Side Eﬀects Computation
Side eﬀect analysis discovers which variables are actually modiﬁed in a procedure
calls. Hence it can compute more precise synthesized data ﬂow information. Side
eﬀect computation could be ﬂow insensitive or ﬂow sensitive. We compute the data
ﬂow information in the body of a caller in a ﬂow sensitive manner. In either case,
since no data ﬂow information is inherited, the value of BI is same as in intraproce-
dural analysis.
After computing the side eﬀects, function calls are treated as basic blocks and
conventional intraprocedural analysis is performed.
Flow insensitive side eﬀects
We present two methods of computing ﬂow insensitive side eﬀects. The ﬁrst method
uses symbolic values to parametrize the context. The other method works for frame-
works in which the ﬂow functions do not contain dependent parts. This method
computes the values of ConstGen and ConstKill components of the summary side
eﬀect ﬂow function of a procedure explicitly. We illustrate the former method for
constant propagation and the latter for live variables analysis.
• Flow insensitive side eﬀects for constant propagation.
© 2009 by Taylor & Francis Group, LLC

Introduction to Interprocedural Data Flow Analysis
249
Startp
b = 2;
c = a+b
print c+d
Endp
xa,xb,xc,xd
xa,xb,xc,xd
xa,2,xc,xd
xa,xb,xc,xd
xa,xb,xc,xd
xa,xb 2,(xa +(xb 2)),xd
xa,2,xc,xd
Startq
a = 1
ci
a = a∗b
Endq
xa,xb
xa,xb
xa 1,xb 2
⊥
⊥,xb 2
1,xb
1,xb
⊥,xb 2
xa,xb
⊥,xb 2
1,xb
FIGURE 7.8
Computing ﬂow insensitive side eﬀect functions for procedures p and q. An edge
u →v denotes the fact that DepGenv(x) depends on the data ﬂow value at u; the re-
quired data ﬂow values have been shown along with the out edges of u. For procedure
q, we show the values of variables a and b only.
Recall that a systematic construction of summary ﬂow functions is possible
only for a limited set of data ﬂow frameworks; in general, it is not possible for
full constant propagation. Here we use Equation (7.1) intuitively to symbol-
ically compute summary function for constant propagation and illustrate the
diﬃculty in automatic construction of ﬂow functions for constant propagation.
Computation of ﬂow insensitive side eﬀect summary ﬂow functions for proce-
dure p and q are illustrated in Figure 7.8. It is easy to see that:
fp(xa,xb,xc,xd) = xa,xb 2,xc (xa +(xb 2)),xd
For computing the summary side eﬀect function for q, we need to incorporate
the eﬀect of procedure p too. For simplicity, only the computation for vari-
ables a and b for procedure p is illustrated in Figure 7.8. The expression that
represents the data ﬂow value of a after processing the assignment a = a ∗b is

xa 1 

(xa 1)∗(xb 2)

. Using monotonicity of the ﬂow function repre-
© 2009 by Taylor & Francis Group, LLC

250
Data Flow Analysis: Theory and Practice
a = 5;b = 3
c = 7;read d
Startmain
Call p
C1
Call p
R1
n1
a = 7
print c+d n1
n2
d = 7 ∗b n2
Call q
C2
Call q
R2
print a +c
Endmain
b = 2
if ( 2 < d)
Startp
n3 c = a+ 2
n3
print c+d
Endp
T
F
a = 1
Startq
Call p
C3
Call p
R3
a = 1 ∗b
Endq
Block n Inn
Outn
Startmain ∅
{a,b,c,d}
C1
{a,b,c,d} {a,b,c,d}
R1
{b,c,d}
{b,c,d}
n1
{b,c,d}
{a,b,c}
n2
{a,b,c}
{a,b,c,d}
C2
{a,b,c,d} {a,b,c,d}
R2
{a,c}
{a,c}
Endmain {a,c}
∅
Startp
{a,c,d}
{a,b,c,d}
n3
{a,b,d}
{a,b,c,d}
Endp
{a,b,c,d} {a,b,c,d}
Startq
{b,c,d}
{a,b,c,d}
C3
{a,b,c,d} {a,b,c,d}
R3
{b,c,d}
{b,c,d}
Endq
{b,c,d}
{a,b,c,d}
FIGURE 7.9
Interprocedural liveness analysis after interprocedural constant propagation using
ﬂow insensitive side eﬀects. The resulting values after constant propagation and
constant folding are shown in circles.
senting multiplication in constant propagation, it can be reduced as follows:
xa 1 

(xa 1)∗(xb 2)

 xa 1

(xa ∗xb)(xa ∗2)(1 ∗2)(1 ∗xb)

 xa 1(xa ∗xb)(xa ∗2)2 xb
 ⊥
Intuitively, a can be both 1 and 2, hence it must be ⊥. Using this, the ﬁnal
summary side eﬀect function is:
fq(xa,xb,xc,xd) = ⊥,xb 2,⊥,xd
Observe that devising a systematic method that can perform the reductions
such as above is not easy.
The details of constants discovered in each basic block are shown in Figure 7.6
on page 247. The resulting constant propagation is shown in Figure 7.9. Ob-
serve that the number 7 in blocks n1 and n2 is a result of constant folding. The
use of ﬂow insensitive side eﬀects in intraprocedural analysis results in more
precise data ﬂow information compared to the data ﬂow information computed
using the conservative approximation of function calls.
© 2009 by Taylor & Francis Group, LLC

Introduction to Interprocedural Data Flow Analysis
251
• Flow insensitive side eﬀects for live variables analysis.
We need to compute MustKill and MayUse sets for procedures p and q. We
compute them for the program in Figure 7.9 on the facing page.
MustKill p = KillStartp ∩Killn3 ∩KillEndp = ∅
MustKillq = KillStartq ∩MustKill p ∩KillEndq = ∅
MayUse p = GenStartp ∪Genn3 ∪GenEndp = {a,c,d}
MayUseq = GenStartq ∪MayUse p ∪GenEndq = {a,b,c,d}
fp(x) = (x−MustKill p) ∪MayUse p = x ∪{a,c,d}
fq(x) = (x−MustKillq) ∪MayUseq = x ∪{a,b,c,d}
Note that fp(x) is a little better than the conservative approximations used in
Section 7.6.1 in that it does not contain b. However, due to ﬂow insensitivity, it
does not recognize that b is killed in procedure p. Hence, the use of b in block
n2 cause b to be considered live at the exit of Startmain. The resulting data ﬂow
information after performing constant propagation using ﬂow insensitive side
eﬀects is shown in Figure 7.9 on the preceding page. Observe that no variable
is dead immediately after its assignment hence dead code elimination is not
possible using this variant also in spite of the fact that some more constant are
discovered and liveness information has become more precise.
Flow sensitive side eﬀects
As in the previous section, we compute Kill and Gen implicitly for constant propa-
gation and explicitly for live variables analysis.
Flow sensitive side eﬀects for constant propagation can be computed by perform-
ing data ﬂow analysis over a called procedure with BI = xa,xb,xc,xd. The resulting
ﬂow functions are represented by the symbolic data ﬂow values at the exit of the
function. It is easy to see that:
fp(xa,xb,xc,xd) = xa,2,xc (xa +2),xd
fq(xa,xb,xc,xd) = 2,2,xc 3,xd
It is clear from Figure 7.6 on page 247 that ﬂow sensitive side eﬀects enable detecting
more constants than ﬂow insensitive side eﬀects. The resulting constant propagation
and constant folding is shown in Figure 7.10 on the next page.
For liveness analysis, we compute ﬂow sensitive MustKill and MayUse by travers-
ing the CFG in post order. MustKill is computed by discovering the sets of variables
that are modiﬁed in basic blocks such that these modiﬁcations are upwards exposed.
If a variable is used in a basic block, it is removed from the set. By contrast, MayUse
© 2009 by Taylor & Francis Group, LLC

252
Data Flow Analysis: Theory and Practice
a = 5; b = 3
c = 7;read d
Startmain
Call p
C1
Call p
R1
n1
a = 7
print c+d n1
n2
d = 14 n2
Call q
C2
Call q
R2
print
2 +c
Endmain
b = 2
if ( 2 < d)
Startp
n3 c = a+ 2
n3
print c+d
Endp
T
F
a = 1
Startq
Call p
C3
Call p
R3
a = 2
Endq
Block n Inn
Outn
Startmain ∅
{a,c,d}
C1
{a,c,d}
{a,c,d}
R1
{c,d}
{c,d}
n1
{c,d}
{c}
n2
{c}
{c,d}
C2
{c,d}
{c,d}
R2
{c}
{c}
Endmain {c}
∅
Startp
{a,c,d}
{a,b,c,d}
n3
{a,b,d}
{a,b,c,d}
Endp
{a,b,c,d} {a,b,c,d}
Startq
{c,d}
{a,c,d}
C3
{a,c,d}
{a,c,d}
R3
{b,c,d}
{b,c,d}
Endq
{b,c,d}
{a,b,c,d}
FIGURE 7.10
Interprocedural liveness analysis after interprocedural constant propagation using
ﬂow sensitive side eﬀects. Highlighted statements are dead assignments and can
be eliminated.
is computed using live variables analysis. BI is ∅for both MustKill and MayUse. The
resulting ﬂow functions are:
MustKill p = {b}
MayUse p = {a,c,d}
fp(x) = (x−{b}) ∪{a,c,d}
MustKillq = {a,b}
MayUseq = {c,d}
fq(x) = (x−{a,b}) ∪{c,d}
Figure 7.10 shows the liveness analysis on the program in which constant propaga-
tion has been performed. Both analyses use ﬂow sensitive side eﬀects to incorporate
the eﬀect of a procedure call. The results of both analyses are more precise com-
pared to the results obtained by using ﬂow insensitive side eﬀects. Further, dead
code elimination also becomes possible. Variable b is not live at the exit of Startmain
and Startp. Hence the assignments to these variables in the respective blocks can
© 2009 by Taylor & Francis Group, LLC

Introduction to Interprocedural Data Flow Analysis
253
a = 5; b = 3
c = 7;read d
Startmain
Call p
C1
Call p
R1
n1
a = a +2
print c+d n1
n2
d = 14 n2
Call q
C2
Call q
R2
print a +c
Endmain
b = 2
if ( 2 < d)
Startp
n3 c = a+ 2
n3
print c+d
Endp
T
F
a = 1
Startq
Call p
C3
Call p
R3
a = a∗2
Endq
Block n Inn
Outn
Startmain ∅
{a,c,d}
C1
{a,c,d} {a,c,d}
R1
{a,c,d} {a,c,d}
n1
{a,c,d} {a,c}
n2
{a,c}
{c,d}
C2
{c,d}
{c,d}
R2
{a,c}
{a,c}
Endmain {a,c}
∅
Startp
{a,c,d} {a,c,d}
n3
{a,d}
{a,c,d}
Endp
{a,c,d} {a,c,d}
Startq
{c,d}
{a,c,d}
C3
{a,c,d} {a,c,d}
R3
{a,c}
{a,c}
Endq
{a,c}
{a,c}
FIGURE 7.11
Interprocedural liveness analysis after interprocedural constant propagation using
context insensitive whole program analysis.
be deleted. Observe that this still does not cover all dead assignments shown in
Figure 7.1 on page 234.
7.6.3
Whole Program Analysis
We now present interprocedural analyses that compute both inherited and synthe-
sized data ﬂow information. As usual, the analyses are ﬂow sensitive. Since inherited
data ﬂow information depends on the callers alone, we present two possible variants:
(a) Context insensitive analysis, and (b) Context sensitive analysis.
Context insensitive whole program analysis
Conceptually, the simplest method of performing context insensitive whole program
analysis is to treat a supergraph as single control ﬂow graph and compute data ﬂow
properties with a block from all its neighbours without distinguishing between in-
terprocedural and intraprocedural edges. Thus, the constants reaching Startp are a
merge of the constants available at C1 and C3. This merged information is then used
to compute the synthesized information which is then propagated to both R1 and
R3. Thus the data ﬂow information at C3 inﬂuences the data ﬂow information at R1
in spite of the fact that there is no control ﬂow from C3 to R1. Thus this method
© 2009 by Taylor & Francis Group, LLC

254
Data Flow Analysis: Theory and Practice
propagates information ﬂow along interprocedurally invalid paths too causing an im-
precision in the context sensitive part of the synthesized data ﬂow information; the
context insensitive part remains unaﬀected.
The details of constants that get propagated to each program point in this method
are presented in Figure 7.6 on page 247. The optimized program after constant prop-
agation and the result of liveness analysis on this program are shown in Figure 7.11
on the previous page. Merging inherited data ﬂow information results in loss of
precision in the synthesized data ﬂow information because interprocedurally invalid
paths are also covered. This happens in spite of computing ﬂow sensitive synthesized
data ﬂow information.
Interestingly, in our example program, this method discovers fewer constants than
the method using ﬂow insensitive side eﬀects. Yet it performs some dead code elim-
ination whereas the latter does not. This is because this method discovers more
precise liveness information: With ﬂow insensitive side eﬀects, the liveness of vari-
able b is not killed in procedure p. On the other hand, the synthesized information
computed by ﬂow sensitive side eﬀects discovers that b is not live. However, this
method does not compare favourably with the method that uses ﬂow sensitive side
eﬀects—the latter computes precise synthesized information while merging inher-
ited data ﬂow information introduces some imprecision in the synthesized data ﬂow
information computed by this method.
Context sensitive whole program analysis
Our ﬁnal method of interprocedural data ﬂow analysis does not merge inherited data
ﬂow information while computing the synthesized data ﬂow information. Thus the
context sensitive part of synthesized data ﬂow information is more precise than in
the context insensitive whole program analysis.
It uses the same ﬂow functions as used by the ﬂow sensitive side eﬀects. The main
diﬀerence is that in that method, the inherited data ﬂow information was represented
by a conservative approximation. This method computes the inherited information
from calling contexts and propagates it within the callee’s body.
The resulting optimization is shown in Figure 7.12 on the facing page. The dead
code discovered by this method matches the dead code shown in Figure 7.1 on
page 234.
Observe that this method fails to discover that the value of c is 3 in Endmain. It can
be discovered by context sensitive whole program conditional constant propagation.
7.7
An Aside on Interprocedural Optimizations
A lot of work that analyses programs at the interprocedural level is directed at in-
terprocedural optimizations like procedure inlining and cloning. The analyses re-
quired for these optimizations are diﬀerent from the analyses that are presented in
© 2009 by Taylor & Francis Group, LLC

Introduction to Interprocedural Data Flow Analysis
255
a = 5; b = 3
c = 7;read d
Startmain
Call p
C1
Call p
R1
n1
a = 7
print
7 +d n1
n2
d = 14 n2
Call q
C2
Call q
R2
print
2 +c
Endmain
b = 2
if ( 2 < d)
Startp
n3 c = a+ 2
n3
print c+d
Endp
T
F
a = 1
Startq
Call p
C3
Call p
R3
a = 2
Endq
Block n Inn
Outn
Startmain ∅
{a,c,d}
C1
{a,c,d} {a,c,d}
R1
{c,d}
{c,d}
n1
{c,d}
{c}
n2
{a,c}
{c,d}
C2
{c,d}
{c,d}
R2
{c}
{c}
Endmain {c}
∅
Startp
{a,c,d} {a,c,d}
n3
{a,d}
{c,d}
Endp
{c,d}
{c}
Startq
{c,d}
{a,c,d}
C3
{a,c,d} {a,c,d}
R3
{c}
{c}
Endq
{c}
{c}
FIGURE 7.12
Interprocedural liveness analysis after interprocedural constant propagation using
context sensitive whole program analysis.
this book. Often they involve a single traversal over program representation. For ex-
ample, procedure inlining analyses parameters and checks that there is no recursive
call. The ﬁnal decision to inline is taken based on a collection of heuristics supported
by empirical evidence. Then a transformation pass renames global variables and per-
forms inlining by traversing the call graph bottom up. Procedure cloning is based on
analyzing actual parameters from diﬀerent call sites and their eﬀects on the called
procedures. Typically, the option of cloning is considered when constant values are
passed as actual parameters. Again, the ﬁnal decision depends on a collection of
thumb rules. Most production compilers gainfully employ these optimizations. An
additional advantage of these optimizations is that they enhance the possibility of
intraprocedural optimizations.
The next set of interprocedural optimizations employed by production compilers
are actually more aggressive intraprocedural optimizations using side eﬀects of pro-
cedure calls. The common side eﬀect that most compilers try to detect is potential
modiﬁcations of global variables and reference parameters.
Finally, many interprocedural optimizations do involve systematic analyses. How-
ever, for reasons of eﬃciency and scalability, most of these analyses are rooted in
speciﬁc optimizations e.g., constant propagation, side eﬀect analysis, points-to anal-
ysis etc. There is a large body of work along these lines but it seems diﬃcult to
© 2009 by Taylor & Francis Group, LLC

256
Data Flow Analysis: Theory and Practice
build useful generalizations across these methods. Besides, eﬃciency and scalabil-
ity concerns have often resulted in these methods being ﬂow insensitive or context
insensitive or both.
7.8
Summary and Concluding Remarks
In this part we focus on generalizations in keeping with the theme of the book and
present generic methods that naturally allow interprocedural analysis of the formu-
lations presented in Part I of the book.
This chapter has presented ﬂow and context sensitivity as two features that inﬂu-
ence the precision of interprocedural data ﬂow information. Further, it has identiﬁed
constructing summary ﬂow functions versus computing values as two fundamentally
diﬀerent approaches of performing interprocedural analysis. Subsequent chapters
use these concepts and primarily focus on methods that are ﬂow and context sensi-
tive. Chapter 8 presents general methods of constructing summary ﬂow functions
whereas Chapter 9 presents methods that compute data ﬂow information at each
point by maintaining distinct contexts.
7.9
Bibliographic Notes
The earliest studies of interprocedural data ﬂow analysis were motivated by the need
of discovering side eﬀects. The work by Spillman [94] was directed at ﬁnding out
side eﬀects in terms of values modiﬁed by the called procedure. This analysis was
performed by traversing a call graph from callees to callers. Allen [6] addressed a
slightly more general problem of additionally ﬁnding out values used by callees also.
However, unlike Spillman’s method, Allen’s method was not suited for recursive
programs. Barth [13, 14] introduced a much more general formulation based on
computing transitive closures of relationships. This method allowed asking a wider
range of questions such as whether variables shared storage or not, whether variables
were modiﬁed or used etc. More importantly, he introduced the notions of must and
may in the data ﬂow information discovered. Banning [12] was the ﬁrst to make a
distinction between ﬂow sensitive and ﬂow insensitive side eﬀect computation.
The concept of context sensitivity was introduced by Sharir and Pnueli [93] which
can be easily called the most inﬂuential work on interprocedural data ﬂow analysis.
We present their concepts in greater details in the next two chapters.
Eﬀectiveness of interprocedural data ﬂow analysis was studied by Richardson and
Ganapathi [83], Grove and Torczon [39], and Martin [72]. Lhot´ak and Hendren [69]
have empirically observed that in the presence of recursive calls, context insensitivity
© 2009 by Taylor & Francis Group, LLC

Introduction to Interprocedural Data Flow Analysis
257
leads to signiﬁcant imprecision.
Duesterwald, Gupta and Soﬀa [32, 33] present an interesting alternative of com-
puting interprocedural data ﬂow information incrementally on demand.
An important issue in interprocedural data ﬂow analysis is precise call graph con-
struction. This becomes diﬃcult in the presence of function pointers in a language
like C and virtual functions and dynamic dispatch of methods in object oriented lan-
guages. Early works along these lines were done by Hall and Kennedy [43] and by
Callahan, Carle, Hall and Kennedy. [20]. Grove and Chambers [38] present a more
recent detailed treatment of call graph construction. We do not address this issue in
this book.
© 2009 by Taylor & Francis Group, LLC

© 2009 by Taylor & Francis Group, LLC

8
Functional Approach to Interprocedural Data
Flow Analysis
Functional approach to interprocedural data ﬂow analysis constructs context inde-
pendent summary ﬂow functions which are then used in the calling contexts to com-
pute the data ﬂow information synthesized by called procedures in the body of the
caller procedures. Data ﬂow information inherited by a procedure is computed from
the calling contexts of the procedure. The main advantage of constructing context
independent summary ﬂow functions is that a procedure needs to be analyzed only
once regardless of the number of calls to it.
We begin by presenting the classical side eﬀects analysis for bit vector frameworks
as a special case of constructing summary ﬂow functions. This is followed by con-
text and ﬂow sensitive whole program analysis. Finally we show how the explicit
construction of summary ﬂow functions can be avoided by enumerating the function
in terms of pairs of input output values.
For simplicity, we focus on data ﬂow analysis of global variables. We present
orthogonal techniques of handling the eﬀects of parameters. We restrict the analysis
to languages that do not contain nested procedures.
8.1
Side Eﬀects Analysis of Procedure Calls
Classical side eﬀects analysis focuses on computing the eﬀect of a callee procedure
on the variables of the caller procedure in order to discover more optimization oppor-
tunities in the caller procedures. In particular, the following side eﬀects are directly
relevant: For a given variable v and a given callee s in a procedure r
• Is the execution of r guaranteed to modify the value of v?
• Can the execution of r modify the value of v?
• Is the execution of r guaranteed to use the value of v before modifying it?
• Can the execution of r use the value of v before modifying it?
The variables for which the above answers are in aﬃrmative are contained in MustKillr,
MayKillr, MustUser, and MayUser respectively. Clearly, the must properties are all
paths properties whereas may properties are some path properties.
259
© 2009 by Taylor & Francis Group, LLC

260
Data Flow Analysis: Theory and Practice
a = 5;b = 3
c = 7;read d
Startmain
Call p
c1
n1
a = a+2
print c+d n1
n2
d = a ∗b n2
Call q
c2
print a +c
Endmain
b = 2
if (b < d)
Startp
n3 c = a+b n3
Call q
c4
print c+d
Endp
T
F
a = 1
Startq
Call p
c3
a = a∗b
Endq
FIGURE 8.1
Modiﬁed version of the program in Figure 7.2.
These basic side eﬀects of a procedure can be used to answer a variety of ques-
tions. For example, liveness analysis can handle call to procedure r by computing
Inn of the call block as follows:
Inn = (Outn −MustKillr) ∪MayUser
Observe that liveness information of a variable is killed only when it is guaranteed
to be modiﬁed in the callee along all execution paths. Available expression analysis,
on the other hand, should use kill the availability of expressions whose operands are
in MayKillr. The variables that are guaranteed to preserve their values across a call
to procedure r are contained in Gvar−MayKillr where Gvar denotes the set of global
variables. The variables that may preserve their values along some path through
procedure r are contained in Gvar−MustKillr.
The ,  and ⊥values for computing the above side eﬀect properties are:
Property

 
⊥
Explanation of ⊥
MustKillr
∩
Gvar
∅
No variable can be guaranteed to be
necessarily killed by r
MustUser
∩
Gvar
∅
No variable can be guaranteed to be
necessarily used in r before being modiﬁed
MayUser
∪
∅
Gvar
Any variable may be used in r along some
path or the other
MayKillr
∪
∅
Gvar
Any variable may be killed in r along some
path or the other
We use the program in Figure 8.1 as a running example in this section. This
program is same as the program in the previous chapter except that we have now
© 2009 by Taylor & Francis Group, LLC

Functional Approach to Interprocedural Data Flow Analysis
261
included a call to procedure q in procedure p to make the program recursive.
8.1.1
Computing Flow Sensitive Side Eﬀects
The side eﬀect properties MustKill, MayKill, MustUse, MayUse are computed by
assuming BI to be empty set.
For computing the MustKill and MayKill, a simple data ﬂow analysis gathers the
variables that are killed on the paths from Startr to Endr. The sets so computed do
not include local variables of r because they are not visible in the caller procedures
even if r is called recursively. The data ﬂow value Outr deﬁnes MayKillr or MustKillr
as the case may be. The data ﬂow equations for computing MustKillr are given below.
Inn =

BI
n is Start block

p∈pred(n)
Out p otherwise
Outn =

Inn ∪MustKills n is a call to s
Inn ∪Genn
otherwise
(8.1)
MustKillr = OutEndr
The initial values of Inn, Outn, and MustKills are  = Gvar.
For computing MayKillr,  is ∪, and the initial values of Inn, Outn, and MayKills
are  = ∅.
Example 8.1
The computation of MustKill and MayKill properties of procedures p and q of
our program in Figure 8.1 are shown in Figure 8.2 on the following page. Since
procedures p and q are mutually recursive, their data ﬂow values are mutually
dependent and require a ﬁxed point computation with  as the initial value.
When procedure p is being analyzed MustKillq is assumed to be {a,b,c,d}.
This results in MustKill p = {b,c} which is then used in computing MustKillq.
The resulting value MustKillq = {a,b,c} is used in the second iteration over p.
Although it causes a change in Outc4, OutEndp does not change. Thus neither
MustKill p, not MustKillq changes. For computing MayKill,  is ∅. Thus the
initial value of MayKillq is ∅resulting in MayKill p = {a,b,c}. When this is used
to compute MayKillq, the result is {a,b,c}. However, the new value of MayKillq
does not result in a change in the value of MayKill p.
Observe that c is contained in MustKill p in spite of the fact that it is com-
puted conditionally. This is because every path from Startp to Endp must
pass through n3: Even if the execution were to follow edge Startp →c4, the
only way to unwind the recursive call to q is to execute the path involving
n3. Since there is only one path through procedure q (with varying depths of
recursion), MustKillq = MayKillq. Also observe that a is contained in MayKill p
but not in MustKill p.
© 2009 by Taylor & Francis Group, LLC

262
Data Flow Analysis: Theory and Practice
Procedure Node
MustKill
MayKill
Iteration #1
Changes in
Iteration #2
Iteration #1
Changes in
Iteration #2
In
Out
In
Out
In
Out
In
Out
p
Startp
∅
{b}
∅
{b}
n3
{b}
{b,c}
{b}
{b,c}
c4
{b}
{a,b,c,d}
{a,b}
{b}
{b}
{a,b,c}
Endp
{b,c}
{b,c}
{b,c}
{b,c} {a,b,c} {a,b,c}
MustKill p = Endp = {b,c}
MayKill p = Endp = {a,b,c}
q
Startq
∅
{a}
∅
{a}
c3
{a}
{a,b,c}
{a}
{a,b,c}
Endq {a,b,c} {a,b,c}
{a,b,c} {a,b,c}
MustKillq = Endq = {a,b,c}
MayKillq = Endq = {a,b,c}
FIGURE 8.2
Computing ﬂow sensitive MustKill and MayKill for the program in Figure 8.1.
The data ﬂow equations for computing MayUse have been provided below. Intu-
itively, MayUser contains the variables that are live at the entry of r assuming that
no variable is live at the exit of r. Thus except for a call statement, the data ﬂow
equations are identical to the data ﬂow equations of liveness analysis. Genn contains
the set of variables with upwards exposed uses in block n.
Inn =
(Outn −MustKillt) ∪MayUset n is a call to t
(Outn −Killn) ∪Genn
otherwise
(8.2)
Outn =

BI
n is End block

s∈succ(n)
Ins otherwise
MayUser = InStartr
For a call statement, the variables in MustKill set of the callee cease to be live whereas
the variables in MayUse set of the callee become live. For MustUse,  is ∪and Kill
for a call statement is MayKill instead of MustKill.
Example 8.2
The data ﬂow analysis for computing MayUse and MustUse of our example
program is provided in Figure 8.3 on the next page. Observe that for com-
puting MayUse p we use  = ∅as the initial value of MayUseq whereas for
computing MustUse p we use  = {a,b,c,d} as the initial value of MustUseq.
The data ﬂow values for computing MayUse do not change in the second
iteration whereas the data ﬂow values for computing MustUse do.
© 2009 by Taylor & Francis Group, LLC

Functional Approach to Interprocedural Data Flow Analysis
263
Procedure Node
MayUse
MustUse
Iteration #1
Iteration #1
Changes in
Iteration #2
Out
In
Out
In
Out
In
p
Endp
∅
{c,d}
∅
{c,d}
n3
{c,d}
{a,b,d}
{c,d}
{a,b,d}
c4
{c,d}
{d}
{c,d} {a,b,c,d}
{d}
Startp {a,b,d}
{a,d}
{a,b,d}
{a,d}
{d}
{d}
MayUse p = Endp = {a,d}
MustUse p = Endp = {d}
q
Endq
∅
{a,b}
∅
{a,b}
c3
{a,b}
{a,d}
{a,b}
{a,d}
{d}
Startq
{a,d}
{d}
{a,d}
{d}
{d}
MayUseq = Endq = {d}
MustUseq = Endq = {d}
FIGURE 8.3
Computing ﬂow sensitive MayUse and MustUse for the program in Figure 8.1.
8.1.2
Computing Flow Insensitive Side Eﬀects
Recall that ﬂow insensitive computation accumulates the eﬀect of each block using
Equation (7.1). As explained in Figure 7.5 on page 242 and Figure 7.8 on page 249,
dependence of data ﬂow values on other data ﬂow values has to be explicitly handled
by adding dependence edges. In the general situation, a path in paths(u) could con-
sist of fragments where the dependent parts in the ﬂow functions are ∅as illustrated
in Figure 8.4 on the following page.
The following lemma shows that if dependent parts are handled explicitly, ﬂow in-
sensitive analysis computes a safe approximation of the corresponding ﬂow sensitive
data ﬂow information.
LEMMA 8.1
Consider a path fragment ρ = (p0, p1,..., pk) along which the dependent parts
of ﬂow functions fpi→pi+1 are ∅. Then,
∀x ∈L :
k
i=0
fi(x) 
fρ (x)
where fi = fpi→pi+1 and fρ = fk ◦fk−1 ◦...◦f1 ◦f0.
PROOF
We prove this by induction on path length.
• Basis: Consider path of length of 1. We need to show that
∀x ∈L : f1(x) f0(x) 
f1(f0(x))
© 2009 by Taylor & Francis Group, LLC

264
Data Flow Analysis: Theory and Practice
0 f0 0
1 f1 1
2 f2 2
3 f3 3
4 f4 4
5 f5 5
Start
0 f0 0
1 f  = f1  f2  f3  f4 1
5 f5 5
End
(a) A path for ﬂow sensitive analysis.
Dependent parts in f1, f2, f3, f4 are ∅.
(b) Modeling the path for ﬂow
insensitive analysis. In5  In5.
FIGURE 8.4
Safety of ﬂow insensitive analysis with dependent parts.
Since there are no dependent parts in ﬂow functions, the ﬂow function
is separable. Thus we can prove the lemma for independent entities.
Further, absence of dependent parts imply that entity functions are
constant functions or the identity function; a non-constant non-identity
ﬂow function requires dependent part. Thus the proof obligation reduces
to
∀α ∈Σ,∀x ∈L : f α
0 (x)  f α
1 (x)  f α
0
f α
1 (x)

We consider the following two cases.
– f α
0
is the identity function. Then the proof obligation reduces to
∀x ∈L : x  f α
1 (x)  f α
1 (x)
which trivially holds.
– f α
0
is some constant function resulting in a particular value y ∈L.
Then the proof obligation reduces to
∀x ∈Lα : y  f α
1 (x)  y
which also trivially holds.
• Inductive step: Assume that the lemma holds for path of length i. Then,
it follows that for fρ = fi ◦fi−1 ◦...◦f1 ◦f0,
∀x ∈L :
i
i=0
fi(x) 
fρ (x)
© 2009 by Taylor & Francis Group, LLC

Functional Approach to Interprocedural Data Flow Analysis
265
Property
Deﬁning expression
Result
Iteration #1
Changes in
iteration #2
MustKill p KillStartp ∩Killn3 ∩MustKillq ∩KillEndp
∅
MayKill p
KillStartp ∪Killn3 ∪MayKillq ∪KillEndp
{b,c}
{a,b,c}
MustUse p GenStartp ∩Genn3 ∩MustUseq ∩GenEndp
∅
MayUse p GenStartp ∪Genn3 ∪MayUseq ∪GenEndp
{a,b,c,d}
MustKillq
KillStartq ∩MustKill p ∩KillEndq
∅
MayKillq
KillStartq ∪MayKill p ∪KillEndq
{a,b,c}
MustUseq GenStartq ∩MustUse p ∩GenEndq
∅
MayUseq GenStartq ∪MayUse p ∪GenEndq
{a,b,c,d}
FIGURE 8.5
Flow insensitive computation of side eﬀects for the program in Figure 8.1.
We need to show that
fi+1(x)
 i
i=0
fi(x)


fi+1

fρ(x)

Since the ﬂow functions are separable we can prove this independently
for diﬀerent entities by considering constant and identity entity functions
f α
i+1 in a manner similar to that in the basis case.
Recall that for ﬂow insensitive analysis, we merge fi(BI) (Equation 7.1). Since BI
is ∅the ﬂow functions deﬁned in Equations (8.1) and (8.2) reduce to:
Property
Flow Function Flow Function with x = BI = ∅
MustKillr,MayKillr
x∪Kill
Kill
MustUser,MayUser (x−Kill)∪Gen
Gen
Example 8.3
The ﬂow insensitive computations of side eﬀects for our example program of
Figure 8.1 is shown in Figure 8.5. Observe the mutual dependence of the data
ﬂow values of procedures p and q due to mutual recursion. We ﬁrst compute
the values for procedure p by using  values for procedure q. The resulting
values for procedure p are then used to compute the values of procedure q.
The resulting values for procedure q are diﬀerent from the initially assumed
 values. However, they do not cause any change in the values of procedure p
except for MayKill p. When this changed value is used for recomputing MayKillq,
there is no change.
© 2009 by Taylor & Francis Group, LLC

266
Data Flow Analysis: Theory and Practice
0. int a,b;
1. main()
2. {
int c,d;
3.
read (a,b,c);
4.
q(a,b,c,d);
/* Call site c1 */
5.
p(c,c);
/* Call site c2 */
6. }
7. q(int w, int x, int y, int z)
8. {
int e;
9.
x = x + 1;
10.
if (x < y)
11.
{
q(x,y,z,e); /* Call site c3 */
12.
p(w,x);
/* Call site c4 */
13.
}
14. }
15. p(int m, int n)
16. {
n=m; }
main()
p(c,c)
c = c
q(a,b,c,d)
p(a,b)
b = a
b=b+1
q(b,c,d,e)
p(b,c)
c = b
c=c+1
q(c,d,e,e)
p(c,d)
d = c
d=d+1
q(d,e,e,e)
e=e+1
FIGURE 8.6
A C program assuming parameter passing by reference. A possible activation tree
shows how variables may be modiﬁed in the program.
8.2
Handling the Eﬀects of Parameters
Recall that we have excluded the eﬀects of parameters from our descriptions of anal-
yses by restricting them to global variables only. If the parameter passing mechanism
is by value, the basic techniques do not change much except that the data ﬂow in-
formation of actual parameters must be propagated as the data ﬂow information of
formal parameters. Thus formal parameters can be considered similar to local vari-
ables except that BI for formal parameters is computed from the calling contexts.
Section 9.5 shows a way of modeling the eﬀect of parameters to capture the transfer
of data ﬂow information between actual and formal parameters.
In this section we look at aliasing between formal parameters in the presence of
parameter passing by reference. In this case the actual parameter and the formal pa-
rameter share the same address and hence are aliased. The main diﬀerence between
this aliasing and the aliasing created by pointer assignments (Section 4.3.2) is that in
pointer assignments, both variables involved in an alias are simultaneously visible;
in the case of parameters this need not hold always. Thus discovering such aliases
requires a diﬀerent technique.
© 2009 by Taylor & Francis Group, LLC

Functional Approach to Interprocedural Data Flow Analysis
267
8.2.1
Deﬁning Aliasing of Parameters
We discover may aliases created by call statements only. Further, we restrict the
aliases to scalar variables only. Hence, all other statements including the control
ﬂow statements are ignored and our analysis is ﬂow insensitive. Observe that there is
no way of killing such aliases; they just become invisible when the variables involved
go out of scope.
Example 8.4
We consider the program in Figure 8.6 for performing side eﬀects analysis of
procedure q. If we assume parameter passing by reference, it is easy to see
that q will modify variable b. However, it is clear from the activation tree of
the program that q can also modify the local variables of main (c and d) and
q (e). This happens because the recursive call to q at the call site c3 passes
its formal parameters as actuals in a diﬀerent order. As a consequence, the
formal parameter x gets aliased to y and z in nested recursive calls. Observe
that it does not get aliased to w and the global variable a cannot be modiﬁed
anywhere in the program. The ﬁrst call to p modiﬁes c whereas the second
call to p modiﬁes b, c, and d.
Our primary goal is to ﬁnd out aliasing of formal parameters of a procedure. Con-
sider two formal parameters x and y of procedure r. They may be aliased to each
other because of any of the following reasons:
• Direct generation. There are two ways in which direct aliases are generated:
– The actual parameters of both x and y are same in some call. They may
well be global variables, local variables of the caller, or formal parame-
ters of the caller.
– In some call, the actual parameter for x (alternatively, y) is a global vari-
able v and the actual parameter of y (alternatively, x) is a formal parame-
ter of the caller and is aliased to v. Observe that a formal parameter of a
procedure can never be aliased to a local variable of the same procedure.
• Indirect generation. x may be passed as y (or vice-versa) in a call in r in a
recursive call sequence.
• Propagation. The actual parameters of x and y may be aliased in a caller’s
body.
We restrict ourselves to languages that do not support nested procedures. In the
case of nested procedures, the formal parameters of an outer procedure are visible
within the nested procedures and must be treated as global variables within them
rather than as formal parameter of the outer procedure.
© 2009 by Taylor & Francis Group, LLC

268
Data Flow Analysis: Theory and Practice
8.2.2
Formulating Alias Analysis of Parameters
We solve the problem in two steps. In the ﬁrst step we ﬁnd out the variables that
may be aliased to formal parameters of a procedure along some call chain leading
to the procedure. These variables may be global variables and formal parameters of
callers. In the second step, we augment this information with the aliasing between
formal parameters of the same procedure.
Let the local variables and formal parameters of procedure r be contained in Localr
and Formalr respectively; we assume that formal parameter names are distinct for
each procedure. We deﬁne a function ψ to map a formal parameter to the correspond-
ing actual parameter at a call site. Let a call site ci in procedure s call procedure r.
Given x ∈Formalr, ψ(ci, x) = y where y ∈Gvar∪Locals ∪Formals.
Our lattice consists of data ﬂow values denoted by x
ci⇒y where x is a formal
parameter of a procedure being called at call site ci and y is a variable which is rep-
resented by x in the called procedure; in the simplest case, y is the actual parameter
corresponding to x. In order to compute data ﬂow information inherited by the callee,
the data ﬂow information of y must be copied to the data ﬂow information of x in
BI of the callee. To compute the data ﬂow information synthesized by the callee, the
data ﬂow information of x must be copied to the data ﬂow information of y. In par-
ticular, if the callee modiﬁes x, y should be considered to be modiﬁed in the caller’s
body. Similarly, if the callee uses x, y should be considered to be used in the caller’s
body. Thus the relation x
ci⇒y is not symmetric; the exact direction of dependence is
governed by the intended use of the data ﬂow information.
The relation x
ci⇒y between x and y becomes symmetric if both x and y are visible
within the called procedure. This is possible only when y is a global variable or
when y is a formal parameter that encloses the called procedures. In such a situation,
a modiﬁcation in y in the callee is should be considered a modiﬁcation in x and vice-
versa. This situation is more appropriately modeled by considering y as a global
variable for the callee rather than as a formal parameter of the caller.
The ﬂow function for a call site ci in procedure s calling a procedure r is deﬁned
as follows:
fci(x) = x ∪ConstGenci ∪DepGenci(x)
ConstGenci = {x
ci⇒y | x ∈Formalr, y = ψ(ci, x), y ∈Gvar∪Formals}
DepGenci(x) = {x
ci⇒z | x ∈Formalr, y = ψ(ci, x), y ∈Formals, y
ci⇒z ∈x}
Observe that ConstGenci excludes y ∈Localr and z in DepGenci could well be a
formal parameter of a caller of s or some other ancestor of s along a call chain
reaching ci.
Let Calls denote all call sites in a program. In the ﬁrst step, the aliasing informa-
tion is computed as the MFP solution of the following equation with  = ∅:
PVA =

ci∈Calls
fci(PVA)
(8.3)
© 2009 by Taylor & Francis Group, LLC

Functional Approach to Interprocedural Data Flow Analysis
269
Formals
x
y
Actuals
w
z
Formals
x
y
Actuals
w
z
FIGURE 8.7
Aliasing relation between actual parameters should be propagated to the correspond-
ing formal parameters.
where PVA is an abbreviation of “Parameters to Variables Aliasing”. PVA contains
the variables to which formal parameters of a function may be aliased. These vari-
ables could be global variables, formal parameters of caller procedures, or formal
parameters of the same procedure in the case of recursion.
To see why PVA contains the indirectly generated aliases of formal parameters in
the presence of recursion, consider x,y ∈Formalr for procedure r that is part of a
recursive call chain. There must be a sequence of corresponding formal parameters
x,y, x,y, etc. of the procedures called in the call sequence. If one of these pa-
rameters (say y) is passed as an actual parameter at a diﬀerent position (say in the
place of x) in the subsequent call, it will result in a pair x cj⇒y in PVA. Due to
transitive propagation deﬁned in DepGencj(x), we will also have the pair x cj⇒y in
PVA. When this pair is propagated to r, we will get the pair x
ck⇒y in PVA. The pairs
x,y, y, x in PVA are not meaningful by themselves because they represent formal
parameters of diﬀerent procedures; their use is mainly in detecting and propagating
indirectly generated aliases.
The semantics captured by the pair x
ck⇒y in PVA when both x and y are in Formalr
requires some explanation. Recall that this relation is not symmetric because it de-
notes the fact that y is represented by x in a nested call. Since both x and y are formal
parameters of the same procedure, this is possible only when an incarnation of y in
a call to r is represented by an incarnation of x in a nested recursive call to r. Thus,
if we have x
ck⇒y in PVA and x is modiﬁed in r, we can conclude that y is modiﬁed
in r. However, if y is modiﬁed in r, then we cannot conclude that x is modiﬁed in r
unless we have y
ck⇒x in PVA. Observe that this is consistent with our semantics of
x
ck⇒y when x and y are formal parameters of diﬀerent procedures.
Let VPAr(x) (abbreviation for “Variables to Parameters Aliasing”) denote the set
of formal parameters of r that are aliased to variable x. It is deﬁned as:
VPAr(x) =

y | x
ci⇒y ∈PVA, y ∈Visibler,"
if x ∈Formalr
y | y
ci⇒x ∈PVA, y ∈Formalr,"
if x ∈Gvar
(8.4)
The meaning of y ∈VPAr(x) is that whenever x is modiﬁed in r, y should also be con-
sidered to be modiﬁed in r; similarly, whenever x is used in r, y should be considered
to be used in r. Clearly, VPAr(x) as deﬁned by Equation (8.4) is not symmetric.
© 2009 by Taylor & Francis Group, LLC

270
Data Flow Analysis: Theory and Practice
• Computing PVA. An element m in the set in a row ci and column l represents
the data ﬂow value l
ci⇒m computed in the corresponding iteration.
Iteration
PVA for procedure q
PVA for procedure p
Call
site
w
x
y
z
Call
site
m
n
#1
c1
{a}
{b}
∅
∅
c2
∅
∅
c3
{x}
{y}
{z}
∅
c4
{w}
{x}
#2
c1
{a}
{b}
∅
∅
c2
∅
∅
c3
{b, x,y}
{y,z}
{z}
∅
c4
{a,w, x}
{b, x,y}
#3
c1
{a}
{b}
∅
∅
c2
∅
∅
c3
{b, x,y,z}
{y,z}
{z}
∅
c4
{a,b,w, x,y} {b, x,y,z}
• Computing VPAr for calls with diﬀerent set of actual parameters at call site c2.
Call at c2
VPAq
VPAp
w
x
y
z
a
b
m
n
a
b
p(c,c) {x,y,z} {y,z} {z}
∅
{w} {w, x} {n}
{m} {m} {m,n}
p(c,d) {x,y,z} {y,z} {z}
∅
{w} {w, x}
∅
∅
{m} {m,n}
FIGURE 8.8
Computing aliasing resulting from reference parameters for our example program.
The aliases contained in VPAr(x) are not complete. What remains is to detect and
propagate the directly generated aliases of formal parameters. When x is a formal
parameter, we augment VPAr(x) as shown below:
VPAr(x) = VPAr(x) ∪


ci∈CallsTor
propVPAr(ci, x)

propVPAr(ci, x) denotes the set of aliases that are propagated to ci ∈CallsTor: When
two aliased formal parameters of a caller of r are passed as actual parameters in a call
to r, the corresponding formal parameters of r get aliased; this has been illustrated
in Figure 8.7. The identiﬁcation of directly generated aliases and their propagation
is achieved by:
propVPAr(ci, x) = directVPAr(ci, x) ∪
y | x
cj⇒w ∈PVA, y
cj⇒z ∈PVA, z ∈VPAs(w),
cj ∈CallsTor, x ∈Formalr, y ∈Formalr
"
© 2009 by Taylor & Francis Group, LLC

Functional Approach to Interprocedural Data Flow Analysis
271
where c j is a call site in procedure s calling r, and
directVPAr(ci, x) = y |ψ(ci, x) = ψ(ci,y), x ∈Formalr, y ∈Formalr
" ∪
y |((ψ(ci, x) = v, y
ci⇒v ∈PVA) or (ψ(ci,y) = v, x
ci⇒v ∈PVA)),
v ∈Gvar, x ∈Formalr, y ∈Formalr
"
Observe that the propagation of aliases to callees results in context insensitive
aliases because the aliases from all callers are combined. This is similar to the con-
text insensitivity observed in PVA.
Example 8.5
The computation of aliases resulting from reference parameters in the program
of Figure 8.6 has been shown in Figure 8.8 on the facing page. Beginning with
 = ∅, we compute successive approximations of PVA using Equation (8.3).
Observe that the indirect aliases for procedure q capture the fact that w
represents x, y, and z in recursive calls and x represents y and z. However,
w is not represented by any other variable.
What this means is that the
assignment to x in procedure q cannot modify w although it can modify y and
z and their actual parameters.
We augment this information with aliasing between formal parameters of
the same procedure, under two diﬀerent situations:
• When the call at call site c2 is p(c,c), and
• when it is p(c,d).
When the call is p(c,c), the formal parameters m and n of procedure p get
aliased. Since the data ﬂow information is context insensitive, our analysis
assumes that this aliasing holds for all calls to p. If we change the call to
p(c,d), m and n are not aliased anymore.
8.2.3
Augmenting Data Flow Analyses Using Parameter Aliases
Now VPAr(x) can be used to augment the data ﬂow information computed by other
analyses. We illustrate it for computing MayKillr.
We deﬁne MayKillr to consist of two components:
MayKillr =Killr ∪


cj∈CallsInr
MayKillt(cj)

∪
(8.5)
#
y | y ∈VPAr(x), x ∈MayKillr
$
where MayKillr represents all variables visible in r that are killed by execution of
r. They include local and global variables as well as formal parameters of r. This
information, augmented with the killing of actual parameters by a call to r at call
© 2009 by Taylor & Francis Group, LLC

272
Data Flow Analysis: Theory and Practice
Procedure Kill
When call at c2 is p(c,c)
When call at c2 is p(c,d)
MayKill
Call speciﬁc MayKill
MayKill
Call speciﬁc MayKill
Call site ci MayKill(ci)
Call site ci MayKill(ci)
p
{n}
{m,n}
c2
{c,c}
{n}
c2
{d}
c4
{w, x}
c4
{x}
q
{x} {w, x,y,z}
c1
{a,b,c,d}
{y,z}
c1
{b,c,d}
c3
{w, x,y,z}
c3
{x,y,z}
FIGURE 8.9
Side eﬀects for the example program of Figure 8.6.
site ci is contained in MayKillr(ci) which is deﬁned below in Equation (8.6). Killr
represents the variables that may be directly killed within r without incorporating
the eﬀect of calls made in r. The variables killed by a call to procedure s made at
call site cj in r are contained in MayKills(cj).
From the gross information MayKillr, we extract MayKill G
r and MayKill F
r that de-
note the global variables and formal parameters of r killed by r. They are deﬁned as
shown below:
MayKill F
r = MayKillr ∩Formalr
MayKill G
r = MayKillr ∩Gvar
Now we need to ﬁnd out the local variables of the caller that may be killed by r. This
can happen only through the formal parameters of r. The complete side eﬀect of a
call to r made at call site ci is represented by MayKillr(ci) which is deﬁned in terms
of MayKill G
r and MayKill F
r as shown below:
MayKillr(ci) = MayKill G
r ∪
#
y | ψ(ci, x) = y, x ∈MayKill F
r
$
(8.6)
Observe that the deﬁnition of MayKillr (Equation 8.5) is ﬂow insensitive. It can
be made ﬂow sensitive by computing Ini and Outi along the control ﬂow and using
MayKillr(ci) as the ﬂow function for call site ci. However, the aliases contained in
VPAr(x) and PVA remain context and ﬂow insensitive.
Example 8.6
The side eﬀects computed for our example have been shown in Figure 8.9.
When the call at c2 is p(c,c), our analysis concludes that the call at c4 kills
both w and x. Hence it concludes that q can kill the global variable a which
has been passed as an actual parameter of w at call site c1. If we change the
call at c2 to p(c,d), m and n are not aliased. Hence our analysis concludes
that the call at c4 kills only x and not w. As a consequence, a is not contained
in the side eﬀect of call at c1.
© 2009 by Taylor & Francis Group, LLC

Functional Approach to Interprocedural Data Flow Analysis
273
m
n
w
x
y
z
c4
c4
c3
c3
c3
FIGURE 8.10
Parameter binding graph for the program in Figure 8.6.
8.2.4
Eﬃcient Parameter Alias Analysis
The parameter analysis presented in Section 8.2.2 models the required computation
instead of designing an eﬃcient algorithm for performing the analysis. The resulting
data ﬂow analysis is non-separable and requires a lot of transitive computation that
may be redundant. To see this, consider an alias x
ci⇒y computed by Equation (8.3).
If the analysis discovers y
cj⇒z1 and y
ck⇒z2, it implies adding the pairs x
ci⇒z1 and
x
ci⇒z2. Now if w
cl⇒x is discovered, the analysis computes w
cl⇒y, w
cl⇒z1, and
w
cl⇒z2. Thus every possible transitive eﬀect of parameters is detected. Observe
that it is not necessary to store all these relations. The core relations that need to be
stored are w
cl⇒x, x
ci⇒y, y
cj⇒z1, and y
ck⇒z2; other aliases can be discovered from
these relations when required.
A simple way of speeding up the analysis is to identify the dependence of formal
parameters on each other and store them in a graph called parameter binding graph.
For our example program, it has been illustrated in Figure 8.10. An edge x →y rep-
resents the relation x
ci⇒y. This graph directly captures the dependence arising out of
non-separability and hence avoid redundant traversals over a call graph. Constructing
this graph is eﬃcient because we only need to construct individual edges; comput-
ing PVA involves identifying all paths in the graph. After this graph is constructed,
aliasing with global variables require only propagating them in the graph along the
edges in the graph starting from the formal parameter for which the global variable
is an actual parameter. Further, indirect aliases are represented by edges between
the formal parameters of the same procedure. Thus PVA involves mapping formal
variables to global variables. There is no need to record mapping between formal
variables. This is particularly useful if there are very few recursive procedures; for
non-recursive procedures, these mappings are irrelevant.
Observe that a use of parameter binding graph is similar to the use of points-to
graph in that both these data structures capture the eﬀect of the dependent part of
ﬂow functions and facilitate a ﬂow insensitive computation in a single pass over
the underlying control ﬂow structure. The only diﬀerence between them is that for
points-to analysis the control ﬂow structure is either a CFG or a supergraph whereas
for a parameter binding graph, it is a call graph.
© 2009 by Taylor & Francis Group, LLC

274
Data Flow Analysis: Theory and Practice
a = 5;b = 3
c = 7;read d
Startmain
Call p
c1
n1
a = d +2
print d
n1
n2
d = a ∗3
n2
Call q
c2
print c
Endmain
b = 2
if (b < d)
Startp
n3 c = a+b n3
Call q
c4
print c+d
Endp
T
F
a = 1
Startq
Call p
c3
a = a∗b
Endq
Block
Flow function fi(x)
Startmain x∪{a,b,c,d}
c1
fp(x)
n1
x−{d}∪{a}
n2
x−{a | d  x}∪{d}
c2
fq(x)
Endmain x
Startp
x−{d}∪{b}
n3
x−{a,b | c  x} ∪{c}
c4
fq(x)
Endp
x
Startq
x∪{a}
c3
fp(x)
Endq
x−{b | a  x}
FIGURE 8.11
Example program for interprocedural faint variables analysis. This is a modiﬁed
version of the program in Figure 8.1.
8.3
Whole Program Analysis
In the previous section, we constructed summary ﬂow functions for speciﬁc anal-
yses. In this section we present the general method of constructing summary ﬂow
functions. We consider ﬂow sensitive methods; the ﬂow insensitive versions can be
devised along the lines described earlier.
We use liveness analysis and faint variables analysis to explain the method. For
liveness analysis, we use the program of Section 8.1. Since it does not have much
scope for faint variables analysis, we use the program in Figure 8.11.
8.3.1
Lattice of Flow Functions
Deﬁning a data ﬂow analysis requires setting up a lattice of the values that are to be
computed by the analysis. This greatly simpliﬁes reasoning about the analysis. The
same approach can be used to deﬁne analyses to construct summary ﬂow functions.
The main diﬀerence is that the data ﬂow values computed by other analyses we have
seen so far represent certain semantics of the entities appearing in the program. The
data ﬂow values for the analysis that constructs summary ﬂow functions are ﬂow
functions that compute the data ﬂow values desired in the end.
When we view the set of ﬂow function F as a lattice, we deﬁne the partial order
relation over ﬂow functions in terms of the partial order relation between the values
© 2009 by Taylor & Francis Group, LLC

Functional Approach to Interprocedural Data Flow Analysis
275
Lattice of
data ﬂow
values
All possible ﬂow functions
Lattice of ﬂow
functions
Single variable a
 = ∅
⊥= {a}
Geni Killi f a→a
i
∅
∅
φid
∅
{a}
φ 
{a}
∅
φ⊥
φ 
φid
φ⊥
Two variables a and b
 = ∅
{a}
{b}
⊥= {a,b}
Geni Killi
fi
Geni Killi
fi
∅
∅
φII
{b}
∅
φI⊥
∅
{a}
φ I
{b}
{a} φ ⊥
∅
{b}
φI 
{b}
{b}
φI⊥
∅
{a,b} φ  
{b} {a,b} φ ⊥
{a}
∅
φ⊥I {a,b}
∅
φ⊥⊥
{a}
{a}
φ⊥I {a,b} {a} φ⊥⊥
{a}
{b} φ⊥ {a,b} {b} φ⊥⊥
{a} {a,b} φ⊥ {a,b} {a,b} φ⊥⊥
φ  
φ I
φI 
φ ⊥
φII
φ⊥ 
φI 
φ I
φ⊥⊥
FIGURE 8.12
Example of lattices of functions for live variables analysis. φxy indicates that the
component ﬂow function for variable a is x and that for variable b is y. The possible
values for x and y are: I for φid,  for φ , and ⊥for φ⊥.
computed by the functions:
∀fi, fj ∈F : fi f fj ⇔∀x ∈L : fi(x)  fj(x)
The function composition and conﬂuence operations required for constructing ﬂow
functions are:
∀fi, fj, fk ∈F : fk = fi f fj ⇔∀x ∈L : fk(x) = fi(x)  fj(x)
∀fi, fj, fk ∈F : fk = fi ◦fj ⇔∀x ∈L : fk(x) = fi
fj(x)
∀f ∈F : f f φ = f
∀f ∈F : f f φ⊥= φ⊥
Recall that φ and φ⊥are constant functions.
8.3.2
Reducing Function Compositions and Conﬂuences
Constructing ﬂow functions requires reducing expressions involving compositions
and conﬂuences of ﬂow functions to a canonical form. This is diﬀerent from func-
tion applications to actual data ﬂow values. As observed in Section 7.6.2, this is easy
© 2009 by Taylor & Francis Group, LLC

276
Data Flow Analysis: Theory and Practice
to do when ﬂow functions have only constant parts. However, when they have depen-
dent parts also, systematic reductions can be devised only when the ﬂow functions
satisfy some additional requirements. In this section, we show how the function com-
positions and conﬂuences can be reduced and characterize the class of ﬂow functions
for which this can be done.
Function compositions and conﬂuences for bit vector frameworks
For bit vector frameworks the ﬂow function f(x) = (x−Kill)∪Gen does not have
dependent parts. To see how function composition can be reduced, let f2 ◦f1 = f3.
Then we wish to compute Kill3 and Gen3. It is easy to see that
f3(x) = f2(f1(x)) = f2
(x−Kill1)∪Gen1

=
(x−Kill1)∪Gen1
−Kill2

∪Gen2
= x−(Kill1∪Kill2) ∪(Gen1 −Kill2) ∪Gen2
Hence,
Kill3 = Kill1 ∪Kill2
(8.7)
Gen3 = (Gen1 −Kill2) ∪Gen2
(8.8)
To see how function conﬂuences can be reduced, let f2  f1 = f3. First we consider
the case when  is ∪.
f3(x) = f2(x)∪f1(x) = (x−Kill2)∪Gen2
 ∪(x−Kill1)∪Gen1

= x−(Kill1 ∩Kill2) ∪Gen1 ∪Gen2

implying that
Kill3 = Kill1 ∩Kill2
(8.9)
Gen3 = Gen1 ∪Gen2
(8.10)
When  is ∩,
f3(x) = f2(x)∩f1(x) = (x−Kill2)∪Gen2
 ∩(x−Kill1)∪Gen1

= x−(Kill1 ∪Kill2) ∪Gen1 ∩Gen2

Kill and Gen are deﬁned by
Kill3 = Kill1 ∪Kill2
(8.11)
Gen3 = Gen1 ∩Gen2
(8.12)
Thus the reduction of function composition and conﬂuences for bit vector frame-
works can be deﬁned in terms of ∪and ∩alone.
© 2009 by Taylor & Francis Group, LLC

Functional Approach to Interprocedural Data Flow Analysis
277
Function compositions and conﬂuences for general frameworks
When the ﬂow functions have dependent parts also the dependence of data ﬂow
values must also be brought in. This dependence may be dependence of values of
the same entity or across diﬀerent entities. Using the notation from Deﬁnition 4.1,
fi = f α
i , f β
i ,··· , f η
i 
where f α
i
: L '→L is a component function computing the data ﬂow value of entity
α. Given f2 ◦f1 = f3, we need to construct f α
3
for all α. We know that:
f α
2
=
β ∈Σ

f
β→α
2

where f
β→α
2
is a primitive entity function computing the data ﬂow value of α from
β. In order to compute f α
3 , we will need to compose f
β→α
2
with every component
function f β
1 which is deﬁned as:
f β
1 =
γ ∈Σ

f
γ→β
1

In other words, we need to compose the pefs of various components function. This
gives us the ﬁrst restriction on the ﬂow functions for systematic reduction of compo-
sitions: It is not possible to compose ﬂow functions unless the component functions
can be deﬁned in terms of pefs. This rules out full constant propagation and points-to
analysis.
For primary framework f α
3
can be constructed as follows:
f α
3
=
β ∈Σ

f
β→α
2
◦f β
1

=
β ∈Σ

f
β→α
2
◦

γ ∈Σ f
γ→β
1

The need to reduce this suggests the second restriction: The pefs must be distribu-
tive. Although, it is not diﬃcult to construct non-distributive pefs of the form L '→L,
most of the known such pefs in practical data ﬂow frameworks are indeed distribu-
tive. Thus f α
3
reduces to
f α
3
=
β,γ ∈Σ

f
β→α
2
◦f
γ→β
1

(8.13)
Function conﬂuences are relatively easy to deﬁne. Given f3 = f1  f2,
f α
3
= f α
1  f α
2
(8.14)
=
β ∈Σ

f
β→α
1
 f
β→α
2

(8.15)
© 2009 by Taylor & Francis Group, LLC

278
Data Flow Analysis: Theory and Practice
due to distributivity.
When we restrict ourselves to primary frameworks, compositions can be reduced
using the following identities. We use the superscript α →β to show the dependency
of entity β on the entity α only when required.
∀f ,∀z ∈L :
φz ◦f = φz
∀α,β ∈Σ : φ
β→γ
id
◦φ
α→β
id
= φ
α→γ
id
∀α,β ∈Σ :
φ
β→γ
id
◦φz = φz
∀α,β ∈Σ,∀a,b ∈Const : φ
β→γ
id
◦φ
α→β
ab
= φ
α→γ
ab
∀a,b ∈Const,∀z ∈L :
φ
α→β
ab
◦φz = φy
wherey = a ×z+b
∀a,b,c,d ∈Const : φ
β→γ
ab
◦φ
α→β
cd
= φ
α→γ
mn
where m = a ×c, n = a ×d +b
Thus all compositions of pefs can be reduced to a single pef. In some cases, function
conﬂuences can also be reduced:
∀f :
f  φ = f
∀f :
f  φ⊥= φ⊥
∀x,y ∈L :
φx  φy = φz wherez =xy
∀a,b,c,d ∈Const,a  c,b  d : φab  φcd = φ⊥
∀a,b ∈Const,z   : φab  φz = φ⊥
∀a,b ∈Const,a  1,b  0 : φab  φid = φ⊥
Note that φ
α→β
id
φ
β→γ
id
cannot be reduced any further.
Recall that in the case of bit vector frameworks,
∀α  β : φ
α→β
id
= φ 
Hence every component function f α in bit vector frameworks is guaranteed to be
reduced to one of the following three functions: φ , φ⊥, and φ
α→α
id
.
8.3.3
Constructing Summary Flow Functions
Having deﬁned the reductions involving function compositions and conﬂuences, it
is now possible to construct summary ﬂow functions by traversing the CFG. Let
Φr
v : L '→L denote the summary ﬂow function associated with program point v in
procedure r. It represents the eﬀect of all paths from Entry(Startr) to v and from v to
Exit(Endr). If appropriate BI is computed for procedure r, applying Φr
v to it results
in the desired data ﬂow information associated with v.
Φr
v is computed from Φr
u of u ∈neighbours(v) as deﬁned below:
Φr
v =
u∈neighbours(v) fu→v ◦Φr
u
© 2009 by Taylor & Francis Group, LLC

Functional Approach to Interprocedural Data Flow Analysis
279
Block
Flow functions fi(x) in terms of pefs
pefs computing
faintness of a
from a,b,c,d
pefs computing
faintness of b
from a,b,c,d
pefs computing
faintness of c
from a,b,c,d
pefs computing
faintness of d
from a,b,c,d
a
b
c
d
a
b
c
d
a
b
c
d
a
b
c
d
Startm φ φ φ φ φ φ φ φ φ φ φ φ φ φ φ φ 
n1
φid φ φ φ φ φid φ φ φ φ φid φ φ⊥φ⊥φ⊥φ⊥
n2
φid φ φ φid φ φid φ φid φ φ φid φ φ φ φ φ 
Endm φid
φ φ φ φ φid φ φ φ φ φid φ φ φ φ φid
Startp φid φ φ φ φ φ φ φ φ φ φid φ φ⊥φ⊥φ⊥φ⊥
n3
φid φ φid φ φ φid φid φ φ φ φ φ φ φ φ φid
Endp
φid φ φ φ φ φid φ φ φ φ φid φ φ φ φ φid
Startq φ φ φ φ φ φid φ φ φ φ φid φ φ φ φ φid
Endq
φid φ φ φ φid φid φ φ φ φ φid φ φ φ φ φid
FIGURE 8.13
Flow functions for faint variables analysis of the program in Figure 8.11 expressed
in terms of pefs.
where the ﬂow function at the entry point is φid and does not change any further.
More speciﬁcally, Φr
w = φid and ∀u : fu→w = φ ; such that w is Entry(Startr) for
forward ﬂows and Exit(Endr) for backward ﬂows. Φr
v is iteratively computed by
taking the initial value as φ .
If edge u →v represents a basic block calls procedure s, fu→v is replaced by the
summary ﬂow function for procedure s. It is deﬁned by Φs
w where w is chosen based
on the following criterion:
• If fu→v is a forward ﬂow function mapping the data ﬂow information before
the call to the data ﬂow information after the call, w is Exit(Ends).
• If fu→v is a backward ﬂow function mapping the data ﬂow information after
the call to the data ﬂow information before the call, w is Entry(Starts).
The termination of construction of summary ﬂow functions depends on the nature
of component ﬂow functions and the structure of lattice of data ﬂow values. Since
we require the pefs to be distributive and closed under composition, each component
ﬂow function f α constituting Φp
v can be reduced to the following canonical form
f α =
i≥0,j≥0
φ0
i ◦φ1
i ◦...◦φ j
i

(8.16)
where φ j
i could be any pef in the framework.
© 2009 by Taylor & Francis Group, LLC

280
Data Flow Analysis: Theory and Practice
Procedure Flow
Function
Deﬁning
Expression
Iteration #1
Changes in
iteration #2
Gen
Kill
Gen
Kill
p
Φp
Endp
fEndp
{c,d}
∅
Φp
n3
fn3 ◦Φp
Endp
{a,b,d}
{c}
Φp
c4
fq ◦Φp
Endp
∅
{a,b,c,d} {d} {a,b,c}
Φp
Startp
fStartp ◦Φp
n3 Φp
c4
 {a,d}
{b,c}
fp
Φp
Startp
{a,d}
{b,c}
q
Φq
Endq
fEndq
{a,b}
{a}
Φq
c3
fp ◦Φq
Endq
{a,d}
{a,b,c}
Φq
Startq
fStartp ◦Φq
c3
{d}
{a,b,c}
fq
Φq
Startq
{d}
{a,b,c}
FIGURE 8.14
Summary ﬂow functions of procedures p and q required for interprocedural liveness
analysis of the program in Figure 8.1 on page 260. The ﬂow functions compute the
value at the entry of the blocks.
In order to guarantee the termination of construction of f α, it should be pos-
sible to bound both the number of terms as well as the size of each term in any
expression of the form in Equation (8.16). Bounding the size of each term requires
that it should be possible to reduce every unbounded sequence of compositions by
a bounded sequence. Bounding the number of terms requires that an inﬁnite meet
must be equivalent to the meet of a ﬁnite number of terms.
For the primary frameworks, the sequence of compositions always reduces to a
single pef. Note that this does not bound the number possible pefs. For example,
consider the pef φ11 in linear constant propagation. It increments the value of its
argument by 1. If we compose k such pefs, the resulting pef is φ1k and the length
of the sequence of compositions is bounded by 1 for all k. However, there is an
unbounded number of φ1k; in particular, one for each k. Thus bounding the size of
each term does not automatically bound the number of terms in the canonical form.
Thus it becomes important to ensure that the conﬂuence of the terms in an ex-
pression of the form in Equation (8.16) can be reduced to a canonical form. This is
possible because of the following reason. In the worst case, each term in the canon-
ical form could compute a distinct value in L. Although, the number of such values
may not be ﬁnite, we restrict our analysis to those frameworks in which every de-
scending chain in L is ﬁnite. Thus every descending chain ending on a distinct i in
the canonical form can be represented by a ﬁnite chain. All that remains is to identify
© 2009 by Taylor & Francis Group, LLC

Functional Approach to Interprocedural Data Flow Analysis
281
Flow
function
pefs computing
faintness of a
from a,b,c,d
pefs computing
faintness of b
from a,b,c,d
pefs computing
faintness of c
from a,b,c,d
pefs computing
faintness of d
from a,b,c,d
a
b
c
d
a
b
c
d
a
b
c
d
a
b
c
d
Φp
Endp
φid φ φ φ φ φid φ φ φ φ φid φ φ φ φ φid
Φp
n3
φid φ φid φ φ φid φid φ φ φ φ φ φ φ φ φid
Φp
c4
φ φ φ φ φ φ φ φ φ φ φ φ φ φ φ φ 
Φp
Startp
φid φ φid φ φ φ φ φ φ φ φ φ φ⊥φ⊥φ⊥φ⊥
Φq
Endq
φid φ φ φ φid φid φ φ φ φ φid φ φ φ φ φid
Φq
c3
φid φ φid φ φ φ φ φ φ φ φ φ φ⊥φ⊥φ⊥φ⊥
Φq
Startq
φ φ φ φ φ φ 
φ φ φ φ φ φ φ φ φ φ⊥
FIGURE 8.15
First iteration of constructing summary ﬂow functions of procedure p and q for in-
terprocedural faint variables analysis of the program in Figure 8.11 on page 274.
The ﬂow functions compute the value at the entry of the blocks. Highlighted entries
show the pefs that diﬀer from the corresponding pefs in the local ﬂow function fi(x)
provided in Figure 8.13 on page 279.
this when summary ﬂow functions are being constructed.
If we examine the primary pefs, the only pef that may give rise an inﬁnite num-
ber of terms in the canonical form is φab. Fortunately, its conﬂuence with every
ﬂow function can be reduced due to the structure of L in constant propagation. All
other pefs are guaranteed to be ﬁnite in number. Hence the canonical form is al-
ways bounded and the construction of summary ﬂow functions follows for primary
frameworks.
Example 8.7
Figure 8.14 on the preceding page provides the summary ﬂow functions for our
example program. We ﬁrst analyze procedure p. We compute summary ﬂow
functions associated with Entry(n) of each block n; the ﬂow function associated
with Exit(n) is left implicit. Each ﬂow function is constructed by computing
Kill and Gen sets for function composition using Equations (8.7) and (8.8).
The conﬂuence required in computing Φp
Startp uses Equations (8.9) and (8.10)
to compute Kill and Gen sets. The analysis initially uses φ (x) = x−{a,b,c,d}
for fq while computing Φp
c4. Hence the Gen and Kill sets of Φp
c4 are approximate
in the ﬁrst iteration. Using these sets, fp is computed and is used in analyzing
procedure q. The resulting fq is diﬀerent from φ . This causes change in Φp
c4
in the second iteration. However, this change does not aﬀect Φp
Start, and hence
© 2009 by Taylor & Francis Group, LLC

282
Data Flow Analysis: Theory and Practice
fp remains same implying that fq computed in the ﬁrst iteration does not
change.
It is not surprising that Kill p and Killq computed above are identical to
MustKill p and MustKillq computed in Figure 8.2 on page 262. Similarly, Genp
and Genq computed here are identical to MayUse p and MayUseq computed
in Figure 8.2 on page 262.
Example 8.8
Figure 8.15 on the preceding page shows the ﬁrst iteration in constructing
summary ﬂow functions for faint variables analysis.
Since the description
of the functions is very verbose, the relevant entries have been highlighted.
While analyzing procedure p, the summary ﬂow function for c4 is assumed
to be φ . When Φp
Startp is computed, we discover that the faintness of a now
depends on c also. This is a result of the composition f
a→a
Startp ◦f
c→a
n3
. When
this is merged with the earlier pef f
c→a
Startp which was φ , it becomes φid. When
procedure q is analyzed, the ﬂow function for c3 is fp which is the same as
Φp
Startp. The summary ﬂow function as Φq
Startq so computed represents the fact
that b and c are faint at the entry of Startq and d is not faint. Observe that
this is due to the eﬀect of the called procedure p. The resulting fq is diﬀerent
from the earlier φ .
When the new value of fq is used in the second iteration, it changes the pef
f
d→d
c3
from φ to φ⊥suggesting that d is not faint at the entry of c4. However,
this does not cause any change in Φp
Startp and the process of constructing
summary ﬂow functions terminates.
8.3.4
Computing Data Flow Information
Φr
u represents the eﬀect of the paths from Entry(Startr) to u for forward problems and
from u to Exit(Endr) for backward problems. This eﬀect includes the intraprocedural
data ﬂow information as well as synthesized data ﬂow information resulting from
the calls along these paths. Thus the data ﬂow information associated with u can
be computed by the function application Φr
u(BIr) where BI represents the data ﬂow
information inherited by r from its callers.
Let CallsTor denote the set of callers of r. These are simply the predecessors of
r in the call graph. Let CallsTor(s) denote the call sites calling r from procedure s.
Given BImain, the data ﬂow information associated with program point u in procedure
r, denoted xr
u, is computed as follows:
BIr =
s∈CallsTor
ci∈CallsTor(s)
Φr
ci(BIs)
(8.17)
xr
u = Φr
u(BIr)
(8.18)
© 2009 by Taylor & Francis Group, LLC

Functional Approach to Interprocedural Data Flow Analysis
283
Procedure
BI
Data ﬂow
variable
Summary ﬂow function
Data ﬂow
Name
Deﬁnition
value
main
∅
InEndm
Φm
Endm
BIm ∪{a,c}
{a,c}
Inc2
Φm
c2

BIm −{a,b,c}∪{d}
{d}
Inn2
Φm
n2

BIm −{a,b,c,d}∪{a,b}
{a,b}
Inn1
Φm
n1
BIm −{a,b,c,d}∪{a,b,c,d} {a,b,c,d}
Inc1
Φm
c1

BIm −{a,b,c,d}∪{a,d}
{a,d}
InStartm
Φm
Startm
BIm −{a,b,c,d}
∅
p
{a,b,c,d}
InEndp
Φp
Endp
BIp ∪{c,d}
{a,b,c,d}
Inn3
Φp
n3

BIp −{c}∪{a,b,d}
{a,b,d}
Inc4
Φp
c4

BIp −{a,b,c}∪{d}
{d}
InStartp
Φp
Startp

BIp −{b,c}∪{a,d}
{a,d}
q
{a,b,c,d}
InEndq
Φq
Endq

BIq −{a}∪{a,b}
{a,b,c,d}
Inc3
Φq
c3

BIq −{a,b,c}∪{a,d}
{a,d}
InStartq
Φq
Startq

BIq −{a,b,c}∪{d}
{d}
FIGURE 8.16
Data ﬂow information computed by interprocedural liveness analysis of the program
in Figure 8.1 on page 260 using the summary ﬂow functions deﬁned in Figure 8.14
on page 280.
The initial value of BIs is assumed to be  ; it is assumed that appropriate boundary
point is chosen depending on the ﬂows i.e., Entry(Startp) for forward ﬂows and
Exit(Endp) for backward ﬂows.
Example 8.9
Figure 8.16 shows the result of interprocedural liveness analysis of the pro-
gram in Figure 8.1 on page 260 using the summary ﬂow functions deﬁned in
Figure 8.14 on page 280. For simplicity, we show the information associated
with block entries only. The liveness information of the main procedure (ab-
breviated by m) can be computed in a single iteration from BI = ∅. For live
variables analysis
BIp = Inn1 ∪InEndq
= Inn1 ∪Φq
Endq(BIq)
BIq = InEndm ∪InEndp = InEndm ∪Φp
Endp(BIp)
© 2009 by Taylor & Francis Group, LLC

284
Data Flow Analysis: Theory and Practice
Procedure
BI
Data ﬂow
variable
Summary ﬂow function
Data ﬂow
Name
Deﬁnition
value
main
{a,b,c,d}
InStartm
Φm
Startm
BIm ∪{a,b,c,d}
{a,b,c,d}
Inc1
Φm
c1

BIm −{d}∪{b,c}
{a,b,c}
Inn1
Φm
n1

BIm −{d}∪{a,c}
{a,b,c}
Inn2
Φm
n2

BIm −{a}∪{c,d}
{b,c,d}
Inc2
Φm
c2
(BIm −{d}∪{a,b,c}
{a,b,c}
InEndm
Φm
Endm
BIm
{a,b,c,d}
p
{a,b,c}
InStartp
Φp
Startp

BIp −{d}∪{a | c  BIp}∪{b,c}
{a,b,c}
Inn3
Φp
n3

BIp −{a,b | c  BIp}∪{c}
{a,b,c}
Inc4
Φp
c4
(BIp −{d}∪{a,b,c}
{a,b,c}
InEndp
Φp
Endp
BIp
{a,b,c}
q
{a,b,c}
InStartq
Φq
Startq
(BIq −{d}∪{a,b,c}
{a,b,c}
Inc3
Φq
c3

BIq −{d}∪{a | c  BIq}∪{b,c}
{a,b,c}
InEndq
Φq
Endq
BIq −{b | a  BIq}
{a,b,c}
FIGURE 8.17
Interprocedural faint variables analysis for the program in Figure 8.11 on page 274
using the summary ﬂow functions constructed in Example 8.8.
Thus BIp and BIq are mutually dependent on each other. Since Inn1 is {a,b,c,d}
which is the ⊥element of the lattice, BIp cannot change any further. From
this, InEndp is computed which turns out to be {a,b,c,d}. Thus BIq is also
{a,b,c,d} and does not change any further.
Our analysis shows that OutStartm contains only a and d. Thus the assign-
ments to b and c in Startm are redundant and can be eliminated. Observe
that although c is used in Endp, it is found to be dead at the entry of c4. This
is because the recursion ending path must pass through block n3 before the
execution reaches Endp from c4. Due to the assignment in n3, c is not live at
the entry of c4.
Example 8.10
Figure 8.17 shows the result of interprocedural faint variables analysis of the
program in Figure 8.11 on page 274 using the summary ﬂow functions com-
puted in Example 8.8.
In faint variables analysis, BI for main is {a,b,c,d}
© 2009 by Taylor & Francis Group, LLC

Functional Approach to Interprocedural Data Flow Analysis
285
because every variable is faint at the end of the program. The data ﬂow in-
formation in main can be computed in a single iteration. BI for p and q is
deﬁned by:
BIp = Inn1 ∩InEndq
= Inn1 ∩Φq
Endq(BIq)
BIq = InEndm ∩InEndp = InEndm ∩Φp
Endp(BIp)
For computing BIp, Inn1 is {a,b,c} and Φq
Endq(BIq) is assumed to be  which
is {a,b,c,d} for faint variables analysis. Thus BIp = {a,b,c} and the data ﬂow
information for all blocks in p also turns out to be {a,b,c}. Thus BIq is also
{a,b,c} and so is the data ﬂow information for all blocks in q.
Thus we conclude that the only relevant assignment in procedures p and
q is the assignment to b in Startp for local use in the condition. Local con-
stant propagation can make even this assignment redundant. To see why the
assignment in Endq is redundant, consider the paths starting at Endq. If all
recursive calls to q are not over, the execution can only reach Endp and from
there Endq. When all recursive calls to q ﬁnish, the execution reaches Endmain
directly or n1 through Endp. Thus there is no use of the value assigned to a in
Endq other than in the same assignment. Hence this assignment is redundant.
This makes both a and b faint making the assignment to c in n3 redundant.
Discovering this through live variable analysis would require repeatedly per-
forming dead code elimination and live variables analysis.
8.3.5
Enumerating Summary Flow Functions
The construction method explained in previous sections requires the component ﬂow
functions to consist of primitive ﬂow functions only. If a component ﬂow function
requires composite entity functions, the method is not applicable to the framework.
The main diﬃculty is in being able to compose entity functions and reduce the com-
positions. Function applications on the other hand do not require any reduction to
be performed. This leads to an interesting possibility: Instead of constructing closed
form summary ﬂow functions, the ﬂow functions can be enumerated in terms of input
output pairs by identifying the data ﬂow values that could appear in the program as
inputs to a ﬂow function. This is possible because every program has a well deﬁned
BI and starting from BI the relevant data ﬂow values can be constructed.
We write the enumerated form of function Φr
u as follows:
EΦr
u = x '→y | x '→x ∈Φr
w,y = Φr
u(x)"
where w is the boundary point of r which is chosen depending upon the direc-
tion of ﬂows. For backward ﬂows w is Exit(Endr) whereas for forward ﬂows w
is Entry(Startr).
Enumeration for the entire program begins at the boundary point w of the main
procedure and is chosen to be the identity function restricted to BI.
E

Φmain
w

= BI '→BI "
© 2009 by Taylor & Francis Group, LLC

286
Data Flow Analysis: Theory and Practice
a = 5
b = 3
c = 7
Startmain
Call p
c1
n1
a = a +2
print c n1
print a +c
Endmain
b = 2
if (...)
Startp
n2 c = a+b n2
Call q
c2
print c
Endp
a = 1
Startq
Call p
c3
a = a∗b
Endq
FIGURE 8.18
Program to illustrate enumeration of ﬂow functions for full constant propagation.
Similarly, enumeration for a given procedure r also begins at its boundary point w;
however, it gets deﬁned by the enumerated summary ﬂow functions corresponding
to the call sites that call procedure r:
EΦr
w =
#
x '→x | y '→x ∈E

Φs
ci

, ci is a call to r in procedure s
$
(8.19)
For other program points v in p:
EΦr
v =
#
x '→y | y =
u∈neighbours(v) fu→v(z), x '→z ∈EΦr
u
$
(8.20)
If edge u →v represents a basic block calls procedure s, fu→v is replaced by the
summary ﬂow function for procedure s. It is deﬁned by EΦs
w where w is chosen
based on the following criterion:
• If fu→v is a forward ﬂow function mapping the data ﬂow information before
the call to the data ﬂow information after the call, w is Exit(Ends).
• If fu→v is a backward ﬂow function mapping the data ﬂow information after
the call to the data ﬂow information before the call, w is Entry(Starts).
Equations (8.19) and (8.20) are computed with ∅as the initial value.
Once all summary ﬂow functions are enumerated, the ﬁnal data ﬂow values are
computed by simply merging the output of a summary ﬂow function for each relevant
input that has already been identiﬁed:
xu =
x'→y ∈EΦr
u y
(8.21)
Observe that this is diﬀerent from the method of using the closed form summary ﬂow
functions; Equations (8.17) and (8.18) require a ﬁxed point computation whereas
Equation (8.21) does not.
© 2009 by Taylor & Francis Group, LLC

Functional Approach to Interprocedural Data Flow Analysis
287
Procedure Block
EΦr
u
Iteration #1
Iteration #2
Iteration #3
p
Startp 5,3,7 '→5,3,7 5,3,7 '→5,3,7
1,2,7 '→1,2,7
5,3,7 '→5,3,7
1,2,7 '→1,2,7
n2
5,3,7 '→5,2,7 5,3,7 '→5,2,7
1,2,7 '→1,2,7
5,3,7 '→5,2,7
1,2,7 '→1,2,7
c2
5,3,7 '→5,2,7 5,3,7 '→5,2,7
1,2,7 '→1,2,7
5,3,7 '→5,2,7
1,2,7 '→1,2,7
Endp 5,3,7 '→5,2,7 5,3,7 '→5,2,7
1,2,7 '→1,2,3
5,3,7 '→52,2,7 3
1,2,7 '→12,2,3
fp
5,3,7 '→5,2,7 5,3,7 '→5,2,7
1,2,7 '→1,2,3
5,3,7 '→⊥,2,⊥
1,2,7 '→⊥,2,3
q
Startq 5,2,7 '→5,2,7 5,2,7 '→5,2,7
1,2,7 '→1,2,7
5,2,7 '→5,2,7
1,2,7 '→1,2,7
c3
5,2,7 '→1,2,7 5,2,7 '→1,2,7
1,2,7 '→1,2,7
5,2,7 '→1,2,7
1,2,7 '→1,2,7
Endq
∅
5,2,7 '→1,2,3
1,2,7 '→1,2,3
5,2,7 '→⊥,2,3
1,2,7 '→⊥,2,3
fq
∅
5,2,7 '→2,2,3
1,2,7 '→2,2,3
5,2,7 '→⊥,2,3
1,2,7 '→⊥,2,3
FIGURE 8.19
Enumerated summary ﬂow functions for constant propagation over procedure p and
q of the program in Figure 8.18.
Example 8.11
Consider the program in Figure 8.18 on the preceding page for constant prop-
agation analysis. When procedure p is called from main, the values of a, b,
and c are 5, 3, and 7 respectively. If p does not call q at all, then the values of
a, b, and c at Endp are 5, 2, and 7 respectively. However, if q is called, then
the value of a is modiﬁed in Startq and Endq. When the recursion unwinds,
the value of c gets modiﬁed. Variable b is assignment 2 in every call to p.
Thus when p returns in main, the value of b is 2 whereas a and c are not
constants.
Iterative computation of EΦr
u for procedures p and q is shown in Fig-
ure 8.19. In the ﬁrst iteration, E

Φq
Endq

remains ∅: mapping 5,2,7 '→1,2,7
at c3 indicates that the value 5,2,7 reaching Startq is mapped to the value
1,2,7 at c3. However, the mapping for 1,2,7 in procedure p has not been
discovered so far; the only mapping for procedure p discovered in ﬁrst iter-
ation is 5,3,7 '→5,2,7. Second iteration discovers 1,2,7 '→1,2,3 for p.
This leads to 5,2,7 '→2,2,3 and 1,2,7 '→2,2,3 for procedure q due the
© 2009 by Taylor & Francis Group, LLC

288
Data Flow Analysis: Theory and Practice
assignment a = a ∗b in Endq. This inﬂuences c2 and Endp where it is discovered
that a can be mapped to 5 if no call to q is made, 1 when p is called from q, and
2 when q returns in p. Similarly, c can be 7 and 3 depending upon whether
q is called or not. Thus fp records 5,3,7 '→⊥,2,⊥ and 1,2,7 '→⊥,2,⊥
whereas fq contains 5,2,7 '→⊥,2,3 and 1,2,7 '→⊥,2,3.
Although we have presented the enumeration of summary ﬂow function as a ﬁxed
point computation performed using a round-robin iterative method, in practice, this
may not be eﬃcient since a program may consist of hundreds of procedures. The
data ﬂow values discovered as inputs to summary ﬂow functions may reach limited
portions of the program. In such situation, it is preferable to use a work list method
and propagate the values to the relevant portions of the program.
We outline a work list based method in the following. The work list contains pairs
(u,x) that represents the fact that EΦr
u has been computed for input value x and its
eﬀect needs to be propagated further. The work list is initialized with the pair (w,BI)
where w is the boundary of the main procedure. As is typical in a work list based
method, an entry from work list is removed, its eﬀect is propagated to its neighbours,
and new entries whose eﬀect needs to be propagated are added to the work list. This
process is repeated until the work list becomes empty.
We use the following notation to describe propagation.
• The meaning of EΦr
ux = y is that the mapping x '→y is included in EΦr
u.
Initially, EΦr
u is assumed to be empty.
• When EΦr
ux appears on the right hand side of an assignment, it denotes
y such that x '→y ∈EΦr
u. If there is no mapping for x in EΦr
u, EΦr
ux
denotes  .
The meaning of propagating the eﬀect of a pair (u,x) to its neighbour v is that
the summary ﬂow function EΦr
v should be constructed. Observe that v need not
be in the same procedure. It could be a program point in a caller procedure or a
called procedure. More precisely, propagation of (u,x) for forward ﬂows is deﬁned
as follows.
• When u is a call node ci calling procedure s. Let the successor of u be node
v in the same procedure. Then EΦr
v should be updated with the value of the
result of applying fs to the value reaching ci.
EΦr
vx = EΦr
vx  EΦs
w

E

Φr
ci
x
where w is Exit(Ends). It is possible that the EΦs
w may not have been deﬁned
for x. Thus the eﬀect should be propagated to Starts as follows:
E

Φs
w

E

Φr
ci
x
= E

Φr
ci
x
where w is Entry(Starts). If EΦr
v changes, add the pair (u,x) to the work
list. If E

Φr
w

changes, add the pair

w,E

Φr
ci
x
to the work list.
© 2009 by Taylor & Francis Group, LLC

Functional Approach to Interprocedural Data Flow Analysis
289
x = x−2
if (...)
Startp
call p
c1
x = x+2
Endp
FIGURE 8.20
Example of linear constant propagation for which a closed form summary ﬂow func-
tion can be created but summary ﬂow functions cannot be enumerated.
• When u is Exit(Endr). In this case, the summary ﬂow function of callers need
to be updated. Find out all callers t of r such that E

Φt
cj
y = x. Let the
successor of cj in t be v. Then,
E

Φt
v
y = E

Φt
v
y  EΦr
ux
If EΦt
v changes, add the pair (v,y) to the work list.
• When u is some other program point. Update the summary ﬂow function of
every neighbour v of u:
EΦr
vx = EΦr
vx  fu→v
EΦr
ux
If EΦr
v changes, add the pair (u,x) to the work list.
The main diﬀerence between the two methods of enumerating the summary ﬂow
function is the fundamental diﬀerence between a round-robin method and a work
list method: In a round-robin method, the relevant computation for a given program
point u is performed by incorporating the eﬀect of all its neighbours. In a work list
method, the inﬂuence of a program point u is propagated to all its neighbours v and
the value at u is updated.
The main advantage of enumerating summary ﬂow functions is that there is no
need to reduce function compositions because the method relies on computing actual
values. However, the main limitation of computing values is that it may not terminate
for a lattice with inﬁnite values. If ﬂow functions can be reduced, the closed form
summary ﬂow functions can be used for lattices with inﬁnite values also.
Example 8.12
Figure 8.20 shows an example of linear constant propagation. If we construct
closed form summary ﬂow functions, we discover that f x in the summary
ﬂow function at Exit(Endp) along the edge Startp →Endp is a composition of
φ1,−2 and φ1,2. Thus the ﬂow function representing p along the call free path
© 2009 by Taylor & Francis Group, LLC

290
Data Flow Analysis: Theory and Practice
is φid. Along the other path, f x in Φp
c1 is φ1,−2. This is composed with the
f x
p = φid to construct Φp
Endp along this path resulting in Φp
Endp = φ1,−2 when
this is composed with fEndp, f x
p
is discovered to be φid along this path also.
Thus fp is found to be φid along all paths and the method concludes that the
value of x before a call to x and after the call remains same.
To see how the enumeration method works for this program, we will need
to know the value of x when the outermost call to p is made. Assume that x
is 10 when p is called from outside. Let w denote Exit(Endp). Then we have
10 '→10 in E

Φp
Startp

and 10 '→8 in E

Φp
c1

. Along the other path,
we get 10 '→10 in E

Φp
w

. In order to propagate the eﬀect of 10 '→8
in E

Φp
c1

, we ﬁnd out if have 8 '→... in E

Φp
w

. Since we don’t have it,
we have to propagate this eﬀect to Startp thereby adding 8 '→8 to both
E

Φp
Startp

and E

Φp
w

. In the next iteration we check if have 6 '→... in
E

Φp
w

. The process does not terminate because the recursive calls generate
an inﬁnite number of values for x.
8.4
Summary and Concluding Remarks
In this chapter we have presented methods that construct context independent sum-
mary ﬂow functions. Side eﬀects analysis constructs summary ﬂow functions for a
ﬁxed set of side eﬀects. The key idea of this approach is to reduce expressions of
sets representing ﬂow functions. These reductions compute the sets that represent
the required summary ﬂow functions.
A natural extension of this idea results in a whole program analysis method that
computes context independent summary ﬂow functions for a given data ﬂow frame-
work. This extension attempts to reduce expressions of functions instead of expres-
sions of sets. The feasibility of reducing expressions consisting of function compo-
sitions and conﬂuences can be established in terms of the pefs that make up ﬂow
functions of a given data ﬂow framework. A related concern of this method is that
the canonical form of reduced expressions may not be compact and may require a lot
of space. Both these concerns are addressed by the method that enumerates functions
in terms of observed input output behaviour. This method does not need to reduce
expressions of functions. However, its main limitation is that it may not terminate if
the lattice of data ﬂow values is not ﬁnite.
An orthogonal issue presented in this chapter is to construct functions that repre-
sent mappings of formal parameters across call sequences.
© 2009 by Taylor & Francis Group, LLC

Functional Approach to Interprocedural Data Flow Analysis
291
8.5
Bibliographic Notes
The side eﬀect analysis presented in this chapter is a generalization of the work by
Barth [13, 14] and Banning [12]. Callahan [19] has tried to solve the same problem
using a diﬀerent representation called program summary graph. The alias analysis
of parameters is based on the work by Cooper [25] and by Cooper and Kennedy [26].
The whole program analysis is based on the classical functional approach deﬁned
by Sharir and Pnueli [93]. However, unlike Sharir and Pnueli, we use primitive
entity functions to describe reductions of ﬂow functions. The alternative approach of
enumerating summary ﬂow functions is an abstract model of the tabulation method
proposed by Sharir and Pnueli. The concept of partial transfer functions by Wilson
and Lam [107] can be viewed as similar to the tabulation method. However, it is
context insensitive in recursive calls.
Another interesting method of interprocedural data ﬂow analysis that belongs to
the category of functional approaches is the method based on graph reachability
proposed by Reps, Horwitz and Sagiv [82, 87]. This approach handles exactly the
same class of frameworks that are handled by the method presented in this chapter.
© 2009 by Taylor & Francis Group, LLC

© 2009 by Taylor & Francis Group, LLC

9
Value-Based Approach to Interprocedural
Data Flow Analysis
In this chapter, we present the other paradigm of context and ﬂow sensitive whole
program analysis. This approach does not involve precomputation of summary ﬂow
functions. Instead, it directly computes the data ﬂow information and propagates the
inherited data ﬂow information from callers to callees and the synthesized data ﬂow
information from callees to callers.
We ﬁrst present the program model and some basic concepts underlying this ap-
proach. Then we present a method for precise ﬂow and context sensitive interproce-
dural data ﬂow analysis for bit vector frameworks. The subsequent section general-
izes this method to general frameworks.
9.1
Program Model for Value-Based Approaches to Interproce-
dural Data Flow Analysis
A value-based approach of interprocedural data ﬂow analysis views a program as
a single large procedure with diﬀerent kinds of paths rather than as a collection of
independent procedures. With this view of programs, interprocedural data ﬂow anal-
ysis reduces to identifying the origins of ifps and traversing them; the only diﬀerence
is that these ifps are interprocedural rather than intraprocedural and hence must be
sensitive to the calling contexts. This is required to distinguish between inherited
data ﬂow information propagated from diﬀerent callers. This enables propagation of
synthesized information to appropriate callers.
A value-based approach uses a supergraph which has been explained in Sec-
tion 7.2. Let a given call site ci in procedure r call procedure s. Then, logically the
program points Entry(Ci) and Exit(Ri) belong to the caller procedure r in that the
data ﬂow information associated with these program points holds for procedure r.
The program points Exit(Ci) and Entry(Ri) belong to the callee procedure s as the
data ﬂow information associated with these points holds for procedure s.
The roles of call and return nodes in a supergraph cannot be abstracted out into
a single kind of node; they must be explicated for value-based interprocedural data
ﬂow analysis. Hence unlike intraprocedural data ﬂow analysis, a general formulation
that is uniformly applicable to both forward and backward data ﬂow frameworks does
293
© 2009 by Taylor & Francis Group, LLC

294
Data Flow Analysis: Theory and Practice
not seem natural; the ﬂow function of the proposed abstract node representing call
and return nodes will have to be predicated on whether the formulation is being used
for forward ﬂows or backward ﬂows. Hence we restrict our formulations to forward
data ﬂow problems for simplicity of exposition,
As observed in Chapter 7, traversing all paths in a supergraph results in context
insensitive analysis. Context sensitivity requires that the propagation of data ﬂow
information must be restricted to interprocedurally valid paths.
DEFINITION 9.1
A path from Startmain to a block n in a supergraph is
an interprocedurally valid path if
1. for every edge Endr →Ri in the path, there is a matching edge Ci →Startr
in the path, and
2. if the subpath from Ci to Ri does not contain any other call or return
node, then after replacing this subpath by a single (ﬁctitious) edge, the
reduced path is interprocedurally valid.
At the base level, a path consisting of only intraprocedural edges is a valid inter-
procedural path. Similarly, a path in which there is no return edge is also a valid
interprocedural path; the validity constraint arises only when a return edge is en-
countered. This is because a return edge that appears in a path must correspond to
the last call edge in the path. This constraint facilitates ensuring that the data ﬂow
information from a callee procedure is propagated back to the correct caller proce-
dure.
Let call site ci call procedure r. In an interprocedurally valid path, this procedure
call is represented by a path segment starting with the call edge Ci →Startr and
ending in the corresponding return edge Endr →Ri. Every such call appearing in an
interprocedurally valid path can be abstracted out by a basic block making the call;
a path containing this basic block remains an interprocedurally valid path.
We view call and return nodes as being signiﬁcant nodes because they deﬁne the
structure of an interprocedural path. Often we will restrict a path to the signiﬁcant
nodes appearing in it. For interprocedural validity, the structure of a path in terms of
signiﬁcant nodes should be derivable from the following context free grammar with
IPVP as its start symbol:
IPVP →ﬁnishedCalls unFinishedCalls
ﬁnishedCalls →Ci ﬁnishedCalls Ri
| ﬁnishedCalls ﬁnishedCalls
| 
unFinishedCalls →Ci ﬁnishedCalls unFinishedCalls
| 
where Ci and Ri are placeholders for terminal symbols representing corresponding
call and return nodes in a supergraph.
© 2009 by Taylor & Francis Group, LLC

Value-Based Approach to Interprocedural Data Flow Analysis
295
DEFINITION 9.2
An ifp ρ from a program point u to a program point
v is a an interprocedurally valid ifp if it is a suﬃx of some interprocedurally
valid path.
An important requirement of traversing interprocedurally valid ifps is discovering
matching Ci for every Ri encountered in a path in the supergraph in order to estab-
lish interprocedural validity of the ifp. Value-based interprocedural analyses achieve
this by embedding the information about contexts within the data ﬂow values being
computed. This information represents the call nodes Ci encountered in the paths
traversed for computing the data ﬂow value. In the presence of recursion, precise
embedding of context information becomes an important issue in value-based inter-
procedural data ﬂow analysis. The methods presented in this chapter handle recursive
program without compromising on precision.
DEFINITION 9.3
A calling context of procedure r is deﬁned as a se-
quence of callers of r starting from the main procedure.
A calling context σ is denoted by a string c1 ···ck of call site names. This string
represents a call sequence r1,...,rk starting from the main procedure, such that
ci ∈CallsInri and ci ∈CallsTori+1. Note that the call sites in a call string or the called
procedures in a call chain need not be distinct.
Value-based interprocedural data ﬂow analysis is deﬁned in terms of data ﬂow
values that are pairs of the form σ,x where σ represents the context and x ∈L is the
actual data ﬂow value. We call a pair σ,x a qualiﬁed data ﬂow value and denote
it by X to distinguish it from x. In some cases, X may be a set of pairs σ,α where
α ∈Σ is an entity. Where the context of usage is suﬃcient to distinguish between the
two, we drop the adjective “qualiﬁed” and refer to both X and x as data ﬂow values.
DEFINITION 9.4
A path ρ in a supergraph is:
• An intraprocedural segment if ρ contains intraprocedural nodes only.
• A call segment if ρ contains intraprocedural nodes and at least one call
node but no return node.
• A return segment if ρ contains intraprocedural nodes and at least one
return node but no call node.
• A symmetric segment if ρ is an interprocedurally valid path from a call
node Ci in procedure r to a return node R j also in procedure r.
An intraprocedural segment does not alter the context in a qualiﬁed data ﬂow value
whereas call and return segments do. A symmetric segment represents a sequence
of ﬁnished calls—it alters the context within the segment but restores it at the end of
the segment.
© 2009 by Taylor & Francis Group, LLC

296
Data Flow Analysis: Theory and Practice
Apart from handling context, an interprocedural analysis must also handle scope
rules and parameter passing mechanisms. These issues are handled as explained in
Section 8.2. For simplicity, we assume that our programs have only global variables.
9.2
Interprocedural Analysis Using Restricted Contexts
Bit vector frameworks have special properties that make it possible to perform in-
terprocedural analysis by remembering a restricted amount of context. The two key
insights that this algorithm uses are:
• For bit vector frameworks, the default value of an entity α at a program point
u, denoted x α
u , can be considered as  . If it becomes ⊥, then it is suﬃcient
to make x α
v = ⊥for all v ∈neighbours(u) such that f u→v
α
is φid. This eﬀect
needs to be propagated transitively.
• This propagation can be done independently of any other ifp. Thus there is no
need to consider any other ifp of α or any ifp of some other entity β.
This allows fully context sensitive analysis by restricting the length of calling
context σ to 1 at each call point in a sequence of calls. Reconstructing the calling
contexts transitively along a call chain does not introduce any imprecision—it is
possible to propagate diﬀerent synthesized data ﬂow information from a procedure
to diﬀerent callers of the procedure.
Let the qualiﬁed data ﬂow value Xu at a program point u in procedure r be a set
of tuples ψ,α where α is the entity whose data ﬂow value at u is ⊥and ψ is the
context information which is either a call site ci ∈CallsTor or “∗”. When ψ is ci, the
data ﬂow value x α
u = ⊥is inherited by r from the call at ci. When ψ is ∗, the data
ﬂow valuex α
u = ⊥is synthesized in r or in some procedure called from within r. The
main diﬀerence between the two is that a data ﬂow value qualiﬁed by ci can only
be propagated to the caller containing the call site ci whereas the data ﬂow value
qualiﬁed by ∗should be propagated to all callers of r.
The exact criteria of propagation of ⊥values in a supergraph for a forward data
ﬂow framework is as described below. For backward data ﬂows, the roles of Ci and
Ri should be interchanged.
• When a pair ψ,α reaches an intraprocedural node n,
– If f α
n
= φ , ψ,α should not be propagated any further.
– If f α
n
= φ⊥, it indicates generation of synthesized data ﬂow information.
Hence the pair ψ,α must be replaced by the pair ∗,α.
– If f α
n
= φid, the pair ψ,α must be propagated further.
© 2009 by Taylor & Francis Group, LLC

Value-Based Approach to Interprocedural Data Flow Analysis
297
• When a pair ψ,α reaches a call node Ci in procedure r, the ⊥value of α must
be propagated to the called procedure with ci as the calling context. Thus the
pair ci,α must be propagated further.
• When a pair ψ,α reaches Ri in procedure r,
– If ψ is ∗, the pair ∗,α must be propagated further in r.
– If ψ is ci, the value ⊥of α has been inherited by r through the call site Ci
so the ⊥value of α must be propagated further in the rest of r. However,
the context from where its ⊥value reached Ci must be recovered. This is
easily done by examining the pairs reaching Ci—if a pair ψ,α reached
Ci, then the required context is ψ. Observe that ψ could be another call
site or could be ∗.
– If ψ is some cj other than ci, it indicates traversal of an interprocedurally
invalid path and the data ﬂow value must be discarded. This is because
the context information cj represents the fact that C j was the last call
node traversed in the path so this qualiﬁed data ﬂow value cannot reach
any other return node; it must reach R j where the calling context will be
reconstructed.
We use INn and OUTn to compute the qualiﬁed data ﬂow values X; the conven-
tional variables Inn and Outn continue to contain the underlying data ﬂow values
x ∈L. The data ﬂow equations are:
INn =

∗,α | α is ⊥in BI" n is Startmain

p∈pred(n)
OUTp
otherwise
(9.1)
OUTn = ConstGENn ∪DepGENn(INn) −
(X−(ConstKILLn −DepKILLn(INn)))
(9.2)
where the constant and dependent components are deﬁned as follows:
ConstGENn =

∅
n is Ci or Ri
∗,α | f α
n
= φ⊥
"
otherwise
DepGENn(X) =

ci,α | ψ,α ∈X"
n is Ci
∗,α | ∗,α ∈X" ∪
ψ,α | ci,α ∈X,ψ,α ∈INCi
"
n is Ri
∅
otherwise
ConstKILLn = ψ,α | α ∈Genn or α ∈Killn
"
DepKILLn(X) = ∅
Observe the use of the component function f α
n for ConstGENn. For data ﬂow frame-
works that use ∪as , f α
n
is φ⊥if α ∈Genn whereas for data ﬂow frameworks that
© 2009 by Taylor & Francis Group, LLC

298
Data Flow Analysis: Theory and Practice
a = 5;b = 3
c = 7;read d
Startmain
{c,d}
Call p
C1
Call p
R1
n1
a = c+d n1
{b,c,d}
n2
d = a ∗b n2
{a,c}
{a,b,c}
Call q
C2
Call q
R2
print a +c
Endmain
{a,c}
b = 2
if (b < c)
Startp
{a,b,d}
{a,c,d}
{a,c,d}
n3 c = b+2 n3
{a,b,c,d}
c = b+2
C4
c = a+b
R4
print c
Endp
{a,b,c,d}
b = c
Startq
{a,c,d}
{a,c,d}
Call p
C3
Call p
R3
c = b
Endq
{a,b,c,d}
{a,b,d}
FIGURE 9.1
An example program for interprocedural live variables analysis.
use ∩as , it is φ⊥if α ∈Killn and α  Genn. Also observe the use of INCi in the
deﬁnition of DepGENn(X).
The ﬁnal set of entities whose data ﬂow values are ⊥are extracted from
Inn =
#
α | ψ,α ∈INn, ψ ∈CallsTor ∪∗"$
(9.3)
Outn =
#
α | ψ,α ∈OUTn, ψ ∈CallsTor ∪∗"$
(9.4)
Example 9.1
Consider the program in Figure 9.1 for interprocedural liveness analysis. All
variables are global variables. Variables that are live at a program point are
shown in graph boxes. Observe that variable d is live in procedure q at Startq
but not before its call in the main procedure. The use of d in block n1 makes
it live in procedure p. Since q is called from p and neither p nor q modify d,
© 2009 by Taylor & Francis Group, LLC

Value-Based Approach to Interprocedural Data Flow Analysis
299
a = 5;b = 3
c = 7;read d
Startmain
Call p
C1
∗,0010
c3,1001
c1,0001
∗,0011
Call p
R1
c1,0111
n1
a = c+d n1
∗,0111
n2
d = a ∗b n2
∗,1110
Call q
C2
Call q
∗,0010
c2,1000
c4,1001
∗,1010
R2
c2,1010
print a +c
Endmain
∗,1010
b = 2
if (b < c)
Startp
∗,0010
c3,1001
c1,0001
n3 c = b+2 n3
∗,0100
c3,1001
c1,0001
c = a+b
C4
∗,0010
c2,1000
c4,1001
∗,0010
c3,1001
c1,0001
c = a+b
R4
∗,0010
c3,1101
c1,0101
c4,1111
print c
Endp
c3,1101
c1,0111
∗,0010
c3,1101
c1,0101
b = c
Startq
∗,0010
c2,1000
c4,1001
Call p
C3
∗,0010
c3,1001
∗,0010
c2,1000
c1,0001
c4,1001
Call p
R3
c3,1101
c = b
Endq
c2,1010
∗,0100
c2,1000
c4,1111
c4,1001
FIGURE 9.2
Result of interprocedural liveness analysis for the program in Figure 9.1 on the facing
page.
© 2009 by Taylor & Francis Group, LLC

300
Data Flow Analysis: Theory and Practice
Ci
Ri
C j
Ck
R j
Rk
c j,xj
ck,xk
ci,xj
ci,xk
ci,y
ci,z
f
• At return node Ri, we wish to re-
construct the values cj, f (xj) and
ck, f (xk).
• If we merge the values of a at Ci and
propagate ci,x j xk, we will not get
the values f (xj) and f (xk).
• If we do not merge the values but
propagate them separately, how can we
know if y should be propagated to R j
or Rk? (Similarly forz)?
FIGURE 9.3
Diﬃculty in handling propagation of multiple values for the same entity.
it remains live at all program points in both the procedures. Similarly, a is
live in procedure p because of the use of a in Endmain but it is not live before
the call to p in procedure main.
The result of interprocedural liveness analysis using this method has been
shown in Figure 9.2 on the preceding page. Since this is a backward data
ﬂow problem, reconstruction of contexts happens at the call nodes rather
than return nodes. Observe that at C1, c3,1001 contained in OUTC1 is ig-
nored, ∗,0010 is allowed to pass through, and the context of c1,0001 is
reconstructed to ∗by examining OUTR1. At R3, the data ﬂow values ∗,0100,
c2,1000, and c4,1001 are also propagated with the new context c3. Similar
actions are taken at other call and return nodes. Blocks n3, Endp and Endq
exhibit generation of synthesized data ﬂow information. This causes a trans-
fer of context from a call site ci to ∗for the entities that are contained in Gen
set of these blocks; the component functions for these entities compute ⊥for
these entities.
To see why this method chooses to propagate a single value, consider Figure 9.3.
Assume that instead of propagating a single value, we wish to propagate two dif-
ferent values xi and xk. In general, these values could be incomparable. For context
sensitive analysis, we wish to get the values f (xi) at R j and f (xk) at Rk. If we merge
xi andxk at Ci, we cannot get independent values f (xi) and f (xk). If we keepxi and
xk separate at Ci and propagate them separately, then we can get two distinct values
y and z but we will not be able to map them to xj and xk. Thus we will not know
which one of y and z should be propagated to R j and which one to Rk. Thus only
one value can be propagated and it should be ⊥rather than  .
The other interesting question that needs to be answered is: Can this method be
used for non-separable frameworks in which the component lattice is { ,⊥}? To
© 2009 by Taylor & Francis Group, LLC

Value-Based Approach to Interprocedural Data Flow Analysis
301
Startr
a = b
Endr
Ci
Ri
C j
Ck
R j
Rk
c j,b
c j,b
ci,b
• From the calling context cj, variable a is
initialized but b is not.
• From the calling context ck, both a and b
are initialized.
• At Ri, data ﬂow values of only the fol-
lowing forms are valid: ∗,α and ci,α.
• Our method can construct c j,b at Ri but
not c j,a.
FIGURE 9.4
Diﬃculty in reconstructing contexts for possibly uninitialized variables analysis.
see why propagating ⊥values using this method is not suﬃcient in the presence of
non-separability, consider the problem of performing possibly uninitialized variables
analysis as illustrated in Figure 9.4. A pair ψ, x indicates that variable x is possibly
uninitialized and this fact has been discovered along the context ψ. The assignment
a = b dictates that a must be considered possibly uninitialized in all contexts in which
b has been discovered to be possibly uninitialized. Even if we generate the pair
ci,a from the pair ci,b after encountering the assignment statement a = b, there
is no context information about a at Ci. Further, this method cannot handle general
constraints that copy the context of b into the context of a whenever a context of b is
reconstructed.
For the qualiﬁed data ﬂow value X, the  is ∅. In order to establish that this method
computes MFP assignment in terms of X, we only need to argue about the termina-
tion. It follows from the fact that at each call node, the incoming context information
is overwritten by the call site. During analysis, the number of tuples representing the
synthesized data ﬂow information at any node in procedure r can at most be |Gvar|
and the number of tuples representing the inherited data ﬂow information is bounded
by |CallsTor|×|Gvar|.
Since Equations (9.1) and (9.2) cover all paths, they cover all interprocedurally
valid ifps also. This ensures safety of data ﬂow analysis. The precision follows from
the fact that data ﬂow analysis is restricted to interprocedurally valid paths only.
9.3
Interprocedural Analysis Using Unrestricted Contexts
The main limitation of interprocedural data ﬂow analysis using a restricted context
is that it requires reconstruction of context. This restricts the method to bit vector
frameworks only. In this section we generalize the method to use unrestricted con-
© 2009 by Taylor & Francis Group, LLC

302
Data Flow Analysis: Theory and Practice
text. This not only eliminates the need of reconstruction of contexts but also of the
special context ∗to represent synthesized data ﬂow information. Further it allows
propagation of any data ﬂow value.
In the presence of recursion, unrestricted context could result in an inﬁnite num-
ber of unbounded length call strings. Thus the main issue in unrestricted context
approach is how to bound the length and the number of contexts. We ﬁrst present the
method without any concern for bounding the contexts. Then we present a general
method of bounding contexts based on data ﬂow values for data ﬂow frameworks
with ﬁnite lattices.
9.3.1
Using Call Strings to Represent Unrestricted Contexts
The call strings method uses X = σ,x as a qualiﬁed data ﬂow value where σ is a
call string representing a calling context. Special symbol λ denotes the empty call
string. Concatenation λ·ci results in the call string ci.
The computation and propagation of qualiﬁed data ﬂow value X is simpler in this
method than in the previous method:
• If a pair σ,x reaches Ci, the context σ is extended and the pair σ·ci,x is
propagated further.
• If a pair σ,x reaches Ri, there are two possibilities:
– If the last call site in σ is ci, i.e. σ = σ ·ci, it indicates a matching Ci and
Ri and thus represents an interprocedurally valid path. In such a situation,
the pair σ,x is propagated further. Note that σ could be λ.
– If the last call site in σ is not ci, or σ is λ, it indicates an interprocedurally
invalid path and the pair σ,x should not be propagated further.
• If a pair σ,x reaches an intraprocedural node n, the context does not change,
only the data ﬂow value changes. Let the ﬂow function for block n be fn. Then
the pair σ, fn
x should be propagated further.
There is no need of the special context ∗because a call string remembers the call sites
corresponding to all unﬁnished calls. This makes it possible to propagate synthesized
data ﬂow information to appropriate callers without the need of reconstructing con-
texts. Note that the above description does not guarantee termination of call strings
in recursive programs; we address this issue independently.
Since this method propagates all values in L rather than only ⊥, multiple qualiﬁed
data ﬂow values reaching a node cannot be combined by plain set union. Instead,
the data ﬂow values associated with the same context must be merged. Thus the
conﬂuence of qualiﬁed data ﬂow values is deﬁned as follows:
X 1Y = σ,xy | σ,x ∈X, σ,y ∈Y " ∪
σ,x | σ,x ∈X, ∀z ∈L,σ,z  Y " ∪
σ,y | σ,y ∈Y, ∀z ∈L,σ,z  X "
© 2009 by Taylor & Francis Group, LLC

Value-Based Approach to Interprocedural Data Flow Analysis
303
The resulting data ﬂow equations computing the qualiﬁed data ﬂow values INn and
OUTn are as deﬁned below:
INn =

λ,BI
n is a Startmain

p∈pred(n)
OUTp otherwise
OUTn = ConstGENn ∪DepGENn(INn) −
(X−(ConstKILLn −DepKILLn(INn)))
where ConstGENn = ConstKILLn = DepKILLn(X) = ∅and
DepGENn(X) =

σ·ci,x | σ,x ∈X" n is Ci
σ,x | σ·ci,x ∈X" n is Ri
σ, fn(x) | σ,x ∈X" otherwise
The above data ﬂow equations should be taken as a speciﬁcation of the computation
to be performed. In practice, we use a work list based iterative algorithm for com-
puted INn and OUTn rather than a round-robin iterative algorithm. This is because
the eﬀect of a change does not aﬀect the entire supergraph directly.
The ﬁnal data ﬂow values at a node n are:
Inn =
σ,x∈INn
x
(9.5)
Outn =
σ,x∈OUTn
x
(9.6)
Example 9.2
Consider the program in Figure 9.1 on page 298 for call strings based in-
terprocedural liveness analysis. A partial result of this analysis is shown in
Figure 9.5 on the next page. It is complete in the sense that it includes all live
variables at all program points. However, it is partial in the sense that it does
not enumerate all call strings. For example, c2c3c4,1010 and c1c4,0011
contained in INStartq could be propagated to OUTC2, only to be ignored at
C2 because these call strings do not end with c2. However, some pairs that
will not be ignored by the algorithm are c2c3c4c3,1110 and c1c4c3,0111 that
should be propagated from INEndp to OUTR4 where new pairs c2c3c4c3c4,1110
and c1c4c3c4,0111 would be created. This will further result in call strings
c2c3c4c3c4c3 and c1c4c3c4c3 and the construction of call strings will not ter-
minate in spite of the fact that no new data ﬂow information is generated.
Observe that in OUTStartp, the data ﬂow information has been shown as
1100+1010 and 0101 +0011 to highlight the fact that it is a merge of the data
ﬂow information propagated from the two successors of Startp.
© 2009 by Taylor & Francis Group, LLC

304
Data Flow Analysis: Theory and Practice
Block
OUTn
INn
Endm λ,0000
λ,1010
R2
λ,1010
c2,1010
C2
c2,1010
λ,1010
n2
λ,1010
λ,1110
n1
λ,1110
λ,0111
R1
λ,0111
c1,0111
C1
c2c3,1010,c1,0011
λ,0011
Startm λ,0011
λ,1111
Endq c2,1010,c2c3c4,1110,c1c4,0111 c2,1100,c2c3c4,1100,c1c4,0101
R3
c2,1100,c2c3c4,1100,
c1c4,0101
c2c3,1100,c2c3c4c3,1100,
c1c4c3,0101
C3
c2c3,1010,c1,0011,
c2c3c4c3,1010,c1c4c3,0011
c2,1010,
c2c3c4,1010,c1c4,0011
Startq c2,1010,c2c3c4,1010,c1c4,0011 c2,1010,c2c3c4,1010,c1c4,0011
Endp c2c3,1100,c1,0111,
c2c3c4c3,1100,c1c4c3,0101
c2c3,1110,c1,0111,
c2c3c4c3,1110,c1c4c3,0111
n3
c2c3,1110,c1,0111,
c2c3c4c3,1110,c1c4c3,0111
c2c3,1100,c1,0101,
c2c3c4c3,1100,c1c4c3,0101
R4
c2c3,1110,c1,0111
c2c3c4,1110,c1c4,0111
C4
c2,1010,c2c3c4c3,1110,
c1c4c3,0111,c2c3c4,1010,
c1c4,0011
c2c3,1010,c1,0011
Startp c2c3,1100 +1010,c1,0101+0011,
c2c3c4c3,1100,c1c4c3,0101
c2c3,1010,c1,0011,
c2c3c4c3,1010,c1c4c3,0011
FIGURE 9.5
Some call strings and associated values for interprocedural live variables analysis of
our example program Figure 9.1 on page 298.
Example 9.3
Figure 9.6 on the facing page provides a recursive procedure r that reverses a
linked list pointed to by the head pointer. Every call to r reverses the pointer
of the head node by assigning the previous pointer and then moves the three
pointers forward. As the recursion unwinds, the same operations are repeated
nullifying the eﬀect of the operations carried out before a recursive call was
made. Thus, at the end of every call to r, regardless of the depth of recursion,
the list is identical to what it was before the call.
Figure 9.7 on page 306 shows the points-to graphs for some contexts dis-
covered by the call strings method. Call site c1 represents a call from main,
whereas c2 represents the recursive call. The points-to graph associated with
© 2009 by Taylor & Francis Group, LLC

Value-Based Approach to Interprocedural Data Flow Analysis
305
0. void r()
1. {
/* Reverse the list */
2.
n = *h;
3.
*h = p;
4.
p = h;
5.
if (n != NULL)
6.
{
h = n;
7.
r();
8.
}
9.
else
10.
{
/* Reversed */
11.
{
p = NULL;
12.
n = NULL;
13.
/* Process it */
14.
}
15.
/* Reverse it again */
16.
n = *h;
17.
*h = p;
18.
p = h;
19.
h = n;
20. }
aa∗b
Startr
n = ∗h
∗h = p
p = h
n1
h = n n2
p = null
n = null
n3
ca∗b C2
ca∗b R2
n = ∗h
∗h = p
p = h
h = n
n4
aa∗b
Endr
FIGURE 9.6
Example program for interprocedural points-to analysis.
Pointer h is the head
pointer, p is the previous pointer, and n is the next pointer.
Endr for the call string c1 represents the data ﬂow information returned to
the main procedure conﬁrming that two reversals of the list have restored the
list to its original structure. This is possible because the call strings method
remembers the history of calls. This ensures that the method traverses inter-
procedurally valid paths only: In every path reaching the main procedure, for
every occurrence of Startr there is a matching Endr and vice-versa. Thus the
number of times the ﬂow function representing a single step of list reversal is
applied remains equal for the control ﬂow path entering the recursion and the
control ﬂow path leaving the recursion.
9.3.2
Issues in Termination of Call String Construction
In non-recursive programs, only a ﬁnite number of call strings can be constructed
and the termination of the method is governed solely by the convergence of data
ﬂow values associated with the call strings. In recursive programs, termination of
call string construction needs to be ensured explicitly. Once the termination of call
strings is ensured, the usual ﬁxed point criterion of data ﬂow values can be applied
© 2009 by Taylor & Francis Group, LLC

306
Data Flow Analysis: Theory and Practice
Node
Points-to graphs at node exists for the input list
a
b
c
d
c1
c1c2
c1c2c2
c1c2c2c2
n1
a
b
c
d
h
p
n
a
b
c
d
h
p
n
a
b
c
d
h
p
n
a
b
c
d
h
p
n
n2
a
b
c
d
h
p
n
a
b
c
d
h
p
n
a
b
c
d
h
p
n
a
b
c
d
h
p
n
C2
a
b
c
d
h
p
n
a
b
c
d
h
p
n
a
b
c
d
h
p
n
n4
a
b
c
d
h
p
n
a
b
c
d
h
p
n
a
b
c
d
h
p
n
a
b
c
d
h
p
n
R2
a
b
c
d
h
p
n
a
b
c
d
h
p
n
a
b
c
d
h
p
n
FIGURE 9.7
Points-to graphs in selected calling contexts. The head pointer points to variable a
in the linked list.
to ensure the termination of analysis exactly as in iterative intraprocedural analysis.
A natural question that needs to be answered is whether call string construction
can be terminated based on convergence of data ﬂow values. In particular, we need
to ascertain whether we need to continue constructing new call strings even when no
new data ﬂow information is generated for the new call strings. We have observed
that in the case of intraprocedural analysis it is possible to compute the data ﬂow val-
ues by a ﬁxed point computation of the data ﬂow variables associated with the nodes
in a loop. The main diﬀerence between recursive contexts and loops is separation of
data space. However, if we restrict ourselves to global variables only, is it possible
to perform a ﬁxed point computation of call strings? The next example shows that if
call strings construction is stopped when data ﬂow values reach a ﬁxed point, it may
result in an unsafe solution.
© 2009 by Taylor & Francis Group, LLC

Value-Based Approach to Interprocedural Data Flow Analysis
307
0. int a,b,c;
1.
2. void main()
3. {
c = a*b;
4.
p();
5. }
6.
7. void p()
8. {
if (...)
9.
{
p();
10.
a = a*b;
11.
}
12. }
a = a∗b
Startmain
c = a∗b
n1
c = a∗b
C1
c = a∗b
R1
c = a∗b
Endmain
a = a∗b
Startp
c = a∗b
C2
c = a∗b
R2
a = a∗b
n2
c = a∗b
Endp
Constructed
call strings Block
Iteration #1
Iteration #2
INn
OUTn
INn
OUTn
c1,1,
c1c2,1
R2
c1,1,
c1c2,1
c1,1
c1,0,
c1c2,1
c1,1
n2
c1,1
c1,0
c1,1
c1,0
Endp c1,0,
c1c2,1
c1,0,
c1c2,1
c1,0,
c1c2,1
c1,0,
c1c2,0
c1,1,
c1c2,1,
c1c2c2,1
R2
c1,1,
c1c2,1,
c1c2c2,1
c1,1,
c1c2,1
c1,0,
c1c2,0,
c1c2c2,1
c1,0,
c1c2,1
n2
c1,1,
c1c2,1
c1,0,
c1c2,0
c1,0,
c1c2,1
c1,0,
c1c2,0
Endp
c1,0,
c1c2,0,
c1c2c2,1
c1,0,
c1c2,0,
c1c2c2,1
c1,0,
c1c2,0,
c1c2c2,1
c1,0,
c1c2,0,
c1c2c2,1
FIGURE 9.8
Available expressions analysis using call strings approach. Unless call string c1c2c2
is constructed, it is not possible to ﬁnd out that a∗b is not available at Entry(n2).
Example 9.4
Consider the program in Figure 9.8. Since variable a is modiﬁed in n2 and
is a global variable, the expression a ∗b is not available at the entry of n2 in
any call of procedure p except for the most deeply nested call from which the
recursion starts unwinding. When the call strings based method constructs
call string c1c2, the expression is available. When the pair c1c2,1 reaches
R2, call site c2 is removed and the data ﬂow value is passed on through the
pair c1,1. At the exit of n2, the qualiﬁed data ﬂow value becomes c1,0 and
is propagated to both R1 and R2. However since the last call site c1 does not
© 2009 by Taylor & Francis Group, LLC

308
Data Flow Analysis: Theory and Practice
Sp
Ci
Sq
C j
Sr
Ck
Cyclic call
sequence
Ep
Rk
Er
R j
Eq
Ri
Cyclic return
sequence
u
v
Sp
Ci
Sq
C j
Sr
Ck
fc
fc
Ep
Rk
Er
R j
Eq
Ri
fr
fr
fh
u
v
(a) Control ﬂow
(b) Flow functions
FIGURE 9.9
Modelling recursion. Startr and Endr for procedure r are abbreviated by Sr and Er.
correspond to R2, this qualiﬁed value is ignored at R2. Thus the only way
we can get the data ﬂow value 0 at Entry(n2) is by ensuring that the cycle
(R2,n2,Endp,R2) is traversed at least once more. This is not possible unless
call string c1c2c2 is constructed in the cycle (C2,Startp,C2).
In terms of information ﬂow paths, our analysis must cover the following
ifp: On2 →IEndp →OEndp →IR2 →OR2 →In2 where Ini and Oni denote Entry(ni)
and Exit(ni) respectively. Observe that node n2 can be reached only via R2 and
the two occurrence of R2 require at least two occurrences of C2. The shortest
interprocedurally valid path that covers this ifp is:
(Startm,n1,C1,Startp,C2,Startp,C2,Startp,Endp,R2,n2,Endp,R2,n2)
The call string corresponding to this ifp is c1c2c2.
This situation arises because a recursive call sequence in a program consists of
two loops rather than one as illustrated in Figure 9.9. The ﬁrst loop represents the
control ﬂow entering the recursive call while the other loop represents the control
ﬂow leaving the recursive calls. We call them as cyclic call sequence and cyclic
return sequence respectively. We denote the ﬂow functions associated with them by
fc and fr respectively. The dashed line from Startq to Endq represents the recursion
ending control ﬂow path and the ﬂow function associated with it is denoted by fh.
Since we do not require the call sites along a cyclic call sequence to be distinct, this
ﬁgure models a general recursive path. In the most general case, there could be a path
from the cyclic return sequence to the cyclic call sequence if there exists a recursive
© 2009 by Taylor & Francis Group, LLC

Value-Based Approach to Interprocedural Data Flow Analysis
309
TC0
TC1
...
TCκc−1 TCκc
...
TCm−κr TCm−κr+1
...
TCm−1 TCm
fc
fc
fc
fc
fc
fc
fc
fc
fc
fc
Same values
TR0
TR1
...
TRκc−1 TRκc
...
TRm−κr TRm−κr+1
...
TRm−1 TRm
fr
fr
fr
fr
fr
fr
fr
fr
fr
fr
Same values
fh
fh
fh
fh
fh
fh
fh
fh
FIGURE 9.10
Computation of data ﬂow values along recursive paths. Dashed arrows indicate func-
tion applications.
call within a loop. Since we do not require fc, fr, and fh to be independent, this does
not aﬀect our Modelling.
In a valid interprocedural path from u to any program point in the recursive proce-
dures, the cyclic call sequence must be traversed at least as many times as the cyclic
return sequence. For a valid interprocedural path from u to v, the cyclic call sequence
must be traversed exactly as least as many times as the cyclic return sequence.
For forward data ﬂow problems, call strings are constructed when the cyclic call
sequence is traversed. Let the sequence of call sites (...ci ...cj ...ck ...) along the
cyclic call sequence from Startq back to Startq be represented by σc. Each appli-
cation of fc suﬃxes σc to every call string reaching Startq. These call strings are
consumed when the corresponding cyclic return sequence is traversed. Each appli-
cation of fr requires traversing the cyclic return sequence once. In the process, the
last occurrence of σc is removed from every call string. Thus, fr can be applied only
as many times as the maximum number of σc in any call string reaching the entry of
Endp. Note that the application of fc does not have such a requirement because the
call strings are constructed rather than consumed while applying fc.
In order to guarantee safety of interprocedural data ﬂow analysis, the call strings
should be long enough to allow computation of all possible data ﬂow values in both
cyclic call and return sequences. We quantify this length in terms of a ﬁxed point
closure bound. A ﬁxed point closure bound of a function h is the smallest number
n > 0 such that ∀x,hn+1(x) = hn(x).
Let the ﬁxed point closure bound of fc be κc and that of fr be κr. Let the number
of occurrences of σc in the longest call string be m > κc. Let the qualiﬁed data ﬂow
value reaching Startq in Figure 9.9 on the facing page be σ,x. Let the sequence
of the qualiﬁed data ﬂow values computed at Startq be denoted by σ·σi
c,TCi. We
© 2009 by Taylor & Francis Group, LLC

310
Data Flow Analysis: Theory and Practice
know that
TCi =

x
i = 0
fc(TCi−1)
1 ≤i ≤m
This recurrence trivially reduces to:
TCi =

fci(x)
0 ≤i < κc
fcκc(x)
κc ≤i ≤m
Let the sequence of the qualiﬁed data ﬂow values computed at Endq be denoted by
σ·σi
c,TRi. Then,
TRi =

fh(TCi)  fr(TRi+1)
0 ≤i < m
fh(TCi)
i = m
The ﬁrst term of  represents the data ﬂow value along the path from Startq to Endq
whereas the second term represents the data ﬂow value computed along the cyclic
return sequence. On substituting the values of TCi, we get
TRi =

TRi  fr(TRi+1)
0 ≤i < κc
TRm  fr(TRi+1)
κc ≤i < m
fh(fcκc(x))
i = m
(9.7)
Since TRi depends on TRi+1, the ﬁnal computation in cyclic return sequence starts
from the last call string as illustrated in Figure 9.10 on the previous page. Clearly, m
should be at least κc +κr. If m < κc +κr, then some data ﬂow values corresponding to
unbounded recursion may not be computed. However, the values of κc and κr are not
known a priori, and there should be some way of terminating the construction of call
strings.
Example 9.5
Consider the program of Figure 9.6 on page 305.
Flow function fc is the
composition of the ﬂow functions for n1 and n2, fh is the composition of the
ﬂow functions for n1, n3, and n4, whereas fr is the ﬂow function for block n4.
If we ignore the head pointer which is conditionally advanced in the cyclic call
sequence, fri(fhi(x)) = x. Further, for the given input value x (consisting of a
linked list of 4 elements), κc = κr = 4. Because of these reasons, it is suﬃcient to
use m = 4 in this special case and stop call string construction when c1c2c2c2c2
is created. This can be readily veriﬁed from Figure 9.7 on page 306. In the
general case, m should be larger then κc +κr for all values of κc and κr.
© 2009 by Taylor & Francis Group, LLC

Value-Based Approach to Interprocedural Data Flow Analysis
311
9.4
Bounding Unrestricted Contexts Using Data Flow Values
A simple approach of allowing unrestricted call strings and yet bounding the overall
set of call strings is to maintain in each procedure r, a single representative call
string for each possible value in the lattice. This technique is deceptively simple and
requires elaborate explanation. We outline the basis of this simple idea in terms of
the following fundamental invariants of the call strings method:
• We observe that the same set of call strings reaches all program points in a
procedure although they may have diﬀerent values associated with them. As
a consequence, if a mechanism is devised to ignore some call strings in a
procedure (e.g., to represent them by other call strings), it would be possible
to reconstruct them wherever they are required.
• If the call strings reaching a procedure are partitioned on the basis of data ﬂow
values, the equivalence classes remain unchanged in the procedure (the data
ﬂow value associated with an equivalence class may be diﬀerent at diﬀerent
program point). More call strings may be included in an equivalence class
across procedure calls because of construction of additional call strings.
• Finally, if there is a way of computing the correct value of σ·σκcc at Endp, call
strings σ·σi, κc < i ≤m need not be constructed. Further, there is not need
to regenerate them explicitly; their implicit regeneration can be simulated by
iterative computation of data ﬂow values.
9.4.1
Call String Invariants
This section proves the call string invariants; the actual details of the method are
presented in Section 9.4.2.
DEFINITION 9.5
A context deﬁning path from program point u to pro-
gram point v is a valid interprocedural path from u to v that consists of only
intraprocedural segments, call segments, or symmetric segments.
If a context deﬁning path contains return segments, they are suﬃxes of symmetric
segments. For the purpose of our discussion, we restrict a context deﬁning path to
the signiﬁcant nodes appearing in it. Thus each adjacent pair of nodes in a context
deﬁning path may correspond to many distinct intraprocedural segments.
DEFINITION 9.6
A program point v is context dependent on program
point u, denoted v ∈Cd(u), if there is a context deﬁning path from u to v.
© 2009 by Taylor & Francis Group, LLC

312
Data Flow Analysis: Theory and Practice
Given procedure r, Cd(Startr) contains all program points within r and all pro-
gram points within all callees in every call chain starting in r. For v ∈Cd(u), we
use Cdp(u,v) to denote the set of context deﬁning paths from u to v and Cs(u,v) to
denote the set of call strings corresponding to paths in Cdp(u,v).
Let dfVal(σ,u) denote the value associated with call string σ at program point u.
DEFINITION 9.7
Call strings σ1 and σ2 are equivalent at program point
u, denoted σ1
u= σ2, if {σ1,σ2} ⊆Cs(Startmain,u) and dfVal(σ1,u) = dfVal(σ2,u).
We assume that the work list based interprocedural analysis traverses interpro-
cedural paths such that all intraprocedural segments are processed completely be-
fore propagating data ﬂow information from a signiﬁcant node to another signiﬁcant
node. This can be achieved by maintaining two separate work lists: One for intrapro-
cedural nodes and the other for signiﬁcant nodes. A signiﬁcant node is selected for
processing only after ensuring that the work list of intraprocedural nodes is empty.
We call such an interprocedural analysis algorithm as being intraprocedurally eager.
LEMMA 9.1
The calling contexts of all intraprocedural program points in a procedure are
identical.
PROOF
Obvious.
Calling contexts of a procedure depend on the callers so they cannot be diﬀerent
for diﬀerent program points within the procedure. For a given call site ci ∈CallsInr,
Exit(Ci) and Entry(Ri) are assumed to logically belong to the callee procedure rather
than r.
The following lemma shows that if σ1 and σ2 are transformed in the same manner
by following the same set of paths, the values associated with them will also be
transformed in the same manner and will continue to remain equal.
LEMMA 9.2
Consider a program point v ∈Cd(u).
Assume that the recursive paths in
Cdp(u,v) are unbounded. When the work list of intraprocedural nodes is empty
in an intraprocedurally eager call strings based method,
σ1
u= σ2 ⇒∀σ ∈Cs(u,v), (σ1 ·σ) v= (σ2 ·σ)
PROOF
There are two cases to consider:
1. There is only one context deﬁning path in Cdp(u,v) leading to a single
sequence σ of call nodes that can be suﬃxed to both σ1 and σ2.
© 2009 by Taylor & Francis Group, LLC

Value-Based Approach to Interprocedural Data Flow Analysis
313
u
σ1,d
σ2,d
Cx
Cy
σ1 ·σ,d
x
σ2 ·σ,d
x
σ1 ·σ,d
y
σ2 ·σ,d
y
Rx
Ry
σ1 ·σ·cx,dx
σ2 ·σ·cx,dx
σ1 ·σ·cy,dy
σ2 ·σ·cy,dy
v
σ1 ·σ,dx dy
σ2 ·σ,dx dy
FIGURE 9.11
Case 2 for Lemma 9.2.
In this case, the eager interprocedural analysis algorithm traverses ex-
actly the same set of paths from u to v for computing the data ﬂow
information associated with the call strings σ1 ·σ and σ2 ·σ. Thus the
data ﬂow values along the call strings σ1 ·σ and σ2 ·σ undergo the same
change. Clearly,
dfVal(σ1,u) = dfVal(σ2,u) ⇒dfVal(σ1 ·σ,v) = dfVal(σ2 ·σ,v)
2. Cdp(u,v) contains multiple context deﬁning paths corresponding to σ.
We prove this case by induction on the length of the maximal common
suﬃx of all paths in Cdp(u,v) which correspond to σ.
• Basis. The basis is the case when there is no common suﬃx.
For simplicity, assume that we have only two paths corresponding
to σ as illustrated in Figure 9.11. Without any loss of generality,
assume that Rx and Ry are the last nodes which are diﬀerent.∗Since
both the paths from u to v correspond to a common call string
σ, Cs(u,Entry(Rx)) contains a call string σ·cx and Cs(u,Entry(Ry))
contains a call string σ·cy.
Let dfVal(σ1,u) = dfVal(σ2,u) = d. Assume that the path segment
from u to Entry(Rx) changes this value to dx and the path segment
from u to Entry(Ry) changes this value to dy.
∗If two context deﬁning paths diﬀer in call nodes which are not followed by matching return nodes, then
the two paths would not correspond to the same call string.
© 2009 by Taylor & Francis Group, LLC

314
Data Flow Analysis: Theory and Practice
Since σ1 and σ2 reach u, σ1 ·σ·cx and σ2 ·σ·cx reach Entry(Rx).
Thus,
dfVal(σ1 ·σ·cx,Entry(Rx)) = dfVal(σ2 ·σ·cx,Entry(Rx)) = dx
Similarly,
dfVal(σ1 ·σ·cy,Entry(Ry)) = dfVal(σ2 ·σ·cy,Entry(Ry)) = dy
At the exit of the return nodes, the two call sites are removed.
Hence at v, we get the pairs σ1 ·σ,dx and σ2 ·σ,dx along one
path whereas along the other path we get the pairs σ1 ·σ,dy and
σ2 ·σ,dy. The data ﬂow values for same call strings from diﬀerent
paths are merged and hence
dfVal(σ1 ·σ,v) = dfVal(σ2 ·σ,v) = dx dy
This proves the basis case for two paths. Extending it to more than
two paths is easy due to the ﬁniteness of L. If there is a recursive
call in a path from u to v, there will be inﬁnitely many context
deﬁning paths corresponding to σ, each with a diﬀerent number
of matchings of some call and return nodes. However, since L is
ﬁnite, these paths can be partitioned based on the data ﬂow values
corresponding to the call strings σ1 ·σ and σ2 ·σ. Thus we will have
a ﬁnite merge and inducting on the number of values (or number
of partitions of paths from u to v) serves the purpose.
• Inductive step. Assume that all paths in Cdp(u,v) which correspond
to σ have a non-empty common suﬃx. Assume further that the
lemma holds for a maximal common suﬃx consisting of k nodes.
To show that it holds for a common suﬃx of k + 1 nodes, observe
that since all call strings traverse essentially the same path segment
from node k to node k+1, the data ﬂow values associated with the
call string will be modiﬁed in the same way. Since the data ﬂow
values are equal after k nodes, they remain equal after k+1 nodes.
Note that this lemma assumes unbounded recursion. If call string construction is
terminated after some repetition of cyclic call sequence (say m), then as illustrated
in Figure 9.10 on page 309, the values of TRi for (m−κr +1) ≤i ≤m are likely to
be diﬀerent in spite of the fact that the values of TCi for the same range of i are
identical (fcκc(x)). The lemma holds for the values of TRi for κc ≤i < (m−κr +1).
However, this exception arising due to bounded call strings does not matter because
the associated values follow a strictly descending chain and converge on the least
value. Hence the result of the merge of TRi,κc ≤i ≤m is the same as the values in
those ranges of i for which the above lemma holds.
© 2009 by Taylor & Francis Group, LLC

Value-Based Approach to Interprocedural Data Flow Analysis
315
Intuitively, the values of TRi for (m−κr +1) ≤i ≤m follow a strictly descending
chain because they are repeatedly computed using the same function and are merged
with the same value (fcκc(x) in our case) at each step. We prove this in the following
lemma.
LEMMA 9.3
Assume that the call strings method constructs call strings long enough so that
all call strings σ·σi
c, 0 ≤i ≤m are constructed where m ≥κc +κr for all possible
values of κc and κr. Then,
∀κr,
TRm−κr  TRi, m−κr ≤i ≤m
PROOF
We prove this by inducting on the distance of i from m by rewrit-
ing TRi, m−κr ≤i ≤m as TRm−j,0 ≤j ≤κr and by showing that
TRm−(j+1)  TRm−j, 0 ≤j < κr
The basis of induction is j = 0. Since TRm−1 = TRm  fr(TRm) it trivially follows
that TRm−1  TRm. For the inductive hypothesis, assume that TRm−(j+1)  TRm−j.
We need to show that TRm−(j+2)  TRm−(j+1). From the deﬁnition of TRi,
TRm−(j+2) = TRm  fr(TRm−(j+1))
0 ≤j ≤κr, m > κc +κr
(9.8)
TRm−(j+1) = TRm  fr(TRm−j)
0 ≤j ≤κr, m > κc +κr
(9.9)
From the inductive hypothesis and monotonicity of ﬂow functions,
TRm−(j+1)  TRm−j ⇒fr(TRm−(j+1))  fr(TRm−j)
The inductive step follows by comparing the right hand sides of Equations (9.8)
and (9.9).
Observe the role of κc in the above proof. Since TRκc−1 does not have the ﬁrst term
as fh(fcκc(x)) unlike TRκc, a partial order relation between TRκc−1 and TRκc cannot
be established and lemma may not hold.
We have deﬁned TCi and TRi for Startq and Endq in Figure 9.9 on page 308.
In particular, the term TRi for Endq involves a merge of the data ﬂow values along
the recursion ending path and the cyclic return sequence. For some other pair of
program points, say Startr and Endr, the term TRi may not be a merge of data ﬂow
values along two paths. However, the data ﬂow values at all program point in the
cyclic return sequence must converge. When the computation of a data ﬂow value
converges at a program point in a cycle, it must converge at each program point in the
cycle. Further, the direction of convergence must be same for each program point.
This convergence immediately suggests that the data ﬂow values associated with
call strings σ·σi
c, κc ≤i ≤m are not required for the ﬁnal data ﬂow value in cyclic
return sequences. This happens because when the data ﬂow values that are being
© 2009 by Taylor & Francis Group, LLC

316
Data Flow Analysis: Theory and Practice
a = ∗a
x
x = {a {b},b {c},c {b}}
f 1(x) = {a {c},b {c},c {b}}
f 2(x) = {a {b},b {c},c {b}}
f 3(x) = {a {c},b {c},c {b}}
a ∅( )
a {a}
a {b}
a {c}
a {a,b} a {a,c} a {b,c}
a {a,b,c} (⊥)
(a) x is a periodic point with a period 2 for the ﬂow
function for pointer assignment a = ∗a.
(b) Lattice of may points-to
information for variable a
FIGURE 9.12
Flow functions in Points-to analysis. Data ﬂow value v S indicates that variable v
may point to the variables contained in S .
merged follow a descending chain, only the last value in the chain matters in the
overall merge and since our lattices are ﬁnite, all descending chains are ﬁnite and
such a last value is guaranteed to exist.
THEOREM 9.1
Assume that the call strings method constructs call strings long enough so that
all call strings σ·σi
c, 0 ≤i ≤m constructed where m ≥κc +κr for all possible
values of κc and κr. Then for each program point v in a return sequence
m
i=0dfVal(σ·σi
c,v) =
max(κc)
i=0 dfVal(σ·σi
c,v)
PROOF
The values dfVal(σ·σi
c,v), 0 ≤i < κc may be diﬀerent. However,
due to the convergence of data ﬂow values for subsequent call strings,
dfVal(σ·σi
c,v) = dfVal(σ·σi+1
c ,v), κc ≤i < m−κr
Thus dfVal(σ·σκcc ,v) is the least value for κc ≤i ≤m. Hence it is suﬃcient to
merge the values of all call strings up to κc number of occurrence of σc.
An aside on ﬂow function with periodic points
For a given function h and a value x, if hn(x) = x and hi(x)  x, 0 < i < n, then x is a
periodic point of h with period n. A ﬁxed point is a periodic point of period one. In
general, ﬂow functions can have periodic points of larger periods even if the func-
tions are monotonic. This is possible only when functions compute incomparable
values. Figure 9.12 shows an example of such a ﬂow function from may Points-to
analysis. x is the data ﬂow information reaching statement n from outside of the
loop. Observe that fn computes incomparable values in all successive applications.
We have restricted the discussion in this chapter to ﬂow functions with period
one. Extending the arguments to functions of larger periods is easy. Consider a ﬂow
© 2009 by Taylor & Francis Group, LLC

Value-Based Approach to Interprocedural Data Flow Analysis
317
Startp
σ1,x
σ2,x
σ1,x
σ2,x
Endp
σ1,x
σ2,x
σ1,x
σ2,x
Startp
σ1,x
σ2,x
σ1,x
Endp
σ1,x
σ1,x
σ2,x
Represent
σ2 by σ1
Regenerate
σ2 from σ1
(a) Using unbounded contexts
(c) Bounding contexts using data ﬂow values
FIGURE 9.13
Representation and regeneration of equivalent call strings.
function which has period n for the incoming data ﬂow value. Then, there are n
periodic points instead of 1. In such a situation, instead of
dfVal(σ·σi+1,Startp) = dfVal(σ·σi,Startp), κc ≤i ≤m
we have
dfVal(σ·σi+n,Startp) = dfVal(σ·σi,Startp), κc ≤i ≤m−n
The convergence holds for call strings corresponding to each periodic point indepen-
dently. For periodic point i, Lemma 9.3 can be proved by inducting on the distance
of the call string from the call string σ·σm−i.
9.4.2
Value-Based Termination of Call String Construction
Given a set of cyclic call strings, Theorem 9.1 allows us to distinguish between two
types of call strings:
• The call strings whose data ﬂow values are relevant for the ﬁnal result of data
ﬂow analysis. These call strings involve up to κ occurrences of any cyclic call
sequence where κ is the largest possible value of κc.
• The call strings which facilitate a suﬃcient number of traversal over return
segment to allow convergence of data ﬂow values. These are the call strings
© 2009 by Taylor & Francis Group, LLC

318
Data Flow Analysis: Theory and Practice
σ·σκcc , fh(y) = TRm
Endp
σ·σκcc , fr(z)
σ·σκcc ,z = TRm  fr(z)
σ·σκc+1
c
,z = TRm  fr(z)
Regenerate
σ·σκc+1
c
from σ·σκc
σ·σκcc , fcκc(x) = y
Startp
σ·σκc+1
c
, fcκc+1(x) = y
σ·σκcc ,y
Represent
σ·σκc+1
c
by σ·σκc
FIGURE 9.14
Representation and regeneration of cyclic call strings whose data ﬂow values reach
convergence in a cyclic call sequence. These call strings are used for convergence of
data ﬂow values in the corresponding cyclic return sequence.
that contain κ additional occurrences of cyclic return sequences where κ is
the largest possible value of κr.
If there is some way of allowing traversal of a cyclic return sequence as many
times as may be required, we may be able to terminate construction of redundant
call strings in the corresponding cyclic call sequence. This is achieved as follows:
A single representative call string for an equivalence class within the
scope of a maximal context dependent region is maintained and at the
end of the region, all call strings belonging to each equivalence class are
reconstructed. Some of them are constructed explicitly while some of
them are constructed implicitly.
For procedure p, the decision of representation is taken at Startp. This represen-
tation remains valid at all program points which are context dependent on Startp.
© 2009 by Taylor & Francis Group, LLC

Value-Based Approach to Interprocedural Data Flow Analysis
319
Endp is the last such point and the call strings must be regenerated so that appropri-
ate data ﬂow values can be propagated to diﬀerent callers of p. Similar to the scope
of variables in a program, this representation may be “shadowed” by other context
dependent regions created by procedure calls in the outer context dependent region.
Let representative(x,Startp) denote a uniquely selected call string which has
value x at Startp. The selection can be made based on some well deﬁned crite-
rion and the choice of this criterion is immaterial so long as it identiﬁes a unique
call string. One example of selecting a unique call string is to select the shortest call
string from among the set of call strings that have the same data ﬂow value. Another
criterion could be to select the ﬁrst call string that is listed in the associated data
structure. The representation of call strings at Startp is deﬁned as follows:
∀σ,x ∈INStartp : represent(σ,Startp) = representative(x,Startp)
The regeneration at Endp is performed as follows:
OUTStartp = represent(σ,Startp),x | σ,x ∈INStartp
"
regenerate(σ,Endp) = {σ,y | represent(σ,Startp) = σ,σ,y ∈INEndp}
OUTEndp =

σ,y∈INEndp
regenerate(σ,Endp)
Regeneration copies the same data ﬂow value to all call strings belonging to the
same equivalence class. For general call strings this process has been illustrated in
Figure 9.13 on page 317. For call strings in recursive programs, this process facil-
itates iterative computation of data ﬂow values in cyclic return sequences without
having to construct redundant call strings in the corresponding cyclic call sequence.
This has been illustrated in Figure 9.14 on the preceding page.
The call string invariants presented in Section 9.4.1 are based on the following
assumptions that should be honoured by work list based method used for call strings
based interprocedural data ﬂow analysis:
• The work list algorithm is assumed to be intraprocedurally eager. Hence data
ﬂow information should be propagated across procedure boundaries only when
no further intraprocedural propagation is possible.
This can be handled by maintaining separate work lists for intraprocedural
nodes and signiﬁcant nodes. A signiﬁcant node should be selected by the
method only when there is no pending intraprocedural node.
• It is assumed that the functions in cyclic return sequence are applied only after
the data ﬂow values in the corresponding cyclic call sequence have reached
a convergence. This matters only in those cases when there is a path from a
cyclic return sequence to a cyclic call sequence e.g., when a function call is
contained in a loop.
This can be handled by maintaining the following invariant in the work list of
signiﬁcant nodes: A call node always precedes any return node in the work
list, regardless of when it is included in the work list.
© 2009 by Taylor & Francis Group, LLC

320
Data Flow Analysis: Theory and Practice
Step
No.
Selected
Node
Qualiﬁed Data Flow Value
Remaining Work List
INn
OUTn
Intraproc.
Nodes
Signiﬁcant
Nodes
1
Startp
c1,1
c1,1
Endp
C2
2
Endp
c1,1
c1,1
C2, R2
3
C2
c1,1
c1c2,1
Startp
R2
4
Startp
c1,1 c1c2,1
c1,1
Endp
C2, R2
(c1c2 is represented by c1)
5
Endp
c1,1
c1,1 c1c2,1
C2, R2
(c1c2 is regenerated from c1)
6
C2
No change
No change
R2
7
R2
c1,1 c1c2,1
c1,1
n2
8
n2
c1,1
c1,0
Endp
9
Endp
c1,0
c1,0 c1c2,0
R2
(c1c2 is regenerated from c1)
10
R2
c1,0 c1c2,0
c1,0
n2
11
n2
c1,0
No change
FIGURE 9.15
Interprocedural data ﬂow analysis of example program in Figure 9.8 on page 307
using value-based termination of call string construction.
• When representation is performed, it is assumed that the corresponding regen-
eration is guaranteed to be performed.
This can be ensured by adding Endr to the work list whenever representation
is performed at Startr; this includes the situation when an equivalence class
remains same but the data ﬂow value associated with the call strings in that
equivalence class changes. It is possible that the data ﬂow values do not change
within procedure r after representation and hence Endr may never be added to
the work list. In such a case, the new qualiﬁed data ﬂow value may not be
generated at Endp.
Example 9.6
Call strings based interprocedural data ﬂow analysis using representation and
regeneration of call strings for the example program in Figure 9.8 on page 307
has been illustrated in Figure 9.15. Observe that in step 2, R2 is inserted in
the work list after C2 rather than before it. In step 4, c1c2,1 ∈INStartp is
not propagated to OUTStartp as it is represented by c1,1. At Endp c1c2,1 is
regenerated. This reaches R2 where c2 is removed and the resulting qualiﬁed
data ﬂow value c1,1 is propagated to n2. Due to the assignment to a in
n2, this data ﬂow value changes to c1,0 and is propagated to Endp where
it is merged with c1,1 arriving from Startp. This causes the value 0 to be
© 2009 by Taylor & Francis Group, LLC

Value-Based Approach to Interprocedural Data Flow Analysis
321
propagated as c1c2,0 and c1,0.
A subsequent traversal over the return
sequence ensures that the data ﬂow value become 0 at Entry(n2) also.
Representation and regeneration discards only those call strings which contain
redundant values and performs the desired computation iteratively.
Recall that for the points-to analysis of program of Figure 9.6 on page 305, addi-
tional call strings are not required for convergence in cyclic return sequence. This
does not inﬂuence our algorithm in any way; we leave it for the reader to verify that
this method computes identical result as in Figure 9.7 on page 306.
THEOREM 9.2
The ﬁnal data ﬂow values computed by representing and regenerating call
strings are identical to the values computed by a call strings method with an
unbounded length call strings.
PROOF
Regeneration explicitly constructs all acyclic call strings and
all cyclic call strings containing κc +1 occurrences of σc. At Endp, σ·σκc+1
c
is
regenerated and the data ﬂow value associated with σ·σκcc is propagated to it.
From Equation (9.7) and Figure 9.14 on page 318, this value is TRm. This value
is then propagated as σ·σκc+1
c
,z = TRm along the cyclic return sequence. This
traversal removes the last occurrence of σc from σ·σκc+1
c
, computes fr(z),
which is merged with the value of σ·σκcc along the recursion ending path.
Thus dfVal(σ·σκcc ,Endp) = TRm  fr(TRm) after one traversal. This is same as
the value associated with call string σ·σm−1
c
where m ≥κc +κr. At Endp, this
is again copied to the call string σ·σκc+1
c
overwriting the previous value and
the pair σ·σκc+1
c
,z = TRm  fr(TRm) is propagated along the cyclic return
sequence. The process repeats as long as new values are computed for σ·σκcc ;
eﬀectively, traversal i over the cyclic return sequence computes the value Tm−i
for σ·σκcc .
The process terminates after κr traversals. This computes the
desired value for σ·σκcc .
After the convergence of data ﬂow values in a cyclic call sequence has been
reached, this method replaces construction of the subsequent call strings by itera-
tively computing the data ﬂow values in the corresponding cyclic return sequence
using a pair of last two call strings.
Observe that representation is performed afresh every time any Start node is vis-
ited. On a subsequent visit to Startp of procedure p, representation could change
because of the following reasons:
• A new call string with the value of an existing call string reaches Startp.
• A new call string with a new value reaches Startp.
• A call string that had reached Startp with a value x now reaches Startp with a
diﬀerent value x.
© 2009 by Taylor & Francis Group, LLC

322
Data Flow Analysis: Theory and Practice
0. int a,b,c;
1.
2. void main()
3. {
c = a*b;
4.
p();
5. }
6.
7. void p()
8. {
while (...)
9.
{
p();
10.
a = a*b;
11.
}
12. }
a = a∗b
Startmain
c = a∗b
n1
c = a∗b
C1
c = a∗b
R1
c = a∗b
Endmain
a = a∗b
Startp
a = a∗b
n3
c = a∗b
C2
c = a∗b
R2
a = a∗b
n2
c = a∗b
Endp
FIGURE 9.16
Modiﬁed program of Figure 9.8 on page 307. Expression a∗b is not available any-
where in procedure p.
In either case, Endp will be added to the work list. Thus all call strings will get
regenerated with appropriate data ﬂow values at Endp.
Example 9.7
Figure 9.16 contains a modiﬁed version of the program in Figure 9.8 on
page 307. Since now the recursive call is in the loop, expression a ∗b is un-
available in nodes Startp and C2 also. A trace of the call strings method using
value-based termination has been provided below.
Step
No.
Selected
node
Qualiﬁed data ﬂow value
Remaining work list
INn
OUTn
Intra.
nodes
Sig.
nodes
1
Startp
c1,1
c1,1
n3
2
n3
c1,1
c1,1
Endp
C2
3
Endp
c1,1
c1,1
C2, R2
4
C2
c1,1
c1c2,1
Startp
R2
5
Startp
c1,1 c1c2,1
c1,1
n3, Endp
R2
(c1c2 is represented by c1)
6
n3
c1,1
c1,1
Endp
C2, R2
7
Endp
c1,1
c1,1 c1c2,1
C2, R2
(c1c2 is regenerated from c1)
8
C2
No change
No change
R2
9
R2
c1,1 c1c2,1
c1,1
n2
10
n2
c1,1
c1,0
n3
© 2009 by Taylor & Francis Group, LLC

Value-Based Approach to Interprocedural Data Flow Analysis
323
Step
No.
Selected
node
Qualiﬁed data ﬂow value
Remaining work list
INn
OUTn
Intra.
nodes
Sig.
nodes
11
n3
c1,0
c1,0
Endp
C2
12
Endp
c1,0
c1,0 c1c2,0
C2, R2
(c1c2 is regenerated from c1)
13
C2
c1,0
c1c2,0
Startp
R2
14
Startp
c1,1 c1c2,0
c1,1 c1c2,0
n3
R2
(Representation has changed)
15
n3
c1,0 c1c2,0
c1,0 c1c2,0
Endp
C2, R2
16
Endp
c1,0 c1c2,0
No change
C2, R2
17
C2
c1,0 c1c2,0
c1c2,0 c1c2c2,0
Startp
R2
18
Startp
c1,1 c1c2,0
c1c2c2,0
No change
Endp
R2
(c1c2c2 is represented by c1c2)
19
Endp
No change
c1,0 c1c2,0
c1c2c2,0
R2
(c1c2c2 is regenerated from c1c2)
20
R2
c1,0 c1c2,0
c1c2c2,0
c1,0 c1c2,0
n2
21
n2
c1,0 c1c2,0
c1,0 c1c2,0
n3
22
n3
No change
No change
Observe that ﬁrst the call string c1c2 is represented by c1 but since the call
is in a loop, after unwinding the recursion once, the data ﬂow value 0 reaches
C2 along the call string c1. This changes the representation at Startp and the
call string c1c2 must be explicitly propagated further. Eventually, call string
c1c2c2 has the same value as c1c2. This results in a diﬀerent representation
and the data ﬂow analysis terminates after a few steps.
THEOREM 9.3
Using the value-based termination of call strings, the maximum number of call
strings at any internal program point is |L|.
PROOF
At Exit(Startp) for any procedure p, the call strings are parti-
tioned by the data ﬂow values associated with them and there can be at most
|L| distinct data ﬂow values.
THEOREM 9.4
Let the maximum number of call sites in any acyclic call chain be K. Then,
using the value-based termination of call strings, the maximum length of any
© 2009 by Taylor & Francis Group, LLC

324
Data Flow Analysis: Theory and Practice
call string is K ×(|L|+1).
PROOF
Consider a call string σ = ...(Ci)1 ...(Ci)2 ...(Ci)3 ...(Ci) j ... where
(Ci) j denotes j th occurrence of Ci. Let j ≥|L| + 1 and let Ci call procedure
p. The set of call strings reaching p is preﬁx closed in the following sense:
All preﬁxes of σ ending in Ci must reach entry Startp. Since only |L| distinct
values are possible, by the pigeon hole principle, at least two preﬁxes ending
with Ci will carry the same data ﬂow value to Startp and the longer preﬁx will
get represented by the shorter preﬁx. Since one more Ci is suﬃxed to discover
ﬁxed point, j ≤|L| +1. In the worst case, all call sites may occur in σ thus the
worst case length of any call string is K ×(|L|+1).
9.5
The Motivating Example Revisited
It is appropriate that our explanation of data ﬂow analysis in this book should end
with the example that it began with. This section presents context sensitive interpro-
cedural liveness analysis of the program in Section 1.1.
The examples in this part have considered programs with global variables. How-
ever, our motivating example from Figure 1.1 on page 2 contains local pointer vari-
ables that are passed as actual parameters. As observed in Section 7.5, this requires
data ﬂow information to be propagated between the actual parameters and formal
parameters. We model this using a couple of assignments and a special edge in the
supergraph as illustrated in Figure 9.17 on the facing page.
For correct Modelling of local pointer variables as actual parameters, we need to
assign them to formal variables in the call node (C2 in our case) and restore them
in the return node (R2 in our case). The assignment in C2 indicates that the heap
memory reachable from succ is reachable from n in a recursive call. The assign-
ment in R2 indicates that the heap memory reachable from n in a recursive call is
reachable from succ in an outer call. Besides, we need to bypass the call by an edge
because the local variables are available in the program fragment beyond the call due
to call by copy semantics. We have achieved this by adding edge n2 →n4. In the
absence of this edge, if formal parameter n is made null in procedure dfTraverse
our assignment in R2 will make succ null in node n4. Since the pointer has been
passed by copy, this is incorrect. The assignment is required because the heap cells
reachable from succ could be inﬂuenced by n but the address contained in succ is
not modiﬁed because succ is a local variable and is not passed by reference.
Liveness analysis is a backward analysis. Hence we interchange the roles of call
nodes and return nodes. Now a call site is appended and call strings grow when a
return node is visited. Call sites are removed at the call nodes. By the same token,
representation is performed at Endr of procedure r and regeneration is performed
© 2009 by Taylor & Francis Group, LLC

Value-Based Approach to Interprocedural Data Flow Analysis
325
n1 succ=n->child n1
Startp
Startp
n2 if (succ!=NULL) n2
C2 n=succ C2
R2 succ=n R2
n4 next=succ->sib 4
n5 succ=next n5
printf(”%d\n”,n->num)
n6
Endp
Endp
T
F
C1 n=tree C1
R1 tree=n R1
FIGURE 9.17
Supergraph for procedure dfTraverse from the program in Figure 1.1 on page 2.
Observe the assignments in call and return nodes and the edge n3 →n4 for handling
parameters.
at Startr. Besides, a return node always precedes the corresponding call node in the
work list of signiﬁcant nodes. Note that the recursive call in this example is contained
in a loop and hence we can expect the representation made at Endp to change.
Our data ﬂow values are access graphs as deﬁned in Section 4.4.3. We use the data
ﬂow equations deﬁned in Section 4.4.4 for computing the eﬀect of intraprocedural
nodes on access graphs representing explicit liveness. Since ﬁeld name sib is deref-
erenced only in node n4, summarization can be achieved without subscripting this
ﬁeld name with the node number. Similar remarks apply to the ﬁeld name child.
Hence, we drop the subscripts of ﬁeld names.
The ﬁnal data ﬂow information is provided in Figure 9.18 on page 327. Below we
list some path fragments to show the ﬂow of information:
• ρ = (Endp,n6,n2,n5,n4,n2,n5,n4,R2,n6,n2)
The data ﬂow value at the start of ρ is c1,EG and the data ﬂow value at the
© 2009 by Taylor & Francis Group, LLC

326
Data Flow Analysis: Theory and Practice
end of ρ is
%
c1c2,
n
sib
&
.
• Further traversal of n2,n1 results in the data ﬂow value
%
c1c2,
n
child
sib
&
.
• Traversal of n2,n5,n4 creates the liveness graph
succ
sib . A further
traversal of n2,n1 results in the graph
n
child
sib . When this com-
bines with the data ﬂow value at n1 obtained in the previous step, we get the
qualiﬁed data ﬂow value
%
c1c2,
n
child
sib
&
.
• The above data ﬂow value reaches n2 along the path n1,Startp,C2,n1 after
removing the call string suﬃx c2 as
%
c1,
n
child
sib
&
. Further, it reaches
Endp along the path n2,n5,n4,R2,Endp as
%
c1c2,
n
child
sib
&
.
We leave it for the reader to ﬁnd out how the other edges get included in the above
liveness graph and how the graphs are propagated to various nodes in the supergraph.
Observe that in the liveness graphs at the entry of n4, there is no edge from succ
to child. Further, there is no graph rooted at succ at the entry of n5. This conﬁrms
our conclusion in Section 1.1 that the pointer succ can be freed between n4 and n5.
Also note that the access path n
child is not live in nodes n2, n4, n5, and n6 in the
data ﬂow information in Figure 9.18. However, it is live in the same nodes in the
data ﬂow information computed with conservative interprocedural summarization in
Section 4.4.5. This is because n
child is not explicitly live; it is only implicitly live
in that it is aliased to an access path that is explicitly live.
9.6
Summary and Concluding Remarks
This chapter has explored the approach of computing distinct values for distinct con-
texts instead of constructing context independent functions. Bit vector frameworks
are amenable to such an analysis when the context is restricted to immediate caller.
This method overwrites the context at every call and recovers it after the call is over.
A natural generalization of this method is to remember the entire call history in
the form of a call string. This method is attractive because it is simple and general.
Beside, it is context sensitive and hence computes precise data ﬂow information. The
© 2009 by Taylor & Francis Group, LLC

Value-Based Approach to Interprocedural Data Flow Analysis
327
Node
Liveness Graphs at the Entry of Nodes
c1
c1c2
c1c2c2
Startp
n
child
sib
n
child
sib
n
child
sib
n1
n
child
sib
n
child
sib
n2
n
succ
child
sib
n
sib
child
succ
child
sib
C2
succ
child
sib
succ
child
sib
R2
n
sib
child
n
sib
child
n4
n
succ
child
sib
n
sib
child
succ
child
sib
n5
n
next
child
sib
n
sib
child
next
child
sib
n6
n
n
sib
child
Endp
EG
n
sib
child
c1c2c2 is represented
by c1c2
FIGURE 9.18
Interprocedural liveness analysis of heap data for the program in Figure 9.17.
main diﬃculty in this method is that the number and length of call strings could be
exponentially large. Further, in the case of recursive programs, the termination of
the construction of call strings must be explicitly ensured. This can be achieved by
adapting the “overwrite-and-recover” technique from the method that uses restricted
© 2009 by Taylor & Francis Group, LLC

328
Data Flow Analysis: Theory and Practice
contexts. This adaptation results in call strings with equivalent values being repre-
sented by a single call string at Start of a procedure and regenerating the represented
call string at the End.
The value-based termination criterion presented in this chapter is diﬀerent from
the original termination criterion of constructing all call strings up to the length of
K ×(| L | +1)2 where K is the maximum number of call sites and L is the lattice. This
number reduces to 3K for bit vector frameworks. This termination length results
in a combinatorially large number of call strings. From Theorem 9.4, when value-
based termination criterion is used, the worst case length of a call string reduces to
K ×(|L|+1). Empirical measurements show a dramatic reduction in the number and
maximum length of call strings compared to those in the original method.
9.7
Bibliographic Notes
The restricted context based analysis presented in this chapter is based on the work
by Myers [79]. The call strings method was proposed by Sharir and Pnueli [93].
The termination criterion using convergence of data ﬂow values has been proposed
by Khedker and B. Karkare [61]. An orthogonal approach of reducing the space
requirements in a context sensitive value-based interprocedural analysis is to use
BDDs to encode data ﬂow information. This has been proposed by Whaley and
Lam [104]. They have found that this makes the method scalable. Although their
approach is context insensitive in recursive contexts, the key idea of using BDDs to
increase scalability seems very useful.
Since ifps in bit vector frameworks consist only of identity functions, it is possible
to use an alternative method of terminating call string construction. As shown by B.
Karkare and Khedker [54], it is suﬃcient to construct all call strings in which a call
site appears at most three times. Note that this is diﬀerent from Sharir and Pnueli’s
termination length of 3K. In Sharir and Pnueli’s method, if the length of a call string
is smaller than 3K, it is extended even if it results in four occurrences of a call cite.
Although the worst case length in B. Karkare and Khedker’s method is same, empiri-
cal measurements of interprocedural reaching deﬁnitions analysis shows a signiﬁcant
reduction in the number and maximum length of call strings.
Sharir and Pnueli [93] also present an approximate call strings method in which
call string suﬃx of a ﬁxed length k is remembered. This retains context sensitivity
for call depths of k but for the call sequences beyond this depth, the method essen-
tially becomes context insensitive. Eﬀectiveness of this method has been empirically
measured by Martin [72] who concluded that a value of k > 2 did not increase the
precision signiﬁcantly for constant propagation. Khedker and B. Karkare [61] have
also presented an approximate version where the imprecision can be adjusted on de-
mand. The basic idea is to allow say k occurrences of a call site in a call string and
use representation and regeneration for all such call strings. When the call string
© 2009 by Taylor & Francis Group, LLC

Value-Based Approach to Interprocedural Data Flow Analysis
329
grows and the number of occurrences of a call site exceeds k, the data ﬂow values
are computed iteratively by retaining the same call string instead of extending it. Un-
like Sharir and Pnueli’s approximate method, this method is context sensitive until k
unfoldings of recursive calls.
The interprocedural points-to analysis by Emami, Ghiya and Hendren [34] can be
viewed as a value-based approach. It uses a variant of call graph called an invocation
graph in which recursive invocations of procedures result in creating two nodes for
a procedure: One node is recursive whereas the other node is approximate. Thus it
is context sensitive in the ﬁrst unfolding of recursion but context insensitive beyond
that. We leave it for the reader to verify that the Emami’s method computes imprecise
points-to graphs for the program in Figure 9.6 on page 305 compared to the points-to
graphs in Figure 9.7 on page 306 computed using call strings method.
© 2009 by Taylor & Francis Group, LLC

© 2009 by Taylor & Francis Group, LLC

Part III
Implementing Data Flow
Analysis
331
© 2009 by Taylor & Francis Group, LLC

© 2009 by Taylor & Francis Group, LLC

10
Implementing Data Flow Analysis in GCC
This chapter presents a generic data flow analyzer for per function (i.e., intraproce-
dural) bit vector data flow analysis in GCC 4.3.0. We call this infrastructure gdfa.
The analyzers implemented using gdfa are called pfbvdfa. gdfa has been used to
implement several bit vector data ﬂow analyses.
The design and implementation of gdfa is motivated by the following objectives:
• Demonstrating the practical signiﬁcance of the following important general-
ization: Instead of implementing speciﬁc analyses directly, it is useful to im-
plement a generic driver that is based on a carefully chosen set of abstractions.
The task of implementing a particular analyzer then reduces to merely speci-
fying the analysis by instantiating these abstractions to concrete values.
• Providing an easy to use and easy to extend data ﬂow analysis infrastructure.
The goal is to facilitate experimentation in terms of studying existing analyses,
deﬁning new analyses, and exploring diﬀerent analysis algorithms.
Section 10.1 describes the speciﬁcation mechanism of gdfa and shows how the re-
sulting pass can be included in GCC 4.3.0. We illustrate it for the bit vector analyses
implemented using gdfa. Section 10.2 demonstrates how pfbvdfa can be used. Sec-
tion 10.3 describes the implementation of gdfa. This section also shows how local
property computation can be driven by speciﬁcations. Finally Section 10.4 suggests
some possible enhancements to gdfa.
The GCC related details in this chapter are interleaved with the description of
gdfa. Appendix A provides a short introduction to GCC, its installation, and how
to obtain its patch for gdfa.∗The code presented in this chapter is a slightly edited
version of the original code. This was required to ﬁt a page size constraints.
10.1
Specifying a Data Flow Analysis
In this section we look at how we can use the generic data ﬂow analysis driver to im-
plement a data ﬂow analysis pass in GCC. The implemented pass has to be registered
∗We use GCC to denote the GNU compiler generation framework using which a compiler can be built for
a given processor. The compiler so generated is denoted by gcc.
333
© 2009 by Taylor & Francis Group, LLC

334
Data Flow Analysis: Theory and Practice
with the pass manager in GCC so that it can be executed by the compiler.
10.1.1
Registering a Pass With the Pass Manager in GCC
gdfa works on the gimple version of the intermediate representation used by GCC.
We have included pfbvdfa passes such that they are invoked by default when gcc is
used for compiling a program. When gcc is built, this causes pfbvdfa passes to run
on the entire source of gcc which consists of over a million lines of C code. This
helps in ensuring that these do not cause any exception in the compilation sequence.
After constructing the gimple representation, gcc views the rest of the compilation
as sequential execution of various passes. This is carried out by traversing a linked
list whose nodes contain pointers to the entry functions of these passes. A pass is
registered with the pass manager through the following steps:
• Instantiating a variable as an instance of
struct tree_opt_pass in some
ﬁle.
• Declaring this variable as an extern variable in header ﬁle tree-pass.h.
• Inserting this variable in the linked list of passes using the macro NEXT_PASS
in function init_optimization_passes in ﬁle passes.c.
Here is the declaration of struct tree_opt_pass. For convenience comments
have been removed and are used in the explanation that follows.
0 struct tree_opt_pass
1 {
2
const char *name;
3
bool (*gate) (void);
4
unsigned int (*execute) (void);
5
struct tree_opt_pass *sub;
6
struct tree_opt_pass *next;
7
int static_pass_number;
8
unsigned int tv_id;
9
unsigned int properties_required;
10
unsigned int properties_provided;
11
unsigned int properties_destroyed;
12
unsigned int todo_flags_start;
13
unsigned int todo_flags_finish;
14
char letter;
15 };
The name of the pass (line 2) is used as a fragment of the dump ﬁle name. We have
used the names like gdfa_ave. The gate function (line 3) is used to check whether
this pass and all its sub-passes should be executed or not. They are executed only if
this function returns true. If no such checking is required, this function pointer can
© 2009 by Taylor & Francis Group, LLC

Implementing Data Flow Analysis in GCC
335
be NULL. The execute function (line 4) is entry function of the pass. If this function
pointer is NULL, there should be sub-passes otherwise this pass does nothing. The
return value tells gcc what more needs to be done. The variable sub (line 5) is a list
of sub-passes that should be executed depending upon the gate predicate. If there
are sub-passes that must be executed unconditionally, then they are listed in next
(line 6). The static pass number (line 7) is used as a fragment of the dump ﬁle name.
If it is speciﬁed as 0, the pass manager computes its value depending on the position
of the pass. It is this that generated numbers 15, 16, 17, 18, and 19 for our data ﬂow
analyses. Variable tv_id is the variable that can be used as a time variable. The rest
of the variables are self-explanatory. The last variable letter is used to annotate
RTL code that is emitted.
We have registered available expressions analysis by creating a structure variable
called pass_gimple_pfbv_ave_dfa as shown below.
struct tree_opt_pass pass_gimple_pfbv_ave_dfa =
{
"gdfa_ave",
/* name */
NULL,
/* gate */
gimple_pfbv_ave_dfa,
/* execute */
NULL,
/* sub */
NULL,
/* next */
0,
/* static_pass_number */
0,
/* tv_id */
0,
/* properties_required */
0,
/* properties_provided */
0,
/* properties_destroyed */
0,
/* todo_flags_start */
0,
/* todo_flags_finish */
0
/* letter */
};
This variable is declared as follows in ﬁle tree-pass.h
extern struct tree_opt_pass pass_gimple_pfbv_ave_dfa;
The next step in registering this pass is to include it in the list of passes. We show
below the relevant code fragment from function init optimization passes in
ﬁle passes.c:
© 2009 by Taylor & Francis Group, LLC

336
Data Flow Analysis: Theory and Practice
NEXT_PASS (pass_build_cfg);
/* Intraprocedural dfa passes begin */
NEXT_PASS (pass_init_gimple_pfbvdfa);
NEXT_PASS (pass_gimple_pfbv_ave_dfa);
NEXT_PASS (pass_gimple_pfbv_pav_dfa);
NEXT_PASS (pass_gimple_pfbv_ant_dfa);
NEXT_PASS (pass_gimple_pfbv_lv_dfa);
NEXT_PASS (pass_gimple_pfbv_rd_dfa);
NEXT_PASS (pass_gimple_pfbv_pre_dfa);
/* Intraprocedural dfa passes end */
Finally, we need to include the new ﬁle names in the GCC build system. This
is done by listing the ﬁle names and their dependencies in Makefile.in in the
gcc-4.3.0/gcc directory. Appendix A provides the steps for building gcc.
10.1.2
Specifying Available Expressions Analysis
The speciﬁcation mechanism supported by gdfa is simple and succinct. It follows
the GCC mechanism of speciﬁcation by using a struct as a hook and by requiring
the user to create a variable by instantiating the members of the struct deﬁned for
the purpose.
For available expressions analysis, we deﬁne a variable called gdfa_ave which is
of the type struct gimple_pfbv_dfa_spec gdfa_ave.
0 struct gimple_pfbv_dfa_spec gdfa_ave =
1 {
2
entity_expr,
/* entity
*/
3
ONES,
/* top_value
*/
4
ZEROS,
/* entry_info
*/
5
ONES,
/* exit_info
*/
6
FORWARD,
/* traversal_order
*/
7
INTERSECTION,
/* confluence
*/
8
entity_use,
/* gen_effect
*/
9
down_exp,
/* gen_exposition
*/
10
entity_mod,
/* kill_effect
*/
11
any_where,
/* kill_exposition
*/
12
global_only,
/* preserved_dfi
*/
13
identity_forward_edge_flow,
/* forward_edge_flow
*/
14
stop_flow_along_edge,
/* backward_edge_flow */
15
forward_gen_kill_node_flow,
/* forward_node_flow
*/
16
stop_flow_along_node
/* backward_node_flow */
17 };
© 2009 by Taylor & Francis Group, LLC

Implementing Data Flow Analysis in GCC
337
Before we explain the above, we present the rest of the code required to complete
the speciﬁcation.
18 pfbv_dfi ** AV_pfbv_dfi = NULL;
19
20 static unsigned int
21 gimple_pfbv_ave_dfa(void)
22 {
23
24
AV_pfbv_dfi = gdfa_driver(gdfa_ave);
25
26
return 0;
27 }
Nothing more is required for specifying available expressions analysis apart from
registering it with the pass manager with function gimple_pfbv_ave_dfa as its
entry point as described in Section 10.1.1. This function calls the gdfa driver passing
the speciﬁcation variable gdfa_ave as actual parameter. The data ﬂow information
computed by the driver is stored in a pointer to an array called AV_pfbv_dfi; each
element of this array represents the data ﬂow information for a basic block and is an
instance of the following type deﬁned by gdfa.
typedef struct pfbv_dfi
{
dfvalue gen;
dfvalue kill;
dfvalue in;
dfvalue out;
} pfbv_dfi;
The semantics expressed by struct gimple_pfbv_dfa_spec gdfa_ave is as
described below: Line 2 declares that the relevant entities for this analysis are expres-
sions (entity_expr). Line 3 speciﬁes that  is “all ONES” implying the universal
set Expr. The speciﬁcation “all ZEROS” on line 4 initializes the BIStart to ∅whereas
ONES on line 5 renders BIEnd irrelevant because it is same as  . Line 6 declares the
direction of traversal to be FORWARD. Note that this is independent of the direction
of ﬂow and only inﬂuences the number of iterations. If we choose the direction of
traversal as BACKWARD, the resulting data ﬂow information will remain same except
that it may take a much larger number of iterations. Line 7 declares the  to be ∩.
Line 12 directs the driver to preserve only the global data ﬂow information (In and
Out); the driver can reclaim the space occupied by the local data ﬂow information
(Gen and Kill).
The most interesting elements of the speciﬁcation are the speciﬁcations of local
© 2009 by Taylor & Francis Group, LLC

338
Data Flow Analysis: Theory and Practice
properties and ﬂow functions:
• Local property speciﬁcation.
Lines 8 to 11 deﬁne the Gen and Kill kill sets for a block. Observe that this
mechanism closely follows the description in Section 2.2.
– Lines 8 and 9 say that when a downwards exposed (down_exp) use of an
entity (entity_use) is found in a basic block, it is included in the Gen
set of the block. From line 2 we know that the entity under consideration
is an expression (entity_expr).
– Lines 10 and 11 say that when a modiﬁcation of an entity (entity_mod)
is found in a basic block, it is included in the Kill set of the block. This
modiﬁcation need not be upwards exposed or downwards exposed, it can
appear any_where.
This is possible because the gdfa driver is aware of the fact that the use of an
entity could be aﬀected by its modiﬁcation and hence the notion of exposition
of an entity is explicated in the speciﬁcation.
• Flow function speciﬁcation.
Lines 13 to 16 specify the ﬂow functions for available expressions analysis
as required by the generic data ﬂow Equations (5.1) and (5.2). The forward
edge ﬂow function
−→
fn→m in available expressions analysis is φid (line 13)
whereas the forward node ﬂow function −→
fn is the conventional Gen-Kill func-
tion f(X) = Gen ∪(In −Kill). Further, there is no backward ﬂow i.e., ←−fn and
←−fn→m are φ (Section 5.1). This is speciﬁed by lines 14 and 16. All these func-
tions are supported by gdfa and it is enough to associate the function pointers
with appropriate functions.
When the nature of data ﬂow is diﬀerent from the default ﬂows, it is also possi-
ble to write custom functions—we show how it is done for partial redundancy
elimination in Section 10.1.3.
10.1.3
Specifying Other Bit Vector Data Flow Analyses
Given the speciﬁcation of available expressions analysis, it is easy to visualize spec-
iﬁcations for other bit vector frameworks. We describe the required changes in the
following:
• Partially available expressions analysis.
Conﬂuence should be UNION,  and BIEnd should be ZEROS.
• Reaching deﬁnitions analysis.
Entity should be entity_defn, conﬂuence should be UNION,  and BIEnd
should be ZEROS.
© 2009 by Taylor & Francis Group, LLC

Implementing Data Flow Analysis in GCC
339
• Anticipable expressions analysis.
The data ﬂow equations for anticipable expressions analysis are Equations (2.9)
and (2.10). In this case it is desirable, though not necessary, to choose the di-
rection of traversal as BACKWARD. The exposition for Gen should be changed
to up_exp. BIStart should be ONES and BIEnd should be ZEROS. Flow functions
would change as follows:
– forward edge ﬂow function −→
fn→m should be stop_flow_along_edge,
– forward node ﬂow function −→
fn should be stop_flow_along_node,and
– backward node ﬂow function ←−fn should be the default Gen-Kill function
backward_gen_kill_node_flow.
• Live variables analysis.
This speciﬁcation would be similar to that of anticipable expressions analysis
except that the entity should be entity_var, conﬂuence should be UNION,  
and BIEnd should be ZEROS.
• Partial redundancy elimination.
Here it would useful to change the gate function to this pass to check that
available expressions analysis and partially available expressions analysis has
been performed.
The speciﬁcation of data ﬂow analysis would be similar to that of antici-
pable expressions analysis except that the ﬂow functions would change. The
data ﬂow equations for anticipable expressions analysis are Equations (2.9)
and (2.10) whereas the data ﬂow equations for partial redundancy elimina-
tion are Equations (2.11) and (2.15). Clearly, the change is only in the ﬂow
function in the equation for Inn. In particular, the forward edge ﬂow function
−→
fn→m and the backward node ﬂow function ←−fn cannot be chosen from the de-
fault functions supported by gdfa. We deﬁne the required functions as shown
below.
dfvalue
forward_edge_flow_pre(basic_block src, basic_block dest)
{
dfvalue temp;
temp = union_dfvalues (OUT(AV_pfbv_dfi,src),
CURRENT_OUT(src));
return temp;
}
© 2009 by Taylor & Francis Group, LLC

340
Data Flow Analysis: Theory and Practice
In this function, src and dest indicate the source and destination of an edge.
Since this ﬂow function is used in computing Inn, dest represents n and src repre-
sents the given predecessor node p. Under the assumption that the data ﬂow infor-
mation of available expressions analysis is stored in the variable AV_pfbv_dfi, the
term OUT(AV_pfbv_dfi,src) represents AvOut p whereas the Out p is represented
by the term CURRENT_OUT(src). Thus this ﬂow function computes AvOut p ∪Out p
for a given predecessor p.
The deﬁnition of backward node ﬂow is similar to that of the default node ﬂow
except that we need to include the value of PavInn. This is easily achieved by the
function deﬁned below:
dfvalue
backward_node_flow_pre(basic_block bb)
{
dfvalue temp1, temp2;
temp1 = backward_gen_kill_node_flow(bb);
temp2 = intersect_dfvalues (IN(PAV_pfbv_dfi,bb),
temp1);
if (temp1)
free_dfvalue_space(temp1);
return temp2;
}
Here bb is the current node n. The default backward node ﬂow function is used to
compute the data ﬂow information in the variable temp1. Under the assumption that
the data ﬂow information of partially available expressions analysis is stored in the
variable PAV_pfbv_dfi, the term IN(PAV_pfbv_dfi,bb) represents PavInn. All
that further needs to be done is to intersect them.
This completes the speciﬁcation of partial redundancy elimination.
10.2
An Example of Data Flow Analysis
We use the example program from Figure 2.1 on page 27 in Chapter 2 to demonstrate
the use of analyzer implemented using gdfa. We show the result of live variables
analysis and available expressions analysis. A C program that represents the CFG in
Figure 2.1 is given below.
© 2009 by Taylor & Francis Group, LLC

Implementing Data Flow Analysis in GCC
341
0 int x, y, z;
1
2 int exmp(void)
3 {
int a, b, c, d;
4
5
b = 4;
6
a = b + c;
7
d = a * b;
8
if (x < y)
9
b = a -c;
10
else
11
{
do
12
{
c = b + c;
13
if (y > x)
14
{
do
15
{
d = a + b;
16
f(b + c);
17
} while(y > x);
18
}
19
else
20
{
c = a * b;
21
f(a - b);
22
}
23
g (a + b);
24
} while(z > x);
25
}
26
h(a-c);
27
f(b+c);
28 }
Since the original example does not show conditions explicitly, we have used
global variables in conditions; these variables are ignored by intraprocedural data
ﬂow analysis. Further, the functions f, g, and h are unspeciﬁed. Since C uses call by
value mechanism, we have ignored the eﬀects of function calls under the assumption
that arrays and addresses of variables are not passed as parameters.
10.2.1
Executing the Data Flow Analyzer
Our example program is not a complete program hence we cannot compile it into an
executable program. For such programs we must use the -c option that creates only
an object ﬁle for the given input C ﬁle. Alternatively, we can use the -S option that
stops the compilation after generating the corresponding assembly ﬁle. We use the
following command to generate text ﬁles that provide the results of our passes.
$ gcc -S -fdump-tree-all -fgdfa
exmp.c
© 2009 by Taylor & Francis Group, LLC

342
Data Flow Analysis: Theory and Practice
The option -fdump-tree-all enables generation of the dump ﬁles for passes
implemented on gimple representation. The option -fgdfa emits the results of our
data ﬂow analysis passes in respective dump ﬁles. The dump ﬁles that are of interest
to us are:
Name
Description of the output
exmp.c.013t.cfg
CFG
exmp.c.015t.gdfa_ave
available expressions analysis
exmp.c.016t.gdfa_pav
partially available expression analysis
exmp.c.017t.gdfa_ant
anticipable expressions analysis
exmp.c.018t.gdfa_lv
live variables analysis
exmp.c.018t.gdfa_rd
reaching deﬁnitions analysis
exmp.c.019t.gdfa_pre
partial redundancy elimination
The numbers indicate the position of the pass in the sequence of passes. Pass
number 014 processes the CFG to discover the entities of interest to us and per-
forms depth ﬁrst numbering of basic blocks so that post order or reverse post order
traversal can be used by our data ﬂow analysis passes. These numbers would change
depending upon the exact sequence of passes in a given version of GCC.
10.2.2
Examining the Gimple Version of CFG
The gimple representation used by GCC consists of three address code statements.
The CFG version of gimple representation identiﬁes basic blocks and explicates con-
trol ﬂow between basic blocks. It also shows the declarations of temporary variables.
There are two categories of temporary variables in gimple:
• Artiﬁcial variables. These variables are created to store the values of global
variables. Subsequently, these variables are used in expressions. Any assign-
ment to a global variable uses the original global variable so that the latest
value can be read into a new artiﬁcial variable for a subsequent use.
Artiﬁcial variables are also created for those instances of local variables that
are assigned a value returned by a function call. The value of these artiﬁcial
variables is then assigned to the local variables.
• Temporary variables. These are the traditional temporary variables which hold
the intermediate results of expression computations. The parameters passed to
functions are also represented by temporary variables.
The declaration part of gimple CFG in exmp.c.013t.cfg is:
© 2009 by Taylor & Francis Group, LLC

Implementing Data Flow Analysis in GCC
343
0
1 ;; Function exmp (exmp)
2
3 exmp ()
4 {
5
int d;
6
int c;
7
int b;
8
int a;
9
int D.1205;
10
int D.1204;
11
int x.7;
12
int z.6;
13
int D.1201;
14
int D.1200;
15
int x.5;
16
int y.4;
17
int D.1197;
18
int x.3;
19
int y.2;
20
int y.1;
21
int x.0;
The gimple representation of our program initially contains eight artiﬁcial vari-
ables: x.7, z.6, x.5, y.4, x.3, y.2, y.1, and x.0. Each use of a global variable
causes a distinct number to be suﬃxed to the variable. The temporary variables
are: D.1205, D.1204, D.1201, D.1200, and D.1197. They represent the param-
eters of the ﬁve calls made in our program. There are no temporary variables for
holding intermediate results of computations because our expressions consist of a
single operation—temporaries are created for expressions containing more than one
operation.
The CFG contains a unique ENTRY block which does not contain any computation
and does not have any predecessor block. Similarly, there is an EXIT block which
does not contain any computation and does not have any successor. An unconditional
control transfer from a block to another block is recorded as fallthru whereas a
conditional transfer is labeled true or false. All auxiliary information about a
block e.g., block number, list of successors and predecessors, nature of control ﬂow
etc. is shown with a # mark as the ﬁrst symbol on a line.
ENTRY and EXIT blocks are not listed explicitly in the dump. Internally they are
numbered block 0 and block 1 respectively. Hence the ﬁrst block that appears in the
CFG is block 2 as shown below. It corresponds to block n1 in Figure 2.1 on page 27.
Observe the use of artiﬁcial variables x.0 and y.1 in the block.
© 2009 by Taylor & Francis Group, LLC

344
Data Flow Analysis: Theory and Practice
22
# BLOCK 2
23
# PRED: ENTRY (fallthru)
24
b = 4;
25
a = b + c;
26
d = a * b;
27
x.0 = x;
28
y.1 = y;
29
if (x.0 < y.1)
30
goto <bb 3>;
31
else
32
goto <bb 4>;
33
# SUCC: 3 (true) 4 (false)
This block has a conditional control transfer at the end of it. Its successor blocks
are blocks 3 and 4 which correspond to blocks n2 and n3 respectively in the CFG
in Figure 2.1. Note that the predecessors of a block are also labeled to indicate the
nature of control transfer (i.e., fallthru, true, or false).
34
# BLOCK 3
35
# PRED: 2 (true)
36
b = a - c;
37
goto <bb 9>;
38
# SUCC: 9 (fallthru)
39
40
# BLOCK 4
41
# PRED: 2 (false) 8 (true)
42
c = b + c;
43
y.2 = y;
44
x.3 = x;
45
if (y.2 > x.3)
46
goto <bb 5>;
47
else
48
goto <bb 7>;
49
# SUCC: 5 (true) 7 (false)
The structure of the control ﬂow between the remaining blocks is a little diﬀerent
from the CFG shown in Figure 2.1. Block 5 in the gcc generated CFG combines
blocks n5 and n6 of Figure 2.1 because there is a strictly sequential control ﬂow
between them. Block 6 consists of a single goto that will be optimized away later.
Figure 2.1 does not have this block. Block 7 corresponds to block n4 and block 8
corresponds to block n7 in Figure 2.1. The last block containing some program code
is block 9 which corresponds to n8 in Figure 2.1. Observe that it has EXIT as its
successor. The details of these blocks are as follows:
© 2009 by Taylor & Francis Group, LLC

Implementing Data Flow Analysis in GCC
345
50
# BLOCK 5
51
# PRED: 4 (true) 5 (true)
52
d = a + b;
53
D.1197 = b + c;
54
f (D.1197);
55
y.4 = y;
56
x.5 = x;
57
if (y.4 > x.5)
58
goto <bb 5>;
59
else
60
goto <bb 6>;
61
# SUCC: 5 (true) 6 (false)
62
63
# BLOCK 6
64
# PRED: 5 (false)
65
goto <bb 8>;
66
# SUCC: 8 (fallthru)
67
68
# BLOCK 7
69
# PRED: 4 (false)
70
c = a * b;
71
D.1200 = a - b;
72
f (D.1200);
73
# SUCC: 8 (fallthru)
74
75
# BLOCK 8
76
# PRED: 6 (fallthru) 7 (fallthru)
77
D.1201 = a + b;
78
g (D.1201);
79
z.6 = z;
80
x.7 = x;
81
if (z.6 > x.7)
82
goto <bb 4>;
83
else
84
goto <bb 9>;
85
# SUCC: 4 (true) 9 (false)
86
87
# BLOCK 9
88
# PRED: 3 (fallthru) 8 (false)
89
D.1204 = a - c;
90
h (D.1204);
91
D.1205 = b + c;
92
f (D.1205);
93
return;
94
# SUCC: EXIT
95
96 }
© 2009 by Taylor & Francis Group, LLC

346
Data Flow Analysis: Theory and Practice
In essence, the CFGs constructed by gcc are quite similar to the CFGs that we
have seen in the earlier parts of the book.
10.2.3
Examining the Result of Data Flow Analysis
The results of an analysis are available in internal data structures in a ready to use
form. Section 10.1.3 shows how they can be used when we describe the implementa-
tion of partial redundancy elimination which needs the result of available expressions
analysis and partially available expressions analysis. Here we present the textual
dump of the results produced by the options -fdump-tree-all and -gdfa.
File exmp.c.018t.gdfa_lv contains the result of liveness analysis. It indicates
that for this example Var = {a,b,c,d} intraprocedural liveness analysis. It also indi-
cates the bit position for each variable. Variable d is the ﬁrst to be considered. This
is because internally, the variables are added to the head of the list of variables rather
than its tail. Observe that the other three category of variables (global, artiﬁcial, and
local) have been eliminated from consideration.†
0 ;; Function exmp (exmp)
1
2 Number of relevant entities: 4
3
4
Bit position and entity mapping is
********************
5
0:(d),1:(c),2:(b),3:(a)
6
7
Initial values **************************************
8
9 Basic Block 2. Preds:
ENTRY. Succs:
3 4
10
----------------------------
11
GEN Bit Vector:
0100
12
GEN Entities:
(c)
13
------------------------------
14
KILL Bit Vector:
1011
15
KILL Entities:
(d),(b),(a)
16
------------------------------
17
IN Bit Vector:
0000
18
IN Entities:
19
------------------------------
20
OUT Bit Vector:
0000
21
OUT Entities:
22
------------------------------
The Inn and Outn properties have been initialized to ∅which is  for live variables
analysis. In the following, we produce only the lines that enumerate the Genn and
†At the moment, our implementation does not consider formal parameters.
© 2009 by Taylor & Francis Group, LLC

Implementing Data Flow Analysis in GCC
347
Killn in terms of entity names rather than bit vectors.
Basic Block 2. Preds:
ENTRY. Succs:
3 4
----------------------------
GEN Entities:
(c)
KILL Entities:
(d),(b),(a)
------------------------------
Basic Block 3. Preds:
2. Succs:
9
----------------------------
GEN Entities:
(c),(a)
KILL Entities:
(b)
------------------------------
Basic Block 4. Preds:
2 8. Succs:
5 7
----------------------------
GEN Entities:
(c),(b)
KILL Entities:
(c)
------------------------------
Basic Block 5. Preds:
4 5. Succs:
5 6
----------------------------
GEN Entities:
(c),(b),(a)
KILL Entities:
(d)
------------------------------
Basic Block 6. Preds:
5. Succs:
8
----------------------------
GEN Entities:
KILL Entities:
------------------------------
Basic Block 7. Preds:
4. Succs:
8
----------------------------
GEN Entities:
(b),(a)
KILL Entities:
(c)
------------------------------
Basic Block 8. Preds:
6 7. Succs:
4 9
----------------------------
GEN Entities:
(b),(a)
KILL Entities:
------------------------------
Basic Block 9. Preds:
3 8. Succs:
EXIT
----------------------------
GEN Entities:
(c),(b),(a)
KILL Entities:
------------------------------
It can be readily veriﬁed from the table in Example 2.3 on page 27 that the local
data ﬂow values given below are identical to the values discovered earlier.
The ﬁnal values are also generated in the same format. We show selected lines
from the ﬁnal result of liveness analysis of our example program:
© 2009 by Taylor & Francis Group, LLC

348
Data Flow Analysis: Theory and Practice
Total Number of Iterations = 2 *********
Basic Block 2. Preds:
ENTRY. Succs:
3 4
----------------------------
IN Entities:
(c)
OUT Entities:
(c),(b),(a)
------------------------------
Basic Block 3. Preds:
2. Succs:
9
----------------------------
IN Entities:
(c),(a)
OUT Entities:
(c),(b),(a)
------------------------------
Basic Block 4. Preds:
2 8. Succs:
5 7
----------------------------
IN Entities:
(c),(b),(a)
OUT Entities:
(c),(b),(a)
------------------------------
Basic Block 5. Preds:
4 5. Succs:
5 6
----------------------------
IN Entities:
(c),(b),(a)
OUT Entities:
(c),(b),(a)
------------------------------
Basic Block 6. Preds:
5. Succs:
8
----------------------------
IN Entities:
(c),(b),(a)
OUT Entities:
(c),(b),(a)
------------------------------
Basic Block 7. Preds:
4. Succs:
8
----------------------------
IN Entities:
(b),(a)
OUT Entities:
(c),(b),(a)
------------------------------
Basic Block 8. Preds:
6 7. Succs:
4 9
----------------------------
IN Entities:
(c),(b),(a)
OUT Entities:
(c),(b),(a)
------------------------------
Basic Block 9. Preds:
3 8. Succs:
EXIT
----------------------------
IN Entities:
(c),(b),(a)
OUT Entities:
------------------------------
We leave it for the reader to verify that these values are identical to the values in
the table in Example 2.3 on page 27.
If the option -fgdfa is replaced by -fgdfa-details, data ﬂow values after each
© 2009 by Taylor & Francis Group, LLC

Implementing Data Flow Analysis in GCC
349
iteration are generated.
The result of data ﬂow analyses involving expressions is produced much the same
way. File exmp.c.015t.gdfa_ave. contains the details of available expressions
analysis. The initial information in this ﬁle is:
0 ;; Function exmp (exmp)
1
2 Number of relevant entities: 5
3
4
Bit position and entity mapping is
**********************
5
0:(b + c),1:(a * b),2:(a - c),3:(a + b),4:(a - b)
6
7
Initial values **************************************
8
9 Basic Block 2. Preds:
ENTRY. Succs:
3 4
10
----------------------------
11
GEN Bit Vector: 11000
12
GEN Entities:
(b + c),(a * b)
13
----------------------------
14
KILL Bit Vector:11111
15
KILL Entities:
(b + c),(a * b),(a - c),(a + b),(a - b)
16
----------------------------
17
IN Bit Vector:
11111
18
IN Entities:
(b + c),(a * b),(a - c),(a + b),(a - b)
19
----------------------------
20
OUT Bit Vector: 11111
21
OUT Entities:
(b + c),(a * b),(a - c),(a + b),(a - b)
22
----------------------------
Unlike live variables analysis for which bit vectors of four bits are created, gdfa
has created a bit vector of ﬁve bits for available expressions analysis of our example
because our example has ﬁve expressions that qualify as local expressions. Observe
that the expressions have been numbered in a diﬀerent order compared to the order in
Figure 2.1 on page 27. This is because gdfa forms the set Expr by making a forward
pass over the program.
The initialization for available expressions analysis uses the entire Expr set which
represents the  value. The value of BI is ∅. Although basic block 2 corresponds to
block n1 for which we had chosen In as BI for initialization, for the CFG constructed
by gcc, BI is associated with the ﬁctitious blocks ENTRY and EXIT as the case may
be.
The local data ﬂow properties for available expressions analysis of our example
program for all blocks are:
© 2009 by Taylor & Francis Group, LLC

350
Data Flow Analysis: Theory and Practice
Basic Block 2. Preds:
ENTRY. Succs:
3 4
----------------------------
GEN Entities:
(b + c),(a * b)
KILL Entities:
(b + c),(a * b),(a - c),(a + b),(a - b)
----------------------------
Basic Block 3. Preds:
2. Succs:
9
----------------------------
GEN Entities:
(a - c)
KILL Entities:
(b + c),(a * b),(a + b),(a - b)
------------------------------
Basic Block 4. Preds:
2 8. Succs:
5 7
----------------------------
GEN Entities:
KILL Entities:
(b + c),(a - c)
------------------------------
Basic Block 5. Preds:
4 5. Succs:
5 6
----------------------------
GEN Entities:
(b + c),(a + b)
KILL Entities:
------------------------------
Basic Block 6. Preds:
5. Succs:
8
----------------------------
GEN Entities:
KILL Entities:
------------------------------
Basic Block 7. Preds:
4. Succs:
8
----------------------------
GEN Entities:
(a * b),(a - b)
KILL Entities:
(b + c),(a - c)
------------------------------
Basic Block 8. Preds:
6 7. Succs:
4 9
----------------------------
GEN Entities:
(a + b)
KILL Entities:
------------------------------
Basic Block 9. Preds:
3 8. Succs:
EXIT
----------------------------
GEN Entities:
(b + c),(a - c)
KILL Entities:
------------------------------
Since block 6 consists of only an unconditional goto statement, Gen6 = Kill6 = ∅.
For other block, the Gen and Kill values are same as in Example 2.9 on page 34. The
ﬁnal data ﬂow values for available expressions analysis have been shown below.
© 2009 by Taylor & Francis Group, LLC

Implementing Data Flow Analysis in GCC
351
Total Number of Iterations = 3 *********
Basic Block 2. Preds:
ENTRY. Succs:
3 4
----------------------------
IN Entities:
OUT Entities:
(b + c),(a * b)
------------------------------
Basic Block 3. Preds:
2. Succs:
9
----------------------------
IN Entities:
(b + c),(a * b)
OUT Entities:
(a - c)
------------------------------
Basic Block 4. Preds:
2 8. Succs:
5 7
----------------------------
IN Entities:
(a * b)
OUT Entities:
(a * b)
------------------------------
Basic Block 5. Preds:
4 5. Succs:
5 6
----------------------------
IN Entities:
(a * b)
OUT Entities:
(b + c),(a * b),(a + b)
------------------------------
Basic Block 6. Preds:
5. Succs:
8
----------------------------
IN Entities:
(b + c),(a * b),(a + b)
OUT Entities:
(b + c),(a * b),(a + b)
------------------------------
Basic Block 7. Preds:
4. Succs:
8
----------------------------
IN Entities:
(a * b)
OUT Entities:
(a * b),(a - b)
------------------------------
Basic Block 8. Preds:
6 7. Succs:
4 9
----------------------------
IN Entities:
(a * b)
OUT Entities:
(a * b),(a + b)
------------------------------
Basic Block 9. Preds:
3 8. Succs:
EXIT
----------------------------
IN Entities:
OUT Entities:
(b + c),(a - c)
------------------------------
We leave it for the reader to verify that these values are identical to the values
obtained in Example 2.9 on page 34.
© 2009 by Taylor & Francis Group, LLC

352
Data Flow Analysis: Theory and Practice
10.3
Implementing the Generic Data Flow Analyzer gdfa
We describe the implementation in terms of the speciﬁcation primitives, interface
with GCC, the generic functions for global property computation, and generic func-
tions for local property computation.
10.3.1
Speciﬁcation Primitives
The main data structure used for speciﬁcation is:
0 struct gimple_pfbv_dfa_spec
1 {
2
entity_name
entity;
3
initial_value
top_value_spec;
4
initial_value
entry_info;
5
initial_value
exit_info;
6
traversal_direction
traversal_order;
7
meet_operation
confluence;
8
entity_manipulation
gen_effect;
9
entity_occurrence
gen_exposition;
10
entity_manipulation
kill_effect;
11
entity_occurrence
kill_exposition;
12
dfi_to_be_preserved
preserved_dfi;
13
14
dfvalue (*forward_edge_flow)(basic_block src,
15
basic_block dest);
16
dfvalue (*backward_edge_flow)(basic_block src,
17
basic_block dest);
18
dfvalue (*forward_node_flow)(basic_block bb);
19
dfvalue (*backward_node_flow)(basic_block bb);
20
21 };
The types appearing on lines 2 to 12 are deﬁned as enumerated types with the
following possible values.
Enumerated Type
Possible Values
entity_name
entity_expr, entity_var, entity_defn
initial_value
ONES, ZEROS
traversal_direction
FORWARD, BACKWARD, BIDIRECTIONAL
meet_operation
UNION, INTERSECTION
entity_manipulation
entity_use, entity_mod
entity_occurrence
up_exp, down_exp, any_where
dfi_to_be_preserved
all, global_only, no_value
© 2009 by Taylor & Francis Group, LLC

Implementing Data Flow Analysis in GCC
353
The type dfvalue is just another name for the type sbitmap supported by GCC.
We have used a diﬀerent name to allow for the possibility of extending gdfa to other
kinds of data ﬂow values.
The entry point of each data ﬂow analysis invokes the driver with its speciﬁcation.
The driver creates space for current data ﬂow values in current data ﬂow analysis in
a variable current_pfbv_dfi which is declared as shown below:
typedef struct pfbv_dfi
{
dfvalue gen;
dfvalue kill;
dfvalue in;
dfvalue out;
} pfbv_dfi;
pfbv_dfi ** current_pfbv_dfi ;
For a basic block bb, diﬀerent members of the data ﬂow information are accessed
using the following macros:
Data ﬂow variable
current_pfbv_dfi
Given dfi
Gen
CURRENT_GEN(bb)
GEN(dfi,bb)
Kill
CURRENT_KILL(bb)
KILL(dfi,bb)
In
CURRENT_IN(bb)
IN(dfi,bb)
Out
CURRENT_OUT(bb)
OUT(dfi,bb)
Now we can describe the default functions that can be assigned to the function
pointers on lines 14 to 19 in struct gimple_pfbv_dfa_spec. Alternatively, the
users can deﬁne their own functions which have the same interface. The default
functions supported by gdfa are:
Function
Returned value
identity_forward_edge_flow(src, dest)
CURRENT_OUT(src)
identity_backward_edge_flow(src, dest)
CURRENT_IN(dest)
stop_flow_along_edge(src, dest)
top_value
identity_forward_node_flow(bb)
CURRENT_IN(bb)
identity_backward_node_flow(bb)
CURRENT_OUT(bb)
stop_flow_along_node(bb)
top_value
forward_gen_kill_node_flow(bb)
CURRENT_GEN(bb) ∪
( CURRENT_IN(bb) -
CURRENT_KILL(bb) )
backward_gen_kill_node_flow(bb)
CURRENT_GEN(bb) ∪
( CURRENT_OUT(bb) -
CURRENT_KILL(bb) )
© 2009 by Taylor & Francis Group, LLC

354
Data Flow Analysis: Theory and Practice
where top_value is of the type initial_value and is constructed based on the
value of top_value_spec (line 3 in struct gimple_pfbv_dfa_spec).
This completes the description of the speciﬁcation primitives.
10.3.2
Interface with GCC
The top level interface of gdfa with GCC is through the pass manager as described in
Section 10.1.1. At the lower level, gdfa uses the support provided by GCC for traver-
sals over CFGs, basic blocks etc.; discovering relevant features of statements, expres-
sions, variables etc.; constructing and manipulating data ﬂow values; and printing
entities appearing in statements.
Traversal Over CFG and Basic Blocks
In a round-robin iterative traversal, the basic blocks in a CFG are usually visited in
the order of along control ﬂow or against the order of control ﬂow. In GCC, this is
achieved as follows:
basic_block bb;
FOR_EACH_BB_FWD(ENTRY_BLOCK_PTR)
{
/* process bb */
}
FOR_EACH_BB_BKD(EXIT_BLOCK_PTR)
{
/* process bb */
}
In the above code, basic_block is a type supported by GCC. ENTRY_BLOCK_PTR
and EXIT_BLOCK_PTR point to ENTRY and EXIT blocks of the current function being
compiled. These macros have been deﬁned by GCC. The two other macros used
above are deﬁned as follows:
#define FOR_EACH_BB_FWD(entry_bb)
\
for(bb=entry_bb->next_bb;
\
bb->next_bb!=NULL;
\
bb=bb->next_bb)
#define FOR_EACH_BB_BKD(exit_bb)
\
for(bb=exit_bb->prev_bb;
\
bb->prev_bb!=NULL;
\
bb=bb->prev_bb)
Given a basic block bb, its predecessor and successor blocks are traversed using
an edge_iterator variable, an edge variable, and the macro FOR_EACH_EDGE as
described below. All these are directly supported by GCC.
© 2009 by Taylor & Francis Group, LLC

Implementing Data Flow Analysis in GCC
355
edge_iterator ei ;
edge e ;
basic_block succ_bb, pred_bb;
FOR_EACH_EDGE(e,ei,bb->preds)
{
pred_bb = e->src;
/* process the predecessor pred_bb */
}
FOR_EACH_EDGE(e,ei,bb->succs)
{
succ_bb = e->dest;
/* process successor succ_bb */
}
A statement is of the type tree. Further, all entities appearing in a statement
are also of the type tree. All statements in a basic block can be traversed using a
block_statement_iterator variable.
basic_block bb;
block_stmt_iterator bsi;
tree stmt;
FOR_EACH_STMT_FWD
{
stmt = bsi_stmt(bsi);
/* process stmt */
}
FOR_EACH_STMT_BKD
{
stmt = bsi_stmt(bsi);
/* process stmt */
}
The macros used in the above code are deﬁned as follows:
#define FOR_EACH_STMT_FWD
\
for(bsi=bsi_start(bb);
\
!bsi_end_p(bsi);
\
bsi_next(&bsi))
#define FOR_EACH_STMT_BKD
\
for(bsi=bsi_last(bb);
\
bsi.tsi.ptr!=NULL;
\
bsi_prev(&bsi))
© 2009 by Taylor & Francis Group, LLC

356
Data Flow Analysis: Theory and Practice
Discovering the Entities in a Statement
Statements can be of many types but only a few types are relevant to local data ﬂow
analysis. The lvalue and rvalue of a given statement stmt are of the type tree and
are extracted as shown below:
tree expr=NULL, lval=NULL;
switch(TREE_CODE(stmt))
{
case COND_EXPR:
expr = TREE_OPERAND(stmt,0);
break;
case MODIFY_EXPR:
lval = TREE_OPERAND(stmt,0);
expr = TREE_OPERAND(stmt,1);
case GIMPLE_MODIFY_STMT:
lval = GIMPLE_STMT_OPERAND(stmt,0);
expr = GIMPLE_STMT_OPERAND(stmt,1);
break;
default:
break;
}
The operands of relevant expressions are extracted as shown below:
tree op0=NULL, op1=NULL;
switch(TREE_CODE(expr))
{
case MULT_EXPR:
case PLUS_EXPR:
case MINUS_EXPR:
case LT_EXPR:
case LE_EXPR:
case GT_EXPR:
case GE_EXPR:
case NE_EXPR:
case EQ_EXPR:
op1 = TREE_OPERAND(stmt,1);
op0 = TREE_OPERAND(stmt,0);
break;
default:
break;
}
Observe that this covers the set of expressions that is currently supported by gdfa.
© 2009 by Taylor & Francis Group, LLC

Implementing Data Flow Analysis in GCC
357
Clearly, extending this set is easy.
Local variables are discovered by traversing cfun->unexpanded_var_list us-
ing TREE_VALUE and TREE_CHAIN macros supported by GCC. Here cfun represents
the current function being compiled.
tree var,list;
list = cfun->unexpanded_var_list;
while (list)
{
var = TREE_VALUE (list);
/* process variables *
list = TREE_CHAIN(list);
}
Discovering deﬁnitions is easy: A statement with TREE_CODE as MODIFY_EXR or
GIMPLE_MODIFY_STMT is detected as a deﬁnition.
Constructing and Manipulating Data Flow Values
We deﬁne the type dfvalue as follows:
typedef sbitmap dfvalue;
sbitmap is a type supported by GCC to represent sets. We use the following
sbitmap functions to construct and manipulate bitmaps. Note that these functions
are not directly used in gdfa. Instead, gdfa code calls dfvalue functions that are
deﬁned in terms of these functions.
Name of the Function
Action
sbitmap_equal(v_a,v_b)
is v_a equal to v_b?
sbitmap_a_and_b(t, v_a, v_b)
t = v_a ∩v_b
sbitmap_union_of_diff(t, v_a, v_b, v_c)
t = v_a ∪( v_b −v_c)
sbitmap_a_or_b(t, v_a, v_b)
t = v_a ∪v_b
sbitmap_ones(v)
set every bit in v to 1
sbitmap_zero(v)
set every bit in v to 0
sbitmap_alloc(n)
allocate a bitmap of n bits
sbitmap_free(v)
free the space occupied by v
Facilities for Printing Entities
We use the function dump_sbitmap to print bitmaps. For printing a statement, the
function print_generic_stmt is used whereas function print_generic_expr
prints an expression expr.
© 2009 by Taylor & Francis Group, LLC

358
Data Flow Analysis: Theory and Practice
10.3.3
The Preparatory Pass
Before the gdfa driver is invoked, some preparatory work has to be performed by an
earlier pass. The top level function of this pass is:
static unsigned int
init_gimple_pfbvdfa_execute (void)
{
local_var_count=0;
local_expr_count=0;
number_of_nodes = n_basic_blocks+2;
assign_indices_to_var();
assign_indices_to_exprs();
assign_indices_to_defns();
dfs_ordered_basic_blocks = NULL;
dfs_numbering_of_bb();
return 0;
}
Function assign_indices_to_varassigns a unique index to each local variable
by traversing cfun->unexpanded_var_list as explained in Section 10.3.2. These
indices represent the bit position of a local variable. This requires adding an integer
ﬁeld to the tree data structure. The variables which are not interesting are assigned
index -1. Function assign_indices_to_defns assigns a unique index to each
statement that is a deﬁnition.
Function assign_indices_to_exprsassigns a unique index to each expression
whose operands are restricted to constants and variables that have been assigned a
valid index. These indices represent the bit position of relevant expressions. Other
expressions are assigned index -1. Unlike local variables, there is no ready list of
expressions. Hence function assign_indices_to_exprs traverses the CFG visit-
ing each statement and examining the expressions appearing in relevant statements.
If the expression used in a statement qualiﬁes as a local expression, it is ﬁrst checked
whether an index has already been assigned to it. This could happen because an
expression could appear multiple times in a program.
Finally, function dfs_numbering_of_bb performs depth ﬁrst numbering of the
blocks in a CFG.
10.3.4
Local Data Flow Analysis
In production compilers, implementing global data ﬂow analyzers is much easier
compared to implementing local data ﬂow analyzers. This is because local data
ﬂow analysis has to deal with the lower level intricate details of the intermediate
© 2009 by Taylor & Francis Group, LLC

Implementing Data Flow Analysis in GCC
359
representation and intermediate representation are the most complex data structures
in practical compilers. Global data ﬂow analyzers are insulated from these lower
level details; they just need to know CFGs in terms of basic blocks. Thus most data
ﬂow analysis engines require the local property computation to be implemented by
the user of the engine.
This situation can change considerably if we view local data ﬂow analysis as a
special case of global data ﬂow analysis. The objective of local data ﬂow analysis
is to compute Genn and Killn of a block n. This computation can be performed by
traversing statements in block n in a manner similar to traversing blocks in a CFG.
The only diﬀerence is that statements in a block cannot have multiple predecessors
of successors.
The way InStart (or OutEnd) is computed by incorporating the eﬀect of blocks in a
CFG, Genn and Killn can also be computed by incorporating the eﬀects of individual
statements in block n. The eﬀect of statement s can be deﬁned in terms of Gens and
Kills. However, we need to overcome the following conceptual diﬃculty: When we
compute Genn for block n, Gens of a statement s must be added to the cumulative
eﬀect of the statements processed so far. However, when we compute Killn, Kills of
statement s should be added to the cumulative eﬀect instead of being removed. This
deviates from the normal meaning of Kill which represents the entities to be removed.
We overcome this conceptual diﬃculty by renaming Gens and Kills as Adds and
Removes respectively. Now local data ﬂow analysis does not depend on knowing
whether the data ﬂow property being computed is Genn or Killn. Given a local prop-
erty speciﬁcation such as below:
typedef struct lop_specs
{
entity_name entity;
entity_manipulation stmt_effect;
entity_occurrence exposition;
} lp_specs;
Local data ﬂow analysis searches for the eﬀect of a given statement speciﬁed
through stmt_effectand stores it in add_entities. If the speciﬁed stmt_effect
is entity_use, the entities that qualify for entity_mod are stored in the variable
remove_entities. Depending upon the exposition, the ﬁnal decision of removal
is taken.
Thus computation of Genn and Killn depends upon setting up a variable of the type
lp_specs and the solving the following recurrence
accumulated_entities = (accumulated_entities −remove_entities)
∪add_entities
Function effect_of_a_statement performs the above computation for a given
© 2009 by Taylor & Francis Group, LLC

360
Data Flow Analysis: Theory and Practice
statement. It is called by the top level function local_dfa_of_bb. The relevant
code fragment for downwards exposed entities is:
FOR_EACH_STMT_FWD
{
stmt = bsi_stmt(bsi);
accumulated_entities = effect_of_a_statement(lps_given,
stmt, accumulated_entities);
}
For upwards exposed entities, the accumulation is against the control ﬂow and the
above traversal is performed using the macro FOR_EACH_STMT_BKD.
The main limitation of this approach is that it requires independent traversal of a
basic block for computing Gen and Kill. However, by using a slightly more compli-
cated data structure that passes both Gen and Kill to function local_dfa_of_bb,
will solve this problem. The other limitation is that due to the generality, there are
many checks that are done in the underlying functions. There are two possible solu-
tions to this problem of eﬃciency:
• This is used as a rapid prototyping tool for a given data ﬂow analysis. Once
the details are ﬁxed, one could spend time writing a more eﬃcient data ﬂow
analyzer.
• Instead of interpreting the speciﬁcations, a program can generate a customized
C code that is compiled with GCC source.
10.3.5
Global Data Flow Analysis
As observed earlier, implementation of global data ﬂow analyzer is much simpler
once local data ﬂow analysis and interface with the underlying compiler infrastruc-
ture is in place. The fact that gdfa use generic data ﬂow Equations (5.1) and (5.2)
makes it possible to execute a wide variety of speciﬁcations without having to know
the name of a particular analysis being performed. In other words, gdfa driver is not
a collection of data ﬂow analysis implementations but is capable of executing any
speciﬁcation within the limits of the possible values of speciﬁcation primitives.
At the top level, the gdfa driver needs to perform the following tasks:
• Create special values like  , BIStart, and BIEnd.
• Create space for data ﬂow values
• Perform local data ﬂow analysis
• Select ﬂow functions
• Perform global data ﬂow analysis
© 2009 by Taylor & Francis Group, LLC

Implementing Data Flow Analysis in GCC
361
Function gdfa_driver performs the above tasks:
0 pfbv_dfi **
1 gdfa_driver(struct gimple_pfbv_dfa_spec dfa_spec)
2 {
3
if (find_entity_size(dfa_spec) == 0)
4
return NULL;
5
initialize_special_values(dfa_spec);
6
create_dfi_space();
7
traversal_order = dfa_spec.traversal_order;
8
confluence = dfa_spec.confluence;
9
10
local_dfa(dfa_spec);
11
12
forward_edge_flow = dfa_spec.forward_edge_flow;
13
backward_edge_flow = dfa_spec.backward_edge_flow;
14
forward_node_flow = dfa_spec.forward_node_flow;
15
backward_node_flow = dfa_spec.backward_node_flow;
16
17
perform_pfbvdfa();
18
19
preserve_dfi(dfa_spec.preserved_dfi);
20
return current_pfbv_dfi;
21 }
Lines 12 to 15 select the ﬂow functions from the speciﬁcations. Below we show
the code fragment of function perform_pfbvdfa when the direction of traversal is
FORWARD.
do
{
iteration_number++;
change = false;
FOR_EACH_BB_IN_SPECIFIED_TRAVERSAL_ORDER
{
bb = VARRAY_BB(dfs_ordered_basic_blocks,visit_bb);
if(bb)
{
if (traversal_order == FORWARD)
{
change_at_in = compute_in_info(bb);
change_at_out = compute_out_info(bb);
change = change||change_at_out||change_at_in;
}
else
/* compute in the opposite order */
}
}
} while(change);
© 2009 by Taylor & Francis Group, LLC

362
Data Flow Analysis: Theory and Practice
The main code fragment of function compute_in_info is as shown below. It
calls function backward_node_flow which is extracted from the speciﬁcation.
if (!bb->preds)
temp = combine(entry_info, backward_node_flow(bb));
else
temp = combine(combined_forward_edge_flow(bb),
backward_node_flow(bb));
old = CURRENT_IN(bb);
change = is_new_info(temp,old);
if (change)
{
CURRENT_IN(bb) = temp;
if (old)
free_dfvalue_space(old);
}
return change;
Function combined_forward_edge_flow computes the following term
p∈pred(n)
−→
f p→n(Out p)
Its main code fragment is shown below. It calls function forward_edge_flow
which is extracted from the speciﬁcation.
edge_vec = bb->preds;
temp = make_initialized_dfvalue(top_value_spec);
if (forward_edge_flow == &stop_flow_along_edge)
return temp;
FOR_EACH_EDGE(e,ei,edge_vec)
{
pred_bb = e->src;
new = combine(temp,forward_edge_flow(pred_bb,bb));
if (temp)
free_dfvalue_space(temp);
temp = new;
}
return temp;
The code sequence corresponding to function compute_out_info is an exact
dual of the above code sequence. This completes the description of generic global
data ﬂow analysis in gdfa.
© 2009 by Taylor & Francis Group, LLC

Implementing Data Flow Analysis in GCC
363
10.4
Extending the Generic Data Flow Analyzer gdfa
Many extensions and enhancements of gdfa are possible. We suggest some of them
by dividing them into the following categories.
• Extensions that do not require changing the architecture of gdfa.
– Include space and time measurement of analyses.
– Consider scalar formal parameters for analysis.
– Support a work list based driver.
– Extend gdfa to support other entities such as statements (e.g., for data
ﬂow analysis based program slicing), and basic blocks (e.g., for data
ﬂow analysis based dominator computation). Both these problems are
bit vector problems.
– Improve the implementation of gdfa to make it more space and time ef-
ﬁcient. This may require compromising on the simplicity of the imple-
mentation but generality should not be compromised.
• Extensions that may require minor changes to the architecture of gdfa.
– Implement incremental data ﬂow analysis and measure its eﬀectiveness
by invoking in just before gimple is expanded into RTL.
This would require a variant of a work list based driver.
– Explore the possibility of extending gdfa to the data ﬂow frameworks
where data ﬂow information can be represented using bit vectors but
the frameworks are not bit vector frameworks because they are non-
separable e.g., faint variables analysis, possibly undeﬁned variables, anal-
ysis, strongly live variables analysis.
This would require changing the local data ﬂow analysis. One possible
option is using matrix based local property computation [53]. The other
option is to treat a statement as an independent basic block.
• Extensions that may require major changes to the architecture of gdfa.
– Extend gdfa to non-separable frameworks in which data ﬂow information
cannot be represented by bit vectors e.g., constant propagation, signs
analysis, points-to analysis, alias analysis, heap reference analysis etc.
Although the main driver would remain same, this would require making
fundamental changes to the architecture.
– Extend gdfa to support some variant of context and ﬂow sensitive inter-
procedural data ﬂow analysis.
© 2009 by Taylor & Francis Group, LLC

© 2009 by Taylor & Francis Group, LLC

A
An Introduction to GCC
A.1
About GCC
GCC is an acronym for GNU Compiler Collection (http://gcc.gnu.org) which
is the de-facto standard compiler generation framework for a number on GNU/Linux
and many other variants of Unix/Linux on a wide variety of machines and is one of
the most dominant softwares in the free software community.
GCC started as C compiler, and was the acronym for GNU C Compiler in the early
days. Over the years, it has been continuously upgraded to support a number of back
end machines. Similarly, on the front end side, it has grown to support a number of
front end languages like C++, Objective C, Java, and FORTRAN to name a few. As
a consequence, it has been renamed as GNU Compiler Collection.
As of 2008, GCC supports six front end languages, and 34 back end machines.
This results in a quite huge code base, and GCC has earned a reputation of being
one of the most complex and major free software/open source projects. A rough line
count of all the C source ﬁles (including header ﬁles) of just the compiler code (i.e.,
only the gcc directory) with the major block comments removed is about 1440336
lines. This does not include the code that describes the supported back end machines,
as well as code for other purposes like the build system, libraries etc.
GCC generates production quality optimizing compilers from descriptions of tar-
get platforms. It supports a wide variety of source languages and target machines
(including operating system speciﬁc variants) in a ready-to-deploy form. Besides,
new machines can be added by describing instruction set architectures and some
other information e.g., calling conventions. This retargetability requirement implies
that target information is incorporated into the compiler at build time rather than at
design time. The GCC sources consist of
• Language dependent front end.
• Language and target independent modules.
• Target architecture speciﬁcation.
A compiler for speciﬁc language-target pair is generated by selecting front end for
desired language and generating back end for speciﬁed target.
365
© 2009 by Taylor & Francis Group, LLC

366
Data Flow Analysis: Theory and Practice
A.2
Building GCC
There are four directories that are useful to describe the user level building of GCC.
They are not required to be deﬁned in practice.
• The directory where we have downloaded the compressed sources. We denote
this by $DOWNLOADDIR
• The directory where we extract the downloaded sources. We denote this by
$GCCHOME
• The directory where we build the compiler for the chosen source language and
target machine. We denote this by $BUILDDIR
• The directory where the built compiler is installed for use. We denote this by
$INSTALLDIR
The GCC build instructions in $GCCHOME/INSTALL/index.html recommend the
use of a distinct build directory and discourages building GCC in $GCCHOME. Any
directory with suitable permissions that is diﬀerent from $GCCHOME may be used.
The binaries, libraries, headers and documentation that is built is installed as a
directory tree under $INSTALLDIR. This is any convenient directory with suitable
permissions, and usually distinct from the others. The default is a system wide in-
stallation directory e.g., /usr/local, but can be speciﬁed when GCC is conﬁgured
for building.
There are four steps to building the compiler.
• change to the $BUILDDIR,
• conﬁgure the pristine GCC sources,
• build the compiler binaries, libraries etc., and
• install the compiler.
In the description below, unless otherwise stated, we assume a GNU/Linux system
running on an i386 with the GNU Bourne Again SHell (bash) as the command shell.
All commands are issued at the bash shell prompt, and shell commands or scripts are
bash scripts.
Conﬁguring GCC
The pristine GCC sources must be informed about some details like the system on
which it will eventually run. A shell script called configure is used for this. Most
pieces of required information have reasonable default values, and the usual way is
to simply issue the configure command, which uses the defaults. However, spe-
ciﬁc non default values can be given to the configure command through command
© 2009 by Taylor & Francis Group, LLC

An Introduction to GCC
367
line switches. Being a retargetable compiler that supports a number of high level
languages (HLLs), the sources need to be informed about the particular source lan-
guage and the target hardware on which the built compiler is to be used. By default,
GCC is conﬁgured to build a compiler for the target on which it is being compiled. If
a compiler for a speciﬁc language is desired, then the switch --enable-languages
can be used. The install directory defaults to /usr/local, but can also be speciﬁed
using the --prefix switch. The conﬁgure --help command lists out various such
options whose details are documented in $GCCHOME/INSTALL/index.html.
To build only a C compiler for a i386 for running on a GNU/Linux operating
system and /home/gcc/gcc-trial-install as the installation directory, we con-
ﬁgure as follows:
1. Change to the build directory
cd $BUILDDIR
2. Specify that we need only the C compiler, to run on an i386 machine running
GNU/Linux and /home/gcc/gcc-trial-install as the installation direc-
tory (each option is shown on a separate line for clarity, but is one single
command line)
$GCCHOME/configure
--enable-languages=c
--target=i386-linux-gnu
--prefix=/home/gcc/gcc-trial-install
The configure program makes a number of checks for a successful build and
generates a Makefile (as $BUILDDIR/Makefile) if all checks pass. It is useful to
redirect the output of configure to some ﬁle for later study as follows:
$GCCHOME/configure > configure.log 2> configure.errors
Compiling GCC
Once the conﬁguration successfully generates the Makefile, the GCC source is
compiled by issuing the make command. The steps are:
1. cd $BUILDDIR
2. make
Compiling GCC involves building the compiler for each source language, the driver
program gcc, the associated header ﬁles, support libraries, and the documentation.
The driver program gcc so generated is the command that users use to compile their
source programs. The driver takes the user’s source ﬁle to be compiled and invokes
a sequence of programs (the compiler, the assembler and the linker) that generate its
binary.
It is useful to redirect the output of make to some ﬁle for later study as follows:
$BUILDDIR/make > make.log 2> make.errors
© 2009 by Taylor & Francis Group, LLC

368
Data Flow Analysis: Theory and Practice
Installing GCC
Final installation installs various components of the compiler like the driver, the
compiler, libraries, the documentation etc., in a well-deﬁned directory structure in
the $INSTALLDIR directory. The following structure is typically used:
• $INSTALLDIR/bin: Directory where the various executables are installed.
• $INSTALLDIR/include: Directory where the various headers are installed.
• $INSTALLDIR/lib: Directory where the various libraries are installed.
• $INSTALLDIR/man: Directory where the various online manual pages are in-
stalled.
• $INSTALLDIR/info: Directory where the various online info pages are in-
stalled.
To install the built sources, use the following command:
$BUILDDIR/make install
It is useful to redirect the output of install to some ﬁle for later study as follows:
$BUILDDIR/make install > install.log 2> install.errors
Downloading and Installing gdfa
A patch of GCC 4.3.0 for gdfa is available at the following URL. Patches for later
versions will be made available on this page whenever possible.
http://www.cse.iitb.ac.in/uday/dfaBook-web
Following steps patch up GCC with gdfa code.
1. cd $GCCHOME
2. patch -p0 < patch_file_with_path
Now GCC can be conﬁgured, compiled, and installed.
A.3
Further Readings in GCC
Here we list further resources for learning about GCC.
© 2009 by Taylor & Francis Group, LLC

An Introduction to GCC
369
• GCC Internals
http://gcc.gnu.org/onlinedocs/gccint.html
This is the oﬃcial internals document which exhaustively describes most de-
tails and is a part of the documentation distributed with the compiler code.
• GCC Internals documents developed at IIT Bombay
http://www.cse.iitb.ac.in/grc/
This is the website of GCC Resource Center at IIT Bombay. It hosts the GCC
documents developed at IIT Bombay.
• The GCC Wiki
http://gcc.gnu.org/wiki/
The oﬃcial GCC Wiki pages where the various aspects of GCC, including
some description of the internals, are being developed by the GCC developers
and others.
• The GCC Internals workshop held at IIT Bombay
http://www.cse.iitb.ac.in/˜uday/gcc-workshop/
This workshop that focused mainly on the machine descriptions was held at
IIT Bombay in June 2007. The slides and some associated software is available
on the Downloads page of the workshop.
• The GCC on Wikipedia
http://en.wikipedia.org/wiki/GNU_Compiler_Collection
• The GCC Internals on Wikipedia
http://en.wikibooks.org/wiki/GNU_C_Compiler_Internals
© 2009 by Taylor & Francis Group, LLC

© 2009 by Taylor & Francis Group, LLC

References
[1] O. Agesen, D. Detlefs, and J. E. Moss. Garbage collection and local variable
type-precision and liveness in Java virtual machines. In Proceedings of the
ACM SIGPLAN Conference on Programming Language Design and Imple-
mentation, pages 269–279. ACM, 1998.
[2] A. V. Aho, J. E. Hopcroft, and J. D. Ullman. The Design and Analysis of
Computer Algorithms. Addison-Wesley, 1974.
[3] A. V. Aho, M. S. Lam, R. Sethi, and J. D. Ullman. Compilers: Principles,
Techniques, and Tools (2/e). Addison-Wesley Longman Publishing Co., Inc.,
2006.
[4] F. E. Allen. Control ﬂow analysis. ACM SIGPLAN Notices, 5(7):1–19, 1970.
[5] F. E. Allen. A basis for program optimization. In IFIP Congress (1), pages
385–390. North Holland Publishing Company, 1971.
[6] F. E. Allen.
Interprocedural data ﬂow analysis.
In Proceedings of IFIP
Congress 74, pages 398–408. North Holland Publishing Company, 1974.
[7] F. E. Allen and J. Cocke. A program data ﬂow analysis procedure. Commu-
nications of the ACM, 19(3):137–147, 1976.
[8] B. Alpern, M. N. Wegman, and F. K. Zadeck. Detecting equality of variables
in programs. In Proceedings of the ACM SIGPLAN-SIGACT Symposium on
Principles of Programming Languages, pages 1–11. ACM, 1988.
[9] L. O. Andersen. Program Analysis and Specialization for the C Programming
Language. PhD thesis, DIKU, University of Copenhagen, 1994.
[10] A. W. Appel and M. Ginsburg. Modern Compiler Implementation in C. Cam-
bridge University Press, 1998.
[11] Andrew W. Appel. SSA is functional programming. ACM SIGPLAN Notices,
33(4):17–20, 1998.
[12] J. Banning. An eﬃcient way to ﬁnd the side eﬀects of procedure calls and
aliases of variables. In Proceedings of the ACM SIGPLAN-SIGACT Sympo-
sium on Principles of Programming Languages, pages 29–41. ACM, 1979.
[13] J. M. Barth. An interprocedural data ﬂow analysis algorithm. In Proceedings
of the ACM SIGPLAN-SIGACT Symposium on Principles of Programming
Languages, pages 119–131. ACM, 1977.
371
© 2009 by Taylor & Francis Group, LLC

372
References
[14] Jeﬀrey M. Barth. A practical interprocedural data ﬂow analysis algorithm.
Communications of the ACM, 21(9):724–736, 1978.
[15] B. Blanchet. Escape analysis for object-oriented languages: application to
Java. In Proceedings of the ACM SIGPLAN Conference on Object-oriented
Programming Systems, Languages, and Applications, pages 20–34. ACM,
1999.
[16] B. Blanchet. Escape analysis for JavaT M: Theory and practice. ACM Trans-
actions on Programming Languages and Systems, 25(6):713–775, 2003.
[17] R. Bodik, R. Gupta, and M. L. Soﬀa. Complete removal of redundant compu-
tations. In Proceedings of the ACM SIGPLAN Conference on Programming
Language Design and Implementation, pages 1–14. ACM, 1998.
[18] P. Briggs, K. D. Cooper, T. J. Harvey, and L. T. Simpson. Practical improve-
ments to the construction and destruction of static single assignment form.
Software—Practice and Experience, 28(8):859–881, 1998.
[19] D. Callahan. The program summary graph and ﬂow-sensitive interprocedural
data ﬂow analysis. In Proceedings of the ACM SIGPLAN Conference on Pro-
gramming Language Design and Implementation, pages 47–56. ACM, 1988.
[20] D. Callahan, A. Carle, M. W. Hall, and K. Kennedy.
Constructing the
procedure call multigraph.
IEEE Transactions on Software Engineering,
16(4):483–487, 1990.
[21] J. Choi, M. Burke, and P. Carini.
Eﬃcient ﬂow-sensitive interprocedural
computation of pointer-induced aliases and side eﬀects. In Proceedings of
the ACM SIGPLAN-SIGACT Symposium on Principles of Programming Lan-
guages, pages 232–245. ACM, 1993.
[22] J. Choi, R. Cytron, and J. Ferrante. Automatic construction of sparse data ﬂow
evaluation graphs. In Proceedings of the ACM SIGPLAN-SIGACT Symposium
on Principles of Programming Languages, pages 55–66. ACM, 1991.
[23] J. Choi, M. Gupta, M. Serrano, V. C. Sreedhar, and S. Midkiﬀ. Escape anal-
ysis for Java. In Proceedings of the ACM SIGPLAN Conference on Object-
oriented Programming Systems, Languages, and Applications, pages 1–19.
ACM, 1999.
[24] J. Cocke. Global common subexpression elimination. ACM SIGPLAN No-
tices, 5(7):20–24, 1970.
[25] K. Cooper. Analyzing aliases of reference formal parameters. In Proceedings
of the ACM SIGPLAN-SIGACT Symposium on Principles of Programming
Languages, pages 281–290. ACM, 1985.
[26] K. D. Cooper and K. Kennedy. Fast interprocedural alias analysis. In Pro-
ceedings of the ACM SIGPLAN-SIGACT Symposium on Principles of Pro-
gramming Languages, pages 49–59. ACM, 1989.
© 2009 by Taylor & Francis Group, LLC

Data Flow Analysis: Theory and Practice
373
[27] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to
Algorithms, (2/e). The MIT Press and McGraw-Hill Book Company, 2001.
[28] R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and F. K. Zadeck.
Eﬃciently computing static single assignment form and the control depen-
dence graph. ACM Transactions on Programming Languages and Systems,
13(4):451–490, 1991.
[29] B. A. Davey and H. A. Priestley. Introduction to Lattices and Order (2/e).
Cambridge University Press, 2002.
[30] D. M. Dhamdhere and U. P. Khedker. Complexity of bidirectional data ﬂow
analysis. In Proceedings of the ACM SIGPLAN-SIGACT Symposium on Prin-
ciples of Programming Languages, pages 397–408. ACM, 1993.
[31] D. M. Dhamdhere, B. K. Rosen, and F. K. Zadeck. How to analyze large
programs eﬃciently and informatively. In Proceedings of the ACM SIGPLAN
Conference on Programming Language Design and Implementation, pages
212–223. ACM, 1992.
[32] E. Duesterwald, R. Gupta, and M. L. Soﬀa. Demand-driven computation of
interprocedural data ﬂow. In Proceedings of the ACM SIGPLAN-SIGACT
Symposium on Principles of Programming Languages, pages 37–48. ACM,
1995.
[33] E. Duesterwald, R. Gupta, and M. L. Soﬀa.
A practical framework for
demand-driven interprocedural data ﬂow analysis. ACM Transactions on Pro-
gramming Languages and Systems, 19(6):992–1030, 1997.
[34] M. Emami, R. Ghiya, and L. J. Hendren. Context-sensitive interprocedural
points-to analysis in the presence of function pointers. In Proceedings of the
ACM SIGPLAN Conference on Programming Language Design and Imple-
mentation, pages 242–256. ACM, 1994.
[35] M. F¨ahndrich, J. S. Foster, Z. Su, and A. Aiken. Partial online cycle elimi-
nation in inclusion constraint graphs. In Proceedings of the ACM SIGPLAN
Conference on Programming Language Design and Implementation, pages
85–96. ACM, 1998.
[36] M. F¨ahndrich, J. Rehof, and M. Das. Scalable context-sensitive ﬂow analysis
using instantiation constraints. In Proceedings of the ACM SIGPLAN Confer-
ence on Programming Language Design and Implementation, pages 253–263.
ACM, 2000.
[37] S. Graham and M. Wegman. A fast and usually linear algorithm for global
data ﬂow analysis. Journal of ACM, 23(1):172–202, 1976.
[38] D. Grove and C. Chambers. A framework for call graph construction al-
gorithms.
ACM Transactions on Programming Languages and Systems,
23(6):685–746, 2001.
© 2009 by Taylor & Francis Group, LLC

374
References
[39] D. Grove and L. Torczon. Interprocedural constant propagation: a study of
jump function implementation. In Proceedings of the ACM SIGPLAN Confer-
ence on Programming Language Design and Implementations, pages 90–99.
ACM, 1993.
[40] D. Grune, H. E. Bal, C. J. H. Jacobs, and K. G. Langendoen. Modern Com-
piler Design. John Wiley & Sons, 2000.
[41] S. Hack. Register Allocation for Programs in SSA Form. PhD thesis, Univer-
sit¨at Karlsruhe, 2007.
[42] S. Hack, D. Grund, and G. Goos. Register allocation for programs in SSA-
form. In Proceedings of the International Conference on Compiler Construc-
tion, pages 247–262. Springer-Verlag, 2006.
[43] M. W. Hall and K. Kennedy. Eﬃcient call graph analysis. ACM Letters on
Programming Languages and Systems, 1(3):227–242, 1992.
[44] M. S. Hecht. Flow Analysis of Computer Programs. Elsevier North-Holland
Inc., 1977.
[45] M. S. Hecht and J. D. Ullman. Flow graph reducibility. In Proceedings of the
ACM Symposium on Theory of Computing, pages 238–250. ACM, 1972.
[46] M. S. Hecht and J. D. Ullman. Characterization of reducible ﬂow graphs.
Journal of ACM, 21(3):367–375, 1974.
[47] M. Hind. Pointer analysis: haven’t we solved this problem yet?
In Pro-
ceedings of the ACM SIGPLAN-SIGSOFT Workshop on Program Analysis
for Software Tools and Engineering, pages 54–61. ACM, 2001.
[48] M. Hind, M. Burke, P. Carini, and J. Choi. Interprocedural pointer alias analy-
sis. ACM Transactions on Programming Languages and Systems, 21(4):848–
894, 1999.
[49] J. B. Kam and J. D. Ullman. Global data ﬂow analysis and iterative algo-
rithms. Journal of ACM, 23(1):158–171, 1976.
[50] J. B. Kam and J. D. Ullman. Monotone data ﬂow analysis frameworks. Acta
Informatica, 7(3):305–317, 1977.
[51] A. Kanade, U. P. Khedker, and A. Sanyal. Heterogeneous ﬁxed points with
application to points-to analysis.
In Proceedings of the Asian Symposium
on Programming Languages and Systems, pages 298–314. Springer-Verlag,
2005.
[52] A. Karkare, A. Sanyal, and U. P. Khedker. Eﬀectiveness of garbage collection
in MIT/GNU Scheme. CoRR, abs/cs/0611093, 2006.
[53] B. Karkare. Complexity and Eﬃciency Issues in Data Flow Analysis. PhD
thesis, Indian Institute of Technology, Bombay, 2007.
© 2009 by Taylor & Francis Group, LLC

Data Flow Analysis: Theory and Practice
375
[54] B. Karkare and U. P. Khedker. An improved bound for call-strings based
interprocedural analysis of bit vector frameworks. ACM Transactions on Pro-
gramming Languages and Systems, 29(6):38, 2007.
[55] K. Kennedy. A global ﬂow analysis algorithm. International Journal of Com-
puter Mathematic, 3(1):5–15, 1971.
[56] K. Kennedy. Node listings applied to data ﬂow analysis. In Proceedings
of the ACM SIGPLAN-SIGACT Symposium on Principles of Programming
Languages, pages 10–21. ACM, 1975.
[57] K. Kennedy. A survey of data ﬂow analysis techniques, 1981. In [77].
[58] R. Kennedy, S. Chan, S. Liu, R. Lo, P. Tu, and F. Chow. Partial redundancy
elimination in SSA form. ACM Transactions on Programming Languages
and Systems, 21(3):627–676, 1999.
[59] U. P. Khedker. Data ﬂow analysis. In Y. N. Srikant and Priti Shankar, editors,
The Compiler Design Handbook: Optimizations & Machine Code Genera-
tion. CRC Press, 2002.
[60] U. P. Khedker and D. M. Dhamdhere. A generalized theory of bit vector data
ﬂow analysis. ACM Transactions on Programming Languages and Systems,
16(5):1472–1511, 1994.
[61] U. P. Khedker and B. Karkare. Eﬃciency, precision, simplicity, and gen-
erality in interprocedural data ﬂow analysis: Resurrecting the classical call
strings method. In Proceedings of the International Conference on Compiler
Construction, pages 213–228. Springer-Verlag, 2008.
[62] U. P. Khedker, A. Sanyal, and A. Karkare. Heap reference analysis using
access graphs. ACM Transactions on Programming Languages and Systems,
30(1):1, 2007.
[63] G. Kildall. A uniﬁed approach to global program optimization. In Proceed-
ings of the ACM SIGPLAN-SIGACT Symposium on Principles of Program-
ming Languages, pages 194–206. ACM, 1973.
[64] K. Knobe and V. Sarkar. Array SSA form and its use in parallelization. In
Proceedings of the ACM SIGPLAN-SIGACT Symposium on Principles of Pro-
gramming Languages, pages 107–120. ACM, 1998.
[65] J. Knoop, O. R¨uthing, and B. Steﬀen. Lazy code motion. In Proceedings
of the ACM SIGPLAN Conference on Programming Language Design and
Implementation, pages 224–234. ACM, 1992.
[66] W. Landi and B. G. Ryder. A safe approximate algorithm for interprocedu-
ral pointer aliasing. In Proceedings of the ACM SIGPLAN Conference on
Programming Language Design and Implementation, pages 235–248. ACM,
1992.
© 2009 by Taylor & Francis Group, LLC

376
References
[67] W. Landi, B. G. Ryder, and S. Zhang. Interprocedural side eﬀect analysis with
pointer aliasing. In Proceedings of the ACM SIGPLAN Conference on Pro-
gramming Language Design and Implementation, pages 56–67. ACM, 1993.
[68] T. Lengauer and R. E. Tarjan. A fast algorithm for ﬁnding dominators in
a ﬂowgraph. ACM Transactions on Programming Languages and Systems,
1(1):121–141, 1979.
[69] O. Lhot´ak and L. Hendren. Context-sensitive points-to analysis: is it worth
it? In Proceedings of the International Conference on Compiler Construction,
pages 47–64. Springer-Verlag, 2006.
[70] E. S. Lowry and C. W. Medlock. Object code optimization. Communications
of the ACM, 12(1):13–22, 1969.
[71] T. J. Marlowe and B. G. Ryder. Properties of data ﬂow frameworks. Acta
Informatica, 28(2):121–163, 1990.
[72] F. Martin. Experimental comparison of call string and functional approaches
to interprocedural analysis. In Proceedings of the International Conference
on Compiler Construction, pages 63–75. Springer-Verlag, 1999.
[73] C. E. McDowell.
Reducing garbage in java.
ACM SIGPLAN Notices,
33(9):84–86, 1998.
[74] E. Morel and C. Renvoise.
Global optimization by suppression of partial
redundancies. Communications of the ACM, 22(2):96–103, 1979.
[75] R. Morgan. Building an Optimizing Compiler. Digital Press, 1998.
[76] S. S. Muchnick. Advanced Compiler Design and Implementation. Morgan
Kaufmann Publishing Co., 1997.
[77] S. S. Muchnick and N. D. Jones. Program Flow Analysis : Theory and Appli-
cations. Prentice-Hall Inc., 1981.
[78] M. M¨uller-Olm and O. R¨uthing. On the complexity of constant propagation.
In Proceedings of the European Symposium on Programming Languages and
Systems, pages 190–205. Springer-Verlag, 2001.
[79] E. M. Myers. A precise inter-procedural data ﬂow algorithm. In Proceedings
of the ACM SIGPLAN-SIGACT Symposium on Principles of Programming
Languages, pages 219–230. ACM, 1981.
[80] F. Nielson, H. R. Nielson, and C. Hankin. Principles of Program Analysis.
Springer-Verlag, 1998.
[81] A. Reid, J. McCorquodale, J. Baker, W. Hsieh, and J. Zachary. The need
for predictable garbage collection.
In Proceedings of the ACM SIGPLAN
Workshop on Compiler Support for System Software. ACM, 1999.
[82] T. W. Reps, S. Horwitz, and S. Sagiv. Precise interprocedural dataﬂow anal-
ysis via graph reachability. In Proceedings of the ACM SIGPLAN-SIGACT
© 2009 by Taylor & Francis Group, LLC

Data Flow Analysis: Theory and Practice
377
Symposium on Principles of Programming Languages, pages 49–61. ACM,
1995.
[83] S. E. Richardson and M. Ganapathi. Interprocedural optimizations : Experi-
mental results. Software Practice and Experience, 19(2):149–169, 1989.
[84] B. K. Rosen. Monoids for rapid data ﬂow analysis. SIAM Journal of Comput-
ing, 9(1):159–196, 1980.
[85] B. K. Rosen, M. N. Wegman, and F. K. Zadeck. Global value numbers and
redundant computations. In Proceedings of the ACM SIGPLAN-SIGACT Sym-
posium on Principles of Programming Languages, pages 12–27. ACM, 1988.
[86] B. G. Ryder and M. C. Paull. Elimination algorithms for data ﬂow analysis.
ACM Computing Surveys, 18(3):277–316, 1986.
[87] M. Sagiv, T. Reps, and S. Horwitz. Precise interprocedural dataﬂow analysis
with applications to constant propagation. Theoretical Computer Science,
167(1–2):131–170, 1996.
[88] S. Sagiv, T. W. Reps, and R. Wilhelm.
Parametric shape analysis via 3-
valued logic. ACM Transactions on Programming Languages and Systems,
24(3):217–298, 2002.
[89] R. Shaham, E. K. Kolodner, and M. Sagiv. On eﬀectiveness of GC in Java.
In International Symposium on Memory Management, pages 12–17. ACM,
2000.
[90] R. Shaham, E. K. Kolodner, and M. Sagiv. Heap proﬁling for space-eﬃcient
java.
In Proceedings of the ACM SIGPLAN Conference on Programming
Language Design and Implementation, pages 104–113. ACM, 2001.
[91] R. Shaham, E. K. Kolodner, and M. Sagiv. Estimating the impact of heap
liveness information on space consumption in Java. In Proceedings of the In-
ternational Symposium on Memory Management, pages 64–75. ACM, 2002.
[92] R. Shaham, E. Yahav, E. K. Kolodner, and S. Sagiv. Establishing local tempo-
ral heap safety properties with applications to compile-time memory manage-
ment. In Proceedings of the International Static Analysis Symposium, pages
483–503. Springer-Verlag, 2003.
[93] M. Sharir and A. Pnueli. Two approaches to interprocedural data ﬂow anal-
ysis. In S. S. Muchnick and N. D. Jones, editors, Program Flow Analysis :
Theory and Applications. Prentice-Hall Inc., 1981.
[94] T. C. Spillman.
Exposing side eﬀects in a PL/I optimizing compiler.
In
Proceedings of IFIP Congress 71, pages 376–381. North Holland Publishing
Company, 1971.
[95] V. C. Sreedhar and G. R. Gao. A linear time algorithm for placing φ-nodes.
In Proceedings of the ACM SIGPLAN-SIGACT Symposium on Principles of
Programming Languages, pages 62–73. ACM, 1995.
© 2009 by Taylor & Francis Group, LLC

Data Flow Analysis: Theory and Practice
378
[96] V. C. Sreedhar, R. Dz-Ching Ju, D. M. Gillies, and V. Santhanam. Translat-
ing out of static single assignment form. In Proceedings of the International
Symposium on Static Analysis, pages 194–210, 1999.
[97] B. Steensgaard.
Points-to analysis in almost linear time.
In Proceedings
of the ACM SIGPLAN-SIGACT Symposium on Principles of Programming
Languages, pages 32–41. ACM, 1996.
[98] R. E. Tarjan. Fast algorithms for solving path problems. Journal of ACM,
28(3):594–614, 1981.
[99] A. Tarski. A lattice-theoretical ﬁxpoint theorem and its applications. Paciﬁc
Journal of Mathematics, 5(2):285–309, 1955.
[100] J. D. Ullman. Fast algorithms for the elimination of common subexpressions.
Acta Informatica, 2(3):191–213, 1973.
[101] V. Vyssotsky and P. Wegner. A graph theoretical FORTRAN source language
analyzer. AT & T Bell Laboratories, Murray Hill, N. J., 1963. (Manuscript).
[102] M. N. Wegman and F. K. Zadeck. Constant propagation with conditional
branches.
ACM Transactions on Programming Languages and Systems,
13(2):181–210, 1991.
[103] M. N. Wegman and F. K. Zadeck. Constant propagation with conditional
branches.
ACM Transactions on Programming Languages and Systems,
13(2):181–210, 1991.
[104] J. Whaley and M. S. Lam. Cloning-based context-sensitive pointer alias anal-
ysis using binary decision diagrams. In Proceedings of the ACM SIGPLAN
Conference on Programming Language Design and Implementation. ACM,
2004.
[105] R. Wilhelm and D. Maurer. Compiler Design. Addison-Wesley, 1995.
[106] R. Wilhelm, T. W. Reps, and S. Sagiv. Shape analysis and applications. In
Y. N. Srikant and Priti Shankar, editors, The Compiler Design Handbook: Op-
timizations & Machine Code Generation, pages 175–218. CRC Press, 2002.
[107] R. P. Wilson and M. S. Lam. Eﬃcient context-sensitive pointer analysis for C
programs. In Proceedings of the ACM SIGPLAN Conference on Programming
Language Design and Implementation, pages 1–12. ACM, 1995.
378
© 2009 by Taylor & Francis Group, LLC

© 2009 by Taylor & Francis Group, LLC

© 2009 by Taylor & Francis Group, LLC


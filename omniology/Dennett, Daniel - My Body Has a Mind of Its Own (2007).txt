6
My Body Has a Mind of Its Own
Daniel C. Dennett
In life, what was it I really wanted? My own conscious and seemingly indivisible self
was turning out far from what I had imagined and I need not be so ashamed of my
self-pity! I was an ambassador ordered abroad by some fragile coalition, a bearer of
conﬂicting orders, from the uneasy masters of a divided empire. . . . As I write these
words, even so as to be able to write them, I am pretending to a unity that, deep in-
side myself, I now know does not exist.
—William Hamilton 1996, p. 134
Language was given to men so that they could conceal their thoughts.
—Charles-Maurice de Talleyrand
‘‘My body has a mind of its own!’’ Everybody knows what this exclamation
means. It notes with some surprise the fact that our bodies can manage
many of their key projects without any conscious attention on our part.
Our bodies can stride along quite irregular ground without falling, avoiding
obstacles and grabbing strategic handholds whenever available, pick berries
and get them into the mouth or the bucket with little if any attention paid,
and—notoriously—initiate preparations for sexual activity on a moment’s
notice without any elaborate decision-making or evaluation discernible, to
say nothing of the tight ship run by our temperature maintenance system
and our immune system. My body can keep life and limb together, as we
say, and even arrange for its own self-replication without any attention
from me. So what does it need me for?
This is another way, perhaps a better way, of asking why consciousness
(our human kind of consciousness, at least) should evolve at all. The sort
of consciousness (if it is a sort of consciousness) that is manifest in alert
and timely discriminations for apt guidance of bodily trajectory, posture,
and resource allocation is exhibited, uncontroversially, by invertebrates all
the way down to single-celled organisms, and even by plants.1 Since the

self-protective dispositions of the lobster can be duplicated, so far as we
can tell, in rather simple robots whose inner states inspire no conviction
that they must ‘‘generate phenomenology’’ or anything like it, the suppo-
sition that nevertheless there must be ‘‘something it is like to be’’ a lobster
begins to look suspicious, a romantic overshooting of anthropomorphism,
however generous-spirited. If a lobster can get through life without a self
(or very much of a self), why should it have a ‘selfy’ self (Dennett 1991)?
Maybe it doesn’t. Maybe it (and insects and worms and . . . .) are ‘‘just
robots.’’ However, leaving matters here leans on an unsatisfactory assump-
tion to the effect that being robotic and being conscious are mutually
exclusive. The ultimate issue, really, is to explain ﬁrst the possibility and
then the (evolutionary) actuality of consciousness in some robots (if not
lobsters). After all, we are conscious, and if materialism is true, we are
made of nothing but robots—cells—and is such a complex not itself a
robot?2 If we want to have a way of expressing our supposition that lob-
sters, say, are ‘‘mere’’ automata, we need to anchor that ‘‘mere’’ to some
upper bound on complexity.
Let’s say that mere automata are entities (organisms or automata) that are
incapable of higher order self-monitoring; while they engage in behaviors
that beneﬁt from feedback guidance, these behaviors and the states that
give rise to them aren’t further represented by the entities in such a way as
to permit reﬂective or retrospective self-evaluation. A mere automaton is
stuck with whatever behaviors it can perform—though these might be
conditionable—capable of being shaped or extinguished by feedback—but
cannot, for instance, note that ‘‘it seemed like a good idea at the time.’’
This would enable us to investigate different styles of nervous system:
from simple and relatively low-priced arrangements lacking central self-
monitoring capabilities to more expensive and sophisticated arrangements
capable of signiﬁcant varieties of higher level self-monitoring, without hav-
ing to confront in advance the vexing question of whether or not any of
these sophistications amounted to, or were sure signs of, consciousness. As
we learn more about the adroitness and versatility and internal organiza-
tion of spiders or octopuses (or bats or dolphins or bonobos), we might
uncover some further impressive thresholds that eventually persuade us
to grant consciousness (of the impressive kind—whatever that means) to
these creatures, but for the time being, we are the only species that every-
one conﬁdently characterizes as conscious. Our conﬁdence is grounded in
the simple fact that we are the only species that can compare notes.
Consider a remark by Daniel Wegner: ‘‘We can’t possibly know (let alone
keep track of) the tremendous number of mechanical inﬂuences on our be-
94
Daniel C. Dennett

havior because we inhabit an extraordinarily complicated machine’’ (2002,
p. 27; italics added).
Wegner presumably wouldn’t have written this if he hadn’t been com-
fortable assuming that we all know what he is talking about, but just who
is this we that ‘‘inhabits’’ the brain? There is the Cartesian answer: each of
us has an immortal, immaterial soul, the res cogitans or thinking thing, the
seat of our individual consciousness. But once we set that answer aside—as
just about everybody these days is eager to do—just what thing or organ or
system could Wegner be referring to? My answer, compressed into a slogan
by Giulio Giorello, who used it as a headline for an interview with me in
Corriere della Serra in 1997, is this: Si, abbiamo un anima. Ma e` fatta di tanti
piccoli robot. Yes, we have a soul. But it’s made of lots of tiny robots. Some-
how, the trillions of robotic (and unconscious) cells that compose our
bodies organize themselves into interacting systems that sustain the activ-
ities traditionally allocated to the soul, the ego or self. But since we have
already granted that simple robots are unconscious (if toasters and thermo-
stats and telephones are unconscious), why couldn’t teams of such robots
do their fancier projects without having to compose me? If the immune
system has a mind of its own, and the hand–eye coordination circuit that
picks the berries has a mind of its own, why bother making a super-mind to
supervise all this?
George Ainslie notes the difﬁculty and has a suggestion: ‘‘Philosophers
and psychologists are used to speaking about an organ of uniﬁcation called
the ‘self’ that can variously ‘be’ autonomous, divided, individuated, fragile,
well-bounded, and so on, but this organ doesn’t have to exist as such’’
(2001, p. 43).
How could this be? How could an organ that doesn’t have to exist as
such exist at all? And, again, why would it exist? Another crafty thinker
who has noticed the problem is the novelist Michael Frayn, whose narrator
in Headlong muses: ‘‘Odd, though, all these dealings of mine with myself.
First I’ve agreed a principle with myself, now I’m making out a case to my-
self, and debating my own feelings and intentions with myself. Who is this
self, this phantom internal partner, with whom I’m entering into all these
arrangements? (I ask myself.)’’ (Frayn 1999, p. 143).
Although Frayn might not have intended to answer his own question, I
think he has in fact provided the key to the answer in his parenthesis: ‘‘I
ask myself.’’ It is only when asking and answering are among the projects
undertaken by the teams of robots that they have to compose a virtual
organ of sorts, an organ that ‘‘doesn’t have to exist as such’’—but does
have to exist.
My Body Has a Mind of Its Own
95

This, in any case, has been suggestively argued by the ethologist
and roboticist David McFarland (1989) in a provocative, if obscure, essay.
According to McFarland, ‘‘Communication is the only behavior that
requires an organism to self-monitor its own control system.’’ I’ve been
musing over this essay and its implications for years, and I still haven’t
reached a stable conviction about it, but in the context of the various intri-
cate constructions around selves and their volition offered in the present
volume including the juxtaposition of Wegner’s and Ainslie’s perspectives,
I think it is worth another outing.
Organisms are correctly seen as multicellular communities sharing, for
the most part, a common fate (they’re in the same boat). So evolution can
be expected to favor cooperative arrangements in general. Your eyes may,
on occasion, deceive you—but not on purpose! (Sterelny 2003). Running
is sure to be a coordinated activity of the limbs, not a battle for supremacy.
Nevertheless, there are bound to be occasions when subsystems work at
cross purposes, even in the best-ordered communities of cells, and these
will, in general, be resolved in the slow, old-fashioned way: by the extinc-
tion of those lineages in which these conﬂicts arise most frequently, are
most costly to ﬁtness, and aren’t ineliminable byproducts that come with
the numerous gains from going multicellular in the ﬁrst place (see the
papers in part III of Hammerstein 2003). The result is control systems that
get along quite well without any internal self-monitoring. The ant colony
has no boss, and no virtual boss either, and gets along swimmingly with
distributed control that so far as we can tell does not engage or need to en-
gage in high-level self-monitoring. According to McFarland, organisms can
very effectively control themselves by a collection of competing but ‘‘myo-
pic’’ task-controllers that can interrupt each other when their conditions
(hunger or need, sensed opportunity, built-in priority ranking, . . .) out-
weigh the conditions of the currently active task controller. Goals are repre-
sented only tacitly, in the feedback loops that guide each task-controller,
but without any global or higher level representation. (One might think
of such a task-controller as ‘‘uncommented’’ code—it works, but there is
nothing anywhere in it that can be read off about what it does or why or
how it does it.) Evolution will tend to optimize the interrupt dynamics of
these modules, and nobody’s the wiser. That is, there doesn’t have to be
anybody home to be wiser!
But communication, McFarland thinks, is a behavioral innovation that
changes that. Communication requires a central clearinghouse of sorts in
order to buffer the organism from revealing too much about its current
96
Daniel C. Dennett

state to competitive organisms. In order to understand the evolution of
communication, as Dawkins and Krebs (1978) showed in a classic arti-
cle, we need to see it as manipulation rather than as purely cooperative
behavior. The organism that has no poker face, that communicates its
state directly to all hearers, is a sitting duck, and will soon be extinct.
What must evolve instead is a communication-control buffer that creates
(1) opportunities for guided deception, and coincidentally (2) opportu-
nities for self-deception (Trivers 1985), by creating, for the ﬁrst time in the
evolution of nervous systems, explicit (and more ‘‘globally’’ accessible) rep-
resentations of its current state, representations that are detachable from
the tasks they represent, so that deceptive behaviors can be formulated
and controlled. This in turn opens up structure that can be utilized in
taking the step, described in detail by Gary Drescher (1991), from simple
situation-action machines to choice machines, the step I describe as the
evolutionary transition from Skinnerian to Popperian creatures (Dennett
1996).
I wish I could spell all this out with the rigor and detail that it deserves.
Fortunately, some of the other papers in this volume make a careful collec-
tive start. The best I can do for my part, at this point anyway, is simply
gesture in the directions that strike me as theoretically promising and en-
courage others to continue mining this ﬁne vein. What follows are some
informal reﬂections that might contribute.
Consider the chess-playing computer programs that I so often discuss.
They are not conscious, even when they are playing world-class chess.
There is no role for a user-illusion within them because, like McFarland’s
well-evolved noncommunicators, they are more or less optimized to budget
their time appropriately for their various subtasks. Would anything change
if the program were enabled/required to communicate with others—either
its opponent or other kibitzers? Some programs now available have a fea-
ture that permits you to see just which move they are currently considering
(e.g., see http://chess.captain.at/), but this is not communication; this is in-
voluntary self-exposure, a shameless display that provides a huge source of
valuable information to anyone who wants to try to exploit it. In contrast,
a program that could consider its communications as informal moves,
social ploys, in the enlarged game of chess—the game that some philoso-
phers (e.g., Haugeland 1998) insist is real chess, unlike the socially trun-
cated game that programs now play—would have to be able to ‘‘look at’’
its internal states the same way a poker player needs to look at his cards in
order to decide what action to take. ‘‘What am I now trying to do, and
My Body Has a Mind of Its Own
97

what would be the effect of communicating information about that project
to this other agent?’’ (it asks itself). In other words, McFarland imports
Talleyrand’s cynical dictum about language and adapts it to reveal a deep
biological truth: explicit self-monitoring was invented to conceal our true
intentions from each other while permitting us to reveal strategic versions
of those intentions to others.
If this is roughly right, then we can also see how this capacity has two
roles to play: export and import. It is not just that we can use communica-
tion to give strategic information about what we are up to, but we can be put
up to things by communication. ‘‘A voluntary action is something a person
can do when asked’’ (Wegner 2002, p. 32). The capacity to respond to such
requests, whether initiated by others or by oneself, is a capacity that must
require quite a revolutionary reorganization of cerebral resources.3 This
prospect is often overlooked by researchers eager to stress the parallels and
similarities between human subjects and animal subjects when they train a
monkey (typically) to ‘indicate’ one thing or another by moving their eyes
to one or another target on screen, or to press one of several buttons to get
a reward. A human subject can be briefed in a few minutes about such a
task, and thereupon, with only a few practice trials, execute the instruc-
tions ﬂawlessly for the duration of the experiment. The fact that preparing
the animal to perform the behavior usually involves hundreds or even
thousands of training trials does little to dampen the conviction that the
resulting behavior counts as a ‘‘report’’ by the animal of its subjective
state.4 But precisely what is missing in these experiments is any ground
for believing that the animal knows it is communicating when it does
what it does. And if it is not in the position of an agent that has decided
to tell the truth about what is going on in it now, there is really no reason
to treat its behavior as an intentional informing. Its carefully sculpted
actions may betray its internal state (the way the chess program willy-nilly
divulges its internal state), but this is not the fruit of self-monitoring.
If something along these lines is right, then we have some reason to con-
clude that contrary to tradition and even ‘‘common sense,’’ there is scant
reason to suppose that it is like anything to be a bat. The bat’s body has a
mind of its own, and doesn’t need a ‘‘me’’ to inhabit it at all. One might
thus say that only we who compare notes (strategically) inhabit the com-
plicated machines known as nervous systems. But on the whole this meta-
phor may by now be causing more confusion than enlightenment. Perhaps
we should say instead: we strategic signalers may be the only selves that
nervous systems produce.
98
Daniel C. Dennett

Notes
1. This robotic sensitivity-cum-action-guidance is what I used to call awareness2, to
distinguish it from the reportable kind of consciousness that might be only a human
gift, awareness1 (Dennett 1969), a pair of awkward terms that never caught on,
though the idea of the distinction is still useful, in my opinion.
2. I ﬁnd it strategically useful to insist that individual cells, whether prokaryotic, ar-
chaic, or eukaryotic, are basically robots that can duplicate themselves, since this is
the take-home message of the last half-century of cell biology. No more romantic vi-
sion of living cells as somehow transcending the bounds of nanobothood has any
purchase in the details of biology, so far as I can see. Even if it turns out that quan-
tum effects arising in the microtubules that crisscross the interiors of these cells play
a role in sustaining life, this will simply show that the robotic motor proteins that
scurry back and forth on those microtubules are robots with access to randomizers.
3. Thomas Metzinger (Being No One, 2003) has some excellent suggestions about
what he calls the phenomenal self-model and its revolutionary capacities.
4. See Sweet Dreams (Dennett 2005, pp. 169–70) for earlier remarks on this. I myself
have given more credence in the past to this proposal than I now think appropriate.
See Dennett (1991, pp. 441–48).
References
Ainslie, G. 2001. Breakdown of Will. Cambridge: Cambridge University Press.
Dawkins, R., and J. R. Krebs. 1978. Animal signals: Information or manipulation? In
J. R. Krebs and N. B. Davies, eds., Behavioural Ecology: An Evolutionary Approach, pp.
282–309. Blackwell, Oxford.
Dennett, D. C. 1969. Content and Consciousness. London: Routledge and Kegan Paul.
Dennett, D. C. 1991. Consciousness Explained. New York: Little Brown.
Dennett, D. C. 1996. Kinds of Minds. New York: Basic Books.
Dennett, D. C. 2005. Sweet Dreams: Philosophical Obstacles to a Science of Conscious-
ness. Cambridge: MIT Press.
Drescher, G. 1991. Made-up Minds: A Constructivist Approach to Artiﬁcial Intelligence.
Cambridge: MIT Press.
Frayn, M. 1999. Headlong. London: Faber and Faber.
Hamilton, W. 1996. Narrow Roads of Gene Land, vol 1. Oxford: Freeman.
Hammerstein, P., ed. 2003. Genetic and Cultural Evolution of Cooperation. Cambridge:
MIT Press.
My Body Has a Mind of Its Own
99

Haugeland, J. 1998. Having Thought: Essays in the Metaphysics of Mind. Cambridge:
Harvard University Press.
McFarland, D. 1989. Goals, no-goals and own goals. In A. Monteﬁore and D. Noble,
eds., Goals, No-Goals and Own Goals: A Debate on Goal-Directed and Intentional Behav-
ior, pp. 39–57. London: Unwin-Hyman.
Metzinger, T. 2003. Being No One. Cambridge: MIT Press.
Sterelny, K. 2003. Thought in a Hostile World: The Evolution of Human Cognition.
Oxford: Blackwell.
Trivers, R. 1985. Social Evolution. Menlo Park, CA: Benjamin/Cummings.
Wegner, D. 2002. The Illusion of Conscious Will. Cambridge: MIT Press.
100
Daniel C. Dennett

Distributed Cognition and the Will
Individual Volition and Social Context
edited by Don Ross, David Spurrett, Harold Kincaid, and G. Lynn Stephens
A Bradford Book
The MIT Press
Cambridge, Massachusetts
London, England

( 2007 Massachusetts Institute of Technology
All rights reserved. No part of this book may be reproduced in any form by any
electronic or mechanical means (including photocopying, recording, or information
storage and retrieval) without permission in writing from the publisher.
MIT Press books may be purchased at special quantity discounts for business or sales
promotional use. For information, please e-mail special_sales@mitpress.mit.edu or
write to Special Sales Department, The MIT Press, 55 Hayward Street, Cambridge,
MA 02142.
This book was set in Stone Serif and Stone Sans on 3B2 by Asco Typesetters, Hong
Kong and was printed and bound in the United States of America.
Library of Congress Cataloging-in-Publication Data
Distributed cognition and the will : individual volition and social context / edited
by Don Ross . . . [et al.].
p.
cm.
‘‘A Bradford book.’’
Includes bibliographical references and index.
ISBN-13: 978-0-262-18261-4 (hardcover : alk. paper)
ISBN-13: 978-0-262-68169-8 (pbk. : alk. paper)
1. Will. 2. Act (Philosophy) 3. Distributed cognition. I. Ross, Don, 1962–
BJ1461.D57
2007
1280.3—dc22
2006030813
10
9
8
7
6
5
4
3
2
1


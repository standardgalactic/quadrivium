
Academic Press is an imprint of Elsevier
The Boulevard, Langford Lane, Kidlington, Oxford OX5 1GB, UK
225 Wyman Street, Waltham, MA 02451, USA
First edition 2014
Copyright © 2014 Elsevier Ltd. All rights reserved.
No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any 
means electronic, mechanical, photocopying, recording or otherwise without the prior written permission of the 
publisher.
Permissions may be sought directly from Elsevier’s Science & Technology Rights Department in Oxford, UK: 
phone (+44) (0) 1865 843830; fax (+44) (0) 1865 853333; email: permissions@elsevier.com. Alternatively you 
can submit your request online by visiting the Elsevier web site at http://elsevier.com/locate/permissions, and 
selecting Obtaining permission to use Elsevier material.
Notice
No responsibility is assumed by the publisher for any injury and/or damage to persons or property as a matter of 
products liability, negligence or otherwise, or from any use or operation of any methods, products, instructions or 
ideas contained in the material herein. Because of rapid advances in the medical sciences, in particular, indepen-
dent verification of diagnoses and drug dosages should be made.
Library of Congress Cataloging in Publication Data
A catalog record for this book is available from the Library of Congress
British Library Cataloguing in Publication Data
A catalogue record for this book is available from the British Library
ISBN: 978-0-12-396502-8
For information on all Elsevier publications
visit our website at www.store.elsevier.com
Printed and bound in Poland.
14 15 16 17 10 9 8 7 6 5 4 3 2 1

xxvii
Signal Processing at Your Fingertips!
Let us flash back to the 1970s when the editors-in-chief of this e-reference were graduate students. 
One of the time-honored traditions then was to visit the libraries several times a week to keep track of 
the latest research findings. After your advisor and teachers, the librarians were your best friends. We 
visited the engineering and mathematics libraries of our Universities every Friday afternoon and poured 
over the IEEE Transactions, Annals of Statistics, the Journal of Royal Statistical Society, Biometrika, 
and other journals so that we could keep track of the recent results published in these journals. Another 
ritual that was part of these outings was to take sufficient number of coins so that papers of interest 
could be xeroxed. As there was no Internet, one would often request copies of reprints from authors 
by mailing postcards and most authors would oblige. Our generation maintained thick folders of hard-
copies of papers. Prof. Azriel Rosenfeld (one of RC’s mentors) maintained a library of over 30,000 
papers going back to the early 1950s!
Another fact to recall is that in the absence of Internet, research results were not so widely dis-
seminated then and even if they were, there was a delay between when the results were published in 
technologically advanced western countries and when these results were known to scientists in third 
world countries. For example, till the late 1990s, scientists in US and most countries in Europe had a 
lead time of at least a year to 18 months since it took that much time for papers to appear in journals 
after submission. Add to this the time it took for the Transactions to go by surface mails to various 
libraries in the world. Scientists who lived and worked in the more prosperous countries were aware of 
the progress in their fields by visiting each other or attending conferences.
Let us race back to 21st century! We live and experience a world which is fast changing with rates 
unseen before in the human history. The era of Information and Knowledge societies had an impact on 
all aspects of our social as well as personal lives. In many ways, it has changed the way we experience 
and understand the world around us; that is, the way we learn. Such a change is much more obvious to 
the younger generation, which carries much less momentum from the past, compared to us, the older 
generation. A generation which has grew up in the Internet age, the age of Images and Video games, the 
age of IPAD and Kindle, the age of the fast exchange of information. These new technologies comprise 
a part of their “real” world, and Education and Learning can no more ignore this reality. Although many 
questions are still open for discussions among sociologists, one thing is certain. Electronic publishing 
and dissemination, embodying new technologies, is here to stay. This is the only way that effective 
pedagogic tools can be developed and used to assist the learning process from now on. Many kids in the 
early school or even preschool years have their own IPADs to access information in the Internet. When 
they grow up to study engineering, science, or medicine or law, we doubt if they ever will visit a library 
as they would by then expect all information to be available at their fingertips, literally!
Another consequence of this development is the leveling of the playing field. Many institutions in 
lesser developed countries could not afford to buy the IEEE Transactions and other journals of repute. 
Even if they did, given the time between submission and publication of papers in journals and the time 
it took for the Transactions to be sent over surface mails, scientists and engineers in lesser developed 
countries were behind by two years or so. Also, most libraries did not acquire the proceedings of confer-
ences and so there was a huge gap in the awareness of what was going on in technologically advanced 
Introduction

xxviii
Introduction
countries. The lucky few who could visit US and some countries in Europe were able to keep up with 
the progress in these countries. This has changed. Anyone with an Internet connection can request or 
download papers from the sites of scientists. Thus there is a leveling of the playing field which will lead 
to more scientist and engineers being groomed all over the world.
The aim of Online Reference for Signal Processing project is to implement such a vision. We all 
know that asking any of our students to search for information, the first step for him/her will be to click 
on the web and possibly in the Wikipedia. This was the inspiration for our project. To develop a site, 
related to the Signal Processing, where a selected set of reviewed articles will become available at a 
first “click.” However, these articles are fully refereed and written by experts in the respected topic. 
Moreover, the authors will have the “luxury” to update their articles regularly, so that to keep up with 
the advances that take place as time evolves. This will have a double benefit. Such articles, besides the 
more classical material, will also convey the most recent results providing the students/researchers with 
up-to-date information. In addition, the authors will have the chance of making their article a more 
“permanent” source of reference, that keeps up its freshness in spite of the passing time.
The other major advantage is that authors have the chance to provide, alongside their chapters, any 
multimedia tool in order to clarify concepts as well as to demonstrate more vividly the performance of 
various methods, in addition to the static figures and tables. Such tools can be updated at the author’s 
will, building upon previous experience and comments. We do hope that, in future editions, this aspect 
of this project will be further enriched and strengthened.
In the previously stated context, the Online Reference in Signal Processing provides a revolutionary 
way of accessing, updating and interacting with online content. In particular, the Online Reference will 
be a living, highly structured, and searchable peer-reviewed electronic reference in signal/image/video 
Processing and related applications, using existing books and newly commissioned content, which 
gives tutorial overviews of the latest technologies and research, key equations, algorithms, applications, 
standards, code, core principles, and links to key Elsevier journal articles and abstracts of non-Elsevier 
journals.
The audience of the Online Reference in Signal Processing is intended to include practicing engi-
neers in signal/image processing and applications, researchers, PhD students, post Docs, consultants, 
and policy makers in governments. In particular, the readers can be benefited in the following needs:
• 
To learn about new areas outside their own expertise.
• 
To understand how their area of research is connected to other areas outside their expertise.
• 
 To learn how different areas are interconnected and impact on each other: the need for a 
“helicopter” perspective that shows the “wood for the trees.”
• 
 To keep up-to-date with new technologies as they develop: what they are about, what is their 
potential, what are the research issues that need to be resolved, and how can they be used.
• 
 To find the best and most appropriate journal papers and keeping up-to-date with the newest, best 
papers as they are written.
• 
To link principles to the new technologies.
The Signal Processing topics have been divided into a number of subtopics, which have also dic-
tated the way the different articles have been compiled together. Each one of the subtopics has been 
coordinated by an AE (Associate Editor). In particular:

xxix
Introduction
 1. Signal Processing Theory (Prof. P. Diniz)
 2. Machine Learning (Prof. J. Suykens)
 3. DSP for Communications (Prof. N. Sidiropulos)
 4. Radar Signal Processing (Prof. F. Gini)
 5. Statistical SP (Prof. A. Zoubir)
 6. Array Signal Processing (Prof. M. Viberg)
 7. Image Enhancement and Restoration (Prof. H. J. Trussell)
 8. Image Analysis and Recognition (Prof. Anuj Srivastava)
 9.  Video Processing (other than compression), Tracking, Super Resolution, Motion Estimation, 
etc. (Prof. A. R. Chowdhury)
10. Hardware and Software for Signal Processing Applications (Prof. Ankur Srivastava)
11. Speech Processing/Audio Processing (Prof. P. Naylor)
12.  Still Image Compression
13.   Video Compression
We would like to thank all the Associate Editors for all the time and effort in inviting authors as well  
as coordinating the reviewing process. The Associate Editors have also provided succinct summaries  
of their areas.
The articles included in the current editions comprise the first phase of the project. In the second 
phase, besides the updates of the current articles, more articles will be included to further enrich the 
existing number of topics. Also, we envisage that, in the future editions, besides the scientific articles 
we are going to be able to include articles of historical value. Signal Processing has now reached an age 
that its history has to be traced back and written.
Last but not least, we would like to thank all the authors for their effort to contribute in this new 
and exciting project. We earnestly hope that in the area of Signal Processing, this reference will help 
level the playing field by highlighting the research progress made in a timely and accessible manner to 
anyone who has access to the Internet. With this effort the next breakthrough advances may be coming 
from all around the world.
The companion site for this work: http://booksite.elsevier.com/9780124166165 includes multimedia 
files (Video/Audio) and MATLAB codes for selected chapters.
Rama Chellappa
Sergios Theodoridis

xxxi
Rama Chellappa received the B.E. (Hons.) degree in Electronics and Communication 
Engineering from the University of Madras, India in 1975 and the M.E. (with 
Distinction) degree from the Indian Institute of Science, Bangalore, India in 1977. 
He received the M.S.E.E. and Ph.D. Degrees in Electrical Engineering from Purdue 
University, West Lafayette, IN, in 1978 and 1981, respectively. During 1981–
1991, he was a faculty member in the department of EE-Systems at University 
of Southern California (USC). Since 1991, he has been a Professor of Electrical 
and Computer Engineering (ECE) and an affiliate Professor of Computer Science 
at University of Maryland (UMD), College Park. He is also affiliated with the 
Center for Automation Research, the Institute for Advanced Computer Studies (Permanent Member) 
and is serving as the Chair of the ECE department. In 2005, he was named a Minta Martin Professor 
of Engineering. His current research interests are face recognition, clustering and video summariza-
tion, 3D modeling from video, image and video-based recognition of objects, events and activities, 
dictionary-based inference, compressive sensing, domain adaptation and hyper spectral processing.
Prof. Chellappa received an NSF Presidential Young Investigator Award, four IBM Faculty 
Development Awards, an Excellence in Teaching Award from the School of Engineering at USC, and 
two paper awards from the International Association of Pattern Recognition (IAPR). He is a recipient 
of the K.S. Fu Prize from IAPR. He received the Society, Technical Achievement, and Meritorious 
Service Awards from the IEEE Signal Processing Society. He also received the Technical Achievement 
and Meritorious Service Awards from the IEEE Computer Society. At UMD, he was elected as a 
Distinguished Faculty Research Fellow, as a Distinguished Scholar-Teacher, received an Outstanding 
Innovator Award from the Office of Technology Commercialization, and an Outstanding GEMSTONE 
Mentor Award from the Honors College. He received the Outstanding Faculty Research Award and the 
Poole and Kent Teaching Award for Senior Faculty from the College of Engineering. In 2010, he was 
recognized as an Outstanding ECE by Purdue University. He is a Fellow of IEEE, IAPR, OSA, and 
AAAS. He holds four patents.
Prof. Chellappa served as the Editor-in-Chief of IEEE Transactions on Pattern Analysis and Machine 
Intelligence. He has served as a General and Technical Program Chair for several IEEE international 
and national conferences and workshops. He is a Golden Core Member of the IEEE Computer Society 
and served as a Distinguished Lecturer of the IEEE Signal Processing Society. Recently, he completed 
a two-year term as the President of the IEEE Biometrics Council.
About the Editors

xxxii
About the Editors
Sergios Theodoridis is currently Professor of Signal Processing and 
Communications in the Department of Informatics and Telecommunications 
of the University of Athens. His research interests lie in the areas of Adaptive 
Algorithms and Communications, Machine Learning and Pattern Recognition, 
Signal Processing for Audio Processing and Retrieval. He is the co-editor of the 
book “Efficient Algorithms for Signal Processing and System Identification,” 
Prentice Hall 1993, the co-author of the best selling book “Pattern Recognition,” 
Academic Press, 4th ed. 2008, the co-author of the book “Introduction to Pattern 
Recognition: A MATLAB Approach,” Academic Press, 2009, and the co-author of 
three books in Greek, two of them for the Greek Open University. He is Editor-in-Chief for the Signal 
Processing Book Series, Academic Press and for the E-Reference Signal Processing, Elsevier.
He is the co-author of six papers that have received best paper awards including the 2009 IEEE 
Computational Intelligence Society Transactions on Neural Networks Outstanding paper Award. He 
has served as an IEEE Signal Processing Society Distinguished Lecturer. He was Otto Monstead Guest 
Professor, Technical University of Denmark, 2012, and holder of the Excellence Chair, Department of 
Signal Processing and Communications, University Carlos III, Madrid, Spain, 2011.
He was the General Chairman of EUSIPCO-98, the Technical Program co-Chair for ISCAS-2006 
and ISCAS-2013, and co-Chairman and co-Founder of CIP-2008 and co-Chairman of CIP-2010. He 
has served as President of the European Association for Signal Processing (EURASIP) and as member 
of the Board of Governors for the IEEE CAS Society. He currently serves as member of the Board of 
Governors (Member-at-Large) of the IEEE SP Society.
He has served as a member of the Greek National Council for Research and Technology and he 
was Chairman of the SP advisory committee for the Edinburgh Research Partnership (ERP). He has 
served as Vice Chairman of the Greek Pedagogical Institute and he was for 4 years member of the 
Board of Directors of COSMOTE (the Greek mobile phone operating company). He is Fellow of IET, 
a Corresponding Fellow of the Royal Society of Edinburgh (RSE), a Fellow of EURASIP, and a Fellow 
of IEEE.

xxxiii
Paulo S. R. Diniz was born in Niterói, Brazil. He received the Electronics 
Engineering degree (Cum Laude) from the Federal University of Rio de Janeiro 
(UFRJ) in 1978, the M.Sc. degree from COPPE/UFRJ in 1981, and the Ph.D. 
from Concordia University, Montreal, P.Q., Canada, in 1984, all in electrical 
engineering. 
Since 1979 he has been with the Department of Electronic Engineering (the 
undergraduate department) UFRJ. He has also been with the Program of Electrical 
Engineering (the graduate studies department), COPPE/UFRJ, since 1984, where 
he is presently a Professor. He served as Undergraduate Course Coordinator and as 
Chairman of the Graduate Department. He has also received the Rio de Janeiro State Scientist award, 
from the Governor of Rio de Janeiro state. 
From January 1991 to July 1992, he was a visiting Research Associate in the Department of Electri-
cal and Computer Engineering of University of Victoria, Victoria, B.C., Canada. He also holds a Do-
cent position at Helsinki University of Technology (now Aalto University). From January 2002 to June 
2002, he was a Melchor Chair Professor in the Department of Electrical Engineering of University of 
Notre Dame, Notre Dame, IN, USA. His teaching and research interests are in analog and digital signal 
processing, adaptive signal processing, digital communications, wireless communications, multirate 
systems, stochastic processes, and electronic circuits. He has published several refereed papers in some 
of these areas and wrote the books ADAPTIVE FILTERING: Algorithms and Practical Implementa-
tion, Springer, NY, Fourth Edition 2013, DIGITAL SIGNAL PROCESSING: System Analysis and 
Design, Cambridge University Press, Cambridge, UK, Second Edition 2010 (with E. A. B. da Silva 
and S. L. Netto), and Block Transceivers: OFDM and Beyond, Morgan & Claypool, Fort Collins, CO, 
2012, (with W. A. Martins and M. V. S. Lim). He has served as General co-Chair of ISCAS2011 and 
Technical Program Chair of the 1995 MWSCAS both held in Rio de Janeiro, Brazil. He was also the 
Technical Program co-Chair of SPAWC2008. He has been on the technical committee of several inter-
national conferences including ICASSP, ISCAS, ICECS, EUSIPCO, and MWSCAS. He has served as 
the Vice President for region 9 of the IEEE Circuits and Systems Society and as Chairman of the DSP 
technical committee of the same Society. He is also a Fellow of IEEE (for fundamental contributions to 
the design and implementation of fixed and adaptive filters and Electrical Engineering Education). He 
has served as Associate Editor for the following Journals: IEEE Transactions on Circuits and Systems 
II: Analog and Digital Signal Processing from 1996 to 1999, IEEE Transactions on Signal Processing 
from 1999 to 2002, and the Circuits, Systems and Signal Processing Journal from 1998 to 2002. He 
was a Distinguished Lecturer of the IEEE Circuits and Systems Society for the year 2000–2001. In 
2004 he served as Distinguished Lecturer of the IEEE Signal Processing Society and received the 2004 
Education Award of the IEEE Circuits and Systems Society.
Section Editors
Section 1

xxxiv
 Section Editors
Johan A.K. Suykens received the master degree in ElectroMechanical Engineering 
and the Ph.D. degree in Applied Sciences from the Katholieke Universiteit Leuven, 
in 1989 and 1995, respectively. In 1996 he has been a Visiting Postdoctoral 
Researcher at the University of California, Berkeley. He has been a Postdoctoral 
Researcher with the Fund for Scientific Research FWO Flanders and is currently 
a Professor (Hoogleraar) with KU Leuven. He is author of the books “Artificial 
Neural Networks for Modeling and Control of Non-linear Systems” (Kluwer 
Academic Publishers) and “Least Squares Support Vector Machines” (World 
Scientific), co-author of the book “Cellular Neural Networks, Multi-Scroll Chaos 
and Synchronization” (World Scientific), and editor of the books “Nonlinear Modeling: Advanced 
Black-Box Techniques” (Kluwer Academic Publishers) and “Advances in Learning Theory: Methods, 
Models and Applications” (IOS Press). He is a Senior IEEE member and has served as associate edi-
tor for the IEEE Transactions on Circuits and Systems (1997–1999 and 2004–2007) and for the IEEE 
Transactions on Neural Networks (1998–2009). He received an IEEE Signal Processing Society 1999 
Best Paper (Senior) Award and several Best Paper Awards at International Conferences. He is a recipi-
ent of the International Neural Networks Society INNS 2000 Young Investigator Award for significant 
contributions in the field of neural networks. He has served as a Director and Organizer of the NATO 
Advanced Study Institute on Learning Theory and Practice (Leuven 2002), as a program co-chair 
for the International Joint Conference on Neural Networks 2004 and the International Symposium 
on Non-linear Theory and its Applications 2005, as an organizer of the International Symposium 
on Synchronization in Complex Networks 2007 and a co-organizer of the NIPS 2010 workshop on 
Tensors, Kernels and Machine Learning. He has been awarded an ERC Advanced Grant 2011.
Section 2

xxxv
CHAPTER 1
Isabela Ferrão Apolinário was born in Brasília, Brazil. She is currently studying 
Electronics Engineering in the Federal University of Rio de Janeiro (UFRJ) 
and will soon join an M.Sc. course in the same university. 
Since 2010 she has been a member of the Audio Processing Group (GPA) 
from the Signal Processing Laboratory (LPS) and has been studying audio pro-
cessing. She has worked as an undergraduate teaching assistance in Calculus II 
and Digital Signal Processing. 
She is currently a student member of the IEEE Circuits and Systems So-
ciety. Her main interest is in digital audio signal processing, more specifically 
in Psychoacoustics and Audio 3D.
CHAPTER 2
José Antonio Apolinário Junior graduated from the Military Academy of Agulhas 
Negras (AMAN), Brazil, in 1981 and received the B.Sc. degree from the 
Military Institute of Engineering (IME), Brazil, in 1988, the M.Sc. degree from 
the University of Brasília (UnB), Brazil, in 1993, and the D.Sc. degree from the 
Federal University of Rio de Janeiro (COPPE/UFRJ), Rio de Janeiro, Brazil, in 
1998, all in electrical engineering. He is currently an Adjoint Professor with the 
Department of Electrical Engineering, IME, where he has already served as the 
Head of Department and as the Vice-Rector for Study and Research. He was 
a Visiting Professor at the em Escuela Politécnica del Ejército (ESPE), Quito, 
Ecuador, from 1999 to 2000 and a Visiting Researcher and twice a Visiting Professor at Helsinki 
University of Technology (HUT), Finland, in 1997, 2004, and 2006, respectively. His research 
interests comprise many aspects of linear and nonlinear digital signal processing, including adaptive 
filtering, speech, and array processing. He has organized and has been the first Chair of the Rio de 
Janeiro Chapter of the IEEE Communications Society. He has recently edited the book “QRDRLS 
Adaptive Filtering” (Springer, 2009) and served as the Finance Chair of IEEE ISCAS 2011 (Rio de 
Janeiro, Brazil, May 2011). He is a senior member of the IEEE.
Carla Liberal Pagliari received the Ph.D. degree in electronic systems engi-
neer-ing from the University of Essex, UK, in 2000. Since 1993 she has been 
with the Department of Electrical Engineering at the Military Institute of 
Engineering (IME), Rio de Janeiro, Brazil. She took part in the team that 
worked toward the development of the Brazilian Digital Television System. 
Her research interests include image processing, digital television, image and 
video coding, stereoscopic and multiview systems, and computer vision. She 
is a senior member of the IEEE and served as the Local Arrangements Chair 
Authors Biography

xxxvi
Authors Biography
of IEEE ISCAS 2011 (Rio de Janeiro, Brazil, May 2011). She is currently an Associate Editor of 
the journal Multidimensional Systems and Signal Processing.
CHAPTER 3
Leonardo Gomes Baltar received his B.Sc. and M.Sc. degrees in Electrical 
Engineering from the Federal University of Rio de Janeiro (UFRJ), Brazil, 
in 2004 and 2006, respectively. Since 2006, he is with the Chair of Circuit 
Theory and Signal Processing at the Technical University of Munich (TUM), 
Germany, as a research and teaching assistant pursuing a Ph.D. degree. His 
activities are mainly in the area of digital signal processing techniques applied 
to wired and wireless communications systems including multirate systems, 
filter banks, block transforms, digital filters design, and efficient processing 
structures. He has been involved in the European FP7 project PHYDYAS and 
in projects with industrial partners.
Josef A. Nossek received his Dipl.-Ing. and Dr. techn. degrees from the 
Technical University of Vienna, Austria, in 1974 and 1980, respectively. He 
was for 15 years with Siemens AG, Munich, in the field of communications, 
including the position of Head of Radio Systems Design for digital commu-
nications. In 1989 he joined the Technical University of Munich (TUM), 
Germany, where he is full Professor and Head of the Chair for Circuit Theory 
and Signal Processing. He is a Fellow of IEEE since 1993 for his contribu-
tions to the design of discrete-time networks and technical leadership in the 
development of radio communication systems. He was the Editor-in-Chief of 
the IEEE Transactions on Circuits and Systems (1993–1995). He was President-Elect, President, 
and Past President of the IEEE Circuits and Systems Society (2001/02/03). In 1998 he received 
the Innovations Award of the Vodafone Foundation for excellent research in mobile communica-
tions. In 2008 he received the Education Award of the IEEE Circuits and Systems Society. In 
2011 he received the IEEE Guillemin-Cauer Best Paper Award. Since 2009 he is member of 
acatech (National Academy of Science and Engineering). He is author or co-author of numerous 
publications and has given a number of invited plenary lectures. He was the President of VDE 
(2007–2008), the German Association of Electrical, Electronics and Information Engineering, 
and Vice-President of VDE (2009–2010).
CHAPTER 4
Luiz W. P. Biscainho was born in Rio de Janeiro, Brazil, in 1962. He received 
the Electronics Engineering degree (magna cum laude) from the EE (now 
Poli) at Universidade Federal do Rio de Janeiro (UFRJ), Brazil, in 1985, and 
the M.Sc. and D.Sc. degrees in Electrical Engineering from the COPPE at 
UFRJ in 1990 and 2000, respectively. Having worked in the telecommu-
nication industry between 1985 and 1993, Dr. Biscainho is now Associate 

xxxvii
Authors Biography
Professor at the Department of Electronics and Computer Engineering (DEL) of Poli and the 
Electrical Engineering Program (PEE) of COPPE (serving as Academic Coordinator in 2010), 
at UFRJ. His research area is digital signal processing, particularly audio processing and adap-
tive systems. He is currently a member of the IEEE (Institute of Electrical and Electronics 
Engineers), the AES (Audio Engineering Society), the SBrT (Brazilian Telecommunications 
Society), and the SBC (Brazilian Computer Society).
CHAPTER 5
Håkan Johansson received the Master of Science degree in computer science 
and the Licentiate, Doctoral, and Docent degrees in Electronics Systems 
from Linkoping University, Sweden, in 1995, 1997, 1998, and 2001, respec-
tively. During 1998 and 1999 he held a postdoctoral position at Signal 
Processing Laboratory, Tampere University of Technology, Finland. He is 
currently Professor in Electronics Systems at the Department of Electrical 
Engineering of Linkoping University. His research encompasses design and 
implementation of efficient and flexible signal processing (SP) systems, 
mainly for communication applications. During the past decade, he has 
developed many different SP algorithms for various purposes, including filtering, sampling rate 
conversion, signal reconstruction, and parameter estimation. He has developed new estimation 
and compensation algorithms for errors in analog circuits such as compensation of mismatch 
errors in time-interleaved analog-to-digital converters and mixers. He is one of the founders 
of the company Signal Processing Devices Sweden AB that sells this type of advanced signal 
processing. He is the author or co-author of four books and some 160 international journal and 
conference papers. He is the co-author of three papers that have received best paper awards 
and he has authored one invited paper in IEEE Transactions and four invited chapters. He 
has served as Associate Editor for IEEE Trans. on Circuits and Systems I and II, IEEE Trans. 
Signal Processing, and IEEE Signal Processing Letters, and he is currently an Area Editor of the 
Elsevier Digital Signal Processing journal and a member of the IEEE Int. Symp. Circuits. Syst. 
DSP track committee.
CHAPTER 6
Lars Wanhammar was born in Vansbro, Sweden, on August 19, 1944. He has 
received the following degrees: Teknisk magister (teknisk fysik) in 1970, civilin-
genjör in 1980, teknisk doktor in 1981, and docent in 1986, in electrical engi-
neering from Linköping University, Sweden. During 1964–1970 he worked at 
Televerket (Royal Swedish Telephone Board), Division of Communication and 
during 1970–1971 as a Lecturer at the technical college in Norrköping. Since 
1971 he has been working at Linköping University, Department of Electrical 
Engineering, Division of Applied Electronics, as Assistant, Research Assistant, 
and from 1982 as Associate Professor (universitetslektor) and from 1997 as 
full Professor and Head of the Division of Electronics Systems, at the Department of Electrical 
Engineering, Linköping University, Sweden. From 2011 he is currently Professor Emeritus.

xxxviii Authors Biography
In addition, from 1995 to 2004 he worked as Adjunct Professor at the Norwegian Institute of 
Technology (NTNU) at the departments of Physical Electronics and Telecommunications.
His research interests are primary theory and design of digital signal processing and telecom-
munication systems, particularly analog and digital filters, and discrete transforms as well as com-
putational properties of DSP algorithms, bit-serial, digit-serial and distributed arithmetic, CAD 
tools, and globally asynchronous locally synchronous techniques for ULSI.
He is the author or co-author of five books in analog and digital filters, one in parallel process-
ing in industrial real-time applications, and one in DSP integrated circuits.
Ya Jun Yu received the B.Sc. and M.Eng. degrees in biomedical engineering 
from Zhejiang University, Hangzhou, China, in 1994 and 1997, respectively, 
and the Ph.D. degree in electrical and computer engineering from the National 
University of Singapore, Singapore, in 2004.
From 1997 to 1998, she was a Teaching Assistant with Zhejiang University. 
She joined the Department of Electrical and Computer Engineering, National 
University of Singapore as a Post Master Fellow in 1998 and remained in the 
same department as a Research Engineer until 2004. She joined the Temasek 
Laboratories at Nanyang Technological University as a Research Fellow in 
2004. Since 2005, she has been with the School of Electrical and Electronic Engineering, Nanyang 
Technological University, Singapore, where she is currently an Assistant Professor. Her research 
interests include digital signal processing and VLSI circuits and systems design.
He has served as an associate editor for Circuits Systems and Signal Processing and IEEE 
TRANSACTIONS ON CIRCUITS AND SYSTEMS II since 2009 and 2010, respectively.
CHAPTER 7
Fred Harris holds the Signal Processing Chair of the Communication Systems 
and Signal Processing Institute at San Diego State University where since 
1967 he has taught courses in Digital Signal Processing and Communication 
Systems. He holds 20 patents on digital receiver and DSP technology and 
lectures throughout the world on DSP applications. He consults for organi-
zations requiring high-performance, cost-effective DSP solutions. He is an 
adjunct member of the IDA-Princeton Center for Communications Research.
Fred Harris has written over 200 journal and conference papers, the most 
well known being his 1978 paper “On the use of Windows for Harmonic 
Analysis with the Discrete Fourier Transform”. He is the author of the book Multirate Signal 
Processing for Communication Systems and he has contributed to a number of other books on 
DSP applications including the “Source Coding” chapter in Bernard Sklar’s 1988 book, Digital 
Communications and the “Multirate FIR Filters for Interpolation and Resampling” and the 
“Time Domain Signal Processing with the DFT” chapters in Doug Elliot’s 1987 book Handbook 
of Digital Signal Processing, and “A most Efficient Digital Filter: The Two-Path Recursive All-
Pass Filter” Chapter and the “Ultra Low Phase Noise DSP Oscillator” Chapter in Rick Lyon’s 
2012 book Streamlining Digital Signal Processing. He is also co-author of the book Software 
Radio Sampling Rate Selection, Design and Synchronization.

xxxix
Authors Biography
In 1990 and 1991 he was the Technical and then the General Chair of the Asilomar Conference 
on Signals, Systems, and Computers and was Technical Chair of the 2003 Software Defined 
Radio Conference and of the 2006 Wireless Personal Multimedia Conference. He became a 
Fellow of the IEEE in 2003, cited for contributions of DSP to communications systems. In 
2006 he received the Software Defined Radio Forum’s “Industry Achievement Award”. His 
paper at the 2006 SDR conference was selected for the best paper award as was his paper at 
the 2010 Autotestcon conference and again his paper at the 2011 Wireless Personal Mobile 
Communications Conference and once again the 2011 SDR conference. He is the former Editor-
in-Chief of the Elsevier DSP Journal. 
The spelling of my name with all lower case letters is a source of distress for typists and spell 
checkers. A child at heart, I collect toy trains and old slide-rules.
Elettra Venosa received the “Laurea” (BS/MS) degree, summa cum laude, in 
Electrical Engineering in January 2007 from Seconda Università degli Studi 
di Napoli, Italy. From January 2007 to November 2007 she was a researcher 
at the Italian National Inter-University Consortium for Telecommunications. 
In January 2011, she received the Ph.D. in Telecommunication/DSP from 
Seconda Università degli Studi di Napoli. From June 2008 to September 
2008 she worked as a  project manager for Kiranet –ICT Research Center 
– to develop an advanced radio identification system for avionics, in collabo-
ration with the Italian Center for Aerospace Research (CIRA). From April 
2009 to September 2009 she worked as associate researcher at Communications and Signal 
Processing Laboratory (CSPL) in the Department of Electrical and Computer Engineering 
at Drexel University, Philadelphia, where she worked on Sparse Sampling Techniques for 
Software Defined Radio Receivers. From January 2011 to July 2012 she worked as principal 
system engineer in IQ-Analog, CA, developing algorithms for digital correction in TI-ADCs. 
From August 2012 to December 2013 she was associate researcher in Qualcomm, CA. Her 
focus was to improve the current commercial modem architectures. Currently, she is work-
ing, as a postdoctoral researcher, on Multirate Signal Processing for Software Defined Radios 
in the Department of Electrical Engineering at San Diego State University, San Diego, CA, 
USA where she also teaches graduate and undergraduate courses. She is also working as a soft-
ware defined radio engineer in Space Micro, CA. She is the author of more than 40 scientific 
publications on SDR and of the book “Software Radio: Sampling Rate Selection Design and 
Synchronization”.
Xiaofei Chen received the Bachelor’s degree in Electrical Engineering in 
June 2006 from Xi’an University of Posts & Telecommunications, China. In 
December 2008, he received the Master’s degree at Electrical & Computer 
Engineering department, San Diego State University, USA. In March 2009, 
he joined the Joint Doctoral Program between San Diego State University 
and University of California, San Diego. His current research interests are in 
the area of multirate signal processing and software defined radio.

xl
Authors Biography
CHAPTER 8
Trac D. Tran (S’94–M’98–SM’08) received the B.S. and M.S. degrees from 
the Massachusetts Institute of Technology, Cambridge, in 1993 and 1994, 
respectively, and the Ph.D. degree from the University of Wisconsin, 
Madison, in 1998, all in electrical engineering.
In July 1998, he joined the Department of Electrical and Computer 
Engineering, The Johns Hopkins University, Baltimore, MD, where he cur-
rently holds the rank of Professor. His research interests are in the field of sig-
nal processing, particularly in sparse representation, sparse recovery, sampling, 
multirate systems, filter banks, transforms, wavelets, and their applications in 
signal analysis, compression, processing, and communications. His pioneering research on integer-
coefficient transforms and pre-/post-filtering operators has been adopted as critical components of 
Microsoft Windows Media Video 9 and JPEG XR—the latest international still-image compres-
sion standard ISO/IEC 29199–2. He is currently a regular consultant for the US Army Research 
Laboratory, Adelphi, MD.
He was the codirector (with Prof. J. L. Prince) of the 33rd Annual Conference on Information 
Sciences and Systems (CISS’99), Baltimore, in March 1999. In the summer of 2002, he was 
an ASEE/ONR Summer Faculty Research Fellow at the Naval Air Warfare Center Weapons 
Division (NAWCWD), China Lake, CA. He has served as Associate Editor of the IEEE 
TRANSACTIONS ON SIGNAL PROCESSING as well as the IEEE TRANSACTIONS ON 
IMAGE PROCESSING. He was a former member of the IEEE Technical Committee on Signal 
Processing Theory and Methods (SPTM TC) and is a current member of the IEEE Image Video 
and Multidimensional Signal Processing (IVMSP) Technical Committee. He received the NSF 
CAREER award in 2001, the William H. Huggins Excellence in Teaching Award from The Johns 
Hopkins University in 2007, and the Capers and Marion McDonald Award for Excellence in 
Mentoring and Advising in 2009.
CHAPTER 9
Yufang Bao has been an Assistant Professor in the Department of Mathematics 
and Computer Science at UNC Fayetteville State University (UNCFSU) 
since 2007. She is also a scholar of the Center of Defense and Homeland 
Security (CDHS) at UNCFSU. She received her first Ph.D. degree in prob-
ability/statistics from Beijing Normal University (BNU), Beijing, China, and 
her second Ph.D. degree in Electrical Engineering from North Carolina State 
University (NCSU), Raleigh, NC. Her research focus was in probability/
statistics. She subsequently directed her research into the area of applying 
mathematics in signal/image processing and analysis. Her contributions in 
signal/image processing included algorithm development that bridged stochastic diffusion and 
multi-scale wavelet theory with scale space analysis methods for image denoising and segmenta-
tion. Between 2002 and 2007, she has worked at the VA Center, UCSF, CA and then at the 
University of Miami, School of Medicine, FL, both as a research scientist focusing on statistical 
image reconstruction in Frequency domain with MR spectroscopy imaging, and with parallel 

xli
Authors Biography
MR image reconstruction using mathematical modeling. Currently, her research interests are 
in applying mathematics to statistical digital signal/image processing and analysis, mathematical 
modeling, and their applications.
Hamid Krim (ahk@ncsu.edu) received his degrees in ECE from University of 
Washington and Northeastern University. He was a Member of Technical Staff 
at AT&T Bell Labs, where he has conducted research and development in the 
areas of telephony and digital communication systems/subsystems. Following 
an NSF postdoctoral fellowship at Foreign Centers of Excellence, LSS/
University of Orsay, Paris, France, he joined the Laboratory for Information 
and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA 
as a Research Scientist and where he was performing and supervising research.
He is presently Professor of Electrical Engineering in the ECE Department, 
North Carolina State University, Raleigh, leading the Vision, Information and Statistical Signal 
Theories and Applications group. His research interests are in statistical signal and image analysis 
and mathematical modeling with a keen emphasis on applied problems in classification and rec-
ognition using geometric and topological tools.
CHAPTER 10
Lisandro Lovisolo was born in Neuquen, Argentina, but considers himself 
brazilian. He received the Electronics Engineering degree from Universidade 
Federal do Rio de Janeiro, in 1999, the M.Sc. degree in Electrical Engineering 
in 2001, and the D.Sc. degree in Electrical Engineering both from Universidade 
Federal do Rio de Janeiro (COPPE/UFRJ). Since 2003 he has been with the 
Department of Electronics and Communications Engineering (the under-
graduate department), UERJ. He has also been with the Postgraduate in 
Electronics Program, since 2008. His research interests lie in the fields of 
digital signal and image processing and communications.
Eduardo A. B. da Silva was born in Rio de Janeiro, Brazil. He received the 
Electronics Engineering degree from Instituto Militar de Engenharia (IME), 
Brazil, in 1984, the M.Sc. degree in Electrical Engineering from Universidade 
Federal do Rio de Janeiro (COPPE/UFRJ) in 1990, and the Ph.D. degree 
in Electronics from the University of Essex, England, in 1995. In 1987 and 
1988 he was with the Department of Electrical Engineering at Instituto 
Militar de Engenharia, Rio de Janeiro, Brazil. Since 1989 he has been with 
the Department of Electronics Engineering (the undergraduate department), 
UFRJ. He has also been with the Department of Electrical Engineering (the 
graduate studies department), COPPE/UFRJ, since 1996. His research interests lie in the fields 
of digital signal and image processing, especially signal compression, digital television, wavelet 
transforms, mathematical morphology, and applications to telecommunications.

xlii
Authors Biography
CHAPTER 11
Suleyman Serdar Kozat received the B.S. degree with full scholarship and 
high honors from Bilkent University, Turkey. He received the M.S. and Ph.D. 
degrees in Electrical and Computer Engineering from University of Illinois at 
Urbana Champaign, Urbana, IL, in 2001 and 2004, respectively.
After graduation, he joined IBM Research, T.J. Watson Research Center, 
Yorktown, NY as a Research Staff Member in Pervasive Speech Technologies 
Group, where he focused on problems related to statistical signal process-
ing and machine learning. While doing his Ph.D., he was also working as a 
Research Associate at Microsoft Research, Redmond, WA, in Cryptography 
and Anti-Piracy Group. He holds several patent applications for his works performed in IBM 
Research and Microsoft Research. Currently, he is an Assistant Professor at the electrical and 
electronics engineering department, Koc University, Turkey. He coauthored more than 50 papers 
in refereed high impact journals and conference proceedings and has several patent applications. 
Overall, his research interests include intelligent systems, adaptive filtering for smart data analyt-
ics, online learning, and machine learning algorithms for signal processing.
He has been serving as an Associate Editor for the IEEE Transactions on Signal Processing and 
he is a Senior Member of the IEEE. He has been awarded IBM Faculty Award by IBM Research 
in 2011, Outstanding Faculty Award by Koc University in 2011, Outstanding Young Researcher 
Award by the Turkish National Academy of Sciences in 2010, ODTU Prof. Dr. Mustafa N. Parlar 
Research Encouragement Award in 2011 and holds Career Award by the Scientific Research 
Council of Turkey, 2009. He has won several scholarships and medals in international and national 
science and math competitions.
Andrew C. Singer received the S.B., S.M., and Ph.D. degrees, all in Electrical 
Engineering and Computer Science, from the Massachusetts Institute of 
Technology. Since 1998, he has been on the faculty of the Department of 
Electrical and Computer Engineering at the University of Illinois at Urbana-
Champaign, where he is currently a Professor in the ECE department and the 
Coordinated Science Laboratory. During the academic year 1996, he was a 
Postdoctoral Research Affiliate in the Research Laboratory of Electronics at 
MIT. From 1996 to 1998, he was a Research Scientist at Sanders, A Lockheed 
Martin Company in Manchester, New Hampshire, where he designed algo-
rithms, architectures, and systems for a variety of DOD applications. His research interests 
include signal processing and communication systems. He was a Hughes Aircraft Masters Fellow 
and was the recipient of the Harold L. Hazen Memorial Award for excellence in teaching in 1991. 
In 2000, he received the National Science Foundation CAREER Award, in 2001 he received the 
Xerox Faculty Research Award, and in 2002 he was named a Willett Faculty Scholar. He has 
served as an Associate Editor for the IEEE Transactions on Signal Processing and is a member of 
the MIT Educational Council, and of Eta Kappa Nu and Tau Beta Pi. He is a Fellow of the IEEE.
In 2005, he was appointed as the Director of the Technology Entrepreneur Center (TEC) in the 
College of Engineering. He also co-founded Intersymbol Communications, Inc., a venture-funded 

xliii
Authors Biography
fabless semiconductor IC company, based in Champaign Illinois. A developer of signal processing 
enhanced chips for ultra-high speed optical communications systems, Intersymbol was acquired 
by Finisar Corporation (NASD:FNSR) in 2007. He serves on the board of directors of a number 
of technology companies and as an expert witness for electronics, communications, and circuit-
related technologies.
CHAPTER 12
Vítor H. Nascimento was born in São Paulo, Brazil. He obtained the B.S. and 
M.S. degrees in Electrical Engineering from the University of São Paulo in 
1989 and 1992, respectively, and the Ph.D. degree from the University of 
California, Los Angeles, in 1999. From 1990 to 1994 he was a Lecturer at 
the University of São Paulo, and in 1999 he joined the faculty at the same 
school, where he is now an Associate Professor. One of his papers received the 
2002 IEEE SPS Best Paper Award. He served as an Associate Editor for the 
IEEE Signal Processing Letters from 2003 to 2005, for the IEEE Transactions 
on Signal Processing from 2005 to 2008, and for the EURASIP Journal on 
Advances in Signal Processing from 2006 to 2009, and was a member of the IEEE-SPS Signal 
Processing Theory and Methods Technical Committee from 2007 to 2012. Since 2010 he is the 
chair of the São Paulo SPS Chapter. His research interests include signal processing theory and 
applications, robust and nonlinear estimation, and applied linear algebra.
Magno T. M. Silva was born in São Sebastião do Paraíso, Brazil. He  received 
the B.S., M.S., and Ph.D. degrees, all in electrical engineering, from Escola 
Politécnica, University of São Paulo, São Paulo, Brazil, in 1999, 2001, and 
2005, respectively. From February 2005 to July 2006, he was an Assistant 
Professor at Mackenzie Presbyterian University, São Paulo, Brazil. Since August 
2006, he has been with the Department of Electronic Systems Engineering at 
Escola Politécnica, University of São Paulo, where he is currently an Assistant 
Professor. From January to July 2012, he worked as a Postdoctoral Researcher 
at Universidad Carlos III de Madrid, Leganés, Spain. His research interests 
include linear and non-linear adaptive filtering.
CHAPTER 14
Ambuj Tewari is with the Department of Statistics, University of Michigan, 
Ann Arbor. He has served on senior program committees of the confer-
ences on Algorithmic Learning Theory (ALT), Conference on Learning 
Theory (COLT), and Neural Information Processing Systems (NIPS). His 
work has received both the student paper award (2005) and the best paper 
award (2011) at COLT. He received his M.A. in Statistics (2005) and Ph.D. 
in Computer Science (2007) from the University of California at Berkeley 
where his advisor was Peter Bartlett. He was a research Assistant Professor in 
Toyota Technological Institute at Chicago (2008–2010), an Assistant Professor 

xliv
Authors Biography
(part-time) in the Department of Computer Science, University of Chicago (2008–2010), and 
a postdoctoral fellow in the Institute for Computational Engineering and Sciences, University 
of Texas at Austin (2010–2012). He has also been a Visiting Researcher at Microsoft Research, 
Redmond.
Peter L. Bartlett is with the Division of Computer Science and Department 
of  Statistics, University of California at Berkeley, and with the School of 
Mathematical Sciences, Queensland University of Technology. He is the co-
author of the book “Learning in Neural Networks: Theoretical Foundations.” He 
has served as Associate Editor of the journals Machine Learning, Mathematics 
of Control Signals and Systems, the Journal of Machine Learning Research, 
the Journal of Artificial Intelligence Research, and the IEEE Transactions 
on Information Theory. He was awarded the Malcolm McIntosh Prize for 
Physical Scientist of the Year in Australia for his work in statistical learning 
theory. He was a Miller Institute Visiting Research Professor in Statistics and Computer Science 
at U.C. Berkeley in Fall 2001, and a Fellow, Senior Fellow, and Professor in the Research School 
of Information Sciences and Engineering at the Australian National University’s Institute for 
Advanced Studies (1993–2003).
CHAPTER 15
Barbara Hammer received her Ph.D. in Computer Science in 1995 and 
her venia legendi in Computer Science in 2003, both from the University 
of Osnabrueck, Germany. From 2000 to 2004, she was a leader of the 
junior research group “Learning with Neural Methods on Structured 
Data” at University of Osnabrueck before accepting an offer as professor 
for Theoretical Computer Science at Clausthal University of Technology, 
Germany, in 2004. Since 2010, she is holding a professorship for Theoretical 
Computer Science for Cognitive Systems at the CITEC cluster of excellence 
at Bielefeld University, Germany. Several research stays have taken her to 
Italy, UK, India, France, the Netherlands, and the USA. Her areas of expertise include hybrid 
systems, self-organizing maps, clustering, and recurrent networks as well as applications in bioin-
formatics, industrial process monitoring, or cognitive science. She is leading the task force “Data 
Visualization and Data Analysis” of the IEEE CIS Technical Committee on Data Mining, and the 
Fachgruppe Neural Networks of the GI.
CHAPTER 16
John Shawe-Taylor is a Professor at the Department of Computer Science, University College 
London (UK). His main research area is Statistical Learning Theory, but his contributions 
range from Neural Networks, to Machine Learning, to Graph Theory. He has published 

xlv
Authors Biography
over 150 research papers. He obtained a Ph.D. in Mathematics at Royal 
Holloway, University of London in 1986. He subsequently completed an 
M.Sc. in the Foundations of Advanced Information Technology at Imperial 
College. He was promoted to Professor of Computing Science in 1996. He 
moved to the University of Southampton in 2003 to lead the ISIS research 
group. He was appointed the Director of the Center for Computational 
Statistics and Machine Learning at University College, London in July 
2006. He has coordinated a number of Europeanwide projects investigating 
the theory and practice of Machine Learning, including the NeuroCOLT 
projects. He is currently the scientific coordinator of a Framework VI Network of Excellence 
in Pattern Analysis, Statistical Modeling and Computational Learning (PASCAL) involving 
57 partners.
Shiliang Sun received the B.E. degree in automatic control from the 
Depart- ment of Automatic Control, Beijing University of Aeronautics and 
Astronautics in 2002, and the Ph.D. degree in pattern recognition and intel-
ligent systems from the State Key Laboratory of Intelligent Technology and 
Systems, Department of Automation, Tsinghua University, Beijing, China, in 
2007. In 2004, he was entitled Microsoft Fellow. Currently, he is a Professor 
at the Department of Computer Science and Technology and the Founding 
Director of the Pattern Recognition and Machine Learning Research Group, 
East China Normal University. From 2009 to 2010, he was a Visiting 
Researcher at the Department of Computer Science, University College London, working within 
the Center for Computational Statistics and Machine Learning. He is a member of the PASCAL 
(Pattern Analysis, Statistical Modelling and Computational Learning) network of excellence, and 
on the editorial boards of multiple international journals. His research interests include machine 
learning, pattern recognition, computer vision, natural language processing, and intelligent trans-
portation systems.
CHAPTER 17
Konstantinos Slavakis received the M.E. and Ph.D. degrees in electrical and 
electronic engineering from Tokyo Institute of Technology (TokyoTech), 
Tokyo, Japan, in 1999 and 2002, respectively. For the period from 2004 to 
2006, he was with TokyoTech as a JSPS PostDoc, and from 2006 to 2007, 
he was a Postdoc in the Department of Informatics and Telecommunications, 
University of Athens, Greece. From 2007 to 2012, he served as an Assistant 
Professor at the Department of Informatics and Telecommunications, 
University of Peloponnese, Tripolis, Greece. Currently, he is a Research 
Associate at the University of Minnesota, Digital Technology Center.

xlvi
Authors Biography
He serves as an Associate and Area Editor of the IEEE Transactions on Signal Processing. His 
research interests include applications of convex analysis and computational algebraic geometry 
to signal processing, machine learning, array, and multidimensional systems problems.
Pantelis Bouboulis received the M.Sc. and Ph.D. degrees in informatics 
and telecommunications from the National and Kapodistrian University 
of Athens, Greece, in 2002 and 2006, respectively. From 2007 till 2008, 
he served as an Assistant Professor in the Department of Informatics and 
Telecommunications, University of Athens. Since 2008, he teaches mathe-
matics in Greek High Schools. His current research interests lie in the areas 
of machine learning, fractals, wavelets, and image processing.
CHAPTER 18
Franz Pernkopf received his M.Sc. (Dipl. Ing.) degree in Electrical Engineering 
at Graz University of Technology, Austria, in summer 1999. He earned a 
Ph.D. degree from the University of Leoben, Austria, in 2002. In 2002 he 
was awarded the Erwin Schrödinger Fellowship. He was a Research Associate 
in the Department of Electrical Engineering at the University of Washington, 
Seattle, from 2004 to 2006. Currently, he is Associate Professor at the 
Laboratory of Signal Processing and Speech Communication, Graz University 
of Technology, Austria. His research interests include machine learning, dis-
criminative learning, graphical models, feature selection, finite mixture mod-
els, and image- and speech processing applications.
Robert Peharz received his M.Sc. degree in Telematics at Graz University of 
Technology (TUG) in 2010. He currently pursues his Ph.D. studies at the 
SPSC Lab, TUG. His research interests include probabilistic graphical mod-
els, sparse coding, nonnegative matrix factorization, and machine learning in 
general, with applications to signal processing, audio engineering, and com-
puter vision.
Sebastian Tschiatschek received the B.Sc. degree and M.Sc. degree in 
Electrical Engineering at Graz University of Technology (TUG) in 2007 
and 2010, respectively. He conducted his Master thesis during a one-year 
stay at ETH Zürich, Switzerland. Currently, he is with the Signal Processing 
and Speech Communication Laboratory at TUG where he is pursuing the 
Ph.D. degree. His research interests include Bayesian networks, informa-
tion theory in conjunction with graphical models and statistical pattern 
recognition.

xlvii
Authors Biography
CHAPTER 19
A. Taylan Cemgil (M’04) received his Ph.D. (2004) from SNN, Radboud 
University Nijmegen, the Netherlands. Between 2004 and 2008 he worked as 
a postdoctoral researcher at Amsterdam University and the Signal Processing 
and Communications Laboratory, University of Cambridge, UK. He is cur-
rently an Associate Professor of Computer Engineering at Bogazici University, 
Istanbul, Turkey. He is a member of the IEEE MLSP Technical Committee 
and an Associate Editor of IEEE Signal Processing Letters and Digital Signal 
Processing. His research interests are in Bayesian statistical methods and infer-
ence, machine learning, and audio signal processing.
CHAPTER 20
Dao Lam is a Ph.D. Candidate at Missouri University of Science and Technology, 
Rolla, MO. He received the B.S. degree from Post and Telecommunications 
Institute of Technology, Ho Chi Minh, Vietnam in 2003. He got his M.S. 
from Waseda University, Japan in 2008. His research interests are image pro-
cessing, robotics, supervised and unsupervised learning, and computational 
intelligence.
Donald Wunsch is the Mary K. Finley Missouri Distinguished Professor 
at Missouri University of Science & Technology (Missouri S&T). Earlier 
employers were: Texas Tech University, Boeing, Rockwell International, 
and International Laser Systems. His education includes: Executive MBA—
Washington University in St. Louis, Ph.D., Electrical Engineering—University 
of Washington (Seattle), M.S., Applied Mathematics (same institution), B.S., 
Applied Mathematics—University of New Mexico, and Jesuit Core Honors 
Program, Seattle University. Key research contributions are: Clustering; 
Adaptive Resonance and Reinforcement Learning architectures, hardware and 
applications; Neurofuzzy regression; Traveling Salesman Problem heuristics; Robotic Swarms; 
and Bioinformatics. He is an IEEE Fellow and previous INNS President, INNS Fellow and Senior 
Fellow 07—present, and served as IJCNN General Chair, and on several Boards, including the St. 
Patrick’s School Board, IEEE Neural Networks Council, International Neural Networks Society, 
and the University of Missouri Bioinformatics Consortium. He has produced 16 Ph.D. recip-
ients in Computer Engineering, Electrical Engineering, and Computer Science; has attracted 
over $8 million in sponsored research; and has over 300 publications including nine books. His 
research has been cited over 6000 times.

xlviii
Authors Biography
CHAPTER 21
Andrzej Cichocki received the M.Sc. (with honors), Ph.D. and Dr.Sc. 
(Habilitation) degrees, all in electrical engineering, from Warsaw University 
of Technology in Poland.
Since 1972, he has been with the Institute of Theory of Electrical Engineering, 
Measurement and Information Systems, Faculty of Electrical Engineering at the 
Warsaw University of Technology, where he obtained a title of a full Professor in 
1995.
He spent several years at University Erlangen-Nuerenberg in Germany, 
at the Chair of Applied and Theoretical Electrical Engineering directed by 
Professor Rolf Unbehauen, as an Alexander-von-Humboldt Research Fellow 
and Guest Professor.
In 1995–1997 he was a team leader of the laboratory for Artificial Brain Systems, at Frontier 
Research Program RIKEN (Japan), in the Brain Information Processing Group.
He is currently senior team leader and the head of the Cichocki Laboratory for Advanced 
Brain Signal Processing, at RIKEN Brain Science Institute in Japan.
CHAPTER 22
Xueyuan Zhou received his Ph.D. degree in computer science from the University of Chicago in 
2011. His research interests include statistical machine learning theory and application in non-
linear high dimensional data. 
Mikhail Belkin is an Associate Professor in the Department of Computer Science and Engineering 
at the Ohio State University. He received his Ph.D. degree in mathematics from the University 
of Chicago in 2003. His research interests include a range of theoretical questions concerning 
the computational and statistical limits of learning and mathematical foundations of learning 
structure from data.
Yannis Kopsinis received the B.Sc. degree from the Department of Informatics 
and Telecommunications, University of Athens, Greece, in 1998 and his 
Ph.D. degree in 2003 from the same department. From January 2004 to 
December 2005 he has been a research fellow with the Institute for Digital 
Communications, School of Engineering and Electronics, the University of 
Edinburgh, UK. From January 2006 to September 2009 he was a senior 
researcher in the same department. From January 2012 to April 2013 he 
was a Ramon Y Cajal Fellow in the School of Applied Physics, University of 
Granada, Spain. He currently holds a Marie Curie Intra-European fellowship 

xlix
Authors Biography
on sparse online learning. His current research interests include adaptive signal processing, time-
frequency analysis, and compressed sensing.
Konstantinos Slavakis received the M.E. and Ph.D. degrees in elec-
trical and electronic engineering from Tokyo Institute of Technology 
(TokyoTech), Tokyo, Japan, in 1999 and 2002, respectively. For the period 
of 2004–2006, he was with TokyoTech as a JSPS PostDoc, and from 
2006 to 2007, he was a Postdoc in the Department of Informatics and 
Telecommunications, University of Athens, Greece. From 2007 to 2012, 
he served as an Assistant Professor at the Department of Informatics 
and Telecommunications, University of Peloponnese, Tripolis, Greece. 
Currently, he is a Research Associate at the University of Minnesota, 
Digital Technology Center.
He serves as an Associate and Area Editor of the IEEE Transactions on Signal Processing. His 
research interests include applications of convex analysis and computational algebraic geometry 
to signal processing, machine learning, array, and multidimensional systems problems.
CHAPTER 24
José C. Principe is currently a Distinguished Professor of Electrical and 
Biomedical Engineering at the University of Florida, Gainesville, USA. 
He is BellSouth Professor and Founder and Director of the University of 
Florida Computational Neuro-Engineering Laboratory (CNEL). He is an 
IEEE fellow and AIMBE fellow, and is the past Editor-in-Chief of the IEEE 
TRANSACTIONS ON BIOMEDICAL ENGINEERING, past President of 
the International Neural Network Society, former Secretary of the Technical 
Committee on Neural Networks of the IEEE Signal Processing Society, and a 
former member of the Scientific Board of the Food and Drug Administration. 
He is involved in biomedical signal processing, in particular, the electroencephalogram (EEG) 
and the modeling and applications of adaptive systems.
Badong Chen received his Ph.D. degree in Computer Science and Technology from 
Tsinghua University, Beijing, China, in 2008. He was a Postdoctoral Associate at 
the University of Florida Computational NeuroEngineering Laboratory (CNEL) 
during the period October, 2010 to September, 2012. He is currently a Professor 
at the Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, 
Xi’an, China. His research interests are in statistical signal processing, information 
theoretic learning, online kernel learning, and their applications in cognition and 
neuroscience.

l
Authors Biography
Luis G. Sanchez Giraldo was born in 1983 in Manizales, Colombia. He received 
the B.S. in electronics engineering and M.Eng. in industrial automation from 
Universidad Nacional de Colombia in 2005 and 2008, respectively, and his 
Ph.D. in electrical and computer engineering from University of Florida in 
2012. Between 2004 and 2008, he was appointed as a research assistant at the 
Control and Digital Signal Processing Group (GCPDS) at Universidad Nacional 
de Colombia. During his Ph.D. studies he worked as a research assistant at the 
Computational Neuro-Engineering Laboratory (CNEL) at University of Florida. 
His main research interests are in machine learning and signal processing.
CHAPTER 25
Enes Makalic was born in 1980. He received the Bachelor of Computer Science (Honors) degree 
in 2002 and the Ph.D. degree in 2007, both from Monash University, Australia. His research 
interests include information theoretic model selection using Minimum Message Length (MML) 
and Minimum Description Length (MDL) theories of statistical inference. He currently holds a 
Postdoctoral position with The University of Melbourne, Australia.
Daniel F. Schmidt was born in 1980. He received the Bachelor of Digital Systems (Honors) degree 
in 2002 and the Ph.D. degree in 2008, both from Monash University, Australia. His research 
interests are primarily information theoretic approaches to statistical inference, model selection, 
and estimation. He currently holds a Postdoctoral position with The University of Melbourne, 
Australia.
Abd-Krim Seghouane received his Ph.D. degree from University of Paris sud (Paris XI) in 2003. 
His research interests are within statistical signal and image processing. He is currently a Senior 
Research Fellow within the department of Electrical and Electronic Engineering, The University 
of Melbourne.
CHAPTER 26
George Tzanetakis is an Associate Professor and Canada Research Chair in 
Computer Analysis of Audio and Music at the Department of Computer 
Science with cross-listed appointments in ECE and Music at the University 
of Victoria, Canada. In 2011 he was a visiting scientist at Google Research. 
He received his Ph.D. in Computer Science at Princeton University in 2002 
and was a Post-Doctoral fellow at Carnegie Mellon University in 2002–2003. 
His research spans all stages of audio content analysis such as feature extrac-
tion, segmentation, classification with specific emphasis on music information 
retrieval. He is also the primary designer and developer of Marsyas an open 

li
Authors Biography
source framework for audio processing with specific emphasis on music information retrieval 
applications. His pioneering work on musical genre classification received a IEEE signal pro-
cessing society young author award and is frequently cited. More recently he has been explor-
ing new interfaces for musical expression, music robotics, computational ethnomusicology, and 
computer-assisted music instrument tutoring. These interdisciplinary activities combine ideas 
from signal processing, perception, machine learning, sensors, actuators, and human-computer 
interaction with the connecting theme of making computers better understand music to create 
more effective interactions with musicians and listeners.

1
CHAPTER
Introduction to Signal Processing
Theory
Isabela F. Apolinário and Paulo S.R. Diniz
Program of Electrical Engineering and the Department of Electronics and Computer Engineering, COPPE/Poli,
Universidade Federal do Rio de Janeiro, Brazil
1.01.1 Introduction
Signal processing is a key area of knowledge that ﬁnds applications in virtually all aspects of modern
life. Indeed the human beings are employing signal processing tools for centuries without realizing it
[1]. In present days the younger generation might not be able to understand how one can live without
carrying a mobile phone, traveling long distances without an almost self piloted airplane, exploring other
planets without human presence, and utilizing a medical facility without a wide range of diagnostic and
intervention equipments.
Signal processing consists of mapping or transforming information bearing signals into another form
of signals at the output, aiming at some application beneﬁts. This mapping deﬁnes a continuous or analog
system if it involves functions representing the input and output signals. On the other hand, the system
is discrete or digital if its input and output signals are represented by sequences of numbers.
Signal processing theory is very rich and as a reward it has been ﬁnding applications in uncountable
areas, among which we can mention bioengineering, communications, control, surveillance, environ-
ment monitoring, oceanography, and astronomy, just to mention a few. In particular, the enormous
advance in digital integrated circuit technology, responsible for the computing and information revo-
lutions that we are so used today, enabled the widespread use of Digital Signal Processing Systems.
These systems allowed advances in ﬁelds such as: speech, audio, image, video, multirate processing,
besides in several applications of wireless communications and digital control.
This chapter is an overview of the classical signal processing tools whose details are encountered in
numerous textbooks such as: [1–15]. The topics treated in the following chapters entail the description
of many signal processing tools, usually not covered in the classical textbooks available in the market.
As a result, we believe that the readers can beneﬁt from reading many of these chapters, if not all, in
order to deepen and widen their current knowledge as well as to start exploiting new ideas.
1.01.2 Continuous-time signals and systems
The processing of signals starts by accessing the type of information we want to deal with. Many signals
originating from nature are continuous in time and as such, the processing involved must include analog
signal acquisition systems followed by the implementation of a continuous-time system if the processing
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00001-2
© 2014 Elsevier Ltd. All rights reserved.
3

4
CHAPTER 1 Introduction to Signal Processing Theory
x (t)
y (t)
t
t
H{·}
FIGURE 1.1
Continuous-time system.
remains analog all the way. Alternatively, assuming the original continuous-time signal contains limited
spectral information, we can sample it in order to generate a sequence representing the continuous-time
signal unambiguously1. In this latter form one can beneﬁt from the advanced digital integrated circuit
technology to process the signal. However, many real life applications still includes continuous-time
signal processing at least at the acquisition and actuator phases of the processing.
A continuous-time system maps an analog input signal represented by x(t) into an output signal
represented by y(t), as depicted in Figure 1.1. A general representation of this mapping is
y(t) = H{x(t)},
where H{·} denotes the operation performed by the continuous-time system. If the system is linear and
time-invariant (LTI), as will be described in Chapter 2, there are several tools to describe the mapping
features of the system. Typically, a general description of the systems consists of a differential equation
which in turn can be solved and analyzed by employing the Laplace transform. A key feature of the LTI
systems is their full representation through their impulse responses. The behavior of a nonlinear system
is more complicated to analyze since it can not be fully characterized by its impulse response.
Another important tools to deal with the continuous-time signals which are periodic and non-
periodic are the Fourier series and Fourier transform, respectively. As will be discussed in Chapter
2, the Fourier and Laplace transforms are closely related, being both essential tools to describe the
behavior of continuous-time signals when applied to LTI systems.
The Laplace transform is suitable to represent a time-domain non-periodic function of a continuous
and real (time) variable, resulting in a frequency-domain non-periodic function of a continuous and
complex frequency variable. That is,
X(s) =
 ∞
−∞
x(t)e−st dt
⇐⇒
x(t) = 1
2π eσt
 ∞
−∞
X(σ + jω)ejωt dω.
The Fourier transform is employed to represent a time-domain non-periodic function of a continuous
and real (time) variable with a frequency-domain non-periodic function of a continuous and imaginary
1As will be seen in Chapter 5, we can completely reconstruct a continuous-time band-limited signal x(t) from its sampled
version x(n) if the sampling frequency is chosen correctly.

1.01.3 Discrete-Time Signals and Systems
5
frequency variable. The Fourier transform is described by
X() =
 ∞
−∞
x(t)e−jt dt
⇐⇒
x(t) = 1
2π
 ∞
−∞
X()ejt d.
The representation of a time-domain periodic function of a continuous and real (time) variable is
performed by the Fourier series. In the frequency-domain, the representation consists of a non-periodic
function of a discrete and integer frequency variable, as following described:
X(k) = 1
T
 T
0
x(t)e−j(2π/T )kt dt
⇐⇒
x(t) =
∞

k=−∞
X(k)ej(2π/T )kt.
1.01.3 Discrete-time signals and systems
A discrete-time signal is represented by a sequence of numbers and can be denoted as x(n) with n ∈Z,
where Z is the set of integer numbers. The samples x(n) might represent numerically the amplitude of
a continuous-time signal sample at every T seconds or can be originated from a discrete information.
In many applications, the sampling period T represents the time interval between two acquired samples
of the continuous-time signal. However, in other situations it might represent the distance between two
sensorsoftwopixelsofanimage,ortheseparationbetweentwoantennas,justtomentionafewexamples.
Discrete-time systems map input sequences into output sequences. A general representation of this
mapping is
y(n) = H{x(n)},
where H{·} denotes the operation performed by the discrete-time system. Figure 1.2 depicts the input-
to-output mapping of sequences. According to the properties of H{·}, the discrete-time system might
be LTI. This way, it beneﬁts from a number of analysis and design tools such as frequency-domain
representations. However, there are many applications employing nonlinear, time-varying, and even
non-causal systems [2,5,7,8,11,12,16].
If the system is linear and time-invariant, as will be described in Chapter 2, there are several tools
to describe the mapping features of the system. Typically, a general description of discrete-time sys-
tems through a difference equation can be solved and analyzed by employing the z transform. Also a
x(n)
y(n)
n
n
H{·}
FIGURE 1.2
Discrete-time signal representation.

6
CHAPTER 1 Introduction to Signal Processing Theory
discrete-time LTI system is fully described by its impulse response, whereas a nonlinear system can not
be fully characterized by its impulse response.
The z transform is the key tool to represent a time-domain non-periodic function of a discrete
and integer variable through a frequency-domain non-periodic function of a continuous and complex
frequency variable. That is,
X(z) =
∞

n=−∞
x(n)z−n
⇐⇒
x(n) =
1
2πj

C
X(z)zn−1 dz.
The discrete-time Fourier transform (DTFT) represents a time-domain non-periodic function of a
discrete and integer variable by a periodic function of a continuous frequency variable in the frequency
domain as follows:
X(ejω) =
∞

n=−∞
x(n)e−jωn
⇐⇒
x(n) = 1
2π
 ∞
−∞
X(ejω)ejωn dω.
Finally, the discrete Fourier transform (DFT) is the right tool to represent a time-domain periodic
function of a discrete and integer variable through a periodic function of a discrete and integer frequency
variable in the frequency domain. The DFT is deﬁned as
X(k) =
N−1

n=0
x(n)e−j(2π/N)kn
⇐⇒
x(n) = 1
N
N−1

k=0
X(k)ej(2π/N)kn.
The DFT plays a key role in digital signal processing since it represents a ﬁnite-length sequence in
the time domain through a ﬁnite-length sequence in the frequency domain. Since both domains utilize
sequences, this feature makes the DFT a natural choice for time-frequency representation of information
in a digital computer. A little mental exercise allows us to infer that the DFT is the perfect representation
in the frequency domain of a ﬁnite-length sequence in the time domain, if we interpret the latter as a
period of a periodic inﬁnite-length sequence. In addition, by employing appropriate zero-padding in
two ﬁnite-length sequences, the product of their DFTs represents the DFT of their linear convolution
(see Chapter 3), turning the DFT a valuable tool for LTI system implementation. There is a plethora of
applications for the DFT in signal processing and communications and some of them can be accessed
in the references [2–13].
Often, whenever a DSP system is LTI, it can be described through a difference equation as follows:
N

i=0
ai y(n −i) +
M

l=0
blx(n −l) = 0.
The above description is suitable for implementation in digital computers. The difference equation
represents a causal LTI system if its auxiliary conditions correspond to its initial conditions and those
are zeros [2]. Assuming a0 = 1, the above equation can be rewritten as
y(n) = −
N

i=1
ai y(n −i) +
M

l=0
blx(n −l).

1.01.4 Random Signals and Stochastic Processes
7
This class of system has inﬁnite impulse response (i.e., y(n) ̸= 0 when n →∞) and, as such, it is
called IIR system or ﬁlter.
The nonrecursive system generates the output signal from past input samples, that is,
y(n) =
M

l=0
blx(n −l).
In this case, the resulting system has ﬁnite impulse response and is known as FIR systems or ﬁlters.
1.01.4 Random signals and stochastic processes
In most practical applications, we have to deal with signals that can not be described by a function
of time, since their waveforms follow a random pattern [17,18]. Take, for example, the thermal noise
inherent to any material. Despite the lack of exact knowledge of the signal values, it is possible to
analyze and extract information the signals contain by employing the mathematical tools available to
deal with random signals. Chapter 4 will present a compact and yet clarifying review of random signals
and stochastic processes which are crucial to understand several concepts that will be discussed in the
more advanced chapters.
The theory starts by deﬁning a random variable as a mapping of the result of a random process
experiment onto the set of numbers. A random variable X is a function that assigns a number x to every
outcome of a random process experiment. An example is given in Figure 1.3(a), in which each outcome
of a throw of a dice, numbers 1–6, corresponds to a determined value, x1–x6, respectively.
x1
X
x 1
x3
x2
x4
t
x1(t)
x2(t)
x3 (t)
xn (t)
t
t
t
{X }
(b) 
(a) 
FIGURE 1.3
Examples of random variable and stochastic process. (a) Random variable. (b) Stochastic process.

8
CHAPTER 1 Introduction to Signal Processing Theory
x (t)
y (t)
t
t
H{·}
FIGURE 1.4
Filtering of a random signal.
The stochastic process is a rule to describe the time evolution of the random variable depending on
the random process experiment, whereas the set of all experimental outcomes is its domain known as
the ensemble. An example is given in Figure 1.3(b), in which, at any time instant t0, the set formed
by the output samples of the stochastic process {X}, {x1(t0), x2(t0), x3(t0), . . . , xn(t0)}, represents a
random variable. A single outcome xi(t) of {X} is a random signal. Since random signals do not have
a precise description of their waveforms, we have to characterize them either via measured statistics
or through a probabilistic model. In general, the ﬁrst- and second-order statistics (mean and variance,
respectively) are sufﬁcient for characterization of the stochastic process, particularly due to the fact that
these statistics are suitable for measurements.
As an illustration, Figure 1.4 shows a random signal as an input of a highpass ﬁlter. We can observe
that the output signal is still random, but clearly shows a faster changing behavior given that the low
frequency contents of the input signal were attenuated by the highpass ﬁlter.
1.01.5 Sampling and quantization
A digital signal processing system whose signals originated from continuous-time sources includes
several building blocks, namely: an analog-to-digital (A/D) converter; a digital signal processing (DSP)
system; a digital-to-analog (D/A) converter; and a lowpass ﬁlter. Figure 1.5 illustrates a typical digital
signal processing setup where:
•
The A/D converter produces a set of samples in equally spaced time intervals, which may retain the
information of the continuous-time signal in the case the latter is band limited. These samples are
converted into a numeric representation, in order to be applied to the DSP system.
•
The DSP performs the desired mapping between the input and output sequences.
•
The D/A converter produces a set of equally spaced-in-time analog samples representing the DSP
system output.
•
The lowpass ﬁlter interpolates the analog samples to produce a smooth continuous-time signal.
Chapter 5 will discuss in detail the conditions which a continuous-time signal must satisfy so that its
sampled version retains the information of the original signal, dictated by the sampling theorem. This
theorem determines that a band limited continuous-time signal can be theoretically recovered from its
sampled version by ﬁltering the sequence with an analog ﬁlter with prescribed frequency response.
It is also important to mention that, while processing the signals in the digital domain, these are
subject to quantization errors, such as: roundoff errors originated from internal arithmetic operations

1.01.5 Sampling and Quantization
9
0 T 2T 
t
x(t)
n
n
t
t
x(t)
x(n)
x(n)
y(n)
y(n)
y (t)
i
y (t)
i
y(t)
y(t)
converter
A/D
converter
D/A
ﬁlter
Lowpass
system
DSP
0
...
12
...
12T
...
12
1 2
0 1 2
FIGURE 1.5
DSP system.
performed in the signals; deviations in the ﬁlter response due to ﬁnite wordlength representation of the
multiplier coefﬁcients inherent to the signal processing operation; and errors due to representation of
the acquired continuous-time signals with a set of discrete levels.
The actual implementation of a DSP system might rely on general purpose digital machines, where
the user writes a computer software to implement the DPS tasks. This strategy allows fast prototyping
and testing. Other mean is to use special-purpose commercially available CPUs, known as Digital Signal

10
CHAPTER 1 Introduction to Signal Processing Theory
x (t)
t
x (n)
n
n
xQ (n)
xQ (n)
n
t
x(t)
t
x(t)
(a)
(b)
(c)
(f)
(e)
(d)
ˆ
ˆ
FIGURE 1.6
A digital signal generation. (a) Original continuous-time signal. (b) A/D converter: sampling. (c) A/D converter:
quantization. (d) Digital signal. (e) D/A converter.(f) Recovered continuous-time signal.
Processors, which are capable of implementing sum of product operations in a very efﬁcient manner.
Yet another approach is to employ special purpose hardware tailored for the given application.
Figure 1.6 depicts a continuous-time signal, its digitized version, and its recovered continuous-time
representation. We can notice, from Figures 1.6(a) to 1.6(f), the effects of a low sampling frequency
and a small number of quantization steps on the A/D and D/A processes.
Figure 1.7 is a detailed example of the steps entailing an A/D and a D/A conversion, where in
Figure 1.7(a) a continuous-time signal is depicted along with its equally spaced samples. These samples
are then quantized as illustrates Figure 1.7(b). Assuming the quantized samples are converted into an
analog signal through a zero-order hold, the output of the D/A converter becomes as illustrated in Figure
1.7(c). The sampled-and-held continuous-time signal is lowpass ﬁltered in order to recover the original
continuous-time signal. As can be observed in Figure 1.7(d), the recovered signal resembles the original
where the difference originates from the quantization effects and the nonideal lowpass ﬁlter at the D/A
converter output.
1.01.6 FIR and IIR ﬁlter design
The general form of an FIR transfer function is given by
H(z) =
M

l=0
blz−l = H0z−M
M

l=0
(z −zl).
Since all the poles of the FIR transfer function are placed at z = 0, it is always stable. As a result,
the transfer function above will always represent a stable ﬁlter. The main feature of the FIR ﬁlters is

1.01.6 FIR and IIR Filter Design
11
0
0.02
0.04
0.06
0.08
0.1
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
time(s)
Amplitude
x(t)
x(n)
(a)
0
0.02
0.04
0.06
0.08
0.1
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
time(s)
Amplitude
x(t)
xQ(n)
(b)
0
0.02
0.04
0.06
0.08
0.1
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
time(s)
Amplitude
x(t)
xZOH(t)
0
0.02
0.04
0.06
0.08
0.1
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
time(s)
Amplitude
x(t)
xrec(t)
(d)
(c)
FIGURE 1.7
A real digital signal generation. (a) A/D converter: sampling. (b) A/D converter: quantization. (c) D/A converter:
zero-order hold. (d) D/A converter: lowpass ﬁlter.
the possibility of designing structurally induced linear-phase ﬁlters. On the other hand, an FIR ﬁlter
requires a higher number of multiplications, additions and storage elements when compared to their IIR
counterparts in order to satisfy a prescribed speciﬁcation for the magnitude response.
There are many methods to design an FIR ﬁlter [19], ranging from the simple ones, such as the
window and frequency sampling methods, to more efﬁcient and sophisticated, that rely on some
kind of optimization method [20–22]. The simple methods lead to solutions which are far from opti-
mum in most practical applications, in the sense that they either require higher order or cannot sat-
isfy prescribed speciﬁcations. A popular optimization solution is the minimax method based on the
Remez exchange algorithm, which leads to transfer functions with minimum order to satisfy prescribed
speciﬁcations.

12
CHAPTER 1 Introduction to Signal Processing Theory
0
500
1000
1500
2000
−120
−100
−80
−60
−40
−20
0
Frequency (Hz)
Magnitude response (dB)
Minimax method
WLS method
0
200
400
600
800
1200
−40
−30
−25
−20
−15
−10
−5
5
10
Frequency (Hz)
Magnitude response (dB)
Minimax method
WLS method
1
2
Αr
+ δp
−δp
f
f
(b)
(a)
FIGURE 1.8
Typical magnitude responses of FIR ﬁlters: Minimax and WLS. (a) Magnitude response. (b) Speciﬁcations
for a lowpass ﬁlter using minimax design.
Nowadays, with the growing computational power, it is possible to design quite ﬂexible FIR ﬁlters
by utilizing weighted least squares (WLS) solutions. These ﬁlters are particularly suitable for multirate
systems, since the resulting transfer functions can have decreasing energy in their stopband [2]. The WLS
method enables the design of ﬁlters satisfying prescribed speciﬁcations, such as the maximum deviation
in the passband and in part of the stopband, while minimizing the energy in a range of frequencies in the
stopband. As can be observed in Figure 1.8, for a given ﬁlter order, the minimax design leads to lower
maximum stopband attenuation, whereas the WLS solution leads to lower stopband energy. Note there
are solutions in between where some maximum values of stopband ripples can be minimized while
the energy of the remaining ones is also minimized. It is possible to observe that the WLS solution
does not satisfy the prescribed speciﬁcations given that it has the same ﬁlter order as the minimax
solution.
The typical transfer function of an IIR ﬁlter is given by
H(z) = Y(z)
X(z) =
M
l=0 blz−l
1 + N
i=1 aiz−i ,
which can be rewritten in a form explicitly showing the poles positions as
H(z) = H0
M
l=0 (1 −z−1zl)
N
i=0 (1 −z−1 pi)
= H0zN−M
M
l=0 (z −zl)
N
i=0 (z −pi)
.
IIR ﬁlters are usually designed by using well-established analog ﬁlter approximations, such as But-
terworth, Chebyshev, and elliptic methods (Figure 1.9) The prototype analog transfer function is then

1.01.7 Digital Filter Structures and Implementations
13
0
500
1000
1500
2000
−100
−90
−80
−70
−60
−50
−40
−30
−20
−10
0
Frequency (Hz)
Magnitude response (dB)
Elliptic approximation
Butherworth approx.
Chebyshev Type I approx.
FIGURE 1.9
Typical magnitude response of an IIR ﬁlter.
transformed into a digital transfer function by using an adequate transformation method. The most
widely used transformation methods are bilinear and the impulse invariance methods [2].
1.01.7 Digital ﬁlter structures and implementations
From a closer examination of the FIR and IIR transfer functions, one can infer that they can be realized
with three basic operators: adder, multiplier and delay (represented by z−1).
For example, Figure 1.10 shows a linear-phase FIR ﬁlter realization of odd order. Linear-phase ﬁlters
have symmetric or anti-symmetric impulse response and the symmetry should be exploited in order to
minimize the number of multipliers in the ﬁlter implementation, as illustrates Figure 1.10.
Figure 1.11 depicts a possible direct-form realization of an IIR transfer function. This realization
requires the minimum number of multiplications, adders and delays to implement the desired IIR transfer
function.
There are many alternative structures to implement the FIR and IIR ﬁlters. For example, IIR transfer
functions can be implemented as a product or as a summation of lower order IIR structures by starting
with their description as:
H(z) =
m

k=1
γ0k + γ1kz−1 + γ2kz−2
1 + m1kz−1 + m2kz−2
= h p
0 +
m

k=1
γ p
1kz + γ p
2k
z2 + m1kz + m2k
,
respectively. The right choice for a ﬁlter realization must take into consideration some issues, such as:
modularity, quantization effects, design simplicity, power consumption, etc. [2]. Chapter 6 will address

14
CHAPTER 1 Introduction to Signal Processing Theory
z −1
z −1
z −1
z −1
z −1
×
×
×
+
+
+
+
x(n)
y(n)
b0
b1
b2
FIGURE 1.10
Odd-order linear-phase FIR ﬁlter structure with M = 5.
advanced methods for FIR and IIR ﬁlter realization and implementation, where elegant and creative
solutions are introduced in a clear manner. This chapter will also discuss strategies to implement digital
ﬁlters efﬁciently.
1.01.8 Multirate signal processing
In digital signal processing it is easy to change the sampling rate of the underlying sequences by a ratio
of integer values. This feature is highly desirable whenever we want to merge information of systems
employing distinct sampling rates. In these cases, those rates should be made compatible [6,9,10].
Systems that internally employ multiple sampling rates are collectively referred to as multirate
systems. In most cases, these systems are time-varying or, at most, periodically time-invariant.
The basic operations of the multirate systems are the decimation and interpolation. The right compo-
sition of these operations allows arbitrary rational sampling rate changes. If this sampling rate change
is arbitrary and non-rational, the only solution is to recover the bandlimited continuous-time signal x(t)
from its samples x(m), and then re-sample it with a different sampling rate, thus generating a distinct
discrete-time signal x(n).
If we assume that a discrete-time signal signal x(m) was generated from an continuous-time signal
y(t) with sampling period T1, so that x(m) = y(mT1), with m = . . . , 0, 1, 2 . . ., the sampling theorem
requires that y(t) should be bandlimited to the range [−π
T1 , π
T1 ]. The sampled continuous-time signal is
given by:
y′(t) =
∞

m=−∞
x(m)δ(t −mT1).

1.01.8 Multirate Signal Processing
15
z −1
×
+
×
+
z −1
×
+
z −1
×
+
×
×
×
x(n)
b0
b1
b2
−a1
−a2
−aN
bM
y(n)
FIGURE 1.11
Direct-Form IIR ﬁlter structure, for M = N.
The spectrum of the sampled signal is periodic with period 2π
T1 . The original continuous-time signal
y(t) can be recovered from y′(t) through an ideal lowpass ﬁlter. This interpolation ﬁlter, whose impulse
response is denoted as h(t), has ideal frequency response H(ejω) as follows:
H(ejω) =
 1, ω ∈[−π
T1 , π
T1 ]
0,
otherwise,
so that
y(t) = y′(t) ∗h(t) = 1
T1
∞

m=−∞
x(m)sinc π
T1
(t −mT1).
By re-sampling y(t) with period T2, we obtain the discrete-time signal in the new desired sampling
rate as follows:
x(n) = 1
T1
∞

m=−∞
x(m)sinc π
T1
(nT2 −mT1),
where x(n) = y(nT2), with n = . . . , 0, 1, 2 . . ..
In this general equation governing sampling rate changes, there is no restriction on the values of T1
and T2. However, if T2 > T1 and aliasing is to be avoided, the interpolation ﬁlter must have a zero gain
for ω /∈[−π
T2 , π
T2 ].

16
CHAPTER 1 Introduction to Signal Processing Theory
In the case the sampling rate change corresponds to a ratio of integer numbers, all we need is simple
decimation and interpolation operations. The decimation operation, or down-sampling, consists of
retaining every Mth sample of a given sequence x(m). Since we are disposing samples from the original
sequence, either the signal is sufﬁciently limited in band or the signal has to be ﬁltered by a lowpass
ﬁlter so that the decimated signal retains the useful information of the original signal. Decimation is a
time-varying operation, since, if x(m) is shifted by m0, the output signal will, in general, differ from
the unshifted output shifted by m0. Indeed, decimation is a periodically time-invariant operation, which
consist of an extra degree of freedom with respect to the traditional time-invariant systems.
The interpolation, or up-sampling, of a discrete-time signal x(m) by a factor of L consists of inserting
L −1 zeros between each of its samples. In the frequency domain, the spectrum of the up-sampled
signal is periodically repeated. Given that the spectrum of the original discrete-time signal is periodic
with period 2π, the interpolated signal will have period 2π
L . Therefore, in order to obtain a smooth
interpolated version of x(m), the spectrum of the interpolated signal must have the same shape of the
spectrum of x(m). This can be obtained by ﬁltering out the repetitions of the spectra beyond [−π
L , π
L ].
Thus, the up-sampling operation is generally followed by a lowpass ﬁlter. The interpolation is only
periodically time-invariant and does not entail any loss of information.
Figure 1.12 illustrates discrete-time signal decimation and interpolation operations. As can be
observed in Figure 1.12(a), the decimation factor of two widens the spectrum of the sampled sig-
nal in comparison to the new sampling rate. Figure 1.12(b) depicts the effect of increasing the sampling
rate of a given sequence by two, where it can be seen that the frequency contents of the original signal
repeats twice as as often and appears narrower in the new sampling rate.
Chapter 7 will further discuss the theory and practice of multirate signal processing. This chapter will
show how sophisticated real-life signal processing problems can be addressed starting from the basic
theory. In particular, the authors address the design of ﬂexible communications systems incorporating
several advanced signal processing tools [4,23–25].
1.01.9 Filter banks and wavelets
In many applications it is desirable to decompose a wideband discrete-time signal into several non-
overlapping narrowband subsignals in order to be transmitted, quantized, or stored. In this case, each
narrow-band channel can have its sampling rate reduced, since the subband signals have lower bandwidth
than their originator signal. The signal processing tool that performs these tasks is the analysis ﬁlter
bank. The analysis ﬁlters, represented by the transfer functions Hi(z), for i = 0, 1, . . . , M −1, consist
of a lowpass ﬁlter H0(z), bandpass ﬁlters Hi(z), for i = 1, 2, . . . , M −2, and a highpass ﬁlter HM−1(z).
Ideally, these ﬁlters have non-overlapping passbands, whereas their spectrum combination should cover
the overall spectrum of the input signal. The outputs of the analysis ﬁlters, denoted as xi(n), for i =
0, 1, . . . , M−1, have together M times the number of samples of the original signal x(n). However, since
each subband signal occupies a spectrum band M times narrower than the input signal, they can be deci-
mated so the number of samples in the subbands does not increase. Indeed, if the input signal is uniformly
split into subbands, we can decimate each xi(n) by a factor of L smaller or equal to M without generating
unrecoverable aliasing effects. Figure 1.13 shows the general conﬁguration of a maximally (or critically)
decimated analysis ﬁlter, where the decimation factor is equal to the number of subbands, i.e., L = M.

1.01.9 Filter Banks and Wavelets
17
¯x(n)
m
x(m )
¯x(n)
M
0 1 2
12
X (ejω)
2π
−2π
p
−
p
X (ejω)
2π
−2π
2ωp
−2ωp
n
0
1
2
6
3
4
5
x(m )
(a) Signal decimation with M = 2.
2π
2
−4π
2
3
x(m )
5
4
n
x(m )
¯x(n)
L
0 1 2
12
X (ejω)
2π
−2π
ωp
−ωp
X (ej )
−2π
2
ωp
2
−ωp
2
¯x(n)
m
0
1
2
6
4π
2
(b) Signal interpolation with L = 2.
FIGURE 1.12
Signal decimation and interpolation. (a) Signal decimation with M = 2. (b) Signal interpolation with L = 2.
Whenever the input signal is split in subbands and decimated, if L ≤M, it is always possible to
recover the input signal by properly designing the analysis ﬁlters in conjunction with the synthesis ﬁlters
Gi(z), for i = 0, 1, . . . , M −1. The synthesis ﬁlters are placed after interpolators and they have the task
of smoothing the up-sampled signals. Figure 1.14 illustrates the general conﬁguration of the synthesis
ﬁlter bank. A key feature of the synthesis ﬁlter bank is to cancel out or reduce the aliasing effects.
If the signals in the subbands are not modiﬁed, the ﬁlter bank output y(n) can be a delayed copy
signal of x(n). The cascade connection of the analysis and synthesis ﬁlter banks satisfying this condition
is called a perfect reconstruction ﬁlter bank.
The cascade of an M-channel synthesis ﬁlter bank with an M-channel analysis ﬁlter bank gives
rise to a transmultiplex system. The transmultiplex model is widely used in communications to model
multiuser, multicarrier, and multiple access systems [4,23,24]. Chapter 7 will utilize multirate signal
processing and transmultiplexers to discuss a design procedure to be applied in cognitive radio.
Chapter 8 will present several methods for designing the analysis ﬁlters Hi(z) and the synthesis ﬁlters
Gi(z), such that perfect reconstruction, as well as other features, are met. This chapter will also discuss

18
CHAPTER 1 Introduction to Signal Processing Theory
M
M
M
M
H0(z)
H1(z)
H2(z)
HM −1(z)
|H 0(ejω)|
|H 1(ejω)|
|H 2(ejω)|
|H M −1(ejω)|
x(n)
x 0(m)
x 1(m)
x 2(m)
x M −1(m)
FIGURE 1.13
Analysis Filter Bank.
the design of ﬁlter banks with the subbands divided in octave bands, giving rise to the discrete-time
wavelet series.
Figure 1.15 shows a four-band ﬁlter bank design and its effect in a composed signal.
Figure 1.16 illustrates how a four-band synthesis ﬁlter bank performs the recomposition of the input
signal starting from the decimated subband signals.
1.01.10 Discrete multiscale and transforms
Discrete-time signals and systems can be characterized in the frequency domain by their Fourier trans-
forms. One of the main advantages of discrete-time signals is that they can be processed and represented

1.01.10 Discrete Multiscale and Transforms
19
π
π
π
π
+
G0(z)
G1(z)
G2(z)
GM −1(z)
ω
G0(ejω)
G1(ejω)
G2(ejω)
ω
ω
GM −1(ejω)
ω
x(n)
x 0(m)
x 1(m)
x 2(m)
x M −1(m)
M
M
M
M
FIGURE 1.14
Synthesis Filter Bank.
in digital computers. However, when we examine the deﬁnition of the discrete-time Fourier transform,
X(ejω) =
∞

n=−∞
x(n)e−jωn,
we notice that such a characterization in the frequency domain depends on the continuous variable ω.
This implies that the Fourier transform, as it is, is not suitable for the processing of discrete-time signals
in digital computers. We need a transform depending on a discrete-frequency variable that, if possible,
preserves the handy interpretations of the Fourier-based tools, which retains important information.
This can be obtained from the Fourier transform itself in a very simple way, by sampling uniformly
the continuous-frequency variable ω as long as the input sequence has ﬁnite length, otherwise the
time-domain signal will be affected by aliasing. Using this strategy it is possible to obtain a mapping
of a signal depending on a discrete-time variable n to a transform depending on a discrete-frequency

20
CHAPTER 1 Introduction to Signal Processing Theory
m
x 0(m)
Hi(ejω)
π
ω
H0
H1
H2
H3
π
4
π
2
3π
4
m
x 3(m)
m
x 2(m)
m
x 1(m)
4
4
4
4
n
y(n)
Analysis Filte r Bank
FIGURE 1.15
Analysis ﬁlter bank design.
variable k, leading to another interpretation for the DFT. Unfortunately, the DFT has its limitations in
representing more general class of signals as will be following discussed.
The wavelet transform of a function belonging to L2{R}, the space of the square integrable functions,
is the function decomposition in a basis composed by expansions, compressions, and translations of a
single mother function ψ(t), called wavelet.
The wavelet transform can be deﬁned as
x(t) =
∞

m=−∞
∞

n=−∞
cm,nψm,n(t)
cm,n =
 ∞
−∞
x(t)ψ∗
m,n(t) dt,
where
ψm,n(t) = 2−m/2ψ(2−mt −n)
ψm,n(t) = 2−m/2ψ(2−mt −n).
This pair of equations deﬁnes a biorthogonal wavelet transform which are characterized by two wavelets:
the analysis wavelet, ψ(t), and the synthesis wavelet, ψ(t). Any function x(t) ∈L2{R} can be

1.01.10 Discrete Multiscale and Transforms
21
x 2(m)
m
m
x 1(m)
m
x 0(m)
x 3(m)
m
n
x(n −N 0)
4
4
4
4
|Gi(ejω)|
G0
G1
G2
G3
4
2
3
4
Synthesis Filter Bank
+
x(n −N 0)
FIGURE 1.16
Synthesis ﬁlter bank design.
decomposed as a linear combination of contractions, expansions, and translations of the synthesis
wavelet ψ(t). The weights of the expansion can be computed via the inner product of x(t) with expan-
sions, contractions, and translations of the analysis wavelet ψ(t).
Functions ψm,n(t) do not comprise an orthogonal set, so neither do the functions ψm,n(t). However,
functions ψm,n(t) are orthogonal to ψm,n(t) so that
⟨ψm,n(t), ψk,l(t)⟩= δ(m −k)δ(n −l).
A byproduct of the development of the wavelet transforms is the so called wavelet series, allowing
the representation of signals in the time and frequency domains through sequences. The cascade of two-
band ﬁlter banks can produce many alternative maximally decimated decompositions. Of the particular
interest is the hierarchical decomposition achieving an octave-band decomposition, in which only the
lowpass band is further decomposed. This conﬁguration gives rise to the widely used wavelets series.
As will be discussed in Chapter 10, there are many transform-based tools in signal processing theory
available to meet the requirement of different applications. The classical Fourier based transforms have
been used for centuries, but their efﬁciency in dealing with nonstationary signals is limited. With wavelet
series, for example, it is possible to obtain good resolution in time and frequency domains, unlike the
Fourier-based analysis. Chapter 9 will include a myriad of transform solutions tailored for distinct
practical situations.
Figure 1.17 illustrates a wavelet series representation of a composed signal, where the input signal
is decomposed by an octave-band ﬁlter bank representing a wavelet series.

22
CHAPTER 1 Introduction to Signal Processing Theory
m
x 0(m)
Hi(ej )
π
H0
H1
H2
H3
π
4
π
2
m
x 3(m)
m
x 2(m)
m
x 1(m)
4
4
4
4
n
x(n)
Analysis Filter Bank
π
8
FIGURE 1.17
Wavelet series of a composed sinusoidal signal.
1.01.11 Frames
Frames consist of a set of vectors that enables a stable representation of a signal starting from an
analysis operation whereby a set of coefﬁcients is generated from the signal projection with the frame
vectors, and a synthesis operation where the synthesis vectors are linearly combined to approximate the
original signal. Chapter 10 will introduce the idea of frames by discussing the concepts of overcomplete
representations, frames and their duals, frame operators, inverse frames and frame bounds. In particular,
it will show how to implement signal analysis and synthesis employing frames generated from a ﬁxed
prototype signal by using translations, modulations and dilations; and analyzes frames of translates,
Gabor frames and wavelet frames. The frames representation is usually redundant by requiring more
coefﬁcients than the minimum, where redundancy is measured by frame bounds [13]. The end result
of employing the framework of frames is a set of signal processing tools such as: the Gaborgram;
time-frequency analysis; the matching pursuit algorithm, etc.
Figure 1.18 shows the representation of a composed signal into frames. As can be observed the
representation of x(n) by x1(n) and x0(n) is redundant, since x1(n) has the same rate as the input signal
x(n), whereas x0(n) has half rate.

1.01.12 Parameter Estimation
23
ω
X 0(ejω)
2π
π
π
2
ω
H (ejω)
π
ω
X 0(ejω)
2π
π
ω
X (ejω)
π
π
2
H (z)
H (z)
x(n)
2
2
ω
H (ejω)
π
π
2
2
G(z)
ω
ˆX (ejω)
π
π
2
ω
X 1(ejω)
π
π
2
+
+
−
x 1(n)
x 0(n)
π
2
ω
G(ejω)
π
FIGURE 1.18
Frame representation.
1.01.12 Parameter estimation
In many signal processing applications it is crucial to estimate the power spectral density (PSD) of a
given discrete-time signal. Examples of the use of PSD are vocal-track modeling, radar systems, antenna
array, sonar systems, synthesis of speech and music, just to name a few. Usually, the ﬁrst step in PSD
estimation consists of estimating the autocorrelation function associated with the given data, followed
by a Fourier transform to obtain the desired spectral description of the process.
In the literature we can ﬁnd a large number of sources dealing with algorithms for performing spectral
estimation. The alternative solutions differ in their computational complexity, precision, frequency
resolution, or other statistical aspects. In general, the spectral estimation methods can be classiﬁed
as nonparametric or parametric methods. The nonparametric methods do not prescribe any particular
structure for the given data, whereas parametric schemes assume that the provided process follows the
pattern of a prescribed model characterized by a speciﬁc set of parameters [14].
Sincetheparametricapproachesaresimplerandmoreaccurate,theywillbeemphasizedinChapter 11.
However, it should be mentioned that the parametric methods rely on some (a priori) information regard-
ing the problem at hand.
Figure 1.19 shows a noisy signal record and the estimated PSDs utilizing distinct methods. As can
be observed, some of the resulting PSDs expose the presence of a sinusoid, not clearly observed in the
time-domain signal.
1.01.13 Adaptive ﬁltering
In many practical situations we can extract information from a signal by comparing it to another signal
or a reference. For the cases where the speciﬁcations are neither known nor time-invariant, an adaptive
ﬁlter is the natural solution. The coefﬁcients of these ﬁlters are data driven and the resulting processing

24
CHAPTER 1 Introduction to Signal Processing Theory
n
x(n)
(a)
PSD 1(dB)
(b)
PSD 2(dB)
(c)
PSD 3(dB)
(d)
FIGURE 1.19
Spectral estimation. (a) Time-domain signal. (b) Estimated PSD1.(C) Estimated PSD2. (d) Estimated PSD3.
task does not meet the homogeneity and additive conditions of linear ﬁltering. An adaptive ﬁlter is also
time-varying since its parameters are continually changing in order to minimize a cost function.
Adaptive ﬁlters are self-designing and perform the approximation step on-line. Starting from an
initial condition, the adaptive ﬁlter searches an optimal solution in an iterative format. Since adaptive
ﬁlters are nonlinear ﬁlters, the analysis of their performance behavior requires the solution of time-
domain equations usually involving the second-order statistics of their internal and external quantities.
Although the analysis of an adaptive ﬁlter is more complicated than for ﬁxed ﬁlters, it is a simple
solution to deal with unknown stationary and nonstationary environments. On the other hand, since
the adaptive ﬁlters are self-designing ﬁlters, their design can be considered less involved than those of
standard ﬁxed digital ﬁlters [3]. An important topic related to adaptive ﬁltering is neural networks and
machine learning [26], not directly covered in this book.
Figure 1.20 depicts the general setup of an adaptive-ﬁltering environment, where, for a given iteration
n, an input signal x(n) excites the adaptive ﬁlter which will produce an output signal y(n) which is then
compared with a reference signal d(n). From this comparison, an error signal e(n) is formed according
to d(n) −y(n). The error signal is the argument of a cost (or objective) function whose minimization
is achieved by properly adapting the adaptive ﬁlter coefﬁcients. In Figure 1.21, it is shown a signal
enhancement where it is recognized in the error signal that the input signal contained a sinusoidal
component. In Figure 1.21 (a) it is difﬁcult to observe the presence of a sinusoidal signal whereas the
error signal clearly shows a nearly sinusoidal behavior.
Chapter 12 will present a comprehensive overview related to adaptive signal processing that uniﬁes
several concepts that go beyond the concepts covered in textbooks such as [3,27,28].

1.01.14 Closing Comments
25
W (z)
+
−
+
x(n)
d(n)
e(n)
FIGURE 1.20
General adaptive ﬁltering conﬁguration.
n
x(n)
(a)
n
e(n)
(b)
FIGURE 1.21
An adaptive ﬁltering application. (a) Input signal. (b) Error signal.
1.01.14 Closing comments
The theory of signal processing has been developing for over four decades at a fast pace. The following
chapters will cover many of the classical tools widely employed in applications that we perceive in our
daily life through the use of mobile phones, media players, medical equipments, transportation and so
on. These chapters will also cover recent advances and describe many new tools which can potentially
be utilized in new applications, as well as can be further investigated. The authors of the chapters are
frequent and important contributors to the ﬁeld of signal processing theory. The goal of each chapter is
to provide an overview and a brief tutorial of important topic pertaining to the signal processing theory,
including key references for further studies. The present chapter attempted to describe the context in
which each family of tools is employed.
The rest of this book is organized as follows:
•
Chapter 2 covers the basic concepts of Continuous-Time Signals and Systems highlighting the main
tools that can be employed to analyze and design such systems. Several examples are included to

26
CHAPTER 1 Introduction to Signal Processing Theory
illustrate the use of the tools in a full continuous-time environment. The content of this chapter is
also essential to make the connection between the continuous-time and the discrete-time systems.
•
Chapter 3 addresses Discrete-Time Signals and Systems emphasizing the basic analysis tools that
will be crucial to understand some more advanced techniques presented in the forthcoming chapters.
This chapter shows how the powerful state-space representation of discrete-time systems can be
employed as an analysis and design tool.
•
Chapter 4 on Random Signals and Stochastic Processes describes in a comprehensive way the
basic concepts required to deal with random signals. It starts with the concept of probability useful
to model chance experiments, whose outcomes give rise to the random variable. The time-domain
signals representing non-static outcomes are known as stochastic processes where their full deﬁnition
is provided. The chapter describes the tools to model the interactions between random signals and
linear-time invariant systems.
•
Chapter 5 covers Sampling and Quantization where the basic concepts of properly sampling a
continuous-time signal and its representation as a sequence of discrete-valued samples are addressed
in detail. The chapter discusses a wide range of topics rarely found in a single textbook such as:
uniform sampling and reconstruction of deterministic signals; the extension to sampling and recon-
struction of stochastic processes; time-interleaved ADCs for high-speed A/D conversion. In addition,
topics related to the correction to analog channel mismatches, principles of quantization, and over-
sampled ADCs and DACs are briey discussed. The chapter also includes the more advanced method
for discrete-time modeling of mixed-signal systems employed in 	
-modulator-based ADCs.
•
Chapter 6 takes us to a tour on design of FIR and IIR transfer functions of ﬁxed ﬁlters satisfy-
ing prescribed speciﬁcations, explaining their basic realizations and exploring more sophisticate
realizations. For IIR ﬁlters, the chapter introduces the concept of wave digital ﬁlter in a concise
and clear manner, providing the motivation for its conception from the analog ﬁlter structures. The
resulting IIR ﬁlter realizations keep the low sensitivity properties of their analog originators. For FIR
ﬁlter structures, the chapter covers the frequency masking approach that exploits redundancy in the
impulse response of typical FIR ﬁlters with high selectivity, aiming at reducing the computational
complexity. The chapter also explains in detail how to implement digital ﬁlters efﬁciently in speciﬁc
hardware by properly scheduling the arithmetic operations.
•
Chapter 7 on Multirate Signal Processing for Software Radio Architecture applies several concepts
presented in the previous chapters as tools to develop and conceive the implementation of radio
deﬁned by software, a subject of great interest in modern communications systems. Software deﬁned
radioentailsimplementingradiofunctionalityintosoftware,resultinginﬂexibleradiosystemswhose
main objective is to provide multi-service, multi-standard, multi-band features, all reconﬁgurable
by software. From the signal processing perspective, several basic concepts of multirate systems
such as, interpolation, decimation, polyphase decomposition, and transmultiplex play a central role.
The chapter reviews the required concepts of signal processing theory and propose a software based
radio architecture.
•
Chapter 8 on Modern Transform Design for Practical Audio/Image/Video Coding Applications
addresses the design of several applications-oriented transform methods. Most signal processing
textbooks present the classical transforms requiring large amount of multiplications, however the
demand for low-power multimedia platforms requires the development of computationally efﬁcient

References
27
transforms for coding applications. This chapter has the unique feature of presenting systematic
procedures to design these transforms while illustrating their practical use in standard codecs.
•
Chapter 9 entitled Discrete Multi-Scale Transforms in Signal Processing presents a deeper high level
exposition of the theoretical frameworks of the classical wavelets and the anisotropic wavelets. The
aim is to enable the reader with enough knowledge of the wavelet-based tools in order to exploit
their potential for applications in information sciences even further. Indeed the material covered in
this chapter is unique in the sense that every discrete multi-scale transform is linked to a potential
application.
•
Chapter 10 on Frames discusses the use of overcomplete signal representations employing frames
and their duals, frame operators, inverse frames, and frame bounds. The signal analysis and synthesis
using frame representation is also addressed in detail. In particular, the chapter emphasis is on frames
generated from a ﬁxed prototype signal by using translations, modulations and dilations, and analyzes
frames of translates, Gabor frames and wavelet frames. The chapter also presents signal analysis
techniques based on the Gaborgram and time-frequency analysis using frames and the matching
pursuit algorithm.
•
Chapter 11 on Parametric Estimation exploits the key engineering concept related to modeling
signals and systems, so that the model provides understanding of the underlying phenomena and
possibly their control. This chapter emphasizes the parametric models leading to simple estimation
of the parameters while exposing the main characteristics of the signals or systems under study.
By employing both statistical and deterministic formulations, the chapter explains how to generate
auto-regressive and moving average models which are then applied to solve problems related to
spectrum estimation, prediction, and ﬁltering.
•
Chapter 12 on Adaptive Filtering presents a comprehensive description of the current trends as well as
some open problems in this area. The chapter discusses the basic building blocks utilized in adaptive
ﬁltering setup as well as its typical applications. Then, the chapter discusses what are the optimal
solutions following by the derivation of the main algorithms. The authors confront for the ﬁrst time
two standard approaches to access the performance of the adaptive ﬁltering algorithms. The chapter
closes by discussing some current research topics and open problems for further investigation.
References
[1] A. Antoniou, On the Roots of Digital Signal Processing, IEEE Circuits Syst. Mag. 7 (1) (2007) 8–18; A.
Antoniou, IEEE Circuits Syst. Mag. 7 (4) (2007) 8–19.
[2] P.S.R. Diniz, E.A. Silva, S.L. Netto, Digital Signal Processing: System Analysis and Design, second ed.,
Cambridge University Press, Cambridge, UK, 2010.
[3] P.S.R. Diniz, Adaptive Filtering: Algorithms and Practical Implementation, fourth ed., Springer, New York,
NY, 2013.
[4] P.S.R. Diniz, W.A. Martins, M.V.S. Lima, Block Transceivers: OFDM and Beyond, Morgan & Claypool
Publishers, Ft, Collins, CO., 2012.
[5] A. Antoniou, Digital Signal Processing: Signals, Systems, and Filters, McGraw-Hill, New York, NY, 2006.
[6] R.E. Crochiere, L.R. Rabiner, Multirate Digital Signal Processing, Prentice-Hall, Englewood Cliffs, NJ, 1983.
[7] A.V. Oppenheim, A.S. Willsky, A.H. Nawab, Signals and Systems, second ed., Prentice-Hall, Englewood
Cliffs, NJ, 1997.

28
CHAPTER 1 Introduction to Signal Processing Theory
[8] A.V. Oppenheim, R.W. Schaffer, Discrete-Time Signal Processing, third ed., Prentice-Hall, Englewood Cliffs,
NJ, 2010.
[9] P.P. Vaidyanathan, Multirate Systems and Filter Banks, Prentice-Hall, Englewood Cliffs, NJ, 1993.
[10] M. Vetterli, J. Kovaˇcevi´c, Wavelets and Subband Coding, Prentice-Hall, Englewood Cliffs, NJ, 1995.
[11] J.G. Proakis, D.G. Manolakis, Digital Signal Processing: Principles, Algorithms, and Applications, fourth ed.,
Prentice-Hall, Englewood Cliffs, NJ, 2007.
[12] J.G. Proakis, D.G. Manolakis, Applied Digital Signal Processing, Cambridge University Press, Cambridge,
UK, 2011.
[13] S. Mallat, A Wavelet Tour of Signal Processing: The Sparse Way, third ed., Academic Press, Burlington, MA,
2009.
[14] S.M. Kay, Modern Spectral Estimation: Theory and Application, Prentice-Hall, Englewood Cliffs, NJ, 1988.
[15] J.H. McClellan, R.W. Schafer, M.A. Yoder, DSP First: A Multimedia Approach, third ed., Prentice-Hall,
Englewood Cliffs, NJ, 1998.
[16] S.K. Mitra, Digital Signal Processing: A Computer-Based Approach, third ed., McGraw-Hill, New York, NY,
2006.
[17] A. Papoulis, S. UnniKrishna Pillai, Probability, Random Variables, and Stochastic Processes, fourth ed.,
McGraw-Hill, New York, NY, 2002.
[18] J.J. Shynk, Probability, Random Variables, and Random Processes: Theory and Signal Processing Applica-
tions, John Wiley & Sons, Hoboken, NJ, 2013.
[19] T. Saramäki, Finite-impulse response ﬁlter design, in: S.K. Mitra, J.F. Kaiser (Eds.), Handbook of Digital
Signal Processing, John Wiley & Sons, New York, NY, pp. 155–177.
[20] R. Fletcher, Practical Methods of Optimization, John Wiley & Sons, Chichester, UK, 1980.
[21] D.G. Luenberger, Introduction to Linear and Nonlinear Programming, second ed., Addison-Wesley, Boston,
MA, 1984.
[22] A. Antoniou, W.-S. Lu, Practical Optimization: Algorithms and Engineering Applications, Springer, New
York, NY, 2013.
[23] D. Tse, P. Viswanath, Fundamentals of Wireless Communication, Cambridge University Press, Cambridge,
UK, 2005.
[24] P.P. Vaidyanathan, S.-M. Phoong, Y.-P. Lin, Signal Processing and Optimization for Transceiver Systems,
Cambridge University Press, Cambridge, UK, 2010.
[25] H.L. Van Trees, Optimum Array Processing, John Wiley & Sons, New York, NY, 2002.
[26] S. Haykin, Neural Networks and Learning Machines, Prentice-Hall, Englewood Cliffs, NJ, 2009.
[27] S. Haykin, Adaptive Filter Theory, fourth ed., Prentice-Hall, Englewood Cliffs, NJ, 2002.
[28] A.H. Sayed, Adaptive Filters, John Wiley & Sons, Hoboken, NJ, 2008.

2
CHAPTER
Continuous-Time Signals and
Systems
José A. Apolinário Jr. and Carla L. Pagliari1
Military Institute of Engineering (IME), Department of Electrical Engineering (SE/3), Rio de Janeiro, RJ, Brazil
Nomenclature
C
capacitance, in Faraday (F)
f
frequency, in cycles per second or Hertz (Hz)
L
inductance, in Henry (H)
R
resistance, in Ohms ()
t
time, in Second (s)
x(t) input signal of a given continuous-time system, expressed in volt (V)
when it corresponds to an input voltage; y(t) is usually used as the output
signal

angular frequency, in radian per second (rad/s) (it is sometimes referred
to as frequency although corresponding to 2π f )
1.02.1 Introduction
Most signals present in our daily life are continuous in time such as music and speech. A signal is
a function of an independent variable, usually an observation measured from the real world such as
position, depth, temperature, pressure, or time. Continuous-time signals have a continuous independent
variable. The velocity of a car could be considered a continuous-time signal if, at any time t, the velocity
v(t) could be deﬁned. A continuous-time signal with a continuous amplitude is usually called an analog
signal, speech signal being a typical example.
Signals convey information and are generated by electronic or natural means as when someone
talks or plays a musical instrument. The goal of signal processing is to extract the useful information
embedded in a signal.
Electronics for audio and last generation mobile phones must rely on universal concepts associated
with the ﬂow of information to make these devices fully functional. Therefore, a system designer, in
order to have the necessary understanding of advanced topics, needs to master basic signals and systems
theory. System theory could then be deﬁned as the relation between signals, input and output signals.
1Authors thank Prof. Ney Bruno for his kind and competent review of this chapter.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00002-4
© 2014 Elsevier Ltd. All rights reserved.
29

30
CHAPTER 2 Continuous-Time Signals and Systems
−1 −0.8 −0.6−0.4 −0.2
0.2 0.4 0.6 0.8
1
−1.5
−1
−0.5
0.5
1
1.5
2
2.5
3
x(t)
t
0
0
FIGURE 2.1
Example of a continuous-time signal.
As characterizing the complete input/output properties of a system through measurement is, in gen-
eral, impossible, the idea is to infer the system response to non-measured inputs based on a limited
number os measurements. System design is a chalenging task. However, several systems can be accu-
rately modeled as linear systems. Hence, the designer has to create between the input and the output, an
expected, or predictable, relationship that is always the same (time-invariant). In addition, for a range of
possible input types, the system should generate a predictable output in accordance to the input/output
relationship.
A continuous-time signal is deﬁned on the continuum of time values as the one depicted in
Figure 2.1, x(t) for t ∈R.
Although signals are real mathematical functions, some transformations can produce complex signals
that have both real and imaginary parts. Therefore, throughout this book, complex domain equivalents
of real signals will certainly appear.
Elementary continuous-time signals are the basis to build more intricate signals. The unit step signal,
u(t), displayed by Figure 2.2a is equal to 1 for all time greater than or equal to zero, and is equal to zero
for all time less than zero. A step of height K can be made with Ku(t).
A unit area pulse is pictured in Figure 2.2b; as τ gets smaller, the pulse gets higher and narrower
with a constant area of one (unit area). We can see the impulse signal, in Figure 2.2c, as a limiting
case of the pulse signal when the pulse is inﬁnitely narrow. The impulse response will help to estimate
how the system will respond to other possible stimuli, and we will further explore this topic on the next
section.
Exponential signals are important for signals and systems as eigensignals of linear time-invariant
systems. In Figure 2.2d, for A and α being real numbers, α negative, the signal is a decaying exponential.
When α is a complex number, the signal is called a complex exponential signal. The periodic signals
sine and cosine are used for modeling the interaction of signals and systems, and the usage of complex
exponentials to manipulate sinusoidal functions turns trigonometry into elementary arithmetic and
algebra.

1.02.2 Continuous-Time Systems
31
(a)
(b)
(c)
(d)
ΔT
1
ΔT
(t)
0
1
t
0
t
0
t
0
t
u
δ(t)
1
ΔT [u(t) −u(t −ΔT )]
x(t) = Ae αt, α < 0
A
FIGURE 2.2
Basic continuous-time signals: (a) unit-step signal u(t), (b) unit-area pulse, (c) impulse signal (unit-area
pulse when τ →0), and (d) exponential signal.
1.02.2 Continuous-time systems
We start this section by providing a simpliﬁed classiﬁcation of continuous-time systems in order to
focus on our main interest: a linear and time-invariant (LTI) system.
Why are linearity and time-invariance important? In general, real world systems can be successfully
modeled by theoretical LTI systems; in an electrical circuit, for instance, a system can be designed using
a well developed linear theory (valid for LTI systems) and be implemented using real components such
as resistors, inductors, and capacitors. Even though the physical components, strictly speaking, do not
comply with linearity and time-invariance for any input signals, the circuit will work quite well under
reasonable conditions (input voltages constrained to the linear working ranges of the components).
A system can be modeled as a function that transforms, or maps, the input signal into a new output
signal. Let a continuous-time system be represented as in Figure 2.3 where x(t) is the input signal and
y(t), its output signal, corresponds to a transformation applied to the input signal: y(t) = T {x(t)}.
The two following properties deﬁne a LTI system.
Linearity: A continuous-time system is said to be linear if a linear combination of input signals,
when applied to this system, produces an output that corresponds to a linear combination of individual
outputs, i.e.,
y(t) = T {a1x1(t) + a2x2(t) + · · · }
= a1T {x1(t)} + a2T {x2(t)} + · · ·
= a1y1(t) + a2y2(t) + · · · ,
(2.1)
where yi(t) = T {xi(t)}.

32
CHAPTER 2 Continuous-Time Signals and Systems
x(t)
y(t)
T {•}
FIGURE 2.3
A generic continuous-time system representing the output signal as a transformation applied to the input
signal: y(t) = T {x(t)}.
x(t)
y(t)
R
C
vR(t)
i(t)
A
B
C
D
FIGURE 2.4
An example of an electric circuit representing a continuous-time system with input x(t) and output y(t).
Time-invariance: A continuous-time system is said to be time-invariant when the output signal, y(t),
corresponding to an input signal x(t), will be a delayed version of the original output, y(t−t0), whenever
the input is delayed accordingly, i.e.,
if
y(t) = T {x(t)};
then y(t −t0) = T {x(t −t0)}.
The transformation caused in the input signal by a linear time-invariant system may be represented
in a number of ways: with a set of differential equations, with the help of a set of state-variables, with
the aid of the concept of the impulse response, or in a transformed domain.
Let the circuit in Figure 2.4 be an example of a continuous-time system where the input x(t) cor-
responds to the voltage between terminals A and B while its output y(t) is described by the voltage
between terminals C and D.

1.02.2 Continuous-Time Systems
33
We know for this circuit that x(t) = y(t)+vR(t) and that the current in the capacitor,i(t), corresponds
to C dy(t)/dt. Therefore, vR(t) = Ri(t) = RC dy(t)/dt, and we can write
RC dy(t)
dt
+ y(t) = x(t),
(2.2)
which is an input-output (external) representation of a continuous-time system given as a differential
equation.
Another representation of a linear system, widely used in control theory, is the state-space represen-
tation. This representation will be presented in the next section since it is better explored for systems
having a higher order differential equation.
In order to ﬁnd an explicit expression for y(t) as a function of x(t), we need to solve the differential
equation. The complete solution of this system, as shall be seen in the following section, is given by
the sum of an homogeneous solution (zero-input solution or natural response) and a particular solution
(zero-state solution or forced solution):
y(t) =
yh(t)

homogeneous solution
+
yp(t)
  
particular solution
.
(2.3)
The homogeneous solution, in our example, is obtained from (2.2) by setting RD dyh(t)/dt +
yh(t) = 0. Since this solution and its derivatives must comply with the homogeneous differential
equation, its usual choice is an exponential function such as Kest, s being, in general, a complex number.
Replacing this general expression in the homogeneous differential equation, we obtain RCs + 1 = 0
(characteristic equation) and, therefore, s = −1/RC, such that the homogenous solution becomes
yh(t) = Ke−t
RC ,
(2.4)
where K is a constant.
The particular solution is usually of the same form as the forcing function (input voltage x(t) in
our example) and it must comply with the differential equation without an arbitrary constant. In our
example, if x(t) = Me−mt, yp(t) would be
M
1−mRC e−mt.
If we set x(t) = u(t), the step function which can be seen as a DC voltage of 1 Volt switched at
time instant t = 0, the forced solution is assumed to be equal to one when t ≥0. For this particular
input x(t) = u(t), the output signal is given by y(t) = (Ke−t
RC + 1)u(t). Constant K is obtained
with the knowledge of the initial charge of the capacitor (initial conditions); assuming it is not charged
(vc(t) = 0, t ≤0), we have
y(t) = (1 −e−t
RC )u(t),
(2.5)
which can be observed in Figure 2.5.
Note that, for this case where x(t) = u(t), the output y(t) is known as the step response, i.e., r(t) =
y(t)|x(t)=u(t). Moreover, since the impulse δ(t) corresponds to the derivative of u(t), δ(t) = du(t)/dt,
and the system is LTI (linear and time-invariant), we may write that dr(t)/dt = dT {u(t)}/dt which
corresponds to T {du(t)/dt}, the transformation applied to the impulse signal leading to
h(t) = dr(t)
dt
,
(2.6)
h(t) known as the impulse response or h(t) = y(t)|x(t)=δ(t).

34
CHAPTER 2 Continuous-Time Signals and Systems
x (t )
y ( t )
0
0
1
1
t
t
FIGURE 2.5
Input and output signals of the system depicted in Figure 2.4. See Video 1 to watch animation.
The impulse response is, particularly, an important characterization of a linear system. It will help
to estimate how the system will respond to other stimuli. In order to show the relevance of this, let us
deﬁne the unit impulse as
δ(t) = lim
τ→0
1
τ [u(t) −u(t −τ)],
(2.7)
such that we may see an input signal x(t) as in Figure 2.6:
x(t) ≈
∞

k=−∞
x(kτ)
u(t −kτ) −u(t −kτ −τ)
τ




δ(t−kτ)
when
τ→0
τ.
(2.8)
The idea is to show that any signal, e.g., x(t), can be expressed as a sum of scaled and shifted impulse
functions.
In (2.8), making τ →0 and representing the resulting continuous time by τ instead of kτ, we
can use the integral instead of the summation and ﬁnd
x(t) =
	 ∞
−∞
x(τ)δ(t −τ)dτ.
(2.9)
In a LTI system, using the previous expression as an input signal to compute the output y(t) =
T {x(t)}, we obtain

 ∞
−∞x(τ)T {δ(t −τ)}dτ such that the output signal can be written as
y(t) =
	 ∞
−∞
x(τ)h(t −τ)dτ.
(2.10)
This expression is known as the convolution integral between the input signal and the system impulse
response, and is represented as
y(t) = x(t) ∗h(t).
(2.11)

1.02.2 Continuous-Time Systems
35
0
x(t)
x(0)
x(2Δτ)
t
Δτ
FIGURE 2.6
A signal x(t) can be obtained from the pulses if we make τ →0.
Please note that the output signal provided by the convolution integral corresponds to the zero-state
solution.
From Figure 2.6, another approximation for x(t) can be devised, leading to another expression
relating input and output signals. At a certain instant t = τ, let the angle of a tangent line to the curve
of x(t) be θ such that tan θ = dx(τ)
dτ . Assuming a very small τ, this tangent can be approximated by
x(τ)
τ
such that
x(τ) = dx(τ)
dτ
τ.
(2.12)
Knowing each increment at every instant t = kτ, we can visualize an approximation for x(t) as a
sum of shifted and weighted step functions u(t −kτ), i.e.,
x(t) ≈
∞

k=−∞
x(kτ)u(t −kτ) =
∞

k=−∞
dx(kτ)
dτ
u(t −kτ)τ.
(2.13)
From the previous expression, if we once more, as in (2.9), make τ →0 and represent the resulting
continuous-time by τ instead of kτ, we can drop the summation and use the integral to obtain
x(t) =
	 ∞
−∞
dx(τ)
dτ
u(t −τ)dτ.
(2.14)
Assuming, again, that the system is LTI, we use the last expression as an input signal to compute the
output y(t) = T {x(t)}, obtaining

 ∞
−∞
dx(τ)
dτ T {u(t −τ)}dτ which can be written as
y(t) =
	 ∞
−∞
dx(τ)
dτ
r(t −τ)dτ
(2.15)

36
CHAPTER 2 Continuous-Time Signals and Systems
1
1
0
0
h(τ)
h(τ)
1
RC
1
RC
τ
τ
t > 0
t > 0
u(t −τ)
u(t −τ)
FIGURE 2.7
Graphical interpretation of the convolution integral. See Video 2 to watch animation.
or, r(t) being the step-response, as
y(t) = dx(t)
dt
∗r(t).
(2.16)
In our example from Figure 2.4, taking the derivative of r(t) = (1 −e−t/RC)u(t), we obtain the
impulse response (the computation of this derivative is somehow tricky for we must bear in mind that
the voltage in the terminals of a capacitor cannot present discontinuities; just as in the case of the current
through an inductor) as:
h(t) =
1
RC e−t
RC .
(2.17)
In order to have a graphical interpretation of the convolution, we make x(t) = u(t) in (2.11) and use
r(t) = h(t) ∗u(t) =
	 ∞
−∞
h(τ)u(t −τ)dτ;
(2.18)
we then compute this integral, as indicated in Figure 2.7, in two parts:
for t < 0 : r(t) = 0;
for t > 0 : r(t) =
	 t
0
h(τ)dτ =
1
RC
	 t
0
e−τ
RC dτ.
(2.19)
Finally, from (2.19), we obtain r(t) =

1 −e−t/RC
u(t), as previously known.
A few mathematical properties of the convolution are listed in the following:
•
Commutative: x(t) ∗h(t) = h(t) ∗x(t).
•
Associative: [x(t) ∗h1(t)] ∗h2(t) = x(t) ∗[h1(t) ∗h2(t)].
•
Distributive: x(t) ∗[h1(t) + h2(t)] = x(t) ∗h1(t) + x(t) ∗h2(t).
•
Identity element of convolution: x(t) ∗δ(t) = x(t).
The properties of convolution can be used to analyze different system combinations. For example,
if two systems with impulse responses h1(t) and h2(t) are cascaded, the whole cascade system will
present the impulse response h1(t) ∗h2(t). The commutative property allows the order of the systems
of the cascade combination to be changed without affecting the whole system’s response.

1.02.3 Differential Equations
37
Two other important system properties are causability and stability.
Causability: A causal system, also known as non-anticipative system, is a system in which its output
y(t) depends only in the current and past (not future) information about x(t). A non-causal, or antici-
pative, system is actually not feasible to be implemented in real-life. For example, y(t) = x(t + 1) is
non-causal, whereas y(t) = x(t −1) is causal.
With respect to the impulse response, it is also worth mentioning that, for a causal system, h(t) = 0
for t < 0.
Stability: A system is stable if and only if every bounded input produces a bounded output, i.e., if
|x(t)| < Bx then |y(t)| < By for all values of t.
More about stability will be addressed in a forthcoming section, after the concept of poles and zeros
is introduced.
The classic texts for the subjects discussed in this section include [1–5].
1.02.3 Differential equations
In many engineering applications, the behavior of a system is described by a differential equation. A
differential equation is merely an equation with derivative of at least one of its variables. Two simple
examples follow:
a d2x
dt2 + bdx
dt + c = sin (t), and
(2.20)
∂2z
∂x2 + ∂2z
∂y2 = x2 + y2
(2.21)
where a, b, and c are constants.
In (2.20), we observe an ordinary differential equation, i.e., there is only one independent variable and
x = f (t), t being the independent variable. On the other hand, in (2.21), we have a partial differential
equation where z = f (x, y), x and y being two independent variables. Both examples have order 2
(highest derivative appearing in the equation) and degree 1 (power of the highest derivative term).
This section deals with the solution of differential equations usually employed to represent the
mathematical relationship between input and output signals of a linear system. We are therefore most
interested in linear differential equations with constant coefﬁcients having the following general form:
aoy + a1
dy
dt · · · + aN
d N y
dt N = b0x + b1
dx
dt + · · · + bM
d Mx
dt M .
(2.22)
The expression on the right side of (2.22) is known as forcing function, f (t). When f (t) = 0,
the differential equation is known as homogeneous while a non-zero forcing function corresponds to a
nonhomogeneous differential equation such as in
aoy + a1
dy
dt + · · · + aN
d N y
dt N = f (t).
(2.23)

38
CHAPTER 2 Continuous-Time Signals and Systems
As mentioned in the previous section, the general solution of a differential equation as the one in
(2.22) is given by the sum of two expressions: yH(t), the solution of the associated homogeneous
differential equation
aoy + a1
dy
dt + · · · + aN
d N y
dt N = 0,
(2.24)
and a particular solution of the nonhomogeneous equation, yP(t), such that
y(t) = yH(t) + yP(t).
(2.25)
Solution of the homogeneous equation: The natural response of a linear system is given by yH(t), the
solution of (2.24). Due to the fact that a linear combination of yH(t) and its derivatives must be equal
to zero in order to comply with (2.24), it may be postulated that it has the form
yH(t) = est,
(2.26)
where s is a constant to be determined.
Replacing the assumed solution in (2.24), we obtain, after simpliﬁcation, the characteristic (or aux-
iliary equation)
a0 + a1s + · · · + aNsN = 0.
(2.27)
Assuming N distinct roots of the polynomial in (2.27), s1 to sN, the homogeneous solution is obtained
as a linear combination of N exponentials as in
yH(t) = k1es1t + k2es2t + · · · + kNesN t.
(2.28)
Two special cases follow.
1. Non-distinct roots: if, for instance, s1 = s2, it is possible to show that es1t and tes2t are independent
solutions leading to
yH(t) = k1es1t + k2tes2t + · · · .
2. Characteristic equation with complex roots: it is known that, for real coefﬁcients (a0 to aN), all
complex roots will occur in complex conjugate pairs such as s1 = α + jβ and s2 = α −jβ, α and
β being real numbers. Hence, the solution would be
yH(t) = k1e(α+ jβ)t + k2e(α−jβ)t + · · ·
= [(k1 + k2) cos βt + j(k1 −k2) sin βt]eαt + · · ·
= (k3 cos βt + k4 sin βt)eαt + · · · ,
k3 and k4 being real numbers if we make k1 = k∗
2. In that case, we will have
yH(t) = k5 cos (βt −θ)eαt + · · · ,
with k5 =

k2
3 + k2
4 and θ = arctan k4
k3 . Also note from the last expression that, in order to have a
stable system (bounded output), α should be negative (causing a decaying exponential).

1.02.3 Differential Equations
39
Table 2.1 Assumed Particular Solutions to Common Forcing Functions
Forcing function f (t)
Assumed yP(t)
K
A0
Kt
A0t + A1
Ktn
A0tn + A1tn−1 + · · · + An−1t + An
Ke−αt
A0e−αt
sin t or cos t
A0 sin t + A1 cos t
e−αt sin t or e−αt cos t
e−αt (A0 sin t + A1 cos t)
vi(t)
vo(t)
t = 0
1V
R
L
C
i(t)
FIGURE 2.8
RLC series circuit with a voltage source as input and the voltage in the capacitor as output.
Solution of the nonhomogeneous equation: A (any) forced solution of the nonhomogeneous equation
must satisfy (2.23) containing no arbitrary constant. A usual way to ﬁnd the forced solution is employing
the so called method of undetermined coefﬁcients: it consists in estimating a general form for yP(t)
from f (t), the forcing function. The coefﬁcients (A0, A1, . . .), as seen in Table 2.1 that shows the
most common assumed solutions for each forced function, are to be determined in order to comply
with the nonhomogeneous equation. A special case is treated slightly differently: when a term of f (t)
corresponds to a term of yH(t), the corresponding term in yP(t) must be multiplied by t. Also, when
we ﬁnd non-distinct roots of the characteristic equation (assume multiplicity m as an example), the
corresponding term in yP(t) shall be multiplied by tm.
An example of a second order ordinary differential equation (ODE) with constant coefﬁcients is
considered as follows:
LC d2vo(t)
dt2
+ RC dvo(t)
dt
+ vo(t) = vi(t),
(2.29)
where R = 2.0 (in ), L = 2.0 (in H), C = 1/2 (in F), and vi(t) = e−t (in V).
This equation describes the input × output relationship of the electrical circuit shown in Figure 2.8
for t > 0.

40
CHAPTER 2 Continuous-Time Signals and Systems
Replacing the values of the components and the input voltage, the ODE becomes
d2vo(t)
dt2
+ dvo(t)
dt
+ vo(t) = e−t

f (t)
.
(2.30)
The associated characteristic equation s2 + s + 1 = 0 has roots s = −1± j
√
3
2
leading to an homoge-
neous solution given by
vo H(t) = k1 cos
√
3
2 t

e−t/2 + k2 sin
√
3
2 t

e−t/2
= k3 cos
√
3
2 t + k4

e−t/2.
(2.31)
Next, from the forcing function f (t) = e−t, we assume vo P(t) = k5e−t and replace it in (2.30),
resulting in k5 = 1 such that:
vo(t) = k1 cos
√
3
2 t

e−t/2 + k2 sin
√
3
2 t

e−t/2 + e−t
= k3 cos
√
3t
2
+ k4

e−t/2 + e−t,
(2.32)
where k1 and k2 (or k3 and k4) are constants to be obtained from previous knowledge of the physical
system, the RLC circuit in this example. This knowledge comes as the initial conditions: an N-order
differential equation having N constants and requiring N initial conditions.
Initial conditions: Usually, the differential equation describing the behavior of an electrical circuit
is valid for any time t > 0 (instant 0 assumed the initial reference in time); the initial conditions
corresponds to the solution (and its N −1 derivatives) at t = 0+. In the absence of pulses, the voltage
at the terminals of a capacitance and the current through an inductance cannot vary instantaneously
and must be the same value at t = 0−and t = 0+.
In the case of our example, based on the fact that there is a key switching vi(t) to the RLC series at
t = 0, we can say that the voltage across the inductor is vo(0−) = 1 (the capacitor assumed charged
with the DC voltage). Therefore, since the voltage across C and the current through L do not alter
instantaneously, we know that vo(0+) = 1 and i(0+) = 0. Since we know that i(t) = C dvo(t)
dt
, we have
dv0(t)
dt

t=0+ = 0. With these initial conditions and from (2.32), we ﬁnd k1 = 0 and k2 = 2
√
3
3 .
Finally, the general solution is given as
vo(t) = 1.1547 sin (0.866t)e−t/2 + e−t.
(2.33)
Figure 2.9 shows vo(t) for 0 ≤t ≤10. An easy way to obtain this result with Matlab© is:
> y=dsolve (’D2y+Dy+y=exp(-t)’,’y(0)=1’,’Dy(0)=0’);
> ezplot (y,[0 10])

1.02.4 Laplace Transform: Deﬁnition and Properties
41
0
2
4
6
8
10
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
t
vo(t)
FIGURE 2.9
Output voltage as a function of time, vo(t), for the circuit in Figure 2.8. See Video 3 to watch animation.
To end this section, we represent this example using the state-space approach. We ﬁrst rewrite (2.29)
using the ﬁrst and the second derivatives of vo(t) as ˙v and ¨v, respectively, and also u = vi(t) as in
LC ¨v + RC ˙v + v = u.
(2.34)
In this representation, we deﬁne a state vector x = [v
˙v]T and its derivative ˙x = [˙v
¨v]T , and from
these deﬁnitions we write the state equation and the output equation:
⎧
⎪⎪⎨
⎪⎪⎩
˙x = Ax + Bu =
 0
1
−1
LC
−R
L
  v
˙v

+
 0
1
LC

vi(t),
y = Cx + Du = [10]
v
˙v

+ 0vi(t),
(2.35)
where A is known as the state matrix, B as the input matrix, C as the output matrix, and D (equal to
zero in this example) would be the feedthrough (or feedforward) matrix.
For further reading, we suggest [6–9].
1.02.4 Laplace transform: deﬁnition and properties
The Laplace transform [10] is named after the French mathematician Pierre-Simon Laplace. It is an
important tool for solving differential equations, and it is very useful for designing and analyzing linear
systems [11].
The Laplace transform is a mathematical operation that maps, or transforms, a variable (or function)
from its original domain into the Laplace domain, or s domain. This transform produces a time-domain
response to transitioning inputs, whenever time-domain behavior is more interesting than frequency-
domain behavior. When solving engineering problems one has to model a physical phenomenon that

42
CHAPTER 2 Continuous-Time Signals and Systems
is dependent on the rates of change of a function (e.g., the velocity of a car as mentioned in Section
1.02.1). Hence, calculus associated with differential equations (Section 1.02.3), that model the phe-
nomenon, are the natural candidates to be the mathematical tools. However, calculus solves (ordinary)
differential equations provided the functions are continuous and with continuous derivatives. In addi-
tion, engineering problems have often to deal with impulsive, non-periodic or piecewise-deﬁned input
signals.
The Fourier transform, to be addressed in Section 1.02.7, is also an important tool for signal analysis,
as well as for linear ﬁlter design. However, while the unit-step function (Figure 2.2a), discontinuous
at time t = 0, has a Laplace transform, its forward Fourier integral does not converge. The Laplace
transform is particularly useful for input terms that are impulsive, non-periodic or piecewise-deﬁned [4].
The Laplace transform maps the time-domain into the s-domain, with s = σ + j, converting
integral and differential equations into algebraic equations. The function is mapped (transformed) to
the s-domain, eliminating all the derivatives. Hence, solving the equation becomes simple algebra in
the s-domain and the result is transformed back to the time-domain. The Laplace transform converts
a time-domain 1-D signal, x(t), into a complex representation, X(s), deﬁned over a complex plane
(s-plane). The complex plane is spanned by the variables σ (real axis) and  (imaginary axis) [5].
The two-sided (or bilateral) Laplace transform of a signal x(t) is the function L{x(t)} deﬁned by:
X(s) = L{x(t)} =
	 ∞
−∞
x(t)e−stdt.
(2.36)
The notation X(s) = L{x(t)} denotes that X(s) is the Laplace transform of x(t). Conversely, the
notation x(t) = L−1{X(s)} denotes that x(t) is the inverse Laplace transform of X(s). This relationship
is expressed with the notation x(t)
L
←→X(s).
As e−st = e−(σ+ j)t = e−σt( cos t −j sin t), Eq. (2.36) can be rewritten as
L{x(t)} = X(s)|s=σ+ j =
	 ∞
−∞
x(t)e−σt cos t dt −j
	 ∞
−∞
x(t)e−σt sin t dt.
(2.37)
This way, one can identify that the Laplace transform real part (Re(X(s)) represents the contribution of
the (combined) exponential and cosine terms to x(t), while its imaginary part (Im(X(s)) represents the
contribution of the (combined) exponential and sine terms to x(t). As the term e−st is an eigenfunction,
we can state that the Laplace transform represents time-domain signals in the s-domain as weighted
combinations of eigensignals.
For those signals equal to zero for t < 0, the limits on the integral are changed for the one-sided (or
unilateral) Laplace transform:
X(s) = L{x(t)} =
	 ∞
0−x(t)e−stdt,
(2.38)

1.02.4 Laplace Transform: Deﬁnition and Properties
43
with x(t) = 0 for t < 0−, in order to deal with signals that present singularities at the origin, i.e., at
t = 0.
As the interest will be in signals deﬁned for t ≥0, let us ﬁnd the one-sided Laplace transform of
x(t) = etu(t):
L{x(t)} = X(s) =
	 ∞
0−ete−stdt =
	 ∞
0−e(1−s)tdt =
1
1 −s e(1−s)t

∞
0−=
1
s −1
(2.39)
The integral, in (2.39), deﬁning L{x(t)} is true if e(1−s)t →0 as t →∞, ∀s ∈C with Re(s) > 1,
which is the region of convergence of X(s).
As the analysis of convergence, as well as the conditions that guarantee the existence of the Laplace
integral are beyond the scope of this chapter, please refer to [4,5].
For a given x(t), the integral may converge for some values of σ, Re(s = σ + j), but not for others.
So, we have to guarantee the existence of the integral, i.e., x(t)e−σt has to be absolutely integrable.
The region of convergence (ROC) of the integral in the complex s-plane should be speciﬁed for each
transform L{x(t)}, that exists if and only if the argument s is inside the ROC. The transformed signal,
X(s) will be well deﬁned for a range of values in the s-domain that is the ROC, which is always given
in association with the transform itself.
When σ = 0, so that s = j, the Laplace transform reverts to the Fourier transform, i.e., x(t) has a
Fourier transform if the ROC of the Laplace transform in the s-plane includes the imaginary axis.
For ﬁnite duration signals that are absolutely integrable, the ROC contains the entire s-plane. As
L{x(t)} = X(s) cannot uniquely deﬁne x(t), it is necessary X(s) and the ROC. If we wish to ﬁnd the
Laplace transform of a one-sided real exponential function x(t) = eatu(t), a ∈R, given by:
x(t) =
 0, t ≤0,
eat, t > 0,
(2.40)
we have
X(s) =
	 ∞
0−e−(s−a)tdt = −
1
s −a e−(s−a)t

∞
0−=
1
s −a .
(2.41)
The integral of (2.41) converges if σ > a, and the ROC is the region of the s-plane to the right of
σ = a, as pictured in Figure 2.10. If σ < a the integral does not converge as X(s) →∞, and if σ = a
we cannot determine X(s).
In other words, if a signal x(t) is nonzero only for t ≥0, the ROC of its Laplace transform lies to
the right hand side of its poles (please refer to Section 1.02.5). Additionally, the ROC does not contain

44
CHAPTER 2 Continuous-Time Signals and Systems
a
a
0
0
j
j
a >
a <
ROC
ROC
0
0
FIGURE 2.10
Region of Convergence (ROC) on the s-plane for the signal deﬁned by Eq. (2.40). Note that, for a stable
signal (a < 0), the ROC contains the vertical axis s = j.
any pole. Poles are points where the Laplace transform reaches inﬁnite value in the s-plane (e.g., s = a
in Figure 2.10).
An example of the two-sided Laplace transform of a right-sided exponential function x(t) =
e−atu(t), a ∈R, is given by
X(s) =
	 ∞
−∞
e−atu(t)e−st dt
=
	 ∞
0−e−(s+a)t dt
=
	 ∞
0−e−(σ+a)te−jt dt
= −
1
s + a e−(s+a)t

∞
0−=
1
s + a .
(2.42)
As the term e−jt is sinusoidal, only σ = Re(s) is important, i.e., the integral given by (2.42), when
t tends to inﬁnity, converges if e−(σ+a)t is ﬁnite (σ > −a) in order to the exponential function to decay.
The integral of Eq. (2.42) converges if Re(s + 1) > 0, i.e., if Re(s) > −a, and the ROC is the region of
the s-plane to the right of σ = −a, as pictured in the left side of Figure 2.11. The ROC of the Laplace
transform of a right-sided signal is to the right of its rightmost pole.

1.02.4 Laplace Transform: Deﬁnition and Properties
45
−a
−a
0
0
j
j
Re(s) >
a
< −
a
−
ROC
ROC
(a)
Re
(b)
(s)
FIGURE 2.11
Regions of Convergence (ROCs) on the s-plane: (a) for right-sided signals, Re(s) > −a, and (b) for left-sided
signals, Re(s) < −a.
If, we wish to ﬁnd the two-sided Laplace transform of a left-sided signal, x(t) = −e−atu(−t), a ∈R,
we have
X(s) = −
	 ∞
−∞
e−atu( −t)e−st dt
= −
	 0−
−∞
e−(s+a)t dt
=
1
s + a e−(s+a)t

0−
∞
=
1
s + a .
(2.43)
The integral of Eq. (2.43) converges if Re(s) < −a, and the ROC is the region of the s-plane to the
left of σ = −a, as pictured in the right side of Figure 2.11. The ROC of the Laplace transform of a
left-sided signal is to the left of its leftmost pole.
For for causal systems, where h(t) = 0, t < 0, and right-sided signals, where x(t) = 0, t < 0, the
unilateral (one-sided) and bilateral (two-sided) transforms are equal. However, it is important to stress
that some properties change, such as the differentiation property reduced to sX(s) for the bilateral case.
Conversely, other properties, such as convolution, hold as is, provided the system is causal and the input
starts at t = 0−.
Common Laplace transform pairs [12] are summarized in Table 2.2.
Recalling Figure 2.3 from Section 1.02.1, where x(t) is the input to a linear system with impulse
response h(t), to obtain the output y(t) one could convolve x(t) with h(t) (2.11).

46
CHAPTER 2 Continuous-Time Signals and Systems
Table 2.2 Laplace Transform Pairs
x(t) = L−1{X (s)}
X (s) = L{x(t)}
ROC
δ(t)
1
for all s
δ(t −τ), τ > 0
e−sτ
for all s
u(t)
1
s
Re(s) > 0
−u( −t)
1
s
Re(s) < 0
tu(t)
1
s2
Re(s) > 0
tnu(t)
n!
sn+1 , n = 1, 2, . . .
Re(s) > 0
e−at u(t)
1
s + a
Re(s) > −Re(a)
−e−at u( −t)
1
s + a
Re(s) < −Re(a)
tne−at u(t)
n!
(s + a)n+1 , n = 1, 2, . . .
Re(s) > −Re(a)
sin tu(t)

s2 + 2
Re(s) > 0
cos tu(t)
s
s2 + 2
Re(s) > 0
e−at sin tu(t)

(s + a)2 + 2
Re(s) > −Re(a)
e−at cos tu(t)
s + a
(s + a)2 + 2
Re(s) > −Re(a)
sin (t + θ)u(t)
s sin θ +  cos θ
s2 + 2
Re(s) > 0
cos (t + θ)u(t)
s cos θ − sin θ
s2 + 2
Re(s) > 0
e−at ( cos t −a sin t)u(t)
s
(s + a)2 + 2
Re(s) > −Re(a)
However, if X(s) and Y(s) are the associated Laplace transforms, the (computationally demanding)
operation of convolution in the time-domain is mapped into a (simple) operation of multiplication in
the s-domain):
y(t) = h(t) ∗x(t)
L
←→Y(s) = H(s)X(s).
(2.44)
Equation (2.44) shows that convolution in time-domain is equivalent to multiplication in Laplace
domain. In the following, some properties of the Laplace transform disclose the symmetry between
operations in the time- and s-domains.

1.02.4 Laplace Transform: Deﬁnition and Properties
47
Linearity: If x1(t)
L
←→X1(s), ROC = R1, and x2(t)
L
←→X2(s), ROC = R2, then
L{a1x1(t) + a2x2(t)} = a1L{x1(t)} + a2L{x2(t)} = a1X1(s) + a2X2(s),
(2.45)
with a1 and a2 being constants and ROC ⊇R1 ∩R2. The Laplace transform is a linear operation.
Time shifting: If x(t)
L
←→X(s), ROC = R, then x(t −t0)
L
←→e−st0 X(s), ∀t0 ∈R, ROC = R.
	 ∞
0−x(t −t0)e−st dt =
	 ∞
0−x(τ)e−s(τ+t0) dτ
= e−st0
	 ∞
0−x(τ)e−s(τ) dτ
= e−st0 X(s),
(2.46)
where t−t0 = τ. Time shifting (or time delay) in the time-domain is equivalent to modulation (alteration
of the magnitude and phase) in the s-domain.
Exponentialscaling(frequencyshifting):If x(t)
L
←→X(s),ROC = R,theneatx(t)
L
←→X(s−a), ROC =
R + Re(a), ∀a ∈R or C.
L{eatx(t)} =
	 ∞
0−eatx(t)e−stdt =
	 ∞
0−x(t)e−(s−a)tdt = X(s −a).
(2.47)
Modulation in the time-domain is equivalent to shifting in the s-domain.
Convolution: If x1(t)
L
←→X1(s), ROC = R1, and x2(t)
L
←→X2(s), ROC = R2, then x1(t) ∗
x2(t)
L
←→X1(s)X2(s), ROC ⊇R1 ∩R2:
	 ∞
0−[x1(t) ∗x2(t)]e−stdt =
	 ∞
0−
	 ∞
0−x1(τ)x2(t −τ)dτ

e−st dt
=
	 ∞
0−
	 ∞
0−x1(τ)x2(t −τ)e−s(t−τ+τ) dτdt
=
	 ∞
0−x1(τ)e−sτdτ
	 ∞
0−x2(υ)e−sυ dυ
= X1(s)X2(s),
(2.48)
where in the inner integral υ = 1 −τ. Convolution in time-domain is equivalent to multiplication in
the s-domain.
Differentiation in time: If x(t)
L
←→X(s), ROC = R, then ˙x(t)
L
←→sX(s) −x(0−), ROC ⊇R. Inte-
grating by parts, we have:
	 ∞
0−
dx(t)
dt
e−st dt = x(t)e−st∞
0−−
	 ∞
0−x(t)( −s)e−st dt
= 0 −x(0−) + s
	 ∞
0−x(t)e−st dt
= sX(s) −x(0−).
(2.49)

48
CHAPTER 2 Continuous-Time Signals and Systems
where any jump from t = 0−to t = 0+ is considered. In other words, the Laplace transform of a
derivative of a function is a combination of the transform of the function, multiplicated by s, and its
initial value. This property is quite useful as a differential equation in time can be turned into an algebraic
equation in the Laplace domain, where it can be solved and mapped back into the time-domain (Inverse
Laplace Transform).
The initial- and ﬁnal-value theorems show that, the initial and the ﬁnal values of a signal in the time-
domain can be obtained from its Laplace transform without knowing its expression in the time-domain.
Initial-value theorem: Considering stable the LTI system, that generated the signal x(t), then
lim
s→∞sX(s) = lim
t→0+ x(t).
(2.50)
From the differentiation in time property, we have that ˙x(t) ←→sX(s) −x(0−). By taking the limit
when s →∞of the ﬁrst expression of (2.49), we ﬁnd
lim
s→∞
	 ∞
0−
dx(t)
dt
e−st dt = lims→∞
	 0+
0−
dx(t)
dt
e0 dt +
	 ∞
0+
dx(t)
dt
e−st dt

= limt→0+x(t) −lim
t→0−x(t) + 0
= limt→0+x(t) −lim
t→0−x(t).
(2.51)
If we take the limit of s to ∞in the result of (2.49), we have
lim
s→∞(sX(s) −x(0−)) = lim
s→∞sX(s) −lim
t→0−x(t),
(2.52)
then we can equal the results of (2.52) and (2.51) and get
lim
s→∞sX(s) −lim
t→0−x(t) = lim
t→0+ x(t) −lim
t→0−x(t).
(2.53)
As the right-hand side of (2.53) is obtained taking the limit when s →∞of the result of (2.49), we
can see from (2.51) and (2.53) that
lim
s→∞sX(s) = lim
t→0+ x(t).
(2.54)
The initial-value theorem provides the behavior of a signal in the time-domain for small time intervals,
i.e., for t →0+. In other words, it determines the initial values of a function in time from its expression
in the s-domain, which is particularly useful in circuits and systems.
Final-value theorem: Considering that the LTI system that generated the signal x(t) is stable, then
lim
s→0 sX(s) = limt→∞x(t).
(2.55)
From the differentiation in time property, we have that ˙x(t)
L
←→sX(s) −x(0−). By taking the limit
when s →0 of the ﬁrst expression of (2.49), we have
lim
s→0
	 ∞
0−
dx(t)
dt
e−st dt =
	 ∞
0−
dx(t)
dt
dt
(2.56)
= lim
t→∞x(t) −lim
t→0−x(t).

1.02.4 Laplace Transform: Deﬁnition and Properties
49
If we take the limit when s →0 of the last expression of (2.49), we get
lim
s→0

sX(s) −x(0−)

= lim
s→0 sX(s) −lim
t→0−x(t);
(2.57)
then, we can write
lim
s→0 sX(s) −lim
t→0−x(t) = lim
t→∞x(t) −lim
t→0−x(t).
(2.58)
As the right-hand side of (2.58) is obtained taking the limit when s →0 of the result of (2.49), we
can see from (2.56) and (2.58) that
lim
s→0 sX(s) = lim
t→∞x(t).
(2.59)
The ﬁnal-value theorem provides the behavior of a signal in the time-domain for large time intervals,
i.e., for t →∞. In other words, it obtains the ﬁnal value of a function in time, assuming it is stable and
well deﬁned when t →∞, from its expression in the s-domain. A LTI system, as will be seen in the
next section, is considered stable if all of its poles lie within the left side of the s-plane. As t →∞, x(t)
must reach a steady value, thus it is not possible to apply the ﬁnal-value theorem to signals such as sine,
cosine or ramp.
A list of one-sided Laplace transform properties is summarized in Table 2.3.
The inverse Laplace transform is given by
L−1{X(s)} = x(t) =
	 σ+ j∞
σ−j∞
X(s)est ds.
(2.60)
The integration is performed along a line, parallel to the imaginary axis, (σ −j∞, σ + j∞) that lies
in the ROC. However, the inverse transform can be calculated using partial fractions expansion with
the method of residues. In this method, the s-domain signal X(s) is decomposed into partial fractions,
thus expressing X(s) as a sum of simpler rational functions. Hence, as each term in the partial fraction
is expected to have a known inverse transform, each transform may be obtained from a table like
Table 2.2. Please recall that the ROC of a signal that is non-zero for t ≥0 is located to the right hand
side of its poles. Conversely, for a signal that is non-zero for t ≤0, the ROC of its Laplace transform
lies to the left-hand side of its poles. In other words, if X(s) = 1/(s + a), a ∈R, its inverse Laplace
transform is given as follows:
L−1

1
s + a

=
−e−at, Re(s) < −a,
e−at, Re(s) > −a,
(2.61)
In practice, the inverse Laplace transform is found recursing to tables of transform pairs (e.g.,
Table 2.2). Given a function X(s) in the s-domain and a region of convergence, its inverse Laplace
transform is given by L−1{X(s)} = x(t) such that X(s) = L{x(t)}, s ∈ROC.
An immediate application of the Laplace transform is on circuit analysis. Assuming that all initial
conditions are equal to zero, i.e., there is no initial charge on the capacitor, the response y(t) of the
circuit with input given by x(t) = u(t) displayed by Figure 2.4 could be obtained in the Laplace domain
to be further transformed into the time-domain.

50
CHAPTER 2 Continuous-Time Signals and Systems
Table 2.3 Properties of (One-Sided) Laplace Transform
Property
time-domain
s-domain
ROC
x(t)
X (s)
R
x1(t)
X1(s)
R1
x2(t)
X2(s)
R2
Linearity
a1x1(t) + a2x2(t)
a1X1(s) + a2X2(s)
⊇R1 ∩R2
Scalar
ax(t)
aX (s)
R
Multiplication
Scaling
x(at)
1
a X
 s
a

R
|a|
Time Shifting
x(t −t0)
e−st0X (s)
R
Exponential
eat x(t)
X (s −a)
R + Re(a)
Scaling
Differentiation
dx(t)
dt
sX (s) −x(0−)
⊇R
d 2x(t)
dt 2
s2X (s) −sx(0−) −˙x(0−)
d nx(t)
dt n
snX (s) −
n
k=1
sn−k x(k−1)(0−)
Differentiation
−tx(t)
dX (s)
ds
R
(in the s-domain)
Convolution
x1(t) ∗x2(t)
X1(s)X2(s)
⊇R1 ∩R2
Convolution
x1(t)x2(t)
1
2πj X1(s) ∗X2(s)
⊇R1 ∩R2
(in the s-domain)
First, the circuit elements are transformed from the time domain into the s-domain, thus creating an
s-domain equivalent circuit. Hence, the RLC elements in the time domain and s-domain are:
Resistor (voltage-current relationship):
v(t) = Ri(t)
L
←→V (s) = RI(s).
(2.62)
The equivalent circuit is depicted in Figure 2.12.
Inductor (initial current i(0−)):
v(t) = L di(t)
dt
L
←→V (s) = sLI(s) −Li(0−);
(2.63)
applying the differentiation property (Table 2.3) leads to
I(s) = 1
sL V (s) + i(0−)
s
.
(2.64)

1.02.4 Laplace Transform: Deﬁnition and Properties
51
v(t)
i(t)
R
R
A
A
B
B
V (s)
I (s)
FIGURE 2.12
Equivalent circuit of a resistor in the Laplace domain.
The equivalent circuit is depicted in Figure 2.13. The inductor, L, is an impedance sL in the s-domain
in series with a voltage source, Li(0−), or in parallel with a current source, i(0−)
s
.
Capacitor (initial voltage v(0−)):
i(t) = C dv(t)
dt
L
←→I(s) = sCV (s) −v(0−);
(2.65)
applying the differentiation property (Table 2.3) leads to
V (s) = I(s)
sC + v(0−)
s
.
(2.66)
The capacitor, C, is an impedance
1
sC in the s-domain in series with a voltage source, v(0−)
s
, or in
parallel with a current source, Cv(0−). The voltage across the capacitor in the time-domain corresponds
to the voltage across both the capacitor and the voltage source in the frequency domain.
As an example, the system’s response given by the voltage across the capacitor, y(t), depicted in
Figure 2.4, is obtained in the Laplace domain as follows (see Figure 2.14):
X(s) = 1
s = I(s)

R + 1
sC

.
(2.67)
From (2.65), we have
Y(s) = I(s)
sC
(2.68)
v(t)
i(t)
i(0 −)
L
A
A
A
B
B
B
V(s)
V(s)
I(s)
I (s)
Li(0 −)
sL
sL
i (0 −)
s
FIGURE 2.13
Equivalent circuit of an inductor in the Laplace domain.

52
CHAPTER 2 Continuous-Time Signals and Systems
v(t)
v(0 −)
i(t)
C
A
A
A
B
B
B
V (s)
V (s)
I (s)
I (s)
v (0 −)
s
1
sC
1
sC
Cv (0 −)
FIGURE 2.14
Equivalent circuit of a capacitor in the Laplace domain.
with
I(s) =
X(s)
R + 1
sC
.
(2.69)
Substituting (2.69) in (2.68) and applying the inverse Laplace transform (Table 2.2) and and the
linearity property (Table 2.3), we get
L−1{Y(s)} = L−1

1
s −
1
s +
1
RC

,
y(t) = u(t) −e−t
1
RC u(t),
(2.70)
which is the same result presented in (2.5).
Another example, where the Laplace transform is useful, is the RLC circuit displayed by Figure 2.8
where the desired response is the voltage across the capacitor C, vo(t) = vC(t). Note that the initial
conditions are part of the transform, as well as the transient and steady-state responses. Given the input
signal vi(t) = e−tu(t) with initial conditions vo(0−) = 1, ˙vo(0−) = 0, and io(0−) = 0, applying the
Kirchhoff’s voltage law to the circuit, we have:
vi(t) = vR(t) + vL(t) + vC(t).
(2.71)
Directly substituting (2.62), (2.63) and (2.65) in (2.71), and applying the Laplace transform to the
input signal, vi(t) = e−tu(t) , we get
1
s + 1 =

RI(s) + sLI(s) −Lio(0−) + 1
sC I(s) + vo(0−)
s

= I(s)

R + sL + 1
sC

−Lio(0−) + vo(0−)
s
= I(s)
sC (s2LC + sRC + 1) −Lio(0−) + vo(0−)
s
,
(2.72)

1.02.4 Laplace Transform: Deﬁnition and Properties
53
which, from (2.65), I(s)
sC = Vo(s) −vo(0−)
s
, we have
1
s + 1 =

Vo(s) −vo(0−)
s

(s2LC + sRC + 1) −Lio(0−)vo(0−)
s
= Vo(s)(s2LC + sRC + 1) −vo(0−)
s
(s2LC + sRC + 1)
−Lio(0−) + vo(0−)
s
.
(2.73)
Substituting the values of R = 2.0 (in ), L = 2.0 (in H), C = 1/2 (in F), the equation becomes
1
s + 1 = Vo(s)(s2 + s + 1) −vo(0−)
s
(s2 + s + 1)
−io(0−) + vo(0−)
s
;
(2.74)
considering that the initial conditions are v0(0−) = 1, ˙vo(0−) = 0, io(0−) = 0, we get
1
s + 1 = Vo(s)(s2 + s + 1) −1
s (s2 + s + 1) −0 + 1
s
= Vo(s)(s2 + s + 1) −s −1 −1
s −0 + 1
s
= Vo(s)(s2 + s + 1) −s −1
Vo(s) =

1
s + 1 + s + 1
 
1
s2 + s + 1

=
s2 + 2s + 2
s + 1
 
1
s2 + s + 1

.
(2.75)
After decomposing (2.75) into partial fractions and ﬁnding the poles and residues, the inverse Laplace
transform is applied in order to ﬁnd the expression of vo(t):
Vo(s) =
1
(s + 1) −j
√
3
3

s + 1
2 −j
√
3
2
 + j
√
3
3

s + 1
2 + j
√
3
2

(2.76)
L−1{Vo(s)} = L−1
⎧
⎨
⎩
1
(s + 1) −j
√
3
3

s + 1
2 −j
√
3
2
 + j
√
3
3

s + 1
2 + j
√
3
2

⎫
⎬
⎭.
(2.77)
Applying the linearity property (Table 2.3) and and recursing to the Laplace transform pair table
(Table 2.2), we get
vo(t) = e−t −j
√
3
3 e−t
2 + jt
√
3
2 + j
√
3
3 e−t
2 −jt
√
3
2 ,
(2.78)

54
CHAPTER 2 Continuous-Time Signals and Systems
considering that the inverse Laplace transforms of
1
s+a , for Re(s) > −Re(a), given by Table 2.2, with
a = 1
2 −j
√
3
2 and a = 1
2 + j
√
3
2 are
L−1
⎧
⎨
⎩
1

s + 1
2 −j
√
3
2

⎫
⎬
⎭= e
−

1
2 −j
√
3
2

tand,
L−1
⎧
⎨
⎩
1

s + 1
2 + j
√
3
2

⎫
⎬
⎭= e
−

1
2 + j
√
3
2

t,
respectively.
Algebraically manipulating (2.78) and substituting the terms of complex exponentials by trigono-
metric identities, we have (for t ≥0)
vo(t) = 2
√
3
2 e−t
2 sin
√
3
2

+ e−t
(2.79)
= 1.1547 sin (0.866t)e−t
2 + e−t.
The solution given by (2.79) in the same given by (2.33). However, in the solution obtained using
the Laplace transform, the initial conditions were part of the transform. We could also have applied the
Laplace transform directly to the ODE (2.30) assisted by Tables 2.3 and 2.2, as follows:
L{e−t} = L
d2vo(t)
dt2
+ dvo(t)
dt
+ vo(t)

,
(2.80)
where
L
d2vo(t)
dt2

= s2Vo(s) −svo(0−) −˙vo(0−),
L
dvo(t)
dt

= sVo(s) −vo(0−),
L{vo(t)} = Vo(s), and
L{e−t} =
1
s + 1.
(2.81)

1.02.5 Transfer Function and Stability
55
Hence, substituting the expressions from (2.81) into (2.80) and considering the initial conditions, we
obtain the same ﬁnal expression presented in (2.75):
1
s + 1 = Vo(s)

s2 + s + 1

−svo(0−) −˙vo(0−) −vo(0−)
= Vo(s)(s2 + s + 1) −s −0 −1
= Vo(s)(s2 + s + 1) −s −1
Vo(s) =

1
s + 1 + s + 1
 
1
s2 + s + 1

=
s2 + 2s + 2
s + 1
 
1
s2 + s + 1

.
(2.82)
In Section 1.02.2 the importance of working with a LTI system was introduced. The convolution
integral in (2.9), that expresses the input signal, x(t), as a sum of scaled and shifted impulse functions,
uses delayed unit impulses as its basic signals. Therefore, a LTI system response is the same linear
combination of the responses to the basic inputs.
Considering that complex exponentials, such as e−st, are eigensignals of LTI systems, Eq. (2.36) is
deﬁned if one uses e−st as a basis for the set of all input functions in a linear combination made of inﬁnite
terms (i.e., an integral). As the signal is being decomposed in basic inputs (complex exponentials), the
LTI system’s response could be characterized by weighting factors applied to each component in that
representation.
In Section 1.02.7, the Fourier transform is introduced and it is shown that the Fourier integral does not
converge for a large class of signals. The Fourier transform may be considered a subset of the Laplace
transform, or the Laplace transform could be considered a generalization (or expansion) of the Fourier
transform. The Laplace integral term e−st, from (2.36), forces the product x(t)e−st to zero as time t
increases. Therefore, it could be regarded as the exponential weighting term that provides convergence
to functions for which the Fourier integral does not converge.
Following the concept of representing signals as a linear combination of eigensignals, instead of
choosing e−(σ+ j)t as the eigensignals, one could choose e−jt as the eigensignals. The latter repre-
sentation leads to the Fourier transform equation, and any LTI system’s response could be characterized
by the amplitude scaling applied to each of the basic inputs e−jt.
Laplace transform and applications are discussed in greater detail in [2,4,5,8,10,11].
1.02.5 Transfer function and stability
The Laplace transform, seen in the previous section, is an important tool to solve differential equations
and therefore to obtain, once given the input, the output of a linear and time invariant (LTI) system.
For a LTI system, the Laplace transform of its mathematical representation, given as a differential
equation—with constant coefﬁcients and null initial conditions—as in (2.22) or, equivalently, given by
a convolution integral as in (2.10), leads to the concept of transfer function, the ratio
H(s) = Y(s)
X(s)
(2.83)

56
CHAPTER 2 Continuous-Time Signals and Systems
between the Laplace transform of the output signal and the Laplace transform of the input signal. This
representation of a LTI system also corresponds to the Laplace transform of its impulse response, i.e.,
H(s) = L{h(t)}.
Assuming that all initial conditions are equal to zero, solving a system using the concept of transfer
function is usually easier for the convolution integral is replaced by a multiplication in the transformed
domain:
Y(s) = L{h(t) ∗x(t)} = H(s)X(s)
(2.84)
such that
y(t) = L−1{H(s)X(s)},
(2.85)
the zero-state response of this system.
A simple example is given from the circuit in Figure 2.4 where
H(s) =
1/RC
s + 1/RC .
If x(t) = u(t), X(s) = 1/s, and
Y(s) = H(s)X(s) =
1/RC
(s + 1/RC)s
= 1
s −
1
s + 1/RC .
Therefore, y(t) = L−1{Y(s)} corresponds to
y(t) = u(t) −e−1
RC tu(t)
which is the same solution given in (2.5).

1.02.5 Transfer Function and Stability
57
Given (2.10) and provided that all initial conditions are null, the transfer function H(s) corresponds
to a ratio of two polynomials in s:
H(s) = P(s)
Q(s).
(2.86)
The roots of the numerator P(s) are named zeros while the roots of the denominator Q(s) are known
as poles. Both are usually represented in the complex plane s = σ + j and this plot is referred to as
pole-zero plot or pole-zero diagram.
We use the ODE (2.29) to provide an example:
H(s) = Vo(s)
Vi(s) =
1
LCs2 + RCs + 1.
(2.87)
Using the same values, R = 2.0 (in ), L = 2.0 (in H), C = 1/2 (in F), we obtain the poles (roots
of the characteristic equation) s = −1± j
√
3
2
as seen in Figure 2.15.
−
3
2 j
3
2 j
0
j
−1
2
FIGURE 2.15
An example of a pole-zero diagram of the transfer function in (2.87) for R = 2.0 (in ), L = 2.0 (in H),
C = 1/2 (in F). The gray curve corresponds to the root-locus of the poles when R varies from R = 0 (poles
at ±j) to R = 4.1 (poles at −1.25 and at −0.8). See Video 4 to watch animation.

58
CHAPTER 2 Continuous-Time Signals and Systems
For a LTI system, a complex exponential est can be considered an eigensignal:
x(t) = est →y(t) = est ∗h(t) =
	 ∞
−∞
h(τ)es(t−τ)dτ
= est
	 ∞
−∞
h(τ)e−sτdτ



H(s)
= H(s)est = P(s)
Q(s)est,
(2.88)
which is valid only for those values of s where H(s) exists.
If there is no common root between P(s) and Q(s), the denominator of H(s) corresponds to the
characteristicpolynomialand,therefore,poles pi ofaLTIsystemcorrespondtotheirnaturalfrequencies.
As mentioned in Section 1.02.2, the stability of a system, in a bounded-input bounded-output (BIBO)
sense, implies that if |x(t)| < Bx then |y(t)| < By, Bx and By < ∞, for all values of t. This corresponds
to its input response being absolutely integrable, i.e.,
	 ∞
−∞
|h(t)|dt = ∥h(t)∥1 < ∞.
(2.89)
As seen previously, the natural response of a linear system corresponds to a linear combination of
complex exponentials epit. Let us assume, for convenience, that H(s) has N distinct poles and that the
order of P(s) is lower than the order of Q(s); in that case, we can write
h(t) = L−1

P(s)
!N
i=1 (s −pi)

=
N

i=1
Aiepitu(t),
(2.90)
where constants Ai are obtained from simple partial fraction expansion.
In order for the system to be stable, each exponential should tend to zero as t tends to zero. Con-
sidering a complex pole pi = σi + ji, limt→∞|h(t)| →0 should imply that limt→∞|e(σi+ ji)t| =
limt→∞|eσit| = 0 or σi < 0; that is, the real part of pi should be negative.
While the location of the zeros of H(s) is irrelevant for the stability of a LTI system, their poles are
of paramount importance. We summarize this relationship in the following:
1. A causal LTI system is BIBO stable if and only if all poles of its transfer function have negative real
part (belong to the left-half of the s-plane).
2. A causal LTI system is unstable if and only if at least one of the following conditions occur: one
pole of H(s) has a positive real part and repeated poles of H(s) have real part equal to zero (belong
to the imaginary axis of the s-plane).
3. A causal LTI system is marginally stable if and only if there are no poles on the right-half of the
s-plane and non-repeated poles occur on its imaginary axis.

1.02.5 Transfer Function and Stability
59
0
10 20 30 40 50 60 70 80
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
0 10 20 30 40 50 60 70 80 90100
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
0 10 20 30 40 50 60 70 80 90100
−600
−400
−200
0
200
400
600
50 55 60 65 70 75 80 85 90 95
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2 x 10
4
jΩ
σ
stable region
unstable region
marginal stability
FIGURE 2.16
Pole location on the s-plane and system stability: examples of impulse responses of causal LTI systems. Visit
the “exploring the s-plane” http://www.jhu.edu/signals/explore/index.html website.
We can also state system stability from the region of convergence of H(s): a LTI system is stable if
and only if the ROC of H(s) includes the imaginary axis (s = j). Figure 2.16 depicts examples of
impulse response for BIBO stable, marginally stable, and unstable systems.
In this section, we have basically addressed BIBO stability which, although not sufﬁcient for asymp-
totic stability (a system can be BIBO stable without being stable when initial conditions are not null), is
usually employed for linear systems. A more thorough stability analysis could be carried out by using
Lyapunov criteria [13].
The Laplace transform applied to the input response of the differential equation provides the transfer
function which poles (roots of its denominator) determines the system stability: a causal LTI system is
said to be stable when all poles lie in the left half of the complex s-plane. In this case, the system is
also known as asymptotically stable for its output always tend to decrease, not presenting permanent
oscillation.
Whenever distinct poles have their real part equal to zero (poles on the imaginary axis), permanent
oscillation will occur and the system is marginally stable, the output signal does not decay nor grows
indeﬁnitely. Figure 2.16 shows the impulse response growing over time when two repeated poles are
located on the imaginary axis. Although not shown in Figure 2.16, decaying exponential will take place
of decaying oscillation when poles are real (and negative) while growing exponentials will appear for
the case of real positive poles.
Although we have presented the key concepts of stability related to a linear system, much more could
be said about stability theory. Hence, further reading is encouraged: [2,13].

60
CHAPTER 2 Continuous-Time Signals and Systems
1.02.6 Frequency response
We have mentioned in the previous section that the complex exponential est is an eigensignal of a
continuous-time linear system. A particular choice of s being j0 = j2π f0, i.e., the input signal
x(t) = e j0t = cos (0t) + j sin (0t)
(2.91)
has a single frequency and the output, from (2.88), is
y(t) = e j0t
	 ∞
−∞
h(τ)e−j0τdτ



H( j0)
.
(2.92)
Being e j0t an eigensignal, H( j0) in (2.92) corresponds to its eigenvalue. Allowing a variable
frequency  = 2π f instead of a particular value o = 2π f0, we deﬁne the frequency response of a
linear system having impulse response h(t) as
H( j) =
	 ∞
−∞
h(τ)e−jτdτ.
(2.93)
We note that the frequency response corresponds to the Laplace transform of h(t) on a speciﬁc region
of the s-plane, the vertical (or imaginary) axis s = j:
H( j) = H(s)|s= j.
(2.94)
It is clear from (2.93) that the frequency response H( j) of a linear system is a complex function
of . Therefore, it can be represented in its polar form as
H( j) = |H( j)|e∠H( j).
(2.95)
As an example, we use the RLC series circuit given in Figure 2.17 with L = 1.0 (in H), C = 1.0 (in
F), and R varying from 0.2 to 1.2 (in ).
The frequency response, magnitude or absolute value (in dB) and argument or phase (in radians),
are depicted in Figure 2.18. For this example, we consider the voltage applied to the circuit as input and
the voltage measured at the capacitor as output. This is worth mentioning since the output could be, for
instance, the current in the circuit or the voltage across the inductor.
In low frequencies, the capacitor tends to become an open circuit such that vo(t) = vi(t) and the
gain tends to 1 or 0 dB. On the other hand, as the frequency  goes towards inﬁnity, the capacitor
tends to become a short circuit such that vo(t) goes to zero, gain in dB tending to minus inﬁnity.
The circuit behaves like a low-pass ﬁlter allowing low frequencies to the output while blocking high
frequencies. Specially, when R has low values (when tending to zero), we observe a peak in |H( j)|
at  ≈0 = 1 rad/s. This frequency, 0 = 2π f0, is termed the resonance frequency and corresponds
to the absolute value of the pole responsible for this oscillation.

1.02.6 Frequency Response
61
vi(t)
vo(t)
R
L
C
FIGURE 2.17
RLC series circuit: the transfer function is H(s) = Vo(s)
Vi(s) , Vo(s) = L{v0(t)} and Vi(s) = L{vi(t)}, and the
frequency response is given by H(j) = H(s)|s=j.
0
1
2
3
4
5
−30
−25
−20
−15
−10
−5
0
5
10
15
0
1
2
3
4
5
−1
−0.8
−0.6
−0.4
−0.2
0
H(jΩ)/π
|H(jΩ)|dB
Ω (in rad/s)
Ω (in rad/s)
FIGURE 2.18
Frequency response in magnitude (|H(j)| in dB) and normalized phase (argument of H(j) divided
by π, i.e., −1 corresponds to −π) for the circuit in Figure 2.17 with L = 1 (in H), C = 1 (in F),
and R varying from 0.2 (in ) (highlighted highest peak in magnitude) to 1.2 (in ). Download the
http://www.ime.eb.br/∼apolin/CTSS/Figs18and19.m Matlab© code.
Let us write the frequency response from H(s), as given in (2.87):
H( j) = H(s)|s= j =
1
(1 −2LC) + jRC .
(2.96)

62
CHAPTER 2 Continuous-Time Signals and Systems
0
10
20
30
40
50
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
h(t)
t(in s)
0
Real Part
Imaginary Part
jΩ
σ
FIGURE 2.19
Impulse responses and poles location in the s-plane (6 different values of R) for the circuit in Figure 2.17.
Note that, for lower values of R,h(t) tends to oscillate and the poles are closer to the vertical axis (marginal
stability region). Download the http://www.ime.eb.br/∼apolin/CTSS/Figs18and19.m Matlab© code.
We note that, when R →0, the peak amplitude of |H( j)| occurs for 1 −2LC →0 (it tends to
inﬁnity as R goes to zero), i.e., the resonance frequency corresponds to 0 =
1
√
LC .
The impulse response as well as the pole diagram are shown in Figure 2.19; from this ﬁgure, we
observe the oscillatory behavior of the circuit as R tends to zero in both time- and s-domain (poles
approaching the vertical axis). The impulse response and the poles location for the minimum value of
the resistance, R = 0.2 (in )), are highlighted in this ﬁgure.
Before providing more details about the magnitude plot of the frequency response as a function
of , let us show the (zero state) response of a linear system to a real single frequency excitation
x(t) = A cos (t + θ). We know that the complex exponential is an eigensignal to a linear system such
that, making s = j,
y(t)|x(t)=e jt = H( j)e jt.
(2.97)
Since we can write A cos (t + θ) as A(e j(t+θ) + e−j(t+θ))/2, the output shall be given as
y(t) = A
2 e jθe jt H( j) + A
2 e−jθe−jt H( −j).
(2.98)
Assumingthat h(t)isreal,itispossibletoassurethatallpolesof H(s)willoccurincomplexconjugate
pairs, leading (as shall be also addressed in the next section) to H∗( j) = H( −j). This conjugate
symmetry property of the frequency response, i.e., |H( j)| = |H( −j)| and ̸ H( j) = (̸ −H( j)),
when applied in (2.98), results in
y(t)|x(t)=A cos (t+θ) = |H( j)|A cos (t + θ + ̸ H( j)).
(2.99)

1.02.6 Frequency Response
63
|H(jΩref)|dB
|H(jΩref)|dB−40
Ωref
10Ωref
FIGURE 2.20
Asymptotical decay of 40 dB per decade.
The previous expression is also valid as regime solution when the input is a sinusoid, that is non-zero
for t ≥0, such as x(t) = A cos (t + θ)u(t).
Now, back to our example of a frequency response given in (2.96), let us express its magnitude
squared:
|H( j)|2 =
1
1 + 2(R2C2 −2LC) + 4L2C2 .
(2.100)
It is easy to see that, when  tends to zero, the magnitude squared tends to one while, when  tends
to inﬁnity, the dominant term becomes 4L2C2:
(a)  →0 ⇒|H( j)| →1 or 0dB;
(b)  →∞⇒|H( j)| →
1
2LC or, in dB, −40 log (/0), 0 =
1
√
LC .
These two regions of |H( j)|, close to zero and tending to inﬁnity, could be visualized by lines
in a semi-log plot, the frequency axis, due to its large range of values, is plotted using a logarithmic
scale. Particularly, when  →∞, the approximation in (b) tells us that, in an interval from ref
to 10ref , ref ≫0 (the resonance frequency), we have an attenuation of 40 dB as shown in
Figure 2.20.
Magnitude in dB and phase of H( j) as a function of  = 2π f , plotted in a logarithmic frequency
scale, is known as Bode Plots or Bode Diagrams. A Bode Diagram may be sketched from lines (asymp-
totes) which are drawn from the structure of H(s), its poles and zeros. For the case of a transfer function
with two conjugate complex roots, we have
H(s) =
2
0
s2 + 2ζ0s + 2
0
,
(2.101)
where, in our example in (2.87), 0 =
1
√
LC and ζ =
R
2

C
L . Also note that (in order to have two
conjugate complex roots) 0 < ζ < 1.

64
CHAPTER 2 Continuous-Time Signals and Systems
10
−2
10
−1
10
0
10
1
10
2
−80
−70
−60
−50
−40
−30
−20
−10
0
10
20
30
Ω in rad/s
Ω0
|H(jΩ)|dB
Low frequency asymptote: 0dB
High frequency asymptote: −40dB per decade
FIGURE 2.21
Example of Bode Plot for H(s) =
2
o
s2+2ζ0s+2o with 0 = 1 and ζ = 0.05. Note that, for this case, the
approximated height of the peak −20 log (2ζ) = 20 dB works well.
The magnitude of the frequency response in dB is given as
|H( j)|dB = −10 log
⎛
⎝

1 −
 
0
22
+

2ζ 
0
2
⎞
⎠.
(2.102)
When  = 0, |H( j)| = −20 log (2ζ); this value being a good approximation for the peak
magnitude (see Figure 2.21) when ζ tends to zero (we have an error lower than 0.5% when ζ < 0.1).
The peak actually occurs in  = 0
&
1 −2ζ 2, having a peak height equal to
1
2ζ√
1−ζ 2 ; we could also
add that the peak is basically observable only when 0 < ζ < 0.5. The Bode Plot of (2.87), with R = 0.1
(in ), L = 1.0 (in H), and C = 1.0 (in F) showing the (low frequency and high frequency) asymptotes
is depicted in Figure 2.21.
Another example with ﬁrst order poles and zeros will end our discussion on Bode Plots. Let transfer
function H(s), with poles at −1 and −10,000, and zeros at −10 and −1000, be represented as follows:
H(s) =
s2 + 1.01s + 10000
s2 + 10, 001s + 10000
= (s + 1, 000)(s + 10)
(s + 10000)(s + 1)
=

1 +
s
1,000
 
1 + s
10


1 +
s
10,000
 
1 + s
 .
(2.103)

1.02.6 Frequency Response
65
Table 2.4 Asymptotes for the Bode Plot of Figure 2.22
 (rad/s)
event
asymptotic |H(j)|dB(dB)
1
New asymptote starts (−20 dB per decade)
0
10
New asymptote starts (+20 dB per decade)
−20
1000
New asymptote starts (+20 dB per decade)
−20
10,000
New asymptote starts (−20 dB per decade)
0
Ω in rad/s
1
10
1,000
10,000
|H(jΩ)|dB
0dB
≈−3dB
≈−17dB
−20dB
Asymptote: 0dB per decade
Asymptote: 0dB per decade
Asymptote: −20dB per decade
Asymptote: +20dB per decade
Asymptote: 0dB per decade
FIGURE 2.22
Bode Magnitude (dB) Plot for H(s) =

1+
s
1,000

1+ s
10


1+
s
10,000

1+ s
1
. Download the http://www.ime.eb.br/∼apolin/CTSS/
Fig22.m Matlab© code.
The magnitude in dB of its frequency response could then be written as
|H( j)|dB = 20 log
1 + j

1, 000
 + 20 log
1 + j 
10

−20 log
1 + j

10, 000
 −20 log |1 + j| .
(2.104)
The ﬁrst term, as in the previous example, can be checked for low and high frequencies:
(a)  ≪1000 ⇒0dB;

66
CHAPTER 2 Continuous-Time Signals and Systems
(b)  ≫1000 ⇒increases 20 dB per decade (from ref to 10ref ) or, equivalently, 6 dB per
octave (from ref to 2ref ).
When  = 1000, the value of this term is 10 log (2) = 3.0103 dB which is the usual error at the these
frequencies (cutoff frequencies).
We could, from the analysis of the ﬁrst term, extend the results to all cutoff frequencies as shown in
Table 2.4. The resulting asymptotic sketch and real curves are shown in Figure 2.22 where we observe
the 3 dB errors at the cutoff frequencies.
A ﬁnal observation is regarded to the phase of the frequency response: similarly to the magnitude, a
sketch of the phase can also be obtained from the poles and zeros of H(s):
̸ H( j) = ̸ 1+ j/1000 + ̸ 1+ j/10 −̸ 1+ j/10,000 −̸ 1+ j.
(2.105)
The ﬁrst term on the right hand side of (2.105), ̸ 1+ j/1000= arctan (/1000), can be checked for
low and high frequencies:
(a)  ≪1000 ⇒arctan (/1000) ≈0;
(b)  ≫1000 ⇒arctan (/1000) ≈90◦.
Considering only this zero (corresponding cutoff frequency 0 = 1000 rad/s), one may assume
0.10 (100 rad/s) and 100 (10, 000 rad/s) sufﬁciently small and sufﬁciently large such that an
asymptote could be drawn between points (102, 0◦) and (104, 90◦) as in Figure 2.23. For a pole, the
asymptote is mirrored with respect to 0◦.
10
1
10
2
10
3
10
4
10
5
−10
0
10
20
30
40
50
60
70
80
90
100
Asymptote
Real phase
Ω in rad/s
arctan(Ω/1,000)in degrees
FIGURE 2.23
Asymptotical behavior of the phase for a single zero.

1.02.7 The Fourier Series and the Fourier Transform
67
10
−1
10
0
10
1
10
2
10
3
10
4
10
5
−100
−80
−60
−40
−20
0
20
40
60
80
100
Ω in rad/s
H(jΩ) in degrees
FIGURE 2.24
Bode Phase Plot for H(s) =

1+
s
1,000

1+ s
10


1+
s
10,000

1+ s
1
. Lighter gray dashed lines represent the asymptotes and the
darker dashed line corresponds to the asymptotic sketch (sum of the asymptotes); the real phase is the
continuous curve in black. Download the http://www.ime.eb.br/∼apolin/CTSS/Fig24.m Matlab© code.
The asymptotic sketch and real phase curves for (2.105) is shown in Figure 2.24; note that we have
two negative slope asymptotes, starting in  = 10−1 rad/s and  = 103 rad/s, and two positive slope
asymptotes, starting at 100 rad/s and 102 rad/s.
The topics discussed in this section are found in the following books: [1,3,8,14,15].
1.02.7 The Fourier series and the Fourier transform
In electronic circuits we often deal with periodic and non-periodic signals. In addition, circuits could
be driven by non-sinusoidal functions. Hence, methods that decompose continuous-time periodic (non-
sinusoidal) and non-periodic signals into contributions from sinusoidal (sin (t) and cos (t)) signals
are extremely valuable for electronic system analysis; this is due to the fact that a LTI system only
changes the amplitude and phase of a real sinusoid. Several properties make sinusoids an ideal choice to
be the elementary basis functions for signal analysis and synthesis. For example, Eq. (2.99) shows that
the LTI system changed the amplitude to |H( j)|A and the phase to θ+∠H( j) of the given sinusoidal
input, x(t) = A cos (t + θ). In addition, as sine and cosine are mutually orthogonal, sinusoidal basis
functions are independent and can be processed independently. Usually, any sinusoidal function, such
as the one mentioned above, can be expressed as a linear combination of sine and cosine functions:
x(t) = C cos (t) −D sin (t). If a signal is a sum of sinusoids, such as
x(t) = A1 cos (1t + θ1) + A2 cos (2t + θ2)
(2.106)

68
CHAPTER 2 Continuous-Time Signals and Systems
its impulse response, considering it is a linear system, would be given by
y(t) = A1|H( j1)| cos (1t + θ1 + ̸ H( j1)) + A2|H( j2)| cos (2t + θ2 + ̸ H( j2))
(2.107)
A periodic signal, x(t) = x(t +nT ), ∀n = ±1, ±2, ±3, . . ., with period T, may contain components
at frequencies  = 2πn/T , where the fundamental frequency is given by 0 = 2π f0, f0 = 1/T . The
frequencies n0 are the harmonics, and any pair of signals are harmonically related if their periods are
related by a integer ratio. In general, the resulting signal from a sum of harmonically related waveforms
is also periodic, and its repetition period is equal to the fundamental period. The harmonic components
exist only at discrete frequencies.
Consider that we add the harmonically related signals, xn(t), with 0 = 2π/T , over T = 1 such
that x(t) = 1
2 + 2
π sin (2πt) +
2
3π sin (6πt) +
2
5π sin (10πt) +
2
7π sin (14πt) + · · ·, and get xN(t) as
the result when 1 ≤N ≤5 and when N →∞, meaning that inﬁnite terms are added:
xN(t) = 1
2 +
N

n=1
xn(t) = 1
2 + 2
π
N

n=1
1
2n −1 sin (2π(2n −1)t).
(2.108)
The resulting waveforms, xN(t) for 1 ≤N ≤5 and xN(t) when N is large, i.e., N →∞, are
displayed by Figure 2.25. It is clear that as the number of terms becomes inﬁnite in (2.108) the result
converges at every value of the period T (to the square wave) except at the discontinuities. Actually, the
ringing effect at the discontinuities (not shown in Figure 2.25 for the curve with the large value of N)
never dies out as N becomes larger, yet reaching a ﬁnite limit (Gibbs phenomenon).
Hence, for a period T, for a fundamental frequency 0 with frequencies that are integer multiples
of the fundamental frequency such that n0, ∀n ∈Z (the set of all integers), the representation of the
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
x N (t)
t
N = 5
N →∞
FIGURE 2.25
Synthesized square wave from Eq. (2.108).

1.02.7 The Fourier Series and the Fourier Transform
69
harmonic components, xn(t), of a signal x(t) can be written as
xn(t) = An sin (n0t + θn)
= an cos (n0t) + bn sin (n0t)
= 1
2(an −jbn)e jn0t + 1
2(an + jbn)e−jn0t.
(2.109)
Equation (2.109) presents three equivalent forms to express the harmonic components xn(t): the ﬁrst
one is written as a sine function with amplitude An and phase θn; the second one as sum of sine and
cosine functions with real coefﬁcients an and bn, respectively; and the last form writes xn(t) in terms
of complex exponentials.
The derivation, providing proofs, of the expressions for computing the coefﬁcients in a Fourier series
is beyond the scope of this chapter. Hence, we simply state, without proof, that if x(t) is periodic with
period T and fundamental frequency 0, any arbitrary real periodic signal x(t) could be represented by
the inﬁnite summation of harmonically related sinusoidal components, known as the Fourier series:
x(t) = 1
2a0 +
∞

n=1
An sin (n0t + θn)
= 1
2a0 +
∞

n=1
(an cos (n0t) + bn sin (n0t)),
(2.110)
where for n = 0, we have the term a0
2 corresponding to the mean value of the signal, with frequency of
0 Hz, known as the DC value (or level), and the other terms are as follows:
an = An sin (θn),
bn = An cos (θn),
An =

a2n + b2n,
θn = arctan
an
bn

.
(2.111)
If in the last form presented in (2.109), we substitute
cn = 1
2(an −jbn),
c−n = 1
2(an + jbn),
(2.112)
to have a third representation of the Fourier series, given by
x(t) = c0 +
∞

n=1
cne jn0t +
−∞

n=−1
cne jn0t
=
∞

n=−∞
cne jn0t,
(2.113)

70
CHAPTER 2 Continuous-Time Signals and Systems
where c0 is real and cn are complex, for n ̸= 0, coefﬁcients. The coefﬁcients of terms for positive (cn)
and negative (c−n) values of n are complex conjugates (c−n = c∗
n). This Fourier series form demands
the summation to be performed over all n ∈Z, as the continuous-time complex sinusoids, e jn0t,
present an inﬁnite number of terms (speciﬁc frequencies n0). Moreover, for every integer value of n,
e jn0t = cos (n0t)+ j sin (n0t) is periodic with T = 2π. Hence, the periodic signal x(t) must have
T = 2π in order to compute the Fourier series expansion. However, the analysis of the Fourier cosine
series of a function x in the interval [0, π] is equivalent to the analysis of its extension ﬁrst to [−π, π]
as an even function, then to all of components with period T = 2π.
The complex exponential representation of the Fourier series, given by (2.113) generates a two-sided
spectrum, as the summation requires all n ∈Z. The ﬁrst and second representations of the Fourier
series, presented in (2.110) produce a one-sided spectra, as the sum is computed only for values of
n > 0 .
Equations (2.110) and (2.113) are called Fourier synthesis equations as it is possible to synthesize
any periodic (time) signal, x(t), from an inﬁnite set of harmonically related signals.
If x(t) is periodic with fundamental frequency 0 and period T, the coefﬁcients of the three Fourier
series equivalent representations are given by
cn = 1
T
	 t1+T
t1
x(t)e−jn0t dt,
(2.114)
and by manipulating (2.112) we obtain
an = 2
T
	 t1+T
t1
x(t) cos (n0t) dt, and
bn = 2
T
	 t1+T
t1
x(t) sin (n0t) dt,
(2.115)
where the integrals in (2.114) and (2.115) are obtained over any interval that corresponds to the period
T, with the initial time t1 for the integration arbitrarily deﬁned.
Equations (2.114) and (2.115) are known as Fourier analysis or Fourier decomposition as the signal
is being decomposed into its spectral components (basic inputs).
The continuous-time Fourier series expands any continuous-time periodic signal into a series of sine
waves. The Fourier analysis methods are named after the French mathematician Jean-Baptiste Joseph
Fourier. In general, almost any periodic signal can be written as an inﬁnite sum of complex exponential
functions (or real sinusoidal functions) and thus be represented as a Fourier series. Therefore, ﬁnding
the response to a Fourier series, means ﬁnding the response to any periodic function, and its effect on its
spectrum. When computing trigonometric coefﬁcients, one could multiply a random signal by sinusoids
of different frequencies to disclose all the (hidden) frequency components of the signal. In this way, it
becomes easier to isolate the signal of the television station we want to watch from the others that are
being simultaneously received by our television set.
In addition, the multiplication of a signal by e j0t means that the signal if being frequency shifted
to 0. Which is equivalent to the Frequency Shift Laplace Property (Table 2.3).

1.02.7 The Fourier Series and the Fourier Transform
71
Consider x(t) = 3/4+cos (0t)+cos (20t +π/3)+1/3 cos (50t +π/4), where 0 = 2π, 3/4
is the DC level, and three non-null harmonics (n = 1, 2, and 5) with the time-domain representation
displayed by Figure 2.1. The signal has several (hidden) frequencies, and no clue is provided when
visually inspecting Figure 2.1. The Fourier series representations are very helpful in this case, showing
the amplitudes and phases of each harmonic in the frequency-domain.
The exponential form, recursing to trigonometric identities, is given by:
x(t) =
3
4

c0
+
⎛
⎜⎜⎝
1
2

c1
e j0t +
1
2

c−1
e−j0t
⎞
⎟⎟⎠+
⎛
⎜⎜⎝
1
2e j π
3
  
c2
e j20t + 1
2e−j π
3
  
c−2
e−j20t
⎞
⎟⎟⎠
+
⎛
⎜⎜⎝
1
6e j π
4
  
c5
e j50t + 1
6e−j π
4
  
c−5
e−j50t
⎞
⎟⎟⎠,
(2.116)
and the Fourier series exponential form coefﬁcients are (all other terms are zero).
Thecomplex-valuedcoefﬁcientcn conveysboththeamplitude(|cn|)andphase(∠cn)ofthefrequency
content of the signal x(t) at each value n0 rad/s as pictured in Figure (2.26).
The sinusoidal basis functions of the Fourier series are smooth and inﬁnitely differentiable but they
only represent periodic signals. The Fourier representation for a non-periodic (time-domain) signal has
to treat the frequency spectrum of non-periodic signals as a continuous function of frequency. This
representation could be obtained by considering a non-periodic signal as a special case of a periodic
signal with an inﬁnite period. The idea is that if the period of a signal is inﬁnite (T →∞), then it is never
repeated. Hence, it is non-periodic. As T →∞, 0 →0, thus decreasing the spacing between each
adjacent line spectra (representing each harmonic contribution in the spectrum) towards a continuous
representation of frequency.
−5 −4 −3 −2 −1
0
1
2
3
4
5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
|cn |
Ω(rad/s)
Ω(rad/s)
−5 −4 −3 −2 −1
0
1
2
3
4
5
−60
−40
−20
0
20
40
60
cn (degrees)
FIGURE 2.26
Two-sided spectrum of periodic signal x(t) depicted in Figure 2.1.

72
CHAPTER 2 Continuous-Time Signals and Systems
The generalization to non-periodic signals is known as the Fourier transform and is given by (2.117).
It is analogous to the Fourier series analysis Eq. (2.114). It expresses the time-domain function x(t) as
a continuous function of frequency X( j) deﬁned as follows:
X( j) = F{x(t)} =
	 ∞
−∞
x(t)e−jtdt.
(2.117)
The inverse Fourier transform is analogous to the Fourier series synthesis equation (2.113). It obtains
the time-domain signal, x(t), from its frequency-domain representation X( j):
x(t) = F−1{X( j)} = 1
2π
	 ∞
−∞
X( j)e jt d.
(2.118)
The notation X( j) = F{x(t)} denotes that X( j) is the Fourier transform of x(t). Conversely,
x(t) = F−1{X( j)} denotes that x(t) is the inverse Fourier transform of X( j). This relationship is
expressed with x(t)
F
←→X( j).
The Fourier series coefﬁcients have distinct units of amplitude, whereas X( j) has units of amplitude
density. The magnitude of X( j) is termed as the magnitude spectrum (|X( j)|) and its phase as the
phase spectrum (̸ X( j)).
Following the concept of representing signals as a linear combination of eigensignals, instead of
choosing e−(σ+ j)t as the eigensignals, one could choose e−jt as the eigensignals. The latter repre-
sentation leads to the Fourier transform equation, and any LTI system’s response could be characterized
by the amplitude scaling applied to each of the basic inputs e−jt.
This transform is very useful as it produces the frequency complex content of a signal, X( j),
from its temporal representation, x(t). Moreover, the mapping provides a representation of the signal
as combination of complex sinusoids (or exponentials) for frequency-domain behavior analysis, where
the (computationally consuming) computation of the convolution in the time-domain is replaced by the
(simple) operation of multiplication in the frequency-domain.
Nevertheless, functions such as the unit-step (Figure 2.2a), discontinuous at time t = 0, does not
have forward Fourier transform as its integral does not converge.
An analysis of convergence is beyond the scope of this chapter, hence we assume that the existence
of the Fourier transform is assured by three (modiﬁed) Dirichlet conditions (for non-periodic signals),
requiring the temporal signal, x(t): to have a ﬁnite number of discontinuities, with the size of each
discontinuity being ﬁnite; to have a ﬁnite number of local maxima an minima; and to be absolutely
integrable, i.e.,
	 ∞
−∞
|x(t)|dt < ∞.
(2.119)
These are sufﬁcient conditions, although not strictly necessary as several common signals, such as
the unit-step, are not absolutely integrable. Nevertheless, we can deﬁne a transform pair that satisﬁes
the Fourier transform properties by the usage of impulses when dealing with this class of signals.
The Fourier transform converts a 1-D time-domain signal, x(t), into its 1-D complex spectrum X( j)
representation in frequency-domain. In Section 1.02.4, the Laplace transform maps a time-domain 1-D

1.02.7 The Fourier Series and the Fourier Transform
73
Table 2.5 Fourier Transform Pairs
x(t) = F−1{X (j)}
X (j) = F{x(t)}
δ(t)
1
1
2πδ()
u(t)
πδ() + 1
j
|t|
−2
2
e−at
1
a+j
e−a|t|, a > 0
2a
a2+2
ej0t
2πδ( −0)
sgn(t)
2
j
sin 0t
−jπ[δ( −0) + δ( + 0)]
cos 0t
π[δ( −0) + δ( + 0)]
signal, x(t), to a complex representation, X(s), deﬁned over a complex plane (s-plane), spanned by its
two variables σ and , as s = σ + j. The Laplace and the Fourier transforms are closely related,
when s = j, i.e., σ = 0, the Laplace integral becomes the Fourier transform. Referring to Figure 2.10
(Section 1.02.4), as the imaginary axis ( j) lies within the ROC, the Fourier transform exists only if
a < 0. For a certain class of signals, there is a simple relationship between the Fourier and the Laplace
transforms given by
F{x(t)} = L{x(t)}|s= j.
(2.120)
The Laplace transform for x(t) = e−atu(t), a > 0 is X(s) = 1/(s + a) for σ > −a (Table 2.2).
X(s) has a pole at s = −a, which lies at the left-half s-plane. The ROC is located at the right side
of −a (right side of Figure 2.11), thus containing the imaginary axis ( j) of the s-plane. Hence, the
signal x(t) has a Fourier transform given by X( j) = 1/( j + a). However, the Fourier transform
does not converge for Re(s) < −a because x(t) is not absolutely integrable, whereas x(t) has a Laplace
transform with the ROC pictured in the left side of Figure 2.11.
Common Fourier transform pairs are summarized in Table 2.5.
As the Laplace transform, the Fourier transform presents a set of properties. A comprehensive
description of the properties of the Fourier transform is beyond the intent of this chapter, and the
properties we examine are due to their importance in the study of electronic system analysis. Much of
the usefulness of the Fourier transform arises from its properties, that have important interpretations
in the context of continuous-time signal processing. Understanding the relationship between time- and
frequency-domains (symmetry between operations) is a key point in the study (and usage) of the Fourier
transform.
(Time and frequency) scaling: If x(t)
F
←→X( j), then x(at)
F
←→1
|a| X

j 
a

, with a ̸= 0 ∈R:
F{x(at)} =
	 ∞
−∞
x(at)e−jt dt,
(2.121)

74
CHAPTER 2 Continuous-Time Signals and Systems
substituting u = at we get
F{x(at)} =
⎧
⎪⎪⎨
⎪⎪⎩
	 ∞
−∞
x(u)e−j u
a  du
a , a > 0;
	 −∞
∞
x(u)e−j u
a  du
a , a < 0.
(2.122)
The two previous expressions can be combined in one single expression given by
F{x(at)} = 1
|a|
	 ∞
−∞
x(u)e−j 
a u du
= 1
|a| X

j 
a

,
(2.123)
implying that a linear expansion of the time axis in the time-domain leads to linear compression of the
frequency axis in the frequency-domain, and vice versa. This property indicates an important trade-off
between the two domains, as narrow time-domain signals have wide Fourier representations (narrow
pulses have wide bandwidth).
Symmetry: The symmetry properties are quite useful as for a real-valued, a real-valued and even, a
real-valued and odd, and an imaginary temporal signal, x(t), we have the following relations:
x(t) real
F
←→
X ∗(j) = X ( −j)
x(t) real and even
F
←→
Im(X (j)) = 0
x(t) real and odd
F
←→
Re(X (j)) = 0
x(t) imaginary
F
←→
X ∗(j) = −X ( −j)
These properties are important as for a real-valued function in time its Fourier transform is conjugate
symmetric, X∗( j) = X( −j). This means that its real part (the magnitude) is an even function
of frequency, and that its imaginary part (phase) is an odd function of frequency. Therefore, when we
obtain the Fourier transform of a real-valued time function, it is only necessary to display the transform
for positive values of . If the signal is even, i.e., x(t) = x( −t), it has a Fourier transform that is
a purely real function, X( j) = |X( j)|. Conversely, the transform of an odd function is an purely
imaginary function.
Furthermore, other properties deserve to be highlighted. We start with the Frequency Shifting prop-
erty presented in Table 2.6. As its representation in the frequency-domain indicates, X( j( −0)),
a multiplication by e j0t in the time-domain, modulates the signal x(t) onto a different frequency. As
most modulation and demodulation techniques involve multiplication, it is important to stress the impor-
tance of the convolution property (Table 2.6). A simple operation of multiplication in time becomes a
computationally intensive operation of convolution in frequency. Conversely, a convolution operation
in the time-domain is much easier to analyze in the frequency-domain (multiplication in the frequency-
domain). Hence, as the convolution in the time-domain deﬁnes the output of a time-invariant ﬁlter to a
given input, the analysis and design of ﬁlters are typically performed in the frequency-domain.
Another important relation is the Parseval’s Relationship. It indicates that the information (energy)
contained in the time-domain is preserved when representing the signal in the frequency-domain. If the

1.02.7 The Fourier Series and the Fourier Transform
75
Table 2.6 Properties of Fourier Transform
Property
Time-domain
Frequency-domain
Linearity
a1x1(t) + a2x2(t)
a1X1(j) + a2X2(j)
Scaling
x(at)
1
|a|X (j 
a )
Time shifting
x(t −t0)
e−jt0X (j)
Frequency shifting
ej0t x(t)
X (j( −0))
Differentiation in time
dx(t)
dt
jX (j)
d nx(t)
dt
(j)nX (j)
Integration in time

 t
−∞x(τ)dτ
1
jX (j) + πX (0)δ()
Differentiation in frequency
−jtx(t)
dX (j)
d
Convolution (in time)
x1(t) ∗x2(t)
X1(j)X2(j)
Convolution (in frequency)
x1(t)x2(t)
1
2π X1(j) ∗X2(j)
power associated with a (complex) signal x(t) is |x(t)|2, Parseval’s Relationship states that the signal
is represented equivalently in either the time- or frequency-domain without lost or gain of energy:
	 ∞
−∞
|x(t)|2dt = 1
2π
	 ∞
−∞
|X( j)|2d.
(2.124)
Hence, we can compute average power in either the time- or frequency-domain. The term |X( j)|2 is
known as the energy spectrum or energy density spectrum of the signal and shows how the energy of
x(t) is distributed across the spectrum.
A list of Fourier transform properties are summarized in Table 2.6.
One more time, we take, as an example, the RLC circuit displayed by Figure 2.8, where the desired
response is the voltage, vo(t), across the capacitor C, vC(t). If we use the Fourier transform, assisted
by its differentiation property (Table 2.6), to solve the ODE in (2.30) we have
F{e−t} = Vi( j) = F

LC d2vo(t)
dt2
+ RC dvo(t)
dt
+ vo(t)

(2.125)
and we get the following result
Vi( j) = −LC2Vo( j) + j RCVo( j) + Vo( j),
(2.126)
where the output in the frequency-domain, Vo( j), is given by
Vo( j) =
Vi( j)
(1 −LC2) + j RC,
(2.127)
which exhibits a resonant behavior exactly as in (2.96). Also note that the frequency response, deﬁned
in (2.93), corresponds to the Fourier transform of the impulse response.
Fourier methods and applications are presented in [1,4,5,16,17].

76
CHAPTER 2 Continuous-Time Signals and Systems
1.02.8 Conclusion and future trends
This chapter provided an introductory overview on the general concepts of signals, systems, and analysis
tools used in the continuous-time domain.
We believe that the knowledge of continuous-time signals and systems, even for a book directed for
methods used in the discrete-time domain, is extremely important for, while exploring their differences,
the reader can enhance the understanding of the natural phenomena.
A few papers discuss the relevance of teaching (analog) circuits and systems, as well its importance
to an engineering career [18].
Analog circuit engineers have to deal with the entire circuit, and many parameters must be considered
in their design. In order to simplify circuit models, designers have to make approximations about the
analog components, which requires expertise and years of education.
This chapter dealt with traditional electrical components and their fundamental variables: resistors
having a direct relationship between voltage and current (v = Ri, R the resistance), inductors relating
voltage and current with its ﬂux (dφ = v dt and dφ = L di, L the inductance), and capacitors relating
voltage and current with charge (dq = C dv and dq = i dt, C the capacitance). After ﬁrst theorized
in the early seventies, the memristor regained the media in 2008 when a ﬁrst real implementation was
announced: the fourth fundamental circuit element relating charge and ﬂux as in its axiomatic deﬁnition,
dφ = M dq (M the memristance) [19]. The use of this component in usual electronic gadgets as well
as teaching its theory even in undergraduate courses seems to be an upcoming technology trend.
In addition, most electronic devices have to interface with the external, analog, world, where data
conversion is needed at the input and output sides. Analog technologies are used along with digital
technologies, spanning from human interface components to analog circuits in wireless components.
Furthermore, analog design is also present in the mixed-signal integrated circuits, and it is becoming
increasingly critical due to high-speed circuitry [20]. Analog circuits are also used to process signals
in very high frequency ranges, such as microwave and RF. In addition, electronic digital circuits are
supplied with power, and the trend is to reduce power consumption of digital circuitry. Hence, knowledge
of analog circuits is required if an engineer is designing a high performance digital circuit.
We have tried to condense in few pages, all pertinent information regarding analog signals and
systems in order to help a complete understanding of the forthcoming chapters. Nevertheless, we have
not exhausted the description of all details. For more interested readers, we outline in the following
suggested references for further reading. We hope you enjoy the rest of the book.
Glossary
BIBO
refers to a certain class of stability known as bounded-input bounded-
output
Convolution
operation between two functions: the output (zero-state solution) of a
linear and time-invariant system corresponds to the convolution inte-
gral between the input signal and the impulse response, i.e., y(t) =

 ∞
−∞x(τ)h(t −τ)dτ

1.02.9 Relevant Websites:
77
Frequency response of a linear system corresponds to the Fourier transform of its impulse
response: H( j) =

 ∞
−∞h(τ)e−jτdτ
Impulse signal
δ(t) is a pulse of inﬁnite amplitude and inﬁnitesimal time characterized
by being equal to zero when t ̸= 0 and

 ∞
−∞δ(t)dt = 1
Impulse response
h(t) is the output of a linear and time-invariant system when the input
corresponds to the impulse δ(t)
LTI
linear and time-invariant system: when the output of a continuous-time
system corresponds to a linear operation on the input signal while not
depending on a particular instant but remaining the same as time goes
by
ODE
ordinary differential equation: a differential equation having only one
independent variable
RLC
is usually related to a circuit with resistor (R), inductor (L) and capacitor
(C)
ROC
region of convergence of the Laplace transform of a given signal x(t):
in the Laplace transform domain, it corresponds to the set of values of
the complex variable s that ensures the existence of L{x(t)}
Transfer function
is the representation in the s-domain of the input-output relation for a
linear system; it corresponds to the Laplace transform of the impulse
response, i.e., H(s) = Y(s)
X(s) = L{h(t)}
Unit-step signal
u(t) is equal to 0 when t < 0, equal to 1 when t > 0 and relates to the
impulse signal as follows: u(t) =

 t
−∞δ(t)dt
1.02.9 Relevant Websites:
<http://www.britannica.com/EBchecked/topic/330320/Pierre-Simon-marquis-de-Laplace/>(articlefrom
the Encyclopedia Britannica about Laplace)
<http://www.genealogy.ams.org/id.php?id=108295>(AmericanMathematicalSocietyGenealogyProject,
Pierre-Simon Laplace)
<http://www.academie-francaise.fr/immortels/base/academiciens/ﬁche.asp?param=336> (mention to
Pierre-Simon de Laplace, immortal of Académie-Française)
<http://www.britannica.com/EBchecked/topic/215097/Joseph-Baron-Fourier/> (article from the Ency-
clopedia Britannica about Fourier)
<http://www.genealogy.ams.org/id.php?id=17981>(AmericanMathematicalSocietyGenealogyProject,
Jean-Baptiste Joseph Fourier)
<http://www.ieeeghn.org/> (IEEE Global History Network)
<http://www.coe.ufrj.br/∼acmq/> (this site contains many tools for circuit analysis and design as well
as a number of interesting links)
Relevant Theory: Signal Processing Theory
See this Volume, Chapter 1 Introduction: Signal Processing Theory
See this Volume, Chapter 3 Discrete-Time Signals and Systems

78
CHAPTER 2 Continuous-Time Signals and Systems
1.02.10 Supplementary data
Supplementary data associated with this article can be found, in the online version, at
http://dx.doi.org/10.1016/B978-0-12-396502-8.00002-4.
References
[1] A. Oppenheim, A. Willsky, S. Nawab, Signals and Systems, Prentice-Hall Signal Processing Series, second
ed., Prentice Hall, 1996.
[2] B.P. Lathi, Linear Systems and Signals, second ed., Oxford University Press, 2004.
[3] B.P. Lathi, Signal Processing and Linear Systems, second ed., Oxford University Press, Incorporated, 2009.
[4] S. Haykin, B. Veen, Signals and Systems, second ed., John Wiley & Sons, 2003.
[5] B. Girod, R. Rabenstein, A. Stenger, Signals and Systems, ﬁrst ed., John Wiley & Sons, 2001.
[6] W. Boyce, R. DiPrima, Elementary differential equations, ninth ed., John Wiley & Sons, USA, 2008.
[7] A.C. Fischer-Cripps, The Mathematics Companion: Mathematical Methods for Physicists and Engineers,
Taylor & Francis, New York, USA, 2005.
[8] R. Dorf, J. Svoboda, Introduction to Electric Circuits, eighth ed., John Wiley & Sons, USA, 2010.
[9] G. Strang, Computational Science and Engineering, Wellesley-Cambridge Press, Massachusetts, USA, 2007.
[10] W. Thomson, Laplace Transformation, Prentice-Hall Electrical Engineering Series, second ed., Prentice-Hall
Inc., 1960.
[11] J. Holbrook, Laplace transforms for electronic engineers, International Series of Monographs on Electronics
and Instrumentation, second ed., Pergamon Press, 1966.
[12] F. Oberhettinger, L. Badii, Tables of Laplace transforms, Grundlehren der mathematischen wissenschaften,
Springer-Verlag, 1973.
[13] L. Padulo, M.A. Arbib, System Theory: A Uniﬁed State-Space Approach to Continuous and Discrete Systems,
W.B. Saunders Company, Philadelphia, USA, 1974.
[14] C. Close, Analysis of Linear Circuits, International edition, Harcourt Publishers Ltd., California, USA, 1974.
[15] C. Desoer, E. Kuh, Basic circuit theory, McGraw Hill International Editions: Electrical and Electronic Engi-
neering Series, New York, USA, 1969.
[16] R. Bracewell, The Fourier transform and its applications, McGraw-Hill Series in Electrical and Computer
Engineering, third ed., McGraw Hill, 2000.
[17] A. Papoulis, The Fourier integral and its applications, McGraw-Hill Electronic Sciences Series, McGraw-Hill,
1962.
[18] P. Diniz, Teaching circuits, systems, and signal processing, in: Proceedings of the 1998 IEEE Inter-
national Symposium on Circuits and Systems 1998, ISCAS’98, vol. 1, 1998, pp. 428 –431, doi:
<http://dx.doi.org/10.1109/ISCAS.1998.704468>.
[19] G.E. Pazienza, J. Albo-Canals, Teaching memristors to EE undergraduate students [class notes], IEEE Circ.
Syst. Mag. 11 (4) (2011) 36–44.
[20] R. Gray, History and trends in analog circuit challenges at the system level, in: 2010 International Symposium
on Electronic System Design (ISED), 2010, p. 24, doi: <http://dx.doi.org/10.1109/ISED.2010.62>.

3
CHAPTER
Discrete-Time Signals and Systems
Leonardo G. Baltar and Josef A. Nossek
Institute for Circuit Theory and Signal Processing, Technische Universität München,
München, Germany
1.03.1 Introduction
Discrete-time systems are signal processing entities that process discrete-time signals, i.e., sequences
of signal values that are generally obtained as equidistant samples of continuous-time waveforms along
the time axis. Usually a clock signal will determine the period T (sampling interval) in which the input
signal values enter the system and, respectively, the output samples leave the system. The interval T
also determines the cycle in which the internal signal values within the system are processed. Typical
representatives of this class of systems are the analog discrete-time signal processing systems (switched
capacitor circuits, charge-coupled devices (CCD), bucket-brigade devices (BBD)) as well as digital
systems. In the case of the last, the sequence value for each sampling interval will also be discretized,
i.e., it will be represented with a ﬁnite set of amplitudes, usually in a binary representation. Chapter 5
covers the topic of sampling and quantization of continuous-time signals.
It is worth mentioning, that in the case of multi-rate discrete-time systems, the interval in which the
internal signals and possibly the output signal are processed will usually be different from T. The topic
of multi-rate discrete-time systems is covered in Chapter 9.
It is important to emphasize that the discussion in this chapter considers the special case of discrete-
time signals with unquantized values. This means that the amplitude of all the signals and the value of
the parameters (coefﬁcients) that describe the discrete-time systems are represented with inﬁnite word
length. This is an abstraction from real digital signals and systems, where the ﬁnite word length represen-
tation of both signals and parameters have to be taken into account to fully cover all important phenom-
ena, such as limit cycles. Design of digital ﬁlters and their implementation will be discussed in Chapter 7.
1.03.2 Discrete-time signals: sequences
Discrete-time signals can arise naturally in situations where the data to be processed is inherently
discrete in time, like in ﬁnancial or social sciences, economy, etc. Generally, the physical quantities
that the discrete-time signal represents evolve continuously over time (for example, voice, temperature,
voltages, currents or scattering variables), but inside a sampling interval of T seconds, can be completely
characterized by means of a single value only, see Figure 3.1. In the last case, the continuous-time signals
are ﬁrst sampled or discretized over time and possibly also quantized in amplitude.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00003-6
© 2014 Elsevier Ltd. All rights reserved.
79

80
CHAPTER 3 Discrete-Time Signals and Systems
...
...
1 2 3
u(1)
u(2)
u(3)
T
u(t)
n(×T) seconds
FIGURE 3.1
Discrete-time signal as a sampled version of a continuous-time waveform.
...
...
0 1
1
2
−1
−2
n(×T) seconds
FIGURE 3.2
Discrete-time unit impulse.
Still images are usually described as two dimensional discrete signals, where the two dimensions are
spatial dimensions. For moving images, a third dimension is added.
Important examples of discrete-time signals that are used in practice are addressed in the following.
Analogously to the continuous-time unit impulse, known as the Dirac delta function, we can also
deﬁne a discrete-time unit impulse, known as the Kronecker delta function, as
δ(n) =
 1, n = 0,
0, else,
(3.1)
where n is an integer number. The discrete time unit impulse is graphically shown in Figure 3.2. Any
arbitrary discrete time sequence can be represented as a sum of weighted and delayed unit impulses.
Also analogous to the continuous-time systems, the input–output behavior of a discrete-time system
can also be described by the impulse response, as we will see in details in a later section.
Another commonly used test signal is a sequence, where the samples are equidistantly taken from a
sinusoid
u(n) = U sin (nT ω + ϕ),
(3.2)
with U and ϕ being real constants. ω is the frequency of the sinusoid in rad/s. In some cases it is important
to analyze a system for certain frequencies and in this case a sinusoid input with the frequencies of
interest can be applied at the input. An example of a discrete-time sinusoid and the corresponding
continuous-time waveform are depicted in Figure 3.3

1.03.3 Discrete-Time Systems
81
...
1 2 3
u(1)
u(2)
u(3)
u(t)
n(×T) seconds
FIGURE 3.3
Discrete-time sinusoid and corresponding continuous-time.
We can also deﬁne a random sequence, an example being the case where the samples are distributed
according to a normal distribution with zero mean and given variance σ 2 and the individual samples
are statistically independent, i.e.,
E

u(n)u(n −k)

=

σ 2, k = 0,
0,
else.
(3.3)
This is the discrete equivalent of the white Gaussian noise and it is useful if the behavior of the discrete
time system is to be analyzed for all frequencies at once. Also in the case where some noise sources are
to be included during the analysis or modeling of a discrete-time system implementation, this random
sequence is the most used one. See Chapter 4 for more details on random sequences.
1.03.3 Discrete-time systems
A discrete-time system with discrete-time input sequence u(n) and output sequence y(n) is an operator
T that performs the mapping
y(n) = T {u(n)}.
(3.4)
It is important to note that this mapping is applied to the entire input sequence u(n), beginning with
some starting instant in the far distant past, including the present instant n and up to a far future instant,
to obtain the present output y(n).
1.03.3.1 Classiﬁcation
There are many different classes of discrete-time systems. They can be categorized according to various
important properties that will allow us to analyze and design systems using speciﬁc mathematical tools.
All the properties explained here ﬁnd an equivalent for continuous-time systems.
1.03.3.1.1
Memoryless systems
One sample of the output sequence y(n) of a memoryless system for a speciﬁc index n0 depends
exclusively on the input value u(n0). The output depends neither on any past or future value of the input
. . . , u(n0 + 1), u(n0 −1), u(n0 −2), . . . nor on any past or future value of the output . . . , y(n0 + 1),

82
CHAPTER 3 Discrete-Time Signals and Systems
y(n0 −1), y(n0 −2), . . ., i.e.,
y(n0) = T {u(n0)}.
(3.5)
1.03.3.1.2
Dynamic systems
In contrast, a memory or dynamic system has its output dependent on at least one past value, either the
output or the input
y(n) = T {u(n), u(n −1), . . . u(n −N), y(n −1), . . . y(n −N)}.
(3.6)
The positive integer constant N is usually called the degree of the system. We have assumed here,
without loss of generality, that the same number of past inputs and outputs inﬂuence a certain output
sample.
1.03.3.1.3
Linear systems
If the system is a linear system, the superposition principle should hold. Let us consider an input u1 for
the system. Then the output is
y1(n) = T {u1(n)}.
(3.7)
Similarly, with another input u2, the output is
y2(n) = T {u2(n)}.
(3.8)
The superposition principle tells us that if each input is scaled by any constant α, and moreover, the
sum of the inputs is employed as a new input, then for the new output, it holds that
T {α1u1(n) + α2u2(n)} = T {α1u1(n)} + T {α2u2(n)}
= α1T {u1(n)} + α2T {u2(n)} = α1y1(n) + α2y2(n).
(3.9)
This can be extended for any number of different inputs and any value of the scaling factors. One of the
simplest and probably most popular linear dynamic system is the recurrence used to generate Fibonacci
sequences [1]:
y(n) = u(n) + y(n −1) + y(n −2),
(3.10)
where u(n) = δ(n).
1.03.3.1.4
Non-linear systems
In the case of non-linear systems, the superposition principle does not apply. An example of this is
the famous logistic map [2] or discrete logistic equation, that arise in many contexts in the biological,
economical and social sciences.
y(n) = u(n) + αy(n −1)(1 −y(n −1)),
(3.11)
where α is a positive real number, u(n) = γ δ(n) and γ represents a initial ratio of population to a max-
imum population. The logistic map is a discrete-time demographic model analogous to the continuous-
time logistic equation [3] and is a simple example of how chaotic behavior can arise. In this chapter we
are mainly interested in linear systems, since there exist many well established analysis tools.

1.03.4 Linear Time-Invariant (LTI) Systems
83
1.03.3.1.5
Time-invariant systems
If the samples of the input sequence are delayed by n1 and the output samples are delayed by the same
interval, a discrete-time system is called time-invariant.
y(n −n1) = T {u(n −n1)}.
(3.12)
Both the Fibonacci recurrence and the logistic map are Time-invariant systems.
1.03.3.1.6
Time-variant systems
In the case of time-variant systems, internal parameters of the system, like multiplier coefﬁcients, can
also vary with time. This is the case for the so-called adaptive systems or adaptive ﬁlters (see Chapter
11 for more details). A simple example of a time-variant system is the amplitude modulation
y(n) = sin (nT ω + ϕ)u(n).
(3.13)
There exists a special case of time-variant systems where the internal multiplier coefﬁcients do not
necessarily vary with time, but the sampling rate is changed within the system. Those systems are called
multi-rate systems and they frequently present a cyclical output. This means that for a certain multiple
delay of the input signal, the output sequence will have the same samples as for the non-delayed input.
1.03.3.1.7
Causal systems
In a causal system, the output samples only depend on the actual or past samples of the input sequence,
and on past samples of the output itself. If the system is non-causal, it means that the output sample
depends on an input value that has not yet entered the system. The deﬁnition of dynamic systems
in (1.03.3.1) is already considered a causal case. Non-causal systems are not relevant for almost all
practical applications.
1.03.4 Linear Time-Invariant (LTI) Systems
This is a technically important class of discrete-time systems, for which a rich mathematical framework
exists.
Linear time-invariant continuous-time systems in the electrical/electronic domain can be built with
memoryless (resistive) elements such as resistors, independent and controlled sources and memory
possessing (reactive) elements, for example, capacitors or inductors. In linear time-invariant discrete-
time systems, we also have memoryless and memory-possessing elements such as
•
adders and multipliers, signal source and sink,
•
delay elements.
The symbolic representations of these aforementioned elements are depicted in Figure 3.4. It is important
to note that in this context, nothing is said about how these elements are realized—neither with which
electronic components nor with which technology.

84
CHAPTER 3 Discrete-Time Signals and Systems
Element
Symbolic
representation
Mathematical
description
Addition
Weight
Source
Sink
Delay
x 1(n)
x 2(n)
α
T
x(n)
x(n)
u(n)
u(n)
y(n)
y(n)
y(n)
y(n)
y(n)
y(n) = x 1(n) + x 2(n)
y(n) = αx(n)
y(n) = x(n −1)
FIGURE 3.4
Basic building blocks of a discrete time system.
1.03.4.1 State-space description
Any linear time-invariant system can be structured into a memoryless linear (2N +2) terminal network
with N delay elements and a single input and a single output. Although the system can easily extend
to multiple inputs and/or multiple outputs, we will consider only single input/single output (SISO)
systems, with which we can study all important phenomena.
Because everything within the linear time-invariant memoryless box in Figure 3.5 can only perform
weighted sums, the (N + 1) dimensional input vector (state vector xxx(n) stacked with scalar input u(n))
is mapped onto the (N + 1) dimensional output vector (state vector xxx(n + 1) stacked with scalar output
y(n)) through multiplication with the (N + 1) × (N + 1) matrix S
⎡
⎢⎢⎢⎢⎢⎣
x1(n + 1)
x2(n + 1)
...
xN(n + 1)
y(n)
⎤
⎥⎥⎥⎥⎥⎦
=
 AAA bbb
cccT d

⎡
⎢⎢⎢⎢⎢⎣
x1(n)
x2(n)
...
xN(n)
u(n)
⎤
⎥⎥⎥⎥⎥⎦
= S ·
xxx(n)
u(n)

.
(3.14)
The matrix S can obviously be partitioned into the well-known state space description
xxx(n + 1) = Ax
Ax
Ax(n) + bbbu(n),
y(n) = cccTxxx(n) + du(n),
(3.15)
where AAA ∈RN×N,bbb ∈RN,cccT ∈R1×N, d ∈R,xxx(n),xxx(n + 1) ∈RN and u(n), y(n) ∈R.
Now we can take (3.15) and reﬁne the block diagram depicted in Figure 3.5 to show Figure 3.6,
which is the general state-space realization. The structure in Figure 3.6 is still rather general, because
for each element of AAA,bbb,ccc and d, a multiplication has to be performed, especially if we assume all of
their elements with non-trivial values. The total number of multiplications is in this case, N(N +2)+1.

1.03.4 Linear Time-Invariant (LTI) Systems
85
u(n)
y(n)
linear, memoryless
time-invariant
S
T
T
T
α
s(n)
αs(n)
si(n)
sj (n)
si(n) + sj (n)
Excitation
Source
Response
Sink
x 1(n + 1)
x 2(n + 1)
x N (n + 1)
x 1(n)
x 2(n)
x N (n)
FIGURE 3.5
Discrete-time system.
T
u(n)
y(n)
A
b
cT
d
x (n)
x (n + 1)
N
FIGURE 3.6
General state space realization a discrete system.
But, as we will see later, that AAA,bbb and ccc are not uniquely determined by a given input–output mapping.
Consequently there is room for optimization, e.g., to reduce the number of actual multiplications to be
performed per sample.
Let us compute the output signal y(n), using an initial state vector xxx(0), and the input signal sequence
from the time instant n = 0 onwards
xxx(n) = AAAxxx(n −1) + bbbu(n −1)
= AAA(AAAxxx(n −2) + bbbu(n −2)) + bbbu(n −1)
= . . .
= AAAnxxx(0) +
n

k=1
AAAk−1bbbu(n −k),
(3.16)

86
CHAPTER 3 Discrete-Time Signals and Systems
y(n) =
cccTAAAnxxx(0)



zero-input response
+
n

k=1

cccTAAAk−1bbbu(n −k)

+ du(n)



zero-state response
.
(3.17)
From this general response we can also derive the so-called impulse response, for which we set
xxx(0) = 000 and u(n) = δ(n), the unit impulse which was deﬁned in (3.1). This leads to
y(n) = h(n) =

d
n = 0,
cccTAAAn−1bbb n ≥1.
(3.18)
By making use of the impulse response h(n), we can reformulate the zero-state response with a
general input sequence u(n)
y(n) =
n

k=1

cccTAAAk−1bbbu(n −k)

+ du(n)
=
n

k=0
h(n)u(n −k) = h(n) ∗u(n),
(3.19)
which is the so-called convolution sum. This summation formulation is corresponding to the so-called
convolution integral for continuous-time signals and systems.
Now it is time to show that we can generate a whole class of equivalent systems (equivalent in the
sense that they have the same zero-state response and the same input-output mapping respectively) with
the aid of a so-called similarity transform.
With a non-singular N × N matrix TTT we can deﬁne a new state vector as the linear transformation of
the original one as xxx′(n) = TTT −1xxx(n), then we can rewrite the state-space equations with the new state
vector xxx′(n) as
TTTxxx′(n + 1) = AAATTTxxx′(n) + bbbu(n),
y(n) = cccTTTTxxx′(n) + du(n),
(3.20)
and by multiplying the ﬁrst equation with TTT −1 from the left side we get
xxx′(n + 1) = TTT −1AAATTTxxx′(n) + TTT −1bbbu(n),
y(n) = cccTTTTxxx′(n) + du(n).
(3.21)
The new state-space representation can now be formulated as
xxx′(n + 1) = AAA′xxx′(n) + bbb′u(n),
y(n) = ccc′Txxx(n) + d′u(n),
(3.22)
where the equations
AAA′ = TTT −1AAATTT ,
bbb′ = TTT −1bbb,
ccc′T = cccTTTT and d′ = d
(3.23)
hold.

1.03.4 Linear Time-Invariant (LTI) Systems
87
By inserting AAA′,bbb′,ccc′ and d′ into the zero-state response, we see that this response is invariant to the
above transformation.
Depending on the choice of the transformation matrix, the matrix AAA′ can have different forms. A par-
ticularly interesting form is the so-called normal form, where the state matrix is diagonalized, allowing
a much lower complexity than a dense matrix. First we apply an eigenvalue decomposition [4] to AAA:
AAA = QQQQQQ−1
Eigenvalue decomposition of AAA,
QQQ = [qqq1,qqq2, . . . ,qqqn]
Modal matrix with the eigenvectors qqqk,
 = diag(λk)n
k=1
Eigenvalue λk with AAAqqqk = λkqqqk,
where we have assumed that AAA is diagonalizable and we do not need to resort to the Jordan form. The
transformation matrix and the new state vector are TTT = QQQ and ξ(n) = QQQ−1xxx(n). The system can then
be described by
ξ(n + 1) = ξ(n) + QQQ−1bbbu(n),
y(n) = cccTQQQξ(n) + du(n),
(3.24)
which leads to the new speciﬁc implementation of Figure 3.7. With this new implementation, the number
of coefﬁcients, and consequently the number of multiplications, is reduced to 3N + 1.
If some of the eigenvalues are complex valued (if this is the case they will always come in complex
conjugate pairs provided AAA is real valued), we always will merge the two corresponding ﬁrst order
d
u(n)
U(z)
q1
Tb
q2
Tb
qN
Tb
cTq1
cTq2
cTqN
λ1
λ2
λN
T
T
T
ξ1(n)
ξ2(n)
ξN(n)
ξ1(n+1)
ξ2(n+1)
ξN(n+1)
y(n)
Y (z)
FIGURE 3.7
Normal form for real valued eigenvalues.

88
CHAPTER 3 Discrete-Time Signals and Systems
systems to one second order system with real valued multiplier coefﬁcients only. This is equivalent to
have a block diagonal state matrix (with 2-by-2 blocks) instead of a diagonal one.
Now we can compute the transfer function by transforming (3.15) to the frequency domain with the
Z-transform.
1.03.4.2 Transfer function of a discrete-time LTI system
Similar to continuous-time systems we can also analyze a discrete-time system with the help of a
functional transform to make a transition to the frequency domain. However, we have only sequences
of signal values in the time domain and not continuous waveforms. Therefore, we have to apply the
Z-transform [5]:
Z{u(n)} =
∞

n=−∞
u(n)z−n = U(z),
Z{y(n)} =
∞

n=−∞
y(n)z−n = Y(z),
Z{xxx(n)} =
∞

n=−∞
xxx(n)z−n = XXX(z).
(3.25)
The delay in the time-domain is represented in the frequency domain by a multiplication by z−1.
Z{xxx(n + 1)}
=
∞

n=−∞
xxx(n + 1)z−n = z
∞

n=−∞
xxx(n + 1)z−(n+1)
n′=n+1
=
z
∞

n′=−∞
xxx[n′]z−n′ = zZ{xxx(n)} = zXXX(z).
(3.26)
Now we can transform the state-space equations to the frequency domain
zXXX(z) = AAAXXX(z) + bbbU(z),
Y(z) = cccTXXX(z) + dU(z),
(3.27)
and after we eliminate the state vector XXX(z), we obtain the transfer function
H(z) = Y(z)
U(z) = cccT(z111 −AAA)−1bbb + d.
(3.28)
Since
z = epT
and hence for technical frequencies
z|p= jω = e jωT
(3.29)
holds, H(e jωT ) is obviously a periodical function of ω resp. f with frequency-period 2π/T resp. 1/T .
The question that arises here is: What type of function is (3.28)? To answer that we have to consider
the expression (z111 −AAA)−1 more closely. We assume of course that (z111 −AAA)−1 is nonsingular and

1.03.4 Linear Time-Invariant (LTI) Systems
89
therefore the inverse exists (this means that the inversion will only be performed for the values of z at
which it is possible). The inverse reads
(z111 −AAA)−1 = adj(z111 −AAA)
det (z111 −AAA),
(3.30)
where the determinant is a polynomial in z of degree N, also called the characteristic polynomial
det (z111 −AAA) = zN + α1zN−1 + · · · + αN−1z + αN,
αk ∈R
(3.31)
and the elements of the adjugate matrix are the transposed cofactors of the matrix. The cofactors on the
other hand are sub-determinants of (z111 −AAA) and therefore polynomials of at most degree (N −1).
The numerator ofcccT(z111−AAA)−1bbb is hence a linear combination of cofactors, i.e., a linear combination
of polynomials of degree (N −1) and therefore a polynomial of at most degree (N −1). If we apply
this result in Eq. (3.28), we will obtain a rational fractional function of z
H(z) = β0zN + β1zN−1 + · · · + βN−1z + βN
zN + α1zN−1 + · · · + αN−1z + αN
,
(3.32)
where β0 ̸= 0 only for d ̸= 0, since β0 = d.
We usually divide the nominator and the denominator by zn and obtain in the numerator and in the
denominator polynomials in z−1:
H(z) = βN z−N + βN−1z−(N−1) + · · · + β1z−1 + β0
αN z−N + αN−1z−(N−1) + · · · + α1z−1 + 1 =
N
k=0 βkz−k
1 + N
k=1 αkz−k = Y(z)
U(z).
(3.33)
By multiplying both sides of (3.33) with the denominator we get
Y(z)

1 +
N

k=1
αkz−k

= U(z)
N

k=0
βkz−k.
(3.34)
Now we go back to the time domain with help of the inverse Z-transform and obtain a global difference
equation
y(n) +
N

k=1
αk y(n −k) =
N

k=0
βku(n −k),
y(n) =
N

k=0
βku(n −k) −
N

k=1
αk y(n −k).
(3.35)
The difference equation is equivalent to the description of a discrete-time system like a differential
equation is for the description of a continuous-time system.
The representation of Eq. (3.35) leads us to an important special case, for which we have no equivalent
in continuous-time systems built with lumped elements: the so-called ﬁnite impulse (FIR) response
systems.

90
CHAPTER 3 Discrete-Time Signals and Systems
1.03.4.3 Finite duration impulse response systems
Those are systems with, as the name says, a ﬁnite duration impulse response (FIR), i.e., the αi’s in
(3.35) are equal to zero such that
y(n) =
N

k=0
βku(n −k)
(3.36)
and with a unit impulse as excitation u(n) = δ(n) we get
y(n) = h(k) =
N

k=0
βkδ(n −k) =
⎧
⎨
⎩
0,
n < 0,
βn, 0 ≤n ≤N,
0,
n > N.
(3.37)
We can see that the coefﬁcients of the polynomial transfer function directly give then values of the
impulse response and so that it takes us directly to an implementation, see Figure 3.8. Note that the two
structures are inter-convertible if the principle of ﬂow reversal is applied to the block diagram.
Let us now write down the state space description for the uppermost structure in Figure 3.8. By
calling the input of the delay elements xk(n + 1) and their outputs xk(n) indexing them from the left
most to the rightmost we can see that
x1(k + 1) = u(n),
x2(k + 1) = x1(n),
... = ...
xN(k + 1) = xN−1(n),
(3.38)
and for the output
y(n) = β1x1(n) + β2x2(n) + · · · + βN xN(n) + β0u(n)
(3.39)
u(n)
u(n)
y(n)
y(n)
T
T
T
T
T
T
T
T
β0
β0
β1
β1
β2
β2
β3
β3
βn
βn
FIGURE 3.8
Two FIR realizations inter-convertible through the principle of ﬂow reversal.

1.03.4 Linear Time-Invariant (LTI) Systems
91
holds. In matrix vector notation we get
⎡
⎢⎢⎢⎣
x1(n + 1)
x2(n + 1)
...
xN(n + 1)
⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎣
0 0 · · · 0
1 0 · · · 0
...
...
... ...
0 · · · 1 0
⎤
⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
x1(n)
x2(n)
...
xN(n)
⎤
⎥⎥⎥⎦+
⎡
⎢⎢⎢⎣
1
0
...
0
⎤
⎥⎥⎥⎦u(n),
(3.40)
y(n) = [β1β2 · · · βN]xxx(n) + β0u(n).
(3.41)
It can be easily demonstrated that the second structure in Figure 3.8 is obtained by a transposition of
the state-space description of the ﬁrst structure, the matrix and vectors for it are then given by
AAA′ = AAAT,
bbb′ = ccc,
ccc′T = bbbT,
d′ = d.
(3.42)
Other usual names for FIR systems are:
•
Non-recursive systems,
•
Moving Average (MA),
•
Transversal ﬁlter.
1.03.4.4 Inﬁnite duration impulse response systems
On the other hand if the αi’s are different from zero, then the impulse response will have inﬁnite
duration, as the name says. We can again clearly derive a realization structure from the corresponding
time-domain description, in this case from Eq. (3.35), see Figure 3.9. This per se inefﬁcient realization
(two times the number of necessary delay elements) can be transformed into the well known direct form
through simple rearranging the two blocks like in Figure 3.10. This is called a canonical realization
because it possesses a minimum number of delay elements.
Now we can setup the state-space equations for the IIR system structure of Figure 3.10. As already
mentioned we label the output of the delays the states xk(n) and their inputs xk(n + 1). By numbering
from the topmost to the lowest delay we can see that
x1(n + 1) = −α1x1(n) −α2x2(n) −. . . −αN xN(n) + u(n)
(3.43)
holds and from the second until the Nth-state we get
x2(n + 1) = x1(n),
x3(n + 1) = x2(n),
... =
...
xN(n + 1) = xN−1(n).
(3.44)
For the output we can see that
y(n) = β0x1(n + 1) + β1x1(n) + β2x2(n) + . . . + βN xN(n)
= (β1 −β0α1)x1(n) + (β2 −β0α2)x2(n) + . . . + (βN −β0αN)xN(n) + β0u(n),
(3.45)

92
CHAPTER 3 Discrete-Time Signals and Systems
u(n)
y(n)
T
T
T
T
T
T
α1
α2
αN
β0
β1
β2
βN
FIGURE 3.9
IIR realization according to Eq. (3.35).
u(n)
y(n)
T
T
T
α1
α2
αN
β0
β1
β2
βN
FIGURE 3.10
Canonical direct realization of an IIR system.

1.03.4 Linear Time-Invariant (LTI) Systems
93
where we have used the deﬁnition of x1(n + 1) of (3.43). In matrix vector notation we get
⎡
⎢⎢⎢⎣
x1(n + 1)
x2(n + 1)
...
xN(n + 1)
⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎣
−α1 −α2 · · · −αN
1
0
· · ·
0
...
...
...
...
0
· · ·
1
0
⎤
⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
x1(n)
x2(n)
...
xN(n)
⎤
⎥⎥⎥⎦+
⎡
⎢⎢⎢⎣
1
0
...
0
⎤
⎥⎥⎥⎦u(n),
(3.46)
y(n) = [β1 −β0α1, β2 −β0α2, · · · βN −β0αN]xxx(n) + β0u(n),
(3.47)
where because of its particular structure, AAA is called a companion matrix.
If we apply the principle of ﬂow reversal to the diagram of Figure 3.10 we will end up with the
realization shown in Figure 3.11. This is again equivalent to transform the state-space description into
the transposed system, i.e., the transformed system is represented by
AAA′ = AAAT,
bbb′ = ccc,
ccc′T = bbbT,
d′ = d.
(3.48)
If we apply these deﬁnitions to (3.28) we can see that
H′(z) = ccc′T 
z111 −AAA′−1 bbb′ + d′ = bbbT(z111 −AAAT)−1ccc + d = (H′(z))T
= cccT 
(z111 −AAAT)−1T
(bbbT)T + d = cccT 
z111 −AAA
−1 bbb + d = H(z).
(3.49)
This means that the transposed realization has the same transfer function as the original system.
T
T
T
u(n)
y(n)
−α1
−αN −1
−αN
β0
β1
βN
βN −1
FIGURE 3.11
Transposed realization of the system from Figure 3.10.

94
CHAPTER 3 Discrete-Time Signals and Systems
Other usual names for IIR systems are:
•
Recursive systems,
•
Autoregressive Moving Average (ARMA),
•
Systems with feedback.
1.03.4.5 Observability and controllability
As we have seen in the previous section every LTI system can be completely characterized by a state-
space description
xxx(n + 1) = Ax
Ax
Ax(n) + bbbu(n),
(3.50)
y(n) = cccTxxx(n) + du(n),
(3.51)
from which we can uniquely derive the corresponding transfer function
H(z) = cccT 
z111 −AAA
−1 bbb + d.
(3.52)
It is important to note that although the way from a given state-space description to the input–output
transfer function is unique, the same is not true for the other direction. For one given transfer function
there are inﬁnitely many different state-space realizations, which all may have a number of different
properties, with the very same input–output relation. One of those properties is the stability of the
so-called LTI system.
An often applied stability criterion is to check whether the zeroes of the denominator polynomial of
the transfer function H(z) lie within the unit circle of the z-plane or equivalently the impulse response
h(n) = Z−1 {H(z)}
(3.53)
decays over time. In the following we will see that this is not telling us the complete truth about the
stability of LTI systems. Because stability depends on the actual implementation, which in detail is
reﬂected in the state-space description and not only in transfer function or impulse response. To get a
clear view, we have to introduce the concept of observability and controllability, and based on these
concepts deﬁne different types of stability.
Let us start with the deﬁnition of observability.
Starting with the state-space description (3.50) we assume that the excitation u(n) = 0 for n ≥0
and an initial state xxx(0) ̸= 000. Let us now compute the system output in time domain:
y(0) = cccTxxx(0),
y(1) = cccTxxx(1) = cccT(AAAxxx(0)),
...
y(N −1) = cccTAAAN−1xxx(0).
(3.54)

1.03.4 Linear Time-Invariant (LTI) Systems
95
Let us stack these subsequent N output values in one vector
yyy(0) =
⎡
⎢⎢⎢⎢⎢⎣
y(0)
y(1)
y(2)
...
y(N −1)
⎤
⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎣
cccT
cccTAAA
cccTAAA2
...
cccTAAAN−1
⎤
⎥⎥⎥⎥⎥⎦
xxx(0) = O

cccT, AAA

xxx(0).
(3.55)
From (3.55) we see that this output vector yyy(0) is given by the product of the so called observability
matrix O

cccT, AAA

, which is completely determined by AAA and cccT and the system state at the time instant
n = 0 : xxx(0). Now if the observability matrix is invertible, we can compute the state vector
xxx(0) = O(cccT, AAA)−1yyy(0).
(3.56)
If this is possible, the system (3.50) is completely observable.
Next we deﬁne controllability in a corresponding way by assuming an initial zero state xxx(0) = 000
and let us look for a sequence u(n), n ≥0 to drive the system into some desired state xxx(k). We look at
the evolution of the state as controlled by the excitation
xxx(1) = bbbu(0),
xxx(2) = AAAxxx(1) + bbbu(1) = AAAbbbu(0) + bbbu(1),
xxx(3) = AAAxxx(2) + bbbu(2) = AAA2bbbu(0) + AAAbbbu(1) + bbbu(2),
...
xxx(N) = AAAN−1bbbu(0) + AAAN−2bbbu(1) + . . . + AAA2bbbu(N −3) + AAAbbbu(N −2) + bbbu(N −1).
(3.57)
This can be put together as
xxx(N) =

bbb, AAAbbb, AAA2bbb, . . . , AAAN−1bbb

⎡
⎢⎢⎢⎢⎢⎣
u(N −1)
u(N −2)
u(N −3)
...
u(0)
⎤
⎥⎥⎥⎥⎥⎦
= C(bbb, AAA)uuu(N),
(3.58)
where C(bbb, AAA) is the so called controllability matrix and uuu(N) is the vector, in which the subsequent
excitation samples have been stacked. If the controllability matrix is invertible, we can solve for the
excitation
uuu(N) = C

bbb, AAA
−1 xxx(N),
(3.59)
which will steer the system from the zero initial state into a speciﬁed desired state xxx(N). If this is
possible, the system (3.50) is completely controllable.

96
CHAPTER 3 Discrete-Time Signals and Systems
1.03.4.6 Stability
Based on the concepts of observability and controllability we can now deﬁne different concepts of
stability:
•
Internal stability,
•
Output-stability,
•
Input-stability,
•
Input–output stability.
We will start with internal stability
internal stability
internal stability by requiring that the Euclidean norm of the state vector hat to
decay over time from any initial value with zero excitation:
∀xxx(0) ∈RN
and u(n) = 0,
for n ≥0 :
lim
n→∞∥xxx(n)∥2 = 0.
(3.60)
This is equivalent to require the state vector converge to the zero vector.
By looking at the evolution of the state vector over time
xxx(1) = AAAxxx(0),
xxx(2) = AAAxxx(1) = AAA2xxx(0),
...
xxx(n) = AAAnxxx(0),
(3.61)
and making use of the eigenvalue decomposition (EVD) of AAA = QQQQQQ−1,  = diag {λi}N
i=1 we get
QQQ−1xxx(n) = nQQQ−1xxx(0).
(3.62)
From (3.62) we can conclude that the vector QQQ−1xxx(n) will converge to the zero vector, i.e.,
limn→∞∥QQQ−1xxx(n)∥2 = 0 if and only if all eigenvalues λi of AAA are smaller than one in magnitude
|λi| < 1 ⇐⇒lim
n→∞∥QQQ−1xxx(n)∥2 = 0 ⇐⇒lim
n→∞∥xxx(n)∥2 = 0.
(3.63)
This equivalent to say that the poles of the transfer function are localized inside the unity circle, since the
eigenvalues of AAA equal to the poles of the transfer function. In the above derivation we have assumed that
AAA is a diagonalizable matrix. If this is not the case because of multiples eigenvalues, we have to refrain
to the so called Jordan form. Although this is slightly more complicated, it leads to the same result.
A somewhat weaker stability criterion is to check only if the system output decays to zero from any
initial state and zero excitation:
∀xxx(0) ∈RN
and u(n) = 0 for n ≥0 :
lim
n→∞|y(n)|2 = 0.
(3.64)
Computing the output we get
y(n) = cccTxxx(n) = cccTAAAnxxx(0) = cccTQQQnQQQ−1xxx(0) = ηTn 
QQQ−1xxx(0)

= [η1
η2 · · · ηN]
⎡
⎢⎢⎢⎣
λn
1
0
· · ·
0
0
λn
2
· · ·
0
...
...
...
...
0
0
· · · λn
N
⎤
⎥⎥⎥⎦

QQQ−1xxx(0)

=
N

i=1
ηi λn
i

QQQ−1xxx(0)

i .
(3.65)

1.03.4 Linear Time-Invariant (LTI) Systems
97
ηT = cccTQQQ is the transformed output vector. If an entry ηi is equal to zero, then the eigenmode (normal
mode) λi will not contribute to the system output and, therefore, a non-decaying eigenmode |λi| ≥1
will not violate the output-stability
output-stability
output-stability criterion. This can only happen if the observability matrix is not full
rank. From (3.55) we have
yyy(0) = O(cccT, AAA)xxx(0) = O(cccT, AAA)QQQ

QQQ−1xxx(0)

= O(ηT, )

QQQ−1xxx(0)

=
⎡
⎢⎢⎢⎢⎢⎣
η1
η2
. . .
ηN
η1λ1
η2λ2
. . .
ηNλN
η1λ2
1
η2λ2
2
. . .
ηNλ2
N
...
...
...
...
η1λN−1
1
η2λN−1
2
. . . ηNλN−1
N
⎤
⎥⎥⎥⎥⎥⎦

QQQ−1xxx(0)

(3.66)
and that O(ηT, ) is rank deﬁcient if a least one ηi is zero, and since QQQ is full rank O(cccT, AAA) must also
be rank deﬁcient. Therefore, a system can be output stable without being internally stable, if it is not
completely observable.
A complementary criterion is that of input-stability
input-stability
input-stability, requiring the state vector converging to the zero
vector, but not from any arbitrary initial state, but only from states which can be controlled from the
system input. Now we start with an initial state xxx(N), which can be generated from an input sequence
(see (3.58))
xxx(N) = C(bbb, AAA)uuu(N).
(3.67)
The transformed state QQQ−1xxx(N) then reads
QQQ−1xxx(N) = QQQ−1C(bbb, AAA)uuu(N) = C(μ, )uuu(N)
(3.68)
with the transformed input vector μ = QQQ−1bbb. After the input uuu(N) has produced the state xxx(N) it is
switched off, i. e. u(n) = 0 for n ≥N.
We ask whether the state vector can converge to the zero vector although the system may not be
internally stable. The answer is given by the structure of
C(μ, ) =
⎡
⎢⎢⎢⎣
μ1
μ2
. . .
μN
μ1λ1
μ2λ2
. . .
μNλN
...
...
...
...
μ1λN−1
1
μ2λN−1
2
. . . μNλN−1
N
⎤
⎥⎥⎥⎦= QQQ−1C(bbb, AAA).
(3.69)
If some μi = 0, then QQQ−1xxx(0) will converge to the zero vector, even if the corresponding |λi| ≥1,
because this non-decaying eigenmode cannot be excited from the regular input. But this is only possible,
if C(bbb, AAA) is not full rank and the system is, therefore, not completely controllable.
Finally we look at the concept of the commonly usedinput–output-stability
input–output-stability
input–output-stability, i.e., we excite the system
with zero initial state with a unit pulse and require the output to converge to zero over time. In this
setting the state and the output evolves as
xxx(n) = AAAnbbb,
(3.70)
y(n) = cccTAAAnbbb = cccTQQQnQQQ−1bbb = ηTnμ =
N

i=1
ηiμiλn
i .
(3.71)

98
CHAPTER 3 Discrete-Time Signals and Systems
The output will converge to zero, even if there are non-decaying eigenmodes λi, as long as for every
such λi there is a ηi = 0 or a μi = 0, or both are zero. We see that a system could be input–output
stable, although it is neither internally stable nor input- or output-stable.
Only for systems, which are completely observable and controllable, the four different stability
criteria coincide.
Example 3.1.
The four stability concepts will now be illustrated with an example: the Cascade
Integrator Comb (CIC) ﬁlter [6]. These ﬁlters are attractive in situations where either a narrow-band
signal should be extracted from a wideband source or a wideband signal should be constructed from
a narrowband source. The ﬁrst situation is associated with the concept of sampling rate decrease or
decimation and the second one leads to the concept of sampling rate increase or interpolation. CIC
ﬁlters are very efﬁcient from a hardware point of view, because they need no multipliers. In the ﬁrst
case, i.e., decimation, the integrators come ﬁrst, then the sampling rate will be decreased by a factor
of R followed by a subsequent cascade of comb ﬁlters (Figure 3.12a), while in the second case, i.e.,
interpolation, the comb ﬁlters come ﬁrst, then the sampling rate increase is followed by a subsequent
cascade of integrators (Figure 3.12b).
The stability problem in these CIC ﬁlters stems from the integrators, which obviously exhibit a
non-decaying eigenmode (pole with unit magnitude). To analyze the stability of such CIC ﬁlters we
set the sampling rate decrease/increase factor R = 1 and the number of integrators and comb ﬁlters
stages to L = 1 without loss of generality. In addition we assume a differential delay of M = 3 sam-
ples per stage. This leads us to the following two implementations to be analyzed which are shown in
Figures 3.13a and b.
Let us start with an analysis of the ﬁlter in Figure 3.13a, the state space description of which reads
AAA1 =
⎡
⎣
1 0 0
1 0 0
0 1 0
⎤
⎦,bbb1 =
⎡
⎣
1
0
0
⎤
⎦,ccc1 =
⎡
⎣
1
0
−1
⎤
⎦, d1 = 1
(3.72)
T
T
MT
MT
u(n)
↓R
−
−y(m)
(a) Decimating CIC-Filter
MT
MT
T
T
u[n]
−
−↑R
y[m]
(b) Interpolating CIC-Filter
FIGURE 3.12
Multiplierless decimator and integrator.

1.03.4 Linear Time-Invariant (LTI) Systems
99
u[n]
−
y[n]
u[n]
−
y[n]
T
T
T
T
≡
T
T
T
x 1[n]
x 2[n]
x 3[n]
(a) Cascade of Integrator and Comb Filter
u[n]
y[n]
u[n]
y[n]
T
T
T
T
≡
T
T
T
x 1[n]
x 2[n]
x 3[n]
−
(b) Cascade of Comb and Integrator Filter
FIGURE 3.13
CIC Filters without sampling rate alteration with R = 1, L = 1 and M = 3.
and leads to the transfer function
H1(z−1) = 1 −z−3
1 −z−1 = 1 + z−1 + z−2.
(3.73)
The observability matrix
O

cccT
1, AAA1

=
⎡
⎣
1 0 −1
1 −1 0
0 0
0
⎤
⎦,
det

O(cccT
1, AAA1)

= 0
(3.74)
is obviously rank deﬁcient and the system is therefore not completely observable.

100
CHAPTER 3 Discrete-Time Signals and Systems
Since the state matrix AAA is not diagonalizable, we have to refrain to a Jordan form
AAA1 = QQQ1JJJQQQ−1
1
=
⎡
⎢⎣
0 0
1
√
3
0 1
1
√
3
1 0
1
√
3
⎤
⎥⎦
⎡
⎣
0 1 0
0 0 1
0 0 1
⎤
⎦
⎡
⎣
−1 0 1
−1 1 0
√
3 0 0
⎤
⎦,
(3.75)
which shows the three eigenvalues are λ1 = 0, λ2 = 0 and λ3 = 1. Not all eigenvalues are less then
one in magnitude. Therefore, the system of Figure 3.13a is not internaly stable. But transforming the
observability matrix according to (3.66), we get
O

ηT, JJJ) = O(cccT
1, AAA1

QQQ1 =
⎡
⎣
1 0 0
1 0 0
0 1 0
⎤
⎦
⎡
⎢⎣
0 0
1
√
3
0 1
1
√
3
1 0
1
√
3
⎤
⎥⎦=
⎡
⎣
−1 0 0
0 −1 0
0
0 0
⎤
⎦
(3.76)
with the third component η3 of the transformed output vector ηT = cccT
1 QQQ1 equal to zero. This clearly
means that the non-vanishing third eigenmode is not observable at the system output. Therefore the
system is output stable, i.e., the output converges in the limit limn→∞y (n)=0, although the state vector
does not converge to the zero vector. But the controllability matrix
C(bbb1, AAA1) =
⎡
⎣
1 1 1
1 1 1
0 0 1
⎤
⎦,
det (C(bbb1, AAA1)) = 1
(3.77)
is full rank, i.e., the system is completely controllable, and an arbitrary input sequence may excite the
non-decaying eigenmode. Therefore, the system is not input-stable.
Now let us analyze the ﬁlter in Figure 3.13b, which has the following state-space description
AAA2 =
⎡
⎣
1 1 0
0 0 1
0 0 0
⎤
⎦,bbb2 =
⎡
⎣
1
0
−1
⎤
⎦,ccc2 =
⎡
⎣
1
0
0
⎤
⎦, d2 = 1
(3.78)
and the same transfer function as the previous ﬁlter from Figure 3.13a. It is interesting to note that the
second ﬁlter can be obtained from the ﬁrst one by applying transposition
AAA2 = AAAT
1,
bbb2 = ccc1,
ccc2 = bbb1, d2 = d1.
(3.79)
The controllability matrix of the second system
C(bbb2, AAA2) =
⎡
⎣
1
1 0
0 −1 0
−1 0 0
⎤
⎦,
det (C(bbb2, AAA2)) = 0
(3.80)
is obviously rank deﬁcient and the system is therefore not completely controllable.

1.03.5 Discrete-Time Signals and Systems with MATLAB
101
Since the state matrix AAA2 = AAAT
1 is not diagonalizable we have again to refrain to a Jordan form
AAA2 = QQQ2JJJQQQ−1
2
=
⎡
⎢⎣
1
√
2
1
√
2
1
−1
√
2
0
0
−1
√
2 0
⎤
⎥⎦
⎡
⎣
0 1 0
0 0 1
0 0 1
⎤
⎦
⎡
⎣
0 −
√
2
0
0
0
−
√
2
1
1
1
⎤
⎦,
(3.81)
which again shows obviously the same eigenvalues λ1 = 0, λ2 = 0 and λ3 = 1. The second system is
again not internally stable. Transforming the above controllability matrix according to (3.68) we get
C(μ, JJJ) = QQQ−1
2 C(bbb2, AAA2) =
⎡
⎣
0 −
√
2
0
0
0
−
√
2
1
1
1
⎤
⎦
⎡
⎣
1
1 0
0 −1 0
−1 0 0
⎤
⎦=
⎡
⎣
0
√
2 0
√
2 0 0
0
0 0
⎤
⎦
(3.82)
with the third component μ3 of the transformed input vector μ = QQQ−1
2 bbb2 being equal to zero. This
shows that the non-decaying third eigenmode will not be excited by any possible input signal. Therefore,
the system is input-stable, although it is not internaly stable.
The observability matrix of the second system
O(cccT
2, AAA2) =
⎡
⎣
1 1 0
1 1 0
1 1 1
⎤
⎦,
det (O(bbb2, AAA2) = 1
(3.83)
is full rank and the second system is completely observable. Therefore the non-decaying eigenmode,
although not excited by any input sequence, is observable at the output. This can always happen because
of an unfavorable initial state accidentally occurring in the switch-on transient of the power supply or
because of some disturbance entering the system not through the regular input path. Therefore, the
system is not output-stable.
But both systems are input–output-stable, because both have the same impulse response with ﬁnite
duration.
This example shows, that the standard input–output stability criterion, i.e., requiring a decaying
impulse response, is not always telling the whole story. Judging whether a system is stable or not needs
a detailed knowledge about the system structure, or in other words the realization, and not only about
input–output-mapping.
1.03.5 Discrete-time signals and systems with MATLAB
In this section we will provide some examples how to generate discrete-time signals in MATLAB
and how to represent and implement basic discrete-time systems. We assume here a basic MATLAB
installation. The commands and programs shown here are not unique in the way they perform the
analysis and implement discrete-time signals and systems. Our objective here is to give an introduction
with very simple commands and functions. With this introduction the reader will get more familiar with
this powerful tool that is widely employed to analyze, simulate and implement digital signal processing
systems. If one has access to the full palette of MATLAB toolboxes, like e.g., Signal Processing Toolbox,
Control System Toolbox, Communications System Toolbox and DSP System Toolbox, many of the
scripts included here can be substituted by functions encountered in those libraries.

102
CHAPTER 3 Discrete-Time Signals and Systems
1.03.5.1 Discrete-time signals
Discrete-time signals are deﬁned in MATLAB as one dimensional arrays, i.e., as vectors. They can be
row or column vectors and their entries can be either real or complex valued. The signals we will generate
and analyze in this section are always row vectors and their entries are always real valued. The length of
the vectors will be deﬁned according to the time span in which the system is supposed to be analyzed.
1.03.5.1.1
Unit impulse
The unit impulse δ(n) can be generated with the commands
L = 100;
delta = [1 zeros(1,L-1)];
where L is the length of the desired input and the function zeros(1,L-1) generates a row vector
with L-1 zeros.
The unit impulse can be further weighted and delayed. For example, 2∗δ(n −5) can be generated by
L = 100;
N = 5;
delta = [zeros(1,N) 2 zeros (1,L-1-N)];
where we have assumed a delay of 5 samples or sampling periods.
The length of the vector containing only a unit impulse depends on the objective of the analysis. For
example, if the impulse response of a discrete-time system is to be studied, there should be enough ele-
ments in the input array so that a signiﬁcant part of the impulse response is contained in the output vector.
1.03.5.1.2
Sinusoid
A sinusoid can be generated with the help of the function sin. Below is an example of a sinusoid where
all its parameters are ﬁrst deﬁned
fs = 1000;
% Sampling frequency in Hz
T = 1/fs;
% Sampling period
n = 1:100;
% Time indexes
U = 0.5;
% Amplitude of the sinusoid
fc = 20;
% Sinusoid frequency in Herz
omega = 2∗pi∗fc;
% Sinusoid angular frequency in rad/s
phi = 0;
% phase in radians
x = U∗sin(omega∗n∗T+phi);
We can see that the comments after each command explain what the parameter means.
If we would like to graphically represent the sinusoid, we could use
stem(n∗T,x);
xlabel(’Time in seconds’);
ylabel(’Amplitude’);
and we get the plot in Figure 3.14, where we can see two periods of the sinusoid. An alternative to
stem is the command plot. If we employ it using the same parameters and the same syntax we obtain

1.03.5 Discrete-Time Signals and Systems with MATLAB
103
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
Time in seconds
Amplitude
FIGURE 3.14
Discrete sinusoid with fc = 20 Hz and fs = 1 kHz plotted in MATLAB.
Figure 3.15, where it should be noted that MATLAB graphically connects the amplitude samples with
a straight line and as a consequence the discrete-time sinusoid looks like a continuous-time one. It is
important to keep in mind that the true discrete-time sinusoid is only deﬁned for certain time instants and
the amplitude between two true samples in the Figure 3.15 is only an approximation of the amplitude
of the equivalent continuous-time sinusoid.
As an exercise the reader could play around with the parameters of the sinusoid. For example, he or
she could change the phase φ to π/2 and plot the sinusoid again, then try to use the function cos instead
of sin and set φ to 0 again and compare with the previous plots. Change the sinusoid frequency until you
reach the Nyquist frequency, which is 500 Hz for the example above. Compare the use of the command
plot for different sampling rates and different sinusoid frequencies to see how the approximation of
the continuous-time sinusoid gets worse.
1.03.5.1.3
White gaussian noise
To generate a WGN signal we have to employ the function randn that generates pseudo-random
numbers following the normal or Gaussian distribution with zero mean and unit variance. By using
L = 100;
sigma2 = 0.5;
% Variance of the WGN
u = sqrt(sigma2)∗randn(1,L);
we generate a white Gaussian noise vector with zero mean and variance σ 2 = 0.5.

104
CHAPTER 3 Discrete-Time Signals and Systems
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
FIGURE 3.15
Discrete-time sinusoid represented in MATLAB with the command plot.
1.03.5.1.4
Elaborated signal model
Many operations can be further performed with the generated signals, they can be, for example, added
to each other or be multiplied by a constant or by another signal. As a last example let us generate a
typical signal model used in communications systems. Let us consider an input signal composed by
a sum of weighted and delayed unit impulses. This could be a data carrying signal generated at the
transmitter. Then we multiply it by a sinusoid, that could represent a modulation to a higher frequency,
for example the radio frequency (RF), to allow the propagation of the signal as an electromagnetic
wave in a physical medium between transmitter and receiver. Finally, we add white Gaussian noise that
represents the thermal noise generated by the analog components used in the transmitter and receiver.
The program to generate this signal is the following
L = 100;
u_in = [0.5∗ones(1,L/4) −0.5∗ones(1,L/4) 0.5∗ones(1,L/4)
−0.5∗ones(1,L/4)];
% Sum of weighted and delayed unit impulses
fs = 1000;
% Sampling frequency in Hz
T = 1/fs;
% Sampling period
n = 1:L;
% Time indexes
U = 1;
% Amplitude of the sinusoid
fc = 50;
% Sinusoid frequency in Herz

1.03.5 Discrete-Time Signals and Systems with MATLAB
105
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
Time in seconds
Amplitude
FIGURE 3.16
Weighted and delayed sum of unit impulses u_in.
omega = 2∗π∗fc;
% Sinusoid frequency in rad/s
phi = 0;
% Phase in radians
u_sin = U∗sin(omega∗n∗T+phi);
% Sinusoid
sigma2 = 0.02;
% Variance of the White
Gaussian Noise
u_awgn = sqrt(sigma2)∗randn(1,L);% White Gaussian Noise Vector
u_mod = u_in.∗u_sin+u_awgn;
% Modulation and noise addition
In Figure 3.16 we can see the signal that is composed by a sum of weighted and delayed unit impulses.
In Figure 3.17 the modulated sinusoid is depicted.
We can see in Figure 3.18 the modulated sinusoid with the additive white Gaussian noise.
1.03.5.2 Discrete-time systems representation and implementation
There are different ways to represent a discrete-time system in MATLAB, like, for example, the space-
state or the transfer function. To start we revisit the elaborated signal model introduced in the last section.
If we would like to, at least approximately, recover the input signal represented by the sum of delayed
and weighted unit impulses, we should ﬁrst demodulate it by multiplying it by a sinusoid with the same
frequency of the one used to modulate
u_demod=u_mod.∗u_sin
We will then obtain the sequence shown in Figure 3.19.

106
CHAPTER 3 Discrete-Time Signals and Systems
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
Time in seconds
Amplitude
FIGURE 3.17
Modulated sinusoid u_in∗u_sin.
The only problem with the new sequence is that not only an approximation of the desired signal is
obtained, but also another modulated version of it, but this time with a sinusoid with the double of the
original modulation frequency, and we still have the additive noise. If the noise variance is low enough
we are still able to approximately recover the original signal without any further processing. But we have
ﬁrst to eliminate the modulated signal component and for this we will apply a discrete-time ﬁlter speciﬁc
designed for this task. We assume here that the ﬁlter is given and we only care on how to represent and
implement it. In [7] and the references therein many methods for the design of discrete-time ﬁlters can
be encountered.
Let us say that a state-space description of the ﬁlter is already known and is deﬁned as
A = [0.6969
0.8263
−0.6089
0.0111;
1
0
0
0;
0
1
0
0;
0
0
1
0];
b = [1;
0;
0;
0];
c = [-0.4272
1.0189
−0.1298
0.0160];
d = [0.9738];
where we can identify the companion matrix structure of AAA.

1.03.5 Discrete-Time Signals and Systems with MATLAB
107
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
Time in seconds
Amplitude
FIGURE 3.18
Modulated sinusoid with additive white Gaussian noise u_mod.
To compute the output of the ﬁlter given an initial state and the input signal, the state equations can
be directly employed and iteratively calculated as
L = 100;
u= [1 zeros (1,L-1)];
% Input signal, here a unit impulse
x= [0; 0; 0; 0];
% Initial state vector
y = zeros(1,L);
% Initialization of the output vector
for n = 1:L
y(n)=c∗x+d∗u(n);
x = A∗x+b∗u(n);
end
where we can see that the values of the state-vector are not stored for all time instants. If one is interested
in looking at the internal states evolution, a matrix could be deﬁned with so many rows as the number
of states and so many columns as the length of the input/output signal, and the state vector for each time
instant can be saved in its columns.
If we substitute the unit impulse by the demodulated signal as the input u = u_demod we obtain
the signal depicted in Figure 3.20. One can see that we do not obtain exactly the input signal u_in but
an approximation. particularly in communications systems that is usually the case, where it is important
to recover the information contained in u_in, even if the waveform has been distorted, and not the
waveform itself.

108
CHAPTER 3 Discrete-Time Signals and Systems
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
Time in seconds
Amplitude
FIGURE 3.19
Demodulated signal u_demod.
But if one would like to implement Eq. (3.17) the program becomes more complicated as shown
below
L=100;
u=[1 zeros(1,L-1)];
% Input signal, here a unit impulse
x_0z=[0; 0; 0; 0];
% Initial state vector
y=zeros(1,L);
% Allocation of the output vector
cAb_array=zeros(1,L);
% Allocation of the array c∗Aˆ(k-1)∗b
A_power=eye(4);
% Allocation of the powers of A matrix
cAb_array(1)=c∗b;
% First element of the array cAb_array
y(1)=d∗u(1);
% Output at sample n=0
for n=2:L
cAb_array(n)=c∗A_power∗b;
% Inclusion of a new element in cAb_array
y(n)=c∗(A∗A_power)∗x_0+sum(cAb_array(1:n).∗u(n:-1:1))+d∗u(n);
% Output
A_power=A∗A_power;
% New power of A
end

1.03.5 Discrete-Time Signals and Systems with MATLAB
109
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
Time in seconds
Amplitude
Filtered and demodulated signal
FIGURE 3.20
Demodulated and ﬁltered signal u_demod.
In both programs presented above the impulse response can be obtained and its ﬁrst 100 samples are
plotted in Figure 3.21.
The similarity transformation can also be easily implemented. Let us consider the transformation
into the normal form. The code to perform it is
[Q,Lambda]=eig(A);
% Eigenvalue decomposition:
% Lambda -> Eigenvalues in main diagonal
% Q -> Eigenvectors in the columns
T=Q;
A_norm=inv(T)∗A∗T;
b_norm=inv(T)∗b;
c_norm=c∗T;
d_norm=d;
The resulting state-space description for our example is

110
CHAPTER 3 Discrete-Time Signals and Systems
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
−0.5
0
0.5
1
Time in seconds
Amplitude
FIGURE 3.21
First 100 samples of the impulse response.
A_norm = [-0.9242
0
0
0
0
0.8057
0
0
0
0
0.7968
0
0
0
0
0.0186];
b_norm = [0.6380;
126.6454;
127.3681;
1.7322];
c_norm = [ −0.7502
0.2283
−0.2268
0.0140];
d_norm = 0.9738;
From the state-space representation it is possible to calculate the coefﬁcients of the numerator and of
the denominator polynomials of the transfer function with help of the Faddeev-Leverrier algorithm [8]
beta = [d zeros(1,3)];
alpha = [1 zeros(1,3)];
F = eye(4);
for i=1:4
alpha(i+1) = -(1/i)∗trace(A∗F);

1.03.6 Conclusion
111
beta(i+1) = c∗F∗b+d∗alpha(i+1);
F = A∗F+alpha(i+1)∗eye(4);
end
where the arrays beta and alpha contain the coefﬁcients of the nominator and denominator
polynomials.
With the coefﬁcients of the transfer function it is also possible to calculate the output given the
input of the system and some past values of the output by using the difference equation that can be
implemented as
L=100;
u=[1 zeros(1,L-1)];
y=[zeros(1,L)];
% We assume a relaxed initial condition
y(1)=beta(1).∗u(1);
% Transition phase or first initial output
y(2)=sum(beta(1:2).∗u(2:-1:1))-alpha(2).∗y(1);
y(3)=sum(beta(1:3).∗u(3:-1:1))-sum(alpha(2:3).∗y(2:-1:1));
y(4)=sum(beta(1:4).∗u(4:-1:1))-sum(alpha(2:4).∗y(3:-1:1));
for n=5:L
y(n)=sum(beta.∗u(n:-1:n-4))-sum(alpha(2:end).∗y(n-1:-1:n-4));
end
As we saw before this equivalent to a state-space realization in direct form.
1.03.6 Conclusion
In this chapter we have introduced an important and fundamental topic in electrical engineering that
provides the basics for digital signal processing. We have started with the presentation of some frequently
used discrete-time signals. Then we have showed how to classify discrete-time systems and started the
study of the widely employed class of linear time-invariant systems. We have showed the most basic
way of representing LTI systems in a way that not only the input–output behavior is described, but also
important internal signals, the so-called state-space representation. We have seen that the state-space
representation is not unique and, consequently, there is room for optimization in the way discrete-time
systems are realized. After that, the transfer function was derived from the state-space description and
the two main classes of discrete-time systems, namely FIR and IIR were introduced. We formulated the
concepts of controllability and observability, and showed how they can be used to evaluate the stability
of the system. We ﬁnally gave some examples how to implement and analyze simple discrete-time
signals and systems with MATLAB.
Relevant Theory: Signal Processing Theory
See this Volume, Chapter 4 Random Signals and Stochastic Processes
See this Volume, Chapter 5 Sampling and Quantization
See this Volume, Chapter 6 Digital Filter Structures and Implementations
See this Volume, Chapter 7 Multirate Signal Processing
See this Volume, Chapter 12 Adaptive Filters

112
CHAPTER 3 Discrete-Time Signals and Systems
References
[1] Wikipedia.Fibonacci number—Wikipedia, the free encyclopedia, 2012 (accessed 16 March 2012).
[2] Robert M. May, Simple mathematical models with very complicated dynamics, Nature, 261 (5560) (1976)
459–467.
[3] Pierre-François Verhulst, Notice sur la loi que la population poursuit dans son accroissement, Correspondance
mathématique et physique, 10 (1838) 113–121.
[4] Gilbert Strang, Linear Algebra and Its Applications, Brooks Cole, third ed., February 1988.
[5] E.I. Jury, Theory and application of the z-transform method. John Wiley, New York, USA, 1964.
[6] E.B. Hogenauer, An economical class of digital ﬁlters for decimation and interpolation, IEEE Trans. Acoust.
Speech Signal Process. 29 (2) (1981) 155–162.
[7] E.A.B.daSilva,P.S.R.Diniz,S.LimaNetto,DigitalSignalProcessing:SystemAnalysisandDesign,Cambridge
University Press, Cambridge, UK, second ed., 2010.
[8] R.A. Roberts, C.T. Mullis, Digital Signal Processing, Number Bd. 1 in Addison-Wesley Series in Electrical
Engineering, Addison-Wesley, 1987.

4
CHAPTER
Random Signals and Stochastic
Processes
Luiz Wagner Pereira Biscainho
DEL/Poli & PEE/COPPE, Universidade Federal do Rio de Janeiro, Rio de Janeiro, Brazil
1.04.1 Introduction
Probability is an abstract concept useful to model chance experiments. The deﬁnition of a numerical
representationfortheoutcomesofsuchexperiments(therandomvariable)isessentialtobuildacomplete
and general framework for probabilistic models. Such models can be extended to non-static outcomes in
the form of time signals,1 leading to the so-called stochastic process, which can evolve along continuous
or discrete time. Its complete description is usually too complicated to be applied to practical situations.
Fortunately, a well-accepted set of simplifying properties like stationarity and ergodicity allows the
modeling of many problems in so different areas as Biology, Economics, and Communications.
There are plenty of books on random processes,2 each one following some preferred order and
notation, but covering essentially the same topics. This chapter is just one more attempt to present
the subject in a compact manner. It is structured in the usual way: probability is ﬁrst introduced, then
described through random variables; within this framework, stochastic processes are presented, then
associated with processing systems. Due to space limitations, proofs were avoided, and examples were
kept at a minimum. No attempt was made to cover many families of probability distributions, for
example. The preferred path was to deﬁne clear and unambiguously the concepts and entities associated
to the subject and whenever possible give them simple and intuitive interpretations. Even risking to
seem redundant, the author decided to explicitly duplicate the formulations related to random processes
and sequences (i.e., continuous- and discrete-time random processes); the idea was to provide always
a direct response to a consulting reader, instead of suggesting modiﬁcations in the given expressions.
Writing technical material always poses a difﬁcult problem: which level of detail and depth will
make the text useful? Our goal was making the text approachable for an undergraduate student as well
as a consistent reference for more advanced students or even researchers (re)visiting random processes.
The author tried to be especially careful with the notation consistence throughout the chapter in order
to avoid confusion and ambiguity (which may easily occur in advanced texts). The choices of covered
topics and order of presentation reﬂect several years of teaching the subject, and obviously match those
of some preferred books. A selected list of didactic references on statistics [1,2], random variables
1Of course, other independent variables (e.g., space) can substitute for time in this context.
2Even if the literature sometimes prefers to employ “stochastic” for processes and “random” for signals, the expression
“random processes” became common use and will be used throughout the chapter.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00004-8
© 2014 Elsevier Ltd. All rights reserved.
113

114
CHAPTER 4 Random Signals and Stochastic Processes
and processes [3,4], and applications [5,6] is given at the end of the chapter. We hope you enjoy the
reading.
1.04.2 Probability
Probabilistic models are useful to describe and study phenomena that cannot be precisely predicted. To
establish a precise framework for the concept of probability, one should deﬁne a chance experiment,
of which each trial yields an outcome s. The set of all possible outcomes is the sample space S.
In this context, one speaks of the probability that one trial of the experiment yields an outcome
s that belongs to a desired set A ⊂S (when one says the event A has occurred), as illustrated in
Figure 4.1.
There are two different views of probability: the subjectivist and the objectivist. For subjectivists, the
probability measures someone’s degree of belief on the occurrence of a given event, while for objectivists
it results from concrete reality. Polemics aside, objectivism can be more rewarding didactically.
From theobjectivist view point, oneof thepossibleways todeﬁneprobabilityis theso-called classical
or a priori approach: given an experiment whose possible outcomes s are equally likely, the probability
of an event A is deﬁned as the ratio between the number N(A) of acceptable outcomes (elements of A)
and the number N(S) of possible outcomes (elements of S):
P(A) = N(A)
N(S) .
(4.1)
The experiment of ﬂipping a fair coin, with S = {head, tail}, is an example in which the probabilities
of both individual outcomes can be theoretically set: P({head}) = P({tail}) = 0.5.
Another way to deﬁne probability is the relative frequency or a posteriori approach: the probability
that the event A occurs after one trial of a given experiment can be obtained by taking the limit of the
ratio between the number n A of successes (i.e., occurrences of A) and the number n of experiment trials,
when the repeats go to inﬁnity:
P(A) = lim
n→∞
n A
n .
(4.2)
S
A
s
FIGURE 4.1
Sample space S, event A, and outcome s.

1.04.2 Probability
115
S
A
A
FIGURE 4.2
Event A and its complement A.
In the ideal coin ﬂip experiment, one is expected to ﬁnd equal probabilities for head and tail. On the
other hand, this pragmatic approach allows modeling the non-ideal case, provided the experiment can
be repeated.
A pure mathematical deﬁnition can provide a sufﬁciently general framework to encompass every
conceptual choice: the axiomatic approach develops a complete probability theory from three axioms:
1. P(A) ≥0.
2. P(S) = 1.
3. Given Am, 1 ≤m ≤M, such that Am ∩Am′ = ∅∀m′ ̸= m, 1 ≤m′ ≤M, then P
M
m=1 Am

=
M
m=1 P(Am).
Referring to an experiment with sample space S:
•
The events Am, 1 ≤m ≤M, are said to be mutually exclusive when the occurrence of one prevents
the occurrence of the others. They are the subject of the third axiom.
•
The complement A of a given event A, illustrated in Figure 4.2, is determined by the non-occurrence
of A, i.e., A = S −A. From this deﬁnition, P(A) = 1 −P(A). Complementary events are also
mutually exclusive.
•
An event A = ∅is called impossible. From this deﬁnition, P(A) = 0.
•
An event A = S is called certain. From this deﬁnition, P(A) = 1.
It should be emphasized that all events A related to a given experiment are completely determined
by the sample space S, since by deﬁnition A ⊂S. Therefore, a set B of outcomes not in S is mapped
to an event B ∩S = ∅. For instance, for the experiment of rolling a fair die, S = {1, 2, 3, 4, 5, 6}; the
event corresponding to B = {7} (showing a 7) is B ∩S = ∅.
According to the experiment, sample spaces may be countable or uncountable.3 For example, the
sample space of the coin ﬂip experiment is countable. On the other hand, the sample space of the
experiment that consists in sampling with no preference any real number from the interval S = (0, 100]
3One could think of mixed cases, but this discussion is better conveyed in the random variable framework, which comprises
every possible mapping from the original experiment to a subset of the real numbers.

116
CHAPTER 4 Random Signals and Stochastic Processes
is uncountable. Given an individual outcome s ∈S, deﬁning A = {s},
P(A)
 ̸= 0, if S is countable;
= 0,
if S is uncountable.
(4.3)
As a consequence, one should not be surprised to ﬁnd an event A ̸= ∅with P(A) = 0 or an event
A ̸= S with P(A) = 1. In the (0, 100] interval sampling experiment, A = {50} ̸= ∅has P(A) = 0;
and A ̸= S has P(A) = 1.
1.04.2.1 Joint, conditional, and total probability—Bayes’ rule
The joint probability of a set of events Am, 1 ≤m ≤M, is the probability of their simultaneous
occurrence M
m=1 Am. Referring to Figure 4.3, given two events A and B, their joint probability can be
found as
P(A ∩B) = P(A) + P(B) −P(A ∪B).
(4.4)
By rewriting this equation as
P(A ∪B) = P(A) + P(B) −P(A ∩B),
(4.5)
one ﬁnds an intuitive result: the term P(A ∩B) is included twice in P(A) + P(B), and should thus
be discounted. Moreover, when A and B are mutually exclusive, we arrive at the third axiom. In the
die experiment, deﬁning A = {s ∈S|s is even} = {2, 4, 6} and B = {s ∈S|s > 3} = {4, 5, 6}:
P(A) = 1/2, P(B) = 1/2, P(A ∩B) = P({4, 6}) = 1/3, and P(A ∪B) = P({2, 4, 5, 6}) = 2/3.
The conditional probability P(A|B) is the probability of event A conditioned to the occurrence of
event B, and can be computed as
P(A|B) = P(A ∩B)
P(B)
, P(B) > 0.
(4.6)
The value of P(A ∩B) accounts for the uncertainty of both A and B; the term P(B) discounts the
uncertainty of B, since it is certain in this context. In fact, the conditioning event B is the new (reduced)
sample space for the experiment. By rewriting this equation as
P(A ∩B) = P(A|B)P(B),
(4.7)
S
A
A
A
B
A
B
B
B
S
FIGURE 4.3
A ∩B and A ∪B.

1.04.2 Probability
117
one gets another interpretation: the joint probability of A and B combines the uncertainty of B with
the uncertainty of A when B is known to occur. Using the example in the last paragraph, P(A|B) =
[P({4, 6}) in sample space B] = 2/3.
Since P(A ∩B) = P(B ∩A) = P(B|A)P(A), the Bayes Rule follows straightforwardly:
P(A|B) = P(B|A)P(A)
P(B)
.
(4.8)
This formula allows computing one conditional probability P(A|B) from its reverse P(B|A). Using
again the example in the last paragraph, one would arrive at the same result for P(A|B) using P(B|A) =
[P({4, 6}) in sample space A] = 2/3 in the last equation.
If the sample space S is partitioned into M disjoint sets Am, 1 ≤m ≤M, such that Am ∩Am′ =
∅∀m′ ̸= m, 1 ≤m′ ≤M, then any event B ⊂S can be written as B = M
m=1(B∩Am). Since
B ∩Am, 1 ≤m ≤M, are disjoint sets,
P(B) =
M

m=1
P(B|Am)P(Am),
(4.9)
which is called the total probability of B.
For a single event of interest A, for example, the application of Eq. (4.9) to Eq. (4.8) yields
P(A|B) =
P(B|A)P(A)
P(B|A)P(A) + P(B|A)P(A)
.
(4.10)
Within the Bayes context, P(A) and P(A|B) are usually known as the a priori and a posteriori
probabilities of A, respectively; P(B|A) is called likelihood in the estimation context, and also tran-
sition probability in the communications context. In the latter case, A = {a} could refer to a symbol a
sent by the transmitter and B = {b}, to a symbol b recognized by the receiver. Knowing P(B|A), the
probability of recognizing b when a is sent (which models the communication channel), and P(A), the
a priori probability of the transmitter to send a, allows to compute P(A|B), the probability of a having
been sent given that b has been recognized. In the case of binary communication, we could partition
the sample space either in the events TX0 (0 transmitted) and TX1 (1 transmitted), or in the events
RX0 (0 recognized) and RX1 (1 recognized), as illustrated in Figure 4.4; the event “error” would be
E = (TX0 ∩RX1) ∪(TX1 ∩RX0).
S
TX0
TX1
S
RX0
RX1
FIGURE 4.4
Binary communication example: partitions of S regarding transmission and reception.

118
CHAPTER 4 Random Signals and Stochastic Processes
1.04.2.2 Probabilistic independence
The events Am, 1 ≤m ≤M are said to be mutually independent when the occurrence of one does
not affect the occurrence of any combination of the others.
For two events A and B, three equivalent tests can be employed: they are independent if and only if.
1. P(A|B) = P(A), or
2. P(B|A) = P(B), or
3. P(A ∩B) = P(A)P(B).
The ﬁrst two conditions follow directly from the deﬁnition of independence. Using any of them in
Eq. (4.6) one arrives at the third one. The reader is invited to return to Eq. (4.7) and conclude that B
and A|B are independent events. Consider the experiment of rolling a special die with the numbers 1,
2, and 3 stamped in black on three faces and stamped in red on the other three; the events A = {1} and
B = {s ∈S|s is a red number} are mutually independent.
Algorithmically, the best choice for testing the mutual independence of more than two events is using
the third condition for every combination of 2, 3, . . . , M events among A1, A2, . . . , AM.
As a ﬁnal observation, mutually exclusive events are not mutually independent; on the contrary, one
could say they are maximally dependent, since the occurrence of one precludes the occurrence of the
other.
1.04.2.3 Combined experiments—Bernoulli trials
The theory we have discussed so far can be applied when more than one experiment is performed at a
time. The generalization to the multiple experiment case can be easily done by using cartesian products.
Consider the experiments Em with their respective sample spaces Sm, 1 ≤m ≤M. Deﬁne the
combined experiment E such that each of its trials is composed by one trial of each Em. An outcome of
E is the M-tuple (s1, s2, . . . , sM), where sm ∈Sm, 1 ≤m ≤M, and the sample space of E can be written
as S = S1 × S2 ×· · ·× SM. Analogously, any event of E can be expressed as A = A1 × A2 ×· · ·× AM,
where Am is a properly chosen event of Em, 1 ≤m ≤M.
In the special case when the sub-experiments are mutually independent, i.e., the outcomes of one do
not affect the outcomes of the others, we have P(A) = 	M
m=1 P(Am). However, this is not the general
case. Consider the experiment of randomly selecting a card from a 52-card deck, repeated twice: if the
ﬁrst card drawn is replaced, the sub-experiments are independent; if not, the ﬁrst outcome affects the
second experiment. For example, for A = {(ace of spades, ace of spades)}, P(A) = (1/52)2 if the ﬁrst
card is replaced, and P(A) = 0 if not.
At this point, an interesting counting experiment (called Bernoulli trials) can be deﬁned. Take a
random experiment E with sample space S and a desired event A (success) with P(A) = p, which
also deﬁnes A (failure) with P(A) = 1 −p. What is the probability of getting exactly k successes
in N independent repeats of E? The solution can be easily found by noticing that the desired result is
composed by k successes and N −k failures, which may occur in

 N
k

different orders. Then,
P(k successes after N trials) =

 N
k

pk(1 −p)N−k.
(4.11)

1.04.3 Random Variable
119
0 
1 
0 
0 
0 
0 
0 
0 
1 
1 
1 
1 
1 
1 
FIGURE 4.5
Three consecutive bits sent through a binary communication system.
When N →∞, p →0, and Np →a constant,
P(k successes after N trials) →(Np)ke−Np
k!
.
(4.12)
Returning to the card deck experiment (with replacement): the probability of selecting exactly 2 aces of
spades after 300 repeats is approximately 5.09% according to Eq. (4.11), while Eq. (4.12) provides the
approximate value 5.20%. In a binary communication system where 0 and 1 are randomly transmitted
with equal probabilities, the probability that exactly 2 bits 1 are sent among 3 bits transmitted is 0.25%;
this result can be easily checked by inspection of Figure 4.5.
1.04.3 Random variable
Mapping each outcome of a random experiment to a real number provides a different framework for the
study of probabilistic models, amenable to simple interpretation and easy mathematical manipulation.
This mapping is performed by the so-called random variable.
Given a random experiment with sample space S, a random variable is any function x(s) that maps
each s ∈S into some x ∈R (see Figure 4.6). The image of this transformation with domain S and
S
s
x
x
FIGURE 4.6
Sample space S mapped into R via random variable x(s).

120
CHAPTER 4 Random Signals and Stochastic Processes
co-domain R, which results of the convenient choice of the mapping function, can be seen as the sample
space of x; any event A of x can be described as a subset of R; and each mapped outcome x is called a
sample of x. The following conditions should be satisﬁed by a random variable x:
1. {x ≤x} is an event ∀x ∈R;
2. P({x = −∞}) = P({x = ∞}) = 0.
We will see later that these conditions allow the proper deﬁnition of the cumulative probability distri-
bution function of x.
As seen in Section 1.04.2, sample spaces (and events) can be countable or uncountable, according to
the nature of the random experiment’s individual outcomes. After mapped into subsets of R, countable
events remain countable—e.g., one could associate with the coin ﬂip experiment a random variable v
such that v(head) = 0 and v(tail) = 1. On the other hand, uncountable events may or may not remain
uncountable. Consider the following four distinct deﬁnitions of a random variable i associated with the
(0, 100] interval experiment described immediately before Eq. (4.3):
1. i1(s) = s;
2. i2(s) =
 −1, s ≤50;
1,
s > 50;
3. i3(s) =
s,
s ≤50;
100, s > 50;
4. i4(s) = min[s, 50].
The sample space of:
1. i1, (0, 100], is uncountable;
2. i2, {−1, 1}, is countable;
3. i3, (0, 50] ∪{100}, is part uncountable, part countable;
4. i4, (0, 50], even if continuous, may be difﬁcult to classify.
The classiﬁcation of sample spaces as countable or uncountable leads directly to the classiﬁcation
of random variables as discrete, continuous, or mixed. A discrete random variable d has a countable
sample space Sd, and has P({d = d}) > 0, ∀d ∈Sd—this is the case of v and i2 deﬁned above.
A continuous random variable c has an uncountable sample space Sd, and (to avoid ambiguity) has
P({c = c}) = 0, ∀c ∈Sc—this is the case of i1 deﬁned above. A mixed variable m has a sample space
Sd composed by the union of real intervals with continuously distributed probabilities, within which
P({m = m}) = 0, and discrete real values with ﬁnite probabilities P({m = m}) > 0—this is the case
of i3 and i4 deﬁned above. Speciﬁcally, since P(0 < i4 < 50) = P(i4 = 50) = 50%, Si4 should rather
be treated as part uncountable, part countable than as simply uncountable.
1.04.3.1 Probability distributions
From the conditions that must be satisﬁed by any random variable x, an overall description of its
probability distribution can be provided by the so-called cumulative probability distribution function
(shortened to CDF),
Fx(x) = P({x ≤x}).
(4.13)

1.04.3 Random Variable
121
0
0.5
1
0
0.5
1
x
Fx
FIGURE 4.7
Example of cumulative probability distribution function.
Since P(x = −∞) = 0, Fx(−∞) = 0. It is obvious that Fx(∞) = 1; but since P({x = ∞}) =
0, P({x < ∞}) = 1. Moreover, 0 ≤Fx(x) ≤1 and is a non-decreasing function of x, i.e., Fx(x1) ≤
Fx(x2) if x1 < x2. Also, Fx(x+) = Fx(x), i.e., Fx(x) is continuous from the right; this will be important
in the treatment of discrete random variables. A typical CDF is depicted in Figure 4.7. One can use the
CDF to calculate probabilities by noticing that
P({x1 < x ≤x2}) = Fx(x2) −Fx(x1).
(4.14)
For the random variable x whose distribution is described in Figure 4.7, one can easily check by
inspection that P

x ≤1
2

= 50%.
The CDF of the random variable i1 associated above with the (0, 100] interval sampling experiment is
Fi1(i1) =
⎧
⎨
⎩
0,
i1 ≤0;
i1, 0 < i1 ≤100;
1,
i1 > 100.
(4.15)
The CDF of the random variable v associated above with the coin ﬂip experiment is
Fv(v) =
⎧
⎨
⎩
0,
v < 0;
0.5, 0 ≤v < 1;
1,
v ≥1.
(4.16)
Given a random variable x, any single value x0 such that P({x = x0}) = p > 0 contributes a step4
of amplitude p to Fx(x). Then, it is easy to conclude that for a discrete random variable d with
4Unit step function: u(x) =
 0,
x < 0;
1,
x ≥0.

122
CHAPTER 4 Random Signals and Stochastic Processes
Sd = {. . . , di−1, di, di+1, . . .},
Fd(d) =

i
P({d = di})u(d −di).
(4.17)
An even more informative function that can be derived from the CDF to describe the probability
distribution of a random variable x is the so-called probability density function (shortened to PDF),
fx(x) = dFx(x)
dx
.
(4.18)
Since Fx(x) is a non-decreasing function of x, it follows that fx(x) ≥0. From the deﬁnition,
Fx(x) =
 x+
−∞
fx(˜x)d˜x.
(4.19)
Then,
 ∞
−∞
fx(x)dx = 1.
(4.20)
The PDF corresponding to the CDF shown in Figure 4.7 is depicted in Figure 4.8. One can use the PDF
to calculate probabilities by noticing that
P({x1 < x ≤x2}) =
 x+
2
x+
1
fx(x)dx.
(4.21)
Again, for the random variable x whose distribution is described in Figure 4.8, one can easily check by
inspection that

x ≤1
2

= 50%.
The PDF of the random variable i1 associated above with the (0, 100] interval sampling experiment is
fi1(i1) =
 1, 0 < i1 ≤100;
0, otherwise.
(4.22)
The PDF of the random variable v associated above with the coin ﬂip experiment is
fv(v) =
∞, v = 0 or v = 1;
0,
otherwise,
(4.23)
which is not well-deﬁned. But coherently with what has been seen for the CDF, given a random variable
x, any single value x0 such that P({x = x0}) = p > 0 contributes an impulse5 of area p to fx(x).
Then, it is easy to conclude that for a discrete random variable d with Sd = {. . . , di−1, di, di+1, . . .},
fd(d) =

i
P({d = di})δ(d −di).
(4.24)
5Unit impulse distribution, or Dirac delta: For x ∈R, δ(x) = 0 ∀x ̸= 0 and
 ∞
−∞δ(x)dx = 1.

1.04.3 Random Variable
123
0
0.5
1
0
2
fx
x
FIGURE 4.8
Example of probability density function.
In particular, for the coin ﬂip experiment the PDF is fv(v) = 0.5δ(v) + 0.5δ(v −1).
In the case of discrete random variables, in order to avoid the impulses in the PDF, one can operate
directly on the so-called mass probability function6:
Pd(d) = P({d = d}).
(4.25)
In this case,
Fd(d) =

i|di≤d
Pd(di).
(4.26)
This chapter favors an integrate framework for continuous and discrete variables, based on CDFs and
PDFs.
It is usual in the literature referring (for short) to the CDF as “the distribution” and to the PDF as “the
density” of the random variable. This text avoids this loose terminology, since the word “distribution”
better applies to the overall probabilistic behavior of the random variable, no matter in the form of a
CDF or a PDF.
1.04.3.2 Usual distributions
The simplest continuous distribution is the so-called uniform. A random variable x is said to be uni-
formly distributed between a and b > a if its PDF is
fx(x) =
⎧
⎨
⎩
1
b −a , a ≤x ≤b;
0,
otherwise,
(4.27)
6This unusual notation is employed here for the sake of compatibility with the other deﬁnitions.

124
CHAPTER 4 Random Signals and Stochastic Processes
0
x
fx
1
b −a
b
a
FIGURE 4.9
Uniform probability distribution.
depicted in Figure 4.9. Notice that the inclusion or not of the interval bounds is unimportant here, since
the variable is continuous. The error produced by uniform quantization of real numbers is an example
of uniform random variable.
Perhaps the most recurrent continuous distribution is the so-called Gaussian (or normal). A Gaussian
random variable x is described by the PDF
fx(x) =
1

2πσ 2x
e
−

x−x
2
2σ2x .
(4.28)
As seen in Figure 4.10, this function is symmetrical around x, with spread controlled by σx. These
parameters, respectively called statistical mean and standard deviation of x, will be precisely deﬁned in
Section 1.04.3.4. The Gaussian distribution arises from the combination of several independent random
phenomena, and is often associated with noise models.
There is no closed expression for the Gaussian CDF, which is usually tabulated for a normalized
Gaussian random variable ˜x with ˜x = 0 and σ 2
˜x = 1 such that
f ˜x(˜x) =
1
√
2π
e−˜x2
2 .
(4.29)
In order to compute p = P({x1 < x ≤x2}) for a Gaussian random variable x, one can build an
auxiliary variable ˜x = x−x
σ 2x
such that p = P({˜x1 < ˜x ≤˜x2}) = F˜x(˜x2) −F˜x(˜x1), which can then be
approximated by tabulated values of F˜x(˜x).
Example 1.
The values r of a 10-% resistor series produced by a component industry can be modeled
by a Gaussian random variable r with σr = 1
30r. What is the probability of producing a resistor within
±1% of r?

1.04.3 Random Variable
125
0
x
fx
1
2πσ2x
x
FIGURE 4.10
Gaussian probability distribution.
Solution 1.
The normalized counterpart of r is ˜r = r−r
1
30r . Then,
P({0.99 r ≤r ≤1.01 r}) = P({−0.3 ≤˜r ≤0.3}) = F˜r(0.3) −F˜r( −0.3) ≈0.2358.
(4.30)
Notice once more that since the variable is continuous, the inclusion or not of the interval bounds
has no inﬂuence on the result.
In Section 1.04.2.3, an important discrete random variable was implicitly deﬁned. A random variable
x that follows the so-called binomial distribution is described by
fx(x) =
N

k=0

 N
k

pk(1 −p)N−kδ(x −k),
(4.31)
and counts the number of occurrences of an event A which has probability p, after N independent repeats
of a random experiment. Games of chance are related to binomial random variables.
The so-called Poisson distribution, described by
fx(x) = e−λT
∞

k=0
(λT)k
k!
δ(x −k),
(4.32)
counts the number of occurrences of an event A that follows a mean rate of λ occurrences per unit time,
during the time interval T. Trafﬁc studies are related to Poisson random variables.
It is simple to derive the Poisson distribution from the binomial distribution. If an event A may occur
with no preference anytime during a time interval (t0, t0 + T0), the probability that A occurs within the
time interval [t, t + T ] ⊂(t0, t0 + T0) is T
T0 . If A occurs N times in (t0, t0 + T0), the probability that
it falls exactly k times in [t, t + T ] ⊂(t0, t0 + T0) is

N
k

pk(1 −p)N−k. If T0 →∞, p →0; if A

126
CHAPTER 4 Random Signals and Stochastic Processes
follows a mean rate of λ = N
T0 occurrences per unit time, then N →∞and the probability of exact k
occurrences in a time interval of duration T becomes e−λT ∞
k=0
(λT )k
k! , where λT substituted for Np.
If a central ofﬁce receives 100 telephone calls per minute in the mean, the probability that more than 1
call arrive during 1s is 1 −e−100
60 1 
1 = 100
60 1

≈49.6%.
Due to space restrictions, this chapter does not detail other random distributions, which can be easily
found in the literature.
1.04.3.3 Conditional distribution
In many instances, one is interested in studying the behavior of a given random variable x under some
constraints. A conditional distribution can be built to this effect, if the event B ⊂R summarizes those
constraints. The corresponding CDF of x conditioned to B can be computed as
Fx(x|B) = P({x ≤x|B}) = P({x ≤x} ∩B)
P(B)
.
(4.33)
The related PDF is simply
fx(x|B) = dFx(x|B)
dx
.
(4.34)
Conditional probabilities can be straightforwardly computed from conditional CDFs or PDFs.
When the conditioning event is an interval B = (a, b], it can be easily deduced that
Fx(x|B) =
⎧
⎪⎪⎨
⎪⎪⎩
0,
x ≤a;
Fx(x) −Fx(a)
Fx(b) −Fx(a), a < x ≤b;
1,
x > b;
(4.35)
and
fx(x|B) =
⎧
⎨
⎩
fx(x)
 b
a fx(x)dx
, a < x ≤b;
0,
otherwise.
(4.36)
Both sentences in Eq. (4.36) can be easily interpreted:
•
Within (a, b], the conditional PDF has the same shape of the original one; the normalization factor
 b
a fx(x)dx ensures P((a, b]) = 100% in the restricted sample space (a, b].
•
By deﬁnition, there is null probability of getting x outside (a, b].
As an example, the random variable H deﬁned by the non-negative outcomes of a normalized Gaussian
variable follows the PDF
fh(h) =

2
π e−h2
2 u(h).
(4.37)
The results shown in this section can be promptly generalized to any conditioning event.

1.04.3 Random Variable
127
1.04.3.4 Statistical moments
The concept of mean does not need any special introduction: the single value that substituted for each
member in a set of numbers produces the same total. In the context of probability, following a frequentist
path, one could deﬁne the arithmetic mean of inﬁnite samples of a random variable x as its statistical
mean x or its expected value E[x].
Recall the random variable v associated with the fair coin ﬂip experiment, with Pv(0) = Pv(1) = 0.5.
After inﬁnite repeats of the experiment, one gets 50% of heads (v = 0) and 50% of tails (v = 1); then,
the mean outcome will be7 E[v] = 0.5. If another variable v′ is associated with an unfair coin with
probabilities Pv′(0) = 0.4 and Pv′(1) = 0.6, the same reasoning leads to E[v′] = 0.6. Instead of
averaging inﬁnite outcomes, just summing the possible values of the random variable weighted by their
respective probabilities also yields its statistical mean. Thus we can state that for any discrete random
variable d with sample space S = {. . . , di−1, di, di+1, . . .},
E[d] =

i
di Pd(di).
(4.38)
This result can be generalized. Given a continuous random variable x, the probability of drawing a
value in the interval dx around x0 is given by fx(x0)dx. The weighted sum of every x ∈R is simply
E[x] =
 ∞
−∞
x fx(x)dx.
(4.39)
By substituting the PDF of a discrete variable d (see Eq. (4.24)) into this expression, one arrives at
Eq. (4.38). Then, Eq. (4.39) is the analytic expression for the expected value of any random variable x.
Suppose another random variable g(x) is built as a function of x. Since the probability of getting the
value x0 is the same as getting the respective g(x0), we can deduce that
E[g(x)] =
 ∞
−∞
g(x) fx(x)dx.
(4.40)
A complete family of measures based on expected values can be associated with a random variable.
The so-called nth-order moment of x (about the origin) is deﬁned as
mn = E[xn], n ∈Z.
(4.41)
The ﬁrst two moments of x are:
•
m1 = x, i.e., the mean of x, given by the centroid of fx(x).
•
m2 = x2, i.e., the mean square value of x.
A modiﬁed family of parameters can be formed by computing the moments about the mean. The
so-called nth-order central moment of x is deﬁned as
μn = E[(x −x)n], n ∈Z.
(4.42)
Subtracting the statistical mean from a random variable can be interpreted as disregarding its “deter-
ministic” part (represented by its statistical mean) Three special cases:
7Notice that the expected value of a random variable is not necessarily meaningful for the associated experiment.

128
CHAPTER 4 Random Signals and Stochastic Processes
•
μ2 = σ 2
x , known as the variance of x, which measures the spread of fx(x) around x. The so-called
standard deviation σx = √μ2 is a convenient measure with the same dimension of x.
•
μ3, whose standardized version μ3
σ 3x is the so-called skewness of x, which measures the asymmetry
of fx(x).
•
μ4, whose standardized version8 minus three μ4
σ 4x −3 is the so-called kurtosis of x, which measures
the peakedness of fx(x). One can say the distributions are measured against the Gaussian, which
has a null kurtosis.
Of course, analogous deﬁnitions apply to the moments computed over conditional distributions.
A useful expression, which will be recalled in the context of random processes, relates three of the
measures deﬁned above:
σ 2
x = x2 −x2.
(4.43)
Rewritten as
x2 = x2 + σ 2
x ,
(4.44)
it allows to split the overall “intensity” of x, measured by its mean square value, into a deterministic
part, represented by x2, and a random part, represented by σ 2
x .
As an example, consider a discrete random variable d distributed as shown in Table 4.1. Their
respective parameters are:
•
mean d = 0.1, indicating the PDF of d is shifted to the right of the origin;
•
mean square value d2 = 1, which measures the intensity of d;
•
variance σ 2
d = 0.49, which measures the random part of the intensity of d;
•
standard deviation σd = 0.7, which measures the spread of d around its mean;
•
skewness μ3
σ 3
d ≈−0.14, indicating the PDF of d is left-tailed;
•
kurtosis μ4
σ 4
d −3 ≈−0.96, indicating the PDF of d is less peaky than the PDF of a Gaussian variable.
At this point, we can discuss two simple transformations of a random variable x whose effects can
be summarized by low-order moments:
•
A random variable y = x + x0 can be formed by adding a ﬁxed offset x0 to each sample of x.
As a consequence of this operation, the new PDF is a shifted version of the original one: fy(y) =
fx(y −x0), thus adding x0 to the mean of x: y = x + x0.
8The classical deﬁnition does not subtracts three.

1.04.3 Random Variable
129
•
A random variable y = αx can be formed by scaling by α each sample of x. As a consequence of
this operation, the new PDF is a scaled version of the original one: fy(y) = 1
α fx
 y
α

, thus scaling
by α the standard deviation of x: σy = ασx.
Such transformations do not change the shape (and therefore the type) of the original distribution. In
particular, one can generate:
•
a zero-mean version of x by making y = x −x, which disregards the deterministic part of x;
•
a unit-standard deviation version of x by making y = x
σx , which enforces a standard statistical
variability to x;
•
a normalized version of x by making y = x−x
σx , which combines both effects.
We already deﬁned a normalized Gaussian distribution in Section 1.04.3.2. The normalized version ˜d
of the random variable d of the last example would be distributed as shown in Table 4.2.
The main importance of these expectation-based parameters is to provide a partial description of an
underlying distribution without the need to resource to the PDF. In a practical situation in which only
a few samples of a random variable are available, as opposed to its PDF and related moments, it is
easier to get more reliable estimates for the latter (especially low-order ones) than for the PDF itself.
The same rationale applies to the use of certain auxiliary inequalities that avoid the direct computation
of probabilities on a random variable (which otherwise would require the knowledge or estimation of
its PDF) by providing upper bounds for them based on low-order moments. Two such inequalities are:
•
Markov’s inequality for a non-negative random variable x : P({x ≥a}) ≤x
a , ∀a > 0.
•
Chebyshev’s inequality for a random variable x: P({|x −x| ≥a}) ≤
σ 2
x
a2 , ∀a > 0.
The derivation of Markov’s inequality is quite simple:
P{x ≥a} =
 ∞
a
fx(x)dx =
 ∞
−∞
fx(x)u(x −a)dx
≤
 ∞
−∞
x
a fx(x)u(x −a)dx ≤
 ∞
−∞
x
a fx(x)dx = E[x]
a
.
(4.45)
Chebyshev’s inequality follows directly by substituting (x −x)2 ≥a2 for x ≥a in Markov’s inequality.
As an example, the probability of getting a sample g > 3 from the normalized Gaussian random variable
g is P({g > 3}) ≈0.0013; Chebyshev’s inequality predicts P({g > 3}) ≤1
9, an upper bound almost
100 times greater than the actual probability.

130
CHAPTER 4 Random Signals and Stochastic Processes
Two representations of the distribution of a random variable in alternative domains provide interest-
ing links with its statistical moments. The so-called characteristic function of a random variable x is
deﬁned as
x(ω) = E[ejωx], ω ∈R,
(4.46)
and inter-relates the moments mn by successive differentiations:
mn = ( −j)n dnx(ω)
dωn

ω=0
.
(4.47)
The so-called moment generating function of a random variable x is deﬁned as
Mx(ν) = E[eνx], ν ∈R,
(4.48)
and provides a more direct way to compute mn:
mn = dnMx(ν)
dνn

ν=0
.
(4.49)
1.04.3.5 Transformation of random variables
Consider the problem of describing a random variable y = g(x) obtained by transformation of another
random variable x, illustrated in Figure 4.11. By deﬁnition,
Fy(y) = P({y ≤y}) = P({g(x) ≤y}).
(4.50)
If x is a continuous random variable and g( · ) is a differentiable function, the sentence g(x) ≤y is
equivalent to a set of sentences in the form h1(y) < x ≤h2(y). Since P({h1(y) < x ≤h2(y)}) =
Fx(h2(y)) −Fx(h1(y)), then Fy(y) can be expressed as a function of Fx(x).
Fortunately, an intuitive formula expresses fy(y) as a function of fx(x):
fy(y) =

n
fx(xn)
dg(x)
dx


x=xn
, with y = g(xn).
(4.51)
S
s
x
x
y
g(.)
FIGURE 4.11
Random variable x mapped into random variable y.

1.04.3 Random Variable
131
Given that the transformation g( · ) is not necessarily monotonic, xn are all possible values mapped
to y; therefore, their contributions must be summed up. It is reasonable that fy(y) must be directly
proportional to fx(x): the more frequent a given x0, the more frequent its respective y0 = g(x0). By
its turn, the term
dg(x)
dx
 accounts for the distortion imposed to fx(x) by g(x). For example, if this
transformation is almost constant in a given region, no matter if increasing or decreasing, then
dg(x)
dx

in the denominator will be close to zero; this just reﬂects the fact that a wide range of x values will be
mapped into a narrow range of values of y, which will then become denser than x in thar region.
As an example, consider that a continuous random variable x is transformed into a new variable
y = x2. For the CDF of y,
y ≤y ⇒x2 ≤y ⇒−√y ≤x ≤√y ⇒Fy(y) = Fx(√y) −Fx( −√y).
(4.52)
By Eq. (4.51), or by differentiation of Eq. (4.52), one arrives at the PDF of y:
fy(y) = fx(√y) + fx( −√y)
√y
.
(4.53)
The case when real intervals of x are mapped into single values of y requires an additional care to
treat the nonzero individual probabilities of the resulting values of the new variable. However, the case
of a discrete random variable x with S = {. . . , xi−1, xi, xi+1, . . .} being transformed is trivial:
fy(y) =

i
Py(yi)δ(y −yi), with yi = g(xin) and Py(yi) =

n
Px(xin).
(4.54)
Again, xin are all possible values mapped to yi.
An interesting application of transformations of random variables is to obtain samples of a given
random variable y from samples of another random variable x, both with known distributions. Assume
that exists y = g(x) such that Fy(y = g(x)) = Fx(x). Then, y = F−1
y
Fx(x), which requires only the
invertibility of Fy(y).
1.04.3.6 Multiple random variable distributions
A single random experiment E (composed or not) may give rise to a set of N random variables, if each
individual outcome s ∈S is mapped into several real values x1, x2, . . . , xN. Consider, for example, the
random experiment of sampling the climate conditions at every point on the globe; daily mean temper-
ature t and relative air humidity h can be considered as two random variables that serve as numerical
summarizing measures. One must generalize the probability distribution descriptions to cope with this
multiple random variable situation: it should be clear that the set of N individual probability distribution
descriptions, one for each distinct random variable, provides less information than their joint probability
distribution description, since the former cannot convey information about their mutual inﬂuences.

132
CHAPTER 4 Random Signals and Stochastic Processes
We start by deﬁning a multiple or vector random variable as the function x that maps s ∈S into
x ∈RN, such that
x =
⎡
⎢⎢⎢⎣
x1
x2...
x N
⎤
⎥⎥⎥⎦
(4.55)
and
x =
⎡
⎢⎢⎢⎣
x1
x2
...
xN
⎤
⎥⎥⎥⎦.
(4.56)
Notice that x1, x2, . . . , xN are jointly sampled from x1, x2, . . . , x N.
The joint cumulative probability distribution function of x1, x2, . . . , x N, or simply the CDF of
x, can be deﬁned as
Fx(x) = Fx1,x2,...,x N (x1, x2, . . . , xN) = P({x1 ≤x1} ∩{x2 ≤x2} ∩. . . ∩{x N ≤xN}).
(4.57)
The following relevant properties of the joint CDF can be easily deduced:
•
Fx( . . . , xn−1, −∞, xn+1, . . . ) = 0, since P({xn ≤−∞}) = 0 for any 1 ≤n ≤N.
•
Fx(∞, ∞, . . . , ∞) = 1, since this condition encompasses the complete S.
•
0 ≤Fx(x) ≤1 and is a nondecreasing function of xn, 1 ≤n ≤N, by construction.
Deﬁne xi containing Ni variables of interest among the random variables in x, and leave the remaining
N −Ni nuisance variables in xn. The so-called marginal CDF of xi1, xi2, . . . , xiNi separately describes
these variables’ distribution from the knowledge of the CDF of x:
Fxi(xi) = Fx(x)

xn1=xn2=...=xnN−Ni =∞.
(4.58)
One says the variables xn1, xn2, . . . , xnN−Ni have been marginalized out: the condition xnm ≤∞simply
means they must be in the sample space, thus one does not need to care about them anymore.
The CDF of a discrete random variable d with Sd = {. . . , di−1, di, di+1, . . .}, analogously to the
single variable case, is composed by steps at the admissible N-tuples:
Fd(d) =

i
Pd(di)u(d1 −d1i)u(d2 −d2i) . . . u(dN −dN i).
(4.59)
The joint probability density function of x1, x2, . . . , x N, or simply the PDF of x, can be deﬁned as
fx(x) = fx1,x2,...,x N (x1, x2, . . . , xN) =
∂N Fx(x)
∂x1∂x2 . . . ∂xN
.
(4.60)
Since Fx(x) is a non-decreasing function of xn, 1 ≤n ≤N, it follows that fx(x) ≥0. From the
deﬁnition,
Fx(x) =
 xN
−∞
· · ·
 x2
−∞
 x1
−∞
fx(˜x)d˜x1 d˜x2 · · · d˜xN.
(4.61)

1.04.3 Random Variable
133
Then,
 ∞
−∞
· · ·
 ∞
−∞
 ∞
−∞
fx(x)dx1 dx2 · · · dxN = 1.
(4.62)
The probability of any event B ∈RN can be computed as
P(B) =

x∈B
fx(x)dx1 dx2 · · · dxN.
(4.63)
Once more we can marginalize the nuisance variables xn1, xn2, . . . , xnN−Ni to obtain the marginal
PDF of xi from the PDF of x:
fxi(xi) =
 ∞
−∞
· · ·
 ∞
−∞
 ∞
−∞
fx(x)dxn1 dxn2 · · · dxnN−Ni.
(4.64)
Notice that if xi and xn are statistically dependent, the marginalization of xn does not “eliminate” the
effect of xn on xi: the integration is performed over x, which describes their mutual inﬂuences. For a
discrete random variable d with Sd = {. . . , di−1, di, di+1, . . .} consists of impulses at the admissible
N-tuples:
fd(d) =

i
Pd(di)δ(d1 −d1i)δ(d2 −d2i) . . . δ(dN −dN i).
(4.65)
Suppose, for example, that we want to ﬁnd the joint and marginal CDFs and PDFs of two random
variables x and y jointly uniformly distributed in the region 0 ≤y ≤x ≤1, i.e., such that
fx,y(x, y) =
2, 0 ≤y ≤x ≤1;
0, otherwise.
(4.66)
The marginalization of y yields
fx(x) =
 ∞
−∞
fx,y(x, y)dy =
 x
0
2dy =
 2x, 0 ≤x ≤1;
0,
otherwise.
(4.67)
The marginalization of x yields
fy(y) =
 ∞
−∞
fx,y(x, y)dx =
 1
y
2dx =
2(1 −y), 0 ≤y ≤1;
0,
otherwise.
(4.68)
By deﬁnition,
Fx,y(x, y) =
 y
−∞
 x
−∞
fx,y(˜x, ˜y)d˜xd ˜y =
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
0,
x < 0 or y < 0;
y(2x −y), 0 ≤y ≤x ≤1;
x2,
0 ≤x ≤1 and y > x;
y(2 −y),
0 ≤y ≤1 and x > 1;
1,
x > 1 and y > 1.
(4.69)

134
CHAPTER 4 Random Signals and Stochastic Processes
The reader is invited to sketch the admissible region 0 ≤y ≤x ≤1 on the (x, y) plane and try to solve
Eq. (4.69) by visual inspection. The marginal CDF of x is given by
Fx(x) = Fx,y(x, ∞) =
⎧
⎨
⎩
0,
x < 0;
x2, 0 ≤x ≤1;
1,
x > 1,
(4.70)
which could also have been obtained by direct integration of fx(x). The marginal CDF of y is given by
Fy(y) = Fx,y(∞, y) =
⎧
⎨
⎩
0,
y < 0;
y(2 −y), 0 ≤y ≤1;
1,
y > 1,
(4.71)
which could also have been obtained by direct integration of fy(y).
Similarly to the univariate case discussed in Section 1.04.3.3, the conditional distribution of a vector
random variable x restricted by a conditioning event B ∈RN can be described by its respective CDF
Fx(x|B) = P({x1 ≤x1} ∩{x2 ≤x2} ∩. . . ∩{x N ≤xN} ∩B)
P(B)
(4.72)
and PDF
fx(x|B) =
∂N Fx(x|B)
∂x1 ∂x2 · · · ∂xN
.
(4.73)
A special case arises when B imposes a point conditioning: Deﬁne xB containing NB variables among
those in x, such that B = {xB = xB}. It can be shown that
fx(x|B) =
fx(x)
fxB(xB),
(4.74)
an intuitive result in light of Eq. (4.6)—to which one may directly refer in the case of discrete random
variables. Returning to the last example, the point-conditioned PDFs can be shown to be
fx(x|y = y) =

1
1−y , 0 ≤y ≤x ≤1;
0,
otherwise;
(4.75)
and
fy(y|x = x) =
 1
x , 0 ≤y ≤x ≤1;
0,
otherwise.
(4.76)
1.04.3.7 Statistically independent random variables
Based on the concept of statistical independence, discussed in Section 1.04.2.2, the mutual statistical
independence of a set of random variables means that the probabilistic behavior of each one is not
affected by the probabilistic behavior of the others. Formally, the random variables x1, x2, . . . , x N are
independent if any of the following conditions is fulﬁlled:

1.04.3 Random Variable
135
•
Fx(x) = 	N
n=1Fxn(xn), or
•
fx(x) = 	N
n=1 fxn(xn), or
•
Fxn(xn|any condition on the remaining variables) = Fxn(xn), ∀1 ≤n ≤N, or
•
fxn(xn|any condition on the remaining variables) = fxn(xn), ∀1 ≤n ≤N.
Returning to the example developed in Section 1.04.3.6, x and y are clearly statistically dependent.
However, if they were jointly uniformly distributed in the region deﬁned by 0 ≤x ≤1, 0 ≤y ≤1,
they would be statistically independent—this veriﬁcation is left to the reader.
The PDF of a random variable y = N
n=1 fxn(xn) composed by the sum of N independent variables
xn can be computed as9
fy(y) = ( fx1∗fx2∗· · · ∗fx N )(y).
(4.77)
1.04.3.8 Joint statistical moments
The deﬁnition of statistical mean can be directly extended to a vector random variable x with known
PDF. The expected value of a real scalar function g(x) is given by:
E[g(x)] =
 ∞
−∞
· · ·
 ∞
−∞
 ∞
−∞
g(x) fx(x)dx1 dx2 · · · dxN.
(4.78)
We can analogously proceed to the deﬁnition of N-variable joint statistical moments. However, due
to the more speciﬁc usefulness of the N > 2 cases, only 2-variable moments will be presented.
An (n + k)th-order joint moment of x and y (about the origin) has the general form
mn,k = E[xnyk], n, k ∈Z.
(4.79)
The most important case corresponds to m = n = 1: m11 = Rx y is called the statistical correlation
between x and y. Under a frequentist perspective, Rx y is the average of inﬁnite products of samples
x and y jointly drawn from fx,y(x, y), which links the correlation to an inner product between the
random variables. Indeed, when Rx y = 0, x and y are called orthogonal. One can say the correlation
quantiﬁes the overall linear relationship between the two variables. Moreover, when Rx y = x y, x
and y are called mutually uncorrelated; this “separability” in the mean is weaker than the distribution
separability implied by the independence. In fact, if x and y are independent,
Rx y =
 ∞
−∞
 ∞
−∞
xy fx,y(x, y)dx dy =
 ∞
−∞
 ∞
−∞
xy fx(x) fy(y)dx dy
=
 ∞
−∞
x fx(x)dx
 ∞
−∞
y fy(y)dy = x y,
(4.80)
i.e., they are also uncorrelated. The converse is not necessarily true.
An (n + k)th-order joint central moment of x and y, computed about the mean, has the general
form
μnk = E[((x −x)n(y −y)k)], n, k ∈Z.
(4.81)
9Convolution integral: (a∗b)(y) =
 ∞
−∞a( ˜y)b(y −˜y)d ˜y.

136
CHAPTER 4 Random Signals and Stochastic Processes
Once more, the case m = n = 1 is specially important: μ11 = Cx y is called the statistical covariance
between x and y, which quantiﬁes the linear relationship between their random parts. Since10 Cx y =
Rx y−x y, x and y are uncorrelated when Cx y = 0. If Cx y > 0, one says x and y are positively correlated,
i.e., the variations of their statistical samples tend to occur in the same direction; if Cx y < 0, one says x
and y are negatively correlated, i.e., the variations of their statistical samples tend to occur in opposite
directions. For example, the age and the annual medical expenses of an individual are expected to be
positively correlated random variables. A normalized covariance can be computed as the correlation
between the normalized versions of x and y:
ρx y =
Cx y
σxσy
= E

x −x
σx
·
y −y
σy

,
(4.82)
known as the correlation coefﬁcient between x and y, −1 ≤ρx y ≤1, and can be interpreted as the per-
centage of correlation between x and y. Recalling the inner product interpretation, the correlation coefﬁ-
cient can be seen as the cosine of the angle between the statistical variations of the two random variables.
Consider, for example, the discrete variables d1 and d2 jointly distributed as shown in Table 4.3.
Their joint second-order parameters are:
•
correlation Rx y = −0.80, indicating d1 and d2 are not orthogonal;
•
covariance Cx y = −0.72, indicating d1 and d2 tend to evolve in opposite directions;
•
correlation coefﬁcient ρx y ≈−0.80, indicating this negative correlation is relatively strong.
Returning to N-dimensional random variables, the characteristic function of a vector random vari-
able x is deﬁned as
x(ω1, ω2, . . . , ωN) = E[ej(ω1x1+ω2x2+···ωN x N )], ω1, ω2, . . . , ωN ∈R,
(4.83)
and inter-relates the moments of order mn1,n2,...,nN by:
mn1,n2,...,nN = ( −j)n1+n2+···+nN ∂n1+n2+···+nN x(ω1, ω2, . . . , ωN)
∂ωn1
1 ωn2
2 · · · ∂ωnN
N

ω1=ω2=···=ωN =0
.
(4.84)
The moment generating function of a vector random variable x is deﬁned as
Mx(ν1, ν2, . . . , νN) = E[eν1x1+ν2x2+···+νN x N ], ν1, ν2, . . . , νN ∈R,
(4.85)
10Equation (4.43) can be seen as the next expression computed for x = y, since Rxx = x2 and Cxx = σ 2
x .

1.04.3 Random Variable
137
and allows the computation of mn1,n2,...,nN as:
mn1,n2,...,nN = ∂n1+n2+···+nN Mx(ν1, ν2, . . . , νN)
∂νn1
1 νn2
2 · · · ∂νnN
N

ν1=ν2=···=νN =0
.
(4.86)
1.04.3.9 Central limit theorem
A result that partially justiﬁes the ubiquitous use of Gaussian models, the Central Limit Theorem (CLT)
states that (under mild conditions in practice) the distribution of a sum y = N
n=1 xn of N independent
variables xn approaches a Gaussian distribution as N →∞. Having completely avoided the CLT proof,
we can at least recall that in this case, fy(y) = limN→∞fy(y) = ( fx1 ∗fx2 ∗· · · ∗fx N )(y) (see Eq.
(4.77). Of course, y = N
n=1 xn and, by the independence property, σ 2
y = N
n=1 σ 2
xn.
Gaussian approximations for ﬁnite-N models are useful for sufﬁciently high N, but due to the shape
of the Gaussian distribution, the approximation error grows as y distances from y.
The reader is invited to verify the validity of the approximation provided by the CLT for successive
sums of independent xn uniformly distributed in [0, 1].
Interestingly, the CLT also applies to the sum of discrete distributions: even if fy(y) remains impul-
sive, the shape of Fy(y) approaches the shape of the CDF of a Gaussian random variable as N grows.
1.04.3.10 Multivariate Gaussian distribution
The PDF of an N-dimensional Gaussian variable is deﬁned as
fx(x) =
1

(2π)Ndet(Cx)
e−
(x−x)TC−1
x
(x−x)
2
,
(4.87)
where x is an N × 1 vector with elements {x}n = xn, 1 ≤n ≤N, and Cx is an N × N matrix
with elements {Cx}mn = Cxmxn, 1 ≤m, n ≤N, such that any related marginal distribution remains
Gaussian. As a consequence, by Eq. (4.74), any conditional distribution fx(x|B) with point conditioning
B is also Gaussian.
By this deﬁnition, we can see the Gaussian distribution is completely deﬁned by its ﬁrst- and second-
order moments, which means that if N jointly distributed random variables are known to be Gaussian,
estimating their mean-vector x and their covariance-matrix Cx is equivalent to estimate their overall
PDF. Moreover, if xm and xn are mutually uncorrelated, ∀m ̸= n, then Cx is diagonal, and the joint PDF
becomes fx(x) = 	N
n=1 fxn(xn), i.e., xn will be mutually independent. This is a strong result inherent
to Gaussian distributions.
1.04.3.11 Transformation of vector random variables
The treatment of a general multiple random variable y = T(x) resulting from the application of the
transformation T(·) to the multiple variable x always starts by enforcing Fx(x) = Fy(y), with y = T(x).

138
CHAPTER 4 Random Signals and Stochastic Processes
The special case when T is invertible, i.e., x = T−1(y) (or xn = Tn−1(y), 1 ≤n ≤N), follows a
closed expression:
fy(y) = fx(x = T−1(y))|J|,
(4.88)
where
J = det
⎛
⎜⎜⎜⎜⎜⎜⎝
⎡
⎢⎢⎢⎢⎢⎢⎣
∂T −1
1
∂y1
∂T −1
1
∂y2 · · · ∂T −1
1
∂yN
∂T −1
2
∂y1
∂T −1
2
∂y2 · · · ∂T −1
2
∂yN
...
...
...
...
∂T −1
N
∂y1
∂T −1
N
∂y2 · · · ∂T −1
N
∂yN
⎤
⎥⎥⎥⎥⎥⎥⎦
⎞
⎟⎟⎟⎟⎟⎟⎠
.
(4.89)
The reader should notice that this expression with N = 1 reduces to Eq. (4.51) particularized to the
invertible case.
Given the multiple random variable x with known mean-vector x and covariance-matrix Cx, its linear
transformation y = Tx will have:
•
a mean-vector y = Tx,
•
a covariance-matrix Cy = TCxTT.
It is possible to show that the linear transformation y of an N-dimensional Gaussian random variable
x is also Gaussian. Therefore, in this case the two expressions above completely determine the PDF of
the transformed variable.
The generation of samples that follow a desired multivariate probabilistic distribution from samples
that follow another known multivariate probabilistic distribution applies the same reasoning followed in
the univariate case to the multivariate framework. The reader is invited to show that given the pair x1, x2
of samples from the random variables x1, x2, jointly uniform in the region 0 ≤x ≤1, 0 ≤y ≤1, it is
possible to generate the pair y1, y2 of samples from the random variables y1, y2, jointly Gaussian with
the desired parameters y1, σ 2
y1, y2, σ 2
y2, and ρy1y2 by making
% y1 = y1 + σy1
√−2 ln x1 cos (2πx2),
y2 = y2 + ρy1y2σy2
√−2 ln x1 cos (2πx2) + σy2

1 −ρ2y1y2
√−2 ln x1 sin (2πx2).
(4.90)
1.04.3.12 Complex random variables
Atﬁrstsight,deﬁningacomplexrandomvariable z mayseemimpossible,sincetheseminalevent{z ≤z}
makes no sense. However, in the vector random variable framework this issue can be circumvented if
one jointly describes the real and imaginary parts (or magnitude and phase) of z.
The single complex random variable z = x + jy, x, y ∈R, is completely represented by fx,y(x, y),
which allows one to compute the expected value of a scalar function g(z) as E[g(z)] =
 ∞
−∞
 ∞
−∞g(z)
fx,y(x, y)dx dy. Moreover, we can devise general deﬁnitions for the mean and variance of z as, respec-
tively:
•
z = x + jy;
•
σ 2
z = E[|z −z|2] = σ 2
x + σ 2
y , which measures the spread of z about its mean in the complex plan.

1.04.3 Random Variable
139
An N-dimensional complex random variable z = x + jy can be easily tackled through a properly
deﬁned joint distribution, e.g., fx,y(x, y). The individual variables zn, 1 ≤n ≤N, will be independent
when
fx,y(x, y) =
N
&
n=1
fxn,yn(xn, yn).
(4.91)
The following deﬁnitions must be generalized to cope with complex random variables:
•
correlation: Rz1z2 = E[z∗
1z2];
•
covariance: Cz1z2 = E[(z1 −z1)∗(z2 −z2)].
Now, Cz1z2 = Rz1z2 −z1
∗z2. As before, z1 and z2 will be:
•
orthogonal when Rz1z2 = 0;
•
uncorrelated when Cz1z2 = 0.
1.04.3.13 An application: estimators
One of the most encompassing applications of vector random variables is the so-called parameter
estimation, per se an area supported by its own theory. The typical framework comprises using a ﬁnite
set of measured data from a given phenomenon to estimate the parameters11 of its underlying model.
A set x1, x2, . . . , x N of N independent identically distributed (iid) random variables such that
fx(x) =
N
&
n=1
fx(xn)
(4.92)
can describe an N-size random sample of a population modeled by fx(x). Any function g(x) (called a
statistic of x) can constitute a point estimator ˆθ of some parameter θ such that given an N-size sample
x1, x2, . . . , xN of x, one can compute an estimate ˆθ = g(x) of θ.
A classical example of point estimator for a ﬁxed parameter is the so-called sample mean, which
estimates θ = x by the arithmetic mean of the available samples xn, n = 1, 2, . . . , N:
ˆx N = 1
N
N

n=1
xn.
(4.93)
Not always deﬁning a desired estimator is trivial task as in the previous example. In general, resorting
to a proper analytical criterion is necessary. Suppose we are given the so-called likelihood function of
θ, L(θ) = fx(x|θ), which provides a probabilistic model for x as an explicit function of θ. Given the
sample x, the so-called maximum likelihood (ML) estimator ˆθML computes an estimate of θ by direct
maximization of L(θ). This operation is meant to ﬁnd the value of θ that would make the available
sample x most probable.
11We will tackle only scalar parameters, without loss of generality.

140
CHAPTER 4 Random Signals and Stochastic Processes
Sometimes the parameter θ itself can be a sample of a random variable θ described by an a priori
PDF fθ(θ). For example, suppose one wants to estimate the mean value of a shipment of components
received from a given factory; since the components may come from several production units, we can
think of a statistical model for their nominal values. In such situations, any reliable information on the
parameter distribution can be taken in account to better tune the estimator formulation; these so-called
Bayesian estimators rely on the a posteriori PDF of θ:
fθ(θ|x) = fx,θ(x, θ)
fx(x)
∝L(θ) fθ(θ).
(4.94)
Given a sample x, the so-called maximum a posteriori (MAP) estimator ˆθMAP computes an estimate
of θ as the mode of fθ(θ|x). This choice is meant to ﬁnd the most probable θ that would have produced
the available data x. Several applications favor the posterior mean estimator, which for a given sample
x computes ˆθPM = E[θ|x], over the MAP estimator.
The quality of an estimator θ can be assessed through some of its statistical properties.12 An overall
measure of the estimator performance is its mean square error MSE(ˆθ) = E[(θ −ˆθ)2], which can be
decomposed in two parts:
MSE(ˆθ) = b2(ˆθ) + σ 2
ˆθ ,
(4.95)
where b(ˆθ) = E[θ −ˆθ] is the estimator bias and σ 2
ˆθ is its variance. The bias measures the deterministic
part of the error, i.e., how much the estimates deviate from the target in the mean, thus providing an accu-
racy measure: the smaller its bias, the more accurate the estimator. The variance, by its turn, measures the
random part of the error, i.e., how much the estimates spread about their mean, thus providing a precision
measure: the smaller its variance, the more precise the estimator. Another property attributable to an esti-
mator is consistence13: ˆθ is said to be consistent when limN→∞P(|ˆθ −θ| ≥ϵ) = 0, ∀ϵ > 0. Using the
Chebyshev inequality (see Section 1.04.3.4), one ﬁnds that an unbiased estimator with limN→∞σ 2
ˆθ = 0
is consistent. It can be shown that the sample mean deﬁned in Eq. (4.93) has b(ˆx N) = 0 (thus is unbi-
ased) and σ 2
ˆx N =
σ 2
x
N (thus is consistent). The similarly built estimator for σ 2
x , the unbiased sample
variance, computes
'
σ 2x =
1
N −1
N

n=1

xn −ˆx N
2
.
(4.96)
The reader is invited to prove that this estimator is unbiased, while its more intuitive form with denom-
inator N instead of N −1 is not.
One could argue how much reliable a point estimation is. In fact, if the range of θ is continuous, one
can deduce that P({ˆθ = θ}) = 0. However, perhaps we would feel safer if the output of an estimator
were something like “θ1 ≤θ ≤θ2 with probability p.” θ1 and θ2 are the conﬁdence limits and p is the
conﬁdence of this interval estimate.
12For the sake of simplicity, we refer only to the estimation of a ﬁxed parameter θ.
13There are other deﬁnitions of consistence.

1.04.4 Random Process
141
Example 2.
Suppose we use a 10-size sample mean to estimate x of a unit-variance Gaussian random
variable x. Determine the 95%-conﬁdence interval for the estimates.
Solution 2.
It is easy to ﬁnd that ˆx10 is a Gaussian random variable with E[ˆx10] = x and σ 2
ˆx10 = 0.1.
We should ﬁnd a such that P({ˆx10 −a ≤x ≤ˆx10 + a}) = 0.95, or P({|x −ˆx10| ≤a}) = 0.95. For
the associated normalized Gaussian random variable, P


−1.96 ≤x−ˆx10
1/
√
10 ≤1.96
(
≈0.95. Then,
a ≈0.62.
1.04.4 Random process
If the outcomes s of a given random experiment are amenable to variation in time t, each one can be
mapped by some x(s(t)) into a real function x(t). This is a direct generalization of the random variable.
As a result, we get an inﬁnite ensemble of time-varying sample functions or realizations x(t) which
form the so-called stochastic or random process x(t). For a ﬁxed t0, x(t0) reduces to a single random
variable, thus a random process can be seen as a time-ordered multiple random variable. An example
of random process is the simultaneous observation of air humidity at every point of the globe, h(t):
each realization h(t) describes the humidity variation in time at a randomly chosen place on the earth
whereas the random variable h(t0) describes the distribution of air humidity over the earth at instant t0.
A similar construction in the discrete time n domain would produce x[n] composed of realizations x[n].
If the hourly measures of air humidity substitute for its continuous measure in the former example, we
get a new random process h[n].
As seen, there can be continuous- or discrete-time random processes. Since random variables have
already been classiﬁed as continuous or discrete, one analogously refers to continuous-valued and
discrete-valued random processes. Combining these two classiﬁcations according to time and value
is bound to result ambiguous or awkwardly lengthy; when this complete categorization is necessary,
one favors the nomenclature random process for the continuous-time case and random sequence for
the discrete-time case, using the words “continuous” and “discrete” only with reference to amplitude
values.14 In the former examples, the continuous random process h(t) and random sequence h[n] would
model the idealized actual air humidity as a physical variable, whereas the practical digitized measures
of air humidity would be modeled by their discrete counterparts.
1.04.4.1 Distributions of random processes and sequences
One could think of the complete description of a random process or sequence as a joint CDF (or
PDF) encompassing every possible instant of time t or n, respectively, which is obviously impractical.
However, their partial representation by Lth-order distributions (employing a slightly modiﬁed notation
to include the reference to time) can be useful. The CDF of a random process x(t) can be deﬁned as
Fx(x1, x2, . . . , xL; t1, t2, . . . , tL) = P({x(t1) ≤x1, x(t2) ≤x2, . . . , x(tL) ≤xL}),
(4.97)
14In this text, for completeness we opted for always explicitly writing both formulations, for processes and sequences.

142
CHAPTER 4 Random Signals and Stochastic Processes
with an associated PDF
fx(x1, x2, . . . , xL; t1, t2, . . . , tL) = ∂L Fx(x1, x2, . . . , xL; t1, . . . , tL)
∂x1∂x2 · · · ∂xL
.
(4.98)
The CDF of a random sequence x[n] can be deﬁned as
Fx(x1, x2, . . . , xL; n1, n2, . . . , nL) = P({x[n1] ≤x1, x[n2] ≤x2, . . . , x[nL] ≤xL}),
(4.99)
with an associated PDF
fx(x1, x2, . . . , xL; n1, n2, . . . , nL) = ∂L Fx(x1, x2, . . . , xL; n1, n2, . . . , nL)
∂x1∂x2 · · · ∂xL
.
(4.100)
This convention can be easily extended to the joint distributions of M random processes xm(t),
m = 1, 2, . . . , M (sequences xm[n], m = 1, 2, . . . , M), as will be seen in the next subsection. The
notation for point-conditioned distributions can also be promptly inferred (see Section 1.04.4.12).
1.04.4.2 Statistical independence
As with random variables, the mutual independence of random processes or sequences is tested by the
complete separability of their respective distributions. The random processes xm(t), m = 1, 2, . . . , M,
are independent when
fx1,x2,...,x M (x11, x12, . . . , x1L1, x21, x22, . . . , x2L2, . . . , xM1, xM2, . . . , xML M ;
t11, t12, . . . , t1L1, t21, t22, . . . , t2L2, . . . , tM1, tM2, . . . , tML M )
=
M
&
m=1
fxm(xm1, xm2, . . . , xmLm; tm1, tm2, . . . , tmLm)
(4.101)
for any choice of time instants. Similarly, the random sequences xm[n], m = 1, 2, . . . , M, are inde-
pendent when
fx1,x2,...,x M (x11, x12, . . . , x1L1, x21, x22, . . . , x2L2, . . . , xM1, xM2, . . . , xML M ;
n11, n12, . . . , n1L1, n21, n22, . . . , n2L2, . . . , nM1, nM2, . . . , nML M )
=
M
&
m=1
fxm(xm1, xm2, . . . , xmLm; nm1, nm2, . . . , nmLm)
(4.102)
for any choice of time instants.
1.04.4.3 First- and second-order moments for random processes and sequences
It is not necessary to redeﬁne moments in the context of random processes and sequences, since we will
always be tackling a set of random variables as in Section 1.04.3.8. But it is useful to revisit the ﬁrst-
and second-order cases with a slightly modiﬁed notation to include time information.
For a random process x(t), the following deﬁnitions apply:

1.04.4 Random Process
143
•
the mean x(t) = E[x(t)];
•
the mean square value or mean instantaneous power x2(t) = E[x2(t)];
•
the variance σ 2
x (t) = E[(x(t) −x(t))2];
•
the auto-correlation Rxx(t1, t2) = E[x(t1)x(t2)];
•
the auto-covariance Cxx(t1, t2) = E[(x(t1) −x(t1))(x(t2) −x(t2))].
As seen for random variables,
σ 2
x (t) = x2(t) −x2(t) ⇒x2(t) = x2(t) + σ 2
x (t),
(4.103)
which means that the mean instantaneous power of the process is the sum of a deterministic parcel with
a random parcel.
Analogously, for a random sequence x[n], the following deﬁnitions apply:
•
the mean x[n] = E[x[n]];
•
the mean square value or mean instantaneous power x2[n] = E[x2[n]];
•
the variance σ 2
x [n] = E[(x[n] −x[n])2];
•
the auto-correlation Rxx[n1, n2] = E[x[n1]x[n2]];
•
the auto-covariance Cxx[n1, n2] = E[(x[n1] −x[n1])(x[n2] −x[n2])].
Again,
σ 2
x [n] = x2[n] −x2[n] ⇒x2[n] = x2[n] + σ 2
x [n],
(4.104)
i.e., the mean instantaneous power of the sequence is the sum of a deterministic parcel with a random
parcel.
Given two random processes x(t) and y(t), one deﬁnes:
•
the cross-correlation Rx y(t1, t2) = E[x(t1)y(t2)];
•
the cross-covariance Cx y(t1, t2) = E[(x(t1) −x(t1))(y(t2) −y(t2))].
The processes x(t) and y(t) are said to be mutually:
•
orthogonal when Rx y(t1, t2) = 0, ∀t1 ̸= t2;
•
uncorrelated when Cx y(t1, t2) = 0, which is the same as Rx y(t1, t2) = x(t1)y(t2) ∀t1 ̸= t2.
The mean instantaneous power of (x(t) + y(t)) is given by
E[(x(t) + y(t))2] = x2(t) + 2E[x(t)y(t)] + y2(t),
(4.105)
which suggests the deﬁnition of E[x(t)y(t)] as the mean instantaneous cross power between x(t)
and y(t).
Analogously, given two random sequences x[n] and y[n], one deﬁnes:
•
the cross-correlation Rx y[n1, n2] = E[x[n1]y[n2]];
•
the cross-covariance Cx y[n1, n2] = E[(x[n1] −x[n1])(y[n2] −y[n2])].
The sequences x[n] and y[n] are said to be mutually:
•
orthogonal when Rx y[n1, n2] = 0, ∀n1 ̸= n2;

144
CHAPTER 4 Random Signals and Stochastic Processes
•
uncorrelated when Cx y[n1, n2] = 0, which is the same as Rx y[n1, n2] = x[n1] y[n2] ∀n1 ̸= n2.
The mean instantaneous power of (x[n] + y[n]) is given by
E[(x[n] + y[n])2] = x2[n] + 2E[x[n]y[n]] + y2[n],
(4.106)
which suggests the deﬁnition of E[x[n]y[n]] as the mean instantaneous cross power between x[n]
and y[n].
The interpretation of this “cross power” is not difﬁcult: it accounts for the constructive or destructive
interaction of the two involved processes (sequences) as dictated by their mutual correlation.
We will dedicate some space later to the properties of second-order moments.
1.04.4.4 Stationarity
The stationarity of random processes and sequences refers to the time-invariance of their statistical
properties, which turns their treatment considerably easier.
Strict-sense is the strongest class of stationarity. A strict-sense stationary (SSS) random process x(t)
(sequence x[n]) bears exactly the same statistical properties as x(t +t), ∀t ∈R (x[n+n], ∀n ∈
Z), which means that its associated joint distribution for any set of time instants does not change when
they are all shifted by the same time lag. A random process (sequence) in which each realization is a
constant sampled from some statistical distribution in time is SSS.
Under this perspective, we can deﬁne that a random process x(t) is Lth-order stationary when
Fx(x1, x2, . . . , xL; t1, t2, . . . , tL) = Fx(x1, x2, . . . , xL; t1 + t, t2 + t, . . . , tL + t)
(4.107)
or, alternatively,
fx(x1, x2, . . . , xL; t1, t2, . . . , tL) = fx(x1, x2, . . . , xL; t1 + t, t2 + t, . . . , tL + t).
(4.108)
Analogously, a random sequence x[n] is Lth-order stationary when
Fx(x1, x2, . . . , xL; n1, n2, . . . , nL) = Fx(x1, x2, . . . , xL; n1 + n, n2 + n, . . . , nL + n) (4.109)
or, alternatively,
fx(x1, x2, . . . , xL; n1, n2, . . . , nL) = fx(x1, x2, . . . , xL; n1 + n, n2 + n, . . . , nL + n). (4.110)
We can say that an SSS random process or sequence is stationary for any order L. Moreover, Lth-order
stationarity implies (L −1)th-order stationarity, by marginalization at both sides of any of the four
equations above.
First-order stationarity says the statistical distribution at any individual instant of time t (n) is the
same, i.e., fx(x; t) does not depend on t ( fx(x; n) does not depend on n). A natural consequence is the
time-invariance of its statistical mean: x(t) = x (x[n] = x). Second-order stationarity says the joint
statistical distribution at any two time instants t and t + τ for a ﬁxed τ (n and n + k for a ﬁxed k) is the
same. A natural consequence is the time-invariance of its auto-correlation, which can depend only of
the time lag between the two time instants: Rxx(t, t + τ) = Rxx(τ) (Rxx[n, n + k] = Rxx[k]).

1.04.4 Random Process
145
A weaker class of stationarity called wide-sense is much used in practice for its simple testability.
A random process x(t) (sequence x[n]) is said to be wide-sense stationary (WSS) when its mean and
auto-correlation are time-invariant. Such conditions do not imply stationarity of any order, although
second-order stationarity implies wide-sense stationarity. As an extension of such deﬁnition, one says
that two processes x(t) and y(t) (sequences x[n] and y[n]) are jointly WSS when they are individually
WSS and their cross-correlation is time-invariant, i.e., can depend only of the time lag between the two
time instants: Rx y(t, t + τ) = Rx y(τ) (Rx y[n, n + k] = Rx y[k]).
1.04.4.5 Properties of correlation functions for WSS processes and sequences
Wide-sense stationarity in random processes and sequences induces several interesting properties, some
of which are listed below. For processes,
•
|Rxx(τ)| ≤Rxx(0);
•
Rxx( −τ) = Rxx(τ);
•
If limτ→∞Rxx(τ) = B, B = x2;
•
If x(t) has a periodic component, Rxx(τ) has the same periodic component;
•
Rx y( −τ) = Ryx(τ);
•
Rx y(τ) ≤

Rxx(0)Ryy(0);
•
Rx y(τ) ≤
Rxx(0)+Ryy(0)
2
.
For sequences,
•
|Rxx[k]| ≤Rxx[0];
•
Rxx[−k] = Rxx[k];
•
If limk→∞Rxx[k] = B, B = x2;
•
If x[n] has a periodic component, Rxx[k] has the same periodic component;
•
Rx y[−k] = Ryx[k];
•
Rx y[k] ≤

Rxx[0]Ryy[0];
•
Rx y[k] ≤
Rxx[0]+Ryy[0]
2
.
Even if the properties are not proven here, some interesting observations can be traced about them:
•
The statistical behavior of the process (sequence) at a given instant of time is maximally correlated
with itself, an intuitive result.
•
The autocorrelation commutes.
•
The statistical behaviors of a WSS process (sequence) with no periodic components at two time
instants separated by τ →∞(k →∞) are uncorrelated. Then, the autocorrelation tends to the
product of two identical statistical means.
•
Periodic components in the process (sequence) cause the correlation maximum and surrounding
behavior to repeat at each fundamental period.
•
The arithmetic and geometric mean properties are in a certain sense linked to the ﬁrst property. The
second property is more stringent than the ﬁrst.

146
CHAPTER 4 Random Signals and Stochastic Processes
1.04.4.6 Time averages of random processes and sequences
The proper use of random processes and sequences to model practical problems depends on the careful
understanding of both their statistical and time variations. The main difference between these two con-
texts, which must be kept in mind, is the fact that time is inexorably ordered, thus allowing the inclusion
in the model of some deterministic (predictable) time-structure. When we examine the statistical auto-
correlation between two time instants of a random process or sequence, we learn how the statistical
samples taken at those instants follow each other. In a WSS process or sequence, this measure for
different lags can convey an idea of statistical periodicity.15 But what if one is concerned with the time
structure and characteristics of each (or some) individual realization of a given process or sequence?
The use of time averages can provide this complementary description.
The time average of a given continuous-time function f (t) can be deﬁned as
A[ f (t)] = lim
T →∞
1
2T
 T
−T
f (t)dt.
(4.111)
Given the random process x(t), the time average of any of its realizations x(t) is
χ = A[x(t)].
(4.112)
The average power of x(t) is
Pxx = A[x2(t)].
(4.113)
The time auto-correlation of x(t) for a lag τ is deﬁned as
Rxx(τ) = A[x(t)x(t + τ)].
(4.114)
The time cross-correlation between the realizations x(t) of process x(t) and y(t) of process y(t) for
a lag τ is deﬁned as
Rxy(τ) = A[x(t)y(t + τ)].
(4.115)
As in the statistical case, an average cross power between x(t) and y(t), which accounts for their
mutual constructive or destructive interferences due to their time structure, can be deﬁned as
Pxy = A[x(t)y(t)].
(4.116)
Computed for the complete ensemble, such measures produce the random variables χ, Pxx, Rxx(τ),
Rxy(τ), and Pxy, respectively. Under mild convergence conditions,
E[χ] = E[A[x(t)]] = A[E[x(t)]] = A[x(t)],
(4.117)
E[Pxx] = E[A[x2(t)]] = A[E[x2(t)]] = A[x2(t)],
(4.118)
E[Rxx(τ)] = E[A[x(t)x(t + τ)]] = A[E[x(t)x(t + τ)]] = A[Rxx(t, t + τ)],
(4.119)
E[Rxy(τ)] = E[A[x(t)y(t + τ)]] = A[E[x(t)y(t + τ)]] = A[Rx y(t, t + τ)],
(4.120)
15We hinted this point when discussing the properties of second-order moments, in Section 1.04.4.5.

1.04.4 Random Process
147
and
E[Pxy] = E[A[x(t)y(t)]] = A[E[x(t)y(t)]],
(4.121)
which are respectively the overall mean value, mean power, and auto-correlation (for lag τ) of process
x(t), and cross-correlation (for lag τ) and mean cross power between processes x(t) and y(t).
The time average of a given discrete-time function f [n] can be deﬁned as
A[ f [n]] = lim
N→∞
1
2N + 1
N

n=−N
f [n].
(4.122)
Given the random sequence x[n], the time average of any of its realizations x[n] is
χ = A[x[n]].
(4.123)
The average power of x[n] is
Pxx = A[x2[n]].
(4.124)
The time auto-correlation of x[n] for lag k is deﬁned as
Rxx[k] = A[x[n]x[n + k]].
(4.125)
The time cross-correlation between the realizations x[n] of sequence x[n] and y[n] of sequence y[n]
for lag k is deﬁned as
Rxy = A[x[n]y[n + k]].
(4.126)
As in the statistical case, an average cross power between x[n] and y[n], which accounts for their
mutual constructive or destructive interferences due to their time structure, can be deﬁned as
Pxy = A[x[n]y[n]].
(4.127)
Computed for the complete ensemble, such measures produce the random variables χ, Pxx, Rxx[k],
Rxy[k], and Pxy, respectively. Under mild convergence conditions,
E[χ] = E[A[x[n]]] = A[E[x[n]]] = A[x[n]],
(4.128)
E[Pxx] = E[A[x2[n]]] = A[E[x2[n]]] = A[x2[n]],
(4.129)
E[Rxx[k]] = E[A[x[n]x[n + k]]] = A[E[x[n]x[n + k]]] = A[Rxx[n, n + k]],
(4.130)
E[Rxy[k]] = E[A[x[n]y[n + k]]] = A[E[x[n]y[n + k]]] = A[Rx y[n, n + k]],
(4.131)
and
E[Pxy] = E[A[x[n]y[n]]] = A[E[x[n]y[n]]],
(4.132)
which are respectively the overall mean value, mean power, and auto-correlation (for lag k) of sequence
x[n], and cross-correlation (for lag k) and mean cross power between sequences x[n] and y[n].
As an example, consider that a deterministic signal s(t) ̸= 0 (with A[s(t)] = 0) is additively con-
taminated by random noise n(t) which can be modeled as a realization of the WSS random process n(t)
(with n(t) = 0 and n2(t) = σ 2
n ), thus yielding the random signal x(t) = s(t)+n(t). The corresponding
process x(t) = s(t) + n(t) is obviously not WSS, since x(t) = s(t). Furthermore, (following the
nomenclature we have adopted) we can compute its:

148
CHAPTER 4 Random Signals and Stochastic Processes
•
mean instantaneous power x2(t) = s2(t) + σ 2
n ;
•
mean power Pxx = A[s2(t)] + A[n2(t)];
•
overall mean power E[Pxx] = A[s2(t)] + σ 2
n .
Deﬁning the signal-to-noise ratio (SNR) as the ratio between signal and noise powers, we can conclude
that SNRx = A[s2(t)]
σ 2x
.
Now, suppose that two differently contaminated versions of s(t) are available: x1(t) = s(t) + n1(t)
and x2(t) = s(t) + n2(t), and that the underlying noise processes n1(t) and n2(t) are jointly WSS
and uncorrelated, with n1(t) = n2(t) = 0, n2
1(t) = σ 2
n1, and n2
2(t) = σ 2
n2. If an average signal
xm(t) = x1(t)+x2(t)
2
is computed, we can guarantee SNRxm > SNRx1 and SNRxm > SNRx2 (i.e., noise
is reduced) as long as 1
3 <
σ 2
n1
σ 2n2
< 3.
1.04.4.7 Ergodicity
The so-called ergodicity is a property that allows interchanging statistic and temporal characteristics
of some random processes (sequences)—which are then called ergodic. There are several levels of
ergodicity, some of which are discussed below.
A random process x(t) with constant statistical mean is said to be mean-ergodic when any time
average χ is equal to x with probability 1, which requires σ 2
χ = 0. If x(t) is WSS, a necessary and
sufﬁcient condition for mean-ergodicity is
lim
T →∞
1
T
 2T
0
Cxx(τ)

1 −τ
2T

dτ = 0.
(4.133)
A random process x(t) with time-invariant auto-correlation is said to be auto-correlation-ergodic when
any time auto-correlation Rxx(τ) is equal to Rxx(τ) with probability 1, which requires σ 2
Rxx(τ) = 0.
Two processes x(t) and y(t) with time-invariant cross-correlation are cross-correlation-ergodic when
any time cross-correlation Rxy(τ) is equal to Rx y(τ) with probability 1, which requires σ 2
Rxy(τ) = 0.
Conditions for correlation-ergodicity of random processes involve 4th-order moments.
A random sequence x[n] with constant statistical mean is said to be mean-ergodic when any time
average χ is equal to x with probability 1, which requires σ 2
χ = 0. If x[n] is WSS, a necessary and
sufﬁcient condition for mean-ergodicity is
lim
M→∞
1
2M + 1
M

k=−M
Cxx[k]

1 −
|k|
2M + 1

= 0.
(4.134)
A random sequence x[n] with time-invariant auto-correlation is said to be auto-correlation-ergodic when
any time auto-correlation Rxx[k] is equal to Rxx[k] with probability 1, which requires σ 2
Rxx[k] = 0.
Two sequences x[n] and y[n] with time-invariant cross-correlation are cross-correlation-ergodic when
any time cross-correlation Rxy[k] is equal to Rx y[k] with probability 1, which requires σ 2
Rxy[k] = 0.
Conditions for correlation-ergodicity of random sequences involve 4th-order moments.

1.04.4 Random Process
149
A process (sequence) that is mean- and auto-correlation-ergodic is called wide-sense ergodic. Two
wide-sense ergodic processes (sequences) that are cross-correlation-ergodic are called jointly wide-
sense ergodic.
A process (sequence) is distribution-ergodic when is ergodic for every moment.
The SSS random process (sequence) formed by random constant realizations is not ergodic in any
sense. From Eqs. (4.133) and (4.134), a WSS process x(t) (sequence x[n]), statistically uncorrelated at
any different time instants t1 and t2 (n1 and n2), i.e., with Cxx(t2 −t1) = δ(t2 −t1) (Cxx[n2 −n1] =
δ[n2 −n1]), is mean-ergodic.
In practical real-life situations, quite often just a single realization of an underlying random process
(sequence) is available. In such cases, if the latter is known to be ergodic, we can make use of the
complete powerful statistical modeling framework described in this chapter. But how can one guarantee
the property is enjoyed by a process (sequence) of which an only sample function is known? This is
not so stringent a requirement: in fact, one needs just to be sure that there can be an ergodic process
(sequence) of which that time function is a realization. Strictly speaking, a given music recording s(t)
additively contaminated by background noise d(t) cannot be modeled as a realization of a mean-ergodic
process,16 since each member of the ensemble would combine the same s(t) with a different d(t). The
random process which describes the noisy signal can be written as x(t) = s(t)+d(t); thus, x(t) = s(t),
while in practice we know that χ = 0.
1.04.4.8 An encompassing example
Deﬁne the random sequences x1[n] = a1 cos

0n + φ1

and x2[n] = a2 sin

0n + φ2

, such that:
•
a1 is a random variable with mean a1 and variance σ 2
a1;
•
a2 is a random variable with mean a2 and variance σ 2
a2;
•
( −π ≤0 < π) rad/sample is a real constant;
•
φ1 is a random variable uniformly distributed between −π and π rad;
•
φ2 is a random variable uniformly distributed between −π and π rad;
•
a1, a2, φ1, and φ2 are mutually independent.
Compute their time and statistical means, mean powers, auto-correlations, cross-correlations, and
mean cross-powers; and discuss their correlation, orthogonality, stationarity, and ergodicity.
Solution 3.
Time-averages:
χ1 = A[x1[n]] = a1A[cos (0n + φ1)] = 0.
(4.135)
χ2 = A[x1[n]] = a1A[sin (0n + φ2)] = 0.
(4.136)
Rx1x1[k] = A[x1[n]x1[n + k]] = a2
1A[cos (0n + φ1) cos (0n + 0k + φ1)]
= a2
1A
)cos (20n + 0k + 2φ1) + cos (0k)
2
*
= a2
1
2 cos (0k);
(4.137)
16Even if the author has done so many times.

150
CHAPTER 4 Random Signals and Stochastic Processes
then,
Px1x1 = Rx1x1[0] = a2
1
2 .
(4.138)
Rx2x2[k] = A[x2[n]x2[n + k]] = a2
2A[sin (0n + φ2) sin (0n + 0k + φ2)]
= a2
2A
)cos (0k) −cos (20n + 0k + 2φ2)
2
*
= a2
2
2 cos (0k);
(4.139)
then,
Px2x2 = Rx2x2[0] = a2
2
2 .
(4.140)
Rx1x2[k] = A[x1[n]x2[n + k]] = a1a2A[cos (0n + φ1) sin (0n + 0k + φ2)]
= a1a2A
)sin (20n + 0k + φ1 + φ2) −sin (φ1 −φ2 −0k)
2
*
= a1a2
2
sin (0k + φ2 −φ1);
(4.141)
then,
Px1x2 = Rx1x2[0] = a1a2
2
sin (φ2 −φ1).
(4.142)
Expected values:
x1 = E
+
x1[n]
,
= E
+
a1
,
E
-
cos

0n + φ1
.
= a1
 π
−π
1
2π cos (0n + φ1)dφ1 = a1.0 = 0.
(4.143)
x2 = E
+
x2[n]
,
= E
+
a2
,
E
-
sin

0n + φ2
.
= a2
 π
−π
1
2π sin (0n + φ2)dφ2 = a2.0 = 0.
(4.144)
Rx1x1[n, n + k] = E
+
x1[n]x1[n + k]
,
= E
-
a2
1
.
E
-
cos

0n + φ1

cos

0n + 0k + φ1
.
=
a1
2 + σ 2
a1
2
E
⎡
⎣
cos

20n + 0k + 2φ1

+ cos (0k)
2
⎤
⎦
=
a1
2 + σ 2
a1
2
cos (0k);
(4.145)
then,
x2
1 = Rx1x1[n, n] =
a1
2 + σ 2
a1
2
.
(4.146)

1.04.4 Random Process
151
Rx2x2[n, n + k] = E
+
x2[n]x2[n + k]
,
= E
-
a2
2
.
E
-
sin

0n + φ2

sin

0n + 0k + φ2
.
=
a2
2 + σ 2
a2
2
E
⎡
⎣
cos (0k) −cos

20n + 0k + 2φ2

2
⎤
⎦
=
a2
2 + σ 2
a2
2
cos (0k);
(4.147)
then,
x2
2 = Rx2x2[n, n] =
a2
2 + σ 2
a2
2
.
(4.148)
Rx1x2[n, n + k] = E
+
x1[n]x2[n + k]
,
= E
+
a1
,
E
+
a2
,
E
-
cos

0n + φ1
.
E
-
sin

0n + 0k + φ2
.
= a1a2.0.0 = 0.
(4.149)
Conclusions:
•
Since x1 and Rx1x1[n, n + k] do not depend on n, x1[n] is WSS.
•
Since x2 and Rx2x2[n, n + k] do not depend on n, x2[n] is WSS.
•
Since x1[n] is WSS, x2[n] is WSS, and Rx1x2[n, n + k] does not depend on n, x1[n] and x2[n] are
jointly WSS.
•
Since Rx1x2[n, n + k] = x1x2, x1[n] and x2[n] are uncorrelated.
•
Since Rx1x2[n, n + k] = 0, x1[n] and x2[n] are orthogonal.
•
Since x1 = χ1, x1[n] is mean-ergodic.
•
Since x2 = χ2, x2[n] is mean-ergodic.
1.04.4.9 Gaussian processes and sequences
A very special case of random processes (sequences) are the Gaussian-distributed. The corresponding
Lth-order PDFs can be written (see Section 1.04.3.10).
•
for processes as
fx(x1, x2, . . . , xL; t1, t2, . . . , tL) =
1

(2π)Ldet(Cx)
e−
(x−x)TC−1
x
(x−x)
2
,
(4.150)
where x is the L × 1 mean-vector with elements {x}l = x(tl), for 1 ≤l ≤L, and Cx is the L × L
covariance-matrix with elements {Cx}l1l2 = Cxx(tl1, tl2), for 1 ≤l1,l2 ≤L;
•
for sequences as
fx(x1, x2, . . . , xL; n1, n2, . . . , nL) =
1

(2π)Ldet(Cx)
e−
(x−x)TC−1
x
(x−x)
2
,
(4.151)

152
CHAPTER 4 Random Signals and Stochastic Processes
where x is the L × 1 mean-vector with elements {x}l = x[nl], for 1 ≤l ≤L, and Cx is the L × L
covariance-matrix with elements {Cx}l1l2 = Cxx[nl1, nl2], for 1 ≤l1,l2 ≤L.
From the deﬁnition above:
•
A WSS Gaussian random process (sequence) is SSS: Since the PDF of a Gaussian process (sequence)
is entirely described by ﬁrst- and second-order moments, if these moments do not depend on t (n),
the PDF itself does not depend on t (n).
•
Uncorrelated Gaussian processes (sequences) are independent: A joint PDF of two uncorrelated
Gaussian processes (sequences) can be easily factorized as the product of their individual PDFs,
since the covariance-matrix becomes block diagonal.
These two strong properties turn Gaussian models mathematically simpler to tackle, and the strict-sense
stationarity and independence conditions easier to meet in practice.
The reader is invited to show that a stationary Gaussian process (sequence) whose auto-correlation
is absolutely integrable in τ (summable in k) is ergodic.
1.04.4.10 Poisson random process
Poisson is an example of discrete random process that we have already deﬁned in Section 1.04.3.2. Each
realization x(t) of x(t) counts the occurrences along time of a certain event whose mean occurrence
rate is λ per time unit, provided that.
•
there are no simultaneous occurrences;
•
occurrence times are independent.
It can model the entry of clients in a store, for example. By convention:
•
x(0) = 0, i.e., the count starts in t = 0;
•
x(t0 > 0) provides the count between t = 0 and t = t0;
•
−x(t0 > 0) provides the count between t = t0 and t = 0.
Time origin can be shifted if necessary.
From Eq. (4.32), the ﬁrst-order PDF of a Poisson process is given by
fx(x; t) =
∞

k=0
(λt)ke−λt
k!
δ(x −k).
(4.152)
Applying the deﬁnition iteratively, one can show that for t1 ≤t2 ≤· · · ≤tL, the corresponding
Lth-order PDF is
fx(x1, x2, . . . , xL; t1, t2, . . . , tL) =
⎧
⎪⎨
⎪⎩
∞
k1=0
∞
k2=k1 · · · ∞
kL=kL−1
(λt1)k1
k1!
	L
l=2
[λ(tl−tl−1)]kl −kl−1
(kl−kl−1)!
e−λtLδ(x1 −k1)δ(x2 −k2) · · · δ(xL −kL),
k1 ≤k2 ≤· · · ≤kL;
0,
otherwise.
(4.153)

1.04.4 Random Process
153
1.04.4.11 Complex random processes and sequences
Analogously to what has been done for random variables, real random processes and sequences can be
generalized to handle the complex case.
A complex random process can be described as z(t) = x(t) + jy(t), where x(t) and y(t) are real
random processes. It is stationary in some sense as long as x(t) e y(t) are jointly stationary in that sense.
The remaining deﬁnitions related to process stationarity are kept the same.
For a complex random process z(t), the following deﬁnitions apply:
•
the mean z(t) = x(t) + y(t);
•
the mean instantaneous power |z|2(t) = |x|2(t) + |y|2(t);
•
the variance σ 2
z (t) = E[(|z(t) −z(t)|2] = σ 2
x (t) + σ 2
y (t);
•
the auto-correlation Rzz(t1, t2) = E[z∗(t1)z(t2)];
•
the auto-covariance Czz(t1, t2) = E[(z(t1) −z(t1))∗(z(t2) −z(t2))].
Given two random processes z1(t) and z2(t), one deﬁnes:
•
the mean instantaneous cross power E[z∗
1(t)z2(t)];
•
the cross-correlation Rz1z2(t1, t2) = E[z∗
1(t1)z2(t2)];
•
the cross-covariance Cz1z2(t1, t2) = E[(z1(t1) −z1(t1))∗(z2(t2) −z2(t2))].
The processes z1(t) and z2(t) are said to be mutually:
•
orthogonal when Rz1z2(t1, t2) = 0, ∀t1 ̸= t2;
•
uncorrelated when Cz1z2(t1, t2) = 0, which is the same as Rz1z2(t1, t2) = z1
∗(t1)z2(t2) ∀t1 ̸= t2.
For a random sequence z[n], the following deﬁnitions apply:
•
the mean z[n] = x[n] + y[n];
•
the mean instantaneous power |z|2[n] = |x|2[n] + |y|2[n];
•
the variance σ 2
z [n] = E[(|z[n] −z[n]|2] = σ 2
x [n] + σ 2
y [n];
•
the auto-correlation Rzz[n1, n2] = E[z∗[n1]z[n2]];
•
the auto-covariance Czz[n1, n2] = E[(z[n1] −z[n1])∗(z[n2] −z[n2])].
Given two random processes z1[n] and z2[n], one deﬁnes:
•
the mean instantaneous cross power E[z∗
1[n]z2[n]]
•
the cross-correlation Rz1z2[n1, n2] = E[z∗
1[n1]z2[n2]];
•
the cross-covariance Cz1z2[n1, n2] = E[(z1[n1] −z1[n1])∗(z2[n2] −z2[n2])].
The processes z1[n] and z2[n] are said to be mutually:
•
orthogonal when Rz1z2[n1, n2] = 0, ∀n1 ̸= n2;
•
uncorrelated when Cz1z2[n1, n2] = 0, which is the same as Rz1z2[n1, n2] = z1
∗[n1]z2[n2]
∀n1 ̸= n2.

154
CHAPTER 4 Random Signals and Stochastic Processes
1.04.4.12 Markov chains
The simplest random processes (sequences) are those whose statistics at a given time are independent
from every other time: a ﬁrst-order distribution sufﬁces to describe them. A less trivial model is found
when the statistical time interdependency assumes a special recursive behavior such that the knowledge
of a past conditioning state summarizes all previous history of the process (sequence). For the so-called
Markov random process (sequence), the knowledge of the past does not affect the expectation of the
future when the present is known.
Mathematically, a random process x(t) is said to be a Markov process when for tn−1 < tn,
fx(xtn; tn|xt; all t ≤tn−1) = fx(xtn; tn|xtn−1; tn−1),
(4.154)
or for t1 < t2 < · · · < tn−1 < tn,
fx(xtn; tn|xtn−1, . . . , xt2, xt1; tn−1, . . . , t2, t1) = fx(xtn; tn|xtn−1; tn−1).
(4.155)
We will restrict ourselves to the discrete-time case. A random sequence x[n] is said to be a Markov
sequence when
fx(xn; n|xn−1, . . . , x1, x0; n −1, . . . , 1, 0) = fX(xn; n|xn−1; n −1),
(4.156)
which can be seen as a transition PDF. From the deﬁnition, one arrives at the chain rule
fx(x0, x1, . . . , xn; 0, 1, . . . , n) = fx(xn; n|xn−1; n −1) · · · fx(x1; 1|x0; 0) fx(x0; 0),
(4.157)
which means that the overall Markov sequence can be statistically described for n ≥0 from the
knowledge of its distribution at n = 0 and its subsequent transition distributions. Some interesting
properties can be deduced from the expressions above:
•
Since fx(xn; n|xn+1, xn+2 . . . , xn+k; n + 1, n + 2, . . . , n + k) = fx(xn; n|xn+1; n + 1), a time-
reversed Markov sequence is also a Markov sequence.
•
For n1 < n2 < n3, fx(xn1, xn3; n1, n3|xn2; n2) = fx(xn3; n3|xn2; n2) fx(xn1; n1|xn2; n2).
A much important property of Markov sequences is the so-called Chapman-Kolmogorov equation:
for n1 < n2 < n3,
fx(xn3; n3|xn1; n1) =
 ∞
−∞
fx(xn3; n3|xn2; n2) fx(xn2; n2|xn1; n1)dxn2,
(4.158)
which provides a recursive way to compute arbitrary transition PDFs.
We can say a Markov sequence x[n] is stationary when fx(xn; n) and fx(xn; n|xn−1; n −1) are
shift-invariant. In this case, the overall sequence can be obtained from fx(x1, x2; 1, 2). A less trivial
(and quite useful) model is provided by homogeneous Markov sequences, which are characterized only
by a shift-invariant transition distribution; they are not stationary in general, but can be asymptotically
stationary (i.e., for n →∞) under certain conditions.
Discrete Markov processes and sequences are called Markov chains, which can assume a countable
number of random states ai described by their state probabilities (P(x[n] = ai) = pi[n] for sequences)
and transition probabilities (P(x[n2] = a j|x[n1] = ai) = i j[n1, n2], n1 < n2 for sequences).
Discrete-time Markov chains enjoy the following properties, for n1 < n2 < n3:

1.04.4 Random Process
155
•

j i j[n1, n2] = 1, which totalizes all possible ways to left state ai in n1;
•

i pi[n1]i j[n1, n2] = p j[n2], which totalizes all possible ways to arrive at state a j in n2;
•
il[n1, n3] = 
j i j[n1, n2] jl[n2, n3] (Chapman-Kolmogorov equation).
If the chain has a ﬁnite number of states, a matrix notation can be employed. The state probability vector
p[n], with elements {pi[n]} = pi[n], and the transition probability matrix [n1, n2], with elements
{i j[n1, n2]} = i j[n1, n2], are related by p[n2] = T [n1, n2]p[n1].
The special class of homogeneous Markov chains enjoys the following additional properties:
•
i j[n1, n2] = i j[k], k = n2 −n1;
•
il[k2 + k1] = 
j i j[k1] jl[k2].
Accordingly, the transition probability matrix becomes [k]. Deﬁning [1] = ,
p[n] = T p[n −1] = (T )np[0],
(4.159)
i.e., [n] = n. When asymptotic stationarity is reachable, one can ﬁnd the steady-state probability
vector p = p[∞] such that p = p.
Consider, for example, the homogeneous Markov chain x[n] depicted in Figure 4.12, with states 1
and 2, state probabilities P(x[n] = 1) = p1[n] and P(x[n] = 2) = p2[n], and transition probabilities
P(x[n] = 2 | x[n −1] = 1) = 12[1] = a and P(x[n] = 1 | x[n −1] = 2) = 21[1] = b. Its
corresponding one-sample transition matrix is
[1] =  =
) 1 −a
a
b
1 −b
*
,
(4.160)
such that
[k] = k =
 b+a(1−a−b)k
a+b
a−a(1−a−b)k
a+b
b−b(1−a−b)k
a+b
a+b(1−a−b)k
a+b

.
(4.161)
It can also be shown that the chain reaches the steady-state probability vector
lim
n→∞p[n] = lim
n→∞
) p1[n]
p2[n]
*
=
)
b
a+b
a
a+b
*
.
(4.162)
1.04.4.13 Spectral description of random processes and sequences
At ﬁrst sight, the direct conversion of each realization x(t) (x[n]) to the frequency domain seems the
easiest way to spectrally characterize a random process x(t) (sequence x[n]). However, it is difﬁcult to
a
b
1
2
1 −a
1 −b
FIGURE 4.12
Homogeneous Markov chain with two states.

156
CHAPTER 4 Random Signals and Stochastic Processes
guarantee the existence of the Fourier transform
X(jω) = F[x(t)] ≜
 ∞
−∞
x(t)e−jωtdt
(4.163)
(in the case os random processes) or
X(ej) = F[x[n]] ≜
∞

n=−∞
x[n]e−jn
(4.164)
(in the case of random sequences) for every realization. And even if possible, we would get a random
spectrum of limited applicability. In Section 1.04.4.5, we have found that the auto-correlation conveys
information about every sinusoidal component found in the process (sequence). A correct interpretation
of the Fourier transform17 suggests that the auto-correlation in fact conveys information about the overall
spectrum of the process (sequence). Furthermore, it is a better behaved function than the individual
realizations, and thus amenable to be Fourier transformed.
In Section 1.04.4.6, Eq. (4.118), we deﬁned the overall mean power of the random process x(t),
which for the general complex case, becomes
Pxx = A[E[|x2(t)|]].
(4.165)
It can be shown that
Pxx = 1
2π
 ∞
−∞
lim
T →∞
E[|X T (jω)|2]
2T
dω,
(4.166)
where
X T (jω) = F[xT (t)]
(4.167)
and
xT (t) =
 x(t), −T < t < T ;
0,
otherwise.
(4.168)
We can then deﬁne a power spectral density
Sxx(jω) = lim
T →∞
E[|X T (jω)|2]
2T
,
(4.169)
such that Pxx =
1
2π
 ∞
−∞Sxx(jω)dω. Some additional algebraic manipulation yields
Sxx(jω) = F[A[Rxx(t, t + τ)]],
(4.170)
which directly relates the auto-correlation to the power spectral density of the process, as predicted.
From the expressions above, Sxx(jω) is a non-negative real function of ω. Furthermore, if x(t) is real,
then Sxx(jω) is an even function of ω.
17The Fourier transform represents a time signal as a linear combination of continuously distributed sinusoids.

1.04.4 Random Process
157
In Section 1.04.4.6, Eq. (4.121), we also deﬁned the overall mean cross-power between the processes
x(t) and y(t), which for the general complex case, becomes
Pxy = A[E[x∗(t)y(t)]].
(4.171)
Following the same steps as above, we arrive at
Pxy = 1
2π
 ∞
−∞
lim
T →∞
E[X∗
T (jω)Y T (jω)]
2T
dω,
(4.172)
where X T (jω) is deﬁned as before,
Y T (jω) = F[yT (t)]
(4.173)
and
yT (t) =
 y(t), −T < t < T ;
0,
otherwise.
(4.174)
We can then deﬁne the corresponding cross-power spectral density
Sx y(jω) = lim
T →∞
E[X∗
T (jω)Y T (jω)]
2T
,
(4.175)
such that Pxy =
1
2π
 ∞
−∞Sx y(jω)dω. In terms of the cross-correlation, the cross-power spectral density
can be written as
Sx y(jω) = F[A[Rx y(t, t + τ)]].
(4.176)
As expected, the cross-power density of orthogonal processes is zero. From the expressions above,
Sx y(jω) = S∗
yx(jω). Furthermore, if x(t) and y(t) are real, then the real part of Sx y(jω) is even and the
imaginary part of Sx y(jω) is odd.
It should be noticed that in the WSS case, Pxx = E[|x2(t)|], Pxy = E[x∗(t)y(t)], Sxx(jω) =
F[Rxx(τ)], and Sx y(jω) = F[Rx y(τ)].
A similar development can be done for random sequences. In Section 1.04.4.6, Eq. (4.129), we
deﬁned the overall mean power of the random sequence x[n], which for the general complex case,
becomes
Pxx = A[E[|x2[n]|]].
(4.177)
It can be shown that
Pxx = 1
2π
 π
−π
lim
N→∞
E[|X N(ej)|2]
2N + 1
d,
(4.178)
where
X N(ej) = F[x N[n]]
(4.179)
and
xN[n] =
 x[n], −N ≤n ≤N;
0,
otherwise.
(4.180)

158
CHAPTER 4 Random Signals and Stochastic Processes
We can then deﬁne a power spectral density
Sxx(ej) = lim
N→∞
E[|X N(ej)|2]
2N + 1
,
(4.181)
such that Pxx =
1
2π
 π
−π Sxx(ej)d. Some additional algebraic manipulation yields
Sxx(ej) = F[A[Rxx[n, n + k]]],
(4.182)
which directly relates the auto-correlation to the power spectral density of the random sequence, as
expected. From the expressions above, Sxx(ej) is a non-negative real function of . Furthermore, if
x[n] is real, then Sxx(ej) is an even function of .
In Section 1.04.4.6, Eq. (4.132), we also deﬁned the overall mean cross-power between the random
sequences x[n] and y[n], which for the general complex case, becomes
Pxy = A[E[x∗[n]y[n]]].
(4.183)
Following the same steps as above, we arrive at
Pxy = 1
2π
 π
−π
lim
N→∞
E[X∗
N(ej)Y N(ej)]
2N + 1
d,
(4.184)
where X N(ej) is deﬁned as before,
Y N(ej) = F[yN[n]]
(4.185)
and
yN[n] =
 y[n], −N ≤n ≤N;
0,
otherwise.
(4.186)
We can then deﬁne the corresponding cross-power spectral density
Sx y(ej) = lim
N→∞
E[X∗
N(ej)Y N(ej)]
2N + 1
,
(4.187)
such that Pxy =
1
2π
 π
−π Sx y(ej)d. In terms of the cross-correlation, the cross-power spectral density
can be written as
Sx y(ej) = F[A[Rx y[n, n + k]]].
(4.188)
As expected, the cross-power density of orthogonal processes is zero. From the expressions above,
Sx y(jω) = S∗
yx(ej). Furthermore, if x[n] and y[n] are real, then the real part of Sx y(ej) is even and
the imaginary part of Sx y(ej) is odd.
It should be noticed that in the WSS case, Pxx = E[|x2[n]|], Pxy = E[x∗[n]y[n]], Sxx(ej) =
F[Rxx[k]], and Sx y(ej) = F[Rx y[k]].

1.04.4 Random Process
159
1.04.4.14 White and colored noise
One usually refers to a disturbing signal d(t) (d[n]) as noise. Since noise signals are typically unpre-
dictable, they are preferably modeled as realizations of some noise random process d(t) (sequence d[n]).
A very special case is the so-called white noise (by analogy with white light), which is a uniform
combination of all frequencies.
Continuous-time white noise w(t) is sampled from a random process w(t) characterized by the
following properties:
•
zero mean: w(t) = 0;
•
non-correlation between distinct time instants: Rww(t, t + τ) = 0, τ ̸= 0;
•
constant power spectral density: Sww(jω) = Sww ∀ω;
•
inﬁnite overall mean power: Pxx = ∞.
From the last property, continuous-time white noise is not physically realizable. Furthermore, WSS
continuous-time white noise has Rww(τ) = Swwδ(τ).
Discrete-time white noise w[n] is sampled from a random sequence w[n] characterized by the
following properties:
•
zero mean: w[n] = 0;
•
non-correlation between distinct time instants: Rww[n, n + k] = 0, k ̸= 0;
•
constant power spectral density: Sww(ej) = A[σ 2
w[n]];
•
overall mean power Pxx = A[σ 2
w[n]].
For WSS discrete-time white noise, Sww(ej) = σ 2
w, Rww[k] = σ 2
wδ[k], and Pxx = σ 2
w.
Unless differently stated, white noise is implicitly assumed to be generated by a WSS random process
(sequence). Notice, however, that the sequence d[n] = w(n) cos (0n), where 0 < 0 < π rad and
w(n) is WSS white noise with unit variance, for example, satisﬁes all conditions to be called white
noise, even if Rdd[n, n + k] = δ[k] cos (0n).
A common (more stringent) model of white noise imposes that values at different time instants of
the underlying process (sequence) be statistically i.i.d.
Any random noise whose associated power spectral density is not constant is said to be colored noise.
One can ﬁnd in the literature several identiﬁed colored noises (pink, grey, etc.), each one with a pre-
speciﬁed spectral behavior. It should also be noticed that any band-limited approximation of white noise
destroys its non-correlation property. Consider d(t) generated by a WSS random process d(t) such that
Sdd(jω) =
% Pπ
W , −W < ω < W;
0,
otherwise.
(4.189)
It is common practice to call d(t) “white noise” of bandwidth W and overall mean power P. Since its
auto-correlation is
Rdd(τ) = P sin (Wτ)
Wτ
,
(4.190)
strictly speaking one cannot say d(t) is white noise.

160
CHAPTER 4 Random Signals and Stochastic Processes
1.04.4.15 Applications: modulation, “Bandpass” and band-limited processes,
and sampling
An interesting application of the spectral description of random processes is modeling of AM (amplitude
modulation). Assuming a carrier signal c(t) = A0 cos (ω0t) modulated by a real random signal m(t)
sampled from a process m(t), the resulting modulated random process x(t) = m(t)A0 cos (ω0t) has
auto-correlation
Rxx(t, t + τ) = A2
0
2 Rmm(t, t + τ)[cos (ω0τ) + cos (2ω0t + ω0τ)].
(4.191)
If m(t) is WSS, then
A[Rxx(t, t + τ)] = A2
0
2 Rmm(τ) cos (ω0τ),
(4.192)
and the corresponding power spectral density is
Sxx(jω) = A2
0
4 [Smm(j(ω −ω0)) + Smm(j(ω + ω0))].
(4.193)
We conclude that the AM constitution of x(t) carries through its auto-correlation, which provides a
simple statistical model for AM.
Quite often we ﬁnd ourselves dealing with band-limited random signals. In the AM discussion
above, typically m(t) has bandwidth W ≪ω0. For a random process x(t) whose spectrum is at least
concentrated around ω = 0 (baseband process), we can attribute it an RMS (root-mean-squared)
bandwidth WRMS such that
W 2
RMS =
 ∞
−∞ω2Sxx(jω)dω
 ∞
−∞Sxx(jω)dω .
(4.194)
If the spectrum is concentrated around a centroid
ω0 =
 ∞
0
ωSxx(jω)dω
 ∞
0
Sxx(jω)dω ,
(4.195)
WRMS is given by

WRMS
2
2
=
 ∞
0
(ω −ω0)2Sxx(jω)dω
 ∞
0
Sxx(jω)dω
.
(4.196)
If the process bandwidth excludes ω = 0, it is called18 a “bandpass” process.
It can be shown that if a band-limited baseband WSS random process x(t) with bandwidth W, i.e.,
such that Sxx(jω) = 0 for |ω| > W, is sampled at rate ωs > 2W to generate the random sequence
x[n] = x

n 2π
ωs

,
(4.197)
then x(t) can be recovered from x[n] with zero mean-squared-error. This is the stochastic version of
the Nyquist criterion for lossless sampling.
18With considerable freedom of nomenclature.

1.04.4 Random Process
161
1.04.4.16 Processing of random processes and sequences
Statistical signal modeling is often employed in the context of signal processing, and thus the interaction
between random signals and processing systems calls for a systematic approach. In this chapter, we will
restrict ourselves to linear time-invariant (LTI) systems.
A system deﬁned by the operation y = L[x] between input x and output y is called linear if any linear
combination of m inputs produce as output the same linear combination of their m corresponding outputs:
•
for a continuous-time system, L
-M
m=1 αmxm(t)
.
= M
m=1 αmL[xm(t)], αm ∈C,
•
for a discrete-time system, L
-M
m=1 αmxm[n]
.
= M
m=1 αm L[xm[n]], αm ∈C.
For a linear system, one can deﬁne an impulse response h as the system output to a unit impulse δ
applied to the system input at a given time instant:
•
for a continuous-time system, a unit impulse δ(·) applied at instant τ, produces an impulse response
hτ(t) = L[δ(t −τ)],
•
for a discrete-time system, a unit impulse19 δ[·] applied at instant k, produces an impulse response
hk[n] = L[δ[n −k]].
A system is called time-invariant if a given input x applied at different time instants produces the same
output y accordingly time-shifted:
•
for a continuous-time system, if L[x(t)] = y(t), then L[x(t −τ)] = y(t −τ)] ∀τ; thus, hτ(t) =
h0(t −τ) ≜h(t −τ),
•
for a discrete-time system, if L[x[n]] = y[n], then L[x[n −k]] = y[n −k]] ∀k; thus, hτ[n] =
h0[n −k] ≜h[n −k].
The output of an LTI system to any input x is the convolution x ∗h between the input and the impulse
response h of the system:
•
for a continuous-time system, L[x(t)] =
 ∞
−∞x(τ)h(t −τ)dτ ≜(x ∗h)(t),
•
for a discrete-time system, L[x[n]] = ∞
k=−∞x[k]h[n −k] ≜(x ∗h)[n].
In the frequency domain, the output Y is related to the input X by the system frequency response H:
•
for a continuous-time system, Y(jω) = X(jω)H(jω), H(jω) = F[h(t)],
•
for a discrete-time system, Y(ej) = X(ej)H(ej), H(ej) = F[h[n]].
The product XH brings the concept of ﬁltering: H deﬁnes how much of each frequency component of
input X passes to the output Y. From now on, when referring to the processing of a random process
(sequence) by a system we imply that each process realization is ﬁltered by that system.
The ﬁltering of a random process x(t) by an LTI system with impulse response h(t) results in another
random process y(t) such that y(t) = (x ∗h)(t). Assuming x(t) WSS and possibly complex, so as h(t),
then y(t) will be also WSS and20:
•
y = x H(j0);
19Unit impulse function, or Kronecker delta: For x ∈Z, δ[x] = 0 ∀x ̸= 0, and δ[0] = 1.
20Where notation f−(t) ≜f ( −t) was used.

162
CHAPTER 4 Random Signals and Stochastic Processes
•
Rx y(τ) = R∗
yx( −τ) = (Rxx ∗h)(τ);
•
Ryy(τ) = (Rx y ∗h∗
−)(τ) = (Rxx ∗h ∗h∗
−)(τ);
•
Sx y(jω) = S∗
yx(jω) = Sxx(jω)H(jω);
•
Syy(jω) = Sx y(jω)H∗(jω) = Sxx(jω)|H(jω)|2.
The processing of a random sequence x[n] by an LTI system with impulse response h[n] results in
another random sequence y[n] such that y[n] = (x ∗h)[n]. Assuming x[n] WSS and possibly complex,
so as h[n], then y[n] will be also WSS and21:
•
y = x H(ej0);
•
Rx y[k] = R∗
yx[−k] = (Rxx ∗h)[k];
•
Ryy[k] = (Rx y ∗h∗
−)[k] = (Rxx ∗h ∗h∗
−)[k];
•
Sx y(ej) = S∗
yx(ej) = Sxx(ej)H(ej);
•
Syy(ej) = Sx y(ej)H∗(ej) = Sxx(ej)|H(ej)|2.
Among the above results, one of the most important is the effect of ﬁltering on the power spectral
density Sxx of the input WSS random process (sequence): it is multiplied by the squared magnitude
|H|2 of the frequency response of the LTI system to produce the power spectral density Syy of the output
WSS random process (sequence). As a direct consequence, any WSS random process (sequence) with
known power spectral density S can be modeled by the output of an LTI system with squared magnitude
response |H|2 ∝S whose input is white noise.
In order to qualitatively illustrate this ﬁltering effect:
•
Figure 4.13 shows 200 samples of white noise w[n];
0
200
n
w
FIGURE 4.13
White noise.
21Where notation f−[n] ≜f [−n] was used.

1.04.4 Random Process
163
0
200
n
l
FIGURE 4.14
Realization of a “low-pass” sequence.
0
200
n
b
FIGURE 4.15
Realization of a “band-pass” sequence.
•
Figure 4.14 shows 200 samples of l[n], a slowly evolving signal obtained by low-pass ﬁltering of
w[n] to 20% of its original bandwidth;
•
Figure 4.15 shows 200 samples of b[n], an almost sinusoidal signal obtained by band-pass ﬁltering
of w[n] to the central 4% of its original bandwidth.
From the area of optimum ﬁltering, whose overall goal is ﬁnding the best ﬁlter to perform a deﬁned
task, we can bring an important application of the concepts presented in this section. The general discrete
Wiener ﬁlter H, illustrated in Figure 4.16, is the linear time-invariant ﬁlter which minimizes the mean
quadratic value E[e2[n]] of the error e[n] = d[n]−y[n] between the desired signal d[n] and the ﬁltered
version y[n] of the input signal x[n]. Assuming that x[n] and d[n] are realizations of two jointly WSS
random sequences x[n] and d[n], if the optimum ﬁlter is not constrained to be causal, one ﬁnds that it

164
CHAPTER 4 Random Signals and Stochastic Processes
H(z)
x[n]
y[n]
d[n]
e[n]
FIGURE 4.16
General discrete Wiener ﬁlter.
must satisfy the equation
(h∗Rxx)[k] = Rxd[k],
(4.198)
i.e., its frequency response is
H(ej) = Sxd(ej)
Sxx(ej).
(4.199)
If one needs a causal FIR solution, a different but equally simple solution can be found for h[n].
We can solve a very simple example where we compute an optimum zero-order predictor for a given
signal of which a noisy version is available. In this case, H(ej) = c (a simple gain to be determined),
the input x[n] = s[n] + v[n] (signal s[n] additively corrupted by noise v[n]) and d[n] = x[n + 1]. The
solution is
c =
Rxx[1]
Rxx[0] + Rvv[0],
(4.200)
which yields the minimum mean quadratic error
E[e2[n]] =
R2
xx[0] + Rxx[0]Rvv[0] −R2
xx[1]
Rxx[0] + Rvv[0]
.
(4.201)
Notice that if no noise is present,
c = Rxx[1]
Rxx[0]
(4.202)
and
E[e2[n]] =
R2
xx[0] −R2
xx[1]
Rxx[0] + Rvv[0].
(4.203)
The reader should be aware that this example studies a theoretical probabilistic model, which may
therefore depend on several parameters which probably would not be available in practice and should
rather be estimated. In fact, it points out the solution a practical system should pursue.
TheWienerﬁlterisakindofbenchmarkforadaptiveﬁlters[7],whichoptimizethemselvesrecursively.

1.04.4 Random Process
165
1.04.4.17 Characterization of LTI systems and WSS random processes
A direct way to ﬁnd the impulse response h(t) of an LTI system is to apply white noise w(t) with power
spectral density Sww(ω) = Sww ∀ω to the system input, which yields Rx y(τ) = Swwh(τ). From an
estimate 
Rx y(t) of the cross-correlation, one can obtain an estimate of the desired impulse response:
ˆh(t) ≈
Rx y(t). In practice, assuming ergodicity, the following approximations are employed:
•
x(t) ≈δ(t);
•

Rx y(τ) = 
Rxy(τ) =
1
2T
 T
−T x(t)y(t + τ)dt for sufﬁciently long T.
Furthermore, the operations are often performed in the discrete-time domain.
The effective bandwidth of an LTI system with frequency response H(jω) can be estimated by its
noise bandwidth Ww. For a low-pass system, one looks for an equivalent ideal low-pass ﬁlter with
bandwidth ±Ww around ω = 0 with bandpass gain H(0); Ww is computed to guarantee that both
systems deliver the same mean output power when the same white noise is applied to their both inputs.
A straightforward algebraic development yields
Ww =
 ∞
0
|H(jω)|2dω
|H(0)|2
.
(4.204)
For a band-pass system with magnitude response centered about ω0, one looks for an ideal band-pass
ﬁlter equivalent with bandwidth Ww around ±ω0 with bandpass gain H(jω0); Ww is computed to
guarantee that both systems delivers the same mean output power when the same equal white noise is
applied to their both inputs. For this case,
Ww =
 ∞
0
|H(jω)|2dω
|H(ω0)|2
.
(4.205)
Areal“bandpass”randomprocess(seeSection1.04.4.15)aboutcenterfrequencyω0 canbeexpressed
as n(t) = m(t) cos[ω0t + θ(t)], where the envelope m(t) and the phase θ(t) are both baseband random
processes. We can write
n(t) = nx(t) cos (ω0t) −ny(t) sin (ω0t),
(4.206)
where nx(t) = n cos (θ(t)) and ny(t) = n sin (θ(t)) are the quadrature components of n(t). In the
typical situation where n(t) is a zero-mean WSS random process with auto-correlation Rnn(τ), we can
explicitly ﬁnd the ﬁrst and second-order moments of the quadrature components that validate the model.
First deﬁne the Hilbert transform of a given signal n(t) as ˆn(t) = (n ∗h)(t), where
H(jω) = F[h(t)] =
−j, ω > 0;
j,
ω < 0.
(4.207)
From Rnn(τ), we can ﬁnd Rˆnn(τ) and proceed to write:
•
nx = ny = 0;
•
Rnxnx (τ) = Rnyny(τ) = Rnn(τ) cos (ω0τ) + Rˆnn(τ) sin (ω0τ);
•
Rnxny(τ) = −Rnynx (τ) = Rnn(τ) sin (ω0τ) −Rˆnn(τ) cos (ω0τ).

166
CHAPTER 4 Random Signals and Stochastic Processes
In the frequency domain, if we deﬁne
S+
nn(jω) ≜
 Snn(jω), ω > 0
0,
ω ≤0,
(4.208)
then
•
Snxnx (jω) = Snyny(jω) = S+
nn(j(ω + ω0)) + S+
nn(j( −ω + ω0));
•
Snxny(jω) = −Snynx (jω) = j[S+
nn(j(ω + ω0)) −S+
nn(j( −ω + ω0))].
In the particular case where n(t) is Gaussian, it can be shown that nx(t) and ny(t) are Gaussian, and
thus completely deﬁned by their ﬁrst- and second-order moments above.
1.04.4.18 Statistical modeling of signals: random sequence as the output
of an LTI system
A discrete-time signal x[n] with spectrum X(ej) can be modeled as the output of a linear time-invariant
system whose frequency response is equal to X(ej) when excited by a unit impulse δ[n]. In this case,
the impulse response of the system is expected to be h[n] = x[n].
In the context of statistic models, an analogous model can be built: now, we look for a linear time-
invariant system which produces at its output the WSS random sequence x[n] with power spectral
density Sxx(ej), when excited by a unit-variance random sequence w[n] of white noise. As before, the
frequency response of the system alone is expected to shape the output spectrum, yet this time in the
mean power sense, i.e., the system must be such that Sxx(ej) = |H(ej)|2.
Assume the overall modeling system is described by the difference equation
x[n] =
p

k=1
ap[k]x[n −k] +
q

k=0
bq[k]w[n −k],
(4.209)
where w[n] is white noise with Sww(ej) = 1. The corresponding transfer function is
H(z) = Bq(z)
Ap(z) =
q

k=0
bq[k]z−k
1 +
p

k=1
ap[k]z−k
.
(4.210)
The output x[n] of this general system model with w[n] at its input is called ARMA (auto-regressive
moving-average22) process of order (p, q). It can be described by the following difference equation
in terms of its auto-correlation:
Rxx[k] =
p

l=1
ap[l]Rxx[k −l] =
⎧
⎪⎨
⎪⎩
q−k

l=0
bk[l + k]h∗[l], 0 ≤k ≤q;
0,
k > q.
(4.211)
22Auto-regressive for its output feedback, moving average for its weighted sum of past input samples.

1.04.4 Random Process
167
The values of Rxx[k] for k < 0 can be easily found by symmetry.
If one restricts the system to be FIR (i.e., to have a ﬁnite-duration impulse response), then p = 0,
i.e., ap[k] = 0 for 1 ≤k ≤p, and
H(z) =
q

k=0
bq[k]z−k.
(4.212)
The output of this “all-zero”23 system model with w[n] at its input is called an MA (moving-average)
process of order q. Its auto-correlation can be found to be
Rxx[k] =
q−|k|

l=0
bk[l + |k|]b∗
q[l].
(4.213)
In this case, Rxx[k] = 0 for k < −q or k > q. This model is more suited for modeling notches in the
random sequence power spectrum.
If one restricts the system to be “all-pole,”24 then q = 0, i.e., bq[k] = 0 for 1 ≤k ≤p, and
H(z) =
b[0]
1 +
p

k=1
ap[k]z−k
.
(4.214)
The output of this system model with w[n] at its input is called an AR (auto-regressive) process of
order p. Its auto-correlation follows the difference equation below:
Rxx[k] =
p

l=1
ap[l]Rxx[k −l] = |b[0]|2δ[k].
(4.215)
Again, the values of Rxx[k] for k < 0 can be easily found by symmetry. This model is more suited
for modeling peaks in the random sequence power spectrum, which is typical of quasi-periodic signals
(as audio signals, for example). It should be noticed that, differently from the ARMA and MA cases,
the equation for the AR process auto-correlation is linear in the system coefﬁcients, which makes their
estimation easier. If Rxx[k] is known for 0 ≤k ≤p, and recalling that Rxx[−k] = R∗
xx[k], one can:
•
solve the pth-order linear equation system obtained by substituting k = 1, 2, . . . , p in Eq. (4.215)
to ﬁnd ap[k], 1 ≤k ≤p;
•
compute |b[0]| from Eq. (4.215) with k = 0.
All-pole modeling of audio signals is extensively used in restoration systems [8].
23This is a misname, since the poles of the system are at the origin. In fact, the zeros are the only responsible for shaping the
frequency response of the system.
24This is another misname, since the zeros of the system are at the origin. In fact, the poles are the only responsible for shaping
the frequency response of the system.

168
CHAPTER 4 Random Signals and Stochastic Processes
Acknowledgments
The author thanks Dr. Paulo A.A. Esquef for carefully reviewing this manuscript and Leonardo de O.
Nunes for kindly preparing the illustrations.
Relevant Theory: Signal Processing Theory
See this Volume, Chapter 2 Continuous-Time Signals and Systems
See this Volume, Chapter 3 Discrete-Time Signals and Systems
References
[1] R. Deep, Probability and Statistics, Academic, San Diego, 2006.
[2] G. Casella, R.L. Berger, Statistical Inference, second ed., Duxbury, Paciﬁc Grove, 2001.
[3] A. Papoulis, P. Unnikrishna, Probability, Random Variables and Stochastic Processes, fourth ed., McGraw-Hill,
New York, 2002.
[4] P.Z. Peebles Jr., Probability, Random Variables and Random Signal Principles, fourth ed., McGraw-Hill, New
York, 2001.
[5] K.S. Shanmugan, A.M. Breipohl, Random Signals: Detection, Estimation and Data Analysis, Wiley, New York,
1988.
[6] M.H. Hayes, Statistical Digital Signal Processing and Modeling, Wiley, Hoboken, 1996.
[7] P.S.R. Diniz, Adaptive Filtering: Algorithms and Practical Implementation, Springer, New York, 2012.
[8] S.J. Godsill, J.W. Rayner, Digital Audio Restoration: A Statistical Model Based Approach, Springer, London,
1988.

5
CHAPTER
Sampling and Quantization
Håkan Johansson
Division of Electronics Systems, Department of Electrical Engineering Linköping University, Sweden
1.05.1 Introduction
The shift from analog signal processing (ASP) to digital signal processing (DSP) lies behind the tremen-
dous development of information processing systems that are used today in practically all areas one
can think of. For example, DSP has enabled the fast growth of mobile communication systems and
has opened up for new sophisticated medical aids, just to mention a couple of important applications.
The foremost advantages of DSP over ASP are its robustness and that it can perform functions with
arbitrary precision. This is because arithmetic operations in the digital domain can be done with as
small numerical errors as desired by simply increasing the resolution, i.e., by allocating more bits for
the operands. This can always be done as the resolution in DSP systems is independent of physical
variations, like temperature changes and processing inaccuracies etc., and is in contrast to ASP systems
which are highly dependent on such variations. The trend during the past decades has therefore been to
use as little ASP as possible and to replace analog functions with the corresponding digital functions.
This trend has been sped up by the fast development of integrated circuit techniques and progress in
the implementation of digital systems. Speciﬁcally, the introduction of cost-efﬁcient signal processors
and so called application-speciﬁc integrated circuits (ASICs) has enabled low-cost implementation and
manufacturing of digital systems. The real world is however analog by nature which means that the need
for ASP cannot be eliminated completely. In particular, it will always be necessary to use analog-to-
digital converters (abbreviated A/D converters or ADCs) and digital-to-analog converters (abbreviated
D/A converters or DACs) for interfacing the two realms.
Evidently, ADCs and DACs are crucial components in virtually all practical applications today. The
fundamental signal processing operations in these components are sampling and quantization (in ADCs),
and signal reconstruction (in DACs). This chapter considers the basic principles of these operations as
detailed below.
1.05.1.1 Scope and prerequisites
The theory of sampling has a long history [1–4], and the literature is abundant, see for example [5–7]
and references therein. In this chapter, we are only able to cover some parts of this ﬁeld. The emphasis
is on basic principles and analysis methods for uniform sampling and quantization which are the most
common sampling and quantization schemes used in conventional ADCs and DACs. Further, we will
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00005-X
© 2014 Elsevier Ltd. All rights reserved.
169

170
CHAPTER 5 Sampling and Quantization
only consider linear models of the sampling and quantization processes. Practical ADCs and DACs
exhibit (small) nonlinearities which also need be analyzed in practice, but it is beyond the scope of this
chapter to cover also this part. In other words, this chapter concentrates on the underlying principles of
uniform sampling and quantization rather than ﬁne implementation details. For such details, we refer to
books on ADCs and DACs [8]. Furthermore, we take an “engineering approach” in that we concentrate
on explaining the concepts that are sufﬁcient for understanding and analyzing practical signals and
systems. Some of the ﬁne mathematical details are therefore sometimes left out.
It is assumed that the reader has a basic knowledge of continuous-time and discrete-time signals and
systems as well as the Fourier, Laplace, and z-transforms. Some of the basics regarding signals and
systems are however brieﬂy recapitulated in Section 1.05.2. Further, Section 1.05.4 assumes that the
reader is familiar with stochastic processes. Most of the material in the other sections can however be
understood without reading Section 1.05.4.
1.05.1.2 Chapter outline
After this introductory section, Section 1.05.2 gives some basic concepts. Then, Section 1.05.3 discusses
the basics of uniform sampling and reconstruction of deterministic signals. The content of this section
is typically also found in introductory books in signal processing [9,10]. Section 1.05.4 considers the
extension to sampling and reconstruction of stochastic processes (also called random processes). In
introductory books in signal processing, this is typically treated very brieﬂy or not at all.
Section 1.05.5 considers an application referred to as time-interleaved ADCs, which is a scheme that
has been introduced for high-speed A/D conversion [11]. The section discusses the principles and signal
reconstruction methods which are required in this scheme due to analog channel mismatches. Without
correction, this scheme corresponds to nonuniform sampling (or generalizations), but the overall goal,
with correction, is to achieve uniform sampling.
Section 1.05.6, covers the principles of quantization, both in ADCs and internally in digital sys-
tems, as they are closely related. In Section 1.05.7, oversampled ADCs and DACs are brieﬂy discussed.
Oversampling techniques are used in practice to, e.g., increase the performance and relax the require-
ments of the analog components. Finally, Section 1.05.8 presents a method for discrete-time modeling
of mixed-signal systems. This is useful in systems with feedback, like so called -modulator-based
ADCs. Such converters can reach a very high resolution using oversampling together with noise shaping
[12,13].
Finally, it is pointed out that, throughout the chapter, real-valued signals are used in the examples.
However, the basic principles to be presented are valid for complex signals as well.
1.05.1.3 Current trends
The research and development of data converters have been quite intensive for several decades, both in
academia and industry. This interest is foreseen to continue for many years to come as the requirements
on the signal processing systems and thus the converters continuously increase. To exemplify, it was
envisaged in [14] that an ADC capable of bandwidths of more than 100 MS/s and 17 bits resolution is
required in fourth-generation communication systems. This is beyond the state-of-the art ADCs which
can manage only some 13–14 effective bits of resolution at such sampling rates [15–17]. At the same
time, it is also desirable to decrease the size and power consumption of the converters. To enable the

1.05.2 Preliminaries
171
high-performance applications of tomorrow, it is therefore vital to continue to conduct research on new
principles of data conversions as well their practical implementations.
There are different paths one can take to increase the performance. One way is to make use of
parallel converters like time-interleaved ADCs [11] (to be discussed in Section 1.05.5) and/or parallel
 ADCs [18]. Such converters have great potentials but due to analog channel mismatches they also
introduce errors that must be compensated for. A recent trend here is “digitally assisted analog circuits,”
where digital circuitry is added to correct for analog errors [19]. With the increasing computational
capacity of digital systems, one can expect that this trend will continue. The digital parts may however
be computationally excessive, and it is therefore vital to devise new algorithms to reduce the complexity.
Another trend is that of sub-Nyquist sampling covering so called sparse sampling and compressed
sampling (also called compressed sensing) [20–22]. This is different from the traditional Nyquist-
sampled systems in that additional knowledge of the input signal is taken into account. Speciﬁcally, if
the signal is sparse in some sense, one can reduce the sampling rate and thereby relax the requirements
on the converters. One scenario is for applications where the signals are locally narrow-band (in time)
but globally wide-band. Conventionally, high-speed converters are then needed. Utilizing the sparse or
compressed sampling paradigm, the sampling rate can be reduced substantially. However, at this point,
mainly the concepts have been presented and analyzed for some speciﬁc cases and classes of signals. It
is an open question how well such principles will work in practical implementations.
1.05.2 Preliminaries
1.05.2.1 Classiﬁcation of signals
We communicate by conveying information (messages), like speech and images. In some contexts, one
wishes to transmit information from one place to another, like in telephone systems. In other cases, it
may be desired to store the information for later use, e.g., music stored on CDs.
Information can be transmitted using information bearing signals. A signal is a quantity that varies
with time or other independent variables. Usually, the information must be converted to a desired signal
type. For example, when we speak in a microphone, the generated sound is converted to an electrical
voltagethatvarieswithtime.Thesoundcanthenberecoveredbyconnectingthevoltagetoaloudspeaker.
(In this context, however, we usually do not consider the sound as being recovered since it, in principal,
is reconstructed at the same time as it is generated.) The same information can be represented in many
different ways. Different types of signals can in other words contain exactly the same information. For
example, speech and music can be represented in analog form on LPs and in digital form on CDs. The
corresponding signals are then analog and digital, respectively.
In mathematical form, a signal is represented as a function (or sequence) of one or several inde-
pendent variables. The signal is a one-dimensional signal if it is a function of one variable and a
multi-dimensional signal if it is a function of several variables. This chapter considers one-dimensional
signals only. An example of such signals is a speech signal. It is customary to let the independent variable
represent time although it may represent whatever we wish, like temperature, pressure, etc. The time
variable in the representation of a signal can be either continuous or discrete (quantized). The signal is a
continuous-time signal if the variable is continuous and a discrete-time signal if the variable is discrete. A
continuous-time signal is thus deﬁned for all time instances (except possibly at discontinuities) whereas a

172
CHAPTER 5 Sampling and Quantization
time
Signal value
Continuous-time signal
Continuous time
Continuous signal values
Continuous-time signal
Continuous time
Discrete signal values
Discrete-time signal
Discrete time
Continuous signal values
Discrete-time signal
Discrete time
Discrete signal values
t0
t1
t2
t –1
t0
t1
t2
t –1
time
time
time
Signal value
Signal value
Signal value
FIGURE 5.1
Different types of signals.
discrete-time signal is deﬁned only at discrete time instances. We also differ between signals that can
take on continuous signal values and those that only can take on discrete signal values. Consequently,
signals can be divided into four different categories according to Figure 5.1.
In the literature, a continuous-time signal with continuous signal values is often referred to as an
analog signal, whereas a discrete-time signal with discrete signal values is called a digital signal.
However, in some (earlier) literature, “analog” and “digital” are only related to the signal values, i.e.,
the signals to the left in Figure 5.1 are then analog whereas the signals to the right in the same ﬁgure
are digital. In this book, we assume hereafter that analog signals are continuous-time signals whereas
digital signals are discrete-time signals.
1.05.2.2 Discrete-time signals—sequences
A continuous-time signal can mathematically be represented by a continuous function x(t) where t is a
continuous variable. A discrete-time signal cannot be represented in this way since it is only deﬁned at
discrete instances. To handle discrete-time signals mathematically, one therefore makes use of sequences
of numbers.
As one can see from Figure 5.1, the signal values of a discrete-time signal constitute a sequence of
numbers. In mathematical form, a sequence of numbers can be written as
{x(n)},
n = . . . , −2, −1, 0, 1, 2, . . . ,
(5.1)

1.05.2 Preliminaries
173
FIGURE 5.2
Graphical representation of a sequence x(n).
FIGURE 5.3
A discrete-time signal and its representation in the form of a sequence x(n).
where {x(n)} denotes the sequence whereas x(n) denotes value number n in the sequence. The values
x(n) are usually called samples because they are often generated through sampling of continuous-
time signals, see the subsequent subsection. The notation of the sequence is thus {x(n)}, but when no
misunderstandings can occur we write “the sequence x(n)” for the sake of simplicity. A sequence can
be represented graphically as illustrated in Figure 5.2. The horizontal axis is drawn as a continuous line
but it is important to remember that the sequence is only deﬁned for discrete values.
When a sequence represents a discrete-time signal, the samples x(n) equal the signal values of the
signal at the time instances t = tn, as illustrated in Figure 5.3. Since sequences often represent discrete-
time signals, we frequently write “the discrete-time signal x(n)” instead of “the sequence x(n).” This is
in accordance with the continuous-time case where one often writes “the continuous-time signal x(t)”
instead of “the function x(t).”
1.05.2.3 Sampling of continuous-time signals
Discrete-time signals are often generated through sampling (measurement) of continuous-time signals.
It is common to use the same time interval T between the sampling instances. The sampling then takes
place at the time instances t = nT , as illustrated in Figure 5.4. The time interval is called the sampling
period whereas fs = 1/T is referred to as the sampling frequency, which thus denotes the number of
sampling instances per second. This type of sampling is called uniform sampling. If xa(t) denotes the

174
CHAPTER 5 Sampling and Quantization
FIGURE 5.4
Sampling of a continuous-time signal.
continuous-time signal,1 the corresponding discrete-time signal x(n) becomes x(n) = xa(nT ). Instead
of using the notation x(n) one may thus use xa(nT ) to stress that the signal values are obtained via
uniform sampling of a continuous-time signal. Throughout this chapter, we mainly use x(n) for the sake
of simplicity.
1.05.2.4 Classiﬁcation of systems
Systems can be classiﬁed in accordance with their input and output signal types. As we saw in the
previous section, we can divide signals into four different categories. Hence, one may also divide
systems into four basic classes as seen in Figure 5.5. A continuous-time system is a system whose input
and output signals are continuous-time signals, whereas a discrete-time system is a system whose input
and output signals are discrete-time signals. Mathematically, the role of a discrete-time system is thus
to generate an output sequence given an input sequence. Therefore, instead of the terms “input signal”
and “output signal,” we often use the terms “input sequence” and “output sequence.” A digital system is
a discrete-time system in which the samples only can take on discrete values. Today, signal processing
systems usually contain both continuous-time and discrete-time parts.
1The subscript “a” in xa(t) stands for analog. We use the notation xa(t) instead of xc(t), which would be natural when we
deal with continuous-time signals. This is to avoid confusion with frequency edges c etc., where “c” stands for cutoff.

1.05.2 Preliminaries
175
FIGURE 5.5
Three different types of systems.
1.05.2.5 Digital signal processing of analog signals
The principle of digital signal processing of analog signals is illustrated in Figures 5.6–5.8. The ﬁrst
step is to convert the signal from analog into digital form. This is done by an ADC which takes care of
both sampling and quantization. The so obtained digital signal can then be processed by a digital system
which takes as input a sequence of samples, xQ(n), and generates an output sequence, yQ(n) [Q indicates
quantized signal values]. Finally, reconstruction takes place in order to go back to a continuous-time
representation. This is performed by a DAC followed by an analog so called reconstruction ﬁlter.
To be able to perform A/D and D/A conversion without errors, the original signal must be bandlimited.
Before the A/D conversion, the signal is therefore bandlimited with the aid of an analog so called anti-
aliasing ﬁlter (wb in xwb(t) in Figure 5.6 stands for wide-band). A/D conversion is done by ﬁrst sampling
and then quantizing the signal, see Figure 5.7a. In practice, the sampling is performed by a sample-and-
hold-circuit (S/H circuit). An S/H circuit samples an analog signal at the time instances t = nT and
holds this sample value until the next sampling instance. The output signal from an ideal S/H circuit
is a new analog signal that ideally is piece-wise constant. The function of the S/H circuit can be seen
in Figure 5.8a and b. The next step in the A/D conversion is to quantize the sample values using a
quantizer. The input signal of the quantizer is thus the output signal from the S/H circuit. The input to
FIGURE 5.6
Digital signal processing of analog signals.

176
CHAPTER 5 Sampling and Quantization
(a)
(b)
(c)
FIGURE 5.7
ADC, digital system, and DAC.
the quantizer needs to be kept constant long enough for the quantizer to produce a correct result which
is why an S/H circuit is needed.
Quantization amounts to representing each sample in a binary form with a ﬁnite wordlength. There
exist many different types of binary representations. One commonly used binary form is the two’s-
complement representation. A number x, which for the sake of simplicity is assumed here to lie between
−1 and 1, is then represented as
x = −x0 +
B−1

i=1
2−i xi,
(5.2)
where xi, i = 0, 1, . . . , B −1, can take on the values zero and one. The number of bits is thus B.
The bits from a quantizer can be delivered in serial or parallel. In the latter case, the output signal
from a B-bit quantizer consists in practice of B continuous-time signals which together represent the
different sample values. In the ideal case, these continuous-time signals are binary. That is, they can
only take on two different values, usually zero and one. The principle can be seen in Figure 5.8c and d.
In practice, the signals must hold the values zero or one long enough so one can store them in registers,
etc. Furthermore, it takes a certain time to perform the quantizations which means that a certain amount
of delay will be introduced. The output signal from the quantizer is the desired digital signal that can
be processed by a digital system. In a practical implementation, the sample values are represented in
the same form as the quantizer output. The output signal from a digital system is thus represented in
the same form as its input signal, see Figure 5.8e and f.
AdigitalsignalcanbeconvertedtoananalogsignalusingaDACfollowedbyananalogreconstruction
ﬁlter. The input signal to the DAC is a digital signal represented in the same form as the output signal
from the ADC, i.e., the output from the quantizer. The output signal from the DAC is a continuous-time
signal that is piece-wise constant with a limited number of levels, see Figure 5.8(g). In other words, it

1.05.2 Preliminaries
177
(a)
(c)
(e)
(g)
(b)
(d)
(f)
(h)
FIGURE 5.8
Typical signals in digital signal processing of analog signals.
is of the same type as the output signal from the S/H circuit with the difference that the DAC output
signal takes on discrete signal values whereas the S/H circuit output signal takes on continuous signal
values. Finally, to obtain the desired analog signal, the output signal from the DAC must be ﬁltered.
The role of this ﬁlter is to “smooth out” the piece-wise-constant signal from the DAC so as to obtain
the desired signal.
As evident from above, sampling and reconstruction as well as quantization play fundamental roles
in digital signal processing. The following sections treat the basic principles of these operations. We
will only deal with relations between continuous-time signals and the discrete-time counterparts. Math-
ematically, this amounts to study relations between functions xa(t) and the corresponding sequences
x(n) = xa(nT ) as well as their transform-domain relations. We will not discuss the physical represen-
tation of the signals seen in Figures 5.7 and 5.8.

178
CHAPTER 5 Sampling and Quantization
FIGURE 5.9
Simple model of a digital communication system.
1.05.2.6 Digital communication—analog signal processing of digital signals
In digital communication systems, the situation is opposite to that in the previous section in that analog
signal processing is now used for processing digital signals. Figure 5.9 shows a simple model of a digital
communication system. The original digital information is contained in xQ(n). This information is D/A
converted before being transmitted over an analog channel. Finally, on the receiver side, the signal is
A/D converted, generating yQ(n). It is desired to have yQ(n) = xQ(n) so as to receive the information
sent. In practice, due to many different error sources in the communication channel, advanced digital
signal processing methods are required to transmit information successfully. Regarding the sampling
and reconstruction, one can make use of the same analysis methods here as for the system depicted
earlier in Figure 5.6.
1.05.3 Sampling of deterministic signals
Discrete-time signals are often generated through sampling of continuous-time signals. It is therefore
of interest to investigate the relations between the Fourier transform of a discrete-time signal and that
of the underlying continuous-time signal. This relation is given by the so called Poisson’s summation
formula which leads us to the sampling theorem. This theorem states that a continuous-time signal
can be sampled and perfectly reconstructed provided that the signal is bandlimited and the sampling
frequency exceeds twice the bandwidth. If the theorem is fulﬁlled, the original signal can be retained
via an ideal pulse amplitude modulator. This section treats basic concepts and theory on sampling and
reconstruction. In practice, one can neither fulﬁll the sampling theorem nor perform the reconstruction
perfectly because ideal pulse amplitude modulators cannot be implemented. Some of these practical
aspects are included in the section. It is also noted that this section deals with deterministic signals. The
extension to stochastic signals is treated in Section 1.05.4.
1.05.3.1 Uniform sampling
Discrete-time signals are typically generated through sampling (measurement) of continuous-time sig-
nals. Most signal processing applications are based on uniform sampling which means that the time
interval between two consecutive sampling instances is constant. The sampling then takes place at the
time instances t = nT , n = . . . , −2, −1, 0, 1, 2, . . ., as illustrated in Figure 5.10. The time interval T
is called the sampling period whereas fs = 1/T is referred to as the sampling frequency, which thus
denotes the number of sampling instances per second. If xa(t) denotes the continuous-time signal, the
corresponding discrete-time signal, represented by the sequence x(n), becomes
x(n) = xa(nT ).
(5.3)

1.05.3 Sampling of Deterministic Signals
179
FIGURE 5.10
Uniform sampling.
1.05.3.2 Poisson’s summation formula
This section derives a relation between the Fourier transforms of x(n) and xa(t). Assume that the
continuous-time signal xa(t) has the Fourier transform Xa( j). We can then express xa(t) in terms of
its inverse Fourier transform according to
xa(t) = 1
2π
 ∞
−∞
Xa( j)e jtd.
(5.4)
In particular, we have for t = nT ,
xa(nT ) = 1
2π
 ∞
−∞
Xa( j)e jnT d,
(5.5)
which can be rewritten as
xa(nT ) = · · · + 1
2π
 −π/T
−3π/T
Xa( j)e jnT d + 1
2π
 π/T
−π/T
Xa( j)e jnT d
+ 1
2π
 3π/T
π/T
Xa( j)e jnT d + · · ·
By making the variable substitutions  → −2πk/T , k integer, utilizing that e−j2πkn = 1, and
changing the order between summation and integration, we obtain
xa(nT ) =
∞

k=−∞
1
2π
 π/T
−π/T
Xa

j −j 2πk
T

e
j

−2πk
T

nT d,
(5.6)
= 1
2π
 π/T
−π/T
∞

k=−∞
Xa

j −j 2πk
T

e jT nd.
(5.7)
With T as integration variable we get
xa(nT ) = 1
2π
 π
−π
1
T
∞

k=−∞
Xa

j −j 2πk
T

e jT nd(T ).
(5.8)

180
CHAPTER 5 Sampling and Quantization
Further, we know that a discrete-time signal x(n) can be expressed in terms of its inverse Fourier
transform according to
x(n) = 1
2π
 π
−π
X(e jT )e jT nd(T ).
(5.9)
Thus, if we let x(n) = xa(nT ), and utilize the uniqueness of the Fourier transform, we obtain the
following relation from (5.8) and (5.9):
X(e jT ) = 1
T
∞

k=−∞
Xa

j −j 2πk
T

.
(5.10)
Equation (5.10) is referred to as Poisson’ summation formula2 which gives the relation between the
Fourier transform of a continuous-time signal and the Fourier transform of the discrete-time signal that
is generated by sampling the continuous-time signal uniformly with the sampling period T (sampling
frequency fs = 1/T ) [23]. Poisson’s summation formula states that the spectrum of the discrete-time
signal is obtained by summing the spectrum of the continuous-time signal and its shifted versions,
weighted with 1/T . This is illustrated in Figure 5.11 in the following example.
(b)
(c)
(a)
(d)
FIGURE 5.11
Spectra for a continuous-time signal and the corresponding sequences generated via sampling with three
different sampling frequencies.
2If the sampled signal is discontinuous at a certain time instant, and if the signal is sampled at this instant, then the sample
value must be chosen as the average of the left and right limits in order to make (5.10) valid.

1.05.3 Sampling of Deterministic Signals
181
FIGURE 5.12
Illustration of the sampling theorem.
1.05.3.2.1
Example of Poisson’s summations formula
Figure 5.11a shows the spectrum of a bandlimited continuous-time signal. Figure 5.11b–d show the
spectra or three different discrete-time signals generated by sampling the continuous-time signal with
the sampling frequencies 100 Hz, 50 Hz, and 40 Hz, respectively, i.e., T = 1/100 s, 1/50 s, and 1/40 s,
respectively. Studying Figure 5.11, the following observations can be made. In the ﬁrst two cases,
the partial spectra do not overlap each other [Figure 5.11b, c]. This means that the shape of Xa( j) is
preserved and it is thereby possible in principle to reconstruct xa(t) from x(n), as we shall see later. In the
third case, the shape of Xa( j) is deteriorated since the partial spectra to some extent overlap each other
[Figure 5.11d]. In this case, we can no longer perfectly reconstruct xa(t) from x(n). What distinguishes
the three sequences is that they have been obtained using three different sampling frequencies. If the
continuous-time signal is bandlimited, it can thus be sampled and reconstructed perfectly provided that
the sampling frequency is sufﬁciently high. This is summarized below in the sampling theorem.
1.05.3.3 The sampling theorem
As illustrated above, a continuous-time signal can be sampled and reconstructed from the generated
samples if the so called sampling theorem is fulﬁlled.
Sampling theorem: If a continuous-time signal xa(t) is bandlimited to  = 0 ( f = f0), i.e.,
Xa( j) = 0,
|| > 0 = 2π f0,
(5.11)
then xa(t) can be recovered from the samples x(n) = xa(nT ) provided that
fs = 1
T > 2 f0.
(5.12)
Thus, if (5.11) and (5.12) are satisﬁed, the continuous-time signal xa(t) can be recovered from
x(n) = xa(nT ). One can understand this by studying Figure 5.12 (and also Figure 5.11). If the partial
spectra in Poisson’s summation formula do not overlap each other, the shape of Xa( j) is preserved

182
CHAPTER 5 Sampling and Quantization
and it is thereby possible, in principle, to reconstruct xa(t) from x(n). From Figure 5.12, we see that
overlap of spectra is avoided if
2π −0T > 0T ⇔0T < π.
(5.13)
Since  = 2π f and fs = 1/T , (5.13) is equivalent to
2π f0T < π ⇔fs = 1
T > 2 f0,
(5.14)
which is the same as (5.12). The frequency fs/2 is called the Nyquist frequency whereas fs is
referred to as the Nyquist sampling frequency. Further, the frequency band  ∈[−π fs, π fs] (thus
f ∈[−fs/2, fs/2]) is referred to as the ﬁrst Nyquist band.
The sampling theorem says that the sampling frequency fs must be strictly greater than 2 f0 in order
to be able to recover the signal. It should be mentioned however that for some classes of signals, like
energy signals, one may select fs equal to 2 f0 and still reconstruct the signals (in a certain sense). For
other classes of signals, like sinusoidal signals, it is necessary to fulﬁll fs > 2 f0. This is because, for a
sinusoidal signal with the frequency f0, one may for example sample the zero-crossings. That is, if fs
equals 2 f0, one may obtain a sequence with zero-valued samples which means that the continuous-time
signal can not be reconstructed.
If fs ≤2 f0, the sampling theorem is not fulﬁlled in which case the signal is undersampled. This
means that the partial spectra will overlap, as was discussed earlier and illustrated in Figure 5.11d. This
in turn implies that the shape of the continuous-time signal’s spectrum is not preserved, which means that
the signal cannot be reconstructed exactly. The error that is then introduced is called aliasing distortion.
The reason for this name is that frequency components above the Nyquist frequency are aliased to the
baseband T ∈[−π, π], see Figure 5.11d. In practice, one often chooses a higher sampling frequency
than that required to fulﬁll the sampling theorem, the reason being that one can thereby reduce the
requirements on the analog components that are used in the A/D and D/A conversions. In such a case,
the signal is oversampled. If fs = M2 f0, one has M times oversampling. Oversampled A/D and D/A
converters are discussed in Section 1.05.7.
1.05.3.3.1
Example of aliasing
To further illustrate that the sampling theorem must be fulﬁlled in order to sample and reconstruct a
signal, we consider stationary sinusoidals. Let xa1(t) and xa2(t) be two stationary sinusoidal signals
according to
xa1(t) = sin (0t) = sin (2π f0t)
(5.15)
and
xa2(t) = −sin (30t) = −sin (6π f0t).
(5.16)
We now form the discrete-time signals x1(n) and x2(n) by sampling xa1(t) and xa2(t), respectively,
with a sampling frequency of fs = 4 f0, i.e., a sampling period of T = 1/(4 f0). This gives us
x1(n) = xa1(nT ) = xa1(0.25n/ f0) = sin (0.5πn)
(5.17)

1.05.3 Sampling of Deterministic Signals
183
(a)
(b)
(c)
(d)
FIGURE 5.13
Illustration of aliasing in the example of Section 1.05.3.3.1.
and
x2(n) = xa2(nT ) = xa2(0.25n/ f0) = −sin (1.5πn).
(5.18)
Since sin (a) = sin (a + 2πn) for all integers n, and sin (a) = −sin ( −a), we can rewrite x2(n) as
x2(n) = −sin (1.5πn) = −sin (1.5πn −2πn),
= −sin ( −0.5πn) = sin (0.5πn) = x1(n).
That is, x2(n) equals x1(n). The reason for this is that the sampling theorem is not fulﬁlled for
xa2(t). This means that the spectrum of xa2(t) is aliased into the band T ∈[−π, π], as illustrated in
Figure 5.13. If the sampling theorem is not met, we can thus obtain the same sequence through sampling
of two different signals.
1.05.3.4 Anti-aliasing ﬁlter
A condition for sampling and perfect reconstruction of a continuous-time signal is that it is strictly
bandlimited. In practice, continuous-time signals are not strictly bandlimited which means that we will

184
CHAPTER 5 Sampling and Quantization
(a)
(b)
(c)
FIGURE 5.14
Band limitation using an anti-aliasing ﬁlter (wb stands for wide-band).
always have aliasing distortion since the partial spectra then overlap each other. This in turn implies
that we can not reconstruct the signal perfectly, but instead we obtain a distorted signal. To obtain an
acceptable level of distortion, one usually have to ﬁlter the signal before the sampling takes place, as
seen in Figure 5.14a. In this context, the ﬁlter H(s) is called anti-aliasing ﬁlter because it is used to
reduce the aliasing distortion.
Apparently, the anti-aliasing ﬁlter affects the signal that is to be sampled. However, in many
cases, the information in the signal lies in the low-frequency region whereas the high-frequency band
only contains undesired frequency components like noise etc. If the anti-aliasing ﬁlter is an ideal
lowpass ﬁlter, the information is unaffected whereas the undesired components are removed. This
is illustrated in Figure 5.14b and c. In practice, one can not implement ideal ﬁlters which means
that one always has to account for a certain amount of distortion. More about distortion is given in
Section 1.05.3.6.
1.05.3.5 Reconstruction
Reconstruction refers to the process that forms a continuous-time signal from a discrete-time signal.
The reconstruction can be done with the aid of pulse amplitude modulation. The system that performs
the pulse amplitude modulation is called pulse amplitude modulator (PAM). The sampling and recon-
struction can be represented as in Figure 5.15. We denote the reconstructed signal as xr(t) to separate
it from the original signal xa(t).

1.05.3 Sampling of Deterministic Signals
185
FIGURE 5.15
Sampling and reconstruction using a PAM.
(a)
(b)
(c)
(d)
FIGURE 5.16
Example of reconstruction (5.19).
The PAM forms the output signal xr(t) through multiplication of the samples x(n) by the pulse
(signal) p(t) and its shifted versions p(t −nT ). The output signal is given by
xr(t) =
∞

n=−∞
x(n)p(t −nT ).
(5.19)
Figure 5.16 shows an example of reconstruction according to (5.19).
1.05.3.5.1
Ideal reconstruction
When the sampling theorem is fulﬁlled, it is in principle possible to obtain xr(t) = xa(t) by properly
selecting p(t). If the original signal xa(t) is perfectly reconstructed from the samples x(n) = xa(nT ),
we have ideal reconstruction. As will be clear below, it is however not possible to perform ideal recon-
struction in practice.

186
CHAPTER 5 Sampling and Quantization
To see how to choose p(t) in order to obtain xr(t) = xa(t), we proceed as follows. If we utilize the
expression for the reconstructed signal xr(t) in (5.19), its Fourier transform can be written as
Xr( j) =
 ∞
−∞
xr(t)e−jtdt =
 ∞
−∞
∞

n=−∞
x(n)p(t −nT )e−jtdt
=
∞

n=−∞
x(n)
 ∞
−∞
p(t −nT )e−jtdt
(5.20)
assuming that integration and summation can be interchanged. The variable substitution v = t −nT in
(5.20) yields
Xr( j) =
∞

n=−∞
x(n)
 ∞
−∞
p(v)e−j(v+nT)dv
=

∞

n=−∞
x(n)e−jT n
	  ∞
−∞
p(v)e−jvdv

= X(e jT )P( j).
We also know that X(e jT ) is related to Xa( j) via Poisson’s summation formula. The Fourier
transform of the reconstructed signal can therefore be related to the Fourier transform of the original
signal according to
Xr( j) = P( j)X(e jT ) = P( j) 1
T
∞

k=−∞
Xa

j −j 2πk
T

.
(5.21)
If the sampling theorem is fulﬁlled, the partial spectra Xa

j −j2πk/T

do not overlap each other
as illustrated in Figure 5.17. We can therefore obtain Xr( j) = Xa( j) by letting P( j) be an ideal
lowpass ﬁlter according to
P( j) =
 T , || ≤c < π/T ,
0, || > c
(5.22)
with 0 < c ≤π/T since P( j) then leaves Xa( j) unaffected and removes the remaining partial
spectra. This is also illustrated in Figure 5.17. We then obtain (utilizing the uniqueness of the Fourier
transform)
Xr( j) = Xa( j) ⇔xr(t) = xa(t).
(5.23)
That is, the original signal xa(t) is reconstructed from the samples x(n).
The impulse p(t) is obtained by taking the inverse Fourier transform of P( j) in (5.22). For example,
with c = π/T , we get
p(t) = sin (πt/T )
πt/T
,
(5.24)
which is a two-sided function and thus not realizable in practice by causal systems.

1.05.3 Sampling of Deterministic Signals
187
(a)
(b)
(c)
(d)
FIGURE 5.17
Illustration of ideal reconstruction.
1.05.3.5.2
Reconstruction using a D/A converter and an analog reconstruction ﬁlter
In practice, the reconstruction is performed using a D/A converter followed by an analog so called
reconstruction ﬁlter as depicted in Figure 5.18. The D/A converter is in principle a PAM and can
therefore be described with a pulse in accordance with the previous section. If we denote this pulse as
q(t), and let x1(t) denote the output from the D/A converter, we get
X1( j) = Q( j)X(e jT ).
(5.25)
Furthermore, we have
Xr( j) = H( j)X1( j).
(5.26)
Combining (5.25) and (5.26) gives us
Xr( j) = Q( j)H( j)X(e jT ).
(5.27)
We can now write Xr( j) as
Xr( j) = P( j)X(e jT ),
(5.28)

188
CHAPTER 5 Sampling and Quantization
FIGURE 5.18
Illustration of ideal reconstruction.
where
P( j) = Q( j)H( j).
(5.29)
We know that, if P( j) is chosen as in (5.22), then xr(t) = xa(t). Hence, also a scheme with a
D/A converter followed by a reconstruction ﬁlter according to Figure 5.18 achieves ideal reconstruction
provided that Q( j)H( j) satisﬁes
Q( j)H( j) =
 T , || > c < π/T ,
0, || > c,
(5.30)
where 0 < c ≤π/T . Dividing the reconstruction into two steps gives us more degrees of freedom.
In particular, one can thereby let q(t) be a simple pulse since the only requirement is that the product
Q( j)H( j) must meet the right-hand side of (5.30). In principle, the actual shape of the individual
responses Q( j) and H( j) are therefore arbitrary. In practice, it is much easier to generate simple
pulses, that subsequently are ﬁltered through a conventional analog ﬁlter, instead of generating the more
complicated pulses directly, like the sinc function in (5.24).
A common type of D/A converter is of the type zero-order hold. In this case, q(t) is a square pulse
according to Figure 5.19d. This pulse can mathematically be expressed as
q(t) =
1, 0 < t < T ,
0, t < 0, t > T .
(5.31)
Its Fourier transform is
Q( j) = e−jT /2 sin (T /2)
/2
.
(5.32)
Hence, the Fourier transform Q( j) is a sinc function with zero-crossings at ±2πk/T , for all
integers k. We note that Q( j)H( j) satisﬁes (5.30) if the frequency response H( j) is selected as
H( j) =
 T /Q( j), || ≤c < π/T ,
0,
|| > c.
(5.33)
For low frequencies, H( j) is thus
H( j) = e jT /2
/2
sin (T /2),
|| ≤c < π/T .
(5.34)

1.05.3 Sampling of Deterministic Signals
189
(a)
(b)
(c)
(d)
(e)
(f)
FIGURE 5.19
Illustration of reconstruction using a D/A converter and an analog ﬁlter.
When c = π/T , the frequency response is as shown in Figure 5.20. The response of this ﬁlter is
discontinuous at c = π/T and it can therefore not be implemented in practice. Further, it would be
very costly to approximate this function since a high ﬁlter order would be required in order to avoid a
large distortion.
To get around these problems in practical implementations, one uses a certain amount of oversam-
pling. This means that the sampling frequency is higher than the sampling theorem requires. One can
thereby let the reconstruction ﬁlter have a transition band as shown in Figure 5.21. The higher is the
sampling frequency, the wider can the transition band be. The ﬁlter requirements are thus relaxed when

190
CHAPTER 5 Sampling and Quantization
0
0.5
1
−8π
−6π
−4π
−2π
0
2π
4π
6π
8π
Q(jΩ)
0
1
2
−8π
−6π
−4π
−2π
0
2π
4π
6π
8π
H(jΩ)
0
0.5
1
−8π
−6π
−4π
−2π
0
2π
4π
6π
8π
Ω
Q(jΩ)H(jΩ)
FIGURE 5.20
Frequency responses in ideal reconstruction using a D/A converter followed by and analog reconstruction
ﬁlter (T = 1).
the sampling frequency is increased. Another advantage of using oversampling is that the quantization
noise that is introduced in the A/D conversion thereby can be reduced. Oversampling will be discussed
later in Section 1.05.7. An oversampled A/D converter generates more samples than is actually required
to reconstruct the signal. To make sure that the subsequent digital system do not have to work at higher
data rates than necessary, one reduces the data rate by using decimation [24]. This can be done without
losing information since the signal has been oversampled. In other words, there is a redundancy in the
corresponding discrete-time signal.
1.05.3.6 Distortion caused by undersampling
In practice, a signal is never strictly bandlimited which means that one will always undersample the
signal and thereby obtain a certain amount of aliasing distortion. The sampling frequency must be high
enough to ensure that the reconstructed signal does not differ too much from the original one. The
following example illustrates how the choice of sampling frequency affects the reconstructed signal.
1.05.3.6.1
Example of undersampling
Let xa(t) be a continuous-time signal according to
xa(t) = e−a|t|,
a > 0.
(5.35)
This signal has a real Fourier transform given by
Xa( j) =
2a
2 + a2 .
(5.36)

1.05.3 Sampling of Deterministic Signals
191
(a)
(b)
(c)
(d)
(e)
FIGURE 5.21
Typical spectra using oversampling and reconstruction via a D/A converter followed by an analog reconstruc-
tion ﬁlter.
Apparently, Xa( j) is not bandlimited. We now sample xa(t) with the sampling frequency fs = 1/T
which gives us the discrete-time signal x(n) according to
x(n) = xa(nT ) = e−a|nT | =

e−aT |n|
.
(5.37)

192
CHAPTER 5 Sampling and Quantization
0
1
2
−2π
−π
0
π
2π
Ω [rad/s]
Xa (jΩ)
Xa (t)
Xr (t)
0
1
2
π
−0.5π
0
0.5ππ
ΩT [rad]
T×X (ejΩT)
T = 1/2
−2
0
2
0
0.5
1
t [s]
−2
0
2
0
0.5
1
t [s]
FIGURE 5.22
Spectra and signals in the example in Section 1.05.3.6.1 for T = 1/2.
The Fourier transform of x(n) is
X(e jT ) =
1 −e−2aT
1 −2e−aT cos (T ) + e−2aT .
(5.38)
When Xa( j) is real, so is X(e jT ) since the two transforms can be related to each other via Poisson’s
summation formula. Finally, we perform reconstruction using an ideal PAM where P( j) is given by
P( j) =
 T , || ≤π/T ,
0, || > π/T .
(5.39)
This gives us the signal xr(t) whose Fourier transform is
Xr( j) =

T X(e jT ), || ≤π/T ,
0,
|| > π/T ,
(5.40)
where X(e jT ) is given by (5.38). Figures 5.22 and 5.23 show the spectra Xa( j) and X(e jT ) as
well as the signals xa(t) and xr(t). In Figure 5.22, T = 1/2 whereas T = 1/20 in Figure 5.23. In both
cases, a = 1. For T = 1/2, the distortion is clearly visible, both in the time and frequency domains. If
we instead choose T = 1/20, i.e., we increase the sampling frequency by a factor of ten, the distortion
is no longer visually noticeable. An acceptable level of distortion can thus be obtained by selecting the
sampling frequency high enough.
1.05.3.7 Distortion measure for energy signals
When the sampling theorem is not satisﬁed, the reconstructed signal xr(t) will differ from the original
signal xa(t). We thereby get an error e(t) according to
e(t) = xr(t) −xa(t).
(5.41)

1.05.3 Sampling of Deterministic Signals
193
0
1
2
−20π −10π
0
10π
20π
Ω [rad/s]
Xa(jΩ)
0
1
2
π
−0.5π
0
0.5ππ
ΩT [rad]
T×X(ejΩT)
T = 1/20
−2
0
2
0
0.5
1
t [s]
xa(t)
−2
0
2
0
0.5
1
t [s]
xr(t)
FIGURE 5.23
Spectra and signals in the example in Section 1.05.3.6.1 for T = 1/20.
For energy signals, a common measure of the “size” of the distortion is the ratio between the error
energy and the signal energy, whereby one computes Ee/Ex where Ex and Ee denote the energies of
xa(t) and e(t), respectively. These energies are given by
Ex =
 ∞
−∞
x2
a(t)dt = 1
2π
 ∞
−∞
|Xa( j)|2d
(5.42)
and
Ee =
 ∞
−∞
e2(t)dt = 1
2π
 ∞
−∞
|E( j)|2d,
(5.43)
where we have utilized Parseval’s formula. If we now make use of (5.41) we can rewrite (5.43) as
Ee = 1
2π
 ∞
−∞
|Xr( j) −Xa( j)|2d.
(5.44)
When the reconstruction is done with an ideal PAM, Xr( j) is given by
Xr( j) =

T X(e jT ), || ≤π/T ,
0,
|| > π/T ,
(5.45)
where
T X(e jT ) =
∞

k=−∞
Xa

j −j 2πk
T

.
(5.46)
The energy Ee can in this case be expressed as
Ee = 1
2π
 π/T
−π/T
|T X(e jT ) −Xa( j)|2d + 1
2π

||≥π/T
|Xa( j)|2d
(5.47)

194
CHAPTER 5 Sampling and Quantization
or, alternatively,
Ee = 1
2π
 π/T
−π/T


k ̸= 0
Xa

j −j 2πk
T

2
d + 1
2π

||≥π/T
|Xa( j)|2d.
(5.48)
The energies Ex and Ee are often difﬁcult to compute analytically. In such cases, one has to use
numerical computations.
1.05.3.7.1
Example of distortion measure
We consider the same signals as in the example of Section 1.05.3.6.1 with a = 1. The energies Ex and
Ee can in this case be computed as
Ex = 1
2π
 ∞
−∞

2
2 + 1
2
d
(5.49)
and
Ee = 1
2π
 π/T
−π/T

1 −e−2aT
1 −2e−aT cos (T ) + e−2aT −
2
2 + 1
2
d
+ 1
2π

||≥π/T

2
2 + 1
2
d,
(5.50)
respectively. Since the integrands are symmetric here, we can alternatively express Ex and Ee according
to
Ex = 1
π
 ∞
0

2
2 + 1
2
d
(5.51)
and
Ee = 1
π
 π/T
0

1 −e−2aT
1 −2e−aT cos (T ) + e−2aT −
2
2 + 1
2
d
+ 1
π
 ∞
π/T

2
2 + 1
2
d
(5.52)
respectively. The integral in (5.51), and the downmost one in (5.52), can easily be computed analytically
whereby one obtains Ex = 1. For the uppermost integral in (5.52), it is more difﬁcult to ﬁnd a closed-
form expression and we therefore compute it numerically. Figure 5.24 plots the ratio Ee/Ex for different
sampling frequencies fs = 1/T . (Here, Ee/Ex equals Ee since Ex = 1.) We see that the distortion
as expected reduces when the sampling frequency is increased. The example in Section 1.05.3.6.1
considered the two cases fs = 2 and fs = 20. In these cases, the distortion ﬁgures are about
6.86 × 10−3 and 5.30 × 10−6, respectively.

1.05.4 Sampling of Stochastic Processes
195
0
20
40
60
80
100
10
-10
10
-5
10
0
fs [Hz]
Ee/Ex
FIGURE 5.24
Distortion as a function of the sampling frequency in the example of Section 1.05.3.7.1.
1.05.3.8 Bandpass sampling
We have so far considered lowpass signals meaning that the signal spectrum Xa( j) is contained
in the low-frequency region  ∈[−0, 0]. When the sampling frequency satisfy fs > 2 f0 =
0/π, sampling and ideal reconstruction can theoretically be achieved. For a given sampling frequency,
fs = 1/T , the signal is in other words restricted to the frequency region  ∈[−π/T , π/T ]. This
frequency region is referred to as the ﬁrst Nyquist band (or Nyquist zone). The theory can then be
extended to bandpass signals for which the spectrum is contained in the band-pass frequency region
 ∈[−Nπ/T , −(N −1)π/T ] ∪[(N −1)π/T , Nπ/T ] which is called the Nth Nyquist band. Using
again Poisson’s summation formula (and following, e.g., the example in Section 1.05.3.2.1) it is then
readily shown that the partial spectra do not overlap which means that the signal is not aliased and
thus theoretically can be reconstructed without errors. In the reconstruction, the pulse p(t) must here
correspond to a bandpass ﬁlter instead of a lowpass ﬁlter.
1.05.4 Sampling of stochastic processes
This section extends the sampling theory from deterministic signals to wide-sense stationary (WSS)
stochastic processes.
1.05.4.1 Uniform sampling
Let Xa(t) be a continuous-time WSS stochastic process with mean m Xa, autocorrelation function
rXa Xa(τ), and power spectrum RXa Xa( j). We now form X(n) by sampling Xa(t) uniformly according
to
X(n) = Xa(nT ).
(5.53)
Graphically, we represent uniform sampling of a stochastic process as in Figure 5.25.

196
CHAPTER 5 Sampling and Quantization
FIGURE 5.25
Uniform sampling of a stochastic process.
As shown below, X(n) is also a WSS process whose mean m X, autocorrelation sequence rX X(k),
and power spectrum RX X(e jT ) are given by
m X = m Xa,
(5.54)
rX X(k) = rXa Xa(kT ),
(5.55)
and
RX X(e jT ) = 1
T
∞

p=−∞
RXa Xa

j −j 2π p
T

.
(5.56)
Equation (5.56) is the same relation as in Poisson’s summation formula for deterministic signals. One
can therefore understand that the following sampling theorem can be formulated for WSS processes.
Sampling theorem for WSS processes: If a continuous-time WSS process Xa(t) is bandlimited to
 = 0 ( f = f0), i.e., if
RXa Xa( j) = 0,
|| > 0 = 2π f0,
(5.57)
then, Xa(t) can be recovered from the samples X(n) = Xa(nT ) if
fs = 1
T > 2 f0.
(5.58)
Here, recovered means that we can form a new process Xr(t) from X(n), such that Xr(t) equals Xa(t)
in the sense that E{[Xr(t) −Xa(t)]2} = 0, where E denotes expectation.
Equations (5.54) and (5.55) follow immediately from their deﬁnitions according to
m X = E{X(n)} = E{Xa(nT )} = m Xa
(5.59)
and
rX X(k) = E{X(n)X(n −k)}
= E{Xa(nT )Xa(nT −kT )} = rXa Xa(kT ),
(5.60)
respectively. This shows that X(n) is a WSS process since both the mean and autocorrelation sequence
are independent of n. As for (5.56), it is ﬁrst noted that the autocorrelation function can be written in
terms of its inverse Fourier transform according to
rXa Xa(τ) = 1
2π
 ∞
−∞
RXa Xa( j)e jτd.
(5.61)

1.05.4 Sampling of Stochastic Processes
197
In particular, one obtains for τ = kT
rXa Xa(kT ) = 1
2π
 ∞
−∞
RXa Xa( j)e jT kd.
(5.62)
By dividing the indeﬁnite integral into an inﬁnite number of deﬁnite integrals, in the same way as we
did in Section 1.05.3.2 when deriving Poisson’s summation formula, we obtain
rXa Xa(kT ) = 1
2π
 π
−π
1
T
∞

p=−∞
RXa Xa

j −j 2π p
T

e jT kd(T ).
(5.63)
We also know that the autocorrelation sequence can be expressed in terms of its inverse Fourier transform
according to
rX X(k) = 1
2π
 ∞
−∞
RX X(e jT )e jT kd(T ).
(5.64)
Since rX X(k) = rXa Xa(kT ), and the Fourier transform is unique, the right-hand sides in the two
equations above must be equal, which ﬁnally gives us (5.56).
1.05.4.2 Reconstruction of stochastic processes
Reconstruction of stochastic processes can be performed with a PAM according to Figure 5.26. The
reconstructed process Xr(t) is given by
Xr(t) =
∞

n=−∞
X(n)p(t −nT ).
(5.65)
One can show that Xr(t) is equal to Xa(t) if the sampling theorem is fulﬁlled and p(t) is chosen as
p(t) = sin (πt/T )
πt/T
= sinc(t/T ).
(5.66)
This is in analogy with ideal reconstruction of deterministic signals. Note again that equality holds
in the sense that E[Xr(t) −Xa(t)]2 = 0. However, in practice, one cannot have strictly bandlimited
processes which introduces aliasing distortion in the sampling process. Further, errors are introduced in
the reconstruction since only approximate sinc functions can be generated. The reconstructed process
will therefore differ from the original process.
FIGURE 5.26
Sampling and reconstruction of a stochastic process using a PAM.

198
CHAPTER 5 Sampling and Quantization
An additional problem here is that Xr(t) generally will not be a WSS process which complicates the
analysis of distortion etc. To get around this dilemma, the sampling and reconstruction in (5.53) and
(5.65) are replaced with
X(n) = Xa(nT + )
(5.67)
and
Xr(t) =
∞

n=−∞
X(n)p(t −nT −),
(5.68)
respectively, where  is a stochastic variable that is uniformly distributed in the interval (0, T ). In
practice, this means that each realization of the sampling and reconstruction process incorporates a
stochastic delay . However, the sampling and reconstruction are still synchronized since they incorpo-
rate the same delay . The purpose of introducing the delay  is that the reconstructed process Xr(t)
then becomes WSS. The stochastic delay  introduced in (5.67) does not have any implications as to
stationarity and ensemble averages. That is, X(n) is still WSS and (5.54)–(5.56) still hold. Further, as
shown below, the same stochastic delay  introduced in (5.68) makes Xr(t) a WSS process with the
following mean m Xr , autocorrelation function rXr Xr (τ), and power spectrum RXr Xr ( j):
m Xr = 1
T m X P(0),
(5.69)
rXr Xr (τ) = 1
T
∞

k=−∞
rX X(k)
 ∞
−∞
p(u)p(u −τ + kT )du,
(5.70)
and
RXr Xr ( j) = 1
T |P( j)|2RX X(e jT ).
(5.71)
Moreover, as is also shown below, the reconstruction error E{[Xr(t) −Xa(t)]2} is given by
E{[Xr(t) −Xa(t)]2} =
1
2πT 2
 ∞
−∞
|P( j) −T |2RXa Xa( j)d
+
1
2πT 2
 ∞
−∞

p ̸= 0
|P( j)|2RXa Xa

j −j 2π p
T

d.
(5.72)
From this equation, we see that, if p(t) is ideal according to (5.66), in which case P( j) is an ideal
lowpass ﬁlter with passband up to π/T and gain T, then the reconstruction error becomes
E{[Xr(t) −Xa(t)]2} = 1
2π

||≥π/T
RXa Xa( j)d
+ 1
2π
 π/T
−π/T

p ̸= 0
RXa Xa

j −j 2π p
T

d,
(5.73)
which can be rewritten as
E{[Xr(t) −Xa(t)]2} = 2
π
 ∞
π/T
RXa Xa( j)d.
(5.74)

1.05.4 Sampling of Stochastic Processes
199
If, in addition, Xa(t) is bandlimited to π/T , i.e., RXa Xa( j) = 0 for || ≥π/T , it follows immediately
from (5.74) that E[Xr(t) −Xa(t)]2 = 0 in which case perfect reconstruction is achieved.
We now show (5.69)–(5.71). The mean of Xr(t) becomes
m Xr = E{Xr(t)} = E

∞

n=−∞
X(n)p(t −nT −)

=
∞

n=−∞
E {X(n)p(t −nT −)}
=
∞

n=−∞
E {X(n)} E {p(t −nT −)} ,
(5.75)
where we have utilized that X(n) and p(t −nT −) are uncorrelated. Further, we have E{X(n)} = m X
and
E {p(t −nT −)} = 1
T
 T
0
p(t −nT −)d = 1
T
 t−nT
t−nT −T
p(v)dv
(5.76)
which gives us
m Xr = 1
T m X
∞

n=−∞
 t−nT
t−nT −T
p(v)dv = 1
T m X
 ∞
−∞
p(v)dv = 1
T m X P(0).
(5.77)
We see that the mean is independent of t.
The autocorrelation function for Xr(t) becomes
rXr Xr (τ) = E {Xr(t)Xr(t −τ)}
= E

∞

n=−∞
X(n)p(t −nT −)
∞

m=−∞
X(m)p(t −τ −mT −)

= E

∞

n=−∞
∞

m=−∞
X(n)X(m)p(t −nT −)p(t −τ −mT −)

=
∞

n=−∞
∞

m=−∞
E {X(n)X(m)}
×E {p(t −nT −)p(t −τ −mT −)}
=
∞

n=−∞
∞

m=−∞
rX X(n −m)
× 1
T
 T
0
p(t −nT −)p(t −τ −mT −)d.
(5.78)

200
CHAPTER 5 Sampling and Quantization
The variable substitutions k = n −m and u = t −nT − yield
rXr Xr (τ) =
∞

n=−∞
∞

k=−∞
rX X(k) 1
T
 t−nT
t−nT −T
p(u)p(u −τ + kT )du
= 1
T
∞

k=−∞
rX X(k)
 ∞
−∞
p(u)p(u −τ + kT )du.
(5.79)
We see that also the autocorrelation function is independent of t. Hence, Xr(t) is a WSS process.
The power spectrum of Xr(t) becomes
RXr Xr (τ) =
 ∞
−∞
rXr Xr (τ)e−jτdτ
=
 ∞
−∞

1
T
∞

k=−∞
rX X(k)
 ∞
−∞
p(u)p(u −τ + kT )du
	
e−jτdτ
= 1
T
∞

k=−∞
rX X(k)
 ∞
−∞
p(u)
 ∞
−∞
p(u −τ + kT )e−jτdτ du.
(5.80)
The variable substitution v = u −τ + kT gives us
RXr Xr (τ) = 1
T
∞

k=−∞
rX X(k)
 ∞
−∞
p(u)
 ∞
−∞
p(v)e−j(u−v+kT )dv du
= 1
T
∞

k=−∞
rX X(k)e−jT k
 ∞
−∞
p(u)e−judu
 ∞
−∞
p(v)e jvdv du
= 1
T RX X(e jT )P( j)P⋆( j)
= 1
T |P( j)|2RX X(e jT ).
(5.81)
The last two equalities hold when p(t) is a real function.
Next, (5.72) is shown. To this end, the reconstruction error is ﬁrst expanded as
E

[Xr(t) −Xa(t)]2
= E

X2
r (t)

+ E

X2
a(t)

−2E {Xr(t)Xa(t)} .
(5.82)
The ﬁrst term in (5.82) can be computed as
E

X2
r (t)

= 1
2π
 ∞
−∞
RXr Xr ( j)d = 1
2π
 ∞
−∞
1
T |P( j)|2RX X(e jT )d
=
1
2πT 2
 ∞
−∞
|P( j)|2
∞

p=−∞
RXa Xa

j −j 2π p
T

d.
(5.83)

1.05.4 Sampling of Stochastic Processes
201
The second term in (5.82) is simply
E

X2
a(t)

= 1
2π
 ∞
−∞
RXa Xa( j)d.
(5.84)
In the third term of (5.82) we have
E {Xr(t)Xa(t)} = E

∞

n=−∞
Xa(nT + )p(t −nT −)Xa(t)

= E

∞

n=−∞
rXa Xa(t −nT −)p(t −nT −)

(5.85)
assuming that Xa(t) and  are uncorrelated. Since it is assumed that  is uniformly distributed on
[0, T ], it follows from (5.85) that
E {Xr(t)Xa(t)} =
∞

n=−∞
1
T
 T
0
rXa Xa(t −nT −)p(t −nT −)d.
(5.86)
The variable substitution u = t −nT − now yields
E {Xr(t)Xa(t)} = 1
T
∞

n=−∞
 t−nT
t−nT −T
rXa Xa(u)p(u)du
= 1
T
 ∞
−∞
rXa Xa(u)p(u)du.
(5.87)
Using Parseval’s relation
 ∞
−∞
x(t)y⋆(t)dt = 1
2π
 ∞
−∞
X( j)Y ⋆( j)d,
(5.88)
together with the fact that p(u) is real, i.e., p(u) = p⋆(u), (5.87) can equivalently be written as
E {Xr(t)Xa(t)} =
1
2πT
 ∞
−∞
RXa Xa( j)P⋆( j)d.
(5.89)
Making use of the facts that RXa Xa( j) = RXa Xa( −j) and P⋆( j) = P( −j), (5.89) can
equivalently be written as
E {Xr(t)Xa(t)} =
1
2πT
 ∞
−∞
RXa Xa( j)P( j)d.
(5.90)

202
CHAPTER 5 Sampling and Quantization
Inserting (5.83), (5.84), (5.89), and (5.90) into (5.82), we ﬁnally obtain
E[Xr(t) −Xa(t)]2 =
1
2πT 2
 ∞
−∞
|P( j)|2
∞

p=−∞
RXa Xa

j −j 2π p
T

d
+ 1
2π
 ∞
−∞
RXa Xa( j)d
−
1
2πT
 ∞
−∞
RXa Xa( j)P⋆( j)d
−
1
2πT
 ∞
−∞
RXa Xa( j)P( j)d
=
1
2πT 2
 ∞
−∞
|P( j) −T |2RXa Xa( j)d
+
1
2πT 2
 ∞
−∞

p̸=0
|P( j)|2RXa Xa

j −j 2π p
T

d.
(5.91)
1.05.4.2.1
Example of reconstruction error power
Let Xa(t) be the output from an analog ﬁlter H(s) with the input Xwb(t). Assume that Xwb(t) is white
noise with zero mean and power spectrum σ 2
Xwb, and that H(s) is a ﬁrst-order lowpass ﬁlter according
to
H(s) =
0
s + 0
.
(5.92)
We now form X(n) and Xr(t) through uniform sampling of Xa(t) and reconstruction with a PAM
according to (5.67) and (5.68), respectively. We assume that P( j) is ideal and given by (5.66). We
now wish to compute the average powers of Xa(t), X(n), and Xr(t). These quantities are obtained by
integrating the corresponding power spectra.
The power spectrum of Xa(t) is
RXa Xa( j) = |H( j)|2RXwb Xwb( j),
(5.93)
where
RXwb Xwb( j) = σ 2
Xwb
(5.94)
and
|H( j)|2 =
2
0
2 + 2
0
.
(5.95)

1.05.4 Sampling of Stochastic Processes
203
The power of Xa(t) therefore becomes
E

X2
a(t)

= 1
2π
 ∞
−∞
RXa Xa( j)d = 1
2π
 ∞
−∞
|H( j)|2RXwb Xwb( j)d
= 1
2π
 ∞
−∞
2
0
2 + 2
0
σ 2
Xwbd =
0σ 2
Xwb
2π
[arctan (/0)]∞
−∞
=
0σ 2
Xwb
2
.
(5.96)
The power of X(n) is the same as that of Xa(t) because
E

X2(n)

= rX X(0) = rXa Xa(0) = E

X2
a(t)

.
(5.97)
The power spectrum of Xr(t) is
RXr Xr ( j) = 1
T |P( j)|2RX X(e jT ),
(5.98)
where
RX X(e jT ) = 1
T
∞

p=−∞
RXa Xa

j −j 2π p
T

.
(5.99)
When p(t) is given by (5.66), one obtains
P( j) =
 T , || ≤π/T ,
0, || > π/T .
(5.100)
The power of Xr(t) thus becomes
E

X2
r (t)

= 1
2π
 ∞
−∞
RXr Xr ( j)d = 1
2π
 π/T
−π/T
T RX X(e jT )d
= 1
2π
 π/T
−π/T
∞

p=−∞
RXa Xa

j −j 2π p
T

d
= 1
2π
∞

p=−∞
 π/T
−π/T
RXa Xa

j −j 2π p
T

d.
(5.101)
The variable substitutions j −j2π p/T →j ﬁnally give us
E

X2
r (t)

= 1
2π
∞

p=−∞
 π/T −j2π p/T
−π/T −j2π p/T
RXa Xa( j)d
= 1
2π
 ∞
−∞
RXa Xa( j)d = E

X2
a(t)

.
(5.102)

204
CHAPTER 5 Sampling and Quantization
That is, also the power of Xr(t) is the same as that of Xa(t). We thus have
E

X2
a(t)

= E

X2(n)

= E

X2
r (t)

.
(5.103)
Finally, it is observed that the reconstruction error is
E

[Xr(t) −Xa(t)]2
= 2
π
 ∞
π/T
RXa Xa( j)d
= 2
π
 ∞
π/T
|H( j)|2RXwb Xwb( j)d
= 2
π
 ∞
π/T
2
0
2 + 2
0
σ 2
Xwbd
=
20σ 2
Xwb
π
[arctan (/0)]∞
π/T
=
20σ 2
Xwb
π
(π/2 −arctan[π/(0T )]).
(5.104)
We see that, with a ﬁxed 0, E

[Xr(t) −Xa(t)]2
→0 when T →0, i.e., we can make the recon-
struction error as small as desired by a proper selection of the sampling period T (sampling frequency
fs = 1/T ).
1.05.5 Nonuniform sampling and generalizations
This section considers the extension to nonuniform sampling [25] and generalizations. This appears in
many different forms and contexts and it is beyond the scope of this section to present a comprehensive
survey. Instead, we will concentrate on one particular application, namely M-channel time-interleaved
A/D converters which in the simplest case corresponds to an M-periodic nonuniform sampling grid
[26–28]. In this application, the sampling grid is close to a uniform grid and, thus, the obtained
nonuniform-sequence is close to the desired uniform-sampling sequence. Nevertheless, reconstruc-
tion is required to recover the uniform-sampling sequence from the nonuniform-sampling sequence.
The generalized case is an extension in the sense that the M channels experience different and general
frequency responses.
1.05.5.1 Time-interleaved ADCs
Time interleaving of multiple parallel ADCs is a technique to increase the effective sampling rate of the
overall converter. Using an M-channel time-interleaved ADC, the effective sampling rate is increased
by a factor of M. Unfortunately, the effective resolution of the individual channel converters is not
maintained in the overall converter because of channel mismatch errors. It is therefore necessary to
compensate for these errors in order to restore the resolution [29,30]. The errors can broadly be divided
into linear and nonlinear mismatch errors [30,31]. This section only deals with linear mismatch errors

1.05.5 Nonuniform Sampling and Generalizations
205
in which case the channels are modeled as linear systems, thus having speciﬁc frequency responses.
There are also static offset mismatch errors present but they are signal independent and straightforward
to compensate for, and therefore not explicitly included in the formulas to be presented in this section.
However, as noted later in Section 1.05.5.5.1, one can make use of the same frequency domain expression
as that used for relating the input and output signals.
Up to a certain resolution, one can assume that the channel frequency responses have frequency-
independent magnitude and phase delay responses, which corresponds to static gain and linear-phase
(time-skew) mismatch errors. Without gain errors, this case corresponds to nonuniform sampling and
the problem is then to recover the uniform-sampling sequence from the nonuniform-sampling sequence.
Numerous papers have addressed this problem over the last decades, see for example [29,32–37] and
references therein. However, to reach a very high resolution for high-speed conversion, one needs to
extend the channel model to general frequency responses, thus with frequency dependent magnitude
and phase delay responses [30,38]. In this case, one has to compensate for these frequency-response
mismatch errors, not only static gain and linear-phase parts which correspond to approximations of the
true responses. Recent papers have considered the more general problem [38–48] which corresponds
to the “generalizations” mentioned above.
Before proceeding, it is also noted that calibration of time-interleaved ADCs requires estimation
of and compensation for the channel frequency response mismatches. This section only discusses the
compensation which requires that accurate models of the channel frequency responses are available.
However, the estimation beneﬁts from efﬁcient compensation techniques, for example when the cali-
bration is done through simultaneous estimation and compensation by minimizing an appropriate cost
measure [42]. The compensation techniques to be discussed here can thus be used either after the channel
frequency responses have been estimated or as a part of a calibration technique.
1.05.5.2 Problem formulation
The point of departure is that we have a continuous-time signal xa(t) bandlimited to 0 < π/T which
means that the Nyquist criterion for uniform sampling with a sampling frequency of 1/T without aliasing
is fulﬁlled. That is, sampling according to x(n) = xa(nT ) does not introduce aliasing and we have in
the frequency domain
X(e jT ) = 1
T Xa( j),
T ∈[−π, π],
(5.105)
where X(e jT ) and Xa( j) denote the Fourier transforms of the sequence x(n) and continuous-time
signal xa(t), respectively. This means that xa(t) can be recovered from x(n). It is generally advisable to
make 0 correspond to some 80–90% of the overall Nyquist band as the computational complexity of
the compensation system becomes prohibitive when it approaches 100%, as exempliﬁed later in Section
1.05.5.6. However, normally this does not impose additional band limitations as a certain amount of
oversampling is required anyhow to get reasonable requirements on the analog anti-aliasing ﬁlter that
precedes the ADC, in accordance with Figure 5.39 seen later in Section 1.05.7.1, which depicts a typical
system for conversion of an analog signal into its digital representation.
In an M-channel time-interleaved ADC, without correction, we do not obtain the desired uniform
sequence x(n) but instead another sequence, say v(n), through interleaving of M subsequences, as seen

206
CHAPTER 5 Sampling and Quantization
FIGURE 5.27
M-channel time-interleaved ADC with different channel frequency responses Qn(j), n = 0, 1, . . . , M −1,
and an M-periodic time-varying compensation system with impulse response hn(k) = hn+M(k).
in Figure 5.27. This can equivalently be viewed as if v(n) is obtained by applying xa(t) to a time-varying
continuous-time system and then sampling the output at t = nT .
Utilizing (5.105), the sequence v(n) can then be expressed via the inverse Fourier transform as
v(n) = 1
2π
 0T
−0T
Qn( j)X(e jT )e jT nd(T ),
(5.106)
where Qn( j)
=
Qn+M( j) is an M-periodic time-varying system frequency response and
Qn( j), n = 0, 1, . . . , M −1, constitute the M ADC channel frequency responses. For example, with
static gain constants, gn, and static time skews, dn (given in percent of the sampling period T = 1/ fs),
the channel frequency responses are modeled as Qn( j) = gne jT dn. With gn = 1, this corresponds
to v(n) = xa(nT + dnT ) and thus a nonuniform sampling and reconstruction problem. It is also noted
that the model above holds also in the more general case when M →∞, thus also for single-channel
ADCs with a time-varying frequency response.
The problem to be solved can now be formulated as follows: Given the sequence v(n) in (5.106),
form a new sequence y(n) that is to approximate the sequence x(n) = xa(nT ) as closely as desired.
This problem is considered in Section 1.05.5.4.
1.05.5.2.1
Relaxed problem
The overall output y(n) should ideally approximate the sequence x(n) = xa(nT ). This requires that
all channel frequency responses Qn( j) are known. In practice, it is customary to determine the
ratio between the channel frequency responses and then match the channels. Typically, one chooses
the ﬁrst channel as a reference and then matches all other channels to this reference channel. In the
reconstruction, this means that we set Q0( j) = 1 and divide the remaining Qn( j) by the actual
Q0( j). In the estimation, the frequency response ratios are obtained directly so we will never have
available Qn( j) but only an estimation of Qn( j)/Q0( j). After the compensation, the overall ADC
will then experience a linear distortion, Q0( j), because we then have Y(e jT ) = Q0( j)X(e jT ).
In other words, the linear distortion after compensation is determined by the frequency response of the
reference ADC. In the compensation, this also means that the samples from the reference channel are
taken as they are and only the other channels’ samples are corrected. Finally, it is noted that, in the

1.05.5 Nonuniform Sampling and Generalizations
207
overall system where the A/D converter is to be used, the linear distortion needs to be equalized but
this problem exists for all types of A/D converters. In a larger composite system, like a communication
system, a single equalizer can be used to equalize all linear distortion at the same time, emanating from
the ﬁlters, A/D converters, communication channel, etc.
1.05.5.3 Reconstruction
Regardless whether the sequence x(n) has been obtained through uniform sampling of xa(t) or through
nonuniform sampling and/or its generalization (like in time-interleaved ADCs considered here), it is
often desired to reconstruct xa(t) from the generated sequence of samples x(n). Thus, in the generalized
case, it is desired to recover xa(t) from the sequence v(n). This can, in principle, be done in two different
ways. The ﬁrst way is to reconstruct xa(t) directly from v(n) through analog reconstruction functions.
Although it is known how to do this in principle (see e.g., [5,25,49,50]), problems arise when it comes
to practical implementations. In particular, it is very difﬁcult to practically implement analog functions
with high precision. It is therefore desired to use the second way which is to perform the reconstruction in
the digital domain, i.e., to ﬁrst recover x(n) = xa(nT ). One then needs only one conventional digital-to-
analog converter (DAC) and analog ﬁlter to obtain xa(t). Such components are much easier to implement
than components that are to approximate complicated analog functions. Recovery of x(n) is also of
interest even if xa(t) is not to be reconstructed. For example, in receivers in digital communication
systems, x(n) is the desired result. The subsequent two sections discusses two reconstruction methods
for recovering x(n) from v(n) in time-interleaved ADCs.
1.05.5.4 Reconstruction using a time-varying FIR system
A time-interleaved ADC corresponds to a time-varying system, and the compensation thus corresponds
to inverting such a system. For an M-channel system, this amounts to determining an M-periodic time-
varying discrete-time system characterized by an M-periodic impulse response, say hn(k) = hn+M(k),
or, equivalently, M time-invariant impulse responses hn(k), n = 0, 1, . . . , M −1.
Using an Nth-order M-periodic time-varying ﬁnite-length impulse response (FIR) system, the recon-
structed output sequence, y(n), is given by the convolution sum as
y(n) =
N/2

k=−N/2
v(n −k)hn(k),
(5.107)
where hn(k) = hn+M(k) denotes the M-periodic impulse response. Thus, to correct each output sample,
one needs to compute N + 1 multiplications and N additions. It is convenient here to make use of a
noncausal system, which means that the impulse response is assumed to be centered around k = 0 which
corresponds to zero delay. A causal system is then obtained by introducing a delay of N/2 samples into
the system. It is noted that we have assumed here that the order N of the system is even, to keep the
notation simple. Odd-order systems can be used as well after some minor appropriate modiﬁcations.
To determine the impulse response coefﬁcients, it is convenient to express the output y(n) in the
time-frequency domain [27], instead of the time-domain given in (5.107). This will be done here in
terms of an M-periodic time-frequency function An( j) = An+M( j), or, equivalently, M frequency

208
CHAPTER 5 Sampling and Quantization
functions An( j), n = 0, 1, . . . , M −1. To this end, we insert (5.106) into (5.107), and interchange
the summation and integration, and obtain
y(n) = 1
2π
 0T
−0T
An( j)X(e jT )e jT nd(T ),
(5.108)
where
An( j) =
N/2

k=−N/2
hn(k)Qn−k( j)e−jT k.
(5.109)
Further, we can write the desired sequence x(n) in terms of the inverse Fourier transform according to
x(n) = 1
2π
 0T
−0T
X(e jT )e jT nd(T ).
(5.110)
Comparing (5.108) and (5.110), it is seen that perfect reconstruction (PR) is obtained if
An( j) = 1,
 ∈[−0, 0]
(5.111)
for n = 0, 1, . . . , M −1, because, then, y(n) = x(n) for all n as An( j) is M-periodic. Thus, if
all An( j) = 1, we get the desired result x(n) = xa(nT ). The problem is thus to determine the
M impulse responses hn(k), n = 0, 1, . . . , M −1, so that An( j) approximate unity. If the channel
frequency responses Qn( j) are known, this can be done optimally in e.g., least-squares or minimax
senses. In practice, one does not explicitly estimate Qn( j) but instead Qn( j)/Q0( j) if Q0( j)
is the reference channel, in accordance with the discussion earlier in Section 1.05.5.2.1. The ratios
Qn( j)/Q0( j) are then used in the design which corresponds to selecting h0(k) = δ(k) (unit
impulse). It thus sufﬁces to determine the M −1 impulse responses hn(k) for n = 1, 2, . . . , M −1,
so that the corresponding An( j) approximate unity. After compensation with these ﬁlters, the overall
ADC will experience the actual linear distortion Q0( j).
1.05.5.5 Error metrics
A common metric of ADCs is the signal-to-noise ratio (SNR). In this case, the aim is to minimize
An( j) −1 in the least-squares sense which means that the quantities Pn given by
Pn = 1
2π
 0T
−0T
|An( j) −1|2d(T )
(5.112)
are minimized, either separately or simultaneously (in which case e.g., the mean of Pn is minimized).
The minimization of Pn corresponds to the minimization of the expected error E{[y(n)−x(n)]2} when
the input signal is a stochastic process with a constant power spectrum in the region [−0, 0] [27].

1.05.5 Nonuniform Sampling and Generalizations
209
Another common metric of ADCs is the spurious-free dynamic range (SFDR). In this case, it is
appropriate to write the input-output relation in the frequency domain in terms of a distortion function
V0(e jT ) and M −1 aliasing functions Vm(e jT ), m = 1, 2, . . . , M −1. This is similar to frequency-
domain input-output relations for multirate ﬁlter banks [24]. One can then write the output Fourier
transform as
Y(e jT ) = V0(e jT )X(e jT ) +
M−1

m=1
Vm(e jT )X(e j(T −2πm/M)),
(5.113)
where
V0(e jT ) = 1
M
M−1

n=0
Bn(e jT )
(5.114)
and
Vm(e jT ) = 1
M
M−1

n=0
e−j2πmn/M Bn(e j(T −2πm/M)),
(5.115)
with Bn(e jT ) being the 2π-periodic extensions of An( j) in (5.109). That is, Bn(e jT ) = An( j)
for −π ≤T ≤π. The distortion function corresponds to a regular frequency response seen from
the input to the output, whereas the aliasing functions give the size of the aliased (frequency shifted)
versions of the input. Speciﬁcally, a frequency component at  = 0 is affected by the modulus
and phase of the distortion frequency response V0(e j0T ), and we also get aliased versions at the
“digital frequencies” mT = 0T + 2πm/M, m = 1, 2, . . . , M −1, with amplitudes determined by
Vm(e j(0T +2πm/M)). Note also that, for real signals which always have frequency components at 0T
and −0T simultaneously, aliased versions also appear at −0T + 2πm/M, m = 1, 2, . . . , M −1.
Ideally, the distortion function should equal unity whereas the aliasing functions should be zero, in
which case we have PR, because then we obtain Y(e jT ) = X(e jT ) and thus y(n) = x(n). In practice,
we can only approximate PR. Typically, one then aims at minimizing two tolerances, say δd and δa,
between unity and zero, respectively. One can then solve the problem in several slightly different ways.
One typical way is to minimize δ subject to the constraints
|V0(e jT ) −1| ≤δ
T ∈[−0T , 0T ]
(5.116)
and
|Vm(e jT )| ≤δ(δa/δd),
T ∈mT ,
(5.117)
for m = 1, 2, . . . , M −1, where
mT =

−0T + 2πm
M , 0T + 2πm
M

.
(5.118)
Clearly, |V0(e jT )| ≤δd and |Vm(e jT )| ≤δa if δ ≤δd. It is noted that δd and δa are related through
|δd| ≤(M −1)|δa| + |An( j) −1|min,
(5.119)

210
CHAPTER 5 Sampling and Quantization
which means that it is usually sufﬁcient to ensure that the aliasing is suppressed as this implies a small
distortion as well. Speciﬁcally, for the relaxed problem, (5.119) reduces to |δd| ≤(M −1)|δa|, provided
|V0(e jT ) −1| in (5.116) is replaced with |V0(e jT ) −Bq(e jT )|, q denoting the reference channel.
The relation above is a consequence of the following equation which can be derived from (5.115):
M−1

m=0
e j2πmq/MVm(e j(T +2πm/M)) = Bq(e jT ).
(5.120)
In a practical system, we cannot minimize δ directly though as we do not have access to the distortion
and aliasing functions. Instead, δ is reduced indirectly to an acceptable level through some estimation
and correction scheme.
1.05.5.5.1
DC offset
It is ﬁnally noted in this section that the representation in (5.113) can be used for DC offset mismatch
as well, if we let X represent a DC signal, because it can be seen as a DC input that is time-interleaved
and ampliﬁed differently in the different channels. In accordance with the discussion on aliasing above,
it is seen that offset mismatches give rise to tones at mT = 2πm/M, m = 0, 1, . . . , M −1, with
amplitudes determined by frequency independent Vm, as Bn in this case are frequency independent
constants equaling the channel offset values. Speciﬁcally, with all Bn being equal, say, Bn = c, we
obtain from (5.115) that V0 = c and Vm = 0 for m = 1, 2, . . . , M −1, as we in this case have a regular
DC offset.
1.05.5.6 Oversampling
A good approximation can generally only be achieved if the input signal is bandlimited in accordance
with the Nyquist theorem. It is generally advisable to use at least 10% oversampling as the complexity
of the reconstructor otherwise may become prohibitive. However, if low complexity is not crucial, the
amount of oversampling can be reduced. The increase of complexity with decreasing oversampling is
illustrated in Figures 5.28 and 5.29 which plot the system (ﬁlter) order versus time skew in a two-channel
time-interleaved ADCs for an reconstruction error [P1 in (5.112)] of−60, −80, and −100 dB. It is seen
that the order is roughly doubled when the don’t-care band between the upper band edge and the Nyquist
frequency fs/2 is halved, (which occurs when we go from 80% to 90%, etc.). However, it is also seen
that for small time skews and moderate reconstruction errors, one may increase the bandwidth as the
order in such cases is relatively low. Recall that, for a reconstruction system of order N, in accordance
with the convolution sum in (5.107), one needs to compute N + 1 multiplications and N additions to
correct each output sample.
1.05.5.7 Reconstruction based on least-squares design
From the previous section, we see that the overall compensation system can be designed by minimizing
the quantities Pn, n = 0, 1, . . . , M −1, in (5.112). The quantities Pn can be minimized separately or

1.05.5 Nonuniform Sampling and Generalizations
211
0
5
10
15
20
25
30
0.01
0.1
0.2
0.3
0.4
Two−channel TIADC, bandwidth = 80%
Time Skew
Filter Order (60, 80, 100 dB)
0
10
20
30
40
50
60
0.01
0.1
0.2
0.3
0.4
Two−channel TIADC, bandwidth = 90%
Time Skew
Filter Order (60, 80, 100 dB)
FIGURE 5.28
System (ﬁlter) order versus time skew in percent of the sampling period T = 1/fs in a two-channel time-
interleaved ADC for 80% and 90% of the Nyquist band.
simultaneously. In the former case, the corresponding M impulse responses hn(k), n = 0, 1, . . . , M −1,
can be computed separately as [45]
hn = −0.5S−1
n cn,
(5.121)
with cn,k, k = −N, −N + 1, . . . , N, being
cn,k = −1
π
 0T
−0T
|Qn−k( j)|
× cos

T k −arg{Qn−k( j)}

d(T ),
(5.122)

212
CHAPTER 5 Sampling and Quantization
0
20
40
60
80
100
0.01
0.1
0.2
0.3
0.4
Two−channel TIADC, bandwidth = 95%
Time Skew
Filter Order (60, 80, 100 dB)
0
50
100
150
200
0.01
0.1
0.2
0.3
0.4
Two−channel TIADC, bandwidth = 97.5%
Time Skew
Filter Order (60, 80, 100 dB)
FIGURE 5.29
System (ﬁlter) order versus time skew in percent of the sampling period T = 1/fs in a two-channel time-
interleaved ADC for 95% and 97.5% of the Nyquist band.
Sn being (N + 1) × (N + 1) symmetric and positive deﬁnite matrices with entries sn,kp, k, p =
−N/2, −N/2 + 1, . . . , N/2 given by
sn,kp = 1
2π
 0T
−0T
|Qn−k( j)∥Qn−p( j)|
× cos

T (p −k) + arg{Qn−k( j)} −arg{Qn−p( j)}

d(T )
(5.123)
and the constant C being C = 0T /π. In order to compute hn in (5.121), it is necessary to compute
the integrals in (5.122) and (5.123), which generally have to be computed numerically.
Above, the overall compensation system is designed by determining the M ﬁlter impulse responses
hn(k) through M separate matrix inversions (M −1 in the practical relaxed case), where the size of

1.05.5 Nonuniform Sampling and Generalizations
213
the matrices is (N + 1) × (N + 1), where N + 1 is the ﬁlter impulse response length [45,47]. In
the alternative case, corresponding to simultaneous design, the impulse responses hn(k) are designed
simultaneously. This was done in [39], which poses the design problem in terms of M synthesis ﬁlters
that are designed simultaneously by inverting one matrix of size M(N + 1) × M(N + 1). Whereas the
separate-design technique yields optimum ﬁlters with respect to the reconstruction error in the band of
interest, the technique in [39] may give somewhat better results as to distortion and aliasing, because it
includes those quantities in the overall error cost measure. The main advantage of the separate-design
approach is that the design problem comprises M smaller subproblems instead of one large problem. It
may therefore be more numerically robust and simpler to design and implement.
In addition to the above mentioned design techniques, [39,45,47], there are a few other related tech-
niques [38,40,41,47]. In [38], separately designed synthesis ﬁlters were obtained through windowing
techniques, which, however, are known to result in suboptimum ﬁlters. A somewhat different compensa-
tion technique, that also utilizes separately designed ﬁlters, was proposed in [40,41], but that technique
needs additional cosine and sine modulators which increases the implementation cost of the compen-
sation system. Finally, it is noted that one may alternatively use numerical optimization in accordance
with [27], but this is costly and therefore less suitable for on-line design applications.
For all methods described above, new values of the impulse responses hn(k) must be computed when-
ever the frequency responses Qn( j) are changed, which occur in a practical time-interleaved ADC
due to temperature variations etc. This requires recomputation of the ﬁlter coefﬁcients using windowing
techniques (including inverse Fourier transforms), matrix inversions, or numerical optimization. This
may be acceptable for off-line design which can be adopted in, e.g., spectrum analysis applications.
In real-time low-power applications, on the other hand, these on-line redesign procedures can become
too costly and time consuming to implement. The next section discusses a compensation structure for
which on-line design is not needed. This structure was introduced in [46]. A similar structure has been
reported in [48].
1.05.5.8 Reconstruction using a polynomial based approach
As discussed in the previous section, the reconstruction can in principle be done through the time
varying system hn(k). For an Nth-order reconstructor, each system hn(k) has then N +1 free parameters
(multipliers) that must be estimated and implemented, and thus (M −1)(N + 1) in total for a practical
M-channel system (in the relaxed case). This becomes difﬁcult from the estimation point of view and
expensive to implement except for small values of M.
Depending on the behavior of the channel responses Qn( j), the number of free parameters may
be reduced by modeling them as Pth-order polynomials in j according to
Qn( j) = ε(0)
n
⎡
⎣1 +
P

p=1
ε(p)
n ( jT )p
⎤
⎦,
(5.124)
where ε(0)
n
is a gain constant (ideally equal to unity) and the remaining ε(p)
n
are constants (ideally equal
to zero) used for modeling the frequency dependency. This does not impose a fundamental restriction
as it is known that any frequency response can be approximated as closely as desired by a properly
chosen polynomial and polynomial order. For example, when the system has frequency independent

214
CHAPTER 5 Sampling and Quantization
time skews, dnT , Qn( j) can be written as
Qn( j) = ε(0)
n e jT dn ≈ε(0)
n [1 +
P

p=1
d p
n ( jT )p/p!],
(5.125)
from which we obtain ε(p)
n
= d p
n /p!. In this case it sufﬁces to estimate two parameters per channel
(three, including offset errors).
A general structure for the reconstruction based on the model in (5.124) is seen in Figure 5.30.
This structure makes use of K sets of ﬁxed subﬁlters G p(z), p = 1, 2, . . . , P, that approximate the
pth-order differentiators in (5.124). Figure 5.30 shows a noncausal ﬁlter structure for convenience. The
corresponding causal structure is obtained by propagating D delay elements into each vertical branch
and replacing ε(p)
n
with ε(p)
n−D, where D is the (possibly approximate) delay of the subﬁlters, which can
be either linear-phase ﬁnite-length impulse response (FIR) subﬁlters, or approximately linear-phase
FIR or inﬁnite-length impulse response (IIR) subﬁlters. Using Nth-order linear-phase FIR subﬁlters,
FIGURE 5.30
Reconstruction based on the model in (5.124).

1.05.5 Nonuniform Sampling and Generalizations
215
D = N/2. The remaining errors in the output yK (n) of the K-stage structure in Figure 5.30 are of
orders (0T ε(p)
n )K+1. This means that one can reach any desired error level by increasing the number
of stages, provided |0T ε(p)
n | is small which is the case in practical time-interleaved ADCs. The error
will in practice decrease with K to a level that is determined by the channel model accuracy, i.e., by the
distances between Qn( j) and the actual frequency responses.
1.05.5.9 Performance discussion
The performance of the overall compensated ADC is naturally bounded by the performance of the
individual converters. In this section, we only consider the compensation of the linear mismatch errors.
Ultimately, the overall ADC’s performance is therefore determined by the nonlinear distortion of the
individual converters. In practice, the compensation structure presented above has virtually no effect on
the nonlinear distortion. That is, the undesired frequency components caused by nonlinear distortion pass
the compensation structure more or less unaffected. This is because the nonlinear distortion components,
which are small themselves, passes a time-varying system that approximates a pure delay, provided
ε(p)
n , p = 1, 2, . . . , P, are small which is the case in practice. This will be illustrated below in an
example. It should be noted though that the time interleaving introduces nonlinear-distortion frequency
components that are not present in the individual converters, but the total nonlinear-distortion power is
virtually the same [30,31].
1.05.5.9.1
Example of frequency response mismatch correction
To illustrate the theory above, an example is provided. It is assumed here that the number of channels
is M = 4, the bandwidth is 0T = 0.9π, and the four channel frequency responses Qn( j), n =
0, 1, 2, 3, are modeled as
Qn( j) =
e jT dn
1 + j 
c (1 + n)
=
e jT dn
1 + jT
fs
2π fc (1 + n)
,
(5.126)
where fs = 1/T denotes the sampling frequency whereas fc and c denote the 3-dB cutoff frequency
and angular frequency, respectively, and dn and n correspond to analog matching errors. Here, we
have chosen fs/ fc = 1/2, which corresponds to a bandwidth of about two times the overall Nyquist
band or, equivalently, eight times the individual channel ADC Nyquist band. Further, we have set
d0 = −0.02, d1 = 0.02, d2 = −0.01, d3 = 0.01, 0 = −0.005, 1 = 0.005, 2 = −0.004, and
3 = 0.004. The term in the numerator in (5.126) models static aperture delay mismatches whereas the
denominatormodelsthesample-and-holdcircuit[30,31,38]. Thevaluesfortheseparameterscorrespond
to time and bandwidth mismatch spurs of some −30 dB and −60 dB, respectively, which is realistic
at least for well-matched ADCs operating in their lower Nyquist bands. Hence, for a 10-bit resolution,
one may ignore the bandwidth mismatch.
We assume that the zeroth channel is the reference channel. Consequently, we need to ﬁnd the
polynomial models of the rational functions Qn( j)/Q0( j), n = 0, 1, 2, 3. Including terms up to
third order, we obtain from (5.126)
Qn( j)
Q0( j) = 1 + jT ε(1)
n
+ ( jT )2ε(2)
n
+ · · · + ( jT )3ε(3)
n ,
(5.127)

216
CHAPTER 5 Sampling and Quantization
where
ε(1)
n
= dn −d0 + 0 −n,
(5.128)
ε(2)
n
= (dn −d0)2
2
−n(0 −n) −(dn −d0)(0 −n)
(5.129)
and
ε(3)
n
= (dn −d0)3
6
+ 2
n(0 −n) −(dn −d0)n(0 −n) + (dn −d0)2
2
(0 −n).
(5.130)
Here, we can compute the different parameters ε(p)
n
needed for the compensation, as we have assumed
that we know the channel frequency responses. In practice, these parameters are obtained through some
estimation technique for this purpose. It is noted that ε(1)
0
= ε(2)
0
= ε(3)
0
= 0 as we have assumed
that the zeroth channel is the reference, i.e., the samples from this channel are taken directly without
compensation.
We have applied a 16-bit multi-sine input to this four-channel system. Without compensation, the
output spectrum becomes as seen in Figure 5.31 which reveals large spurs. Figure 5.31 also plots the
output spectra after compensation using one, two, and three stages, respectively. It is seen that, after
three stages, the errors have been reduced to a level that corresponds to some 16 bits. This reveals
that any desired error level can be reached in the frequency band  ∈[−0, 0] by increasing the
number of stages, provided the model order is high enough. In this example, the third-order model
in (5.127) is required to reach 16 bits. Using instead a second-order model, we end up with spurs of
some −80 dB for K ≥2, which corresponds to 13–14 bits. Hence, the approximation of (5.126) by
a numerator polynomial of a certain order imposes a performance bound that cannot be surpassed by
increasing K alone. In other words, to reach the desired result, one must select both the model order
and number of stages appropriately. It is also noted that one stage sufﬁces to reach some −60 dB, thus
about 10 bits.
Finally, to illustrate that the compensation has virtually no effect on nonlinear distortion, as discussed
previously, we have also applied a two-tone input including the distortion term 0.0001x2(n) and plotted
the output spectra in Figure 5.32 for the uncompensated system and compensated system with K = 3.
From the ﬁgure, it is seen that the two large unwanted spurs (of some −40 dB) caused by the interleaving
have been suppressed whereas the smaller nonlinear-distortion spurs (of some −90 dB to −85 dB) are
practically unaffected by the compensation.
1.05.6 Quantization
In ideal linear systems, all signal values are represented with real numbers and all computations are
done with inﬁnite precision. In a practical implementation, one can however only use ﬁnite preci-
sion both for the input and output signal values and the internal signal values. Hence, instead of
the ideal system in Figure 5.33a, we will in practice implement the one shown in Figure 5.33b. The
boxes Q (with additional indices) represent quantizers. The function of a quantizer Q depends on the

1.05.6 Quantization
217
−100
−50
0
0
0.2
0.4
0.6
0.8
1
Amplitude [dB]
Without compensation
−100
−50
0
0
0.2
0.4
0.6
0.8
1
Normalized frequency
Amplitude [dB]
One−stage compensation (K=1)
−100
−50
0
0
0.2
0.4
0.6
0.8
1
Amplitude [dB]
Two−stage compensation (K=2)
−100
−50
0
0
0.2
0.4
0.6
0.8
1
Normalized frequency
Amplitude [dB]
Three−stage compensation (K=3)
FIGURE 5.31
Spectra in the example of frequency response mismatch correction.
type of arithmetic that is to be used in the implementation. In general-purpose computers, one uses
ﬂoating-point arithmetic. However, for many ﬁxed functions (like ﬁlters), the large number range pro-
vided by ﬂoating-point arithmetic is not needed and thus becomes unnecessarily expensive to implement.
In very-large-scale integration (VLSI) implementations, one therefore usually employs ﬁxed-point arith-
meticinordertominimizetheimplementationcost.Wehenceforthunderstandthatﬁxed-pointarithmetic
is used. One may further choose between several different formats like sign and magnitude, one’s com-
plement, and two’s complement, etc. Here, we consider two’s complement format which is commonly
used in VLSI implementations.

218
CHAPTER 5 Sampling and Quantization
−100
−50
0
0
0.2
0.4
0.6
0.8
1
Amplitude [dB]
Without compensation
−100
−50
0
0
0.2
0.4
0.6
0.8
1
Amplitude [dB]
Three−stage compensation (K=3)
Normalized frequency
FIGURE 5.32
Spectra in the example of frequency response mismatch correction with the presence of a nonlinear distortion
term 0.0001x2(n).
(a)
(b)
FIGURE 5.33
(a) Ideal linear system. (b) Actual nonlinear system in a practical implementation.
Using two’s complement arithmetic, a quantized number xQ, within the number range −Xm ≤xQ ≤
Xm(1 −Q), is represented with the B-bit fractional binary number xB given by
xB = −x0 +
B−1

i=1
2−i xi,
(5.131)

1.05.6 Quantization
219
where xi, i = 0, 1, . . . , B −1, are either zero or one and where xB is related to xQ according to
xQ = XmxB.
(5.132)
The quantity x0 is referred to as the sign bit which is zero for positive numbers (including zero) and
one for negative numbers. Further, it is seen that −1 ≤xB ≤1−Q. Hence, the quantity Xm is a scaling
constant that is used for relating numbers within the range −1 ≤xB ≤1 −Q to numbers within the
range −Xm ≤xQ ≤Xm(1 −Q). For example, in the context of A/D conversion, we can view Xm as
the full-scale amplitude of the A/D converter. Another role of Xm is to enable representation of numbers
larger than one in magnitude. In this case, Xm = 2P, for some integer P, which can be interpreted as if
the binary point has been moved to the right. Often, this scaling constant is not explicitly implemented
but instead implicit.
The quantizers that inevitably are present introduce a number of errors that need to be analyzed
when designing and implementing digital systems. The errors can be divided into four broad categories
as discussed below. It is beyond the scope of this section to discuss all of these issues in detail. The
subsequent subsections concentrate on quantization errors in the A/D conversion process and the related
round-off errors in digital systems. For a comprehensive treatment of quantization effects, we refer in
particular to the digital ﬁlter literature [51–53].
•
Quantization errors in the A/D conversion—A/D conversion consists of sampling followed by quan-
tization of the sample values. Thereby, a quantization error is introduced in the sampled signal.
•
Overﬂow errors—Overﬂow errors occur when the available number range is exceeded. This may
give rise to large sustaining errors referred to as parasitic large-scale oscillations. Digital system must
be designed so that such errors are suppressed once the disturbance that incurred them vanishes.
This is done by scaling the signal levels appropriately, not only at inputs and outputs but also inside
the systems.
•
Round-off errors—In digital systems, the results of arithmetic operations normally need be rounded
(truncated). This gives rise to round-off errors at the output of the systems.
•
Coefﬁcient errors—The coefﬁcients of a digital system must be represented with ﬁnite precision.
This give rise to a static error in the corresponding transfer function, frequency response, etc.
1.05.6.1 Quantization errors in A/D conversion
A/D conversion consists of sampling followed by quantization according to Figure 5.34. In the sampling
process, the samples x(n) are obtained as x(n) = xa(nT ). Quantization means that each sample is
represented in binary form with ﬁnite wordlength. This gives rise to a quantization error, i.e., an error
in the representation of xa(nT ), according to
e(n) = xQ(n) −x(n).
(5.133)
This is also illustrated in Figure 5.34. The size of the quantization error depends on the types of
binary representation and quantizer that are used. It is common to use ﬁxed-point two’s complement
representation and uniform quantization in which case x(n) is rounded to the nearest number in the
number representation.

220
CHAPTER 5 Sampling and Quantization
FIGURE 5.34
A/D conversion—sampling and quantization.
Ifweassumethattwo’scomplementrepresentationisused,andthatthenumberrangeforthequantizer
is −Xm ≤xQ ≤Xm(1 −Q), where Q is the quantization step, xQ(n) can for a B-bit quantizer be
written as
xQ(n) = XmxB(n)
(5.134)
with
xB(n) = −x0(n) +
B−1

i=1
2−i xi(n),
(5.135)
where xi(n), i = 0, 1, . . . , B −1, are either zero or one. When the samples xQ(n) are represented
with B bits, they can take on 2B different values, as illustrated in Figure 5.35 for a uniform quantizer,
assuming for simplicity that Xm = 1. The quantization step Q is given by
Q = Xm2−(B−1).
(5.136)
Quantization is a nonlinear operation which in general is much more difﬁcult to analyze than a linear
operation. To analyze the effects of the quantization errors, one therefore makes use of a linear model
of the quantization according to Figure 5.36. In the linear model, xQ(n) is obtained by adding x(n) and
an error sequence e(n) according to
xQ(n) = x(n) + e(n),
(5.137)
where it is assumed that x(n) and e(n) are uncorrelated. Further, e(n) is assumed to be uniformly
distributed white noise. That is, one understands that the samples e(n) are uncorrelated and the errors
that e(n) can take on occur with equal probability. This model is appropriate when xa(t) varies irregularly
since the quantization errors then vary more or less randomly and independently of previous errors.

1.05.6 Quantization
221
(a)
(b)
FIGURE 5.35
(a) Uniform quantization with Xm = 1 and B = 3 bits. (b) Quantization error.
FIGURE 5.36
Linear model of quantization.
Using uniform quantization, we have
|e(n)| ≤Q/2,
(5.138)
where Q is the quantization step according to (5.136). Each sample e(n) is therefore assumed to have
the probability density function
f (e) =
1/Q, |e| ≤Q/2,
0,
|e| > Q/2.
(5.139)
The mean value and variance of e(n) become
me = 0
(5.140)
and
σ 2
e = Q2
12 = X2
m
2−2(B−1)
12
,
(5.141)
respectively.
A measure that is commonly used for comparing the (useful) signal power and noise power is the
SNR which is deﬁned as
SNR = 10 × log10
Signal power
Noise power

.
(5.142)

222
CHAPTER 5 Sampling and Quantization
Since the noise power is here the variance of e(n) [due to me = 0], i.e., σ 2
e in (5.141), we can write the
SNR as
SNR = 10 × log10

Signal power

−10 × log10

X2
m
2−2(B−1)
12
	
,
(5.143)
which alternatively can be written as
SNR = 10 × log10

Signal power

+ 4.77 + 6.02B −20 × log10

Xm

.
(5.144)
We note that the SNR increases by 6 dB for each bit that we add. For a full-scale sinusoidal signal with
the amplitude Xm, the signal power is X2
m/2 which corresponds to 20 × log10 (Xm) −3.01 dB. In this
case, the SNR is
SNR = 1.76 + 6.02B.
(5.145)
In for example a CD player, B = 16 bits. The SNR becomes in this case 98.1 dB.
1.05.6.2 Round-off errors
This section considers round-off errors (or round-off noise) in digital systems. The round-off noise
is computed under the assumption that the quantization errors can be modeled as white noise, which
is appropriate in many practical situations. It is further assumed that the digital system operate under
normal conditions which means that overﬂows or other abnormal disturbances do not occur. In other
words, we assume a linear model of the system incorporating one input signal and one or several noise
sources. The problem is then to determine the round-off noise power and SNR at the output of the system.
To this end, one makes use of the theory of linear systems excited with white stochastic processes. We
will explain the principle by means of examples, where we assume Xm = 1 for simplicity, but without
loss of generality.
1.05.6.2.1
Quantization and round-off noise example 1
Consider a system represented by the structure in Figure 5.37a. First, sampling and quantization take
place, with the quantization step Q1, i.e., A/D conversion with B1 bits where Q1 = 2−(B1−1). The
output from the quantization is xQ(n) which subsequently is ﬁltered. The ﬁlter is realized by a ﬁrst-
order direct form recursive IIR ﬁlter structure. In an implementation of this ﬁlter, the result of the
arithmetic operations must at some point(s) inside the ﬁlter be rounded or truncated. Otherwise, the
wordlength would increase in each iteration of the ﬁlter algorithm since it is recursive. Here, only
one quantizer with the quantization step Q2 = 2−(B2−1) is used and it is placed after the upper-most
addition3.
We now wish to analyze the effects at the output of the ﬁlter caused by quantization errors. To be
able to separate the origins of the errors we distinguish between the errors emanating from the A/D
conversion, here referred to as quantization noise, and the errors coming from the quantizations of the
3The results of the remaining operations must thereby in this case be represented with more bits but this is only one option.
For example, one may instead introduce quantizers after each operation. This will increase the noise at the output of the ﬁlter
but, on the other hand, one may then use a shorter wordlength inside the ﬁlter. It is generally not obvious which alternative
one should use in order to arrive at the cheapest implementation. It has to be investigated in each speciﬁc case.

1.05.6 Quantization
223
(a)
(b)
(c)
(d)
(e)
FIGURE 5.37
Signal-ﬂow graphs used for computation of the round-off noise.
arithmetic operations inside the ﬁlter, usually referred to as round-off noise. To be able to analyze the
different errors, we make use of a linear model of the quantizations which results in the structure shown
in Figure 5.37b. The error sequences e1(n) and e2(n) are assumed to be white noise with zero mean and
variances σ 2
e1 and σ 2
e2, respectively4. These error sequences are often called noise sources.
4When quantizing already quantized numbers, as is the case at the output of Q2, one should actually use a discrete rectangular
probability function. The difference is however negligible except when only a few bits are discarded.

224
CHAPTER 5 Sampling and Quantization
Since we use linear models, we can regard the input and noise sources one at a time according to
Figure 5.37c–e. The total output round-off noise variance is then computed as the sum
σ 2
y = σ 2
y1 + σ 2
y2,
(5.146)
where
σ 2
y1 = σ 2
e1
∞

n=−∞
h2
1(n)
(5.147)
is the output quantization noise variance and
σ 2
y2 = σ 2
e2
∞

n=−∞
h2
2(n)
(5.148)
is the output round-off noise variance. Here, h1(n) and h2(n) are the impulse responses as seen from
the respective noise source to the output. They can be obtained through inverse transformation of the
corresponding transfer functions H1(z) and H2(z).
Here, the transfer function from e1(n) to y(n) is the same as that from x(n) to y(n) and given by
H1(z) = a0 + a1z−1
1 + b1z−1 ,
(5.149)
whereas the transfer function from e2(n) to y(n) is
H2(z) =
1
1 + b1z−1 .
(5.150)
In the causal-ﬁlter case, the region of convergence is |z| > b1 for both of these transfer functions. We
then get
h1(n) = a0( −b1)nu(n) + a1( −b1)n−1u(n −1)
(5.151)
and
h2(n) = ( −b1)nu(n),
(5.152)
respectively, where u(n) denotes the unit-step sequence. This gives us the corresponding noise gains
∞

n=−∞
h2
1(n) = a2
0 +

a0 −a1
b1
2 ∞

n=1
b2n
1 = a2
0 + a2
1 −2a0a1b1
1 −b2
1
(5.153)
and
∞

n=−∞
h2
2(n) =
∞

n=0
b2n
1 =
1
1 −b2
1
,
(5.154)
respectively.

1.05.6 Quantization
225
If the quantizations Q1 and Q2 correspond to B1 and B2 bits, respectively, the variances σ 2
e1 and σ 2
e2
become
σ 2
e1 = Q2
1
12 ,
σ 2
e2 = Q2
2
12 .
(5.155)
This gives us the variances σ 2
y1 and σ 2
y2 according to
σ 2
y1 = σ 2
e1
∞

n=−∞
h2
1(n) = Q2
1
12
a2
0 + a2
1 −2a0a1b1
1 −b2
1
(5.156)
and
σ 2
y2 = σ 2
e2
∞

n=−∞
h2
2(n) = Q2
2
12
1
1 −b2
1
,
(5.157)
respectively. Finally, we obtain
σ 2
y = σ 2
y1 + σ 2
y2 = Q2
1
12
a2
0 + a2
1 −2a0a1b1
1 −b2
1
+ Q2
2
12
1
1 −b2
1
,
(5.158)
which can be rewritten as
σ 2
y = Q2
1
12
1
1 −b2
1

a2
0 + a2
1 −2a0a1b1 + Q2
2
Q2
1
	
.
(5.159)
From the equation above, we see that the contribution from the quantizer Q2 can be neglected if
we choose Q2 sufﬁciently small as compared to Q1. That is, by increasing the number of bits within
the ﬁlter, the round-off noise can be made negligible compared to the quantization noise. This shows
that digital systems can be implemented with as high resolution as desired. However, an increased
internal wordlength also results in a more expensive implementation. One should therefore not use
longer wordlengths than necessary to meet the speciﬁcation at hand, typically given in terms of an SNR
required. It should also be noted that the digital signal that in the end is to be D/A converted must be
quantized to B1 bits if we want to use the same number of bits as that used in the A/D converter. This
will give rise to additional round-off noise, equally large as that in the A/D conversion, which in turn
degrades the resolution by half a bit. One can alleviate this problem by using oversampling techniques.
1.05.6.2.2
Quantization and round-off noise example 2
In this example, we compute the round-off noise at the output of a linear-phase direct-form FIR ﬁlter
structure and the increase in SNR that is achieved by using so called L2-norm scaling instead of safe
scaling.
Assuming that quantization takes place after each multiplication, the linear model of the whole ﬁlter
is as shown in Figure 5.38. Apparently, the errors pass directly to the output, i.e., the transfer function
from each noise source to the ﬁlter’s output is equal to unity. Assuming that the errors are uncorrelated
white noise sources, the total output noise variance equals the sum of the individual variances. As

226
CHAPTER 5 Sampling and Quantization
FIGURE 5.38
Linear model of a linear-phase FIR ﬁlter with quantizers.
a consequence, if all noise sources have the same variance, say σ 2
e , the total output variance for an
Nth-order ﬁlter, N being even, becomes
σ 2
y = (N/2 + 1)σ 2
e .
(5.160)
With N = 26, as in this example, we thus have
σ 2
y = 14σ 2
e .
(5.161)
This corresponds to a degradation of almost two bits as compared to the case where one quantizer
is placed at the output. That is, the outputs of quantizers placed inside the ﬁlter need to be two bits
longer than the output of a single quantizer located at the output, in order to achieve (roughly) the same
round-off noise. On the other hand, this means that we can use adders with shorter wordlengths which
reduces the implementation cost. The alternative of using one quantizer at the output requires the use
of full-length adders inside the ﬁlter which increases the implementation cost.
Next, we consider the increase in SNR that is achieved by using so called L2-norm scaling instead of
safe scaling in the ﬁlter. Scaling of signal levels is used to avoid or reduce the probability of overﬂows,
but it may also be used to increase the SNR. In the FIR ﬁlter in Figure 5.38, the only node that needs to
be scaled (assuming a scaled input) is the output which is scaled by multiplying all impulse response
values h(n) by the scaling coefﬁcient c. In this ﬁlter, the output SNR is affected by the scaling because
the output round-off noise variance is independent of the multiplier coefﬁcient values whereas the output
signal power is scaled with c2. With safe scaling, c is chosen as
c =
1
∞
n=−∞|h(n)|.
(5.162)
Using the convolution sum and the triangle inequality, one can show that c according to (5.162) guaran-
tees no overﬂow. On the other hand, it also results in a lower SNR. The SNR can be increased by using
a larger value of c, but this also comes with overﬂow with a certain probability. With L2-norm scaling,
c is chosen as
c =
1
∞
n=−∞|h(n)|2
.
(5.163)

1.05.7 Oversampling Techniques
227
Using L2-norm scaling for white-noise Gaussian input signals, the probability of overﬂow at the output
will be the same as the probability of overﬂow at the input.
There is thus a trade-off between high SNR and probability of overﬂow. In this example, we obtain
c = 1.238477 using safe scaling and c = 3.330192 using L2-norm scaling. The increase in SNR that
is achieved by using L2-norm scaling instead of safe scaling is therefore
20 × log10 (3.330192) −20 × log10 (1.238477) ≈8.59 [dB]
(5.164)
which roughly corresponds to some 1.5 bits. That is, using L2-norm scaling instead of safe scaling, we
can implement the ﬁlter using 1.5 bits shorter data wordlength, which in practice means 1 bit or 2 bits
shorter wordlength.
1.05.7 Oversampling techniques
This section discusses oversampling techniques which are used in A/D and D/A conversions for relaxing
the requirements on the analog ﬁlters and quantizers.
1.05.7.1 Oversampled A/D converters
The principle of an oversampled A/D converter is shown in Figure 5.39. To elaborate on this principle,
assume the typical spectra shown in Figure 5.40. Say that we have a signal xwb(t) containing information
in the baseband up to π/T and undesired frequency components above π/T , like wideband noise
[Figure 5.40a]. The information can, in principle, be sampled and reconstructed without errors if the
sampling frequency is fs = 1/T . Due to the wideband noise, the sampling will however also introduce
aliasing distortion. Theoretically, aliasing distortion can be avoided by band limiting the input signal
using an ideal analog lowpass ﬁlter with passband up to π/T , but such a ﬁlter cannot be realized in
practice. Hence, we must let the ﬁlter have a transition band. The wider this band is, the simpler it
becomes to implement the ﬁlter. On the other hand, we must then increase the sampling frequency so
as to avoid aliasing distortion.
Assume that we use M times oversampling, i.e., that we choose a sampling frequency fs1 according
to
fs1 = M fs.
(5.165)
This corresponds to a sampling period T1 according to
T1 = T /M.
(5.166)
FIGURE 5.39
Oversampled A/D converter.

228
CHAPTER 5 Sampling and Quantization
(a)
(b)
(c)
(d)
(e)
(f)
FIGURE 5.40
Principle spectra in an oversampled A/D converter.

1.05.7 Oversampling Techniques
229
We can now avoid aliasing distortion by selecting the stopband edge of the anti-aliasing ﬁlter Ha(s)
appropriately. It sufﬁces to ensure that aliasing into the baseband is avoided since the information is
then unaffected, see Figure 5.40c. Frequency components that are aliased into the remaining frequency
region are allowed since they can be eliminated by the digital ﬁlter H(z) that succeeds the A/D converter,
see Figure 5.40d. We see that the stopband edge of the analog ﬁlter must satisfy
2π −asT1 > π/M ⇐⇒as < 2π M fs −π fs
(5.167)
in order to avoid aliasing distortion. In practice, the distortion is suppressed to a level determined by
the attenuation of the analog anti-aliasing ﬁlter. To make sure that one can reduce the distortion to
any desired level, (5.167) must be satisﬁed as the distortion then is determined by the ﬁlter’s stopband
attenuation.
In addition, one has to take into account that the digital ﬁlter has a transition band as well, which
means that the overall design is somewhat more complicated than indicated above. Speciﬁcally, to avoid
aliasing into the baseband, the information must therefore be limited to an angle frequency below π/T .
If the information is limited to, say c, then the transition band of the digital ﬁlter ranges from cT1
to π/M.
We see that x1(m) corresponds to the information oversampled by a factor of M. The A/D converter
thus generates M times more samples per time unit than actually needed for reconstruction of the
information. To make sure that the succeeding digital system does not have to work at a higher data rate
than necessary, one therefore reduces the sampling rate through decimation (downsampling) by a factor
of M. Since we have M times oversampling after H(z), we can do this without loosing information.
When M is an integer, the decimation is done by simply extracting only every Mth sample in the output
sequence of H(z) [24,54,55]. This is done by a downsampler, represented by ↓M. (For a noninteger
M, more elaborate schemes must be employed.) The result is the sequence y(n) corresponding to the
information sampled with the frequency fs = 1/T , see Figure 5.40f.
One motivation for using oversampling is, as mention earlier, that one in this way can relax the
requirements on the analog anti-aliasing ﬁlter. Another reason is that oversampling enables the use
of an A/D converter with lower resolution than the desired one, i.e., with fewer bits than the desired
number of bits. The reason is that the digital ﬁlter removes a large part of the noise power whereas it,
virtually, leaves the information unaffected. This means that the signal-to-noise ratio (SNR) of y(n) is
higher than that of x(m). To elaborate on this point, say that x(m) is quantized to B1 bits (and again
assuming a maximum signal value of Xm = 1 for simplicity). We then know that the noise variance is
σ 2
e = 2−2(B1−1)
12
.
(5.168)
Further, we know that the noise variance at the output of the ﬁlter H(z), say σ 2
h , is given by
σ 2
h = σ 2
e
1
2π
 π
−π
|H(e jT )|2d(T ).
(5.169)
If we now for the sake of simplicity assume that H(z) is an ideal unity-gain lowpass ﬁlter with passband
up to π/M, we get
σ 2
h = σ 2
e
1
2π
 π
−π
|H(e jT )|2d(T ) = 1
2π
 π/M
−π/M
|H(e jT )|2d(T ) = σ 2
e
M .
(5.170)

230
CHAPTER 5 Sampling and Quantization
Since H(z) leaves the information unaffected, the SNR is M times higher at the output of the ﬁlter
than at the input of the ﬁlter, i.e., the output of the A/D converter. We can therefore obtain a desired
resolution of B bits by using oversampling and an A/D converter with a lower resolution of B1 bits.
From the equations above, we obtain
2−2(B1−1)
12M
= 2−2(B−1)
12
,
(5.171)
which gives us
B1 = B −0.5 log2 (M).
(5.172)
We see that for each doubling of the sampling frequency, we gain half a bit in resolution. By using
oversampling techniques, one can thus use A/D converters with lower resolution than the desired one.
In the extreme case, so called one-bit converters are employed. For such an A/D converter, one must
however use a very high oversampling factor if a high resolution is desired. This causes problems in
wideband applications since the sampling frequency then becomes too high to handle. By utilizing so
called -modulators, the oversampling factor required to achieve a certain resolution can be reduced
[12]. The basic principle is to shape the noise so that most of its energy is located in the high-frequency
region (for a lowpass converter). In this way, one gains more than a factor of M in noise reduction when
using a lowpass ﬁlter with a passband width of π/M. It is beyond the scope of this chapter to discuss
-converters in detail. However, Section 1.05.8 deals with modeling of mixed-signal systems, which
is useful in the analysis of such converters. That section will also provide an example of second-order
-modulators.
1.05.7.2 Oversampled D/A converters
As discussed in the previous section, the requirements on the analog anti-aliasing ﬁlters can be relaxed
by using oversampled A/D converters. The requirements on the analog reconstruction ﬁlter can likewise
be relaxed by using oversampled D/A converters according to Figure 5.41.
We start with the sequence y(n) generated by the oversampled A/D converter in Figure 5.39. To
illustrate the principle we use again the spectra in Figure 5.40. The spectrum of y(n) is then as shown
in Figure 5.42a. The ﬁrst step in an oversampled D/A converter is digital interpolation by a factor of
M. This is done through an upsampler, represented by ↑M, followed by a ﬁlter H(z) [24,54,55]. The
result is the sequence x1(m) whose spectrum is shown in Figure 5.42c. We see that this sequence has
the same spectrum as x1(m) in the oversampled A/D converter, see Figures 5.39 and 5.40. In other
words, it corresponds to the information oversampled by a factor of M. In the next step, reconstruction
is performed through a D/A converter followed by a reconstruction ﬁlter. The D/A converter works at
the higher sampling frequency M fs. As in Section 1.05.3.5.2, the D/A converter is here described by
FIGURE 5.41
Oversampled D/A converter.

1.05.7 Oversampling Techniques
231
(a)
(b)
(c)
(d)
(e)
(f)
FIGURE 5.42
Principle spectra in an oversampled D/A converter.

232
CHAPTER 5 Sampling and Quantization
the pulse q(t) according to Figure 5.19d. Ideally, Q( j)Ha( j) and Ha( j) are then as shown in
Figures 5.42d and e, respectively. We see that the stopband edge of the analog reconstruction ﬁlter must
satisfy
as < 2π M fs −π fs,
(5.173)
which is the same requirement as that of the anti-aliasing ﬁlter in an oversampled A/D converter. By
increasing M, we thus relax the ﬁlter requirements since a wider transition band is then allowed.
In practice, one can let the digital ﬁlter equalize the deviation of Q( j), which is needed since
Q( j) is not constant in the baseband. That is, H(z) is designed so that
|H(e jT1)Q( j)| ≈constant,
for|| < π fs.
(5.174)
As analog reconstruction ﬁlter, one can then use a conventional lowpass ﬁlter with (approximately)
constant gain in both the passband and stopband. In that way, the requirements on this ﬁlter are further
relaxed. Instead, the requirements on the digital ﬁlter has become more severe, but it is much easier to
implement digital ﬁlters with stringent requirements than analog ﬁlters.
1.05.8 Discrete-time modeling of mixed-signal systems
Many systems today are mixed-signal systems conducting both continuous-time and discrete-time signal
processing. From the analysis point of view, it is often advantageous to use a discrete-time model of the
mixed-signal system. This section presents an analysis method for linear time-invariant systems contain-
ing sampling and reconstruction inside a feedback loop [56]. Using this method, the input/output stability
as well as the frequency responses of such mixed systems can easily be analyzed. One application is
ADCs realized with sigma-delta modulators (-modulators) with continuous-time input signals. An
example is seen in Figure 5.43 which is a linear model of a double feedback loop continuous-time-input
-modulator. Figure 5.44 shows a generalization where F0(s) and F1(s) are transfer functions of
general loop ﬁlters. However, the analysis method that will be presented here is applicable to any linear
and time-invariant system that can be represented by the general structure shown in Figure 5.45, where
N is a two-input/one-output system and where the single output is uniformly sampled and reconstructed
through pulse amplitude modulation before being fed back again to N. Apparently, the structures of
Figures 5.43 and 5.44 belong to the class of all structures that can be represented by the general structure
of Figure 5.45.
FIGURE 5.43
Linear model of a second-order double feedback loop continuous-time-input -modulator.

1.05.8 Discrete-Time Modeling of Mixed-Signal Systems
233
FIGURE 5.44
Generalization of the scheme in Figure 5.44.
FIGURE 5.45
General hybrid continuous-time/discrete-time system setup with a feedback loop.
FIGURE 5.46
Linear model of a second-order discrete-time-input -modulator.
It is of practical interest to analyze systems such as that of Figure 5.43 with respect to stability as well
as frequency responses as seen from xa(t) and e(n) to the output y(n). Here, e(n) models quantization
errors that occur in the quantizer that follows the sampler. For continuous-time-input -modulators,
stability and frequency response analyses are often performed by using the so called “sampled-data
equivalent” discrete-time-input -modulator seen in Figure 5.46 [12,13,57], where integrators are
replaced with accumulators. However, these “sampled-data equivalents” are only approximations of the
actual behavior of the continuous-time-input -modulator, as will be exempliﬁed later in this section.
That is, they do not correctly describe the properties of the continuous-time-input circuit.
To get a correct description, the system at hand can instead be represented by the system in
Figure 5.47, where the continuous-time input signal xa(t) is preﬁltered through a continuous-time
system G p before being sampled uniformly. The resulting sampled signal and the system output signal

234
CHAPTER 5 Sampling and Quantization
FIGURE 5.47
State-space representation of the system in Figure 5.48.
are then used as input signals to a linear discrete-time system Gs, usually represented in state-space
form [12,58]. It is then required to determine a corresponding input-output difference equation using
state variable representation. To that end, a state matrix, A, of Gs is needed, requiring the computation
of the power series
A = eAc =
∞

k=0
1
k! Ak
c,
(5.175)
where Ac is the state matrix of the system N in Figure 5.45, as well as the inversion of two particular
matrices. This method gives a correct description of the circuit at hand but requires the computation of
(5.175), which can be a non-trivial task, and the above mentioned matrices often are singular. In fact,
this is always the case when F0(s) and F1(s) represent ideal integrators 1/s.
In this section, we discuss an alternative method for the analysis of the external stability as well
as the frequency responses from the inputs xa(t) and e(n) to the output y(n) of the general system in
Figure 5.47 [56]. This is done by using the models shown in Figure 5.48. It is readily shown that all
linear time-invariant systems belonging to the class of systems that can be represented by the system
in Figure 5.45, can be represented as in Figure 5.48. It should however be noted that the systems in
Figure 5.48 are only used for input/output analysis purposes and do not capture the internal properties
of the underlying system of interest. From the input/output stability and frequency response points of
view, the systems in Figure 5.48 are equivalent to their underlying system in Figure 5.45.
1.05.8.1 Input/output relations
For mixed-signal systems, direct use of transfer functions is less appropriate as they depend on two
variables (s and z). Therefore, the analysis becomes somewhat more involved than for pure discrete-
time or continuous-time systems. The mixed-signal systems are analyzed here by using equivalent
discrete-time systems that produce the same output samples as the actual mixed-signal system.
To be precise, stability is analyzed by ﬁrst determining G(z) in the z-transform relation between
y(n), v(n), and e(n) given as
Y(z) = G(z)V (z) + G(z)E(z).
(5.176)
Stability is then investigated by considering the poles of G(z) and the boundedness of the resulting
output signal. As to the frequency responses, as seen from xa(t) and e(n) to the output y(n), it is

1.05.8 Discrete-Time Modeling of Mixed-Signal Systems
235
(a)
(b)
FIGURE 5.48
Two equivalent models for the system in Figure 5.45.
appropriate to express the output Fourier transform as
Y(e jT ) = H( j) 1
T Xa( j) + G(e jT )E(e jT ),
(5.177)
where H( j) represents the signal frequency response whereas G(e jT ) represents the noise frequency
response. The latter is immediately obtained from the noise transfer function G(z) by evaluating it for
z = e jT .
In order to arrive at (5.176) and (5.177), the models of Figure 5.48 are used to represent the system
in Figure 5.45. In Figure 5.48, P(s) represents pulse amplitude modulation.
1.05.8.2 Determining G(z)
We will make use of the generalized Poisson’s summation formula which relates the Laplace transform
of a signal xa(t) with the corresponding z-transform of its uniformly sampled version x(n) = xa(nT )
according to
X(esT ) = 1
T
∞

p=−∞
Xa

s −j 2π p
T

.
(5.178)
with the z-transform evaluated for z = esT . Further, we will make use of the following equation that
relates the output xr(t) of the pulse amplitude modulator p(t) with its input x(n):
Xr(s) = P(s)X(esT ).
(5.179)

236
CHAPTER 5 Sampling and Quantization
Using the above relations, the z-transform of the output signal y(n) in Figure 5.48a is easily derived as
Y(esT ) = E(esT )
+ 1
T
∞

p=−∞
H1

s −j 2π p
T

Xa

s −j 2π p
T

−Y(esT ) 1
T
∞

p=−∞
H2

s −j 2π p
T

P

s −j 2π p
T

,
(5.180)
where E(z) is the z-transform of e(n) and P(s) is deﬁned by the PAM used. Here, a rectangular pulse
p(t) is assumed according to
p(t) =
1, 0 ≤t ≤T ,
0, otherwise,
(5.181)
which is commonly used in D/A conversion. This gives
P(s) = 1 −e−sT
s
.
(5.182)
Equation (5.180) can be rearranged according to (5.176) with
V (esT ) = 1
T
∞

p=−∞
H1

s −j 2π p
T

Xa

s −j 2π p
T

,
(5.183)
which corresponds to the z-transform of v(n) in Figure 5.48b, i.e., a uniformly sampled version of va(t),
and
G(z) =
1
1 + L(z) = N(z)
D(z)
(5.184)
with
L(esT ) = 1
T
∞

p=−∞
H2

s −j 2π p
T

P

s −j 2π p
T

.
(5.185)
Equation (5.176) can, further, be interpreted as shown in Figure 5.49.
In Figure 5.48, H1(s) models the output signal dependency of the input signal whereas H2(s)
describes the behavior of the feedback signal. Eqs. (5.176) and (5.183)–(5.185) do not give us suitable
FIGURE 5.49
Representation of the system in Figure 5.48 in view of (5.176).

1.05.8 Discrete-Time Modeling of Mixed-Signal Systems
237
expressions for the stability analysis. In order to express L(z) in a more suitable form for this purpose,
we make use of the observation that L(z) is the z-transform of the sequence l(n) = la(nT ), i.e., a
uniformly sampled version of the signal la(t). Further, from (5.185), it is seen that la(t) is the inverse
Laplace transform of H2(s)P(s). Hence, a more convenient representation of L(z) and thereby G(z)
can be found using the following procedure.
Starting with H2(s)P(s), its inverse Laplace transform gives la(t) from which l(n) = la(nT ) follows
directly. A closed form expression for L(z) is then obtained through the computation of the z-transform
of l(n). The approach is applicable for general H2(s) and P(s), and is straightforward when H2(s) and
P(s) are rational functions in s.
1.05.8.3 Stability
In order to investigate the stability, it is convenient to ﬁrst consider the system in Figure 5.50a, where
the numerator and denominator polynomials of G(z) in Figure 5.49, i.e., N(z) and D(z), respectively,
have been shown explicitly. The system of Figure 5.45 will be bounded-input/bounded-output (BIBO)
stable if the following two conditions are satisﬁed:
1. The poles of G(z), i.e., the roots of D(z), lie inside the unit circle,
2. The signal w(n) in Figure 5.50 is bounded.
The ﬁrst condition above is easily checked once G(z) is available. The second condition requires an
analysis of H1(s) and N(z). We consider the following two different cases which are of practical interest.
Case 1: H1(s) is stable. This gives directly that the second condition above is satisﬁed and BIBO
stability is then ensured by also fulﬁlling the ﬁrst condition.
Case 2: H1(s) can be factored according to
H1(s) = A(s) 1
s Q1
Q2

q=1
1
s + pq
,
(5.186)
where A(s) is stable and pq are imaginary roots. Then, H1(s) contains Q1 poles at s = 0 and
Q2 poles at s = −pq. In order to investigate stability in this case, it is convenient to consider
(a)
(b)
FIGURE 5.50
Representations of the system in Figure 5.49. They are useful in the stability analysis.

238
CHAPTER 5 Sampling and Quantization
the system shown in Figure 5.50b, where
Na(s) = N(esT ).
(5.187)
Since N(z) is of the form
N(z) =
K

k=0
akz−k,
(5.188)
the signal w(n) of Figure 5.50(a) is identical to that in Figure 5.50b. This follows from
Poisson’s summation formula together with the fact that Na(s) above is periodic. It then
remains to investigate if H1(s)Na(s) is stable, i.e., if it exists for all ℜ{s} ≥0, assuming a
causal system, and if G(z) [or, equivalently, 1/D(z)] is stable.
1.05.8.4 Frequency responses H( j) and G(e jT )
The frequency response from e(n) to the output y(n) i.e., G(e jT ), is directly obtained from G(z) in
(5.176) if it exists for z = e jT . It is then given by
G(e jT ) = N(e jT )
D(e jT ) =
1
1 + 1
T
∞
p−∞H2

j −j 2π p
T

P

j −j 2π p
T
.
(5.189)
As to the signal frequency response, H( j), it is ﬁrst noted from (5.176) and (5.177) that
H( j) 1
T Xa( j) = G(e jT )V (e jT ), where
V (e jT ) = 1
T
∞

p−∞
H1

j −j 2π p
T

Xa

j −j 2π p
T

.
(5.190)
Assume now that the input signal xa(t) is bandlimited according to the Nyquist criterion for sampling
with the sampling period T, i.e.,
Xa( j) = 0,
|| ≥π/T .
(5.191)
In this case only the term for p = 0 in (5.190) is non-zero in the interval −π ≤T ≤π. Hence,
Y(e jT ) can be written as in (5.177) with
H( j) = H1( j)G(e jT ).
(5.192)
1.05.8.5 Example 1
Consider the second-order continuous-time-input  modulator shown earlier in Figure 5.43. This is
a special case of the system of Figure 5.44 with F1(s) and F2(s) being ideal integrators, i.e.,
F1(s) = K1
s ,
F2(s) = K2
s .
(5.193)

1.05.8 Discrete-Time Modeling of Mixed-Signal Systems
239
In this case, one obtains that H1(s) and H2(s) are given by
H1(s) = F1(s)F2(s),
H2(s) = [1 + F1(s)]F2(s).
(5.194)
From this equation, we obtain
La(s) = H2(s)P(s) =
 K2
s2 + K1K2
s3

(1 −e−sT ).
(5.195)
Taking the inverse Laplace transform, we get
la(t) = K2[tu(t) −(t −T )u(t −T )] + K1K2
2
[t2u(t) −(t −T )2u(t −T )].
(5.196)
Sampling la(t) at t = nT then gives us
l(n) = la(nT ) = K2T [n × u(n) −(n −1) × u(n −1)]
+ K1K2T 2
2
[n2 × u(n) −(n −1)2 × u(n −1)],
(5.197)
where u(n) is the unit-step sequence. Then, taking the z-transform of l(n) gives us
L(z) = K2T z−1
1 −z−1

1 + K1T
2
1 + z−1
1 −z−1

,
(5.198)
whereby
G(z) = N(z)
D(z) =
1
1 + L(z) =
(1 −z−1)2
1 + az−1 + bz−2 ,
(5.199)
where
a = K2T + 0.5K1K2T 2 −2
(5.200)
and
b = 1 −K2T + 0.5K1K2T 2.
(5.201)
Further, with F0(s) and F1(s) as in (5.193), we get
H1(s)Na(s) = K1K2
s2
(1 −e−sT )2,
(5.202)
which is a stable system because the corresponding impulse response is ﬁnite. Hence, stability is in this
case solely determined by the stability of G(z) given by (5.199). For example with K1 = K2 = K, it is
straightforward to show that stability in this case is ensured by constraining K according to 0 < K <
2/T . Using instead the often employed and so called “sampled-data equivalents,” where integrators 1/s
are replaced by the accumulators 1/(1 −z−1), and z−1/(1 −z−1), one obtains 0 < K < 1.236/T .
Hence, the accumulator-based structures are not appropriate models.

240
CHAPTER 5 Sampling and Quantization
Furthermore, from (5.192), (5.194), and (5.199), we obtain the frequency response
H( j) = K0K1
( j)2
(1 −e−jT )2
1 + ae−jT + be−j2T
(5.203)
This expression differs from that obtained via the accumulator-based structures. Also the noise transfer
functions are different. Again, this means that such structures are not appropriate. Figures 5.51 and 5.52
illustrate these differences by plotting the modulus of the signal and noise transfer functions in the two
cases.
1.05.8.6 Example 2
Consider the scheme in Figure 5.53 with F(s) being a second order transfer function according to
F(s) =
K
s(s + p)
(5.204)
with a real-valued pole at s = −p, with p > 0 assumed. For the equivalent representation of
Figure 5.48, H1(s) = H2(s) = F(s). Thus, H1(s) follows (5.186) as H1(s) = A(s)/s with A(s) =
K/(s + p).
We ﬁrst investigate the stability. We note that Case 2 applies here since H1(s) has a pole at s = 0.
To derive G(z), we ﬁrst note that H1(s)P(s), using partial fraction expansion, becomes
H1(s)P(s) =
 K/p
s2
−K/p2
s
+ K/p2
s + p

(1 −e−sT ).
(5.205)
0
1
2
0
0.2π
0.4π
0.6π
0.8π
π
K1=K2=1
0
1
2
0
0.2π
0.4π
0.6π
0.8π
π
K1=K2=1.05
Magnitude
0
1
2
0
0.2π
0.4π
0.6π
0.8π
π
K1=K2=0.95
ΩT [rad]
FIGURE 5.51
Modulus of the signal frequency responses for second-order continuous-time-input (dashed) and discrete-
time-input -modulators (solid).

1.05.8 Discrete-Time Modeling of Mixed-Signal Systems
241
0
5
0
0.2π
0.4π
0.6π
0.8π
π
K1=K2=1
0
5
0
0.2π
0.4π
0.6π
0.8π
π
K1=K2=1.05
Magnitude
0
5
0
0.2π
0.4π
0.6π
0.8π
π
K1=K2=0.95
ΩT [rad]
FIGURE 5.52
Modulus of the noise frequency responses for second-order continuous-time-input (dashed) and discrete-
time-input -modulators (solid).
FIGURE 5.53
System with a single feedback loop.
Hence, l(n) = la(nT ) is given by
l(n) = la0(nT ) −la0(nT −T )
(5.206)
with la0(t) being
la0(t) =
 K
p t −K
p2 + K
p2 e−pt

u(t).
(5.207)
Hence, taking the z-transform of l(n), we obtain
L(z) =
K T
p(z −1) −K
p2 + K
p2
z −1
z −e−pT .
(5.208)

242
CHAPTER 5 Sampling and Quantization
Finally, this gives us
G(z) =
1
1 + L(z) = (1 −z−1)(1 −z−1e−pT )
1 + az−1 + bz−2
,
(5.209)
where
a = K T
p
−(1 + e−pT ) −K
p2 (1 −e−pT )
(5.210)
and
b = K
p2 (1 −e−pT ) +

1 −K T
p

e−pT .
(5.211)
We note that
H1(s)Na(s) = A(s)(1 −esT )(1 −e−(s+p)T )
s
(5.212)
which corresponds to a stable system, because it consists of (readily shown) stable subsystems. Hence,
the stability is given by the poles of G(z), which are most easily found numerically.
Further, we get the frequency responses
G(e jT ) = (1 −e−jT )(1 −e−jT e−pT )
1 + ae−jT + be−j2T
(5.213)
and
H( j) = H1( j)G(e jT )
=
K
j( j + p)
(1 −e−jT )(1 −e−jT e−pT )
1 + ae−jT + be−j2T
.
(5.214)
Relevant Theory: Signal Processing Theory
See this Volume, Chapter 2 Continuous-Time Signals and Systems
See this Volume, Chapter 3 Discrete-Time Signals and Systems
See this Volume, Chapter 4 Random Signals and Stochastic Processes
See this Volume, Chapter 7 Multirate Signal Processing
References
[1] E.T. Whittaker, On the functions which are represented by the expansion of interpolating theory, Proc. Roy.
Soc. Edinburgh 35 (1915) 181–194.
[2] H. Nyquist, Certain topics in telegraph transmission theory, Trans. Am. Inst. Elect. Eng. 47 1928.
[3] V.A. Kotelnikov, On the transmission capacity of ether and wire in electrocommunications, Izd. Red. Upr,
Svyazi RKKA, Moscow, 1933.
[4] C.E. Shannon, Communication in the presence of noise, Proc. IRE 37 (1949) 10–21.
[5] A.J. Jerri, The shannon sampling theorem–its various extensions and applications: A tutorial review, IEEE
Proc. 65 (11) (1977) 1565–1596.
[6] M. Unser, Sampling – 50 years after Shannon, IEEE Proc. 88 (4) (2000) 569–587.

References
243
[7] P.P. Vaidyanathan, Generalizations of the sampling theorem: Seven decades after Nyquist, IEEE Trans. Circ.
Syst. I 48 (9) (2001) 1094–1109.
[8] R.J. van de Plassche, CMOS Integrated Analog-to-Digital and Digital-to-Analog Converters, Kluwer Aca-
demic Publ., 2003.
[9] A.V. Oppenheim, R.W. Schafer, Discrete-Time Signal Processing, Prentice Hall, 1989.
[10] S.K. Mitra, Digital Signal Processing: A Computer-Based Approach, McGraw-Hill, 2006.
[11] W.C. Black, D.A. Hodges, Time interleaved converter arrays, IEEE J. Solid-State Circ. SC–15 (6) (1980)
1022–1029.
[12] S.R. Norsworthy, R. Schreier, G.C. Temes, Delta-Sigma Data Converters: Theory, Design, and Simulation,
IEEE Press, 1997.
[13] R. Schreier, G.C. Temes, Understanding Delta-Sigma Data Converters, John Wiley and Sons, NJ, 2005.
[14] S.Y. Hui, K.H. Yeung, Challenges in the migration to 4G mobile systems, Comm. Mag. 41 (12) (2003) 54–59.
[15] B. Le, T.W. Rondeau, J.H. Reed, C.W. Bostian, Analog-to-digital converters: a review of the past, present,
and future, Signal Process. Mag. 22 (6) (2005) 69–77.
[16] Analog Devices Inc., <http://www.analog.com>.
[17] Texas Instruments Inc., <http://www.ti.com>.
[18] R. Khoini-Poorfard, L.B. Lim, D.A. Johns, Time-interleaved oversampling A/D converters: Theory and prac-
tice, IEEE Trans. Circ. Syst. II: Anal. Digital Signal Process. 44 (8) (1997) 634–645.
[19] B. Murmann, Trends in low-power, digitally assisted A/D conversion, IEICE Trans. Electron. E93–C (6)
(2010) 718–729.
[20] E.J. Candes, M.B. Wakin, An introduction to compressive sampling, IEEE Signal Process. Mag. 25 (2) (2008)
21–30.
[21] J.A. Tropp, J.N. Laska, M.F. Duarte, J.K. Romberg, R.G. Baraniuk, Beyond nyquist: efﬁcient sampling of
sparse bandlimited signals, IEEE Trans. Inf. Theory 56 (1) (2010) 520–544.
[22] M. Mishali, Y.C. Eldar, Sub-Nyqust sampling: bridging theory and practice, IEEE Signal Process. Mag. 28
(6) (2011) 98–124.
[23] A. Papoulis, The Fourier Integral and Its Applications, McGraw-Hill, 1962.
[24] P.P. Vaidyanathan, Multirate Systems and Filter Banks, Prentice Hall, 1993.
[25] F. Marvasti, Nonuniform Sampling: Theory and Practice, Kluwer, NY, 2001.
[26] Y.C. Eldar, A.V. Oppenheim, Filterbank reconstruction of bandlimited signals from nonuniform and general-
ized samples, IEEE Trans. Signal Process. 48 (10) (2000) 2864.
[27] H. Johansson, P. Löwenborg, Reconstruction of nonuniformly sampled bandlimited signals by means of time-
varying discrete-time FIR ﬁlters, Hindawi Publishing Corporation EURASIP J. Appl. Signal Process., Vol.
2006, Article ID 64185, 2006, pp. 1–18, <http://dx.doi.org/10.1155/ASP/2006/64185>.
[28] H. Johansson, P. Löwenborg, K. Vengattaramane, Reconstruction of M-periodic nonuniformly sampled sig-
nals using multivariate polynomial impulse response time-varying FIR ﬁlters, in: Proc. XII European Signal
Processing Conf., Florence, Italy, September 2006.
[29] C. Vogel, The impact of combined channel mismatch effects in time-interleaved ADCs, IEEE Trans. Instrum.
Meas. 55 (1) (2005) 415–427.
[30] C. Vogel, Modeling, Identiﬁcation, and Compensation of Channel Mismatch Errors in Time-Interleaved
Analog-to-Digital Converters, Diss., Graz Univ., 2005.
[31] C. Vogel, G. Kubin, Modeling of time-interleaved ADCs with nonlinear hybrid ﬁlter banks, AEU - Int. J.
Electronics Comm. 59 (5) (2005) 288–296.
[32] J. Elbornsson, F. Gustafsson, J.E. Eklund, Blind equalization of time errors in a time-interleaved ADC system,
IEEE Trans. Signal Process. 53 (4) (2005) 1413.
[33] V. Divi, G. Wornell, Scalable blind calibration of timing skew in high-resolution time-interleaved ADCs, in
Proc. IEEE Int. Symp. Circ. Syst, Kos, Greece, May 2006, pp. 21–24.

244
CHAPTER 5 Sampling and Quantization
[34] S. Huang, B.C. Levy, Adaptive blind calibration of timing offset and gain mismatch for two-channel time-
interleaved ADCs, IEEE Trans. Circ. Syst. I 53 (6) (2006) 1278–1288.
[35] S. Huang, B.C. Levy, Blind calibration of timing offsets for four-channel time-interleaved ADCs, IEEE Trans.
Circ. Syst. I 54 (4) (2007) 863–876.
[36] A. Haftbaradaran, K.W. Martin, A background sample-time error calibration technique using random data for
wide-band high-resolution time-interleaved ADCs, IEEE Trans. Circ. Syst. II 55 (3) (2008) 234–238.
[37] M. El-Chammas, B. Murmann, General analysis on the impact of phase-skew mismatch in time-interleaved
ADCs, in Proc. IEEE Int. Symp. Circ. Syst, Seattle, USA, May 2008, pp. 18–21.
[38] T. Tsai, P.J. Hurst, S.H. Lewis, Bandwidth mismatch and its correction in time-interleaved analog-to-digital
converters, IEEE Trans. Circ. Syst. II 53 (10) (2006) 1133–1137.
[39] M. Seo, M.J.W. Rodwell, U. Madhow, Comprehensive digital correction of mismatch errors for a 400-
msamples/s 80-dB SFDR time-interleaved analog-to-digital converter, IEEE Trans. Microwave Theory Techn.
53 (3) (2005) 1072–1082.
[40] S. Mendel and C. Vogel, A compensation method for magnitude response mismatches in two-channel time-
interleaved analog-to-digital converters, in: Proc. IEEE Int. Conf. Electronics, Circ. Syst., Nice, France,
December 2006.
[41] S. Mendel and C. Vogel, On the compensation of magnitude response mismatches in M-channel time-
interleaved ADCs, in: Proc. IEEE Int. Symp. Circ. Syst., New Orleans, USA, May 2007, pp. 3375–3378.
[42] M.Seo,M.J.W.Rodwell,U.Madhow,Generalizedblindmismatchcorrectionfortwo-channeltime-interleaved
A-to-D converters, in: Proc. IEEE Int. Conf. Acoustics, Speech, Signal Processing, Hawaii, USA, April 2007.
[43] M. Seo, M.J.W. Rodwell, U. Madhow, Generalized blind mismatch correction for a two-channel time-
interleaved ADC: Analytic approach, in: Proc. IEEE Int. Symp. Circ. Syst, New Orleans, USA, May 2007,
pp. 27–30.
[44] P. Satarzadeh, B.C. Levy, P.J. Hurst, Bandwidth mismatch correction for a two-channel time-interleaved A/D
converter, in: Proc. IEEE Int. Symp. Circ. Syst, New Orleans, USA, May 2007.
[45] H. Johansson, P. Löwenborg, A least-squares ﬁlter design technique for the compensation of frequency-
response mismatch errors in time-interleaved A/D converters, IEEE Trans, Circ. Syst. II: Express Briefs 55
(11) (2008) 1154–1158.
[46] H. Johansson, A polynomial-based time-varying ﬁlter structure for the compensation of frequency-response
mismatch errors intime-interleavedADCs: special issue on DSP techniques for RF/analog circuit impairments,
IEEE J. Selected Topics Signal Process. 3 (3) (2009) 384–396.
[47] Y.C. Lim, Y.X. Zou, J.W. Lee, S.C. Chan, Time-interleaved analog-to-digital converter compensation using
multichannel ﬁlters, IEEE Trans., Circ. Syst. I: Regular Papers 56 (10) (2009) 2234–2247.
[48] C. Vogel, S. Mendel, A ﬂexible and scalable structure to compensate frequency response mismatches in
time-interleaved ADCs, IEEE Trans. Circ. Syst. I: Regular papers 56 (11) (2009) 2463–2475.
[49] J.L. Yen, On nonuniform sampling of bandlimited signals, IRE Trans. Circuit Theory CT–3 (1956).
[50] A. Papoulis, Generalized sampling expansion, IEEE Trans. Circ. Syst. CAS–24 (11) (1977) 652–654.
[51] A. Antoniou, Digital Filters: Analysis, Design, and Applications, second ed., McGraw-Hill, 1993.
[52] L.B. Jackson, Digital Filters and Signal Processing, third ed., Kluwer Academic Publishers, 1996.
[53] L. Wanhammar and H. Johansson, Digital Filters using Matlab, Linköping University, 2011.
[54] R.E. Crochiere, L.R. Rabiner, Multirate Digital Signal Processing, Prentice Hall, NJ, 1983.
[55] N.J. Fliege, Multirate Digital Signal Processing, Wiley, New York, 1994.
[56] P.Löwenborg,H.Johansson,Analysisofcontinuous-time-inputA/Dmodulatorsandtheirgeneralizations,
in: Proc. European Conf. Circuit Theory Design, Krakow, Poland, September 1–4 2003.
[57] S.H. Ardalan and J.J. Paulos, An analysis of nonlinear behavior in delta-sigma modulators, IEEE Trans. Circ.
Syst. CAS–34 (6) (1987) 593–603.
[58] H. Freeman, Discrete-Time Systems, John Wiley, 1965.

6
CHAPTER
Digital Filter Structures
and Their Implementation
Lars Wanhammar* and Ya Jun Yu†
*Department of Electrical Engineering, Linköping University, Sweden
†School of Electrical & Electronic Engineering, Nanyang Technological University, Singapore
1.06.1 Introduction
There has been a clear trend over the past decades towards an increasing use of digital processing rather
than analog processing of signals. This is in large part due to the inherent ﬂexibility and reliability of dig-
ital processing. Other important factors, that support this trend, are the rapid development in integrated
circuit technology and the associated tools, i.e., that simpliﬁes and reduces the cost of implementing
digital signal processing systems, and digital ﬁlters in particular. Of interest here is frequency-selective
ﬁlters, for example, lowpass and bandpass ﬁlters.
1.06.1.1 Properties of digital ﬁlters
The following properties are characteristic of digital ﬁlters.
1.06.1.1.1
Flexibility
The frequency response can easily and quickly be changed. This means that a digital ﬁlter implemen-
tation can be time-shared between a number of input signals and act as several ﬁlters. Furthermore, a
digital ﬁlter can be multiplexed in such a way that it simultaneously acts as several ﬁlters with one input
signal and realize, for example, a ﬁlter bank. The high ﬂexibility is exploited in adaptive ﬁlters.
1.06.1.1.2
Special transfer functions
Discrete-time and digital ﬁlters can realize special transfer functions that are not realizable with
continuous-time, lumped element ﬁlters, for example ﬁlters with exact linear phase.
1.06.1.1.3
Coefﬁcient sensitivity
Digital ﬁlters are not affected by temperature variations, variations in the power supply voltage, stray
capacitances, etc., which are major problems in analog ﬁlters. The reason for this is that the properties
of a digital ﬁlter is determined by the numerical operations on numbers and not dependent on any
tolerances of electrical components. For the same reason there is no aging or drift in digital ﬁlters. The
independence of element sensitivity leads to high ﬂexibility, miniaturization, and high signal quality.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00006-1
© 2014 Elsevier Ltd. All rights reserved.
245

246
CHAPTER 6 Digital Filter Structures and Their Implementation
1.06.1.1.4
Reproducibility
Exact reproducible ﬁlters can be manufactured. This is also a consequence of the fact that the properties
of the ﬁlter are only dependent on the numerical operation and not on the tolerances of the electrical
components. Thus, tolerance problems as such do not exist in digital ﬁlters. However, we will see
that sensitivity of a ﬁlter structure to errors in the coefﬁcient has a direct inﬂuence on the required
signal wordlength and implementation cost. Furthermore, no trimming is necessary, which leads to low
cost. Precision digital ﬁlters can be manufactured with, in principle, arbitrary precision, linearity, and
dynamic range, however, at an increasing cost.
1.06.1.1.5
Frequency range
Digital ﬁlters can be manufactured to operate over a wide range of frequencies, from zero up to several
hundred MHz, depending on the complexity of the ﬁlter and on the technology used for the imple-
mentation. However, the cost in terms of power consumption and the number of gates in the integrated
circuits increases rapidly with the order of the ﬁlter and the sample frequency.
1.06.1.1.6
Power consumption
Digital ﬁlters have in general rather high power consumption, but ﬁlters with low power consumption
can be implemented for applications with low sample frequencies. However, as the geometries of the
CMOS process used for the implementation are reduced, the power consumption is also reduced. The
power consumption compared with a corresponding analog ﬁlter counterpart is therefore often lower
for a digital ﬁlter.
1.06.1.1.7
Implementation techniques
Digital ﬁlters can be implemented by using several techniques. At low sample rates and arithmetic
workloads, typically below a few MHz, standard signal processors can be used while application-
speciﬁc or algorithm-speciﬁc digital signal processors can be used up to a few tenth of MHz. For high
sample rates specialized implementation techniques are required [1].
1.06.1.2 Design target
The design should meet the requirements with minimal cost. The cost to be minimized, is typically
composed of several different parameters, e.g., power consumption, chip area, design effort. To com-
plicate the issue even further, the designer has many alternative design options available. Hence, ﬁlter
design is a challenging task, and it typically involves several conﬂicting optimization problems.
1.06.1.2.1
Power consumption
Until recently throughput and chip area was the main metrics for evaluation of DSP algorithms and
their implementations. However with the advancement of CMOS technology it is no longer a major
problem to achieve high enough throughput and the cost of the chip area can usually be neglected,
except in very high-volume products. Today, the focus should be placed on design efﬁciency and low
power consumption since many implementations are battery-powered and a too high power dissipation
may require more expensive packaging and cooling. Further, there is also a strong trends to use a more
sophisticated and complex digital signal processing which tend to increase the power dissipation.

1.06.1 Introduction
247
CMOS technology is currently and will continue to be the dominating implementation technology.
The feature size is rapidly shrinking towards 10 nm and below. Circuits implemented using modern
CMOS processes are no longer characterized by the number of transistors on the chip. Instead the
overall throughput and power consumption are more appropriate metrics. Here we assume the digital
ﬁlter is operated continuously and therefore the leakage and short-circuit currents can be neglected [2].
The power dissipation associated with switching in CMOS circuits is approximately given by
P = α fCLCLVDDVDD,
(6.1)
where α is the activity factor, i.e., the average number of transitions from 0 to 1 for the equivalent
electrical node per clock cycle, fCL is the clock frequency, CL is the equivalent switched capacitance of
the whole circuit, VDD is the power supply voltage, and VDD is the voltage swing. In most cases we
use CMOS circuits with full swing, i.e., VDD = VDD. Reducing any of the factors in (6.1) can reduce
the power consumption. Below we brieﬂy review various techniques that can be applied to reduce the
power consumption in digital ﬁlters and are useful in many other DSP algorithms.
The activity factor α can be reduced in several ways. Most techniques exploit known statistical
properties of the signals. For example, operations to be executed on an adder or multiplier can be
scheduled with respect to the correlation between successive operands so that the carry propagation and
glitches are minimized. For example, in the ﬁrst moment only the ﬁrst full-adder (LSB) in a bit-parallel
ripple-carry adder has correct inputs and performs a valid addition. The other full-adders have carry-ins
that are not yet determined, hence, they may perform invalid additions that consume energy. In the
next moment, the second full-adder has correct inputs and performs a valid addition while the ﬁrst full-
adder is idle. In each instance, the remaining full-adders may change their states and contribute with
an increase in the power dissipation. Note that the bit-parallel ripple-carry adder performs a sequential
bit-by-bit addition while in bit-serial arithmetic a bit-by-bit addition is performed sequentially on a
single full-adder. The execution time for a bit-parallel addition using a ripple-carry adder is the same
as for a bit-serial addition if the safety margins needed for the ﬂip-ﬂops that separate two successive bit
additions are neglected.
The use of a redundant number representation [3] is another technique to avoid long carry propagation
and glitches, which are costly in terms of power consumption. Glitches in logic circuits occur when the
difference in arrival times of two logic signals to a common gate is so large that false transitions occur
and change the states of some electrical nodes. Slowing down the signal paths that are too fast so that
the signals arrive simultaneously can minimize this effect and may reduce the power consumption by
up to 40% in a bit-parallel multiplier.
In older CMOS processes the equivalent switched capacitance is due to devices and wires. The
dominant wire capacitance is between the wire and substrate. In deep submicron CMOS technologies
the capacitance associated with the devices are relatively small and the dominant wire capacitance is
instead dominated by interwire capacitance as illustrated in Figure 6.1. For long buses it is therefore
important to order the transmission of successive data on the bus so that the charging/discharging of the
interwire capacitors is minimized. A useful measure to minimize is the sum of the Hamming distances
of the transmitted words. In speech signals, for example, it may be advantageous to use sign-magnitude
representation on long buses, since most samples have small magnitudes, which causes only the least
signiﬁcant bits to vary while in two’s-complement representation almost all bits change when a sample
value changes sign.

248
CHAPTER 6 Digital Filter Structures and Their Implementation
FIGURE 6.1
Interwire and wire-to-substrate capacitance model.
The clock frequency, fCL, of the circuitry should, of course, not be reduced since this would reduce
the computational work performed by the circuitry and the inherent processing capacity of the circuitry
would not be fully utilized. Generally, it is efﬁcient to use minimum size transistors throughout the whole
integrated circuit, except for a few critical paths in which slightly larger and thereby faster transistors
are used. This strategy leads to a signiﬁcant increase in the maximal clock frequency at the expense of
a small increase in the switched capacitance.
If a long logic path requier gates with extra drive capacity in order to meet the throughput requirement,
it may often be more efﬁcient to break the long path into smaller paths in terms, n order to reduce the
chip area and power consumption.
The required clock frequency for the algorithm may, however, be reduced at the expense of increased
chip area, by exploiting computational parallelism of the algorithm by mapping the arithmetic operations
to several processing elements (PEs) or by using multirate techniques in which parts of the digital ﬁlter
operate at lower sample rates. Parts of the algorithm that operate at a lower rate can often be multiplexed
onto a single hardware structure in order to save chip area.
The equivalent switched capacitance, CL, can be made small by using a combination of different
techniques. On the algorithm level most techniques focus on using ﬁlter structures with a small number
of arithmetic operations per sample. Multirate ﬁlter structures are typical examples where a signiﬁ-
cant reduction in arithmetic workload can be obtained. Low-sensitivity structures require shorter data
wordlengths and shorter and more favorable coefﬁcients that allow ﬁxed multipliers to be simpliﬁed
[1]. Canonic signed-digit code (CSDC) [1,3], and multiple-constant techniques are efﬁcient techniques
to simplify the realization of ﬁxed multipliers [4–11]. Trade-off between the two extremes, bit-parallel
and bit-serial arithmetic, i.e., digit-serial arithmetic is a promising route to lower power consumption
as well as smaller chip area [1]. As mentioned above, transmission of data over long buses is costly in
terms of time and power consumption. Architectures with local computations are therefore preferred.

1.06.1 Introduction
249
Digit-serial processing elements yield smaller chip area, and, hence, shorter communication distances
and is a good compromise between the large number of glitches in bit-parallel arithmetic and many
clock cycles per operation of the ﬂip-ﬂops in bit-serial arithmetic. At the logic level the use of efﬁcient,
low-power CMOS circuit and clocking techniques is important [2].
The power consumption can be signiﬁcantly reduced by using a lower power supply voltage. Unfor-
tunately, this has the effect that the maximal clock frequency of the CMOS circuit is reduced [2]
according to
fCL = k(VDD −VT )β
VDD
,
(6.2)
where k is process-dependent constant, β is a constant slightly larger than one in modern CMOS
processes, and VT is the threshold voltage of an MOSFET. However, the reduction in throughput can
often be compensated in parallel algorithms by exploiting more of the computational parallelism [1].
From (6.2) it is evident that from a speed point of view, a CMOS process with a low threshold voltage is
to be preferred. In practice, many circuits are therefore manufactured with a process with two threshold
voltages. Transistors with low threshold voltages are used for high-speed circuits while transistor with
high threshold voltages are used for low speed circuits since the latter have smaller leakage currents.
1.06.1.2.2
Chip area
The required chip area is important since is directly affect the cost of manufacturing the ﬁlter. Moreover,
the leakage current will be proportional to the chip area. A designer can minimize the required chip area
by using ﬁlter structures with few arithmetic operations per sample, simpliﬁed arithmetic operations,
and small memory requirement. That is, we are interested in structures that have low element sensitivity,
i.e., a structure where a small error in the coefﬁcients causes only a small error in the frequency response.
Hence, it is possible to select favorable coefﬁcient values that deviate slightly from their ideal values
and still meet the requirements on the frequency response. We will later discuss ﬁlter structures with
very low coefﬁcient sensitivities that allows us to reduce the required chip area.
The data wordlength generally affects the execution time and the required chip area for the arithmetic
operations. It has been shown by Jackson [12] that the roundoff noise is bounded from below by the
coefﬁcient sensitivities, i.e.,
σ 2 ≥

iσ 2
0i

∂
H(e jωT )

∂ci

2
p
,
(6.3)
where p = 1 or p = 2, σ 2
0 is the variance of the roundoff noise sources, and the sum is over all noise
sources due to rounding at the outputs of the coefﬁcients, ci. Hence, the right hand side of (6.3) is small
for a low-sensitive structure and the roundoff noise at the output of the structure may be low. In fact,
it will be low, even though there is no proof for it. A structure with high sensitivity will generate a
large roundoff noise and it will therefore require a large data wordlength. To summarize, the element
sensitivity affects both the required data and coefﬁcient wordlengths.
1.06.1.2.3
Throughput
The throughput of a non-recursive algorithm is in principle unlimited. In practice, however, the amount
of circuitry as well as the inherent speed of the used CMOS technology becomes a limit. Recursive
algorithms have an inherent limit on the throughput.

250
CHAPTER 6 Digital Filter Structures and Their Implementation
0
50
100
150
200
-0.1
-0.05
0
0.05
0.1
x
0
50
100
150
200
-500
0
500
yQ
FIGURE 6.2
Persistent parasitic overﬂow oscillation.
1.06.1.2.4
Robustness
Recursive algorithms requires that the data wordlength is rounded or truncated at least once in every
recursive loop that contain at least one multiplication with a non-integer coefﬁcient. Otherwise the data
wordlength would increase to inﬁnity. Rounding/truncation and overﬂow of the available signal range
are non-linear operations that will in most cases cause parasitic oscillations. That is, the output of the
ﬁlter will contain a periodic or even a chaotic signal component that is not due to the input signal
[13,14]. In fact, this unwanted distortion can be very large. Of course, this is highly undesirable and
should be avoided. Figure 6.2 shows an example of the input and output of a recursive loop when the
input signal overﬂows the signal range just before it becomes zero. Obviously this is not an acceptable
behavior. We will later discuss a class of ﬁlter structures, i.e., wave digital ﬁlters, that have the property
to suppress such undesirable oscillations.
1.06.1.3 Design of digital ﬁlters
The design of digital ﬁlters can be done along several different routs.
1.06.1.3.1
Design and realization of FIR ﬁlters
If the ﬁlter has a ﬁnite-length impulse response (FIR), then the frequency response is typically designed
directly in the z-domain using computer-based optimization methods. In Section 1.06.2 we will brieﬂy
discuss the design of classical FIR ﬁlters and associated structures, which are suitable for cases when the
transition band is not too small. Unfortunately, for requirements with narrow transition band, the ﬁlter

1.06.1 Introduction
251
Digital
Specification
Analog
Specification
⇒
⇒
⇒
Synthesis of lumped element filter
Synthesis of commensurate-length
transmisson line filter
Synthesis of wave digital filter
⇒
⇒
Synthesis of H(s)
⇒
FIGURE 6.3
Design process for wave digital ﬁlters.
order and arithmetic workload becomes very large for FIR ﬁlters. In such cases, we instead recommend
recursive structures (IIR), frequency-masking techniques, which are discussed in Section 1.06.8, or
multirate techniques [14–16].
1.06.1.3.2
Design and realization of IIR ﬁlters
In the case of inﬁnite-length impulse response (IIR) ﬁlters there are many options available. For complex
and non-standard requirements the frequency response is often determined directly in the z-domain using
computer-based optimization methods [14,17].
For frequency selective ﬁlters with standard requirements, i.e., ﬁlters with a passband that approxi-
mate a constant gain in the passband, it is favorable to employ the vast available knowledge of design
of analog ﬁlters. There are a number of advantages of this approach, which is illustrated in Figure 6.3:
•
Accurate numerical algorithms have been developed for analog ﬁlters [18–21].
•
Classical analog attenuation approximations can be converted using the bilinear transformation to
their digital counterparts. The bilinear transformation [14,22–24] between the s- and z-domains is
s = 2
T
z −1
z + 1.
(6.4)
Optimal highpass ﬁlters can be designed by frequency transformation of an optimal lowpass ﬁlter.
Near optimal bandpass and bandstop ﬁlters can also be designed by frequency transformations. However,
we will take an indirect route that will yield a low-sensitive digital ﬁlter algorithm:
•
Once the analog transfer function, H(s), (frequency response) is determined, it is realized using
a lumped element (LC) ﬁlter structure, e.g., a ladder structure. We will later show that a properly
designed LC ﬁlter will have optimal element sensitivity in the passband. Unfortunately, this structure
is not possible to simulate in the z-domain, because delay-free loops occur.
•
To circumvent this problem, we use an analogy between a lumped element network and
commensurate-length transmission line network. The later contain sources, resistors, and lossless
transmission lines of equal length. The transmission line network has a periodic frequency response
and there is a one-to-one mapping between the continues-time transmission line network and it
digital counterpart. We will later discuss this in more detail.

252
CHAPTER 6 Digital Filter Structures and Their Implementation
•
The digital algorithm is obtained by using a wave description, i.e., wave digital ﬁlters [25–27],
consisting of a linear combination of voltages and current in the analog network. This allows us to
simulate power in the analog ﬁlter, which we later will show is a necessity to obtain a low-sensitive
algorithm.
•
Moreover, the concept of pseudo-power in the digital domain allows us to suppress parasitic oscil-
lations. That is, there exists a Lyapunov function for wave digital ﬁlters.
We will therefore describe various steps in the design process, illustrated in Figure 6.3, and options
the designer has to minimize the implementation cost. First, we discuss in Section 1.06.3, the analog
approximation problem and show that classical design recommendations are not always the best. In
Section 1.06.4, we discuss the realization of the analog transfer function using a doubly resistively
terminated network. We particularly discuss what the designer can do in order to minimize the element
sensitivity. We also discuss the realization of commensurate-length transmission line ﬁlters. In Sections
1.06.5 and 1.06.6 we discuss two of the most common analog ﬁlter networks and their properties.
In Section 1.06.7, we present the basic principles of wave digital ﬁlters and their building blocks. In
addition we compare two types of wave digital ﬁlter realizations using an example. In Section 1.06.8
we discuss techniques that are particularly efﬁcient for ﬁlters with very narrow transition bands. These
techniques may employ both FIR and IIR structures, but ﬁrst we discuss in the next section classical FIR
ﬁlters. Finally, in Sections 1.06.9–1.06.12, we will discuss some computational properties of algorithms
and some techniques for their implementation.
1.06.2 Digital FIR ﬁlters
Digital ﬁlters can be categorized into FIR (Finite-length Impulse Response) and IIR (Inﬁnite-length
Impulse Response) ﬁlters. Advantages of FIR ﬁlters over IIR ﬁlters are that they can be guaranteed to
be stable and may have a linear-phase response. Further, they require generally shorter data wordlength
than the corresponding IIR ﬁlters. However, FIR ﬁlters usually require much higher orders than IIR
ﬁlters for the same magnitude speciﬁcation and they often introduce a large group delay, that make
them unsuitable in many applications. FIR ﬁlters are well suited for many multirate ﬁlter realizations.
An advantage is that FIR ﬁlters are always stable, except when they are used inside a recursive loop.
High-order FIR ﬁlters can be realized efﬁciently using the FFT (Fast Fourier Transform) [14,28–30]
and in some cases using multirate techniques [15,16,31–33].
The most common method to design a linear-phase FIR ﬁlter, that satisﬁes a speciﬁcation of the
magnitude response, is to use a numeric optimization procedure to determine the impulse response h(n)
in (6.5). A widely used algorithm for designing linear-phase FIR ﬁlters with multiple passbands and
stopbands is that of McClellan, Parks, and Rabiner [17,30,34,35].
The impulse response h(n) of a causal FIR ﬁlter of order N (length L = N + 1) is nonzero for
0 ≤n ≤L −1 and zero for all other values of n as illustrated in Figure 6.4.
The transfer function and frequency response of an Nth order FIR ﬁlter is
H(z) =
N

n=0
h(n)z−n.
(6.5)

1.06.2 Digital FIR Filters
253
h(n)
n
L–1
0
FIGURE 6.4
Impulse response of a causal FIR ﬁlter of order N (length L = N + 1).
The poles of the transfer function can only be placed at the origin of the z-plane for nonrecursive
FIR ﬁlters. The zeros can be placed anywhere in the z-plane, but they are usually located on the unit
circle or as pairs that are mirrored in the unit circle.
Of main interest among FIR ﬁlters are ﬁlters with linear-phase for which the impulse response
exhibit symmetry or antisymmetry. Linear-phase FIR ﬁlters are widely used in digital communication
systems, speech and image processing systems, spectral analysis and particularly in applications where
nonlinear phase distortion cannot be tolerated. Most ﬁlter design software packages support the design
of linear-phasePlease note that the Reference number seem to be missing. FIR ﬁlters [17,30,34,35].
A linear-phase FIR ﬁlter is obtained by letting the impulse response exhibit symmetry around
n = N/2, i.e.,
h(n) = h(N −n)
(6.6)
or antisymmetry around n = N/2, i.e.,
h(n) = −h(N −n),
(6.7)
for n = 0, 1, . . . , N. Since the order N is either even or odd, there are four different types of FIR ﬁlters
with linear phase. These are denoted as Type I, II, III, IV linear-phase FIR ﬁlters, respectively, according
to the following:
Type I
h(n) = h(N −n)
N even,
Type II
h(n) = h(N −n)
N odd,
Type III
h(n) = −h(N −n)
N even,
Type IV
h(n) = −h(N −n)
N odd.
Figure 6.5 shows typical impulse responses for the different cases. Note that the centre value h(N/2)
is always equal to zero for Type III ﬁlters. Also, when N is even, the point of symmetry is an integer
corresponding to one of the sample values. When N is odd, the point of symmetry lies between two
sample values.
A special case, is so-called half-band ﬁlters, where about half of the impulse response values are zero
and, hence, they do not require any arithmetic operations. A half-band ﬁlter has a frequency response
that is antisymmetric around ωT = π/2. Hence, ωcT + ωsT = π.
Figure 6.6 show a typical speciﬁcation of the zero-phase response for a lowpass linear-phase FIR
ﬁlter. Several approximate expressions for the required ﬁlter order for equiripple, linear-phase FIR ﬁlters
have been determined empirically.

254
CHAPTER 6 Digital Filter Structures and Their Implementation
n
L
h(n)
Type I, N even
n
L
h(n)
Type II, N odd
n
h(n)
Type III, N even
n
L
h(n)
Type IV, N odd
L
FIGURE 6.5
Impulse responses for the four types of linear-phase FIR ﬁlters.
HR(ejωT)
ωcT
1+δc
1
1–δc
δs
–δs
ωsT
π
ωT
Passband
Stopband
Transition band
[rad]
FIGURE 6.6
Speciﬁcation of FIR ﬁlters.
One such simple estimate of the required ﬁlter order for a linear-phase lowpass (or highpass) ﬁlter
that meets the speciﬁcation shown in Figure 6.6, is
N ≈−4π
3
log(13δcδs)
ωT
,
(6.8)
where ωT = ωsT −ωcT is the transition band. Note that the required ﬁlter order will be high for
ﬁlters with narrow transition bands.
Here we are concerned with linear-phase FIR ﬁlters, that are frequency selective, i.e., lowpass,
highpass, bandpass, and bandstop ﬁlters, etc. When designing such ﬁlters it is convenient to consider

1.06.2 Digital FIR Filters
255
the zero-phase frequency response HR(e jωT ), which is deﬁned by
H(e jωT ) = HR(e jωT )e jφ(ωT ).
(6.9)
Note that |H(e jωT )| = |HR(e jωT )|.
The speciﬁcation of a lowpass ﬁlter is commonly given as
1 −δc ≤|H(e jωT )| ≤1 + δc
ωT ∈[0, ωcT ]
|H(e jωT )| ≤δs
ωT ∈[ωsT , π] .
(6.10)
For linear-phase FIR ﬁlters, the speciﬁcation of (6.10) can be restated with the aid of HR(e jωT )
according to
1 −δc ≤HR(e jωT ) ≤1 + δc
ωT ∈[0, ωcT ]
−δs ≤HR(e jωT ) ≤δs
ωT ∈[ωsT , π]
(6.11)
The McClellan-Parks-Rabiners program will ﬁnd a unique set of ﬁlter coefﬁcients that minimizes a
weighted error function. The algorithm solves the approximation problem
min

E∞

= max
E

e jωT ,
ωT ∈,
(6.12)
where E(e jωT ) is the weighted (Chebyshev) error function given by
E(e jωT ) = W(e jωT )

HR(e jωT ) −D(e jωT )

,
ωT ∈
(6.13)
with  being the union of the passband and stopband regions. In practice,  is a dense set of frequency
samples taken from the passbands and stopbands, including the bandedges. It should be stressed that
all functions involved here, HR(e jωT ), E(e jωT ), D(e jωT ), and W(e jωT ) are real functions of ωT . The
desired function D(e jωT ) is the one to be approximated by HR(e jωT ). For example, D(e jωT ), is for
the standard lowpass ﬁlter
D(e jωT ) =
	 1 ωT ∈[0, ωcT ],
0 ωT ∈[ωsT , π].
(6.14)
That is, D(e jωT ) is for conventional ﬁlters equal to one in the passbands and zero in the stopbands.
The weighting function W(e jωT ) speciﬁes the cost of the deviation from the desired function. The use
of the weighting function makes it possible to obtain different ripples in different frequency bands. The
larger the weighting function is, the smaller is the ripple. For example, for a standard lowpass ﬁlter,
W(e jωT ) is usually given as
W(e jωT ) =
	 1 ωT ∈[0, ωcT ],
δc
δs
ωT ∈[ωsT , π].
(6.15)
The speciﬁcation of (6.11) is then satisﬁed if E∞≤δc. The resulting ﬁlter is optimized in the Cheby-
shev (minimax) sense. MATLAB programs for design of FIR are found in [17,36,37]. Design of special
cases of linear-phase FIR ﬁlters, as well as non-linear phase FIR ﬁlters, is discussed in detail in [14].

256
CHAPTER 6 Digital Filter Structures and Their Implementation
1.06.2.1 FIR structures
An FIR ﬁlter can be realized using both recursive or nonrecursive algorithms. The former, however,
suffer from a number of drawbacks and should rarely be used in practice. An exception is so-called
running-sum ﬁlters, which often are use in decimation ﬁlters for 
  converters.
Nonrecursive ﬁlters are always stable and cannot sustain parasitic oscillations, except when the ﬁlters
are a part of a recursive loop. They generate little roundoff noise and can therefore be operated with
short data wordlengths. However, they usually require a large number of arithmetic operations and a
large amount of memory. Therefore large efforts have been directed towards developing methods to
reduce the arithmetic workload.
This can be done by reducing the number of multiplications per sample, or by simplifying the
coefﬁcient values so that the multiplications becomes simpler to implement. The latter approaches
results in what is referred to as multiplier-free structures, since all multiplications have been replaced
with addition/subtraction and shift operations. That is, the coefﬁcients are selected among a restricted
set of values, i.e., as a sum-of-powers-of-two, SPOT, c = ±2±n1 ± 2±n2 ± · · · ± 2±nm. Most FIR ﬁlters
can be realized using no more than four powers of two. Today the arithmetic workload for a properly
designed FIR structure is still relatively large, but the costs, i.e., chip area and power consumption,
associated with the memory is becoming more and more signiﬁcant.
1.06.2.1.1
Direct form FIR structures
There exist several structures that are of interest for the realization of FIR ﬁlters, particularly for ﬁlters
with several sample frequencies so-called multirate ﬁlters. One simple structure is the direct form
FIR, which has relatively small element sensitivity, i.e., the deviation in the frequency response due
to a small change in any of the coefﬁcients, is relatively small. This structure has many arithmetic
operations and is therefore only suitable for relatively short FIR ﬁlters.The roundoff noise variance is
(N + 1) Q2
12 if all products are rounded off. For ﬁlters with very narrow transition bands we recommend
frequency-masking structures, which are discussed later.
1.06.2.1.2
Transposed direct form
The transposition theorem [33] can be used to generate new structures that realizes the same transfer
function as the original ﬁlter structure. The transposed direct form FIR structure, shown in Figure 6.8,
is derived from the direct form structure. The throughput is higher for the transposed form because the
longest critical path is through only one multiplier and an adder, but the capacitive load at the input
node is larger, since all multipliers load this node. The transposed structure is often preferred for an
FPGA implementation.
The direct form structures requires in general N + 1 multiplications, which may become a too large
workload for high order FIR ﬁlters. However, the required amount of hardware can be signiﬁcantly
reduced, since many multiplications are performed with the same input value, or the transposed case
where many signals are multiplied and then added. The internal signal level increases from left to right
as more and more internal signals are added. Also in this structure the roundoff noise is (N + 1) Q2
12 if
all products are rounded off.

1.06.2 Digital FIR Filters
257
T
T
T
T
x(n)
h(0)
x(n–1)
h(1)
h(2)
x(n–2)
y(n)
h(N–2)
x(n–N+1)
h(N–1)
x(n–N)
h(N)
...
FIGURE 6.7
Direct form FIR structure (Transversal ﬁlter).
T
T
T
x(n)
h(1)
h(0)
y(n)
h(N–2)
h(N–1)
h(N)
v1(n–1)
vN(n–1)
vN–1(n–1)
FIGURE 6.8
Transposed direct form FIR structure.
1.06.2.1.3
Linear-phase FIR structures
A major reason why FIR ﬁlters are used in practice is that they can realize an exact linear-phase response.
Linear-phase implies that the impulse response is either symmetric or antisymmetric. An advantage of
linear-phase FIR ﬁlters is that the number of multiplications can be reduced by exploiting the symmetry
(or antisymmetry) in the impulse response, as illustrated in Figure 6.9. This structure is called a direct
form linear-phase FIR structure since the phase response is independent of the coefﬁcient values.
The number of multiplications for the direct form linear-phase structure is (N + 1)/2 for N = odd
and N/2 + 1 for N = even. The number of multiplications is thus signiﬁcantly smaller than for the
more general structures in Figures 6.7 and 6.8, whereas the number of additions is the same. Subtractors
are used instead of adders if the impulse response is antisymmetric. The signal levels at the inputs of
the multipliers are twice as large at the input signal. Hence, the input signal should be divided by two
and the ﬁlter coefﬁcients should be scaled so that a proper out signal level is obtained. The roundoff
noise is only (N + 1)Q2/24 and (N + 2)Q2/24 for N = odd and even, respectively, independently of
the ﬁlter coefﬁcients.

258
CHAPTER 6 Digital Filter Structures and Their Implementation
x(n)
h(1)
h(0)
y(n)
T
T
T
T
T
T
T
x(n–1)
h(M –2)
h(M –1)
x(n–M –1)
x(n–M)
T
T
h(N /2–1)
h(N/2)
x(n–N /2–1)
x(n–N/2)
x(n–M +1)
x(n–N)
x(n–N+1)
x(n–2)
N = odd
N = even
±
±
±
±
±
M =
2
N+1
FIGURE 6.9
Direct form linear-phase FIR structure.
A major drawback is that the group delay for linear-phase FIR ﬁlters is often too large to be useful in
many applications. Even-order FIR ﬁlters are often preferred since the group delay is an integer multiple
of the sample period.
The number of arithmetic operations is further reduced in linear-phase, half-band ﬁlters. Each zero-
valued coefﬁcient makes one multiplication and one addition redundant. The number of actual multi-
plications in a linear-phase, half-band ﬁlter is only (N + 6)/4. The order is always N = 4m + 2 for
some integer m. A half-band ﬁlter is a special case of the Nyquist ﬁlters.
1.06.2.1.4
Delay-complementary FIR structure
Even more dramatic reductions of the required amount of arithmetic operations is possible when a
pair of delay-complementary FIR ﬁlters are needed. We illustrate the basic ideas by an example. The
delay-complementary ﬁlter Hc(z) to an even-order linear-phase ﬁlter of type I or II can be obtained
from the direct form linear-phase structure by subtracting the ordinary ﬁlter output from the delayed
input value, as shown in Figure 6.10.
Odd order delay-complementary FIR ﬁlters are not feasible, since the symmetry axis, as shown in
Figure 6.5, do not coincide with a sample in the impulse response. The cost of realizing the delay-
complementary ﬁlter seems to be only one subtraction. However, the passband requirement on the FIR
ﬁlter realizing H(z) must usually be increased signiﬁcantly in order to meet the stopband requirements
on the complementary ﬁlter, Hc(z). Fortunately, a reduction of the passband ripple requires only a
slight increase in the ﬁlter order, see (6.8). Furthermore, only even-order ﬁlters are useful. Hence,
the arithmetic workload is somewhat larger than that for one single FIR ﬁlter, but is still signiﬁcantly
smaller than that for two separate ﬁlters. This technique can, of course, also be applied to linear-phase
FIR ﬁlter structures, with either symmetric or antisymmetric impulse response, and their transposes.

1.06.2 Digital FIR Filters
259
x(n)
h(1)
h(0)
y(n)
T
h(2)
T
T
T
h(N–3)
h(N–2)
h(N–1)
–
+
yc(n)
...
...
h(N/2)
T
x(n-N /2)
FIGURE 6.10
Delay-complementary FIR ﬁlter with N = even.
Complementary half-band FIR ﬁlters are particularly useful, since the saving in arithmetic workload
is substantial.
In many applications the need arises to split the input signal into two or more frequency bands.
For example, in certain transmission systems for speech, the speech signal is partitioned into several
frequency bands using a ﬁlter bank. A ﬁlter bank is a set of bandpass ﬁlters with staggered center
frequencies so that the whole frequency range is covered. The ﬁrst and the last ﬁlter are lowpass and
highpass ﬁlters, respectively. The ﬁltered signals are then processed individually to reduce the number
of bits that has to be transmitted to the receiver, where the frequency components are added into an
intelligible speech signal.
A special case of band splitting ﬁlters, a lowpass and highpass ﬁlter pair, is obtained by impos-
ing the following symmetry constraints. Let H(z) be an even order FIR ﬁlter (N even). The delay-
complementary transfer function Hc(z) is deﬁned by
|H(e jωT ) + Hc(e jωT )| = 1.
(6.16)
This requirement can be satisﬁed by two FIR ﬁlters that are related according to
H(z) + Hc(z) = z−N/2.
(6.17)
Hence, the two transfer functions are complementary. A signal is split into two parts so that if the two
parts are added they combine into the original signal except for a delay corresponding to the group delay
of the ﬁlters. Note that the attenuation is 6.02 dB at the cross-over. The output of the complementary ﬁlter
Hc(z) can be obtained, as shown in Figure 6.10, by subtracting the ordinary ﬁlter output from the central
value x(n −N/2), which reduces the arithmetic workload signiﬁcantly. However, the complementary
ﬁlter is not obtained for free, because if the stopband attenuation is large, then the passband ripple of
the ordinary ﬁlter must be very small. Hence, the ﬁlter order must be increased. The complementary
FIR ﬁlters considered here cannot be of odd order since the center value of the impulse response in that
case is not available.

260
CHAPTER 6 Digital Filter Structures and Their Implementation
b11
b01
b21
T
T
y(n)
x(n)
b12
b221
T
T
b1M
b2M
T
T
b02
b0M
FIGURE 6.11
FIR ﬁlter in cascade form.
1.06.2.1.5
Cascade form FIR structures
High-order FIR and IIR ﬁlters may be realized in cascade form, i.e., as a cascade of several lower-order
subﬁlters, as shown in Figure 6.11 for an FIR ﬁlter. The overall transfer function is
H(z) =
M

k=1
Hk(z),
(6.18)
where the subﬁlters Hk(z) usually have order one, two, or four. Cascading of two minimum-phase ﬁlters
yields an overall minimum-phase ﬁlter. Moreover cascading linear-phase ﬁlter also results in an overall
linear-phase ﬁlter.
The beneﬁt of cascading several ﬁlters is that the stopband attenuations of the ﬁlters adds. Hence, each
subﬁlter need only to contribute with a small fraction of the overall stopband attenuation and they can
therefore be simpliﬁed. The coefﬁcients in the subﬁlters need not to be as accurate, i.e., the coefﬁcient
sensitivity in the stopband is small. The beneﬁt obtained in the stopband sensitivity is, however, offset
by an increased sensitivity in the passband since errors in the lower-order ﬁlters add. Furthermore, the
cascade form suffers from a decrease in the dynamic signal range. Hence, the internal data wordlength
must be increased. The ordering of the sections is important, since it affects dynamic signal range
signiﬁcantly. There is M! possible ways to order the sections for an FIR ﬁlter realized with M sections.
A minimum-phase ﬁlter can be partitioned into a linear-phase ﬁlter and a ﬁlter with nonlinear phase
in cascade form [14].
1.06.2.1.6
Lattice FIR structures
LatticeFIRstructuresareusedinawiderangeofapplications,forexample,speechandimageprocessing,
adaptive ﬁlters, and ﬁlter banks [31]. The lattice structure can be of FIR or IIR type. Note, however,
the lattice structures discussed in this section are a different type of structure compared to the lattice
wave digital ﬁlter structures that will be discussed in Section 1.06.7.9, although confusingly they have
the name lattice in common.

1.06.2 Digital FIR Filters
261
x(n)
h(1)
h(0)
y(n)
h(N–2)
h(N–1)
h(N)
HM(z)
HM(z)
HM(z)
MT
MT
MT
FIGURE 6.12
FIR ﬁlter with identical subnetworks.
1.06.2.1.7
FIR ﬁlters with identical subnetworks
Saramäki has developed a general theory for the design of structural frequency transformation of linear-
phase FIR ﬁlters. The resulting structures consist of a tapped cascaded interconnection of identical FIR
subﬁlters [37–39]. Figure 6.12 shows an example of these structures where the subﬁlters are identical,
even-order linear-phase ﬁlters.
1.06.2.1.8
Recursive frequency-sampling structures
Traditionally, most textbooks [24,29,40] contain a discussion of so-called frequency-sampling FIR
ﬁlter structures. These realizations are recursive algorithms that rely on pole-zero canceling techniques.
Although they may seem to be interesting they are not recommended due to their high coefﬁcient
sensitivity, low dynamic range, and severe stability problems [14].
1.06.2.1.9
Shifted permuted difference coefﬁcient structure
The PDC approach was proposed by Nakayama [41]. This method was proposed for implementation
of FIR ﬁlters, but it can be used to compute any sum-of-products. The basic idea is to exploit the fact,
that the adjacent values in the impulse response have similar magnitudes, to simplify the multiplier
network into a network consisting of only add/subtractors and shift operations. The shifted permuted
difference method (SPDC) [14,42] is a generalization of the PDC method that yields fewer arithmetic
operations. These approaches are simple alternatives to using more sophisticated multiple-constant
multiplier techniques, which often yields better results. Multiple-constant multiplier techniques are
discussed further in Section 1.06.12.1.
1.06.2.1.10
Multirate FIR ﬁlters
Multirate ﬁlters are efﬁcient in terms of arithmetic workload since a large part of the arithmetic oper-
ations can be performed at a lower rate [15,16]. Polyphase structures are efﬁcient for decimation
and interpolation since redundant arithmetic operations are avoided. FIR and IIR ﬁlters with nar-
row transition bands can be realized by methods based on decimation-ﬁltering-interpolation schemes
[14]. Common to these techniques is that the effective number of arithmetic operations per sample is
reduced.

262
CHAPTER 6 Digital Filter Structures and Their Implementation
A(ω)
Amin
Amax
ωc
ωs
ω
Passband
Transition band
Stopband
[dB]
FIGURE 6.13
Standard speciﬁcation of a digital lowpass ﬁlter.
1.06.3 The analog approximation problem
Frequency selective ﬁlters are speciﬁed in the frequency domain in terms of an acceptable deviation from
the desired behavior of the magnitude or attenuation function. Figure 6.13 shows a typical attenuation
speciﬁcation for an analog lowpass ﬁlter. The variation (ripple) in the attenuation (loss) function in the
passband may not be larger than Amax and the attenuation in the stopband may not be smaller than
Amin. Usually the acceptable tolerances are piecewise constant. It is convenient during the early stages
of the ﬁlter design process to use a normalized ﬁlter with unity gain, i.e., the minimum attenuation is
normalized to 0 dB. The ﬁlter is provided with the proper gain in the later stages of the design process.
1.06.3.1 Typical requirements
Passband: The frequency band occupied by the wanted signal is referred to as the passband. In
this band the ideal requirement for the ﬁlter is to provide constant loss and constant group delay
so that the wanted signal will be transmitted with no distortion. The passband frequency and the
acceptable tolerance Amax for an analog lowpass ﬁlter is from 0 to ωc.
Stopband: The frequency bands occupied by the unwanted signals are referred to as the stopbands.
In these bands the common form of speciﬁcation merely requires the attenuation relative to the
lower limit set for the passband, to be equal to or greater than some minimum amount. The stopband
begins at ωs for an analog lowpass ﬁlter and extends towards inﬁnity. For a digital lowpass ﬁlter
the stopband begins at ωsT and extends to π rad. It is normally of no interest whether the actual
ﬁlter loss in the stopband exceeds the speciﬁed amount by 1 dB or even 20 dB, and in this sense

1.06.3 The Analog Approximation Problem
263
stopband attenuations have only lower limits, in contrast to passbands where there are both upper
and lower limits.
Transition band: The transition band is from ωc to ωs. There are no requirements on the attenuation
in the transition band. The loss function of a ﬁlter is a continuous function of frequency and is
thus unable to have jump discontinuities. For this reason there must always be some interval in
the frequency spectrum, separating the edge of the passband from the edge of the stopband, in
which the loss can raise from the low value in the passband to that required in the stopband. The
bandwidth allocated to the transition band is one of the main factors determining the order of the
ﬁlter needed to meet the speciﬁcation. Moreover, as the width of the transition bands are decreased,
not only does the complexity of the ﬁlter increase, but it becomes also more difﬁcult to meet the
speciﬁcation for the passband. Narrower transition bands mean that the attenuation has to change
more rapidly near the passband edges, and this causes the passband response to be more sensitive
to both component losses and component tolerances.
Phase response: Linear-phase or near linear-phase response is often desirable or even required in
many ﬁlter applications. However, this is usually speciﬁed in terms of the group delay.
Group delay: For the same reason that one cannot get exactly constant loss over a ﬁnite band, it is
also impossible to get exactly constant group delay over a ﬁnite band with lumped element ﬁlters.
However it is possible to get exactly linear-phase with ﬁlters built with distributed elements and
with digital FIR ﬁlters.
Impulse response: The impulse response is mainly used as a theoretical concept and very rarely
the impulse response is speciﬁed.
Step response: In most applications frequency domain characteristics such as phase and group
delay requirements must be met, but in some cases additional requirements in the time domain are
used. For example, step response and intersymbol interference requirements.
1.06.3.2 Standard lowpass approximations
Many ﬁlter approximations, have been developed to meet different requirements, particularly for analog
ﬁlters. The main work has focused on approximations of lowpass ﬁlters, since highpass, bandpass, and
bandstop ﬁlters can be obtained from lowpass ﬁlters through frequency transformations [18,20,43]. It
is also possible to use these results to design digital ﬁlters. The classical lowpass ﬁlter approximations
(standardapproximations),whichcanbedesignedbyusingmoststandardﬁlterdesignprograms[21]are:
Butterworth: The magnitude function is maximally ﬂat at the origin and monotonically decreasing
in both the passband and the stopband. The group delay and variation within the passband is
large. This approximation requires a larger ﬁlter order to meet a given speciﬁcation than the ﬁlter
approximations discussed below.
Chebyshev I: The magnitude function has equal ripple in the passband and decreases monotonically
in the stopband. The group delay and the variation of the group delay within the passband is
somewhat less than for a Butterworth approximation that meets the same attenuation requirement.
A lower ﬁlter order is required compared to the Butterworth approximation.
Chebyshev II (Inverse Chebyshev): The magnitude function is maximally ﬂat at the origin, decreases
monotonically in the passband, and has equal ripple in the stopband. The group delay is smallest

264
CHAPTER 6 Digital Filter Structures and Their Implementation
of the four approximations and it has the smallest variation within the passband. The same ﬁlter
order is required as for the Chebyshev I approximation.
Cauer: The magnitude function has equal ripple in both the passband and the stopband. The group
delay and its variation is almost as low as the for Chebyshev II approximation. The Cauer ﬁlter,
also called elliptic ﬁlter, requires the smallest order to meet a given magnitude speciﬁcation.
1.06.3.2.1
Comparison of the standard approximations
In comparing the standard approximations Butterworth, Chebyshev I, Chebyshev II, and Cauer ﬁlters,
we ﬁnd that the two latter has lower variations in the group delay. In literature it is often stated that
Cauer ﬁlters have larger variation in the group delay than, i.e., Butterworth ﬁlters and that this is a
valid reason for using the Butterworth approximation. The mistake in this argument is that the two
ﬁlter approximations are compared using the same ﬁlter order. This is obviously not correct, which is
evident of the following example, since Cauer ﬁlters can handle a considerably stricter requirement
on the magnitude function. Even the step response for a Cauer ﬁlter is better. The difference between
Chebyshev II and Cauer ﬁlters is however relatively small, the former has a somewhat smaller group
delay, but the order is on the other hand larger.
1.06.3.2.2
Example 1
Compare the Butterworth, Chebyshev I, Chebyshev II, and Cauer approximations, which meet the
same standard LP speciﬁcation: Amax = 0.00695 dB (ρ = 4%), Amin = 45 dB, ωc = 1 rad/s and
ωs = 2 rad/s.
As will be further discussed in Section 1.06.4.5, Amax has been chosen very small in order to achieve
low passband sensitivity and we may use components with large tolerances ε at the expense of a slightly
higher ﬁlter order.
We get the following ﬁlter orders with the four standard approximations:
Butterworth: NB = 12.12 ==> NB = 13,
Chebyshev I and Chebyshev II: NC = 6.904 ==> NC = 7,
Cauer: NCa = 4.870 ==> NCa = 5.
Note the large difference between the required orders for the standard approximations that meet the
same requirement. The difference tends to increase if the transition band is reduced.
Figure 6.14 shows the attenuation for the four approximations. The allowed passband ripple is very
small and we are only interested in that the requirement is met, and not in detailed variation inside the
passband.
The attenuation in the transition band and the stopband varies between the different ﬁlters. The
Chebyshev II ﬁlter has a more gradual transition between the passband and stopband compared with
the Chebyshev I ﬁlter in spite of the fact that they have the same order. Note that the Cauer ﬁlter has
a smaller transition band than required. The attenuation approaches, in this case, i.e., odd order ﬁlters,
inﬁnity for all of the ﬁlters. Figure 6.15 shows the corresponding group delays. The difference in the
group delay is large between the different approximations and the group delays maximum lies above
the passband edge ωc = 1 rad/s. The Butterworth and Chebyshev I ﬁlters have larger group delay
in the passband while the Chebyshev II and Cauer ﬁlters have considerably smaller group delay and

1.06.3 The Analog Approximation Problem
265
0
1
2
3
4
5
6
7
8
9
10
0
20
40
60
80
ω   [rad/s]
A(ω)   [dB]
Butterworth
Chebyshev I
Chebyshev II
Cauer
FIGURE 6.14
Attenuation for the four standard approximations.
0
0.5
1
1.5
2
2.5
3
0
2
4
6
8
10
12
14
ω   [rad/s]
τg (ω)   [s]
Butterworth
Chebyshev I
Chebyshev II
Cauer
FIGURE 6.15
Group delays for the four approximations.
the difference between the latter two is relatively small. In literature, it is commonly stated that the
Butterworth ﬁlter has the best group delay properties. This is obviously not correct and is based on an
unfair comparison between the standard approximations of the same order. According to Figure 6.15,
Chebyshev II and Cauer ﬁlters have considerably lower group delays.
The element sensitivity for an LC ﬁlter is proportional to the group delay. The group delays at
the passband edge and the passband variations are shown in Table 6.1. Notice that the Chebyshev II

266
CHAPTER 6 Digital Filter Structures and Their Implementation
Table 6.1 Group Delays [s]
τg(ωc)
τg(ωc) −τg(0)
Butterworth
8.87115
2.39415
Chebyshev I
8.15069
3.4990
Chebyshev II
3.36427
1.1934
Cauer
4.41338
2.0718
approximation have the smallest variation. We will in Section 1.06.4.5 discuss the affect of the group
delay on the sensitivity of a class of ﬁlter structures.
1.06.3.3 Frequency transformations
Optimal highpass ﬁlters can be derived from an optimal lowpass ﬁlter using frequency transformations
[18,20,43]. However, the frequency transformations yield suboptimal bandpass and bandstop ﬁlters.
Optimal bandpass and stopband ﬁlters require more advanced methods [18,20,21,43].
1.06.4 Doubly resistively terminated lossless networks
Passive LC ﬁlters belongs to the oldest implementation technologies, but they still plays an important
role since they are being used as prototypes for the design of advanced frequency selective ﬁlters.
A drawback with LC ﬁlters is that it is difﬁcult to integrate resistors and coils with sufﬁciently high
quality in integrated circuit technology. LC ﬁlters are for this reason not well suited for systems, that
are implemented in an integrated circuit.
A more important aspect is, that LC ﬁlters are used as basis for realizing high-performance frequency
selective ﬁlters. This is the case for mechanical, active, discrete-time, and SC ﬁlters as well as for digital
ﬁlters. Examples of such methods are: immitance simulation using generalized immitance converters
and inverters, Gorski-Popiel’s, Bruton’s, wave active ﬁlters, and topological simulations methods like
leapfrog structures [20]. The main reason is, that the magnitude function for a well-designed LC ﬁlter
has low sensitivity in the passband for variations in the element values. It is important that the reference
ﬁlter is designed for low sensitivity, since the active and digital ﬁlters inherits its sensitivity properties.
In fact, the sensitivity properties of the reference ﬁlter become a “lower” bound for the active and digital
counterparts.
1.06.4.1 Maximal power transfer
To explain the good sensitivity properties of correctly designed LC ﬁlters, we ﬁrst consider the power
transferred from the source to the load in the circuit shown in Figure 6.16. A lossless reciprocal network
can be realized by using only lossless circuit elements, e.g., inductors, capacitors, transformers, gyrators,
and lossless transmission lines. Although, these ﬁlters are often referred to as LC ﬁlters. The maximal
power that can be transferred to the load, which occur when the input impedance to the lossless network

1.06.4 Doubly Resistively Terminated Lossless Networks
267
Lossless
Network
+
–
Rs
RL
Vin
FIGURE 6.16
Doubly resistively terminated lossless network.
equals Rs, is
PLmax = |Vin|2
4Rs
.
(6.19)
The ratio of the output power and the maximal output power is
Pout
Poutmax
= 4Rs
RL

Vout( jω)
Vin( jω)

2
≤1,
(6.20)
where Vin( jω) is a sinusoidal input signal. An important observation is that the power, that the signal
source can deliver to the load, is limited. The upper bound for the maximal power transfer is the base
for the design of ﬁlter structures with low element sensitivity.
We deﬁne the frequency response as the ratio between input and output voltages, i.e., the relation
between signal quantities and corresponding physical signal carrier, according to
H( jω) =

4Rs
RL
Vout( jω)
Vin( jω) .
(6.21)
It is convenient to normalize the magnitude response to |H( jω)| ≤1.
1.06.4.2 Reﬂection function
Note that the signal source, shown in Figure 6.16, does not deliver maximal power for all frequencies,
since the input impedance to the reactance network is in general not equal to Rs. This can be interpreted
as a fraction of the maximally available power is reﬂected back to the source. The relationship between
the power that is absorbed in RL and the power, which is reﬂected back to the source, is illustrated in
Figure 6.17. Feldtkeller’s equation, which is based on the assumption that the source delivers constant
power, relate the power delivered to the load and the power reﬂected back to the source using the
reﬂection function, ρ( jω). We have
4Rs
RL
|H( jω)|2 + |ρ( jω)|2 = 1.
(6.22)
The reﬂection function is deﬁned
ρ( jω) = Z( jω) −Rs
Z( jω) + Rs
,
(6.23)

268
CHAPTER 6 Digital Filter Structures and Their Implementation
1
0
ω
ωr0
ωr1
ωr2
ωz1
ωz2
ωz
∞
P2
P imax
Maximum Power Transferred
No Power Transferred
Pr
P imax
Reflected Power
FIGURE 6.17
Transferred and reﬂected power.
where Z( jω) is the input impedance to the lossless network in Figure 6.16. The magnitude of the
reﬂection function will be small in the passband. We will later show that the reﬂection function is a key
to design of low sensitive ﬁlter structures.
1.06.4.3 Element sensitivity
A measure of sensitivity is relative sensitivity of the magnitude function
|H( jω)|
Sx
=
∂|H( jω)|
|H( jω)|
∂x
x
.
(6.24)
It is difﬁcult to ﬁnd a simple and good measure of how the attenuation changes, when several circuit
element varies at the same time. The reason for this is that the inﬂuence of errors in different element
values interacts. In fact, for a doubly resistively terminated reactance network we will demonstrate, that
they tend to cancel. We shall therefore use and interpret sensitivity measures according to (6.24) with
care. It is very difﬁcult to compare different ﬁlter structures in a fair way.
1.06.4.4 Passband sensitivity
The sensitivity of, for example, the magnitude function with respect to a circuit element, x, is a function
of the angular frequency. The sensitivity in the passband can be determined from the derivative of the
Feldtkeller’s equation with respect to an arbitrary circuit element, x. We get
|H( jω)|
Sx
= −RL
4Rs

ρ( jω)
H( jω)
 |ρ( jω)|
Sx
.
(6.25)
For a doubly resistively terminated LC ﬁlter we have
|ρ1( jω)| = |ρ2( jω)| =

100.1Amax −1,
(6.26)

1.06.4 Doubly Resistively Terminated Lossless Networks
269
where Amax is the acceptable ripple in the passband. Hence, the magnitude of the reﬂection function
will be small in the passband if Amax is small.
Fettweis showed (1960) that the sensitivity becomes minimal if the ﬁlter is designed for maximal
power transfer at a number of angular frequencies in the passband. At these angular frequencies, the
reﬂection function ρ( jω) is zero, since input impedance to the network in Figure 6.16 is Z( jω) = Rs.
The sensitivity at these frequencies is therefore, according to (6.25), zero. If Amax is small, both the
reﬂection coefﬁcient, according to (6.23) and the magnitude of the reﬂection function |ρ( jω)|, according
to (6.26), will be small throughout the passband. Hence, the sensitivity will be small. If the ripple is
decreased in the passband, the sensitivity is also decreased.
The fact that a doubly resistively terminated LC ﬁlter has low element sensitivity can also be realized
through the following reasoning. Irrespective of if the element value is increased or decreased from
its nominal value, Pout will decrease, since Pout = Poutmax for the nominal element value. Since the
derivative is zero where the function has a maximum, i.e., for ω = ωk with nominal element values.
If there are many angular frequencies, ωk, with maximal power transfer, the sensitivity will be low
throughout the passband. This line of reasoning is referred to as Fettweis-Orchard’s argument [20,44].
1.06.4.5 Errors in the elements in doubly terminated ﬁlters
Figure 6.18 shows the typical deviation in the attenuation for a doubly resistively terminated LC ﬁlters
due to errors in the reactive elements.
It can be shown that the deviation in the passband attenuation, as shown in Figure 6.18, for a doubly
resistively terminated ﬁlter is [45].
A(ω) ≤8.69ε |ρ( jω)|
|H( jω)|2 ωτg(ω),
(6.27)
where e = |L/L| = |C/C| represent the uniformly distributed errors in the inductors and the
capacitors, i.e., (1 −ε)L ≤L ≤(1 + ε)L, etc. It can be shown that A(ω) is proportional to
the electric and magnetic energy stored in the capacitors and inductors and that (6.27) also holds
for commensurate-length transmission line ﬁlters as well. The deviation will, according to (6.27), be
largest for frequencies where ωτg(ω) is largest, since the reﬂection function, |ρ( jω)|, is small and
|H( jω)| ≈1 in the passband. Hence, a doubly resistively terminated ﬁlter with 3 dB ripple in the
passband is signiﬁcantly more sensitive for element errors, than a ﬁlter with smaller passband ripple,
e.g., 0.01 dB. Moreover, the Q factors of the poles will be small if the passband ripple is small. Thus, it
is often better to design a ﬁlter with a small ripple at the expense of a slightly higher ﬁlter order. Note
that (6.27) is not valid for singly resistively terminated ﬁlters.
1.06.4.6 Errors in the terminating resistors
The sensitivities with respect to Rs and RL are proportional to the reﬂection function. Hence, the
sensitivities are small in the passband, since |ρ( jω)| ≪1, and equals zero for the frequencies at
maximal power transfer. In addition, the errors will essentially appear as a small change in the gain of
the ﬁlter and not affect the frequency selectivity.

270
CHAPTER 6 Digital Filter Structures and Their Implementation
A(ω)
ΔA = |A(ω) – ANom (ω)|
ω
ANom (ω)
A(ω)
FIGURE 6.18
Deviation in the attenuation due to element errors.
0
0.2
0.4
0.6
0.8
1
0
0.5
1
A(ω)  [dB]
ω  [rad/s]
0
0.2
0.4
0.6
0.8
1
0
0.5
1
ΔA(ω)  [dB]
ω  [rad/s]
1
1.5
2
2.5
3
3.5
4
30
35
40
45
50
A(ω)  [dB]
ω  [rad/s]
FIGURE 6.19
Deviation in the passband attenuation (top), bound on the attenuation (middle), and deviation in the stopband
attenuation (bottom).
1.06.4.7 Effects of lossy elements
The effect on the attenuation of lossy reactive elements can be estimated in terms of their Q factors,
where we assume that all inductor have the same Q factor and the same holds for the capacitors [43,45].
A(ω) = 8.69
2
 1
QL
+
1
QC

ωτg(ω) + 8.69
2
 1
QL
−
1
QC

|ρ|max.
(6.28)

1.06.4 Doubly Resistively Terminated Lossless Networks
271
Also in this case it is favorable to select a small passband ripple. The deviation will be largest at the
passband edge, i.e., where ωτg(ω) is largest. However, in the case of wave digital ﬁlters, which will be
discussed later, the components do not have any losses.
1.06.4.7.1
Example 2
Consider a ﬁfth-order Chebyshev I ﬁlter with Amax = 0.5 dB, Amin = 42 dB, ωc = 1 rad/s, and
ωs = 2 rad/s and we assume that the components errors are uniformly distributed with ε = ±1%.
Figure 6.19 shows the deviations in the attenuation in the passband and stopband as well as the bound
on the deviation according to (6.27).
A signiﬁcant number of ﬁlters do not meet the speciﬁcation, since the design margin is very small. In
practice,adesignmarginshouldbeallocatedtothetwobandsaswellastothebandedges[46].Obviously,
the deviation increases towards the passband edge while it is insigniﬁcant at low frequencies. Moreover,
the sensitivity at the bandedge is large, and the cutoff frequency is sensitive to component errors.
1.06.4.8 Filters with diminishing ripple
Due to deviations in the attenuation, caused by errors in the element values, a part of the allowed ripple
in the passband, Amax must be reserved to allow for errors in the component values. The ﬁlter must
therefore be synthesized with a design margin, i.e., with a ripple, which is less than required by the
application. According to (6.27), the deviation is smaller for low frequencies and increases towards
the passband edge. In practice, however, in order to simplify the synthesis, the design margin is for
the standard approximations distributed evenly in the passband, even though the margin will not be
exploited for lower frequencies. In order to exploit the allowed passband ripple better, we may let the
reﬂection function |ρ( jω)| of the synthesized ﬁlter decrease at the same rate as the other factors in
A(ω) increases so that A(ω) + A(ω) ≤Amax, as shown in Figure 6.20. The ripple will decay
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
A(ω)
Amax
A(ω)+ΔA(ω)
FIGURE 6.20
Passband attenuation for a ﬁfth-order Chebyshev I ﬁlter with diminishing ripple and equiripple A(ω) +
A(ω) ≤Amax (green) (For interpretation of the references to color in this ﬁgure legend, the reader is
referred to the web version of this book.).

272
CHAPTER 6 Digital Filter Structures and Their Implementation
towards the passband edge and the corresponding LC ﬁlter can be implemented with components with
larger tolerances, i.e., the ﬁlter can be implemented with a lower overall cost. The group delay of a ﬁlter
with diminishing ripples is slightly smaller than for the original ﬁlter. An additional advantage is that
this will reduce the thermal noise as well [47].
1.06.4.8.1
Example 3
The deviation due to losses and component errors, for the ﬁlters in Example 1, with QL > 200 and QC >
50, are shown in Table 6.2.
Using a Chebyshev II or Cauer ﬁlter, instead of a Butterworth ﬁlter, reduces the group delay with
a factor of 2.64 and 2, respectively. Hence, the component tolerances can be increased with the same
factor. An additional improvement of a factor 2 to 3 can be obtained by using a diminishing ripple
approximation, that allocate a larger design margin and reduces the sensitivity at the upper part of the
passband. Components, with large tolerances, are considerably cheaper than those with small tolerances.
In addition, the number of components is fewer; 9 and 7 compared to 13 for Butterworth. Therefore it is
important to use an approximation with small group delay. Cauer is often the preferred approximation,
since the require order is signiﬁcantly lower than for Chebyshev II and the group delay is almost a low.
In addition, Chebyshev II or Cauer ﬁlters have lower Q factors as well. The Q factors for the four
ﬁlters are shown in Table 6.3.
Table 6.2 Deviation in the Attenuation
A(ω) according to (6.28)
A(ω) according to (6.27)
Butterworth
0.4080 ϵ dB
3.0880 ϵ dB
Chebyshev I
0.2479 ϵ dB
2.8380 ϵ dB
Chebyshev II
0.1023 ϵ dB
1.1713 ϵ dB
Cauer
0.1342 ϵ dB
1.5365 ϵ dB
Table 6.3 Q Factors for the Four Filters
Butterworth
Chebyshev I
Chebyshev II
Cauer
N = 13
N = 7
N = 7
N = 5
0.50000
0.50000
0.50000
0.50000
0.51494
0.68955
0.60835
0.83127
0.56468
1.33370
1.03162
3.12162
0.66799
4.34888
3.19218
0.88018
1.41002
4.14812

1.06.5 Ladder Structures
273
The conclusion is that Cauer is the best approximation in most cases, i.e., when we have requirements
on both the attenuation and group delay. In addition, the Cauer approximation yields LC ﬁlter with fewer
components and with almost as low sensitivity to errors in the element values as Chebyshev II ﬁlters.
1.06.4.9 Design of doubly resistively terminated analog ﬁlters
Instead of using expensive components with low tolerances and large Q factor, we can compensate
for an increase in ε, i.e., using components with larger tolerances, using some or all of the following
possible trade-offs:
•
Use a doubly resistively terminated reactance network that is designed for maximal power transfer,
i.e., (6.27) is valid.
•
Reduce |ρ( jω)| by reducing the passband ripple, Amax, of the ﬁlter more than required by the
application. However, this requires that the ﬁlter order is increased. That is, we can use a few more,
but cheaper components to reduce the overall cost of the implementation.
•
Use an approximation that have low group delay, i.e., Chebyshev II and Cauer ﬁlters are preferred
over Butterworth and Chebyshev I ﬁlters.
•
Use an approximation with diminishing ripple.
1.06.5 Ladder structures
LC ladder structures are often used to realize ﬁlters with transfer functions with zeros on the jω-axis
or in the left-hand side of the s-plane. Zeros in the right-hand side of the s-plane cannot be realized
by ladder structures, i.e., only minimum-phase transfer functions can be realized. Doubly terminated
ladder structures have low element sensitivity in the passband, as discussed above, and relatively small
sensitivity in the stopband.
1.06.5.1 Structures for lowpass ﬁlters
Figures 6.21 and 6.22 show ladder structures of T and π type for realization of lowpass ﬁlters without
ﬁnite zeros, respectively. T and π type refer to the leftmost stage of the ladder. These ladder structures
can realize Butterworth and Chebyshev I lowpass ﬁlters.
Figures 6.23 and 6.24 show ladder structures that can realize transfer functions with ﬁnite zeros, e.g.,
Chebyshev II and Cauer ﬁlters.
+
–
Vin
Vout
+
–
L1
L3
LN
C2
C4
Rs
+
–
Vout
CN
RL
RL
FIGURE 6.21
Nth-order T ladder structure for lowpass ﬁlters without ﬁnite zeros.

274
CHAPTER 6 Digital Filter Structures and Their Implementation
+
–
Vout
+
–
L1
L4
C1
C3
Rs
+
–
Vout
LN
CN
RL
RL
Vin
FIGURE 6.22
Nth-order π ladder structure for lowpass ﬁlters without ﬁnite zeros.
Vout
+
–
Vout
+
–
L1
L3
LN
C2
C4
Rs
+
–
LN–1
CN–2
RL
RL
L2
L4
LN–1
CN–1
LN–2
CN
Vin
FIGURE 6.23
Nth-order T ladder structure with ﬁnite zeros.
+
–
Vout
+
–
C2
Rs
LN –2
RL
L2
LN –1
CN –1 CN
C1
C3
Vin
+
–
Vout
RL
CN –2
CN –1
LN
FIGURE 6.24
Nth-order π ladder structure with ﬁnite zeros.
There are two main methods for creating transmission zeros. The best method is to use series or
parallel resonance circuits in the shunt and series arms in a ladder structure, respectively. The transmis-
sion zeros are created by reﬂecting the signal at the resonance circuits back to the source. The stopband
sensitivity will be small if the transmission zeros are created by reﬂection. In fact, a zero pair on the
jω-axis is determined by only two elements. The passband sensitivity and the ratio of the largest and
smallest element value, depends on the ordering of the transmission zeros. For lowpass ﬁlters, are mini-
mal sensitivity obtained if the transmission zeros closest to the passband edge is positioned in the center
of the ladder structure. The positioning of the zeros is very important in narrow-band bandpass ﬁlters.
In lattice ﬁlters, which will be discussed in Section 1.06.7.9, the transmission zeros are instead
created by cancellation. In fact, lattice ﬁlters have low passband sensitivities, but very poor stopband

1.06.6 Lattice Structures
275
sensitivities. The lattice structure is nevertheless useful for piezoelectric and ceramic resonator ﬁlters
and as reference ﬁlter for digital ﬁlters, since the element errors in these technologies are small.
To summarize, we conclude that creating transmission zeros by means of power reﬂection at the shunt
or series arms is a superior method compared to methods based on signal cancellation or dissipation.
A singly resistively terminated reactive network do not adhere to Feldtkeller’s equation and the power
transferred is either zero or upwards unbounded. It is well known that singly terminated ﬁlters are much
more sensitive than doubly terminated ﬁlters. Hence, singly terminated ﬁlters are not recommended as
reference ﬁlters.
1.06.5.1.1
Design of ladder structures
Doubly terminated ladder structures are generally designed by using the insertion loss method. This
method involves long, complicated, and numerically ill-conditioned calculations, which must be done
by a computer. Two exceptions are the Butterworth and Chebyshev I lowpass ﬁlters for which analytical
solutions have been found. See [20] for details on the synthesis of ladder structures as well as design of
highpass, bandpass, and bandstop LC ﬁlers. Efﬁcient MATLAB functions for synthesis of ladder ﬁlters
have been implemented in the toolbox [21].
1.06.6 Lattice structures
Lattice structures can be used to derive digital ﬁlter structures with low roundoff noise, high degree of
parallelism, and modular circuitry that are suitable for implementation. The problems associated with
unstable and inaccurate element values are not present in digital ﬁlters, since the coefﬁcients are ﬁxed
binary values.
Figure 6.25 shows a symmetric analog lattice ﬁlter with the lossless reactances X1 and X2 and
Rs = RL. An analog lattice ﬁlter is in practice unusable, because of its high sensitivity in the stopband,
except if the reactances are realized by highly stable and accurate components, for example quarts and
ceramic resonators. In fact, due to the high stopband sensitivity, the lattice structure is often used as a
measuring device. Lattice ﬁlters, which are designed for maximal power transfer, have, however, low
coefﬁcient sensitivity in the passband.
Vin
Rs
X1
X2
X1
X2
Vout
RL
+
_
FIGURE 6.25
Analog lattice ﬁlter.

276
CHAPTER 6 Digital Filter Structures and Their Implementation
Lattice structures can be used to derive digital ﬁlter structures with low roundoff noise, high degree
of parallelism, and modular circuits that are suitable for implementation. The problems associated with
unstable and inaccurate element values are not present in digital ﬁlters, since the coefﬁcients are ﬁxed
binary values.
1.06.6.1 Wave description of two-ports
Instead of using voltages and currents to describe electrical networks, it is convenient to use a linear
combination thereof. One such linear combination is voltage waves. An impedance Z is normally
described by the ratio Z = V1/I1, but it can also be uniquely described using voltage waves, i.e.,
	 A1 = V1 + RI1,
B1 = V1 −RI1,
(6.29)
where R is a positive constant. Other possibilities are, for example, current waves and power waves
[14,48]. The latter is often used to describe distributed networks.
1.06.6.1.1
Analog lattice structure
Consider the lattice structure shown in Figure 6.26 where we have added a second optional input signal
source.
The lattice structure can be described by the incident (A1 and A2) and reﬂected (B1 and B2) voltage
waves
	 A1 = V1 + RI1 = Vin1,
B1 = V1 −RI1,
(6.30)
and
	 A2 = V2 + RI2 = Vin2,
B2 = V2 −RI2.
(6.31)
We deﬁne the (voltage wave) scattering matrix by
 B1
B2

=
 s11 s12
s21 s22
  A1
A2

= S
 A1
A2

.
(6.32)
Vin1
R
X1
X2
X1
X2
A1
B1
B2
A2
I1
I2
+
_
+
_
V1
V2
Vin2
R
FIGURE 6.26
Symmetric analog lattice ﬁlter.

1.06.6 Lattice Structures
277
We get after elimination of voltages and currents in (6.30) through (6.32)
	2B1 = S1(A1 −A2) + S2(A1 + A2),
2B2 = S1(A2 −A1) + S2(A1 + A2),
(6.33)
where
S1 = X1 −R
X1 + R ,
(6.34)
S2 = X2 −R
X2 + R
(6.35)
are the reﬂectance functions for X1 and X2. Note that S1 and S2 are allpass functions if X1 and X2 are
reactances. We can rewrite (6.33) as
	 B1 = S1+S2
2
A1 + S2−S1
2
A2,
B2 = S2−S1
2
A1 + S1+S2
2
A2
(6.36)
from which it follows that
s11 = S1 + S2
2
,
(6.37)
s21 = S2 −S1
2
.
(6.38)
The scattering parameter s21(s) correspond to the transfer function and s11(s) to the reﬂection
function. Feldtkeller’s equation can be expressed in terms of the scattering parameters s11 and s21 as
|s11|2 + |s21|2 = 1
(6.39)
Hence, s11 and s21 are power-complementary transfer functions. Figure 6.27 shows the wave-ﬂow
diagram for the lattice ﬁlter. Note that the ﬁlter consists of two allpass ﬁlters in parallel and that the two
outputs are obtained by adding and subtracting the outputs of the allpass ﬁlters. The normal output is
B2. Only odd-order lowpass ﬁlters can be realized using a lattice structure. Hence, S1 and S2 are odd
(even) and even (odd), respectively. See [14,49,50] how to select S1 and S2 for more general cases.
1.06.6.2 Realization of reactances
The reﬂectance functions S1 and S2 correspond according to (6.34) and (6.35) to the reactances X1 and
X2. A reactance, X(s) can be realized in several canonic ways, see [20] for more design details and
MATLAB programs. For example, Foster I and II realizations are shown in Figure 6.28. Alternative
realizations are Cauer I and II ladders, which are shown in Figure 6.29.
An arbitrary Nth-order reactance X(s) can be realized by N lossless commensurate-length transmis-
sion lines (unit elements) using a Richards’ structure, which is shown in Figure 6.30. The structure is
terminated with either a short-circuit or open-circuit.

278
CHAPTER 6 Digital Filter Structures and Their Implementation
+
–
S1
–
+
S2
A1
A2
2B1
2B2
FIGURE 6.27
Wave-ﬂow diagram for a lattice ﬁlter.
L∞
L1
L2
LN
C0
C1
C2
CΝ
C∞
L0
C1
C2
CN
L1
L2
LN
FIGURE 6.28
Foster I (upper) and II (lower) structures.
1.06.6.3 Design of analog lattice structures
Note that both S1 and S2 are allpass functions. The scheme of splitting an odd-order lowpass transfer
function into two allpass sections, is as follows:
•
Assign the real pole to S1 and remove it from the set of poles. Note that only odd-order lowpass and
highpass ﬁlters are feasible.
•
Next, assign to S2 the pole pair with the smallest angle between the negative real axis and the vector
from the origin to the positive pole and remove the pole pair from the set of pole.
•
Continue in the same way by alternating assigning the pole pair with the smallest angle to S1 and
S2 until all poles have been assigned.

1.06.6 Lattice Structures
279
L1
L3
L5
C 2
C4
C6
C1
C3
C5
L2
L4
L6
FIGURE 6.29
Cauer I (upper) and II (lower) structures.
UE
R3
UE
R1
UE
R2
UE
RN
FIGURE 6.30
Realization of an Nth-order reactance.
•
The two allpass ﬁlters are obtained by adding the zeros according to sz ←−sp. That is, each pole
has a corresponding zero, but in the right-hand side half-plane.
•
Determine the reﬂectance functions from the allpass functions.
•
Finally, select suitable realizations of the two reactances.
1.06.6.4 Commensurate-length transmission line networks
In a commensurate-length transmission line networks, all transmission lines have a common electrical
propagation time. Commensurate-length transmission line networks constitutes a special case of dis-
tributed element networks that can easily be designed by using a mapping to a lumped element structure,
which can be designed using common analytical or computer-aided tools. This mapping is based on a
one-to-one mapping between the lumped element network and the transmission line network expressed
in terms of a complex frequency variable, Richards’ variable, . Lossless commensurate-length trans-
mission lines can be used to build low sensitivity ﬁlters.
A commensurate-length transmission line with length d is illustrated in Figure 6.31. The voltage and
current at various positions along the line can be described by an incident and a reﬂected wave. Under
stationary conditions we have
	 V (x) = Ae−γ x + Beγ x,
I(x) =
1
Z0 (Ae−γ x −Beγ x),
(6.40)
where A and B are constants, which can be determined by the boundary conditions. For example,
V1 = V (0) and I1 = I(0). Further we have
γ 2 = (r + jωl)(g + jωc)
(6.41)

280
CHAPTER 6 Digital Filter Structures and Their Implementation
I2
Vin
Rs
I1
V(x)
Z2
x
d
V1
Vout
I(x)
FIGURE 6.31
Transmission line.
and
Z2
0 = r + jωl
g + jωc,
(6.42)
where r, l, g, and c are the primary constants for the line: resistance, inductance, and conductance per
unit length, respectively. γ is the propagation constant and Z0 is the characteristic impedance. Z0 is
real-valued positive constant, Z0 = R =

l
c, for lossless transmission lines and is therefore sometimes
called characteristic resistance.
1.06.6.4.1
Richards’ variable
Commensurate-length transmission line ﬁlters constitute a special case of distributed element networks
that can easily be designed by mapping them to a lumped element structure. This mapping involves
Richards’ variable, which is deﬁned
 = ssT −1
ssT + 1 = tanh
sT
2

,
(6.43)
where  =  + j and T is the propagation time for a wave through the transmission line and
back. Richards’ variable is a dimensionless complex-valued variable. The real frequencies in the s- and
-domains are related by
 = tan
ωT
2

.
(6.44)
NotethesimilaritybetweenthebilineartransformationandRichards’variable.Infact,commensurate-
length transmission line ﬁlters have the same periodic frequency responses as digital ﬁlters. Substituting
Richards’ variable into the chain matrix for a transmission line we get [14]
K =
1
√
1 −2
 1 Z0

Z0
1

.
(6.45)
The matrix in (6.45) has element values that are rational functions in Richards’ variable, except for
the square-root factor. However, this factor can be handled separately during the synthesis. The synthesis
procedures (programs) used for lumped element design can therefore be used with small modiﬁcations
for synthesis of commensurate-length transmission line ﬁlters.

1.06.6 Lattice Structures
281
Zin
Z0
ZL
FIGURE 6.32
Terminated transmission line (UE–Unit Element).
1.06.6.4.2
Unit elements
The transmission line ﬁlters of interest are with a few exceptions built using only one-ports. At this
stage it is therefore interesting to study the input impedance of the one-port shown in Figure 6.32. A
lossless transmission line described by (6.45) is called a unit element (UE). The input impedance to a
unit element, which is terminated by the impedance ZL, can be derived from (6.45).
We get the input impedance to a transmission line, with characteristic impedance Z0 and loaded with
an impedance ZL, as
Zin() = ZL + Z0
Z0 + ZL .
(6.46)
We are interested in the input impedance of a lossless transmission line with characteristic impedance
Z0 = R that is terminated by an impedance in the following three cases.
1.06.6.4.3
ZL = Z0 (matched termination)
For case, ZL = Z0, we have according to (6.46)
Zin = R.
(6.47)
Hence, we have a matching between the unit element and the load, and an incident wave to the load,
will not be reﬂected.
1.06.6.4.4
ZL = ∞(open-ended)
We get
Zin = R
 .
(6.48)
Hence, an open-ended unit element can be interpreted as a new kind of capacitor, i.e., a -plane capacitor
with the value 1/R.
1.06.6.4.5
ZL = 0 (short-circuited)
A short-circuited unit element has the input impedance
Zin = R
(6.49)
which can be interpreted as a -plane inductor with the value R.

282
CHAPTER 6 Digital Filter Structures and Their Implementation
1.06.7 Wave digital ﬁlters
In this section we discuss the design of wave digital ﬁlters based on different classes of doubly resistively
terminated networks consisting of lossless commensurate-length transmission lines. Wave digital ﬁlters
inherit the low element sensitivity properties from their analog counterparts, which should be design
for maximal power transfer in the passband [14,20]. Hence, simple and short coefﬁcients can be used
in the wave digital ﬁlters. In addition, the dynamic signal range is large and the roundoff noise is low
in these low-sensitivity structures.
Wave digital ﬁlters constitute a wide class of digital IIR ﬁlters and in some special cases FIR ﬁlters.
Wave digital ﬁlters have many advantageous properties, and they are therefore the recommended type
of IIR ﬁlters. A wave digital ﬁlter is derived from an analog ﬁlter, which is referred to as its reference
ﬁlter. Due to this relationship between the wave digital ﬁlter and the corresponding analog reference
ﬁlter, wave digital ﬁlter structures inherit many fundamental properties from their analog counterparts.
Of foremost interest are favorable stability properties and low sensitivity with respect to variations in
element values. Furthermore, the approximation problem for the wave digital ﬁlter can be carried out
in the analog domain using well-known design programs. Wave digital ﬁlters comprise a whole variety
of subclasses, derived from the corresponding analog ﬁlter structures.
Inorder toretainthelow element sensitivityandrobustness of theanalogreferenceﬁlter it is necessary
to adhere to the maximal power transfer principle, which was discussed in Section 1.06.4.1. For the
representation of power, or energy, it is necessary to use two variables, e.g., voltage and current [51].
Thus the signal-ﬂow graphs discussed in Section 1.06.2.1 can not represent power.
Alfred Fettweis has developed a comprehensive theory, so-called wave digital ﬁlter theory, which
solve these problems [27,48,52]. In fact, this theory is not only a digital ﬁlter design theory, it is a general
theory describing the relations between distributed element and discrete-time networks. Furthermore,
it inherently contains an energy concept, which is important for stability analysis of nonlinear digital
networks as well as for retaining the low-sensitivity properties of the reference ﬁlter.
An important property of wave digital ﬁlters is the guaranteed stability, which is inherited from the
reference ﬁlter. In practice, the inductors in an LC ladder ﬁlter are nonlinear. Such nonlinearities may
cause and sustain parasitic oscillations. However, in the passive LC ﬁlter such oscillations are attenuated,
since the ﬁlter dissipates signal power. Hence, any oscillation will eventually vanish. This property is
retained in the wave digital ﬁlter.
Wave digital ﬁlters, particularly wave digital lattice and circulator-tree ﬁlters, are suitable for high-
speed applications. They are modular and possess a high degree of parallelism, which makes them easy
to implement in hardware.
1.06.7.1 Design of wave digital ﬁlters
A wave digital ﬁlter is derived from an analog reference ﬁlter. The basic relations between the wave
digital ﬁlter, the corresponding reference ﬁlter, i.e., the transmission line ﬁlter, and the lumped element
ﬁlter, are summarized in Figure 6.33. The purpose of the lumped element ﬁlter is only to simplify the
design process by allowing the use of conventional, lumped element ﬁlter theory and design tools. In
principle, all commensurate-length transmission line networks can be used as reference ﬁlters for wave

1.06.7 Wave Digital Filters
283
T
Vin
Rs
R1
R3
R2
RL
R1ψ
RL
Vin
Rs
R3ψ
L1s
RL
Vin
Rs
L3s
T
T
A1 = Vin
B1
–1
–1
B2
A2 = 0
ANALOGY
1
C2s
R 2
ψ
ψ =
z −1
z + 1
ψ = e sT −1
esT + 1
z = e
sT
Commensurate-length transmission line filter
Ψ-domain filterwith distributed elements
Lumped element filter
Wave digital filter
FIGURE 6.33
Summary of the design process for wave digital ﬁlters.
digital ﬁlters. Furthermore, wave digital ﬁlters representing classical ﬁlter structures are also viable
since all lumped element networks can be uniquely mapped onto commensurate-length transmission
line networks using Richards’ variable.
However, certain reference structures result in wave digital ﬁlter algorithms that are not sequentially
computable, because the wave-ﬂow graph contains delay-free loops. Therefore, one of the main issues is
to avoid these delay-free loops. There are three main approaches to avoid delay-free loops in wave digital
ﬁlters [20,27,48]. Naturally, combinations of these methods can also be used. The main approaches to
avoid delay-free loops are:
1. By using cascaded transmission lines, so called Richards’ structures, and certain types of circulator
ﬁlters.

284
CHAPTER 6 Digital Filter Structures and Their Implementation
Network
I
V
R
A
B
+
–
FIGURE 6.34
Incident and reﬂected waves into a port with a port resistance R.
2. By introducing transmission lines between cascaded two-ports.
3. By using reﬂection-free ports.
1.06.7.2 Wave descriptions
It is necessary to use two variables to represent power [25,51]. In the analog domain we usually use
voltages and currents, but we may also use any linear combination of voltage and current to describe
a network [27,48]. In fact, a network can be described using incident and reﬂected waves instead of
voltages and currents. Scattering parameter formalism has been used in microwave theory for a long
time for describing networks with distributed circuit elements.
1.06.7.2.1
Voltage waves
The one-port network shown in Figure 6.34 can be described by the incident and reﬂected waves instead
of voltages and currents.
The voltage waves are deﬁned as
	 A = V + RI,
B = V −RI,
(6.50)
where A is the incident wave, B is the reﬂected wave, and R is a positive real constant, called port
resistance. The port resistance corresponds to the characteristic impedance in a transmission line.
1.06.7.2.2
Current waves
In a similar way, current waves can be deﬁned, but they results in the same digital ﬁlter structure as
voltages waves if we instead used the dual reference ﬁlter. Since the current wave description do not
provide any additional structures we will not discuss current waves any further. There are also simple
relations between wave digital ﬁlters derived using voltage, current, and power waves [27].

1.06.7 Wave Digital Filters
285
1.06.7.2.3
Power waves
In microwave theory, so-called power waves are used

A =
V
√
R +
√
RI,
B =
V
√
R −
√
RI.
(6.51)
The name power waves comes from the fact that their squared values have the dimension of power.
We will mainly use voltage waves, because they provide simpler digital realizations compared to using
a power wave description.
1.06.7.2.4
Reﬂectance function
A one-port can be described by the reﬂectance function which is deﬁned by
S = B
A.
(6.52)
The reﬂectance function serves a similar purpose as impedances when voltages and currents are used
to describe a network. For example the reﬂectance for an impedance Z is
S = Z −R
Z + R .
(6.53)
The reﬂectance is an allpass function for a reactance.
1.06.7.3 Wave-ﬂow building blocks
The basic building blocks for the reference ﬁlter are unit elements, that are either open- or short-circuited
at the far end. The frequency response of such a unit element ﬁlter is periodic with a period of 2π/τ, i.e.,
the same as for a digital ﬁlter. The signals and components of the unit element ﬁlter can be mapped to
a digital ﬁlter by sampling with the sample period, T = τ, where τ is the round-trip for a wave through
the unit element. In the following sections, we will derive the wave-ﬂow equivalents to some common
circuit elements and interconnection networks.
1.06.7.3.1
Circuit elements
In this section we derive the wave-ﬂow representation for some basic circuit elements.
1.06.7.3.2
Open-ended unit element
The input impedance to an open-circuited lossless unit element with Z0 = R (a -plane capacitor) is
given in (6.48). The reﬂectance is
S() = Zin −R
Zin + R = 1 −
1 +  = e−sT
(6.54)
and
S(z) = z−1.
(6.55)
Figure 6.35 show the wave-ﬂow of an open-ended, lossless transmission line and its wave-ﬂow
equivalent.

286
CHAPTER 6 Digital Filter Structures and Their Implementation
A
R
B
T
Zin
R
FIGURE 6.35
Open-ended lossless transmission line and its wave-ﬂow equivalent.
A
R
B
T
–1
R
Zin
FIGURE 6.36
Short-circuited lossless transmission line and its wave-ﬂow equivalent.
1.06.7.3.3
Short-Circuited Unit element
The input impedance to a short-circuited, lossless unit element with Z0 = R (a -plane inductor) is
given by (6.49). The reﬂectance is
S() = Zin −R
Zin + R =  −1
 + 1 = −e−sτ
(6.56)
and
S(z) = −z−1.
(6.57)
Figure 6.36 shows the short-circuited lossless transmission line and its wave-ﬂow equivalent.
For sake of simplicity we will not always distinguish between wave-ﬂow graphs in the time and
frequency domains. Hence, the wave variables in the ﬁgures are denoted A and B, but we will when
convenient use the notation a(n) and b(n).
An open-ended unit element corresponds to a pure delay, while a short-circuited unit element corre-
sponds to a delay and a 180◦phase shift.
1.06.7.3.4
Matched termination
The reﬂectance for a lossless unit element terminated at the far end with a resistor with Z0 = R (matched
termination) is
S() = 0.
(6.58)
Hence, an input signal to the unit element is not reﬂected back to the input.
1.06.7.3.5
Resistive load
The reference ﬁlters of interest are, for sensitivity reasons, doubly resistively terminated. The load
resistor and its corresponding wave-ﬂow equivalent, a wave sink, are shown to the right in Figure 6.37.

1.06.7 Wave Digital Filters
287
R
A
B
A
B  = 0
V
+
–
I
R
FIGURE 6.37
Wave-ﬂow equivalent for a resistor, R.
A
V  = 0
B
+
–
I
A
R  = 0
B
–1
FIGURE 6.38
Wave-ﬂow equivalent for a short-circuit.
1.06.7.3.6
Short-circuit
For a short-circuit, we have V = 0, which yields
S() = −1
(6.59)
and B = −A. Note that this holds independently of the port resistance. The corresponding wave-ﬂow
graph is shown to the right in Figure 6.38.
1.06.7.3.7
Open-circuit
For an open-circuit, we have I = 0, which yields B = A. This result holds independently of the port
resistance R. An open-circuit has the reﬂectance
S() = 1.
(6.60)
The wave-ﬂow graph for an open-circuit is shown to the right in Figure 6.39.
1.06.7.3.8
Voltage signal source
For a signal source with a source resistance we have Vin = V −RI, which yields B = Vin. The
incident wave is not reﬂected by the source since the source resistance equals the port resistance. The
corresponding wave-ﬂow graph is shown to the right in Figure 6.40.

288
CHAPTER 6 Digital Filter Structures and Their Implementation
A
B
V
+
–
I = 0
A
B
R = ∞
FIGURE 6.39
Wave-ﬂow equivalent for an open-circuit.
A
B
V
+
–
I
A
R
R
B  = V in
V in
+
–
R
FIGURE 6.40
Wave-ﬂow equivalent for a voltage source with source resistance.
1.06.7.3.9
Circulator
The symbol for a three-port called circulator and its corresponding wave-ﬂow graph is shown in
Figure 6.41. The name comes from the fact that an incident wave is “circulated to the next port as
shown in Figure 6.41. Note that an incident wave to port 1 is reﬂected to port 2, and an incident wave
to port 2 is reﬂected to port 3, and so on. Hence, the circulator “circulates the incident waves to the
next port in order. Circulators can be realized in the microwave range by using ferrites. Circulators are
used, for example, to direct the signals from the transmitter to the antenna and from the antenna to the
receiver in a radar. The circulator is a useful component that is inexpensive to implement in wave digital
ﬁlter.
1.06.7.4 Interconnection networks
In order to interconnect different wave-ﬂow graphs it is necessary to obey the Kirchhoff’s laws at
the interconnection. Generally, at the point of connection, the incident waves are partially transmitted
and reﬂected as illustrated in Figure 6.42. The transmission and reﬂection at the connection port is
described by a wave-ﬂow graph called an adaptor. There are several types of adaptors corresponding to
different types of interconnection networks [53]. It can be shown that any interconnection network can
be decomposed into a set of interconnected two-port and three-port adaptors. Since the interconnection
network is lossless, therefore the corresponding adaptor network will also be lossless (see Figure 6.43).

1.06.7 Wave Digital Filters
289
V1
–
+
–
+
–
+
A1
B1
V2
A2
B2
I2
I1
B3
A3 V3
I3
R
A1
B1
A2
B2
B3
A3
FIGURE 6.41
Three-port circulator and the corresponding wave-ﬂow graph.
V1
A1
B1
B2
A2
I1
I2
V2
Network II
R2
Network I
R1
+
–
+
–
FIGURE 6.42
Connection of two ports.
A 1
B1
B 2
A 2
R2
R1
α1
FIGURE 6.43
Symmetric two-port adaptor.
1.06.7.4.1
Symmetric two-port adaptor
Figure 6.44 shows the symbol for the symmetric two-port adaptor. The incident and reﬂected waves for
the two-port are
	 A1 = V1 + RI1,
B1 = V1 −RI1
(6.61)
and
	 A2 = V2 + RI2,
B2 = V2 −RI2.
(6.62)

290
CHAPTER 6 Digital Filter Structures and Their Implementation
A1
B 2
R1
R2
A 2
B1
α
+
–
FIGURE 6.44
Symmetric two-port adaptor.
At the point of interconnection we have according to Kirchhoff’s current and voltage laws
	 I1 = −I2,
V1 = V2.
(6.63)
By elimination of the voltages and currents, we get the following relations between incident and
reﬂected waves for the symmetric two-port adaptor
	 B1 = A2 + α(A2 −A1),
B2 = A1 + α(A2 −A1)
(6.64)
and
α = R1 −R2
R1 + R2
,
(6.65)
which are illustrated by the wave-ﬂow graph in Figure 6.44.
Note that |α| ≤1. The adaptor coefﬁcient α is usually written on the side corresponding to port 1
and/or one of the parallel lines are thicker. As can be seen, the wave-ﬂow graph is almost symmetric.
Note that α = 0 for R1 = R2 and that the adaptor degenerates into a direct connection of the two
ports and the incident waves are not reﬂected at the point of interconnection. For R2 = 0 we get α = 1
and the incident wave at port 1 is reﬂected and multiplied by −1 while for R2 = ∞we get α = −1
and the incident wave at port 1 is reﬂected without a change of sign.
Several different adaptor types exist that correspond to interconnections between circuit elements
[53]. Symmetric adaptors are commonly used in the ﬁrst step in the design of wave digital ﬁlters. In
subsequent design steps these adaptors can be transformed into other types in order to optimize the
dynamic signal range.

1.06.7 Wave Digital Filters
291
I2
V2
VN
IN
V1
I1
Vk
Ik
+
–
–
+
+
–
–
+
FIGURE 6.45
Series connected ports.
1.06.7.4.2
Series adaptors
Consider the network topology shown in Figure 6.45 with N ports where the currents that ﬂow through
the ports are the same. Hence, the ports are connected in series. We have
	 I1 = I2 = · · · = IN,
V1 + V2 + · · · + VN = 0.
(6.66)
After elimination of voltages and currents we get
	 Bk = Ak −αk A0,
A0 = 
N
k=1 Ak,
(6.67)
where
αk =
2Rk

N
n=1Rn
.
The symbol used for an N-port series adaptor is shown in Figure 6.46. It is straightforward to show that
the sum of the adaptor coefﬁcients is
N

k=1
αk = 2.
(6.68)
Hence, the adaptor coefﬁcients are linearly dependent and one of the coefﬁcients can therefore be
eliminated, i.e., expressed in terms of the others. A port for which the adaptor coefﬁcient has been
eliminated is called dependent port. It is favorable to select the port with the largest adaptor coefﬁcient
as dependent port. The number of different adaptor coefﬁcients can be reduced further if some of the
port resistances are the same.

292
CHAPTER 6 Digital Filter Structures and Their Implementation
B2
R2
A2
AN
RN
B N
A1
R1
B1
B k
Rk
A k
FIGURE 6.46
Series adaptor.
A1
R1
B1
α 1
A 2
R 2
B 2
α 2
FIGURE 6.47
Two-port series adaptor.
1.06.7.4.3
Two-port series adaptor
Figure 6.47 shows the symbol for the two-port series adaptor. In this case, we have only two adaptor
coefﬁcients and one of them can be eliminated since they are linearly dependent. The resulting wave-ﬂow
graph, where α2 has been eliminated, is shown in Figure 6.48. The remaining adaptor coefﬁcient is
α1 =
2R1
R1 + R2
(6.69)
and 0 ≤α1 ≤2.
1.06.7.4.4
Three-port series adaptor
A symbol for the three-port series adaptor is shown in Figure 6.49 and in Figure 6.50 the corresponding
realization of the three-port series adaptor, where adaptor coefﬁcient α3 has been eliminated. Hence,
port 3 depends on the adaptor coefﬁcients for the two other ports.

1.06.7 Wave Digital Filters
293
B2
R 1
A 2
A1
R 2
B1
–α1
–1
FIGURE 6.48
Wave-ﬂow graphs for a two-port series adaptor.
A1
R1
B1
α 3
A 2
R2
B 2
A3
R3
B3
α 1
α 2
FIGURE 6.49
Three-port series adaptor.
B3
–1
A3
A1
B2
B1
–α1
–α2
A0
A2
FIGURE 6.50
Three-port series adaptor with port 3 as the dependent port.

294
CHAPTER 6 Digital Filter Structures and Their Implementation
Vk
Ik
I1
IN
I2
+
+
+
+
VN
V2
V1
–
–
–
–
FIGURE 6.51
Parallel connection of ports.
This three-port series adaptor requires two multiplications and six additions. It is often favorable,
since it required multiplications with shorter coefﬁcient wordlength, to eliminate the adaptor coefﬁcient
with the largest magnitude. Hence, the corresponding port is select as the dependent port.
1.06.7.4.5
Parallel adaptors
If all ports have the same voltage, as illustrated in Figure 6.51, then the ports are connected in parallel.
We get by using the deﬁnition of parallel connection of the ports, i.e.,
	 V1 = V2 = · · · = VN,
I1 + I2 + · · · + IN = 0.
(6.70)
By elimination of voltages and currents we get
	 Bk = A0 −Ak,
A0 = 
N
k=1 αk Ak,
(6.71)
where
αk =
2Gk

N
n=1 Gn
and Gk = 1/Rk. The symbol for an N-port parallel adaptor is shown in Figure 6.52. Also for the parallel
adaptor we have
N

k=1
αk = 2.
(6.72)
Hence, one of the adaptor coefﬁcients can be expressed in terms of the others. The number of
multiplications can be reduced further if some of the port conductances are equal.

1.06.7 Wave Digital Filters
295
B2
R2
A2
AN
RN
BN
A1
R1
B1
Ak
Rk
Bk
FIGURE 6.52
N-port parallel adaptor.
A1
R1
B1
B2
A2
R2
α1
FIGURE 6.53
Two-port parallel adaptor.
A1
R1
B1
B2
A2
R2
–α1
–1
FIGURE 6.54
Wave-ﬂow graph for two-port parallel adaptor.
1.06.7.4.6
Two-port parallel adaptor
Figure 6.53 shows the symbol for a two-port parallel adaptor and the corresponding wave-ﬂow graph
is shown in Figure 6.54.
The sum of the adaptor coefﬁcients in an adaptor is always equal to 2 with the exception of the
symmetric two-port adaptor. Hence, one of the coefﬁcients can be expressed in terms of the others and
can therefore be eliminated.
The adaptor coefﬁcient in Figure 6.54 is
α1 =
2G1
G1 + G2
(6.73)
and 0 ≤α1 ≤2.

296
CHAPTER 6 Digital Filter Structures and Their Implementation
B3
R3
A3
A1
R1
B1
A2
R2
B2
α1
α2
α3
FIGURE 6.55
Three-port parallel adaptor.
A1
B1
A3
–α1
B3
B2
A2
–α2
–1
–1
FIGURE 6.56
Three-ports parallel adaptor with port 3 as dependent port.
1.06.7.4.7
Three-port parallel adaptor
A general three-port parallel adaptor requires two multiplications and six additions. The symbol for the
three-port parallel adaptor is shown in Figure 6.55.
Figure 6.56 shows the wave-ﬂow graph for the corresponding three-port parallel adaptor with port 3
as the dependent port.
1.06.7.4.8
Direct interconnection of adaptors
Arbitrary interconnection networks can be built using only two- and three-port series and parallel
adaptors. However, in some cases also four-port adaptors are used. Unfortunately, delay-free loops will
in general occur if two adaptors are connected, as illustrated in Figure 6.57. The port resistances of the
two connected ports must, of course, be equal. This problem can, however, be solved by a judicious
choice of the port resistances in one of the connected adaptors.
By selecting the port resistance so that one of the adaptor coefﬁcients, corresponding to the con-
nected ports, becomes equal to unity. We select, for example, αN = 1, which for the series adaptor is
equivalent to
RN =
N−1

k=1
Rk
(6.74)

1.06.7 Wave Digital Filters
297
RN
RN
FIGURE 6.57
Potentially delay-free loop.
A1
B1
A3
–α1
B3
B2
A2
–1
–1
FIGURE 6.58
Series adaptor with port 3 and 2 as dependent and reﬂection-free port, respectively.
and
BN = −
N−1

k=1
Ak.
(6.75)
Hence, the reﬂected wave BN is independent of the incident wave AN and the delay-free loop is
broken. For a parallel adaptor we have
G N =
N−1

k=1
Gk
(6.76)
and
BN =
N−1

k=1
αk Ak.
(6.77)
In both cases, the expression for BN is independent of AN, i.e., a direct path from AN to BN in
the wave-ﬂow graph does not exist. Hence, the two ports can be connected without that a delay-free
loop occurs. There are no restrictions on the adjacent adaptor. The port with the coefﬁcient equal to
unity is called a reﬂection-free port. Since αN = 1, one of the remaining coefﬁcients can be eliminated.
Figure 6.58 shows a three-port series adaptor with α3 = 1 and port 2 as the dependent port.

298
CHAPTER 6 Digital Filter Structures and Their Implementation
A1
B1
A3
–α1
B3
B2
A2
–1
FIGURE 6.59
Parallel adaptor with port 3 as dependent port and port 2 is reﬂection-free, respectively.
BN
AN
FIGURE 6.60
Adaptor with reﬂection-free port.
Note the non-existent path between A3 and B3. A three-port parallel adaptor with α2 = 1 and port 3
as dependent port is shown in Figure 6.59. We denote a reﬂection-free port with the symbol as shown
in Figure 6.60.
1.06.7.5 Adaptor transformations
Adaptor transformations, i.e., replacing series adaptors with parallel adaptors and vice versa, yet with
the same adaptor coefﬁcients, can be used to improve the dynamic signal range, lower roundoff noise,
and improve the computational properties of the algorithm [53]. Replacing a series adaptor with a
parallel adaptor corresponds in the reference ﬁlter to replacing an impedance in a series and with a
shunt impedance embedded between two gyrators [20]. Thus, a series adaptor is replaced by a parallel
adaptor with a pair of inverse multipliers at each port and an inverter at each output terminal as shown
in Figure 6.61.
Figure 6.62 shows an N-port parallel adaptor and its equivalent series adaptor. A pair of inverse
multipliers can be eliminated, or replaced with any other pair of inverse multipliers, and thereby changing
the signal levels in the parallel adaptor network. Note that a pair of inverse multipliers correspond to
a transformer in the reference ﬁlter. These equivalences transformation changes neither the adaptor
coefﬁcients nor the transfer functions, expect for a possible change in the gain. Note however, inserting

1.06.7 Wave Digital Filters
299
B2
R2
A2
AN
RN
BN
A1
R1
B1
Ak
Rk
Bk
α1
α2
αk
αN
B2
A2
AN
BN
Ak
Bk
α1
α2
αk
αN
–α1
A1
B1
1
α2
1
αk
1
αN
–α2
–αk
–αN
1
α1
FIGURE 6.61
N-port series adaptor and its equivalent parallel adaptor.
B2
R2
A2
AN
RN
BN
A1
R1
B1
Ak
Rk
Bk
α1
α2
αk
αN
B2
A2
AN
BN
Ak
Bk
α1
α2
αk
αN
α1
A1
B1
–1
α2
–1
αk
–1
αN
α2
αk
αN
–1
α1
FIGURE 6.62
N-port parallel adaptor and its equivalent series adaptor.
a pair of inverse multipliers requires that the coefﬁcients are of the form c = 2±n, where n is an integer,
since the product c(1/c), were both factors are in binary representation, must be equal to 1. A network
with n adaptors, have 2n equivalent realizations, which, however differ with respect to roundoff noise
and overﬂow probability and dynamic signal range.
1.06.7.6 Resonance circuits
First- and second-order resonance circuits play an important role as building blocks for frequency
selective ﬁlters. We are in practice only interested in low order resonance circuits, i.e., ﬁrst- and second-
order circuits. In this section we discuss the design and properties of some resonance circuits and their
wave digital counterparts.
1.06.7.6.1
First-order circuits
A ﬁrst-order wave digital circuit can be derived from a -plane capacitor or inductor using series,
parallel, or symmetric adaptors. Note that a -plane capacitor represents an open-ended unit element

300
CHAPTER 6 Digital Filter Structures and Their Implementation
T
–1
A1
R1
R2
B1
α
+
A1
R1
R2
α
+ 
T
–
–
(a)
(b)
FIGURE 6.63
First-order section with (a) inductor, (b) capacitor.
while a -plane inductor represents a short-circuited unit element. Figure 6.63 show a ﬁrst-order circuit
corresponding to (a) an inductor and (b) a capacitor realized using the symmetric adaptor.
The reﬂectance function for the inductor, R2, is
SL(z) = −αz + 1
z + α .
(6.78)
The reﬂectance function for the capacitor, R2/, is
SC(z) = 1 −αz
z −α ,
(6.79)
where α is given by (6.69). Alternatively, ﬁrst-order circuits corresponding to an inductor or a capacitor
can be derived using series or parallel adaptors [14,27,53].
1.06.7.6.2
Second-order series resonance circuit
Figure 6.64 shows a second-order series resonance circuit and its wave digital realization using a three-
port series adaptor. The reﬂectance function is
S = B1
A1
= (1 −α1)z2 −(2α2 + α1 −2)z + 1
z2 −(2α2 + α1 −2)z + 1 −α1
.
(6.80)
Note that the reﬂection function is an allpass function.
R1
B1
A1
R3
R1
T
T
R2Ψ
R3
Ψ
–1
R2
α2
α1
α3
FIGURE 6.64
Realization of a series resonance circuit.

1.06.7 Wave Digital Filters
301
B1
A1
R3
R1
T
T
R2Ψ
R3
Ψ
R1
–1
R2
α2
α1
α3
FIGURE 6.65
Parallel resonance circuit and its WDF counterpart.
1.06.7.6.3
Second-order parallel resonance circuit
Figure 6.65 shows a parallel resonance circuit and its wave digital realization using a three-port parallel
adaptor. The reﬂectance function is
S = −(1 −α1)z2 + (2α3 + α1 −2)z + 1
z2 + (2α3 + α1 −2)z + 1 −α1
.
(6.81)
The pole density is the same as for the series resonance circuit [14].
1.06.7.6.4
Second-order Richards’ structures
As discussed in Section 1.06.6.2 a Richards’ structure that is either open-circuited or short-circuited
at the far end can realize an arbitrary reactance. Figure 6.66 shows second-order Richards’ structure
R
R1 Ψ
R3
B1
A1
II
I
R3
R4
R4
R2
Ψ
R
T
T
α2
α1
R3
FIGURE 6.66
Richards’ structure equivalent to second-order series resonance circuit with corresponding wave-ﬂow graph.

302
CHAPTER 6 Digital Filter Structures and Their Implementation
and its wave digital counterpart. It can be shown by using Kuroda-Levy identities that this Richards’
structure corresponds to a series resonance circuit in the -domain [14,27].
The reﬂectance function is
S(z) = −α1z2 + (α1 −1)α2z + 1
z2 + (α1 −1)α2z −α1
.
(6.82)
A Richard’s parallel resonance circuit has the same reﬂectance function, except that α1 has changed
sign. Hence, the two structures have the same pole density.
1.06.7.7 Parasitic oscillations
Parasitic oscillations can occur only in recursive structures. Nonrecursive digital ﬁlters can have parasitic
oscillations only if they are used inside a closed loop. Nonrecursive FIR ﬁlters are therefore robust ﬁlters
structures that do not support any kind of parasitic oscillation. Among the IIR ﬁlter structures, wave
digital ﬁlters, and certain related state-space structures, are of major interest as they can be designed
to suppress parasitic oscillations. A. Fettweis has shown that overﬂow oscillations can be suppressed
completely in wave digital ﬁlters by placing appropriate restrictions on the overﬂow characteristic
[27,54–57]. To show that a properly designed wave digital ﬁlter suppresses any parasitic oscillation
we use the concept of pseudo-power, which corresponds to power in analog networks. Note that the
pseudo-power concept is deﬁned only for wave digital ﬁlters and not for arbitrary ﬁlter structures.
The instantaneous pseudo-power entering the adaptor network, shown in Figure 6.67, is deﬁned
p(n) =
N

k=1
Gk

ak(n)2 −bk(n)2
.
(6.83)
Ideally, p(n) = 0, since the adaptors are lossless and an arbitrary network of adaptors is also lossless.
Now, in order to suppress a disturbance (error signal component) caused by a nonlinearity, it is
sufﬁcient to introduce losses in each port of the network such that the nonlinearities corresponds to pure
losses. This can be accomplished by making each term ak(n)2 −bk(n)2 > 0, i.e., each recursive loop
will be lossy. Note that all recursive loops must contain at least one delay element for a sequentially
computable algorithm. Any parasitic oscillation will therefore by necessity appear at one or several of
the ports with delay elements. Hence, a parasitic oscillation, which, of course, has ﬁnite pseudo-energy
T
T
T
Adaptor Network
a1
G1
b1
b3
G3
a3
G4
b4
a4
bN
GN
aN
b2
G2
a2
FIGURE 6.67
Wave digital ﬁlter.

1.06.7 Wave Digital Filters
303
and is not supported from an external signal source will decay to zero since it will dissipate energy in the
lossy ports of the adaptor network. Lossy ports can be obtained by quantizing the reﬂected waves such
that their magnitudes are always decreased. Hence, for each nonzero reﬂected wave we required that
	b(n)Q
 < b(n)exact,
b(n)Qb(n)exact ≥0,
(6.84)
where the second constraint is to assure that the signals have the same sign.
In order to suppress parasitic oscillations, all of the output signals from the adaptors should be sign-
magnitude truncated. It can be shown that the sign-magnitude truncation can be substituted by truncation
of the signals after the multiplications inside the adaptors and by adding a correcting term at the outputs
of the adaptors [14]. This implies considerable hardware simpliﬁcations. In practice, it seems to be
sufﬁcient that the correcting terms are added only at those ports to which delays are connected
1.06.7.8 Ladder wave digital ﬁlters
Doubly resistively terminated ladder structures, which are designed for maximal power transfer in the
passband, are characterized by their low element sensitivity. A corresponding wave digital ﬁlter inherits
the sensitivity properties as well as the stability properties of the analog reference ﬁlter. The positions of
the transmission zeros are also relative insensitive since they are realized by reﬂections in the resonance
circuits. This is true for ﬁlters that approximate a constant passband gain. However, it is not true for,
for example, a ﬁlter that approximate a differentiator.
There are several alternative ladder design techniques:
•
Unit elements can be inserted between the branches in the ladder structure in order to avoid delay-free
loops [14,25,27,52,58]. These ﬁlters are of less importance.
•
Reference ﬁlters of Richards’ type, i.e., cascaded transmission lines, may be used to realize lowpass
and highpass ﬁlters and a restricted set of bandpass and stopband ﬁlters [14]. Also these ﬁlters are
of less importance.
•
A commonly used class of wave digital ﬁlters are derived from a singly resistively terminated
Richards’ structure [14,59–62]. The corresponding wave digital ﬁlter is used to realize the denomi-
nator. The numerator is realized by either injecting the input signal into several nodes in the recursive
wave digital ﬁlter structure or using linear combination of the signal in several nodes, or a combina-
tion thereof. Singly resistively terminated ﬁlter has very high element sensitivity [20] and they are
not recommended for use as frequency-selective ﬁlters. Moreover, these structures suffer from large
variations in the internal signal levels and they also generate large roundoff noise. The design of the
above wave digital ﬁlters is discussed in detail in [14,17].
•
Here we recommend wave digital ﬁlters derived from a doubly resistively terminated ladder and we
use the technique with reﬂection-free ports to obtain a wave digital ﬁlter without delay-free loop.
•
Alternatively we may select a wave digital ﬁlter derived from a doubly resistively terminated lattice
ﬁlter, and use reﬂection-free ports or a cascade of allpass sections, to obtain a wave digital ﬁlter
without delay-free loop.

304
CHAPTER 6 Digital Filter Structures and Their Implementation
A1
B1
2
ΙI
1
2
3
T
T
–1
IΙΙ
1
2
3
T
I
1
2
3
T
1
3
2
ΙV
1
2
3
T
VI
T
–1
1
3
2
V
1
2
3
T
1
2
3
T
VII
–1
1
3
2
X
1
2
3
T
B2
A2
ΙX
1
2
3
T
VIII
–1
1
3
2
VII
1
2
3
T
XI
XII
XIII
v1
v2
v4
v3
v5
v6
v9
v10
v7
v11
v8
v13
v12
T
0.5
2
0.5
0.5
2
T
2
0.5
0.5
2
2
0.5
0.5
2
FIGURE 6.68
Ninth-order ladder wave digital ﬁlter.
0
20
40
60
80
A(ejωT)  [dB]
ωT [rad]
0
0.1 π
0.2 π
0.3 π
0.4 π
0.5 π
0.6 π
0.7 π
0.8 π
0.9 π
π
FIGURE 6.69
Variation in the attenuation for the ninth-order ladder ﬁlter.
1.06.7.8.1
Example 4
Consider a lowpass ﬁlter with the following speciﬁcation Amax = 0.1 dB, Amin = 65 dB, ωcT =
0.55φ rad, and ωsT = 0.6φ rad. Minimum order is Nmin = 8.499, which we increase to N = 9.
The ladder structure can be scaled by inserting transformers between adjacent adaptors. Each trans-
former allows one node in an adaptor to be scaled. The adaptors with a reﬂection-free port have only
one critical node, i.e., the input to the multiplier. Hence, all critical nodes can be scaled, except one node
in adaptor XIII, which have two critical nodes. The L2-norm scaled structure is shown in Figure 6.68.
Note that ﬁlters should be scaled in order to fairly compare the sensitivity properties (see Figure 6.69).
In order to illustrate and make a crude comparison of the coefﬁcient sensitivity of a π-ladder and
lattice ﬁlter with cascaded allpass sections, we generate 100 frequency responses where we randomly
vary the coefﬁcients in the range −2−8 to 2−8 around their nominal values. The overall variation in the
attenuation is shown in Figure 6.72.

1.06.7 Wave Digital Filters
305
0
0.05
0.1
0.15
0.2
A(ejωT)  [dB]
ωT [rad]
T
0
0.1 π
0.2 π
0.3 π
0.4 π
0.5 π
0.6 π
FIGURE 6.70
Passband variation in the attenuation for the ninth-order ladder ﬁlter.
A1
α1
α7
B2
1/2
α2
α3
T
T
T
T
T
T
α6
α5
T
α4
α9
T
α8
T
FIGURE 6.71
Ninth-order lattice WDF.
The variation is most signiﬁcant close to the stopband edge. In fact, we will later see that the
variation in the positions of the zeros is smaller for the ladder structure compared to a corresponding
lattice structure. The variations in the zeros are relatively small, because a zero is determined by only
one coefﬁcient. For example, consider the structure, shown in Figure 6.68. Port 2 of adaptor II is

306
CHAPTER 6 Digital Filter Structures and Their Implementation
0
20
40
60
80
A(ej
e ωT)  [dB]
T
ω T [rad]
T [rad]
T
0
0.1 π
0.2 π
0.3 π
0.4 π
0.5 π
0.6 π
0.7 π
0.8 π
0.9 π
π
FIGURE 6.72
Variation in the attenuation for the ninth-order lattice ﬁlter.
0
0.05
0.1
0.15
0.2
A(ej
e ωT)  [dB]
T
ωT [rad]
T [rad]
T
0
0.1 π
0.2 π
0.3 π
0.4 π
0.5 π
0.6 π
FIGURE 6.73
Passband variation in the attenuation for the ninth-order lattice ﬁlter.
reﬂection-free and hence α2 = 1 and therefore we have α1 + α3 = 1. Thus, the transmission zero,
which is restricted to the unit circle, is determined by only a single coefﬁcient and the accuracy is
determined by the coefﬁcient wordlength. The variation in the attenuation in the passband is shown in
Figure 6.73.
The variation increases towards the passband edge, which is in agreement with (6.27). A realization,
possibly with somewhat shorter coefﬁcient wordlength, may have been found if the ﬁlter instead was
synthesized with a diminishing passband ripple. Note that it is most likely that a set of coefﬁcients can
be found with shorter wordlength than 9 bit. Typically, the coefﬁcients can be represented using only
two or three nonzero bits, i.e., requiring only one or two adders. 14 multipliers are required for the ladder

1.06.8 Frequency Response Masking (FRM) Structure
307
structure. Wave digital ladder structures appear to yield complicated wave-ﬂow graphs that may make
then difﬁcult to implement efﬁciently. However, a detailed analysis shows that this is not the case [14].
1.06.7.9 Lattice wave digital ﬁlters
Lattice wave digital ﬁlters are derived from an analog bridge structure and have therefore very high
stopband sensitivity. Different wave digital lattice structures are obtained depending on how the lattice
branches are realized. We may use Forster and Cauer networks or high-order Richards ’structures.
A realization consisting of a set of interconnected three-port circulators that are loaded with ﬁrst- and
second-order reactances have become popular, mainly due to the fact that they are easy to design and
that they consist of simple allpass sections [14,63–65].
1.06.7.9.1
Example 5
Figure 6.71 shows a ninth-order lattice wave digital ﬁlter where the two allpass branches has been
realized by a circulator structures [20] that is loaded with ﬁrst- and second-order sections of the types
shown in Figure 6.66 (see Figure 6.70).
In order to get a similar deviation in the stopband attenuation as in Figures 6.72 and 6.73, we restrict
the error range of −2−15 to 2−15. That is, to about 27 = 128 times smaller range. The resulting
variation in the attenuation is shown in Figure 6.72. It is evident that the stopband sensitivity is much
larger compared to the sensitivity for the corresponding ladder structure and that the precision in the
adaptor coefﬁcients is not enough. The passband variation, when the error range is decreased to −2−8
to 2−8, is shown in Figure 6.73.
1.06.8 Frequency response masking (FRM) structure
For given passband ripple and stopband attenuation, the ﬁlter order, as well as the number of non-trivial
coefﬁcients, of a direct form FIR ﬁlter, is according to (6.8) inversely proportional to the transition width.
Therefore, sharp ﬁlters suffer from high computational complexity due to the high ﬁlter orders. Lim [66]
proposed a frequency response masking (FRM) FIR ﬁlter structure in 1986 for the design of sharp FIR
ﬁlters with low computational complexity. The structure was thereafter modiﬁed and improved by many
researchers to achieve even further computation savings, and has been extended to the design of various
types of ﬁlters, such as half-band ﬁlters, multirate ﬁlters, recursive ﬁlters, and two-dimensional ﬁlters.
1.06.8.1 Basic FRM structure
Just as the name implies, FRM uses masking ﬁlters to obtain the desired frequency responses. The overall
structure of an FRM is a subﬁlter network, consisting of three subﬁlters, as shown in Figure 6.74. The
three subﬁlters are a bandedge shaping prototype ﬁlter with z-transform transfer function F(z) and two
masking ﬁlters with z-transform transfer functions G1(z) and G2(z), respectively; their corresponding
frequency responses are denoted as F(e jωT ), G1(e jωT ), and G2(e jωT ), respectively.
In FRM structure, every delay of the bandedge shaping prototype ﬁlter F(z) is replaced by L delays.
The frequency response of the resulting ﬁlter, F(zL), is a frequency compressed (by a factor of L) and
periodic version of F(z), as shown in Figures 6.75 a and b; the transition band of F(z) is mapped to

308
CHAPTER 6 Digital Filter Structures and Their Implementation
)
( Lz
F
)
(
1 z
G
2
/
F
LN
z−
)
(
2 z
+
+
In
Out
G
FIGURE 6.74
Structure of the frequency response masking ﬁlter.
L transition bands with transition width shrunk by a factor of L. The complementary ﬁlter of F(zL)
is obtained by subtracting F(zL) from a pure delay term, z−LNF/2, where NF is the order of F(z).
(To avoid a half delay, NF is chosen to be even.) Thus, the entire frequency region from DC to Nyquist
frequency is decomposed into L bands; F(zL) and its complement hold alternate band respectively, as
shown in Figure 6.75b. Using two masking ﬁlters G1(z) and G2(z), whose frequency responses are
given in Figure 6.75c, to ﬁlter the outputs of F(zL) and its complement, the desired frequency bands
are kept and then combined to obtain the ﬁnal frequency response, shown in Figure 6.75d.
Throughout this chapter, for simplicity, the number of delays replacing one delay in the impulse
response of the bandedge shaping prototype ﬁlter, L, is referred to as the compression factor of the FRM
ﬁlter.
The overall z-transform transfer function of the FRM structure, H(z), is given by,
H(z) = F(zL)G1(z) +

z−LNF/2 −F(zL)

G2(z)
(6.85)
In a synthesis problem, the passband and stopband edges of the overall ﬁlter H(z), denoted as ωcT
and ωsT , respectively, are given, whereas the passband and stopband edges of the prototype ﬁlter F(z),
denoted as θ and ϕ, have to be determined. Depending on, if the transition band is generated from F(zL)
or its complement, as shown in Figures 6.75d and f, respectively, ωcT and ωsT are given by
ωcT = 2lπ + θ
L
,
ωsT = 2lπ + ϕ
L
(6.86)
and
ωcT = 2lπ −ϕ
L
,
ωsT = 2lπ −θ
L
(6.87)
respectively. Denoting the two cases as Case A and Case B, for a given L, it is shown that only one or
none of the above two cases results in an F(e jωT ), so that 0 ≤θ ≤ϕ ≤π is satisﬁed.
Case A,
θ = ωcT L −2lπ,
ϕ = ωsT L −2lπ
for l = ⌊ωcT L/2π⌋.
Case B,
θ = 2lπ −ωsT L,
ϕ = 2lπ −ωcT L
for l = ⌈ωsT L/2π⌉,
where ⌊x⌋is the largest integer less than x, and ⌈x⌉is the smallest integer larger than x. Figures 6.75c
and d show a Case A example, whereas Figures 6.75e and f show a Case B example.

1.06.8 Frequency Response Masking (FRM) Structure
309
(a)
(b)
(c)
(d)
π
0
0
0
0
0
0
1
0 0
1
1
1
)
(
T
je
F
ω
θ
φ
T
s
ω
)
(
1
T
je
G
ω
)
(
2
T
je
G
ω
)
(
T
jL
e
F
ω
)
(
2
/
T
jL
T
jLN
e
F
e
F
ω
ω
−
−
π
π
T
ω
π
(e)
(f)
0
0
0
0
1
1
)
(
1
T
je
G
ω
)
(
2
T
je
G
ω
π
π
)
(
T
je
H
ω
)
(
T
je
H
ω
L
l
φ
π +
2
L
l
φ
π −
+ )
1
(
2
L
l
θ
π −
2
L
l
θ
π +
2
L
l
θ
π−
+ )
1
(
2
L
l
φ
π +
2
L
l
φ
π −
+ )
1
(
2
L
l
θ
π+
+ )1
(
2
T
ω
T
ω
T
ω
T
ω
T
ω
T
c
ω
T
c
ω
T
s
ω
FIGURE 6.75
Frequency response of FRM subﬁlters.
Since the transition width of F(z) is L(ωsT −ωcT ) for given ωcT and ωsT , and the sum of the
transition width of G1(z) and G2(z) is 2φ/L, the complexity of F(z) decreases with increasing L, while
the complexity of G1(z) and G2(z) increases with increasing L. The best L minimizing the arithmetic
complexity in terms of the number of multipliers is achieved by
L =
1
√2β(ωsT −ωcT )/π ,
(6.88)
where β = 1 if the subﬁlters are optimized separately [67], and β = 0.6 if the subﬁlters are optimized
jointly [68].
The basic FRM structure, utilizing the complementary periodic ﬁlter pairs and masking ﬁlters,
synthesizes sharp ﬁlter with low arithmetic complexity. When [67]
ωsT −ωcT < 0.126π rad
the arithmetic complexity saving is achieved by using FRM structure, compared with the direct form
FIR structures. In a typical design with ωsT −ωcT < 0.002π rad, the FRM structure reduces the

310
CHAPTER 6 Digital Filter Structures and Their Implementation
number of multipliers by a factor of 10, and the price paid for the arithmetic saving is a less than 5%
increase in the overall ﬁlter order [68].
A special case of FRM ﬁlter, named as interpolated FIR (IFIR) ﬁlter [69], was earlier developed by
Neuvo et al. IFIR consists of the upper branch of the FRM structure, and can therefore only be used to
design narrow and wide band sharp FIR ﬁlters.
Some modiﬁed FRM structures were proposed in [70–72] for further complexity reductions mainly
of the masking ﬁlters.
1.06.8.2 Multistage FRM structure
In the above basic FRM structure, if the order of F(z) is too high due to a sharp transition width of
F(z), the FRM technique can be used recursively to form a multistage FRM structure to reduce the
complexity.
In a K-stage FRM structure, the z-transform transfer function of the overall system H(z) is recursively
constructed as
H(z) = F0(z),
Fk−1(z) = Fk(zLk)Gk
1(z) +

z−
Lk Nk
F
2
−Fk(zLk)

Gk
2(z),
for k = 1, 2, . . . , K, where N k
F is the ﬁlter order of the kth stage bandedge shaping prototype ﬁlter
Fk(z), Gk
1(z) and Gk
2(z) are the kth stage masking ﬁlters, with orders of N k
1 and N k
2 , respectively. An
example of a 3-stage FRM structure is shown in Figure 6.76.
It was discovered theoretically in [67] that the optimum number of stages to achieve the minimum
arithmetic complexity is independent of the band ripple requirements, but solely determined by the
)
(
1
2
3
3
L
L
L
z
F
2
/
3
1
2
3
F
N
L
L
L
z
−
)
(
1
2
3
2
L
Lz
G
+
In
)
(
1
2
3
1
L
Lz
G
+
)
(
1
2
2
L
z
G
+
)
(
1
2
1
Lz
G
+
2
/)
,
max(
3
2
3
1
1
2
N
N
L
L
z−
)
(
1
2
z
G
+
)
(
1
1 z
G
+
2
/)
,
max(
2
2
2
1
1
N
N
L
z−
Out
)
(
1
2
2
L
L
z
F
)
(
1
1
Lz
F
)
(
)
(
0 z
F
z
H
=
FIGURE 6.76
A 3-stage FRM structure.

1.06.8 Frequency Response Masking (FRM) Structure
311
transition width. The optimum value of the number of stage K satisﬁes the constraint
βb(K) ≤ωsT −ωcT ≤βb(K −1),
(6.89)
where βb(K) ∼= 1
4

K+1
K+2
(K+1)(K+2)
. When a ﬁlter is constructed in FRM structure with the opti-
mum number of stage Kopt satisfying the constraint in (6.89), the compression factors of Lk for
k = 1, 2, . . . , K, tends to be equal, and are given by
 Kopt + 1
Kopt
Kopt
≤Lk ≤
 Kopt + 2
Kopt + 1
Kopt+2
.
(6.90)
It is interesting to note that both the lower bound and upper bound in (6.90) approaches
e(∼=2.7, the base of the nature logarithm) when K is large.
Practical design experience showed that for a given ﬁlter transition width and a given stage K, the
minimum arithmetic complexity is achieved when [68]
Lk =

1
(K+1)√2β(ωsT −ωcT )/π

,
(6.91)
for k = 1, 2, . . . , K, where β = 1 if the subﬁlters are optimized separately [67], and β = 0.6 if the
subﬁlters are optimized jointly [68].
Considering both the theoretic analysis in [67] and the practical design experience in [68], the overall
arithmetic complexity of a given ﬁlter speciﬁcation is in proximity to the minimum if K in (6.91) is
chosen such that Lk is 2 or 3.
1.06.8.3 FRM structure for half-band ﬁlter and Hilbert transformer
The synthesis of half-band ﬁlters using FRM has to ensure, that the alternate coefﬁcient values are trivial
except, that the centre coefﬁcient has a value of 0.5 [73].
Consider a zero phase half-band bandedge shaping prototype ﬁlter of order NF = 4N −2, and
transfer function F(z) given by
F(z) = 1 + F0(z),
(6.92)
where F0(z) consists of only odd powers of z. A half-band FRM ﬁlter H(z) can be constructed from
this prototype ﬁlter by
H(z) =

1 + 1
2 F0(zL)

G1(z) +

1 −1
2 F0(zL)

[1 −G1( −z)]
(6.93)
for odd L, where G1(z) is an even order lowpass masking ﬁlter. Let Go(z) consist of odd power terms
of z, and Ge(z) consist of even power terms of z, of the masking ﬁlter G1(z) , respectively. Hence
G1(z) = Go(z) + Ge(z), and we have
H(z) = 1
2 + Go(z) + F0(zL)[2Ge(z) −1].
(6.94)

312
CHAPTER 6 Digital Filter Structures and Their Implementation
1/2
Go(z)
Fo(zL)
2Ge(z) 1
Input
Output
2Fo(jLzL)
2Ge(jz) 1
Input
Output
2Go(jz)
(a)
(b)
FIGURE 6.77
Structure for synthesizing a (a) half-band ﬁlter and (b) Hilbert transformer.
Thus, H(z) consisting of a constant term and terms with odd power of z, is a half-band ﬁlter. The
ﬁlter structure is shown in Figure 6.77a.
A Hilbert transformer may be derived from a unity half-band ﬁlter by subtracting the constant 1/2
from its transfer function, modulating the remaining coefﬁcients by e jnπ/2, and then scaling the response
by 2. Thus, the transfer function of the Hilbert transformer HH(z) can be written from (6.94) to [74]
HH(z) = 2Go( jz) + 2Fo( j LzL)[2Ge( jz) −1]
(6.95)
The structure of Hilbert transformer is shown in Figure 6.77 (b).
1.06.8.4 FRM structure for multirate ﬁlter and ﬁlter bank
1.06.8.4.1
Decimator and interpolator
In polyphase implementation of multirate ﬁlters, such as interpolators and decimators with a sampling
rate change factor of M, that are realized as a cascade of two ﬁlters, generally only one of the two ﬁlters
may achieve the “factor of M” reduction in computational complexity, unless the output of the ﬁrst ﬁlter
is nonzero only at intervals of M. In the original FRM structure shown in Figure 6.74, if L is an integer
multiple of M, the outputs of F(zL) and its complementary ﬁlter can be nonzero only at intervals of M.
However, to synthesize a ﬁlter with transition band centered at π/M as in the interpolator and decimator,
the original FRM has constraints on the selection of the compression factor L that neither LωcT /π nor
LωsT /π is an integer according to (6.86) and (6.87), and ⌊LωcT /π⌋= ⌊LωsT /π⌋. Therefore, due
to these two constraints, L cannot be selected as an integer multiple of M to achieve a “factor of M
reduction” in computational complexity in multirate ﬁlters.
A variant of the FRM structure proposed in [74] achieves the “factor of M reduction” in a multi-
rate ﬁlter with sampling rate change factor M. The following is an illustration of an interpolator with
an upsampling rate M. The same technique applies to decimators as well. The variant of the FRM is
constructed from
H(z) = 1
2Gs(z) + Fo(zL)Gd(z),
(6.96)
where
Gs(z) = G1(z) + G2(z),
Gd(z) = G1(z) −G2(z),

1.06.8 Frequency Response Masking (FRM) Structure
313
Fo(zL)
Gd(z)
Input
Output
Gs(z)
1
2
FIGURE 6.78
FRM structure for the synthesis of decimation and interpolation ﬁlters.
and Fo(z)isgivenintheconditionof(6.92)forahalf-bandprototypebandedgeshapingﬁlter F(z); G1(z)
and G2(z) are the masking ﬁlters as that in Figure 6.74. The factor M interpolation or decimation ﬁlter,
constructed in such a manner, has a transition band centered at π/M. The ﬁlter structure is shown in
Figure 6.78.
For M even, 2L has to be an odd multiple of M, while for M odd, 2L has to be an even multiple of
M. In addition, when M is odd, F(z) is replaced by F(ze−j π
2L ), and G1(z) and G2(z) are replaced by
G1(ze−j 2θ
L ) and G2(ze j 2θ
L ), respectively, where G(z) is a prototype masking ﬁlter speciﬁed by passband
and stopband edges of θG =

2L−M

π
2LM
and ϕG = (2L+M)π
2LM
, respectively.
In implementation, both Gs(z) and Fo(zL) can be decomposed into M polyphase components efﬁ-
ciently using polyphase structures since the inputs are nonzero only at intervals of M. For Gd(z), besides
the fact that the output of Fo(zL) is nonzero at intervals of M samples, only the ﬁrst polyphase compo-
nent of Fo(zL) has nonzero coefﬁcients, because it is a function of z−2L and 2L is an integer multiple
of M. Therefore, Gd(z) may be decomposed into M polyphase components efﬁciently as well.
In [75], two alternative FRM structures are proposed for the design of interpolation and decimation
by a factor of M, where the transition band is not restricted to include π/M. In this approach, additional
constraints are imposed to the masking ﬁlters, such that
G2(z) = G1(z) −
M
M −1G10(zM) +
1
M −1
(6.97)
or
G1(z) = G2(z) −
M
M −1G20(zM) +
1
M −1,
(6.98)
where G10(z) and G20(z) are the 0-th polyphase components in the polyphase representations of G1(z)
and G2(z), respectively. The relations in (6.97) and (6.98) specify the class I and class II FRM structures.
These relations impose further constraints on the selection of l and L in (6.86) and (6.87). However,
the available choices are sufﬁcient to obtain arbitrary frequency selectivity in the ﬁlter design and
meanwhile achieve complexity reduction.
When the masking ﬁlters are related to each other as those shown in (6.97) and (6.98), both the
bandedge shaping ﬁlter and the masking ﬁlters can be decomposed into polyphase structure, such that
all ﬁlters are working at the lower sampling rate involved in the interpolation or decimation.

314
CHAPTER 6 Digital Filter Structures and Their Implementation
FIGURE 6.79
Two-channel maximally decimated ﬁlter bank.
It is interested to note that both (6.97) and (6.98) are reduced to G2(z) = 1−G1(z) when M = 2; this
relation is the same as that in the half-band FRM structure given in (6.93). In fact, when the transition
band is restricted to include π/M, the proposed approach is a generalization of that in [73] and an Mth
band ﬁlters are obtained.
Besides the above two FRM ﬁlter structures, a factor of two interpolation and decimation ﬁlters are
developed in [76], based on a recursive ﬁlter structure introduced in next section.
1.06.8.4.2
Filter banks
A two-channel maximally decimated ﬁlter bank utilizing the FRM structure is proposed in [77]. In
the diagram of a two-channel ﬁlter bank shown in Figure 6.79, each analysis and synthesis ﬁlter of
the proposed technique is realized through a modiﬁed FRM structure with the same bandedge shaping
prototype ﬁlter. The transfer functions of the analysis and synthesis ﬁlters are given by
Hak(z) = F(zL)Gk1(z) + Fc(zL)Gk2(z),
Hsk(z) = F(zL)Gk1(z) −Fc(zL)Gk2(z),
for k = 0, 1.
In this FRM structure, the original magnitude complementary ﬁlter of the bandedge shaping ﬁlter,
1 −F(z), is modiﬁed to an approximately power complementary ﬁlter, i.e., Fc(z) is related to F(z) in
a form of
Fc(z) = F( −z)
and the ﬁlter order is constrained to odd order. In addition, the compression factor L is selected to be
odd. By relating the masking ﬁlters in the lower branch, G11(z) and G12(z), to that in the upper branch,
G01(z) and G02(z), respectively as
G11(z) = −G02( −z)
and
G12(z) = G01( −z)
an aliasing free ﬁlter bank is constructed.
This technique is further extended to a modulated M-channel ﬁlter banks by using the analysis
and synthesis ﬁlters in the upper branch of the two-channel ﬁlter bank as the prototype ﬁlters for

1.06.8 Frequency Response Masking (FRM) Structure
315
modulation [78]. Together with a cosine modulation block and a sine modulation block, a near perfect
reconstructed M-channel maximally decimated ﬁlter bank is developed.
Other FRM structures for ﬁlter banks are mainly developed for two-channel ﬁlter banks, including
non-uniform ﬁlter banks with rational sampling factor [79] and multi-plet perfect reconstruction ﬁlter
banks [80]. In addition, a non-multirate fast ﬁlter bank (FFB) is developed in [81], to realize the function
of sliding FFT ﬁlter bank with low computational complexity, but achieving high frequency selectivity
and passband performance.
1.06.8.4.3
Cosine-modulated transmultiplexer
Due to the same constraints on the selection of the compression factor L, the prototype bandedge shaping
ﬁlter for an M band transmultiplexer (TMUX) is not realizable if L is an integer multiple of M. Unlike the
variant structure proposed in [74], in [82], an implementation of an M band cosine-modulated TMUX
(CMT) for
L = 2Ka M + M
Kb
,
(6.99)
where Ka is a non-negative integer and Kb is a positive integer, is proposed.
If only the upper branch in the original FRM structure is used as the prototype ﬁlter for the CMT,
the transfer function for the analysis ﬁlter becomes
Hm(z) =
N

n=0
cm,n

f L ∗g1

(n)z−n,
(6.100)
where cm,n = 2 cos

(2m+1)

n−N
2

2M
+ ( −1)m π
4

for m = 0, 1, . . . , M −1 and n = 0, 1, . . . , N −1,
and the term ( f L ∗g1)(n) denotes the convolution between the impulse responses of F(zL) and G1(z).
Thus, F(zL) can be decomposed to Q = 2Kb polyphase components assuming that NF = QKc −1
with Kc being a positive integer, and G1(z) can be decomposed to 2M polyphase components assuming
that N1 = 2Kd M −1 with Kd being a positive integer. Under the relation of L and M in (6.99), and
cm,(n+2kM) = (−1)kcm,n, the overall polyphase representation of Hm(z) is given by
Hm(z) =
Q−1

q=0
⎡
⎣z−Lq Eq
F( −zLQ) ×
2M−1

p=0
cm,

n+ M
Kb q
z−pE p
G

−z2M
⎤
⎦,
where Eq
F is the qth polyphase component of the prototype bandedge shaping ﬁlter F(z), given by
Eq
F(z) =
Kc

k=0
(−1)Kaq f (kQ + q)z−k
and E p
G(z) is the pth polyphase component of the masking ﬁlter G1(z) given by
E p
G(z) =
Kd−1

k=0
g1(2kM + p)z−k.

316
CHAPTER 6 Digital Filter Structures and Their Implementation
F1(zL)
F2(zL)
G1(z)
G2(z)
Input
Output
1/2
+
+
+
FIGURE 6.80
The overall structure of the recursive ﬁlter.
If both branches in the FRM design are used, this decomposition can be applied to the lower branch,
and by enforcing both masking ﬁlters to have the same order N1 = N2, the responses of the two branches
can be added together just before the modulation stage.
1.06.8.5 FRM structure for recursive ﬁlter
While FRM structures were originally and mainly developed for the design of sharp ﬁlters with low
computational complexity, a hybrid IIR and FIR ﬁlter structure was proposed for the design of high
speed-recursive digital ﬁlters [83].
In the proposed structures, the bandedge shaping prototype ﬁlters are power complementary IIR
ﬁlters, denoted as F(z) and Fc(z), realized as a parallel connection of two allpass ﬁlters, and the
masking ﬁlters are linear-phase FIR ﬁlters. The bandedge shaping prototype ﬁlters are expressed as
F(z) = F1(z) + F2(z)
2
and
Fc(z) = F1(z) −F2(z)
2
,
where F1(z) and F2(z) are stable allpass ﬁlters. When the magnitude responses of the power comple-
mentary IIR ﬁlter pair F(z) and Fc(z) are bounded by unity, the overall transfer function is expressible
as
H(z) = F1(zL) + F2(zL)
2
G1(z) + F1(zL) −F2(zL)
2
G2(z),
where G1(z) and G2(z) are the linear-phase FIR masking ﬁlters. The structure of the overall ﬁlter is
shown in Figure 6.80.
Since the maximum sample frequency of recursive ﬁlters is determined by the ratio of the number
of delay elements and the operational latency in the critical loop of the ﬁlter realization, see (6.102),
this structure introduced at least L delay elements in its critical loop, resulting in an L-fold increase of
the maximal sample frequency.
1.06.8.6 Two-dimensional FRM structure
The extension of the FRM structure to two-dimensional (2D) sharp ﬁlters is not straightforward. The
main difﬁculty lies in the generation of complementary bandedge shaping ﬁlters, whose frequency
responses of the compressed passbands should be separated by stopbands and able to be masked by the
subsequent masking ﬁlters.

1.06.8 Frequency Response Masking (FRM) Structure
317
R1
R1
R1
R1
R1
R2
R2
R3
R3
R4
R4
/L
R2
/L
/L
/L
R2
FIGURE 6.81
The four frequency regions for the complementary components of the 2D FRM ﬁlter.
A 2D diamond-shaped ﬁlter using FRM structure was proposed in [84]. The idea is to divide the
frequency spectrum into four suitably chosen complementary components Rk for k = 1, 2, 3, and 4,
as shown in Figure 6.81, in the 2D region

−π
L , π
L
2, for a compression factor of L. These regions are
also deﬁned for the periodic repetitions of the 2D region

−π
L , π
L
2.
These four complementary regions are obtained by four ﬁlters ˆFk(z1, z2) for k = 1, 2, 3, and 4. To
make the four ﬁlters complementary, an error ﬁlter, denoted as Fe(z1, z2), is introduced, such that
4

k=1
ˆFk(z1, z2) + Fe(z1, z2) = 1.
(6.101)
Thus, the four bandedge shaping prototype ﬁlters, denoted as Fk(z1, z2), are given by
Fk(z1, z2) = ˆFk(z1, z2) + Fe(z1, z2)
4
.
Each delay, in each dimension of the four bandedge shaping prototype ﬁlters, is replaced by L delays
and four complementary bandedge shaping ﬁlters Fk(zL
1 , zL
2 ) are obtained. The output of Fk(zL
1 , zL
2 )
are then ﬁltered by respective masking ﬁlters Gk(z1, z2), and the resulting outputs are summed to
form the ﬁnal output. The generation of the complementary bandedge shaping ﬁlter and the overall 2D
diamond-shaped FRM ﬁlter structure is shown in Figure 6.82.
Thus, based on the same principle of one-dimensional FRM ﬁlters, the speciﬁcations of the bandedge
shaping prototype ﬁlters and masking ﬁlters can be derived for the design procedure.

318
CHAPTER 6 Digital Filter Structures and Their Implementation
F1(z1
L, z2
L)
F2(z1
L, z2
L)
F3(z1
L, z2
L)
F4(z1
L, z2
L)
G1(z1, z2)
G2(z1, z2)
G3(z1, z2)
G4(z1, z2)
Input
Output
0.25
+
+
+
+
+
+
+
+
+
+
FIGURE 6.82
The overall ﬁlter structure of the 2D diamond-shaped FRM ﬁlter.
1.06.8.7 Summary
As one of the most successful low computational complexity digital ﬁlter structures, basic FRM ﬁlter
consists of an elegant ﬁlter network with two branches comprising three subﬁlters and realizes frequency
responses owning sharp transition band. The price paid, for the high computational saving, is a small
increase in the ﬁlter order. Further developments on the basic FRM structure have extended the technique
to various areas of digital ﬁlters, including but not limited to recursive ﬁlters, multirate ﬁlters, ﬁlter banks,
and 2D ﬁlters.
1.06.9 Computational properties of ﬁlter algorithms
In order to compare and evaluate different ﬁlter structures we need to discuss a few basic properties of
algorithms and their implementation.
1.06.9.1 Latency and throughput
We deﬁne latency, as the time it takes to generate an output value from the corresponding input value,
as illustrated in Figure 6.83, for an arithmetic operation. The throughput (samples/s) is deﬁned as the
reciprocal of the time between successive outputs. In sequential algorithms it is possible to increase the
throughput of an algorithm–for example, by using pipelining [1].
1.06.9.2 Maximal sample rate
The maximal sample rate of an algorithm is determined only by its recursive loops [1,85]. Nonrecursive
parts of the signal-ﬂow graph, e.g., input and output branches, generally do not limit the sample rate, but
to achieve this limit, additional delay elements may have to be introduced into the nonrecursive branches
[1]. The maximal sample frequency for an iterative recursive algorithm, described by a fully-speciﬁed
signal-ﬂow graph [1], is
fmax = min
i
	 Ni
Top
 
,
(6.102)

1.06.9 Computational Properties of Filter Algorithms
319
Input x
Input a
Output y
TLatency
t
1
Throughput
FIGURE 6.83
Latency and throughput.
where Top is the total latency due to the arithmetic operations, and Ni is the number of delay elements
in the directed loop i. We will later deﬁne the latency for different arithmetic operations. The loop with
the lowest fmax is called the critical loop. The minimum sample period is also referred to as the iteration
period bound. It is, of course, not possible to improve the iteration period bound for a given algorithm.
However, a new algorithm with a higher bound can often be derived from the original algorithm using
algorithm transformations [1,86,87]. We recognize from (6.102) that there are several possibilities to
improve the bound:
•
Reduce the operation latency, Top, in the critical loop.
•
Introduce additional delay elements into the loop, without changing the behavior.
•
Remove superﬂuous arithmetic or logic operations from the loop.
1.06.9.3 Cyclic scheduling
In order to achieve a maximally fast implementation that achieve the bound in (6.102), we must generally
connect m computation graphs, that represents the operations within single sample interval [1], as
illustrated in Figure 6.84.
This will result in a periodic scheduling formulation that allows a schedule period, that is m times
longer then the sample interval. This cyclic formulation will under certain conditions result in a maxi-
mally fast, periodic schedule. Unfortunately, it is not possible to determine the best choice of m in the
general case. In order to attain the minimum sample period, it is necessary to perform cyclic scheduling
of the operations belonging to several successive sample intervals if
•
The latency for a PE is longer than Tmin or
•
The critical loop(s) contains more than one delay element.
Generally, the critical loop should be at least as long as the longest latency for any of the PEs in the loop.
Bit-serial and digit-serial operations with latencies longer than the minimal sample period can thereby

320
CHAPTER 6 Digital Filter Structures and Their Implementation
x(mn)
y(mn)
x(mn+1)
y(mn+m–1)
N m–1
N0
N1
...
...
...
...
t
FIGURE 6.84
Cyclic scheduling.
be completed within the scheduling period. The minimum number of sample periods for the schedule is
m =
⎡
⎢⎢⎢
max (Tlatency,i)
i
Tmin
⎤
⎥⎥⎥
,
(6.103)
where Tlatency,i is the latency of operation i. From the cyclic scheduling formulation it is possible to ﬁnd
a resource-optimal schedule with a minimum number of concurrent operations.
1.06.10 Architecture
A major problem when implementing an algorithm is to decide upon a suitable hardware architecture.
We may select to use one or several processing elements (PE) and one or several memories, or memory
ports, and among a variety of communication networks between PEs and the memories [1]. Typically
the design target is to minimize the implementation cost, e.g., chip area and power consumption, while
meeting the throughput requirement. Normally, in digital signal processing applications there is no
beneﬁt in having a higher throughput than required.
We recommend that the arithmetic operations are scheduled cyclically to attain a maximally fast
implementation as illustrated in Figure 6.84. A maximally fast schedule and the corresponding imple-
mentation is obtained by using an isomorphic mapping between the arithmetic operations and the
processing elements [88–90]. This allow the hardware to operate with a low power supply voltage
and low power consumption. Moreover, the communication network becomes simple and static. There
exists graphic based methods to determine the number of independent memories or memory ports that
are required [1]. Recently, it has been shown that a maximally fast implementation is obtainable using
a numerically equivalent state-space representation and distributed arithmetic [91].

1.06.11 Arithmetic Operations
321
1.06.11 Arithmetic operations
Here we are mainly interested in the arithmetic operations: addition, subtraction, and multiplications as
well as sum-of-products. There exist many alternative arithmetic algorithms to perform these operations.
Consider, for example, addition. Addition of two numbers, independently of their sign, can be performed
using the same circuit, if two’s-complement representation is used. Moreover changing the circuit for
addition into a subtractor requires only that the bits of one of the inputs are inverted. An advantage
of the two’s-complement number representation is that several numbers can be added, with possible
intermediate overﬂows, if we can guarantee that the result is within the correct number range. This
situation occur frequently in digital ﬁlter algorithms. A drawback is, however, that the carry propagation
limits the latency. Several techniques to speed-up the computation of the carries as well as carry-free
number representations have been proposed [3].
1.06.11.1 Addition and subtraction
The operation of adding two or more numbers is in many ways the most fundamental arithmetic opera-
tion, since most other operations in one way or another, are based on addition. The operands of concern
here are either two’s-complement or unsigned representation. Most DSP applications use fractional
arithmetic instead of integer arithmetic [1]. The sum of two W-bit numbers is a (W + 1)-bit number
while the product of two binary W-bit numbers is a 2W-bit number. In many cases, and always in recur-
sive algorithms, the resulting number needs to be quantized to a W-bit number. Hence, the question
is which bits of the result are to be retained. In fractional arithmetic, two’s-complement numbers are
interpreted as being in the range [−1,1], i.e.,
x = −x0 +
W−1

i=1
xi2−i.
(6.104)
Hence, the most signiﬁcant part of a product is contained in the bits (x0, x1, . . .). We use the graphic
representation shown in Figure 6.85 to represent the operands and the sum bits with the most signiﬁcant
bit to the left.
+
FIGURE 6.85
Illustration of addition of two binary numbers.

322
CHAPTER 6 Digital Filter Structures and Their Implementation
FA
FA
FA
x0 y0
s0
FA
xWf −1 yWf −1xWf yWf
xWf −2yWf −2
sWf
sWf−2
sWf−1
cin=0
cout
FIGURE 6.86
Ripple-carry adder.
1.06.11.1.1
Ripple-carry addition
A straightforward way to add two numbers is to sequentially add the two bits of the same signiﬁcance
and the carry from the previous stage using a full adder (FA) and propagate the carry bit to the next stage.
This is called ripple-carry addition and is illustrated in Figure 6.86. Note that the operation propagate
from right to the left and that only one FA perform have correct inputs at a give moment. Hence, the
circuit performs addition sequentially, one bit at a time, even though it is referred to as a parallel adder.
This type of adder can add both unsigned and two’s-complement numbers.
The major drawback with the ripple-carry adder is that the worst-case delay is proportional to the
wordlength. Also, typically the ripple-carry adder will produce many glitches since the full adders
perform switching operations without having correct carries. This situation is improved if the delay for
the carry bit is smaller than that of the sum bit [92].
However, due to the simple design the energy per computation is still reasonable small [93]. Alter-
natively all pairs of two bits of the same signiﬁcance can be added simultaneously and then the carries
are added using some efﬁcient scheme. There are many adder schemes proposed, for more details we
refer to e.g., [93]. For carry-save addition we refer to [94,95]. It is also possible to perform addition in
constant time using redundant number systems such as signed-digit or carry-save representations. An
alternative is to use residue number systems (RNS), which breaks the carry-chain into several shorter
ones [96].
1.06.11.1.2
Bit-serial addition and subtraction
In bit-serial arithmetic the inputs x and y are normally processed with the least-signiﬁcant bit ﬁrst.
Bit-serial numbers in two’s-complement representation can be added or subtracted with the circuits
shown in Figure 6.87.
Since the carries are saved from one bit position to the next, the circuits are called carry-save adder
and carry-save subtractor, respectively. At the start of the addition the D ﬂip-ﬂop is reset (set) for the
adder (subtractor), respectively.
Figure 6.90 shows two latency models for bit-serial adders. The left most is suitable for static CMOS
and the right most for dynamic (clocked) logic styles. The time to add two W-bit numbers is W or W + 1
clock cycles. Note that the ﬁrst bit of the sum, which is delayed by the latency given in Figure 6.90, can
directly be feed to any subsequent bit-serial processing element (PE). Thus, the throughput of a complex
computation may not necessarily be determined by the long addition time [1] (see Figure 6.88).

1.06.11 Arithmetic Operations
323
D
D
x
y
C
Reset
Set
x
y
Sum
Diff
C
FA
FA
FIGURE 6.87
Bit-serial adder and subtracter.
D
FA
x(n)
y(n)
Sum(n)
x0
xWd–1
…
y0
x1
y1
yWd–1
…
…
∑0
∑1
Latency
+
D
D
FA
x(n)
y(n)
Sum(n)
x0
…
y0
x1
y1
yWd–1
∑Wd–1
…
…
∑0
∑1
Latency
+
xWd–2
yWd–2
Model 1
Model 0
xWd–1
∑Wd–1
FIGURE 6.88
Latency models for a bit-serial adder.
D
xd
x1
x0
yd
y1
y0
FA
FA
FA
sd
s1
s0
FIGURE 6.89
Digit-serial adder with digit-size d.
Comparing the ripple-carry adder and the bit-serial adder, we note that both operate sequentially with
one bit addition at a time. In the ripple-carry adder, each bit addition is mapped to its private FA, while
in the bit-serial adder, all bit additions are mapped to a time-multiplexed FA. Hence, the difference in
throughput is mainly due to the time lost in the D ﬂip-ﬂop. The chip area for bit-serial circuits is small
and consequently the wiring is also small.

324
CHAPTER 6 Digital Filter Structures and Their Implementation
x0
x1
xWd–2
yWd+Wcf
…
…
Latency
y0
yWd–1
…
Serial/parallel
multiplier
α x
)
n
(
y
)
n
(
x
α
D
xWd–1
Model 1
x0
x1
xWd–1
yWd+Wcf
…
…
Latency
y0
yWd–1
…
Serial/parallel
multiplier
α x
)
n
(
y
)
n
(
x
α
Model 0
FIGURE 6.90
Latency models for a serial/parallel multiplier.
1.06.11.1.3
Digit-serial addition and subtraction
From speed and power consumption points of view it may sometimes be advantageous to process several
bits at a time, so-called digit-serial processing [97–103]. The number of bits processed in a clock cycle
is referred to as the digit size. Figure 6.89 shows a d-bit digital-serial adder.
Theﬂip-ﬂoptransmitthecarrybetweensuccessivedigits.Incaseofsubtractionofatwo’s-complement
number, the negative value is instead added by inverting the bits and setting the carry ﬂip-ﬂop. Most of
the principles for bit-serial arithmetic can easily be extended to digit-serial arithmetic. An advantage
of bit-serial and digit-serial arithmetic is that less chip area is required and therefore the equivalent
switched capacitance and leakage current is low [104]. The difference in throughput between a conven-
tional word level and a digit-serial implementation is not great [105]. Both bit-serial and digit-serial are
suitable for implementations in FPGA circuits.
1.06.11.2 Multiplication
Bit-parallel multiplication of two numbers can be divided into two main steps:
•
partial product generation that determines the bits to be added, and
•
accumulation of the partial products.
Two main approaches to perform the accumulation have been proposed, namely array and tree
type accumulation [3]. Bit-serial and digit-serial multiplications are usually based on a shift-and-add
approach [1,106]. Here, however, we will focus on implementation techniques that use addition and
subtractions as the generic arithmetic operations.
To identify the critical loop in the ﬁlter, the total latency of the operations inside the loops must
be determined. The total latency of several sequential arithmetic operations depend on how they are
realized at the logic level, because the clock frequency is determined by the longest propagation path
between any two registers. The propagation paths do not only depend on the internal circuitry in an

1.06.11 Arithmetic Operations
325
arithmetic unit; they also depend on the paths between units. Hence, it may be efﬁcient to use pipelining
inside the critical loop in order to avoid long propagation paths between arithmetic units.
Two latency models for a serial/parallel multiplier are shown in Figure 6.90. Denoting the number
of fractional bits of the coefﬁcient by Wcf , the latencies are Wcf for latency model 0 and Wcf + 1 for
latency model 1. The corresponding latency models for digit-serial arithmetic are deﬁned in the same
manner.
In model 0, which corresponds to a static CMOS logic style without pipelining of the gates, the
latency is equal to the gate delay of a full adder. In model 1, which corresponds to a dynamic CMOS
logic style, or a static CMOS logic style with pipelining at the gate level, the full adder followed by a
D ﬂip-ﬂop making the latency equal to one clock cycle. Model 1 generally results in faster bit-serial
implementations, due to the shorter logic paths between the ﬂip-ﬂops in successive operations.
1.06.11.2.1
Example 6
Consider a lattice wave digital ﬁlter. The critical loop in a ﬁrst-order allpass section is indicated in
Figure 6.91 [89,90].
This loop containing two additions, one multiplication with α0, and one delay element has the
minimal sample period given by
Tmin1 = Tα0 + 2Tadd
1
.
(6.105)
In Figure 6.92 the critical loop in a second-order allpass section is indicated. For this loop consisting
of four additions, two multiplications, and one delay element, the corresponding minimal sample period
becomes
Tmin2 = Tαi + Tαi−1 + 4Tadd
1
.
(6.106)
The minimal sample period of the ﬁlter is then bounded by the section with the lowest throughput, i.e.,
the loop i that yields Tmin, since each section has to operate with the same throughput. Figure 6.93 shows
the scheduling of the operations belonging to m sample periods for a ﬁrst-order allpass section. The
shaded areas indicate execution time for the operations and darker shaded areas indicate their latency.
Scheduling of second-order allpass sections can be found in [1]. Since a bit-serial processing element
only processes 1 bit in each clock cycle, a complete arithmetic operation requires at least Wd clock
cycles. Therefore, if the minimal sample period of Tmin clock cycles, where Tmin < Wd, operations
T
α0
α0
T
FIGURE 6.91
First-order allpass section.

326
CHAPTER 6 Digital Filter Structures and Their Implementation
T
T
αi–1
αi
αi–1
αi
T
T
FIGURE 6.92
The critical loop in a second-order allpass section.
t
0
Tmin
mTmin
2Tmin…(m–1)Tmin
...
...
N0
N1
Nm–1
x(mn+1)
x(mn)
x(mn+m–1)
y(mn)
y(mn+1)
y(mn+m–1)
FIGURE 6.93
Maximally fast schedule for a ﬁrst-order allpass section.
belonging to m sample periods need to be included in the schedule. It can be shown that m must be
selected as
m ≥
Wcf + Wd + mn0
Tmin

,
(6.107)
where mn0 is 0 for model 0 and 1 for model 1, respectively in order to allow an arithmetic operation
requiring Wcf + Wd + mn0 clock cycles to be completed within the scheduling period. Thus, the

1.06.12 Sum-of-Products (SOP)
327
...
...
...
...
...
...
...
...
...
...
...
...
...
x(mn)
x(mn+1)
x(mn+m–1)
S0
S1
S2
Sk/2
S3
Sk/2–1
...
2y(mn)
2y(mn+1)
2y(mn+m–1)
FIGURE 6.94
Complete lattice wave digital ﬁlter.
c1
c2
cN
y
xN
x2
x1
c1
c2
cN
y1
yN
y2
x
FIGURE 6.95
Substructures suitable for multiple-constant multiplication.
scheduling period for a maximally fast schedule becomes mTmin. The implementation of the complete
ﬁlter is illustrated in Figure 6.94.
1.06.12 Sum-of-products (SOP)
In this section we discuss the two main techniques to simplify the hardware implementation of sum-of-
products and their transpose. Note that the arithmetic networks shown in Figure 6.84 consists of a set
of sum-of-products of the type shown in Figure 6.95. Consider the sum-of-products
y =
N

k=1
akxk.
(6.108)
Typically, the coefﬁcients, ak, are ﬁxed and xk are variable, but there are several common cases
where both ak and xk are variable, for example, computation of correlation, which require a general
multiplier.

328
CHAPTER 6 Digital Filter Structures and Their Implementation
Distributed arithmetic is an efﬁcient procedure for computing sum-of-products between a ﬁxed and
a variable data vector. The basic principle is owed to Croisier et al. [107]. Distributed arithmetic is
suitable for implementation of relative short sum-of-products, discrete cosine transforms (DCT), as
well as complex multiplications [1].
1.06.12.1 Multiple-constant multiplication (MCM)
Multiple-constant multiplication techniques [1,3] are efﬁcient methods to simplify multipliers with ﬁxed
coefﬁcients and thereby reduce the power consumption. Single multipliers and substructures of the type
showninFigure6.95canbesimpliﬁedsigniﬁcantlybyusingmultiple-constantmultiplicationtechniques
[4–7,108,109]. For the multiple-constant multiplication case several effective algorithms have been
proposed that avoids the problem of number representation dependency [110,111]. Theoretical lower
bounds for related problems have been presented in [11]. Moreover, using linear programming, the ﬁlter
coefﬁcient optimization and the MCM implementation can be jointly addressed [112,113].
Figure 6.96 shows a typical example of an MCM network. The basic idea is to reduce the number of
basic operations (i.e., add/sub-and-shift operations) by factoring out common factors in the coefﬁcients
ci. By ﬁrst generating a set of common subexpressions, d1, d2, d3, that in the next step are multiplied
with simple coefﬁcients, or added/subtracted, to generate new subexpressions, and so on [8,95]. A
main drawback of these techniques is that they are highly specialized for a particular SOP and can
therefore not be multiplexed to perform other SOPs. For multiple-constant multiplication, with many
coefﬁcients (for example for long FIR ﬁlters) the average increase of the number of adders/subtractors,
approaches one, for an increase of the length of the ﬁlter by one. This is due to the fact that most
subexpressions have already been generated and only a single addition/subtraction is needed to generate
another coefﬁcient. The problem of ﬁnding optimal solutions to the multiple-constant multiplication
problem is NP complete. Several heuristic algorithms have therefore been derived, e.g., [4–6,114,115].
Figure 6.97 shows a graph that simultaneously realize the multiplication with 7, 11, and 106. Only
three adder/subtractors and some shift operations are required. These techniques are efﬁcient for imple-
menting bit-parallel, digit-serial as well as bit-serial processing elements [1].
y4
y2
y3
y1
d1
d2
x
d3
FIGURE 6.96
MCM network.

1.06.12 Sum-of-Products (SOP)
329
128
4
8
–2
–1
106
1
11
7
x
y
FIGURE 6.97
MCM example.
An implementation according to Figure 6.84 will consist of a set of MCM networks where the input
to a network is taken from a register and the different subexpressions are SOP to be stored into other
registers.
1.06.12.2 Distributed arithmetic
Consider the sum-of-products in (6.108) and assuming that the coefﬁcients, ak, k = 1, 2, . . . , N are
ﬁxed. Two’s-complement representation is used for both coefﬁcients and data. The inputs are scaled so
that |xk| ≤1. The sum-of-products can be rewritten
y =
N

k=1
ak
⎛
⎝−xk0 +
Wd−1

i=1
xki2−i
⎞
⎠,
(6.109)
where xki is the ith bit in xk. By interchanging the order of the two summations we get
y = −
N

k=1
akxk0 +
Wd−1

i=1
N

k=1
(akxki)2−i
(6.110)
which can be written
y = −F0(x10, x20, . . . , xN0) +
Wd−1

i=1
Fi(x1i, x2i, . . . , xNi)2−i,
(6.111)
where
Fi(x1i, x2i, . . . , xNi) =
N

k=1
akxki.
(6.112)
F is a function of N binary variables, the kth variable being the ith bit in the data xk. Since Fi can take
on only a ﬁnite number of values, 2N, it can be computed and stored in a ROM (Read-Only Memory).
Equation (6.111) can, using Horner’s method for evaluating a polynomial for x = 0.5, be rewritten as
y = (((((0 + FWd−1)2−1 + . . . + F2)2−1 + F1)2−1 −F0)).
(6.113)
Figure 6.98 shows a block diagram for computing a sum-of-products according to (6.113).

330
CHAPTER 6 Digital Filter Structures and Their Implementation
ROM
   2N
words
WROM
LSB
x1
xN
Add/Sub
Reg.
WROM
WROM
SR
y
FIGURE 6.98
Block diagram for distributed arithmetic.
Inputs, x1, x2, . . . , xN are shifted bit-serially out from the shift registers with the least-signiﬁcant
bit ﬁrst. The bits xki are used as an address to the ROM storing the look-up table. Computation of
the sum-of-products starts by adding FWd−1 to the initially cleared accumulator register, REG. In the
next clock cycle, outputs from the shift registers address FWd−2, which is added to the value in the
accumulator register. The result is divided by 2 and stored back into REG. After Wd −1 clock cycles,
F0 is subtracted from the value in the accumulator register. The computation time is Wd clock cycles.
The required wordlength in the ROM, WROM, depends on the Fimax with the largest magnitude and the
coefﬁcient wordlength, Wc, and WROM = ⌈Wc + log2 (N)⌉.
The hardware cost is similar to a multiplier. In fact, most multiplier structures, e.g., serial/parallel,
array, and three structures multipliers, can be used for distributed arithmetic. Distributed arithmetic can
be used to implement ﬁrst- and second-order direct form I sections, FIR ﬁlters, as well as wave digital
ﬁlters [20,107,116–119].
1.06.12.2.1
Implementation of FIR ﬁlters using distributed arithmetic
Figure 6.99 shows an implementation of an eleventh-order linear-phase FIR ﬁlter. N/2 bit-serial adders
(subtractors) are used to sum the symmetrically placed values in the delay line. This reduces the number
of terms in the sum-of-products. Only 64 words are required whereas 212 = 4096 words are required
for the general case, e.g., a nonlinear-phase FIR ﬁlter. For higher-order FIR ﬁlters the reduction in the
number of terms by 50% is signiﬁcant. Further, the logic circuitry has been pipelined by introducing D
ﬂip-ﬂops between the adders (subtractors) and the ROM, and between the ROM and the shift-and-add
accumulator.
The number of words in the ROM is 2N where N is the number of terms in the sum-of-products.
The chip area for the ROM is small for sum-of-products with up to 5–6 terms. The basic approach is
useful for up to 10–11 terms. However, sum-of-products containing many terms, can be partitioned into
a number of smaller sum-of-products, which can be computed and summed by using either distributed
arithmetic or an adder tree. A complete sum-of-products PE and several similar types of PEs, for example
adaptors and butterﬂies, can be based on distributed arithmetic. The main parts of a sum-of-products
PE are the shift-accumulator, the coefﬁcient ROM with decoder, and the control circuits. Overﬂow and
quantization circuitry are also necessary, but it is often advantageous to move parts of this circuitry to
the serial/parallel converters used in the interconnection network [1].

1.06.12 Sum-of-Products (SOP)
331
D
D
FA
D
D
D
D
D
D
T
T
T
T
T
T
y(n)
D
D
ROM
SHIFT-ACCUMULATOR
D
FA
D
FA
D
FA
D
FA
D
FA
T
T
T
T
T
x(n)
FIGURE 6.99
Eleventh-order linear-phase FIR ﬁlter.
Distributed arithmetic can, of course, be implemented in parallel form, i.e., by allocating a ROM to
each of the terms in (6.113) and use an adder tree for the accumulation.
1.06.12.2.2
Memory reduction techniques
The amount of memory required becomes very large for long sum-of-products. There are mainly two
ways to reduce the memory requirements. Namely, partitioning and coding of input signals. The two
methods can be applied at the same time to obtain a very small amount of memory.
One way to reduce the overall memory requirement is to partition the memory into smaller pieces
that are added before the shift-accumulator as shown in Figure 6.100.
The amount of memory is reduced from 2N words to 2 · 2
N
2 words if the original memory is
partitioned into two parts. For example, for N = 10 we get 210 = 1024 words to 2 · 25 = 64 words.
Hence, this approach reduces the memory signiﬁcantly at the cost of an additional adder. In addition,
the partitioning may in some case be done so that long and short values are stored in different partitions.
Notice, depending on the values in the ROM, it is often favorable from both speed and area points of
view to implement a ROM by logic gates. Large ROMs tend to be slow.
The second approach is based on a special coding of the inputs, xi, which result in a symmetric ROM
content [1,107]. Memory size can be halved by using the ingenious scheme [107] based on the identity
x = 1
2(x −( −x)).
(6.114)
Thisapproachisalsousedforimplementationofacomplexmultiplierusingonlytwoshift-accumulators
and a common ROM [1].

332
CHAPTER 6 Digital Filter Structures and Their Implementation
ROM
   2N/2
words
LSB
X1
XN/2
Reg.
X2
ROM
   2N/2
words
XN/2+1
XN/2+2
XN
Add
Add/Sub
Y
FIGURE 6.100
Reducing the memory by partitioning.
1.06.13 Power reduction techniques
Many approaches to reduce the power consumption are based on various techniques to decrease the arith-
metic workload. This can be done at the algorithmic level by using efﬁcient, low-sensitivity structures
that allows few and simple coefﬁcients, and at the hardware level by using simpliﬁed multiplier real-
izations. Usually the power consumption, due to the memory, communication, and control, is neglected
in these techniques, although their contribution may be signiﬁcant. Polyphase structures and frequency
masking techniques are commonly used to reduce the arithmetic workload.
Anotherapproachisbasedonpowersupplyvoltagescalingwheretheimplementationisoperatedwith
a low supply voltage. Pipelining and interleaving are not applicable to recursive algorithms. However,
pipelining inside recursive loops can be performed at the logic level. It is therefore important to exploit
all parallelism of the recursive algorithm in order to obtain excess speed that allows a reduction in the
power supply voltage.
We propose the following strategy to design recursive digital ﬁlters with a low power consumption:
•
Select an inherently fast and low-sensitivity ﬁlter algorithm, e.g., ladder or lattice wave digital ﬁlters,
possibly in combination with frequency masking techniques.
•
Use an approximation with a diminishing ripple, which reduces the passband sensitivity in ladder
WDFs, and use discrete optimization techniques to reduce the operation latencies in the critical loop.
•
Use pipelining to split the nonrecursive parts into smaller pieces so that Tmin can be achieved.
•
Find a maximally fast and resource minimal schedule.
•
Use an isomorphic mapping to an optimal hardware structure with bit- or digit-serial PEs.
•
Use CMOS logic circuits that yield bit- or digit-serial PEs with a low power consumption, especially
for the D ﬂip-ﬂops in the bit-serial case.
•
Convert any excess circuit speed to a reduced power consumption by reducing the power supply
voltage.

References
333
Relevant Theory: Signal Processing Theory
See this Volume, Chapter 3 Discrete-Time Signals and Systems
See this Volume, Chapter 5 Sampling and Quantization
See this Volume, Chapter 7 Multirate Signal Processing
References
[1] L. Wanhammar, DSP Integrated Circuits, Academic Press, 1999.
[2] A.P. Chandrakasan, R.W. Brodersen, Low Power Digital CMOS Design, Kluwer, 1995.
[3] I. Koren, Computer Arithmetic Algorithms, Prentice-Hall, 1993.
[4] D.R. Bull, D.H. Horrocks, Primitive operator digital ﬁlters, IEE Proc. G 138 (3) (June 1991) 401–412.
[5] A.G. Dempster, M.D. Macleod, Constant integer multiplication using minimum adders, IEE Proc. Circuits
Devices Systems 141 (5) (1994) 407–413.
[6] A.G. Dempster, M.D. Macleod, Use of minimum-adder multiplier blocks in FIR digital ﬁlters, IEEE Trans.
Circuits Syst. II CAS-II-42 (9) (1995) 569–577.
[7] R.I. Hartley, Subexpression sharing in ﬁlters using canonic signed digit multipliers, IEEE Trans. Circuits
Syst. II, CAS-II-43, (10) (1996) 677–688.
[8] M.Martinez-Peiro,E.Boemo,L.Wanhammar,Designofhighspeedmultiplierlessﬁltersusinganonrecursive
signed common subexpression algorithm, IEEE Trans. Circuits Syst. II, CAS-II-49 (3) (2002) 196–203.
[9] O. Gustafsson, A.G. Dempster, On the use of multiple constant multiplication in polyphase FIR ﬁlters and
ﬁlter banks, in: Proc. Nordic Signal Processing Symp., vol. 3, Espoo, Finland, June 9–11, 2004, pp. 53–56.
[10] O. Gustafsson, H. Johansson, L. Wanhammar, MILP design of frequency-response masking FIR ﬁlters with
few SPT terms, in: Proc. Int. Symp. Control, Communications, Signal Processing, Hammamet, Tunisia,
March, 2004, pp. 21–24.
[11] O. Gustafsson, Lower bounds for constant multiplication problems, IEEE Trans. Circuits Syst. II CAS-II-54
(11) (2007) pp. 974–978.
[12] L.B. Jackson, Roundoff noise bounds derived from coefﬁcient sensitivities for digital ﬁlters, IEEE Trans.
Circ. Sys. CAS-23 (8) (1976) pp. 481–485.
[13] L.O. Chua, T. Lin, Chaos in digital ﬁlters, IEEE Trans. Circ. Sys. CAS-35 (6) (1988) pp. 648–658.
[14] L. Wanhammar, H. Johansson, Digital Filters Using MATLAB, Springer, 2013.
[15] G. Jovanovic-Dolecek (Ed.), Multirate Systems: Design and Applications, Idea Group Publ, Hersey, USA,
2001.
[16] L. Milic, Multirate Filtering for Digital Signal Processing: MATLAB Applications, Information Science
Reference, 2009.
[17] L. Wanhammar, H. Johansson, Toolbox: Digital Filters Using MATLAB.
<http://www.es.isy.
liu.se/software/>, 2012.
[18] R.W. Daniels, Approximation Methods for Electronic Filter Design, McGraw-Hill, New York, 1974.
[19] M.D. Lutovac, D.V. Tosic, B.L. Evan, Filter Design for Signal Processing Using MATLAB and Mathematica,
Prentice Hall, 2001.
[20] L. Wanhammar, Analog Filters Using MATLAB, Springer, 2009.
[21] L. Wanhammar, Toolbox: Analog Filters Using MATLAB. <http://www.es.isy.liu.se/software/>, 2009.
[22] A. Antoniou, Digital ﬁlters, Analysis, Design and Applications, McGraw-Hill, 2000.
[23] M.G. Bellanger, G. Bonnerot, M. Coudreuse, Digital ﬁltering by polyphase network: Application to sample-
rate alteration and ﬁlter banks, IEEE Trans. Acoust., Speech, Signal Process. ASSP-24 (2) (1976) 109–114.
[24] E.C. Ifeachor, B.W. Jervis, Digital Signal Processing: A Practical Approach, Addison-Wesley, 2002.

334
CHAPTER 6 Digital Filter Structures and Their Implementation
[25] A. Fettweis, Digital ﬁlter structures related to classical ﬁlter networks, Archiv fur Elektronik und Übertra-
gungstechnik 5 (2) (1971) 79–89.
[26] A. Fettweis, On the connection between multiplier word length limitation and roundoff noise in digital ﬁlters,
IEEE Trans. Circuit Theory CT-19 (5) (1972) pp. 486–491.
[27] A. Fettweis, Wave digital ﬁlters: theory and practice, Proc. IEEE 2 (2) (1986) 270–327.
[28] D.F. Elliott (Ed.), Handbook of Digital Signal Processing, Engineering Applications, Academic Press, 1988.
[29] J.G. Proakis, D.G. Manolakis, Digital Signal Processing, Principles, Algorithms, and Applications, third Ed.,
Prentice Hall, 1996.
[30] L.R. Rabiner, B. Gold, Theory and Application of Digital Signal Processing, Prentice Hall, Englewood Cliffs,
NJ, 1975.
[31] N.J. Fliege, Multirate Digital Signal Processing, John Wiley and Sons, New York.
[32] P.P. Vaidyanathan, Multirate Systems and Filter Banks, Prentice Hall, 1993.
[33] R.E. Crochiere, L.R. Rabiner, Multirate Digital Signal Processing, Prentice Hall, NJ, 1983.
[34] J.H. McClellan, T.W. Parks, L.R. Rabiner, A computer program for designing optimum FIR linear phase
digital ﬁlters, IEEE Trans. Audio Electroacoust. AU-21 (1973) 506–526.
[35] M. Ahsan, T. Saramäki, A MATLAB based optimum multibands FIR ﬁlters design program following the
original idea of the Remez multiple exchange algorithm, in: Proc. IEEE Intern. Symp. on Circuits and
Systems, ISCAS-11, Rio de Janeiro, Brazil, pp. 137–140.
[36] MATLAB, Signal Processing Toolbox User’s Guide, The MathWorks, 2012a.
[37] C.H. Chen (Ed.), The Circuits and Filters Handbook, second ed., CRC Press, New York, 2003.
[38] S.K. Mitra, J.F. Kaiser (Eds.), Handbook for Digital Signal Processing, John Wiley and Sons, 1993.
[39] T. Saramäki, M. Renfors, A novel approach for the design of IIR ﬁlters as a tapped cascaded interconnection
of identical allpass subﬁlters, in: Proc. IEEE Intern. Symp. on Circuits and Systems, ISCAS-87, vol. 2,
Philadelphia, May 4–7, 1987, pp. 629–632.
[40] A.V. Oppenheim, R.W. Schafer, Discrete-Time Signal Processing, Prentice Hall, 1989.
[41] K. Nakayama, Permuted difference coefﬁcient realization of FIR digital ﬁlters, IEEE Trans. Acoust., Speech,
Signal Process. ASSP-30 (2) (1982) 269–278.
[42] H. Ohlsson, O. Gustafsson, L. Wanhammar, A shifted permuted difference coefﬁcient method, in: Proc.
IEEE Int. Symp. Circuits Syst., vol. 3, Vancouver, Canada, May 23–26, 2004, pp. 161–164.
[43] A.S. Sedra, P.O. Brackett, Filter Theory and Design: Active and Passive, Pitman, London, 1978.
[44] H.J. Orchard, Inductorless ﬁlters, Electron. Lett. 2 (1966) 224–225.
[45] L. Wanhammar, A bound on the passband deviation for symmetric and antimetric commensurate transmission
line ﬁlters, 1991. <http://www.es.isy.liu.se/publications/>
[46] M.D. Lutovac, D.V. Tosic, B.L. Evans, Advanced ﬁlter design, in: Proc. 34th Asilomar Conf. on Signals,
Systems and Computers, Paciﬁc Grove, CA, November 2–5, 1997, pp. 710–715.
[47] G. Groenewold, Noise and group delay in active ﬁlters, IEEE Trans. Circuits and Syst. I CAS-I-54 (7) (2007)
1471–1480.
[48] A. Fettweis, Digital ﬁlter structures related to classical ﬁlter networks, Archiv fur Elektronik und Übertra-
gungstechnik 25 (2) (1971) 79–89.
[49] M. Yaseen, On the design of multiband transmission functions synthesized by one wave digital lattice
structure, Intern. J. Circ Theor App (2011).
[50] M. Yaseen, Robust simultaneous amplitude and phase approximations oriented to bandpass wave digital
lattice ﬁlters, Intern. J. Circ Theor App 26 (1998) 179–189.
[51] J.A. Nossek, M.T. Ivrla, On the relation of circuit theory and signals, systems and communications, in:
Proc. IEEE Intern. Symp. on Circuits and Systems, ISCAS-11, Rio de Janeiro, Brazil, May 15–18, 2011,
pp. 603–604.

References
335
[52] A. Fettweis, Some principles of designing digital ﬁlters imitating classical ﬁlter structures, IEEE Trans.
Circuit Theory CT-18 (2) (March 1971) 314–316.
[53] A. Fettweis and K. Meerkötter, On adaptors for wave digital ﬁlters, IEEE Trans. Acoust., Speech, Signal
Process. ASSP-23 (6) (1975) 516–525.
[54] A. Fettweis, Pseudopassivity, sensitivity, and stability of wave digital ﬁlters, IEEE Trans. Circuit Theory
CT-19 (6) (1972) pp. 668–673.
[55] A. Fettweis, K. Meerkötter, Suppression of parasitic oscillations in wave digital ﬁlters, in: Proc. IEEE Intern.
Symp. on Circuits and Systems, ISCAS-74, San Francisco, April 1974, pp. 682–686.
[56] A. Fettweis, K. Meerkötter, Suppression of parasitic oscillations in wave digital ﬁlters, IEEE Trans. Circuits
and Systems CAS-22 (3) (1975) 239–246.
[57] A. Fettweis, Passivity and losslessness in digital ﬁltering, Archiv fur Elektronik und Übertragungstechnik
42 (1) (1988) pp. 1–8.
[58] A. Fettweis, Scattering properties of wave digital ﬁlters, in: Proc. Florence Sem. on digital ﬁltering, Florence,
Italy, September 1972, pp. 1–8.
[59] A.H. Gray, J.D. Markel, Digital lattice and ladder ﬁlter synthesis, IEEE Trans. Audio Electroacoust. AU-21
(6) (1973) 491–500.
[60] A.H. Gray, J.D. Markel, A normalized digital ﬁlter structure, IEEE Trans. Acoust., Speech, Signal Process.
ASSP-23 (3) (1975) 268–277.
[61] A.H. Gray, J.D. Markel, Roundoff noise characteristics of a class of orthogonal polynomial structures, IEEE
Trans. Acoust., Speech, Signal Process. ASSP-23 (October 1975) 473–486.
[62] A.H. Gray, J.D. Markel, Fixed-point implementation algorithms for a class of orthogonal polynomial ﬁlter
structures, IEEE Trans. Acoust., Speech, Signal Process. ASSP-23 (5) (1975) 486–494.
[63] A.N. Willson, H.J. Orchard, Insights into digital ﬁlters made as a sum of two allpass functions, IEEE Trans.
Circuits Syst. I CAS-I-42 (3) (1995) 129–137.
[64] V.I. Anzova, J. Yli-Kaakinen, T. Saramäki, An algorithm for the design of multiplierless IIR ﬁlters as
a parallel connection of two all-pass ﬁlters, in: Proc. IEEE Asia Paciﬁc Conf. on Circuits and Systems.
APCCAS, Singapore, December 2006, pp. 744–747.
[65] B. Jaworski, T. Saramäki, Linear phase IIR ﬁlters composed of two parallel allpass sections, in: IEEE Intern.
Symp. on Circuits and Systems, ISCAS-94, vol. 2, London, May 1994, pp. 537–540.
[66] Y.C. Lim, Frequency-response masking approach for the synthesis of sharp linear phase digital ﬁlters, IEEE
Trans. Circuits and Systems CAS-33 (4) (1986) 357–364.
[67] Y.C. Lim, Y. Lian, The optimum design of one- and two-dimensional FIR ﬁlters using the frequency response
masking technique, IEEE Trans. Circuits and Syst. II CAS-II-40 (1993) 88–95.
[68] J. Yli-Kaakinen, T. Saramäki, An efﬁcient algorithm for the optimization of FIR ﬁlters synthesized using
the multistage frequency-response, Circ. Syst. Signal. Process. 30 (1) (2011) 157–183.
[69] Y. Neuvo, D. Cheng-Yu, and S.K. Mitra, Interpolated ﬁnite impulse response ﬁlters, IEEE Trans. Acoust.,
Speech, Signal Process. ASSP-32 (3) (1984) pp. 563–570.
[70] Y.C. Lim, Y. Lian, Frequency-response masking approach for digital ﬁlter design: complexity reduction via
masking ﬁlter factorization, IEEE Trans. Circuits and Syst. II CAS-II-41 (1994) 518–525.
[71] O. Gustafsson, H. Johansson, L. Wanhammar, Single ﬁlter frequency-response masking FIR ﬁlters, J. Circ.
Sys. Comp. 12 (5) (2003) pp. 601–630.
[72] Y. Lian, C.Z. Yang, Complexity reduction by decoupling the masking ﬁlters from the bandedge shaping ﬁlter
in the FRM technique, Circ. Syst. Signal. Process. 22 (2) (2003) 115–135.
[73] T. Saramäki, Y.C. Lim, R. Yang, The synthesis of half-band ﬁlter using frequency-response masking tech-
nique, IEEE Trans. Circuits Syst. II CAS-II-42 (1) (1995) 58–60.
[74] Y.C. Lim, Y.J. Yu, Synthesis of very sharp Hilbert transformer using the frequency-response masking tech-
nique, SP-53 (7) (2005) 2595–2597.

336
CHAPTER 6 Digital Filter Structures and Their Implementation
[75] H. Johansson, Two classes of frequency-response masking linear-phase FIR ﬁlters for interpolation and
decimation, Circuits, Syst. and Signal Processing, Special issue on Computationally Efﬁcient Digital Filters:
Design and Applications 25 (2) (2006) 175–200.
[76] H. Johansson, L. Wanhammar, Filter structures composed of allpass and FIR ﬁlters for interpolation and
decimation by a factor of two, IEEE Trans. Circuits Syst. II CAS-II-46 (7) (1999) 896–905.
[77] H. Johansson, P. Löwenborg, On the design of adjustable fractional delay FIR ﬁlters, IEEE Trans. Circuits
Syst. II 50 (4) (2003) 164–169.
[78] L. Rosenbaum, P. Löwenborg, H. Johansson, An approach for synthesis of modulated M-channel ﬁlter banks
utilizing the frequency-response masking technique, EURASIP J. Advances Signal Processing - Special
Issue on Multirate Systems and Applications 2007 (1) (2007).
[79] R. Bregovic, Y.C. Lim, T. Saramäki, Frequency-response masking-based design of nearly perfect-
reconstruction two-channel FIR ﬁlterbanks with rational sampling factors, Trans. Circuits and Syst. I CAS-
I-55 (7) (2008) 2002–2012.
[80] K.M. Tsui, S.C. Chan, Y.C. Lim, Design of multi-plet perfect reconstruction ﬁlter banks using frequency-
response masking technique, IEEE Trans. Circuits and Syst. I CAS-I-55 (9) (2008) 2707–2715.
[81] Y.C. Lim, B. Farhang-Boroujeny, Fast ﬁlter bank (FFB), IEEE Trans. Circuits and Syst. II CAS-II-39 (5)
(1992) 316–318.
[82] P.S.R. Diniz, L.C.R. de Barcellos, S.L. Netto, Design of high-resolution cosine-modulated transmultiplexers
with sharp transition band, Trans. Signal Process. 5 (2004).
[83] H. Johansson, L. Wanhammar, High-speed recursive digital ﬁlters based on the frequency-response masking
approach, IEEE Trans. Circuits Syst. II CAS-II-47 (1) (2000) 48–61.
[84] Y.C. Lim, S.H. Lou, Frequency-response masking approach for the synthesis of sharp two-dimensional
diamond-shaped ﬁlters, IEEE Trans. Circuits and Syst. II CAS-II-45 (12) (1998) 1573–1584.
[85] M. Renfors, Y. Neuvo, The maximal sampling rate of digital ﬁlters under hardware speed constraints, IEEE
Trans. Circuits and Systems CAS-28 (3) (1981) 196–202.
[86] K. Palmkvist, M. Vesterbacka, L. Wanhammar, Arithmetic transformations for fast bit-serial VLSI imple-
mentations of recursive algorithms, in: Proc. Nordic Signal Processing Symp., NORSIG’96, Espoo, Finland,
September 24–27, 1996, pp. 391–394.
[87] H. Ohlsson, O. Gustafsson, L. Wanhammar, Arithmetic transformations for increased maximal sample rate of
bit-parallel bireciprocal lattice wave digital ﬁlters, in: Proc, IEEE Int. Symp. Circuits Syst, Sydney, Australia,
May 2001, pp. 6–9.
[88] M. Vesterbacka, K. Palmkvist, P. Sandberg, L. Wanhammar, Implementation of fast bit-serial lattice wave
digital ﬁlters, in: Proc. IEEE Intern. Symp. on Circuits and Systems, ISCAS ’94, vol. 2, London, May 30–June
1, 1994, pp. 113–116.
[89] M. Vesterbacka, K. Palmkvist, L. Wanhammar, On implementation of fast, bit-serial loops, in: Proc. IEEE
1996 Midwest Symp. on Circuits and Systems, vol. 1, Ames, Iowa, August 18–21, 1996, pp. 190–193.
[90] M. Vesterbacka, K. Palmkvist, L. Wanhammar, Maximally fast, bit-serial lattice wave digital ﬁlters, in: Proc.
IEEE DSP Workshop 96, Loen, Norway, September 2–4, 1996, pp. 207–210.
[91] O. Gustafsson, L. Wanhammar, Maximally fast scheduling of bit-serial lattice wave digital ﬁlters using three-
port adaptor allpass sections, in: Proc. IEEE Nordic Signal Processing Symp., Kolmäarden, Sweden, June
13–15, pp. 441–444.
[92] K. Johansson, O. Gustafsson, L. Wanhammar, Power estimation for ripple-carry adders with correlated
input data, in: Proc. IEEE Intern. Workshop on Power and Timing Modeling, Optimization and Simulation,
Santorini, Greece, September 15–17 2004.
[93] R. Zimmermann, Binary adder architectures for cell-based VLSI and their synthesis. Ph.D. Thesis, Swiss
Federal Institute of Technology (ETH), Zurich, Hartung-Gorre Verlag, 1998.

References
337
[94] O. Gustafsson, L. Wanhammar, Low-complexity constant multiplication using carry-save arithmetic for
high-speed digital ﬁlters, in: Int. Symp. Image, Signal Processing, Analysis, Istanbul, Turkey, 2007,
pp. 27–29.
[95] O. Gustafsson, A.G. Dempster, L. Wanhammar, Multiplier blocks using carry-save adders, in: Proc. IEEE
Intern. Symp. Circuits Systems, Vancouver, Canada, May 2004, pp. 23–26.
[96] V.G. Oklobdzija, D. Villeger, S.S. Liu, A method for speed optimized partial product reduction and generation
of fast parallel multipliers using an algorithmic approach, IEEE Trans. Computers C-45 (3) (1996).
[97] M.J. Irwin, R.M. Owens, Design issues in digit serial signal processors, in: Proc. IEEE Intern. Symp. on
Circuits and Systems, vol. 1, May 8–11, 1989, pp. 441–444.
[98] H. Lee, G.E. Sobelman, Digit-serial reconﬁgurable FPGA logic block architecture, in: Proc. IEEE Workshop
on Signal Processing Systems, SIPS 98, October 8–10, 1998, pp. 469–478.
[99] K.K. Parhi, A systematic approach for design of digit-serial signal processing architectures, IEEE Trans.
Circ. Sys. CAS-38, no. 4, pp. 358–375, April 1991.
[100] H. Lee, G.E. Sobelman, Digit-serial DSP library for optimized FPGA conﬁguration, in: Proc. IEEE Symp.
on FPGAs for Custom Computing Machines, April 15–17, 1998, pp. 322–323.
[101] H. Suzuki, Y.-N. Chang, K.K. Parhi, Performance tradeoffs in digit-serial DSP systems, in: Proc. Thirty-
Second Asilomar Conf., vol. 2, Paciﬁc Grove, CA, November 1–4, 1998, pp. 1225–1229.
[102] Y.-N. Chang, J.H. Satyanarayana, K.K. Parhi, Systematic design of high-speed and low-power digit-serial
multipliers, IEEE Trans. Circuits and Syst. II, CAS-II-45 (12) (1998) pp. 1585–1596.
[103] Y.-N.C.J.H., Satyanarayana, K.K. Parhi, Low-power digit-serial multipliers, in: Proc. IEEE Intern. Symp.
on Circuits and Systems, ISCAS ’97, vol. 3, June 9–12, 1997, pp. 2164–2167.
[104] P. Nilsson, Arithmetic and architectural design to reduce leakage in nano-scale digital circuits, in: Proc. 18th
European Conference on Circuit Design, ECCTD 07, Seville, Spain, 2007, pp. 373–375.
[105] A.N. Willson, H.J. Orchard, A design method for half-band FIR ﬁlters, IEEE Trans. Circuits Syst. I CAS-I-45
(1) (1999) pp. 95–101.
[106] K. Johansson, O. Gustafsson, L. Wanhammar, Low-complexity bit-serial constant-coefﬁcient multipliers, in:
Proc. IEEE Int. Symp. Circuits Syst., vol. 3, Vancouver, Canada, May 23–26, 2004, pp. 649–652.
[107] A. Croisier, D.J. Esteban, M.E. Levilion, V. Rizo, Digital Filter for PCM Encoded Signals. U.S. Patent
3777130, December 4, 1973.
[108] A.G. Dempster, M.D. Macleod, O. Gustafsson, Comparison of graphical and sub-expression elimination
methods for design of efﬁcient multipliers, in: Proc. Asilomar Conf. Signals Syst. Comp., Monterey, CA,
November 7–10, 2004.
[109] M. Potkonjak, M.B. Srivastava, A.P. Chandrakasan, Multiple constant multiplications: efﬁcient and versatile
framework and algorithms for exploring common subexpression elimination, IEEE Trans. Computer-Aided
Design CAD-15 (2) (1996) 151–165.
[110] O. Gustafsson, A difference based adder graph heuristic for multiple constant multiplication problems, in:
Proc. IEEE Int. Symp. Circuits Syst., New Orleans, LA, May 27–30, 2007, pp. 1097–1100.
[111] Y. Voronenko and M. Püschel, Multiplierless multiple constant multiplication, ACM Trans. Algorithms 3
(2) (2007).
[112] Y.J. Yu, Y.C. Lim, Design of linear phase FIR ﬁlters in subexpression space using mixed integer linear
programming, IEEE Trans. Circuits Syst. I 54 (10) (2007) 2330–2338.
[113] D. Shi, Y.J. Yu, Design of linear phase FIR ﬁlters with high probability of achieving minimum number of
adders, IEEE Trans. Circuits Syst. I 58 (1) (2011) 126–136.
[114] K. Johansson, O. Gustafsson, L. Wanhammar, Multiple constant multiplication for digit-serial implementa-
tion of low power FIR ﬁlters, WSEAS Trans. Circuits Syst. 5 (7) (2006) 1001–1008.

338
CHAPTER 6 Digital Filter Structures and Their Implementation
[115] K. Johansson, O. Gustafsson, L.S. DeBrunner, L. Wanhammar, Minimum adder depth multiple constant
multiplication algorithm for low power FIR ﬁlters, in Proc. IEEE Int. Symp. Circuits Syst, Rio de Janeiro,
Brazil, May, 2011, pp. 15–18.
[116] M. Büttner, H.W. Schüssler, On structures for the implementation of the distributed arithmetic, Nachricht-
entechn. Z. 29 (6) (1976) 472–477.
[117] A. Peled, B. Liu, Digital Signal Processing: Theory, Design and Implementation, John Wiley and Sons, 1976.
[118] L. Wanhammar, Implementation of wave digital ﬁlters using vector-multipliers, in: Proc. First European
Signal Processing Conf., EUSIPCO-80, Lausanne, Switzerland, September 1980, pp. 21–26.
[119] S. White, On applications of distributed arithmetic to digital signal processing, a tutorial review, IEEE ASSP
Magazine, pp. 4–19, July 1989.

7
CHAPTER
Multirate Signal Processing
for Software Radio Architectures
Fred Harris, Elettra Venosa, and Xiaofei Chen
San Diego State University, Department of Electrical and Computer Engineering,
San Diego, CA, USA
After the introductory sections on resampling theory and basics of digital ﬁlters, the architectures of up
and down converter channelizers are presented. Those paragraphs are followed by the main core of this
chapter in which the authors present novel digital up converter (DUC) and digital down converter (DDC)
architectures for software deﬁned radios (SDRs) which are based on variations of standard polyphase
channelizers.
The proposed DUC is able to simultaneously up convert multiple signals, having arbitrary band-
widths, to arbitrarily located center frequencies. On the other side of the communication chain, the
proposed DDC is able to simultaneously down convert multiple received signals with arbitrary band-
width and arbitrarily located in the frequency domain. Both the proposed structures avoid the digital
data section replication, which is necessary in the current digital transmitters and receivers, when mul-
tiple signals have to be handled. Due to the inverse discrete Fourier transform (IDFT), performed by
using inverse fast Fourier transform (IFFT) algorithm, embedded in the channelizers, the proposed
architectures are very efﬁcient in terms of total workload.
This chapter is structured into two main parts which are composed, in total, of 10 sections. Its
structure is described in Figure 7.1. The ﬁrst part provides preliminary concepts on the sampling process,
resampling process of a digital signal, digital ﬁlters, multirate structures, standard M-path polyphase
up and down converter channelizers as well as their modiﬁed versions that represent the core of the
structures we present in the second part of this document. These are well known topics in the digital
signal processing (DSP) area and they represent the necessary background for people who want to
start learning about software radios. The second part of this chapter goes through new frontiers of the
research presenting the novel transmitting and receiving designs for software radios and demonstrating,
while explaining the way in which they work, the reasons that make them the perfect candidates for the
upcoming cognitive radios.
1.07.1 Introduction
Signal processing is the art of representing, manipulating, and transforming wave shapes and the infor-
mation content that they carry by means of hardware and/or software devices. Until 1960s almost
entirely continuous time, analog technology was used for performing signal processing. The evolution
of digital systems along with the development of important algorithms such as the well known fast
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00007-3
© 2014 Elsevier Ltd. All rights reserved.
339

340
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
FIGURE 7.1
Structure of the chapter.
Fourier transform (FFT), by Cooley and Tukey in 1965, caused a major shift to digital technologies
giving rise to new digital signal processing architectures and techniques. The key difference between
analog processing techniques and digital processing techniques is that while the ﬁrst one processes
analog signals, which are continuous functions of time; the second one processes sequences of values.
The sequences of values, called digital signals, are series of quantized samples (discrete time signal
values) of analog signals.
The link between the analog signals and their sampled versions is provided by the sampling pro-
cess. When sampling is properly applied it is possible to reconstruct the continuous time signal from

1.07.1 Introduction
341
its samples preserving its information content. Thus the selection of the sampling rate is crucial: an
insufﬁcient sampling frequency causes, surely, irrecoverable loss of information, while a more than
necessary sampling rate causes useless overload on the digital systems.
The string of words multirate digital signal processing indicates the operation of changing the sample
rate of digital signals, one or multiple times, while processing them. The sampling rate changes can
occur at a single or multiple locations in the processing architecture. When the multirate processing
applied to the digital signal is a ﬁltering then the digital ﬁlter is named multirate ﬁlter; a multirate ﬁlter
is a digital ﬁlter that operates with one or more sample rate changes embedded in the signal processing
architecture. The opportunity of selecting the most appropriate signal sampling rate at different stages of
the processing architecture, rather than having a single (higher) sampling rate, enhances the performance
of the system while reducing its implementation costs. However, the analysis and design of multirate
systems could result complicated because of the fact that they are time-varying systems. It is interesting
to know that the ﬁrst multirate ﬁlters and systems were developed in the context of control systems. The
pioneer papers on this topic were published in the second half of 1950s [1–3]. Soon the idea spilled in
the areas of speech, audio, image processing [4,5], and communication systems [6]. Today, multirate
structures seem to be the optimum candidates for cognitive and software deﬁned radio [7–11] which
represents the frontier of innovation for the upcoming communication systems.
One of the well known techniques that ﬁnd its most efﬁcient application when embedded in multirate
structures is the polyphase decomposition of a prototype ﬁlter. The polyphase networks, of generic order
M, originated in the late 1970s from the works by Bellanger et al. [12,13]. The term polyphase is the
aggregation of two words: poly, that derives from the ancient greek word polys, which means many,
and phase. When applied to an M-path partitioned ﬁlter these two words underline the fact that, on each
path, each aliased ﬁlter spectral component, experiences a unique phase rotation due to both their center
frequencies and the time delays, which are different in each path because of the way in which the ﬁlter
has been partitioned. When all the paths are summed together the undesired spectral components, having
phaseswithoppositepolarity,canceleachotherwhileonlythedesiredcomponents,whichexperiencethe
samephaseonallthearmsofthepartitions,constructivelyaddup.Byapplyingappropriatephaserotators
to each path we can arbitrarily change the phases of the spectral components selecting the spectral com-
ponent that survives as a consequence of the summation. It is interesting to know that the ﬁrst applications
of the polyphase networks were in the areas of real-time implementation of decimation and interpolation
ﬁlters, fractional sampling rate changing devices, uniform DFT ﬁlter banks as well as perfect recon-
struction analysis/synthesis systems. Today polyphase ﬁlter banks, embedded in multirate structures,
are used in many modern DSP applications as well as in communication systems where they represent,
according to the author’s belief, one of the most efﬁcient options for designing software-based radio.
The processing architecture that represents the starting point for the derivation of the standard
polyphase down converter channelizer is the single channel digital down converter. In a digital radio
receiver this engine performs the operations of ﬁltering, frequency translation and resampling on the
intermediate frequency (IF) signals. The resampling is a down sampling operation for making the signal
sampling rate commensurate to its new reduced bandwidth. When the output sampling rate is selected
to be an integer multiple of the signal’s center frequency, the signal spectrum is shifted, by aliasing,
to base-band and the complex heterodyne defaults to unity value disappearing from the processing
path. The two remaining operations of ﬁltering and down sampling are usually performed in a single
processing architecture (multirate ﬁlter). After exchanging the positions of the ﬁlter and resampler, by

342
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
applying polyphase decomposition, we achieve the standard M-path polyphase down converter chan-
nelizer which shifts a single narrowband channel to base-band while reducing its sampling rate. By
following a similar procedure the standard up converter channelizer can also be obtained. It performs
the operation of up converting while interpolating a single narrowband channel.
When the polyphase channelizer is used in this fashion the complex phase rotators can be applied to
each arm to select, by coherent summation, a desired channel arbitrarily located in the frequency domain.
Moreover, the most interesting and efﬁcient application of this engine is in the multichannel scenario.
When the inverse discrete Fourier transform block is embedded, the polyphase channelizer acquires the
capability to simultaneously up and down convert, by aliasing, multiple, equally spaced, narrowband
channels having equal bandwidths. Due to its computational efﬁciency, the polyphase channelizer, and
its modiﬁed versions, represents the best candidate for building up new ﬂexible multichannel digital
radio architectures [14] like software deﬁned radio promises to be.
Software deﬁned radio represents one of the most important emerging technologies for the future
of wireless communication services. By moving radio functionality into software, it promises to give
ﬂexible radio systems that are multi-service, multi-standard, multi-band, reconﬁgurable, and repro-
grammable by software. The goal of software deﬁned radio is to solve the issue of optimizing the use
of the radio spectrum that is becoming more and more pressing because of the growing deployment of
new wireless devices and applications [15–18].
In the second part of this chapter we address the issue of designing software radio architectures for
both the transmitter and the receiver. The novel structures are based on modiﬁed versions of the standard
polyphase channelizer, which provides the system with the capability to optimally adapt its operating
parameters according to the surrounding radio environment [7,19–21]. This implies, on the transmitter
side, the capability of the radio to detect the available spectral holes [22,23] in the spanned frequency
range and to dynamically use them for sending signals having different bandwidths at randomly located
center frequencies. The signals could be originated from different information sources or they could
be spectral partitions of one signal, fragmented because of the unavailability of free space in the radio
spectrum or for protecting it from unknown and undesired detections.
The core of the proposed transmitter is a synthesis channelizer [24]. It is a variant of the standard
M-path polyphase up converter channelizer [6,8] that is able to perform 2-to-M up sampling while
shifting, by aliasing, all the base-band channels to the desired center frequencies. Input signals with
bandwidths wider than the synthesis channelizer bandwidth are pre-processed through small down
converter channelizers that disassemble their bandwidths into reduced bandwidth sub-channels. The
proposed transmitter avoids the need to replicate the sampled data section when multiple signals have
to be simultaneously transmitted and it also allows partitioning of the signal spectra, when necessary,
before transmitting them. Those spectral fragments will be reassembled in the receiver after being
shifted to base-band without any loss of energy because perfect reconstruction ﬁlters are used as low-
pass prototype in the channelizer polyphase decompositions.
On the other side of the communication chain, a cognitive receiver has to be able to simultaneously
detect multiple signals, recognize, when necessary, all their spectral partitions, ﬁlter, down convert and
recompose them without energy losses [7] independently of their bandwidths or center frequencies. An
analysis channelizer is the key element of the proposed receiver [7]. This engine is able to perform M-to-
2 down sampling while simultaneously demodulating, by aliasing, all the received signal spectra having

1.07.2 The Sampling Process and the “Resampling” Process
343
arbitrary bandwidths residing at arbitrary center frequencies [9]. Post-processing up converter channel-
izers are used for reassembling, from the analysis channelizer base-line channels, signal bandwidths
wider than the analysis channelizer channel bandwidth.
In the transmitter complex frequency rotators apply the appropriate frequency offsets for arbitrary
center frequency positioning of the spectra. When the frequency rotators are placed in the proposed
receiver, they are responsible for perfect DC alignment of the down converted signals.
This chapter is composed of 10 main sections. The ﬁrst four sections are dedicated to the basics
of digital signal processing, multirate signal processing, and polyphase channelizers. These sections
contain preliminary concepts necessary for completely understanding the last sections of this work in
which actual research issues on software radio (SR) architecture design are discussed and innovative
results are presented. In particular, Section 1.07.2 recalls basic concepts on the resampling process
of a discrete time signal as opposed to the sampling process of an analog, continuous time, signal.
Section 1.07.3 introduces the readers to digital ﬁlters providing preliminaries on their design techniques.
Section 1.07.5 presents the standard single path architectures for down sampling and up sampling digital
signals. Section 1.07.6 introduces the standard version of the M-path polyphase down and up converter
channelizers. Both of them are derived, step by step, by the single path structures that represent the
current state of the technology. In Section 1.07.7 we present the modiﬁed versions of these engines.
These are the structures that give form to the proposed digital down and up converters for SDRs which
are presented in Section 1.07.9 which also provides the simulation results. Section 1.07.8 introduces
the readers to the concept of software radio while Section 1.07.10 gives concluding remarks along with
suggestions for future research works in this same area.
1.07.2 The Sampling process and the “Resampling” process
In the previous section we mentioned that the sampling process is the link between the continuous time
world and the discrete time world. By sampling a continuous time signal we achieve its discrete time
representation. When sampling frequency is properly selected the sampling process becomes invertible
and it is possible to re-shift from the discrete time representation to the continuous time representation
of a signal maintaining its information content during the process.
It is well known that, when the signal is bandlimited and has no frequency components above fMAX,
it can be uniquely described by its samples taken uniformly at frequency
fs ≥2 fMAX.
(7.1)
Equation (7.1) is known as Nyquist sampling criterion (or uniform sampling theorem) and the sampling
frequency fs = 2 fMAX is called the Nyquist sampling rate. This theorem represents a theoretically
sufﬁcient condition for reconstructing the analog signal from its uniformly spaced samples.
Even though sampling is practically implemented in a different way, it is convenient, in order to
facilitate the learning process, to represent it as a product of the analog waveform, x(t), with a periodic
train of unit impulse functions deﬁned as
xδ(t) =
∞

n=−∞
δ(t −nT s),

344
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
FIGURE 7.2
Sampling process as a product of the analog waveform, x(t), with a periodic train of unit impulse func-
tions xδ(t).
where Ts = 1/ fs is the sampling period. By using the shifting property of the impulse function we
obtain
xs(t) = x(t)xδ(t) = x(t)
∞

n=−∞
δ(t −nT s) =
∞

n=−∞
x(nT s)δ(t −nT s).
(7.2)
A pictorial description of Eq. (7.2) is given in Figure 7.2.
For facilitating the readers’ understanding of the uniform sampling effects on the bandlimited con-
tinuous time signal, x(t), we shift from time domain to the frequency domain where we are allowed to
use the properties of the Fourier transform. The product of two functions in time domain becomes the
convolution of their Fourier transforms in the frequency domain. Thus, if X( f ) is the Fourier transform
of x(t) and Xδ( f ) is the Fourier transform of xδ(t), then the Fourier transform of xs(t) is
Xs( f ) = X( f ) ∗Xδ( f ),
where ∗indicates linear convolution and
Xδ( f ) = 1
Ts
∞

k=−∞
δ( f −k f s).
Note that the Fourier transform of a impulse train is another impulse train with the values of the periods
reciprocally related to each other. Then in the frequency domain Eq. (7.2) becomes
Xs( f ) = X( f ) ∗Xδ( f ) = X( f ) ∗1
Ts
∞

k=−∞
δ( f −k f s) = 1
Ts
∞

k=−∞
X( f −k f s),
(7.3)
whose pictorial view is shown in Figure 7.3.

1.07.2 The Sampling Process and the “Resampling” Process
345
FIGURE 7.3
Sampling process in the frequency domain.
From Eq. (7.3) we conclude that the spectrum of the sampled signal, in the original signal bandwidth
([−fs/2, fs/2]), is the same as the continuous time one (except for a scale factor, 1
Ts ) however, as a
consequence of the sampling process, this spectrum periodically repeats itself with a period of fs = 1
Ts .
We can easily recognize that it should be possible to recover the original spectrum (associated to the
spectral replica which resides in [−fs/2, fs/2]), by ﬁltering the periodically sampled signal spectrum
with an appropriate low-pass ﬁlter.
Notice, from Eq. (7.3) that the spacing, fs, between the signal replicas is the reciprocal of the sampling
period Ts. A small sampling period corresponds to a large space between the spectral replicas.
A fundamental observation, regarding the selection of the uniform sampling frequency, needs to be
done at this point: when the sampling rate is selected to be less than the maximum frequency component
of the bandlimited signal ( fs < 2 fMAX) the periodic spectral replicas overlap each other. The amount
of the overlap depends of the selected sampling frequency. Smaller the sampling frequency, larger the
amount of overlap experienced by the replicas. This phenomenon is well known as aliasing. When
aliasing occurs it is impossible to recover the analog signal from its samples. When fs = 2 fMAX
the spectral replicas touch each other without overlapping and it is theoretically (but not practically)
possible to recover the analog signal from its samples; however a ﬁlter with inﬁnite number of taps
would be required. As a matter of practical consideration, we need to specify here that the signals (and
ﬁlters) of interest are never perfectly bandlimited and some amount of aliasing always occurs as effect
of sampling however some techniques can be used to limit the phenomena making it less harmful.
Afterthesamplinghasbeenappliedtheamplitudeofeachsampleisonefromaninﬁnitesetofpossible
values. This is the reason for which the samples are not compatible with a digital system yet. A digital

346
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
system can, in fact, only deal with a ﬁnite number of values. The digital samples need to be quantized
before being sent to the digital data system. The quantization process limits the amplitude of the samples
to a ﬁnite set of values. After the quantization process the signal can still be recovered, however some
additional imprecision is added to it. The amount of imprecision depends of the quantization levels used
in the process and it has the same effect on the signal as white noise. This is the reason for which it is
referred to as quantization noise. The sampling of a continuous time signal and the quantization of its
discrete time samples are both performed with devices called analog-to-digital converters (ADCs).
The selection of the appropriate sampling rate is a fundamental issue faced when designing digital
communication systems. In order to preserve the signal, the Nyquist criterion must be always satisﬁed.
Also, it is not difﬁcult to ﬁnd tasks for which, at some points in the digital data section of the transmitter
and the receiver, it is recommended to have more than two samples per signal bandwidth. On the other
hand, large sample rates cause an increase in the total workload of the system. Thus the appropriate
sampling rate must be selected according to both the necessities: to have the required number of samples
and to minimize the total workload. Most likely the optimum sampling frequency changes at different
points in the digital architecture. It would be a huge advantage to have the option of changing the signal
sampling rate at different parts in the systems while processing it, thus optimizing the number of samples
as a function of the requirements. The process of changing the sampling rate of a digital signal is referred
to as resampling process. The resampling process is the key concept in multirate signal processing.
FIGURE 7.4
Resampling by zeroing sample values and by inserting zero valued samples.

1.07.2 The Sampling Process and the “Resampling” Process
347
After the brief previous discussion about the sampling process of a continuous time signal, we address
now the process of resampling an already sampled signal. Notice that when a continuous time signal is
sampled there are no restrictions on the sample rate or the phase of the sample clock relative to the time
base of the continuous time signal. On the other hand, when we resample an already sampled signal, the
output sample locations are intimately related to the input sample positions. A resampled time series
contains samples of the original input time series separated by a set of zero valued samples. The zero
valued time samples can be the result of setting a subset of input sample values to zero or the result
of inserting zeros between existing input sample values. Both options are shown in Figure 7.4. In the
ﬁrst example shown in this ﬁgure, the input sequence is resampled 2-to-l, keeping every second input
sample starting at sample index 0 while zeroing the interim samples. In the second example, the input
sequence is resampled 1-to-2, keeping every input sample but inserting a zero valued sample between
each input samples. These two processes are called down sampling and up sampling respectively.
The non-zero valued samples of two sequences having the same sample rate can occur at different
time indices (i.e., the same sequence can have different initial time offset) as in Figure 7.5 in which the
two 2-to-1 down sampled sequences, s2(n) and s2(n −1), have different starting time index, 0 and 1,
which gives equal magnitude spectral components with different phase proﬁles. This is explicitly shown
in Figures 7.6 and 7.7, in which the time domain and the frequency domain views of the resampled
sequences s5(n) and s5(n−1) are depicted respectively. Remember, in fact, the time shifting property of
FIGURE 7.5
Two examples of 2-to-1 down sampling of a time series with different time offsets.

348
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
FIGURE 7.6
Sampling sequence s5(n) in time and frequency domain.
FIGURE 7.7
Sampling sequence s5(n −1) in time and frequency domain.

1.07.3 Digital Filters
349
the discrete Fourier transform (DFT) for which a shift in time domain is equivalent to a linear phase shift
in the frequency domain. The different phase proﬁles play a central role in multirate signal processing.
The M zero valued samples inserted between the signal samples create M periodic replicas of the
original spectrum at the frequency locations k/M, with k = 0, . . . , M−1 in the normalized domain.
This observation suggests that resampling can be used to affect translation of spectral bands, up and
down conversion, without the use of sample data heterodynes.
In the following paragraphs we clarify this concept showing how to embed the spectral translation
of narrowband signals in resampling ﬁlters and describe the process as aliasing.
A ﬁnal comment about the resampling process is that it can be applied to a time series or to the
impulse response of a ﬁlter, which, of course, is simply another time series. When the resampling
process is applied to a ﬁlter, the architecture of the ﬁlter changes considerably and the ﬁlter is called
multirate ﬁlter.
1.07.3 Digital ﬁlters
Filtering is the practice of processing signals which results in some changes of their spectral contents.
Usually the change implies a reduction or ﬁltering out some undesired input spectral components.
A ﬁlter allows certain frequencies to pass while attenuating others. While analog ﬁlters operate on
continuous-time signals, digital ﬁlters operate on sequences of discrete sampled value (see Figure 7.8)
although digital ﬁlters perform many of the same functions as analog ﬁlters, they are different!
The two classes of ﬁlters share many of the same or analogous properties. In the standard order of
our educational process we ﬁrst learn about analog ﬁlters and later learn about digital ﬁlters. To ease the
entry into the digital domain we emphasize the similarities of the two classes of ﬁlters. This order is due
to fact that an analog ﬁlter is less abstract than a digital ﬁlter. We can touch the components of an analog
ﬁlter; capacitors, inductors, resistors, operational ampliﬁers and wires. On the other hand a digital ﬁlter
doesn’t have the same physical form because a digital ﬁlter is merely a set of instructions that perform
arithmetic operations on an array of numbers. The operations can be weighted sums or inner products
(see Figure 7.9). It is convenient to visualize the array as a list of uniformly spaced sample values of an
analog waveform. The instructions to perform these operations can reside as software in a computer or
microprocessor or as ﬁrmware in a dedicated collection of interconnected hardware elements.
Let us examine the similarities that are emphasized when we learn about digital ﬁlters with some
level of prior familiarity with analog ﬁlters and the tools to describe them. Many analog ﬁlters are
formed by interconnections of lumped linear components modeled as ideal capacitors, inductors, and
resistors while others are formed by interconnections of resistors, capacitors, and operational ampli-
ﬁers. Sampled data ﬁlters are formed by interconnections of registers, adders, and multipliers. Most
analog ﬁlters are designed to satisfy relationships deﬁned by linear time invariant differential equations.
The differential equations are recursive which means the initial condition time domain response, called
the homogeneous or undriven response, is a weighted sum of exponentially decaying sinusoids. Most
sampled data ﬁlters are designed to satisfy relationships deﬁned by linear time invariant difference
equations. Here we ﬁnd the ﬁrst major distinction between the analog and digital ﬁlters: the difference
equations for the sampled data ﬁlters can be recursive or non recursive. When the difference equations
are selected to be recursive the initial condition response is, as it was for the analog ﬁlter, samples of a

350
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
Digital Filter
Analog Filter
f
H(f)
H(f)
h(n)
h(t)
H(f)
H(f)
f
x(n)
y(t)
y(n)
y(t)
Similar
Different
FIGURE 7.8
Digital and analog ﬁlters are different even though they perform similar tasks.
Digital Filter
Analog Filter
C1
L2
L4
C3
C5
R6
IIN
VOUT
z-1
z-1
z-1
x(n)
a
a
a
b
b
b
b
b
1,1
2,1
2,2
1,1
2,1
2,2
1,0
2,0
-
-
z-1
z-1
a
a
b
b
b
3,1
3,2
3,1
3,2
3,0
-
y(n)
FIGURE 7.9
Digital ﬁlter: registers, adders, and multipliers. Analog ﬁlter, capacitor, inductors, and resistors.
weighted sum of exponentially decaying sinusoids. On the other hand, when the difference equation is
non recursive the initial condition response is anything the designer wants it to be and is limited only
by her imagination but is usually designed to satisfy some frequency domain speciﬁcations.
When we compare the two ﬁlter classes, analog and digital, we call attention to the similarities of the
tools we use to describe, analyze, and design them. They are described by differential and difference
equations which are weighted sums of signal derivates or weighted sums of delayed sequences. They
both have linear operators or transforms, the Laplace transform L{h(t)} = H(s), and the z transform
z{h(n)} = H(z), that offer us insight into the internal structure of the differential or difference equations
(see Figure 7.10). They both have operator descriptions that perform the equivalent functions. These are
the integral operator, denoted by s−1, and the delay operator, denoted by z−1, in which reside the system
memories or state of the analog and digital ﬁlters respectively. Both systems have transfer functions
H(s) and H(z), ratios of polynomials in the operator variable s and z. The roots of the denominator
polynomial, the operator version of the characteristic equation, are the ﬁlter poles. These poles describe
the ﬁlter modes, the exponentially damped sinusoids, of the recursive structure. The roots of numerator
polynomial are the ﬁlter zeros. The zeros describe how the ﬁlter internal mode responses are connected
to the ﬁlter input and output ports. They tell us the amplitude of each response component, (called
residues to impress us) in the output signal’s initial condition response. The transforms of the two ﬁlter

1.07.3 Digital Filters
351
0
0
3
2
1
3
2
1
1
2
3
1
2
3
0
0
3
2
1
3
2
1
1
2
3
1
2
3
1
2
3
1
2
3
0
0
1
2
3
1
2
3
( )
( )
( )
( )
( )
( )
(
)(
)(
)
(
)(
)(
)
( )
( )
(
)(
)(
)
(
)(
)(
)
( )
n
st
n
H z
h n z
H s
h t e
dt
z
b z
b z
b
s
b s
b s
b
H z
b
H s
b
z
a z
a z
a
s
a s
a s
a
z
z
z
z
z
z
s
z
s
z
s
z
H z
b
H s
b
z
p
z
p
z
p
s
p
s
p
s
p
H z
∞
∞
−
−
=
=
+
+
+
=
=
+
+
+
+
+
+
+
+
+
−
−
−
=
=
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
=
Σ
∫
3
1
2
1
2
2
0
1
2
2
0
1
2
2
1
2
2
1
2
2
0
1
1
2
2
3
3
0
1
2
3
( )
( )
( )
( )
p t
p t
p t
n
n
n
p
p
p
z                  z                 z
c
c
c
c
H s
c
c
c
c
z
p
z
p
z
p
s
p
s
p
s
p
h n
c
c p
c p
c p
h t
c
t
c e
c e
c e
δ
−
−
−
=
+
+
+
+
+
+
=
=
=
FIGURE 7.10
Similarity of system equations for digital and analog ﬁlters.
classes have companion transforms the Fourier transform H(ω) and the sampled data Fourier series
H(θ) which describe, when they exist, the frequency domain behavior, sinusoidal steady state gain
and phase, of the ﬁlters. It is remarkable how many similarities there are in the structure, the tools,
and the time and frequency responses of the two types of ﬁlters. It is no wonder we emphasize their
similarities.
We spend a great deal of effort examining recursive digital ﬁlters because of the close relationships
with their analog counterparts. There are digital ﬁlters that are mappings of the traditional analog ﬁlters.
These include maximally ﬂat Butterworth ﬁlters, equal ripple pass-band Tchebychev-I ﬁlters, equal
ripple stop-band Tchebychev-II ﬁlters, and both equal ripple pass-band and equal ripple stop-band
Elliptic ﬁlters. The spectra of these inﬁnite impulse response (IIR) ﬁlters are shown in Figure 7.11. All
DSP ﬁlter design programs offer the DSP equivalents of their analog recursive ﬁlter counterparts.
In retrospect, emphasizing similarities between the analog ﬁlter and the equivalent digital was a poor
decision. We make this claim for a number of reasons. The ﬁrst is there are limited counterparts in the
analog domain to the non-recursive ﬁlter of the digital domain. This is because the primary element of the
non recursive ﬁlter is the pure delay, represented by the delay operator z−1. We cannot form a pure delay
response, represented by the analog delay operator e−sT, in the analog domain as the solution of a linear
differentialequation.Toobtainpuredelay,werequireapartialdifferentialequationwhichoffersthewave
equation whose solution is a propagating wave which we use to convert distance to time delay. The prop-
agation velocity of electromagnetic waves is too high to be of practical use in most analog ﬁlter designs
but the velocity of sound waves in crystal structures enables an important class of analog ﬁlters known as
acoustic surface wave (SAW) devices. Your cell phone contains one or more such ﬁlters. In general, most
of us have limited familiarity with non-recursive analog ﬁlters so our ﬁrst introduction to the properties
and design techniques for ﬁnite duration impulse response (FIR) ﬁlters occurs in our DSP classes.
The second reason for not emphasizing the similarities between analog and digital ﬁlters is that we
start believing the statement that “a digital ﬁlter is the same as an analog ﬁlter” and that all the properties
of one are embedded in the other. That seemed like an ok perspective the ﬁrst 10 or 20 times we heard
that claim. The problem is, it is not true! The digital ﬁlter has a resource the analog ﬁlter does not have!
The digital ﬁlter can perform tasks the analog ﬁlter cannot do! What is that resource? It is the sampling
clock! We can do applied magic in the sampled data domain by manipulating the sample clock. There

352
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
FIGURE 7.11
Frequency response of digital IIR ﬁlter realizations of standard analog ﬁlters.
is no equivalent magic available in the analog ﬁlter domain! Analog designers shake their heads in
disbelief when they see what a multirate ﬁlter can do. Actually digital designers also shake their heads
in disbelief when they learn that manipulating the clock can accomplish such amazing things.
We might ask how can the sampling clock affect the performance or more so, enhance the capabilities
of a digital ﬁlter? Great question! The answer my friend is written in the way deﬁne frequency of a
sampled data signal. Let us review how frequency as we understand it in the analog world is converted
to frequency in the digital world. We now present a concise review of complex and real sinusoids with
careful attention paid to their arguments.
The exponential function ex has the interesting property that it satisﬁes the differential equation
shown in Eq. (7.4). This means the exponential function replicates under the differential operator, an
important property inherited by real and complex sinusoids. It is easy to verify that the Taylor series
shown in Eq. (7.5) satisﬁes Eq. (7.4). When we replace the argument “x” of Eq. (7.5) with the argument
“ jθ” we obtain the series shown in Eq. (7.6) and when we gather the terms corresponding to the even
and odd powers respectively of the argument “ jθ” we obtain the partitioned series shown in Eq. (7.7).
We recognize that the two Taylor series of the partitioned Eq. (7.7) are the Taylor series of the cosine and
sine which we show explicitly in Eq. (7.8). We note that the argument of both the real exponential and of

1.07.3 Digital Filters
353
the complex exponential must be dimensionless otherwise we would not be able to sum the successive
powers of the series. We also note that the units of the argument “θ” are radians, a dimensionless unit
formed by the ratio of arc length on a circle normalized by the radius of the circle. When we replace the
“θ” argument of Eq. (7.8) with a time varying angle “θ(t)” we obtain the time function shown in Eq.
(7.9). The simplest time varying angle is one the changes linearly with time, θ(t) = ωct, which when
substituted in Eq. (7.9) we have Eq. (7.10). All of this discussion leads us to the next statement. Since
the argument of a sinusoid has the dimensionless units radians and the units of “t” has units of seconds,
then the units of ωc must be rad/s, a velocity. In other words, the frequency ωc is the time derivative of
the linearly increasing phase angle ωct
d
dx f (x) = f (x),
(7.4)
ex =
∞

k=0
xk
k! = 1 + x + x2
2 + x3
6 + x4
24 + x5
120 + · · · ,
(7.5)
e jθ =
∞

k=0
( jθ)k
k!
= 1 + jθ + ( jθ)2
2
+ ( jθ)3
6
+ ( jθ)4
24
+ · · · ,
(7.6)
e jθ =

1 −θ2
2 + θ4
24 −θ6
720 + · · ·

+ j

θ −θ3
6 + θ5
120 −
θ7
5040 + · · ·

,
(7.7)
e jθ = cos (θ) + j sin (θ),
(7.8)
s(t) = e jθ(t) = cos (θ(t)) + j sin (θ(t)),
(7.9)
s(t) = e jωct = cos (ωct) + j sin (ωct).
(7.10)
We now examine the phase argument of the sampled data sinusoid. The successive samples of a sampled
complex sinusoid are formed as shown in Eq. (7.11) by replacing the continuous argument t with the
sample time positions nT as was done in Eq. (7.12). We now note that while the argument of the sampled
data sinusoid is dimensionless, the independent variable is no longer “t” with units of seconds but rather
“n” with units of sample. This is consistent with the dimension of “T” which of course is s/sample.
Consequently the product ωcT has units of (rad/s) · (s/smpl) or rad/smpl, thus digital frequency is
rad/smpl or emphatically the angle change per sample! A more useful version of Eq. (7.12) is obtained by
replacing ωc with 2π fc as done in Eq. (7.13) and then replace T with 1/ fs as was done in Eq. (7.14). We
ﬁnally replace 2π fc/ fs with θc as was done in Eq. (7.15). Here we clearly see that the parameter θc has
units of rad/smpl, which described the fraction of the circle the argument traverses per successive sample
s(n) = s(t)|t=nT = cos (ωct)|t=nT + j sin (ωct)|t=nT ,
(7.11)
s(n) = cos (ωcTn) + j sin (ωcTn),
(7.12)
s(n) = cos (2π fcTn) + j sin (2π fcTn),
(7.13)
s(n) = cos

2π fc
fs
n

+ j sin

2π fc
fs
n

,
(7.14)
s(n) = cos (θcn) + j sin (θcn).
(7.15)

354
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
0
2
4
6
8
10
12
14
16
18
20
-1
0
1
Amplitude
0
2
4
6
8
10
12
14
16
18
20
-1
0
1
Amplitude
0
2
4
6
8
10
12
14
16
18
20
-1
0
1
Amplitude
0
2
4
6
8
10
12
14
16
18
20
-1
0
1
Time Index
Amplitude
-0.5
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
-100
-50
0
log Mag (dB)
-0.5
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
-100
-50
0
log Mag (dB)
-0.5
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
-100
-50
0
log Mag (dB)
-0.5
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
-100
-50
0
log Mag (dB)
Normalized Frequency
FIGURE 7.12
Time series and spectra of sinusoid aliased to new digital frequencies as a result of down-sampling.
Now suppose we have a sampled sinusoid with sample rate 8-times the sinusoid’s center frequency so
that there are 8-samples per cycle. This is equivalent to the sampled sinusoid having a digital frequency
of 2π/8 rad/smpl. We can visualize a spinning phasor rotating 1/8th of the way around the unit circle per
sample. Figure 7.12 subplots 1 and 2 show 21 samples of this sinusoid and its spectrum. Note the pair of
spectral lines located at ±0.125 on the normalized frequency axis. We can reduce the sample rate of this
time series by taking every other sample. The spinning rate for this new sequence is 2·2π/8 or 2π/4. The
down sampled sequence has doubled its digital frequency. The newly sampled sequence and its spectrum
are shown in Figure 7.12 subplots 3 and 4. Note the pair of spectral lines are now located at ±0.25 on the
normalized frequency axis. We can further reduce the sample rate of by again taking every other sample
(or every 4th sample of the original sequence). The spinning rate for this new sequence is 4 · 2π/8 or
2π/2. The down sampled sequence has again doubled its digital frequency. The newly sampled sequence
and its spectrum are shown in Figure 7.12 subplots 5 and 6. Note the pair of spectral lines are now located
at ±0.5 on the normalized frequency axis. We once again reduce the sample rate of by taking every other
sample (or every 8th sample of the original sequence). The spinning rate for this new sequence is 8·2π/8
or 2π, but since a 2π rotation is equivalent to no rotation, the spinning phasor appears to be stationary or
rotating at 0 rad/smpl. The newly sampled sequence and its spectrum are shown in Figure 7.12 subplots
7 and 8. Note the pair of spectral lines are now located at ±1 which is equivalent to 0 on the normalized
frequency axis. Frequency shifts due to sample rate changes are attributed to aliasing. Aliasing enables
us to move spectrum from one frequency location to another location without the need for the complex
mixers we normally use in a digital down converter to move a spectral span to base-band.

1.07.3 Digital Filters
355
ADC
  ANALOG 
ANTI-ALIAS
    FILTER
M:1
s(t)
d(t)
s(n)
h(n)
r(n)
r(nM)
s(n)
e
e-jθ0
LO
CLK
LOWPASS
FILTER
n
-jθ0n
FIGURE 7.13
Building blocks of conventional digital down converter.
0
0
0
0
fs/2
fs/2
fs/2
fs/M
-fs/2
-fs/2
-fs/2
-fs/M
f
f
f
f
CHANNEL OF INTEREST
TRANSLATED SPECTRUM
FILTERED SPECTRUM
SPECTRAL REPLICATES AT DOWN-SAMPLED RATE
OUTPUT DIGITAL FILTER RESPONSE
INPUT ANALOG FILTER RESPONSE
....
....
FIGURE 7.14
Spectra at consecutive points in conventional digital down converter.
Speaking of the standard digital down converter Figure 7.13 presents the primary signal ﬂow blocks
based on the DSP emulation of the conventional analog receiver. Figure 7.14 shows the spectra that will
be observed at successive output ports of the DDC. We can follow the signal transformations of the DDC
by following the spectra in the following description. The top ﬁgure shows the spectrum at the output
of the analog to digital converter. The second ﬁgure shows the spectrum at the output of the quadrature
mixer. Here we see the input spectrum has been shifted so that the selected channel band resides at zero

356
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
  ANALOG 
ANTI-ALIAS
    FILTER
ADC
M:1
d(t)
s(t)
h(n)
s(n)
r(nM)
r(n)
r(nM)
e
e
LO
CLK
BANDPASS
   FILTER
jθ0n
jθ0n
ejθ0Mn
e-jθ0Mn
FIGURE 7.15
Building blocks of aliasing digital down converter with M-to-1 down sampler following ﬁlter.
frequency. The third ﬁgure shows the output of the low-pass ﬁlter pair. Here we see that a signiﬁcant
fraction of the input bandwidth has been rejected by the stop band of the low pass ﬁlter. Finally, the last
ﬁgure shows the spectrum after the M-to-1 down sampling that reduced the sample rate in proportion
to the reduction in bandwidth performed by the ﬁlter. We will always reduce sample rate if we are
reducing bandwidth. We have no performance advantage for over satisfying the Nyquist criterion, but
incur signiﬁcant penalties by operating the DSP processors at higher than necessary sample rates.
Figure 7.15 presents the primary signal ﬂow blocks based on aliasing the selected band pass span to
base-band by reducing the output sample rate of the band pass ﬁlter. Figure 7.16 shows the spectra that
will be observed at successive output ports of the aliasing DDC. We can follow the signal transformations
of the aliasing DDC by following the spectra in the following description. The top ﬁgure shows the
spectrum at the output of the analog to digital converter. The second ﬁgure shows the spectrum of the
complex band pass ﬁlter and the output spectrum from this ﬁlter. The band pass ﬁlter is formed by
up-converting, by a complex heterodyne, the coefﬁcients of the prototype low pass ﬁlter. We note that
this ﬁlter does not have a mirror image. Analog designers sigh when they see this digital option. The
third ﬁgure shows the output of the band pass ﬁlter pair following the M-to-1 down sampling. If the
center frequency of the band is a multiple of the output sample rate the aliased band resides at base
band. For the example shown here the center frequency was  f above k fs/Mand since all multiples
of fs/M alias to base-band, our selected band has aliased to  f above zero frequency. Finally, the last
ﬁgure shows the spectrum after the output heterodyne from  f to base-band. Note that the heterodyne
is applied at the low output rate rather than at the input at the high input rate as was done in Figure 7.13.
Our last section of this preface deals with the equivalency of cascade operators. Here is the core idea.
Suppose we have a cascade of two ﬁlters, say a low pass ﬁlter followed by a derivative ﬁlter as shown
in Figure 7.17. The ﬁlters are linear operators that commute and are distributive. We could reorder the
two ﬁlters and have the same output from their cascade or we could apply the derivative ﬁlter operator
to the low pass ﬁlter to form a composite ﬁlter and obtain the same output as from the cascade. This
equivalency is shown in Figure 7.18. The important idea here is that we can ﬁlter the input signal and
then apply an operator to the ﬁlter output or we can apply the operator to the ﬁlter and have the altered
ﬁlter perform both operators to the input signal simultaneously.
Here comes the punch-line! Let us examine the aliasing digital down converter of Figure 7.15 which
contains a band pass ﬁlter followed by an M-to-1 resampler. This section is redrawn in the upper half of
Figure 7.19. If we think about it, this almost silly: Here we compute one output sample for each input
sample and then discard M-1 of these samples in the M-to-1 down sampler. Following the lead of the

1.07.3 Digital Filters
357
0
0
0
0
fs/2
fs/2
fs/M
fs/M
-fs/2
-fs/2
-fs/M
-fs/M
f
f
f
f
CHANNEL OF INTEREST
TRANSLATED FILTER
FILTERED SPECTRUM
ALIASED REPLICATES AT DOWN-SAMPLED RATE
TRANSLATED REPLICATES AT DOWN-SAMPLED RATE
INPUT ANALOG FILTER RESPONSE
....
....
....
....
FIGURE 7.16
Spectra at consecutive points in aliasing digital down converter with down sampling following ﬁlter.
cascade linear operators, the ﬁlter and resampler, being equivalent to a composite ﬁlter containing the
resampler applied to the ﬁlter we replace the pair of operators with the composite operator as shown in
the lower half of Figure 7.19.
Embedding the M-to-1 resampler in the band pass ﬁlter is accomplished by the block diagram shown
in Figure 7.20. This ﬁlter accepts M inputs, one for each path, and computes 1 output. Thus we do
not waste operations computing output sample scheduled to be discarded. An interesting note here is
that the complex rotators applied to the prototype low pass ﬁlter in Figure 7.15 have been factored out
of each arm and are applied once at the output of each path. This means that if the input data is real,
the samples are not made complex till they leave the ﬁlter. This represents a 2-to-1 savings over the
architecture of Figure 7.13 in which the samples are made complex on the way into the ﬁlter which
means there are 2 ﬁlters, one for each path of the complex heterodyne. The alias based DDS with

358
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
Low Pass
   Filter
Low Pass
Derivative
   Filter
Derivative
 Operator
x(n)
x(n)
y(n)
y(n)
.
y(t)
.
h(n)
h(n)
d
dn
d
dn
FIGURE 7.17
A cascade of two ﬁlters is same as a single ﬁlter formed by a composite ﬁlter.
FIGURE 7.18
Spectrum of cascade ﬁlters is same as spectrum of single ﬁlter containing combined responses.
resampler embedded in the ﬁlter is not a bad architecture! It computes output samples at the output rate
and it only uses one partitioned ﬁlter rather than 2. Not bad is understatement! The cherry on top of this
dessert is the rotator vector applied to the output of the path ﬁlters. This rotator can extract any band
from the aliased frequency spans that have aliased to base-band by the resampling operator. If it can
extract any band then we have the option of extracting more than one band, and in fact we can extract
every band from the output of the partitioned ﬁlter. That’s amazing! This ﬁlter can service more than
one output channel simultaneously; all it needs is additional rotator vectors. As we will see shortly, the
single ﬁlter structure can service all the channels that alias to base-band and the most efﬁcient bank of

1.07.3 Digital Filters
359
Low Pass
Filter
Re-sampled 
Low Pass Filter
x(n)
x(n)
y(n)
y(nM)
y(nM)
h(n)
h(nM)
M:1
M:1
FIGURE 7.19
Cascade of ﬁlter and M-to-1 resampler equivalent to M-to-1 resample ﬁlter.
H ( z )
0
H ( z )
1
H ( z )
2
H    ( z )
M-2
H    ( z )
M-1
....
....
x(n)
y(nM,k)
e
e
e
e
e
j     k0
j     k1
j     k2
j     k(M-2)
j     k(M-1)
2
2
2
2
2
M
M
M
M
M
π
π
π
π
π
FIGURE 7.20
M-path M-to-1 resample band pass ﬁlter performs aliased digital down converter.
rotators is implemented by an M-point IFFT. More to come! Be sure to read on! We have only scratched
the surface and the body of material related to multirate ﬁlters if ﬁlled with many wonderful properties
and applications.

360
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
1.07.4 Windowing
As speciﬁed in the previous section, digital ﬁlters can be classiﬁed in many ways, starting with the most
general frequency domain characteristics such as low pass, high pass, band pass, band stop and ﬁnishing
with, secondary characteristics such as uniform and non-uniform group delay. We now also know that
an important classiﬁcation is based on the ﬁlter’s architectural structure with a primary consideration
being that of ﬁnite impulse response and inﬁnite impulse response ﬁlter. Further sub-classiﬁcations,
such as canonic forms, cascade forms, lattice forms are primarily driven by consideration of sensitivity
to ﬁnite arithmetic, memory requirements, ability to pipeline arithmetic, and hardware constraints.
The choice to perform a given ﬁltering task with a recursive or a non-recursive ﬁlter is driven by
a number of system considerations, including processing resources, clock speed, and various ﬁlter
speciﬁcations. Performance speciﬁcations, which include operating sample rate, pass band and stop
band edges, pass band ripple, and out-of-band attenuation, all interact to determine the complexity
required of the digital ﬁlter.
In ﬁlters with inﬁnite impulse response each output sample depends on previous input samples and
on previous ﬁlter output samples. That is the reason for which their practical implementation always
requires a feedback loop. Thus, like all feedback based architectures, IIR ﬁlters are sensitive to input
perturbations that could cause instability and inﬁnite oscillations at the output. However inﬁnite impulse
response ﬁlters are usually very efﬁcient and require far few multiplications than an FIR ﬁlter per output
sample. Notice that an IIR ﬁlter could have an inﬁnite sequence of output samples even if the input
samples become all zeros. It is this characteristic which gives them their name.
Finite impulse response ﬁlters use only current and past input samples and none of the ﬁlter’s previous
output samples to obtain a current output sample value (remember that they are also sometimes referred
to as non-recursive ﬁlters). Given a ﬁnite duration of the input signal, the FIR ﬁlter will always have a
ﬁnite duration of non-zero output samples and this is the characteristic which gives them their name.
The procedure by which the FIR ﬁlters calculate the output samples is the convolution of the input
sequence with the ﬁlter impulse response which is shown in Eq. (7.16)
y(n) =
M−1

k=0
h(k)x(n −k),
(7.16)
where x(n) is the input sequence, h(n) is the ﬁlter impulse response and M is the number of ﬁlter taps.
The convolution is nothing but a series of multiplications followed by the addition of the products while
the impulse response of a ﬁlter is nothing but what the name tells us: it is the time domain view of the
ﬁlter’s output values when the input is an impulse (a single unity-valued sample preceded and followed
by zero-valued samples).
In the following paragraphs we introduce the basic window design method for FIR ﬁlters. Because
low pass ﬁltering is the most common ﬁltering task, we will introduce the window design method for
low pass FIR ﬁlters. However the relationships between ﬁlter length and ﬁlter speciﬁcations and the
interactions between ﬁlter parameters remain valid for other ﬁlter types. Many other techniques can be
also used to design FIR ﬁlters and, at the end of this section, we will introduce the Remez algorithm,

1.07.4 Windowing
361
FIGURE 7.21
Frequency response of a prototype ideal low pass ﬁlter.
FIGURE 7.22
Impulse response of a prototype ideal low pass ﬁlter.
which is one of the most used ﬁlter design technique, as well as one of its modiﬁed versions which
allows us to achieve 1/f type of decay for the out of band side-lobes.
The frequency response of a prototype low pass ﬁlter is shown in Figure 7.21. The pass band is seen
to be characterized by an ideal rectangle with unity gain between the frequencies ± f1 Hz and zero gain
elsewhere.
TheattractionoftheideallowpassﬁlterH(f)asaprototypeisthat,fromitsclosedforminverseFourier
transform, we achieve the exact expression for its impulse response h(t) which is shown in Eq. (7.17)
h(t) = 2 f1
sin(2π f1t)
2π f1t
.
(7.17)
The argument of the sin (x)/x function is always the product of 2π, half the spectral support (2 f1)/2,
and the independent variable t. The numerator is periodic and becomes zero when the argument is a
multiple of π. The impulse response of the prototype ﬁlter is shown in Figure 7.22. The sin (x)/x ﬁlter
shown in Figure 7.22 is a continuous function which we have to sample to obtain the prototype sampled
data impulse response. To preserve the ﬁlter gain during the sampling process we scale the sampled
function by the sample rate 1/ fs. The problem with the sample set of the prototype ﬁlter is that the
number of samples is unbounded and the ﬁlter is non-causal. If we had a ﬁnite number of samples,
we could delay the response to make it causal and solve the problem. Then our ﬁrst task is to reduce
the unbounded set of ﬁlter coefﬁcients to a ﬁnite set. The process of pruning an inﬁnite sequence to a
ﬁnite sequence is called windowing. In this process, a new limited sequence is formed as the product

362
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
of the ﬁnite sequence w(n) and the inﬁnite sequence as shown in Eq. (7.18) where the .∗operator is the
standard MATLAB point-by-point multiply
hw(n) = w(n).∗h(n).
(7.18)
Applying the properties of the Fourier transforms we obtain the expression for the spectrum of the
windowed impulse response which is the circular convolution of the Fourier transform of h(n) and w(n).
The symmetric rectangle we selected as our window abruptly turns off the coefﬁcient set at its bound-
aries.ThesampledrectangleweightingfunctionhasaspectrumdescribedbytheDirichletkernelwhichis
the periodic extension of the transform of a continuous time rectangle function. The convolution between
the spectra of the prototype ﬁlter with the Dirichlet kernel forms the spectrum of the rectangle windowed
ﬁlter coefﬁcient set. The contribution to the corresponding output spectrum is seen to be the stop band
ripple, the transition bandwidth, and the pass band ripple. The pass band and stop band ripples are due to
the side-lobes of the Dirichlet kernel moving through the pass band of the prototype ﬁlter while the tran-
sition bandwidth is due to the main lobe of the kernel moving from the stop band to the pass band of the
prototype. A property of the Fourier series is that their truncated version forms a new series exhibiting the
minimum mean square (MMS) approximation to the original function. We thus note that the set of coefﬁ-
cientsobtainedbyarectanglewindowexhibitstheminimummeansquareapproximationtotheprototype
frequency response. The problem with MMS approximations in numerical analysis is that there is no
mechanism to control the location or value of the error maxima. The local maximum errors are attributed
to the Gibbs phenomena, the failure of the series to converge in the neighborhood of a discontinuity.
These errors can be objectionably large. A process must now be invoked to control the objectionably
high side-lobe levels. We have two ways to approach the problem. First we can redeﬁne the frequency
response of the prototype ﬁlter so that the amplitude discontinuities are replaced with a speciﬁed tapering
in the transition bandwidth. In this process, we trade-off the transition bandwidth for side-lobe control.
Equivalently, knowing that the objectionable stop band side-lobes are caused by the side-lobes in the
spectrum of the window, we can replace the rectangle window with other even symmetric functions with
reduced amplitude side-lobe levels. The two techniques, side-lobe control and transition-bandwidth con-
trol are tightly coupled. The easiest way to visualize control of the side-lobes is by destructive cancelation
between the spectral side-lobes of the Dirichlet kernel associated with the rectangle and the spectral side-
lobes of translated and scaled versions of the same kernel. The cost we incur to obtain reduced side-lobe
levels is an increase in main lobe bandwidth. Remembering that the window’s two-sided main lobe width
is an upper bound to the ﬁlter’s transition bandwidth, we can estimate the transition bandwidth of a ﬁlter
required to obtain a speciﬁed side-lobe level. The primary reason we examined windows and their spec-
tral description as weighted Dirichlet kernels was to develop a sense of how we trade the main lobe width
of the window for its side-lobe levels and in turn ﬁlter transition bandwidth and side-lobe levels. Some
windows perform this trade-off of bandwidth for side-lobe level very efﬁciently while others do not.
The Kaiser-Bessel window is very effective while the triangle (or Fejer) window is not. The Kaiser-
Bessel window is in fact a family of windows parameterized over the time-bandwidth product, β, of the
window. The main lobe width increases with β while the peak side-lobe level decreases with β. The
Kaiser-Bessel window is a standard option in ﬁlter design packages such as Matlab and QED-2000.
Other ﬁlter design techniques include the Remez algorithm [6], sometimes also referred to as the
Parks-McClellan or P-M, the McClellan, Parks and Rabiner or MPR, the Equiripple, and the Multiple
Exchange algorithm, which found large acceptance in practice. It is a very versatile algorithm capable

1.07.5 Basics on Multirate Filters
363
of designing FIR ﬁlters with various frequency responses, including multiple pass band and stop band
frequency responses with independent control of ripple levels in the multiple bands. The desired pass
band cut off frequencies, the frequencies where the attenuated bands begin and the desired pass band and
stop band ripple are given as input parameters to the software which will generate N time domain ﬁlter
coefﬁcients. N is the minimum number of taps required for the desired ﬁlter response and it is selected
by using the harris approximation or the Hermann approximation (in Matlab). The problem with the
Remez algorithm is that it shows equiripple side-lobes which is not a desirable characteristic. We would
like to have a ﬁlter frequency response which has a l/f out-of-band decay rate rather than exhibit equirip-
ple. The main reasons for desiring that characteristic are related to system performance. We often build
systems comprising a digital ﬁlter and a resampling switch. Here the digital ﬁlter reduces the bandwidth
and is followed by a resampling switch that reduces the output sample commensurate with the reduced
output bandwidth. When the ﬁlter output is resampled, the low level energy residing in the out-of-band
spectral region aliases back into the ﬁlter pass band. When the reduction in sample rate is large, there
are multiple spectral regions that alias or fold into the pass band. For instance, in a 16-to-l reduction
in sample rate, there are 15 spectral regions that fold into the pass band. The energy in these bands is
additive and if the spectral density in each band is equal, as it is in an equiripple design, the folded
energy level is increased by a factor of sqrt(15). The second reason for which we may prefer FIR ﬁlters
with l/f side-lobe attenuation as opposed to uniform side-lobes is ﬁnite arithmetic. A ﬁlter is deﬁned
by its coefﬁcient set and an approximation to this ﬁlter is realized by a set of quantized coefﬁcients.
Given two ﬁlter sets h(n) and g(n), the ﬁrst with equiripple side-lobes, the second with l/f side-lobes,
we form two new sets, hQ(n) and gQ(n), by quantizing their coefﬁcients. The quantization process is
performed in two steps: ﬁrst we rescale the ﬁlters by dividing with the peak coefﬁcient. Second, we
represent the coefﬁcients with a ﬁxed number of bits to obtain the quantized approximations. The zeros
of an FIR ﬁlter residing on the unit circle perform the task of holding down the frequency response in the
stop band. The interval between the zeros contains the spectral side-lobes. When the interval between
adjacent zeros is reduced, the amplitude of the side-lobe between them is reduced and when the interval
between adjacent zeros is increased the amplitude of the side-lobe between them is increased. The zeros
of the ﬁlters are the roots of the polynomials H(z) and G(z). The roots of the polynomials formed by the
quantized set of coefﬁcients differ from the roots of the non-quantized polynomials. For small changes in
coefﬁcient size, the roots exhibit small displacements along the unit circle from their nominal positions.
The amplitude of some of the side-lobes must increase due to this root shift. In the equiripple design,
the initial side-lobes exactly meet the designed side-lobe level with no margin for side-lobe increase
due to root shift caused by coefﬁcient quantization. On the other hand, the ﬁlter with 1/f side-lobe levels
has plenty of margin for side-lobe increase due to root shift caused by coefﬁcient quantization.
More details on the modiﬁed Remez algorithm can be found in [6].
1.07.5 Basics on multirate ﬁlters
Multirate ﬁlters are digital ﬁlters that contain a mechanism to increase or decrease the sample rate while
processing input sampled signals. The simplest multirate ﬁlter performs integer up sampling of 1-to-
M or integer down sampling of Q-to-1. By extension, a multirate ﬁlter can employ both up sampling
and down sampling in the same process to affect a rational ratio sample rate change of M-to-Q. More
sophisticated techniques exist to perform arbitrary and perhaps slowly time varying sample rate changes.

364
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
FIGURE 7.23
Down sampling process; ﬁltering and sample rate reduction.
FIGURE 7.24
Up sampling process; sample rate increasing and ﬁltering.
The integers M and Q may be selected to be the same so that there is no sample rate change between input
and output but rather an arbitrary time shift or phase offset between input and output sample positions
of the complex envelope. The sample rate change can occur at a single location in the processing chain
or can be distributed over several subsections.
Conceptually, the process of down sampling can be visualized as a two-step progression indicated
in Figure 7.23. There are three distinct signals associated with this procedure. The process starts with
an input series x(n) that is processed by a ﬁlter h(n) to obtain the output sequence y(n) with reduced
bandwidth. The sample rate of the output sequence is then reduced Q-to-1 to a rate commensurate with
the reduced signal bandwidth. In reality the processes of bandwidth reduction and sample rate reduction
are merged in a single process called multirate ﬁlter. The bandwidth reduction performed by the digital
ﬁlter can be a low-pass or a band-pass process.
In a dual way, the process of up sampling can be visualized as a two-step process as indicated in
Figure 7.24. Here too there are three distinct time series. The process starts by increasing the sample rate
of an input series x(n) by resampling it 1-to-M. The zero-packed time series with M-fold replication of
the input spectrum is processed by a ﬁlter h(n) to reject the spectral replicas and output the sequence
y(m) with the same spectrum as the input sequence but sampled at M times higher sample rate. In
reality the processes of sample rate increase and selected bandwidth rejection are also merged in a
single process again called multirate ﬁltering.
The presented structures are the most basic forms of multirate ﬁlters; in the following we present
more complicated multirate architecture models. In particular we focus on the polyphase decomposition
of a prototype ﬁlter which is very efﬁcient when embedded in multirate structures. We also propose the
derivation, step by step, of both the polyphase down converter and up converter channelizers. These are
the two standard engines that we will modify for designing a novel polyphase channelizer that better
ﬁts the needs of the future software deﬁned radio.

1.07.6 From Single Channel Down Converter
365
1.07.6 From single channel down converter to standard down
converter channelizer
The derivation of the standard polyphase channelizer begins with the issue of down converting a single
frequency band, or channel, located in a multi channel frequency division multiplexed (FDM) input
signal whose spectrum is composed of a set of M equally spaced, equal bandwidth channels, as shown
in Figure 7.25. Note that this signal has been band limited by analog ﬁlters and has been sampled at a
sufﬁciently high sample rate to satisfy the Nyquist criterion for the full FDM bandwidth.
We have many options available for down converting a single channel; The standard processing chain
for accomplishing this task is shown in Figure 7.26. This structure performs the standard operations of
down converting a selected frequency band with a complex heterodyne, low pass ﬁltering to reduce the
output signal bandwidth to the channel bandwidth, and down sampling to a reduced rate commensurate
with the reduced bandwidth. The structure of this processor is seen to be a digital signal processor
implementation of a prototype analog I-Q down converter. We mention that the down sampler is com-
monly referred to as a decimator, a term that means to destroy one sample every tenth. Since nothing
is destroyed and nothing happens in tenths, we prefer, and will continue to use, the more descriptive
name, down sampler.
The output data from the complex mixer is complex, hence it is represented by two time series,
I(n) and Q(n). The ﬁlter with real impulse response h(n) is implemented as two identical ﬁlters, each
processing one of the quadrature time series. The convolution process between a signal and a ﬁlter is
often performed by simply multiply and sum operations between signal data samples and ﬁlter coefﬁ-
cients extracted from two sets of addressed memory registers. In this form of the ﬁlter, one register set
contains the data samples while the other contains the coefﬁcients that deﬁne the ﬁlter impulse response.
FIGURE 7.25
Spectrum of multichannel input signal, processing task: extract complex envelope of selected channel.
H(z)
FIGURE 7.26
Standard single channel down converter.

366
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
H(ze-jθk)
FIGURE 7.27
Band-pass ﬁlter version of single channel down converter.
By using the equivalency theorem, which states that the operations of down conversion followed by a
low-pass ﬁlter are equivalent to the operations of band-pass ﬁltering followed by a down conversion,
we can exchange the positions of the ﬁlter and of the complex heterodyne achieving the block diagram
shown in Figure 7.27.
Note here that the up converted ﬁlter, h(n) exp ( jθkn), is complex and as such its spectrum resides
only on the positive frequency axis without a negative frequency image. This is not a common structure
for an analog prototype because of the difﬁculty of forming a pair of analog quadrature ﬁlters exhibiting
a 90◦phase difference across the ﬁlter bandwidth. The closest equivalent structure in the analog world
is the ﬁlter pair used in image-reject mixers and even there, the phase relationship is maintained by a
pair of complex heterodynes.
Applying the transformation suggested by the equivalency theorem to an analog prototype system
does not make sense since it doubles the required hardware. We would have to replace a complex scalar
heterodyne (two mixers) and a pair of low-pass ﬁlters with a pair of band-pass ﬁlters, containing twice
the number of reactive components, and a full complex heterodyne (four mixers). If it makes no sense
to use this relationship in the analog domain, why does it make sense in the digital world? The answer
is found in the fact that we deﬁne a digital ﬁlter as a set of weights stored in the coefﬁcient memory.
Thus, in the digital world, we incur no cost in replacing the pair of low-pass ﬁlters h(n) required in the
ﬁrst option with the pair of band-pass ﬁlters h(n) cos (nθk) and h(n) sin (nθk) required for the second
one. We accomplish this task by a simple download to the coefﬁcient memory.
An interesting historical perspective is worth noting here. In the early days of wireless, radio tuning
was accomplished by sliding a narrow band ﬁlter to the center frequency of the desired channel to be
extractedfromtheFDMinputsignal.Theseradioswereknownastunedradiofrequency(TRF)receivers.
The numerous knobs on the face of early radios adjusted reactive components of the ampliﬁer chain
to accomplish the tuning process. Besides the difﬁculty in aligning multiple tuned stages, shifting the
center frequency of the ampliﬁer chain often initiated undesired oscillation due to the parasitic coupling
between the components in the radio. Edwin Howard Armstrong, an early radio pioneer, suggested
moving the selected channel to the ﬁxed frequency ﬁlter rather than moving the ﬁlter to the selected
channel. This is known as the superheterodyne principle, a process invented by Armstrong in 1918
and quickly adopted by David Sarnoff of the Radio Corporation of America (RCA) in 1924. Acquiring
exclusive rights to Armstrong’s single-tuning dial radio invention assured the commercial dominance of
RCA in radio broadcasting as well the demise of hundreds of manufacturers of TRF radio receivers. It
seems we have come full circle. We inherited from Armstrong a directive to move the desired spectrum
to the ﬁlter and we have readily applied this legacy to DSP-based processing. We are now proposing
that, under appropriate conditions, it makes more sense to move the ﬁlter to the selected spectral region.

1.07.6 From Single Channel Down Converter
367
H(z e-jθk)
FIGURE 7.28
Down sampled band-pass down converter.
H(z e-j2πk/M)
FIGURE 7.29
Band-pass down converter aliased to base-band by down sampler.
We still have to justify the full complex heterodyne required for the down conversion at the ﬁlter output
rather than at the ﬁlter input, which is done in the subsequent paragraphs.
Examining Figure 7.27, we note that following the output down conversion, we perform a sample
rate reduction in which we retain one sample out of M-samples. Because we ﬁnd it useless to down
convert the samples we discard in the next down sample operation, we move the down sampler before
the down converter achieving the demodulator scheme shown Figure 7.28. We note in this ﬁgure that
also the time series of the complex sinusoid has been down sampled. The rotation rate of the sampled
complex sinusoid is θk and Mθk radians per sample at the input and output of the M-to-1 resampler
respectively. This change in observed rotation rate is due to aliasing. When aliased, a sinusoid at one
frequency or phase slope appears at another phase slope due to the resampling.
We now invoke a constraint on the sampled data center frequency of the down converted channel.
We choose center frequencies θk, which will alias to DC as a result of down sampling to Mθk. This
condition is assured if Mθk is congruent to 2π, which occurs when Mθk = k2π, or more speciﬁcally,
when θk = k2π/M. The modiﬁcation to Figure 7.28 to reﬂect this provision is seen in Figure 7.29. The
constraint, that the center frequencies be limited to integer multiples of the output sample rate, assures
aliasing to base-band by the sample rate change. When a channel aliases to base-band because the
resampling operation the corresponding resampled heterodyne defaults to a unity-valued scalar, which
consequently is removed from the signal processing path.
Note that if the center frequency of the aliased signal is offset by θ rad/smpl from a multiple of
the output sample rate, the aliased signal will reside at an offset of θ rad/smpl from zero frequency
at base-band and a complex heterodyne, or base-band converter, is needed to shift the signal by the
residual θ offset. This base-band mixer operates at the output sample rate rather than at the input
sample rate for a conventional down converter. We can consider this required ﬁnal mixing operation as
a post conversion task and allocate it to the next processing block.

368
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
FIGURE 7.30
Spectrum, down sampled output signal.
The spectral effect of the signal processing performed by the structure in Figure 7.29 is shown in
Figure 7.30. The savings realized by this form of down conversion is due to the fact that we no longer
require a quadrature oscillator or the pair of input mixers to effect the required frequency translation.
Applying again the idea that it is useless to ﬁlter those samples that will be lately discarded by the
down sampler we apply the noble identity to exchange the operations of ﬁltering and down sampling.
The noble identity states that a ﬁlter processing every Mth input sample followed by an output M-to-1
down sampler is the same as an input M-to-1 down sampler followed by a ﬁlter processing every input
sample. Its interpretation is that the M-to-l down sampled time series from a ﬁlter processing every
Mth input sample presents the same output by ﬁrst down sampling the input by M-to-1, to discard the
samples not used by the ﬁlter when computing the retained output samples, and then operating the ﬁlter
on only the retained input samples. The noble identity works because samples of delay at the input clock
rate is the same interval as one-sample delay at the output clock rate.
At this point, with only a ﬁlter followed by a resampler, we can apply the polyphase decomposition
to the ﬁlter and separate it into M-parallel paths for achieving the standard M-path polyphase down

1.07.6 From Single Channel Down Converter
369
converter channelizer. In order to make the understanding process easier for the reader, we ﬁrst perform
the polyphase decomposition of a low-pass prototype ﬁlter and then we extend the results to the band-
pass ﬁlter case. Equation (7.19) represents the z transform of a digital low-pass prototype ﬁlter h(n)
H(z) =
N−1

n=0
h(n)z−n = h(0) + h(1)z−1 + h(2)z−2 + · · · + h(N −1)z−(N−1).
(7.19)
In order to achieve the polyphase partition of H(z), we rewrite the sum in Eq. (7.19), as shown in
Eq. (7.20), partitioning the one-dimensional array of weights in a two-dimensional array. In this mapping
we load an array by columns but process it by rows. The partition forms columns of length M containing
M successive terms in the original array, and continues to form adjacent M-length columns until we
account for all the elements of the original one-dimensional array
h(0) + h(M + 0)z−M + h(2M + 0)z−2M + · · ·
h(1)z−1 + h(M + 1)z−(M+1) + h(2M + 1)z−(2M+1) + · · ·
H(z) =
h(2)z−2 + h(M + 2)z−(M+2) + h(2M + 2)z−(2M+2) + · · ·
(7.20)
...
h(M −1)z−(M−1) + h(2M −1)z−(2M−1) + h(3M −1)z−(3M−1) + · · ·
We note that the ﬁrst row of the two-dimensional array is a polynomial in zM, which we will denote
H0(zM) a notation to be interpreted as an addressing scheme to start at index 0 and increment in strides of
length M. The second row of the same array, while not a polynomial in zM, is made into one by factoring
the common term z−1 and then identifying this row as z−1H1(zM). It is easy to see that each row of the
two-dimensional array described by Eq. (7.8) can be written as z−r Hr(zM) achieving the more compact
form as shown in Eq. (7.9) where r is the row index which is coincident with the path index
H(z) =
M−1

r=0
z−r Hr

zM
=
M−1

r=0
z−r
N
M −1

n=0
h(r + nM)z−nM.
(7.21)
The block diagram depicting the M-path polyphase decomposition of the resampled low-pass ﬁlter of
Eq. (7.21) is depicted in Figure 7.31.
At this point, in the structure depicted in Figure 7.31, we can pull the resampler on the left side of
the adder and down sample the separate ﬁlter outputs performing the sum only for the retained ﬁlter
output samples. With the resamplers at the output of each ﬁlter we can invoke again the noble identity
and place them at the input of each ﬁlter.
The resamplers operate synchronously, all closing at the same clock cycle. When the switches are
closed, the signal delivered to the ﬁlter on the top path is the current input sample. The signal delivered
to the ﬁlter one path down is the content of the one stage delay line, which, of course, is the previous
input sample. Similarly, as we traverse the successive paths of the M-path partition, we ﬁnd upon
switch closure, that the kth path receives a data sample delivered k samples ago. We conclude that the
interaction of the delay lines in each path with the set of synchronous switches can be likened to an input

370
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
FIGURE 7.31
M-path partition of prototype low-pass ﬁlter with output resampler.
z
z
z
z
FIGURE 7.32
M-path partition of prototype low-pass ﬁlter with input delays and M-to-1 resamples replaced by input
commutator.
commutator that delivers successive samples to successive arms of the M-path ﬁlter. This interpretation
is shown in Figure 7.32 which depicts the ﬁnal structure of the polyphase decomposition of a low-pass
prototype ﬁlter.
At this point we can easily achieve the band-pass polyphase partition from the low-pass partition by
applying the frequency translation property of the z-Transform that states the following.

1.07.6 From Single Channel Down Converter
371
If
H(z) = h(0) + h(1)z−1 + h(2)z−2 + · · · + h(N −1)zN−1 =
N−1

n=0
h(n)z−n
and
G(z) = h(0) + h(1)e jθz−1 + h(2)e j2θz−2 + · · · + h(N −1)e j(N−1)θzN−1
= h(0) + h(1)
	
e−jθz

−1
+ h(2)
	
e−jθz

−2
+ · · · + h(N −1)
	
e−j(N−1)θz

−(N−1)
=
N−1

n=0
h(n)
	
e−jθz

−n
,
then
G(z) = H(z)|z=e−jθz = H

e jθz

.
By using this property and replacing in Eq. (7.21) each z−r with z−re jrθ, where θ = k 2π
M , we achieve
H(ze−j(2πk/M)) =
M−1

r=0
z−re jr(2πk/M)Hr

zM
.
(7.22)
The complex scalars e
jr

2πk
M

attached to each path of the M-path ﬁlter can be placed anywhere along
the path. We choose to place them after the down sampled path ﬁlter segments Hr(zM). This change is
shown in Figure 7.33 that depicts the standard polyphase ﬁlter bank used for down converting a single
channel. The computation of the time series obtained from the output summation in Figure 7.33 is shown
z
z
z
z
FIGURE 7.33
Resampling M-path down converter.

372
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
in Eq. (7.23). Here, the argument nM reﬂects the down sampling operation, which increments through
the time index in strides of length M, delivering every Mth sample of the original output series. The
variable yr(nM) is the nMth sample from the ﬁlter segment in the rth path, and y(nM, k) is the nMth
time sample of the time series from the rth center frequency. Remember that the down converted center
frequencies located at integer multiples of the output sample frequency alias to zero frequency after the
resampling operation. Note that the output y(nM, k) is computed as a phase coherent summation of
the output series yr(nM). This phase coherent sum is, in fact, an inverse discrete Fourier transform of
the M-path outputs, which can be likened to beam forming the output of the path ﬁlters
y(nM, k) =
M−1

r=1
yr(nM)e j(2π/M)rk.
(7.23)
The beam forming perspective offers an interesting insight to the operation of the resampled down con-
verter system we have just examined; the reasoning proceeds as follows: the commutator delivering con-
secutive samples to the M input ports of the M-path ﬁlter performs a down sampling operation. Each port
of the M-path ﬁlter receives data at 1/Mth of the input rate. The down sampling causes the M-to-1 spectral
folding, effectively translating the M multiples of the output sample rate to base-band. The alias terms in
each path of the M-path ﬁlter exhibit unique phase proﬁles due to their distinct center frequencies and the
time offsets of the different down sampled time series delivered to each port. These time offsets are, in
fact, the input delays shown in Figure 7.31 and Eq. (7.3). Each of the aliased center frequency experiences
a phase shift shown in Eq. (7.24) equal to the product of its center frequency and the path time delay.
ϕ(r, k) = wkTr = 2π fs
M krT s = 2π fs
M kr 1
fs
= 2π
M kr.
(7.24)
The phase shifters of the IDFT perform phase coherent summations, very much like that performed in
narrowband beam forming, extracting from the myriad of aliased time series, the alias with the particular
matching phase proﬁle. This phase sensitive summation aligns contributions from the desired alias to
realize the processing gain of the coherent sum while the remaining alias terms, which exhibit rotation
rates corresponding to the M roots of unity, are destructively canceled in the summation. The inputs to the
M-path ﬁlter are not narrowband, and phase shift alone is insufﬁcient to cause the destructive cancelation
over the full bandwidth of the undesired spectral contributions. Continuing with our beam-forming
perspective, to successfully separate signals with unique phase proﬁles due to the input commutator
delays, we must perform the equivalent of time-delay beam forming. The M-path ﬁlters, obtained by
M-to-1 down sampling of the prototype low-pass ﬁlter supply the required time delays. The M-path ﬁlters
are approximations to all-pass ﬁlters, exhibiting, over the channel bandwidth, equal ripple approximation
to unity gain and the set of linear phase shifts that provide the time delays required for the time-delay
beam-forming task. The ﬁlter achieves this property by virtue of the way it is partitioned. Each of the
M-path ﬁlters, hr(n), for instance, with weights hr(r + nM), is formed by starting with an initial offset
of r samples and then incrementing in strides of M samples. The initial offsets, unique to each path, are
the source of the different linear phase-shift proﬁles. It is because of the different linear phase proﬁles,
that the ﬁlter partition is known as polyphase ﬁlter. An useful perspective is that the phase rotators
following the ﬁlters perform phase alignment of the band center for each aliased spectral band while the
polyphase ﬁlters perform the required differential phase shift across these same channel bandwidths.

1.07.6 From Single Channel Down Converter
373
FIGURE 7.34
Standard polyphase down converter channelizer: commutator, M-path polyphase ﬁlter and M-point IFFT.
When the polyphase ﬁlter is used to down convert and down sample a single channel, the phase
rotators are implemented as external complex products following each path ﬁlter. When a small number
of channels are being down converted and down sampled, appropriate sets of phase rotators can be
applied to the ﬁlter stage outputs and summed to form each channel output. Therefore the most advan-
tageous applications of that structure are those in which the number of channels to be down converted
becomes sufﬁciently large. Sufﬁciently large means on the order of log2 (N). Since the phase rotators
following the polyphase ﬁlter stages are the same as the phase rotators of an IDFT, we can use the
IDFT to simultaneously apply the phase shifters for all the channels we wish to extract from the aliased
signal set. This is reminiscent of phased-array beam forming. For computational efﬁciency, the IFFT
algorithm implements the IDFT.
The complete structure of a standard M-path polyphase down converter channelizer is shown in
Figure 7.34.
To summarize: in this structure the commutator performs an input sample rate reduction by commu-
tating successive input samples to selected paths of the M-path ﬁlter. Sample rate reduction occurring
prior to any signal processing causes spectral regions residing at multiples of the output sample rate to
alias to base-band. The partitioned M-path ﬁlter performs the task of aligning the time origins of the offset
sampled data sequences delivered by the input commutator to a single common output time origin. This
is accomplished by the all-pass characteristics of the M-path ﬁlter sections that apply the required differ-
ential time delay to the individual input time series. The IDFT performs the equivalent of a beam-forming
operation; the coherent summation of the time-aligned signals at each output port with selected phase
proﬁles. The phase coherent summation of the outputs of the M-path ﬁlters separates the various aliases
residing in each path by constructively summing the selected aliased frequency components located in
each path, while simultaneously destructively canceling the remaining aliased spectral components.
We conclude the reasoning on the standard M-path down converter channelizer by stating that it
simultaneously performs the following three basic operations: sample rate reduction, due to the input
commutator; bandwidth reduction, due to the M-path partitioned ﬁlter weights and Nyquist zone selec-
tion, due to the IFFT block.

374
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
FIGURE 7.35
Impulse response and frequency response of prototype low-pass ﬁlter.
In the following, in order to facilitate the understanding process for the reader, we show some
ﬁgures, which are results of Matlab simulation, to support the theoretical reasoning of the previous para-
graphs. The example concerns a 6-path down converter channelizer that performs 6-to-1 down sampling.
In particular Figure 7.35 shows the impulse response and the frequency response of the low-pass pro-
totype ﬁlter used for the polyphase partition.
Figure 7.36 shows the impulse responses of the zero-packed ﬁlter on each arm of the 6-path polyphase
partition while Figure 7.37 shows the spectra corresponding to the ﬁlters of Figure 7.36. It is evident
from this ﬁgure that the zero packing process has the effect of producing spectral copies of the ﬁlter
frequency response.
In Figure 7.38 we show the phase of each spectral copy of the zero packed prototype ﬁlter on each
arm of the partition. In this ﬁgure the phases of the spectral copies of each arm are overlaid and each
line of the plot corresponds to a speciﬁc arm in the partition.
In Figure 7.39 the same phase proﬁles of Figure 7.38 are de-trended, zoomed, and made causal.
In Figure 7.40 the 3-D view of the ﬁlter spectral copies, with their phase proﬁles, is shown for each
path, while, in Figure 7.41, the ﬁnal result of the channelization process is shown, again in a 3-D fashion.

1.07.6 From Single Channel Down Converter
375
FIGURE 7.36
Impulse response of 6-path polyphase partition prior to 6-to-1 resampling.
1.07.6.1 From single channel up converter to standard up converter channelizer
The standard M-path up converter channelizer that performs 1-to-M up sampling while up converting the
input time series is derived from the basic structure, shown in Figure 7.11, of a single channel interpolator
formed of an up sampler followed by an appropriate low-pass ﬁlter. In this conﬁguration, the up sampler
converts the Nyquist interval, which is the observable frequency span, from the input sample rate to a
span M times wider, which is the output sample rate. The 1-to-M up sampler zero packs the input time
series, effectively decreasing the distance between the input samples without modifying the spectral
content of the series. The wider Nyquist interval, spanning M input Nyquist intervals, presents M spectral
copies of the input spectrum to the low-pass ﬁlter. The amplitude of each of the M copies is 1/Mth of
the amplitude of the input signal spectrum. When a low-pass ﬁlter is scaled so that the peak coefﬁcient
is unity, the ﬁlter exhibits a processing gain inversely proportional to its fractional bandwidth. Thus, as
the ﬁlter eliminates the M-1 spectral copies, reducing the bandwidth by a factor of 1/M, the ﬁlter gain
precisely compensates for the attenuation of the input spectra due to the zero packing of the input series.
We start the modiﬁcations of this basic architecture by observing that the zero valued samples of the
zero-packed input time series do not contribute to the weighted sums formed at the ﬁlter output. Since
they do not contribute, there is no need to perform the product and sum from the input data registers

376
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
FIGURE 7.37
Frequency response of 6-path polyphase partition prior to 6-to-1 resampling.
containing the known zero-valued samples. Since only the non-zero packed samples contribute to the
ﬁlter output, we can track their location in the ﬁlter and perform the weighted sum only from the register
locations containing these samples. These locations are separated by M sample values, and their position
shifts through the ﬁlter as each new zero-valued input is presented to the input of the ﬁlter. Keeping
track of the coefﬁcient stride and the position of each coefﬁcient set is automatically performed by the
polyphase partition of the ﬁlter which is represented in Eq. (7.25). This equation describes the ﬁlter as a
sum of successively delayed sub-ﬁlters with coefﬁcients separated by the stride of M samples. Equation
(7.26) is a compact representation of Eq. (7.25) where the rth stage Hr(zM) of the polyphase ﬁlter is
formed by the coefﬁcient set that starts at index r and increments in steps of length M
H(z) =
M−1

r−0
z−r

N
M

−1

n=0
h(r + nM)z−nM,
(7.25)

1.07.6 From Single Channel Down Converter
377
FIGURE 7.38
Overlaid phase response of 6-path polyphase partition prior to 6-to-1 resampling.
H(z) =
M−1

r−0
z−r Hr

zM
.
(7.26)
The pictorial form of the ﬁlter in the M-path polyphase partition is shown in Figure 7.42. This structure
enables the application of the noble identity in which we slide the resampler through the ﬁlter and
replace the M units of delay at the output clock rate with one unit of delay at the input clock rate. Note
that the resampler cannot slide through the delays z−r following each ﬁlter segment Hr.
The resamplers following the separate ﬁlter stages up sample each time series by a factor of M,
and the delays in each arm shift each resulting time series at different time increments so that only
one non-zero time sample is presented to the summing junction at each output time. Thus, rather than
performing the sum with multiple zeros, we can simply point to the arm that sequentially supplies
the non-zeros samples. The output commutator, shown in Figure 7.43, performs this selective access.
Figure 7.42 represents the standard polyphase interpolator or up sampler. Due to the low-pass nature
of the ﬁlter on which the polyphase decomposition has been applied, this structure does not perform

378
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
FIGURE 7.39
De-trended overlaid phase response; 6-path partition prior to 6-to-1 resampling.
the up conversion of the interpolated signal yet. Its output is the same low-pass input spectrum with a
sample rate that is M times higher than the input sample rate.
We recall here that the purpose of the interpolation process is to increase the input sample rate while
translating the input spectrum to a higher carrier frequency. If used in a communication system, such
a structure directly translates the input signal to an intermediate frequency (IF), and then outputs the
digital IF signal via a single DAC. This option reduces the system costs by using a single DAC and a
band-pass ﬁlter to replace the standard base-band process requiring matched DACs, matched low-pass
ﬁlters, matched balanced mixers, and a quadrature oscillator to form the IF frequency band.
In the structure of Figure 7.24, the 1-to-M interpolating process zero packed the input data giving us
access to M spectral copies of the input time series. The spectral copies reside at multiples of the input
sample rate. The low-pass ﬁlter rejected the spectral copies, retrieving only the base-band copy centered
at DC that is then sent through a digital up converter for translation to the desired center frequency. It
is possible to perform the spectral translation as a part of the interpolation process when the desired
center frequency coincides with one of the multiples of the input sample rate. Rather than extracting
the spectral copy at base-band from the replicated set of spectra and then translating it by means of a

1.07.6 From Single Channel Down Converter
379
FIGURE 7.40
3-D Paddle-Wheel phase proﬁles; 6-path partition prior to 6-to-1 resampling.
complex heterodyne, we can directly extract one of the spectral copies by using a band-pass ﬁlter as
opposed to a low-pass ﬁlter. The band-pass ﬁlter is simply an up converted version of the low-pass ﬁlter
h(n) with weights shown in Eq. (7.27). Here the center frequency is interpreted as the kth multiple of
1
M of the output sample rate.
g(n, k) = h(n) exp

j 2π
M kn

.
(7.27)
The z-transform of the band-pass ﬁlter g(n, k), is shown in Eq. (7.28)
Gk(z) =
N−1

n=0
h(n) exp

j 2π
M kn

z−n.
(7.28)
The z-transform of the polyphase decomposition of this ﬁlter is shown in Eq. (7.29). Here we see that
the length M stride in coefﬁcient index due to the 1-to-M resampling aliases the phase rotators in the
polyphase ﬁlter stages to DC and hence has no effect on the polyphase weights. The phase rotator does,
however, have a contribution related to the delay associated with each arm of the partition. The output
commutator process absorbs the delays while the phase rotators are applied to each arm to obtain the

380
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
FIGURE 7.41
Overlaid 3-D Paddle-Wheel phase proﬁles; 6-path partition prior to 6-to-1 resampling.
z
z
z
z
z
z
z
FIGURE 7.42
Initial structure of 1-to-M polyphase interpolator.

1.07.6 From Single Channel Down Converter
381
z
z
z
z
FIGURE 7.43
Standard structure of polyphase interpolator.
spectral translation as a part of the interpolation.
Gk(z) =
M−1

r=0
z−r exp

j 2π
M rk
 N
M −1

n=0
h(r + nM) exp

j 2π
M nMk

z−nM
=
M−1

r=0
z−r exp

j 2π
M rk
 N
M −1

n=0
h(r + nM)z−nM.
(7.29)
The polyphase ﬁlter that accomplishes the task of simultaneous interpolation and up conversion to the
kth Nyquist zone is shown in Figure 7.44. This engine up samples and up converts, by aliasing, a single
narrowband channel to a particular center frequency which is a multiple of the input sample rate. By
noticing, as we did in the previous section of this chapter for the dual down converter channelizer, that
the phase rotators applied in each arm are the coefﬁcients of an inverse discrete Fourier transform it
is easy to generalize this structure to the standard multichannel polyphase up converter channelizer by
applying an IDFT block at the input of the ﬁlter bank.
Figure 7.45 shows the complete structure of the standard M-path polyphase up converter channelizer.
It is composed of an M point IDFT block, an M-path partitioned ﬁlter and an output commutator.
In this structure, we enter the M-channel process at the IDFT block and leave the process by the output
commutator. The M-point IDFT performs two simultaneous tasks; an initial up sampling of 1-to-M,
forming an M-length vector for each input sample x(n, k) and further imparts a complex phase rotation
of k cycles in M-samples on the up sampled output vector. It generates a weighted sum of complex
vectors containing integer number of cycles per M-length vector. The polyphase ﬁlter forms a sequence
of column coefﬁcient weighted, MATLAB’s dot-multiply, versions of these complex spinning vectors.
The sum of these columns, formed by the set of inner products in the polyphase partitioned ﬁlter, is the

382
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
z
z
z
z
FIGURE 7.44
Structure of 1-to-M polyphase interpolator with phase rotators for selecting the spectrum centered in Mth
nyquist zone.
FIGURE 7.45
Polyphase up converter channelizer: M-point IFFT, M-path polyphase ﬁlter and commutator.
shaped version of the up converted M-length vector output from the IFFT. On each output port of this
engine we ﬁnd the input base-band channel aliased to the speciﬁc Nyquist zone with a new sampling
rate that is commensurate with its reduced bandwidth.

1.07.7 Modiﬁcations of the Standard Down Converter Channelizer
383
A closing comment on both the standard polyphase up converter and down converter channelizer is
that the operations of sampling rate change, spectral shaping and Nyquist zone selection are completely
independent of each other. The channel bandwidth, the channel spacing and the output sampling rate
do not have necessarily to be equal but they can be all modiﬁed according to the applications.
1.07.7 Modiﬁcations of the standard down converter
channelizer—M:2 down converter channelizer
In the standard polyphase down converter channelizer shown in Figure 7.34 the channel bandwidth
fBW, the channel spacing f, and the sampling frequency fs are ﬁxed to be equal.
This conﬁguration could represent a good choice when the polyphase channelizer is used for com-
munication systems. In these kind of applications, in fact, an output sample rate matching the channel
spacing is sufﬁcient to avoid adjacent channel cross talk since the two-sided bandwidth of each channel
is less than the channel spacing. An example of a signal that would require this mode of operation is the
Quadrature Amplitude Modulation (QAM) channels of a digital cable system. In North America, the
channels are separated by 6 MHz centers and operate with square-root cosine tapered Nyquist-shaped
spectra with 18% or 12% excess bandwidth, at symbol rates of approximately 5.0 MHz. The minimum
sample rate required of a cable channelizer to satisfy the Nyquist criterion would be 6.0 MHz (The
European cable plants have channel spacing of 8.0 MHz and symbol rates of 7.0 MHz). The actual sam-
ple rate would likely be selected as a multiple of the symbol rate rather than as a multiple of the channel
spacing. Systems that channelize and form samples of the Nyquist-shaped spectrum often present the
sampled data to an interpolator to resample the time series collected at bandwidth-related Nyquist rate to
a rate offering two samples per symbol or twice symbol rate. For the TV example just cited, the 6 Ms/s,
5 MHz symbol signal would have to be resampled by 5/3 to obtain the desired 10 Ms/s. This task is done
quite regularly in the communication receivers and it may represent a signiﬁcant computational burden.
It would be appropriate if we could avoid the external interpolation process by modifying the design of
the standard polyphase channelizer for directly providing us the appropriate output sampling frequency.
Many others applications desire channelizers in which the output sampling frequency is not equal
to the channel spacing and the channel bandwidth. The design of software deﬁned radio receiver and
transmitter is only one of them. Another one can be found in [8].
We have concluded the previous section mentioning that the channel spacing, the channel bandwidth
and the output sampling frequency of the polyphase channelizer are completely independent of each
other and that they can be arbitrary selected based on the application. Figure 7.46 shows some possible
options in which the channel bandwidth, the output sampling rate and the channel spacing of the
channelizer are not equal. In this chapter, for the purpose of designing ﬂexible radio architectures, the
third option, when the selected low-pass prototype ﬁlter is a Nyquist ﬁlter (perfect reconstruction ﬁlter),
results to be the most interesting one.
In this section of the chapter we derive the reconﬁgured version of the standard down converter
channelizer that is able to perform the sample rate change from the input rate fs to the output sampling
rate 2 fs/M maintaining both the channel spacing and the channel bandwidth equal to fs. Similar
reasoning can be applied for implementing different selections.

384
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
FIGURE 7.46
Some possible channel width, channel spacing, and output sampling frequency.
Note that the standard down converter channelizer is critically sampled when the channel spacing
and the channel bandwidth are both equal to fs and when the output sample rate is also equal to fs/M.
This particular choice, in fact, causes the transition band edges of the channelizer ﬁlters to alias onto
itself which would prevent us from further processing the signals when they are arbitrarily located in
the frequency domain which is the case of software radio. This problem can be visualized in the upper
example depicted in Figure 7.46.
For the record, we remind that a polyphase ﬁlter bank can be operated with an output sample rate
which can be any rational ratio of the input sample rate. With minor modiﬁcations the ﬁlter can be
operated with totally arbitrary ratios between input and output sample rates. This is true for the sample
rate reduction imbedded in a polyphase receiver as well as for the sample rate increase embedded in a
polyphase modulator.
We have control on the output sampling rate of the down converter channelizer by means of the input
commutator that delivers input data samples to the polyphase stages. We normally deliver M successive
input samples to the M-path ﬁlter starting at port M-1 and progressing up the stack to port 0 and by
doing so we deliver M inputs per output which means to perform an M-to-1 down sampling operation.

1.07.7 Modiﬁcations of the Standard Down Converter Channelizer
385
z
z
z
z
z
z
z
z
z
z
z
z
z
FIGURE 7.47
M-path ﬁlter and M/2:1 down sampler.
To obtain the desired (M/2)-to-1 down sampling, we have to modify the input commutator in a way that
it delivers M/2 successive input samples starting at port (M/2)-1 and progressing up the stack to port 0.
We develop and illustrate the modiﬁcations with the aid of Figures 7.47–7.50.
Figure 7.47 represents the polyphase M-path ﬁlter partition shown in Eq. (7.9) with an M/2-to-1
rather than the conventional M-to-1 down sample operation after the output summing junction. We need
it in order to perform the desired sampling rate change.
In Figure 7.48 we apply the noble identity to the polyphase paths by pulling the M/2-to-1 down
sampler through the path ﬁlters which converts the polynomials in zM operating at the high input rate
to polynomials in z2 operating at the lower output rate.
Figure 7.49 shows the second application of the noble identity in which we again take the M/2-to-1
down samplers through the z−M/2 parts of the input path delays for the paths in the second or bottom
half of the path set.
In Figure 7.50 the M/2-to-1 down samplers switch and their delays are replaced with a two pronged
commutator that delivers the same sample values to path inputs with the same path delay. Here we also
merged the z−1 delays in the lower half of ﬁlter bank with their path ﬁlters.
Figure 7.51 shows and compares the block diagrams of the path ﬁlters in the upper and lower half
of this modiﬁed polyphase partition.

386
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
z
z
z
z
z
z
z
z
z
z
z
z
z
FIGURE 7.48
Noble identity applied to M-path ﬁlters.
When the input commutator is so designed, the M/2 addresses to which the new M/2 input samples
are delivered have to be ﬁrst vacated by their former contents, the M/2 previous input samples. All the
samples in the two-dimensional ﬁlter undergo a serpentine shift of M/2 samples with the M/2 samples
in the bottom half of the ﬁrst column sliding into the M/2 top addresses of the second column while
the M/2 samples in the top half of the second column slide into the M/2 addresses in the bottom half
of the second column, and so on. This is equivalent to performing a linear shift through the prototype
one-dimensional ﬁlter prior to the polyphase partition. In reality, we do not perform the serpentine
shift but rather perform an addressing manipulation that swaps two memory banks. This is shown in
Figure 7.52.
After each M/2-point data sequence is delivered to the partitioned M-stage polyphase ﬁlter, in the
standard channelizer conﬁguration, the outputs of the M stages are computed and conditioned for
delivery to the M-point IFFT. What we need to do at this point is the time alignment of the shifting
time origin of the input samples in the M-path ﬁlter with the stationary time origin of the phase rotator
outputs of the IFFT. We can understand the problem by visualizing, in Figure 7.53, a single cycle of

1.07.7 Modiﬁcations of the Standard Down Converter Channelizer
387
z
z
z
z
z
z
z
z
z
z
z
z
z
z
z
FIGURE 7.49
Noble identity applied to delays.
a sine wave extending over M samples being inserted in the input data register, the ﬁrst column of the
polyphase ﬁlter in segments of length M/2. We can assume that the data in the ﬁrst M/2 addresses are
phase aligned with the ﬁrst M/2 samples of a single cycle of the sine wave offered by the IFFT.
When the second M/2 input samples are delivered to the input data register the ﬁrst M/2 input samples
shift to the second half of the M-length array. Its original starting point is now at address M/2 but the
IFFT’s origin still resides at address 0. The shift of the origin causes the input sine wave in the register to
have the opposing phase of the sine wave formed by the IFFT; in fact the data shifting into the polyphase
ﬁlter stages causes a frequency dependent phase shift of the form shown in Eq. (7.30). The time delay
due to shifting is nT where n is the number of samples, and T is the time interval between samples. The
frequencies of interest are integer multiple k of 1/Mth of the sample rate 2π/T . Substituting these terms
in Eq. (7.30) and canceling terms, we obtain the frequency dependent phase shift shown in Eq. (7.31).
From this relationship we see that for time shifts n equal to multiples of M, as demonstrated in Eq. (7.32),
the phase shift is a multiple of 2π and contributes zero offset to the spectra observed at the output of the
IFFT. The M-sample time shift is the time shift applied to the data in the normal use of the polyphase
ﬁlter. Now suppose that the time shift is M/2 time samples. When substituted in Eq. (7.31) we ﬁnd, as
shown in Eq. (7.33), as frequency dependent phase shift of kπ from which we conclude that odd-indexed

388
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
z
z
z
z
z
z
z
FIGURE 7.50
Path delays replaced by input commutator.
z
z
z
FIGURE 7.51
M-path ﬁlters with and without extra delays.

1.07.7 Modiﬁcations of the Standard Down Converter Channelizer
389
FIGURE 7.52
Data memory loading for successive M/2-point sequences in an M-stage polyphase channelizer.
frequency terms experience a phase shift of π radians for each successive N/2 shift of input data
θ(ω) = t · ω,
(7.30)
θ(ωk) = nT · k 1
M
2π
T = nk
M 2π,
(7.31)
θ(ωk)|n=M = nk
M 2π

n=M
= 2πk,
(7.32)
θ(ωk)|n=M/2 = nk
M 2π

n=M/2
= πk.
(7.33)
This π radiants phase shift is due to the fact that the odd-indexed frequencies alias to the half sample
rate when the input signal is down sampled by M/2. What we are observing is the sinusoids with an odd

390
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
FIGURE 7.53
Illustrating phase reversal of M-point sinusoid input to M/2 path polyphase ﬁlter.

1.07.7 Modiﬁcations of the Standard Down Converter Channelizer
391
FIGURE 7.54
Cyclic shift of input data to IFFT to absorb phase shift due to linear time shift of data through polyphase ﬁlter.
number of cycles in the length M array alias to the half sample rate when down sampled M/2-to-1. Note
that, when down sampled M/2-to-1, the sinusoids with an even number of cycles in the length M array
alias to DC. We can compensate for the alternating signs in successive output samples by applying
the appropriate phase correction to the spectral data as we extract successive time samples from the
odd-indexed frequency bins of the IFFT. The phase correction here is trivial, but for other down-sampling
ratios, the residual phase correction would require a complex multiply at each transform output port.
Alternatively, since time delay imposes a frequency dependent phase shift, we can use time shifts to
cancel the frequency dependent phase shifts. We accomplish this by applying a circular time shift of
N/2 samples to the vector of samples prior to their presentation to the IFFT. As in the case of the ser-
pentine shift of the input data, the circular shift of the polyphase ﬁlter output data is implemented as an

392
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
FIGURE 7.55
M-to-2 modiﬁed down converter channelizer.
address-manipulated data swap. This data swap occurs on alternate input cycles and a simple two-state
machine determines for which input cycle the output data swap is applied. This is shown in Figure 7.54.
The complete structure of the modiﬁed version of the M-to-2 down converter channelizer with the
input data buffer and the circular data buffer is shown in Figure 7.55.
For brevity of notation, we avoid reporting here all the dual mathematical derivations that led at the
ﬁnal structure of the modiﬁed up converter channelizer. We brieﬂy explain its block diagram in the next
subsection.
1.07.7.1 Modiﬁcations of the standard up converter channelizer—2:M up
converter channelizer
Dual reasoning drives us to the reconﬁgured version of the M-path polyphase up converter channelizer
that is able to perform the sample rate change from 2 fs/M to fs maintaining both the channel spacing
and the channel bandwidth equal to fs. The choice of using a 2-to-M up sampler avoids the difﬁculty
of having the sample rate that precisely matches the two sided bandwidth of the input signals as well as
permitting a shorter length prototype channelizer ﬁlter due to an allowable wider transition bandwidth. In
this chapter we brieﬂy derive its block diagram that is shown in Figure 7.45. More details on this structure
can be found in [6,8,9]. We develop and illustrate the modiﬁcations with the aid of Figures 7.56–7.61.
In these ﬁgures we do not consider the IFFT block because, for now, it does not affect our reasoning.
Moreover we will introduce it later motivating the reason for which it is important to include a circular
buffer in the design of the modiﬁed up converter channelizer.
Figure 7.56 presents the structure of the M-path ﬁlter implementation of the polyphase partition
shown in Eq. (7.7). Note the 1-to-M/2 up-sample operation at the input port normally described as the

1.07.7 Modiﬁcations of the Standard Down Converter Channelizer
393
z
z
z
z
z
z
z
z
z
z
z
z
z
FIGURE 7.56
M-path polyphase ﬁlter.
zero-packing process. In Figure 7.57 we apply the noble identity to the polyphase paths and pull the 1-to-
M/2 up sampler through the path ﬁlters which convert the polynomials in zM operating at the high output
rate to polynomials in z2 operating at the low input rate. Note that the paths are now polynomials in z2
rather than zM as is the normal mode that we identify as the dual of the maximally decimated ﬁlter bank.
Figure 7.58 shows the second application of the noble identity in which we again take the 1-to-M/2
up sampler through the z−M/2 part of the output path delays for the paths in the second or bottom half
of the path set. The resultant delay, z−1, now operating at the input clock rate, is then interchanged with
its path ﬁlter as shown in Figure 7.59.
In Figure 7.60 the 1-to-M/2 up samplers switch and their delays are replaced with a pair of commu-
tators that add the path outputs with the same path delay. Finally, in Figure 7.61 we fold the single delay
in front of the lower set of path ﬁlters into the path ﬁlter. After having changed the output sampling rate
of the polyphase channelizer, the ﬁnal modiﬁcation we need to introduce is the time alignment of the
phase rotators from the input IFFT and the shifted time origin in the M-path ﬁlter.
From Figure 7.62, we note the locations of the non-zero coefﬁcients in the polynomials in z2 and
conclude that the input sine wave only contributes to the output time samples in the path ﬁlters located in
the upper half of the path set. When the next scaled sine wave output from the IFFT is inserted in the ﬁrst
column of the path ﬁlter memory, the previous scaled sine wave is shifted one column in the memory
and it will now contribute to the output time samples from the ﬁlter paths in the lower half of the path
ﬁlter set. The problem is that the sine wave samples in the lower half of the path ﬁlter set have opposite
polarity of the sine wave samples in the upper half of the path ﬁlter set. The samples from the two half

394
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
z
z
z
z
z
z
z
z
z
z
z
z
z
FIGURE 7.57
Noble identity applied to M-path ﬁlter.
ﬁlter sets are added and to do so they must have the same phase. We note that this alternating sign effect
only occurs for the odd indexed IFFT frequency bins which have an odd number of cycles per interval.
An alternate description of the sign reversal is that in the M/2 resampling of the modiﬁed M-path ﬁlter
the even indexed frequencies alias to multiples of the input sample rate and the odd indexed frequencies
alias to odd multiples of the half sample rate.
We recall here that there are two methods for performing the phase alignment of the successive output
vectors from the IFFT. In the ﬁrst method we simply invert the input phase of successive input samples
to the odd indexed IFFT bins. In the second method, recognizing that equivalency of phase shift and
time delay for sinusoids, on alternate outputs from the IFFT we apply an M/2 circular shift to its output
buffer prior to delivering the phase aligned vector to the path ﬁlter memory. This end around shift of
the output buffer occurs during the data transfer in memory and requires no additional manipulation of
the time samples.
Figure 7.63 shows the complete block diagram of the modiﬁed M-path up converter channelizer.
In this engine, the input commutator delivers the samples to the M-point IFFT. It applies the com-
plex phase rotation to the separate base-band input signals as well as performs the initial 1-to-M
up sampling of the input samples. The circular output buffer performs the correct data offset of the
two M/2 point halves of the IFFT output vector to maintain phase alignment with the M/2 chan-

1.07.8 Preliminaries on Software Deﬁned Radios
395
z
z
z
z
z
z
z
z
z
z
z
z
z
z
z
z
FIGURE 7.58
Noble identity applied to delays.
nelizer output vector. The complex sinusoid output by the IFFT always deﬁnes its time origin as
the initial sample of its output vector. The output of the polyphase ﬁlter exhibits a time origin that
shifts due to the M/2 time sample shift embedded in the output commutator. The M/2 time sample
shift of the output time series causes sinusoids with an odd number of cycles in the length M array
to alternate sign on successive shifts. The alternating sign is the reason that the odd indexed fre-
quency bins up convert to a frequency k + N/2 rather than frequency index k. Rather than reverse
phase alternate input samples to the odd indexed IFFT bins we perform an M/2 point circular shift of
alternate M-length vectors from the IFFT for applying the correct phase alignment to all frequencies
simultaneously.
The polyphase down converter and up converter channelizers shown in Figures 7.55 and 7.63 respec-
tively are the key elements of the proposed receiver and transmitter structures that we present in the
following sections. Because of the signal processing tasks they handle, in the following, we refer to
them as analysis and synthesis channelizers respectively.
1.07.8 Preliminaries on software deﬁned radios
The 20th century saw the explosion of hardware deﬁned radio as a means of communicating all forms of
data, audible and visual information over vast distances. These radios have little or no software control.
Their structures are ﬁxed in accordance with the applications; the signal modulation formats, the carrier

396
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
z
z
z
z
z
z
z
z
z
z
z
z
z
z
z
z
FIGURE 7.59
Interchange unit delay and path ﬁlters.
frequencies and bandwidths are only some of the factors that dictate the radio structures. The smallest
change to one of these parameters could imply a replacement of the entire radio system. A consequence
of this is, for example, the fact that a television receiver purchased in France does not work in England.
The reason, of course, is that the different geographical regions employ different modulation standards
for the analog TV as well as for digital TV. Then, the citizens cannot use the same TV for receiving
signals in both countries; they need to buy a new television for each country in which they decide to
live. Sometimes, even if the communication devices are designed for the same application purposes and
they work in the same geographical area, they are not able to communicate between each other. One
of the most evident examples of this is that the city police car radio cannot communicate with the city
ﬁre truck radio, or with the local hospital ambulance radio even if they have the common purpose of
helping and supporting the citizen. Also, the city ﬁre truck radio cannot communicate with the county
ﬁre truck radio, or with the radios of the ﬁre truck operated by the adjacent city, or by the state park
service, or the international airport. None of these services can communicate with the National Guard,
or with the local Port Authority, or with the local Navy base, or the local Coast Guard base, or the US
Border Patrol, or US Customs Service. In an hardware deﬁned radio, if we decide to change one of
the parameters of the transmitted signal, like bandwidth or carrier frequency (for example because the
carrier frequency we want to use is the only one available at that particular moment), we need to change
the transmitter. On the other side, every time we want to receive a signal having different bandwidth
or center frequency, we need to change the receiver. Hardware deﬁned transmitter and receiver devices

1.07.8 Preliminaries on Software Deﬁned Radios
397
z
z
z
z
z
z
z
z
z
z
z
FIGURE 7.60
Insert commutator add same delay paths.
are not ﬂexible at all; we must modify their structure every time we change even one of the transmitting
and receiving signal parameters.
In 1991, Joe Mitola coined the term software deﬁned radio. It was referred to a class of repro-
grammable (and reconﬁgurable) devices. At that time it was not clear at which level the digitization
should occur to deﬁne a radio as software but the concept sounded pretty interesting, and the dream of
building a completely reconﬁgurable radio device involved scientists from all over the world. Today the
exact deﬁnition of software deﬁned radio is still controversial, and no consensus exists about the level
of reconﬁgurability needed to qualify a radio as software.
Figure 7.64 shows, in a simple block diagram, all the possible places in which the digitization can
occur in a radio receiver. The exact dual block diagram can be portrayed for the radio transmitter. Current
radios, often referred to as digital but sometimes referred as software deﬁned radios (depending on their
particular structure), after shifting the signals to intermediate frequency, digitize them and assign all
the remaining tasks to a digital signal processor. One of the main reasons for shifting the signals to
intermediate frequency, before digitizing them, is to reduce their maximum frequency so that a smaller
number of samples can be taken for preserving the information content.
Implementation of ideal software radios requires digitization at the antenna, allowing complete
ﬂexibility in the digital domain. Then it requires both the design of a ﬂexible and efﬁcient DSP-based

398
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
z
z
z
z
z
z
z
z
z
z
z
FIGURE 7.61
Fold unit delays into path ﬁlters.
z
z
z
FIGURE 7.62
Comparison of path ﬁlter polynomials in z 2 with and without the additional input delay.

1.07.8 Preliminaries on Software Deﬁned Radios
399
FIGURE 7.63
2-to-M modiﬁed up converter channelizer.
FIGURE 7.64
Simple block diagram indicating all the possible places in which the digitization can occur in a radio receiver.
structure and the design of a completely ﬂexible radio frequency front-end for handling a wide range
of carrier frequencies, bandwidths and modulation formats. These issues have not been exploited yet in
the commercial systems due to technology limitations and cost considerations.
As pointed out in the previous section, in a current software deﬁned radio receiver the signals are
digitized in intermediate frequency bands. The receiver employs a super heterodyne frequency down
conversion, in which the radio frequency signals are picked up by the antenna along with other spurious,

400
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
unwanted signals (noise and interferences), ﬁltered, ampliﬁed with a low noise ampliﬁer and mixed with
a local oscillator to shift it to intermediate frequency. Depending on the application, the number of stages
of this operation may vary. Digitizing the signal in the IF range eliminates the last analog stage in the
conventional hardware deﬁned radios in which problems like carrier offset and imaging are encountered.
When sampled, digital IF signals give spectral replicas that can be placed accurately near the base-band
frequency, allowing frequency translation and digitization to be carried out simultaneously. Digital
ﬁltering and sample rate conversion are often needed to interface the output of the ADC to the processing
hardware to implement the receiver. Likewise, on the transmitter side, digital ﬁltering and sample rate
conversion are often necessary to interface the digital hardware, that creates the modulated waveforms,
to the digital to analog converter. Digital signal processing is usually performed in radio devices using
ﬁeld programmable gate arrays (FPGAs), or application speciﬁc integrated circuits (ASICs).
Even if the dream of building a universal radio is still far away, current software deﬁned radio archi-
tectures are quite ﬂexible, in the sense that they usually down convert to IF a collection of signals and,
after sampling, they are able to shift these signals to base-band via software. Changes in the signal band-
widths and center frequencies are performed by changing some parameters of the digital data section.
The ﬂexibility of such a structure can be improved by moving the analog to digital converter closest to the
receiver antenna. By digitizing the signals immediately after (and before in the transmitter) the antenna,
which is the ideal software radio case, the down conversion (and up conversion) processes are performed
completely in software and the radio acquires the capability of changing its personality, possibly in real-
time, guaranteeing a desired quality of service (QoS). The digitization after the receiver antenna, in fact,
allowsserviceproviderstoupgradetheinfrastructureandmarketnewservicesquickly.Itpromisesmulti-
functionality, global mobility, ease of manufacture, compactness and power efﬁciency. The ﬂexibility in
hardware architectures combined with ﬂexibility in software architectures, through the implementation
of techniques such as object oriented programming, can provide software radio also with the ability to
seamlessly integrate itself into multiple networks with widely different air and data interfaces.
1.07.9 Proposed architectures for software radios
A digital transmitter accepts binary input sequences and outputs radio frequency amplitude and phase
modulated wave shapes. The digital signal processing part of this process starts by accepting b-bit words
from a binary source at input symbol rate. These words address a look-up table that outputs gray coded
ordered pairs, i.e., I-Q constellation points, that control the amplitude and phase of the modulated carrier.
The I-Q pair is input to DSP-based shaping ﬁlters that form 1-to-4 up sampled time series designed
to control the wave shape and limit the base-band modulation bandwidth. The time series from the
shaping ﬁlter are further up sampled by a pair of interpolating ﬁlters to obtain a wider spectral interval
between spectral replicas of the sampled data. The interpolated data are then heterodyned by a digital up
converter to a convenient digital intermediate frequency and then moved from the sampled data domain
to the continuous analog domain by a digital to analog converter and analog IF ﬁlter. Further analog
processing performs the spectral transformations required to couple the wave shape to the channel.
On the other side of the communication chain, a digital receiver has to perform the tasks of ﬁltering,
spectral translation and analog to digital conversion to reverse the dual tasks performed at the trans-
mitter. The receiver must also perform a number of other tasks absent in the transmitter for estimating

1.07.9 Proposed Architectures for Software Radios
401
the unknown parameters of the received signal such as amplitude, frequency and timing alignment.
It samples the output of the analog IF ﬁlter and down converts the intermediate frequency centered
signal to base-band with a digital down converter (DDC). The base-band signal is down sampled by a
decimating ﬁlter and ﬁnally processed in the matched ﬁlter to maximize the signal-to-noise ratio (SNR)
of the samples presented to the detector. The DSP portion of this receiver includes carrier alignment,
timing recovery, channel equalization, automatic gain control, SNR estimation, signal detection, and
interference suppression blocks. In order to suppress the undesired artifacts introduced by the analog
components, the receiver also incorporates a number of digital signal processing compensating blocks.
Figure 7.65 shows the ﬁrst tier processing block diagrams of a typical digital transmitter-receiver
chain. The depicted transmitter is designed for sending, one per time, signals having ﬁxed bandwidths at
precisely located center frequencies. On the other hand, the depicted receiver is also designed for down
converting to base-band ﬁxed bandwidth signals having ﬁxed center frequencies. Therefore, the goal
of a cognitive radio is quite different: cognitive transmitter and receiver should be completely ﬂexible.
The transmitter has, in fact, to be able to simultaneously up convert, at desired center frequencies,
which are selected based on the temporary availability of the radio spectrum, a collection of signals
FIGURE 7.65
Block diagram of primary signal processing tasks in a typical transmitter and receiver.

402
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
having arbitrary bandwidths. It should also be able to decompose a signal in spectral fragments when
a sufﬁciently wide portion of the radio spectrum is not available for transmission. Of course no energy
loss has to occur in the partitioning process. A cognitive receiver has to be able to simultaneously
down convert the transmitted signal spectra wherever they are positioned in the frequency domain and
whatever bandwidths they have. Also it should have the capability of recognizing spectral segments
belonging to the same information signal and recompose them, after the base-band translation, without
energy losses. It could be possible to reach this goal by using the current technology but the cost would
be very high: for every signal we want to transmit we have to replicate the digital data section of both the
transmitter and the receiver. The more signals we have to simultaneously up and down convert, the more
sampled data sections we need to implement! Also, it is not possible, by using the current technology,
to fragment a single signal, transmit its spectral partitions using the temporary available spectral holes
and to perfectly recombine them at the receiver. Today the transmission happens under the constraint
that the available space in the radio spectrum is large enough to accommodate the entire bandwidth of
the signal to be transmitted.
In the next sections of this chapter, we present novel channelizers to simultaneously up and down
convert arbitrary bandwidth signals randomly located in the frequency domain. Differently from the
previous contents of this document, from this point onwards the presented material reproduces the results
of the authors’ research on software deﬁned radio design, thus it is new. The proposed architectures
can be used for the transmitter and for the receiver devices of a cognitive radio respectively. By using
them we avoid the need to replicate the sampled data sections for simultaneously transmitting and
receiving more signals with arbitrary bandwidths over multiple desired center frequencies. We also
gain the capability to partition the signal spectra before transmitting them and to perfectly reassemble
them in the receiver after the base-band shifting.
The core of the proposed transmitter structure is the variant of the standard M-path polyphase up
converter channelizer that is able to perform 2-to-M up sampling while shifting, by aliasing, all the
channels to desired center frequencies. It has been presented in Section 1.07.5 of this same chapter. In
the following we use the term synthesis channelizer for referring to this structure. This name has been
given for the tasks that this structure accomplishes when embedded in a SDR transmitter. When the input
signals have bandwidths wider than the synthesis channelizer bandwidth they are pre-processed through
small down converter channelizers that disassemble the signals’ bandwidths into reduced bandwidth
sub-channels which are matched to the base-line synthesis channelizer bandwidths. Complex sampled
frequency rotators are used to offset the signal spectra by arbitrary fractions of the sample frequency
before processing them through the channelizers. This is necessary for the completely arbitrary center
frequency positioning of the signals.
On the other side the variation of the standard M-path down converter channelizer, presented in
Section 1.07.6, represents the key element of the proposed receiver. It is able to perform M-to-2 down
sampling while simultaneously down converting, by aliasing, all the received signal spectra having
arbitrary bandwidths. In the following, we refer to this structure as analysis channelizer. Post-processing
channelizers, that are a smaller version of the synthesis channelizer composing the transmitter, are used
for reassembling, from the analysis channelizer base-line channels, signal bandwidths wider than the
analysis channelizer channel bandwidth. Possible residual frequency offsets can be easily solved with
the aid of a digital complex heterodyne while a post-analysis block performs further channelization,

1.07.9 Proposed Architectures for Software Radios
403
when it is required, for separating signal spectra falling in the same channelizer channel after the analysis
processing.
1.07.9.1 Proposed digital down converter architecture
The standard M-path polyphase down converter channelizer is able to shift to base-band, by aliasing, the
input signals that are exactly located on the channel’s center frequencies, kfc, where k = 0, 1, . . . , M−1.
In this engine the center frequencies of the channels are integer multiples of the channelizer output
sampling frequency as well as the channelizer channel bandwidth.
We recall here that the standard M-path down sampling channelizer simultaneously down converts,
by aliasing, from ﬁxed center frequencies to base-band, M bands narrower than the channelizer channel
bandwidth. If k fc ±  fk are the center frequencies of the input spectra, where ± fk are the arbitrary
frequency offsets from the channel center frequencies, k fc, after the down conversion process the output
spectra will be shifted by the same offset from DC, i.e., the down converted signals will be centered
at the frequency locations ± fk because the polyphase down converter channelizer only compensates
the k fc frequency terms. It is unable to compensate the frequency offsets, ± fk, that are responsible
for the arbitrary center frequency positioning of the input signals. The frequency offset compensation is
one of the motivations for which we need to add further processing blocks in the digital down converter
design when it is intended to be used in a software radio receiver. Other issues, like receiver resolution,
base-band reassembly of wide bandwidths and arbitrary interpolation of the base-band shifted signals
need to be considered too. All these issues are addressed and solved in the following paragraphs.
We also recall here that the standard M-to-1 down converter channelizer with the channel spacing
and channel bandwidth equal to the output sampling frequency is critically sampled (see upper plot of
Figure 7.29). In order to use it in a cognitive radio receiver, we need to modify it, redesigning the channel
spacing, channel bandwidth, and output sampling frequency. We presented one of these modiﬁcations
in Section 1.07.5, where the M-to-2 down converter channelizer has been derived. Also the selection
of the low-pass prototype ﬁlter is an important issue that has to be addressed in order to avoid energy
losses while the signal is processed.
Figure 7.66 shows the complete block diagram of the proposed down converting chain. The input
signal is processed, at ﬁrst, by an analysis channelizer which is the modiﬁed version of the standard
M-to-1 down converter channelizer presented in Section 1.07.5. By performing an M/2-to-1 down sam-
pling of the input time series this engine simultaneously shifts all the aliased channels to base-band
presenting an output sampling rate that is twice the sample rate of the standard M-to-1 channelizer.
Nyquist ﬁlters are used, as low-pass prototype, for avoiding energy losses while processing the signals.
They are brieﬂy described in one the next subsections. The interested reader can ﬁnd more details on
Nyquist ﬁlter design in [6].
Note that if more signals or even spectral fragments belonging to different signals, are channelized
in the same channel, further processing is needed to separate them at the output of the analysis down
converter. The extra ﬁltering process is performed in the post-analysis block that is connected to a
channel conﬁguration block that provides the receiver with the necessary information about the input
signal bandwidths and center frequencies. Different options can be implemented for simplifying the
design of the post-analysis block. One of the possible options is to decrease the bandwidth of the perfect
reconstruction prototype low-pass ﬁlter and to increase the number of points of the IFFT block in the

404
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
FIGURE 7.66
Block diagram of the proposed down converter; analysis down converter channelizer, post analysis block,
synthesis up converter channelizers, complex frequency rotators and arbitrary interpolators.
analysis channelizer. By designing the channel spacing and bandwidth to accommodate the most likely
expected signal width we minimize the possibility of having more than one signal in each channel. It
is also possible to modify the analysis channelizer for having a more convenient frequency positioning
of the aliased channels. The optimal choice, of course, depends on the receiver application and on the
workload requirements.
At the output of the analysis channelizer, most of the signals with bandwidths narrower than the
channel bandwidth are already down converted by means of the channelization process. However the
spectra wider than the channel bandwidth, or the spectra that, as a consequence of the arbitrary center
frequency positioning, are processed by two or more adjacent channels, have been fragmented and
their segments have all been aliased to the ﬁrst Nyquist zone; these segments need to be recomposed
before being translated to DC. The recomposition of spectral fragments is the task performed by the
synthesis channelizers. They have been presented in Section 9.5.1 and their block diagram is depicted
in Figure 7.59. These are small 2-to-Pn polyphase up converters in which the IFFT size, Pn, is properly
chosen in order to span the bandwidth of the nth received signal spectrum. We summarize the reason for
including the synthesizers in our design in this way: at the output of the M-to-2 analysis channelizer all
the signals have been down sampled and their spectra, or their spectral fragments, have been translated
to the ﬁrst Nyquist zone by the channelizing process. In order to reassemble the segments into a wider
bandwidth super channel, the time series from each segment must be up sampled and frequency shifted
to their appropriate positions so that they can be added together for forming the time series corresponding
to the wider bandwidth assembled signal.

1.07.9 Proposed Architectures for Software Radios
405
At the end of the down conversion and up conversion processes, all the received spectra have been
frequency translated and, when necessary, recomposed in the ﬁrst Nyquist zone. The analysis down
converterchannelizershiftedtobase-bandthesignalspectraandtheirfragments,thatareexactlycentered
onitschannels’centerfrequencies.Thisenginewasnotabletocompensatethefrequencyoffsetsderiving
from the arbitrary frequency positioning of the received signals.
The frequency offset compensation task is accomplished by the complex frequency rotators that
follow the small up converter synthesizers. They are connected with the channel conﬁguration block
that provides them proper information about the signal center frequencies.
Once the signals are perfectly centered at DC, arbitrary interpolators are used to adjust their sampling
rate to provide us exactly two samples per symbol needed for the further processing stages that are
performed in the digital receiver.
1.07.9.1.1
Post-analysis block and synthesis up converter channelizers
At the output of the analysis channelizer all the channels have been aliased to base-band. As a conse-
quence of that, the channelized segments of the received spectrum have been aliased to the ﬁrst Nyquist
zone (see Figure 7.61).
The following three options, that cover all the possible cases of bandwidths positioning in the
channelized spectrum, have to be considered and solved, by further processing the analysis channelizer
outputs, in order to achieve, at the end of the analysis-synthesis chain, all the received bands shifted to
base-band:
1. The aliased base-band channel could contain only one signal spectrum whose bandwidth is narrower
than the channel bandwidth. The carrier frequency of the signal, generally, does not coincide with
the center frequency of the channel.
2. The aliased base-band channel could contain two or more spectra, or also their fragments, belonging
to different signals. These spectra are arbitrarily positioned in the channel bandwidth.
3. The base-band channel could contain only one spectral fragment belonging to one of the received
signals which has bandwidth larger than the channel bandwidth.
In the ﬁrst option, at the output of the analysis channelizer, the signal spectra that are entirely contained
in one single channel reside in the ﬁrst Nyquist zone. Eventually the receiver has to compensate the
frequency offsets derived from their arbitrary center frequency positioning and resample them for
obtaining the desired output sampling rate of two samples per symbol. The complex frequency rotators
and the arbitrary interpolators perform these tasks at the end of the receiver chain.
For the case in which two or more spectra are processed by one single channel, more tasks need to
be performed before frequency offset compensation and arbitrary interpolation occur. We, in fact, have
to separate, by ﬁltering, the bands, or their fragments, belonging to different signals before processing
all of them independently. The separation task is performed by the post-analysis block. It performs a
further channelization process that ﬁlters, from every base-band aliased channel, the bands belonging to
different signals. Note that some of the ﬁltered signals, and precisely the ones with bandwidths narrower
than the channel bandwidth and entirely contained in the channel, only have to be frequency shifted
before being delivered to the arbitrary interpolator; else, the spectral fragments belonging to signals
processed in more than one channel have to be passed through the up converter synthesizers before
being frequency shifted and resampled.

406
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
FIGURE 7.67
Two stage arbitrary interpolator.
In the third case, that concerns the signals with bandwidths wider than the channel spacing, the anal-
ysis channelizer partitioned the bandwidths into several fragments and aliased every fragments to base-
band. In order to recombine them we must ﬁrst, up sample each input time series and second, translate
them to their proper spectral region. We can then form the sum to obtain the super channel representation
of the original signal bandwidth. Those are exactly the tasks performed by the synthesis channelizers.
1.07.9.1.2
High quality arbitrary interpolator
After all the bands have been translated to zero frequency, we need to resample them for obtaining
exactly the two samples per symbol needed for the subsequent processing performed in the receiver.
In this subsection we present the high quality interpolator structure used for obtaining two samples
per symbol needed for the second and third tier processing tasks in a digital receiver. It is well known
that the dynamic range of an arbitrary interpolator should match the system’s quantization noise level
[6]. The error due to the linear interpolation process, in fact, is not observable if it is below the noise
level attributed to the signal quantization process. Since the error due to the b-bit quantized signal is
2−b, the interpolation error or, equivalently, the level of residual spectral artifacts has to be below this
threshold. In other words, if the oversampling factor, N, satisﬁes Eq. (7.34), then the interpolation error
will not be noticeable:
 1
2N
2
≤1
2b .
(7.34)
Thus, if we interpolate a 16-bit data set, we keep the spectral artifacts below the quantization noise level
if N is greater than or equal to 128. To interpolate the signals by a factor of N = 128, we break the up
sampling process of the base-band centered spectra in two stages.
As depicted in Figure 7.67, we perform an initial 1-to-4 up sampling followed by a 1-to-32 up
sampling obtained by using the high quality arbitrary interpolator shown in Figure 7.68. The initial
4-times oversampling has the effect of strongly decreasing the length of the polyphase ﬁlters used in the
interpolator which signiﬁcantly reduces the total workload of this structure. More details on this topic
can be found in [6].
Note that the initial 1-to-4 signal up sampling can either be achieved by using two half-band ﬁlters
in cascade or we can previously up sample the signals by properly choosing Pn, the IFFT size of the
synthesis channelizers. Also, at the output of the synthesizers some of the signals could be already up
sampled by some factors because the IFFT size of the synthesizer has to be an even number [6] and it
also has to satisfy Nyquist criteria for every signal band.
The arbitrary interpolator shown in Figure 7.68 performs the linear interpolation between two avail-
able adjacent signal samples (two neighbor interpolation). The interpolation process operates as follows:

1.07.9 Proposed Architectures for Software Radios
407
z
FIGURE 7.68
Block diagram of high quality arbitrary interpolator.
A new input data sample is delivered to the interpolation ﬁlter register on each accumulator overﬂow.
The integer part of the accumulator content selects one of the M ﬁlter weights, hk(n), and one of the
M derivative ﬁlter weights, dhk(n), while the ﬁlters compute the amplitude y(n + k/M) and the ﬁrst
derivative ˙y(n + k/M) at the kth interpolated point in the M-point interpolated output grid. Then the
structure uses the local Taylor series to form the interpolated sample value between grid points. The
output clock directs the accumulator process to add the desired increment  (acc_incr in Figure 7.68)
to the modulo-M accumulator and increments the output index m. The increment  can be computed
according to the following equation: = M fin/ fout. This process continues until the accumulator over-
ﬂows at one of the output clock increments. Upon this overﬂow the input index is incremented, a new
input data point is shifted into the ﬁlter, and the process continues as before.
1.07.9.1.3
Nyquist ﬁlters
Particular attention has to be paid in designing the low-pass prototype ﬁlters used in the analysis channel-
izer. The signal spectra have to be randomly located in the frequency domain and their bandwidths can
easily span and occupy more than one base-line channel; also, in the signal disassembling and reassem-
bling processes, we need to collect all the energy corresponding to a single signal without losses.
Nyquist ﬁlter presents the interesting property of having a band edge gain equal to 0.5 (or −6 dB).
By using this ﬁlter as prototype in our channelizers, we place M of them across the whole spanned
spectrum with each ﬁlter centered on k fs/M. All adjacent ﬁlters exhibit −6 dB overlap at their band-
edges. The channelizer working under this conﬁguration is able to collect all the signal energy across
its full operating spectrum range even if signals occupy more than one adjacent channel or it resides

408
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
in the channel’s overlapping transition bandwidths. In the following paragraphs, for completeness, we
provide a brief introduction on Nyquist ﬁlters.
Nyquist pulses is the name given to the wave shapes f (n) required to communicate over band-limited
channels with no inter-symbol interference (ISI). When sampled at equally spaced time increments they
have to verify the requirement of Eq. (7.35) which is known as Nyquist pulse criterion for zero ISI:
f (n) =
0 if n ̸= 0,
1 if n = 0.
(7.35)
There are inﬁnite such functions that satisfy this set of restrictions. The one with minimum bandwidth
is the ubiquitous sin(x)/x which is variously known as the cardinal pulse when used for band limited
interpolation and the Nyquist pulse when used in pulse shaping. The transform of this wave shape,
R(f), is the unit area rectangle with spectral support 1/T Hz. Unfortunately this waveform is non-causal
and further it resides on an inﬁnite support. If the pulse resided on a ﬁnite support we could delay the
response sufﬁciently for it to be causal. We have to form ﬁnite support approximations to the sin(x)/x
pulse. The ﬁrst approximation to this pulse is obtained by convolving the rectangular spectrum, R(f),
with an even symmetric, continuous spectrum W(f) with ﬁnite support α/T . The convolution between
R(f) and W(f) in the frequency domain is equivalent to a product in the time domain between the r(t)
and w(t), where w(t) is the inverse transform of W(f). The effect of the spectral convolution is to increase
the two-sided bandwidth from 1/T to (1+α)/T . The excess bandwidth α/T is the cost we incur to form
ﬁlters on ﬁnite support. The term α is called the roll-off factor and is typically on the order of 0.5–0.1
with many systems using values of α = 0.2. The transition bandwidth caused by the convolution is
seen to exhibit odd symmetry about the half amplitude point of the original rectangular spectrum. This
is a desired consequence of requiring even symmetry for the convolving spectral mass function. When
the windowed signal is sampled at the symbol rate 1/T Hz, the spectral component residing beyond
the 1/T bandwidth folds about the frequency ±1/2T into the original bandwidth. This folded spectral
component supplies the additional amplitude required to bring the spectrum to the constant amplitude
of R(f).
Following this reasoning, we note that the signiﬁcant amplitude of the windowed wave shape is
conﬁned to an interval of approximate width 4T /α so that a ﬁlter with α = 0.2 spans approximately
20T, or 20 symbol durations. We can elect to simply truncate the windowed impulse response to obtain
a ﬁnite support ﬁlter, and often choose the truncation points at ±2T /α. A second window, a rectangle,
performs this truncation. The result of this second windowing operation is a second spectral convolution
with its transform. This second convolution induces pass-band ripple and out-of-band side-lobes in the
spectrum of the ﬁnite support Nyquist ﬁlter. The description of this band-limited spectrum normalized
to unity pass-band gain is presented in Eq. (7.36):
HNYQ(w) =
⎧
⎪⎪⎨
⎪⎪⎩
1
for
|w|
wSYM ≤(1 −α),
0.5 ∗

1 + cos

2π
α
	
w
wSYM −(1 −α)


for (1 −α) ≤
|w|
wSYM ≤(1 + α),
0
for
|w|
wSYM ≥(1 + α).
(7.36)
The continuous time domain expression for the cosine-tapered Nyquist ﬁlter is shown in Eq. (7.37).
Here we see the windowing operation of the Nyquist pulse as a product with the window that is the

1.07.9 Proposed Architectures for Software Radios
409
transform of the half-cosine spectrum:
hNYQ(t) = fSYM
sin (π fSYMt)
(π fSYMt)
cos (πα fSYMt)
[1 −(2α fSYMt)2].
(7.37)
Since the Nyquist ﬁlter is band limited, we can form the samples of a digital ﬁlter by sampling the
impulse response of the continuous ﬁlter. Normally this involves two operations. The ﬁrst is a scaling
factor applied to the impulse response by dividing by the sample rate, and the second is the sampling
process in which we replace t with nTs or n/ fs. The sample rate must exceed the two-sided bandwidth
of the ﬁlter that, due to the excess bandwidth, is wider than the symbol rate. It is standard to select the
sample rate fs to be an integer multiple of the symbol rate fSYM so that the ﬁlter operates at M-samples
per symbol. It is common to operate the ﬁlter at 4 or 8 samples per symbol.
In the design of the proposed analysis channelizer, the Nyquist prototype low-pass ﬁlter has to be
designed with its two sided 3 dB bandwidth equal to 1/Mth of the channelizer input sampling frequency.
This is equivalent to the ﬁlter impulse response having approximately M samples between its peak and
ﬁrst zero crossing and having approximately M samples between its zero crossings. The integer M is also
the size of the IFFT as well as the number of channels in the analysis channelizer. The prototype ﬁlter
must also exhibit reasonable transition bandwidth and sufﬁcient out of band attenuation or stop-band
level. We designed our system for a dynamic range of 80 dB which is the dynamic range of a 16-bit
processor.
1.07.9.2 Digital down converter simulation results
For simulation purposes, in this section we consider, at the input to the M-to-2 analysis channel-
izer, a composite spectrum that contains twelve QAM signals with ﬁve different bandwidths randomly
located in the frequency domain. In particular, the signal constellations are 4-QAM, 16-QAM, 64-QAM,
and 256-QAM while the signal bandwidths are 1.572132 MHz, 3.892191 MHz, 5.056941 MHz,
5.360537 MHz, and 11.11302 MHz respectively. It is clear that we used two different signal band-
widths for the 256-QAM constellation (5.360537 MHz and 11.11302 MHz).
The signals are shaped by square-root Nyquist ﬁlters with 20% excess bandwidth. At the input of the
analysis channelizer all the signals are resampled for achieving 192 MHz sample rate. The spectrum
is shown in Figure 7.56. In particular, the upper subplot of Figure 7.69 shows, superimposed on the
composite received spectrum, the 61 channels of the analysis channelizer. It is easy to recognize that
the received spectra are arbitrarily located. Their center frequencies do not coincide with the channel
center frequencies. That is clearly shown in the lower subplot of Figure 7.69 in which the enlarged
view of one of the received signals is presented. The arbitrary signal positioning is the reason for
which the polyphase down converter channelizer, by itself, is not able to directly shift them to DC.
The IFFT size of the analysis M-to-2 down converter channelizer is M = 48 with an output sample rate
of 8 MHz.
Figure 7.70 shows the impulse response and the magnitude response of the designed prototype low-
pass Nyquist ﬁlter. It is designed to have 48 samples per symbol. Its length is 1200 taps while its stop
band attenuation is −80 dB. Note that, since this ﬁlter is M-path partitioned, the length of each ﬁlter in
the M-path bank is only 25 taps.

410
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
Magnitude [dB]
28
30
32
34
36
38
40
42
44
-100
-80
-60
-40
-20
0
20
Frequency [MHz]
Zoom of Signal #
-80
-60
-40
-20
0
20
40
60
80
-100
-80
-60
-40
-20
0
20
Magnitude [dB]
Channelized Received Spectrum
Signal #
FIGURE 7.69
Channelized received spectrum and zoom of one of the received signals.
The 61 spectra, at the output of the analysis channelizer, are depicted in Figure 7.71. The down
converter channelizer has partitioned the entire frequency range into 48 segments. It is easy to recognize
in this ﬁgure the different spectra composing the received signal. Note that, because of the arbitrary
frequencypositioning,itispossiblethatalsothesignalshavingbandwidthsnarrowerthanthechannelizer
channel bandwidth occupy more than one analysis channelizer channel. Also in this case, before shifting
these signals to zero frequency by using complex frequency rotators, we need to pass them through a
synthesis channelizer that reassembles their fragments.
Before delivering the analysis channelizer outputs to the synthesis channelizers, we need to separate,
if necessary, by ﬁltering, those spectra that belong to different signals lying in the same channel. An
example of this is represented by channel 30 in Figure 7.71. It contains fragments of two spectra
belonging to different signals. The ﬁlter design in the post analysis block, of course, depends on the
bands that have to be resolved. Their sample rate has to be the same as the analysis channelizer output
rate (8 MHz). An example of post analysis ﬁlters along with the ﬁltered signals, for the 30th analysis
channelizer channel, is shown in Figure 7.72. In particular, the signal spectra and the ﬁlters used for
separating them are shown in the upper subplot while the separated spectra are shown in the lower
subplots.
At the output of the post analysis block all the spectra, with bandwidths narrower than the analysis
channel bandwidth, lying in a single analysis channel, can be directly delivered to the complex hetero-
dyne that translates them to DC. All the other signals, the ones with bandwidths wider than the analysis
channel bandwidth and, the ones with bandwidths narrower than the analysis channelizer channel band-
width that, as consequence of the analysis process, are delivered to two different channelizer outputs,
need to be processed by the small synthesis channelizers. We have, in fact, to up sample, frequency shift,
and recombine the time series from coupled analysis channelizer outputs. In the example of Figure 7.69

1.07.9 Proposed Architectures for Software Radios
411
0
200
400
600
800
1000
1200
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
Impulse Response
-200
-150
-100
-50
0
50
100
150
200
-120
-100
-80
-60
-40
-20
0
20
Frequency Response
Frequency (MHz)
Magnitude (dB)
-4
-2
0
2
4
-2
0
2
x 10
-3
Zoom into passband
MHz
dB
FIGURE 7.70
Nyquist prototype ﬁlter.
four of the received spectra, the narrowest ones, are directly sent to the frequency rotators. Eight of them
are processed through the synthesis channelizer. The IFFT sizes Pn, with n = 0, 1, 2, of the synthesiz-
ers we selected to process the three remaining bandwidths are: P0 = 6, P1 = 4, and P2 = 2 points.
These sizes are the minimum possible chosen to satisfy Nyquist sampling criterion for each output
signal. Note that because of the structure of the synthesizers, we can only have an even number of IFFT
points [6].
At the output of the synthesizer, all the signal fragments are recombined in base-band but still some of
them have a residual frequency offset that needs to be compensated. The signal spectra, before frequency
offset compensation are shown in Figure 7.73. Here it is clearly visible that many of the signals are not
centered at DC (the red1 line in Figure 7.73 represents the signals’ center frequency). We compensate
these frequency offsets by using complex frequency rotators. Note that by estimating the signal energy
in the synthesizer channels, we could easily recover the carrier offsets affecting the received spectra.
The frequency offset recovery is another of the many applications of the polyphase channelizer. Other
papers are being prepared for addressing this topic.
1For interpretation of color in Figure 7.73, the reader is referred to the web version of this book.

412
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
-4
-2
0
2
4
-40
-20
0
20
ch1
-4
-2
0
2
4
-40
-20
0
20
ch2
-4
-2
0
2
4
-40
-20
0
20
ch3
-4
-2
0
2
4
-40
-20
0
20
ch4
-4
-2
0
2
4
-40
-20
0
20
ch5
-4
-2
0
2
4
-40
-20
0
20
ch6
-4
-2
0
2
4
-40
-20
0
20
ch7
-4
-2
0
2
4
-40
-20
0
20
ch8
-4
-2
0
2
4
-40
-20
0
20
ch9
-4
-2
0
2
4
-40
-20
0
20
ch10
-4
-2
0
2
4
-40
-20
0
20
ch11
-4
-2
0
2
4
-40
-20
0
20
ch12
-4
-2
0
2
4
-40
-20
0
20
ch13
-4
-2
0
2
4
-40
-20
0
20
ch14
-4
-2
0
2
4
-40
-20
0
20
ch15
-4
-2
0
2
4
-40
-20
0
20
ch16
-4
-2
0
2
4
-40
-20
0
20
ch17
-4
-2
0
2
4
-40
-20
0
20
ch18
-4
-2
0
2
4
-40
-20
0
20
ch19
-4
-2
0
2
4
-40
-20
0
20
ch20
-4
-2
0
2
4
-40
-20
0
20
ch21
-4
-2
0
2
4
-40
-20
0
20
ch22
-4
-2
0
2
4
-40
-20
0
20
ch23
-4
-2
0
2
4
-40
-20
0
20
ch24
-4
-2
0
2
4
-40
-20
0
20
ch25
-4
-2
0
2
4
-40
-20
0
20
ch26
-4
-2
0
2
4
-40
-20
0
20
ch27
-4
-2
0
2
4
-40
-20
0
20
ch28
-4
-2
0
2
4
-40
-20
0
20
ch29
-4
-2
0
2
4
-40
-20
0
20
ch30
-4
-2
0
2
4
-40
-20
0
20
ch31
-4
-2
0
2
4
-40
-20
0
20
ch32
-4
-2
0
2
4
-40
-20
0
20
ch33
-4
-2
0
2
4
-40
-20
0
20
ch34
-4
-2
0
2
4
-40
-20
0
20
ch35
-4
-2
0
2
4
-40
-20
0
20
ch36
-4
-2
0
2
4
-40
-20
0
20
ch37
-4
-2
0
2
4
-40
-20
0
20
ch38
-4
-2
0
2
4
-40
-20
0
20
ch39
-4
-2
0
2
4
-40
-20
0
20
ch40
-4
-2
0
2
4
-40
-20
0
20
ch41
-4
-2
0
2
4
-40
-20
0
20
ch42
-4
-2
0
2
4
-40
-20
0
20
ch43
-4
-2
0
2
4
-40
-20
0
20
ch44
-4
-2
0
2
4
-40
-20
0
20
ch45
-4
-2
0
2
4
-40
-20
0
20
ch46
-4
-2
0
2
4
-40
-20
0
20
ch47
-4
-2
0
2
4
-40
-20
0
20
ch48
FIGURE 7.71
Analysis channelizer outputs.
When they are DC centered, the signals need to be resampled. The signals at the outputs of the
synthesis channelizers have different sampling frequencies. All that we need at this point is to interpolate
them for obtaining exactly two samples per symbol for each of them. We use an arbitrary interpolator for
achieving the sample rate conversion. The interpolated, DC shifted, signals are shown in Figure 7.74.
We also match-ﬁltered each of the eight channelized and reconstructed time signals and present their
constellations in Figure 7.75, here we see that all of the QAM constellations are perfectly reconstructed
which demonstrates the correct functionality of the proposed receiver.
1.07.9.3 Proposed up converter architecture
Figure 7.76 shows the complete block diagram of the proposed up converter, which has the structure
that is a dual of the down converter structure presented in the previous sections. It is composed of
a 2-to-M up sampler synthesis channelizer, which has been presented in Section 9.5.1, preceded by
N pre-processing analysis blocks which are small Pn-to-2 analysis down converter channelizers. Their
number, N, corresponds to the number of the signal spectra wider than the synthesis channelizer channel
bandwidth. Their IFFT size, Pn, is chosen to completely span the bandwidth of the nth input signal
spectrum. Their task is to decompose the wider input spectra into Pn partitions matching the bandwidth
and sample rate of the base-line synthesis channelizer which will coherently recombine them in the
receiver.

1.07.9 Proposed Architectures for Software Radios
413
-4
-3
-2
-1
0
1
2
3
4
Frequency / MHz
-4
-3
-2
-1
0
1
2
3
4
Frequency / MHz
Frequency / MHz
Magnitude / dB
Post Analysis Seperation Filters
Magnitude / dB
0
-100
-4
-3
-2
-1
0
1
2
3
4
-100
-50
-100
-50
-50
0
0
Magnitude / dB
FIGURE 7.72
Post analysis ﬁlters and ﬁltered signals for channel 30 of the analysis channelizer.
All the input signals to the 2-to-M synthesis channelizer with bandwidths less than or equal to the
channel spacing are to be up sampled and translated from base-band to the selected center frequency by
the channelizing process. For these signals we may only have to ﬁlter and resample to obtain the desired
sampling rate of two samples per channel bandwidth. However, we expect that many of the spectra
we presented to the synthesis channelizer have bandwidths that are wider than the synthesizer channel
spacing. For accommodating these signals we need to use the small analysis channelizers that partition
their bandwidths into several fragments, translate all of them to base-band, and reduce their sample
rate to twice the channel bandwidth. The analysis channelizers are designed as Pn-to-2 polyphase down
converter channelizers where Pn is approximately twice the number of base-line channels spanned
by the wideband spectrum. They have been presented in Section 1.07.5 of this same document and
Figure 7.55 shows their block diagram.
We brieﬂy recall here that such a system accepts N/2 input samples and outputs time samples from N
output channels. The N-point input buffer is fed by a dual input commutator with an N/2 sample offset.
The N-path ﬁlter contains polynomials of the form Hr(z2) and z−1Hr+N/2(z2) in its upper and lower
halves respectively. The circular output buffer performs the phase rotation alignment of the IFFT block

414
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
FIGURE 7.73
Log magnitude of synthesizer outputs with frequency offsets in the normalized frequency domain.
with the N/2 stride shifting time origin of the N-path polyphase ﬁlter. The IFFT size for these analysis
channelizers, Pn, is also the number of their ﬁlter outputs.
InordertorecombinethesegmentedsignalcomponentswehavetosatisfytheNyquistcriteriafortheir
sum. Since these are pre-processor analysis channelizers that feed the M-path synthesis channelizer we
must have their output sample rate two times their channel bandwidth. We can achieve this by selecting
Pn to be approximately twice the number of channels being merged in the synthesizer and setting its
input sample rate to be 2 fs Pn so that the pre-processor output rate per channel is the required 2 fs.
Remember that the IFFT block sizes must be even to perform the 2-to-N resampling by the technique
described in Section 1.07.5; also remember that, an actual system may have a few standard size IFFT’s
to be used for the analysis channelizer and the user may have to choose from the small list of available
block sizes.
The channel selector placed between the analysis channelizer bank and the synthesis channelizer
also connected with the input channel conﬁguration block provides the correct outputs from the pre-
processor analysis channelizer to the input synthesizer while the channel conﬁguration block provides
the necessary information to the selector block that is connected to the synthesis input series. The

1.07.9 Proposed Architectures for Software Radios
415
FIGURE 7.74
Log magnitude of the heterodyned and interpolated spectra in the frequency domain [MHz].
selector routes all the segments required to assemble the wider bandwidth channel to the synthesizer
which performs their frequency shift and reassembly.
Depending on the desired center frequency of the disassembled spectrum an earlier complex het-
erodyne may be required before the analyzers to shift of the about to be disassembled signals with the
proper frequency offset.
1.07.9.4 Digital up converter simulation results
In the simulation results shown in this section we consider a set of eight distinct base-band input signals
to be delivered to the 2-to-M up converter synthesis channelizer. These are QPSK signals with three
different bandwidths as shown in Figure 7.77. The symbol rates chosen for the three signals denoted
1, 2, and 3, are 7.5, 15.0, and 30.0 MHz. These signals are shaped by square-root Nyquist ﬁlters with
25% excess bandwidth, hence the two sided bandwidths are 7.5·1.25, 15.0·1.25, and 30.0·1.25 MHz
respectively.
The IFFT size of the base-line 2-to-M up converter channelizer is M = 48 with 10 MHz channel
spacing for which the required input sample rate per input signal is 20 MHz and for which the output

416
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
-1
0
1
-1
-0.5
0
0.5
1
-1
0
1
-1
-0.5
0
0.5
1
-1
0
1
-1
-0.5
0
0.5
1
-1
0
1
-1
0
1
-1
0
1
-1
0
1
-1
0
1
-1
-0.5
0
0.5
1
-1
0
1
-1
0
1
-1
0
1
-1
0
1
-1
0
1
-1
0
1
-1
0
1
-1
0
1
-1
0
1
-1
0
1
-1
0
1
-1
0
1
FIGURE 7.75
Heterodyned and interpolated constellations.
sample rate will be 480 MHz. For ease of signal generation, all three signals were shaped and up-
sampled to 60 MHz sample rate with shaping ﬁlters designed for 8, 4, and 2 samples per symbol. Signal
1, represented in the ﬁrst line of Figure 7.77, is down sampled 3-to-1 to obtain the desired 20 MHz
sample rate for the synthesis channelizer. Signals 2 and 3, respectively on the second and third line of
Figure 7.77, are down sampled 6-to-2 in the six point IFFT analysis channelizers which form 10 MHz
channels at 20 MHz sample rate. Signal 2 is spanned by three 10 MHz channels which will feed 3 input
ports of the 48 point synthesizer IFFT while signal 3 is spanned by ﬁve 10 MHz channels which will

1.07.9 Proposed Architectures for Software Radios
417
FIGURE 7.76
Block diagram of the proposed up converter; arbitrary interpolators, complex frequency rotators, analysis
down converter channelizers and synthesis up converter channelizer.
feed 5 input ports of the 48 point IFFT. The IFFT inputs are controlled by a channel control block that
routes them to the proper synthesis channelizer ports.
Any of the signals presented to the analysis channelizers is previously shifted to a desired frequency
offset, by means of a complex heterodyne if required. The output channels that span the offset input
bandwidth of the analysis channelizer are the channels passed on the synthesizer and these change due
to a frequency offset. The inserted base-band frequency offset will survive the synthesis channelizer.
Figure 7.78 shows all the spectra of the output time series from the 6-channel polyphase 6-to-2
analysis channelizer engine processing signal 3 which is the signal with the widest band. Also seen, in
the same ﬁgure, is the spectrum of signal 3 and the frequency response of all the 6 channels formed
by one of the analysis channelizers. Note the spectra in the upper subplots have been ﬁltered by the
channelizer with the 10 MHz Nyquist pass band frequency response, have been translated to base-band
and have been sampled at 20 MHz. Five of these segments are presented to the ﬁve input ports of the
2-to-48 synthesis channelizer centered on the desired frequency translation index.
The up converter polyphase synthesis channelizer accepts time sequences sampled at 20 MHz with
bandwidths less than 10 MHz. We have delivered three signals that satisfy these constraints along
with four signals that were conditioned by analysis channelizers that partitioned their bandwidths into
segments that also satisﬁed the input signal constraints. The spectra of the separate components delivered
to the synthesis channelizer are shown in Figure 7.79. It is easy to recognize in this ﬁgure the different

418
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
z
z
FIGURE 7.77
Example of base-band spectra to be up converted.
FIGURE 7.78
Spectral fragments formed by 6-channel 6-to-2 down sample analysis channelizer processing signal 3.

1.07.9 Proposed Architectures for Software Radios
419
-20
0
20
-40
0
40
ch1
-20
0
20
-40
0
40
ch2
-20
0
20
-40
0
40
ch3
-20
0
20
-40
0
40
ch4
-20
0
20
-40
0
40
ch5
-20
0
20
-40
0
40
ch6
-20
0
20
-40
0
40
ch7
-20
0
20
-40
0
40
ch8
-20
0
20
-40
0
40
ch9
-20
0
20
-40
0
40
ch10
-20
0
20
-40
0
40
ch11
-20
0
20
-40
0
40
ch12
-20
0
20
-40
0
40
ch13
-20
0
20
-40
0
40
ch14
-20
0
20
-40
0
40
ch15
-20
0
20
-40
0
40
ch16
-20
0
20
-40
0
40
ch17
-20
0
20
-40
0
40
ch18
-20
0
20
-40
0
40
ch19
-20
0
20
-40
0
40
ch20
-20
0
20
-40
0
40
ch21
-20
0
20
-40
0
40
ch22
-20
0
20
-40
0
40
ch23
-20
0
20
-40
0
40
ch24
-20
0
20
-40
0
40
ch25
-20
0
20
-40
0
40
ch26
-20
0
20
-40
0
40
ch27
-20
0
20
-40
0
40
ch28
-20
0
20
-40
0
40
ch29
-20
0
20
-40
0
40
ch30
-20
0
20
-40
0
40
ch31
-20
0
20
-40
0
40
ch32
-20
0
20
-40
0
40
ch33
-20
0
20
-40
0
40
ch34
-20
0
20
-40
0
40
ch35
-20
0
20
-40
0
40
ch36
-20
0
20
-40
0
40
ch37
-20
0
20
-40
0
40
ch38
-20
0
20
-40
0
40
ch39
-20
0
20
-40
0
40
ch40
-20
0
20
-40
0
40
ch41
-20
0
20
-40
0
40
ch42
-20
0
20
-40
0
40
ch43
-20
0
20
-40
0
40
ch44
-20
0
20
-40
0
40
ch45
-20
0
20
-40
0
40
ch46
-20
0
20
-40
0
40
ch47
-20
0
20
-40
0
40
ch48
FIGURE 7.79
2-to-M up converter channelizer inputs.
FIGURE 7.80
Up converted synthesized spectrum with unequal bandwidths fragmented and defragmented spectral
components.

420
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
spectra composing the received signal. Here we see that ﬁlters 4-through-8 are segments of a single
frequency band fragmented in Figure 7.78.
At this point, we up sample, frequency shift, and recombine the time series from coupled channel
outputs using the synthesis channelizer. The frequency shifted spectra of the eight signals, including six
that have been fragmented in analysis channelizers and then defragmented in the synthesis channelizer
are plotted in Figure 7.80.
1.07.10 Closing comments
This chapter had the intention of providing the basic concepts on multirate signal processing and
polyphase ﬁlter banks to which, when an IDFT block and a commutator are applied, are known as
polyphase channelizer because of the processing task that they perform on the input signals. This
chapter also had the intention of presenting innovative results on software and cognitive radio designs.
While the polyphase ﬁlter banks and polyphase channelizers are well known topics in the DSP area,
the software radio design is still an open research topic.
The document started with preliminaries, in Section 1.07.2, on the resampling process of a digital
signal as opposed to the sampling process of an analog signal. In Section 1.07.3 an introduction on digital
ﬁlters has been provided and the differences with the analog ﬁlters have been explained. In Section 1.07.4
an introduction on the window method for digital ﬁlter design has been provided. Multirate ﬁlters have
been deﬁned and explained in Section 1.07.5 while the polyphase decomposition of a prototype ﬁlter
has been introduced in Section 1.07.6 along with the standard up sampler and down sampler polyphase
channelizers. In Section 1.07.7 the modiﬁcations of the standard polyphase channelizer that allow us
to change the output sampling rate have been presented. These engines are the basic components of the
proposed architectures (presented in Sections 1.07.8 and 1.07.9): the synthesis and analysis channelizers,
that are suitable for being used as software deﬁned transmitter and receiver respectively.
The synthesis channelizer, in fact, when supported by small analysis channelizers, is able to simulta-
neously up convert multiple signals having arbitrary bandwidths to randomly located center frequencies.
In this engine small analysis channelizers pre-process wider bandwidth signals into segments acceptable
to the following synthesis channelizer that up samples, translates to the proper center frequency, and
reassembles them. A channel selector block, connected with a channel conﬁguration block, is used to
properly deliver the analysis channelizer outputs to the synthesis channelizer.
On the other side of the communication chain, the analysis channelizer, that is thought for being
embedded in a SDR receiver, when supported by small synthesis channelizers, is able to simultaneously
demodulate these signals. A channel conﬁguration block, inserted in the receiver, communicates with
a selector block that properly delivers the analysis channelizer outputs to the synthesizer up converters
that follow it. These synthesizers up sample, translate and reassemble the spectral fragments when they
belong to the same source signal. Nyquist prototype low-pass ﬁlters, which are perfect reconstruction
ﬁlters, are used to allow the reconstruction of the signal fragments without energy losses.
Complex frequency rotators are used for compensating residual frequency offsets and arbitrary
interpolators provide us exactly two samples per symbol required for the following processing tasks of
the receiver.
Theoretical reasoning and simulation results are presented, for both the receiver and the transmitter,
in order to demonstrate their correct functionality.

References
421
Note that because in both of the proposed DUC and DDC structures, the IFFT sizes of the small
synthesis and analysis channelizers are chosen in order to give us at least two samples per symbol for
every processed bandwidth, they result to be very efﬁcient, from a computational point of view, when
the received signal is composed of spectra with widely varying bandwidths.
Slightly different versions of channelizers can be used based on different input signals and/or for
adding functionalities to the proposed architectures. Channelizers are, in fact, highly efﬁcient and
very ﬂexible structures. Among their most common applications we ﬁnd spectral analysis, modem
synchronization (phase, frequency and time) and channel equalization.
Glossary
Filter
calculation procedure that transforms the input signals into
others
Digital ﬁlter
ﬁlter that operates on digital signals
Multirate ﬁlter
digital ﬁlter that contains a mechanism to increase or decrease
the sampling rate while processing input signals
Polyphase ﬁlter
digital ﬁlter partitioned by following Eq. (7.3)
Polyphase channelizer
ﬂexible digital device which can arbitrarily change the sample
rate and the bandwidth of the input signal and can also select
randomly located Nyquist zones
Software radio
radio device in which the digitization of the signal occurs
before the intermediate frequency translation
Cognitive radio
software radio with the capability to adapt its operating param-
eters according to the interactions with the surrounding radio
environment
References
[1] George M. Kranc, Input-output analysis of multirate feedback systems, IRE Trans. Automat. Control AC-2
(1956) 21–28.
[2] E.I. Jury, F.J. Mullin, The analysis of sampled-data control systems with a periodically time-varying sampling
rate, IRE Trans. Automat. Control-4 (1959) 15–20.
[3] E.J. Jury, F.J. Mullin, The analysis of sampled data control system with a periodically time varying sampling
rate, IRE Trans. Automat. Control. AC-4 (1959) 15–21.
[4] P. Vary, U. Heute, A short-time spectrum analyzer with polyphase-network and DFT, Signal Process. 2 (1)
(1980) 55–65.
[5] R.W. Schafer, L.R. Rabiner, Design of digital ﬁlter banks for speech analysis, Bell Syst. Tech. J. 50 (1971)
3097–3115.
[6] F.J. Harris, Multirate Signal Processing for Communication systems, Prentice Hall, Upper Saddle River,
New Jersey, 2004.
[7] F. Harris, W. Lowdermilk, Software deﬁned radio, IEEE Instrum. Meas. Mag. (2010).

422
CHAPTER 7 Multirate Signal Processing for Software Radio Architectures
[8] F. Harris, C. Dick, X. Chen, E. Venosa, Wideband 160 channel polyphase ﬁlter bank cable TV channelizer,
IET Signal Process. (2010).
[9] F. Harris, C. Dick, X. Chen, E. Venosa, M-path channelizer with arbitrary center frequency assignments, in:
WPMC 2010, March 2010.
[10] X. Chen, E. Venosa, F. Harris, Polyphase synthesis ﬁlter bank up-converts unequal channel bandwidths with
arbitrary center frequencies-design II, in: SDR 2010, Washington, DC, December 2010.
[11] E. Venosa, X. Chen, F. Harris, Polyphase analysis ﬁlter bank down-converts unequal channel bandwidths with
arbitrary center frequencies-design II, in: SDR 2010, Washington, DC, December 2010.
[12] M. Bellanger, G. Bonnerot, M. Coudreuse, Digital ﬁltering by polyphase network: application to sample-rate
alteration and ﬁlter bank, IEEE Trans. Acoust. Speech Signal Process. 24 (2) (1976) 109–114.
[13] M. Bellanger, J. Daguet, TDM-FDM transmultiplexer: digital polyphase and FFT, IEEE Trans. Commun. 22
(9) (1974) 1199–1205.
[14] M. Ribey, Exploration of transmultiplexers in telecommunication networks, IEEE Trans. Commun. COM-30
(1982) 1493–1497.
[15] B. Wang, K.J. Ray Liu, Advances in cognitive radio networks: a survey, IEEE J. Sel. Top. Signal Process.
5 (1) (2011).
[16] A. Sahai, S.M. Mishra, R. Tandra, K.A. Woyach, Cognitive radios for spectrum sharing, IEEE Signal Process.
Mag. 26 (1) (2009).
[17] Matthew Sherman, Christian Rodriguez, Ranga Reddy, IEEE standards supporting cognitive radio and net-
works, dynamic spectrum access, and coexistence, IEEE Commun. Mag. 46 (7) (2008).
[18] Jun Ma, G. Ye Li, Biing Hwang (Fred) Juang, Signal processing in cognitive radio, Proc. IEEE 97 (7) (2009).
[19] T. Ulversøy, Software deﬁned radio: challenges and opportunities, IEEE Commun. Surv. 2 (4) (2010).
[20] R. Bagheri, A. Mirzaei, M.E. Heidari, Software-deﬁned radio receiver: dream to reality, IEEE Commun. Mag.
44 (8) (2006).
[21] J. Mitola, Cognitive radio architecture evolution, Proc. IEEE 97 (4) (2009).
[22] R. Tandra, S.M. Mishra, A. Sahai, What is a spectrum hole and what does it take to recognize one? Proc. IEEE
97 (5) (2009).
[23] Fredric J. Harris, On detecting white space spectra for spectral scavenging in cognitive radios, Springer J.
Wireless Pers. Commun. http://dx.doi.org/10.1007/s11277-008-9460-y.
[24] F. Harris, C. Dick, M. Rice, Digital receivers and transmitters using polyphase ﬁlter banks for wireless
communications, Microwave Theory Tech. 51 (4) (2003) 1395–1412 (special issue).

8
CHAPTER
Modern Transform Design for
Practical Audio/Image/Video
Coding Applications
Trac D. Tran
Department of Electrical and Computer Engineering, The Johns Hopkins University, MD, USA
1.8.1 Introduction
With the recent explosion in popularity of the Internet, wireless communication, and portable computing,
the demands for and the interests in digital multimedia (digitized speech, audio, image, video, computer
graphics, and their combination) are growing exponentially. A high-performance ﬁlter bank is typically
at theheart of everystate-of-the-art digital multimediasystem whosecompressionparadigm is illustrated
in Figure 8.1. Current popular international image/video compression standards such as JPEG [1],
MPEG2 [2], and H.263 [3] are all based on the 8 × 8 Discrete Cosine Transform (DCT), an 8-channel
8-tap orthogonal ﬁlter bank with fast computational algorithms based on sparse factorizations. The latest
image compression JPEG2000 standard [4] is employing the wavelet transform, an iterated 2-channel
bi-orthogonal ﬁlter bank, as its de-correlation engine. Of extreme importance is the ability to design a
ﬁlter bank that can fully exploit (i) the statistical properties of a particular signal or class of signals; (ii)
the goals of the applications; and (iii) the computational resources available.
Although the roots of signal transformations can be traced back to works in the 19th century by
prominent mathematicians such as Laplace and Fourier, the exciting development of modern, practical
digital transforms and ﬁlter banks only have a few years of history [5–8] as illustrated in the timeline
of Figure 8.2. These new systems provide more effective tools to represent digital signals not only for
analysis but also for processing and compression purposes. For multimedia signals, representations by
transform coefﬁcients are usually more compact and efﬁcient than the time representations, but are just
as informative. Taking advantage of the normally sparse transform coefﬁcient matrix, we can perform
a majority of signal processing tasks directly in the transform domain at a lower level of computational
complexity.
On the other hand, fast, efﬁcient, and low-cost transforms have played a crucial role in revolutionary
advances throughout the history of digital signal processing. The Discrete Fourier Transform (DFT)
is an excellent signal analysis tool. However, its popularity in practical systems was not widespread
until the discovery of the Fast Fourier Transform (FFT) by Cooley and Tukey in 1965 that reduces the
complexity level from O(n2) to O(n log n) [9]. Another example, the 8 × 8 Discrete Cosine Transform
[10], has a theoretically elegant closed-form expression. It was chosen as the transform-of-choice in
most of current image and video coding standards mainly because it possesses various fast algorithms
[11] that can reduce the transform complexity from 64 multiplications and 56 additions to as low as 5
multiplications and 29 additions per 8 transform coefﬁcients.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00008-5
© 2014 Elsevier Ltd. All rights reserved.
423

424
CHAPTER 8 Modern Transform Design for Practical Audio/Image/Video
FIGURE 8.1
Standard compression paradigm in current digital multimedia systems.
FIGURE 8.2
A historical timeline of signiﬁcant developments in signal decomposition and transformation.
In this chapter, we review the design and development progress of modern transforms for application
in digital image/video coding and processing. These new state-of-the-art transforms provide versa-
tility and ﬂexibility in time-frequency mapping, coding performance, cost of implementation as well
as integration into modern digital multimedia processing/communication systems. We assert that the
most successful philosophy in transform design is to construct highly-complex systems from modular

1.8.2 Background and Fundamentals
425
cascades of similar, simple building blocks, each propagating a set of desired transform properties.
These novel transforms will be designed to have as many of the following features as possible:
•
orthogonal or at least near-orthogonal,
•
symmetric/anti-symmetric (linear-phase) basis functions,
•
good stop-band attenuation,
•
smoothness of the basis functions for perceptually pleasant reconstruction,
•
integer-to-integer mapping capability with exact recovery for a unifying lossless/lossy coding frame-
work,
•
fast computation and efﬁcient implementations in both software and hardware: multiplier-less
property for low-power real-time systems; in-place computation; low memory buffering; VLSI-
friendliness with high modularity and regularity in construction; facilitating region-of-interest cod-
ing/decoding and computational parallelism.
The organization of the chapter is as follows. In Section 1.8.2, we offer a review of important
background materials, concepts, motivations, and previous related works in transform design. Com-
mon design strategy and desirable cost functions are discussed in Section 1.8.3. Next, Section 1.8.4
describes the direct scaling-based design method for modern integer-coefﬁcient transforms. Section
1.8.5 presents a totally different philosophy in transformation design via general parameterization and
construction of polyphase matrices based on lifting steps (also known as ladder structures). The section
also discusses in details the subset of solutions that allows the construction and implementation of
various dyadic-coefﬁcient transforms purely from shift-and-add operations. The design procedure of
spectral factorization of maxﬂat half-band ﬁlters that lead to popular wavelet ﬁlter pairs is described
in Section 1.8.6. Advanced design mainly via optimization with a larger number of channels and/or
longer ﬁlter lengths is covered in Section 1.8.7. Numerous design examples along with current practical
applications will be demonstrated throughout along with discussions on each design’s advantages as
well as disadvantages and other interesting properties. Finally, we brieﬂy summarize the chapter with
a few concluding remarks in Section 1.8.8.
1.8.2 Background and fundamentals
We provide in this section the notation, common background, a brief description of the historical
development of transforms (especially for compression applications), and a discussion on desirable
properties of a state-of-the-art modern transform in multimedia coding and processing.
1.8.2.1 Notation
Let R, Q, and Z denote the sets of real numbers, rational numbers, and integers, respectively. Also,
let D denote the set of dyadic rationals, i.e., all rational numbers that can be represented in the form
of
k
2m where k, m ∈Z. Bold-faced lower case characters are used to denote vectors while bold-faced
upper case characters are used to denote matrices. Let AT , A−1, |A|, and ai j denote respectively the
transpose, the inverse, the determinant, and the ith jth element of the matrix A. Several special matrices
with reserved symbols are: the polyphase matrix of the analysis bank E(z), the polyphase matrix of
the synthesis bank R(z), the identity matrix I, the reversal or anti-diagonal matrix J, the null matrix

426
CHAPTER 8 Modern Transform Design for Practical Audio/Image/Video
(or vector) 0, the unity vector 1, the permutation matrix P, and the diagonal matrix D. The letter M is
usually reserved for the number of channels of the ﬁlter bank or the size of the transform. Finally, we
denotetheℓp normofanN-pointvectorx as∥x∥p =
N−1
i=0 |xi|p1/p
wheretheℓ2 norm∥x∥2 =
√
xT x
is of special interest.
1.8.2.2 Transform fundamentals
A transform is most often thought of as a linear mapping which can be carried out by a simple matrix
multiplication. A block transform TA of size M is simply an M × M scalar matrix, mapping the
signal vector x to its corresponding transform coefﬁcients c. Here, the subscript A is used to denote the
analysis transform often employed in an encoder to analyze or to de-correlate input data samples. At the
decoder, we need the inverse operator TS to recover x back where the subscript S denotes the synthesis
or signal reconstruction operation. The series of mappings are depicted in Figure 8.3. The operation Q
in the middle of Figure 8.3 represents either approximation (in the form of quantization for instance),
processing, coding, communication or any combination thereof. Obviously, if the coefﬁcients can be
recovered losslessly or ˆc = c, we simply require TS to be the exact inverse of TA to reconstruct the
input signal perfectly.
The columns φi of TT
A are often called the analysis basis functions while the columns ˆφi of TS are
called the synthesis basis functions. TA is said to be an invertible transform or a transform that has
perfect reconstruction or bi-orthogonal when TSTA = TATS = I. In the special case that the synthesis
transform has real coefﬁcients and is simply the transpose of the analysis transform TS = T−1
A = TT
A,
then the transform TA is said to be orthogonal. In this case, we can conceptually think of representing
the N-point input signal as
x = c0φ0 + c1φ1 + · · · + cN−1φN−1 =
N−1

i=0
ciφi,
where ci = ⟨φi, x⟩.
(8.1)
FIGURE 8.3
Signal decomposition and reconstruction as matrix operations.

1.8.2 Background and Fundamentals
427
However, our interest is to design the transform that yields the sparsest and most compact representation
x =

i∈S;|S|=K≪N
ciφi.
(8.2)
In (8.2) above, the set of signiﬁcant coefﬁcients are indexed by the set S whose cardinality K is much
smaller than the signal dimension N.
For two-dimensional signals such as still images or frames in video sequences, we typically employ
the separable transformation approach where all rows are transformed and then all columns are trans-
formed or vice versa. Mathematically, if we let X be the image of interest in the form of a matrix, then
the transform coefﬁcients C of X can be computed as
C = TAXTT
A,
(8.3)
while the inverse synthesis mapping can be computed as
X = TSCTT
S .
(8.4)
Note that the computation order of rows or columns in 8.3 and 8.4 is not critical and the strategy can be
straightforwardly extended to signals of any higher dimension.
1.8.2.3 Optimal orthogonal transform
Obtaining the sparsest representation as in (8.2) above is certainly a signal-dependent task. Let c be
the coefﬁcients of the input signal x, obtained from a certain linear transformation T. Let us further
assume that the input signal x can be modeled as a white-sense stationary stochastic process. Then, the
correlation matrix of the output signal c is
Rcc = E{TxTT } = TRxxTT .
The optimal orthogonal linear transform T here is the one that can fully de-correlate x. In order to
achieve this, Rcc must be a diagonal matrix with the eigenvalues of Rxx being its diagonal entries, and
each row of T being an eigenvector of Rxx. This optimal transform is often called the Karhunen-Loève
Transform (KLT) of signal x, also known as principal component analysis (PCA) or the Hotelling
transform. Obviously, the KLT depends on the auto-correlation matrix Rxx. Thus it is signal dependent,
computationally expensive, and costly to communicate to the decoder if the signal of interest is not
stationary. To ﬁnd fast algorithms and signal-independent transforms for practical applications, a typical
approach is to start with a simpliﬁed stationary signal model, try to ﬁnd the approximation of its KLT,
and then develop fast computational algorithms of the resulting approximation. The Discrete Cosine
Transform (DCT) is developed following this approach [10,11].
1.8.2.4 Popular transforms in signal processing: DFT, WHT, DCT
The problem of transform design has undergone several important evolutions. In the beginning, trans-
formation is mainly thought of as a continuous-time continuous-amplitude change of basis operation

428
CHAPTER 8 Modern Transform Design for Practical Audio/Image/Video
(and often in inﬁnite-dimension functional space) to solve complicated set of equations. Discrete-time
transforms are often obtained from sampling the continuous-time version. Four popular discrete-time
transforms in signal processing are the Discrete Fourier Transform (DFT), the Walsh-Hadamard Trans-
form (WHT), the type-II Discrete Cosine Transform (DCT) and the type-IV Discrete Cosine Transform
(DCT), shown below in the matrix representation format respectively.

F

=
1
√
M

W mn
M

; 0 ≤m, n ≤M −1; WM = e−j 2π
M .
(8.5)
H2 =
1
1
1
−1
	
;
H2m = H2 ⊗H2m−1 =
 H2m−1
H2m−1
H2m−1
−H2m−1
	
; m ∈Z, m ≥2.
(8.6)

CI I
M

=

2
M

Km cos
m(n + 1/2)π
M
	
; 0 ≤m, n ≤M −1; Ki =
 1
√
2
i = 0
1
i > 0 .
(8.7)

CI V
M

=

2
M

cos
(m + 1/2)(n + 1/2)π
M
	
; 0 ≤m, n ≤M −1.
(8.8)
Out of the three mentioned above, only the Hadamard transform has integer coefﬁcients. Unfortu-
nately, it is too simplistic to offer reasonable coding performance. On the other hand, the DCT offers
very good compression performance but it has irrational coefﬁcients. The DFT is not only irrational,
but it also has complex-valued coefﬁcients.
1.8.2.5 The ﬁlter bank connection
A typical discrete-time, maximally-decimated M-channel ﬁlter bank is depicted in Figure 8.4. At
the analysis stage, the input signal x[n] is passed through a bank of M analysis ﬁlters Hi(z), i =
0, 1, . . . , M −1, each of which preserves a frequency band. These M ﬁltered signals are then decimated
by M to preserve the system’s overall sampling rate. The resulting subband signals can be coded, pro-
cessed, and/or transmitted independently or jointly. At the synthesis stage, the subbands are combined
by a set of upsamplers and M synthesis ﬁlters Fi(z), i = 0, 1, . . . , M −1, to form the reconstructed
signal ˆx[n]. In the absence of processing errors (e.g., quantization), ﬁlter banks that yield the output
ˆx[n] as a pure delayed version of the input x[n], i.e., ˆx[n] = x[n −n0], are called perfect reconstruction
ﬁlter banks. From a traditional transform perspective, the analysis ﬁlter hi[n] is the ith analysis basis
function φi. Similarly, the synthesis ﬁlter fi[n] yields the ith synthesis basis function ˆφi.
The ﬁlter bank in Figure 8.4a can also be represented in terms of its polyphase matrices as shown in
Figure 8.4b. E(z) is the analysis bank’s polyphase matrix and R(z) is the synthesis bank’s polyphase
matrix.NotethatbothE(z)andR(z)are M×M matriceswhoseelementsarepolynomialsinz[7].IfE(z)
is invertible with a minimum-phase determinant (stable inverse), one can obtain perfect reconstruction
by simply choosing R(z) = E−1(z). In other words, any choice of R(z) and E(z) that satisﬁes
R(z)E(z) = E(z)R(z) = z−lI,
l ≥0
(8.9)
yields perfect reconstruction. Since ﬁlter banks with ﬁnite impulse response (FIR) ﬁlters are very
valuable in practice, we can restrict the determinants of both polyphase matrices need to be monomials

1.8.2 Background and Fundamentals
429
(a)
(b)
FIGURE 8.4
M-channel ﬁlter bank. (a) Conventional representation. (b) Polyphase representation.
[6–8] as well:
|R(z)| = z−m
and |E(z)| = z−n m, n ∈Z.
(8.10)
A popular choice of R(z) yielding paraunitary or orthogonal systems is
R(z) = z−K ET (z−1),
(8.11)
where K is the order of a properly-designed E(z). In the case where E(z) may not be paraunitary but
(8.9) still holds, the ﬁlter bank is said to be bi-orthogonal. Now, the design of a complicated M-band
FIR perfect reconstruction ﬁlter bank is reduced to choosing appropriate polynomial matrices E(z) and
R(z) such that (8.9) and (8.10) are satisﬁed.

430
CHAPTER 8 Modern Transform Design for Practical Audio/Image/Video
When the ﬁlter length equals the number of channel M, the polyphase matrices E(z) and R(z) become
scalar matrices of zero order and they become equivalent to the conventional transforms above, i.e.,
E(z) = TA and R(z) = TS. When the ﬁlter length is higher than the number of channel, we have to
deal with matrices whose entries are polynomials in z. In the wavelet transform, M = 2 and we almost
never have a scalar polyphase matrix, except the Haar wavelet. When M equals the support of the input
signal, we have a global transform and maximum frequency resolution becomes achievable. On the
other hand, when M is much smaller than the input size (for example, most imaging applications have
the transform size of 16 × 16 or less), we have a block transform coding framework where the down-
samplers and the delay chain on the left of Figure 8.4b serve as serial-to-parallel blocking mechanism
whereas the up-samplers and the delay (or advance) chain on the right of Figure 8.4b implement the
parallel-to-serial unblocking mechanism. In this case, the global transform matrices TA and TS have
block-diagonal structure and maximum time/space resolution becomes achievable.
1.8.2.6 The lapped transform connection
The lapped transform (LT) is deﬁned as a linear transformation that partitions the input signal into
small overlapped blocks and then processes each block independently. The equivalence between the
lapped transform, the ﬁlter bank in Section 1.8.2.5, as well as the general linear transformation in
Section 1.8.2.2 has been well-established in [12].
Consider the M-channel perfect-reconstruction ﬁlter bank with FIR ﬁlters, all of length L = K M for
presentation elegance. The polyphase matrix E(z) as shown in Figure 8.4b is of order K −1 and can be
expressed as E(z) = K−1
i=0 Eiz−1. Similarly, the synthesis polyphase matrix R(z) can be represented
as R(z) = K−1
i=0 Riz−1. With J as the time-reversal matrix, if we deﬁne
E = [EK−1J · · · E1J E0J] and R =
⎡
⎢⎢⎢⎣
JR0
JR1
...
JRK−1
⎤
⎥⎥⎥⎦,
then the global transform matrices can be shown to take the following forms
TA =
⎡
⎢⎢⎢⎢⎢⎢⎣
...
0
E
E
E
0
...
⎤
⎥⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎢⎣
...
...
...
0
EK−1J
EK−2J
· · ·
E0J
EK−1J
EK−2J
· · ·
E0J
EK−1J
EK−2J
· · ·
E0J
0
...
...
...
⎤
⎥⎥⎥⎥⎥⎥⎦
(8.12)

1.8.3 Design Strategy
431
and
TS =
⎡
⎢⎢⎢⎢⎢⎢⎣
...
0
R
R
R
0
...
⎤
⎥⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
...
0
...
JR0
JR1
JR0
...
JR1
JR0
JRK−1
...
JR1
JRK−1
...
JRK−1
...
0
...
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(8.13)
Although the blocking windows overlap, the total number of produced transform coefﬁcients is
the same as the total number of input samples. Hence, the lapped transform does not introduce any
redundancy. In traditional block-transform processing, the number of input samples from each block is
the same as the number of basis functions, i.e., L = M. In this case, the transform matrix P becomes
square (M × M) and there is no overlapping between neighboring blocks. However, as the basis vectors
of block transforms do not overlap, there may be discontinuities along the boundary regions of the blocks
when heavy quantization is involved. Different approximations of those boundary regions in each side
of the border may cause an artiﬁcial “edge” in between blocks. This is the so-called blocking effect.
Allowing overlapping block boundary leads to coding improvement over non-overlapped transforms
such as the DCT on two counts: (i) from the analysis viewpoint, the overlapping basis takes into account
inter-block correlation, hence, provides better energy compaction; (ii) from the synthesis viewpoint, the
overlapping basis either eliminates or signiﬁcantly reduces blocking discontinuities.
However, the global transform matrices TA and TS remain to be very sparse, leading to fast computa-
tion of transform coefﬁcients c. The philosophy is that a few borrowed samples are enough to eliminate
blocking artifacts and to improve coding performance.
1.8.3 Design strategy
All of the transforms presented in the previous sections are designed to have high practical value. They all
have perfect reconstruction. Some of them even have real and symmetric basis functions. However, for
the transforms to achieve high coding performance, several other properties are also needed. Transforms
can be obtained using unconstrained nonlinear optimization where some of the popular cost criteria are:

432
CHAPTER 8 Modern Transform Design for Practical Audio/Image/Video
codinggainCCG,DCleakageCDC,attenuationaroundmirrorfrequenciesCM,andstop-bandattenuation
in both analysis and synthesis bank—CA and CS. In the particular ﬁeld of image compression, all of
these criteria are well-known desired properties in yielding the best reconstructed image quality [6].
The cost function in the optimization process can be a weighted linear combination of these measures
as follows
COverall = α1CCG + α2CDC + α3CM + α4CA + α5CS,
(8.14)
where each of the individual cost function will be further elaborated in the next section.
1.8.3.1 Desirable transform properties
•
Symmetry: Jφi = ±φi. Linear-phase basis functions are critical in image/video processing appli-
cations (but not so much in audio processing applications).
•
Orthogonality: offers mathematically elegance and norm preservation property. However, practical
codecs often employ bi-orthogonal transforms (e.g., the 9/7-tap wavelet in JPEG2000). In other
words, the relaxation of the tight orthogonal constraint can sometimes provide a much needed
ﬂexibility in transform design.
•
Energy compaction: also known as coding gain
CCG = 10 × log10
⎛
⎜⎝
σ 2
input

M
i=1 σ 2
i ∥ˆφi∥2
1/M
⎞
⎟⎠,
(8.15)
where σ 2
input is the variance of the input signal; σ 2
i is the variance of the ith subband (generated
from the ith analysis basis function φi); and ∥ˆφi∥2 is the norm-squared of the ith synthesis basis
function. Note that for orthogonal transforms, this term disappears and Eq. 8.15 reduces down to
the ratio (in dB) of the arithmetic mean over the geometric mean of the sub-band variances. The
signal model is the commonly used AR(1) process with intersample autocorrelation coefﬁcient
ρ = 0.95. As aforementioned, the coding gain can be thought of as an approximate measure
of the transform’s energy compaction capability. Among the listed criteria, higher coding gain
correlates most consistently with higher objective performance (measured in MSE or PSNR) in
coding applications. Transforms with higher coding gain compact more signal energy into a fewer
number of coefﬁcients, leading to a sparser, more compact, representation and more efﬁcient entropy
coding.
•
Stop-band attenuation: deﬁned as the summation of the stop-band energy of all analysis and/or
synthesis basis functions. It can be formulated as
Analysis stop-band attenuation CA =
M−1

i=0

ω∈i
|φi(e jω)|2dω,
(8.16)
Synthesis stop-band attenuation CS =
M−1

i=0

ω∈i
| ˆφi(e jω)|2dω,
(8.17)

1.8.4 Approximation Approach via Direct Scaling
433
where i denotes the stop-band of the ith basis function.
Stop-band attenuation is a classical performance criterion in ﬁlter design. On the analysis side, the
stop-band attenuation cost helps in improving the signal decorrelation and decreasing the amount
of aliasing. In meaningful images, we know a priori that most of the energy is concentrated in the
low frequency region. Hence, besides the few basis functions that are designated to cover this low
frequency range, high stop-band attenuation for the remaining in this part of the frequency spectrum
becomes extremely desirable. On the contrary, at the synthesis side, the synthesis basis functions (or
ﬁlters) covering low-frequency bands need to have high stop-band attenuation near and/or at ω = π
to enhance their smoothness.
•
Zero DC leakage: The DC leakage cost function measures the amount of DC energy that leaks out
to the bandpass and highpass subbands. The main idea is to concentrate all signal energy at DC
into the DC coefﬁcients. This proves to be advantageous in both signal decorrelation and in the
prevention of discontinuities in the reconstructed signals. Low DC leakage can prevent the annoying
checkerboard artifact that usually occurs when high frequency bands are severely quantized. Zero
DC leakage is equivalent to attaining one vanishing moment in wavelet design (a necessary condition
for the convergence of the wavelet construction): ⟨φi, 1⟩= 0; ∀i ̸= 0. Depending on whether the
focus is on the transform coefﬁcients or the ﬁlter bank coefﬁcients, it can be stated as
CDC =

i̸=0
⟨φi, 1⟩or CDC =
M−1

i=1
L−1

n=0
hi[n].
(8.18)
•
Attenuation at mirror frequencies: The mirror frequency cost function is a generalization of CDC
above. The concern is now at every aliasing frequencies ωm = 2mπ
M ; m ∈Z, 1 ≤m ≤M
2 , instead of
just at DC. Frequency attenuation at mirror frequencies are very important in the further reduction of
blocking artifacts: the ﬁlter responses (except the only high-pass ﬁlter in the system) should vanish
at these mirror frequencies as well. The corresponding cost function is:
CM =
M−2

i=0
|Hi(e jωm)|2;
ωm = 2mπ
M , m ∈Z, 1 ≤m ≤M
2 .
(8.19)
Low DC leakage and high attenuation near the mirror frequencies are not as essential to the coder’s
objective performance as coding gain. However, they do improve the visual quality of the recon-
structed signal signiﬁcantly.
•
Integer-to-integer mapping with exact invertibility and tight dynamic range extension: these two
properties are critical for lossless coding applications.
•
Practical considerations: integer or dyadic-rational coefﬁcients; hardware and software friendly;
low latency; small bus-width; parallelizability; structural regularity; low memory buffering; and
in-place computation.
1.8.4 Approximation approach via direct scaling
The simplest approach to design an integer transform is to approximate a well-known inﬁnite-precision
(represented using ﬂoating-point precision) transform by a close ﬁnite-precision (ﬁxed-point) version.

434
CHAPTER 8 Modern Transform Design for Practical Audio/Image/Video
For instance, an integer-coefﬁcient transform TM can be obtained from a well-known irrational-
coefﬁcient transform CM as follows
TM

α, CM

= round

αCM

,
(8.20)
where α is a parameter to control the precision of the approximation. Obviously, the larger α is, the
closer the approximation gets along with a higher implementation cost and a larger output dynamic
range.
1.8.4.1 H.264 4 × 4 Transform design
The most well-known integer transform successfully designed from this scaling approach is the 4-point
bit-exact DCT approximation in the latest video coding international standard H.264 or MPEG-4 Part
10 [13,14]. In this particular example, the target transform is the 4-point type-II DCT in (8.7), which
can be expressed as follows
C4 =
⎡
⎢⎢⎣
1
1
1
1
c
s
−s
−c
1
−1
−1
1
s
−c
c
−s
⎤
⎥⎥⎦; c =
√
2 cos π
8 ,
s =
√
2 sin π
8 .
(8.21)
In the earlier design, the approximation T4 transform is designed with the scaling parameter α = 26
yielding the following integer version
T4(26) = round

26C4

=
⎡
⎢⎢⎣
13
13
13
13
17
7
−7
−17
13
−13
−13
13
7
−17
17
−7
⎤
⎥⎥⎦,
(8.22)
which happens to retain symmetry, orthogonality, as well as the coding gain of the DCT. The only
drawback here is the extended dynamic range which requires the transformation stage to be implemented
on a 32-bit architecture. The standardization committee later selected the following integer transform,
proposed independently by Microsoft and Nokia [14]
T4
5
2

= round
5
2C4

=
⎡
⎢⎢⎣
1
1
1
1
2
1
−1
−2
1
−1
−1
1
1
−2
2
−1
⎤
⎥⎥⎦,
(8.23)
whose inverse turns out to be
T−1
4
5
2

= 1
20
⎡
⎢⎢⎣
5
4
5
2
5
2
−5
−4
5
−2
−5
4
5
−4
5
−2
⎤
⎥⎥⎦.
(8.24)

1.8.4 Approximation Approach via Direct Scaling
435
FIGURE 8.5
Fast implementation of the H.264 transform (left) and inverse transform (right). No multiplications are
needed, only binary additions and shifts.
The current transform in H.264 is designed with the scaling parameter α = 2.5 (actually, any
parameter in the range 2.3 ≤α < 3 also yields exactly the same result). Its coding gain as deﬁned in
(8.15) is only 0.02 dB lower than the original DCs. Similar to the earlier design, this transform retains
symmetry as well as orthogonality, and offers an efﬁcient multiplier-less implementation as depicted in
Figure 8.5. Most importantly, the lower dynamic range allows the entire transformation stage to ﬁt within
a 16-bit architecture. It is interesting to note that when the binary shifts of 2 and 1
2 in the structures of
Figure 8.5 are removed, we are left with the Walsh-Hadamard transform H4 as deﬁned in (8.6). Despite
the important improvements, several drawbacks still exist: (i) it is not an integer-to-integer transform
with exact invertibility (its inverse involves a division by 5); and (ii) its dynamic range control is still
not tight enough. Hence, H.264 loses the capability of lossy and lossless coding based on the same
compression framework.
1.8.4.2 Integer DCT design via the principle of dyadic symmetry
The direct scaling approach is actually studied extensively by Cham, whose focus is on the two cases
M = 8 and M = 16 [15–17]. Chams approach can be characterized as direct scaling with orthogonal
constraint. In other words, since there is no guarantee that scaling retains the orthogonality property,
we seek additional condition(s) on the integer transform coefﬁcients such that the scaled-up integer
transform is always orthogonal. Let us illustrate this design procedure with the M = 8 case, in which
the free integer parameters come from the set of {a, b, c, d, e, f }.
ICT8 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
1
1
1
1
1
1
1
a
b
c
d
−d
−c
−b
−a
e
f
−f
−e
−e
−f
f
e
1
−1
−1
1
1
−1
−1
1
c
−a
d
b
−b
−d
a
−c
f
−e
e
−f
−f
e
−e
f
d
−c
b
−a
a
−b
c
−d
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(8.25)
In this particular case, it can be proven that the only condition they have to satisfy in order for the
resulting integer cosine transform (ICT) to be orthogonal [15] is
ab = ac + bd + cd.
(8.26)

436
CHAPTER 8 Modern Transform Design for Practical Audio/Image/Video
An exhaustive search, where the cost function is set to be the transform coding gain CCG as deﬁned
in (8.15), up to a certain maximum resolution is performed to identify a rather large family of ICTs.
One example of a parameter set that yields a high-performance ICT is a = 230, b = 201, c = 134,
d = 46, e = 3, and f = 1.
1.8.4.3 Direct scaling of a rotation angle
It is worth noting that although direct scaling of a given orthogonal matrix does not generally retain
orthogonality, it always holds in the 2 × 2 case. In this special case, any orthogonal matrix can be
characterized by a single rotation angle, and it is easy to see that orthogonality is robust under direct
scaling. For any rotation angle Rθ, the direct scaling method produces the approximation
ˆRθ = round

αRθ

= round

α
 cos θ
sin θ
−sin θ
cos θ
	
=
 ˆc
ˆs
−ˆs
ˆc
	
(8.27)
that always maintain orthogonality regardless of the scaling parameter choice:
ˆRT
θ ˆRθ =
 ˆc
−ˆs
ˆs
ˆc
	  ˆc
ˆs
−ˆs
ˆc
	
=
 ˆc2 + ˆs2
0
0
ˆc2 + ˆs2
	
.
(8.28)
The observation above leads to the following systematic integer-approximating design procedure
which does not require complicated orthogonality constraints on the parameter set:
•
Obtain a rotation-based sparse factorization of the target transform.
•
Apply direct scaling to each rotation.
Note that each rotation angle can be approximated independently with a different degree of accuracy.
This independent approximation approach above is actually very well-known in practical ﬁxed-point
implementations. Most ﬁxed-point IDCT structures are obtained following this procedure [18].
The direct scaling design approach in this section offers a few practical advantages:
•
Several desirable properties of the original transform can be robustly retained (most importantly,
symmetry, orthogonality, and high coding gain).
•
The design procedure leads to efﬁcient integer implementations that are often equivalent to ﬁxed-
point hardware implementations.
However, this approach is only applicable to transform approximation, not construction of new ones.
Furthermore, a few disadvantages still remains in this ad-hoc trial-and-error method:
•
There is in general no exact inverse (not useful for lossless coding).
•
In fact, the inverse operation often involves division operations.
•
It is difﬁcult to control the dynamic range expansion and to apply when the transform size/complexity
increases.
1.8.5 Approximation approach via structural design
The structural design approach seeks to capture structurally as many desirable properties as possible
and then optimize the parameters of the structure for high coding gain (or any other desirable property

1.8.5 Approximation Approach via Structural Design
437
for the targeted application). In other words, we use basic low-order building blocks (such as butterﬂies,
lifting steps, scaling factors, delay elements) to construct much more complicated high-order transforms
with certain predetermined properties (such as integer-mapping, perfect invertibility, symmetry). This
design approach can be applied to transform approximation as well as new transform construction.
Integer- or dyadic-rational-coefﬁcient transforms can be easily designed by approximating the optimal
structural parameters by (dyadic) rationals.
1.8.5.1 Lifting step
One critical component in modern transform design is the lifting step [19], which is simply an elementary
upper-triangular or a lower-triangular matrix with unity diagonal elements, also known as a shear
operation in matrix theory or the ladder structure in signal processing literature [20]. Figure 8.6 depicts
a typical lifting step on the left and its corresponding inverse on the right. The upper-triangular matrix
as shown in Figure 8.6 is often referred to as the update lifting step while the lower-triangular matrix
is called the prediction lifting step in the literature. The operator P in Figure 8.6 is very general as
far as perfect reconstruction is concerned: P can certainly be any scalar value; P actually can be any
polynomial with delays (as popularized in fast wavelet transform implementation); in fact, P can even
be any non-linear operator (to generate morphological wavelet for instance). Invertibility is structurally
guaranteed since to invert a lifting step, we only need to subtract out (add in) what has been added in
(subtracted out) at the forward transform.
One advantage that the lifting scheme offers is the versatility and the simplicity in constructing fast
transforms that can map integers to integers. If a ﬂoor (or round, or ceiling, or any other quantization)
operator is placed after P in the lifting step in Figure 8.6, the transformation can now map integers to
integers with perfect reconstruction regardless of the lifting step has integer coefﬁcients or not. This
property can be conﬁrmed through a simple inspection as demonstrated in Figure 8.7.
FIGURE 8.6
A lifting step (left) and its corresponding inverse (right).
FIGURE 8.7
Integer-to-integer mapping with exact invertibility of a lifting step.

438
CHAPTER 8 Modern Transform Design for Practical Audio/Image/Video
FIGURE 8.8
Lifting step with (a) dyadic rational coefﬁcient and (b) the resulting multiplier-less implementation.
Moreover, if the lifting step is chosen to be dyadic, the nonlinear operation can be incorporated
into the division using binary shift as illustrated in Figure 8.8. A scalar lifting step of value
k
2m can
be implemented as a multiplication by k followed by a division by 2m. Division by 2m followed by a
truncation is equivalent to a binary right-shift by m places. The numerator k can be easily implemented
using bit shift and add operations as well, or we can split the fraction
k
2m into a cascade of pure
power-of-two lifting steps, i.e.,
k
2m = 
i
1
2i . The latter approach leads to an implementation with
solely binary right-shifts, preventing the bit-depth expansion in intermediate results. With all scaling
factors set to unity, multiplier-less transforms can be easily constructed via a cascade of multiple lifting
steps. If the scaling factors are powers of two, we can still retain the multiplier-less feature on both
analysis and synthesis side. Besides the integer-friendly property, the lifting scheme also offers several
other interesting advantages: simplicity of wavelet design on irregular intervals, in-place computation,
connection to spatial prediction, and capability of incorporating nonlinear ﬁltering.
It is quite straightforward to extend the lifting result above to the higher dimension case of any
arbitrary M × M matrix since it is well-known from the LDU matrix factorization that every M × M
invertible matrix V can be completely characterized by M(M −1) elementary matrices, M diagonal
scaling factors, and a permutation matrix. In other words, any invertible matrix V can be factorized as
V = PLDU where the upper-triangular matrix U can be constructed from M(M−1)
2
elementary update
lifting steps labeled ui j, the lower-triangular matrix V can be constructed from M(M−1)
2
elementary
prediction lifting steps labeled pi j, and the diagonal matrix D contains the scaling factors αi. The
construction of V as described above is depicted in Figure 8.9. Note that the parameterization can be
accomplished with rotation angles and diagonal scaling factors as well (see Figure 8.10).
FIGURE 8.9
General parameterization of any invertible matrix via lifting steps.

1.8.5 Approximation Approach via Structural Design
439
FIGURE 8.10
General parameterization of any invertible matrix via rotation angles.
FIGURE 8.11
Representation of a rotation angle via either 3 lifting steps or 2 lifting steps and 2 scaling factors.
1.8.5.2 Lifting-based approximation
Lifting steps are also very convenient in transform approximation as well. Figure 8.11 shows the
equivalent representation of the popular rotation angle by 3 lifting steps or 2 lifting steps and 2 scaling
factors. Sometimes, the latter representation can lead to a more efﬁcient approximation since the 2
scaling factors can be absorbed into the quantization stage. Figure 8.12 demonstrates a systematic
approach to rotation approximation via multiplier-free lifting:
•
ﬁrst compute the theoretical values of the lifting steps,
•
obtain dyadic-rational approximating values from theoretical ones (the resolution of the approxi-
mating values is the trade-off between computational complexity and coding performance),
•
obtain the multiplier-less implementation from the dyadic-rational lifting steps.
Since most existing fast transform algorithms are obtained from sparse factorization of the transform
matrix as a cascade of rotation angles, we can systematically break down the design procedure one
rotation independently at a time. An example of a successful application of this lifting-based approxi-
mation approach is the design of multiplier-less IDCT for video coding in an effort to end the drifting
effects from IDCT mismatch. Liu et al shows that standard-compliance IDCT can be achieved on a
16-bit architecture whereas on a 32-bit architecture, approximation via lifting and butterﬂy can be so
accurate that the reconstructed video sequences are virtually drifting-free when pairing with a 64-bit
ﬂoating-point IDCT [21].
1.8.5.3 Lossless color transform design
We demonstrate in this section the simplest design example based on lifting construction for M >
2 −3 × 3 color transforms. Figure 8.13 depicts two different designs developed independently and
submitted to the JVT standardization committee at the same time: the RGB-to-YCoCg transform

440
CHAPTER 8 Modern Transform Design for Practical Audio/Image/Video
FIGURE 8.12
Example of a rotation approximation by dyadic-rational lifting steps.
FIGURE 8.13
Multiplierless 3 × 3 color transforms with exact invertibility.
(on the left of Figure 8.13) by Malvar et al. [22] and the RGB-to-YFbFr transform (on the right of
Figure 8.13) by Topiwala et al. [23].
It is interesting to note that two designs are remarkably similar since they are both designed using our
lifting approximation framework described in the previous sections. First, the optimal KLT is obtained
where the auto-correlation matrix is derived from a set of high-quality test color images acquired by
Kodak. Lifting-based approximation is then applied to obtain the dyadic lifting coefﬁcients. Practical
experiments show that both transforms de-correlate the three color planes more efﬁciently than existing

1.8.5 Approximation Approach via Structural Design
441
popular color transforms such as the RGB-to-YUV and the RGB-to-YCbCr transform while additionally
providing the pivotal feature of lossless integer mapping with low dynamic range (there is only a 1-bit
expansion on the two chrominance components).
Finally, both transforms only utilize four lifting steps (instead of theoretically six) without any
diagonal scaling. At the cost of one more addition and one more bit-shift operation per luminance
sample, the RGB-to-YFbFr transform attains a coding gain of 4.812 dB while the RGB-to-YCoCg
transform achieves CCG
=
4.619
dB (the optimal KLT in this case has a coding gain of
4.966 dB).
1.8.5.4 Integer DCT design
The DCT is the most widely used transform in current international image/video compression standards.
This 4-point dyadic-coefﬁcient design example is a close approximation of the 4-point DCT and it
follows the DCT’s factorization very closely. The resulting structure is depicted in Figure 8.14.
In the sparse 4-point DCT factorization case, there are three butterﬂies and only one irrational rotation
angle of π
8 [24]. We can employ only 2 lifting steps for this rotation angle (since the scaling factors
associated with the 2-lifting-step structure can be combined with the quantization step sizes to save
computational complexity further) while keeping the butterﬂies intact. Many different transforms can
be represented using this structure the difference between them lies only in the parameter choices as
demonstrated below:
•
u = 1; p = 1/2: essentially the Walsh-Hadamard transform,
•
u = 7/16; p = 3/8: Liang et al. submission to H.264 [25,26],
•
u = 1/2; p = 2/5: the current 4-point H.264 transform (the last 2 basis functions are off by 2, 5/2
scaling),
•
u = 1/2; p = 1/2:thecurrent4-pointPhotoCoreTransform(PCT)inHD-Photo/JPEG-XR[27,28].
It is interesting to note that the last two options yield two transforms with identical coding gain of
7.55 dB. The lifting implementation allows the PCT [27,28] to map integers to integers losslessly and
to have a much tighter dynamic range control comparing to the rotation-based implementation of H.264
transform (this is where H.264 loses its lossless coding capability). The second option with u = 7/16
FIGURE 8.14
4 × 4 integer DCT structure.

442
CHAPTER 8 Modern Transform Design for Practical Audio/Image/Video
and p = 3/8 essentially achieves the same coding gain as the theoretical 4-point DCT (7.57 dB for
AR(1) signal model with ρ = 0.95). Note that all four options allow 16-bit implementation with 9-bit
input data.
Integer DCTs of larger sizes (M > 4) can be easily designed using the same procedure. The rotation-
factorized representation of the transform of interest is ﬁrst obtained, then each rotation angle can be
approximated methodically as demonstrated in Section 1.8.5.2. With various degrees of approximation
accuracy (more accuracy obviously requires more complexity), these integer DCT can be tuned to cover
thegapbetweentheWalsh-HadamardtransformandtheoriginalDCT.Thecorrespondingsolutionsoften
allows a 16-bit implementation, enables lossless compression, and maintains satisfactory compatibility
with the ﬂoating-point DCT. The 8 × 8 and 16 × 16 integer DCT are depicted in Figures 8.15 and 8.16,
respectively.
Table 8.1 lists the analytical values of all the lifting parameters and some conﬁgurations of the integer
DCT family (called binary DCT or binDCT in short) as shown in Figure 8.15. The dyadic lifting values
are obtained by truncating or rounding the corresponding analytical values with different accuracy
levels. Cg(8) and Cg(4) are the coding gains of these 8 × 8 binDCTs and the 4 × 4 binDCTs embedded
FIGURE 8.15
8 × 8 integer DCT.

1.8.5 Approximation Approach via Structural Design
443
FIGURE 8.16
16 × 16 integer DCT.
in them. Table 8.1 also shows the MSE between the binDCT output and the true DCT output for the
popular 8 × 8 case. The MSE here is the expected ℓ2-norm of the error in the produced coefﬁcients and
it is computed as follows. Assume that C is the true M × M DCT, and ˆC is its approximated integer
version. For an input column vector x, the difference between the DCT coefﬁcients and the binDCT
coefﬁcients is:
e = Cx −ˆCx = (C −ˆC)x ≜Dx
(8.29)
and the MSE of each coefﬁcient is
ϵ ≜1
M E[eT e] = 1
M trace{eeT } = 1
M trace{DRxxDT },
(8.30)
where Rxx ≜E[xxT] is the autocorrelation matrix of the input signal. The low MSE level as depicted
in Table 8.1 indicate that the integer transform approximation is very accurate. The interested reader
is referred to [21,26] for more details on the design of this particular family of multiplier-less cosine
transforms.

444
CHAPTER 8 Modern Transform Design for Practical Audio/Image/Video
Table 8.1 Different Conﬁgurations of the 8 × 8 Integer DCT in Figure 8.15
Floating-point
binDCT-C1
C2
C3
C4
C5
p1
0.4142135623
13/32
7/16
3/8
1/2
1/2
u1
0.3535533905
11/32
3/8
3/8
3/8
1/2
p2
0.6681786379
11/16
5/8
7/8
7/8
1
u2
0.4619397662
15/32
7/16
1/2
1/2
1/2
p3
0.1989123673
3/16
3/16
3/16
3/16
1/4
u3
0.1913417161
3/16
3/16
3/16
1/4
1/4
p4
0.4142135623
13/32
7/16
7/16
7/16
1/2
u4
0.7071067811
11/16
11/16
11/16
3/4
3/4
p5
0.4142135623
13/32
3/8
3/8
3/8
1/2
Shifts
–
27
23
21
18
13
Adds
–
42
37
36
33
28
MSE
–
1.1E−5
8.5E−5
4.2E−4
5.8E−4
2.3E−3
Cg(8) (dB)
–
8.8251
8.8220
8.8159
8.8033
8.7686
Cg(4) (dB)
–
7.5697
7.5697
7.5566
7.5493
7.5485
1.8.5.5 Lifting for complex-coefﬁcient transforms
So far, we have focused on real-coefﬁcient transforms since they are mostly employed in image/video
coding applications. For complex-coefﬁcient transforms such as the FFT, it turns out that lifting steps
can be as easily applied as in the real-coefﬁcient case. Figure 8.17 illustrates how to model a complex
multiplication operation by a rotation angle and the resulting equivalent lifting representation [20,29].
Each lifting step in Figure 8.17 can then be approximated by a dyadic-rational value as previously
shown. One particularly nice feature of the FFT is its regularity in factorized format: its implementation
is recursive and all values of rotation angles involved are easily computed. Hence, complex-coefﬁcient
transforms such as the FFT can be easily constructed or approximated in multiplier-less fashion as well.
Again, the lifting coefﬁcients appearing in the structures can be quantized directly to obtain different
resolutions(resultingintrade-offincomputationalcost)whilepreservingtheexactreversibilityproperty.
1.8.6 Wavelet ﬁlters design via spectral factorization
1.8.6.1 Wavelets and ﬁlter banks
One of the newest additions to the transform ﬁeld is the wavelet transform which can be interpreted
as an iteration of a two-channel ﬁlter bank with certain degrees of regularity on its low-pass output
depicted in Figure 8.18. Part of the beauty and power of wavelets is their elegance: complicated tiling
of the time-frequency plane can be easily achieved by merely iterating a two-channel decomposition.
Furthermore, the wavelet representation of signals as a coarse approximation and detailed components at

1.8.6 Wavelet Filters Design via Spectral Factorization
445
FIGURE 8.17
Implementation of a complex multiplication operation via lifting steps.
FIGURE 8.18
The discrete wavelet transform as iteration of a special two-channel ﬁlter bank on its low-pass output.

446
CHAPTER 8 Modern Transform Design for Practical Audio/Image/Video
different resolutions allows fast execution of many DSP applications such as image browsing, database
retrieval, scalable multimedia delivery, etc.
The theory and design of two-channel ﬁlter banks as well as two-band wavelets have been studied
extensively and we refer the readers to several excellent text books on the topic [5–8,30]. The simplest
design procedure is the spectral factorization of a low-pass half-band ﬁlter with some regularity con-
straints (vanishing moments). Unlike all other transforms mentioned in this chapter, all of the degrees
of freedom in wavelet design are allocated to regularity. In the scope of this chapter, we will only brieﬂy
cover this most successful wavelet ﬁlters design approach for completeness only.
1.8.6.2 Spectral factorization
It can be veriﬁed in a straightforward fashion that in order for the 2-channel ﬁlter bank in Figure 8.18
to have perfect reconstruction (i.e., ˆx[n] = x[n −D], where D is a certain time delay), the four ﬁlters
involved have to satisfy the following two conditions
Aliasing Cancellation: F0(z)H0( −z) + F1(z)H1( −z) = 0,
(8.31)
Distortion Elimination: F0(z)H0(z) + F1(z)H1(z) = 2z−D.
(8.32)
The following alternating-sign construction provides an elegant way to cancel aliasing and to reduce
the design parameters involved by a factor of two
 F0(z) = H1( −z),
F1(z) = −H0( −z).
(8.33)
With the relationship between the analysis and synthesis ﬁlters set as in (8.33), the Distortion Elim-
ination condition in (8.32) reduces to
F0(z)H0(z) −F0( −z)H0( −z) = 2z−D.
(8.34)
If we deﬁne the product ﬁlter P0(z) as P0(z)
△= F0(z)H0(z), then it has to satisfy the following
condition
P0(z) −P0( −z) = 2z−D,
(8.35)
which is well-known in the signal processing community as the half-band condition (a ﬁlter that has
one and only one odd power of z−1). Hence, the design of a 2-channel perfect-reconstruction ﬁlter bank
with all four FIR ﬁlters can follow the standard procedure listed below:
•
Design a low-pass half-band ﬁlter P0(z).
•
Factor P0(z) into H0(z) and F0(z).
•
Use the aliasing cancellation condition in (8.33) to obtain H1(z) and F1(z).
The difference between any common ﬁlter pair designed from the approach above and a good wavelet
ﬁlter pair is the degree of regularity or the level of smoothness of the resulting basis functions, called the
scaling function and the wavelet function, after many levels of repeated iterations. As aforementioned,
Daubechies decided to allocate all of the degrees of freedom in the design to the degree of regularity
or the number of vanishing moments [30]. In other words, an additional constraint labeled maxﬂat is

1.8.6 Wavelet Filters Design via Spectral Factorization
447
imposed on top of the half-band condition in (8.35), resulting in the maxﬂat half-band ﬁlter deﬁned as
follows
P0(z) = (1 + z−1)2P Q0(z).
(8.36)
This key ﬁlter has 2P zeroes (roots) at z = −1 or ω = π and Q0(z) is the polynomial of minimum
order (which turns out to be 2P −2) such that the half-band condition in (8.35) is met. The closed-form
expression of this family of maxﬂat half-band ﬁlter can be found in [5,6,8,30].
The key step in the design process obviously lies in the second spectral factorization procedure.
There are multiple ways to split the roots {zi} of P0(z) and the desirable properties of the resulting
ﬁlters {H0(z), H1(z), F0(z), F1(z)} dictate the constraints in the root assignment. Here are three popular
desirable properties and their associated constraints in spectral factorization
•
For the ﬁlters to have real coefﬁcients, we need zi and z∗
i to stay together: assign both to the same
ﬁlter.
•
To obtain orthogonal solutions, we need to separate zi and z−1
i
to different ﬁlters: we must assign
zi to H0(z) and z−1
i
to F0(z) or vice versa.
•
On the contrary, to obtain linear-phase solutions, we need zi and z−1
i
to stay together: both are either
assigned to H0(z) or to F0(z).
Notethat,fromthesecondandthirdconditionabove,wecannotdesignasolutionwithbothorthogonality
and linear phase in the 2-channel case (except the trivial 2-tap Haar solution). When the number of
channels increases (M > 2), having both orthogonality and linear phase is actually quite easy to
achieve—the type-II DCT in (8.7) and the WHT in (8.6) are two typical examples.
1.8.6.2.1
5-/3-tap symmetric and 4-tap orthogonal wavelet ﬁlters
Let’s consider the following maxﬂat half-band ﬁlter with four roots at z = −1 and two real roots at
z−1 = 2 ±
√
3:
P0(z) = 1
16

−1 + 9z−2 + 16z−3 + 9z−4 −z−6
= 1
16

1 + z−14 
−1 + 4z−1 −z−2
.
(8.37)
Assigning two roots at z = −1 to F0(z) and the rest to H0(z), we obtain the following real-coefﬁcient
symmetric ﬁlter pair

H0(z) = −1
8 + 1
4z−1 + 3
4z−2 + 1
4z−3 −1
8z−4,
H1(z) = −1
2 + z−1 −1
2z−2.
(8.38)
This particular solution is ﬁrst published in [31], so they are often referred to as the Le Gall 5/3 ﬁlters.
Not only do they have dyadic coefﬁcients; their lifting multiplier-less implementation also provides an
efﬁcient reversible integer-to-integer mapping as discuss later in Section 1.8.6.3. The discrete wavelet
transform with these 5-/3-tap ﬁlters is the default transformation option for the lossless mode in the
international image coding standard JPEG2000 [4].

448
CHAPTER 8 Modern Transform Design for Practical Audio/Image/Video
On the other hand, if we assign the roots {−1, −1, 2 −
√
3} to H0(z) while giving the roots
{−1, −1, 2 +
√
3} to F0(z), we arrive at the celebrated compact-support orthogonal Daubechies D4
wavelet pair
⎧
⎪⎨
⎪⎩
H0(z) =
1
4
√
2

(1 +
√
3) + (3 +
√
3)z−1 + (3 −
√
3)z−2 + (1 −
√
3)z−3
,
H1(z) =
1
4
√
2

(1 −
√
3) −(3 −
√
3)z−1 + (3 +
√
3)z−2 −(1 +
√
3)z−3
.
(8.39)
With this particular maxﬂat half-band ﬁlter P0(z), there are only two possible orthogonal solutions. The
pair in (8.39) above is called the minimum-phase solution. Switching the role of Hi(z) and Hi(z) yields
the other maximum-phase solution. There are quite a few more real symmetric solutions. In fact, the
2-/6-tap and the 4-/4-tap pair are also quite popular in the image processing community.
1.8.6.2.2
9-/7-tap symmetric and 8-tap orthogonal wavelet ﬁlters
Let’s consider the higher-order symmetric maxﬂat half-band ﬁlter with eight roots at z = −1, two real
roots, and four complex roots; the complex plane of P0(z) is shown on the top part of Figure 8.19
P0(z) = (1 + z−1)8
2048

−5 + 40z−1 −131z−2 + 208z−3 −131z−4 + 40z−5 −5z−6
.
There are exactly four orthogonal solutions from spectral factorization in this case: one minimum-phase,
one maximum-phase, and two mixed-phase solutions (they are time-reversal of each other just like the
minimum-phase is the time-reversed version of the maximum-phase) as illustrated in Figure 8.19.
In the bi-orthogonal linear-phase case, there are many more non-trivial possibilities. The best solu-
tion in term of attaining the highest theoretical coding gain as well as the highest consistent coding
performance in practice is the 9-/7-tap ﬁlter pair with H0(z) having four roots at z = −1 and four com-
plex roots as depicted in Figure 8.20. This is often referred to as the Daubechies 9/7 wavelet ﬁlter pair
[32], which is JPEG2000’s default for ﬂoating-point high performance option [4]. Various interesting
properties of these JPEG2000’s wavelet ﬁlters have been explored in depth in [33].
It is worth noting that sacriﬁcing a few vanishing moments (precisely, reducing the number of
roots at z = −1 from eight to six in P0(z)) opens up more ﬂexibility in achieving other desirable
properties such as dyadic coefﬁcient. The following 9-/7-tap ﬁlter pair is called binlet—binary wavelet.
The analysis bank now only has two vanishing moments (instead of four as in the Daubechies 9/7
JPEG2000 pair) whereas the synthesis bank still retains four vanishing moments since the vanishing
moments—intimately related to the level of smoothness—are more critical in signal reconstruction than
in signal decomposition. The 9-/7-tap binlet ﬁlter coefﬁcients are tabulated below.

h0[n] = 1
64[1 0 −8 16 46 16 −8 0 1],
h1[n] = 1
16[1 0 −9 16 −9 0 1].
(8.40)
1.8.6.3 Lifting and the wavelet transform
The lifting scheme is originally designed for the wavelet transform and two-channel FB decomposition
[19]. Two-channel ﬁlter banks implemented with the lifting scheme save roughly half of the computa-
tional complexity due to the exploitation of the inter-subband relationship between coefﬁcients. It has

1.8.7 Higher-Order Design Approach via Optimization
449
FIGURE 8.19
Orthogonal wavelet ﬁlter design via spectral factorization.
been proven that any 2×2 polyphase matrix E(z) with unity determinant can be factored into a cascade
of prediction P(z), update U(z) lifting steps, and a diagonal scaling matrix as shown in Figure 8.21
[34].
As previously mentioned, one of the most elegant as well as the most popular wavelet ﬁlter pair in
practice is the Le Gall 5/3 [31] whose polyphase matrix can constructed with 2 lifting steps: P(z) =
1
2(1 + z−1) and U(z) = 1
4(1 + z). This transform depicted in Figure 8.22 leads to a multiplier-less
implementation, and since it can map integers to integers with exact invertibility, it is naturally adopted
by JPEG2000 for the lossless (or low cost) compression mode [4]. It has a very intuitive time-domain
interpretation: each high-pass detail wavelet coefﬁcient is simply the prediction error between the
average of 2 neighboring even-indexed samples and the in-between odd-indexed sample. Figure 8.23
shows the lifting implementation of the irrational-coefﬁcient 9/7 Daubechies wavelet ﬁlter pair [32,34]
as previously discussed in Section 1.8.6.2.2 (see Figure 8.22).
1.8.7 Higher-order design approach via optimization
One major drawback with non-overlapped block transforms such as the DCT and its integer versions as
described in the previous section is the existence of annoying blocking artifacts from discontinuities at

450
CHAPTER 8 Modern Transform Design for Practical Audio/Image/Video
FIGURE 8.20
Linear-phase bi-orthogonal wavelet ﬁlter design via spectral factorization.
FIGURE 8.21
General wavelet construction and implementation with lifting steps.
block boundaries in low bit-rate applications. Transforms with overlapping basis functions such as the
wavelet transform, the wavelet packet, and the lapped transform (LT) can elegantly solve the blocking
problems, producing much more visually pleasant reconstructed images and video frames.
From a ﬁlter bank perspective, higher-order design lengthens the ﬁlters, leading to drastic
improvement in frequency resolution and hence decorrelation efﬁciency while still retaining time or
space resolution. As aforementioned, from a ﬁlter bank perspective, wavelet and lapped transform
are simply higher-order systems whose polyphase matrices contain polynomials in z whereas all
examples discussed so far are block transforms whose polyphase matrices are regular scalar matrices
of zero order.

1.8.7 Higher-Order Design Approach via Optimization
451
FIGURE 8.22
The 5/3-tap integer wavelet implementation in lifting.
FIGURE 8.23
The 9/7-tap double-precision wavelet implementation in lifting.
1.8.7.1 General modular construction
In the wavelet case where the basic building block is a 2-channel FB, the most general design approach
is the spectral factorization of a low-pass half-band ﬁlter. The lifting scheme is used mainly for fast and
efﬁcient wavelet implementation. When the number of channels M increases, spectral factorization is not
applicable and the most successful design approach is the modular construction from the lattice structure
pioneered by Vaidyanathan [7] and Malvar [12]. This ﬁrst ﬁlter bank design based on this philosophy
is illustrated in Figure 8.24 where the each modular stage is a rotation angle (coupled with a delay
element), propagating the orthogonality property. This simple structure has been proven to utilize the
minimum number of delay elements in the resulting implementation since it shares the delays between
the two channels. More importantly, the structure in Figure 8.24 covers all possible orthogonal solutions
in the two channel case. Indeed, when K = 2, the particular selection of {θ0 = 60◦, θ1 = −15◦}
generates the D4 orthogonal Daubechies wavelet ﬁlters in (8.39). It is a simple exercise for the reader
to verify that the resulting polyphase matrix here always satisfy the paraunitary condition in (8.11), i.e.,
E−1(z) = z−(K−1)ET (z−1).
Similarly, in the more general M-channel case, we can construct the high-order polyphase matrix
E(z) as in Figure 8.25 from a cascade of many low-order building blocks Gi(z), each propagating certain
desirable properties such as symmetry, perfect reconstruction (or more elegantly, orthogonality), and
zero DC leakage. The free parameters of the structure within each building block Gi(z) are then further
optimized to achieve other remaining desirable properties such as coding gain and stop-band attenuation

452
CHAPTER 8 Modern Transform Design for Practical Audio/Image/Video
FIGURE 8.24
First 2-channel 2K-tap ﬁlter bank design via the lattice structure propagating orthogonality.
FIGURE 8.25
Construction of a high-order polyphase matrix E(z) from a cascade of low-order building blocks Gi(z).
as discussed in Section 1.8.3. This modular construction design approach yields numerous interesting
ﬁlter bank solutions, which the reader can learn about in much greater details in two excellent textbooks
[6,7].
1.8.7.2 Higher-order design with pre-/post-processing operators
Within the more restrictive constraint of this chapter, let us present in depth a general structure that has
an intuitive time-domain interpretation [35] as illustrated in Figure 8.26.
1.8.7.2.1
A simple example
Several basic properties of this design approach are best explained through an almost trivial example—
the addition of 2-point pre- and post-processing (or pre-/post-ﬁltering) operators linking any two DCT
blocks in traditional block coders such as JPEG [1] depicted in Figure 8.27a. This is a particular pre-
processor in the general framework of Figure 8.26 and the post-processor has been chosen to be the
exact inverse of the pre-processing operator.
Let {xi}, {pi}, { ˆpi}, and {ˆxi} be the set of the pre-processor’s input samples, the pre-processor’s output
samples, the post-processor’s input samples, and the post-processor’s output samples, respectively. We
examine the post-processor ﬁrst since its operation is slightly more intuitive than the pre-processor’s.
Consider ˆpi and ˆpi+1 in Figure 8.27b—two input samples of the post-processor at the boundary of
two neighboring IDCT blocks—and their corresponding output samples, ˆxi and ˆxi+1. Deﬁne a crude
measure of blocking artifact D as the absolute difference between two samples at the block boundary.
Without any post-processing, D = | ˆpi −ˆpi+1|. It is intuitive that in order to reduce the blocking artifact

1.8.7 Higher-Order Design Approach via Optimization
453
FIGURE 8.26
General transformation framework for block coding systems with pre- and post-processing operators.
s
1/2
1/2
0
1
2
3
0
1
2
3
DCT
DCT
4
5
6
7
4
5
6
7
Coding / Communication Channel
0
1
2
3
0
1
2
3
IDCT
IDCT
4
5
6
7
4
5
6
7
1/s
1/2
1/2
(b)
(a)
...
...
...
After Pre-Filtering
x i
x i+1
Before Pre-Filtering
p i
p i+1
...
...
p i
^
p i+1
^
Before Post-Filtering
...
x i
^
x i+1
^
After Post-Filtering
P
P
-1
FIGURE 8.27
Basic demonstration of the pre-/post-processing design. (a) Simple 2 × 2 pre-/post-processing operator pair
{P, P−1} linking two DCT blocks. (b) Pre-processed and post-processed effects.
D, we simply have to pull ˆpi and ˆpi+1 closer (blurring out the discontinuity at boundary) depending
on how far they are apart. One systematic method to achieve this objective is presented in Figure 8.27a
where the scaling factor s is the single degree of freedom.
With P−1 = 1
2
 1
1
1 −1
	  1
0
0 1/s
	  1
1
1 −1
	
, we can easily derive the following relationships
ˆxi = ˆpi + 1/s −1
2
( ˆpi −ˆpi+1) and
ˆxi+1 = ˆpi+1 −1/s −1
2
( ˆpi −ˆpi+1).

454
CHAPTER 8 Modern Transform Design for Practical Audio/Image/Video
Hence, after post-processing,
%D = |ˆxi −ˆxi+1| = |( ˆpi −ˆpi+1) + (1/s −1)( ˆpi −ˆpi+1)|
= |( ˆpi −ˆpi+1)/s| = |1/s|| ˆpi −ˆpi+1|.
(8.41)
Choosing |1/s| < 1 (or equivalently |s| > 1), guarantees a decrease in blocking artifact. This
post-processing effect is demonstrated in Figure 8.27b. For example, if we choose s = 2, then the
post-processing operator partitions the distance D = | ˆpi −ˆpi+1| into four segments of equal length,
moves ˆxi up one segment length from ˆpi, and moves ˆxi+1 down one segment length from ˆpi+1. So,
post-processing adaptively reduces the observed discrepancy D by a factor of two.
The pre-processor modiﬁes the samples in the exact opposite direction. It attempts to “decorrelate”
the boundary by lowering the smaller-value sample xi while increase the larger-value xi+1 based on
a percentage of their difference |xi −xi+1|. Again, the choice s > 1 makes intuitive sense. If s < 1,
samples are adjusted in the wrong direction. A good choice of s in the energy compaction sense for a
smooth AR(1) signal model is the Golden Ratio 1+
√
5
2
or a close approximation such as 8
5 [35] (note
that this choice also yields a multiplier-less post-processor).
1.8.7.2.2
More general solutions
The seemingly trivial example above actually provides a valuable lesson. When more than two samples
are involved, the pre-processing operator should be designed as a ﬂattening operator. It should attempt to
make the input to the DCT as homogeneous as possible; thus, improving the overall energy compaction
of the combined decomposition. This is quite consistent with most pre-processing schemes in practice:
smoothing the input signal improves coding efﬁciency. However, in this framework, perfect recon-
struction can be easily maintained (by choosing the post-processor as the inverse of the pre-processor).
High-frequency signal components are never eliminated; they are only slightly shifted spatially. In
other words, we take full advantage of the block-based framework by carefully aligning high-frequency
components at block boundaries. Discontinuities between DCT blocks, i.e., actual high-frequency con-
tents, do not affect coding performance whereas, within each block, data samples are smoothened out,
enhancing the core block transform’s effectiveness in energy compaction.
The most general higher-order transform as shown in Figure 8.27 is equivalent to an M-channel
critically-sampled ﬁlter banks with ﬁlters of length L = K M whose analysis polyphase matrix turns
out to be
E(z) = G0G1(z)G1(z) . . . GK−1(z),
(8.42)
where each propagating component takes on the form Gi(z) = (z) Pi with
(z)
△=
 I
0
0
zI
	  0
I
I
0
	
=
 0
I
zI
0
	
(8.43)
being the permuted advance chain. The synthesis polyphase matrix is then
R(z) = G−1
K−1(z)G−1
K−2(z) . . . G−1
0 .
(8.44)

1.8.7 Higher-Order Design Approach via Optimization
455
FIGURE 8.28
Construction of an M-channel ﬁlter bank from two M × M matrices P and C (top) and the equivalent time-
domain interpretation of pre- and post-processing (bottom).
The more detailed polyphase representation is shown in Figure 8.28 for the case of M = 4 and K = 1.
By chosen the inverse G−1
i
(z) = P−1
i
−1(z), we ensure that the resulting ﬁlter bank or decomposition
has perfect reconstruction. Enforcing the tighter constraint P−1 = PT leads to a paraunitary system,
i.e., an orthogonal mapping.
To obtain linear-phase basis functions, further structural constraint has to be imposed on the basic
building block P. A general closed form M-point pre-processing operator P that works well with DCT
of any size M, generating a linear-phase mapping, is presented in [35]. This general pre-processing
block operator is shown in Figure 8.29a and it should be placed squarely between two adjacent DCT
blocks as illustrated in Figure 8.28. The linear operator consists of two stages of butterﬂies and a matrix
V between them:
P = 1
2
 I
J
J −I
	  I
0
0 V
	  I
J
J −I
	
,
(8.45)
where the reader is reminded that I, J, and 0 are the M
2 × M
2 identity matrix, the reversal matrix,
and the null matrix, respectively. The matrix V, controlling pre- and post-processing behavior, can be
further chosen as
V = JCI I T
M/2SCI V
M/2J,
(8.46)
where CI I
M/2 is the M
2 -point type-II DCT matrix in (8.7), CI V
M/2 is the M
2 -point type-IV DCT matrix in
(8.8), and S is a diagonal matrix that introduces bi-orthogonality. One example of S is diag{ 8
5, 1, . . . , 1}.
The structure produces orthogonal pre-processing linear operator if S is chosen to be the identity matrix.

456
CHAPTER 8 Modern Transform Design for Practical Audio/Image/Video
 DCT
   IV
J
IDCT
   II
J
1/2
1/2
1/2
1/2
1/2
1/2
1/2
1/2
s
V
P
(a)
1/2
1/2
1/2
1/2
1/2
1/2
1/2
1/2
4/3
8/7
8/7
8/7
3/4
3/8
1/2
1/8
3/8
3/8
V
(b)
P
FIGURE 8.29
Pre-processors for the DCT. (a) General closed form M-point pre-ﬁlter. (b) Fast 8-point rational-coefﬁcient
pre-processing operator.
FIGURE 8.30
Pre-processing’s block-wise ﬂattening effect with 8 × 8 block size. From left to right: original image; after
2-point, 4-point, 6-point, and 8-point pre-processing, respectively.
In general, it can be straightforwardly proven that any orthogonal matrix V would yield a linear-phase
orthogonal transform. For the particular case of 8-point pre/post-processing, a fast rational-coefﬁcient
approximation of P is shown in Figure 8.29b.
It is interesting to note the ﬂattening property of the pre-processing scheme as demonstrated in an
image processing example shown in Figure 8.30 where pre-processing is carried out in 2D separably.
When the size of the pre-processing operator grows, the pre-processed image (which will then be the
input of the traditional block DCT decomposition) becomes more blocky since each 8×8 block becomes
smoother, less correlated with its neighbors, and more high-frequency components are shifted to the
block boundary. The closed-form pre-processor P of (8.45) depicted in Figure 8.29a is used to generate
this example. At the decoder side, the post-processor—if chosen as the inverse of its counterpart—
serves as a smoothening interpolating operator placed between block boundaries, mitigating blocking
artifacts.

1.8.7 Higher-Order Design Approach via Optimization
457
1.8.7.2.3
HD photo or JPEG-XR transform
Both concepts of multiplier-less transform design via the lifting scheme as described in Section 1.8.5.4
and pre-/post-processing of Section 1.8.7.2 are on full display in JPEG-XR [27,28]. The still-image
compression algorithm/ﬁle format JPEG-XR has been widely distributed along with Windows Vista and
Windows 7 operating systems and was ofﬁcially approved as an ISO/IEC International standard in 2010.
The entire transformation stage in JPEG-XR (formerly known as Microsoft HD-Photo)—including a
base integer transform called Photo Core Transform (PCT) C and an additional pre-processing operator
called Photo Overlap Transform (POT) C—is just one particular solution of the much general framework
illustrated in Figures 8.26 and 8.28.
Both operators are constructed from cascades of hardware-friendly dyadic-rational lifting steps. In
fact, the choice of the two dyadic lifting steps 1
2 in the core transform PCT comes from Conﬁguration
C5 as shown previously in Table 8.1. In Figure 8.31, two pairs of scaling factors
√
2 and
1
√
2 are shown
merely to illustrate the conceptual design. The actual implementation in JPEG-XR utilizes dyadic-
rational lifting steps to approximate each pair of irrational scaling. Therefore, the entire transformation
stage (including both PCT and POT) in JPEG-XR is constructed from a cascade of only dyadic-rational
lifting steps [27,28].
1.8.7.3 Adaptive decomposition design
Here, by adaptivity, we refer to time-varying adaptable operators that the encoder or the decoder can
select on-the-ﬂy based on the local statistical behavior of the input. Adaptivity can lead to signiﬁcant
coding improvements if the amount of side information is cleverly avoided or kept to a minimum. It
is clear that long basis functions associated with large operator size are best for representing smooth
signal regions. On the other hand, transient signals, abrupt discontinuities, and strong edges are best
captured with short high-time-resolution basis. Based on this observation, various adaptive operators
are developed to provide a powerful decomposition framework with strong time-frequency localization.
Several options are under consideration:
•
varying the support of pre- and post-processing operator at each block boundary;
•
varying the transform block size;
FIGURE 8.31
The photo overlap transform P (left) and the photo core transform C (right) in HD photo or JPEG-XR trans-
formation stage.

458
CHAPTER 8 Modern Transform Design for Practical Audio/Image/Video
•
a combination of both of the above;
•
allowing every operator’s parameters to be time-varying.
1.8.7.3.1
Adaptive pre-/post-processing support
Within the pre-/post-processing framework in Section 1.8.7.2, based on the energy of the transform
coefﬁcients generated, we can decide to turn on or off pre/post-processing. It is just as easy to vary the
size of pre-processing support dynamically. This adaptive-borrowing signal decomposition is illustrated
in Figure 8.32a where the block size is ﬁxed to M = 4 while the pre-processing operator can be chosen
among: no processing, 2 × 2 processing, or 4 × 4 processing. In other words, from top to bottom, we
are switching from a 4 × 8 to a 4 × 7 to a 4 × 6 to a 4 × 5 and ﬁnally to a 4 × 4 linear mapping.
With the transform set to the DCT of a ﬁxed block size (e.g., 8) and the size of the pre-/post-processing
operator can be chosen from the set {0, 2, 4, 8}, then the side information for each block boundary is
only 2 bits. The side information can be lowered with Huffman or context-based adaptive arithmetic
coding.
DCT
0
1
2
3
0
1
2
3
V
1/2
0
1
6
7
8
9
1/2
1/2
1/2
2
3
4
5
DCT
0
1
2
3
0
1
2
3
DCT
0
1
2
3
0
1
2
3
V
1/2
1/2
1/2
1/2
10
11
Coding / Communication Channel
DCT
0
1
2
3
0
1
2
3
V
12
13
18
19
20
21
1/2
1/2
14
15
16
17
DCT
0
1
2
3
0
1
2
3
DCT
0
1
0
1
V
1/2
1/2
DCT
0
1
2
3
0
1
2
3
V
1/2
0
1
6
7
8
9
1/2
1/2
1/2
2
3
4
5
DCT
0
1
2
3
0
1
2
3
0
1
2
3
0
1
2
3
V
1/2
1/2
1/2
1/2
10
11
Coding / Communication Channel
DCT
4
5
6
7
4
5
6
7
12
13
18
19
20
21
14
15
16
17
DCT
0
1
2
3
0
1
2
3
4
5
4
5
V
1/2
1/2
1/2
1/2
(a)
(b)
FIGURE 8.32
Examples of adaptive time-varying decomposition. (a) Adaptive pre-processing with ﬁxed DCT block size.
(b) Adaptive DCT block size with ﬁxed pre-processing operators.

1.8.7 Higher-Order Design Approach via Optimization
459
FIGURE 8.33
Visual demonstration of adaptive decomposition algorithms. From left to right: pre-/post-processing mode
selection for maximum local coding efﬁciency via exhaustive search; pre-/post-processing mode selection for
fast overﬂow/underﬂow criterion; portion of the original Bike image; optimal block-size selection based on
maximum coding efﬁciency criterion.
A visual illustration of adaptive pre-/post-processing is depicted in the left-most binary image in
Figure 8.33. Here, dark pixels indicate blocks which the coder select the minimum-size 2 × 2 operator
whereas light pixels indicate blocks where the maximum-size 8×8 pre-processing operator is preferred.
The decision is based on the minimum bit-length needed to encode each particular data block and we
have employed an exhaustive search [36]. It is clear that pre- and post-processing should be avoided at
or near strong edges of the input image.
1.8.7.3.2
Adaptive transform block size
Another adaptive decomposition scheme is to employ variable block sizes. In slowly-changing homo-
geneous part of the signal, a large data block is desirable (higher frequency resolution is needed). On
the other hand, in fast-changing transient part of the signal, it is more advantageous to switch to a small
block size (higher time resolution is needed). Such a signal-adaptive switching scheme has proven to
be very effective in practice. For instance, MPEG-4’s Advanced Audio Coder switches back-and-forth
between a 256-point high-time-resolution short window and a 2048-point high-frequency-resolution
long window to avoid pre-echo and to improve coding efﬁciency [37]. A variable-block-size decom-
position scheme as depicted in Figure 8.32b can be employed where the pre-processing operator is
ﬁxed whereas the DCT block size is allowed to vary. The side information, just like in the adaptive-
processing case, can be kept manageable as well with a well chosen set of block sizes, e.g., {4, 8, 16, 32}
for image/video applications.
A demonstration of the adaptive block transform algorithm is shown in the right of Figure 8.33.
An expensive exhaustive search algorithm is set up to generate this example: pre- and post-processing
is ﬁxed to the maximum size allowable based on the size of any two neighboring DCT blocks; block
size selection is restricted to {4, 8, 16}; and the criterion is again the minimum amount of bits require
to encode each local 16 × 16 image region. This example vividly conﬁrms that optimally adaptive
decomposition algorithm has a tendency to latch onto strong local image features [36].

460
CHAPTER 8 Modern Transform Design for Practical Audio/Image/Video
In the more general case, both adaptive pre-processing with different sizes, with multiple stages,
and adaptive variable block size can be combined. This decomposition scheme generates a large library
of basis functions that the encoder can choose from depending on the input signal behavior. How to
make the right decision quickly and how to minimize the amount of side information involved are two
important open research problems.
1.8.7.4 Modulated design
One drawback with the cascading structural approach in (8.42) and illustrated in Figure 8.26 is that
the optimization becomes problematic when the application demands or desires a larger number of
channels M and/or a longer ﬁlter length L = K M. In speech and audio coding applications, for
example, frequency resolution of the transform is often critical and the number M of frequency bins (or
channels) is typically in the hundreds or even thousands [12,38].
Given that most of the cost function in Section 1.8.3.1 is not convex, the huge number of free param-
eters in the matrices {G0, Pi}—from 2K
 M/2
2

in the linear-phase orthogonal case to K
 M
2

in the orthogonal case to K M2 in the most general case [35]—poses a major challenge to any opti-
mization approach. One particular elegant design that can efﬁciently deal with this obstacle is the
cosine-modulated approach where a low-pass prototype ﬁlter is designed and the rest of the ﬁlters are
obtained from cosine or sine modulation as illustrated in Figure 8.34. Many different cosine-modulation
approaches have been developed and the most signiﬁcant difference among them is the low-pass pro-
totype choice and the phase of the cosine/sine sequence [6,7,12].
For the case K = 1 (M × 2M transform), the modulated design with the type-IV DCT in (8.8) is
often referred to as the Modiﬁed Discrete Cosine Transform (MDCT) [39] or the Modulated Lapped
Transform (MLT) [40]. The transform basis functions (or the ﬁlter coefﬁcients) are given as
hk[n] =

2
M h[n] cos

n + M + 1
2
 
k + 1
2
 π
M
	
(8.47)
for 0 ≤k ≤M −1, 0 ≤n ≤2M −1 and h[n] can be thought of as a symmetric window modulating
the cosine sequence or the impulse response of a low-pass prototype (with cut-off frequency at
π
2M ). It
has been shown that as far as perfect reconstruction is concerned, the choice of the prototype window
h[n] is rather ﬂexible: all it has to satisfy is the following two constraints
 h[n] = h[2M −1 −n],
h2[n] + h2[M −1 −n] = 1.
(8.48)
A nice choice with a closed-form expression for the prototype ﬁlter h[n] is
h[n] = −sin

n + 1
2
 π
2M
	
,
(8.49)
which the reader can easily verify that it does not only satisfy both conditions in (8.48) but it also
produces a smooth symmetric low-pass ﬁlter.

1.8.7 Higher-Order Design Approach via Optimization
461
FIGURE 8.34
Construction via cosine/sine modulation.
It is interesting to note that the MDCT/MLT as deﬁned in (8.47) and (8.49) also ﬁts the pre- and
post-processing framework in Figure 8.28. In this interpretation, the prototype window is completely
governed by the pre-processing operator P while the orthogonal core transform CI V takes care of
the cosine-modulation part. The particular window choice in (8.49) leads to a series of M
2 pairwise
rotation angles, which yields an overall orthogonal transform. Unfortunately, we did have to give up the
linear-phase property. However, for speech and audio applications, that is not a critical property. The
MDCT/MLT as depicted in Figure 8.35 still has a very efﬁcient implementation. Finally, as discussed in
Section 1.8.5.2, the MDCT can easily be approximated with dyadic lifting steps, rendering a multiplier-
less transformation with exact integer-to-integer mapping [41].
Both MP3 and MPEG-2 AAC employs the choice of window in (8.49). However, this is not the only
choice. In fact, the open-source Vorbis codec selects a slightly different sine window
h[n] = sin
 π
2 sin2

n + 1
2
 π
2M
	&
.
(8.50)
MP3 utilizes a hybrid adaptive decomposition approach: the MDCT/MLT is applied to the output of a
32-channel 512-tap near-perfect-reconstruction ﬁlter bank. MP3’s hybrid ﬁlter bank adapts to the signal
characteristics via the block switching technique similarly to the adaptive mechanism shown in Figure
8.32: the codec adaptively selects either a 6×12 MDCT (labeled short block for higher time resolution)

462
CHAPTER 8 Modern Transform Design for Practical Audio/Image/Video
FIGURE 8.35
Modulated lapped transform or MDCT implementation with pre-/post-processing operators.
or a 18 × 36 MDCT (labeled long block for higher frequency resolution). On the other hand, MPEG-4
AAC dynamically switches between a 128 × 256 MDCT (short blocks) and a 1024 × 2048 MDCT
(long blocks) [38]. The long block’s frequency resolution is rather high, offering improved coding
efﬁciency for stationary signals whereas the shorter blocks provide optimized coding capability for
transient signals. The transform choice in MP3 and AAC clearly indicates the importance of adaptivity
in state-of-the-art compression systems.
Finally, if the application requires even higher frequency resolution, the reader should consider the
extended version of the MDCT/MLT, namely the extended lapped transform (ELT) [12,42] whose basis
functions are
hk[n] = h[n] cos
 
k + 1
2
	 
n −L −1
2
 π
M +

N + 1
 π
2
	&
(8.51)
for 0 ≤k ≤M −1, 0 ≤n ≤L −1, and h[n] is again the impulse response of the prototype low-
pass window. A major advantage of the ELT is the existence of various fast implementation algorithms
which still rely on the DCT type-IV as the modulation engine but employ more layers of pre-processing
operators (similarly to the overall framework shown in Figure 8.26).

1.8.8 Conclusion
463
1.8.8 Conclusion
In this chapter, we ﬁrst brieﬂy review the deep and rich history of transform design and then present
a general systematic approach for modern transform design: the structural construction of high-order
transforms from low-order basic building blocks: butterﬂies, lifting steps, and possibly scaling factors.
We concentrate on the integer or dyadic-rational coefﬁcient case which yields powerful integer invert-
ible transforms. Numerous key examples in mainstream audio, image, and video applications are also
presented.
References
[1] W.B. Pennebaker, J.L. Mitchell, JPEG: Still Image Compression Standard, Van Nostrand Reinhold, New
York, NY, 1993.
[2] J.L. Mitchell, D. LeGall, C. Fogg, MPEG Video Compression Standard, Chapman & Hall, New York, NY,
1996.
[3] ITU Telecom. Standardization Sector of ITU, Video coding for low bit rate communication, ITU-T Recom-
mendation H.263, March 1996.
[4] D.S. Taubman, M.W. Marcellin, JPEG2000: Image Compression Fundamentals, Standards, and Practice,
Kluwer Academic Publishers, Norwell, MA, 2002.
[5] S.G. Mallat, A Wavelet Tour of Signal Processing, Academic Press, San Diego, CA, 1998.
[6] G. Strang, T.Q. Nguyen, Wavelets and Filter Banks, second ed., Wellesley-Cambridge Press, Boston, 1998.
[7] P.P. Vaidyanathan, Multirate Systems and Filter Banks, Prentice Hall, Englewood Cliffs, NJ, 1993.
[8] M. Vetterli, J. Kovacevic, Wavelets and Subband Coding, Prentice Hall, Englewood Cliffs, NJ, 1995.
[9] J.W. Cooley, J.W. Tukey, An algorithm for the machine calculation of complex Fourier series, Math. Comput.
19 (1965) 297–301.
[10] N. Ahmed, T. Natarajan, K.R. Rao, Discrete cosine transform, IEEE Trans. Comput. C23 (1974) 90–93.
[11] K.R. Rao, P. Yip, Discrete Cosine Transform: Algorithms, Advantages, Applications, Academic Press, New
York, NY, 1990.
[12] H.S. Malvar, Signal Processing with Lapped Transforms, Artech House, Boston, MA, 1992.
[13] I.E.G. Richardson, H.264 and MPEG-4 Video Compression, John Wiley and Sons, 2003.
[14] H. Malvar, A. Hallapuro, M. Karczewicz, L. Kerofsky, Low-complexity transform and quantization in
H.264/AVC, IEEE Trans. Circuits Syst. Video Technol. 13 (2003) 598–603.
[15] W.K. Cham, Development of integer cosine transforms by the principle of dyadic symmetry, IEE Proc. 136
(1989) 276–282.
[16] W.K. Cham, Y.T. Chan, An order-16 integer cosine transform, IEEE Trans. Signal Process. 39 (1991)
1205–1208.
[17] K.T. Lo, W.-K. Cham, Development of simple orthogonal transforms for image compression, IEE Proc. Vis.
Image Signal Process. 142 (1995) 22–26.
[18] I.D. Tun, S.U. Lee, On the ﬁxed-point-error analysis of several fast DCT algorithms, IEEE Trans. Circuits
Syst. Video Technol. 3 (1993) 27–41.
[19] W. Sweldens, The lifting scheme: a custom-design construction of bi-orthogonal wavelets, Appl. Comput.
Harmon. Anal. 3 (1997) 185–200.

464
CHAPTER 8 Modern Transform Design for Practical Audio/Image/Video
[20] A.A.M.L. Bruekens, A.W.M. vanden Enden, New networks for perfect inversion and perfect reconstruction,
IEEE J. Sel. Areas Commun. 10 (1992) 130–137.
[21] L. Liu, T.D. Tran, P. Topiwala, From 16-bit to high-accuracy idct approximation: fruits of single architecture
afﬁliation, in: Proceedings of SPIE Applications of Digital Image Processing XXX, August 2007.
[22] H.S. Malvar, G.J. Sullivan, YCoCg-R: A Color Space with RGB Reversibility and Low Dynamic Range,
JVT ISO/IEC MPEG and ITU-T VCEG, Document No. JVT-I014r3, July 2003.
[23] P. Topiwala, C.Tu, New Invertible Integer Color Transforms Based on Lifting Steps and Coding of 4:4:4
Video, JVT ISO/IEC MPEG and ITU-T VCEG, Document No. JVT-I014r8, July 2003.
[24] K.R. Rao, P. Yip, V. Britanak, Discrete Cosine Transform: Algorithms, Advantages, Applications, Academic
Press, 2007.
[25] J. Liang, T.D. Tran, P.Topiwala, A 16-Bit Architecture for h.26l, Treating DCT Transforms and Quantization,
ITU-T Q.6/SG16, Document No. VCEG-M16, June 2001.
[26] J. Liang, T.D. Tran, Fast multiplier-less approximations of the DCT with the lifting scheme, IEEE Trans.
Signal Process. 49 (2001) 3032–3044.
[27] S. Srinivasan, C.Tu, S.L. Regunathan, G.J. Sullivan, Hd photo: a new image coding technology for digi-
talphotography, in: Proceedings of SPIE Applications of Digital Image Processing XXX, vol. 6696, August
2007.
[28] C.Tu,S.Srinivasan,G.J.Sullivan,S.Regunathan,H.S.Malvar,Low-complexityhierarchicallappedtransform
for lossy-to-lossless image coding in jpeg xr/hd photo, in: Proceedings of SPIE Applications of Digital Image
Processing XXXI, vol. 7073, August 2008.
[29] S. Oraintara, Y.J. Chen, T. Nguyen, Integer fast Fourier transform, IEEE Trans. Signal Process. 50 (2002)
607–618.
[30] I. Daubechies, Ten lectures on wavelets, in: Siam CBMS-NSF Regional Conference Series in Applied
Mathematics, Philadelphia, PA, 1992.
[31] D. LeGall and A.Tabatabai, Subband coding of digital images using symmetric short kernel ﬁlters and
arithmetic coding techniques, in: Proceedings of the IEEE International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), 1988, pp. 761–765.
[32] M. Antonini, M. Barlaud, P. Mathieu, I. Daubechies, Image coding using wavelet transform, IEEE Trans.
Image Process. 1 (1992) 205–220.
[33] M. Under, T. Blu, Mathematical properties of the JPEG2000 wavelet ﬁlters, IEEE Trans. Image Process. 12
(2003) 1080–1090.
[34] I. Daubechies, W.Sweldens, Factoring wavelet transforms into lifting steps, J. Fourier Anal. Appl. 4 (3)
(1998) 247–269.
[35] T.D. Tran, J. Liang, C. Tu, Lapped transform via time-domain pre- and post-ﬁltering, IEEE Trans. Signal
Process. 51 (2003) 1557–1571.
[36] W. Dai, L.Liu, T.D. Tran, Adaptive block-based image coding with pre-/post-ﬁltering, in: Proceedings of the
Data Compression Conference, March 2005, pp. 73–82.
[37] J.D. Johnston, S.R. Quackenbush, J. Herre, B. Grill, Review of MPEG-4 general audio coding, in: Multimedia
Systems, Standards, and Networks, 2000, pp. 131–155.
[38] A. Spanias, T. Painter, V. Atti, Audio Signal Processing and Coding, John Wiley and Sons, Hoboken, NJ,
2007.
[39] J.P. Princen, A.W. Johnson, A.B. Bradley, Subband/transform coding using ﬁlter bank designs based on time
domain aliasing cancellation, in: Proceedings of the IEEE International Conference on Acoustics, Speech,
and Signal Processing (ICASSP), May 1987, pp. 2161–2164.

References
465
[40] H.S. Malvar, Lapped transforms for efﬁcient transform/subband coding, IEEE Trans. Acoust. Speech Signal
Process. 38 (1990) 969–978.
[41] Y. Yokotani, R.Geiger, G.Schuller, S.Oraintara, K.R. Rao, Lossless audio coding using the IntMDCT and
rounding error shaping, IEEE Trans. Audio Speech Lang. Process. 14 (2006) 2201–2211.
[42] H.S. Malvar, Extended lapped transforms: properties, applications and fast algorithms, IEEE Trans. Signal
Process. 40 (1992) 2703–2714.

9
CHAPTER
Discrete Multi-Scale Transforms
in Signal Processing
Yufang Bao* and Hamid Krim†
*Department of Mathematics and Computer Science/Center of Defense and Homeland Security (CDHS),
UNC Fayetteville State University, Fayetteville, NC, USA
†Electrical and Computer Engineering Department, North Carolina State University, Raleigh, NC, USA
1.09.1 Introduction
1.09.1.1 A overview of multiscale transforms
One’s ﬁrst and most natural reﬂex when presented with an unfamiliar object is to carefully look it over,
and hold it up to the light to inspect its different facets in the hope of recognizing it, or of at least relating
any of its aspects to a more familiar and well known entity. This almost innate strategy pervades all
science and engineering disciplines.
Physical phenomena (e.g., earth vibrations) are monitored and observed by way of measurements
in a form of temporal and/or spatial data sequences. Analyzing such data is tantamount to extracting
information which is useful to further understanding of the underlying process (e.g., frequency and
amplitude of vibrations may be an indicator for an imminent earthquake). Visual or manual analysis
of typically massive amounts of acquired data (e.g., in remote sensing) are impractical, making one
resort to adapted mathematical tools and analysis techniques to better cope with potential intricacies
and complex structure of the data. Among these tools, ﬁgure a variety of functional transforms (e.g.,
Fourier Transform) which in many cases, may facilitate and simplify an analytical track of a problem,
and frequently (and just as importantly) provide an alternative view of, and a better insight into, the
problem (this, in some sense, is analogous to exploring and inspecting data under a “different light” ).
An illustration of such a “simpliﬁcation” is shown in Figure 9.1, where a rather intricate signal x(t)
shown in the leftmost ﬁgure may be displayed/viewed in a different space as two elementary tones. In
Figure 9.2, a real bird chirp is similarly displayed as a fairly rich signal which, when considered in an
appropriate space, is reduced and “summarized” to a few “atoms” in the Time-Frequency representation
(TF). Transformed signals may formally be viewed as convenient representations in a different domain
which is itself described by a set of vectors/functions {φi(t)}i={1,2,...,N}. A contribution of a signal x(t)
along a direction “φi(t)” (its projection) is given by the following inner product
Ci(x) = ⟨x(t), φi(t)⟩=
 ∞
−∞
x(t)φi(t)dt,
(9.1)
where the compact notation “⟨·, ·⟩” for an inner product is used. The choice of functions in the set is
usually intimately tied to the nature of information that we are seeking to extract from the signal of
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00009-7
© 2014 Elsevier Ltd. All rights reserved.
467

468
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
Y
X
Simplified Representation by Projection
FIGURE 9.1
A canonical function-based projection to simplify a representation.
interest. A Fourier function for example, is speciﬁed by a complex exponential “e jωt,” and reﬂects the
harmonic nature of a signal, and provides it with a simple and an intuitively appealing interpretation as
a “tone.” Its spectral (frequency) representation is a Dirac impulse which is well localized in frequency.
A Fourier function is hence well adapted to represent a class of signals with a fairly well localized
“spectrum,” and is highly inadequate for a transient signal which, in contrast, tends to be temporally well
localized. This thus calls for a different class of analyzing functions, namely those with good temporal
compactness. The wavelet transform, by virtue of its mathematical properties, indeed provides such an
analysis framework which in many ways is complementary to that of the Fourier Transform.
The success of classical wavelets has motived the active research in the multiscale analysis ﬁeld where
multidisciplinary researchers pursue the best multiscale basis/frame in order to efﬁciently approximate
a function that has singularities across scales. Although the classical wavelets can reach the optimal
approximation for one-dimensional (1-D) functions, it has problems in generating sparse coefﬁcients
to approximate a two-dimensional (2-D) function with discontinuities. The classical wavelets locate
singularities of a 2-D function in isotropic and multiscale representations. The Fourier frequency plane
is partitioned with scaled square windows that are only in the horizontal/vertical directions. The band
pass wavelet atoms, in general, are conceptually equivalent to applying differential operators (such as
calculating the gradient difference) to an image to measure the regularities in localized areas along the
horizontal/vertical directions. This isotropic measurement lacks ﬂexibility, and therefore, although the
classical wavelets can identify discontinuities efﬁciently, they can not efﬁciently detect the direction
oriented curves in an image even at ﬁne scales. Even though some improvements has been introduced
to adopt anisotropic dilation in different directions, the problems still persist.

1.09.1 Introduction
469
0
200
400
600
800
1000
1200
−1
−0.5
0
0.5
1
Time
Frequency
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.5
0.6
0.7
0.8
0.9
1
FIGURE 9.2
Bird Chirp with a simpliﬁed representation in an appropriate basis.
A 2-D signal, visually represented as an image, typically contains spatially distributed geometrical
shapes that can be characterized by edges that signal the transients from one object to other objects.
The edges are the discontinuities in an image intensity function along curves, contours, or lines that
are directional oriented. Scientists have acknowledged [1–3] the instinct of a human eye in recognizing
objects in a multiscale manner. The eye can quickly zoom into the most important isotropic or anisotropic
features. Researchers in mathematics and engineering ﬁelds have extended this vision natural to the
new era anisotropic wavelets. These new transformations have allowed researchers to selectively capture
edges, which are important features in image processing.
The recently developed curvelets [4–7], contourlets [8,9], and shearlets [10–13] aim to mathemati-
cally animate the eye’s function in order to catch the anisotropic curve features. We generally refer them
to the anisotropic wavelets. The beauty with the anisotropic wavelets is their ﬂexibility in partitioning
the Fourier plane into a collection of atoms with varied scales and orientations. One advantage with
the anisotropic wavelets is that, when a two dimensional function is projected into these wedge shaped
atoms, the energy of singularities can be conﬁned to only a sparse number of atoms, resulting only a
small amount of signiﬁcant coefﬁcients across scales. They are more efﬁcient tools than the classical
wavelets in capturing the direction oriented curve features. Another advantage with the anisotropic
wavelets is that many nice features of the classical wavelets are retained under the anisotropic wavelets.
Some essential results are parallel to the classical wavelets, for example, the exact reconstruction and

470
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
energy conservation formula. This has not only made the anisotropic wavelets more attractive tools, but
also allows fast development of the anisotropic wavelets using the classical wavelets as guidelines.
The goal of this chapter being of a tutorial nature, we cannot help but provide a somewhat high level
exposition of the theoretical frameworks of the classical wavelets and the anisotropic wavelets, while
our focus will be to provide a working knowledge of the tools as well as their potential for applications
in information sciences in general. Some of the MATLAB code for the pictures used in this paper can
be downloaded in [14]. Before we get into introduction of the various wavelets, we will introduce some
classical Fourier transform related topics in this remaining section.
1.09.1.2 Signal representation in functional bases
As noted above, a proper selection of a set of analyzing functions heavily impacts the resulting repre-
sentation of an observed signal in the dual space, and hence the resulting insight as well.
When considering ﬁnite energy signals (these are also said to be L2(Rd)), i.e.,

Rd |x(t)|2dt < ∞,
(9.2)
a convenient functional vector space is that which is endowed with a norm inducing inner product,
namely a Hilbert space, which will also be our assumed space throughout.
Deﬁnition 1.
A vector space endowed with an inner product, which in turn induces a norm, and such
that every sequence of functions xn(t) whose elements are asymptotically close is convergent (this
convergence is in the Cauchy sense whose technical details are deliberately left out to help the ﬂow of
the chapter), is called a Hilbert space.
Remark.
An L2(Rd), d = 1, 2 space is a Hilbert space.
Deﬁnition 2.
Given α > 0, a function x(t) is said to be Lipschitz-α at t0 ∈Rd, if there exists a
positive constant K and a polynomial pn,t0 with degree n = [α] such that, for all t in a neighborhood
of t0, we have
|x(t) −pn,t0(t)| ≤K|t −t0|α.
(9.3)
It is shown that function x(t) is necessarily m times differentiable at point t0 if x(t) is uniformly
Lipschitz with α > m in a neighborhood of t0. Further, if x(t) is Lipschitz α with α < 1 at t0, then f is
not differentiable at t0.
Deﬁnition 3.
A set of vectors or functions {φi(t)}, i = 1, . . ., which are linearly independent and
which span a vector space, is called a basis. If in addition these elements are orthonormal, then the basis
is said to be an orthonormal basis.
Among the desirable properties of a selected basis in such a space are adaptivity and a capacity to
preserve and reﬂect some key signal characteristics (e.g., signal smoothness). Fourier bases have in the
past been at the center of all science and engineering applications. They have in addition, provided an
efﬁcient framework for analyzing periodic as well as non-periodic signals with dominant modes.

1.09.1 Introduction
471
1.09.1.3 The continuous Fourier transform
The Fourier transform is a powerful tool in understanding features of signals/images through a basis. A
Fourier Transform essentially measures the spectral content of a signal x(t) across all frequencies “ω,”
view as row vector, and is deﬁned as the decomposition of the signal over a basis {e−jωt} as.
Deﬁnition 4.
For a signal x(t), t ∈Rd viewed as a column vector, that is an absolutely integrable
function, its Fourier transform is deﬁned as
X(ω) =

Rd x(t)e−jωt dt.
(9.4)
The formula is also referred to as the Fourier integral. The signal x(t) can be recovered by its inverse
Fourier transform that is deﬁned as
x(t) =
1
(2π)d

Rd X(ω)e jωt dω.
(9.5)
As an orthonormal transform, the FT enjoys a number of interesting properties [15]. To simplify,
we are constrained to 1-D function f (t) in the following until we get to the sections of anisotropic
wavelets. If we use FT ( f )(ω) = F(ω) to represent the Fourier transform of f (t), we can list some of
its important properties as follows,
•
Linearity:
FT ( f + g)(ω) = F(ω) + G(ω).
•
Convolution:
FT ( f ⋆g)(ω) = F(ω)G(ω).
•
Multiplication:
FT ( f g)(ω) = F(ω) ⋆G(ω).
•
Shifting:
FT (x(t −s))(ω) = e−jsωX(ω).
•
Scaling:
FT (x(at))(ω) = |a|−1X(a−1ω).
•
Differentiation:
FT
∂nx(t)
∂tn

(ω) = ( jω)n X(ω).
Throughout, and unless otherwise speciﬁed, the ⋆symbol between two functions usually indicates a
convolution between the two functions, and as a superscript denotes a complex conjugate. Furthermore,
a useful property of 1-D the Fourier transform is its energy preservation in the dual space (transform
domain),

R
|x(t)|2dt =

R
|X(ω)|2dω.
This is also true for signals in Rd, and is referred to as the Plancherel/Parseval property [15].

472
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
1.09.1.4 Fourier series
If a canonical function of a selected basis is φ(t) = e jωt, where ω ∈R, then we speak of a Fourier basis
whichyieldsaFourierSeries(FS)forperiodicsignalsandaFourierTransform(FT)foraperiodicsignals.
The Fourier Transform is deﬁned for a broad class of signals [16], and may also be specialized to
derive the Fourier Series (FS) of a periodic signal. This is easily achieved by noting that a periodic
signal x(t) may be evaluated over a period T, which in turn leads to,
˜x(t) = x(t + T ) =
∞

n=−∞
αx
ne jnω0t,
(9.6)
with ω0 = 2π/T and αx
n = 1/T
 T
0 x(t)e−jnω0t dt.
In applications, however, x(t) is measured and sampled at discrete times, requiring that the afore-
mentioned transform be extended to obtain a spectral representation which closely approximates the
theoretical truth and which remains just as informative. Towards that end, we proceed to deﬁne a Discrete
Time Fourier Transform (DTFT) as
X(e jω) =
∞

i=−∞
x(i)e−jωi.
(9.7)
This expression may also be extended for ﬁnite observation (ﬁnite dimensional) signals via the Fast
Fourier Transform [15]. While these transforms have been, and remain crucial in many applications,
they show limitations in problems requiring a good “time-frequency” localization as often encountered
in transient analysis. This may easily be understood by reinterpreting Eq. (9.7) as a weighted transform
where each of the time samples is equally weighted in the summation for X(e jω). Prior to introducing
alternatives to uniform weighting in the Fourier space, we ﬁrst discuss the practical implementation and
exploitation of the DTFT.
1.09.1.5 The discrete Fourier transform
In application, the Discrete Fourier Transform is the computational surrogate for the continuous Fourier
Transform and a natural extension of the DTFT. It results in a periodic sequence of numbers that is
mapped to discretely sampled values of an analog signal (viewed as a time function). The sampling is
performed over a N-long window which is extended periodically. The discrete Fourier transform (DFT)
also views a ﬁnite length signal, x(n), as a periodic signal, and is deﬁned as
X(k) =
N−1

n=0
x(n)e
−j2πkn
N
.
The original signal may be recovered by applying an inverse transform
x(n) = 1
N
N−1

k=0
X(k)e
j2πkn
N
.
The Discrete Fourier transform decomposes a signal into an orthonormal basis. The energy is preserved
by the Plancherel formula described earlier and written as
∥f ∥2 =
N−1

n=0
|x(k)|2 = 1
N
N−1

k=0
|X(k)|2.

1.09.1 Introduction
473
The discrete Fourier transform of a two-dimensional signal (image) is correspondingly deﬁned as
X(k1, k2) =
N−1

n1,n2=0
x(n1, n2)e
−j2π(k1n1+k2n2)
N
.
The original signal may also be recovered by applying an inverse transform
x(n1, n2) = 1
N 2
N−1

k1,k2=0
X(k1, k2)e
j2π(k1n1+k2n2)
N
.
Since the DFT is based on the sampling taken over a certain length, the frequency content of the original
function may not be adequately represented by the DFT when the sampling rate is insufﬁciently low. This
will cause the high frequency content to fold back into the lower frequency content and yield aliasing.
To overcome some of the limitations of the uniform weighting of a function of a classical FT, Gabor
[17] ﬁrst proposed to use a different weighting window leading to the so-called windowed FT which
served well in many practical applications [18] and is discussed next.
1.09.1.6 Windowed Fourier transform
As just mentioned, when our goal is to analyze very local features, such as those present in transient
signals for instance, it then makes sense to introduce a focusing window as follows:
Wμ,ω(t) = e jωtW(t −μ),
where ∥W(μ,ω)∥= 1 ∀(μ, ω) ∈R2 and where W( · ) is typically a smooth function with a compact
support. This yields the following parameterized transform:
XW(ω, μ) = ⟨x(t), Wμ,ω(t)⟩.
(9.8)
The selection of a proper window is problem-dependent and is ultimately resolved by the desired spectro-
temporal trade-off which is itself constrained by the Heisenberg uncertainty principle [18,19]. From the
T-F distribution perspective, and as discussed by Gabor [17] and displayed in Figure 9.3, the Gaussian
window may be shown to have minimal temporal as well as spectral support of any other function.
It hence represents the best compromise between the temporal and spectral resolution. Its numerical
implementation entails a discretization of the modulation and translation parameters, and results in a
uniform partitioning of the T-F plane as illustrated in Figure 9.4. Different windows result in various
T-F distributions of elementary atoms, favoring either temporal or spectral resolution as may be seen
for the different windows in Figure 9.5. While representing an optimal time-frequency compromise, the
uniform coverage of the T-F plane by the Gabor transform falls short of adequately resolving a signal
whose components are spectrally far apart. This may easily and convincingly be illustrated by the study
case in Figure 9.6, where we note the number of cycles which may be enumerated within a window of
ﬁxed time width. It is readily seen that while the selected window (shown grid) may be adequate for one
ﬁxed frequency component, it is inadequate for another lower frequency component. An analysis of a
spectrum exhibiting such a time-varying behavior is ideally performed by way of a frequency-dependent

474
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
Frequency
Time
FIGURE 9.3
A Gaussian waveform results in a uniform analysis in the time-frequency plane.
ω/4
Time
Frequency
ω/2
ω
FIGURE 9.4
A time frequency tiling of dyadic wavelet basis by a proper sub-sampling of a wavelet frame.
time window in different designs as we elaborate in the next several sections. The wavelet transform
described next, offers a highly adaptive window which is of compact support, and which, by virtue of its
dilations and translations covers different spectral bands at all instants of time. The anisotropic wavelets
using different windows, such as the curvelet transform, the contourlet transform, and the shearlet
transform, will be introduced following the discussion of wavelet transform and wavelet decomposition.

1.09.1 Introduction
475
−1
−0.5
0
0.5
1
0
0.5
1
Gaussian
−1
−0.5
0
0.5
1
0
0.5
1
Fourier Transf
−1
−0.5
0
0.5
1
0
0.5
1
Hamming
−1
−0.5
0
0.5
1
0
0.5
1
Fourier Transf
−1
−0.5
0
0.5
1
0
0.5
1
Hanning
−1
−0.5
0
0.5
1
0
0.5
1
Fourier Transf
−1
−0.5
0
0.5
1
0
0.5
1
Kaiser
−1
−0.5
0
0.5
1
0
0.5
1
Fourier Transf
FIGURE 9.5
Trade-off resulting from windows.
x
3
2.5
2
1.5
1
0.5
1
0.5
0
-0.5
-1
FIGURE 9.6
Time windows and frequency trade-off.

476
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
1.09.2 Wavelets: a multiscale analysis tool
This section is organized as follows: In Section 1.09.2.1, we deﬁne a multiscale analysis based on a
wavelet basis and elaborate on their properties and their implications, as well as on their applications. In
Section 1.09.2.2, we discuss a speciﬁc estimation technique which is believed to be sufﬁciently generic
and general to be useful in a number of different applications of information sciences in general, and
of signal processing and communications in particular.
1.09.2.1 Wavelet transform
Much like the FT, the WT is based on an elementary function, which is well localized in time and
frequency. In addition to a compactness property, this function has to satisfy a set of properties to be
admissible as a wavelet. The ﬁrst fundamental property is stated next,
Deﬁnition 5.
[18,20,21] A wavelet is a ﬁnite energy function ψ( · ) (i.e., ψ( · ) ∈L2(R)) with zero
mean,
 ∞
−∞
ψ(t)dt = 0.
(9.9)
Commonly normalized so that ∥ψ∥=

|ψ|2dt = 1, it also constitutes a fundamental building block
in the construction of functions (atoms) spanning the time-scale plane by way of dilation and translation
parameters. We hence write,
ψμ,ξ(t) =
1
√ξ ψ
t −μ
ξ

,
where the scaling factor ξ
1
2 ensures an energy invariance of ψμ,ξ(t) over all dilations “ξ” ∈R+ and
translations “μ” ∈R. With such a function in hand, and towards mitigating the limitation of a windowed
FT, we proceed to deﬁne a Wavelet Transform (WT) with such a capacity. A scale-dependent window
with good time localization properties as shown in Figure 9.7, yields a transform for x(t) given by
Wx(μ, ξ) =
 ∞
−∞
x(t) 1
√ξ ψ∗
t −μ
ξ

dt,
(9.10)
where “∗” denotes complex conjugate. This is of course in contrast to the Gabor transform whose
window width remains constant throughout. A time-frequency plot of a continuous wavelet transform
is shown in Figure 9.8 for a corresponding x(t).
1.09.2.2 Inverting the wavelet transform
Similarly to the weighted FT, the WT is a redundant representation, which with a proper normalization
factor, leads to a reconstruction formula,
x(t) =
1
Cψ
 ∞
0
 ∞
−∞
Wx(μ, ξ) 1
√ξ ψ
t −μ
ξ
 dξ
ξ2 dμ,
(9.11)
with Cψ =
 +∞
0
ˆ
2(ω)
ω
dω.

1.09.2 Wavelets: A Multiscale Analysis Tool
477
−5
0
5
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
(a) Mexihat Wavelet
0
0.1
0.2
0.3
0.4
0.5
0
1
2
3
4
5
6
7
8
9
10
(b) Spectrum of Mexihat
FIGURE 9.7
Admissible Mexican hat wavelet.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−10
−5
0
5
10
15
20
log2(s)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1
2
3
4
5
6
7
8
9
FIGURE 9.8
Continuous signal with corresponding wavelet transform with cones of inﬂuence around singularities.

478
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
While the direct and inverse WT have been successfully applied in a number of different prob-
lems [18], their computational cost due to their continuous/redundant nature is considered as a serious
drawback. An immediate and natural question arises about a potential reduction in the computational
complexity, and hence in the WT redundancy. This has clearly to be carefully carried out to guarantee a
sufﬁcient coverage of the T-F plane, and thereby ensure a proper reconstruction of a transformed signal.
Upon discretizing the scale and dilation parameters, a dimension reduction relative to a continuous trans-
form is achieved. The desired adaptivity of the window width with varying frequency naturally results by
selecting a geometric variation of the dilation parameter ξ = ξm
0 , m ∈Z (set of all positive and negative
integers). To obtain a systematic and consistent coverage of the T-F plane, our choice of the translation
parameter “μ” should be in congruence with the fact that at any scale “m,” the coverage of the whole
line R (for instance) is complete, and the translation parameter be in step with the chosen wavelet ψ(t),
i.e., μ = nμ0ξm
0 with n ∈Z. This hence gives the following scale and translation adaptive wavelet:
ψ(t)m,n = ξ−m/2
0
ψ
 t
ξm
0
−nμ0

,
(9.12)
where the factor “ξ−m/2
0
” ensures a unit energy function. This reduction in dimensionality yields a
redundant discrete wavelet transform endowed with a structure which lends itself to an iterative and
fast inversion/reconstruction of a transformed signal x(t).
Deﬁnition 6.
The set of resulting wavelet coefﬁcients {⟨x(t), ψmn(t)⟩}(m,n)∈Z2 completely character-
izes x(t), and hence leads to a stable reconstruction [18,19] if ∀x(t) ∈L2(R) (or ﬁnite energy signal)
the following condition on the energy holds,
A∥x(t)∥2 ≤

m,n
|⟨x(t), ψmn(t)⟩|2 ≤B∥x(t)∥2.
(9.13)
Such a set of functions {ψmn(t)}(m,n)∈Z2 then constitutes a frame.
The energy inequality condition intuitively suggests that the redundancy should be controlled to avoid
instabilities in the reconstruction (i.e., too much redundancy makes it more likely that any perturbation
of coefﬁcients will yield an unstable/inconsistent reconstruction). If the frame bounds “A” and “B” are
equal, the corresponding frame is said to be tight, and if furthermore A = B = 1, it is an orthonormal
basis. Note, however, that for any A ̸= 1 a frame is not a basis. In some cases, the frame bounds specify
a redundancy ratio which may help guide one in inverting a frame representation of a signal, since
a unique inverse does not exist. An efﬁcient computation of an inverse (for reconstruction), or more
precisely of a pseudo-inverse, may only be obtained by a judicious manipulation of the reconstruction
formula [18,19]. This is, in a ﬁnite dimensional setting, similar to a linear system of equations with a rank
deﬁcient matrix whose column space has an orthogonal complement (the union of the two subspaces
yields all of the Hilbert space), and hence whose inversion is ill conditioned. The size of this space is
determined by the order of the deﬁciency (number of linearly dependent columns), and explains the
potential for instability in a frame projection-based signal reconstruction [18]. This type of “effective
rank” utilization is encountered in numerical linear algebra applications (e.g., signal subspace methods
[22], model order identiﬁcation, etc.). The close connection between the redundancy of a frame and
the rank deﬁciency of a matrix, suggests that solutions available in linear algebra [23] may provide

1.09.2 Wavelets: A Multiscale Analysis Tool
479
insight in solving our frame-based reconstruction problem. A well known iterative solution to a linear
system and based on a gradient search solution is described in [24], (and in fact coincides with a popular
iterative solution to the frame algorithm for reconstructing a signal) for solving
f = L−1b,
where “L” is a matrix operating on f, and b is the data vector. Starting with an initial guess f0 and
iterating on it leads to
fn = fn−1 + α(b −Lfn−1).
(9.14)
When α is appropriately selected, the latter iteration may be shown to yield [18]
lim
n→+∞fn = f.
(9.15)
Numerous other good solutions have also been proposed in the literature and their detailed descriptions
are given in [18,19,25,26].
1.09.2.3 Wavelets and multiresolution analysis
While a frame representation of a signal is of lower dimension than that of its continuous counterpart, a
complete elimination of redundancy is only possible by orthonormalizing a basis. A proper construction
of such a basis ensures orthogonality within scales as well as across scales. The existence of such a
basis with a dyadic scale progression was ﬁrst shown, and an explicit construction was given by Meyer
[27]and Daubechies et al. [19]. The connection to subband coding discovered by Mallat [28] resulted in
a numerically stable and efﬁcient implementation which helped propel wavelet analysis at the forefront
of computational sciences (see Ref. [29] for a comprehensive historical as well technical development
[19] which also led to Daubechies celebrated orthonormal wavelet bases).
To help maintain a smooth ﬂow of this chapter, and achieve the goal of endowing an advanced
undergraduate with a working knowledge of the Multiresolution Analysis (MAR) framework, we give a
high level introduction, albeit comprehensive, and defer most of the technical details to the sources in the
bibliography.Forexample,thetrade-offintime-frequencyresolutionadvocatedearlier,liesattheheartof
theoftentechnicalwaveletdesignandconstruction.Thebalancebetweenthespectralandtemporaldecay
which constitutes one of the key design criteria, has led to a wealth of new functional bases, and this upon
the introduction of the now classical multiresolution analysis framework [18,30]. The pursuit of a more
reﬁned analysis arsenal resulted in the recently introduced nonlinear multiresolution analysis in [31–33].
1.09.2.3.1
Multiresolution analysis
The Multiresolution Analysis theory (MRA) developed by Mallat [34] and Meyer [27], may be viewed
as a functional analytic approach to subband signal analysis, which had previously been introduced in
and applied to engineering problems [35,36]. The clever connection between an orthonormal wavelet
decomposition of a signal and its ﬁltering by a bank of corresponding ﬁlters led to an axiomatic for-
malization of the theory and subsequent equivalence. This as a result, opened up a new wide avenue of
research in the development and construction of new functional bases as well as ﬁlter banks [21,37–39].
The construction of a telescopic set of nested approximation and detail subspaces {Vj} j∈Z and {W j} j∈Z

480
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
W1
V2
W2
V1
FIGURE 9.9
Hierarchy of wavelet bases.
each endowed with an orthonormal basis, as shown in Figure 9.9, is a key step of the analysis. An inter
and intra scale orthogonality of the wavelet functions, as noted above, is preserved, with the inter-scale
orthogonality expressed as
Wi ⊥W j
∀i ̸= j.
(9.16)
By replacing the discrete parameter wavelet ψi j(t) where (i, j) ∈Z2 (set of all positive and negative
2-tuple integers) respectively denote the translation and the scale parameters, in Eq. (9.10) (i.e., μ =
2 ji, ξ = 2J, we obtain the orthonormal wavelet coefﬁcients as [18],
Ci
j(x) = ⟨x(t), ψi j(t)⟩.
(9.17)
The orthogonal complementarity of the scaling subspace and that of the residuals/details amount to
synthesizing the higher resolution subspace
Vi ⊕Wi = Vi−1.
Iterating this property, leads to a reconstruction of the original space where the observed signal lies, and
which in practice, is taken to be V0. That is, the observed signal at its ﬁrst and ﬁnest resolution (this may
be viewed as implicitly accepting the samples of a given signal as the coefﬁcients in an approximation
space with a scaling function corresponding to that which the subsequent analysis is based on). The
dyadic scale progression has been thoroughly investigated, and its wide acceptance/popularity is due
to its tight connection with subband coding, whose practical implementation is fast and simple. Other
nondyadic developments have also been explored (see for e.g., [21]).
The qualitative characteristics of the MRA we have thus far discussed, may be succinctly stated as
follows,

1.09.2 Wavelets: A Multiscale Analysis Tool
481
Deﬁnition 7.
[18]Asequence{Vi}i∈Z ofclosedsubspacesofL2(R)isamultiresolutionapproximation
if the following properties hold:
•
∀( j, k) ∈Z2, x(t) ∈Vj ↔x(t −2 jk) ∈Vj,
•
∀j ∈Z, Vj+1 ⊂Vj,
•
∀j ∈Z, x(t) ∈Vj ↔x
	 t
2

∈Vj+1,
•
lim
j→−∞Vj =
∞

j=−∞
Vj = {0},
•
lim
j=−∞=
∞

j=−∞
Vj = L2(R),
•
There exists a function φ(t) such that {φ(t −n)}n∈Z is a Riesz basis of V0, where the “overbar”
denotes closure of the space.
1.09.2.3.2
Properties of wavelets
A wavelet analysis of a signal assumes a judiciously preselected wavelet, and hence a prior knowledge
about the signal itself. As stated earlier, the properties of an analyzing wavelet have a direct impact on
the resulting multiscale signal representation. Carrying out a useful and meaningful analysis is hence
facilitated by a good understanding of some fundamental wavelet properties.
1.09.2.3.3
Vanishing moments
Recall that one of the fundamental admissibility conditions of a wavelet is that its ﬁrst moment be zero.
This is intuitively understood in the sense that a wavelet focuses on residuals or oscillating features
of a signal. This property may in fact be further exploited by constructing a wavelet with an arbitrary
number of vanishing moments. We say that a wavelet has n vanishing moments if

ψ(t)tidt = 0,
i = {0, 1, . . . , n −1}.
(9.18)
Reﬂecting a bit on the properties of a Fourier Transform of a function [15], it is easy to note that the num-
ber of zero moments of a wavelet reﬂects the behavior of its Fourier Transform around zero. This property
is also useful in applications like compression, where it is highly desirable to maximize the number
of small/negligible coefﬁcients, and only preserve a minimal number of large coefﬁcients. The associ-
ated cost with increasing the number of vanishing moments is that of an increased support size for the
wavelet, hence that of the corresponding ﬁlter [18,19], and hence of some of its localizing potential.
1.09.2.3.4
Regularity/smoothness
The smoothness of a wavelet “ψ(t)” is important for an accurate and parsimonious signal approximation.
For a large class of wavelets (those relevant to applications), the smoothness (or regularity) property of a
wavelet, which may also be measured by its degree of differentiability (dαψ(t)/dtα) or equivalently by
its Lipschitzity “γ ,” is also reﬂected by its number of vanishing moments [18]. The larger the number
of vanishing moments, the smoother the function. In applications, such as image coding, a smooth
analyzing wavelet is useful for not only compressing the image, but for controlling the visual distortion
due to errors as well. The associated cost (i.e., some trade-off is in order) is again a size increase in the

482
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
wavelet support, which may in turn make it more difﬁcult to capture local features, such as important
transient phenomena.
1.09.2.3.5
Wavelet symmetry
At the exception of a Haar wavelet, compactly supported real wavelet are asymmetric around their
center point. A symmetric wavelet clearly corresponds to a symmetric ﬁlter which is characterized by a
linear phase. The symmetry property is important for certain applications where symmetric features are
crucial (e.g., symmetric error in image coding is better perceived). In many applications, however, it is
generally viewed as a property secondary to those described above. When such a property is desired,
truly symmetric biorthogonal wavelets have been proposed and constructed [19,40], with the slight
disadvantage of using different analysis and synthesis mother wavelets. In Figure 9.10, we show some
−2
0
2
4
−0.2
−0.1
0
0.1
0.2
0.3
(a) Symmlet
0
0.1
0.2
0.3
0.4
0.5
0
1
2
3
4
5
6
(b) Spectrum of Symmlet
−2
−1
0
1
2
3
−0.2
0
0.2
0.4
(c) Spline Biorthogonal Wavelet
0
0.1
0.2
0.3
0.4
0.5
0
2
4
6
8
(d) Spectrum of Spline Wavelet
FIGURE 9.10
Biorthogonal wavelets preserve symmetry and symlets nearly do.

1.09.2 Wavelets: A Multiscale Analysis Tool
483
illustrative cases of symmetric wavelets. Other nearly symmetric functions (referred to as symlets) have
also been proposed, and a detailed discussion of the pros and cons of each, is deferred to Daubechies [19].
1.09.2.3.6
A ﬁlter bank view: implementation
As noted earlier, the connection between a MRA of a signal and its processing by a ﬁlter bank was not
only of intellectual interest, but was of breakthrough proportion for general applications as well. It indeed
provided a highly efﬁcient numerical implementation for a theoretically very powerful methodology.
Such a connection is most easily established by invoking the nestedness property of MR subspaces.
Speciﬁcally, it implies that if φ(2−jt) ∈Vj and Vj ⊂Vj−1, and we can hence write,
1
2 j/2 φ
	
2−jt

=
1
2
j−1
2
∞

k=−∞
h(k)φ(2−j+1t −k),
(9.19)
where h(k) = ⟨φ(2−jt), φ(2−j+1t −k)⟩, i.e., the expansion coefﬁcient at time shift k. By taking the
FT of Eq. (9.19), we obtain
(2 jω) =
1
21/2 H(2 j−1ω)(2 j−1ω),
(9.20)
which when iterated through scales leads to
(ω) =
∞

p=−∞
h(2−pω)
√
2
(0).
(9.21)
The complementarity of scaling and detail subspaces noted earlier stipulates that any function in sub-
space W j may also be expressed in terms of Vj−1 = Span{φ j−1,k(t)}( j,k)∈Z2, or
1
2 j/2 ψ(2−jt) =
∞

i=−∞
1
2( j−1)/2 g(i)φ(2−j+1t −i).
(9.22)
In the Fourier domain, this leads to

(2 jω) =
1
√
2
G(2 j−1ω)(2 j−1ω),
(9.23)
whose iteration also leads to an expression of the wavelet FT in terms of the transfer function G(ω), as
previously given for (ω) in Eq. (9.21). In light of these equations it is clear that the ﬁlters {G(ω), H(ω)}
may be used to compute functions at successive scales. This in turn implies that the coefﬁcients of any
function x(t) may be similarly obtained as they are merely the result of an inner product of the signal
of interest x(t) with a basis function,
⟨x(t), ψi j(t)⟩=

⟨x(t), g(k)
1
2( j−1)/2 φ(2−j+1(t −2−ji) −k)⟩
= mkg(k)A j+1
i−k (x).
(9.24)
To complete the construction of the ﬁlter pair {H( · ), G( · )}, the properties between the approximation
subspaces
	
{Vj} j∈Z

and detail subspaces
	
{W j} j∈Z

are exploited to derive the design criteria for

484
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
the discrete ﬁlters. Speciﬁcally, the same scale orthogonality property between the scaling and detail
subspaces in

k∈Z
(2 jω + 2kπ)
(2 jω + 2kπ) = 0,
∀j,
(9.25)
is the ﬁrst property that the resulting ﬁlters should satisfy. For the sake of illustration, ﬁx j = 1 and use
√
2(2ω) = H(ω)(ω).
(9.26)
Using Eq. (9.26) together with the orthonormality property of jth scale basis functions { jk(t)} [18],
i.e.,  |(ω + 2kπ)|2 = 1, where we separate even and odd terms, yields the ﬁrst property of one of
the so-called conjugate mirror ﬁlters,
|H(ω)|2 + |H(ω + π)|2 = 1.
(9.27)
Using the non-overlapping property expressed in Eq. (9.25), together with the evaluations of 
(2ω)
and (2ω), and making a similar argument as in the preceding equation, yield the second property of
conjugate mirror ﬁlters,
H(ω)G∗(ω) + H(ω + π)G∗(ω + π) = 0.
(9.28)
The combined structure of the two ﬁlters is referred to as “a conjugate mirror ﬁlter bank,” and their
respective impulse responses completely specify the corresponding wavelets (see numerous references,
e.g., [20] for additional technical details). Note that the literature in the MR studies tends to follow one
of two possible strategies:
•
A more functional analytic approach, which is mostly followed by applied mathematicians/
mathematicians and scientists [18,19,30] (we could not possibly do justice to the numerous good
texts now available, the author cites what he is most familiar with).
•
A more ﬁltering oriented approach widely popular among engineers and some applied mathemati-
cians [21,38,41], see Figures 9.11 and 9.12.
1,k
{
}
C
2
h
g
x(t)
h
g
2 
2 
1
1,k
{
}
C
2 
FIGURE 9.11
Filter bank implementation of a wavelet decomposition.

1.09.2 Wavelets: A Multiscale Analysis Tool
485
0
5
10
15
20
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
D−8 Scaling Function
0
5
10
15
20
−1.5
−1
−0.5
0
0.5
1
D−8 Wavelet Function
0
2
4
6
8
0
0.2
0.4
0.6
0.8
1
D−8 Scaling F−Transform
x
0
2
4
6
8
0
0.2
0.4
0.6
0.8
1
D−8 wavelet F−Transform
x
FIGURE 9.12
A Daubechies-8 function and its spectral properties.
1.09.2.3.7
Reﬁning a wavelet basis: wavelet packet and local cosine bases
A selected analysis wavelet is not necessarily well adapted to any observed signal. This is particularly
the case when the signal is time varying and has a rich spectral structure.
One approach is to then partition the signal into homogeneous spectral segments and by the same
token ﬁnd it an adapted basis. This is tantamount to further reﬁning the wavelet basis, and resulting in
what is referred to as a wavelet packet basis. A similar adapted partitioning may be carried out in the
time domain by way of an orthogonal local trigonometric basis (sine or cosine). The two formulations
are very similar, and the solution to the search for an adapted basis in both cases is resolved in precisely
the same way. In the interest of space, we focus in the following development on only wavelet packets.

486
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
1.09.2.3.8
Wavelet packets
We maintained above that a selection of an analysis wavelet function should be carried out in function of
the signal of interest. This of course assumes that we have some prior knowledge about the signal at hand.
While plausible in some cases, this assumption is very unlikely in most practical cases. Yet it is highly
desirable to select a basis which might still lead to an adapted representation of an apriority “unknown”
signal. Coifman and Meyer [42] proposed to reﬁne the standard wavelet decomposition. Intuitively,
they proposed to further partition the spectral region of the details (i.e., wavelet coefﬁcients) in addition
to partitioning the coarse/scaling portion as ordinarily performed for a wavelet representation. This as
shown in Figure 9.13 yields an overcomplete representation of a signal, in other words a dictionary
of bases with a tree structure. The binary nature of the tree as further discussed below, affords a very
efﬁcient search for a basis which is best adapted to a given signal in the set.
Formally, we accomplish a partition reﬁnement of say a subspace U j by way of a new wavelet basis
which includes both its approximation and its details, as shown in Figure 9.13. This construction due to
Coifman and Meyer [42] may be simply understood as one’s ability to ﬁnd a basis for all the subspaces
{Vj} j∈Z and {W j} j∈Z by iterating the nestedness property. This then amounts to expressing two new
functions 
ψ0
j (t) and 
ψ1
j (t) in terms of {
ψ j−1,k}( j,k)∈Z2, an orthonormal basis of a generic subspace
U j−1 (which in our case may be either Vj−1 or W j−1),

ψ0
j (t) =
∞

k=−∞
h(k)
ψ j−1(t −2 j−1k),
(9.29)

ψ1
j (t) =
∞

k=−∞
g(k)
ψ j−1(t −2 j−1k),
(9.30)
where h(·) and g(·) are the impulse responses of the ﬁlters in a corresponding ﬁlter bank, and the
combined family {
ψ0
j (t −2 jk), 
ψ1
j (t −2 jk)}( j,k)∈Z2 is an orthonormal basis of U j. This, as illustrated
in Figure 9.13, is graphically represented by a binary tree where, at each of the nodes reside the
First Scale Coeff
First Wavelet  Coeff.
Time Signal
Coarser Temporal Resolution
Finer Spectral Resolution
0
π
Frequency
Wavelet Packet coeff.
FIGURE 9.13
Wavelet packet bases structure.

1.09.2 Wavelets: A Multiscale Analysis Tool
487
h
g
x(t)
h
g
h
g
2 
2 
2 
2 
h
g
1,k
{
}
0
1
{
}
3,k
C
1
1,k
{
}
C
C
FIGURE 9.14
Wavelet packet ﬁlter bank realization.
corresponding wavelet packet coefﬁcients. The implementation of a wavelet packet decomposition is a
straightforward extension of that of a wavelet decomposition, and consists of an iteration of both H( · )
and G( · ) bands (see Figure 9.14) to naturally lead to an overcomplete representation. This is reﬂected
on the tree by the fact that, at the exception of the root and bottom nodes which only have respectively
two children nodes and one ancestor node, each node bears two children nodes and one ancestor node.
1.09.2.3.9
Basis search
To identify the best adapted basis in an overcomplete signal representation, as just noted above, we
ﬁrst construct a criterion which when optimized, will reﬂect desired properties intrinsic to the signal
being analyzed. The earliest proposed criterion applied to a wavelet packet basis search is the so-called
entropy criterion [43]. Unlike Shannon’s information theoretic criterion, this is additive and makes use
of the coefﬁcients residing at each node of the tree in lieu of computed probabilities. The presence of
more complex features in a signal necessitates such an adapted basis to ultimately achieve an ideally
more parsimonious/succinct representation. As pointed out earlier, when searching for a wavelet packet
or local cosine best basis, we typically have a dictionary D of possible bases with a binary tree structure.
Each node ( j, j′) (where j ∈{0, . . . , J} represents the depth and j′ ∈{0, . . . , 2 j −1} represents the
branches on the jth level) of the tree then corresponds to a given orthonormal basis B j, j′ of a vector
subspace of ℓ2({1, . . . , N}) (ℓ2({1, . . . , N}) is a Hilbert space of ﬁnite energy sequences). Since a
particular partition p ∈P of [0, 1] is composed of intervals I j, j′ = [2−j j′, 2−j( j′+1)[, an orthonormal
basis of ℓ2({1, . . . , N}) is given by B p = ∪{( j, j′)|I j, j′∈P}B j, j′. By taking advantage of the property
Span{B j, j′} = Span{B j+1,2 j′}
⊥⊕Span{B j+1,2 j′+1},
(9.31)
where ⊕denotes a subspace direct sum, we associate to each node a cost C( · ). We can then per-
form a bottom-up comparison of children versus parent costs (this in effect will eliminate the redun-
dant/inadequate leaves of the tree) and ultimately prune the tree.

488
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
FIGURE 9.15
Tree pruning in search for a best-basis.
Our goal is to then choose the basis which leads to the best approximation of {x[t]} among a collection
of orthonormal bases {B p = {
xi p}1≤i≤N|p ∈P}, where the subscript xi emphasizes that it is adapted
to {x[t]}. Trees of wavelet packet bases studied by Coifman and Wickerhauser [44] are constructed by
quadrature mirror ﬁlter banks and comprise functions that are well-localized in time and frequency.
This family of orthonormal bases partitions the frequency axis into intervals of different sizes, with
each set corresponding to a speciﬁc wavelet packet basis. Another family of orthonormal bases studied
by Malvar [45], and Coifman and Meyer [42], can be constructed with a tree of windowed cosine
functions, and correspond to a division of the time axis into intervals of dyadically varying sizes.
For a discrete signal of size N (e.g., the size of the WP tableau shown in Figure 9.13), one can show
that a tree of wavelet packet bases or local cosine bases has P = N(1 + log2 N) distinct vectors but
includes more than 2N/2 different orthogonal bases. One can also show that the signal expansion in these
bases is computed with algorithms that require O(N log2 N) operations. Coifman and Wickerhauser
[43] proposed that for any signal {x[m]} and an appropriate functional K(·), one ﬁnds the best basis
B p0 by minimizing an “additive” cost function
Cost(x, B p) =
N

i=1
K(|⟨x, 
xi p⟩|2)
(9.32)
over all bases. As a result, the basis which results from minimizing this cost function corresponds
to the “best” representation of the observed signal.The resulting pruned tree of Figure 9.15 bears the
coefﬁcients at the remaining leaves/nodes.
1.09.2.4 MR applications in estimation problems
The computational efﬁciency of a wavelet decomposition together with all of its properties have trig-
gered unprecedented interest in their application in the area of information sciences (see e.g., [46–51]).
Speciﬁc applications have ranged from compression [52–55] to signal/image modeling [56–59], and
from signal/image enhancement to communications [60–67]. The literature in statistical applications
as a whole has seen an explosive growth in recent years, and in the interest of space, we will focus

1.09.2 Wavelets: A Multiscale Analysis Tool
489
our discussion on a somewhat broader perspective which may in essence, be usefully reinterpreted in a
number of different instances.
1.09.2.4.1
Signal estimation/denoising and modeling
Denoising may be heuristically interpreted as a quest for parsimony of a representation of a signal.
Wavelets as described above, have a great capacity for energy compaction, particularly at or near singu-
larities. Given a particular wavelet, its corresponding basis as noted above, is not universally optimal for
all signals and particularly not for a noisy one; this difﬁculty may be lifted by adapting the representation
to the signal in a “best” way possible and according to some criterion. The ﬁrst of the two possible ways,
is to pick an optimal basis in a wavelet packet dictionary and discard the negligible coefﬁcients [68].
The second which focuses on reconstruction of a signal in noise, and which we discuss here, accounts
for the underlying noise statistics in the multiscale domain to separate the “mostly” signal part from the
“mostly” noise part [69–71]. We opt here to discuss a more general setting which assumes unknown noise
statistics and where a signal reconstruction is still sought in some optimal way, as we elaborate below.
This approach is particularly appealing in that it may be reduced to the setting in earlier developments.
1.09.2.4.2
Problem statement
Consider an additive noise model
x(t) = s(t) + n(t),
(9.33)
where s(t) is an unknown but deterministic signal corrupted by a zero-mean noise process n(t), and
x(t) is the observed, i.e., noisy, signal. The objective is to recover the signal {s(t)} based on the obser-
vations {x(t)}.
The underlying signal is modeled with an orthonormal basis representation,
s(t) =

i
Cs
i ψi(t),
and similarly the noise is represented as
n(t) =

i
Cn
i ψi(t).
By linearity, the observed signal can also be represented in the same fashion, its coefﬁcients given by
Cx
i = Cs
i + Cn
i .
A key assumption we make is that for certain values of i, Cs
i = 0; in other words, the corresponding
observation coefﬁcients Cx
i represent “pure noise,” rather than signal corrupted by noise. As shown by
Krim and Pesquet [72], this is a reasonable assumption in view of the spectral and structural differences
between the underlying signal s(t) and the noise n(t) across scales. Given this assumption, wavelet-
based denoising consists of determining which wavelet coefﬁcients represent primarily signal, and
which mostly capture noise. The goal is to then localize and isolate the “mostly signal” coefﬁcients.
This may be achieved by deﬁning an information measure as a function of the wavelet coefﬁcients. It
identiﬁes the “useful” coefﬁcients as those whose inclusion improves the data explanation. One such
measure is Rissanen’s information-theoretic approach (or Minimum Description Length (MDL)) [73].
In other words, the MDL criterion is utilized for resolving the tradeoff between model complexity (each

490
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
retained coefﬁcient increases the number of model parameters) and goodness-of-ﬁt (each truncated
coefﬁcient decreases the ﬁt between the received—i.e., noisy—signal and its reconstruction).
1.09.2.4.3
The coding length criterion
Wavelet thresholding is essentially an order estimation problem, one of balancing model accuracy
against overﬁtting, and one of capturing as much of the “signal” as possible, while leaving out as
much of the “noise” as possible. One approach to this estimation problem is to account for any prior
knowledge available on the signal of interest, which usually is of probabilistic nature. This leads to a
Bayesian estimation approach as developed in [74,75]. While generally more complex, it does provide
a regularization capacity which is much needed in low Signal to Noise Ratio environments.
A parsimony-driven strategy which we expound on here, addresses the problem of modeling in gen-
eral, and that of compression in particular. It provides in addition a fairly general and interesting frame-
work where the signal is assumed deterministic and unknown, and results in some intuitively sensible and
mathematically tractable techniques [68]. Speciﬁcally, it brings together Rissanen’s work on stochastic
complexity and coding length [73,76], and Huber’s work on minimax statistical robustness [77,78].
Following Rissasen, we seek the data representation that results in the shortest encoding of both
observations and complexity constraints. As a departure from the commonly assumed Gaussian like-
lihood, we rather assume that the noise distribution f of our observed sequence is a (possibly) scaled
version of an unknown member of the family of ε-contaminated normal distributions,
Pε = {(1 −ε) + εG : G ∈F},
where  is the standard normal distribution, F is the set of all suitably smooth distribution functions,
and ε ∈(0, 1) is the known fraction of contamination (this is no loss of generality, since ε may always
be estimated if unknown). Note that this study straightforwardly reduces to the additive Gaussian noise
case, by setting the mixture parameter ε = 0, and is in that sense more general.
For ﬁxed model order, the expectation of the MDL criterion is the entropy, plus a penalty term which
is independent of both the distribution and the functional form of the estimator. In accordance with the
minimax principle, we seek the least favorable noise distribution and evaluate the MDL criterion for
that distribution. In other words, we solve a minimax problem where the entropy is maximized over
all distributions in Pε, and the description length is minimized over all estimators in S. The saddle-
point (provided its existence) yields a minimax robust version of MDL, which we call the Minimax
Description Length (MMDL) criterion.
In [68], it is shown that the least favorable distribution in Pε, which also maximizes the entropy,
is one which is Gaussian in the center and Laplacian (“double exponential”) in the tails, and switches
from one to the other at a point whose value depends on the fraction of contamination ε.
Proposition 1.
The distribution fH ∈Pε that minimizes the negentropy is
fH(c) =
⎧
⎪⎨
⎪⎩
(1 −ε)φσ(a)e
1
σ2 (ac+a2)
c ≤−a,
(1 −ε)φσ(c)
−a ≤c ≤a,
(1 −ε)φσ(a)e
1
σ2 (−ac+a2) a ≤c,
(9.34)

1.09.2 Wavelets: A Multiscale Analysis Tool
491
where φσ is the normal density with mean zero and variance σ 2, and a is related to ε by the equation
2
φσ(a)
a/σ 2 −σ( −a)

=
ε
1 −ε .
(9.35)
1.09.2.4.4
Coding for worst case noise
Let the set of wavelet coefﬁcients obtained from the observed signal be denoted by CN = {Cx
1 , Cx
2 , . . . ,
Cx
N} as a time series without regards to the scale, and where the superscript indicates the corresponding
process. Let exactly K of these coefﬁcients contain signal information, while the remainder only contain
noise. If necessary, we re-index these coefﬁcients so that
Cx
i =
Cs
i + Cn
i
i = 1, 2, . . . , K,
Cn
i
otherwise.
(9.36)
By assumption, the set of noise coefﬁcients {Cn
i } is a sample of independent, identically distributed
random variates drawn from Huber’s distribution fH. It follows, by Eq. (9.36), that the observed coef-
ﬁcients Cx
i obey the distribution fH(c −Cs
i ) when i = 1, 2, . . . , K, and fH(c) otherwise. Thus, the
likelihood function is given by
ℓ(CN; K) =

i≤K
fH(Cx
i −Cs
i )

i>K
fH(Cx
i ).
Since fH is symmetric and unimodal with a maximum at the origin, the above expression is maximized
(with respect to the signal coefﬁcient estimates

Cs
i

) by setting

Cs
i = Cx
i
for i = 1, 2, . . . , K. It follows that the maximized likelihood (given K) is
ℓ∗(CN; K) =

i≤K
fH(0)

i>K
fH(Cx
i ).
Thus, the problem is reduced to choosing the optimal value of K, in the sense of minimizing the MDL
criterion,
L(CN; K) = −log ℓ∗(CN; K) + K log N = −

i≤K
log fH(0) −

i>K
log fH(Cx
i ) + K log N. (9.37)
Neglecting terms independent of K, this is equivalent to minimizing
˜L(CN; K) =
1
2σ 2

i>K
η(Cx
i ) + K log N,
where
η(c) =
c2
if |c| < a,
a|c| −a2
otherwise
is proportional to the exponent in Huber’s distribution fH. This can simply be achieved by a thresholding
scheme [68].

492
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
Proposition 2.
1. When log N >
a2
2σ 2 , the coefﬁcient |Cx
i | is truncated if
|Cx
i | < a
2 + σ 2
a log N.
2. When log N ≤
a2
2σ 2 , the coefﬁcient |Cx
i | is truncated if
|Cx
i | < σ

2 log N.
Remarks.
More ample details may be found in [68].
When σ 2 →0, the thresholding scheme reduces to Case 2, and Cx
i is never truncated; since
this represents the no-noise case, it is reasonable that all coefﬁcients should be retained in the
reconstruction. On the other hand, for large σ 2, the thresholding scheme reduces to Case 1, which
is more conservative. For σ 2 →∞, the signal-to-noise ratio becomes zero and the best one can
do is to estimate the signal as identically zero.
Similarly, when a →∞, the noise distribution becomes purely Gaussian, and the thresholding
scheme reduces to Case 2, as expected. The resulting threshold of this particular noise case coincides
with the results of [69,70] and is qualitatively similar to that derived in [71]. On the other hand, when
a →0, the noise distribution becomes purely Laplacian, and the thresholding scheme reduces
to Case 1.
Finally, when N →1, the thresholding scheme reduces to Case 2, suggesting that outliers are
unlikely to occur in a small sample, and it is hence more reasonable to assume purely Gaussian
noise. On the other hand, for large N, the thresholding scheme reduces to Case 1, since outliers are
highly likely to occur in a large sample.
It is important to distinguish the minimax error result obtained in [69] which was achieved over a
signal smoothness class, from those discussed here and derived in [68] which are obtained over a
family of noise distributions.
1.09.2.4.5
Numerical experiments
In the examples that follow, we demonstrate the performance of the robust thresholding procedure
described above, and compare it with that of the thresholding scheme based upon the assumption of
normally distributed noise.
Example I.
Using WAVELAB (available from the Stanford Statistics Department, courtesy of D. L.
Donoho and I. M. Johnstone), we synthesized a broken ramp signal of length N = 1024. This signal
admits an efﬁcient representation in a wavelet basis, i.e., one with very few non-zero coefﬁcients. The
noise was additive and i.i.d., obeying a N(0, σ 2) distribution contaminated by a fraction ε = 10%
of white Gaussian noise with distribution N(0, 9σ 2). The overall Signal-to-Noise Ratio (SNR) was
maintained at 10 dB (see Figure 9.16, top).

1.09.2 Wavelets: A Multiscale Analysis Tool
493
0
100
200
300
400
500
600
700
800
900
1000
−1
−0.5
0
0.5
t
Xn(t)
Noisy Signal
0
100
200
300
400
500
600
700
800
900
1000
−1
−0.5
0
0.5
t
Xh(t)
Classical Reconstruction
0
100
200
300
400
500
600
700
800
900
1000
−1
−0.5
0
0.5
t
Xh(t)
Robust Reconstruction
FIGURE 9.16
Noisy ramp signal, and its Gaussian and robust reconstructions.

494
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
−25
−20
−15
−10
−5
0
5
0
5
10
15
20
25
30
SNR
Cumulative Error
Cumulative Error for Reconstructions over 100 Realizations
−.: L2 Error for Gaussian Reconstruction
−−: L2 Error for Robust Reconstruction
−+: L1 Error for Gaussian Reconstruction
−: L1 Error for Robust Reconstruction
FIGURE 9.17
L1 and L2 error performance versus SNR, for the Gaussian and robust estimators.
We implemented two estimators, the ﬁrst one of which is based on a purely Gaussian noise assumption
(i.e., ε = 0), and where the thresholding scheme due to [72,79] was used. The second was the MMDL
robust estimator described above. The reconstructions based on each estimator appear in Figure 9.16.
As may easily be observed, and in contrast to the MMDL technique, the Gaussian assumption induces
a high susceptibility to outliers.
Monte-Carlo simulations were carried out to evaluate the reconstruction performance over a range of
SNRs. At each value of the SNR, 100 experiments were conducted, and the cumulative reconstruction
error is displayed in Figure 9.17. The robust estimator uniformly outperforms the classic estimator in
both L1 and L2 errors over a wide range of SNRs. Furthermore, the performances of the Gaussian
and robust estimators become indistinguishable at high SNRs, i.e., small noise variance, showing that
robustness does not come at the cost of reduced efﬁciency.
Bounding the reconstruction error.
Although the robust estimator-based reconstruction error is much improved it is still potentially
unbounded. As further discussed in [68], and due to the compactness of wavelets, unbounded noise
will still result in unbounded reconstruction error, a property that may be considered undesirable.

1.09.2 Wavelets: A Multiscale Analysis Tool
495
This problem may be circumvented by making the assumption that the signal has bounded energy;
in that case, one of at least two alternatives is possible:
1. In practice, the signal is known to be bounded, and prior knowledge of the physical properties
of the signal may be used to determine the ∥· ∥∞of the sequence of signal wavelet coefﬁcients
{Cs
i }. This information may be used to truncate observed coefﬁcients {Cx
i } not only below, as
discussed earlier, but also above.
2. In the absence of such prior knowledge, it may still be possible to bound the reconstruction
error through an adaptive supremum–secondary thresholding scheme based upon some repre-
sentation criterion, e.g., entropy.
The ﬁrst of these approaches uses the following modiﬁed thresholding: let α > 0 be an upper
bound on the magnitude of the signal coefﬁcients; then,

Cs
i =
⎧
⎪⎨
⎪⎩
0
if |Cx
i | ≤a
2 + σ 2
a log N,

Cs
i = Cx
i
if a
2 + σ 2
a log K ≤|Cx
i | ≤α,
α sgn(Cx
i ) if α ≤|Cx
i |
provided that log N > a2/2σ 2 and α > (a/2) + (σ 2/a) log N. The graph shows that although
the robust estimator’s reconstruction error initially grows more slowly than that of the Gaussian
estimator, the two errors soon converge as the variance of the outliers grows. The reconstruction
error for the bounded-error estimator, however, levels off past a certain magnitude of the outlier,
as expected.
Sensitivity analysis for the fraction of contamination.
Although such crucial assumptions as the normality of the noise or exact knowledge of its variance
σ 2 usually go unremarked, it is often thought that Huber-like approaches are limited on account
of the assumption of known ε. We demonstrate the resilience and robustness of the approach by
studying the sensitivity of the estimator to changes in the assumed value of ε.
Figure 9.18 shows the total reconstruction error as a function of variation in the true fraction
of contamination ε. In other words, an abscissa of “0” corresponds to an assumed fraction of
contamination equal to the true fraction; larger abscissas correspond to outliers of larger magnitude
than assumed by the robust estimator, and vice versa. Clearly, the Gaussian estimator assumes
zero contamination throughout. The ﬁgure shows that the reconstruction error for the Gaussian
estimator grows very rapidly as the true fraction of contamination increases, whereas that of the
robust estimator is nearly ﬂat over a broad range. This should not come as a surprise: outliers are,
by deﬁnition, rare events, and for a localized procedure such as wavelet expansion, the precise
frequency of outliers is much less important than their existence at all.
Example II.
The example above assumed a ﬁxed wavelet basis. As discussed above, however, highly
non-stationary signals can be represented most efﬁciently by using an adaptive basis that automatically
chooses resolutions as the behavior of the signal varies over time. Using an L2 error criterion we search
for the best basis of a chirp and show the reconstruction in Figure 9.19 (see [80] for more details).
The separability property of most carefully designed wavelets, and their inability to achieve ﬁne tuned
directional analysis has led to seeking alternative wavelets which are more sensitive to arbitrary curves

496
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
−40
−20
0
20
40
60
0
0.005
0.01
0.015
0.02
0.025
Percent Change in Epsilon
Reconstruction Error
Robust Estimator Sensitivity
−40
−20
0
20
40
60
0
0.005
0.01
0.015
0.02
0.025
Percent Change in Epsilon
Reconstruction Error
Gaussian Estimator Sensitivity
FIGURE 9.18
Error stability vs. variation in the mixture parameter ε.
0
200
400
600
800
1000
−1500
−1000
−500
0
500
1000
m (samples)
s[m]
0
200
400
600
800
1000
−1000
−500
0
500
1000
1500
m (samples)
s[m]
FIGURE 9.19
A noisy chirp reconstructed from its best basis and denoised as shown in the ﬁgure on the right.

1.09.3 Curvelets and Their Applications
497
and image edges. These include the introduction of curvelets, contourlets (an efﬁcient implementation
of curvelets) and to shearlets, which are respectively discussed in more detail, in the next three sections.
1.09.3 Curvelets and their applications
In this section, we introduce the 2D curvelet transform developed by Candès and Donoho around
2000 [4]. Initially, they proposed the ridgelet transform in Cartesian Fourier plane [81] in order to
locate segmented curve features in an image. Soon, they discovered that multi-scale partition of the
Fourier plane using polar coordinate parameters can provide much simpler structures. This led to their
development of the current continuous curvelet transform. This is closely related to Smith’s parabolic
scaling based transform using the Fourier Integral Operators [82]. The convenience in using polar
coordinates in deﬁning the continuous curvelet transform indeed has a cost in implementing it in the
digital world. Candès, Donoho, and their collaborators have also introduced the discretized curvelet
transform, in which, the discrete Fourier transform involving polar coordinates has been resampled
using Cartesian coordinates [83].
It is worth to point out that the varying numbers of wedge-shaped atoms are used across scales when
the anisotropic curvelets are used to partition the Fourier plane. This is very different from the classical
wavelets, in which only a ﬁxed number of isotropic squares are used. In 2-D spatial plane, the curvelet
atoms effective impact is similar to applying differential operators to an image along selected directions
to measure the regularities in localized areas that can be contained inside rotated rectangles. Large
coefﬁcients can be obtained when a differential operator is applied across the discontinuities along a
curve, and thus can be used to characterize salient features in an image.
To start, we ﬁrst review the conversion between polar coordinates and Cartesian coordinates. In the
Fourier frequency plane, a point  = (x, y) with Cartesian coordinates, can be converted to polar
coordinates (r, σ) by
r = ±|| = ±

2x + 2y,
σ = arctan (y/x),
assume we conﬁne the angle σ to ( −π/2, π/2) (the angle between the polar axis and the ray from the
pole to the point). Meanwhile, the polar coordinates (r, σ) can be converted to the Cartesian coordinates
(x, y), see Figure 9.20, by
x = r cos σ,
y = r sin σ.
(9.38)
Let Rθ be the operator that rotates a point  in the counter clockwise (ccw) direction around the
origin by an angle of θ radians, the end point in polar coordinates will be Rθ() = (r, σ + θ). In
Cartesian coordinates, the operator can be written as a matrix
Rθ =
cos θ
−sin θ
sin θ
cos θ

,
and the end point of the rotation, Rθ() = Rθ · , is the multiplication of the rotating matrix and the
vector  ( can be viewed as a column vector).
The following proposition states that, when the input of a function is rotated by θ radians in the ccw
direction, its Fourier transform is also rotated by θ radians in the Fourier plane, see Figure 9.21.

498
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
FIGURE 9.20
The polar and rectangle coordinates.
FIGURE 9.21
First row, left, the selected Fourier content of an image, right, its corresponding spatial waveform. Second
row, left, the Fourier content is rotate ccw by 45◦, right, its corresponding spatial waveform is also rotated
ccw by 45◦.

1.09.3 Curvelets and Their Applications
499
Proposition 3.
Let the Fourier transform of f (x) ∈L2(R2) be F(), we have
F(Rθ()) =

R2 f (Rθ(x))dx.
(9.39)
1.09.3.1 The continuous curvelet transform
The continuous curvelet transform is deﬁned as the inner product of a function with curvelet atoms at
various dilating scales, rotating angles, and translating locations. It calculates the energy of the function
correlated to each curvelet atom so that the frequency content of the function can be examined separately
in each localized subband. At each scale, the curvelet atoms are generated from rotating a smooth 2-D
window with compact support in its Frequency domain, and shifting its waveform in spatial domain to
various locations. This window is called the mother curvelet at scale a, see Figure 9.22.
1.09.3.1.1
The mother curvelets
At scale a, the Fourier transform of a mother curvelet is written as 
a(x, y) in Cartesian coordinates,
or 
a(r, σ) in the polar coordinates. It is deﬁned using two 1-D windows, along the radial and angular
directions, and is given by

a(r, σ) = a3/4
1(ar)
2
 σ
√a

,
0 < a ≤1,
(9.40)
where the radial (or amplitude) window, 
1(r), is a smooth, bandpass, and non-negative real-valued
function that has its support r ∈[1/2, 2]. This interval is chosen for the convenience of using dyadic
intervals for discretization. The angular (or phase) window, 
2(σ), is a smooth, lowpass, and non-
negative real-valued even function that has its support σ ∈[−1, 1].
At scale a, the mother curvelet 
a(r, σ) is constructed by scaling the two window functions, 
1 and

2, in two different ways. The support of this mother curvelet is a single polar wedge with an area of
Const. · a−3/2, and is located inside

(r, σ) : 1
2a < r < 2
a , |σ| < √a

. It is symmetric with respect to
the polar axis. The length of the wedge in the polar axis direction is at the same order as a−1, and the
larger arc length in the angular direction is at the same order as a−1/2, see Figure 9.22. Approximately,
FIGURE 9.22
A mother curvelet window 
a() at scale a = 1.

500
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
the wedge has width ≈length2. It can be observed that the mother curvelets at two different scales
a, a′, with a ̸= a′, have no scaling relation between them, so they differ slightly at various scales.
1.09.3.1.2
The amplitude and angular windows
The choice of the amplitude and angular windows not only decides the properties of the mother curvelets,
but also decides the properties of the curvelet atoms. The above construction shows that the curvelet
windows have compact supports in the Fourier plane because both 
1 and 
2 have compact sup-
ports. Further, the following Proposition 4 [84] implies that their corresponding spatial waveforms
ψ1(x), ψ2(t) ∈C∞. Hence the mother curvelets are C∞in the spatial plane.
Proposition 4.
A function ψ(x) is bounded and n times continuously differentiable with bounded
derivatives if its Fourier transform 
(x) satisﬁes
 +∞
−∞
|
(x)|(1 + |x|n)dx < +∞.
(9.41)
The compact support of a mother curvelet in the Fourier plane indicates that the spatial waveform of
the mother curvelet does not have a compact support in the spatial plane. Its waveform spreads out and
is known as the side robe effects. To allow the curvelets to focus on localized areas in the spatial domain
for detecting the singularities, the windows 
1 and 
2 should be chosen as smooth as possible in the
Fourier plane so that their spatial waveforms decay rapidly. The rapid decay of a function is deﬁned as,
there exists a constant Mn such that, the following is true for all integer n > 0 for f (x),
| f (x)| ≤
Mn
(1 + |x|n),
∀x.
(9.42)
Proposition 4 is also true for the inverse Fourier transform. If the spatial waveform of a mother
curvelet decays fast, then it indicates that Fourier windows 
1(r) and 
2(σ) should be Cn. Therefore,
the windows 
1 and 
2 should be smooth so they be n times continuously differentiable. In the literature,
the windows are often required to be C∞to ensure the spatial functions decay rapidly.
At scale a, the mother waveform oscillates along the horizontal direction with most of its support
located inside an ellipsoid shaped area along a ridge in the vertical direction. The ridge is deﬁned as
the (line) locations where the amplitude of a function reaches its maximum amplitude. This ellipsoid
shape becomes “needle-like” shape when the scale a gets smaller. Namely, the area becomes longer and
narrower.
Finally, the amplitude and angular windows 
1 and 
2 should satisfy the following admissibility
conditions:
 ∞
0

2
1(r)
r
dr = C
1,
(9.43)
 1
−1

2
2(σ)dσ = C
2.
(9.44)
The admissibility conditions in Eqs. (9.43) and (9.44) can be normalized such that C
1 = 1, and
C
2 = 1. Equation (9.43) indicates that 
1(0) = 0, namely, 
1(r) is the transfer function of a bandpass
ﬁlter.

1.09.3 Curvelets and Their Applications
501
A continuously differentiable Fourier transfer function, 
1, that satisﬁes 
1(0) = 0, is sufﬁcient to
ensure that 
1 satisﬁes the admissibility condition in Eq. (9.43) [84]. The selection of the windows for
constructing the curvelets will be further discussed in detail in the discrete case.
1.09.3.1.3
The curvelet atoms
Deﬁnition 8.
A curvelet atom is referred to an element in the set {ψa,θ,b(x), x ∈R2, a ∈
	
0, 1

, θ ∈

0, 2π

, b ∈R2}.ItcanbedeﬁnedastheinverseFouriertransformofafunction,
a,θ,b(),asfollowing

a,θ,b() = exp (−j ⟨b, ⟩)
a(Rθ),
(9.45)
where 
a() is a mother curvelet. The polar coordinate format of 
a(Rθ) is equal to 
a(r, σ + θ).
Speciﬁcally, at scale a, the curvelet atom at θ = 0, b = (0, 0) (we usually use 0 = (0, 0)) is the
mother curvelet because

a,0,0() = 
a().
(9.46)
The spatial waveform of the curvelet atom, ψa,θ,b(x), takes a complex value, and can be written in
terms of a mother curvelet as,
ψa,θ,b(x) = ψa,0,0(x)(Rθ(x −b)),
a ∈
	
0, 1

, θ ∈

0, 2π

, b ∈R2.
(9.47)
Figure 9.21 shows two curvelet atoms (in both Fourier and spatial planes) at different orientations.
Although complex valued curvelet atoms are capable of separating the amplitude and the phase infor-
mation contained in a 2-D function, in many situations the real-valued curvelet atoms can be built by
modifying the mother curvelet windows, namely, replacing the radial window 
1(r) with 
R,1(r) =

1(r) + 
1(−r), or replacing the angular window 
2(σ) with 
R,2(σ) = 
2(σ) + 
2(σ + π). It
results in mother curvelets supported on two polar wedges that are symmetric with respect to the x, y
axes in the rectangle coordinates. Figure 9.23 shows the frequency support (the gray area) of a mother
curvelet that generates complex-valued curvelet atoms in spatial domain, while Figure 9.24 shows the
frequency support of a mother curvelet that generates real-valued curvelet atoms.
ThecurveletatomsgeneratedbythemothercurveletsdissecttheFourierplaneintolocalizedsubbands
so that the Fourier plane is packed redundantly with all the dilated and rotated curvelet atoms. Note that
shifting in the spatial plane has no effect on the amplitude of the content in the Fourier plane.
1.09.3.1.4
The deﬁnition of continuous curvelet transform
Deﬁnition 9.
Let f (x) ∈L2(R2), and let {ψa,θ,b(x)}a∈
	
0,1

,θ∈

0,2π

,b∈R2 be the families of curvelet
atoms deﬁned above. The continuous curvelet transform, see Figure 9.25, is deﬁned as
C f (a, θ, b) =

f , ψa,θ,b

=

R2 f (x)ψ∗
a,θ,b(x)dx,
(9.48)
where {C f (a, θ, b), a ∈
	
0, 1

, θ ∈

0, 2π

, b ∈R2} are called the curvelet coefﬁcients of the
function f (x).
Note that here we mentioned the families of curvelet atoms. This is because each mother curvelet
at scale a generates a family of curvelet atoms. The curvelet transform projects a 2-D function into the

502
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
FIGURE 9.23
Shown here is a mother curvelet that generates a family of complex valued curvelets. The mother curvelet
has its frequency content supported in a single wedge, represented by the gray area.
FIGURE 9.24
Shown here is a mother curvelet that generates a family of real-valued curvelets. The mother curvelet has its
frequency content supported in a pair of symmetric wedge, represented by the gray area.

1.09.3 Curvelets and Their Applications
503
(a)
(b)
(d)
(c)
FIGURE 9.25
(a) The real part of a wavelet atom in the spatial space. (b) The Fourier transform of the wavelet atom shown.
(c) The graph of oscillatory function along θ direction taken from the real part of the wavelet atom. In this
picture, the horizontal direction is the pixel location along θ, the vertical direction is the function’s value.
(d) The Fourier transform from (b) is zoomed to the center 64 × 64 pixels for a better view of the content.
The pixels size is shown in (a, b, d).
familiesofcurveletatomstoreceiveaseriesofdecomposedcoefﬁcients. Eachcoefﬁcientreﬂectsthecor-
relation between the function and a curvelet atom focusing inside a localized area. Typically, the curvelet
coefﬁcients are calculated in the Fourier plane using the following result, which is the Parseval’s formula.
Proposition 5.
The curvelet transform can be calculated in the Fourier plane as,
C f (a, θ, b) =

F, 
a,θ,b

=
1
(2π)2

R2 F(x, y)
∗
a,θ,b(x, y)dx dy,
(9.49)
for a ∈
	
0, 1

, θ ∈

0, 2π

, b ∈R2.
1.09.3.1.5
The properties of the curvelet transform
The vanishing moments and the oscillatory behavior
In the classical wavelet theory, a wavelet with n vanishing moments has been interpreted as a multiscale
differential operator of order n. A similar concept can be extended to the curvelets in parallel. A mother

504
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
FIGURE 9.26
A curvelet atom in its spatial plane and Fourier plane.
curvelet ψa,0,0 is said to have q vanishing moments if its waveform in spatial plane satisﬁes
 ∞
−∞
ψa,0,0(x, y)xndx = 0,
for all 0 < n ≤q.
(9.50)
The curvelet windows we selected can have inﬁnitely many vanishing moments as Eq. (9.50) is satisﬁed
for any q.
The curvelet window that has inﬁnitely many vanishing moments ensures that the coefﬁcients of a
smooth image function tend to zero when a mother curvelet is centered at x. In the spatial plane, a rotated
curvelet atom with oscillatory direction at θ radians, if shifted to the spatial location b ∈R2, plays the
role of an angular differential operator in a localized area centered at b, to measure the regularity along
the orientation, see Figure 9.26. This area is referred to as the ellipse support area. If an image function
satisﬁes a localized Lipschitz regularity along the radial direction at location x, then a polynomial can be
used to approximate the image function in a localized area. The aforementioned inﬁnitely many vanish-
ing moments of the curvelet windows will therefore lead to many curvelet coefﬁcients tending to zero.
The decay rate of the curvelet coefﬁcients
As we mentioned above, the requirement for inﬁnitely many vanishing moments ensures that the
curvelets decay rapidly in the spatial domain. A curvelet coefﬁcient therefore measures the correla-
tion between a function and a curvelet atom focusing in its ellipse support area over the spatial support.
The resulting coefﬁcients may be classiﬁed into the following three cases [7]:
1. The magnitude of a curvelet coefﬁcient tends to zero if the ellipse support of the curvelet atom does
not intersect with a singularity of the function.
2. The magnitude of a curvelet coefﬁcient tends to zero if the ellipse support of the curvelet atom
intersects with a singularity, but its ridge direction is not oriented at the tangential direction of a curve.

1.09.3 Curvelets and Their Applications
505
3. The magnitude of a curvelet coefﬁcient decays slowly if the ellipse support of the curvelet atom
intersects with a discontinuity, and it is ridge is oriented along the tangential direction of the curve
where the singularities are located. More precisely, the curvelet oscillates along the normal direction
of the curve to measure the regularities in a local area centered at b.
1.09.3.1.6
The reproducing formula
Similarly to the classical wavelets, the admissibility conditions, Eqs. (9.43) and (9.44), ensure that
all the curvelet atoms cover the Fourier plane completely, so the curvelet transform is a redundant
representation of a function that can be perfectly reconstructed.
Theorem 1.
If the Fourier Transform of a highpass function f ∈L2(R2) vanishes (equal to 0) for
|| < 2/a0, this function can be perfectly reconstructed from the curvelet coefﬁcients according to the
following Calderon-like reproducing formula:
f (x) =
 2π
0
 a0
0

R2 C f (a, θ, b)ψa,θ,b(x) db
a1/2
da
a3/2
dθ
a .
(9.51)
The proof of this theorem is deferred to [7].
This reproducing formula can be extended to reconstruct any function f ∈L2(R2) that includes
both low frequency and high frequency components. To do so, we can introduce a scaling function,
φ(x), the so-called father curvelet, through its Fourier Transform, (), given by,
2() = 1 −
2(),
where 
2() =
 a0||
0
|
1(r)|2 dr
r .
(9.52)
The function () can be viewed as an aggregation of curvelets at scales greater than a0, written as
() =
 +∞
a0
|
1(r||)|2
r
dr =
 +∞
a0||
|
1(r)|2
r
dr
(9.53)
and may further be written as
() =
⎧
⎨
⎩
1
|| ≤1/(2a0),
(1 −
2())1/2
1/(2a0) < || < 2/a0,
0
|| ≥2/a0.
Note that a0 is limited to 0 < a0 ≤1, it is typically set to a0 = 1 as in the following.
The high frequency content in ()equaling zero indicates that the father curvelet is the impulse
response of a lowpass frequency ﬁlter. Shifting the father curvelet φ(x) builds the shifting atoms
φb(x) = φ(x −b), b ∈R2. It can be seen that the father curvelet is isotropic and is not scaled. It only
produces an atom that is shifted to spatial location b.
Theorem 2.
Any f ∈L2(R2) can be reproduced as
f (x) =

R2 ⟨φb, f ⟩φb(x)db +
 1
0
 2π
0

R2 C f (a, θ, b)ψa,θ,b(x) db
a3/2
da
a
dθ
a1/2
(9.54)

506
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
and it satisﬁes the Plancherel’s formula
∥f (x)∥2
2 =

R2 ⟨φb, f ⟩2db +
 1
0
 2π
0

R2 |C f (a, θ, b)|2 db
a3/2
da
a
dθ
a1/2 .
(9.55)
The term db
a3/2
da
a
dθ
a1/2 may be viewed as the reference measure in the parameter space of (a, θ, b). It
suggeststhatthecurveletsuseanisotropicmeasuringoftheunitcellinthespaceof(a, θ, b).Thistheorem
ensures that both the curvelets and the classical wavelets can be decomposed into a low frequency
component and a combination of detail components. The polar wedge shaped curvelet windows in the
frequency domain outperform the classical wavelet in 2-D case, as it allows one to study 2-D functions
by measuring the energy inside the windows oriented along selective orientations to efﬁciently identify
the regularities.
1.09.3.2 The discrete curvelet transform
The continuous curvelets can be sampled along its scale, rotation, and translation parameters to form
a pyramid of discrete curvelets for numerical computation. A tight frame can be adopted for efﬁciency
so that a 2-D function may be projected onto the discrete curvelet frame with the least number of large
coefﬁcients. Here only one tight frame is introduced. First we introduce the window functions to be
used in generating the curvelets.
1.09.3.2.1
The selection of windows
As we mentioned earlier, the mother curvelets are primarily deﬁned by two window functions, 
1
and 
2. A good choice is the Meyer wavelet windows in the Fourier domain. Figure 9.27 shows the
highpass and lowpass windows that are orthogonal and overlapping. The lowpass window in the 1-D
Fourier domain may be used as the angular window, 
2(σ), and is deﬁned as

2(σ) =
⎧
⎨
⎩
1
|σ| ≤1/3,
cos
	 π
2 β(3|σ| −1)

1/3 < |σ| ≤2/3,
0
|σ| > 2/3.
(9.56)
The highpass window in the 1-D Fourier domain may also be used as the radial window 
1(r) and is
deﬁned as

1(r) =
⎧
⎪⎪⎨
⎪⎪⎩
0
|r| ≤2/3, and |r| ≥8/3,
sin

π
2 β

3|r|
2 −1

2/3 < |r| ≤4/3,
cos

π
2 β

3|r|
4 −1

4/3 < |r| < 8/3,
where β(x) is any smooth function that deﬁnes the transition bands. It is deﬁned on [0, 1] and satisﬁes
β(x) + β(1 −x) = 1,
x ∈R.
(9.57)
The following admissibility conditions ensure that the discretized curvelet atoms generated from
these windows can completely cover the Fourier plane.

1.09.3 Curvelets and Their Applications
507
FIGURE 9.27
The Meyer’s wavelet windows: lowpass 
2(t), in blue color, and highpass 
1(t), in red color, only its com-
ponent in the positive axis is shown here. (For interpretation of the references to color in this ﬁgure legend,
the reader is referred to the web version of this book.)
Proposition 6.
On a discrete scale, the two windows, 
1 and 
2 selected above, satisfy the following
admissibility conditions,
∞

k=−∞
|
1(2−kr)|2 = 1,
r > 0,
(9.58)
∞

l=−∞
|
2(σ −l)|2 = 1,
σ ∈R.
(9.59)
The proof of the proposition only needs to be shown in the overlapped regions between two adjacent
windows; for example, for 
1(r), when r ∈(2/3, 3/4), we have
∞

k=−∞
|
1(2−kr)|2 = |
1(r)|2 + |
1(2r)|2 = sin2
π
2 β
3|r|
2
−1

+ cos2
π
2 β
3|2r|
4
−1

= 1.
(9.60)

508
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
Refer to [85] for details of the proof for a different set of window functions. Note that Eq. (9.59) is the
same as Eq. (9.44), while Eq. (9.58) is similar to Eq. (9.43) for the continuous case when C
1 = ln 2.
The selected windows can be scaled to ﬁt supp{
1(r)} = [1/2, 2], and supp 
2(σ) = [−1, 1].
The discretized windows are used in the following to build the discrete curvelet atoms. Meanwhile,
the coarse scale father curvelet window can be deﬁned as the lowpass window (r) in the radial direction
which satisﬁes
|(r)|2 +

k≥0
|
1(2−kr)|2 = 1.
(9.61)
The choice of β(x) determines the transition band of the Meyer window. The deﬁnition of the β(x),
for example, can be chosen as
β(x) = x4(35 −84x + 70x2 −20x3),
(9.62)
the resulting windows 
1 and 
2 are C3. Or
β(x) = x3(5 −5x + x2),
(9.63)
the resulting windows 
1 and 
2 are C2. Or
β(x) = x2(3 −2x),
(9.64)
the resulting windows 
1 and 
2 are C1. The 1-D window 
1 has inﬁnitely many vanishing moments in
the parameter r because 
1(r) = 0 for |r| < 2/3. The mother curvelets constructed from the windows

1 and 
2 should have inﬁnitely many moments too [83,85].
1.09.3.2.2
Constructing a tight frame of curvelet atoms
Now we are to construct a tight Parseval curvelet frame by sampling the continuous curvelets. With a
tight frame, the whole Fourier plane can be covered with discrete curvelet atoms with minimum overlap,
and the following equation is satisﬁed,

n∈
| ⟨f , ψn⟩|2 = C∥f ∥2
2.
(9.65)
The three parameters (a, θ, b) will be sampled in order to obtain a tight frame. First, at each scale k, the
radial scale is sampled on a dyadic interval at ak = 2−k, k ≥0, resulting in the selection of the mother
curvelet, ψk,0,0, deﬁned as the inverse Fourier transform of 
k,0,0(r, σ). 
k,0,0(r, σ) is deﬁned using
the polar coordinates as,

k,0,0(r, σ) =
⎧
⎨
⎩

1(2−k · r)
2

2(k+2)/2σ
π

2−3k/4, k is an even integer,

1(2−k · r)
2

2(k+1)/2σ
π

2−3k/4, k is an odd integer.
Second, the rotation angular parameter is sampled at θk,l, given as
θk,l =

πl2−(k+2)/2, k is an even integer, 0 ≤l ≤2
(k+2)
2
+1 −1,
πl2−(k+1)/2, k is an odd integer, 0 ≤l ≤2
(k+1)
2
+1 −1.

1.09.3 Curvelets and Their Applications
509
For example, at k = 0 and k = 1, θk,l = π
2 l,l = 0, 1, 2, 3; at k = 2 and k = 3, θk,l = π
4 l,l =
0, 1, 2, . . . , 7. It can be deduced that
2
(k+2)
2
+1−1

l=0

2
 
2(k+2)/2(σ + θk,l)
π
!
2
= 1,
for k is an even integer
(9.66)
and
2
(k+1)
2
+1−1

l=0

2
 
2(k+1)/2(σ + θk,l)
π
!
2
= 1,
for k is an odd integer.
(9.67)
This results in the discrete curvelet atoms,
ψk,l,b(x) = ψk,0,0
	
Rθk,l(x −b)

(9.68)
Finally, the parameter b is sampled at
bk,l
s
= R−1
θk,l(s12−k, s22−k/2),
where the bk,l
s is the grids deﬁned by rotating the dyadic points by an angle θk,l. The discrete curvelet
atoms are given as,
ψk,l,s(x) = ψk,0,0

Rθk,l

x −bk,l
s

,
s = (s1, s2).
(9.69)
This curvelet frame consists of interleaved curvelet atoms, separately selected from the samplings
of two adjacent scales, the even and odd scales. Under this tight frame, the discrete curvelet transform
coefﬁcients are given by
C fk,l,s =

f , ψk,l,s

.
(9.70)
The coefﬁcients are calculated in the Fourier plane using the following formula based on the mother
curvelet:
C fk,l,s =
1
(2π)2

R2 F()
∗
k
	
Rθk,l()

e
j
"
bk,l
s ,
#
d.
(9.71)
Theorem 3.
For a highpass function f ∈L2(R2), Eqs. (9.58), (9.66), (9.67) guarantee that a repro-
ducing formula can be obtained by
f =

k

l

s=(s1,s2)∈R2
C fk,l,sψk,l,s,
(9.72)
and the energy conservation is maintained.
∥f ∥2
2 =

k

l

s=(s1,s2)∈R2
|C fk,l,s|2.
(9.73)
Meanwhile, if we add the father curvelet φb(x) into the frame, calculate the father curvelet coefﬁcients
from the inner product between functions φb(x) and f, and put both with the curvelet coefﬁcients, and
we still write them as C fk,l,s, then, any function in L2(R2) can be reconstructed from the integrated
families of the curvelet atoms, and resulting in the same formulas of Eqs. (9.72) and (9.73).

510
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
Under this curvelet tight frame, Candès and Donoho [5] have proved the following theorem, which
estimates the approximation error under the curvelets coefﬁcients. It shows a faster convergence rate
than that using the classical wavelets in 2-D.
Theorem 4.
Let f (x)beatwicedifferentiablefunctioninR2 exceptatdiscontinuitiesalongaC2 curve.
Let f C
N be the N-term approximation of f reconstructed from using the n largest curvelet coefﬁcients
obtained by simple thresholding. The approximation error follows:
$$$ f −f C
N
$$$
2
2 ≤C N −2( log N)3.
The proof can be found in [5].
1.09.3.2.3
Implementation procedure for discrete curvelet transform
The discrete curvelet transform is implemented using a lowpass ﬁlter P0, together with a set of passband
ﬁlters 1, 2, . . . , s. This bank of ﬁlters partition the whole frequency plane into s+1 decompositions
that are concentrated, and are located near certain frequencies in a sequence of dyadic intervals. Indeed,
in the digital curvelet implementation, this bank of ﬁlters are generated using concentric squares instead
of concentric circles.
To generate the concentric squares, see Figure 9.28, a lowpass ﬁlter at scale k is ﬁrst generated using
a smooth Meyer window, (r), following Eq. (9.56), as
k(x, y) = (2−kx)(2−ky).
(9.74)
The P0 refers to the window when k = 0. Then the bandpass ﬁlters k corresponding to bandpass
window functions, 
1,k(x, y), are deﬁned as

1,k(x, y) =

2
k+1(x, y) −2
k(x, y),
k ≥0.
The angular window 
2,k(x, y) in the Cartesian plane is also deﬁned using the function 
2(σ)
in Eq. (9.56),

2,k(x, y) =
⎧
⎪⎨
⎪⎩

2
 2k/2y
x

k is an even integer,

2
 2(k−1)/2y
x

k is an odd integer.
The mother curvelet at scale k can be obtained in the Fourier domain as,

k() = 2−3k/4
1,k()
2,k(),
 = (x, y).
The curvelet atoms at b = 0 can be obtained in the Fourier domain as,

k,l() = 
k(S−1
θk,l) = 2−3k/4
1,k()
2,k

S−1
θk,l()

,
 = (x, y),
where Sθk,l, is the shear matrix, with its inverse S−1
θk,l. Both matrices are given as
Sθk,l =

1
0
−tan (θk,l) 1

and
S−1
θk,l =

1
0
tan (θk,l) 1

.

1.09.3 Curvelets and Their Applications
511
FIGURE 9.28
Shown here are windows at several scales, namely the window at (a) The low frequency level. (b) k = 1 level.
(c) k = 2 level. (d) k = 3 level. (e) k = 4 level. (f) The remaining high frequency at k = 4 level. Note that
number of pixels in x, y direction is shown for each image because of the different size of the window at
different scale.
The selection of θk,l is deﬁned through
tan (θk,l) =
l · 2−k/2
k is an even integer, l = −2k/2, . . . , 2k/2 −1,
l · 2−(k−1)/2
k is an odd integer, l = −2(k−1)/2, . . . , 2(k−1)/2 −1.
The 
k,l(x, y) indeed deﬁnes the Cartesian approximation of the curvelet atoms in the polar Fourier
plane along the horizontal cone direction. Rotating this system by π/2 deﬁnes the Cartesian approxima-
tion along the vertical cone direction. The two systems together deﬁne the Cartesian approximation of
the curvelet atoms [83,86,87]. Using the discretized curvelet atoms, the Curvelab software [88] calcu-
lates the discrete curvelet coefﬁcients using two different approaches, the unequispaced FFTs (USFFT),
and the Wrapping methods. It is believed that the Wrapping method is simpler to understand. Here we
summarize the two implementation procedures in the following.

512
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
The unequispaced FFTs (USFFT) method
The USFFT implements the method that carries out the operation of shearing a curvelet atom to shearing
F according to the following formula:
C f (k,l, s) =
1
(2π)2

R2 F()
k(S−1
θ ) exp

j
"
S−T
θ
b, 
#
dx dy
=
1
(2π)2

R2 F(Sθ)
k() exp ( j ⟨b, ⟩)dx dy
(9.75)
with θ = θk,l, b ≃(s12−k, s22−k/2). Note that 
k() is a real valued function, therefore 
∗
k () =

k() is used in Eq. (9.75). The procedure for implementing the USFFT to calculate the discrete
curvelet coefﬁcients is summarized in the following, for more details, refer to [83,86].
1. Apply a 2-D FFT to a function f (x), and obtain its discrete Fourier samples F(n1, n2), 0 < n1, n2 ≤
N, where N is the sample size.
2. For each scale/angle pair (k,l), resample (or interpolate) F(n1, n2) to obtain sheared sample values,
written as F(n1, n2 −n1 tan θk,l) for (n1, n2) ∈Rk. The Rk is a rectangle of Length = 2k,
Width = 2k/2, that is at the center of the bandpass content of the mother curvelet at scale k.
3. MultiplytheresampledFwiththemothercurveletwindow
k,l,wehaveC fk,l(n1, n2) = F(n1, n2−
n1 tan θk,l)
k,l(n1, n2).
4. Apply an inverse 2-D FFT to each C fk,l(n1, n2) obtained in the previous step to calculate the discrete
curvelet coefﬁcients C f (k,l, s).
In Curveletlab [88], the following package in MATALB is used to decompose the original image into
curvelet coefﬁcients:
% is_real
Type of the transform
%
0: complex-valued curvelets
%
1: real-valued curvelets
Curveletcoef = fdct_usfft (X,is_real);
The inverse curvelet transform is simply computed by “reversing” all the operations of the direct
transform, with some adjustments:
1. For each scale/angle pair (k,l), perform a (properly normalized) 2-D FFT of each curvelet coefﬁcient
array C f (k,l, s), to obtain (C f )k,l(n1, n2) in 
k,l.
2. The (C f )k,l(n1, n2) should be viewed as samples on the sheared grid. Resample them back to the
standard Cartesian grid.
3. Integrate (C f )k,l(n1, n2) from all scales and angles together. This recovers F(n1, n2).
4. Take a 2-D inverse FFT to obtain f (x, y).
Figure 9.29 provides the diagram for implementing the USFFT curvelet transform and its inverse
transform.
In Curveletlab, the following package in MATLAB is used to reconstruct the original image from
the curvelet coefﬁcients with syntax as:
Y = ifdct_usfft(C,is_real);

1.09.3 Curvelets and Their Applications
513
FIGURE 9.29
The USFFT curvelet transform diagram (in black color) and its inverse transform (in red color). (For interpre-
tation of the references to color in this ﬁgure legend, the reader is referred to the web version of this book.)
The computational cost for the forward and inverse transform requires about O(n2 log n) operations,
and O(n2) memory storage.
The wrapping method
The Wrapping is implemented by using the following formula to calculate the curvelet coefﬁcients:
C f (k,l, s) =
1
(2π)2

R2 F()
k(S−1
θ ()) exp ( j ⟨b, ⟩)dx dy.
Note that the S−1
θ b in Eq. (9.75) is replaced by b that takes its value on the rectangle grid. In this method,
the curvelet atoms, 
k,l = 
k(S−1
θk,l()) , should be obtained by applying the shear operation S−1
θk,l to
the rectangle Rk as in the USFFT method. The resulting window should be a trapezoidal shaped region.
The following is the procedure for implementing the Wrapping technique to calculate the discrete
curvelet coefﬁcients,
1. Calculate the 2-D FFT of an image f (x) to obtain the Fourier samples F(n1, n2).

514
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
2. For (k,l), decompose F(n1, n2) into each curvelet window by calculating F(n1, n2)
k,l(n1, n2).
3. Perform wrapping in this stage, namely, wrap this product around the origin in the Fourier plane.
This is done by relabeling the samples.
4. Apply the inverse 2-D FFT to the wrapped F(n1, n2) to calculate the discrete curvelet coefﬁcients
C f (k,l, s).
In Curveletlab, the following package is used to calculate the curvelet coefﬁcients using the wrapping
method with syntax as:
Curveletcoefwrp = fdct_wrapping(X, 0).
The inverse curvelet transform is computed as
1. For each (k,l), perform a properly normalized 2-D FFT of each curvelet coefﬁcient array C f (k,l, s).,
to obtain (C f )k,l(n1, n2) in 
k,l.
2. For each (k,l), multiply the array of the coefﬁcients by the corresponding wrapped curvelet

k,l(n1, n2) which gives (|
k,l|2F)(n1, n2).
3. Unwrap each array (|
k,l|2F)[n1, n2]. with 
k,l(n1, n2) on the frequency grid and integrate them
all to form F(n1, n2).
4. Finally, take a 2-D inverse FFT of F(n1, n2) to obtain f (x, y).
In Curveletlab, the following package is used to reconstruct the original image from the curvelet
coefﬁcients with syntax as:
image = fdct_wrapping_dispcoef(Curveletcoefwrp).
In the wrapping approach, both the forward and inverse transforms are computed in O(n2 log n)
operations, and require O(n2) storage.
1.09.3.3 Applications
Researchers have applied the curvelet transform in solving PDEs and in numerous other applications
including image processing, seismic exploration, ﬂuid mechanics, and data compression [87,89–100].
All applications take advantage of the ﬂexibility of curvelets to selectively capture curve-based features.
For illustration in this manuscript, we limit ourselves to the application of curvelets in image denoising.
The assumption about noisy signals in previous sections, can be similarly made for 2-D noisy images
of interest herein. It is well known that a 2-D isotropic wavelet transform of an image exhibits a large
number of signiﬁcant coefﬁcients at every ﬁne scale, making wavelet denoising more complex as
many more wavelet coefﬁcients are needed for reconstructing edges while smoothing out the noise
[68–70,101]. The development of curvelet theory has shown that discrete curvelet coefﬁcients provide
near-optimal way to represent smooth objects with discontinuities along C2 continuous curves in an
image. The curvelet-based denoising method is similar to that of the wavelet. It consists of determining
whichcurveletcoefﬁcientsrepresentthetransientfeaturesinanimage.Itisshownthatthehighamplitude
curvelet coefﬁcients usually indicate the position of edges, and the low amplitue curvelet coefﬁcients
usually capture noise present in the image. Therefore, the denoising is accomplished by applying a
threshold to the curvelet coefﬁcients.

1.09.4 Contourlets and Their Applications
515
FIGURE 9.30
Shown in the left is a noisy image, in the right is the image after applying the hard threshold denoising
method.
A hard threshold estimator is deﬁned as,
IT (x) =
 x
if |x| ≥T ,
0
if |x| < T .
Applying the thresholds is equivalent to locally averaging the noise contribution present in the image
around the local area the image is smooth over.
Here we assume the noise normal distribution, N(0, σ 2), and denote by y(n) the discrete curvelet
coefﬁcients from a noisy image. The following hard-threshold is applied to estimate the curvelet coef-
ﬁcients, ˜y(n), of the denoised image as follows:
˜y(n) =
 y(n) if |y(n)|σ ≥Ta,
0
if |y(n)|σ < Ta.
The threshold Ta is chosen as scale depended, and can be estimated from the noisy image.
It is shown that the curvelet-based denoising displays higher sensitivity than the wavelet-based
denoising, see Figure 9.30. In curvelet denoising, salient features in the noisy image can be clearly
enhanced, while the noise can be selectively removed [102]. However, even though the pure discrete
curvelet denoising has improved the image denoising method, it is interesting to see better denoising
effects has been achieved by a hybrid approach [89] called total variational method (TV) that combines
the discrete curvelet transform with the classical wavelet method. The TV method has shown a higher
capability in exploiting curve-like features.
1.09.4 Contourlets and their applications
1.09.4.1 Contourlet transform
In this section, we introduce the contourlet transform, proposed by Do and Vetterli [9]. The con-
tourlet transform is a discrete ﬁlter bank implementation that expands multi-dimensional signals into

516
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
multiresolution copies for analysis, approximation, and compression [9,103]. Here we consider only
two dimensional (2-D) signals-images. A ﬁlter bank technique uses multirate signal processing tech-
nique that has been successfully applied in many application areas [41,104]. It is a natural and practical
approach to efﬁciently compute a set of good basis functions for an image [105,106]. The contourlet ﬁl-
ter bank technique proceeds to decompose the frequency domain into angularly oriented directions from
ﬁne to coarse resolution. It implements similar features of the curvelet analysis so that certain frequency
regions in coarse or detailed version can be focused on under a selected basis/frame. This advances
the classical discrete ﬁlter bank technique that is used in the classical wavelet analysis. It allows the
representation of a 2-D signal to be optimized using only a few signiﬁcant coefﬁcients corresponding
to the geometrical features in an image, such as edges with continuous curves.
The multiresolution decomposition ﬁlter bank technique is represented by the Laplacian pyramid
decomposition, originally developed by Burt and Adelson [107] for image coding. Its development has
inspired the usage of discrete ﬁlter bank technique to decompose a 2-D image into images of multireso-
lution in discrete-time wavelet bases. This has further inspired the research for building the connection
between the ﬁlter banks and classical wavelets led by Mallat [84] and Daubechies [20]. In turn, this
has led to the development of the ﬁlter bank technique, which has played an active role in construct-
ing various bases for classical wavelets. It has strengthened the connection between harmonic analysis
and discrete signal processing. A ﬁlter bank with a directional oriented decomposition capacity was
later proposed by Bamberger [108]. The recently proposed contourlet ﬁlter banks have integrated the
Laplacian pyramid (LP) ﬁlter bank and the directional ﬁlter bank (DFB) for multiresolution anisotropic
wavelet decomposition.
The coutourlet transform is a tree-structured ﬁlter bank that cascades multiple LP and DFB ﬁlter
banks iteratively. In general, the ﬁlter banks use maximally decimated ﬁlter banks similar to those
in subband coding, and invoke the usage of multirate signal processing to reduce the computational
complexity. Each ﬁlter bank in the structure can be divided into two stages: an analysis ﬁlter bank and
a synthesis ﬁlter bank. An analysis ﬁlter bank decomposes a signal into different version sub-signals.
Each subsignal is down-sampled to its frequency content concentrating in a segment in the Fourier
plane. A synthesis ﬁlter bank up-samples and combines the output signals from the analysis ﬁlter bank
into one signal. The basic structure of the contourlets is a two channel maximally decimated ﬁlter bank,
referred to as a quadrature mirror ﬁlter (QMF) bank.
In synthesis, a perfect reconstruction is achieved when the output signal equals to the original
signal, which is of interest for an optimal approximation of the original signal. This is useful in signal
compression, as an individual image of reduced size is useful for transmission even though the total
sampling size is increased. To avoid a redundant expansion of a signal, ﬁlters that implement orthonormal
bases are sought. Filters that implement biorthogonal bases or tight frames are, however, necessary in
some cases in order to relax some constraints.
A digital contourlet ﬁlter bank inevitably inherits the distortion problem in digital signal processing;
to avoid this problem, a non downsampling version of the contourlets has been proposed [109], which
is parallel to the non sub-sampled discrete wavelet transform (NSWT) with the à trous algorithm.
Additionally, an M-band method that provided non-redundant DFB was proposed in [110].
In this section, we downplay the distortion problems with downsampling/upsampling to keep our
focus on multiresolution analysis. As a polyphase decomposition is typically used to form a fast com-
puting system for implementing ﬁlter banks [41,106,111], in the following, we start by introducing

1.09.4 Contourlets and Their Applications
517
some basic sampling operations in digital image processing. We then introduce the LP ﬁlter banks, and
the DFB ﬁlter banks before we introduce the contourlets as the combination of the two type ﬁlter banks.
1.09.4.1.1
The downsampling and upsampling operators
Downsampling is a decimation operator that reduces the original image size, while upsampling is the
interpolation operator that increases the original image size. The two operations are the basic techniques
that form the multirate digital system. In the 2-D digital image processing, downsampling is to generate
multiple components of reduced size for the economy of analyzing an image, while upsampling is
used to recover the parent image. In this section and the following, n ∈Z2 is viewed as a column
vector n = (n1, n2)T since matrix multiplication is involved. Here “T ” is referred to the transpose of a
vector/matrix.
Both sampling operations involve a 2 × 2 sampling matrix given by
M =
m1
0
0
m2

,
where m1, m2 ∈Z.
Given a 2-D image, x(n), n = (n1, n2) ∈Z2, with its z-transform X(z), see appendix (A.1), an
M-fold downsampling (decimator) takes x(n) as an input, and produces an output, xd(n), given by
xd(n) = x(Mn)
An M-fold upsampling (expander, or interpolator) takes x(n) as an input, and produces an output,
xu(n), given by
xu(n) =

x(s) if n = Ms, s ∈Z2
0
otherwise.
The z-transform of an M-fold downsampled signal, xd(n), is given as
Xd(z) = |det(M)|−1

k=(k1,k2)∈S
X

z1/m1
1
e−j2πk1/m1, z1/m2
2
e−j2πk2/m2

.
The S is the set of all integer points in Mt, t ∈[0, 1)×[0, 1). The z-transform of an M-fold upsampled
signal, xu(n) is given as
Xu(z) = X(zm1
1 , zm2
2 ), with z = (z1, z2).
The dyadic subsampling occurs in x or y direction when |det(M)| = 2, where det(M) is the
determinant of a matrix M. Dyadic subsampling in both x and y directions occurs when m1 = 2 and
m2 = 2 in the diagonal matrix M. The dyadic sampling is usually used in the digital ﬁlter bank design of
a discrete wavelet transform or discrete anisotropic wavelet transform. Sampling operations that invoke
non-diagonal matrix are to be introduced in the following section.
1.09.4.1.2
The polyphase decomposition
The polyphase decomposition involves subsampling and upsampling operations which use a non-
diagonal matrix, M, of non-singular integer-value, given as
M =
m11
m12
m21
m22

.

518
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
FIGURE 9.31
The resampling lattice generated by a dyadmic sampling matrix.
This corresponds to a shearing (resampling) operation. The collection of all the subsampling vectors,
Mn, is called a lattice given by
L AT (M) = {m : m = Mn, n ∈Z2}.
The L AT (M) is indeed a vector space spanned by the linear combination of basis column vectors
from M, written as n1M1 + n2M2, with M1 = (m11, m21)T , M2 = (m12, m22)T . In our context, a
non-singular subsampling matrix M refers to the matrices deﬁned in the section of DFB.
Let S be the set of points of Mt, t ∈[0, 1) × [0, 1), called the fundamental parallelepiped of matrix
M. It consists of all real-valued vectors in an area of the parallelogram controlled by M1 and M2. An
example of S is shown in the shadowed square area in Figure 9.31. However, we are only interested in
the integer valued points in S. According to the division theorem in number theory [41], any integer
vector can be written as Mn + s with s = (s1, s2) ∈S, so there are D = |det(M)| integer points in S.
We can label each integer vector s ∈S using index k = 0, 1, . . . , D −1. Note that k cannot be a natural
ordering like 1-D case. The sampling lattice of Mn + s results from shifting LAT (M) to the location
s ∈S, yielding the kth coset of L AT (M) generated by an integer valued points inside S. Therefore,
the original lattice of integer numbers in the coordinate system is the union of all the cosets.
For a 2-D image, the polyphase decomposition is the subsampling over the cosets deﬁned above.
The kth component sub image takes a value at the location vector in the kth coset associated with s, and
is deﬁned as
x(k,s)(n) = x(Mn + s).
Denote by x(k,s)
u
(n) the upsampled version of the kth polyphase component, x(k,s)(n), with a shifting
by s ∈S, deﬁned as
x(k,s)
u
(n) =

x(k,s)(m) if n = Mm + s, s ∈S,
0
otherwise.
The parent image can then be reconstructed as the sum of x(k,s)
u
(n), namely,
x(n) =
D−1

k=0
x(k,s)
u
(n).

1.09.4 Contourlets and Their Applications
519
Its z-transform can be written as
X(z) =

s
z−s1
1
z−s2
2
X(k,s)(zM),
with z = (z1, z2), s = (s1, s2) ∈S.
Here, for a non-diagonal matrix M, zMstands for
	
zm11
1
zm21
2
, zm12
1
zm22
2

, and X(k,s)(z) is the z-transform
of x(k,s)(n), see [39].
Deﬁnition 10.
To be consistent with the subsampling concepts, the polyphase representation of the
z-transform, X(z), of an image x(n) is deﬁned as a vector, x(z), written as
x(z) =
	
X0(z), X1(z), . . . , X D−1(z)

,
with Xk(z) = X(k,s)(z), for k = 0, 1, . . . , D −1.
For example, a dyadic subsampling matrix M will have its cosets generated by points (i, j), i, j =
0, 1 in S, and the z-transform of x(n) can be written as
X(z) = X0(z2
1, z2
2) + z−1
1 X1(z2
1, z2
2) + z−1
2 X2(z2
1, z2
2) + z−1
1 z−1
2 X3(z2
1, z2
2),
where Xk(z1, z2) = 
n1

n2 z−n1
1
z−n2
2
x(2n1 +i, 2n2 + j), k = 2 ∗j +i, i, j = 0, 1. The polyphase
representation can then be written as
x(z) =
	
X0(z1, z2), X1(z1, z2), X2(z1, z2), X3(z1, z2)

.
The use of a polyphase decomposition in the design of a ﬁnite input response (FIR) ﬁlter bank reduces
the computational complexity in the realization of a ﬁlter bank, in which the polyphase representation
is frequently used. Note that X(z) is a vector, the z-transfer function takes a scalar value, while the
polyphase representation x(z) takes vector value. We assume the signal is periodically expanded if it is
of ﬁnite length.
A ﬁlter bank with a solo implementation of the polyphase decomposition is also termed as a lazy
ﬁlter bank because of its simple reconstruction structure that involves only the basic operations, such
as addition, delay, downsampling and upsampling. A iterated lazy ﬁlter bank leads to the so called lazy
wavelet.
1.09.4.2 The Laplacian pyramid frames
Introduced by Burt and Adelson [8,107], the LP was initially aimed at representing data in a mul-
tiresolution setting from ﬁne to coarse levels for data compression purpose. The analysis ﬁlter bank
of LP iteratively decomposes an image into multiple sub images of coarse approximation, namely, the
low resolution sub images with lowpass content, and detail images representing the difference between
the lowpass ﬁltered image and the input image. The synthesis ﬁlter bank integrates the outputs from
the analysis ﬁlters into an estimate of the original image. The perfect reconstruction occurs when the
estimate is a replica of the original image.
The analysis ﬁlter bank of the LP results in a sequence of bandpass images and a further upsampled
low resolution image in the end level. The synthesis ﬁlters are used to recover the original image from this

520
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
sequence of sub-images. The double layered ﬁlter bank analyzes an image in various components of size-
reduced images with their frequency contents limited to sub-bands. The pyramid design has inspired
the connection of ﬁlter banks with the wavelet implementation and calculating linear expansions of
multiresolution signals.
1.09.4.2.1
The analysis ﬁlter banks and synthesis ﬁlter banks of the Laplacian pyramid
The analysis ﬁlter bank is shown in Figure 9.32. It compute the approximation and the detail image at
one single level. Take x0(n) as the input image, the analysis LP ﬁlter bank decomposes the image using
a lowpass ﬁlter H with the impulse response ¯h(n) = h(−n). The output from the lowpass ﬁlter H is
down-sampled, resulting in x1(n) that can be written as
x1(n) =

s∈Z2
x0(s)h(Mn −s) = ⟨x, ¯h( · −Mn)⟩,
where M is a sampling matrix which deﬁnes the downsampling. The z-transform of x1(n) can be written
in a polyphase representation as
X1(z) = h(z)xT
0 (z)
(9.76)
with
x0(z) = (X00(z), X01(z), . . . , X0(D−1)(z)),
and
h(z) = (H0(z), H1(z), . . . , HD−1(z))
both are 1 × D polyphase representation vectors.
The detail image is the difference between the approximation image, x1(n), and the original image,
x0(n). It is calculated as the difference between a different version of the approximation image, p(n),
and the original image, x0(n), written as
d(n) = x0(n) −p(n).
The lowpass image p(n) of the original size is obtained as the following,
p(n) =

s∈Z2
x1(s)g(n −Ms).
FIGURE 9.32
The LP ﬁlter bank in a single level.

1.09.4 Contourlets and Their Applications
521
To calculate p(n), the already down-sampled image x1(n) is upsampled, and ﬁltered by another lowpass
ﬁlter G with impulse response g(n). The polyphase representation p(z) can then be written as
pT (z) = (P0(z), P1(z), . . . , PD−1(z))T = gT (z)X1(z),
(9.77)
where g(z) = (G0(z), G1(z), . . . , G D−1(z)) is a 1× D vector, and X1(z) is a scalar function. Thus the
polyphase representation of d(n), can be calculated using Eqs. (9.76) and (9.77), an can be written as
dT (z) = (I −gT (z)h(z))xT
0 (z).
(9.78)
If we cascade Eqs. (9.78) and (9.76), we will have a vector equation
 X1(z)
dT (z)

=

h(z)
I −gT (z)h(z)

xT
0 (z) = (z)xT
0 (z),
(9.79)
where (z) is a matrix of size (D + 1) × D.
In the synthesis ﬁlter bank, a simple reconstruction of the original image can be computed by directly
inversing the steps in the analysis ﬁlter. A more efﬁcient reconstruction method is to search for the inverse
operator under an orthogonal frame system to calculate the orthogonal projections that is deﬁned as the
pseudo inverse of matrix (z). This approach provides the best approximation [112].
The LP ﬁlter bank can be realized through a polyphase ﬁlter bank structure, in which the lowpass
ﬁlter H is determined by its transfer function H(z), and the highpass ﬁlter Ki, i = 0, 1, 2, . . . , D −1,
is determined by a transfer function whose polyphase representation is given as the ith row vector of
the matrix (z) = (I −gT (z))h(z). Therefore the highpass ﬁlters are derived from the lowpass ﬁlter
H and G in the ﬁlter bank described in Figure 9.32. Figure 9.33 shows the polyphase structure ﬁlter
bank for dyadic subsampling which is to be used in the contourlet ﬁlter bank.
Based on the structure of a single level analysis ﬁlter bank shown in Figure 9.32, the procedure of
multiresolution decomposition using the Laplacian pyramid (LP) ﬁlter bank is listed as follows:
•
Input an image x0(n) into the one level LP ﬁlter bank. The outputs are the low resolution upsampled
image x1(n) and a detailed difference image d1(n).
FIGURE 9.33
The LP ﬁlter implemented using the polyphase decomposition with dyadic subsampling.

522
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
•
Input the x1(n) into the analysis ﬁlters again. The outputs are a low resolution image x2(n) that is
further upsampled, and a detailed image d2(n) that is the difference between x1(n) and x2(n).
•
Iterate the procedure K times.
The output sequence {xk}k=1,2,...,K is called a Gaussian pyramid because the Gaussian lowpass ﬁlters
are typically used. The ﬁltering structure for generating the band pass sequence {dk}k=1,2,...,K is called
the Laplacian pyramid. As the xk is further decomposed, the ﬁnal outputs are the sequence of detail
images dk, with only one low resolution image at the end level, thus yielding a ﬁlter bank is generally
referred to as the Laplacian pyramid. It is a cascade of a sequence of ﬁlter banks for a multiresolution
approximation of an original image.
1.09.4.2.2
The wavelet frame associated with the Laplacian pyramid
A wavelet transform can be implemented by computing the ﬁne to coarse multiresolution decomposition
as shown in Figure 9.34. The wavelet frame [8] deﬁned in Eq. (9.13) for the decomposition can be
speciﬁed by using the ﬁlters H and G of the LP ﬁlter bank to determine the mother/father wavelets.
In the LP ﬁlter bank, a frame operator 
Ki can be deﬁned as 
Ki(x, φi) = ⟨x, φi⟩, where each φi is the
(i + 1)th row of the matrix (z) in Eq. (9.79). It characterizes the ﬁlter Ki in the polyphase ﬁlter bank.
This shows that the Laplacian pyramid, with bounded output for any bounded input, provides a frame
expansion in l2(Z2) [9]. If the frame vector {φi} is normalized with ∥φi∥= 1 and we have A = B = 1,
then {φi} is an orthonormal basis in Eq. (9.13).
A non-orthonormal frame induces a redundant representation of an image, for which the recon-
struction has to use a redundant frame operator that involves a dual frame of 
Ki, which performs the
inverse/pseudo inverse of the matrix . It generates instability that affects the quality of reconstructing
the original image. A perfect reconstruction occurs when the original image is replicated. In numeri-
cal analysis, the research interests have been focused on searching for orthonormal bases for optimal
approximations. With an orthogonal basis, when a signal x(n) is projected onto a subspace V, given as
PV (x), it generates the minimum approximation error. In some case, due to the requirement of the basis
functions, we have to relax the requirement for an orthonormal basis and replace it with an orthogonal
basis, a biorthogonal basis, or even a tight frame for multiresolution analysis. This is used to inject some
ﬂexibilities, such as, to minimize the redundancy of the expansion.
FIGURE 9.34
An LP ﬁlter bank that decomposes the frequency content of an image into three levels is shown.

1.09.4 Contourlets and Their Applications
523
The Laplacian pyramid uses a pair of biorthogonal ﬁlters with a sampling matrix M, and satisﬁes
⟨¯h(· −Mv), g(· −Ml)⟩= δ(v −l).
The ﬁlters become orthogonal when they satisfy
h(n) = g(−n) and ⟨g(· ), g( · −Mn)⟩= δ(n).
The orthogonal requirement on the ﬁlters H and G is equivalent to
H∗(z) = G(z) and G∗(z)G(z) = 1.
A perfect reconstruction can then be achieved, thus providing a simple structure that connects the
LP ﬁlter bank with the classical wavelets [84] for multiresolution analysis. A similar implementation
involving a directional decomposition that further connects the anisotropic curvelets with the discrete
ﬁlter banks is discussed next.
1.09.4.3 The directional ﬁlter banks
Introduced by Bamberger and Smith [108] in 1992, a directional ﬁlter bank (DFB) is a maximally
decimated QMF bank that partitions the 2-D Fourier plane with 2r directional oriented wedge-shaped
sub-bands at each level r. A perfect reconstruction of the original image may be obtained. The imple-
mentation introduced here is using a polyphase like tree structured FIR ﬁlter bank [9,113]. This DFB
has a simpliﬁed structure and uses fan ﬁlters and resampling operators with a dyadic downsampling in
each level.
1.09.4.3.1
The resampling operators
The design of a directional ﬁlter bank requires resampling operators controlled by sampling matrices,
such as shearing operators. Here we ﬁrst introduce the resampling matrices.
Deﬁnition 11.
A quincunx sampling matrix is deﬁned as one of the following two matrices
Q0 =
1 −1
1
1

,
Q1 =
 1
1
−1 1

.
Deﬁnition 12.
An integer matrix M is said to be a unimodular matrix if |det(M)| = 1. Here, we refer
to the following four matrices Ri, i = 0, 1, 2, 3,
R0 =
 1 1
0 1

,
R1 =
 1 −1
0
1

,
R2 =
 1 0
1 1

,
R3 =
 1
0
−1 1

.
The two quincunx matrices can be further decomposed as
Q0 = R1 D0 R2 = R2 D1R1
and
Q1 = R0 D0 R3 = R3 D1R0,
where the two diagonal matrices D0 and D1 are the dyadic subsampling matrix along x, or y directions,
and are given as
D0 =
2 0
0 1

,
D1 =
 1 0
0 2

.

524
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
A ﬁlter that uses Q0 or Q1 carries out a downsampling operation that sheers an image along the ori-
entation at 45◦or −45◦. In the DFB, Q0 and Q1 are typically cascaded together to result in dyadic down-
sampling in both x and y directions. This is because Q0 Q1 = Q1 Q0 = 2I, where I is the unit matrix.
1.09.4.3.2
The design of the quincunx ﬁlter banks
A quincunx ﬁlter bank (QFB) [113,114] is a two-channel QMF as shown in Figure 9.35. It incorporates
the analysis and synthesis ﬁlters with the decimators and expanders being the quincunx resampling
matrices. A QFB with fan ﬁlters used in the analysis part is a typical example. The fan ﬁlters are non-
separable ﬁlters. In Figure 9.36 is shown the support of one fan ﬁlter which can be designed starting
from a 1-D ﬁlter [41].
The single level structure analysis ﬁlter bank of QFB decomposes the frequency plane of a 2-D image
into two wedge shaped cones. The horizontal cone uses H0, a horizontal pass fan ﬁlter. The vertical
cone uses H1, a vertial pass fan ﬁlter. The synthesis ﬁlters, denoted by G0 and G1, perfectly reconstruct
the original image when the two dual ﬁlter frames satisfy the following conditions,
H0(ω)G0(ω) + H1(ω)G1(ω) = 2,
FIGURE 9.35
The quincunx ﬁlter bank with resampling matrix Q.
FIGURE 9.36
The support of a fan ﬁlter in the Fourier plane. The shaded area is the bandpass area.

1.09.4 Contourlets and Their Applications
525
H0(ω + π)G0(ω) + H1(ω + π)G1(ω) = 0.
If the synthesis ﬁlters are the reversed versions of the analysis ﬁlters then the frame generated by the
QFB is an orthogonal frame. This simpliﬁes the design of the synthesis ﬁlters. When the analysis and
synthesis ﬁlters are restricted to be ﬁnite impulse response (FIR), the perfect reconstruction conditions
imply that the synthesis ﬁlters are speciﬁed by the analysis ﬁlters (up to a shift and scale factor i) as the
following,
G0(z) = zi H1(−z),
G1(z) = −zi H0(z).
1.09.4.3.3
The design of the directional ﬁlter banks
A QFB is the essential QMF bank in a tree structured DFB ﬁlter bank. An iterative DFB decomposes the
frequency space of a 2-D image into 2r small angularly oriented wedges at the rth level. The DFB is a
cascade of multiple QFBs, together with selective resampling matrices to determine the shearing angle
of the segmented wedge in Fourier plane. The iterative DFB can be viewed as proceeding in multiple
stages. According to the Nobel identity shown in Figure 9.38a, the overall output in two adjacent stages
can be viewed as applying a decimation M = Q0Q1 using the overall analysis ﬁlter H1(z)H2(zMT ).
Figure 9.38b shows the overall effect of a DFB in two stages.
The DFB decompositions in multiple levels are carried out in the following procedures:
•
At the ﬁrst level decomposition, the DFB uses a pair of fan ﬁlters with supports complementing each
other. The output is resampled using Q1. The two fan ﬁlters split the Fourier plane into two cone
shaped regions: one along the vertical direction, and another one along the horizontal direction. The
output is downsampled by two in the x direction.
•
Atthesecondlevel,thesamepairoffanﬁltersareused,however,theoutputimageisre-sampledusing
Q1, so that the four sub images correspond to the four directional sub-bands shown in Figure 9.39.
The ﬁlter structure for generating these four images is shown in Figure 9.37. The outputs at this level
are down-sampled by 2 in both x and y directions.
•
At the level r ≥3, the DFB partitions the Fourier plane into 2r wedges. Figure 9.40 shows the
partition at the level r = 3. This is realized by attaching to the QFB at the previous level with one of
FIGURE 9.37
The partition of the Fourier plane using DFB at the ﬁrst two levels.

526
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
(a)
(b)
FIGURE 9.38
The effect of Noble identity.
FIGURE 9.39
The four directional bands output from the ﬁrst two level partitions.
FIGURE 9.40
The eight directional bands output from a DFB at the level r = 3.

1.09.4 Contourlets and Their Applications
527
FIGURE 9.41
(a) A resampling operator is attaching to a QFB to implement the DFB, (b) Its effects on partitioning the
Fourier plane: the 3rd region in the four partitions is used as the input, the output is the wedge indicated
in the last picture.
the four uni-modular resampling operations, Ri, i = 0, 1, 2, 3, before and after the analysis ﬁlters.
Figure 9.41 shows the ﬁlter bank structure for generating the 0th wedge in Fourier frequency plane
at the 3rd level.
To ensure that each channel of the ﬁlters uses a diagonal sampling matrix, a back-sampling operation
[115] is applied so that, at each level r, the vth channel sub-band can be equivalently viewed as being
ﬁltered by an analysis ﬁlter Hv, for v = 0, 1, . . . , 2r −1. The diagonal sampling operator is represented
by the sampling matrix
S(r)
v
=
diag(2r−1, 2) 0 ≤v < 2r−1,
diag(2, 2r−1) 2v−1 ≤v < 2r.
The DFB is a perfect reconstruction ﬁlter bank if and only if the QFB used at each level implements
perfect reconstruction.
1.09.4.3.4
Constructing the basis for the directional ﬁlter bank
As shown in Figure 9.42, in the DFB at the rth level, every analysis ﬁlter Hv with impulse response
hv, is followed by a diagonal sampling S(r)
v , v = 0, 1, . . . , 2r −1. The outputs are 2r wedges in the
Fourier plane. Each wedge is an angularly oriented segment of the original image’s Fourier content.
Correspondingly, the original image can be reconstructed from applying the diagonal sampling matrix

528
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
FIGURE 9.42
The analysis and synthesis ﬁlters in a DFB that partitions the Fourier plane into 2R wedges.
S(r)
v
to the outputs at the rth level, followed by applying the synthesis ﬁlters {Gv}v=0,1,...,2r−1, with
impulse response {gv(n)}v=0,1,...,2r−1. We also write gv(n), hv(n) as g(r)
v (n), h(r)
v (n) for the rth level.
Each ﬁlter pair, with Hv and Gv in the ﬁlter bank as the analysis and synthesis ﬁlters, is related to
the angular wedge region Uv, v = 0, 1, . . . , 2r −1. The ﬁlters generate a pair of dual frames of l2(Z2).
Therefore, a basis of l2(Z2) can be obtained by translating the impulse responses of synthesis ﬁlters,
gv(n), as
%
g(r)
v,m(n) = g(r)
v
	
n −S(r)
v m

: 0 ≤v < 2r, m ∈Z2&
. Under this basis, an image x(n) ∈
l2(Z2) can be uniquely reconstructed as
x(n) =
2r−1

v=0

m∈Z2
cv(m)g(r)
v

n −S(r)
v m

,
where cv(m) =
"
x( · ), h(r)
v
	
S(r)
v m −·

#
, v = 0, 1, . . . , 2r −1 are the coefﬁcients with respect to a
basis hv
	
S(r)
v m −·

of l2(Z2) at the rth DFB decomposition level. The two dual bases l2(Z2) satisfy
the following conditions when they are biorthogonal frames:
"
g(r)
v′

· −S(r)
v′ m′
, h(r)
v

S(r)
v m −·
#
= δ(v −v′)δ(m −m′),
with the Dirac function given as
δ(a −a′) =
 0, when a ̸= a′,
1, when a = a′.
An orthogonal basis of l2(Z2) is obtained when h(r)
v (n) = g(r)
v ( −n), that is, when the synthesis ﬁlters
of the DFB are time-reverses of the analysis ﬁlters.
The following are two extreme cases of DFBs. In the ﬁrst case, the ﬁlter system generates a basis of
the l2(Z2) whose elements have compact supports that do not overlap each other in the spatial domain.
The second case shows a ﬁlter system that generates a basis of l2(Z2) whose elements have compact
supports that do not overlap each other in the Fourier domain.

1.09.4 Contourlets and Their Applications
529
Case 1: In this case, the analysis DFB implements a polyphase decomposition with the vth polyphase
component xv(n) = x
	
pT
v + S(r)
v n

, where
pv =
(v, v)
if 0 ≤v < 2r−1,
(v + 1 −2r−1, v −2r−1) if 2r−1 ≤v < 2r.
The polyphase decomposition is realized when each ﬁlter Gv takes the z-transform as Gv(z) = z−pv,
where the basis g(r)
v
	
n −S(r)
v m

= δ
	
n −pT
v −S(r)
v m

extends a standard orthonormal basis of
l2(Z2) .
Case 2: Another orthonormal basis of l2(Z2) is obtained using a DFB with ideal fan ﬁlters at each
level, resulting in the synthesis ﬁlters Gv with Fourier transform given by
Gv(ω) = 2v/2δRv(ω) exp
%
j
"
ω, pT
v
#&
.
These two cases, however, will have non-compact support in their dual spaces. Namely, in the ﬁrst
case, the Fourier content of each element in the basis is supported in the whole Fourier plane, while in
the second case, each element in the basis is a sinc function with support in the whole spatial domain.
In application, a DFB realization that generates a basis of compact support in the Fourier plane and
fast decay in the spatial plane works better to separate a signal into sub-signals that are localized in
various frequency regions. In the contourlets [9], a biorthogonal ﬁlter pair “9–7” is usually used.
1.09.4.4 The contourlet ﬁlter bank
Now we are ready to address the contourlet ﬁlter bank, which is a tree structured double layed ﬁlter
bank that combines the LP and DFB to decompose an original image iteratively. Figure 9.43 shows
the multiscale decomposition result generated by the contourlet ﬁlter bank. The implementation uses a
combination of LP and DFB ﬁlter banks.
1.09.4.4.1
The wavelet frames of Laplacian ﬁlter banks
The LP ﬁlters in the contourlet ﬁlter bank are octave-band ﬁlters: The image is decomposed into a
lowpass frequency content, and a bandpass content. The lowpass ﬁltered image is an approximation
of the high resolution image with a dyadic downsampling applied in both directions. The bandpass
ﬁltered image is the detail difference between the original image and the approximation image. This
decomposition proceeds iteratively. At level k, the original input image is decomposed into several sub
images that are focused on sub-bands with a corona support as shown in Figure 9.34, where the Fourier
plane of an image is decomposed into three levels.
The LP ﬁlter provides a frame expansion in L2(R2). Under the multiresolution analysis (MRA), the
LP ﬁlter bank is associated with a wavelet frame represented by nested subspaces {Vk}k∈Z (refer to
Section 1.09.2) that satisﬁes
Vk ⊂Vk−1,
for ∀k ∈Z,
Closure
 
∞

k=−∞
Vk
!
= L2(R2).

530
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
FIGURE 9.43
The contourlet ﬁlter bank partitions the Fourier plane into wedges for three levels.
The lowpass ﬁlter G in Figure 9.32 deﬁnes a wavelet frame of L2(R2). The ﬁlter structure uniquely
deﬁnes φ(t) that satisﬁes the following equation (called an orthogonal scaling function),
φ(t) = D
1
2

n∈Z2
g(n)φ(Mt −n).
Recallthat,ingeneral, D = |det(M)|,where M isadiagonalmatrixthatcorrespondstosubsampling.
In our LP ﬁlter bank introduced here, the subsampling is constrained to a dyadic subsampling only;
therefore we have D = 4, and
M =
 2 0
0 2

.
The sub-space Vk is spanned by an orthogonal basis {φk,n}n∈Z2 that is deﬁned as
φk,n(t) = D−k
2 φ(M−kt −n).
Writing the orthogonal complement of Vk in Vk−1 as Wk (see Section 1.09.2), we have
Vk−1 = Vk
'
Wk.
Now let { fi}i=0,1,2,3 represent the impulse responses of the highpass synthesis ﬁlters {Fi}i=0,1,2,3
in the reconstruction part of the LP polyphase structured ﬁlter bank, see Figure 9.77. It generates the
mother wavelet function ψi for each i = 0, 1, 2, 3 as the following equation:
ψi(t) = D
1
2

s∈Z2
fi(s)φ(Mt −s), t ∈R2, i = 0, 1, 2, 3.

1.09.4 Contourlets and Their Applications
531
A tight frame of the detail space Wk can be obtained as a family of dilated and translated functions,
ψk,i,n(t), given as
ψk,i,n(t) = D−k
2 ψi(M−kt −n),
for k ∈Z, n ∈Z2,
t ∈R2,
i = 0, 1, 2, 3.
(9.80)
Here, ψk,i,n(t) corresponds to the ith polyphase component of LP ﬁlter bank at level k. Under this
set of tight frames, given an input image x(n), the outputs of the LP ﬁlter bank at the kth level are
the approximation wavelet coefﬁcients ck(n), and the detail wavelet coefﬁcients dk,i(n), they can be
obtained from the following formulas based on the approximation coefﬁcients at the previous level,
ck(n) = ⟨x, φk,n⟩=

s∈Z2
ck−1(s)g(s −Mn),
dk,i(n) = ⟨x, ψk,i,n⟩=

s∈Z2
ck−1(s) fi(s −Mn).
Theorem 5.
[112] It can be shown that L2(R2) can be decomposed into detail spaces {Wk, k ∈Z}
that are mutually orthogonal, and can be written as
L2(R2) =
'
k∈Z
Wk.
At a scale level k, the family of functions {ψk,i,n(t) : 0 ≤i < D, n ∈Z2} is a tight frame of Wk. In
addition, the entire family {ψk,i,n(t) : 0 ≤i < D, k ∈Z, n ∈Z2} forms a tight frame of L2(R2).
1.09.4.4.2
The wavelet frames of directional ﬁlter banks
In a contourlet ﬁlter bank, the highpass content output from the LP ﬁlter bank is the input for the DFB
ﬁlter bank. The DFB generates 2r directional oriented wedges that partition the Fourier plane. Typically,
only the high frequency content is decomposed since it corresponds to salient features in an image. The
directional oriented ﬁlter bank provides an anisotropic basis for an efﬁcient decomposition.
As provided in Section 1.09.4.3.4, under the DFB, at level r, the family

g(r)
v,n(m) = g(r)
v (m−S(r)
v n) :
0 ≤v < 2r, n ∈Z2
is generated and is called a directional orthonormal basis of l2(Z2). Each g(r)
v,n(m)
is the impulse response of a directional ﬁlter. When v = 0, . . . , 2r−1 −1, it corresponds to an angle in
[−45◦, 45◦], and when v = 2r−1, . . . , 2r −1, it corresponds to an angle in [45◦, 135◦].
This directional orthonormal basis

g(r)
v,n(m) : 0 ≤v < 2r, n ∈Z2
is used to construct the frames
of the further decomposed detail space. Since the detail subspaces Wk from the LP correspond to four
components of the polyphase decomposition, the following function is introduced using Eq. (9.80),
μk,2n+si (t) = ψk,i,s(t),
where s0 = (0, 0), s1 = (1, 0), s2 = (0, 1), s3 = (1, 1). With a lattice of dyadic subsampling in both
directions, the points si, i = 0, 1, 2, 3, represent all the cosets with the dyadic subsampling, therefore,
any integer vector s can be written as n + si, and we have the family μk,s(t) constituting a tight frame
of Wk at level k in the LP decomposition.

532
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
Now deﬁne
ψ(r)
k,v,n(t) =

s∈Z2
g(r)
v

s −S(r)
v n

μk,s(t).
The set of functions,
%
ψ(r)
k,v,n
&
n∈Z2, constitutes a tight frame of the ﬁner partition subspace of the
detail space Wk, written as W (r)
k,v, with v referring to the vth angular direction. The frame bounds are
equal to 1 for each k = 1, . . . , K, and v = 0, . . . , 2r −1. Also note that the indexing v is referred to an
angular direction in DFB, which is different from the indexing i, which refers to a polyphase component
in the LP ﬁlter.
A directional subspace W (r)
k,v is deﬁned on a wedge contained in a rectangular grid that has support in
a spatial domain as an oval area, similar to what is shown in Figure 9.26. These subspaces are orthogonal
with
W (r)
k,v = W (r+1)
k−1,2v
'
W (r+1)
k−1,2v+1,
and
Wk =
2r−1
'
v=0
W (r)
k,v.
Further, combining ψ(r)
k,v,n(t) together with φk0,n for k ≤k0, 0 ≤v < 2r, n ∈Z2, we obtain a tight
frame of l2(R2), as in the following Theorem [114],
Theorem 6.
The l2(R2) can be decomposed into mutually orthogonal subspaces as
l2(R2) = Vk0
'
⎛
⎝'
k≤k0
Wk
⎞
⎠.
For a sequence of ﬁnite positive integers k ≤k0, the family
{φk0,n(t), ψ(r)
k,v,n(t) : k ≤k0, 0 ≤v < 2r, n ∈Z2}
is a tight frame of L2(R2).
Theorem 7.
l2(R2) can be decomposed into mutually orthogonal subspaces as
l2(R2) =
'
k∈Z
Wk.
The family
%
ψ(r)
k,v,n(t) : k ∈Z, 0 ≤v ≤2r −1, n ∈Z2&
is a directional wavelet tight frame of L2(R2). In each case, the frame bounds are equal to 1.
This combination of frames from both the LP and DFB ﬁlter banks yields frequency partitions similar
to those achieved by curvelets. The support of the LP is reduced to one fourth of its original size while

1.09.5 Shearlets and Their Applications
533
FIGURE 9.44
Shown in the left is a noisy image, in the right is the image after applying the hard threshold denoising
method using the contourlets.
the number of directions of the DFB is doubled. Hence, it connects the directional anisotropic wavelets
with the digital ﬁlter bank system.
In Contourlet MALAB program [116], the syntax that is used for decomposition is:
coeffs = pdfbdec(double(im), pﬁlter, dﬁlter, nlevels);
The syntax that is used for reconstruction the original image from the contourlet coefﬁcients is:
imrec = pdfbrec(coeffs, pﬁlter, dﬁlter);
Since the contourlets can be viewed as a ﬁlter bank implementation of the curvelets, here we only
show in Figure 9.44 an image that demonstrates the denoising outcome using the contourlets.
1.09.5 Shearlets and their applications
The sheartlet transform is an alternative anisotropic multiresolution system proposed by Guo, Labate
and their colleagues [11–13,117–124]. It was inspired by the composite wavelet theory, which takes
advantage of geometric afﬁne transforms, such as translation and dilation (or resampling) in a multi-
dimensional Cartesian coordinate system (our focus is on 2-D). In the spatial plane, the afﬁne transforms
generate a frame of functional elements (also called building blocks) that are oscillatory waveforms
across various scales, locations, and orientations.
Similar to the curvelet transform, a shearlet transform provides a Parseval (normalized tight) frame
equipped with well localized functions at various scales, locations and directions. When a 2-D function
is projected into this system, sparse signiﬁcant coefﬁcients, which correspond to discontinuities occur
along C2 piecewise smooth curves, can be used to optimally approximate 2-D functions. It overcomes
the problem with the classical 2-D wavelet transform, in which a larger amount of coefﬁcients are
needed in order to optimally represent a 2-D function with discontinuities.

534
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
Meanwhile, the shearlet transform has its advantages over the curvelet transform. One advantage
with the continuous shearlet transform is that, it can be deﬁned directly in a rectangular coordinate
system using an afﬁne transform. It avoids the usage of polar coordinates (as curvelets) and hence avoid
the computationally complex numerical conversion from polar coordinates to rectangular coordinates.
The second advantage with the shearlet transform is that, it exhibits a better capability at distinguishing
different types of corner (junction) points. This can be used to separate corner (junction) points from
the regular edge points. In addition, shearlet transform can be mathematically related to group theoretic
methods, and can hence be interpreted as a natural building blocks in measuring the smoothness of a
2-D function. It can also be related to the so-called co-orbit space theory [125].
Inthissubsection,weﬁrstdescribethecompositewavelets[10,126]beforeweintroducetheshearlets,
since the shearlets be viewed as a special class of the composite wavelets. Applications of the shearlet
transform in image processing enjoys some interesting properties in applications in edge detection,
feature extraction, and denoising. We will also discuss more recent applications at the end of this
section.
1.09.5.1 The composite wavelets
The composite wavelet system provides a general framework with angularly oriented oscillatory spatial
waveforms that can be used for representing geometrical features of curves. Since it heavily relies on the
afﬁne transform, we proceed by ﬁrst introducing the afﬁne transform system, H, given as the following:
Deﬁnition 13.
Given a function ψ(x) ∈L2(R2), a continuous afﬁne system, H, generated by ψ(x)
is deﬁned as
H =
%
ψG,b(x) = |det G|1/2ψ(G(x −b)) : b, x ∈R2, G ∈G
&
,
(9.81)
where G is a set of invertible 2 × 2 matrices.
All elements in this afﬁne system, H, are called composite wavelets when this afﬁne system forms
a tight (Parseval) frame of L2(R2), namely, for any f ∈L2(R2), the following equation holds:

b,G
|⟨f , ψG,b⟩|2 = ∥f ∥2,
(9.82)
where G is an invertible 2 × 2 matrix in the set G.
Under this general framwork of composite wavelets, various wavelet systems can be introduced as
special cases for decomposing an image. The 2-D isotropic continuous (the classical) wavelet transform
is a special example. It can be obtained when G, the set of matrices in Eq. (9.81), takes the form
G = {cI : c > 0}, here I is the unit matrix. Furthermore, the separable classical wavelets can also
be obtained when we further use a 2-D separable function ψ(x), given as ψ(x, y) = ψ1(x)ψ2(y).
The most exciting case of composite wavelets is the shearlet transform system obtained from using
the shearing and translating operations. A shearlet transform system deﬁnes a new class of wavelets
that produces a set of efﬁcient anisotropic components that can ﬂexibly be adapted to represent the
geometrical shapes.

1.09.5 Shearlets and Their Applications
535
1.09.5.2 The shearlet atoms in the spatial plane
The shearlet atoms are deﬁned by selecting the matrix set G in Eq. (9.81), in the afﬁne system, H, as
G = {Mas : a > 0, s ∈R}, with Mas given as
Mas =
a
−√as
0
√a

.
(9.83)
It is useful to write the matrix Mas = Bs Aa, where
Aa =
a
0
0
√a

and
Bs =
 1 −s
0
1

.
Note that the inverse matrix M−1
as can be written as A−1
a B−1
s , and is given as
M−1
as =
a−1
sa−1
0
a−1/2

.
(9.84)
Indeed, the matrix Mas is the composition of two afﬁne operations in a plane: a parabolic scaling,
Aa, followed by a shearing, Bs. The shearlets are deﬁned on the basis of the operator, DMas, deﬁned
by the Matrix Mas, as
DMasψ(x) = |det(Mas)|−1
2 ψ

M−1
as x

,
and the translation operator Tb, deﬁned as
Tbψ(x) = (x −b),
yielding the shearlet atoms deﬁned as
ψa,s,b(x) = TbDMasψ(x).
(9.85)
Deﬁnition 14.
The family of sheartlet atoms in the spatial plane is deﬁned as the collection of all
atoms in the afﬁne system generated by ψ(x) ∈L2(R2), given by
ψa,s,b(x) = |det(Mas)|−1
2 ψ

M−1
as (x −b)

,
a > 0, s ∈R, x, b ∈R2.
(9.86)
The function ψ(x) ∈L2(R2) is called the mother shearlet window.
Note that, because det(Mas) = a3/2, Eq. (9.86) can also be written as
ψa,s,b(x) = a−3
4 ψ

M−1
as (x −b)

.
(9.87)
In the literature about shearlets, the matrix Mas can also be chosen as
Mas =
a
√as
0
√a

, with M−1
as =
a−1
−sa−1
0
a−1/2

.
In this paper, we stick with the matrix Mas, given by (9.83), for Eq. (9.86) to achieve the shearing
effects. Hence, the inverse matrix, M−1
as , is used in its Fourier domain deﬁnition; however, one can
alternatively use M−1
as in the spatial domain, and use Mas in the Fourier plane [119].

536
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
1.09.5.3 The shearlet atoms in the frequency domain
The mother shearlet window deﬁnes the properties of the shearlet atoms. Every shearlet atom is a result of
an afﬁne transform of the mother shealet window. Hence, We ﬁrst discuss the choice of the mother shear-
let window in order to quickly address the geometrical features of an image using a shearlet transform.
1.09.5.3.1
The mother shearlet window
The rule of thumb for selecting a mother shearlet window in a shearlet transform, is to generate shearlet
atoms that are able to focus on localized frequency content to represent geometrical features. Under this
shearlet system, the energy of a function f (x) will be mostly focused on a sparse number of coefﬁcients,
when the function is decomposed. In general, the mother shearlet window is chosen as a C∞function
with a compact support in the Fourier plane, so that it decays rapidly in the spatial domain. The mother
shearlet window function, ψ(x, y), is deﬁned by its Fourier transform, 
(),  = (x, y), as

() = 
1(x)
2
y
x

,
where 
1(x) and 
2(y) satisfy the following two conditions:
•

1(x) is a 1-D highpass odd function that satisﬁes the regular 1-D admissibility condition,
 ∞
0
|
1(ax)|2 da
a = 1.
(9.88)
•

2(y) is a 1-D positive lowpass even function that satisﬁes,

R
|
2
2(y)|dy = 1.
(9.89)
A good choice for the two 1-D windows are the Meyer’s wavelet windows deﬁned in Section 1.09.3.
1.09.5.3.2
The Fourier content of shearlet atoms
Given the mother shearlet window deﬁned above, the Fourier transform of the shearlet atoms deﬁned
in Eq. (9.86) can be written as

a,s,b() = a
3
4 e−j2π⟩,b<

MT
as

= a
3
4 e−j2π⟨,b⟩
1(ax)
2
√a(y −sx)
ax

= a
3
4 e−j2π⟨,b⟩
1(ax)
2

a−1
2
y
x
−s

,
a > 0, s ∈R, b ∈R2. (9.90)
In addition to requiring that the functions 
1 and 
2 satisfy the conditions in Eqs. (9.88) and (9.89),
we also assume supp{
1} ⊂

−2, −1
2

∪
 1
2, 2

and supp{
2} ⊂[−1, 1]. The frequency content of each
shearlet atom, which is yielded from the mother shearlet window, is supported on a pair of trapezoids
that are symmetric with respect to the origin point. They are centered between the two lines, y = mx,
with slopes m = s ± √a. We refer to s as the orientation of this pair of trapezoids. Figure 9.45 shows

1.09.5 Shearlets and Their Applications
537
FIGURE 9.45
The frequency supports of some shearlet atoms in the Fourier plane, a is the scale, s is the orientation of a
pair of trapezoids.
some examples of the trapezoid pairs. Each supporting region may be written as

(x, y) ∈R2 : x ∈
,
−2
a , −1
2a
-
∪
, 1
2a , 2
a
-
, −√a + s ≤y
x
≤√a + s
.
.
So far, the mother shearlet window generates real-valued spatial shearlet atoms; however, the complex
valued spatial shearlet atoms can also be obtained if the support of 
1 is conﬁned to
 1
2, 2

. In both cases,
the mother shearlet reproduces the shearlet atoms that are angularly oriented waveforms in the spatial
domain, with its shape controlled by the shearing parameter s and scaling parameter a, see Figure 9.46.
1.09.5.4 The continuous shearlet transform
Deﬁnition 15.
Let f (x) ∈L2(R2) , and let {ψa,s,b(x) : a > 0, s ∈R, b ∈R2} be the family of
shearlet atoms deﬁned in Eq. (9.86), the continuous shearlet transform is deﬁned as
γ f (a, s, b) =

f , ψa,s,b

=

R2 f (x)ψ∗
a,s,b(x)dx,
(9.91)
where {γ f (a, s, b)}a>0,s∈R,b∈R2 are called the shearlet coefﬁcients of the function f (x).
Note that the shearlet coefﬁcients of the function f (x) can also be obtained in the Fourierdomain as
γ f (a, s, b) =

F, 
a,s,b

=

R2 F()
∗
a,s,b()d

538
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
FIGURE 9.46
The eight spatial waveform images of shearlet atoms correspond to different orientations at scale = 3 are
shown here. Note that the images are zoomed to show only the center portions of the images.
= a3/4

R2 F()
1(ax)
2

a−1
2
y
x
−s

e j2π⟨,b⟩d.
(9.92)
It can be proved that the mother shearlet window function given in the previous section satisﬁes the
following admissibility condition,

R
 ∞
0
|
(MT
as)|2a−3/2da ds = 1 for a.e.  ∈R2.
This admissibility condition shows that the family of shearlets generates a reproducing system, which
yields the following theorem,
Theorem 8.
For a function f ∈L2(R2), the exact reproduction can be obtained by the following
generalized Calderòn reproducing formula:
f (x) =

R2
 ∞
−∞
 ∞
0
γ f (a, s, b)ψa,s,b(x)da
a3 ds db.
(9.93)
In addition, we have
∥f (x)∥2 =

R2
 ∞
−∞
 ∞
0
|γ f (a, s, b)|2 da
a3 ds db.
(9.94)
1.09.5.4.1
Properties of the sheartlet transform
A 2-D function f that is smooth away from discontinuities along a curve  can be modeled as an image
in [0, 1]2 showing an object and a boundary, written as, O ∪ = ∪L
n=1(On ∪Cn), namely, L small

1.09.5 Shearlets and Their Applications
539
objects On, n = 1, 2, . . . , L form a connected open set O in the image with a boundary, , connected
by piecewise smooth curves of ﬁnite length, with ﬁnite number of corner (junction) points.
As a →0, a shearlet atom has a slim support in the spatial domain, see Figure 9.46. If this slim
support is centered at a point on a edge curve and is aligned along the normal direction of the curve, it
results in slow decay or large value shearlet coefﬁcients.
The following conclusions demonstrate that the location and orientation of singularities can be well
characterized by the decay rate of continuous shearlet coefﬁcients, which can be used to further classify
the geometrical features at singularity points [121,122,127].
•
If b /∈, then
lim
a→0+ a−Nγ f (a, s, b) = 0,
for ∀s ∈R, ∀N ∈N,
namely, the continuous shearlet coefﬁcients satisfy
|γ f (a, s, b)| ≤Const.aN,
∀N ∈N,
when a →0 + .
•
If b ∈ is a regular edge point, namely, the curve is at least C2 smooth near b, but s is not at the
normal direction of the curve at b then we have
lim
a→0+ a−Nγ f (a, s, b) = 0,
namely, γ f (a, s, b) decays rapidly to 0 as a →0.
•
If b ∈ is a regular edge point, namely, the curve is at least C2 smooth near b, and s is at the normal
direction of the curve at b; that is, s is at the orientation that is perpendicular to  at b, we have
lim
a→0+ a−3/4γ f (a, s, b) = Const. fb,
where fb is the jump of a function f at b along the normal direction.
In regard to the case that b ∈ is a corner (junction) point, the following results follow:
•
If b ∈ is a corner (junction) point, that is, more than two curves merged together at b, and the
curves are not smooth at b, and if s corresponds to one of the normal directions of the curves, we
have
0 < lim
a→0+ a−3/4γ f (a, s, b) < +∞.
Namely, γ f (a, s, b) decays as a3/4, as a →0.
•
If b ∈ is a corner (junction) point, but s does not correspond to any of the normal directions
of the curves, the shearlet transform decays as a9/4 except in some special case. Note that, in the
corner points, as a →0+, we will have γ f (a, s, b) decays as a3/4 in two or more different normal
directions at b when two or more curves are merged together at b.
•
A spike-type singularity point has a different behavior for the decay of the shearlet coefﬁcients. For
example, for a Dirac delta function centered at b0, we have |γ f (a, s, b)| ≍a−3/4, as a →0 for all
s when b = b0. Therefore, the shearlet coefﬁcients actually grow at ﬁne scales for b = b0. This is
in contrast to the case that |γ f (a, s, b)| rapidly decays to zero if b is a regular edge point.

540
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
The Lipschitz regularity is commonly used to describe the local regularity of a function. In shearlet
theory, it is shown [121] that the decay rate of the shearlet coefﬁcients of a function f depends on the
local regularity of f. This further says that amplitudes of the shearlet coefﬁcients are useful to describe
the regularity points of a curve when the curve can be measured by Lipschitz regularity. If inequality
(9.3) holds for all b ∈O with constant K > 0 that is independent of b0, then the function f is uniformly
Lipschitz α over an open set O.
The following result [121] provides us with a further insight of the shearlet coefﬁcients. It takes
advantage of the fact that the mother shearlet has an inﬁnite number of vanishing moments, namely

xkψ(x, y)dx = 0,
for k = 0, 1, . . .
Theorem 9.
If function f (x) ∈L2(R2) is Lipschitz-α, α > 0 near a point b0, we will have
|γ f (a, s, b)| ≤Const.a
1
2

α+ 3
2
 
1 +
a−1/2(b −b0)


, for a < 1.
1.09.5.4.2
Shearlets in the horizontal and vertical directions
In this section, we explain that the family of shearlet atoms can be split into two families, one in
the horizontal direction, another one in the vertical direction. The separation will ease the numerical
implementation of the shearlet transform because the non-uniform angular covering of the frequency
plane becomes a problem when the slope of a trapezoid is too steep, namely, when |s| takes a large
value, see Figure 9.45.
The following proposition ensures that reproducing a function f ∈L2(R2) using shearlets can be
separated into reconstruction under three systems: two (horizontal and vertical) shearlet systems, plus a
coarse scale isotropic system. First, we introduce the horizontal shearlet family, which can be deﬁned by
restricting the value of a and s to 0 < a < 1, −2 ≤s ≤2. The shearlets are focused on the horizontal
direction zone C(h) deﬁned as
C(h) =

(x, y) ∈R2 : |x| ≥2 and

y
x
 ≤1
.
.
Proposition 7.
Any function f (x) ∈L2(R2) that has its support in the horizontal zone C(h) can be
exactly reconstructed by the following reproducing formula:
f (x) =

R2
 2
−2
 1
0
γ f (a, s, b)ψa,s,b(x)da
a3 ds db,
(9.95)
assume the mother shearlet ψ(x) satisﬁes the conditions given in Section 1.09.5.3.1, and the shearlet
atoms, ψa,s,b(x), are deﬁned as in Eq. (9.86).
The proof of this proposition can be found in [120]. The overall requirement for s is to indeed satisfy
−√a −1 < s < √a + 1. This proposition can be extended to show that any function f (x) ∈L2(R2)
with support in the vertical zone deﬁned as,
C(v) =

(x, y) ∈R2 : |x| ≥2 and

y
x
 > 1
.
,

1.09.5 Shearlets and Their Applications
541
can be exactly reproduced using the shearlet system deﬁned by Eq. (9.86); however, the matrix Mas
should be replaced by MT
as, and the mother shearlet window be replaced by ψ(v) with its Fourier
transform, 
(v)(), being deﬁned as

(v)() = 
1(y)
2
x
y

.
To distinguish the mother shearlet functions for the vertical/horizontal zones, we write the one used
in generating the horizonal shearlets as ψ(h)(x) (ψ(v)(x) respectively). In addition, we can also deﬁne
an isotropic function φ(x) as a C∞window function in the spatial plane R2, its Fourier transform is
deﬁned as () = 1 for  ∈[−1/2, 1/2]2, () = 0 for  /∈[−2, 2]2. This function, , can be
called the scaling function, or the father shearlet function.
Now, any function f (x) ∈L2(R2) can be reconstructed using these three systems introduced
above as
f (x) =

R2⟨f ( · ), φ( · −b)⟩φ(x −b)db
+

R2
 2
−2
 1
0
γ (h)
f
(a, s, b)ψ(h)
a,s,b(x)da
a3 db ds
+

R2
 2
−2
 1
0
γ (v)
f (a, s, b)ψ(v)
a,s,b(x)da
a3 db ds.
(9.96)
Typically, in ﬁnding the salient features of an image, only the high frequency content of a function in
L2(R2) matters, and it corresponds to the curvelet atoms in the horizontal and vertical cones, written as
γ (h)
f
(a, s, b) and γ (v)
f (a, s, b). In general, we refer to the following two horizontal/vertical cones after
we separate the lowpass content from the bandpass content.
1. The horizontal cone,
D(h) =

(x, y) :

y
x
 ≤1
.
.
2. The vertical cone,
D(v) =

(x, y) :

x
y
 ≥1
.
.
Separating shearlet atoms into two families that cover the horizontal or vertical directions has made it
easier for numerical implementation of the shearlet transform. Indeed, each “vertical” direction shearlet
atom can be obtained by rotating the corresponding “horizontal” shearlet atom by π
2 . Figure 9.47 shows
some examples of horizontal and vertical shearlet atoms for s = 0, s = 1 and some a. In the following,
we focus on the numerical implementation of shearlets in these two directions.
1.09.5.5 The discrete shearlet atoms
Several discretization methods may be chosen to generate the discrete shearlet atoms along the horizontal
cone and the vertical cone directions. We discretize the continuous shearlet transform by sampling the

542
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
FIGURE 9.47
The frequency support of some horizontal and vertical shearlet atoms in the Fourier plane at two different
scales.
scale, shear and translation parameters as follows [117]: the scale parameter is sampled at a = 2−2k,
k ≥0; the shearing parameter is sampled as sk,l = l2−k, −2k ≤l < 2k; and the translation parameter
is sampled at b = Ak
2Blm, with m ∈Z2, and
Ak
2 =
 2−2k
0
0
2−k

and
Bl =
 1 −l
0
1

.
After sampling, Eq. (9.87) is rewritten as
ψk,l,m(x) = 23k/2ψ

B−1
l
A2−k 
x −Ak
2Blm

.
(9.97)
The following equation leads to the discretization method that we introduce in this section,
%
TAk
2Bl b
&
DAk
2Blψ(x) = DAk
2Bl Tbψ(x),
x ∈R2.
The discrete shearlet atoms in the horizontal direction are therefore given as the following set,
%
ψ(h)
k,l,m(x) = DAk
2Bl Tbψ(h)(x) : k ≥0, −2k ≤l < 2k, m ∈Z2&
.
The discrete shearlet atoms can be further written as,
ψ(h)
k,l,m(x) = 23k/2ψ(h) 
B−1
l
A−k
2 x −m

,
(9.98)
where
B−1
l
A−k
2
=
 22k
l2k
0
2k

and
Ak
2Bl =
 2−2k
−l2−2k
0
2−k

.

1.09.5 Shearlets and Their Applications
543
The Fourier transform of ψ(h)
k,l,m(x) can be written as

(h)
k,l,m() = 2−3k/2
(h)

Ak
2Bl
T


e−j2π

Ak
2 Blm,

= 2−3k/2
1

2−2kx


2

2k y
x
−l

e−j2π

Ak
2 Blm,

.
(9.99)
The mother shearlet function can be similarly chosen as in the continuous case. Recall that the
function 
1(x), 
2(y) are C∞(R), and they are well localized in the Fourier plane. The window

1(x) is supported on [−2, −1/2] ∪[1/2, 2] in the Fourier plane, and it satisﬁes
∞

k=0

1

2−2kx

2
= 1, for |x| > 1
2.
(9.100)
The window 
2(y) is supported on [−1, 1] in the Fourier plane, and it satisﬁes
|
2(y −1)|2 + |
2(y)|2 + |
2(y + 1)|2 = 1, for |y| ≤1,
(9.101)
which leads to the following equation,
2k−1

l=−2k

2

2ky + l

2
= 1, for |y| ≤1, k ≥0.
(9.102)
The Fourier content of a discrete shearlet atom, 
(h)
k,l,m, is supported on a pair of trapezoids oriented
along a line with slope l2−k, of size about 22k × 2k, written as
Wk,l =

 = (x, y) : x ∈
/
−22k+1, −22k−10
∪
/
22k−1, 22k+10
,

y
x
−l2−k
 ≤2−k
.
.
This system of discrete shearlet atoms deﬁnes a Parseval frame on the horizontal cone D(h). An exact
reproducing theorem can then be obtained and stated as,
Theorem 10.
Let the horizontal shearlet atoms be deﬁned as in Eq. (9.98), using the shearlet window
that satisﬁes Eqs. (9.100) and (9.102). This shearlet system is a Parseval frame for L2(D(h),2).
The discrete shearlets on the vertical zone can be similarly deﬁned as above. In addition, the lowpass
isotropic scaling shearlets can be generated by the translations of a scaling function.
1.09.5.6 Numerical implementation of shearlet transform
A numerically efﬁcient implementation of a shearlet transform was previously introduced in [119,128]
based on combining a Laplacian pyramid with appropriate shearing ﬁlters. The problem with the imple-
mentation is the large side lobe effects around signiﬁcant edges. Recently, an improved implemen-
tation based on separately calculating the vertical and horizontal shearlets was proposed by Yi et al.
[13,123,124] and will be introduced here.

544
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
For convenience, the new implementation reformulates the shearlet transform by introducing the
following functions in the horizontal and vertical zones separately,
W (h)
kl () = 2k/2
2

2k y
x
−l

χD(h)(),
(9.103)
W (v)
kl () = 2k/2
2

2k x
y
−l

χD(v)().
(9.104)
The function W (d)
kl () where d = v, h is a window function supported on a pair of trapezoids in the
Fourier plane. It corresponds to a spatial function w(d)
kl (n1, n2). Now the Fourier content of the shearlets
in Eq. (9.98) can be written as

(d)
k,l,m() = 2−2kV (d)(2−2k)W (d)
kl ()e−j2π⟨m,⟩,
d = v, h,
where V (d)(x, y), d = v, h is deﬁned as
V (h)(x, y) = 
1(x) and V (v)(x, y) = 
1(y).
The discrete shearlet transform of a discrete sampled function f (n1, n2) can be computed as
γ (d)
f
(k,l, m) =

R2 F()
(d∗)
k,l,m() d
=

R2 2−2k F()V (d∗)(2−2k)W (d∗)
kl
()e j2π⟨m,⟩d
=

R2 V (d∗)
k
f ()W (d∗)
kl
()e j2π⟨m,⟩d for d = v, h.
(9.105)
Note that here “∗” as a sup-script indicates complex conjugate, with
V (d)
k
f () = aF()V (d)(a),
a = 2−2k.
The V (d)
k
f () is the output from the highpass ﬁlter in the horizontal/vertical zone. Equation (9.105)
can be viewed as the inverse Fourier transform, which is equivalent to a convolution in the spatial plane,
and can be written as
γ (d)
f
(k,l, m) =

v(d)
k
f ∗w(d)
kl

(m).
Here v(d)
k
f is the inverse Fourier transform of the function V (d∗)
k
f (), and can be written as
v(d)
k
f (b) =

R2 V (d∗)
k
f ()e j2π⟨b,⟩d.
The value of v(d)
k
f (b) is obtained through an iterative calculation as in the algorithm introduced
later. The value of w(d)
kl (b) is the inverse Fourier transform of W (d∗)
kl
(),obtained through a mapping of

1.09.5 Shearlets and Their Applications
545
FIGURE 9.48
The mapping from a wedge in the Fourier plane to a rectangle in the Pseudo-polar coordinates according to
Eq. (5.6).
the Fourier plane from the Cartesian grid to the pseudo-polar grid, which is deﬁned using the following
pseudo-polar coordinates (ζx, ζy) ∈R2:
(ζx, ζy) =

x, y
x

if (x, y) ∈D(v);
(ζx, ζy) =

y, x
y

if (x, y) ∈D(h).
Figure 9.48 shows the mapping from the Fourier plane to the pseudo-polar coordinates. Under this
mapping, the following result is obtained,
V (d)
a
f (ζx, ζy) = V (d)
a
f (x, y),
W (d)(2k(ζy −l)) = W (d)
kl (x, y).
Note that each directional window W (d)
kl
in the Fourier plane can be obtained by translating the window
W (d) in the pseudo-polar grid using the function 
2 in Eqs. (9.103) and (9.104), resulting in the
directional localization supported in a wedge in the Fourier plane, as shown in Figure 9.49. This results
in the spatial waveform w(d)
kl (m) to be used in the following algorithm.
The digital implementation of the discrete shearlet transform in Eq. (9.105) can be realized through
a ﬁlter bank as follows. Let Hk (Gk) be the lowpass (highpass) ﬁlter of a wavelet transform with
2k −1 zeros inserted between consecutive coefﬁcients of a given 1-D ﬁlter H (G respectively). Deﬁne
u ∗(H, G) to be the separable convolution of the rows and the columns of u with H and G, respectively.
Notice that G is the wavelet ﬁlter corresponding to a highpass odd function ψ1, H is the ﬁlter
corresponding to the coarse scale. The window W (d) is related to an even lowpass function ψ2. The
ψ1and ψ2 can be selected through the 1-D Meyer’s wavelets.
The following is a cascade algorithm for implementing the discrete shearlet transform.
Let f ∈l2(Z2) be a 2-D image, deﬁne
S0 f = f (x)
Sk f = (Sk−1 f ) ∗(Hk, Hk), k ≥1.

546
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
FIGURE 9.49
The 23 partition windows of the Fourier plane.
For d = v, h, the discrete shearlet transform can be calculated by the following
γ (d)
f
(k,l, m) =

v(d)
k
f ∗w(d)
kl

(m),
for k ≥0, −2k ≤l ≤2k −1, m ∈Z2,
(9.106)
where
v(h)
k
f = (Sk f ) ∗(Gk, δ), v(v)
k
f = (Sk f ) ∗(δ, Gk).
Figure 9.50 shows a representation of the discrete shearlet coefﬁcients for different orientations at
decomposition levels 2 and 3. For convenience, the vertical and horizontal shearlet coefﬁcients are
re-labeled according to the parameter l that represents the orientation,
γ f (k,l, m) =

γ (h)
f
(k,l −1 −2k, m)
1 ≤l ≤2k+1,
γ (v)
f
(k, 3 ∗2k −1 −l, m) 2k+1 < l ≤2k+2.
(9.107)
1.09.5.7 Applications
In the following, we are to separately introduce the applications of shearlet transform in processing edge
related information, for example, estimation of edge orientation, edge detection, feature classiﬁcation,
and image denoising.
Edge related tasks, such as edge detection can be very difﬁcult when noise, mixed with multiple
edges, is presented in an image. The asymptotic decay property of the shearlet coefﬁcients γ f suggests
that a shearlet transform can be a very efﬁcient tool to determine the edges and their orientations inside an
image f. A shearlet transform not only captures the singularities in a 2-D function, but also identiﬁes the
geometrical feature of curves where the singularities are located. This leads to the feature classiﬁcation,
namely, to determine whether a singularity point is a regular edge point or a irregular edge point, such
as a corner and junction point.

1.09.5 Shearlets and Their Applications
547
FIGURE 9.50
(a) The image of a disk is shown. (b1-b4) The representations of shearlet coefﬁcients of the disk image at mul-
tiple scales are shown for several values of the orientation index l ) for scale = 2. (c1-c8) The representations
for scale = 3 are shown.
Another difﬁcult task in image processing is denoising. Noise is the most prevalent as spike singu-
larities are classiﬁed as isolated points that have signiﬁcantly higher gradients than theirs neighbors.
The fact that the shearlet transform coefﬁcients on spike points actually grow at ﬁne scales shows that
a shearlet transform can be used to distinguish noise points from true edge points, and can therefore be
an efﬁciency tool for denoising.
1.09.5.7.1
Estimation of edge orientation
Detecting edge orientation using a shearlet transform takes advantage of geometrical information con-
tributions at various scales, directions, and locations. Multiple orientations may be found when there

548
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
are more than two curves merged at a location. Recently, Yi et al. [13,123,124] proposed to estimate
the edge orientation using the following formula:
l(k, m) = arg max
l
|γ f (k,l, m)|,
(9.108)
where m is an edge point, k is a ﬁne scale. The orientation angle, l(k, m), can then be directly calculated.
Note that this angle is a discrete value associated withl = 1, 2, 3, . . . , 2k possible orientations. To reduce
the quantization error, the values of the orientations will be interpolated using a parabolic function of l to
model the continuous shearlet transform γ f (k,l, m), and the orientation angle of the edge is associated
with the maximum value in the parabolic function. The following procedure describes the algorithm
that uses shearlet transform to detect the orientation of an edge.
Shearlet Orientation Detection Algorithm:
•
Compute l1 = l(k, m) using Eq. (9.108); let l0 = (l1 −1)(mod N),l2 = (l1 + 1)(mod N), where
N is the size of the set of indices l.
•
li, i = 0, 1, 2 are the slopes of three adjacent discretized directions. Let θi, i = 0, 1, 2, be the angles
of orientations calculated from the li.
•
Let S(θ) = c1θ2 + c2θ + c3. When θ = θi, S(θi) is identiﬁed with γ f (k,li, b), i = 0, 1, 2.
Then c1, c2, c3 is obtained by ﬁtting the system S(θi) = c1θ2
i + c2θi + c3. The detected angle,
θmax = −c2
2c1 , c1 < 0 is the value where the parabolic function S(θ) achieves its maximum.
Figure 9.51 provides an error analysis of applying this algorithm to an image with a circle object.
1.09.5.7.2
Feature classiﬁcation
We present in this section a computationally efﬁcient shearlet approach for classifying several different
feature points in an image. This method has advantages over wavelet-based method, on account of the
fact that the shearlet transform catches the directional features of an image.
In a typical image, there usually exist four classes of points, namely, junction and corner points;
points near edges; points on smooth edges; points inside a smooth region. The different types of points
may be classiﬁed by examining the behavior of the discrete shearlet transform coefﬁcients according
to the shearlet properties at these points.
First, at a ﬁxed scale k0, we deﬁne the energy function at each spatial point m as
Ek0(m) =

l
|γ f (k0,l, m)|.
(9.109)
The points with large energy values E(m) are classiﬁed as boundary points.
Second, deﬁne the following function:
sm0(l) = |γ f (k0,l, m0)|.
The function is tested on the boundary points, and the corners and junction points will be separated
from the regular edge points by applying the following criteria:
•
The point m0 is a corner/junction point if more than one peak is presented in the function sm0(l) at m0.
•
The point m0 is a regular edge point if the function sm0(l) has only a single peak at m0.

1.09.5 Shearlets and Their Applications
549
FIGURE 9.51
Courtesy image [124]. Comparison of the average error in the estimation of edge orientation (expression
(3.4.9)), for the disk image shown on the left, using the wavelet method (dashed line) versus the shearlet
method (solid line), as a function of the scale a, for various SNRs (additive white Gaussian noise).
In order to implement this algorithm, for each boundary point m at scale k0, let Lm be the set
that collects all the orientations at which the local maxima of the amplitudes of the discrete shearlet
coefﬁcients occur along parameter l, namely,
Lm = {l : |γ f (k0,l, m)| > |γ f (k0,l + 1, m)| and |γ f (k0,l, m)| > |γ f (k0,l −1, m)|}.
To verify the signiﬁcance of the peaks in Lm for each boundary point m, in the following algorithm,
we use normalized shearlet coefﬁcients at the point m deﬁned as the following in order to ﬁnd the local
maximum points,
pm(l) =

|γ f (k0,l,m)|

l∈Lk |γ f (k0,l,m)|, l ∈Lk,
0,
l /∈Lk.
(9.110)

550
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
Feature Classiﬁcation Algorithm:
•
Use the energy function Ek0(m) deﬁned in Eq. (9.109) to cluster the image points into three mutually
exclusive sets I1, I2, I3, the K-Mean clustering algorithm with Euclidean metric is used in the
clustering algorithm.
•
Reorder the index of sets Ii, namely, let
imax = argmax
i
1
|Ii|

m∈Ii
Ek0(m), and imin = argmin
i
1
|Ii|

m∈Ii
Ek0(m).
The set Iimax is the set of boundary points, because the signiﬁcant coefﬁcients only occur at the
boundary points. The set Iimin contains regular points inside a region because the coefﬁcients rapidly
decay to zero at these points. The set Ii between sets Iimin and Iimax contains the near edge points.
•
For a point m ∈Iimax, sort the entries of the normalized coefﬁcients pm(l) from the greatest to the
smallest, and denote it as [ ˜pm(1), . . . , ˜pm(N)]. Using again the K-Mean clustering algorithm on
Iimax, this set can be further classiﬁed into one group of smooth edge points, and another group of
corner and junction points according to the number of peaks at each boundary point.
Notethatcornerpointscanbefurtherseparatedfromjunctionpoints,andjunctionpointscanbeseparated
into different classes corresponding to different geometric features. Figure 9.52 illustrates the result of
applying the feature classiﬁcation algorithm.
1.09.5.7.3
Edge detection
Edge detection is a difﬁcult task when noise is presented, and when several edges intersect each other
and they are close together. The asymptotic decay rate of the continuous shearlet transform provides a
better solution to precisely capture the geometry of edges, and it can be used to obtain both locations and
orientations of edges in an image f. Here, we describe an effective edge detection scheme to precisely
detect the shape of an edge using the shearlet transform that can be attributed to the anisotropic analytic
and geometric properties of shearlets.
In this algorithm, the ﬁrst step is to identify the edge candidates in an image f. They are selected by
the shearlet transform modulus maxima at the kth scale level, that is, those points, m, that are the local
maxima of the function
Mk f (m)2 =

l
|γ f (k,l, m)|2.
The second step is to conﬁrm edge points by excluding the noise points. According to the property
of shearlet transform, if m is an edge point, the shearlet transform of f has the property that
|γ f (k,l, m)| ∼C2−βk, β > 0.
Note that points with β < 0 will be classiﬁed as noise; therefore, the edge points can be identiﬁed as
those points that have β > 0, and the task of edge detection is transferred to the task of determining the
sign of β.
Theoretically, this can be estimated by computing the best linear ﬁt to data
{log |γ f (k,l, m)|}k=1,2,...,L.

1.09.5 Shearlets and Their Applications
551
FIGURE 9.52
Courtesy image [124]. (a1–a3) Test images. (b1–b3) Identiﬁcation of corners and junctions. (c1–c3) Iden-
tiﬁcation of smooth edge points. (d1–d3) Identiﬁcation of points near the edges. (e1–e3) Identiﬁcation of
regular points (smooth regions).
The magnitude of the shearlet coefﬁcients at each point is compared with the values of its neighbor’s
along the gradient direction (this is obtained from the orientation map of the shearlet decomposition). If
the magnitude is smaller, the point is discarded; if it is the largest, it is kept. This yields a set of possible
edge points. This is the so called non-maximal suppression routine. Further, this edge candidate set
will be ﬁltered by a window ﬁlter, and a smooth edge set can be selected by threshold. In practice, the
following shearlet edge detection algorithm is implemented to simplify the computation complexity.
Shearlet Edge Detection Algorithm:
•
Let f ∈l2(Z2) be a 2-D image, calculate γ (d)
f
(k,l, m) given as follows:
γ (d)
f
(k,l, m) =

v(d)
k
f ∗w(d)
kl

(m),
for k ≥0, −2k ≤l ≤2k −1, m ∈Z2,
(9.111)
where
S0 f = f (x),

552
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
Sk f = (Sk−1 f ) ∗(Hk, Hk), k ≥1,
v(h)
k
f = (Sk f ) ∗(Gk, δ),
v(v)
k
f = (Sk f ) ∗(δ, Gk).
•
Deﬁne
χ(d)
l
(m) =

1 if γ (d)
f
(k,l, m) > γ (d)
f
(k −1,l, m)
0 otherwise
,
d = v, h.
(9.112)
This deﬁnes a function to indicate the points that has negative Lipschitz regularity.
•
Based on this indicator function, deﬁne
R(d)
k
f (m) =

l
γ (d)
f
(k,l, m)χ(d)
l
(m),
d = v, h.
•
Modify v(d)
k
f , d = v, h, according to the following formula:
v(d)
k
f =

v(d)
k
f + R(d)
k
f
if |v(d)
k
f | ≤|R(d)
k
f |
R(d)
k
f
otherwise
,
d = v, h.
(9.113)
This formula modiﬁes v(d)
k
f so that the value of v(d)
k
f is kept small (unchanged) for locations with
positive Lipschitz regularity and the value of v(d)
k
f is increased over the scales for locations with
negative Lipschitz regularity.
•
After the discrete shearlet γ (d)
f
(k,l, m) is calculated using the updated v(d)
k
f . The edge at level k
will be decided by checking the local maxima of the following function:
Ek(m) =
 
l
γ (v)
f (k,l, m)
!2
+
 
l
γ (h)
f
(k,l, m)
!2
.
Figure 9.53 shows the results of edge detection, in which the Pratt’s ﬁgure of merit (FOM) is used
to measure the performance of an edge detector [129].
1.09.5.7.4
Image denoising
The properties of shearlet coefﬁcients at ﬁne scales show that a 2-D function f can be reproduced using
the following shearlet representation, see [130],
f (b) =

k,l,m∈M1
⟨f , ψk,l,m⟩ψk,l,m(b) +

k,l,m∈M2
⟨f , ψk,l,m⟩ψk,l,m(b),
(9.114)
where M1 is the set of coefﬁcients associated with the smooth regions of f, and M2 is the set of coefﬁcients
associated with the edges of f.
A simple approach to achieve image denoising is carried out by using a shrinkage approach to remove
shearlet coefﬁcients below a threshold, τ. A denoised image using the discrete shearlet transform can

1.09.5 Shearlets and Their Applications
553
FIGURE 9.53
Courtesy image [124]. Results of edge detection methods. From (a) to (e): (a) original image, (b) noisy image
(PSNR = 24.58 dB), (c) Sobel result (FOM = 0.54), (d) wavelet result (FOM = 0.84), and (e) shearlet result
(FOM = 0.94).
be expressed as
˜f (b) ≈

k,l,m∈M1
⟨f , ψk,l,m⟩ψk,l,m(b) +

k,l,m∈Mc
2
⟨f , ψk,l,m⟩ψk,l,m(b),
(9.115)
where Mc
2 is the set of shearlet coefﬁcients whose values are greater than τ. This approach is able
to optimally capture directional features, and reach the desired denoising effect. Figure 9.54 shows a
denoising result using the shrinkage method.

554
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
FIGURE 9.54
In this picture, the noisy brain image is shown to the left, and the ﬁltered image using the shearlet denoising
method, is shown to the right.
A Appendix
A.1 The z-transform
Much like in the continuous time domain, we can cover the whole complex plane by using a Laplace
Transform in contrast to the harmonic Fourier Transform along the imaginary axis, we may cover the
whole interior of the unit circle in the discrete domain by invoking the so-called z-Transform. For
purposes of direct relevance to the derivations of the various multi-scale transforms in this chapter, we
deﬁne the z-transform in the 2-D plane. Given a 2-D discretized signal, x(n), n ∈Z2, its z-transform,
X(z), z ∈Z2 is deﬁned as
X(z) =

n∈Z2
= x(n)z−n =

n∈Z2
x(n)z−n1
1
z−n2
2
, with n = (n1, n2), z = (z1, z2).
Note that the z-transform of an image x(n) is reduced to its discrete time Fourier series when we evaluate
its z-transform function at z = e jω.
Given a discrete 2-D image x(n) as an input to a 2-D linear discrete invariant system with an impulse
response of h(n), the resulting output is the following convolution sum:
y(n) = x(n) ∗h(n) =

s∈Z2
x(s1, s2)h(n1 −s1, n2 −s2).
The z-transform of the output, y(n) of the system is similarly (to the continuous analog) Y(z) =
H(z)X(z), where z-transform of the impulse response h(n), is written as H(z), and called the transfer
function of the ﬁlter system. The z-transform is a very widely used tool in the analysis and design of
discrete ﬁlter systems.

References
555
Relevant Theory: Signal Processing Theory and Statistical Signal Processing
See this Volume, Chapter 2 Continuous-Time Signals and Systems
See this Volume, Chapter 3 Discrete-Time Signals and Systems
See this Volume, Chapter 7 Multirate Signal Processing
See this Volume, Chapter 10 Frames
See Vol. 3, Chapter 4 Bayesian Computational Methods in Signal Processing
References
[1] D.H. Hubel, T.N. Wiesel, Receptive ﬁelds, binocular interaction and functional architecture in the cats visual
cortex, J. Physiol. 160 (1962) 106–154.
[2] B. Olshausen, D. Field, Emergence of simple-cell receptive led properties by learning a sparse code for
natural images, Nature 381 (1996) 607–609.
[3] B. Olshausen, D. Field, Sparse coding of sensory inputs, Curr. Opin. Neurobiol. 14 (2004) 481–487.
[4] E.J. Candès, D. Donoho, Curvelets – a surprisingly effective nonadaptive representation for objects with
edges, in: A. Cohen, C. Rabut, L. Schumaker (Eds.), Curves and Surface Fitting: Saint-Malo 1999, Vanderbilt
University Press, Nashville, 2000, pp. 105–120.
[5] E.J. Candès, D.L. Donoho, New tight frames of curvelets and optimal representations of objects with piece-
wise c2 singularities, Pure Appl. Math. (2004) (vol. to appear, comm.).
[6] E. Candès, D. Donoho, Continuous curvelet transform: I. Resolution of the wavefront set, Appl. Comput.
Harmon. Anal. 19 (2003) 162–197.
[7] E. Candès, D. Donoho, Continuous curvelet transform: II. Discretization and frames, Appl. Comput. Harmon.
Anal. 19 (2003) 198–222.
[8] M.N. Do, M. Vetterli, Contourlets, in Beyond Wavelets, in: G.V. Welland (Ed.), Academic Press, 2003.
[9] M.N. Do, M. Vetterli, The contourlet transform: an efﬁcient directional multiresolution image representation,
IEEE Trans. Image Process. 14 (2005) 2091–2106.
[10] K. Guo, D. Labate, W. Lim, G. Weiss, E. Wilson, Wavelets with composite dilations, Electr. Res. Announc.
AMS 10 (2004) 78–87.
[11] K. Guo, D. Labate, W. Lim, G. Weiss, E. Wilson, Wavelets with composite dilations and their MRA properties,
Appl. Comput. Harmon. Anal. 20 (2006) 202–236.
[12] D. Labate, W. Lim, G. Kutyniok, G. Weiss, Sparse multidimensional representation using shearlets, in: SPIE
Conf. Wavelets XI, San Diego, USA, 2005, pp. 254–262.
[13] S. Yi, D. Labate, G.R. Easley, H. Krim, A shearlet approach to edge analysis and detection, IEEE Trans.
Image Process. 18 (2009) 929–941.
[14] <http://www.vissta.ncsu.edu/>.
[15] A. Papoulis, Signal Analysis, McGraw Hill, 1977.
[16] E. Brigham, The Fast Fourier Transform. Prentice-Hall, Englewood Cliffs, NJ, 1974.
[17] D. Gabor, Theory of communication, J. IEEE 93 (1946) 429–457.
[18] S. Mallat, A Wavelet Tour of Signal Processing, Academic Press, Boston, 1997.
[19] I. Daubechies, S. Mallat, A. Willsky, Special edition on wavelets and applications, IEEE Trans. IT (1992).
[20] I. Daubechies, Ten Lectures on Wavelets, CBMS-NSF, SIAM, Philadelphia, 1992.
[21] M. Vetterli, J. Kovacevic, Wavelets and Subband Coding, Prentice Hall, Englewood Cliffs, NJ, 1995.

556
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
[22] H. Krim, On the distribution of optimized multiscale representations, in: ICASSP, vol. 5, IEEE, Munich,
Germany, 1997.
[23] G.H. Golub, C.F. VanLoan, Matrix Computations, The Johns Hopkins University Press, Baltimore, Maryland,
1984.
[24] D. Luenberger, Optimization by Vector Space Methods, John Wiley, New York, London, 1968.
[25] K. Gröchenig, Acceleration of the frame algorithm, IEEE Trans. Signal Process. 41 (1993) 3331–3340.
[26] A.B. Hamza, H. Krim, Relaxed minimax approach to image denoising, IEEE Trans. Signal Process. 49 (12)
(2001) 3045–3054.
[27] Y. Meyer, Ondelettes et opérateur, vol. 1, Hermann, Paris, 1990.
[28] S. Mallat, A theory for multiresolution signal decomposition: the wavelet representation, IEEE Trans. Pattern
Anal. Mach. Intell. PAMI-11 (1989) 674–693.
[29] Y. Meyer, Wavelets and Applications, ﬁrst ed., SIAM, Philadelphia, 1992.
[30] Y. Meyer, Ondelettes et opérateur, vol. 1, Hermann, Paris, 1990.
[31] F.J. Hampson, J.-C. Pesquet, A nonlinear decomposition with perfect reconstruction, 1998 (Preprint).
[32] P.L. Combettes, J.-C. Pesquet, Convex multiresolution analysis, IEEE Trans. Pattern Anal. Machine Intell.
20 (1998) 1308–1318.
[33] W. Sweldens, The lifting scheme: a construction of second generation wavelets, SIAM J. Math. Anal. 29(9)
(1997) 511–546.
[34] S. Mallat, Multiresolution approximation and wavelet orthonormal bases of L2(R), Trans. Amer. Math. Soc.
315 (1989) 69–87.
[35] J. Woods, S. O’Neil, Sub-band coding of images, IEEE Trans. ASSP 34 (1986) 1278–1288.
[36] M. Vetterli, Multi-dimensional subband coding: some theory and algorithms, Signal Process. 6 (1984)
97–112.
[37] F. Meyer, R. Coifman, Brushlets: a tool for directional image analysis and image compression, Appl. Comput.
Harmonic Anal. (1997) 147–187.
[38] G. Strang, T. Nguyen, Wavelets and Filter Banks, ﬁrst ed., Wellesley-Cambridge Press, Boston, 1996.
[39] P.P. Vaidyanathan, Multirate digital ﬁlters, ﬁlter banks, polyphase networks, and applications: a tutorial,
Proc. IEEE, 78 (1990) 56–93.
[40] A. Cohen, I. Daubechies, J.C. Feauveau, Biorthogonal bases of compactly supported wavelets, Comm. Pure
Appl. Math. 45 (1992) 485–560.
[41] P.P. Vaidyanathan, Multirate Systems and Filter Banks, Prentice Hall, New Jersey, 1992.
[42] R. Coifman, Y. Meyer, Remarques sur l’analyse de Fourier á fenêtre, C.R. Acad. Sci. Série I 312 (1991)
259–261.
[43] R.R. Coifman, M.V. Wickerhauser, Entropy-based algorithms for best basis selection, IEEE Trans. Inform.
Theory IT-38 (1992) 713–718.
[44] M.V. Wickerhauser, INRIA lectures on wavelet packet algorithms, in: Ondelettes et paquets d’ondelettes
(Roquencourt), 17–21 June 1991, pp. 31–99.
[45] H. Malvar, Lapped transforms for efﬁcient transform subband coding, IEEE Trans. Acoust. Speech, Signal
Process. ASSP-38, June 1990, pp. 969–978.
[46] A. Aldroubi, E.M. Unser, Wavelets in Medicine and Biology, CRC Press, 1996.
[47] A. Akansu, M.J.T. Smith, Subband and Wavelet Transforms, Kluwer, 1995.
[48] A. Arneodo, F. Argoul, E. Bacry, J. Elezgaray, J. Muzy, Ondelettes, Multifractales et Turbulence, Diderot,
Paris, France, 1995.
[49] A. Antoniadis, E.G. Oppenheim, Wavelets and Statistics, Lecture Notes in Statistics, Springer Verlag, 1995.
[50] P. Mueller, E.B. Vidakovic, Bayesian Inference in Wavelet-Based Models, Lecture Notes in Statistics, ﬁrst
ed., vol. LNS 141, Springer-Verlag, 1999.
[51] B. Vidakovic, Statistical Modeling by Wavelets, John Wiley, New York, 1999.

References
557
[52] K. Ramchandran, M. Vetterli, Best wavelet packet bases in a rate-distorsion sense, IEEE Trans. Image
Process. 2 (1993), 160–175.
[53] J. Shapiro, Embedded image coding using zerotrees of wavelet coefﬁcients, IEEE Trans. Signal Process. 41
(1993) 3445–3462.
[54] P. Cosman, R. Gray, M. Vetterli, Vector quantization of image subbands:a survey, IEEE Trans. Image Process.
5 (1996) 202–225.
[55] A.
Kim,
H.
Krim,
Hierarchical
stochastic
modeling
of
SAR
imagery
for
segmentation/
compression, IEEE Trans. Signal Process. 47 (1999) 458–468.
[56] M. Basseville, A. Benveniste, K.C. Chou, S. Golden, R. Nikoukhah, A.S. Willsky, Modeling and estimation
of multiresolution stochastic processes, IEEE Trans. Information Theory, IT-38 (1992) 766–784.
[57] M. Luettgen, W. Karl, A. Willsky, Likelihood calculation for multiscale image models with applications in
texture discrimination, IEEE Trans. Image Process. 4 (1995) 194–207.
[58] E. Fabre, New fast smoothers for multiscale systems, IEEE Trans. Signal Process. 44 (1996) 1893–1911.
[59] P. Fieguth, W. Karl, A. Willsky, C. Wunsch, Multiresolution optimal interpolation and statistical ananlysis
of topex/poseidon satellite altimetry, IEEE Trans. Geosci. Remote Sensing 33 (1995) 280–292.
[60] M.K. Tsatsanis, G.B. Giannakis, Principal component ﬁlter banks for optimal multiresolution analysis, IEEE
Trans. Signal Process. 43 (1995) 1766–1777.
[61] M.K. Tsatsanis, G.B. Giannakis, Optimal linear receivers for DS-CDMA systems: a signal processing
approach, IEEE Trans. Signal Process. 44 (1996) 3044–3055.
[62] A. Scaglione, G.B. Giannakis, S. Barbarossa, Redundant ﬁlterbank precoders and equalizers, parts I and II,
IEEE Trans. Signal Process. 47 (1999) 1988–2022.
[63] A. Scaglione, S. Barbarossa, G.B. Giannakis, Filterbank transceivers optimizing information rate in block
transmissions over dispersive channels, IEEE Trans. Information Theory 45 (1999) 1019–1032.
[64] R. Learned, H. Krim, B. Claus, A.S. Willsky, C. Karl, Wavelet-packet based multiple access communication,
Proc. SPIE 2303 (1994) 246–259.
[65] R. Learned, A. Willsky, D. Boroson, Low complexity optimal joint detection for oversaturated multiple
access communications, IEEE Trans. Signal Process. 45 (1997) 113–123.
[66] A. Lindsey, Wavelet packet modulation for orthogonally multiplexed communication, IEEE Trans. Signal
Process. 45 (1997) 1336–1339.
[67] K. Wong, J. Wu, T. Davidson, Q. Jin, Wavelet packet division multiplexing and wavelet packet design under
timing error effects, IEEE Trans. Signal Process. 45 (1997) 2877–2886.
[68] H. Krim, I. Schick, Minimax description length for signal denoising and optimized representation, IEEE
Trans. Information Theory, pp. 809–908, April 1999. IEEE Trans. on IT Special Issue, Eds. H. Krim, W.
Willinger, A. Iouditski and D. Tse.
[69] D.L. Donoho, I.M. Johnstone, Ideal spatial adaptation by wavelet shrinkage, Biometrika 81 (1994) 425–455.
[70] H. Krim, S. Mallat, D. Donoho, A. Willsky, Best basis algorithm for signal enhancement, in: ICASSP ’95,
IEEE, Detroit, MI, May 1995.
[71] N. Saito, Local feature extraction and its applications using a library of bases, PhD Thesis, Yale University,
December 1994.
[72] H. Krim, J.-C. Pesquet, On the Statistics of Best Bases Criteria, vol. Wavelets in Statistics of Lecture Notes
in Statistics, Springer-Verlag, July 1995, pp. 193–207.
[73] J. Rissanen, Modeling by shortest data description, Automatica 14 (1978) 465–471.
[74] B. Vidakovic, Nonlinear wavelet shrinkage with Bayes rules and Bayes, J. Am. Statist. Assoc. 93 (1998)
173–179.
[75] D. Leporini, J.-C. Pesquet, H. Krim, Best Basis Representation with Prior Statistical Models, Lecture Notes
in Statistics, Springer-Verlag, 1999 (Chapter 11).
[76] J. Rissanen, Stochastic complexity and modeling, Ann. Statist. 14 (1986) 1080–1100.

558
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
[77] P. Huber, Robust estimation of a location parameter, Ann. Math. Stat. 35 (1964) 1753–1758.
[78] P. Huber, Théorie de l’inférence statistique robuste, tech. rep., Univ. de Montreal, Montreal, Quebec, Canada,
1969.
[79] D. Donoho, I. Johnstone, Adapting to unknown smoothness via wavelet shrinkage, JASA 90 (1995) 1200-
1223.
[80] H. Krim, D. Tucker, S. Mallat, D. Donoho, Near-optimal risk for best basis search, IEEE Trans. Inform.
Theory (1999) 2225–2238.
[81] E.J. Candès, D. Donoho, Ridgelets: a key to higher-dimensional intermittency?, Philos. Trans. R. Soc. Lond.
Ser. A Math. Phys. Eng. Sci. 357 (1999) 2495–2509.
[82] H.F. Smith, A parametrix construction for wave equations with c1,1 coefﬁcients, Ann. Inst. Fourier (Greno-
ble), 48 (1998) 797–835.
[83] E.J. Candès, L. Demanet, D. Donoho, L. Ying, Fast discrete curvelet transforms, Multiscale Modeling and
Simulation 5 (3) (2006), 861–899.
[84] S. Mallat, Adaptive Wavelet Collocation for Nonlinear A Wavelet Tour of Signal Processing, Academic
Press, San Diego, 1998.
[85] J. Ma, G. Plonka, The curvelet transform, IEEE Signal Process. Mag. (2010) 118–133.
[86] M. Fadili, J. Starck, Curvelets and Ridgelets, Encyclopedia of Complexity and Systems Science, Meyers,
Robert (Ed.), vol. 3, Springer, New York, 2009, pp. 1718–1738.
[87] F.J. Herrmann, P.P. Moghaddam, C.C. Stolk, Sparsity- and continuity-promoting seismic image recovery
with curvelet frames, Appl. Comput. Harmon. Anal. 24 (2) (2008) 150–173.
[88] <http://www.curvelet.org/software.html>.
[89] E.J. Candès, F. Guo, New multiscale transforms, minimum total variation synthesis: applications to edge-
preserving image reconstruction (Signal Processing, special issue on Image and Video Coding Beyond
Standards) 82 (11) (2002) 1519–1543.
[90] E.J. Candès, D.L. Donoho, Recovering edges in ill-posed inverse problems: optimality of curvelet frames,
Ann. Statist. 30 (2002) 784 –842.
[91] E.J. Candès, L. Demanet, Curvelets and Fourier integral operators, C.R. Math. Acad. Sci. Paris, Ser. 336 (5)
(2003) 395–398.
[92] E.J. Candès, D.L. Donoho, Curvelets: new tools for limited-angle tomography, Technical Report, California
Institute of Technology, 2004.
[93] E. Candès, J. Romberg, T. Tao, Stable signal recovery from incomplete and inaccurate information, Commun.
Pure Appl. Math. 59 (2006) 1207–1233.
[94] E.J. Candès, L. Demanet, Curvelets and fast wave equation solvers, Technical report, California Institute of
Technology, 2005.
[95] J. Ma, A. Antoniadis, F.-X.L. Dimet, Curvelet-based snake for multiscale detection and tracking for geo-
physical ﬂuids, IEEE Trans. Geosci. Remote Sensing 44 (12) (2006) 3626–3637.
[96] J. Ma, Curvelets for surface characterization, Appl. Phys. Lett. 90 (2007) 054109.
[97] J. Ma, Deblurring using singular integrals and curvelet shrinkage, Phys. Lett. A. 368 (2007) 245–250.
[98] J.L. Starck, E.J. Candès, D.L. Donoho, The curvelet transform for image denoising, IEEE Trans. Image
Process. 11–6 (2002) 670–684.
[99] J.L Starck, M. Elad, D.L Donoho, Image decomposition via the combination of sparse representation and a
variational approach, IEEE Trans. Image Process. 14(10) (2004) 1570–1582.
[100] C. Zhang, L. Cheng, Z. Qiu, L. Cheng, Multipurpose watermarking based on multiscale curvelet transform,
IEEE Trans. Inform. Forensics Security 3 (4) (2008) 611–619.
[101] Y. Bao, H. Krim, Upon bridging scale-space to multiscale frame analysis, vol. 19, in: A. Petrosian, F.G.
Meyer (Eds.), Wavelets in Signal and Image Analysis: From Theory to Practice, Computational Imaging and
Vision, Springer, Netherlands (Chapter 6).

References
559
[102] M. Elad, Why simple shrinkage is still relevant for redundant representations? IEEE Trans. IT 52 (2006)
5559–5569.
[103] Y. Lu, M.N. Do, Multidimensional directional ﬁlter banks and surfacelets, IEEE Trans. Image Process. 16
(2007) 918–931.
[104] H.S.
Malvar,
Signal
Processing
with
Lapped
Transforms,
Artech
House,
Norwood,
MA,
1992.
[105] K.-O. Cheng, N.-F. Law, W.-C. Siu, Multiscale directional ﬁlter bank with applications to structured and
random texture retrieval, Pattern Recogn. 40 (2007) 1182–1194.
[106] M. Vetterli, J. Kovaˇcevi´c, Wavelets and Subband Coding, Prentice-Hall, Englewood Cliffs, NJ, 1995.
[107] P.J. Burt, E.H. Adelson, The Laplacian pyramid as a compact image code, IEEE Trans. Commun. 31 (1983)
532–540.
[108] R.H. Bamberger, M.J.T. Smith, A ﬁlter bank for the directional decomposition of images: Theory and design,
IEEE Trans. Signal Process. 40 (1992) 882–893.
[109] A.L. Cunha, J. Zhou, M.N. Do, The nonsubsampled contourlet transform: Theory, design, and applications,
IEEE Trans. Image Process. 15 (2006) 3089–3101.
[110] S. Durand, M-band ﬁltering and nonredundant directional wavelets, Appl. Comput. Harmon. Anal. 22 (2006)
124–139.
[111] S.K. Mitra, Digital Signal Processing: A Computer-Based Approach, third ed., McGraw Hill, 2004.
[112] M.N. Do, M. Vetterli, Framing pyramids, IEEE Trans. Signal Process. 51 (2003) 2329–2342.
[113] S. Park, M.J.T. Smith, R.M. Mersereau, The contourlet transform: an efﬁcient directional multiresolution
image representation, IEEE Trans. Image Process. 48 (2005) 797–835.
[114] M.N. Do, Directional multiresolution image representations, PhD Thesis, Swiss Federal Institute of Tech-
nology, Lausanne, November 2001.
[115] S. Park, M.J.T. Smith, R.M. Mersereau, A new directional ﬁlterbank for image analysis and classiﬁcation,
Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (1999) 1417–1420.
[116] <http://www.ifp.illinois.edu/∼minhdo/software/>.
[117] K. Guo, G. Kutyniok, D. Labate, Sparse multidimensional representations using anisotropic dilation and
shear operators, in: G. Chen, M. Lai (Eds.), Wavelets and Splines, Nashboro Press, Nashville, TN, 2006, pp.
189–201.
[118] K. Guo, D. Labate, Optimally sparse multidimensional representation using shearlets, SIAM J. Math. Anal.
(2007) 298–318.
[119] G. Easley, D. Labate, W.-Q. Lim, Sparse directional image representations using the discrete shearlet trans-
form, Appl. Comput. Harmon. Anal. 25 (2008) 25–46.
[120] G. Kutyniok, D. Labate, Resolution of the wavefront set using continuous shearlets, Trans. Am. Math. Soc.
361 (2009) 2719–2754.
[121] G. Easley, K. Guo, D. Labate, Analysis of singularities and edge detection using the shearlet transform, in:
Proceedings of SAMPTA’09, 2009.
[122] K. Guo, D. Labate, W. Lim, Edge analysis and identiﬁcation using the continuous shearlet transform, Appl.
Comput. Harmon. Anal. 27 (2009) 24–46.
[123] S. Yi, D. Labate, G.R. Easley, H. Krim, Edge detection and processing using shearlets, ICIP, 2008, pp.
1148–1151.
[124] S. Yi, Shape Dynamic Analysis, PhD Thesis, North Carolina State University, Dept. of EECS, Raleigh, NC,
2011.
[125] S. Dahlke, G. Kutyniok, G. Steidl, G. Teschke, Shearlet coorbit spaces and associated banach frames, Appl.
Comput. Harmon. Anal. 27 (2009) 195–214.
[126] K. Guo, W. Lim, D. Labate, G. Weiss, E. Wilson, The theory of wavelets with composite dilations, in: C.
Heil (Ed.), Harmonic Analysis and Applications, Birkauser, Boston, 2006, pp. 231–249.

560
CHAPTER 9 Discrete Multi-Scale Transforms in Signal Processing
[127] G. Easley, D. Labate, F. Colonna, Shearlet based total variation for denoising, IEEE Trans. Image Process.
18 (2009) 260–268.
[128] <http://www.math.uh.edu/∼dlabate/software.html>.
[129] W. Pratt, Digital Image Processing, Wiley Interscience Publications, San Diego, 1978.
[130] K. Guo, D. Labate, Characterization and analysis of edges using the continuous shearlet transform, SIAM
Imaging Sci. 2 (2009) 959–986 .

10
CHAPTER
Frames in Signal Processing
Lisandro Lovisolo* and Eduardo A.B. da Silva†
*Universidade do Estado do Rio de Janeiro (UERJ), Department of Electronics and Telecommunications
(DETEL)/Program of Electronics Engineering (PEL), Lab of Signal Processing,
Intelligent Applications and Communications (PROSAICO)
†Universidade Federal do Rio de Janeiro (UFRJ), Program of Electrical Engineering—COPPE/UFRJ,
Department of Electronics—School of Engineering
1.10.1 Introduction
Frames were introduced by Dufﬁn and Schaeffer [1] in 1952 for the study of non-harmonic Fourier
series [2,3]. The central idea was to represent a signal by its projections on a sequence of elements
{e jλnt}n, n, ∈Z, not restricting the λn to be multiples n of a fundamental frequency  as in harmonic
(traditional) Fourier series. One can readily see that the set {e jλnt}n is highly overcomplete, in the sense
that, for example, in a space L2( −T , T ), it may consist of a sequence of elements or functions that are
greater in number than a basis for L2( −T , T ), being the last given for example by harmonic Fourier
series, among others.
As a side effect, overcompleteness may make it hard to ﬁnd a representation of a function f (t) ∈
L2( −T , T ) using the set {e jλnt}n, in the form
f (t) =

n
cne jλnt.
(10.1)
This is because, due to overcompleteness, the weights or coefﬁcients cn in Eq. (10.1) can not be
computed by the simple approach of projecting f (t) into each element of {e jλnt}n. This would be
equivalent to using cn = ⟨f (t), e jλnt⟩, where ⟨f (t), g(t)⟩represents the inner product

f (t)g∗(t)dt
and g∗(t) denotes the complex conjugate of g(t). One should note that, when using an orthonormal Basis
for representing a signal (as is the sequence of elements deﬁning harmonic Fourier series), computing
the inner product is the standard approach for ﬁnding the cn.
Despite that difﬁculty, overcompleteness may bring some advantages as one may obtain more com-
pact, robust or stable signal representations than using Bases [4–7]. This is why frames are being
widely researched and employed in the last two decades in mathematics, statistics, computer science
and engineering [4].
The basic idea leading to the deﬁnition of frames is the representation of signals from a given space
using more “points” or coefﬁcients than the minimum necessary—this is the whole concept behind
overcompleteness. This idea is behind modern analog-to-digital converters (ADC) that sample signals
at rates that are higher than the Nyquist rate with low resolution [8]. Since, the signal is sampled at
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00010-3
© 2014 Elsevier Ltd. All rights reserved.
561

562
CHAPTER 10 Frames in Signal Processing
high rate, the requirement on the sample precision can be relaxed [6]. This is a different perspective
as compared to sampling based on the Nyquist criterion, when in general one assumes that a good
quantization precision is employed so that quantization errors can be neglected.
In addition to the above, there is the motivation of achieving nonuniform and/or irregular sampling
(since sampling clock jitter is equivalent to irregular sampling). This has led to a large set of results
and some applications of the “frame theory” [9]. These ideas were further extrapolated leading to the
deﬁnition of “Rate of Innovation” of signals [10]. In this case one considers the number of degrees of
freedom per unit of time of a class of signals; this is employed in [10] for sampling purposes. In some
way, these concepts have lead to the so-called “Compressed Sensing” (CS) perspective [11].
Although frames provide the mathematical justiﬁcation for several signal processing methods and
are at the heart of the ﬂow of ideas and achievements shortly and brieﬂy described above, it took a long
time until frames came deﬁnitely into play. The ﬁrst book concerning the topic was written by Young
in 1980 [2]. In [12] Daubechies, Grossman and Meyer realized that frames can somehow be understood
in a way very similar to expansions using bases. This established the link among frames and wavelets
[5,13,14], and new breath was given to the research on frames. Applications of the “frame theory” have
broadly appeared. Frames have been an active area of research, allowing the production of a plethora
of theoretical results and applications. Good material on frames can be found in [4,5,13], and a book
dedicated to this topic is [6]. We try to show a few of the relevant results regarding frames in this chapter.
1.10.1.1 Notation
In the following we use x, to denote a signal in a discrete space, for a ﬁnite dimensional discrete space
the same notation is employed. The value of the nth coordinate or sample of x is referred to by x[n].
A bold capital is used for matrices, as G. Continuous functions or signals are explicitly denoted with
reference to the independent variable, as in x(t). When just a letter like x is employed it is because
the deﬁnition or result applies in both cases—discrete or continuous spaces. ∥x∥refers to ⟨x, x⟩. The
inner product ⟨f, g⟩is to be understood as ⟨f (t), g(t)⟩=

f (t)g∗(t)dt in continuous spaces, and
⟨f, g⟩= 
n f[n]g∗[n] in discrete spaces.
1.10.2 Basic concepts
Let us try to brieﬂy introduce what are frames. A frame of a space H is a set of elements that spans H.
Therefore, a frame G = {gk}k∈K (a set of elements gk which are indexed by k ∈K), can be used to
express any x ∈H by means of
x =

k∈K
ckgk,
(10.2)
where the ck are called the frame coefﬁcients.
The only restriction that we have imposed on G = {gk}k∈K for it being a frame is that it should span
H. Therefore, G is in general overcomplete, meaning that the set of frame coefﬁcients {ck}k∈K that can
express x by means of Eq. (10.2) is not unique, as the overcompleteness of G means that its elements
are not linearly independent.
From above one sees that there may exist different ways to compute the frame coefﬁcients ck used in
Eq. (10.2) to synthesize x. We employ interchangeably both terms “expansion” as well as representation

1.10.2 Basic Concepts
563
since all the frame elements (generally in larger number than the space dimension) may a priori be
employed to express the signal. Due to overcompleteness one usually employs more elements than the
support or dimension of the signal space H to represent the signal. This is equivalent to saying that
the frame coefﬁcients for a given signal may be in larger number than the coefﬁcients employed in the
canonical signal representation.
Example 10.1.
Consider the set of elements G in R2, G = {[0 1/2], [1/4 −1/4], [−1/2 0]}. Any
vector x = [x[0], x[1]] projected into these elements gives a set of coefﬁcients:
˜c =
⎡
⎣
˜c0
˜c1
˜c2
⎤
⎦=
⎡
⎣
0
1/2
1/4 −1/4
−1/2
0
⎤
⎦
	x[0]
x[1]

=
⎡
⎣
x[1]/2

x[0] −x[1]

/4
−x[0]/2
⎤
⎦.
(10.3)
The original vector can be obtained from ˜c using, for example
	 x[0]
x[1]

= ˜GT˜c =
	 0 0 −2
2 0
0

 ⎡
⎣
˜c0
˜c1
˜c2
⎤
⎦or
	x[0]
x[1]

= ˜GT˜c =
	0
0
−2
0 −4 −2

 ⎡
⎣
˜c0
˜c1
˜c2
⎤
⎦,
(10.4)
among several other possibilities for ˜GT. That is, the ˜GT that reconstructs x from its projection on a set
G is not unique.
Consider that G is the matrix constructed from the frame elements by stacking them as rows. G is
used to compute the coefﬁcients vector ˜c using
˜c = Gx.
(10.5)
Note that in the current example dim(x) = 2×1, dim(G) = 3×2, dim(˜c) = 3×1 and dim( ˜GT) = 2×3.
If a frame for RN has K > N elements, one has that dim(x) = N × 1, dim(G) = K × N, dim(˜c) =
K × 1 and dim( ˜GT) = N × K. That is, one has that the matrix ˜GT used to obtain x = ˜GT˜c must be
such that ˜GTG = IN (IN is the identity matrix of dimension equating the dimension of the vectors in
G, which is 2 in the present example).
Obviously, the roles of the matrices G and ˜G above can be interchanged. Since ˜GTG = IN one has
that GT ˜G = IN, and thus one can employ ˜G as a projection operator on x for ﬁnding a set of coefﬁcients
c, and reconstruct x using
x = Gc.
(10.6)
Note that in this case one can make
˜GT =
	1
2
−1
1 −2 −1

,
(10.7)
or deﬁne ˜G as in (10.4).
□
As we have seen in the example above, one should note that the set c is redundant in the sense that
the vectors employed to compute the {ck}k∈K (onto which x is projected) may be linearly dependent.
One advantage of this is that the wrong computation of some coefﬁcients of the expansion or even
their losses may still allow the recovery of the original signal from the remaining coefﬁcients within

564
CHAPTER 10 Frames in Signal Processing
an acceptable error, as one has different possible signal reconstruction operators provided by the frame
concept. These ideas have led to applications of frame expansions for signal sampling with low quanti-
zation requirements [4], and also to the development of transmission systems that are robust to erasures
(when data is lost or erasured) over a network, which corresponds to the deletion of the correspondent
frame elements [15–17].
1.10.2.1 The dual frame
The above example links to the strategy for computing frame coefﬁcients using the inverse or dual frame
[4–7], that provides the dual reconstruction formulas
x =

k∈K
⟨x, gk⟩˜gk =

k∈K
⟨x, ˜gk⟩gk.
(10.8)
From Eq. (10.8) one may deﬁne ˜G = {˜gk}k∈K as the inverse or dual frame to the frame G = {gk}k∈K.
For a given signal x, since G is overcomplete, ˜G is, in general, not unique [6], as shown in Example
10.1.
Considering a frame expansion of a signal—the set of coefﬁcients ck, the frame expansion can be
viewed as a measure of “how much” the signal “has” of each frame element {gk}k∈K. As it can be readily
seen, this property can be used to infer or analyze signal characteristics. Since in an overcomplete frame
its elements are linearly dependent, at least two of its elements are not orthogonal. This implies that char-
acteristics that are different, but similar to one another, can be effectively observed by projecting the sig-
nal into different frame elements that are similar to these characteristics. If an orthogonal basis was used
instead, such characteristics would not be as evident from the projections, since each projection would
have at most a small amount of each characteristic. This way, the similar characteristic would be mixed
among different projections, and would therefore not be as evident as in the case of a redundant frame.
The reasoning in the above paragraph has been largely employed to justify the use of the so-called
Gabor frames, that are discussed in Section 1.10.5.4. Gabor frames are constructed by modulation and
translation of a prototype function, providing thus a time-frequency analysis capability [4,18–22]. Gabor
frames employ sets of time and frequency translated versions of a predeﬁned function to analyze or
synthesize a signal. These frames are named after Gabor due the their origins in [23]. The main idea is to
explore functions that have some desired energy concentration in time and frequency domain to analyze
the content or information that is represented in a signal. Several tools have been developed for achieving
this, using the decomposition of a signal into a Gabor frame [24–26]. As the example regarding the
Gaborgram in Section 1.10.6.2 ahead shows, the inverse frame elements may not resemble the frame
elements and some care must be taken when using frame expansions to capture speciﬁc signal features.
Although the elements of a frame that are used to express a signal are in general selected a priori,
frame expansions allow for the use of elements with special properties. The Balian-Low theorem [13]
shows that it is not possible to construct a basis with elements that are well localized in both time
and frequency domains simultaneously. Frames do not impose uniqueness to the signal representation,
therefore the deﬁnition of the frame elements is less restrictive than it is for basis elements. Hence, we can
achieve, for a frame, a better localization of its elements simultaneously in time and frequency domain
than for a basis, better dealing with the limitations imposed by the uncertainty principle [4,19,21].

1.10.2 Basic Concepts
565
The property of frames discussed above has been largely employed by the so-called Windowed
Fourier Analysis schemes [4,27]. In addition, since the frame coefﬁcients may not be unique for a given
signal, the set of frame coefﬁcients can be altered or improved—considering the linear dependencies
among frame elements—in order to highlight desired signal features. Moreover, frames can be designed
for speciﬁc applications depending on the features that one desires to extract or analyze in the signal.
Examples of such speciﬁc designs are ones using wavelets [4,5,7] and other time-frequency [4,28] or
structurally/ geometrically oriented analysis techniques [29,30].
In general, the computational burden of obtaining signal representations into frames surpasses the
computational demands of traditional expansions into basis, as, for example, the ones of the Fast Fourier
Transform (FFT) [31]. However, as mentioned above, with frame expansions we can obtain a separation
of signal structures that is much higher than the one that is possible using traditional approaches. Mallat
[4] explores this in his proposed super-resolution techniques.
Due to the characteristics above, frame applications range from signal coding [4], signal sampling
[1,4,6], signal analysis [4,5] and transient detection [24,25], to communication systems design [6,32].
Frames have also been related to the analysis and design of Filter Banks [33,34].
1.10.2.2 Signal analysis and synthesis using frames
A frame or its dual can be employed to analyze a signal and obtain the signal expansion, i.e., the frame
coefﬁcients, and to synthesize it from the coefﬁcients obtained in the analysis process. This is shown in
Figure 10.1. As it can be noticed, from Eq. (10.22), the roles of {gk}k∈K and {˜gk}k∈K can be interchanged
in Figure 10.1, providing different perspectives for signal analysis.
˜gK
˜g1
˜g2
gK
g1
g2
x
x =
k ck ˜gk
c1 =
x g1
c2 =
x g2
ck =
x gk
Analysis
Synthesis
FIGURE 10.1
Analysis and synthesis of a signal using a frame and its inverse—as noted in the text, the roles can be
interchanged.

566
CHAPTER 10 Frames in Signal Processing
One notes that the “system model” of Figure 10.1 is very similar to the one employed in several
signal processing tasks—for example in ﬁlter banks. This resemblance between frames and several
signal processing tasks provides a large range of potential applications for frames. To highlight an
important feature, the frame elements may be designed to highlight or analyze desired signal features
that may be “overlapping” or linearly dependent.
In the scenario of ﬁlter banks, a relevant and special case is the one of a “perfect reconstruction ﬁlter
bank” [31]. In this case the analysis and reconstruction ﬁlter banks correspond to a pair of dual frames.
A more detailed analysis of the input-output mapping in this case is provided in Section 1.10.4.1 where
we discuss frames of discrete spaces.
Some examples of common frame constructions for signal analysis purposes are Gabor and Wavelet
frames. These provide expansion systems or representations that have different supports on the time-
frequency plane. We will discuss them in Sections 1.10.5.4 and 1.10.5.5 respectively.
1.10.3 Relevant deﬁnitions
Now, that we have discussed some basic properties of frames, we are ready to present a more formal
deﬁnition.
Deﬁnition 10.1.
A sequence of elements G = {gk}k∈K in a space H is a frame for H if there exist
constants A and B, 0 < A < B < ∞, such that [4–7]
A∥x∥2 ≤

k∈K
|⟨x, gk⟩|2 ≤B∥x∥2,
∀x ∈H.
(10.9)
i. The numbers A and B are called the lower and upper frame bounds, respectively, and are not unique.
The optimal lower frame bound is the supremum on all A and the optimal upper frame bound is
the inﬁmum on all B [6].
ii. It is said that the frame is normalized [5] if ||gk|| = 1,
∀k ∈K.
□
A commonly addressed problem, related to frames, is how to determine if a given sequence of
elements {gk}k∈K in H is a frame of H. This is often refereed to as the frame characterization problem
[6]. This characterization is commonly accomplished by the frame bounds. From Eq. (10.9), one notes
that if for any signal one obtains A = 0 then the signal is orthogonal to all the elements {gk}k∈K and the
signal can not be represented using these elements. That is the reason for the requirement that A > 0.
Also, if for a given signal there is no upper bound in Eq. (10.9) then this means that the elements are
too “dense” for that signal and the frame does not provide a stable representation.
Given a set of elements in a space H, in order to verify if it is or is not a frame one may compute the
frame bounds. It is in general much easier to ﬁnd the upper frame bound B than the lower frame bound A.
Example 10.2.
For instance, note that in a space RN any quantity M > N of ﬁnite norm vectors will
always provide a B < ∞. However, for the lower bound deﬁnes if the set spans or not H and in this
case a more careful analysis is required.
□
1.10.3.1 The frame operator
Let us now deﬁne the frame operator and state the frame decomposition result.

1.10.3 Relevant Deﬁnitions
567
Deﬁnition 10.2.
Deﬁne the frame operator S{·} as
S : H →H, S{x} =

k∈K
⟨x, gk⟩gk.
(10.10)
From this deﬁnition some properties hold [6]:
i. The frame operator is bounded, invertible, self-adjoint and positive. Therefore, it assumes an inverse
S−1{·}. This is referred to as the inverse frame operator.
x = S{S−1{x}} =

k∈K
⟨S−1 {x} , gk⟩gk.
(10.11)
ii. If the lower and upper frame bound of {gk}k∈K are, respectively A and B, then

S−1{gk}

k∈K is a
frame with bounds B−1 and A−1, and the frame operator for

S−1{gk}

k∈K is S−1.
□
From the deﬁnition of the frame operator one has the frame decomposition result that provides the
reconstruction formulas [6]
x =

k∈K
⟨x, gk⟩S−1{gk} =

k∈K
⟨x, S−1{gk}⟩gk
(10.12)
Note that this provides a way to compute the ˜gk = S−1{gk} which are the elements of the inverse frame
or the so-called canonical dual.
Example 10.3.
Suppose that a normalized frame

gk

k∈K is used to express the information of a
signal x, to transmit or store this information and to reconstruct x. In this scenario, assume that one
transmits/stores the frame coefﬁcients
ck = ⟨x, S−1 
gk

⟩,
(10.13)
where we employ the inverse frame operator. From the deﬁnitions one has that
x =

k
ckgk.
(10.14)
However, due to quantization coefﬁcients are corrupted by noise. In this case one must reconstruct ˆx
ˆx =

k∈K

ck + wk

gk =

k∈K

x, S−1 
gk

+ wk

gk.
(10.15)
Obviously, due to the frame reconstruction one has that
ˆx = x +

k∈K
wkgk.
(10.16)
The reconstructed signal is corrupted by noise.

568
CHAPTER 10 Frames in Signal Processing
One should note that in this scenario, we know that the ck (obtained by means of Eq. (10.13) are the
expansion of a signal over a frame). However, this may not be the case for the set of coefﬁcients

⟨x, S−1 
gk

⟩+ wk

k∈K
(10.17)
used to reconstruct ˆx in Eq. (10.15). Assuming that wk ∈ℓ2(N)—i.e., the noise is bounded, that fact
may be compensated by projecting

⟨x, S−1 
gk

⟩+ wk

k∈K using the operator [4,6]:
Q {ck} =

k
ckS−1 
gk

, g j

j∈K
.
(10.18)
Using this operator in the projected-quantized frame coefﬁcients scenario above one has that (apply
Q{·} to Eq. (10.17)
Q

⟨x, S−1 
gk

⟩+ wk

k∈K = {ck}k∈K + Q {wk}k∈K .
(10.19)
Using this, the signal can be reconstructed by means of
ˆx =

k∈K

ck + Q {wk}
gk = x +

k∈K
Q {w}k gk.
(10.20)
Supposing that the frame

gk

k∈K is normalized, considering the quantization noise in Eq. (10.15) to
be white and zero-mean, using E[·] to denote the expected value, then in [4] it is shown that
E

|Q {w} |2
≤1
A E

|w|2
.
(10.21)
That is, the resulting noise when reconstructing the signal by means of the processed coefﬁcients as in
Eq. (10.19) is reduced if A (the lower frame bound) is greater than one.
□
1.10.3.2 The inverse frame
Deﬁnition 10.3.
The inverse frame ˜G = {˜gk}k∈K to a frame G = {gk}k∈K, is the one that provides
the reconstruction formulas
x =

k∈K
⟨x, ˜gk⟩gk =

k∈K
⟨x, gk⟩˜gk.
(10.22)
□
Example 10.4.
Let us go back to Example 10.1, where we have seen two different possibilities for
“inversion” of the frame given by the rows of the matrix
G =
⎡
⎣
0
1/2
1/4 −1/4
−1/2
0
⎤
⎦.
(10.23)

1.10.3 Relevant Deﬁnitions
569
However, one can now ﬁnd its canonical dual, associated with the inverse frame operator. Using a
pseudo-inversion algorithm [4] one ﬁnds the pseudo-inverse G+ to G(G+G = I2):
G+ =
	 1/3 2/3 −5/3
5/3 −2/3 −1/3

.
(10.24)
Note that, this is just another example of possible frame inversion, as it is shown in [4], since G assumes
an inﬁnite number of left inverses.
□
Deﬁnition 10.4.
When A = B the frame is said to be tight [6] and
S−1{·} = S{·}/A.
(10.25)
In addition:
i. Frames for which A ≈B are said to be snug [34] and for these S−1{·} ≈S{·}/A.
□
Deﬁnition 10.5.
A frame G = {gk}k∈K is said to be normalized when
∥gk∥= c, ∀k ∈K,
c > 0 is a real constant.
(10.26)
□
1.10.3.3 Characterization of frames: basic property
Frame bounds provide limits for the energy scaling of signals when they are represented using the
projections into the frame elements. Due to that, frames are often characterized in terms of their frame
bounds. It is very common to deﬁne the frame bounds ratio A/B [13]. It is also possible to deﬁne the
“tightness” of a frame from its frame bounds ratio: the closer that A and B are, then the tighter the frame is.
Daubechies [5] shows that the use of the frame operator (Eq. (10.10)) gives
S{x} = A + B
2

x −R{x}

, i.e., x =
2
A + B

k∈K
⟨x, gk⟩gk + R{x},
(10.27)
where R{x}can be understood as the error incurred in reconstructing x from the projections of x into
the frame elements instead of into the inverse frame elements.
In addition, she also shows that
R{x} = I{x} −
2
A + B S{x},
(10.28)
where I{x} is the identity operator. Therefore one has that
−B −A
B + A I{x} ≤R{x} ≤B −A
B + A I{x}
−→
∥R{x}∥≤B −A
B + A∥x∥.
(10.29)
Hence, one sees that if B/A is close to one, then the error incurred in the reconstruction by equating
the inverse frame to the direct frame is small. This error gets smaller as A and B become closer.

570
CHAPTER 10 Frames in Signal Processing
Therefore, from an analysis-synthesis perspective if a frame is tight there is no need to ﬁnd its inverse.
Tight frames are self-dual [35], that is, the inverse frame to a tight frame is a scaled version of the frame
itself.
For normalized frames, Daubechies calls A+B
2
the frame redundancy. Several studies have been
developed on normalized tight frames. For example [36] shows that these frames have some noise
reduction property. That is, if a given reconstruction distortion is required one can use less bits to
represent the frame coefﬁcients in comparison to the precision required to represent basis coefﬁcients.
This is related to the acquisition-quantization problem we have discussed previously.
Example 10.5.
Let {e1, e2, e3} be an orthonormal basis of the three-dimensional space. Deﬁne the
vectors:
g1 =
√
2
2 e1 +
√
2
2 e2
g2 =
√
2
2 e2 +
√
2
2 e3
g3 =
√
2
2 e1 +
√
2
2 e3
g4 = 1
2e1 +
√
3
2 e2
g5 = 1
2e2 +
√
3
2 e3
g6 =
√
3
2 e1 + 1
2e3.
For any x ∈R3 we have that

k
|⟨x, gk⟩|2 =


x,
√
2
2 e1

+

x,
√
2
2 e2

2
+


x,
√
2
2 e2

+

x,
√
2
2 e3

2
+


x,
√
2
2 e1

+

x,
√
2
2 e3

2
+


x, 1
2e1

+

x,
√
3
2 e2

2
+


x, 1
2e2

+

x,
√
3
2 e3

2
+


x,
√
3
2 e1

+

x, 1
2e3

2
= 2 |⟨x, e1⟩+ ⟨x, e2⟩+ ⟨x, e3⟩|2
= 2∥x∥.
Therefore the six vectors deﬁne a tight frame with A = B = 2. This number can in some sense be
understood as a measure of redundancy of the frame expansion [4].
□
Tight frames turned out to be relevant, due to their operational simplicity and applications. For
example tight frames were shown to be the most resistant to coefﬁcients erasures [15]. Erasures means
that when transiting information over a network one knows that the information is unreliable. If a frame
is used to compute the information that is transmitted over the network then the reconstruction error
under erasures is lower for a tight frame than for non tight ones. Tight frames can also provide sparse
signal decompositions [37]. Due to these and other interesting properties a lot of attention has been
given to the construction of tight frames [16,37–40].

1.10.4 Some Computational Remarks
571
1.10.4 Some computational remarks
In general signal processing algorithms are implemented in digital processors. In this case, the signal
is discrete and some simpliﬁcations take place.
1.10.4.1 Frames in discrete spaces
Consider the case of discrete spaces as the ℓ2(Z) (the space of squareable summable sequences). In gen-
eral, in such spaces, signal analysis is accomplished by using sliding-windows. This is actually the case
for Filter-Banks [31], using the structure depicted in Figure 10.1, the synthesis equation becomes [33,34]
x[n] =
K−1

k=0
∞

m=−∞
ck,mgk,m[n].
(10.30)
In the above equation each signal’s sample x[n], n ∈Z, is synthesized as a sum of the samples
gk,m[n] = gk[n −mN], N ≤K, K is the cardinality of the frame, the gk,m are translated versions
of the gk, referring to the mth N-length signal block.
Similar concepts can be brought to frames in ℓ2(Z), bearing in mind the correct usage of a frame
and its dual. In this case, one should note that Eq. (10.30) must be understood as
x[n] =
K−1

k=0
∞

m=−∞
⟨x, ˜gk,m⟩gk,m[n].
(10.31)
That is, as discussed in Section 1.10.2.2, the frame and its dual should be employed in the two distinct
tasks of analysis/decomposition and synthesis/recomposition of x.
The previous deﬁnitions on frames considered any space H. When considering ℓ2(Z), the space in
which digital signal processing applications exist (as well as in ﬁnite vector spaces—which are discussed
in the next subsection) using ﬁnite support vectors may be handy, as in FIR Filters.
A signal x ∈ℓ2(Z) can be expressed using Eq. (10.30) if the family of functions [33]
G =

gk,m : gk,m[n] = gk[n −mN], k = 0, 1, . . . , K −1, m ∈Z

(10.32)
is a frame for ℓ2(Z). This result provides an important feature of frames for ℓ2(Z).
Deﬁnition 10.6.
A set of vectors G as deﬁned in Eq. (10.32) (block translated) is a frame of ℓ2(Z), if
there exist constants 0 < A < B < ∞such that for any x ∈ℓ2(Z) one has
A∥x∥2 ≤
K−1

k=0
∞

m=−∞

x, gk,m
2 ≤B∥x∥2.
(10.33)
□
One should note that if Eq. (10.33) holds for G as in Eq. (10.32), then there exists another frame
˜G =

˜gk,m : ˜gk,m[n] = ˜gk[n −mN],
k = 0, 1, . . . , K −1, m ∈Z

(10.34)

572
CHAPTER 10 Frames in Signal Processing
that can be employed for obtaining the coefﬁcients ck,m in Eq. (10.31), Obviously, the roles of G and ˜G
can be interchanged, providing
x[n] =
K−1

k=0
∞

m=−∞
⟨x, gk,m⟩˜gk,m[n].
(10.35)
As it can be noted, given G as in Eq. (10.32), the ˜G as (10.34) satisfying Eqs. (10.31) and (10.35) is
not unique.
1.10.4.2 Finite vector spaces
In a ﬁnite vector space HN one in general restricts the frame to have K elements; and thus Eq. (10.9)
becomes
A∥x∥2 ≤
K

k=1
|⟨x, gk⟩|2 ≤B∥x∥2,
x ∈HN.
(10.36)
In this case, as motivated in previous examples some computational simpliﬁcations take place.
Deﬁnition 10.7.
Analysis and Synthesis operators:
i. Let the synthesis operator be
T {·} : CK →HN,
T {ck}k=K
k=1 =
K

k=1
ckgk,
(10.37)
where CK is a K-dimensional complex vector space.
ii. Let the analysis operator T ∗{·} (adjunct operator of T {·}) be given by
T ∗{·} : HN →CK ,
T ∗{x} = {⟨x, gk⟩}k=K
k=1 .
(10.38)
iii. The operator T {·} synthesizes x from the frame coefﬁcients ck that are obtained by the analysis
operator of a dual frame given by
 T ∗{·} : HN →CK ,
 T ∗{x} = {⟨x, ˜gk⟩}k=K
k=1 .
(10.39)
iv.
Using the analysis and synthesis operators the frame operator is then given by
S : HN →HN,
S{x} = T {T ∗{x}} =
K

k=1
⟨x, gk⟩gk.
(10.40)
□
In vector spaces the operators T {·} and T ∗{·} can be interpreted as matrices [6], as in the examples
provided before.

1.10.4 Some Computational Remarks
573
Assuming that x is a column vector that is dim(x) = N × 1 and the coefﬁcients are organized in a
column vector c(dim(c) = K × 1) as well. One has that
x
dim(x) = N × 1
c
dim(c) = K × 1
gk
dim(gk) = N × 1
T =
⎡
⎢⎣
gT
1...
gT
K
⎤
⎥⎦
dim(T) = K × N
(10.41)
T {c} = TTc
dim(TT) = N × K
(10.42)
T ∗{x} = Tx
dim(T∗) = K × N
(10.43)
S{x} = T {T{x}} = TTTx = Sx
dim(S) = N × N
(10.44)
S−1{x} =

TTT
−1
x = S−1x
dim

S−1
= N × N.
(10.45)
The dual frame elements can be computed by means of
˜gk = S−1{gk} =

TT T
−1
gk,
(10.46)
and we have that
 T {c} = ˜TT c = T

TTT
−1
c
dim( ˜TT) = N × K
(10.47)
 T ∗{x} = ˜T∗x =

TTT
−1
Tx
dim( ˜T∗) = K × N.
(10.48)
Example 10.6.
As we have seen above S = TT T. Let ρi be the eigenvalues of S, then the frame
bounds are given by A = mini ρi, and B = maxi ρi [6]. Thus, if TT T = AIN (IN is the identity matrix
of size N) then the frame is tight (A = B). Hence, for a tight frame S−1S = IN.
□
Example 10.7.
The Gram matrix or Gramian of a sequence of vectors {gk}k∈K is the matrix G,
whose elements are deﬁned by gi, j = {⟨gi, g j⟩}i, j∈K. It has been employed to analyze different frames
characteristics. For example, in [41], it is employed to investigate the symmetry properties of tight
frames, providing tight frames design procedures. In [37] the Gramian is also employed for analyzing
and designing frames with desired features.
We have seen the analysis, synthesis, and frame operators, and its inverse frames counterparts. As one
may notice, a lot of the frame characteristics can be extracted from them. Now, deﬁne the Gram matrix
G = T T T . One readily notes that dim

G

= K × K and that each of its entries gi, j = {⟨gi, g j⟩}i, j∈K.
This Gram matrix provides how similar are the frame elements one to another, therefore, it can be
understood as an indicative of the connections among the different ck.
□
Example 10.8.
In a vector space if the lower frame bound A were zero for a signal x then the
frame elements would all be orthogonal to x, and the frame would not be capable of representing x. In
addition, note that if the frame has a ﬁnite set of elements, then, necessarily, due to the Cauchy-Schwartz
inequality, the upper frame bound condition will be always satisﬁed.
□

574
CHAPTER 10 Frames in Signal Processing
1.10.5 Construction of frames from a prototype signal
It is common to process signals in order to search for speciﬁc patterns in time, frequency or even scale.
In this case, one often a priori speciﬁes a set of desired behaviors or patterns to be searched in signals
to be analyzed. These are speciﬁed as a set of pre-deﬁned waveforms. If synthesis is not required the
restrictions on the set are milder than in the case when synthesis is required. In the later case, one should
guarantee the set of pre-deﬁned functions or waveforms to be a frame. This is what we discuss now.
We suppose a signal processing scenario where a given signal has to be analyzed and synthesized using
a set of pre-deﬁned waveforms that are derived from operations such as change of position in time
(translation), change of frequency (modulation) and change of size (dilation) over a prototype signal.
In this section we discuss some conditions that make a frame to be generated when these operations are
applied to a prototype waveform.
1.10.5.1 Translation, modulation and dilation operators
There are several ways to construct frames from operations on a prototype signal, some of them are based
on translations, modulations and dilations of a function g(t) ∈L2(R)(the space of square integrable
functions) [6].
Deﬁnition 10.8.
Translation by a ∈R:
Ta : L2 
R

→L2 
R

,
Tag(t) = g

t −a

.
(10.49)
□
Deﬁnition 10.9.
Modulation by b ∈R:
Eb : L2 
R

→L2 
R

,
Ebg(t) = g(t)e2π jbt.
(10.50)
□
Deﬁnition 10.10.
Dilation by c ∈R −{0}:
Dc : L2 
R

→L2 
R

,
Dcg(t) =
1
√c g
# t
c
$
.
(10.51)
□
1.10.5.2 Common frame constructions
The most common approaches to construct frames from a prototype are:
•
A frame in L2(R) constructed by translations of a given function g(t) ∈L2(R), through
{Tnag(t)}n∈Z,
with a > 0,
(10.52)
is called a frame of translates [6];

1.10.5 Construction of Frames from a Prototype Signal
575
•
A Weyl-Heisenberg or Gabor frame is a frame in L2(R) constructed from a ﬁxed function g(t) ∈
L2(R), by means of
{EmbTnag(t)}m,n∈Z,
with a, b > 0.
(10.53)
SuchframeisalsocalledaGaborSystemoraWindowedFourierFrame [4–6,42]duetoitsconnection
to time-frequency analysis techniques;
•
A frame constructed by dilations and translations of a prototype (mother) function g(t) ∈L2(R) by
{Tnac j Dc j g(t)} j,n∈Z =

c
−j
2 g

c−jt −na

j,n∈Z ,
with c > 1, and a > 0.
(10.54)
is called a wavelet frame [4–7].
In the following subsections we enumerate some conditions for obtaining frames in the above ways
and some examples of their applications or relevance.
1.10.5.3 Frames of translates
Digital signal processing is heavily based on signal sampling, that is, the capability of sampling a
function (whose independent variable may not be time) using regular intervals and reconstructing
the signal from the samples [4,6,31,43]. A sample corresponds to a value that in turn represents an
evaluation/measurement of the function/signal. The operation of sampling can be modelled as the
projection of the function/signal over a pre-deﬁned element. Regular sampling corresponds to translating
the element into different positions regularly sampled and taking a sample (measurement/projection) at
the correspondent position. Although the following results employ time as the independent variable it
could be any other.
Deﬁnition 10.11.
Aframeoftranslatesisaframefor L2(R)obtainedthroughoperations{Tnag(t)}n∈Z,
on a ﬁxed function g(t) with a > 0.
□
Which conditions should hold on g(t) in order to {Tnag(t)}n∈Z being a frame? In this case it can be
shown that [6] {Tnag(t)}n∈Z is a frame with bounds A and B if and only if
aA ≤G(ω) ≤aB,
ω ∈[0, 1] −N,
N = {γ ∈[0, 1] : G(ω) = 0}.
(10.55)
Frames of translates are intimately related with sampling. The example below highlights their con-
nection with Shannon Sampling Theorem [43].
Example 10.9.
In this example one employs the Shannon Sampling Theorem [43,44]. Let a band
limited function of time s(t) be sampled using a train of impulses at a rate 1/T . Assume that the signal
bandwidth is W. If T <
1
2W , one can reconstruct the signal using
s(t) =

n
s(nT )sin
%
t −nT ) π
T
&
(t −nT ) π
T
.
(10.56)
This is so because the inﬁnite set of sinc pulses centered at nT provides a basis/frame of the W Hz
band-limited space.

576
CHAPTER 10 Frames in Signal Processing
Deﬁne R as the “amount of over-sampling”
R =
1
2WT ,
forR ≥1,
(10.57)
One notes that the Nyquist rate [31] implies 2WT = 1 and R ≥1 makes 1/T ≥2W. One has that [44]
s(t) = 1
R

n
s(nT )sin
%
(t −nT ) π
RT
&
(t −nT ) π
RT
.
(10.58)
Note that for R ≥1, two sinc pulses centered at kT and lT (k,l ∈Z) not just overlap, but are
actually not orthogonal. Indeed, the set of T temporally-spaced sinc pulses is an overcomplete basis of
the W-bandlimited signal space, in the case R ≥1. That is, the sinc pulses centered at nT with R > 1
are not linearly independent. However, the inﬁnite set of sinc pulses above is shown to be a tight frame
and R is interpreted as a redundancy factor for the frame [10,44]. One notes that for R = 1 (10.58)
equates (10.56) where the signal samples occur where the sinc pulses equate one (for t = nT ) or zero
(other cases). However, as R increases the sinc pulse centered at nT gets wider and is not zero anymore
for t −nT = kT , k ∈Z∗.
□
1.10.5.4 Gabor frames
While the introduction of the frame concept remounts to 1952 [1], according to Christensen [6], the ﬁrst
mention of what are now called Gabor of Weyl-Heisenberg frames can be traced back even before that
to the work of Gabor on communications [23] in 1946 and to the book of von Neumann on quantum
mechanics originally published in 1932 [45].
Let us simplify the idea of Gabor and simply state that its vision was to represent a signal s(t) by
means of
s(t) =

m

n
˜cm,n EmbTnag(t).
(10.59)
The function g(t) is chosen to be a Gaussian due to its time-frequency concentration [19,21,46]. This
representation can provide an analysis of the time frequency content around the points (na, mb) in the
time-frequency plane. The resemblance between this idea and the frame deﬁnition sparks, and then a
straightforward deﬁnition emerges.
Deﬁnition 10.12.
AGaborframeisaframefor L2(R)obtainedthroughoperations{EmbTnag(t)}m,n∈Z,
on a ﬁxed function g(t) with a, b > 0.
□
Now, one has the problem of guaranteeing that the set {EmbTnag(t)}n,m∈Z is capable of representing
any signal s(t) ∈L2(R), i.e., is the set {EmbTnag(t)}n,m∈Z2a frame of L2(R)? In addition, how does
one compute the set

cm,n

n,m∈Z2 that allows the synthesis of a signal using Eq. (10.59)? The answer
to the later question lies in the concept of inverse frame. But, we will start from the ﬁrst question: for
a given g(t), when {EmbTnag(t)}m,n∈Z is a frame? The answer depends on a complicated interplay
between g(t), a and b. For example in [47] a set of non-intuitive conditions was shown to hold on a
and b to generate a Gabor System based on the characteristic function. In what follows, several results
collected from the frame literature are presented, which provide either sufﬁcient or necessary conditions
for {EmbTnag(t)}m,n∈Z to constitute a frame.

1.10.5 Construction of Frames from a Prototype Signal
577
b
a
Frequency
Time
FIGURE 10.2
Gabor system elements location in the time-frequency plane.
For {EmbTnag(t)}m,n∈Z to compose a frame it is necessary that ab ≤1 [4–6]. That is, if ab > 1
a frame will not be obtained; however, the assumption ab ≤1 does not guarantee the generation of a
frame for any g(t), see for example [47]. It should be observed that ab is a measure of the density of the
frame in the time-frequency plane [4,5,19–21,42]; the smaller ab is the denser is the frame. Figure 10.2
illustrates this concept.
If {EmbTnag(t)}m,n∈Z constitutes a frame then the frame bounds necessarily satisfy [6]
∀t ∈R,
A ≤1
b

n
|g(t −na)|2 ≤B
(10.60)
∀ω ∈R,
A ≤1
b

k
 ˆg
#
ω −k b
2π
$
2
≤B.
(10.61)
A well known sufﬁcient condition for {EmbTnag(t)}m,n∈Z to be a frame is presented in [14]. Let a, b >
0, g(t) ∈L2(R), denote g∗(t) the complex conjugate of g(t) and suppose that ∃A, B > 0 such that
A ≤

n∈Z
|g(t −na)|2 ≤B∀t ∈R and
(10.62)

k̸=0



n∈Z
Tnag(t)Tna+ k
b g∗(t)


∞
< A,
(10.63)
then {EmbTnag(t)}m,n∈Z is a Gabor frame for L2(R).

578
CHAPTER 10 Frames in Signal Processing
A more general sufﬁcient condition for the generation of a frame {EmbTnag(t)}m,n∈Z for a, b > 0
given and g(t) ∈L2(R) is [6,48]: if
B :=1
b sup
t∈[0,a]

k∈Z


n∈Z
g

t −na

g∗
#
t −na −k
b
$ < ∞, and
(10.64)
A :=1
b
inf
t∈[0,a]
⎡
⎣
n∈Z
g

t −na
2 −

k̸=0


n∈Z
g

t −na

g∗
#
t −na −k
b
$
⎤
⎦> 0
(10.65)
then {EmbTnag(t)}m,n∈Z is a frame for L2(R) with frame bounds A, B. Note that this result shows
that if g(t) has a limited support [0, 1/x] then for any set ab ≤1 and b < x a Gabor frame is
obtained.
Other conditions for the generation of Gabor frames exist (see for example [4–6,27]). In [49] an
extension of the results in Eqs. (10.64) and (10.65) for irregularly sampled time-frequency parameters
(when the set (an, bm) is replaced by any pair

an,m, bn,m

∈[na, (n + 1)a] × [mb, (m + 1b]) is
provided.
In subSection 1.10.6.2, we discuss the use of such frames for time-frequency analysis by means of
the Gaborgram and in Section 1.10.6.3 we present an example of “how to ﬁnd” the inverse frame to a
Gabor frame. In Section 1.10.6.4 a condition for constructing Gabor frames in discrete spaces from its
continuous counterpart is stated.
1.10.5.5 Wavelet frames
Wavelet analysis asks if translated and scaled versions of function ψ(t) can be employed for representing
a signal s(t) ∈L(R). In this sense, the signal is to be represented using functions like
ψc,a(t) =
1
√cψ
#t −a
c
$
.
(10.66)
If one thinks about the signal projections over different ψc,a(t) to represent the signal, the similarity
with the Gabor “time-frequency” approach is unavoidable. One applies operations to a ﬁxed prototype
function and use the resulting modiﬁed versions of the prototype to express the signal. Discrete pairs
(a, c) deﬁne how the prototype function has been modiﬁed and thus provide what signal characteristics
are being analyzed.
If one employs a continuum of (a, c) pairs, one has the so-called Continuous Wavelet Transform
[4,5,7,50]
Wψ(a, c) = ⟨f (t), ψc,a(t)⟩=
'
f (t) 1
√cψ
#t −a
c
$
dt.
(10.67)
One can compute the inverse wavelet transform using
f (t) = Kψ
' ∞
0
'
Wψ(a, c) 1
√cψ
#t −a
c
$
da dc
c2
(10.68)

1.10.5 Construction of Frames from a Prototype Signal
579
In the equations above, we have considered that ψ(t) is real and Kψ is a constant that depends on ψ(t)
[4,51,52]. It is worthy to say that, similarly, the Gabor frame can be linked to the so-called Short Time
Fourier Transform [4,21].
In the case of wavelet frames, we are concerned with the case of a discrete set of pairs (a, c). In this
sense in 1910 [53], Haar has shown that for the function
ψ(t) =
⎧
⎨
⎩
1, 0 ≤t < 1/2,
1, 1/2 ≤t < 1,
0, otherwise
(10.69)
the set of dilated and translated versions
ψ j,k(t) = 2 j/2ψ

2 jt −k

,
j, k ∈Z,
what means that ψ j,k(t) = Tk D2−j { f (t)}
(10.70)
is a basis for L2(R).
A more throughout study of such possibilities occurred in the beginning of the 1980s. Calderon in
1964 [51] and Grossman and Morlet in 1985 [52] introduced the Wavelet Transform. In continuation,
the ﬁrst wavelet frame construction appeared in 1986 in [12]. Great effort was dedicated to the study of
wavelets due to their multi-resolution analysis [4,5,7,13,50]. A recent book [54] collects some relevant
papers for wavelet analysis theory.
One should note that in Eqs. (10.66)–(10.70), the order in which the translation and dilation operation
are applied differs from the one previously presented in Section 1.10.5.2. Therefore, for clarity, we now
deﬁne what is meant by a Wavelet Frame.
Deﬁnition 10.13.
For c > 1, a > 0 and g ∈L2(R),aframeconstructedbydilationsandtranslationsas

Tnac j Dc j g(t)

j,n∈Z =

c
j
2 g

c jt −na

j,n∈Z
(10.71)
is called a wavelet frame.
□
In [5] both necessary and sufﬁcient conditions are provided to construct wavelet frames. For exam-
ple, if

Tnac j Dc j g(t)

j,n∈Z =

c
j
2 g

c jt −na

j,n∈Z is a frame with frame bounds A and B then
necessarily [55]
A ≤1
a

j∈Z
 ˆg

c jω

2
≤B,
(10.72)
where ˆg

ω

is the Fourier transform of g(t).
A sufﬁcient condition to generate a wavelet frame [6] is: suppose that c > 1, a > 0 and g(t) ∈L2(R)
are given, if
B := 1
b
sup
|ω|∈[1,c]

j,n∈Z
 ˆg

c jω

ˆg

c jω + n
a
 < ∞,
and
(10.73)
A := 1
b
inf
|ω|∈[1,c]
⎡
⎣
n∈Z
 ˆg

c jω

2
−

n̸=0

j∈Z
 ˆg

c jω

ˆg

c jω + n
a

⎤
⎦> 0
(10.74)

580
CHAPTER 10 Frames in Signal Processing
Localized Sinusoid
Localized Phenomenon
Localized Damped Sinusoid
FIGURE 10.3
Examples of ﬁxed functions that can be used to construct frames.
then {Tnac j Dc j g(t)} j,n∈Z is a frame for L2(R) with bounds A, B given by the expressions above.
In [49] an extension of this condition for irregularly sampled time-scale parameters, when the set

an, c j
is replaced by any pair

an, j, cn, j

∈[c jan, c ja

n + 1

] × [c j, c j+1], is provided.
1.10.5.6 Finite vector spaces
When considering ﬁnite vector spaces HN the simpler solution is to truncate or box-window the elements
of the discrete space frame; however, this simple approach alters the frame bounds [56]. An alternative
is to consider that the vector space is generated from an N-length section of an N-periodic l2(Z) space,
where the translation operator is a circular shift. The circular shift of a signal does not change the vector
norm; this way the frame in HN has the same frame bounds as the frame in the N-periodic l2(Z).
1.10.6 Some remarks and highlights on applications
1.10.6.1 Signal analysis
We have discussed the construction of different frames from a ﬁxed prototype function. We have
also presented examples that span the possibility of signal representation, analysis and synthesis using
functions with prescribed characteristics. These are some of the main features of frames that are relevant
for signal processing applications: detection of signal features, analyzing and synthesizing signals using
a set of pre-deﬁned signals. Frames bring freedom when compared to basis as the number of elements
to be employed in the analysis-synthesis process can be larger than the signal space dimension.
In the literature, there is a large number of examples of different frame constructions using a ﬁxed
prototype signal; we have presented some in previous sections. Figure 10.3 illustrates different functions
with different characteristics that can be used in this framework. These ﬁgures illustrate possible ﬁxed
functions to be employed to detect localized phenomena in a signal.

1.10.6 Some Remarks and Highlights on Applications
581
Gabor Frame Elements Heisenber Boxes
Time
Wavelet Frame Elements Heisenber Boxes
Time
Frequency
Frequency
FIGURE 10.4
Heisenberg boxes in the time-frequency plane for Gabor (left) and wavelet (right) frames.
The Gabor and wavelet approaches commonly used for building frames from a ﬁxed function were
discussed. In some cases, the density in time, frequency or scale of the phenomena to be analyzed or
detected in the signal [24,25,57] may be such that techniques for reducing the number of frame elements
may be handy [58].
Example 10.10.
As the reader may have already noticed, Wavelet frames can be used in a similar way
to Gabor frames for time-frequency analysis. However, in this case a better phrasing would be time-
scale analysis. Figure 10.4 depicts the different tilings of the time-frequency plane obtained by these
approaches. These tilings depict the Heisenberg boxes [4] of the frame elements—these boxes roughly
correspond to the time-frequency plane area where most of the elements’ energy is concentrated.
□
1.10.6.2 Gaborgram
In the beginning of Section 1.10.5.4 we have presented the idea of Gabor. That was representing a signal
s(t) by means of
s(t) =

m

n
cm,n EmbTnag(t) =

m

n
cm,ne jmbg(t −na).
(10.75)
As it can be noticed, this equation assumes that there is a representation for any s(t) using several
gmb,na(t) = EmbTnag(t). We have seen that for these to be true it is enough for the set {EmbTnag(t)}m,n
to be a frame. What is commonly referred to as the Gaborgram is the plot of the coefﬁcients cm,n using the
lattice presented in Figure 10.2. As discussed above a similar concept can be used for Wavelet Frames.

582
CHAPTER 10 Frames in Signal Processing
We have also seen that to compute the

cm,n

m,n, the inverse frame to {EmbTnag(t)}m,n must be used.
The next section illustrates how to obtain the inverse frame in the case of an exponential function. A
Gabor frame of damped sinusoids is inspired on the connection among transients and damped sinusoids
in physical systems.
1.10.6.3 Inverse gabor frame
The power of Gabor system for signal analysis and synthesis is not difﬁcult to perceive. The problem
is how to ﬁnd the coefﬁcients cm,n that provide the reconstruction
s(t) =

m

n
cm,n EmbTnag(t) =

m

n
cm,ne jmbg(t −na).
(10.76)
The inverse frame elements { ˜g(t)}m,n can be used. In this case one has that
s(t) =

m

n
⟨c, ˜gm,n(t)⟩EmbTnag(t).
(10.77)
Daubechies [13] has shown that, for Gabor frames, the inverse frame must be such that
˜gm,n(t) = EmbTna ˜g(t).
(10.78)
And one has that
s(t) =

m

n
⟨c, EmbTna ˜g(t)⟩EmbTnag(t).
(10.79)
However, one still has to ﬁnd ˜g(t). Equation (10.79) provides a biorthogonality condition (which is
the “frame decomposition” formulas discussed in Section 1.10.3.1). In that sense, in [59] it is shown
that Eq. (10.79) leads to
1
ab
'
g(t) ˜g

t −n
b

e−j2πm t
a dt = δ(n)δ(m),
(10.80)
where δ(x) denotes the impulse of x. In [59] it is also discussed that Eq. (10.80) accepts more than one
solution in the oversampled case (ab < 1) [27].
Example 10.11.
In [24] the problem of building a Gabor system/frame based on the one sided
exponential is presented. In this case, one has that
g(t) =
+√
2αe−αt, t ≥0
0,
otherwise
(10.81)
Using the Zak Transform [47,60] in [24] it is derived that for large oversampling (ab < 1)
˜g(t) ≈
⎧
⎪⎨
⎪⎩
−ke
−α

t+ 2
b

, −1 ≤tb < 0
ke−αt,
0 ≤tb < 1
0,
otherwise
(10.82)
Where k is a constant that depends on a, b and α [24].
Figure 10.5 illustrates the ˜g(t) for different cases of a and b. In order to just focus on the waveform of
˜g(t), we ignore here the value of the constant k and make all functions scaled so that maxt ˜g(t) = 1.□

1.10.6 Some Remarks and Highlights on Applications
583
1.10.6.4 Gabor frames in discrete spaces
Obviously, due to the time-shift frequency-shift structure of Gabor frames, these have a broad range
of applications for signal processing. Since most algorithms take place in a discrete space we should
evaluate how to construct Gabor frames in this space.
Frames in discrete spaces (l2(Z)) can be obtained by time-sampling the elements of frames in L2(R)
[6]. If g(t) ∈L2(R) is such that
lim
ϵ→0

k∈Z
1
ϵ
'
1
2 ϵ
−1
2 ϵ
|g(k + t) −g(k)|2dt = 0,
(10.83)
and if {Em/QTn/Pg(t)}m,n∈Z with Q, P ∈N is a frame for L2(R) with frame bounds A and B, then
{Em/QTn/PgD}n∈Z,m=0,1,...,M−1 is a frame for l2(Z) with frame bounds A and B [6]. The vector gD is
the discretized version of g(t) with one sample per time unit, i.e., gD = {g( j)} j∈Z.
1.10.6.5 Fast analysis and synthesis operators for gabor frames
Gabor or Weyl-Heisenberg frames in N-dimensional spaces are also interesting because one can ﬁnd
fast algorithms to compute their analysis and synthesis operators. Now, we consider that the number
of “points” of the Weyl-Heisenberg frame in the frequency axis is such that Q = N/r, Q,r ∈N and
0
1
2
Time
b=1, α =1
−2
−1
−2
−1
−2
−1
0
1
2
0
0.5
1
Time
b=1, α =4
0
1
2
Time
b=1, α =1/2
−0.5
0
0.5
−1
0
1
0
0.5
1
−1
0
1
Time
b=4, α =1/4
FIGURE 10.5
Inverse Gabor frame prototype function of the one sided exponential—Biorthogonal function ˜g(t) to g(t) =
√
2αe−αtu(t), for different values of α.

584
CHAPTER 10 Frames in Signal Processing
also that the number of points in the time axis P ∈N (the same restrictions were employed in Section
1.10.6.4 above. These restrictions actually lead to a = 1/Q and b = N/P in Eq. (10.59). In this case,
the cn,m are deﬁned for n ∈{0 . . . P −1} and m ∈{0 . . . Q −1} and are given by
cn,m = ⟨x, Em 1
Q Tn N
P g⟩=
N−1

l=0
x[l]e
−j2πlm
Q
Tn N
P g[l].
(10.84)
Deﬁning
fn N
P [k] = x[k]Tn N
P g[k] = x[k]g
	#
k −n N
P
$
modN

(10.85)
fn N
P =

fn N
P [0], . . . , fn N
P [N −1]

(10.86)
we obtain
cn,m = ⟨x, Em 1
Q Tn N
P g⟩=
N−1

l=0
fn N
P [l]e
−j2πlm
Q
.
(10.87)
For Q = N/r,r ∈N we obtain
cn,m = ⟨x, Em 1
Q Tn N
P g⟩=
N−1

l=0
fn N
P [l]e
−j2πl
N
mr = DFT {fn N
P }[mr],
(10.88)
where DFT {x}[k] is the kth sample of the Discrete Fourier Transform of x, which can be computed
using fast algorithms (FFT) [31].
Using the result in Eq. (10.88) the analysis operator (which was discussed in Section 1.10.4.2) of
such Weyl-Heisenberg frames is given by
T∗{·} : HN →CPQ,
(10.89)
T∗{x} : cn,m = {⟨x, Em 1
Q Tn N
P g⟩}, n ∈[0, P −1], m ∈[0, Q −1]
(10.90)
cn,m = F FT {fn N
P }[mr], fn N
P [k] = x[k]g
	#
k −n N
P
$
modN

(10.91)
Similarly, for the synthesis operator of such frames we have that
T{·} : CPQ →HN,
T{cn,m} = [t[0], . . . , t[N −1]] =
P−1

n=0
Q−1

m=0
cn,m Em 1
Q Tn N
P g.
(10.92)
The last equation is the sum of PQ N-length vectors and can be computed using the Inverse Discrete
Fourier Transform as below
t[l] =
P−1

n=0
Q−1

m=0
cn,me j 2πml
Q fn N
P [l] =
P−1

n=0
fn N
P [l]
Q−1

m=0
cn,me j 2π
N lmr.
(10.93)

1.10.6 Some Remarks and Highlights on Applications
585
Denoting cn(m) = cn,m we can deﬁne
cn =
%
cn[0], cn[1], . . . , cn[Q −1]
&
,
(10.94)
and its up-sampled version
U(cn)[k] =
+cn[k/r], k/r ∈{0, 1, . . . , Q −1}
0,
otherwise
(10.95)
Thus, the synthesis operator is such that
t[l] =
P−1

n=0
fn N
P [l]
Q−1

m=0
U(cn)[mr]e j 2π
N lmr
=
P−1

n=0
fn N
P [l]I F FT{U(cn)}[l].
(10.96)
Figure 10.6 presents a way to compute the expansion coefﬁcients or the reconstructed signal when a
frame is used either as an analysis or as a synthesis operator. Note that in the outputs of each FFT block, in
Figure 10.6, there is a serial to parallel converter, while at the IFFT inputs a parallel to serial converter
exists. Figure 10.6 shows that all the frame coefﬁcients can be obtained using PN(1 + log2 (N))
operations, which can be reduced if one takes into account that just Q FFT/IFFT coefﬁcients need to be
computed at each FFT/IFFT branch.
1.10.6.6 Time-frequency content analysis using frame expansions
As we have seen in several cases, frames provide a powerful tool for signal analysis. This is specially
true when one considers Gabor and Wavelet frames. These provide a natural tiling of the time-frequency
plane. We now discuss how can a signal expansion be mapped into a time-frequency plot.
The Wigner-Ville distribution (WD) is probably one of the most well-known and widely used tools
for time-frequency analysis of signals. The WD of a signal x(t) is deﬁned as [4,18,20,61]
W Dx

t, f

=
' +∞
−∞
x

t + τ
2

x∗
t −τ
2

e−2π j f τdτ,
(10.97)
Some applications have used the WD of signal decompositions in order to analyze the time-frequency
content of signals [4,20]. The Wigner-Ville distribution of a signal x(t), W Dx(t, f ), is a “measure”
of the energy density in the signal in both time (t) and frequency (f) simultaneously. However, it is
just meaningful when regions of the time-frequency plane are considered, that is as local averages,
and it can not be considered at a given time-frequency point (t′, f ′) due to uncertainty principle [4,20].
In addition, the WD of a signal has another drawback since it is not restricted to be positive, a mandatory
characteristic for an energy density.
In Section 1.10.3, we have seen that a signal x can be decomposed into a frame {gk}k∈Kor into its
dual { ˜gk}k∈K, and reconstructed by means of
x =

k∈K
⟨x, ˜gk⟩gk =

k∈K
⟨x, gk⟩˜gk.
(10.98)

586
CHAPTER 10 Frames in Signal Processing
FIGURE 10.6
Weyl-Heisenberg fast analysis and synthesis using FFTs and IFFTs.
Once x is decomposed into the frame coefﬁcients ⟨x, ˜gk⟩, its time-frequency content can be analyzed
using
W D ˜x(t, f ) =

k∈K
⟨x(t), ˜gk(t)⟩W Dgk

t, f

.
(10.99)
Using approaches like this, the inference of signal characteristics from signal decompositions is a
common and powerful tool for signal analysis, detection and estimation. Speciﬁcally, in the case of
time-frequency analysis, such approaches avoids the cross terms that appear when the original signal is
directly analyzed [4,18–22,42,61].
However, as we have seen the coefﬁcients ⟨x(t), ˜gk(t)⟩used above may be obtained with a ˜gk(t) that
does not, neither physically, nor visually and nor in its time-frequency content, resembles the associated
gk(t). That is why in general this approach is left aside for others.
For instance, in [42] the time-frequency content of signals is estimated using the Matching Pur-
suits (MP) signal decomposition algorithm - a step based decomposition algorithm that iterates for
successively obtaining better signal approximations [62]. The approximation is accomplished by

1.10.7 Conclusion
587
selecting the dictionary element that best matches the signal. This match is evaluated by projecting
the signal on the dictionary elements. The best matching element is scaled by its correspondent projec-
tion and subtracted from the signal generating a residual error. The process is then applied to the residual
error iteratively. This algorithm is considered to be greedy [63–65], in the sense that it minimizes the
approximation error at each iteration. However, this may not be optimal as a sequence of iterations
occur, and due to this fact several variants for the MP have been presented [65–68].
Although the dictionary employed in the MP is not required to be a frame, it has been a common
procedure to employ a frame [58,69,70] as a dictionary. This is so because this choice guarantees the
analysis and synthesis of any signal. In general, a mix of Gabor and Wavelet frames is employed to
build the dictionary used together with the MP [42]. Doing this, the dictionary elements are placed in
different locations in the time-frequency plane and the several scales provide different concentrations
for the energy of the elements in time and frequency. It is common to employ a dictionary formed by
Gaussian functions in different scales with different time and frequency shifts. One then obtains the MP
expansion of signal x(t) into the Gabor dictionary
x(t) ≈
M

n=1
γngi(n)(t),
(10.100)
where the i(n) indexes the element selected for approximating the residual error in the decomposition
step n. From this representation the time-frequency content of x(t) is analyzed by means of
M

n=1
γnW Dgi(n)(t, f ).
(10.101)
In [71] the same approach is used, but using a modiﬁed version of the MP to obtain the signal expan-
sion. Note the resemblance between the approach in Eq. (10.99) and the ones in Eqs. (10.100) and
(10.101).
1.10.7 Conclusion
In this chapter we have made an overview of frames. We started by introducing the concept of overcom-
plete decompositions, highlighting its main characteristics. We then commented on the dual frames,
and how frames can be employed to perform analysis and synthesis of signals. After this, we pro-
vided formal deﬁnitions of the frame operator, inverse frames and frame bounds. We then formalized
the use of frames in discrete and ﬁnite dimensional spaces. Shifting into more practical matters, we
showed how to generate frames from a prototype function, by using translations, modulations and dila-
tions. From this, we analyzed the widely used frames of translates, Gabor frames and wavelet frames.
We then described some applications in signal analysis, including the Gaborgram, and presented a
way to compute it more efﬁciently using the Fast Fourier Transform. We ﬁnished the chapter by pre-
senting ways to compute time-frequency analysis using decompositions and the matching pursuits
algorithm.

588
CHAPTER 10 Frames in Signal Processing
Relevant Theory: Signal Processing Theory
See this Volume, Chapter 2 Continuous-Time Signals and Systems
See this Volume, Chapter 3 Discrete-Time Signals and Systems
See this Volume, Chapter 5 Sampling and Quantization
See this Volume, Chapter 7 Multirate Signal Processing for Software Radio Architectures
See this Volume, Chapter 8 Modern Transform Design for Practical Audio/Image/Video Coding
Applications
See this Volume, Chapter 9 Discrete Multi-Scale Transforms in Signal Processing
References
[1] R.J. Dufﬁn, A.C. Schaeffer, A class of nonharmonic fourier series, Trans. Am. Math. Soc. 72 (1952) 341–366.
[2] R. Young, An Introduction to Nonharmonic Fourier Series, Academic Press, New York, 1980.
[3] R. Paley, N. Wiener, The fourier transforms in the complex domain, Am. Math. Soc. Colloqium Publ. Ser. 19
(1934).
[4] S. Mallat, A Wavelet Tour of Signal Processing, ﬁrst ed., Academic Press, San Diego, California, USA, 1998.
[5] I. Daubechies, Ten Lectures on Wavelets, Society for Industrial and Applied Mathematics, Philadelphia,
Pennsylvania, USA, 1991.
[6] O. Christensen, An Introduction To Frames And Riesz Bases, Applied and Numerical Harmonic Analysis,
ﬁrst ed., Birkhuser, Boston, Basel, Berlin, 2002.
[7] C.S. Burrus, R.A. Gopinath, H. Guo, Introduction To Wavelets and Wavelets Transforms A Primer, ﬁrst ed.,
Prentice Hall, Upper Saddle River, New Jersey 07458, USA, 1998.
[8] Z. Cvetkovic, M. Vetterli, On simple oversampled a/d conversion in l2(R), IEEE Trans. Inform. Theory. 47
(1) (2001) 146–154.
[9] F. Marvasti (Ed.), Nonuniform Sampling: Theory and Practice, ﬁrst ed., Springer, 2001.
[10] M. Vetterli, P. Marziliano, T. Blu, Sampling signals with ﬁnite rate of innovation, IEEE Trans. Signal Process.
50 (6) (2002) 1417–1428.
[11] R.G. Baraniuk, Compressive sensing [lecture notes], IEEE Signal Process. Mag. 24 (4) (2007) 118–121.
[12] I. Daubechies, A. Grossman, Y. Meyer, Painless nonhorthogonal expansions, J. Math. Phys. 27 (1986) 1271–
1283.
[13] I. Daubechies, The wavelet transform, time-frequency localization and signal analysis, IEEE Trans. Inform.
Theory. 36 (5) (1990) 961–1005.
[14] C. Heil, D. Walnut, Continuous and discrete wavelet transforms, SIAM Review 31 (1989) 628–666.
[15] V.K. Goyal, J. Kovacevic, J.A. Kelner, Quantized frame expansions with erasures, Appl. Comput. Harmon.
Anal. 10 (2001) 203–233.
[16] P.G. Casazza, J. Kovacevic, Equal-norm tight frames with erasures, Adv. Comput. Math.–Especial ISSUE on
Frames (2002) 387–430.
[17] J. Kovacevic, P.L. Dragotti, V.K. Goyal, Filter bank frame expansions with erasures, IEEE Trans. Inform.
Theory. 48 (6) (2002) 1439 –1450.
[18] G. Matz, F. Hlawatsch, Wigner distributions (nearly) everywhere: Time-frequency analysis of signals, systems,
random processes, signal spaces, and frames, Elsevier Signal Process. 83 (2003) 1355–1378.
[19] L. Cohen, Time-frequency distributions—a review, Proc. of the. IEEE 77 (7) (1989) 941–981.
[20] F. Hlawatsch, G.F. Boudreaux-Bartels, Linear and quadratic time-frequency signal representations, IEEE
Signal Process. Mag. 9 (1992) 21–67.

References
589
[21] Leon Cohen, Time-Frequency Analysis, Prentice Hall Signal Processing Series. Prentice Hall, Prentice Hall
PTR, Englewood Cliffs, New Jersey 07632, 1995.
[22] Kalheiz Grchenig, Foundations of Time-Frequency Analysis, Birkhuser, New York, USA, 2001.
[23] D. Gabor, Theory of communications, IEEE J. 93 (1946) 429–457.
[24] B. Friedlander, A. Zeira, Oversampled gabor representation for transient signals, IEEE Trans. Signal Process.
43 (9) (1995) 2088–2094.
[25] B. Friedlander, B. Porat, Detection of transient signals by the gabor representation, IEEE Trans. Acoust.
Speech Signal Process. 37 (2) (1989) 169–180.
[26] M. Zibulski, Y.Y. Zeevi, Discrete multiwindow gabor-type transforms, IEEE Trans. Signal Process. 45 (1997)
1428–1442.
[27] M. Zibulski, Y.Y. Zeevi, Oversampling in the gabor scheme, IEEE Trans. Signal Process. 43 (9) (1993)
2679–2687.
[28] D.L. Donoho, M. Vetterli, R.A. DeVore, I. Daubechies, Data compression and harmonic analysis, IEEE Trans.
Inform. Theory. 44 (6) (1998) 2435–2476.
[29] E.J. Candes, D.L. Donoho, Ridgelets: a key to higher-dimensional intermittency?, Philos. Trans.: Math., Phys.
Eng. Sci. 357 (1999) 2495–2509.
[30] D.L. Donoho, A.G. Flesia, Can recent innovations in harmonic analysis ‘explain’ key ﬁndings in natural image
statistics?, Network: Computation in Neural Systems, Taylor & Francis, vol. 12, pp. 371–393, 2001.
[31] P.S.R. Diniz, E.A.B. da Silva, S. Lima Netto, Digital Signal Processing: System Analysis and Design, Cam-
bridge University Press, 2001.
[32] T. Strohmer, R.W. Heath Jr., Grassmannian frames with applications to coding and comunications, App.
Comput. Harmon. Anal. 14 (3) (2003) 257–275.
[33] Z. Cvetkovic, Martin Vetterli, Oversampled ﬁlter banks, IEEE Trans. Signal Process. 46 (5) (1998) 1245–1255.
[34] H. Bölcksei, F. Hlawatsh, H.G. Feichtinger, Frame-theoretic analysis of oversampled ﬁlter banks, IEEE Trans.
Signal Process. 46 (12) (1998) 3256–3268.
[35] V.K. Goyal, M. Vetterli, N.T. Thao, Quantized overcomplete expansions in RN : Analysis, synthesis, and
algorithms, IEEE Trans. Infor. Theory. 44 (1) (1998) 16–31.
[36] N J Munch, Noise reduction in tight weyl-heisenberg frames, IEEE Trans. Inform. Theory. 38 (2) (1992)
608–616.
[37] J.A. Tropp, I.S. Dhillon, R.W. Heath Jr., T. Strohmer, Designing structured tight frames via an alternating
projection method, IEEE Trans. Inform. Theory. 51 (2005) 188–209.
[38] Y.C. Eldar, G.D. Forney Jr., Optimal tight frames and quantum measurement, IEEE Trans. Inform. Theory.
48 (3) (2002) 599–610.
[39] Y.C. Eldar, H. Bolcskei, Geometrically uniform frames,” IEEE Trans. Inform. Theory. 49 (4) (2003) 993–1006.
[40] J.J. Benedetto, M. Fickus, Frame potentials, Adv. Comput. Math 18 (2003) 357–385.
[41] R. Vale, S. Waldron, Tight frames and their symmetries, Constr. Approx. 21 (2005) 83–112.
[42] S. Mallat, Z. Zhang, Matching pursuits with time-frequency dictionaries, IEEE Trans. Signal Process. 41 (12)
(1993) 3397–3415.
[43] A.J. Jerri, The shannon sampling theorem—its various extensions and applications: A tutorial review, Proc.
IEEE 65 (11) (1977) 1565–1596.
[44] R.J. Marks II, Introduction to Shannon Sampling and Interpolation Theory, Springer-Verlag, 1991.
[45] J. von Neumann, Mathematische Grundlagen der Quantenmechanik, Springer Berlin, 1932, English version:
Mathematical Foundations of Quantum Mechanichs, Princeton Univ. Press, 1955.
[46] S. Qian, D. Chen, Joint time-frequency analysis, IEEE Signal Process. Mag. (1999) 52–67.
[47] A.J.E.M. Janssen, Zak Transforms With Few Zeros and The Tie, in: H.G. Feichtinger, T.Strohmer (Eds.),
Advances in Gabor Analysis, Birkhuser, Boston, 2002.

590
CHAPTER 10 Frames in Signal Processing
[48] P.G. Casazza, O. Christensen, Weyl-heisenberg frames for subspaces of L2(R), Proc. Am. Math. Soc. 129
(2001) 145–154.
[49] W. Sun, X. Zhou, Irregular wavelet/gabor frames, Appl. Comput. Harmon. Anal. 13 (2002) 63–76.
[50] Mladen Victor Wickerhauser, Adapted Wavelet Analysis from Theory to Software, IEEE Press, Piscataway,
NJ, USA, 1998.
[51] A.P. Calderon, Intermediate spaces and interpolation, the complex method, Studia Math. 24 (1964) 113–190.
[52] A. Grossmann, J. Morlet, Decomposion of hardy functions into square integrable wavelets of constant shape,
SIAM J. Math. Anal. 15 (1984) 723–736.
[53] A. Haar, Zur theorie der orthogonalen funktionensysteme, Math. Ann. 69 (1910) 331–371. Translated Version:
On the theory of orthogonal function systems, by G Zimmermann.
[54] Christopher Heil, David F. Walnut, Fundamental Papers in Wavelet Theory, Princeton University Press, 2006.
[55] C. Chui, X. Shi, Inequalities of Littlewod-Paley type for frames and wavelets, Siam J. Math. Anal. 24 (1993)
263–277.
[56] T. Strohmer, Numerical analysis of the non-uniform sampling problem, Comput. Appl. Math. 122 (2000)
297–316.
[57] R. Balan, I. Daubechies, V. Vaishampayan, The analysis and design of windowed fourier frame based multiple
description source coding schemes, IEEE Trans. Inform. Theory. 46 (2000) 2491–2536.
[58] K. Engan, S.O. Aase, J.H. Husoy, Designing frames for matching pursuits algorithms, in: IEEE International
Conference on Acoustics, Speech, and Signal Processing, May 1998, pp. 1817–1820.
[59] J. Wexler, S. Raz, Discrete gabor expansions, Signal Process. 21 (3) (1990) 207–221.
[60] J. Zak, Finite translations in solid-state physics, Phys. Rev, Lett. 19 (1967) 1385.
[61] T. Claasen W. Mecklenbrauker, The aliasing problem in discrete-time wigner distributions, IEEE Trans.
Acoust. Speech Signal Process. 31 (1983) 1067–1072.
[62] Lisandro Lovisolo, Eduardo A.B. da Silva, Paulo S.R. Diniz, On the statistics of matching pursuit angles,
Signal Process. 90 (2010).
[63] R.A. DeVore, V.N. Temlyakov, Some remarks in greedy algorithms, Adv. Comput. Math. 5 (1996) 173–187.
[64] G. Davis, S. Mallat, M. Avellaneda, Adaptive greedy approximations, J. Constr. Approx. 13 (1997) 57–98.
[65] J.A. Tropp, Greed is good: algorithmic results for sparse approximation, IEEE Trans. Inform. Theory. 50
(2004) 2231–2241.
[66] Y.C. Pati, R. Rezaiifar, P.S. Krishnaprasad, Orthogonal matching pursuit: recursive function approximation
with applications to wavelet decomposition, in: IEEE Twenty-Seventh Asilomar Conference on Signals,
Systems and Computers, 1993.
[67] J.A. Tropp, A.C. Gilbert, Signal recovery from random measurements via orthogonal matching pursuit, IEEE
Trans. Inform. Theory. 53 (2007) 4655–4666.
[68] L.
Rebollo-Neira,
D.
Lowe,
Optimized
orthogonal
matching
pursuit
approach,
IEEE
Signal
Process. Lett. 9 (2002) 137–140.
[69] L. Lovisolo, M.A.M. Rodrigues, E.A.B. da Silva, P.S.R. Diniz, Efﬁcient coherent decompositions of power
systems signals using damped sinusoids, IEEE Trans. Signal Process. 53 (2005) 3831–3846.
[70] P.J. Durka, D. Ircha, K.J. Blinowska, Stochastic time-frequency dictionaries for matching pursuit, IEEE Trans.
Signal Process. 49 (2001) 507–510.
[71] A.Papandreou-Suppappola,S.B.Suppappola,Analysisandclassiﬁcationoftime-varyingsignalswithmultiple
time-frequency structures, IEEE Signal Process. Lett. 9 (2002) 92–95.

11
CHAPTER
Parametric Estimation
Suleyman Serdar Kozat∗and Andrew C. Singer†
*Department of Electrical and Electronics Engineering, Koc University Istanbul, Turkey
†110 CSL, 1308 West Main Street, Urbana, IL, USA
1.11.1 Introduction
This chapter considers the topic of parametric estimation, which is an important engineering concept
that is often used for modeling signals and systems. When a complex signal or system is encountered,
it is often desirable to construct a model for the signal or system in question such that it can be readily
analyzed. For an input-output system, this may be to understand its behavior, its potential weaknesses, or
its stability to bounded inputs. For signals of interest, this may be to understand the content of the signal,
such as to determine the words that were spoken in an acoustic signal containing a recording of human
speech. In many such cases, models for the signals or systems of interest are sought that can be practically
analyzed without excess computation. As in Occam’s razor, a parsimonious model is better than a
needlesslycomplicatedone.Agoodmodelwouldbeonethatissufﬁcientlyrichtoenableinsightsintothe
salient characteristics of the signals or systems under study, but not any more so than necessary. By using
parametric models, whose behavior is uniquely determined by a ﬁnite set of parameters, the complexity
of the model can often be controlled by limiting the number of parameters. In this chapter, we focus on the
estimation of such model parameters under a variety of scenarios. Using both statistical and deterministic
formulations, rational models (linear models with a ﬁnite number of poles and zeros) are considered
in both the batch and online recursive formulations. Such autoregressive and moving average models
form the basis for our initial investigation, after which we explore advanced topics, such as spectrum
estimation, prediction, and ﬁltering, as well as a number of nonlinear parametric modeling techniques.
The background assumed in this chapter is a working knowledge of discrete-time signals and systems,
some basic probability theory including knowledge of stationary random processes, and the mathemat-
ical skills of a senior level undergraduate student in electrical engineering or a similar discipline. An
exemplary treatment of these topics includes the texts by Oppenheim et al. [1], Rabiner and Gold [2],
and Rabiner and Schafer [3]. These texts cover discrete-time signal and system theory, with the latter
covering some random processes. Additional material on probability and stochastic processes as applied
to the problems in this chapter can be found in [4,5].
1.11.2 Deterministic and stochastic signals
Inthischapter,wewillconsiderdiscrete-timesignalsthroughanumberofdifferentlenseswiththegoalof
developing parametric models for signals and systems that are useful in a variety of engineering contexts.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00011-5
© 2014 Elsevier Ltd. All rights reserved.
591

592
CHAPTER 11 Parametric Estimation
In order to proceed, we will often make use of two distinct models for signals in our discussions. The
ﬁrst, and perhaps the simplest formulation, is to assume that the signals of interest are deterministic,
that is, that the signal values x[n] can take on any ﬁnite real value and there are no probabilities or other
constraints to these values. This might be a useful way for us to capture the inability of our modeling
methods to provide any other meaningful measure to the data we observe than the simple statement that
the data can take on any values.
In this light, we will often use measures of performance such as the accumulated square error of an
estimate ˆx[n] for a signal x[n] over a block of samples, n = 0, . . . , N,
ELS =
N

n=0
(x[n] −ˆx[n])2,
(11.1)
or we may formulate this as an integral square error measured in the frequency domain,
EI S = 1
2π
 π
−π
|Xd(ω) −ˆX(ω)|2dω,
(11.2)
as a means of capturing the degree to which the approximating time domain signal ˆx[n] or frequency
domain estimate X(ω) match or model the deterministic signal of interest x[n] or Xd(ω).
As a simple example, suppose that we have measured a signal x[n] that corresponds to the daily
temperature of the water in a local pond. If we wanted to compute a good approximating signal, ˆx[n] = c,
i.e., a signal whose value remains constant over the entire interval, and we use the accumulated square
error over the collected data signal as a measure of performance, we could write
ELS =
N

n=0
(x[n] −c)2
(11.3)
and seek to ﬁnd the value of that minimizes the total accumulated square error over the observation
interval. Differentiating ELS with respect to c, and setting the result equal to zero, we obtain,
∂ELS
∂c
= −2
N

n=0
(x[n] −c),
(11.4)
0 = −2
N

n=0
(x[n] −c),
(11.5)
which can be solved for the minimizing value of c, yielding
c =
1
N + 1
N

n=0
x[n].
(11.6)
This is the well-known result that the sample-average of a sequence is the best approximating value
for the sequence, if we restrict the set of possible approximating signals to those that take on only a
single constant value.
Figure 11.1 shows the result of approximating a signal x[n] with another signal that takes on only
a single constant value. The signal in this example has an average value of c over the interval shown.

1.11.2 Deterministic and Stochastic Signals
593
FIGURE 11.1
Approximating a signal with a constant value.
As a result, the best approximating constant valued signal, in terms of minimizing the squared approx-
imation error over the interval shown, is the mean of the signal, c.
Another approach that we will often pursue is the use of stochastic (random) signal models in
our development. Random signal models enable the use of probabilistic methods to model some of
the uncertainty, or relative uncertainty, in the evolution of a time-series or a given signal or system
model. As a result, we may choose measures of performance that are similar in spirit to those of our
deterministic signal analyses, but for which we make explicit use of a probabilistic model for the
signals of interest. In statistical signal processing, square-error methods are often used, again for their
mathematical tractability as well as our ability to capture much of the behavior of the signals of interest
with a relatively small number of statistical quantities. Speciﬁcally, we will often use a model for random
signals that makes use of the so-called second-order statistics of the signal of interest [6]. Speciﬁcally,
for a discrete-time random signal, x[n], to have a complete statistical characterization of the signal
would require knowledge of the probability density function for all possible probabilistic quantities
such as fx(x[n1], x[n2], x[n3], . . . , x[nk]) for all possible n1, n2, n3, . . . , nk, and for all possible k.
This complete statistical knowledge may be available for certain signal or system models, however, in
general such a complete characterization is either not available or not appropriate.
As a result, we will often use second-order statistical models for random signals of interest by
assuming that we either have knowledge of, or can make estimates of, the following two quantities.
Speciﬁcally, the mean-value of the signal is deﬁned as
mx[n] = E{x[n]},
(11.7)
where the E{ } denotes the statistical expectation operation which is taken with respect to the complete
stochastic characterization of the random signal x[n]. The covariance function of the signal is deﬁned as
Cx[m, n] = E{(x[m] −mx[m])(x[n] −mx[n])∗},
Cx[m, n] = E{(x[m]x[n] ∗)} −mx[m]mx[n]∗,
(11.8)

594
CHAPTER 11 Parametric Estimation
where, again, the expectation is taken with respect to the complete stochastic characterization of the
randomsignal,andthesecondterminthedeﬁnitionisconjugated,iftherandomsignaliscomplexvalued.
When the random signal x[n] is wide-sense stationary (WSS), then both the mean-value of the signal
and also the covariance function take on a special form [1,6]. Speciﬁcally, for wide-sense stationary
random signals, the mean-value of the signal is not a function of time, i.e., mx[n] = mx, and the
covariance function is only a function of the difference in times between the two samples, i.e.,
Cx[m, n] = Cx[0, n −m] ≜Cx[n −m],
(11.9)
where, with a slight abuse of notation, the same name is given to both the general and the wide-sense
stationary covariance functions. In general terms, a random signal is stationary if the statistical properties
of the signal do not depend on the speciﬁc time (of day, of the year, etc.) but rather samples of the signal
have correlation (or other statistical dependence) that is merely a function of how close in time the
samples are taken with respect to one another.
Returning to our earlier example of representing a signal, now a random signal, by a single constant
estimate of that signal, we might seek to minimize the following mean square error, rather than the
accumulated square error as before,
EMS = E{(x[n] −ˆx[n])2},
EMS = E{(x[n] −c)2},
(11.10)
where, once again we seek a single value of the constant to represent the signal over the region of
interest. If the signal x[n] is stationary, then we can ﬁnd a simple solution to this estimation problem
by differentiating with respect to the parameter as follows:
∂EMS
∂c
= −E{2(x[n] −c)},
0 = −2E{x[n]} + 2c,
c = mx.
(11.11)
This provides the stochastic version of the deterministic least squares estimation problem, whereby the
minimum mean square error (MMSE) constant estimate of a wide-sense stationary random signal is
given by the mean of the signal.
1.11.3 Parametric models for signals and systems
There are a wide variety of ways in which a signal can be represented, or modeled using the tools of
signal processing and random processes. In this section, we will consider models that are parametric
in that they contain a number of free parameters that can be adjusted to optimize a given criterion
of interest, such as the least square or mean square error metrics described previously. Perhaps the
simplest such models are the so-called autoregressive (AR) and moving average (MA) models that use
as free parameters, the coefﬁcients of the numerator and denominator polynomials of a rational transfer
function that is driven by a deterministic or random process.

1.11.3 Parametric Models for Signals and Systems
595
In contrast to such parametric signal models, methods that attempt to model or approximate a given
signal without the use of such a global parametric model are often called non-parametric signal models.
A few examples of non-parametric methods for estimating a signal of interest include estimates of his-
tograms for signal values, piecewise constant or piecewise linear signal models, truncated Fourier series
models, and nearest neighbor models. We will not explore such non-parametric methods in this chapter.
1.11.3.1 Autoregressive (AR) models
A discrete-time signal x[n] is an autoregressive (AR) process of order p if it can be written in the form [6]
x[n] + a1x[n −1] + · · · + apx[n −p] = w[n],
(11.12)
where a1, . . . , ap are parameters of the AR model and the signal w[n] is a zero mean white noise
process of variance σ 2, i.e., we have that Cw[m] = σ 2δ[m]. By rearranging the order of the terms in
the deﬁnition, we can write the autoregressive process recursively, as
x[n] = −a1x[n −1] −a2x[n −2] −· · · −apx[n −p] + w[n],
(11.13)
which shows that a pth order autoregressive process can be written as a linear combination of the p most
recent values of the signal plus a term that is statistically independent of the past values of the process;
this is the so-called innovations sequence, and represents the new information, or the unpredictable
components in the random signal (see Figure 11.2).
Another way to view autoregressive signals is by recognizing that the deﬁnition of the process also
represents a convolution of the input sequence x[n] with the autoregressive parameters, such that the
output signal is the innovations sequence, i.e.,
p

k=0
akx[n −k] = w[n],
(11.14)
FIGURE 11.2
Autoregressive process ﬂow diagram.

596
CHAPTER 11 Parametric Estimation
where we set a0 = 1. The signiﬁcance of this can be seen if we slightly rearrange terms, yielding
x[n] −

−
p

k=1
akx[n −k]


	

ˆx[n]
= w[n]
(11.15)
in which we can view the summation as an estimate of the signal x[n], given by its most recent p
parameters, i.e., we have
x[n] −ˆx[n] = w[n].
(11.16)
By writing the process in this form, we see that the sequence x[n] is rather unique in that it can be best
estimated by its most recent past values and that after this predictable component is subtracted off, all
that remains is a white-noise sequence that is independent of the past values. The term “autoregressive”
comes from the ability to write the sequence x[n] using a “regression model” over its own past values,
i.e., using a ﬁnite linear combination of its own past values.
By using the linear regression, or auto-regression model for the sequence x[n], we see that there is
an input-output relationship between the sequences w[n] and x[n] that is, we can view the creation of
the sequence x[n] by processing the sequence w[n] with a linear, time-invariant ﬁlter. Ignoring that the
sequences x[n] and w[n] may be random for now, and considering only the input-output relationship
between them, we can take the z-transform of both sides (assuming for now that the sequences are
deterministic, or given), we have
x(z) +
p

k=1
akz−k X[z] = W[z],
(11.17)
x(z)

1 +
p

k=1
akz−k

= W[z],
(11.18)
x(z) =

1
1 + p
k=1 akz−k

W[z].
(11.19)
This input-output relationship implies that we can view the sequence x[n] as being generated by ﬁltering
the sequence w[n] with an “all-pole” ﬁlter, since the transfer function H(z) = X(z)/W(z), relating
X(z) and W(z) can be written as a rational transfer function where the numerator polynomial is given
by B(z) = 1, and the denominator polynomial is given by A(z) = 1 + p
k=1 akz−k (see Figure 11.3).
This input-output interpretation of an AR process provides two interesting ways to view the relation-
ship between the signals x[n] and w[n]. Speciﬁcally, we can use the relationship X(z) = H(z)W(z),
FIGURE 11.3
Input-output transfer function perspective of the autoregressive model.

1.11.3 Parametric Models for Signals and Systems
597
FIGURE 11.4
Tracking colored noise with an AR model.
to look at the signal x(n) as being generated by ﬁltering a white-noise signal w[n] with an all-pole
ﬁlter. This provides us with a simple way to generate random processes (or deterministic signals) with
a spectral shape that can be parametrically controlled through the coefﬁcients of A[z]. An alternative
viewpoint arises from writing the signal in the so-called “prediction error form” [3,6] whereby we have
x[n] −ˆx[n] = w[n]. By writing it this way, we can view the signal x[n] as being ﬁltered using the
prediction error ﬁlter with transfer function A(z), such that the output is the white-noise signal w[n]. In
this sense, the predictable component of x[n] can be completely removed by the FIR prediction error
ﬁlter with transfer function A(z), leaving only the “innovation” sequence, or unpredictable component,
x[n] −ˆx[n] = w[n].
One application of the AR model is to use the parametric model for prediction of the sequence x[n].
For example, one might attempt to predict the next value of a colored noise signal with an AR model.
Figure 11.4 shows the input w[n] and output x[n] of an FIR prediction ﬁlter that is designed to track
the input using only past values of the input. The signal used in Figure 11.1 is reused in Figure 11.4.
Since colored noise can be generated by ﬁltering a white-noise signal, we can view the signal of
interest as if it were originally generated by ﬁltering such a white-noise signal. Thus, we expect that the
error between the predicted and measured signals will be a white-noise sequence. Figure 11.5 shows
the prediction error sequence, which indeed appears uncorrelated from sample to sample, i.e., white.
1.11.3.2 Moving average (MA) models
A discrete-time signal x[n] is called a moving average (MA) process of order q if it can be written in
the form [3,6]
x[n] = b0w[n] + b1w[n −1] + · · · + bqw[n −q],
(11.20)

598
CHAPTER 11 Parametric Estimation
FIGURE 11.5
Prediction error from estimating colored noise using an AR model.
where b0, . . . , bq are parameters of the MA model and the signal w[n] is a white noise process, i.e.,
we have that Cw[m] = σ 2δ[m]. Since the sequence x[n] can be viewed as a linear combination of the
most recent values of the sequence then it is called a “moving average.” From this structure, whenever
the sequence x[n] is given as the output of an FIR ﬁlter whose input is a white-noise sequence, then we
can refer to as a moving average sequence. More generally, even when the input is not a white-noise
sequence, the process of ﬁltering with an FIR ﬁlter can be viewed as taking a “moving average” of the
input. Financial models often use 50-day and 100-day moving averages as indicators of trends in various
stock prices and indices. In such a context, q and bk are usually taken such that bk =
1
q+1, k = 0, . . . , q,
such that x[n] =
1
q+1
q
k=0 w[n −k], i.e., the arithmetic mean of the most recent q + 1 values of w[n].
Viewing the relationship between the sequences x[n] and w[n] with a signal ﬂowgraph model, in
Figure 11.6a, we see that x[n] can be viewed as an FIR ﬁltered version of the sequence w[n]. Similarly,
we can view this in Figure 11.6b. via an input-output transfer function relationship between the two
sequences.
1.11.3.3 Autoregressive moving average (ARMA) models
A discrete-time signal x[n] is an autoregressive moving average (ARMA) process of order q, p if it can
be written in the form [3,6]
x[n] + a1x[n −1] + · · · + apx[n −p] = b0w[n] + · · · + bqw[n −q],
(11.21)
where a1, . . . , ap, b0, . . . , bq are parameters of the ARMA model and the signal w[n] is a white noise
process, i.e., we have that Cw[m] = σ 2δ[m]. We can once again relate the sequences x[n] and w[n]

1.11.3 Parametric Models for Signals and Systems
599
FIGURE 11.6a
Moving average ﬂowgraph model.
FIGURE 11.6b
Transfer function perspective of the MA model relating x[n] and w[n].
through a transfer function relationship, such as
X(z) =

q
k=0bkz−k
1 + p
k=1akz−k

W(z).
(11.22)
This input-output relationship implies that we can view the sequence x[n] as being generated by ﬁltering
the sequence w[n] with a rational function, i.e., a “pole-zero” ﬁlter with ﬁnite order numerator and
denominator polynomials, where the numerator polynomial is given by B(z) = q
k=0 bkz−k, and the
denominator polynomial is given by A(z) = 1 + p
k=1 akz−k. It is usually assumed that the rational
function is “strictly proper,” which means that the numerator order is strictly less than that of the
denominator, i.e., q<p. For this reason, we may refer to the sequence as an ARMA(p) process, rather
than ARMA(q, p). Figures 11.7a and 11.7b illustrate the ﬂowgraph and transfer function perspective
of the ARMA model relating the sequences x[n] and w[n].
1.11.3.4 Parametric modeling for system function approximation
One of the primary reasons that parametric models are used is to obtain ﬁnite-order (rational system)
approximations to systems that we encounter in real-world engineering problems. Whether the data of
interest are taken from a mechanical system, an electrical device, a digital communications link, or even
a ﬁnancial or economic model, we often seek input-output relationships that can explain the behaviors
we observe such that we might be able to better understand them, or even hope to control them.
There are a number of ways in which input-output behavior can be modeled though two are perhaps
most common. One is based on observations of input-output pairs in the time domain, from which

600
CHAPTER 11 Parametric Estimation
FIGURE 11.7a
ARMA ﬂowgraph model.
FIGURE 11.7b
Transfer function perspective of the ARMA model relating x[n] and w[n].
an explanation of such behavior is sought. Another other comes from attempts to ﬁt a given system
function or frequency response that either arose from a more detailed physical model, or which was
measured using instrumentation, such as a spectrum analyzer in response to a variety of probe signals.
These two approaches can certainly be linked, through the Fourier transform, and we will move back
and forth between the two domains depending on the methods at hand. For example, given a frequency
response H(e jω) that we sought to model, we could readily transform this into the input-output pair
x[n] = δ[n], y[n] = h[n], deﬁning the impulse response of the system as the response of the system to
an impulse as input, using the deﬁnition of the discrete-time Fourier transform [1],
H(e jω) =
∞

n=−∞
h[n]e−jωn,
(11.23)
h[n] = 1
2π
 π
−π
H(e jω)e jωn dw.
(11.24)
In general, direct ARMA modeling is difﬁcult, since the formulation of the optimization of interest is
highly nonlinear in the parameters of the model. For example, given a frequency response H(e jω), the
deterministic least squares optimal ARMA(q, p) model satisﬁes the following minimization
min
{ak}p
k=1{bk}q
k=0
1
2π
 π
−π
H(e jω) −
q
k=0bke−jkω
1 + p
k=1ake−jkω

2
dω,
(11.25)
which is highly nonlinear in the parameters of the ARMA model. This can be rewritten in the time
domain, by application of Pareval’s relation [1], which equates energy calculated in the time domain

1.11.3 Parametric Models for Signals and Systems
601
with that calculated in the frequency domain, yielding,
min
{ak}p
k=1{bk}q
k=0
∞

n=−∞
h[n] −1
2π
 π
−π
q
k=0bke−jkω
1 + p
k=1ake−jkωe jωndω

2
,
(11.26)
which is equally cumbersome.
This problem formulation is difﬁcult due to the manner in which the AR parameters of interest
are nonlinearly related to the optimization criterion. If we were only concerned with a MA estimate
and wanted to use this least-squares criterion, then the time-domain formulation can provide a simple
solution, whereby
min
{bk}q
k=0
∞

n=−∞
h[n] −1
2π
 π
−π
q

k=0
bke−jkωe jωndω

2
,
(11.27)
which can be rewritten
min
{bk}q
k=0
∞

n=−∞
h[n] −
a

k=0
bkδ[n −k]

2
,
(11.28)
yielding the simple, though somewhat unsatisfying, solution,
bk = h[k],
k = 0, . . . , q.
(11.29)
This is nothing more than a simple truncation approximation to the desired impulse response for the
system of interest. If a more elaborate response is desired, for example, one that is IIR, then poles are
necessary, in addition to the zeros of the approximating transfer function, and an AR or ARMA model
should be considered. This again would lead to the nonlinearities we just encountered.
An approach that had proven useful for AR modeling is one that takes the engineering approach to
problem solving, that is, if a problem is difﬁcult to solve directly, attempt to change it into one that is
more readily solvable. In this light, we will see how the nonlinear optimization for the AR formulation
can be transformed into a linear optimization through a clever insight. First, we recall the form of the
AR model for the transfer function, namely,
H(z) =
1
1 + p
k=1akz−k .
(11.30)
Beginning with this expression and taking the inverse z-transform, we see that the impulse response of
the modeled system must satisfy
h[n] +
p

k=1
akh[n −k] = δ[n].
(11.31)
Now given a desired (or measured) impulse response, which we will denote, hd[n], we could attempt
to minimize the deviation of this expression from that which is expected from this recursion and replace
the nonlinear optimization above with the simpler form
min
{ak}p
k=1
∞

n=−∞
hd[n] +
p

k=1
akhd[n −k]

2
,
(11.32)

602
CHAPTER 11 Parametric Estimation
which is a quadratic minimization in the parameters of interest and therefore admits a closed-form
solution! Speciﬁcally, denoting the quadratic error term in the minimization above as E we can write
0 = ∂E
∂aℓ
= 2
∞

n=−∞

hd[n] +
p
k=1akhd[n −k]

hd[n −ℓ],
ℓ= 1, . . . , p,
(11.33)
which comprise a set of linear equations in aℓ, ℓ= 1, . . . , p. We can write these equations more
compactly by exchanging the order of summation, as
p

k=1
ak
∞

n=−∞
hd[n −k]hd[n −ℓ]

	

ˆRhd hd [k−ℓ]
= −
∞

n=−∞
hd[n]hd[n −ℓ]

	

ˆRhd hd [ℓ]
,
ℓ= l, . . . , p.
(11.34)
We have made use of the deﬁnition which is often called the deterministic auto-correlation sequence
for the sequence. We can then summarize the set of equations in matrix form, sometimes referred to as
the Normal Equations [3,6],
R⃗a = −⃗r,
(11.35)
or
⎡
⎢⎣
ˆRhdhd[0]
. . . ˆRhdhd[p −1]
...
...
...
ˆRhdhd[p −1] . . .
ˆRhdhd[0]
⎤
⎥⎦
⎡
⎢⎣
a1
...
ap
⎤
⎥⎦= −
⎡
⎢⎣
ˆRhdhd[1]
...
ˆRhdhd[p]
⎤
⎥⎦,
(11.36)
which has the solution,
⃗a = −R−1⃗r.
(11.37)
The matrix R has special structure that makes solving such linear equations not only straightforward,
but also fast. Namely, R is known as a symmetric Toeplitz matrix, for which fast linear equation solvers
can be readily obtained [3,6]. This special structure, together with the right hand side of the linear
equations led to a particularly fast algorithm for its solution, known as the Levinson-Durbin recursion
[3]. If we were to replace the least-squares enforcement of the prediction error criterion with a mean
square error criterion, i.e., we sought to minimize the mean square error
min
{ak}p
k=1
E
⎧
⎨
⎩
hd[n] +
p

k=1
akhd[n −k]

2⎫
⎬
⎭,
(11.38)
we would naturally arrive at a similar form for its solution, namely, we would have
R⃗a = −⃗r
(11.39)
or
⎡
⎢⎣
Rhdhd[0]
. . . Rhdhd[p −1]
...
...
...
Rhdhd[p −1] . . .
Rhdhd[0]
⎤
⎥⎦
⎡
⎢⎣
a1
...
ap
⎤
⎥⎦= −
⎡
⎢⎣
Rhdhd[1]
...
Rhdhd[p]
⎤
⎥⎦,
(11.40)

1.11.3 Parametric Models for Signals and Systems
603
which has the solution,
⃗a = −R−1⃗r,
(11.41)
where, the only difference in our solution lies in replacing the deterministic auto-correlation with the
statistical auto-correlation, Rhdhd[ℓ] = E{hd[n]hd[n −ℓ]}, where the expectation is taken over the
distribution of the random sequence hd[n]. This approach to autoregressive modeling is often called
“linear prediction” or “linear predictive modeling” due to the use of the prediction error form that is
used to make the optimization problem linear in the parameters of interest [3,6,7].
Returning to the ARMA modeling problem, there are a number of ways of incorporating zeros into
the modeling problem while still using the much simpler prediction-error formulation developed so far.
Perhaps the simplest is to just use the zeros to match the ﬁrst values of the impulse response exactly,
that is to ﬁrst solve for the denominator coefﬁcients as above (predictive AR modeling) and then select
bℓ= hd[ℓ] +
p

k=1
akhd[ℓ−k],
k = 0, . . . q,
(11.42)
which follows from the system function relation
Hd(z) =

q
k=0bkz−k
1 + p
k=1akz−k

.
(11.43)
This two-step process, namely (1) determine the AR parameters using linear predictive modeling,
followed by (2) select the MA parameters by exactly matching the ﬁrst q samples of the impulse
response is often called Prony’s method [3,7]. In general, this method provides a reasonable set of AR
parameters; however the MA parameters are not particularly well-modeled. That is, if a system were
known to have a given ARMA form, and its parameters were to be estimated from noisy observations
using Prony’s method, the AR parameters would likely well-match the true underlying system, however
the MA parameters would likely be a poor match.
An improvement over this approach is to once again use a least squares, or MMSE criterion to obtain
the MA parameters in a modiﬁed two-step approach. The ﬁrst of the two steps remains the same, that
is, use the predictive-least squares (or MMSE) approach to ﬁnd the AR parameters according to the
Normal equations as before. However, now that a set of AR parameters are given, the sequence v[n] is
deﬁned as follows:
v[n] = −
p

k=1
akv[n −k] + δ[n],
(11.44)
i.e., v[n] is deﬁned to be the impulse response of the AR model derived in step (1). Now, the goal is to
minimize the discrepancy between the desired impulse response hd[n] and the output of the cascade of
the AR model and the MA model, such that the sequence v[n] passed through the FIR ﬁlter with transfer
function B(z) is close to the desired impulse response. Speciﬁcally, we select the MA coefﬁcients to
minimize the following criterion,
min
{bk}q
k=0
∞

n=−∞
hd[n] +
q

k=0
bkv[n −k]

2
,
(11.45)

604
CHAPTER 11 Parametric Estimation
which is once again a quadratic minimization admitting a closed-form solution. The linear equations
reduce to
q

k=0
bk
∞

n=−∞
v[n −k]v[n −ℓ]

	

ˆRvv[k−ℓ]
= −
∞

n=−∞
hd[n]v[n −ℓ]

	

ˆRhd v[ℓ]
,
ℓ= 0, . . . , q,
(11.46)
which admits the solution,
⃗b = −
⎡
⎢⎣
Rvv[0] . . . Rvv[q]
...
...
...
Rvv[q] . . . Rvv[0]
⎤
⎥⎦
−1 ⎡
⎢⎣
Rhd v[0]
...
Rhd v[q]
⎤
⎥⎦,
(11.47)
⃗b = −(Rvv)−1⃗rhv,
(11.48)
where, we denote
Rvv =
⎡
⎢⎢⎣
Rvv[0] . . . Rvv[q]
...
...
...
Rvv[q]
...
Rvv[0]
⎤
⎥⎥⎦,
⃗rhv =
⎡
⎢⎣
Rhd v[0]
...
Rhd v[q]
⎤
⎥⎦.
(11.49)
1.11.3.5 Parametric modeling for joint process estimation
One of the classical signal estimation and modeling problems that makes use of parametric estimation
methods like those developed here, include the adaptive ﬁltering and joint process estimation problem
[2]. Most generally, these problems can be viewed as a parametric modeling problem depicted as follows:
where the parameters of a model for H(e jω) are adjusted to minimize the discrepancy e[n] = (d[n] −
ˆd[n]) in a suitable manner [8]. When the parameters of the model are adjusted in an online (sequential)
manner, such that the ﬁlter parameters {h[k]}p
k=0 are updated after each observation of the desired signal,
this is commonly referred to as “adaptive ﬁltering” [8]. When a batch of data {x[k]}N
k=0 is observed and
the parameters {h[k]}p
k=0 are selected based on this entire set of observations, this problem is commonly
referred to as batch or block processing. We will ﬁrst consider the stochastic case, where the discrepancy
to be minimized takes the form
{hMMSE[k]}p
k=0 = arg
min
{h[k]}p
k=0
ϵ{(d[n] −ˆd[n])
2}.
(11.50)
Writing out this minimization using the parameters of the model, leads to
{hMMSE[k]}p
k=0 = arg
min
{h[k]}p
k=0
ϵMMSE = E
⎧
⎨
⎩

d[n] −
p

k=0
h[k]x[n −k]
2⎫
⎬
⎭.
(11.51)

1.11.3 Parametric Models for Signals and Systems
605
To proceed, we consider that the sequences x[n] and d[n] are wide-sense stationary stochastic processes,
with zero mean and covariance functions [6]
Cx[n, n −m] = E{x[n]x[n −m]} = Rx[m] = Rx[−m],
(11.52)
Cd[n, n −m] = E{d[n]d[n −m]} = Rd[m] = Rd[−m],
(11.53)
Cxd[n, n −m] = E{x[n]d[n −m]} = Rxd[m] = Rdx[−m].
(11.54)
We can now solve for the minimizing set of parameters by differentiating this quadratic expression with
respect to each of the unknown parameters, which leads to
0 = ∂ϵMMSE
∂h[ℓ]
= −2E

d[n] −
p

k=0
h[k]x[n −k]

x[n −ℓ]

,
ℓ= 0, . . . , p.
(11.55)
Rearranging terms, we arrive at
0 = E{d[n]x[n −ℓ]} −
p

k=0
h[k]E{x[n −k]x[n −ℓ]},
ℓ= 0, . . . , p,
(11.56)
E{d[n]x[n −ℓ]} =
p

k=0
h[k]E{x[n −k]x[n −ℓ]},
ℓ= 0, . . . , p,
(11.57)
Rdx[ℓ] =
p

k=0
h[k]Rxx[k −ℓ],
l = 0, . . . , p,
(11.58)
which can be recognized as another form of the auto-correlation normal equations we have seen previ-
ously for AR modeling. We have
⎡
⎢⎣
Rxx[0] . . . Rxx[p]
...
...
...
Rxx[p] . . . Rxx[0]
⎤
⎥⎦
⎡
⎢⎣
h[0]
...
h[p]
⎤
⎥⎦=
⎡
⎢⎣
Rdx[0]
...
Rdx[p]
⎤
⎥⎦,
(11.59)
which has the solution,
⎡
⎣
hMMSE[0]
. . .
hMMSE[p]
⎤
⎦=
⎡
⎢⎢⎢⎣
Rxx[0]
...
Rxx[p]
...
...
...
Rxx[p]
...
Rxx[0]
⎤
⎥⎥⎥⎦
−1 ⎡
⎢⎣
Rdx[0]
...
Rdx[p]
⎤
⎥⎦,
(11.60)
⃗hMMSE = R−1 ⃗p,
(11.61)
taking a similar form to the AR modeling case. Once again, since the matrix to be inverted is an auto-
correlation matrix, and is symmetric, positive semideﬁnite, and Toeplitz, there exist fast, and numerically

606
CHAPTER 11 Parametric Estimation
stable algorithms for solving for the MMSE-optimal parameters [6,8]. This set of parameters minimizes
the mean square error, and is hence called the MMSE-optimal set of model parameters. They are also
often referred to as the Wiener solution, or the Wiener ﬁlter [8].
A deterministic formulation of the problem could be made, using the least-squares criterion, which
would again lead to a set of equations in a similar form, speciﬁcally, deﬁning the error criterion to be
{hLS[k]}p
k=0 = arg
min
{h[k]}p
k=0
ϵLS =
∞

n=−∞

d[n] −
p

k=0
h[k]x[n −k]
2
.
(11.62)
This leads to
∞

n=−∞
d[n]x[n −ℓ] =
p

k=0
h[k]
∞

n=−∞
x[n −k]x[n −ℓ],
ℓ= 0, . . . , p,
(11.63)
ˆRdx[ℓ] =
p

k=0
h[k] ˆRxx[k −ℓ],
ℓ= 0, . . . , p,
(11.64)
where ˆRdx[ℓ] and ˆRxx[ℓ] are deﬁned using summations over the observed data, rather than taking a
statistical expectation. The resulting set of normal equations are identical, with the statistical auto- and
cross-correlation functions replaced by their deterministic counterparts, arriving at the solution,
⎡
⎣
hLS[0]
. . .
hLS[p]
⎤
⎦=
⎡
⎢⎢⎣
ˆRxx[0] . . . ˆRxx[p]
...
...
...
ˆRxx[p] ...
ˆRxx[0]
⎤
⎥⎥⎦
−1 ⎡
⎢⎣
ˆRdx[0]
...
ˆRdx[p]
⎤
⎥⎦,
(11.65)
⃗hLS = ⃗R−1 ˆ⃗p.
(11.66)
1.11.3.6 Sequential parametric modeling
In certain signal processing problems, the observations may arrive sequentially and the application
may require immediate output based on only the available data. In other applications, different model
parameters may be suitable for different parts of the observations such that parameters are updated in
time for better ﬁt. A common approach to consolidate these is to produce the estimated parameters
in a sequential manner. Unlike the previous approaches where we have the batch data {x[k]}N
k=0, the
parameters {x[k]}p
k=0 are selected based on only currently available data {x[k]}n
k=−∞. As in the previous
sections, a deterministic formulation of the problem could be made, using the least-squares criterion,
based only on the available data as
{hLS[k, n]}p
k=0 = argmin
{h[k]}p
k=0
n

t=−∞

d[t] −
p

k=0
h[k]x[t −k]
2
.
(11.67)

1.11.3 Parametric Models for Signals and Systems
607
Theestimatedparametersarenowfunctionoftimeandupdatedasnewobservationsarrive. Toemphasize
the most recent data in the estimate, the least-squares criterion is often deﬁned over a window of
observations or a weighted least squares problem is formulated, such as
{hLS[k, n]}p
k=0 = argmin
{h[k]}p
k=0
n

t=−∞
λn−t

d[t] −
p

k=0
h[k]x[t −k]
2
,
(11.68)
where the emphasized window length is intrinsically set with weighting parameter 0 < λ ≤1, e.g.,
when λ = 0.9 the most recent data is emphasized in that any samples that are more than (
1
1−λ) = 10
samples old are weighted by 1/e or less, as compared with the most recent data sample. The sequential
and weighted least-squares criterion leads to a similar set of normal equations
⎡
⎣
hLS[0, n]
. . .
hLS[p −n]
⎤
⎦=
⎡
⎢⎣
ˆRxx[0, n] . . . ˆRxx[p, n]
...
...
...
ˆRxx[p, n] . . . ˆRxx[0, n]
⎤
⎥⎦
2 ⎡
⎢⎣
ˆRdx[0, n]
...
ˆRdx[p, n]
⎤
⎥⎦,
(11.69)
⃗hLS[n] = ˆR[n]−1 ˆ⃗p[n].
(11.70)
However, here, the sample auto-correlation matrix and cross-correlation vector are calculated using
only the available data, and hence, are time varying,
ˆRxx[k −ℓ, n] =
n

t=−∞
λn−tx[t −k]x[t −ℓ],
l = 0, . . . , p,
(11.71)
ˆRdx[ℓ, n] =
n

t=−∞
λn−td[t]x[t −ℓ],
l = 0, . . . , p.
(11.72)
Although successful in most signal processing applications, this online processing requires calculating
the sample correlation matrix, cross-correlation vector and performing inversion for each new sample,
which may be computationally infeasible for some applications. However, by recognizing the time
recursions in
ˆRxx[k −ℓ, n + 1] = λ ˆRxx[k −ℓ, n] + x[n + 1 −k]x[n + 1 −ℓ],
ℓ= 0, . . . , p,
(11.73)
ˆRdx[ℓ, n + 1] = λ ˆRdx[ℓ, n] + d[n + 1]x[n + 1 −ℓ],
ℓ= 0, . . . , p,
(11.74)
and using the matrix inversion lemma [8], we can solve the corresponding estimation problem in a
recursive form such that the estimated model parameters at time n + 1 can be readily obtained from the
estimated parameters at time n as
e[n] = d[n] −ˆd[n],
(11.75)
⃗g[n] = p[n]⃗x[n]{λ + ⃗x[n]T p[n]⃗x[n]}
−1,
(11.76)
p[n + 1] = λ−1 p[n] −⃗g[n]⃗x[n]T λ−1 p[n],
(11.77)
⃗h[n + 1] = ⃗h[n] + ⃗g[n]e[n]
(11.78)

608
CHAPTER 11 Parametric Estimation
yielding the “recursive least squares” (RLS) algorithm [8], where P[n] = ˆR[n]−1. While there exist
more computationally efﬁcient lattice implementations of this update without the matrix inversion
lemma, the RLS algorithm is shown to be numerically more stable [8].
We observe from this recursive update that the corresponding estimated parameter vector ⃗h[n] at
time n is basically updated by an additive vector, called the gain vector, times the estimation error to
yield the parameters at time n + 1 as
⃗h[n + 1] = ⃗h[n] + ⃗g[n]e[n],
(11.79)
where the gain vector is the inverse correlation matrix multiplied by the data vector ⃗x[n] with certain
power scaling. A common approach in signal processing to reduce the computational complexity of this
update is to replace the scaled inverse correlation matrix by the inverse of the trace of the correlation
matrix
⃗h[n + 1] = ⃗h[n] +
1
Trace( ˆR[n])
⃗x[n]e[n].
(11.80)
Subsequently, the inverse of the trace, which corresponds to the power in the stochastic process, is
further replaced by a positive constant μ > 0 as
⃗h[n + 1] = ⃗h[n] + μ⃗x[n]e[n],
(11.81)
yielding the celebrated least mean squares (LMS) adaptive algorithm [9].
When the underlying stochastic model is known, i.e., the cross and auto-correlation functions are
known, the parameters that minimize the MSE are evaluated by solving the normal equations
⃗hMMSE = R−1 ⃗p.
(11.82)
The same normal equations can be iteratively solved by applying a gradient descent algorithm [8] to
the mean square error cost function, yielding a recursive update
⃗h(k+1) = ⃗h(k) −μ▽⃗h(k) E[e2[n]],
(11.83)
⃗h(k+1) = ⃗h(k) −μ( ⃗p −R
→
h (k)),
(11.84)
whichconvergesto ⃗hMMSE ask increasesprovidedthatμisselectedappropriately[8,9].Iftheunderlying
R and p are not known, then the same iteration can be carried in time domain by replacing the expected
variables by their temporal values, in other words by replacing E[e2[n]] with e2[n], to yield
⃗h[n+1] = ⃗h[n] −μ▽⃗h[n]e2[n],
(11.85)
⃗h[n+1] = ⃗h[n] −μ(⃗x[n]e[n],
(11.86)
which yields the stochastic interpretation of the LMS algorithm. Hence, the LMS algorithm can be
considered a gradient descent algorithm, where a “stochastic” gradient is used in the update instead of
the true gradient as shown in Figure 11.8 [8].

1.11.3 Parametric Models for Signals and Systems
609
FIGURE 11.8
Graphical depiction of the convergence of the stochastic gradient (LMS) algorithm, depicted for a two tap
ﬁlter initialized at h = [0, 0].
1.11.3.7 Universal model order estimation
Even when a parametric model is known to be appropriate for a particular application, e.g., an AR,
ARMA or MA model, we still need to decide on the number of parameters in the corresponding
parametric model. Determination of the proper number of parameters to use for a parametric model is
known to be a difﬁcult problem and has a rich history in a number of applications in signal processing,
machine learning and communications [8,10]. If the complete statistical properties of the underlying
signals were known, then increasing the number of parameters would have only increased the modeling
power, i.e., more parameters are better. However, since the model parameters should be learned from
a limited amount of training data, this can cause over ﬁtting, especially in short-data regimes, and lead
to poor extrapolation outside of the training set, i.e., too many parameters can be bad. To avoid such
over ﬁtting problems, much of the early work in the model order determination literature focused on
methods that are based on assumed probabilistic models, such as the minimum description length (MDL)
or the Akaike information criterion (AIC) [6,11]. These methods establish a balance between modeling
power of the parameters using certain maximum likelihood, or information theoretic measures, while
penalizing the number of parameters using certain regularization terms. For example, for AR modeling
with white Gaussian noise, the leading term of the penalty term in the MDL for a model of order p and
a signal length n is given by p
2 log (n). Hence, for a signal d[t] according to the original deﬁnition of

610
CHAPTER 11 Parametric Estimation
MDL, the model order is selected by minimizing
min
p
⎧
⎨
⎩min
{h[k]}p
k=0
n

t=1

d[t] −
p

k=1
h[k]d[t −k]
2
+
 p
2

log (n)
⎫
⎬
⎭.
(11.87)
While these statistical approaches have been widely used in different signal processing applications,
they require considerable statistical information on the signals of interest and can be fragile to operating
in regimes in which the signals, for example, do not behave according to the assumed statistical model.
However, recent work in the machine learning and information theory communities have applied a
new approach to tackling the model order selection problem [12,13]. As an example, in the universal
model order selection approach, instead of making hard decisions at each instant based on an assumed
statistical model, one uses a performance based mixture of all models of different orders in an attempt
to perform as well as or better than the best model in the mixture. The resulting algorithms then
successfully navigate the short-data record regime by placing more emphasis on lower order models,
while achieving the ultimate precision of higher order models as the data record grows to accommodate
them. The elegance of the universal mixture methods is that this occurs naturally by monitoring the
performance of each model on the data observed so far and placing more emphasis on the results of
better performing models in an online manner.
This model combination approach is successfully applied to model order selection in the context of
RLS adaptive ﬁltering when several different linear predictors, e.g., AR models, are used for prediction,
regression or estimation. Clearly, the order of linear predictor heavily depends on the available amount
of data and assumed statistical models. Here, instead of ﬁxing a particular model order, one can use an
explicit adaptive convex combination of different order linear predictors ˆx p[n], up to some order M, as
ˆx[n] =
M

p=1
μp[n]ˆx p[n],
(11.88)
where the corresponding adaptive combination weights μp[n] are calculatedly intuitively as μp[n] ∼
exp

−n
t=1 (x(t) −ˆx p[t])

, i.e., based on the performance of th predictor on the observed data so far.
An example weighted mixture that combines m adaptive ﬁlters is shown in Figure 11.9. The output is
simply calculated as a performance-weighted mixture. For these applications, an efﬁcient lattice ﬁlter
structure can be used to not only implement each of the elements of the competing class, but to also
construct the universal predictor output all with complexity that is only linear in the largest model order
in the competing class of models [9]. Such a model combination is shown to achieve, for any n [12]
1
n
N

n=1
(x[n] −ˆx −ˆx[n])2 ≤minp<M
1
n
N

n=1
(x[n] −ˆx p[n])2 + 1
n ln (M).
(11.89)
The ability to asymptotically achieve the performance of the best model order in the set of models gives
rise to the term “universal ﬁlter” and hence such methods are often called “universal” methods.

1.11.3 Parametric Models for Signals and Systems
611
FIGURE 11.9
Adaptively combining the outputs of multiple adaptive ﬁlters.
1.11.3.8 Universal linear predictor
In parameter estimation, the universal algorithms approach can be generalized to construct estimators
that are, in a deterministic sense, min-max optimal [13]. As an example, once again let us focus on
AR parameter estimation under the deterministic total square estimation error. However, instead of
constructing sequential normal equations, as in the previous section, we hypothetically implement
all of the continuum of AR models for each ⃗h ∈R p and contruct a sequential estimate based on a
performance-weighted mixture of all these AR models as in the previous section. By taking an implicit
performance-weighted mixture over the outputs of all adaptive ﬁlters ⃗h ∈R p and using a priori Gaussian
weighting p(ˆh), the integral

R p
⃗hT ⃗x[n]p(⃗h)
n−1

t=1
e2[t]d ⃗h,
(11.90)
analogous to the weighted combination in the previous section, can be evaluated in closed form and
leads to a diagonally loaded RLS adaptive ﬁltering algorithm [12]
⃗huni[n] = ⃗R[n −1]−1 ˆ⃗p[n].
(11.91)
However, in this new recursive update, we have ˆR[n −1] = n−1
t=1 ⃗x[t]⃗tT + δI. Hence, the diagonal
loading term is the missing ingredient that takes the standard RLS adaptive ﬁltering algorithm from being
simply universal to attaining the min-max optimal performance [13]. When applied to any bounded and
arbitrary sequence, this universal recursive algorithm yields the min-max optimal performance bound

612
CHAPTER 11 Parametric Estimation
for any N as
1
n
N

n=1
(x[n] −⃗huni[n]T ⃗x[n])
2 ≤min
⃗h
1
n
N

n=1
(d[n] −⃗hT ⃗x[n])
2 + 1
n 0(p ln N).
(11.92)
This is min-max optimal, in that no other approach can achieve a lower “regret” than the 1
n 0(p ln N)
term above [14].
1.11.3.9 Universality with respect to classes that vary in space: nonlinear
classes/piecewise in space
Linear AR, MA, or ARMA models discussed in the previous sections are extensively used in signal
processing due to their tractability and adequate accuracy in modeling [9]. Recently, nonlinear models
andthosebasedonpiecewiselinearandlocallylinearapproximationshavegainedsigniﬁcantattentionas
they capture the salient characteristics of many physical phenomena. Nonlinear parametric estimation
methods, for example, Volterra ﬁlters [8] have proven attractive for a number of applications. By
removing structural constraints on linearity, nonlinear models are perhaps more appropriate for a variety
of data exhibiting saturation effects, threshold phenomena or other nonlinear behavior.
Piecewise linear modeling can be viewed as a natural extension to linear modeling, in which the
space spanned by past observations is partitioned into a union of disjoint regions over each of which
an afﬁne model is ﬁtted. In each region, a linear model can be estimated, and as the number of regions
grows, the piecewise linear model can better approximate any smoothly varying nonlinear estimator.
As an example to a piecewise linear estimator, suppose the space of past observations ⃗x[n] ∈R p is
partitioned into k disjoint regions R p = U K
k=1Vk. Based on this partitioning, the observations can be
divided into k disjoint sets, where k different normal equations are written as
⃗Rk =
N

n=1
⃗x[n]⃗x[n]T Ik[n]),
(11.93)
⃗pk =
N

n=1
d[n]⃗x[n]T Ik(⃗x[n]),
(11.94)
and Ik(⃗x[n]) is the indicator function of the kth region, i.e., Ik(⃗x[n]) = 1 if ⃗x[n] ∈Vk, for each region.
This partitioning of the observations yields a linear estimator,
⃗hk = ˆR−1
k
ˆ⃗pk,
(11.95)
k = 1, . . . , k, at each region, yielding a piecewise linear model. A similar formulation is possible when
the data arrives sequentially, however, in that case, the AR parameters are estimated sequentially using
time dependent normal equations. As an example, for sequential processing, we can run k different
LMS algorithms, one for each region, to get an adaptive piecewise linear model trained as
⃗hk[n + 1] = ⃗hk[n] + μek[n]⃗x[n]Ik(⃗x[n]),
(11.96)

1.11.3 Parametric Models for Signals and Systems
613
where ek[n] = d[n] −⃗hk[n]T ⃗x[n]. These types of piecewise linear models have been referred to in
the signal processing literature as “nonlinear autoregressive models” and in the signal processing and
statistics literature as “self-exciting threshold auto regressive models,” and have been used in modeling
a wide range of data in ﬁelds ranging from population biology to econometrics to glottal ﬂow in voiced
speech [15].
To fully exploit the potential modeling power of piecewise linear estimators, we need to be cautious
while selecting the number of regions and boundaries of these regions. Piecewise linear models can
approximate a wide class of nonlinear models, which satisfy certain regularity conditions. As the
number of regions increases, we point out that the normal equations are calculated using a ﬁnite number
of observations assigned to each region, which on a per-region basis, necessarily decreases with the
number of regions. Hence, over ﬁtting may occur if there are not enough observations assigned to a
particular region. Clearly, how the boundaries are selected also affects the effectiveness of the piecewise
linear models and how the observations are assigned to particular regions. These design considerations
are especially severe in sequential processing, where the observations can only accumulate in time and
available data is sparse in the initial phase of the adaptation.
One solution to avoid possible over ﬁtting is to include the boundaries of regions as design parameters
along with the corresponding AR parameters. Along these lines, recently, context tree based methods
have been used to compactly represent a doubly exponential number of partitions of the space of past
observations and provide an elegant way to design the partition of the past observations together with
the AR parameters.
An example context tree is given in Figure 11.10 which partitions the space of past observations,
where the depth of this context tree is 2, i.e., K = 2 [16]. For a context tree of depth K = 2, we have
four leaves, where each leaf is assigned to a region of the space of past observations. Subsequently, each
node on the context tree is assigned to a region that is the union of regions assigned to its children nodes.
Using this context tree, we can deﬁne different partitions of the space of past observations as the union
of regions assigned to nodes and the leaves. A context tree of depth can deﬁne a doubly exponential
number of different partitions, e.g., in Figure 11.10, we present ﬁve different partitions of the space
constructed by the context of depth K = 2. We represent a partition of the space of past observations as
Pk = {Vk,1, . . . , Vk,Kk}, where UKk
j=1Vk, j and each Vk, j assigned to a node on the context tree. Each of
these partitions can be used to construct a piecewise linear model since they deﬁne complete partitions
of the space of past observations. Some of the partitions are coarser and require fewer data samples
to accurately calculate their linear estimators, while, as an example, the partition corresponding to the
leaves has the ﬁnest partitioning and requires substantially more data for training. However, if there is
sufﬁcient data, then the ﬁnest partition naturally achieves the best modeling accuracy.
By using the context tree weighting method, we can construct a sequential piecewise AR model
that hypothetically constructs all the piecewise linear estimators corresponding to all of these different
partitions. This context tree based estimator achieves the performance of the best piecewise region with
the corresponding sequential estimated parameters in each node as
N

n=1
(x[n] −⃗hT
ctw[n]⃗x[n])2 ≤min
k
 N

n=1
(x[n] −⃗hT
Pk[t]⃗x[n])2 + C(Pk)

,
(11.97)

614
CHAPTER 11 Parametric Estimation
FIGURE 11.10
A context tree of depth K = 2, which has K = 4 leaves and 3 inner nodes. This tree can represent ﬁve different
partitions, P1, . . . , P5, where each partition deﬁnes a different piecewise linear estimator.
where C(Pk) is a certain constant that depends on the partition Pk, ⃗h Pk[n] are the sequentially estimated
AR parameters by the piecewise estimator assigned to the Pk partition. As an example, if the LMS
algorithms are used in each node to train the corresponding linear models in different regions, then we
have ⃗h Pk[n] = ⃗hVk, j [n] if ⃗x[n] ∈Vk, j, and
⃗hVk, j [n + 1] = ⃗hVk, j [n] + μeVk, j [n]⃗x[n]IVk, j (⃗x[n]).
(11.98)
We can use different adaptation methods, e.g., the RLS algorithm instead of the LMS algorithm, in each
node or region such that only the update equation should be changed accordingly. This implies that the
context tree algorithm can uniformly achieve the performance of the best partition that can be represented
on the context tree for any bounded deterministic signal, where the computational complexity of this
algorithm is only in the order of the depth of the context tree.

1.11.3 Parametric Models for Signals and Systems
615
1.11.3.10 Universality with respect to classes that vary in time
For non-stationary data models or time varying environments, the best choice of an adaptive estimator
as well as the structure of this best estimator may change over time. As an example, different order
AR models are needed to accurately model voiced and unvoiced time segments of a speech signal [3].
While RLS algorithm is known to converge faster than the LMS algorithm, since it minimizes a true
least square error criterion, the LMS algorithm is known to better track certain non-stationary data [8]. In
classical adaptive ﬁltering, this kind of time variation is usually overcome by incorporating windowing
or weighting by deﬁning appropriate cost functions as done in the previous sections.
Recently, the universal model combination approaches gave rise to a host of model combination
methods, where several different sequential estimation methods are combined in order to outperform
the best in the combination [12]. In this sense, instead of using windowing or weighting, the best of
choice of algorithms are intrinsically selected by the underlying data based on the performance so far.
For example, by using a combination of adaptive algorithms that are fast tracking along with others
that have superior steady state mean square error performance, one can strike a balance and achieve the
tracking performance of the fast-converging adaptive ﬁlter, while maintaining the superior steady state
performance of the slower, more precise adaptive ﬁlter counterpart.
Hence, rather than pushing the time variation into a known statistical model or adaptation, one can
try to exploit the time varying nature of the best choice of algorithm for any given realization. As an
example, given the batch data we can partition the observations in time into contiguous segments
{⃗x[1], . . . , ⃗x[n1 −1]}{⃗x[n1], . . . , ⃗x[n2 −1]} . . . {⃗x[nk], . . . , ⃗x[N]}
(11.99)
and construct independent normal equations using
Rk =
nk

n=nk−1+1
⃗x[n]⃗x[n]T ,
(11.100)
ˆ⃗pk =
nk

n=nk−1+1
x[n]⃗x[n],
(11.101)
where n0 = 0. This yields K independent AR models one for each contiguous segment as
⃗hk = ˆR−1
k
ˆ⃗pk,
(11.102)
⃗h[n] = ⃗hk,
nk−1 + 1 ≤n ≤nk.
(11.103)
Since each normal equation only depends on the observations belonging to a speciﬁc segment, this
adaptiveprocessingcanreadilytrackvariationsintime.However,thebestpartition,i.e.,howtooptimally
divide the observations in time can only be chosen after observing the whole data. Hence, we cannot
construct a sequential version of such optimization and are forced to resort to windowing or weighting
to emulate such piecewise partition in time.
However, inspired by the universal source coding methods, rather than trying to ﬁnd the best partition
(possible best switching points) or the best number of transitions, we can construct a sequential adaptive
algorithm that simply achieves the performance of the best partition directly and simultaneously for all

616
CHAPTER 11 Parametric Estimation
FIGURE 11.11
A time-varying AR(1) model used as input to the predictor that permits switching among different linear
predictors in time.
different partitions and for any possible observation sequence. This switching approach has been suc-
cessfully applied by a number of researchers in a variety of problems, including tracking the best among
a ﬁxed class of algorithms as well as tracking the best from parametrically-continuous classes of algo-
rithms [13]. By using transition diagrams, we can construct sequential algorithms that asymptotically
achieve the performance of the best switching algorithm, i.e.,
N

n=1
(x[n] −⃗hT
ctw[n]⃗x[n])2 ≤
min
n1,...,nK ,⃗h1,...,⃗hK
⎧
⎨
⎩
K

k=0
nk

n=nk+1
(x[n] −⃗hT
k ⃗x[n])2 + L(K)
⎫
⎬
⎭
(11.104)
tuned to the underlying sequence of observations without any stochastic assumptions, where L(K) is a
constant that only depends on the number of contiguous segments.
The performance of a universal linear predictor that switches in time is demonstrated with the
following example.
In this example, we are tracking a process similar to a speech sample such that the state of the process
switches states abruptly every 500 samples, as shown in Figure 11.11. The state of the process at each
segment is represented by an AR model driven by a Gaussian noise [8]. Figure 11.12 shows the mean

1.11.3 Parametric Models for Signals and Systems
617
FIGURE 11.12
Mean square prediction error exhibited by (i) a fading memory RLS linear predictor and (ii) the universal
piecewise constant (in time) linear predictor.
square error that results when we attempt to track the process using a fading memory RLS algorithm
versus that obtained using a universal piecewise linear algorithm.
It can be seen that the universal piecewise linear algorithm recovers from the state transitions much
more quickly, since it competes against the best switching pattern, while the fading memory RLS
algorithm cannot recover in the short-data regime between transitions. Therefore, in such a situation, a
universal piecewise linear model gives superior performance relative to the standard single ﬁlter model.
Relevant Theory: Signal Processing Theory, Machine Learning and Statistical Signal Processing
See this Volume, Chapter 2 Continuous-Time Signals and Systems
See this Volume, Chapter 3 Discrete-Time Signals and Systems
See this Volume, Chapter 4 Random Signals and Stochastic Processes
See this Volume, Chapter 6 Digital Filter Structures and their Implementation
See this Volume, Chapter 9 Discrete Multi-Scale Transforms in Signal Processing
See this Volume, Chapter 11 Parametric Estimation
See this Volume, Chapter 12 Adaptive Filters
See this Volume, Chapter 14 Learning Theory
See this Volume, Chapter 17 Online Learning

618
CHAPTER 11 Parametric Estimation
See this Volume, Chapter 25 A Tutorial on Model Selection
See Vol. 3, Chapter 2 Model Order Selection
See Vol. 3, Chapter 3 Non-Stationary Signal Analysis Time-Frequency Approach
See Vol. 3, Chapter 8 Performance Analysis and Bounds
References
[1] A. Oppenheim, R.W. Schafer, J.R. Buck, Discrete-Time Signal Processing, second ed., Prentice Hall, 1999.
[2] L. Rabiner, B. Gold, Theory and Application of Digital Signal Processing, Prentice-Hall, Inc., Englewood
Cliffs, 1975.
[3] L.R. Rabiner, R.W. Schafer, Introduction to Digital Speech Processing, Prentice Hall, Inc., 1978.
[4] W. Feller, An Introduction to Probability Theory and its Applications, Wiley, 1968.
[5] B. Hajek, An Exploration of Random Processes for Engineers, 2011. Retrieved from An Exploration of
Random Processes for Engineers: <http://www.ifp.illinois.edu/∼hajek/Papers/randomprocesses.html>.
[6] S. Kay, Fundamentals of Statistical Signal Processing, Estimation Theory, vol. 1, Prentice Hall, 1993.
[7] J. Mahkoul, Linear prediction: a tutorial review, Proc. IEEE (1975) 561–580.
[8] A.H. Sayed, Fundamentals of Adaptive Filtering, Wiley-IEEE Press, 2003.
[9] M.L. Honig, D.G. Messerschmitt, Adaptive Filters: Structures, Algorithms and Applications, Springer, 1984.
[10] J.G. Proakis, Digital Communications, McGraw-Hill, 1983.
[11] H.V. Poor, An Introduction to Signal Detection and Estimation, Springer, New York, 1988.
[12] A.C. Singer, M. Feder, Universal linear prediction by model order weighting, IEEE Trans. Signal Process.
47 (10) (1999) 2685–2699.
[13] N. Cesa-Bianchi, G. Lugosi, Prediction Learning and Games, Cambridge University Press, 2006.
[14] A.C. Singer, S.S. Kozat, M. Feder, Universal linear least squares prediction upper and lower bounds, IEEE
Trans. Inform. Theory 48 (8) (2002) 2354–2362.
[15] O.J. Michel, A.O. Hero, A.E. Badel, Tree-structured nonlinear signal modeling and prediction, IEEE Trans.
Signal Process. 47 (11) (1999) 3027–3040.
[16] S.S. Kozat, A.C. Singer, G.C. Zeitler, Universal piecewise linear prediction via context trees, IEEE Trans.
Signal Process. 55 (7) (2007) 3730–3745.

12
CHAPTER
Adaptive Filters
Vítor H. Nascimento and Magno T. M. Silva
Department of Electronic Systems Engineering, University of São Paulo, São Paulo, SP, Brazil
1.12.1 Introduction
Adaptive ﬁlters are employed in situations in which the environment is constantly changing, so that a
ﬁxed system would not have adequate performance. As they are usually applied in real-time applications,
they must be based on algorithms that require a small number of computations per input sample.
These algorithms can be understood in two complementary ways. The most straightforward way
follows directly from their name: an adaptive ﬁlter uses information from the environment and from
the very signal it is processing to change itself, so as to optimally perform its task. The information
from the environment may be sensed in real time (in the form of a so-called desired signal), or may be
provided a priori, in the form of previous knowledge of the statistical properties of the input signal (as
in blind equalization).
On the other hand, we can think of an adaptive ﬁlter also as an algorithm to separate a mixture of two
signals. The ﬁlter must have some information about the signals to be able to separate them; usually this
is given in the form of a reference signal, related to only one of the two terms in the mixture. The ﬁlter
has then two outputs, corresponding to each signal in the mixture (see Figure 12.1). As a byproduct of
separating the signals, the reference signal is processed in useful ways. This way of viewing an adaptive
ﬁlter is very useful, particularly when one is learning the subject.
In the following sections we give an introduction to adaptive ﬁlters, covering from basic principles
to the most important recent developments. Since the adaptive literature is vast, and the topic is still
an active area of research, we were not able to treat all interesting topics. Along the text, we reference
important textbooks, classic papers, and the most promising recent literature. Among the many very
good textbooks on the subject, four of the most popular are [1–4].
We start with an application example, in the next section. In it, we show how an adaptive ﬁlter
is applied to cancel acoustic echo in hands-free telephony. Along the text we return frequently to
this example, to illustrate new concepts and methods. The example is completed by an overview of
how adaptive ﬁlters work, in which we use only deterministic arguments and concepts from basic linear
systems theory, in Section 1.12.1.2. This overview shows many of the compromises that arise in adaptive
ﬁlter design, without the more complicated math. Section 1.12.1.3 gives a small glimpse of the wide
variety of applications of adaptive ﬁlters, describing a few other examples. Of course, in order to fully
understand the behavior of adaptive ﬁlters, one must use estimation theory and stochastic processes,
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00012-7
© 2014 Elsevier Ltd. All rights reserved.
619

620
CHAPTER 12 Adaptive Filters
adaptive
ﬁlter
Mixture of signals
Output 2 (“error”signal)
Reference signal
Output 1
d1(n)
y1(n)
x1(n)
e1(n)
Adaptation
Algorithm
Filter
Adaptive filter
FIGURE 12.1
Inputs and outputs of an adaptive ﬁlter. Left: detailed diagram showing inputs, outputs and internal variable.
Right: simpliﬁed diagram.
which is the subject of Sections 1.12.2 and 1.12.3. The remaining sections are devoted to extensions to
the basic algorithms and recent results.
Adaptive ﬁlter theory brings together results from several ﬁelds: signal processing, matrix analysis,
control and systems theory, stochastic processes, and optimization. We tried to provide short reviews
for most of the necessary material in separate links (“boxes”), that the reader may follow if needed.
1.12.1.1 Motivation—acoustic echo cancellation
Suppose that you are talking with a person who uses a hands-free telephone inside a car. Let us assume
that the telephone does not have any acoustic echo canceler. In this case, the sound of your voice will
reach the loudspeaker, propagate inside the car, and suffer reﬂections every time it encounters the walls
of the car. Part of the sound can be absorbed by the walls, but part of it is reﬂected, resulting in an
echo signal. This signal is fed back to the microphone and you will hear your own voice with delay and
attenuation, as shown in Figure 12.2.
car
speech
echo
time (s)
time (s)
1
1
1
2
2
3
3
4
4
-1
0
0
0
0
-0.2
0.2
FIGURE 12.2
Echo in hands-free telephony. Click speech.wav to listen to a speech signal and echo.wav to listen to the
corresponding echo signal.

1.12.1 Introduction
621
adaptive
ﬁlter
car
speech
echo
time (s)
time (s)
1
1
1
2
2
3
3
4
4
-1
0
0
0
0
-0.2
0.2
FIGURE 12.3
Acoustic echo cancellation using an adaptive ﬁlter.
adaptive
ﬁlter
signal
+residual echo
echo
echo
near-end signal
near-end
path
far-end signal
FIGURE 12.4
General echo canceler conﬁguration.
The echo signal depends on the acoustic characteristics of each car and also on the location of the
loudspeaker and microphone. Furthermore, the acoustic characteristics can change during a phone call,
since the driver may open or close a window, for example. In order to follow the variations of the
environment, the acoustic echo must be canceled by an adaptive ﬁlter, which performs on-line updating
of its parameters through an algorithm. Figure 12.3 shows the insertion of an adaptive ﬁlter in the system
of Figure 12.2 to cancel the echo signal. In this conﬁguration, the adaptive ﬁlter must identify the echo
path and provide at its output an estimate of the echo signal. Thus, a synthesized version of the echo is
subtracted from the signal picked up by the car microphone. In an ideal scenario, the resulting signal
will be free of echo and will contain only the signal of interest. The general set-up of an echo canceler
is shown in Figure 12.4. The near-end signal (near with respect to the echo canceler) may be simply
noise when the person inside the car is in silence, as considered in Figures 12.2 and 12.3, or it can be a
signal of interest, that is, the voice of the user in the car.
You most likely have already heard the output of an adaptive echo canceler. However, you probably
did not pay attention to the initial period, when you can hear the adaptive ﬁlter learning the echo
impulse response, if you listen carefully. We illustrate this through a simulation, using a measured
impulse response of 256 samples (sampled at 8 kHz), and an echo signal produced from a recorded

622
CHAPTER 12 Adaptive Filters
−0.03
−0.02
−0.01
0.01
0.02
0.03
0
0
10
20
30
time (ms)
FIGURE 12.5
Measured impulse response (256 samples).
voice signal. The measured impulse response is shown in Figure 12.5. In the ﬁrst example, the near-
end signal is low-power noise (more speciﬁcally, white Gaussian noise with zero mean and variance
σ 2 = 10−4). Listen to e_nlms.wav, and pay attention to the beginning. You will notice that the voice
(the echo) is fading away, while the adaptive ﬁlter is learning the echo impulse response. This ﬁle is the
output of an echo canceler adapted using the normalized least-mean squares algorithm (NLMS), which
is described in Section 1.12.3.2.
Listen now to e_lsl.wav in this case the ﬁlter was adapted using the modiﬁed error-feedback
least-squares lattice algorithm (EF-LSL) [5], which is a low-cost and stable version of the recursive
least-squares algorithm (RLS) described in Section 1.12.3.3. Now you will notice that the echo fades
away much faster. This fast convergence is characteristic of the EF-LSL algorithm, and is of course
a very desirable property. On the other hand, the NLMS algorithm is simpler to implement and more
robust against imperfections in the implementation. An adaptive ﬁltering algorithm must satisfy several
requirements, such as convergence rate, tracking capability, noise rejection, computational complexity,
and robustness. These requirements are conﬂicting, so that there is no best algorithm that will outperform
all the others in every situation. The many algorithms available in the literature are different compromises
between these requirements [1–3].
Figure 12.6 shows 4 s of the echo signal, prior to cancellation (bottom) and a measure of the quality
of echo cancellation, the so-called echo return loss enhancement (ERLE) (top). The ERLE is deﬁned
as the ratio
ERLE = Power of original echo
Power of residual echo.
The higher the ERLE, the better the performance of the canceler. The ERLE drops when the original
echo is small (there is nothing to cancel!), and increases again when the echo is strong. The ﬁgure shows
that EF-LSL performs better than NLMS (but at a higher computational cost).

1.12.1 Introduction
623
0
0
1
2
3
4
60
40
20
time (s)
ERLE (dB)
EF-LSL
NLMS
FIGURE 12.6
Echo return loss enhancement for NLMS ( ˜μ = 0.5, δ = 10−5, M = 256) and EF-LSL (λ = 0.999,
δ = 10−5) algorithms. Bottom trace: echo signal prior to cancellation. The meaning of the several parameters
is explained in Sections 1.12.3.1, 1.12.3.2, and 1.12.3.3.
Assuming now that the near-end signal is another recorded voice signal (Click DT_echo.wav to listen
to this signal), we applied again the EF-LSL algorithm to cancel the echo. This case simulates a person
talking inside the car. Since the speech of this person is not correlated to the far-end signal, the output of
the adaptive ﬁlter converges again to an estimate of the echo signal. Thus, a synthesized version of the
echo is subtracted from the signal picked up by the car microphone and the resulting signal converges to
the speech of the person inside the car. This result can be veriﬁed by listening to the ﬁle e_lslDT.wav.
This is a typical example of use of an adaptive ﬁlter. The environment (the echo impulse response)
changes constantly, requiring a constant re-tuning of the canceling ﬁlter. Although the echo itself is not
known, we have access to a reference signal (the clean speech of the far-end user). This is the signal that is
processed to obtain an estimate of the echo. Finally, the signal captured by the near-end microphone (the
so-called desired signal) is a mixture of two signals: the echo, plus the near-end speech (or ambient noise,
whenthenear-enduserissilent).Thetaskoftheadaptiveﬁltercanbeseenastoseparatethesetwosignals.
We should mention that the name desired signal is somewhat misleading, although widespread in
the literature. Usually we are interested in separating the “desired” signal into two parts, one of which
is of no interest at all!
We proceed now to show how an algorithm can perform this task, modeling all signals as periodic
to simplify the arguments.
1.12.1.2 A quick tour of adaptive ﬁltering
Before embarking in a detailed explanation of adaptive ﬁltering and its theory, it is a good idea to present
the main ideas in a simple setting, to help the reader create intuition. In this section, we introduce the
least-mean squares (LMS) algorithm and some of its properties using only deterministic arguments and
basic tools from linear systems theory.

624
CHAPTER 12 Adaptive Filters
x(n)
Echo path
y(n)
v(n)
d(n)
+
+
FIGURE 12.7
A model for the echo.
1.12.1.2.1
Posing the problem
Consider again the echo-cancellation problem, seen in the previous section (see Figure 12.7). Let y(n)
represent the echo, v(n) represent the voice of the near-end user, and x(n) the voice of the far-end user
(the reference). d(n) represents the signal that would be received by the far-end user without an echo
canceler (the mixture, or desired signal).
The echo path represents the changes that the far-end signal suffers when it goes through the digital-
to-analog (D/A) converter, the loudspeaker, the air path between the loudspeaker and the microphone
(including all reﬂections), the microphone itself and its ampliﬁer, and the analog-to-digital (A/D) con-
verter. The microphone signal d(n) will always be a mixture of the echo y(n) and an unrelated signal
v(n), which is composed of the near-end speech plus noise. Our goal is to remove y(n) from d(n). The
challenge is that we have no means to measure y(n) directly, and the echo path is constantly changing.
1.12.1.2.2
Measuring how far we are from the solution
How is the problem solved, then? Since we cannot measure y(n), the solution is to estimate it from the
measurable signals x(n) and d(n). This is done as follows: imagine for now that all signals are periodic,
so they can be decomposed as Fourier series
x(n) =
K0

k=1
Ak cos (kω0n + ϕk),
v(n) =
K1

ℓ=1
Bℓcos (ℓω1n + θℓ),
(12.1)
for certain amplitudes Ak and Bℓ, phases ϕk and θℓ, and frequencies ω0 and ω1. The highest frequencies
appearing in x(n) and v(n) must satisfy K0ω0 < π, K1ω1 < π, since we are dealing with sampled
signals (Nyquist criterion). We assume for now that the echo path is ﬁxed (not changing), and can
be modeled by an unknown linear transfer function H(z). In this case, y(n) would also be a periodic
sequence with fundamental frequency ω0,
y(n) =
K0

k=1
Ck cos (kω0n + ψk),
with Cke jψk = H(e jkω0)Ake jϕk. The signal picked by the microphone is then
d(n) = y(n) + v(n) =
K0

k=1
Ck cos (kω0n + ψk) +
K1

ℓ=1
Bℓcos (ℓω1n + θℓ).

1.12.1 Introduction
625
x(n)
H(z)
y(n)
v(n)
d(n)
ˆy(n)
+
+
+
−
ˆH(z)
e(n)
FIGURE 12.8
Echo canceler.
Since we know x(n), if we had a good approximation 
H(z) to H(z), we could easily obtain an approx-
imation ˆy(n) to y(n) and subtract it from d(n), as in Figure 12.8.
How could we ﬁnd 
H(z) in real time? Recall that only d(n), x(n), ˆy(n), and e(n) can be observed.
For example, how could we know that we have the exact ﬁlter, that is, that 
H(z) = H(z), by looking
only at these signals? To answer this question, take a closer look at e(n). Let the output of ˆH(z) be
ˆy(n) =
k0

k=1
ˆCk cos (kω0n + ˆψk),
and thus
e(n) = d(n) −ˆy(n) = y(n) −ˆy(n) + v(n)
=
K0

k=1
[Ck cos (kω0n + ψk) −ˆCk cos (kω0n + ˆψk)]
+
K1

ℓ=1
Bℓcos (ℓω1n + θℓ)
=
K0

k=1

Ck cos (kω0n + ˜ψk) +
K1

ℓ=1
Bℓcos (ℓω1n + θℓ),
(12.2)
where 
Cke j ˜ψk = Cke jψk −ˆCke j ˆψk, k = 1, . . . , K0.

626
CHAPTER 12 Adaptive Filters
Assuming that the cosines in x(n) and v(n) have no frequencies in common, the simplest approach
is to measure the average power of e(n). Indeed, if all frequencies kω0 and ℓω1 appearing in e(n) are
different from one another, then the average power of e(n) is
P =
K0

k=1

C2
k
2 +
K1

ℓ=1
B2
ℓ
2 .
(12.3)
If ˆy(n) = y(n), then P is at its minimum,
Po = min

H(z)
P =
K1

ℓ=1
B2
ℓ
2 ,
which is the average power of v(n). In this case, e(n) is at its optimum: e(n) = eo(n) = v(n), and the
echo is completely canceled.
It is important to see that this works because the two signals we want to separate, y(n) and v(n), are
uncorrelated, that is, they are such that
lim
N→∞
1
N + 1
N/2

−N/2
y(n)v(n) = lim
N→∞
1
N + 1
N/2

−N/2
y(n)eo(n) = 0.
(12.4)
This property is known as the orthogonality condition. A form of orthogonality condition will appear
in general whenever one tries to minimize a quadratic function, as is the case here. This is discussed in
more detail in Section 1.12.2.
The average power P could then be used as a measure of how good is our approximation ˆy(n). The
important point is that it is very easy to ﬁnd a good approximation to P. It sufﬁces to low-pass ﬁlter
e2(n), for example,
P(n) = 1
N
N−1

k=0
e2(n −k)
(12.5)
is a good approximation if the window length N is large enough.
The adaptive ﬁltering problem then becomes an optimization problem, with the particularity that the
cost function that is being minimized is not known exactly, but only through an approximation, as in
(12.5). In the following sections, we discuss in detail the consequences of this fact.
It is also important to remark that the average error power is not the only possible choice for cost
function. Although it is the most popular choice for a number of reasons, in some applications other
choices are more adequate. Even posing the adaptive ﬁltering problem as an optimization problem is
not the only alternative, as shown by recent methods based on projections on convex sets [6].
1.12.1.2.3
Choosing a structure for the ﬁlter
Our approximation ˆy(n) will be built by minimizing the estimated average error power (12.5) as a
function of the echo path model 
H(z), that is, our estimated echo path will be the solution of

Ho(z) = arg min

H(z)
P(n).
(12.6)

1.12.1 Introduction
627
Keep in mind that we are looking for a real-time way of solving (12.6), since P(n) can only be
obtained by measuring e(n) for a certain amount of time. We must then be able to implement the
current approximation 
H(z), so that ˆy(n) and e(n) may be computed.
This will impose practical restrictions on the kind of function that 
H(z) may be: since memory and
processing power are limited, we must choose beforehand a structure for 
H(z). Memory and processing
power will impose a constraint on the maximum order the ﬁlter may have. In addition, in order to write
the code for the ﬁlter, we must decide if it will depend on past outputs (IIR ﬁlters) or only on past inputs
(FIR ﬁlters). These choices must be made based on some knowledge of the kind of echo path that our
system is likely to encounter.
To explain how these choices can be made, let us simplify the problem a little more and restrict the
far-end signal x(n) to a simple sinusoid (i.e., assume that K0 = 1). In this case,
y(n) = C1 cos (ω0n + ϕ1),
with C1e jψ1 = H(e jω0)A1e jϕ1. Therefore, 
H(z) must satisfy


H(e jω) = H(e jω)

ω=ω0
(12.7)
only for ω = ω0—the value of 
H(e jω) for other frequencies is irrelevant, since the input x(n) has only
one frequency. Expanding (12.7), we obtain
Re{ 
H(e jω0)} = Re{H(e jω0)},
Im{ 
H(e jω0)} = Im{H(e jω0)}.
(12.8)
The optimum 
H(z) is deﬁned through two conditions. Therefore an FIR ﬁlter with just two coefﬁcients
would be able to satisfy both conditions for any value of H(e jω0), so we could deﬁne

H(z) = w0 + w1z−1,
and the values of w0 and w1 would be chosen to solve (12.6). Note that the argument generalizes for
K0 > 1: in general, if x(n) has K0 harmonics, we could choose 
H(z) as an FIR ﬁlter with length 2K0,
two coefﬁcients per input harmonic.
This choice is usually not so simple: in practice, we would not know K0, and in the presence of
noise (as we shall see later), a ﬁlter with fewer coefﬁcients might give better performance, even though
it would not completely cancel the echo. The structure of the ﬁlter also affects the dynamic behavior of
the adaptive ﬁlter, so simply looking at equations such as (12.7) does not tell the whole story. Choosing
the structure for 
H(z) is one of the most difﬁcult steps in adaptive ﬁlter design. In general, the designer
must test different options, initially perhaps using recorded signals, but ultimately building a prototype
and performing some tests.
1.12.1.2.4
Searching for the solution
Returning to our problem, assume then that we have decided that an FIR ﬁlter with length M is adequate
to model the echo path, that is,

H(z) =
M−1

k=0
wkz−k.

628
CHAPTER 12 Adaptive Filters
Our minimization problem (12.6) now reduces to ﬁnding the coefﬁcients w0, . . . , wM−1 that solve
min
w0,...,wM−1
P(n).
(12.9)
Since P(n) must be computed from measurements, we must use an iterative method to solve (12.9).
Many algorithms could be used, as long as they depend only on measurable quantities (x(n), d(n), ˆy(n),
and e(n)). We will use the steepest descent algorithm (also known as gradient algorithm) as an example
now. Later we show other methods that also could be used. If you are not familiar with the gradient
algorithm, see Box 1 for an introduction.
The cost function in (12.9) is
P(n) = 1
N
N−1

k=0
e2(n −k) = 1
N
N−1

k=0
[d(n −k) −ˆy(n −k)]2.
We need to rewrite this equation so that the steepest descent algorithm can be applied. Recall also that
now the ﬁlter coefﬁcients will change, so deﬁne the vectors
w(n) = [w0(n) w1(n)
· · ·
wM−1(n)]T ,
and
x(n) = [x(n)
x(n −1)
· · ·
x(n −M + 1)]T ,
where (·)T denotes transposition. At each instant, we have ˆy(n) = wT (n)x(n), and our cost function
becomes
P(n) = 1
N
N−1

k=0

d(n −k) −wT (n −k)x(n −k)
2
,
(12.10)
which depends on w(n), . . . , w(n −N + 1). In order to apply the steepest descent algorithm, let us
keep the coefﬁcient vector w(n) constant during the evaluation of P, as follows. Starting from an initial
condition w(0), compute for n = 0, N, 2N, . . . (The notation is explained in Boxes 2 and 6.)
w(n + N) = w(n) −α ∂P(n + N −1)
∂wT

w=w(n)
,
(12.11)
where α is a positive constant, and w(n + N −1) = w(n + N −2) = · · · = w(n). Our cost function
now depends on only one w(n) (compare with (12.10)):
P(n + N −1) = 1
N
N−1

k=0

d(n + N −1 −k) −wT (n)x(n + N −1 −k)
2
= 1
N
N−1

k=0

d(n + k) −wT (n)x(n + k)
2
, n = 0, N, 2N, . . .
(12.12)
We now need to evaluate the gradient of P. Expanding the expression above, we obtain
P(n + N −1) = 1
N
N−1

k=0

d(n + k) −
M−1

ℓ=0
wℓ(n)x(n + k −ℓ)
	2
,

1.12.1 Introduction
629
so
∂P(n −N + 1)
∂wm(n)
= −2
N
N−1

k=0

d(n + k) −wT (n)x(n + k)

x(n + k −m)
= −2
N
N−1

k=0
e(n + k)x(n + k −m),
and
∂P(n −N + 1)
∂wT
= −2
N
N−1

k=0
e(n + k)x(n + k), n = 0, N, 2N, . . .
(12.13)
As we needed, this gradient depends only on measurable quantities, e(n + k) and x(n + k), for
k = 0, . . . , N −1. Our algorithm for updating the ﬁlter coefﬁcients then becomes
w(n + N) = w(n) + μ 1
N
N−1

k=0
e(n + k)x(n + k), n = 0, N, 2N, . . . ,
(12.14)
where we introduced the overall step-size μ = 2α.
We still must choose μ and N. The choice of μ is more complicated and will be treated in Sec-
tion 1.12.1.2.5. The value usually employed for N may be a surprise: in almost all cases, one uses
N = 1, resulting in the so-called least-mean squares (LMS) algorithm
w(n + 1) = w(n) + μe(n)x(n),
(12.15)
proposed initially by Widrow and Hoff in 1960 [7] (Widrow [8] describes the history of the creation
of the LMS algorithm). The question is, how can this work, if no average is being used for estimating
the error power? An intuitive answer is not complicated: assume that μ is a very small number so that
μ = μ0/N for a large N. In this case, we can approximate e(n + k) from (12.15) as follows.
w(n + 1) = w(n) + μ0
N e(n)x(n) ≈w(n),
if N is large.
Since w(n+1) ≈w(n), we have e(n+1) = d(n+1)−wT (n+1)x(n+1) ≈d(n+1)−wT (n)x(n+1).
Therefore, we could approximate
e(n + k) ≈d(n + k) −wT (n)x(n + k),
k = 0, . . . , N −1,
so N steps of the LMS recursion (12.15) would result
w(n + N) ≈w(n) + μ0
N
N−1

k=0

d(n + k) −wT (n)x(n + k)

x(n + k),
just what would be obtained from (12.14). The conclusion is that, although there is no explicit average
being taken in the LMS algorithm (12.15), the algorithm in fact computes an implicit, approximate
average if the step-size is small. This is exactly what happens, as can be seen in the animations in

630
CHAPTER 12 Adaptive Filters
(a) µ =
(b) µ = 0.5
0.1
(c) µ = 1.1
(d) µ = 1.9
FIGURE 12.9
LMS algorithm for echo cancelation with sinusoidal input. Click LMS video ﬁles to see animations on your
browser.

1.12.1 Introduction
631
Figure 12.9. These simulations were prepared with
x(n) = cos (0.4πn + ϕ1),
v(n) = 0.2 cos (0.2πn + θ1),
where ϕ1 and θ1 were chosen randomly in the interval [0, 2π). Several step-sizes were used. The
estimates computed with the LMS algorithm are marked by the crosses (red in the web version). The
initial condition is at the left, and the theoretical optimum is at the right end of each ﬁgure. In the
simulations, the true echo path was modeled by the ﬁxed ﬁlter
H(z) = 1.5 + 0.5z−1.
For comparison, we also plotted the estimates that would be obtained by the gradient algorithm using
the exact value of the error power at each instant. These estimates are marked with small dots (blue in
the web version).
Note that when a small step-size is used, such as μ = 0.1 in Figure 12.9a, the LMS ﬁlter stays close
to the dots obtained assuming exact knowledge of the error power. However, as the step-size increases,
the LMS estimates move farther away from the dots. Although convergence is faster, the LMS estimates
do not reach the optimum and stay there: instead, they hover around the optimum. For larger step-sizes,
the LMS estimates can go quite far from the optimum (Figure 12.9b and c). Finally, if the step-size is
too large, the algorithm will diverge, that is, the ﬁlter coefﬁcients grow without bounds (Figure 12.9d).
1.12.1.2.5
Tradeoff between speed and precision
The animations in Figure 12.9 illustrate an important problem in adaptive ﬁlters: even if you could
magically chose as initial conditions the exact optimum solutions, the adaptive ﬁlter coefﬁcients would
not stay there! This happens because the ﬁlter does not know the exact value of the cost function it
is trying to minimize (in this case, the error power): since P(n) is an approximation, noise (and the
near-end signal in our echo cancellation example) would make the estimated gradient non-zero, and
the ﬁlter would wander away from the optimum solution. This effect keeps the minimum error power
obtainable using the adaptive ﬁlter always a little higher than the optimum value. The difference between
the (theoretical, unattainable without perfect information) optimum and the actual error power is known
as excess mean-square error (EMSE), and the ratio between the EMSE and the optimum error is known
as misadjustment (see Eqs. (12.171) and (12.174)).
For small step-sizes the misadjustment is small, since the LMS estimates stay close to the estimates
that would be obtained with exact knowledge of the error power. However, a small step-size also means
slow convergence. This trade-off exists in all adaptive ﬁlter algorithms, and has been an intense topic
of research: many algorithms have been proposed to allow faster convergence without increasing the
misadjustment. We will see some of them in the next sections.
The misadjustment is central to evaluate the performance of an adaptive ﬁlter. In fact, if we want to
eliminate the echo from the near-end signal, we would like that after convergence, e(n) ≈v(n). This
is indeed what happens when the step-size is small (see Figure 12.10a). However, when the step-size
is increased, although the algorithm converges more quickly, the performance after convergence is not
very good, because of the wandering of the ﬁlter weights around the optimum (Figure 12.10b–d—in
the last case, for μ = 1.9, the algorithm is diverging.).

632
CHAPTER 12 Adaptive Filters
0
50
100
150
−1.5
−1
−0.5
0
0.5
1
1.5
2
n
0
50
100
150
−0.5
0
0.5
1
1.5
2
n
0
50
100
150
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
n
0
50
100
150
−10
−8
−6
−4
−2
0
2
4
6
8
10
n
(a) µ =
(b) µ = 0.5
0.1
(c)
e(n)
e(n)
e(n)
e(n)
µ = 1.1
(d) µ = 1.1
FIGURE 12.10
Compromise between convergence rate and misadjustment in LMS. Plots of e(n)xn for several values of μ.
We will stop this example here for the moment, and move forward to a description of adaptive ﬁlters
using probability theory and stochastic processes. We will return to it during discussions of stability
and model order selection.
The important message from this section is that an adaptive ﬁlter is able to separate two signals in a
mixture by minimizing a well-chosen cost function. The minimization must be made iteratively, since
the true value of the cost function is not known: only an approximation to it may be computed at each
step. When the cost function is quadratic, as in the example shown in this section, the components of
the mixture are separated such that they are in some sense orthogonal to each other.

1.12.1 Introduction
633
Box 1: Steepest descent algorithm
Given a cost function
J(w) = J(w0, . . . , wM−1),
with w = [w0, . . . , wM−1]T ((·)T denotes transposition), we want to ﬁnd the minimum of J, that is,
we want to ﬁnd wo such that
wo = arg min
w J(w).
The solution can be found using the gradient of J(w), which is deﬁned by
∇w J(w) = ∂J
∂wT =
⎡
⎢⎢⎢⎢⎣
∂J
∂w0
∂J
∂w1...
∂J
∂wM−1
⎤
⎥⎥⎥⎥⎦
.
See Box 2 for a brief explanation about gradients and Hessians, that is, derivatives of functions of several
variables.
The notation ∂J/∂wT is most convenient when we deal with complex variables, as in Box 6. We use it
for real variables for consistency.
Since the gradient always points to the direction in which J(w) increases most quickly, the steepest
descent algorithm searches iteratively for the minimum of J(w) taking at each iteration a small step in
the opposite direction, i.e., towards −∇w J(w):
w(n + 1) = w(n) −μ
2 ∇w J(w(n)),
(12.16)
where μ is a step-size, i.e., a constant controlling the speed of the algorithm. As we shall see, this
constant should not be too small (or the algorithm will converge too slowly) neither too large (or the
recursion will diverge).
As an example, consider the quadratic cost function with M = 2
J(w) = 1 −2wT r + wT Rw,
(12.17)
with
r =
 1
0

,
R =

4
3 −2
3
−2
3
4
3
	
.
(12.18)
The gradient of J(w) is
∇w J(w) = ∂J
∂wT = −2r + 2Rw.
(12.19)
Of course, for this example we can ﬁnd the optimum wo by equating the gradient to zero:
∇w J(wo) = 0 =⇒Rwo = r =⇒wo = R−1r =
 0.9877
0.4902

.
(12.20)

634
CHAPTER 12 Adaptive Filters
Note that in this case the Hessian of J(w) (the matrix of second derivatives, see Box 2) is simply
∇2
w J = 2R. As R is symmetric with positive eigenvalues (λ1 = 2 and λ2 = 2
3), it is positive-deﬁnite,
and we can conclude that wo is indeed a minimum of J(w) (See Boxes 2 and 3. Box 3 lists several
useful properties of matrices.)
Even though in this case we could compute the optimum solution through (12.20), we will apply the
gradient algorithm with the intention of understanding how it works and which are its main limitations.
From (12.16) and (12.17), we obtain the recursion
w(n + 1) = w(n) + μ(r −Rw(n)).
(12.21)
The user must choose an initial condition w(0).
In Figure 12.11 we plot the evolution of the approximations computed through (12.21) against the
level sets (i.e., curves of constant value) of the cost function (12.17). Different choices of step-size μ are
shown. In Figure 12.11a, a small step-size is used. The algorithm converges to the correct solution, but
slowly. In Figure 12.11b, we used a larger step-size—convergence is now faster, but the algorithm still
needs several iterations to reach the solution. If we try to increase the step-size even further, convergence
at ﬁrst becomes slower again, with the algorithm oscillating towards the solution (Figure 12.11c). For
even larger step-sizes, such as that in Figure 12.11d, the algorithm diverges: the ﬁlter coefﬁcients get
farther and farther away from the solution.
It is important to ﬁnd the maximum step-size for which the algorithm (12.21) remains stable. For
this, we need concepts from linear systems and linear algebra, in particular eigenvalues, their relation to
stability of linear systems, and the fact that symmetric matrices always have real eigenvalues (see Box 3).
The range of allowed step-sizes is found as follows. Rewrite (12.21) as
w(n + 1) =

I −μR

w(n) + μr,
which is a linear recursion in state-space form (I is the identity matrix).
Linear systems theory [9] tells us that this recursion converges as long as the largest eigenvalue of
A = I −μR has absolute value less than one. The eigenvalues of A are the roots of
det (β I −(I −μR)) = det ((β −1)I + μR) = −det ((1 −β)I −μR) = 0.
Let 1−β = ν. Then β is an eigenvalue of A if and only if ν = 1−β is an eigenvalue of μR. Therefore,
if we denote by λi the eigenvalues of R, the stability condition is
−1 < 1 −μλi < 1 ∀i ⇔0 < μ < 2
λi
∀i ⇔0 < μ <
2
max{λi}.
The stability condition for our example is thus 0 < μ < 1, in agreement with what we saw in
Figure 12.11.
The gradient algorithm leads to relatively simple adaptive ﬁltering algorithms; however, it has an
important drawback. As you can see in Figure 12.11a, the gradient does not point directly to the direction
of the optimum. This effect is heightened when the level sets of the cost function are very elongated,
as in Figure 12.12. This case corresponds to a quadratic function in which R has one eigenvalue much
smaller than the other. In this example, we replaced the matrix R in (12.18) by
R′ = 1
9
 25 20
20 25

.
This matrix has eigenvalues λ′
1 = 5 and λ′
2 = 5
9.

1.12.1 Introduction
635
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
w1
w2
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
w1
w2
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
w1
w2
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
w1
w2
(a) µ =
(b) µ = 0.5
0.1
(c) µ = 0.9
(d) µ = 1.1
FIGURE 12.11
Performance of the steepest descent algorithm for different step-sizes. The crosses (x) represent the suc-
cessive approximations w(n), plotted against the level curves of (12.17). The initial condition is the point in
the lower left.
Let us see why this is so. Recall from Box 3 that for every symmetric matrix there exists an orthogonal
matrix U (that is, U−1 = UT ) such that
UT RU =
 λ1 0
0 λ2

= ,
(12.22)
where we deﬁned the diagonal eigenvalue matrix .

636
CHAPTER 12 Adaptive Filters
Let us apply a change of coordinates to (12.19). The optimum solution to (12.17) is
wo = R−1r.
Our ﬁrst change of coordinates is to replace w(n) by ˜w(n) = wo −w(n) in (12.19). Subtract (12.19)
from wo and replace r in (12.19) by r = Rwo to obtain
˜w(n + 1) = ˜w(n) −μR ˜w(n) =

I −μR

˜w(n).
(12.23)
Next, multiply this recursion from both sides by UT = U−1
UT ˜w(n + 1) = UT 
I −μR

UUT
  
=I
˜w(n) =

I −μ

UT ˜w(n).
Deﬁning ¯w(n) = UT ˜w(n), we have rewritten the gradient equation in a new set of coordinates, such
that now the equations are uncoupled. Given that  is diagonal, the recursion is simply
 ¯w1(n + 1)
¯w2(n + 1)

=
 (1 −μλ1) ¯w1(n)
(1 −μλ2) ¯w2(n)

.
(12.24)
Note that, as long as |1 −μλi| < 1 for i = 1, 2, both entries of ¯w(n) will converge to zero, and
consequently w(n) will converge to wo.
The stability condition for this recursion is
|1 −μλi| < 1, i = 1, 2 ⇒μ <
2
max{λ1, λ2}.
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
w1
w2
FIGURE 12.12
Performance of the steepest descent algorithm for a problem with large ratio of eigenvalues. The crosses
(x) represent the successive approximations w(n), plotted against the level curves of (12.17). The initial
condition is the point in the lower left. Step-size μ = 0.1.

1.12.1 Introduction
637
When one of the eigenvalues is much larger than the other, say, when λ1 ≫λ2, the rate of convergence
for the direction relative to the smaller eigenvalue becomes
1 > 1 −μλ2 > 1 −2 λmin
λmax
≈1,
and even if one of the coordinates in ¯w(n) converges quickly to zero, the other will converge very slowly.
This is what we saw in Figure 12.12. The ratio λmax/λmin is known as the eigenvalue spread of a matrix.
In general, the gradient algorithm converges slowly when the eigenvalue spread of the Hessian is
large. One way of solving this problem is to use a different optimization algorithm, such as the Newton
or quasi-Newton algorithms, which use the inverse of the Hessian matrix, or an approximation to it, to
improve the search direction used by the gradient algorithm.
Although these algorithms converge very quickly, they require more computational power, compared
to the gradient algorithm. We will see more about them when we describe the RLS (recursive least-
squares) algorithm in Section 1.12.3.3.
For example, for the quadratic cost-function (12.17), the Hessian is equal to 2R. If we had a good
approximation ˆR to R, we could use the recursion
w(n + 1) = w(n) + μ ˆR
−1[r −Rw(n)].
(12.25)
This class of algorithms is known as quasi-Newton (if ˆR
−1 is an approximation) or Newton (if ˆR
−1 is
exact). In the Newton case, we would have
w(n + 1) = (1 −μ)w(n) + wo,
that is, the algorithm moves at each step precisely in the direction of the optimum solution. Of course,
this is not a very interesting algorithm for a quadratic cost function as in this example (the optimum
solution is used in the recursion!) However, this method is very useful when the cost function is not
quadratic and in adaptive ﬁltering, when the cost function is not known exactly.
Box 2: Gradients
Consider a function of several variables J(w), where
w =
⎡
⎢⎢⎢⎣
w0
w1
...
wM−1
⎤
⎥⎥⎥⎦.
Its gradient is deﬁned by
∇w J(w) = ∂J
∂wT =
⎡
⎢⎢⎢⎢⎣
∂J
∂w0
∂J
∂w1...
∂J
∂wM−1
⎤
⎥⎥⎥⎥⎦
.

638
CHAPTER 12 Adaptive Filters
The real value of the notation ∂J/∂wT will only become apparent when working with functions of
complex variables, as shown in Box 6. We also deﬁne, for consistency,
∂J
∂w =

∂J
∂w0
∂J
∂w1 . . .
∂J
∂wM−1

.
As an example, consider the quadratic cost function
J(w) = b −2wT r + wT Rw.
(12.26)
The gradient of J(w) is (verify!)
∇w J(w) = ∂J
∂wT = −2r + 2Rw.
(12.27)
If we use the gradient to ﬁnd the minimum of J(w), it would be necessary also to check the second-
order derivative, to make sure the solution is not a maximum or a saddle point. The second order
derivative of a function of several variables is a matrix, the Hessian. It is deﬁned by
∇2
w J =
∂2J
∂w ∂wT =
⎡
⎢⎢⎢⎣
∂
∂w

∂J
∂w0

...
∂
∂w

∂J
∂wM−1

⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎣
∂2 J
∂w2
0
. . .
∂2 J
∂w0 ∂wM−1
...
...
...
∂2 J
∂wM−1 ∂w0 . . .
∂2 J
∂w2
M−1
⎤
⎥⎥⎥⎦.
(12.28)
Note that for the quadratic cost-function (12.26), the Hessian is equal to R + RT . It is equal to 2R if
R is symmetric, that is, if RT = R. This will usually be the case here. Note that R + RT is always
symmetric: in fact, since for well-behaved functions J it holds that
∂2J
∂wi ∂w j
=
∂2J
∂w j ∂wi
,
the Hessian is usually symmetric.
Assume that we want to ﬁnd the minimum of J(w). The ﬁrst-order conditions are then
∂J
∂w

wo
= 0.
The solution wo is a minimum of J(w) if the Hessian at wo is a positive semi-deﬁnite matrix, that is, if
xT ∇2
w J(wo)x ≥0
for all directions x.

1.12.1 Introduction
639
Box 3: Useful results from Matrix Analysis
Here we list without proof a few useful results from matrix analysis. Detailed explanations can be found,
for example, in [10–13].
Fact 1 (Traces).
The trace of a matrix A ∈CM×M is the sum of its diagonal elements:
Tr(A) =
M

i=1
aii,
(12.29)
in which the aii are the entries of A. For any two matrices B ∈CM×N and C ∈CN×M, it holds that
Tr(BC) = Tr(C B).
(12.30)
In addition, if λi, i = 1, . . . , M are the eigenvalues of a square matrix A ∈CM×M, then
Tr(A) =
M

i=1
λi.
(12.31)
Fact 2 (Singular matrices).
A matrix A ∈CM×M is singular if and only if there exists a nonzero
vector u such that Au = 0.
The null space, or kernel N(A) of a matrix A ∈CM×N is the set of vectors u ∈CN such that Au = 0.
A square matrix A ∈CM×M is nonsingular if, and only if, N(A) = {0}, that is, if its null space contains
only the null vector.
Note that, if A = K
k=1 ukvH
k , with uk, vk ∈CM, then A cannot be invertible if K < M. This is
because when K < M, there always exists a nonzero vector x ∈CM such that vH
k x = 0 for 1 ≤k ≤K,
and thus Ax = 0 with x ̸= 0.
Fact 3 (Inverses of block matrices).
Assume the square matrix A is partitioned such that
A =
 A11 A12
A21 A22

,
with A11 and A22 square. If A and A11 are invertible (nonsingular), then
A−1 =

I −A−1
11 A12
0
I
 
A−1
11
0
0
−1
 
I
0
−A21 A−1
11 I

,
where  = A22 −A21 A−1
11 A12 is the Schur complement of A11 in A. If A22 and A are invertible, then
A−1 =

I
0
−A−1
22 A21 I
 

′−1
0
0
A−1
22
 
I −A12 A−1
22
0
I

,
where ′ = A11 −A12 A−1
22 A21.

640
CHAPTER 12 Adaptive Filters
Fact 4 (Matrix inversion Lemma).
Let A and B be two nonsingular M × M matrices, D ∈CK×K
be also nonsingular, and C, E ∈CM×K be such that
A = B + C DEH.
(12.32)
The inverse of A is then given by
A−1 = B−1 −B−1C(D−1 + EH B−1C)−1EH B−1.
(12.33)
The lemma is most useful when K ≪M. In particular when K = 1, D + EH B−1C is a scalar, and
we have
A−1 = B−1 −
B−1C EH B−1
D−1 + EH B−1C .
Fact 5 (Symmetric matrices).
Let A ∈CM×M be Hermitian Symmetric (or simply Hermitian), that
is, AH =

AT ∗= A. Then, it holds that
1. All eigenvalues λi, i = 1, . . . , M of A are real.
2. A has a complete set of orthonormal eigenvectors ui, i = 1, . . . , M. Since the eigenvectors are
orthonormal, uH
i u j = δi j, where δi j = 1 if i = j and 0 otherwise.
3. Arranging the eigenvectors in a matrix U =
 u1 . . . uM

, we ﬁnd that U is unitary, that is, U HU =
UU H = I. In addition,
U H AU =  =
⎡
⎢⎢⎢⎣
λ1 0 . . . 0
0 λ2 . . . 0
...
... ...
...
0 0 . . . λM
⎤
⎥⎥⎥⎦.
4. If A = AT ∈RM×M, it is called simply symmetric. In this case, not only the eigenvalues, but also
the eigenvectors are real.
5. If A is symmetric and invertible, then A−1 is also symmetric.
Fact 6 (Positive-deﬁnite matrices).
If A ∈CM×M is Hermitian symmetric and moreover for all
u ∈CM it holds that
uH Au ≥0,
(12.34)
then A is positive semi-deﬁnite. Positive semi-deﬁnite matrices have non-negative eigenvalues: λi ≥0,
i = 1, . . . , M. They may be singular, if one or more eigenvalue is zero.
If the inequality (12.34) is strict, that is, if
uH Au > 0
(12.35)
for all u ̸= 0, then A is positive-deﬁnite. All eigenvalues λi of positive-deﬁnite matrices satisfy λi > 0.
Positive-deﬁnite matrices are always nonsingular.
Finally, all positive-deﬁnite matrices admit a Cholesky factorization, i.e., if A is positive-deﬁnite,
there is a lower-triangular matrix L such that A = LLH [14].

1.12.1 Introduction
641
Fact 7 (Norms and spectral radius).
The spectral radius ρ(A) of a matrix A ∈CM×M is
ρ(A) = max
1≤i≤M |λi|,
(12.36)
in which λi are the eigenvalues of A. The spectral radius is important, because a linear system
x(n + 1) = Ax(n)
is stable if and only if ρ(A) ≤1, as is well known. A useful inequality is that, for any matrix norm ∥· ∥
such that for any A, B ∈CM×M
∥AB∥≤∥A∥∥B∥,
it holds that
ρ(A) ≤∥A∥.
(12.37)
This property is most useful when used with norms that are easy to evaluate, such as the 1-norm,
∥A∥1 = max
1≤i≤M
M

j=1
|ai j|,
where ai j are the entries of A. We could also use the inﬁnity norm,
∥A∥∞= max
1≤j≤M
M

i=1
|ai j|.
1.12.1.3 Applications
Due to the ability to adjust themselves to different environments, adaptive ﬁlters can be used in different
signal processing and control applications. Thus, they have been used as a powerful device in several
ﬁelds, such as communications, radar, sonar, biomedical engineering, active noise control, modeling,
etc. It is common to divide these applications into four groups:
1. interference cancellation;
2. system identiﬁcation;
3. prediction; and
4. inverse system identiﬁcation.
In the ﬁrst three cases, the goal of the adaptive ﬁlter is to ﬁnd an approximation ˆy(n) for the signal
y(n), which is contained in the signal d(n) = y(n) + v(n). Thus, as ˆy(n) approaches y(n), the signal
e(n) = d(n) −ˆy(n) approaches v(n). The difference between these applications is in what we are
interested. In interference cancellation, we are interested in the signal v(n), as is the case of acoustic
echo cancellation, where v(n) is the speech of the person using the hands-free telephone (go back
to Figures 12.4 and 12.7). In system identiﬁcation, we are interested in the ﬁlter parameters, and in

642
CHAPTER 12 Adaptive Filters
adaptive
ﬁlter
d(n)
v(n)
y(n)
e(n)
x(n)
ˆ(n)
y
FIGURE 12.13
Scheme for interference cancellation and system identiﬁcation. x(n) and y(n) are correlated to each other.
prediction, we may be interested in the signal v(n) and/or in the ﬁlter parameters. Note that in all
these applications (interference cancellation, system identiﬁcation and prediction), the signal e(n) is
an approximation for v(n) and should not converge to zero, except when v(n) ≡0. In inverse system
identiﬁcation, differently from the other applications, the signal at the output of the adaptive ﬁlter must
be as close as possible to the signal d(n) and thus, ideally, the signal e(n) should be zeroed. In the
sequel, we give an overview of these four groups in order to arrive at a common formulation that will
simplify our analysis of adaptive ﬁltering algorithms in further sections.
1.12.1.3.1
Interference cancellation
In a general interference cancellation problem, we have access to a signal d(n), which is a mixture of
two other signals, v(n) and y(n). We are interested in one of these signals, and want to separate it from
the other (the interference) (see Figure 12.13). Even though we do not know v(n) or y(n), we have some
information about y(n), usually in the form of a reference signal x(n) that is related to y(n) through a
ﬁltering operation H. We may know the general form of this operation, for example, we may know that
the relation is linear and well approximated by an FIR ﬁlter with 200 coefﬁcients. However, we do not
know the parameters (the ﬁlter coefﬁcients) necessary to reproduce it. The goal of the adaptive ﬁlter is
to ﬁnd an approximation ˆy(n) to y(n), given only x(n) and d(n). In the process of ﬁnding this ˆy, the
adaptive ﬁlter will construct an approximation for the relation H. A typical example of an interference
cancellation problem is the echo cancellation example we gave in Section 1.12.1.1. In that example,
x(n) is the far-end voice signal, v(n) is the near-end voice signal, and y(n) is the echo. The relation H
is usually well approximated by a linear FIR ﬁlter with a few hundred taps.
The approximation for H is a by-product, that does not need to be very accurate as long as it leads to
a ˆy(n) close to y(n). This conﬁguration is shown in Figure 12.13 and is called interference cancellation,
since the interference y(n) should be canceled. Note that we do not want to make e(n) ≡0, otherwise
we would not only be killing the interference y(n), but also the signal of interest v(n).
There are many applications of interference cancellation. In addition to acoustic and line echo can-
cellation, we can mention, for example, adaptive notch ﬁlters for cancellation of sinusoidal interference
(a common application is removal of 50 or 60 Hz interference from the mains line), cancellation of
the maternal electrocardiography in fetal electrocardiography, cancellation of echoes in long distance
telephone circuits, active noise control (as in noise-canceling headphones), and active vibration control.

1.12.1 Introduction
643
adaptive
ﬁlter
d(n)
v(n)
ˆy(n)
e(n)
x(n)
y(n)
controller
controller
controller
plant
parameters
model parameters
design
reference
speciﬁcations
FIGURE 12.14
Scheme for plant identiﬁcation in a control system.
1.12.1.3.2
System identiﬁcation
In interference cancellation, the coefﬁcients of the adaptive ﬁlter converge to an approximation for the
true relation H, but as mentioned before, this approximation is a by-product that does not need to be
very accurate. However, there are some applications in which the goal is to construct an approximation
as accurate as possible for the unknown relation H between x(n) and y(n), thus obtaining a model
for the unknown system. This is called system identiﬁcation or modeling (the diagram in Figure 12.13
applies also for this case). The signal d(n) is now composed of the output y(n) of an unknown system
H, plus noise v(n). The reference signal x(n) is the input to the system which, when possible, is chosen
to be white noise. In general, this problem is harder than interference cancellation, as we shall see in
Section 1.12.2.1.5, due to some conditions that the reference signal x(n) must satisfy. However, one
point does not change: in the ideal case, the signal e(n) will be equal to the noise v(n). Again, we do
not want to make e(n) ≡0, otherwise we would be trying to model the noise (however, the smaller the
noise the easier the task, as one would expect).
In many control systems, an unknown dynamic system (also called as plant in control system termi-
nology) is identiﬁed online and the result is used in a self-tuning controller, as depicted in Figure 12.14.
Both the plant and the adaptive ﬁlter have the same input x(n). In practical situations, the plant to be
modeled is noisy, which is represented by the signal v(n) added to the plant output. The noise v(n) is
generally uncorrelated with the plant input. The task of the adaptive ﬁlter is to minimize the error model
e(n) and track the time variations in the dynamics of the plant. The model parameters are continually
fed back to the controller to obtain the controller parameters used in the self-tuning regulator loop [15].
There are many applications that require the identiﬁcation of an unknown system. In addition to
control systems, we can mention, for example, the identiﬁcation of the room impulse response, used
to study the sound quality in concert rooms, and the estimation of the communication channel impulse
response, required by maximum likelihood detectors and blind equalization techniques based on second-
order statistics.

644
CHAPTER 12 Adaptive Filters
1.12.1.3.3
Prediction
In prediction, the goal is to ﬁnd a relation between the current sample d(n) and previous samples
d(n −L), . . . , d(n −L −M + 1), as shown in Figure 12.15. Therefore, we want to model d(n) as a
part y(n) that depends only on d(n −L), . . . , d(n −L −M + 1), and a part v(n) that represents new
information. For example, for a linear model, we would have
d(n) =
M−1

k=0
hkd(n −L −k)



y(n)
+v(n),
Thus, we in fact have again the same problem as in Figure 12.13. The difference is that now the
reference signal is a delayed version of d(n) : x(n) = d(n −L), and the adaptive ﬁlter will try to
ﬁnd an approximation ˆy(n) for y(n), thereby separating the “predictable” part y(n) of d(n) from the
“new information” v(n), to which the signal e(n) = d(n) −ˆy(n) should converge. The system H in
Figure 12.13 now represents the relation between d(n −L) and the predictable part of d(n).
Prediction ﬁnds application in many ﬁelds. We can mention, for example, linear predictive coding
(LPC) and adaptive differential pulse code modulation (ADPCM) used in speech coding, adaptive line
enhancement, autoregressive spectral analysis, etc.
For example, adaptive line enhancement (ALE) seeks the solution for a classical detection problem,
whose objective is to separate a narrowband signal y(n) from a wideband signal v(n). This is the case,
for example, of ﬁnding a low-level sine wave (predictable narrowband signal) in noise (non-predictable
wideband signal). The signal d(n) is constituted by the sum of these two signals. Using d(n) as the
reference signal and a delayed replica of it, i.e., d(n−L), as input of the adaptive ﬁlter as in Figure 12.15,
the output ˆy(n) provides an estimate of the (predictable) narrowband signal y(n), while the error signal
e(n) provides an estimate of the wideband signal v(n). The delay L is also known as decorrelation
parameter of the ALE, since its main function is to remove the correlation between the wideband signal
v(n) present in the reference d(n) and in the delayed predictor input d(n −L).
adaptive
ﬁlter
d(n)
z −L
e(n)
x(n)
ˆy(n)
FIGURE 12.15
Scheme for prediction. x(n) = d(n −L) is a delayed version of d(n).

1.12.1 Introduction
645
adaptive
ﬁlter
channel
d(n)
z −L
η(n)
e(n)
s(n)
x(n)
ˆy(n)
H(z)
FIGURE 12.16
Simpliﬁed communications system with an adaptive equalizer in the training mode.
adaptive
ﬁlter
decision
device
d(n)=s(n−L)
e(n)
x(n)
ˆy(n)
ˆ
FIGURE 12.17
Adaptive equalizer in the decision-directed mode.
1.12.1.3.4
Inverse system identiﬁcation
Inverse system identiﬁcation, also known as deconvolution, has been widely used in different ﬁelds as
communications, acoustics, optics, image processing, control, among others. In communications, it is
also known as channel equalization and the adaptive ﬁlter is commonly called as equalizer. Adaptive
equalizers play an important role in digital communications systems, since they are used to mitigate the
inter-symbol interference (ISI) introduced by dispersive channels. Due to its importance, we focus on
the equalization application, which is explained in the sequel.
A simpliﬁed baseband communications system is depicted in Figure 12.16. The signal s(n) is trans-
mitted through an unknown channel, whose model is constituted by an FIR ﬁlter with transfer function
H(z) = h0 + h1z−1 + · · · + hK−1zK−1 and additive noise η(n). Due to the channel memory, the
signal at the receiver contains contributions not only from s(n), but also from the previous symbols
s(n −1), s(n −2), . . . , s(n −K + 1), i.e.,
x(n) =
L−1

k=0
hks(n −k)



pre-ISI
+hLs(n −L) +
K−1

k=L+1
hks(n −k)



post-ISI
+η(n).
(12.38)

646
CHAPTER 12 Adaptive Filters
adaptive
ﬁlter
d(n)
e(n)
x(n)
ˆy(n)
FIGURE 12.18
Inputs and output of an adaptive ﬁlter.
Assuming that the overall channel-equalizer system imposes a delay of L samples, the adaptive ﬁlter
will try to ﬁnd an approximation ˆy(n) for d(n) = s(n −L) and for this purpose, the two summations
in (12.38), which constitute the inter-symbol interference, must be mitigated. Of course, when you are
transmitting information the receiver will not have access to d(n −L). The ﬁlter will adapt during a
training phase, in which the transmitter sends a pre-agreed signal. You can see that in this case, the role
of x(n) and d(n) is the reverse of that in system identiﬁcation. In this case, you would indeed like to
have e(n) ≡0. The problem is that this is not possible, given the presence of noise in x(n). The role of
the adaptive ﬁlter is to approximately invert the effect of the channel, at the same time trying to suppress
the noise (or at least, not to amplify it too much). In one sense, we are back to the problem of separating
two signals, but now the mixture is in the reference signal x(n), so what can and what cannot be done
is considerably different from the other cases.
The scheme of Figure 12.16 is also called training mode, since the delayed version of the transmitted
sequence d(n) = s(n −L) (training sequence) is known at the receiver. After the convergence of the
ﬁlter, the signal d(n) is changed to the estimate ˆs(n −L) obtained at the output of a decision device, as
shown in Figure 12.17. In this case, the equalizer works in the so-called decision-directed mode. The
decision device depends on the signal constellation—for example, if s(n) = ±1, the decision device
returns +1 for ˆy(n) ≥0, and −1 for ˆy(n) < 0.
1.12.1.3.5
A common formulation
In all the four groups of applications, the inputs of the adaptive ﬁlter are given by the signals x(n) and
d(n) and the output by ˆy(n). Note that the input d(n) appears effectively in the signal e(n) = d(n)−ˆy(n),
which is computed and fed back at each time instant n, as shown in Figure 12.18. In the literature, the
signal d(n) is referred to as desired signal, x(n) as reference signal and e(n) as error signal. These
names unfortunately are somewhat misleading, since they give the impression that our goal is to recover
exactly d(n) by ﬁltering (possibly in a nonlinear way) the reference x(n). In almost all applications,
this is far from the truth, as previously discussed. In fact, except in the case of channel equalization,
exactly zeroing the error would result in very poor performance.
The only application in which d(n) is indeed a “desired signal” is channel equalization, in which
d(n) = s(n −L). Despite this particularity in channel equalization, the common feature in all adaptive
ﬁltering applications is that the ﬁlter must learn a relation between the reference x(n) and the desired

1.12.2 Optimum Filtering
647
signal d(n) (we will use this name to be consistent with the literature.) In the process of building this
relation, the adaptive ﬁlter is able to perform useful tasks, such as separating two mixed signals; or
recovering a distorted signal. Section 1.12.2 explains this idea in more detail.
1.12.2 Optimum ﬁltering
We now need to extend the ideas of Section 1.12.1.2, that applied only to periodic (deterministic) signals,
to more general classes of signals. For this, we will use tools from the theory of stochastic processes.
The main ideas are very similar: as before, we need to choose a structure for our ﬁlter and a means
of measuring how far or how near we are to the solution. This is done by choosing a convenient cost
function, whose minimum we must search iteratively, based only on measurable signals.
In the case of periodic signals, we saw that minimizing the error power (12.3), repeated below,
P =
K0

k=1

C2
k
2 +
K1

ℓ=1
B2
ℓ
2 ,
would be equivalent to zeroing the echo. However, we were not able to compute the error power
exactly—we used an approximation through a time-average, as in (12.5), repeated here:
P(n) = 1
N
N−1

k=0
e2(n −k),
where N is a convenient window length (in Section 1.12.1.2, we saw that choosing a large window length
is approximately equivalent to choosing N = 1 and a small step-size). Since we used an approximated
estimate of the cost, our solution was also an approximation: the estimated coefﬁcients did not converge
exactly to their optimum values, but instead hovered around the optimum (Figures 12.9 and 12.10).
The minimization of the time-average P(n) also works in the more general case in which the echo is
modeled as a random signal—but what is the corresponding exact cost function? The answer is given
by the property of ergodicity that some random signals possess. If a random signal s(n) is such that its
mean E{s(n)} does not depend on n and its autocorrelation E{s(n)s(k)} depends only on n −k, it is
called wide-sense stationary (WSS) [16]. If s(n) is also (mean-square) ergodic, then
Average power of s(n) = lim
N→∞
1
N + 1
N

n=0
s2(n) = E{s2(n)}.
(12.39)
Note that E{s2(n)} is an ensemble average, that is, the expected value is computed over all possible
realizations of the random signal. Relation (12.39) means that for an ergodic signal, the time average
of a single realization of the process is equal to the ensemble-average of all possible realizations of the
process. This ensemble average is the exact cost function that we would like to minimize.
In the case of adaptive ﬁlters, (12.39) holds approximately for e(n) for a ﬁnite value of N if the
environment does not change too fast, and if the ﬁlter adapts slowly. Therefore, for random variables we
will still use the average error power as a measure of how well the adaptive ﬁlter is doing. The difference

648
CHAPTER 12 Adaptive Filters
is that in the case of periodic signals, we could understand the effect of minimizing the average error
power in terms of the amplitudes of each harmonic in the signal, but now the interpretation will be in
terms of ensemble averages (variances).
Although the error power is not the only possible choice for cost function, it is useful to study this
choice in detail. Quadratic cost functions such as the error power have a number of properties that
make them popular. For example, they are differentiable, and so it is relatively easy to ﬁnd a closed-
form solution for the optimum, and in the important case where all signals are Gaussian, the optimum
ﬁlters are linear. Of course, quadratic cost functions are not the best option in all situations: the use
of other cost functions, or even of adaptive ﬁlters not based on the minimization of a cost function, is
becoming more common. We will talk about some of the most important alternatives in Sections 1.12.5.2
and 1.12.5.4.
Given the importance of quadratic cost functions, in this section we study them in detail. Our focus
is on what could and what could not be done if the ﬁlter were able to measure perfectly E{e2(n)} at each
instant. This discussion has two main goals: the ﬁrst is to learn what is feasible, so we do not expect more
from a ﬁlter than it can deliver. The second is to enable us to make more knowledgeable choices when
designing a ﬁlter—for example, which ﬁlter order should be used, and for system identiﬁcation, which
input signal would result in better performance. In Section 1.12.4.4.3 we will study the performance of
adaptive ﬁlters taking into consideration their imperfect knowledge of the environment.
1.12.2.1 Linear least-mean squares estimation
In the examples we gave in previous sections, a generic adaptive ﬁltering problem resumed to this: we
are given two sequences, {x(n)} and {d(n)}, as depicted in Figure 12.18. It is known, from physical
arguments, that d(n) has two parts, one that is in some sense related to x(n), x(n −1), . . ., and another
that is not, and that both parts are combined additively, that is,
d(n) = H(x(n), x(n −1), . . . )



y(n)
+v(n),
(12.40)
where H(·) is an unknown relation, and v(n) is the part “unrelated” to {x(n)}. In our examples so far,
H(·) was linear, but we do not need to restrict ourselves to this case. Our objective is to extract y(n) and
v(n) from d(n), based only on observations of {x(n)} and {d(n)}. We saw in Section 1.12.1.2 that the
average power of the difference e(n) between d(n) and our current approximation ˆy(n) could be used
as a measure of how close we are to the solution. In general, H(·) is time-variant, i.e., depends directly
on n. We will not write this dependence explicitly to simplify the notation.
In this section we ask in some sense the inverse problem, that is: given two sequences {x(n)} and
{d(n)}, what sort of relation will be found between them if we use E{e2(n)} as a standard? The difference
is that we now do not assume a model such as (12.40); instead, we want to know what kind of model
results from the exact minimization of E{e2(n)}, where e(n) is deﬁned as the difference between d(n)
and a function of x(n), x(n −1), . . .
We can answer this question in an entirely general way, without specifying beforehand the form of
H. This is done in Box 4. However, if we have information about the physics of a problem and know that

1.12.2 Optimum Filtering
649
the relation H(x(n), x(n −1), . . . ) is of a certain kind, we can restrict the search for the solution ˆH to
this class. This usually reduces the complexity of the ﬁlter and increases its convergence rate (because
less data is necessary to estimate a model with less unknowns, as we will see in Section 1.12.3). In this
section we focus on this second option.
The ﬁrst task is to describe the class F of allowed functions ˆH. This may be done by choosing
a relation that depends on a few parameters, such as (recall that the adaptive ﬁlter output is ˆy(n) =
ˆH(x(n), x(n −1), . . . ))
FFIR (FIR ﬁlter):
ˆy(n) = w0x(n) + · · · + wM−1x(n −M + 1),
(12.41)
FIIR (IIR ﬁlter):
ˆy(n) = −a1 ˆy(n −1) + b0x(n),
(12.42)
FV (Volterra ﬁlter):
ˆy(n) = w0x(n) + w1x(n −1) + w0,0x2(n)
+ w0,1x(n)x(n −1) + w1,1x(n −1)2,
(12.43)
FS (Saturation):
ˆy(n) = arctan(ax(n)).
(12.44)
In each of these cases, the relation between the input sequence {x(n)} and d(n) is constrained to a
certain class, for example, linear length-M FIR ﬁlters in (12.41), ﬁrst-order IIR ﬁlters in (12.42), and
second-order Volterra ﬁlters in (12.43). Each class is described by a certain number of parameters: the
ﬁlter coefﬁcients w0, . . . , wM−1 in the case of FIR ﬁlters, a1 and b0 in the case of IIR ﬁlters, and so on.
The task of the adaptive ﬁlter will then be to choose the values of the parameters that best ﬁt the data.
For several practical reasons, it is convenient if we make the ﬁlter output depend linearly on the
parameters, as happens in (12.41) and in (12.43). It is important to distinguish linear in the parameters
from input-output linear: (12.43) is linear in the parameters, but the relation between the input sequence
{x(n)} and the output ˆy(n) is nonlinear. What may come as a surprise is that the IIR ﬁlter of Eq. (12.42)
is not linear in the parameters: in fact, ˆy(n) = −a1 ˆy(n −1) + b0x(n) = a2
1 ˆy(n −2) −a1b0x(n −1) +
b0x(n) = · · ·— you can see that ˆy(n) depends nonlinearly on a1 and b0.
Linearly parametrized classes F, such as FFIR (12.41) and FV (12.43) are popular because in general
it is easier to ﬁnd the optimum parameters, both theoretically and in real time. In fact, when the ﬁlter
output depends linearly on the parameters and the cost function is a convex function of the error, it can
be shown that the optimal solution is unique (see Box 5). This is a very desirable property, since it
simpliﬁes the search for the optimal solution.
In the remainder of this section we will concentrate on classes of relations that are linear in the
parameters.AsFV shows,thisdoesnotimplythatwearerestrictingourselvestolinearmodels.Thereare,
however, adaptive ﬁltering algorithms that use classes of relations that are not linear in the parameters,
such as IIR adaptive ﬁlters. On the other hand, blind equalization algorithms are based on non-convex
cost functions.
Assume then that we have chosen a convenient class of relations F that depends linearly on its
parameters. That is, we want to solve the problem
min
ˆH∈F
E

d(n) −ˆH(x(n), x(n −1), . . . )
2
.
(12.45)

650
CHAPTER 12 Adaptive Filters
We want to know which properties the solution to this problem will have when F depends linearly
on a ﬁnite number of parameters. In this case, F is a linear combinations of certain functions φi of
x(n), x(n −1), . . .
ˆy(n) = w0φ0 + w1φ1 + · · · + wM−1φM−1
= wT φ,
(12.46)
where in general φi = φi(x(n), x(n −1), . . . ), 0 ≤i ≤M −1. The vector φ is known as regressor. In
the case of length-M FIR ﬁlters, we would have
φ0 = x(n), φ1 = x(n −1), . . . , φM−1 = x(n −M + 1),
whereas in the case of second-order Volterra ﬁlters with memory 1, we would have
φ0 = x(n), φ1 = x(n −1), φ2 = x2(n), φ3 = x(n)x(n −1), φ4 = x2(n −1).
Our problem can then be written in general terms as: ﬁnd wo such that
wo = arg min
w E{(d −wT φ)2}.
(12.47)
We omitted the dependence of the variables on n to lighten the notation. Note that, in general, wo will
also depend on n.
To solve this problem, we use the facts that wT φ = φT w to expand the expected value
J(w) = E

d −wT φ
 2
= E{d2} −2wT E{dφ} + wT E{φφT }w.
Recall that the weight vector w is not random, so we can take it out of the expectations.
Deﬁne the autocorrelation of d, and also the cross-correlation vector and autocorrelation matrix
rd = E{d2},
rdφ = E{dφ},
Rφ = E{φφT }.
(12.48)
The cost function then becomes
J(w) = rd −2wT rdφ + wT Rφw.
(12.49)
Differentiating J(w) with respect to w, we obtain
∂J
∂wT = −2rdφ + 2Rφw,
and equating the result to zero, we see that the optimal solution must satisfy
Rφwo = rdφ.
(12.50)
These are known as the normal, or Wiener-Hopf, equations. The solution, wo, is known as the Wiener
solution. A note of caution: the Wiener solution is not the same thing as the Wiener ﬁlter. The Wiener
ﬁlter is the linear ﬁlter that minimizes the mean-square error, without restriction of ﬁlter order [17]. The
difference is that the Wiener solution has the ﬁlter order pre-speciﬁed (and is not restricted to linear
ﬁlters, as we saw).

1.12.2 Optimum Filtering
651
When the autocorrelation matrix is non-singular (which is usually the case), the Wiener solution is
wo = R−1
φ rdφ.
(12.51)
Given wo, the optimum error will be
vo = d −wT
o φ.
(12.52)
Note that the expected value of vo is not necessarily zero:
E{vo} = E{d} −wT
o E{φ}.
(12.53)
If, for example, E{φ} = 0 and E{d} ̸= 0, then E{vo} = E{d} ̸= 0. In practice, it is good to keep
E{vo} = 0, because we usually know that vo should approximate a zero-mean signal, such as speech or
noise. We will show shortly, in Section 1.12.2.1.4, how to guarantee that vo has zero mean.
1.12.2.1.1
Orthogonality condition
A key property of the Wiener solution is that the optimum error is orthogonal to the regressor φ, that is,
E{voφ} = E{φ(d −wT
o φ

=φT wo
)} = E{dφ} −E{φφT }wo = rdφ −Rφwo = 0.
(12.54)
We saw a similar condition in Section 1.12.1.2. It is very useful to remember this result, known as
the orthogonality condition: from it, we will ﬁnd when to apply the cost function (12.47) to design an
adaptive ﬁlter. Note that when vo has zero mean, (12.54) also implies that vo and φ are uncorrelated.
Remark 1.
You should not confuse orthogonal with uncorrelated. Two random variables x and y are
orthogonal if E{xy} = 0, whereas they are uncorrelated if E{xy} = E{x} E{y}. The two concepts
coincide only if either x or y have zero mean.
Remark 2.
The orthogonality condition is an intuitive result, if we remember that we can think of
the space of random variables with ﬁnite variance as a vector space. In fact, deﬁne the cross-correlation
rxy = E{xy} between two random variables x and y as the inner product. Then the autocorrelation
of a random variable x, E{x2}, would be interpreted as the square of the “length” of x. In this case,
our problem (12.47) is equivalent to ﬁnding the vector in the subspace spanned by φ0, . . . , φM−1 that
is closest to d. As we know, the solution is such that the error is orthogonal to the subspace. So, the
orthogonality condition results from the vector space structure of our problem and the quadratic nature
of our cost function. See Figure 12.19.
Remark 3.
Thedifferencebetween(12.54)andthecorrespondingresultobtainedinBox4,Eq.(12.105),
is that here the optimum error is orthogonal only to the functions φi of x(n), x(n −1), . . . that were
included in the regressor (and their linear combinations), not to any function, as in (12.105). This is not
surprising: in this section the optimization was made only over the functions in class F, and in Box 4 we
allow any function. Both solutions, (12.106) and (12.51), will be equal if the general solution according
to Box 4 is indeed in F.

652
CHAPTER 12 Adaptive Filters
Remark 4.
The optimal mean-square error is (recall that E{voφ} = 0)
Jmin = rv,o = E{v2
o} = E{vo(d −wT
oφ)} = E{vod}
= E{(d −wT
o φ)d} = E{d2} −wT
o rdφ = rd −rT
dφ R−1
φ rdφ.
(12.55)
Now that we have the general solution to (12.47), we turn to the question: for which class of problems
is the quadratic cost function adequate?
1.12.2.1.2
Implicit vs. physical models
From (12.54), we see that the fact that we are minimizing the mean-square error between d and a linear
combination of the regressor φ induces a model
d = wT
o φ + vo,
(12.56)
in which vo is orthogonal to φ. This is not an assumption, as we sometimes see in the literature, but a
consequence of the quadratic cost function we chose. In other words, there is always a relation such as
(12.56) between any pair (d, φ), as long as both have ﬁnite second-order moments. This relation may
make sense from a physical analysis of the problem (as we saw in the echo cancellation example), or
be simply a consequence of solving (12.47) (see Section 1.12.2.1.3 for examples).
On the other hand, the orthogonality condition allows us to ﬁnd out when the solution of (12.47) will
be able to successfully solve a problem: assume now that we know beforehand, by physical arguments
d
φ 0
φ1
ˆy
FIGURE 12.19
Orthogonality in vector spaces with inner products.

1.12.2 Optimum Filtering
653
about the problem, that d and φ must be related through
d = wT
∗φ + v,
(12.57)
where w∗is a certain coefﬁcient vector and v is orthogonal to φ. Will the solution to (12.47) be such
that wo = w∗and vo = v?
To check if this is the case, we only need to evaluate the cross-correlation vector rdφ assuming the
model (12.57):
rdφ = E{dφ} = E{φ( wT
∗φ

=φT w∗
+v)} = E{φφT }w∗+ E{φv}
  
=0
= Rφw∗.
(12.58)
Therefore, w∗also obeys the normal equations, and we can conclude that wo = w∗, vo = v, as long as
Rφ is nonsingular.
Even if Rφ is singular, the optimal error vo will always be equal to v (in a certain sense). In fact, if Rφ
is singular, then there exists a vector a such that Rφa = 0 (see Fact 2 in Box 3), and thus any solution
wo of the normal equations (we know from (12.58) that there is at least one, w∗) will have the form
wo = w∗+ a, a ∈N(Rφ),
(12.59)
where N(Rφ) is the null-space of Rφ. In addition,
0 = aT Rφa = aT E{φφT }a = E{aT φφT a} = E

(aT φ)2
.
Therefore, aT φ = 0 with probability one. Then,
vo = d −wT
o φ = d −(w∗+ a)T φ = d −wT
∗φ,
with probability one. Therefore, although we cannot say that v = vo always, we can say that they are
equal with probability one. In other words, when Rφ is singular we may not be able to identify w∗,
but (with probability one) we are still able to separate d into its two components. More about this in
Section 1.12.2.1.5.
1.12.2.1.3
Undermodeling
We just saw that the solution to the quadratic cost function (12.47) is indeed able to separate the two
components of d when the regressor that appears in physical model (12.57) is the same φ that we used
to describe our class F, that is, when our choice of F includes the general solution from Box 4. Let us
check now what happens when this is not the case.
Assume there is a relation
d = wT
∗φe + v,
(12.60)
in which v is orthogonal to φe, but our class F is deﬁned through a regressor φ that is a subset of the
“correct” one, φe. This situation is called undermodeling: our class is not rich enough to describe the
true relation between {x(n)} and {d(n)}.

654
CHAPTER 12 Adaptive Filters
Assume then that
φe =
φ
θ

,
(12.61)
where φ ∈RM, θ ∈RK−M, with K > M. The autocorrelation of φe is
Rφe = E{φeφT
e } =
 Rφ Rφθ
RT
φθ Rθ

,
where Rφθ = E{φθ T }. From our hypothesis that φe is orthogonal to v, that is,
0 = E{vφe} =
E{vφ}
E{vθ}

,
we conclude that φ is also orthogonal to v, so
rdφ = E

φ(φT
e w∗+ v)

= E{φφT
e }w∗+ E{φv}
  
=0
=
 Rφ Rφθ

w∗.
Assuming that Rφ is nonsingular, solving the normal Eq. (12.50) we obtain
wo = R−1
φ
 Rφ Rφθ

w∗=

I R−1
φ Rφθ

w∗.
(12.62)
Now we have two interesting special cases: ﬁrst, if φ and θ are orthogonal, i.e., if Rφθ = 0, then wo
contains the ﬁrst M elements of w∗. Let us consider a speciﬁc example: assume that
φ =
⎡
⎢⎢⎢⎣
x(n)
x(n −1)
...
x(n −M + 1)
⎤
⎥⎥⎥⎦,
but φe =
⎡
⎢⎢⎢⎣
x(n)
x(n −1)
...
x(n −K + 1)
⎤
⎥⎥⎥⎦,
(12.63)
with M < K. Then θ =
 x(n −M) . . . x(n −K + 1) T . This situation is very common in practice.
In this case, Rφθ = 0 when {x(n)} is zero-mean white noise. This is one reason why white noise is
the preferred input to be used in system identiﬁcation: even in the (very likely in practice) case in which
M < K, at least the ﬁrst elements of w∗are identiﬁed without bias.
On the other hand, if Rφθ ̸= 0, then wo is a mixture of the elements of w∗. This happens in (12.63)
when the input sequence {x(n)} is not white. In this case, the optimum ﬁlter wo takes advantage of the
correlation between the entries of φ and θ to estimate θ given φ, and uses these estimated values to
obtain a better approximation for d. Sure enough, if you write down the problem of approximating θ
linearly from φ, namely,
Wo = arg min
W
E
!!!θ −W T φ
!!!
2
,
you will see that the solution is exactly Wo = R−1
φ Rφθ.

1.12.2 Optimum Filtering
655
What is important to notice is that in both cases the optimum error vo is not equal to v:
vo = d −wT
o φ = wT
∗φe + v −wT
o φ.
Partitioning w∗=
wT
∗,1 wT
∗,2
T , with w∗,1 ∈RM, w∗,2 ∈RK−M, we have
vo =

w∗,1 −wo
T φ + wT
∗,2θ + v = wT
∗,2 RT
φθ R−1
φ φ + wT
∗,2θ + v
= wT
∗,2

θ −RT
φθ R−1
φ φ
 
+ v.
Although vo ̸= v, you may check that indeed E{voφ} = 0.
As another example, assume that we have two variables such that d = ax + bx3 + v, in which v
has zero mean and is independent of x, and a and b are constants. Assume that we choose as regressor
simply φ = x, so we try to solve
wo = arg min
w
E

(d −wx)2
.
In this case we have
Rφ = E{x2},
rdφ = E{xd} = E{ax2 + bx4},
since E{xkv} = E{xk}E{v} = 0. The optimum solution is thus
wo = aE{x2} + bE{x4}
E{x2}
,
and the optimum error is
vo = d −wox = ax + bx3 + v −aE{x2} + bE{x4}
E{x2}
x = v + b
"
x2 −E{x4}
E{x2}
#
x ̸= v.
However, vo is again orthogonal to x (but not to x3, which we did not include in the regressor).
What happens in the opposite situation, when we include more entries in φ than necessary? As one
would expect, in this case the parameters related to these unnecessary entries will be zeroed in the
optimum solution wo. This does not mean, however, that we should hasten to increase φ to make F as
large as possible, and leave to the ﬁlter the task of zeroing whatever was not necessary. This is because
increasing the number of parameters to be estimated has a cost. The ﬁrst and more obvious cost is in
terms of memory and number of computations. However, we saw in Section 1.12.1.2 that an actual
adaptive ﬁlter usually does not converge to the exact optimum solution, but rather hovers around it.
Because of this, increasing the number of parameters may in fact decrease the quality of the solution.
We explain this in more detail in Section 1.12.4.
1.12.2.1.4
Zero and non-zero mean variables
Up to now we did not assume anything about the means of d and φ. If both d and φ have zero mean,
we can interpret all autocorrelations as variances or covariance matrices, according to the case. To see
this, assume that
E{d} = 0,
E{φ} = 0.
(12.64)

656
CHAPTER 12 Adaptive Filters
Then E{ ˆy} = E{wT
o φ} = 0, and thus
E{vo} = E{d} −wT
o E{φ} = 0 + wT
o 0 = 0,
so the optimum error has zero mean. In this case,rd = E{d2} = σ 2
d is the variance of d, and E{v2
o} = σ 2
v,o
is the variance of vo, so (12.55) becomes
Jmin = σ 2
v,o = E{v2
o} = σ 2
d −rT
dφ R−1
φ rdφ.
(12.65)
Since Rφ is positive-deﬁnite, we conclude that the uncertainty in vo (its variance) is never larger than
the uncertainty in d (Box 3).
However, vo may have a nonzero mean if either d or φ has nonzero mean. This is veriﬁed as follows.
Deﬁne
¯d = E{d},
¯φ = E{φ},
(12.66)
then
E{vo} = E{d −rT
dφ R−1
φ φ} = ¯d −rT
dφ R−1
φ ¯φ ̸= 0
in general (for example, if ¯φ = 0, but ¯d ̸= 0).
In many applications, vo should approximate some signal that is constrained to have zero mean, such
as speech or noise, or because vo will be passed through a system with a high DC gain (such as an
integrator), so it may be useful to guarantee that vo has zero mean. How can this be done if the means of
d and φ are not zero? This problem is quite common, particularly when φ includes nonlinear functions
of x(n). Fortunately, there are two easy solutions: First, if one knew the means of d and φ, one could
deﬁne zero-mean variables
dc = d −E{d},
φc = φ −E{φ},
(12.67)
and use them instead of the original variables in (12.47). This is the simplest solution, but we often do
not know the means ¯d and ¯φ.
The second solution is used when the means are not known: simply add an extra term to φ, deﬁning
an extended regressor (not to be confused with the extended regressor from Section 1.12.2.1.3)
φe =
 1
φ

,
(12.68)
and an extended weight vector we ∈RM+1. The ﬁrst coefﬁcient of we will take care of the means, as
we show next. The extended cross-correlation vector and autocorrelation matrix are
rdφe =

¯d
rdφ

,
Rφe =

1
¯φT
¯φ
Rφ
	
.

1.12.2 Optimum Filtering
657
Assuming that Rφe and Rφ are nonsingular, we can compute wo,e if we evaluate the inverse of Rφe
using Schur complements (See Fact 3 in Box 3), as follows
wo,e = R−1
φe rdφe
=

1
0T
−R−1
φ ¯φ I
 
(1 −¯φT R−1
φ ¯φ)−1 0T
0
R−1
φ
	 
1 −¯φT R−1
φ
0
I
	 
¯d
rdφ

=

1
0T
−R−1
φ ¯φ I
 
(1 −¯φT R−1
φ ¯φ)−1 0T
0
R−1
φ
	 
¯d −¯φT R−1
φ rdφ
rdφ
	
=

1
0T
−R−1
φ ¯φ I

⎡
⎢⎣
¯d−¯φT R−1
φ rdφ
1−¯φT R−1
φ ¯φ
R−1
φ rdφ
⎤
⎥⎦=
⎡
⎢⎢⎣
¯d−¯φT R−1
φ rdφ
1−¯φT R−1
φ ¯φ
R−1
φ rdφ −
¯d−¯φT R−1
φ rdφ
1−¯φT R−1
φ ¯φ R−1
φ ¯φ
⎤
⎥⎥⎦.
Using this result, we can evaluate now E{vo,e} = E{d} −wT
o,eE{φe} as follows (we used the fact that
the inverse of a symmetric matrix is also symmetric from Box 3):
E{vo,e} = ¯d −
¯d −¯φT R−1
φ rdφ
1 −¯φT R−1
φ ¯φ
−rT
dφ R−1
φ ¯φ +
¯d −¯φT R−1
φ rdφ
1 −¯φT R−1
φ ¯φ
¯φT R−1
φ ¯φ
= ¯d −rT
dφ R−1
φ ¯φ −
¯d −¯φT R−1
φ rdφ
1 −¯φT R−1
φ ¯φ

1 −¯φT R−1
φ ¯φ
 
= 0.
1.12.2.1.5
Sufﬁciently rich signals
We saw in Section 1.12.2.1.2 that even when Rφ is singular, we can recover the optimum vo (with
probability one), but not the optimum w∗. Let us study this problem in more detail, since it is important
for system identiﬁcation. So, what does it mean in practice to have a singular autocorrelation function
Rφ? The answer to this question can be quite complicated when the input signals are nonstationary
(i.e., when Rφ is not constant), so in this case we refer the interested reader to texts in adaptive control
(such as [18]), which treat this problem in detail (although usually from a deterministic point of view).
We consider here only the case in which the signals {d(n), x(n)} are jointly wide-sense stationary.
In this case, Rφ may be singular because some entries of φ are linear combinations of the others.
For example, if we choose φ0 = x(n) and φ1 = 2x(n), then
Rφ = E{x2(n)}
 1 2
2 4

,
which is singular.
However, there are more subtle ways in which Rφ may become singular. Assume for example that
our input sequence is given by
x(n) = cos (ω0n + ϕ),
(12.69)

658
CHAPTER 12 Adaptive Filters
where 0 < ω0 ≤π is the frequency, and ϕ is a random phase, uniformly distributed in the interval
[0, 2π). This kind of model (sinusoidal signals with random phases), by the way, is the bridge between
the periodic signals from Section 1.12.1.2 and the stochastic models discussed in this section. In this
case, consider a vector φ formed from a delay line with x(n) as input,
φ = [ x(n) x(n −1) . . . x(n −M + 1)]T .
If M = 2, we have
Rφ =

E{x2(n)}
E{x(n)x(n −1)}
E{x(n)x(n −1)}
E{x2(n −1)}

= 1
2

1
cos (ω0)
cos (ω0)
1

.
The determinant of Rφ is 1 −cos2 (ω0) = sin2 (ω0) ̸= 0 if ω0 ̸= 0 and ω0 ̸= π.
However, for any M ≥3, the resulting Rφ will be singular: we can write
φ = 1
2e j(ω0n+ϕ)
⎡
⎢⎢⎢⎣
1
e−jω0
...
e−j(M−1)ω0
⎤
⎥⎥⎥⎦+ 1
2e−j(ω0n+ϕ)
⎡
⎢⎢⎢⎣
1
e jω0
...
e j(M−1)ω0
⎤
⎥⎥⎥⎦,
and since E{(e jω0n+ jϕ)2} = E{e j2(ω0n+ϕ)} = 0, E{e j(ω0n+ϕ)e−j(ω0n+ϕ)} = 1, we have
Rφ = 1
4
⎡
⎢⎢⎢⎣
1
e−jω0
...
e−j(M−1)ω0
⎤
⎥⎥⎥⎦

1 e jω0 . . . e j(M−1)ω0 
+1
4
⎡
⎢⎢⎢⎣
1
e jω0
...
e j(M−1)ω0
⎤
⎥⎥⎥⎦

1 e−jω0 . . . e−j(M−1)ω0 
.
Since Rφ is the sum of two rank-one matrices, its rank is at most two. It will be two when 0 < ω0 < π,
sinceinthiscasethevectorsarelinearlyindependent.Weconcludethatwhentheinputsignalisasinusoid
as in (12.69), the optimum solution to (12.47) will never be able to identify more than two coefﬁcients.
This argument can be generalized: if x(n) is composed of K sinusoids of different frequencies, then
Rφ will be nonsingular if and only if M ≤2K.
The general conclusion is that Rφ will be singular or not, depending on which functions are chosen
as inputs, and on the input signal. We say that the input is sufﬁciently rich for a given problem if it is
such that Rφ is nonsingular. We show a few examples next.
1.12.2.1.6
Examples
The concepts we saw in Sections 1.12.2.1.2–1.12.2.1.5 can be understood intuitively if we return to our
example in Section 1.12.1.2. In that example, the true echo was obtained by ﬁltering the input x(n) by

1.12.2 Optimum Filtering
659
an unknown H(z), so we had
d(n) =
K

k=0
hkx(n −k) + v(n),
where K is the true order of the echo path, v(n) = B cos (ω1n + θ) and x(n) = A cos (ω0n + ϕ). We
considered only one harmonic for simplicity.
We are trying to cancel the echo, by constructing an approximated transfer function 
H(z), which is
modeled as an FIR ﬁlter with M coefﬁcients. As we saw in Section 1.12.1.2, if x(n) is a simple sinusoid,
the echo will be completely canceled if we satisfy (12.8), reproduced below
Re{ 
H(e jω0)} = Re{H(e jω0)},
Im{ 
H(e jω0)} = Im{H(e jω0)}.
If K = M = 1, so H(z) = h0 + h1z−1 and 
H(z) = w0 + w1z−1, this condition becomes
w0 + w1 cos (−ω0) = h0 + h1 cos (−ω0),
w1 sin (−ω0) = h1 sin (−ω0),
with a single solution w0 = h0, w1 = h1.
On the other hand, if the input were as before (a single sinusoid), but K = 2, so H(z) = h0 +
h1z−1 + h2z−2, the equations would be
w0 + w1 cos (−ω0) = h0 + h1 cos (−ω0) + h2 cos (−2ω0),
w1 sin (−ω0) = h1 sin (−ω0) + h2 sin (−2ω0),
and the solution would be unique (since Rφ is nonsingular), but would not approximate h0 and h1 in
general.
This is an example of an undermodeled system, as we saw in Section 1.12.2.1.3. Figure 12.20
shows the LMS algorithm derived in Section 1.12.1.2 applied to this problem. Note that, even though
w0(n), w1(n) do not converge to h0, h1, in this example the error e(n) does approximate well v(n). This
happens because in this case x(n) is periodic, and the term x(n −2) can be obtained exactly as a linear
combination of x(n) and x(n −1) (Check that using the expressions for v0 from Section 1.12.2.1.3, we
obtain v0 = v in this case.)
If we tried to identify H(z) by adding an extra coefﬁcient to 
H(z), the equations would be
w0 + w1 cos (−ω0) + w2 cos (−2ω0) = h0 + h1 cos (−ω0) + h2 cos (−2ω0),
w1 sin (−ω0) + w2 sin (−2ω0) = h1 sin (−ω0) + h2 sin (−2ω0).
(12.70)
Now w0 = h0, w1 = h1, and w2 = h2 is a solution, but not the only one. Depending on the initial
condition, the ﬁlter would converge to a different solution, as can be seen in Figure 12.21 (the level
sets are not shown, because they are not closed curves in this case). The input is not rich enough (in
harmonics) to excite the unknown system H(z) so that the adaptive ﬁlter might identify it, as we saw
in Section 1.12.2.1.5. However, for all solutions the error converges to v(n).
If the input had a second harmonic (K0 = 2), we would add two more equations to (12.70), cor-
responding to 
H(e2 jω0) = H(e2 jω0), and the ﬁlter would be able to identify systems with up to four
coefﬁcients.

660
CHAPTER 12 Adaptive Filters
−1
−0.5
0
0.5
1
1.5
2
−1
−0.5
0
0.5
1
1.5
2
w0(n)
w1(n)
(a) LMS trajectory in (w0,w1)-space.The diamond
        marks the true value of the ﬁrst two terms in H(z).
0
50
100
150
−1
−0.5
0
0.5
1
1.5
2
n
e(n)
(b) Output error
FIGURE 12.20
LMS in undermodeled case.
−1
−0.5
0
0.5
1
1.5
2
−1
−0.5
0
0.5
1
1.5
2
w0(n)
w1(n)
(a) LMS trajectory in (w0,w1)-space.The diamond
        marks the true value of the ﬁrst two terms in H(z).
0
50
100
150
−1.5
−1
−0.5
0
0.5
1
1.5
n
(b) Output error
FIGURE 12.21
LMS under non-sufﬁciently rich input.
The important message is that the adaptive ﬁlter only “sees” the error d(n) −ˆy(n). If a mismatch
between the true echo path H(z) and the estimated one 
H(z) is not observable through looking only at
the error signal, the ﬁlter will not converge to H(z). Whether a mismatch is observable or not through
the error depends on both H(z) and the input signal being used: the input must excite a large enough

1.12.2 Optimum Filtering
661
number of frequencies so that H(z) can be estimated correctly. For this reason, in system identiﬁcation
a good input would be white noise.
1.12.2.2 Complex variables and multi-channel ﬁltering
Adaptiveﬁltersusedinimportantapplicationssuchascommunicationsandradarhavecomplexvariables
as input signals. In this section, we extend the results of Section 1.12.2.1 to consider complex signals.
We start with the general solution, then restrict the result to the more usual case of circular signals.
We ﬁrst show that the general solution for complex signals is equivalent to an adaptive ﬁlter with two
inputs and two outputs: Indeed, if all input variables are complex, then the error e(n) = d(n) −ˆy(n)
will also be complex, that is, e(n) = er(n) + jei(n), where er(n), ei(n) ∈R are the real and imaginary
parts of e(n). We must then minimize E{|e(n)|2} = E{e2
r (n)}+E{e2
i (n)}, the total power of the complex
signal e(n). The quadratic cost function (12.45) must be changed to
J(w) = E{|e(n)|2} = E{e(n)e∗(n)} = E{e2
r (n)} + E{e2
i (n)},
where e∗(n) is the complex conjugate of e(n). Now, we need to be careful about what exactly is
the coefﬁcient vector w in this case: If the regressor is φ = φr + jφi and d = dr + jdi, in principle
ˆy = ˆyr + j ˆyi should be such that both its real and imaginary parts depend on both the real and imaginary
parts of φ, that is,
ˆyr = wT
rrφr + wT
riφi,
ˆyi = wT
irφr + wT
ii φi.
(12.71)
Note that there is no a priori reason for us to imagine that there should be a special relation between
the pairs (wrr, wri) and (wir, wii). That is, we are dealing with two different coefﬁcient vectors and one
extended regressor:
wr =
 wrr
wri

,
wi =
wir
wii

,
θ =
 φr
φi

.
(12.72)
Let us proceed for the time being separating the real and imaginary channels of our ﬁlter. Later, in
Section 1.12.2.2.1, we return to complex algebra. With these deﬁnitions, we can write
ˆyr = wT
r θ,
ˆyi = wT
i θ.
(12.73)
The cost function then becomes
J(wr, wi) = E{(dr −wT
r θ)2} + E{(di −wT
i θ)2}
= E{d2
r −2wT
r θdr + wT
r θθ T wr} + E{d2
i −2wT
i θdi + wT
i θθ T wi}
= rdr −2wT
r rdrθ + wT
r Rθwr + rdi −2wT
i rdiθ + wT
i Rθwi,
(12.74)
where the autocorrelation matrix and cross-correlation vectors are given by
Rθ =
 Rr Rri
Rir Ri

,
rdrθ =
 rrr
rri

,
rdiθ =
 rir
rii

,
(12.75)

662
CHAPTER 12 Adaptive Filters
where we simpliﬁed the notation, writing Rr instead of Rφr, Rri instead of Rφrφi, rrr instead of rdrφr,
and so on.
Differentiating (12.74) to ﬁnd the minimum, we obtain
∂J
∂wTr
= −2rdrθ + 2Rθwr,
∂J
∂wT
i
= −2rdiθ + 2Rθwi.
(12.76)
Therefore, the optimum ﬁlters satisfy
 Rr Rri
Rir Ri
  wrr,o
wri,o

=
 rrr
rri

,
 Rr Rri
Rir Ri
  wir,o
wii,o

=
 rir
rii

.
(12.77)
The optimum error is then
vr,o = dr −
 wT
rr,o wT
ri,o

θ,
vi,o = di −
wT
ir,o wT
ii,o

θ.
(12.78)
There is also an orthogonality condition for this case: using (12.77), we obtain
E{vr,oθ} = E{drθ} −E{θθ T }
wrr,o
wri,o

= 0,
(12.79a)
E{vi,oθ} = E{diθ} −E{θθ T }
 wir,o
wii,o

= 0.
(12.79b)
The value of the cost function at the minimum can be obtained using this result. As in the case of real
variables,
E{v2
r,o} = E
$
vr,o

dr −
wT
rr,o wT
ri,o

θ
%
= E{vr,odr} = E

d2
r

−
wT
rr,o wT
ri,o

E{drθ},
where we used (12.79a) in the second equality. Doing similarly for vi,o, we obtain
rvr,o = E{v2
r,o} = rdr −
wT
rr,o wT
ri,o

rdrθ,
(12.80a)
rvi,o = E{v2
i,o} = rdi −
 wT
ir,o wT
ii,o

rdiθ,
(12.80b)
Jmin = rvr,o + rvi,o.
(12.80c)
What we have just described is a two-input/two-output optimal ﬁlter. In the next section, we re-derive
it using complex algebra. Note that multi-channel ﬁlters have applications that are unrelated to complex
variables, particularly in control [18], stereo echo cancellation [19,20], and active noise control [21–24].
1.12.2.2.1
Widely-linear complex least-mean squares
This general solution can be obtained in a more straightforward way if we use the complex gradients
described in Box 6. Keeping the regressor θ as in (12.72) and deﬁning the extended complex weight
vector as ω = wr + jwi, we can write the cost function as
J(ω) = E

(d −ωHθ)(d −ωHθ)∗
= rd −ωH rdθ −r H
dθω + ωH Rθω,

1.12.2 Optimum Filtering
663
where we deﬁned rd = E{|d|2} and rdθ = E{d∗θ}. Note that (·)H represents the Hermitian transpose,
that is, transpose followed by conjugation. Differentiating J(ω) with respect to ωH, as described in
Box 6, we obtain
∂J
∂ωH = −rdθ + Rθω.
(12.81)
The normal equations then become
Rθωo = rdθ.
(12.82)
Expanding the real and imaginary parts of (12.82), we recover (12.77). Similarly, the optimum error
and the minimum value of the cost are
νo = d −ωH
o θ,
Jmin = E{|νo|2} = E{|d|2} −ωH
o rdθ.
(12.83)
Comparing with (12.80), we see that νo = vr,o + jvi,o, and the value of Jmin is the same as before.
However, as you noticed, using a complex weight vector ω and the complex derivatives of Box 6, all
expressions are much simpler to derive.
The ﬁlter using a complex weight vector ω and the real regressor θ constitutes a version of what
is known as widely-linear complex ﬁlter. This version was recently proposed in [25] as a reduced-
complexity alternative to the widely-linear complex ﬁlter originally proposed in [26,27], which uses as
regressor the extended vector
φe =
 φ
φ∗

,
(12.84)
composed of the original regressor φ and its complex conjugate φ∗(computational complexity is reduced
because the number of operations required to compute ωHθ is about half of what is necessary to evaluate
ωHφe). Widely-linear estimation has been receiving much attention lately, due to new applications that
have appeared [28–31].
But why the name widely linear? The reason for this name is that the complex regressor φ does not
appear linearly in the expressions, but only by separating its real and imaginary parts (as in θ) or by
including its complex conjugate (as in φe). Both these expressions, from the point of view of a complex
variable, are nonlinear.
What would be a linear complex ﬁlter, then? This is the subject of the next section.
1.12.2.2.2
Linear complex least-mean squares
There are many applications in which the data and regressor are such that their real and imaginary parts
are similarly distributed, so that
Rr = Ri,
rdr = rdi,
Rri = −Rir,
rrr = rii,
rri = −rir.
(12.85)
Under these conditions, we say that the pair (d, φ) is complex circular, or simply circular.
In this case, we can rewrite (12.77) as
 Rr
Rri
−Rri Rr
  wrr
wri

=
 rrr
rri

,
 Rr
Rri
−Rri Rr
  wir
wii

=
−rri
rrr

.

664
CHAPTER 12 Adaptive Filters
Expand these equations as follows (notice the change in order in (12.86b))
Rrwrr + Rriwri = rrr,
−Rriwrr + Rrwri = rri,
(12.86a)
Rrwii −Rriwir = rrr,
Rriwii + Rrwir = −rri.
(12.86b)
Now we can see that, if (wrr, wri) is a solution to (12.86a), then
(wii = wrr, wir = −wri)
(12.87)
is a solution to (12.86b). Therefore, when the input signals are circular, the number of degrees of freedom
in the problem is actually smaller. In this case, there is a very nice way of recovering the solutions,
working only with complex algebra. Indeed, deﬁning w = wrr + jwri, we have
wHφ = (wrr −jwri)T (φr + jφi)
=

wT
rrφr + wT
riφi
 
+ j

wT
rrφi −wT
riφr
 
,
(12.88)
which is equivalent to (12.71) with the identities (12.87). Using this deﬁnition, it is possible to obtain
the optimum solution (12.86a), (12.86b), and (12.87) using only complex algebra, following exactly
the same steps as our derivation for the real case in the previous section.
This path is very useful, we can derive almost all results for real or complex circular adaptive ﬁlters
using the same arguments. Let us see how it goes. First, deﬁne ˆy = wHφ, so our problem becomes
min
w∈CM E
d −wHφ

2
.
(12.89)
Expanding the cost, recalling that now
d −wHφ
2 = (d −wHφ)(d −wHφ)∗= (d −wHφ)(d∗−
φHw), we obtain
J(w) = E

|d|2 −wHφd∗−dφHw + wHφφHw

= rd −wH rdφ −r H
dφw + wH Rφw,
(12.90)
where we deﬁned rd = E{|d|2}, rdφ = E{d∗φ}, Rφ = E{φφH}.
We need to ﬁnd the minimum of this cost.One easy solution is to use the rules for differentiation of
real functions of complex variables, as described in Box 6. These rules are very useful, and surprisingly
easy to use: basically, we can treat w and its conjugate wH as if they were independent variables, that is
∂J
∂wH = −rdφ + Rφw.
(12.91)
Equating the gradient to zero, the solution is (assuming that Rφ is nonsingular)
wo = R−1
φ rdφ,
(12.92)
which is exactly equal to (12.51). This is the advantage of this approach: the expressions for complex
and real problems are almost equal. The important differences are the conjugates that appear in the

1.12.2 Optimum Filtering
665
deﬁnitions of Rφ and rdφ, and the absence of a factor of 2 in the gradient (12.91). This follows from
the deﬁnition of complex derivative we used (see Box 6). Here this difference is canceled out in the
solution (12.92), but further on we will ﬁnd situations in which there will be a different factor for the
case of real and for the case of complex variables.
Let us check that (12.92) is really equivalent to (12.86a). Expanding Rφ and rdφ and using the
circularity conditions (12.85), we obtain
Rφ = E{φφH} = E{(φr + jφi)(φr −jφi)T } = Rr + Ri + j

Rir −Rri

,
(12.93a)
= 2Rr −2 j Rri,
(12.93b)
and
rdφ = E{d∗φ} = E{(dr −jdi)(φr + jφi)} = rrr + rii + j

rri −rir

= 2rrr + 2 jrri,
(12.94)
so, equating the gradient (12.91) to zero, wo = wr,o + jwi,o must satisfy
2

Rr −j Rri
 
wr,o + jwi,o

= 2

rrr + jrri

,
Rrwr,o + Rriwi,o + j

Rrwi,o −Rriwr,o

= rrr + jrri.
(12.95)
Comparing the real and imaginary parts of (12.95) with (12.86a), we see that they indeed correspond
to the same set of equations.
A few important remarks:
Remark 5.
The orthogonality condition still holds in the complex circular case, but with a small
difference: deﬁning the optimum error as before,
vo = d −wH
o φ,
we obtain, noting that (wH
o φ)∗= φHwo,
E{v∗
oφ} = E{d∗φ −φφHwo} = rdφ −Rφwo = 0.
(12.96)
Remark 6.
The complex gradient (12.91) seems strange because it does not include the factors of 2
that would appear in the real case. This difference will appear in many expressions in the following
sections, whenever we need to differentiate between the real and complex cases. However, the difference
is illusory: if you go back to (12.93b) and (12.94), you will see that the factor of two is actually there,
just hidden by the deﬁnition of autocorrelation and cross-correlation under the circularity conditions.
Remark 7.
The optimum error power can be obtained as in the real case, and is equal to
E{|vo|2} = rd −wH
o rdφ = rd −r H
dφ R−1
φ rdφ.
(12.97)

666
CHAPTER 12 Adaptive Filters
Box 4: Least mean-squares estimation
Here we consider the general problem of ﬁnding the optimum relation ˆHo between d(n) and x(n),
such that
ˆHo = arg min
ˆH
E

d(n) −ˆH(x(n), x(n −1), . . . )

,
(12.98)
without restricting H to a particular class.
In order to understand well this problem, let us consider ﬁrst the simpler case where the relation
between x(n) and d(n) is memoryless, that is, our function depends only on the current value of x(n).
Dropping the time index to simplify the notation, our problem then becomes how to ﬁnd a function
ˆH that, applied to a certain random variable x, approximates a certain random variable d so that the
mean-square error is minimized, that is, we want to solve
min
ˆH
E{[d −ˆH(x)]2}.
(12.99)
Assume that the joint probability density function pdx(d, x) of d and x is known (we are assuming
that d and x are continuous random variables, but the ﬁnal result in (12.100) also holds if they are
discrete). Then,
E{[d −ˆH(x)]2} =
& &
[d −ˆH(x)]2 pdx(d, x)d d dx
=
& &
[d −ˆH(x)]2 pd|x(d|x)d d




Ed{[d−ˆH(x)]2|x}
px(x)dx
= Ex

Ed

[d −ˆH(x)]2|x

,
(12.100)
where we use the subscripts Ed and Ex to highlight that the expectations are taken with respect to d or
x only.
Note that the inner integrand Ed{[d−ˆH(x)]2|x} in (12.100) is nonnegative. Therefore, if we minimize
it for each value of x, we will obtain the minimum of the full expression. That is, the optimum ˆHo is
the function deﬁned by
ˆHo(x) = arg min
ˆH(x)
Ed

[d −ˆH(x)]2|x

.
(12.101)
It is important to remember that, ﬁxed x, ˆH(x) is simply a number. This can be seen more easily
expanding the expectations in (12.101)
Ed

[d −ˆH(x)]2|x

=
& 
d2 −2d ˆH(x) + ˆH2(x)
 
pd|x(d|x)dd
=
&
d2 pd|x(d|x)dd −2 ˆH(x)
&
dpd|x(d|x)dd + ˆH2(x)
&
pd|x(d|x)dd
= Ed{d2|x} −2E{d|x} ˆH(x) + ˆH2(x).

1.12.2 Optimum Filtering
667
Differentiating with respect to ˆH(x) and equating to zero, we obtain the solution
ˆHo(x) = Ed{d|x} = ˆy(n),
(12.102)
which is indeed a function of x, as desired.
Let us study the properties of this solution. First, deﬁne
vo = d −ˆHo(x).
Note that vo and ˆHo are not necessarily equal to v and H from (12.40). Evaluating the average of ˆHo(x),
we obtain
Ex{ ˆHo(x)} = Ex {Ed{d|x}} = E{d},
by an argument similar to that used to obtain (12.100). We conclude that vo has zero mean. In addition,
for any function f (x) of x for which the expected values exist, we have
E{vo f (x)} = Ex {Ed{d f (x)}} −E {Ed{d|x} f (x)} = 0 = E{vo}E{ f (x)},
(12.103)
since Ed{d f (x)|x} = f (x)Ed{d|x}. This result means that the error vo is uncorrelated to any function
of the reference data x, which is a very strong condition. It basically means that we have extracted all
second-order information available in x about d.
We remark that (12.103) does not imply that x and vo are independent. A simple counter-example is
the following: consider two discrete-valued random variables r and s with joint probabilities given by
Table 12.1. Then E{s f (r)} = 1/4 · (−1) · f ( + 1) + 1/4 · ( + 1) · f ( + 1) + 1/2 · 0 · f (−1) = 0, ∀f (·),
but r and s are not independent.
In the more general case in which ˆH is allowed to depend on previous samples of x(n), the arguments
and the ﬁnal conclusion are similar:
arg min
ˆH
E

d −ˆH(x(n), x(n −1), . . . )
2
= E {d| x(n), x(n −1), . . .} ,
(12.104)
and deﬁning
vo(n) = d(n) −E {d | x(n), x(n −1), . . .} ,
Table 12.1 Counter-Example: (12.103) does not Imply Independence
r \ s
−1
0
1
−1
0
1/2
0
1
1/4
0
1/4

668
CHAPTER 12 Adaptive Filters
it holds that vo(n) has zero mean and is uncorrelated with any function of x(n), x(n −1), . . .:
E{vo(n)} = 0,
E {vo(n) f (x(n), x(n −1), . . .} = 0.
(12.105)
Note that, in general, the optimal solution of (12.104) depends on n, so we should write
ˆHo,n(x(n), x(n −1), . . . ) = E {d| x(n), x(n −1), . . .} ,
(12.106)
to make the time dependence explicit.
We can now return to the question at the start of this section: From our results thus far, we see that
minimizing E{e2(n)} leads to a model which relates d(n) and the sequence {x(n)} such that
d(n) = ˆHo,n(x(n), x(n −1), . . . ) + vo(n),
in which vo(n) is uncorrelated with any function f (·) of x(n), x(n −1), . . . In other words, the solution
of (12.104) imposes such a model on our data, even if this model makes no physical sense at all. This is
an important point: such a model will always result from the minimization of the mean-square error, at
least as long as the statistics of d(n) and {x(n)} are such that the expected values in (12.104) are ﬁnite.
Now, what if we knew beforehand, by physical arguments about the data, that a model such as (12.40)
holds between d(n) and {x(n)}, such that v(n) has zero mean and is uncorrelated to any function of
x(n), x(n −1), . . .? Will it result that the solution of (12.104) will be such that
ˆHo(x(n), x(n −1), . . . ) = H(x(n), x(n −1), . . . ),
vo(n) = v(n)?
To answer this question, let us go back to (12.99), and use (12.40) to ﬁnd the solution. We thus have
(we omit the arguments of d(n), v(n) and H(·) for simplicity)
min
ˆH
E{[d −ˆH(x)]2} = min
ˆH
E

[Hn + v −ˆH]2
.
Since v is orthogonal to any function of x(n), x(n −1), . . . by assumption, it will be orthogonal to Hn
and ˆH in particular. Therefore, we can simplify the cost as
min
ˆH
E{[d −ˆH(x)]2} = min
ˆH

E

(H −ˆH)2
+ E{v2}

.
(12.107)
Both terms in (12.107) are positive, and only the ﬁrst depends on ˆH. Therefore, the solution is indeed
ˆH = H and vo = v, as one would wish (to be precise, H and ˆH could differ on a set of probability
zero).
We conclude that we can use (12.102) as a criterion for separating the two parts y(n) and v(n) of
our model for d(n) if and only if v(n) satisﬁes (12.105). This holds, for example, if v(n) has zero mean
and is independent of x(n −k) for all k ≥0.

1.12.2 Optimum Filtering
669
s1
s2
λs1+ (1−λs2)
(a) Convex set
s1
s2
λs1+ (1−λs2)
(b) Non-convex set
FIGURE 12.22
Convexity.
Note that since ˆy(n) = ˆHo,n(x(n), x(n −1), . . . ) is itself a function of x(n), x(n −1), . . ., (12.105)
implies that
E{vo(n) ˆy(n)} = 0,
and we have
E{v2
o(n)} = E{vo[d(n) −ˆy(n)]} = E{vod(n)}
= E{[d(n) −ˆy(n)]d(n)}
= E{d2(n)} −E{d(n) ˆy(n)}
= E{d2(n)} −E{[ ˆy(n) + vo(n)] ˆy(n)}
= E{d2(n)} −E{ ˆy2(n)}.
Recalling that E{ ˆy(n)} = E{d(n)}, if we add and subtract E2{d(n)} to the last result, we conclude that
E{v2
o(n)} = σ 2
d −σ 2
ˆy ≤σ 2
d ,
(12.108)
where σ 2
d and σ 2
ˆy are the variances of d(n) and of ˆy(n).
The solution we just found is very general (it assumes little about H), and there are ways of com-
puting it adaptively from the data, using for example algorithms known as particle ﬁlters [32]. The
solution requires, however, that one builds approximations for the joint probability distribution of d(n)
and x(n), x(n −1), . . ., which in general requires a large amount of data, resulting in relatively high
complexity and relatively slow convergence. If you know more about the relation between {x(n)} and
d(n), this information might be used to constrain the search to a smaller class of problems, potentially
leading to simpler algorithms with faster convergence. That is the goal of adaptive ﬁlters, and the reason
why we restricted ourselves to this case in the main text.

670
CHAPTER 12 Adaptive Filters
x1
x2
x
y
(a) Convex function
x1
x2
x
y
(b) Non-convex function
FIGURE 12.23
Convex functions.
Box 5: Convex functions
Convex sets are such that lines between any two points in the set always stay entirely in the set (see
Figure 12.22). More formally, a set S ∈Rn is convex if
s1, s2 ∈S ⇒λs1 + (1 −λ)s2 ∈S,
for all λ ∈[0, 1].
Convex functions, on the other hand, are functions f (x) : Rn →R such that for all x1, x2 ∈Rn
and 0 < λ < 1,
f (λx1 + (1 −λ)x2) ≤λ f (x1) + (1 −λ) f (x2).
The function is called strictly convex if the inequality is strict (< instead of ≤). For example, f (x) = x2
is strictly convex. Figure 12.23 shows examples of convex and non-convex functions. An important
property of strictly convex functions is that they always have a unique minimum, that is, they never
have sub-optimal local minima.
Box 6: Wirtinger derivatives and minima of functions of a complex variable
Suppose you want to minimize a function
f : C →R,
w ∈C →f (w) ∈R,
(12.109)

1.12.2 Optimum Filtering
671
that is, a function that takes a complex argument to a real value. The minimization problem is well
deﬁned, since the image of f is real. However, we cannot use standard arguments about derivatives and
stationary points to ﬁnd the minimum, because a function like this is never analytic, that is, the standard
derivative used in complex analysis does not exist.
To see why, we need to think about f as a function of two variables, x and y, the real and imaginary
parts of w. Take f (w) = |w|2 = ww∗for example. We can write it as a function of x and y as
ˆf (x, y) = f (x + jy) = x2 + y2.
(12.110)
Since it is a function of two variables, the derivatives in principle depend on the direction we are moving
along. Consider the derivative at a point w along the x axis, that is, keeping y constant:
lim
ϵ→0
(x + ϵ)2 + y2 −(x2 + y2)
ϵ
= 2x.
On the other hand, along the y axis, we have
lim
ϵ→0
x2 + (y + ϵ)2 −(x2 + y2)
ϵ
= 2y,
that is, the derivative at point w is different according to the direction we are moving. This is essentially
what is meant by saying that the function is not analytic at w.
For contrast, consider now the analytic function g = w2. Its derivative is the same, no matter which
direction we take:
lim
ϵ→0
(x + ϵ + jy)2 −(x + jy)2
ϵ
= lim
ϵ→0
(x + ϵ)2 −y2 + 2 j(xy + ϵy) −(x2 −y2 + 2 jxy)
ϵ
= 2w.
Similarly, differentiating along the y axis we obtain again the same result
lim
ϵ→0

x + j(y + ϵ)
2 −(x + jy)2
ϵ
= 2w.
When the derivative at a certain point w0 is the same along every direction, we can deﬁne derivatives
of complex functions writing simply
dg(w0)
dw
=
lim
w→0
g(w0 + w) −g(w0)
w
,
(12.111)
since we know the limit does not depend on the direction along which w goes to zero.
It can be shown that a function is analytic at a given point only if the Cauchy-Riemann conditions
hold at that point. Separating a generic complex function g into its real and imaginary parts g(w) =
u(x, y) + jv(x, y), the Cauchy-Riemann conditions are [33]
∂u
∂x = ∂v
∂y ,
∂u
∂y = −∂v
∂x .

672
CHAPTER 12 Adaptive Filters
In the case of a real function of a complex variable, such as f deﬁned as in (12.109), v ≡0, and of
course the conditions could hold only for points in which the real part has zero partial derivatives.
This means that we cannot search for the stationary points of a function such as (12.109) using the
derivative deﬁned as in (12.111). We could of course always go back and interpret the function as a
function of two variables, as we did in (12.110). In order to ﬁnd the minimum of f (w), we would need
to ﬁnd the stationary points of ˆf (x, y)
∂ˆf
∂x = 0,
∂ˆf
∂y = 0.
This works just ﬁne, of course, but there is a nice trick, which allows us to keep working with complex
variables in a simple way. The idea is to deﬁne derivatives differently, such that optimization problems
become easy to solve, using what is known as Wirtinger calculus.
Real functions of complex variables are usually deﬁned in terms of variables and its conjugates, since
w + w∗and ww∗are both real. The idea is to deﬁne derivatives such that w and w∗can be treated as
if they were independent variables, as follows. Despite the motivation, the new derivatives are deﬁned
for general functions g : C →C.
Deﬁne partial derivatives of a function g : C →C with respect to w and w∗as
∂g
∂w = 1
2
∂g
∂x −j ∂g
∂y

,
∂g
∂w∗= 1
2
∂g
∂x + j ∂g
∂y

.
(12.112)
You can check that if g is analytic at a given point (so g satisﬁes the Cauchy-Riemann conditions at that
point), then ∂g/∂w∗= 0 at that point.
Consider, for example, g(w) = wm(w∗)n = (x + jy)m(x −jy)n. Then,
∂g
∂w = 1
2

m(x + jy)m−1(x −jy)n + n(x + jy)m(x −jy)n−1
−j( jm(x + jy)m−1(x −jy)n −jn(x + jy)m(x −jy)n−1)

= 1
2

mwm−1(w∗)n + nwm(w∗)n−1 + mwm−1(w∗)n −nwm(w∗)n−1
= mwm−1(w∗)n.
As advertised, the ﬁnal result is what we would obtain if we had treated w and w∗as two independent
variables. Similarly, you can check that
∂g
∂w∗= nwm(w∗)n−1.
An important special case is the quadratic function f (w) = d −w∗r −r∗w + w∗wR, which assumes
only real values if d and R are real. Using the previous result, we see that
∂f
∂w = −r∗+ w∗R,
∂f
∂w∗= −r + wR.

1.12.2 Optimum Filtering
673
Note that there is no factor of 2 in this case. This difference between Wirtinger derivatives and standard
real derivatives will propagate to many expressions in this text.
Now, the deﬁnitions (12.112) are useful for minimizing real functions of complex variables as in
(12.109), since from (12.112)
∂f
∂w∗= 0 ⇔∂ˆf
∂x = 0 and
∂ˆf
∂y = 0.
Applying this idea to our quadratic function, we see that
∂f
∂w∗= 0 ⇒Rwo = r ⇒wo = r/R,
if R ̸= 0. In addition,
∂2 f
∂w ∂w∗= R,
and we can conclude that wo is a minimum as long as R > 0.
The same conclusions could easily be obtained working directly with ˆf . The Wirtinger derivatives
are merely a shortcut, that makes working with complex variables very similar to working with real
variables. A good detailed description of Wirtinger derivatives can be found in [34].
In the vector case, the deﬁnitions and results are very similar. Given a function
g : CM →R,
w =
⎡
⎢⎢⎢⎣
w0
w1
...
wM−1
⎤
⎥⎥⎥⎦∈CM →g(w),
with wi = xi + jyi, i = 0, . . . , M −1, we deﬁne
∂g
∂w =

∂g
∂w0 . . .
∂g
∂wM−1

= 1
2

∂g
∂x0 −j ∂g
∂y0 . . .
∂g
∂xM−1 −j
∂g
∂yM−1

,
∂g
∂wH =
⎡
⎢⎢⎣
∂g
∂w∗
0...
∂g
∂w∗
M−1
⎤
⎥⎥⎦= 1
2
⎡
⎢⎢⎣
∂g
∂x0 −j ∂g
∂y0
...
∂g
∂xM−1 −j
∂g
∂yM−1
⎤
⎥⎥⎦.
Using our deﬁnition of gradients, the second derivative of f (its Hessian) is
∂2 f
∂w ∂wH =
⎡
⎢⎢⎢⎣
∂
∂w

∂f
∂w∗
0

...
∂
∂w

∂f
∂w∗
M−1

⎤
⎥⎥⎥⎦=
⎡
⎢⎢⎢⎣
∂2 f
∂w0 ∂w∗
0
. . .
∂2 f
∂wM−1 ∂w∗
0
...
...
...
∂2 f
∂w0 ∂w∗
M−1 . . .
∂2 f
∂wM−1 ∂w∗
M−1
⎤
⎥⎥⎥⎦.

674
CHAPTER 12 Adaptive Filters
The most important example is the quadratic function
f (w) = rd −wH rdφ −r H
dφw + wH Rφw,
where rd is a real variable, and Rφ is Hermitian symmetric. Then, we have
∂f
∂wH = −rdφ + Rφw.
We see that a stationary point of f is such that Rφwo = rdφ. In addition,
∂2 f
∂w ∂wH = Rφ,
and we conclude that wo is the single point of minimum of f if and only if Rφ is positive-deﬁnite.
Note that in the complex case, the gradient and the Hessian do not have the factors of 2 that we saw
in the real case (see Box 1).
1.12.3 Stochastic algorithms
In this section, we focus on three of the most important adaptive algorithms: the least-mean squares
(LMS) algorithm, the normalized least-mean squares (NLMS) algorithm, and the recursive least-squares
(RLS) algorithm, which are the basis of many other algorithms. These three stochastic algorithms are
derived in detail and analyzed using different methods. We present different approaches for analyzing
an adaptive ﬁlter, because the variety and complexity of their behavior makes it difﬁcult for a single
technique to be able to handle all situations. The analyses provided here allow us to predict the behavior
of the algorithms in transient and steady-state, and choose values of parameters (such as the step-size)
for which the ﬁlters operate adequately. In particular, we study conditions under which the ﬁlters will
remain stable.
We assume linearly parametrized classes, such as FIR (FFIR) and Volterra (FV) ﬁlters (see Section
1.12.2.1). In these classes, each element of the input regressor vector is given by a certain function φi
i = 0, 1, . . . , M −1 of x(n), x(n −1), . . . , x(n −N). In the case of length-M FIR ﬁlters, we have
φ(n) =
φ0(n) φ1(n) · · · φM−1(n)T
=
 x(n) x(n −1) · · · x(n −M + 1)T ,
(12.113)
whereas in the case of second-order Volterra ﬁlters with memory N = 1 and real-valued signals, we
have
φ(n) =
φ0(n) φ1(n) φ2(n) φ3(n) φ4(n)T
=

x(n) x(n −1) x2(n) x(n)x(n −1) x2(n −1)
T .
(12.114)
As much as possible, our presentation in this section is independent of the choices of F and φ(·).
However, some algorithms are designed for a speciﬁc class F (the most common case are fast algorithms
designed for FIR ﬁlters (12.113)). In these cases, we will alert the reader of the restriction.

1.12.3 Stochastic Algorithms
675
Using the results of Box 6, the derivation of algorithms for real or circular complex inputs is very
similar, the only differences being the need of conjugating a few variables in some expressions, and the
appearance of a factor β = 2 in the expressions of gradients of functions of real variables, and β = 1
in the complex case.
In general, the output of a length-M adaptive ﬁlter is given by
ˆy(n) = wH(n)φ(n) = w∗
0(n)φ0(n) + w∗
1(n)φ1(n) + · · · + w∗
M−1(n)φM−1(n).
(12.115)
In the case of Volterra ﬁlters, it is common to use the notation wk,ℓfor the weight related to the input
x(n −k)x(n −ℓ) (see Eq. (12.43)). However, to obtain a common formulation independent of the class
F, we denote the weight vector as
w(n) =
w0(n) w1(n) w2(n) · · · wM−1(n) T .
(12.116)
The task of the adaptive ﬁlter will be to choose in a recursive form the values of the parameters
w0, w1, . . . , wM−1 that best ﬁt the data at each time instant n.
Figure 12.24 shows the scheme of a length-M adaptive ﬁlter for linearly parametrized classes, where
 denotes the set of functions φi(·), i = 0, 1, . . . , M −1 of x(n), x(n −1), . . . , x(n −N) that generate
the elements φ0(n), φ1(n), . . . , φM−1(n) of the input regressor vector φ(n). The number of samples of
the signal x(n) depends on the class F and is given by N + 1. An FIR ﬁlter with length M requires
N + 1 = M samples. On the other hand, a Volterra ﬁlter with memory N, requires N + 1 samples, but
the length of the corresponding adaptive ﬁlter depends on the order of the Volterra series expansion.
At each time instant, the output of the adaptive ﬁlter ˆy(n) is compared with the desired signal d(n) to
compute the error e(n) = d(n) −ˆy(n). In order to minimize a cost function of this error, a stochastic
algorithm uses e(n) to adjust the ﬁlter weights.
1.12.3.1 LMS algorithm
The update equation of the steepest-descent algorithm is given by
w(n + 1) = w(n) −μ
β ∇w J(n),
(12.117)
where μ is a step-size and β = 1 (resp., β = 2) for complex (resp., real) data (see Box 6). This algorithm
requires exact measurements of the gradient vector. For the case of minimization of the mean-square
error, that is, if J(n) = E{|e(n)|2},
∇w J(n) = −βrdφ + β Rφw(n)
(12.118)
at each iteration n (see Box 1 for an introduction to gradient algorithms). Note that the exact gradient
requires a prior knowledge of the correlation matrix Rφ and the cross-correlation vector rdφ, which is
not feasible in real-time applications. For example, in the hands-free telephone application described
in Section 1.12.1.1, the input signal of the adaptive ﬁlter is the far-end speech (see Figures 12.3 and
12.4). A speech signal is nonstationary and therefore, it is not possible to obtain exact estimates for Rφ
and rdφ at each time instant. Furthermore, good estimates would demand much processing time, which

676
CHAPTER 12 Adaptive Filters
z −1
z −1
z −1
. . .
. . .
. . .
x (n)
x (n−1)
x (n−2)
x(n−N)
Φ
w ∗
0 (n )
w ∗
1 (n )
w ∗
2 (n )
w ∗
M−1 (n)
φ0 (n)
φ1 (n)
φ2 (n)
φM −1 (n)
ˆy (n)
d (n)
e (n)
FIGURE 12.24
Scheme for a length-M adaptive ﬁlter assuming linearly parametrized classes.
could insert an inadmissible delay in the system, making this approach not feasible, except in off-line
applications. Similar restrictions appear in virtually all adaptive ﬁltering applications.
To circumvent this problem, a stochastic version of the steepest descent algorithm was proposed,
the least-mean squares (LMS) algorithm. Instead of minimizing the exact mean-square error, the LMS
algorithm minimizes a straightforward approximation to it: |e(n)|2. The idea, as we saw in Section
1.12.1.2.4, is that the algorithm will approximately average the cost-function if adaptation is slow
(small step-size). The estimation error is
e(n) = d(n) −wH(n)φ(n),
(12.119)
so the gradient of the instantaneous cost function is
∇w J(n) = −βφ(n)d∗(n) + βφ(n)φH(n)w(n) = −βφ(n)e∗(n).
(12.120)
Since e(n) depends on the old estimate of the weight vector, it is called an a priori estimation error,
by opposition to the a posteriori error that will be introduced in (12.129). Instead of minimizing the
instantaneous cost function, the stochastic gradient deﬁned in (12.120) can be obtained by replacing
the instantaneous estimates
Rφ(n) = φ(n)φH(n)
(12.121)
and
rdφ(n) = φ(n)d∗(n)
(12.122)
in (12.118).

1.12.3 Stochastic Algorithms
677
Table 12.2 Summary of the LMS Algorithm
Initialization:
Set w(0) = 0
For n = 0, 1, 2, . . ., compute
ˆy(n) = wH(n)φ(n)
e(n) = d(n) −ˆy(n)
w(n + 1) = w(n) + μe∗(n)φ(n)
Table 12.3 Computational Cost of the LMS Algorithm for Complex and Real-Valued Signals in
Terms of Real Multiplications and Real Additions Per Iteration
Term
Real Signals
Complex Signals
×
+
×
+
ˆy ∗(n) = φH(n)[w(n)]
M
M −1
4M
4M −2
e∗(n) = d ∗(n) −[ˆy ∗(n)]
1
2
μ[e∗(n)]
1
2
[μe∗(n)]φ(n)
M
4M
2M
w(n + 1) = [w(n)] + [μe∗(n)φ(n)]
M
2M
TOTAL per iteration
2M + 1
2M
8M + 2
8M
Replacing (12.120) in (12.117), we arrive at the update equation for LMS, i.e.,
w(n + 1) = w(n) + μφ(n)e∗(n).
(12.123)
As explained in Section 1.12.1.2.4 and illustrated in the animations of Figure 12.9, although there is no
explicit average being taken in the LMS algorithm, it in fact computes an implicit approximate average
if the step-size is small.
The LMS algorithm is summarized in Table 12.2. Its computational cost increases linearly with
the length of the ﬁlter, i.e., the computational cost is O(M). Table 12.3 shows the number of real
multiplications and real additions per iteration for real and complex-valued signals. The quantities
inside brackets were computed in previous steps. We assume that a complex multiplication requires 4
real multiplications and 2 real additions (it is possible to use less multiplications and more additions),
and a complex addition requires 2 real additions. Different ways of carrying out the calculations may
result in a different computational cost. For example, if we ﬁrst computed μ × φ(n) followed by

μφ(n)

×e∗(n), LMS would require M −1 more real multiplications per iteration than the ordering of
Table 12.3, considering real-valued signals. For complex-valued signals, the cost would be even higher,
since LMS would require 4M −2 and 2M more real multiplications and real additions, respectively.

678
CHAPTER 12 Adaptive Filters
1.12.3.1.1
A deterministic approach for the stability of LMS
Finding the range of step-sizes such that the recursion (12.123) converges is a complicated task. There
are two main approaches to ﬁnding the maximum step-size: an average approach and a deterministic
(worst-case) approach. The average approach will be treated in Section 1.12.4.3. Since the deterministic
approach is easy to explain intuitively, we address it in the sequel (although a precise proof is not so
simple, see [35,36] for a full analysis).
For simplicity, assume that our ﬁlter is such that d(n) ≡0. Although this is not a very likely situation
to encounter in practice, it allows us to avoid technical details that would complicate considerably the
discussion, and still arrive at the important conclusions.
Let us rewrite the LMS recursion, expanding the error with d(n) = 0 as e∗(n) = −(wH(n)φ(n))∗=
−φH(n)w(n) to obtain
w(n + 1) =

I −μφ(n)φH(n)

w(n).
(12.124)
This is a linear system, but not a time-invariant one. It will be stable if the eigenvalues λi(n) of the
matrix A(n) = I −μφ(n)φH(n) satisfy
max
i
|λi(n)| ≤1,
for all n ≥0.
(12.125)
Be careful, if d(n) were not identically zero, condition (12.125) would have to be slightly modiﬁed to
guarantee stability (see, e.g., [36,37]).
The eigenvalues of A(n) are easy to ﬁnd, if we note that φ(n) is an eigenvector of A(n) (the case
φ(n) = 0 is trivial):
A(n)φ(n) = φ(n) −μφ(n)φH(n)φ(n) = (1 −μ∥φ(n)∥2)φ(n),
where ∥φ(n)∥2 = φH(n)φ(n) is the square of the Euclidean norm. The corresponding eigenvalue is
λ1 = 1 −μ∥φ(n)∥2. All other eigenvalues are equal to one (the eigenvectors are all vectors orthogonal
to φ(n).) Therefore, the LMS algorithm (12.124) will be stable whenever
−1 ≤1 −μ∥φ(n)∥2 ≤1 ⇐⇒0 ≤μ ≤
2
supn≥0 ∥φ(n)∥2 .
(12.126)
The supremum (sup) of a sequence is an extension to the maximum, deﬁned to avoid a technicality.
Consider the sequence f (n) = {1 −0.5n}, n ≥0. Strictly speaking, it has no maximum, since there is
no n0 for which f (n0) ≥f (n) for all n ≥0. The supremum is deﬁned so we can avoid this problem:
supn≥0 f (n) = 1. Therefore, the supremum is equal to the maximum whenever the later exists, but is
also deﬁned for some sequences that do not have a maximum. The inﬁmum ( inf ) has a similar relation
to the minimum.
This condition is sufﬁcient for stability, but is too conservative, since ∥φ(n)∥2 is not at its highest
value at all instants. A popular solution is to use normalization. The idea is simple: adopt a time-variant
step-size μ(n) such that
μ(n) =
˜μ
ε + ∥φ(n)∥2 ,
(12.127)
with 0 < ˜μ < 2 and where ε > 0 is used to avoid division by zero. The resulting algorithm is known
as normalized LMS (NLMS). We will talk more about it in the next section.

1.12.3 Stochastic Algorithms
679
1.12.3.2 Normalized LMS algorithm
One problem with the LMS algorithm is how to choose the step-size. How large should it be to enable a
high convergence rate, provide an acceptable misadjustment, and even ensure the stability of LMS? More
importantly, how can this choice be made so that the ﬁlter will work correctly even for very different
input signals? One answer to this problem is the normalized LMS algorithm which we describe now. It
uses a time varying step-size, which is particularly useful when the statistics of the input signals change
quickly.
Thus, replacing the ﬁxed step-size μ by a time varying step-size μ(n) in (12.123), we obtain
w(n + 1) = w(n) + μ(n)φ(n)e∗(n).
(12.128)
In order to increase the convergence speed, we could try to ﬁnd μ(n) such that the a posteriori estimation
error,
ξ(n) = d(n) −wH(n + 1)φ(n),
(12.129)
is zeroed. Note that, in contrast to the a priori estimation error deﬁned in (12.119), ξ(n) depends on the
updated estimate of the weight vector, hence the name a posteriori estimation error. Replacing (12.128)
in (12.129), we obtain
ξ(n) = d(n) −wH(n)φ(n)



e(n)
−μ(n)φH(n)φ(n)e(n)
= [1 −μ(n)φH(n)φ(n)]e(n).
(12.130)
In order to enforce ξ(n) = 0 at each time instant n, we must select
μ(n) =
1
φH(n)φ(n)
=
1
∥φ(n)∥2 .
(12.131)
Replacing (12.131) in (12.128), we obtain the update equation of the normalized least-squares algo-
rithm, i.e.,
w(n + 1) = w(n) +
1
∥φ(n)∥2 φ(n)e∗(n).
(12.132)
Note that the time varying step-size μ(n) depends inversely on the instantaneous power of the input
vector φ(n). Indeed, we will show in Section 1.12.4.3 that the maximum value of the ﬁxed step-size
μ to ensure the convergence and mean-square stability of LMS depends inversely on the power of the
input signal.
In order to make (12.132) more reliable for practical implementations, it is common to introduce
two positive constants: a ﬁxed step-size ˜μ to control the rate of convergence (and the misadjustment)
and the regularization factor ε to prevent division by a small value when ∥φ(n)∥2 is small. With these
constants, the NLMS update equation reduces to
w(n + 1) = w(n) +
˜μ
ε + ∥φ(n)∥2 φ(n)e∗(n).
(12.133)

680
CHAPTER 12 Adaptive Filters
Some authors refer to (12.132) as NLMS and to (12.133) as ε-NLMS. However, for simplicity we
will refer to both as NLMS. The name “normalized” is due to a most useful property of NLMS: the
range of values of ˜μ for which the algorithm remains stable is independent of the statistics of x(n), i.e.,
0 < ˜μ < 2, as can be observed in Section 1.12.3.1.1.
As pointed out in [3], the NLMS algorithm can also be interpreted as a quasi-Newton algorithm. As
brieﬂy explained in Box 1, a quasi-Newton algorithm updates the weights using approximations for the
Hessian matrix and gradient vector, i.e.,
w(n + 1) = w(n) −μ
β

∇2
w J(n)
−1 ∇w J(n).
(12.134)
Assuming the following instantaneous approximations
∇w J(n) = −βφ(n)d∗(n) + βφ(n)φH(n)w(n) = −βφ(n)e∗(n)
(12.135)
and
∇2
w J(n) = εI + φ(n)φH(n),
(12.136)
and replacing them in (12.134), we arrive at the stochastic recursion
w(n + 1) = w(n) + μ

εI + φ(n)φH(n)
−1
φ(n)e∗(n).
(12.137)
The term εI guarantees that (12.136) is invertible (a rank-one matrix such as φ(n)φH(n) is never
invertible).
To compute [εI + φ(n)φH(n)]−1, we can use the matrix inversion lemma (Box 3), which gives us
an expression for the inverse of an M × M matrix of the form
A = B + C DC H,
(12.138)
where B and D are two square matrices with dimensions M and N, respectively, and C is an M × N
matrix. According to the matrix inversion lemma, the inverse of A is given by
A−1 = B−1 −B−1C(D−1 + C H B−1C)−1C H B−1.
(12.139)
Thus, identifying
A = εI + φ(n)φH(n),
B = εI,
C = φ(n),
and
D = 1,
we obtain
[εI + φ(n)φH(n)]−1 = ε−1I −
ε−2
1 + ε−1φH(n)φ(n)
φ(n)φH(n).
(12.140)
Multiplying (12.140) from the right by φ(n), after some algebraic manipulations, we get
[εI + φ(n)φH(n)]−1φ(n) =
φ(n)
ε + φH(n)φ(n)
=
φ(n)
ε + ∥φ(n)∥2 .
(12.141)

1.12.3 Stochastic Algorithms
681
Table 12.4 Summary of the NLMS Algorithm
Initialization:
Set w(0) = 0 and 0 < ˜μ < 2
For n = 0, 1, 2, . . ., compute
ˆy(n) = wH(n)φ(n)
e(n) = d(n) −ˆy(n)
w(n + 1) = w(n) +
˜μ
ε + ∥φ(n)∥2 e∗(n)φ(n)
Replacing (12.141) in (12.137), we arrive at (12.133). Since Newton-based algorithms converge faster
than gradient-based algorithms, it is expected that NLMS exhibits a higher convergence rate than that
of LMS, which is indeed the case in general.
The NLMS algorithm is summarized in Table 12.4. To obtain its computational cost, we can use the
same procedure considered in the computation of the number of operations of LMS (see Table 12.3).
For complex-valued signals, NLMS requires 10M + 2 real multiplications, 10M real additions and one
real division per iteration. For real-valued signals, it requires 3M + 1 real multiplications and 3M real
additions per iteration. Note that for input vectors with a shift structure such as the FIR ﬁlters of Eq.
(12.113), its computational cost can be reduced if ∥φ(n)∥2 is computed by the following recursion
∥φ(n)∥2 = ∥φ(n −1)∥2 −|x(n −M)|2 + |x(n)|2
(12.142)
with initialization ∥φ(−1)∥= 0.
1.12.3.3 RLS algorithm
The convergence rate of LMS-type algorithms varies considerably with the input signal x(n), since they
are based on steepest-descent optimization algorithms. If x(n) is a white noise, the convergence rate
is high. On the other hand, if the correlation between successive samples of x(n) is high, LMS-type
algorithms converge slowly. The RLS algorithm does not have this drawback. However, this advantage
has a price: a considerable increase in the computational cost. Furthermore, numerical errors play a more
important role in RLS and may even lead to divergence of the weight estimates. Different versions of RLS
were proposed to circumvent this problem. In the sequel, we obtain the equations of the conventional
RLS algorithm.
The RLS can be derived in different ways (much as NLMS). Each form of arriving at the algorithm
highlights a different set of its properties: one that leads to many new algorithms and insights is the
connection between RLS and Kalman ﬁlters described in [38].
We present here shorter routes, starting with a derivation based on deterministic arguments. In fact,
RLS solves a deterministic least-squares problem, which is based on the weighted least-squares cost
function
JLS(n) =
n

ℓ=0
λn−ℓ|ξ(ℓ)|2,
(12.143)

682
CHAPTER 12 Adaptive Filters
where 0 ≪λ < 1 is a constant known as forgetting factor,
ξ(ℓ) = d(ℓ) −wHφ(ℓ),
(12.144)
are a posteriori errors and
w(n + 1) = arg min
w
JLS(n).
(12.145)
The factor λn−ℓemphasizes the most recent errors, forgetting the errors from the remote past. This is
important, otherwise the algorithm would give too much weight to the remote past and would not be
able to track variations in the input signals. Note that w(n + 1) minimizes JLS(n) at each time instant.
Thus, RLS provides an exact solution for the least-squares problem at each time instant. As the NLMS
algorithm, RLS seeks to minimize the a posteriori error |ξ(n)|2, searching for an exact solution to an
optimization problem. The difference is that RLS searches for a weight vector w(n + 1) that takes into
consideration the whole past history of inputs, minimizing λn|ξ(0)|2 + λn−1|ξ(1)|2 + · · · + λ|ξ(n −
1)|2 + |ξ(n)|2 at each time instant n, not only the current value |ξ(n)|2 as in NLMS.
The error sequence ξ(ℓ) weighted by λ(n−ℓ)/2 in the interval 0 ≤ℓ≤n can be written in a vectorial
form, i.e.,
ξ(n) = d(n) −(n)w∗,
(12.146)
where
ξ(n) =

ξ(n) λ1/2ξ(n −1) · · · λn/2ξ(0)
T ,
(12.147)
d(n) =

d(n) λ1/2d(n −1) · · · λn/2d(0)
T
(12.148)
and
(n) =
⎡
⎢⎢⎢⎢⎢⎣
φ0(n)
φ1(n)
· · ·
φM−1(n)
λ1/2φ0(n −1)
λ1/2φ1(n −1)
· · ·
λ1/2φM−1(n −1)
λφ0(n −2)
λφ1(n −2)
· · ·
λφM−1(n −2)
...
...
...
...
λn/2φ0(0)
λn/2φ1(0)
· · ·
λn/2φM−1(0)
⎤
⎥⎥⎥⎥⎥⎦
.
(12.149)
Note that the weighted least-squares cost function can be rewritten in terms of the vector ξ(n), i.e.,
JLS(n) = ξ H(n)ξ(n) = ∥ξ(n)∥2 =
n

ℓ=0
λn−ℓ|ξ(ℓ)|2.
(12.150)
Now, we have to ﬁnd the weight vector w(n + 1) that minimizes (12.150). The gradient of JLS(n)
with relation to wH is given by
∇w JLS(n) = −βT (n)d∗(n) + βT (n)∗(n)w,
(12.151)
where as usual, β = 1 (resp., β = 2) for complex (resp., real) data. Deﬁning
Rφ(n) = T (n)∗(n) =
n

ℓ=0
λn−ℓφ(ℓ)φH(ℓ)
(12.152)

1.12.3 Stochastic Algorithms
683
and
ˆrdφ(n) = T (n)d∗(n) =
n

ℓ=0
λn−ℓd∗(ℓ)φ(ℓ),
(12.153)
in which φ(n) is deﬁned as before, (12.151) can be rewritten as
∇w JLS(n) = −β ˆrdφ(n) + βRφ(n)w.
(12.154)
Equating ∇w JLS(n) to the null vector, we get the normal equations
Rφ(n)w(n + 1) = ˆrdφ(n).
(12.155)
Note that these are a deterministic version of the normal equations we saw in Section 1.12.2.1. Therefore,
to minimize JLS(n), the weight vector w(n + 1) must satisfy
w(n + 1) = R−1
φ (n)ˆrdφ(n),
(12.156)
which requires the computation of the inverse correlation matrix R−1
φ (n) at each time instant. Note that
this is only valid if R−1
φ (n) is nonsingular. In particular, the inverse does not exist if n < M (Box 3).
Assuming λ = 1 and that the signals x(n) and d(n) are jointly stationary and ergodic, we obtain the
following steady-state relations
lim
n→∞
1
n + 1
Rφ(n)|λ=1 = Rφ
and
lim
n→∞
1
n + 1 ˆrdφ(n)|λ=1 = rdφ.
Therefore, the deterministic normal equations converge to the stochastic normal equations when λ = 1,
and the weight vector obtained from (12.156) converges to the Wiener solution, i.e.,
lim
n→∞w(n + 1)|λ=1 = R−1
φ rdφ = wo.
(12.157)
We should mention that with λ = 1 the convergence rate of RLS goes to zero, i.e., it loses the ability to
follow the statistical variations of the observed data. An equivalent result is obtained for LMS or NLMS
when the ﬁxed step-size μ or ˜μ is replaced by 1/n [39].
The computational cost of (12.156) is high due to the computation of the inverse matrix. In order to
solve the normal equations in real-time and with less cost, we should rewrite (12.152) and (12.153) as
recursions, i.e.,
Rφ(n) = λRφ(n −1) + φ(n)φH(n),
(12.158)
and
ˆrdφ(n) = λˆrdφ(n −1) + d∗(n)φ(n),
(12.159)
with initializations Rφ(−1) = δI and ˆrdφ(−1) = 0, where δ is a small positive constant. Note that
(12.159) is identical to (12.153), but (12.158) leads to the estimate
Rφ(n) = λn+1δI +
n

ℓ=0
λn−ℓφ(ℓ)φH(ℓ),
(12.160)

684
CHAPTER 12 Adaptive Filters
and therefore the initialization guarantees the existence of the inverse at all instants. For 0 ≪λ < 1,
the initialization Rφ(−1) = δI is forgotten and (12.158) becomes close to (12.152) as n increases.
Furthermore, if you want the algorithm to forget quickly the initialization, besides using 0 ≪λ < 1,
you should choose δ small (δ−1 large). Note however that a certain amount of regularization may be
useful in practice, to avoid problems when the input signal has long stretches with low power.
In order to obtain a recursion for the inverse matrix R−1
φ (n), we can use the matrix inversion lemma
(Box 3) by comparing (12.138) with (12.158) and identifying
A = Rφ(n),
B = λRφ(n −1),
C = φ(n),
and
D = 1.
Thus, using (12.139), we obtain
R−1
φ (n) = λ−1R−1
φ (n −1) −
λ−2R−1
φ (n −1)φ(n)φH(n)R−1
φ (n −1)
1 + λ−1φH(n)R−1
φ (n −1)φ(n)
.
(12.161)
Deﬁning P(n) ≜R−1
φ (n), with some algebraic manipulations in (12.161), we arrive at
P(n) = 1
λ

P(n −1) −P(n −1)φ(n)φH(n)P(n −1)
λ + φH P(n −1)φ(n)
	
(12.162)
with initialization P(−1) = δ−1I. This equation allow us to compute the inverse correlation matrix in
a recurrent form, avoiding the explicit computation of a matrix inverse. However, the implementation
of (12.162) in ﬁnite precision arithmetic requires some care to avoid numerical problems, as explained
in Section 1.12.3.3.1.
To simplify the following equations, it is convenient to deﬁne the column vector
k(n) =
P(n −1)φ(n)
λ + φH(n)P(n −1)φ(n)
,
(12.163)
which is the so-called Kalman gain. Thus, (12.162) can be rewritten as
P(n) = λ−1 
I −k(n)φH(n)

P(n −1).
(12.164)
Using (12.164) and some algebraic manipulations in (12.163), we arrive at
k(n) = λ−1 P(n −1)φ(n) −λ−1k(n)φH(n)P(n −1)φ(n)
= λ−1 
I −k(n)φH(n)

P(n −1)φ(n)
= P(n)φ(n).
(12.165)
Thus, using (12.159) and (12.165) in (12.156), the weight vector w(n + 1) can also be computed
recursively, i.e.,
w(n + 1) = P(n)ˆrdφ(n)
= λP(n)ˆrdφ(n −1) + P(n)φ(n)d∗(n)
= λP(n)ˆrdφ(n −1) + k(n)d∗(n).
(12.166)

1.12.3 Stochastic Algorithms
685
Table 12.5 Summary of the Conventional RLS Algorithm
Initialization:
Set P(−1) = δ−1I; δ = small positive constant
w(0) = 0 and 0 ≪λ < 1
For n = 0, 1, 2, . . ., compute
k(n) =
P(n −1)φ(n)
λ + φH(n)P(n −1)φ(n)
ˆy(n) = wH(n)φ(n)
e(n) = d(n) −ˆy(n)
w(n + 1) = w(n) + k(n)e∗(n)
P(n) = λ−1 
I −k(n)φH(n)

P(n −1)
Replacing (12.164) in (12.166), we arrive at
w(n + 1) = P(n −1)ˆrdφ(n −1) −k(n)φH(n)P(n −1)ˆrdφ(n −1) + k(n)d∗(n)
= w(n) −k(n)φH(n)w(n) + k(n)d∗(n)
= w(n) + k(n)[d(n) −wH(n)φ(n)



e(n)
]∗.
(12.167)
The RLS algorithm was derived here as an exact solution for the least-squares problem. However, it
also can be interpreted as a quasi-Newton algorithm assuming the instantaneous approximation for the
gradient vector as in (12.135) and considering P(n) as an approximation for the inverse Hessian matrix.
Replacing these approximations in (12.134), we arrive at the same stochastic recursion (12.167).
The RLS algorithm is summarized in Table 12.5. Its computational cost increases with M2, much
faster than those of LMS and NLMS. Different ways of carrying out the calculations may result in a
slightly different computational cost, but all with the same order of magnitude, i.e., O(M2). Performing
the calculations in the following order [3]:
φH(n) × [w(n)]
d∗(n) −[φH(n)w(n)]
λ−1φ(n)
P(n −1) × [λ−1φ(n)]
φH(n) × [λ−1 P(n −1)φ(n)]
1 + [λ−1φH(n)P(n −1)φ(n)]
1/[1 + λ−1φH(n)P(n −1)φ(n)]
[λ−1φH(n)P(n −1)φ(n)] ×

1
1 + λ−1φH(n)P(n −1)φ(n)


686
CHAPTER 12 Adaptive Filters
[λ−1 P(n −1)φ(n)] ×

λ−1φH(n)P(n −1)φ(n)
1 + λ−1φH(n)P(n −1)φ(n)
	
P(n)φ(n) = [λ−1 P(n −1)φ(n)] −

λ−2 P(n −1)φ(n)φH(n)P(n −1)φ(n)
1 + λ−1φH(n)P(n −1)φ(n)
	
[P(n)φ(n)] × [d∗(n) −φH(n)w(n)]
w(n + 1) = [w(n)] + [P(n)φ(n)(d∗(n) −φH(n)w(n))],
the conventional RLS requires 4M2 +16M +1 real multiplications, 4M2 +12M −1 real additions, and
one real division per iteration for complex-valued signals. For real-valued data, it requires M2 +5M +1
real multiplications, M2 +3M real additions, and one real division per iteration. Note that the quantities
inside brackets were computed in previous steps.
1.12.3.3.1
Practical implementation of RLS
The RLS algorithm must be implemented carefully, because it is sensitive to numerical errors, which
may accumulate and make the algorithm unstable. The problem can be understood intuitively as follows.
In the conventional RLS algorithm, the inverse of the autocorrelation matrix is computed recursively
by (expanding (12.161))
P(n) = λ−1 P(n −1) −λ−2 P(n −1)φ(n)φH(n)P(n −1)
1 + λ−1φH(n)P(n −1)φ(n)
,
i.e., as the difference between two positive (semi-) deﬁnite matrices. Note that, since λ ≤1, the factor
multiplying the current P(n −1) is λ−1 ≥1. In inﬁnite precision arithmetic, any growth due to this
factor will be compensated by the second term. However, in ﬁnite precision arithmetic this compensation
may not take place, and the factor λ−1 may make the recursion numerically unstable—usually, what
happens is that P(n) looses its positive character, so the matrix will have a negative eigenvalue, which in
its turn leads to the divergence of the coefﬁcients. Thus, to avoid problems one would need to guarantee
that P(n) stays symmetric and positive-deﬁnite for all time instants n (these properties are known as
backward consistency of RLS [40,41]). Symmetry can be guaranteed by computing only the upper or
lower triangular part of P(n), and copying the result to obtain the rest of elements. However, it is also
necessary to guarantee positive-deﬁniteness. Reference [42] describes how this can be achieved.
Due to the fast convergence rate of RLS, a large literature is devoted to ﬁnding numerically stable and
low-cost (i.e., O(M)) versions of RLS. This is not an easy problem, that required the work of numerous
researchers to be solved. Nowadays the user has a very large number of different versions of RLS to
choose from. There are versions with formal proofs of stability, versions that work well in practice,
but without formal proofs, versions with O(M2) complexity, and versions with O(M) complexity. The
interested reader can ﬁnd more references in Section 1.12.5.1.2.
One approach to ensure the consistency of P(n) uses a property of symmetric and positive-deﬁnite
matrices (see Box 3): Cholesky factorizations. A symmetric and positive-deﬁnite matrix P(n) may

1.12.3 Stochastic Algorithms
687
always be factored as
P(n) = P(n)P H(n),
(12.168)
where P(n) is a lower triangular M ×M matrix, called Cholesky factor of P(n). Thus, an algorithm that
computes P(n) instead of P(n) can avoid the numerical instability of the conventional RLS algorithm.
There are many algorithms based on this main idea; for some of them a precise stability proof is available
[40]. A recent and very complete survey of QR-RLS algorithms is available in [43].
Another approach is available when the regressor sequence {φ(n)} has shift structure. In this case,
it is possible to derive lattice-based RLS ﬁlters that are in practice stable, although no formal proof is
available at this time, as described, for example, in [5].
A more practical solution for the implementation of the RLS algorithm was proposed recently in
[44]. This approach avoids propagation of the inverse covariance matrix, using an iterative method to
solve (12.155) at each step. The computational complexity is kept low by using the previous solution
as initialization, and by restricting the majority of the multiplications to multiplications by powers of
two (which can be implemented as simple shifts). See Section 1.12.5.1.3.
1.12.3.4 Comparing convergence rates
To compare the convergence of LMS, NLMS, and RLS, we show next two examples. In Example 1, we
consider a system identiﬁcation application and in Example 2, we use these algorithms to update the
weights of an equalizer.
As we will see in more detail in Section 1.12.4, adaptive ﬁlters are compared based on how well they
handle different classes of signals. For example, we could be interested in seeing how two ﬁlters handle
white noise, or how they handle a certain level of correlation at the input. Given the stochastic nature of
the inputs, the performance measures are deﬁned in terms of averaged cost functions, most commonly
the mean-square error (MSE), that is, the average of |e(n)|2 over all possible inputs in a given class.
These averages can be computed theoretically, as we show in Section 1.12.4, or via simulations.
For example, the MSE as a function of time, that is, the curve E{|e(n)|2} × n, is called a ﬁlter’s
learning curve. A comparison of learning curves obtained theoretically and from simulations can be
seen in Figure 12.30 further ahead. When simulations are used, one runs a ﬁlter L times for an interval
of N samples, starting always from the same conditions. For each run ℓ, the error e(ℓ)(n) is computed
for 1 ≤n ≤N, and one obtains the so-called ensemble-average learning curve
E(n) = 1
L
L

ℓ=1
|e(ℓ)(n)|2.
Usually the ensemble-average learning curve will be a reasonable approximation of the true learning
curve for a reasonably small value of L (say, from 50 to 1000), but in some situations this may not be
true—see [45,46] for examples and a detailed explanation.
Example 1.
The aim of this example is to identify the system wo = [1 −0.8]T , assuming that the
regressor φ(n) is obtained from a process x(n) generated with a ﬁrst-order autoregressive model, whose
transfer function is 1/(1 −0.8z−1). This model is fed with an i.i.d. Gaussian random process with
unitary variance. Moreover, additive i.i.d. noise v(n) with variance σ 2
v = 0.01 is added to form the
desired signal.

688
CHAPTER 12 Adaptive Filters
0
0
0.5
2
1
−0.5
−1.5
−1
−1
−2
−2
w0
w1
NLMS
LMS
RLS
FIGURE 12.25
LMS (μ = 0.01), NLMS ( ˜μ = 0.05, ε = 0.5) and RLS (λ = 0.99, δ = 1) initialized at w(0) =
[−1.5 −1.4]T , σ 2
v = 0.01. Click the animation video ﬁle.
Considering an adaptive ﬁlter with M = 2 coefﬁcients, we can obtain a contour plot of the mean-
square error cost as a function of w0 and w1 as shown in Figure 12.25. The animations in Figure 12.25
illustrate the behavior of LMS (μ = 0.01), NLMS ( ˜μ = 0.05, ε = 0.5), and RLS (λ = 0.99, δ = 1)
initialized at w(0) = [−1.5 −1.4]T . The correspondent curves of MSE along the iterations, estimated
from the ensemble-average of 1000 independent runs, are shown in Figure 12.26. As expected for a
colored input signal, RLS converges much faster than NLMS and LMS. NLMS converges faster than
LMS and achieves the solution through a different path. To obtain a good behavior of NLMS in this
example, we had to choose a relatively large regularization factor ε. This is always necessary when
NLMS is used with few coefﬁcients since in this case, ∥φ(n)∥2 has a high probability of becoming too
close to zero and de-stabilizing the algorithm. This is shown in the analysis of Section 1.12.4.
Example 2.
Now, we assume the transmission of a QPSK (quadrature phase shift-keying) signal,
whose symbols belong to the alphabet {±1 ± j1}. This signal suffers the effect of the channel (taken
from [47])
H(z) = (−0.2 + j0.3) + (−0.5 + j0.4)z−1 + (0.7 −j0.6)z−2
+(0.4 + j0.3)z−3 + (0.2 + j0.1)z−4
(12.169)
and of additive white Gaussian noise (AWGN) under a signal-to-noise ratio (SNR) of 30 dB. The
equalizer is an adaptive ﬁlter with M = 15 coefﬁcients initialized with zero and the desired signal is the
transmittedsequencedelayedby L = 9samples(gobacktoFigure12.16toseetheequalizationscheme).
The adaptation parameters (μ, ˜μ, ε, λ) were chosen in order to obtain the same steady-state performance
of LMS, NLMS, and RLS (note that with a longer ﬁlter, the regularization parameter ε can be much

1.12.4 Statistical Analysis
689
0
0
200
400
600
800
1000
−25
−20
−15
−10
−5
15
10
5
MSE (dB)
LMS
NLMS
RLS
iterations
FIGURE 12.26
MSE along the iterations for LMS (μ = 0.01), NLMS ( ˜μ = 0.05, ε = 0.5) and RLS (λ = 0.99, δ = 1)
initialized at w(0) = [−1.5 −1.4]T , σ 2
v = 0.01, and ensemble-average of 1000 independent runs.
smaller than in the previous example). Figure 12.27 shows the MSE along the iterations, estimated
from the ensemble-average of 1000 independent runs. Again, RLS presents the fastest convergence
rate, followed by NLMS and LMS. These algorithms achieve the same steady-state MSE and therefore,
present the same performance after convergence, which can be observed in Figure 12.28 where the
equalizer output signal constellations are shown. The Matlab ﬁle used in this simulation is available at
http://www.lps.usp.br.
1.12.4 Statistical analysis
Closed-form expressions for the mean-square performance of adaptive ﬁlters facilitate comparisons
between different algorithms and provide information about stability, convergence rate, mean-square
error, and tracking capability. Since adaptive algorithms are obtained from stochastic approximations
of the cost-functions, gradients and Hessians, their performance will degrade in comparison with the
performance of the exact algorithms. Thus, stochastic analyses are also important to measure the per-
formance degradation in relation to the exact algorithms.
The analysis of an adaptive algorithm constitutes a very hard problem, since adaptive ﬁlters are time-
variant, stochastic, and nonlinear systems. Therefore, it is common to introduce simplifying approxi-
mations. They consist in most cases of disregarding the dependence between variables, for example,
approximating the expected value of the product of two random variables, say, E{xy}, by the product
of the expected values, E{x}E{y}. For historical reasons, these approximations are usually referred to
as assumptions in the literature.

690
CHAPTER 12 Adaptive Filters
0
0
1000
2000
3000
4000
5000
−20
−15
−10
−5
5
MSE (dB)
LMS
NLMS
RLS
iterations
FIGURE 12.27
MSE along the iterations for LMS (μ = 10−3), NLMS ( ˜μ = 0.08, ε = 10−5), and RLS (λ = 0.997,
δ = 1) initialized at w(0) = 0; QPSK, L = 9, M = 15, and ensemble-average of 1000 independent runs;
equalization of the channel (12.169).
0
0
−1
−1
1
1
(a) LMS
Im{ˆy}
Re{ˆy}
0
0
−1
−1
1
1
(b) NLMS
Im{ˆy}
Re{ˆy}
0
0
−1
−1
1
1
(c) RLS
Im{ˆy}
Re{ˆy}
FIGURE 12.28
Equalizer output signal constellations after convergence for (a) LMS (μ = 10−3), (b) NLMS ( ˜μ = 0.08,
ε = 10−5), and (c) RLS (λ = 0.997, δ = 1); initialized at w(0) = 0; QPSK, L = 9, M = 15; equalization of
the channel (12.169).
In some situations, in particular for LMS and some of its variants, the approximations are proved to
converge to the exact solution in the limiting case of slow adaptation (μ →0 in the case of LMS), and
their predictions tend to be reasonable for the range of step-sizes used in practice. We will not reproduce
these proofs here, since they are in general very technical, but they can be found in [48–53] for the case
of LMS, and [39] for more general ﬁlters.

1.12.4 Statistical Analysis
691
The literature contains a few exact analytical results, but only for LMS, under very particular con-
ditions [54–56]. In addition, the complexity of these exact models grows very quickly with the number
of coefﬁcients, so they are only useful for short ﬁlters (up to about ﬁve coefﬁcients.).
The performance of an adaptive ﬁlter is usually described by evaluating the mean and the mean
square (variance) of the weight error vector and of the error signal e(n). This usually works very well
for slow adaptation (i.e., small step-size). However, for fast adaptation, the behavior of an adaptive ﬁlter
is considerably more complicated than can be seen by looking only at means and variances. Some of the
phenomena that appear in LMS with large step-sizes are described in [46,57]. References [45,58–60]
study algorithms with strong nonlinearities, for which the usual approximations are not always valid.
The analysis of NLMS was considered in [61–68]. Many works consider only tapped-delay line
regressors, the case of Volterra ﬁlters is considered for example in [69–73]. For RLS ﬁlters, important
models for inﬁnite-precision arithmetic can be found in [74,75]. The case of ﬁnite-precision arithmetic
is discussed in Section 1.12.5.1.2.
The real importance of the approximations used in the adaptive ﬁltering literature is that they lead
to reasonably simple models that capture well the overall behavior of an adaptive ﬁlter. Simple models
provide intuition about how a ﬁlter will behave if a given parameter is changed, which is important
for a designer to make good decisions. Some approximations are made because without them the
mathematical analysis would not be feasible, but others are used mainly because they lead to simpler
models. Of course, there are situations in which it is important to study a ﬁlter in more detail, reducing
the number of assumptions used in the analysis, as in the references just cited.
In this section we analyze three of the most common adaptive ﬁlters—LMS, NLMS, and RLS, starting
with LMS in Section 1.12.4.3, and then showing how to extend these results to the other algorithms
in Section 1.12.4.4. To start, we need to deﬁne some useful performance measures. The mean-square
error (MSE) is simply the expected value of |e(n)|2:
J(n) = E{|e(n)|2}.
(12.170)
The most widely measure of performance used in the literature of adaptive ﬁltering is the excess mean-
square error (EMSE), which is deﬁned as
ζ(n) ≜E{|ea(n)|2},
(12.171)
where the excess a priori error is given by
ea(n) = ˜wH(n)φ(n),
(12.172)
and the weight error vector by
˜w(n) = wo(n) −w(n).
(12.173)
Note that the optimal solution wo(n) is now allowed to be time-variant. Note also the difference between
ea(n) and e(n), deﬁned in (12.119). Both are a priori errors, but ea(n) is computed with ˜w(n), whereas
e(n) is computed with w(n). The EMSE measures how much J(n) exceeds its minimum value Jmin(n)
due to adaptation: if you were able to ﬁnd the optimum ﬁlter coefﬁcients wo(n) at each time, the EMSE
would be zero. Since the actual ﬁlter coefﬁcients are never exactly equal to the optimum values, the

692
CHAPTER 12 Adaptive Filters
EMSE measures the effect of this difference on the error variance. Closely related to the EMSE, the
misadjustment is deﬁned as
M(n) ≜ζ(n)/σ 2
v .
(12.174)
The excess a posteriori error ep(n) is mostly used in the analysis and derivations of algorithms. It
is the error obtained after using the current data φ(n) to update the coefﬁcient vector (thus the names a
priori and a posteriori), i.e.,
ep(n) = ˜wH(n + 1)φ(n).
(12.175)
Again, note the difference with respect to ξ(n), deﬁned in (12.129).
Another common performance measure is the mean-square deviation (MSD). It measures how far
w(n) is from wo(n) in average:
χ(n) = E{∥˜w(n)∥2}.
(12.176)
The EMSE is the most frequently performance measure used for comparisons between adaptive ﬁlters;
however in some applications, particularly for system identiﬁcation, the MSD is more adequate.
We now will discuss a general model for the input signals, considering a set of approximations that
leads to reasonable, yet simple models.
1.12.4.1 Data model
From the orthogonality principle (recall Section 1.12.2), the optimal solution wo(n) satisﬁes the property
E{φ(n)[d(n) −wH
o (n)φ(n)]} = 0,
(12.177)
i.e., the input regressor vector φ(n) is uncorrelated with the optimal estimation error
vo(n) ≜d(n) −wH
o (n)φ(n),
(12.178)
whose mean is zero and whose variance σ 2
v (n) = E{|vo(n)|2} is equal to the minimum cost Jmin(n).
Thus, a linear regression model for d(n) holds, i.e.,
d(n) = wH
o (n)φ(n) + vo(n).
(12.179)
It then follows that
e(n) = d(n) −wH(n)φ(n) = ˜wH(n)φ(n) + vo(n) = ea(n) + vo(n),
(12.180)
and the MSE is given by
J(n) = E{|e(n)|2} = E{|ea(n)|2} + 2E{ea(n)vo(n)} + E{|vo(n)|2} =
= ζ(n) + 2E{ea(n)vo(n)} + σ 2
v (n).
The ﬁrst approximation (or assumption) used in adaptive ﬁltering analysis is to disregard the cross-term
E{ea(n)vo(n)}. Since vo(n) and φ(n) are uncorrelated, but not necessarily independent, the cross-
term does not vanish in general. From observations, however, one notes that the cross-term is usually
negligible, compared to the other terms. In order to obtain a simple model for the adaptive ﬁlter, we will
assume that φ(n) and vo(n) are independent of each other, and of previous samples φ(n −k), v(n −k)
for k > 0:

1.12.4 Statistical Analysis
693
Assumption 1.
The sequences {φ(n)} and {vo(n)} are independent and identically distributed (iid), and
independent of each other (not only orthogonal). In addition, the sequence {φ(n)} is assumed stationary
and {vo(n)} is zero-mean.
The last two assumptions are not in fact necessary, but they simplify the analysis. In general the
sequence {φ(n)} is not iid: for example if φ(n) is formed from a tap-delay line or from a Volterra series
expansion (see Eqs. (12.113) and (12.114)) φ(n) and φ(n −1) will have terms in common. However,
assuming that the sequences are iid leads to simple models that in general are good approximations for
the range of step-sizes usually found in practice, in particular for small step-sizes. The analysis based
on Assumption 1 is known in the literature as independence theory.
Going back to the adaptive ﬁlter recursions (see Tables 12.2–12.5, and also Eq. (12.221) further
ahead) we see that the current weight error ˜w(n) is a function of past inputs φ(n −1), φ(n −2), . . . and
noise samples v(n −1), v(n −2), . . ., but not of the current samples φ(n) and vo(n). Therefore, under
Assumption 1, ˜w(n) is independent of vo(n) and φ(n), so that
E{ea(n)vo(n)} = E{ ˜wH(n)}E{φ(n)}E{vo(n)} = 0.
We conclude that under Assumption 1, the MSE J(n) = E{|e(n)|2} is related to the EMSE via
J(n) = ζ(n) + Jmin(n) = ζ(n) + σ 2
v ,
(12.181)
since Assumption 1 implies that Jmin(n) is a constant.
We also need a model for the variation of the optimal solution. Again, our choice is a model that cap-
tures the main consequences of a non-constant wo(n), without unnecessarily complicating the analysis.
Assumption 2 (Random-walk model).
In a nonstationary environment, wo varies as
wo(n + 1) = wo(n) + q(n),
(12.182)
where q(n) is zero-mean i.i.d vector with positive-deﬁnite autocorrelation matrix Q = E{q(n)q H(n)},
independent of the initial condition w(0), and of {φ(ℓ), v(ℓ)} for all ℓ.
The covariance matrix of wo, according to (12.182), grows slowly to inﬁnity. A more general model
for the variation of wo uses the recursion
wo(n + 1) = wo(n) + θ(n),
θ(n) = αθ(n −1) + q(n),
0 ≤|α| < 1,
whose covariance matrix remains bounded, but the analysis becomes more complicated [3]. Therefore,
it is common in the literature to adopt the model (12.182). Note that both models assume that {x(n)} is
a stationary random sequence, although wo(n) is allowed to vary with n.
1.12.4.2 Relating the covariance matrix of the weight-error vector to the EMSE
and the MSD
The covariance matrix of the weight-error vector is deﬁned as
S(n) ≜E

˜w(n) ˜wH(n)

.
(12.183)

694
CHAPTER 12 Adaptive Filters
This matrix is important in the statistical analysis of adaptive ﬁlters since it is related to the MSD and
the EMSE as we show next.
Recall that the MSD is given by
χ(n) = E

∥˜w(n)∥2
=
M−1

i=0
E

˜w2
i (n)

.
Now, the terms E{ ˜w2
i (n)} are exactly the terms that appear in the diagonal of S(n), therefore, recalling
that the trace of a square matrix is the sum of its main-diagonal elements (see Box 3), we obtain
χ(n) =
M−1

i=0
E{ ˜w2
i (n)} =
M−1

m=0
smm(n) = Tr(S(n)).
(12.184)
Under Assumption 1, φ(n) and ˜w(n) are independent, as we just saw. Therefore,
ζ(n) = E{| ˜wH(n)φ(n)|2} = E{ ˜wH(n)φ(n)( ˜wH(n)φ(n))∗}
= E{ ˜wH(n)φ(n)φH(n) ˜w(n)} = E

E{ ˜wH(n)φ(n)φH(n) ˜w(n)| ˜w(n)}

= E

˜wH(n)E{φ(n)φH(n)| ˜w(n)} ˜w(n)

= E{ ˜wH(n)Rφ ˜w(n)},
where we used Assumption 1 to write E{φ(n)φH(n)} = Rφ (constant), and two properties of expected
values. The ﬁrst is that, for any two random variables a, b,
E{ab} = E {E{ab|b}} = E {E{a|b}b} ,
where the inner expected value is taken with respect to a, and the second with respect to b. The second
property is that, if a and b are independent, then E{a|b} does not depend on b, i.e., E{a|b} = E{a}.
To proceed, we apply a useful trick from matrix theory. You can check by direct computation that,
if A ∈CK×L and B ∈CL×K , then Tr(AB) = Tr(B A). Applying this property with A ←˜wH(n) and
B ←Rφ ˜w(n), we obtain
ζ(n) = Tr

RφE{ ˜w(n) ˜wH(n)}
 
= Tr

Rφ S(n)

.
(12.185)
Since Rφ is symmetric, there always exists an orthogonal matrix U (i.e., U−1 = U H) that diago-
nalizes Rφ, that is,
U H RφU =
⎡
⎢⎢⎢⎣
λ1
0
· · ·
0
0
λ2
· · ·
0
...
...
...
...
0
0
0
λM
⎤
⎥⎥⎥⎦
= ,
(12.186)
where λ1, λ2, . . . , λM are the eigenvalues of Rφ (see Box 3). Deﬁning the rotated matrix
S′(n) ≜U H S(n)U

1.12.4 Statistical Analysis
695
and recalling that UU H = I, (12.184) and (12.185) can be rewritten respectively as
χ(n) = Tr(U U H S(n)U



S′(n)
U H) = Tr(U S′(n)U H)
(12.187)
and
ζ(n) = Tr(U U H RφU




U H S(n)U



S′(n)
U H) = Tr(US′(n)U H).
(12.188)
Using again the fact that Tr(AB) = Tr(B A), with A ←U, we obtain
χ(n) = Tr

S′(n)

(12.189)
and
ζ(n) = Tr

S′(n)

.
(12.190)
Note that both the MSD (12.189) and the EMSE (12.190) depend only on the diagonal entries of S′(n).
We can work therefore only with these diagonal entries, deﬁning the vectors
s′(n) = diag(S′(n)) =

s′
0(n)s′
1(n) · · · s′
M−1(n)
T ,
(12.191)
and
l =

λ0, . . . , λM−1
T ,
(12.192)
where diag(A) represents a column vector with the diagonal elements of A, and evaluating the MSD
and EMSE respectively as
χ(n) =
M−1

k=0
s′
k(n)
(12.193)
and
ζ(n) = lT s′(n) =
M−1

k=0
λks′
k(n).
(12.194)
Again,theMSDandEMSEdependonlyontheelementsofthediagonaloftherotatedmatrix S′(n).Inthe
case of EMSE, these elements are weighed by the eigenvalues of the autocorrelation matrix Rφ. As we
shall see in Section 1.12.4.3, the stability of LMS can be studied through a recursion for the vector s′(n).
Thus, according to the previous results, in order to obtain closed-form expressions for the MSD and
the EMSE of an adaptive algorithm, one path is to ﬁnd a recursion for the covariance matrix S(n) or
for the vector s′(n). However, this is not the only method to analyze adaptive algorithms, as we shall
see in Section 1.12.4.4.

696
CHAPTER 12 Adaptive Filters
1.12.4.3 Statistical analysis of the LMS algorithm
In this section, we present a statistical analysis for the LMS algorithm, assuming that the optimal solution
wo remains constant (q(n) ≡0, for all n). This analysis predicts the behavior of LMS in steady-state
and in transient, and provides a range of the step-size μ for which LMS operates adequately. If you are
familiar with the LMS analysis, go to Section 1.12.4.4, where we present a uniﬁed analysis for LMS,
NLMS, and RLS algorithms in a nonstationary environment.
We ﬁrst rewrite (12.123) in terms of the weight-error vector ˜w. Thus, subtracting both sides of
(12.123) from wo, we obtain
wo −w(n + 1) = wo −w(n) −μφ(n)e∗(n),
˜w(n + 1) = ˜w(n) −μφ(n)e∗(n).
(12.195)
Replacing e∗(n) = e∗
a(n) + v∗
o(n) = φH(n) ˜w(n) + v∗
o(n) in (12.195), we arrive at
˜w(n + 1) =

I −μφ(n)φH(n)

˜w(n) −μφ(n)v∗
o(n).
(12.196)
First-order analysis. Taking expectations on both sides of (12.196), we obtain
E{ ˜w(n + 1)} = E

I −μφ(n)φH(n)

˜w(n)

−μE{φ(n)v∗
o(n)}
= E

I −μφ(n)φH(n)

˜w(n)

,
(12.197)
where the last equality follows from the fact that φ(n) is orthogonal to v∗
o(n), according to the orthog-
onality condition (see Eq. (12.96)). Since ˜w(n) is assumed to be independent of φ(n) (see Assumption
1), we arrive at
E{ ˜w(n + 1)} =

I −μRφ

E{ ˜w(n)}.
(12.198)
This is the same recursion we saw in Box 1. According to linear systems theory, this recursion converges
as long as the largest eigenvalue of A = I −μRφ has absolute value less than one, which implies (see
Box 1)
|1 −μλ| < 1 ⇔0 < μ < 2
λk
,
∀k ⇔
0 < μ <
2
λmax
,
(12.199)
where λk, k = 1, 2, . . . , M are the eigenvalues of Rφ, and λmax is its maximum eigenvalue. We should
notice that this range for μ does not ensure the stability of LMS, since it ensures the convergence of
the mean of ˜w(n) = wo −w(n) towards 0, but the autocovariance of ˜w(n) may still be diverging. We
show next that the range for μ based on a second-order analysis is indeed more restrict.

1.12.4 Statistical Analysis
697
Second-order analysis. Now, we use (12.196) to ﬁnd a recursion for S(n) and (12.185) to evaluate the
EMSE of LMS. Multiplying (12.196) by its Hermitian, taking the expectations of both sides, we arrive at
E{ ˜w(n + 1) ˜wH(n + 1)} = E{ ˜w(n) ˜wH(n)}
−
A



μE{ ˜w(n) ˜wH(n)φ(n)φH(n)}
−
B



μE{φ(n)φH(n) ˜w(n) ˜wH(n)}
+
C



μ2E{φ(n)φH(n) ˜w(n) ˜wH(n)φ(n)φH(n)}
+
D



μ2E{|vo(n)|2φ(n)φH(n)}
−
E



μE{vo(n) ˜w(n)φH(n)}
−
F



μE{v∗
o(n)φ(n) ˜wH(n)}
+
G



μ2E{vo(n)φ(n)φH(n) ˜w(n)φH(n)}
+
H



μ2E{v∗
o(n)φ(n) ˜wH(n)φ(n)φH(n)} .
(12.200)
Now, using Assumption 1, we can evaluate the terms A −H of (12.200):
A- Recalling that Assumption 1 implies that ˜w(n) is independent from φ(n), the term A can be
approximated by
A = μE

E

˜w(n) ˜wH(n)φ(n)φH(n)|φ(n)

≈μE

E

˜w(n) ˜wH(n)

φ(n)φH(n)

= μS(n)E

φ(n)φH(n)

= μS(n)Rφ.
(12.201)
B- Analogously, we obtain for B
B ≈μRφ S(n).
(12.202)
C- Using Assumption 1, the term C can be rewritten as
C = μ2E

φ(n)φH(n) ˜w(n) ˜wH(n)φ(n)φH(n)

≈μ2E

φ(n)φH(n)S(n)φ(n)φH(n)

.
(12.203)
We return to this term further on.

698
CHAPTER 12 Adaptive Filters
D- Using the fact that vo(n) is independent of φ(n) and ˜w(n), D reduces to
D = μ2E{|vo(n)|2φ(n)φH(n)} ≈μ2σ 2
v Rφ.
(12.204)
E- Using the fact that vo(n) is a zero-mean random variable, which is independent of φ(n) and
˜w(n), E is a M × M null matrix. Using the same arguments, the terms F, G, and H are also null
matrices. We should point out that the zero-mean assumption for vo(n) is not necessary, but it really
facilitates the analysis. Furthermore, as we saw in Section 1.12.2.1.4, we can enforce E{vo(n)} = 0
using simple methods.
From the previous results, (12.200) reduces to
S(n + 1) ≈S(n) −μS(n)Rφ −μRφ S(n)
+ μ2E

φ(n)φH(n)S(n)φ(n)φH(n)




C
+μ2σ 2
v Rφ.
(12.205)
This recursion is convenient to obtain closed-form expressions for the EMSE and the MSD of LMS.
Observe that the four ﬁrst terms on the right-hand side of (12.205) are linear in S(n). Assuming that μ
is sufﬁciently small, the term C can be neglected with respect to the three ﬁrst terms. Thus, for small
step-sizes, (12.205) reduces to
S(n + 1) ≈S(n) −μ

S(n)Rφ + Rφ S(n)

+ μ2σ 2
v Rφ,
(12.206)
with initialization S(0) =

wo −w(0)
 
wo −w(0)
H.
Since we can obtain the EMSE and the MSD from S(n) and Rφ through (12.185) and (12.184), we
should use recursion (12.206) to compute S(n) and then, evaluate ζ(n) = Tr(S(n)Rφ) and χ(n) =
Tr(S(n)). Particularly for n →∞, since S(n + 1) ≈S(n), we obtain
S(∞)Rφ + Rφ S(∞) = μσ 2
v Rφ.
(12.207)
Taking the trace of both sides of (12.207), we arrive at an analytical expression for the steady-state
EMSE of LMS, i.e.,
ζ LMS(∞) ≈μσ 2
v Tr(Rφ)
2
(for sufﬁciently small μ).
(12.208)
This expression indicates that the EMSE of LMS increases with μ and with the length of the ﬁlter. The
dependence with the ﬁlter length is through the trace of Rφ. It is more evident when the class of ﬁlters
we are considering is FIR, that is, when φ(n) =
 x(n) . . . x(n −M + 1)T (a tapped-delay line). In
this case, if {x(n)} is stationary, then Tr(Rφ) = ME{x2(n)}, M times the average power of x(n).
Turning now to the ﬁlter coefﬁcients, if we multiply both sides of (12.207) from the right by R−1
φ ,
taking the trace of both sides, and recalling that Tr(AB) = Tr(B A), we obtain an expression for the
steady-state MSD of LMS, i.e.,
χLMS(∞) ≈μσ 2
v M
2
(for sufﬁciently small μ).
(12.209)

1.12.4 Statistical Analysis
699
Again, the smaller the step-size μ and the ﬁlter length M, the smaller the MSD of LMS.
These results explain what we observed in the simulations in Section 1.12.1.2 (Figures 12.9 and
12.10): for larger values of μ the ﬁlter converges faster, but hovers around the optimum solution in a
larger region. The size of this region is what the MSD attempts to measure—the EMSE measures how
much this hovering affects the error.
These quantities are important in practical situations. For example, in an actual implementation of an
acoustic echo canceler, when the near-end user is speaking, his voice is part of v(n). Since the near-end
speech is normally stronger than the echo, σ 2
v tends to be quite large when the near-end speaker is
talking. If the adaptive ﬁlter were allowed to keep adapting at these moments, (12.208) shows that the
ﬁlter estimates would wander too far from the optimum. To avoid this problem, the step-size is reduced
to zero whenever the near-end user is speaking. A double-talk detector is used to decide when to stop
adaptation.
It can be shown that (12.206) is stable for sufﬁciently small μ. However, (12.206) cannot be used
to ﬁnd a range of values of μ that guarantee stability, due to the approximations made, particularly the
discarding of term C. Thus, in order to study the stability of LMS, we consider next an approximation
for this term, based the assumption that the input vector is Gaussian. This assumption allows us to
approximate C as
C ≈RφTr(S(n)Rφ) + β Rφ S(n)Rφ,
(12.210)
where as usual, β = 1 (resp., β = 2) for complex (resp., real) data. See Box 7 for the derivation of
(12.210).
Replacing (12.210) in (12.205), we arrive at
S(n + 1) ≈S(n) −μ

S(n)Rφ + Rφ S(n)

+μ2 
RφTr

S(n)Rφ

+ β Rφ S(n)Rφ + σ 2
v Rφ

.
(12.211)
The stability of (12.211) is determined through a recursion for the vector s′(n), which contains the
elements of the diagonal of the rotated matrix S′(n) = U H S(n)U (see Section 1.12.4.2). A recursion
for S′(n) can be obtained by multiplying (12.211) from the left by U H and from the right by U and
recalling that UU H = I, which leads to
S′(n + 1) ≈S′(n) −μ

S′(n) + S′(n)

+μ2 
Tr(S′(n)) + βS′(n)

+ μ2σ 2
v ,
(12.212)
where  is deﬁned as in (12.186). Thus, a recursion for s′(n + 1) is given by
s′(n + 1) =

I −2μ + μ2 
β2 + llT  
s′(n) + μ2σ 2
v l,
(12.213)
where l is deﬁned in (12.192).
The system matrix A of (12.213) is given by
A = I −2μ + μ2 
β2 + llT  
=

I −μ
2 + μ2(β −1)2 + μ2llT .

700
CHAPTER 12 Adaptive Filters
Although it is possible to obtain the exact values of all eigenvalues of A, as done in [36], we follow here
a much simpler, approximate route. We can use Fact 7 in Box 3 to ﬁnd an upper bound for the largest
eigenvalue νi of A, using the 1-norm of A:
max
0≤i≤M−1 |νi| ≤∥A∥1 =
max
0≤i≤M−1
M−1

j = 0
|ai j|,
where ai j are the entries of A. If we ﬁnd a range of μ for which ∥A∥1 ≤1, then the recursion will be
stable. This will be of course a conservative condition (sufﬁcient, but not necessary).
Now note that the ith column of A has entries λiλ j if i ̸= j, and (1−μλi)2 +μ2(β −1)λ2
i +μ2λ2
i
in the term corresponding to the diagonal. Since all terms are positive, the 1-norm is
max
0≤i≤M−1
⎡
⎣(1 −μλi)2 + μ2(β −1)λ2
i + μ2λi
M−1

j = 0
λ j
⎤
⎦
=
max
0≤i≤M−1

(1 −μλi)2 + μ2(β −1)λ2
i + μ2λiTr()

.
Recall that Tr() = Tr(Rφ) (Fact 1 from Box 3). Therefore, the recursion will be stable if
1 −2μλi + μ2λ2
i + μ2(β −1)λ2
i + μ2λiTr(Rφ) ≤1,
0 ≤i ≤M −1.
Simplifying, we obtain the condition
μ ≤
2
βλi + Tr(Rφ),
0 ≤i ≤M −1.
The smallest bound is for λi = λmax:
μ ≤
2
βλmax + Tr(Rφ).
(12.214)
If we replace λmax by Tr(Rφ) ≥λmax in the denominator, we obtain a simpler, but more conservative
condition:
μ ≤
2
(β + 1)Tr(Rφ).
(12.215)
For real data, this range reduces to
0 < μ <
2
3Tr(Rφ)
(real data)
(12.216)
and for complex data to
0 < μ <
1
Tr(Rφ)
(complex data).
(12.217)

1.12.4 Statistical Analysis
701
Table 12.6 Parameters of LMS, NLMS, and RLS Algorithms
Algorithm
ρ(n)
M−1(n)
LMS
μ
I
NLMS
˜μ
ε + ∥φ(n)∥2
I
RLS
1 −λ
(1 −λ)
n
ℓ=1
λn−ℓφ(ℓ)φH(ℓ)
If the input is a tapped-delay line as in (12.113), Tr(Rφ) = M−1
k=0 E{|x(n −k)|2}. If {x(n)} is a
stationary sequence, the stability condition for LMS is easy to obtain: we need to choose μ in the range
0 < μ <
2
(β + 1) × M × (average power of x(n)).
(12.218)
This concludes our stability analysis for LMS. We now show how to extend these results to NLMS
and RLS, in a uniﬁed way. At the end of the next section, we give examples comparing the models to
simulated results.
1.12.4.4 A uniﬁed statistical analysis
In this section, we assume a nonstationary environment, where the optimal solution varies according
to the random walk model (see Assumption 2). We start by obtaining a general update equation for the
LMS, NLMS, and RLS algorithms, in order to provide a uniﬁed statistical analysis for these algorithms.
1.12.4.4.1
A general update equation
In order to provide a uniﬁed analysis for the LMS, NLMS and RLS algorithms, we consider the following
general update equation
w(n + 1) = w(n) + ρ(n)M(n)φ(n)e∗(n),
(12.219)
where ρ(n) is a step-size, M(n) is a non-singular matrix with hermitian symmetry (M H(n) = M(n)),
and e(n) = d(n) −wH(n)φ(n) is the estimation error. Many adaptive algorithms can be written as
in (12.219), by proper choices of ρ(n) and M(n). The LMS, NLMS, and RLS algorithms employ the
step-sizes ρ(n) and the matrices M(n) as in Table 12.6, where I is the M × M identity matrix, μ and
0 < ˜μ < 2 are step-sizes, and 0 ≪λ < 1 is a forgetting factor. For RLS, M(n) = (1 −λ)−1 P(n) (see
Eq. (12.162)). Note that, since (1 −λ) plays the role of step-size in RLS, we multiply the matrix P(n)
by (1 −λ)−1, keeping the update equation of the RLS weights as in (12.167).
We must rewrite (12.219) in terms of the weight-error vector ˜w. Thus, subtracting both sides of
(12.219) from wo(n + 1) = wo(n) + q(n) (Eq. (12.182)), we obtain
wo(n + 1) −w(n + 1) = wo(n) + q(n) −w(n) −ρ(n)M(n)φ(n)e∗(n),
˜w(n + 1) = ˜w(n) −ρ(n)M(n)φ(n)e∗(n) + q(n).
(12.220)

702
CHAPTER 12 Adaptive Filters
Replacing e∗(n) = e∗
a(n) + v∗(n) = φH(n) ˜w(n) + v∗(n) in (12.220), we arrive at
˜w(n + 1) =

I −ρ(n)M(n)φ(n)φH(n)

˜w(n) −ρ(n)M(n)φ(n)v∗(n) + q(n).
(12.221)
We next use recursion (12.221) to evaluate the EMSE and the MSD of LMS, NLMS, and RLS.
1.12.4.4.2
Alternative analysis methods
Closed-form expressions for the EMSE and MSD can be obtained in two ways:
i. Finding a recursion for the correlation matrix of the weight-error vector, denoted by S(n). The
EMSE and MSE can be easily computed given S(n), as we saw in Section 1.12.4.2. This is the
earlier approach in the literature [1,2] and therefore, we refer to it here as traditional method.
ii. Using the energy conservation analysis of [3,76–78]. The energy conservation approach relies
on an energy conservation relation that holds for a large class of adaptive ﬁlters. The energy
conservation method presents some advantages when compared to the traditional one. For example,
using energy conservation, one can obtain steady-state results directly, bypassing several of the
difﬁculties encountered in obtaining them as a limiting case of a transient analysis. This method
can also be used in transient analyses of adaptive algorithms (see, e.g., Part V of [3]).
Itisimportanttounderstandthatbothmethodsinvolvedifferentsetsofapproximations(assumptions),
so they do not always arrive at the same expressions. Generally, they tend to be equal in the limit of slow
adaptation (i.e., if ρ(n) →0 in (12.221)). For larger values of ρ(n), which method will provide the
best approximation depends on the algorithm. As for the work necessary to arrive at the ﬁnal answers,
the energy conservation approach is almost always easier to use if one is interested only in steady-state
results. If one is interested in transient performance and stability studies, then the traditional method
may be simpler, depending on the algorithm under study.
To give an overview of both methods, we ﬁrst use the traditional method to obtain the transient and
steady-state performance of the LMS, NLMS, and RLS algorithms. This is an extension of what we did
for LMS in Section 1.12.4.3, taking advantage of the similarities between the algorithms. In the sequel,
we employ the energy conservation method to obtain closed-form expressions for the steady-state EMSE
of the same algorithms.
In this section we allow the environment to be nonstationary, i.e., the optimal solution wo is time-
variant. The results for stationary environments can be obtained simply by making Q = 0 in the
equations below. In order to simplify the arguments, we require that the regressor sequence {φ(n)} be
wide-sense stationary, that is, we assume that Rφ is constant.
1.12.4.4.3
Analysis with the traditional method
The traditional analysis method consists of ﬁnding a recursion for S(n), and using (12.184) and (12.185)
to evaluate the MSD and the EMSE, respectively. A recursion for S(n) can be obtained by multiply-
ing (12.221) by its Hermitian, taking the expectations of both sides, and recalling that Assumption 1
implies that ˜w(n) is independent of φ(n) and of the zero-mean vo(n). Furthermore, using the fact that

1.12.4 Statistical Analysis
703
E{ ˜w(n)q H(n)} = 0 since the sequence {q(n)} is iid (Assumption 2), we arrive at
E{ ˜w(n + 1) ˜wH(n + 1)} = E{q(n)q H(n)} + E{ ˜w(n) ˜wH(n)}
−
A



E{ρ(n) ˜w(n) ˜wH(n)φ(n)φH(n)M(n)}
−
B



E{ρ(n)M(n)φ(n)φH(n) ˜w(n) ˜wH(n)}
+
C



E{ρ2(n)M(n)φ(n)φH(n) ˜w(n) ˜wH(n)φ(n)φH(n)M(n)}
+
D



E{ρ2(n)|vo(n)|2M(n)φ(n)φH(n)M(n)} .
(12.222)
To simplify (12.222), we also need two additional assumptions:
Assumption 3.
Matrix M(n) varies slowly in relation to ˜w(n). Thus, when M(n) appears inside the
expectations of (12.222), we simply replace it by its mean. For LMS and NLMS, this assumption is not
necessary, since M(n) = I. For RLS, M(n) = (1 −λ)−1 P(n) ≈(1 −λ)−1E{P(n)} if λ ≈1;
Assumption 4.
The input regressor vector φ(n) is assumed to be Gaussian. This assumption makes
the analysis more tractable, since Gaussianity facilitates the computation of the expected values in C.
Now, using the Assumptions 1–4, we can evaluate the terms A-D of (12.222):
A- The term A can be approximated by
A = E

E

ρ(n) ˜w(n) ˜wH(n)φ(n)φH(n)M(n)|φ(n)

≈E

ρ(n)E

˜w(n) ˜wH(n)

φ(n)φH(n)

E {M(n)}
= S(n)E

ρ(n)φ(n)φH(n)

E {M(n)} .
(12.223)
For LMS, this term reduces to
ALMS ≈μS(n)Rφ.
(12.224)
For NLMS, we must obtain an approximation for
E

ρ(n)φ(n)φH(n)

= E
'
˜μφ(n)φH(n)
ε + φH(n)φ(n)
(
.
(12.225)
It is possible to evaluate this expected value exactly (see [61,63,64]), but our intention here is to
obtain the simplest possible model. Therefore, we will use two additional approximations:
Assumption 5.
The number of coefﬁcients M is large enough for each element φ(n)φH(n) in
the numerator to be approximately independent from the denominator M−1
k=0 |φ(n −k)|2.

704
CHAPTER 12 Adaptive Filters
This is equivalent to apply the averaging principle proposed in [79], since for large M, ∥φ(n)∥2 tends
to vary slowly compared to the individual entries of φ(n)φH(n).
Thus, (12.225) can be approximated as
E
'
φ(n)φH(n)
ε + φH(n)φ(n)
(
≈E

1
ε + φH(n)φ(n)

E

φ(n)φH(n)

.
(12.226)
Under Assumption 5, the ﬁrst expected value in (12.226) can by approximated by:
E

1
ε + φH(n)φ(n)

≈
1
E{ε + φH(n)φ(n)}
=
1
ε + Tr(Rφ).
(12.227)
An alternative, more accurate approximation is obtained when φ(n) is Gaussian and a tapped-delay
line:
Assumption 6.
The regressor φ(n) is formed by a tapped-delay line with Gaussian entries as in
(12.113), and ε = 0.
Under Assumptions 4, 5, and 6 it can be shown (see [65] for details) that
E

1
ε + φH(n)φ(n)

≈
1
σ 2x (M −2).
(12.228)
Both approximations are only valid for long ﬁlters. However, (12.228) shows that NLMS needs a
large regularization constant if M < 3, which is veriﬁed in practice [63]. We will present the ﬁnal
results for NLMS in terms of Assumption 6, but recall that a less precise, but more general option is
available using only Assumption 4 and approximations such as (12.227).
Thus, the term A for NLMS reduces to (using (12.228))
ANLMS ≈
˜μ
σ 2x (M −2) S(n)Rφ.
(12.229)
Finally, for RLS, we obtain
ARLS ≈S(n)Rφ P(n),
(12.230)
where P(n) ≜E{R−1
φ (n)}, which we will evaluate later on.
B- Analogously, we obtain for B
B ≈E {M(n)} E

ρ(n)φ(n)φH(n)

S(n).
(12.231)
Particularizing for each algorithm, we have
BLMS ≈μRφ S(n),
(12.232)
BNLMS ≈
˜μ
σ 2x (M −2) Rφ S(n),
(12.233)
and
BRLS ≈P(n)Rφ S(n).
(12.234)

1.12.4 Statistical Analysis
705
C- The term C can be approximated by
C = E

ρ2(n)M(n)φ(n)φH(n) ˜w(n) ˜wH(n)φ(n)φH(n)M(n)

≈E {M(n)} E

ρ2(n)φ(n)φH(n)S(n)φ(n)φH(n)

E {M(n)} .
(12.235)
Under the Gaussianity assumption 4, it holds that (see Box 7)
E

φ(n)φH(n)S(n)φ(n)φH(n)

= RφTr(S(n)Rφ) + β Rφ S(n)Rφ,
(12.236)
where β = 1 (resp., β = 2) for complex (resp. real) data. Thus, for LMS we have
CLMS ≈μ2 
RφTr(S(n)Rφ) + β Rφ S(n)Rφ

.
(12.237)
For NLMS, using the same arguments to get (12.226), the following approximation holds for large M:
E

ρ2(n)φ(n)φH(n)S(n)φ(n)φH(n)

≈˜μ2E
'
1

φH(n)φ(n)
2
(
E

φ(n)φH(n)S(n)φ(n)φH(n)

.
(12.238)
Under Assumption 5, the ﬁrst expectation on the r.h.s. of (12.238) is given by
E
'
1

φH(n)φ(n)
2
(
=
1
ε + Tr(Rφ) + βTr(R2
φ).
(12.239)
On the other hand, under Assumption 6, we obtain [65]
E
'
1

φH(n)φ(n)
2
(
=
1
σ 4x (M −2)(M −4).
(12.240)
Replacing (12.240) and (12.236) in (12.238), we arrive at
CNLMS ≈
˜μ2
σ 4x (M −2)(M −4)

Rφ Tr(S(n)Rφ) + β Rφ S(n)Rφ

.
(12.241)
Finally, for RLS we obtain
CRLS ≈P(n)

Rφ Tr(S(n)Rφ) + β Rφ S(n)Rφ

P(n).
(12.242)
D- Using the fact that vo(n) is independent of φ(n) and ˜w(n), D reduces to
D = E{ρ2(n)|vo(n)|2M(n)φ(n)φH(n)M(n)}
≈σ 2
v E{M(n)}E{ρ2(n)φ(n)φH(n)}E{M(n)}.
(12.243)

706
CHAPTER 12 Adaptive Filters
Particularizing for each algorithm, we obtain
DLMS ≈μ2σ 2
v Rφ(n),
(12.244)
DNLMS ≈
˜μ2
σ 4x (M −2)(M −4)σ 2
v Rφ(n),
(12.245)
where we could also have used (12.239), and
DRLS ≈σ 2
v P(n)Rφ P(n).
(12.246)
From the previous results, (12.222) reduces to
S(n + 1) ≈S(n) −E{ρ(n)}

S(n)RφE{M(n)} + E{M(n)}Rφ S(n)

+E{ρ2(n)}E{M(n)}

Rφ Tr(S(n)Rφ) + β Rφ S(n)Rφ

E{M(n)}
+σ 2
v E{ρ2(n)}E{M(n)}RφE{M(n)} + Q
(12.247)
with initialization S(0) =

wo(0) −w(0)
 
wo(0) −w(0)
H.
For RLS, we still need an approximation for P(n) = E{P(n)}. Initializing the estimate of the
autocorrelation matriz with Rφ(−1) = δI, we will have at iteration n
Rφ(n) = λnδI +
n

ℓ=1
λn−ℓφ(ℓ)φH(ℓ).
(12.248)
Taking the expectations of both sides of (12.248), we obtain
E
$Rφ(n)
%
= λnδI + Rφ
1 −λn
1 −λ .
(12.249)
For λ ≈1, the variance of Rφ(n) is small, and we can approximate P(n) ≈

E
$Rφ(n)
%−1, so that
P(n) ≈

λnδI + Rφ
1 −λn
1 −λ
−1
.
(12.250)
This approximation is good for large n and in steady-state. During the initial phases of the transient,
however, the approximation will only be reasonable if δ is large enough to make the last term in (12.249)
small compared to λnδI. This is because for n < M, the term n
ℓ=1 λn−ℓφ(ℓ)φH(ℓ) in (12.248) will
be singular, and thus will not be well approximated by (1 −λn)Rφ/(1 −λ).
Using the previous approximations, (12.247) can be particularized for the LMS, NLMS, and RLS
algorithms as shown in Table 12.7. Since we can obtain the EMSE and the MSD from S(n) and Rφ
through (12.185) and (12.184), we should use the recursions of Table 12.7 to compute S(n) and then
evaluate ζ(n) = Tr(S(n)Rφ) and χ(n) = Tr(S(n)).
Example 3.
To verify the accuracy of the expressions of Table 12.7, we consider a system identiﬁcation
application in a stationary environment ( Q = 0). The optimal solution is a lowpass FIR ﬁlter with linear

1.12.4 Statistical Analysis
707
Table 12.7 Recursions for Covariance Matrix S(n +1). The Expressions for NLMS Assume that
the Regressor is a Tapped-Delay Line. Alternative Expressions for NLMS are Available Using
(12.227) and (12.239)
Algorithm
S(n + 1)
LMS
S(n + 1) ≈S(n) −μ

S(n)Rφ + RφS(n)

+μ2 
RφTr

S(n)Rφ

+ βRφS(n)Rφ + σ 2
v Rφ

+ Q
NLMS
S(n + 1) ≈S(n) −
˜μ
σ 2x (M −2)

S(n)Rφ + RφS(n)

+
˜μ2
σ 4x (M −2)(M −4)

RφTr

S(n)Rφ

+ βRφS(n)Rφ + σ 2
v Rφ

+ Q
RLS
S(n + 1) ≈S(n) −

S(n)RφP(n) + P(n)RφS(n)

+P(n)

RφTr

S(n)Rφ

+ βRφS(n)Rφ + σ 2
v Rφ

P(n) + Q
phase, whose coefﬁcients are shown in Figure 12.29. The regressor φ(n) is obtained from a process
x(n) generated with a ﬁrst-order autoregressive model, whose transfer function is
√
1 −b2/(1−bz−1).
Note that the sequence {φ(n)} is not iid. This model is fed with an iid Gaussian random process, whose
variance is such that Tr(Rφ) = 1. Moreover, additive iid noise vo(n) with variance σ 2
v = 0.01 is added
to form the desired signal.
Figure 12.30 shows the EMSE for RLS (λ = 0.995, δ = 1), NLMS ( ˜μ = 0.1, ε = 10−5), and
LMS (μ = 0.05), estimated from the ensemble-average of 2000 independent runs. The dashed lines
represent the theoretical EMSE computed as ζ(n) = Tr(S(n)Rφ). We can observe a good agreement
between theory and simulation.
Figure 12.31 shows the EMSE of RLS (λ = 0.995), assuming different values for δ. We can observe
that the theoretical EMSE of RLS presents a good agreement with simulation during the initial phases
of the transient, but only for large values of δ due to the approximation (12.250).
1.12.4.4.4
Steady-state analysis with the energy conservation method
This method (also known as feedback approach [78,80]) is based on an energy conservation relation that
holds for a large class of adaptive ﬁlters. To obtain this relation for the algorithms of the form (12.219),
we ﬁrst assume a stationary environment (q(n) ≡0).

708
CHAPTER 12 Adaptive Filters
5
10
15
20
25
−0.1
0
0
0.1
0.2
0.3
0.4
0.5
amplitude
sample
FIGURE 12.29
Impulse response wo of the unknown lowpass ﬁlter.
2000
0
1000
3000
4000
5000
−15
−20
−25
−30
−35
−40
iterations
EMSE (dB)
RLS
LMS
NLMS
FIGURE 12.30
EMSE for RLS (λ = 0.995, δ = 1), NLMS ( ˜μ = 0.1, ε = 10−5), and LMS (μ = 0.05); b = 0.8, σ 2
v =
10−2, σ 2
q = 0; mean of 2000 independent runs. The dashed lines represent the predicted values of ζ(n)
for each algorithm.
Recall that the excess a posteriori error is given by
ep(n) = ˜wH(n + 1)φ(n).
(12.251)
Then, we multiply both sides of the Hermitian of (12.220) (with q(n) ≡0) from the right by φ(n) to
obtain
˜wH(n + 1)φ(n) = ˜wH(n)φ(n) −ρ(n)φH(n)M(n)φ(n)e(n).
(12.252)
Using (12.172) and (12.251), (12.252) reduces to
ep(n) = ea(n) −ρ(n)∥φ(n)∥2
M(n)e(n),
(12.253)

1.12.4 Statistical Analysis
709
−10
0
0
1000
2000
5000
−20
−30
−40
iterations
EMSE(dB)
(a) δ = 10−5
−10
0
300 0
4000
0
1000
2000
3000
4000
5000
−20
−30
−40
iterations
EMSE(dB)
(b) δ = 10−2
−10
0
1000
2000
3000
4000
5000
−15
−20
−25
−30
−35
−40
iterations
EMSE(dB)
(c) δ = 102
−10
0
1000
2000
3000
4000
5000
−15
−20
−25
−30
−35
−40
iterations
EMSE(dB)
(d) δ = 105
FIGURE 12.31
EMSE for RLS considering λ = 0.995 and different values of δ; b = 0.8, σ 2
v = 10−2, σ 2
q = 0; mean of
2000 independent runs. The dashed lines represent the predicted values of ζ(n) for each algorithm.
where ∥u∥2
A = uH Au stands for the Euclidean norm of u weighted by the matrix A. Using (12.253)
to eliminate e(n) in (12.220) (with q(n) ≡0) and assuming that φ(n) ̸= 0, we get
˜w(n + 1) + M(n)φ(n)
∥φ(n)∥2
M(n)
e∗
a(n) = ˜w(n) + M(n)φ(n)
∥φ(n)∥2
M(n)
e∗
p(n).
(12.254)
Deﬁning
¯ρ(n) ≜
1/∥φ(n)∥2
M(n), if φ(n) ̸= 0
0,
otherwise
(12.255)

710
CHAPTER 12 Adaptive Filters
to include the case of zero regressor and equating the squared weighted norms on both sides of (12.254)
with M−1(n) as a weighting matrix, the cross-terms cancel and we obtain
∥˜w(n + 1)∥2
M−1(n) + ¯ρ(n)|ea(n)|2 = ∥˜w(n)∥2
M−1(n) + ¯ρ(n)|ep(n)|2.
(12.256)
This relation shows how the weighted energies of the weight-error vectors at two successive time
instants are related to the energies of the excess a priori and excess a posteriori errors. This relation
is the reason why this approach is known as the energy conservation method—(12.256) relates the
(weighted) coefﬁcient error energies at instants n and n +1 to the a priori and a posteriori errors. In [3],
this energy conservation relation is used extensively to ﬁnd models for a wide class of adaptive ﬁlters.
We follow here a shorter route, however, and derive in a more direct way the relations necessary for the
study of just LMS, NLMS and RLS.
We can obtain the steady-state EMSE by computing the squared weighted norms on both sides of
(12.220) with M−1(n) as a weighting matrix, which leads to
∥˜w(n + 1)∥2
M−1(n) = ∥˜w(n)∥2
M−1(n) + ρ2(n)|e(n)|2∥φ(n)∥2
M(n)
−ρ(n)

ea(n)e∗(n) + e∗
a(n)e(n)

+ ∥q(n)∥M−1(n).
(12.257)
By taking expectations on both sides of (12.257), we arrive at (we already used Assumption 2 to eliminate
cross-terms with q(n) on the right-hand side)
A



E

∥˜w(n + 1)∥2
M−1(n)

=
B



E

∥˜w(n)∥2
M−1(n)

+
C



E

ρ2(n)|e(n)|2∥φ(n)∥2
M(n)

−
D



E
$
ρ(n)

ea(n)e∗(n) + e∗
a(n)e(n)
%
+
E



∥q(n)∥M−1(n) .
(12.258)
To proceed, we have to assume again that matrix M−1(n) varies slowly in relation to ˜w(n) and
q(n), as in Assumption 3. Since now we are dealing only with the steady-state, this approximation is
in general good. Instead of Assumption 1, in energy conservation analysis the following assumptions
are used:
Assumption 7.
∥φ(n)∥2
M(n) is independent of ea(n) at the steady-state.
Assumption 8.
The noise sequence {vo(n)} is iid and independent of the input sequence {φ(n)}.
Assumption 7 is referred to in the literature as separation principle and is reasonable in steady-state
since for large n, ea(n) tends to be less sensitive to the regressor data, in particular for long ﬁlters.
Assumption 8 is a weakened version of Assumption 1, which is necessary to relate the MSE with the
EMSE through (12.181).
Under model 2 and Assumptions 3 and 7, we can now evaluate the terms A −E:

1.12.4 Statistical Analysis
711
A- Using 3, we obtain
A ≈E

∥˜w(n + 1)∥2
E{M−1(n)}

.
(12.259)
For LMS and NLMS, we have
ALMS = ANLMS ≈E

∥˜w(n + 1)∥2
.
(12.260)
For RLS,
ARLS ≈E

∥˜w(n + 1)∥2
Rφ

.
(12.261)
B- Assuming that the ﬁlter is stable and reaches a steady-state, we have
B = E

∥˜w(n)∥2
M−1(n)

≈E

∥˜w(n + 1)∥2
M−1(n)

.
(12.262)
C- Given that e(n)
=
ea(n) + vo(n) and since vo(n) is assumed independent of ea(n)
(Assumption 8), we obtain
E{|e(n)|2} ≈E{|ea(n)|2} + σ 2
v .
Then, using Assumptions 3 and 8, we arrive at
C ≈E

ρ2(n)φH(n)E{M(n)}φ(n)
 
E{|ea(n)|2} + σ 2
v
 
.
(12.263)
For LMS, this term reduces to
CLMS ≈μ2Tr(Rφ)

E{|ea(n)|2} + σ 2
v
 
.
(12.264)
For NLMS,
CNLMS ≈˜μ2E

1
ε + φH(n)φ(n)
 
E{|ea(n)|2} + σ 2
v
 
,
(12.265)
which for long ﬁlters and Gaussian tapped-delay line regressors can be approximated as (see
Eq. (12.228), and the alternative (12.227) for general regressors)
CNLMS ≈
˜μ2
σ 2x (M −2)

E{|ea(n)|2} + σ 2
v
 
.
(12.266)
Finally, for RLS we obtain
CRLS ≈(1 −λ)2M

E{|ea(n)|2} + σ 2
v
 
.
(12.267)
D- Using the same arguments, D reduces to
D ≈2E{ρ(n)}E{|ea(n)|2}.
(12.268)
For LMS, DLMS ≈2μE{|ea(n)|2}. Using (Eq. (12.228)) for NLMS, we get
DNLMS ≈
2 ˜μ2
σ 2x (M −2)E{|ea(n)|2}.
For RLS, this term is given by DRLS ≈2(1 −λ)E{|ea(n)|2}.

712
CHAPTER 12 Adaptive Filters
Table 12.8 Expressions for the Steady-State EMSE Obtained from the Energy Conservation
Method for LMS, NLMS, and RLS
Algorithm
ζ(∞)
ζ(∞)
for a wider range of μ, ˜μ,
and λ
for small μ and ˜μ, and λ ≈1
LMS
μσ 2v Tr(Rφ) + μ−1Tr(Q)
2 −μTr(Rφ)
μσ 2v Tr(Rφ) + μ−1Tr(Q)
2
NLMS
˜μσ 2v + ˜μ−1σ 2x (M −2)Tr(Q)
2 −˜μ
˜μσ 2v + ˜μ−1σ 2x (M −2)Tr(Q)
2
RLS
σ 2v (1 −λ)M +
Tr(QRφ)
1 −λ
2 −(1 −λ)M
σ 2v (1 −λ)M +
Tr(QRφ)
1 −λ
2
E- Using Assumption 3 and recalling that Tr(AB) = Tr(B A), we obtain
E ≈E

q H(n)E{M−1(n)}q(n)

= Tr

E

M−1(n)

E{q(n)q H(n)}
 
.
For LMS and NLMS this reduces to
ELMS = ENLMS ≈Tr( Q).
(12.269)
For RLS,
ERLS ≈Tr(Rφ Q).
(12.270)
Using these approximations in (12.258), we arrive at the following steady-state approximation
E{|ea(n)|2} ≈σ 2
v E
$
ρ2(n)φH(n)E{M(n)}φ(n)
%
+ Tr( QE{M−1(n)})
2E{ρ(n)} −E
$
ρ2(n)φH(n)E{M(n)}φ(n)
%
.
(12.271)
Particularizing (12.271) for LMS, NLMS, and RLS, we obtain the results of the second column of Table
12.8, which hold over a wide range of step-sizes and forgetting factors.
1.12.4.4.5
Relation between the results obtained with both analysis methods
As we explained before, each method is based on a different set of approximations. We now show how
they are related.
The energy conservation method is capable of obtaining closed-form expressions for the EMSE
using less restrictive assumptions, in particular φ(n) is not required to be Gaussian. These results are
equivalent to those obtained for LMS and RLS using the traditional analysis, taking the recursions
of Table 12.7 to the limit as n →∞, and assuming β = 0 (recall that the values of β used for the
traditional analysis are only valid if φ(n) is Gaussian). The assumption of β = 0 implies the following
approximation:
E

φ(n)φH(n)S(n)φ(n)φH(n)

≈RφTr(S(n)Rφ).

1.12.4 Statistical Analysis
713
Table 12.9 Expressions for the Optimal Adaptation Parameters (ρo) and Minimum Steady-State
EMSE (ζmin(∞)) Obtained from the Expressions of the Third Column of Table 12.8
Algorithm
ρo
ζmin(∞)
LMS
μo =
)
Tr(Q)
σ 2v Tr(Rφ)
*
σ 2v Tr(Rφ)Tr(Q)
NLMS
˜μo =
)
(M −2)σ 2x Tr(Q)
σ 2v
*
σ 2v σ 2x (M −2)Tr(Q)
RLS
1 −λo =
)
Tr(QRφ)
Mσ 2v
*
σ 2v MTr(QRφ)
For small step-sizes, this approximation does not have a large impact, since we are disregarding a term
of O(μ2), that is small compared to other O(μ) terms. For NLMS, in order to recover (12.271) using
the traditional method, we would choose β = 0 and make
E
'
1

ε + φH(n)φ(n)
2
(
≈

E

1
ε + φH(n)φ(n)
2
,
which leads to reasonable results at the steady-state for long ﬁlters.
For small step-sizes (or forgetting factors close to one), the denominator of (12.271) can be approxi-
mated by 2E{ρ(n)}. In this case, the expressions of the second column reduce to those of the third column
of Table 12.8. Again, these results can be obtained from the recursions of Table 12.7, disregarding the
term C (Eq. (12.235)), which will be negligible compared to B in (12.222) if ρ ≈0.
The results for steady-state analysis using the traditional method are commonly obtained as the
limiting case of a transient analysis. Although this procedure is adequate for understanding both the
steady-state and the transient behavior of an adaptive algorithm, it tends to be more laborious than
using the the energy conservation method to analyze the steady-state behavior. This is one of the largest
advantages of this last method.
1.12.4.4.6
Optimal step-size for tracking
From the results of the third column of Table 12.8, we can observe that the expressions for the steady-
state EMSE have two terms: one for the stationary environment, that increases as the step-size increases,
and another that appears in the nonstationary case, and increases as the step-size decreases. Therefore,
there exist optimal adaptation parameters ρo, that minimize the EMSE. These parameters are μo, ˜μo,
and 1 −λo for LMS, NLMS, and RLS, respectively. The corresponding minima, denoted by ζmin(∞),
can also be evaluated. All these results are summarized in Table 12.9. The results for a wider range of
adaptation parameters lead to more complicated expressions for ρo and ζo(∞) (see, e.g., [3]).
We can use the results of Table 12.9 to compare the tracking performance of the algorithms. For
instance, comparing the minimum EMSE for LMS to that of RLS, we obtain the ratio
ζ RLS
min
ζ LMS
min
=
)
MTr( QRφ)
Tr(Rφ)Tr( Q).
(12.272)

714
CHAPTER 12 Adaptive Filters
Clearly, the results of this comparison depend on the environment. There are situations where RLS has
superior tracking capability compared to LMS, and vice versa. This is highlighted considering three
different choices for matrix Q. It can be shown [81] that
i. If Q is a multiple of I: the performance of LMS is similar to that of RLS;
ii. If Q is a multiple of Rφ: LMS is superior; and
iii. If Q is a multiple of R−1
φ : RLS is superior.
These choices for Q do not model practical situations, but they highlight that, even though the conver-
gence rate of RLS is in general much higher than that of LMS, this advantage, unexpectedly, does not
necessarily follow to the problem of tracking a slowly-varying parameter vector.
Example 4.
To verify the accuracy of the expressions of Tables 12.8 and 12.9, we use the LMS,
NLMS, and RLS ﬁlters to identify the same lowpass FIR system of Figure 12.29. Now, the environment
is assumed nonstationary with Q = 10−6I. The input signal and minimum MSE (σ 2
v ) are the same of
Example 3.
Figure 12.32 shows the measured steady-state EMSE with varying adaptation factors considering the
theoretical and experimental results for LMS, NLMS, and RLS. Each value of experimental EMSE was
obtainedfromtheensemble-averageof100independentruns.ThetheoreticalminimumEMSEpredicted
by the expressions of Table 12.9 are ζ LMS
o
(∞) = ζ RLS
o
(∞) = −32.9251 dB and ζ NLMS
o
(∞) =
−33.0989 dB, which correspond to the optimal adaptation parameters μo = 5.1 × 10−2, ˜μo = 4.9 ×
10−2 and 1 −λo = 2 × 10−3 (λo = 0.998). The experimental values for ζmin(∞) and ρo are close to
the predicted by the expressions of Table 12.9 as shown in Figure 12.32.
Example 5.
Again, we consider a system identiﬁcation application but with the initial optimal solution
given by
wT
o (0) = [ 0.5349 0.9527 −0.9620 −0.0158 −0.1254 ].
The input signal is assumed to be colored Gaussian noise with variance σ 2
x = 0.2. This signal is obtained
from the ﬁltering of a white Gaussian noise by a ﬁrst-order autoregressive model (b = 0.8). We also
assume that the minimum MSE is σ 2
v = 0.01.
As predicted by (12.272), the performance of RLS and LMS are similar when Q = σ 2
q I. Thus,
assuming Q = 10−6I, we computed μo and λo from the expressions of Table 12.9 and used these
parametersintheadaptation,suchthatthesameminimumEMSEcouldbeachievedbybothalgorithmsin
this situation. Figure 12.33 shows the EMSE estimated from the ensemble-average of 5000 independent
runs for RLS (λ = λo = 0.9955, δ = 4.5 × 10−3) and LMS (μ = μo = 0.0224). At every 7 × 104
iterations, the nonstationary environment is changed. During the ﬁrst 7 × 104 iterations, Q = 10−6Rφ
and LMS presents a slightly better tracking performance than that of RLS. When Q = 10−6R−1
φ , this
behavior changes: RLS becomes better than LMS. Finally for Q = 10−6I, RLS and LMS present
similar performance. The dashed lines represent the theoretical values of ζ(∞) for each algorithm,
predicted from the expressions of the second column of Table 12.8.

1.12.4 Statistical Analysis
715
−18
−20
−22
−24
−26
−28
−30
−32
−34
−3
−2
−1
10
10
10
(a)
µ
ζ(∞)
−18
−20
−22
−24
−26
−28
−30
−32
−34
−3
−2
−1
0
10
10
10
10
(b)
˜µ
ζ(∞)
−18
−20
−22
−24
−26
−28
−30
−32
−34
−4
−3
−2
10
10
10
(c)
1−λ
ζ(∞)
FIGURE 12.32
Theoretical and experimental EMSE as a function of the adaptation parameter for a) LMS, b) NLMS, and c)
RLS. The asterisks represent simulation results and the solid lines represent the theoretical models.
Box 7: Fourth-order moments of Gaussian vectors
The second-order moments of a random sequence are related to its average power (of course, this is an
approximation unless the sequence is stationary and ergodic.) This makes second-order moments intu-
itive to work with and relatively easy to evaluate in practice. Therefore, when studying an adaptive ﬁlter,
it is common to try to describe its performance in terms of the autocorrelation of the input sequence x(n).
During the analysis of LMS, one encounters a term that contains fourth-order powers of the input
vector x(n):
F = E{x(n)xH(n) ˜w(n) ˜wH(n)x(n)xH(n)}.
(12.273)

716
CHAPTER 12 Adaptive Filters
0
0.5
1
1.5
2
×105
−25
−30
−35
−40
iterations
EMSE (dB)
RLS
Q = σ2
v R −1
φ
Q = σ2
v I
LMS
Q = σ2
v R φ
FIGURE 12.33
EMSE for RLS (λ = 0.9955, δ = 4.5 × 10−3), and LMS (μ = 0.0224); b = 0.8, σ 2
v = 10−2, σ 2
q = 10−6;
mean of 5000 independent runs. The dashed lines represent the predicted values of ζ(∞) for each algorithm.
In order to evaluate a term such as this, one needs to know much more information about the statistics
of x(n) than is provided simply by its mean and autocorrelation. However, in general this additional
information is not available, and one assumes that the signals are Gaussian in order to proceed.
Why not any other distribution? First, because many processes in Nature are indeed approximately
Gaussian. Second, because uncorrelated Gaussian variables are also independent, which is of great help
when evaluating fourth-order terms, such as E{x2(n)x2(n −1)} or E{x3(n)x(n −2)}.
Although in many cases the input sequence is not Gaussian (for example, speech does not follow
a Gaussian distribution), this approximation describes the most important features of adaptive ﬁlter
learning with reasonable accuracy, and leads to a relatively simple model.
In order to simplify the notation, we will write simply x and ˜w instead of x(n) and ˜w(n) in the
following. Assume then that x is a Gaussian random vector, with autocorrelation E{xx} = R, and that
˜w is another vector independent of x, and with autocorrelation E{ ˜w ˜wH} = S (we do not need to know
the exact distribution of ˜w.) Assume also that all variables have zero mean. Our goal is to evaluate
F = E{xxH ˜w ˜wH xxH}.
Since ˜w is independent of x, we can write
F = E{xxH ˜w ˜wH xxH} = E{xxHE{ ˜w ˜wH}xxH} = E{xxH SxxH}.
(12.274)
The autocorrelation of x is in general a full matrix, that is,
R =
⎡
⎢⎢⎢⎣
r11 r12 . . . r1M
r∗
12 r22 . . . r2M
...
...
...
...
r∗
1M r∗
2M . . . rM M
⎤
⎥⎥⎥⎦,

1.12.5 Extensions and Current Research
717
that is, in general the elements of x are correlated. R has hermitian symmetry and is non-negative
deﬁnite, which implies that there exists a unitary matrix U such that
U H RU =  =
⎡
⎢⎢⎢⎣
λ1
0
. . .
0
0
λ2
. . .
0
...
...
...
...
0
0
. . . λM
⎤
⎥⎥⎥⎦,
where λi ≥0 for i = 1, . . . , M. Deﬁne x′ = U H x. Then E{x′x′H} = E{U H xxHU} = U HE{xxH}
U = , and we see that the entries of x′ are uncorrelated. Since x is Gaussian, andalinear transformation
of a Gaussian vector is also Gaussian, this means that the entries of x′ are independent of each other.
Multiplying (12.274) by U H on the left and U on the right, and recalling that U HU = UU H = I, we
obtain
F′ = U H FU = E{U H xxH UU H
  
=I
S UU H
  
=I
xxHU} = E{x′x′HU H SUx′x′H}.
Deﬁning ¯S = U H SU, we can expand F in terms of the elements s′
i j of ¯S and x′
k of x′. Each element
of F will have terms of the form
E{x′
k1x′k2x′∗k3x′∗
k4}s′
i j.
If the signals are real, then, as x′
k1 is independent of x′
k2 if k1 ̸= k2, these expected values will be nonzero
only if k1 = k2 = k3 = k4 or if k1 = k3 and k2 = k4, or k1 = k4 and k2 = k3, or if k1 = k2 and k3 = k4.
Evaluating all such terms that appear in the expression for F′ will result in
F|real signals = RTr(SR) + 2RSR.
(12.275)
If,ontheotherhand,thesignalsarecomplex-valued,thenweneedtoknowhowtherealandimaginary
parts of each entry of x relate to each other. One common assumption is that x is circularly-Gaussian.
We explain more about this in Section 1.12.2.2.1. For now, we only remark that if this is the case, the
real and imaginary parts of each entry of x and of x′ are such that E{x2
k } = E{x′2
k } = 0 for all k (note
that E{|xk|2} = rkk, E{|x′
k|2} = λk are still nonzero.) This will result in a different expression for F:
F|complex signals = RTr(SR) + RSR.
(12.276)
1.12.5 Extensions and current research
In this section we brieﬂy describe some important extensions to the basic adaptive ﬁltering algorithms,
as well as some promising current research topics. The intention is not to describe in any detail all
these techniques, but rather to give a list of key references where the interested reader may ﬁnd more
information.
It is usual when starting such lists to put a disclaimer, that the authors do not claim to have included
all the important contributions, nor that their list is really of the most important contributions to an area.

718
CHAPTER 12 Adaptive Filters
This is not intended for legal purposes, nor false modesty: the size of the literature is indeed so large
(a search on “adaptive ﬁlter” on Google gives over one million hits) that we cannot claim to know with
certainty the best of it all. We included therefore the techniques that we found useful during our own
research and experience, which is perforce limited.
1.12.5.1 Finite-precision arithmetic
Any operation made using ﬁnite-precision arithmetic may involve errors. For example, the product of
a = 0.957 and b = 0.542 would result in c = 0.518694. However, if all numbers were stored keeping
only three digits, the ﬁnal result would be ¯c = 0.519, with an error of c −¯c = −3.06 × 10−4 [82,83].
These quantization errors will accumulate and modify the behavior of an adaptive ﬁlter in important
ways. A precise analysis of the effect of quantization errors is quite complicated, since these errors are
highly nonlinear functions of the variables.
When ﬁnite-precision arithmetic effects are studied, it is usually important to know exactly how and
in which order the arithmetic operations are performed. In particular, the exact numerical representation
used is important: ﬁxed-point vs. ﬂoating-point, truncation vs. rounding, etc. For this reason, it is
more difﬁcult to perform analyses that are equally valid for a large class of ﬁlters. The most important
differences are between gradient-based algorithms (i.e., algorithms similar to LMS and NLMS) and
Hessian-based algorithms (i.e., algorithms similar to RLS). We will give references to works treating
both cases, starting with LMS.
1.12.5.1.1
Finite-precision effects in LMS
The simplest models for adaptive ﬁlters in ﬁnite-precision arithmetic treat the quantization error as
random noise, uniformly distributed and independent of all variables [84–87]. These models show that
the quantization errors will add another term to the EMSE. However, this term does not converge to
zero when the step-size is reduced to zero, even in a stationary environment: quite the opposite, it is
inversely proportional to the step-size (similar to the EMSE in a nonstationary environment).
Another important undesirable effect is that the product μe(n) may be rounded down to zero when
e(n) becomes small (underﬂow), virtually stopping the adaptation. When the step-size is small, this
may happen when the ﬁlter is rather far from the optimum solution. Therefore, there is an optimum
value for the step-size—neither too large, to avoid increasing the misadjustment, neither too small, to
avoid underﬂow and excessive growth of the EMSE. A similar effect happens in RLS [1, p. 595].
This underﬂow problem, which is usually known as stopping phenomenon, was studied in more detail
in [88–91], using a nonlinear model. These works show that the adaptation does not really stop, but
rather convergence is reduced to a very low rate. Note that this phenomenon is less important in ﬂoating-
point arithmetic, since ﬂoating-point is able to represent much smaller numbers without underﬂow than
ﬁxed-point arithmetic.
Finite precision effects may have worse consequences, in particular they may make the adaptive
ﬁlter unstable. In the case of LMS, this phenomenon is rare, and will only happen when the regressor
sequence {φ} has a very ill-conditioned covariance matrix, and the noise vo has a nonzero mean (this
nonzero mean may itself appear due to quantization errors) [92–94]. This problem is not difﬁcult to
solve, using regularization as described in Section 1.12.5.2. The problem of numerical stability in RLS
is much more serious and harder to solve, and has received much attention, as we see next.

1.12.5 Extensions and Current Research
719
In addition to studying the effect of quantization errors in the performance of an adaptive ﬁlter, another
important topic is how to reduce the computational cost of a ﬁlter, taking advantage of implementation
details. One such approach is to replace the error e(n) in the LMS recursion by its sign, i.e., to use the
recursion
w(n + 1) = w(n) + μsign

e(n)

φ(n),
(12.277)
where sign(·) is the sign function, deﬁned by
sign

e

=
⎧
⎨
⎩
1,
if e > 0,
0,
if e = 0,
−1, if e < 0.
(12.278)
The resulting algorithm, known as sign-LMS, has a smaller computational cost, if μ is restricted to
be a power of two, that is, if μ = 2k for an integer (usually negative) k. In this case, μsign

e(n)

is
always equal to ±2k (or zero), and the product

μsign

e(n)

φ(n) can be implemented in ﬁxed-point
arithmetic as M shifts, instead of M multiplications. In ﬂoating-point arithmetic, the multiplications
are replaced by sums, since we would need to add k to the exponent terms of all entries of φ(n). This
algorithm is very robust (see also Section 1.12.5.4), although its convergence rate under Gaussian inputs
is considerably slower than that of LMS. Sign-LMS was thoroughly studied in [81,95–98].
Another possibility, that works surprisingly well, reducing the computational cost with only a slight
decrease in convergence speed is the power-of-two LMS algorithm [99]. In it, the error is rounded to
the nearest power of two, using the function:
f

e

=
⎧
⎨
⎩
sign(e),
|e| ≥1,
sign(e)2⌊ln (|e|)⌋, 2−B+1 ≤|e| < 1,
0,
|e| < 2−B+1,
where B is the number of bits used to represent the error, and ⌊x⌋returns the largest integer smaller
than x. In this case, μ should also be a power of two.
1.12.5.1.2
Finite-precision effects in RLS
As we brieﬂy described in Section 1.12.3.3.1, the conventional RLS is numerically unstable in ﬁnite
precision arithmetic and has O(M2) complexity. To solve these problems, different versions of RLS
were proposed in the literature. Many of these versions are based on coordinate transformations of the
state-space representation of the conventional RLS algorithm. This representation can be obtained by
replacing k(n) by P(n)φ(n) in(12.166), i.e.,
w(n + 1) = λP(n)ˆrdφ(n −1) + P(n)φ(n)d∗(n).
(12.279)
Recalling that w(n) = P(n −1)ˆrdφ(n −1), we can replace ˆrdφ(n −1) in (12.279) as a function of
P−1(n −1) and w(n), which leads to
w(n + 1) = λP(n)P−1(n −1)w(n) + P(n)φ(n)d∗(n).
(12.280)

720
CHAPTER 12 Adaptive Filters
This equation in conjunction with the deﬁnition of the a priori error e(n) characterize the adaptation
and ﬁltering operations of the RLS algorithm and constitute its state-space representation, i.e.,
 w(n + 1)
e∗(n)

=
 λP(n)P−1(n −1)
P(n)φ(n)
−φH(n)
1
  w(n)
d∗(n)

.
(12.281)
In this representation, w(n) is the state, e∗(n) is the output, and d∗(n) is the input.
Performing a coordinate transformation, (12.281) can be transformed to an unlimited number of
systems with the same input-output relation, and hence solving the same least-squares problem (that is,
the output error is always the same). Although all these realizations are equivalent in inﬁnite precision
arithmetic, the numerical behavior will vary from one coordinate system to another. Thus, the numerical
instability of the conventional RLS can be avoided by choosing a convenient transformation of (12.281)
[40,100].
An alternative method of implementing RLS is based on the QR decomposition (QRD) [14] to
triangularize the input data matrix. The main advantages of QRD-RLS-based algorithms are the possi-
bility of implementation in systolic arrays (e.g., [101]) and the improved numerical behavior in ﬁnite
precision arithmetic. Some versions require O(M2) operations per iteration, but others have a reduced
computational cost of O(M). The low-cost versions of QRD-RLS algorithms are known as fast-QR
algorithms. Some important references on this subject are [38,101–116]. A good tutorial on QRD-RLS-
based algorithms can be found in [43], where the most recent developments as well as the basic concepts
are covered.
Another important class of fast algorithms solves the least-squares problem in a recursive form based
onthelatticerealization.Thesealgorithmsareveryattractiveduetothepossiblemodularimplementation
and low cost (O(M)). Lattice-RLS-based algorithms are derived by solving the forward and backward
linear prediction problems and require time-update and order-update recursions. They explore the time-
shift property of the input signal vector (such as in (12.113)) and do not compute explicitly the ﬁlter
weights. Recall that many applications do not require the computation of the ﬁlter weights and have
regressors that satisfy the time-shift property as, for example, channel equalization and interference
cancellation (see Section 1.12.1.3). In these applications, a fast RLS algorithm can be a reasonable
choice since it presents a good tradeoff between convergence rate and computational cost. One of
these algorithms is the modiﬁed EF-LSL (error-feedback least-squares lattice algorithm) that presents
reliable numerical properties when implemented in ﬁnite-precision arithmetic [5]. This algorithm was
proposed for echo-cancellation applications and was used in the example of Section 1.12.1.1. Some
lattice-RLS-based algorithms can be found in the references [5,117–124].
More recently, lattice-RLS algorithms have been generalized to ﬁlter structures other than tapped-
delay lines, in particular, to Laguerre ﬁlters [125–127].
Other fast algorithms that solve the least-squares problem in a recursive form are the fast transversal
RLS (FTRLS) algorithms. Unlike the lattice-based algorithms, the FTRLS algorithms require only
time-recursive equations and therefore, compute explicitly the ﬁlter weights. The main drawback of
these algorithms is that they are very sensitive to quantization effects and become unstable if certain
actions are not taken [1]. The list of fast transversal RLS algorithms is vast. Some of the most important
versions can be found in the references [128–134].

1.12.5 Extensions and Current Research
721
1.12.5.1.3
DCD-RLS
The DCD-RLS algorithm, proposed in [44], is a low-complexity alternative to the RLS algorithm,
based on the dichotomous coordinate-descent (DCD) algorithm for function minimization proposed in
[135]. The DCD algorithm is designed to be easily implementable in hardware (such as FPGAs, for
example), and thus avoids multiplications, divisions and other operations that are costly to implement
in hardware—most of its operations are additions and comparisons.
The RLS weight vector estimates are given by the solution of the following set of linear equations,
which we repeat from Section 1.12.3.3:
Rφ(n)w(n + 1) = ˆrdφ(n),
(12.282)
where Rφ(n) and ˆrdφ(n) are given by the recursions
Rφ(n) = λRφ(n −1) + φ(n)φH(n),
(12.283)
and
ˆrdφ(n) = λˆrdφ(n −1) + d∗(n)φ(n).
(12.284)
The difﬁculty in solving these expressions is that a general solution for (12.282) involves a number
of operations of the order of M3(O(M3)) for a ﬁlter with M coefﬁcients. This is usually too costly
for practical applications, except perhaps for very short ﬁlters. The classical RLS algorithm described
in Section 1.12.3.3 solves this problem partially by using the matrix inversion lemma to compute the
inverse of Rφ(n) recursively using the already computed inverse of Rφ(n −1). This reduces the total
number of computations to O(M2). However, the resulting algorithm is still difﬁcult to implement in
practice, because numerical errors may easily cause divergence.
The DCD-RLS algorithm avoids both these problems by using a couple of clever tricks. First, assume
that you have a good approximation ˆw(n) for w(n). Then you could use a recursive algorithm to ﬁnd
an approximation ˆw(n + 1) to the solution of (12.282), using your current approximation ˆw(n) as an
initial condition. Since the initial condition is already close to the solution, you would need only a few
iterations of your recursive algorithm to obtain ˆw(n +1). However, this idea only helps if each iteration
of the recursive algorithm is very cheap to compute. We explain next how this can be done using DCD.
DCD minimization of quadratic functions
Let us consider ﬁrst a one-dimensional quadratic problem
min
w

f (w) = 1
2aw2 −bw + c

,
where a > 0, b, and c are constants. Assume in addition that you want to implement your algorithm in
hardware, using ﬁxed-point arithmetic.
In order to solve this problem, you could proceed as follows. Assume you have an initial approxi-
mation ˆw(0). Choose a step-size H > 0 and compute
 f+ = f ( ˆw(0) + H) −f ( ˆw(0)),
 f−= f ( ˆw(0) −H) −f ( ˆw(0)).

722
CHAPTER 12 Adaptive Filters
ˆw0−H
ˆw0 + H
ˆw0
w
f (w)
FIGURE 12.34
DCD applied to one-dimensional minimization.
Then, if  f+ < 0, choose ˆw(1) = ˆw(0) + H, and if  f−< 0, choose ˆw(1) = ˆw(0) −H. If both
are positive, choose ˆw(1) = ˆw(0) and reduce the step-size H ←H/2 (note that if f (·) is convex, the
case of both  f+ < 0 and  f−< 0 can never occur). With the new estimate ˆw(1), the procedure can
be repeated to ﬁnd a new improved estimate ˆw(2), and so on. It can be shown that this algorithm will
converge to the minimum of f (·) whenever f (·) is a convex function—see Figure 12.34.
Although the description given so far explains the general working of the DCD algorithm, it does
not show how it can be efﬁciently implemented, using as few operations as possible. We will turn to this
point now, but already extending the algorithm to minimize a quadratic function of several variables.
Consider then the problem of ﬁnding the solution wo to
min
w

f (w) = 1
2wT Rw −bT w + c

,
where R is a positive-deﬁnite matrix, b is a vector, and c is a constant (note that the minimum wo of
f (w) is also the solution wo to Rwo = b, that is, any algorithm to ﬁnd wo can be also used to solve
the normal equations (12.282)).
Assume that we have an initial approximation ˆw(0) to the solution, and we want to ﬁnd an improved
approximation, changing one entry of ˆw(0) at a time. In order to differentiate each new approximation,
we use the following notation. Deﬁne ˆw(0)(0) = ˆw(0). We shall ﬁrst seek an improved estimate to the
ﬁrst entry of ˆw(0)(0). The resulting vector will be denoted by ˆw(1)(0). Continuing, we ﬁnd an improved
estimate to the second entry of ˆw(1)(0), resulting in vector ˆw(2)(0) and so on. After updating all entries
of the initial estimate, we have ˆw(M)(0). We then let ˆw(0)(1) = ˆw(M)(0), and repeat the procedure.
Then we need to check if f ( ˆw(0)(0) ± He1) < f ( ˆw(0)) if we add or subtract H to the ﬁrst entry of
ˆw(0)(0) (e1 is the vector [1 0 . . . 0]T ). The change  f+ in the cost-function obtained by adding H to
the ﬁrst entry of ˆw(0)(0) is
 f+ = 1
2

ˆw(0)(0) + He1
 T
R

ˆw(0)(0) + He1
 
−bT 
ˆw(0)(0) + He1
 
+ c
(12.285)
−
"1
2 ˆw(0)T (0)R ˆw(0)(0) −bT ˆw(0)(0) + c
#

1.12.5 Extensions and Current Research
723
= HeT
1 R ˆw(0)(0) + 1
2 H2eT
1 Re1 −HbT e1.
(12.286)
Similarly, the variation  f−obtained from subtracting H from the ﬁrst entry of ˆw(0) is
 f−= −HeT
1 R ˆw(0)(0) + 1
2 H2eT
1 Re1 + HbT e1.
(12.287)
Therefore, we should make
ˆw(1)(0) =
⎧
⎪⎨
⎪⎩
ˆw(0)(0) + He1, if  f+ < 0,
ˆw(0)(0) −He1, if  f−< 0,
ˆw(0)(0),
otherwise.
(12.288)
The same steps can be repeated for the second entry of ˆw(1)(0) and so on, that is, we can update the
(m + 1)th entry of ˆw(m)(0) to obtain a new approximation ˆw(m+1)(0) for m = 0, . . . , M −1. After
obtaining ˆw(M)(0), we have updated all entries of ˆw(0). We can then make ˆw(0)(1) = ˆw(M)(0) and
repeat the procedure. However, if no update is made for m = 1, . . . , M, before repeating we decrease
the step H by a factor of two (i.e., H ←H/2). By reducing H only when no update is necessary for
any entry of the vector, we avoid problems that would appear if the initial value of H were too small.
Implemented as described so far, the algorithm has a high computational cost. For example, for the
evaluation of  f+ at each step we must
•
Evaluate eT
m R ˆw(m−1)(i). This is equivalent to multiplying the mth row of R by ˆw(m−1)(i), involving
M multiplications and M −1 additions.
•
Multiply −eT
m R ˆw(m−1)(i) + bT em by H and add the result to (H2/2)Rm,m (Rm,m is the element
(m, m) of R). This requires 2 multiplications and 2 additions, assuming H2/2 is pre-computed.
The same steps would be necessary for the computation of  f−. Note that all these computations would
have to be repeated at each step, therefore the update of all entries of ˆw(0)(i) would require more than
M2 multiplications.
However, the number of computations can be reduced substantially, if we take advantage of some
properties of hardware implementations of arithmetic operations, as we describe next. Assume in the
following that all variables are ﬁxed-point binary numbers (see Box 8).
First, we can avoid the computation of R ˆw(m−1)(i) by introducing a residue vector that is updated
at each step. Let r(0)(0) = b −R ˆw(0)(0) be the initial residue vector. Note that this is the residue of
solving Rw = b using ˆw(0)(0) as an approximation to the solution.
At the end of the ﬁrst step, ˆw(0)(0) is updated as in (12.288). Since only one entry of ˆw(0)(0) is
changed, the modiﬁcation in the residue is simple to evaluate
r(1)(0) = b −R ˆw(1)(0) =
⎧
⎨
⎩
b −R( ˆw(0)(0) + He1) = r(0)(0) −H Re1, if  f+ < 0,
b −R( ˆw(0)(0) −He1) = r(0)(0) + H Re1, if  f−< 0,
r(0)(0),
otherwise.
(12.289)
This still requires M multiplications (H multiplied by the ﬁrst column of R). However, if H is a
power of two, that is, if H = 2k for some integer k, than the computation of r(1)(0) in ﬁxed-point

724
CHAPTER 12 Adaptive Filters
arithmetic requires only M shifts and M additions, which are much easier to implement in hardware
than multiplications. Moreover, if both  f+ and  f−are positive, no update is necessary.
Next, we note that it is not really necessary to compute both  f+ and  f−: from (12.286) and
(12.287), when updating the mth element of ˆw(m−1)(i) at step i, we have
 f+ = −HeT
mr(m−1)(i) + 1
2 H2eT
m Rem = −Hr(m−1)
m
(i) + 1
2 H2Rm,m,
(12.290)
where r(m−1)
m
(i) is the mth entry of the residue vector r(m−1)(i), r(0)(i + 1) = r(M)(i), and em is the
mth column of the M × M identity matrix. Similarly,
 f−= Hr(m−1)
m
(i) + 1
2 H2Rm,m.
(12.291)
Then, recalling that H and Rm,m are positive (R is positive-deﬁnite),  f+ < 0 only if
r(m−1)
m
(i) > 1
2 H Rm,m ≥0.
Repeating the arguments for  f−, we see that the mth entry of ˆw(m−1)(i) will be updated if and only if
|r(m−1)
m
(i)| > 1
2 H Rm,m,
(12.292)
with updates
ˆw(m)(i) = ˆw(m−1)(i) + Hsign(r(m−1)
m
(i))em,
(12.293)
r(m)(i) = r(m−1)(i) −Hsign(r(m−1)
m
(i))Rem.
(12.294)
Equations 12.292–12.294 require one comparison, M + 1 shifts, and M + 1 additions. Table 12.10
summarizes the DCD algorithm for minimization of quadratic cost-functions. Note that the algorithm
will update the entries of ˆw(i) bit by bit. It therefore makes sense to choose the initial step-size H the
power-of-two value represented by the most-signiﬁcant bit in the ﬁxed-point representation being used.
The conditional jump in step 10 prevents problems in case H is chosen too small.
As seen in Table 12.10, the total number of operations never exceeds (2M +2)(B −1)+ Nu(4M +3)
(note that here we are counting additions, comparisons and shits as operations—the algorithm does not
involve multiplications). In DCD-RLS, the maximum number of updates Nu can be chosen between 1
and 4 with good performance, while keeping the cost low.
When applied to RLS, one ﬁnal simpliﬁcation can be used when implementing DCD: instead of
updating all entries of vector ˆw(i), we can only update the entry corresponding to the largest residue.
As explained in [44], the resulting algorithm is still guaranteed to converge to the optimum solution.
DCD-RLS
Aswementionedbefore,DCD-RLSusestheDCDalgorithmjustdescribedtosolvethenormalequations
as each new sample arrives. Since DCD is an iterative algorithm, we can use the current solution w(n)
as the initial value for computation of w(n + 1). In this way, the number of updates Nu of DCD can be

1.12.5 Extensions and Current Research
725
Table 12.10 Summary of DCD Algorithm for Minimization of a Quadratic Cost-Function
Step
Initialization:
Number of
Choose step H = 2k , number of bits B,
Additions, Shifts,
Multiplications
and maximum update count Nu.
Comparisons
Let ˆw = 0, r = b, h = H, and c = 0.
1
for b = 0 : B −1
2
h ←h/2
1
0
3
ﬂag←0
0
0
4
for m = 1 : M
5
if |rm| > 1
2hRm,m
2
0
6
ˆwm ←ˆwm + hsign(rm)
1
0
7
r ←r −hsign(rm)Rem
2M
0
8
c ←c + 1, ﬂag←1
1
0
9
if c > Nu, stop algorithm.
1
0
10
if ﬂag = 1, go to step 2
1
0
Total: (worst case)
≤(2M + 2)(B −1)
0
Nu(2M + 3)
restricted to a small number (in many situations from 1 to 4 updates is enough for performance close to
that of exact RLS). This further reduces the computational cost of the algorithm.
In the following, we will use w(n + 1) to denote the exact solution of (12.282) (i.e., the exact RLS
estimate), and ˆw(n + 1) to denote the approximation computed by DCD-RLS (both would be equal if
we let Nu →∞). Assume that at time instant n, we have available both ˆw(n) and the residue
r(n) = ˆrdφ(n −1) −Rφ(n −1) ˆw(n),
(12.295)
In order to apply the DCD algorithm to solve for ˆw(n + 1), it is convenient to update the difference
w(n + 1) = w(n + 1) −ˆw(n).
(12.296)
Deﬁne also
Rφ(n) = Rφ(n) −Rφ(n −1),
(12.297)
ˆrdφ(n) = ˆrdφ(n) −ˆrdφ(n −1).
(12.298)
The normal Eq. (12.282) then become
Rφ(n)

ˆw(n) + w(n + 1)

= ˆrdφ(n),
and thus
Rφ(n)w(n + 1) = ˆrdφ(n −1) + ˆrdφ(n) −Rφ(n −1) ˆw(n) −Rφ(n) ˆw(n)
= r(n) + ˆrdφ(n) −Rφ(n) ˆw(n) = β(n).
(12.299)

726
CHAPTER 12 Adaptive Filters
Given an approximate solution  ˆw(n+1) to these equations, we obtain an updated approximate solution
ˆw(n + 1) = ˆw(n) +  ˆw(n + 1). Note that the residue of solving (12.282) using ˆw(n + 1),
r(n + 1) = ˆrdφ(n) −Rφ(n)w(n + 1)
= ˆrdφ(n −1) + ˆrdφ(n) −Rφ(n)

ˆw(n) +  ˆw(n + 1)

= ˆrdφ(n −1) + ˆrdφ(n) −Rφ(n) ˆw(n + 1) −
Rφ(n −1) + Rφ(n)

ˆw(n)
= r(n) + ˆrdφ(n) −Rφ(n) ˆw(n + 1) −Rφ(n) ˆw(n)
= β(n) −Rφ(n) ˆw(n + 1),
(12.300)
is exactly the residue of approximately solving (12.299) by  ˆw(n + 1).
To complete, we only need to evaluate Rφ(n) and rdφ(n). From (12.297), (12.298), (12.283)
and (12.284), we obtain
Rφ(n) = (λ −1)Rφ(n −1) + φ(n)φT (n),
(12.301)
ˆrdφ(n) = (λ −1)ddφ(n −1) + d(n)φ(n).
(12.302)
Substituting these results in the deﬁnition of β(n), we obtain
β(n) = r(n) + (λ −1)ˆrdφ(n −1) + d(n)φ(n) −

(λ −1)Rφ(n −1) + φ(n)φT (n)

ˆw(n)
= r(n) + (λ −1)r(n) + e(n)φ(n) = λr(n) + e(n)φ(n).
(12.303)
We can therefore use the DCD algorithm to compute an approximate solution to (12.299) and update
the residue deﬁned by (12.300), as described in Table 12.11.
As we mentioned before, the number of operations of the DCD algorithm can be reduced if we
modify the algorithm in Table 12.10 to update only the entries of the weight vector corresponding to
the largest residue entry, as described in Table 12.12.
The total number of operations of DCD-RLS as described in Tables 12.11 and 12.12 is M2 + 4M
multiplications and M2/2 + 3.5M + (2M + 1)Nu + B additions. Most of these operations are due
to the updating of Rφ in Table 12.11, which is responsible for the terms in M2. While this cannot be
helped for general regressors φ(n), the operation count can be substantially reduced when our ﬁlter is
modeling an FIR relation as in (12.41) (i.e., φ(n) is a tap-delay line), as we show next.
If
φ(n) =
 x(n)
x(n −1) . . .
x(n −M + 1)T ,
(12.304)

1.12.5 Extensions and Current Research
727
Table 12.11 Summary of the DCD RLS Algorithm for General Regressors
Initialization:
ˆw(0) = 0, r(0) = 0, Rφ(−1) =  > 0
for n = 0, 1, 2, . . .
Rφ(n) = λR(n −1) + φ(n)φT (n) (use (12.306) for tap-delay lines)
ˆy(n) = ˆwT (n)φ(n),
e(n) = d(n) −ˆy(n),
β(n) = λr(n) + e(n)φ(n),
Use DCD algorithm to compute new  ˆw(n + 1) and r(n + 1) from Rφ(n) ˆw(n + 1) = β(n),
ˆw(n + 1) = ˆw(n) +  ˆw(n + 1).
Table 12.12 Summary of Leading-Element DCD Algorithm for Use in DCD-RLS, from [44]
Step
Initialization: Choose initial condition step H = 2k , number of bits B, and maximum
update count Nu. Let  ˆw = 0, r = β(n), h = H/2, b = 1.
for k = 1 : Nu
1
p = arg max1≤m≤M{|rm|}, go to step 4
2
b ←b + 1, h ←h/2
3
Stop if b > B
4
Go to step 2 if |rp| ≤(h/2)Rp,p
5
 ˆwp =  ˆwp + sign(rp)h
6
r = r −sign(rp)hRφep
then R(n) and R(n −1) share a common structure. Assume that the initial condition R(−1) =  =
diag(λM−1, λM−2, . . . , 1). Then
R(0) = λ
⎡
⎢⎢⎢⎣
λM−1
0
. . . 0
0
λM−2
. . . 0
...
...
...
...
0
0
. . . 1
⎤
⎥⎥⎥⎦+
⎡
⎢⎢⎢⎣
x(0)
0
...
0
⎤
⎥⎥⎥⎦[ x(0) 0 . . . 0 ]
=
⎡
⎢⎢⎢⎣
λM + x2(0)
0
. . .
0
0
λM−1
. . .
0
...
...
...
...
0
0
. . . λ
⎤
⎥⎥⎥⎦=
⎡
⎣
λM + x2(0)
0T
0
[Rφ(−1)]1:M−1,1:M−1
⎤
⎦,
where [Rφ(−1)]1:M−1,1:M−1 denotes the top-left (M −1) × (M −1) block of Rφ(−1). We note that,
except for its ﬁrst row and column, R(0) can be obtained directly from R(−1).

728
CHAPTER 12 Adaptive Filters
Following this observation, we claim that for tapped-delay line regressors, the computation of Rφ(n)
in Table 12.11 can be replaced by an update of only its ﬁrst column [Rφ(n)]1, that is,
[Rφ(n)]1 = λ[Rφ(n −1)]1 + x(n)φ(n).
(12.305)
Since Rφ(n) is symmetric, its ﬁrst row is the transpose of (12.306), and does not need to be evaluated
again.
We show next by induction that this pattern holds for all n. We just showed that it it true for n = 0.
Assume then that for a certain n, it holds that
Rφ(n) =
⎡
⎣
(·)T
λ[Rφ(n −1)]1 + x(n)φ(n)
[Rφ(n −1)]1:M−1,1:M−1
⎤
⎦,
(12.306)
where we use (·)T to indicate that the entries of the ﬁrst row are the same as the entries of the ﬁrst
column. We show next that Rφ(n + 1) must follow the same pattern. Indeed,
Rφ(n + 1) = λRφ(n) +
⎡
⎢⎢⎢⎣
x(n + 1)
x(n)
...
n(n −M + 2)
⎤
⎥⎥⎥⎦
 x(n + 1)
x(n) . . .
x(n −M + 2)
=
⎡
⎣
(·)T
λ[Rφ(n)]1
λ[Rφ(n −1)]1:M−1,1:M−1
⎤
⎦
+
⎡
⎢⎢⎢⎢⎢⎣
(·)T
x(n + 1)φ(n + 1) ⎡
⎢⎣
x2(n)
. . .
x(n)x(n −M + 2)
...
...
...
x(n)x(n −M + 2) . . .
x2(n −m + 2)
⎤
⎥⎦
⎤
⎥⎥⎥⎥⎥⎦
=
⎡
⎣
(·)T
λ[Rφ(n)]1 + x(n + 1)φ(n + 1)
[Rφ(n)]1:M−1,1:M−1
⎤
⎦,
where in the last step we identiﬁed the (2, 2) block of the intermediate result with the ﬁrst M −1 rows
and columns of Rφ(n), from (12.283).
Note that in practice we should not actually move the entries of Rφ(n−1) to their new position; rather,
we should use an indexing system to access the entries of Rφ(n) to avoid this costly moving operation.
The total number of operations of DCD-RLS using (12.306) is 3M multiplications and 2M Nu + 6M
additions. This is not much larger than the complexity of NLMS. The DCD-RLS algorithm has been
implemented in FPGAs and run for long periods of time without observation of divergence [44].
Figure 12.35 shows a comparison of RLS and DCD-RLS in a system identiﬁcation problem. We are
trying to identify an M = 10 FIR ﬁlter, so we use (12.306) to update Rφ(n) with lower cost. The input

1.12.5 Extensions and Current Research
729
100
200
400
500
700
800
−30
−25
−20
−15
−10
−5
15
10
5
0
0
EMSE (dB)
Nu=4
Nu=2
Nu=1
RLS
n
300
600
FIGURE 12.35
EMSE for RLS and DCD-RLS for different values of Nu, with λ = 0.95, M = 10, and for DCD, H = 2 and
B = 16. Average of L = 500 realizations.
signal x(n) is white Gaussian noise with zero mean and unit variance. The optimum coefﬁcient vector
wo(n) follows a random-walk model as in (12.182) with Q = 10−5I (the initial value, wo(0) is random,
taken from a Gaussian distribution with covariance matrix equal to I). Both RLS and RLS-DCD use
a forgetting factor of λ = 0.95. RLS-DCD is implemented with leading DCD (Table 12.12), B = 16
and H = 2. The maximum number of updates Nu is varied from 1 to 8. The optimum vector wo(n)
suffers an abrupt change at n = 400. The learning curves were obtained from the average of L = 500
realizations. As the ﬁgure shows, even for Nu = 1, the performance of RLS-DCD is very close to that of
RLS. The only difference is a smaller convergence rate at the start of the algorithm. However, after the
abrupt modiﬁcation of wo(n) at instant n = 400, all ﬁlters converge equally quickly to the new vector.
This shows that RLS-DCD does not loose in tracking capability when compared to standard RLS.
1.12.5.2 Regularization
There are applications in which the autocorrelation matrix Rφ may become very ill-conditioned (i.e.,
nearly singular). In these cases, it can be shown that the LMS weight estimates may slowly drift to quite
large values, even though the error remains small. The problem arises when the ﬁlter is implemented
in ﬁnite-precision arithmetic, because the variables holding the ﬁlter weights may overﬂow, thereby
making the error grow suddenly very large.
This can happen, for example, for certain kinds of equalizers in communications [87,92,93]. The
mechanism through which the LMS coefﬁcients may slowly drift is explained in [94,136].
The usual solution to this problem is to modify the cost function to add a term proportional to ∥w∥2,
so that large values of the weight vector are penalized:
Jleaky = E
d(n) −wHφ(n)

2
+ α∥w∥2,
(12.307)

730
CHAPTER 12 Adaptive Filters
where ∥·∥is the Euclidean norm and α > 0 is a small constant. With this cost function, the corresponding
algorithm is
w(n + 1) = (1 −μα)w(n) + μe∗(n)φ(n).
(12.308)
This is known as leaky-LMS algorithm [4]. It can be shown that the introduction of leakage will bias the
solution away from the Wiener solution, but will prevent any excessive growth of the weight coefﬁcients.
A detailed analysis of the leaky-LMS algorithm is available in [137]. Reference [94] proposes a modiﬁed
version of the algorithm without the bias.
Difﬁculties with singular or near-singular autocorrelation matrices also appears with RLS. For exam-
ple, if the input is a periodic signal with little noise, the autocorrelation matrix will become nearly
singular, and matrix P(n) in RLS will diverge. This problem, as well as a solution to it using variable
forgetting-factor, is described in [138]. A constant regularization method based on DCD can be found
in [139]. Another approach, valid only for short ﬁlters, but which allows constant regularization, is
described in [140].
The leaky-LMS algorithm has found other important applications, most particularly in beamforming,
in which it is used to turn the beamformer robust against array imperfections [141]. The algorithms used
in beamforming are examples of constrained adaptive ﬁlters, in which the cost function incorporates
equality conditions that must be satisﬁed at all times—see Section 1.12.5.11.
Another algorithm closely related to leaky-LMS, in which the extra term in the cost function penal-
izes large values of the ﬁlter output (that is, α|wH(n)φ(n)|2 instead of α∥w(n)∥2 in (12.307)), was
recently employed to improve the performance of FIR adaptive ﬁlters in the presence of saturation
nonlinearities [142].
More recent applications of regularization involve the use of the ℓ1-norm,
∥w∥1 = |w0| + |w1| + . . . + |wM−1|,
instead of the Euclidean norm. This is because the ℓ1 norm promotes solutions that are sparse, that is,
a ﬁlter that minimizes the cost function
Jsparse = E
d(n) −wHφ(n)

2
+ α∥w∥1,
(12.309)
will favor solutions in which the vector w has only a few nonzero entries (an intuitive explanation
for this can be found in [143], which is a good introduction to compressive sensing). Algorithms for
promoting sparsity have been receiving much attention lately, due to the interest in compressive sensing
applications [144–148]. See also the references for the proportionate NLMS (PNLMS) algorithm in
Section 1.12.5.3.
1.12.5.3 Variable step-size
Thesteady-stateanalysisofSection1.12.4.4showsthatthestep-size μplaysanimportroleincontrolling
the performance of the LMS algorithm since it is one of the main factors affecting the convergence rate
and the misadjustment. To obtain a better tradeoff between convergence speed and steady-sate EMSE,
several authors proposed different forms of adjusting the step-size of LMS. Variable step-size algorithms
allow the ﬁlters to dynamically adjust their performance in response to conditions in the input data and
error signals.

1.12.5 Extensions and Current Research
731
In this context, one of the most popular algorithms is the variable step-size LMS (VSLMS) proposed
in [149]. In the VSLMS algorithm, each coefﬁcient wk(n), k = 0, . . . , M −1 is updated with a time-
varying step-size μk(n), whose adjustment is given by
μk(n) = μk(n −1) + ρsign

e(n)φk(n)

sign[e(n −1)φk(n −1)],
(12.310)
where ρ is a small positive constant. This algorithm operates in a very intuitive way. When the algorithm
has not yet converged, the gradient term e(n)φk(n) shows a positive or negative direction in successive
iterations and the step-size μk(n) increases up to an allowed maximum value. On the other hand, near
steady-state e(n)φk(n) approaches zero and its sign changes in successive iterations, which reduces
μk(n) (the step-size is lower-bounded by a small minimum value). Therefore, during the initial conver-
gence VSLMS uses a large step-size, which leads to a high convergence rate. When its coefﬁcients are
close to the optimum solution, the algorithm is updated with a small step-size, which allows VSLMS
to achieve a small EMSE. A variant of this algorithm is obtained by dropping the sign functions in
(12.310), i.e.,
μk(n) = μk(n −1) + ρe(n)e(n −1)φk(n)φk(n −1).
(12.311)
The choice between (12.310) and (12.311) depends on the application [150].
The NLMS algorithm, proposed independently by Nagumo and Noda [151] and Albert and Gardner
[152],canbeinterpretedavariablestep-sizeLMSwithaparticularchoiceforμ(n)(seeSection1.12.3.2).
An important variant of NLMS is the so-called power-normalized LMS (PN-LMS) algorithm, which
uses the following variable step-size
μ(n) =
˜μ
ε + p(n),
(12.312)
where
p(n) = γ p(n −1) + (1 −γ )|φ0(n)|2,
(12.313)
with p(−1) = 0 and γ being a positive scalar chosen from within the interval 0 < γ ≤1. This
algorithm is useful when the regressor is a tapped-delay line, that is, when φi(n) = x(n −i), so that
the scalar p(n) is an estimate for the power of the input sequence {x(n)}. The range of ˜μ in (12.312) is
0 ≤˜μ ≤2
M [3].
Over the years, several variable step-size algorithms were proposed in the literature. Some of these
algorithms are in the papers [153–167] and in their references. Besides the variable step-size approaches,
an alternative scheme to improve the tradeoff between convergence speed and steady-sate EMSE is
constituted by combinations of adaptive algorithms, described in Section 1.12.5.8.1.
Another class of variable step-size algorithms was developed speciﬁcally to accelerate the conver-
gence of ﬁlters whose optimum weight vector wo is sparse, that is, has only a small fraction of nonzero
elements. The primary application for this is echo cancellation, mainly for echo due to reﬂections in the
telephone connection. The algorithms are known as proportionate NLMS (PNLMS) algorithms, and
have usually very good performance for approximating sparse weight vectors [168–172].

732
CHAPTER 12 Adaptive Filters
Table 12.13 Fourth- and Sixth-Order Moments of Different Distributions
Distribution
E{(x−μx )4}
σ 4x
E{(x−μx )6}
σ 6
x
Binary
1
1
Uniform
9/5
27/7
Gaussian
3
15
Exponential
6
90
1.12.5.4 Non-Gaussian noise and robust ﬁlters
Consider a problem in which we know that the regressor φ(n) and the desired signal d(n) are related
through a model (we assume for simplicity that all variables are real)
d(n) = wT
o φ(n) + v(n),
(12.314)
in which v(n) is independent of φ(n). It is shown in [173] that minimization of the fourth power of
the error may lead to an adaptive ﬁlter with better compromise between convergence rate and excess
mean-square error than LMS, if v(n) is sub-Gaussian. Let us explain what this means.
Recall that for any random variable x, it holds that 0 ≤σ 2
x = E{x2} −

E{x}
2. Similarly, if we
deﬁne μx = E{x},
E

(x −μx)4
≥

E

(x −μx)2 2
= σ 4
x ,
E

(x −μx)6
≥

E

(x −μx)2 3
= σ 6
x .
In general, we can write E{(x −μx)4} = ασ 4
x , with α ≥1. For example, consider the distributions
in Table 12.13. Distributions for which E
$
(x−μx)4%
σ 4x
< 3, such as binary and uniform, are called sub-
Gaussian, and those for which
E
$
x4%

E{(x−μx)2}
2 > 3, such as the exponential, are called super-Gaussian.
These relations are usually given in terms of the kurtosis γ4 of a distribution
γ4 = E
$
(x −μx)4%
σ 4x
−3.
Therefore, a distribution is sub-Gaussian if its kurtosis is negative, and super-Gaussian if its kurtosis
is positive. A distribution with positive kurtosis is such that its probability density function (pdf) f (x)
decays more slowly than the Gaussian as |x| →∞.
The authors of [173] show that when the noise v(n) is sub-Gaussian, an algorithm based on the
minimization of
JLMP = E{|e(n)|p},
(12.315)
with p > 2 results in lower steady-state excess mean-square error (EMSE) than LMS. They study
particularly the case p = 4, for which the stochastic gradient algorithm is of the form
w(n + 1) = w(n) + μe3(n)φ(n).
(12.316)

1.12.5 Extensions and Current Research
733
This algorithm is known as least-mean fourth (LMF) algorithm. Analyses of general algorithms with
error nonlinearities can be found in [174], and a uniﬁed analysis is available in [175].
The LMF algorithm can be understood intuitively as a variable-step version of LMS:
w(n + 1) = w(n) +

μe2(n)
 
e(n)φ(n).
When the error is large, the “equivalent” step-size μe2(n) is large, and the ﬁlter converges quickly.
On the other hand, when the error is small, the “equivalent” step-size is reduced, which consequently
reduces the EMSE.
This idea works well if the distributions of φ(n) and v(n) are indeed sub-Gaussian, but it was soon
noticed that the algorithm becomes sensitive to outliers (large but rare values of the input signals). This
is also easy to understand: if the error is too large, the “equivalent” step-size may be such that the
recursion becomes unstable. References [176,177] proposed changing the cost function to a mixture of
quadratic and quartic, in an attempt to reduce this sensitivity. In [178, p. 313], a different modiﬁcation
of the cost function is proposed, with the same purpose. The new cost function is the following
JLMF, mod = E
$
f

e(n)
%
,
f (e) =
e4, if |e| ≤γ,
e2, if |e| > γ,
(12.317)
in which γ is a constant. References [45,59] show that when the regressor is Gaussian, LMF has a
non-zero probability of diverging. From these results one can see that the solution proposed in [178]
indeed stabilizes the algorithm.
Since the LMF recursion contains a cubic nonlinearity, it can be used as a simpler platform to
understand the behavior of blind-equalization algorithms such as CMA (see Section 1.12.5.5), which
also contains a cubic nonlinearity.
Set-membership algorithms [1], which are explained in more detail in Section 1.12.5.10, can be seen
as an extreme version of (12.317), in which f (e) is reduced to simply 0 when |e| ≤γ . The corresponding
cost function is
JSM = E
$
f

e(n)
%
,
f (e) =
0,
if |e| ≤γ,
e2, if |e| > γ.
(12.318)
Set-membership algorithms are particularly well-suited to bounded noise, i.e., to the case in which one
knows that |v| ≤γ .
All these methods are suited to the case in which the noise v(n) is sub-Gaussian. When the noise
is super-Gaussian, there is a small but signiﬁcant probability that v(n) will assume a very large value.
This may make the performance of even LMS become poor. The solution is to give less weight to large
errors, and one choice is to use p < 2 in (12.315). This will give rise to several robust adaptive ﬁlters,
that is, algorithms that are insensitive to outliers. It should be noticed that LMS itself is quite robust in
terms of energy (quadratic) relations: in fact, [179] shows that LMS is optimal in the H∞sense (see also
[178, ch. 17]). However, for noise distributions with higher kurtosis, it is convenient to use a smaller
value of p [180–183].
One important algorithm of this class is the sign-LMS algorithm [98,184], which is obtained from
(12.315) with p = 1, and which we already saw in Section 1.12.5.1.1.
For very impulsive signals, methods based on order statistics (such as the median, for example), may
produce better results, although their complexity tends to be higher [185].

734
CHAPTER 12 Adaptive Filters
adaptive
ﬁlter
blind
algorithm
HOS of
s(n)
channel
η(n)
e(n)
s(n)
x (n)
y(n)
H(z)
ˆ
FIGURE 12.36
Simpliﬁed communications system with a blind adaptive equalizer.
1.12.5.5 Blind equalization
As described in Section 1.12.1.3, the objective of equalization is to mitigate the intersymbol interference
(ISI) introduced by dispersive channels in order to recover the transmitted sequence. Assuming that the
equalizer is an adaptive FIR ﬁlter, its coefﬁcients may be updated in two different ways:
i.
using a supervised algorithm in the training and decision-directed modes, as explained in Section
1.12.1.3; or
ii.
using a blind equalization algorithm, which uses higher-order statistics (HOS) of the transmitted
signal instead of a training sequence, as shown in Figure 12.36. In this case, the available bandwidth
is used in an more efﬁcient manner due to the absence of a training sequence.
The literature contains many blind equalization algorithms based on HOS, but the most popular is
the constant modulus algorithm (CMA), proposed independently by Godard [186] and Treichler and
Agee [187] in the 1980s. CMA seeks to minimize the constant-modulus cost function deﬁned as
JCM = E

(r −| ˆy(n)|2)2
,
(12.319)
where r = E{|s(n)|4}/E{|s(n)|2} is a dispersion constant, which contains information about higher-
order statistics of the transmitted signal s(n) and ˆy(n) = wH x(n) is the equalizer output. This function
penalizesdeviationsinthemodulusoftheequalizedsignalawayfromthedispersionconstantr.Different
from the mean-square-error cost function used in supervised adaptive ﬁltering, JCM is not convex in
relation to the coefﬁcients of the equalizer (see the deﬁnition of convex functions in Box 5). It has local
minima, and constant-modulus-based algorithms can get stuck at these suboptimal solutions.
CMA is obtained from an instantaneous approximation for the gradient of JCM in relation to w.
Deﬁning the estimation error
e(n) = (r −| ˆy(n)|2) ˆy(n),
(12.320)

1.12.5 Extensions and Current Research
735
0
0
0
0.6
0.5
−0.6
1
1
1.5
−1
global
local
minima
minima
local maximum
J CM
w0
w1
FIGURE 12.37
Constant-modulus cost function as a function of the equalizer weights.
the update equation of CMA can be written as
w(n + 1) = w(n) + μe∗(n)x(n),
(12.321)
where μ stands for a step-size. Since CMA is a stochastic-gradient type algorithm, the similarity of
(12.321) to the LMS update equation is not surprising. Due to this similarity, CMA is sometimes inter-
preted as a blind version of the LMS algorithm [188]. Thus, as in LMS, small step-sizes lead to a small
misadjustment and slow convergence. However, the similarity to LMS stops here. The multimodality
of the CM cost function makes the prediction of the behavior of CMA a hard task.
To illustrate the constant-modulus cost function, we assume the transmission of a binary signal
{−1, +1} through the IIR channel H(z) = 1/(1 + 0.6z−1) in the absence of noise [189]. Considering
an FIR equalizer with M = 2 coefﬁcients, we can obtain a 3-D plot of JCM as a function of w0 and w1
as shown in Figure 12.37. As indicated in the ﬁgure, JCM contains two global minima at ±[1 0.6]T and
two local minima. Note that if w converges to one of the global minima, the transmitted sequence or
its inverse will be recovered. These two possibilities occur due the fact that CMA seeks to recover only
the modulus of the transmitted signal and does not solve phase ambiguities introduced by the channel.
The phase ambiguities can be corrected by using differential code, which is based on the change of sign
between two successive samples.
The animations in Figure 12.38 illustrate the behavior of CMA initialized at the different points.
Initializing at w(0) = [1 0]T , the weight vector converges to [+1 + 0.6]T . Now, considering w(0) =
[−0.4 0.05]T , we get w(n) ≈[−1 −0.6]T at the steady-state. On the other hand, if CMA is initialized
at [0.05 −0.4]T , the algorithm gets stuck at a local minimum. We should notice that in the presence
of noise or depending on the step-size, CMA also can escape from a minimum and drift to another
solution.

736
CHAPTER 12 Adaptive Filters
0
0
0.6
−0.6
1
−1
w 0
w 1
(a)w(0) = [1 0] T
0
0
0.6
−0.6
1
−1
w 0
w 1
(b)w(0) = [ −0.4 0.05]T
0
0
0.6
−0.6
1
−1
w 0
w 1
(c)w(0) = [0 .05
−0.4]T
FIGURE 12.38
CMA initialized at different points, μ = 0.016, channel H(z) = 1/(1 + 0.6z−1) in the absence of noise.
Click on each caption to see animations on your browser.
Despite the name of the algorithm, CMA also works for non-constant modulus signals. However,
as shown analytically in [80,190], its best performance occurs for constant-modulus signals since the
variability in the transmitted signal modulus plays a role similar to the measurement noise in supervised
algorithms.
The stochastic analysis of constant-modulus based algorithms is a hard task. Due to the nonlinearity
and multimodality of the CM cost function, additional assumptions are necessary. A good review of
the main results in the analysis of CMA can be found in [191], but many results were obtained after its

1.12.5 Extensions and Current Research
737
publication. These results address different issues of constant-modulus based algorithms as the steady-
state and transient behavior [77,80,190,192–196], convergence and stability [58,60,197–200], phase
rotation [201–203], solutions for non-constant modulus signals [204–207], among others.
1.12.5.6 Subband and transform-domain adaptive ﬁlters
In transform-domain adaptive ﬁlters, the input signals are transformed (using the FFT, the DCT or other
convenient transform), and the adaptation is performed in the transform domain. The gains are twofold:
ﬁrst, using block processing, one can obtain large reductions in computational cost. Second, by using
the decorrelation properties of the transforms, it is possible to use different step-sizes for each tap in the
transformed domain, thereby reducing the equivalent eigenvalue spread and increasing the convergence
rate [208–216].
A related approach is that of subband adaptive ﬁlters, which are also useful to both reduce the
computational cost and increase the convergence speed of long ﬁlters, in particular for acoustic echo.
The main idea is to split the input signal in several parts using an analysis ﬁlter bank. Each part, relative
to a particular band of the full spectrum, is processed separately, and ﬁnally all parts are joined together
again by a synthesis ﬁlter bank [217]. Since each subsignal has a smaller bandwidth, the sampling rate
can be reduced. The whole process must be made carefully to avoid as much as possible a deterioration
of the signals due to aliasing, while still gaining in computational cost, in comparison with standard
full-band ﬁlters [218–224]. It is important to point out that it is possible to avoid the introduction of
extra delay (compared with a full-band ﬁlter) [225–228].
Good introductions to the subject can be found in [1,3,150].
1.12.5.7 Afﬁne projections algorithm
The afﬁne projections algorithm (APA) is an extension of NLMS, designed to obtain a convergence rate
closer to that of RLS, but with a low computational cost, as close as possible to that of NLMS. We saw
in Section 1.12.3.2 that NLMS can be derived as a method for choosing the step-size so that the current
a posteriori estimation error is zeroed (12.130). The APA extends this idea, by canceling a vector of a
posteriori errors, obtained from data from times n −K to n, and the weight vector at time n + 1:
ξ(n) = d(n) −(n)w∗(n + 1),
(12.322)
in which
d(n) =
⎡
⎢⎢⎢⎣
d(n)
d(n −1)
...
d(n −K −1)
⎤
⎥⎥⎥⎦,
(n) =
⎡
⎢⎢⎢⎣
φT (n)
φT (n −1)
...
φT (n −K −1)
⎤
⎥⎥⎥⎦.
(12.323)
The afﬁne projections algorithm chooses as next weight estimate the vector w(n + 1) closest to the
current estimate w(n) such that ξ(n) = 0 [229–234]:
w(n + 1) = arg
min
w
s.t. ξ(n)=0
|w −w(n)|2 .
(12.324)

738
CHAPTER 12 Adaptive Filters
The solution to this problem is a recursion
w(n + 1) = w(n) + T (n)

∗(n)T (n)
 −1
e(n),
(12.325)
where
e(n) = d(n) −(n)w∗(n).
(12.326)
Note that (12.325) reduces to NLMS when K = 1. Similarly to what is done for NLMS, it is
convenient to introduce a regularization term εI, with ε > 0 and a step-size 0 < μ ≤1 in (12.325),
leading to
w(n + 1) = w(n) + μT (n)

εI + ∗(n)T (n)
 −1
e(n).
(12.327)
The advantage of this algorithm is that ∗(n)T (n) is K × K (as opposed to the M × M matrix
that must be recursively inverted for RLS), and the value of K does not need to be large, which keeps
the computational cost low. In general, a value of two or three already gives a good improvement to the
convergence rate even for regressors whose covariance matrix has large spreading factor.
In order to implement (12.327), it is useful to remember that computing the inverse explicitly is never
a good idea: it is much better to solve the linear system

εI + ∗(n)T (n)
 
a = e(n),
(12.328)
and then compute T (n)a. From a numerical point of view, the best option would be to use the QR
decomposition of (n) and avoid forming ∗(n)T (n), but the computational cost would be larger.
When the regressor is a tapped-delay line and K is large, one can use a fast version of APA, in which
the solution of (12.328) is obtained in O(K) operations, instead of the usual O(K 3) [235–237].
Recent works focusing in the analysis of the APA are [238–241]. The problem of regularization
(choice of ε) is considered in [242,243]. Applications to echo cancellation can be found in [244,245].
A related approach is the sliding-window RLS algorithm, a version of RLS in which only a ﬁnite
memory is considered, instead of a forgetting factor as used in Section 1.12.3.3 [246,247].
1.12.5.8 Cooperative estimation
In this section, we brieﬂy describe important advances in adaptive ﬁltering through cooperative esti-
mation. We ﬁrst describe combinations of adaptive ﬁlters and in the sequel, we focus on distributed
adaptive ﬁltering.
1.12.5.8.1
Combinations of adaptive ﬁlters
Combinations of adaptive ﬁlters have received considerable attention lately, since they decrease the
sensitivity of the ﬁlter to choices of parameters such as the step-size, forgetting factor or ﬁlter length.
The idea is to combine the outputs of two (or several) different independently-run adaptive algorithms
to achieve better performance than that of a single ﬁlter. In general, this approach is more robust than
variable parameter schemes [190].

1.12.5 Extensions and Current Research
739
φ( n )
d(n)
e(n)
e1(n)
e2(n)
ˆy1(n)
y2(n)
ˆy ( n )
η( n )
1 −η( n )
w1(n)
w2(n)
w(n)
FIGURE 12.39
Convex combination of two transversal adaptive ﬁlters.
The ﬁrst combined scheme that attracted attention was the convex combination of adaptive ﬁlters
due to its relative simplicity and the proof that the optimum combination is universal, i.e., the combined
estimate is at least as good as the best of the component ﬁlters in steady-state, for stationary inputs [248].
This scheme was proposed in [249] and further extended and analyzed in [248,250]. The original idea
was to combine one fast and one slow LMS ﬁlter to obtain an overall ﬁlter with fast convergence and low
misadjustment, but it was extended to other algorithms to take advantages of different characteristics,
as explained below.
Figure 12.39 shows the convex combination of two adaptive ﬁlters, in which the output of the overall
ﬁlter is computed as
ˆy(n) = η(n) ˆy1(n) + [1 −η(n)] ˆy2(n),
(12.329)
where η(n) ∈[0, 1] is the mixing parameter, ˆyi(n) = wH
i (n)φ(n), i = 1, 2, are the outputs of the
transversal ﬁlters, φ(n) is the common regressor vector, and wi(n) are the weight vectors of the com-
ponent ﬁlters. Note that the weight vector and the estimation error of the overall ﬁlter are given respec-
tively by
w(n) = η(n)w1(n) + [1 −η(n)]w2(n)
(12.330)
and
e(n) = d(n) −ˆy(n) = η(n)e1(n) + [1 −η(n)]e2(n),
(12.331)
where ei(n) = d(n) −ˆyi(n) are the estimation errors of each component ﬁlter.

740
CHAPTER 12 Adaptive Filters
In order to restrict the mixing parameter to the interval [0, 1] and to reduce gradient noise when
η(n) ≈0 or η(n) ≈1, a nonlinear transformation and an auxiliary variable a(n) are used, i.e.,
η(n) = sgm[a(n)] ≜
1
1 + e−a(n) ,
(12.332)
where a(n) is updated to minimize the overall squared error e2(n), i.e.,
a(n + 1) = a(n) −μa
2 ∇ae2(n) = a(n) + μa

ˆy1(n) −ˆy2(n)

e(n)η(n)

1 −η(n)

.
(12.333)
In practice, a(n) must be restricted by saturation of (12.333) to an interval [−a+, a+], since the factor
η(n)

1 −η(n)

would virtually stop adaptation if a(n) were allowed to grow too much. The correct
adjustment of the step-size μa depends on the input signal and additive noise powers and also on the
step-sizes of the component ﬁlters. To make the choice of μa independent of the ﬁltering scenario, a
normalized scheme was proposed in [251].
The combination of one fast and one slow LMS (with μ1 > μ2) operates in a very intuitive way.
When fast changes appear, the fast ﬁlter outperforms the slow one, making η(n) ≈1. In stationary
situations, the slow ﬁlter gets a lower quadratic error and η(n) ≈0, which allows the overall ﬁlter
to achieve the misadjustment of the slow LMS ﬁlter. There are situations in which the combination
outperforms each component ﬁlter and in these cases, 0 < η(n) < 1. This behavior can be observed in
the simulation results shown in Figure 12.40, where the convex combination of two LMS ﬁlters with
different step-sizes (μ1 = 0.1 and μ2 = 0.01) was used to identity the system (from [248])
 0.9003 −0.5377 0.2137 −0.0280 0.7826 0.5242 −0.0871 
.
The regressor φ(n) is obtained from a process x(n) generated with a ﬁrst-order autoregressive model,
whose transfer function is
√
1 −b2/(1−bz−1). This model is fed with an iid Gaussian random process,
whose variance is such that Tr(Rφ) = 1. Moreover, additive iid noise v(n) with variance σ 2
v = 0.01 is
added to form the desired signal. Figure 12.40a shows the EMSE curves estimated from the ensemble-
average of 500 independent runs and ﬁltered by a moving-average ﬁlter with 128 coefﬁcients to facil-
itate the visualization. The combined ﬁlter acquires the faster convergence of μ1-LMS and attains the
EMSE of μ2-LMS. Figure 12.40b shows the time evolution of the mixing parameter. We can observe
that it rapidly changes toward sgm[a+] during the initial convergence, while its steady-state value is
1 −sgm[a+].
After the publication of [249], many papers on combinations of adaptive ﬁlters appeared in the
literature. Apparently, the idea of combining the outputs of several different independently-run adaptive
algorithms was ﬁrst proposed in [252], and later improved in [253,254]. The algorithms proposed
in [252–254] are based on a Bayesian argument, and construct an overall (combined) ﬁlter through a
linear combination of the outputs of several independent adaptive ﬁlters. The weights are the a posteriori
probabilities that the underlying models used to describe each individual algorithm are “true.” Since the
weights add up to one, in a sense these ﬁrst papers also proposed “convex” combinations of algorithms.
Unconstrained linear combinations of adaptive ﬁlters were also proposed in [255]. However, the method
of [248,249] has received more attention due to its relative simplicity and the proof that the optimum
combination is universal. By this, we mean that if the combination uses at every instant the optimum

1.12.5 Extensions and Current Research
741
combination
µ2-LMS
µ1-LMS
EMSE(dB)
(a)
0
0
1
2
3
−10
−20
−30
−40
−50
E {η( n ) }
(b)
0
0
1
1
0.5
× 10 4
2
3
iterations
FIGURE 12.40
(a) EMSE for μ1-LMS, μ2-LMS, and their convex combination; (b) ensemble-average of η(n); μ1 = 0.1, μ2 =
0.01, μa = 100, a+ = 4, b = 0.8; mean of 500 independent runs.
value of η, then the combined ﬁlter is always at least as good as the best component ﬁlter. However, in
practice η must also be estimated, so the performance of the combination may be slightly worse than
that of the best ﬁlter.
In the sequel, we brieﬂy describe some of the most important recent contributions in this area.
In [250], the convergence of the overall ﬁlter is greatly improved by transferring a part of the fast
ﬁlter w1 to the slow ﬁlter w2 when λ(n) ≥γ , that is,
w2(n + 1) = α

w2(n) + μ2e2(n)x(n)

+ (1 −α)w1(n),
(12.334)
where α is a parameter close to 1 and γ is a threshold close to the maximum value that can be reached
by λ(n).
A steady-state analysis of the combined ﬁlter was presented in [248]. This analysis shows that the
convex combination is universal if the optimum η is used, a property that was exploited to design ﬁlters

742
CHAPTER 12 Adaptive Filters
with improved tracking performance. Due to the nonlinear function (12.332), a transient analysis of
the convex combination needs additional simpliﬁcations. This issue was addressed in [256] and [257],
which provide models for the transient of the convex combination based on ﬁrst and second-order Taylor
series approximations.
The convex combination was used in [258] to improve the performance of a variable tap-length LMS
algorithm in a low signal-to-noise environment (SNR ≤0 dB). The adaptation of the tap-length in
the variable tap-length LMS algorithm is highly affected by the parameter choice and the noise level.
Combination approaches improve such adaptation by exploiting advantages of parallel adaptive ﬁlters
with different parameters.
The convex combination was exploited in [190] to improve the tracking performance of adaptive
ﬁlters by combining ﬁlters with different tracking capabilities, as are the cases of LMS and RLS (see
Section 1.12.4.4). The combination of algorithms of different families was also addressed in [259],
where it was shown that a combination of two ﬁlters from the same family (i.e., two LMS or two
RLS ﬁlters) cannot improve the performance over that of a single ﬁlter of the same type with optimal
selection of the step-size (or forgetting factor). However, combining LMS and RLS ﬁlters, it is possible
to simultaneously outperform the optimum LMS and RLS ﬁlters. In other words, combination schemes
can achieve smaller errors than optimally adjusted individual ﬁlters.
Convex combinations were used for sparse echo cancellation in [260], considering the combination
of two improved proportionate normalized least-mean-square (IPNLMS) ﬁlters. The combined scheme
increases the IPNLMS robustness to channels with different degrees of sparsity and also alleviates
the tradeoff between rate of convergence versus steady-state misadjustment imposed by the selection
of the step-size. Still considering the echo cancellation application, [261] proposes a combination of
adaptive Volterra kernels that presents a similar behavior to that of the complete Volterra ﬁlters, but
with a reduction in the computational load. This scheme is robust regardless of the level of nonlinear
distortion, which is a desired property for nonlinear echo cancellation.
In [262], the convex combination was also used as a scheme for adaptively biasing the weights of
adaptive ﬁlters using an output multiplicative factor. Interpreting the biased estimator as the combination
of the original ﬁlter and a ﬁlter with constant output equal to zero, practical schemes to adaptively adjust
the multiplicative factor were proposed. This scheme provides a convenient bias versus variance tradeoff,
leading to reductions in the ﬁlter mean-square error, especially in situations with a low signal-to-noise
ratio.
Extending [248], the authors of [263] proposed an afﬁne combination of two LMS algorithms, where
the condition on the mixing parameter is relaxed, allowing it to be negative. Thus, this scheme can be
interpreted as a generalization of the convex combination since the mixing parameter is not restricted
to the interval [0, 1]. This approach allows for smaller EMSE in theory, but suffers from larger gradient
noise in some situations. Under certain conditions, the optimum mixing parameter was proved to be
negative in steady-state. Although the optimal linear combiner is unrealizable, two realizable algorithms
were introduced. One is based on a stochastic gradient search and the other is based on the ratio of the
average error powers from each individual adaptive ﬁlter. Under some circumstances, both algorithms
present performance close to the optimum.
Similarly to the convex combination, the correct adjustment of the step-size for the updating of
the mixing parameter in the afﬁne combination (using the stochastic algorithm of [263]) depends on
characteristics of the ﬁltering scenario. This issue was addressed in [193], where a transient analysis for

1.12.5 Extensions and Current Research
743
N k
Node k
{dk (n ), φk (n )}
FIGURE 12.41
Diffusion network with N = 7 nodes: at time n, every node k takes a measurement {dk(n), φk(n)}.
the afﬁne combination of two adaptive ﬁlters was presented. The results of this analysis were used to
facilitate the adjustment of the free parameters of the scheme and to propose two normalized algorithms
to update the mixing parameter. Recently, the scheme based on the ratio of error powers of the two
ﬁlters proposed in [263] to update the mixing parameter was analyzed in [264].
Finally, Kozat et al. [265] studied different mixing strategies in which the overall output is formed
as the weighted linear combination (not necessarily constrained to convex or afﬁne, as before) of the
outputs of several component algorithms for stationary and certain nonstationary data.
1.12.5.8.2
Distributed adaptive ﬁltering
Adaptive networks use the information from data collected at nodes distributed over a certain region.
Each node collects noisy observations related to a parameter of interest and interacts with its neighbors
considering a certain network topology. The objective is to obtain an estimate of the parameter of interest
as accurate as the one that would be obtained if each node had access to the information across the entire
network [266,267]. Distributed estimation algorithms are useful in several contexts, including wireless
and sensor networks, where scalability, robustness, and low power consumption are desirable. They
also ﬁnd applications in precision agriculture and environmental monitoring and transportation [268].
Figure 12.41 shows a network composed by N nodes distributed over some geographical area. At time
instant n, every node k takes a measurement {dk(n), φk(n)} to estimate some parameter vector wo. The
scalar measurement dk(n) represents the desired signal and φk(n) denotes the length-M input regressor
vector for node k. There are different solutions to the problem of estimating wo. In the centralized
solution, every node transmits its data {dk(n), φk(n)} to a fusion center for processing. This solution
is non-robust to failure in the fusion center. In distributed solutions, every node exchanges information
with a subset of its neighboring nodes, and the processing is distributed among all nodes in the network.
The set of nodes connected to node k (including k itself) is denoted by Nk and is called the neighborhood
of node k.
There are three main network topologies used in distributed estimation: incremental, diffusion, and
probabilistic diffusion [267]. In incremental networks, the nodes are connected so that information ﬂows
sequentially from node to node, in a cyclic manner (Figure 12.42a). In diffusion networks, each node
communicates with its entire neighborhood at each instant (Figure 12.42b). Finally, in probabilistic

744
CHAPTER 12 Adaptive Filters
Node k
(a) Incremental
Nk
Node k
(b) Diffusion
Nk
Node k
(c) Probabilistic  diffusion
FIGURE 12.42
Different network topologies.
diffusion networks each node communicates with a (possibly random) subset of its neighbors at each
instant (Figure 12.42c).
Several estimation algorithms have been proposed in the context of distributed adaptive ﬁltering,
such as incremental LMS, incremental RLS, diffusion LMS, diffusion RLS, diffusion Kalman ﬁltering,
and smoothing algorithms. In these algorithms, the nodes share information with their neighbors, and
perform local adaptation and merging of information using convex combinations of available estimates.
Distributed algorithms only require local communications between neighboring nodes, and can attain
good estimation performance compared to centralized solutions. This is a current area of intense research
[268–280].
Recently, adaptive distributed algorithms have also been used to model biological systems due to
their the self-organization property. Examples include ﬁsh joining together in schools, bacterial motility,
and bird ﬂight formations [281–286].
1.12.5.9 Adaptive IIR ﬁlters
Adaptive inﬁnite impulse response (IIR) ﬁlters represent an advantageous alternative in relation to
adaptive FIR ﬁlters, due to their capacity of providing long impulse responses with a small number of
coefﬁcients. Their use may be desirable in applications requiring hundreds or thousands of taps, such as
satellite-channel and mobile-radio equalizers. The advantages of adaptive IIR ﬁlters have a price: slow
convergence, possible instability, error surface with local minima or biased global minimum [1,287–
290]. In addition, not all long impulse responses will necessarily be well approximated by an IIR ﬁlter
with low order.
The nonlinear relation between the adaptive-ﬁlter coefﬁcients and the internal signals makes the
gradient computation and convergence analysis more complicated than those of adaptive FIR ﬁlters
[1]. Over the years, problems related to local minima, stability, and the effect of poles close to the unit

1.12.5 Extensions and Current Research
745
φ(n)
d(n) −w T (n)φ(n) = γ
d(n) −w T (n)φ(n) = −γ
d(n) −w T (n)φ(n) = 0
S(n)
FIGURE 12.43
Constraint imposed by data at time n.
circle have been addressed by several authors leading to different adaptive algorithms and realization
structures [291–300].
Another difﬁculty with adaptive IIR ﬁlters is that the performance of simple constant-gain algorithms
may rapidly degrade as the order of the ﬁlter grows. Even in the absence of local minima, the adaptation
of ﬁlters with order greater than two can remain almost stopped in regions where the mean square error
is relatively high. This issue was addressed in [287], which was able to obtain a large acceleration in
the convergence rate of IIR ﬁlters.
1.12.5.10 Set-membership and projections on convex sets
When the desired signal and the regressor are related as
d(n) = wH
o φ(n) + v(n),
where v(n) is bounded, that is, |v(n)| ≤γ for all n, it is natural to consider that each pair

d(n), φ(n)

deﬁnes a region in which the true solution w∗must lie:
wo ∈S(n) =

w ∈CM :
d(n) −wHφ(n)
 ≤γ

.
(12.335)
For example, if all vectors are two-dimensional and real, the region S(n) would be as in Figure 12.43.
Set-membership algorithms aim to ﬁnd the set of possible solutions, by tracking the intersection
T (n) =
n/
k=0
S(k)
(12.336)
of all S(n) [301]. In the control literature and earlier algorithms, this is made by approximating T (n) by
ellipsoids [301–303]. However, very good performance is obtained if one uses just any point contained
in T (n) as a point estimate of wo at each instant. In fact, it is more interesting to restrict the memory,

746
CHAPTER 12 Adaptive Filters
so that the ﬁlter can track variations of wo. In this case, only the last K constraint sets S(n) are used:
TK (n) =
n/
k=n−K+1
S(k).
(12.337)
This approach has a very nice property: close to steady-state, if the weight estimates are already inside
TK (n), it is not necessary to perform any update. This may reduce considerably the average complexity
of the ﬁlter [304–310].
More recently, the approach has been extended to a more general setting, using projections onto
convexsets andsubgradients. This allows theextensionof theresults tonon-differentiablecost functions,
with different descriptions for the constraints (not restricted to (12.335)), also allowing the design of
kernel adaptive ﬁlters [6,311–313] (an introduction to kernel adaptive ﬁlters can be found in [314]).
1.12.5.11 Adaptive ﬁlters with constraints
In some applications, one wishes to minimize a cost function given certain constraints: a typical case is
beamforming. In the simplest example, an array of antennas or microphones receives a signal coming
from a certain (known) direction, and interference from other (unknown) directions. The output of the
array is a linear combination of the signals φi(n) received at each antenna, e.g.,
ˆy(n) = w∗
0φ0(n) + w∗
1φ1(n) + · · · + w∗
M−1φM−1(n).
One way of improving the signal-to-noise ratio at the output is to minimize the output power, under the
restriction that the gain of the array in the direction of the desired signal is one. This is described by a
constraint of the kind
cHw = 1,
where w = [w0 . . . wM−1] is the weight vector, and c ∈CM is a vector describing the constraint. The
goal of the ﬁlter would then be to ﬁnd the optimum wo that is the solution of
wo = arg
min
w
s.t. cH w=1
 ˆy(n)
2 .
This solution is known as minimum-variance distortionless response (MVDR), or Capon beamformer,
since it was originally proposed in [315]. There is an extensive literature on this subject [312,316–325].
Introductions to the topic can be found in [326–328].
1.12.5.12 Reduced-rank adaptive ﬁlters
In applications in which large ﬁlters are necessary, such as acoustic echo cancellation and nonlinear
models using Volterra expansions, it may be useful to reduce the order of the problem, by working in
a smaller subspace of the original problem. This reduces the number of parameters to be estimated,
increasing the convergence speed and reducing the excess mean-square error [329]. Several methods
have been proposed, such as [330,331], based on eigen-decompositions, the multi-stage Wiener ﬁlter
of [332–334], the auxiliary vector ﬁltering algorithm [335], and the joint iterative optimization method
of [336–339]. This last method has the advantage of having low computational cost.

References
747
Box 8: [Fixed-point arithmetic]
In ﬁxed-point arithmetic, all variables are restricted to lie in a ﬁxed interval [−a, a), and are stored as a
sequence of B bits b0b1 . . . bB−1. Assuming that two’s complement arithmetic is used and that a = 1 ,
then b0 represents the signal of the variable and the sequence of bits represents the number
−b0 +
B−1

m=1
bm2−m.
Acknowledgment
The authors thank Prof. Wallace A. Martins (Federal University of Rio de Janeiro) for his careful reading and
comments on the text.
Relevant Theory: Signal Processing Theory and Machine Learning
See this Volume, Chapter 3 Discrete-Time Signals and Systems
See this Volume, Chapter 4 Random Signals and Stochastic Processes
See this Volume, Chapter 6 Digital Filter Structures and Their Implementation
See this Volume, Chapter 11 Parametric Estimation
See this Volume, Chapter 25 A Tutorial on Model Selection
References
[1] P.S.R. Diniz, Adaptive Filtering: Algorithms and Practical Implementation, third ed., Springer, 2008.
[2] S. Haykin, Adaptive Filter Theory, fourth ed., Prentice Hall, 2001.
[3] A.H. Sayed, Adaptive Filters, Wiley-IEEE Press, 2008.
[4] B. Widrow, S.D. Stearns, Adaptive Signal Processing, Prentice Hall, Englewood Cliffs, 1985.
[5] M.D. Miranda, M. Gerken, M.T.M. Silva, Efﬁcient implementation of error-feedback LSL algorithm, Elec-
tron. Lett. 35 (16) (1999) 1308–1309.
[6] S. Theodoridis, K. Slavakis, I. Yamada, Adaptive learning in a world of projections, IEEE Signal Process.
Mag. 28 (1) (2011) 97–123.
[7] B. Widrow, M. Hoff, Adaptive switching circuits, in: IRE WESCON Conv. Rec., pt. 4, 1960, pp. 96–104.
[8] B. Widrow, Thinking about thinking: the discovery of the LMS algorithm, IEEE Signal Process. Mag. 22
(1) (2005) 100–106.
[9] A.V. Oppenheim, A.S. Willsky, Signals and Systems, Prentice-Hall, 1996.
[10] R.A. Horn, C.R. Johnson, Matrix Analysis, Cambridge University Press, 1987.
[11] R.A. Horn, C.R. Johnson, Topics in Matrix Analysis, Cambridge University Press, Cambridge, MA, 1991.
[12] A.J. Laub, Matrix Analysis for Scientists and Engineers, Society for Industrial and Applied Mathematics,
PA, December 2005.
[13] C.D. Meyer, Matrix Analysis and Applied Linear Algebra, SIAM, Philadelphia, USA, 2000.
[14] G.H. Golub, C.F.V. Loan, Matrix Computations, third ed., Johns Hopkins, 1996.

748
CHAPTER 12 Adaptive Filters
[15] K.J. Åström, B. Wittenmark, Adaptive Control, Addison-Wesley, 1995.
[16] A. Leon-Garcia. Probability, Statistics, and Random Processes for Electrical Engineering, third ed., Prentice
Hall, January 2008.
[17] T. Kailath, A.H. Sayed, B. Hassibi, Linear Estimation, Prentice Hall, NJ, 2000.
[18] P. Ioannou, B. Fidan, Adaptive Control Tutorial, SIAM, Society for Industrial and Applied Mathematics,
December 2006.
[19] T. Gansler, J. Benesty, New insights into the stereophonic acoustic echo cancellation problem and an adaptive
nonlinearity solution, IEEE Trans. Speech Audio Process. 10 (5) (2002) 257–267.
[20] H.I.K. Rao, B. Farhang-Boroujeny, Fast LMS/Newton algorithms for stereophonic acoustic echo cancelation,
IEEE Trans. Signal Process. 57 (8) (2009) 2919–2930.
[21] C.H. Hansen, S.D. Snyder, Active Control of Noise and Vibration, Taylor & Francis, 1997.
[22] S.M. Kuo, D.R. Morgan, Active Noise Control Systems – Algorithms and DSP Implementations, Wiley
Series in Telecommunications and Signal Processing, Wiley-Interscience, 1996.
[23] P.A. Nelson, S.J. Elliot, Active Control of Sound, Academic Press, 1992.
[24] S.D. Snyder, Active Noise Control Primer, Springer-Verlag, 2000.
[25] F.G. de Almeida Neto, V.H. Nascimento, M.T.M. Silva, Reduced-complexity widely linear adaptive estima-
tion, in: 7th International Symposium on Wireless Communication Systems, September 2010, pp. 399–403.
[26] B. Picinbono, Wide-sense linear mean square estimation and prediction, in: International Conference on
Acoustics, Speech, and Signal Processing, 1995, ICASSP-95, vol. 3, May 1995, pp. 2032–2035.
[27] B. Picinbono, P. Bondon, Second-order statistics of complex signals, IEEE Trans. Signal Process. 45 (2)
(1997) 411–420.
[28] T. Adali, H. Li, R. Aloysius, On properties of the widely linear MSE ﬁlter and its LMS implementation, in:
43rd Annual Conference on Information Sciences and Systems, March 2009, pp. 876–881.
[29] A. Chinatto, C. Junqueira, J.M.T. Romano, Interference mitigation using widely linear arrays, in: 17th
European Signal Processing Conference, September 2009, pp. 353–357.
[30] S.C. Douglas, D.P. Mandic, Performance analysis of the conventional complex LMS and augmented complex
LMS algorithms, in: Proc. IEEE Int. Acoustics Speech and Signal Processing (ICASSP) Conf., 2010, pp.
3794–3797.
[31] D.P. Mandic, S. Still, S.C. Douglas, Duality between widely linear and dual channel adaptive ﬁltering, in:
Proc. IEEE Int. Conf. Acoustics, Speech and Signal Processing, ICASSP 2009, 2009, pp. 1729–1732.
[32] P.M. Djuric, J.H. Kotecha, J. Zhang, Y. Huang, T. Ghirmai, M.F. Bugallo, J. Miguez, Particle ﬁltering, IEEE
Signal Process. Mag. 20 (5) (2003) 19–38.
[33] W.R. LePage, Complex Variables and the Laplace Transform for Engineers, Dover, 1980.
[34] K. Kreutz-Delgado, The complex gradient operator and the CR-Calculus, 0906.4835, June 2009.
[35] W.A. Sethares, The least mean square family, in: N. Kalouptsidis, S. Theodoridis (Eds.), Adaptive System
Identiﬁcation and Signal Processing Algorithms, Prentice Hall, 1993.
[36] V. Solo, Averaging analysis of the LMS algorithm, in: C.T. Leondes (Ed.), Control and Dynamic Systems,
Stochastic Techniques in Digital Signal Processing Systems, Part 2 of 2, vol. 65, Academic Press, 1994, pp.
379–397.
[37] B.D.O. Anderson, R.R. Bitmead, C.R.J. Johnson Jr., P.V. Kokotovic, R.L. Kosut, I.M.Y. Mareels, L. Praly,
B.D. Riedle, Stability of adaptive systems: passivity and averaging analysis, MIT Press, Cambridge, MA,
1986
[38] A.H. Sayed, T. Kailath, A state-space approach to adaptive RLS ﬁltering, IEEE Signal Process. Mag. 11 (3)
(1994) 18–60.
[39] H.J. Kushner, G.G. Yin, Stochastic Approximation Algorithms and Applications, Applications of Mathe-
matics, Springer, 1997.

References
749
[40] P.A. Regalia, Numerical stability properties of a QR-based fast least squares algorithm, IEEE Trans. Signal
Process. 41 (6) (1993) 2096–2109.
[41] D.T.M. Slock, The backward consistency concept and round-off error propagation dynamics in recursive
least-squares ﬁltering algorithms, Opt. Eng. 31 (1992) 1153–1169.
[42] G.E. Bottomley, S.T. Alexander, A novel approach for stabilizing recursive least squares ﬁlters, IEEE Trans.
Signal Process. 39 (8) (1991) 1770–1779.
[43] J.A. Apolinário Jr. (Ed.), QRD-RLS Adaptive Filtering, Springer, NY, 2009.
[44] Y. Zakharov, G. White, J. Liu, Low-complexity RLS algorithms using dichotomous coordinate descent
iterations, IEEE Trans. Signal Process. 56 (7) (2008) 3150–3161.
[45] V.H. Nascimento, J.C.M. Bermudez, Probability of divergence for the least-mean fourth (LMF) algorithm,
IEEE Trans. Signal Process. 54 (4) (2006) 1376–1385.
[46] V.H. Nascimento, A.H. Sayed, On the learning mechanism of adaptive ﬁlters, IEEE Trans. Signal Process.
48 (6) (2000) 1609–1625.
[47] S. Chen, Low complexity concurrent constant modulus algorithm and soft directed scheme for blind equal-
ization, IEE Proc. – Vis. Image Signal Process. 150 (2003) 312–320.
[48] H.J. Butterweck, Iterative analysis of the steady-state weight ﬂuctuations in LMS-type adaptive ﬁlters,
Technical Report EUT 96-E-299, Eindhoven University of Technology, Netherlands, June 1996.
[49] H.J. Butterweck, A wave theory of long adaptive ﬁlters, IEEE Trans. Circ. Syst. I 48 (6) (2001) 739–747.
[50] E. Eweda, O. Macchi, Convergence of an adaptive linear estimation algorithm, IEEE Trans. Autom. Control
AC-19 (2) (1984) 119–127.
[51] O. Macchi, E. Eweda, Convergence analysis of self-adaptive equalizers, IEEE Trans. Inform. Theory IT-30
(2) (1984) 161–176.
[52] J.E. Mazo, On the independence theory of equalizer convergence, Bell Syst. Tech. J. 58 (1979) 963–993.
[53] V. Solo, X. Kong, Adaptive Signal Processing Algorithms, Prentice Hall, Englewood Cliffs, NJ, 1995.
[54] S.C. Douglas, W. Pan, Exact expectation analysis of the LMS adaptive ﬁlter, IEEE Trans. Signal Process. 43
(12) (1995) 2863–2871.
[55] A. Feuer, E. Weinstein, Convergence analysis of LMS ﬁlters with uncorrelated Gaussian data, IEEE Trans.
Acoust. Speech Signal Process. ASSP-33 (1) (1985) 222–229.
[56] S.
Florian,
A.
Feuer,
Performance
analysis
of
the
LMS
algorithm
with
a
tapped
delay
line (two-dimensional case), IEEE Trans. Acoust. Speech Signal Process. ASSP-34 (6) (1986)
1542–1549.
[57] A.H. Sayed, V.H. Nascimento, Energy conservation and the learning ability of LMS adaptive ﬁlters, in: S.
Haykin, B. Widrow (Eds.), Least-Mean-Square Adaptive Filters, Wiley, 2003.
[58] O. Dabeer, E. Masry, Convergence analysis of the constant modulus algorithm, IEEE Trans. Inform. Theory
49 (6) (2003) 1447–1464.
[59] P.I. Hübscher, J.C.M. Bermudez, V.H. Nascimento, A mean-square stability analysis of the least mean fourth
(LMF) adaptive algorithm, IEEE Trans. Signal Process. 55 (8) (2007) 4018–4028.
[60] V.H. Nascimento, M.T.M. Silva, Stochastic stability analysis for the constant-modulus algorithm, IEEE
Trans. Signal Process. 56 (10) (2008) 4984–4989.
[61] T.Y. Al-Naffouri, M. Moinuddin, Exact performance analysis of the ϵ-NLMS algorithm for colored circular
Gaussian inputs, IEEE Trans. Signal Process. 58 (10) (2010) 5080–5090.
[62] T.Y. Al-Naffouri, M. Moinuddin, M.S. Sohail, Mean weight behavior of the NLMS algorithm for correlated
Gaussian inputs, IEEE Signal Process. Lett. 18 (1) (2011) 7–10.
[63] N.J. Bershad, Analysis of the normalized LMS algorithm with Gaussian inputs, IEEE Trans. Acoust. Speech
Signal Process. ASSP-34 (1986) 793–806.
[64] N.J. Bershad, Behavior of the ϵ-normalized LMS algorithm with Gaussian inputs, IEEE Trans. Acoust.
Speech Signal Process. ASSP-35, May 1987, pp. 636–644.

750
CHAPTER 12 Adaptive Filters
[65] M.H. Costa, J.C.M. Bermudez, An improved model for the normalized LMS algorithm with Gaussian inputs
and large number of coefﬁcients, in: Proc. ICASSP 2002, vol. 2, pp. 1385–1388.
[66] V.H. Nascimento, A simple model for the effect of normalization on the convergence rate of adaptive ﬁlters,
in: Proc. of the 2004 IEEE International Conference on Acoustics, Speech and, Signal Processing, 2004, pp.
II-453–II-456.
[67] D.T.M. Slock, On the convergence behavior of the LMS and the normalized LMS algorithms, IEEE Trans.
Signal Process. 41 (9) (1993) 2811–2825.
[68] M. Tarrab, A. Feuer, Convergence and performance analysis of the normalized LMS algorithm with uncor-
related Gaussian data, IEEE Trans. Inform. Theory 34 (4) (1988) 680–691.
[69] A. Carini, E. Mumolo, G.L. Sicuranza, V-vector algebra and its application to Volterra-adaptive ﬁltering,
IEEE Trans. Circ. Syst. II 46 (5) (1999) 585–598.
[70] F. Kuech, W. Kellermann, Orthogonalized power ﬁlters for nonlinear acoustic echo cancellation, Signal
Process. 86 (6) (2006) 1168–1181.
[71] V.J. Mathews, G.L. Sicuranza, Polynomial Signal Processing, Wiley-Interscience, New York, 2000.
[72] A. Stenger, W. Kellermann, Adaptation of a memoryless preprocessor for nonlinear acoustic echo cancelling,
Signal Process. 80 (9) (2000) 1747–1760.
[73] G.L. Sicuranza, A. Carini, A. Fermo, Nonlinear adaptive ﬁlters for acoustic echo cancellation in mobile
terminals, in: K.E. Barner, G.R. Arce (Eds.), Nonlinear Signal and Image Processing: Theory, Methods,
and Applications, The Electrical Engineering and Applied Signal Processing Series, CRC Press, 2003, pp.
223–255 (Chapter 7).
[74] E. Eleftheriou, D. Falconer, Tracking properties and steady-state performance of RLS adaptive ﬁlter algo-
rithms, IEEE Trans. Acoust. Speech Signal Process. 34 (5) (1986) 1097–1110.
[75] G.V. Moustakides, Study of the transient phase of the forgetting factor RLS, IEEE Trans. Signal Process. 45
(10) (1997) 2468–2476.
[76] A.H. Sayed, M. Rupp, Error-energy bounds for adaptive gradient algorithms, IEEE Trans. Signal Process.
44 (8) (1996) 1982–1989.
[77] N. Yousef, A.H. Sayed, A uniﬁed approach to the steady-state and tracking analysis of adaptive ﬁlters, IEEE
Trans. Signal Process. 49 (2) (2001) 314–324.
[78] M. Rupp, A.H. Sayed, A time-domain feedback analysis of ﬁltered-error adaptive gradient algorithms, IEEE
Trans. Signal Process. 44 (1996) 1428–1439.
[79] C. Samsom, V.U. Reddy, Fixed point error analysis of the normalized ladder algorithm, IEEE Trans. Acoust.
Speech, Signal Process. 31 (5) (1983) 1177–1191.
[80] J.Y. Mai, A.H. Sayed, A feedback approach to the steady-state performance of fractionally spaced blind
adaptive equalizers, IEEE Trans. Signal Process. 48 (1) (2000) 80–91.
[81] E. Eweda, Comparison of RLS, LMS, and sign algorithms for tracking randomly time-varying channels,
IEEE Trans. Signal Process. 42 (11) (1994) 2937–2944.
[82] J.H. Wilkinson, Rounding Errors in Algebraic Processes, Her Majesty’s Stationery Ofﬁce, 1963.
[83] J.H. Wilkinson, The Algebraic Eigenvalue Problem, Oxford University Press, London, UK, 1965.
[84] S.T. Alexander, Transient weight misadjustment properties for the ﬁnite precision LMS algorithm, IEEE
Trans. Acoust. Speech Signal Process. ASSP-35 (9) (1987) 1250–1258.
[85] S. Ardalan, Floating-point error analysis of recursive least-squares and least-mean-squares adaptive ﬁlters,
IEEE Trans. Circ. Syst. 33 (12) (1986) 1192–1208.
[86] C. Caraiscos, B. Liu, A roundoff error analysis of the LMS adaptive algorithm, IEEE Trans. Acoust. Speech
Signal Process. ASSP-32 (1) (1984) 34–41.
[87] R.D. Gitlin, J.E. Mazo, M.G. Taylor, On the design of gradient algorithms for digitally implemented adaptive
ﬁlters, IEEE Trans. Circ. Theory, CT-20 (2) (1973) 125–136.

References
751
[88] J.C.M. Bermudez, N.J. Bershad, A nonlinear analytical model for the quantized LMS algorithm the arbitrary
step size case, IEEE Trans. Signal Process. 44 (5) (1996) 1175–1183.
[89] J.C.M.Bermudez,N.J.Bershad,Transient and tracking performance analysis of the quantized LMS algorithm
for time-varying system identiﬁcation, IEEE Trans. Signal Process. 44 (8) (1996) 1990–1997.
[90] N.J. Bershad, J.C. Bermudez, New insights on the transient and steady-state behavior of the quantized LMS
algorithm, IEEE Trans. Signal Process. 44 (10) (1996) 2623–2625.
[91] N.J. Bershad, J.C.M. Bermudez, A nonlinear analytical model for the quantized LMS algorithm-the power-
of-two step size case, IEEE Trans. Signal Process. 44 (11) (1996) 2895–2900.
[92] J.M. Ciofﬁ, J.J. Werner, The tap-drifting problem in digitally implemented data-driven echo cancellers, Bell
Syst. Tech. J. 64 (1) (1985) 115–138.
[93] R.D.Gitlin,H.C.MeadorsJr.,S.B.Weinstein,Thetap-leakagealgorithm:analgorithmforthestableoperation
of a digitally implemented, fractionally spaced adaptive equalizer, Bell Syst. Tech. J. 61 (8) (1982) 1817–
1839.
[94] V.H. Nascimento, A.H. Sayed, Unbiased and stable leakage-based adaptive ﬁlters, IEEE Trans. Signal Pro-
cess. 47 (12) (1999) 3261–3276.
[95] E. Eweda, Almost sure convergence of a decreasing gain sign algorithm for adaptive ﬁltering, IEEE Trans.
Acoust. Speech Signal Process. 36 (10) (1988) 1669–1671.
[96] E. Eweda, A tight upper bound of the average absolute error in a constant step-size sign algorithm, IEEE
Trans. Acoust. Speech Signal Process. 37 (11) (1989) 1774–1776.
[97] E. Eweda, Convergence analysis of the sign algorithm without the independence and Gaussian assumptions,
IEEE Trans. Signal Process. 48 (9) (2000) 2535–2544.
[98] A. Gersho, Adaptive ﬁltering with binary reinforcement, IEEE Trans. Inform. Theory 30 (2) (1984) 191–199.
[99] P. Xue, B. Liu, Adaptive equalizer using ﬁnite-bit power-of-two quantizer, IEEE Trans. Acoust. Speech
Signal Process. 34 (6) (1986) 1603–1611.
[100] S. Ljung, L. Ljung, Error propagation properties of recursive least-squares adaptation algorithms, Automatica
21 (2) (1985) 157–167.
[101] J. Ciofﬁ, The fast adaptive ROTOR’s RLS algorithm, IEEE Trans. Acoust. Speech Signal Process. 38 (4)
(1990) 631–653.
[102] S. Alexander, A. Ghimikar, A method for recursive least squares ﬁltering based upon an inverse QR decom-
position, IEEE Trans. Signal Process. 41 (1) (1993) 20.
[103] S.T. Alexander, A.L. Ghimikar, A method for recursive least squares ﬁltering based upon an inverse QR
decomposition, IEEE Trans. Signal Process. 41 (1) (1993).
[104] J.A. Apolinário Jr., P. Diniz, A new fast QR algorithm based on a priori errors, IEEE Signal Process. Lett. 4
(11) (1997) 307–309.
[105] M.D. Miranda, L. Aguayo, M. Gerken. Performance of the a priori and a posteriori QR-LSL algorithms in
a limited precision environment, in: Proc. IEEE Int Acoustics, Speech, and Signal Processing ICASSP-97
Conf., vol. 3, 1997, pp. 2337–2340.
[106] M.D. Miranda, M. Gerken, A hybrid QR-lattice least squares algorithm using a priori errors, in: Proc. 38th
Midwest Symp. Circuits and Systems, Proceedings, vol. 2, 1995, pp. 983–986.
[107] M.D. Miranda, M. Gerken, A hybrid least squares QR-lattice algorithm using a priori errors, IEEE Trans.
Signal Process. 45 (12) (1997) 2900–2911.
[108] I.K. Proudler, J.G. McWhirter, T.J. Shepherd, Computationally efﬁcient QR decomposition approach to least
squares adaptive ﬁltering, IEE Proc. F Radar Signal Process. 138 (4) (1991) 341–353.
[109] P.A. Regalia, M.G. Bellanger, On the duality between fast QR methods and lattice methods in least-squares
adaptive ﬁltering, IEEE Trans. Signal Process. 39 (1991) 879–891.
[110] A. Rontogiannis, S. Theodoridis, On inverse factorization adaptive LS algorithms, Signal Process. 52 (1997)
35–47.

752
CHAPTER 12 Adaptive Filters
[111] A.A. Rontogiannis, S. Theodoridis, New fast inverse QR least squares adaptive algorithms, in: Proc. Int.
Acoustics, Speech, and Signal Processing, vol. 2, 1995, pp. 1412–1415.
[112] A.A. Rontogiannis, S. Theodoridis, New multichannel fast QRD-LS adaptive algorithms, in: Proc. 13th Int.
Conf. Digital Signal Processing, vol. 1, 1997, pp. 45–48.
[113] A.A. Rontogiannis, S. Theodoridis, Multichannel fast QRD-LS adaptive ﬁltering: new technique and algo-
rithms, IEEE Trans. Signal Process. 46 (11) (1998) 2862–2876.
[114] A.A. Rontogiannis, S. Theodoridis, New fast QR decomposition least squares adaptive algorithms, IEEE
Trans. Signal Process. 46 (8) (1998) 2113–2121.
[115] M. Shoaib, S. Werner, J.A. Apolinário, Multichannel fast QR-decomposition algorithms: weight extraction
method and its applications, IEEE Trans. Signal Process. 58 (1) (2010) 175–188.
[116] M. Shoaib, S. Werner, J.A. Apolinário, T.I. Laakso, Solution to the weight extraction problem in fast QR-
decomposition rls algorithms, in: Proc. IEEE Int. Acoustics, Speech and Signal Processing Conf., ICASSP
2006, vol. 3, 2006.
[117] B. Friedlander, Lattice ﬁlters for adaptive processing, IEEE Trans. Signal Process. 70 (8) (1982) 829–867.
[118] D. Lee, M. Morf, B. Friedlander, Recursive least squares ladder estimation algorithms, IEEE Trans. Circ.
Syst. 28 (6) (1981) 467–481.
[119] H. Lev-Ari, T. Kailath, J. Ciofﬁ, Least-squares adaptive lattice and transversal ﬁlters: a uniﬁed geometric
theory, IEEE Trans. Inform. Theory 30 (2) (1984) 222–236.
[120] F. Ling, D. Manolakis, J. Proakis, Numerically robust least-squares lattice-ladder algorithms with direct
updating of the reﬂection coefﬁcients, IEEE Trans. Acoust. Speech Signal Process. 34 (4) (1986) 837–845.
[121] V.J. Mathews, Z. Xie, Fixed-point error analysis of stochastic gradient adaptive lattice ﬁlters, IEEE Trans.
Acoust. Speech Signal Process. 38 (1) (1990) 70–80.
[122] R.C. North, J.R. Zeidler, W.H. Ku, T.R. Albert, A ﬂoating-point arithmetic error analysis of direct and
indirect coefﬁcient updating techniques for adaptive lattice ﬁlters, IEEE Trans. Signal Process. 41 (5) (1993)
1809–1823.
[123] M.J. Reed, B. Liu, Analysis of simpliﬁed gradient adaptive lattice algorithms using power-of-two quantiza-
tion, in: Proc. IEEE Int. Circuits and Systems Symp., 1990, pp. 792–795.
[124] C. Samson, V. Reddy, Fixed point error analysis of the normalized ladder algorithm, IEEE Trans. Acoust.
Speech Signal Process. 31 (5) (1983) 1177–1191.
[125] R. Merched, Extended RLS lattice adaptive ﬁlters, IEEE Trans. Signal Process. 51 (9) (2003) 2294–2309.
[126] R. Merched, A.H. Sayed, Order-recursive RLS laguerre adaptive ﬁltering, IEEE Trans. Signal Process. 48
(11) (2000) 3000–3010.
[127] R. Merched, A.H. Sayed, RLS-Laguerre lattice adaptive ﬁltering: error-feedback, normalized, and array-
based algorithms, IEEE Trans. Signal Process. 49 (11) (2001) 2565–2576.
[128] J.-L. Botto, G.V. Moustakides, Stabilizing the fast Kalman algorithms, IEEE Trans. Acoust. Speech Signal
Process. 37 (9) (1989) 1342–1348.
[129] J. Ciofﬁ, T. Kailath, Fast, recursive-least-squares transversal ﬁlters for adaptive ﬁltering, IEEE Trans. Acoust.
Speech Signal Process. 32 (2) (1984) 304–337.
[130] J. Ciofﬁ, T. Kailath, Windowed fast transversal ﬁlters adaptive algorithms with normalization, IEEE Trans.
Acoust. Speech Signal Process. 33 (3) (1985) 607–625.
[131] D. Falconer, L. Ljung, Application of fast Kalman estimation to adaptive equalization, IEEE Trans. Commun.
26 (10) (1978) 1439–1446.
[132] G.V. Moustakides, S. Theodorides, Fast Newton transversal ﬁlters a new class of adaptive estimation algo-
rithms, IEEE Trans. Signal Process. 39 (10) (1991) 2184–2193.
[133] D.T.M. Slock, T. Kailath, Fast transversal ﬁlters with data sequence weighting, IEEE Trans. Acoust., Speech
Signal Process. 37 (3) (1989) 346–359.

References
753
[134] D.T.M. Slock, T. Kailath, Numerically stable fast transversal ﬁlters for recursive least squares adaptive
ﬁltering, IEEE Trans. Signal Process. 39 (1) (1991) 92–114.
[135] Y. Zakharov, T. Tozer, Multiplication-free iterative algorithm for LS problem, Electron. Lett. 40 (9) (2004)
567–569.
[136] W.A. Sethares, D.A. Lawrence, C.R. Johnson Jr., R.R. Bitmead, Parameter drift in LMS adaptive ﬁlters,
IEEE Trans. Acoust. Speech Signal Process. ASSP-34 (1986) 868–878.
[137] K. Mayyas, T. Aboulnasr, The leaky LMS algorithm: MSE analysis for Gaussian data, IEEE Trans. Signal
Process. 45 (4) (1997) 927–934.
[138] C.S. Ludovico, J.C.M. Bermudez, A recursive least squares algorithm robust to low-power excitation,
in: IEEE International Conference on Acoustics, Speech, and Signal Processing, 2004, Proceedings
(ICASSP’04), 2004.
[139] J. Liu, Y. Zakharov, Low complexity dynamically regularised RLS algorithm, Electron. Lett. 44 (14) (2008)
885–886.
[140] M.C. Tsakiris, C.G. Lopes, V.H. Nascimento, An array recursive least-squares algorithm with generic non-
fading regularization matrix, IEEE Signal Process. Lett. 17 (12) (2010) 1001–1004.
[141] N. Jablon, Adaptive beamforming with the generalized sidelobe canceller in the presence of array imperfec-
tions, IEEE Trans. Antennas Propag. 34 (8) (1986) 996–1012.
[142] J.C.M. Bermudez, M.H. Costa, Optimum leakage factor for the MOV-LMS algorithm in nonlinear modeling
and control systems, in: Proc. IEEE Int Acoustics, Speech, and Signal Processing (ICASSP) Conf., vol. 2,
2002.
[143] E.J. Candès, M.B. Wakin, People hearing without listening: an introduction to compressive sampling, IEEE
Signal Process. Mag. 25 (2) (2008) 21–30.
[144] D. Angelosante, J.A. Bazerque, G.B. Giannakis, Online adaptive estimation of sparse signals: where RLS
meets the ℓ1-norm, IEEE Trans. Signal Process. 58 (7) (2010) 3436–3447.
[145] D. Angelosante, G.B. Giannakis, RLS-weighted Lasso for adaptive estimation of sparse signals, in: Proc.
IEEE Int. Conf. Acoustics, Speech and Signal Processing, ICASSP 2009, 2009, pp. 3245–3248.
[146] Y. Chen, Y. Gu, A.O. Hero, Sparse LMS for system identiﬁcation, in: Proc. ICASSP 2009, April 2009, pp.
3125–3128.
[147] Y. Kopsinis, K. Slavakis, S. Theodoridis, Online sparse system identiﬁcation and signal reconstruction using
projections onto weighted ℓ1 balls, IEEE Trans. Signal Process. 59 (3) (2011) 936–952.
[148] G. Mileounis, B. Babadi, N. Kalouptsidis, V. Tarokh, An adaptive greedy algorithm with application to
nonlinear communications, IEEE Trans. Signal Process. 58 (6) (2010) 2998–3007.
[149] R. Harris, D. Chabries, F. Bishop, A variable step (VS) adaptive ﬁlter algorithm, IEEE Trans. Acoust. Speech
Signal Process. 34 (2) (1986) 309–316.
[150] B. Farhang-Boroujeny, Adaptive Filters – Theory and Applications, John Wiley & Sons, West Sussex, 1998.
[151] J. Nagumo, A. Noda, A learning method for system identiﬁcation, IEEE Trans. Autom. Control 12 (3) (1967)
282–287.
[152] A.E. Albert, L.S. Gardner Jr., Stochastic Approximation and Nonlinear Regression, MIT Press, 1967.
[153] T. Aboulnasr, K. Mayyas, A robust variable step-size LMS-type algorithm, IEEE Trans. Signal Process. 45
(3) (1997) 631–639.
[154] W.-P. Ang, B. Farhang-Boroujeny, A new class of gradient adaptive step-size LMS algorithms, IEEE Trans.
Signal Process. 49 (4) (2001) 805–810.
[155] J. Benesty, H. Rey, L. Vega, S. Tressens, A nonparametric VSS NLMS algorithm, IEEE Signal Process. Lett.
13 (10) (2006) 581–584.
[156] N. Bershad, On the optimum gain parameter in LMS adaptation, IEEE Trans. Acoust. Speech Signal Process.
35 (7) (1987) 1065–1068.

754
CHAPTER 12 Adaptive Filters
[157] R.C. Bilcu, P. Kuosmanen, K. Egiazarian, A transform domain LMS adaptive ﬁlter with variable step-size,
IEEE Signal Process. Lett. 9 (2) (2002) 51–53.
[158] J.B. Evans, P. Xue, B. Liu, Analysis and implementation of variable step size adaptive algorithms, IEEE
Trans. Signal Process. 41 (8) (1993) 2517–2535.
[159] B. Farhang-Boroujeny, Variable-step-size LMS algorithm: new developments and experiments, IEE Proc.
Vis. Image Signal Process. 141 (5) (1994) 311–317.
[160] A. Gupta, S. Joshi, Variable step-size LMS algorithm for fractal signals, IEEE Trans. Signal Process. 56 (4)
(2008) 1411–1420.
[161] R.H. Kwong, E.W. Johnston, A variable step size LMS algorithm, IEEE Trans. Signal Process. 40 (7) (1992)
1633–1642.
[162] V.J. Mathews, Z. Xie, A stochastic gradient adaptive ﬁlter with gradient adaptive step size, IEEE Trans.
Signal Process. 41 (6) (1993) 2075–2087.
[163] C. Paleologu, J. Benesty, S. Ciochina, A robust variable forgetting factor recursive least-squares algorithm
for system identiﬁcation, IEEE Signal Process. Lett. 15 (2008) 597–600.
[164] C. Paleologu, S. Ciochina, J. Benesty, Variable step-size NLMS algorithm for under-modeling acoustic echo
cancellation, IEEE Signal Process. Lett. 15 (2008) 5–8.
[165] D.I. Pazaitis, A.G. Constantinides, A novel kurtosis driven variable step-size adaptive algorithm, IEEE Trans.
Signal Process. 47 (3) (1999) 864–872.
[166] H.-C. Shin, A.H. Sayed, W.-J. Song, Variable step-size NLMS and afﬁne projection algorithms, IEEE Signal
Process. Lett. 11 (2) (2004) 132–135.
[167] L. Vega, H. Rey, J. Benesty, S. Tressens, A new robust variable step-size NLMS algorithm, IEEE Trans.
Signal Process. 56 (5) (2008) 1878–1893.
[168] J. Benesty, S.L. Gay, An improved PNLMS algorithm, in: International Conference on Acoustics, Speech
and Signal Processing, vol. 2, IEEE, 2002, pp. 1881–1884.
[169] F. das Chagas de Souza, O.J. Tobias, R. Seara, D.R. Morgan, A PNLMS algorithm with individual activation
factors, IEEE Trans. Signal Process. 58 (4) (2010) 2036–2047.
[170] F. das Chagas de Souza, O.J. Tobias, R. Seara, D.R. Morgan, Stochastic model for the mean weight evolution
of the IAF-PNLMS algorithm, IEEE Trans. Signal Process. 58 (11) (2010) 5895–5901.
[171] D.L. Duttweiler, Proportionate normalized least-mean-squares adaptation in echo cancellers, IEEE Trans.
Speech Audio Process. 8 (5) (2000) 508–518.
[172] T. Gansler, S.L. Gay, M.M. Sondhi, J. Benesty, Double-talk robust fast converging algorithms for network
echo cancellation, IEEE Trans. Speech Audio Process. 8 (6) (2000) 656–663.
[173] E. Walach, B. Widrow, The least mean fourth (LMF) adaptive algorithm and its family, IEEE Trans. Inform.
Theory IT-30 (2) (1984) 275–283.
[174] R. Sharma, W.A. Sethares, J.A. Bucklew, Asymptotic analysis of stochastic gradient-based adaptive ﬁltering
algorithms with general cost functions, IEEE Trans. Signal Process. 44 (9) (1996) 2186–2194.
[175] T.Y. Al-Naffouri, A.H. Sayed, Transient analysis of adaptive ﬁlters with error nonlinearities, IEEE Trans.
Signal Process. 51 (3) (2003) 653–663.
[176] J.A. Chambers, O. Tanrikulu, A.G. Constantinides, Least mean mixed-norm adaptive ﬁltering, Electron. Lett.
30 (19) (1994).
[177] O. Tanrikulu, J.A. Chambers, Convergence and steady-state properties of the least-mean mixed-norm
(LMMN) adaptive algorithm, IEEE Trans. Signal Process. 143 (3) (1996) 137–142.
[178] A.H. Sayed, Fundamentals of Adaptive Filtering, Wiley-Interscience, 2003.
[179] B. Hassibi, A.H. Sayed, T. Kailath, LMS is H∞-optimal, in: Proc. Conference on Decision and Control,
San Antonio, TX, vol. 1, December 1993, pp. 74–79.
[180] G. Aydin, O. Arikan, A.E. Cetin, Robust adaptive ﬁltering algorithms for α-stable random processes, IEEE
Trans. Circ. Syst. II 46 (2) (1999) 198–202.

References
755
[181] S.R. Kim, A. Efron, Adaptive robust impulse noise ﬁltering, IEEE Trans. Signal Process. 43 (8) (1995)
1855–1866.
[182] E. Masry, Alpha-stable signals and adaptive ﬁltering, IEEE Trans. Signal Process. 48 (11) (2000) 3011–3016.
[183] M. Shao, C.L. Nikias, Signal processing with fractional lower order moments: stable processes and their
applications, Proc. IEEE 81 (7) (1993) 986–1010.
[184] D. Hirsch, W. Wolf, A simple adaptive equalizer for efﬁcient data transmission, IEEE Trans. Commun.
Technol. 18 (1) (1970) 5–12.
[185] A. Flaig, G.R. Arce, K.E. Barner, Afﬁne order-statistic ﬁlters: medianization of linear FIR ﬁlters, IEEE
Trans. Signal Process. 46 (8) (1998) 2101–2112.
[186] D.N. Godard, Self-recovering equalization and carrier tracking in two-dimensional data communication
systems, IEEE Trans. Commun. 28 (11) (1980) 1867–1875.
[187] J.R. Treichler, B. Agee, A new approach to multipath correction of constant modulus signals, IEEE Trans.
Acoust. Speech Signal Process. ASSP-28 (1983) 334–358.
[188] C.B. Papadias, D.T.M. Slock, Normalized sliding window constant modulus and decision-direct algorithms:
a link between blind equalization and classical adaptive ﬁltering, IEEE Trans. Signal Process. 45 (1997)
231–235.
[189] Z. Ding, Y. Li, Blind Equalization and Identiﬁcation, Marcel Dekke, 2001.
[190] M.T.M. Silva, V.H. Nascimento, Improving the tracking capability of adaptive ﬁlters via convex combination,
IEEE Trans. Signal Process. 56 (7) (2008) 3137–3149.
[191] C.R. Johnson Jr., P. Schniter, T.J. Endres, J.D. Behm, D.R. Brown, R.A. Casas, Blind equalization using the
constant modulus criterion: a review, Proc. IEEE 86 (10) (1998) 1927–1950.
[192] R. Candido, M.T.M. Silva, M.D. Miranda, V.H. Nascimento, A statistical analysis of the dual-mode CMA,
in: Proc. IEEE Int. Circuits and Systems (ISCAS) Symp., 2010, pp. 2510–2513.
[193] R. Candido, M.T.M. Silva, V.H. Nascimento, Transient and steady-state analysis of the afﬁne combination
of two adaptive ﬁlters, IEEE Trans. Signal Process. 58 (8) (2010) 4064–4078.
[194] I. Fijalkow, C.E. Manlove, C.R. Johnson Jr., Adaptive fractionally spaced blind CMA equalization: excess
MSE, IEEE Trans. Signal Process. 46 (1) (1998) 227–231.
[195] M.T.M. Silva, M.D. Miranda, Tracking issues of some blind equalization algorithms, IEEE Signal Process.
Lett. 11 (9) (2004) 760–763.
[196] M.T.M. Silva, V.H. Nascimento, Tracking analysis of the constant modulus algorithm, in: Proc., ICASSP
2008, Las Vegas, NV, USA, pp. 3561–3564.
[197] M.D. Miranda, M.T.M. Silva, V.H. Nascimento, Avoiding divergence in the constant modulus algorithm, in:
Proc., ICASSP 2008, Las Vegas, NV, USA, 2008, pp. 3565–3568.
[198] M.D. Miranda, M.T.M. Silva, V.H. Nascimento, Avoiding divergence in the Shalvi-Weinstein algorithm,
IEEE Trans. Signal Process. 56 (11) (2008) 5403–5413.
[199] P.A. Regalia, M. Mboup, Undermodeled equalization: a characterization of stationary points for a family of
blind criteria, IEEE Trans. Signal Process. 47 (1999) 760–770.
[200] R. Suyama, R.R.F. Attux, J.M.T. Romano, M. Bellanger, On the relationship between least squares and
constant modulus criteria for adaptive ﬁltering, in: The 37th Asilomar Conference on Signals, Systems and
Computers, vol. 2, 2003, pp. 1293–1297.
[201] J. Yang, J.-J. Werner, G.A. Dumont, The multimodulus blind equalization and its generalized algorithms,
IEEE J. Sel. Areas Commun. 20 (2002) 997–1015.
[202] J.-T. Yuan, T.-C. Lin, Equalization and carrier phase recovery of CMA and MMA in blind adaptive receivers,
IEEE Trans. Signal Process. 58 (6) (2010) 3206–3217.
[203] J.-T. Yuan, K.-D. Tsai, Analysis of the multimodulus blind equalization algorithm in QAM communication
systems, IEEE Trans. Commun. 53 (2005) 1427–1431.

756
CHAPTER 12 Adaptive Filters
[204] S. Abrar, A. Nandi, Blind equalization of square-QAM signals: a multimodulus approach, IEEE Trans.
Commun. 58 (6) (2010) 1674–1685.
[205] K. Banovic, E. Abdel-Raheem, M.A.S. Khalid, A novel radius-adjusted approach for blind adaptive equal-
ization, IEEE Signal Process. Lett. 13 (2006) 37–40.
[206] J. Mendes Filho, M.T.M. Silva, M.D. Miranda, A family of algorithms for blind equalization of QAM
signals, in: Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Process, Prague, Czech Republic, 2011, pp.
3388–3391.
[207] J. Mendes Filho, M.T.M. Silva, M.D. Miranda, V.H. Nascimento, A region-based algorithm for blind equal-
ization of QAM signals, in: Proc. of the IEEE/SP 15th Workshop on Statistical Signal Processing, Cardiff,
UK, 2009, pp. 685–688.
[208] F. Beaufays, Transform-domain adaptive ﬁlters: an analytical approach, IEEE Trans. Signal Process. 43 (2)
(1995) 422–431.
[209] G. Clark, S. Mitra, S. Parker, Block implementation of adaptive digital ﬁlters, IEEE Trans. Acoust. Speech
Signal Process. 29 (3) (1981) 744–752.
[210] B. Farhang-Boroujeny, Analysis and efﬁcient implementation of partitioned block LMS adaptive ﬁlters,
IEEE Trans. Signal Process. 44 (11) (1996) 2865–2868.
[211] E. Ferrara, Fast implementations of LMS adaptive ﬁlters, IEEE Trans. Acoust. Speech Signal Process. 28
(4) (1980) 474–475.
[212] R. Merched, A.H. Sayed, An embedding approach to frequency-domain and subband adaptive ﬁltering, IEEE
Trans. Signal Process. 48 (9) (2000) 2607–2619.
[213] S. Narayan, A. Peterson, M. Narasimha, Transform domain LMS algorithm, IEEE Trans. Acoust. Speech
Signal Process. 31 (3) (1983) 609–615.
[214] S.S. Narayan, A.M. Peterson, Frequency domain least-mean-square algorithm, Proc. IEEE 69 (1) (1981)
124–126.
[215] J.J. Shynk, Frequency-domain and multirate adaptive ﬁltering, IEEE Signal Process. Mag. 9 (1) (1992)
14–37.
[216] J.-S. Soo, K.K. Pang, Multidelay block frequency domain adaptive ﬁlter, IEEE Trans. Acoust. Speech Signal
Process. 38 (2) (1990) 373–376.
[217] P.P. Vaidyanathan, Multirate Systems and Filter Banks, Prentice Hall, Englewood Cliffs, NJ, 1993.
[218] I. Furukawa, A design of canceller of broadband acoustic echo, in: Proc. Int. Teleconference Symposium,
Tokyo, Japan, 1984, pp. 1–8.
[219] A. Gilloire, M. Vetterli, Adaptive ﬁltering in subbands with critical sampling: analysis, experiments, and
application to acoustic echo cancellation, IEEE Trans. Signal Process. 40 (8) (1992) 1862–1875.
[220] W. Kellermann, Kompensation akustischer Echos in Frequenzteilbändern, Frequenz 39 (1985) 209–215.
[221] M.R. Petraglia, R.G. Alves, P.S.R. Diniz, New structures for adaptive ﬁltering in subbands with critical
sampling, IEEE Trans. Signal Process. 48 (12) (2000) 3316–3327.
[222] M.R. Petraglia, P.B. Batalheiro, Nonuniform subband adaptive ﬁltering with critical sampling, IEEE Trans.
Signal Process. 56 (2) (2008) 565–575.
[223] M.R. Petraglia, S.K. Mitra, Adaptive ﬁr ﬁlter structure based on the generalized subband decomposition of
ﬁr ﬁlters, IEEE Trans. Circ. Syst. II 40 (6) (1993) 354–362.
[224] S. Weiss, R.W. Stewart, R. Rabenstein, Steady-state performance limitations of subband adaptive ﬁlters,
IEEE Trans. Signal Process. 49 (9) (2001) 1982–1991.
[225] N. Hirayama, H. Sakai, S. Miyagi, Delayless subband adaptive ﬁltering using the Hadamard transform, IEEE
Trans. Signal Process. 47 (6) (1999) 1731–1734.
[226] R. Merched, M. Petraglia, P.S.R. Diniz, A delayless alias-free subband adaptive ﬁlter structure, IEEE Trans.
Signal Process. 47 (6) (1999) 1580–1591.

References
757
[227] D.R. Morgan, J.C. Thi, A delayless subband adaptive ﬁlter architecture, IEEE Trans. Signal Process. 43 (8)
(1995) 1819–1830.
[228] K. Nishikawa, H. Kiya, Conditions for convergence of a delayless subband adaptive ﬁlter and its efﬁcient
implementation, IEEE Trans. Signal Process. 46 (4) (1998) 1158–1167.
[229] J. Apolinário Jr., M.L.R. Campos, P.S.R. Diniz, Convergence analysis of the binormalized data-reusing LMS
algorithm, IEEE Trans. Signal Process. 48 (11) (2000) 3235–3242.
[230] S.G. Kratzer, D.R. Morgan, The partial-rank algorithm for adaptive beamforming, in: Proc. SPIE, vol. 564,
1985, pp. 9–14.
[231] D.R. Morgan, S.G. Kratzer, On a class of computationally efﬁcient, rapidly converging, generalized NLMS
algorithms, IEEE Signal Process. Lett. 3 (8) (1996) 245–247.
[232] K. Ozeki, T. Umeda, An adaptive ﬁltering algorithm using an orthogonal projection to an afﬁne subspace
and its properties, Electron. Commun. Jpn. 67-A (1984) 19–27.
[233] S. Roy, J.J. Shynk, Analysis of the data-reusing LMS algorithm, in: Proc. 32nd Midwest Symp. Circuits and
Systems, 1989, pp. 1127–1130.
[234] M. Rupp, A family of adaptive ﬁlter algorithms with decorrelating properties, IEEE Trans. Signal Process.
46 (3) (1998) 771–775.
[235] S.L. Gay, S. Tavathia, The fast afﬁne projection algorithm, in: Proc. Int. Acoustics, Speech, and Signal
Processing ICASSP-95 Conf., vol. 5, 1995, pp. 3023–3026.
[236] G. Rombouts, M. Moonen, A fast exact frequency domain implementation of the exponentially windowed
afﬁne projection algorithm, in: Proc. and Control Symp Adaptive Systems for Signal Processing, Commu-
nications 2000, AS-SPCC, The IEEE 2000, 2000, pp. 342–346.
[237] M. Tanaka, S. Makino, J. Kojima, A block exact fast afﬁne projection algorithm, IEEE Trans. Speech Audio
Process. 7 (1) (1999) 79–86.
[238] S.J.M. de Almeida, J.C.M. Bermudez, N.J. Bershad, A stochastic model for a pseudo afﬁne projection
algorithm, IEEE Trans. Signal Process. 57 (1) (2009) 107–118.
[239] S.J.M. de Almeida, J.C.M. Bermudez, N.J. Bershad, M.H. Costa, A statistical analysis of the afﬁne projection
algorithm for unity step size and autoregressive inputs, IEEE Trans. Circ. Syst. I 52 (7) (2005) 1394–1405.
[240] S.G. Sankaran, A.A.L. Beex, Convergence behavior of afﬁne projection algorithms, IEEE Trans. Signal
Process. 48 (4) (2000) 1086–1096.
[241] H.-C. Shin, A.H. Sayed, Mean-square performance of a family of afﬁne projection algorithms, IEEE Trans.
Signal Process. 52 (1) (2004) 90–102.
[242] C. Paleologu, J. Benesty, S. Ciochina, Regularization of the afﬁne projection algorithm, IEEE Trans. Circ.
Syst. II 58 (6) (2011) 366–370.
[243] H. Rey, L.R. Vega, S. Tressens, J. Benesty, Variable explicit regularization in afﬁne projection algorithm:
robustness issues and optimal choice, IEEE Trans. Signal Process. 55 (5) (2007) 2096–2109.
[244] J. Benesty, P. Duhamel, Y. Grenier, A multichannel afﬁne projection algorithm with applications to multi-
channel acoustic echo cancellation, IEEE Signal Process. Lett. 3 (2) (1996) 35–37.
[245] C. Paleologu, J. Benesty, S. Ciochina, A variable step-size afﬁne projection algorithm designed for acoustic
echo cancellation, IEEE Trans. Audio Speech Lang. Process. 16 (8) (2008) 1466–1478.
[246] D.Manolakis,F.Ling,J.Proakis,Efﬁcienttime-recursiveleast-squaresalgorithmsforﬁnite-memoryadaptive
ﬁltering, IEEE Trans. Circ. Syst. 34 (4) (1987) 400–408.
[247] K. Maouche, D.T.M. Slock, Performance analysis and FTF version of the generalized sliding window recur-
sive least-squares (GSWRLS) algorithm, in: Proc. Conf. Signals, Systems and Computers Record of the 29th
Asilomar Conf., vol. 1, 1995, pp. 685–689.
[248] J. Arenas-García, A.R. Figueiras-Vidal, A.H. Sayed, Mean-square performance of a convex combination of
two adaptive ﬁlters, IEEE Trans. Signal Process. 54 (3) (2006) 1078–1090.

758
CHAPTER 12 Adaptive Filters
[249] M. Martinez-Ramon, J. Arenas-García, A. Navia-Vazquez, A.R. Figueiras-Vidal, An adaptive combination
of adaptive ﬁlters for plant identiﬁcation, in: 14th International Conference on Digital Signal Processing
(DSP 2002), 2002, pp. 1195–1198.
[250] J. Arenas-García, M. Martinez-Ramón, A. Navia-Vázquez, A.R. Figueiras-Vidal, Plant identiﬁcation via
adaptive combination of transversal ﬁlters, Signal Process. 86 (2006) 2430–2438.
[251] L.A.Azpicueta-Ruiz,A.R.Figueiras-Vidal,J.Arenas-García,Anormalizedadaptationschemefortheconvex
combination of two adaptive ﬁlters, in: Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Process., Las
Vegas, NV, 2008, pp. 3301–3304.
[252] P. Andersson, Adaptive forgetting in recursive identiﬁcation through multiple models, Int. J. Control 42
(1985) 1175–1193.
[253] M. Nied´zwiecki, Identiﬁcation of nonstationary stochastic systems using parallel estimation schemes, IEEE
Trans. Autom. Control 35 (3) (1990) 329–334.
[254] M. Nied´zwiecki, Multiple-model approach to ﬁnite memory adaptive ﬁltering, IEEE Trans. Signal Process.
40 (2) (1992) 470–473.
[255] S.S. Kozat, A.C. Singer, Multi-stage adaptive signal processing algorithms, in: Proc. SAM Signal Process.
Workshop, 2000, pp. 380–384.
[256] V.H. Nascimento, M.T. Silva, R. Candido, J. Arenas-García, A transient analysis for the convex combination
of adaptive ﬁlters, in: Proc. IEEE Workshop on Statistical Signal Process., Cardiff, UK, 2009, pp. 53–56.
[257] M.T.M. Silva, V.H. Nascimento, J. Arenas-García, A transient analysis for the convex combination of two
adaptive ﬁlters with transfer of coefﬁcients, in: Proc., ICASSP 2010, Dallas, USA, pp. 3842–3845.
[258] Y. Zhang, J. Chambers, Convex combination of adaptive ﬁlters for a variable tap-length LMS algorithm,
IEEE Signal Process. Lett. 13 (10) (2006) 628–631.
[259] V.H. Nascimento, M.T.M. Silva, L.A. Azpicueta-Ruiz, J. Arenas-García, On the tracking performance of
combinations of least mean squares and recursive least squares adaptive ﬁlters, in: Proc. IEEE Int. Acoustics
Speech and Signal Processing (ICASSP) Conf., 2010, pp. 3710–3713.
[260] J. Arenas-García, A.R. Figueiras-Vidal, Adaptive combination of proportionate ﬁlters for sparse echo can-
cellation, IEEE Trans. Audio Speech Lang. Process. 17 (2009) 1087–1098.
[261] L.A. Azpicueta-Ruiz, M. Zeller, A.R. Figueiras-Vidal, J. Arenas-García, W. Kellermann, Adaptive combi-
nation of Volterra kernels and its application to nonlinear acoustic echo cancellation, IEEE Trans. Acoust.
Speech Signal Process. 19 (2011) 97–110.
[262] M. Lazaro-Gredilla, L.A. Azpicueta-Ruiz, A.R. Figueiras-Vidal, J. Arenas-García, Adaptively biasing the
weights of adaptive ﬁlters, IEEE Trans. Signal Process. 58 (7) (2010) 3890–3895.
[263] N.J. Bershad, J.C.M. Bermudez, J.-Y. Tourneret, An afﬁne combination of two LMS adaptive ﬁlters—
transient mean-square analysis, IEEE Trans. Signal Process. 56 (5) (2008) 1853–1864.
[264] J.C.M. Bermudez, N.J. Bershad, J.-Y. Tourneret, Stochastic analysis of an error power ratio scheme applied
to the afﬁne combination of two LMS adaptive ﬁlters, Signal Process. 91 (11) (2011) 2615–2622.
[265] S.S. Kozat, A.T. Erdogan, A.C. Singer, A.H. Sayed, Steady-state MSE performance analysis of mixture
approaches to adaptive ﬁltering, IEEE Trans. Signal Process. 58 (8) (2010) 4050–4063.
[266] C.G. Lopes, A.H. Sayed, Incremental adaptive strategies over distributed networks, IEEE Trans. Signal
Process. 55 (2007) 4064–4077.
[267] C.G. Lopes, A.H. Sayed, Diffusion least-mean squares over adaptive networks: formulation and performance
analysis, IEEE Trans. Signal Process. 56 (7) (2008) 3122–3136.
[268] F.S. Cattivelli, A.H. Sayed, Diffusion LMS strategies for distributed estimation, IEEE Trans. Signal Process.
58 (3) (2010) 1035–1048.
[269] F.S. Cattivelli, C.G. Lopes, A.H. Sayed, Diffusion recursive least-squares for distributed estimation over
adaptive networks, IEEE Trans. Signal Process. 56 (5) (2008) 1865–1877.

References
759
[270] F.S. Cattivelli, A.H. Sayed, Diffusion strategies for distributed Kalman ﬁltering and smoothing, IEEE Trans.
Autom. Control 55 (9) (2010) 2069–2084.
[271] F.S. Cattivelli, A.H. Sayed, Analysis of spatial and incremental LMS processing for distributed estimation,
IEEE Trans. Signal Process. 59 (4) (2011) 1465–1480.
[272] F.S. Cattivelli, A.H. Sayed, Distributed detection over adaptive networks using diffusion adaptation, IEEE
Trans. Signal Process. 59 (5) (2011) 1917–1932.
[273] R.L.G. Cavalcante, I. Yamada, B. Mulgrew, An adaptive projected subgradient approach to learning in
diffusion networks, IEEE Trans. Signal Process. 57 (7) (2009) 2762–2774.
[274] S. Chouvardas, K. Slavakis, S. Theodoridis, Adaptive robust distributed learning in diffusion sensor networks,
IEEE Trans. Signal Process. 59 (10) (2011) 4692–4707.
[275] L. Li, J. Chambers, A new incremental afﬁne projection-based adaptive algorithm for distributed networks,
Signal Process. 88 (10) (2008) 2599–2603.
[276] G. Mateos, I.D. Schizas, G.B. Giannakis, Closed-form MSE performance of the distributed LMS algorithm,
in:Proc.IEEE13thDigitalSignalProcessingWorkshopand5thIEEESignalProcessingEducationWorkshop
DSP/SPE 2009, 2009, pp. 66–71.
[277] G. Mateos, I.D. Schizas, G.B. Giannakis, Distributed recursive least-squares for consensus-based in-network
adaptive estimation, IEEE Trans. Signal Process. 57 (11) (2009) 4583–4588.
[278] V.H. Nascimento, A.H. Sayed, Continuous-time distributed estimation, in: Proc. of the 45th Asilomar Con-
ference on Signals, Systems, and Computers, Paciﬁc Grove, CA, November 2011, pp. 1–5
[279] I.D. Schizas, G. Mateos, G.B. Giannakis, Distributed LMS for consensus-based in-network adaptive pro-
cessing, IEEE Trans. Signal Process. 57 (6) (2009) 2365–2382.
[280] N. Takahashi, I. Yamada, A.H. Sayed, Diffusion least-mean squares with adaptive combiners: formulation
and performance analysis, IEEE Trans. Signal Process. 58 (9) (2010) 4795–4810.
[281] F.S. Cattivelli, A.H. Sayed, Modeling bird ﬂight formations using diffusion adaptation, IEEE Trans. Signal
Process. 59 (5) (2011) 2038–2051.
[282] J. Chen, A.H. Sayed, Bio-inspired cooperative optimization with application to bacteria motility, in: Proc.
IEEE Int. Acoustics, Speech and Signal Processing (ICASSP) Conf., 2011, pp. 5788–5791.
[283] J. Chen, X. Zhao, A.H. Sayed, Bacterial motility via diffusion adaptation, in: Proc. Conf. Signals, Systems
and Computers (ASILOMAR) Record of the 44th Asilomar Conf., 2010, pp. 1930–1934.
[284] S.-Y. Tu, A.H. Sayed, Foraging behavior of ﬁsh schools via diffusion adaptation, in: Proc. 2nd Int. Cognitive
Information Processing (CIP), Workshop, 2010, pp. 63–68.
[285] S.-Y. Tu, A.H. Sayed, Tracking behavior of mobile adaptive networks, in: Proc. Conf. Signals, Systems and
Computers (ASILOMAR) Record of the 44th Asilomar Conf., 2010, pp. 698–702.
[286] S.-Y. Tu, A.H. Sayed, Cooperative prey herding based on diffusion adaptation, in: Proc. IEEE Int. Acoustics,
Speech and Signal Processing (ICASSP) Conf., 2011, pp. 3752–3755.
[287] P.M.S. Burt, P. Regalia, A new framework for convergence analysis and algorithm development of adaptive
IIR ﬁlters, IEEE Trans. Signal Process. 53 (2005) 3129–3140.
[288] S.L. Netto, P.S.R. Diniz, P. Agathoklis, Adaptive IIR ﬁltering algorithms for system identiﬁcation: a general
framework, IEEE Trans. Educ. 38 (1) (1995) 54–66.
[289] P.A. Regalia, Adaptive IIR Filtering in Signal Processing and Control, Marcel Dekker, 1995.
[290] J.J. Shynk, Adaptive IIR ﬁltering, IEEE ASSP Mag. 6 (2) (1989) 4–21.
[291] P.M.S. Burt, M. Gerken, A polyphase IIR adaptive ﬁlter: error surface analysis and application, in: Proc.
IEEE Int. Conf. Acoustics, Speech, and, Signal Process., vol. 3, April 1997, pp. 2285–2288.
[292] P.M.S. Burt, M. Gerken, A polyphase IIR adaptive ﬁlter, IEEE Trans. Circ. Syst. II 49 (5) (2002) 356–359.
[293] J.E. Cousseau, P.S. Diniz, A general consistent equation-error algorithm for adaptive IIR ﬁltering, Signal
Process. 56 (2) (1997) 121–134.

760
CHAPTER 12 Adaptive Filters
[294] J.E. Cousseau, P.S.R. Diniz, New adaptive IIR ﬁltering algorithms based on the Steiglitz-McBride method,
IEEE Trans. Signal Process. 45 (5) (1997) 1367–1371.
[295] P.S.R. Diniz, J.E. Cousseau, A. Antoniou, Fast parallel realization of IIR adaptive ﬁlters, IEEE Trans. Circ.
Syst. II 41 (8) (1994) 561–567.
[296] H. Fan, A structural view of asymptotic convergence speed of adaptive IIR ﬁltering algorithms. I. Inﬁnite
precision implementation, IEEE Trans. Signal Process. 41 (4) (1993) 1493–1517.
[297] C.R. Johnson Jr., Adaptive IIR ﬁltering: current results and open issues, IEEE Trans. Inform. Theory 30 (2)
(1984) 237–250.
[298] R. Lopez-Valcarce, F. Perez-Gonzalez, Adaptive lattice IIR ﬁltering revisited: convergence issues and new
algorithms with improved stability properties, IEEE Trans. Signal Process. 49 (4) (2001) 811–821.
[299] P.A. Regalia, Stable and efﬁcient lattice algorithms for adaptive IIR ﬁltering, IEEE Trans. Signal Process.
40 (2) (1992) 375–388.
[300] S. Stearns, Error surfaces of recursive adaptive ﬁlters, IEEE Trans. Acoust. Speech Signal Process. 29 (3)
(1981) 763–766.
[301] D.P. Bertsekas, I.B. Rhodes, Recursive state estimation for a set-membership description of uncertainty,
IEEE Trans. Autom. Control AC-16 (2) (1971) 117–128.
[302] S. Dasgupta, Y.-F. Huang, Asymptotically convergent modiﬁed recursive least-squares with data-dependent
updating and forgetting factor for systems with bounded noise, IEEE Trans. Inform. Theory 33 (3) (1987)
383–392.
[303] J.R. Deller Jr., M. Nayeri, S.F. Odeh, Least-square identiﬁcation with error bounds for real-time signal
processing and control, Proc. IEEE 81 (6) (1993) 815–849.
[304] T. Aboulnasr, K. Mayyas, Complexity reduction of the NLMS algorithm via selective coefﬁcient update,
IEEE Trans. Signal Process. 47 (5) (1999) 1421–1424.
[305] R.C. de Lamare, P. Diniz, Set-membership adaptive algorithms based on time-varying error bounds for
CDMA interference suppression, IEEE Trans. Veh. Technol. 58 (2) (2009) 644–654.
[306] P.S.R. Diniz, S. Werner, Set-membership binormalized data-reusing LMS algorithms, IEEE Trans. Signal
Process. 51 (1) (2003) 124–134.
[307] K. Dogancay, O. Tanrikulu, Adaptive ﬁltering algorithms with selective partial updates, IEEE Trans. Circ.
Syst. II 48 (8) (2001) 762–769.
[308] S.C. Douglas, Adaptive ﬁlters employing partial updates, IEEE Trans. Circ. Syst. II 44 (3) (1997) 209–216.
[309] S. Werner, J.A. Apolinário, M.L. de Campos, P.S. Diniz, Low-complexity constrained afﬁne-projection
algorithms, IEEE Trans. Signal Process. 53 (12) (2005) 4545–4555.
[310] S. Werner, M.L.R. de Campos, P.S.R. Diniz, Partial-update NLMS algorithms with data-selective updating,
IEEE Trans. Signal Process. 52 (4) (2004) 938–949.
[311] K. Slavakis, S. Theodoridis, I. Yamada, Online kernel-based classiﬁcation using adaptive projection algo-
rithms, IEEE Trans. Signal Process. 56 (7) (2008) 2781–2796.
[312] K. Slavakis, S. Theodoridis, I. Yamada, Adaptive constrained learning in reproducing kernel hilbert spaces:
the robust beamforming case, IEEE Trans. Signal Process. 57 (12) (2009) 4744–4764.
[313] I. Yamada, K. Slavakis, K. Yamada, An efﬁcient robust adaptive ﬁltering algorithm based on parallel sub-
gradient projection techniques, IEEE Trans. Signal Process. 50 (5) (2002) 1091–1101.
[314] W. Liu, J. Principe, S. Haykin, Kernel Adaptive Filtering: A Comprehensive Introduction, Wiley Publishing,
2010.
[315] J. Capon, High-resolution frequency-wavenumber spectrum analysis, Proc. IEEE 57 (8) (1969) 1408–1418.
[316] S. Applebaum, Adaptive arrays, IEEE Trans. Antennas Propag. 24 (5) (1976) 585–598.
[317] N. Bornhorst, M. Pesavento, A.B. Gershman, Distributed beamforming for multi-group multicasting relay
networks, IEEE Trans. Signal Process. 60 (1) (2012) 221–232.

References
761
[318] H. Cox, R. Zeskind, M. Owen, Robust adaptive beamforming, IEEE Trans. Acoust. Speech, Signal Process.
35 (10) (1987) 1365–1376.
[319] O.L. Frost III, An algorithm for linearly constrained adaptive array processing, Proc. IEEE 60 (8) (1972)
926–935.
[320] G.-O. Glentis, K. Berberidis, S. Theodoridis, Efﬁcient least squares adaptive algorithms for FIR transversal
ﬁltering, IEEE Signal Process. Mag. 16 (4) (1999) 13–41.
[321] L. Lei, J.P. Lie, A.B. Gershman, C.M.S. See, Robust adaptive beamforming in partly calibrated sparse sensor
arrays, IEEE Trans. Signal Process. 58 (3) (2010) 1661–1667.
[322] B.D. Van Veen, An analysis of several partially adaptive beamformer designs, IEEE Trans. Acoust. Speech
Signal Process. 37 (2) (1989) 192–203.
[323] B.D. van Veen, Minimum variance beamforming with soft response constraints, IEEE Trans. Signal Process.
39 (9) (1991) 1964–1972.
[324] S.A. Vorobyov, H. Chen, A.B. Gershman, On the relationship between robust minimum variance beamform-
ers with probabilistic and worst-case distortionless response constraints, IEEE Trans. Signal Process. 56 (11)
(2008) 5719–5724.
[325] S.A. Vorobyov, A.B. Gershman, Z.-Q. Luo, N. Ma, Adaptive beamforming with joint robustness against
mismatched signal steering vector and interference nonstationarity, IEEE Signal Process. Lett. 11 (2) (2004)
108–111.
[326] H. Krim, M. Viberg, Two decades of array signal processing research: the parametric approach, IEEE Signal
Process. Mag. 13 (4) (1996) 67–94.
[327] H.L.V. Trees, Optimum Array Processing, Wiley, 2002.
[328] B.D. Van Veen, K.M. Buckley, Beamforming: a versatile approach to spatial ﬁltering, IEEE ASSP Mag. 5
(2) (1988) 4–24.
[329] L. Scharf, D. Tufts, Rank reduction for modeling stationary signals, IEEE Trans. Acoust. Speech Signal
Process. 35 (3) (1987) 350–355.
[330] A.M. Haimovich, Y. Bar-Ness, An eigenanalysis interference canceler, IEEE Trans. Signal Process. 39 (1)
(1991) 76–84.
[331] Y. Song, S. Roy, Blind adaptive reduced-rank detection for DS-CDMA signals in multipath channels, IEEE
J. Sel. Areas Commun. 17 (11) (1999) 1960–1970.
[332] J.S. Goldstein, I.S. Reed, L.L. Scharf, A multistage representation of the Wiener ﬁlter based on orthogonal
projections, IEEE Trans. Inform. Theory 44 (7) (1998) 2943–2959.
[333] M.L. Honig, J.S. Goldstein, Adaptive reduced-rank interference suppression based on the multistage Wiener
ﬁlter, IEEE Trans. Commun. 50 (6) (2002) 986–994.
[334] L.L. Scharf, E.K.P. Chong, M.D. Zoltowski, J.S. Goldstein, I.S. Reed, Subspace expansion and the equiv-
alence of conjugate direction and multistage Wiener ﬁlters, IEEE Trans. Signal Process. 56 (10) (2008)
5013–5019.
[335] D.A. Pados, G.N. Karystinos, An iterative algorithm for the computation of the MVDR ﬁlter, IEEE Trans.
Signal Process. 49 (2) (2001) 290–300.
[336] R.C. de Lamare, R. Sampaio-Neto, Reduced-rank interference suppression for DS-CDMA based on inter-
polated ﬁr ﬁlters, IEEE Commun. Lett. 9 (3) (2005) 213–215.
[337] R.C. de Lamare, R. Sampaio-Neto, Reduced-rank adaptive ﬁltering based on joint iterative optimization of
adaptive ﬁlters, IEEE Signal Process. Lett. 14 (12) (2007) 980–983.
[338] R.C. de Lamare, R. Sampaio-Neto, Adaptive reduced-rank processing based on joint and iterative interpola-
tion, decimation, and ﬁltering, IEEE Trans. Signal Process. 57 (7) (2009) 2503–2514.
[339] M. Yukawa, R. de Lamare, R. Sampaio-Neto, Efﬁcient acoustic echo cancellation with reduced-rank adaptive
ﬁltering based on selective decimation and adaptive interpolation, IEEE Trans. Speech Audio Process. 16
(4) (2008) 696–710.

13
CHAPTER
Introduction to Machine Learning
Johan A.K. Suykens
KU Leuven, ESAT-SCD/SISTA, Leuven (Heverlee), Belgium
1.13.1 Scope and context
Classically,withinthesignalprocessingcommunity,linearparametricmodelshavebeenamethodofﬁrst
choice in several applications. Historically, many computationally efﬁcient algorithms have been devel-
oped for on-line and adaptive signal processing with e.g., LMS, recursive least squares and Kalman ﬁlter-
ing type algorithms [1]. However, more recently considerable progress has been made also on the use of
ﬂexiblenonlinearmodels,e.g.,relatedtokernelmethods,supportvectormachines[2–10]andprobabilis-
tic models [11–16], and the importance of regularization techniques has been realized both in the context
of parametric models and non-parametric models. This is witnessed also by the progress in the area of
compressed sensing and sparse models [17–22]. Moreover, many emerging applications in e.g., big data,
networks applications, bioinformatics, brain-machine interfaces, are posing new challenges for predic-
tive models towards handling large amounts of data in high dimensional input spaces. In this Machine
Learning Section we therefore take a broad view on the subject of signal processing and machine
learning in connection also to other related areas as pattern recognition and neural networks [23–25],
mathematics and statistics [3,26,27], optimization [28,29], and information theory [30] (Figure 13.1).
In general one distinguishes between different types of learning models, such as supervised, unsu-
pervised and semi-supervised learning [9,31,32], various tasks such as e.g., classiﬁcation, regression,
clustering and different types of models, including e.g., linear and nonlinear parametric models, kernel-
basedmodels,andprobabilisticmodels(Figure13.2).Formanyofthesuccessfulmethodsitisinteresting
to trace back to the original roots. For on-line learning of linear models in classiﬁcation problems, the
perceptron has originally served as a paradigm. However, soon one has encountered its limitations. In
the neural networks area this led to introducing one or more hidden layers with multilayer perceptron
neural networks. Backpropagation as the original learning algorithm for such feedforward networks, in
its on-line learning form, could be interpreted as an extension of the LMS algorithm as used in adaptive
signal processing [33]. On a different track, the perceptron has also been studied within the context
of statistical learning theory [9,34,35]. Here one is interested in characterizing the generalization
error of the model, which is typically expressed in terms of the error on the training data and a
complexity term.
Multilayer perceptrons are universal approximators [36] which make them powerful tools to param-
eterize nonlinear functions. In order to overcome the problem of overﬁtting with ﬂexible nonlinear
models, an important technique to use is regularization [15,23]. In the objective function one not only
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00013-9
© 2014 Elsevier Ltd. All rights reserved.
765

766
CHAPTER 13 Introduction to Machine Learning
neural networks
pattern recognition
statistics
mathematics
optimization
information theory
signal processing
machine learning
FIGURE 13.1
Signal processing and machine learning and several related areas.
minimizes then the error on the training data but one also keeps the estimated parameter values small.
This leads to the notion of effective number of parameters which is relevant then to characterize the
model complexity, instead of the number of parameters. The ﬂexibility of the model is controlled by the
regularization term. In a Bayesian inference and probabilistic modeling picture the regularization term
corresponds then to the prior distribution on the unknown parameters. Classical regularization schemes
minimize the ℓ2 norm on the unknown parameters, which is known as ridge regression in statistics

1.13.1 Scope and Context
767
Learning modes
supervised learning
unsupervised learning
semi-supervised learning
reinforcement learning
inductive learning
transductive learning
ensemble learning
transfer learning
Tasks
regression
classiﬁcation
clustering
density estimation
component analysis
dimensionality reduction
data visualization
manifold learning
structure/feature selection
multi-task learning
dynamical systems modelling
time-series analysis
Models
linear parametric
non-linear parametric
polynomial model
multilayer perceptron
radial basis function network
splines
kernel-based model
support vector machines
graphical models
probabilistic models
mixture models
FIGURE 13.2
Learning modes, learning tasks and examples of different possible models.
and dates back also to ill-posed problems and Tikhonov regularization [37]. In recent years there has
been considerable interest in alternative regularization schemes based on ℓ0, ℓ1, and ℓp regulariza-
tion to achieve sparsity in the solution vector (Figure 13.3), in connection also to compressed sensing
[17–20,22].
Regularization also plays an important role in non-parametric and kernel-based models. The use of
positive deﬁnite kernels and reproducing kernels dates back to the early work of Mercer [38], Moore
[39], Aronszajn [40] and are key ingredients within methods of function estimation in reproducing
kernel Hilbert spaces, the theory of splines and radial basis function networks [10,41]. Early use of
reproducing kernel Hilbert spaces in signal processing is e.g., [42–44]. In Gaussian processes the kernel
function relates to the correlation function [14,16]. An increasing and renewed interest in kernel-based
methods appeared with the introduction of nonlinear support vector machines for classiﬁcation and
regression [9]. The use of a positive deﬁnite kernel is viewed here in connection to a feature map
(often called the kernel trick, which relates to the Mercer theorem), where in the primal a constrained
optimization problem formulation is given on the model that is expressed in terms of the feature map.
The Lagrange dual problem results then into a kernel-based model representation. In standard support

768
CHAPTER 13 Introduction to Machine Learning
Parametric
2, ridge regression
1, LASSO
p (0 < p ≤1)
group LASSO
elastic net
spectral regularization
nuclear norm
Kernel-based
RKHS function estimation
splines
regularization networks
Gaussian processes
support vector machines
LS-SVMs
Regularization
FIGURE 13.3
Regularization and its role in parametric and non-parametric modeling approaches.
vector machines a sparse kernel-based model is then achieved through the choice of the loss function,
typically the hinge loss in classiﬁcation and the epsilon-insensitive loss function in function estimation.
The kernel trick on its own has also been frequently employed to obtain nonlinear kernel versions
of classically known linear estimation schemes, e.g., kernel principal component analysis [45] as an
extension to the classical linear principal component analysis [46]. Special kernel functions have also
been designed to handle speciﬁc data types or in speciﬁc applications area such as e.g., text mining or
bioinformatics [5,6,47]. It is also possible to relate kernel functions to probabilistic graphical models and
graphs. In least squares support vector machines one works with simple core models within the primal-
dual setting for a wide range of problems in supervised and unsupervised learning and beyond [8,48].
The primal representation relates then to parametric picture, while the dual representation to a non-
parametric. Depending on the nature of the given problem (large number of data versus dimensionality
of the input space) this choice in representation can be exploited for developing efﬁcient large scale
algorithms [8,48].
An advantage of support vector machines for classiﬁcation and regression is that the problem is
recasted as a convex optimization problem, up to a small amount of tuning parameters of regularization
constant(s) and kernel parameter(s). This has been viewed as a considerable advantage over other

1.13.2 Contributions
769
nonlinear models such as multilayer perceptrons which suffer from the existence of many local minima
solutions. Also towards sparse models and compressed sensing, convex optimization is playing an
important role [29] (Figure 13.3). In many emerging applications one often has to cope with large
amounts of data in often high-dimensional input spaces. This is posing new challenges for scalable
optimization algorithms. In this direction efﬁcient ﬁrst order methods, on-line optimization, stochastic
optimization or distributed optimization are suitable possible algorithms [49,50].
In the next section a brief overview is given on the chapter contributions that present introductory
and tutorial contributions related to Signal Processing and Machine Learning.
1.13.2 Contributions
In [51] the authors present an overview of learning theory including statistical and computational aspects,
with emphasis on classiﬁcation and regression problems. Empirical risk minimization is discussed
and concepts for characterizing the generalization performance of the model such as Rademacher
complexity, covering numbers, Vapnik-Chervonenkis and fat shattering dimension. In connection to
this, the problem of model selection is addressed.
In [52] an overview is presented on different types of neural networks for supervised and unsuper-
vised learning. Starting from the perceptron, feedforward networks and backpropagation is explained.
Next recurrent neural networks and recursive structure processing are discussed. Neural architectures
for principal component analysis and topographic mapping for data mining and data visualization is
outlined.
In [53] the authors give an introduction to the foundations and implementations of kernel methods,
computational issues and recent developments. This includes the kernel trick, properties and types
of kernels, kernel principal component analysis, kernel canonical correlation analysis, kernel Fisher
discriminant analysis, support vector machines for classiﬁcation and regression, and Gaussian processes.
In [54] on-line learning in reproducing kernel Hilbert spaces is presented. First parameter estima-
tion is discussed in regression and classiﬁcation tasks and how to overcome overﬁtting by applying
regularization. It is explained how a nonlinear task can be mapped to a linear task. In this way kernel
LMS and complex kernel LMS are extended to kernel versions of the well-known LMS algorithm in
signal processing. For least squares learning algorithms extensions to kernel recursive least squares are
discussed. Finally, convex analysis concepts for online learning are provided.
In [55] an introduction to probabilistic graphical models is given. It includes three representations of
probabilistic graphical models: Markov networks (or undirected graphical models), Bayesian networks
(or directed graphical models) and factor graphs. An overview about structure and parameter learning
techniques is given on maximum likelihood and Bayesian learning, and generative and discriminative
learning. Exact inference methods and approximate inference techniques are addressed. Applications for
each of the three representations are given: Bayesian networks for expert systems, dynamic Bayesian
networks for speech processing, Markov random ﬁelds for image processing, and factor graphs for
decoding error-correcting codes.
In [56] a tutorial introduction to Monte Carlo Methods, Markov Chain Monte Carlo and Particle
Filtering is given. Starting from the Monte Carlo principle and basic techniques for simulating and trans-
forming random variables, Markov Chain Monte Carlo is explained. Other topics that are addressed are

770
CHAPTER 13 Introduction to Machine Learning
rejection sampling, detailed balance, the Gibbs sampler, sequential Monte Carlo, importance sampling,
resampling, and advanced Monte Carlo methods.
In [57] an introduction to clustering is given. Different clustering algorithms are discussed includ-
ing hierarchical clustering, the K-means algorithm, fuzzy C-means algorithm, mixture density-based
clustering, neural network-based clustering based on adaptive resonance theory, spectral clustering,
subspace clustering and biclustering, and deep learning clustering.
In [58] unsupervised learning algorithms and latent variable models are presented. Basic linear and
multilinear models for matrix and tensor factorizations and decompositions are discussed. Constrained
matrix and tensor decompositions for sparse representation of data and their extensions are addressed.
Various constraints such as orthogonality, statistical independence, nonnegativity, and/or sparsity are
explained. The importance of matrix/tensor decompositions is given for blind source separation, dimen-
sionality reduction, pattern recognition, object detection, classiﬁcation, multiway clustering, sparse
representation and coding, and data fusion.
In [59] an introduction is presented on semi-supervised learning. Discussed topics include transduc-
tive support vector machine and low density separation, co-training and multi-view, co-regularization
and expectation-maximization for mixture models. Finally graph-based semi-supervised learning is
addressed with graph Laplacian regularization, manifold regularization, measure-based regularization,
and semi-supervised learning for structured outputs.
In [60] an overview is given on sparsity-aware learning and compressed sensing. The Least Absolute
Shrinkage and Selection Operator (LASSO), sparse signal representation, ℓ2, ℓ0, ℓ1 norm minimizers
and their geometric interpretation are discussed. In view of conditions for equivalence of the ℓ0 and ℓ1
minimizer, mutual coherence and the Restricted Isometry Property (RIP) is explained. Robust sparse
signal recovery from noisy measurements and compressed sensing is covered. Sparsity-promoting algo-
rithms are discussed like Orthogonal Matching Pursuit, the Least Angle Regression (LARS) algorithm
and Iterative Shrinkage Algorithms. A case study on time-frequency analysis is provided.
In[61]theauthorspresentinformationbasedlearningapproaches.Startingfrominformationtheoretic
descriptors as entropy, divergence and mutual information, a unifying information theoretic framework
for machine learning is outlined. Filtering, classiﬁcation, feature extraction and nonparametric infor-
mation estimators are discussed. Next a reproducing kernel Hilbert space framework for information
based learning is proposed. Illustrative examples are given on adaptive system training, classiﬁcation,
information cut for clustering, and independent component analysis.
In [62] model selection aspects are discussed. The Akaike information criterion and the Kullback
information criterion are explained with linear regression as an example application. Then consistency
and efﬁciency are addressed. Other topics that are included are Bayesian approaches to model selection,
the Bayesian information criterion, Markov-Chain Monte-Carlo Bayesian methods, model selection by
compression, minimum message length, model selection consistency, parameter estimation consistency,
and sequential variants of minimum description length.
In [63] an overview is given on music mining. Topics that are addressed include ground truth acquisi-
tion and evaluation, audio feature extraction, extracting context information about music, content-based
similarity retrieval, genre classiﬁcation, emotion/mood classiﬁcation, music clustering, automatic tag
annotation, audio ﬁngerprinting, and cover song detection.

References
771
Acknowledgments
The author acknowledges support from KU Leuven, the Flemish Government, FWO, the Belgian Federal Science
Policy Ofﬁce, and the European Research Council (ERC AdG A-DATADRIVE-B, CoE EF/05/006, GOA MANET,
IUAP DYSCO, FWO G.0377.12, POM II, Cost IntelliCIS, iMinds Future Health Department).
References
[1] S. Haykin, Adaptive Filter Theory, third Ed., Prentice-Hall, 1996.
[2] N. Cristianini, J. Shawe-Taylor, An Introduction to Support Vector Machines, Cambridge University Press,
2000.
[3] F. Cucker, D.-X. Zhou, Learning Theory: An Approximation Theory Viewpoint, Cambridge University Press,
2007.
[4] W. Liu, J. Principe, S. Haykin, Kernel Adaptive Filtering: A Comprehensive Introduction, Wiley, Hoboken,
New Jersey, 2010.
[5] B. Schölkopf, A. Smola, Learning with Kernels, MIT Press, Cambridge, MA, 2002.
[6] J. Shawe-Taylor, N. Cristianini, Kernel Methods for Pattern Analysis, Cambridge University Press, June
2004.
[7] I. Steinwart, A. Christmann, Support Vector Machines, Springer, New York, 2008.
[8] J.A.K. Suykens, T. Van Gestel, J. De Brabanter, B. De Moor, J. Vandewalle, Least Squares Support Vector
Machines, World Scientiﬁc, Singapore, 2002.
[9] V. Vapnik, Statistical Learning Theory, John Wiley & Sons, New York, 1998.
[10] G. Wahba, Spline Models for Observational Data, Series in Applied Mathematics, vol. 59, SIAM, Philadel-
phia, 1990.
[11] C.M. Bishop, Pattern Recognition and Machine Learning, Springer, 2006.
[12] M.I. Jordan, Learning in Graphical Models, MIT Press, 1999.
[13] D. Koller, N. Friedman, Probabilistic Graphical Models: Principles and Techniques, MIT Press, 2009.
[14] D.J.C. MacKay, Introduction to Gaussian processes, in: C.M. Bishop (Ed.), Neural networks and machine
learning, Springer NATO-ASI Series F: Computer and Systems Sciences, vol. 168, 1998, pp. 133–165.
[15] D.J.C. MacKay, Information Theory, Inference, and Learning Algorithms, Cambridge University Press,
2003.
[16] C.E. Rasmussen, C.K.I. Williams, Gaussian Processes for Machine Learning, MIT Press, 2006.
[17] R.G. Baraniuk, V. Cevher, M.F. Duarte, C. Hegde, Model-based compressive sensing, IEEE Trans. Inform.
Theory 56 (4) (2010) 1982–2001.
[18] A.M. Bruckstein, D.L. Donoho, M. Elad, From sparse solutions of systems of equations to sparse modeling
of signals and images, SIAM Rev. 51 (1) (2009) 34–81.
[19] E.J. Candes, J. Romberg, T. Tao, Stable recovery from incomplete and inaccurate measurements, Commun.
Pure Appl. Math. 59 (8) (2006) 1207–1223.
[20] E.J. Candes, M.B. Wakin, An introduction to compressive sampling, IEEE Signal Process. Mag. 25 (2)
(2008) 21–30.
[21] S. Chen, D.L. Donoho, M. Saunders, Atomic decomposition by basis pursuit, SIAM J. Sci. Comput. 20 (1)
(1998) 33–61.
[22] D.L. Donoho, M. Elad, Optimally sparse representation in general (nonorthogonal) dictionaries via ℓ1 min-
imization, in: Proceedings of National Academy Sciences, 2003, pp. 2197–2202.
[23] C.M. Bishop, Neural Networks for Pattern Recognition, Oxford University Press, 1995.

772
CHAPTER 13 Introduction to Machine Learning
[24] R.O. Duda, P.E. Hart, D.G. Stork, Pattern Classiﬁcation, second ed., John Wiley & Sons, New York, 2001.
[25] S. Theodoridis, K, Pattern Recognition, Academic Press, Koutroumbas, 2009.
[26] F. Cucker, S. Smale, On the mathematical foundations of learning theory, Bull. AMS 39 (2002) 1–49.
[27] T. Hastie, R. Tibshirani, J, Friedman, The Elements of Statistical Learning, Springer-Verlag, 2001.
[28] H.H. Bauschke, P.L. Combettes, Convex Analysis and Monotone Operator Theory in Hilbert Spaces,
Springer, 2011.
[29] S. Boyd, L, Convex Optimization, Cambridge University Press, Vandenberghe, 2004.
[30] J.C. Principe, Information Theoretic Learning: Renyi’s Entropy and Kernel Perspectives, Springer, 2010.
[31] M. Belkin, P. Niyogi, V. Sindhwani, Manifold regularization: a geometric framework for learning from
labeled and unlabeled examples, J. Mach. Learn. Res. 7 (2006) 2399–2434.
[32] O. Chapelle, B. Schölkopf, A. Zien (Eds.), Semi-Supervised Learning, MIT Press, 2006.
[33] B. Widrow, R.G. Winter, Neural nets for adaptive ﬁltering and adaptive pattern recognition, IEEE Comput.
Mag. 21 (3) (1988) 25–39.
[34] V. Vapnik, A. Lerner, Pattern recognition using generalized portrait method, Autom. Remote Control 24
(1963) 774–780.
[35] V. Vapnik, A. Chervonenkis, A note on one class of perceptrons, Autom. Remote Control 25 1964.
[36] K. Hornik, M. Stinchcombe, H. White, Multilayer feedforward networks are universal approximators, Neural
Networks 2 (1989) 359–366.
[37] A.N. Tikhonov, V.Y. Arsenin, Solution of Ill-Posed Problems, Winston, Washington, DC, 1977.
[38] J. Mercer, Functions of positive and negative type and their connection with the theory of integral equations,
Philos. Trans. Roy. Soc. Lond. 209 (1909) 415–446.
[39] E.H. Moore, On properly positive Hermitian matrices, Bull. Am. Math. Soc. 23 (1916) 59.
[40] N. Aronszajn, Theory of reproducing kernels, Trans. Am. Math. Soc. 68 (1950) 337–404.
[41] T. Poggio, F. Girosi, Networks for approximation and learning, Proc. IEEE 78 (9) (1990) 1481–1497.
[42] T. Kailath, RKHS approach to detection and estimation problems: Part I: Deterministic signals in Gaussian
noise, IEEE Trans. Inform. Theory 17 (5) (1971) 530–549.
[43] T. Kailath, A view of three decades of linear ﬁltering theory, IEEE Trans. Inform. Theory 20 (2) (1974)
146–181.
[44] E. Parzen, Statistical inference on time series by RKHS methods, Department of Statistical Stanford Uni-
versity Technical Report 14, January 1970.
[45] B. Schölkopf, A. Smola, K.-R. Müller, Nonlinear component analysis as a kernel eigenvalue problem, Neural
Comput. 10 (1998) 1299–1319.
[46] I.T. Jolliffe, Principal Component Analysis, Springer Series in Statistics, Springer-Verlag, 1986.
[47] G. Bakir, T. Hofmann, B. Schölkopf, A. Smola, B. Taskar, S.V.N. Vishwanathan (Eds.), Predicting Structured
Data, MIT Press, Cambridge, MA, 2007.
[48] J.A.K. Suykens, C. Alzate, K. Pelckmans, Primal and dual model representations in Kernel-based learning,
Stat. Surv. 4 (2010) 148–183.
[49] S. Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein, Distributed optimization and statistical learning via the
alternating direction method of multipliers, Found. Trends Mach. Learn. 3 (1) (2011) 1–122.
[50] Y.E. Nesterov, A method for solving the convex programming problem with convergence rate O(1/k2),
Dokl. Akad. Nauk SSSR 269 (1983) 543–547 (in Russian).
[51] A.Tewari,P.L.Bartlett,Learningtheory,in:S.Theodoridis,R.Chellappa(Eds.),LibraryinSignalProcessing,
vol. 1, Academic Press, 2013.
[52] B. Hammer, Neural networks, in: S. Theodoridis, R. Chellappa (Eds.), Library in Signal Processing, vol. 1,
Academic Press, 2013.
[53] J. Shawe-Taylor, S. Sun, Kernel methods and support vector machines, in: S. Theodoridis, R. Chellappa
(Eds.), Library in Signal Processing, vol. 1, Academic Press, 2013.

References
773
[54] K. Slavakis, P. Bouboulis, S. Theodoridis, Online learning in reproducing Kernel Hilbert spaces, in:
S. Theodoridis, R. Chellappa (Eds.), Library in Signal Processing, vol. 1, Academic Press, 2013.
[55] F. Pernkopf, R. Peharz, S. Tschiatschek, Introduction to probabilistic graphical models, in: S. Theodoridis,
R. Chellappa (Eds.), Library in Signal Processing, vol. 1, Academic Press, 2013.
[56] A.T. Cemgil, A tutorial introduction to Monte Carlo methods, Markov Chain Monte Carlo and particle
ﬁltering, in: S. Theodoridis, R. Chellappa (Eds.), Library in Signal Processing, vol. 1, Academic Press, 2013.
[57] D. Lam, D.C. Wunsch, Clustering, in: S. Theodoridis, R. Chellappa (Eds.), Library in Signal Processing,
vol. 1, Academic Press, 2013.
[58] A. Cichocki, Unsupervised learning algorithms and latent variable models: PCA/SVD, CCA/PLS, ICA,
NMF, etc., in: S. Theodoridis, R. Chellappa (Eds.), Library in Signal Processing, vol. 1, Academic Press,
2013.
[59] X. Zhou, M. Belkin, Semi-supervised learning, in: S. Theodoridis, R. Chellappa (Eds.), Library in Signal
Processing, vol. 1, Academic Press, 2013.
[60] S. Theodoridis, Y. Kopsinis, K. Slavakis, Sparsity-aware learning and compressed sensing: an overview, in:
S. Theodoridis, R. Chellappa (Eds.), Library in Signal Processing, vol. 1, Academic Press, 2013.
[61] J.C. Principe, B. Chen, L.G. Sanchez Giraldo, Information based learning, in: S. Theodoridis, R. Chellappa
(Eds.), Library in Signal Processing, vol. 1, Academic Press, 2013.
[62] E. Makalic, D.F. Schmidt, A.-K. Seghouane, A tutorial on model selection, in: S. Theodoridis, R. Chellappa
(Eds.), Library in Signal Processing, vol. 1, Academic Press, 2013.
[63] G. Tzanetakis, Music mining, in: S. Theodoridis, R. Chellappa (Eds.), Library in Signal Processing, vol. 1,
Academic Press, 2013.

14
CHAPTER
Learning Theory
Ambuj Tewari∗and Peter L. Bartlett†
*439 West Hall 1085 South University Ann Arbor, MI, USA
†387 Soda Hall #1776 Berkeley, CA, USA
1.14.1 Introduction
In a section on Learning Machines in one of his most famous papers [1], Turing wrote:
Instead of trying to produce a programme to simulate the adult mind, why not rather try
to produce one which simulates the child’s? If this were then subjected to an appropriate
course of education one would obtain the adult brain. Presumably the child brain is some-
thing like a notebook as one buys it from the stationer’s. Rather little mechanism, and lots
of blank sheets. (Mechanism and writing are from our point of view almost synonymous.)
Our hope is that there is so little mechanism in the child brain that something like it can
be easily programmed. The amount of work in the education we can assume, as a ﬁrst
approximation, to be much the same as for the human child.
The year was 1950. Earlier, in 1943, McCulloch and Pitts [2] had already hit upon the idea that
mind-like machines could be built by studying how biological nerve cells behave. This directly lead
to neural networks. By the late 1950s, Samuels [3] had programmed a machine to play checkers and
Rosenblatt [4] had proposed one of earliest models of a learning machine, the perceptron. The impor-
tance of the perceptron in the history of learning theory cannot be overemphasized. Vapnik says that
the appearance of the perceptron was “when the mathematical analysis of learning processes truly
began.” Novikoff’s perceptron theorem (we will prove it in Section 1.14.6.5.2) was also proved in
1962 [5].
Vapnik [6] points out the interesting fact that the 1960s saw four major developments that were to have
lasting inﬂuence on learning theory. First, Tikhonov and other developed regularization theory for the
solution of ill posed inverse problems. Regularization theory has had and continues to have tremendous
impact on learning theory. Second, Rosenblatt, Parzen, and Chentsov pioneered nonparametric methods
for density estimation. These beginnings were crucial for a distribution free theory of non-parametric
classiﬁcation and regression to emerge later. Third, Vapnik and Chervonenkis proved the uniform law
of large numbers for indicators of sets. Fourth, Solomonoff, Kolmogorov, and Chaitin all independently
discovered algorithmic complexity: the idea that the complexity of a string of 0’s and 1’s can be deﬁned
by the length of the shortest program that can generate that string. This later led Rissanen to propose
his Minimum Description Length (MDL) principle.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00014-0
© 2014 Elsevier Ltd. All rights reserved.
775

776
CHAPTER 14 Learning Theory
In 1986, a major technique to learn weights in neural networks was discovered: backpropagation [7].
Around the same time, Valiant [8] published his paper introducing a computational model of learning that
was to be called the PAC (probably approximately correct) model later. The 1990s saw the appearance
of support vector machines [9] and AdaBoost [10]. This brings us to the brink of the second millenium
which is where we end our short history tour.
As we can see, learning theory has absorbed inﬂuences from a variety of sources: cybernetics,
neuroscience, nonparametric statistics, regularization theory of inverse problems, and the theory of
computation and algorithms. Ongoing research is not only trying to overcome known limitations of
classic models but is also grappling with new issues such as dealing with privacy and web-scale data.
Learning theory is a formal mathematical theory. Assumptions are made, beautiful lemmas are
proved, and deep theorems are discovered. But developments in learning theory also lead to practical
algorithms. SVMs and AdaBoost are prime examples of high impact learning algorithms evolving out
of a principled and well-founded approach to learning. We remain hopeful that learning theory will help
us discover even more exciting learning algorithms in the future.
Finally, a word about the mathematical background needed to read this overview of learning theory.
Previous exposure to mathematical reasoning and proofs is a must. So is familiarity with probability
theory. But measure theory is not needed as we will avoid all measure-theoretic details. It will also
be helpful, but not essential, to be aware of the basics of computational complexity such as the notion
of polynomial time computation and NP-completeness. Similarly, knowing a little bit of functional
analysis will help.
1.14.2 Probabilistic formulation of learning problems
A basic problem in learning theory is that of learning a functional dependence f : X →Y from
some input space X to some output or label space Y. Some special choices for the output space
deserve particular attention since they arise often in applications. In multiclass classiﬁcation, we have
Y = {1, . . . , K} and the goal is to classify any input x ∈X into one of K given classes. A special case of
this problem is when K = 2. Then, the problem is called binary classiﬁcation and it is mathematically
convenient to think of the output space as Y = {−1, +1} rather than Y = {0, 1}. In regression, the
output space Y is some subset of the set R of real numbers. Usually Y is assumed to be a bounded
interval, say [−1, +1].
We assume that there is an underlying distribution P over X × Y and that the learner has access to
n samples
(X1, Y1), . . . , (Xn, Yn),
where the (Xi, Yi) are independent and identically distributed (or iid) random variables with the same
distribution P. Note that P itself is assumed to be unknown to the learner. A learning rule or learning
algorithm ˆfn is simply a mapping
ˆfn : (X × Y)n × X →Y.
That is, a learning rule maps the n samples to a function from X to Y. When the sample is clear from
context, we will denote the function returned by a learning rule itself by ˆfn.

1.14.2 Probabilistic Formulation of Learning Problems
777
1.14.2.1 Loss function and risk
In order to measure the predictive performance of a function f : X →Y, we use a loss function. A loss
function ℓ: Y × Y →R+ is a non-negative function that quantiﬁes how bad the prediction f (x) is if
the true label is y. We say that ℓ( f (x), y) is the loss incurred by f on the pair (x, y). In the classiﬁcation
case, binary or otherwise, a natural loss function is the 0–1 loss:
ℓ(y′, y) = 1[y′ ̸= y].
The 0–1 loss function, as the name suggests, is 1 if a misclassiﬁcation happens and is 0 otherwise. For
regression problems, some natural choices for the loss function are the squared loss:
ℓ(y′, y) = (y′ −y)2,
or the absolute loss:
ℓ(y′, y) = |y′ −y|.
Given a loss function, we can deﬁne the risk of a function f : X →Y as its expected loss under the
true underlying distribution:
R( f ) = E(X,Y)∼P[ℓ( f (X), Y)].
Note that the risk of any function f is not directly accessible to the learner who only sees the samples.
But the samples can be used to calculate the empirical risk of f:
R( f ) = 1
n
n

i=1
[ℓ( f (Xi), Yi)].
Minimizing the empirical risk over a ﬁxed class F ⊆YX of functions leads to a very important learning
rule, namely empirical risk minimization (ERM):
ˆfn = argmin
f ∈F
R( f ).
Under appropriate conditions on X, Y, and F, an empirical risk minimizer is guaranteed to exist though
it will not be unique in general. If we knew the distribution P then the best function from F would be
f ⋆
F = argmin
f ∈F
R( f ),
as f ⋆
F minimizes the expected loss on a random (X, Y) pair drawn from P. Without restricting ourselves
to the class F, the best possible function to use is
f ⋆= argmin
f
R( f ),
where the inﬁmum is with respect to all1 functions.
1We should say “all measurable functions” here. We will, however, ignore measurability issues in this overview article.

778
CHAPTER 14 Learning Theory
For classiﬁcation with 0–1 loss, f ⋆is called the Bayes classiﬁer and is given simply by
f ⋆(x) = argmax
y∈Y
P(Y = y|X = x).
Note that f ⋆depends on the underlying distribution P. If we knew P, we could achieve minimum
misclassiﬁcation probability by always predicting, for a given x, the label y with the highest conditional
probability (ties can be broken arbitrarily). The following result bounds the excess risk R( f ) −R( f ⋆)
of any function f in the binary classiﬁcation case.
Theorem 1.
For binary classiﬁcation with 0–1 loss, we have
R( f ) −R( f ⋆) = E[1[ f (X) ̸= f ⋆(X)]|2η(X) −1|],
where η(x) = P(Y = +1|X = x).
For the regression case with squared loss, f ⋆is called the regression function and is given simply
by the conditional expectation of the label given x:
f ⋆(x) = E[Y|X = x].
In this case, the excess risk takes a particularly nice form.
Theorem 2.
For regression with squared loss, we have
R( f ) −R( f ⋆) = E[| f (X) −f ⋆(X)|2].
1.14.2.2 Universal consistency
A particularly nice property for a learning rule ˆfn to have is that its (expected) excess risk converges to
zero as the sample size n goes to inﬁnity. That is,
E[R( ˆfn)] −R( f ⋆) →0.
Note R( ˆfn) is a random variable since ˆfn depends on a randomly drawn sample. A rule ˆfn is said to be
universally consistent if the above convergence holds irrespective of the true underlying distribution P.
This notion of consistency is also sometimes called weak universal consistency in order to distinguish
it from strong universal consistency that demands
R( ˆfn) −R( f ⋆) →0
with probability 1.
The existence of universally consistent learning rules for classiﬁcation and regression is a highly non-
trivial fact which has been known since Stone’s work in the 1970s [11]. Proving universal consistency
typically requires addressing the estimation error versus approximation error trade-off. For instance,
often the function ˆfn is guaranteed to lie in a function class Fn that does not depend on the sample. The
classes Fn typically grow with n. Then we can decompose the excess risk of ˆfn as
R( ˆfn) −R( f ⋆) = R( ˆfn) −R( f ⋆
Fn



)
estimation error
+ R( f ⋆
Fn) −R( f ⋆)



approximation error
.

1.14.2 Probabilistic Formulation of Learning Problems
779
This decomposition is useful as the estimation error is the only random part. The approximation error
depends on how rich the function class Fn is but does not depend on the sample. The reason we have
to trade these two errors off is that the richer the function class Fn the smaller the approximation error
will be. However, that leads to a large estimation error. This trade-off is also known as the bias-variance
trade-off. The bias-variance terminology comes from the regression with squared error case but tends
to be used in the general loss setting as well.
1.14.2.3 Learnability with respect to a ﬁxed class
Unlike universal consistency, which requires convergence to the minimum possible risk over all func-
tions, learnability with respect to a ﬁxed function class F only demands that
E[R( ˆfn)] −R( f ⋆
F) →0.
We can additionally require quantitative bounds on the number of samples needed to reach, with high
probability, a given upper bound on excess risk relative to f ⋆
F. The sample complexity of a learning rule
ˆfn is the minimum number n0(ε, δ) such that for all n ≥n0(ε, δ), we have
R( ˆfn) −R( f ⋆
F) ≤ε
with probability at least 1 −δ. We now see how we can arrive at sample complexity bounds for ERM
via uniform convergence of empirical means to true expectations.
We mention some examples of functions classes that arise often in practice.
Linear Functions: This is one of the simplest functions classes. In any ﬁnite dimensional Euclidean
space Rd, deﬁne the class of real valued functions
F = {x →⟨w, x⟩: w ∈W ⊆Rd},
where⟨w, x⟩= d
j=1 w j x j denotesthestandardinner product. The weightvector mightbeadditionally
constrained. For instance, we may require ∥w∥≤W for some norm ∥· ∥. The set W takes care of such
constraints.
Linear Threshold Functions or Halfspaces: This is class of binary valued function obtained by
thresholding linear functions at zero. This gives us the class
F = {x →sign(⟨w, x⟩) : w ∈W ⊆Rd}.
Reproducing Kernel Hilbert Spaces: Linear functions can be quite restricted in their approximation
ability. However, we can get a signiﬁcant improvement by ﬁrst mapping the inputs x ∈X into a feature
space through a feature mapping  : X →H where H is a very high dimensional Euclidean space (or
an inﬁnite dimensional Hilbert space) and then considering linear functions in the feature space:
F = {x →⟨w, (x)⟩H : w ∈H},
where the inner product now carries the subscript H to emphasize the space where it operates. Many
algorithms access the inputs only through pairwise inner products
⟨(xi), (x j)⟩H.

780
CHAPTER 14 Learning Theory
In such cases, we do not care what the feature mapping is or how complicated it is to evaluate, provided
we can compute these inner products efﬁciently.
This idea of using only inner products leads us to reproducing kernel Hilbert space. We say that a
function K : X × X →R is symmetric positive semideﬁnite (psd) if it satisﬁes the following two
properties.
Symmetry: For any x, x′ ∈X, we have K(x, x′) = K(x′, x).
Positive Semideﬁniteness: For any (x1, . . . , xn) ∈X n, the symmetric matrix
K = (K(xi, x j))i, j ∈Rn×n
is psd.2 The matrix K is often called the Gram matrix.
Note that, given a feature mapping , it is trivial to verify that
K(x, x′) = ⟨(x), (x′)⟩H
is a symmetric psd function. However, the much less trivial converse is also true. Every symmetric psd
function arises out of a feature mapping. This is a consequence of the Moore-Aronszajn theorem. To
state the theorem, we ﬁrst need a deﬁnition. A reproducing kernel Hilbert space or RKHS H is a Hilbert
space of functions f : X →R such that there exists a kernel K : X ×X →R satisfying the properties:
•
For any x ∈X, the function Kx = K(x, ·) is in H.
•
For any x ∈X, f ∈H, the reproducing property holds: f (x) = ⟨f , Kx⟩H.
Theorem 3 (Moore-Aronszajn).
Given a symmetric psd function K : X × X →R, there is a unique
RKHS H with reproducing kernel K.
This theorem gives us the feature mapping easily. Starting from a symmetric psd function K, we
use the Moore-Aronszajn theorem to get an RKHS H whose reproducing kernel is K. Now, by the
reproducing property, for any x, x′ ∈X,
K(x, x′) = Kx(x′) = ⟨Kx, Kx′⟩H
which means that  : X →H given by (x) = Kx is a feature mapping we were looking for. Given
this equivalence between symmetric psd functions and RKHSes, we will use the word kernel for a
symmetric psd function itself.
Convex Hulls: This example is not about a function class but rather about a means of obtaining a
new function class from an old one. Say, we have class F of binary valued functions f : X →{±1}.
We can consider functions that take a majority vote over a subset of functions in F. That is,
f ′(x) = sign
	 L

ℓ=1
fℓ

, fℓ∈F.
2Recall that a symmetric matrix K ∈Rn×n is psd iff for all u ∈Rn, ⟨u, Ku⟩≥0.

1.14.2 Probabilistic Formulation of Learning Problems
781
More generally, we can assign weights wℓ≥0 to fℓand consider a weighted majority
f ′(x) = sign
	 L

ℓ=1
wℓfℓ

.
Constraining the weights to sum to no more than W gives us the scaled convex hull
W · conv(F ∪{0}) =
 L

ℓ=1
wℓfℓ: L ∈N, wℓ≥0,
L

ℓ
wℓ≤W

.
The class above can, of course, be thresholded to produce binary valued functions.
1.14.2.4 ERM and uniform convergence
The excess risk of ERM relative to f ⋆
F can be decomposed as
R( fn) −R( f ⋆
F) = (R( fn) −R( fn)) + (R( fn) −R( f ⋆
F)) + (R( f ⋆
F) −R( f ⋆
F)).
By the deﬁnition of ERM, the difference R( fn)−R( f ⋆
F) is non-positive. The difference R( f ⋆
F)−R( f ⋆
F)
is also easy to deal with since it deals with a ﬁxed (i.e. non-random) function f ⋆
F. Using Hoeffding’s
inequality (see Section 1.14.3.1), we have, with probability at least 1 −δ,
R( f ⋆
F) −R( f ⋆
F) ≤

log (1/δ)
2n
.
This simply reﬂects the fact that as the sample size n grows, we expect the empirical average of f ⋆
F to
converge to its true expectation.
The matter with the difference R( fn) −R( fn) is not so simple since fn is a random function. But
we do know fn lies in F. So we can clearly bound
R( fn) −R( fn) ≤sup
f ∈F
(R( f ) −R( f )).
(14.1)
This provides a generalization bound (or, more properly, a generalization error bound) for ERM. A
generalization bound is an upper bound on the true expectation R( fn) of a learning rule in terms
of its empirical performance R( fn) (or some other performance measure computed from data). A
generalization bound typically involves additional terms that measure the statistical complexity of the
class of functions that the learning rule uses. Note the important fact that the bound above is valid not
just for ERM but for any learning rule that outputs a function fn ∈F.
The measure of complexity in the bound (14.1) above is the maximum deviation between true
expectation and empirical average over functions in F. For each ﬁxed f ∈F, we clearly have
R( f ) −R( f ) →0
both in probability (weak law of large numbers) and almost surely (strong law of large numbers). But
in order for
sup
f ∈F
R( f ) −R( f ) →0
the law of large numbers has to hold uniformly over the function class F.

782
CHAPTER 14 Learning Theory
To summarize, we have shown in this section that, with probability at least 1 −δ,
R( fn) −R( f ⋆
F) ≤sup
f ∈F
(R( f ) −R( f )) +

log (1/δ)
2n
.
(14.2)
Thus, we can bound the excess risk of ERM if we can prove uniform convergence of empirical means
to true expectations for the function class F. This excess risk bound, unlike the generalization error
bound (14.1), is applicable only to ERM.
1.14.2.5 Bibliographic note
There are several book length treatments of learning theory that emphasize different aspects of the
subject. The reader may wish to consult Anthony and Biggs [12], Anthony and Bartlett [13], Devroye
et al. [14], Vapnik [15], Vapnik [6], Vapnik [16], Vidyasagar [17], Natarajan [18], Cucker and Zhou
[19], Kearns and Vazirani [20], Cesa-Bianchi and Lugosi [21], Györﬁet al. [22], and Hastie et al. [23].
1.14.3 Uniform convergence of empirical means
Let us consider the simplest case of a ﬁnite function class F. In this case, Hoeffding’s inequality (see
Theorem 5) gives us for any f ∈F, with probability at least 1 −δ/|F|,
R( f ) −R( f ) ≤c

log (|F|/δ)
n
.
Therefore, denoting the event that the above inequality fails to hold for function f by A f , we have
P(∃f ∈F, A f holds) ≤

f ∈F
P(A f ) ≤|F| ·
δ
|F| = δ.
The ﬁrst inequality is called a union bound in probability theory. It is exact if and only if all the events it
is applied to are pairwise disjoint, i.e., A f ∩A f ′ = ∅for f ̸= f ′. Clearly, if there are functions that are
“similar” to each other, the bound above will be loose. Nevertheless, the union bound is an important
starting point for the analysis of uniform convergence of empirical means and for a ﬁnite class, it gives
us the following result.
Theorem 4.
Consider a ﬁnite class F and a loss function bounded by 1. Then, we have, with probability
at least 1 −δ,
sup
f
(R( f ) −R( f )) ≤

log |F| + log (1/δ)
2n
.
Therefore, using (14.2), we have the following result for ERM. With probability at least 1 −2δ,
R( fn) −R( f ⋆
F) ≤

log |F|
2n
+

2 log (1/δ)
n
.

1.14.3 Uniform Convergence of Empirical Means
783
It is instructive to focus on the ﬁrst term on the right hand side. First, it decreases as n increases.
This means that ERM gets better as it works with more data. Second, it increases as the function class
F grows in size. But the dependence is logarithmic and hence mild.
However, the bound above is useless if |F| is not ﬁnite. The goal of the theory we will build below
(often called “VC theory" after its architects Vapnik and Chervonenkis) will be to replace log |F| with
an appropriate measure of the capacity of an inﬁnite function class F.
1.14.3.1 Concentration inequalities
Learning theory uses concentration inequalities heavily. A typical concentration inequality will state
that a certain random variable is tightly concentrated around its mean (or in some cases median). That
is, the probability that they deviate far enough from the mean is small.
The following concentration inequality, called Hoeffding’s inequality, applies to sums of iid random
variables.
Theorem 5.
Let Zi ∈[0, 1] be bounded iid random variables with common expectation μ. Then we
have, for ε > 0
P

μ −1
n
n

i=1
Zi > ε

≤exp ( −2nε2).
This can be restated as follows. For any δ ∈(0, 1),
P

μ −1
n
n

i=1
Zi ≤

log (1/δ)
2n

> 1 −δ.
A couple of remarks are in order. First, even though we have stated Hoeffding’s inequality as applying
to bounded iid random variables, only the boundedness and independence are really needed. It is possible
to let the Zi’s have different distributions. Second, the above result provides probability upper bounds
for deviations below the mean μ but a similar result holds for deviations of 1
n
n
i=1 Zi above μ as well.
In fact, Hoeffding’s inequality can be considered a special case of the following inequality that is
often called McDiarmid’s inequality or the bounded differences inequality.
Theorem 6.
Let F : X n →R be a function that satisﬁes the bounded differences property: for all
i ∈[n] and x1:n ∈X n, x′
i ∈X,
| f (x1, . . . , xi, . . . , xn) −f (x1, . . . , x′
i, . . . , xn)| ≤ci
for some constants ci. Then, for any independent X1, . . . , Xn, the random variable Z = f (X1, . . . , Xn)
satisﬁes,
P[Z −EZ ≥ε] ≤exp
	
−
2ε2
n
i=1 c2
i

as well as
P[Z −EZ ≤−ε] ≤exp
	
−
2ε2
n
i=1 c2
i

for any ε > 0.

784
CHAPTER 14 Learning Theory
1.14.3.2 Rademacher complexity
Let us recall that we need to control the following quantity:
sup
f ∈F
R( f ) −R( f ).
Since this satisﬁes the conditions of McDiarmid’s inequality with ci = 1/n, we have, with probability
at least 1 −δ,
sup
f ∈F
R( f ) −R( f ) ≤E

sup f ∈F R( f ) −R( f )

+

log (1/δ)
2n
.
Now, we can appeal to a powerful idea known as symmetrization. The basic idea is to replace
sup
f ∈F
R( f ) −R( f )
with
sup
f ∈F
1
n
n

i=1
(E[ℓ( f (X′
i), Y ′
i )] −ℓ( f (Xi), Yi)),
where (X′
1, Y ′
1), . . . , (X′
n, Y ′
n) are samples drawn from the distribution P that are independent of each
other and of the original sample. The new sample is often referred to as the ghost sample. The point of
introducing the ghost sample is that, by an application of Jensen’s inequality, we get
E

sup f ∈F R( f ) −R( f )

= E

sup f ∈F
1
n
n

i=1
(E[ℓ( f (X′
i), Y ′
i )] −ℓ( f (Xi), Yi))

≤E

sup f ∈F
1
n
n

i=1
(ℓ( f (X′
i), Y ′
i ) −ℓ( f (Xi), Yi))

.
Now, the distribution of
sup
f ∈F
1
n
n

i=1
(ℓ( f (X′
i), Y ′
i ) −ℓ( f (Xi), Yi))
is invariant under exchanging any Xi, Yi with an X′
i, Y ′
i . That is, for any ﬁxed choice of ±1 signs
ϵ1, . . . , ϵn, the above quantity has the same distribution as
sup
f ∈F
1
n
n

i=1
ϵi(ℓ( f (X′
i), Y ′
i ) −ℓ( f (Xi), Yi)) .

1.14.3 Uniform Convergence of Empirical Means
785
This allows us introduce even more randomization by making the ϵi’s themselves random. This gives,
E

sup f ∈F R( f ) −R( f )

≤Eϵ,X,Y,X′,Y ′

sup f ∈F
1
n
n

i=1
ϵi(ℓ( f (X′
i), Y ′
i ) −ℓ( f (Xi), Yi))

≤Eϵ,X′,Y ′

sup f ∈F
1
n
n

i=1
ϵiℓ( f (X′
i), Y ′
i )

+Eϵ,X,Y

sup f ∈F
1
n
n

i=1
( −ϵi)ℓ( f (Xi), Yi)

= 2 · Eϵ,X,Y

sup f ∈F
1
n
n

i=1
ϵiℓ( f (Xi), Yi)

.
This directly yields a bound in terms of the Rademacher complexity:
Rn(ℓ◦F) = Eϵ,X,Y

sup f ∈F
1
n
n

i=1
ϵiℓ( f (Xi), Yi)

,
where ϵi is a Rademacher random variable that is either -1 or +1, each with probability 1
2. The subscript
below the expectation serves as a reminder that the expectation is being taken with respect to the
randomness in Xi, Yi’s, and ϵi’s. We have thus proved the following theorem.
Theorem 7 (Symmetrization).
For a function class F and loss ℓ, deﬁne the loss class
ℓ◦F = {(x, y) →ℓ( f (x), y) : f ∈F}.
We have,
E

sup f ∈F R( f ) −R( f )

≤2Rn(ℓ◦F).
An interesting observation is that the sample dependent quantity
Rn(ℓ◦F) = Eϵ

sup f ∈F
1
n
n

i=1
ϵiℓ( f (Xi), Yi)

also satisﬁes the bounded differences condition with ci = 1/n. This is the empirical Rademacher
average and the expectation in its deﬁnition is only over the Rademacher random variables. Applying
McDiarmid’s inequality, we have,
Rn(ℓ◦F) ≤Rn(ℓ◦F) +

log (1/δ)
2n
with probability at least 1 −δ.
To prove a bound on the Rademacher complexity of a ﬁnite class, the following lemma, due to [24],
is useful.

786
CHAPTER 14 Learning Theory
Lemma 8 (Massart’s ﬁnite class lemma).
Let A be a ﬁnite subset of Rn and ϵ1, . . . , ϵn be independent
Rademacher random variables. Let r = maxa∈A ∥a∥2. Then, we have,
E

supa∈A
1
n
n

i=1
ϵiai

≤r

2 log |A|
n
.
Therefore, for any ﬁnite class F consisting of functions bounded by 1,
Rn(F) ≤

2 log |F|
n
.
Since the right hand side is not random, the same bound holds for Rn(F) as well.
The Rademacher complexity, and its empirical counterpart, satisfy a number of interesting structural
properties.
Theorem 9 (Structural Properties of Rademacher Complexity).
Let F, F1, . . . , Fk and H be
classes of real valued functions. Then Rn(·) (as well as Rn(·)) satisﬁes the following properties:
Monotonicity: If F ⊆H, Rn(F) ≤Rn(H).
Convex Hull: Rn(conv(F)) = Rn(F).
Scaling: Rn(cF) = |c|Rn(F) for any c ∈R.
Contraction: If φ : R →R is Lipschitz with constant Lφ, then Rn(φ ◦F)≤LφRn(F).
Translation: For any bounded function h, Rn(F + h) = Rn(F).
Subadditivity: Rn
 k
i=1 Fi

≤n
i=1 Rn(Fi).
For proofs of these properties, see [25]. However, note that the Rademacher complexity is deﬁned
there such that an extra absolute value is taken before the supremum over f ∈F. That is,
Rabs
n
(F) = E

sup f ∈F

1
n
n

i=1
f (Xi)


.
Nevertheless, the proofs given there generalize quite easily to deﬁnition of Rademacher complexity
that we are using. For example, see [26], where the Rademacher complexity as we have deﬁned it here
(without the absolute value) is called free Rademacher complexity.
Using these properties, it is possible to directly bound the Rademacher complexity of several inter-
esting functions classes. See [25] for examples. The contraction property is a very useful one. It allows
us to transition from the Rademacher complexity of the loss class to the Rademacher complexity of the
function class itself (for Lipschitz losses).
The Rademacher complexity can also be bounded in terms of another measure of complexity of a
function class: namely covering numbers.
1.14.3.3 Covering numbers
The idea underlying covering numbers is very intuitive. Let (D, ρ) be any (pseudo-) metric space and
ﬁx a subset T ⊆D. A set T ′ ⊆D is said to be an α-cover of T if
∀f ∈T , ∃g ∈T ′s.t. ρ( f , g) ≤α.

1.14.3 Uniform Convergence of Empirical Means
787
In other words “balls” of radius α placed at elements of T ′ “cover” the set T entirely. The covering
number (at scale α) of T is deﬁned as the size of the smallest cover of T (at scale α).
N(T , ρ, α) = min {|T ′| : T ′ is an α-cover of T }.
The logarithm of covering number is called entropy.
Given a sample x1:n = (x1, . . . , xn), deﬁne the data dependent metric on F as
ρp( f , g) =
	
1
n
n

i=1
| f (xi) −g(xi)|p

1/p
,
where p ∈[1, ∞). Taking the limit p →∞suggests the metric
ρ∞( f , g) = max
i∈[n] | f (xi) −g(xi)|.
These metrics gives us the p-norm covering numbers
Np(F, x1:n, α) = N(F, ρ, α).
These covering numbers increase with p. That is, for p ∈[1, ∞],
N1(F, x1:n, α) ≤Np(F, x1:n, α) ≤N∞(F, x1:n, α).
Finally, we can also deﬁne the worst case (over samples) p-norm covering number
Np(F, n, α) =
sup
x1:n∈X n Np(F, x1:n, α).
A major result connecting Rademacher complexity with covering numbers is due to [27].
Theorem 10 (Dudley’s entropy integral bound).
For any F consisting of real valued functions
bounded by 1, we have
Rn(F) ≤α + 12
 1
α

log N2(F, X1:n, β)
n
dβ.
Therefore, using Jensen’s inequality, we have
Rn(F) ≤α + 12
 1
α

log EN2(F, X1:n, β)
n
dβ.
1.14.3.4 Binary valued functions: VC dimension
For binary function classes F ⊆{±1}X , we can provide distribution free upper bounds on the
Rademacher complexity or covering numbers in terms of a combinatorial parameter called the
Vapnik-Chervonenkis (VC) dimension.

788
CHAPTER 14 Learning Theory
To deﬁne it, consider a sequence x1:n = (x1, . . . , xn). We say that x1:n is shattered by F if each of
the 2n possible ±1 labelings of these n points are realizable using some f ∈F. That is, x1:n is shattered
by F iff
|{( f (x1), . . . , f (xn)) : f ∈F}| = 2n.
The VC dimension of F is the length of the longest sequence that can shattered by F:
VCdim(F) = max {n : ∃x1:n ∈X ns.t. x1:n is shattered by F}.
The size of the restriction of F to x1:n is called the shatter coefﬁcient:
S(F, x1:n) = |{( f (x1), . . . , f (xn)) : f ∈F}|.
Considering the worst sequence gives us the growth function
S(F, n) =
max
x1:n∈X n S(F, x1:n).
If VCdim(F) = d, then we know that S(F, n) = 2n for all n ≤d. One can ask: what is the behavior of
the growth function for n > d? The following combinatorial lemma proved independently by Vapnik
and Chervonenkis [28], Sauer [29], and Shelah [30], gives the answer to this question.
Lemma 11.
If VCdim(F) = d, then we have, for any n ∈N,
S(F, n) =
d

i=0
n
i

.
In particular, if d > 2, S(F, n) ≤nd.
Thus, for a class with ﬁnite VC dimension, the Rademacher complexity can be bounded directly
using Massart’s lemma.
Theorem 12.
Suppose 2 < VCdim(F) = d < ∞. Then, we have,
Rn(F) ≤

2 log S(F, n)
n
≤

2d log n
n
.
It is possible to shave off the log n factor in the bound above plugging an estimate of the entropy
log N2(F, n, α) for VC classes due to [31] into Theorem 10.
Theorem 13.
Suppose VCdim(F) = d < ∞. We have, for any n ∈N,
N2(F, n, α) ≤Cd log 1
α
for a universal constant C.
These results show that for a class F with ﬁnite VC dimension d, the sample complexity required by
ERM to achieve ε excess risk relative to f ⋆
F is, with probability at least 1−δ, O( d
ε2 +log 1
δ ). Therefore,
a ﬁnite VC dimension is sufﬁcient for learnability with respect to a ﬁxed functions class. In fact, a ﬁnite
VC dimension also turns out to be necessary (see, for instance, [14, Chapter 14]). Thus, we have the
following characterization.

1.14.3 Uniform Convergence of Empirical Means
789
Theorem 14.
In the binary classiﬁcation setting with 0–1 loss, there is a learning algorithm with
bounded sample complexity for every ε, δ ∈(0, 1) iff VCdim(F) < ∞.
Note that, when VCdim(F) < ∞, the dependence of the sample complexity on 1/ε is polynomial
and on 1/δ is logarithmic. Moreover, such a sample complexity is achieved by ERM. On the other
hand, when VCdim(F) = ∞, then no learning algorithm, including ERM, can have bounded sample
complexity for arbitrarily small ε, δ.
1.14.3.5 Real valued functions: fat shattering dimension
For real valued functions, getting distribution free upper bounds on Rademacher complexity or covering
numbers involves combinatorial parameters similar to the VC dimension. The appropriate notion for
determining learnability using a ﬁnite number of samples turns out to be a scale sensitive combinatorial
dimension known as the fat shattering dimension.
Fix a class F consisting of bounded real valued functions f : X →[0, 1] and a scale α > 0. We
say that a sequence x1:n = (x1, . . . , xn) is α-shattered by F if there exists a witness sequence s1:n ∈Rn
such that, for every ϵ1:n ∈{±1}n, there is an f ∈F such that
ϵi( f (xi) −si) ≥α .
In words, this means that, for any of the 2n possibilities for the sign pattern ϵ1:n, we have a function
f ∈F whose values at the points xi is above or below si depending on whether ϵi is +1 or −1.
Moreover, the gap between f (xi) and si is required to be at least α.
The fat-shattering dimension of F at scale α is the size of the longest sequence that can be α-shattered
by F:
fatα(F) = max{n : ∃x1:n ∈X n s.t. x1:n is α-shattered by F}.
A bound on ∞-norm covering numbers in terms of the fat shattering dimension were ﬁrst given by
Alon et al. [32].
Theorem 15.
Fix a class F ⊆[0, 1]X . Then we have, for any α > 0,
N∞(F, n, α) ≤2
4n
α2
d log (en/dα)
,
where d = fatα/2(F).
Using this bound, the sample complexity of learning a real valued function class of bounded range
using a Lipschitz loss3 ℓis O( d
ε2 log 1
ε +log 1
δ ) where d = fatcε(F) for a universal constant c > 0. The
importance of the fat shattering dimension lies in the fact that if fatα(F) = ∞for some α > 0, then no
learning algorithm can have bounded sample complexity for learning the functions for arbitrarily small
values of ε, δ.
Theorem 16.
In the regression setting with either squared loss or absolute loss, there is a learning
algorithm with bounded sample complexity for every ε, δ ∈(0, 1) iff ∀α > 0, fatα(F) < ∞.
3By this we mean ℓ(·, y) is Lipschitz for all y.

790
CHAPTER 14 Learning Theory
Note that here, unlike the binary classiﬁcation case, the dependence of the sample complexity on
1/ε is not ﬁxed. It depends on how fast fatαF grows as α →0. In particular, if fatα(F) grows as 1/α p
then the sample complexity is polynomial in 1/ε (and logarithmic in 1/δ).
1.14.4 Model selection
In general, model selection refers to the task of selecting an appropriate model from a given family
based on available data. For ERM, the “model” is the class F of functions f : X →Y that are intended
to capture the functional dependence between the input variable X and the output Y. If we choose too
small an F then ERM will not have good performance if f ⋆is not even well approximated by any
member of F. One the other hand, if F is too large (say all functions) then the estimation error will be
so large that ERM will not perform well even if f ⋆∈F.
1.14.4.1 Structural risk minimization
Consider the classiﬁcation setting with 0–1 loss. Suppose we have a countable sequence of nested
function classes:
F(1) ⊂F(2) ⊂· · · ⊂F(k) · · · ,
where F(k) has VC dimension dk. The function f (k)
n
chosen by ERM over Fk satisﬁes the bound
R

f (k)
n

≤R

f (k)
n

+ O
	
dk log n
n

with high probability. As k increases, the classes become more complex. So, the empirical risk term
will decrease with k whereas the VC dimension term will increase. The principle of structural risk
minimization (SRM) chooses a value of k by minimizing the right hand side above.
Namely, we choose
k = argmink R

f (k)
n

+ C

dk log n
n
,
(14.3)
for some universal constant C.
It can be shown (see, for example, [14, Chapter 18]) that SRM leads to universally consistent rules
provided appropriate conditions are placed on the functions classes Fk.
Theorem 17 (Universal Consistency of SRM).
Let F(1), F(2), . . . be a sequence of classes such that
for any distribution P,
lim
k→∞R( f ⋆
F(k)) = R( f ⋆).
Assume also that VCdim(F(1)) < VCdim(F(2)) < . . . are all ﬁnite. Then the learning rule f (k)
n
with
k chosen as in (14.3) is strongly universally consistent.
The penalty term in (14.3) depends on the VC dimension of F(k). It turns out that it is not essential.
All we need is an upper bound on the risk of ERM in terms of the empirical risk and some measure of
the complexity of the function class. One could also imagine proving results for regression problems by

1.14.4 Model Selection
791
replacing the VC dimension with fat-shattering dimension. We can think of SRM as penalizing larger
classes by their capacity to overﬁt the data. However, the penalty in SRM is ﬁxed in advance and is not
data dependent. We will now look into more general penalties.
1.14.4.2 Model selection via penalization
Given a (not necessarily nested) countable or ﬁnite collection of models {F(k)}k and a penalty function
pen : N →R+, deﬁne k as the minimizer of
R

f (k)
n

+ pen(k).
Recall that f (k)
n
is the result of ERM over F(k). Also note that the penalty can additionally depend on
the data. Now deﬁne the penalized estimator as
˜fn = f (k)
n .
The following theorem shows that any probabilistic upper bound Un,k on the risk R( fn) can be used to
design a penalty for which a performance bound can be given.
Theorem 18.
Assume that there are positive numbers c and m such that for each k we have the
guarantee
∀ε > 0, P

R

f (k)
n

> Un,k + ε

≤c exp (−2mε2).
Then the penalized estimator ˜fn with
pen(k) = Un,k −R

f (k)
n

+

log k
m
satisﬁes
E[R( ˜fn)] −R( f ⋆) ≤min
k

E[pen(k)] +

R( f ⋆
F(k)) −R( f ⋆)

+

log (ce)
2m
.
Note that the result says that the penalized estimator achieves an almost optimal trade-off between
the approximation error R( f ⋆
F(k)) −R( f ⋆) and the expected complexity E[pen](k). For a good upper
bound Un,k, the expected complexity will be a good upper bound on the (expected) estimation error.
Therefore, the model selection properties of the penalized estimator depend on how good an upper
bound Un,k we can ﬁnd.
Note that structural risk minimization corresponds to using a distribution free upper bound
Un,k = R

f (k)
n

+ O
	
dklogn
n

.
The looseness of these distribution free upper bounds has prompted researchers to design data dependent
penalties. We shall see an examples based on empirical Rademacher averages in the next section.

792
CHAPTER 14 Learning Theory
Proof.
For any ε > 0,
P

R( ˜fn) −(R( ˜fn) + pen(k)) > ε

≤P

∃k s.t. R( f (k)
n ) −(R( f (k)
n ) + pen(k)) > ε

≤
∞

k=1
P

R( f (k)
n ) −(R( f (k)
n ) + pen(k)) > ε

=
∞

k=1
P

R( f (k)
n ) −Un,k >

log k
m
+ ε

≤
∞

k=1
c exp

−2m(ε +

log k/m)2
≤
∞

k=1
c exp

−2m(ε2 + log k/m)

= c exp ( −2nε2)
∞

k=1
1
k2 ≤2c exp ( −2mε2).
By integrating the ﬁnal upper bound with respect to ε, we get the bound
E

R( ˜fn) −(R( ˜fn) + pen(k))

≤

log (ce)
2m
.
(14.4)
Also, for any k, we have
E

R( ˜fn) + pen(k) −R( f ⋆
F(k))

≤E

R( f (k)
n ) + pen(k) −R( f ⋆
F(k))

≤E
R( f ⋆
F(k)) + pen(k) −R( f ⋆
F(k))

= E

pen(k)

.
The ﬁrst inequality above follows by deﬁnition of k and ˜fn. The second holds because f (k)
n
minimizes
the empirical risk over F(k). Summing this with (14.4), we get, for any k,
E

R( ˜fn)

≤E[pen(k)] + R( f ⋆
F(k)) +

log (ce)
2m
.
Now subtracting R( f ⋆) from both sides and minimizing over k proves the theorem.
□
1.14.4.3 Data driven penalties using Rademacher averages
The results of Section 1.14.3.2 tell us that there exists universal constants C, c such that
P[R( fn) > R( fn) + 2Rn(F(k)) + ε] < c exp ( −2Cnε2).

1.14.5 Alternatives to Uniform Convergence
793
Thus, we can use the data dependent penalty
pen(k) = 2Rn(F(k)) +

log k
Cn
in Theorem 18 to get the bound
E[R( ˜fn)] −R( f ⋆) ≤min
k

2Rn(F(k)) +

R( f ⋆
F(k)) −R( f ⋆)

+

log k
Cn

+

log (ce)
2Cn
.
Note that the penalty here is data dependent. Moreover, except for the

log k
Cn
term, the penalized
estimator makes the optimal trade-off between the Rademacher complexity, an upper bound on the
estimation error, and the approximation error.
1.14.4.4 Hold-out estimates
Suppose out of m + n iid samples we set aside m samples where m is much smaller than n. Since the
hold-out set of size m is not used in the computation of f (k)
n , we can estimate R( f (k)
n ) by the hold-out
estimate
Un,k =
m

i=1
ℓ

f (k)
n (Xn+i), Yn+i

.
By Hoeffding’s inequality, the assumption of Theorem 18 holds with c = 1. Moreover E

Un,k

=
R( f (k)
n ). Therefore, we have
E

R( ˜fn)

−R( f ⋆) ≤min
k

E

R( f (k)
n ) −R( f (k)
n )

+

R( f ⋆
F(k)) −R( f ⋆)

+

log k
m

+

1
2m .
1.14.5 Alternatives to uniform convergence
The approach of deriving generalization bounds using uniform convergence arguments is not the only
one possible. In this section, we review three techniques for proving generalization bounds based on
sample compression, algorithmic stability, and PAC-Bayesian analysis respectively.
1.14.5.1 Sample compression
The sample compression approach to proving generalization bounds was introduced by [33]. It applies
to algorithms whose output fn,S (that is learned using some training set S) can be reconstructed from a
compressed subset Si. Here i is a sequence of indices
i = (i1, i2, . . . , ik), 1 ≤i1 < i2 < . . . < ik ≤n,

794
CHAPTER 14 Learning Theory
and Si is simply the subsequence of S indexed by i. For the index sequence i above, we say that its length
|i| is k. To formalize the idea that the algorithms output can be reconstructed from a small number of
examples, we will deﬁne a compression function
C :
∞
 
n=1
(X × Y)n →I,
where I is the set of all possible index sequences of ﬁnite length. It is assumed that |C(S)| ≤n for an
S is of size n. We also deﬁne a reconstruction function
R :
∞
 
n=1
(X × Y)n →YX .
We assume that the learning rule satisﬁes, for any S,
fn,S = R(SC(S)).
(14.5)
This formalizes our intuition that to reconstruct fn,S it sufﬁces to remember only those examples whose
indices are given by C(S).
The following result gives a generalization bound for such an algorithm.
Theorem 19.
Let fn be a learning rule for which there exists compression and reconstructions
functions C and R respectively such that the equality (14.5) holds for all S. Then, we have, with
probability at least 1 −δ,
R( fn) ≤
n
n −k
R( fn) +
!
"
"
"
#log
n
k

+ log n
δ
2(n −k)
,
where k = |C(S)|.
Proof.
The proof uses the simple idea that for a ﬁxed index set i, S¯i is a sample of size n −|i| that
is independent of Si. Here ¯i denotes the sequence [n]\i. By this independence, the risk of any function
that depends only on Si is close to its empirical average on S¯i by Hoeffding’s inequality.
Denote the empirical risk of f calculated on a subset Si of S by Ri( f ). We have, for any f,
RC(S)( f ) ≤
n
n −k
R( f ),
where k = |C(S)|. Thus, all we need to show is
P

R( fn) −RC(S)( fn) ≥εn,|C(S)|

≤δ,
where
εn,k =
!
"
"
"
#log
n
k

+ log n
δ
2(n −k)

1.14.5 Alternatives to Uniform Convergence
795
To show this, we proceed as follows,
P

R( fn) −RC(S)( fn) ≥εn,|C(S)|

= P

R(R(SC(S))) −RC(S)(R(SC(S))) ≥εn,|C(S)|

≤P

∃i s.t. |i| = k, R(R(Si)) −R¯i(R(Si)) ≥εn,k

≤

i:|i|=k
P

R(R(Si) −R¯i(R(Si)) ≥εn,k

≤
n

k=1

i:|i|=k
exp (−2(n −k)ε2
n,k)
=
n

k=1
n
k

exp (−2(n −k)ε2
n,k)
≤
n

k=1
δ
n = δ.
The last step holds because, by our choice of εn,k,
n
k

exp (−2(n −k)ε2
n,k) = δ
n .
□
1.14.5.2 Stability
Stability is property of a learning algorithm that ensures that the output, or some aspect of the output,
of the learning algorithm does not change much if the training data set changes very slightly. This is an
intuitive notion whose roots in learning theory go back to the work of Tikhonov on regularization of
inverse problems. To see how stability can be used to give generalization bounds, let us consider a result
of Bousquet and Elisseeff [34] that avoids going through uniform convergence arguments by directly
showing that any learning algorithm with uniform stability enjoys exponential tail generalization bounds
in terms of both the training error and the leave-one-out error of the algorithm.
Let S\i denote the sample of size n−1 obtained by removing a particular input (xi, yi) from a sample
S of size n. A learning algorithm is said to have uniform stability β if for any S ∈(X × Y)n and any
i ∈[n], we have,
sup
(x,y)∈X ×Y
|ℓ( fn,S(x), y) −ℓ( fn−1,S\i (x), y)| ≤β.
We have the following generalization bound for a uniformly stable learning algorithm.
Theorem 20.
Let fn have uniform stability β. Then, with probability at least 1 −δ,
R( fn) ≤R( fn) + 2β + (4βn + 1)
$
log 1
δ
2n .

796
CHAPTER 14 Learning Theory
Note that this theorem gives tight bound when β scales as 1/n. As such it cannot be applied directly
to the classiﬁcation setting with 0–1 loss where β can only be 0 or 1. A good example of a learning
algorithm that exhibits uniform stability is regularized ERM over a reproducing kernel Hilbert space
using a convex loss function that has Lipschitz constant Cℓ:
∀y, y′, y′′ ∈Y ⊆R, |ℓ(y′, y) −ℓ(y′′, y)| ≤Cℓ· |y′ −y′′|.
A regularized ERM using kernel K is deﬁned by
fn = argmin
f ∈FK
R( f ) + λ|| f ||2
K ,
where λ is a regularization parameter. Regularized ERM has uniform stability β for β = C2
ℓκ2
2λn , where
κ = sup
x∈X

K(x, x).
The literature on stability of learning algorithms is unfortunately thick with deﬁnitions. For instance,
Kutin and Niyogi [35] alone consider over a dozen variants! We chose a simple (but quite strong)
deﬁnition from Bousquet and Elisseeff above to illustrate the general point that stability can be used to
derive generalization bounds. But we like to point the reader to Mukherjee et al. [36] where a notion
of leave-one-out (LOO) stability is deﬁned and shown sufﬁcient for generalization for any learning
algorithm. Moreover, for ERM they show it is necessary and sufﬁcient for both generalization and
learnability with respect to a ﬁxed class. Shalev-Shwartz et al. [37] further examine the connections
between stability, generalization and learnability in Vapnik’s general learning setting that includes
supervised learning (learning from example, label pairs) as a special case.
1.14.5.3 PAC-Bayesian analysis
PAC-Bayesian analysis refers to a style of analysis of learning algorithms that output not just a single
function f ∈F but rather a distribution (called a “posterior distribution”) over F. Moreover, the
complexity of the data-dependent posterior distribution is measured relative to a ﬁxed distribution chosen
in advance (that is, in a data independent way). The ﬁxed distribution is called a “prior distribution.”
Note that the terms “prior” and “posterior” are borrowed from the analysis of Bayesian algorithms
that start with a prior distribution on some parameter space and, on seeing data, make updates to the
distribution using Bayes’ rule to obtain a posterior distribution. Even though the terms are borrowed,
their use is not required to be similar to their use in Bayesian analysis. In particular, the prior used in
the PAC-Bayes theorem need not be “true” in any sense and the posterior need not be obtained using
Bayesian updates: the bound holds for any choice of the prior and posterior.
Denote the space of probability distributions over F by (F). There might be measurability issues
in deﬁning this for general function classes. So, we can assume that F is a countable class for the
purpose of the theorem below. Note, however, that the cardinality of the class F never enters the bound
explicitly anywhere. For any ρ ∈(F), deﬁne
R(ρ) = E f ∼ρ[R( f )],
R(ρ) = E f ∼ρ[R( f )].

1.14.5 Alternatives to Uniform Convergence
797
In the context of classiﬁcation, a randomized classiﬁer that, when asked for a prediction, samples a
function from a distribution ρ and uses if for classiﬁcation, is called a Gibbs classiﬁer. Note that R(ρ)
is the risk of such a Gibbs classiﬁer.
Also recall the deﬁnition of Kullback-Leibler divergence or relative entropy:
D(ρ||π) =

f ∈F
log ρ( f )
π( f )dρ( f ).
For real numbers p, q ∈[0, 1] we deﬁne (with some abuse of notation):
D(p||q) = p log p
q + (1 −p) log 1 −p
1 −q .
With these deﬁnitions, we can now state the PAC Bayesian theorem [38].
Theorem 21 (PAC-Bayesian Theorem).
Let π be a ﬁxed distribution over F. Then, we have, with
probability at least 1 −δ,
∀ρ ∈(F), D
R(ρ)||R(ρ)

≤D(ρ||π) + log 2n
δ
n −1
.
In particular, for any learning algorithm that, instead of returning a single function fn ∈F, returns a
distribution ρn over F, we have, with probability at least 1 −δ,
D
R(ρn)||R(ρn)

≤D(ρn||π) + log 2n
δ
n −1
.
To get more interpretable bounds from this statement, the following inequality is useful for 0 ≤p <
q ≤1:
(p −q)2
2
≤(p −q)2
2q
≤D(p||q).
This implies that if D(p||q) ≤x then two inequalities hold:
q ≤p +
√
2x,
q ≤p +

2px + 2x.
The ﬁrst gives us a version of the PAC-Bayesian bound that is often presented:
R(ρn) −R(ρn) ≤
$
2(D(ρn||π) + log 2n
δ )
n −1
.
The second gives us the interesting bound
R(ρn) −R(ρn) ≤
$
2R(ρn)(D(ρn||π) + log 2n
δ )
n −1
+ 2 D(ρn||π) + log 2n
δ
n −1
.
If the loss R(ρn) is close to zero, then the dominating term is the second term that scales as 1
n . If not,
then the ﬁrst term, which scales as
1
√n , dominates.

798
CHAPTER 14 Learning Theory
1.14.6 Computational aspects
So far we have only talked about sample complexity issues ignoring considerations of computational
complexity. To properly address the latter, we need a formal model of computation and need to talk about
how functions are represented by the learning algorithm. The branch of machine learning that studies
these questions is called computational learning theory. We will now provide a brief introduction to
this area.
1.14.6.1 The PAC model
The basic model in computational learning theory is the Probably Approximately Correct (PAC) model.
It applies to learning binary valued functions and uses the 0–1 loss. Since a ±1 valued function is
speciﬁed unambiguously be specifying the subset of X where it is one, we have a bijection between
binary valued functions and concepts, which are simply subsets of X. Thus we will use (binary valued)
function and concept interchangeably in this section. Moreover, the basic PAC model considers what is
sometimes called the realizable case. That is, there is a “true" concept in F generating the data. This
means that yi = f (xi) for some f ∈F. Hence the minimal risk in the class is zero: R( f ⋆
F) = 0. Note,
however, that the distribution over X is still assumed to be arbitrary.
To deﬁne the computational complexity of a learning algorithm, we assume that X is either {0, 1}d
or Rd. We assume a model of computation that can store a single real number in a memory location and
can perform any basic arithmetic operation (additional, subtraction, multiplication, division) on two
real numbers in one unit of computation time.
The distinction between a concept and its representation is also an important one when we study
computational complexity. For instance, if our concepts are convex polytopes in Rd, we may choose
a representation scheme that speciﬁes the vertices of the polytope. Alternatively, we may choose to
specify the linear equalities describing the faces of the polytope. Their vertex-based and face-based
representations can differ exponentially in size. Formally, a representation scheme is a mapping rep :
( ∪R)⋆→F that maps a representation (consisting of symbols from a ﬁnite alphabet  and real
numbers) to a concept. As noted above, there could be many σ’s such that rep(σ) = f for a given
concept f. The size of representation is assumed to be given by a function size : ( ∪R)⋆→N.
A simple choice of size could be simply the number of symbols and real numbers that appear in the
representation. Finally, the size of a concept f is simply the size of its smallest representation:
size( f ) =
min
σ:rep(σ)= f size(σ).
The last ingredient we need before can give the deﬁnition of efﬁcient learnability in the PAC model
is the representation class H used by the algorithm for producing its output. We call H the hypothesis
class and it is assumed that there is a representation in H for every function in F. Moreover, it is required
that for every x ∈X and any h ∈H, we can evaluate h(x) ( = rep(h)(x)) in time polynomial in d and
size(h).
Recall that we have assumed X = {0, 1}d or Rd. We say that an algorithm efﬁciently PAC learns
F using hypothesis class H if, for any f ∈F, given access to iid examples (xi, f (xi)) ∈X × {±1},

1.14.6 Computational Aspects
799
inputs ε ∈(0, 1) (accuracy), δ ∈(0, 1) (conﬁdence), and size( f ), the algorithm satisﬁes the following
conditions:
1. With probability at least 1 −δ, it outputs a hypothesis h ∈H with R(h) ≤ε.
2. It runs in time polynomial in 1
ε, 1
δ , size( f ), and d.
For examples of classes F that are efﬁciently PAC learnable (using some H), see the texts [12,18,20].
1.14.6.2 Weak and strong learning
In the PAC learning deﬁnition, the algorithm must be able to achieve arbitrarily small values of ε and δ.
A weaker deﬁnition would require the learning algorithm to work only for ﬁxed choices of ε and δ. We
can modify the deﬁnition of PAC learning by removing ε and δ from the input to the algorithm. Instead,
we assume that there are ﬁxed polynomials pδ(·, ·) and pε(·, ·) such that the algorithm outputs h ∈H
that satisﬁes
R(h) ≤1
2 −
1
pε(d, size( f ))
with probability at least
1
pδ(d, size( f )).
Such an algorithm is called a weak PAC learning algorithm for F. The original deﬁnition, in contrast,
can be called a deﬁnition of strong PAC learning. The deﬁnition of weak PAC learning does appear quite
weak. Recall that an accuracy of 1
2 can be trivially achieved by an algorithm that does not even look
at the samples but simply outputs a hypothesis that outputs a random ±1 sign (each with probability
a half) for any input. The desired accuracy above is just an inverse polynomial away from the trivial
accuracy guarantee of 1
2. Moreover, the probability with which the weak accuracy is achieved is not
required to be arbitrarily close to 1.
A natural question to ask is whether a class that is weakly PAC learnable using H is also strongly
learnable using H? The answer, somewhat surprisingly, is “Yes.” So, the weak PAC learning deﬁnition
only appears to be a relaxed version of strong PAC learning. Moreover, a weak PAC learning algorithm,
given as a subroutine or blackbox, can be converted into a strong PAC learning algorithm. Such a
conversion is called boosting.
A boosting procedure has to start with a weak learning algorithm and boost two things: the conﬁdence
and the accuracy. It turns out that boosting the conﬁdence is easier. The basic idea is to run the weak
learning algorithm several times on independent samples. Therefore, the crux of the boosting procedure
lies in boosting the accuracy. Boosting the accuracy seems hard until one realizes that the weak learning
algorithm does have one strong property: it is guaranteed to work no matter what the underlying
distribution of the examples is. Thus, if a ﬁrst run of the weak learning only produces a hypothesis
h1 with slightly better accuracy than 1
2, we can make the second run focus only on those examples
where h1 makes a mistake. Thus, we will focus the weak learning algorithm on “harder portions” of
the input distribution. While this simple idea does not directly work, a variation on the same theme
was shown to work by Schapire [39]. Freund [40] then presented a simpler boosting algorithm. Finally,
a much more practical boosting procedure, called AdaBoost, was discovered by them jointly [10].

800
CHAPTER 14 Learning Theory
AdaBoost counts as one of the great practical success stories of computational learning theory. For
more details on boosting, AdaBoost, and the intriguing connections of these ideas to game theory and
statistics, we refer the reader to [41].
1.14.6.3 Random classiﬁcation noise and the SQ model
Recall that the basic PAC model assumes that the labels are generated using a function f belonging
to the class F under consideration. Our earlier development of statistical tools for obtaining sampling
complexity estimates did not require such an assumptions (though the rates did depend on whether
R( f ⋆
F) = 0 or not).
There are various noise models that extend the PAC model by relaxing the noise-free assumption and
thus making the problem of learning potentially harder. Perhaps the simplest is the random classiﬁcation
noise model [42]. In this model, when the learning algorithm queries for the next labeled example of
the form (xi, f (xi)), it receives it with probability 1−η but with probability η, it receives (xi, −f (xi)).
That is, with some probability η < 1
2, the label is ﬂipped. The deﬁnition of efﬁcient learnability remains
the same except that now the learning algorithm has to run in time polynomial in 1
ε, 1
δ , d, size( f ) and
1
1−2η0 where η0 < 1
2 is an input to the algorithm and is an upper bound on η.
Kearns [43] introduced the statistical query (SQ) model where learning algorithms do not directly
access the labeled example but only make statistical queries about the underlying distribution. The
queries take the form (χ, τ) where χ : X × {±1} →{±1} is a binary valued function of the labeled
examplesandτ ∈(0, 1]isatoleranceparameter.Givensuchaquery,theoracleintheSQmodelresponds
with an estimate of P(χ(x, f (x)) = +1) that is accurate to within an additive error of τ. It is easy to
see, using Hoeffding’s inequality, that given access to the usual PAC oracle that provides iid examples
of the form (xi, f (xi)), we can simulate the SQ oracle by just drawing a sample of size polynomial
in
1
τ 2 and log 1
δ and estimating the probability P(χ(x, f (x)) = 1) based on it. The simulation with
succeed with probability 1 −δ. Kearns gave a more complicated simulation that mimics the SQ oracle
given access to only a PAC oracle with random misclassiﬁcation noise.
Lemma 22.
Given a query (χ, τ), the probability P(χ(x, f (x)) = 1) can be estimated to within τ
error, with probability at least 1 −δ, using
O

1
τ 2(1 −2η)2 log 1
δ

examples that have random misclassiﬁcation noise at rate η < 1
2 in them.
This lemma means that learning algorithm that learns a concept class F in the SQ model can also
learn F in the random classiﬁcation noise model.
Theorem 23.
If there is an algorithm that efﬁciently learns F from statistical queries using H, then
there is an algorithm that efﬁciently learns F using H in the random classiﬁcation noise model.
1.14.6.4 The agnostic PAC model
The agnostic PAC model uses the same deﬁnition of learning as the one we used in Section 1.14.2.3).
That is, the distribution of examples (x, y) is arbitrary. In particular, it is not assumed that y = f (x) for

1.14.6 Computational Aspects
801
any f. The goal of the learning algorithm is to output a hypothesis h ∈H with R(h) −R( f ⋆
F) ≤ε with
probability at least 1 −δ. We say that a learning algorithm efﬁciently learns F using H in the agnostic
PAC model if the algorithm not only outputs a probably approximately correct hypothesis h but also
runs in time that is polynomial in 1
ε, 1
δ , and d.
The agnostic model has proved to be a very difﬁcult one for exhibiting efﬁcient learning algorithms.
Note that the statistical issue is completely resolved: a class F is learnable iff it has ﬁnite VC dimension.
However, the “algorithm" implicit in this statement is ERM which, even for the class of halfspaces
{x →sign(⟨w, x⟩−θ) : w ∈Rd, θ ∈R},
leads to NP-hard problems. For instance, see [44] for very strong negative results about agnostic learning
of halfspaces. However, these results apply to proper agnostic learning only. That is, the algorithm
outputs a hypothesis that is also a halfspace. An efﬁcient algorithm for learning halfspaces in the
agnostic PAC model using a general hypothesis class would be a major breakthrough.
1.14.6.5 The mistake bound model
The mistake bound model is quite different from the PAC model. Unlike the PAC model, no assumption
is made on the distribution of the inputs. There is also no distinction between a training phase where
the learner uses a batch of examples to learn a hypothesis and a test phase where the risk of the learned
hypothesis determines the learner’s expected loss. Instead, learning happens in a series of trials (or
rounds) in an online fashion. At any given round t ∈N, the order of events is as follows:
•
Learner receives xt ∈X where X = {0, 1}d or Rd.
•
Learner outputs a label yt ∈Y.
•
Learner receives the correct label yt = f (xt).
Note that the basic model assumes that is a true function f generating the labels (the case when such
an f is assumed to exist is often called the realizable case). The identity of this function is, of course,
hidden from the learner. The goal of the learner is to minimize the total number of mistakes it makes:
∞

t=1
ℓ(yt, f (xt)),
where ℓis the 0–1 loss. If a learning algorithm can guarantee, for any choice f ∈F of the true concept,
that the total mistake bound will be a polynomial function of d and size( f ) as well as the computation
per round is polynomial in the same parameters, then we say that the algorithm efﬁciently learns F in
the mistake bound model.
Recall that, in the PAC model, VC dimension of F characterizes learnability of F if we ignore
computational considerations. Moreover, VC dimension characterizes learnability in the agnostic PAC
model as well. In the mistake bound model, it is the Littlestone dimension [45] whose ﬁniteness provides
a necessary and sufﬁcient condition for learnability. There is also an agnostic version of the mistake
bound model where we drop the assumption that yt = f (xt) for some f ∈F. It is also known that
the Littlestone dimension characterizes learnability (ignoring efﬁciency) even in the agnostic online
mistake bound model [46].

802
CHAPTER 14 Learning Theory
The Littlestone dimension of a class F ⊆{±1}X is deﬁned as follows. A X-valued tree (or simply an
X-tree) of height t is a complete binary tree of height t whose internal nodes are labeled with instances
from X. A (complete binary) tree of height t has exactly 2t leaves which we identify, from left to right,
with the 2t sequences length t ±1 sequences ordered lexicographically (with −1 taking precedence
over +1 in determining the lexicographic order). For example, the leftmost leaf corresponds to all −1’s
sequence whereas the rightmost leaf corresponds to the all +1’s sequences. If λ is a leaf, we denote its
associated ±1 sequence by ϵ(λ).
An X-tree of height t is said to be shattered by F if for each of the 2t leaves the following is true:
there is a function f ∈F such that the sequence of values taken by f on the internal nodes on the path
from the root to the leaf λ is exactly ϵ(λ). The Littlestone dimension of F is the height of the tallest tree
that is shattered by F.
It is easy to see that if an X-valued sequence (x1, . . . , xt) is shattered by F then the X-tree of height
t obtained by repeating xi across level i for i ∈[t] is also shattered by F. This proves the following
relationship between VCdim(F) and Ldim(F).
Theorem 24.
For any F ⊆{±1}X we have
VCdim(F) ≤Ldim(F).
A converse to the above theorem is not possible. Indeed, the class of threshold functions on R:
F = {x →sign(x −θ) : θ ∈R}
has VCdim(F) = 1 but Ldim(F) = ∞. A ﬁnite class F has a ﬁnite Littlestone dimension satisfying
the upper bound Ldim(F) ≤log2 (|F|).
1.14.6.5.1
Halving and weighted majority
Note that we proved risk bounds for a ﬁnite class in the probabilistic framework using Hoeffding’s
inequality along with a simple union bound. Here we show how to deal with a ﬁnite class in the online
mistake bound model. In the realizable case, a simple algorithm called halving is guaranteed to make
no more than log2 (|F|) mistakes.
At the beginning of round t, the halving algorithm considers a subset Ft ⊆F of the original function
class. Predictions are made by taking a majority vote over Ft−1:
yt = sign
⎛
⎝
f ∈Ft−1
f (xt)
⎞
⎠.
Therefore, if a mistake is made, then |Ft−1| reduces by at least a factor of 2. Moreover, the true function
is never eliminated since it will always satisfy f (xt) = yt. Therefore, if Mt is the number of mistakes
made by the halving algorithm in t rounds, then we have
1 ≤|Ft| ≤|F0|
2Mt = |F|
2Mt .
This implies that Mt ≤log2 (|F|). Thus we have proved a mistake bound for halving.

1.14.6 Computational Aspects
803
Algorithm 1. Halving
Initialize F0 = F
for t = 1, 2, . . . do
Receive xt
Predict yt using a majority vote over Ft−1
Receive true label yt
if y ̸= yt then
Ft = { f ∈Ft−1 : f (xt) = yt}
end if
end for
Theorem 25.
The halving algorithm when run on a ﬁnite class F enjoys a mistake bound of log2 (|F|).
In the non-realizable case, halving does not work directly but a suitable modiﬁcation does. Since no
function in the class can be guaranteed to be correct all the time, it does not make sense to eliminate
functions that make mistakes. The crucial idea here is to keep weights for different functions in the
class. Then, when a functions makes a mistake, it is not eliminated but its weight is multiplied by a
factor of e−η < 1 where η > 0 is a parameter. The prediction are now taken using a weighted majority
vote over all F:
yt = sign
⎛
⎝
f ∈F
ρt−1( f ) f (xt)
⎞
⎠.
(14.6)
Algorithm 2. Weighted Majority
Initialize ρ0( f ) = π( f ) for some distribution π over F
for t = 1, 2, . . . do
Receive xt
Predict yt using a ρt−1-weighted majority vote over Ft−1
Receive true label yt
for f ∈F do
ρt( f ) = ρt−1( f )exp( −ηℓ( f (xt), yt))
end for
end for
Theorem 26.
At any the end of any given round, let M f be the number of mistakes made by a function
f ∈F so far. Then the number of mistakes M made by weighted majority so far is bounded as
∀f ∈F, M ≤2 ·
ηM f + log
1
π( f )
1 −e−η
.

804
CHAPTER 14 Learning Theory
In particular, using the uniform distribution π( f ) = 1/|F|, we get
∀f ∈F, M ≤2 · ηM f + log |F|
1 −e−η
.
Proof.
Fix a round index T ≥1 and let M and M f be deﬁned as the numbers of mistakes in the ﬁrst
T rounds. Let Ft be the fraction of weights on the functions that made a mistake on round t ≤T . Since
we make a mistake if and only if Ft ≥1
2, we have
M =
T

t=1
1
)
Ft ≥1
2
*
≤
T

t=1
2Ft.
(14.7)
Deﬁne the total weight
Wt =

f ∈F
ρt( f ).
At round t, the total weight changes as
Wt+1 ≤(1 −(1 −e−η)Ft)Wt.
Hence the ﬁnal total weight is
WT +1 = W0
T+
t=1
(1 −(1 −e−η)Ft).
Note that W0 = 1. If the function f made M f mistakes, its weight is
ρt+1( f ) = π( f )e−ηM f .
Since ρt+1( f ) ≤Wt+1, we have
π( f )e−ηM f ≤
T+
t=1
(1 −(1 −e−η)Ft).
Taking negative logs of both sides reverses the inequality giving
log
1
π( f ) + ηM f ≥−
T

t=1
log (1 −(1 −e−η)Ft).
Using the elementary but useful inequality −log (1 −x) > x gives
(1 −e−η)
T

t=1
Ft ≤ηM f + log
1
π( f ).
Combining this with the bound (14.7) proves the theorem.
□

1.14.6 Computational Aspects
805
It turns out we can shave off a factor of 2 in the above mistake bound by using a randomized version
of the weighted majority algorithm. Instead of taking the majority vote with the weights ρt−1( f ), we can
consider a Gibbs classiﬁer that chooses a function ft randomly from F with probability proportional
to ρt−1( f ) and then outputs yt = ft(xt). This makes the number of mistakes a random variable but
the bound (14.7) turns into an equality without the factor of 2. That is, for the randomized weighted
majority algorithm
E[M] =
T

t=1
Ft.
Thus we have the following result.
Theorem 27.
Suppose that we change the weighted majority step (14.6) to the randomized weighted
majority step
ft ∼ρt−1, yt = sign( ft(xt)).
Then, this randomized version of weighted majority satisﬁes the expected mistake bound
∀f ∈F, E[M] ≤
ηM f + log
1
π( f )
1 −e−η
.
1.14.6.5.2
Perceptron and winnow
We now consider two online algorithms that learn a linear threshold function. Perceptron is a classic
algorithm that dates back to the 1960s. The Winnow algorithm was proposed by Littlestone [45] as a
better alternative when there are many irrelevant features in the inputs.
Both algorithms follow the general schema given in Algorithm 3. Perceptron uses the additive update
rule:
LTF-Update(wt−1, η, xt, yt) = wt−1 −ηytxt,
whereas Winnow uses a multiplicative update rule
LTF-Update(wt−1, η, xt, yt) = wt−1 ⊙exp(−ηytxt)
Zt
,
where ⊙and exp denote entry wise multiplication and exponentiation respectively. The normalization
constant Zt = d
j=1 wt−1, j exp(−ηytxt, j) ensures that the weight vector remains a probability distri-
bution. Also note that in Perceptron, we use the initialization w0 = 0 whereas Winnow initializes w0
with the uniform distribution over the d features.
To give a mistake bound for Perceptron and Winnow, we will assume that we are in realizable case.
That is, there is some weight vector w⋆that can perfectly classify all the examples that the learner sees.
Moreover, we can assume that there is a margin available in the correct classiﬁcations. For a correct
classiﬁcation, it should simply be the case that yt⟨w⋆, xt⟩> 0. The margin condition will ensure that
there is a lower bound γ > 0 such that
∀t, yt⟨w⋆, xt⟩≥γ.
(14.8)

806
CHAPTER 14 Learning Theory
Algorithm 3. Online Learning of Linear Threshold Functions
Initialize w0
for t = 1, 2, . . . do
Receive xt
Predict yt = sign(⟨w, xt⟩)
Receive true label yt
if yt ̸= yt then
wt ←LTF-Update(wt, η, xt, yt)
else
wt ←wt−1
end if
end for
Theorem 28.
Suppose there exists a w⋆satisfying the margin condition (14.8). Then Perceptron with
learning rate η = 1 enjoys the mistake bound
M ≤∥w⋆∥2
2 · B2
2
γ 2
,
where B2 = maxt ∥xt∥2.
Proof.
Let T ≥1 and note that the number M = MT of mistakes till time T is simply
MT =
T

t=1
mt
where mt = 1 [y ̸= yt] = 1 [yt⟨wt, xt⟩≤0]. Let us calculate how the inner product of wt with the
vector w⋆evolves. Note that we can write
wt = wt−1 + mt ytxt.
Thus, we have,
⟨w⋆, wt⟩= ⟨w⋆, wt−1 + mt ytxt⟩
= ⟨w⋆, wt−1⟩+ mt yt⟨w⋆, xt⟩
≥⟨w⋆, wt−1⟩+ mtγ,
where the last step is due to the margin assumption (14.8). Summing the above inequality for t =
1, . . . , T yields
γ MT + ⟨w⋆, w0⟩≤⟨w⋆, wT ⟩.
Since w0 = 0 and ⟨w⋆, wT ⟩≤∥w⋆∥2∥wT ∥2, we get
γ MT ≤∥w⋆∥2∥wT ∥2.
(14.9)

1.14.6 Computational Aspects
807
Let us now try to upper bound ∥wT ∥2. We have,
∥wt∥2
2 = ∥wt + mt ytxt∥2
2
= ∥wt∥2
2 + 2mt yt⟨wt, xt⟩+ m2
t y2
t ∥xt∥2
2
≤∥wt∥2
2 + m2
t y2
t ∥xt∥2
2,
where the last line is true because yt⟨wt, xt⟩≤0 when mt > 0. Also note that y2
t = 1, m2
t = mt, and
∥xt∥2 ≤B2. Therefore,
∥wt∥2
2 ≤∥wt∥2
2 + mt B2
2.
Summing this for t = 1, . . . , T gives
∥wT ∥2
2 ≤∥w0∥2 + MT B2
2 = MT B2
2.
Combining this with (14.9) gives
γ MT ≤∥w⋆∥2

MT B2,
which gives us the ﬁnal bound
MT ≤∥w⋆∥2
2 · B2
2
γ 2
.
□
Theorem 29.
Suppose there exists a w⋆with positive entries that satisﬁes the margin condition (14.8).
Let B∞= maxt∥xt∥∞. Then Winnow enjoys the mistake bound
M ≤log d
C(η)
whenever
C(η) =

ηγ
∥w⋆∥1
−log
eηB∞+ e−ηB∞
2

> 0.
In particular, by setting η optimally, we get,
M ≤2∥w⋆∥2
1 · B2
∞
γ 2
.
Proof.
Let u⋆= w⋆/∥w⋆∥1. Since w⋆> 0 (inequality is entrywise), u⋆is a probability distribution.
Moreover, Winnow maintains a probability distribution wt at all times. We will track the progress of
the algorithm using the relative entropy
D(u⋆||wt) =
d

j=1
u⋆
j log
u⋆
j
wt, j
between u⋆and wt.

808
CHAPTER 14 Learning Theory
Suppose a mistake occurs at time t. Then we have,
D(u⋆||wt) −D(u⋆||wt−1) =
d

j=1
u⋆
j log wt−1, j
wt, j
=
d

j=1
u⋆
j log
Zt
exp(ηytxt, j)
= log Zt
d

j=1
u⋆
j −ηyt
d

j=1
u⋆
j xt, j
= log Zt −ηyt⟨u⋆, xt⟩
≤log Zt −
ηγ
∥w⋆∥1
,
(14.10)
where the last step is due to the margin assumption (14.8). Note that ytxt j ∈[−B∞, B∞].
□
For any α ∈[−B∞, B∞] and η > 0, we have the inequality
eηα ≤1 + α/B∞
2
eηB∞+ 1 −α/b∞
2
e−ηB∞.
To see this consider a random variable Z that takes value ηL with probability (1 + α/B∞)/2 and
takes value −ηL with probability (1−α/B∞)/2. Then the inequality above is claims that exp(E[Z]) ≤
E[exp(Z)] which is indeed true by Jensen’s inequality. Using the inequality above, we have
Zt =
d

j=1
wt−1, jeηyt xt−1, j
≤
d

j=1
wt−1, j
1 + ytxt−1, j/B∞
2
eηB∞+ 1 −ytxt−1, j/B∞
2
e−ηB∞

=
eηB∞+ e−ηB∞
2

d

j=1
wt−1, j +
eηB∞−e−ηB∞
2B∞

d

j=1
ytwt−1, j xt, j
=
eηB∞+ e−ηB∞
2

+
eηB∞−e−ηB∞
2B∞

d

j=1
yt⟨wt−1, xt⟩
≤
eηB∞+ e−ηB∞
2

,
where the last step is true because, in a mistake round, yt⟨wt−1, xt⟩≤0. Combining this bound on Zt
with (14.10) gives us, on any mistake round,
D(u⋆||wt) −D(u⋆||wt−1) ≤−C(η),

1.14.7 Beyond the Basic Probabilistic Framework
809
where, we have deﬁned
C(η) =
ηγ
∥w⋆∥1
−log
eηB∞+ e−ηB∞
2

.
Thus, for any round,
D(u⋆||wt) −D(u⋆||wt−1) ≤−C(η)mt,
where mt = 1[yt ̸= yt]. Summing this over t = 1, . . . , T gives
D(u⋆||wT ) −D(u⋆||w0) ≤−C(η)
T

t=1
mt = −C(η)MT .
Since relative entropy is always non-negative D(u⋆||wT ) ≥0. On the other hand,
D(u⋆||w0) =
d

j=1
u⋆
j log (u⋆
jd) ≤
d

j=1
u⋆
j log d = log d.
Thus, we have
−log d ≤−C(η)MT
which gives, whenever C(η) > 0,
MT ≤log d
C(η).
By choosing η to maximize C(η), we get
η =
1
B∞
log
 L + γ/∥w⋆∥1
L −γ/∥w⋆∥1

which yields the bound
MT ≤
log d
g

γ
B∞∥w⋆∥1
,
where
g(x) = 1 + x
2
log (1 + x) + 1 −x
2
log (1 −x).
The second statement of the theorem now follows because g(x) ≥x2/2 for x ∈[−1, 1].
1.14.7 Beyond the basic probabilistic framework
Our overview of learning theory emphasized the classical setting in which a learning algorithm sees
fully labeled data that is an iid draw from an unknown distribution. There are many extensions possible

810
CHAPTER 14 Learning Theory
to this basic setting. We mention a few directions that are have been explored by researchers. Many of
these continue to evolve actively as new results appear.
Richer Output Spaces: We mostly talked about classiﬁcation and regression. One can certainly
consider richer output spaces. The ﬁeld of structured prediction [47] deals with exponentially large
output spaces consisting of graphs, trees, or other combinatorials objects. Another output space that
arises in ranking applications is the space of permutations of a ﬁnite set of objects. There has also been
work on learning vector valued functions [48].
Learning with Dependent Data: The iid assumption has also been relaxed. Aldous and Vazirani [49],
Gamarnik [50] consider Markov chains. Irle [51], Meir [52], Vidyasagar [17], Lozano et al. [53], Mohri
and Rostamizadeh [54], Steinwart et al. [55] consider mixing processes. There is also a negative result
[56] showing that there is no universally consistent learning algorithm for stationary ergodic processes.
Learning with Adversarially Generated Data: As in the mistake bound model, it is possible to develop
a theory of learning from online adversarially generated data provided one deﬁnes learnability in terms
of low regret
T

t=1
ℓ( ft−1(xt), yt) −inf
f ∈F ℓ( f (xt), yt)
which compares the performance of the learner’s functions f1, . . . , fT to the best function chosen in
hindsight(notethatthelearnerhastocommitto ft−1 seeingonlyexamplestill(xt−1, yt−1).Analoguesof
Rademacher complexity, covering numbers, and fat shattering dimension have been developed [57]. This
topic has fascinating connections to information theory and game theory. See [21] for a comprehensive
overview.
Active Learning: In active learning, we consider situations where the learner has some control over
the data it uses to learn. This is in contrast to the basic iid model where the learner passively sees iid data
drawn from a distribution. In particularly useful model of active learning, we assume that the learning
has access to unlabeled points X1, . . . , Xn drawn iid from a distribution either all at one (a pool) or
one at a time (a stream). The learner can only query for labels of examples in the pool or the stream.
The label complexity of the learner is the number of label queries it needs to make in order to achieve
a desired accuracy with a given level of conﬁdence. In a lot of situations, unlabeled data is cheap and
easy to get. So, it makes sense to charge the learner only for the number of labels it requires. The label
complexity of different problems in an active learning has been studied (see, for example, [58] and
references therein). Given the practical importance of reducing the number of labels required and the
relatively unexplored nature of active learning, a lot remains to be done.
Semisupervised Learning: Semisupervised learning refers to learning with labeled data and large
amounts of unlabeled data. A lot of work has been done in this area including methods that try to
learn classiﬁers whose decision boundary passed through low density regions of the unlabeled data
to graph based methods that use regularization to penalize functions that are nonsmooth along edges
of a graph built from unlabeled data. For a good collection of articles on semisupervised learning,
see the collection [59]. In the same collection, Balcan and Blum propose a PAC style framework for
semisupervised learning.
Learning Multiple Tasks: In multitask learning, the learner is faced with learning multiple related
tasks. Because of task relatedness, we expect the learner to require fewer samples when the tasks
are learned together rather than separately. Many approaches try to encode task relatedness into a

1.14.8 Conclusions and Future Trends
811
regularization function that couples all the tasks together. Some important advances in our theoretical
understanding of multitask learning have been made ([60–63]) but a comprehensive theory, if one exists,
remains to be found.
1.14.8 Conclusions and future trends
Understanding intelligence and constructing intelligent machines is a great scientiﬁc challenge. An
understanding of learning is central to an understanding of intelligence. Whether we talk about learning
on an evolutionary time scale or learning within the lifespan of an organism, learning plays a crucial role
in the development of intelligent behavior. What learning theory has attempted to do so far is provide
a solid mathematical framework within which to pose and solve questions regarding the process of
learning. Admittedly, the most mature part of learning theory is the theory of supervised learning
including classiﬁcation and regression problems. But this is just a beginning rather than the end. We
hope that future research will make learning theory much richer and applicable to a broader variety of
problems in both natural and artiﬁcial systems.
Learning theory is an interdisciplinary ﬁeld. It draws ideas and inspiration from a variety of dis-
ciplines including biology, computer science, economics, philosophy, psychology, statistical physics,
and statistics. It attempts to achieve clarity and rigor by using the language of mathematics to precisely
state assumptions and prove results. We expect that interdisciplinary cross-fertilization of ideas will
continue to enrich and give new directions to learning theory. For example, learning theory has much
to contribute to problems arising in diverse areas such as control [17] and dynamical systems [64],
learning in games [65], privacy [66], evolution [67], and mechanism design [68].
Glossary
AdaBoost
Acronym for Adaptive Boosting, a popular boosting algorithm
Bayes classiﬁer
In classiﬁcation problems with 0–1 loss, a classiﬁer with the
least risk
Boosting
The process of converting a weak learning algorithm (that is,
one that outputs classiﬁers whose performance is just a little
bit better than random guessing) into a strong one
Concentration inequality
A probabilistic inequality stating that some random variable
will not deviate too far from its mean or median
Excess risk
The difference between the risk of a given function and the
minimum possible risk over a function class
Empirical risk
Same as risk except that the expectation under the unknown
data distribution is replaced with an empirical average over the
samples observed
Empirical risk minimization
A learning rule that minimizes the empirical risk to choose a
prediction function

812
CHAPTER 14 Learning Theory
ERM
Empirical Risk Minimization
Fat shattering dimension
A scale sensitive generalization of the VC dimension.
Generalization error bound
An upper bound stating that the risk of a learning rule is not
going to differ too much from its empirical risk
Kernel
A symmetric positive semideﬁnite function
Littlestone dimension
A combinatorial quantity associated with a class of binary val-
ued functions. Plays a fundamental role in determining worst
case bounds in the online mistake bound model
Loss function
A function used to measure the quality of a prediction given
the true label.
Mistake bound model
A model of learning that, unlike the PAC model, does not
assume anything about the way data is generated
Model selection
The task of selecting an appropriate statistical model from a
family based on available data
PAC model
A computational model for analyzing the resources (time,
memory, samples) required for performing learning tasks.
Originally deﬁned for the task of binary classiﬁcation using
the 0–1 loss. The acronym PAC stands for “Probably Approx-
imately Correct”
Rademacher complexity
A quantity associated with a function class that measures its
capacity to overﬁt by measuring how well the function class
can ﬁt randomly generated ±1 signs
Regression function
In regression problems with the squares loss, the prediction
function with the least risk
Reproducing kernel Hilbert space A Hilbert space of functions for which there exists a kernel
with the reproducing property: point evaluations of functions
can be written as linear functionals
Risk
Theexpectedlossofapredictionfunctionundertheunderlying
unknown distribution generating input, output pair
Sample complexity
The number of samples requires to achieve a given level of
accuracy (usually with high probability)
Sample compression
An approach to obtaining generalization error bounds for algo-
rithms whose output can be reconstructed from a small number
of examples belonging to the original sample
Stability
The property of a learning algorithm that ensures that its output
doesnotchangemuchifthetrainingdatasetischangedslightly
Supervised learning
The task of learning a functional dependence between an input
space and an output or label space using given examples of
input, output pairs
Universal consistency
The property of a learning rule to converge to the minimum
possible risk as the number of samples grows to inﬁnity

1.14.8 Conclusions and Future Trends
813
Vapnik-Chervonenkis dimension
A combinatorial quantity associated with a binary valued func-
tion class that serves as a measure of the capacity of the func-
tion class to overﬁt the data
VC dimension
Vapnik-Chervonenkis dimension
Relevant Theory: Machine Learning
See this Volume, Chapter 14 Learning Theory
See this Volume, Chapter 15 Neural Networks
See this Volume, Chapter 16 Kernel Methods and SVMs
See this Volume, Chapter 17 Online Learning in Reproducing Kernel Hilbert Spaces
See this Volume, Chapter 18 Introduction to Probabilistic Graphical Models
See this Volume, Chapter 20 Clustering
See this Volume, Chapter 22 Semi-supervised Learning
See this Volume, Chapter 23 Sparsity-Aware Learning and Compressed Sensing: An Overview
See this Volume, Chapter 25 A Tutorial on Model Selection
Relevant websites
•
Scholarpedia has a section devoted to machine learning containing links to articles written by leading
researchers.
http://www.scholarpedia.org/article/Category:Machine_learning/
•
The Encyclopedia of Machine Learning is a valuable online resource available through subscription.
http://www.springerlink.com/content/978-0-387-30768-8/
•
The Journal of Machine Learning Research a leading open access journal publishing high quality
articles in machine learning and related topics.
http://jmlr.csail.mit.edu/
•
Another high quality journal publishing articles on machine learning and related topics is Machine
Learning.
www.springer.com/computer/ai/journal/10994
•
There is a Google group for postings of possible interest to learning theory researchers.
http://groups.google.com/group/learning-theory/
•
The website of the International Machine Learning Society. The society organizes the annual Inter-
national Conference on Machine Learning (ICML).
http://www.machinelearning.org/
•
The website of the Association for Computational Learning. The association organizes the annual
Conference on Learning Theory (COLT).
http://seed.ucsd.edu/joomla/

814
CHAPTER 14 Learning Theory
•
The website of the Neural Information Processing Systems (NIPS) Foundation. The foundation
organizes the annual NIPS conference.
http://nips.cc/
•
The arXiv is good way to ﬁnd recent preprints in machine learning and learning theory.
http://arxiv.org/list/cs.LG/recent/
References
[1] A.M. Turing, Computing machinery and intelligence, Mind 59 (1950) 433–460.
[2] W.S. McCulloch, W. Pitts, A logical calculus of the ideas immanent in neural nets, Bulletin of Mathematical
Biophysics 5 (1943) 115–137.
[3] A.L. Samuels, Some studies in machine learning using the game of checkers, IBM J. Res. Develop. 3 (3)
(1959) 210–233.
[4] F. Rosenblatt, Two theorems of statistical separability in the perceptron, in: Proceedings of a Symposium on
the Mechanization of Thought Processes, Her Majesty’s Stationary Ofﬁce, London, 1959, pp. 421–456.
[5] A.B.J. Novikoff, On convergence proofs for perceptrons, in: Proceedings of the Symposium on the Mathe-
matical Theory of Automata (New York, 1962), 1963, pp. 615–622.
[6] V. Vapnik, Statistics for engineering and information science, The Nature of Statistical Learning Theory,
second ed., Springer, 2000.
[7] D.E. Rumelhart, G. Hinton, R.J. Williams, Learning representations by back-propagating errors, Nature 323
(1986) 533–536.
[8] L.G. Valiant, A theory of the learnable, J. ACM 27 (11) (1984) 1134–1142.
[9] C. Cortes, V.N. Vapnik, Support-vector networks, Mach. Learn. 20 (3) (1995) 273–295.
[10] Y. Freund, R.E. Schapire. A decision-theoretic generalization of online-learning and an application to
boosting, in: Proceedings of the Second European Conference on Computational Learning Theory, 1995,
pp. 23–37.
[11] C.J. Stone, Consistent nonparametric regression, Ann. Stat. 5 (4) (1977) 595–620.
[12] M. Anthony, N. Biggs, Computational Learning Theory: An Introduction, Number 30 in Cambridge Tracts
in Computer Science, Cambridge University Press, 1997.
[13] M. Anthony, P.L. Bartlett, Neural Network Learning: Theoretical Foundations, Cambridge University Press,
1999.
[14] L. Devroye, L. Györﬁ, G. Lugosi, A Probabilistic Theory of Pattern Recognition, Stochastic Modelling and
Applied Probability, vol. 31, Springer, 1996.
[15] V. Vapnik, Estimation of Dependences Based on Empirical Data; Empirical Inference Science: Afterword
of 2006, second ed., Springer, 2006 (Reprint of 1982 edition with afterword of 2006).
[16] V. Vapnik, Adaptive and learning systems for signal processing, communications, and control, Statistical
Learning Theory, Wiley, 1998.
[17] M. Vidyasagar, Learning and Generalization: With Application to Neural Networks, Communications and
control engineering, second ed., Springer, 2003.
[18] B.K. Natarajan, Machine Learning: A Theoretical Approach, Morgan Kauffmann, 1991.
[19] F. Cucker, D.-X. Zhou, Learning Theory: An Approximation Theory Viewpoint, Cambridge Monographs
on Applied and Computational Mathematics, Cambridge University Press, 2007.
[20] M.J. Kearns, U.V. Vazirani. An Introduction to Computational Learning Theory, MIT Press, 1994.
[21] N. Cesa-Bianchi, G. Lugosi, Prediction, Learning, and Games, Cambridge University Press, 2006.

References
815
[22] L. Györﬁ, M. Kohler, A. Krzyjak, H. Walk, A Distribution Free Theory of Nonparametric Regression,
Springer Series in Statistics, Springer, 2002.
[23] T. Hastie, R. Tibshirani, J. Friedman, The Elements of Statistical Learning, Springer Series in Statistics,
second ed., Springer, 2009.
[24] P. Massart, Concentration inequalities and model selection, Lecture Notes in Mathematics, vol. 1896,
Springer, 2007.
[25] P.L. Bartlett, S. Mendelson, Rademacher and gaussian complexities: Risk bounds and structural results, J.
Mach. Learn. Res. 3 (2002) 463–482.
[26] A. Ambroladze, E. Parrado-Hernández, J. Shawe-Taylor, Complexity of pattern classes and the lipschitz
property, Theor. Comp. Sci. 382 (3) (2007) 232–246.
[27] R.M. Dudley, The sizes of compact subsets of Hilbert space and continuity of Gaussian processes, J. Funct.
Anal. 1 (3) (1967) 290–330.
[28] V.N. Vapnik, A. Ya, Chervonenkis, On the uniform convergence of relative frequencies of event to their
probabilities, Soviet Mathematics Doklady 9 (1968) 915–918.
[29] N. Sauer, On the density of families of sets, J. Comb. Theory A 13 (1972) 145–147.
[30] Saharon Shelah, A combinatorial problem; stability and order for models and theories in inﬁnitary languages,
Pac. J. Math. 41 (1972) 247–261.
[31] D. Haussler, Sphere packing numbers for subsets of the boolean n-cube with bounded vapnik-chervonenkis
dimension, J. Comb. Theory A 69 (2) (1995) 217–232.
[32] N. Alon, S. Ben-David, N. Cesa-Bianchi, D. Haussler, Scale-sensitive dimensions, uniform convergence,
and learnability, J. ACM 44 (4) (1997) 615–631.
[33] M.K. Warmuth, Sample compression, learnability, and the vapnik-chervonenkis dimension, in: Proceedings
of the Third European Conference on Computational Learning Theory, 1997, pp. 1–2.
[34] O. Bousquet, A. Elisseeff, Stability and generalization, J. Mach. Learn. Res. 2 (2002) 499–526.
[35] S. Kutin, P. Niyogi, Almost-everywhere algorithmic stability and generalization error, in: Proceedings of the
18th Conference on Uncertainty in Artiﬁcial Intelligence, 2002, pp. 275–282.
[36] S. Mukherjee, P. Niyogi, T. Poggio, R.M. Rifkin, Learning theory: stability is sufﬁcient for generalization
and necessary and sufﬁcient for consistency of empirical risk minimization, Adv. Comput. Math. 25 (1–3)
(2006) 161–193.
[37] S. Shalev-Shwartz, O. Shamir, N. Srebro, K. Sridharan, Learnability, stability and uniform convergence, J.
Mach. Learn. Res. 11 (2010) 2635–2670.
[38] David A. McAllester, PAC-Bayesian stochastic model selection, Mach. Learn. 51 (1) (2003) 5–21.
[39] Robert E. Schapire, The strength of weak learnability, Mach. Learn. 5 (1990) 197–227.
[40] Y. Freund, Boosting a weak learning algorithm by majority, Info. Comput. 121 (2) (1995) 256–285.
[41] Y. Freund, R.E. Schapire, Game theory, on-line prediction and boosting, in: Proceedings of the Ninth Annual
Conference on Computational Learning Theory, 1996, pp. 325–332.
[42] D. Angluin, P.D. Laird, Learning from noisy examples, Mach. Learn. 2 (4) (1987) 343–370.
[43] M. Kearns, Efﬁcient noise-tolerant learning from statistical queries, J. ACM 45 (6) (1998) 983–1006.
[44] V. Feldman, P. Gopalan, S. Khot, A.K. Ponnuswami, On agnostic learning of parities, monomials, and
halfspaces, SIAM J. Comput. 39 (2) (2009) 606–645.
[45] N. Littlestone, Learning quickly when irrelevant attributes abound: a new linear-threshold algorithm, Mach.
Learn. 2 (4) (1987) 285–318.
[46] S Ben-David, D. Pál, S. Shalev-Shwartz, Agnostic online learning, in: Proceedings of the 22nd Annual
Conference on Learning Theory, 2009.
[47] G.H. Bakir, T. Hofmann, B. Schölkopf, A.J. Smola, Ben Taskar, S.V.N. Vishwanathan, Predicting Structured
Data, MIT Press, 2007.
[48] C.A. Micchelli, M. Pontil, On learning vector-valued functions, Neural Comput. 17 (1) (2005) 177–204.

816
CHAPTER 14 Learning Theory
[49] D. Aldous, V. Vazirani, A Markovian extension of Valiant’s learning model, in: Proceedings of the 31st
Annual Symposium on Foundations of Computer Science, vol. 1, 1990, pp. 392–396.
[50] D. Gamarnik, Extension of the PAC framework to ﬁnite and countable Markov chains, in: Proceedings of
the twelfth annual conference on Computational learning theory, 1999, pp. 308–317.
[51] A. Irle, On the consistency in nonparametric estimation under mixing assumptions, J. Multivar. Anal. 60
(1997) 123–147.
[52] R. Meir, Nonparametric time series prediction through adaptive model selection, Mach. Learn. 39 (1) (2000)
5–34.
[53] A. Lozano, S. Kulkarni, R. Schapire, Convergence and consistency of regularized boosting algorithms with
stationary β-mixing observations, Adv. Neural Inform. Process. Syst. 18 (2006) 819–826.
[54] M. Mohri, A. Rostamizadeh. Rademacher complexity bounds for non-i.i.d. processes, in: Advances in Neural
Information Processing 21, 2009, pp. 1097–1104.
[55] I. Steinwart, D. Hush, C. Scovel, Learning from dependent observations, J. Multivar. Anal. 100 (1) (2009)
175–194.
[56] A. Nobel, Limits to classiﬁcation and regression estimation from ergodic processes, Ann. Stat. 27 (1) (1999)
262–273.
[57] A. Rakhlin, K. Sridharan, A. Tewari, Online learning: Random averages, combinatorial parameters, and
learnability, Adv. Neural Inform. Process. Syst. 23 (2010) 1984–1992.
[58] S. Hanneke, Rates of convergence in active learning, Ann. Stat. 39 (1) (2011) 333–361.
[59] O. Chapelle, B. Schölkopf, A. Zien (Eds.), Semi-Supervised Learning, MIT Press, Cambridge, MA, 2006.
[60] J. Baxter, A model of inductive bias learning, J. Artif. Intell. Res. 12 (2000) 149–198.
[61] R. Caruana, Multitask learning, Mach. Learn. 28 (1) (1997) 41–75.
[62] T. Evgeniou, C.A. Micchelli, M. Pontil, Learning multiple tasks with kernel methods, J. Mach. Learn. Res.
6 (2005) 615–637.
[63] A. Maurer, Bounds for linear multi-task learning, J. Mach. Learn. Res. 7 (2006) 117–139.
[64] M. Campi, P.R. Kumar, Learning dynamical systems in a stationary environment, Syst. Control Lett. 34 (3)
(1998) 125–132.
[65] H. Peyton Young, Strategic Learning and Its Limits, Oxford University Press, 2004.
[66] M.-F. Balcan, A. Blum, S. Fine, Y. Mansour, Distributed learning, communication complexity and privacy,
in: Proceedings of the 25th Annual Conference on Learning Theory, JMLR Workshop and Conference
Proceedings, vol. 23, 2012, pp. 26.1–26.22.
[67] L.G. Valiant, Evolvability, J. ACM 56 (1) 2009.
[68] M.-F. Balcan, A. Blum, J.D. Hartline, Y. Mansour, Reducing mechanism design to algorithm design via
machine learning, J. Comp. Syst. Sci. 74 (8) (2008) 1245–1270.

15
CHAPTER
Neural Networks
Barbara Hammer
CITEC centre of excellence, Bielefeld University, D-33594 Bielefeld, Germany
1.15.1 Introduction
Roughly speaking, Neuroinformatics is based on the paradigm to turn ideas from biological neural
networks, which serve as very effective information processing units in real life, into efﬁcient artiﬁcial
machine learning methods in Computer Science. Thereby, the lines between biological motivation and
statistical methodology are often blurred, and, today, a thorough mathematical background exists for
most techniques from Neuroinformatics independent of their biological counterparts, i.e., Neuroinfor-
matics has become a mature research ﬁeld on its own independent of its biological roots. There exists a
variety of excellent textbooks in this ﬁeld covering mathematical background as well as modern training
algorithms in this context, such as [1–6].
The early beginnings of Neuroinformatics date back to 1943, when McCulloch and Pitts showed the
ability of circuits of simple neurons to realize every logical calculation [7]. In 1949, a training paradigm
was proposed by Hebb [8], the famous Hebbian learning which states the principle to increase the
strength of neurons if they are simultaneously active. This principle forms the base of many learning
algorithms which are used today. Algorithmic realizations of Hebbian learning rules for simple neural
networks without any intermediate connections have been realized by Rosenblatt [9] and, independently,
by Widrow and Hoff [10]. Remarkably, perceptron learning as proposed by Rosenblatt is accompanied
by a formal proof of its convergence to a solution if existing. However, research came to a rapid stop
after the famous book of Minsky and Papert, “Perceptrons,” has been published. In this book, the authors
proved the principled limitation of the Rosenblatt-Perceptron and other simple models [11].
A number of researchers continued to work on relevant topics related to Neuroscience including,
e.g., associative memory [12] and the development of the visual system [13]. The breakthrough which
brought Neuroscience back to the focus of a large number of researchers was the reinvention of back-
propagation for training feedforward neural networks by Rumelhart et al. [14]. This algorithm has been
proposed previously by Werbos [15] and it became widely accepted after it has been published a second
time. It forms the base for most learning algorithms for multilayer neural networks which are used
to date. Today a variety of different learning paradigms, neural architectures, and training algorithms
exists which offer efﬁcient tools for machine learning in diverse areas including pattern recognition,
robotics, bioinformatics, or natural language processing.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00015-2
© 2014 Elsevier Ltd. All rights reserved.
817

818
CHAPTER 15 Neural Networks
Often, the biological background only plays a minor role in modern applications and neural learning
techniques constitute special cases of more general statistical learners such as, e.g., general regression
or classiﬁcation models, generative statistical models, Bayesian techniques, and similar, see textbooks
such as [1]. In this context, current problems typically center around the question of how to deal with
larger and larger data sets, how to arrive at even more efﬁcient learning techniques, how to integrate
prior knowledge and structural constraints, how to make the decisions of neural networks accessible for
humans, and similar. These research questions are no longer speciﬁc for neural networks, rather they
occur in virtually all machine learning scenarios.
However, quite a few recent developments in the context of neural networks have been triggered
by biological counterparts, again: deep learning architectures can be traced back to typical processing
pipelines in the human visual system and its progressive representation of more and more abstract
visual stimuli [16]. At present, deep learning architectures are discussed as one general principle to
directly address complex input patterns in neural network architectures without the necessity of complex,
domain-speciﬁc preprocessing of the given signals. Reservoir computing constitutes another example
where biologically motivated paradigms offer a possible solution to long-standing problems in Neu-
roinformatics. Partially recurrent neural networks deal with time dependent signals as input data, and,
thus, have potential impact on areas such as temporal signal processing, speech recognition, or motion
generation. Training such networks, however, has been a problem due to inherent numeric difﬁculties of
typical learning algorithms. Reservoir networks are based on a randomly wired recurrent network with
very high dimensionality and a very simple readout, similar to intuitive recurrent processing as could
happen in the brain [17,18]. They offer a partial answer to the complexity of recurrent neural network
training by simply making it superﬂuous. These issues will be addressed in this chapter in more detail.
The main goal of this chapter is to give a representative picture of different relevant architectures of
the “zoo” of neural networks. The emphasis lies on the principled idea behind the respective models
and an understanding of their strengths and restrictions with links to further reading. We will give hints
on principled training algorithms, and we will discuss mathematical issues which arise when training
such networks using given examples. Naturally, this overview is necessarily limited to some of the most
important neural architectures which are used today.
1.15.2 Learning with single neurons
Let us start with a simple problem: we would like to automatically classify handwritten digits, e.g.,
to automatically read the ZIP code hand written on letters and collected in a US postal ofﬁce. For
this purpose, we accumulate examples where we know the intended digit, assigned to by a per-
son, the training set. Depending on the digitalization, we can assume that each example consists of
an input vector in, say, R64, corresponding to a representation of the digit in a 8 × 8 matrix with
gray values. We represent the output class by a number. Generally, inputs stem from Rn, n being
any ﬁxed number, the input dimensionality. Outputs are class labels. For simplicity, we start with
only two classes for the moment, 0 and 1. Actually, data sets of such type constitute typical bench-
marks for neural classiﬁcation algorithms even today, two popular data sets being the USPS (see
http://www-stat-class.stanford.edu/∼tibs/ElemStatLearn/data.htmlﬁlecalledZIPcode)andtheMNIST
(see http://yann.lecun.com/exdb/mnist/) data sets.

1.15.2 Learning with Single Neurons
819
Neural networks offer one possible mechanism to infer a classiﬁcation describing an unknown
regularity based on a ﬁnite number of examples. We start with the simplest neural network one can
think of: a single neuron, more precisely, the perceptron.
1.15.2.1 The simple perceptron
The function of biological neural networks is well investigated and their dynamics can be described
referring to the Hodgkin-Huxley model, for example [19]. Biological neurons can be interpreted as
signal processing units based on electric currents. While the biological Hodgin-Huxley model describes
the dynamics of the most relevant ions regarding their spatio-temporal location in detail, abstractions
in Neuroinformatics integrate over the spatial location of the electric current of all ions, or they even
abstract from the precise temporal dynamics, referring to the mean ﬁring rate of neurons.
The perceptron constitutes a particularly simple neuron which restricts information to a simple digital
signal: 0 or 1 corresponding to active or inactive synapses. This abstraction allows to characterize a
perceptron by its function:
Rn ∋x →H(wt x −θ) ∈R,
whereby w ∈Rn is the so-called weight-vector of the neuron, and θ ∈R the bias. As usual, wt x denotes
the dot product of the two vectors. H : R →R denotes a nonlinear activation function, in the case of
the perceptron it is the Heaviside function with
H(x) =
1 if x ≥0,
0 otherwise.
Thus, the neuron computes the weighted sum of inputs, compares the result with the bias, and outputs a
signal depending on its activation wt x−θ. This procedure resembles biological neurons which integrate
incoming signals and ﬁre depending on their action potential whereby, for the artiﬁcial counterpart,
spatiotemporal aspects are neglected.
This function has an exact geometric counterpart: a perceptron deﬁnes a separating hyperplane
of the data. The weight vector w determines the orientation of the hyperplane, the bias θ the offset.
Thus, it deﬁnes a simple linear classiﬁcation boundary, see Figure 15.1. For simple data sets or high
dimensionality, a linear classiﬁer is often sufﬁcient to separate a given data set appropriately.
How can we get appropriate parameters w and θ given a set of training data? The perceptron training
algorithm is a very simple method which can be formulated in a few lines of code:
init
w, θ randomly
repeat:
choose a training example (x, y)
if H(wt x −θ) ̸= y
w := w + x, θ := θ −1 if y = 1
w := w −x, θ := θ + 1 if y = 0
The adaptation rule constitutes an instance of Hebbian learning: given an example which is wrongly
classiﬁed as 0 but it should be 1, all weight components which receive a positive signal are reinforced
by adding the input signal x. For a wrong classiﬁcation as 1 which should be 0, the weight components

820
CHAPTER 15 Neural Networks
weight
bias
FIGURE 15.1
Two-class classiﬁcation problem represented by points in the euclidean plane. Since the set is linearly sep-
arable, a simple perceptron can serve as classiﬁer. It is uniquely characterized by its weight vector and bias.
which receive a negative signal are increased. One can show that this learning rule converges towards
a solution without errors, if it exists. It ﬁnds a solution with a minimum number of errors with high
probability, provided the examples are randomly chosen and the best solution found so far is stored [9].
Because of the fact that a perceptron corresponds to a separating hyperplane, training sets which
can correctly be classiﬁed by a perceptron are called linearly separable. Obviously, a perceptron can
solve exactly those problems (up to a few errors), which are (almost) linearly separable. For very high
dimensional data as present, e.g., in biomedical application dealing with mass spectra or microarrays,
this is often the case. In general, decision boundaries are nonlinear in realistic problems. But even if
we restrict to linear problems, a few questions arise at this point: how complex is perceptron learning,
i.e., what is the number of iterations the algorithm requires until convergence? Can we guarantee, that
perceptron learning ﬁnds the underlying regularity of the classiﬁcation which we would like to learn
and does not only map the given training set correctly? These two questions concern the complexity
of learning and the generalization ability. We will consider these questions after having deﬁned more
general feed-forward neural networks for nonlinear problems.
1.15.2.2 Feedforward networks
A feedforward neural network consists of a number of neurons which are connected in an acyclic
directed graph. Often, the neurons are arranged in layers, the neurons without predecessors forming the
input layer, the neurons without successors forming the output layer, the ones in between forming the
hidden layer or hidden layers, see Figure 15.2. Such a network computes a highly nonlinear function
by combining the functions of single neurons according to the given connection graph.
Formally, this function can be deﬁned as follows: Assume the number of input neurons is n, the
number of output neurons is m, w ji constitutes the weight of neuron i pointing from neuron j, and
θi is the bias of neuron i. The activation function of every neuron is ﬁxed, often the logistic function

1.15.2 Learning with Single Neurons
821
input layer
hidden layer
output layer
input layer
hidden layer
output layer
FIGURE 15.2
A layered feedforward neural network with one hidden layer.
σ(x) = (1 + exp ( −x))−1 or the hyperbolic tangent tanh (x) are chosen. Such a network computes
a function fW : Rn →Rm, x →(oi1, . . . , oim), which maps an input vector to the output values of
the output neurons of the network. The function is parameterized by the weights and biases W in the
network. The numbers i j constitute the indices of the output neurons. The output oi of a neuron i is
recursively deﬁned as follows:
oi =
 xi
if i is an input neuron,
σ( 
j→i w jio j −θi) otherwise.
The activation of a single neuron is abbreviated by the term neti = 
j→i w jio j −θi. Obviously,
such a network can realize highly nonlinear functions. The perceptron activation function is substituted
by the sigmoidal function which approximates the perceptron function for large inputs, but which is
differentiable. This feature is used to derive training algorithms for multi-layer networks, as we will
see below. A single sigmoidal unit can be interpreted as an approximation of a simple perceptron in its
saturated regions. Often, however, sigmoidal neurons are activated in their linear area around 0. This
feature can be motivated by another observation from biology: taking real values as average ﬁring rate
of neurons, a sigmoidal function mirrors the typical transfer dynamics of biological neurons.
A feedforward neural network can not only represent a simple binary classiﬁcation such as the per-
ceptron, but classiﬁcation with more than two classes or real-valued functions. How can such a network
be trained? Since the activation function σ is differentiable, the following mathematical principle can
be taken: Given a set of training patterns of the form (x p, yp) the mean square error is deﬁned as the
quantity
E = 1/2 ·

p
( fW(x p) −y p)2.
This error measure constitutes a differentiable function which maps the weights W of the network
to a real number indicating whether the current function approximately represents the given data set
or not. Training consists in a minimization of this cost function by means of a numeric optimization

822
CHAPTER 15 Neural Networks
method, commonly a variation of a gradient descent. A simple gradient descent is given by the following
pseudocode
init W randomly
repeat:
W := W −η · ∇W E
Thereby, ∇W E constitutes the vector of derivatives of E with respect to the weights wi j and biases θ.
η > 0 is a (small) learning rate. Initialization of the weights is usually done with small random values
around 0.
A stochastic gradient descent decomposes the mean square error into the contributions of single
patterns E = 
p E p = 1/2 · 
p ( fW(x p) −y p)2 and iteratively adapts according to a randomly
chosen pattern:
init W randomly
repeat:
choose p randomly,
W := W −η · ∇W E p
This can also be applied in an online scenario where the patterns are not known beforehand but generated
during training.
For feedforward networks, a particularly efﬁcient way to compute the derivative ∇W E p resp. ∇W E
is used in the famous back-propagation algorithm. Note that the recursive deﬁnition of the outputs oi
allows to compute the derivative of oi with respect to weights and biases directly in a recursive way,
however, the complexity of this method is O(|W|2), |W| being the number of weights. Back-propagation
deﬁnes the error terms δ j = ∂E p/∂net j. It holds
∂E p/∂wi j = oi · δ j
and
∂E p/∂θ j = −δ j.
The error terms δ j can be computed in linear time by back-propagation
δ j =
(o j −y p
j ) · σ ′(net j)
if j is an output neuron

j→k w jkδk · σ ′(net j) otherwise
propagating the error signals from the output neurons back to the hidden neurons. Note that numeric
optimization techniques with a better convergence rate are usually applied today such as, e.g., approxi-
mate conjugate gradient techniques or alternative higher order methods. Still, these numeric techniques
center around the partial derivatives and its efﬁcient computation by means of back-propagation. Fur-
ther, simple back-propagation has the beneﬁt that it can be applied in classical batch mode as well as in
online scenarios. Batch learning usually refers to the situation where all training examples are known
in advance. In such cases, the full error function is available and numerical techniques which take into
account, e.g., the curvature of the cost function can be used. In online settings such as, e.g., reinforce-
ment learning or robotics, training data become available on the ﬂy and adaptation has to take place
immediately after a novel training example has been presented. In this case, a stochastic gradient descent
can be used which corresponds to simple back-propagation applied separately for every given data point.

1.15.2 Learning with Single Neurons
823
1.15.2.3 The training pipeline
A variety of questions arises in this context:
1. How do we choose the architecture?
2. What can we do if the training error does not decrease?
3. Does the network behave correctly for future patterns if it maps the training data correctly?
1.15.2.3.1
How do we choose the architecture?
There is an easy answer to this question and a more complex one: the easy one—just try it and choose
the best architecture you can ﬁnd. Commonly, the suitability of an architecture is evaluated by means
of cross-validation. That means, the architecture is trained on a part of the data set and its mean squared
error evaluated on the remaining test set. To reduce statistical effects, this is repeated k times using
disjoint test sets which decompose the given set of training data.
The complex answer to the question of how to choose the architecture puts some light on relevant
aspects of feedforward networks: Unlike simple perceptrons, feedforward neural networks are universal
approximators, i.e., they are capable of approximating every reasonable (e.g., continuous) function on
a compact set up to any desired precision provided at least one hidden layer with a sufﬁcient number of
hidden neurons is available and an activation function such as the logistic function is used [20]. Hence,
feedforward networks can, in principle, represent any regularity arbitrarily well. In consequence, an
appropriate architecture can be found for any given training problem. In principle, the number of
hidden layers can be restricted to one hidden layer, or, in practice, a small number of hidden layers.
How do we evaluate an architecture? One evaluation measure is the error on the training set. If this
is large, the architecture is not expressive enough to represent the regularity which has to be learned.
However, if the error is small, this does not necessarily guarantee that the underlying regularity has been
learned, only the part represented by the training set is correct. One can think of a neural architecture as
a parameterized function class, whereby the number of parameters depends on the number of neurons of
the architecture. It has to be guaranteed, that a sufﬁcient number of parameters is available to represent
the training data correctly. At the same time it has to be guaranteed that no free parameters of the
architecture are left which are not sufﬁciently speciﬁed given the current training set. If so, the output
for data points not present in the training set could be random.
This problem is known as bias-variance-dilemma: architectures which are too small have a large
bias, i.e., a large deviation from the desired outputs already on the training set. Architectures which are
too large have a large variance, i.e., the output of patterns not contained in the training set is random
because the parameters of the architecture are not determined by the training set.
1.15.2.3.2
How to minimize the training error?
When applying simple back-propagation, training is often a very tedious task. This is due to several
properties of the error surface when training neural networks. First of all, the error surface is multimodal
and gradient descent methods are often trapped in local optima instead of global (or good) optima
of the cost function. This problem is partially unavoidable because the problem of ﬁnding a global
minimum of the mean squared error for neural networks is an NP-hard problem, as shown, e.g., in
[21,22]. Remarkably, NP-hardness is already present when minimizing the mean squared error of a
single neuron, or when training simple perceptrons for non-linearly separable data sets.

824
CHAPTER 15 Neural Networks
In practice, complex error surfaces and bad local optima can partially be avoided by simpliﬁcations
of the training task using data preprocessing. This way, all relevant information is available for the
network in explicit form and the dependency of the function value on the inputs is more simple. Standard
preprocessing includes, for example, standard procedures such as normalization of the input variables
as well as a variety of problem-dependent features, e.g., for image processing (such as ﬁlters) or time
series analysis (such as Fourier or wavelet transform). Appropriate preprocessing of data can be time
consuming and there is no recipe how to solve this problem in general.
Current research connected to Neuroinformatics centers around the question how complex prepro-
cessing can be avoided. Alternatively, how can an appropriate representation of data autonomously be
detected by the neural system itself? A very promising approach centers around deep learning, which
deals with neural architectures with many hidden layers but a highly regularized and symmetric struc-
ture [16]. Intuitively, these architectures can extract complex features from the data by an iterative
combination of simple features as represented by parts of those networks. High symmetry such as,
e.g., weight sharing in different parts of the network make sure that globally meaningful and uniform
local feature extractors result. However, training deep neural networks using back-propagation is usu-
ally very difﬁcult if not impossible due to numeric problems, such that alternative iterative learning
strategies are usually applied. Recent developments in this ﬁeld can be found, e.g., at a NIPS 2010
workshop (see http://deeplearningworkshopnips2010.wordpress.com/schedule/) or the deep learning
web site (see http://deeplearning.net/).
The problems which make a simple gradient technique unsuitable for training very deep architectures,
also partially occur for standard feedforward networks of reasonable size. Because of the logistic
activation function, the error surface of sigmoidal networks is composed of plateaus mixed with very
steep parts. This causes oscillation or stagnation of the optimization procedure, depending on the
curvature of the surface and the learning rate. Thereby, these effects can occur simultaneously with
respect to different parameters which have to be optimized.
Various improvements of simple back-propagation try to overcome these problems such as higher
order optimization methods [4]. One particularly powerful and, at the same time, very simple learning
schemes which allows to train standard feedforward networks very efﬁciently is offered by resilient
propagation (RProp) [23]. The idea is to use a separate adaptive learning rate for every parameter which
determines the step size into this direction. The derivative does only contribute the direction of the step,
but not its size. The step size is increased if the gradient points into the same direction in consecutive
steps (i.e., likely a plateau is present) and it is decreased if the sign changes (i.e., oscillation takes place),
whereby bad steps are retracted.
Another very interesting line of research centers around even more efﬁcient training algorithms
which do not rely on numeric optimization techniques. The extreme learning machine is based on
the observation that feed-forward neural networks can approximate every given training set provided
enough hidden neurons are available even if the connections of the inputs to the hidden neurons are
random. In consequence, the extreme learning machine randomly generates a large hidden layer and
only trains the linear readout. Interestingly, this very simple mechanism can yield good results for
practical applications [24]. This principle is very similar to reservoir computing, a technique to deal
efﬁciently with recurrent neural networks, which we will discuss below.

1.15.3 Recurrent Neural Networks
825
1.15.2.3.3
Does the network generalize to novel data points?
We have already discussed this issue in the context of how to choose an architecture: if the architecture
is too ﬂexible, it maps the training data correctly, but it can behave randomly on novel data points not
used for training. Therefore, in practice, the generalization ability of a network is often tested by means
of a hold out set which has not been used for training, the test set.
However, the question remains whether there exist any guarantees that a small training error also leads
to a small test error provided the training set is large enough. Such guarantees can be derived in the frame-
work of statistical learning theory: one can limit the “freedom” which is contained in a network with |W|
parameters. One popular quantity which measures this capacity of a network is offered by the so-called
Vapnik-Chervonenkis dimension of the function class [25]: The Vapnik-Chervonenkis dimension mea-
sures the combinatorial capacity of the architecture, i.e., the number of points for which every possible
classiﬁcation can be realized within the given neural architecture. The VC dimension of a sigmoidal
feedforward network can be limited by a polynomial in the number of weights |W| [26]. Depending on
this quantity, the deviation of the empirical error and the generalization error of the network for new
examples can be limited with high probability regardless of the underlying distribution [25].
This formalization and test of the ability of a neural architecture to learn an underlying regularity is
often referred to as the frequentist point of view, since the generalization error is limited by means of
statistical counting arguments. Apart from an explicit posterior bound or estimation of the generalization
ability obtained this way, it motivates another objective of neural networks training: a minimization of
the training error is often accompanied by additional mechanisms which explicitly reduce the degrees
of freedom of the network architecture. A popular example of this paradigm is given by a limitation
of the weight sizes, a mechanism which can be linked to Thikonov regularization in some cases.
Another example refers to a pruning of connections, weight sharing, or some other form of regularizer
incorporated in the cost function [1,27]. One particularly popular form of regularization is included in
training of the support vector machine or some forms of learning vector quantization, an optimization
of the margin of the classiﬁer, as we will explain below.
This frequentist point of view is complemented by a Bayesian treatment as explained, e.g., in the
seminal work [27]. Here, instead of focussing on just one architecture which, from a statistical point of
view often corresponds to a single parameter setting which is most likely given the current observations,
a full probabilistic model is inferred for the data and the range of possible parameters. This includes
an inherent regularization of the result such that overﬁtting does not occur. This beneﬁt is payed for
by a more complex training procedure where integrals corresponding to the inﬁnite number of possible
parameter settings have to be addressed. Here, a rich repertoire of techniques has been developed which
make this problem feasible ranging closed form solutions for speciﬁc cases to efﬁcient and general
Markov chain Monte Carlo techniques [28].
1.15.3 Recurrent neural networks
Feedforward architectures are characterized by acyclic graphs. Biological networks possess back-
connections. What happens, if we introduce cycles in a neural network? So-called recurrent neural
networks consist of neurons which are connected in an arbitrary cyclic graph. Due to these cycles, the

826
CHAPTER 15 Neural Networks
activation of neurons depends on each other, and a simple iterative deﬁnition is no longer possible,
rather, the dynamics of how the neurons are activated has to be taken into account.
There exist basically two different types of recurrent networks: fully recurrent networks which are
used as associative memory, and partially recurrent networks which are used to model dynamic systems
and to process time series.
1.15.3.1 A brief look at Hopﬁeld networks
Hopﬁeld networks are fully recurrent networks where the single neurons resemble perceptrons with
Heaviside activation function. Original Hopﬁeld networks are restricted to symmetric connections, i.e.,
wi j = w ji, without self-repression, i.e., wii ≥0. Because of the recurrence, outputs of neurons are time
dependent. Given a state o j(t) of the system at a point in time t, every neuron computes is activation
neti(t) =

j→i
w jio j(t) −θ j.
Typically, Hopﬁeld networks are driven in asynchronous mode, i.e., at every time point a neuron is
picked at random and changes its state according to
oi(t + 1) = H(neti(t))
if discrete time steps are considered. The output of all other neurons is not changed in this step.
Alternative to this dynamics, Hopﬁeld networks can be driven in synchronous mode, in which all neurons
change their state simultaneously according to the current input. This dynamics is particularly interesting
if a generalization to continuous time is considered and the dynamics is described by corresponding
differential equations.
It can be shown that, due to the symmetry of weights, Hopﬁeld networks converge towards a stable
pattern if driven in asynchronous mode, i.e., a state where no neuron changes its activation any more
[5]. This way, Hopﬁeld networks can be used as associative memory which assigns a completed pattern
to an initial, possibly disrupted initialization. For a synchronous update mode, cycles of length two can
occur in discrete time, but convergence is observed in continuous time.
Training Hopﬁeld networks refers to the task to store a given set of patterns as stable states of a
networks. A very fast training heuristic is offered by Hebbian learning: the weight wi j is set proportional
to the number of patterns for which the neurons i and j have identical activation. This learning rule works
reliably for orthogonal or nearly orthogonal patterns. In general, cross-correlations occur and the stored
patterns become disrupted.
A more general method consists in a reduction of Hopﬁeld training to a perceptron learning prob-
lem [5]. This procedure yields to a solution if existing. It also demonstrates a principled restriction
of Hopﬁeld networks: the patterns which can be stored by Hopﬁeld networks correspond to linearly
separable data. More general networks which also possess hidden neurons have been proposed: Boltz-
mann machines substitute the crisp binary-valued activation by probabilistic neurons which states are
distributed according to a Boltzmann distribution for a given temperature T [5]. Training refers to an
optimization of this distribution such that it approximates the uniform distribution on a given set of
pattern as much as possible as measured by the Kullback-Leibler divergence. Interestingly, Boltzmann

1.15.3 Recurrent Neural Networks
827
machines can be trained using hidden neurons, such that their storage capability is strictly enhanced as
compared to simple Hopﬁeld networks, and they can, in principle, represent every given set of patterns.
It turned out that Boltzmann machines have a large potential for the unsupervised training of the layers
in deep feedforward networks [16].
1.15.3.2 Recurrent neural networks
Partially recurrent networks have only a limited feedback which can be used to model temporal depen-
dencies, e.g., in dynamic systems. This way, temporal sequences can be addressed. The dynamic is
driven by the input sequence x(t) with temporal dependency t. A standardized dynamics is offered by
the following equation
z(t) = f (x(t), z(t −1)),
o(t) = g(z(t)).
x(t) is the input at time step t, z(t) describes the internal state of the recurrent network at time t, and f
and g are computed by standard feedforward neural networks. See Figure 15.3 for an example. These
dynamic equations can be generalized to a continuous dynamics by means of differential equations
which characterize the state change over time.
If such as network is used for time series prediction, a sequence (x(t), y(t)) of training patterns is
given. Similar to feedforward networks, training can take place by minimization of the mean squared
error E = 1/2 · 
t (y(t) −o(t))2. This can be done by a gradient descent, since the function is
differentiablewithrespecttotheweights.Thereby,thereexistbasicallytwodifferentmethodstocompute
the gradients within recurrent networks: back-propagation through time and real time recurrent learning.
f
g
FIGURE 15.3
Partially recurrent neural network in the Elman style, recurrent connections enable a processing of time
dependent inputs in their temporal context.

828
CHAPTER 15 Neural Networks
Back-propagation through time directly transfers back-propagation to recurrent networks. The key
observation consists in the fact that a recurrent network can be substituted by an equivalent feedforward
network by unfolding in time. Thereby, the weights are shared between the copies. The gradient with
respect to a weight wi j results from the sum of gradients with respect to all copies of wi j in this
unfolded network. These latter terms can be computed by means of standard back-propagation. The
time complexity is O(|W|·T ), T being the length of the sequence, and the required space is O(|W|·T ).
As an alternative, one can compute the derivatives with respect to the weights directly from left to right
in the unfolded network based on the recursive deﬁnition of the activation of neurons. This procedure
has time complexity O(|W|2 · T ) but the required space is independent of the maximum time length T.
In particular, the derivatives with respect to copies of wi j for early time steps are available independent
of the remaining time series. Hence, applying the changes according to early time steps directly, this
method can also be used in an online scenario. This method is referred to as real time recurrent learning.
As for training of feedforward networks, several questions occur: how do we choose the architec-
ture? In principle, partially recurrent networks are approximation complete in the sense that they can
approximate any dynamic system with continuous transition function on a ﬁnite time horizon [29]. For
a given training task, the architecture is chosen by cross-validation, as for feedforward networks.
What is the complexity of training recurrent neural networks? Since training feedforward networks
constitutes a special case of training recurrent networks, the problem is NP-hard in the worst case.
Besides this theoretical result, training recurrent networks turned out to be considerably more difﬁcult
than training feedforward networks. Already for small networks containing only two neurons, recurrent
neural networks can induce a rich dynamics which includes the full dynamic spectrum ranging from
stable states, periodic and quasiperiodic behavior, to chaotic systems [30]. Thereby stable behavior
need not be preserved in the course of learning [31] and additional effort has to be taken to guarantee
stability of the outcome [32]. Training using a gradient descent faces principled restrictions due to the
so-called problem of long-term dependencies [33]. This refers to the fact that error signals either blow
up within the neural network or vanish when back-propagated over time, such that they can hardly be
stored over a long time context. Speciﬁc recurrent architectures which address this problem include
long short term memory as a prominent example [34] where recurrence is restricted to linear neurons
for which gradients can safely be back-propagated.
1.15.3.3 Learning without training recurrent connections
An alternative for recurrent neural network training has recently become popular: in reservoir comput-
ing, the recurrent part of the network is not trained according to given supervised signals. Instead, a
sufﬁciently rich recurrent part is initialized, and a trainable readout is added to this state representation
which can efﬁciently be trained because the mean squared error for a linear function gives rise to a con-
vex cost function. Reservoir computing includes early approaches such as fractal prediction machines,
and the recent popular echo state networks and liquid state machines [17,35,36]. Here we only explain
the ﬁrst two, since the latter is based on spiking neurons, which are biologically more plausible, but
beyond the scope of this chapter.
Fractal prediction machines directly encode sequences over a ﬁnite alphabet by means of fractal
codes. Assume the number of symbols is s. A ﬁxed dimensionality of the internal state space is chosen
and the unit hypersquare in this space is decomposed into s regular hypersquares. A function fa is

1.15.3 Recurrent Neural Networks
829
a
b
d
c
d...
da..
FIGURE 15.4
Fractal encoding of sequences over a ﬁnite alphabet, fractal prediction machines encode symbolic sequences
in a real vector space in a similar way as recurrent networks with small weights. In the limit, time series give
rise to a fractal set in the encoding space.
associated with every symbol a. The function is the afﬁne mapping which transforms the unit square to
the square associated with a. Starting with a ﬁxed initial value 0, the sequence a1 . . . at can be encoded
as fa1 ◦· · · ◦fat (0). See Figure 15.4 for an example. Note that the set of points which are obtained by
encoding all possible sequences constitutes a fractal set in the considered space. The precision of the
location of a code determines the length of the sequence which can uniquely be decoded. On top of this
code, a standard network or any alternative classiﬁer can be trained. It has been shown in [36] that these
simple models are often as powerful as standard recurrent neural networks. In particular, they exactly
resemble the behavior of recurrent networks with small weights, i.e., recurrent neural networks during
the initial phase of training.
Echo state networks use a large state space dimensionality such that a rich reservoir is available. The
encoding function f of the recurrent network is given by a simple sigmoidal network with sparse random
connections of the neurons and random weights. Thereby, the echo state property must be fulﬁlled, i.e.,
the internal state of the network is independent of the initial condition if a long enough recurrence is
applied. This can be guaranteed, e.g., by a contractive transition function, which can easily be tested
considering the spectral radius of the linear part of the transition. This way, the recurrence constitutes
an encoding very similar to fractal encodings due to the echo state property, whereby a rich reservoir
accounts for sufﬁcient precision of the representation. On top of this code, a trainable (often even only
linear) network is added. Despite their simplicity, echo state machines constitute quite powerful models
with successful applications and biological plausibility [17].
It has been shown in [37] that the restriction of the recurrence to contractive mappings has beneﬁcial
effects with respect to the generalization ability of the network. For contractive networks, the generaliza-
tion ability does not depend on the dimensionality of the state space, but on the contraction coefﬁcient of
the transition function. Generalization bounds can be obtained which depend on this contraction coef-
ﬁcient and the number of examples used for training. For general recurrent networks, it is not possible
to obtain generalization bounds just in terms of the number of network parameters as shown in [38].
Unlike for feedforward networks, the concrete data distribution has to be taken into account to develop
generalization bounds for general recurrent networks. Concrete bounds which take the data distribution
(possibly as manifested in the training set) into account have been developed in [38], for example.
1.15.3.4 Connection to the Chomsky hierarchy
Interestingly, a very strong link of different types of recurrent networks to classical computing mech-
anisms exists. The motivation behind this fact can, among other things, be traced back to the question

830
CHAPTER 15 Neural Networks
whether recurrent networks constitute suitable models to explain the perception of natural language
by humans. Remarkable results on language processing with recurrent networks have been achieved
already by Elman [39], the debate about the suitability of recurrent models for language processing still
going on, see, e.g., [40–42].
Ithasrigorouslybeenshownin[37]thatrecurrentneuralnetworkswithsmallweights,i.e.,contractive
transition function are equivalent to deﬁnite memory machines, i.e., models which have only a ﬁnite
time horizon.
For larger weights, a strong connection to ﬁnite automata has been established, showing, on the one
hand, the possibility to simulate every automaton on an inﬁnite time horizon [43], on the other hand,
establishing methods for automata insertion prior to training and automata extraction after training
[44]. The capacity of recurrent networks with arbitrary weights, however, is larger. Several approaches
succeeded in training networks on simple context free resp. context sensitive languages which include
a counting mechanism [34,45]. This is particularly interesting since natural language is believed to be
somewhere between the class of context free and context sensitive languages.
When neglecting learnability of the languages, one can even go a step further and connect recurrent
networks to general Turing machines. The notion of simulation of possibly non halting computations
with Turing machines in polynomial time has to be established. For this purpose, the Turing band is
encoded in the activation of several speciﬁc neurons, and a speciﬁc neuron is identiﬁed which indicates
the halting of a computation. Under this condition, it has been shown in [46] that neural networks with
rational weights and the so-called semilinear activation function (lin(x) = 0 for x ≤0, lin(x) = x for
x ∈[0, 1], lin(x) = 1 for x ≥1) can simulate every Turing machine. A similar result (but requiring
exponential time) has been shown for sigmoidal recurrent networks in [47]. Interestingly, recurrent
networks with arbitrary (irrational) weights are even more powerful: sigmoidal networks can compute
every function (in exponential time) on unary inputs [48]. Recurrent networks with arbitrary weights
and the semilinear activation function are in fact equivalent to non-uniform Boolean circuits [46], a
class which is beyond Turing computability. Of course, these functions cannot, in principle, be learned
from a ﬁnite training set, such that the relevance of these results is more of theoretical interest.
1.15.3.5 Recurrent networks for structure processing
Partial recurrent neural networks can be used for time series processing, e.g., language, sensor data,
ﬁnancial time series, etc. Thereby, the structure of the data coincides with the dynamics of the network
due to the time dependency of the series. A variety of generalizations of recurrent networks has been
proposed which extend the applicability to more general data structures.
1.15.3.6 Spatial data
Spatial data such as DNA sequences differ from time series in their causality: for time series, strong
causality holds in the sense that the future depends on the past but not vice versa. For spatial sequences,
entries at the beginning or the end might inﬂuence each other without any restriction. Thus, if sequences
are processed by recurrent networks, a causality assumption which might not be correct is assumed.
In particular, the result of the processing can be different depending on the fact whether sequences are
processed from the right or the left. Because of this fact, bicausal neural networks have been proposed in
the approach [49]. These networks consist of two recurrent neural networks which process the sequence

1.15.3 Recurrent Neural Networks
831
T
C
G
A
A
T
...
...
FIGURE 15.5
Bicausal recurrent network for spatial sequence processing, e.g., DNA sequences. Due to two “directions”
of the processing, spatial dependencies can be captured by these networks.
simultaneously from the left resp. from the right. The readout processes the output of both networks
together such that, at this position, full information is available. The network is trained as a whole, i.e.,
full information is available also during training, see Figure 15.5. This method has successfully been
used for splice site recognition and various tasks in protein structure prediction [49–51].
Another alternative model has been proposed in [52]. This is based on so-called recursive cascade
correlation. Cascade correlation constitutes a method to simultaneously obtain the architecture and the
weights of feedforward networks. It starts with a single output neuron without hidden neurons which is
trained on the given training task. Then the following procedure is repeated until the classiﬁcation result
is satisfactory: a hidden neuron is added and connections from the input neuron and all existing hidden
neurons to the new one are drawn. These connections are trained such that the correlation of the output
of this hidden neuron and the error of the network as exists so far is maximized. The idea behind this
procedure is that the hidden neuron can then be used for error correction. After training the connections
to the hidden neuron are frozen. After introducing a hidden neuron, all connections to the output are
trained such that the classiﬁcation error is minimized. This procedure is repeated, adding the necessary
number of hidden neurons while training the network.
The same procedure can be introduced for recurrent networks whereby a hidden neuron is pointed to
by recurrent connections from the neuron itself and all previous hidden neurons. This way, we can intro-
duce functional dependencies not only from the activation in the given time step, but also the previous
time step. The principled training procedure is the same as for simple cascade correlation, whereby error
signals are back-propagated through the recurrent connections as in back-propagation through time.
A recurrent cascade correlation network (RCC), unlike a simple recurrent network, possesses
restricted recurrence since no recurrent connections from hidden neurons pointing to neurons intro-
duced earlier are available. This restriction has the effect that RCC is strictly weaker than full recurrent
networks since, as shown in [53], RCC networks cannot represent all ﬁnite automata, whereas recurrent
networks can. However, this restriction has the effect that recurrent connections which refer to later
time steps can be introduced as long as this is restricted to hidden neurons which are already frozen.

832
CHAPTER 15 Neural Networks
The resulting dynamic deﬁnition is acyclic. These networks, contextual recurrent cascade correlation,
have been successfully introduced in [52]. They are trained in the same way as RCC networks, but
integrate more context suitable for spatial data. It has been shown in [54] that, although these networks
are weaker compared to recurrent networks when considering approximation for arbitrary size of the
sequences, they are approximation complete when considering approximation in probability. Thus, they
can approximate every possibly non-causal function on sequences up to sets of small probability.
1.15.3.7 Tree structures
The idea of a time series driving a recurrent network can immediately be transferred to tree structures
with limited fan-out since they possess a similar recursive structure. Assume v is a vertex of a tree with
label l(v), and l and r are the left and right subvertex, respectively. Then the state of a recursive network
is deﬁned as
z(v) = f (l(v), z(l), z(r)).
Thus, starting from the leaves of the tree, an output for every vertex of the tree can be obtained by
recursive processing of the vertices within the context of their children, see Figure 15.6. These recursive
networks have been proposed, e.g., in [55] and successfully been applied to various tasks including
ﬁngerprint recognition, logo recognition, classiﬁcation chemical structures, etc. Interestingly, they con-
stitute universal approximators for mappings on tree structures to real vectors as shown in [56].
1.15.3.8 Graph structures
RCC can be extended to recursive cascade correlation (RecCC) by using the same recurrence which
takes the context of all children of a vertex into account, not just one single successor. It has been shown
in [54] that RecCC is approximation complete for tree structures when considering approximation in
Context
Context
b
c
a
Context
Input
input tree 
a(b,c)
FIGURE 15.6
Unfolding of a recursive neural network for a given tree structure. The dynamics of standard recurrent
networks is extended to tree structures with a ﬁxed fan-out of the vertices.

1.15.4 Learning by Focussing on the Generalization Ability
833
probability, as beforehand. Interestingly, contextual processing can be added to RecCC in the same way
as CRCC: each hidden neuron is equipped not only with recurrent connections referring to the children,
but also recurrent connections referring to its parents pointing from all hidden neurons introduced
previously. This way, we cannot only process tree structures, but also acyclic graphs with contextual
cascade correlation (CRecCC). Interestingly, one can show that such networks are approximation
completeforfunctionsontreestructuresaswellasforacyclicgraphswhichcanbepairwisedistinguished
by at least one path in the graph [54], thus the resulting architecture is quite powerful.
Another approach has been proposed in [57]. The graph neural network processes every vertex in
the context of its neighbored vertices in the graph. That means, the activation of a neuron for a speciﬁc
vertex in the graph is computed as function depending on the labeling of the vertex, and the activation
of the neurons for the neighbors of the vertex. Since a graph is usually cyclic, hence activations of
neighbored vertices inﬂuence each other, this deﬁnition depends on the ordering of the computation of
the activations. In the graph neural network, the functions implemented by the neurons are restricted to
contractive mappings. This way, the standard Banach ﬁxed-point theorem guarantees that, in the limit,
a unique activation is computed for every neuron and, thus, the overall dynamics is well deﬁned. In
[57], a training algorithm based on gradient descent is proposed which propagates the error signals of
stable states of the network. This way, an efﬁcient training procedure arises. Graph networks have been
successfully applied, e.g., in the context of web page rankings and applications to chemistry.
1.15.4 Learning by focussing on the generalization ability
So far, networks have been trained in the following way: a ﬁxed architecture and training algorithm is
chosen. Then, training aims at a minimization of the error on the training set. This method is referred
to as empirical risk minimization [25] since, after a limitation of the principled function class by the
architecture and training algorithm, the aim is to get an empirical error as small as possible. However, we
are not interested in the empirical error, rather the generalization error, i.e., the behavior of the network
for unknown examples should be considered. Thus, the goal is twofold: minimization of the empirical
error and minimization of the structural freedom of the function beyond the training set.
There exist alternative training methods which explicitly address the so-called structural risk during
training, i.e., the degrees of freedom which are not necessary to suit the training set.
1.15.4.1 Support vector machine
One of the most popular methods based on structural risk minimization is offered by the support vector
machine (SVM). First, we consider a linear SVM, which is just a simple perceptron. For simplicity, we
neglect the bias (which is often not necessary in practical applications). There are two different methods
to limit the generalization ability of a perceptron: on the one hand, the number of parameters (the more
parameters, the more data points are necessary to determine the parameters). On the other hand, we can
consider the so-called margin which is the minimum distance of a training point from the separating
hyperplane (see Figure 15.7). The larger the margin, the more tolerance with respect to noise in the
training patterns can be observed [25].

834
CHAPTER 15 Neural Networks
margin
support vectors
FIGURE 15.7
A linear classiﬁer with maximum margin; the support vector machine uses the margin trick to reach good
generalization also in high dimensional feature space.
1.15.4.1.1
Primal and dual problem
The margin can be parameterized as 2/∥w∥, w being the weight vector of the perceptron. Therefore,
maximizing the margin is equivalent to a minimization of ∥w∥2/2. Assume patterns have the form
(x p, y p) with y p being in {−1, 1} (instead of {0, 1}). Then x p is mapped correctly (w.l.o.g. we can
assume that the activation of the perceptron is of absolute value at least one) if and only if y p ·wt x p ≥1.
This yields to the primal formulation of SVM training for a linear SVM without bias:
minimize ∥w∥2/2
such that
y p · wt x p ≥1 for all p.
Usually, this problem is transferred to the equivalent Wolfe dual problem using the standard Karush-
Kuhn-Tucker conditions of quadratic programming [25]:
maximize

αi −1/2 ·

αiα j yi y j(xi)t x j
such that αi ≥0.
The Lagrange parameter αi will become 0 for all points but the so-called support vectors which are
vectors closest to the separating hyperplane. The hyperplane can be recovered from the Lagrange
parameters by w =  αi yi xi.
1.15.4.1.2
Adatron
Usually, this optimization problem is solved using, e.g., efﬁcient interior point methods. However, a
very simple optimization scheme is offered by the so-called adatron algorithm [58], a gradient ascent
of the dual cost function which takes the constraints into account:
init αi = 1
repeat

1.15.4 Learning by Focussing on the Generalization Ability
835
αi := αi + η ·

1 −yi 
j α j y j(x j)t xi
αi := max{αi, 0}
This procedure ﬁnds a solution of the perceptron problem which optimizes the margin. Unlike the
solution provided by the perceptron algorithm, the solution of a SVM is unique due to the regularization
in terms of the margin. The result is robust with respect to its generalization ability. It has been shown
in [25] that the VC dimension, i.e., a measure for the degree of freedom within the function class as
introduced above, scales inverse with the margin. Thus, the larger the margin, the better theoretical
generalization bounds In addition, the solution with maximum margin turns perceptron training into an
efﬁcient polynomial problem: the dual optimization problem can be solved in time O(n3), n being the
number of points.
1.15.4.1.3
Alternative formulations
So far, the training algorithm ﬁnds a solution if and only if a separating hyperplane without errors exists.
In general, this is not possible. For this case, the optimization problem for SVM training can be relaxed
introducing slack variables ξp:
minimize ∥w∥2/2 + C

p
ξp
such that
y p · wt x p ≥1 −ξp,
ξp ≥0 for all p.
where C is a positive constant which regulates the number of allowed misclassiﬁcations. The Wolfe
dual yields
maximize

αi −1/2 ·

αiα j yi y j(xi)t x j
such that C ≥αi ≥0.
That is, the size of the Lagrange variables is limited to prevent divergence. Support vectors are the
vectors where the Lagrange parameter is non vanishing which includes points closest to the separating
hyperplane (αp < C) and misclassiﬁed points (αp = C).
Note that, again, due to the regularization, the NP hard problem of training a perceptron in case of
errors is substituted by a polynomial optimization problem. This complexity can even be improved.
Working set techniques such as sequential minimal optimization [59] can severely speed up SVM train-
ing in practical applications, however, without formal guarantees concerning their computational com-
plexity. Provably linear techniques rely on approximation algorithms by means of core set techniques,
see, e.g., [60]. There exists a variety of further SVM training schemes which address different problems
than simple classiﬁcation such as regression problems, ranking problems, novelty detection, etc. [61].
1.15.4.2 Kernels
Similar to the simple perceptron the linear SVM is appropriate only for those training sets which are
(almost) linearly separable. To extend the applicability, the so-called kernel trick is used. The basic

836
CHAPTER 15 Neural Networks
idea is to include a ﬁxed nonlinear preprocessing 	 of the data points before separating the nonlinearly
transformed points. If the dimensionality of the feature space is large enough and the preprocessing is
sufﬁciently nonlinear, the image points 	(x p) become linearly separable [62,63].
Two problems arise: on the one hand, the dimensionality becomes large such that the number of free
parameters is large and the generalization ability might become weak. This problem is prevented by the
fact that the hyperplane with maximum margin is chosen.
1.15.4.2.1
The kernel trick
On the other hand scalar products have to be computed in the feature space. For high dimensionality,
this becomes ineffective. To prevent this problem, the kernel trick is used: only those 	 are chosen for
which a mapping k exists with k(x, y) = 	(x)t	(y), i.e., k provides an alternative efﬁcient computation
method for the scalar product of the nonlinear images of two vectors. Note that 	 is used within SVM
training and evaluation only within a scalar product such that the knowledge of k is sufﬁcient for SVM
training. Functions k which fulﬁll the equality k(x, y) = 	(x)t	(y) are called kernels. Interestingly,
one can check whether a function is a kernel without an explicit knowledge of 	, using, e.g., the so-called
Mercer condition [61].
Popular kernels include
1. The polynomial kernel k(x, y) = (xt y + 1)d, d denoting the degree. This kernel corresponds to a
map 	 which builds all monomials of the input dimensions up to degree d, i.e., the dimensionality
of the feature space scales exponentially in the input dimensionality.
2. The Gaussian kernel k(x, y) = exp ( −∥x −y∥2/σ 2), σ being the width of the Gaussians. This
kernel corresponds to a map 	 with inﬁnite dimensional feature space.
Recently, a kernel inspired by extreme learning machines has been proposed in [64]. The extreme
learning machine kernel relies on the idea to randomly map data into a feature space by means of a
layer of randomly wired hidden neurons. Interestingly, under certain assumptions, the resulting dot
product in feature space can be computed explicitly for the limit of an inﬁnite number of neurons.
1.15.4.2.2
Structure kernels
Kernels constitute the interface of the data to the SVM. Thereby, the data need not be restricted to real
vectors, rather, every space for which a kernel can be deﬁned is approrpiate. A variety of kernels for struc-
tural inputs including sequences, trees, and graphs have been proposed. Mathematical closure properties
as established in [65,66] allow to design kernels based on a variety of principles including the reference
to projections or substructures and the combination of such procedures. Two different approaches to
design structure kernels are very popular: kernels which count the number of common occurrences of
substructures, and kernels which base the comparison of structures on probabilistic models.
Substructure kernels include, for example, the string kernel and the spectrum kernel for sequences,
which count common subsequences, whereby the variants differ in the fact whether subsequences need
to be contiguous or not and whether exact matches are required [67,68]. In principle, given two strings
s1 and s2, these kernels compute a term 
s⊂ks1,s′⊂ks2 d(s, s′) whereby s ⊂k s1 refers to some formalism
which speciﬁes that s is substring of s1 (contiguous or noncontiguous) and its length is limited to k.
d(s, s′) deﬁnes a simple kernel on strings of the same length. The main problem of this deﬁnition

1.15.4 Learning by Focussing on the Generalization Ability
837
consists in the task to design an efﬁcient computation method for this kernel, since the number of
possible substrings increases exponentially with the length k. Basically two ways have been proposed
which allow a rather efﬁcient computation: dynamic programming or sufﬁx trees. Generalizations to tree
structures are possible as described in [69,70]. Similar procedures for graph structures face problems
because of the complexity of graph isomorphism. Here, an alternative is given by just comparing paths
in a graph. The diffusion kernel constitutes a concrete method to realize this idea [71].
One of the most popular kernels based on probabilistic models is the Fisher kernel [72]. The compar-
ison of two structures is substituted by the comparison of relevant quantities of statistical models ﬁtted
to the data. In the case of the Fisher kernel, a manifold of parameterized generative models is considered,
e.g., hidden Markov models in the case of sequences. Each data point stems from a most likely parameter
setting. The Fisher kernel compares the tangent vectors in these points instead of the structures itself,
whereby the natural metric to compare these tangents is based on the Fisher information.
1.15.4.3 Learning vector quantization
The SVM provides solutions in terms of support vectors which are points at the boundary or misclassiﬁed
examples, i.e., the SVM expands solutions in terms of atypical training points. There exist alternatives
such as the relevance vector machine which selects solutions in terms of typical points, based on a
statistical formulation of the training problem [73]. However, we would like to consider an even simpler
method which represents solutions in forms of typical points or prototypes: learning vector quantization
(LVQ) [12]. An LVQ network consists of prototypes which are represented by prototypical locations
wi in the input space and their output class c(wi) which is one of a ﬁnite number of possible output
classes. An LVQ network computes a classiﬁcation by means of a winner takes all mechanism:
x →c(wi) for which ∥x −wi∥is minimal,
∥·∥denoting the euclidian metric, see Figure 15.8 for an example. A variety of different intuitive learning
algorithms have been proposed which are commonly based on the principle of Hebbian learning and
which constitute heuristics. The simplest one, LVQ1, can be described in a few lines:
init wi
repeat
choose a training pattern x with output class y
determine the closest prototype wi
if y = c(wi) adapt wi = wi + η(x −wi)
if y ̸= c(wi) adapt wi = wi −η(x −wi)
Figure 15.9 depicts a learning step.
There exists a variety of alternatives to account for faster convergence or better adaptation to the
optimum decision boundary. Only comparably little is known about theoretical guarantees for LVQ such
as the convergence of the algorithm. Recently, several prototypical scenarios have been investigated
within the framework of so-called online learning [74] which demonstrate the divergence of some
algorithms (e.g., LVQ2.1) also in quite simple situations. We would like to introduce one alternative
which can be derived as stochastic gradient descent method of a cost function and, therefore, shows
quite good numerical stability, generalized LVQ (GLVQ)[75].

838
CHAPTER 15 Neural Networks
FIGURE 15.8
An LVQ network: the classiﬁcation is based on the winner-takes-all rule,this way decomposing the input
space into receptive ﬁelds.
FIGURE 15.9
Training of an LVQ network by Hebbian learning; given a data point, its closest prototype is moved
towards/away from the data point depending on whether the classiﬁcation is correct.
Assume patterns (x p, y p) are available. Denote by w+(x p) the prototype closest to x p with the
same label and by w−(x p) the closest prototype with a different label. Then, GRLVQ minimizes the
cost function
1
2

p
∥x p −w+(x p)∥2 −∥x p −w−(x p)∥2
∥x p −w+(x p)∥2 + ∥x p −w−(x p)∥2
by means of a stochastic gradient decent. In some cases, the summands are nonlinearly transformed by
a sigmoidal function. Note that the term ∥x p −w+(x p)∥2 −∥x p −w−(x p)∥2 is negative if and only
if w p is correctly classiﬁed, thus, the nominator accounts for a correct classiﬁcation of the data points.
The denominator scales this term such that it lies in the interval ( −1, 1).
We use the shorthand notation d+ for ∥x p −w+(x p)∥2 and d−for ∥x p −w−(x p)∥2. Taking the
derivatives yields the following update rules which are also valid for the classiﬁcation boundaries, as
shown in [76]:
w+(x p) := w+(x p) + η ·
2d−
(d+ + d−)2 · (x p −w+(x p))
for the closest correct prototype and
w−(x p) := w+(x p) −η ·
2d+
(d+ + d−)2 · (xp −w−(x p))
for the closest incorrect prototype, i.e., this method differs from standard LVQ mainly by different
scaling terms of the step size. The method is often more robust and yields a better classiﬁcation accuracy
compared to simple LVQ [76].

1.15.4 Learning by Focussing on the Generalization Ability
839
ForLVQtypenetworks,stronggeneralizationboundsholdintermsoftheso-calledhypothesismargin
of the system. Consider the difference d−−d+, the hypothesis margin for pattern x p. Obviously, the
larger the term, the more secure the classiﬁcation of x p. For large margin, the hypothesis can be altered,
i.e., the classiﬁcation boundary can be shifted, without affecting the classiﬁcation of x p. For small
hypothesis margin, already a small shift of the hypothesis will alter the classiﬁcation. It has been shown
in [77] that, for a two-class classiﬁcation problem, the generalization error of LVQ type networks can
be limited by the following inequality:
E ≤ˆEρ + O
 P2B3√ln (1/δ)
ρ√m
+
√ln (1/δ)
√m

,
where P is the number of prototypes, B the size of the support of the training set, δ the conﬁdence of the
bound, m the number of examples, and ρ a priorly ﬁxed constant. E denotes the error of the classiﬁer for
unknown training samples, i.e., the error we are interested in, and ˆEρ denotes the number of examples
which are classiﬁed with margin smaller than ρ or misclassiﬁed, divided by m. Hence, the larger the
margin achieved by an LVQ classiﬁer, the better the generalization.
Note that GLVQ includes the margin in the cost function, hence it directly aims at margin maximiza-
tion during training, achieving excellent generalization ability.
1.15.4.4 General metrics
Note that, in principle, every differentiable metric d can be used in GRLVQ instead of the euclidian
one. Substituting this metric into d+ and d−and taking the derivative directly yields to adaptation
formulas for training as shown in [78]. The metric can, for example, be derived from an arbitrary kernel
(in which case the generalization bounds are still valid), or any other similarity measure (for which
the generalization bounds do not necessarily hold.) The choice of the metric as the standard euclidian
metric enhanced by relevance terms proved particularly suitable for practical applications:
dλ(x, w) =

i
λi(xi −wi)2,
whereλi ≥0and λi = 1.Thequantityλi scalesinputdimensioniaccordingtoitsrelevance,therefore
the algorithm is referred to as generalized relevance LVQ (GRLVQ) [76]. Thereby, the relevance of the
dimensions need not be known a priori, rather, the relevance terms can automatically be determined
during training by applying a stochastic gradient decent with respect to both, the prototypes and the
relevance terms. The resulting updates for the relevance terms are
λl := λl + η ·

p

4d−
(d+ −d−)2 · (x p
l −w+
l (x p))2 −
4d+
(d+ −d−)2 · (x p
l −w−
l (x p))2

.
After every adaptation step, the relevance terms have to be normalized. Note that the generalization
bounds are also valid for this more general approach as shown in [77].
A variety of further alternatives have been proposed and successfully applied in practice, whereby
the metrics can be equipped with adaptive relevance parameters which are automatically adapted during
training by means of a gradient descent. Examples for alternative metrics include speciﬁc choices for
time series [78,79] or a full adaptive matrix [80].

840
CHAPTER 15 Neural Networks
1.15.5 Unsupervised learning
So far, we have considered classiﬁcation or regression tasks, i.e., input-output pairs have been available.
In unsupervised learning, only input vectors are given and the task is to extract useful information from
these data. This problem is, of course, rather vague and a variety of different application domains exists:
data preprocessing and dimensionality reduction, data visualization, data clustering, data compression,
data mining, etc. We will consider a few popular approaches within this framework.
1.15.5.1 Unsupervised training of single neurons
Assume a stream of vectors x p is given. For simplicity, we assume that the expectation E(x p
i ) of each
component is 0, otherwise, we can easily normalize every dimension. We assume that a single neuron
is given with the identity as activation function and bias 0, i.e., it computes the function
x →y = wt x.
Since we do not exactly know what we should do, we can formulate simple Hebbian learning in this
scenario
w := w + η · y · x,
which, for positive output, reinforces those weights of the neuron which receive a positive input signal,
for negative y, it is vice versa. However, unfortunately, this simple rule diverges, i.e., the size of w
becomes inﬁnite.
1.15.5.1.1
Oja’s rule
Oja’s rule introduces an additional correction to Hebb learning [5]:
w := w + η · (y · x −y2 · w).
What is the behavior of this dynamics? We deﬁne the correlation matrix C by E(xxt). For a ﬁnite set
of samples this is proportional to the sum 
p x p(x p)t. Now consider the function −1/2 · wtCw. A
stochastic gradient descent of this function yields an update proportional to xxtw = yx, i.e., it yields
the Hebb term as introduced above. Obviously, the above function is smaller the larger the size of w,
hence the divergence. One can show that the additional correction term introduced by Oja’s rule also
optimizes this function, but it restricts the solution to ∥w∥= 1 [5].
Thus, Oja’s rule leads to a vector e with ∥e∥2 = 1 such that etCe is maximum. The matrix C is a
symmetric positive (semi-)deﬁnite matrix, therefore it can be diagonalized, i.e., there exist n eigenvectors
ei with ∥ei∥2 = 1 and eigenvalues λi such that Cei = λi ei, and the matrix C is the diagonal matrix
with entries λi when represented in the base ei. (Here we assume that C is positive deﬁnite and the
eigenvalues are pairwise different.) The optimum e is nothing else but the eigenvector corresponding
to the largest eigenvalue λ1.
The decomposition of C into its eigenvectors is also called principal component analysis (PCA) and
the eigenvectors are the principal components. They have an intuitive meaning: the principal components
are pairwise orthogonal, and the ﬁrst principal component is the direction of the data with largest

1.15.5 Unsupervised Learning
841
second principal component
first principal component
FIGURE 15.10
Principal components of a data set: the directions with maximum variation are captured this way.
variance, the second one is the direction with second largest variance, and so on, see Figure 15.10.
Therefore, PCA is often used for data preprocessing and data compression since they preserve as much
information in terms of variances of the points as possible. Oja’s rule does nothing else but extract the
dimension with maximum information in this sense.
Extensions of Oja’s rule to extract more than only the ﬁrst principle component exists by repeated
application of Oja’s rule and decorrelation of the directions, e.g., Sanger’s rule.
1.15.5.1.2
Independent component analysis
Independent component analysis (ICA) [81] is, in some sense, dual to PCA. As for PCA, n-dimensional
patterns x are observed, which are generated by unknown sources s. Both, PCA and ICA assume that
the observations are generated by a linear mapping x = A · S with A = W −1, W denoting the weights
of the trained neurons. The goal of PCA is to recover these sources under the assumption that they are
pairwise orthogonal and they explain as much information as possible with respect to the variance, which
is relevant, e.g., for compression. ICA tackles the problem that the sources are mixed as in real life,
e.g., the mixing of different speakers in a cocktail party where the original speakers should be retrieved
from a mixture of different speakers. These signals are usually not orthogonal. The assumption of ICA
is, that the signals are (almost) independent (and non Gaussian since, otherwise, the mixture would just
look the same).
The task of ICA is, thus, to retrieve the sources s = A−1x from a mixed signal x such that the sig-
nals s1, s2, … are as independent as possible. See Figure 15.11 for an example where two independent
uniformly distributed sources are mixed, here, the task is to retrieve the two directions of the parallel-
ogram. Now the question occurs how to measure independence? It is assumed that the expectation of
each component of x is 0 and the variance is 1, further, it is assumed that they are uncorrelated. If these
assumptions are not fulﬁlled, data have to be preprocessed accordingly, so called whitening.
Independence of sources can be measured by the mutual information I(x) =

i H(xi) −
H(x1, . . . , xn) where H(x) = − P(xi) log P(xi) denotes the entropy. For whitened data it holds
I(x) = const −
i J(xi) where J(x) = H(xG) −H(x) is the negentropy which compares the vari-
able x to a Gaussian variable xG. Thus, minimizing the mutual information is the same as maximizing
the negentropy, i.e., minimizing the Gaussianity of a source. Thus, we have to ﬁnd sources which are

842
CHAPTER 15 Neural Networks
FIGURE 15.11
Linear mixture of two independent uniform distributions; unlike principal component analysis, independent
component analysis aims at retrieving these diemensions.
minimum Gaussian to ﬁnd independent sources. A classical measure for the Gaussianity of a variable
is the kurtosis kurt(x) = E(x4) −3E(x2)2. This is maximized by one of the ﬁrst ICA methods by
means of a simple gradient ascent, obtaining the ﬁrst independent component. Proceeding this way, all
independent components can be obtained.
However, the optimization of kurtosis is not very robust. Therefore, alternatives have been proposed,
among them fast-ICA, which approximates the negentropy and maximizes this approximation by a fast
ﬁxed point method resulting in the update:
w := E(xg(wt x)) −E(g′(wt x))w
followed by normalization where x is the whitened signal and g a non quadratic function such as
g(x) = −exp ( −x2/2).
1.15.5.1.3
Nonlinear components
Both, PCA and ICA retrieve linear mixtures. This gives quite remarkable results in a variety of applica-
tions such as blind source separation of signals. However, in practice, there is no guarantee that sources
are mixed in a linear way. Therefore, both ICA and PCA have been extended to nonlinear methods, e.g.,
introducing kernelization [61].
Here, we shortly mention another very intuitive method. Assume, as before, signals x are given. PCA
ﬁnds directions such that as much information as possible is preserved, i.e., one can think of PCA as a
form of compression or dimensionality reduction. Consider a simple feedforward network with n inputs
and outputs and one hidden layer with n′ < n neurons, n being the dimensionality of the data. Assume
this network is trained such that it approximates the identity on a given set of patterns x p. Since n′ < n,
the hidden layer serves as an encoder layer where the information of the data has to be compressed.
Thus, the transformation of the hidden neurons likely resembles directions similar to the ﬁrst principle
components, since these directions allow a fairly effective encoding. Naturally, transformation within
the vector space spanned by the ﬁrst components do not alter this result. This idea can immediately
be transferred to nonlinear directions: instead of just one hidden layer, more than one hidden layer is
added. This way, the encoding function becomes nonlinear.

1.15.5 Unsupervised Learning
843
1.15.5.2 Slow feature analysis
Another recent approach to blind source separation and, more generally, to retrieve semantically mean-
ingful information in a stream of observations is offered by slow feature analysis (SFA) [82]. The idea
is to nonlinearly transform time dependent input signals such that the results have small variation over
time, the function depends on one time step only, and the results are orthogonal to avoid trivial solutions.
This way, the technique is forced to extract signals which do not change quickly over time; these signals
correspond to potentially meaningful semantic entities in the given data. Under certain assumptions
on the form of the nonlinear transformation, the problem can be solved algorithmically by a standard
eigenvalue problem for preprocessed signals. Slow feature analysis has proven beneﬁcial in diverse
areas ranging form brain computer interfaces to image processing.
1.15.5.3 Training topographic maps
Unlike PCA and ICA, topographic maps combine several neurons which are trained in an unsupervised
way, whereby the neurons are cooperating. This way, complex behavior such as topographic ordering
according to local PCA directions can be achieved.
1.15.5.3.1
Self organizing map
The self-organizing map (SOM) has been proposed by Kohonen [12] as a plausible learning method
to achieve a topographic mapping of signals as can be observed in the human cortex. A SOM consists
of a number of neurons which are equipped with a weight wi ∈Rn representing the typical sensor
signal for this neuron, and a neighborhood structure of the neurons n(i, j). Often, the neighborhood
structure is induced by a regular lattice structure such as a square, hexagonal, or hyperbolic lattice. A
SOM computes a winner takes all function just as LVQ networks x →i where ∥x −wi∥is minimum.
Thus, a new signal is mapped to a position in the map.
1.15.5.3.2
Online training
Training takes place such that a topographic mapping is found which faithfully represents the input
signals. Online SOM training differs from LVQ in the fact that the neighborhood structure is taken into
account:
init
repeat
choose x
determine the winner neuron wi0
adapt all neurons wi := wi + η · exp ( −n(i, i0)/σ 2) · (x −wi)
decrease η and σ
Hence, all neurons are adapted whereby the strength of the adaptation is deﬁned by a Gaussian function
around the winning neuron. σ deﬁnes the neighborhood range of the adaptation.
Neighbored neurons in the map represent neighbored regions in the space of stimuli. If a two-
dimensional map is used, a SOM can be used for data visualization: data are assigned to the place in
the plane which corresponds to the respective winning neuron.

844
CHAPTER 15 Neural Networks
The degree of topology preservation of the resulting map can be measured using, e.g., the topographic
product (if the curvature of the manifold is not too strong) or the topographic function [83]. Both
measures relate the neighborhood structure in the lattice space to the neighborhood structure in the
input space.
Despite the simple training model, the mathematics of SOM is rather difﬁcult and, in several parts,
still unexplored [84,85]. Important topics concern the convergence of SOM to a topology preserving
state, and the relation of the weight distribution to the data density. For both questions, only preliminary
solutions (e.g.,for low dimensional settings or constant learning rate) could be found so far. If the notion
of the winner of x is changed to the neuron w(x) = i with average smallest distance, i.e., smallest

j exp (−n(i, j)/σ 2)∥w j −x∥2, a cost function which is also valid for continuous input distributions
can be found [86]
	 
i
δi
w(x)

j
exp ( −n(i, j)/σ 2)∥w j −x∥2P(x)dx.
δ j
i denoting the Kronecker symbol. In this case, a version of SOM with a slightly changed winner
notation constitutes a stochastic gradient descent of this cost function.
1.15.5.3.3
Batch training
SOM training can be accelerated by an alternative optimization scheme, batch training. This assumes
that training data xi are given a priori. Then batch optimization proceeds as follows:
init
repeat
compute the winner w(x p) = argmini

j exp ( −n(i, j)/σ 2)∥w j −x p∥2 for every x
compute the weights wi =

j exp (−n(i,w(x j))/σ 2)x j

j exp (−n(i,w(x j)/σ 2)
decrease σ
It has been shown in [87] that this procedure converges in a ﬁnite number of steps towards a (local)
optimum of the cost function deﬁned by Heskes [86]. However, this procedure is quite sensitive to
initialization due to the rapid convergence. Therefore, the weights should be initialized properly, e.g.,
along the ﬁrst two principal components of the data.
Interestingly, batch training can be transferred to more general metrics by the so-called generalized
median. Assume only pairwise distances d(xi, x j) are available, but no embedding vector space of the
data. The median SOM optimizes a lattice by restricting prototype locations to the location of example
data points. The distance of prototypes and data points can be easily computed this way, and the winner
of every data point can be determined as beforehand. Instead of an explicit formula for wi, the prototype
wi is chosen as the location xl for which 
j exp ( −n(i, w(x j))d(xl, x j) is minimum. This algorithm
also converges in a ﬁnite number of steps [87].
1.15.5.3.4
Consciousness
For standard SOM, the ﬁnal prototypes are arranged within the input space. Thereby, the number of
input signals for which a neuron becomes winner is usually not the same for different prototypes.

1.15.5 Unsupervised Learning
845
Rather, regions of the data space with few input stimuli are emphasized, i.e., neurons which are located
in regions of the data space with sparse coverage are less often winner compared to neurons in densely
coveredregions. Thisissuboptimalfromaninformationtheoreticpointofview: prototypesareusedinan
optimum way if the winner frequency is the same for every neuron. There exists a variety of mechanisms
which prevent this behavior and which introduce so-called magniﬁcation control in neural training [88].
A very popular and fast heuristics is offered by the de Sieno-algorithm [89]. During online training,
each neuron adapts its mean winner frequency after the presentation of pattern x by
pi := pi + B · (δi
w(x) −pi),
where 0 < B ≪1. The winner is determined based on the distance which is enhanced by a conscious
mechanism. Denote the number of neurons by N. The optimum winner frequency is 1/N. The distance
is altered by a term which relates this optimum frequency to the current one, emphasizing neurons with
low winner frequency. Thus, the winner is
w(x) = argmini∥x −wi∥2 −C · (1/N −pi),
where C > 0 is a constant. The size of C allows to determine the strength of consciousness learning in
comparison to the standard Kohonen rule.
1.15.5.3.5
Neural gas
SOM is beneﬁcial if data should be visualized. However, if only a clustering or topographic map should
be obtained, the choice of a ﬁxed prior topology limits the quality of the result in case this topology does
not coincide with the data topology. Neural gas (NG) constitutes an alternative topographic mapping
which determines the neighborhood cooperation in dependence of the data, resulting in an optimum
lattice structure [90,91]. Unlike SOM, the resulting connections are not regular such that visualization
requires additional work.
Neural gas can be derived from the cost function
	 
i, j
exp ( −ri(x)/σ 2)(∥w j −x∥2P(x)dx.
ri(x) denotes the rank of neuron i when the distances ∥x −w j∥2 are sorted according to their size, i.e.,
the rank is 0 for the winner, 1 for the second closest prototype, etc. Neural gas learning constitutes a
stochastic gradient descent on this cost function, resulting in the update:
init
repeat
choose x
determine the ranks ri(x)
adapt wi := wi + η exp ( −ri(x)/σ 2)(x −wi)
decrease σ and η
Obviously, during adaptation, the neighborhood cooperation is given by the rank of the neurons with
respect to an input pattern. One can introduce explicit connections which constitute an optimum lattice
by connecting neurons if they are the ﬁrst and second winner for at least one pattern [90].

846
CHAPTER 15 Neural Networks
Similar to SOM, an alternative batch optimization scheme exists given ﬁxed patterns x j:
init
repeat
determine the ranks ri(x j)
adapt wi :=

j exp (−ri(x j)/σ 2)x j
exp (−ri(x j)/σ 2)
decrease σ
Similar to SOM, this version can be adapted to proximity data not contained in a real-vector space.
Like SOM, neural gas does not yield to a uniform winner frequency of the prototypes. Rather, the
prototype distribution and the data distribution follow a power law with exponent m/(m + 2) where m
is the intrinsic dimension of the data manifold. This is different from the information theoretic optimum
1 in particular for low dimensional intrinsic dimensionality. A variety of magniﬁcation control schemes
exist for NG and batch NG, see, e.g., [83,92].
There exists a variety of alternatives to the standard SOM which rely on different fundamental
mathematical principles such as a constraint mixture modeling proposed in the context of the generative
topographic mapping [93], or information theoretic approaches [94].
1.15.5.4 Extensions to structures
Albeit topographic mapping depends on the distance of data points only, the algorithms require an under-
lying Euclidean vector space for the updates of prototypes. If complex data are dealt with, the Euclidean
distance is often not appropriate. There exist two principled approaches to extend unsupervised topo-
graphic maps to general data structures: one can equip the dynamics with recurrent connections, such
that time series, tree structures, or general graph structures can be dealt with, resulting in architectures
very similar to supervised recursive networks for structures. Examples for such approaches can be found
in [95,96].
Alternatively, a problem speciﬁc dissimilarity measure can be taken. Examples include alignment
for biological sequence data, compression distance for text, graph distances for structures, and similar.
In the last years, extensions of methods such as SOM and NG to kernels or general relational data
described by pairwise dissimilarities have been proposed. Median SOM or NG, as mentioned above,
constitute very simple (but also very limited) approaches.
As an alternative, kernel NG extends the standard NG cost function to a kernel mapping by means
of the identity
∥	(x) −	(w)∥2 = k(x, x) −2k(x, w) + k(w, w)
assuming a kernel k with corrspoding feature map 	 is given. For a linear combination w =  αi	(xi),
this reduces to a term which depends on the Gram matrix of the data only. Online adaptation of the
coefﬁcients αi or batch adaptation can be derived thereof, see [97,98].
Instead of a kernel, a general dissimilarity measure can be chosen, because of the equality
∥x −

αi xi∥2 = [Dtα]i −1/2 · αt Dα,
where α refers to the vector of coefﬁcients which sums up to one, and D denotes a symmetric dissim-
ilarity matrix storing the pairwise dissimilarities of xi. This formula allows a direct transfer of batch

1.15.6 Applications
847
learning to general dissimilarity data [99]. Unlike for kernels, however, these adaptations correspond
to vector operations in the so-called pseudo-Euclidean embedding of the given dissimilarity matrix,
where negative eigenvalues can occur in the dot product. In consequence, the NG algorithm is no longer
guaranteed to converge to a local optimum of the cost function. Because of quadratic optimization being
NP hard in such spaces, this is partially unavoidable.
1.15.6 Applications
Corresponding to the wide variety of neural architectures, these techniques populate nearly every pos-
sible application area one can think of. Applications can be found from commercial areas, e.g., in the
context of the ﬁnancial market, to standard tools in bioinformatics such as prediction of the secondary
structure of proteins, up to hot research topics such as humanoid robotics. Thus, rather than accumulating
a list of application scenarios, a list of typical tasks the different architectures are used for follows:
Feed-forward network: Classiﬁcation, regression tasks.
Recurrent network: Associative memory, time series prediction and generation, dynamic system
modeling; can be extended to deal with structures.
Support vector machine: Today, SVMs constitute the standard tool for classical regression and
classiﬁcation tasks.
Learning vector quantization: Multiclass classiﬁcation, relevance learning, data compression and
inspection.
ICA/PCA/SFA: Blind source separation, data preprocessing.
Topographic mapping: Data mining and visualization.
1.15.7 Open issues and problems
We have already discussed some open problems when considering the single architectures. In general,
many open problems center around the following issues:
•
How can the system be made more autonomous? Neural networks constitute very efﬁcient methods
provided a human has shaped the learning problem in a suitable way, e.g., as regression problem.
Unlike human learners, however, neural networks are hardly capable of structuring a complex learn-
ing problem themselves, and of actively extracting important structures inherent in the scenario.
Deep learning and autonomous learning are two topics which are currently discussed in this frame.
•
How can neural systems learn from very few examples? Humans are capable of rapid learning and
reasonable actions albeit they have experienced only few, probably only one example of how to react.
In contrast, typical neural systems require a number of examples to perform valid generalization.
This problem is tackled in topics such as instantaneous learning, or models to tackle the problem of
compositionality, i.e., the capability of dealing with entirely novel combinations of data where only
the parts are known, but they have never been experienced in this combination.
•
How can neural systems deal with complex structures? At present, most neural systems still deal
with standard Euclidean vectors. Great strides towards more general data structures such as graph

848
CHAPTER 15 Neural Networks
structures or dissimilarity data have been made in the context of recursive approaches or structure
kernels. Still, however, problems such as the computational complexity, or structured outputs pose
major challenges in this realm.
•
Howcanneuralsystemsbeinterpreted?Manyneuralnetworksofferhighqualitysolutions,but,dueto
their distributed representation of information, a human can hardly understand why a neural network
proposes a certain output. The question of how neural networks can be inspected by humans has been
tackled in the literature at several places, but no universally accepted solution exists. Approaches
deal with rule extraction, or visualization, for example, but usually still require knowledge of the
systems to be interpretably. With problems becoming more and more complex, often a human-in-
the-loop approach is taken to eventually arrive at the relevant information. For such approaches,
human insight is vital for success. Current research can be found in the ﬁeld of visual analytics, for
example.
•
How can larger and larger problems be addressed? Electronic data are getting more and more
common, such that larger and larger data sets have to be dealt with. This opens the way to new
research areas such as, e.g., the necessity for streaming algorithms which require at most one loop
over the data sets, or parallelization schemes to make solutions feasible, thereby relying, e.g., on
possibilities as offered by cloud computing or the map-reduce framework.
1.15.8 Implementation, code, and data sets
It becomes more and more common in Neuroinformatics that publications in the ﬁeld are accompanied
by links where to retrieve the corresponding programs and data sets. Further, the ﬁeld of neural networks
constitutes a standard topic taught at many universities, often accompanied by demo-source code or
applets. In consequence, a huge number of code and benchmark data sets can be found all over the web.
Here, we list a few sites where program packages and data sets can be found, whereby the list is
necessarily far from being complete, even far from being representative:
•
The by far mostly used data repository with more than 200 data sets of differet type is the UCI
Machine learning repository at http://archive.ics.uci.edu/ml/.
•
Large data sets are regularly published in the frame of the KDD contest paginationhttp://www.kdd.
org/kddcup/index.php.
•
A series of challenges with with publicly available data is organized in the frame of the PASCAL
network of excellence http://www.pascal-network.org/?q=node/15.
•
Very large datasets of images with different preprocessing as well as a semantic annotation are
prepared in the project ImageNet http://www.image-net.org/.
•
A popular open source java package including machine learning tools and preprocessing is offered
by Weka http://www.cs.waikato.ac.nz/ml/weka/.
•
A (probably a bit old but still one of the largest open source software packages) simulator for neural
networks is SNNS http://www.ra.cs.uni-tuebingen.de/SNNS/.
•
Another relatively novel full-featured neural network simulator for all usual platforms is offered by
Emergent http://grey.colorado.edu/emergent/index.php/Main_Page.

1.15.9 Conclusions and Future Trends
849
1.15.9 Conclusions and future trends
Basically, a variety of different models and training algorithms has been introduced, including the
popular standard algorithms such as perceptron training, back-propagation, recurrent networks, support
vector machine, learning vector quantization, self organizing maps, and neural gas. Obviously, this
is not a homogeneous set of models, rather different models with different motivations, biological
relevance, heuristic motivation, or mathematical formulation exist. Thereby, the concrete algorithms
are often surprisingly simple, formulated in just a few lines of code. However, both, mathematics
behind these models as well as further algorithmic development are non trivial and subject of ongoing
research.
Glossary
Adatron
an online training algorithm which ﬁnds a linear separation of
data with maximum margin
Back-propagation
efﬁcient way to compute the gradients in a feed-forward neural
network, standard training algorithm for such networks
Boltzmann machine
associative memory with probabilistic neurons, can involve hid-
den neurons
Cascade correlation
training algorithm which simultaneously determines the weights
and the architecture of a feed-forward network
Chomsky hierarchy
hierarchy of formal languages based on the complexity of the
rules of the underlying grammar, includes the standard model
of what can be addressed by digital computation as the highest
level
Convex optimization
optimization of a convex function in a convex set, under this
condition any local optimum is a global one, such that efﬁcient
methods can be designed
Cross-validation
a technique for estimating the performance of a predictive model
by means of a repeated split of the given data
Deep learning
learning multiple levels of representation autonomously, e.g.,
using deep convolutional networks or deep auto-encoders
Extreme learning machine
a feed-forward architecture where the ﬁrst layer is random and
only the output is trained
Feed-forward network
neural network with a dynamics determined by an acyclic graph
for regression or classiﬁcation tasks
Gradient descent
optimization method for a differentiable function by means of
iterativeadaptationstepsintothedirectionofthesteepestdescent

850
CHAPTER 15 Neural Networks
Hebbian learning
learning paradigm which emphasizes connections in between
neurons if they are simultaneously active
Hodgin-Huxley model
differential equations which describe the action potential of a
biological neuron
Hopﬁeld network
fully recurrent neural network used as associative memory
Independent component analysis technique to decompose a linear mixture of signals in indepen-
dent sources
Kernel
implicit nonlinear embedding function of data in a high dimen-
sional feature space
Learning vector quantization
prototype-based classiﬁcation algorithm trained by means of
Hebbian learning
Long-short term memory
recurrent network where error back-propagation is restricted to
linear units to avoid numerical difﬁculties
Magniﬁcation factor
quantity which characterizes the distribution of prototypes as
compared to the underlying data in topographic maps
Mean squared error
common cost function to evaluate the quality of a neural network
on a given data set
Neural gas
topographic mapping with data driven neighborhood coopera-
tion
NP-hard problem
problem which is at least as hard as the famous satisﬁability
(SAT) problem, no efﬁcient solution is known today
Oja learning rule
unsupervised learning rule for a single neuron to perform online
principal component analysis
Perceptron
a single neuron used as a linear classiﬁer
Principal component analysis
procedure to transform data corresponding to the directions of
highest statistical variation
Real time recurrent learning
training algorithm to train recurrent networks in an online sce-
nario
Recurrent neural network
neural network with cyclic connections to simulate associative
processing or dynamical systems
Relational clustering
clustering data which are characterized by pairwise dissimilari-
ties (relations) only
Relevance learning
automatic adaptation of the relevance of input dimensions
according to a given task
Reservoir computing
recurrent neural networks based on a randomly connected rich
recurrent reservoir and a trainable linear readout
Self-organizing map
unsupervised learning method for neurons arranged in lattice
structure to arrive at a topographic mapping of given stimuli
Slow feature analysis
technique to extract slowly changing entities from dynamic stim-
uli
Support vector machine
learning principle which separates data linearly in a nonlinear
high-dimensional feature space by optimizing the margin
Synapse
junctions between biological neurons

References
851
Topographic mapping
arrangement of neurons such that they represent stimuli accord-
ing to their underlying topology
Vapnik-Chervonenkis dimension combinatorial measure to judge the capacity of a function class
Relevant Theory: Machine Learning
See this Volume, Chapter 14 Learning Theory
See this Volume, Chapter 16 Kernel Methods and SVM
See this volume, Chapter 17 Online Learning in Reproducing Kernel Hilbert Spaces
See this Volume, Chapter 25 A Tutorial on Model Selection
References
[1] C. Bishop, Pattern Recognition and Machine Learning, Springer, 2006.
[2] R.O. Duda, P.E. Hart, D.G. Stork, Pattern Classiﬁcation, second ed., Wiley, 2001.
[3] T. Hastie, R. Tibshirani, J. Friedman, The Elements of Statistical Learning: Data Mining, Inference, and
Prediction, Springer Series in Statistics, second ed., Springer, New York, NY, 2009.
[4] S. Haykin, Neural Networks: A Comprehensive Foundation, Pearson, 1998.
[5] J. Hertz, A. Krogh, R. Palmer, Introduction to the Theory of Neural Computation, Addison Wesley, 1991.
[6] S. Theodoridis, K. Koutroumbas, Pattern Recognition, third ed., Academic Press, Inc., Orlando, FL, USA,
2006.
[7] W. McCulloch, W. Pitts, A logical calculus of ideas immanent in nervous activities, Bull. Math. Biophys. 5
(1943) 115–133.
[8] D. Hebb, The Organization of Behavior, Wiley, 1949.
[9] F. Rosenblatt, Principles of Neurodynamics, Spartan, 1962.
[10] B. Widrow, M. Hoff, Adaptive switching circuits, in: 1960 IRE WESCON Convention Record, 1960, pp.
96–104.
[11] M. Minsky, S. Papert, Perceptrons, MIT Press, 1969.
[12] T. Kohonen, Self Organizing Maps, Springer, 1995.
[13] C. von der Malsburg, Self-organization of orientation sensitive cells in the striate cortex, Kybernetik 14 (1973)
85–100.
[14] D. Rumelhart, G. Hinton, R. Williams, Learning representations by back-propagating errors, Nature 323
(1986) 533–536.
[15] P. Werbos, The Roots of Backpropagation: From Ordered Derivatives to Neural Networks and Political Fore-
casting, Wiley, 1994.
[16] G.E. Hinton, R. Salakhutdinov, Reducing the dimensionality of data with neural networks, Science 313 (5786)
(2006) 504–507.
[17] H. Jaeger, The echo state approach to analysing and training recurrent neural networks, Technical Report,
GMD Report 148, German National Research Center for Information Technology, 2001.
[18] W. Maass, Motivation, Theory, and Applications of Liquid State Machines, Imperial College Press, 2011.
[19] A.L. Hodgkin, A.F. Huxley, A quantitative description of membrane current and its application to conduction
and excitation in nerve, J. Physiol. 117 (1952) 500–544.
[20] K. Hornik, M. Stinchcombe, H. White, Multilayer feedforward networks are universal approximators, Neural
Networks 2 (1989) 359–366.

852
CHAPTER 15 Neural Networks
[21] B. DasGupta, B. Hammer, On approximate learning by multi-layered feedforward circuits, Theor. Comput.
Sci. 348 (1) (2005) 95–127.
[22] J. Sima, Back propagation is not efﬁcient, Neural Networks 9 (6) (1996) 1017–1023.
[23] M.
Riedmiller,
H.
Braun,
Rprop—description
and
implementation
details,
Technical
Report,
Universitat Karlsruhe, 1994.
[24] G.-B. Huang, Q.-Y. Zhu, C.-K. Siew, Extreme learning machine: theory and applications, Neurocomputing
70 (1–3) (2006) 489–501.
[25] V. Vapnik, Statistical Learning Theory, Wiley, 1998.
[26] M. Karpinski, A. Macintyre, Polynomial bounds for VC dimension of sigmoidal and general pfafﬁan neural
networks, J. Comput. Syst. Sci. 54 (1) (1997) 169–176.
[27] D.J. MacKay, Bayesian interpolation, Neural Comput. 4 (1991) 415–447.
[28] R.M. Neal, Bayesian Learning for Neural Networks, Springer-Verlag, New York, Inc., Secaucus, NJ, USA,
1996.
[29] E.D. Sontag, Neural nets as systems models and controllers, in: 7th Yale Workshop on Adaptive and Learning
Syst., 1992, pp. 73–79.
[30] X. Wang, Period-doublings to chaos in a simple neural network: an analytic proof, Complex Syst. 5 (1991)
425–442.
[31] B.A. Pearlmutter, Gradient calculations for dynamic recurrent neural networks: a survey, IEEE Trans. Neural
Networks 6 (5) (1995) 1212–1228.
[32] J.A.K. Suykens, B.D. Moor, J. Vandewalle, Robust local stability of multilayer recurrent neural networks,
IEEE TNN 11 (1) (2000) 222–229.
[33] Y. Bengio, P. Simard, P. Frasconi, Learning long-term dependencies with gradient descent is difﬁcult, IEEE
TNN 5 (2) (1994) 157–166.
[34] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural Comput. 9 (8) (1997) 1735–1780.
[35] T. Natschläger, W. Maass, H. Markram, The liquid computer: a novel strategy for real-time computing on
time series, TELEMATIK 8 (1) (2002) 39–43.
[36] P. Tino, G. Dorffner, Predicting the future of discrete sequences from fractal representations of the past, Mach.
Learn. 45 (2) (2001) 187–217.
[37] B. Hammer, P. Tiño, Recurrent neural networks with small weights implement deﬁnite memory machines,
Neural Comput. 15 (8) (2003) 1897–1929.
[38] B. Hammer, Generalization ability of folding networks, IEEE Trans. Knowl. Data Eng. 13 (2) (2001) 196–206.
[39] J.L. Elman, Finding structure in time, Cognitive Sci. 14 (2) (1990) 179–211.
[40] S. Frank, Learn more by training less: systematicity in sentence processing by recurrent networks, Connect.
Sci. 18 (2006) 287–302.
[41] M.H. Tong, A.D. Bickett, E.M. Christiansen, G.W. Cottrell, 2007 special issue: learning grammatical structure
with echo state networks, Neural Networks 20 (3) (2007) 424–432.
[42] F. Van der Velde, M. de Kamps, Neural blackboard architectures of combinatorial structures in cognition,
Behav. Brain Sci. 29 (2006) 37–70.
[43] R.C. Carrasco, M.L. Forcada, M.A. Valdes-Munoz, R.P. Neco, Stable encoding of ﬁnite-state machines in
discrete-time recurrent neural nets with sigmoid units, Neural Comput. 12 (2000) 2129–2174.
[44] C. Omlin, C, Giles, Rule revision with recurrent neural network, IEEE TKDE 8 (1) (1996) 183–188.
[45] P. Rodriguez, J. Wiles, J.L. Elman, A recurrent neural network that learns to count, Connect. Sci. 11 (1) (1999)
4–40.
[46] H.T. Siegelmann, E.D. Sontag, Analog computation, neural networks, and circuits, Theoretical Comput. Sci.
131 (1994) 331–360.
[47] J. Kilian, H.T. Siegelmann, The dynamic universality of sigmoidal neural networks. Inf. Comp. 128 (1996)
48–56.

References
853
[48] B. Hammer, On the approximation capability of recurrent neural networks, Neurocomputing 31 (1–4) (2000)
107–123.
[49] P. Baldi, S. Brunak, P. Frasconi, G. Pollastri, G. Soda, Exploiting the past and the future in protein secondary
structure prediction, Bioinformatics 15 (11) (1999).
[50] P. Baldi, S. Brunak, P. Frasconi, G. Pollastri, G. Soda, Bidirectional dynamics for protein secondary structure
prediction, Lect. Notes Comput. Sci. 1828 (2001) 80.
[51] P. Baldi, G. Pollastri, The principled design of large-scale recursive neural network architectures—DAG-RNNs
and the protein structure prediction problem, J. Mach. Learn. Res. 4 (2003) 575–602.
[52] A.M. Bianucci, A. Micheli, A. Sperduti, A. Starita, Application of cascade correlation networks for structures
to chemistry, J. Appl. Int. 12 (2000) 117–146.
[53] C.L. Giles, D. Chen, G.-Z. Sun, H.-H. Chen, Y.-C. Lee, M.W. Goudreau, Constructive learning of recur-
rent neural networks: limitations of recurrent casade correlation and a simple solution, IEEE Trans. Neural
Networks 6 (4) (1995) 829–836.
[54] B. Hammer, A. Micheli, A. Sperduti, Universal approximation capability of cascade correlation for structures,
Neural Comput. 17 (2005) 1109–1159.
[55] P. Frasconi, M. Gori, A. Sperduti, A general framework for adaptive processing of data structures IEEE TNN
9 (5) (1997) 768–786.
[56] B. Hammer, Learning with recurrent neural networks, Lect. Notes Contr. Info. Sci., vol. 254, Springer, 2000.
[57] F. Scarselli, M. Gori, A.C. Tsoi, M. Hagenbuchner, G. Monfardini, The graph neural network model, IEEE
Trans. Neural Networks 20 (1) (2009) 61–80.
[58] T. Friess, N. Cristianini, C. Campbell, The kernel adatron algorithm: a fast and simple learning procedure for
support vector machine, 1998.
[59] J.C. Platt, Fast training of support vector machines using sequential minimal optimization, MIT Press, Cam-
bridge, MA, USA, 1999, pp. 185–208.
[60] I.W. Tsang, J.T. Kwok, P.-M. Cheung, Core vector machines: fast svm training on very large data sets, J.
Mach. Learn. Res. 6 (2005) 363–392.
[61] B. Schölkopf, A. Smola, Learning with Kernels, MIT, 2002.
[62] B. Hammer, K. Gersmann, A note on the universal approximation capability of support vector machines,
Neural Process. Lett. 17 (1) (2003) 43–53.
[63] I. Steinwart, On the inﬂuence of the kernel on the consistency of support vector machines, J. Mach. Learn.
Res. 2 (2002) 67–93.
[64] B. Frénay, M. Verleysen, Parameter-insensitive kernel in extreme learning for non-linear support vector regres-
sion, Neurocomputing 74 (16) (2011) 2526–2531.
[65] D. Haussler, Convolution kernels on discrete structures, Technical Report UCSCCRL- 99-10, Department of
Computer Science, University of California at Santa Cruz, 1999.
[66] C. Watkins, Dynamic alignment kernels, in: A.J. Smola, P. Bartlett, B. Scholkopf, D. Schuurmans (Eds.),
Advances in Large Margin Classiﬁers, MIT Press, 1999 (Chapter 3).
[67] C.
Leslie,
E.
Eskin,
W.S.
Noble,
The
spectrum
kernel:
a
string
kernel
for
svm
protein
classiﬁcation, in: Pac. Symp. Biocomput., 2002, pp. 564–575.
[68] H. Lodhi, J. Shawe-Taylor, N. Cristianini, C.J.C.H. Watkins, Text classiﬁcation using string kernels, in: NIPS,
2000, pp. 563–569.
[69] T. Kuboyama, H. Kashima, K.F. Aoki-Kinoshita, K. Hirata, H. Yasuda, A spectrum tree kernel, J. Jpn. Soc.
Artif. Int. 22 (2) (2007).
[70] S.V.N. Vishwanathan, A.J. Smola, Fast kernels on strings and trees, in: NIPS 15, 2002.
[71] R. Kondor, J. Lafferty, Diffusion kernels on graphs and other discrete structures, in: Proceedings of the 19th
International Conference on Machine Learning ICML, 2002.

854
CHAPTER 15 Neural Networks
[72] T. Jaakkola, M. Diekhaus, D. Haussler, Using the Fisher kernel method to detect remote protein homologies,
in: 7th Intell. Syst. Mol. Biol., 1999, 149–158.
[73] M.
Tipping,
The
relevance
vector
machine,
in:
Advances
in
Neural
Information
Processing
Systems, Morgan Kaufmann, San Mateo, CA, 2000.
[74] M. Biehl, A. Ghosh, B. Hammer, Dynamics and generalization ability of LVQ algorithms, J. Mach. Learn.
Res. 8 (2007) 323–360.
[75] S. Sato, K. Yamada, Generalized learning vector quantization, in: G. Tesauro, D. Touretzky, T. Leen (Eds.),
Adv. in Neural Information Processing Systems, vol. 7, 1995, pp. 423–429.
[76] B. Hammer, T. Villmann, Generalized relevance learning vector quantization, Neural Networks 15 (8–9)
(2002) 1059–1068.
[77] B. Hammer, M. Strickert, T. Villmann, On the generalization ability of GRLVQ networks, Neural Process.
Lett. 21 (2) (2005) 109–120.
[78] B. Hammer, M. Strickert, T. Villmann, Supervised neural gas with general similarity measure, Neural Process.
Lett. 21 (1) (2005) 21–44.
[79] M. Strickert, U. Seiffert, N. Sreenivasulu, W. Weschke, T. Villmann, B. Hammer, Generalized relevance LVQ
(GRLVQ) with correlation measures for gene expression analysis, Neurocomputing 69 (2006) 651–659, ISSN:
0925-2312.
[80] P. Schneider, M. Biehl, B. Hammer, Adaptive relevance matrices in learning vector quantization, Neural
Comput. 21 (12) (2009) 3532–3561.
[81] A. Hyvärinen, J. Karhunen, E. Oja, Independent Component Analysis, Wiley, 2001.
[82] L. Wiskott, T. Sejnowski, Slow feature analysis: unsupervised learning of invariances, Neural Comput. 14 (4)
(2002) 715–770.
[83] T. Villmann, R. Der, T.M. Martinetz, Topology preservation in self-organizing feature maps: exact deﬁnition
and measurement, IEEE Trans. Neural Networks 8 (2) (1997) 256–266.
[84] M. Cottrell, J.C. Fort, G. Pagès, Two or three things that we know about the Kohonen algorithm, in: M.
Verleysen (Ed.), Proc. ESANN’94, European Symp. on Artiﬁcial Neural Networks, D Facto Conference
Services, Brussels, Belgium, 1994, pp. 235–244.
[85] J. Fort, SOM’s mathematics, Neural Networks 19 (6) (2006) 812–816.
[86] T. Heskes, Energy functions for self-organizing maps, in: E. Oja, S. Kaski (Eds.), Kohonen Maps, Elsevier,
Amsterdam, 1999, pp. 303–315.
[87] M. Cottrell, B. Hammer, A. Hasenfuss, T. Villmann, Batch and median neural gas, Neural Networks 19 (2006)
762–771.
[88] T. Villmann, J.-C. Claussen, Magniﬁcation control in self-organizing maps and neural gas, Neural Comput.
18 (2005) 446–469.
[89] D. de Sieno, Adding consciousness to competitive learning, in: Proc. 2nd IEEE Int. Conf. Neural Networks,
vol. 1, 1988, pp. 117–124.
[90] T.
Martinetz,
K.
Schulten,
Topology
representing
networks,
Neural
Networks
7
(3)
(1994)
507–522.
[91] T.M. Martinetz, K.J. Schulten, A neural-gas network learns topologies, in: T. Kohonen, K.M.O. Simula, J.
Kangas (Eds.), Artiﬁcial Neural Networks, North-Holland, 1991, pp. 397–402.
[92] B. Hammer, A. Hasenfuss, T. Villmann, Magniﬁcation control for batch neural gas, Neurocomputing 70 (7–9)
(2007) 1225–1234.
[93] C.M. Bishop, C.K.I. Williams, Gtm: the generative topographic mapping, Neural Comput. 10 (1998) 215–234.
[94] M.M.V. Hulle, Entropy-based kernel mixture modeling for topographic map formation, IEEE Trans. Neural
Networks 15 (4) (2004) 850–858.
[95] C.M. Bishop, Gtm through time, in: IEE Fifth International Conference on Artiﬁcial Neural Networks, 1997,
pp. 111–116.

References
855
[96] B.Hammer,A.Micheli,A.Sperduti,M.Strickert,Recursiveself-organizingnetworkmodels,NeuralNetworks
17 (8–9) (2004) 1061–1085.
[97] A.K. Qin, P.N. Suganthan, Kernel neural gas algorithms with application to cluster analysis, in: ICPR, vol. 4,
2004, pp. 617–620.
[98] H. Yin, On the equivalence between kernel self-organising maps and self-organising mixture density networks,
Neural Networks 19 (6–7) (2006) 780–784.
[99] B. Hammer, A. Hasenfuss, Topographic mapping of large dissimilarity data sets, Neural Comput. 22 (9) (2010)
2229–2284.

16
CHAPTER
Kernel Methods and Support
Vector Machines
John Shawe-Taylor* and Shiliang Sun†
*Department of Computer Science, University College London, Gower Street, London, United Kingdom
†Department of Computer Science and Technology, East China Normal University, 500 Dongchuan Road,
Shanghai, China
Nomenclature
X
The data matrix with each row as an observation
κ(x, z)
The kernel function with input vectors x and z
⟨x, z⟩
The inner product between two vectors x and z
φ(x)
The mapping of x to the feature space F
In
The n × n identity matrix
K
The kernel matrix with entry Ki j being the kernel function value
for the ith and jth inputs
∥w∥
The Euclidean norm of the vector w
trace(K)
The trace of the matrix K
cov(x, u)
The covariance between two random scalar variable x and u
var(x)
The variance of the random scalar variable x
x ⪰( ⪯)z
The vector x is larger (less) than the vector z elementwise
K ⪰0
The matrix K is positive semideﬁnite
1.16.1 Introduction
Data often possess some intrinsic regularities which, if revealed, can facilitate people to understand
data themselves or make predictions about new data from the same source. These regularities are called
patterns, and pattern analysis, which has been studied broadly such as in statistics, artiﬁcial intelligence
and signal processing, deals with the automatic detection of patterns in data.
The development of pattern analysis algorithms can be summarized with three important stages
[1]. In the 1950s and 1960s, efﬁcient algorithms such as the perceptron [2] were used. They are well
understood and effective for detecting linear patterns, though were shown to be limited in complexity.
In the 1980s, with the introduction of both the backpropagation algorithm for multi-layer networks
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00016-4
© 2014 Elsevier Ltd. All rights reserved.
857

858
CHAPTER 16 Kernel Methods and Support Vector Machines
[3] and decision trees [4,5], pattern analysis underwent a nonlinear revolution. These methods made
a high impact to efﬁciently and reliably detect nonlinear patterns, though they are largely heuristical
with limited statistical analysis and often get trapped with local minima. In the 1990s, the emerging
of kernel methods [1,6] for which support vector machines (SVMs) [7,8] are the earliest and foremost
inﬂuential ﬁnally enabled people to deal with nonlinear patterns in the input space via linear patterns
in high dimensional spaces. This third generation of pattern analysis algorithms are well-founded just
like their linear counterparts, but wipe off the drawbacks of local minima and limited statistical analysis
which are typical for multi-layer neural networks and decision trees. Since the 1990s, the algorithms
and application scopes of kernel methods have been extended rapidly, from classiﬁcation to regression,
to clustering and many other machine learning tasks.
The approach of kernel methods has four key aspects: (i) Data are embedded into a Euclidean feature
space; (ii) Linear relations are sought in the feature space; (iii) Algorithms are implemented so that
only inner products between vectors in the feature space are required; (iv) The products can be directly
computed from the original data by an efﬁcient “short-cut" known as a kernel function (or kernel for
short). This is also known as the kernel trick. The idea of using kernel functions as inner products in a
feature space is not new. It was introduced into machine learning in 1964 with the method of potential
functions [9] and this work is mentioned in a footnote of Duda and Hart’s pattern classiﬁcation book
[10]. Through this route, the authors of [7] noticed this idea, combined it with large margin hyperplanes
in the later SVMs and thus introduced the notion of kernels into the mainstream of the machine learning
literature.
Although basic kernel methods are rather mature techniques, research combining them with other
techniques is still going on, e.g., kernels have been successfully applied to multi-view learning, semi-
supervised learning and multitask learning problems [11–16]. This forms a continual impetus along the
line of research on kernel-based learning methods. More importantly, recent work on multiple kernel
learning [17] has promoted the study of kernel methods to a new level. This article reviews both classical
and some recent research developments on kernel methods, with emphases on the “plug-and-play” ﬂavor
of kernel methods.
The rest of this article is organized as follows. Section 1.16.2 introduces the kernel trick and prop-
erties and types of kernels, which constitute the foundations of kernel methods. In addition to the ker-
nel ridge regression method presented in Section 1.16.2, Section 1.16.3 reviews some fundamental
kernel methods including kernel principal component analysis, kernel canonical correlation analysis,
kernel Fisher discriminant analysis, support vector machines, and Gaussian processes. Section 1.16.4
discusses the computational issues of kernel methods and algorithms towards their efﬁcient implementa-
tions. Section 1.16.5 brieﬂy surveys the recent developments on multiple kernel learning. Section 1.16.6
presents some practical applications of kernel methods and SVMs. Finally, open issues and problems
are discussed in Section 1.16.7 after a brief concluding summary of the article.
1.16.2 Foundations of kernel methods
In this section, we ﬁrst illustrate key concepts for kernel methods from kernel ridge regression, and then
discuss properties of valid kernels. Finally, kernel design strategies are introduced.

1.16.2 Foundations of Kernel Methods
859
1.16.2.1 The kernel trick: ridge regression as an example
Consider the problem of ﬁnding a homogeneous real-valued linear function
g(x) = ⟨w, x⟩= x⊤w =
n

i=1
wi xi,
(16.1)
that best interpolates a given training set S = {(x1, y1), . . . , (xm, ym)} of points xi ∈Rn with corre-
sponding labels yi ∈R. A commonly chosen measure of the discrepancy between a function output
and the real observation is
fg((x, y)) = (g(x) −y)2.
(16.2)
Suppose the m inputs of S are stored in the matrix X as row vectors, and the corresponding outputs
constitute vector y with y = [y1, . . . , ym]⊤. Hence we can write ξ = y−Xw for the vector of differences
between g(xi) and yi. Ridge regression corresponds to solving the following optimization with a simple
norm regularizer
min
w Lλ(w, S) = min
w λ∥w∥2 + ∥ξ∥2,
(16.3)
where λ > 0 deﬁnes the relative tradeoff between the norm and loss. Setting the derivative of Lλ(w, S)
with respect to the parameter vector w equal to 0 gives
X⊤Xw + λw = (X⊤X + λIn)w = X⊤y,
(16.4)
where In is the n × n identity matrix. Thus we get the primal solution (referring to the explicit repre-
sentation) for the weight vector
w = (X⊤X + λIn)−1X⊤y,
(16.5)
from which the resulting prediction function g(x) can be readily given.
Alternatively, from (16.4) we get
w = X⊤1
λ(y −Xw) = X⊤α =
m

i=1
αixi,
(16.6)
where parameters α = [α1, . . . , αm]⊤≜λ−1(y −Xw) are known as the dual variables. Substituting
w = X⊤α into α = λ−1(y −Xw), we obtain
α = (XX⊤+ λIm)−1y,
(16.7)
which is called the dual solution. The dual solution expresses the weight vector w as a linear combination
of the training examples. Denote the term XX⊤by K. It follows that Ki, j = ⟨xi, x j⟩. Now the resulting
prediction function is formulated as
g(x) = x⊤w = x⊤X⊤α =

x,
m

i=1
αixi

=
m

i=1
αi⟨x, xi⟩.
(16.8)
There are two ingredients embedded in the dual form of ridge regression: computing vector α and
evaluation of the prediction function. Both operations only involve inner products between data inputs.

860
CHAPTER 16 Kernel Methods and Support Vector Machines
Since the computation only involves inner products, we can substitute for all occurrences of ⟨·, ·⟩a
kernel function κ that computes κ(x, z) = ⟨φ(x), φ(z)⟩and we obtain an algorithm for ridge regression
in the feature space F deﬁned by the mapping φ : x 	→φ(x) ∈F. This is an instantiation of the
kernel trick for ridge regression and results in the kernel ridge regression algorithm. Through kernel
ridge regression we can perform linear regression in very high-dimensional spaces efﬁciently, which is
equivalent to performing non-linear regression in the original input space.
1.16.2.2 Properties of kernels
Deﬁnition 1 (Kernel function).
A kernel is a function κ that for all x, z from a nonempty set X
(which need not be a vector space) satisﬁes
κ(x, z) = ⟨φ(x), φ(z)⟩,
(16.9)
where φ is a mapping from the set X to a Hilbert space F that is usually called the feature space
φ : x ∈X 	→φ(x) ∈F.
(16.10)
To verify whether a function is a valid kernel, one approach is to construct a feature space for which
the function value for two inputs corresponds to ﬁrst performing an explicit feature mapping and then
computing the inner product between their images. An alternative approach, which is more widely used,
is to investigate the ﬁnitely positive semideﬁnite property [1,6,18–20].
Deﬁnition 2 (Finitely positive semideﬁnite function).
A function κ : X × X →R satisﬁes the
ﬁnitely positive semideﬁnite property if it is a symmetric function for which the kernel matrices K with
Ki j = κ(xi, x j) formed by restriction to any ﬁnite subset of X are positive semideﬁnite.
The feasibility of the above property for characterizing kernels is justiﬁed by the following
theorem [1,6,21].
Theorem 3 (Characterization of kernels).
A function κ : X × X →R which is either continuous
or has a countable domain, can be decomposed as an inner product in a Hilbert space F by a feature
map φ applied to both its arguments
κ(x, z) = ⟨φ(x), φ(z)⟩
(16.11)
if and only if it satisﬁes the ﬁnitely positive semideﬁnite property.
A Hilbert space F is deﬁned as an inner product space that is complete where completeness means
that every Cauchy sequence of elements of F converges to an element in this space. If the separability
property is further added to the deﬁnition of a Hilbert space, where a space is separable if there is a
countable set of elements from this space such that the distance between each element of this space and
some element of this countable set is less than any predeﬁned threshold, the existence of “kernels are
continuous or the domain is countable” in Theorem 3 is then necessary.
The Hilbert space constructed in proving Theorem 3 is called the reproducing kernel Hilbert space
(RKHS) because the following reproducing property of the kernel resulting from the deﬁned inner
product holds
⟨fF( · ), κ(x, ·)⟩= fF(x),
(16.12)

1.16.2 Foundations of Kernel Methods
861
FIGURE 16.1
One instantiation of the feature mapping using a Gaussian kernel.
where fF is a function of the function space F, and function κ(x, ·) is the mapping φ(x) which actually
represents the similarity of x to all other points in X, as shown in Figure 16.1 [6].
By construction, fF( · ) takes the form of an arbitrarily-weighted linear combination of countable
images of the original inputs. For any two such functions
fF1( · ) =
ℓ1

i=1
αiκ(xi, ·),
fF2( · ) =
ℓ2

j=1
β jκ(x′
j, ·)
(16.13)
where ℓ1, ℓ2 ∈N, αi, β j ∈R and xi, x′
j ∈X, the dot product is deﬁned as
⟨fF1( · ), fF2( · )⟩≜
ℓ1

i=1
ℓ2

j=1
αiβ jκ(xi, x′
j).
(16.14)
The inner product space or pre-Hilbert space formed by fF( · ) is then completed to form the Hilbert
space F where the mathematical trick “completion” refers to adding all limit points of Cauchy sequences
to the space [6].
It should be noted that there are different approaches to constructing feature spaces for any given
kernel. Besides the above construction, the Mercer kernel map [22], though not mentioned much here,
is also widely applicable, especially in the SVM literature. The feature spaces constructed in different
ways can even have different dimensions. However, since we are only interested in dot products, these
spaces can be regarded as identical.
For some kernels, the feature map and feature space can be explicitly built with a simple form. For
instance, consider the homogeneous quadratic kernel
κ(x, z) = ⟨x, z⟩2,
(16.15)
which can be reformulated as
κ(x, z) = (x′z)2 = z′(xx′)z = ⟨vec(zz′), vec(xx′)⟩,
(16.16)
where vec(A) stacks the column of matrix A on top of each other in the manner that the ﬁrst column
situates at the top. The feature map corresponding to κ would be φ(x) = vec(xx′). The feature space
can be the Euclidean space with dimensionality being the total number of entries of vec(xx′).

862
CHAPTER 16 Kernel Methods and Support Vector Machines
1.16.2.3 Types of kernels
The use of kernels provides a powerful and principled approach to modeling nonlinear patterns through
linear patterns in a feature space. Another beneﬁt is that the design of kernels and linear methods can
be decoupled, which greatly facilitates the modularity of machine learning methods.
Representative kernels include the linear kernel κ(x, z) = ⟨x, z⟩, inhomogeneous polynomial kernel
κ(x, z) = (⟨x, z⟩+ R)d
(16.17)
where d is the degree of the polynomial and parameter R ∈R, and the Gaussian radial basis function
(RBF) kernel (Gaussian kernel for short) with parameter σ > 0
κ(x, z) = exp

−∥x −z∥2
2σ 2

.
(16.18)
The polynomial kernel (16.17) can be expanded by the binomial theorem as
(⟨x, z⟩+ R)d =
d

s=0
d
s

Rd−s⟨x, z⟩s.
(16.19)
Hence, the features for each component in the sum together form the features of the kernel. In other
words, we have a reweighting of the features of the homogeneous polynomial kernel
κ(x, z) = ⟨x, z⟩s,
s = 0, . . . , d,
(16.20)
where one construction of the feature map corresponding to kernel (16.20) is using a vector with entries
being all ordered monomials (e.g., x1x2 and x2x1 are treated as separate features) of degree s, that is, each
entry is an instantiation of product x j1 . . . x js with j1, . . . js ∈{1, . . . , n} [6]. The parameter R allows
the control of the relative weightings of the monomials with different degrees. The weight formulation
d
s

Rd−s indicates that increasing R decreases the relative weighting of higher order monomials [1].
For the Gaussian kernel (16.18) the images of all points have norm 1 in the feature space as a result
of κ(x, x) = 1. It can be obtained by normalizing exp (⟨x, z⟩/σ 2)
exp

−∥x −z∥2
2σ 2

= exp
⟨x, z⟩
σ 2
−⟨x, x⟩
2σ 2 −⟨z, z⟩
2σ 2

=
exp (⟨x, z⟩/σ 2)
	
exp (∥x∥2/σ 2) exp (∥z∥2/σ 2)
.
(16.21)
Because an exponential function can be arbitrarily closely approximated by polynomials with positive
coefﬁcients
exp (x) =
∞

i=0
1
i!xi,
(16.22)
the function exp (⟨x, z⟩/σ 2) is arguably a kernel. Therefore, the Gaussian kernel (16.18) is a polynomial
kernel of inﬁnite degree, and its features can be all ordered monomials of input features with no restric-
tion placed on the degrees. However, with increasing degree the weighting of individual monomials
falls off as i! [1].
One appeal of using kernel methods is that kernels are not restricted to vectorial data, making it
possible to apply the techniques to diverse types of objects. Not surprisingly, kernels can be designed

1.16.2 Foundations of Kernel Methods
863
for sets, strings, text documents, graphs and graph-nodes [1]. For these kernels, we would not elaborate
here. However, an effective design of kernels has to be embedded with some prior knowledge on how
to characterize similarity between data.
We now focus on two types of kernels induced by probabilistic models, marginalization kernels and
Fisher kernels. These techniques are useful for combining generative and discriminative methods for
machine learning. The marginalization kernels are deﬁned as follows.
Deﬁnition 4 (Marginalization kernels).
Given a set of data models M and a prior distribution PM
on M, the probability that an example pair x and z is generated together can be computed as
PM(x, z) =

m∈M
P(x|m)P(z|m)PM(m).
(16.23)
If we consider the mapping function
φ : x 	→(P(x|m))m∈M ∈F
(16.24)
in a feature space F indexed by M, PM(x, z) corresponds to the inner product
⟨f , g⟩=

m∈M
fmgm PM(m)
(16.25)
between φ(x) and φ(z). PM(x, z) is referred to as the marginalization kernel for the model class M.
The above computation can be viewed as a marginalization operation for the probability distribution
of triples (x, z, m) over m (with conditional independence of x and z given a speciﬁc model m), and
therefore comes the name marginalization kernels. The assumption of conditional independence is a
sufﬁcient condition for positive semi-deﬁniteness. For an input, marginalization kernels treat the output
probability given one model as a feature. Since the information from a single model is quite limited,
they usually adopt multiple different models to reach a representation of the input.
Fisher kernels, deﬁned by [23,24], are an alternative way of extracting information, usually from
a single generative model, however. The single model is required to be smoothly parameterized so
that derivatives of the model with respect to the parameters is computable. An intuitive interpretation
of Fisher kernels is that it describes data points by the variation of some quantity (say the log of the
likelihood function) caused by slight parameter perturbations.
Deﬁnition 5 (Fisher score and Fisher information matrix).
For a given setting of the parameters
θ0 (e.g., obtained by the maximum likelihood rule) the log-likelihood of a data point x with respect to
the model m(θ0) is deﬁned to be log P(x|θ0). Consider the gradient vector of the log-likelihood
g(θ, x) =
∂log P(x|θ)
∂θi
N
i=1
,
(16.26)
where θ ∈RN. The Fisher score of a data point x with respect to the model m(θ0) is g(θ0, x). The
Fisher information matrix with respect to the model m(θ0) is given by
IFisher = E

g(θ0, x)g(θ0, x)⊤
,
(16.27)
where the expectation is over the distribution of the data point x.

864
CHAPTER 16 Kernel Methods and Support Vector Machines
The Fisher score embeds a data point into the feature space RN, and provides direct constructions
of kernels.
Deﬁnition 6 (Fisher kernel).
The invariant Fisher kernel with respect to the model m(θ0) for a given
setting of the parameters θ0 is deﬁned as
κ(x, z) = g(θ0, x)⊤I−1
Fisherg(θ0, z).
(16.28)
The practical Fisher kernel is deﬁned as
κ(x, z) = g(θ0, x)⊤g(θ0, z).
(16.29)
The invariant Fisher kernel is computationally more demanding as it requires the computation and
inversion of the Fisher information matrix. It is named “invariant” because the resulting kernel would
not change if we reparameterize the model with an invertible differentiable transformation ψ = ψ(θ).
Suppose ˜κ is the transformed kernel. It follows that
g(θ0, x)⊤=
∂log P(x|ψ)
∂ψi
N
i=1
⊤
J(ψ) = g(ψ0, x)⊤J(ψ0),
(16.30)
where matrix J(ψ0) is the Jacobian of the transformation ψ evaluated at ψ0 [1]. Now we have
˜κ(z1, z2) = g(ψ0, z1)⊤E

(J(ψ0)−1)⊤g(θ0, x)g(θ0, x)⊤J(ψ0)−1−1
g(ψ0, z2)
= g(θ0, z1)⊤E

g(θ0, x)g(θ0, x)⊤−1
g(θ0, z2)
= κ(z1, z2).
(16.31)
Hence, the invariant Fisher kernel is desirable if the choice of parameterizations is somewhat arbitrary.
But for this kernel there is a caveat when the natural approximation of the Fisher information matrix by
its empirical estimate is used
ˆIFisher = ˆE

g(θ0, x)g(θ0, x)⊤
= 1
m
m

i=1
g(θ0, xi)g(θ0, xi)⊤,
(16.32)
in which case ˆIFisher is the empirical covariance matrix of the Fisher scores. The invariant Fisher kernel
is thus equivalent to whitening the scores. The negative effect is that we may amplify noise if some
parameters are not relevant for the information, and therefore the signal to noise ratio is possibly reduced.
This can be regarded as the cost of the invariance.
Apart from the kernels introduced so far, more complicated kernels can be constructed with them as
building blocks. The following theorem [1] lists some strategies for kernel constructions.
Theorem 7 (Kernel constructions).
Let κ1, κ2, and κ3 be valid kernels, φ any feature map to the
domain of κ3, a ≥0, f ( · ) any real-valued function, and B a positive semi-deﬁnite matrix. Then the
following functions are valid kernels:
•
κ(x, z) = κ1(x, z) + κ2(x, z),
•
κ(x, z) = aκ1(x, z),

1.16.3 Fundamental Kernel Methods
865
•
κ(x, z) = κ1(x, z)κ2(x, z),
•
κ(x, z) = f (x) f (z),
•
κ(x, z) = κ3(φ(x), φ(z)),
•
κ(x, z) = x⊤Bz (for now x and z are vectorial data).
1.16.3 Fundamental kernel methods
In this section, we introduce some fundamental kernel methods ranging from unsupervised learning to
supervised learning. These methods have a large popularity either because they are among the ﬁrst uses
of kernels or because they address very fundamental learning problems.
1.16.3.1 Kernel principal component analysis
Principal component analysis (PCA) ﬁnds a set of orthogonal directions which forms a subspace to
maximize variances. In this way, data can be reconstructed with minimal quadratic error. Suppose the
inputs of the data set S given in Section 1.16.2.1 is centered with mean 0. The direction that maximizes
the variance can be found by solving the following problem
max
w
w⊤Cw
s.t. ∥w∥= 1,
(16.33)
where C = 1
m X⊤X is the covariance matrix (strictly speaking, an empirical estimate of the covariance)
of the input data. The solution is given by the eigenvector of C corresponding to the largest eigenvalue
with the objective value being the eigenvalue. The direction of the second largest variance can be
searched for in the subspace orthogonal to the direction already found. This results in the eigenvector
corresponding to the second largest eigenvalue. It is readily provable that PCA projects data into the
space spanned by the k largest eigenvectors of C if we would like to ﬁnd a k-dimensional subspace.
The new coordinates by which we represent the data are known as principal components. Although
centering data before performing PCA is not a must, it has the advantage of reducing the overall sum
of the eigenvalues and thus removing irrelevant variance arising from data shift [1].
The kernel PCA [25] extends the linear PCA algorithm to extracting nonlinear structures in terms of
kernels. Now we provide a simple derivation of the kernel PCA by exploiting the relationship between
X⊤X and XX⊤[1]. It is easy to show that these two matrices have the same rank. More interestingly,
their eigen-decompositions correspond to each other. Suppose that w, λ is an eigenvector-eigenvalue
pair for X⊤X, then Xw, λ is for XX⊤
(XX⊤)Xw = X(X⊤X)w = λXw,
(16.34)
and conversely, if α, λ is an eigenvector-eigenvalue pair for the matrix XX⊤, then X⊤α, λ is for X⊤X
(X⊤X)X⊤α = X⊤(XX⊤)α = λX⊤α.
(16.35)
This gives a dual representation for the eigenvector of X⊤X from the eigen-decomposition of XX⊤.
XX⊤is actually a kernel matrix if we replace each row x⊤
i of X by its image φ(xi)⊤in a feature space,
and X⊤X would be the scaled covariance matrix without centering.

866
CHAPTER 16 Kernel Methods and Support Vector Machines
Centering data in a feature space is not so simple as in the original space. Suppose that a kernel κ is
adopted with the kernel matrix K computed from the original data. Centering data in the feature space
corresponds to deﬁning a new feature map ˆφ(x) = φ(x) −1
m
m
i=1 φ(xi). The new kernel matrix for
the centered data would be
ˆK = K −1
m jj⊤K −1
m Kjj⊤+ 1
m2 (j⊤Kj)jj⊤,
(16.36)
where j is the all 1s vector [1]. Suppose that ˆα, ˆλ is an eigenvector-eigenvalue pair for the kernel matrix
ˆK = ˆX ˆX⊤where ∥ˆα∥= 1 and the ith row of ˆX is ˆφ(xi)⊤. Then ˆX⊤ˆα is the eigenvector of the covariance
matrix 1
m ˆX⊤ˆX which has the same eigenvectors with ˆX⊤ˆX. Usually we require that the ﬁnal projection
vector is normalized, that is, ∥ˆX⊤ˆα∥= 1. Because for ∥ˆα∥= 1 we have
∥ˆX⊤ˆα∥2 = ˆα⊤ˆX ˆX⊤ˆα = ˆα⊤ˆK ˆα = ˆλ,
(16.37)
to meet ∥ˆX⊤ˆα∥= 1, ˆα should be further divided by
	
ˆλ. Hence, the k projection directions derived
from kernel PCA should be
⎧
⎨
⎩
1

ˆλi
ˆX⊤ˆαi
⎫
⎬
⎭
k
i=1
,
(16.38)
where {ˆαi, ˆλi}k
i=1 are the k leading eigenvector-eigenvalue pairs for the kernel matrix ˆK and the norms
of {ˆαi}k
i=1 are all 1. The projections of a new input x would be the inner products between the above
directions and φ(x) −1
m
m
i=1 φ(xi).
1.16.3.2 Kernel canonical correlation analysis
Canonical correlation analysis (CCA), proposed by [26], works on a paired dataset (i.e., data with two
representations) to ﬁnd two linear transformations each for one of the two representations such that the
correlations between the transformed variables are maximized. It was later generalized to more than two
sets of variables in several ways [27,28]. Here we only focus on the situation of two sets of variables.
Suppose we have a paired dataset Sx,u = {(x1, u1), . . . , (xm, um)}. For example, xi and the cor-
responding ui can be the representations of a same sematic content in two different languages. CCA
attempts to seek the projection directions wx and wu to maximize the following empirical correlation
cov(w⊤
x x, w⊤
u u)
	
var(w⊤x x)var(w⊤
u u)
=
w⊤
x Cxuwu
	
(w⊤x Cxxwx)(w⊤
u Cuuwu)
,
(16.39)
where covariance matrix Cxu is deﬁned as (deﬁnitions for Cxx and Cuu can be obtained analogously)
Cxu = 1
m
m

i=1
(xi −mx)(ui −mu)⊤
(16.40)
with mx and mu being the means of the two representations, respectively
mx = 1
m
m

i=1
xi,
mu = 1
m
m

i=1
ui.
(16.41)

1.16.3 Fundamental Kernel Methods
867
Because the scales of wx and wu have no effects on the value of (16.39), we can constrain each of
the two terms in the denominator to take value 1. Thus we reach another widely used objective for CCA
max
wx,wu
ρ = w⊤
x Cxuwu
s.t. w⊤
x Cxxwx = 1, w⊤
u Cuuwu = 1.
(16.42)
The solution is given by ﬁrst solving the generalized eigenvalue problem [1]
 0
Cxu
Cux
0
 wx
wu

= λ
 Cxx
0
0
Cuu
  wx
wu

,
(16.43)
and then normalizing the resulting directions to comply with the constraints of (16.42). Note that the
eigenvalue λ for a particular eigenvector
wx
wu

gives the corresponding correlation value
ρ = w⊤
x Cxuwu = w⊤
x (λCxxwx) = λ.
(16.44)
Consequently, all eigenvalues lie in the interval [−1, +1]. Interestingly, if
wx
wu

, λ is an eigenvector-
eigenvalue pair, so is
 wx
−wu

, −λ. Therefore, only half the spectrum, e.g., the positive eigenvalues,
are necessary to be considered, and the corresponding eigenvectors constitute desirable projection
directions (as with PCA, we often need more than one projection directions). The eigenvectors with
largest eigenvalues identify the strongest correlations.
Now we give the dual form of CCA to facilitate the derivation of kernel CCA [29–31]. Assume
that the dataset Sx,u is centered, that is, the mean value of each of the two representations is zero. We
consider expressing wx and wu as linear combinations of training examples
wx = X⊤αx,
wu = U⊤αu,
(16.45)
where the rows of X and U are vectors x⊤
i and u⊤
i (i = 1, . . . , m), respectively. Substituting (16.45)
into (16.42) results in
max
αx,αu
α⊤
x XX⊤UU⊤αu
s.t. α⊤
x XX⊤XX⊤αx = 1, α⊤
u UU⊤UU⊤αu = 1.
(16.46)
Since the above formulation only involves inner products among training examples, we can write down
the objective for kernel CCA simply as
max
αx,αu
α⊤
x KxKuαu
s.t. α⊤
x K2
xαx = 1, αu⊤K2
uαu = 1,
(16.47)
where Kx and Ku are the kernel matrices for the two representations, respectively (if data are not
centered in feature spaces, techniques similar to centering for kernel PCA can be adopted).
It was shown that overﬁtting with perfect correlations which fail to distinguish spurious features from
those revealing the underlying semantics can appear using the above versions of CCA and kernel CCA

868
CHAPTER 16 Kernel Methods and Support Vector Machines
[1,27]. In other words, some kind of regularization is needed to detect meaningful patterns. Statistical
stability analysis shows that controlling the norms of the two projection directions is a good way for
regularization [1]. Hence, we have the regularized CCA whose objective is to maximize
w⊤
x Cxuwu

(1 −τx)w⊤x Cxxwx + τx∥wx∥2 
(1 −τu)w⊤
u Cuuwu + τu∥wu∥2,
(16.48)
where regularization parameters τx and τu vary in the interval [0, 1]. The kernel regularized CCA
corresponding to (16.47) is given by optimizing
max
αx,αu
α⊤
x KxKuαu
s.t. (1 −τx)α⊤
x K2
xαx + τxα⊤
x Kxαx = 1,
(1 −τu)α⊤
u K2
uαu + τuα⊤
u Kuαu = 1.
(16.49)
1.16.3.3 Kernel Fisher discriminant analysis
The Fisher discriminant is a classiﬁcation function
f (x) = sign(w⊤x + b),
(16.50)
where the weight vector w is found through a speciﬁc optimization to well separate different classes.
In particular, a direction is found which maximizes the distance between projected class means and
simultaneously minimizes the projected class variances. In this article, the binary case is considered.
The parameter b in the Fisher discriminant is usually determined by projecting training data to w and
then identifying the middle point of two class means.
Suppose examples from two different classes are given by S1
=
{x1
1, . . . , x1
m1} and
S2 = {x2
1, . . . , x2
m2}. Fisher discriminant analysis [32,33] ﬁnds w which maximizes
J(w) = w⊤SBw
w⊤SWw,
(16.51)
where
SB = (m1 −m2)(m1 −m2)⊤,
SW =

i=1,2

x∈Si
(x −mi)(x −mi)⊤
(16.52)
are respectively the between and within class scatter matrices and mi is deﬁned by mi =
1
mi
mi
j=1 xi
j.
The solution is the eigenvector corresponding to the largest eigenvalue of the generalized eigen-
decomposition
SBw = λSWw.
(16.53)
Since the matrix SB has rank 1, only the leading eigenvector contains meaningful information.

1.16.3 Fundamental Kernel Methods
869
Let φ be a nonlinear map to some feature space F. Kernel Fisher discriminant analysis attempts to
ﬁnd a direction w ∈F to maximize
J(w) = w⊤Sφ
Bw
w⊤Sφ
Ww
,
(16.54)
where
Sφ
B = (mφ
1 −mφ
2 )(mφ
1 −mφ
2 )⊤,
Sφ
W =

i=1,2

x∈Si

φ(x) −mφ
i
 
φ(x) −mφ
i
⊤
(16.55)
with mφ
i =
1
mi
mi
j=1 φ(xi
j).
Deﬁne S = S1 ∪S2 and denote its elements by {x1, . . . , xm} with m = m1 + m2. We would like to
ﬁnd an expansion for w in the form w = m
i=1 αiφ(xi). It follows that
w⊤mφ
i = 1
mi
m

j=1
mi

k=1
α jκ(x j, xi
k) = α⊤Mi,
(16.56)
where vector Mi is deﬁned as (Mi) j =
1
mi
mi
k=1 κ(x j, xi
k) and the dot products are replaced with
kernels [33]. Based on (16.56), the numerator of (16.54) can be rewritten as
w⊤Sφ
Bw = α⊤Mα,
(16.57)
where M = (M1 −M2)(M1 −M2)⊤. And the denominator is rewritten as
w⊤Sφ
Ww = α⊤Nα,
(16.58)
where N = 
j=1,2 K j(I −1m j )K⊤
j , K j is an m × m j matrix with (K j)ik = κ(xi, x j
k), I is the identity
matrix and 1m j is the matrix with all entries
1
m j [33].
Hence, (16.54) is reformulated as
J(α) = α⊤Mα
α⊤Nα .
(16.59)
The problem can be solved similarly to (16.51). To enhance numerical stability and perform capacity
control in the feature space, N in the above formulation is usually replace by N + μI with positive μ.
An alternative regularization is penalizing ∥w∥2 as in kernel CCA instead of the current ∥α∥2 which
corresponds to the term μI.
1.16.3.4 SVMs for classiﬁcation and regression
Given the training set S = {(x1, y1), . . . , (xm, ym)} of points xi ∈Rn with corresponding labels
yi ∈{1, −1}, SVM classiﬁers attempt to ﬁnd a classiﬁcation hyperplane induced from the maximum

870
CHAPTER 16 Kernel Methods and Support Vector Machines
margin principle [7,8]. In real applications data are usually not linearly separable. Thus a loss on the
violation of the linearly separable constraints has to be introduced. A common choice is the hinge loss
max

0, 1 −yi(w⊤xi + b)

,
(16.60)
which can be represented by a slack variable ξi.
The optimization problem for SVM classiﬁcation is formulated as
min
w,b,ξ
1
2∥w∥2 + C
m

i=1
ξi
s.t.
yi(w⊤xi + b) ≥1 −ξi,
i = 1, . . . , m,
ξi ≥0,
i = 1, . . . , m,
(16.61)
where the scalar C controls the balance between the margin and empirical loss, and ξ = [ξ1, . . . , ξm]⊤.
The large margin principle is reﬂected by minimizing 1
2∥w∥2 with 2/∥w∥being the margin between
two hyperplanes w⊤x + b = 1 and w⊤x + b = −1 (For the linearly separable case, the concepts of the
margin and classiﬁcation hyperplane are illustrated in Figure 16.2). The SVM classiﬁer would be
cw,b(x) = sign(w⊤x + b).
(16.62)
The Lagrangian of problem (16.61) is
L(w, b, ξ, λ, γ ) = 1
2∥w∥2 + C
m

i=1
ξi −
m

i=1
λi

yi(w⊤xi + b) −1 + ξi

−
m

i=1
γiξi,
λi ≥0, γi ≥0,
(16.63)
FIGURE 16.2
An illustration of the margin and classiﬁcation hyperplane for the linearly separable binary case.

1.16.3 Fundamental Kernel Methods
871
where λ = [λ1, . . . , λm]⊤and γ = [γ1, . . . , γm]⊤are the associated Lagrange multipliers. Using the
superscript star to denote the solutions of the optimization problem, according to the KKT (Karush-
Kuhn-Tucker) conditions [34,35], we obtain
∂wL(w∗, b∗, ξ∗, λ∗, γ ∗) = w∗−
m

i=1
λ∗
i yixi = 0,
(16.64)
∂bL(w∗, b∗, ξ∗, λ∗, γ ∗) = −
m

i=1
λ∗
i yi = 0,
(16.65)
∂ξi L(w∗, b∗, ξ∗, λ∗, γ ∗) = C −λ∗
i −γ ∗
i = 0,
i = 1, . . . , m.
(16.66)
From (16.64), the solution w∗has the form
w∗=
m

i=1
λ∗
i yixi.
(16.67)
Since examples with λ∗
i = 0 can be omitted from the expression, the training examples for which
λ∗
i > 0 are called support vectors.
By substituting (16.64)–(16.66) into the Lagrangian, we can ﬁnally get the dual optimization problem
[35]
max
λ
λ⊤j −1
2λ⊤Dλ
s.t. λ⊤y = 0,
λ ⪰0,
λ ⪯Cj,
(16.68)
where j is the vector with all entries being 1, y = [y1, . . . , ym]⊤and D is a symmetric m × m matrix
with entries Di j = yi y jx⊤
i x j [1,36].
The complementary slackness condition (also called the zero KKT-gap requirement) [6] implies
λ∗
i

yi(x⊤
i w∗+ b∗) −1 + ξ∗
i

= 0,
i = 1, . . . , m,
γ ∗
i ξ∗
i = 0,
i = 1, . . . , m.
(16.69)
Combining (16.66) and (16.69), we can solve b∗= yi −x⊤
i w∗for any support vector xi with
0 < λ∗
i < C. The existence of 0 < λ∗
i < C is a reasonable assumption, though there lacks a rigorous
justiﬁcation [36]. Once λ∗and b∗are solved, the SVM classiﬁer is given by
c∗(x) = sign
 m

i=1
yiλ∗
i x⊤xi + b∗

.
(16.70)
Using the kernel trick, the optimization problem (16.68) for SVMs becomes
max
λ
λ⊤j −1
2λ⊤Dλ

872
CHAPTER 16 Kernel Methods and Support Vector Machines
s.t. λ⊤y = 0,
λ ⪰0,
λ ⪯Cj,
(16.71)
where the entries of D are Di j = yi y jκ(xi, x j). The solution for the corresponding SVM classiﬁer is
formulated as
c∗(x) = sign
 m

i=1
yiλ∗
i κ(xi, x) + b∗

.
(16.72)
For regression problems, the labels in the training set S are real numbers, that is yi ∈R (i = 1, . . . , m).
In order to induce a sparse representation for the decision function (i.e., some training examples can be
ignored), [8] devised the following ϵ-insensitive function and applied it to support vector regression
|y −f (x)|ϵ = max{0, |y −f (x)| −ϵ},
ϵ ≥0.
(16.73)
The standard form of support vector regression is to minimize
1
2∥w∥2 + C
m

i=1
|yi −f (xi)|ϵ,
(16.74)
where the positive scalar C reﬂects the trade-off between the margin and the empirical loss. An equivalent
optimization that is commonly used is
min
w,b,ξ,ξ∗
1
2∥w∥2 + C
m

i=1
ξi + C
m

i=1
ξ∗
i
s.t. ⟨w, φ(xi)⟩+ b −yi ≤ϵ + ξi,
yi −⟨w, φ(xi)⟩−b ≤ϵ + ξ∗
i ,
ξi, ξ∗
i ≥0,
i = 1, . . . , m,
(16.75)
where φ(xi) is the image of xi in the feature space, and ξ, ξ∗are deﬁned similarly as before. The
prediction output of support vector regression is
c∗(x) = ⟨w∗, φ(x)⟩+ b∗,
(16.76)
where w∗and b∗are the solution of (16.75). For support vector regression, the derivation for the
dual representation of solutions and the dual optimization problem can consult the counterpart for
classiﬁcation, and thus is omitted here.
1.16.3.5 Bayesian kernel methods: Gaussian processes
All the previous methods introduced in this section can be summarized into the framework of risk min-
imization. The Bayesian learning approach differs from them in several aspects. The key distinction is
that the Bayesian approach intuitively incorporates prior knowledge into the process of estimation [6].
Another beneﬁt of the Bayesian framework is the possibility of measuring the conﬁdence of the estima-
tion in a straightforward manner. However, algorithms designed by the Bayesian approach (e.g., with
maximum a posterior estimation) can have similar counterparts originating from the risk minimization

1.16.3 Fundamental Kernel Methods
873
framework. Below we focus on the Gaussian process approach for regression, which is a classical
Bayesian kernel method.
The Gaussian process models have two kinds of equivalent representations, namely the function-
space view and the weight-space view [37]. We will start with the weight-space view to illustrate the
explicit roles of kernels using the Bayesian treatment of linear regression, followed by a very brief
introduction of the function-space view.
Suppose the training set S is {(x1, y1), . . . , (xm, ym)} as deﬁned in Section 1.16.2.1. The standard
linear regression model with Gaussian noise is
f (x) = w⊤x,
y = f (x) + ε,
(16.77)
where f is the function value, y is the noisy observed value, and the noise obeys an independent,
identically distributed Gaussian distribution with mean zero and variance σ 2
n
ε ∼N(0, σ 2
n ).
(16.78)
This gives rise to the likelihood of the independent observations in the training set
p(y|X, w) =
m

i=1
p(yi|xi, w) =
m

i=1
1
√
2πσn
exp

−(yi −w⊤xi)2
2σ 2n

=
1
(2πσ 2n )m/2 exp

−∥y −Xw∥2
2σ 2n

= N(Xw, σ 2
n I),
(16.79)
where y = [y1, . . . , ym]⊤and X⊤= [x1, . . . , xm]. Suppose we specify a Gaussian prior on the
parameters with mean zero and covariance matrix p [37]
w ∼N(0, p).
(16.80)
According to Bayes’ rule, the posterior of the parameters is proportional to the product of the prior and
likelihood
p(w|X, y) ∝exp( −1
2w⊤−1
p w)exp

−1
2σ 2n
(y −Xw)⊤(y −Xw)

∝exp

−1
2(w −¯w)⊤A(w −¯w)

,
(16.81)
where A =
1
σ 2n X⊤X+−1
p , and ¯w =
1
σ 2n A−1X⊤y. It tends out that the posterior is a Gaussian distribution
with mean ¯w and covariance A−1.
The predictive distribution for a test example x is given by averaging the outputs of all possible linear
models from the above Gaussian posterior
p( f (x)|x, X, y) =

p( f (x)|x, w)p(w|X, y)dw
= N
 1
σ 2n
x⊤A−1X⊤y, x⊤A−1x

.
(16.82)

874
CHAPTER 16 Kernel Methods and Support Vector Machines
Now suppose we use a function φ( · ) to map the inputs in the original space to a feature space, and
perform linear regression there. The predictive distribution would be
p( f (x)|x, X, y) = N
 1
σ 2n
φ(x)⊤A−1⊤y, φ(x)⊤A−1φ(x)

,
(16.83)
where ⊤= [φ(x1), . . . , φ(xm)], and A =
1
σ 2n ⊤ + −1
p . Using matrix transformations such as the
matrix inversion lemma, we can rewrite (16.83) as
p( f (x)|x, X, y) = N

φ(x)⊤p⊤(K + σ 2
n I)−1y ,
φ (x)⊤pφ(x) −φ(x)⊤p⊤(K + σ 2
n I)−1pφ(x)

,
(16.84)
where K = p⊤[37]. Notice that in the above formulation the terms related to the images in the
feature space can be represented in the form of φ(x)⊤pφ(x′) with x and x′ in either the training or
test sets [37]. Deﬁne κ(x, x′) = φ(x)⊤pφ(x′). By Theorem 7, we know that κ(x, x′) is a valid kernel
function. In the Gaussian process literature, it is often called the covariance function.
The function-space view of the Gaussian processes is given by the following deﬁnition which
describes a distribution over functions [37].
Deﬁnition 8 (Gaussian processes).
A Gaussian process is a collection of random variables, any ﬁnite
number of which have a joint Gaussian distribution.
A Gaussian process is speciﬁed by its mean function and covariance function. If we deﬁne the mean
function m(x) and the covariance function k(x, x′) of a real process f (x) as
m(x) = E[ f (x)],
k(x, x′) = E

f (x) −m(x)
 
f (x′) −m(x′)

,
(16.85)
the Gaussian process can be written as
f (x) ∼GP

m(x), k(x, x′)

.
(16.86)
Figure 16.3 shows samples of functions drawn from a speciﬁc Gaussian process.
The Bayesian linear regression model f (x) = w⊤φ(x) with parameter prior w ∼N(0, p) can be
cast into the above function-space view. It is simple to see that the function values f (x1), . . . , f (xq)
corresponding to any number of inputs q are jointly Gaussian, and the mean and covariance are given by
E[ f (x)] = E[w⊤]φ(x) = 0,
E[ f (x) f (x′)] = φ(x)⊤E[ww⊤]φ(x′) = φ(x)⊤pφ(x′),
(16.87)
where the second equation recovers our deﬁnition of the kernel function for the weight-space view. In
other words, now m(x) = 0 and k(x, x′) = κ(x, x′) = φ(x)⊤pφ(x′).

1.16.4 Computational Issues of Kernel Methods
875
−1
−0.5
0
0.5
1
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Input
Output
FIGURE 16.3
Samples from a Gaussian process with zero mean and a Gaussian kernel as the covariance function.
1.16.4 Computational issues of kernel methods
Implementation of kernel methods often involve eigen-decomposition of kernel matrices or inversion of
the sum of a kernel matrix and a scaled identity matrix. The computational complexity of this operation
is typically O(m3) with m being the number of training examples. This can be very demanding for large
training sets, and therefore approximation algorithms are desirable.
Goodlow-rankapproximationsofkernelmatricesareusuallyenoughtodealwiththeaboveproblems.
For example, the matrix inversion lemma can use the low-rank decomposition to invert the sum of the
kernel matrix and a scaled identity matrix efﬁciently. Eigen-decomposition of kernel matrices can also
be converted to do the same operation on much smaller matrices. Here we just give a pointer to some
of the approximation methods. Interested readers can refer to the corresponding literature for detailed
implementation techniques.
Partial Gram-Schmidt orthogonalization [38] and incomplete Cholesky decomposition [27] are good
approachesforﬁndinglow-rankapproximationsofkernelmatrices.Thesetwoapproachesareessentially
equivalent, since performing a Cholesky decomposition of the kernel matrix is equivalent to performing

876
CHAPTER 16 Kernel Methods and Support Vector Machines
Gram-Schmidt orthogonalization in the feature space [1]. In other words, incomplete Cholesky decom-
position can be viewed as the dual implementation of the partial Gram-Schmidt orthogonalization.
The sparse greedy matrix approximation [39] and the Nyström approximation [40] are two alternative
approaches. The idea of the former is to select a collection of basis functions to obtain an approximate
kernel matrix 
K whose distance to the original kernel matrix K is small. The Nyström approximation
is much simpler, which randomly chooses r rows/columns of K without replacement, and then sets

K = Km,r K −1
r,r Kr,m, where Km,r is the m × r block of K and similar deﬁnitions apply to the other
blocks. For a given r, the sparse greedy matrix approximation produces a better approximation to K,
but computationally more demanding [40].
1.16.5 Multiple kernel learning
In practical problems, for a given decision-making task, there can be multiple different data sources.
These data can be heterogeneous, which means that they represent different properties (e.g., visual fea-
tures or lingual features) or have different forms (e.g., continuous value or discrete value). Consequently,
using a different kernel to account for each of the data sources and then combining them is sensible. In
other cases, even if the data are homogeneous, we may still want to adopt multiple kernels to capture
more information. This problem of learning a combination of multiple kernels is termed multiple kernel
learning [17] and now is an active research topic [41–47]. Here we review some multiple kernel learning
methods, several of which have sparsity regularizations [48,49], to provide a brief outline of the research
progress.
The kernel learning approach proposed by [17] is to add to the original optimization problems some
extra constraints on the symmetric kernel matrix K, e.g., by
K =
t
i=1
μi Ki,
μi ∈R,
i = 1, . . . , t,
K ⪰0,
trace(K) ≤c,
(16.88)
or
K =
t
i=1
μi Ki,
μi ≥0,
i = 1, . . . , t,
K ⪰0,
trace(K) ≤c,
(16.89)
where t is the number of individual kernels, and then formulate the problem in terms of semideﬁnite pro-
gramming (SDP) [34,35]. The advantages of the second set of constraints over the ﬁrst include reducing
computational burden and facilitating the study of some statistical properties of kernel matrices [17].

1.16.6 Applications
877
However, the learning problem would become intractable when the number of training examples or
kernels grow large.
[48] reformulated the problem and proposed a sequential minimal optimization (SMO) algorithm
to improve the efﬁciency. They used second-order cone programming (SOCP) and Moreau-Yosida
regularization to derive the SMO algorithm and made multiple kernel learning applicable for medium-
scale problems. The corresponding KKT conditions not only lead to support vectors, but also to “support
kernels” which means a sparse combination of candidate kernels can be expected. [50] adopted semi-
inﬁnite liner programming (SILP) to formulate the multiple kernel learning problem, based on which
they iteratively solved a standard SVM problem with a single kernel and a linear program whose
constraints increase with iterations. This approach makes multiple kernel learning applicable to large-
scale problems. Later, [49] further improved the efﬁciency by using a formulation of a weighted 2-norm
regularization with sparsity considerations imposed on the weights. Evidence show that it is globally
faster than the mentioned SILP approach but with more kernels selected.
[41] considered multiple kernel learning with inﬁnite number of basic kernels. In particular, kernels
are selected from the convex hull of continuously parameterized kernels. Making use of the conju-
gate function and von Neumann minimax theorem [44], they adopted a greedy algorithm to solve the
optimization problem, where the DC (difference of convex functions) programming techniques that
attempt to optimize a non-convex function by the difference of two convex functions [51] were used to
optimize a subroutine of the algorithm. Experimental results indicated the advantage of working with
a continuous parameterization over a predesignated ﬁnite number of basic kernels.
For Bayesian multiple kernel learning, recently, [52] proposed a new variational approximation for
inﬁnite mixtures of Gaussian processes. The mixtures of Gaussian processes have the advantages of
characterizing varying covariances or multimodal data and reducing the cubic computational complexity
of the single Gaussian process model [53–55]. They used mean ﬁeld variational inference and a truncated
stick-breaking representation of the Dirichlet process to approximate the posterior of latent variables,
and applied the approach to trafﬁc prediction problems.
1.16.6 Applications
The applications of kernel methods and SVMs are rather broad. Here we just list some of its typical
applications.
Biometrics refers to the identiﬁcation of humans based on their physical or behavioral traits, which
can be used for access control. Typical methods for biometrics include face recognition, signature
recognition and EEG-based biometrics [56]. Over the past years, kernel methods and SVMs have been
successfully applied to this ﬁeld [57,58].
Intelligent transportation systems are an important application platform for machine learning tech-
niques. Representative applications include pedestrian recognition, trafﬁc ﬂow forecasting, and trafﬁc
bottleneck identiﬁcation. Kernel methods including Gaussian processes have achieved very good per-
formance in this area [52,59].
Research on brain-computer interfaces which aim to enable severely disabled people to drive com-
munication or control devices, arouses many interests recently. The discrimination of different brain

878
CHAPTER 16 Kernel Methods and Support Vector Machines
signals is essentially a pattern classiﬁcation problem, where SVMs have been shown to be a very useful
tool [60,61].
Natural language processing, e.g., text classiﬁcation and retrieval, is an active research ﬁeld which
has used a lot of machine learning methods. Kernel techniques applied to this task include kernel design,
supervised classiﬁcation and semi-supervised classiﬁcation [14,62,63].
1.16.7 Open issues and problems
In this article, we have presented some key techniques for using kernel methods, such as how to derive
the dual formulation of an original method, what are essential conditions for valid kernels, typical
kernel functions, and how to construct new kernels. This constitutes the foundations of kernel methods.
Then, we introduced some fundamental kernel methods which are well-known and now used widely
for unsupervised or supervised learning. In particular, as a representative of Bayesian kernel methods,
Gaussian processes were introduced.
The computational complexity of kernel methods is usually cubic with respect to the number of
training examples. Therefore, reducing the computational costs has been an important research topic. For
thisproblem,webrieﬂypointedoutfourmethods—partialGram-Schmidtorthogonalization,incomplete
Cholesky decomposition, sparse greedy matrix approximation and the Nyström approximation, and
explained the idea on why they can be used to alleviate the computational burden.
In addition, we have introduced the recent developments on multiple kernel learning which has shown
its merit over single kernel learning in the past few years. However, for multiple kernel learning, we have
to learn both the combination coefﬁcients for candidate kernels and other parameters inherited from
traditional single kernel learning. Therefore, there are various efforts to reformulate the optimization
problem to accelerate learning, and indeed people have achieved some encouraging results.
Studies on kernel methods can be further deepened in different aspects, e.g., the above mentioned
multiple kernel learning, and combining kernel techniques with other machine learning mechanisms.
Another line of important open problems would be performing theoretical analysis on the generalization
errors of newly emerging kernel methods, such as the multitask SVMs and multitask multiclass SVMs.
We hope this article is helpful to promote the applications and theoretical developments of kernel
methods in the future.
Glossary
Canonical correlation analysis
A method to ﬁnd two linear transformations respectively for two
representations such that the correlations between the transformed
variables are maximized
Fisher discriminant analysis
A method for classiﬁcation which seeks a direction to maximize the
distance between projected class means and simultaneously mini-
mize the projected class variances
Gaussian process
A collection of random variables, any ﬁnite number of which have
a joint Gaussian distribution

References
879
Kernel trick
A method to extend any algorithm only involving computations of
inner products from the original space to a feature space with kernel
functions
Multiple kernel learning
A learning mechanism which aims to learn a combination of multi-
ple kernels to capture more information or reduce the computational
complexity
Principal component analysis
A method to ﬁnd a set of orthogonal directions which forms a sub-
space to maximize data variances along the directions in this sub-
space
Reproducing kernel Hilbert
space
A function space which is a Hilbert space possessing a reproducing
kernel
Support vector machine
A method to learn a hyperplane induced from the maximum margin
principle, which has wide applications including classiﬁcation and
regression
Acknowledgment
This work was supported in part by the National Natural Science Foundation of China under Project 61075005,
the Fundamental Research Funds for the Central Universities, and the PASCAL2 Network of Excellence. This
publication only reﬂects the authors’ views.
References
[1] J. Shawe-Taylor,N.Cristianini,Kernel Methods for Pattern Analysis, Cambridge University Press, Cambridge,
UK, 2004.
[2] F. Rosenblatt, The perceptron: A probabilistic model for information storage and organization in the brain,
Psychol. Reviews 65 (1958) 386–408.
[3] J. Hertz, A. Krogh, R. Palmer, Introduction to the Theory of Neural Computation, Addison-Wesley, Reading,
MA, 1991.
[4] L. Breiman, J. Friedman, R. Olshen, C. Stone, Classiﬁcation and Regression Trees, Wadsworth International,
Belmont, CA, 1984.
[5] J. Quinlan, C4.5: programs for Machine Learning, San Mateo, California: Morgan Kauffmann, 1993.
[6] B. Schölkopf, A. Smola, Learning with Kernels, MIT Press, Cambridge, MA, 2002.
[7] B. Boser, I. Guyon, V. Vapnik, A training algorithm for optimal margin classiﬁer, in: Proceedings of the 5th
ACM Worksop on Computational Learning Theory, 1992, pp. 144–152.
[8] V. Vapnik, The Nature of Statistical Learning Theory, Springer-Verlag, New York, 1995.
[9] M. Aizerman, E. Braverman, L. Rozonoer, Theoretical foundations of the potential function method in pattern
recognition learning, Autom. Remote Control 25 (1964) 821–837.
[10] R. Duda, P. Hart, Pattern Classiﬁcation and Scene Analysis, John Wiley & Sons, New York, 1973.
[11] T. Evgeniou, M. Pontil, Regularized multi-task learning, in: Proceedings of the 10th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data Mining, 2004, pp. 109–117.
[12] J. Farquhar, D. Hardoon, H. Meng, J. Shawe-Taylor, S. Szedmak, Two view learning: SVM-2K, theory and
practice, Adv. Neural Inform. Process. Syst. 18 (2006) 355–362.

880
CHAPTER 16 Kernel Methods and Support Vector Machines
[13] D. Rosenberg, V. Sindhwani, P. Bartlett, P. Niyogi, Multiview point cloud kernels for semisupervised learning,
IEEE Signal Process. Mag. 26 (2009) 145–150.
[14] S. Sun, J. Shawe-Taylor, Sparse semi-supervised learning using conjugate functions, J. Mach. Learn. Res. 11
(2010) 2423–2455.
[15] Y. Ji, S. Sun, Multitask multiclass support vector machines, in: Proceedings of the IEEE International Con-
ference on Data Mining Workshops, 2011, pp. 512–518.
[16] S. Sun, Multi-view Laplacian support vector machines, Lect. Notes Comput. Sci. 7121 (2011) 209–222.
[17] G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, M. Jordan, Learning the kernel matrix with semideﬁnite
programming, J. Mach. Learn. Res. 5 (2004) 27–72.
[18] C. Bishop, Pattern Recognition and Machine Learning, Springer-Verlag, New York, 2006.
[19] R. Duda, P. Hart, D. Stork, Pattern Classiﬁcation, John Wiley & Sons, New York, 2001.
[20] S. Theodoridis, K. Koutroumbas, Pattern Recognition, Academic Press 2008.
[21] N. Aronszajn, Theory of reproducing kernels, Trans. Am. Math. Soc. 68 (1950) 337–404.
[22] J. Mercer, Functions of positive and negative type and their connection with the theory of integral equations,
Philos. Trans. Roy. Soc., London, A 209 (1909) 415–446.
[23] T. Jaakkola, D. Haussler, Exploiting generative models in discriminative classiﬁers, Adv. Neural Inform.
Process. Syst. 11 (1999a) 487–493.
[24] T. Jaakkola, D. Haussler, Probabilistic kernel regression models, in: Proceedings of the International Confer-
ence on AI and, Statistics, 1999b, pp. 1–9.
[25] B. Schölkopf, A. Smola, K. Müller, Nonlinear component analysis as a kernel eigenvalue problem, Neural
Comput. 10 (1998) 1299–1319.
[26] H. Hotelling, Relations between two sets of variates, Biometrika 28 (1936) 321–377.
[27] F. Bach, M. Jordan, Kernel independent component analysis, J. Mach. Learn. Res. 3 (2002) 1–48.
[28] J. Kettenring, Canonical analysis of several sets of variables, Biometrika 58 (1971) 433–451.
[29] S. Akaho, A kernel method for canonical correlation analysis, in: Proceedings of the International Meeting of
the Psychometric Society 2001.
[30] C. Fyfe, P. Lai, ICA using kernel canonical correlation analysis, in: Proceedings of the International Workshop
on Independent Component Analysis and Blind Singal Separation, 2000, pp. 279–284.
[31] T. Melzer, M. Reiter, H. Bischof, Nonlinear feature extraction using generalized canonical correlation analysis,
in: Proceedings of the International Conference on Artiﬁcial, Neural Networks, 2001, pp. 353–360.
[32] K. Fukunaga, Introduction to Statistical Pattern Recognition, Academic Press, San Diego, CA, 1990.
[33] S. Mika, G. Rätsch, J. Weston, B. Schölkopf, K. Müller, Fisher discriminant analysis with kernels, Neural
Netw. Signal Process. IX (1999) 41–48.
[34] S. Boyd, L. Vandenberghe, Convex Optimization, Cambridge University Press, England, 2004.
[35] J. Shawe-Taylor, S. Sun, A review of optimization methodologies in support vector machines, Neurocomputing
74 (2011) 3609–3618.
[36] E. Osuna, R. Freund, F. Girosi, Support vector machines: training and applications, Technical Report AIM-
1602, Massachusetts Institute of Technology, MA, 1997.
[37] C. Rasmussen, C. Williams, Gaussian Processes for Machine Learning, MIT Press, Cambridge, MA, 2006.
[38] D. Hardoon, S. Szedmak, J. Shawe-Taylor, Canonical correlation analysis: an overview with application to
learning methods, Neural Comput. 16 (2004) 2639–2664.
[39] A. Smola, B. Schölkopf, Sparse greedy matrix approximation for machine learning, in: Proceedings of the
17th International Conference on, Machine learning, 2000, pp. 911–918.
[40] C. Williams, M. Seeger, Using the Nyström method to speed up kernel machines, Adv. Neural Inform. Process.
Syst. 13 (2001) 682–688.
[41] A. Argyriou, R. Hauser, C. Micchelli, M. Pontil, A DC-programming algorithm for kernel selection, in:
Proceedings of the 23rd International Conference on, Machine Learning, 2006, pp. 41–48.

References
881
[42] M. Girolami, S. Rogers, Hierarchic Bayesian models for kernel learning, in: Proceedings of the 22nd Inter-
national Conference on, Machine Learning, 2005, pp. 241–248.
[43] J. Li, S. Sun, Nonlinear combination of multiple kernels for support vector machines, in: Proceedings of the
20th International Conference on, Pattern Recognition, 2010, pp. 1–4.
[44] A. Micchelli, M. Pontil, Learning the kernel function via regularization, J. Mach. Learn. Res. 6 (2005) 1099–
1125.
[45] C. Ong, A. Smola, R. Williamson, Learning the kernel with hyperkernels, J. Mach. Learn. Res. 6 (2005)
1043–1071.
[46] Y. Ying, D. Zhou, Learnability of Gaussians with ﬂexible variances, J. Mach. Learn. Res. 8 (2007) 249–276.
[47] A. Zien, C. Ong, Multiclass multiple kernel learning, in: Proceedings of the 24th International Conference
on, Machine Learning, 2007, pp. 1191–1198.
[48] F. Bach, G. Lanckriet, M. Jordan, Multiple kernel learning, conic duality and the SMO algorithm, in: Pro-
ceedings of the 21st International Conference on, Machine Learning, 2004, pp. 6–13.
[49] A. Rakotomamonjy, F. Bach, S. Canu, Y. Grandvalet, More efﬁciency in multiple kernel learning, in: Pro-
ceedings of the 24th International Conference on, Machine Learning, 2007, pp. 775–782.
[50] S. Sonnenburg, G. Raetsch, C. Schaefer, B. Schölkopf, Large scale multiple kernel learning, J. Mach. Learn.
Res. 7 (2006) 1531–1565.
[51] R. Horst, V. Thoai, DC programming: Overview, J. Optimiz. Theory App. 103 (1999) 1–41.
[52] S. Sun, X. Xu, Variational inference for inﬁnite mixtures of Gaussian processes with applications to trafﬁc
ﬂow prediction, IEEE Trans. Intel. Transp. Syst. 12 (2011) 466–475.
[53] E. Meeds, S. Osindero, An alternative inﬁnite mixture of Gaussian process experts, Adv. Neural Inform.
Process. Syst. 18 (2006) 883–890.
[54] C. Rasmussen, Z. Ghahramani, Inﬁnite mixtures of Gaussian process experts, Adv. Neural Inform. Process.
Sys. 14 (2002) 881–888.
[55] V. Tresp, Mixtures of Gaussian processes, Adv. Neural Inform. Process. Syst. 13 (2001) 654–660.
[56] S. Sun, Multitask learning for EEG-based biometrics. in: Proceedings of the 19th International Conference
on, Pattern Recognition, 2008, pp. 1–4.
[57] A. Tefas, C. Kotropoulos, I. Pitas, Using support vector machines to enhance the performance of elastic graph
matching for frontal face authentication, IEEE Trans. Pattern Anal. Mach. Intel. 23 (2001) 735–746.
[58] E. Justino, F. Bortolozzi, R. Sabourin, A comparison of SVM and HMM classiﬁers in the off-line signature
veriﬁcation. Pattern Recogn. Lett. 26, 1377–1385.
[59] S. Munder, D. Gavrila, An experimental study on pedestrian classiﬁcation, IEEE Trans. Pattern Anal. Mach.
Intell. 28 (2006) 1863–1868.
[60] D. Garrett, D. Peterson, C. Anderson, M. Thaut, Comparison of linear, nonlinear, and feature selection methods
for EEG signal classiﬁcation, IEEE Trans. Neural Syst. Rehabil. Eng. 11 (2003) 141–144.
[61] S. Sun, C. Zhang, D. Zhang, An experimental evaluation of ensemble methods for EEG signal classiﬁcation,
Pattern Recogn. Lett. 28 (2007) 2157–2163.
[62] M. Collins, N. Duffy, Convolution kernels for natural language, Advances in Neural Information Processing
Systems 14 (2001) 625–632.
[63] T. Joachims, Text categorization with support vector machines: Learning with many relevant features, Lect.
Notes Comput. Sci. 1398 (1998) 137–142.

17
CHAPTER
Online Learning in Reproducing
Kernel Hilbert Spaces
Konstantinos Slavakis∗, Pantelis Bouboulis†, and Sergios Theodoridis†
*University of Peloponnese, Department of Telecommunications Science and Technology,
Karaiskaki St., Tripolis 22100, Greece
†University of Athens, Department of Informatics and Telecommunications, Ilissia, Athens 15784, Greece
Nomenclature
Training Data
A set of input-output measurements that are used to
train a speciﬁc learning machine
Online Learning
A learning task, where the data points arrive sequen-
tially and used only once
Batch Learning
A learning task, where the training data are known
beforehand
Regression Task
A learning task that estimates a parametric input-
output relationship that best ﬁts the available data
Classiﬁcation Task
A learning task that classiﬁes the data into one or
more classes
Overﬁtting
A common notion in the Machine Learning litera-
ture. If the adopted model is too complex then the
learning algorithm tends to ﬁt to the training data as
much as possible (e.g., it ﬁts even the noisy data)
and cannot perform well when new data arrive
Regularization
A technique that biases the solution towards a
smoother result. This is usually achieved by adding
the norm of the solution to the cost function of the
minimization task
Sparsiﬁcation
A method that biases the solution towards a result,
that the least signiﬁcant components are pushed to
zero
Reproducing Kernel Hilbert Spaces (RKHS)
An inner product space of functions that is complete
and has a special structure
Kernel
A positive deﬁnite function of two variables that is
associated to a speciﬁc RKHS
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00017-6
© 2014 Elsevier Ltd. All rights reserved.
883

884
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
Fréchet Differentiation
A mathematical framework that generalizes the
notion of the standard derivative, to more general
function spaces
Least Mean Squares
This is a class of stochastic online learning tasks,
which aim to ﬁnd the parameters of an input-output
relation by minimizing the least mean squares of
the error at each iteration (difference between the
desired and the actual output), using a gradient
descent rationale
Recursive Least Squares
A class of deterministic online learning tasks, which
aimtoﬁndtheparametersofaninput-outputrelation
by minimizing the least mean squares of the total
error (the sum of differences between the desired
and the actual output up to the current iteration)
using a recursive Newton-like method
Adaptive Projected Subgradient Method (APSM) A convex analytic framework for online learning.
Based on set theoretic estimation arguments, and
ﬁxed point theory, the APSM is tailored to attack
online learning problems related to general, con-
vexly constrained, non-smooth optimization tasks
Wirtinger’s Calculus
A methodology for computing gradients of real val-
ued functions that are deﬁned on complex domains,
using the standard rules of differentiation
1.17.1 Introduction
A large class of Machine Learning tasks becomes equivalent with estimating an unknown functional
dependence that relates the so called input (cause) to the output (effect) variables. Most often, this
function is chosen to be of a speciﬁc type, e.g., linear, quadratic, etc., and it is described by a set of
unknown parameters. There are two major paths in dealing with this set of parameters.
The ﬁrst one, known as Bayesian Inference, treats the parameters as random variables, [1]. According
to this approach, the dependence between the input and output variables is expressed via a set of pdfs.
The task of learning is to obtain estimates of the pdf that relates the input-output variables as well as
the pdf that describes the random nature of the parameters. However, the main concern of the Bayesian
Inference approach is not in obtaining speciﬁc values for the parameters. Bayesian Inference can live
without this information. In contrast, the alternative path, which will be our focus in this paper, considers
the unknown parameters to be deterministic and its major concern is to obtain speciﬁc estimates for
their values.
Both trends share a common trait; they dig out the required information, in order to build up their
estimates, from an available set of input-output measurements known as the training set. The stage

1.17.1 Introduction
885
of estimating the pdfs/parameters is known as training and this philosophy of learning is known as
supervised learning. This is not the only approach which Machine Learning embraces in order to
“learn” from data. Unsupervised and semi-supervised learning are also major learning directions, [2].
In the former, no measurements of the output variable(s) are available in the training data set. In the latter,
there are only a few. In this paper, we will focus on the supervised learning of a set of deterministically
treated parameters. We will refer to this task as parameter estimation.
In the parameter estimation task, depending on the adopted functional dependence, different models
result. At this point, it has to be emphasized that the only “truth” that the designer has at his/her disposal
is the set of training data. The model that the designer attempts to “ﬁt” in the data is just a concept in
an effort to be able to exploit and use the available measurements for his/her own beneﬁt; that is, to
be able to make useful predictions of the output values in the future, when new measurements of the
inputs, outside the training set, become available. The ultimate goal of the obtained predictions is to
assist the designer to make decisions, once the input measurements are disclosed to him/her. In nature,
one can never know if there is a “true” model associated with the data. Our conﬁdence in “true” models
only concerns simulated data. In real life, one is obliged to adopt a model and a training method to
learn its parameters so that the resulting predictions can be useful in practice. The designed Machine
Learning system can be used till it is substituted by another, which will be based on a new model and/or
a new training method, that leads to improved predictions and which can be computed with affordable
computational resources.
In this paper, we are concerned with the more general case of nonlinear models. Obviously, different
nonlinearities provide different models. Our approach will be in treating a large class of nonlinearities,
which are usually met in practice, in a unifying way by mobilizing the powerful tool of Reproducing
KernelHilbertSpaces(RKHS).Thisisanoldconcept,thatwasgraduallydevelopedwithinthefunctional
analysis discipline [3] and it was, more recently, popularized in Machine Learning in the context of
Support Vector Machines. This methodology allows one to treat a wide class of different nonlinearities
in a unifying way, by solving an equivalent linear task in a space different than the one where the original
measurements lie. This is done by adopting an implicit mapping from the input space to the so called
feature space. In the realm of RKHSs, one needs not to bother about the speciﬁc nature of this space,
not even about its dimensionality. Once the parametric training has been completed, the designer can
“try” different implicit mappings, which correspond to different nonlinearities and keep the one that ﬁts
best his/her needs. This is achieved during the so-called validation stage. Ideally, this can be done by
testing the performance of the resulting estimator using a data set that is independent of the one used
for training. In case this is not possible, then various techniques are available in order to exploit a single
data set, both for training and validation, [2].
Concerning the estimation process, once a parametric modeling has been adopted, the parameter
estimation task relies on adopting, also, a criterion that measures the quality of ﬁt between the predicted
values and the measured ones, over the training set. Different criteria result in different estimates and it
is up to the designer to ﬁnally adopt the one that better ﬁts his/her needs. In the ﬁnal step of parameter
estimation, an algorithm has to be chosen to optimize the criterion. There are two major philosophies
that one has to comply with. The more classical one is the so-called batch processing approach. The
training data are available in “one go.” The designer has access to all of them simultaneously, which
are then used for the optimization. In some scenarios, algorithms are developed so that to consider one

886
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
training point at a time and consider all of them sequentially, and this process goes on till the algorithm
converges. Sometimes, such schemes are referred to as online; however, these schemes retain their batch
processing ﬂavor, in the sense that the training data set is ﬁxed and the number of the points in it is
known before the training starts. Hence, we will refer to these as batch techniques and keep the term
online for something different.
All batch schemes suffer from a major disadvantage. Once a new measurement becomes available,
after the training has been completed, the whole training process has to start from scratch, with a new
training set that has been enriched by the extra measurement. No doubt, this is an inefﬁcient way to
attack the task. The most efﬁcient way is to have methods that perform the optimization recursively, in
the sense of updating the current estimate every time a new measurement is received, by considering only
this newly received information and the currently available estimate of the parameters. The previously
used training data will never be needed again. They have become part of the history; all information
that they had to contribute to the training process now resides in the current estimate. We will refer to
such schemes as online or adaptive, although many authors call them time-recursive or sequential. The
term Adaptive Learning is mainly used in Signal Processing and such schemes have also been used
extensively in Communications, System Theory and Automatic Control from the early sixties, in the
context of the LMS, RLS and Kalman Filtering, e.g., [4–6]. The driving force behind the need for such
schemes was the need for the training mechanism to be able to accommodate slow time variations of the
learning environment and slowly forget the past. As a matter of fact, such a philosophy imitates better
the way that human brain learns and tries to adapt to new situations. Online learning mechanisms have
recently become very popular in applications such as Data Mining and Bioinformatics and more general
in cases where data reside in large data bases, with massive number of training points, which cannot
be stored in the memory and have to be considered one at a time. It is this path of online parameter
estimation learning that we will pursue in this paper. As we will see, it turns out that very efﬁcient
schemes, of even linear complexity per iteration with respect to the number of the unknown parameters,
are now available.
Our goal in this overview is to make an effort to collect a number of online/adaptive algorithms,
which have been proposed over a number of decades, in a systematic way under a common umbrella;
they are schemes that basically stem from only a few generic recurrences. It is interesting to note that
some of these algorithms have been discovered and used under different names in different research
communities, without one being referring to the other.
In an overview paper, there is always the dilemma how deep one can go in proofs. Sometimes proofs
are completely omitted. We decided to keep some proofs, associated with the generic schemes; those
which do not become too technical. Proofs give the newcomer to the ﬁeld the ﬂavor of the required
mathematical tools and also give him/her the feeling of what is behind the performance of an algorithm;
in simple words, why the algorithm works. For those who are not interested in proofs, simply, they can
bypass them. It was for the less mathematically inclined readers, that we decided to describe ﬁrst two
structurally simple algorithms, the LMS and the RLS. We left the rest, which are derived via convex
analytic arguments, for the later stage. As we move on, the treatment may look more mathematically
demanding, although we made an effort to simplify things as much as we could. After all, talking about
algorithms one cannot just use “words” and a “picture” of an algorithm. Although sometimes this is a
line that is followed, the authors of this paper belong to a different school. Thus, this paper may not
address the needs of a black-box user of the algorithms.

1.17.2 Parameter Estimation: The Regression and Classiﬁcation Tasks
887
1.17.2 Parameter estimation: The regression and classiﬁcation tasks
In the sequel, R, N, and N∗will stand for the set of all real numbers, non-negative and positive integers,
respectively.
Two of the major pillars in parametric modeling in Machine Learning are the regression and classi-
ﬁcation tasks. These are two closely related, yet different tasks.
In regression, given a set of training points, (yn, xn), n = 1, 2, . . . , yn ∈R, xn ∈Rl,1 the goal is
to establish the input-output relationship via a model of the form
yn = f (xn) + ηn,
n = 1, 2, . . . ,
(17.1)
where ηn is a noise, unobservable, sequence and the nonlinear function is modeled in the form
f (x) = θ0 +
K−1

k=1
θkφk(x),
(17.2)
where θk, k = 0, 1, . . . , K −1, comprise the set of the unknown parameters, and φk(x) are preselected
nonlinear functions
φk( · ) : Rl →R,
k = 1, 2, . . . , K −1.
The input vectors are also known as regressors. Combining (17.1) and (17.2) we obtain
yn = θ0 +
K−1

k=1
θkφk(xn) + ηn := θ T φ(xn) + ηn,
(17.3)
where
φ( · ) = [φ1( · ), φ2( · ), . . . , φK−1( · ), 1]T , and θ = [θ1, θ2, . . . , θK−1, θ0]T .
θ0 is known as the bias or the intercept and it has been absorbed in θ by simultaneously adding 1 as the last
element in φ. The goal of the parameter estimation task is to obtain estimates ˆθ of θ using the available
training set. Once ˆθ has been obtained and given a value of x, the prediction of the corresponding output
value is computed according to the model
ˆy = ˆθ
T φ(x).
(17.4)
Figure 17.1 shows the graphs of (17.4) for two different cases of training data, (yn, xn). In Figure 17.1a,
the ﬁtted model is
ˆy = −0.5 + x,
and for the case of Figure 17.1b, the model is
ˆy = −3 −2φ1(x) + φ2(x),
1In general, y ∈Rm. Unless otherwise stated, we will consider y to be a real value for simplicity. Generalizations to Euclidean
spaces are also possible.

888
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
−5
0
5
−5
0
5
x
y
(a) Linear model.
−5
0
5
−5
0
5
y
x
(b) Nonlinear model.
FIGURE 17.1
Fitting sets of training data (yn, xn)N
n=1 by a linear and a quadratic function.
where
φ1(x) = x,
φ2(x) = x2.
The ﬁgures also reveal the main goal of regression, which is to “explain” the generation mechanism of
the data, and the graph of the curve deﬁned in (17.4) should be designed so that to follow the spread of
the data in the (y, x) space as close as possible.
In classiﬁcation, the task is slightly different. The output variables are of a discrete/qualitative nature.
That is, yn ∈D, where D is a ﬁnite set of discrete values. For example, for the simple case of a two-class
classiﬁcation problem, one may select D = {1, −1} or D = {1, 0}. The output values are also known
as class labels. The input vectors, x, are known as feature vectors, and they are chosen so that the
respective components to encapsulate as much class-discriminatory information as possible. The goal
of a classiﬁcation task is to design a function (or in the more general case a set of functions), known as
the classiﬁer, so that the corresponding (hyper)surface f (x) = 0, in the x space, to separate the points
that belong to different classes as much as possible. That is, the purpose of a classiﬁer is to divide the
input space into regions, where each one of the regions is associated with one and only one of the classes.
The surface (and let us stay here in the simple case of the two-class problem with a single function) that
is realized in the input space is known as decision surface. In this paper, we will consider functions of
the same form as in (17.2). Hence, once more, designing a classiﬁer is cast as a parameter estimation
task. Once estimates of the unknown parameters are computed, say, ˆθ, given a feature vector x, which
results from a pattern whose class label, y, is unknown, the predicted label is obtained as
ˆy = g( f (x)),
(17.5)
where g( · ) is a nonlinear indicator function, which indicates on which side of f (x) = 0 the pattern x
lies; typically g(·) = sign(·), that is, the sign function. Figure 17.2a shows examples of two classiﬁers.
In Figure 17.2 the classiﬁer is linear, i.e.,
f (x) = x1 −3x2 = 0,

1.17.2 Parameter Estimation: The Regression and Classiﬁcation Tasks
889
−8
−4
0
4
8
−3
−1
0
1
3
x1
x1
(a) Linear classiﬁer.
−8
−4
0
4
8
−6
−2
0
2
6
x2
x2
(b) Nonlinear classiﬁer.
FIGURE 17.2
Two examples of classiﬁers.
and in Figure 17.2b is a nonlinear one
f (x) = −0.23 + φ1(x) + φ2(x) = 0,
where
φ1(x) = xT
 0.61
−4.64

,
φ2(x) = xT
−0.45 0.93
0.93 −0.45

x.
Note that, as it is apparent from both examples, the classes cannot, in general, be completely separated
by the classiﬁers and errors in predicting the class labels are unavoidable. Tasks with separable classes
are possible to occur, yet this is rather the exception than the rule.
Regression and Classiﬁcation are two typical Machine Learning tasks, and a wide range of learning
applications can be formulated as either one of the two. Moreover, we discussed that both tasks can
be cast as parameter estimations problems, which comprise the focus of the current paper. Needless to
say that both problems can also be attacked via alternative paths. For example, the k-nearest neighbor
method is another possibility, which does not involve any parameter estimation phase; the only involved
parameter, k, is determined via cross validation, e.g., [2].
Let us now put our parameter estimation task in a more formal setting. We are given a set of training
points (yn, xn) ∈R × Rl. For classiﬁcation tasks, we replace R with D. We adopt a parametric class
of functions,
F =

fθ( · ) : θ ∈A ⊆RK 
.
(17.6)
Our goal is to ﬁnd a function in F, denoted as fθ∗( · ), such that, given a measured value of x and an
adopted prediction model (e.g., (17.4) for regression or (17.5) for classiﬁcation), it approximates the
respective output value, ˆy, in an optimal way. Obviously, the word optimal paves the way to look for an
optimizing criterion. To this end, a loss function is selected, from an available palette of non-negative
functions,
L : R × R →[0, ∞),
(17.7)

890
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
and compute θ∗so that to minimize the total loss over all the training data points, i.e.,
fθ∗( · ) : θ∗∈argmin
θ∈A
J(θ),
(17.8)
where
J(θ) :=
N

n=1
L(yn, fθ(xn)),
(17.9)
assuming that a minimum exists. In an adaptive setting, minimizing the cost function takes place
iteratively in the form
θn = θn−1 + error correction term,
(17.10)
as n increases. Different class of functions F and different loss functions result in different estimators.
In practice, for ﬁxed number of training points, one uses cross validation to determine the “best”
choice. Of course, in practice, when adaptive techniques are used and n is left to vary, cross validation
techniques loose their meaning, especially when the statistical properties of the environment are slowly
time varying. In such cases, the choice of the class of functions as well as the loss functions, which now
has to be minimized adaptively, are empirically chosen. The critical factors, as we will see, for such
choices are (a) computational complexity, (b) convergence speed, (c) tracking agility, (d) robustness to
noise and to numerical error accumulation. In this paper, a number of different class functions as well
as loss functions will be considered and discussed.
1.17.3 Overﬁtting and regularization
A problem of major importance in any Machine Learning task is that of overﬁtting. This is related
to the complexity of the adopted model. If the model is too complex, with respect to the number of
training points, N, then is tends to learn too much from the speciﬁc training set on which it is trained.
For example, in regression, it tries to learn not only the useful information associated with the input data
but also the contribution of the noise. So, if the model is too complex, it tends to ﬁt very well the data
points of the training set, but it does not cope well with data outside the training set; as we say, it cannot
generalize well. In classiﬁcation, the source of overﬁtting lies in the class overlap. A complex classiﬁer
will attempt to classify correctly all the points of the training set, and this is quite common in practice
when very complex classiﬁers are employed. However, most of the class discriminatory information
lies in the points lying in the “non-overlapping” regions. This is the information that a classiﬁer has to
learn in order to be able to generalize well, when faced with data outside the training set. A classiﬁer
which is designed to give very low error rates on the training set, usually results in large errors when
faced with unseen data.
On the other hand, if our model is too simple it does not have the ability to learn even the useful
information, which is contributed by the input data themselves, hence leading to very poor predictions.
In practice, one seeks for a tradeoff, see, e.g., [2]. Figure 17.3 shows three cases of ﬁtting by a very
simple, a very complex and the correct model (that corresponds to the model which generated the data
points). The ﬁrst one is a linear model, the second one corresponds to a polynomial of high degree, i.e.,
80, and the third one to a quadratic polynomial. Note that for the case of Figure 17.3b, the resulting

1.17.3 Overﬁtting and Regularization
891
−1
−0.5
0
0.5
1
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
x
y
(a) Linear polynomial.
−1
−0.5
0
0.5
1
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
x
y
(b) High degree polynomial.
−1
−0.5
0
0.5
1
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
x
y
(c) Quadratic polynomial.
FIGURE 17.3
Fitting noisy data by polynomials of various degrees.
curve passes through all the points. However, this is not what we want, since the points we have are
the result of a noisy process, and the curve “follows” a path which is the combined result of the useful
as well as the noise-contributed information. In practice, the performance of the resulting estimator is
measured as the misﬁt, using a loss criterion (e.g., mean squared error, probability of error) between the
predicted values and the true ones, over an independent set of data or using cross-validation techniques;
for example, the resulting misﬁt is expected to be larger for the case of ﬁtting the high order polynomial
compared to the one corresponding to the quadratic polynomial of Figure 17.3c.
The reader may have noticed that we have avoided to deﬁne the term “complex” and we used it in
a rather vague context. Complexity is often directly related to the number of the unknown parameters.
However, this is not always the case. Complexity is a more general property and sometimes we may have
classiﬁers in, even, inﬁnite dimensional spaces, trained with a ﬁnite number of points, which generalize
very well, [7].
A way to cope with the overﬁtting problem is to employ the regularization method. This is an elegant
mathematical tool, that simultaneously cares to serve both desires; to keep the contribution of the loss
function term as small as possible, since this accounts for the accuracy of the estimator, but at the same
time to retain the ﬁtted model as less complex as possible. This is achieved by adding a second term in

892
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
the cost function in (17.9), whose goal is to keep the norm of the parameters as small as possible, i.e.,
J(θ) :=
N

n=1
L(yn, fθ(xn)) + λ( ∥θ∥),
(17.11)
where ( · ) is a strictly increasing monotonic non-negative function and ∥·∥is a norm, e.g., the
Euclidean (l2) or the l1 norms. The parameter λ is known as the regularization parameter and it controls
the relative signiﬁcance of the two terms in the cost function. For ﬁxed N, λ is usually determined via
cross-validation. In a time varying adaptive mode of operation, when cross-validation is not possible,
then empirical experience comes to assist or some adhoc techniques can be mobilized, to make λ = λN,
i.e., time varying. Take as an example the case where the environment does not change and the only
reason that we use an adaptive algorithm is due to complexity reasons. Then, as N →∞, the value of
λ should go to zero. For very large values of N, overﬁtting is no problem, since the complexity remains
a small portion of the data and only the loss-related term in the cost function is sufﬁcient. As a matter
of fact, the regularization term, in this case, will only do harm by biasing the solution away from the
optimum, this time without reason.
Very often,  is chosen so that
( ∥θ∥) =
K−1

k=0
|θk|2, or ( ∥θ∥) =
K−1

k=0
|θk|,
the latter being the l1 norm, which has been popularized in the context of sparsity-aware learn-
ing/compressed sensing. Note that, usually, the bias term is not included in the norm, since it can
make the solution sensitive to the shifts of the origin. This can easily be checked out for the case of the
Least Squares loss function
L(y, fθ(x)) = (y −θ T φ(x))2.
In practice, very often, if the mean values are not zero, we use centered data by subtracting from x its
sample mean, and then we can neglect θ0. Besides the norm, other functions have also been used as
regularizers, e.g., [8].
Using (17.2) and plugging it in the loss function-related term (17.11), then after minimization and
due to the presence of the norm-related term, some of the components of θ are pushed towards zero.
This pushing towards zero is more aggressive when the l1 norm is used, see, e.g., [9]. Obviously, the
terms which are pushed to zero are those whose elimination affects the least the loss function term,
which implicitly results in less complex models.
Sparsiﬁcation is another term used to indicate that the adopted estimation method has the abil-
ity to push the least signiﬁcant, from the accuracy point of view (as this is measured my the loss
function-related term in the regularized cost function) to zero. This is a way to guard against overﬁt-
ting. Regularization techniques gain in importance and become unavoidable within a certain class of
nonlinear modeling.
Consider the following modeling,
φk(x) = κ(x, xk),
k = 1, 2, . . . , N,
(17.12)

1.17.4 Mapping a Nonlinear to a Linear Task
893
where
κ(·, ·) : Rl × Rl →R,
(17.13)
which, dressed with some properties, as we will soon see, is known as kernel. In other words, the
expansion of our nonlinear function in (17.2) involves as many terms as our training data points, and
each term in the expansion is related with one point from the training set, that is,
f ( · ) =
N

k=1
θkκ(·, xk) + θ0.
(17.14)
Such models are met in the context of Support Vector Machines as well as in Relevance Vector Machines,
[1,2,7]. As we will soon see, this class of models has a strong theoretical justiﬁcation. In any case,
such models demand drastic sparsiﬁcation, otherwise they will suffer for strong overﬁtting. Moreover,
besides overﬁtting, in the adaptive case, where N keeps increasing, such a model would soon become
unmanageable. This paper will discuss a number of related sparsiﬁcation techniques.
1.17.4 Mapping a nonlinear to a linear task
Let us now look at the expansion in (17.2) in conjunction with our prediction models given in (17.4)
for the regression and (17.5) for the classiﬁcation tasks. Basically, all that such a modeling says is that
if we map our original input l-dimensional space into a new K-dimensional space,
Rl ∋x 	→φ(x) ∈RK ,
(17.15)
we expect our task to be sufﬁciently well modeled by adopting a linear model in this new space. It
is only up to us to choose the proper functions, φk( · ), as well as the appropriate dimensionality of
the new space, K. For the classiﬁcation task, such a procedure has a strong theoretical support from
Cover’s theorem, which states that: Consider N randomly located points in an l-dimensional space and
use a mapping to map these points into a higher dimensional space. Then the probability of locating
them, in the new space, in linearly separable groupings tends to one as the dimensionality, K, of the new
space tends to inﬁnity. Figure 17.4 illustrates how an appropriate mapping can transform a nonlinearly
separable task to a linearly separable one. Notice that nature cannot be fooled. After the mapping in the
higher 2-dimensional space, the points continue to live in an l-dimensional manifold [10] (paraboloid
in this case). The mapping gives the opportunity to our l-dimensional points to take advantage of the
freedom, which a higher dimensional space provides, so that to move around and be placed in linearly
separable positions, while retaining their original l-dimensional (true degrees of freedom) nature; a
paraboloid in the 2-dimensional space can be fully described in terms of one free parameter.
Although such a method sounds great, it has its practical drawbacks. To transform a problem into
a linear one, we may have to map the original space to a very high dimensional one, and this can
lead to a huge number of unknown parameters to be estimated. However, the mathematicians have,
once more, offered to us an escape route that has been developed many years ago. The magic name is
Reproducing Kernel Hilbert Spaces. Choosing to map to such spaces, and some of them can even be of

894
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
R
R
FIGURE 17.4
Transforming linearly non-separable learning tasks to separable ones by using nonlinear mappings to higher
dimensional spaces. The original data, i.e., two “squares” and a “circle,” are placed on the 1-dimensional
space, R, rendering thus the linear classiﬁcation task intractable. However, by using the polynomial function
(x) : = x2, ∀x ∈R, we are able to map the original data to a manifold which lies in the 2-dimensional
space R2. The mapped data can be now easily linearly separated, as this can be readily veriﬁed by the linear
classiﬁer which is illustrated by the straight, black, solid line of the ﬁgure.
inﬁnite dimension, makes computations much easier; moreover, there is an associated strong theorem
that justiﬁes the expansion in (17.14). Assume that we have selected a mapping
Rl ∋x 	→φ(x) ∈H,
(17.16)
where H is RKHS. Then, the following statements are valid:
•
The Kernel Trick: Let x1, x2 be two points in our original l-dimensional space and φ(x1), φ(x2)
their respective images in an RKHS, H. Since H is a Hilbert space, an inner product, ⟨·, ·⟩H, is
associated with it, by the deﬁnition of Hilbert spaces. As we will more formally see in the next
section, the inner product of any RKHS is uniquely deﬁned in terms of the so called kernel function,
κ(·, ·) and the following holds true
⟨φ(x1), φ(x2)⟩H = κ(x1, x2).
(17.17)
This is a remarkable property, since it allows to compute inner products in a (high dimensional)
RKHS as a function operating in the lower dimensional input space. Also, this property allows the
following “black box” approach, that has extensively been exploited in Machine Learning:
•
Design an algorithm, which operates in the input space and computes the estimates of the
parameters of a linear modeling.
•
Cast this algorithm, if of course this is possible, in terms of inner product operations only.
•
Replace each inner product with a kernel operation.

1.17.5 Reproducing Kernel Hilbert Spaces
895
Then the resulting algorithm is equivalent with solving the (linear modeling) estimation task in an
RKHS, which is deﬁned by the selected kernel. Different kernel functions correspond to different
RKHSs and, consequently, to different nonlinearities; each inner product in the RKHS is a nonlinear
function operation in the input space. The kernel trick was ﬁrst used in [11,12].
•
Representer Theorem: Consider the cost function in (17.11), and constrain the solutions to lie within
an RKHS space, deﬁned by a kernel function κ(·, ·). Then the minimizer of (17.11) is of the form
f ( · ) =
N

n=1
θnκ(·, xn) + θ0.
(17.18)
This makes crystal clear the reason for the speciﬁc modeling in (17.14). In other words, if we choose
to attack a nonlinear modeling by “transforming” it to a linear one, after performing a mapping of
our data into a RKHS in order to exploit the advantage of efﬁcient inner product operations, then
the (linear with respect to the parameters) modeling in (17.14) results naturally from the theory. Our
only concern now is to use an appropriate regularization, in order to cope with overﬁtting problems.
All the previously presented issues will be discussed in more detail in the sections to come. Our goal,
so far, was to try to give the big picture of the paper in simple terms and focus more on the notions than
on mathematics. We will become more formal in the sections to come. The reader, however, may bypass
some of the sections, if his/her current focus is not on the mathematical details but on the algorithms.
As said already before, we have made an effort to write the paper in such a way, so that to serve both
needs: mathematical rigor and practical ﬂavor, as much as we could.
1.17.5 Reproducing kernel Hilbert spaces
In kernel-based methods, the notion of the Reproducing Kernel Hilbert Space (RKHS) plays a crucial
role. A RKHS is a rich construct (roughly, a space of functions with an inner product), which has been
proven to be a very powerful tool. Kernel-based methods are utilized in an increasingly large number
of scientiﬁc areas, especially where nonlinear models are required. For example, in pattern analysis, a
classiﬁcation task of a set X ⊂Rl is usually reformed by mapping the data into a higher dimensional
space (possibly of inﬁnite dimension) H, which is a Reproducing Kernel Hilbert Space (RKHS). The
advantage of such a mapping is to make the task more tractable, by employing a linear classiﬁer in the
feature space H, exploiting Cover’s theorem (see [2,13]). This is equivalent with solving a nonlinear
problem in the original space.
Similar approaches have been used in principal components analysis, in Fisher’s linear discriminant
analysis, in clustering, regression, image processing and in many other subdisciplines. Recently, pro-
cessing in RKHS is gaining in popularity within the Signal Processing community in the context of
adaptive learning.
Although the kernel trick works well for most applications, it may conceal the basic mathematical
steps that underlie the procedure, which are essential if one seeks a deeper understanding of the problem.
These steps are: 1) Map the ﬁnite dimensionality input data from the input space X (usually X ⊂Rl) into
a higher dimensionality (possibly inﬁnite) RKHS H (feature space) and 2) Perform a linear processing
(e.g.,adaptiveﬁltering)onthemappeddatainH.Theprocedureisequivalentwithanonlinearprocessing

896
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
FIGURE 17.5
Mapping from input space X to feature space H.
(nonlinear ﬁltering) in X (see Figure 17.5). The speciﬁc choice of the kernel κ deﬁnes, implicitly, an
RKHS with an appropriate inner product. Moreover, the speciﬁc choice of the kernel deﬁnes the type
of nonlinearity that underlies the model to be used.
1.17.5.1 A historical overview
In the past, there have been two trends in the study of these spaces by the mathematicians. The ﬁrst one
originated in the theory of integral equations by Mercer [14,15]. He used the term “positive deﬁnite
kernel” to characterize a function of two points κ(x, y) deﬁned on X2, which satisﬁes Mercer’s theorem:
N

n,m=1
anamκ(xn, xm) ≥0,
(17.19)
for any numbers an, am, any points xn, xm, n = 1, . . . N, and for all N ∈N∗. Later on, Moore [16–18]
found that to such a kernel there corresponds a well determined class of functions, H, equipped with
a speciﬁc inner product ⟨·, ·⟩H, in respect to which the kernel κ possesses the so called reproducing
property:
f (x) = ⟨f , κ(·, x)⟩H,
(17.20)
for all functions f ∈H and x ∈X. Those that followed this trend used to consider a speciﬁc given pos-
itive deﬁnite kernel κ and studied it in itself, or eventually applied it in various domains (such as integral
equations, theory of groups, general metric theory, interpolation, etc.). The class H corresponding to κ
was mainly used as a tool of research and it was usually introduced a posteriori. The work of Bochner

1.17.5 Reproducing Kernel Hilbert Spaces
897
[19,20], which introduced the notion of the “positive deﬁnite function” in order to apply it in the theory
of Fourier transforms, also belongs to the same path as the one followed by Mercer and Moore.
On the other hand, those who followed the second trend were primarily interested in the class of
functions H, while the associated kernel was employed essentially as a tool in the study of the functions
of this class. This trend is traced back to the works of Zaremba [21,22] during the ﬁrst decade of the
20th century. He was the ﬁrst to introduce the notion of a kernel, which corresponds to a speciﬁc class
of functions and to state its reproducing property. However, he did not develop any general theory, nor
did he gave any particular name to the kernels he introduced. In this, second trend, the mathematicians
were primarily interested in the study of the class of functions H and the corresponding kernel κ, which
satisﬁes the reproducing property, was used as a tool in this study. To the same trend belong also the
works of Bergman [23] and Aronszajn [3]. Those two trends evolved separately during the ﬁrst decades
of the 20th century, but soon the links between them were noticed. After the second world war, it was
known that the two concepts of deﬁning a kernel, either as a positive deﬁnite kernel, or as a reproducing
kernel, are equivalent. Furthermore, it was proved that there is a one to one correspondence between
the space of positive deﬁnite kernels and the space of reproducing kernel Hilbert spaces.
It has to be emphasized that examples of such kernels have been known for a long time prior to
the works of Mercer and Zaremba; for example, all the Green’s functions of self-adjoint ordinary
differential equations belong to this type of kernels. However, some of the important properties that
these kernels possess have only been realized and used in the beginning of the 20th century and since
then have been the focus of research. In the following, we will give a more detailed description of
these spaces and establish their main properties, focusing on the essentials that elevate them to such
a powerful tool in the context of machine learning. Most of the material presented here can also be
found in more detail in several other papers and textbooks, such as the celebrated paper of Aronszajn
[3], the excellent introductory text of Paulsen [24] and the popular books of Schölkoph and Smola
[13] and Shawe-Taylor and Cristianini [25]. Here, we attempt to portray both trends and to highlight
the important links between them. Although the general theory applies to complex spaces, to keep the
presentation as simple as possible, we will mainly focus on real spaces.
1.17.5.2 Deﬁnition
We begin our study with the classic deﬁnitions on positive deﬁnite matrices and kernels as they were
introduced by Mercer. Given a function κ : X × X →R and x1, . . . , xN ∈X (typically X is a
compact subset of Rl,l > 0), the square matrix K = (Kn,m) with elements Kn,m = κ(xn, xm), for
n, m = 1, . . . , N, is called the Gram matrix (or kernel matrix) of κ with respect to x1, . . . , xN. A
symmetric matrix K = (Kn,m) satisfying
cT · K · c =
N

n=1,m=1
cncm Kn,m ≥0,
for all c ∈RN, n = 1, . . . , N, where the notation (·)T denotes transposition, is called positive deﬁnite.
In matrix analysis literature, this is the deﬁnition of a positive semideﬁnite matrix. However, as positive
deﬁnite matrices were originally introduced by Mercer and others in this context, we employ the term
positive deﬁnite, as it was already deﬁned. If the inequality is strict, for all non-zero vectors c ∈RN, the

898
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
matrix will be called strictly positive deﬁnite. A function κ : X × X →R, which for all N ∈N∗and
all x1, . . . , xN ∈X gives rise to a positive deﬁnite Gram matrix K, is called a positive deﬁnite kernel.
In the following, we will frequently refer to a positive deﬁnite kernel simply as kernel. We conclude
that a positive deﬁnite kernel is symmetric and satisﬁes
N

n=1,m=1
cncmκ(xn, xm) ≥0,
for all c ∈RN, n = 1, . . . , N, x1, . . . , xN ∈X, and for all N ∈N∗. Formally, a Reproducing Kernel
Hilbert space is deﬁned as follows:
Deﬁnition 1 (Reproducing Kernel Hilbert Space (RKHS)).
Consider a linear space H of real valued
functions, f, deﬁned on a set X. Suppose, further, that in H we can deﬁne an inner product ⟨·, ·⟩H with
corresponding norm ∥· ∥H and that H is complete with respect to that norm, i.e., H is a Hilbert space.
We call H a Reproducing Kernel Hilbert Space (RKHS), if there exists a function κ : X × X →R with
the following two important properties:
1. For every x ∈X, κ(·, x) belongs to H (or equivalently κ spans H, i.e., H = span{κ(·, x), x ∈X},
where the overline stands for the closure of a set).
2. κ has the so called reproducing property, i.e.,
f (x) = ⟨f , κ(·, x)⟩H, for all f ∈H, x ∈X,
(17.21)
in particular κ(x, y) = ⟨κ(·, y), κ(·, x)⟩H. Furthermore, κ is a positive deﬁnite kernel and the
mapping  : X →H, with (x) = κ(·, x), for all x ∈X is called the feature map of H.
♦
To denote the RKHS associated with a speciﬁc kernel κ, we will also use the notation H(κ). Fur-
thermore, under the aforementioned notations κ(x, y) = ⟨(y), (x)⟩H, i.e., κ(x, y) is the inner
product of (y) and (x) in the feature space. This is the essence of the kernel trick mentioned in
Section 1.17.4. The feature map  transforms the data from the low dimensionality space X to the
higher dimensionality feature space H. Linear processing in H involves inner products in H, which can
be calculated via the kernel κ disregarding the actual structure of H.
1.17.5.3 Some basic theorems
In the following, we consider the deﬁnition of a RKHS as a class of functions with speciﬁc properties
(following the second trend) and show the key ideas that underlie Deﬁnition 1. To that end, consider a
linear class H of real valued functions, f, deﬁned on a set X. Suppose, further, that in H we can deﬁne an
inner product ⟨·, ·⟩H with corresponding norm ∥· ∥H and that H is complete with respect to that norm,
i.e., H is a Hilbert space. Consider, also, a linear functional T, from H into the ﬁeld R. An important
theorem of functional analysis states that such a functional is continuous, if and only if it is bounded.
The space consisting of all continuous linear functionals from H into the ﬁeld R is called the dual space
of H. In the following, we will frequently refer to the so called linear evaluation functional Ty. This is
a special case of a linear functional that satisﬁes Ty( f ) = f (y), for all f ∈H.

1.17.5 Reproducing Kernel Hilbert Spaces
899
We call H a Reproducing Kernel Hilbert Space (RKHS) on X over R, if for every y ∈X, the
linear evaluation functional, Ty, is continuous. We will prove that such a space is related to a positive
deﬁnite kernel, thus providing the ﬁrst link between the two trends. Subsequently, we will prove that
any positive deﬁnite kernel deﬁnes implicitly a RKHS, providing the second link and concluding the
equivalent deﬁnition of RKHS (Deﬁnition 1), which is usually used in the machine learning literature.
The following theorem establishes an important connection between a Hilbert space H and its dual
space.
Theorem 2 (Riesz Representation).
Let H be a general Hilbert space and let H∗denote its dual
space. Every element  of H∗can be uniquely expressed in the form:
( f ) = ⟨f , φ⟩H,
for some φ ∈H. Moreover, ∥∥H∗= ∥φ∥H.
♦
Following the Riesz representation theorem, we have that for every y ∈X, there exists a unique
element κy ∈H, such that for every f ∈H, f (y) = Ty( f ) = ⟨f , κy⟩H. The function κy is called the
reproducing kernel for the point y and the function κ(x, y) = κy(x) is called the reproducing kernel of
H. In addition, note that ⟨κy, κx⟩H = κy(x) = κ(x, y) and ∥Ty∥2
H∗= ∥κy∥2
H = ⟨κy, κy⟩H = κ(y, y).
Proposition 3.
The reproducing kernel of H is symmetric, i.e., κ(x, y) = κ(y, x).
♦
Proof.
Observe that ⟨κy, κx⟩H = κy(x) = κ(x, y) and ⟨κx, κy⟩H = κx(y) = κ(y, x). As the inner
product of H is symmetric (i.e., ⟨κy, κx⟩H = ⟨κx, κy⟩H) the result follows.
□
In the following, we will frequently identify the function κy with the notation κ(·, y). Thus, we write
the reproducing property of H as:
f (y) = ⟨f , κ(·, y)⟩H,
(17.22)
for any f ∈H, y ∈X. Note that due to the uniqueness provided by the Riesz representation theorem,
κ is the unique function that satisﬁes the reproducing property. The following proposition establishes
the ﬁrst link between the positive deﬁnite kernels and the reproducing kernels.
Proposition 4.
The reproducing kernel of H is a positive deﬁnite kernel.
♦
Proof.
Consider N > 0, the real numbers a1, a2, . . . , aN and the elements, x1, x2, . . . , xN ∈X.
Then
N

n=1
N

m=1
anamκ(xn, xm) =
N

n=1
N

m=1
anam⟨κ(·, xm), κ(·, xn)⟩H =
N

n=1
an
 N

m=1
amκ(·, xm), κ(·, xn)

H
=
 N

m=1
amκ(·, xm),
N

n=1
anκ(·, xn)

H
=
					
N

n=1
anκ(·, xn)
					
2
H
≥0.
Combining Proposition 3 and the previous result, we complete the proof.
□
Remark 5.
In general, the respective Gram matrix associated with a reproducing kernel is
strictly positive deﬁnite. For if not, then there must exist at least one non zero vector a such that

900
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
			

N
n=1 anκ(·, xn)
			
2
H = 0, for some ﬁnite set of points x1, x2, . . . , xN. Hence, for every f ∈H, we
have that 
n an f (xn) =

f , 
n anκ(·, xn)

H = 0. Thus, in this case there exists a linear dependence
between the values of every function in H at some ﬁnite set of points. Such examples do exist (e.g.,
Sobolev spaces), but in most cases the reproducing kernels deﬁne Gram matrices that are always strictly
positive and invertible.
The following proposition establishes a very important fact; any RKHS, H, can be generated by the
respective reproducing kernel κ. Note that the overline denotes the closure of a set (i.e., if A is a subset
of H, A is the closure of A).
Proposition 6.
Let H be a RKHS on the set X with reproducing kernel κ. Then the linear span of the
functions κ(·, x), x ∈X, is dense in H, i.e., H = span{κ(·, x), x ∈X}.
♦
Proof.
We will prove that the only function of H orthogonal to A = span{κ(·, x), x ∈X} is the
zero function. Let f be such a function. Then, as f is orthogonal to A, we have that f (x) = ⟨f , κ(·,
x)⟩H = 0, for every x ∈X. This holds true if and only if f = 0. Thus A⊥= A⊥= {0}. Suppose
that there is f ∈H such that f /∈A. As A is a closed (convex) subspace of H, there is a g ∈A which
minimizes the distance between f and points in A (theorem of best approximation). For the same g we
have that f −g⊥A. Thus, the non-zero function h = f −g is orthogonal to A. However, we proved
that there isn’t any non-zero vector orthogonal to A. This leads us to conclude that A = H.
□
In the following we give some important properties of the speciﬁc spaces.
Proposition 7 (Norm convergence implies point-wise convergence).
Let H be a RKHS on X and
let { fn}n∈N∗⊆H. If limn ∥fn −f ∥H = 0, then f (x) = limn fn(x), for every x ∈X. Conversely, if
for any sequence { fn}n∈N∗of a Hilbert space H, such that limn ∥fn −f ∥H = 0, we have also that
f (x) = limn fn(x), then H is a RKHS.
♦
Proof.
For every x ∈X we have that
| fn(x)−f (x)|H = |⟨fn, κ(·, x)⟩H −⟨f , κ(·, x)⟩H| = |⟨fn −f , κ(·, x)⟩H| ≤∥fn−f ∥H·∥κ(·, x)∥H.
As limn ∥fn −f ∥= 0, we have that limn | fn(x) −f (x)| = 0, for every x ∈X. Hence f (x) =
limn fn(x), for every x ∈X.
For the converse, consider the evaluation functional Ty : H →R, Ty( f ) = f (y) for some y ∈H.
We will prove that Ty is continuous for all y ∈H. To this end, consider a sequence { fn}n∈N∗of H,
with the property limn ∥fn −f ∥H = 0, i.e., fn converges to f in the norm. Then |Ty( fn) −Ty( f )| =
| fn(y) −f (y)| →0, as f (x) = limn f (x) = limn fn(x). Thus Ty( f ) = limn Ty( fn) for all y ∈X
and all converging sequences { fn}n∈N∗of H.
□
Proposition 8 (Different RKHS’s cannot have the same reproducing kernel).
Let H1, H2 be
RKHS’s on X with reproducing kernels κ1, κ2. If κ1(x, y) = κ2(x, y), for all x, y ∈X, then H1 = H2
and ∥f ∥H1 = ∥f ∥H2 for every f.
♦
Proof.
Let κ(x, y) = κ1(x, y) = κ2(x, y) and Ai = span{κi(·, x), x ∈X}, i = 1, 2. As shown
in Proposition 6, Hi = Ai, i = 1, 2. Note that for any f ∈Ai, i = 1, 2, we have that f (x) =

n anκi(·, xn), for some real numbers an and thus the values of the function are independent of whether
we regard it as in A1 or A2. Furthermore, for any f ∈Ai, i = 1, 2, as the two kernels are identical, we
have that ∥f ∥2
H1 = 
n,m anamκ(xm, xn) = ∥f ∥2
H2. Thus, ∥f ∥H1 = ∥f ∥H2, for all f ∈A1 = A2.

1.17.5 Reproducing Kernel Hilbert Spaces
901
Finally, we turn our attention to the limit points of A1 and A2. If f ∈H1, then there exists a
sequence of functions, { fn}n∈N ⊆A1 such that limn ∥f −fn∥H1 = 0. Since { fn}n∈N is a converging
sequence, it is Cauchy in A1 and thus it is also Cauchy in A2. Therefore, there exists g ∈H2 such that
limn ∥g −fn∥H2 = 0. Employing proposition 7, we take that f (x) = limn fn(x) = g(x). Thus, every
f in H1 is also in H2 and by analogous argument we can prove that every g ∈H2 is also in H1. Hence,
H1 = H2 and as ∥f ∥H1 = ∥f ∥H2 for all f in a dense subset (i.e., A1), we have that the norms are equal
for every f. To prove the latter, we use the relation limn ∥fn∥Hi = ∥f ∥Hi , i = 1, 2.
□
The following theorem is the converse of Proposition 4. It was proved by Moore and it gives us
a characterization of reproducing kernel functions. Also, it provides the second link between the two
trends that have been mentioned in Section 1.17.5.1. Moore’s theorem, together with Proposition 4,
Proposition 8 and the uniqueness property of the reproducing kernel of a RKHS, establishes a one-to-
one correspondence between RKHS’s on a set and positive deﬁnite functions on the set.
Theorem 9 (Moore [16]).
Let X be a set and let κ : X × X →R be a positive deﬁnite kernel. Then
there exists a RKHS of functions on X, such that κ is the reproducing kernel of H.
♦
Proof.
We will give only a sketch of the proof. The interested reader is referred to [24]. The ﬁrst step
is to deﬁne A = span{κ(·, x), x ∈X} and the linear map P : A × A →R such that
P

m
amκ(·, ym),

n
bnκ(·, yn)

=

n,m
ambnκ(yn, ym).
We prove that P is well deﬁned and that it satisﬁes the properties of the inner product. Then, given the
vector space A and the inner product P, one may complete the space by taking equivalence classes of
Cauchy sequences from A to obtain the Hilbert space A. Finally, the reproducing property of the kernel
κ with respect to the inner product P is proved.
□
In view of the aforementioned theorems, the Deﬁnition 1 of the RKHS given in 1.17.5.2, which is
usually used in the machine learning literature, follows naturally.
We conclude this section with a short description of the most important points of the theory developed
by Mercer in the context of integral operators. Mercer considered integral operators Tκ generated by
a kernel κ, i.e., Tκ : L2(X) →L2(X), such that (Tκ f )(x) :=

X κ(x, y) f (y)d y. He concluded the
following theorem [14]:
Theorem 10 (Mercer kernels are positive deﬁnite [14]).
Let X ⊆Rl be a nonempty set and let
κ : X × X →R be continuous. Then κ is a positive deﬁnite kernel, if and only if,

X2 f (x)κ(x, y) f (y)dxd y ≥0,
for all functions f ∈L2(X). Moreover, if κ is positive deﬁnite, the integral operator Tκ : L2(X) →
L2(X) : (Tκ f )(x) :=

X κ(x, y) f (y)d y is positive deﬁnite, that is,
⟨Tκ f , f ⟩=

X
f (y)(Tκ f )(x)d y =

X2 κ(x, y) f (x) f (y)dx d y ≥0,
for all f ∈L2(X), and if ψi ∈L2(X) are the normalized orthogonal eigenfunctions of Tk associated
with the eigenvalues σi > 0 then:

902
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
κ(x, y) =

i
σiψi(x)ψi(y).
♦
Note that the original form of above theorem is more general, involving σ-algebras and probability
measures. However, as in the applications concerning this manuscript such general terms are of no
importance, we decided to include this simpler form. The previous theorems established that Mercer’s
kernels, as they are positive deﬁnite kernels, are also reproducing kernels. Furthermore, the ﬁrst part of
Theorem 10 provides a useful tool of determining whether a speciﬁc function is actually a reproducing
kernel.
1.17.5.4 Examples of kernels
Before proceeding to some more advanced topics in the theory of RKHS, it is important to give some
examples of kernels that appear more often in the literature and are used in various applications. Perhaps
the most widely used reproducing kernel is the Gaussian radial basis function deﬁned on X × X, where
X ⊆Rl, as:
κσ(x, y) = exp

−∥x −y∥2
2σ 2

,
(17.23)
where σ > 0. Equivalently the Gaussian RBF function can be deﬁned as:
κt(x, y) = exp

−t∥x −y∥2
,
(17.24)
for t > 0.
Other well-known kernels deﬁned in X × X, X ⊆Rl are:
•
The homogeneous polynomial kernel: κd(x, y) = ⟨x, y⟩d.
•
The inhomogeneous polynomial kernel: κd(x, y) =

⟨x, y⟩+ c
d, where c ≥0 a constant.
2
0
2
2
0
2
0.0
0.5
1.0
(a)
(b)
FIGURE 17.6
(a) The Gaussian kernel for the case X = R, σ = 0.5. (b) The element (0) = κ(·, 0) of the feature space
induced by the Gaussian kernel for various values of the parameter σ.

1.17.5 Reproducing Kernel Hilbert Spaces
903
2
0
2
2
0
2
5
0
5
(a)
(b)
FIGURE 17.7
(a) The homogeneous polynomial kernel for the case X = R, d = 1. (b) The element (x0) = κ(·, x0) of the
feature space induced by the homogeneous polynomial kernel (d = 1) for various values of x0.
2
0
2
2
0
2
0
20
40
(a)
(b)
FIGURE 17.8
(a) The homogeneous polynomial kernel for the case X = R, d = 2. (b) The element (x0) = κ(·, x0) of the
feature space induced by the homogeneous polynomial kernel (d = 2) for various values of x0.
•
The spline kernel: κp(x, y) = B2p+1(∥x −y∥2), where Bn = n
i=1 I
−i
2 , i
2
.
•
The cosine kernel: κ(x, y) = cos (∠(x, y)).
•
The Laplacian kernel: κt(x, y) = exp

−t∥x −y∥

.
Note that the RKHS associated to the Gaussian RBF kernel and the Laplacian kernel are inﬁnite
dimensional, while the RKHS associated to the polynomial kernels have ﬁnite dimension [13].
Figures 17.6–17.10, show some of the aforementioned kernels together with a sample of the ele-
ments κ(·, x) that span the respective RKHS’s for the case X = R. Figures 17.11–17.13, show some
of the elements κ(·, x) that span the respective RKHS’s for the case X = R2. Interactive ﬁgures
regarding the aforementioned examples can be found in http://bouboulis.mysch.gr/kernels.html. More-
over, Appendix A highlights several properties of the popular Gaussian kernel.

904
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
2
0
2
2
0
2
0
10
20
30
40
(a)
(b)
FIGURE 17.9
(a) The inhomogeneous polynomial kernel for the case X = R, d = 2. (b) The element (x0) = κ(·, x0) of
the feature space induced by the inhomogeneous polynomial kernel (d = 2) for various values of x0.
2
0
2
2
0
2
0.0
0.5
1.0
(a)
(b)
FIGURE 17.10
(a) The Laplacian kernel for the case X = R, t = 1. (b) The element (0) = κ(·, 0) of the feature space
induced by the Laplacian kernel for various values of the parameter t.
1.17.5.5 Some properties of the RKHS
In this section, we will refer to some more advanced topics on the theory of RKHS, which are useful
for a deeper understanding of the underlying theory and show why RKHS’s constitute such a powerful
tool. We begin our study with some properties of RKHS’s and conclude with the basic theorems that
enable us to generate new kernels. As we work in Hilbert spaces, the two Parseval’s identities are an
extremely helpful tool. When {es : s ∈S} (where S is an arbitrary set) is an orthonormal basis for a

1.17.5 Reproducing Kernel Hilbert Spaces
905
2
0
2
2
0
2
0.0
0.5
1.0
(a) σ = 0.5.
2
0
2
2
0
2
0.0
0.5
1.0
(b) σ = 0.8.
2
0
2
2
0
2
0.0
0.5
1.0
(c)
σ =1.
2
0
2
2
0
2
0.0
0.5
1.0
(d) σ = 1.5.
FIGURE 17.11
The element (0) = κ(·, 0) of the feature space induced by the Gaussian kernel (X = R2) for various values
of the parameter σ.
Hilbert space H, then for any h ∈H we have that:
h =

s∈S
⟨h, es⟩es,
(17.25)
∥h∥2 =

s∈S
|⟨h, es⟩|2.
(17.26)
Note that these two identities hold for a general arbitrary set S (not necessarily ordered). The convergence
in this case is deﬁned somewhat differently. We say that h = 
s∈S hs, if for any ϵ > 0, there exists a
ﬁnite subset F0 ⊆S, such that for any ﬁnite set F : F0 ⊆F ⊆S, we have that ∥h −
s∈F hs∥< ϵ.
Proposition 11 (Cauchy-Schwarz Inequality).
If κ is a reproducing kernel on X then
∥κ(x, y)∥2 ≤κ(x, x) · κ(y, y).
♦

906
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
2
0
2
2
0
2
0.0
0.5
1.0
(a) x0 = (0, 0)T .
2
0
2
2
0
2
0.0
0.5
1.0
(b) x0 = (0, 1)T .
2
0
2
2
0
2
0.0
0.5
1.0
(c) x0 = (1, 0)T .
2
0
2
2
0
2
0.0
0.5
1.0
(d) x0 = (1, 1)T .
FIGURE 17.12
The element (x0) = κ(·,x0) of the feature space induced by the Gaussian kernel (X = R2) with σ = 0.5.
Proof.
The proof is straightforward, as κ(x, y) is the inner product ⟨(y), (x)⟩H of the
space H(κ).
□
Theorem 12.
Every ﬁnite dimensional class of functions deﬁned on X, equipped with an inner product,
is an RKHS. Let h1, . . . , hN constitute a basis of the space and the inner product is deﬁned as follows
⟨f , g⟩=
N

n,m=1
αn,mγnζm,
for f = 
N
n=1 γnhn and g = 
N
n=1 ζnhn, where the N × N real matrix A = (αn,m)N is strictly positive
deﬁnite. Let B = (βn,m)N be the inverse of A, then the kernel of the RKHS is given by

1.17.5 Reproducing Kernel Hilbert Spaces
907
2
0
2
2
0
2
0.0
0.5
1.0
(a)
t = 0.5.
2
0
2
2
0
2
0.0
0.5
1.0
(b)
t = 1.
2
0
2
2
0
2
0.0
0.5
1.0
(c)
t = 2.
2
0
2
2
0
2
0.0
0.5
1.0
(d)
t = 4.
FIGURE 17.13
The element (0) = κ(·, 0) of the feature space induced by the Laplacian kernel (X = R2) for various values
of the parameter t.
κ(x, y) =
N

n,m=1
βn,mhn(x)hm(y).
(17.27)
♦
Proof.
The reproducing property is immediately veriﬁed by Eq. (17.27):
⟨f , κ(·, x)⟩H =
 N

n=1
γnhn,
N

n,m=1
βn,mhm(x) · hm

H
=
N

n,m=1
αn,mγn
N

k=1
βm,khk(x)

908
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
=
N

n,k=1
 N

m=1
αn,mβm,k

γnhk(x) =
N

n=1
γnhn(x)
= f (x).
□
The following theorem gives the kernel of a RKHS (of ﬁnite or inﬁnite dimension) in terms of the
elements of an orthonormal basis.
Theorem 13.
Let H be a RKHS on X with reproducing kernel κ. If {es : s ∈S ⊂N∗} is an orthonormal
basis for H, then κ(x, y) = 
s∈S es(y)es(x), where this series converges pointwise.
♦
Proof.
For any y ∈X we have that ⟨κ(·, y), es⟩H = ⟨es, κ(·, y)⟩H = es(y). Hence, employing
Parseval’s identity (17.25), we have that κ(·, y) = 
s∈S es(y)es( · ), where these sums converge in
the norm on H. Since the sums converge in the norm, they converge at every point. Hence, κ(x, y) =

s∈S es(y)es(x).
□
Proposition 14.
If H is a RKHS on X with respective kernel κ, then every closed subspace F ⊆H is
also a RKHS. In addition, if F1(κ1) and F2(κ2) are complementary subspaces in H then
κ = κ1 + κ2.
♦
Proposition 15.
Let H be a RKHS on X with kernel κ and {gn} is an orthonormal system in H. Then
for any sequence of numbers {an} such that 
n a2
n < ∞(i.e., {an} ∈ℓ2) we have

n
|an||gn(x)| ≤κ(x, x)
1
2

n
|an|2
 1
2
.
♦
Proof.
We have seen that gn(y) = ⟨gn, κ(·, y)⟩and that ∥κ(·, y)∥2
H = κ(y, y). Thus, considering that
gn’s are orthonormal and taking the Parseval’s identity (17.26) for κ(·, y) with respect to the orthonormal
basis, we have:

n
|gn(y)|2 =

n
|⟨gn, κ(·, y)⟩H|2 ≤∥κ(·, y)∥2
H = κ(y, y).
Therefore, applying the Cauchy-Schwartz inequality we take

n
|an||gn(x)| ≤

n
|an|2
 1
2 
n
|gn(x)|2
 1
2
≤κ(x, x)
1
2

n
|an|2
 1
2
.
□
Theorem 16 (Representer Theorem [26]).
Denote by  : [0, +∞) →R a strictly monotonic
increasing function, by X a nonempty set and by L : X × R2 →R ∪{∞} an arbitrary loss function.
Then each minimizer f ∈H of the regularized minimization problem:
min
f
L((x1, y1, f (x1)), . . . , (xN, yN, f (xN)) + (∥f ∥2
H),
admits a representation of the form
f =
N

n=1
θnκ(·, xn),
(17.28)
where θn ∈R, for n = 1, 2, . . . , N.
♦

1.17.5 Reproducing Kernel Hilbert Spaces
909
Proof.
We may decompose each f ∈H into a part contained in the span of the kernels centered at
the training points, i.e., κ(·, x1), . . . , κ(·, xN), (which is a closed linear subspace) and a part in the
orthogonal complement of the previous span. Thus each f can be written as:
f =
N

n=1
θnκ(·, xn) + f⊥.
Applying the reproducing property and considering that ⟨f⊥, κ(·, xn)⟩H = 0, for n = 1, . . . , N, we
take:
f (xn) = ⟨f , κ(·, xn)⟩H =
N

i=1
θiκ(xn, xi) + ⟨f⊥, κ(·, xn)⟩H =
N

i=1
θiκ(xn, xi).
Thus, the value of the loss function L depends only on the part contained in the span of the kernels
centered at the training points, i.e., on a1, . . . , aN. Furthermore, for all f⊥we have:
(∥f ∥2) = 
⎛
⎝
					
N

n=1
θnκ(·, xn)
					
2
+ ∥f⊥∥2
H
⎞
⎠≥
⎛
⎝
					
N

n=1
θnκ(·, xn)
					
2⎞
⎠.
Thus, for any ﬁxed θ1, . . . , θn the value of the cost function is minimized for f⊥= 0. Hence, the
solution of the minimization task will have to obey this property too.
□
Examples of loss functions L as the ones mentioned in Theorem 16 are for example the total squared
error:
L((x1, y1, f (x1)), . . . , (xN, yN, f (xN)) =
N

n=1
( f (xn) −yn)2,
and the l1 norm measured error
L((x1, y1, f (x1)), . . . , (xN, yN, f (xN)) =
N

n=1
| f (xn) −yn|.
The aforementioned theorem is of great importance to practical applications. Although one might be
trying to solve an optimization task in an inﬁnite dimensional RKHS H (such as the one that generated
by the Gaussian kernel), the Representer Theorem states that the solution of the problem lies in the span
of N particular kernels, those centered on the training points.

910
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
0.2
0.4
0.6
0.8
1.0
x
45
50
55
f x
(a) Solution with bias λ = 0.007.
0.2
0.4
0.6
0.8
1.0
x
35
40
45
50
55
f x
(b) Solution without bias λ = 0.007.
0.2
0.4
0.6
0.8
1.0
x
45
50
55
f x
(c) Solution with bias λ = 0.001.
0.2
0.4
0.6
0.8
1.0
x
45
50
55
f x
(d) Solution without bias λ = 0.001.
FIGURE 17.14
Solving the regression problem minf 1
N

N
n=1 (yn −f (xn))2 +λ∥f ∥2
H, on a set of 11 points (a), (c) with a bias,
i.e., f admits the form of (17.29) and (b), (d) without a bias, i.e., f admits the form of (17.28). In all the
examples the Gaussian kernel function was employed. In (a) and (b) we set σ = 0.15, λ = 0.007. In (c) and
(d) we set σ = 0.15, λ = 0.001. Observe that for λ = 0.007, the unbiased solution takes values signiﬁcantly
lower compared to the values of the training points. For the smaller λ = 0.001, the difference between (c)
and (d) is reduced (compared to the case λ = 0.007). Moreover, one may observe that for the larger value,
λ = 0.007, the resulting curves are smoother compared to the curves that correspond to λ = 0.001.
In practice, we often include a bias factor to the solution of kernel-based regularized minimization
tasks, that is, we assume that f admits a representation of the form
f =
N

n=1
θnκ(·, xn) + b,
(17.29)
where b ∈R. This has been shown to improve the performance of the respective algorithms [2,13,27],
for two main reasons. Firstly, the introduction of the bias, b, enlarges the family of functions in which
we search for a solution, thus leading to potentially better estimations. Moreover, as the regularization
factor (∥f ∥2
H) penalizes the values of f at the training points, the resulting solution tends to take
values as close to zero as possible, for large values of λ (compare Figures 17.14b and 17.14d). At this
point, another comment is of interest also. Note that it turns out that the regularization factor ∥f ∥2
H for
certain types of kernel functions (e.g., Gaussian) involves the derivatives of f of all orders [13,28]. As
a consequence the larger the λ is, the smoother the resulting curve becomes (compare Figures 17.14a
with 17.14c and Figures 17.14b with 17.14d). The use of the bias factor is theoretically justiﬁed by the
semi-parametric Representer theorem.
Theorem 17 (Semi-parametric Representer Theorem [13]).
Suppose that in addition to the assump-
tions of theorem 16, we are given a set of M real valued functions {ψm}M
m=1 : X →R, with the
property that the N × M matrix (ψm(xn))n,m has rank M. Then any ˜f := f + h, with f ∈H and

1.17.5 Reproducing Kernel Hilbert Spaces
911
h ∈span{ψm; m = 1, . . . , M}, solving
min
˜f
L((x1, y1, ˜f (x1)), . . . , (xN, yN, ˜f (xN)) + (∥f ∥2
H),
admits a representation of the form
˜f =
N

n=1
θnκ(·, xn) +
M

m=1
bmψm( · ),
(17.30)
with θn ∈R, bm ∈R, for all n = 1, . . . , N, m = 1, . . . , M.
♦
The following results can be used for the construction of new kernels.
Proposition 18 (Conformal Transformations).
If f : X →R is any function, then κ1(x, y) =
f (x) f (y) is a reproducing kernel. Moreover, if κ is any other reproducing kernel then κ2(x, y) =
f (x)κ(x, y) f (y) is also a reproducing kernel.
♦
Proof.
The ﬁrst part is a direct consequence of theorem 12. For the second part, consider x1, . . . , xN ∈
X and a1, . . . , aN ∈R. Then
N

n,m=1
anam f (xn)κ(xn, xm) f (xm) =
N

n,m=1
anam f (xn) f (xm)⟨(xm), (xn)⟩H
=
 N

m
am f (xm)(xm),
N

n
an f (xn)(xn)

H
=
					
N

n
an f (xn)(xn)
					
2
≥0.
Moreover, as
cos

∠(2(x), 2(y))

=
f (x)κ(x, y) f (y)
√f (x)κ(x, x) f (x)√f (y)κ(y, y) f (y)
=
κ(x, y)
√κ(x, x)√(κ(y, y)) = cos

∠((x), (y))

,
this transformation of the original kernel, preserves angles in the feature space.
□
Theorem 19 (Restriction of a kernel).
Let H be a RKHS on X with respective kernel κ. Then κ
restricted to the set X1 ⊂X is the reproducing kernel of the class H1 of all restrictions of functions
of H to the subset X1. The respective norm of any such restricted function f1 ∈H1 (originating from
f ∈H) has norm ∥f1∥H1 = min{∥f ∥H : f ∈H, f |X1 = f1}.
♦
Proposition 20 (Normalization of a kernel).
Let H be a RKHS on X with respective kernel κ. Then
ˆκ(x, y) =
κ(x, y)
√κ(x, x)κ(y, y),
(17.31)
is also a positive deﬁnite kernel on X. Note that |ˆκ(x, y)| ≤1, for all x, y ∈X.
♦

912
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
Proof.
Let x1, x2, . . . , xN ∈X and c1, . . . , cN be real numbers. Then
N

n,m=1
cncm ˆκ(xn, xm) =
N

n,m=1
cncm
κ(xn, xm)
√κ(xn, xn)κ(xm, xm)
=
N

n,m=1
cn
√κ(xn, xn)
cm
√κ(xm, xm)κ(xn, xm) ≥0,
as κ is a positive deﬁnite kernel.
□
Theorem 21 (Sum of kernels [3]).
Let H1, H2 be two RKHS’s on X with respective kernels κ1, κ2.
Then κ = κ1 + κ2 is also a reproducing kernel. The corresponding RKHS, H, contains the functions
f = f1 + f2, where fi ∈Hi, i = 1, 2. The respective norm is deﬁned by
∥f ∥H = min{∥f1∥+ ∥f2∥: f = f1 + f2, fi ∈Hi, i = 1, 2}.
♦
Proof.
It is trivial to show that κ1 + κ2 is a positive deﬁnite kernel. The difﬁcult part is to associate
this kernel with the speciﬁc RKHS H. Consider the Hilbert space F = H1 × H2. The respective inner
product and the corresponding norm are deﬁned as
⟨( f1, f2), (g1, g2)⟩F = ⟨f1, g1⟩H1 + ⟨f2, g2⟩H2,
∥( f1, f2)∥2
F = ∥f1∥2
H1 + ∥f2∥2
H2,
for f1, g1, ∈H1 and f2, g2 ∈H2. If H1 and H2 have only 0 in common, it easy to show that there
is a one-to-one correspondence between F and H = H1 + H2, as each f ∈H can be decomposed
into two parts (one belonging to H1 and the other in H2) uniquely. The difﬁcult part is to discover
such a relation, if H0 = H1 ∩H2 is larger than {0}. To make this fact clear, consider this simple
example: Let H1 and H2 be the linear classes of polynomials of orders up to 1 and up to 2 respectively.
Obviously, H = H1 + H2 = H2, as H1 ⊂H2. Let f (x) = x2 + 5x, f ∈H. Then f can be
decomposed into two parts (one belonging to H1 and the other in H2) in more than one ways. For
example f (x) = (x2) + (5x), or f (x) = (x2 + 4x) + (x), or f (x) = (x2 + 2x) + (3x), e.t.c. Thus,
the mapping between f = f1 + f2 ∈H and ( f1, f2) ∈F is not one-to-one. However, in such cases,
we can still ﬁnd a smaller subspace of F, which can be identiﬁed to H.
To this end, deﬁne F0 = {( f , −f ) : f ∈H0}. It is clear that F0 is a linear subspace of F. We will
show that it is a closed one. Consider the converging sequence in F0 : ( fn, −fn) →( ˜f1, ˜f2). Then
fn →˜f1 and −fn →˜f2. Thus ˜f1 = −˜f2 and ( ˜f1, ˜f2) is in F0. As F0 is a closed linear subspace of F,
we may consider its complementary subspace F⊥
0 : F = F0 ⊕F⊥
0 .
As a next step, consider the linear transformation T : F →H : T ( f1, f2) = f1 + f2. The kernel
of this transformation is the subspace F0. Hence, there is a one-to-one correspondence between F⊥
0
and H. Consider the inverse transformation

T |F⊥
0
−1
and let

T |F⊥
0
−1
( f ) = ( f ′, f ′′), for f ∈H,
where f ′ ∈H1 and f ′′ ∈H2, i.e., through

T |F⊥
0
−1
we decompose f uniquely into two components,
one in H1 and the other in H2. This decomposition enables us to deﬁne an inner product in H, i.e.,
⟨f , g⟩H = ⟨f ′ + f ′′, g′ + g′′⟩H = ⟨f ′, g′⟩H1 + ⟨f ′′, g′′⟩H2 = ⟨( f ′, f ′′), (g′, g′′)⟩F,

1.17.5 Reproducing Kernel Hilbert Spaces
913
for f , g ∈H. To prove that to this H there corresponds the kernel κ = κ1 + κ2, we make the following
remarks:
1. For every y ∈X, κ(·, y) = κ1(·, y) + κ2(·, y) ∈H.
2. For every y ∈X, let T −1(κ(·, y)) = (κ′(·, y), κ′′(·, y)). Thus
κ(x, y) = κ′(x, y) + κ′′(x, y) = κ1(x, y) + κ2(x, y),
and consequently κ1(x, y) −k′(x, y) = −(κ2(x, y) −κ′′(x, y)). This means that (κ1(x, y) −
k′(x, y), κ2(x, y) −κ′′(x, y)) ∈F0. Hence, for every y ∈X we have
f (y) = f ′(y) + f ′′(y) = ⟨f ′, κ1(·, y)⟩H1 + ⟨f ′′, κ2(·, y)⟩H2
= ⟨( f ′, f ′′), (κ1(·, y), κ2(·, y))⟩F
= ⟨( f ′, f ′′), (κ′(·, y), κ′′(·, y))⟩F + ⟨( f ′, f ′′), (κ1(·, y) −κ′(·, y), κ2(·, y) −κ′′(·, y)⟩F.
As (κ1(x, y) −k′(x, y), κ2(x, y) −κ′′(x, y)) ∈F0 and ( f ′, f ′′) ∈F⊥
0 , we conclude that
f (y) = ⟨( f ′, f ′′), (κ′(·, y), κ′′(·, y)⟩F = ⟨f ′ + f ′′, κ′(·, y) + κ′′(·, y)⟩H = ⟨f , κ(·, y)⟩H.
This is the reproducing property.
Finally, to prove the last part of the theorem, consider again f ∈H and let fi ∈Hi, i = 1, 2, such
that f = f1 + f2 and let f ′ ∈H1 and f ′′ ∈H2 be the unique decomposition of f through T −1. As
f1 + f2 = f ′ + f ′′ we obtain that f ′ −f1 = −( f ′′ −f2), which implies that ( f ′ −f1, f ′′ −f2) ∈F0.
Thus, we take:
∥f1∥2
H1 + ∥f2∥2
H2 = ∥( f1, f2)∥2
F = ∥( f ′, f ′′)∥2
F + ∥( f1 −f ′, f2 −f ′′)∥2
F
= ∥f ′∥2
H1 + ∥f ′′∥2
H2 + ∥( f1 −f ′, f2 −f ′′)∥2
F
= ∥f ∥2
H + ∥( f1 −f ′, f2 −f ′′)∥2
F.
From the last relation we conclude that ∥f ∥2
H = ∥f1∥2
H1 + ∥f2∥2
H2, if and only if f1 = f ′ and
f2 = f ′′. In this case we take the minimum value of ∥f1∥2
H1 +∥f2∥2
H2, for all possible decompositions
f = f1 + f2. This completes the proof.
□
Despite the sum of kernels, other operations preserve reproducing kernels as well. Below, we give
an extensive list of such operations. For a description of the induced RKHS and a formal proof (in the
cases that are not considered here) the interested reader may refer to [3,13].
1. If κ(x, y) is a positive deﬁnite kernel on X, then λκ(x, y) is also a positive deﬁnite kernel for any
λ ≥0. It is obvious that in this case H(λκ) = H(κ), if λ > 0. If λ = 0, then H(0) = {0}.
2. If κ1(x, y) and κ2(x, y) are positive deﬁnite kernels on X, then κ1(x, y)+κ2(x, y) is also a positive
deﬁnite kernel, as Theorem 21 established.
3. If κ1(x, y) and κ2(x, y) are positive deﬁnite kernels on X, then κ1(x, y) · κ2(x, y) is also a positive
deﬁnite kernel.

914
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
4. If κn(x, y) are positive deﬁnite kernels on X, such that limn κn(x, y) = κ(x, y), for all x, y ∈X,
then κ(x, y) is also a positive deﬁnite kernel.
5. If κ(x, y) is a positive deﬁnite kernel on X and p(z) is a polynomial with non-negative coefﬁcients,
then p(κ(x, y)) is also a positive deﬁnite kernel.
6. If κ(x, y) is a positive deﬁnite kernel on X, then eκ(x,y) is also a positive deﬁnite kernel. To prove
this, consider the Taylor expansion formula of ez, which may be considered as a limit of polynomials
with non-negative coefﬁcients.
7. If κ(x, y) is a positive deﬁnite kernel on X and  : X′ →X is a function, then κ((x), (y)) is a
positive deﬁnite kernel on X′.
8. If κ1(x, y) and κ2(x′, y′) are positive deﬁnite kernels on X and X′ respectively, then their tensor
product (κ1 ⊗κ2)(x, y, x′, y′) = κ1(x, y)κ2(x′, y′), is a kernel on X × X′.
9. If κ1(x, y) and κ2(x′, y′) are positive deﬁnite kernels on X and X′ respectively, then their direct sum
(κ1 ⊕κ2)(x, y, x′, y′) = κ1(x, y) + κ2(x′, y′), is a kernel on X × X′.
1.17.5.6 Dot product and translation invariant kernels
There are two important classes of kernels that follow certain rules and are widely used in practice.
The ﬁrst one includes the dot product kernels, which are functions deﬁned as κ(x, y) = f (⟨x, y⟩),
for some real function f. The second class are the translation invariant kernels, which are deﬁned as
κ(x, y) = f (x −y), for some real function f deﬁned on X. The following theorems establish necessary
and sufﬁcient conditions for such functions to be reproducing kernels.
Theorem 22 (Power Series of dot product kernels [29]).
Let f : R →R. A function κ(x, y) =
f (⟨x, y⟩) deﬁned on X, such that f has the power series expansion f (t) = 
n antn, is a positive deﬁnite
kernel, if and only if we have an ≥0 for all n.
♦
Theorem 23 (Bochner’s-Fourier Criterion for translation invariant kernels [20]).
Let f : X →R.
A function κ(x, y) = f (x −y) deﬁned on X ⊆Rl, is a positive deﬁnite kernel, if the Fourier transform
F[κ](ω) = (2π)−N
2

X
e−i⟨ω,x⟩f (x)dx,
is non-negative.
♦
Remark 24.
Bochner’s theorem is more general, involving Borel measures and topological spaces.
For the sake of simplicity we give only this simple form.
Employing the tools provided in this section, one can readily prove the positivity of some of the
kernels given in Section 1.17.5.4. For example:
•
Homogeneous polynomial kernel: As ⟨x, y⟩is a positive deﬁnite kernel and p(z) = zd is a polyno-
mial with non-negative coefﬁcients, p(⟨x, y⟩) = (⟨x, y⟩)d is a positive deﬁnite kernel.
•
Inhomogeneous polynomial kernel: As ⟨x, y⟩is a positive deﬁnite kernel, and p(z) = (z + c)d is a
polynomial with non-negative coefﬁcients (for positive c), p(⟨x, y⟩) = (c + ⟨x, y⟩)d is a positive
deﬁnite kernel.
•
The cosine kernel: Note that cos (∠(x, y)) =
⟨x,y⟩
∥x∥∥y∥. Thus the cosine kernel is the normalization of
the simple kernel ⟨x, y⟩.

1.17.6 Least Squares Learning Algorithms
915
To prove that the Gaussian and the Laplacian are positive kernels we need another set of tools. This
is the topic of Appendix A.
1.17.5.7 Differentiation in Hilbert spaces
1.17.5.7.1
Fréchet’s differentiation
In the following sections, we will develop cost functions deﬁned on RKHS, that are suitable for min-
imization tasks related with adaptive ﬁltering problems. As most minimization procedures involve
computation of gradients or subgradients, we devote this section to study differentiation on Hilbert
spaces. The notion of Fréchet’s Differentiability, which generalizes differentiability to general Hilbert
spaces, lies at the core of this analysis.
Deﬁnition 25 (Fréchet’s Differential).
Let H be a real Hilbert space, L : H →R a function, and
f ∈H. The function L is said to be Fréchet differentiable at f, if there exists a g ∈H such that
lim
∥h∥H→0
L( f + h) −L( f ) −⟨h, g⟩H
∥h∥H
= 0,
(17.32)
where ⟨·, ·⟩H is the inner product of the Hilbert space H and ∥· ∥H = √⟨·, ·⟩H is the induced norm. ♦
The element g ∈H is called the gradient of the operator at f, and is usually denoted as ∇L( f ). This
relates to the standard gradient operator known by Calculus in Euclidean spaces. The Fréchet’s Differ-
ential is also known as Strong Differential. There is also a weaker deﬁnition of Differentiability, named
Gâteaux’s Differential (or Weak Differential), which is a generalization of the directional derivative.
The Gâteaux differential dL( f , h) of L at f ∈H in the direction h ∈H is deﬁned as
dL( f , h) = lim
ϵ→0
L( f + ϵh) −L( f )
ϵ
.
(17.33)
In the following, whenever we are referring to a derivative or a gradient we will mean the one produced
by Fréchet’s notion of differentiability. The interested reader is addressed to [30–35], (amongst others)
for a more detailed discussion on the subject. The well known properties of the derivative of a real
valued function of one variable, which are known from elementary Calculus, like the product and chain
rules, apply to the Fréchet’s derivatives as well.
The following simple examples demonstrate the differentiation procedure in arbitrary spaces.
Example 26.
Consider the real Hilbert space H, with inner product ⟨·, ·⟩H, and L : H →R : L( f ) =
⟨f , g⟩H, where g ∈H ﬁxed. We can easily show (using Fréchet’s deﬁnition) that L is differentiable at
any f ∈H and that ∇L( f ) = g. In the case where L : H →R : L( f ) = ∥f ∥2
H, we can easily show
(using Fréchet’s deﬁnition) that L is differentiable at any f ∈H and that ∇L( f ) = 2 f .
1.17.6 Least squares learning algorithms
Having discussed the basic deﬁnitions and properties of RKHS we now turn to our main focus of the
paper to discuss online learning schemes, which can operate in such spaces. We will start with two
celebrated schemes, which are built around the Least Squares criterion, namely the LMS and the RLS.

916
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
Although we can look at these algorithms as special cases of the schemes to be considered later on,
in the more general setting of convex analytic tools, we chose to treat them separately. The reason is
that the LMS and RLS are widely known, and the less familiar reader could follow their extension to
RKHSs easier.
1.17.6.1 Least mean square (LMS)
Consider the sequence of examples {(xn, yn)}n=1,2,..., where xn ∈X ⊂Rl, and yn ∈R for n =
1, . . . , N, and a parametric class of functions F =

fθ( · ) : θ ∈A ⊆RK 
. The goal of a typical
online/adaptive learning task is to infer an input-output relationship fθ from the parametric class of
functions F, based on the given data, so that to minimize a certain loss function, L(θ), that measures
the error between the actual output, yn, and the estimated output, ˆyn = fθ(xn), at iteration n.
Least mean squares (LMS) algorithms are a popular class of adaptive learning systems that adopt the
least mean squares error (i.e., the mean squared difference between the desired and the actual signal)
to estimate the unknown coefﬁcients θ. Its most typical form was invented in the 1960s by Bernard
Widrow and his Ph.D. student Ted Hoff [36]. In the standard least mean square (LMS) setting, one
adopts the class of linear functions, i.e., F = { fθ(x) = θ T x : θ ∈Rl} and employs the mean square
error, L(θ) = E
 
|yn −θ T xn|2!
, as the loss function. To this end, the gradient descent rationale, e.g.,
[4–6], is employed and at each time instant, n = 1, 2, . . . , N, the gradient of the mean square error,
i.e., ∇L(θ) = −2E[(yn −θ T xn)xn], is approximated via its current measurement, i.e., ∇L(θ) =
−2E[(yn −θ T xn)xn] ≈−2(yn −θ T xn)xn = ∇Ln(θ), where
Ln(θ) := (yn −θ T xn)2,
n = 1, 2, . . .
If we deﬁne en : = yn −θ T
n−1xn as the a-priori error, where θn−1 is the current estimate, then the
previous discussion leads naturally to the step update equation
θn = θn−1 −μ∇Ln(θn−1)
= θn−1 + μenxn,
n = 1, 2, . . .
(17.34)
Another derivation of this classical recursion, through a convex analytic path, will be given in Section
1.17.7. Such a viewpoint will set the LMS as an instant of a larger family of algorithms.
Assuming that the initial θ0 = 0, a repeated application of (17.34) gives
θn = μ
n

i=1
ei xi,
n = 1, 2, . . . ,
(17.35)
while the predicted output of the learning system at iteration n becomes
ˆyn = fθn−1(xn) = μ
n−1

i=1
ei xT
i xn,
n = 1, 2, . . .
(17.36)
There are a number of convergence criteria for the LMS algorithm [5]. Among the most popular
ones are:

1.17.6 Least Squares Learning Algorithms
917
•
The convergence of the mean property, i.e., E[ϵn] →0, as n →∞, where ϵn = θn −θ∗and θ∗
is the Wiener solution. Unfortunately, this is of little practical use, as any sequence of zero mean
arbitrary random numbers converges in this sense.
•
The convergence in the mean square property: If the true input-output relation is given by yn =
θ T
∗xn +ηn, and xn is a weakly stationary process, then L(θn−1) = E[|en|2] →constant, as n →∞,
provided that μ satisﬁes the condition 0 < μ <
2
σmax , where σmax is the largest eigenvalue of the
correlation matrix R = E[xnxT
n ]. We will see in Example 56 that the quantity
2
σmax stems also from
a convex analytic point of view of the LMS-related estimation task. In practice, one usually uses
0 < μ <
2
tr(R), where tr( · ) denotes the trace of a matrix.
•
The Misadjustment property: The Misadjustment ratio, i.e., ρ := L(∞)−Lmin
Lmin
, where
L(∞) := lim
n→∞L(θn−1) = lim
n→∞E[|en|2],
and Lmin is the minimum mean squared error associated with the optimum Wiener ﬁlter, is equal to
ρ =
l
i=1
μσi
2 −μσi
,
provided that the step update parameter μ satisﬁes 
l
i=1
2σi
2−μσi < 1, where σ1, . . . , σl are the
eigenvalues of the correlation matrix R. This is a dimensionless quantity, providing a measure
of closeness of the LMS algorithm from optimality in the mean square sense. The smallest ρ is
(compared to 1), the more accurate the learning system is. Usually ρ is expressed as a percentage.
A misadjustment of 10% is, ordinarily, considered to be satisfactory in practice [5].
LMS is sensitive to the scaling of the inputs xn. This makes it hard to choose a learning rate μ that
guarantees the stability of the algorithm for various inputs. The Normalized LMS (NLMS) is a variant
of the LMS designed to cope with this problem. It solves the problem by normalizing with the power
of the input. Hence, the step update equation becomes
θn = θn−1 + μen
∥xn∥2 xn,
n = 1, 2, . . . ,
(17.37)
where μ ∈(0, 2). It has been proved that the optimal learning rate of the NLMS is μ = 1 [5,6]. We
will also see in Section 1.17.7 that the NLMS has a clear geometrical interpretation; it is the repetition
of a relaxed (metric) projection mapping onto a series of hyperplanes in the Euclidean space Rl. More
on the LMS family of algorithms, and more speciﬁcally, on its impact in modern signal processing, can
be found in [4–6].
1.17.6.2 The Kernel LMS
In kernel-based algorithms, we estimate the output of the learning system by a nonlinear function f in
a speciﬁc RKHS, H, which is associated to a positive deﬁnite kernel κ. The sequence of examples are
transformed via the feature map  : X →H, (x) = κ(·, x) and the LMS rationale is employed on

918
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
the transforssmed data
{((x1), y1), ((x2), y2), . . . , ((xn), yn), . . .},
while the estimated output at iteration n takes the form ˆyn = ⟨(xn), f ⟩H, for some f ∈H. Although
this is a linear algorithm in the RKHS H, it corresponds to a nonlinear processing in X. To calculate
the gradient of the respective loss function, L( f ) = E
 
|yn −⟨(xn), f ⟩H|2!
, the Fréchet’s notion of
differentiabilityisadopted,leadingto:∇L( f ) = −2E[en(xn)];this,accordingtotheLMSrationale,is
approximated at each time instant by its current measurement, i.e., −2en(xn). Consequently, following
a gradient descent rationale, the step update equation of the KLMS is given by
fn = fn−1 + μen(xn),
= fn−1 + μenκ(·, xn),
n = 1, 2, . . .
(17.38)
As it was stated for (17.34), the recursion (17.38) will be re-derived through convex analytic arguments
in Section 1.17.7.
Assuming that f0 = 0, i.e., the zero function, a repeated application of (17.38) gives
fn = μ
n

i=1
ei(xi),
n = 1, 2, . . . ,
(17.39)
and the output of the ﬁlter at iteration n becomes
ˆyn = ⟨(xn), fn−1⟩H = μ
n−1

i=1
ei⟨(xn), (xi)⟩H = μ
n−1

i=1
eiκ(xi, xn).
(17.40)
Equation (17.39) provides the KLMS estimate at time n. This is an expansion (in the RKHS H) in
termsofthefeaturemapfunctioncomputedateachtrainingpointuptotimen.Observethat,alternatively,
one could derive Eq. (17.40), simply by applying the kernel trick to (17.36). It is important to emphasize
that, in contrast to the standard LMS, where the solution is a vector of ﬁxed dimension, which is updated
at each time instant, in KLMS the solution is a function of H and thus it cannot be represented by a
machine. Nevertheless, as it is common in kernel-based methods, one needs not to compute the actual
solution (17.39), but only the estimated output of the learning system, ˆyn, which is given in terms of the
kernel function centered at the points, xi, i = 1, . . . , n −1, of the given sequence (Eq. (17.40)). Under
this setting, the algorithm needs to store into memory two pieces of information: a) the centers (training
input points), xi, of the expansion (17.39), which are stored into a dictionary D and b) the coefﬁcients of
(17.39), i.e., μei, which are represented by a growing vector θ. Observe that (17.39) is inline with what
we know by the Representer theorem given in Section 1.17.5 (Theorem 16). Algorithm 27 summarizes
this procedure.
If in place of (17.38), we use its normalized version, i.e.,
fn = fn−1 +
μ
κ(xn, xn)en(xn)
= fn−1 +
μ
κ(xn, xn)enκ(·, xn),
(17.41)

1.17.6 Least Squares Learning Algorithms
919
Algorithm 27. Kernel LMS (KLMS)
Require: (x1, y1), . . . , (xn, yn), . . .
1: Set θ = 0, D = ∅. Select the step parameter μ and the parameters of the kernel.
2: for n = 1, 2, . . ., do
3:
if n = 1 then
4:
ˆyn = 0.
5: else
6:
Compute the learning system’s output: ˆyn = 
n−1
i=1 θiκ(ui, xn).
7: end if
8: Compute the error: en = yn −ˆyn.
9: θn = μen.
10: Add the new center un = xn to the list of centers, i.e., D = D ∪{xn}, θ = (θT , θn)T .
Output: The n-dimensional vector θ and the dictionary D = {u1, . . . , un}, which produce the solution
fn = 
n
i=1 θiκ(ui, ·).
11: end for
where μ ∈(0, 2), the normalized KLMS counterpart (KNLMS) results. This comprises replacing in
Algorithm 27 the step θn = μen by θn = μen/γ , where γ = κ(xn, xn). The convergence and stability
properties of KLMS is, still, a hot topic for research. One may consider that, as the KLMS is the LMS in
a RKHS H, the properties of the LMS are directly transferred to the KLMS [37]. However, we should
note that the properties of the LMS have been proved for Euclidean spaces, while very often the RKHS
used in practice are of inﬁnite dimension.
1.17.6.2.1
Sparsifying the solution
The main drawback of the KLMS algorithm is that a growing number of training points, xn, is involved
in the estimation of the learning system’s output (17.40). The set of these points can be thought of as a
“dictionary,” D, which is stored into the memory, as in Algorithm 27. This is a typical situation that arises
in any kernel-based online learning scheme. As a consequence, increasing memory and computational
resources are needed, as time evolves (the complexity of KLMS, as presented in Section 1.17.6.2 is
O(n) at each iteration). Furthermore, it is impossible to use the KLMS in real world’s applications, since
the dictionary grows unbounded. In such a case, after a signiﬁcant amount of iterations the expansion
(17.40) will become so large that it will ﬁll up the memory of the machine. In addition, it will require a
huge amount of time to be computed, rendering the application useless. Hence, it is important to ﬁnd a
way to limit the size of this expansion. Several strategies have been proposed to cope with this problem.
According to these strategies, the dictionary of points is created at the beginning of the algorithm and
new points are inserted into it, only if they satisfy a speciﬁc rule, as in Algorithm 28. Adopting such a
strategy, the complexity of KLMS is reduced to O(Mn), where Mn is the size of the dictionary at time
instant n.
For example, in the popular Plat’s novelty criterion sparsiﬁcation scheme [38], whenever a new data
pair (xn, yn) is considered, a decision is immediately made of whether to add the new point, xn, to D.
The decision is reached following two simple rules:

920
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
Algorithm 28. Kernel LMS (KLMS) with sparsiﬁcation
Require: (x1, y1), . . . , (xn, yn), . . ..
1: Set θ = 0, D = ∅, M = 0. Select the step parameter μ and the parameters of the kernel.
2: for n = 1, 2, . . ., do
3:
if n = 1 then
4:
ˆyn = 0.
5: else
6:
Compute the learning system’s output: ˆyn = 
M
i=1 θiκ(ui, xn).
7: end if
8:
Compute the error: en = yn −ˆyn.
9:
θn = μen.
10: Check the Sparsiﬁcation Rules.
11:
if Sparsiﬁcation Rules are not satisﬁed, then
12:
M = M + 1.
13:
Add the new center uM = xn to the list of centers, i.e., D = D ∪{xn}, θ = (θT , θn)T .
14:
end if
Output: The M-dimensional vector θ and the dictionary D = {u1, . . . , uM}.
The solution is then given as fn = 
M
i=1 θiκ(ui, ·).
15: end for
•
First, the distance of the new point, xn, from the current dictionary, Dn−1, is evaluated:
d(xn, Dn−1) =
inf
uk∈Dn−1
{∥xn −uk∥}.
If this distance is smaller than a given threshold δ1 (i.e., the new input vector is close to a point,
which is already in the existing dictionary), then the newly arrived point is not added to Dn−1. Thus
Dn = Dn−1.
•
Otherwise, we compute the prediction error en = yn −ˆdn. If |en| is smaller than a predeﬁned
threshold, δ2, then the new point is discarded and we set Dn = Dn−1. Only if |en| ≥δ2, then xn is
inserted into Dn−1, forming the new dictionary Dn = Dn−1
"{xn}.
Note that whenever we insert a new point into D, we must also insert the respective coefﬁcient,
θn = μen, to the growing vector θ.
Another popular scheme is the so called coherence-based sparsiﬁcation strategy [39], where the
point xn is inserted into the dictionary, if its coherence is above a given threshold ϵ0, i.e.
max
ui∈Dn
{|κ(xn, ui)|} ≤ϵ0.
(17.42)
It has been proved that the dimension of the dictionary determined under rule (17.42) remains ﬁnite, as
n goes to inﬁnity [39].
A more sophisticated strategy is the so called surprise criterion [37], which employs ideas from
information theoretic learning. The surprise of a new data pair (xn, yn) with respect to a learning

1.17.6 Least Squares Learning Algorithms
921
system T is deﬁned as the negative log likelihood of (xn, yn), given the learning system’s hypothesis
on the data distribution, i.e.,
ST (xn, yn) = −ln p((xn, yn)|T ).
According to this measure, one may classify the new data pair into the following three categories:
•
Abnormal: ST (xn, yn) > δ1,
•
Learnable: δ1 ≥ST (xn, yn) ≥δ2,
•
Reduntant: ST (xn, yn) < δ2,
where δ1, δ2 are problem related parameters. In surprise-based sparsiﬁcation, if the new data pair is
classiﬁed as learnable, it is inserted into the dictionary. For the case of the KLMS with Gaussian inputs,
the surprise measure is
Sn = 1
2 ln (rn) + e2
n
2rn
,
(17.43)
where
rn = λ + κ(xn, xn) −max
ui∈Dn
#κ2(xn, ui)
κ(ui, ui)
$
and λ is a user-deﬁned regularization parameter.
A main drawback of the aforementioned sparsiﬁcation techniques is that if one data point is inserted
in the dictionary, it remains in it indeﬁnitely. This may effect the tracking performance of the algorithm
in time varying environments. Another technique, that one can adopt to impose sparsity on the solution
of the KLMS, that, in addition, changes the coefﬁcients of the solution’s expansion, is the quantization
of the training data in the input space, as in the Quantized KLMS [40]. While each data point xn arrives
sequentially, the algorithm checks if it is a new point or a redundant point. If its distance from the
current dictionary Dn is greater than or equal to the quantization size δ (i.e., xn cannot be quantized to
a point already contained in Dn) then xn is classiﬁed as a new point and it is inserted into the dictionary
Dn = Dn−1 ∪{un}. Otherwise, xn is classiﬁed as a “redundant” point and the algorithm uses this
information to update the coefﬁcient of the closest center, say ul0 ∈Dn. Algorithm 29 summarizes this
procedure.
An alternative method to impose sparsity in an adaptive setting has been considered in [41–43].
This is based on regularization and it has the additional beneﬁt of introducing an exponential forgetting
mechanism of past data. This mechanism is very natural to the projection-based philosophy of these
algorithms, and sparsiﬁcation only requires an extra projection onto a convex ball.
1.17.6.2.2
Simulation results
To demonstrate the performance of the KLMS, we consider a typical nonlinear channel equalization
task (see Figure 17.15) similar to the one presented in [44,45]. The nonlinear channel consists of a
linear ﬁlter:
tn = −0.8 · yn + 0.7 · yn−1,
and a memoryless nonlinearity
qn = tn + 0.25 · t2
n + 0.11 · t3
n.
(17.44)

922
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
FIGURE 17.15
The equalization task.
Algorithm 29. Quantized Kernel LMS (QKLMS)
Require: (x1, y1), . . . , (xn, yn), . . ..
1: Set θ =0, D=∅, M =0. Select the step parameter μ, the parameters of the kernel and the quantization size δ.
2: for n = 1, 2, . . ., do
3:
if n = 1 then
4:
ˆyn = 0.
5:
else
6:
Compute the learning system’s output: ˆyn = 
M
i=1 θi · κ(ui, xn).
7:
end if
8:
Compute the error: en = yn −ˆyn.
9:
θn = μen.
10:
Compute the distance of xn from D : d(xn, D)=infui∈D ∥xn−ui∥=∥xn −ul0∥, for an l0 ∈{1, . . . M}.
11:
if d(xn, D) > δ, then
12:
M = M + 1.
13:
Add the new center uM = xn to the list of centers, i.e., D = D ∪{un}, θ = (θT , θn)T .
14:
else
15:
Keep the codebook unchanged and update the coefﬁcient θl0, i.e., θl0 = θl0 + μen.
16:
end if
17:
The M-dimensional vector θ and the dictionary D = {u1, . . . , uM}.
The solution is then given as fn = 
M
i=1 θiκ(·, ui).
18: end for
The signal is then corrupted by additive white Gaussian noise and then it is observed as xn. The level
of the noise was set equal to 15 dB. The aim of a channel equalization task is to design an inverse ﬁlter,
which acts on the output of the channel, xn, and reproduces the original input signal, yn, as close as

1.17.6 Least Squares Learning Algorithms
923
0
1000
2000
3000
4000
5000
−10
−5
0
5
10
15
10*log10(MSE)
n
Non linear channel Equalization
KNLMS
NLMS
FIGURE 17.16
Learning curves of normalized LMS (μ = 1) and the normalized Gaussian Kernel LMS (μ = 1/2, σ = 5) for
the equalization problem (ﬁlter length l = 5, delay D = 2).
possible. To this end, we apply the normalized KLMS algorithm to the set of samples
(xn, yn−D) :=

(xn, xn−1, . . . , xn−l+1), yn−D

,
(17.45)
where l > 0 is the equalizer’s length and D the equalization time delay, which is present to, almost, any
equalization set up. In other words, the output of the equalizer at time n, provides the estimate of yn−D.
This is to account for the non minimum phase property of the linear ﬁlter that was used to model the
channel [5].
Experimentswereconductedon50setsof5000samplesoftheinputsignal(Gaussianrandomvariable
with zero mean and unit variance) comparing the standard LMS and the KLMS. Regarding the KLMS,
we employed the Gaussian kernel (with σ = 5) and we adopted the novelty criterion sparsiﬁcation
strategy (δ1 = 0.15, δ2 = 0.2). The step update parameter, μ, was tuned for the best possible results (in
terms of the steady-state error rate). Time delay D was also set for optimality. Figure 17.16 shows the
learning curves of KLMS versus the standard LMS algorithm. The superiority of the KLMS is evident
and it was expected, since the standard LMS cannot cope with the nonlinearity.
Figures 17.17a and 17.17b compare the performance of the various sparsiﬁcation strategies presented
in Section 1.17.6.2.1, considering the same experimental set up. The user-deﬁned parameters for the
novelty detection approach were set to δ1 = 0.15, δ2 = 0.2, for the coherence-based scenario were
set to ϵ0 = 0.99 and for the surprise-based sparsiﬁcation were set to λ = 0.01, δ1 = 2, δ2 = −2.
In this experiment, one observes that although both the surprise and the coherence-based scenarios
perform almost equally well in terms of sparsity, the latter gives a slightly lower steady state MSE,
whereas the novelty detection approach outperforms the other two both in terms of sparsity and the
steady state MSE. Moreover, we should note that the coherence-based and the surprise strategies are

924
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
0
1000
2000
3000
4000
5000
−10
−8
−6
−4
−2
0
2
10*log10(MSE)
n
Non linear channel Equalization
KNLMS
KNLMS(nov)
KNLMS(coh)
KNLMS(surp)
(a)
0
1000
2000
3000
4000
5000
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
Expansion size (M)
n
Evolution of the Expansion‘s size
KNLMS
KNLMS(nov)
KNLMS(coh)
KNLMS(surp)
(b)
FIGURE 17.17
(a) Learning curves of the normalized Kernel LMS (KNLMS) with μ = 1/2, using the Gaussian kernel (σ = 5),
under various sparsiﬁcation strategies for the equalization problem (ﬁlter length l = 5, time delay D = 2).
(b) Evolution of the expansion’s size at each time instant, for each sparsiﬁcation strategy. The user-deﬁned
parameters for the novelty detection approach were set to δ1 = 0.15, δ2 = 0.2, for the coherence-based
scenario were set to ϵ0 = 0.99 and for the surprise-based sparsiﬁcation were set to λ = 0.01, δ1 = 2,
δ2 = −2.
more computationally demanding compared to the novelty criterion, as they involve several evaluations
of the kernel function. Figure 17.18 compares the learning curves and the expansion’ size evolution of
the QKLMS and the novelty detection KLMS at the same experimental set up. There it is shown that
QKLMS, while offering a signiﬁcant sparser solution, at the same time accomplishes a lower steady
state MSE. In fact, the convergence behavior of QKLMS is almost identical to the KLMS without
sparsiﬁcation! Moreover, in terms of computational resources QKLMS is comparable to the novelty
detection sparsiﬁcation scenario.
Finally, Figures 17.19 and 17.20 compare the performances of the aforementioned sparsiﬁcation
strategies in a set up where the channel undergoes a signiﬁcant change. In this particular experiment,
the input signal was fed to a channel consisting of the linear part tn = −0.8yn + 0.7yn−1 −0.6yn−2
+ 0.1yn−3 and the nonlinear part qn = tn + 0.08t2
n. The signal was then corrupted by additive white
Gaussian noise and then observed as xn. After the 2000th time instant the channel changes so that
tn = 0.6yn −0.9yn−1 + 0.5yn−2 −0.4yn−3 and qn = tn + 0.1t2
n. Experiments were conducted
on 50 sets of 4000 samples of the input signal (Gaussian random variable with zero mean and unit
variance). The QKLMS in this test gives excellent results, as it achieves the same steady state MSE as
the unsparsiﬁed KLMS using only the 1/8th of the training centers. The three sparsiﬁcation strategies
(novelty, coherence, surprise) carry the information learned while the channel was in its original state
throughout the learning phase. However, the two ﬁrst (novelty and coherence) seem to be able to cope
with the change of the channel, while the latter fails. The code for the experiments presented in this
section can be found in http://bouboulis.mysch.gr/kernels.html. More simulation results for the KLMS

1.17.6 Least Squares Learning Algorithms
925
0
1000
2000
3000
4000
5000
−9
−8
−7
−6
−5
−4
−3
−2
−1
0
1
10*log10(MSE)
n
Non linear channel Equalization
KNLMS
KNLMS(nov)
QKLMS
(a)
0
1000
2000
3000
4000
5000
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
Expansion size (M)
n
Evolution of the Expansion‘s size
KNLMS
KNLMS(nov)
QKLMS
(b)
FIGURE 17.18
(a) Learning curves of the normalized Kernel LMS (KNLMS) with μ = 1/2, using the Gaussian kernel (σ = 5),
the novelty-detection normalized KNLMS and the QKLMS for the equalization problem (ﬁlter length l = 5,
time delay D = 2). (b) Evolution of the expansion’s size at each time instant, for the KNLMS, novelty KNLMS,
QKNLMS. The user-deﬁned parameters for the novelty detection approach were set to δ1 = 0.15, δ2 = 0.2.
The quantization size (for the QKNLMS) was set δ = 1.
algorithm are given in Sections 1.17.6.6.1 and 1.17.7.6.2, where it is validated against other kernel-based
online learning methodologies, such as the Kernel RLS, and the projection-based Adaptive Projected
Subgradient Method (APSM).
1.17.6.3 The complex case
Complex-valued signals arise frequently in applications as diverse as communications, bioinformatics,
radar, etc. In this section we present the LMS rationale suitably adjusted to treat data of this nature.
1.17.6.3.1
Complex LMS
Let zn ∈Cν and dn ∈C be the input and the output of the original ﬁlter, respectively. In the
typical Complex LMS (CLMS) rationale, we estimate the output of the ﬁlter using the so called
C-linear function: ˆdn = fθ(zn) = θ H zn. The CLMS aims to compute θ ∈Cν, such that the error
L(θ) = E[|dn−θ H zn|2] is minimized. To compute the respective gradients, the notion of the Wirtinger’s
Calculus [46,47] is employed as it simpliﬁes calculations. In a nutshell, Wirtinger’s Calculus considers
two types of derivatives, with respect to z and z∗respectively (following the standard differentiation
rules). However, only the second one is important for optimization tasks. The alternative to using
Wirtinger’s calculus would be to consider the complex variables as pairs of two real ones and employ
the common real partial derivatives. However, this approach, usually, is more time consuming and leads
to more cumbersome expressions. Hence, applying the rules of Wirtinger’s calculus to this case, we

926
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
0
500
1000
1500
2000
2500
3000
3500
4000
−8
−6
−4
−2
0
2
4
6
10*log10(MSE)
n
Non linear channel Equalization
KNLMS
KNLMS(nov)
KNLMS(coh)
KNLMS(surp)
(a)
0
500
1000
1500
2000
2500
3000
3500
4000
0
500
1000
1500
2000
2500
3000
3500
4000
Expansion size (M)
n
Evolution of the Expansion‘s size
KNLMS
KNLMS(nov)
KNLMS(coh)
KNLMS(surp)
(b)
FIGURE 17.19
(a) Learning curves of the normalized Kernel LMS (KNLMS) with μ = 1/2, using the Gaussian kernel
(σ = 5), under various sparsiﬁcation strategies for the two states equalization problem (ﬁlter length l = 5,
time delay D = 2). (b) Evolution of the expansion’s size at each time instant, for each sparsiﬁcation strategy.
The user-deﬁned parameters for the novelty detection approach were set to δ1 = 0.15, δ2 = 0.2, for
the coherence-based scenario were set to ϵ0 = 0.99 and for the surprise-based sparsiﬁcation were set to
λ = 0.01, δ1 = 2, δ2 = −2.
take that ∂L(θ)
∂θ∗
= −e∗
nzn, where en = dn −ˆdn. Thus, the step update for the CLMS is
θn = θn−1 + μe∗
nzn,
and the estimated output of the ﬁlter at iteration n, assuming that θ0 = 0, is
ˆdn = μ
n−1

i=1
ei zH
i zn.
A different ﬁltering structure has been brought to light more recently, mainly due to the works of
Picinbono [48–50]. It is based on the notion of the widely linear estimation functions, where the output
is estimated so that to be linear in terms of both z and z∗, i.e., ˜dn = fθ, η(zn) = θ H zn + ηH z∗
n. It
turns out that the traditional approach to linearity, as employed by the CLMS, is rather “unorthodox,”
as it excludes a large class of linear functions from being considered in the estimation process. More-
over, it can be shown that the correct complex linear estimation is the widely linear one [51]. In the
context of the widely linear, or Augmented, CLMS (ACLMS), the vectors θ, η ∈Cν are computed,
so that the error L(θ) = E[|dn −fθ,η(zn)|2] is minimized. Applying Wirtinger’s Calculus we com-
pute the respective gradients as ∂L(θ,η)
∂θ∗
= −e∗
nzn and ∂L(θ,η)
∂η∗
= −e∗
nz∗
n. Thus, the step updates of

1.17.6 Least Squares Learning Algorithms
927
0
500
1000
1500
2000
2500
3000
3500
4000
−7
−6
−5
−4
−3
−2
−1
0
1
2
10*log10(MSE)
n
Non linear channel Equalization
KNLMS
KNLMS(nov)
QKNLMS
(a)
0
500
1000
1500
2000
2500
3000
3500
4000
0
500
1000
1500
2000
2500
3000
3500
4000
Expansion size (M)
n
Evolution of the Expansion‘s size
KNLMS
KNLMS(nov)
QKLMS
(b)
FIGURE 17.20
(a) Learning curves of the normalized Kernel LMS (KNLMS) with μ = 1/2, using the Gaussian kernel
(σ = 5), the novelty-detection normalized KNLMS and the QKLMS for the two-states equalization problem
(ﬁlter length l = 5, time delay D = 2). (b) Evolution of the expansion’s size at each time instant, for the
KNLMS, novelty KNLMS, QKNLMS. The user-deﬁned parameters for the novelty detection approach were
set to δ1 = 0.15, δ2 = 0.2. The quantization size (for the QKNLMS) was set δ = 1.
the ACLMS become
θn = θn−1 + μe∗
nzn,
(17.46a)
ηn = ηn−1 + μe∗
nz∗
n,
(17.46b)
and the estimated output at iteration n, assuming that θ0 = η0 = 0, is
˜dn = μ
n−1

i=1
ei(zi + z∗
i )H zn.
(17.47)
An important notion that has been associated with the case of complex data is the so called circularity.
Circularity is intimately related to the rotation in the geometric sense. A complex random variable Z
is called circular, if for any angle φ both Z and Zeiφ (i.e., the rotation of Z by angle φ) follow the
same probability distribution [52,53]. It has been shown that, in the case of circular inputs, ACLMS and
CLMS have identical performances [48,49,52,53]. However, ACLMS may show some performance
gains when non-circular input sources are considered, although its convergence rate is somewhat slower
than the CLMS. This is due to the fact that widely linear estimation processes are able to capture the full
second-order statistics of a given complex signal, especially in the case where the signal is non-circular,
by considering both the covariance and the pseudo-covariance matrices [49,50].
1.17.6.3.2
Complex kernel LMS
Following a similar procedure, as the one adopted in the derivation of the KLMS from the standard
LMS, kernel-based analogues of the CLMS can be produced. In general, to generate kernel adaptive

928
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
ﬁltering algorithms on complex domains, according to [54], one can adopt two methodologies. A ﬁrst
straightforward approach is to use directly a complex RKHS, using one of the complex kernels given
in Section 1.17.5, and map the original data to the complex RKHS through the associated feature map
C(z) = κC(·, z). This is equivalent to applying the kernel trick to the standard complex LMS rationale
in the usual black-box approach. Another alternative technique is to use real kernels through a rationale
that is called complexiﬁcation of real RKHSs. This method has the advantage of allowing modeling
in complex RKHSs, using popular well-established and well understood, from a performance point of
view, real kernels (e.g., Gaussian, polynomial, etc). While in the ﬁrst case, we map the data directly
to the complex RKHS through the feature map, in the complexiﬁcation scenario we employ the map
(z) = R(x, y) + iR(x, y), where z = x + i y, and R is the feature map of the chosen real
kernel κR, i.e., R(x, y) = κR(·, (x, y)). In the following, we will deal with the ﬁrst scenario only.
The complexiﬁcation case is treated in [54].
Consider the complex RKHS H, with the respective complex reproducing kernel κC(·, ·) (meaning
that it takes complex arguments and returns a complex number) deﬁned on X × X, where X ⊆Cν.
We apply the feature map C of the RKHS, H, to the points zn and employ the LMS rationale to
the transformed data {(C(zn), dn) : n = 1, 2, . . .}, modeling the desired output either as ˆdn =
⟨C(zn), f ⟩H, for some f ∈H, or as ˜dn = ⟨C(zn), f ⟩H + ⟨∗
C(zn), g⟩H, for some f , g ∈H, if the
more general widely linear approach is considered. In the ﬁrst case, considering the corresponding loss
as L( f ) = E[|dn −ˆdn|2], the Pure Complex KLMS (PCKLMS) is generated. To this end, the gradient
descent rationale is adopted and the gradient of the loss function is estimated via its current measurement.
To this end, Wirtinger’s calculus had to be generalized in the framework of Freshet derivation, to cope
with inﬁnite dimensional complex Hilbert spaces, [55]. It turns out that, ∇f ∗L( f ) = −E[e∗
nC(zn)] ≈
−e∗
nC(zn). This leads to the following step update equation:
fn = fn−1 + μe∗
nC(zn).
(17.48)
Assuming that f0 = 0, and by applying a repetition of (17.48), we take formulas similar to the KLMS
case. Algorithm 30 summarizes the procedure of PCKLMS.
To sparsify the solution one may employ any of the methods described in Section 1.17.6.2.1. Adopt-
ing the widely linear estimation process, i.e., ˜dn = ⟨C(zn), f ⟩H + ⟨∗
C(zn), g⟩H, and following a
similar procedure, the Augmented Pure Complex KLMS (APCKLMS) is produced. In this case, as the
corresponding loss is L(θ, η) = E[|dn −˜dn|2], the respective estimation of gradients via their current
measurements becomes ∇f ∗L( f , g) ≈−e∗
nC(zn) and ∇g∗L( f , g) ≈−e∗
n∗
C(zn). It turns out that
the step update equations become:
fn = fn−1 + μe∗
nC(zn),
gn = gn−1 + μe∗
n∗
C(zn).
The APCKLMS is treated in details in [51], where it is shown that it performs signiﬁcantly better than
the PCKLMS. The problem of complex estimation using kernels in the special case of ﬁnite dimensional
Euclidean spaces has also been considered in [56].

1.17.6 Least Squares Learning Algorithms
929
Algorithm 30. Pure Complex Kernel LMS (PCKLMS)
Require: (z1, d1), . . . , (zn, dn), . . ..
1: Set θ = 0, D = ∅, M = 0. Select the step parameter μ and the parameters of the kernel.
2: for n = 1, 2, . . ., do
3:
if n = 1 then
4:
ˆdn = 0.
5:
else
6:
Compute the ﬁlter output: ˆdn = 
M
i=1 θiκC(ui, zn).
7:
end if
8:
Compute the error: en = dn −ˆdn.
9:
θn = μen.
10:
Check Sparsiﬁcation Rules.
11:
if Sparsiﬁcation Rules are satisﬁed,
12:
M = M + 1.
13:
Add the new center uM = zn to the list of centers, i.e., D = D ∪{zn}, θ = (θT , θn)T .
14:
end if
Output: The M-dimensional vector θ and the dictionary D = {u1, . . . , uM}.
The solution is then given as fn = 
M
i=1 θiκ(ui, ·).
15: end for
1.17.6.4 Recursive Least Squares (RLS)
Recall that we assumed the sequence of examples {(x1, y1), (x2, y2), . . . , (xn, yn), . . .}, where xn ∈
X ⊂Rl, and yn ∈R. In the Recursive Least Squares (RLS) algorithm, we adopt the parametric class
of linear functions, i.e., F = { fθ(x) = θ T x : θ ∈Rl}, as the ﬁeld in which we search for the optimal
input-output relationship between xn and yn, while the loss function at iteration n takes the form:
Ln(θ) =
n

i=1
|yi −fθ(xi)|2 + λ∥θ∥2,
n = 1, 2, . . . ,
(17.49)
for some chosen λ > 0. Observe that the loss function consists of two parts. The ﬁrst one measures the
error between the estimated output, fθ(xi), and the actual output, yi, at each time instant, i, up to n,
while the second is a regularization term. The existence of the second term is crucial to the algorithm,
as it prevents it from being ill-posed and guards against overﬁtting. At each time instant, n, the RLS
ﬁnds the solution to the minimization problem:
min
θ
Ln(θ).
(17.50)
The RLS algorithm, usually, exhibits an order of magnitude faster convergence compared to the LMS,
at the cost of higher computational complexity.
The rationale of the RLS algorithm can be described in a signiﬁcant more compact form, if one
adopts a matrix representation. To this end, we deﬁne the l × n matrices Xn = (x1, x2, . . . , xn) and

930
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
the vectors yn = (y1, y2, . . . , yn)T ∈Rn, for n = 1, 2, . . .. Then, the loss function (17.49) can be
equivalently deﬁned as:
Ln(θ) =
			yn −XT
n θ
			
2
+ λ ∥θ∥2 = (yn −XT
n θ)T (yn −XT
n θ) + λ∥θ∥2
=
		yn
		2 −2θ T Xn yn + θ T Xn XT
n θ + λθ T θ.
As this is a strictly convex function, it has a unique minimum, which it is obtained at the point, θn :=
argminθ{Ln(θ)}, where its gradient vanishes, i.e., ∇Ln(θn) = 0. Note, that the gradient of the loss
function can be derived using standard differentiation rules:
∇Ln(θ) = −2Xn yn + 2Xn XT
n θ + 2θ.
(17.51)
Equating the gradient to zero, we obtain the solution to the minimization problem (17.50):
θn = (λIl + Xn XT
n )−1Xn yn,
(17.52)
where Il is the l × l identity matrix. Observe, that this solution includes the inversion of a matrix at
each time step, which is usually, a computationally demanding procedure. To overcome this obstacle,
the RLS algorithm computes the solution recursively, exploiting the matrix inversion lemma:
(A + BC D)−1 = A−1 −A−1B(C−1 + DA−1B)−1 DA−1.
(17.53)
Let Pn = λIl + xnxT
n . Then, we can easily deduce that Pn = Pn−1 + xnxT
n . Thus, using the matrix
inversion lemma (17.53) we take:
P−1
n
=

Pn−1 + xnxT
n
−1
= P−1
n−1 −P−1
n−1xnxT
n P−1
n−1
1 + xTn P−1
n xn
.
Also, considering that Xn yn = Xn−1 yn−1 + xnyn, the solution θn becomes
θn = Pn Xn yn =

P−1
n−1 −P−1
n−1xnxT
n P−1
n−1
1 + xTn P−1
n xn

(Xn−1 yn−1 + xndn)
= P−1
n−1Xn−1 yn−1 + Pn−1xnyn −P−1
n−1xnxT
n P−1
n−1 Pn−1Xn−1 yn−1
1 + xTn P−1
n xn
−P−1
n−1xnxT
n P−1
n−1 Pn−1xnyn
1 + xTn P−1
n xn
= θn−1 −P−1
n−1xnxT
n θn−1
1 + xTn P−1
n−1xn
+

P−1
n−1xnyn −P−1
n−1xnxT
n PT
n−1xnyn
1 + xTn P−1
n−1xn

= θn−1 −P−1
n−1xnxT
n θn−1
1 + xTn P−1
n−1xn
+
P−1
n−1xnyn
1 + xTn P−1
n−1xn
= θn−1 +
P−1
n−1xn
1 + xTn P−1
n−1xn

yn −xT
n θn−1

.

1.17.6 Least Squares Learning Algorithms
931
0
500
1000
1500
2000
2500
3000
−30
−20
−10
0
10
20
30
40
50
10*log10(MSE)
n
Linear channel identification
NLMS
RLS
FIGURE 17.21
Learning curves of the normalized LMS (μ = 1) and the RLS (λ = 0.01) for a ﬁlter identiﬁcation problem
(ﬁlter length l = 200).
Thus, the solution θn is computed recursively, as
θn = θn−1 +
P−1
n−1xn
1 + xTn P−1
n−1xn
en,
(17.54)
where en = yn −θ T
n−1xn is the estimation error at iteration n.
As can be seen in Figure 17.21, which considers a 200-taps ﬁlter identiﬁcation problem (where
the input signal is a Gaussian random variable with zero mean and unit variance), the RLS learning
procedure typically exhibits signiﬁcantly faster convergence and achieves a lower steady state error
compared to the NLMS (in this example the steady state MSE achieved by the RLS is 25% lower than
the one achieved by the NLMS). Such a performance is expected for the stationary environment case.
The RLS is a Newton-type algorithm and the LMS a gradient descent type. Moreover, concerning the
excess error for the LMS, this is due to the non-diminishing value of μ, which as we know increases
the steady state error.
1.17.6.5 Exponentially weighted RLS
The RLS learning rationale utilizes information of all past data at each iteration step. Although this tactic
enables RLS to achieve better convergence speed, it may become a burden in time-varying environments.
In such cases, it would be preferable to simply “forget” some of the past data, which correspond to
an earlier state of the learning task. We can incorporate such an approach to the RLS algorithm by

932
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
introducing some weighting factors to the cost function (17.49), i.e.,
Ln(θ) =
n

i=1
w(n, i)|yi −fθ(xi)|2 + λw(n)∥θ∥2,
n = 1, 2, . . . ,
(17.55)
where w is an appropriately chosen weighting function. A typical weighting scenario that is commonly
used is the exponential weighting factor (or the forgetting factor), where we choose w(n, i) = wn−i and
w(n) = wn, for some 0 < w ≤1. Small values of w (close to 0) imply a strong forgetting mechanism,
which makes the algorithm more sensitive to recent samples and might lead to poor performance. Hence,
we usually select w such that it is close to 1. Obviously, for w = 1 we take the ordinary RLS described
in 1.17.6.4. In the method of Exponentially Weighted RLS (EWRLS) we minimize the cost function
Ln(θ) =
n

i=1
wn−i|yi −θ T xi|2 + λwn∥θ∥2,
n = 1, 2, . . .
(17.56)
Using the matrix notation the aforementioned cost function becomes L(θ) =
				W
1
2n (yn −Xnθ)
				
2
+
wnλθ T θ, where Xn and yn are deﬁned as in Section 1.17.6.4 and W is the diagonal matrix with elements
the powers of w up to n −1, i.e.,
Wn =
⎛
⎜⎜⎜⎝
wn−1
0
0 . . . 0
0
wn−2 0 . . . 0
...
...
...
...
0
0
0
1
⎞
⎟⎟⎟⎠.
The solution to the respective minimization problem is
θn = (XnWn XT
n + wnλIl)−1XnWn yn.
(17.57)
Following a similar rationale as in Section 1.17.6.4, it turns out that:
P−1
n
= w−1 P−1
n−1 −w−2 P−1
n−1xnxT
n P−1
n−1
1 + w−1xTn P−1
n−1xn
,
and
θn = θn−1 +
w−1 P−1
n−1xn
1 + w−1xTn P−1
n−1xn
en.
Algorithm 31 describes the EWRLS machinery in details. We test the performance of the EWRLS in
a time-varying channel identiﬁcation problem. The signal (zero-mean unity-variance Gaussian random
variable) is fed to a linear channel (length K = 200) and then corrupted by white Gaussian noise. After
2000 samples of data, the coefﬁcients of the linear channel undergo a signiﬁcant change. Figure 17.22
shows the performance of the EWRLS algorithm compared to the standard RLS and the NLMS for

1.17.6 Least Squares Learning Algorithms
933
Algorithm 31. Exponential Weighted Recursive Least Squares (RLS)
Require: The data

(xn, yn)

n∈N∗, the weighting factor w and the regularization parameter λ.
1: Set P = λIl, θ = 0.
2: for n = 1, 2, . . ., do
3:
Compute the learning system’s output: ˆyn = θT xn.
4:
Compute the error: en = yn −ˆyn.
5:
r = 1 + w−1xTn P−1xn.
6:
k = (rw)−1 P−1xn.
7:
Update solution: θ = θ + enk.
8:
Update the inverse matrix: P−1 = w−1 P−1 −kkT r.
Output: The vector θ ∈Rl.
9: end for
various values of the weighting coefﬁcient w. While the standard RLS fails to adapt to the change of the
environment, the EWRLS learns the new channel effectively, at a cost of a slightly larger steady state
MSE due to the weighting of the error at each time step. Note that the tracking performance of both the
LMS and the RLS algorithms depends on the input signal. For some signals the LMS can track better.
This is in contrast to the initial convergence speed, where the RLS always converges faster. [5,6].
1.17.6.6 The Kernel RLS
Similar to the case of the KLMS, in the Kernel RLS (KRLS) we estimate the learning system’s output by
a function f deﬁned on the RKHS, H, which is associated to a positive deﬁnite kernel κ. The sequence
of examples are transformed via the feature map  : X →H, (x) = κ(·, x) to generate the input
data
{((x1), y1), ((x2), y2), . . . , ((xn), yn), . . .}
and the employed loss function takes the form
Ln( f ) =
n

i=1
|yi −⟨(xi), f ⟩H|2 + λ ∥f ∥2
H ,
n = 1, 2, . . .
(17.58)
TheRepresenterTheorem16ensuresthatthesolutiontotheminimizationtask, fn := argmin f ∈HLn( f ),
lies in the ﬁnite dimensional subspace of the span of the n kernels that are centered on the training points
x1, x2, . . . , xn. In other words:
fn =
n

i=1
θn,iκ(·, xi)
(17.59)
and thus, exploiting the kernel trick, the estimated output at iteration n will be given by
ˆyn = ⟨(xn), fn⟩H =
n

i=1
θn,iκ(xi, xn).
(17.60)

934
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
0
1000
2000
3000
4000
5000
6000
−30
−20
−10
0
10
20
30
40
50
10*log10(MSE)
n
Linear channel identification
NLMS
RLS
EWRLS
(a) ω = 0.996.
0
1000
2000
3000
4000
5000
6000
−30
−20
−10
0
10
20
30
40
50
10*log10(MSE)
n
Linear channel identification
NLMS
RLS
EWRLS
(b) ω = 0.994.
0
1000
2000
3000
4000
5000
6000
−30
−20
−10
0
10
20
30
40
50
10*log10(MSE)
n
Linear channel identification
NLMS
RLS
EWRLS
(c) ω = 0.992.
FIGURE 17.22
Learning curves of the normalized LMS (μ = 1), the RLS and EWRLS (λ = 0.001) for various values of the
weighting factor w, for a time-varying ﬁlter identiﬁcation problem (ﬁlter length l = 200).
As this expansion grows unbounded while time evolves, analogous to the case of KLMS, a sparsiﬁcation
mechanism, which keeps the most informative training points and discards the rest, is also needed.
KRLS’s main task is to estimate the (growing) vector θn = (θn,1, . . . , θn,n)T in a recursive manner.
The ﬁrst approach to the KRLS task was presented in [44] by Engel, Mannor, and Meir as a part of
Engel’sPh.D.thesis.Theirmethodisbuiltaroundaspeciﬁcsparsiﬁcationstrategy,whichiscalledAprox-
imate Linear Dependency (ALD). To this end, consider a dictionary of centers Dn−1 = {u1, . . . , uMn−1},
of size Mn−1, that keeps some of the past training points. In this setting, we test whether the newly arrived
data (xn) is approximately linearly dependent on the dictionary elements (u1), . . . (uMn−1). If not,

1.17.6 Least Squares Learning Algorithms
935
we add it to the dictionary, otherwise (xn) is approximated by a linear combination of the dictionary’s
elements. The speciﬁc condition is whether
δn = min
an
⎧
⎪⎨
⎪⎩
						
Mn−1

m=1
an,m(um) −(xn)
						
2⎫
⎪⎬
⎪⎭
≤ϵ0,
(17.61)
where ϵ0 is a user-deﬁned parameter. Using matrix-notation, this minimization problem can be equiv-
alently recasted as:
δn = min
an

aT
n ˜K n−1an −2aT
n βn + κ(xn, xn)

,
where ( ˜K n−1)i, j = κ(ui, u j), for i, j = 1, 2, . . . , Mn−1, and βn = (κ(u1, xn), κ(u2, xn), . . . ,
κ(uMn−1, xn))T . Assuming that ˜K n−1 is invertible, the solution to this minimization problem is
an = ˜K
−1
n−1βn,
(17.62)
while the attained minimum value is δn = κ(xn, xn) −βT
n an. In the case where δn > ϵ0, xn is added
to the dictionary, which now contains Mn = Mn−1 + 1 centers. If δn ≤ϵ0, then xn is not included to
the dictionary, i.e., Dn = Dn−1, Mn = Mn−1, and we approximate (xn) as
(xn) ≈
Mn

m=1
an,m(um).
(17.63)
Using (17.63), we can approximate the full kernel matrix K n (i.e., (K n)i, j = κ(xi, x j), for i, j =
1, . . . , n) as
K n ≈An · ˜K n · AT
n ,
(17.64)
where An = (a1, a2, . . . , an)T is the Mn × Mn matrix that contains the coefﬁcients of the linear
combinations associated with each um, m = 1, . . . , Mn. This is due to the fact that at time index n each
kernel evaluation is computed as
κ(xi, x j) = ⟨(xi), (x j)⟩H =
 Mn

k=1
ai,k(uk),
Mn

l=1
a j,l(ul)

H
=
Mn

k=1
Mn

l=1
a j,lai,k⟨(uk), (ul)⟩H =
Mn

k=1
Mn

l=1
a j,lai,kκ(uk, ul),
for all k,l = 1, . . . , Mn. Note that ai, j = 0 for all j > i. This is the kick-off point of KRLS rationale.
In matrix notation, the respective loss of the KRLS minimization task at iteration n is given by
Ln(θ) = ∥yn −K nθ∥2.
As K n ≈An · ˜K n · AT
n , L(θ) can be recasted as
Ln(θ) ≈∥yn −An ˜K n AT
n θ∥2 = ∥yn −An ˜K n ˜θ∥2 = Ln(˜θ),

936
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
where ˜θ = AT
n θ is the new parameter to be optimized. Observe that while the original vector θn ∈Rn is a
n-dimensional vector, the new vector, ˜θn ∈RMn, is signiﬁcantly smaller (depending on the sparsiﬁcation
strategy). The solution to the modiﬁed minimization task is
˜θn = ( ˜K
T
n AT
n An ˜K n)−1 ˜K
T
n AT
n yn
= ˜K
−1
n (AT
n An)−1 ˜K
−T
n
˜K
T
n AT
n yn
= K −1
n P−1
n AT
n yn,
(17.65)
where Pn = AT
n An, assuming that the inverse matrices exist. We distinguish the two different cases of
the ALD sparsiﬁcation and derive recursive relations for ˜θn accordingly.
CASE I. If δn < ϵ0, then the data point arrived at iteration n, i.e., (xn, yn), is not added to the
dictionary. Hence Dn = Dn−1, Mn = Mn−1, ˜K n = ˜K n−1 and the expansion of (xn) as a linear
combination of the dictionary’s elements is added to An, i.e.,
An =
 An−1
aT
n

,
(17.66)
where an is given by (17.62). Therefore, Pn = AT
n An can be computed as
Pn = AT
n An = ( AT
n−1 An )
 An−1
aT
n

= AT
n−1 An−1 + anaT
n = Pn−1 + anaT
n .
Applying the matrix inversion lemma, we take the recursive relation
P−1
n
= P−1
n−1 −P−1
n−1anaT
n P−1
n−1
1 + aTn P−1
n−1an
.
(17.67)
Substituting in (17.65) and considering that AT
n yn = AT
n−1 yn−1 + anyn we take
˜θn = ˜K
−1
n P−1
n AT
n yn
= ˜K
−1
n

P−1
n−1 −P−1
n−1anaT
n P−1
n−1
1 + aTn P−1
n−1an

(AT
n−1 yn−1 + anyn)
= ˜θn−1 +
˜K
−1
n P−1
n−1anyn −˜K
−1
n P−1
n−1anaT
n P−1
n−1 AT
n−1 yn−1
1 + aTn P−1
n−1an
= ˜θn−1 +
K −1
n P−1
n−1an

yn −aT
n P−1
n−1 AT
n−1 yn−1

1 + aTn P−1
n−1an
.

1.17.6 Least Squares Learning Algorithms
937
However, relation (17.65) implies that ˜K n−1 ˜θn−1 = P−1
n−1 AT
n−1 yn−1. Hence,
˜θn = ˜θn−1 +
K −1
n P−1
n−1an

yn −aT
n ˜K n−1 ˜θn−1

1 + aTn P−1
n−1an
= ˜θn−1 +
K −1
n P−1
n−1an

yn −βT
n ˜θn−1

1 + aTn P−1
n−1an
= ˜θn−1 + K −1
n P−1
n−1anen
1 + aTn P−1
n−1an
,
where the last relations are generated by substituting βT
n = aT
n ˜K n−1 (see relation (17.62)) and consid-
ering that the estimated output at time index n by the learning system is ˆyn = βT
n ˜θn = aT
n ˜K n ˜θn.
CASE II. If δn > ϵ0, then the data point arrived at iteration n, i.e., (xn, yn), is added to the dictionary,
i.e., Dn = Dn−1
"{xn}, Mn = Mn−1 + 1 and the matrices An and ˜K n are constructed as follows
An =
 An−1
0
0T
1

,
˜K n =
 ˜K n−1
βn
βT
n
κ(xn, xn)

.
Moreover,
Pn = AT
n An =

AT
n−1
0T
0
1
  An−1
0
0T
1

=
 AT
n−1 An−1
0
0T
1

.
Hence, applying the matrix inversion lemma to ˜K n and substituting an = ˜K
−1
n−1βn, we have
˜K
−1
n
=
 ˜K
−1
n−1 + 1
δn ˜K
−1
n−1βnβT
n ˜K
−1
n−1
−1
δn ˜K
−1
n−1βn
−1
δn βT
n ˜K
−1
n
1
δn

= 1
δn

δn ˜K
−1
n−1 + anaT
n
−an
−aT
n
1

.
and
P−1
n
=
(AT
n An)−1
0
0T
1

=

P−1
n−1
0
0T
1

.
Observe that while we replace ˜K
−1
n−1βn by an using relation (17.62), the actual nth row of An is
(0, 0, . . . , 0, 1). Substituting in (17.65) we take:

938
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
˜θn = 1
δn

δn ˜K
−1
n−1 + anaT
n
−an
−aT
n
1
 
P−1
n−1
0
0T
1
 
AT
n−1
0T
0
1
  yn−1
yn

= 1
δn

δn ˜K
−1
n−1 P−1
n−1 AT
n−1 yn−1 + anaT
n P−1
n−1 AT
n−1 yn−1 −anyn
−aT
n P−1
n−1 AT
n−1 yn−1 + yn

= 1
δn
δn ˜θn−1 + anaT
n ˜K n−1 ˜θn−1 −anyn
−aT
n ˜K n−1 ˜θn−1 + yn

= 1
δn
δn ˜θn−1 + anβT
n ˜θn−1 −anyn
−βT
n ˜θn−1 + yn

=

˜θn−1 −an
δn en
−en
δn

.
The procedure is summarized in Algorithm 32. The KRLS algorithm has complexity O(M2
n), at
time n, where Mn is the size of the dictionary. Note, that we developed the speciﬁc recursive strategy
assuming the invertibility of ˜K n and AT
n An for each n. For the case of the RKHS induced by the
Gaussian kernel, which is the most popular kernel for practical applications, we can prove that ˜K nis
indeed invertible. Furthermore, by construction, the matrix An has full-rank. Hence, AT
n An is strictly
positive deﬁnite and thus invertible too.
A methodology of similar spirit to the KRLS, which also takes advantage of the loss function
(17.58) in the Support Vector Machines (SVM) rationale, can be found in [57–60]. An alternative
path to the KRLS was followed in [61], via a formulation based on Gaussian Processes. Such an
approach allows for an explicit forgetting factor mechanism to be incorporated in the adaptation ﬂow.
An alternative philosophy for forgetting past data in kernel-based adaptive schemes, in the frame-
work of regularization, which can efﬁciently be implemented via projections, will be discussed in
Section 1.17.7.
1.17.6.6.1
Simulation results
In order to study the behavior of the KRLS, we consider some nonlinear channel equalization set-ups
similar to the ones used in Section 1.17.6.2.2. In the ﬁrst experiment, the nonlinear channel consists of
a linear ﬁlter:
tn = −0.8 · yn + 0.7 · yn−1 −0.6 · yn−2 + 0.4 · yn−3
and a memoryless nonlinearity
qn = tn + 0.08 · t2
n.
(17.68)
The input signal that was fed to the channel followed the Gaussian distribution with zero mean and
unity variance. At the receiver end of the channels, the signal is corrupted by white Gaussian noise and
then observed as xn. The level of the noise was set to 15dB. To solve the equalization task, we apply the
KRLS-based learning systems to the set of samples (xn, yn) :=

(xn, xn−1, . . . , xn−l+1), yn−D

, where
l > 0 is the equalizer’s length and D the equalization time delay. Figure 17.23 shows the performance
of KRLS compared to KLMS over a set of 30 training sequences consisting of 5000 samples each. The
Quantization sparsiﬁcation strategy was adopted for the KLMS, as it demonstrates superior performance

1.17.6 Least Squares Learning Algorithms
939
Algorithm 32. Kernel Recursive Least Squares (KRLS)
Require: The data

(xn, yn)

n∈N∗, and the ALD parameter ϵ0.
1: Set ˜K−1 = [1/κ(x1, x1)], P−1 = [1], ˜θ = [y1/κ(x1, x1)], M = 1.
2: for n = 2, 3, . . . do
3:
Compute β = (κ(u1, xn), . . . , κ(uM, xn))T .
4:
Compute the learning system’s output: ˆyn = θT β.
5:
Compute the error: en = yn −ˆyn.
6:
FOR ALD TEST:
7:
Compute a = ˜K−1β.
8:
Compute δ = κ(xn, xn) −βT a.
9:
if δ > ϵ0 then
10:
Include the new sample to the dictionary: D = D ∪{xn}, M = M + 1.
11:
Update ˜K−1 = 1
δ

δ ˜K−1 + aaT −a
−a
1

.
12:
Update P−1 =
 P−1 0
0T
1

.
13:
Update solution: ˜θ =
 ˜θ −a
δ en
−en
δ

.
14:
else
15:
q =
P−1a
1+aT P−1a .
16:
Update P−1 = P−1 −qaT P−1.
17:
Update solution: ˜θ = ˜θ + K−1qen.
18:
end if
Output: The vector ˜θ ∈RM and the dictionary D = {u1, . . . uM}.
19: end for
(see Section 1.17.6.2.2). The sparsiﬁcation parameters were set to ϵ0 = 0.1 for the KRLS, and δ = 4
for the quantization of KLMS. It is clear that KRLS signiﬁcantly outperforms KLMS both in terms of
convergence and sparsity.
In the second experiment, the linear part of the channel is reduced, i.e.,
tn = −0.8 · yn + 0.7 · yn−1
(17.69)
and the memoryless nonlinearity remains the same. Figure 17.24 shows the performance of the two
algorithms over a set of 30 training sequences consisting of 5000 samples each. The parameters used
for sparsiﬁcation were set to ϵ0 = 0.1 for the KRLS, and δ = 6 for the quantization of KLMS.
Finally, in the third experiment, the linear part is identical to (17.69), while the non linear part is
qn = tn + 0.25 · t2
n + 0.11 · t3
n
(17.70)
and the memoryless nonlinearity remains the same. Figure 17.25 shows the performance of the two
algorithms over a set of 30 training sequences consisting of 5000 samples each, using the same parame-
ters for the sparsiﬁcation as in the second experiment. In general, KRLS shows the best performance, as

940
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
−6
−5
−4
−3
−2
−1
0
1
10*log10(MSE)
n
Non linear channel Equalization
QKNLMS
KRLS
(a)
0
1000
2000
3000
4000
5000
0
20
40
60
80
100
120
Expansion size (M)
n
Evolution of the Expansion‘s size
QKNLMS
KRLS
(b)
FIGURE 17.23
(a) Learning curves of the quantized KNLMS (μ = 1/2), and KRLS based on the ALD sparsiﬁcation scenario
(ϵ0 = 0.1) using the Gaussian kernel (σ = 5), for the equalization problem (17.68) (ﬁlter length l = 5,
time delay D = 2). The quantization size was set to δ = 4. (b) Evolution of the expansion’s size at each time
instant, for each algorithm.
0
1000
2000
3000
4000
5000
−7
−6
−5
−4
−3
−2
−1
0
1
10*log10(MSE)
n
Non linear channel Equalization
 
 
QKNLMS
KRLS
(a)
0
1000
2000
3000
4000
5000
0
10
20
30
40
50
60
Expansion size (M)
n
Evolution of the Expansion‘s size
 
 
QKNLMS
KRLS
(b)
FIGURE 17.24
(a) Learning curves of the quantized KNLMS (μ = 1/2), and KRLS based on the ALD sparsiﬁcation scenario
(ϵ0 = 0.1) using the Gaussian kernel (σ = 5), for the equalization problem (17.69) (ﬁlter length l = 5,
time delay D = 2). The quantization size was set to δ = 6. (b) Evolution of the expansion’s size at each time
instant, for each algorithm.

1.17.7 A Convex Analytic Toolbox for Online Learning
941
0
1000
2000
3000
4000
5000
−7
−6
−5
−4
−3
−2
−1
0
1
10*log10(MSE)
n
Non linear channel Equalization
 
 
QKNLMS
KRLS
(a)
0
1000
2000
3000
4000
5000
0
50
100
150
200
250
Expansion size (M)
n
Evolution of the Expansion‘s size
 
 
QKNLMS
KRLS
(b)
FIGURE 17.25
(a) Learning curves of the quantized KNLMS (μ = 1/2), and KRLS based on the ALD sparsiﬁcation scenario
(ϵ0 = 0.1) using the Gaussian kernel (σ = 5), for the equalization problem (17.70) (ﬁlter length l = 5,
time delay D = 2). The quantization size was set to δ = 4. (b) Evolution of the expansion’s size at each time
instant, for each algorithm.
it appears to achieve lower or the same steady state MSE as the QKLMS using somewhat sparser repre-
sentations. However, there are cases where its performance is quite similar to the QKNLMS. The code
for the experiments presented in this section can be found in <http://bouboulis.mysch.gr/kernels.html>.
1.17.7 A convex analytic toolbox for online learning
During the last ten years or so, convex analysis [34,62] has emerged as the main stage where mathemat-
ically sound optimization tools for signal processing and machine learning tasks are developed. This
section attempts to give a short introduction on several convex analytic arguments which are central
to modern optimization techniques. We do not follow the mainstream of the literature which develops
around the classical Lagrange multipliers’ methodology [63] or the powerful interior point algorithms
[64], but instead, we abide by the less popular, to the engineering community, operator/mapping and
ﬁxed point theoretic framework [65–67], established in general Hilbert spaces. The reason to adopt such
a framework is its geometrical nature, generality, and ability to engulf numerous modern tools of convex
and variational analysis [62,65]. Our objective here is to highlight a few fragments of this powerful
framework. For more details, and for a deeper view of the subject, the interested reader is referred to
[62,65–67].
In the sequel, the stage of our discussion is a real Hilbert space H, equipped with an inner product
⟨·, ·⟩, and with an induced norm ∥·∥: = √⟨·, ·⟩. A sequence ( fn)n∈N of elements of H is said to
converge strongly to an f∗∈H, and it will be denoted as fn →f∗, if limn→∞∥fn −f∗∥= 0.
The sequence ( fn)n∈N is said to converge weakly to an f∗∈H, and it will be denoted as fn ⇀f∗,

942
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
lev 0( )
0
( f2)
f2
epi( )
f1
λ
( f1) + (1−λ)
( f2)
( f1)
≤
FIGURE 17.26
A convex function L, its epigraph, and the lower level set of L at height 0.
if ∀g ∈H, limn→∞⟨fn −f∗, g⟩= 0. Strong convergence implies the weak one, but, in general, the
converse does not hold true in Hilbert spaces. However, it does hold true in the case of Euclidean spaces.
We start, now, with few notions of fundamental importance to convex analysis.
1.17.7.1 Closed convex sets and metric projection mappings
Deﬁnition 33 (Convex set, convex function).
A non-empty subset C of H is called convex if ∀f1, f2 ∈
H, and ∀λ ∈[0, 1], the following holds true: λ f1 + (1 −λ) f2 ∈C.
Moreover, a function L : H →R is called convex if ∀f1, f2 ∈H, and ∀λ ∈[0, 1], L(λ f1 +
(1 −λ) f2) ≤λL( f1) + (1 −λ)L( f2). The function f is called strictly convex if ∀λ ∈(0, 1) and
∀f1, f2 ∈H, such that f1 ̸= f2, we have L(λ f1 + (1 −λ) f2) < λL( f1) + (1 −λ)L( f2). Finally, if
there exists a β > 0 such that ∀λ ∈(0, 1) and ∀f1, f2 ∈H, the following holds true:
L

λ f1 + (1 −λ) f2

+ λ(1 −λ)β
2 ∥f1 −f2∥2 ≤λL( f1) + (1 −λ)L( f2),
then L is called strongly convex with constant β.
♦
The epigraph of a convex function L is deﬁned as the set
epi(L) := {(h,r) ∈H × R : L(h) ≤r} .
In other words, the epigraph of L is the set of all points of H × R which belong to and lie above the
graph of L. Moreover, given a real number ξ, the lower level set of L at height ξ is deﬁned as the set
lev≤ξ(L) := {h ∈H : L(h) ≤ξ} .
For the geometry behind the previous deﬁnitions, see Figure 17.26. A simple inspection of Figure 17.26
suggests that L is convex if and only if epi(L) is convex.
Deﬁnition 34 (The metric distance function).
Given a closed convex set C ⊂H, the metric distance
function to C is deﬁned as follows:
∀f ∈H,
d( f , C) := inf {∥f −g∥: g ∈C} .

1.17.7 A Convex Analytic Toolbox for Online Learning
943
f
C
d( f, C)
PC ( f )
g = PC (g)
FIGURE 17.27
A closed convex subset of H, and the associated metric projection mapping.
In the case where the set C is a singleton, i.e., it contains a single element C = {g}, then, clearly, the
metric distance d( f , C) becomes our familiar distance of f from g : d( f , g).
It can be easily veriﬁed by the triangle inequality of ∥·∥, that d(·, C) is a convex function. Moreover,
notice that lev≤0

d(·, C)

= C.
♦
Deﬁnition 35 (The metric projection mapping).
Given a non-empty closed convex set C ⊂H,
the metric projection mapping onto C is deﬁned as the operator that maps to each f ∈H the unique
PC( f ) ∈C such that
∥f −PC( f )∥= d( f , C).
In other words, the point PC( f ) is the unique minimizer of the function ∥f −g∥, g ∈C. Obvi-
ously, in the case where g ∈C, then PC(g) = g. For the geometry behind the mapping PC, see
Figure 17.27.
♦
Let us give now a couple of examples of closed convex sets in order to illustrate the previous concepts.
Example 36 (Closed ball).
Given a radius  > 0 and a center g0 ∈H, the closed ball B[g0, ] is
deﬁned as the following closed convex set
B[g0, ] := {g ∈H : ∥g −g0∥≤} .
The associated metric projection mapping PB[g0,] can be easily veriﬁed to be
PB[g0,]( f ) = g0 +

max {, ∥f −g0∥}( f −g0),
∀f ∈H.
Example 37 (Hyperplane).
A hyperplane H is the closed convex subset of H which is deﬁned as
H := {g ∈H : ⟨a, g⟩= y} ,
for some nonzero a ∈H and some y ∈R. The (metric) projection mapping PH onto H is given as
follows:
PH( f ) = f −⟨f , a⟩−y
∥a∥2
a,
∀f ∈H.
(17.71)

944
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
f
0
a
H
−f,a
y
a 2 a
PH ( f )
FIGURE 17.28
A hyperplane and its associated projection mapping.
For the geometry behind the previous concepts, see Figure 17.28. Notice that a stands as the normal
vector of the hyperplane.
In the special case of an RKHS H, which is associated to a kernel function κ, let a := κ(x, ·), for some
x ∈Rl. Then, due to the kernel trick, the hyperplane H obtains a simple form: H = {g ∈H : g(x) = y}.
Moreover, the (metric) projection mapping becomes as follows:
PH( f ) = f −f (x) −y
κ(x, x) κ(x, ·),
∀f ∈H.
(17.72)
The previous example helps us calculate the metric projection mapping onto the following renowned
closed convex set.
Example 38 ((Closed) hyperslab).
A (closed) hyperslab S[ϵ], where ϵ > 0 is a user-deﬁned param-
eter, is deﬁned as the following closed convex set:
S[ϵ] := {g ∈H : | ⟨g, a⟩−y| ≤ϵ} ,
where a is a non-zero element of H, and y ∈R. Geometrically, a hyperslab can be visualized as
the set of all those points which belong onto and between the following two “parallel” hyperplanes;
H+ϵ : = {g ∈H : ⟨g, a⟩= y + ϵ} and H−ϵ : = {g ∈H : ⟨g, a⟩= y −ϵ}. Based on this fundamental
observation, the metric projection mapping PS[ϵ] can be directly calculated by employing the results of
Example 37; ∀f ∈H,
PS[ϵ]( f ) =
⎧
⎨
⎩
PH+ϵ( f ), if ⟨f , a⟩> y + ϵ,
f ,
if | ⟨f , a⟩−y| ≤ϵ,
PH−ϵ( f ), if ⟨f , a⟩< y −ϵ.
=
⎧
⎪⎨
⎪⎩
f −⟨f ,a⟩−y−ϵ
∥a∥2
a, if ⟨f , a⟩> y + ϵ,
f ,
if | ⟨f , a⟩−y| ≤ϵ,
f −⟨f ,a⟩−y+ϵ
∥a∥2
a, if ⟨f , a⟩< y −ϵ.
(17.73)

1.17.7 A Convex Analytic Toolbox for Online Learning
945
As in Example 37, in the special case of an RKHS H, which is associated to a kernel function κ, let
a := κ(x, ·), for some x ∈Rl. Then, under such a scenario, (17.73) takes the following special form:
∀f ∈H,
PS[ϵ]( f ) =
⎧
⎪⎨
⎪⎩
f −f (x)−y−ϵ
κ(x,x)
κ(x, ·), if f (x) > y + ϵ,
f ,
if | f (x) −y| ≤ϵ,
f −f (x)−y+ϵ
κ(x,x)
κ(x, ·), if f (x) < y −ϵ.
(17.74)
Example 39 (Afﬁne set).
Assume a positive integer m, a number of nonzero elements {a1, a2, . . . , am}
of H, and a set of real numbers {y1, y2, . . . , ym}. Upon deﬁning the hyperplanes Hi
: =
{g ∈H : ⟨ai, g⟩= yi} , ∀i = 1, . . . , m, let the set of all common points of the previous hyperplanes:
V :=
m
.
i=1
Hi.
(17.75)
Since the previous V might be empty, let us generalize it as follows in order to cover also such an
unpleasant situation. Deﬁne the linear mapping A : H →Rm as follows:
A( f ) :=
⎡
⎢⎣
⟨a1, f ⟩
...
⟨am, f ⟩
⎤
⎥⎦,
∀f ∈H.
Notice here that in the case where our Hilbert space H becomes a Euclidean one, e.g., Rl, and the
elements {a1, a2, . . . , am} become the l-dimensional vectors {a1, a2, . . . , am}, then the mapping A is
nothing but the following matrix A := [a1, a2, . . . , am]T , where the superscript T stands for transpo-
sition.
If we let y := [y1, y2, . . . , ym]T , then deﬁne
V := argmin
g∈H
∥y −A(g)∥2 .
(17.76)
Notice that if the previous minimum attains the value 0, then the intersection in (17.75) is nonempty.
Hence, the Deﬁnition (17.76) generalizes the special (17.75) one. It can be shown that V is a nonempty
afﬁne set. Notice also here that in the case where our Hilbert space H becomes a Euclidean one Rl,
then the afﬁne set of (17.76) becomes the classical V = argminθ∈Rl ∥y −Aθ∥2, which is often met in
least-squares penalized learning tasks.
Under the previous setting, it can be veriﬁed [68] that the metric projection mapping PV onto the
afﬁne set V of (17.76) is given as:
PV ( f ) = f + A† 
y −A( f )

,
∀f ∈H,
(17.77)
where A† : Rm →H stands for the (Moore-Penrose) pseudoinverse mapping of A.
Upon deﬁning the adjoint mapping A∗of A as the mapping A∗: Rm →H which satisﬁes ξ T A( f ) =
⟨f , A∗(ξ)⟩, ∀f ∈H, ∀ξ ∈Rm, then it can be veriﬁed that
A∗(ξ) =
m

i=1
ξiai,
∀ξ ∈Rm.

946
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
Notice that in the case where our Hilbert space H becomes a Euclidean Rl one, then the adjoint mapping
is nothing but the transpose of A, i.e., A∗= AT . A classical result states that A† = A∗
AA∗† [69].
Notice that the mapping AA∗: Rm →Rm is nothing but the Gram matrix
AA∗=
⎡
⎢⎢⎢⎣
⟨a1, a1⟩⟨a1, a2⟩· · · ⟨a1, am⟩
⟨a2, a1⟩⟨a2, a2⟩· · · ⟨a2, am⟩
...
...
...
...
⟨am, a1⟩⟨am, a2⟩· · · ⟨am, am⟩
⎤
⎥⎥⎥⎦.
In the case where the Hilbert space H becomes an RKHS associated to a kernel function κ, and
ai : = κ(xi, ·), for some xi ∈Rl, ∀i = 1, 2, . . . , m, then the previous Gram matrix AA∗is usually
called the kernel matrix associated to the set {x1, x2, . . . , xm}; the (i, j)th element of AA∗becomes
κ(xi, x j).
1.17.7.2 The subgradient
Deﬁnition 40 (Subdifferential, subgradient).
Given a convex function L, deﬁned on H, and a point
f ∈H, the subdifferential of L at f is deﬁned as the set-valued operator ∂L that maps to f the set of all
the subgradients of L at f, i.e.,
∂L( f ) := {g ∈H : ⟨g, h −f ⟩+ L( f ) ≤L(h), ∀h ∈H} .
♦
Let us give now a geometrical point of view to the notion of the subgradient. Fix an f ∈H, and choose
any subgradient g ∈∂L( f ). This subgradient deﬁnes the following hyperplane in the space H × R:
Xg := {(h,r) ∈H × R : 0 = ⟨g, h −f ⟩+ L( f ) −r} .
(17.78)
Moreover, the following interesting result can be easily obtained.
Proposition 41 (Geometric interpretation of the subgradient).
Given a convex function L, assume
without any loss of generality that lev≤0(L) ̸= ∅(if not, a vertical shift of the L downwards by some
constant sufﬁces to guarantee the non-emptiness of lev≤0(L) ). Assume, also, an f ∈H \lev≤0(L) (see
Figure 17.29 ). Then, for any subgradient g of L at f, i.e., g ∈∂L( f ), the Xg, deﬁned in (17.78 ), acts
as a separating hyperplane between the point ( f , 0) and epi(L).
♦
Proof.
Since f /∈lev≤0(L), it is easy to see that L( f ) > 0. Hence, for the point ( f , 0),
⟨g, f −f ⟩+ L( f ) −0 = L( f ) > 0.
(17.79)
On the other hand, for every (h,r) ∈epi(L), the deﬁnition of the subgradient suggests that
⟨g, h −f ⟩+ L( f ) −r ≤⟨g, h −f ⟩+ L( f ) −L(h) ≤0.
(17.80)
A simple inspection of (17.79) and (17.80) implies that Xg stands as a separating hyperplane between
( f , 0) and epi(L).
□

1.17.7 A Convex Analytic Toolbox for Online Learning
947
f
(f )
g
epi(  )
0
lev 0( )
T ( f )
≤
FIGURE 17.29
The hyperplane Xg (17.78), generated by a subgradient g of L at f, stands as a separating hyperplane
between the point (f , 0) and epi(L) in the augmented space H × R.
θ
θ
g
FIGURE 17.30
The graph of the function |·|, and the supporting hyperplanes generated by the subgradients of |·| at 0.
Since lev≤0(L) belongs to epi(L), then Xg separates f and lev≤0(L) (see Figure 17.29). It can be
readily veriﬁed that the point ( f , L( f )) belongs to Xg. Hence, as it can be also veriﬁed by Figure 17.29,
Xg supports the epi(L) at the point ( f , L( f )).
Let us give now the subdifferentials of some celebrated convex functions.
Example 42 (ℓ1 norm).
The stage of discussion is the Euclidean Rl. Assume, ﬁrst, the classical case
of L(θ) := |θ|, θ ∈R. Then, the subdifferential of L is given as follows:
∂L(θ) =
# [−1, 1],
if θ = 0,
sign(θ),
if θ ̸= 0,
where sign( · ) stands for the sign of a real number. For the geometry associated to this cost function
see Figure 17.30.

948
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
Assume, now, that the function L becomes the more involved ℓ1 norm, i.e.,
L(θ) := ∥θ∥1 :=
l
i=1
|θi|,
∀θ ∈Rl.
Then, the subdifferential of L becomes as follows; given any θ ∈Rl deﬁne ﬁrst the index set Jθ : =

j ∈{1, 2, . . . ,l} : θ j = 0

. Then, based on Jθ, let a set of vectors τ 1, τ 2, . . . , τ 2ν, where ν : =
card(Jθ), and where the jth component of the ith τ i is given by
τi j :=
# sign(θ j),
if j /∈Jθ,
+1 or −1, if j ∈Jθ.
Then,
∂L(θ) =
⎧
⎪⎨
⎪⎩

sign(θ1), . . . , sign(θl)
T 
, if θ ̸= 0 and Jθ = ∅,
conv

τ 1, τ 2, . . . , τ 2ν
,
if θ ̸= 0 and Jθ ̸= ∅,
conv

τ 1, τ 2, . . . , τ 2l

,
if θ = 0,
where conv( · ) stands for the convex hull of a set [34,62].
♦
Example 43 (Distance function).
The subdifferential of the metric distance function to a closed
convex set C ⊂H is given as follows:
∂d( f , C) =
5 NC( f ) ∩B[0, 1], if f ∈C,

f −PC( f )
d( f ,C)

,
if f ∈H \ C,
where NC( f ) : = {v ∈H : ⟨v, g −f ⟩≤0, ∀g ∈C}, and B[0, 1] : = { f ∈H : ∥f ∥≤1}. Moreover,
the function d2(·, C) is (Fréchet) differentiable:
∂d2( f , C) = ∇d2( f , C) = {2( f −PC( f ))} ,
∀f ∈H.
1.17.7.3 Nonexpansive and quasi-nonexpansive mappings
Let us start with a few fundamental concepts regarding a mapping T : H →H which is deﬁned in the
Hilbert space H.
Deﬁnition 44 (A toolbox of mappings).
1. The ﬁxed point set of T is the set of all those points of H which are unaffected by T, i.e., Fix(T ) :=
{ f ∈H : T ( f ) = f }.
2. The mapping T is called γ -Lipschitzian, or γ -Lipschitz continuous, if there exists a γ > 0 such that
∥T ( f1) −T ( f2)∥≤γ ∥f1 −f2∥, ∀f1, f2 ∈H.
3. The mapping T is called nonexpansive if it is 1-Lipschitzian. It is well-known that whenever Fix(T )
is nonempty, then it is a closed convex subset of H [65]. A subclass of nonexpansive mappings
are the averaged ones. More speciﬁcally, the mapping T is called α-averaged nonexpansive, with
α ∈(0, 1), if there exists a nonexpansive mapping R : H →H such that T = (1 −α)I + αR. A
celebrated example of an 1
2-averaged nonexpansive mapping is the metric projection mapping PC
onto a nonempty, closed convex subset C of H. In this case, it can be easily veriﬁed that Fix(PC) = C.
For an illustration of a nonexpansive mapping, see Figure 17.31.

1.17.7 A Convex Analytic Toolbox for Online Learning
949
f 1
f 2
H
C
f 1 −f 2
PC(f 2)
PC(f 1)
PC(f 1) −PC(f 2)
A nonexpansive mapping.
f
Fix(T)
H
T (f )
g
f −g
T (f ) −g
A quasi-nonexpansive mapping.
(b)
(a)
FIGURE 17.31
Illustration of a nonexpansive and a quasi-nonexpansive mapping.
4. The mapping T is called strictly contractive if it is γ -Lipschitzian, with γ ∈(0, 1). Such a mapping
is the engine behind the celebrated Banach-Picard ﬁxed point theorem [65], which states that the
sequence generated by the repetition of T converges strongly to the unique ﬁxed point of T, i.e., for an
arbitrary f0 ∈H, the sequence fn+1 := T ( fn), ∀n, converges strongly to f∗, where Fix(T ) = { f∗}.
5. The mapping T, with Fix(T ) ̸= ∅, is called quasi-nonexpansive if
∥T ( f ) −g∥≤∥f −g∥,
∀f ∈H, ∀g ∈Fix(T ).
The ﬁxed point set of a quasi-nonexpansive mapping is closed and convex. Every nonexpansive
mapping is a quasi-nonexpansive one, but, in general, not vice-versa. The illustration of a quasi-
nonexpansive mapping can be found in Figure 17.31b.
♦
The following is a celebrated example of a γ -Lipschitzian mapping.
Example 45.
Consider a Euclidean space Rl, and the following quadratic function:
L(θ) := 1
2θ T Rθ −rT θ + ξ,
∀θ ∈Rl,
where R is a positive semideﬁnite matrix, of dimensions l × l, with maximum eigenvalue σmax > 0,
and ξ ∈R, r ∈Rl.
It can be easily veriﬁed that L′(θ) = Rθ −r,
∀θ ∈Rl. Then, ∀θ1, θ2 ∈Rl,
		L′(θ1) −L′(θ2)
		 = ∥R(θ1 −θ2)∥≤∥R∥∥θ1 −θ2∥= σmax ∥θ1 −θ2∥,
where ∥R∥= σmax is the spectral norm of R. Hence, L′ is σmax-Lipschitzian.
As the following theorem suggests, there is a strong link between γ -Lipschitz continuity and non-
expansiveness.
Fact 46 (The Baillon-Haddad theorem [70,71]).
Assume a differentiable convex function
L : H →R, with its gradient denoted by L′ : H →H. Then the following two statements are
equivalent.

950
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
1. L′ is γ -Lipschitzian.
2. The mapping T := I −2
γ L′ : H →H is nonexpansive, where I stands for the identity mapping in
H.
The following is an example of an averaged nonexpansive mapping.
Example 47 (Proximal mapping).
Given a convex continuous2 function L : H →R, the Moreau
envelope of index γ > 0 of L is the function
γ L : H →R : f 	→inf
g∈H

L(g) + 1
2γ ∥f −g∥2

.
(17.81)
Then, the proximal mapping proxγ L is deﬁned as the mapping which maps to an f ∈H the unique
minimizer of (17.81) [72–74]. It can be veriﬁed that the proximal mapping proxγ L is 1
2-averaged
nonexpansive with ﬁxed point set Fix(proxγ L) = argmin f ∈HL( f ) [72,73].
A well-known example of the proximal mapping is the classical projection operator onto closed
convex sets. Indeed, if C is a non-empty closed convex subset of H, then proxγ L = PC, ∀γ > 0, if
the function L takes the special form of the indicator function of C, ιC : H →R ∪{+∞}, which is
deﬁned as follows; ιC( f ) = 0, if f ∈C, and ιC( f ) = +∞, if f /∈C.3 Another celebrated example of
a proximal mapping is the well-known soft-thresholding operator (Example 2.16) [72].
Let us give now an example of a quasi-nonexpansive mapping.
Example 48 (Subgradient projection mapping).
Assume a convex function L : H × R, such that
its level set of height 0 is nonempty, i.e., lev≤0(L) ̸= ∅. Then, the subgradient projection mapping
TL : H →H with respect to L is deﬁned as follows:
TL( f ) :=
5
f −
L( f )
∥L′( f )∥2 L′( f ), if f /∈lev≤0(L),
f ,
otherwise,
where L′( f ) is any subgradient taken from the subdifferential ∂L( f ) at the point f. For the geometry
behind the concept of TL see Figure 17.29. It can be veriﬁed that TL( f ) is a quasi-nonexpansive
mapping, with Fix(T ) = lev≤0(L) [65].
Remark 49.
If we rely again on geometry, it can be seen that in the case where f /∈lev≤0(L), then
the image of f under TL is nothing but the (metric) projection of f onto the hyperplane which is deﬁned
by the intersection of XL′( f ) with H; see Figure 17.29.
Proof.
In Figure 17.29, the intersection of Xg, or better XL′( f ), with H is given if we set r = 0 in
(17.78). In other words,
XL′( f ) ∩H =

h ∈H : 0 =

L′( f ), h −f

+ L( f )

=

h ∈H :

L′( f ), h

=

L′( f ), f

−L( f )

.
This is clearly a hyperplane, and by using Example 37 the previous claim is established.
□
2In general, L is assumed to be lower semicontinuous [65].
3Notice that the function ιC is convex and lower semicontinuous.

1.17.7 A Convex Analytic Toolbox for Online Learning
951
For the sake of illustration, let us calculate the subgradient projection mappings of several loss
functions.
Example 50.
Assume the loss function
L( f ) := d( f , C),
∀f ∈H,
where C is a nonempty subset of H.
It is easy to see that lev≤0(L) = lev≤0

d(·, C)

= C. Hence, in the case where f /∈C, we have by
Example 48 and Example 43 that
Td(·,C)( f ) = f −
L( f )
∥L′( f )∥2 L′( f )
= f −
d( f , C)
			 f −PC( f )
d( f ,C)
			
2
f −PC( f )
d( f , C)
= f −

f −PC( f )

= PC( f ).
(17.82)
In other words, the subgradient projection mapping with respect to the metric distance function is
nothing but the classical metric projection mapping PC.
♦
Example 51.
It is interesting to calculate the subgradient projection mapping TL with respect to the
loss function L( · ) = d2(·, C). By following a procedure similar to the previous one, and by using
Example 43, one can verify that
Td2(·,C) = I + PC
2
,
where I stands for the identity mapping in H.
We stress here the difference between the subgradient projection mappings Td(·,C) and Td2(·,C).
Although both of the loss functions d(·, C) and d2(·, C) have the same level set lev≤0

d(·, C)

=
lev≤0

d2(·, C)

= C, it seems better to use the non-differentiable function d(·, C) in order to approach
C; a single application of Td(·,C) takes us directly onto C, while the application of Td2(·,C) covers only
half of the distance.
Finally, let us give an example which will be useful in Section 1.17.7.5. The following example
introduces the concurrent or parallel processing of multiple closed convex sets by the single application
of a properly deﬁned mapping.
Example 52.
Assume a number of q closed convex sets {C1, C2, . . . , Cq} such that 6q
i=1 Ci ̸= ∅.
Assume, also, a point f ∈H, which, for the sake of illustration, is assumed to satisfy f /∈6q
i=1 Ci.
How could we use the concept of the subgradient projection mapping in order to employ concurrently
all of {C1, C2, . . . , Cq} and to push the point f closer to 6q
i=1 Ci?
Since f /∈6q
i=1 Ci, then there exists an index set I f which indicates those closed convex sets that
f does not belong to, i.e.,
I f := {i ∈{1, 2, . . . , q} : f /∈Ci} .

952
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
Given I f , assign to each Ci, i ∈I f , a convex weight ωi, i.e., ωi ∈(0, 1] such that 
i∈I f ωi = 1.
Deﬁne the function:
L(h) :=

i∈I f
ωid( f , Ci)

j∈I f ω jd( f , C j)d(h, Ci),
∀h ∈H.
Notice here that if we deﬁne
βi :=
ωid( f , Ci)

j∈I f ω jd( f , C j),
∀i ∈I f ,
then it is easy to verify that L(·) = 
i∈I f βid(·, Ci) with 
i∈I f βi = 1. In other words, L is a
convex combination of the convex functions d(·, Ci), i ∈I f ; hence, it is convex. Notice, also, that
lev≤0(L) = 6
i∈I f Ci.
One can verify by Example 43 that
L′( f ) =

i∈I f
ωid( f , Ci)

j∈I f ω jd( f , C j)
f −PCi ( f )
d( f , Ci)
= 1
L

i∈I f
ωi

f −PCi ( f )

,
where L : = 
j∈I f ω jd( f , C j). Then, the subgradient projection mapping, applied to f, becomes as
follows:
TL( f ) = f −
1
L

i∈I f ωid2( f , Ci)
1
L2
			
i∈I f ωi

f −PCi ( f )
			
2
1
L

i∈I f ωi

f −PCi ( f )

= f + M

i∈I f ωi PCi ( f ) −f

,
where
M :=

i∈I f ωid2( f , Ci)
			
i∈I f ωi

f −PCi ( f )
2			
.
(17.83)
It becomes clear by the convexity of the function ∥·∥2 that M ≥1.
The relaxed version of TL is deﬁned as the mapping (1 −μ′)I + μ′TL, where μ′ ∈(0, 2), and I is
the identity mapping in H. Hence, the relaxation of TL, applied to the previous f, becomes as follows:
(1 −μ′) f + μ′TL( f ) = f + μ′M
⎛
⎝
i∈I f
ωi PCi ( f ) −f
⎞
⎠
= f + μ
⎛
⎝
i∈I f
ωi PCi ( f ) −f
⎞
⎠,

1.17.7 A Convex Analytic Toolbox for Online Learning
953
where μ := μ′M ∈(0, 2M). Notice that since M ≥1, the parameter μ can attain values larger than
or equal to 2.
In the case where f ∈6q
i=1 Ci, it is natural to leave f as it is. To summarize, the previous discussion
introduced the following mapping:
f 	→
5
f + μ

i∈I f ωi PCi ( f ) −f

, if f /∈6q
i=1 Ci,
f ,
if f ∈6q
i=1 Ci,
where μ ∈(0, 2M), with M given by (17.83). We will again meet these concepts later on, in
Example 67.
♦
1.17.7.4 The basic algorithmic ingredients for nonsmooth convex batch learning
In this section, we lay the foundations on which the subsequent section of online learning (Section
1.17.7.5) will be based. The objective of this section is to study several basic algorithmic components
related to batch learning, i.e., the task of solving an estimation problem after all the training data
have been gathered and made available. Such training data are utilized in order to generate a ﬁxed
convex loss function L : H →R, which quantiﬁes the designer’s perception of penalty or deviation
from the underlying model. If a nonempty closed convex set C ⊂H is also used as the available a-
priori constraint to the problem at hand, then batch learning breaks down to the following fundamental
optimization task:
minimize L( f ) such that f ∈C ⊂H.
(17.84)
The only assumption we make on the convex function L is to be a continuous one. Neither smoothness,
i.e., any form of differentiability, nor strict convexity is assumed for L. Such a generality is crucial if one
wishes to build algorithmic tools for modern learning tasks; indeed, the ℓ1 : Rl →R loss function is a
celebrated example of a nonsmooth, nonstrict, convex penalty function which infuses sparsity attributes
in a learning task.
The backbone of this section will be the following fundamental recursion; ﬁx arbitrarily an initial
point f0 ∈H and execute ∀n ∈N∗,
fn := PC

fn−1 −μnL′( fn−1)

,
(17.85)
where PC stands for the metric projection mapping onto C, L′( fn−1) is the subgradient of L at fn−1, and
(μn)n∈N∗is a sequence of user-deﬁned, non-negative real numbers, which are usually ﬁxed judiciously
in order for the sequence ( fn)n∈N to converge in some sense.
Recently, the fundamental recursion (17.85) lent itself to generalizations, and more speciﬁcally, to the
replacement of the metric projection mapping PC of (17.85) with the more general proximal mappings
(see Example 47), in order to derive the proximal-gradient, aka forward-backward algorithm, and the
Douglas-Rachford method [65,72,73,75,76]. Such a generalization corresponds to the minimization of
a sum of convex functions L :=  + ψ, where  is a convex function, which accommodates the misﬁt
of the observed data to the model, and ψ is the proximal function, introduced in order to serve a variety
of objectives, depending on the problem at hand. For example, ψ takes up the role of a regularizer in
order to control the “size” of an estimate in classiﬁcation or regression tasks, or that of a penalty term

954
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
which enforces a-priori knowledge, like sparsity, into the design. Such a path, is followed also by the
machine learning community in the very recent studies of [77,78].
Algorithm 53 (Projected Gradient Method (PGM) [79,80]).
Assume a nonempty closed convex
set C ⊂H, and a differentiable convex function L : H →R, such that its derivative L′ : H →H is
γ -Lipschitzian, and argmin f ∈CL( f ) ̸= ∅. Then, for any starting point f0 ∈H, and any μ ∈

0, 2
γ

,
the sequence ( fn)n∈N generated by the Projected Gradient Method (PGM):
fn := PC

fn−1 −μL′( fn−1)

,
∀n ∈N∗,
(17.86)
converges weakly to a point in argmin f ∈CL( f ).
Remark 54 (Alternative view of the PGM).
According to the Baillon-Haddad theorem, i.e., Theorem
46, the mapping T : = I −2
γ L′ is nonexpansive. Hence, PGM is nothing but the composition of the
projection mapping PC and the α-averaged nonexpansive mapping αT + (1 −α)I (see Deﬁnition 44),
for α ∈(0, 1), i.e.,
fn := PC

α

I −2
γ L′

+ (1 −α)I

( fn−1),
∀n ∈N∗.
Givenseveralscenarios,withrespecttoH, L,andC,thePGMishiddenbehindcelebratedalgorithmic
tools, as the following examples suggest.
Example 55 (Projected Landweber Method [65]).
Assume a matrix A ∈Rm×l, a vector y ∈Rm,
and a nonempty closed convex set C ⊂Rl. For an arbitrarily ﬁxed initial point θ0 ∈Rl, and an
μ ∈

0, 2/ ∥A∥2
, where ∥A∥stands for the spectral norm of A, the Projected Landweber Method:
θn := PC

θn−1 −μAT 
Aθn−1 −y

,
∀n ∈N∗,
converges to a point in argminθ∈C
1
2 ∥Aθ −y∥2 ̸= ∅.
♦
Proof.
If we deﬁne L(θ) := 1
2 ∥Aθ −y∥2 , ∀θ ∈Rl, then it can be easily veriﬁed that
L(θ) = 1
2θ T AT Aθ −yAθ + 1
2 ∥y∥2 ,
θ ∈Rl.
According to Example 45, the derivative L′(θ) = AT Aθ −AT y, ∀θ ∈Rl, is σmax-Lipschitzian, where
σmax is the maximum eigenvalue of AT A. Since by deﬁnition σmax = ∥A∥2 [81], if we also set C := H,
then the Projected Landweber Method is a direct application of the PGM to the speciﬁc L.
□
Example 56(MinimumMean-SquaredError(MMSE)estimation[5,6]).
Assumethelinearmodel:
y = xT θ + η,
(17.87)
where θ ∈Rl is a parameter vector, y ∈R is the observed signal, and x ∈Rl, η ∈R are the
vector-valued input signal and the noise random variables, respectively. The correlation matrix of the

1.17.7 A Convex Analytic Toolbox for Online Learning
955
input signal R : = E
 
xxT !
, where E[·] stands for the expectation operator, is obviously a positive
semideﬁnite matrix. In practice, available to us are only realizations of (17.87) via the sequences
(xn, yn)n∈N∗and (ηn)n∈N∗which are related to each other via yn = xT
n θ + ηn, ∀n ∈N∗.
Deﬁne the mean square error as follows:
L(θ) := 1
2E

(y −xT θ)2
,
∀θ ∈Rl.
Our objective is to compute an estimate of θ, by the least mean squares rationale:
ﬁnd θ∗∈argmin
θ∈Rl L(θ).
(17.88)
It is easy to verify that
L(θ) = 1
2θ T Rθ −E

yxT 
θ + 1
2E

y2
= 1
2θ T Rθ −rT θ + ξ,
where R := E[yx] and ξ := 1
2E
 
y2!
.
According to Example 45, the function L is σmax-Lipschitzian, where σmax is the maximum eigen-
value of R. Hence, the sequence (θn)n∈N generated by the PGM of Algorithm 53:
θn := θn−1 −μ

Rθn−1 −r

,
∀n ∈N∗,
(17.89)
converges to a point θ∗∈argminθ∈CL(θ), provided that μ ∈

0,
2
σmax

. Indeed, this iterative procedure
is of paramount importance to the celebrated MMSE and Wiener estimation theory [5,6]. Moreover,
this iteration is the motivation for the classical LMS algorithm (17.34). To see this, notice that (17.89)
can be equivalently written ∀n ∈N∗as:
θn = θn−1 −μ

Rθn−1 −r

= θn−1 −μ

E

xnxT
n

θn−1 −E
 
ynxn
!
= θn−1 −μE

(xT
n θn−1 −yn)xn

.
It can be readily veriﬁed that (17.34) is an approximation of the previous iteration. Notice also that the
Lipschitz constant
2
σmax of the L′ mapping is of central importance for the proof of convergence in the
mean of the LMS algorithm (see Section 1.17.6.1).
Let us now go back to (17.85) for the case where L is not differentiable. Although there are several
strategies to control the step-sizes (μn)n∈N in (17.85), in this note we focus on the following method.
Algorithm 57 (Projected Subgradient Method (PSMa) [82,83]).
For any f0 ∈H, let the following
recursion
fn := PC

fn−1 −
μn
max{1,∥L′( fn−1)∥}L′( fn−1)

,
n = 1, 2, . . . ,
(17.90)

956
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
where L′( fn−1) stands for the subgradient of L at fn−1, and the non-negative numbers (μn)n∈N∗satisfy
the following conditions:
∞

n=1
μn = ∞,
(17.91a)
∞

n=1
μ2
n < ∞.
(17.91b)
Regarding the previous algorithm, the following hold true.
1. If the previous algorithm generates an inﬁnite sequence ( fn)n∈N , then lim infn→∞L( fn) =
inf f ∈C L( f ).
2. If the set of solutions CL∗of (17.84) is nonempty, then either (17.90) stops at some iteration n∗,
in which case fn∗∈CL∗, or it generates an inﬁnite sequence which converges weakly to some
f∗∈CL∗.
3. If CL∗is empty, then ( fn)n∈N is unbounded.
Remark 58.
The previous convergence results hold also true in the case where the L′( fn) is taken from
the ϵ-subdifferential (ϵ ≥0) of L at fn, which stands as a generalization of the classical subdifferential
[62]. It has been observed in [83] that a slight variation of (17.90) is sufﬁcient to guarantee convergence
of the sequence of estimates ( fn)n∈N in the strong sense.
The unrestricted case of (17.84), i.e., the case where C : = H, was studied in [84]. In [84], the
condition (17.91b) is relaxed to the limn→∞μn = 0, and the recursion is formed as [84]
fn :=
# fn−1 −
μn
∥L′( fn−1)∥L′( fn−1), if L′( fn−1) ̸= 0,
fn−1,
otherwise.
(17.92)
It is proved in [84] that the generating sequence ( fn)n∈N is a minimizing one, i.e., lim infn→∞L( fn) =
inf f ∈C L( f ), but no results on the convergence of the ( fn)n∈N are given. The ﬁnite dimensional
unconstrained case of (17.85), i.e., H is a Euclidean space and C := H, but with different strategies on
designing the quantities (μn)n∈N∗, was studied in [85,86].
For the sake of illustration, let us apply the PSMa to a classical learning task, and show that this
classical rule falls under the previously generic recursion.
Example 59 (The Perceptron).
Let two patterns 1, 2, from which a number of ﬁnite dimensional
observations emanate: xi ∈Rl, i = 1, 2, . . . , N1, from 1, and ξ j ∈Rl, j = 1, 2, . . . , N2, from 2.
Assume also a reproducing kernel function κ which generates an RKHS H of very high, possibly inﬁnite,
dimension. The observed data are mapped into H by means of the following mapping: xi 	→κ(xi, ·),
and ξ j 	→κ(ξ j, ·).
We assume that the patterns 1, 2 are linearly, or better, afﬁnely separable in the feature space H,
i.e., there exist f ∈H and β ∈R, such that
⟨f , κ(xi, ·)⟩+ β = f (xi) + β ≥0,
∀i ∈{1, 2, . . . , N1},

f , κ(ξ j, ·)

+ β = f (ξ j) + β ≤0,
∀j ∈{1, 2, . . . , N2}.
(17.93)

1.17.7 A Convex Analytic Toolbox for Online Learning
957
Our task becomes that of employing an iterative algorithm in order to ﬁnd an f ∈H and a β ∈R
which achieve the previous goal. An example of such an iterative procedure is the perceptron, which
was introduced in [87,88], in order to solve the task (17.93).
Proposition 60.
Assume that there exist f ∈H and β ∈R such that
f (xi) + β > 0,
∀i ∈{1, 2, . . . , N1},
f (ξ j) + β < 0,
∀j ∈{1, 2, . . . , N2}.
Then, Algorithm 57 (PSMa) solves the task (17.93) in a ﬁnite number of steps.
♦
Proof.
See Appendix B.
□
According to the studies of [84,85], the PSMa of Algorithm 57 and its variants exhibit in general
a rather slow convergence performance, due to the selection policies of the step-sizes (μn)n∈N . To
remedy such a situation, Polyak [89] introduced the following algorithm.
Algorithm 61 (Projected Subgradient Method (PSMb) [89]).
Deﬁne, ﬁrst, L∗: = inf f ∈C L( f ).
For an arbitrarily ﬁxed f0 ∈H, assume the recursion ∀n ∈N∗:
fn :=
5
PC

fn−1 −μn L( fn−1)−L∗
∥L′( fn−1)∥2 L′( fn−1)

, if L′( fn−1) ̸= 0,
PC( fn−1),
otherwise,
(17.94)
where μn ∈(0, 2).
Assume that the set of solutions CL∗of (17.84) is nonempty, and let a point f∗∈CL∗. If, in
addition, there exists an ϵ > 0 such that μn ∈[ϵ, 2 −ϵ], ∀n, and there exists a c > 0 such that
		L′( f )
		 ≤c, ∀f ∈B
 
f∗, ∥f0 −f∗∥!
∩C, where B
 
f∗, ∥f0 −f∗∥!
stands for the closed ball centered
at f∗with radius ∥f0 −f∗∥, then
1. limn→∞L( fn) = L∗.
2. The sequence ( fn)n∈N converges weakly to a point in CL∗.
♦
Proof.
See Appendix C.
□
The following proposition shows that the PSMb, i.e, Algorithm 61, has a clear geometric description
as the composition of two projections, a relaxed version of a subgradient one, and a metric one.
Proposition 62.
If we assume that the set of all minimizers of the function L on C is nonempty, i.e.,
CL∗̸= ∅, then the PSM Algorithm 61 can be cast in the following equivalent form:
fn = PC

(1 −μn)I + μnT ˜L

( fn−1),
∀n ∈N∗,
where I stands for the identity mapping in H, and the mapping T ˜L : H →H stands for the subgradient
projection mapping (see Example 48) with respect to the function ˜L( · ) := L( · ) −L∗.
♦
Proof.
First, notice that the mapping
(1 −μ)I + μT ˜L,
μ ∈(0, 2),
is called the relaxed T ˜L [65].

958
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
It is also easy to verify that lev ≤0( ˜L) = { f ∈H : L( f ) ≤L∗}. Since CL∗̸= ∅, it can be clearly
seen that ∅̸= CL∗⊂lev ≤0( ˜L). Moreover, since ˜L is nothing but a vertical shift of L, it can be also
readily seen that ∂˜L = ∂L. Under this setting, the claim of the proposition can be easily established by
a simple inspection of Example 48.
□
1.17.7.5 Algorithmic tools for nonsmooth convex online learning
Based on Section 1.17.7.4, we will present here a variety of algorithmic tools designed for online
learning, i.e., learning tasks where the training data keep on ﬂowing sequentially into the processing
unit. As it has already been stated in the introduction of the chapter, such a continuous ﬂow is dictated by
the need to monitor possible ﬂuctuations or abrupt changes of the surrounding environment, as well as
any time-varying a-priori knowledge about the model under study. The sequential ﬂow of the training
data is dictated by real-time units where the need for processing those data one by one, and not in
batches as in Section 1.17.7.4, is the key ingredient for simpler computational operations, and thus, for
less consumed power, as well as for easier handling of massive volumes of incoming data.
In order to cast the previous considerations into a mathematical formulation, we let the non-negative
integer index n to account not only for the recursion index, as this was the case in Section 1.17.7.4, but
also to denote time instants. In other words, the set of all non-negative integers N stands also for the
entire time horizon. Hence, instead of the ﬁxed loss function L and the constraint set C of (17.84), we
will deal here with a sequence of loss functions (Ln)n∈N∗and constraint sets (Cn)n∈N∗, and the task of
(17.84) becomes now that of an asymptotic minimization one.
As in Section 1.17.7.4, the loss functions (Ln)n∈N∗are not conﬁned to be either differentiable or
strictly convex. In such a way, design freedom is favored against easiness; although the differentiability
or strict convexity of (Ln)n∈N∗might result into elegant and easy-to-implement algorithmic tools, as
well as stronger convergence results, our priority is not the mathematical tractability, but the selection of
those convex loss functions that are in agreement with the available model and the a-priori knowledge.
Our data generation model, in its most general form, is given as follows:
yn = f∗(xn) + ηn,
∀n ∈N∗,
(17.95a)
where f∗is an element of an RKHS H, and thus, it is in general a nonlinear function f∗: Rl →R, and
(ηn)n∈N∗is the noise stochastic process. Notice that due to the reproducing property of the RKHS, the
quantity f∗(xn) = ⟨f∗, κ(xn, ·)⟩; in other words, the l-dimensional input data (xn)n∈N∗are mapped
into the highly dimensional feature space H as xn 	→κ(xn, ·). The real-valued sequence (yn)n∈N∗
contains the observed data. For short, the sequence (yn, xn)n∈N∗will be called the sequence of training
data in the sequel. Given these training data, our task is to estimate the unknown nonlinear function f∗.
The nonlinear model of (17.95a) takes a simpler form whenever the kernel behind the feature RKHS
H is chosen to be the linear one. In such a case, H becomes an l-dimensional Euclidean space, and
(17.95a) reduces to the classical linear model
yn = xT
n θ∗+ ηn,
∀n ∈N∗,
(17.95b)
where θ∗∈Rl is now the unknown vector to be estimated.

1.17.7 A Convex Analytic Toolbox for Online Learning
959
The previous training data (yn, xn)n∈N∗as well as the underlying model (17.95a) are used to construct
the sequence of loss functions (Ln)n∈N∗. Given also the sequence of closed convex sets (Cn)n∈N∗,
which depicts the time-varying a-priori information, the recursion which runs beneath almost all of the
algorithmic tools developed for convex online learning tasks is a generalization of (17.85) to the current
time-adaptive setting; given any initial point f0 ∈H, execute
fn := PCn

fn−1 −μnL′
n( fn−1)

,
∀n ∈N∗.
(17.96)
In what follows, and as a special case of (17.96), we provide with a classical time-adaptive algorithm.
Algorithm 63 (Least Mean Squares (LMS) algorithm [5,6,36,38]).
Regarding (17.95a), assume
the loss function
Ln( f ) := 1
2

yn −f (xn)
2 = 1
2

yn −⟨f , κ(xn, ·)⟩2 ,
∀f ∈H.
Clearly, this loss function is everywhere differentiable, and
L′
n( f ) =

f (xn) −yn

κ(xn, ·),
∀f ∈H.
Adopting the rationale of the PGM, i.e., Algorithm 53, leads to the Kernel Least Mean Squares
(KLMS) recursion:
fn := fn−1 −μL′
n( fn−1)
=
fn−1 −μ

fn−1(xn) −yn

κ(xn, ·),
∀n ∈N,
(17.97)
where μ takes values in order to guarantee the convergence of the sequence ( fn)n∈N in some sense.
In the case where the RKHS H becomes the Euclidean Rl, via a linear kernel, the nonlinear model
(17.95a) reduces to (17.95a), and the KLMS (17.97) becomes the classical LMS algorithm:
θn = θn−1 −μ

xT
n θn−1 −yn

xn,
∀n ∈N∗.
♦
Motivated by the geometry behind the batch Algorithm 61, the following online counterpart was
introduced in [90,91].
Algorithm 64 (Adaptive Projected Subgradient Method (APSM) [90,91]).
Assume a sequence of
non-negative, continuous, convex loss functions (Ln : H →R)n∈N∗, not necessarily differentiable.
Let, also, a nonempty closed convex set C ⊂H. Then, for an arbitrarily ﬁxed initial point f0, the
Adaptive Projected Subgradient Method (APSM) is formulated as follows; ∀n ∈N∗,
fn :=
⎧
⎨
⎩
PC

(1 −μn) fn−1 + μn

fn−1 −
Ln( fn−1)
∥L′n( fn−1)∥2 L′
n( fn−1)

, if L′
n( fn−1) ̸= 0,
PC( fn−1),
if L′
n( fn−1) = 0,
(17.98)
where μn ∈(0, 2), and L′
n( fn−1) stands for any subgradient of the function Ln at fn−1, ∀n ∈N∗.
♦

960
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
Algorithm 65. Adaptive Projected Subgradient Method (APSM)
Require: The training data

(xn, yn)

n∈N∗, the sequence of non-negative convex loss functions (Ln)n∈N∗,
an RKHS H, and a closed convex set C.
1: Choose any f0 ∈H.
2: for n = 1, 2, . . . do
3: Available is the current estimate fn−1.
4: Choose any μn ∈(0, 2).
5: if L′n( fn−1) ̸= 0 then
6:
fn := PC

(1 −μn) fn−1 + μn

fn−1 −
Ln( fn−1)
∥L′n( fn−1)∥2 L′n( fn−1)

.
7: else
8:
fn := PC( fn−1).
9: end if
10: end for
Output: The sequence of estimates ( fn)n∈N .
Remark 66.
In order to spread the applicability range of the APSM to scenarios with more
involved a-priori information usage than the single projection mapping PC in (17.98), the study in
[92] extended the APSM to the case where certain nonexpansive mappings take the place of PC in
(17.98). Moreover, very recently, further generalization has been achieved for the case where the more
ﬂexible and general quasi-nonexpansive mappings, which include, for example, the proximal ones
(see Example 47) as a special case, take up the role of handling the time-varying a-priori information
[92–94]. Indeed, such generalizations span the applicability of the APSM [95] from classiﬁcation [96]
and regression tasks [42] in RKHSs, to sparsity-aware signal recovery [97], and sensor network [98]
applications.
♦
The following section provides several important versions of the generic scheme of Algorithm 65.
1.17.7.6 The kernel adaptive projected subgradient method (KAPSM)
We are now ready to state the kernel APSM algorithm, which together with the KLMS and KRLS,
comprise three major online/time adaptive schemes in Signal Processing. Moreover, as we will see, the
KAPSM, as it is the case with its linear counterpart, enjoys a number of advantages, including its agility
to cope efﬁciently both with regression as well as with classiﬁcation problems, and also to allow for
straightforward modiﬁcations to cope with robustness, when the environment dictates so. The reason
behind this agility lies on the freedom of choice for the sequence of convex loss functions (Ln)n∈N∗in
Algorithm 65; the nature of the problem, e.g., a classiﬁcation or a regression task, and the objectives of
the designer dictate the choice of (Ln)n∈N∗. In order to be more concrete, we start our discussion with
Example 67, which incorporates a sequence of specialized loss functions (Ln)n∈N∗in the APSM, but
with a wide applicability range. For example, the KNLMS (see (17.41)), as well as the Kernel Afﬁne
Projection Algorithm (KAPA) [37,68], which is a kernelized version of the classical APA [6,99,100],
become its special cases.

1.17.7 A Convex Analytic Toolbox for Online Learning
961
Example 67 ([101]).
Assume a sequence of closed convex sets (Cn)n∈N∗, where each Cn is associated
to a pair of training data (yn, xn). Given a positive integer q, the following discussion will evolve along
the lines of the rationale given in Example 52, and it will introduce the concurrent or parallel processing
of multiple closed convex sets or data per time instant n. Hence, given a point fn−1 ∈H, deﬁne, ﬁrst,
the index set
I fn−1 := {i ∈{n −q + 1, . . . , n} : fn−1 /∈Ci} .
To each i ∈I fn−1, we assign a convex weight ω(n)
i
, i.e., ω(n)
i
∈(0, 1] such that 
i∈I fn−1 ω(n)
i
= 1.
Deﬁne the function:
Ln( f ) :=

i∈I fn−1
ω(n)
i
d( fn−1, Ci)

j∈I fn−1 ω(n)
j d( fn−1, C j)
d( f , Ci),
∀f ∈H.
(17.99)
By mimicking the subgradient projection mapping, the procedure of Example 52, and for some μ′
n ∈
(0, 2), we obtain
(1 −μ′
n) fn−1 + μ′
n

fn−1 −
Ln( fn−1)
		L′n( fn−1)
		2 L′
n( fn−1)

= fn−1 −μ′
n
Ln( fn−1)
		L′n( fn−1)
		2 L′
n( fn−1)
= fn−1 + μ′
nM′
n
⎛
⎝
i∈I fn−1
ω(n)
i
PCi ( fn−1) −fn−1
⎞
⎠,
where
M′
n :=

i∈I fn−1 ω(n)
i
d2( fn−1, Ci)
			
i∈I fn−1 ω(n)
i

fn−1 −PCi ( fn−1)
			
2 .
Hence, the APSM for this special case of (Ln)n∈N∗becomes as follows:
fn = fn−1 + μn
⎛
⎝
i∈I fn−1
ω(n)
i
PCi ( fn−1) −fn−1
⎞
⎠,
(17.100a)
where the extrapolation parameter μn ∈(0, 2Mn), with
Mn :=
5
M′
n, if 
i∈I fn−1 ω(n)
i

fn−1 −PCi ( fn−1)

̸= 0,
1,
otherwise.
(17.100b)
As the following example demonstrates, a couple of celebrated Signal Processing and Machine
Learning tools stem from (17.100a).

962
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
Example 68 (Kernel Afﬁne Projection Algorithm (KAPA) [6,37,68,99,100] and Kernel Normal-
ized Least Mean Squares (KNLMS) algorithm [5,6,37,102–104]).
Given the sequence of training
data (yn, xn)n∈N∗, and a user-deﬁned positive integer parameter m, deﬁne the following linear mapping
according to Example 39:
An( f ) :=
⎡
⎢⎣
⟨f , κ(xn, ·)⟩
...
⟨f , κ(xn−m+1, ·)⟩
⎤
⎥⎦=
⎡
⎢⎣
f (xn)
...
f (xn−m+1)
⎤
⎥⎦,
∀f ∈H, ∀n ∈N∗.
Let also yn := [yn, . . . , yn−m+1]T ∈Rm, and deﬁne the sequence of afﬁne sets:
Vn := argmin
f ∈H
		yn −An( f )
		2 ,
n ∈N∗,
and the sequence of loss functions:
Ln( f ) := d( f , Vn),
∀f ∈H, ∀n ∈N∗.
Note that this scenario is a special case of the APSM of Example 67, where q := 1, and Cn := Vn, ∀n ∈
N∗.
Now, motivated by (17.77) and (17.82), and for a sequence of user-deﬁned parameters μn ∈
(0, 2), n ∈N∗, we form the following recursive process by means of a relaxed version of the subgradient
projection mapping TL\ with respect to Ln; for an arbitrary initial point f0 ∈H, and ∀n ∈N∗,
fn := (1 −λn) fn−1 + μnTL\( fn−1) = (1 −μn) fn−1 + μn PVn( fn−1)
=
fn−1 + μn

PVn( fn−1) −fn−1

= fn−1 −μn A†
n

An( fn−1) −yn

,
(17.101)
where A†
n stands for the pseudoinverse mapping of An. Equation (17.101) is nothing but the Kernel
APA [37,68].
In the case where the linear kernel is chosen for H, then H reduces to a Euclidean Rl, and the linear
mapping An becomes:
An(θ) :=
⎡
⎢⎣
xT
n θ
...
xT
n−m+1θ
⎤
⎥⎦=
⎡
⎢⎣
xT
n...
xT
n−m+1
⎤
⎥⎦θ = Anθ,
∀θ ∈Rl, ∀n ∈N∗,
where An : = [xn, . . . , xn−m+1]T . Hence, (17.101) reduces to the classical APA [99,100]; for an
arbitrarily ﬁxed initial point θ0, for μn ∈(0, 2), and ∀n ∈N∗,
θn = θn−1 −μn A†
n

Anθn−1 −yn

,
where A†
n is the Moore-Penrose pseudoinverse of An.

1.17.7 A Convex Analytic Toolbox for Online Learning
963
It is straightforward to verify also that in the case where m : = 1 in the KAPA, then the KNLMS
algorithm (17.41) is obtained. Thus, KNLMS and its Euclidean version, i.e., the classical NLMS [5,6,
102,104], become also special cases of the APSM of Example 67.
It is worthy to notice here that the APA appears to be sensitive when operating in substantially noisy
environments [101]. Moreover, it is prone to numerical instability issues due to the need for computation
of the Moore-Penrose pseudoinverse in (17.101). However, this is not the case for the APSM. The study
[101] showed that the APSM version of Example 67 is resilient to noise, and that no computation of a
Moore-Penrose pseudoinverse is needed in order to achieve the same goal as the APA; indeed, given
the sequence of training data

(xn, yn)

n∈N∗, deﬁne Cn := Hn, ∀n ∈N∗, where
Hn := { f ∈H : ⟨f , κ(xn, ·)⟩= yn} ,
∀n ∈N∗,
and set q := m in Example 67 in order to obtain an algorithmic procedure with the same objective as
the APA, but free from the computationally thirsty task of calculating the Moore-Penrose pseudoinverse
of a matrix.
Let us tailor now the APSM of Example 67 to suit regression tasks. To this end, we will use a special
class of closed convex sets, namely the (closed) hyperslabs of Example 38. Given the sequence of
training data

(xn, yn)

n∈N∗, and a user-deﬁned parameter ϵ > 0, let the (closed) hyperslab (∀n ∈N∗):
Sn[ϵ] : = { f ∈H : | f (xn) −yn| ≤ϵ}
= { f ∈H : | ⟨f , κ(xn, ·)⟩−yn| ≤ϵ} ,
be the set that gathers all those points of the RKHS H which achieve a ﬁtness to the observed value yn
of tolerance up to ϵ, when evaluated onto xn. To illustrate the way that the APSM operates, let us apply
the sequence of the previous hyperslabs (Sn[ϵ])n∈N∗to Example 67, i.e., use the speciﬁc (Sn[ϵ])n∈N∗
in the place of the closed convex sets (Cn)n∈N∗in Example 67. The objective of the APSM, as well as
of any other online learning algorithm, is the following; given the training data

(xi, yi)
n
i=1, compute
an n-dimensional real-valued vector θ = (θ1, . . . , θn)T in order to produce the estimate fn ∈H as a
linear combination of the elements of the dictionary {κ(xi, ·)}n
i=1, i.e.,
fn =
n

i=1
θiκ(xi, ·).
(17.102)
In general, the vector θ depends on the time index n. However, we have suppressed here this dependency
for notational convenience. Each learning algorithm has its own way to calculate θ; for example, the
APSM of Example 67 follows the way illustrated in Algorithm 1.17.7.6.
The Algorithm 69 is nothing but a realization of Algorithm 65, where (Sn[ϵ])n∈N∗take the place of
(Cn)n∈N∗, and where calculations take place in the domain of coefﬁcients θ, instead of the functional
space H. To be more rigorous, such a transfer of operations from H to the domain of coefﬁcients is due
to the linear mapping:
span

{κ(xi, ·)}n
i=1

→Rn
fn =
n

i=1
θiκ(xi, ·) 	→(θ1, . . . , θn)T = θ.

964
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
Algorithm 69. The APSM of Example 67 with hyperslabs
Require: The training data ((xn, yn))n∈N∗, an RKHS H, and an integer q > 1.
1: Set D := ∅.
2: for n = 1, 2, . . . do
3:
Insert xn into D, i.e., set un := xn, and D := {u1, . . . , un−1, un}.
4:
Increase the length of θ by one, i.e., let θ := (θ1, . . . , θn−1, θn)T , where θn := 0.
5:
Deﬁne J := {max{1, n −q + 1}, . . . , n}, of length q, at most.
6:
For every i ∈J , deﬁne
βi :=
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩

n−1
j=1 θ jκ(u j,ui)−yi−ϵ
κ(u j,ui)
if 
n−1
j=1 θ jκ(u j, ui) > yi + ϵ,
0,
if −ϵ≤
n−1
j=1 θ jκ(u j, ui) < yi −ϵ,

n−1
j=1 θ jκ(u j,ui)−yi−ϵ
κ(u j,ui)
if 
n−1
j=1 θ jκ(u j, ui) < yi −ϵ.
7:
Deﬁne the index set I fn−1 := {i ∈J : βi ̸= 0}.
8:
Choose the convex weights ω(n)
i
∈(0, 1], ∀i ∈I fn−1, such that 
i∈I fn−1 ω(n)
i
= 1.
9:
if 
i, j∈I fn−1 ω(n)
i
ω(n)
j βiβ jκ(ui, u j) ̸= 0, then
10:
Deﬁne Mn :=

i∈I fn−1 ω(n)
i
β2
i κ(ui,ui)

i, j∈I fn−1 ω(n)
i
ω(n)
j βiβ jκ(ui,u j).
11:
else
12:
Let Mn := 1.
13:
end if
14:
Choose any μn ∈(0, 2Mn).
15:
For every i ∈I fn−1, deﬁne θi := θi −μnω(n)
i
βi.
Output: The dictionary D = {u1, . . . , un}, and the vector θ = (θ1, . . . , θn)T , which produce the estimate
fn = 
n
i=1 θiκ(ui, ·).
16: end for
Let us give here a few more comments in order for the reader to grasp the connection between the
Algorithms 12 and 13. Given the estimate fn−1 = 
n−1
i=1 θiκ(xi, ·), Step 6 of Algorithm 69 is based
on (17.74), and it contains the decisive term in order to calculate PSi[ϵ]( fn−1) in Example 67. If the
quantity βi is non-zero, then it follows from (17.74) that fn−1 /∈Si[ϵ]; this is hidden behind the Step 7
of Algorithm 69. Finally, notice that Step 15 refers to (17.100a).
The way to incorporate the hyperslab Sn[ϵ] in Example 67 was via the non-negative distance function
d(·, Sn[ϵ]), as (17.99) clearly suggests. The connection between Sn[ϵ] and d(·, Sn[ϵ]) has been already
made clear by Def. 34; indeed, lev≤0

d(·, Sn[ϵ])

= Sn[ϵ]. However, by the freedom that the general
form of the APSM in (17.98) infuses into the design, as well as by the way that the loss function is
deﬁned in Example 67, one would expect also other candidates of convex loss functions, which take the
place of d(·, Sn[ϵ]) in (17.99), but still maintain Sn[ϵ] as their 0th level set, i.e., lev≤0( · ). For example,
the ϵ-insensitive loss:
ℓ: H →R : f 	→ℓ( f ) :=

| f (xn) −yn| −ϵ

+ ,

1.17.7 A Convex Analytic Toolbox for Online Learning
965
where ( · )+ stands for the positive part of a real number (see Figure 17.33), is a non-negative convex
function that achieves the previous objective. The previous loss function candidate was not a random
choice; it has been used extensively in Support Vector Regression (SVR) [2,7,13], due to its remarkable
robustness attributes. The previous discussion illustrates the ﬂexibility of the APSM with respect to the
choice of the loss function in (17.98) or in (17.99). Such a choice is often dictated by the nature of the
learning task at hand; a sequence of halfspaces were utilized in [96], as a special case of the (Cn)n∈N∗
in Example 67, in order to attack a classiﬁcation problem. Moreover, another choice for the sequence of
loss functions (Ln)n∈N∗in (17.98) led to a convex analytic alternative for the celebrated RLS in [105],
where the need for inverting an autocorrelation matrix is bypassed.
1.17.7.6.1
Sparsiﬁcation
As we have seen in Section 1.17.6.2.1, any online learning algorithm, which operates in a potentially
inﬁnite dimensional RKHS, suffers from the large number of incoming training data (xn, yn)n∈N∗, since
such an abundance of data causes an explosion of the system’s computational complexity and memory
requirements, via kernel series expansions, like (17.39) and (17.102). This can be also observed in
Algorithm 69. Hence, sparsiﬁcation of the expansions in (17.39) or (17.102) is needed. To make these
concepts as concrete as possible, let us elaborate more on the realization of the APSM given in Algorithm
69. In order to enforce sparsiﬁcation into the algorithm, and motivated by the discussion related to
(17.11), the study in [96] imposed an additional constraint to the sequence of estimates ( fn)n∈N ; the
following closed ball, with center 0 and radius  > 0 (see also Example 36):
B[0, ] := { f ∈H : ∥f ∥≤},
was used in the place of the closed convex set C in (17.98). It turns out that such a sparsiﬁcation scheme
is equivalent to a forgetting factor mechanism that forgets data in the remote past; see [96] for a more
detailed presentation. Since such a forgetting mechanism renders the contribution of “old” training
data weak in kernel series expansions like (17.102), it is natural to adopt a buffer of length M where
only the M more recent training data are kept. For the sake of illustration, this sparsiﬁcation strategy is
summarized by the pseudocode formulation of Algorithm 70. For more advances on the capability of
the APSM to incorporate constraints in its formulation, the interested reader is referred to [42,92,93].
The main difference between the Algorithms 13 and 14 lies on the previous closed-ball sparsiﬁcation
strategy, which is realized in the latter algorithm. More speciﬁcally, this strategy can be seen from
Step 15 to Step 29 of Algorithm 70. In Step 15, ν stands for the square of the norm of the term
fn−1 + μn

i∈I fn−1 ω(n)
i
PCi ( fn−1) −fn−1

, met in (17.100a). The lines from Step 17 to Step 22
refer to the application of the metric projection mapping PB[0,], i.e., Example 36, , onto the closed ball
constraint B[0, ], according to the scheme of the general APSM, depicted in (17.98). Finally, the lines
starting from Step 23 to Step 29, ensure that the size of the dictionary stays at most M, by “throwing
away” the “oldest” contribution in D. As it was explained in [96], such a brute-force method is justiﬁed
by the effect that the mapping PB[0,] has on the coefﬁcients of θ; it has the tendency to trim down the
“oldest” ones.
In principle, any other sparsiﬁcation scheme, as the ones discussed in Section 1.17.6.2.1, can be
used instead of the closed-ball one, seen in Algorithm 70. For example, instead of “throwing away” the
“oldest” coefﬁcient of θ in Algorithm 70, a more “gentle” approach would be to discard that coefﬁcient

966
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
0
500
1000
1500
2000
2500
3000
3500
4000
4500
−5.5
−5
−4.5
−4
−3.5
−3
−2.5
−2
−1.5
−1
−0.5
0
10*log10(MSE)
n
Non linear channel Equalization
 
 
QKNLMS
QKAPSM
KRLS
(a) q = 2.
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
−5.5
−5
−4.5
−4
−3.5
−3
−2.5
−2
−1.5
−1
−0.5
0
10*log10(MSE)
n
Non linear channel Equalization
 
 
QKNLMS
QKAPSM
KRLS
(b) q = 4.
0
500
1000
1500
2000
2500
3000
3500
4000
4500
−5.5
−5
−4.5
−4
−3.5
−3
−2.5
−2
−1.5
−1
−0.5
0
10*log10(MSE)
n
Non linear channel Equalization
 
 
QKNLMS
QKAPSM
KRLS
(c)
(e)
q = 8.
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
−5.5
−5
−4.5
−4
−3.5
−3
−2.5
−2
−1.5
−1
−0.5
0
10*log10(MSE)
n
Non linear channel Equalization
 
 
QKNLMS
QKAPSM
KRLS
(d) q = 16.
0
1000
2000
3000
4000
5000
0
10
20
30
40
50
60
70
Expansion size (M)
n
Evolution of the Expansion‘s size
 
 
QKNLMS − QKAPSM
KRLS
Growth of the size of the expansion for each algorithm.
FIGURE 17.32
Learning curves for the QKAPSM of Example 67 employing the Quantization sparsiﬁcation procedure (section
1.17.6.2.1), QKNLMS, and KRLS, and for the same nonlinear equalization task as in Sections 1.17.6.2.2
and 1.17.6.6.1, for various values of the parameter q. Note that the expansions of QKNLMS and QKAPSM
are identical in terms of the training point, as shown in (d). The parameters of each expansion, though,
are different.

1.17.7 A Convex Analytic Toolbox for Online Learning
967
of θ with the least contribution in the kernel series expansion of (17.102). All of these sparsiﬁcation
strategies which rely on the closed ball constraint exhibit a linear computational complexity with respect
to the number of elements of D [96]. The APSM, combined with a more involved sparsiﬁcation strategy
along the lines of the ALD strategy of the KRLS, can be found in [68]. Finally, inspired by the sparsiﬁ-
cation scheme of the QKLMS (see Algorithm 27.6.3), a quantized alternative to Algorithm 70 is given
by the QKAPSM in Algorithm 71. Here, the size of the dictionary D is determined by the quantization
parameter δ. Although this sparsiﬁcation strategy shows a computational complexity lower that the one
of Algorithm 70, it demonstrates a remarkable performance, as this can be seen by the numerical results
of Section 1.17.7.6.2.
Algorithm 70. The APSM for Example 67 with hyperslabs and the closed-ball sparsiﬁcation scheme
Require: The training data

(xn, yn)

n∈N∗, an RKHS H, an integer q > 1, a radius  > 0,
and an upper bound for the size of the dictionary M > q.
1: Set D := ∅, M′ := 0, and ∥f0∥= 0.
2: for n = 1, 2, . . . do
3:
Insert xn into D, i.e., set uM′+1 := xn, and D := {u1, . . . , uM′, uM′+1}.
4:
Increase the length of θ by one, i.e., let θ := (θ1, . . . , θM′, θM′+1)T , where θM′+1 := 0.
5:
Deﬁne J :=

max{1, M′ −q + 2}, . . . , M′ + 1

, of length q, at most.
6:
For every i ∈J , deﬁne
βi :=
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩

M′
j=1θ jκ(u j,ui)−yi−ϵ
κ(ui,ui)
, if 
M′
j=1θ jκ(u j, ui) > yi + ϵ,
0,
if −ϵ ≤
M′
j=1 θ jκ(u j, ui) −yi ≤ϵ,

M′
j=1θ jκ(u j,ui)−yi+ϵ
κ(ui,ui)
, if 
M′
j=1θ jκ(u j, ui) < yi −ϵ.
7:
Deﬁne the index set I fn−1 := {i ∈J : βi ̸= 0}.
8:
Choose the convex weights ω(n)
i
∈(0, 1], ∀i ∈I fn−1, such that 
i∈I fn−1 ω(n)
i
= 1.
9:
if 
i, j∈I fn−1 ω(n)
i
ω(n)
j βiβ jκ(ui, u j) ̸= 0, then
10:
Deﬁne Mn :=

i∈I fn−1 ω(n)
i
β2
i κ(ui,ui)

i, j∈I fn−1 ω(n)
i
ω(n)
j βiβ jκ(ui,u j).
11:
else
12:
Let Mn := 1.
13:
end if
14:
Choose any μn ∈(0, 2Mn).
15:
Deﬁne
ν :=
		 fn−1
		2 + μ2
n

i, j∈I fn−1
ω(n)
i
ω(n)
j βiβ jκ(ui, u j)
−2μn
M′

j=1

i∈I fn−1
ω(n)
i
βiθ jκ(ui, u j).

968
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
Algorithm 70. (Continued)
16:
For every i ∈I fn−1, deﬁne θi := θi −μnω(n)
i
βi.
17:
if √ν > , then
18:
For every i ∈{1, . . . , M′ + 1}, let θi := 
√ν θi.
19:
Deﬁne ∥fn∥:= .
20:
else
21:
Let ∥fn∥:= ν.
22:
end if
23:
if M′ ≥M, then
24:
Let ∥fn∥2 := ∥fn∥2 + θ2
1 κ(u1, u1) −2θ1

M′+1
i=1
θiκ(u1, ui).
25:
Let D := D \ {u1}, and θ := (θ2, . . . , θM′+1)T .
26:
For every i ∈{1, . . . , M′}, let ui := ui+1, and θi := θi+1.
27:
else
28:
M′ := M′ + 1.
29:
end if
Output: The dictionary D = {u1, . . . , uM′}, of length at most M, and the vector θ = (θ1, . . . , θM′)T ,
which produce the estimate fn = 
M′
i=1 θiκ(ui, ·).
30: end for
1.17.7.6.2
Simulation results
In order to validate the APSM of Example 67, we consider the same nonlinear channel equalization
scenario to the one used in Sections 1.17.6.2.2 and 1.17.6.6.1. More speciﬁcally, the nonlinear channel
consists of a linear ﬁlter tn = −0.8 · yn + 0.7 · yn−1 −0.6 · yn−2 + 0.4 · yn−3, and a memoryless
nonlinearity qn = tn +0.08·t2
n, ∀n ∈N∗. The parameters for the QKNLMS and the KRLS are identical
to the ones used in Sections 1.17.6.2.2 and 1.17.6.6.1. As for the APSM, its QKAPSM version, i.e.,
Algorithm 71, was employed. For the present scenario, the following parameters were used for the
APSM; q : = 2, 4, 8, 16, μn = 0.5, ϵ : = 10−10, ∀n ∈N∗, while the parameter for the sparsiﬁcation
procedure was set to δ = 6, for both APSM and QKNLMS (See ﬁg. 17.32).
Notice that the APSM achieves superior performance, when compared to the QKNLMS, while both
achieve the same expansion size. This is a natural consequence of the ability of the APSM to activate
data reuse or concurrent processing, by letting the parameter q to take values larger than 1. Recall, by
Example 68, that the KNLMS is a byproduct of the APSM for q = 1. In general, the more the q is
increased, the faster the convergence of the KAPSM becomes. The ﬂexibility offered to the APSM by
the theory of convex analysis pays off, also, in the case where the APSM is compared to the classical
APA, or KAPA (see Example 68). Even if both APSM and APA employ data reuse, the way the APSM
is derived (Example 67) precludes the system from the calculation of a matrix inversion, as is the case
for the APA where the computation of a pseudoinverse is necessary. This becomes beneﬁcial in cases
where algorithmic stability is the issue. Notice also that the misadjustment level of the APSM is inferior

1.17.7 A Convex Analytic Toolbox for Online Learning
969
to the KRLS. Recall, here, that the computational complexity of the KRLS is O(M2
n), as opposed to the
O(qMn), where Mn is the size of the dictionary at the time instant n. Recall, also, that the computational
complexity of the QKNLMS is O(Mn).
More on the validation of the APSM, for several learning tasks, can be found in [41–43,
95,97,98,106,107,112]. The code for the experiments presented in this section can be found in
<http://bouboulis.mysch.gr/kernels.html>.
Algorithm 71. Quantized Kernel APSM (QKAPSM) for Example 67 with hyperslabs
Require: The training data

(xn, yn)

n∈N∗, an RKHS H, an integer q > 1, and the quantization parameter δ >0.
1: Set D := ∅, and M := 0.
2: for n = 1, 2, . . ., do
3:
Compute the distance of xn from D:
d(xn, D) = infui∈D∥xn −ui∥= ∥xn −ul0∥, for some l0 ∈{1, . . . , M}.
In the case where there are multiple such l0s, then choose the largest one.
To leave no place for ambiguities, let also d(xn, ∅) := +∞.
4:
if d(xn, D) > δ, then
5:
Insert xn into D, i.e., set uM+1 = xn, and D = {u1, . . . , uM, uM+1}.
6:
Increase the length of θ by one, i.e., let θ := (θ1, . . . , θM, θM+1)T , where θM+1 := 0.
7:
Deﬁne J := {max{1, M −q + 2}, . . . , M + 1}.
8:
else
9:
if l0 ≥max{1, M −q + 1}, then
10:
J := {max{1, M −q + 1}, . . . , M}.
11:
else
12:
J := {l0, M −q + 2, . . . , M}.
13:
end if
14:
end if
15:
For every i ∈J , deﬁne
βi :=
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩

M
j=1θ jκ(u j,ui)−yi−ϵ
κ(ui,ui)
, if 
M
j=1θ jκ(u j, ui) > yi + ϵ,
0,
if −ϵ ≤
M
j=1θ jκ(u j, ui) −yi ≤ϵ,

M
j=1θ jκ(u j,ui)−yi+ϵ
κ(ui,ui)
, if 
M
j=1θ jκ(u j, ui) < yi −ϵ.
16:
Deﬁne the index set I fn−1 := {i ∈J : βi ̸= 0}.
17:
Choose the convex weights ω(n)
i
∈(0, 1], ∀i ∈I fn−1, such that 
i∈I fn−1 ω(n)
i
= 1.
18:
if 
i, j∈I fn−1 ω(n)
i
ω(n)
j βiβ jκ(ui, u j) ̸= 0, then
19:
Deﬁne Mn :=

i∈I fn−1 ω(n)
i
β2
i κ(ui,ui)

i, j∈I fn−1 ω(n)
i
ω(n)
j βiβ jκ(ui,u j).
20:
else

970
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
Algorithm 71. (Continued)
21:
Let Mn := 1.
22:
end if
23:
Choose any μn ∈(0, 2Mn).
24:
For every i ∈I fn−1, deﬁne θi := θi −μnω(n)
i
βi.
25:
if d(xn, D) > δ, then
26:
M := M + 1.
27:
end if
Output: The dictionary D = {u1, . . . , uM}, and the real-valued vector θ = (θ1, . . . , θM)T , which produce
the estimate fn = 
M
i=1 θiκ(ui, ·).
28: end for
1.17.8 Related work and applications
The recursions (17.86), (17.90), (17.94), and (17.96), met in the previous section, are the basis of
a plethora of algorithmic solutions for signal processing [5,6,27,41–43,45,68,94,95,97,98,106–113]
and machine learning [57–61,77,78,114–128], and, thus, they are the engine behind numerous applica-
tions which span from echo cancellation, equalization of linear/nonlinear channels, image processing,
and wireless sensor networks, to pattern recognition and classiﬁcation, social network analysis, bioin-
formatics, and sparsity-aware learning. This section aims to shedding light on the connection of the
convex analytic framework of Section 1.17.7 with several recent works on online learning. We focus
mainly on machine learning techniques, since their connection with the framework of Section 1.17.7 is
not as much illuminated as is the signal processing side of learning [95,129,130].
The recursion (17.96) is the basic ingredient hidden behind the NORMA method of [122]. In [122],
the sequence of closed convex sets (Cn)n∈N∗is not imposed into the design, i.e., the unconstrained case,
and the loss function Ln : = n + λ ∥·∥2 , ∀n ∈N∗, where λ > 0 is a regularization parameter. This
form of loss functions originates from the very successful Support Vector Machines (SVM) rationale. By
assuming Lipschitz continuity for all the loss functions (n)n∈N∗, a convergence analysis is conducted
in [122] in order to derive error bounds whenever the learning step sizes (μn)n∈N∗follow the diminishing
rule of μn ∝
1
√n , ∀n ∈N∗. The SVM rationale and the recursion (17.96) is also hidden behind [126].
The Lipschitz continuity assumption of [122] on (n)n∈N∗is bypassed, the sequence (Cn)n∈N∗is ﬁxed
to a single closed convex set, e.g., a closed ball, and the convergence analysis for the error bounds is con-
ducted on the assumption of (Ln)n∈N∗being a sequence of strongly convex functions (see Deﬁnition 33),
with μn ∝1
n , ∀n ∈N∗. Notice, here, that the rules for the learning step parameters (μn)n∈N∗, adopted
in both [122,126], are very closely related to the discussion on (17.92), and the classical method of [84].
A study on the stability of the basic recursion (17.96), under a stochastic framework, in the case where
the loss functions (Ln)n∈N∗are differentiable, is given in the very recent work of [125]. Moreover, by

1.17.9 Conclusions
971
following the rationale of proximal mappings (see Example 47), as well as the strategies of the forward-
backward and the Douglas-Rachford algorithms [65,72,73,75,76] (see the discussion below (17.85)),
generalizations of (17.96) in the online learning context can be found in [77,78,110,113,124].
The classical LMS recursion (17.34) is studied through the machine learning magnifying glass in
[118]. Inner products, in properly deﬁned inner product spaces, are used in order to model estimates.
The gradient descent algorithm of [118] is utilized to derive several worst-case bounds of the total
sum of the squared prediction errors. A normalized version of the gradient descent algorithm, which is
nothing but the direct analogue of the NLMS method (17.37) to general inner product spaces, appears
and is analyzed also in [118]. The interaction of the gradient descent rationale with noisy environments
was very recently studied in [117], where, in particular, several theoretical results were stated regarding
the question of to what extent independently perturbed copies of each training datum assist the gradient
descent algorithm in its online learning task.
An algorithm of the same spirit to the KNLMS (see Section 1.17.6.2) is studied in [119]. The
kernel series expansion estimate is updated every time instant by projecting the current estimate onto
a single halfspace, whenever a classiﬁcation problem is considered, or a hyperslab (see Example 38),
whenever a regression task is studied. These solution sets are deﬁned by the newly arrived training
data as the set of all minimizers of properly deﬁned hinge loss functions. Variants of this scheme are
also studied in [119] by means of a parameter which controls how much aggressive the projection
onto the halfspace should be. A generalization of [119], towards using multiple halfspaces at each time
instant, similarly to the strategy of Examples 52, 67, and 68, is given in [114]. However, the study of
[114] follows a different approach than the one in Examples 52, 67, and 68, by transferring the original
problem to the dual domain, by approximating it via a multiple number of simpler problems, and by
elaborating inner iterations, at each step of the algorithm, in order to calculate various optimal step
parameters.
1.17.9 Conclusions
This manuscript presented an overview of online learning techniques; both classical and modern. To
serve the needs of both the signal processing and machine learning communities, this work has focused
on bringing forth the basic principles of a fundamental machine learning tool, the Reproducing Ker-
nel Hilbert Spaces, and on highlighting a theoretical device, mostly known to the signal processing
community, i.e., the elegant and geometrically rich convex analytic side of operators/mappings in
general Hilbert spaces. Despite its tutorial nature, the manuscript does not spare mathematical rigor;
proofs are given in detail in places where we feel that the way to use the previous mathematical
tools should be demonstrated. We have also tried to provide with a framework that unites classical
and modern stuff; numerous classical online learning techniques stem as special cases of the intro-
duced mathematical framework. Numerical examples have been also given in order to exemplify
the common attributes which run along the spine of online learning tasks in both signal processing
and machine learning principles, i.e., the nonlinear estimation theory and the classiﬁcation/regression
framework.

972
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
Appendices
A Key properties of the gaussian kernel
As the Gaussian kernel is the most widely used in applications, we dedicate this section to present some
of its most important properties. We begin our study showing that the Gaussian radial basis function is
indeed a reproducing kernel. To this end, we introduce some new notions.
Deﬁnition A.1 (Negative Deﬁnite Kernel).
Let X be a set. A function κ : X × X →R is called a
negative deﬁnite kernel if it is symmetric, i.e., κ(y, x) = κ(x, y), and
N

n,m=1
cncmκ(xn, xm) ≤0,
for any x1, . . . , xN ∈X, c1, . . . , cN ∈R, with 
N
n=1 cn = 0, and for all N ∈N∗.
♦
Examples of negative kernels are the constant functions and all functions of the form −κ, where κ
is a positive deﬁnite kernel. Furthermore, the following proposition holds:
Proposition A.2.
Let X be a non empty set, the functions ψk : X × X →R be negative kernels and
αk > 0, for k ∈N∗. Then
•
Any positive combination of a ﬁnite number of negative kernels is also a negative kernel, i.e.,
ψ = 
k αkψk, with α1, . . . , αn > 0 is a negative kernel.
•
The limit of any converging sequence of negative kernels is also a negative kernel, i.e. if ψ(x, y) =
limk ψk(x, y), for all x, y ∈X, then ψ is a negative kernel.
♦
Proof.
For the ﬁrst part, consider the numbers c1, . . . , cN such that 
N
n=1 cn = 0, x1, . . . , xN ∈X
and M ∈N∗. Then
N

n,m=1
cncm
M

k=1
αkψk(xn, xm) =
M

k=1
αk
N

n,m=1
cncmψk(xn, xm) ≤0.
Finally, to prove the second part we take:
N

n,m=1
cncmψ(xn, xm) =
N

n,m=1
cncm lim
k ψk(xn, xm) = lim
k
N

n,m=1
cncmψk(xn, xm) ≤0.
□
Lemma A.3.
Let X be a nonempty set, V be a vector space equipped with an inner product and
T : X →V . Then the function
ψ(x, y) = ∥T (x) −T (y)∥2
V ,
is a negative deﬁnite kernel on X.
♦

A Key Properties of the Gaussian Kernel
973
Proof.
Consider the numbers c1, . . . , cN such that 
N
n=1 cn = 0 and x1, . . . , xN ∈X. Then
N

n,m=1
cncm∥T (xn) −T (xm)∥2
V =
N

n,m=1
cncm⟨T (xn) −T (xm), T (xn) −T (xm⟩V
=
N

n,m=1
cncm

∥T (xn)∥2
V + ∥T (xm)∥2
V −⟨T (xn), T (xm)⟩V
−⟨T (xm), T (xn)⟩V

=
N

m=1
cm
N

n=1
cn∥T (xn)∥2
V +
N

n=1
cn
N

m=1
cm∥T (xm)∥2
V
−
 N

n=1
cnT (xn),
N

m=1
cmT (xm)

V
−
 N

m=1
cmT (xm),
N

n=1
cnT (xn)

V
.
As 
N
n=1 cn = 0, the ﬁrst two terms of the summation vanish and we take:
N

n,m=1
cncm∥T (xn) −T (xm)∥2
V = −2
					
N

n=1
cnT (xn)
					
2
V
≤0.
Thus ψ is a negative deﬁnite kernel.
□
Lemma A.4.
Let ψ : X × X →R be a function. Fix x0 ∈X and deﬁne
κ(x, y) = −ψ(x, y) + ψ(x, x0) + ψ(x0, y) −ψ(x0, x0).
Then ψ is a negative deﬁnite kernel if and only if κ is a positive deﬁnite kernel.
♦
Proof.
Let x1, . . . , xN ∈X. For the if part, consider the numbers c1, . . . , cN such that 
N
n=1 cn = 0.
Then
N

n,m=1
cncmκ(xn, ym) = −
N

n,m=1
cncmψ(xn, xm) +
N

n,m=1
cncmψ(xn, x0)
+
N

n,m=1
cncmψ(x0, xm) −
N

n,m=1
cncmψ(x0, x0)
= −
N

n,m=1
cncmψ(xn, xm) +
N

n
cm
N

n
cnψ(xn, x0)
+
N

n=1
cn
N

m=1
cmψ(x0, xm) −
N

m=1
cm
N

m=1
cnψ(x0, x0).

974
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
As 
N
n=1 cn = 0 and 
N
n,m=1 cncmκ(xn, ym) ≥0, we take that 
N
n,m=1 cncmψ(xn, ym) ≤0. Thus ψ
is a negative deﬁnite kernel.
For the converse, take c1, . . . , cN ∈R and deﬁne c0 = −
N
n=1 cn. By this simple trick, we generate
the numbers c0, c1, . . . , cN ∈R, which have the property 
N
n=0 cn = c0 + 
N
n=1 cn = 0. As ψ is a
negative deﬁnite kernel, we take that 
N
n,m=0 cncmψ(xn, xm) ≤0, for any x0 ∈X. Thus,
N

n,m=0
cncmψ(xn, xm) =
N

n,m=1
cncmψ(xn, xm) +
N

m=1
c0cmψ(x0, xm)
+
N

n=1
c0cnψ(xn, x0) + c2
0ψ(x0, x0)
=
N

n,m=1
cncmψ(xn, xm) −
N

n,m=1
cncmψ(x0, xm)
−
N

n,m=1
cncmψ(xn, x0) +
N

n,m=1
cncmψ(x0, x0)
=
N

n,m=1
cncm

ψ(xn, xm) −ψ(x0, xm) −ψ(xn, x0) + ψ(x0, x0)

= −
N

n,m=1
cncmκ(xn, xm).
Thus 
N
n,m=1 cncmκ(xn, xm) ≥0 and κ is a positive deﬁnite kernel.
□
Theorem A.5 (Schoenberg).
Let X be a nonempty set and ψ : X × X →R. The function ψ is a
negative kernel if and only if exp ( −tψ) is a positive deﬁnite kernel for all t ≥0.
♦
Proof.
For the if part, recall that
ψ(x, y) = lim
t→0
1 −exp ( −tψ(x, y))
t
.
As exp ( −tψ) is positive deﬁnite, −exp ( −tψ) is negative deﬁnite and the result follows from
Proposition A.2. It sufﬁces to prove the converse for t = 1, as if ψ is a negative deﬁnite kernel so is tψ,
for any t ≥0. Take x0 ∈X and deﬁne the positive deﬁnite kernel κ(x, y) = −ψ(x, y) + ψ(x, x0) +
ψ(x0, y) −ψ(x0, x0) (Lemma A.4). Then
e−ψ(x,y) = e−ψ(x,x0)eκ(x,y)e−ψ(x0,y)eκ(x0,x0).
Let f (x) = e−ψ(x,x0). Then, as ψ is a negative kernel and therefore symmetric, one can readily prove
that the last relation can be rewritten as
e−ψ(x,y) = eκ(x0,x0) · f (x)eκ(x,y) f (y).

A Key Properties of the Gaussian Kernel
975
Since eκ(x0,x0) is a positive number, employing the properties of positive kernels given in
Section 1.17.5.5, we conclude that e−ψ(x,y) is a positive deﬁnite kernel.
□
Corollary A.6.
The Gaussian radial basis function is a reproducing kernel.
Although all properties of positive kernels do not apply to negative kernels as well (for example
the product of negative kernels is not a negative kernel), there are some other operations that preserve
negativity.
Proposition A.7.
Let ψ : X × X →R be negative deﬁnite. In this case:
1. If ψ(x, x) ≥0, for all x ∈X, then ψ p(x, y) is negative deﬁnite for any 0 < p ≤1.
2. If ψ(x, x) ≥0, for all x ∈X, then log (1 + ψ(x, y)) is negative deﬁnite.
3. If ψ : X × X →(0, +∞), then log ψ(y, x) is negative deﬁnite.
♦
Proof.
We give a brief description of the proofs.
1. We use the formula:
ψ(x, y)p =
p
(1 −p)
 ∞
0
t−p−1 
1 −e−tψ(x,y)
dt,
where the Gamma function is given by (z) =
 ∞
0
e−ttz dt. As e−tψ(x,y) is positive deﬁnite
(Theorem A.5) and and t−p−1 is a positive number, it is not difﬁcult to prove that the expression
inside the integral is negative deﬁnite for all t > 0.
2. Similarly, we use the formula:
log (1 + ψ(x, y)) =
 ∞
0
e−t
t

1 −e−tψ(x,y)
dt.
3. For any c > 0, log (ψ(x, y) + 1/c) = log (1 + cψ(x, y)) −log (c). We can prove that the second
part is negative deﬁnite. Then, by taking the limit as c →0, one completes the proof.
□
As a direct consequence, one can prove that since ∥x −y∥2 is a negative kernel, so is ∥x −y∥2p,
for any 0 < p ≤1. Thus, for any 0 < p ≤2, ∥x −y∥p is a negative kernel and exp ( −t∥x −y∥p)
is a positive kernel for any any t > 0. Therefore, for p = 2 we take another proof of the positivity of
the Gaussian radial basis function. In addition, for p = 1 one concludes that the Laplacian radial basis
function is also a positive kernel. Moreover, for the Gaussian kernel the following important property
has been proved.
Theorem A.8 (Full rank of the Gaussian RBF Gram matrices).
Suppose that x1, . . . , xN ⊂X are
distinct points and σ ̸= 0. The Gram matrix given by K = (Kn,m), where
Kn,m = exp

−∥xn −xm∥2
2σ 2

,
has full rank.
♦

976
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
As a consequence, for any choice of discrete points x1, . . . , xN, we have that 
N
m=1 amκ(xn, xm) =
0, for all n = 1, 2 . . . , N, if and only if a1 = . . . = aN = 0. However, observe that for any a1, . . . , aN
N

m=1
amκ(xn, xm) =
N

m=1
am⟨κ(·, xm), κ(·, xn)⟩H =
 N

m=1
amκ(·, xm), κ(·, xn)

H
= ⟨f , κ(·, xn)⟩H,
where f = 
N
m=1 amκ(·, xm) ∈span{κ(·, xm), m = 1, . . . , N}. In addition, if for an f ∈span{κ
(·, xm), m = 1, . . . , N} we have that ⟨f , κ(·, xn)⟩H = 0 for all n = 1, . . . , N, if and only if f = 0.
Hence, if f is orthogonal to all (xn), then f = 0. We conclude that f = 
N
m=1 amκ(·, xm) = 0 if
and only if a1 = . . . = aN = 0. Therefore, the points (xm) = κ(·, xm), m = 1, . . . , N, are linearly
independent, provided that no two xm are the same. Hence, a Gaussian kernel deﬁned on a domain of
inﬁnite cardinality, produces a feature space of inﬁnite dimension. Moreover, the Gram matrices deﬁned
by Gaussian kernels are always strictly positive deﬁnite and invertible.
In addition, for every x, y ∈X we have that κ(x, x) = 1 and κ(x, y) ≥0. This means that all x ∈X
are mapped through the feature map  to points lying in the surface of the unit sphere of the RKHS H
and that the angle between any two mapped points (x) and (y) is between 0
◦and 90
◦degrees. We
conclude this section with the following two important formulas, which hold for the case of the RKHS
induced by the Gaussian kernel. For the norm of f ∈H, one can prove that:
∥f ∥2
H =

X

n
σ 2n
n!2n (On f (x))2dx,
(17.103)
with O2n = n and O2n+1 = ∇n,  being the Laplacian and ∇the gradient operator. The implication
ofthisisthataregularizationtermoftheform∥f ∥2
H (whichisusuallyadoptedinpractice)"penalizes"the
derivatives of the minimizer. This results to a very smooth solution of the regularized risk minimization
problem. Finally, the Fourier transform of the Gaussian kernel κσ is given by
F[κ](ω) = |σ| exp

−σ 2ω2
2

.
(17.104)
B Proof of Proposition 60
First, let us deﬁne the stage of discussion. For the sake of convenience, we bind fs and βs together. To
this end, deﬁne the Cartesian product
H := H × R := {( f , β) : f ∈H, β ∈R} .
To save space, let us denote every ( f , β) ∈H by f. Further, to equip H with an inner product, deﬁne
∀f1 := ( f , β), ∀f2 := (g, γ ) ∈H,
⟨f1, f2⟩1H = ⟨( f , β), (g, γ )⟩H := ⟨f , g⟩+ βγ.

B Proof of Proposition 60
977
(ξ ) +
ξ
0
FIGURE 17.33
The positive part of a real number is not a differentiable function.
Under this deﬁnition, the task (17.93) obtains the following equivalent formulation:
ﬁnd f ∈H such that
5 
f, kxi

H ≥0,
∀i ∈{1, 2, . . . , N1},
7
f, kξ j
8
H ≤0,
∀j ∈{1, 2, . . . , N2},
(17.105)
where we have deﬁned kxi :=

κ(xi, ·), 1

∈H and kξi :=

κ(ξi, ·), 1

∈H.
Let us develop this formulation a little further. Given a parameter vector kz ∈H, deﬁne the following
closed convex set, namely, the closed halfspace:
H−
kz :=

f ∈H : ⟨f, kz⟩H ≤0

.
Under this setting, (17.105) takes the following equivalent form:
ﬁnd f ∈
 N1
.
i=1
H−
−kxi

∩
⎛
⎝
N2
.
j=1
H−
kξ j
⎞
⎠.
(17.106)
Now, let us deﬁne our perceptron loss/penalty function L : H →R as follows:
L(f) :=
N1

i=1

f, −kxi

H

+ +
N2

j=1
7
f, kξ j
8
H

+ ,
(17.107)
where the function ( · )+ stands for the positive part of a real number; see Figure 17.33.
The reasoning behind the deﬁnition of (17.107) is based on the following rationale. Since our goal
is to compute an f which satisﬁes (17.106), we have to penalize somehow those f that do not. To this
end, and without any loss of generality, assume an f and an i0 such that f /∈H−
−kxi0
. By the deﬁnition
of H−
−kxi0
, we have
7
f, −kxi0
8
H > 0. In such a way, the loss function L scores a penalty, and the
term
7
f, −kxi0
8
H appears in (17.107). Moreover, the more f lies further from H−
−kxi0
, i.e., the larger the
quantity
7
f, −kxi0
8
H is, the larger the penalty becomes. On the contrary, in the case where f ∈H−
−kxi0
,
i.e.,
7
f, −kxi0
8
H ≤0, there is no penalty in (17.107) with respect to H−
−kxi0
.

978
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
It can be easily seen by Figure 17.33 that the function l( · ) := ( · )+ is convex, with the following
subdifferential:
∂l(θ) =
⎧
⎨
⎩
{0},
if θ < 0,
[0, 1], if θ = 0,
{1},
if θ > 0.
(17.108)
If we also deﬁne the linear mapping Akz : H →R : f 	→Akz(f) := ⟨f, kz⟩H, where kz is a parameter
vector, then we can readily verify that (17.107) can be given by the following equivalent form:
L(f) :=
N1

i=1
l

A−kxi (f)

+
N2

j=1
l

Akξ j (f)

,
i.e., L is the sum of a number of compositions of the convex function l( · ) with the linear mappings
A−kxi , Akξ j ; hence it is convex. An inspection of (17.107) shows that
lev≤0(L) =
 N1
.
i=1
H−
−kxi

∩
⎛
⎝
N2
.
j=1
H−
kξ j
⎞
⎠.
In the sequel, we will show that Algorithm 57 solves the task (17.93), when the function L is chosen as
previously. To this end, let us recall here the deﬁnition of the interior of a set; a point g ∈int

lev≤0(L)

iff there exists a positive δ > 0 such that the open ball centered at g with a radius δ belongs to lev≤0(L),
i.e., B(g, δ) : = {f ∈H : ∥g −f∥< δ} ⊂lev≤0(L). Since the boundaries of the closed halfspaces
H−
−kxi and H−
ky j are all those f such that

f, −kxi

= 0 and
7
f, ky j
8
= 0, respectively, the assumption of
Proposition 60 is equivalent to
int

lev≤0(L)

̸= ∅.
(17.109)
Based on the deﬁnition of L, we also have that
L(f) =

i: f/∈H−
−kxi

f, −kxi

H +

j: f/∈H−
ky j
7
f, ky j
8
H ,
∀f /∈lev≤0(L).
Clearly, in such a case, the function is differentiable:
L′(f) = −

i: f/∈H−
−kxi
kxi +

j: f/∈H−
ky j
ky j ,
∀f /∈lev≤0(L).
(17.110)
Hence,
		L′(f)
		
H ≤

i: f/∈H−
−kxi
		kxi
		
H +

j: f/∈H−
ky j
			ky j
			
H
≤
N1

i=1
		kxi
		
H +
N2

j=1
		ky j
		
H := K,
∀f /∈lev≤0(L).
(17.111)

B Proof of Proposition 60
979
Let us apply, now, the PSMa Algorithm 57 to the loss function of (17.107). Notice that in this case, the
minimization task is unconstrained, i.e., C : = H. In what follows, we assume that fn−1 /∈lev≤0(L),
since otherwise, the PSMa Algorithm 57 stalls to a point which is the solution of (17.106). Under the
previous setting, for any initial point f0, for any f∗∈int

lev≤0(L)

, and for any iteration index n, we
have:
∥fn −f∗∥2
H =
		fn−1 −μnL′(fn−1) −f∗
		2
H
= ∥fn−1 −f∗∥2
H + μ2
n
		L′(fn−1)
		2
H −2μn

fn−1 −f∗, L′(fn−1)

H
= ∥fn−1 −f∗∥2
H + μ2
n
		L′(fn−1)
		2
H −2μn

fn−1, L′(fn−1)

H
+2μn

f∗, L′(fn−1)

H .
Notice, now, by (17.110) and the fact that fn−1 /∈lev≤0(L) that

fn−1, L′(fn−1)

H =

i: fn−1 /∈H−
−kxi

fn−1, −kxi

H +

j: fn−1 /∈H−
kξ j
7
fn−1, kξ j
8
H ,
(17.112a)

f∗, L′(fn−1)

H =

i: fn−1 /∈H−
−kxi

f∗, −kxi

H +

j: fn−1 /∈H−
kξ j
7
f∗, kξ j
8
H .
(17.112b)
It can be readily veriﬁed by (17.112a) that

fn−1, L′(fn−1)

H > 0, so that
∥fn −f∗∥2
H ≤∥fn−1 −f∗∥2
H + μ2
n
		L′(fn−1)
		2
H + 2μn

f∗, L′(fn−1)

H .
Now, let us focus on the term

f∗, L′(fn−1)

H. Due to assumption (17.109), there exists an ϵ > 0 such
that
∀i = 1, 2, . . . , N1,
⟨f∗, −kxi ⟩H ≤−ϵ < 0,
∀j = 1, 2, . . . , N2,
⟨f∗, kξ j ⟩H ≤−ϵ < 0.
It becomes clear, then, by (17.112b) that

f∗, L′(fn−1)

H ≤−ϵ < 0.
For this reason, we end up in
∥fn −f∗∥2
H ≤∥fn−1 −f∗∥2
H + μ2
n
		L′(fn−1)
		2
H −2ϵμn
= ∥fn−1 −f∗∥2
H +
μ2
n
		L′(fn−1)
		2
H
max2 
1, ∥L′(fn−1)∥H
 −
2μnϵ
max

1, ∥L′(fn−1)∥H

≤∥fn−1 −f∗∥2
H + μ2
n
		L′(fn−1)
		2
H
∥L′(fn−1)∥2
H
−
2μnϵ
max{1, K}
≤∥fn−1 −f∗∥2
H + μ2
n −
2μnϵ
max{1, K},
(17.113)

980
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
where we used the deﬁnition of μn and the quantity K introduced in (17.111). A repetition of the
telescoping (17.113) gives the following:
∥fn −f∗∥2
H ≤∥f0 −f∗∥2
H +
n

k=1
μ2
k −
2ϵ
max{1, K}
n

k=1
μk
≤∥f0 −f∗∥2
H +
∞

n=1
μ2
n −
2ϵ
max{1, K}
n

k=1
μk.
(17.114)
Since 
∞
n=1 μn = ∞, there exists an n0 ∈N∗such that ∀n ≥n0,
n

k=1
μk ≥max{1, K}
2ϵ

∥f0 −f∗∥2
H +
∞

n=1
μ2
n

.
This result and (17.114) imply that ∀n ≥n0,
0 ≤∥fn −f∗∥2
H ≤0,
i.e., ∀n ≥n0, fn = f∗. This completes the proof.
C Proof of convergence for algorithm 61
Since the set of all minimizers, CL∗, of (17.84) is nonempty, let us choose any f∗∈CL∗. Then, if we
let νn := μn L( fn−1)−L∗
∥L′( fn−1)∥2 , we observe that
∥fn −f∗∥2 =
		PC

fn−1 −νnL′( fn−1)

−PC( f∗)
		2 ≤
		 fn−1 −f∗−νnL′( fn−1)
		2
= ∥fn−1 −f∗∥2 + ν2
n
		L′( fn−1)
		2 −2νn

fn−1 −f∗, L′( fn−1)

≤∥fn−1 −f∗∥2 + ν2
n
		L′( fn−1)
		2 −2νn

L( fn−1) −L( f∗)

= ∥fn−1 −f∗∥2 + μ2
n

L( fn−1) −L∗
2
∥L′( fn−1)∥2
−2μn

L( fn−1) −L∗
2
∥L′( fn−1)∥2
= ∥fn−1 −f∗∥2 −μn(2 −μn)

L( fn−1) −L∗
2
∥L′( fn−1)∥2
≤∥fn−1 −f∗∥2 −ϵ2

L( fn−1) −L∗
2
∥L′( fn−1)∥2
,
or in other words,
ϵ2

L( fn−1) −L∗
2
∥L′( fn−1)∥2
≤∥fn−1 −f∗∥2 −∥fn −f∗∥2 .
(17.115)

C Proof of Convergence for Algorithm 61
981
Hence, ∥fn −f∗∥≤∥fn−1 −f∗∥, which implies that the limn→∞∥fn −f∗∥2 exists. Since the previ-
ous analysis was conducted for any point f∗∈CL∗, we obtain that given any f∗∈CL∗, there exists a
δ f∗≥0 such that
lim
n→∞∥fn −f∗∥= δ f∗.
(17.116)
Due to the fact that
∥fn −f∗∥2
n∈N is convergent, it is also a Cauchy sequence, which suggests, in
turn, that
lim
n→∞

∥fn−1 −f∗∥2 −∥fn −f∗∥2
= 0.
(17.117)
Moreover, due to the monotonicity of the sequence
∥fn −f∗∥2
n∈N , we observe that ∥fn −f∗∥≤
∥f0 −f∗∥, or in other words, fn ∈B
 
f∗, ∥f0 −f∗∥!
∩C, and hence
		L′( fn)
		 ≤c, ∀n. Based on this
observation, we have by (17.115) that
ϵ2
c2

L( fn−1) −L∗
2 ≤∥fn−1 −f∗∥2 −∥fn −f∗∥2 .
Hence, by (17.117), limn→∞L( fn) = L∗= min f ∈C L( f ).
Since ( ∥fn −f∗∥)n∈N is bounded, so is ( fn)n∈N . Thus, ( fn)n∈N is weakly compact [65], and
necessarily possesses at least one sequentially weak cluster point. Moreover, ( fn)n∈N lies in C, which
is closed and convex, and thus weakly closed, which implies that the cluster point ˜f∗belongs to C.
Since L is weakly lower semicontinuous [65], we ﬁnally obtain that
L∗≤L( ˜f∗) ≤lim inf
k→∞L( fnk) = lim
k→∞L( fnk) = lim
n→∞L( fn) = L∗,
i.e., ˜f∗∈CL∗.
Let us show, now, that the sequence ( fn)n∈N converges weakly to a point in CL∗. We have already
shown previously that the set of sequentially weak cluster points W

( fn)n∈N

of ( fn)n∈N is non-empty,
and that W

( fn)n∈N

⊂CL∗. To derive a contradiction, let two points ˜f∗1, ˜f∗2 ∈W

( fn)n∈N

such
that ˜f∗1 ̸=
˜f∗2.Sincebothof ˜f∗1, ˜f∗2 aresequentiallyweakclusterpoints,thereexisttwosubsequences
( fnk)k∈N and ( fmk)k∈N such that fnk ⇀˜f∗1 and fmk ⇀˜f∗2. By (17.116),
lim
n→∞
			 fn −˜f∗1
			 = δ ˜f∗1,
lim
n→∞
			 fn −˜f∗2
			 = δ ˜f∗2.
Without any loss of generality, assume that δ ˜f∗1 ≤δ ˜f∗2. Then, notice that
δ2
˜f∗2 = lim
n→∞
			 fn −˜f∗2
			
2
= lim
k→∞
			 fmk −˜f∗2
			
2
= lim
k→∞
			 fmk −˜f∗1 + ˜f∗1 −˜f∗2
			
2
= lim
k→∞
			 fmk −˜f∗1
			
2
+
			 ˜f∗1 −˜f∗2
			
2
+ 2
7
fmk −˜f∗1, ˜f∗1 −˜f∗2
8
= δ2
˜f∗1 +
			 ˜f∗1 −˜f∗2
			
2
+ 2
7
˜f∗2 −˜f∗1, ˜f∗1 −˜f∗2
8
= δ2
˜f∗1 +
			 ˜f∗1 −˜f∗2
			
2
−2
			 ˜f∗1 −˜f∗2
			
2
= δ2
˜f∗1 −
			 ˜f∗1 −˜f∗2
			
2
< δ2
˜f∗1 ≤δ2
˜f∗2,
which is evidently a contradiction. Hence, there is only one sequentially weak cluster point of ( fn)n∈N ,
which implies that ( fn)n∈N converges weakly to that point. This completes the proof.

982
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
Relevant Theory: Signal Processing Theory
See this Volume, Chapter 12 Adaptive Filters
References
[1] C.M. Bishop, Pattern Recognition and Machine Learning, corr. second printing ed., Springer, 2007.
[2] S. Theodoridis, K. Koutroumbas, Pattern Recognition, fourth ed., Academic Press, 2009.
[3] N. Aronszajn, Theory of reproducing kernels, Trans. Am. Math. Soc. 68 (3) (1950) 337–404.
[4] P.S.R. Diniz, Adaptive Filtering: Algorithms and Practical Implementation, third ed., Springer, 2008.
[5] S. Haykin, Adaptive Filter Theory, third ed., Prentice-Hall, New Jersey, 1996.
[6] A.H. Sayed, Fundamentals of Adaptive Filtering, John Wiley & Sons, New Jersey, 2003.
[7] V.N. Vapnik, The Nature of Statistical Learning Theory, second ed., Springer, 2000.
[8] T. Hastie, R. Tibshirani, J. Friedman, The Elements of Statistical Learning: Data Mining, Inference, and
Prediction, second ed., Springer, 2009.
[9] E.J. Candès, M.B. Wakin, An introduction to compressive sampling, IEEE Signal Process. Mag. 25 (2)
(2008) 21–30.
[10] J.M. Lee, Introduction to Smooth Manifolds, Springer, 2003.
[11] M.A. Aizerman, E.M. Braverman, L.I. Rozonoer, Theoretical foundations of the potential function method
in pattern recognition learning, Autom. Remote Control 25 (1964) 821–837.
[12] B. Boser, I. Guyon, V. Vapnik, A training algorithm for optimal margin classiﬁers, in: Fifth Annual Workshop
on Computational Learning Theory, Pittsburgh, 1992, pp. 144–152.
[13] B. Schölkopf, A.J. Smola, Learning with Kernels, MIT Press, Cambridge, MA, 2001.
[14] J. Mercer, Functions of positive and negative type and their connection with the theory of integral equation,
Philos. Trans. Roy. Soc. Lond. 209 (1909) 415–446.
[15] J. Mercer, Sturm-Liuville series of normal functions in the theory of integral equations, Philos. Trans. Roy.
Soc. Lond. Ser. A 211 (1911) 111–198.
[16] E.H. Moore, On properly positive hermitian matrices, Bull. Am. Math. Soc. 23 (1916) 59.
[17] E.H. Moore, General analysis, Part I, Memoirs of the American Philosophical Society, 1935.
[18] E.H. Moore, General analysis, Part II, Memoirs of the American Philosophical Society, 1939.
[19] S. Bochner, Vorlesungen Ueber Fouriersche Integrale, 1932.
[20] S. Bochner, Hilbert distances and positive deﬁnite functions, Ann. Math. 42 (1941) 647–656.
[21] S. Zaremba, L’ equation biharmonique et une classe remarquable de fonctions fondamentales harmoniques,
Bulletin international de l’ Academie des sciences de Cracovie, 1907, pp. 147–196.
[22] S. Zaremba, Sur le calcul numerique des fonctions demandees dans le probleme de dirichlet et le probleme
hydrodynamique, Bulletin international de l’ Academie des sciences de Cracovie, 1908.
[23] S. Bergman, Ueber die entwicklung der harmonischen functionen der ebene und des raumes nach orthonal-
functionen, Math. Ann. 86 (1922) 238–271.
[24] V.I. Paulsen, An Introduction to the Theory of Reproducing Kernel Hilbert Spaces, September 2009 (Notes).
[25] J. Shawe-Taylor, N. Cristianini, Kernel Methods for Pattern Analysis, Cambridge University Press,
2004.
[26] G.S. Kimeldorf, G. Wahba, Some results on Tchebychefﬁan spline functions, J. Math. Anal. Appl. 33 (1971)
82–95.
[27] P. Bouboulis, K. Slavakis, S. Theodoridis, Adaptive kernel-based image denoising employing semi-
parametric regularization, IEEE Trans. Image Process. 19 (6) (2010) 1465–1479.

References
983
[28] A.L. Yuille, N.M. Grzywacz, A mathematical analysis of the motion coherence theory, Int. J. Comput. Vis.
3 (2) (1989) 155–175.
[29] I.J. Schoenberg, Positive deﬁnite functions on spheres, Duke Math. J. 9 (1942) 96–108.
[30] A.V. Balakrishnan, Applied Functional Analysis, Springer, 1976.
[31] L. Debnath, P. Mikusinski, Introduction to Hilbert Spaces with Applications, second ed., Academic Press,
1999.
[32] S. Lang, Real and Functional Analysis, third ed., Springer, 1993.
[33] L.A. Liusternik, V.J. Sobolev, Elements of Functional Analysis, Frederick Ungar Publishing Co., 1961.
[34] R.T. Rockafellar, Convex Analysis, Princeton University Press, Princeton, NJ, 1970.
[35] J. van Tiel, Convex Analysis: An Introductory Text, John Wiley & Sons Ltd., 1984.
[36] B. Widrow, M.E. Hoff Jr., Adaptive switching circuits, in: IRE WESCON Conv. Rec., 1960, pp. 96–104 (pt.
4).
[37] W. Liu, J. Príncipe, S. Haykin, Kernel Adaptive Filtering: A Comprehensive Introduction, Wiley, Hoboken,
New Jersey, 2010.
[38] W. Liu, P. Pokharel, J. Príncipe, The kernel least mean squares algorithm, IEEE Trans. Signal Process. 56
(2) (2008) 543–554.
[39] C. Richard, J. Bermudez, P. Honeine, Online prediction of time series data with kernels, IEEE Trans. Signal
Process. 57 (3) (2009) 1058–1067.
[40] B. Chen, S. Zhao, P. Zhu, J. Principe, Quantized kernel least mean square algorithm, IEEE Trans. Neural
Networks Learn. Syst. 23 (1) (2012) 22–32.
[41] P. Bouboulis, K. Slavakis, S. Theodoridis, Adaptive learning in complex reproducing kernel Hilbert spaces
employing Wirtinger’s subgradients, IEEE Trans. Neural Networks Learn. Syst. 23 (3) (2012) 425–438.
[42] K. Slavakis, S. Theodoridis, I. Yamada, Adaptive constrained learning in reproducing kernel Hilbert spaces:
the robust beamforming case, IEEE Trans. Signal Process. 57 (12) (2009) 4744–4764.
[43] K. Slavakis, P. Bouboulis, S. Theodoridis, Adaptive multiregression in reproducing kernel Hilbert spaces:
the multiaccess MIMO channel case, IEEE Trans. Neural Networks Learn. Syst. 23 (2) (2012) 260–276.
[44] Y. Engel, S. Mannor, R. Meir, The kernel recursive least-squares algorithm, IEEE Trans. Signal Process. 52
(8) (2004) 2275–2285.
[45] D.J. Sebald, J.A. Bucklew, Support vector machine techniques for nonlinear equalization, IEEE Trans. Signal
Process. 48 (2000) 3217–3226.
[46] K.
Kreutz-Delgado,
The
complex
gradient
operator
and
the
CR-calculus,
2009.
<http://arxiv.org/abs/0906.4835>
[47] H. Li, Complex-valued adaptive signal processing using wirtinger calculus and its application to independent
component analysis, PhD Thesis, University of Maryland, 2008.
[48] B. Picinbono, On circularity, IEEE Trans. Signal Process. 42 (12) (1994) 3473–3482.
[49] B. Picinbono, P. Chevalier, Widely linear estimation with complex data, IEEE Trans. Signal Process. 43 (8)
(1995) 2030–2033.
[50] B. Picinbono, P. Bondon, Second order statistics of complex signals, IEEE Trans. Signal Process. 45 (2)
(1997) 411–420.
[51] P. Bouboulis, S. Theodoridis, M. Mavroforakis, The augmented complex kernel LMS, IEEE Trans. Signal
Process. (2012).
[52] T. Adali, H. Li, Complex-valued adaptive signal processing, Adaptive Signal Processing: Next Generation
Solutions, Wiley, NJ, 2010, pp. 1–74.
[53] D. Mandic, V. Goh, Complex Valued Nonlinear Adaptive Filters, Wiley, 2009.
[54] P. Bouboulis, S. Theodoridis, Extension of Wirtinger calculus in RKH spaces and the complex kernel LMS,
in: Proceedings of the IEEE International Workshop on Machine Learning for Signal Processing, Kittil,
Finland, 2010.

984
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
[55] P. Bouboulis, S. Theodoridis, Extension of Wirtinger’s calculus to reproducing kernel Hilbert spaces and the
complex kernel LMS, IEEE Trans. Signal Process. 59 (3) (2011) 964–978.
[56] A. Kuh, D. Mandic, Applications of complex augmented kernels to wind proﬁle prediction, in: Proceedings
of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), April 2009,
pp. 3581–3584.
[57] A. Navia-Vázquez, F. Pérez-Cruz, A. Artés-Rodríguez, A.R. Figueiras-Vidal, Weighted least squares training
of support vectors classiﬁers which leads to compact and adaptive schemes, IEEE Trans. Neural Networks
12 (5) (2001) 1047–1059.
[58] J.A.K. Suykens, J. de Brabanter, L. Lukas, J. Vandewalle, Weighted least squares support vector machines:
robustness and sparse approximation, Neurocomputing 48 (2002) 85–105.
[59] J.A.K. Suykens, T. van Gestel, J. de Brabanter, B. de Moor, J. Vandewalle, Least Squares Support Vector
Machines, World Scientiﬁc, Singapore, 2002.
[60] J.A.K. Suykens, J. Vandewalle, Least squares support vector machine classiﬁers, Neural Process. Lett. 9
(1999) 293–300.
[61] S. van Vaerenbergh, M. Lazaro-Gredilla, I. Santamaría, Kernel recursive least-squares tracker for time-
varying regression, IEEE Trans. Neural Networks Learn. Syst. 23 (8) (2012) 1313–1326.
[62] R.T. Rockafellar, R.J.-B. Wets, Variational Analysis, Springer, Berlin, 2004.
[63] D.P. Bertsekas, A. Nedic, A.E. Ozdaglar, Convex Analysis and Optimization, Athena Scientiﬁc, 2003.
[64] M. Todd, Semideﬁnite optimization, Acta Numer. 10 (2001) 515–560.
[65] H.H. Bauschke, P.L. Combettes, Convex Analysis and Monotone Operator Theory in Hilbert Spaces,
Springer, 2011.
[66] K. Goebel, S. Reich, Uniform Convexity, Hyperbolic Geometry, and Nonexpansive Mappings, Pure and
Applied Mathematics, vol. 83, Marcel Dekker, New York, 1984.
[67] K. Goebel, W.A. Kirk, Topics in Metric Fixed Point Theory, Cambridge Studies Advanced Mathematics,
vol. 28, Cambridge University Press, 1990.
[68] K. Slavakis, S. Theodoridis, Sliding window generalized kernel afﬁne projection algorithm using projection
mappings, EURASIP J. Adv. Signal Process. 2008 (2008) 16.
[69] D.G. Luenberger, Optimization by Vector Space Methods, John Wiley & Sons, New York, 1969.
[70] J.-B. Baillon, G. Haddad, Quelques propriétés des opérateurs angle-bornés et n-cycliquement monotones,
Isr. J. Math. 26 (1977) 137–150.
[71] H.H. Bauschke, P.L. Combettes, The Baillon-Haddad theorem revisited, J. Convex Anal. 17 (2010) 781–787.
[72] P.L. Combettes, V.R. Wajs, Signal recovery by proximal forward-backward splitting, Multiscale Model.
Simul. 4 (2005) 1168–1200.
[73] P.L. Combettes, J.-C. Pesquet, Proximal splitting methods in signal processing, in: Fixed-Point Algorithms
for Inverse Problems in Science and Engineering, Springer-Verlag, 2011.
[74] J.-J. Moreau, Fonctions convexes duales et points proximaux dans un espace Hilbertien, Acad. Sci. Paris
Sér. A Math. 255 (1962) 2897–2899.
[75] P.L. Combettes, J.-C. Pesquet, A Douglas-Rachford splitting approach to nonsmooth convex variational
signal recovery, IEEE J. Sel. Top. Signal Process. 1 (4) (2007) 564–574.
[76] J. Douglas Jr., H.H. Rachford Jr., On the numerical solution of heat conduction problems in two and three
space variables, Trans. Am. Math. Soc. 82 (1956) 421–439.
[77] J. Duchi, Y. Singer, Efﬁcient online and batch learning using forward backward splitting, J. Mach. Learn.
Res. 10 (2009) 2899–2934.
[78] J. Duchi, E. Hazan, Y. Singer, Adaptive subgradient methods for online learning and stochastic optimization,
J. Mach. Learn. Res. 12 (2011) 2121–2159.
[79] A.A. Goldstein, Convex programming in Hilbert space, Bull. Am. Math. Soc. 70 (5) (1964) 709–710.

References
985
[80] E.S. Levitin, B.T. Polyak, Constrained minimization methods, Zh. u¯ychisl. Mat. mat. Fiz. 6 (5) (1966)
787–823.
[81] R.A. Horn, C.R. Johnson, Matrix Analysis, Cambridge University Press, New York, 1985.
[82] Y.I. Alber, A.N. Iusem, M.V. Solodov, On the projected subgradient method for nonsmooth convex opti-
mization in a Hilbert space, Math. Program. 81 (1998) 23–35.
[83] P.-E. Maingé, Strong convergence of projected subgradient methods for nonsmooth and nonstrictly convex
minimization, Set-Valued Anal. 16 (2008) 899–912.
[84] B.T. Polyak, A general method for solving extremal problems, Dokl. Akad. Nauk. SSSR 174 (1) 1967 33–36.
[85] Y.M. Ermol’ev, Methods for solving non-linear extremal problems, Kibernetika 4 (1966) 1–17.
[86] N.Z. Shor, On the structure of algorithms for the numerical solution of optimal planning and design problems,
PhD Thesis, Cybernetics Institute, Academy of Sciences, Kiev, 1964.
[87] A.B. Novikoff, On convergence proofs on perceptrons, in: Symposium on the Mathematical Theory of
Automata, 1962, pp. 615–622.
[88] F. Rosenblatt, The perceptron: a probabilistic model for information storage and organization in the brain,
Psychol. Rev. 65 (1958) 386–408.
[89] B.T. Polyak, Minimization of unsmooth functionals, Zh. u¯ychisl. Mat. mat. Fiz. 9 (3) (1969) 509–521.
[90] I. Yamada, Adaptive projected subgradient method: a uniﬁed view for projection based adaptive algorithms,
J. IEICE 86 (8) (2003) 654–658 (in Japanese).
[91] I. Yamada, N. Ogura, Adaptive projected subgradient method for asymptotic minimization of sequence of
nonnegative convex functions, Numer. Funct. Anal. Optim. 25 (7/8) (2004) 593–617.
[92] K. Slavakis, I. Yamada, N. Ogura, The adaptive projected subgradient method over the ﬁxed point set of
strongly attracting nonexpansive mappings, Numer. Funct. Anal. Optim. 27 (7/8) (2006) 905–930.
[93] K. Slavakis, I. Yamada, The adaptive projected subgradient method constrained by families of
quasi-nonexpansive mappings and its application to online learning, submitted for publication.
<http://arxiv.org/abs/1008.5231>.
[94] M.Yukawa,K.Slavakis,I.Yamada,Multi-domainadaptivelearningbasedonfeasibilitysplittingandadaptive
projected subgradient method, IEICE Trans. Fundam. E93-A (2) (2010) 456–466.
[95] S. Theodoridis, K. Slavakis, I. Yamada, Adaptive learning in a world of projections: a unifying framework
for linear and nonlinear classiﬁcation and regression tasks, IEEE Signal Process. Mag. 28 (1) (2011) 97–123.
[96] K. Slavakis, S. Theodoridis, I. Yamada, Online kernel-based classiﬁcation using adaptive projection algo-
rithms, IEEE Trans. Signal Process. 56 (7) (2008) 2781–2796.
[97] Y. Kopsinis, K. Slavakis, S. Theodoridis, Online sparse system identiﬁcation and signal reconstruction using
projections onto weighted ℓ1 balls, IEEE Trans. Signal Process. 59 (3) (2011) 936–952.
[98] S. Chouvardas, K. Slavakis, S. Theodoridis, Adaptive robust distributed learning in diffusion sensor networks,
IEEE Trans. Signal Process. 59 (10) (2011) 4692–4707.
[99] K. Ozeki, T. Umeda, An adaptive ﬁltering algorithm using an orthogonal projection to an afﬁne subspace
and its properties, IEICE Trans. 67-A (5) (1984) 126–132 (in Japanese).
[100] T. Hinamoto, S. Maekawa, Extended theory of learning identiﬁcation, Trans. IEE Jpn. 95-C (10) (1975)
227–234 (in Japanese).
[101] I. Yamada, K. Slavakis, K. Yamada, An efﬁcient robust adaptive ﬁltering algorithm based on parallel sub-
gradient projection techniques, IEEE Trans. Signal Process. 50 (5) (2002) 1091–1101.
[102] A.E. Albert, L.A. Gardner, Stochastic Approximation and Nonlinear Regression, MIT Press, 1967.
[103] A.V. Malipatil, Y.-F. Huang, S. Andra, K. Bennett, Kernelized set-membership approach to nonlinear adaptive
ﬁltering, in: Proceedings of IEEE ICASSP, IEEE, 2005, pp. 149–152.
[104] J. Nagumo, J. Noda, A learning method for system identiﬁcation, IEEE Trans. Autom. Control 12 (3) (1967)
282–287.

986
CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces
[105] K. Slavakis, Y. Kopsinis, S. Theodoridis, Revisiting adaptive least-squares estimation and application to
online sparse signal recovery, in: Proceedings of the IEEE ICASSP, Prague, Czech Republic, 2011, pp.
4292–4295.
[106] R. Cavalcante, I. Yamada, Flexible peak-to-average power ratio reduction scheme for OFDM systems by the
adaptive projected subgradient method, IEEE Trans. Signal Process. 57 (4) (2009) 1456–1468.
[107] Y. Kopsinis, K. Slavakis, S. Theodoridis, S. McLaughlin, Generalized thresholding sparsity-aware algorithm
for low complexity online learning, in: Proceedings of the IEEE ICASSP, Kyoto, Japan, 2012, pp. 3277–3280.
[108] C. Breining, P. Dreiseitel, E. Hänsler, A. Mader, B. Nitsch, H. Puder, T. Schertler, G. Schmidt, J. Tilp,
Acoustic echo control—An application of very-high-order adaptive ﬁlters, IEEE Signal Process. Mag. 16
(4) (1999) 42–69.
[109] M. Martínez-Ramón, J.L. Rojo-Álvarez, G. Camps-Valls, C.G. Christodoulou, Kernel antenna array pro-
cessing, IEEE Trans. Antennas Propag. 55 (3) (2007) 642–650.
[110] Y. Murakami, M. Yamagishi, M. Yukawa, I. Yamada, A sparse adaptive ﬁltering using time-varying soft-
thresholding techniques, in: Proceedings of the IEEE ICASSP, Dallas, USA, March 2010, pp. 3734–3737.
[111] S. Werner, P.S.R. Diniz, Set-membership afﬁne projection algorithm, IEEE Signal Process. Lett. 8 (8) (2001)
231–235.
[112] M. Yukawa, I. Yamada, Efﬁcient adaptive stereo echo canceling schemes based on simultaneous use of
multiple state data, IEICE Trans. Fundam. E87-A (8) (2004) 1949–1957.
[113] M. Yukawa, Multi-kernel adaptive ﬁltering, IEEE Trans. Signal Process., in press.
[114] Y. Amit, S. Shalev-Shwartz, Y. Singer, Online learning of complex prediction problems using simultaneous
projections, J. Mach. Learn. Res. 9 (2008) 1399–1435.
[115] A. Bordes, S. Ertekin, J. Weston, L. Bottou, Fast kernel classiﬁers with online and active learning, J. Mach.
Learn. Res. 6 (2005) 1579–1619.
[116] L. Bottou, Y. LeCun, Large scale online learning, in: Advances in Neural Information Processing Systems
(NIPS), MIT Press, Cambridge, MA, 2004.
[117] N. Cesa-Bianchi, S. Shalev-Shwartz, O. Shamir, Online learning of noisy data, IEEE Trans. Inform. Theory
57 (12) (2011) 7907–7931.
[118] N. Cesa-Bianchi, P.M. Long, M.K. Warmuth, Worst-case quadratic loss bounds for prediction using linear
functions and gradient descent, IEEE Trans. Neural Networks 7 (3) (1996) 604–619.
[119] K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, Y. Singer, Online passive-aggressive algorithms, J.
Mach. Learn. Res. 7 (2006) 551–585.
[120] A. Elisseeff, M. Pontil, Leave-one-out error and stability of learning algorithms with applications, in:
Advances in Learning Theory: Methods, Models and Applications, IOS Press, 2003.
[121] M. Herbster, M. Pontil, Prediction on a graph with the perceptron, in: Advances in Neural Information
Processing Systems (NIPS), MIT Press, Cambridge, MA, 2006, pp. 577–584.
[122] J. Kivinen, A.J. Smola, R.C. Williamson, Online learning with kernels, IEEE Trans. Signal Process. 52 (8)
(2004) 2165–2176.
[123] J. Langford, L. Li, T. Zhang, Sparse online learning via truncated gradient, J. Mach. Learn. Res. 10 (2009)
777–801.
[124] A.F.T. Martins, N.A. Smith, E.P. Xing, P.M.Q. Aguiar, M.A.T. Figueiredo, Online learning of structured
predictors with multiple kernels, in: Proceedings of the International Conference on Artiﬁcial Intelligence
and Statistics (AISTATS), Fort Lauderdale, FL, USA, 2011.
[125] T. Poggio, S. Voinea, L. Rosasco, Online learning, stability, and stochastic gradient descent, September 2011.
<http://arxiv.org/abs/1105.4701v3>.
[126] S. Shalev-Shwartz, Y. Singer, N. Srebro, A. Cotter, Pegasos: primal estimated sub-gradient solver for SVM,
Math. Program. 127 (1) (2011) 3–30.

References
987
[127] S. Shalev-Shwartz, Online learning and online convex optimization, Found. Trends Mach. Learn. 4 (2) (2012)
107–194.
[128] Y. Ying, M. Pontil, Online gradient descent learning algorithms, Found. Comput. Math. 8 (5) (2008) 561–596.
[129] P.L. Combettes, The foundations of set theoretic estimation, Proc. IEEE 81 (2) (1993) 182–208.
[130] D.C. Youla, H. Webb, Image restoration by the method of convex projections: Part 1—Theory, IEEE Trans.
Med. Imag. MI-1 (1982) 81–94.

18
CHAPTER
Introduction to Probabilistic
Graphical Models
Franz Pernkopf, Robert Peharz, and Sebastian Tschiatschek
Graz University of Technology, Laboratory of Signal Processing and Speech Communication Inffeldgasse 16c,
8010 Graz, Austria
Nomenclature
X, Y, Z, Q, C
Random variable (RV)
x, y, z, q, w, c
Instantiation of random variable
X, Y, Z, A, B, D, O, Q, H
Set of random variables
x, y, z, o, q, h, w
Set of instantiations of random variables
val(X)
Set of values (states) of random variable X
sp(X) = |val(X)|
Number of states of random variable X
PX(x)
Probability density function (PDF) of a continuous random
variable X
PX(x) = P(X = x)
Probability mass function (PMF) of a discrete random
variable X
P(X, Y)
Joint probability distribution of random variable X and Y
P(X|Y)
Conditional probability distribution (CPD) of X conditioned on Y

Outcome space composed of a set of events
ωi
Elementary event

Event space
θ
Parameters
θ, 
Set of parameters
R
Set of real numbers
R+
Set of nonnegative real numbers
E[X]
Expected value of random variable X
1{A}
Indicator function; equals 1 if statement A is true and 0 otherwise
N(· |μ; σ 2)
Gaussian distribution with mean μ and variance σ 2
B
Bayesian network (BN)
M
Markov network (MN)
F
Factor graph (FG)
D
Set of i.i.d. samples
C
Class of models, e.g., Bayesian networks
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00018-8
© 2014 Elsevier Ltd. All rights reserved.
989

990
CHAPTER 18 Introduction to Probabilistic Graphical Models
L(·; ·)
Likelihood function (LH)
ℓ(·, ·)
Log-likelihood function
KL(PA||PB)
Kullback-Leibler (KL) divergence between distribution PA and PB
L(·, ·)
Lagrangian function
ν
Lagrangian multiplier
Dir(·; α)
Dirichlet distribution parametrized by the vector α
B(α)
Beta-function parametrized by the vector α
CR(·)
Classiﬁcation rate (CR)
CLL(·; ·)
Conditional log likelihood (CLL)
MM(·; ·)
Margin objective function (MM)
G
Generator matrix of a linear block code
H
Parity-check matrix of a linear block code
A
Finite alphabet of a code
G
Directed or undirected graph
E
Set of edges
PaG(X)
Set of all parents of X in graph G
ChG(X)
Set of all children of X in graph G
NbG(X)
Set of neighbors of X in graph G
AncG(X)
Set of ancestors of X in graph G
DescG(X)
Set of descendants of X in graph G
NonDescG(X)
Set of nondescendants of X in graph G
C
Clique
S
Separator set
	C
Nonnegative potential functions over the maximal cliques C
fi
Positive function of a factor graph F
F
Set of factor nodes
μX→Y (y)
Message from node X to node Y
α(·)
Forward probabilities
β(·)
Backward probabilities
1.18.1 Introduction
Machine learning and pattern recognition are elementary building blocks of intelligent systems. The aim
is to capture relevant phenomena hidden in empirical data using computational and statistical methods.
Hence, the task is to automatically recognize and identify complex patterns in data which are further
cast into a model. A prerequisite is that the learned model generalizes well in order to provide useful
information for new data samples. To account for this, machine learning relies on many ﬁelds in science
such as statistics and probability calculus, optimization methods, cognitive science, computer science,
et cetera.

1.18.1 Introduction
991
Learning can be divided into supervised, unsupervised, and reinforcement learning problems. For
supervisedlearningthetrainingdatacomprisesoffeaturevectorswhichcontainanabstractdescriptionof
the object and their corresponding target or class label. If the desired target value is continuous, then this
is referred to as regression. In the case where the input vector is assigned to a ﬁnite number of categories
this is called classiﬁcation. In unsupervised learning, the training data consists of a set of feature vectors
without corresponding target value. The aim is to discover groups/clusters within the data or to model
the distribution of the data, i.e., representations of the data are modeled. In reinforcement learning, an
agent should perform actions in an environment so that it maximizes a long-term reward. The learning
algorithm cannot access a ﬁnite set of labeled samples for training as in supervised classiﬁcation. It
must discover which actions of the agent result in an optimal long-term reward. Typically, the actions do
not only affect an immediate reward but also have impact on future actions, i.e., taking the best reward
at each time step is insufﬁcient for the global optimal solution.
Probabilistic graphical models (PGMs) [1] are important in all three learning problems and have
turned out to be the method of choice for modeling uncertainty in many areas such as computer vision,
speech processing, time-series and sequential data modeling, cognitive science, bioinformatics, prob-
abilistic robotics, signal processing, communications and error-correcting coding theory, and in the
area of artiﬁcial intelligence in general. One reason is that this common modeling framework allows
to transfer concepts and ideas among different application areas. Another reason stated in [2] is that
common-sense reasoning is naturally embedded within the syntax of probability calculus. PGMs com-
bine probability theory and graph theory. They offer a compact graph-based representation of joint
probability distributions exploiting conditional independencies among the random variables. Condi-
tional independence assumptions alleviate the computational burden for model learning and inference.
Hence, graphical models provide techniques to deal with two inherent problems throughout applied
mathematics and engineering, namely, uncertainty and complexity [3]. Many well-known statistical
models, e.g., (dynamic) Bayesian networks, mixture models, factor analysis, hidden Markov mod-
els (HMMs), factorial HMMs, Kalman ﬁlters, Boltzmann machines, the Ising model, amongst others,
can be represented by graphical models. The framework of graphical models provides techniques for
inference (e.g., sum product algorithm, also known as belief propagation or message passing) [2] and
learning [1].1 In this article, three popular representations of graphical models are presented: Markov
networks(MNs)(alsoknownasundirectedgraphicalmodels(UGMs)orMarkovrandomﬁelds(MRFs)),
Bayesian networks (BNs) (also known as directed graphical models (DGMs) or belief networks) and
factor graphs (FGs).
The research in PGMs has focused on two major ﬁelds which both are tightly connected to advanced
optimization methods:
1. Learning of graphical models: Recent advances in structure learning are in ﬁnding structures sat-
isfying some optimality conditions. This is in contrast to search heuristics which in general do not
provide any guarantees. Current research in learning the parameters goes beyond classical maxi-
mum likelihood parameter learning which belongs to generative learning. Popular objectives are
the conditional likelihood or a probabilistic formulation of the margin. Optimizing these objectives
can lead to better classiﬁcation performance, particularly when the class conditional distributions
1In Bayesian statistics, learning and inference are the same, i.e., learning is inference of the parameters and structure.

992
CHAPTER 18 Introduction to Probabilistic Graphical Models
poorly approximate the true distribution [4]. This is often referred to as discriminative learning with
the aim of optimizing the model for one inference scenario, namely classiﬁcation.
2. Efﬁcient inference: Inference means answering queries using the PGM. In more technical language,
inference deals with determining the marginal or most likely conﬁguration of random variables given
observed variables using the joint distribution modeled as PGM. Generally, exact inference is not
tractable in large and dense graphical models and, therefore, approximate inference techniques are
needed. Over the past decades, tractable approximate inference has been one of the most challenging
and active research ﬁelds.
This tutorial is organized as follows: In Section 1.18.3 we present three types of representations,
namely BNs, MNs, and FGs. Then in Sections 1.18.4 and 1.18.5 different learning approaches and
inference methods are discussed. In Section 1.18.6, we present typical examples for each of the three
graphical model representations. Sections 1.18.7 and 1.18.8 provide references to implementations
and data sets. Finally, Section 1.18.9 concludes the tutorial providing a brief perspective on advanced
methods and future trends.
In this tutorial article we presume that the reader is familiar with elementary probability calculus
and fundamental linear algebra concepts. This introduction to probabilistic graphical models is nec-
essarily incomplete due to the vast amount of methods developed over the last decades. Nevertheless,
we hope to provide the reader with an easily accessible and interesting introduction to the ﬁeld of
PGMs. For additional details on any of the covered subjects the interested reader is referred to the cited
literature.
1.18.2 Preliminaries
In this section we introduce parts of the notation used throughout this tutorial. Further, we review the
basics of probability theory and present elementary deﬁnitions from graph theory.
1.18.2.1 Probability theory
In this section we present a short review of the most important concepts from probability theory. For a
more detailed introduction we refer the reader to [1,5,6]. We start with some basic deﬁnitions.
Deﬁnition 1 (Outcome Space, Elementary Events).
An outcome space  is a non-empty set. The
elements of  are called elementary events.
Example 1 (Outcome Space, Elementary Events).
We can deﬁne an outcome space of a die game
according to  = {ω1, ω2, ω3, ω4, ω5, ω6}. The elementary event ωi denotes the outcome “die shows
number i.”
We further deﬁne the notion of an event space.
Deﬁnition 2 (Event Space, Events).
Given an outcome space , an event space  over  is a set of
subsets of , satisfying the following properties:
•
∅∈ and  ∈.
•
If σ1 ∈ and σ2 ∈, then also σ1 ∪σ2 ∈.

1.18.2 Preliminaries
993
•
If σ ∈, then also  \ σ ∈.
The elements of  are called events.  is called the certain event and ∅is the impossible event.
Example 2 (Event Space, Events).
Given the outcome space of the die game  = {ω1, ω2, ω3, ω4,
ω5, ω6} of Example 1, we give three examples of possible event spaces:
•
The event space 1 = {∅, {ω1, ω3, ω5}, {ω2, ω4, ω6}, } contains the impossible event, the certain
event, and the two events “outcome is odd” and “outcome is even.”
•
The event space 2 = {∅, {ω1, ω2, ω3}, {ω4, ω5, ω6}, } contains the impossible event, the certain
event, and the two events “outcome is smaller or equal 3” and “outcome is larger than 3.”
•
The event space 3 = 2, where 2 is the power set of , contains all subsets of , i.e., all possible
combinations of elementary events.
We are now ready to deﬁne probability distributions.
Deﬁnition 3 (Probability Distribution, Probability Space).
Let  be an outcome space and  an
event space over . A probability distribution over (, ) is a function P :  →R satisfying the
following properties:
•
P(σ) ⩾0, ∀σ ∈.
•
P() = 1.
•
If σi ∩σ j = ∅, then P(σi ∪σ j) = P(σi) + P(σ j), ∀σi, σ j ∈, i ̸= j.
The triplet (, , P) is called a probability space.
Example 3 (Probability Distribution).
Let 3 = 2 be the event space from Example 2. We can
deﬁneaprobabilitydistributionbysetting P(ω1) = P(ω2) = P(ω3) = P(ω4) = P(ω5) = P(ω6) = 1
6,
i.e.,weassumeafairdie.SincePhastobeaprobabilitydistribution,itfollowsthat P(σ) = |σ|
6 , ∀σ ∈3,
where |σ| denotes the cardinality of σ.
Throughout the article, we use events and probability distributions deﬁned via random variables, which
are characterized by their cumulative distribution functions.
Deﬁnition 4 (Random Variable, Cumulative Distribution Function (CDF)).
Let (, , P) be a
probability space and S a measurable space (in this tutorial we assume S = R). A random variable (RV)
X is a function X :  →S, where “X ⩽x” = {ω ∈ : X(ω) ⩽x} is an event for every x ∈S, and
for the events “X = −∞” = {ω ∈ : X(ω) = −∞} and “X = ∞” = {ω ∈ : X(ω) = ∞} it must
hold that P(X = −∞) = P(X = ∞) = 0. The probability of the event “X ⩽x” is the cumulative
distribution function (CDF)
FX(x) = P(X ⩽x).
(18.1)
More generally, let X = {X1, X2, . . . , X N} be an ordered set of N RVs and x a corresponding set of N
values from S. The joint CDF of X is deﬁned as
FX(x) = P(X1 ⩽x1 ∩X2 ⩽x2 ∩· · · ∩X N ⩽xN).
(18.2)
The image of an RV X is denoted as val(X) = {x ∈S|∃ω ∈ : X(ω) = x}. We say that X takes
values out of val(X). Similarly, the set of values which can be assumed by a set of random variables X
is denoted as val(X).

994
CHAPTER 18 Introduction to Probabilistic Graphical Models
WhenweareonlyinterestedineventsconnectedwithRVs, theCDFfullycharacterizestheunderlying
probability distribution. If the CDF of an RV X is continuous, then we say that X is a continuous RV.
For continuous RVs we deﬁne the probability density function.
Deﬁnition 5 (Probability Density Function (PDF)).
Let FX(x) be the CDF of a continuous RV X.
The probability density function (PDF) of X is deﬁned as
PX(x) = dFX(x)
dx
.
(18.3)
If PX(x) exists, then the CDF of X is given as
FX(x) =
 x
−∞
PX(x)dx.
(18.4)
More generally, let FX(x) be the CDF of a set of continuous RVs X. The joint PDF of X is deﬁned as
PX(x) =
∂N FX(x)
∂x1 . . . ∂xN
.
(18.5)
If PX(x) exists, then the CDF of X is given as
FX(x) =
 x1
−∞
. . .
 xN
−∞
PX(x)dx1 . . . dxN.
(18.6)
The PDF, if it exists, is an equivalent representation of the CDF, i.e., the PDF also represents the
underlying probability distribution. It follows from the deﬁnition of probability distributions, that a
PDF is nonnegative and integrates to one, i.e., PX(x) ⩾0, ∀x, and

PX(x)dx = 1.
A RV is called discrete when the CDF of X is piecewise constant and has a ﬁnite number of
discontinuities. For discrete RVs we deﬁne the probability mass function.
Deﬁnition 6 (Probability Mass Function (PMF)).
Let X be a discrete RV. The probability mass
function (PMF) of X is a function PX : val(X) →R and deﬁned as
PX(x) = P(X = x), x ∈val(X).
(18.7)
More generally, let X be a set of discrete RVs. The joint PMF of X is a function PX : val(X) →R and
deﬁned as
PX(x) = P(X1 = x1 ∩X2 = x2 ∩· · · ∩X N = xN), x = {x1, x2, . . . , xN} ∈val(X).
(18.8)
It follows from the deﬁnition of probability distributions, that a PMF is nonnegative and sums to one,
i.e., PX(x) ⩾0, ∀x ∈val(X), and 
x∈val(X) PX(x) = 1. Unlike the PDF, a PMF always exists in the
case of discrete RVs.
We use the same symbol PX(x) to denote PDFs and PMFs throughout the manuscript, since most of
the discussion holds for both objects. From now on, we assume that PX(x) exists and fully represents
the underlying probability distribution, and with slight abuse of notation, we refer to PX(x) itself as

1.18.2 Preliminaries
995
distribution. Usually we omit the subscripts of PDFs/PMFs, i.e., we use the shorthand P(x) = PX(x).
Furthermore, we use the notation P(X) for RVs, P(x) for instantiations x of RVs X, P(X ∪Y) =
P(X, Y), and P({X} ∪Y) = P(X, Y). Throughout this tutorial, we assume that X = {X1, . . . , X N}.
Unless stated otherwise, the variables X1, . . . , X N are assumed to be discrete and the number of possible
states of variable Xi is denoted as sp(Xi) = |val(Xi)|. Similarly, for a set of RVs X, the total number
of possible assignments is given as
sp(X) =
N

i=1
sp(Xi).
(18.9)
When Y is a subset of X, and x is a set of values for X, then x(Y) denotes the set of values corresponding
to Y.
An important quantity of RVs is the expected value.
Deﬁnition 7 (Expected Value).
Let X be an RV with distribution P(X). The expected value of X is
deﬁned as
E[X] =
 
x∈val(X) x P(x) if X is discrete,

val(X) x P(x)dx
if X is continuous.
(18.10)
More generally, the expected value of a function f : val(X) →R is deﬁned as
E[ f (X)] =
 
x∈val(X) f (x)P(x) if X is discrete,

val(X) f (x)P(x)dx
if X is continuous.
(18.11)
Similarly, we can deﬁne expectations over sets of RVs.
When the joint distribution P(X) is given, one is often interested in the distribution P(Y) of a subset
Y ⊆X of RVs. Also, one often considers the distributions of subsets Y ⊆X, conditioned on the values
of the remaining variables Z = X \ Y.
Deﬁnition 8 (Marginal Distribution, Conditional Distribution).
Let P(X) be a distribution over a
set of RVs X. Further let Y ⊆X, and Z = X \ Y, i.e., X = Y ∪Z and P(X) = P(Y, Z). Assuming
discrete RVs, the marginal distribution P(Y) over Y is given as
P(Y) =

z∈val(Z)
P(Y, Z = z) =

z∈val(Z)
P(Y, z),
(18.12)
and the conditional distribution P(Y|Z) over Y, conditioned on Z, is given as
P(Y|Z) = P(Y, Z)
P(Z)
=
P(Y, Z)

y∈val(Y) P(y, Z).
(18.13)
In the case of continuous RVs, summations are replaced by integration in a straightforward manner.
The deﬁnition of the marginal and conditional distribution leads to an important rule, which allows to
invert the “direction of conditioning.” Using (18.13), it is easily seen that for two sets of discrete RVs

996
CHAPTER 18 Introduction to Probabilistic Graphical Models
Y and Z the following holds:
P(Y, Z) = P(Y, Z),
(18.14)
P(Y|Z)P(Z) = P(Z|Y)P(Y),
(18.15)
P(Y|Z) = P(Z|Y)P(Y)
P(Z)
=
P(Z|Y)P(Y)

y∈val(Y) P(Z|y)P(y).
(18.16)
Equation (18.16) is known as the Bayes’ rule or the rule of inverse probability. For continuous RVs, the
summation is replaced by an integral.
From the conditional distribution follows the concept of statistical independence. Informally, two
RVs X and Y are independent, if knowing the state of one variable, i.e., conditioning on this variable, does
not change the distribution over the other variable. Formally, statistical independence and conditional
statistical independence are deﬁned as follows.
Deﬁnition 9 (Statistical Independence, Conditional Statistical Independence).
Let X, Y, and Z be
mutually disjoint sets of RVs. X and Y are conditionally statistically independent given Z, annotated
as X ⊥Y|Z, if and only if the following equivalent conditions hold:
•
P(X|Y, Z) = P(X|Z).
•
P(Y|X, Z) = P(Y|Z).
•
P(X, Y|Z) = P(X|Z)P(Y|Z).
For the special case Z = ∅, we say that X and Y are statistically independent, annotated as X ⊥Y.
Furthermore, using conditional distributions, there are many ways to factorize an arbitrary joint
distribution P(X), i.e., any joint probability over X = {X1, . . . , X N} can be written as
P(X) = P(X N|X N−1, X N−2, . . . , X1)P(X N−1|X N−2, . . . , X1) . . . P(X2|X1)P(X1) (18.17)
= P(X1)
N

i=2
P(Xi|Xi −1, . . . , X1).
(18.18)
This is called the chain rule of probability. The chain rule holds for any permutation of the indexes of
the RVs X.
1.18.2.2 Graph theory
In the context of PGMs graphs are used to represent probability distributions in an intuitive and easily
readable way. Formally, a graph is deﬁned as follows:
Deﬁnition 10 (Graph, Vertex, Edge).
A graph G = (X, E) is a tuple consisting of a set of vertices,
also denoted as nodes, X and a set of edges E.
In PGMs each RV corresponds to exactly one node, and vice versa. That is, there is a one-to-one
relationship between RVs and nodes. Therefore, we use these terms equivalently.
The edges of the graph deﬁne speciﬁc relationships between these variables. Any two nodes Xi and
X j, i ̸= j, in a graph can be connected by either a directed or an undirected edge. An undirected edge
between node i and j is denoted as (Xi −X j), and a directed edge from node Xi to X j as (Xi →X j).

1.18.2 Preliminaries
997
X1
X2
X3
X4
X5
X6
X7
(a)
X1
X2
X3
X4
X5
X6
X7
(b)
X1
X2
X3
X4
X5
X6
X7
(c)
FIGURE 18.1
Graphical representation of different graph types. (a) Undirected graph. (b) Directed graph. (c) Mixed graph.
While an undirected edge is a symmetric relationship, i.e., the edge (Xi −X j) is the same as the edge
(X j −Xi), a directed edge represents an asymmetric relationship. Note that we do not allow these two
types of edges to exist between any pair of nodes simultaneously and that there must not be an edge
connecting some node with itself.
Depending on the types of edges in a graph we deﬁne the following three graph types:
Deﬁnition 11 (Directed, Undirected, and Mixed Graph).
A graph G = (X, E) is directed (undi-
rected) if all edges e ∈E are directed (undirected). Further, a graph is said to be a mixed graph if the
set of edges contains undirected and directed edges.
At this point we want to introduce a graphical representation of PGMs; every node is represented by
a circle and edges are depicted as lines, in the case of undirected edges, or as arrows, in the case of
directed edges, connecting the corresponding nodes.
Example 4 (Graphical representation of graphs).
Consider Figure 18.1. For all shown graphs
G = (X, E) the set of nodes is X = {X1, . . . , X7}. The corresponding edge sets are
a. E = {(X1 −X2), (X2 −X3), (X2 −X4), (X3 −X5), (X4 −X6), (X4 −X7)},
b. E = {(X1 →X2), (X2 →X3), (X2 →X4), (X3 →X5), (X4 →X6), (X7 →X4)}, and
c. E = {(X1 →X2), (X2 →X3), (X2 −X4), (X3 −X5), (X4 →X6), (X7 →X4)}.
In graphs we can deﬁne relationships between variables. In directed and mixed graphs we introduce a
parent–child relationship:
Deﬁnition 12 (Parent and Child).
Let G = (X, E) be a directed or mixed graph and Xi, X j ∈X,
i ̸= j. If (Xi →X j) ∈E, then Xi is a parent of X j and X j is a child of Xi. The set PaG(X j) consists
of all parents of X j and the set ChG(Xi) contains all children of Xi.

998
CHAPTER 18 Introduction to Probabilistic Graphical Models
Example 5 (Parents and Children).
In Figure 18.1b X3 is a child of X2 and X2 is a parent of X3.
Further, the set PaG(X3) = {X2}, as the only parent of X3 is X2. The set of all children of X2 is
ChG(X2) = {X3, X4}.
For undirected and mixed graphs we deﬁne the notion of neighborhood:
Deﬁnition 13 (Neighbor).
Let G = (X, E) be an undirected or mixed graph and Xi, X j ∈X, i ̸= j.
If (Xi −X j) ∈E, then Xi is said to be a neighbor of X j. The set of all neighbors of Xi is denoted as
NbG(Xi), i.e.,
NbG(Xi) = {X j : (Xi −X j) ∈E, X j ∈X}.
(18.19)
Example 6 (Neighborhood).
In Figure 18.1a X1, X3 and X4 are neighbors of X2. As these are all
neighbors of X2 in the graph,
NbG(X2) = {X1, X3, X4}.
(18.20)
While edges represent direct relationships between two nodes, the notion of paths and trails describes
indirect relationships across several nodes:
Deﬁnition 14 (Path, Directed Path, Trail).
Let G = (X, E) be a graph. A sequence of nodes
Q = (X1, . . . , Xn), with X1, . . . , Xn ∈X is a path from X1 to Xn if
∀i ∈{1, . . . , n −1} : (Xi →Xi+1) ∈E ∨(Xi −Xi+1) ∈E.
(18.21)
A path is directed if
∃i ∈{1, . . . , n −1} : (Xi →Xi+1) ∈E,
(18.22)
i.e., if there is at least one directed edge on the path.
The sequence of nodes Q is a trail if
∀i ∈{1, . . . , n −1} : (Xi →Xi+1) ∈E ∨(Xi+1 →Xi) ∈E ∨(Xi −Xi+1) ∈E,
(18.23)
i.e., if Q is a path from X1 to Xn replacing all directed edges by undirected edges.
Example 7 (Paths).
Again, consider Figure 18.1.
a. In Figure 18.1a, the sequence (X2, X4, X6) is a path from X2 to X6, and there exists a path between
all pairs of distinct nodes.
b. In Figure 18.1b, (X1, X2, X4) is a directed path from X1 to X4, but there does not exist a path from
X6 to X7 because of the directed edges. But, X6 and X7 are connected by the trail (X6, X4, X7).
c. In Figure 18.1c, the sequence of nodes (X7, X4, X2) is a directed path, but again, there does not
exist a path from X6 to X7.
An important class of undirected graphs are trees:
Deﬁnition 15 (Tree).
An undirected graph G is a tree if any two distinct nodes are connected by
exactly one path.
Example 8 (Tree).
Figure 18.1a represents a tree.

1.18.2 Preliminaries
999
In directed graphs we can identify parts of the graph that appear before and after a certain node:
Deﬁnition 16 (Ancestor, Descendant).
In a directed graph G = (X, E) with Xi, X j ∈X, i ̸= j,
the node Xi is an ancestor of X j if there exists a directed path from Xi to X j. In this case, X j is also
denoted as a descendant of Xi.
The set of all ancestors of some node X j ∈X is denoted as AncG(X j), i.e.,
AncG(X j) = {Xi|Xi is an ancestor of X j}.
(18.24)
Similarly, the set of all descendants of some node Xi ∈X is denoted as DescG(Xi), i.e.,
DescG(Xi) = {X j|X j is a descendant of Xi}.
(18.25)
We further deﬁne the nondescendants of Xi as
NonDescG(Xi) = X \ (DescG(Xi) ∪{Xi} ∪PaG(Xi)),
(18.26)
i.e., the nondescendants of Xi are all nodes in the graph that are neither descendants of Xi, nor the node
Xi itself or the parents PaG(Xi) of this node.
A special type of paths are directed cycles:
Deﬁnition 17 (Directed cycle).
Let G = (X, E) be a graph, Xi ∈X, and let P be a directed path from
Xi to Xi. Then P is denoted as a directed cycle.
Using these notion, we can deﬁne an important type of directed graphs:
Deﬁnition 18 (Directed acyclic graphs).
A graph G = (X, E) is a directed acyclic graph (DAG) if
it contains no directed cycles.
Example 9 (DAG).
An example of a DAG is given in Figure 18.1b.
Sometimes, we will consider graphs that are contained in a larger graph:
Deﬁnition 19 (Subgraph, Supergraph).
The graph G = (X, E) is a subgraph of G′ = (X′, E′), if
and only if X ⊆X′ and E ⊆E′. Further, G′ is a supergraph of G if and only if G is a subgraph of G′.
For some of our considerations we need to identify special subsets of the nodes of an undirected graph:
Deﬁnition 20 (Clique, Maximal Clique).
In an undirected graph G = (X, E) a set of nodes C ⊆X
is a clique if there exists an edge between all pairs of nodes in the subset C, i.e., if
∀Ci, C j ∈C, i ̸= j : (Ci −C j) ∈E.
(18.27)
The clique C is a maximal clique if adding any node X ∈X \ C makes it no longer a clique.
Example 10 (Cliques, Maximal Cliques).
Consider Figure 18.2. The set of nodes {X2, X3, X4} is a
maximal clique as {X1, X2, X3, X4} is no clique. The set {X1, X3} is a clique that is not maximal as
{X1, X2, X3} is also a clique.

1000
CHAPTER 18 Introduction to Probabilistic Graphical Models
X1
X3
X2
X4
FIGURE 18.2
Clique and maximal clique.
X1
X2
X3
X6
X4
X7
X5
FIGURE 18.3
Topologically ordered graph.
The notion of cliques is closely related to that of complete (sub) graphs:
Deﬁnition 21 (Complete Graph).
The undirected graph G = (X, E) is complete if every pair of
distinct nodes is connected by an edge, i.e., if
∀Xi, X j ∈X, i ̸= j : (Xi −X j) ∈E.
(18.28)
The set C ⊆X is a clique if the induced graph G′ = (C, E′) is complete, where
E′ = {(Xi −X j) ∈E : Xi, X j ∈C, i ̸= j}.
(18.29)
At the end of this section, we introduce the notion of a topological ordering of the nodes in a directed
graph:
Deﬁnition 22 (Topological Ordering).
Let G = (X, E) be a directed graph and X = {X1, . . . , X N}.
The nodes X are topologically ordered if for any edge (Xi →X j) ∈E also i ⩽j.
Example 11.
The nodes of the graph in Figure 18.3 are topologically ordered, while the nodes of the
graph in Figure 18.1b are not.

1.18.3 Representations
1001
1.18.3 Representations
In this section we introduce three types of PGMs, namely Bayesian networks in Section 1.18.3.1,
Markov networks in Section 1.18.3.2 and factor graphs in Section 1.18.3.3. Each type has its speciﬁc
advantages and disadvantages, captures different aspects of probabilistic models, and is preferred in a
certain application domain. Finally, we conclude the section by an example demonstrating how Markov
chains can be represented in each of the three types of PGMs.
1.18.3.1 Bayesian networks (BNs)
Bayesian networks are directed graphical models. A dynamic BN is a BN unrolled over time. This
essentially means that at each time slice the BN has the same structure [7]. There are two equivalent
ways to deﬁne BNs, namely (a) by factorization properties, or by conditional independencies. We
introduce BNs by their factorization properties and show that this is equivalent to a set of conditional
independence assumptions in Section 1.18.3.1.2.
1.18.3.1.1
Deﬁnition and factorization properties
A BN is deﬁned as follows:
Deﬁnition 23 (Bayesian Network).
A Bayesian network B is a tuple (G, {PX1, . . . , PX N }), where
G = (X, E) is a DAG, each node Xi corresponds to an RV, and PX1, . . . , PX N are conditional probability
distributions (CPDs) associated with the nodes of the graph. Each of these CPDs has the form
PXi (Xi|PaG(Xi)).
(18.30)
The BN B deﬁnes the joint probability distribution PB(X1, . . . , X N) according to
PB(X1, . . . , X N) =
N

i=1
PXi (Xi|PaG(Xi)).
(18.31)
We will use the shorthand notation P(Xi|PaG(Xi)) for PXi (Xi|PaG(Xi)) when the meaning is
clear from the context. Usually each conditional probability distribution PXi is speciﬁed by a param-
eter set θi, and we collect all the parameters into the set  = {θ1, . . . , θ N} and use the terms
B = (G, {PX1, . . . , PX N }) and B = (G, ) equivalently. Similarly, in the case of parameterized distri-
butions, we write P(Xi|PaG(Xi), θi) for PXi (Xi|PaG(Xi)).
Example 12.
Consider the BN in Figure 18.3. The joint probability distribution PB(X1, . . . , X7)
factorizes according to the graph structure as
PB(X1, . . . , X7) =
7

i=1
P(Xi|PaG(Xi))
(18.32)
= P(X1)P(X2|X1)P(X3|X2)P(X4|X3)P(X5)P(X6|X2, X5)P(X7|X6)
(18.33)

1002
CHAPTER 18 Introduction to Probabilistic Graphical Models
Assuming discrete RVs with r = sp(Xi) states, a total of (r −1) + (r −1)r + (r −1)r + (r −1)r +
(r −1) + (r −1)r2 + (r −1)r = 2(r −1) + 4(r −1)r + (r −1)r2 parameters are required to rep-
resent PB(X1, . . . , X7). In contrast, a non-factored representation of PB(X1, . . . , X7) requires r7 −1
parameters. Hence, the representation of a joint probability distribution as BN can dramatically reduce
the number of parameters. This is true particularily when the factorization of the joint distribution
implies conditional independencies among the variables. The number of parameters for the condi-
tional probability of Xi given its parents increases exponentially with the number of parents, i.e.,
(sp(Xi) −1)sp(PaG(Xi)). The factorization of the joint probability distribution is beneﬁcial for learn-
ing and inference as shown in Section 1.18.4 and Section 1.18.5, respectively.
In the sequel, we give two commonly used examples of parametric BNs, one with discrete nodes,
and one with conditional Gaussian probabilities.
Example 13 (Discrete Variables).
Assume a BN B with discrete variables X1, . . . , X N, where each
Xi takes one out of a ﬁnite set of states. For simplicity, we identify the states of Xi with natural
numbers in {1, . . . , sp(Xi)}. In this case, the conditional distributions P(Xi|PaG(Xi), θi) are PMFs,
parameterized by θi such that
P(Xi = j|PaG(Xi) = h, θi) = θi
j|h,
(18.34)
where h is the instantiation of the parent variables PaG(Xi) and j the instantiation of Xi. Note that the
parameters θi must satisfy
θi
j|h ⩾0,
∀j ∈val(Xi), h ∈val(PaG(Xi)) and
(18.35)
sp(Xi)

j=1
θi
j|h = 1,
∀h ∈val(PaG(Xi)),
(18.36)
in order to specify valid probability distributions. Therefore, the CPDs corresponding to the ith variable
can be stored in a table with (sp(Xi) −1) · sp(PaG(Xi)) entries.
The BN distribution in Eq. (18.31) becomes
PB(X = x) =
N

i=1
sp(Xi)

j=1

h∈val(PaG(Xi))

θi
j|h
	ui
j|h ,
(18.37)
where ui
j|h = 1{xi= j and x(PaG(Xi))=h}. The symbol 1{A} denotes the indicator function which equals 1
if the statement A is true and 0 otherwise.
Example 14 (Conditional Gaussian Variables).
In this example we assume that the variables of a BN
B are real-valued and that the CPDs are speciﬁed by Gaussian distributions, with the mean depending
linearly on the value of the parents, and shared variance σ 2. That is, the CPD of node Xi, is given as
P(Xi|PaG(Xi), θi) = N
⎛
⎝Xi


Xk∈PaG(Xi)
αi,kxk + βi; σ 2
⎞
⎠,
(18.38)

1.18.3 Representations
1003
where
N(X|μ; σ 2) =
1
√
2πσ 2 exp

−(X −μ)2
2σ 2

(18.39)
denotes the Gaussian distribution with mean μ and variance σ 2. Here, the parameter set θi = {αi, βi}
contains the linear weights αi = (αi,1, . . . , αi,|PaG(Xi)|) and the bias βi. It can be shown that in
this case the BN distribution in Eq. (18.31) is a multivariate Gaussian distribution, with mean vector
μ = (μ1, μ2, . . . , μN) and covariance matrix , which are recursively given as [4]:
μi = E[Xi] =

Xk∈PaG(Xi)
αi,kμk + βi,
(18.40)
i, j = E[(Xi −E[Xi])(X j −E[X j])T]
=

Xk∈PaG(X j)
α j,ki,k + Ii, jσ 2,
(18.41)
where I denotes the identity matrix, i, j the entry in  in the ith row and jth column.
1.18.3.1.2
Conditional independence properties
BNs provide means for representing conditional independence properties of a probability distribution.
Exploiting conditional independencies among variables supports efﬁcient inference and reduces com-
plexity. The factorization properties of the joint distribution of BNs as given in the last section imply a
set of conditional independencies [1]:
Theorem 1 (Local Conditional Independencies).
Let B = (G, {PX1, . . . , PX N }) be a BN and PB
the joint probability deﬁned by B. Then, PB satisﬁes the local independencies
Xi ⊥NonDescG(Xi)|PaG(Xi) ∀i,
(18.42)
i.e., any variable Xi is conditional independent of its nondescendants given its parents.
Proof.
To prove the stated conditional independence property, it sufﬁces to show that
PB(Xi|NonDescG(Xi), PaG(Xi)) = PB(Xi|PaG(Xi)).
(18.43)
The conditional distribution PB(Xi|NonDescG(Xi), PaG(Xi)) can be written as
PB(Xi|NonDescG(Xi), PaG(Xi)) = PB(Xi, NonDescG(Xi), PaG(Xi))
PB(NonDescG(Xi), PaG(Xi))
.
(18.44)

1004
CHAPTER 18 Introduction to Probabilistic Graphical Models
The numerator is
PB(Xi, NonDescG(Xi), PaG(Xi)) =

DescG(Xi)
PB(Xi, NonDescG(Xi), PaG(Xi), DescG(Xi)) (18.45)
=

DescG(Xi)
PB(Xi|PaG(Xi)

X j∈DescG(Xi)
PB(X j|PaG(X j)) ·

Xk∈NonDescG(Xi)∪PaG(Xi)
PB(Xk|PaG(Xk))
(18.46)
= PB(Xi|PaG(Xi)

Xk∈NonDescG(Xi)∪PaG(Xi)
PB(Xk|PaG(Xk))



=A
·

DescG(Xi)

X j∈DescG(Xi)
PB(X j|PaG(X j))



=1
.
(18.47)
In the same way, the denominator is given as
PB(NonDescG(Xi), PaG(Xi)) =

Xk∈NonDescG(Xi)∪PaG(Xi)
PB(Xk|PaG(Xk))



=A
.
(18.48)
Consequently, we obtain
PB(Xi|NonDescG(Xi), PaG(Xi)) = PB(Xi|PaG(Xi))
(18.49)
and the desired statement follows.
□
Theorem 1 could have also been used as a basis for the deﬁnition of BNs, as, without proof, the
local conditional independence assumptions from the Theorem imply the factorization properties of PB
given in Eq. (18.31).
Additionally to the local conditional independencies, several other conditional independencies hold
in BNs [1]. Some of them can be read off the graph by the concept of d-separation. Before turning to
d-separation, we shortly introduce the notion of context-speciﬁc independence. In BNs with discrete
variables the conditional distributions P(Xi|PaG(Xi), θi) are represented as conditional probability
tables (CPTs). Regularities in these CPTs can render RV Xi independent of some parents condi-
tioned on speciﬁc values of other parent variables. An example of context-speciﬁc independence is
given in [1,8].
Deﬁnition 24 (d-separation [2]).
Let B = (G, {PX1, . . . , PX N }) be a BN, G = (X, E), and X =
{X1, . . . , X N}. Two RVs Xi and X j, i ̸= j, are d-separated if for all trails between Xi and X j there is
an intermediate variable Xk, i ̸= k, j ̸= k, such that
•
the connection is serial or diverging and the state of Xk is observed, or

1.18.3 Representations
1005
Xi
Xk
Xj
FIGURE 18.4
Serial connection.
Xk
Xi
Xj
FIGURE 18.5
Diverging connection.
•
the connection is converging and neither the state of Xk nor the state of any descendant of Xk
is observed,
where the term connection means the path corresponding to the trail.
We now introduce three simple graphs illustrating the underlying concept of the deﬁnition, i.e., the
different connection types and the thereby implied conditional independencies.
Serial connection: A BN B showing a so called serial connection is presented in Figure 18.4.
According to Eq. (18.31) the corresponding joint distribution is given as
PB(Xi, Xk, X j) = P(Xi)P(Xk|Xi)P(X j|Xk).
(18.50)
We now condition the joint probability on Xk, i.e., we assume Xk to be observed. Then, by probability
calculus,
PB(Xi, X j|Xk) =
=PB(Xi,Xk)



P(Xi)P(Xk|Xi) P(X j|Xk)
PB(Xk)
,
(18.51)
= PB(Xi|Xk)P(X j|Xk),
(18.52)
where PB(Xi, X j|Xk), PB(Xi, Xk), and PB(Xi|Xk) are conditional and marginal distributions resulting
from the joint distribution PB(Xi, Xk, X j). That is, Xi and X j are conditionally independent given Xk,
i.e., Xi ⊥X j|Xk. However, if Xk is not observed, the RVs Xi and X j are dependent in general.
Diverging connection: A diverging connection is shown in Figure 18.5. Again, by exploiting the
factorization properties of the BN according to (18.31), we obtain the joint probability
PB(Xi, Xk, X j) = P(Xk)P(Xi|Xk)P(X j|Xk).
(18.53)

1006
CHAPTER 18 Introduction to Probabilistic Graphical Models
Xi
Xj
Xk
FIGURE 18.6
Converging connection.
Conditioning this probability on Xk yields
PB(Xi, X j|Xk) =
=PB(Xi,Xk)



P(Xk)P(Xi|Xk) P(X j|Xk)
PB(Xk)
,
(18.54)
= PB(Xi|Xk)P(X j|Xk).
(18.55)
Hence, Xi ⊥X j|Xk. That is, when Xk is observed, then the variables Xi and X j are conditionally
independent given Xk, while, when Xk is not observed, they are dependent in general.
Converging connection: A converging connection is illustrated in Figure 18.6. The represented joint
distribution is given as
PB(Xi, Xk, X j) = P(Xi)P(X j)P(Xk|Xi, X j).
(18.56)
In contrast to the two cases before, the RVs Xi and X j are a-priori independent since

xk∈valXk
P(X K = xk|Xi, X j) = 1.
(18.57)
That is
PB(Xi, X j) = P(Xi)P(X j),
(18.58)
which can be stated as Xi ⊥X j. To this end, consider the probability PB(Xi, X j|Xk) which can be
computed as
PB(Xi, X j|Xk) = P(Xi)P(X j)P(Xk|Xi, X j)
PB(Xk)
.
(18.59)
In general, this does not factorize in probabilities over Xi and X j. Hence, the RVs Xi and X j are
independent if Xk is not observed but become dependent upon observation of variable Xk (or any
descendant of Xk). This is known as the Berkson’s paradox [9] or the explaining away phenomenon.
Without proof, the d-separation Theorem expresses the conditional independence properties that
follow from two sets of variables being d-separated (a proof is provided in [10]):
Theorem 2 (d-separation [2]).
Let B = (G, {PX1, . . . , PX N }) be a BN and G = (X, E). Furthermore,
let A, B, and D be mutually disjoint subsets of X, where the variables in D are observed. If every pair
of nodes A ∈A and B ∈B are d-separated, then
A ⊥B|D,
(18.60)
that is A and B are conditionally independent given D.

1.18.3 Representations
1007
1.18.3.2 Markov networks (MNs)
Markov networks, also known as Markov Random Fields (MRFs) or undirected graphical models, are
represented by undirected graphs. As in BNs, an MN encodes certain factorization and conditional
independence properties of the joint probability distribution.
1.18.3.2.1
Deﬁnition and factorization properties
Deﬁnition 25 (Markov Network).
A Markov network is a tuple M = (G, {	C1, . . . , 	CL}), where
G = (X, E) is an undirected graph with maximal cliques C1, . . . , CL. The nodes X correspond to RVs
and 	Ci : val(Ci) →R+ are nonnegative functions, called factors or potentials, over the maximal
cliques of the graph G.
The MN deﬁnes a probability distribution according to
PM(X1, . . . , X N) = 1
Z
L

l=1
	Cl(Cl),
(18.61)
where Z is a normalization constant given as
Z =

x∈val(X)
L

l=1
	Cl(x(Cl)).
(18.62)
Often, Z is also referred to as partition function.
Example 15 (Markov Network).
Consider the MN shown in Figure 18.7. The maximal cliques in
this graph are surrounded by dotted boxes and are given as
C1 = {X1, X2, X3},
(18.63)
C2 = {X3, X4},
and
(18.64)
C3 = {X3, X5}.
(18.65)
Hence, the joint probability distribution of the represented model is
PM(X1, . . . , X5) = 1
Z 	C1(X1, X2, X3)	C2(X3, X4)	C3(X3, X5).
(18.66)
1.18.3.2.2
Conditional independence properties
Similarly as in BNs, MNs deﬁne a set of conditional independencies. In the following we present three
conditional independence properties implied by MNs with strictly positive factors. If the factors are
not strictly positive, the conditional independence properties stated below are not equivalent in general.
Details regarding this are provided in [1].

1008
CHAPTER 18 Introduction to Probabilistic Graphical Models
X1
X2
X3
X4
X5
C1
C2
C3
FIGURE 18.7
Example of an MN.
The joint probability distributions represented by MNs with strictly positive factors satisfy the fol-
lowing equivalent conditional independencies:
i. Local Markov property: Any RV Xi is conditionally independent from all other RVs given its
neighbors. Formally,
Xi ⊥(X \ (NbG(Xi) ∪{Xi}))|NbG(Xi).
(18.67)
ii. Pairwise Markov property: Any two non-adjacent RVs are conditionally independent given all
other RVs, i.e.,
Xi ⊥X j|(X \ {Xi, X j}),
if (i −j) /∈E.
(18.68)
iii. Global Markov property: Let A, B, and D be mutually disjoint subsets of X, and the variables in
D are observed. Then,
A ⊥B|D,
(18.69)
if every path from any node in A to any node in B passes through D. In this case, the set D is also
denoted as separating subset.
We illustrate these conditional independence statements in an example.
Example 16 (Conditional Independencies in an MN).
Again, consider the MN shown in
Figure 18.7. Then, for example,
a. by the local Markov property, X1 is conditionally independent from the RVs X4 and X5 given its
neighbors X2 and X3, i.e.,
X1 ⊥{X4, X5}|{X2, X3}.
(18.70)
This is illustrated in Figure 18.8a.

1.18.3 Representations
1009
X1
X2
X3
X4
X5
(a) 
X1
X2
X3
X4
X5
(b) 
X1
X2
X3
X4
X5
(c)
FIGURE 18.8
Conditional independencies in MNs. (a) Local Markov property. (b) Pairwise Markov property. (c) Global
Markov property.
b. by the pairwise Markov property, X4 and X5 are conditionally independent given X1, X2, and X3.
Formally,
X4 ⊥X5{X1, X2, X3}.
(18.71)
This is illustrated in Figure 18.8b.
c. by the global Markov property, X1 and X2 are conditionally independent of X4 and X5 given X3,
i.e.,
{X1, X2} ⊥{X4, X5}|X3.
(18.72)
This is illustrated in Figure 18.8c.
Difference between BNs and MNs: As introduced in the last sections, both BNs and MNs imply
certain conditional independence statements. One might ask if any BN and its conditional independence

1010
CHAPTER 18 Introduction to Probabilistic Graphical Models
X1
X2
X3
(a)
X1
X2
X3
(b)
X1
X2
X3
(c)
FIGURE 18.9
Representation of conditional independencies in BNs by MNs. (a) BN to be represented as an MN.
(b) Candidate MN. (c) Another candidate MN.
properties can be represented by an MN and vice versa. Two simple examples demonstrate, that this is
not the case:
Example 17 (Conditional Independencies in BNs and MNs).
Consider the PGM with RVs X1, X2,
and X3 represented by the BN in Figure 18.9a. We want to represent this BN by an MN that captures
all the conditional independence properties of the BN:
•
An appropriate MN must contain the edges (X1 −X3) and (X2 −X3). Otherwise, X3 would not
depend on the state of X1 and X2 as in the BN. Such an MN is shown in Figure 18.9b. However,
while in the BN in general X1 ̸⊥X2|X3 the MN satisﬁes X1 ⊥X2|X3 by the pairwise Markov
property.
•
Hence, we additionally need to add the edge (X1 −X2) to the MN resulting in the MN represented
in Figure 18.9c. However, now the MN does in general not satisfy that X1 ⊥X2 while the BN does.
Consequently, the conditional independencies of the BN in Figure 18.9a cannot be represented by
an MN.
Example 18 (Conditional Independencies in BNs and MNs).
Consider the MN in Figure 18.10.
We want to represent this MN by a BN capturing all the independence properties that hold in the MN.
By the pairwise Markov property X1 is conditionally independent from X4 given X2 and X3, and X2
is conditionally independent from X3 given X1 and X4, i.e.,
X1 ⊥X4|{X2, X3},
and
(18.73)
X2 ⊥X3|{X1, X4}.
(18.74)
However, these conditional independencies cannot be encoded in a BN. The interested reader can ﬁnd
a proof for this in [1].
Nevertheless, there exist distributions which can be represented by both MNs and BNs. An example
is the Markov chain, cf. Section 1.18.3.4. This can be illustrated graphically by the notion of perfect
maps. A graph is a perfect map for a probability distribution if it represents all conditional independence
properties of the distribution, and vice versa. Now, consider probability distributions over a given set of
variables. Then, the set of probability distributions {PB} for which some BN is a perfect map is a subset
of all probability distributions {P}. Similarly, the set of probability distributions {PM} for which some

1.18.3 Representations
1011
X1
X2
X3
X4
FIGURE 18.10
Representation of conditional independencies in MNs by BNs.
{P}
{PB}
{PM}
{P
P
B}
∩{ M}
FIGURE 18.11
Venn diagram depicting the set of all probability distributions {P} over a given set of variables, and the sets
of probability distributions {PB} and {PM} for which some BN or some MN are a perfect map, respectively.
MN is a perfect map is also a subset of all probability distributions. Additionally, there are probability
distributions {PB} ∩{PM} for which both BNs and MNs are perfect maps. This is illustrated by the
Venn diagram in Figure 18.11 [4].
1.18.3.3 Factor graphs (FGs)
As last type of probabilistic model representation we introduce factor graphs (FGs) which explicitly
represent the factorization properties of a function and have initially been used in the coding and image
processing community.
1.18.3.3.1
Deﬁnition
FGs are deﬁned using the notion of bipartite graphs. These are graphs G = (X, E) for which the set of
nodes can be divided into two disjoint subsets such that there are only edges between nodes from one
subset to nodes of the other subset, as follows:

1012
CHAPTER 18 Introduction to Probabilistic Graphical Models
X1
f1
X2
f2
f3
X3
f4
X4
FIGURE 18.12
Example of an FG.
Deﬁnition 26 (Factor Graph).
A factor graph is a tuple F = (G, { f1, . . . , fL}), where
•
G = (V, E) is an undirected bipartite graph such that V = X ∪F, where X, and F are disjoint, and
the nodes X = {X1, . . . , X N} correspond to RVs. The nodes X are variable nodes, while the nodes
F are factor nodes.
•
Further, f1, . . . , fL are positive functions and the number of functions L equals the number of nodes
in F = {F1, . . . , FL}. Each node Fi is associated with the corresponding function fi and each fi is
a function of the neighboring variables of Fi, i.e., fi = fi(NbG(Fi).
The factor graph F deﬁnes the joint probability distribution
PF(X1, . . . , X N) = 1
Z
L

l=1
fl(NbG(Fl)),
(18.75)
where Z is the normalization constant given as
Z =

x∈val(X)
L

l=1
fl(x(NbG(Fl))).
(18.76)
As each factor node corresponds to a factor we use both terms equivalently. We illustrate the deﬁnition
of FGs by the following example:
Example 19(FactorGraph). ConsiderFigure18.12.ThegivenfactorgraphF isF = (G, { f1, . . . , f4}),
where the nodes of G are the union of the set of variable nodes {X1, . . . , X4} and the set of factor
nodes {F1, . . . , F4}. The functions corresponding to these factor nodes are f1(X1, X2), f2(X1, X3),
f3(X2, X3), and f4(X3, X4). The represented joint probability density is
PF(X1, . . . , X4) = 1
Z f1(X1, X2) f2(X1, X3) f3(X2, X3) f4(X3, X4),
(18.77)
where Z is the normalization constant as given in the deﬁnition of FGs.

1.18.3 Representations
1013
X1
X2
X3
X4
(a)
X1
f1
X2
f2
X3
f3
X4
(b)
X1
X2
f
X3
X4
(c)
FIGURE 18.13
Conversion of a BN to an FG. (a) BN to be converted to an FG. (b) FG for representing the BN. (c) Alternative
FG for representing the BN.
1.18.3.3.2
BNs and MNs as FGs
FGs allow the representation of both BNs and MNs. However, note that there is in general no unique
transformation from a BN/MN to an FG. We illustrate the transformation of a BN or MN to an FG in
the following two examples:
Example 20 (Conversion of a BN to an FG).
The BN shown in Figure 18.13a can be represented as
the FG in Figure 18.13b where the factors f1, f2, f3 are given as
f1(X1, X2) = PX2(X2)PX1(X1|X2),
(18.78)
f2(X1, X2, X3) = PX3(X3|X1, X2),
(18.79)
f3(X3, X4) = PX4(X4|X3).
(18.80)
In this way, the factor graph represents the probability distribution
PF(X1, . . . , X4) = f1(X1, X2) f2(X1, X2, X3) f3(X3, X4),
(18.81)
= PX2(X2)PX1(X1|X2)PX3(X3|X1, X2)PX4(X4|X3),
(18.82)
which is exactly the one given by the BN. Alternatively, the BN could also be represented by the FG
shown in Figure 18.13c, where the factor f is given as
f (X1, . . . , X4) = PX2(X2)PX1(X1|X2)PX3(X3|X1, X2)PX4(X4|X3).
(18.83)
Example 21 (Conversion of an MN to an FG).
The MN in Figure 18.14a represents the factorization
PM(X1, . . . , X4) = 	C1(X1, X2, X3)	C2(X3, X4),
(18.84)
where the maximal cliques are C1 = {X1, X2, X3} and C2 = {X3, X4}, respectively. This factorization
can be represented by the FG in Figure 18.14b, where
f1(X1, X2, X3) = 	C1(X1, X2, X3),
and
(18.85)
f2(X3, X4) = 	C2(X3, X4).
(18.86)

1014
CHAPTER 18 Introduction to Probabilistic Graphical Models
X1
X2
X3
X4
(a)
X1
X2
f1
X3
f2
X4
(b)
FIGURE 18.14
Conversion of an MN to an FG. (a) MN to be converted to an FG. (b) FG for representing the MN.
1.18.3.4 Markov chains
To conclude the section on PGM representations we consider Markov chains (MCs), and how they can
be represented in each of the three types of PGMs. An MC is deﬁned as follows:
Deﬁnition 27 (Markov chain).
Let X1, . . . , X N be a sequence of RVs. These RVs form a Markov
chain if they satisfy
P(Xi|X1, . . . , Xi−1) = P(Xi|Xi−1),
∀i.
(18.87)
This is called Markov property.
In an MC the RVs preceding some given RV Xi are conditionally independent of all RVs succeeding
Xi, i.e.,
{X1, . . . , Xi−1} ⊥{Xi+1, . . . , X N}|Xi.
(18.88)
Furthermore, since any joint probability distribution P(X1, . . . , X N) factorizes as
P(X1, . . . , X N) =
N

i=1
P(Xi|Xi−1, . . . , X1),
(18.89)
this simpliﬁes due to the Markov property to
P(X1, . . . , X N) =
N

i=1
P(Xi|Xi−1).
(18.90)
We can represent MCs as BNs, MNs and FGs, as presented in the following:

1.18.3 Representations
1015
X1
X2
· · ·
XN
FIGURE 18.15
BN modeling an MC.
X1
X2
· · ·
XN
FIGURE 18.16
MN modeling an MC.
•
Bayesian network: A BN B modeling an MC is shown in Figure 18.15. The represented joint
distribution is
PB(X1, . . . , X N) =
N

i=1
P(Xi|PaG(Xi)),
(18.91)
= PB(X1, . . . , X N) = P(X1)
N

i=2
P(Xi|Xi−1),
(18.92)
where the only parent of Xi is Xi−1.
•
Markov network: The MN M for representing an MC in Figure 18.16 has the maximal cliques
Ci = {Xi, Xi+1} for i = 1, . . . , N −1. If we deﬁne the factor 	Ci to represent the function
P(Xi+1|Xi) for i = 2, . . . , N −1 and 	C1 = P(X1)P(X2|X1), then the MN speciﬁes the joint
probability distribution of an MC, i.e.,
PM(X1, . . . , X N) =
N−1

l=1
	Cl(Cl),
(18.93)
= P(X1)
N

i=2
P(Xi|Xi−1).
(18.94)
•
Factor graph: Finally, the FG F in Figure 18.17 with factors f1 = P(X1) and fi = P(Xi|Xi−1)
for i = 2, . . . , N speciﬁes the distribution of an MC, i.e.,
PF(X1, . . . , X N) =
N

l=1
fl(NbG(Fl)),
(18.95)
= P(X1)
N

i=2
P(Xi|Xi−1).
(18.96)

1016
CHAPTER 18 Introduction to Probabilistic Graphical Models
f 1
X 1
f 2
X 2
f 3
· · ·
f N
X N
FIGURE 18.17
FG modeling an MC.
1.18.4 Learning
Our goal is now to determine a PGM which models the variables of interest. One possibility is to
specify the PGM by hand. Especially BNs are suited to be speciﬁed by a domain expert, since the
directed edges can be treated as causal directions and local probability tables can be elicited relatively
easily. However, the more attractive approach might be to automatically determine a PGM from a set of
training data. Assume we want to ﬁnd a PGM for a set of random variables X = {X1, . . . , X N} which are
distributed according to an unknown joint distribution P∗(X). Further, assume that we have a collection
of L independent and identically distributed (i.i.d.) samples D = {X(1), x(2), . . . , x(L)}, drawn from
distribution P∗(X). In this section we assume complete data, i.e., each data case x(i) contains values of
all variables in X. For incomplete data, learning becomes much harder and is typically tackled using
expectation-maximization (EM) methods [11], or related techniques. The task of learning a PGM can be
described as follows: given D, return a PGM which approximates P∗(X) best. This type of learning is
known as generative learning, since we aim to model the process which generated the data. Generative
learning can be seen as a “multi-purpose” tool, since we directly strive for the primary object of interest:
the joint distribution P∗(X). All other quantities of the model which might be interesting, such as
marginal distributions of single variables/sets of variables, conditional distributions, or expectations,
can be derived from the joint distribution. However, it is rarely possible to exactly retrieve P∗(X),
especially when using a ﬁnite sample D. This can be obstructive, when the ultimate goal is not to
retrieve P∗(X), but to use the PGM in a specialized way. An example for this is when we want to use the
PGM as classiﬁer, i.e., we want to infer the value of a class variable, given the values of the remaining
variables. In the classiﬁcation context we are primarily interested in minimizing the loss resulting
from erroneous decisions, not in modeling the overall generative process. It can be easily shown that
knowledge of P∗(X) leads to optimal classiﬁcation, and therefore generative learning seems to be
suitable to learn classiﬁers. However, a somewhat surprising but consistently occurring phenomenon
in machine learning literature is that so-called discriminative learning methods, which refrain from
modeling P∗(X), but directly address the classiﬁcation problem, often yield better classiﬁcation results
than the generative approach. The discriminative paradigm is discussed in Section 1.18.4.3. For now,
let us focus on the generative approach.
1.18.4.1 Principles of generative learning
Let us assume a speciﬁc class of models C, e.g., the class of all BNs, or the class of all MNs. We deﬁne
a prior distribution P(C) over the model class, which represents our preference for certain models. For
example, if C is the class of all BNs, we may prefer networks with fewer edges, and therefore deﬁne a
prior which decreases with the number of edges. Furthermore, let D be a set of i.i.d. training samples

1.18.4 Learning
1017
drawn from the true distribution P∗(X). In the generative approach, we are interested in the posterior
probability distribution over the model class, conditioned on the given data. Using Bayes’ law, this
distribution is
P(C|D) = P(D|C)P(C)
P(D)
=
P(D|C)P(C)

C P(D|M′)P(M′)dM′ ∝P(D|C)P(C).
(18.97)
The data generation probability P(D|C) is widely known as likelihood L(C; D). The only difference
between P(D|C) and L(C; D) is that the likelihood is interpreted as a function of C, while the data
generation probability is viewed as a probability of D. In (18.97) we see, that P(C|D) is proportional
to the product of prior P(C) and likelihood L(C; D), i.e., the posterior P(C|D) represents an “updated”
version of the prior P(C), incorporating the likelihood L(C; D). There exist two general approaches to
use the posterior distribution for learning:
i. Maximum a-posteriori (MAP) approach: This approach aims to ﬁnd the most probable model
MMAP ∈C for given data, i.e.,
MMAP = arg max
M∈C P(M|D),
(18.98)
= arg max
M∈C L(M; D)P(M).
(18.99)
In the MAP approach we accept the single model MMAP as best explanation for the data and
consequently use it for further tasks.
ii. Bayesian approach: In the Bayesian approach, we refrain to decide for a single model out of C.
Instead, we maintain the uncertainty about which model might be the “true” one and keep the whole
model class C, together with the posterior distribution P(C|D).
To illustrate the difference of these two approaches, let us assume we are interested in the probability
of an unseen sample x′, after observing D. The MAP approach yields
P(X = x′|D) = P(x′|MMAP),
(18.100)
while the Bayesian approach results in
P(X = x′|D) =

C
P(x′|M)P(M|D)dM.
(18.101)
The Bayesian approach proceeds more carefully and treats the uncertainty by marginalizing over all
possible models. The MAP and the Bayesian approach often agree in the large sample limit, since for
large L the mass of the posterior P(C|D) is typically concentrated on a single model. For small sample
sizes, the Bayesian approach typically surpasses the MAP approach. However, it is rarely possible to
solve the integral in (18.101) exactly, which is the main drawback of the Bayesian approach. Typically,
(18.101) has to be approximated, e.g., by integrating (or summing) over a small portion of all models.
In the special case when the prior distribution P(C) is ﬂat, i.e., no model is preferred over the other,
the posterior P(C|D) is proportional to the likelihood L(C; D) and the MAP solution coincides with
the maximum-likelihood (ML) solution, i.e.,
MMAP = MML = arg max
M∈C L(M; D).
(18.102)

1018
CHAPTER 18 Introduction to Probabilistic Graphical Models
Since the data samples in D are i.i.d., the likelihood is given as
L(C; D) = P(D|C) =
L

l=1
P(x(l)|C).
(18.103)
In practise, it is often convenient and numerically more stable to work with the log-likelihood
log L(C; D) =
L

l=1
log P(x(l)|C).
(18.104)
Since the logarithm is an increasing function, maximizing the log-likelihood is equivalent to maximizing
the likelihood. Intuitively, (log-) likelihood is a measure of “goodness of ﬁt,” i.e., a model with maximum
likelihood is considered as best explanation for the training data. Furthermore, there is an interesting
connection between likelihood and information theory; For the sample D, the empirical data distribution
is deﬁned as
PD(x) = 1
L
L

l=1
1{x=x(l)},
(18.105)
where 1{·} is the indicator function. It is easily shown that maximizing the likelihood is equivalent to
minimize the Kullback-Leibler (KL) divergence between empirical distribution and model distribution,
which is given as
KL(PD||P( · |C)) =

x∈var(X)
PD(x) log PD(x)
P(x|C).
(18.106)
The KL divergence is a dissimilarity measure of the argument distributions. In particular, KL(P||Q) ⩾0
and KL(P||Q) = 0 if and only if P(X) and Q(X) are identical. Furthermore, KL(P||Q) measures the
minimal average coding overhead, for encoding data using the distribution Q(X), while the data is
actually distributed according to P(X) [5].
1.18.4.2 Generative learning of Bayesian networks
We now specify the model class C, and concentrate on the problem of learning BNs from data. As
discussed in Section 1.18.3.1, a BN B = (G, ) deﬁnes a distribution over the model variables X
according to
PB(X) =
N

i=1
P(Xi|PaG(Xi), θi),
(18.107)
which is a product over local probability distributions, one for each variable Xi. Equation (18.107)
highlights the two aspects of a BN: the structure G, which determines the parents for each variable,
and the parameters  = {θ1, θ2, . . . , θ N} which parameterize the local distributions. Although G and
 are coupled, e.g., the number of parameters in θi depends on G, an isolated consideration of G
and  is possible and reasonable. This leads to the two tasks of learning BNs: parameter learning
and structure learning. For parameter learning, we assume that the network structure G is ﬁxed, either

1.18.4 Learning
1019
because a domain expert speciﬁed the independence relationships among the model variables, or because
a structure learning algorithm found an appropriate structure. For ﬁxed structure G, we introduce the
shorthand Pai = PaG(Xi) and xPai = x(PaG(Xi)). In the next section we discuss ML parameter
learning, and in Section 1.18.4.2.2 we introduce a full Bayesian approach for BN parameter learning.
In Section 1.18.4.2.3 we address the problem of learning also the structure G from data.
1.18.4.2.1
Maximum likelihood parameter learning
For an i.i.d. sample D, the log-likelihood of a BN model is given as
log L(B; D) =
L

l=1
N

i=1
log P

x(l)
i |x(l)
Pai , θi	
=
N

i=1
ℓ(θi, D).
(18.108)
Equation (18.108) shows that the log-likelihood decomposes, i.e., log L(B; D) equals to a sum over
local terms, one for each variable, given as
ℓ(θi, D) =
L

l=1
log P

x(l)
i |x(l)
Pai , θi	
.
(18.109)
Thus the overall BN log-likelihood is maximized by maximizing the individual local terms w.r.t. the
local parameters θi. This holds true as long as each local term ℓ(θi; D) only depends on θi. If there
exists a coupling between the parameters, as for instance introduced by parameter tying, we loose this
decomposition property.
Maximum likelihood parameter learning for discrete variables: If the variables X are discrete, the
most general CPDs are discrete distributions (cf. Example 13):
P(Xi = j|Pai = h, θi) = θi
j|h.
(18.110)
In order to maximize the likelihood, we aim to individually maximize the local terms ℓ(θi; D) (18.109),
which are given as
ℓ(θi; D) =
L

l=1
log
⎛
⎝
sp(Xi)

j=1

h∈val(Pai)
θi
j|h
ui,l
j|h
⎞
⎠,
(18.111)
=

h

l:xl∈Dh
sp(Xi)

j=1
ui,l
j|h log θi
j|h,
(18.112)
=

h
log L

θi
·|h; Dh
	
.
(18.113)
Here ui,l
j|h = 1{x(l)
i = j and x(l)
Pai =h}. The data set Dh is a subset of the training data D, containing those
samples x(l) where x(l)
Pai = h. In (18.112) sums are interchanged and some terms which would be

1020
CHAPTER 18 Introduction to Probabilistic Graphical Models
evaluated to zero, are discarded. In (18.113) the local terms ℓ(θi; D) decompose to a sum of local
log-likelihoods
log L

θi
·|h; Dh
	
=

l:xl∈Dh
sp(Xi)

j=1
ui,l
j|h log θi
j|h.
(18.114)
Inserting this result into (18.108), we see that the BN log-likelihood is given as a sum of local
log-likelihoods:
log L(B; D) =
N

i=1

h∈val(Pai)
log L

θi
·|h; Dh
	
(18.115)
Since each θi
·|h only occurs in a single summand in (18.115), the overall log-likelihood is maximized by
individually maximizing each local log-likelihood. Note that θi
·|h are the parameters of Xi conditioned
on a particular h, i.e., they are the parameters of single dimensional discrete distributions. Finding these
ML parameters is easy. For notational simplicity, we derive the ML estimate for a generic variable Y
with data DY = {y(1), . . . , y(L)} and generic parameters θY = (θY
1 , . . . , θY
sp(Y)). The ML estimate ˆθ
Y
is found by solving the optimization problem:
maximizeθ y
L

l=1
sp(Y)

j=1
ul
j log θY
j ,
(18.116)
s.t.
sp(Y)

j=1
θY
j = 1.
(18.117)
Here ul
j is the indicator function 1{y(l)= j}. The Lagrangian function of this problem is given as
L(θY , ν) =
L

l=1
sp(Y)

j=1
ul
j log θY
j + ν
⎛
⎝1 −
sp(Y)

j=1
θY
j
⎞
⎠.
(18.118)
Since the objective in (18.116) is concave and we have a linear equality constraint (18.117), stationarity
of L(θY , ν) is necessary and sufﬁcient for optimality [12], i.e., the constraint (18.117) has to hold and
the partial derivatives w.r.t. θY
j have to vanish:
∂L(θY , ν)
∂θY
j
=
L

l=1
ul
j
1
θY
j
−ν != 0.
(18.119)
We see that the optimal parameters have the form
ˆθY
j =
L
l=1 ul
j
ν
= n j
ν ,
(18.120)

1.18.4 Learning
1021
where n j = L
l=1 ul
j is the number of occurrences of Y = j in the data set DY . Since (18.117) has
to hold, we see that ν has to be equal to sp(Y)
j=1 n j, and therefore the ML parameters are simply the
relative frequency counts according to DY :
ˆθY
j =
n j
sp(Y)
j′=1 n j′
.
(18.121)
Applying this result to θi
·|h, we obtain the ML parameters as
ˆθi
j|h =
L
l=1 ui,l
j|h
sp(X j)
j′=1
L
l=1 ui,l
j′|h
.
(18.122)
This estimate is well-deﬁned if there is at least one sample in the subset Dh. If there is no sample in
Dh, an arbitrary distribution (e.g., the uniform distribution) can be assumed, since in this case θi
·|h does
not contribute to the likelihood.
Maximum likelihood parameter learning for conditional Gaussian variables: Now let us assume that
the variables are real-valued, and that the CPDs are conditional Gaussians (cf. Example 14), i.e.,
P(Xi|Pai, θi) = N

Xi
αT
i xPai + βi; σ 2 	
,
(18.123)
where we assume a ﬁxed shared variance σ 2. Again, in order to maximize the BN likelihood, we aim
to individually maximize the local terms ℓ(θi; D) in (18.109). Inserting the logarithm of (18.123) in
(18.109) yields
ℓ(θi; D) =
L

l=1
⎡
⎢⎣−1
2 log (2πσ 2) −

x(l)
i
−αT
i x(l)
Pai −βi
	2
2σ 2
⎤
⎥⎦
(18.124)
= −1
2σ 2
L

l=1

x(l)
i
2 + αT
i x(l)
Pai x(l)T
Pai αi + β2
i −2x(l)
i αT
i x(l)
Pai
−2x(l)
i βi + 2αT
i x(l)
Pai βi

+ const.,
(18.125)
where additive terms which are independent of αi and βi are summarized in a constant. The overall
log-likelihood of the BN is maximized by individually maximizing (18.124) w.r.t. θi = {αi, βi}. The
function ℓ(θi; D) is the negative square of an afﬁne function and therefore concave in αi and βi [12].
Thus, a maximum can be found be setting the partial derivatives to zero. For βi we get:
∂ℓ(θi; D)
∂βi
= −1
2σ 2
L

l=1

2βi −2x(l)
i
+ 2x(l)T
Pai αi
 != 0.
(18.126)
Hence the ML parameter ˆβi is given as
ˆβi = 1
L
L

l=1

x(l)
i
−x(l)T
Pai αi

= ˆxi −ˆxT
Pai αi,
(18.127)

1022
CHAPTER 18 Introduction to Probabilistic Graphical Models
where ˆxi and ˆxPai are the sample means of Xi and Pai, respectively. For αi we have
∇αi ℓ(θi; D) = −1
2σ 2
L

l=1

2x(l)
Pai x(l)T
Pai αi −2x(l)
i x(l)
Pai + 2x(l)
Pai βi
 !=0.
(18.128)
Using ˆβi in (18.128), we get the ML parameters ˆαi by the following chain of equations:
L

l=1

x(l)
Pai

x(l)T
Pai −ˆxT
Pai
	
ˆαi =
L

l=1

x(l)
i
−ˆxi
	
x(l)
Pai

,
(18.129)
L

l=1

x(l)
Pai −ˆxPai + ˆxPai
	 
x(l)T
Pai −ˆxT
Pai
	
ˆαi =
L

l=1

x(l)
i
−ˆxi
	 
x(l)
Pai −ˆxPai + ˆxPai
	
,
(18.130)
1
L −1
L

l=1

x(l)
Pai −ˆxPai
	 
x(l)T
Pai −ˆxT
Pai
	
ˆαi =
1
L −1
L

l=1

x(l)
i
−ˆxi
	 
x(l)
Pai −ˆxPai
	
,
(18.131)
ˆCPa ˆαi = ˆc,
(18.132)
ˆαi = ˆC−1
Pa ˆc.
(18.133)
Here ˆCPa is the estimated covariance matrix of the parent variables, and ˆc is the estimated covariance
vector between parents and the child. In (18.131) we used the fact that L
l=1 v

x(l)T
Pai −ˆxT
Pai
	
= 0 and
L
l=1

x(l)
i
−ˆxi
	
v = 0, for any vector v which does not depend on l.
1.18.4.2.2
Bayesian parameter learning
In a Bayesian approach, we do not assume that there exists a single set of “true” parameters, but
maintain the uncertainty about the parameters. Therefore, we explicitly include the parameters into the
model as in Figure 18.18, where a BN over variables X1, . . . , X4 is augmented with their local CPD
parameters θ1, . . . , θ4. Learning in the Bayesian setting means to perform inference over parameters;
in the Bayesian view, there is actually no difference between learning and inference.
Note that there are no edges between any θi and θ j, which means that we assume parameter inde-
pendence. This kind of independence is called global parameter independence (as opposed to local
parameter independence, discussed below) [1,13]. Global parameter independence, although reason-
able in many cases, does not necessarily hold in every context. However, in the case of global parameter
independence any prior over  = {θ1, . . . , θ N} factorizes according to
P() =
N

i=1
P(θi).
(18.134)
Furthermore, if all model variables X1, . . . , X N are observed, then the local parameters θ1, . . . , θ N are
pairwise d-separated, and thus independent. Furthermore, the same is true for multiple i.i.d. samples,

1.18.4 Learning
1023
θ 1
θ 2
X1
X2
θ 3
X3
X 4
θ 4
FIGURE 18.18
Bayesian network over variables X1, . . . ,X4, augmented with local CPD parameters θ1, . . . ,θ4.
i.e., for complete data sets D. In this case, if global (a-priori) parameter independence holds, also
a-posteriori parameter independence holds, and the posterior over  factorizes according to
P(|D) =
N

i=1
P(θi|D).
(18.135)
Combining (18.135) with (18.107), we obtain the model posterior as
PB(X, |D) = PB(X|)P(|D),
(18.136)
=
 N

i=1
P(Xi|Pai, θi)
  N

i=1
P(θi|D)

,
(18.137)
=
N

i=1
P(Xi|Pai, θi)P(θi|D).
(18.138)
Since one is usually interested in the data posterior P(X|D), one has to marginalize over :
P(X|D) =


P(X, |D)d,
(18.139)
=

θ1

θ2 · · ·

θ N
 N

i=1
P(Xi|Pai, θi)P(θi|D)

dθ1dθ2 . . . dθ N,
(18.140)
=
N

i=1

θi P(Xi|Pai, θi)P(θi|D)dθi

.
(18.141)
The exchange of integrals and products in (18.141) is possible since each term in the product in (18.140)
depends only on a single θi. Hence, under the assumption of global parameter independence and
completely observed data, the integral over the entire parameter space in (18.139) factorizes into a
product of several simpler integrals (18.141).

1024
CHAPTER 18 Introduction to Probabilistic Graphical Models
θ
X
FIGURE 18.19
Simple Bayesian network containing a single variable X , augmented with parameters θ.
Bayesianparameterlearningfordiscretevariables:Wenowreturntotheexample,whereallvariables
of the BN are discrete. As a basic example, and in order to introduce concepts which are required later,
let us consider a simple BN with only a single variable, i.e., N = 1 and set X = X1 and J = sp(X).
The probability of X being in state j is
P(X = j|θ) = θ j.
(18.142)
Figure 18.19 shows the corresponding BN, where θ is explicitly included. The parameters can be
represented as a vector θ = (θ1, . . . , θJ)T of RVs, which has to be an element of the standard simplex
S =

θ|θ j ⩾0, ∀j, J
j=1 θ j = 1
 
. We need to specify a prior distribution over θ, where a natural
choice is the Dirichlet distribution, deﬁned as
Dir(θ; α) =

1
B(α)
!J
j=1 θ
α j−1
j
if θ ∈S,
0
otherwise.
(18.143)
The Dirichlet distribution is parameterized by the vector α = (α1, . . . , αJ)T with α j > 0 for all j. The
term
1
B(α) is the normalization constant of the distribution, where the beta-function B(α) is given as
B(α) =

S
J
j=1
θ
α j−1
j
dθ =
!J
j=1 (α j)

J
j=1 α j
	.
(18.144)
Here (·) is the gamma function, i.e., a continuous generalization of the factorial function. In the special
case that α j = 1, ∀j, the Dirichlet distribution is uniform over all θ ∈S.
Assuming a Dirichlet prior over θ, the distribution of the BN in Figure 18.19 is given as P(X, θ) =
PB(X|θ)Dir(θ; α). Usually one is interested in the posterior over X, which is obtained by marginalizing
over θ:
P(X = j′) =

θ
P(X|θ)Dir(θ; α)dθ,
(18.145)
=

S
θ j′
1
B(α)
J
j=1
θ
α j−1
j
dθ,
(18.146)

1.18.4 Learning
1025
=
1
B(α)
(α j′ + 1) !
j̸= j′ (α j)


1 + J
j=1 α j
	
,
(18.147)
= B(α)
B(α)
α j′
J
j=1 α j
,
(18.148)
=
α j′
J
j=1 α j
.
(18.149)
The integral in (18.146) can be solved using (18.144), and (18.148) follows from the properties of
the gamma function. We see that (18.148) has the form of relative frequency counts, exactly as
the ML estimator in (18.122). Thus, the Dirichlet parameters α gain the interpretation of a-priori
pseudo-data-counts, which represent our belief about θ. Now assume that we have an i.i.d. sample
D = (x(1), . . . , x(L)) of variable X. Learning in the Bayesian framework means to incorporate the
information given by D, i.e., to modify our belief about θ. Formally, we want to obtain the posterior
distribution over θ:
P(θ|D) ∝P(D|θ)P(θ) =
 L

l=1
P(x(l)|θ)

Dir(θ; α),
(18.150)
=
⎛
⎝
L

l=1
J
j=1
θ
ul
j
j
⎞
⎠
⎛
⎝
1
B(α)
J
j=1
θ
α j−1
j
⎞
⎠,
(18.151)
=
1
B(α)
J
j=1
θ
n j+α j−1
j
,
(18.152)
where ul
j = 1{x(l)= j} and n j = L
l=1 ul
j. Note that (18.152) has again the functional form of a Dirichlet
distribution, although not correctly normalized. Hence, we can conclude that the posterior P(θ|D) is
also a Dirichlet distribution. After normalizing, we obtain
P(θ|D) = Dir(θ; α + n),
(18.153)
where n = (n1, n2, . . . , n J)T . The property that the posterior is from the same family of distributions
as the prior, comes from the fact that the Dirichlet distribution is a so-called conjugate prior [4] for the
discrete distribution. The intuitive interpretation of (18.153) is that the distribution over θ is updated
by adding the real data counts n to our a-priori pseudo-counts α. Replacing the prior P(θ) with the
posterior P(θ|D) in 18.145–18.148 yields
P(X = j′|D) =
α j′ + n′
j
J
j=1
"
α j + n j
#.
(18.154)
Now we extend the discussion to general BNs. First, we interpret the CPD parameters  =
{θ1, θ2, . . . , θ N} as RVs, i.e., we deﬁne a prior P(). We assume global parameter independence,

1026
CHAPTER 18 Introduction to Probabilistic Graphical Models
θ3
·|h 1
. . .
θ3
·|h q
θ1
X 1
X 3
θ2
X 2
FIGURE 18.20
Bayesian network over variables X1,X2,X3,X4.
such that the prior takes the form P() = !N
i=1 P(θi). Note that θi can be interpreted as a matrix
of RVs, containing one column θi
·|h for each possible assignment h of the parent variables Pai. We
can additionally assume that the columns of θi are a-priori independent, i.e., that the prior over θi
factorizes as
P(θi) =

h∈val(Pai)
P

θi
·|h
	
.
(18.155)
This independence assumption is called local parameter independence, as incontrast toglobal parameter
independence [1,13]. Similar, as global parameter independence, local parameter independence is often
a reasonable assumption, but has to be considered with care. For a speciﬁc assignment h, the vector
θi
·|h =

θi
1|h, . . . , θi
sp(Xi)|h
	T
represents a single (conditional) discrete distribution over Xi. As in the
basic example with only one variable, we can assume a Dirichlet prior for θi
·|h, each with its own
parameters αi
h =

αi
1|h, . . . , αi
sp(Xi)|h
	
. Together with the assumption of global and local parameter
independence, the prior over the whole parameter set  is then given as
P() =
N

i=1

h
Dir

θi
·|h; αi
h
	
.
(18.156)
For global parameter independence, we observed that the parameters θ1, . . . , θ N remain independent,
when conditioned on a completely observed data set (cf. (18.134) and (18.135)) since the parameters are
d-separated by the observed nodes X1, . . . , X N. For local parameter independence, it does not happen
that any two sets of parameters θi
·|h and θi
·|h′, h ̸= h′, are d-separated.
Example 22.
Consider Figure 18.20 and let q = sp(X1)sp(X2) be the total number of parent
conﬁgurations for node X3. The parameter sets θ3
·|h1, . . . , θ3
·|hq are not d-separated, since X3 is observed.
However, note that d-separation is sufﬁcient for conditional independence, but not necessary. This means
that even if two sets of nodes are not d-separated, they still can turn out to be independent. Fortunately,
this is the case here. To see this, assume that local (a-priori) parameter independence holds, and that
all nodes X1, . . . , X N are observed. For some arbitrary node Xi, the posterior parameter distribution is

1.18.4 Learning
1027
given as
P(θi|X) = P(θi|Xi, Pai) = P(Xi|θi, Pai)P(θi|Pai)
P(Xi|Pai)
,
(18.157)
=
P

Xi|θi
·|Pai , Pai
	 !
h P

θi
·|h
	
P(Xi|Pai)
,
(18.158)
=
!
h f (θi
·|h, X)
P(Xi|Pai) ,
(18.159)
where
f (θi
·|h, X) =
⎧
⎨
⎩
P

Xi|θi
·|h, Pai
	
P

θi
·|h
	
if h = Pai,
and
P

θi
·|h
	
otherwise.
(18.160)
We see that the posterior parameter distribution factorizes over the parent conﬁgurations, i.e., that the
parameters θi
·|h1, . . . , θi
·|hq, q = sp(Pai), remain independent when conditioned on X. This conclusion
comes from two observations in (18.158): (i) All θi
·|h except θi
·|Pai are independent from Xi, when Pai
is given, and (ii) θi is a-priori independent from Pai.
Furthermore, local a-posteriori parameter independence also holds for multiple observed i.i.d.
samples D, i.e.,
P(θi|D) =

h
P

θi
·|h|D
	
.
(18.161)
Since we assumed Dirichlet priors, which are conjugate priors for discrete distributions, the posterior
over θi takes the form (cf. (18.153)),
P(θi|D) =

h
Dir

θi
·|h|αi
h + ni
h
	
,
(18.162)
where ni
h =

ni
1|h, ni
2|h, . . . , ni
sp(Xi)|h
	T
, and ni
j|h = L
l=1 ui,l
j|h. This result can now be applied to
(18.141), where we showed (under the assumption of global parameter independence), that
P(X|D) =
N

i=1

θi P

Xi|Pai, θi	
P(θi|D)dθi

.
(18.163)

1028
CHAPTER 18 Introduction to Probabilistic Graphical Models
Inserting (18.162) in (18.163), we obtain
P(X = x|D) =
N

i=1
⎛
⎝

θi
⎛
⎝
sp(Xi)

j=1

h
θi
j|hui
j|h
⎞
⎠

h
Dir

θi
·|h|αi
h + ni
h
	
dθi
⎞
⎠,
(18.164)
=
N

i=1
⎛
⎝

θi

h
⎡
⎣
⎛
⎝
sp(Xi)

j=1
θi
j|h
ui
j|h
⎞
⎠Dir

θi
·|h|αi
h + ni
h
	
⎤
⎦dθi
⎞
⎠,
(18.165)
=
N

i=1
⎛
⎝

θi
·|h1
. . .

θi
·|hqi

h
⎡
⎣
⎛
⎝
sp(Xi)

j=1
θi
j|h
ui
j|h
⎞
⎠Dir

θi
·|h|αi
h + ni
h
	
⎤
⎦dθi
·|h1 · · · dθi
·|hqi
⎞
⎠,(18.166)
=
N

i=1

h
⎛
⎝

θi
·|h
⎛
⎝
sp(Xi)

j=1
θi
j|h
ui
j|h
⎞
⎠Dir

θi
·|h|αi
h + ni
h
	
dθ·|h
⎞
⎠,
(18.167)
where qi = sp(Pai). The integration over the entire parameter space factorizes into a product of many
simpler integrals. Each integral in (18.167) has the same form as in (18.145) and is given as

θi ·|h
⎛
⎝
sp(Xi)

j=1
θi
j|h
ui
j|h
⎞
⎠Dir

θi
·|h|αi
h + ni
h
	
dθi
·|h =
⎧
⎨
⎩
αi
xi |h+ni
xi |h
sp(Xi )
j=1

αi
j|h+ni
j|h
	 if h = xPai ,
1
otherwise.
(18.168)
Therefore, under the assumptions of global and local parameter independence and using Dirichlet
parameter priors, the Bayesian approach is feasible for BNs with discrete variables and reduces to
a product over closed-form integrations. This result can be seen as the Bayesian analog to (18.115),
where we showed that the BN likelihood is maximized by solving many independent closed-form ML
problems.
1.18.4.2.3
Learning the structure of Bayesian networks
So far we have discussed parameter learning techniques for BNs, i.e., we assumed that the BN structure
G is ﬁxed and learned the parameters . The goal is now to also learn the structure G from data, where
we restrict the discussion to discrete variables from now on. Generally there are two main approaches
to structure learning:
i. Constraint-based approach: In this approach, we aim to ﬁnd a structure G which represents the same
conditional independencies as present in the data. While being theoretically sound, these methods
heavily rely on statistical independence tests performed on the data, which might be inaccurate and
usually render the approach unsuitable in practise. Therefore, we do not discuss constraint-based
approaches here, but refer the interested reader to [1,14,15], and references therein.

1.18.4 Learning
1029
ii. Scoring-based approach: Here we aim to ﬁnd a graph which represents a “suitable” distribution to
represent the true distribution P∗(X), where “suitability” is measured by a scoring function score
G. There are two key issues here: Deﬁning an appropriate scoring function, and developing a search
algorithm which ﬁnds a score-maximizing structure G.
A natural choice for the scoring function would be the log-likelihood, i.e., the aim to maximize
log L(B; D) = log L(G, ; D), w.r.t. G and . Using the ML parameter estimates ˆ from Section
1.18.4.2.1, this problem reduces to maximize
log L(G, ˆ; D) =
L

l=1
log P(X = x(l)|G, ˆ),
(18.169)
w.r.t. G. However, the likelihood score in (18.169) is in general not suitable for structure learning. When
G is a subgraph of G′, it is easy to show that a BN deﬁned over G′ contains all distributions which can
also be represented by a BN deﬁned over G. Consequently, the supergraph G′ will always have at least
the same likelihood-score as G, and furthermore, any fully connected graph will also be a maximizer
of (18.169). We see that likelihood prefers more complex models and leads to overﬁtting. Note that
overﬁtting stems from using a too ﬂexible model class for a ﬁnite data set, and that the ﬂexibility of a
model is reﬂected by the number of free parameters. The number of parameters T (G) in a BN is given
as
T (G) =
N

i=1
(sp(Xi) −1)(sp(PaG(Xi))),
(18.170)
which grows exponentially with the maximal number of parents. The likelihood-score merely aims to
ﬁt the training data, but is blind to model complexity.
As a remedy, we can modify the pure likelihood-score in order to account for model complexity. One
way delivers the minimum description length (MDL) principle [16], which aims to ﬁnd the shortest,
most compact representation of the data D. As discussed in Section 1.18.4.1, maximizing likelihood
is equivalent to minimizing KL(PD||P( · |C)), i.e., the KL divergence between the empirical data
distribution and the model distribution. The KL divergence measures the average coding overhead for
encoding the data D using P(X|C) instead of PD(X). The MDL principle additionally accounts for the
description length of the model, which is measured by the number of parameters. For BNs with discrete
variables, the following MDL score was derived [17,18]:
MDL(G) = −
L

l=1
log P(x(l)|G, ML) + log L
2
T (G),
(18.171)
which is a negative score, since we aim to ﬁnd a graph G minimizing MDL(G). The MDL score is
simply the negative likelihood score (18.169), plus a penalization term for the number of parameters. In
typical applications, the likelihood decreases linearly with L, while the penalization term in (18.171)
grows logarithmically. Therefore, for large L, the likelihood dominates the penalization term and the
MDL score becomes equivalent to likelihood. However, when L is small, then the penalization term
signiﬁcantly inﬂuences (18.171), resulting in a preference for BNs with fewer parameters.

1030
CHAPTER 18 Introduction to Probabilistic Graphical Models
An alternative way to avoid overﬁtting is delivered by a Bayesian approach. The main problem
with the likelihood score (18.169) is that it is not aware of model complexity, i.e., the number of free
parameters. While with more densely connected graphs G the dimensionality of the parameter space
 increases exponentially, nevertheless only a single point estimate is used from this space, i.e., the
ML parameters ML. Therefore, the ML approach can be described as overly optimistic, trusting in
the single “true” parameters ML. In contrast to the ML approach, the Bayesian approach (cf. Section
1.18.4.2.2) uses the parameter space in its entirety. We introduce a prior distribution P() over the
parameters, and marginalize over , leading to the Bayesian score
Bayes(G) = P(D|G) =


P(D|, G)P()d,
(18.172)
=


L

l=1
P(x(l)|, G)P()d.
(18.173)
We see that this score actually represents a semi-Bayesian approach: While being Bayesian about , we
still follow the ML principle for the structure G. A Bayesian approach, which requires approximation
techniques, is provided in [19]. Assuming global and local parameter independence, and using Dirichlet
priors for the local parameters (cf. Section 1.18.4.2.2) leads to the Bayesian-Dirichlet score (BD) [20,21]
BD(G) =


⎛
⎝
L

l=1
N

i=1
sp(Xi)

j=1

h
θi
j|h
ui,l
j|h
⎞
⎠
⎛
⎝
N

i=1

h
1
B(αi
h)
sp(Xi)

j=1
θi
j|h
αi
j|h−1
⎞
⎠d, (18.174)
=


N

i=1

h
1
B(αi
h)
sp(Xi)

j=1
θi
j|h
αi
j|h+ni
j|h−1d,
(18.175)
=
N

i=1

h
1
B(αi
h)
⎛
⎝

θi
·|h
sp(Xi)

j=1
θi
j|h
αi
j|h+ni
j|h−1dθi
·|h
⎞
⎠,
(18.176)
=
N

i=1

h
B(αi
h + ni
h)
B(αi
h)
,
(18.177)
=
N

i=1

h

sp(Xi)
j=1
αi
j|h
	

sp(Xi)
j=1
αi
j|h + ni
j|h
	
sp(Xi)

j=1


αi
j|h + ni
j|h
	


αi
j|h
	
.
(18.178)
In (18.176) we can interchange integrals with products, since each term only depends on a single θ i
·|h. The
solution of the integrals is given in (18.144). Heckerman et al. [13] pointed out that the parameters αi
h can
not be chosen arbitrarily, when one desires a consistent, likelihood-equivalent score, meaning that the
same score is assigned for two networks representing the same set of independence assumptions. They
show how such parameters can be constructed, leading to the likelihood-equivalent Bayesian-Dirichlet

1.18.4 Learning
1031
score (BDe). A simple choice for αi
h, already proposed in [20], is
αi
j|h =
A
sp(Xi)sp(PaG(Xi)),
(18.179)
where A is the equivalent sample size, i.e., the number of virtual a-priori samples (a typical value would
be A = 1). These parameters result from the a-priori assumption of an uniform distribution over the state
space of X. Consequently, the BDe score using parameters according (18.179) is called BDe uniform
score (BDeu).
Furthermore, there is a connection between the BD score and the MDL score: It can be shown [1], that
for L →∞, the logarithm of the BD score converges to the so-called Bayesian information criterion
score (BIC)
BIC(G) =
L

l=1
log P(x(l)|G, ML) −log N
2
T (G) + const.,
(18.180)
which is the negative MDL score (18.171), up to some constant. Thus, although derived from two
different perspectives, the MDL score and the BIC score are equivalent.
We now turn to the problem of ﬁnding a graph G which maximizes a particular score. All the scores
discussed so far, namely log-likelihood (18.169), MDL (18.171) and BIC (18.180), and (the logarithm
of) the BD/BDe score (18.178) can be written in the form
score(G) =
N

i=1
scorei(Xi, PaG(Xi)),
(18.181)
which means that the global network score decomposes into a sum of local scores, depending only
on the individual variables and their parents. This property is very important to ﬁnd efﬁcient structure
learning algorithms. For example, if an algorithm decides to add a parent to some variable Xi, then the
overall score is affected only via the the change of a single additive term. Therefore, decomposability
of the scoring function alleviates the problem of ﬁnding a score-maximizing graph G. Unfortunately,
even for decomposable scores the problem of ﬁnding a score-maximizing BN structure G is NP-hard in
general, even if the maximum number of parents is restricted to K ⩾2 [22,23]. For K = 1, i.e., when
the BN is constrained to be a directed tree, the Chow-Liu algorithm [24] learns the optimal network
structure in polynomial time. Here the structure learning problem is reduced to a maximum spanning
tree problem, where the edge weights are given by mutual information estimates. It is shown [24] that
this is equivalent to maximizing the log-likelihood over the class of BNs with a directed tree structure.
The class of directed trees is typically restrictive enough to avoid overﬁtting, i.e., the likelihood score
is an appropriate choice in this case.
For learning more complicated structures, there exist a large variety of approximative techniques.
One of the ﬁrst general search algorithms was the K2 algorithm [21] which relies on an initial variable
ordering, and greedily adds parents for each node, in order to increase the BD score. A more general
scheme is hill-climbing (HC) [13,25], which does not rely on an initial variable ordering. HC starts
with an unconnected graph G. In each iteration, the current graph G is replaced with a neighboring
graph G′ with maximal score, if score(G′) > score(G). Neighboring graphs are those graphs which

1032
CHAPTER 18 Introduction to Probabilistic Graphical Models
can be created from the current graph by single edge-insertions, edge-deletions and edge-reversals,
maintaining acyclicity constraints. When no neighboring graph has higher score than the current graph,
the algorithm stops. HC greedily increases the score, and will typically terminate in a local maximum.
HC can be combined with tabu-search [26] in order to escape local maxima: When the algorithm is
trapped in a local maximum, then the last several iterations leading to the local maximum are reverted
and marked as forbidden. In this way, the algorithm is forced to use alternative iterations, possibly
leading to a better local maximum. A related method is simulated annealing (SA) which randomly
generates a neighbor solution in each step. If the new solution has higher score than the current one, it is
immediately accepted. However, also a solution with lower score is accepted with a certain probability.
While this probability is high in the beginning, leading to a large exploration over the search space,
it is successively reduced according to a cooling schedule. At the end of the search process, SA only
accepts score-increasing solutions. [25] applied SA for BN structure learning. While these heuristic
methods perform a greedy search in the space of all DAGs, [27] proposed to greedily search over the
space of equivalence classes. An equivalence class contains all BN structures which represent the same
set of independence assumptions. A related approach can be found in [28], which proposes to perform a
search over possible variable orderings. This approach uses the fact, as already mentioned in [21], that
for a given variable ordering the optimal structure can be found in polynomial time.
Recently, several exact approaches for ﬁnding globally optimal structures have been proposed. Meth-
ods based on dynamic programming [29–31] have exponential time and memory requirements, which
restricts their application to approximately 30 variables. Furthermore, there are branch-and-bound meth-
ods [32–34] which cast the combinatorial structure learning problem to a relaxed continuous problem.
The relaxed continuous problem is successively divided (branched) into sub-problems, until the solu-
tions of the relaxed sub-problems coincide with feasible structures, i.e., DAGs. The scores of the relaxed
solutions provide an upper bound of the achievable score, and each feasible solution provides a lower
bound of this score. Branches whose upper bound is lower than the currently best lower bound can
consequently be pruned. Although branch-and-bound methods also have an exponential runtime, they
provide two key advantages: (i) They maintain (after some initial time) a feasible solution, i.e., they
can be prematurely terminated, returning the currently best solution. Methods based on dynamic pro-
gramming do not offer this possibility; (ii) They provide upper and lower bounds on the maximal score,
which represents a worst-case certiﬁcate of sub-optimality.
1.18.4.3 Discriminative learning in Bayesian networks
So far, we discussed generative learning of BNs, aiming to retrieve the underlying true distribution
P∗(X) from a sample D = (x(1), . . . , x(L)). Now, let us assume that our ﬁnal goal is classiﬁcation, i.e.,
we aim to predict the value of a class variable C ∈X, given the values of the features Z = X \ {C}.
Without loss of generality, we can assume that C is X1, and we renumber the remaining variables, such
that X = {C, Z} = {C, X1, X2, . . . , X N−1}. In classiﬁcation, we are interested in ﬁnding a decision
function f (Z), which can take values in {1, . . . , sp(C)}, such that the expected classiﬁcation rate
CR( f ) = EP∗[1{ f (z)=c}]
(18.182)
=

{c,z}∈val({C,Z})
P∗(c, z)1{ f (z)=c}
(18.183)

1.18.4 Learning
1033
is maximized. It is easily shown that a function f ∗(Z) maximizing (18.182) is given by [35]
f ∗(Z) = arg max
c∈val(C) P∗(c|Z) = arg max
c∈val(C) P∗(c, Z).
(18.184)
Therefore, since the generative approach aims to retrieve P∗(X), we are in principle able to learn an
optimal classiﬁer, when (i) we use a sufﬁciently expressive model, (ii) we have a sufﬁcient amount of
training data, and (iii) we train the model using a generative approach, such as ML. Classiﬁers trained
this way are called generative classiﬁers. Generative classiﬁers are amenable to interpretation and for
incorporating prior information, and are ﬂexible to versatile inference scenarios. However, we rarely
have sufﬁcient training data, and typically we restrict the model complexity, partly to avoid overﬁtting
(which occurs due to a lack of training data), partly due to computational reasons. Therefore, although
theoretically sound, generative classiﬁers usually do not yield the best classiﬁcation results.
Discriminative approaches, on the other hand, directly represent aspects of the model which are
important for classiﬁcation accuracy. For example, some approaches model the class posterior proba-
bility P(C|Z), while others, such as support vector machines (SVMs) [36,37] or neural networks [4,38],
model information about the decision function, without using a probabilistic model. In each case, a dis-
criminative objective does not necessarily agree with accurate joint distribution estimates, but rather
aims for a low classiﬁcation error on the training data. Discriminative models are usually restricted to
the mapping from observed input features Z to the unknown class output variable C, while inference of
Z for given C is impossible or unreasonable. There are several reasons for using discriminative rather
than generative classiﬁers, one of which is that the classiﬁcation problem should be solved most simply
and directly, and never via a more general problem such as the intermediate step of estimating the joint
distribution [39]. Discriminative classiﬁers typically lead to better classiﬁcation performance, particu-
larly when the class conditional distributions poorly approximate the true distribution [4]. The superior
performance of discriminative classiﬁers has been reported in many application domains.
When using PGMs for classiﬁcation, both parameters and structure can be learned using either gen-
erative or discriminative objectives [40]. In the generative approach, the central object is the parameter
posterior P(|D), yielding the MAP, ML and the Bayesian approach. In the discriminative approach,
we use alternative objective functions such as conditional log-likelihood (CLL), classiﬁcation rate (CR),
or margin, which can be applied for both structure learning and for parameter learning. Unfortunately,
while essentially all generative scores (likelihood, MDL/BIC, BDe) decompose into a sum over the
nodes, it turns out that this does not happen for discriminative scores. Therefore, although many of the
structure learning algorithms discussed for the generative case can also be applied for discriminative
scores, the problem becomes computationally more expensive in the discriminative case. We restrict
our discussion to BN, since most of the literature for discriminative learning is concerned about this
type of PGMs.
Conditional log-likelihood (CLL): The log-likelihood over a model class C can be written as
log L(C; D) =
L

l=1
log P(x(l)|C),
(18.185)
=
L

l=1
log P(c(l)|z(l), C) +
L

l=1
log P(z(l)|C),
(18.186)

1034
CHAPTER 18 Introduction to Probabilistic Graphical Models
= CLL(C; D) + log L(C; DZ),
(18.187)
where the log-likelihood decomposes into two terms. The term log L(C; DZ) is the log-likelihood of
the attributes Z, where the data set DZ contains only the samples of the features Z, without the values
of C. The term CLL(C; D) is called conditional log-likelihood, which is the log-likelihood of the class
variable C, conditioned on the respective values of Z. While CLL(C; D) reﬂects how well some model
M ∈C ﬁts the empirical conditional distribution PD(C|Z), the term L(C; DZ) measures the ﬁtness of
M to represent the empirical distribution over the features PDZ(Z). For discriminative learning, we are
primarily interested in optimizing CLL(C; D), but not L(C; DZ). Therefore, for discriminative learning
we discard the second term in (18.187) and maximize the conditional likelihood:
CLL(C; D) =
L

l=1
⎡
⎣log P(c(l), z(l)|C) −log
|C|

c=1
P(c, z(l)|C)
⎤
⎦.
(18.188)
Suppose we found a model M∗maximizing the CLL, and we wish to classify a new sample z. The
decision function according to M∗is then
fM∗(z) = arg max
c∈val(C) P(c|z, M∗).
(18.189)
However, although CLL is connected with classiﬁcation rate, maximizing CLL is not the same as
maximizing the classiﬁcation rate on the training set. Friedman [41] shows that improving CLL even
can be obstructive for classiﬁcation. This can be understood in a wider context, when our goal is
not to maximize the number of correct classiﬁcations, but to make an optimal decision based on the
classiﬁcation result. As an example given in [4], assume a system which classiﬁes if a patient has a lethal
disease, based on some medical measurements. There are 4 different combinations of the state of the
patient and the system diagnosis, which have very different consequences: the scenario that a healthy
patient is diagnosed as ill is not as disastrous as the scenario when an ill patient is diagnosed as healthy.
For such a system it is not enough to return a 0/1-decision, but additionally a value of conﬁdence
has to be provided. These requirements are naturally met by maximum CLL learning. Generally, one
can deﬁne a loss-function L(k,l) which represents the assumed loss when a sample with class c = k
is classiﬁed as c = l. Maximizing CLL is tightly connected with minimizing the expected loss for a
loss-function L(k,l) satisfying L(k, k) = 0, and L(k,l) ⩾0, k ̸= l.
In [42], an algorithm for learning the BN parameters for a ﬁxed structure G was proposed. This
algorithm uses gradient ascend to increase the CLL, while maintaining parameter feasibility, i.e., non-
negativity and sum-to-one constraints. Although CLL is a concave function, these constraints are non-
convex, such that the algorithm generally ﬁnds only a local maximum of the CLL. However, in [43,44]
it is shown that for certain network structures the parameter constraints can be neglected, yielding a
convex optimization problem and thus a globally optimal solution. More precisely, the network structure
G has to fulﬁll the condition:
∀Z ∈ChG(C) : ∃X ∈PaG(Z) : PaG(Z) ⊆PaG(X) ∪{X}.
(18.190)
In words, every class child Zi must have a parent X, which together with its own parents PaG(X)
contain all parents PaG(Zi). It is shown that for structures fulﬁlling this condition, always a correctly

1.18.5 Inference
1035
normalized BN with the same CLL objective value as for the unconstrained solution can be found. This
implies that the obtained BN parameters globally maximize the CLL. In [45], a greedy hill-climbing
algorithm for BN structure learning is proposed using CLL as scoring function.
Empirical classiﬁcation rate (CR): Often one is interested in the special case of 0/1-loss, where each
correctly classiﬁed sample has loss 0, while each misclassiﬁed sample causes a loss of 1. As pointed out
in [41], maximum CLL training does not directly correspond to optimal 0/1-loss. An objective directly
reﬂecting the 0/1-loss is the empirical classiﬁcation rate
CR(C; D) = 1
L
L

l=1
1{c(l)=arg maxc∈val(C) P(c|z(l),C)}.
Parameter learning using CR is hard, due to the non-differentiability and non-convexity of the function.
In [40,46], CR was used for greedy hill-climbing structure learning.
Maximum margin (MM): The probabilistic multi-class margin [47] for the lth sample can be
expressed as
d(l)
C = min
c̸=c(l)
P
"
c(l)|z(l), C
#
P
"
c|z(l), C
#
=
P
"
c(l), z(l)|C
#
maxc̸=c(l) P
"
c, z(l)|C
#.
(18.191)
If d(l)
C
> 1, then sample l is correctly classiﬁed and vice versa. The magnitude of d(l)
C is related to the
conﬁdence of the classiﬁer about its decision. Taking the logarithm, we obtain
log d(l)
C = log P

c(l), z(l)|C
	
−max
c̸=c(l)

log P

c, z(l)|C
		
.
(18.192)
Usually, the maximum margin approach maximizes the margin of the sample with the smallest margin
for a separable classiﬁcation problem [37], i.e., we use the objective function
MM(C; D) =
min
l=1,...,L log d(l)
C .
(18.193)
For non-separable problems, this is relaxed by introducing a so-called soft margin, which has been used
for parameter learning [47,48] and for structure learning [49,50].
1.18.5 Inference
In the last section the basics of learning PGMs have been introduced. Now, assuming that the process
of learning has already been performed, i.e., the model structure and parameterization are established,
we move onto the task of inference. This task deals with assessing the marginal and/or most likely
conﬁguration of variables given observations. For this, the set of RVs X in the PGM is partitioned into
three mutually disjoint subsets O, Q and H, i.e., X = O ∪Q ∪H and O ∩Q = O ∩H = Q ∩H = ∅.
Here, O denotes the set of observed nodes, i.e., the evidence variables, Q denotes the set of query
variables, and H refers to the set of nodes which belong neither to O or Q, also known as latent or
hidden variables.

1036
CHAPTER 18 Introduction to Probabilistic Graphical Models
There are two basic types of inference queries:
i. Marginalization query: This ﬁrst type of query infers the marginal distribution of the query variables
conditioned on the observation O, i.e., it computes
P(Q|O = o) = P(Q, O = o)
P(O = o) .
(18.194)
The terms P(Q, O = o) and P(O = o) can be determined by marginalization over H and H ∪Q
of the joint probability P(X) = P(O, Q, H) represented by the PGM, respectively. That is,
P(Q, O = o) =

h∈val(H)
P(O = o, Q, H = h),
and
(18.195)
P(O = o) =

q∈val(Q)
P(O = o, Q = q).
(18.196)
Example 23.
Assume a PGM used by a car insurance company to calculate the insurance fees for
individuals. Then, a query could ask for the probabilities of certain damage sums (this corresponds
to query variables) arising for the case of a driver with age 25 and no children (this corresponds to
evidence variables). Other variables in the model, such as e.g., the number of accidents in the past, are
marginalized out since they are not observed.
ii. Maximum a posteriori query: This query determines the most likely instantiation of the query
variables given some evidence, i.e., it computes
q∗= arg
max
q∈val(Q) P(Q = q|O = o).
(18.197)
This is equivalent to
q∗= arg
max
q∈val(Q)

h∈val(H)
P(Q = q, H = h|O = o),
(18.198)
= arg
max
q∈val(Q)

h∈val(H)
P(Q = q, H = h, O = o).
(18.199)
Note, that the most likely instantiation of the joint variables q∗does in general not correspond to
the states obtained by selecting the most likely instantiation for all variables in Q individually. An
example for this is shown in [1], Chapter 2.
Example 24.
In hidden Markov models (HMMs) this amounts to ﬁnding the most likely state sequence
q∗explaining the given observation sequence. This state sequence is determined by applying the Viterbi
algorithm as shown in Section 1.18.6.1.
While both inference queries can in principle be answered by directly evaluating the sums in the
corresponding equations, this approach becomes intractable for PGMs with many variables. This is
due to the number of summands growing exponentially with the number of variables. Therefore, more

1.18.5 Inference
1037
X 1
X 6
X 2
X 4
X 5
X 3
X 7
µ X 1
X 4 (x 4)
µ X 2
X 4 (x 4)
µ X 3
X 4 (x 4)
µ X 5
X 4 (x 4)
µ X 7
X 5 (x 5)
µ X 6
X 5 (x 5)
FIGURE 18.21
Message passing in a tree-shaped MN M to determine the marginal PM(X4).
efﬁcient inference methods exploiting the structure of the underlying PGMs have been developed;
The exact solution to (i) can be found using the sum-product algorithm, also known as belief propa-
gation or message passing algorithm [2,51], assuming tree-structured graphical models, and (ii) can
be calculated by applying the max-product or in the log-domain the max-sum algorithm. In this tuto-
rial, the focus is on marginalization queries P(Q|O). Details on MAP queries are provided in [1],
Chapter 9.
The remainder of this section is structured as follows: In Section 1.18.5.1 the sum-product algorithm
is derived and the junction tree algorithm for exact inference in arbitrary graphs is introduced. In cases,
where exact inference is computationally too demanding, approximate inference techniques can be
used. A short introduction to these techniques is given in Section 1.18.5.2.
1.18.5.1 Exact inference
Given some evidence o ∈val(O), the direct calculation of the marginal in (18.196) of the joint proba-
bility distribution
P(O = o) =

q∈val(Q)

h∈val(H)
P(O = o, Q = q, H = h)
(18.200)
expands to !|Q|
i=1 sp(Qi) !|H|
i=1 sp(Hi) summations. Hence, assuming that each variable can be in one
of k states the evaluation of Eq. (18.200) requires O(k|Q|+|H|) summations, i.e., the evaluation of a
sum with exponentially many terms in the number of variables. This renders the direct computation
intractable for large values of k and many variables. Fortunately, the underlying graph structure, and
the thereby introduced factorization of the joint probability, can be exploited. This is demonstrated in
the following example:
Example 25.
Consider the MN M in Figure 18.21. The joint probability factorizes according to the
maximal cliques C1 = {X1, X4}, C2 = {X2, X4}, C3 = {X3, X4}, C4 = {X4, X5}, C5 = {X5, X6},
and C6 = {X5, X7} as
PM(X1, X2, X3, X4, X5, X6, X7) = 1
Z 	C1(X1, X4)	C2(X2, X4)	C3(X3, X4) · (18.201)
	C4(X4, X5)	C5(X5, X6)	C6(X5, X7).
(18.202)

1038
CHAPTER 18 Introduction to Probabilistic Graphical Models
Suppose, we are interested in determining the marginal probability PM(X4). Then,
PM(X4) =

x1
· · ·

x3

x5
· · ·

x7
PM(X1, X2, X3, X4, X5, X6, X7)
= 1
Z
⎡
⎣

x1∈val(X1)
	C1(x1, x4)
⎤
⎦



=μX1→X4(x4)
⎡
⎣

x2∈val(X2)
	C2(x2, x4)
⎤
⎦



=μX2→X4(x4)
⎡
⎣

x3∈val(X3)
	C3(x3, x4)
⎤
⎦



=μX3→X4(x4)
·
⎡
⎢⎢⎢⎢⎢⎢⎣

x5∈val(X5)
	C4(x4, x5)
⎡
⎣

x6∈val(X6)
	C5(x5, x6)
⎤
⎦



=μX6→X5(x5)
⎡
⎣

x7∈val(X7)
	C6(x5, x7)
⎤
⎦



=μX7→X5(x5)
⎤
⎥⎥⎥⎥⎥⎥⎦



=μX5→X4(x4)
= 1
Z μX1→X4(x4)μX2→X4(x4)μX3→X4(x4)μX5→X4(x4),
where the terms μXi→X j (x j) denote so-called messages from Xi to X j. The grouping of the factors
with the corresponding sums essentially exploits the distributive law [52] and reduces the computational
burden. In this example, we require O(k2) summations and multiplications assuming that sp(Xi) = k for
all variables Xi. Direct calculation of PM(X4) would instead require O(kN−1) arithmetic operations,
where N = 7. The normalization constant Z can be determined by summing the product of incoming
messages over x4, i.e.,
Z =

x4∈val(X4)
μX1→X4(x4)μX2→X4(x4)μX3→X4(x4)μX5→X4(x4).
(18.203)
In the sequel, we generalize the example above to pairwise MNs, while inference in more general
MNs, BNs and FGs is covered later in this section. This generalization leads to the sum-product rule:
Messages μXi→X j (x j) are sent on each edge (Xi −X j) ∈E from Xi to X j. These messages are
updated according to
μXi→X j (x j) =

xi∈val(Xi)
	{Xi,X j}(xi, x j)

Xk∈(NbG(Xi)\{X j})
μXk→Xi (xi),
(18.204)
i.e., the new message μXi→X j (x j) is calculated as the product of the incoming messages with the
potential function 	{Xi,X j} and summation over all variables except X j. This message update is sketched
in Figure 18.22.
The sum-product algorithm schedules the messages such that an updated message is sent from Xi
to X j when Xi has received the messages from all its neighbors except X j, i.e., from all nodes in
NbG(Xi) \ {X j} [53].

1.18.5 Inference
1039
...
Xi
Xj
µXk→Xi(xi)
µXi→Xj(xj)
Xk∈(NbG(Xi)\{Xj})
FIGURE 18.22
Sum-product update rule.
X1
X6
X2
X4
X5
X3
X7
1
3
1
3
3
1
2
2
1
3
1
3
FIGURE 18.23
Message update of the sum-product algorithm.
Example 26.
This message update schedule of the sum-product algorithm for our previous example is
illustrated in Figure 18.23. The sent messages are illustrated as arrows in the ﬁgure, where the numbers
next to the arrows indicate the iteration count at which message updates are sent to neighboring nodes.
After three iterations the updates converge for all nodes in the graph.
The sum-product algorithm is exact if the MN is free of cycles [2], i.e., if it is a tree. Exact means
that the marginals P(Xi) (also called beliefs) obtained from the sum-product algorithm according to
P(Xi) = 1
Z

Xk∈NbG(Xi)
μXk→Xi (xi)
(18.205)
are guaranteed to converge to the true marginals. Again, considering the graph in Figure 18.23, each
clique factor 	C includes two variables. Consequently, the complexity of the sum-product algorithm is
O(k2) assuming that sp(Xi) = k for all variables Xi.
So far, we are able to obtain the marginals P(Xi) for any Xi ∈Q for tree-shaped MNs. However,
the aim is to determine the marginal distribution over the query variable Qi ∈Q conditioned on the
evidence O = o. This requires the computation of P(Qi, o), cf. Eq. (18.194) assuming only one query

1040
CHAPTER 18 Introduction to Probabilistic Graphical Models
variable. The evidence is introduced to each clique factor 	C that depends on evidence nodes, i.e., to
each 	C for which there is an Oi ∈C that is also in O. Therefore, we identify all factors 	C with
Oi ∈C for any Oi ∈O and determine the new factor by multiplying 	C with the indicator function
1{Oi=oi}, i.e., 	C( · ) is modiﬁed according to 	C( · )1{Oi=oi}. In other words, factors are set to zero for
instantiations inconsistent with O = o. Interestingly, it turns out that running the sum-product algorithm
on the updated factors leads to an unnormalized representation of P(Qi, o). The regular P(Qi|o) is
obtained by normalizing P(Qi, o) by Z = 
qi∈val(Qi) P(Qi = qi, o) [53,54].
Message Passing in FGs: For FGs, the sum-product algorithm for determining the marginal distribu-
tion of the variable Xi, given all the observations can be formulated analogously [55]. The observations
are absorbed into the factors instead of the clique factors. Neighboring nodes communicate with each
other: whenever a node, either a variable or factor node, has received all messages except on one edge,
it sends a new message to its neighbor over this edge. At the beginning, messages from variable nodes
are initialized to 1. The message from variable node X to factor node F is
μX→F(x) =

Fk∈
"
NbG(X)\{F}
# μFk→X(x),
(18.206)
while a message from factor node F to variable node X is computed as
μF→X(x) =

NbG(F)\{X}
⎛
⎜⎝f
"
NbG(F)
#

Xk∈
"
NbG(F)\{X}
# μXk→F(Xk)
⎞
⎟⎠.
(18.207)
The marginal at each variable node can be determined as P(Xi) = 1
Z
!
F∈NbG(Xi) μF→Xi (Xi), where
Z is a normalization constant.
So far, we have shown how to efﬁciently compute the marginals of a factorized joint probability
distribution given some evidence in the case of tree-structured MNs and FGs. Generally, the sum-
product algorithm is exact only if performed on a graphical model that is a tree. The remaining question
is how to perform inference in BNs and arbitrarily structured BNs and MNs?
Message passing in arbitrarily structured BNs and MNs: Inference in BNs can be performed by
ﬁrst transforming the BN into an MN. Inference is then performed on this transformed model. The
transformation of the model is known as moralization. Further, any arbitrary directed (and undirected)
graphical model can be transformed into an undirected tree (junction tree or clique tree) where each
node in the tree corresponds to a subset of variables (cliques) [3,53,54,56–59]. This is done with the
goal of performing inference on this tree, exploiting that inference on trees is exact. The transformation
of an arbitrary graph to a junction tree involves the following three steps, where the ﬁrst step is only
required when transforming a BN to an MN:
i. Moralization: In the case of a directed graphical model, undirected edges are added between all pairs
of parents PaG(Xi) having a common child Xi for all Xi ∈X. This is known as moralization [54].
The moral graph is then obtained by dropping the directions of the edges. Moralization essentially
results in a loss of conditional independence statements represented by the original graph. However,
this is necessary for representing the original factorization by the undirected graph.

1.18.5 Inference
1041
X1
X2
X3
X4
X5
X6
(a)
X1
X2
X3
X4
X5
X6
(b)
FIGURE 18.24
Moralization example. (a) BN to be transformed to an MN. (b) Moral graph; the added edge is red.
Example 27 (Moralization).
The joint probability of the BN B in Figure 18.24a is
PB(X1, . . . , X6) = PX1(X1)PX2(X2|X1)PX3(X3|X1)PX4(X4|X2)PX5(X5|X3)PX6(X6|X4, X5).
(18.208)
The corresponding moral graph is shown in Figure 18.24b. It factorizes as a product of the fac-
tors of the maximal cliques C1 = {X1, X2}, C2 = {X1, X3}, C3 = {X2, X4}, C4 = {X3, X5}, and
C5 = {X4, X5, X6} as
PM(X1, . . . , X6) = 	C1(X1, X2)	C2(X1, X3)	C3(X2, X4)	C4(X3, X5)	C5(X4, X5, X6).
(18.209)
Each CPD PXi (Xi|PaG(Xi)) of the BN is mapped into the factor of exactly one clique, i.e., each
clique factor is initialized to one and then each PXi (Xi|PaG(Xi)) is multiplied into a clique factor
which contains {Xi} ∪PaG(Xi). In this example, this results in the following factors: 	C1(X1, X2) =
PX1(X1)PX2(X2|X1), 	C2(X1, X3) = PX3(X3|X1), 	C3(X2, X4) = PX4(X4|X2), 	C4(X3, X5) =
PX5(X5|X3), and 	C5(X4, X5, X6) = PX6(X6|X4, X5). The normalization factor of PM(X1, . . . , X6)
in this case is Z = 1.
ii. Triangulation: The next step in converting an arbitrary MN into a junction tree is the triangulation
of the moral graph. The triangulated graph is sometimes called chordal graph.
Deﬁnition 28 (Triangulation [54]).
A graph G is triangulated if there is no cycle of length ⩾4
without an edge joining two non-neighboring nodes.
The triangulated graph can be found by the elimination algorithm [1] that eliminates one variable
from the graph in each iteration: Assuming a given elimination order of the variables the ﬁrst node from
the ordering is selected and in the moral graph all pairs of neighbors of that node are connected. The
resulting graph is collected in a set of graphs Ge. Afterwards, this node and all its edges are removed
from the graph. Then, the next node in the elimination ordering is selected for elimination. We proceed
in the same way as above until all nodes have been eliminated. The graph resulting from the union

1042
CHAPTER 18 Introduction to Probabilistic Graphical Models
X 1
X 2
X 3
X 4
X 5
X 6
FIGURE 18.25
Triangulated graph; added edges are dashed.
of all the graphs in Ge is triangulated. Depending on the ordering, there typically exist many different
triangulations. For efﬁcient inference, we would like to choose a ordering such that e.g., the state-space
size of the largest clique is minimal. Finding the optimal elimination ordering is NP-hard [1]. Greedy
algorithms for determining an ordering so that the induced graph is close to optimal are summarized in
[1], Chapter 9.
Example 28 (Triangulation).
An example of a triangulated graph obtained from Figure 18.24b with
elimination ordering X6, X5, X4, X3, X2, X1 is shown in Figure 18.25. The elimination of node X5
and X4 introduces the edges (X3 −X4) and (X2 −X3), respectively. The dashed edges are added to
triangulate the moral graph.
Each maximal clique of the moral graph is contained in the triangulated graph Gt. Using a similar
mechanism as in Example 27, we can map the factors of the moral graph to the factors of the triangulated
graph. The cliques in Gt are maximal cliques and
PM(X) = 1
Z

C∈Gt
	C(C).
(18.210)
For efﬁciency, the cliques of the triangulated graph can be identiﬁed during variable elimination; A node
which is eliminated from the moral graph forms a clique with all neighboring nodes, when connecting
all pairs of neighbors. If this clique is not contained in a previously determined maximal clique, it is a
maximal clique in Gt.
Example 29.
ThemaximalcliquesofthetriangulatedgraphinFigure18.25are{X4, X5, X6}, {X3, X4,
X5}, {X2, X3, X4} , and {X1, X2, X3}.
iii. Building a junction tree: In the last step, the maximal cliques are treated as nodes and are connected
to form a tree. The junction tree has to satisfy the running intersection property, i.e., an RV Xi
present in two different cliques Ci and C j, i ̸= j, has to be also in each clique on the path

1.18.5 Inference
1043
X 1, X 2, X 3
X 2, X 3
X 2, X 3, X 4
X 3, X 4
X 4, X 5, X 6
X 4, X 5
X 3, X 4, X 5
FIGURE 18.26
Junction tree.
connecting these two cliques.2 The cliques in the junction tree are connected by a separator set
S = Ci ∩C j. The number of variables in S determines the edge weight for joining these two
cliques in the junction tree. These weights can be used to ﬁnd the junction tree using the maximum
spanning tree algorithm [60]. The tree with maximal sum of the weights is guaranteed to be a
junction tree satisﬁng the running intersection property.3
Example 30 (Junction Tree).
A junction tree of the example in Figure 18.25 with separators (square
nodes) is shown in Figure 18.26.
Once the junction tree is determined, message passing between the cliques is performed for inference.
This essentially amounts to the sum-product algorithm. First, the potential of a separator 	S(S) is
initialized to unity. The clique potential of neighboring cliques Ci and C j is 	Ci (Ci) and 	C j (C j),
respectively. The message passed from Ci to C j is determined as
˜	S(S) ←

Ci\S
	Ci (Ci),
and
	C j (C j) ←
˜	S(S)
	S(S)	C j (C j),
where the sum is over all variables in Ci which are not in S.
The message passing schedule is similar as for tree-structured models, i.e., a clique sends a message
to a neighbor after receiving all the messages from its other neighbors. Furthermore, the introduction of
evidence in the junction tree can be performed likewise as above. After each node has sent and received
a message, the junction tree is consistent and the marginals P(Qi, o) can be computed by identifying
the clique potential 	Ci (Ci) which contains Qi and summing over Ci \ {Qi} as
P(Qi, o) =

Ci\{Qi}
	Ci (Ci).
2This property is important for the message passing algorithm to achieve local consistency between cliques. Local consistency
means that 	S(S) = 
Ci \S 	Ci (Ci) for any Ci and neighboring separator set S. Because of this property, local consistency
implies global consistency [1].
3A proof of this result is provided in [61].

1044
CHAPTER 18 Introduction to Probabilistic Graphical Models
The computational complexity of exact inference in discrete networks is exponential in the size of the
largest clique (the width of the tree4). Hence, there is much interest in ﬁnding an optimal triangulation.
Nevertheless, exact inference is intractable in large and dense PGMs and approximation algorithms are
necessary.
1.18.5.2 Approximate inference
In the past decade, the research in PGMs has focused on the development of efﬁcient approximate
inference algorithms. While exact inference algorithms provide a satisfactory solution to many inference
and learning problems, there are large-scale problems where a recourse to approximate inference is
inevitable. In the following, we provide a brief overview of the most popular approximate inference
methods. Details can be found in [1,4,62].
1.18.5.2.1
Variational methods
The underlying idea in variational methods [4,62–65] is the approximation of the posterior probability
distribution P(Q|O), assuming H = ∅, by a tractable surrogate distribution g(Q), where we assume
that the evaluation of P(Q, O) is computational less expensive than that of P(Q|O) and P(O). The
logarithm of P(O) can be decomposed as
ln P(O) =

q
g(q) ln
) P(O, q)
g(q)
*



LB(g)
+
⎧
⎨
⎩−

q
g(q) ln
) P(q|O)
g(q)
*⎫
⎬
⎭



KL(g||P)
,
where the term LB(g) denotes a lower bound for ln P(O), i.e., LB(g) ⩽ln P(O), and KL(g||P)
denotes the Kullback-Leibler divergence between g(Q) and P(Q|O). Since ln P(O) does not depend on
g(q), and LB(g) and KL(g||P) are non-negative, maximizing LB(g) amounts to minimizing KL(g||P).
Hence, maximizing LB(g) with respect to g(q) results in the best approximation of the
posterior P(Q|O).
Alternatively, the lower bound can be determined as
ln P(O) = ln

q
P(q, O) = ln

q
g(q) P(q, O)
g(q)
⩾

q
g(q) ln
) P(O, q)
g(q)
*
= LB(g),
(18.211)
where we applied Jensen’s inequality [5]. Furthermore, the lower bound LB(g) can be derived by using
convex duality theory [65].
In variational approaches, g(Q) is restricted to simple tractable distributions. The simplest possibility,
also called mean-ﬁeld approximation, assumes that g(Q) factorizes as
g(Q) =
|Q|

i=1
gi(Qi).
(18.212)
4The tree-width of a graph is deﬁned as the size (i.e., number of variables) of the largest clique of the moralized and triangulated
directed graph minus one.

1.18.5 Inference
1045
The variational inference approach can be combined with exact inference applied on parts of the
graph. In particular, exact inference is performed on tractable substructures. This is called structured
variational approximation [63], where much of the structure of the original graph is preserved.
1.18.5.2.2
Loopy message passing
While message passing algorithms can be shown to be exact on PGMs with tree structure, convergence
of these algorithms on arbitrary graphs with cycles (loops) is not guaranteed. Moreover, even after
convergence, the resulting solution might be only an approximation of the exact solution. Surprisingly,
message passing on loopy graphs often converges to stable posterior/marginal probabilities. The most
signiﬁcant breakthrough came with the insight that for certain graph structures the ﬁxed points of
the message passing algorithm are actually the stationary points of the Bethe free energy [66]. This
clariﬁed the nature of message passing and led to more efﬁcient algorithms. Further, this established a
bond to a large body of physics literature and generalized belief propagation (GBP) algorithms [67] were
developed. The GBP algorithms operate on regions of nodes and messages are passed between these
regions. In contrast to GBP, Yuille [68] proposes a concave-convex procedure to directly optimize the
Bethe free energy. Experiments on spin class conﬁgurations show that this method is stable, converges
rapidly, and leads to even better solutions than message passing and GBP in some cases.
Convergence of loopy message passing has been experimentally conﬁrmed for many applications
(the maybe most popular being “turbo codes” [69,70]). There has also been a lot of theoretical work
investigating convergence properties of loopy message passing [67,71–73]. Recently, theoretical con-
ditions guaranteeing convergence of loopy message passing to a unique ﬁxed point have been proposed
in [74]. Minka [75] showed that many of the proposed approximate inference approaches, such as varia-
tional message passing, loopy message passing, expectation propagation amongst others can be viewed
as instances of minimizing particular information divergences.
1.18.5.2.3
Sampling Methods
Sampling methods are computational tractable methods aiming at computing the quantities of interest
by means of Monte Carlo procedures. One of the simplest of such methods is known as importance
sampling and sampling importance resampling [76] for estimating expectations of functions. There
are severe limitations of importance sampling in high dimensional sample spaces. However, Markov
Chain Monte Carlo methods scale well with the dimensionality. Special cases are Gibbs sampling and
the Metropolis-Hastings algorithm (see, e.g., [77,78] for an introduction). One of the most prominent
application of Monte Carlo methods (i.e., sequential importance sampling) are particle ﬁlters [79,80]
for online, non-linear and non-Gaussian tracking, where the posterior distribution is represented by a
ﬁnite set of particles (samples). Particle ﬁlters generalize traditional Kalman ﬁlters which assume that
the evolution of the state space is linear and that probability densities are Gaussian. Classical particle
ﬁlters solve the inference task of computing the posterior P(Q|O = o) for a limited family of graph
structures such as Kalman ﬁlters. Recently, Koller et al., Sudderth et al., and Ihler and McAllester
[81–83] extended particle methods to solve inference problems deﬁned on general graphical model
structures.

1046
CHAPTER 18 Introduction to Probabilistic Graphical Models
1.18.6 Applications
PGMs have a long tradition in modeling uncertainty in intelligent systems and are important in many
application areas such as computer vision [84–87], speech processing [88,89], time-series and sequential
modeling [90], cognitive science [91], bioinformatics [92], probabilistic robotics [93], signal process-
ing, communications and error-correcting coding theory [55,70,76,94], and in the area of artiﬁcial
intelligence.
In this tutorial, we restrict ourselves to present some typical applications for each of the three PGM
representations, namely for BNs, MNs and FGs. The ﬁrst application is about dynamic BNs for time-
series modeling of speech signals, followed by BNs for expert systems. Then, we present MRFs for
image analysis, and FGs for decoding error-correcting codes.
1.18.6.1 Dynamic BNs for speech processing and time-series modeling
Speech recognition is the task of retrieving the sequence of words from a speech recording. The main
components of an automatic speech recognition (ASR) system are:
1. Feature extraction: A feature sequence X = (x1, . . . , xT ) is determined by extracting T feature
vectors from the audio signal. Commonly, mel-cepstral coefﬁcients, delta, and double delta features
[89] are derived from overlapping windows of ∼25 ms length of the speech signal.
2. Recognition: Based on the extracted features, a sequence of N words w = (w1, . . . , wN) is deter-
mined.
The most likely word sequence w∗given some sequence of feature vectors can be determined by the
posterior probability P(w|X) as
w∗= arg max
w∈W P
"
w|X
#
,
(18.213)
= arg max
w∈W
P
"
X|w
#
P
"
w
#
P(X)
,
(18.214)
= arg max
w∈W P
"
X|w
#
P
"
w
#
,
(18.215)
where W denotes the set of all possible word sequences. The acoustic model probability P(X|w) for an
utterance w can be modeled as the concatenation of HMMs representing acoustic units such as words.
In practice, it might even be beneﬁcial to model subunits of words, e.g., bi- or triphones5 or syllable
units.
An HMM is a (hidden) Markov chain over states {Q1, . . . , QN} and observations {X1, . . . , X N}.
The states generate the observations such that observation Xi stochastically depends only on state Qi,
i.e.,
P
"
Xi|Q1, . . . , QN, X1, . . . , Xi−1, Xi+1, . . . , X N
#
= P
"
Xi|Qi
#
.
(18.216)
5In linguistics, a biphone (triphone) is a sequence of two (three) phonemes.

1.18.6 Applications
1047
Q1
Q2
· · ·
QN
X 1
X 2
· · ·
X N
FIGURE 18.27
Hidden Markov Model.
The number of states of Qi and Xi (considering discrete observations) is the same for each
i ∈{1, . . . , N}. An HMM can be represented as dynamic BN shown in Figure 18.27. In the case
of Gaussian distributed state variables and observation probabilities P
"
Xi|Qi
#
, this model structure is
known as Kalman ﬁlter.
In the context of HMMs, we usually have to deal with three fundamental problems: (i) Evalua-
tion problem; (ii) Decoding problem; and (iii) Learning (estimation) problem. Approaches to each
of these problems for discrete observation variables are discussed in the following. The observations
{X1, . . . , X N} are collected in X and the states {Q1, . . . , QN} in Q.
i. Evaluation problem: The aim is to determine the likelihood P(X|) for an observation sequence
X given the model parameters . This likelihood is typically used in sequence classiﬁcation tasks.
The joint probability distribution6 of the HMM in Figure 18.27 is
PB(X, Q|) = P(Q1|θ1)P(X1|Q1, θ3)
N

i=2
P(Qi|Qi−1, θ2)P(Xi|Qi, θ3),
(18.217)
where θ1, θ2, and θ3 are known as prior, transition, and observation (emission) probabilities,
respectively. The parameters θ2 and θ3 are assumed to be constant for all i. The likelihood P(X|)
can be obtained by marginalizing over all possible state sequences according to
P(X|) =

q∈val(Q)
PB(X, Q = q|),
(18.218)
i.e., determining P(X|) is a simple marginalization query. A naive summation over all state
sequences would be exponential in the length of the sequence N. Fortunately, the HMM has a
tree structure which can be exploited to compute the likelihood P(X|) efﬁciently. Similar as in
Example 25, we can rearrange the sums to the relevant terms in PB(X, Q|), i.e., we can determine
P(X|) recursively. Hence, neglecting the parameters , we have
6In implementations on computers the joint probability distribution is usually computed in the log-domain where the products
turn into sums. This alleviates numerical problems.

1048
CHAPTER 18 Introduction to Probabilistic Graphical Models
P(X|) =

q1∈val(Q1)
P(q1)P(X1|q1)

q2∈val(Q2)
P(q2|q1)P(X2|q2) ·

q3∈val(Q3)
P(q3|q2)P(X3|q3) · · ·
(18.219)
· · ·

qN−1∈val(QN−1)
P(qN−1|qN−2)P(X N−1|qN−1)

qN ∈val(QN )
P(qN|qN−1)P(X N|qN)



=

qN ∈val(QN )
P(qN ,X N |qN−1)=P(X N |qN−1)=β(qN−1)



=

qN−1∈val(QN−1)
P(qN−1,X N−1|qN−2)β(qN−1)=P(X N−1,X N |qN−2)=β(qN−2)
,
(18.220)
where we deﬁned the backward probability recursively as
β(Qi = qi) = P(Xi+1, . . . , X N|Qi = qi),
(18.221)
=

qi+1∈val(Qi+1)
β(Qi+1 = qi+1) ·
P(Qi+1 = qi+1|Qi = qi)P(Xi+1|Qi+1 = qi+1).
(18.222)
The recursion for computing β(QN−1) backwards to β(Q1) is initialized with β(QN) = 1. The
likelihood P(X|) can be determined from β(Q1 = q1) according to
P(X|) =

q1∈val(Q1)
P(Q1 = q1)P(X1|Q1 = q1)β(Q1 = q1),
(18.223)
=

q1∈val(Q1)
P(Q1 = q1)P(X1|Q1 = q1)P(X2, . . . , X N|Q1 = q1), (18.224)
=

q1∈val(Q1)
P(Q1 = q1, X1, . . . , X N).
(18.225)
This recursive algorithm reduces the computational complexity from O(N · sp(Q)N)
to O(N · sp(Q)2).
In a similar manner, forward probabilities α(Qi = qi) can be deﬁned recursively as
α(Qi = qi) = P(X1, . . . , Xi, Qi = qi),
(18.226)
=

qi−1∈val(Qi−1)
α(Qi−1 = qi−1) ·
P(Qi = qi|Qi−1 = qi−1)P(Xi|Qi = qi),
(18.227)
=

qi−1∈val(Qi−1)
P(X1, . . . , Xi−1, Qi−1 = qi−1) ·
P(Qi = qi|Qi−1 = qi−1)P(Xi|Qi = qi),
(18.228)
=

qi−1∈val(Qi−1)
P(X1, . . . , Xi, Qi−1 = qi−1, Qi = qi).
(18.229)

1.18.6 Applications
1049
After initializing α(Q1) to α(Q1) = P(Q1)P(X1|Q1) the forward probabilities can be computed
for i = 2, . . . , N and the likelihood P(X|) is determined as
P(X|) =

qN ∈val(QN )
α(QN = qN) =

qN ∈val(QN )
P(X1, . . . , X N, QN = qN).
(18.230)
Interestingly, P(X|) can be determined as
P(X|) =

qi∈val(Qi)
α(Qi = qi)β(Qi = qi),
(18.231)
=

qi∈val(Qi)
P(X1, . . . , Xi, Qi = qi)P(Xi+1, . . . , X N|Qi = qi),
(18.232)
=

qi∈val(Qi)
P(X, Qi = qi).
(18.233)
Computing P(X|) using α(Qi) and/or β(Qi) is computationally efﬁcient and is known as
forward/backward algorithm [89]. Both algorithms are an instance of the more general sum-product
algorithm.
ii. Decoding problem: Decoding is concerned with ﬁnding the state sequence which best explains
a given observation sequence X in model . This relates decoding to the maximum a posteriori
query, cf. Section 1.18.5. In other words, we are interested in the most likely instantiation of Q
given X and the model , i.e.,
q∗= arg
max
q∈val(Q) P(Q = q|X, ) = arg
max
q∈val(Q)
P(Q = q, X|)
P(X|)
.
(18.234)
Since P(X|) is the same for every state sequences, we obtain
q∗= arg
max
q∈val(Q) P(Q = q, X|).
(18.235)
The Viterbi algorithm is an efﬁcient approach for determining q∗in HMMs. Similarly as the forward
probabilities, we deﬁne δ(Qi = q) as
δ(Qi = qi) =
max
q1∈val(Q1),...,qi−1∈val(Qi−1)P(Q1 = q1, . . . , Qi−1 = qi−1, Qi = qi, X1, . . . , Xi|).
(18.236)
This term describes the largest probability for the partial observation sequence X1, . . . , Xi in model
 along a single state sequence which ends at variable Qi in state qi. Similar observations as for
the evaluation problem apply and we can express the computation of δ(Qi = qi) recursively as
δ(Qi = qi) =
max
˜q∈val(Qi−1)
.
δ(Qi−1 = ˜q)P(Qi = qi|Qi−1 = ˜q)
/
P(Xi|Qi = qi).
(18.237)
This equation is closely related to the computation of α(Qi) in (18.227), i.e., the sum over all states
in Qi−1 is replaced by the maximum operator. At the end of the recursion δ(QN = qN) gives the

1050
CHAPTER 18 Introduction to Probabilistic Graphical Models
largest probability along a single state sequence for X and . Since we are interested in the most
likely state sequence q∗, we have to memorize for each qi the best previous state qi−1 in φ(Qi = q)
according to
φ(Qi = qi) = arg
max
˜q∈val(Qi−1)
.
δ(Qi−1 = ˜q)P(Qi = qi|Qi−1 = ˜q)
/
.
(18.238)
This enables to extract q∗once δ(Qi = qi) and φ(Qi = qi) have been determined for 1 ⩽i ⩽N.
The Viterbi algorithm consists of the following steps:
(a) Initialization:
δ(Q1 = q) = P(Q1 = q)P(X1|Q1 = q)
∀q ∈val(Q),
(18.239)
φ(Q1 = q) = 0
∀q ∈val(Q).
(18.240)
(b) Recursion:
δ(Qi = q) =
max
˜q∈val(Qi−1)
.
δ(Qi−1 = ˜q)P(Qi = q|Qi−1 = ˜q)
/
·
P(Xi|Qi = q),
∀q ∈val(Q), i = 2, . . . , N,
(18.241)
φ(Qi = q) = arg
max
˜q∈val(Qi−1)
.
δ(Qi−1 = ˜q)P(Qi = q|Qi−1 = ˜q)
/
,
∀q ∈val(Q), i = 2, . . . , N.
(18.242)
(c) Termination:
P(X, Q = q∗|) =
max
q∈val(QN ) δ(QN = q),
(18.243)
q∗
N = arg
max
q∈val(QN ) δ(QN = q).
(18.244)
(d) Backtracking:
q∗
i = φ(Qi+1 = q∗
i+1) i = N −1, . . . , 1.
(18.245)
iii. Learning problem: The focus of learning is determining the parameters  of an HMM given
observation sequences X(l). Usually, there is a set of L training sequences D =
0
X(1), . . . , X(L)1
,
where X(l) =

x(l)
1 , . . . , x(l)
Nl
 
. The parameters  in HMMs are determined by maximum likelihood
estimation, i.e.,
ML = arg max
 log L(; D),
(18.246)
where
log L(; D) = log

q∈val(Q)
PB(X, Q = q|),
(18.247)
= log

q∈val(Q)
P(Q1 = q1|θ1)P(X1|Q1 = q1, θ3).
N

i=2
P(Qi = qi|Qi−1 = qi−1, θ2)P(Xi|Qi = qi, θ3).
(18.248)

1.18.6 Applications
1051
So the aim is to maximize the log-likelihood log L(; D) with respect to  given D.
In HMMs we have to determine the parameters
P(Q1 = q|θ1) = θ1
q
∀q ∈val(Q),
(18.249)
P(Qi = q|Qi−1 = ˜q, θ2) = θ2
q|˜q
∀q, ˜q ∈val(Q),
(18.250)
P(Xi = j|Qi = q, θ3) = θ3
j|q
∀j ∈val(X), q ∈val(Q).
(18.251)
ML parameter learning for discrete variables derived in Section 1.18.4.2.1 essentially amounts to count-
ing and normalizing. In particular, we have
ˆθ1
q =
L
l=1u1,l
q
sp(Q1)
j=1
L
l=1u1,l
j
,
(18.252)
ˆθ2
q|˜q =
L
l=1
Nl
i=2ui,l
q|˜q
sp(Q)
j=1
L
l=1
Nl
i=2ui,l
j|˜q
,
(18.253)
ˆθ3
j|q =
L
l=1
Nl
i=2ui,l
j|q
sp(X)
j′=1
L
l=1
Nl
i=2ui,l
j′|q
,
(18.254)
where L
l=1 u1,l
q
is the number of occurrences of Q1 = q, L
l=1
Nl
i=2 ui,l
q|˜q denotes the number of
transitions from state ˜q to q, and L
l=1
Nl
i=2 ui,l
j|q is the number of observing Xi = j in state Qi = q.
Unfortunately, these counts cannot be determined directly from D since we only observe the variables
X but not Q. The variables Q are hidden. Hence, the data D are incomplete and parameter learning is
solved by using the EM-algorithm [11]7. The EM-algorithm consists of an inference step (E-step) for
estimating the values of the unobserved variables. Subsequently, the maximization step (M-step) uses
this knowledge in (18.252), (18.253), and (18.254) to update the estimate. The EM-algorithm iterates
between both steps until convergence of log L(; D). In [11], it is proven that the log likelihood
increases in each iteration. In (18.252) we replace u1,l
q by the probability of Q1 = q given X(l) obtained
in the E-step as
u1,l
q
= P(Q1 = q|X(l)),
(18.255)
= P(Q1 = q, X(l))
P(X(l))
,
(18.256)
= α(l)(Q1 = q)β(l)(Q1 = q)

Qi α(l)(Qi)β(l)(Qi)
.
(18.257)
7In the context of HMMs the EM-algorithm is also known as Baum-Welch algorithm.

1052
CHAPTER 18 Introduction to Probabilistic Graphical Models
Similarly, for estimating the transition probabilities
ui,l
q|˜q = P(Qi = q, Qi−1 = ˜q|X(l)),
(18.258)
= P(Qi = q, Qi−1 = ˜q, X(l))
P(X(l))
,
(18.259)
= P(X(l)
1 , . . . , X(l)
i−1, Qi−1 = ˜q)P(Qi = q|Qi−1 = ˜q)P(X(l)
i |Qi = q)P(X(l)
i+1, . . . , X(l)
N |Qi = q)
P(X(l))
,
(18.260)
= α(l)(Qi−1)P(Qi = q|Qi−1 = ˜q)P(X(l)
i |Qi = q)β(l)(Qi)

Qi α(l)(Qi)β(l)(Qi)
(18.261)
and for the observation probabilities
ui,l
j|q = P(Qi = q|X(l))1{Xi= j},
(18.262)
= α(l)(Qi = q)β(l)(Qi = q)

Qi α(l)(Qi)β(l)(Qi)
1{Xi= j}.
(18.263)
The α(Qi) and β(Qi) probabilities from the recursive forward/backward algorithm are determined in
the E-step to facilitate the parameter estimation in (18.252), (18.253), and (18.254) of the M-Step. At the
beginning of the EM-Algorithm, the parameters  have to be initialized to reasonable values. Details
are provided in [89].
We continue with the natural language processing model for the term P(w) in Eq. (18.215). For
instance, stochastic language models [95] such as n-grams decompose the probability of a sequence of
words as
P
"
w1, . . . , wN
#
=
N

i=1
P
"
wi|w1, . . . , wi−1
#
≈
N

i=1
P

wi|wi−
"
n−1
#, . . . , wi−1
	
.
(18.264)
A bigram model (2-gram), which basically is a ﬁrst order Markov chain, leads to
P(w) =
N

i=1
P
"
wi|wi−1
#
.
(18.265)
The most probable word sequence w∗in Eq. (18.215) is determined using a decoding algorithm such
as the Viterbi algorithm.
The approach in Eq. (18.215) can also be used in statistical machine translation [96], where the
acoustic model P(X|w) is replaced by a translation model P(f|w) which models the relation of a pair
of word sequences of different languages f and w, where f = ( f1, . . . , fM) is a sequence with M words.
The probability P(f|w) can be interpreted as the probability that a translator produces sequence f when
presented sequence w.

1.18.6 Applications
1053
HMMs [89] for acoustic modeling of speech signals have enjoyed remarkable success over the last
forty years. However, recent developments showed that there are many sophisticated extensions to
HMMs represented by dynamical BNs, cf. [88] for an overview. Furthermore, in the seminal paper
of [97], discriminative HMM parameter learning based on the maximum mutual information (MMI)
criterion—closely related to optimizing the conditional log-likelihood objective—has been proposed,
which attempts to maximize the posterior probability of the transcriptions given the speech utterances.
This leads to signiﬁcantly better recognition rates compared to conventional generative maximum
likelihood learning. In speech processing, discriminative training of HMMs celebrated success over the
years and in this context the extended Baum-Welch algorithm [98,99] has been introduced.
ASR for high-quality single-talker scenarios is performing reliably with good recognition perfor-
mance. However, in harsh environments where the speech signal is distorted by interference with other
acoustic sources, e.g., the cocktail party problem [100], ASR performs far from satisfactory. Recently,
factorial-HMMs (FHMMs) including speaker interaction models and constraints on temporal dynamics
have won the monaural speech separation and recognition challenge [101]. Remarkably, these models
slightly outperformed human listeners [102] and are able to separate audio mixtures containing up to
four speech signals. In the related task of multipitch tracking, Wohlmayr et al. [103] have obtained
promising results using a similar FHMM model. FHMMs [63] enable to track the states of multiple
Markov processes evolving in parallel over time, where the available observations are considered as a
joint effect of all single Markov processes. Combining two single speakers in this way using an inter-
action model, we obtain the FHMM shown in Figure 18.28. The hidden state RVs denoted by X(t)
k
model the temporal dynamics of the pitch trajectory, where k indicates the Markov chain representing
the kth speaker and t is the time frame. Realizations of observed RVs of the speech mixture spectrum at
time t are collected in a D-dimensional vector Y(t) ∈RD and S(t)
k
models the single-speaker spectrum
conditioned on state X(t)
k . At each time frame, the observation Y(t) is considered to be produced jointly
by the two single speech emissions S(t)
1
and S(t)
2 . This mixing process is modeled by an interaction
model as for example by the MIXMAX model described in [104].
1.18.6.2 BNs for expert systems
Expert and decision support systems enjoy great popularity, especially in the medical domain. They
consist of two parts: a knowledge base and an inference engine. The knowledge base contains speciﬁc
knowledge of some domain and the inference engine is able to process the encoded knowledge and
extract information. Many of these systems are rule-based using logical rules. BNs have been applied
in order to deal with uncertainty in these systems [54]. For instance, the Quick Medical Reference
(QMR) database [105] has been developed to support medical diagnosis which consists of 600 diseases
and 4000 symptoms as depicted in Figure 18.29. The aim of inference is to determine the marginal
probabilities of some diseases given a set of observed symptoms. In [65], variational inference has been
introduced as exact inference is infeasible for most of the practically occurring inference queries.
1.18.6.3 MRFs for image analysis
MRFs [84–86] have been widely used in image processing tasks over the past decades. They
are employed in all stages of image analysis, i.e., for low-level vision such as image denoising,

1054
CHAPTER 18 Introduction to Probabilistic Graphical Models
X (1)
1
X (2)
1
X (3)
1
X (4)
1
· · ·
S(1)
1
S(2)
1
S(3)
1
S(4)
1
Y (1)
Y (2)
Y (3)
Y (4)
S(1)
2
S(2)
2
S(3)
2
S(4)
2
X (1)
2
X (2)
2
X (3)
2
X (4)
2
· · ·
FIGURE 18.28
FHMM for speaker interaction. Each Markov chain models the pitch trajectory of a single speaker. At each
time frame, the single speech emissions S(t)
1 and S(t)
2 jointly produce the observation Y(t).
· · ·
diseases
· · ·
symptoms
FIGURE 18.29
QMR database.
segmentation, texture and optical ﬂow modeling, and edge detection as well as for high-level tasks
like face detection/recognition and pose estimation. Image processing problems can often be repre-
sented as the task of assigning labels to image pixels. For example, in the case of image segmentation,
each pixel belongs to one particular label representing a segment: in the MRF in Figure 18.30, the image
pixels Y are observed and the underlying labels are represented as latent, i.e., unobserved, variables
X. The edges between the variables X model contextual dependencies, e.g., using the fact that neigh-
boring pixels have similar features. Inferring the distribution PM(X|Y) is intractable for large-scale
MRFs using exact methods. Approximate inference methods, such as loopy belief propagation, energy
optimization, and sampling methods (MCMC) amongst others, have been deployed.

1.18.6 Applications
1055
Y1
Y2
Y3
X 1
X 2
X 3
Y4
Y5
Y6
X 4
X 5
X 6
Y7
Y8
Y9
X 7
X 8
X 9
FIGURE 18.30
Markov random ﬁeld for image analysis.
1.18.6.4 FGs for decoding error-correcting codes
Transmission of information over noisy channels requires redundant information to be added to the
transmitted sequence of symbols in order to facilitate the recovery of the original information from a
corrupted received signal. Most error-correcting codes are either block codes or convolutional codes,
including the low-density parity-check codes developed by Gallager [94].
In a block code [76] the source symbol sequence of length K is converted into a symbol sequence
X of length N for transmission. Redundancy is added in the case of N being greater than K. Hence, a
(N, K) block code is a set of 2K codewords

x1, . . . , x2K  
, where each codeword has a length of N
symbols of a ﬁnite alphabet A, i.e., xi ∈AN [76,106]. Here, we assume a binary code, i.e., A ∈{0, 1}.
In a linear block code, the added extra bits are a linear function of the original symbol. Those bits are
known as parity-check bits and are redundant information. The transmitted codeword x is obtained from
the source sequence s by calculating x = GT s using modulo-2 arithmetic, where G is the generator
matrix of the code. For the (7, 4) Hamming code [76] a generator matrix is
G =
⎡
⎢⎢⎣
1 0 0 0 1 0 1
0 1 0 0 1 1 0
0 0 1 0 1 1 1
0 0 0 1 0 1 1
⎤
⎥⎥⎦,
(18.266)
and the corresponding parity-check matrix H can be derived (details are given in [76]) as
H =
⎡
⎣
1 1 1 0 1 0 0
0 1 1 1 0 1 0
1 0 1 1 0 0 1
⎤
⎦.
(18.267)

1056
CHAPTER 18 Introduction to Probabilistic Graphical Models
f 1
f 2
f 3
X 1
X 2
X 3
X 4
X 5
X 6
X 7
FIGURE 18.31
FG for the binary (7,4) Hamming code.
Each valid codeword x must satisfy Hx = 0. This parity-check can be expressed by the indicator
function
I(x1, . . . , x7) = 1{Hx=0},
(18.268)
= δ(x1 ⊕x2 ⊕x3 ⊕x5)δ(x2 ⊕x3 ⊕x4 ⊕x6)δ(x1 ⊕x3 ⊕x4 ⊕x7), (18.269)
where δ(a) denotes the Dirac delta function and ⊕is the modulo-2 addition. The term I(x1, . . . , x7) = 1
if and only if x is a valid codeword. Each row in H corresponds to one δ( · ) term. The function
I(x1, . . . , x7) can be represented as the FG shown in Figure 18.31. It consists of the following factors:
f1(X1, X2, X3, X5) = δ(x1 ⊕x2 ⊕x3 ⊕x5), f2(X2, X3, X4, X6) = δ(x2 ⊕x3 ⊕x4 ⊕x6), and
f3(X1, X3, X4, X7) = δ(x1 ⊕x3 ⊕x4 ⊕x7).
After encoding, the codewords are transmitted over a noisy channel. One of the simplest noisy
channel models is the binary symmetric channel where each transmitted bit is ﬂipped (corrupted) with
probability ϵ. That is, if the input to the channel is denoted as X and the output of the channel is Y, then
P(Y = 0|X = 1) = ϵ,
(18.270)
P(Y = 1|X = 1) = 1 −ϵ,
(18.271)
P(Y = 0|X = 0) = 1 −ϵ,
and
(18.272)
P(Y = 1|X = 0) = ϵ.
(18.273)
This channel is memoryless and the channel model can be represented as P(Y|X) = !N
i=1 P(Yi|Xi) =
!N
i=1 gi(Xi, Yi). Decoding the received symbol Y requires to determine the transmitted codeword X
basedontheobservedsequenceY. Thisispossibleusingtheposteriorprobability P(X|Y)andexploiting
that
P(X|Y) = P(Y|X)P(X)
P(Y)
,
(18.274)
∝P(Y|X)I(X) =
N

i=1
P(Yi|Xi)I(X),
(18.275)
where the probability P(X) is represented by the indicator function I(X) of the code, i.e., all codewords
are assumed to be equally likely. The code together with the noisy channel model can be represented

1.18.7 Implementation/Code
1057
f 1
f 2
f 3
X 1
X 2
X 3
X 4
X 5
X 6
X 7
g1
g2
g3
g4
g5
g6
g7
Y1
Y2
Y3
Y4
X 5
X 6
Y7
FIGURE 18.32
FG for the binary (7,4) Hamming code and the binary symmetric channel.
by the FG shown in Figure 18.32. The probability P(X|Y) is obtained by performing the sum-product
algorithm in (18.206) and (18.207) or the max-product algorithm. If the FG contains cycles, as in this
example, the decoding problem is iterative and amounts to loopy message passing, also known as turbo
decoding in the coding community [70].
1.18.7 Implementation/code
In this section, we provide a list of recently developed code for PGMs. With respect to the features of
the listed implementations and comparisons to other software, the interested reader is referred to the
given references and websites.
•
Infer.NET: Infer.NET provides a framework for large-scale Bayesian inference in PGMs and is devel-
oped at Microsoft Research Cambridge
(research.microsoft.com/en-us/um/cambridge/projects/
infernet).
•
WEKA: WEKA [107] is a data mining software applicable to various tasks in machine learning.
Some of the implemented methods are directly related to PGMs such as discriminative parameter
learning or structure learning heuristics of BNs.
•
LibDAI: LibDAI [108] is an open source C++ library providing various exact and approximate
inference methods for FGs with discrete variables.
•
HTK: The HTK toolkit from Cambridge University (htk.eng.cam.ac.uk) is a state-of-the-art tool
used for time-series modeling such as speech recognition. It implements an HMM using discrete
and continuous observation probabilities, e.g., mixtures of Gaussians. Furthermore, the tool sup-
ports parameter tying, discriminative HMM training and many other methods important for speech
recognition.
•
GMTK: GMTK (ssli.ee.washington.edu/∼bilmes/gmtk) implements dynamic BNs for sequential
data processing. It is mainly designed for speech recognition.
•
MMBN: We recently developed a maximum margin parameter learning algorithm for BN classiﬁers.
Details about the algorithm are given in [48]. The maximum margin parameter training method can

1058
CHAPTER 18 Introduction to Probabilistic Graphical Models
be downloaded at www.spsc.tugraz.at/tools/MMBN. Furthermore, some data sets and example code
are provided.
1.18.8 Data sets
In the following, we list a selection of common data sets and repositories in machine learning. This list
includes only a small subset of publicly available data. Detailed information are supplied in the given
references:
•
UCI repository [109]: One of the largest collection of data sets is the UCI repository. It includes
more than 200 data sets for classiﬁcation, clustering, regression, and time-series modeling. The
types of features are categorical, real, and mixed. Benchmark results for most of the data are given
in accompanying references.
•
MNIST data [110]: The MNIST data set of handwritten digits contains 60000 samples for training
and 10000 for testing. The digits are centered in a 28×28 gray-level image. Benchmark classiﬁcation
results and the data are available at http://yann.lecun.com/exdb/mnist/.
•
USPS data: This data set contains 11000 handwritten digit images collected from zip codes of mail
envelopes (www.cs.nyu.edu/∼roweis/data.html). Each digit is represented as a 16 × 16 gray-scale
image. Other variants of this data set with different sample sizes are available. More details are given
in the Appendix of [37].
•
TIMIT data [111]: This data set is very popular in the speech processing community. It contains
read speech of eight major dialects of American English sampled at 16 kHz. The corpus includes
time-aligned orthographic, phonetic, and word transcriptions. This data has been used for various
tasks, like time-series modeling, phone recognition and classiﬁcation [112,113].
•
Caltech101 and Caltech256: For image analysis two data sets have been provided by Caltech con-
taining images of objects belonging to either 101 or 256 categories. The data, performance results,
and relevant papers are provided at http://www.vision.caltech.edu/Image_Datasets/Caltech101.
1.18.9 Conclusion
PGMs turned out to be one of the best approaches for modeling uncertainty in many domains. They offer
an unifying framework which allows to transfer methods among different domains. In this article, we
summarized the main elements of PGMs. In particular, we reviewed three types of representations—
Bayesian networks, Markov networks, and factor graphs, and we provided a gentle introduction to
structure and parameter learning approaches of Bayesian networks. Furthermore, inference techniques
were discussed with focus on exact methods. Approximate inference was covered brieﬂy. The interested
reader can immerse oneself following the references provided throughout the article.
Many relevant and interesting topics are not covered in detail within this review. One of these
topics is approximate inference methods. In fact, tractable approximate inference has been one of
the most challenging and active research ﬁelds over the past decade. Many of these advanced tech-
niques are discussed in [1,4,62,67,75]. Another focal point of current research interests are learning

Acknowledgments
1059
techniques for MNs. While we concentrated on structure and parameter learning of BNs under various
perspectives, we completely omitted learning of MNs. Parameter learning of undirected models is essen-
tially performed using iterative gradient-based methods. Each iteration requires to perform inference
which makes parameter learning computationally expensive. Score-based structure learning suffers from
similar considerations. One advantage of learning the structure of MNs is that there are no acyclicity
constraints which renders structure learning in BNs hard.
Glossary
Probabilistic graphical model
a probabilistic graphical model consists of a graph and a set of ran-
domvariables.Itrepresentsajointprobabilitydistributionwherethe
graph captures the conditional independence relationships among
the random variables
Bayesian network
a special instance of probabilistic graphical models consisting of
a directed acyclic graph and a set of conditional probability tables
associated with the nodes of the graph
Markov network
an undirected probabilistic graphical model. The joint probability
is deﬁned as the product of clique potentials associated with the
graph
Factor graph
a probabilistic graphical model represented by a bipartite graph.
One set of nodes represents the RVs in the graph, the other set of
nodes the factors. The joint probability is deﬁned as the product of
the factors
Probabilistic inference
computation of the distribution or the most likely conﬁguration of
some variables while potentially observing some other variables
using the joint probability distribution
Parameter learning
speciﬁcation of the parameters of the probability distributions
Structure learning
speciﬁcation of the factorization of the joint probability distribution
represented by the structure of a probabilistic graphical model
Acknowledgments
The authors Franz Pernkopf, Robert Peharz, and Sebastian Tschiatschek contributed equally to this work. This
work was supported by the Austrian Science Fund (Project number P22488-N23) and (Project number S10610).
Relevant Theory: Signal Processing, Machine Learning
See this Volume, Chapter 11 Parametric Estimation
See this Volume, Chapter 19 Monte-Carlo methods (MCMC, Particle Filters)
See this Volume, Chapter 20 Clustering

1060
CHAPTER 18 Introduction to Probabilistic Graphical Models
References
[1] D. Koller, N. Friedman, Probabilistic Graphical Models: Principles and Techniques, The MIT Press, 2009.
[2] J. Pearl, Probabilistic reasoning in intelligent systems: networks of plausible inference, Morgan Kaufmann,
1988.
[3] M.I. Jordan, Learning in Graphical Models, MIT Press, 1999.
[4] C.M. Bishop, Pattern Recognition and Machine Learning, Springer, 2006.
[5] T. Cover, J. Thomas, Elements of Information Theory, John Wiley & Sons, 1991.
[6] A. Papoulis, S.U. Pillai, Probability, Random Variables, and Stochastic Processes, McGraw-Hill, 2002.
[7] J.A.
Bilmes,
Dynamic
Bayesian
Multinets,
in:
International
Conference
on
Uncertainty
in
Artiﬁcial Intelligence, Morgan Kaufmann, 2000.
[8] C. Boutilier, N. Friedman, M. Goldszmidt, D. Koller, Context-speciﬁc independence in Bayesian
networks,
in:
International
Conference
on
Uncertainty
in
Artiﬁcial
Intelligence
(UAI),
1996,
pp. 115–123.
[9] J. Berkson, Limitations of the application of fourfold table analysis to hospital data, Biometrics Bull. 2 (3)
(1946) 47–53.
[10] S.L. Lauritzen, Graphical Models, Oxford Science Publications, 1996.
[11] A.Dempster,N.Laird,D.Rubin,MaximumlikelihoodestimationfromincompletedataviatheEMalgorithm,
J. Roy. Stat. Soc. 30 (B) (1977) 1–38.
[12] S. Boyd, L. Vandenberghe, Convex Optimization, Cambridge University Press, 2004.
[13] D. Heckerman, D. Geiger, D.M. Chickering, Learning Bayesian networks: the combination of knowledge
and statistical data, Mach. Learn. (Kluwer Academic Publishers) 20 (1995) 197–243.
[14] P. Spirtes, C. Glymour, R. Scheines, Causation, Prediction, and Search, The MIT Press, 2001.
[15] T. Verma, J. Pearl, An algorithm for deciding if a set of observed independencies has a causal explanation,
in: International Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 1992.
[16] J. Rissanen, Modeling by shortest data description, Automatica 14 (1978) 465–471.
[17] W. Lam, F. Bacchus, Learning Bayesian belief networks: an approach based on the MDL principle, Comput.
Intell. 10 (3) (1994) 269–293.
[18] J. Suzuki, A construction of Bayesian networks from databases based on an MDL principle, in: International
Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 1993, pp. 266–273.
[19] N. Friedmann, D. Koller, Being Bayesian about network structure, in: International Conference on Uncer-
tainty in Artiﬁcial Intelligence (UAI), 2000, pp. 201–210.
[20] W.L. Buntine, Theory reﬁnement on Bayesian networks, in: International Conference on Uncertainty in
Artiﬁcial Intelligence (UAI), 1991, pp. 52–60.
[21] G. Cooper, E. Herskovits, A Bayesian method for the induction of probabilistic networks from data, Mach.
Learn. 9 (1992) 309–347.
[22] D.M. Chickering. Learning Bayesian networks is NP-Complete, in: Learning From Data: Artiﬁcial Intelli-
gence and Statistics V, Springer-Verlag, 1996, pp. 121–130.
[23] D.M. Chickering, D. Geiger, D. Heckerman, Learning Bayesian Networks is NP-Hard, Technical Report,
Technical Report No. MSR-TR-94-17, Microsoft Research, Redmond, Washington, 1994.
[24] C.K. Chow, C.N. Liu, Approximating discrete probability distributions with dependence trees, IEEE Trans.
Inform. Theory 14 (1968) 462–467.
[25] D.M.
Chickering,
D.
Geiger,
D.
Heckerman,
Learning
Bayesian
networks:
search
methods
and
experimental
results,
in:
International
Conference
on
Artiﬁcial
Intelligence
and
Statistics
(AISTATS), 1995, pp. 112–128.

References
1061
[26] F. Glover, Heuristics for integer programming using surrogate constraints, Decision Sci. 8 (1) (1977)
156–166.
[27] D.M. Chickering, Learning equivalence classes of Bayesian-network structures, J. Mach. Learn. Res. 2
(2002) 445–498.
[28] M. Teyssier, D. Koller, Ordering-based search: A simple and effective algorithm for learning Bayesian
networks, in: International Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2005, pp. 584–590.
[29] M. Koivisto, K. Sood, Exact Bayesian structure discovery in Bayesian networks, J. Mach. Learn. Res. 5
(2004) 549–573.
[30] T. Silander, P. Myllymäki, A simple approach for ﬁnding the globally optimal Bayesian network structure,
in: International Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2006.
[31] A.P. Singh, A.W. Moore, Finding Optimal Bayesian Networks by Dynamic Programming, Carnegie Mellon
University, Technical Report, 2005.
[32] C.P. de Campos, Z. Zeng, Q. Ji, Structure learning of Bayesian networks using constraints, in: International
Conference on Machine Learning (ICML), 2009, pp. 113–120.
[33] T. Jaakkola, D. Sontag, A. Globerson, M. Meila, Learning Bayesian network structure using LP relaxations,
in: International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2010, pp. 358–365.
[34] J. Suzuki, Learning Bayesian belief networks based on the minimum description length principle: an efﬁcient
algorithm using the B&B technique, in: International Conference on Machine Learning (ICML), 1996, pp.
462–470.
[35] J.O. Berger, Statistical Decision Theory and Bayesian Analysis, Springer-Verlag, 1985.
[36] C.J.C. Burges, A tutorial on support vector machines for pattern recognition, Data Min. Knowl. Disc. 2 (2)
(1998) 121–167.
[37] B. Schölkopf, A.J. Smola, Learning with Kernels: Support Vector Machines, Regularization, Optimization,
and Beyond, MIT Press, 2001.
[38] C. Bishop, Neural Networks for Pattern Recognition, Oxford University Press, 1995.
[39] V. Vapnik, Statistical Learning Theory, Wiley & Sons, 1998.
[40] F. Pernkopf, J. Bilmes, Efﬁcient heuristics for discriminative structure learning of Bayesian network classi-
ﬁers, J. Mach. Learn. Res. 11 (2010) 2323–2360.
[41] J.H. Friedman, On bias, variance, 0/1-loss, and the curse of dimensionality, Data Min. Knowl. Disc. 1 (1997)
55–77.
[42] R. Greiner, W. Zhou, Structural extension to logistic regression: discriminative parameter learning of belief
net classiﬁers, in: Conference of the AAAI, 2002, pp. 167–173.
[43] T. Roos, H. Wettig, P. Grünwald, P. Myllymäki, H. Tirri, On discriminative Bayesian network classiﬁers and
logistic regression, Mach. Learn. 59 (2005) 267–296.
[44] H. Wettig, P. Grünwald, T. Roos, P. Myllymäki, H. Tirri, When discriminative learning of Bayesian network
parameters is easy, in: International Joint Conference on Artiﬁcial Intelligence (IJCAI), 2003, pp. 491–496.
[45] D. Grossman, P. Domingos, Learning Bayesian network classiﬁers by maximizing conditional likelihood,
in: International Conference of Machine Lerning (ICML), 2004, pp. 361–368.
[46] E.J. Keogh, M.J. Pazzani, Learning augmented Bayesian classiﬁers: a comparison of distribution-based and
classiﬁcation-based approaches, in: International Workshop on Artiﬁcial Intelligence and Statistics, 1999,
pp. 225–230.
[47] Y. Guo, D. Wilkinson, D, Schuurmans, Maximum margin Bayesian networks, in: International Conference
on Uncertainty in Artiﬁcial Intelligence (UAI), 2005.
[48] F. Pernkopf, M. Wohlmayr, S. Tschiatschek, Maximum margin Bayesian network classiﬁers, IEEE Trans.
Pattern Anal. Mach. Intell. 34 (3) (2012) 521–532.

1062
CHAPTER 18 Introduction to Probabilistic Graphical Models
[49] F.
Pernkopf,
M.
Wohlmayr,
Stochastic
margin-based
structure
learning
of
Bayesian
network
classiﬁers, Pattern Recogn. 46 (2) (2013) 464–471, Elsevier Inc., New York, NY, USA. <http://dx.doi.org/
10.1016/j.patcog.2012.08.007>.
[50] R. Peharz, F. Pernkopf, Exact maximum margin structure learning of Bayesian networks, in: International
Conference on Machine Learning (ICML), 2012.
[51] J.S. Yedidia, W.T. Freeman, Y. Weiss, Understanding Belief Propagation and Its Generalizations, Technical
Report TR-2001-22, Mitsubishi Electric Research Laboratories, 2002.
[52] S.M.Aji,R.J.McEliece,Thegeneralizeddistributivelaw,IEEE Trans.Inform.Theory46(2) (2000) 325–543.
[53] F.V. Jensen, An Introduction to Bayesian Networks, UCL Press Limited, 1996.
[54] R.G. Cowell, A.P. Dawid, S.L. Lauritzen, D.J. Spiegelhalter, Probabilistic Networks and Expert Systems,
Springer, 1999.
[55] F.R. Kschischang, B.J. Frey, H.-A. Loeliger, Factor graphs and the sum-product algorithm, IEEE Trans.
Inform. Theory 47 (2) (2001) 498–519.
[56] R. Cowell, Introduction to inference for Bayesian networks, in: M.I. Jordan (Ed.), Learning in Graphical
Models, MIT Press, 1999.
[57] P. Dawid, Applications of a general propagation algorithm for probabilistic expert systems, Stat. Comput. 2
(1992) 25–36.
[58] C. Huang, A. Darwiche, Inference in belief networks: a procedural guide, Int. J. Approx. Reason. 15 (1996)
225–263.
[59] S. Lauritzen, D. Spiegelhalter, Local computations with probabilities on graphical structures and their appli-
cation to expert systems, J. Roy. Stat. Soc. B 50 (1988) 157–224.
[60] J.B. Kruskal, On the shortest spanning subtree and the traveling salesman problem, in: Proceedings of the
American Mathematical Society, vol. 7, 1956, pp. 48–50.
[61] F.V. Jensen, F. Jensen, Optimal junction trees, in: International Conference on Uncertainty in Artiﬁcial
Intelligence (UAI), 1994, pp. 360–366.
[62] M.J.Wainwright,M.I.Jordan,Graphicalmodels,exponentialfamilies,andvariationalinference,Foundations
Trends Mach. Learn. 1 (2008) 1–305.
[63] Z. Ghaharamani, M.I. Jordan, Factorial hidden Markov models, Mach. Learn. 29 (1997) 245–275.
[64] T. Jaakkola, Tutorial on variational approximation methods, in: Advanced Mean Field Methods: Theory and
Practice, MIT Press, 2000.
[65] M.I. Jordan, Z. Ghahramani, T.S. Jaakkola, L.K. Saul, An introduction to variational methods for graphical
models, Mach. Learn. 37 (1999) 183–233.
[66] J. Yedidia, W.T. Freeman, Y. Weiss, Generalized belief propagation, in: Advances in Neural Information
Processing Systems (NIPS), 2000, pp. 689–695.
[67] J. Yedidia, W.T. Freeman, Y. Weiss, Constructing free-energy approximations and generalized belief propa-
gation algorithms, IEEE Trans. Inform. Theory 51 (7) (2005) 2282–2312.
[68] A.L. Yuille, CCCP algorithms to minimize the Bethe and Kikuchi free energies: convergent alternatives to
belief propagation, Neural Comput. 14 (7) (2002) 1691–1722.
[69] F.R. Kschischang, B.J. Frey, Iterative decoding of compound codes by probability propagation in graphical
models, IEEE J. Sel. Areas Commun. 16 (1998) 219–230.
[70] R.J. McEliece, D.J.C. MacKay, J.F. Cheng, Turbo decoding as an instance of Pearl’s ‘belief propagation’
algorithm, IEEE J. Sel. Areas Commun. 16 (2): 140–152, 1998.
[71] S. Aji, G. Horn, R. McEliece, On the convergence of iterative decoding on graphs with a single cycle, in:
IEEE International Symposium on Information Theory, 1998, pp. 276–282.
[72] Y. Weiss, Correctness of local probability propagation in graphical models with loops, Neural Comput. 12
(2000) 1–41.

References
1063
[73] Y.Weiss,W.T.Freeman,CorrectnessofbeliefpropagationinGaussiangraphicalmodelsofarbitrarytopology,
Neural Comput. 13 (10) (2001) 2173–2200.
[74] J.M. Mooij, H.J. Kappen, Sufﬁcient conditions for convergence of the sum-product algorithm, IEEE Trans.
Inform. Theory 52 (2) (2007) 4422–4437.
[75] T. Minka, Divergence Measures and Message Passing, Technical Report MSR-TR-2005-173, Microsoft
Research Ltd., Cambridge, UK, 2005.
[76] D.J.C. MacKay, Information Theory, Inference, and Learning Algorithms, Cambridge University Press,
2003.
[77] W. Gilks, S. Richardson, D. Spiegelhalter, Markov Chain Monte Carlo in Practice, Chapman and Hall, 1996.
[78] R.M. Neal, Probabilistic Inference Using Markov Chain Monte Carlo methods, Technical Report CRG-TR-
93-1, University of Toronto, Department of Computer Science, 1993.
[79] S. Arulampalam, S. Maskell, N. Gordon, T. Clapp, A tutorial on particle ﬁlters for on-line
non-linear/non-gaussian Bayesian tracking, IEEE Trans. Signal Process. 50 (2) (2002) 174–188.
[80] A. Doucet, On Sequential Monte Carlo Sampling Methods for Bayesian Filtering, Technical Report CUED/F-
INFENG/TR. 310, Cambridge University, Department of Engineering, 1998.
[81] A. Ihler, D. McAllester, Particle belief propagation, in: International Conference on Artiﬁcial Intelligence
and Statistics (AISTATS), 2009, pp. 256–263.
[82] D. Koller, U. Lerner, D. Angelov, A general algorithm for approximate inference and its application to hybrid
Bayes nets, in: International Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 1999, pp. 324–333.
[83] E.B. Sudderth, A.T. Ihler, W.T. Freeman, A.S. Willsky, Nonparametric belief propagation, in: Conference
on Computer Vision and Pattern Recognition (CVPR), vol. 1, 2003, pp. 605–612.
[84] J.E. Besag, On the statistical analysis of dirty pictures, J. Roy. Stat. Soc. B 48 (1986) 259–302.
[85] S. Geman, D. Geman, Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images,
IEEE Transactions on Pattern Analysis and Machine Intelligence 6 (6) (1984) 721–741.
[86] S.Z. Li, Markov Random Field Modeling in Image Analysis, Springer-Verlag, 2009.
[87] A.S. Willsky, Multiresolution Markov models for signal and image processing, Proc. IEEE 90 (8) (2002)
1396–1458.
[88] J.A.
Bilmes,
C.
Bartels,
Graphical
model
architectures
for
speech
recognition,
IEEE
Signal
Process. Mag. 22 (5) (2005) 89–100.
[89] L.R. Rabiner, A tutorial on Hidden Markov Models and selected applications in speech recognition, Proc.
IEEE 77 (2) (1989) 257–286.
[90] D. Barber, A.T. Cemgil, Graphical models for time-series, IEEE Signal Proc. Mag. 27 (6) (2010) 18–28.
[91] N. Chater, J.B. Tenenbaum, A. Yuille, Probabilistic models of cognition: conceptual foundations, Trends
Cogn. Sci. 10 (7) (2006) 287–291.
[92] D. Husmeier, R. Dybowski, S. Roberts, Probabilistic Modelling in Bioinformatics and Medical Informatics,
Springer-Verlag, 2005.
[93] S. Thrun, W. Burgard, D. Fox, Probabilistic Robotics, The MIT Press, 2006.
[94] R.G. Gallager, Low-Density Parity-Check Codes, MIT Press, 1963.
[95] D. Jurafsky and J.H. Martin, Speech and Language Processing: An Introduction to Natural
Language Processing, Computational Linguistics, and Speech Recognition, Prentice Hall, 2008.
[96] P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, R.L. Mercer, The mathematics of statistical machine trans-
lation: parameter estimation, Comput. Linguist. 19 (2) (1993) 263–311.
[97] L.R. Bahl, P.F. Brown, P.V. de Souza, R.L. Mercer, Maximum mutual information estimation of HMM
parameters for speech recognition, in: IEEE International Conference on Acoustics, Speech, and Signal
Processing (ICASSP), 1986, pp. 49–52.
[98] P.S. Gopalakishnan, D. Kanevsky, A. Nadas, D. Nahamoo, An inequality for rational functions with appli-
caitons to some statistical estimation problems, IEEE Trans. Inform. Theory 37 (1) (1991) 107–113.

1064
CHAPTER 18 Introduction to Probabilistic Graphical Models
[99] P.C. Woodland, D. Povey, Large scale discriminative training of hidden Markov models for speech recogni-
tion, Comput. Speech Lang. 16 (2002) 25–47.
[100] E.C. Cherry, Some experiments on the recognition of speech, with one and with two ears,
J. Acoust. Soc. Am. 25 (1953) 975–979.
[101] M. Cooke, J.R. Hershey, S.J. Rennie, Monaural speech separation and recognition challenge, Comput. Speech
Lang. 24 (2010) 1–15.
[102] J.R. Hershey, S.J. Rennie, P.A. Olsen, T.T. Kristjansson, Super-human multi-talker speech recognition: a
graphical modeling approach, Comput. Speech Lang. 24 (2010) 45–66.
[103] M. Wohlmayr, M. Stark, F. Pernkopf, A probabilistic interaction model for multipitch tracking with factorial
hidden Markov model, IEEE Trans. Audio Speech Lang. Process. 19 (4) (2011) 799–810.
[104] A. Nadas, D. Nahamoo, M.A. Picheny, Speech recognition using noise-adaptive prototypes, IEEE Trans.
Acoust. Speech Signal Process. 37 (10) (1989) 1495–1503.
[105] M.A. Shwe, B. Middleton, D.E. Heckerman, M. Henrion, E.J. Horvitz, H.P. Lehmann, G.E. Cooper, Proba-
bilistic diagnosis using a reformulation of the Internist-1/QMR knowledge base. ii. evaluation of diagnostic
performance, Method. Inform. Med. 30 (1991) 256–267.
[106] H.-A. Loeliger, An introduction to factor graphs, IEEE Signal Proc. Mag. 21 (1) (2004) 28–41.
[107] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, I.H. Witten, The WEKA data mining software:
an update, SIGKDD Explor. Newsl. 11 (2009) 10–18.
[108] J.M. Mooij, libDAI: a free and open source C++ library for discrete approximate inference in graphical
models, J. Mach. Learn. Res. 11 (2010) 2169–2173.
[109] A. Frank, A. Asuncion, UCI machine learning repository, 2010. <http://archive.ics.uci.edu/ml>.
[110] Y. LeCun, L. Bottou, Y. Bengio, P. Haffner, Gradient-based learning applied to document recognition, Proc.
IEEE 86 (11) (1998) 2278–2324.
[111] J.S. Garofolo, L.F. Lamel, W.M. Fisher, J.G. Fiscus, D.S. Pallett, N.L. Dahlgren, Darpa timit acoustic phonetic
continuous speech corpus cdrom, 1993.
[112] A.K.
Halberstadt,
J.
Glass,
Heterogeneous
measurements
for
phonetic
classiﬁcation,
in:
Proceedings of EUROSPEECH, 1997, pp. 401–404.
[113] F. Pernkopf, T. Van Pham, J. Bilmes, Broad phonetic classiﬁcation using discriminative Bayesian networks,
Speech Commun. 51 (2) (2009) 151–166.

19
CHAPTER
A Tutorial Introduction to Monte
Carlo Methods, Markov Chain
Monte Carlo and Particle Filtering
A. Taylan Cemgil
Department of Computer Engineering, Bo˘gaziçi University 34342 Bebek, Istanbul, Turkey
1.19.1 Introduction
Monte Carlo (method) (MC) is an umbrella name for an arsenal of numerical techniques for computing
approximate estimates via random sampling. The estimates are very often given as the result of an
intractable integral and the technique could have been named “numerical integration in high dimensions
via random sampling.” The term Monte Carlo was initially coined during 1940’s by von Neumann, Ulam
and Metropolis [1,2] as a “cuteness” [3] to refer to the famous casino of 1940s. However, due to the
central importance of the subject and wide use of the concept, it soon became an established technical
term.
Monte Carlo techniques have been further popularized in applied sciences with the wider availabil-
ity of computing power, starting from the 90s, most notably in statistics, computer science, operational
research and signal processing. In signal processing or operational research, MC experiments are exten-
sively used for assessing the performance of a system or a computational method by generating a
random sample of typical inputs. While the term Monte Carlo experiment is also used extensively here,
the goal in such applications is “system simulation,” i.e., to understand the properties of a system under
uncertainty. The Monte Carlo methods in this paper are slightly different from system simulation; we
have a well deﬁned “deterministic” computational problem with a single answer and randomization
is used as a tool for approximate computation. As such, we will refer to MC in this context and the
applications will be parameter estimation, prediction and model selection.
In this tutorial, we will sketch the aims, the basic techniques and the principles of Monte Carlo com-
putation. After the illustration of the law of large numbers and central limit theorem [4], we cover basic
Monte Carlo methods for sampling from elementary distributions: inversion, transform and rejection
techniques [5]. We cover also Markov Chain Monte Carlo (MCMC) methods [6]; but rather than giving
only the basic algorithms, we sketch the implications of some of the key results in the theory of ﬁnite
Markov chains [7]. We also cover importance sampling and sequential Monte Carlo methods [8–10].
Finally we give an overview of a more advanced technique, the reversible jump method [11]. Our goal
is to provide a self contained introduction with a uniﬁed notation, build up a basic appreciation of
modern Monte Carlo techniques and to sharpen the readers intuition using several toy examples. Parts
of this material have been used in advanced undergraduate and introductory graduate courses on Monte
Carlo computation, taken primarily by students interested in techniques for data analysis with signal
processing, machine learning or data mining background with some working knowledge of probability
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00019-X
© 2014 Elsevier Ltd. All rights reserved.
1065

1066
CHAPTER 19 A Tutorial Introduction to Monte Carlo Methods
theory and statistics. Experience has shown that even tutorial texts on statistical computation were not
immediately accessible due to the notation barrier and students beneﬁted mostly from toy examples.
Inevitably, in an elementary tutorial we had to omit a lot of techniques but still tried to retain the “gist”
of the subject.
1.19.2 The Monte Carlo principle
In an abstract setting, the main principle of a Monte Carlo technique is to generate a set of samples
x(1), . . . , x(N) from a target distribution π(x) to estimate some features of this target density π. Features
are simply expectations of “well behaving” functions that can be approximated as averages:
Eπ(ϕ(x)) ≈ϕ(x(1)) + · · · + ϕ(x(N))
N
≡¯Eϕ,N.
Provided that N is large enough, we hope that our estimate ¯Eϕ,N converges to the true value of the
expectation Eπ(ϕ(x)). More concrete examples of test functions ϕ(x) will be provided in the next
section. For independent and identically distributed samples, this is guaranteed by two key mathematical
results: the strong Law of Large Numbers (LLN) and the Central Limit Theorem (CLT) [4,12]. Assuming
that Eπ(ϕ(x)) = μ and Vπ(ϕ(x)) = σ 2 (we have ﬁnite mean and variance), the LLN states that
¯Eϕ,N →μ a.s.
Here, a.s. denotes convergence almost surely, meaning that Pr{limN→∞|μ −Eϕ,N| = 0} = 1. Whilst
ﬂuctuations are inevitable (since our approximation will be based on a random and ﬁnite sample set
x(1) . . . x(N)) we wish these ﬂuctuations to be small. This is guaranteed by the CLT: for sufﬁciently
large N, the ﬂuctuations are approximately Gaussian distributed
¯Eϕ,N ∼N

¯Eϕ,N|μσ 2/N

and the variance of the estimate drops with increasing N, while its mean is the desired value. This
result has important practical consequences. If we can generate i.i.d. samples from the distribution of
x, denoted as π(x), we can estimate expectations of ϕ(x):
1. Monte Carlo provides a “noisy” but unbiased estimate of the true value μ = Eπϕ(x).
2. The error behaves as O(N −1/2).
3. The convergence rate is independent of the dimensionality of x.
Thethirdpoint is particularlyimportant. It suggests that, at least inprinciple, withaquitesmall number of
samples one can compute approximate solutions for arbitrary large parameter estimation problems. All
one needs is obtaining independent samples. The difﬁculty with this approach, however, is in generating
independent samples from a target distribution π(x) and various Monte Carlo methods aim to provide
techniques for this.

1.19.2 The Monte Carlo Principle
1067
1.19.2.1 Illustration of the MC principle
In this section, we will illustrate the MC principle, that averages obtained from several random experi-
ments tends to stabilize. As the running example we will focus on a problem mentioned in the famous
letters between Pascal and Fermat, ﬁrst writings that are considered to mark the start of the modern
probability theory [13]. A French nobleman and gambler Chevalier de Méré contacted Pascal. Méré
was betting that in four rolls of a single die, at least one six would turn up. Later, Méré “extended” his
schema where he was betting that in 24 rolls of two dice, a pair of sixes would turn up. He was not
happy with this new schema and was calling Pascal for an explanation.
Indeed, the analytical solution for this problem is elementary; we merely calculate the probability
that in 4 consecutive independent throws at least a 6 comes up. A quick calculation shows indeed there
is a slight advantage for Méré:
Pr{Méré wins in 4 throws} = 1 −Pr{no 6 is thrown}4
= 1 −(5/6)4 = 0.5177.
In a sense, this experiment is equivalent to playing head and tail with a carefully forged coin. The bias is
small and may not be easily detected by a naive opponent. Like all Casinos, Méré exploited this subtle
bias to his advantage to earn money on average in repeated trials.
However, the analytical solution for the two dice game reveals that throwing the dice only 24 times
is not a good bet for Méré:
Pr{ Méré wins in 24 throws} = 1 −(35/36)24 = 0.4914,
but with 25 throws, the odds turn into his favor as
Pr{Méré wins in 25 throws} = 1 −(35/36)25 = 0.5055.
The natural question here is if one could detect with certainty that there is indeed a systematic deviation
from 0.5 by just looking at the outcomes of a sequence of games. Indeed for many problems of interest,
such probability calculations will be signiﬁcantly harder, if at all possible, and it is natural to ask how
reliable it is to estimate such quantities using data obtained from MC experiments.
Playing the game repetitively is a MC experiment. By just recording the outcome of each game (and
the total number of games played), we could in principle estimate the winning probability. To be more
precise, consider the single die, 4 times throw game. Let x(n)
k
denote the outcome of the die at kth throw
in the nth game. Formally, a game is represented by the tuple x ≡(x1, x2, x3, x4). We assume a fair
die, as such for k = 1, . . . , 4
xk ∼U(1, 6),
where U(a, b) denotes a uniform distribution on integers x such that a ≤x ≤b. Since each throw in
the game is independent, the target distribution π(x) is the uniform distribution on X = {1, . . . , 6}4
and each game is simply a random point in X.
To estimate the probability of winning, we deﬁne formally the indicator function for winning the nth
game
ϕ(x(n)) = I

0 <
4

k=1
I

x(n)
k
= 6

.

1068
CHAPTER 19 A Tutorial Introduction to Monte Carlo Methods
Here, I{x} is an indicator function that is 1 if x is true and 0 if false. The test function ϕ looks awkward
but it just checks if in 4 throws at least a 6 is obtained. Finally, the probability θ that Méré wins in 4
throws can be estimated as
θ = Eπ(ϕ(x)) ≈1
N
N

n=1
ϕ(x(n)) = ¯Eϕ,N.
Similarly, for the two dice, 24 throws game, formally we let x(n)
i,k denote the outcome of the die i at kth
throw in the nth game.
ϕ(x(n)) = I

0 <
24

k=1
I

x(n)
1,k = 6

I

x(n)
2,k = 6

,
¯Eϕ,N = 1
N
N

n=1
ϕ(x(n)).
(19.1)
The mean and variance of φ(x(n)) are
E

ϕ(x(n))

= θ
Var{ϕ(x(n))} = θ(1 −θ)
respectively.1 As these are ﬁnite, the strong law of large numbers applies. The law guarantees the very
intuitive fact that with increasing number of games N, the estimates become increasingly more and more
accurate. Technically, when N →∞we have convergence
¯Eϕ,N →θ.
The “speed” is given by the central limit theorem, that says that for large N the estimate is distributed
¯Eϕ,N ∼N( ¯Eϕ,N
		 θ, θ(1 −θ)/N).
Provided that we can generate independent samples, the error will be O(N −1/2). The most important
aspect of this convergence result, that can not be overemphasized is that the error does not depend on
the dimensionality of the parameter to be estimated!
To illustrate these concepts with an example consider Figure 19.1 where we simulate the estimate
for a two dice, 24 throw game case for several N. The path shows the estimate ¯Eϕ,N in Eq. (19.1) for
each N. We can see that the path converges to the true value. For each N, the error bars correspond to
the standard deviations given by the central limit theorem.
1The mean and variance of a Bernoulli random variable.

1.19.2 The Monte Carlo Principle
1069
0
100
200
300
400
500
600
700
800
900
1000
0.2
0.3
0.4
0.5
0.6
0.7
0.8
CLT
θ = 0.5
N
¯E ϕ,N
FIGURE 19.1
The estimate of winning probability for a pair of 6 in 24 throws game. The horizontal solid line shows the
true odds θtrue that is smaller than 0.5. The path shows the estimate ¯Eϕ, N as a function of N. Law of large
numbers says that all these curves eventually will converge to the true θtrue, shown as a solid line.
To illustrate the implication of the central limit theorem, consider now a collection of estimates,
obtained from results of N games per night with a total of K different nights. Formally, the outcome of
the nth game at the kth night is denoted as ϕ(x(n,i)) for i = 1, . . . , K and n = 1, . . . , N. The estimate
at the ith night after observing the Nth game as
¯E(i)
ϕ,N = 1
N
N

n=1
ϕ(x(n,i)).
Now, we visualise all the paths together, where each path is depicted corresponds to the sequence of
estimates for each independent night i (Figure 19.2). Each path corresponds to the estimate as a function
of the number of games played N. We can clearly see that all the independent paths eventually stay in
the “narrow tube,” as predicted by the CLT.
In the above examples, instead of experimenting with real dice, we have simulated the experiment on a
computer. It turns out that it is surprisingly delicate to simulate “truly” random numbers on deterministic
hardware. Instead, pseudo-random numbers are generated using deterministic algorithms and are brieﬂy
reviewed in the next section.

1070
CHAPTER 19 A Tutorial Introduction to Monte Carlo Methods
0
100
200
300
400
500
600
700
800
900
1000
0.2
0.3
0.4
0.5
0.6
0.7
0.8
CLT
θ = 0.5
N
¯E ϕ,N
FIGURE 19.2
Veriﬁcation of the central limit theorem. We estimate the winning probability for the 2 dice, 24 throws game.
Each path corresponds to the estimate ¯E (i)
ϕ, N as a function of N, where i = 1, . . . , K denotes the independent
sequences of games. CLT says that at any ﬁxed N, ¯E (i)
ϕ, N the values of these estimates is approximately
normal distributed with standard deviation σ/
√
N. For each N, the red dotted curves designate the [θtrue −
3σ/
√
N, θtrue+3σ/
√
N]. It is a common folklore in statistics to plot the ±3σ interval for a univariate Gaussian
as this interval contains more than 99% of the probability mass. As given by the CLT, the paths exit the
±3σ/
√
N tube only occasionally.
1.19.2.2 Random number generation
“The generation of random numbers is too important to be left to chance”2 and truly random numbers
are impossible to generate on a deterministic computer. Published tables or other mechanical methods
such as throwing dice, ﬂipping coins, shufﬂing cards or turning the roulette wheels are clearly not
very practical for generating the random numbers that are needed for computer simulations. Other
techniques, more suited to computation exist; these rely on chaotic behavior, such as the thermal noise
in Zener diodes or other analog circuits as well as the atmospheric noise (see, e.g., www.Random.org)
or running a hash function against a frame of a video stream. Still, the vast amount of random numbers
are obtained from pseudo-random number generators. Apart from being very efﬁcient, one additional
advantage of these techniques is that the sequences are reproducible by setting a seed, this property is
key for debugging MC code.
The most well known method for generating random numbers is based on a Linear Congruential
Generator (LCG). The theory is well understood, the method is easy to implement and it runs very fast.
2Wikipedia entry on Pseudo Random Number generator.

1.19.3 Basic Techniques for Simulating Random Variables
1071
A LCG is deﬁned by the recurrence relation:
xn+1 ≡(axn + c) (mod M).
If the coefﬁcients a and c are chosen carefully (e.g., relatively prime to M), x will be roughly uniformly
distributed between 0 and M −1. Roughly uniformly means that, the sequence of numbers xnwill pass
many reasonable tests for randomness. A such test suite is the so called DIEHARD tests, developed
by George Marsaglia [14] that are a battery of statistical tests for measuring the quality of a random
number generator.
A more recently proposed generator is the Mersenne Twister algorithm, by Matsumoto and Nishimura
[15]. It has several desirable features such as a long period and being very fast. Many public domain
implementations exist and it is the preferred random number generator for statistical simulations and
Monte Carlo computations.
Good random number generators provide uniformly distributed numbers on an interval. Hence,
provided we have a good random number generator that can generate uniformly random integers (say
between 0 and 264 −1), the resulting integers can be used to generate double precision numbers almost
uniformly distributed in [0, 1). We will assume in the sequel that we have access to a good uniform
random number generator that can generate real numbers on [0, 1); knowing that in practice such
a generator generates only rational numbers (say double precision ﬂoating point numbers). We will
denote this generator by U(0, 1) and denote a realization, i.e., a sample drawn as u ∼U(0, 1).
1.19.3 Basic techniques for simulating random variables
The techniques described in this section sketch the basic techniques to generate (mostly univariate and
bivariate random variables) with a given density. This is a surprisingly deep and interesting subject
touching many facets of computer science. For further information, the reader is invited to look at the
fantastic book by Luc Devroye, on non uniform random variate generation [16].
1.19.3.1 Inversion
The basic technique is based on transformation; we generate a uniform random number u ∼U(0, 1) and
transform it to obtain x = g(u) where g is a function. This method requires calculating the cumulative
density function and inverting it. Recall the deﬁnition of a Cummulative Density function (CDF)
FX(x) =

 x
−∞
fX(τ)d⊤,
where fX is the probability density of the random variable X. The key observation behind the inversion
method is the fact that when X ∼fX, the quantity U = FX(X), when viewed as a random variable,
is uniformly distributed on [0, 1). To see this, assume that the inverse of FX exists, and we can ﬁnd
X = F−1
X (U),
FX(x) = Pr{X < x},
FX(F−1
X (u)) = Pr{F−1
X (U) < F−1
X (u)},
u = Pr{U < u} 0 ≤u ≤1.

1072
CHAPTER 19 A Tutorial Introduction to Monte Carlo Methods
1
2
3
4
5
6
0
0.2
0.4
p(x)
1
2
3
4
5
6
0
0.5
1
u
FIGURE 19.3
Generation of samples from a discrete distribution. We generate u uniformly on [0, 1) and ﬁnd the corre-
sponding x via the generalised inverse F −(u). The discrete case is particularly intuitive: think of dividing a
stick of length 1 into pieces of length πi = fX (xi) and label each region with xi. Select a point u uniformly
random and return the label of the region that u falls in.
By taking the derivative, we see that the density of u is uniform. The argument works also in the opposite
direction: when we ﬁrst choose uniform U and pass it through the function X = F−1
X (U), the density
of X will be fX. To allow for discrete distributions or continuous densities with atoms (where single
points have positive probability mass), we deﬁne the generalised inverse CDF
F−(u) ≡inf{x : F(x) ≥u}.
The x generated by this method are guaranteed to be distributed by fX. Figure 19.3 illustrates this
construction. The generalized inverse allows for jumps in the cumulative density when atoms have
nonzero probability (see Figure 19.4).
Example 1.
Exponential random variables can be derived by the inversion method from uniform
samples. The exponential density with rate parameter λ is
E(x; λ) = λexp( −λx).
The CDF of the exponential distribution is found as
F(x) =

 x
0
λexp( −λτ) = −exp( −λx) + 1.
The generalised inverse is obtained via
u = F(x) = 1 −exp( −λx),
x = −log (1 −u)/λ = F−(u).

1.19.3 Basic Techniques for Simulating Random Variables
1073
x
0
0.2
0.4
0.6
0.8
1
F(x)
F−(u)
u
FIGURE 19.4
Visualisation of the generalised inverse. The blue curve shows a CDF of a probability distribution with an
atom, as can be seen from the discontinuity. For a given probability u, F −(u) is the “minimum” x such that
the probability of the interval (∞, x] is equal or exceeds u.
We can even avoid calculating 1 −u by noting that both u and w = 1 −u are distributed uniformly on
[0, 1], so
x = F−(w) = −log (w)/λ.
1.19.3.2 Transformation
The generalised inverse method works by applying a suitable function (the generalized inverse F−(u))
on a random input with a known density (u with a uniform density). This method can be made more
general by a technique known as the change of variables [4].
To illustrate the transformation method, we will ﬁrst give an example where the mapping function
will be afﬁne (that is linear plus a constant term). Suppose we are given a random variable X with density
fX(x) and wish to ﬁnd the density of Y where
Y = aX + b.
It is clear that the density of Y, denote by fY should be somewhat related to fX. For example if X is
uniform ( fX = U) on [0, 1) and a = 2 and b = −1, one could see that Y is uniform on [−1, 1). The
general case, when fX is any density, is perhaps slightly harder to see. To ﬁnd the density of Y, we ﬁrst
will ﬁnd the CDF FY (y) of Y and obtain the desired density fY (y) by taking the derivative w.r.t. y. In
particular,
FY (y) = Pr{Y ≤y} = Pr{aX + b ≤y} =
 Pr{X ≤(y −b)/a}, a > 0,
Pr{X ≥(y −b)/a}, a < 0.
By taking derivative we see that the density is given by
fY (y) = dFY
dy = 1
|a| fX((y −b)/a).

1074
CHAPTER 19 A Tutorial Introduction to Monte Carlo Methods
For the more general case, we have a more general mapping g where
Y = g(X),
X = g−1(Y).
The density can be found as
FY (y) = Pr{Y ≤y} = Pr{g(X) ≤y} = Pr{X ≤g−1(y)}
fY (y) = dFY
dy = fX(g−1(y))
				
dg−1(y)
dy
				 ≡fX(x(y))|J(y)|
(19.2)
here, the derivative of g−1 is denoted as the Jacobian and the absolute value ensures the positivity of
the densities. In many textbooks it is common to drop the explicit reference to the function g; authors
often use y(x) for y = g(x) and x(y) for the inverse g−1(y). In the sequel, we will also adopt this
notation. The multivariate case is analogous; here we illustrate the bivariate case, the most encountered
case in derivations. The function g, that is also called a transform, is deﬁned as
(y1, y2) = g(x1, x2).
If we can invert g, which is not always easy to do analytically, we ﬁnd
x1 = x1(y1, y2),
x2 = x2(y1, y2).
The derivative term is the Jacobian determinant deﬁned as
J =
				
∂x1/∂y1 ∂x2/∂y1
∂x1/∂y2 ∂x2/∂y2
				 = J(y1, y2).
Consequently, the density of the transformed variable, similar to Eq. (19.2), is given by
fY (y1, y2) = fX(x1(y1, y2), x2(y1, y2))|J(y1, y2)|.
Example 2.
To illustrate the method, we describe the Box-Müller method for generating normal
random variables. The Box-Müller method generates the pair (x1, x2)
x1 ∼E(x1; 1/2) = 1
2exp( −x1/2),
x2 ∼U(x2; 0, 2π) = 1
2π I{0 ≤x2 ≤2π},
where x1 is the square of the magnitude and x2 is the angle of a point in polar coordinates. The method
transforms this point into cartesian coordinates
y1 = √x1 cos (x2),
y2 = √x1 sin (x2).

1.19.3 Basic Techniques for Simulating Random Variables
1075
It turns out, that y1 and y2 obtained via this method are independent and unit Gaussian (N(0, 1))
distributed. To show this, we ﬁnd the inverse mapping
x1 = y2
1 + y2
2,
x2 = arctan(y2/y1).
The Jacobian determinant is found as
J =
				
∂x1/∂y1 ∂x2/∂y1
∂x1/∂y2 ∂x2/∂y2
				 =
					
2y1
1
1+(y2/y1)2
−y2
y2
1
2y2
1
1+(y2/y1)2
1
y1
					 = 2.
The resulting density is
fY (y1, y2) = E(x1(y1, y2); 1/2)U(x2(y1, y2); 0, 2π)|J(y1, y2)|
= 1
2exp( −(y2
1 + y2
2)/2) 1
2π 2 =
1
√
2π
exp( −y2
1/2)
1
√
2π
exp( −y2
2/2)
= N(y1; 0, 1)N(y2; 0, 1).
1.19.3.3 Rejection sampling
Rejection sampling is a technique for indirectly sampling from a target distribution π by sampling
from a proposal distribution q. We reject some of the generated samples to compensate for the fact that
q ̸= π. The algorithm is as follows: We sample x(i) for i = 1, . . . , N independently from q. We accept
the sample x(i) with acceptance probability a(x(i)) where the acceptance probability is
a(x) ≡
π(x)
Mq(x).
Here, M is a positive number that guarantees π(x) ≤Mq(x) for all x, i.e., that Mq(x) completely
covers the density π(x). The approach also works when the normalization constant Z of π is unknown,
that is we can only evaluate φ pointwise where
π(x) = 1
Z φ(x).
In this case, we let the acceptance probability be
˜a(x) ≡
φ(x)
˜Mq(x)
,
where φ(x) ≤
˜Mq(x) for all x. If a suitable M (or
˜M) can be found, the algorithm is simple to
implement and requires only sampling from q and pointwise evaluation of φ. To understand the idea
behind rejection sampling, we observe that for any density function f (x), (including obviously our
target π and the proposal q) we have the following identity:
f (x) =

f (x)
0
1 dτ =

I{0 ≤τ ≤f (x)}dτ.
(19.3)

1076
CHAPTER 19 A Tutorial Introduction to Monte Carlo Methods
In other words, f (x) can be written as a marginal density of a distribution with density I{0 ≤τ ≤f (x)}
on the extended space (x, τ). This density is simply the uniform density over the volume under the
curve f (x). The equation Eq. (19.3) says that generating samples from f (x) is equivalent to uniformly
generating samples on the set A = {(x, τ) : 0 ≤τ ≤f (x)} and then simply ignoring the τ coordinate.
When f (x) = φ(x)/Z and we don’t know Z
f (x) = 1
Z φ(x) = 1
Z

 φ(x)
0
1dτ = 1
Z

I{0 ≤τ ≤φ(x)}dτ.
(19.4)
By integrating over x on both sides we get the normalization constant Z as the area of the region
A = {(x, τ) : 0 ≤τ ≤φ(x)}:
Z =

 
I{0 ≤τ ≤φ(x)}dτ dx.
Hence
f (x) =

I{0 ≤τ ≤φ(x)}dτ
 
I{0 ≤τ ≤φ(x)}dτ dx .
This equation suggests that f (x) is the marginal density of uniform density over the set A. A useful
metaphor for understanding the concept is of thinking ﬁshes in a lake [5], where x is a coordinate on the
water surface and τ is the depth, which is illustrated in Figure 19.5. If all ﬁshes in the lake are distributed
uniformly in the water, when viewed from above they will be marginally distributed with density f (x)
(see Figure 19.6).
Note that the reasoning of Eq. (19.3) works also in the reverse direction: when we have samples
x(i) for i = 1, . . . , N generated from a proposal q(x), we can select the τ coordinate for each x(i)
simply by choosing τ (i) uniformly on [0, Mq(x(i))]. The resulting pairs (x(i), τ (i)) will be uniformly
distributed on the set AMq = {(x, τ) : 0 ≤τ ≤Mq(x)}. Provided that φ(x) ≤Mq(x), if we now
select only the pairs such that τ (i) < φ(x(i)), these accepted tuples will be distributed uniformly on the
set Aφ = {(x, τ) : 0 ≤τ ≤φ(x)}. But, by Eq. (19.4), the marginal of this uniform density will be p.
FIGURE 19.5
Fishes in a lake. We view the bivariate density function f (x) as the bottom surface of a lake. Take uniformly
distributed points in this volume. When viewed from above, i.e., when we ignore their depth, the points will
be distributed more densely where the lake is deeper.

1.19.4 Markov Chain Monte Carlo
1077
−8
−6
−4
−2
0
2
4
6
8
10
0.5
1
1.5
2
2.5
3
τ
x
φ(x)
˜Mq(x)
FIGURE 19.6
Rejection sampling. Sampling x from the proposal q(x) and then sampling the associated τ uniformly on
[0, ˜Mq] results in uniformly distributed tuples (x, τ) (all the points under ˜Mq(x)). Restricting those to only
points such that τ ≤φ(x) provides the accepted points (red dots), the other points are rejected (blue crosses).
Note that the probability Pr{τ ≤φ(x)} = φ(x)/ ˜Mq(x) is the acceptance probability ˜a(x).
Algorithm 1. Rejection Sampling
1: Construct an easy to sample density q(x) and positive number ˜M such that φ(x) < ˜Mq(x)
2: for i = 1, 2, 3, . . . do
3:
Draw x(i) ∼q(x)
4:
Accept x(i) with probability ˜α(x(i)) = φ(x(i))/ ˜Mq(x(i))
5: end for
1.19.4 Markov Chain Monte Carlo
In Markov Chain Monte Carlo methods [6,17–20], instead of directly attempting to sample from π(x),
we sample from a Markov chain with transition density (“kernel”) K(x(n)|x(n−1)) where n indexes the
sample number.3 To see how this connects to our ultimate desire to sample from a target π(x) consider
ﬁrst drawing samples from the Markov chain. Given an initial state x(1), we draw x(2), then conditioned
on x(2) we draw x(3) and so on by iteratively drawing from K(x(n)|x(n−1)). The distribution of state
occupancy at time n, πn(x), is found from
πn(x) =

K(x|x′)πn−1(x′)dx′
3Note that, the sample number is conceptually different than the “time” index in a time series model; for example, in a time
series model xt corresponds to the state at time t, whereas here xn can correspond to a vector of all state variables—a full
state trajectory. In this context, it is convenient to think of n simply as the iteration number of the sampling algorithm and
x(n) as a particular realisation of all random variables being sampled.

1078
CHAPTER 19 A Tutorial Introduction to Monte Carlo Methods
which we write more compactly as πn = Kπn−1. This generates a sequence of marginal distributions
π1, π2, . . . Does this sequence of distributions converge and, if so, what does it converge to ? A stationary
distribution, π satisﬁes π = Kπ. A key result here is that for an ergodic chain (i.e., irreducible (chain)
and aperiodic (chain)) there exists a unique distribution π for which π = Kπ and therefore a unique
stationary distribution to which all the occupancy distributions converge, irrespective of their initial
states [7]. For ﬁnite state Markov chains, irreducibility means that each state can be visited starting
from each one (related to connectedness of the state transition diagram) and aperiodicity means that
each state can be visited at any time n larger than some ﬁxed number.
The ﬁrst question is, given a Markov chain with transition kernel K, how to ﬁnd the marginal
distribution πn(x) for large n. The second question is how to construct a transition kernel that is easy
to sample from and converges to the desired target distribution. In the ﬁnite state case, a solution to the
ﬁrst question is provided by the eigenvalue-eigenvector decomposition of the transition matrix that will
be introduced in the next section. The answer to the second question turns out to be very elegant and
simple and underlies the celebrated Metropolis-Hastings Markov Chain Monte Carlo method [2,21]
that will be covered afterwards.
1.19.4.1 Stationary distribution of a Markov Chain with ﬁnite number of states
We ﬁrst focus our attention on the convergence of a Markov chain and illustrate results in the ﬁnite state
case. The ﬁnite state case may seem to be too restrictive, and in a sense it indeed is. From a mathematical
perspective, the theory of Markov chains for more general spaces is more delicate and it does not rely
on linear algebra. Nevertheless, it turns out that the results for more general state spaces have similar
ﬂavor and understanding what is happening in the ﬁnite case provides the basic intuition about the
issues involved. For technicalities on countable state spaces, see [7] or uncountable state spaces, see
[22], Chapter 13.
For the ﬁnite state case, we proceed as follows. First, we note that the marginals of a Markov chain
satisfy the following recursive relation
πn(xn) =

xn−1
K(xn|xn−1)π(xn−1).
Now, a probability mass function on a ﬁnite state space is simply a vector of positive entries which sum
up to one so in index notation we write
πn(i) =

j
Ki, jπn−1( j),
where the marginal distribution of the chain is denoted by πn(i) ≡πn(xn) and the transition model is
denoted by the transition matrix K with the entries Ki, j = K(xn = i|xn−1 = j). Adopting the matrix
notation, we write πn = Kπn−1. This gives the expression for the marginal πn as a function of the
initial occupancy density π0
πn = K nπ0.
This is simply a (linear) ﬁxed point iteration and the question reduces to ﬁnding a stationary point π
that satisﬁes
π = lim
n→∞K nπ0 ≡K ∞π0,

1.19.4 Markov Chain Monte Carlo
1079
where K ∞= limn→∞K n. An eigen-decomposition of the transition matrix K
K = DD−1
provides insight about the limit behavior of K n where D is the matrix of eigenvectors and  is a diagonal
matrix of eigenvalues. It turns out that for transition matrices K, the maximum eigenvalue is always 1. If
the magnitude of the second largest eigenvalue λ2 is strictly less, |λ2| < 1, we have a unique stationary
distribution. To see this consider
K n = DD−1DD−1 · · · DD−1 = Dn D−1.
But as  is diagonal, n for large n is a matrix very close to all zeros matrix with only a single 1 on the
diagonal. Say without loss of generality that the ﬁrst eigenvalue λ1 = 1 is at position (1, 1) (otherwise
construct a permutation matrix P and consider T = (DP⊤)(PP⊤)(P D−1) = ˜D ˜ ˜D−1). Then K ∞
is the rank-one matrix given by the outer product
K ∞= D( :, 1)D−1(1, : ).
Here, D( :, i) denotes the ith column of the matrix D and D−1(i, : ) is the ith row of D−1. These are
respectively the right and left eigenvectors. But what is D−1(1, : ) ? By deﬁnition we have
D−1K = D−1,
D−1(1, : )K = (1, 1)D−1(1, : ) = D−1(1, : ).
But as K is a transition matrix, all its columns sum up to one. In other words, if we would left multiply
K by an all-ones vector v, we get vK = v so it is easy to see that the left eigenvector corresponding to
the largest eigenvalue is proportional to the all ones vector
D−1(1, : ) =
 1 1 . . . 1
.
But this implies that the columns of K ∞are all the same as
K ∞= [D( :, 1), D( :, 1), . . . , D( :, 1)].
We can conclude that, regardless of the initial state, the probabilities are all the same and the stationary
density is proportional to the (right) eigenvector corresponding to the eigenvalue λ1 = 1. This is
pictorially illustrated using a random transition matrix K in Figure 19.7. In the MC literature, we use
the term transition kernel instead of transition matrix as the latter is restricted only to countable state
spaces.
A natural question to ask is how fast a chain converges to stationarity. For Markov Chains, the rate
of convergence is governed by the second eigenvalue. The convergence to the stationary distribution is
geometric and is given by
∥K nπ0 −π∥var ≤C|λ2|n,
where C is a positive constant and the function ∥·∥var measures the distance between two densities. It
is known as the total variation norm and is given by
∥P −Q∥var ≡1
2

s∈X
|P(s) −Q(s)|.

1080
CHAPTER 19 A Tutorial Introduction to Monte Carlo Methods
K
K 2
K 5
K 10
K ∞
x 0
x 0
x 0
x 0
x 0
x 1
x 2
x 5
x 10
x ∞
FIGURE 19.7
The powers K τ of a transition matrix K(xn = i|xn−1 = j) with τ = 1, 2, 5, 10, ∞. Darker colors correspond
to higher transition probabilities. The identical columns of K ∞depicts the stationary distribution, which
is reached independent of any starting density. We see that for this transition matrix K, even after a few
iterations τ the columns of K τ are close to the stationary density, suggesting a rapid convergence. The
magnitude of the second eigenvalue is |λ2| ≈0.64 which veriﬁes that our observation is indeed correct.
However, for a large transition matrix, it may be hard to show algebraically that |λ2| < 1. Fortunately,
there is a convergence result that states that if K is irreducible and aperiodic, then there exist 0 < r < 1
and C > 0 s.t.
∥K nπ0 −π∥var ≤Crn,
where π is the invariant distribution [20]. This result also generalizes to more general Markov chains,
such as on countable inﬁnite or uncountable state spaces where explicit calculation of the second largest
eigenvalue of the transition kernel is intractable.
To see an example how convergence speed is affected consider a very simple chain xn with two
states, labeled as 1 and 2 with the transition matrix
K =

ϵ
1 −ϵ
1 −ϵ
ϵ

,
(19.5)
where 0 ≤ϵ ≤1. In other words, if at time t the chain is in state i, with probability ϵ, the chain stays
put, or jumps to the other state with probability 1 −ϵ. One natural question to ask is: where would we
ﬁnd the chain if we stop it at a large n ? Say we start initially from state i but in the previous section we
saw that this initial choice won’t matter (most of the time). It turns out that for 0 < ϵ < 1 the answer
is 0.5: it is equally likely to ﬁnd the chain in any of the two states, regardless of the initial state but for
each ϵ the rate of convergence is different. We illustrate these concepts on Figure 19.8.
1.19.4.2 Designing a Markov Chain for a given target distribution: The
Metropolis-Hastings method
The discussion of the previous section suggests that, if we can design a transition kernel K such that
the target density π(x) is its stationary distribution, at least in principle one can generate samples from
the Markov chain that eventually will tend to be drawn from the target distribution. After ignoring sam-
ples obtained from an initial “burn in” period, as the chain moves towards the stationary distribution,
the generated samples can be subsequently used to estimate expectations under the target distribu-
tion, as if they are independent. There are a couple of caveats: formally we require the chain to be

1.19.4 Markov Chain Monte Carlo
1081
1
2
1
2
1
2
1
2
1
2
1
2
1
2
n
= 0
= 1 / 6
= 2 / 6
= 3 / 6
= 4 / 6
= 5 / 6
= 1
FIGURE 19.8
Convergence to the stationary distribution for the transition matrix K deﬁned in Eq. (19.5) with ϵ =
0, 1/6, 2/6, . . . , 1 from top to bottom. Horizontal axis denotes the time n and the vertical axis is the state x.
The probability πn(x) is high for darker regions. For ϵ = 1 (bottom), the state transition matrix is K = I and
the states are no longer connected. The chain fails to be irreducible and the chain fails to have a unique
stationary distribution. For ϵ = 0 (top), the states alternate periodically. The chain is periodic and similarly
fails to have a unique stationary distribution. For all other cases with 0 < ϵ < 1 we have convergence to the
uniform distribution, whilst with different speed. The second eigenvalues are λ2 = 2ϵ −1; as expected the
chains converge slower when |λ2| is closer to one.
ergodic—otherwise the chain might never reach the desired stationary distribution. Even if the chain is
ergodic, typically we would not know in practice how close the chain is to the stationary distribution.
Here we set aside the issue of ergodicity and concentrate on designing a transition kernel K for a given
target π. This is surprisingly simple via the approach proposed by Metropolis [1], later generalised by
Hastings [21], for a rigorous treatment see [17,23]. Note that this approach does not provide explicit
formula for the transition Kernel K (which is often intractable to compute even in ﬁnite state spaces);
it merely states a procedure to draw a new sample given the previous one. Suppose we are given a
target density π = φ/Z, where Z is a (possibly unknown) normalisation constant. The Metropolis-
Hastings (MH) algorithm uses a proposal density q(x′|x), which generates a candidate sample x′
possibly dependent4 on the current sample x. The algorithm is very simple, we deﬁne a chain by the
4Note that whilst one can consider proposals q(x′) which are independent of x, the actual MH transition K(x′|x) nevertheless
depends on x.

1082
CHAPTER 19 A Tutorial Introduction to Monte Carlo Methods
following rule: Accept the proposed sample with probability
α(x →x′) = min

1, q(x|x′)π(x′)
q(x′|x)π(x)

,
where α is the acceptance probability satisfying clearly the condition 0 ≤α(x|x′) ≤1. If the sample is
not accepted, the chain stays put at the old location x. This procedure implicitly deﬁnes the following
MH transition kernel K:
K(x|x′) = q(x|x′)α(x|x′) + δ(x −x′)ρ(x′),
(19.6)
here, δ(x) is the Dirac delta ρ is the rejection probability satisfying 0 ≤ρ(x′) ≤1 is given by
ρ(x′) ≡

(1 −α(x|x′))q(x|x′)dx.
This simply sums up all the probability of rejection. Note that we don’t directly sample from K, we
merely sample from the proposal q but reject some of the samples. In practice we typically don’t need
to evaluate K – indeed very often this is intractable. Recall that the stationary distribution should satisfy
the following condition
π(x) =

K(x|x′)π(x′)dx′.
Algorithm 2. Metropolis-Hastings
1: Initialise x(1) arbitrarily
2: for n = 2, 3, . . . do
3:
Propose a candidate: xnew ∼q(xnew|x(n−1))
4:
Compute acceptance probability:
α(xnew		 x(n−1)) = min

1, q(x(n−1)|xnew)π(xnew)
q(xnew|x(n−1))π(x(n−1))

5:
Sample from uniform distribution: a ∼U(0, 1)
6:
if a < α then
7:
Accept candidate: x(n) ←xnew
8:
else
9:
Reject candidate: x(n) ←x(n−1)
10:
end if
11: end for
For an arbitrary kernel K, it is hard to verify stationarity. However, if K happens to satisfy a stronger
condition,5 known as the detailed balance condition
K(x′|x)π(x) = K(x|x′)π(x′),
5A sufﬁcient condition; a kernel K may have π as the stationary distribution while not satisfying detailed balance.

1.19.4 Markov Chain Monte Carlo
1083
one sees directly that π must be the stationary distribution by integrating both sides over x′. Veriﬁcation
of detailed balance for the Metropolis-Hastings kernel is straightforward:
K(x|x′)π(x′) = (q(x|x′)α(x|x′) + δ(x −x′)ρ(x′))π(x′)
= q(x|x′)min

1, q(x′|x)π(x)
q(x|x′)π(x′)

π(x′) + δ(x −x′)ρ(x′)π(x′)
= min{q(x|x′)π(x′), q(x′|x)π(x)} + δ(x −x′)ρ(x′)π(x′)
= q(x′|x)min
q(x|x′)π(x′)
q(x′|x)π(x) , 1

π(x) + δ(x′ −x)ρ(x)π(x)
= K(x′|x)π(x).
Note that to compute the acceptance probability α, we only need to evaluate π up to a normalisation,
since the normalisation constant cancels out. For a given target π and assumed proposal q(x′|x) we now
have a procedure for sampling from the Markov chain K(x′|x) with stationary distribution π. Sampling
from the transition (19.6) can be achieved using the procedure detailed in Algorithm 2.
1.19.4.3 Illustration
In this section, we will derive a MH algorithm for sampling from a discrete distribution π(x) = φ(x)/Z
where x ∈{1, 2, . . . , 20}. The target, which has two bumps is shown on Figure 19.9. Our proposal
q(x′|x) is a mixture of a random walk with a uniform density: with probability ν, we choose a random
walk. If we choose the random walk, with equal probability we let x′ ←max{0, x −1} or x′ ←
min{20, x + 1}. Alternatively, with probability 1 −ν, we choose x′ uniformly on {1, 2, . . . , 20}. The
random walk component corresponds to a “local exploration” whereas the uniform jump corresponds
to a “restart.” Here, the parameter ν is a algorithm parameter that will effect the behavior of our overall
proposal and we will show how this may effect the convergence rate of the resulting MH algorithm. In
Figure 19.10, we show the proposals and corresponding MH transition matrices.
0
2
4
5
10
15
20
0.5
1
1.5
2
2.5
x
φ(x)
5
10
15
20
5
10
15
20
0.2
0.4
0.6
0.8
1
x (n−1)
x new
FIGURE 19.9
Target density and the acceptance probability α(xnew |x(n−1)) under a symmetric proposal q(xnew |x(n−1)) =
q(x(n−1)|xnew ).

1084
CHAPTER 19 A Tutorial Introduction to Monte Carlo Methods
5
10
15
20
5
10
15
20
0
0.1
0.2
0.3
0.4
0.5
x(n–1)
x(n)
x(n)
x(n)
x(n)
x(n)
x(n)
ν = 0 .1
5
10
15
20
5
10
15
20
0
0.1
0.2
0.3
0.4
0.5
x (n–1)
x (n–1)
x (n–1)
x (n–1)
ν = 0 .5
5
10
15
20
5
10
15
20
0
0.1
0.2
0.3
0.4
0.5
ν = 0 .99
5
10
15
20
5
10
15
20
0.2
0.4
0.6
x(n–1)
ν = 0 .1
5
10
15
20
5
10
15
20
0.2
0.4
0.6
ν = 0 .5
5
10
15
20
5
10
15
20
0.2
0.4
0.6
ν = 0 .99
FIGURE 19.10
(Top, left to right) The proposal density with ν = 0.1, 0.5, 0.99. (Bottom, left to right) The corresponding
transition matrices K, computed by Eq. (19.6).
A moments thought would reveal that the local moves are not sufﬁcient to cross the low probability
region between the two bumps of the target, so setting ν close to 1 might be a poor choice in terms of
the convergence, also known as the mixing time. In this case, having the uniform proposal is certainly
useful as it may help the chain to investigate the state space, hence reducing the mixing time. However,
longer jumps have the risk of hitting low probability regions, increasing the rejections. The optimal
choice for ν seems to be somewhere in between those two extremes and we will numerically illustrate
this.
In this simple example, we can numerically compute the effective MH transition matrix and compute
the spectral gap, given as 1 −|λ2|; the distance between magnitudes of two largest eigenvalues. For
all ν, the spectral gap is plotted in Figure 19.11. The bigger this gap, the faster is the convergence: an
illustration of this fact is shown in Figure 19.12.
For real problems, the above computations are typically intractable but fortunately they are also not
needed at all for the application of the MH algorithm. All we need is to sample from the proposal and
evaluate the acceptance probability. In Figure 19.13, we show the results that would reﬂect a typical
MH run. For different parameter settings with ν = 0.1, 0.5, 0.99, we generate sample paths using the
algorithm in 2. Then, following a burn in period of 100 iterations, for each state x we simply count the
number of times the chain visits x to estimate π(x). Formally, we construct the estimator
¯πn(x) =
1
n −100
n

τ=101
[x(τ) = x].
(19.7)
Note that, while the samples x(τ) are correlated, we calculate our estimate as if they are independent. This
is still valid as after convergence all the x(τ) are effectively drawn from the same stationary distribution
π(x). The results are shown in Figure 19.13.

1.19.4 Markov Chain Monte Carlo
1085
0
0.2
0.4
0.6
0.8
1
0
0.05
0.1
0.2
0.15
0.25
ν
1 −| λ2|
FIGURE 19.11
The spectral gap (1 −|λ2|) computed numerically for the transition matrix K obtained using the proposals
shown in Figure 19.10. The picture suggests that for fastest convergence, we need to set ν ≈0.1. Only
using the local moves (ν = 1 case) results in poor mixing and uniform proposal seems to be a better choice
for this problem. Yet, using local moves occasionally improves mixing as the gap at ν = 0.1 is slightly bigger
than ν = 0.
5
10
15
20
25
30
5
10
15
20
0
10
20
30
0
1
2
x n
π n −π
var
n
ν = 0 .1
5
10
15
20
25
30
5
10
15
20
0
10
20
30
0
1
2
x n
π n −π
var
n
ν = 0 .5
5
10
15
20
25
30
5
10
15
20
0
10
20
30
1
1.5
2
x n
π n −π
var
n
ν = 0 .99
FIGURE 19.12
Convergence to the stationarity when using the proposal with parameters ν = 0.1, 0.5, 0.99. (Top) The
marginals πn, as computed via K nπ0 where darker color indicates higher probability. (Bottom) The total
variation distance between stationary distribution π = π∞and the marginal πn. As predicted, the distance
drops very slowly for ν = 0.99 and faster for the other cases. This example suggests that the algorithm is
not very sensitive to ν and a broad range of values are useful in practice.
Example 3(Objectpositionestimationfromrangemeasurements).
Inthissection,wewillillustrate
the MH algorithm on a synthetic example of object localization. In this example, we have 3 noisy sensors
at known positions s j, j = 1, . . . , 3, each measuring with noise the distance to an object at an unknown

1086
CHAPTER 19 A Tutorial Introduction to Monte Carlo Methods
0
200
400
600
800
1000
5
10
15
20
200
400
600
800
1000
5
10
15
20
xn
xn
xn
xn
xn
xn
n
ν = 0.1
0
200
400
600
800
1000
5
10
15
20
200
400
600
800
1000
5
10
15
20
n
ν  = 0.5
0
200
400
600
800
1000
5
10
15
20
200
400
600
800
1000
5
10
15
20
n
ν = 0.99
FIGURE 19.13
Sample paths and estimates of the marginal when using the proposal with parameters ν = 0.1, 0.5, 0.99.
(Top) An example of a sample paths generated by the algorithm. (Bottom) Estimate of the marginal ¯πτ(x)
(see Eq. (19.7)) using averages over a single realization after an initial burn-in period of 100 iterations.
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
1
x 1
x 2
FIGURE 19.14
Sensor locations s1, s2 and s3 (round red points) and the true object position (blue square). The observations
are noisy versions of the true distances.
location x. The setup is shown in Figure 19.14. Each sensor observes a noisy distances with the following
model
y j|x, s j ∼N(y j; ∥x −s j∥, R),
where ∥x∥≡

k x2
k
1/2 denotes the Euclidian distance and R is the noise variance of each sensors.
We assume that we don’t have any information about the position of the object so we choose a ﬂat prior

1.19.4 Markov Chain Monte Carlo
1087
x1
x2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
x1
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
x1
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
x1
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
x2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
x2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
x2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
1
FIGURE 19.15
(Top) The likelihood terms p(yi|x), (Bottom) The target posterior, proportional to φ(x) = 
i p(yi|x).
p(x) ∝1. The solution for this problem is given by the posterior
π(x) = p(x|y1, y2, y3) = 1
Z p(y1|x)p(y2|x)p(y3|x)p(x) ≡1
Z φ(x).
The true target posterior density is depicted in Figure 19.15. As the target posterior π(x) is nonstandard,
we will sample from it using a MH algorithm. We design a MH algorithm with a symmetric random
walk proposal where q(x′|x) = N(x, σ 2I). The acceptance probability is
α(x →x′) = min

1, φ(x′)
φ(x)

.
Note that the symmetric proposal has been canceled out. By construction, the resulting kernel will
admit π as the unique stationary density. In Figure 19.16, we show a sequence of samples obtained from
this MH algorithm. We see that the chain visits states around the high probability region. The consecutive

1088
CHAPTER 19 A Tutorial Introduction to Monte Carlo Methods
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
x1
x2
1
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
FIGURE 19.16
(Left) A state trajectory obtained from the MH algorithm. (Right) A 2-D histogram estimate of the target
density (from a longer state trajectory).
samples are dependent (as can be seen from the connecting lines) but as expected, a 2-D histogram
obtained from a single chain shows a fairly accurate approximation.
1.19.4.4 The Gibbs sampler
The MH schema is a very general technique for deriving MCMC algorithms. This generality stems
partially from the arbitrariness of the proposal q; in complex models the design of a reasonable proposal
such that the chain mixes well can require a lot of work. It would be desirable if somehow the proposal
design phase could be automated. The Gibbs sampler [20,24] is an attempt in this direction.
The Gibbs sampler is suitable for sampling a multivariate random variable x = (x1, . . . , xD) with
intractable joint target density π(x). Gibbs sampling proceeds by partitioning the set of variables x into
a chosen variables xd and the rest, x = (xd, x−d). The assumption is that the full conditional densities
π(xd|x−d) are tractable. One then proceeds coordinatewise by sampling from full conditionals as in
Algorithm 3. The Gibbs sampler is actually a MH schema with a sequence of proposals for d = 1, . . . , D
qd(x|x′) = p(xd|x′
−d)δ

x−d −x′
−d

each corresponding to a Gibbs transition kernels Kd. Moreover, it is easy to check that using this
proposal results in a MH acceptance probability of 1, so that every move is accepted. This guarantees
that each kernel Kd admits the same stationary distribution π as a stationary distribution, so we have
π = Kdπ for all d. Now, we see that π = K1π and π = K2π implies
π = K1K2π

1.19.4 Markov Chain Monte Carlo
1089
Algorithm 3. Gibbs sampler
1: Initialize x(1) = (x1, . . . , xD)(1) arbitrarily
2: for n = 2, 3, . . . do
3:
x(n)
1
∼p(x1|x(n−1)
2
, x(n−1)
3
, . . . , x(n−1)
D
)
4:
x(n)
2
∼p(x2|x(n)
1 , x(n−1)
3
, . . . , x(n−1)
D
)
...
5:
x(n)
D
∼p(xD|x(n)
1 , x(n)
2 , . . . , x(n)
D−1)
6: end for
and by induction
π = (K1K2 . . . K D)π ≡Kπ.
Here, the transition kernel K is the effective Gibbs kernel after we sweep over all the variables xd
once. Unless the full conditionals are degenerate, the effective kernel K will be aperiodic and irreducable
while none of the individual kernels Kd are. Note that we could visit the variables in any deterministic
or order, provided each xd is visited inﬁnitely often in the limit. If we choose a deterministic order,
the resulting sampler is known as a deterministic sca Gibbs sampler; if we choose a random order, not
surprisingly, the sampler is called a random scan Gibbs sampler.
The ease of use of the Gibbs sampler has resulted the method being employed in many applications.
There are very effective programs that generate a Gibbs sampler automatically from a model speciﬁ-
cation, such as BUGS. In the sequel, we will illustrate the Gibbs sampler on a change-point model for
count data [25].
Example 4 (Gibbs sampling for a change-point model).
In this model, at each time t we observe the
count of an event yt. All the counts up to an unknown time τ come from the same distribution after which
the distribution changes. We assume that the change-point τ is uniformly distributed over 1, . . . , T . The
two different distributional regimes up to and after τ are indicated by the random variables λi, i = 1, 2,
which are assumed to follow a Gamma distribution.
G(λi; a, b) =
1
(a)baλa−1
i
e−bλi = e(a−1) log λ−bλ−log (a)+a log b.
Under regime λi, the counts are assumed to be identically Poisson distributed
PO(yt; λi) = λyt
i
yt! e−λi = eytlogλi−λi−log (yt!).
This leads to the following generative model:
p(τ) = 1/T
uniform,
λi ∼G(λi; a, b),
i = 1, 2,
yt ∼
PO(yt; λ1) 1 ≤t ≤τ,
PO(yt; λ2) τ < t ≤T .

1090
CHAPTER 19 A Tutorial Introduction to Monte Carlo Methods
0
5
10
15
20
25
30
35
40
45
50
0
1
2
3
4
5
6
t
yt
FIGURE 19.17
A typical realisation from the change-point model. True intensities are shown with a dotted line. The intensity
has dropped from λ1 = 3.2 to λ2 = 1.2 at τ = 26. The time index is indicated by t and the number of
counts by yt. Generating such synthetic datasets provide intuition about the underlying models qualitative
behaviour.
A typical draw from this model is shown in Figure 19.17. The inferential goal is to compute the posterior
distribution of the change-point location and the intensities given the count data, p(λ1, λ2, τ|y1:T ). In
this problem the posterior is actually tractable [26] and could serve to assess the quality of the Gibbs
sampling approximation.
To implement Gibbs sampling we need to compute the distribution of each variable, conditioned on
the rest. These conditionals can be derived by writing the log of the full joint distribution and collecting
terms that depend only on the free variable.6 To derive the full conditional densities p(λ1| · ), p(λ2| · )
and p(τ| · ), we proceed by writing the full joint density. The log of the full joint density is given by:7
p(y1:T , λ1, λ2, τ) =
 τ
t=1
p(yt|λ1)
 
T
t=τ+1
p(yt|λ2)

p(λ1)p(λ2)p(τ),
log p(y1:T , λ1, λ2, τ) =
τ

t=1
( + yt log λ1 −λ1 −log (yt!))
+
T

t=τ+1
( + yt log λ2 −λ2 −log (yt!))
6since p(Xd|x−d = x−d) = p(Xd, x−d = x−d)/p(x−d = x−d) ∝p(Xd, x−d = x−d).
7Here, f (x) =+ g(x) means f (x) = g(x) + C where C is an irrelevant constant. This implies exp( f (x)) ∝exp(g(x)).

1.19.4 Markov Chain Monte Carlo
1091
+(a −1) log λ1 −bλ1 −log (a) + a log b
+(a −1) log λ2 −bλ2 −log (a) + a log b −log T ,
and collect terms that depend only on the free variable.
log p(λ1|τ, λ2, y1:T ) = +

a +
τ

t=1
yt −1

log λ1 −(τ + b)λ1 =+ log G(a +
τ

t=1
yt, τ + b).
Calculation of p(λ2| · ) is similar.
log p(τ|λ1, λ2, y1:T )
=
τ

t=1

+yt log λ1 −λ1 −log (yt!)

+
M

t=τ+1

+yt log λ2 −λ2 −log (yt!)

= +
 τ

t=1
yt

log λ1 −τλ1 +

T

t=τ+1
yt

log λ2 −(T −τ)λ2.
We have to evaluate the above expression for τ = 1, . . . , T . This Gibbs procedure is summarized in
Algorithm 4.
Algorithm 4. A Gibbs sampler for the change-point model
1: Initialize λ1
2, τ1
2: for n = 2, 3, . . . do
3:
λ(n)
1
∼p(λ1|λ(n−1)
2
, τ(n−1), y1:T ) = G(a + τ
t=1 yt, τ + b)
4:
λ(n)
2
∼p(λ2|λ(n)
1 , τ(n−1), y1:T ) = G(a + T
t=τ+1 yt, T −τ + b)
5:
τ(n) ∼p(τ|λ(n)
1 , λ(n)
2 , y1:T )
6: end for
We illustrate the algorithm on a coal mining disaster dataset [27], see Figure 19.18. The data consists
of the number of deadly coal-mining disasters in England per year over a time span of 112 years from
1851 to 1962. It is widely agreed in the statistical literature that a change in the intensity (the expected
value of the number of disasters) occurs around the year 1890, after new health and safety regulations
were introduced. In Figure 19.18 we show the samples obtained from the posterior of the changepoint
location. The posterior of the intensities λ1, λ2 are shown in Figure 19.19.

1092
CHAPTER 19 A Tutorial Introduction to Monte Carlo Methods
1860
1870
1880
1890
1900
1910
1920
1930
1940
1950
1960
0
2
4
6
1860
1870
1880
1890
1900
1910
1920
1930
1940
1950
1960
0
0.2
0.4
τ
yt
p (τ |x 1: T)
FIGURE 19.18
Estimation of change points on the coal mining disaster dataset. (Top) The number of deadly disasters yt
each year t. (Bottom) The posterior distribution of the changepoint location p(τ|y1:T ).
0
1
2
3
4
0
0.5
1
1.5
2
2.5
3
3.5
4
λ1
λ2
FIGURE 19.19
Samples drawn from the posterior intensities p(λ1, λ2|y1:T ) on the coal mining disaster dataset. This results
clearly indicate that after the health and safety regulations, the intensity has dropped from about λ1 = 3 to
λ2 = 1.
1.19.5 Sequential Monte Carlo
The MCMC techniques described in the previous section are widely used in many areas of applied
sciences and engineering. However, they may have some potential drawbacks in signal processing:
MCMC methods are by construction batch algorithms that require availability of all data records.
This can be prohibitive in some signal processing application such as tracking when the data arrives

1.19.5 Sequential Monte Carlo
1093
sequentially and decisions have to be taken in real time. One could in principle design online MCMC
algorithms that operate on the full data record observed so far. However, such batch algorithms take
increasingly more time. In such cases, it is desirable to have alternative methods which process data
sequentially and take a constant time per observation.
Sequential Monte Carlo (SMC) is an umbrella term for Monte Carlo techniques that process data in
an online fashion [9]. Popular techniques such as particle ﬁltering or sequential importance sampling
(SIS) are special SMC algorithms. SMC techniques are particularly natural for probabilistic signal
models where data has a natural order.8 A very large class of signal models can be described as state
space models, that have the following general form:
x0 ∼p(x0),
xt|xt−1 ∼p(xt|xt−1),
(19.8)
yt|xt ∼p(yt|xt).
(19.9)
Here, xt is the state and yt are the observations and t = 0, . . . , T is a discrete time index. Note that t
does not have to correspond to the wall clock, it only speciﬁes some natural ordering of the underlying
the data. The Eq. (19.8) and Eq. (19.9) are known as transition and observation models. State space
models, also named as hidden Markov models, are ubiquitous in signal processing and many popular
models, such as linear dynamical systems or autoregressive (AR) models, can be formulated as special
cases.
There are several inference problems of interest for state space models, where inference in this
context is the task of using a distribution to answer questions of interest. A common inference problem
is the prediction of an unseen future variable yT +1, which requires computing p(T +1|y1:T ) from a joint
distribution. Another one is the so called ﬁltering problem, estimation of the state xt given observations
so far y1:t. This is achieved via calculation of the posterior quantity p(xt|y1:t). One of the challenges in
statistical signal processing is to develop efﬁcient inference routines for answering such questions.
In this context, SMC techniques [10,28] have proved very useful. SMC methods are based on
importance sampling/resampling which we review in the following sections. We ﬁrst review importance
sampling and resampling techniques and subsequently introduce the SMC algorithm as a sequential
importance sampler.
1.19.5.1 Importance sampling (IS)
Consider a target distribution π(x) = φ(x)/Z where the non-negative function φ(x) is known, but
the overall normalisation constant Z is assumed to be computationally intractable. The main idea of
(IS) is to estimate expectations Eπ(ϕ(x)) by using weighted samples from a tractable IS distribution
q(x). Note that unlike other MC methods, our goal is not directly generating samples but estimating
expectations via weighted samples. However, we can also generate unweighted independent samples,
if we desire to so by a mechanism called resampling.
More speciﬁcally, we can write the normalization constant as
Z =

φ(x)dx =

 φ(x)
q(x) q(x)dx =

W(x)q(x)dx = Eq(W(x)),
8SMC techniques are not necessarily limited to such systems, though, see, e.g., [20].

1094
CHAPTER 19 A Tutorial Introduction to Monte Carlo Methods
where W(x) ≡φ(x)/q(x) is called weight function. Therefore we have
Eπ(ϕ(x)) = 1
Z

ϕ(x)φ(x)
q(x) q(x)dx = Eq(ϕ(x)W(x))
Eq(W(x))
.
A Monte Carlo estimate of Eπϕ(x) is then given by
ˆμN =
N
i=1 W (i)ϕ(x(i))/N
N
i=1 W (i)/N
,
where W (i) ≡W(x(i)) and the “particles” x(1), . . . , x(N) are sampled from q(x). Using normalised
weights w(i) ≡W (i)/ N
i′=1 W (i′) we can write the approximation as
ˆμN =
N

i=1
w(i)ϕ(x(i)).
In other words, instead of sampling from the target density p(x), we sample from a different tractable
distribution q(x) and correct by reweighing the samples accordingly. An example is given in
Figure 19.20.
1.19.5.1.1
Resampling
A practical issue is that, unless the IS sampling density q(x) is close to the target density π(x), the
normalised weights will typically have mass in only a single component. This can be partially addressed
using re-sampling as we now describe. Given a weighted particle system N
i=1 w(i)δ(x −x(i)), resam-
pling is the term for a set of methods for generating randomly an unweighted particle system of the
form 1
M
M
j=1 δ(x −˜x(i)) such that
E
⎛
⎝1
M
M

j=1
δ(x −˜x( j))
⎞
⎠=
N

i=1
w(i)δ(x −x(i)),
where the expectation is over the draws from the resampling algorithm. In resampling, typically the
total number of particles are unchanged, so we have N = M. Alternatively, one can view resampling as
a randomised pruning algorithm where we discard particles with low weight and increase the number of
particles with high weight. Unlike a deterministic pruning algorithm, the random nature of resampling
does not introduce a systematic bias and ensures an asymptotically consistent algorithm when the
number of samples N goes to inﬁnity. In Figure 19.21, we illustrate the result of resampling from
a weighted particle set. It is instructive to compare histograms obtained from weighted particle set
(Figure 19.20); the resampling stage introduces additional Monte Carlo variation but the histogram
is still a reasonable approximation to the underlying density. Two popular methods for resampling are
multinomial resampling and systematic resampling, which we will review in the sequel. For a discussion
and comparison of various resampling schemata, see [10,29].

1.19.5 Sequential Monte Carlo
1095
−10
−5
0
5
10
15
20
25
0
0.1
0.2
−10
−5
0
5
10
15
20
25
0
10
20
30
−10
−5
0
5
10
15
20
25
0
0.1
0.2
q(x)
W(x)
(x)
φ
FIGURE 19.20
Illustration of Importance Sampling. (Top): The solid curve denotes the unnormalised target density φ(x)
and the dashed curve the tractable IS density q(x). We assume that it is straightforward to generate samples
x(i) from q(x); these are plotted on the x axis. Middle: To account for the fact that the samples are from
q and not from the target π = φ/Z, we need to reweight the samples. In this example, the IS density q
generated far too many samples where π has low mass, and too few where π has high mass. The samples in
these regions are reweighted according to the weight function W (x). Bottom: Binning the weighted samples
from q, we obtain an approximation to π such that averages with respect to this approximation will be close
to averages with respect to π.
Multinomial resampling equivalent to the inversion method when the target is a discrete distribution,
as explained in Figure 19.3. Here, we view the weighted sample set x(i) as a discrete distribution with
categories i = 1, . . . , N where the probability of the ith category is given by the normalized weight
w(i). To sample M times independently from this target, we generate u( j) ∼U(0, 1) for j = 1, . . . , M
and we obtain an unweighted set of particles by evaluating the generalized inverse at each u( j), as
explained in Figure 19.3.
Systematic resampling is quite similar to multinomial resampling, only the generation of u( j) is not
entirely random but “systematic.” To generate M samples we only select u(1) uniformly random from
the interval [0, 1/M] and set u( j) = u(1) + ( j −1)/M. The u are located on a uniform grid with a
random initial shift. In Figure 19.22, we illustrate both methods.

1096
CHAPTER 19 A Tutorial Introduction to Monte Carlo Methods
−10
−5
0
5
10
15
20
25
0
0.1
0.2
−10
−5
0
5
10
15
20
25
0
10
20
30
−10
−5
0
5
10
15
20
25
0
1
2
−10
−5
0
5
10
15
20
25
0
0.1
0.2
˜x
q(x)
W(x)
(x)
φ
FIGURE 19.21
Importance Sampling with Resampling. We can generate from the weighted samples x generated by impor-
tance sampling unweighted samples ˜x by resampling. Some of the particles in the original set will be
replicated while others will be simply discarded.
From only a theoretical perspective, the actual details of a resampling schema are not important and
the estimates converge to their true values in the inﬁnite particle limit. One has to only ensure that the
expectation under the resampled particle set is on average equal to the expectation under the original
weighted particle set. However, with a ﬁnite number of particles, there may be notable differences.
It can be shown that systematic resampling achieves a lower variance while both methods have
the same expectation and no bias. Coupled with its ease of implementation, this makes systematic
resampling a popular choice in practice.
1.19.5.2 Sequential importance sampling
We now apply importance sampling to the state space model introduced earlier in Eq. (19.8) and Eq.
(19.9). “Plain” importance sampling is actually not very effective in high dimensional problems but
when executed sequentially using resampling techniques, it has become a very efﬁcient and effective
tool for inference. The resulting sequential IS methods are also known as particle ﬁlters [10]. The goal

1.19.5 Sequential Monte Carlo
1097
1
2
3
4
5
6
0
0.1
0.2
0.3
0.4
w(i) (Normalised weights)
1
2
3
4
5
6
0
0.5
1
u1
uk
1
2
3
4
5
6
0
0.5
1
1.5
2
i (Particle Index)
Ni (Counts)
1
2
3
4
5
6
0
0.1
0.2
0.3
0.4
w(i) (Normalised weights)
1
2
3
4
5
6
0
0.5
1
u1
uk
1
2
3
4
5
6
0
0.5
1
1.5
2
i (Particle Index)
Ni (Counts)
FIGURE 19.22
Multinomial Resampling (left column) versus Systematic Resampling (right column). (Top) True weights,
(Middle) Cummulative weight function, (Bottom) Bin counts after resampling. Note that the resulting his-
tograms can be thought of as approximations of the original discrete distribution.
is to draw samples from the posterior
p(x1:t|y1:t) = p(y1:t|x1:t)p(x1:t)



φ(x1:t)
/ p(y1:t)
  
Zt
,
where we assume that the normalisation term Zt is intractable. An importance sampling approach uses
at each time t an importance distribution qt(x1:t), from which we draw samples x(i)
1:t with corresponding
importance weights
W (i)
t
= φ(x(i)
1:t )/qt(x(i)
1:t ).
The key idea in SMC is the sequential construction of the IS distribution q and the recursive calculation
of the importance weights. Without loss of generality, we may write
qt(x1:t) = qt(xt|x1:t−1)qt(x1:t−1).
In particle ﬁltering, one chooses a IS proposal q that only updates the current xt and leaves previous
samples unaffected. This is achieved using
qt(x1:t) = qt(xt|x1:t−1)qt−1(x1:t−1).
As we are free to chose the IS proposal q fairly arbitrarily, we can also “construct” the proposal on the
ﬂy conditioned on the observations seen so far:
qt(x1:t|y1:t) = qt(xt|x1:t−1, y1:t)qt−1(x1:t−1|y1:t−1).

1098
CHAPTER 19 A Tutorial Introduction to Monte Carlo Methods
In the sequel, we won’t include y1:t in our notation but it should be understood that the proposal q can
be constructed at the observations so far.
Due to the sequential nature of the state space model and q, the weight function Wt(x1:t) admits a
recursive computation
Wt(x1:t) = φ(x1:t)
qt(x1:t) = p(yt|xt)p(xt|xt−1) t−1
τ=1 p(yτ|xτ)p(xτ|xτ−1)
qt(xt|x1:t−1) t−1
τ=1 qτ(xτ|x1:τ−1)
= p(yt|xt)p(xt|xt−1)
qt(xt|x1:t−1)



vt
Wt−1(x1:t−1),
where vt is called the incremental weight. Particle ﬁltering algorithms differ in their choices for
qt(xt|x1:t−1). The optimal choice (in terms of reducing the variance of weights) is the one step ﬁl-
tering distribution [30]
qt(xt|x1:t−1) = p(xt|xt−1, yt).
However, sampling from this ﬁltering density is often difﬁcult in practice, and simpler methods are
required. The popular bootstrap ﬁlter uses the model transition density as the proposal
qt(xt|x1:t−1) = p(xt|xt−1),
for which the incremental weight is vt = p(yt|xt). For the bootstrap ﬁlter, the IS distribution does not
make any use of the recent observation and therefore has the tendency to lose track of the high-mass
regions of the posterior. Indeed, it can be shown that the variance of the importance weights for the
bootstrap ﬁlter increases in an unbounded fashion [20,30] so that the state estimates are not reliable.
In practice, therefore, after a few time steps the particle set typically loses track of the exact posterior
mode. A crucial extra step to make the algorithm work is resampling which prunes branches with low
weights and keeps the particle set located in high probability regions. It can be shown that although the
particles become dependent due to resampling, the estimations are still consistent and converge to the
true values as the number of particles increases to inﬁnity. A generic particle ﬁlter algorithm is shown
in Algorithm 5.
Example 5 (Particle Filtering).
We will consider the following model of a sequence of Poisson
random variables yt with intensities exp(lt):
yt|lt ∼p(yt|lt) = PO(yt; exp(lt)) = exp(ytlt −exp(lt) −log (yt!)).
(19.10)
The latent log intensities lt are assumed to be changing slowly with t; this can be modeled by a random
drift
lt|lt−1 ∼p(lt|lt−1) = N(lt;lt−1, R) = exp

−1
2
(lt −lt−1)2
R
−1
2 log 2π R

.
(19.11)
This model can be considered as an alternative to the single change point model introduced in Section 4.
Rather than allowing for a single abrupt change, this model proposes slowly varying intensities where
the drift is controlled by the transition variance parameter R. If R is small, consecutive log intensities lt

1.19.5 Sequential Monte Carlo
1099
Algorithm 5. Particle Filter
for i = 1, . . . , N do
Compute the IS distribution: qt(xt|x(i)
1:t−1) possibly using the latest observation yt
Generate offsprings: ˆx(i)
t
∼qt(xt|x(i)
1:t−1)
Evaluate importance weights:
v(i)
t
= p(yt|ˆx(i)
t )p(ˆx(i)
t |x(i)
t−1)
qt(ˆx(i)
t |x(i)
1:t−1)
,
W (i)
t
= v(i)
t
W (i)
t−1
end for
ESS = 1/ 
i

˜w(i)
1:t
2
if ESS > Threshold then
No need to Resample
Extend particles: x(i)
1:t = (x(i)
1:t−1, ˆx(i)
t
), i = 1, . . . , N
else
Resample
Normalise importance weights:
˜Zt ←
j W ( j)
t
,
˜wt ←(W (1)
t
, . . . , Wt (N))/ ˜Zt
Generate Associations via a Resampling method
(a(1), . . . , a(N)) ←Resample( ˜wt)
Discard or Keep particles and Reset weights:
x(i)
0:t ←(xa(i)
0:t−1, ˆxa(i)
t
), W (i)
t
←˜Zt/N, i = 1, . . . , N
end if
and lt+1 will be close to each other. In the limit of R going to zero, the model degenerates into a constant
intensity model. In this example, we will assume that we know R.
To design a SMC algorithm, we need to decide upon a proposal mechanism and derive the expression
for the incremental importance weight
vt(lt|l1:t−1) = p(yt|lt)p(lt|lt−1)
qt(lt|l1:t−1)
.
For illustration purposes, we will consider here two different proposal mechanisms:
1. Transition density as the proposal (the Bootstrap ﬁlter):
Here, we choose as our proposal the models transition density
qt(lt|l1:t−1) = p(lt|lt−1).
This choice is the most widely used one for constructing proposals because of its simplicity,
ease of implementation and the intuitive interpretation as a “the survival of the ﬁttest” algorithm.
The resulting algorithm is known in the SMC literature as the bootstrap ﬁlter and in computer

1100
CHAPTER 19 A Tutorial Introduction to Monte Carlo Methods
vision as the condensation algorithm. For the bootstrap ﬁlter, the incremental weight expression
becomes
vt(lt|l1:t−1) = p(yt|lt)p(lt|lt−1)
p(lt|lt−1)
= p(yt|lt)
= exp(ytlt −exp(lt) −log (yt!)),
2. The observation likelihood as the proposal
Here, we choose a density that is proportional to the likelihood term
qt(lt|l1:t−1) = qt(lt) = p(yt|lt)/ct,
where ct =

p(yt|lt)dlt. Note that this proposal can only be used when the normalizer ct is not
inﬁnite hence a density exists.9 This option is perhaps less intuitive to understand but could be a
lot more efﬁcient than the bootstrap in problems where the transition model is very diffuse but
the observations are very informative. Using a bootstrap ﬁlter may be inefﬁcient in this case, as
most of the proposed particles will fall into low probability regions.
vt(lt|l1:t−1) = p(yt|lt)p(lt|lt−1)
p(yt|lt)/ct
= ct p(lt|lt−1).
In practice, unless we need to explicitly evaluate the marginal likelihood p(y1:t), we do not need
to evaluate normalizer ct as it will cancel out when calculating the normalized weights.
To ﬁnd this proposal density, we make the following observation: the Poisson density, when
viewed as a function of the intensity is proportional to a Gamma density10
PO(y; λ) ∝exp(y log λ −λ)
∝exp((y + 1 −1) log λ −λ −log (y + 1)) = G(λ; y + 1, 1).
So for a given Poisson observation, we can easily generate λ by a unit gamma random number
generator with shape parameter y + 1. To generate the log intensities l, we sample λ and set
l = log (λ). The density of l = log λ when λ is gamma distributed is found via the transformation
method
l(λ) = log (λ),
λ(l) = exp(l),
|J| = ∂λ(l)/∂l = exp(l),
p(l) = G(λ(l); a, b)exp(l).
So the density of our proposal is
qt(lt) ∝exp((yt + 1)lt −exp(lt) −log (yt + 1)).
9Here, ct is not to be confused with the actual normalizing constant of the model which is the marginal likelihood Zt =
p(yt|y1:t−1).
10This is due to the so called conjugacy property.

1.19.6 Advanced Monte Carlo Methods
1101
The weight expression is
vt(lt|l1:t−1) =
exp(ytlt −exp(lt) −log (yt))exp

−1
2
(lt−lt−1)2
R

exp((yt + 1)lt −exp(lt) −log (yt + 1))
∝exp

−1
2
(lt −lt−1)2
R
−lt

.
Here, we have simply ignored the terms that do not depend on lt as these do not contribute to the
normalized importance weight.
As mentioned earlier, in practice the transition model is taken often as the proposal density. It should
be stressed that, the choice of an efﬁcient proposal has a profound effect on the performance and the
transition model need not to be always the best choice. As the above example illustrates, there are
typically always alternative proposals and in the context of an application, at least a few alternatives
should be considered before opting to the transition density. As a guideline, one should look for a
proposal that is close to the ﬁltering density but that does not require extensive computation to sample.
If a proposal is too costly to sample from, one may consider using a simpler but poorer proposal with
more samples for a given computational cost.
1.19.6 Advanced Monte Carlo methods
We have reviewed some of the basic Monte Carlo techniques in previous sections. However, there
are many problems that need techniques beyond standard strategies. In this section, we will review
such a technique: reversible jump MCMC (RJ-MCMC) [11,31]. This technique is an extension of the
Metropolis-Hastings method to more general state spaces [23]. It is mostly used in settings where we
wish to perform model selection, such as inferring a model where the order is unknown.
1.19.6.1 Reversible jump MCMC
The reversible jump MCMC is a Metropolis-Hastings method for sampling from Markov chains in state
spaces of varying dimensionality. Such state spaces arise naturally in model selection problems where
we have several candidate models {Mk} with model index k, formally denoted as k ∈K for some
ﬁnite set K. With each model Mk there is an associated parameter vector θk of dimensionality Nk, i.e.,
θk ∈RNk. The inferential goal is to sample from the joint density π(k, θk) of model index k and the
parameters θk where
π(θk, k) = 1
Zk
φk(θk) ˜pk,
(19.12)
˜pk ≡
pk

K ′∈K pk′ .
(19.13)
Here, for all models k ∈K, Zk =

dθkφk(θk) are unknown normalizing constants and pk are prior
weights, usually taken as ﬂat with pk = 1. These quantities have direct counterparts in a Bayesian

1102
CHAPTER 19 A Tutorial Introduction to Monte Carlo Methods
model selection setting: we have a probability model that couples observations x with parameters as
p(x, θk, k) = p(x, θk|k)p(k). In this context, the target density is the posterior
π(θk, k) = p(θk, k|x) = p(θk|k, x)p(k|x)
and the joint distribution of parameters and data for each model Mk is φk(θk) = p(x, θk|k) and
Zk =

dθkφk(θk) = p(x|k). To ﬁnd the model order after observing the data x we would need the
posterior marginal
p(k|x) =
p(k, x)

k′∈K p(k, x) =
˜pk Zk

κ∈K ˜pκ Zκ
=
pk Zk

κ∈K pκ Zκ
,
where the equalities follow by noting that p(k, x) =

p(θk, k, x)dθk = ˜pk Zk and p(x) = 
k ˜pk Zk.
With these results, the posterior can be written as
π(θk, k) = 1
Zk
φk(θk)
pk Zk

κ∈K pκ Zκ
= φk(θk)
pk

κ∈K pκ Zκ
.
One possible approach for calculating the posterior of models order p(k|x) would require estimation
of individual normalizing constants Zk. The RJ-MCMC approach circumvents this need by setting up
a chain to sample from (k, θk) pairs. The difﬁculty is that now the target density π is deﬁned on a
nonstandard state space E where
E =

k∈KEk,
Ek ≡{{k} × RNk},
and we need to set up a so called transdimensional Markov chain that can jump across spaces of different
dimensions Ek and Ek′ as (k, θk) →(k′, θk′). This chain is “nonstandard” in the sense that its dimension
varies over time but is otherwise completely natural for the class of model selection problems.
There is a technical caveat: The “ordinary” Metropolis-Hastings algorithm requires that we design
a proposal
q(k′, θk′|k, θk) = q(θk′|k′, k, θk)q(k′|k, θk)
and evaluate the acceptance probability as
α((k, θk) →(k′, θk′)) = min

1, q(k, θk|k′, θk′)π(k′, θk′)
q(k′, θk′|k, θk)π(k, θk)

.
We need to construct the proposals carefully, as as a proper lower dimensional space embedded into a
higher dimensional space may have measure zero with respect to the probability measure deﬁned on
the larger space.11 The remedy comes from the transformation method introduced earlier. By introduc-
ing auxiliary variables, we design the proposal as a “reversible” transformation between both spaces.
11The probability of jumping to an isolated point (a proper low dimensional space) under a proposal with an absolutely
continuous density is zero. As an example, think of the probability of hitting an interval ( −ϵ, ϵ) when ϵ →0.

1.19.6 Advanced Monte Carlo Methods
1103
Suppose we wish to jump from a lower dimensional space Ek to a higher dimensional space Ek′. We
deﬁne ¯Nk such that Nk′ = Nk + ¯Nk and draw the ¯Nk dimensional vector uk from a proposal qk(uk) and
match the dimensions of both spaces via a transformation deﬁned as
θk′ = gk→k′(θk, uk).
The mapping gk→k′ is fairly general but must be invertible such that g−1
k→k′ = gk′→k exists. As an
alternative notation for g or its inverse g−1, we omit the letter g and write θk′(θk, uk)and (θk(θk′), uk(θk′))
respectively. More naturally, it is also possible to match dimensions on a higher dimensional space where
Nk′ + ¯Nk′ = Nk + ¯Nk where (θk′, uk′) = g(θk, uk) but we won’t discuss this option further here.
In order to design a MH algorithm, we will construct our proposal in two stages as in
q(k′, θk′|k, θk) = q(k′|k, θk)q(θk′|k′, k, θk).
The ﬁrst factor in this proposal is the probability of choosing a jump from k to k′ when the chain is in
θk. Often, this term is taken to be independent of θk, so q(k′|k) = q(k′|k, θk). We will denote the jump
probabilities q(k′|k) as qk→k′. The second factor, which we will denote as fk′(θk′) ≡q(θk′|k′, k, θk),
will be deﬁned via the variable transformation gk→k′. Noting that θk′ = gk→k′(θk, uk), for some density
fk(θk), the density of θk′ can be written as is
fk′(θk′) = fk(θk(θk′))qk(uk(θk′))
					
∂g−1
k→k′(θk′)
∂θk′
					
= fk(θk(θk′))qk(uk(θk′))
				
∂gk→k′(θk, uk)
∂(θk, uk)
				
−1
.
To simplify the notation, in the sequel we will denote the Jacobian terms as
Jk→k′ ≡∂gk→k′(θk, uk)
∂(θk, uk)
Jk′→k ≡∂g−1
k→k′(θk′)
∂θk′
= J −1
k→k′.
Now, interpret fk(θk) as a reverse jump proposal q(θk|k, k′, θk′). The ratio of the proposals to compute
the MH-acceptance probability can be written for this case as
q(k, θk|k′, θk′)
q(k′, θk′|k, θk) =
fk(θk)qk′→k
fk′(θk′)qk→k′ =
fk(θk)qk′→k
fk(θk(θk′))qk(uk(θk′))qk→k′
				
∂gk→k′(θk, uk)
∂(θk, uk)
				
=
qk′→k
qk(uk(θk′))qk→k′ |Jk→k′| .
The ratio of the targets is
π(k′, θk′)
π(k, θk) =
φk′(θk′)
pk′

κ∈K pκ Zκ
φk(θk)
pk

κ∈K pκ Zκ
= φk′(θk′)pk′
φk(θk)pk
.

1104
CHAPTER 19 A Tutorial Introduction to Monte Carlo Methods
Consequently, we arrive at the rather complicated looking formula for the RJ-MCMC acceptance
probability
α((k, θk) →(k′, θk′)) = min

1, φk′(θk′)pk′
φk(θk)pk
qk′→k
qk(uk)qk→k′ |Jk→k′|

(19.14)
≡min{1,rk→k′}
(19.15)
≡αk→k′.
(19.16)
The rk→k′ argument min function is known as the MH-ratio. The acceptance probability of the jump in
the reverse direction is
αk′→k = min

1, φk(θk)pk
φk′(θk′)pk′
qk(uk)qk→k′
qk′→k
|Jk′→k|

= min{1,rk′→k} = min

1,r−1
k→k′

Note that for a pair of reversible jumps (k →k′) and (k′ →k), the corresponding MH-ratios are simply
the inverses of each other rk′→k = r−1
k→k′.
1.19.6.2 RJ-MCMC algorithm
The RJ-MCMC algorithm is actually a special Metropolis Hastings method with a special proposal
mechanism for transdimensional moves. To construct the algorithm, for each pair k, k′ ∈K we need to
decide on the jump probabilities qk→k′ and qk′→k. Note that these are just parameters of the sampling
algorithm and not the model, but a careful choice is important for good mixing. In practice, often only
jumps between “adjacent” k and k′ are used as it is relatively simpler to design proposals with reasonable
acceptanceprobability.Here,apopularchoiceisonlyallowingmoveswithk′ = k+1, k′ = k−1, k′ = k,
where these are called birth, death and update moves.
If a pair of moves have nonzero probability,qk→k′qk′→k > 0, we have to have to design the following:
•
For jumps to a larger space Nk′ > Nk we design a suitable invertible transformation gk→k′ and a
proposals for qk(uk) to sample a ¯Nk dimensional random vector to match the dimensions such that
Nk′ = Nk + ¯Nk. Note that our choice completely ﬁxes the moves in the reverse direction as well.
•
Calculate the Jacobian determinants |Jk→k′| = |Jk′→k|−1.
•
Calculate the acceptance probabilities. Remember that if
αk′→k = min {1,rk′→k} ⇔αk→k′ = min {1,rk→k′} = min

1,
1
rk′→k

A summary of the RJ-MCMC is shown as Algorithm 6
Example 6 (Reversible Jump - MCMC).
As a illustrative toy example, we deﬁne a target density on
the union of two spaces E = E1 ∪E2 where
E1 = {{1} × r}
E2 = {{2} × (x, y)}

1.19.6 Advanced Monte Carlo Methods
1105
Algorithm 6. Reversible Jump Markov Chain Monte Carlo
1:Initialise

k(1), θ(1)
(1)

arbitrarily
2: for τ = 2, 3, . . . do
3:
Let (k, θk) =

k(τ−1), θ(τ−1)
k(τ−1)

4:
Propose a move type: knew ∼qk→k′
5:
if k = knew, a ordinary move in the same space then
6:
θnew ∼qk

θk|θ(τ−1)
k

7:
end if
8:
if k < knew, a transdimensional move to a larger space then
9:
Propose u ∼qk(uk)
10:
θnew ←gk→knew(θk, u)
11:
Compute the acceptance probability
α

(k, θk) →(knew, θnew)

= min

1, φknew(θknew)pknew
φk(θk)pk
qknew→k
qk(u)qk→knew |Jk→knew|

12:
end if
13:
if k > knew, a transdimensional move to a smaller space then
14:
(θnew, u) ←gk→knew(θk)
15:
Compute the acceptance probability
α

(k, θk) →(knew, θnew)

= min

1, φknew(θknew)pknew
φk(θk)pk
qknew(u)qknew→k
qk→knew
|Jk→knew|

16:
end if
17:
Sample from uniform distribution: a ∼U(0, 1)
18:
if a < α

(k, θk) →(knew, θnew)

then
19:
Accept candidate:

k(τ), θ(τ)
(τ)

←(knew, θnew)
20:
else
21:
Reject candidate and stay put:

k((τ), θ(τ)
k(τ)

←

k(τ−1), θ(τ−1)
k(τ−1)

22:
end if
23: end for
with parameters corresponding to each space as
θ1 ≡r
θ2 ≡(x, y)
We choose, for k = 1, a target proportional to a truncated exponential distribution. For k = 2, our target
is a two dimensional Gaussian density truncated on an elliptical region
φ1(r) = e−λr[0 ≤r ≤1]
φ2(x, y) = exp

−(x −μx)2 + (y −μy)2
2σ 2
  x
a
2
+
 y
b
2
≤1
!

1106
CHAPTER 19 A Tutorial Introduction to Monte Carlo Methods
1
2
3
4
5
0
1
2
3
4
5
t
(a)
1
2
3
4
5
0
1
2
3
4
5
t
(b)
1
2
3
4
5
0
1
2
3
4
5
t
(c)
1
2
3
4
5
0
1
2
3
4
5
exp( λt  )
exp( λt  )
exp( λt  )
exp( λt  )
exp( λt  )
t
(d)
1
2
3
4
5
0
1
2
3
4
5
t
(e)
FIGURE 19.23
Illustration of the Bootstrap particle ﬁlter for the model in Eq. (19.10) and Eq. (19.11) using N = 3 particles.
The observations yt are shown as black crosses. The squares show the true state of the process exp(λt) used
for generating the observations yt. Remember that ⟨yt⟩= exp(λt) due to the Poisson assumption so it is
natural to plot the yt and exp(λt) on the same plot. At each time t, the particles are shown as individual
trajectories of λ1:t with the circle size proportional to the normalized weight at time t. The edges denote the
ancestral links. While the term “particle” suggests that each particle is only a point in λt, it is actually more
natural to view a particle as an entire trajectory of λ1:t. At each time t, each particle selects a new position
lt+1 and the new weight Wt+1(l1:t+1) is computed recursively. If the effective sample size drops below a
chosen threshold, we compute the normalized weights ˜w (1:N) and and select each particle via resampling
(See at time t = 4) and reset the weights to 1/N.
the normalization constants Z1 =
 1
0 e−λrdr and Z2 =
 
φ2(x, y)dxdy are assumed to be unknown.
As shown earlier, these cancel out in the acceptance probability. The target density is
π(k, θk) =
1
p1Z1 + p2Z2
([k = 1]φ1(r)p1 + [k = 2]φ2(x, y)p2)
(19.17)

1.19.6 Advanced Monte Carlo Methods
1107
0
0.5
1
0
0.5
1
 
 
−3
−2
−1
0
1
2
3
−1
−0.5
0
0.5
1
0
0.2
0.4
0.6
0.8
E1
E2
φ
1
φ 2
r
x
y
0
0.5
1
0
500
1000
 
 
−3
−2
−1
0
1
2
3
−1
−0.5
0
0.5
1
0
100
200
300
1
r
x
y
φ
FIGURE 19.24
(Top) The target density π ∝[k = 1]φ1(r)p1 +[k = 2]φ2(x, y)p2, with parameters λ = 1, a = 3, b = 1, μx =
1.5, μy = 1, σ 2 = 4, p1 = p2 = 1. (Bottom) Histograms obtained from samples of the RJ-MCMC algorithm.
This target density is illustrated on Figures 19.23,19.24,19.25.
We will derive now a RJ-MCMC method to sample from the target density deﬁned in Eq. (19.17).
1.19.6.2.1
Acceptance probabilities
To design a RJ-MCMC algorithm, we should deﬁne an “ordinary” MCMC algorithm to sample within
each space Ek for all k ∈K and transdimensional moves for each pair k, k′ such that k ̸= k′ using the
general formula for the RJ-MCMC acceptance probability α as given in 19.14.

1108
CHAPTER 19 A Tutorial Introduction to Monte Carlo Methods
1
2
0
0.2
0.4
0.6
0.8
1
k
Zk
Z1+ Z2
FIGURE 19.25
Histogram estimate of the marginal π(k). By simply counting and normalizing the number of steps the
chain spends in each space, we can estimate the marginal π(k), with probabilities p1Z1/(p1Z1 + p2Z2) and
p2Z2/(p1Z1 + p2Z2). Such quantities are useful for model comparison.
E1 →E1.
We use a Metropolis algorithm with an independent uniform proposal such that
r′∼U(0, 1),
giving the acceptance probability as
α1→1 = min

1, φ1(r′)
φ1(r)

,
(19.18)
E2 →E2
We choose a symmetric random walk proposal
 x′
y′

=
 x
y

+ ϵ,
(19.19)
where ϵ ∼N(0, vI). Since the proposal is symmetric, the Metropolis acceptance probability is
α2→2 = min

1, φ2(x′, y′)
φ2(x, y)

(19.20)
E1 →E2
In order to make a transition from the one dimensional space E1 to the two dimensional space E2, we
deﬁne the auxiliary variable u1 such that
u1 ∼q1(u1) = U(0, 2π) = 1
2π
(19.21)
and deﬁne the following one-to-one transformation (x, y) = g(r, u) as
x = ar cos (u)
y = br sin (u).

1.19.6 Advanced Monte Carlo Methods
1109
0
0.5
1
0
0.5
1
0
0
0.05
0.1
0.15
0.2
 
 
−3
−2
−1
0
1
2
3
−1
−0.5
0
0.5
1
0
1
2
3
2π
q(u )
u
φ
1
r
x
y
FIGURE 19.26
The mapping (x, y) = g(r, u). For trans-dimensional jumps between the spaces E1 (top, right) and E2
(bottom), to match the dimensions, we draw an angle uniformly u ∼q(u) = U(0, 2π) and construct a one-to-
one mapping g. A proposal f1 on E1 translates to a proposal f2 on E2 as f2(x, y) = f1(r(x, y))q(u(x, y))|J2→1|.
The acceptance probability does not depend on f1 or f2 as these cancel out; only the term q(u(x, y))|J2→1|
remains.
Each r is mapped uniformly into a point on the ellipse x2/a + y2/b = r2. This mapping is depicted
in Figures 19.26,19.27. To compute the acceptance probability, we need the Jacobian. The partial
derivatives of the transformation are obtained as follows
∂x
∂r = a cos (u)
∂x
∂u = −ar sin (u)
(19.22)
∂y
∂r = b sin (u)
∂y
∂u = br cos (u)
and the Jacobian term can then be determined as
|J1→2| =
				
∂x
∂r
∂y
∂u −∂x
∂u
∂y
∂r
				
= |a cos (u)br cos (u) + ar sin (u)b sin (u)| =
			abr( cos2 (u) + sin2 (u))
			 = abr
Finally, substituting the results the acceptance probability α1→2 is obtained as
α1→2 = min

1, φ2(x, y)p2
φ1(r)p1
q2→1
q1→2
1
(1/2π)|J1→2|

(19.23)

1110
CHAPTER 19 A Tutorial Introduction to Monte Carlo Methods
0
0.5
1
0
0.5
1
−3
−2
−1
0
1
2
3
−1
−0.5
0
0.5
1
0
1
2
3
E1
E2
φ
1
φ 2
r
x
y
FIGURE 19.27
Illustration of the RJ-MCMC algorithm on the toy problem. The dotted lines corresponds to accepted moves
within the same space, and arrows correspond to transdimensional moves. The algorithm proceeds as follows:
Suppose at step τ, the chain is in space Ek(τ). We ﬁrst propose the next index k′ with probability qk(τ)→k′.
If k′ = k(τ), we use an ordinary MH move. Otherwise, we draw a new u ∼p(u) and propose a new point
θk′(θ(τ)
k(τ), u) ∈Ek′ and accept it with probability αk(τ)→k. If the move is accepted we set (k(τ+1), θ(τ+1)
k(τ+1)) ←
(k′, θk′). Otherwise, the chain stays at the same point point is equal to the old point (k(τ+1), θ(τ+1)
k(τ+1)) ←
(k(τ), θ(τ)
k(τ)).
E2 →E1
We ﬁnd the inverse transformation g−1 ,
r =
"
x2
a2 + y2
b2
u = arccos

xb
#
x2b2 + y2a2

The Jacobian J2→1 is obtained by calculating the partial derivatives
∂r
∂x =
xb
a
#
x2b2 + y2a2
∂r
∂y =
ya
b
#
x2b2 + y2a2
∂u
∂x = −
yab
x2b2 + y2a2
y
|y|
∂u
∂y =
xab
x2b2 + y2a2
y
|y|

1.19.7 Open Issues and Problems
1111
and the determinant is
|J2→1| =
1
#
x2b2 + y2a2
Note that we could have avoided this calculation by directly using the Jacobian of the inverse as
|J2→1| = |J1→2|−1 = 1/(abr), giving the same result. The acceptance probability α2→1 is found as
follows:
α2→1 = min

1,
φ1(r)p1
φ2(x, y)p2
q1→2(1/2π)
q2→1
|J2→1|

(19.24)
1.19.7 Open issues and problems
In this tutorial, we have sketched in a self contained manner the basic techniques and the principles of
Monte Carlo computation, along with several examples. Monte Carlo computation is a very broad topic
and with the available computing power, researchers are tackling increasingly more complex models
and problems. The need for extracting structure from large datasets forces the researchers to devise
increasingly more complicated models that ask for more efﬁcient inference methodologies. In some
research communities, most notably machine learning, the MC methods are considered not very scalable
to very large data sets due to their heavy computational requirements. It is an open issue in research for
developing MC methods for such datasets. The research frontier, at the time of this writing, is swiftly
moving. Researchers deal with techniques such as adaptive methods that learn suitable proposals[32],
combining sequential Mote Carlo with MCMC [33], likelihood free inference [6] or exploiting parallel
computation [34] for scalability.
1.19.8 Further reading
Toinvestigatetopicsmoredeeper,wesuggestafewkeyreferences.Forbasicprobabilitytheory,Grimmet
and Stirzaker [4] is a very good reference. More rigorous material, needed for a deeper understanding
of advanced subjects is covered in Rosenthal [12] and for the theory of Markov chains Norris is a
key reference [7]. A very gentle introduction to basic Monte Carlo methods from a machine learning
perspective is in MacKay’s book [5]. A good tutorial introduction to Markov chain Monte Carlo methods
can be found in [6,18]. There are also excellent tutorials on sequential Monte Carlo methods, most
notably by Doucet et. al. [10,30] or Liu et. al. [8,20].
Glossary
acceptance probability
17
aperiodic (chain)
14
Box-Müller method
14
CDF Cummulative Density Function
8
central limit theorem
4

1112
CHAPTER 19 A Tutorial Introduction to Monte Carlo Methods
CLT Central Limit Theorem
2
detailed balance
19
deterministic sca
25
ergodic chain
14
full conditional density
23
generalised inverse
8
Gibbs sampler
23
inversion method
8
irreduciable (chain)
14
IS importance sampling
29
Jacobian
10
law of large numbers
4
LCG Linear Congruential Generator
7
Linear Congruential Generator a method for generating pseudorandom numbers
based on a recursive formula
47
LLN Law of Large Numbers
2
Markov Chain Monte Carlo a Monte Carlo method that depends on simulating
from a ergodic Markov chain to generate estimates of a stationary distribution
14
MCMC Markov Chain Monte Carlo
1
MersenneTwisteraLinearCongruentialGeneratorwithaprovablyverylongperiod
7
MH Metropolis-Hastings
17
Monte Carlo (method) a family of numerical techniques for computing approximate
estimates via random sampling
1
Multinomial resampling
31
particle ﬁltering
27
pseudo-random numbers
7
random scan
25
rejection probability
18
resampling
29,30
RJ-MCMC reversible jump MCMC
36,38,40
RJ-MCMC acceptance probability
40
SIS sequential importance sampling
27
SMC sequential Monte Carlo
27,29
spectral gap
20
Systematic resampling
31
total variation norm
16
transformation method
10
transition kernel
16
transition matrix
15

References
1113
References
[1] N. Metropolis, S. Ulam, The Monte Carlo method, Journal of the Am. Statist. Assoc. 44 (247) (1949)
335–341.
[2] N. Metropolis, A. Rosenbluth, M. Rosenbluth, A. Teller, E. Teller, Equations of state calculations by fast
computing machines, J. Chem. Phys. 21 (1953) 1087–1091.
[3] C. Geyer, Handbook of Markov Chain Monte Carlo, chapter Introduction to Markov Chain Monte Carlo
(Chapman & Hall/CRC Handbooks of Modern Statistical Methods) 2011.
[4] G.R. Grimmett, D.R. Stirzaker, Probability and Random Processes, Oxford University Press, 2001.
[5] D.J.C. MacKay, Information Theory, Inference and Learning Algorithms, Cambridge University Press, 2003.
[6] S. Brooks, A. Gelman, G. Jones, X. Meng (eds.), Handbook of Markov Chain Monte Carlo (Chapman &
Hall/CRC Handbooks of Modern Statistical Methods) 2011.
[7] J.R. Norris, Markov Chains, Cambridge University Press, 1997.
[8] J.S. Liu, R. Chen, Sequential Monte Carlo methods for dynamic systems, J. Am. Statist. Assoc. 93 (1998)
1032–1044.
[9] A. Doucet, N. de Freitas, N.J. Gordon (eds.), Sequential Monte Carlo Methods in Practice, Springer Verlag,
2001.
[10] A. Doucet, A.M. Johansen, Handbook of Nonlinear Filtering, chapter A Tutorial on Particle Filtering and
Smoothing: Fifteen years Later, Oxford University Press, 2010.
[11] P.J Green, Reversible jump Markov chain Monte Carlo computation and Bayesian model determination,
Biometrika 82 (4) (1995) 711-732.
[12] Jeffrey S. Rosenthal, A ﬁrst look at rigorous probability theory, World Scientiﬁc, second ed., 2006.
[13] Charles M. Grinstead, Laurie J. Snell, American Mathematical Society, fourth ed., July 2006.
[14] The marsaglia random number cd-rom with the diehard battery of tests of randomness.
[15] M. Matsumoto, T. Nishimura, Mersenne twister: a 623-dimensionally equidistributed uniform pseudorandom
number generator, ACM Trans. Model Comput. Simul. 8 (1) (1998) 3–30.
[16] Luc Devroye, Non-Uniform Random Variate Generation, 1986.
[17] Luke Tierney, Markov chains for exploring posterior distributions, Ann. Statist. 22 (1994) 1701–1762.
[18] W.R. Gilks, S. Richardson, D.J Spiegelhalter (Eds.), Markov Chain Monte Carlo in Practice, CRC Press,
London, 1996.
[19] G.O. Roberts, J.S. Rosenthal, Markov Chain Monte Carlo: Some practical implications of theoretical results,
Can. J. Statist. 26 (1998) 5–31.
[20] J.S. Liu, Monte Carlo Strategies in Scientiﬁc Computing, Springer, 2004.
[21] W.K. Hastings, Monte Carlo sampling methods using Markov chains and their applications, Biometrika 57
(1970) 97–109.
[22] O. Cappé, E. Moulines, T. Rydén, Inference in Hidden Markov Models, Springer-Verlag, New York, 2005.
[23] Luke Tierney, A note on metropolis-hastings kernels for general state spaces, Ann. Appl. Probab. 8 (1998)
1–9.
[24] S. Geman, D. Geman. Stochastic Relaxation, Gibbs Distributions and the Bayesian Restoration of Images,
in: M.A. Fischler, O. Firschein (Eds.), Readings in Computer Vision: Issues, Problems, Principles, and
Paradigms, Kaufmann, Los Altos, CA, 1987, pp. 564–584.
[25] A.M. Johansen, L. Evers, N. Whiteley, Monte Carlo Methods, Online Resource, 2008 (Lecture Notes).
[26] P. Fearnhead, Exact and Efﬁcient Bayesian inference for multiple changepoint problems, Technical report,
Dept. Math. Stat., Lancaster University, 2003.
[27] Jarrett, A note on the intervals between coal mining disasters, Biometrika 66 (1979) 191–193.

1114
CHAPTER 19 A Tutorial Introduction to Monte Carlo Methods
[28] A. Doucet, N. de Freitas, N.J. Gordon (eds.), Sequential Monte Carlo Methods in Practice, Springer-Verlag,
New York, 2001.
[29] O.Cappe,R.Douc,E.Moulines,Comparisonofresamplingschemesforparticleﬁltering,in:4thInternational
Symposium on Image and Signal Processing and Analysis (ISPA), Zagrep, Croatia, September 2005.
[30] A. Doucet, S. Godsill, C. Andrieu, On sequential Monte Carlo sampling methods for Bayesian ﬁltering,
Statist. Comput. 10 (3) (2000) 197-208.
[31] Y. Fan, S. Sisson, Handbook of Markov Chain Monte Carlo, Chapter Reversible jump Markov chain Monte
Carlo (Chapman & Hall/CRC Handbooks of Modern Statistical Methods) 2011.
[32] C. Andrieu, J. Thoms. A tutorial on adaptive mcmc, Statist. Comput. 18 (4) (2008) 343–373.
[33] Pierre Del Moral, Arnaud Doucet, Ajay Jasra. Sequential Monte Carlo samplers, J. Roy. Statist. Soc. Ser. B
Stat. Methodol. 68 (3) (2006) 411–436.
[34] Anthony Lee, Christopher Yau, Michael B Giles, Arnaud Doucet, Christopher C. Holmes, On the utility
of graphics cards to perform massively parallel simulation of advanced Monte Carlo methods, May 2009,
pp. 4–6.

20
CHAPTER
Clustering
Dao Lam and Donald C. Wunsch
Missouri University of Science and Technology, Applied Computational Intelligence Lab,
Department of Electrical and Computer Engineering, Rolla, MO, USA
1.20.1 Introduction
Every day, society generates large amounts of data, which can be subjected to analysis and management.
One vital way to handle these data is to classify or group them into a set of categories or clusters. In order
to learn a new object or understand a new phenomenon, people seek features to describe it; additionally,
they compare it with other known objects or phenomena based on similarity or dissimilarity, generalized
as proximity, according to some certain standards or rules. In unsupervised classiﬁcation, also called
clustering or exploratory data analysis, no labeled data are available. The goal of clustering is to separate
a ﬁnite, unlabeled data set into a ﬁnite, discrete set of “natural,” hidden data structures, rather than to
accurately characterize unobserved samples generated from the same probability distribution [1,2]. As
noted by Backer and Jain, “in cluster analysis a group of objects is split up into a number of more or less
homogeneous subgroups on the basis of an often subjectively chosen measure of similarity (i.e., chosen
subjectively based on its ability to create “intersecting” clusters), such that the similarity between objects
within a subgroup is larger than the similarity between objects belonging to different subgroups” [3].
Clustering algorithms partition data into clusters (groups, subsets, or categories), but these clusters
have no universally agreed-upon deﬁnition [4]. A cluster can be “a set of entities which are alike, and
entities from different clusters are not alike” or “an aggregate of points in the test space such that the
distance between any two points in the cluster is less than the distance between any point in the cluster
and any point not in it” [4]. A cluster also can be deﬁned as a continuous region of a space containing
a relatively high density of points, separated from other such regions by regions containing a relatively
low density of points.
Furthermore, a cluster can be deﬁned based on the method used to solve the clustering problem.
In hierarchical clustering, a cluster is an entity of connectivity. In partition-based clustering, each
cluster is represented by a single prototype vector. In model-based clustering, each cluster is modeled
by a statistical distribution. In density estimation clustering, each cluster is deﬁned as a connected
dense region. Subspace clustering views clusters as a subspace. If graph theory techniques are used for
clustering, each cluster typically is called a clique, a subset of nodes fully connected among themselves.
From a membership point of view, clustering can be classiﬁed as hard or fuzzy. In hard clustering, each
object belongs to one and only one cluster, while in fuzzy clustering, each object has some degree of
membership in each cluster.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00020-6
© 2014 Elsevier Ltd. All rights reserved.
1115

1116
CHAPTER 20 Clustering
Feature selection or
extraction
Knowledge
Data samples
Clustering algorithm
design or selection
Cluster
validation
Result
interpretation
Clusters
FIGURE 20.1
Clustering procedure (From [5]). Many if not most papers consider mainly the algorithm design or selection,
but all the steps shown are important.
In describing a cluster, most researchers consider internal homogeneity and external separation
[6–8], i.e., patterns in the same cluster should be similar to each other, while patterns in different
clusters should not. Similarities and dissimilarities both should have the potential to be examined in a
clear and meaningful way.
Figure 20.1 depicts the four basic steps of the cluster analysis procedure.
Clustering has been applied in a wide variety of ﬁelds, ranging from engineering (machine learning,
artiﬁcial intelligence, pattern recognition, mechanical engineering, electrical engineering), computer
sciences (web mining, spatial database analysis, textual document collection, image segmentation) and
life and medical sciences (genetics, biology, microbiology, paleontology, psychiatry, clinic, pathology),
to earth sciences (geography, geology, remote sensing), social sciences (sociology, psychology, arche-
ology, education), and economics (marketing, business) [4,9]. Accordingly, clustering also is known as
numerical taxonomy, learning without a teacher (or unsupervised learning), typological analysis and
partitioning. While the diversity of these names reﬂects the important position of clustering in scien-
tiﬁc research, the differing terminologies and goals also results in confusion. Clustering algorithms
developed to solve a particular problem in a specialized ﬁeld usually make assumptions in favor of the
application of interest. These biases inevitably affect the algorithm’s performance in other problems
that do not satisfy the same premises.
Clustering has a long history, dating back to Aristotle [8]. General references on clustering tech-
niques include textbooks by Duda et al. [10], Hartigan [9], Everitt et al. [4], Jain and Dubes [6],
Späth [11], Duran and Odell [12], Gordon [7], and Anderberg [13]. Important survey papers on
clustering techniques also exist in the literature. Starting from a statistical pattern recognition per-
spective, Jain, Murty and Flynn reviewed clustering algorithms and other important issues related
to cluster analysis [14], while Hansen and Jaumard described clustering problems under a mathe-
matical programming scheme [8]. Kolatch and He investigated applications of clustering algorithms
for spatial database systems [15] and information retrieval [16], respectively. Berkhin [17] further
expanded the topic to the ﬁeld of data mining. Murtagh [18] reported advances in hierarchical clustering

1.20.2 Clustering Algorithms
1117
Table 20.1 Notations Used in this Chapter
K,c
Number of clusters
X = {x1, . . . ,xj , . . . ,xN}
A set of N input patterns
x = (x1,x2, . . . ,xd )T ∈ℜd
A pattern belonging to a d-dimensional pattern space
{C1, . . . ,CK }
A K-partition of input patterns
D( · , · )
Distance function
S( · , · )
Similarity function
J( · )
Criterion function
M = (m1, . . . ,mK )
Prototype matrix of clusters
 = {γij }
Partition matrix
θ = (θ1, . . .θK )
Model parameter matrix
algorithms, and Baraldi surveyed several fuzzy and neural network clustering methods [19]. Additional
survey papers include those by Baraldi and Schenato [20], Bock [21], Dubes [22], Fasulo [23], and Jain
et al. [24]. In addition to these review papers, comparative research on clustering algorithms is also
signiﬁcant [25] presented empirical results for ﬁve typical clustering algorithms. Wei et al., [26] com-
pared fast algorithms for large databases. Scheunders [27] compared several clustering techniques for
color image quantization, emphasizing computation time and the possibility of obtaining global optima.
Applications and evaluations of different clustering algorithms for the analysis of gene expression data
from DNA microarray experiments were described by Shamir and Sharan, [28] and Tibshirani et al.
[29]. An experimental evaluation of document clustering techniques based on hierarchical and K-means
clustering algorithms was summarized by Steinbach et al. [30].
High-dimensional clustering has become very signiﬁcant as data continue to become more complex.
Surveys regarding biclustering [31,32] and tutorials on subspace clustering [33] review a variety of
clustering techniques for such high-dimensional data. On the other hand, researchers such as Andrew
Ng [34] have suggested achieving improved performance in classiﬁcation using unsupervised methods.
This idea combines both supervised and unsupervised learning to accomplish recognition tasks. The
purpose of this chapter is to extend the review offered by Xu and Wunsch [35] in order to provide a
descriptionofsomeinﬂuentialandimportantclusteringalgorithmsrootedinstatistics,computerscience,
and machine learning, with an emphasis on signal and image processing. Much of this chapter’s material
has been taken or adapted from [5,35–37].
1.20.2 Clustering algorithms
Several taxonomies of clustering algorithms [4,6,8,14,15,17] have been utilized, but the most widely
accepted clustering techniques are hierarchical clustering and partitional clustering based on the prop-
erties of the clusters they generate [14]. Hierarchical clustering gathers data objects with a sequence of
partitions, either from singleton clusters to a cluster including all individuals or vice versa. Partitional

1118
CHAPTER 20 Clustering
clustering directly divides data objects into some pre-speciﬁed (either known or estimated) number of
clusters without the hierarchical structure.
We begin with a discussion of hierarchical clustering followed by classical partitional clustering
algorithms. Then, recent advanced techniques for subspace clustering and biclustering will be discussed.
Finally, we will review unsupervised feature learning in the last part of the section. Table 20.1 lists
notations used in this chapter.
1.20.2.1 Hierarchical clustering
As described by Xu and Wunsch [35], hierarchical clustering (HC) algorithms organize data into a
hierarchical structure according to the proximity matrix. The results of HC usually are depicted by a
binary tree or dendrogram, as depicted in Figure 20.2. The root node of the dendrogram represents
the whole data set, and each leaf node represents a data object. The intermediate nodes thus describe
the extent to which the objects are proximal to each other, and the height of the dendrogram usually
expresses the distance between each pair of objects or clusters, or one object and one cluster. The best
clustering results can be obtained by cutting the dendrogram at different levels. This representation
provides very informative descriptions and a visualization of the potential data clustering structures,
especially when real hierarchical relationships exist in the data, such as in the data from evolutionary
x1
x2
x3
x4
x5
x6
x7
Divisive 
hierarchical 
clustering
FIGURE 20.2
An example of a dendrogram in hierarchical clustering. The dotted line corresponds to choosing a number
of clusters based on where the tree is cut. Note that the line does not necessarily have to be a straight,
horizontal cut.

1.20.2 Clustering Algorithms
1119
research on different species of organisms. HC algorithms are classiﬁed primarily as agglomerative
methods or divisive methods. Agglomerative clustering starts with N clusters, each of which includes
exactly one object. A series of merge operations then combines all objects into the same group. Divisive
clustering proceeds in an opposite way. The entire data set initially belongs to a single cluster, and then
a procedure successively divides it until all clusters are singletons. For a cluster with N objects, there
are 2N−1 −1 possible two-subset divisions, which is very computationally expensive [38]. Therefore,
divisive clustering is not commonly used in practice. We focus on agglomerative clustering in the
following discussion.
The variety of agglomerative clustering algorithms originally came about because of the different
deﬁnitions for the distance between two clusters. The general formula for distance was proposed by
Lance and Williams [39] as:
D(Ci, (Ci, C j)) = αi D(Cl, Ci) + α j D(Cl, Ci) +
βD(Ci, C j) + γ |D(Cl, Ci) −D(Cl, C j)|,
(20.1)
where D() is the distance function, and αi, α j, β, and γ are coefﬁcients whose values depend on the
scheme used.
The formula describes the distance between a cluster l and a new cluster formed by the merging of
the two clusters i and j. Note that when αi = α j = 1/2, β = 0, and γ = −1/2, the formula becomes
D(Cl, (Ci, C j)) = min (D(Cl, Ci), D(Cl, C j)),
(20.2)
which corresponds to the single linkage method. When αi = α j = γ = 1/2 and β = 0, the formula is
D(Cl, (Ci, C j)) = max (D(Cl, Ci), D(Cl, C j)),
(20.3)
which corresponds to the complete linkage method.
Several more complicated agglomerative clustering algorithms, including group average linkage,
median linkage, centroid linkage, and Ward’s method, can also be constructed by selecting appropriate
coefﬁcients in the formula. A detailed table describing the coefﬁcient values for different algorithms is
offered in [6,18]. Single-linkage, complete-linkage and average-linkage methods, all of which also are
calledgraphmethods,considerallpointsofapairofclusterswhencalculatingtheirinter-clusterdistance.
Theothermethodsarecalledgeometricmethodsbecausetheyusegeometriccenterstorepresentclusters,
and they determine the distances between clusters based on these centers. The important features and
properties of these methods were summarized by Everitt [4]. More inter-cluster distance measures,
especially the mean-based ones, were introduced by Yager [40], who further discussed their potential
for controlling the hierarchical clustering process.
Recently, to handle the limits of HC clustering and large-scale data sets, many new HC techniques
have been proposed. Guha, Rastogi and Shim [41] developed an HC algorithm called CURE to explore
more sophisticated cluster shapes. The crucial feature of CURE lies in its use of a set of well-scattered
points to represent each cluster, which makes it possible to ﬁnd rich cluster shapes other than hyper-
spheres while avoiding both the chaining effect [4] of the minimum linkage method and the tendency
to favor clusters with similar centroid sizes. These representative points are further gathered towards
the cluster centroid according to an adjustable parameter in order to weaken the effects of outliers.

1120
CHAPTER 20 Clustering
CURE utilizes a random sample (and partition) strategy to reduce computational complexity [42]
also proposed another agglomerative HC algorithm, ROCK, to group data with qualitative attributes.
They used a novel measure, a “link,” to describe the relationship between a pair of objects and their
common neighbors. Like CURE, ROCK uses a random sample strategy to handle large data sets.
Noticing the inability of centroid-based HC to identify arbitrary cluster shapes, relative hierarchical
clustering (RHC) was developed, which considers both the internal distance (distance between a pair
of clusters that may be merged to yield a new cluster) and the external distance (distance from the
two clusters to the rest). It uses the ratio of these distances to decide proximities [43]. Leung et al.
[44] demonstrated an interesting hierarchical clustering technique based on scale-space theory. They
interpreted clustering using a blurring process, in which each data point is regarded as a light point in an
image, and a cluster is represented as a blob. [45] extended agglomerative HC to handle both numeric
and nominal data. The proposed algorithm, called SBAC (Similarity-Based Agglomerative Clustering),
employs a mixed data measurement scheme that pays extra attention to less common matches of feature
values. Parallel techniques for HC are discussed by Olson [46] and Dahlhaus [47].
1.20.2.2 Partitional clustering
As opposed to hierarchical clustering, partitional clustering assigns data into K clusters without any
hierarchical structure by optimizing a criterion function, one of which is the sum of squared error
criterion. Suppose we want to organize a set of objects x j ∈ℜd, j = 1, . . . , N, into K subsets C =
{C1, . . . , CK }. The squared error criterion then is deﬁned as:
J(U, M) =
K

i=1
N

j=1
ui j∥x j −mi∥2,
(20.4)
where
M = [m1, . . . , mk] is the cluster prototype (means) matrix.
U = {ui j} is a partition matrix, ui j ∈[0, 1] and K
i=1 ui j = 1. ui j is the membership of data x j in
cluster Ci. In hard clustering, ui j = 0 or 1, as is the case with well-known k-means. In fuzzy clustering,
ui j can have any value from 0 to 1, as is the case with Fuzzy C-means (FCM).
1.20.2.3 K-means and Fuzzy C-means algorithm
The K-means algorithm is one of the best-known clustering algorithms [48]. The basic steps of K-means
are as follows:
S1. Initialize a K-partition randomly or based on some prior knowledge, and calculate the cluster
prototype matrix M = [m1, . . . , mK ];
S2. Assign each object in the data set to the nearest cluster Cw;
S3. Recalculate the cluster prototype matrix based on the current partition:
mi = 1
Ni

x j∈Ci
x j.
(20.5)

1.20.2 Clustering Algorithms
1121
S4. Repeat steps 2–3 until there is no change for each cluster.
Note that the update in step S3 requires all the data points. One way to enhance the K-means to online
learning is to adjust the cluster means each time a data point is presented:
mnew = mold + η(x −mold),
(20.6)
where η is the learning rate.
In the signal processing community, an algorithm similar to k-means, known as the Linde-Buzo-
Gray (LBG) algorithm, was suggested for vector quantization (VQ) [49] in order to signal compression.
In this context, prototype vectors are called code words, which constitute a code book. VQ aims to
represent the data with a reduced number of elements while minimizing information loss.
As stated above regarding fuzzy clustering, the object can hold a certain degree of membership in all
clusters. This is particularly useful when the boundary between the clusters is ambiguous. The procedure
of fuzzy C-means [50] is similar to that of k-means, with the exception of the following updating rules:
S1. Select appropriate values for m, c, and a small positive number ε. Initialize the prototype matrix
M randomly. Set step variable t = 0;
S2. Calculate (at t = 0) or update (at t > 0) the membership matrix U using:
u(t+1)
i j
= 1
 c

l=1

D(x j, mi)/D(x j, ml)
2/(1−m)

,
for i = 1, . . . , c and
j = 1, . . . , N;
(20.7)
S3. Update the prototype matrix M using:
m(t+1)
i
=
⎛
⎝
N

j=1

u(t+1)
i j
m
x j
⎞
⎠
 ⎛
⎝
N

j=1

u(t+1)
i j
m
⎞
⎠,
for i = 1, . . . , c
(20.8)
S4. Repeat steps 2–3 until
M(t+1) −M(t) < ε.
FCM, like k-means, suffers from initial partition dependence, as well as noise and outliers. Yager
and Filev [51] proposed the mountain method to estimate the cluster centers as an initial partition. Gath
and Geva [52] addressed the initialization problem by dynamically adding cluster prototypes, which
are located in the space that is not represented well by the previously generated centers. Changing the
proximity distance can improve the performance of FCM in relation to outliers [53]. In another approach
for mitigating the effect of noise and outliers, Keller interpreted memberships as “the compatibility of
the points with the class prototype” [54] rather than as the degree of membership. This relaxes to and
ui j to ui j > 0 results in a possibilistic C-means clustering algorithm.
1.20.2.4 Mixture density-based clustering
From a probabilistic perspective, as described by Xu and Wunsch [35], data objects are assumed to be
generated according to several probability distributions. Data points in different clusters are generated

1122
CHAPTER 20 Clustering
by different probability distributions and can be derived from different types of density functions (e.g.,
multivariate Gaussian or t-distribution), or from the same families but with different parameters. If
the distributions are known, ﬁnding the clusters of a given data set is equivalent to estimating the
parameters of several underlying models. Suppose the prior probability (also known as the mixing
probability) P(Ci) for cluster Ci, i = 1, . . . , K (here, K is assumed to be known; methods for estimating
K are discussed in section II-M) and the conditional probability density p(x|Ci, θi) (also known as the
component density), where θi is the unknown parameter vector, are known. Then, the mixture probability
density for the entire data set is expressed as:
P(x|θ) =
K

i=1
p(x|Ci, θi)P(Ci),
(20.9)
where θ = (θ1, . . . , θK ), and K
i=1 P(Ci) = 1. As long as the parameter vector θ is decided, the
posterior probability for assigning a data point to a cluster can be calculated easily with Bayes’s theorem.
Here, the mixtures can be constructed with any type of component, but more commonly, a multivariate
Gaussian density is used due to its complete theory and analytical tractability [4,36]. Parameter θ can
be estimated using maximum likelihood estimation (ML).
Unfortunately, because the solutions of the likelihood equations cannot be obtained analytically in
most circumstances [55], iteratively optimal approaches are required to approximate the ML estimates.
Among these methods, the expectation-maximization (EM) algorithm is the most popular [56]. EM
regards the data set as incomplete and divides each data point x j into two parts, x j = {xg
j , xm
j }, in which
xg
j represents the observable features and xm
j = (xm
j1, . . . , xm
j K ) is the missing data, where xm
ji chooses
a value of 1 or 0 according to whether or not x j belongs to the component i. Thus, the complete data
log-likelihood is:
l(θ) =
N

j=1
K

i=1
xm
ji log[P(Ci)p(xg
j |θi)].
(20.10)
The standard EM algorithm generates a series of parameter estimates {θ0, θ1, . . . , θT }, where T
represents reaching the convergence criterion, as accomplished through the following steps:
1. Initialize θ0 and set t = 0;
2. E-step: Compute the expectation of the complete data log-likelihood:
Q(θ, θt) = E[log p(xg, xm|θ)|xg, θt].
(20.11)
3. M-step: Select a new parameter estimate that maximizes the Q-function:
θt+1 = arg max
θ
Q(θ, θt).
(20.12)
4. Increase t = t + 1; Repeat steps 2 and 3 until the convergence condition is satisﬁed.

1.20.2 Clustering Algorithms
1123
The major disadvantages of the EM algorithm are its sensitivity to the selection of initial parameters,
the effect of a singular covariance matrix, the possibility of convergence to a local optimum, and the
slow convergence rate [56]. Variants of EM that address these problems were discussed by McLachlan
and Krishnan [56].
A valuable theoretical note pertains to the relationship between the EM algorithm and the K-means
algorithm. Celeux and Govaert [57] proved that a classiﬁcation EM (CEM) algorithm under a spherical
Gaussian mixture is equivalent to the K-means algorithm.
MCLUST is a software implementation of a multivariate Gaussian distribution developed by Fraley
and Raftery [58]. The initial guess is computed from agglomerative hierarchical clustering, along with
the maximum number of clusters. Each component is parameterized by virtue of eigenvalue decompo-
sition, represented as  = λAT , where λ is a scalar,  is the orthogonal matrix of eigenvectors,
and A is the diagonal matrix based on the eigenvalues of . The optimal clustering result is achieved
by checking the Bayesian Information Criterion (BIC) value.
Gaussian Mixture Density Decomposition (GMDD) also is based on multivariate Gaussian densities
and is designed as a recursive algorithm that sequentially estimates each component [59]. GMDD views
data points that are not generated from a distribution as noise and the original data as a contaminated
model. It utilizes an enhanced model-ﬁtting estimator to iteratively construct each component from the
contaminated model. AutoClass considers more families of probability distributions (e.g., Poisson and
Bernoulli) for different data types [60]. AutoClass uses a Bayesian approach to ﬁnd the optimal partition
of the given data based on the prior probabilities.
1.20.2.5 Neural network-based clustering
Popular neural network-based clustering algorithms include Self-Organizing Feature Maps (SOFM)
and Adaptive Resonance Theory (ART), both of which were reviewed by Xu and Wunsch [5,35].
The objective of SOFM is to represent high-dimensional input patterns with prototype vectors that
can be visualized in a usually two-dimensional lattice structure [61,62]. Each unit in the lattice is
called a neuron, and adjacent neurons are connected to each other, offering a clear map depicting how
this neuron network ﬁts itself to the input space. Input patterns are fully connected to all neurons via
adaptable weights, and during the training process, neighboring input patterns are projected into the
lattice, corresponding to adjacent neurons. In this sense, some authors prefer to think of SOFM as a
method by which to display latent data structures in a visual way rather than via a clustering approach.
While SOFM enjoy the merits of input space density approximation and independence regarding
the order of input patterns, a number of user-dependent parameters become problematic when applied
in real practice. Like the K-means algorithm, SOFM must predeﬁne the size of the lattice, i.e., the
number of clusters, which is unknown in most circumstances. Additionally, trained SOFM may suffer
from input space density misrepresentation [63], where areas of low and high pattern density may
be over-represented and under-represented, respectively. Kohonen [62] reviewed a variety of SOFM
variants that help to eliminate the drawbacks of basic SOFM and broaden its applications. SOFM also
can be integrated with other clustering approaches (e.g., K-means algorithm or HC) to provide more
effective, faster clustering. Su and Chang [64] and Vesanto and Alhoniemi [65] illustrated two such
hybrid systems. The basic procedures of SOFM are then summarized using an algorithm, as follows:

1124
CHAPTER 20 Clustering
1. DeterminethetopologyoftheSOFM.Initializetheweightvectors w j(0)forj = 1,…,K,randomly;
2. Present an input pattern x to the network. Choose the winning node J that has the minimum
Euclidean distance to x,
J = arg j min (∥x −w j∥).
(20.13)
3. Calculate the current learning rate and size of the neighborhood;
4. Update the weight vectors of all the neurons in the neighborhood of J,
w j(t + 1) = w j(t) + hJj(t)(x −w j(t)),
(20.14)
where h J jt is the neighborhood function, deﬁned, for example, as
hJj(t) = η(t) exp
−∥rJ −r j∥2
2σ 2(t)

,
(20.15)
where rJ and r j represent the positions of the corresponding neurons on the lattice, and σ(t) is
the monotonically decreasing Gaussian kernel width function.
5. Repeat steps 2–4 until the change of the neuron’s position is below a pre-speciﬁed small positive
number.
Adaptive resonance theory (ART) was developed by Carpenter and Grossberg [66] as a solution to
the plasticity and stability dilemma. ART can learn arbitrary input patterns in a stable, fast and self-
organizing way, thus overcoming the effect of learning instability that plagues many other competitive
networks. ART is not, as is popularly imagined, a neural network architecture. It is a learning theory
suggesting that resonance in neural circuits can trigger fast learning. As such, it subsumes a large
family of current and future neural network architectures with many variants. ART1, the ﬁrst member,
only deals with binary input patterns [67], although it can be extended to arbitrary input patterns by a
variety of coding mechanisms. ART2 extends the applications to analog input patterns [66], and ART3
introduces a new mechanism originating from elaborate biological processes to achieve a more efﬁcient
parallel search in hierarchical structures [66]. By incorporating two ART modules, which receive input
patterns (ARTa) and corresponding labels (ARTb), respectively, with an inter-ART module, the resulting
ARTMAP system can be used for supervised classiﬁcations [68]. The match-tracking strategy ensures
consistency in category prediction between two ART modules by dynamically adjusting the vigilance
parameter of ARTa.
As seen in Figure 20.3, ART1 consists of 2-layer nodes, the feature representation ﬁeld F1 and
category (cluster) representation F2. The neurons in layer F1 are activated by input patterns, while the
neurons in layer F2 store cluster information. The neurons in layer F2 already being used as represen-
tations of input patterns are said to be committed; otherwise, they are uncommitted. Two layers are
connected by adaptive weights W 12 and W 21. The parameter ρ determines when the pattern and the
expectation match, at which point weight adaption occurs.

1.20.2 Clustering Algorithms
1125
Layer F1
Input 
pattern I
Layer F2
Orienting 
subsystem
Reset
21
W
12
W
ρ
Attentional 
subsystem
…
…
FIGURE 20.3
ART1 architecture. Two layers are included in the attentional subsystem, connected via bottom-up and top-
down adaptive weights. Their interactions are controlled by the orienting subsystem through a vigilance
parameter. An attractive feature of this approach is that the number of clusters does not need to be speciﬁed
in advance.
The ART1 algorithm can be described as follows:
1. Initialize weight matrices W12 and W21 as , W12 = ξ/(ξ +d −1), where d is the dimensionality
of the binary input x, ξ is a parameter that is larger than 1 and W21
ji = 1;
2. For a new pattern x, calculate the input from layer F1 to layer F2 as:Tj = d
i=1 W12
i j xi;
3. Activate layer F2 by choosing node J with the winner-takes-all rule TJ = max
j {Tj};
Compare the expectation from layer F2 with the input pattern. If ρ ≤|x∩W21
J |
|x|
, where ∩represents
the logic AND operation, go to step 5a; otherwise, go to step 5b.
4.
a. Update the corresponding weights for the active node as W21
J
=
x ∩W21
J
and
W12
J =
ξW12
J
ξ −1 + |W12
J
;
b. Send a reset signal to disable the current active node using the orienting subsystem, and
return to step 3;
5. Present another input pattern, and return to step 2 until all patterns are processed.
Note the relationship between the ART network and other clustering algorithms described in tradi-
tional and statistical language. Moore [69] used several clustering algorithms to explain the clustering
behaviors of ART1 and therefore induced and proved a number of important ART1 properties, notably,
its equivalence to varying K-means clustering. She also showed how to adapt these algorithms under
the ART1 framework.
Fuzzy ART (FA) beneﬁts from incorporating fuzzy set theory with ART. FA operates in a way
similar to ART1 and uses the fuzzy set operators to replace the binary operators, allowing it to work

1126
CHAPTER 20 Clustering
for all real data sets. FA exhibits many desirable characteristics, such as fast and stable learning and
atypical pattern detection. Huang and Heileman [70] investigated and revealed more FA properties,
classiﬁed as template, access, reset and the number of learning epochs. To overcome these shortcomings,
Williamson [71] described Gaussian ART (GA), in which each cluster is modeled with a Gaussian
distribution and represented geometrically as a hyperellipsoid. GA does not inherit the ofﬂine fast
learning property of FA. Hypersphere ART (HA) [72] for hyperspherical clusters and ellipsoid ART
(EA) ([73] for hyperellipsoidal clusters are proposed to offer a more efﬁcient representation of clusters
while maintaining the important properties of FA.
1.20.2.6 Spectral clustering
Spectral clustering has become popular because it is easy to implement yet outperforms traditional
algorithms, such as k-means.
The name spectral clustering comes from the technique of using the spectrum (eigenvalues) of the
similarity matrix to perform dimension reduction before clustering with traditional clustering methods.
The similarity matrix is built from the dataset.
Given a dataset x1, . . . , xn the similarity matrix can be built as follows. Each vertex vi represents a
data point xi. Two vertices are connected by an edge weighted by wi j, which is the similarity between
the two data points, usually given in the clustering problem. If no prior information about the similarity
exists, a Gaussian similarity function can be used instead: wi j,i̸= j = exp

−(∥xi−x j∥)2
2σ 2

, where σ
controls the width of the neighborhood. The degree of vertex vi is deﬁned as di = n
j=1 wi j.
The adjacent matrix is given by W = (wi j)i, j=1...n′ and the degree matrix is deﬁned as the diagonal
matrix D with the degrees d1, . . ., dn on the diagonal. From those, a Laplacian matrix can be built:
Unnormalized Laplacian matrix: L = D −W.
Normalized Laplacian matrix [74]: Lsym = D−1L.
Normalized Laplacian matrix [75]: Lrw = D−1/2L D−1/2.
Each Laplacian matrix has a respective corresponding spectral clustering algorithm. As these algo-
rithms are similar, we only list here the normalized spectral clustering algorithm by Ng et al. [74]; for
other algorithms, see [76]:
S1. Compute the normalized Laplacian Lsym.
S2. Compute the ﬁrst k eigenvectors u1, . . . , uk of Lsym.
S3. Let U ∈Rnxk be the matrix containing the vectors u1, . . . , uk as columns.
S4. From the matrix T, form U by normalizing the rows to norm 1.
S5. For i-1,…,n, let yi ∈Rk be the vector corresponding to the ith row of T.
S6. Cluster the points yi with k-means into clusters C1, . . . , Ck.
A special spectral clustering technique is to change the representation of the abstract data points xi
into points y ∈Rk. Laplacian properties make this change of representation useful so that clusters can
be made trivially using k-means.
Concerning computational complexity, spectral clustering is O(n3) because of the computation of
eigenvectors. One can reduce this complexity substantially by making the similarity matrix sparse and
applying a special routine for the sparse matrix to compute only the ﬁrst k smallest eigenvectors.

1.20.2 Clustering Algorithms
1127
1.20.2.7 Subspace clustering and biclustering
As technology continues to develop, more and more increasingly complex (especially in dimension)
data are being generated, and the assumption that the number of elements is larger than the number
of features no longer holds true. This voids many approaches and demands new algorithms to process
high-dimensional data.
In this section, we will ﬁrst clarify the difference between biclustering (BC) and subspace clustering
(SC) by explaining their applications and intra-cluster relationships. Then, we will review a variety of
popular algorithms for both clustering techniques.
The inputs for clustering are a set of features F (called the set of genes V in BC, or the set of
dimensions D in SC, row-wise) and a set of samples S (called the set of conditions U in BC, or the set
of vectors V in SC, column-wise). F coupled with S forms an input matrix for the clustering problem,
matrix E, whose size is |F| × |S|, where || denotes the cardinal of the set.
A bicluster (U′, V′) is deﬁned by a subset of genes V′ in V and a subset of conditions U′ in U.
Different clustering algorithms use different criteria to qualify a bicluster solution. In order to deﬁne
a subspace cluster, let {x j ∈Rd} be a set of points drawn from the union of n subspaces. A subspace
cluster then is deﬁned as:
Si = {x : x = μi + Ui y}n
i=1,
(20.16)
where μi is a point in subspace Si, U ∈RD×di is a basis for subspace Si, and y ∈Rdi is a low-
dimensional representation of the subspace. The goal of subspace clustering is to determine the number
of subspaces n, their dimensions, and their bases and then to segment the points according to the
subspaces (see Figure 20.4a).
The following section addresses the difference between biclustering and subspace clustering in
partition structures, intra-cluster relationships, and application datasets.
FIGURE 20.4a
Three subspaces—two lines and a plane.

1128
CHAPTER 20 Clustering
FIGURE 20.4b
Subspace clusters and biclusters.
1.20.2.7.1
Comparing subspace clustering and biclustering
1.20.2.7.1.1
Partition structure
SC is a 1-dimensional matrix clustering that groups the elements in V into groups that form a subspace
in the D-dimension. A permutation is performed on matrix E after clustering produces consecutive
samples that belong to the same group (see Figure 20.4b). On the other hand, BC provides a new way to
look at the data structure. It is a 2-dimensional clustering, also called co-clustering, in which a bicluster
of E is a submatrix of E formed by a subset of F and a subset of S. Performing a permutation on matrix E
after biclustering reveals that the biclusters form small rectangles inside the big rectangle E. Therefore,
biclustering and subspace clustering produce very different results.
Biclustering, ﬁrst used by Cheng and Church [77] in the bioinformatics community, addresses this
problem by performing clustering simultaneously on both the row (gene) and column (sample) dimen-
sions instead of clustering these two dimensions separately [31,78]. In essence, biclustering can be
regarded as a combination of clustering and automatic feature selection if one dimension (e.g., column)
is treated as data objects and the other dimension (e.g., row) as description features. This task becomes
particularly challenging without ground truth.
Note that the feature selection in biclustering is different from the feature selection usually considered
in subspace clustering in that biclustering selects different subsets of features for different clusters of
data objects, while standard feature selection chooses a subset of features from the candidate pool for all
data objects. As such, biclustering can identify local relationships between subsets of genes and subsets
of samples or conditions. Biclustering indicates gene groups that display similar patterns across a set of
conditions, which is important for gene functional annotation and co-regulated gene identiﬁcation [79]
or in terms of gene groups that are related to certain cancer types (for cancer classiﬁcation discovery and
diagnosis) [78]. In contrast, subspace clustering focuses on uncovering global relationships between
genes and samples or conditions. In ﬁelds other than bioinformatics, biclustering is also known as
co-clustering or block clustering, among other names.
1.20.2.7.1.2
Intra-cluster relationship
One way in which BC and SC differ is in the data relationships between elements in the same cluster. In
BC, data in the same clusters are homogeneous, i.e., the bicluster in BC must have some kind of homo-
geneity across the rows or columns of the sub-matrix, meaning that the rows or columns must be the same
or have coherent additive or multiplicative values [31]. On the contrary, data in the same SC cluster can be
very different, but as long as the samples lie in the same subspace, they are grouped into the same cluster.

1.20.2 Clustering Algorithms
1129
FIGURE 20.5
Normal cluster (row cluster, column cluster) vs. bicluster (co-cluster) (From [48]).
Figure 20.5 illustrates the relationships between elements in the same bicluster. Matrix E is formed
from the relationship between points in R2, which includes 20 points for the row objects (circles) and
40 points for the column objects (squares). Matrix E has the entry Ei, j = Euclidian distance between
the ith point among the row objects and the jth point among the column objects. Points are chosen so
that they form three clusters for row objects and four clusters for column objects. The most remarkable
observation in this arrangement is that the points in row and column objects form two co-clusters (the
portion of the ﬁgure that circles and squares cohabit). The length of a line connecting a circle to a square
is the value of the entry in matrix E. Therefore, the subset of rows and columns where points cohabit in
matrix E displays homogeneity in that the entries in this sub-matrix are much smaller than those in the
other sub-matrix.
1.20.2.7.1.3
Dataset
Biclustering can be applied comfortably to relational datasets, those in which some relationship exists
between the rows and columns. In other words, datasets with relational characteristics are very good
for biclustering. Below are some examples of biclustering data.
•
Gene expression: Each sample comprises a gene, and each gene is proﬁled differently in each sample.
See [31] for more information about gene expression.
•
Document classiﬁcation: Each row is a document, and each column is a word in a document. An
entry in matrix E is the occurrence of a word in a document.
•
Netﬂix database:Rows are the number of movies, and columns are the number of people who rent
movies from Netﬂix. A matrix E entry measures a user’s satisfaction with a speciﬁc movie.
Unlike BC, SC is suitable for space datasets, meaning that if the positions of all the samples in the
dataset can be plotted, they all will form subspaces. The problem is that those subspaces are not easy to

1130
CHAPTER 20 Clustering
visualize and typically are not independent. The most applicable form of subspace clustering is motion
segmentation, which can be found in [80].
1.20.2.7.2
Subspace clustering algorithms
1.20.2.7.2.1
Matrix factorization algorithms
Algebraic clustering algorithms are based on linear algebra matrix factorization. As a pure math theory
development, the algorithms require independent subspaces and noise-free data.
Let X be the matrix representing the dataset and [S1, S2, .., Sn] = X be the sorted X according
to the n subspaces, where subspace Si ∈RD×Ni is the matrix containing Ni points  ∈Rn×n and
is an unknown permutation matrix. Si can be factorized as si = UiYi, where Ui ∈RD×di is a basis
of subspace i and Y ∈Rdi×Ni is the low-dimensional representation of Si. Substitute Si into X as
follows:
X = [U1, U2, . . . , Un]
⎡
⎢⎢⎣
Y1
Y2
· · ·
Yn
⎤
⎥⎥⎦= UY,
(20.17)
where U ∈RD×r and Y ∈Rr×N with r = rank(X) = n
i=1 di.
Algebraic clustering next involves trying to ﬁnd matrix  such that X can be factorized to U and
Y. This can be achieved using the linear algebra manipulation over X [81,82].
1.20.2.7.2.2
Iterative methods
Iterative algorithms are based on a two-step iteration. Given an initial clustering, we can ﬁt each cluster
into a subspace using PCA, and then assign a data point to each cluster based on its distance from the
subspace. Iteration between the two steps occurs until convergence is achieved. These algorithms work
in a way similar to k-means clustering. Algorithms such as the k-means projective algorithm [83] are
based on this idea. In general, a K-subspace iterative method works by letting wi j be the membership
of data point j in cluster j; wi j = 1 if j belongs to subspace i, and 0 otherwise. Minimizing the square
distance allows us to ﬁnd, {μi}, {Ui}, and {y j}, as well as membership wi j:
min
{μi}, {Ui}, {yi}, {wi j}
N

j=1
n

i=1
wi j∥x j −μi −Uiy j∥2
(20.18)
s.t wi j ∈{0, 1} and
n

i=1
wi j = 1.
We begin solving Eq. 20.18 by initializing {μi}, {Ui} and {yj}, which allows us to ﬁnd the optimal
wi j using:
wi j =
1
if i = argmin∥x j −μk −Uky j∥
0 otherwise
.
(20.19)
Once wi j is known, μi, Ui, and y j can be recomputed using PCA for each cluster. This simple
algorithm guarantees convergence but requires a good initialization and cannot handle outliers.

1.20.2 Clustering Algorithms
1131
1.20.2.7.2.3
Statistical method
We start by building a generative model for the data. As in [84], data in a single subspace S can be
modeled as a Gaussian:
x = μ + Uy + e,
(20.20)
where y is Gaussian with a mean of zero and a unit variance of ∼N(0, I) and e is also Gaussian with
a mean of zero and a unit variance of ∼N(0, σ 2I).
x is then normally distributed ∼N(μ, UUT + σ 2I).
Mixture probabilistic PCA extends the single Gaussian by regarding each subspace as Gaussian and
the dataset as a mixture of Gaussians, which therefore can be clustered using an EM algorithm. Let
p(x) be the Gaussian mixture of the dataset:
p(x) =
n

i=1
πiN(x; μi, UiUiT + σ 2
i I),
(20.21)
where πi is the weighted mixture.
Now we can iterate between steps E and M in EM to compute the Gaussian parameters (μi, Ui, σi, πi)
according to [80].
In step E:
Ri j = p(xj|i)πi
p(xj)
.
(20.22)
In step M:
πi = 1
N
N

j=1
Ri j,
(20.23)
μi =
N
j=1 Ri jxj
N
j=1 Ri j
.
(20.24)
Ui and noise variance σ 2
i are determined from the SVD of the local responsibility weighted con-
variance matrix:
si =
1
πi N
N

j=1
Ri j(xj −μi)(xj −μi)T .
(20.25)
These two steps are repeated until convergence occurs.
1.20.2.7.2.4
RANSAC method
RANSAC [85] is a powerful statistical method for ﬁtting a model given a dataset that is corrupted by
noise and outliers. This technique is used widely in computer vision when solving the fundamental
matrix in a stereo correspondence problem. The basic idea of RANSAC is to pick d points from the
dataset, enough to estimate and ﬁt model, and then compute the residue of each point to this model,

1132
CHAPTER 20 Clustering
choosing only points whose residue is lower than a certain threshold as inliers. These steps then are
repeated with other randomly selected d points until enough samples are drawn. RANSAC can be applied
in subspace clustering [86] by ﬁnding and situating one subspace as the model and then marking certain
points in that subspace as inliers and the rest as outliers. After the ﬁrst subspace is discovered, the inliers
are removed from the dataset, and another RANSAC is repeated to discover the next subspace. This
procedure is repeated until all the subspaces are recovered. PCA is used to compute the basis of the
subspace for each set of inliers.
1.20.2.7.2.5
Sparse subspace clustering
Sparse representation and compressive sensing are emerging areas in signal and image processing
[87]. Due to the development of several efﬁcient L1-norm solvers, sparse representation has been
applied in many research areas, especially in clustering. The idea behind subspace clustering using
compressed sensing [88] is that a point x j will have sparse representation in the sensing matrix X =
[x1, x2 . . . x j−1, x j+1, . . . , xN], which means that the nonzero coefﬁcients in sparse representation
correspond to the points that lie in the same subspace as y j. After applying sparse representation to all
the points, we build a matrix of sparse representation coefﬁcients, CN×N. Subspace segmentation can
be achieved by applying K-means to a subset of eigenvectors of the Laplacian of C.
Let wi j be the linear combination of x j. If the data are noise-free, wi j can be found using
l1-optimization:
min

i̸= j
|wi j|
(20.26)
s.t.x j =

i̸= j
wi jxi.
If the data contain noise, a relaxation is added into the optimization:
min
wi j

i̸= j
|wi j| + μ∥x j −

i̸= j
wi jxi∥2,
(20.27)
where μ is a positive relaxation coefﬁcient.
In the case of outliers, sparse vector outliers are added to the sparse representation of
x j =

k̸= j
wi jxi + e j.
(20.28)
The l1-norm optimization becomes:
min
wi j,e j

i̸= j
|wi j| + μ

x j −

i̸= j
wi jxi −e j

2
.
(20.29)
The sparse representation of each data point then is used as the featured vector for spectral clustering
[76].
In [89], Vidal extended the solution to disjointed subspaces and derived theoretical bounds relating the
principal angles between the subspace and the distribution of the data points across all of the subspaces.

1.20.2 Clustering Algorithms
1133
1.20.2.7.3
Biclustering algorithms
Let I ⊂G and J ⊂S be subsets of the rows and columns for the gene expression data matrix E;
EI J = (I, J) is then the submatrix with rows I and columns J. A bicluster corresponds to such a
submatrix that exhibits certain homogeneity. The goal of a biclustering algorithm, then, is to identify
a set of biclusters with pairs of row and column subsets. The complexity of the biclustering problem
has been shown to be NP-complete [31], which leads to many heuristics falling into the ﬁve major
categories summarized in Table 20.2, according to Madeira and Oliveira.
The ﬁrst biclustering research was presented by Cheng and Church [77]. The mean squared residue
is used to measure the coherence of the rows and columns in a bicluster, which is deﬁned as:
H(I, J) =
1
|I||J|

i∈I, j∈J
(ei j −ei J −eI j + eI J)2,
(20.30)
Table 20.2 Biclustering Algorithms
Algorithm category
Description
Examples
Combination o iterative row
and column clustering
Also known as two-way
clustering. Uses existing
clustering algorithms for the
row and column dimensions
separately. Combines the
one-way results to produce
biclusters.
Interrelated two-way clustering
(ITWC) [90] Double conjugated
clustering (DCC) [91]; Coupled
two-way clustering (CTWC)
[92]; Fuzzy adaptive subspace
iteration-based two-way
clustering (FASIC) [93]
Divide and conquer
Divides the original problem
into a set of smaller
subproblems. Solves the
subproblems and combines
the solutions to obtain the
ﬁnal solution to the original
problem.
Block clustering and its variants
[94,95]
Greedy iterative search
Iteratively inserts or removes
rows or columns from the
biclusters, guided with some
criterion function.
δ-biclusters [77]; Flexible
overlapped biclustering (FLOC)
[96]; xMOTIF [95]; Order
preserving submatrices
(OPSM) [97]
Exhaustive bicluster
enumeration
Performs exhaustive
enumeration but with some
restrictions on the size of
biclusters.
Statistical algorithmic method
for bicluster analysis (SAMBA)
[98]; pclusters [99]
Distribution parameter
identiﬁcation
Considers underlying
statistical models for
biclusters. Estimates model
parameters that best ﬁt the
data.
Probabilistic relational models
[99]; Plaid models [100]

1134
CHAPTER 20 Clustering
where ei J is the mean of the ith row, eI j is the mean of the jth column, and eI J is the mean of the
bicluster. A submatrix EI J is called a δ-bicluster if H(I, J) ≤δ for some δ ≥0. Possible ways to ﬁnd
the largest δ-biclusters, which should also have relatively high row variance, include the evolutionary
algorithm [101], multi-objective particle swarm optimization [102], and other greedy iterative search
approaches.
Another popular biclustering algorithm is the interrelated two-way clustering (ITWC) algorithm.
ITWC generates biclusters by combining clustering results from each dimension of the data matrix
in an iterative way. Within each iteration, a set of gene clusters ﬁrst is created by standard clustering
algorithms, e.g., K-means or SOFM, followed by the independent clustering of the sample dimension
based on each gene cluster. The clustering results from the previous steps then are combined, and
heterogeneous groups, which are pairs of columns that do not share gene features used for clustering,
are identiﬁed. Finally, the genes are sorted in descending order of the cosine distance, and only the ﬁrst
one-third of sorted genes is kept so that a reduced gene set is obtained for each heterogeneous group. The
above steps then are repeated using the reduced gene set until some stopping conditions are satisﬁed.
Other two-way clustering algorithms include the coupled two-way clustering (CTWC) algorithm [92],
which uses hierarchical clustering with the Euclidean distance, the double conjugated clustering (DCC)
algorithm [91], which uses SOFM with the dot product for similarity, and the fuzzy adaptive subspace
iteration-based two-way clustering (FASIC) algorithm, which is integrated with the concept of fuzzy
membership.
Xu and Wunsch [37] developed a new two-way clustering technique called BARTMAP, the structure
of which consists of two Fuzzy ART modules, ARTa and ARTb, which communicate through the inter-
ART module. The inputs of the ARTa module are samples, and the inputs of ARTb are genes. The
goal of BARTMAP is to integrate the clustering results on the dimensions of columns and rows of the
data matrix to create biclusters that capture the local relationships between genes and samples. More
concretely, the ﬁrst step of BARTMAP is to create a set of Kg gene clusters Gi, i = 1, . . . , Kg, for N
genes by using the ARTb module. Then, upon the presentation of a new sample in ARTa, the candidate
sample cluster that is eligible to represent this sample is decided based on the winner take all rule.
If this candidate cluster corresponds to an uncommitted neuron, a new one-element cluster is created.
However, if a committed neuron is picked, the weights of this winning neuron are only updated if it
displays behaviors similar to those of other members in the cluster formed in the ARTb module, as
measured by the Pearson correlation coefﬁcient.
Lazzeroni and Owen [100] developed a plaid model, treating the matrix as the sum of additive layers.
This model was extended further to combine external grouping information and to generate biclusters
of proﬁles of repeated measures [103]. Gu and Liu [104] proposed a Bayesian biclustering model
with a Gibbs sampling procedure for statistical inference. McAllister et al. [105] considered modeling
biclustering as either a network ﬂow problem or a traveling salesman problem. A systematic comparison
of ﬁve greedy search-based biclustering algorithms, together with a reference method (Bimax), was
presented by Preli´c et al. [106].
CTWC uses hierarchical clustering with the Euclidean distance, while DCC relies on self-organizing
maps with the dot product for similarity [91]. FASIC adopts the concept of fuzzy set theory and applies
the fuzzy-adaptive subspace iteration algorithm to generate gene clusters under a progressive clus-
tering framework. Those clusters then are scored to generate p-values indicating the signiﬁcance of
the differential expression [93]. Tchagang et al. [107] demonstrated the effectiveness of biclustering

1.20.2 Clustering Algorithms
1135
for detecting and diagnosing ovarian cancer. Mitra et al. [108] showed a biclustering application for
microRNA expression proﬁle-based cancer classiﬁcation of multiple cancer types, including melanoma,
ovarian, renal, colon, and leukemia.
1.20.2.8 Deep learning clustering
All of the clustering algorithms presented so far have not required any explicit feedback, i.e., they all
engage in unsupervised learning. In contrast to this is supervised learning, in which the program learns
to map functions from input to output by training data with labels. However, in practice, the available
data will include some labeled samples and a large collection of unlabeled samples. Algorithms that
work with such a mix of data are said to engage in semi-supervised learning; one of the most powerful
semi-supervised clustering techniques is deep learning.
As human-computer interaction (HCI) continues to develop, more and more images and data will
be created. A large and rapidly-growing demand exists for the ability to search, organize and recognize
media content. The most popular methods are based on hand-crafted features and therefore are very
time and resource consuming [109,110]. On the other hand, researchers such as Andrew Ng [34] have
suggested the novel idea of achieving best performance in classiﬁcation using unsupervised methods.
This idea combines both supervised and unsupervised learning to accomplish recognition tasks.
The central idea behind unsupervised feature learning is the sparse coding neural network [111].
Sparse coding retains only the characteristic structure of the image at the input of the network. In short,
the goal of sparse coding is as follows:
Input: given images X1, X2, . . . , Xm ∈Rn×n.
Output: dictionary bases φ1, φ2, . . . , φk ∈Rn×n.
So that in every image X = n
j=1 a jφ j, most a j’s are 0.
1.20.2.8.1
Sparse auto-coding network
Sparse auto-coding uses a 3-layer neural network with input, hidden and output layers. The terminology
auto sparse encoderimplies something about this network’s structure. The value at the output layer is
exactly the same as at the input layer. The input and output layers have the same number of neurons,
which differs from the hidden layer, in which the interesting structure of the data is retained (see
Figure 20.6).
When training the sparse auto-coding network, a sparse constraint is imposed on the hidden layer
in order to force the network to reﬂect the structure of the input. We denote the average activation of
hidden unit j (over the entire training set) as ρ j and the number of neurons in the hidden layer as s. We
will enforce ρ j = ρ, where ρ is a sparsity parameter, by adding the penalty term s
j=1 = K L(ρ∥ˆρ j)
in the objective function of backpropagation [112]:
Jsparse(W, b) = J(W, b) + β
s

K L(ρ∥ˆρ j),
(20.31)
where W represents the weight of the connections in the network, J(W, b) is the sum of the square-error
cost function of training the neural network, and KL refers to Kullback-Leiber [113], which measures

1136
CHAPTER 20 Clustering
FIGURE 20.6
Example structure of a sparse auto-coding network.
the difference in the distribution of ρ and ρ j.
K L(ρ∥ˆρ j) = ρ log ρ
ˆρ j
+ (1 −ρ) log 1 −ρ
1 −ˆρ j
.
(20.32)
To clarify the result of training the sparse auto-encoder network, we investigate the hidden layer:
ai = f
⎛
⎝
n

j=1
Wi j x j
⎞
⎠,
(20.33)
where ai is the output of neuron i in the hidden layer, and wi j is the weight of the connection from
input x j to neuron i, by which we can determine the maximum value of ai. To avoid arriving at a trivial
solution, we force ∥x∥<= 1 (by normalization). ai obtains the maximum when:
x jmax =
Wi j
n
j=1W2
i j
,
(20.34)
which indicates that the image with the structure Wi j will be ﬁred maximally on ai. Due to the properties
of the backpropagation algorithm, the weights connecting the hidden layer to the output layer are updated
separately from those connecting the input layer to the hidden layer; those weights are not considered
here because they do not have a speciﬁc meaning in sparse representation.
The result of training 200 images in a 5 × 5 grid is shown in Figure 20.7.

1.20.2 Clustering Algorithms
1137
FIGURE 20.7
Each cell represents features detected in the training data and stored in the hidden unit. The features
detected here are edges at different positions and orientations (From Andrew Ng; Sparse autoencoder. CS
294A Machine Learning lecture notes).
1.20.2.8.2
Independent subspace analysis (ISA) network
Another neural network that can be used to auto-learn features is the 3-layer independent subspace
analysis network (ISA) [34]. The hidden layer has square nonlinearity, and the second layer has square
root nonlinearity. The hidden layer is connected to the output layer via a few links of adjacent neighbors
and represents the subspace structure of the neural network. The structure of an ISA network is illustrated
in Figure 20.8a.
The output of the ISA network is:
pi = (x; W, V) =





m

k=1
V ik
⎛
⎝
n

j=1
Wkj x j
⎞
⎠
2
.
(20.35)
Similar to the sparse auto-encoding network, the ISA network learns the structure of the image by
ﬁnding the sparse representation in the hidden layer by solving:
min
W
T

t=1
m

i=1
pi(xt; W, V),
(20.36)
subject WW T = I,

1138
CHAPTER 20 Clustering
FIGURE 20.8a
ISA structure (from [34]).
FIGURE 20.8b
ISA response with image changes. These plots show that the ISA network is robust to translation (phase)
and selective to frequency and rotation changes.
where {xt} are whitened input examples, W ∈Rk×n represents the weights connecting the input layer
to the hidden layer, and V ∈Rm×k represents the weights connecting the hidden layer to the output
layer. V often is ﬁxed in order to force the sparse representation of x, and the orthogonal constraint
ensures that feature learning is diverse (see Figure 20.8b).
The features learned by the ISA are invariant to translation, frequency and rotation and therefore are
suitable for recognition tasks [114]. Many experiments have conﬁrmed that ISA performs much better
than sparse coding. However, to further extract features from large images or videos, those networks
that auto-learn features can be stacked to form hierarchical networks.
After training the network, any image can be fed into it, and the output at both layers can be used
as features for clustering. To process videos, features are extracted for each frame of the video and
then quantized into visual words [115] using K-means clustering. A video is represented as a frequency
histogram over those visual words by assigning features to the closest vocabulary word.

1.20.3 Clustering Validation
1139
Based on unsupervised feature learning, several clustering applications have been developed, includ-
ing speech processing. Lee et al. [116], action recognition [34], and character recognition [117].
1.20.3 Clustering validation
As Figure 20.1 depicted, the indispensable step in clustering is validation. Clustering validation is used
to objectively and quantitatively evaluate the obtained clustering structure. There are three validation
index categories: external indices, internal indices and relative indices [7].
Given a data set X and a clustering structure C obtained from a certain clustering algorithm on X,
external criteria were used to compare C to a pre-speciﬁed structure containing a priori information
about the data structure X. In contrast, internal criteria were used to evaluate the clustering structure
exclusively from X. Relative criteria were used to compare C with other clustering structures from
different clustering algorithms or from the same clustering algorithm with different parameters on X.
The primary interest regarding relative criteria is how to estimate the optimal number of clusters. Xu
and Wunsch [5] discussed those criteria in detail.
Another problem closely related to cluster validation is the assessment of clustering tendencies, in
which the algorithm tries to determine whether clusters are present one step prior to actual clustering.
Most assessment techniques utilize data visualization to present a view of the data structure. Yang et
al. [96] proposed an interactive hierarchical display by which to look at the data at various levels. One
of the most popular approaches comes from [118], in which the data are reordered according to their
similarity, resulting in an intensity image display. This method has produced several derivatives, such
as iVAT and reVAT [119,120].
1.20.4 Applications
1.20.4.1 Image segmentation
One of the broadest applications of clustering in image processing is image segmentation. The simplest
approach to segmenting an image is using hierarchical clustering [121]. K-means can also be applied
for segmentation. In many case in which segments are not connected and can be very widely scattered,
the feature vector in k-means will include pixel coordinates instead of just pixel colors. Segmentation
also can be regarded as a graph cut problem in which spectral clustering has been developed, yielding
very good results [74,75].
1.20.4.2 MRI images
Magnetic resonance imaging (MRI) provides a visualization of the internal structures of objects and
living organisms. MRI images have better contrast than computerized tomography; therefore, most
medical image segmentation research uses MRI images. Segmenting an MRI image is a key task in
many medical applications, such as surgical planning and abnormality detection. MRI segmentation
aims to partition an input image into signiﬁcant anatomical areas, each of which is uniform according
to certain image properties.

1140
CHAPTER 20 Clustering
MRI segmentation can be formulated as a clustering problem in which a set of feature vectors
obtained through transformation image measurements and pixel positions is grouped into a number of
structures [14]. There are signiﬁcantly fewer clusters than intensity levels in the original image, so this
clustering process can be regarded as removing redundant information from the MRI.
Balafar [122] used FCM for MRI segmentation. As FCM only considers image intensity, its results
are not acceptable when working with noisy images. Many algorithms have been introduced to make
FCM robust against noise [123,124].
Another approach to image segmentation is to model each of the segments as a Gaussian distribution
and apply mixture density-based clustering to the MRI images [125].
1.20.4.3 Vision-guided robot
One of the main problems in robotics is enabling the robot to guide itself autonomously through the
environment. A vision-guided robot equipped with a camera can attain that goal by clustering the images
captured from the environment. Martinez and Vitria [126] found a successful solution, developing a
normal mixture model-based (NMM) algorithm to cluster the images of the surrounding environment.
This algorithm views images as column vectors whose dimensions are the intensities of the image pixels;
this is the ﬁrst clustering method projected to lower the dimensional space, which it accomplishes using
principal component analysis. EM clustering then is applied to learn all the models in the mixtures in
the PCA space. Each model corresponds to a different area of the environment. In order to improve
the results, they employed a Genetic Algorithm that uses a population of mixture models rather than
a single mixture. The resulting algorithm has a higher successful recognition rate than PCA or Fisher
discriminant analysis.
1.20.4.4 Motion clustering
One application of subspace clustering is motion clustering, in which rigid objects in a given video
are segmented based on their motions in the scene. Features in the same object will have the same
motion pattern with 3 degrees of freedom in space. Under an afﬁne projection model, this motion
was positioned in an afﬁne subspace of 3 dimensions at most, i.e., in a linear 4-dimensional subspace.
More speciﬁcally, suppose over F frames of the video, we successfully extract and track a set of N
feature points {x f i ∈R2}i = 1 . . . N, f = 1 . . . F. The set of {x f i ∈R2}, f = 1 . . . F forms the
trajectory; by stacking them, we obtain a data point yi in 2F-dimension. As noted previously, data point
lies in a subspace of, at most, 3 dimensions. The problem of motion segmentation then becomes the
problem of clustering those data points yi in R2F, which is the problem of subspace clustering. Vidal
[127] contributed a signiﬁcant amount of research exploring the use of different subspace clustering
techniques on this application. Experiments were tested on the Hopkins 155 dataset, showing that sparse
subspace clustering outperforms other subspace clustering algorithms.
1.20.5 Open issues and problems
Most clustering algorithms are available through software packages or are easy to implement. While
offering substantial merits, they also have many disadvantages that make them inappropriate for some

1.20.6 Conclusion
1141
signal and image processing needs. For example, the complexity of agglomerative hierarchical clustering
is O(N 2), which is not suitable for large-scale data clustering.
The typical clustering challenges are summarized as follows:
Scalability: Scalability in both time and memory is the most important challenge faced by clustering
algorithms because of the ever-increasing amount of data from all applications. For example, some
applications require the clustering of billions of images or documents available on the Internet. It
is impractical for a clustering algorithm to run for several months to complete one run, especially
if there is an extraordinary need for memory. Therefore, linear or near-linear complexity is very
desirable.
Dimensionality: The algorithm should be able to handle data with many features, even more than the
number of objects in the data set. Identifying relevant features and capturing the intrinsic dimension
is important for describing the real data structure.
Robustness: Noise and outliers are always present in the data, so the clustering algorithm must be
able to detect and remove them.
Cluster number: A classical clustering problem is to correctly identify the numbers of clusters, as
many current algorithms require the user to input this parameter. This is difﬁcult because a lack of
prior knowledge or an incorrect estimation of the true number of clusters will prevent the clustering
process from uncovering the real clustering structures.
Parameter reliance: Many algorithms, not just limited to clustering algorithms, require users to
specify 3 or more parameters, which can decrease a user’s conﬁdence in utilizing the algorithm
unless he knows the problem well enough to select these values within a sensible range. Thus, a
good clustering algorithm should depend on only one parameter.
Cluster shape: Clustering algorithms should have the ability to detect all possible cluster shapes,
rather than being conﬁned to some particular shape, such as hyperspheres or hyper-rectangles.
Order insensitivity: One of the most important properties of a clustering algorithm, especially
when being applied to large-scale data, is its online or incremental ability. However, the result of
clustering may depend on the order of the presentation of input patterns. Decreasing the effects of
the order of input patterns is a challenging problem.
Good visualization: Good visualization will help users understand the resulting clusters, allowing
them to interpret and extract useful information to solve their problems.
1.20.6 Conclusion
This chapter has offered a review of emerging clustering algorithms with broad applications in computer
science, machine learning, and statistics. While some of the clustering techniques, such as hierarchical,
mixture, and fuzzy clustering, are suitable for typical data, biclustering and subspace clustering are
very effective for high-dimensional datasets. To automatically learn features from a dataset, unsuper-
vised feature learning clustering techniques may offer superior performance to hand-crafted feature
selection.

1142
CHAPTER 20 Clustering
Glossary
ART (Adaptive Resonance
Theory)
an unsupervised learning model in which learning occurs as a result
of interaction between top-down observer expectations and bottom-up
sensory information through a reset module, resulting in a theory that
learning is mediated by resonance in a neural circuit
Afﬁnity matrix
in the context of spectral clustering, an N×N matrix measures the afﬁn-
ity between data points. The measuring function is usually a Gaussian
function
BIC (Bayesian Information
Criterion)
a criterion for model selection when ﬁtting data to a model, used to
mitigate the problem of over ﬁtting
Biclustering
a clustering approach that clusters the matrix of a data set in both columns
and rows. Many clustering algorithms tessellate the data space. Bicluster-
ing selects data that is common to two subspaces, and can be considered
as an unsupervised version of heteroassociative learning
Cluster
a group of objects that share some common property
Clustering
the process of dividing objects into clusters
Cluster validation
a post-processing step in clustering that objectively and qualitatively
evaluates the resulting clusters
Competitive learning neuron
network
a network in which the output nodes compete for the right to respond to
a subset of the input data
Dendogram
a binary tree used to illustrate the arrangement of the clusters produced
by hierarchical clustering
Dissimilarity
a proximity measurement of the difference between two data points
Eigenvalues, eigenvectors
in a square matrix, a non-zero vector when multiplied by the matrix yields
a vector that lies parallel to the original
Expectation Maximization
(EM)
an iterative method that ﬁnds the maximum likelihood of the estimate
of parameters in statistical models, where the model has some latent
variables. An iteration performs the Expectation step, which creates a
function evaluating the expectation of the log-likelihood of the current
parameters, and a Maximization step, which computes the parameters,
thus maximizing the function in the Expectation step
Fuzzy clustering
a type of clustering in which the data point can belong to many different
clusters denoted by its membership
Fuzzy c-means (FCM)
a partition-based clustering method in which a data point can belong to
two or more clusters. The procedure is very similar to k-means clustering

1.20.6 Conclusion
1143
Gaussian mixture
a probabilistic model in which a whole probability distribution is repre-
sented by the summation of several Gaussian distributions
Hard clustering
a type of clustering in which the data point can only belong to one cluster
Hierarchical clustering
a type of clustering that organizes data into hierarchical structures based
on a proximity matrix
Image segmentation
the process of assigning a label to every pixel in an image such that pixels
with the same label have the same visual characteristics
Independent component
analysis
a method of ﬁnding a linear representation of non-Gaussian data so that
the components are statistically independent
Kernel method
an approach in which a kernel function is used to map the data into a
higher dimensional feature space and which allows a variety of cluster-
ing methods. The kernel functions allow computation through an inner
product rather than on the coordinates of the data, thus resulting in a
lower computational requirement
K-means
a classical clustering method of partitioning data into k clusters in which
each data point belongs to the nearest mean
K-medoids
a process similar to k-means except that the mean is replaced by a data
point as the center. K-medoids works with an arbitrary distance and is
not limited to L2
L1-norm
the sum of the absolute value of the components in a vector. The L1-norm
is used widely in minimizing linear underestimated equations because
of its property of sparse solutions
Large-scale data clustering
a special clustering problem in which the amount of data is extremely
large (Big Data)
Learning rate
a parameter controlling the adjustments made during the training process.
If the learning rate is large, the algorithm learns quickly; otherwise, the
result in slow learning
Mahalanobis distance
a distance measure that takes into account the covariance among the
dimensions
Maximum Likelihood
Estimation (MLE)
A method of estimating the parameters of a statistical model in which
the parameters are chosen to maximize the joint likelihood function
Online learning
a learning algorithm in which learning occurs sequentially as the data
are introduced
Outlier
in statistics, a sample that lies far away from the rest of the data
Partitional clustering
as opposed to hierarchical clustering, this is the process of partitioning
data into k clusters, which often is based on some optimized criterion
function
Possibilistic c-means
a generalization of FCM, in which the membership of a data point in one
cluster has nothing to do with other cluster. Membership is interpreted
here as the degree of compatibility with the cluster prototype

1144
CHAPTER 20 Clustering
Principle component analysis a procedure that uses orthogonal transformation to extract a set of values
of linearly uncorrelated variables called principle components
Prototype
a representation of a cluster, usually the means of data points in that
cluster
Rand index
an external criterion for cluster validation
Self-organizing feature map
a neuron network trained using unsupervised learning to produce a low-
dimensional, discrete representation of the training samples
Semi-supervised clustering
a type of clustering in which data are both labeled and unlabeled
Spectral clustering
a clustering technique making use of eigenvectors to perform dimension
reduction before clustering in fewer dimensions
Subspace clustering
a type of clustering in which each cluster is a subspace
Unsupervised learning
a learning algorithm in which the data are not labeled
Vector quantization
a variant of k-means used in communications problems such as optimiz-
ing analog to digital conversion.
Vigilance parameter
a parameter that controls adaptation in ART
Relevant Theory: Signal Processing, Machine Learning
See this Volume, Chapter 15 Neuron Networks
See this Volume, Chapter 21 Unsupervised Learning and Latent Variable Models
See this Volume, Chapter 22 Semi-supervised Learning
References
[1] A. Baraldi, E. Alpaydin, Constructive feedforward ART clustering networks – Part I and II, IEEE Trans.
Neural Networks 13 (2002) 645–677.
[2] V. Cherkassky, F. Mulier, Learning from Data: Concepts, Theory, and Methods, John Wiley & Sons, Inc.,
1998.
[3] E. Backer, A. Jain, A clustering performance measure based on fuzzy set decomposition, IEEE Trans. Pattern
Anal. Mach. Intell. 3 (1981) 66–75.
[4] B. Everitt, S. Landau, M. Leese, Cluster Analysis, Arnold, 2001.
[5] R. Xu, D. Wunsch, II, Wiley-IEEE Press, Clustering, 2009.
[6] A. Jain, R. Dubes, Algorithms for Clustering Data, Prentice Hall, Englewood Cliffs, 1988.
[7] A. Gordon, Classiﬁcation, second ed., Chapman and Hall/CRC Press, 1999.
[8] P. Hansen, B. Jaumard, Cluster analysis and mathematical programming, Math. Program. 79 (1997) 191–215.
[9] J. Hartigan, Clustering Algorithms, John Wiley & Sons, Inc., 1975.
[10] R. Duda, P. Hart, D. Stork, Pattern Classiﬁcation, second ed., John Wiley & Sons, Inc., 2001.
[11] H. Späth, Cluster Analysis Algorithms, Ellis Horwood Ltd, 1980.
[12] B. Duran, P. Odell, Cluster Analysis: A Survey, Springer-Verlag, 1974.
[13] M. Anderberg, Cluster Analysis for Applications, Academic Press, 1973.
[14] A. Jain, M. Murty, P. Flynn, Data clustering: a review, ACM Comput. Sur. 31 (1999) 264–323.
[15] E.
Kolatch,
Clustering
algorithms
for
spatial
databases:
a
survey,
2001,
Available
from:
<http://citeseer.nj.nec.com/436843.html>.
[16] Q. He, A review of clustering algorithms as applied to IR, University of Illinois, Urbana-Champaign, 1999.

References
1145
[17] P. Berkhin, Survey of clustering data mining techniques, 2001, Available from:
<http://www.accrue.com/products/rpclusterreview.pdf>, cached at
<http://citeseer.nj.nec.com/berkhin02survey.html>.
[18] F. Murtagh, A survey of recent advances in hierarchical clustering algorithms, Comput. J. 26 (1983) 354–359.
[19] A. Baraldi, P. Blonda, A survey of fuzzy clustering algorithms for pattern recognition – Part I and II, IEEE
Trans. Syst. Man Cybern. Part B Cybern. 29 (1999) 778–801.
[20] A. Baraldi, L. Schenato, Soft-to-Hard Model Transition in Clustering: A Review, International Computer
Science Institute, Berkeley, CA, 1999, pp. TR-99–010.
[21] H. Bock, Probabilistic models in cluster analysis, Computational Statistics & Data Analysis 23 (1996) 5–28.
[22] R. Dubes, Cluster Analysis and Related Issue, World Science Publishing Company, 1993.
[23] D. Fasulo, An analysis of recent work on clustering algorithms, University of Washington, Department of
Computer Science & Engineering, 1999.
[24] A. Jain, R. Duin, J. Mao, Statistical pattern recognition: a review, IEEE Trans. Pattern Anal. Mach. Intel. 22
(2000) 4–37.
[25] A. Rauber, J. Paralic, E. Pampalk, Empirical evaluation of clustering algorithms, J. Inform. Organ. Sci. 24
(2000) 195–209.
[26] C. Wei, Y. Lee, C. Hsu, Empirical comparison of fast clustering algorithms for large data sets, in: Proceedings
of 33rd Hawaii International Conference on System Sciences, Maui, Hawaii, 2000.
[27] P. Scheunders, A comparison of clustering algorithms applied to color image quantization, Pattern Recog-
nition Letter 18 (1997) 1379–1384.
[28] R. Shamir, R. Sharan, Algorithmic approaches to clustering gene expression data, in: T. S. T. Jiang, Y. Xu,
M. Zhang (eds.), Current Topics in Computational Molecular Biology, MIT Press, 2002, pp. 269–300.
[29] R. Tibshirani, T. Hastie, M. Eisen, D. Ross, D. Botstein, P. Brown, Clustering methods for the analysis of
DNA microarray data, Technical Report, Department of Statistics, Stanford University, 1999.
[30] M. Steinbach, G. Karypis, V. Kumar, A comparison of document clustering techniques, in: KDD Workshop
on Text Mining, 2000.
[31] S.C. Madeira, A.L. Oliveira, Biclustering algorithms for biological data analysis: a survey, IEEE trans.
comput. biol. bioinf. 1 (2004) 24–45.
[32] A. Tanay, R. Sharan, R. Shamir, Biclustering Algorithms: A Survey, in: E. b. A. S. Chapman (ed.), Handbook
of Computational Molecular Biology, CRC Computer and Information Science Series, 2005.
[33] L. Parsons, E. Haque, H. Liu, Subspace clustering for high dimensional data: a review, SIGKDD Explor.
Newsl. 6 (2004) 90–105.
[34] Q. V. Le, W. Y. Zou, S. Y. Yeung, A. Y. Ng, Learning hierarchical invariant spatio-temporal features for
action recognition with independent subspace analysis, in: Proc. IEEE Conf. Computer Vision and Pattern
Recognition, CVPR, 2011, pp. 3361–3368.
[35] R. Xu, D. Wunsch, Survey of clustering algorithms II, IEEE Trans. Neural Networks 16 (2005) 645–678.
[36] R. Xu. D.C. Wunsch, Clustering algorithms in biomedical research: a review, IEEE Rev. Biomed. Eng. 3
(2010) 120–154.
[37] R. Xu, D. Wunsch, BARTMAP: A viable structure for biclustering, Neural Netw. 24 (2011) 709–716.
[38] H. Sharp Jr, Cardinality of ﬁnite topologies, J. Combinatorial Theory 5 (1968) 82–86.
[39] G. Lance, W. Williams, A general theory of classiﬁcation sorting strategies: 1, Hierarchical systems, Comput.
J. 9 (1967) 373–380.
[40] R. Yager, Intelligent control of the hierarchical agglomerative clustering process, IEEE Trans. Syst. Man
Cybern. Part B Cybern. 30 (2000) 835–845.
[41] S. Guha, R. Rastogi, K. Shim, CURE: an efﬁcient clustering algorithm for large databases, in: Proceedings
of ACM SIGMOD International Conference on Management of Data, 1998, pp. 73–84.

1146
CHAPTER 20 Clustering
[42] S. Guha, R. Rastogi, K. Shim, ROCK: a robust clustering algorithm for categorical attributes, Inform. Syst.
25 (2000) 345–366.
[43] R. Mollineda, E. Vidal, A relative approach to hierarchical clustering, in: E. M. Torres, A. Sanfeliu (ed.),
Pattern Recognition and Applications, Frontiers in Artiﬁcial Intelligence and Applications. vol. 56, IOS
Press, Amsterdam, 2000, pp. 19–28.
[44] Y. Leung, J. Zhang, Z. Xu, Clustering by scale-space ﬁltering, IEEE Transactions on Pattern Analysis and
Machine Intelligence 22 (2000) 1396–1410.
[45] C. Li, G. Biswas, Unsupervised learning with mixed numeric and nominal data, IEEE Trans. Knowl. Data
Eng. 14 (2002) 673–690.
[46] C. Olson, Parallel algorithms for hierarchical clustering, Parallel Comput. 21 (1995) 1313–1325.
[47] E. Dahlhaus, Parallel algorithms for hierarchical clustering and applications to split decomposition and parity
graph recognition, J. Algorithms 36 (2000) 205–240.
[48] S. Theodoridis, K. Koutroumbas, Pattern Recognition, Acedemic Press, 2009.
[49] A. Gersho, R. Gray, Vector Quantization and Signal Compression, Kluwer Academic Publishers, 1992.
[50] J. Bezdek, Pattern Recognition with Fuzzy Objective Function Algorithms, Plenum Press, New York, 1981.
[51] R. Yager, D. Filev, Approximate clustering via the mountain method, IEEE Trans. Syst. Man Cybern. Part
B Cybern. 24 (1994) 1279–1284.
[52] I. Gath, A. Geva, Unsupervised optimal fuzzy clustering, IEEE Trans. Pattern Anal. Mach. Intel. 11 (1989)
773–781.
[53] R. Hathaway, J. Bezdek, Y. Hu, Generalized fuzzy c-Means clustering strategies using Lp norm distances,
IEEE Trans. Fuzzy Syst. 8 (2000) 576–582.
[54] R. Krishnapuram, J. Keller, A possibilistic approach to clustering, IEEE Trans. Fuzzy Syst. 1 (1993) 98–110.
[55] M. Figueiredo, A. Jain, Unsupervised learning of ﬁnite mixture models, IEEE Transactions on Pattern
Analysis and Machine Intelligence, 24 (2002) 381–396.
[56] G. McLachlan, T. Krishnan, The EM Algorithm and Extensions, Wiley, New York, 1997.
[57] G. Celeux, G. Govaert, A classiﬁcation EM algorithm for clustering and two stochastic versions, Comput.
Stat. Data Anal. 14 (1992) 315–332.
[58] C.
Fraley,
A.
Raftery,
Model-based
clustering,
discriminant
analysis,
and
density
estimation,
J. Am. Stat. Assoc. 97 (2002) 611–631.
[59] X. Zhuang, Y. Huang, K. Palaniappan, Y. Zhao, Gaussian mixture density modeling, decomposition, and
applications, IEEE Trans. Image Process. 5 (1996) 1293–1302.
[60] P. Cheeseman, J. Stutz, Bayesian classiﬁcation (AutoClass): theory and results, in: G. P.-S. U. Fayyad, P.
Smyth, R. Uthurusamy (eds.), Advances in Knowledge Discovery and Data Mining, AAAI Press, 1996, pp.
153–180.
[61] T. Kohonen, The self-organizing map, in: Proceedings of the IEEE, 1990, pp. 1464–1480.
[62] T. Kohonen, Self-Organizing Maps, Springer, 2001.
[63] S. Haykin, Neural networks: A Comprehensive Foundation, second ed., Prentice Hall, 1999.
[64] M. Su, H. Chang, Fast self-organizing feature map algorithm, IEEE Trans. Neural Networks 11(3) (2000)
721–733.
[65] J. Vesanto, E. Alhoniemi, Clustering of the self-organizing map, IEEE Trans. Neural Networks, 11 (3) (2000)
586–600.
[66] G. Carpenter, S. Grossberg, ART2: self-organization of stable category recognition codes for analog input
patterns, Applied Optics. (1987) 26.
[67] G. Carpenter, S. Grossberg, A massively parallel architecture for a self-organizing neural pattern recognition
machine, Computer Vision, Graphics, and Image Processing 37 (1987).
[68] G. Carpenter, S. Grossberg, J. Reynolds, ARTMAP: supervised real-time learning and classiﬁcation of
nonstationary data by a self-organizing neural network, IEEE Trans. Neural Networks 4 (1991) 169–181.

References
1147
[69] B. Moore, ART1 and pattern clustering, in: Proceedings of the 1988 Connectionist Models
Summer School, 1989.
[70] M.G.J. Huang, G. Heileman, Fuzzy ART properties, IEEE Trans. Neural Networks 8 (2) (1995) 203–213.
[71] J. Williamson, Gaussian ARTMAP: a neural network for fast incremental learning of noisy multidimensional
maps, IEEE Trans. Neural Networks 9 (5) (1996) 881–897.
[72] G. Anagnostopoulos, M. Georgiopoulos, Hypersphere ART and ARTMAP for unsupervised and super-
vised incremental learning, in: Proceedings of the IEEE-INNS-ENNS Intl. Joint Conf. on Neural Networks
(IJCNN’00), 2000.
[73] G. Anagnostopoulos, M. Georgiopoulos, Ellipsoid ART and ARTMAP for incremental unsupervised and
supervised learning, in: presented at the Proceedings of the IEEE-INNS-ENNS Intl. Joint Conf. on Neural
Networks, 2001.
[74] A. Ng, M. Jordan, Y. Weiss, On Spectral Clustering: analysis and an algorithm, in: Advances in Neural
Information Processing Systems, 2001, pp. 849–856.
[75] S. Jianbo, Normalized Cuts and Image Segmentation, IEEE Transactions on Pattern Analysis and Machine
Intelligence 22 (2000) 888–905.
[76] U. Luxburg, A tutorial on spectral clustering, Stat. Compu. 17 (2007) 395–416.
[77] Y. Cheng, G. Church, Biclustering of expression data, in: Presented at the Proceedings of Eighth International
Conference on Intelligent Systems for, Molecular Biology, 2000.
[78] S. Busygin, O. Prokopyev, P. Pardalos, Biclustering in data mining, Comput. Oper. Res. 35 (2008) 2964–2987.
[79] S. Madeira, M. Teixeira, I. Sá-Correia, A. Oliveira, Identiﬁcation of regulatory modules in time series gene
expression data using a linear time biclustering algorithm, IEEE/ACM Trans. Comput. Biol. Bioinf. 7 (2010)
153–165.
[80] R. Vidal, A tutorial on subspace clustering, IEEE Signal Process. 2010.
[81] J. Costeira, T. Kanade, A multibody factorization method for independently moving objects., Int. J. Comput.
Vision 29 (1998) 159–179.
[82] C.W. Gear, Multibody grouping from motion images, Int. J. Comput. Vision 29 (1998) 133–150.
[83] P.S. Bradley, O.L. Mangasarian, k-plane clustering, J. of Global Optim. 16 (2000) 23–32.
[84] M. Tipping, C. Bishop, Probabilistic principal component analysis, J. Roy. Stat. Soc. 61 (1999) 611–622.
[85] M. Fischler, R. Bolles, Random sample consensus: a paradigm for model ﬁtting with applications to image
analysis and automated cartography, Commun. ACM 24 (1981) 381–395.
[86] J. Yan, M. Pollefeys, Articulated motion segmentation using RANSAC with priors, in: Presented at the
Workshop on Dynamical Vision, 2005.
[87] L. Donoho, Compressed Sensing, IEEE Trans. Information Theory 52 (2006) 1289–1306.
[88] E. Elhamifar, R. Vidal, Sparse subspace clustering, in: Presented at the IEEE Conference on Computer Vision
and, Pattern Recognition, 2009.
[89] E.
Elhamifar,
R.
Vidal,
Sparse
Subspace
Clustering,
Algorithm,
Theory,
and
Applications,
arXiv:1203.1005v1, 2012.
[90] C. Tang, A. Zhang, Interrelated two-way clustering and its application on gene expression data, Int. J. Artif.
Intell. Tools 14 (2005) 577–597.
[91] G. J. S. Busygin, E. Kramer, Double conjugated clustering applied to leukemia microarray data, in: SIAM
Data Mining Workshop on Clustering High Dimensional Data and Its Applications, 2002.
[92] G. Getz, H. Gal, I. Kela, D. Notterman, E. Domany, Coupled two-way clustering analysis of breast cancer
and colon cancer gene expression data, Bioinformatics 1 (2003) 1079–1089.
[93] J. Shaik and M. Yeasin, Fuzzy-adaptive-subspace-iteration-based two way clustering of microarray data,
IEEE/ACM Trans. Comput. Biol. Bioinf. 6 (2009) 244–259.
[94] J. Hartigan, Direct clustering of a data matrix, Journal of American Statistical Association 67 (1972) 123–129.

1148
CHAPTER 20 Clustering
[95] D. Duffy, A. Quiroz, A permutation based algorithm for block clustering, Journal of Classiﬁcation 8 (1991)
65–91.
[96] J. Yang, W. Wang, H. Wang, P. Yu, Enhanced biclustering on expression data, in: Proceedings of the Third
IEEE Conference on Bioinformatics and, Bioengineering, 2003.
[97] J. Yang, M.O. Ward, E.A. Rundensteiner, Interactive hierarchical displays: a general framework for visual-
ization and exploration of large multivariate data sets, Comput. Graphics 27 (2003) 265–283.
[98] G. Babu, M. Murty, A near-optimal initial seed value selection in K-Means algorithm using a genetic
algorithm, Pattern Recognition Letters 14 (1993).
[99] J. G. K. Beyer, R. Ramakrishnan, U. Shaft, When is nearest neighbor meaningful, in: Proceedings of 7th
International Conference on Database Theory, 1999, pp. 217–235.
[100] L. Lazzeroni, A. Owen, Plaid models for gene expression data, Statistica Sinica, 6 (2002).
[101] F. Divina, J. Aguilar-Ruiz, Biclustering of expression data with evolutionary computation, IEEE Trans.
Knowl. Data Eng 18 (2006) 590–602.
[102] Z. L. J. Liu, X. Hu, Y. Chen, Biclustering of microarray data with MOSPO based on crowding distance,
BMC Bioinformatics, 10 (Suppl. 4) (2009) S9.
[103] H. Turner, T. Bailey, W. Krzanowski, C, Hemingway, Biclustering models for structured microarray data,
IEEE/ACM Transactions on Computational Biology and Bioinformatics, 2005, pp. 316–329.
[104] Gu, J. Liu, Bayesian biclustering of gene expression data, BMC Genomics 9 (2007).
[105] P. DiMaggio Jr., S. McAllister, C. Floudas, X. Feng, J. Rabinowitz, H. Rabitz, Biclustering via optimal
re-ordering of data matrices in systems biology: rigorous methods and comparative studies, BMC Bioinf. 9
(1) (2008) 458.
[106] A. Preli´c, S. Bleuler, P. Zimmermann, A. Wille, P. Bühlmann, W. Gruissem, L. Hennig, L. Thiele, E. Zitzler,
A systematic comparison and evaluation of biclustering methods for gene expression data, Bioinformatics
22 (2006) 1122–1129.
[107] A. Tchagang, A. Tewﬁk, M. DeRycke, K. Skubitz, A. Skubitz, Early detection of ovarian cancer using group
biomarkers, Mol. Cancer Ther. 7 (1) (2008) 27–37.
[108] R. Mitra, S. Bandyopadhyay, U. Maulik, M. Zhang, SFSS Class: an integrated approach for miRNA based
tumor classiﬁcation, BMC Bioinformatics 11 (Suppl 1) (2010) S22.
[109] N. Dalal, B. Triggs, Histograms of oriented gradients for human detection, in: Proc. IEEE Computer Society
Conf. Computer Vision and Pattern Recognition CVPR, 2005, pp. 886–893.
[110] D. G. Lowe, Object recognition from local scale-invariant features, in: The Proc. Seventh IEEE Int Computer
Vision Conf., 1999, pp. 1150–1157.
[111] B. Olshausen, D. Field, Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for
natural images, Nature 381 (1996) 607–609.
[112] P. Werbos, Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences, Harvard
University, 1974.
[113] S. Kullback, R. A. Leibler, On Information and Sufﬁciency, Annals of Mathematical Statistics, 1951, 79–86.
[114] I. Goodfellow, Q. Le, A. Saxe, H. Lee, A. Ng, Measuring invariances in deep networks, in NIPS, 2010.
[115] H. Wang, M. M. Ullah, A. Klaser, I. Laptev, C. Schmid, Evaluation of local spatio-temporal features for
action recognition, in: BMVC, 2010.
[116] H. Lee, Y. Largman, P. Pham, A. Y. Ng, Unsupervised feature learning for audio classiﬁcation using con-
volutional deep belief networks, in: Advances in Neural Information Processing Systems 22, 2009, pp.
1096–1104.
[117] A. Coates, B. Carpenter, C. Case, S. Satheesh, B. Suresh, T. Wang, A. Y. Ng, Text detection and character
recognition in scene images with unsupervised feature learning, in: Proceedings of the 11th International
Conference on Document Analysis and Recognition, 2011.

References
1149
[118] J.C. Bezdek, R.J. Hathaway, J.M. Huband, Visual assessment of clustering tendency for rectangular dissim-
ilarity matrices, IEEE Trans. Fuzzy Syst. 15 (2007) 890–903.
[119] T.C. Havens, J.C. Bezdek, An efﬁcient formulation of the improved visual assessment of cluster tendency
(iVAT) algorithm, IEEE Trans. Knowl. Data Eng. 24 (2012) 813–822.
[120] J. M. Huband, J. C. Bezdek, R. J. Hathaway, Revised visual assessment of (cluster) tendency (reVAT), in:
Fuzzy Information, IEEE Annual Meeting of the Processing NAFIPS ’04, vol. 1, 2004, pp. 101–104.
[121] D. Forsyth, J. Ponce, Computer vision: a modern approach: Prentice Hall, 2002.
[122] M. Balafar, Medical image segmentation using fuzzy C-mean (FCM), Bayesian method and user interaction,
in: International Conference on Wavelet Analysis and, Pattern Recognition, 2008, pp. 68–73.
[123] D. Q. Zhang, S. C. Chen, A novel kernelized fuzzy c-means algorithm with application in medical image
segmentation, Artif. Intel. Med. vol. 32 (2004) 37–50.
[124] M. Balafar, MRI segmentation of medical images using FCM with initialized class centers via genetic
algorithm, in: International Symposium on Information Technology, 2008, pp. 1–4.
[125] A. Diplaros, N. Vlassis, T. Gevers, A spatially constrained generative model and an EM algorithm for image
segmentation., IEEE Trans. Neural Netw. 18 (2007) 798–808.
[126] A. Martinez, J. Vitria, Clustering in image space for place recoginition and visual annotations for human-robot
interaction, IEEE Trans. syst. Man Cyber. B 31 (2001) 669–682.
[127] R. Vidal, Subspace Clustering, IEEE Signal Process. Mag. 28 (2011) 52–68.

21
CHAPTER
Unsupervised Learning Algorithms
and Latent Variable Models:
PCA/SVD, CCA/PLS, ICA, NMF, etc.
Andrzej Cichocki
Laboratory for Advanced Brain Signal Processing, RIKEN, Brain Science Institute,
Wako-shi, Saitama 3510198, Japan
1.21.1 Introduction and of problems statement
Matrix factorizations and their extensions to tensor (multiway arrays) factorizations and decomposi-
tions have become prominent techniques for Latent Variable Models (LVM), linear and Multiway or
Multilinear Blind Source Separation (MBSS), and (multiway) Generalized Component Analysis (GCA)
(especially, multilinear Independent Component Analysis (ICA), Nonnegative Matrix and Tensor Fac-
torizations (NMF/NTF), Smooth Component Analysis (SmoCA), and Sparse Component Analysis
(SCA). Moreover, matrix and tensor decompositions have many other applications beyond blind source
separation, especially for feature extraction, classiﬁcation, dimensionality reduction and multiway clus-
tering. The recent trends in MBSS and GCA are to consider problems in the framework of matrix and
tensor factorizations or more general multi-dimensional data for signal decomposition within proba-
bilistic generative models and exploit a priori knowledge about the true nature, diversities, morphology
or structure of latent (hidden) variables or sources such as spatio-temporal-spectral decorrelation, sta-
tistical independence, nonnegativity, sparseness, smoothness or lowest possible complexity. The goal of
MBSS can be considered as estimation of true physical sources and parameters of a mixing system, while
the objective of GCA is to ﬁnd a reduced (or hierarchical and structured) component representation for
the observed (sensor) multi-dimensional data, that can be interpreted as physically or physiologically
meaningful coding or blind signal decomposition. The key issue is to ﬁnd such a transformation (or cod-
ing) which has true physical meaning and interpretation. In this chapter we brieﬂy review some efﬁcient
unsupervised learning algorithms for linear and multilinear blind source separation, blind source extrac-
tion and blind signals decomposition, using various criteria, constraints and assumptions. Moreover,
we brieﬂy overview emerging models and approaches for constrained matrix/tensor decompositions
with applications to group and linked multilinear BSS/ICA, feature extraction, multiway Canonical
Correlation Analysis (CCA), and Partial Least Squares (PLS) regression problems.
Although the basic models for matrix and tensor (i.e., multiway array) decompositions and factoriza-
tions, such as Tucker and Canonical Polyadic (CP) decomposition models, were proposed long a time
ago [1–7], they have recently emerged as promising tools for the exploratory analysis of large-scale
and multi-dimensional data in diverse applications, especially, for dimensionality reduction, multilinear
independent component analysis, feature extraction, classiﬁcation, prediction, and multiway clustering
[5,8–12]. By virtue of their multiway nature, tensors constitute powerful tools for the analysis and fusion
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00021-8
© 2014 Elsevier Ltd. All rights reserved.
1151

1152
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
of large-scale, multi-modal massive data, together with a mathematical backbone for the discovery of
underlying hidden complex data structures [8,13].
The problem of separating or extracting source signals from a sensor array, without knowing the
transmission channel characteristics and the sources, is addressed by a number of related BSS or
GCA methods such as ICA and its extensions: Topographic ICA, Multilinear ICA, Kernel ICA, Tree-
dependent Component Analysis, Multi-resolution Subband Decomposition ICA [13–18], Non-negative
Matrix Factorization (NMF) [19,20], Sparse Component Analysis (SCA) [21–25], and Multi-channel
Morphological Component Analysis (MCA) [26,27].
The mixing and ﬁltering processes of the unknown input sources x j(t) ( j = 1, 2, . . . , J) may
correspond to different mathematical or physical models, depending on the speciﬁc applications [17,28].
Most linear BSS models can be expressed algebraically in their simplest forms as some speciﬁc form
of a constrained matrix factorization: Given observation (often called sensor or input) data matrix
Y = [yit] = [y(1), . . . , y(T )] ∈RI×T , we perform the matrix factorization (see Figure 21.1a):
Y = AX + E = ABT + E,
(21.1)
where A ∈RI×J represents the unknown mixing matrix, E ∈RI×T is an unknown matrix representing
errors or noises, X = BT = [x(1), x(2), . . . , x(T )] ∈RJ×T contains the corresponding latent (hidden)
components that give the contribution of each basis vector, T is the number of available samples, I is
the number of observations and J is the number of sources or components.
In general the number of source signals J is unknown, and it can be larger, equal or smaller than the
number of observations. The above model can be written in an equivalent scalar (element-wise) form
(see Figure 21.1b):
yit =
J

j=1
ai j x jt + eit
or
yi(t) =
J

j=1
ai j x j(t) + ei(t) (t = 1, 2, . . . , T ).
(21.2)
Usually, the latent components represent unknown source signals with speciﬁc statistical properties
or temporal structures and matrices have clear statistical properties and meanings. For example, the
rows of the matrix X that represent unknown sources or components should be statistically independent
for ICA, sparse for SCA [21–24,29], nonnegative for NMF [8], or have other speciﬁc and additional
morphological properties such as smoothness, continuity, or orthogonality [14,16,27].
In many signal processing applications the data matrix Y = [y(1), y(2) . . . , y(T )] ∈RI×T is
represented by vectors y(t) ∈RI (t = 1, 2, . . . , T ) of multiple measurements or recordings for a set
of discrete time points t. As mentioned above, the compact aggregated matrix Eq. (21.1) can be written
in a vector form as a system of linear equations (see Figure 21.1c), that is,
y(t) = Ax(t) + e(t) (t = 1, 2, . . . , T ),
(21.3)
where y(t) = [y1(t), y2(t), . . . , yI (t)]T is a vector of the observed signals at the discrete time point t
whereas x(t) = [x1(t), x2(t), . . . , xJ(t)]T is a vector of unknown sources at the same time point.
In order to estimate sources, sometimes we try ﬁrst to identify the mixing matrix A or its (pseudo)-
inverse (unmixing) matrix W and then estimate the sources. Usually, the inverse (unmixing) system

1.21.1 Introduction and of Problems Statement
1153
(a)
A
X
E
Y
(
)
J
T
x
(
)
I
T
x
(
)
I
J
x
(b)
1
1
11
12
1
2
21
22
2
2
2
2
x
x
x
(c)
(d)
Unknown
primary
sources
Unknown
matrix
Observable
mixed
signals
Demixing matrix
Separated
source
signals
+
+
+
+
+
+
+
+
x
t
1 ( )
x
t
J ( )
Learning
Algorithm
a11
w11
a1J
aJ1
aIJ
y
t
1 ( )
e
t
1 ( )
e
t
I ( )
y
t
I ( )
w1I
wJ1
wJI
x1 (t)
x
t
J ( )
FIGURE 21.1
Basic linear (instantaneous) BSS model: (a) Block diagram expressed as a matrix factorization: Y = AX + E,
(b) its detailed model, (c) BSS via separating (demixing) matrix W = A†, and (d) its detailed model.

1154
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
should be adaptive in such a way that it has some tracking capability in a nonstationary environment
(see Figure 21.1d). We assume here that only the sensor vectors y(t) are available and we need to estimate
the parameters of the unmixing system online. This enables us to perform indirect identiﬁcation of the
mixing matrix A (for I ≥J) by estimating the separating matrix W = ˆA†, where the symbol ( · )†
denotes the Moore–Penrose pseudo-inverse, and simultaneously estimate the sources. In other words,
for I ≥J the original sources can be estimated by the linear transformation
ˆx(t) = Wy(t) (t = 1, 2, . . . , T ).
(21.4)
However, instead of estimating the source signals by projecting the observed signals by using the demix-
ing matrix W, it is often more convenient to identify an unknown mixing matrix A (e.g., when the demix-
ing system does not exist, especially when the system is underdetermined, i.e., the number of observa-
tionsislowerthanthenumberofsourcesignals: I < J)andthenestimatethesourcesignalsbyexploiting
some a priori information about the source signals by applying a suitable optimization procedure.
There appears to be something magical about the BSS since we are estimating the original source
signals without knowing the parameters of the mixing and/or ﬁltering processes. It is difﬁcult to imagine
that one can estimate this at all. In fact, without some a priori knowledge it is not possible to uniquely esti-
mate the original source signals. However, one can usually estimate them up to certain indeterminacies.
In mathematical terms these indeterminacies and ambiguities can be expressed as arbitrary scaling and
permutations of the estimated source signals. Let  denote a speciﬁc BSS algorithm, then it holds that
X = (Y) = (AX) = PDX,
(21.5)
where P and D are the permutation matrix and a diagonal scaling matrix (with nonzero entries), respec-
tively. As a result, the original source signals represented by the rows of X are recovered as X with the
unavoidable ambiguities of scaling and permutations. If the BSS via constrained matrix factorization
leads to the estimation of true components (sources) with arbitrary scaling and permutations, we call
such factorization essentially unique. These indeterminacies preserve, however, the waveforms of the
original sources. Although these indeterminacies seem to be rather severe limitations, in a great number
of applications these limitations are not crucial, since the most relevant information about the source
signals is contained in the temporal waveforms or time–frequency patterns of the source signals and
usually not in their amplitudes or the order in which they are arranged in the system output. For some
models, however, there is no guarantee that the estimated or extracted signals will have exactly the same
waveforms as the source signals, and then the requirements must be sometimes further relaxed to the
extent that the extracted waveforms are distorted (i.e., time delayed, ﬁltered, or convolved) versions of
the primary source signals.
In some applications the mixing matrix A is ill-conditioned. In such cases some special models and
algorithms should be applied. Although some decompositions or matrix factorizations provide an exact
reconstruction of the data (i.e., Y = AX), we will consider here constrained factorizations which are
approximative in nature. In fact, many problems in signal and image processing can be solved in terms
of matrix factorizations. However, different cost functions and imposed constraints may lead to different
types of matrix factorizations.
The BSS problems are closely related to the concept of linear inverse problems or more generally, to
solving a large ill-conditioned system of linear equations (overdetermined or underdetermined), where

1.21.1 Introduction and of Problems Statement
1155
it is required to estimate the vectors x(t) (also in some cases to identify the matrix A) from noisy
data [14,30,31]. Physical systems are often contaminated by noise, thus, our task is generally to ﬁnd
an optimal and robust solution in a noisy environment. Wide classes of extrapolation, reconstruction,
estimation, approximation, interpolation, and inverse problems can be converted into minimum norm
problems of solving underdetermined systems of linear Eqs. (21.1) for J > I [14,31]. Generally
speaking, in signal processing applications, an overdetermined (I > J) system of linear Eqs. (21.1)
describes ﬁltering, enhancement, deconvolution and identiﬁcation problems, while the underdetermined
case describes inverse and extrapolation problems [14,30]. Although many different BSS criteria and
algorithms are available, most of them exploit various diversities or constraints imposed for estimated
components and/or factor matrices such as orthogonality, mutual independence, nonnegativity, sparsity,
smoothness, predictability or lowest complexity. By diversities we usually mean different morphological
characteristics or features of the signals.
More sophisticated or advanced approaches for GCA use combinations or the integration of various
diversities, in order to separate or extract sources with various constraints, morphology, structures, or
statistical properties, and to reduce the inﬂuence of noise and undesirable interferences [14,32].
All the above-mentioned BSS models are usually based on a wide class of unsupervised or semi-
supervised learning algorithms. Unsupervised learning algorithms try to discover a structure underlying
a data set, extract the meaningful features, and ﬁnd useful representations of the given data. Since data
can always be interpreted in many different ways, some knowledge is needed to determine which features
or properties best represent true latent (hidden) components. For example, PCA ﬁnds a low-dimensional
representation of the data that captures most of its variance. On the other hand, SCA tries to explain
the data as a mixture of sparse components (usually, in the time–frequency domain), and NMF seeks
to explain the data by parts-based localized additive representations (with nonnegativity constraints).
Most linear BSS or GCA models can be represented as constrained bilinear matrix factorization
problems, with suitable constraints imposed on the component matrix:
Y = ABT + E =
J

j=1
a j bT
j + E
=
J

j=1
a j ◦b j + E,
(21.6)
where Y ∈RI×T is a known data matrix (representing observations or measurements), E ∈RI×T
represents errors or noise, A = [a1, a2, . . . , aJ] ∈RI×J is the unknown full column rank mixing
(basis) matrix with J basis vectors a j ∈RI, and B = XT = [b1, b2, . . . , bJ] ∈RT ×J represent the
unknown components, latent variables or sources b j ∈RT (see Figure 21.2).
Note that we have symmetry of the factorization: For Eq. (21.6) we could write YT ∼= BAT , so the
meaning of “sources” and “mixture” are often somewhat arbitrary.
The unknown components usually have clear statistical properties and meaning. For example, the
columns of the matrix B should be as statistically mutually independent as possible, or sparse, piecewise
smooth, and/or nonnegative, etc., according to the physical meaning of the components, and to research

1156
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
a1
Y
E
+
aJ
+
b1
+
aJ
=
(
)
I
T
x
(
)
I
T
x
T
bJ
T
FIGURE 21.2
Bilinear BSS model. The data matrix Y ∈RI×T is approximately represented by a sum or linear combination
of rank-1 matrices Y(j) = aj ◦bj = aj bT
j
∈RI×T . The objective is to estimate all vectors aj and bj and
the optimal index (rank) J. In general such decomposition is not unique, and we need to impose various
constraints (e.g., nonnegativity, orthogonality, independence, or sparseness) to extract unique components
with desired diversities or properties.
the ﬁeld from which the data are collected. The BSS methods estimate these components maximizing
the a priori information by imposing proper constraints on the components, then leading to various
categories of BSS, such as ICA, SCA, SmoCA, NMF, etc. [8,14,33,34].
Very often multiple subject, multiple task data sets can be represented by a set of data matrices Yn.
It is therefore necessary to perform simultaneous constrained matrix factorizations:
Yn ≈AnBT
n
(n = 1, 2, . . . , N),
(21.7)
subject to various additional constraints (e.g., An = A for all n, and their columns are mutually indepen-
dentand/orsparse).ThisproblemisrelatedtovariousmodelsofgroupICA,withsuitablepre-processing,
dimensionality reduction and post-processing procedures [35–37]. This form of factorization is typical
for EEG/MEG related data for multi-subjects, multi-tasks, while the factorization for the transposed of
Y is typical for fMRI data [35–38].
The multilinear BSS concept introduced in Section 1.21.5.4 is more general and ﬂexible than the
group ICA, since various constraints can be imposed on the component matrices for different modes
(i.e., not only mutual independence but also nonnegativity, sparseness, smoothness or orthogonality).
There are neither theoretical nor experimental reasons why statistical independence (used by ICA)
should be the only one right concept to be used to extract latent components [33]. In real world scenarios,
latent (hidden) components have various complex properties and features. In other words, true unknown
components are seldom all statistically independent. Therefore, if we apply only one criterion like ICA,
we may fail to extract all desired components with correct interpretations. Rather we need to apply the
fusion of several strategies by employing several suitably chosen criteria and their associated learning
algorithms to extract all desired components for speciﬁc modes [8,33]. For these reasons we often
consider the multilinear BSS methods, which exploit multiple criteria or diversities for the component
matrices for various modes, rather than only statistical independence (see Section 1.21.5).
1.21.2 PCA/SVD and related problems
Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) are well established
tools with applications virtually in all areas of science, machine learning, signal processing, engineering,

1.21.2 PCA/SCD and Related Problems
1157
genetics and neural computation, to name just a few, where large data sets are encountered [39,40]. The
basic aim of PCA is to ﬁnd a few linear combinations of variables, called principal components, which
point in orthogonal directions while explaining as much of the variance of the data as possible.
PCA is perhaps one of the oldest and best-known technique form multivariate analysis and data
mining. It was introduced by Pearson, who used it in a biological context and further developed by
Hotelling in research work related psychometry. PCA was also developed independently by Karhunen
in the context of probability theory and was subsequently generalized by Loève (see [14,39,41] and
references therein). The purpose of PCA is to derive a relatively small number of uncorrelated linear
combinations (principal components) of a set of random zero-mean variables y(t), while retaining as
much of the information from the original variables as possible.
Following are several of the objectives of PCA/SVD:
1. dimensionality reduction;
2. determination of linear combinations of variables;
3. feature selection: the choice of the most useful variables;
4. visualization of multi-dimensional data;
5. identiﬁcation of underlying variables;
6. identiﬁcation of groups of objects or of outliers.
Often the principal components (PCs) (i.e., directions on which the input data has the largest variance)
areusuallyregardedasimportantorsigniﬁcant,whilethosecomponentswiththesmallestvariancecalled
minor components (MCs) are usually regarded as unimportant or associated with noise. However, in
some applications, the MCs are of the same importance as the PCs, for example, in the context of curve
or surface ﬁtting or total least squares (TLS) problems.
Generally speaking, PCA is often related and motivated by the following two problems:
1. Given random vectors y(t) ∈RI (t = 1, 2, . . . , T ), with ﬁnite second order moments and
zero mean, ﬁnd the reduced J-dimensional (J < I) linear subspace that minimizes the expected
distance of y(t) from the subspace. This problem arises in the area of data compression, where
the task is to represent all the data with a reduced number of parameters while assuring minimum
distortion due to the projection.
2. Given random vectors y(t) ∈RI, ﬁnd the J-dimensional linear subspace that captures most of
the variance of the data. This problem is related to feature extraction, where the objective is to
reduce the dimension of the data while retaining most of its information content.
It turns out that both problems have the same optimal solution (in the sense of least-squares error),
which is based on the second order statistics (SOS), in particular, on the eigen structure of the data
covariance matrix.
Remark.
If the signals have standard deviation (SD) one, the covariance and correlation matrices
are identical. The correlation matrix is the covariance matrix of the mean centered and scaled vectors.
Since, we consider here zero mean signals with SD one, both matrices are equivalent.
PCA is designed for a general random variable regardless whether it is of zero mean or not. But at
the ﬁrst step we center the data, otherwise the ﬁrst principal component will converge to its mean value.

1158
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
1.21.2.1 Standard PCA
Without loss of generality let us consider a data matrix Y ∈RT ×I with T samples and I variables and
with the columns centered (zero mean) and scaled. Equivalently, it is sometimes convenient to consider
the data matrix Y′ = [y(1), y(2), . . . , y(T )] ∈RI×T . The standard PCA can be written in terms of
the scaled sample covariance matrix Ry y = E{y yT } ∈RI×I as follows: Find orthonormal vectors by
solving the following optimization problem
max
vi {viRy yvT
i },
s.t. vT
i vi = 1,
vT
i vk = 0 ∀i > k (i = 1, 2, . . . , I).
(21.8)
PCA can be converted to the eigenvalue problem of the covariance matrix of y(t) (t = 1, 2, . . . , T ),
and it is essentially equivalent to the Karhunen-Loève transform used in image and signal processing.
In other words, PCA is a technique for the computation of eigenvectors and eigenvalues for the scaled
estimated covariance matrix
Ry y = E{y(t)yT (t)} = VVT =
I

i=1
λivivT
i ,
(21.9)
where  = diag{λ1, λ2, . . . , λI} is a diagonal matrix containing the I eigenvalues (ordered in decreasing
values) and V = [v1, v2, . . . , vI ] ∈RI×I is the corresponding orthogonal or unitary matrix consisting
of the unit length eigenvectors referred to as principal eigenvectors. It should be noted that the covariance
matrix is symmetric positive deﬁnite, that is why the eigenvalues are nonnegative. We assume that for
noisy data all eigenvalues are positive and we put them in descending order.
The basic problem we try to solve is the standard eigenvalue problem which can be formulated using
the equations
Ry yvi = λivi
(i = 1, 2, . . . , I),
(21.10)
where vi are the orthonormal eigenvectors, λi are the corresponding eigenvalues and Ry y = E{y yT }
is the covariance matrix of zero-mean signals y(t) and E is the expectation operator. Note that (21.10)
can be written in matrix form as VT Ry yV = , where  is the diagonal matrix of eigenvalues (ranked
in descending order) of the estimated covariance matrix Ry y.
The Karhunen-Loéve-transform determines a linear transformation of an input vector y as
yP(t) = VT
S y(t),
(21.11)
where y(t) = [y1(t), y2(t), . . . , yI (t)]T is the zero-mean input vector,yP = [y1(t),y2(t), . . . ,yJ(t)]T
is the output vector called the vector of principal components (PCs), and VS = [v1, v2, . . . , vJ] ∈
RI×J, with J < I, is the set of signal subspace eigenvectors, with the orthonormal vectors v j =
[v j1, v j2, . . . , v j I]T , (i.e., (vT
i v j = δi j) for j ≤i, where δi j is the Kronecker delta that equals 1
for i = j, and zero otherwise. The vectors v j ( j = 1, 2, . . . , J) are eigenvectors of the covariance
matrix, while the variances of the PCs y j(t) = vT
j y(t) are the corresponding principal eigenvalues, i.e.,
λ j = E{y2
j }.
On the other hand, the (I −J) minor components are given by
yM(t) = VT
N y(t),
(21.12)
where VN = [vJ+1, vJ+2, . . . , vI] consists of the (I −J) eigenvectors associated with the smallest
eigenvalues.

1.21.2 PCA/SCD and Related Problems
1159
1.21.2.2 Determining the number of principal components
A quite important problem arising in many application areas is the determination of the dimensions
of the signal and noise subspaces. In other words, a central issue in PCA is choosing the number of
principal components to be retained [42–44]. To solve this problem, we usually exploit a fundamental
property of PCA: It projects the sensor data y(t) ∈RI from the original I-dimensional space onto a
J-dimensional output subspace y(t) ∈RJ (typically, with J ≪I), thus performing a dimensionality
reduction which retains most of the intrinsic information in the input data vectors. In other words,
the principal components y j(t) = vT
j y(t) are estimated in such a way that, for J ≪I, although the
dimensionality of data is strongly reduced, the most relevant information is retained in the sense that the
input data can be approximately reconstructed from the output data (signals) by using the transformation
ˆy = VSy, that minimizes a suitable cost function. A commonly used criterion is the minimization of
the mean squared error ∥y −VSVT
S y∥2
2 subject to orthogonality constraint VT
SVS.
Under the assumption that the power of the signals is larger than the power of the noise, the PCA
enables us to divide observed (measured) sensor signals: y(t) = ys(t) + e(t) into two subspaces: the
signal subspace corresponding to principal components associated with the largest eigenvalues called
the principal eigenvalues: λ1, λ2, . . . , λJ, (I > J) and associated eigenvectors VS = [v1, v2, . . . , vJ]
called the principal eigenvectors and the noise subspace corresponding to the minor components asso-
ciated with the eigenvalues λJ+1, . . . , λI. The subspace spanned by the J ﬁrst eigenvectors v j can
be considered as an approximation of the noiseless signal subspace. One important advantage of this
approach is that it enables not only a reduction in the noise level, but also allows us to estimate the
number of sources on the basis of distribution of the eigenvalues. However, a problem arising from
this approach is how to correctly set or estimate the threshold which divides eigenvalues into the two
subspaces, especially when the noise is large (i.e., the SNR is low). The scaled covariance matrix of the
observed data can be written as
Ry y = [VS, VN ]
S
0
0 N

[VS, VN ]T
= VSSVT
S + VN N VT
N ,
(21.13)
where VSSVT
S is a rank-J matrix, VS ∈RI×J contains the eigenvectors associated with J principal
(signal + noise subspace) eigenvalues of S = diag{λ1 ≥λ2 · · · ≥λJ} in a descending order. Similarly,
the matrix VN ∈RI×(I−J) contains the (I −J) (noise subspace) eigenvectors that correspond to the
noise eigenvalues N = diag{λJ+1, . . . , λI} = σ 2
e II−J. This means that, theoretically, the (I −J)
smallest eigenvalues of Ry y are equal to σ 2
e , so we can theoretically (in the ideal case) determine the
dimension of the signal subspace from the multiplicity of the smallest eigenvalues under the assumption
that the variance of the noise is relatively low and we have a perfect estimate of the covariance matrix.
However, in practice, we estimate the sample covariance matrix from a limited number of samples
and the (estimated) smallest eigenvalues are usually different, so the determination of the dimension
of the signal subspace is not an easy task. Usually we set the threshold between the signal and noise
eigenvalues by using some heuristic procedure or a rule of thumb. A simple ad hoc rule is to plot the
eigenvalues in decreasing order and search for an elbow, where the signal eigenvalues are on the left side
and the noise eigenvalues are on the right side. Another simple technique is to compute the cumulative
percentage of the total variation explained by the PCs and retain the number of PCs that represent, say

1160
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
95% of the total variation. Such techniques often work well in practice, but their disadvantage is that
they need a subjective decision from the user [44].
Alternatively, can use one of the several well-known information theoretic criteria, namely, Akaike’s
Information Criterion (AIC), the Minimum Description Length (MDL) criterion and Bayesian Infor-
mation Criterion (BIC) [43,45,46]. To do this, we compute the probability distribution of the data for
each possible dimension (see [46]).
Unfortunately, AIC and MDL criteria provide only rough estimates (of the number of sources) that
are rather very sensitive to variations in the SNR and the number of available data samples.
Many sophisticated methods have been introduced such as a Bayesian model selection method, which
is referred to as the Laplace method. It is based on computing the evidence for the data and it requires
integrating out all the model parameters. The BIC method can be thought of as an approximation to the
Laplace criterion. The calculation involves an integral over the Stiefel manifold which is approximated
by the Laplace method [43]. One problem with the AIC, MDL and BIC criteria mentioned above is
that they have been derived by assuming that the data vectors y(t) have a Gaussian distribution [46].
This is done for mathematical tractability, by making it possible to derive closed form expressions.
The Gaussianity assumption does not usually hold exactly in the BSS and other signal processing
applications. For setting the threshold between the signal and noise eigenvalues, one might even suppose
that the AIC, MDL and BIC criteria cannot be used for the BSS, particularly ICA problems, because
in those cases we assume that the source signals x j(t) are non-Gaussian. However, it should be noted
that the components of the data vectors y(t) = Ax(t) + e(t) are mixtures of the sources, and therefore
they often have distributions that are not too far from a Gaussian distribution.
An alternative simple heuristic method [42,47] computes the GAP (smoothness) index between
consecutive eigenvalues deﬁned as
GAP(p) =
var[{˜λi}I−1
i=p+1]
var[{˜λi}I−1
i=p]
=
ˆσ 2
p+1
ˆσ 2p
(p = 1, 2, . . . , I −2),
(21.14)
where ˜λi = λi −λi+1 and λ1 ≥λ2 ≥· · · ≥λI > 0 are the eigenvalues of the covariance matrix for
the noisy data and the sample variances are computed as follows:
ˆσ 2
p = var[{˜λi}I−1
i=p] =
1
I −p
I−1

i=p
⎛
⎝˜λi −
1
I −p
I−1

i=p
˜λi
⎞
⎠
2
.
(21.15)
The number of components (for each mode) are selected using the following criterion:
J = arg
min
p=1,2,...,I−3{GAP(p)}.
(21.16)
Recently, Ulfarsson and Solo [44] proposed an alternative more sophisticated method called SURE
(Stein’s Unbiased Risk Estimator) which allows to estimate the number of PC components.
Extensive numerical experiments indicate that the Laplace method usually outperforms the BIC
method while the GAP and SURE methods can often achieve signiﬁcantly better performance than the
Laplace method.

1.21.2 PCA/SCD and Related Problems
1161
1.21.2.3 Whitening
Some adaptive algorithms for blind separation require prewhitening (also called sphering or normalized
spatial decorrelation) of mixed (sensor) signals. A random, zero-mean vector y is said to be white if
its covariance matrix is an identity matrix, i.e., Ryy = E{yyT } = IJ or E{ ˜yi ˜y j} = δi j, where δi j
is the Kronecker delta. In whitening, the sensor vectors y(t) are pre-processed using the following
transformation:
y(t) = Qy(t).
(21.17)
Here y(t) denotes the whitened vector, and Q is an J × I whitening matrix. For J < I, where J
is known in advance, Q simultaneously reduces the dimension of the data vectors from I to J. In
whitening, the matrix Q is chosen such that the scaled covariance matrix E{y(t)y(t)T } becomes the unit
matrix IJ. Thus, the components of the whitened vectors y(t) are mutually uncorrelated and have unit
variance, i.e.,
Ryy = E{yyT } = E{Qy yT QT } = QRy yQT = IJ.
(21.18)
Generally, the sensor signals are mutually correlated, i.e., the covariance matrix Ry y = E{y yT } is a
full (not diagonal) matrix. It should be noted that the matrix Q ∈RJ×I is not unique, since multiplying
it by an arbitrary orthogonal matrix from the left still preserves property (21.18). Since the covariance
matrix of sensor signals y(t) is symmetric positive semi-deﬁnite, it can be decomposed as follows:
Ry y = VVT = V1/21/2VT ,
(21.19)
where V is an orthogonal matrix and  = diag{λ1, λ2, . . . , λJ} is a diagonal matrix with positive
eigenvalues λ1 ≥λ2 ≥· · · ≥λJ > 0. Hence, under the condition that the covariance matrix is positive
deﬁnite, the required decorrelation matrix Q (also called a whitening matrix or Mahalanobis transform)
can be computed as follows:
Q = −1/2VT = diag

1
√λ1 ,
1
√λ2 , . . . ,
1
√λJ

VT
(21.20)
or
Q = U−1/2VT ,
(21.21)
where U is an arbitrary orthogonal matrix. This can be easily veriﬁed by substituting (21.20) or (21.21)
into (21.18):
Ry y = E{y yT } = −1/2VT VVT V−1/2 = IJ,
(21.22)
or
Ry y = U−1/2VT VVT V−1/2UT = IJ.
(21.23)
Alternatively, we can apply the Cholesky decomposition
Ry y = LLT ,
(21.24)
where L is a lower triangular matrix. The whitening (decorrelation) matrix in this case is
Q = UL−1 ,
(21.25)

1162
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
where U is an arbitrary orthogonal matrix, since
Ry y = E{y yT } = QRy yQT = UL−1LLT (L−1)T UT = IJ.
(21.26)
In the special case when y(t) = e(t) is colored Gaussian noise with Ree = E{eeT }
̸=
σ 2
e IJ, the
whitening transform converts it into a white noise (i.i.d.) process. If the covariance matrix is positive
semi-deﬁnite, we can take only the positive eigenvalues and the associated eigenvectors.
It should be noted that after pre-whitening of sensor signals y(t), in the model Y ∼= AX, the new
mixing matrix deﬁned as A∗= QA is orthogonal (i.e., A∗AT
∗= IJ), if source signals are uncorrelated
(i.e., Rxx = E{xxT } = IJ), since we can write Ryy = E{yyT } = A∗E{xxT }AT
∗= IJ.
1.21.2.4 SVD
The Singular Value Decomposition (SVD) is a tool of both great practical and theoretical importance
in signal processing and data analysis [40]. The SVD of a data matrix Y ∈RT ×I of rank-J with J ≤I,
assuming without loss of generality that T > I, leads to the following matrix factorization
Y = UVT = J
j=1 σ ju jvT
j ,
(21.27)
where the matrix U = [u1, u2, . . . , uT ] ∈RT ×T contains the T left singular vectors,  ∈RT ×I
+
has nonnegative elements on the main diagonal representing the singular values σ j and the matrix
V = [v1, v2, . . . , vI ] ∈RI×I represents the I right singular vectors called the loading factors. The
nonnegative quantities σ j, sorted as σ1 ≥σ2 ≥· · · ≥σJ > σJ+1 = σJ+2 = · · · = σI = 0 can be
shown to be the square roots of the eigenvalues of the symmetric matrix YT y ∈RI×I. The term u jvT
j
is a T × I rank-1 matrix often called the jth eigenimage of Y. Orthogonality of the SVD expansion
ensures that the left and right singular vectors are orthogonal, i.e., uT
i u j = δi j and vT
i v j = δi j, with δi j
the Kronecker delta (or equivalently UT U = I and VT V = I).
In many applications, it is more practical to work with the truncated form of the SVD, where only
the ﬁrst P ≤J, (where J is the rank of Y, with J < I) singular values are used, so that
Y ∼= UP PVT
P =
P

j=1
σ ju jvT
j ,
(21.28)
where UP = [u1, u2, . . . , uP] ∈RT ×P,  P = diag{σ1, σ2, . . . , σP} and V = [v1, v2, . . . , vP] ∈
RI×P. This is no longer an exact decomposition of the data matrix Y, but according to the Eckart-
Young theorem it is the best rank-P approximation in the least-squares sense and it is still unique
(neglecting signs of vectors ambiguity) if the singular values are distinct.
For noiseless data, we can use the following decomposition:
Y = [US, UN ]
 S 0
0 0

[VS, VN ]T ,
(21.29)
where US = [u1, . . . , uJ] ∈RT ×J, S = diag{σ1, . . . , σJ} and UN = [uJ+1, . . . , uJ]. The set
of matrices {US, S, VS} represents this signal subspace and the set of matrices {UN , N , VN }

1.21.2 PCA/SCD and Related Problems
1163
represents the null subspace or, in practice for noisy data, the noise subspace. The J columns of U,
corresponding to these non-zero singular values that span the column space of Y, are called the left
singular vectors. Similarly, the J columns of V are called the right singular vectors and they span the
row space of Y. Using these terms the SVD of Y can be written in a more compact form as:
Y = USSVT
S =
J

j=1
σ ju jvT
j .
(21.30)
Perturbation theory for the SVD is partially based on the link between the SVD and the PCA and
symmetric Eigenvalue Decomposition (EVD). It is evident, that from the SVD of the matrix Y = UVT
with rank-J ≤I ≤T , we have
YYT = U2
1UT ,
(21.31)
YT Y = V2
2VT ,
(21.32)
where 1 = diag{σ1, . . . , σT } and 2 = diag{σ1, . . . , σI} . This means that the singular values of Y are
the positive square roots of the eigenvalues of YT Y and the eigenvectors U of YYT are the left singular
vectors of Y. Note that if I < T , the matrix YYT will contain at least T −I additional eigenvalues that
are not included as singular values of Y.
An estimate ˆRy y of the covariance matrix corresponding to a set of (zero-mean) observed vectors
y(t) ∈RI may be computed as ˆRy y = (1/T ) T
t=1 y(t)yT (t). An alternative equivalent way of
computing Ry y is to form a data matrix yT = [y(1), y(2), . . . , y(T )] ∈RI×T and represent the
estimated covariance matrix by ˆRy y =
1
T YyT . Hence, the eigenvectors of the sample covariance
matrix ˆRy y are the left singular vectors U of y and the singular values σ j of y are the positive square
roots of the eigenvalues of ˆRy y.
From this discussion it follows that all the algorithms for PCA and EVD can be applied (after some
simple tricks) to the SVD of an arbitrary matrix Y without any need to directly compute or estimate
the covariance matrix. The opposite is also true; the PCA or EVD of the covariance matrix ˆRy y can be
performed via the SVD numerical algorithms. However, for large data matrices Y, the SVD algorithms
usually become more costly, than the relatively efﬁcient and fast PCA adaptive algorithms. Several
reliable and efﬁcient numerical algorithms for the SVD do however, exist [48].
The approximation of the matrix Y by a rank-1 matrix σuvT
of two unknown vectors
u = [u1, u2, . . . , uT ]T ∈RT and v = [v1, v2, . . . , vI]T ∈RI, normalized to unit length, with an
unknown scaling constant term σ, can be presented as follows:
Y = σuvT + E,
(21.33)
where E ∈RT ×I is the matrix of the residual errors eit. In order to compute the unknown vectors we
minimize the squared Euclidean error as [49]
D1(Y∥σuvT ) = ∥E∥2
F =

it
e2
it =
I

i=1
T

t=1
(yti −σutvi)2.
(21.34)

1164
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
The necessary conditions for the minimization of (21.34) are obtained by equating the gradients to zero:
∂D1
∂ut
= −2σ
I

i=1
(yti −σutvi)vi = 0,
(21.35)
∂D1
∂vi
= −2σ
T

t=1
(yti −σutvi)ut = 0.
(21.36)
These equations can be expressed as follows:
I

i=1
ytivi = σut
I

i=1
v2
i ,
T

t=1
ytiut = σvi
T

t=1
u2
t .
(21.37)
It is interesting to note that (taking into account that σut = I
i=1 ytivi/ I
i=1 v2
i (see Eq. (21.37)) the
cost function (21.34) can be expressed as [49]
D1 = ∥E∥2
F =
I

i=1
T

t=1
(yti −σutvi)2
=
I

i=1
T

t=1
y2
ti −2
T

t=1
(σut)
I

i=1
(ytivi) +
T

t=1
(σut)2
I

i=1
v2
i
=
I

i=1
T

t=1
y2
ti −
I
i=1(T
t=1ytivi)2
I
i=1v2
i
.
(21.38)
In matrix notation the cost function can be written as
∥E∥2
F = ∥Y∥2
F −vT YT Yv
∥v∥2
2
= ∥Y∥2
F −σ 2,
(21.39)
where the second term is called the Rayleigh quotient. The maximum value of the Rayleigh quotient is
exactly equal to the maximum eigenvalue λ1 = σ 2
1 .
Taking into account that the vectors are normalized to unit length, that is, uT u = T
t=1 u2
t = 1 and
vT v = I
i=1 v2
i = 1, we can write the above equations in a compact matrix form as
Yv = σu,
YT u = σv
(21.40)
or equivalently (by substituting one of Eq. (21.40) into another)
YT Yv = σ 2v,
YYT u = σ 2u,
(21.41)
which are classical eigenvalue problems which estimate the maximum eigenvalue λ1 = σ 2
1 = σ 2
max
with the corresponding eigenvectors u1 = u and v1 = v. The solutions of these problems give the best
ﬁrst rank-1 approximation of Eq. (21.27).

1.21.2 PCA/SCD and Related Problems
1165
In order to estimate the next singular values and the corresponding singular vectors, we may apply
a deﬂation approach, that is,
Y j = Y j−1 −σ ju jvT
j
( j = 1, 2, . . . , J),
(21.42)
where Y0 = Y. Solving the same optimization problem (21.34) for the residual matrix Y j yields the
set of consecutive singular values and corresponding singular vectors. Repeating the reduction of the
matrix yields the next set of solution until the deﬂation matrix YJ+1 becomes zero. In the general case,
we have:
Yv j = σ ju j,
YT u j = σ jv j
( j = 1, 2, . . . , J).
(21.43)
Using the property of orthogonality of the eigenvectors and the equality uT Yv = vT YT u = σ,
we can estimate the precision of the matrix approximation with the ﬁrst P ≤J pairs of singular
vectors [49]:
∥Y −
P

j=1
σ ju jvT
j ∥2
F =
T

t=1
I

i=1
(yti −
J

j=1
σ jut jvi j)2
= ∥Y∥2
F −
P

j=1
σ 2
j ,
(21.44)
and the residual error reduces exactly to zero when the number of singular values is equal to the matrix
rank, that is, for P = J. Thus, we can write for the rank-J matrix:
∥Y∥2
F =
J

j=1
σ 2
j .
(21.45)
Note that the SVD can be formulated as the following constrained optimization problem:
minU,V,{∥Y −UVT ∥2
F},
s.t. UT U = I, VT V = I,
diag() ≥0.
(21.46)
The cost function can be expressed as follows:
∥Y −UVT ∥
2
F = tr(Y −UVT )T tr(Y −UVT )
= ∥Y∥2
F −2
J

j=1
σ juT
j Yv j +
J

j=1
σ 2
j .
(21.47)
Hence, for J = 1 (with u1 = u and v1 = v) the problem (21.46) (called the best rank-1 approximation):
minu,v{∥Y −σuvT ∥2
F},
∥u∥2 = ∥v∥2 = 1, σ > 0,
(21.48)
canbereformulatedasanequivalentmaximizationproblem(calledthemaximizationvarianceapproach):
maxu,v{uT Yv},
s.t. ∥u∥2 = ∥v∥2 = 1,
(21.49)
with a maximal singular value σ = uT Yv.

1166
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
1.21.2.5 Very large-scale PCA/SVD
Suppose that Y = UVT is the SVD decomposition of an T × I-dimensional matrix, where T is very
large and I is moderate. Then matrices  and V can be obtained form EDV/PCA decomposition of
the I × I-dimensional matrix YT Y = V2VT . The orthogonal matrix U can be then obtained from
U = YV−1.
Remark.
It should be noted that for moderate large-scale problems with I ≫T and typically I < 104
we can more efﬁciently solve the SVD problem by noting that the PCA/SVD of a semi-positive deﬁnite
matrix (of much smaller dimension)
RYY′ = YYT = UUT ∈RT ×T ,
(21.50)
with U ∈RT ×J, has the same eigenvalues as the matrix RY′Y = YT Y, since we can write
RYY′ = YYT = UVT VUT = U2UT and RY′Y = YT Y = VUT UVT = V2VT and
 = diag{σ1, . . . , σJ}.
Moreover, if the vector v is an eigenvector of the matrix RY′Y, then Yv is an eigenvector of RYY′,
since we can write
YT Yv = λv,
(YYT )(Yv) = λ(Yv).
(21.51)
In many practical applications dimension I ≫T could be so large that the data matrix Y ∈RT ×I can
not be loaded into the memory of moderate class of computers. The simple solution is to partition of data
matrix Y into, say, Q slices as Y = [Y1, Y2, . . . YQ], where the size of the qth slice Yq ∈RT ×(I/Q) and
can be optimized depending on the available computer memory [50,51]. The scaled covariance matrix
(cross matrix product) can be then calculated as
YYT = 1
Q
Q

q=1
YqYT
q = U2UT ∈RT ×T ,
(21.52)
since Yq = UVT
q , where Vq ∈R(I/Q)×J are orthogonal matrices. Instead of computing the whole
right singular matrix V = YT U−1 ∈RI×J, we can perform calculations on the slices of the partitioned
data matrix Y as VT
q = −1UT Yq for q = 1, 2, . . . , Q. The concatenated slices [VT
1 , VT
2 , . . . , VT
Q]T
form the matrix of the right singular vectors VT . Such approach do not require loading the entire data
matrix Y at once in the computer memory but instead use a sequential access to dataset. Of course, this
method will work only if the dimension T is moderate, typically less than 10,000 and the dimension I
can be theoretically arbitrary large.
1.21.2.6 Basic algorithms for PCA/SVD
There are three basic or fundamental methods to compute PCA/SVD:
•
The minimal reconstruction error approach:
v∗= argmin
vT v=1
{
T

t=1
[y(t) −(vT y(t))v]2}.
(21.53)

1.21.2 PCA/SCD and Related Problems
1167
•
The best rank-1 approximation approach:
(u∗, v∗, σ ∗) = argmin
u,v,σ

∥Y −σuvT ∥
2
F

s.t. ∥u∥2 = ∥v∥2 = 1,
σ > 0.
(21.54)
•
The maximal variance approach:
v∗= argmax
v
{vT YT Yv} s.t. ∥v∥2 = 1.
(21.55)
or in the more general form (for j = 1, 2, . . . , J):
(u∗
j, v∗
j) = argmax
u j,v j
{uT
j Yv j},
(21.56)
s.t. ∥u j∥2 = ∥v j∥2 = 1 uT
j ui = 0, vT
j vi = 0, ∀i < j.
All the above approaches are connected or linked and almost equivalent. Often we impose additional
constraints such as sparseness and/or nonnegativity (see Section 1.21.2.6.3).
1.21.2.6.1
Simple iterative algorithm using minimal reconstruction error approach
One of the simplest and intuitively understandable approaches to the derivation of adaptive algorithms
for the extraction of true principal components without need to compute explicitly the covariance matrix
is based on self-association (also called self-supervising or the replicator principle) [30,52]. According
to this approach, we ﬁrst compress the data vector y(t) to one variable y1(t) = vT
1 y(t) and next we
attempt to reconstruct the original data from y1(t) by using the transformation ˆy(t) = v1y1(t). Let
us assume, that we wish to extract principal components (PCs) sequentially by employing the self-
supervising principle (replicator) and a deﬂation procedure [30].
Let us consider a simple processing unit (see Figure 21.3)
y1(t) = vT
1 y(t) =
I

i=1
v1i yi(t),
(21.57)
which extracts the ﬁrst principal component, with λ1 = E{y2
1(t)}. Strictly speaking, the factor y1(t)
is called the ﬁrst principal component of y(t), if the variance of y1(t) is maximally large under the
constraint that the principal vector v1 has unit length.

1168
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
å
1y~
12
2
y
y =
I
I
y
y
1
=
I
v1
1 2
v
11
v
11
1
y
y =
2
+
_
+
+
+
+
+
+
_
_
_
+
+
+
+
+
_
_
32
y
I
y3
31
y
I
v2
22
v
21
v
11
v
I
v1
12
v
21
v
I
v2
22
v
22
y
I
y2
21
y
å
å
å
å
å
å
å
y~
FIGURE 21.3
The sequential extraction of principal components and the deﬂation procedure.
The vector v1 = [v11, v12, . . . , v1I]T should be determined in such a way that the reconstruction
vector ˆy(t) = v1y1(t) will reproduce (reconstruct) the training vectors y(t) as correctly as possible,
according to the following cost function J(v1) = E{∥e1(t)∥2
2 ≈T
t=1 ∥e1(t)∥2
2, where e1(t) = y(t) −
v1y1(t).
In general, the loss (cost) function is expressed as
Di(vi) =
T

t=1
∥ei(t)∥2
2,
(21.58)
where
e j = y j+1 = y j −v jy j,
y j = vT
i y j,
y1(t) = y(t).
In order to increase the convergence speed, we can minimize the cost function (21.58) by employing
thecascaderecursiveleast-squares(CRLS)approachforoptimalupdatingofthelearningrate ηi [52,53]:
y1(t) = y(t),
(21.59)
y j(t) = vT
j (t)y j(t) ( j = 1, 2, . . . , J),
(21.60)
v j(t + 1) = v j(t) + y j(t)
η j(t)[y j(t) −y j(t)v j(t)],
(21.61)
η j(t + 1) = η j(t) + |y j(t)|2,
η j(0) = 1
T
I

i=1
T

t=1
y2
ji(t),
(21.62)
y j+1(t) = y j(t) −y j(t)v j∗,
(21.63)
v j+1(0) = v j −[v1∗, . . . , v j∗][v1, . . . , v j∗]T v j,
(21.64)
where vi∗denotes the vector v j(t) after achieving convergence. The above algorithm is fast and accurate
for extracting sequentially an arbitrary number of eigenvectors in PCA.

1.21.2 PCA/SCD and Related Problems
1169
1.21.2.6.2
Power methods for standard PCA
Many iterative algorithms for PCA exploit the Rayleigh quotient (RQ) of the speciﬁc covariance matrix
as the cost function and power methods [54,55]. The Rayleigh quotient R(v) is deﬁned for v ̸= 0, as
R(v) = R(v, Ry y) = vT Ry yv
vT v ,
(21.65)
It is worth to note that
λmax = max R(v, Ry y),
λmin = min R(v, Ry y),
(21.66)
where λmax and λmin denote the largest and the smallest eigenvalues of the covariance matrix Ry y,
respectively.
More generally, the critical points and critical values of R(v, Ry y) are the eigenvectors and
eigenvalues of Ry y. Assuming that the eigenvalues of the covariance matrix can be ordered as
λ1 ≥λ2 ≥· · · ≥λJ, and that the principal eigenvector v1 has unit length, i.e., vT
1 v1 = 1, we
can estimate it using the following iterations
v1(k + 1) =
Ry yv1(k)
vT
1 (k)Ry yv1(k).
(21.67)
Taking into account that y(k)
1 (t) = vT
1 (k)y(t) and Ry y = ⟨y(t)yT (t)⟩, we can then use the following
simpliﬁed formula
v1(k + 1) =
T
t=1y(k)
1 (t)y(t)
T
t=1[y(k)
1 (t)]2
(21.68)
or more generally, for a number of higher PCs, we may use the deﬂation approach as
v j(k + 1) =
T
t=1y(k)
j (t)y j(t)
T
t=1[ ˜y(k)
j (t)]2
( j = 1, 2, . . . , J),
(21.69)
where y(t)
j (t) = vT
j (k)y j(t). After the convergence of the vector v j(k) to v j∗, we perform the deﬂation
as: y j+1 = y j −v j∗y j, y1 = y.
The above fast PCA algorithm can be derived also in a slightly modiﬁed form by minimizing the
cost function:
D(v, y(k)) =
T

t=1
∥y(t) −˜y(k)v∥
2
2
(21.70)
=
T

t=1
∥y(t)∥2
2 + ∥v∥2
2
T

t=1
[ ˜y(k)(t)]2 −2vT
T

t=1
˜y(k)(t)y(t),
subject to the constraint ∥v∥2 = 1. The cost function achieves equilibrium when the gradient of D is
zero, i.e.,
v∗=
T
t=1 ˜y∗(t)y(t)
T
t=1 ˜y2∗(t)
.
(21.71)

1170
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
This suggests the following iteration formula [55,56]:
v(k + 1) =
T
t=1 ˜y(k)(t)y(t)
∥T
t=1 ˜y(k)(t)y(t)∥2
.
(21.72)
Remark.
It should be noted that the convergence rate of the power algorithm depends on a ratio λ2/λ1,
where λ2 is the second largest eigenvalue of Ry y. This ratio is generally smaller than one, allowing
adequate convergence of the algorithm. However, if the eigenvalue λ1 = λmax has one or more other
eigenvalues of Ry y close by, in other words, when λ1 belongs to a cluster of eigenvalues then the
ratio can be very close to one, causing very slow convergence, and as a consequence, the estimated
eigenvector v may be inaccurate for noisy data or collinear components. For multiple eigenvalues, the
power method will fail to converge.
1.21.2.6.3
Extensions of standard PCA
1.21.2.6.3.1
Sparse PCA
The importance of the standard PCA is mainly due to the following three important properties:
1. The principal components sequentially capture the maximum variability (variance) of the data
matrix Y, thus guaranteeing minimal information loss in the sense of mean squared errors.
2. The principal components are uncorrelated, i.e., E{ ˜yi ˜y j} = δi jλ j.
3. The PCs are hierarchically organized with the respect to decreasing values of their variances
(eigenvalues of the covariance matrix).
ThestandardPCA/SVDhasalsoseveraldisadvantages,especiallyforlarge-scaleandnoisyproblems.
A particular disadvantage that the standard principal components y j(t) = vT
j y(t) = I
i=1 vi j yi(t) are
usually a linear combination of all the variables y j(t). In other words, all weights vi j (referred as load-
ings) are not zero. This means that the principal vectors v j are dense (not sparse) what makes often
the physical interpretation of the principal components difﬁcult in many applications. For example, in
many applications (from biology to image understanding) the coordinate axes have a physical inter-
pretations (each axis might correspond to a speciﬁc feature), but only if the components are sparsely
represented, i.e., by a very few non zero loadings (coordinates). Recently several modiﬁcations of PCA
have been proposed which impose some sparseness for the principal (basis) vectors and corresponding
components are called sparse principal components [54,57–62]. The main idea in SPCA is to force the
basis vectors to be sparse, however the sparsity proﬁle should be adjustable or well controlled via some
parameters in order to discover speciﬁc features in the observed data. In other words, our objective is
to estimate sparse principal components, i.e., the sets of sparse vectors v j spanning a low-dimensional
space that represent most of the variance present in the data Y.
Many methods have been proposed for estimating sparse principal components, based on either
the maximum-variance property of the principal components, or on the regression/reconstruction error
property. The most important approaches which extract the principal eigenvectors sequentially can be
summarized as the following constrained or penalized optimization problems [54,57,59–63]:

1.21.2 PCA/SCD and Related Problems
1171
1. l1-normconstrainedSPCAortheSCoTLASSprocedure[59]usesthemaximalvariancecharacter-
ization for the principal components. The ﬁrst sparse principal component solves the optimization
problem:
max
v {vT YT Yv} s.t. ∥v∥2
2 ≤1, ∥v∥1 ≤c1
(21.73)
and subsequent components solve the same problem but with the additional constraint that they
must be orthogonal to the previous components. When the parameter c1 is sufﬁciently large, then
the optimization problem (21.73) simply yields the ﬁrst standard principal component of Y, and
when c1 is small, then the solution is sparse.
2. l1-norm penalized SPCA [64]
max
v {vT YT Yv −α∥v∥1} s.t. ∥v∥2
2 ≤1,
(21.74)
where the penalty parameter α ≥0 controls the level of sparseness.
3. l0-norm penalized SPCA [57]
max
v {vT YT Yv −α∥v∥0} s.t. ∥v∥2
2 ≤1,
(21.75)
4. SPCA using Penalized Matrix decomposition (PMD) [61,64]
max
u,v {uT Yv},
s.t. ∥u∥2
2 ≤1, ∥v∥2
2 ≤1, P1(u) ≤c1, P2(v) ≤c2,
(21.76)
where positive parameters c1, c2 control sparsity level and the convex penalty functions P1(u)
and P2(v) can take a variety of different forms. Useful examples are:
P(v) = ∥v∥1 =
I

i=1
|vi| LASSO,
P(v) = ∥v∥0 =
I

i=1
|sign(vi)| CARDINALITY PENALTY,
P(v) =
I

i=1
|vi| + λ
I

i=2
|vi −vi−1| FUSED LASSO.
5. SPCA via regularized SVD (sPCA-rSVD) [54,65]
max
u,v {uT Yv −αP(v)} s.t. ∥u∥2
2 ≤1, ∥v∥2
2 ≤1,
(21.77)
6. Two-way functional PCA/SVD [66]
max
u,v {uT Yv −α
2 P1(u)P2(v)},
s.t. ∥u∥2
2 ≤1, ∥v∥2
2 ≤1.
(21.78)

1172
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
7. Sparse SVD [67]
max
u,v

uT Yv −1
2uT uvT v −α1
2 P1(u) −α2
2 P2(v)

.
(21.79)
8. Generalized SPCA [68]
max
u,v

uT QYRv −α1
2 P1(u) −α2
2 P2(v)

,
s.t. uT Qu ≤1, vT Rv ≤1,
(21.80)
where Q ∈RT ×T and R ∈RI×I are symmetric positive deﬁnite matrices.
9. Generalized nonnegative SPCA [63]
max
u,v {uT YRv −α∥v∥1},
s.t. uT u ≤1, vT Rv ≤1, v ≥0.
(21.81)
10. Robust SPCA [69]
max
v {Var(vT y(1), vT y(2), . . . , vT y(T )) −α∥v∥1},
s.t. vT v = 1,
(21.82)
where y(t) ∈RI (t = 1, 2, . . . , T ) are multivariate observations collected in the rows of the data
matrix Y ∈RT ×I and Var is a robust variance measure.
There exist some connections between the above listed optimizations problems, and in fact they
often deliver almost identical or at least similar solutions. All the optimization problems addressed
here are rather difﬁcult, generally non-convex maximization problems, and we can not make a claim
with respect their global optimality. Even if the goal of obtaining a local minimizer is, in general,
unattainable we must be content with convergence to a stationary point. However, using so formulated
optimization problems, the common and interesting feature of the resulting algorithms is, that they
provide a closed-form iterative updates (which is a generalized power method):
v =
S(YT Yv)
∥S(YT Yv)∥2
,
(21.83)
or
u =
S(Yv)
∥S(Yv)∥2
,
v =
S(YT u)
∥S(YT u)∥2
,
(21.84)
where S is a simple operator (shrinkage) which can be written in explicit form and can be efﬁciently
computed [54,60].
For example, by applying the optimization problem (21.77) we obtain the sparse SVD algorithm.
Sparse SVD can be performed by solving the following optimization problem:
max
u,v {uT Yv −α1∥u∥1 −α2∥v∥1}
(21.85)
s.t. uT u ≤1, vT v ≤1,
(21.86)

1.21.2 PCA/SCD and Related Problems
1173
which leads to the following update formulas:
u =
S1(Yv, α1)
∥S1(Yv, α1)∥2
,
v =
S1(YT u, α2)
∥S1(YT u, α2)∥2
,
(21.87)
where the non-negative parameters α1α2 control sparsity level and the operator S1 means the shrinkage
function deﬁned as
S1(Yv, α) = sign(Yv)[|Yv| −α]+.
(21.88)
In fact, when S is the identity operator, the above update schemes are nothing else but power method
for the standard PCA/SVD (see Algorithm 1).
Algorithm 1. Power Method for Sparse PCA/SVD
Require: Data matrix Y(1) = Y,
sparsity coefﬁcients αn ≥0, (n = 1, 2), initial vectors u and v
1: For j = 1, 2, . . . , J:
Repeat until convergence of u j and v j;
2:
u j =
S1(Y( j)v j,α1)
∥S1(Y( j)v j,α1)∥2
;
3:
v j =
S1(Y( j)T u j,α2)
∥S1(Y( j)T u j,α2)∥2
;
4:
σ j = uT
j Y( j)v j;
5:
Y( j+1) = Y( j) −σ ju jvT
j ;
6: return U∗= [u1, u2, . . . , uJ ], V∗= [v1, v2, . . . , vJ ], ∗= diag{σ1, . . . , σJ }.
In contrast to standard PCA, SPCA enables to reveal often multi-scale hierarchical structures from the
data [62]. For example, for EEG data the SPCA generates spatially localized, narrow bandpass functions
as basis vectors, thereby achieving a joint space and frequency representation, what is impossible to
obtain by using standard PCA. The algorithm for sparse PCA/SVD can be easily extended to sparse
CCA/PLS (see Section 1.21.2.6.6).
1.21.2.6.4
Deﬂation techniques for SPCA
In sparse PCA the eigenvectors are not necessary mutually orthogonal. In fact, we usually alleviate or
even completely ignore the orthogonality constraints. Due to this reason the standard Hotelling deﬂation
procedure:
R( j+1)
y y
←R( j)
y y −λ jv jvT
j
( j = 1, 2, . . . , J),
(21.89)
does not guarantee the positive semi-deﬁniteness of the covariance matrix.
Due to this reason we usually use the Schur compliment deﬂation [70]:
R( j+1)
y y
←R( j)
y y −R( j)
y y v jvT
j R( j)
y y /λ j
(21.90)
or alternatively the projection deﬂation
R( j+1)
y y
←(I −v jvT
j )R( j)
y y (I −v jvT
j ).
(21.91)

1174
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
Empirical studies by Mackey [70] indicate that the Schur deﬂation and the projection deﬂation outper-
form the standard Hotelling’s deﬂation consistently for SPCA, and usually the projection deﬂation is
even better than (or at least comparable with) the Schur deﬂation.
Algorithm 2. Deﬂation Algorithm for Sparse PCA [70]
Require: Covariance matrix R(0)
y y = Ry y and J,
1: (1) Q0 ←I,
2: For j = 1, 2, . . . , J:
3:
v j = argmax{vT R( j−1)v}
s.t. ∥v∥1 ≤1;
4:
q j = Q j−1v j;
5:
R( j) ←(I −q jqT
j )R( j−1)
y y
(I −q jqT
j );
6:
Q( j) = Q( j−1)(I −q jqT
j );
7:
v( j) = v( j)/∥v( j)∥2;
8: return V∗= [v1, v2, . . . , vJ ].
1.21.2.6.5
Kernel PCA
Standard PCA applies linear transformation for dimensionality reduction and may not be effective for
nonlinear data. In kernel PCA we apply implicitly nonlinear transformation to potentially very high
dimensional feature space F.
Let consider a data matrix X = [x1, x2, . . . , xN] ∈RI×N with vectors xn ∈RI (typically, with
N < I), which are nonlinearly projected feature subspace
φ : xn ∈RI →φ(xn) ∈F.
(21.92)
The covariance matrix in the feature space can be expressed as
ˆC = 1
N (X)T (X) = 1
N
N

n=1
φ(xn)φT (xn),
(21.93)
where (X) = [φ(x1), φ(x1), . . . , φ(xN)] (we assume without loos of generality that 
n φ(xn) = 0).
However, in practice, we almost never compute the nonlinear mapping φ(x) explicitly, since the feature
space can be very high dimensional in the kernel method, but rather exploit so called “kernel trick”, i.e.,
employ the nonlinear mapping implicitly [71].
Let v j be eigenvector of the covariance matrix ˆRxx = 1/N N
n=1 xnxT
n , then the vector v j belongs
to linear space spanned by the data points xn (n = 1, 2, . . . , N) since Rxxv j = λ jv j implies that
v j = 1/(λ j N) N
n=1 xn(xT
n v) = N
n=1 α jnxn. So, the standard PCA can be formulated in dual form
as XT Xα j = Nλ jα j, where K = XT X ∈RN×N is referred to as linear kernel matrix with entries
knm = xT
n xm expressed by inner products.
Similarly, eigenvector v j of the covariance matrix ˆC deﬁned by Eq. (21.93) can be written as
v j = N
n=1 α jnφ(xn). Taking into account that
ˆCv j = λ jv j
( j = 1, 2, . . . , J)
(21.94)

1.21.2 PCA/SCD and Related Problems
1175
we can write
 N

n=1
φ(xn)φT (xn)
  N

m=1
αmφ(xm)

= λ j
 N

m=1
α jmφ(xm)

.
(21.95)
By multiplying the both side of the above equation by φT (xn), we obtain
N

n=1
N

m=1
φT (xn)φ(xn)φT (xn)φ(xm)α jm = λ j
N

m=1
α jmφT (xn)φ(xm),
(21.96)
which can be written in a compact matrix form as K2α j = λ jKα j or equivalently
Kα j = λ jα j,
(21.97)
where K is a kernel positive deﬁnite symmetric matrix with entries deﬁned as knm = φT (xn)φ(xm).
We can estimate the kernel PCA features for new sample as
y j(x) = φT (x)v j =
N

n=1
α jnk(x, xn)
(21.98)
Summarizing, the Kernel PCA algorithm can be formulated as follows:
1. Step 1: Select kernel function (e.g., Gaussian kernel: knm(xn, xm) = exp ( −∥xn −xm∥2/2σ 2)
or polynomial kernel: knm(xn, xm) = (xT
n xm + c)d) and compute the kernel matrix with entries
knm = φT (xn)φ(xm) (n, m = 1, 2, . . . , N) for training set.
2. Step 2: Compute eigenvalues and eigenvectors pairs of K as {λ j, α j} ( j = 1, 2, . . . , J).
3. Step 3: Normalize the eigenvectors α j ←α j/λ j.
4. Step 4: To project a test feature φ(x) onto v j we need to compute y j(x) = φT (x)v j =
φT (x)( N
n=1 α jnφ(xn)) = N
n=1 α jnk(xn, x). So, explicit nonlinear mapping φ( · ) is not
required here.
1.21.2.6.6
Sparse CCA
The Canonical Correlation Analysis (CCA), introduced by Hotelling [72], can be considered as a
generalization or extension of PCA (see [73,74]. The standard CCA is a classical method for determining
the relationship between two sets of variables. Given two zero-mean (i.e., centered) data sets X ∈RI×N
and Y ∈RI×M on the same set of I observations, CCA seeks linear combinations of the variables in
X and the variables in Y that are maximally correlated with each other. Formally, the classical CCA
computes two projection vectors w(1)
x
= wx ∈RN and w(1)
y
= wy ∈RM such that the correlation
coefﬁcient
ρ =
wT
x XT Ywy

(wTx XT Xwx)(wTy YT Ywy)
(21.99)
is maximized.

1176
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
In a similar way, we can formulate a kernel CCA by replacing inner product matrices by kernel
matrices:
ρ = max
αx,αy
αT
x KT
x Kyαy

(αTx KTx Kxαx)(αTy KTy Kyαy)
,
(21.100)
where Kx ∈RI×I and Ky ∈RI×Iare suitably designed kernel matrices. The above optimization
problem can be solved by generalized eigenvalue decomposition.
Since ρ is invariant to the scaling of the vectors wx and wy, the standard CCA can be formulated
equivalently as the following constrained optimization problem:
max
wx,wy{(wT
x XT Ywy)2} s.t. wT
x XT Xwx = wT
y YT Ywy = 1.
(21.101)
We will refer to t1 = Xwx and u1 = Ywy as the canonical variables.
For sparse CCA we usually assume that the columns of X and Y have been standardized to have
mean zero and standard deviation one.
In order to obtain sparse CCA we must impose suitable sparsity constraints on the canonical vectors,
for example by applying the PMD approach [61,64]:
max
wx,wy{(wT
x XT Ywy)2} s.t. ∥wx∥2
2 ≤1, ∥wy∥2
2 ≤1, P1(wx) ≤c1, P2(wy) ≤c2,
(21.102)
where P1 and P2 are convex penalty functions and positive parameters c1, c2 control sparsity level (see
Section 1.21.2.6.3). Since P1 and P2 are generally chosen to yield sparse canonical vectors wx and wy,
we call this criterion the sparse CCA.
Note that since the data are standardized, cross product matrices XT X and YT Y are approximated
by identity matrices, and consequently the constraints wT
x XT Xwx ≤1 and wT
y YT Ywy ≤1 may be
replaced by ∥wx∥2
2 ≤1 and ∥wy∥2
2 ≤1, respectively [64].
The algorithms for the sparse CCA will be similar to sparse SVD/PCA (see Algorithm 3) in which
the data matrix Y in the SPCA should be replaced by Z = XT Y in the SCCA and the vectors u and v
by the vectors wx and wy, respectively.
In order to compute multiple canonical vectors for CCA we need to apply a suitable deﬂation in a
similar way as for SPCA. For example, we can compute the vectors w( j)
x
and w( j)
y
for j = 1, 2, . . . , J
by applying criterion (21.102) to the data sets:
Z(1) = XT Y,
Z( j+1) = Z( j) −(w( j)T
x
Z( j)w( j)
y )w( j)
x w( j)T
y
.
(21.103)
Note that the deﬂation and generalized power method do not impose automatically orthogonality con-
straints vectors w( j)
x
and w( j)
y . In order to enforce orthogonality for the subsequently computed on the
vectors w( j+1)
x
, we can apply the Gram-Schmidt orthogonalization method [68]:
w( j+1)
x
=
(I −W( j)
x W( j)T
x
)(XT Yw( j)
y )
∥(I −W( j)
x W( j)T
x
)(XT Yw( j)
y )∥2
( j = 1, 2, . . . ),
(21.104)
where W( j)
x
= [w(1)
x , w(2)
x , . . . , w( j)
x ] ∈RN× j. Similar formulas can be written for w( j+1)
y
.

1.21.2 PCA/SCD and Related Problems
1177
1.21.2.6.7
Sparse partial least squares
The Partial Least Squares (PLS) methods, originally developed in chemometrics and econometrics
by Herman Wold and his coworkers (see [75] and references therein) are particularly suitable for the
analysis of relationships among multi-modal brain data (e.g., EEG, MEG, ECoG (electrocorticogram),
fMRI) or relationships between measures of brain activity and behavior data [76,77]. The standard
PLS approaches have been recently summarized in [75,78] and their suitability to model relationships
between brain activity and behavior (experimental design) has been highlighted in [76].
There are two related basic PLS methods: PLS correlation (PLSC), which analyzes correlations or
(associations) between two or more sets of data (e.g., two modalities brain data or brain and behavior
data) and PLS regression (PLSR) methods, which attempt to predict one set of data from another
independent set of data that constitutes the predictors (e.g., predict experimental behavior data from
brain data such as multichannel ECoG or scalp EEG from ECoG by performing simultaneous recordings
for epileptic patients).
In order to predict the response variables represented by the matrix Y from the independent variables
X, the standard PLSR techniques ﬁnd a set of common orthogonal latent variables (also called latent
vectors, score vectors or components) by projecting both X and Y onto a subspace, which ensures a
maximal covariance between the latent variables of X and Y. In other words, the prediction is achieved
by simultaneous approximative decompositions training data sets: X ∈RI×N and Y ∈RI×M into
factor matrices (components) (T = [t1, t2, . . . , tJ] ∈RI×J, P = [p1, p2, . . . , pJ] ∈RN×J and
U = [u1, u2, . . . , uJ] ∈RI×J, Q = [q1, q2, . . . , qJ] ∈RM×J), respectively:
X = TPT + E = J
j=1 t jpT
j + E,
(21.105)
Y = UQT + F = J
j=1 u jqT
j + F,
(21.106)
with the constraint that these components explain as much as possible of the covariance between X and
Y [64,77]. In other words, the key concept of basic PLS algorithms is to maximize covariance between
the vectors t j and u j expressed in a general form as:
max{g(tT
j u j)} ∀j,
(21.107)
subject to suitable constraints, where the function g( · ) may take different forms: the identity, the
absolute value or the square function, and we deﬁne the vectors as t j = Xw( j)
x
and u j = Yw( j)
y .
More generally, the objective of the standard PLS is to ﬁnd in the ﬁrst step directions deﬁned by
Wx = [w(1)
x , w(2)
x , . . . , w(J)
x ] ∈RN×J and Wy = [w(1)
y , w(2)
y , . . . , w(J)
y ] ∈RM×J that maximize the
set of cost functions (see Figure 21.4):
max
w( j)
x ,w( j)
y
{g(w( j)T
x
XT Yw( j)
y )} s.t. ∥w( j)
x ∥2 = 1,
∥w( j)
y ∥2 = 1,
(21.108)
w( j)T
x
XT Xw(k)
x
= 0,
w( j)T
y
YT Yw(k)
y
= 0 ∀k < j
( j = 1, 2, . . . , J).

1178
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
(
)
I
M
Y
U
Q
T
(
)
I
J
(
)
J
M
(
)
I
M
F
j=1
J
uj
qj
T
(
)
I
M
F
(
)
I
N
X
T
(
)
I
J
P
T
(
)
J
N
(
)
I
M
F
E
(
)
I
N
j=1
J
tj
(
)
I
N
E
pj
T
FIGURE 21.4
Illustration of a basic PLS model. In this model X is a matrix of predictors, Y is the response matrix, P and Q
are loading matrices for the predictor and response with corresponding T and U ≈TD component matrices,
and matrices E and F are corresponding error terms. Our objective is to perform approximative decompositions
of the matrices X and Y and by imposing additional orthogonality and optional sparsity constraints.
In other words, the basic approach to the standard PLS is to ﬁnd these direction vectors w( j)
x
and w( j)
y
from successive optimizations problems (see also Eq. (21.102)). The latent components can be deﬁned
as T = XWx and U = YWy, or equivalently t j = Xw( j)
x
and u j = Yw( j)
y
for j = 1, 2, . . . , J.
The main difference between PLS and CCA is that the CCA maximize the correlation while the PLS
maximize the covariance. Moreover, in the PLS the primary objective is to estimate matrices T and U
while in the CCA we estimate only projection matrices Wx and Wy. Anyway, the PLS and CCA are
quite similar for the standardized data (that is after the columns of X and Y have been standardized to
have mean zero and standard deviation) one the both method are equivalent.
The PLS decomposition allows us to ﬁnd relationships between the components which can be used to
predict the values of the dependent variables Ynew for new situations, if the values of the corresponding
independent variables Xnew are available:
Tnew = Xnew[PT ]†,
Ynew ≈TnewDQT .
(21.109)
The predictive relationship between the two sets of data is driven by a linear regression model
involving J pairs of latent variables: u j = d jt j + ˜e j, (or in a matrix form U = TD + ˜E), where d j is
a scaling regression coefﬁcient and ˜e j represents residual errors [79]. Using this basic inner regression
model the standard PLS model can be expressed as
Y =
J

j=1
d jt jqT
j + E = XB + E
(21.110)
where the regression coefﬁcients matrix has been deﬁned as B = J
j=1 d jw( j)
x qT
j .

1.21.2 PCA/SCD and Related Problems
1179
Algorithm 3. Power Method for Sparse CCA/PLS
Require: Normalized (standardized) data matrices X(1) = X, Y(1) = Y, sparsity coefﬁcients αn ≥0
(n = 1, 2), initial vectors w(1)
x
and w(1)
y
and Z(1) = X(1)T Y(1),
1: For j = 1, 2, . . . , J:
(a) Repeat until convergence of w( j)
x
and w( j)
y ;
2:
w( j)
x
=
S1(Z( j)w( j)
y ,α1)
∥S1(Z( j)w( j)
y ,α1)∥2
;
3:
w( j)
y
=
S1(Z( j)T w( j)
x ,α2)
∥S1(Z( j)T wx,α2)∥2 ;
4: (b) Apply optional orthogonalization procedure according to Eq. (21.104)
5: (c) σ j = |w( j)T
x
Z( j)w( j)
y |;
6: (d) Z( j+1) = Z( j) −σ jw( j)
x w( j)T
y
;
7: return W∗x = [w(1)
x , w(2)
x , . . . , w(J)
x
], W∗y = [w(1)
y , w(2)
y , . . . , w(J)
y
],
∗= diag{σ1, . . . , σJ };
8: return Optional T = XWx, U = YWy
Assuming that the vectors component matrices are estimated sequentially (using deﬂation approach),
the loading factors and regression coefﬁcient can be calculated as:
q j = YT u j
uT
j u j
,
d j =
tT
j u j
tT
j t j
( j = 1, 2, . . . , J).
(21.111)
Often, we impose additional (optional) sparseness and/or orthogonality constraints (imposed on
components matrices T and U or the loading matrices), in order to achieve better predictive power.
For high dimensional data sets, it is often appropriate to assume that independent and dependent data
has unit covariance matrices, i.e., XT X ≈IN and YT Y ≈IM. In the case of the sparse PLS, we can
formulate as the following optimization problem
max
w(1)
x ,w(1)
y

g(w(1)T
x
XT Yw(1)
y ) −α1∥w(1)
x ∥1 −α2∥w(1)
y ∥1

(21.112)
s.t. ∥w(1)
x ∥
2
2 ≤1,
∥w(1)
y ∥
2
2 ≤1.
The PLS algorithms have been proven to be particularly powerful for strongly collinear data and for
underdetermined independent data (i.e., more variables than observations) [77]. The widely reported
sensitivity to noise of the PLS regression is attributed to redundant (irrelevant) latent variables, their
selection remains an open problem. Other problems include the difﬁculty of interpreting the loadings
corresponding to the latent variables, and the fact that the number of latent variables cannot exceed the
minimum number of predictors and observations.
Sparse PCA, CCA, and PLS deliver components (or latent vectors) that can be considered as a
compromise between maximizing the variance, correlation or covariance, respectively and improving
the interpretability of the data, especially for large-scale data with possibly fewer observations than

1180
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
Algorithm 4. Alternative Algorithm for Sparse PLS
Require: Normalized (standardized) data matrices X(1) =X, Y(1) = Y, sparsity coefﬁcients αn ≥0, (n =1, 2);
1: For j = 1, 2, . . . , J:
Set (a) Z( j) = X( j)T Y( j) and use the SVD to compute the ﬁrst pair of singular vectors:
the left singular vector w( j)
x
and the right singular vector w( j)
y ;
(b) Repeat until the convergence of sparse w( j)
x
and w( j)
y ;
2:
w( j)
x
←S1(Z( j)w( j)
y , α1), normalize new vector w( j)
x ;
3:
w( j)
y
←S1(Z( j)T w( j)
x , α2), normalize new vector w( j)
y ;
4: (b) Apply optional orthogonalization procedure according to Eq. (21.104)
5: (c) t j = X( j)w( j)
x
w( j)T
x
w( j)
x
,
u j = Y( j)w( j)
y
w( j)T
y
w( j)
y
;
6: (d) p j = X( j)T t j
tT
j t j
,
q j = Y( j)T u j
uT
j u j ;
7: (e) X( j+1) = X( j) −t jpT
j ;
8:
Y( j+1) = Y( j) −u jqT
j ;
9: return Wx, Wy, T, P, U, Q.;
variables. The extraction of components and latent variables sequentially, one by one, by applying
deﬂation techniques, allows us to stop the algorithms after achieving the desired number of components
and/or achieving an error measure below a speciﬁed threshold.
1.21.3 ICA and related problems
There are at least three basic approaches for ICA, coming from different models of source signals
[13,14,17]. The ﬁrst simple model considers weakly stationary Gaussian processes and exploits directly
the second order statistics (SOS). These signals are separable if their spectra are distinct; therefore, the
separation is based on the spectral diversity of the sources. The second approach takes the nonstationarity
of the signals into account by modeling them as independently distributed Gaussian variables whose
variance is changing in time. For the third basic approach one assumes that source signals are represented
by sequence of identically and independently distributed random variables. The condition of separability
of such source signals requires that at most only one signal is Gaussian, so the approach is said to be
based on the non-Gaussianity and it exploits higher order statistics (HOS). Strictly speaking the ﬁrst
two approaches do not explicitly or implicitly assume statistical independence but rather generalized
decorrelation via simultaneous joint diagonalization of time-delayed covariance matrices.
Spatial decorrelation (or prewhitening) is often considered a necessary (but not sufﬁcient) condition
for stronger stochastic independence criteria. After prewhitening ICA tasks usually become somewhat
easier and well-posed (or less ill-conditioned), because the subsequent separating (unmixing) system is
described by an orthogonal matrix for real-valued signals and by a unitary matrix for complex-valued
signals and weights. Furthermore, the spatio-temporal and time-delayed decorrelation can be used to
identify the mixing matrix and to perform blind source separation of colored sources under certain weak

1.21.3 ICA and Related Problems
1181
conditions [13,14,17,18,80]. Temporal, spatial and spatio-temporal decorrelations play important roles
in EEG/MEG data analysis. These techniques are based only on second-order statistics (SOS). They are
the basis for modern subspace methods of spectrum analysis and array processing and are often used in
a preprocessing stage in order to improve the convergence properties of adaptive systems, to eliminate
redundancy or to reduce noise. Spatio-temporal data has both spatial (i.e., location) and temporal (i.e.,
time related) components.
One possible extension of the ICA is the so-called Independent Subspace Analysis (ISA). ISA
can be applied to situations where not all source signals can be separated from each other by linear
transformations. The goal here is to decompose the linear space spanned by rows of Y to a direct
(orthogonal) sum of linear subspaces, such that elements of each subspace are statistically independent
of the others. The ISA problem can be approached by applying an ICA algorithm, which aims to separate
each component from the others as much as possible. In the postprocessing step we usually perform
clustering of the obtained components according to their residual mutual dependence.
1.21.3.1 The AMUSE algorithm and its properties
AMUSE (Algorithm for Multiple Unknown Signal Extraction) is probably the simplest BSS algorithm
which belongs to the group of Second-Order Statistics Spatio-Temporal Decorrelation (SOS-STD)
algorithms [14,81]. It provides similar decompositions for noiseless data to the well known and popular
SOBI and TDSEP algorithms [82,83]. This class of algorithms are sometimes classiﬁed or referred
to as ICA algorithms. However, these algorithms do not exploit implicitly or explicitly the statistical
independence. Moreover, in contrast to standard higher order statistics ICA algorithms, they are able to
estimate colored Gaussian distributed sources and their performance in estimation of original sources
is usually better if the sources have temporal structures.
The AMUSE algorithm has some similarity with the standard PCA. The main difference is that the
AMUSE employs PCA two times in two separate steps: In the ﬁrst step, standard PCA is applied for the
whitening (sphering) of the data and in the second step PCA is applied to the time delayed covariance
matrix of the pre-whitened data. Mathematically the AMUSE algorithm consists of the following two
stage procedure: In the ﬁrst step we apply standard or robust prewhitening as a linear transformation
y(t) = Qy(t), where Q = R−1/2
y y
= (VVT )−1/2 = V()−1/2VT of the standard covariance matrix
Ry y = E{y(t)yT (t)}, and y(t) is a vector of observed data for time point t. Next, (for the pre-whitened
data)theSVDisappliedforthetime-delayedcovariancematrixRy(p) = E{y(t)yT (t−p)} = U11VT
1
(typically, with p = 1), where 1 is a diagonal matrix with decreasing singular values and U1, V1 are
orthogonal matrices containing the of left and right singular vectors. Then, an unmixing (separating)
matrix is estimated as W = UT
1 Q.
The main advantages of the AMUSE algorithm over the other BSS/ICA algorithms is its simplicity
and that it allows the automatic ordering of the components, due to application of SVD. In fact, the
components are ordered according to the decreasing values of the singular values of the time-delayed
covariance matrix. In other words, the AMUSE algorithm exploits the simple principle that the estimated
components tends to be less complex or more precisely that they have better linear predictability than any
mixtureofthosesources.ItshouldbeemphasizedthatallcomponentsestimatedbyAMUSEareuniquely
deﬁned and consistently ranked. The consistent ranking is due to the fact that these singular values are
always ordered in decreasing values. For real-world data probability that two singular values achieve

1182
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
exactly the same value is very small, so ordering is consistent and unique [14]. The one disadvantage
of the AMUSE algorithm it is that is very sensitive to additive noise since the algorithm exploits only
one time delayed covariance matrix.
1.21.3.2 The SOBI algorithm and its extensions
The second-order blind identiﬁcation (SOBI) algorithm is a classical blind source separation (BSS)
algorithm for wide-sense stationary (WSS) processes with distinct spectra [82]. This algorithm has
proven to be very useful in biomedical applications and it has became more popular than the other
algorithms.
There is a common practice in ICA/BSS research to exploit the “average eigen-structure” of a large
set of data matrices formed as functions of the available data (typically, covariance or cumulant matrices
for different time delays). In other words, the objective is to extract reliable information (e.g., estimation
of sources and/or of the mixing matrix) from the eigen-structure of a possibly large set of data matrices
[14,82,84,85]. However, since in practice we only have a ﬁnite number of samples of signals corrupted
by noise, the data matrices do not exactly share the same eigen-structure. Furthermore, it should be
noted that determining the eigen-structure on the basis of one or even two data matrices usually leads
to poor or unsatisfactory results, because such matrices, based usually on an arbitrary choice, may have
degenerate eigenvalues leading to loss of the information contained in other data matrices. Therefore,
from a statistical point of view, in order to provide robustness and accuracy, it is necessary to consider
the average eigen-structure by taking into account simultaneously a possibly large set of data matrices
[14,85].
The average eigen-structure can be easily implemented via the linear combination of several time-
delayed covariance matrices and by applying the standard EVD or SVD. An alternative approach to
EVD/SVD is to apply the approximate joint diagonalization procedure (JAD). The objective of this
procedure is to ﬁnd the orthogonal matrix U which diagonalizes a set of matrices [82,84,85]:
Ry(pr) = UDrUT + εr
(r = 1, 2, . . . , R),
(21.113)
where Ry(pr) = E{y(t)yT (t −pr)} ∈RJ×J are data matrices (for example, time-delayed covariance
matrices), the Dr are diagonal and real, and εr represent additive errors (as small as possible). If R > 2
for arbitrary matrices Rx(pr), the problem becomes overdetermined and generally we can not ﬁnd
an exact diagonalizing matrix U with εr = 0, ∀r. An important advantage of the Joint Approximate
Diagonalization(JAD)isthatseveralnumericallyefﬁcientalgorithmsexistforitscomputation,including
Jacobi techniques (one sided and two sided), Alternating Least Squares (ALS), PARAFAC (Parallel
Factor Analysis) and subspace ﬁtting techniques employing the efﬁcient Gauss–Newton optimization
[85]. This idea has been implemented as a robust SOBI algorithm which can be brieﬂy outlined as
follows [14,86]:
1. Perform robust orthogonalization ˜y(t) = Qy(t), similar as to the AMUSE algorithm.
2. Estimate the set of covariance matrices:
Ry(pr) = (1/T )
T

t=1
˜y(t)˜yT (t −pr) = QR˜y(pr)QT
(21.114)
for a preselected set of time lags (p1, p2, . . . , pR) or bandpass ﬁlters Br.

1.21.3 ICA and Related Problems
1183
3. Perform JAD: Ry(pr) = UDrUT , ∀r, i.e., estimate the orthogonal matrix U using one of the
available numerical algorithms.
4. Estimate the source signals asx(t) = UT Qy(t), and then the mixing matrix as A = Q+U.
The main advantage of the SOBI algorithm is its robustness with respect to additive noise if number
of covariance matrices is sufﬁciently large (typically, R > 100).
Several extensions and modiﬁcations of the SOBI algorithm have been proposed, e.g., Robust SOBI
[14], thinICA [87], and WASOBI [88,89] (the weights-adjusted variant of SOBI), which is asymp-
totically optimal (in separating Gaussian parametric processes). The WASOBI for the separation of
independent stationary sources is modeled as Auto-Regressive (AR) random processes. The matrices
are computed for the case of Gaussian sources, for which the resulting separation would be asymptoti-
cally optimal approaching the corresponding Cramer-Rao bound (CRB) for the best possible separation
[88,90].
1.21.3.3 ICA based on higher order statistics (HOS)
The ICA of a random vector y(t) ∈RIcan be performed by ﬁnding an J ×I, (with I ≥J), full rank sep-
arating (transformation) matrix W, such that the output signal vectorx(t) = [x1(t),x2(t), . . . ,xJ(t)]T
(components) estimated by
ˆx(t) = Wy(t) = A† y,
(21.115)
are as independent as possible as evaluated by an information-theoretic cost function such as the mini-
mum of the Kullback-Leibler divergence [14,17,91,92].
The statistical independence of random variables is a more general concept than decorrelation.
Roughly speaking, we say that the random variables xi and x j are statistically independent if knowledge
of the value of xi provides no information about the values of x j. Mathematically, the independence of
xi and x j can be expressed by the relationship
p(xi, x j) = p(xi)p(x j),
(21.116)
where p(x) denotes the probability density function (pdf) of the random variable x. In other words,
signals are independent if their joint pdf can be factorized.
If independent signals are zero-mean, then the generalized covariance matrix of f (xi) and g(x j),
where f (x) and g(x) are different, odd nonlinear activation functions (e.g., f (x) = tanh (x) and
g(x) = x for super-Gaussian sources) is a non-singular diagonal matrix:
Rfg = E{f(x)gT (x)} =
⎡
⎢⎣
E{ f (x1)g(x1)}
0
...
0
E{ f (xJ)g(xJ)}
⎤
⎥⎦,
(21.117)
i.e., the covariances E{ f (xi)g(x j)} are all zero. It should be noted that for odd f (x) and g(x), if
the probability density function of each zero-mean source signal is even, then the terms of the form
E{ f (xi)}E{g(xi)} equal zero. The true general condition for the statistical independence of signals is the
vanishing of all high-order cross-cumulants [93–95]. The diagonalization principle can be expressed as
R−1
f g = −1,
(21.118)

1184
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
where  is any diagonal positive deﬁnite matrix (typically,  = I). By pre-multiplying the above
equation by the matrices W and , we obtain:
R−1
f g W = W,
(21.119)
which suggests the following simple iterative multiplicative learning algorithm [96]
˜W(k + 1) = R−1
f g W(k),
(21.120)
W(t + 1) = ˜W(t + 1)[ ˜WT (t + 1) ˜W(t + 1)]−1/2,
(21.121)
where the last equation represents the symmetric orthogonalization that keeps the algorithm stable. The
above algorithm is very simple, but it needs the prewhitening of the sensor data and it is sensitive to
noise [14,97].
Assuming prior knowledge of the source distributions
p j(x j), we can estimate W
=
[w1, w2, . . . , wJ]T by using maximum likelihood (ML):
L = T log | det W| +
T

t=1
J

j=1
log p j(wT
j y(t)).
(21.122)
Using the gradient descent of the likelihood we obtain the infomax update rule
W(k + 1) = W(k) −η ∂L
∂W = W(k) + η([WT (k)]−1 −1
T
T

t=1
f(ˆx(t))yT (t),
(21.123)
where ˆx(t) = W(k)y(t) and f(x) = [ f1(x1), f2(x2), . . . , fJ(xJ)]T is an entry-wise nonlinear score
function deﬁned by:
f j(x j) = −
p′
j(x j)
p j(x j) = −d log (p j(x j)
d(x j)
.
(21.124)
Using the natural gradient descent to increase the likelihood we get [98]:
W = W(k + 1) −W(k) = −η ∂L
∂WWT W
= η[I −⟨f(x)xT ⟩]W(k).
(21.125)
Alternatively, for signals corrupted by additive Gaussian noise, we can use higher order matrix
cumulants. As an illustrative example, let us consider the following cost function which is a measure
of independence [99]:
J(W,x) = −1
2 log | det (WWT )| −
1
1 + q
n

i=1
|C1+q(xi)|,
(21.126)
where we use the following notations: Cq(x1) denotes the q-order cumulants of the sig-
nal xi and Cp,q(x,x) denotes the cross-cumulant matrix whose elements are [Cpq(x,x)]i j
=
Cum(xi,xi, . . . ,xi



p
,x j,x j, . . . ,x j



q
).

1.21.3 ICA and Related Problems
1185
Table 21.1 Basic Equivariant Adaptive Learning Algorithms for ICA. x(t) = Wy(t) = A
† y(t)
No.
Learning Algorithm
References
1.
W = η[ −⟨f(x)gT (x)⟩]W
[93,95]
A = −ηA[ −⟨f(x)gT (x)⟩]
2.
W = η[ −⟨f(x)xT ⟩]W,
f (yi ) = −p′(yi )/p(yi )
[91]
λii = ⟨f (yi (k))yi (k)⟩or λii = 1, ∀i
[98]
3.
W = η[I −⟨xxT ⟩−⟨f(x)xT ⟩+ ⟨xfT (x)⟩]W
[100]
4.
W = η[I −⟨xxT ⟩−⟨f(x)xT ⟩+ ⟨f(x)fT (x)⟩]W
[45]
5.
˜W = W + η[ −⟨f(x)xT ⟩]W, λii = ⟨f (yi )yi ⟩
[17]
ηii = [λii + ⟨f ′(yi )⟩]−1;
W = ( ˜W ˜WT )−1/2 ˜W
6.
W = η[I −−1⟨xxT ⟩]W
[101]
λii (k) = ⟨x2
i (t))⟩
[28]
7.
W(k + 1) = [I ∓η[I −⟨f(x)gT (x)⟩]]∓1W(k)
[102]
8.
W = η[I −C1,q(x,x)Sq+1(x)]W
[99]
C1,q(xi , xj ) = Cum(yi , xj , . . . , xj



q
)
The ﬁrst term in (21.126) assures that the determinant of the global matrix will not approach zero. By
including this term we avoid the trivial solution xi = 0, ∀i. The second term forces the output signals
to be as far as possible from Gaussianity, since the higher order cumulants are a natural measure of
non-Gaussianity and they will vanish for Gaussian signals. It can be shown that for such a cost function,
we can derive the following equivariant and robust (with respect to the Gaussian noise) algorithm
[87,99,102]
W(t) = W(k + 1) −W(k) = ηl[I −C1,q(x,x)Sq+1(x)]W(k),
(21.127)
where Sq+1(x) = sign(diag(C1,q(x,x))) and F(x) = I −C1,q(x,x)Sq+1(x).
A wide class of equivariant algorithms for ICA can be expressed in a general form as (see
Table 21.1) [14]
∇W(k) = W(k + 1) −W(k) = ηF(x)W(k),
(21.128)
where ˆx(t) = Wy(t) and the matrix F(x) can take different forms, for example F(x) = −⟨f(x)gT (y)⟩
with suitably chosen nonlinearities f(x) = [ f (x1), . . . , f (xJ)] and g(x) = [g(x1), . . . , g(xJ)]
[14,95,96,99].
It should be noted that ICA based on HOS can perform blind source separation, i.e., allows us to
estimate the true sources only if they are all statistically independent and are non Gaussian (except
possibly of one) [14,91].
1.21.3.4 Blind source extraction
There are two main approaches to solve the problem of blind source separation. The ﬁrst approach, which
was mentioned brieﬂy in the previous section, is to simultaneously separate all sources. In the second

1186
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
approach, we extract the sources sequentially in a blind fashion, one by one, rather than separating them
all simultaneously. In many applications a large number of sensors (electrodes, sensors, microphones
or transducers) are available but only a very few source signals are the subject of interest. For example,
in the modern EEG or MEG devices, we typically observe more than 100 sensor signals, but only a
few source signals are interesting; the rest can be considered as interfering noise. In another example,
the cocktail party problem, it is usually essential to extract the voices of speciﬁc persons rather than
separate all the source signals of all speakers available (in mixing form) from an array of microphones.
For such applications it is essential to apply reliable, robust and effective learning algorithms which
enable us to extract only a small number of source signals that are potentially interesting and contain
useful information.
The blind source extraction (BSE) approach has several advantages over the simultaneous blind
separation/deconvolution, such as
•
Only the “interesting” signals need to be extracted. For example, if the source signals are mixed
with a large number of noise terms, we may extract only the speciﬁc signals which possess some
desired statistical properties.
•
The signals can be extracted in a speciﬁed order according to the statistical features of the source
signals, e.g., in the order determined by the absolute values of the generalized normalized kurtosis.
Blind extraction of sources can be considered as a generalization of the sequential extraction of
principal components, where uncorrelated output signals are extracted according to the decreasing
order of their variances.
•
The available unsupervised learning algorithms for BSE are local, stable and typically biologically
plausible.
We can use two different criteria. The ﬁrst criterion is based on higher order statistics (HOS), which
assumes that the sources are mutually statistically independent and non-Gaussian (at most one can be
Gaussian). For independence criteria we will use some measure of non-Gaussianity [14]. The second
criterion is based on the concept of linear predictability and it assumes that the source signals have some
temporal structure, i.e., the sources are colored with different autocorrelation functions, or equivalently
they have different spectral shapes.
1.21.3.4.1
Blind extraction of sources with temporal structures
In this approach we exploit the temporal structure of the signals rather than their statistical independence
[103,104]. Intuitively speaking, the source signals x j(t) have less complexity than the mixed sensor
signals y j(t). In other words, the degree of temporal predictability of any source signal is higher than
(or equal to) that of any mixture. For example, waveforms of a mixture of two sine waves with different
frequencies are more complex (or less predictable) than either of the original sine waves. This means
that applying the standard linear predictor model and minimizing the mean squared error E{ε2}, which
is a measure of predictability, we can separate (or extract) signals with different temporal structures.
More precisely, by minimizing the error, we maximize a measure of temporal predictability for each
recovered signal [105].

1.21.3 ICA and Related Problems
1187
A (z)
1
A (z)
J
+
+
A
Q
Learning
Algorithm
+
+
B (z)
j
+
-
Unknown
+
+
AR model of sources
Mixing
Robust
Prewhitening
Blind Extraction
x ( )t
~
1
y~
t( )
J
y
t( )
1
x ( )t
1
x ( )t
J
x ( )t
~
J
y
t( )
I
y~
t( )
1
wj1
wjJ
x
t( )
j
xˆ~
ˆ
t( )
j
t
ε ( )
j
FIGURE 21.5
Block diagram illustrating the implementation of learning algorithm for the blind extraction of temporally
correlated sources.
It is worth noting that two criteria used in BSE, namely temporal linear predictability and non-
Gaussianity based on kurtosis, may lead to different results. Temporal predictability forces the extracted
signals to be smooth and possibly less structurally complex, while the non-Gaussianity measure forces
the extracted signals to be as independent as possible, with sparse representation for the sources that
have positive kurtosis.
Let us assume that temporally correlated source signals are modeled by autoregressive processes
(AR) (see Figure 21.5) as
x j(t) = x j(t) +
L

p=1
a jpx j(t −p) = x j(t) + A j(z)x j(t),
( j = 1, 2, ..., J)
(21.129)
where A j(z) = L
p=1a jpz−p, z−px(t) = x(t −p) andx j(t) are i.i.d. unknown (innovative) processes.
In practice, the AR model can be extended to more general models such as the Auto Regressive Moving
Average (ARMA) model or the Hidden Markov Model (HMM) [14,17,101].
For ill-conditioned problems (when a mixing matrix is ill-conditioned and/or the source signals have
different amplitudes), we can apply optional pre-whitening to the sensor signals y(t) in the form of
y(t) = Qy(t), where Q ∈RJ×I is a whitening matrix that ensures that the auto-correlation matrix is
an identity matrix: Ryy = E{yyT } = IJ.
To model temporal structures of source signals, we consider a linear processing unit with an adaptive
ﬁlter with the transfer function B j(z) (which estimates one A j(z)) as illustrated in Figure 21.5. Let us
assume for simplicity, that we want to extract only one source signal, e.g., ˆx j(t), from the available

1188
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
sensor vector y(t). For this purpose, we employ a single processing unit described as
ˆx j(t) = wT
j y(t) =
J

i=1
w jiy j(t),
(21.130)
ε j(t) = ˆx j(t) −
L

p=1
b jp ˆx j(t −p) = wTy(t) −bT
j ¯x j(t),
(21.131)
where w j
=
[w j1, w j2, . . . , w j J]T ,¯x j(t)
=
[ˆx1(t −1), ˆx j(t −2), . . . , ˆx j(t −L)]T , b j
=
[b j1, b j2, . . . , b j L]T and B j(z) = L
p=1 b jpz−p is the transfer function of the corresponding FIR
ﬁlter. It should be noted that the FIR (Finite Impulse Response) ﬁlter can have a sparse vector b j. In
particular, only one single processing unit, e.g., with delay p and b jp ̸= 0 can be used instead of the L
parameters. The processing unit has two outputs: ˆx j(t) which estimates the extracted source signals, and
ε j(t), which represents a linear prediction error or estimation of the innovation, after passing the output
signal ˆx j(t) through the B j(z) ﬁlter. Our objective is to estimate the optimal values of the vectors w1
and b1, in such a way that the processing unit successfully extracts one of the sources. This is achieved
if the global vector deﬁned as g j = AT w j = (wT
j A)T = c j ei contains only one nonzero element,
e.g., in the jth row, such that x j(t) = c j xi(t), where c j is an arbitrary nonzero scaling factor. For this
purpose we reformulate the problem as a minimization of the cost function
D j(w j, b j) = E{ε2
j}.
(21.132)
The main motivation for applying such a cost function is the assumption that the primary source
signals (signals of interest) have temporal structures and can be modeled by an autoregressive model
[14,106,107].
According to the AR model of source signals, the ﬁlter output can be represented as ε j(t) = ˆx j(t)−
ˆx j(t), where ˆx j(t) = L
p=1 b jp ˆx j(t −p) is deﬁned as an error or estimator of the innovation source
˜x j(t). The mean squared error E{ε2
j(t)} achieves a minimum c2
j E{˜x2
j (t)}, where c j is a positive scaling
constant, if and only if x1 = ±c1x j for any j ∈{1, 2, . . . , m} or x j = 0 holds.
The cost function (21.132) can be evaluated as follows:
E{ε2
j} = wT
j Ryyw j −2wT
j Ry¯x j b j + bT
j R¯x j ¯x j b j,
(21.133)
where Ryy ≈E{yyT }, Ry¯x j ≈E{y¯xT
j } and R¯x j ¯x j ≈E{¯x j ¯xT
j } are estimators of the true values of
correlation and cross-correlation matrices: Ryy, Ry¯x j , R¯x j ¯x j , respectively.
In order to estimate the vectors w j and b j, we evaluate the gradients of the cost function and make
them equal to zero as follows:
∂D j(w j, b j)
∂w
= 2Ryyw j −2Ry¯x j b j = 0,
(21.134)
∂D j(w j, b j)
∂b j
= 2R¯x j ¯x j b j −2R¯x jyw j = 0.
(21.135)

1.21.4 NMF and Related Problems
1189
Solving the above matrix equations we obtain the iterative formulas:
˜w j = R−1
yy Ry¯x j b j,
w j =
˜w j
∥˜w j∥2
,
(21.136)
b j = R−1
¯x j ¯x j R¯x jyw j = R−1
¯x j ¯y j R¯x j x j ,
(21.137)
where in order to avoid the trivial solution w j = 0, we normalize the vector w j to unit length in each
iteration step as w j(k + 1) ←w j(k + 1)/∥w j(k + 1)∥2 (which ensures that E{x2
j } = 1). It is worth
to note that in our derivation the matrices R¯x j ¯x j and R¯x jx j are assumed to be independent of the vector
w j(k + 1), i.e., they are estimated based on w j(k) form the previous iteration step. This two-phase
procedure is similar to the expectation maximization (EM) scheme: (i) Freeze the correlation and cross-
correlation matrices and learn the parameters of the processing unit (w j, b j); (ii) freeze w j and bj and
learn the new statistics (i.e., matrices R¯x j ˆx j and R¯x j ¯x j ) of the estimated source signal, then go back to
(i) and repeat. Hence, in phase (i) our algorithm extracts a source signal, whereas in phase (ii) it learns
the statistics of the source.
The above algorithm can be further simpliﬁed. It should be noted that in order to avoid the inversion
of the autocorrelation matrix Ryy, in each iteration step we can perform the standard prewhitening
or standard PCA as a preprocessing step and then normalize the sensor signals to have unit variance.
In such cases Ryy = I and the algorithm is considerably simpliﬁed to [106]
w j = Ry¯x j b j = Ryx j ,
w j ←
w j
∥w j∥2
,
(21.138)
where Ryˆx j = 1
T
T
t=1y(t)ˆx j(t) = ⟨y(t)x j(t)⟩or in equivalent form
w j(k + 1) = ⟨y(t)ˆx j(t)⟩
⟨x2
j (t)⟩.
(21.139)
From (21.138) and (21.139) it follows that the derived update rules are similar to the power
method in ﬁnding the eigenvector associated with the maximal eigenvalue of the matrix Ry(b j) =
E{L
p=1 b jpy(t)yT (t −p)}. This observation suggests that it is often not necessary to minimize the
cost function with respect to the parameters {b jp} but it is enough to choose an random set of them for
which the largest eigenvalue is unique (single). More generally, if all eigenvalues of the generalized
covariance matrix Ry(bj) are distinct, then we can extract all sources simultaneously by estimating the
principal eigenvectors of Ry(bj).
1.21.4 NMF and related problems
NMF has been investigated by many researchers, e.g., Paatero and Tapper [108], but it has gained
popularity through the works of Lee and Seung published in Nature and NIPS [20,109]. Based on the
argument that the nonnegativity is important for human perception, they proposed simple algorithms
(often called the Lee-Seung algorithms) for ﬁnding nonnegative representations of nonnegative data
and images.

1190
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
1.21.4.1 Why nonnegativity and sparsity constraints?
Many real-world data are nonnegative and the corresponding hidden components have a physical mean-
ing only when nonnegative. In practice, both nonnegative and sparse decompositions of the data are
often either desirable or necessary, when the underlying components have a physical interpretation. For
example, in image processing and computer vision, the involved variables and parameters may corre-
spond to pixels, and the nonnegative sparse decomposition is related to the extraction of relevant parts
from the images [20,109]. In computer vision and graphics, we often encounter multi-dimensional data,
such as images, videos, and medical data, one type of which is MRI (magnetic resonance imaging). A
color image can be considered as a 3D nonnegative data sets, two of the dimensions (rows and columns)
being spatial, and the third one being a color plane (channel) depending on its color space, while a color
video sequence can be considered as a 4D nonnegative data sets with time being the fourth dimension. A
sparse representation of the data by a limited number of components is an important research problem.
In machine learning sparseness is closely related to feature selection and certain generalizations of
learning algorithms, while nonnegativity relates to probability distributions. Generally speaking, com-
positional data (i.e., positive sum of components or real vectors) are natural representations when the
variables (features) are essentially the probabilities of complementary and mutually exclusive events.
Furthermore, we may note that NMF is an additive model which does not allow subtraction; therefore
it often describes quantitatively the parts that comprise the entire entity. In other words, NMF can be
considered as a part-based representation in which a zero-value represents the absence and a positive
number represents the presence of some event or component. Speciﬁcally, in the case of facial image
data, the additive or part-based nature of NMF has been shown to result in a basis of facial features,
such as eyes, nose, and lips [20]. Furthermore, the low-rank matrix factorization methods that exploit
nonnegativity and sparsity constraints usually lead to estimation of the hidden components with spe-
ciﬁc structures and physical interpretations, in contrast to other blind source separation methods. In
economics, variables and data such as volume, price and many other factors are nonnegative and sparse.
Sparseness constraints may increase the efﬁciency of a portfolio, while nonnegativity both increases the
efﬁciency and reduces the risk [110,111]. In information retrieval the documents are usually represented
as relative frequencies of words form a prescribed vocabulary. In environmental science the scientists
investigate a relative proportion of different pollutants in water or air [112]. In biology each coordinate
axis may correspond to a speciﬁc gene, and the sparseness is necessary for ﬁnding local patterns hidden
in the data, whereas the nonnegativity is required to give physical or physiological meaning. This is also
important for the robustness of biological systems, where any observed change in the expression level of
a speciﬁc gene emerges from either positive or a negative inﬂuence, rather than a combination of both,
which partly cancel each other [109,111]. It is clear, however, that with constraints such as sparsity and
nonnegativity some of the explained variance (ﬁt) may decrease. In other words, it is natural to seek
a trade-off between the two goals of interpretability (making sure that the estimated components have
physical or physiological meaning) and statistical ﬁdelity (explaining most of the variance of the data)
if the data are consistent and do not contain too much noise.
1.21.4.2 Basic NMF models
The basic NMF problem can be stated as follows: Given a nonnegative data matrix Y ∈RI×T
+
(with
yit ≥0 ∀it or compactly Y ≥0) and a reduced rank-J (J ≤min (I, T )), ﬁnd two nonnegative

1.21.4 NMF and Related Problems
1191
matrices A = [a1, a2, . . . , aJ] ∈RI×J
+
and X = BT = [b1, b2, . . . , bJ]T ∈RJ×T
+
, which factorize
Y = AX + E = ABT + E, as well as possible in the sense that a norm or divergence of error matrix E
is minimized (see Figure 21.1). Since we usually operate on the column vectors of matrices (in order
to avoid a complex or confusing notation) it is often convenient to use the matrix B = XT instead of
the matrix X. The factors A and B may have different physical meaning in different applications. In a
BSS problem, A plays the role of a mixing matrix, while the columns of B express source signals. In
clustering problems, A is the basis matrix while B denotes the weight matrix. In acoustic analysis, A
represents the basis patterns, while each column of B expresses the time points (positions) when the
sound patterns are activated. In standard NMF we only assume nonnegativity of the factor matrices A
and B. Unlike blind source separation methods based on independent component analysis (ICA), here
we do not assume that the sources are independent, although we often impose additional constraints on
A and/or B.
The NMF model can also be represented as a special form of the bilinear model (see Figure 21.2):
Y = J
j=1 a j ◦b j + E = J
j=1 a j bT
j + E. Thus, we can build an approximate representation of the
nonnegative data matrix Y as a sum of rank-1 nonnegative matrices a j bT
j . If such decomposition is
exact (i.e., E = 0) then it is called the Nonnegative Rank Factorization (NRF) [113]. Among the many
possible series representations of the data matrix Y by nonnegative rank-1 matrices, the smallest integer
J for which such a nonnegative rank-1 series representation is attained is called the nonnegative rank of
the nonnegative matrix Y and it is denoted by rank+(Y). The nonnegative rank satisﬁes the following
bounds [113]:
rank(Y) ≤rank+(Y) ≤min{I, T }.
(21.140)
Although the NMF can be applied to BSS problems for nonnegative sources and nonnegative mixing
matrices, its application is not limited to BSS and it can be used for other diverse applications far beyond
BSS. In many applications we require additional constraints on the elements of the matrices A and/or
B, such as smoothness, sparsity, symmetry, and orthogonality.
1.21.4.2.1
Special forms of NMF
The NMF models may take several different forms:
Symmetric NMF: In the special case when A = B ∈RI×J
+
the NMF is called a symmetric NMF,
given by
Y = AAT + E.
(21.141)
This model is also considered equivalent to Kernel K-means clustering and Laplace spectral
clustering [114].
If the exact symmetric NMF exists (E = 0) then a nonnegative matrix Y ∈RI×I
+
is said to be com-
pletely positive (cp) and the smallest number of columns of A ∈RI×J
+
satisfying the exact factorization
Y = AAT is called the cp-rank of the matrix Y, denoted by rankcp(Y). If Y is cp, then the upper bound
estimate for the cp-rank is given by [113]:
rankcp(Y) ≤rank(Y)(rank(Y) + 1)
2
−1,
(21.142)
provided that rank(Y) > 1.

1192
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
Orthogonal NMF: The orthogonal or semi-orthogonal NMF can be deﬁned as
Y = ABT + E,
(21.143)
subject to nonnegativity constraints A ≥0 and B ≥0 (component-wise) and an additional orthogonality
constraint:AT A = IJ orBT B = IJ.ItcanbeprovedthattheorthogonalNMFisequivalenttoaweighted
variant of spherical K-means clustering.
Probably the simplest and most efﬁcient way to impose orthogonality for the matrix A (or B) is to
perform the following transformation after each iteration
A ←A[AT A]−1/2
or B ←[BT B]−1/2BT .
(21.144)
Semi-NMF and Nonnegative Factorization of an Arbitrary Matrix: In some applications the
observed input data is unsigned (unconstrained or bipolar) as indicated by Y = Y± ∈RI×T which
allows us to relax the constraints regarding nonnegativity of one factor (or only speciﬁc vectors of a
matrix). This leads to the approximative semi-NMF which can take the following form
Y± = A±BT
+ + E or Y± = A+BT
± + E,
(21.145)
where the subscript in A+ indicates that the matrix is constrained to be nonnegative. Usually, in order
to provide uniqueness of the semi-NMF additional constraints must be imposed.
Nonnegative Factorization (NF): In some applications the data matrix Y has some negative entries.
Such case when the matrices A and/or B are restricted to contain nonnegative entries, but the data matrix
Y may have entries with mixed signs, is referred to as the Nonnegative Factorization (NF) [115,116].
NMF with Offset (Afﬁne NMF): In NMF with offset (also called afﬁne NMF, aNMF), the goal is
to remove the base line or constant bias from the matrix Y by using a slightly modiﬁed NMF model:
Y = ABT + a01T + E,
(21.146)
where 1 ∈RT is a vector of all ones and a0 ∈RI
+ is a vector which is selected in such a way that
the matrix B is zero-grounded, that is, with a possibly large number of zero entries in each column
(or for noisy data close to zero entries). The term Y0 = a01T denotes the offset, which together with
the nonnegativity constraint often ensures the sparseness of the factored matrices. The main role of the
offset is to absorb the constant values of a data matrix, thereby making the factorization sparser and
therefore improving (relaxing) conditions for the uniqueness of NMF (see next sections).
Three-factor NMF: Three-factor NMF (also called the tri-NMF) can be considered as a special case
of the multi-layer NMF, and it can take the following general form [117,118]
Y = ASBT + E,
(21.147)
where the nonnegativity constraints are imposed on all or only on selected factor matrices:
A ∈RI×J, S ∈RJ×R, and/or B ∈RT ×R.
It should be noted that if we do not impose any additional constraints to the factors (besides nonneg-
ativity), the three-factor NMF can be reduced to the standard (two-factor) NMF by the transformation
A ←AS or B ←BST . However, the three-factor NMF is not equivalent to the standard NMF if we
apply special constraints or conditions as illustrated by the following special cases.

1.21.4 NMF and Related Problems
1193
(
)
I T
x
(
)
I J
x
(
)
J R
x
(
)
R T
x
Y
A
X
S
FIGURE 21.6
Illustration of three factor NMF (tri-NMF). The goal is to estimate two matrices A ∈RI×J
+
and B = XT ∈RT ×R
+
,
assuming that the matrix S ∈RJ×R is given, or to estimate all three factor matrices A, S, and X, subject to
additional constraints such as orthogonality, smoothness or sparsity.
Non-Smooth NMF: Non-smooth NMF (nsNMF) was proposed by [119] and it is a special case of
the three-factor NMF model in which the matrix S is ﬁxed and known, and it is used for controlling the
sparsity or the smoothness of the matrix B and/or A. Typically, the smoothing matrix S ∈RJ×J takes
the form:
S = (1 −)IJ + 
J 1J×J,
(21.148)
where IJ is the J × J identity matrix and 1J×J is the square matrix of all ones. The scalar parameter
0 ≤ ≤1 controls the smoothness of the matrix operator S. For  = 0, S = IJ, and the model reduces
to the standard NMF for  →1 strong smoothing is imposed on S causing increased sparseness of
both A and B, in order to maintain the faithfulness of the model.
CUR Decomposition: In the CUR decomposition, a given data matrix Y ∈RI×T is decomposed as
follows [120–124]:
Y = CUR + E,
(21.149)
where C ∈RI×C is a matrix constructed from C selected columns of Y, R ∈RR×T consists of R rows
of Y and the matrix U ∈RC×R is chosen to minimize the error E ∈RI×T .
The CUR decomposition for data analysis can be considered as an alternative as low-rank approx-
imation to PCA and SVD, specially for large-scale and sparse data sets. On this regard, the idea of
the CUR decomposition is to provide a representation of the data as a linear combination of a few
“meaningful” components which are exact replicas of columns and rows of the original data matrix.
[121,123–125]. Since typically, C ≪T and R ≪I, the objective is to ﬁnd a matrix U and select rows
and columns of Y such that f the error cost function ∥E∥2
F is minimized. There are several strategies
for the selection of suitable columns and rows. The main principle is to select columns and rows that
exhibit high “statistical leverage” and provide the best low-rank ﬁt to the data matrix [120–122].
The core matrix U can be easily computed as U = W†, where W ∈RC×Ris matrix build from
entries of Y located on the cross-intersections of selected rows and columns. In other words, the core
matrix U is the Moore–Penrose pseudo-inverse of a matrix W ∈RR×C, i.e., U = W†, which is deﬁned
by the intersections of the selected rows and columns. Alternatively, we can compute a core matrix U as
U = C†YR†, but in this case the knowledge of the whole data matrix Y is necessary (see Figure 21.6).
In the special case, when we assume that UR = X, we have the CX decomposition:
Y = CX + E.
(21.150)

1194
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
X
( )
L
X
(2)
X
(1)
E
Y
A
( )
L
A
(1)
A
(2)
(
)
J
T
(
)
I
T
FIGURE 21.7
Multilayer NMF model. In this model the global factor matrix A = A(1)A(2) · · · A(L) has a distributed sparse
representation in which each matrix A(l) can be sparse.
The CX and CUR decompositions are low-rank matrix decompositions that are explicitly expressed
in terms of a small number of actual columns and/or actual rows of the original data matrix and
they have recently received increasing attention within the data analysis community, especially for
nonnegative data, due to its many applications [121–124]. The CUR decomposition has the advantage
that components (factor matrices C and R) are directly obtained from the rows and columns of the data
matrix Y, preserving desired properties such as nonnegativity or sparsity. Because they are constructed
from actual data elements, the CUR decomposition is often more easily interpretable by practitioners
form the ﬁeld from which the data are collected (to the extent that the original data points and/or features
are interpretable) [123].
Orthogonal Three-Factor NMF: Orthogonal three-factor NMF imposes additional constraints upon
the two matrices: AT A = IJ and BT B = IR while the matrix S can be an arbitrary unconstrained matrix
(i.e., it has both positive and negative entries) [117,118].
For the uni-orthogonal three-factor NMF only one matrix (A or B) is orthogonal and all three matrices
are usually nonnegative.
Multi-layer NMF: In multi-layer NMF the basic matrix A is replaced by a set of cascaded (factor)
matrices. Thus, the model can be described as (see Figure 21.7)
Y = A(1)A(2) · · · A(L)X + E.
(21.151)
Since the model is linear, all the matrices can be merged into a single matrix A if no special constraints
are imposed upon the individual matrices A(l) (l = 1, 2, . . . , L). However, multi-layer NMF can be used
to considerably improve the performance of standard NMF algorithms due to its distributed structure
and the alleviation the problem of local minima.
To improve the performance of the NMF algorithms (especially for ill-conditioned and badly-scaled
data) and to reduce the risk of converging to local minima of a cost function due to nonconvex alternating
minimization, we have developed a simple hierarchical multi-stage procedure [126] combined with a
multi-start initialization, in which we perform a sequential decomposition of nonnegative matrices as
follows. In the ﬁrst step, we perform the basic approximate decomposition Y ∼= A(1)X(1) ∈RI×T using
any available NMF algorithm. In the second stage, the results obtained from the ﬁrst stage are used to
build up an input data matrix Y ←X(1), that is, in the next step, we perform a similar decomposition
X(1) ∼= A(2)X(2) ∈RJ×T , using the same or different update rules. We continue our decomposition
by taking into account only the last obtained components. The process can be repeated for an arbitrary
number of times until some stopping criterion is satisﬁed.

1.21.4 NMF and Related Problems
1195
Thus, our multi-layer NMF model has the form:
Y ∼= A(1)A(2) · · · A(L)X(L),
(21.152)
with the ﬁnal results A = A(1)A(2) · · · A(L) and X = X(L). Physically, this means that we build up
a distributed system that has many layers or cascade connections of L mixing subsystems. The key
point of this approach is that the learning (update) process to ﬁnd the parameters of the matrices X(l)
and A(l) (l = 1, 2, . . . , L) is performed sequentially, layer-by-layer, where each layer is randomly
initialized with different initial conditions. We have found that the hierarchical multi-layer approach
can improve the performance of most NMF algorithms discussed in this chapter [8,127].
Simultaneous NMF: In simultaneous NMF (siNMF) we have available two or more linked input
data matrices (say, Y1 and Y2) and the objective is to decompose them into nonnegative factor matrices
in such a way that one of the factor matrices is common (which is a special form of the Nonnegative
Tensor Factorization (NTF) model, for example [8]
Y1 = A1BT + E1,
Y2 = A2BT + E2.
(21.153)
Such a problem arises, for example, in bio-informatics if we combine gene expression and transcription
factor regulation [128]. In this application the data matrix Y1 ∈RI1×T is the expression level of gene t
for the data sample i1 (i.e., the index i1 denotes the samples, while t stands for genes) and Y2 ∈RI2×T
is the transcription matrix (which is 1 whenever transcription factor i2 regulates gene t).
Quadratic NMF: Quadratic NMF (QNMF) has several forms [129]:
1)
The asymmetric form (AQNMF) Y ≈AAT X,
2)
The symmetric form (SQNMF): Y ≈AXAT ,
3)
Projective NMF (PNMF): Y ≈AAT Y, for X = Y,
4)
Symmetric NMF (SNMF) Y ≈AAT , for X = I.
1.21.4.3 Basic approaches to estimate parameters of the standard NMF
In order to estimate factor matrices A and B for the standard NMF, we need to consider the similarity
measure or divergence to quantify the difference between the data matrix Y and the approximative NMF
model matrix Y = ABT . The choice of the similarity measure (also referred to as distance, divergence
or measure of dissimilarity) depends mostly on the probability distribution of the estimated signals or
components and on the structure of the data or the distribution of the noise [130]. The simplest and most
often used measure is based on the Frobenius norm:
DF(Y∥ABT ) = 1
2∥Y −ABT ∥
2
F,
(21.154)
which is also referred to as the squared Euclidean distance. It should be noted that the above cost
function is convex with respect to either one of the elements of the matrix A or the matrix B, but not
with respect to both. Although the NMF optimization problem is not convex, the objective functions
are separately convex in each of the two factors A and B, which implies that ﬁnding the optimal factor
matrix A corresponding to a ﬁxed matrix B reduces to a convex optimization problem and vice versa.
However, the convexity is lost as soon as we try to optimize both factor matrices simultaneously [116].

1196
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
Alternating minimization of such cost function leads to the ALS (Alternating Least Squares) algo-
rithm which can be described as follows:
1. Initialize A randomly or by using a speciﬁc deterministic strategy.
2. Estimate B from the matrix equation AT ABT = AT Y by solving
min
B {DF(Y∥ABT ) = 1
2∥Y −ABT ∥
2
F},
with ﬁxed A.
(21.155)
3. Set all the negative elements of B to zero or to some small positive value ε.
4. Estimate A from the matrix equation BT BAT = BT YT by solving
min
A {DF(Y∥ABT ) = 1
2∥YT −BAT ∥
2
F},
with ﬁxed B.
(21.156)
5. Set all negative elements of A to zero or to some small positive value ε.
6. Repeat the procedure till we achieve convergence.
The above ALS algorithm can be written in the following form (Note that the max operator is applied
element-wise, that is, each element of a matrix is compared with the scalar parameter ε):
B ←max{ε, (YT A(AT A)−1},
A ←max{ε, YB(BT B)−1},
(21.157)
where ε is a small constant (typically, 10−16) used to enforce positive entries. Various additional con-
straints (such as sparsity or smoothness) can also be imposed on either A or B.
Today the ALS method is considered as the basic “workhorse" approach, however it is not guaranteed
to converge to a global minimum nor even to a stationary point, but only to a solution where the cost
functions cease to decrease [5,112]. Moreover, it is often not sufﬁciently accurate. The ALS method
can be dramatically improved and its computational complexity reduced as it will be shown later.
It is interesting to note that the NMF problem can be considered as a natural extension of a Nonneg-
ative Least Squares (NLS) problem formulated as the following optimization problem: given a matrix
A ∈RI×J and a set of observed values given by the vector y ∈RI, ﬁnd a nonnegative vector x ∈RJ
which minimizes the cost function J(x) = 1
2∥y −Ax∥2
2, i.e.,
min
x≥0
1
2∥y −Ax∥2
2,
(21.158)
subject to x ≥0. There is a large volume of literature devoted to NLS problems which can be exploited
and adopted to NMF [8].
Most of the existing approaches minimize only one kind of cost function by alternately switching
between the sets of parameters. However, we can adopt a more general and ﬂexible approach in which,
instead of one cost function, we exploit two or more cost functions (with the same global minima);
one of them is minimized with respect to A and the other one is minimized with respect to B. Such an
approach is fully justiﬁed as A and B may have different distributions or different statistical properties
and therefore different cost functions may be optimal for each one of them.
Let us consider a basic penalized cost function by considering additional penalty terms:

1.21.4 NMF and Related Problems
1197
DFr(Y∥ABT ) = 1
2∥Y −ABT ∥
2
F + αADA(A) + αBDB(B)
(21.159)
s.t. ai j ≥0,
bt j ≥0, ∀i, j, t,
where αA and αB are nonnegative regularization parameters and the penalty terms DB(B), and DA(A)
are used to enforce certain application-dependent characteristics for the desired solution. As a special
practical case, we have DB(B) = 
t j ϕB(bt j), where ϕ(bt j) are suitably chosen functions which are
measures of smoothness or sparsity. To achieve a sparse representation we usually chose ϕB(bt j) =
|bt j| or simply ϕB(bt j) = bt j (since, bt j are nonnegative). Similar regularization terms can also be
implemented for the matrix A. Note that we treat both matrices A and B in a similar way.
Applying the standard gradient descent approach, we have
ai j ←ai j −ηi j
∂DFr(A, B)
∂ai j
,
bt j ←bt j −ηt j
∂DFr(A, B)
∂bt j
,
(21.160)
where ηi j and ηt j are positive learning rates. The gradients above can be expressed in compact matrix
form as
∂DFr(A, B)
∂ai j
= [−YB + ABT B]i j + αA
∂DA(A)
∂ai j
,
(21.161)
∂DFr(A, B)
∂bt j
= [−YT A + BAT A]t j + αB
∂DB(B)
∂bt j
.
(21.162)
At this point we can follow the Lee and Seung approach to choose the learning rates [20,109]
ηi j =
ai j
[ABT B]i j
,
ηt j =
bt j
[BAT A] jt
,
(21.163)
leading to multiplicative updates (refereed to as NMF_MU):
ai j ←ai j
[[YB]i j −αAA(ai j)]+
[ABT B]i j
,
bt j ←bt j
[[YT A]t j −αBB(bt j)]+
[BAT A]t j
,
(21.164)
where the functions A(ai j) and B(bt j) are deﬁned as
A(ai j) = ∂DA(A)
∂ai j
,
B(bt j) = ∂DB(B)
∂bt j
(21.165)
and the regularization parameters αAand αB should be sufﬁciently small to ensure the nonnegativity
in (21.164) at each iteration, or we can avoid this problem by a half-wave rectiﬁer [x]+ = max{x, ε}
with a small ε.
The above update rules (21.164), (for αA = αB = 0) often called Lee-Seung NMF algorithm can be
considered as an extension of the well known ISRA (Image Space Reconstruction Algorithm) algorithm
proposed ﬁrst by Daube-Witherspoon and Muehllehner [131] and investigated by many researchers,
especially by De Pierro and Byrne [132–136].

1198
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
In order to impose sparsity, instead of using the ℓ2-norm, we can use regularization terms based on
the ℓ1-norm, and minimize the cost function:
DF1(A, B) = 1
2∥Y −ABT ∥
2
F + αA∥A∥1 + αB∥B∥1,
(21.166)
s.t. ai j ≥0,
bt j ≥0, ∀i, j, t,
where the ℓ1-norms for matrices are deﬁned as ∥A∥1 = 
i j |ai j| and ∥B∥1 = 
t j |bt j|.
This way, the multiplicative learning rules (21.164) for sparse NMF (with controlled sparseness) can
be simpliﬁed as
ai j ←ai j

[YB]i j−αA

+

ABT B

i j
,
bt j ←bt j

[YT A] jt−αB

+

BAT A

t j
,
(21.167)
where the normalization of the columns of the matrix A at each iteration is performed as ai j ←
ai j/
p apj. This algorithm provides a sparse representation for the estimated matrices; the degree of
sparseness increases with an increase in the values of the regularization coefﬁcients (typically, αA =
αB = 0.01 −0.5).
The above multiplicative update rules can be represented in compact matrix form
A ←A ⊛

YB −αA1I×J
 
+ ⊘(ABT B),
(21.168)
B ←B ⊛

YT A −αB1T ×J
 
+ ⊘(BAT A),
(21.169)
where the symbols “⊛” and “⊘” denote the Hadamard element-wise product and division, respectively.
1.21.4.4 The HALS algorithm and its extensions
This section focuses on a brief description of a simple local ALS method referred to as the HALS
(Hierarchical Alternating Least Squares) algorithm. We call such ALS algorithm hierarchical, since
we minimize sequentially a set of simple cost functions which are linked to each other hierarchically
via the residual matrices Y( j), which approximate rank-1 bilinear decomposition. Moreover, the HALS
algorithm is usually used for multi-layer models in order to improve performance. We highlight the
suitabilityofthismethodforlarge-scaleNMFproblems,andsparsenonnegativecodingorrepresentation
[126,137].
We can generalize the concept of ALS by using not only one or two cost functions but rather a
set of cost functions to be minimized sequentially or simultaneously. For A = [a1, a1, . . . , aJ] and
B = XT = [b1, b2, . . . , bJ], we can express the squared Euclidean cost function as
D(a1, a1, . . . , aJ, b1, b2, . . . , bJ) = 1
2∥Y −ABT ∥
2
F = 1
2∥Y −
J

j=1
a j bT
j ∥2
F.
(21.170)
An underlying idea is to deﬁne the residuals as:
Y( j) = Y −

p̸= j
a pbT
p = Y −ABT + a j bT
j = E + a j bT
j
( j = 1, 2, . . . , J),
(21.171)

1.21.4 NMF and Related Problems
1199
and alternately minimize the set of cost functions with respect to the unknown variables a j, b j:
D( j)
A (a) = 1
2∥Y( j) −abT
j ∥
2
F,
for a ﬁxed b j,
(21.172a)
D( j)
B (b) = 1
2∥Y( j) −a j bT ∥
2
F,
for a ﬁxed a j,
(21.172b)
for j = 1, 2, . . . , J subject to a ≥0 and b ≥0, respectively.
In other words, we perform alternatively the minimization of the set of cost functions
D( j)
F (Y( j)∥a j bT
j ) = 1
2∥Y( j) −a j bT
j ∥
2
F,
(21.173)
for j = 1, 2, . . . , J subject to a j ≥0 and b j ≥0.
In order to estimate the stationary points we simply compute the gradients of the local cost functions
(21.173) with respect to the unknown vectors a j and b j (assuming that the other vectors are ﬁxed) as
follows:
∇a j D( j)
F (Y( j)∥a j bT
j ) =
∂D( j)
F (Y( j)∥a j bT
j )
∂a j
= a j bT
j b j −Y( j)b j,
(21.174)
∇b j D( j)
F (Y( j)∥a j bT
j ) =
∂D( j)
F (Y( j)∥a j bT
j )
∂b j
= aT
j a j b j −Y( j)T a j.
(21.175)
a j ←
1
bT
j b j
[Y( j)b j]+,
b j ←
1
aT
j a j
[Y( j)T a j]+
( j = 1, 2, . . . , J),
(21.176)
where [A]+ = max{ϵ, A}. We refer to these update rules as the HALS algorithm which was ﬁrst
introduced for the NMF in [126]. The same or similar update rules for the NMF have been proposed,
extended or rediscovered independently in other publications [116,138–140]. However, our practical
implementations of the HALS algorithm are quite different and allow various extensions to sparse and
smooth NMF, and also for the N-order NTF [8]. It was proved in that for every constant ε > 0 the limit
points of the HALS algorithm initialized with positive matrices and applied to the optimization problem
(21.173) are stationary points (see also [139]).
The nonlinear projections can be imposed individually for each source bj and/or vector a j, so the
algorithm can be directly extended to a semi-NMF, in which some parameters are relaxed to be uncon-
strained (by removing the half-wave rectifying [·]+ operator, if necessary). In practice it is necessary to
normalize the column vectors a j and/or b j to unit length vectors (in ℓp-norm sense (p = 1, 2, . . . , ∞))
at each iteration step. In the special case of the ℓ2-norm, the above algorithm can be further simpliﬁed
by ignoring the denominators in (21.176) and by imposing a vector normalization after each iterative
step, to give a simpliﬁed scalar form of the HALS algorithm:
bt j ←
!I
i=1 ai j y( j)
it
"
+ ,
ai j ←
!T
t=1 bt j y( j)
it
"
+ ,
(21.177)
with ai j ←ai j/∥a j∥2, where y( j)
it
= [Y( j)]it = yit −
p̸= j aipbtp.

1200
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
The above simple algorithm can be further extended or improved with respect to the convergence rate
and performance by imposing additional constraints such as sparsity and smoothness. Firstly, observe
that the residual matrix Y( j) can be rewritten as
Y( j) = Y −

p̸= j
a pbT
p = Y −ABT + a j bT
j ,
= Y −ABT + a j−1bT
j−1 −a j−1bT
j−1 + a j bT
j .
(21.178)
It then follows that instead of computing explicitly the residual matrix Y( j) at each iteration step, we
can just perform smart updates (see [8,141].
Different cost functions can be used for the estimation of the columns of matrix A and/or B, possibly
with various additional regularization terms [141]. Furthermore, the columns of A can be estimated
simultaneously,andthecolumnsofBsequentially.Inotherwords,byminimizingthesetofcostfunctions
in (21.173) with respect to bj, and simultaneously the cost functions (21.154) with normalization of the
columns a j to unit ℓ2-norm, we obtain a very efﬁcient NMF learning algorithm in which the individual
columns of B = [b1, b2, . . . , bJ] are updated locally (column-by-column) and the matrix A is updated
globally using the global nonnegative ALS (all columns a j simultaneously):
b j ←[Y( j)T a j]+,
A ←[YB(BT B)−1]+,
(21.179)
with the normalization (scaling) of the columns of A to unit length in the sense of the ℓ2-norm after
each iteration.
In order to impose sparseness and smoothness constraints for the vectors bj (source signals), we can
minimize the following set of cost functions [34,139]:
D( j)
F (Y( j)∥a j bT
j ) = 1
2∥Y( j) −a j bT
j ∥
2
F + αsp∥b j∥1 + αsm∥ϕ(Lb j)∥1,
(21.180)
for j = 1, 2, . . . , J subject to a j ≥0 and b j ≥0, where αsp > 0, αsm > 0 are regularization
parameters controlling the levels of sparsity and smoothness, respectively, L is a suitably designed
matrix (the Laplace operator) which measures the smoothness (by estimating the differences between
neighboring samples of b j) and ϕ : R →R is an edge-preserving function applied component-wise [8].
1.21.4.5 Large-scale NMF
In many applications, especially for dimension reduction applications, the data matrix Y ∈RI×T can
be very large (with millions of entries), but it can be approximately factorized using a much smaller
number of nonnegative components (J), that is, J ≪I and J ≪T . Then the problem Y ≈ABT
becomes highly redundant and we do not need to use the information about all entries of Y in order to
estimate precisely the factor matrices A ∈RI×J and B ∈RT ×J. In other words, to solve the large-scale
NMF problem we do not need to know the whole data matrix, but only a small random part of it.
Such an approach can outperform considerably the standard NMF methods, especially for extremely
overdetermined systems.

1.21.4 NMF and Related Problems
1201
There are several strategies to choose the columns and rows of the input data matrix [120,123,142,
143]. In Eqs. (21.168), (21.169), and (21.176), the major bottleneck is caused by the matrix multiplica-
tions with the large matrices Y and Yi. If these large matrices can be replaced by much smaller matrices,
the efﬁciency of the NMF_MU and HALS algorithm can be improved. For this reason we consider the
following cost function:
min
˜A, ˜B;A,B
D( ˜A, ˜B; A, B) = ∥Y −˜A ˜BT ∥
2
F + ∥˜A ˜BT −ABT ∥
2
F,
(21.181)
s.t. A ∈RI×J
+
, B ∈RT ×J
+
, A ∈RI×R,B ∈RT ×R,
where the data matrix is ﬁrst approximated by any low-rank approximation Y ≈˜A ˜BT [144]. Therefore,
we can solve (21.181) instead of (21.154) with the same global minimum.
We call this procedure low-rank approximation based NMF (LRA-NMF) [144], which can be per-
formed as follows:
•
Step 1: Perform the low-rank approximation using min{∥Y −ABT ∥2
F}, where A and B are of low-
rank J, without imposing any nonnegativity constraints;
•
Step 2: Perform nonnegative matrix factorization using min{∥ABT −ABT ∥2
F} with ﬁxed and known
A,B, where A, B are nonnegative and possibly with additional constraints such as sparsity, smooth-
ness, etc.
The Step 1 of the LRA-NMF can be solved efﬁciently by using standard PCA or truncated SVD or
any other efﬁcient low-rank approximation algorithms such as CUR decomposition. In the Step 2, we
assume that the optimal A and B have been obtained and then min{∥˜A ˜BT −ABT ∥
2
F} can be solved.
From (21.168)–(21.169) by simply substituting the low-rank approximation Y ≈ABT , it yields
A ←A ⊛
A(BT B) −αA1I×J
 
+ ⊘(ABT B),
(21.182)
B ←B ⊛
B(AT A) −αB1T ×J
 
+ ⊘(BAT A).
(21.183)
The algorithm based on (21.182)–(21.183) is named as LRA-NMF_MU. At a ﬁrst glance there is not a
big difference between it (21.168)–(21.169). However, we should note that the dimensionality of A,B
is much smaller than that of Y. As a matter of fact, LRA-NMF_MU has much lower time complexity
of O(T J 2) and space complexity of O(T J). In other words, LRA-NMF_MU is about I/J times faster
than NMF_MU. In addition, we do not need to load the whole matrix Y in the memory during the
iterations. Instead, A,B are employed explicitly during the NMF iterations.
Similarly to the HALS algorithm, let Y j = AB−
p̸= j a pbT
p and the formulas in (21.176) become
b j ←
1
aT
j aj
!
B(AT a j) −B j(AT
j a j)
"
+ ,
(21.184)
a j ←
1
bT
j bj
!
A(BT b j) −A j(BT
j b j)
"
+ ,
(21.185)

1202
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
where A j ∈RI×(J−1) and B j ∈RT ×(J−1) are the sub-matrices of A and B obtained by removing
their jth columns. The algorithm based on (21.184) is called LRANMF_HALS. The time complexity
of LRA-NMF_HALS per iteration is O(N R) and the space complexity is only O(N R).
Similar approach can be applied to nonnegative tensor factorization (NTF) (see Section 1.21.5).
1.21.4.6 Robust NMF algorithms
In many applications the observed data sets are corrupted by large noise (not necessary with a Gaussian
distribution). In such cases we need to use a suitably chosen cost function or divergence.
1.21.4.6.1
The alpha-beta divergences
For positive measures Y and ˆY consider the following generalized dissimilarity measure, which we will
refer to as the AB-divergence [130]:
D(α,β)
AB (Y∥ˆY) = −1
αβ

it
#
yα
it ˆyβ
it −
α
α + β yα+β
it
−
β
α + β ˆyα+β
it
$
(21.186)
for α, β, α + β ̸= 0,
or equivalently
D(α,λ−α)
AB
(Y∥ˆY) =
1
(α −λ)α

it
#
yα
it ˆyλ−α
it
−α
λ yλ
it −λ −α
λ
ˆyλ
it
$
,
(21.187)
for α ̸= 0, α ̸= λ, λ = α + β ̸= 0,
We shall now illustrate haw a suitable choices of the (α, β) parameters simplify the AB-divergence
into some existing divergences, including the well-known Alpha- and Beta-divergences [8,145,146].
When α + β = 1 the AB-divergence reduces to the Alpha-divergence [130,145,147].
On the other hand, when α = 1, it reduces to the Beta-divergence [19,146,148,149].
The AB-divergence reduces to the standard Kullback–Leibler (KL) divergence (DK L(·, ·)) for α = 1
and β = 0
D(1,0)
AB (Y∥ˆY) = DK L(Y∥ˆY) =

it
#
yit ln yit
ˆyit
−yit + ˆyit
$
,
(21.188)
and it reduces to the standard Itakura-Saito divergence (DI S(·, ·)) for α = 1 and β = −1 [19,148]
D(1,−1)
AB
(Y∥ˆY) = DI S(Y∥ˆY) =

it
#
ln ˆyit
yit
+ yit
ˆyit
−1
$
.
(21.189)
Using the 1 −α deformed logarithm deﬁned as
ln1−α (z) =
 zα−1
α , α ̸= 0,
ln z,
α = 0,
(21.190)
where z > 0, we observe that the AB-divergence is symmetric with respect to both arguments for
α = β ̸= 0 and it takes the form of a metric distance
D(α,α)
AB (Y∥ˆY) = DE( ln1−α (Y)∥ln1−α ( ˆY)) = 1
2

it
( ln1−α (yit) −ln1−α ( ˆyit))2,
(21.191)

1.21.4 NMF and Related Problems
1203
in the transform domain φ(x) = ln1−α (x). As particular cases, it includes the scaled squared Euclidean
distance (for α = 1) and the Hellinger distance (for α = 0.5).
1.21.4.6.2
The derivation of robust NMF algorithms based on the AB-divergence
We can easily derive NMF algorithms by employing the AB-divergence with Y = [yit] ∈RI×T
+
, ˆY =
[ ˆyit] = ABT ∈RI×T
+
, whereyit = [ABT ]it = 
j ai jbt j. In this case the gradient of the AB-divergence
(21.187) can be expressed in a compact form (for any α, β ∈R) in terms of 1 −α deformed logarithm
(see (21.190))
∂D(α,β)
AB
∂bt j
= −
I

i=1
ˆyλ−1
it
ai j ln1−α
# yit
ˆyit
$
,
(21.192)
∂D(α,β)
AB
∂ai j
= −
T

t=1
ˆyλ−1
it
bt j ln1−α
# yit
ˆyit
$
.
(21.193)
The multiplicative learning rules are gradient descent based in the natural parameter space φ(x) of the
proposed divergence:
ai j ←φ−1

φ(ai j) −ηi j
∂D(α,β)
AB
∂φ(ai j)

,
bt j ←φ−1

φ(bt j) −ηt j
∂D(α,β)
AB
∂φ(bt j)

.
(21.194)
These updates can be considered as a generalization of the exponentiated gradient (EG) algorithm
[130,150].
In general, such a nonlinear scaling (or transformation) provides a stable solution and the resulting
gradients are much better behaved in the φ space. We use the 1 −α deformed logarithm transformation
φ(z) = ln1−α (z), whose inverse transformation is the 1 −α deformed exponential
φ−1(z) = exp1−α (z) =
⎧
⎨
⎩
exp (z)
for α = 0,
(1 + αz)
1
α for α ̸= 0 and 1 + αz ≥0,
0
for α ̸= 0 and 1 + αz < 0.
(21.195)
For positive measures z > 0, the direct transformation φ(z) and the composition φ−1(φ(z)) are bijective
functions which deﬁne a one to one correspondence, so we have φ−1(φ(z)) = z.
By choosing suitable learning rates
ηt j =
b2α−1
t j
I
i=1 ai j ˆyλ−1
it
,
ηi j =
a2α−1
i j
T
t=1 bt j ˆyλ−1
it
.
(21.196)
a generalized multiplicative NMF algorithm (refereed to as the AB-multiplicative NMF algorithm) is
obtained as [130]:
bt j ←bt j exp1−α
 I

i=1
ai j ˆyλ−1
it
I
i=1ai j ˆyλ−1
it
ln1−α
# yit
ˆyit
$
,
ai j ←ai j exp1−α
 T

t=1
bt j ˆyλ−1
it
t
t=1bt j ˆyλ−1
it
ln1−α
# yit
ˆyit
$
,
(21.197)

1204
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
In these equations, the deformed logarithm of order 1 −α of the quotients yit/ ˆyit plays a key role in
controlling the relative error terms whose weighted mean provides the multiplicative corrections. This
deformation in the relative error is controlled by the parameter α. The parameter α > 1 gives more
relevance to large values of the quotient, while the case of α < 1 puts more emphasis on smaller values
of the quotient. On the other hand, the parameter λ −1 (where λ = α + β) controls the inﬂuence of the
values of the approximation ( ˆyit) on the weighing of the deformed error terms. For λ = 1, this inﬂuence
disappears.
It is interesting to note that the multiplicative term of the main updates
Mα(z, w, S) = exp1−α

1

i∈S wi

i∈S
wi ln1−α (zi)

,
(21.198)
can be interpreted as a weighted generalized mean across the elements with indices in the set S.
Depending on the value of α we obtain the following particular cases: the minimum of the vector
z (for α →−∞), its weighted harmonic mean (α = −1), the weighted geometric mean (α = 0),
the arithmetic mean (α = 1), the weighted quadratic mean (α = 2) and the maximum of the vector
(α →∞), i.e.,
Mα(z, w, {1, . . . , n}) =
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
min{z1, . . . , zn},
α →−∞,
n
i=1 wi
 )n
i=1
wi
zi
*−1
, α = −1,
+n
i=1 z
wi
n
i=1wi ,
α = 0,
1
n
i=1wi
n
i=1 wizi,
α = 1,
)
1
n
i=1wi
n
i=1 wiz2
i
*1/2
, α = 2,
max{z1, . . . , zn},
α →∞.
(21.199)
The generalized weighted means are monotonically increasing functions of α, i.e., if α1 < α2, then
Mα1(z, w, S) < Mα2(z, w, S).
(21.200)
Thus, by increasing the values of α, we put more emphasis on large relative errors in the update formulas
(21.197).
In the special case of α ̸= 0, the above update rules can be simpliﬁed as:
ai j ←ai j
T
t=1bt j yα
it ˆyβ−1
it
T
t=1bt j ˆyα+β−1
it
1/α
,
bt j ←bt j
 I
i=1ai j yα
it ˆyβ−1
it
I
i=1 ai j ˆyα+β−1
it
1/α
,
(21.201)
where ˆyit = [ABT ]it and at every iteration the columns of A are normalized to the unit length. The
above multiplicative update rules can be written in a compact matrix form as
A ←A ⊛[(ZB) ⊘( ˆY.[α+β−1]B)].[1/α],
(21.202)
B ←B ⊛[(ZT A) ⊘([ ˆY.[α+β−1]]T A)].[1/α],
(21.203)
where ˆY = max{AX, ε} and Z = Y.[α] ⊛ˆY.[β−1].

1.21.4 NMF and Related Problems
1205
In order to address the scaling indeterminacy between the columns of A and the rows of X, in practice,
after each iteration, we can usually evaluate the l1-norm of the columns of A and normalize the elements
of the matrices as ai j ←ai j/ 
p apj. This normalization does not alter ˆY = AX, thus, preserving the
value of the AB-divergence.
The above multiplicative update rules are natural extensions of many exiting algorithms for NMF,
includingtheISRA,EMML,Lee-SeungalgorithmsandAlpha-andBeta-multiplicativeNMFalgorithms
[8,130]. For example, by selecting α +β = 1, we obtain the Alpha-NMF algorithm, for α = 1, we have
Beta-NMF algorithms, for α = −β ̸= 0 we obtain a family of multiplicative NMF algorithms based
on the extended Itakura-Saito distance [19,148]. Furthermore, for α = 1 and β = 1, we obtain the
ISRA algorithm and for α = 1 and β = 0 we obtain the EMML (Expectation Maximization Maximum
Likelihood) algorithm.
It is important to note that in low-rank approximations, we do not need access to all input data yit.
In other words, the above algorithms can be applied for low-rank approximations even if some data
are missing or they are purposely omitted or ignored. For large-scale problems the learning rules can
be written in a more efﬁcient form by restricting the generalized mean only to those elements whose
indices belong to the preselected subsets ST ⊂{1, . . . , T } and SI ⊂{1, . . . , I} of the whole set of
indices [130].
Using a duality property (D(α,β)
AB (Y∥ˆY) = D(β,α)
AB ( ˆY∥Y)) of the AB-divergence, we obtain now the
dual update rules for β ̸= 0
ai j ←ai j

t∈ST bt j yα−1
it
ˆyβ
it

t∈ST bt j yα+β−1
it
1/β
,
bt j ←bt j

i∈SI ai j yα−1
it
ˆyβ
it

i∈SI ai j yα+β−1
it
1/β
.
(21.204)
1.21.4.6.3
Why is the AB-divergence potentially robust?
To illustrate the role of the hyperparameters α and β on the robustness of the AB-divergence with respect
to errors and noises we will compare the behavior of the AB-divergence with the standard Kullback–
Leibler divergence [130]. We will assume, without loss of generality, that the proposed factorization
model ˆY (for given noisy (observed) Y) is a function of the vector of parameters θ and that each of its
elements ˆyit(θ) > 0 is non-negative for a certain range of the parameters, say .
The estimator ˆθ obtained for the Kullback-Leibler divergence between two discrete positive measures
Y and ˆY, is the solution of
∂DK L(Y∥ˆY)
∂θ
= −

it
∂ˆyit
∂θ ln0
# yit
ˆyit
$
= 0,
(21.205)
while, for the Beta-divergence, the estimator solves
∂D(β)
B (Y∥ˆY)
∂θ
= −

it
∂ˆyit
∂θ ˆyβ
it ln0
# yit
ˆyit
$
= 0.
(21.206)
The main difference between these equations is in the weighting factors ˆyβ
it for the Beta-divergence
which are controlled by the parameter β. In the context of probability distributions, these weighting

1206
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
factors may control the inﬂuence of the likelihood ratios yit/ ˆyit. The parameter β determines a tradeoff
between robustness to outliers (for β > 0) and efﬁciency (for β near 0) (see e.g., [130]). In the special
case of β = 1 the Euclidean distance is obtained, which is known to be more robust and less efﬁcient
than the Kullback–Leibler divergence (for β = 0).
On the other hand, for the Alpha-divergence, the estimating equation takes a different form
∂D(α)
A (Y∥ˆY)
∂θ
= −

it
∂ˆyit
∂θ ln1−α
# yit
ˆyit
$
= 0.
(21.207)
In this case, the inﬂuence of the values of individual ratios yit/ ˆyit is controlled not by weighting factors
but by the deformed logarithm of order 1 −α. This feature can be interpreted as a zoom or over-weight
of the interesting details of the likelihood ratio. For α > 1 (a zoom-out), we emphasize the relative
importance of larger values of the ratio yit/ ˆyit, whereas for α < 1 (a zoom-in), we put more emphasis on
smaller values of yit/ ˆyit. The major consequence is the inclusive (α →∞) and exclusive (α ←−∞)
behavior of the Alpha-divergence discussed by [147].
The estimating equation for the AB divergence combines both effects:
∂D(α,β)
AB (Y∥ˆY)
∂θ
= −

it
∂ˆyit
∂θ
ˆyα+β−1
it
  
weights
ln1−α (yit/ ˆyit)



α-zoom
= 0,
(21.208)
and therefore is much more ﬂexible and powerful regarding robustness to error and noise. Depending on
the value of α we can zoom-in or zoom-out the interesting sets of the ratios yit/ ˆyit and simultaneously
weight these ratios by scaling factors ˆyλ−1
it
controlled by the parameter λ = α + β. Therefore, the
parameter α can be used to control the inﬂuence of large or small ratios in the estimator, while the
parameter β provides some control on the weighting of the ratios depending on the demand to provide
better ﬁt to larger or smaller values of the model [130].
1.21.4.7 Convolutive NMF
The Convolutive NMF (CNMF) is a natural extension and generalization of the standard NMF. In the
CNMF, we process a set of nonnegative matrices or patterns which are shifted (or time delayed) versions
of the primary matrix X [151]. In its simplest form the CNMF can be described as
Y =
R−1

r=0
ArXTr + E =
R−1

r=0
Ar
r→
X +E,
(21.209)
where Y ∈RI×T
+
is a given input data matrix, Ar ∈RI×J
+
is a set of unknown nonnegative basis
matrices, X =
0→
X ∈RJ×T
+
is a matrix representing primary sources or patterns, and
r→
X is a shifted by
r columns version of X. In other words,
r→
X means that the columns of X are shifted to the right r spots
(columns), while the entries in the columns shifted into the matrix from the outside are set to zero. We
denote a shift matrix Tr of size T × T by a binary matrix with ones only on the rth superdiagonal for

1.21.4 NMF and Related Problems
1207
r > 0, or on the rth subdiagonal for r < 0, and zeroes elsewhere.
r→
X = XTr is an r column shifted
version of X to the right, with the columns shifted in from outside the matrix set to zero. Note that,
0→
X =
←0
X = X.
The shift operator can be illustrated by the following simple example:
X =
 1 2 3
4 5 6

,
1→
X =
 0 1 2
0 4 5

,
2→
X =
0 0 1
0 0 4

,
←1
X =
2 3 0
5 6 0

.
The goal is to estimate the input sources represented by the nonnegative matrix X ∈RJ×T
+
(typi-
cally, T ≫J) and to identify the convoluting system, i.e., to estimate a set of nonnegative matrices
{A0, A1, . . . , AR−1} (Ar ∈RI×J
+
, r = 0, 1, . . . , R −1) knowing only the input data matrix Y ∈RI×T .
Each operator Tr(r = 0, 1, . . . , R −1) performs a horizontal shift of the columns in X by one spot.
In the simplest case scenario, update rules for Ar and X can be derived by minimizing a standard
cost function:
D(Y∥Y) = 1
2∥Y −,
Y∥2
F = 1
2∥Y −
R−1

r=0
ArXTr∥2
F.
(21.210)
Note that, the approximation of Y can be expressed as a standard NMF problem with rank-JR as
Y =
A0 A1 · · · AR−1

⎡
⎢⎢⎢⎢⎢⎣
X
1→
X
...
(R−1)→
X
⎤
⎥⎥⎥⎥⎥⎦
+ E = AB
T + E,
(21.211)
where A = [A0, A1, . . . , AR−1] ∈RI×RJ and B = [X,
1→
X , . . . ,
(R−1)→
X
]T ∈RRT ×J.
From (21.209) and (21.210), we can apply any existing standard NMF algorithm. For example, we
can employ a simple multiplicative update rule using the ISRA algorithm [8]:
A ←A ⊛(YB) ⊘(YB) = A ⊛[YTT
r XT ]R−1
r=0 ⊘[YTT
r XT ]R−1
r=0 ,
(21.212)
which can be rewritten for the component matrices Ar,
Ar ←Ar ⊛(YTT
r XT ) ⊘(YTT
r XT ) (r = 0, 1, . . . , R −1).
(21.213)
X ←X ⊛
)R−1
r=0 AT
r YTT
r
*
⊘
)R−1
r=0 AT
r YTT
r
*
.
(21.214)
The update rules (21.213)–(21.214) are particular cases of the multiplicative algorithm for CNMF2D
proposed ﬁrst in [152] and extended in [153].
For the more general scenarios, the CNMF searches for basis patterns which shift vertically [154]
or horizontally [155] or in both directions [152] in the nonnegative data matrix. More advanced and
efﬁcient algorithms for generalized and ﬂexible CNMF models can be found in [153,156].

1208
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
The CNMF has found a number of applications in music analysis, source detection, image processing
[155,157]. In the CNMF model, temporal continuity exhibited by many audio signals are usually
expressed efﬁciently in the time-frequency domain, especially for signals whose frequencies vary with
time.
1.21.5 Future directions: constrained multi-block tensor factorizations
and multilinear blind source separation
In this section, we discuss an important problem: how to extend established and efﬁcient algorithms
for constrained matrix factorization techniques (especially, PCA, ICA and NMF) to tensor scenarios.
In what follows, this approach is referred to as multiway Generalized Component Analysis (GCA) or
multilinear blind source separation (MBSS).
The constrained matrix factorization techniques are widely applied to PCA, ICA, and NMF, dimen-
sionality reduction, data compression and feature extraction. Although standard 2D BSS (constrained
matrix factorizations) approaches, such as PCA, ICA, NMF, and their variants, are invaluable tools
for BSS, feature extraction and selection, dimensionality reduction, noise reduction, and data mining,
it should be noted that they have only two modes or two-way representations (typically, space and
time), and their use is therefore limited. In many applications the data structures often contain higher-
order ways (modes) such as subjects, groups, trials, classes, and conditions, together with the intrinsic
dimensions of space, time, and frequency. For example, studies in neuroscience often involve multiple
subjects (people or animals) and trials, leading to experimental data structures conveniently represented
by multiway arrays or blocks of multiway data.
How to ﬁnd informative and sparse/compact representations of experimental or measured multi-
dimensional large tensor data is a fundamental and challenging problem in data mining and data analysis.
1.21.5.1 Basic tensor multilinear operations
Tensors are denoted by underlined capital boldface letters, e.g., Y ∈RI1×I2×···×IN . The order of a tensor
is the number of modes, also known as ways or dimensions (e.g., space, time, frequency, subjects, trials,
classes, groups, and conditions). In contrast, matrices (second-order tensors) are denoted by boldface
capital letters, e.g., Y; vectors (ﬁrst-order tensors) are denoted by boldface lowercase letters, e.g., the
columns of the matrix A are denoted by a j and scalars are denoted by lowercase letters, e.g., ai j.
Throughout this section, standard notations and basic tensor operations proposed in the literature
[5,8] are used. Speciﬁcally, the mode-n product
Y = G ×n A
(21.215)
of a tensor G ∈RJ1×J2×···×JN and a matrix A ∈RI×Jn is a tensor Y ∈RJ1×···×Jn−1×I×Jn+1×···×JN ,
with elements
y j1, j2,..., jn−1,i, jn+1,..., jN =
Jn

jn=1
(g j1, j2,...,JN )(ai, jn).
(21.216)

1.21.5 Future Directions
1209
The mode-n multiplication of a tensor Y ∈RI1×I2×···×IN by a vector a ∈RIn is denoted by
Y ¯×na
(21.217)
and has dimension I1 × · · · × In−1 × In+1 × · · · × IN, that is,
Z = Y ¯×na ∈RI1×···×In−1×In+1×···×IN ,
(21.218)
and element-wise, we have
zi1,i2,...,in−1,in+1,...,iN =
In

in=1
yi1,i2,...,iN ain.
(21.219)
A bar over the operator × indicates a contracted product. For example, using this notation we can write:
Yw = Y ¯×2w and vT Yw = Y ¯×1v ¯×2w. Multiplying a third-order tensor by vectors in two modes
results in a ﬁrst-order tensor (a vector); multiplying it in all modes results in a scalar. We can exchange
the order of multiplication by the using following rule:
Y ¯×ma ¯×nb = (Y ¯×ma) ¯×nb = (Y ¯×nb) ¯×ma,
(21.220)
For example, the mode-n multiplication of a tensor G ∈RJ×R×P by vectors a ∈RJ, b ∈RR and
c ∈RP can be expressed as
z = G ¯×1a ¯×2b ¯×3c =
J

j=1
R

r=1
P

p=1
g jrpa jbrcp.
More generally, for G ∈RJ1×J2×···×JN and a(n) ∈RJn, the multiplication by all vectors in all modes
(n = 1, 2, . . . , N) gives a scalar:
y = G ¯×1a(1) ¯×2a(2) · · · ¯×N a(N) = G ¯×{a} ∈R,
(21.221)
whereas the multiplication in every mode except mode-n results in a vector x of length Jn:
x = G ¯×1a(1) · · · ¯×n−1a(n−1) ¯×n+1a(n+1) · · · ¯×N a(N)
(21.222)
= G(n)(a(N) ⊗· · · ⊗a(n+1) ⊗a(n−1) ⊗· · · ⊗a(1)) = G ¯×−n{a} ∈RJn.
Also note that multiplication in every mode except mode-n and mode-m results in a matrix of
size Jn × Jm.
The mode-1 product of two tensors: A ∈RI1×I2×···×IN and B ∈RJ1×J2×···×JM , e.g., with a common
ﬁrst mode I1 = J1 is deﬁned as a tensor:
C = A ×1 B ∈RI2×···×IN ×J2×···×JM ,
(21.223)
with entries ci2:N ,j2:M
=
I1
i=1 ai,i2:N bi,j2:M , (where we use the shorthand MATLAB notation
ip:q = {i p, i p+1, . . . , iq−1iq}).

1210
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
The outer product C = A ◦B of two tensors A ∈RI1×···×IN and B ∈RJ1×···×JM is a tensor
C ∈RI1×···×IN ×J1×···×JM with entries ci1,...,iN , j1,..., jM = ai1,...,iN b j1,..., jM . Speciﬁcally, the outer prod-
uct of two nonzero vectors a ∈RI, b ∈RJ produces a rank-1 matrix X = a ◦b = abT ∈RI×J and the
outer product of three nonzero vectors: a ∈RI, b ∈RJ and c ∈RK produces a 3rd order rank-1 tensor:
X = a ◦b ◦c ∈RI×J×K , whose entries are xi jk = aib jck. A tensor X ∈RI1×I2×···×IN is said to be
rank-1 if it can be expressed exactly by X = b1 ◦b2 ◦· · · ◦bN with entries xi1,i2,...,iN = bi1bi2 · · · biN ,
where bn ∈RIn are nonzero vectors.
The symbol ⊗denotes the Kronecker product, i.e., A ⊗B = [ai jB], and the symbol ⊙denotes the
Khatri-Rao product or column-wise Kronecker product, i.e., A ⊙B = [a1 ⊗b1 · · · aJ ⊗bJ].
Subtensors are formed when a subset of indices is ﬁxed. Particularly, a ﬁber is deﬁned as a vector by
ﬁxing every index but one, while a tensor slice is a two-dimensional section (matrix) of a tensor, obtained
by ﬁxing all indices but two. Unfolding (matricization, ﬂattening) of a tensor Y ∈RI1×I2×···×IN in the
n-modeisdenotedasY(n) ∈RIn×(I1···In−1In+1···IN ),whichconsistsofarrangingallpossiblen-modeﬁbers
as the columns of a matrix [5]. For simplicity, we deﬁne ˘Ik = n̸=k In, ˘Jk = n̸=k Jn. -
n̸=k A(n) =
A(N) ⊗· · · A(n+1) ⊗A(n−1) · · · ⊗A(1) and .
n̸=k A(n) = A(N) ⊙· · · A(k+1) ⊙A(k−1) · · · ⊙A(1). An
n-mode ﬁber is obtained by ﬁxing all indices except the index for one dimension. For a third-order tensor
Y ∈RI×T ×K its n-mode ﬁbers are called column ﬁbers (n = 1), row ﬁbers (n = 2) and tube ﬁbers
(n = 3).1 In other words, the unfolding operation consists of arranging the ﬁbers of an N-way array
into a matrix in a speciﬁc order, e.g., in reverse lexicographical order [5]. Given an Nth-order tensor
Y ∈RI1×I2×...×IN we deﬁne the n-mode unfolding matrix by Y(n) ∈RIn×+
m̸=n Im, which is obtained
by grouping all indices for dimensions m ̸= n. For example, given a third-order tensor Y ∈RI×T ×K
the corresponding 1-mode unfolding matrix is obtained by creating a new index, corresponding to the
grouping of indices t and k, which is denoted by Y(1)(i, (tk)) = yitk.
We use calligraphic style letters to denote a subset of indices for a given dimension, for example,
for 3D tensors with indices i = 1, 2, . . . , I, t = 1, 2, . . . , T and k = 1, 2, . . . , K, we deﬁne the
subsets containing P1 ≤I, P2 ≤T , P3 ≤K indices as: I = [i1, i2, . . . , iP1], T = [t1, t2, . . . , tP2],
and K = [k1, k2, . . . , kP3]. Then we deﬁne the subtensor W ∈RP1×P2×P3 as the one determined by
indices I, T and K, i.e., W = Y(I, T , K) and its unfolding matrices will be denoted by W(1), W(2)
and W(3). We may also extract matrices containing the tensor ﬁbers determined by these subsets of
indices as follows: C(1) = Y(1)( :, T × K) ∈RI×P2 P3, C(2) = Y(2)( :, I × K) ∈RT ×P1 P3 and
C(3) = Y(3)( :, I × J ) ∈RK×P1 P2 for the column, row, and tube ﬁbers respectively, where “:” denotes
all the indices in a dimension and J × K denotes all indices produced as a combination of an index
in J and an index in K [120]. Readers can refer to [5,8] for more details regarding the notations and
tensor operations.
1.21.5.2 Basic tensor decompositions models
There are two basic models for tensor decompositions: Tucker and CP (CANDECOMP/PARAFAC)
decomposition. The Tucker-N decomposition or rank-J1, J2, . . . , JN Tucker model, the data is decom-
posed as the product of a core tensor with N mode factor (component) matrices [6], i.e., a given data
1For convenience, for third-order tensors we use simpliﬁed indices I = I1, T = I2 and K = I3.

1.21.5 Future Directions
1211
=
Y
G
E
+
(
)
I
J
3
3
x
B
(3)
B
(1)
B
(2)T
I1
I3
(
)
I
J
1
1
x
(
)
J
J
J
1
2
3
x
x
(
)
J
I
2
2
x
I2
(
)
I
I
I
1
2
3
x
x
(
)
I
I
I
1
2
3
x
x
(a)
Y
B
(1)
(
)
I
I
I
1 x
x
2
3
(
)
J
I
I
x
x
1
2
3
(
)
I
J
x
1
1
G
(1)
B
(2)T
G
(2)
(
)
I
J
I
x
x
1
2
3
(
)
J
I
x
2
2
G
(3)
B
(3)
(
)
I
I
J
x
x
1
2
3
(
)
I
J
x
3
3
(b)
FIGURE 21.8
Illustration of a third-order tensor decomposition (a) using a constrained Tucker-3 model: Y = G ×1 B(1) ×2
B(2) ×3 B(3) + E = [[G; B(1), B(2), B(3)]] + E = J1,J2,J3
j1,j2,j3 gj1j2j3(b(1)
j1 ◦b(2)
j2 ◦b(3)
j3 ) + E. The objective is to estimate
the components b(n)
jn , i.e., the columns of the component matrices B(n) = [b(n)
1 , b(n)
2 , . . . , b(n)
Jn ] ∈RIn×Jn,
with desired diversities or statistical properties and a possibly sparse core tensor G ∈RJ1×J2×J3, typically
with Jn ≪In, (n = 1, 2, 3). Note that some selected component matrices from the Tucker-3 model can
be absorbed in the core tensor G, which leads to Tucker-1 models (b): Y = G(n) ×n B(n) + En. Instead of
applying the standard Alternating Least Squares (ALS) algorithms to the Tucker-3 model, we can apply the
unfolding of the data tensor according to Tucker-1 models and then perform constrained matrix factorizations
for the unfolded matrices (multilinear BSS) by imposing desired constraints (nonnegativity, sparseness,
independence, smoothness, or decorrelation, etc.). More generally, we wish to model complex interactions
embedded in multidimensional data and/or predict missing entries.
tensor Y ∈RI1×I2×···×IN , is decomposed as (see Figure 21.8a)
Y = G ×1 B(1) ×2 B(2) · · · ×N B(N) + E = Y + E
=
J1

j1=1
· · ·
JN

jN =1
g j1 j2··· jN (b(1)
j1 ◦b(2)
j2 ◦· · · ◦b(N)
jN ) + E,
(21.224)
where G ∈RJ1×J2×···×JN is the core tensor, B(n)
= [b(n)
1 , b(n)
2 , . . . , b(n)
Jn ] ∈RIn×Jn is the
mode-n component matrix for n = 1, 2, . . . , N, ◦denotes the outer product, and the tensor E ∈
RI1×I2×···×IN represents errors or noise. For simplicity, we also use a shorter notation for (21.224) as
Y = G; B(1), B(2), . . . , B(N) .

1212
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
The Tucker-N model with orthogonal factor matrices B(n) is also known Multilinear SVD or Higher-
order SVD (HOSVD) [9]. In fact, the original Tucker model made the assumptions of orthogonality
of the factor matrices (analogous to SVD) and orthogonality of the core tensor [8,138,158,159]. How-
ever, in many practical applications we ignore or relax such constraints to make model more ﬂexible.
For example, by imposing nonnegativity constraints the problem of estimating the component matri-
ces and the core tensor is converted into a generalized NMF problem called the Nonnegative Tucker
Decomposition (NTD). Several implementations of Tucker decomposition algorithms with nonnega-
tivity constraints together with a number of other constraints are given in [8,138,158,160]. The NTD
imposes nonnegativity constraints for all component matrices and the core tensor, while a semi-NTD
(analogous to semi-NMF) imposes nonnegativity constraints to only some component matrices and/or
some elements of the core tensor [8].
Note that the Tucker-N model (21.224) can be represented equivalently by a set of N different matrix
factorizations with three factors:
Y(n) ≈B(n)G(n)Z(n)
(n = 1, 2, . . . , N),
(21.225)
where the matrix Z(n) = [B(N) ⊗· · · ⊗B(n+1) ⊗B(n−1) · · · ⊗B(1)]T was the Kronecker structure.
In many applications, we usually need to estimate only a pre-selected number of component matrices.
Without loss of generality, let us assume for simplicity that we are interested in extractions only the ﬁrst
K component matrices, with K ≤N. In that case, we can use a simpliﬁed Tucker-K model described as
Y = ˘G ×1 B(1) ×2 B(2) · · · ×K B(K) + E,
(21.226)
where the core tensor ˘G ∈R
J1···×JK ×IK+1···×IN is expressed as
˘G = G ×K+1 B(K+1) ×K+2 B(K+2) · · · ×N B(N).
(21.227)
Furthermore, the Tucker decomposition (21.224) can be represented by using simple Tucker-1 decom-
positions, with only one component matrix B(n) in each mode (see Figure 21.8b):
Y ≈G(¯n) ×n B(n)
or in matrix form Y(n) ≈B(n)G(¯n)
(n),
(21.228)
where G(¯n) = G ×1 B(1) · · · ×n−1 B(n−1) ×n+1 B(n+1) · · · ×N B(N) is a tensor, (n = 1, 2, . . . , N). This
means that Tucker decomposition problems can be converted to a set of matrix factorization problems,
with suitable constraints imposed on the component matrices in order to achieve essential uniqueness.
In the special case, when the core tensor G is a hypercube, i.e., J1 = J2 = · · · = JN = J, and
it has nonzero entries only on its diagonal, the Tucker model is reduced to the CP model (CANDE-
COMP/PARAFAC) decomposition, analyzed by two independent research groups [2,4,161]). In the CP
model a data tensor is represented as a linear combination of outer products of columns (components)
of the component matrices:
Y = G ×1 B(1) ×2 B(2) · · · ×N B(N) + E
=
J

j=1
λ j b(1)
j
◦b(2)
j
· · · ◦b(N)
j
+ E,
(21.229)

1.21.5 Future Directions
1213
C
(
)
I
T Q
x
x
(
)
I
J
x
(
)
J
J
J
x
x
(
)
J
T
x
(
)
Q
J
x
=
Y
A
G
+
+
c1
a1
b1
cj
aj
bj
aJ
bJ
cJ
1
J
X=B
T
FIGURE 21.9
Illustration of a 3-way CP model performing tensor decomposition Y ∼= G×1 A×2 B×3 C = J
j=1 λj(aj ◦bj ◦cj)
with a diagonal core tensor G =  ∈RJ×J×J, with entries λjjj = λj.
where G =  ∈RJ×J×···J is a diagonal tensor with all entries zero except the diagonal entries λ j.
We usually assume that the vectors b(n)
j
(for n = 1, 2, . . . , N) are normalized to unit length. In other
words, the CP model represents multiway data as a sum of rank-1 components [5,162].
The CP model decomposes a given tensor into a sum of multilinear terms, in a way analogous to
the bilinear matrix decomposition (see Figure 21.9). Unlike standard SVD, usually CP does not impose
any constraints. Since model is unique under mild conditions. A model which imposes nonnegativity
on factor matrices is called a Nonnegative Tensor Factorization (NTF) or Nonnegative PARAFAC. A
nonnegative version of CP was ﬁrst introduced by Carroll et al. [163]. Later, more efﬁcient approaches
were developed [164,165], based on the modiﬁed NLS (Nonnegative Least Squares) by Paatero [166,
167] who extended his earlier 2-way positive matrix factorization (PMF) method to the three-way
CP model, referring to the result as PMF3 (three-way positive matrix factorization). Although such
constrained nonnegativity based model may not match perfectly the input data (i.e., it may have larger
residual errors E than the standard CP without any constraints), such decompositions are often more
meaningful and have clearer physical interpretations [8,34].
The advantage of the Tucker model over the CP model is that the Tucker model is more general since
the number of components in each mode can be different, and the components are linked via a core
tensor, and hence it allows us to model more complex hidden data structures. Both the CP and the Tucker
decompositions attempt to obtain optimal low-rank (J or {J1, J2, . . . , JN}, respectively) approximations
to the original data. However, quite different from the matrix case, the optimal low-rank approximation
may not exist at all or, if it exists, it may not be unique for high-order tensors [168,169]. Particularly, an
unconstrained Tucker decomposition is in general non-unique, as it has arbitrary rotational ambiguity.

1214
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
Due to many degrees of freedom, the results of such decompositions are difﬁcult to interpret because
the components do not have a clear physical meaning. The CP model in contrast is essentially unique
if the fundamental Kruskal uniqueness conditions are satisﬁed, which was addressed by Kruskal for
third-order tensors and then extended to Nth-order tensors by Sidiropoulos et al. [170,171]. These are
sufﬁcient (but not necessary) conditions. Basically, algebraic properties, e.g., the ranks of the tensors
and their component matrices, play a central role for such uniqueness analyze.
If we impose additional constraints to the CP model, such as nonnegativity, sparsity, or orthogonality,
the conditions for uniqueness can be relaxed. Moreover, by imposing the nonnegativity or orthogonality
constraints the CP model has an optimal solution, i.e., there is no risk for degenerate solutions [172].
Imposing nonnegativity constraints makes degenerate solutions impossible, since no factor can coun-
teract the effect of another factor, and usually it improves the convergence of the learning algorithms
since the search space is greatly reduced. For the Tucker and CP decompositions there exist many
algorithms [5,8]. Most of them are based on the ALS (Alternating Least Squares), HALS (Hierarchical
ALS) [8,34,173,174] and CUR tensor decompositions [120]. However, the ALS algorithms are often
slow for noisy or ill-conditioned data and they may get stuck in local minima due to the non-convexity
of the optimization problem, especially when nonnegativity or other constraints are imposed on the
component matrices.
In contrast to most of the existing tensor decomposition methods, where only the best ﬁt of a
decomposed tensor to the original data tensor is pursued, in this section, the multilinear Blind Source
Separation (MBSS) approach is brieﬂy described and we show how to perform constrained tensor
decompositions in which both the satisfactory ﬁt and a wide variety of desired properties/constraints
and diversities of estimated components can be pursued simultaneously.
1.21.5.3 Sparse CP decomposition using the generalized power method
AlthoughtheCPtensordecompositionisingeneraluniquewithoutanyconstraints,insomeapplications,
especially for large-scale problems, we need components b(n)
j
that are sparse in one speciﬁc mode or in
all modes. Let us consider now the basic CP model. In order to estimate the ﬁrst set of components we
can consider the following optimization problem (see Section 1.21.2.6.3)
min
b(n)
j
⎧
⎪⎨
⎪⎩
//////
Y −
J

j=1
λ j(b(1)
j
◦b(2)
j
· · · ◦b(N)
j
)
//////
2
F
⎫
⎪⎬
⎪⎭
.
Assuming that we want to extract the desired components sequentially one by one, we can ﬁrst consider
the constrained optimization problem
min
b(n)
1
{∥Y −λ1(b(1)
1
◦b(2)
1 · · · ◦b(N)
1
)∥
2
F},
s.t. ∥b(n)
1 ∥2 ≤1, ∥b(n)
1 ∥1 ≤1, P1(b(n)
1 )∀C1 ≤c1, P2(b(n)
1 ) ≤c2
∀n.

1.21.5 Future Directions
1215
Algorithm 5. Power Method for Sparse CP Decomposition
Require: Data tensor Y(1) = Y, sparsity coefﬁcients αn ≥0, (n = 1, . . . , N) and initial vectors b(n)
j .
1: For j = 1, 2, . . . , J and n = 1, 2, . . . , N:
(a) Repeat until convergence of b(n)
j ;
2:
b(n)
j
= Y( j) ¯×1b(1)
1
· · · ¯×n−1b(n−1)
1
¯×n+1b(n+1)
1
· · · ¯×N b(N)
1
;
3:
b(n)
j
=
S1(b(n)
j ,αn)
∥S1(b(n)
j ,αn)∥2
(for αn = 0 take b(n)
j
=
b(n)
j
∥b(n)
j ∥2
);
4: (b) λ j = |Y( j) ¯×1b(1)
j
¯×2b(2)
j
· · · ¯×N b(N)
j
|;
5: (c) Y( j+1) = Y( j) −λ j(b(1)
j
◦b(2)
j
· · · ◦b(N)
j
);
6: return B(n) = [b(n)
1 , b(n)
2 , . . . , b(n)
J ],
∗= diag{λ1, λ2, . . . , λJ }.
The problem is equivalent to the following optimization problem
max
b(n)
1
{Y ¯×1b(1)
1
¯×2b(2)
1 · · · ¯×N b(N)
1
},
(21.230)
s.t. ∥b(n)
1 ∥2 ≤1,
∥b(n)
1 ∥1 ≤1, P1(b(n)
1 ) ≤c1, P2(b(n)
1 ) ≤c2 ∀n.
If we impose sparsity constraints, we can formulate the following optimization problem
max
b(n)
1
{Y ¯×1b(1)
1
¯×2b(2)
1 · · · ¯×N b(N)
1
−

n
αn∥b(n)
1 ∥1},
s.t. ∥b(n)
1 ∥2 ≤1 ∀n.
This optimization leads to the following simple update rules
b(n)
1
=
S1(b(n)
1 , αn)
∥S1(b(n)
1 , αn)∥2
,
(21.231)
where
S1(b, α) = sign(b)[|b| −α]+,
(21.232)
and
b(n)
1
= Y ¯×1b(1)
1 · · · ¯×n−1b(n−1)
1
¯×n+1b(n+1)
1
· · · ¯×N b(N)
1
.
(21.233)
This update formula simpliﬁes in the absence of sparsity constraints (i.e., αn = 0∀n) as
b(n)
1
=
b(n)
1
∥b(n)
1 ∥2
.
(21.234)
In order to extract the next set of components, we need to apply deﬂation approach for j = 1, 2, . . . , J
λ j = |Y( j) ¯×1b(1)
j
¯×2b(2)
j
· · · ¯×N b(N)
j
|,
(21.235)
Y( j+1) = Y( j) −λ j(b(1)
j
◦b(2)
j
· · · ◦b(N)
j
),
(21.236)
with Y(1) = Y (see Algorithm 5).

1216
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
1.21.5.4 Multilinear BSS via constrained unique Tucker decompositions
There are two possible approaches to represent constrained Tucker decompositions for the MBSS. In the
ﬁrst approach the columns of the component matrices B(n) represent the desired components or latent
variables, and the core tensor G represents some kind of a "mixing process". More precisely, the core
tensor shows the links among the components from different modes, while the data tensor Y represents
a collection of 1-D or 2-D mixing signals. In the second approach the core tensor represents the desired
(but unknown hidden) N-dimensional signal (e.g., 3D MRI image or 4D video), and the component
matricesrepresentspeciﬁctransformations,e.g.,timefrequencytransformationsorwaveletsdictionaries
(mixing or ﬁltering processes). In this case the data tensor Y represents observed N-dimensional signal,
which may be distorted, transformed, compressed, or mixed, depending on the speciﬁc applications. In
this section, we will only consider the ﬁrst interpretation or approach.
We can implement multilinear BSS algorithms in several ways. First of all, to estimate desired
components, we can minimize a global cost function with suitable penalty and regularization terms
(see Eq. (21.224)):
DF(Y∥Y) = ∥Y −G ×1 B(1) ×2 B(2) · · · ×N B(N)∥
2
F +

n
αn Dn(B(n)),
(21.237)
where ∥Y∥F = ( I1
i1=1 · · · IN
iN =1 y2
i1···iN )
1
2 ; αn ≥0 are penalty coefﬁcients and Dn(B(n)) are penalty
terms, which are added to achieve desired characteristics of the components. For example, if we
need to impose mutual independence constraints the penalty terms can take the following form:
Dn(B(n)) = J
j=1

p̸= j b(n)T
j
fn(b(p)
n ), where fn(u) are suitable nonlinear functions.
In principle, this way leads to penalized ALS algorithms which allow us to ﬁnd the component
matrices B(n) and the associated core tensor. However, the method involves heavy computations and it
is very time consuming.
A simpler, practical, and much more efﬁcient approach is to exploit unfolding in each mode of the
data tensor Y, according to the Tucker-N or Tucker-1 decompositions (21.228), to allow us to directly
apply suitable constrained low-rank matrix factorizations algorithms (PCA/SVD, ICA, NMF, SCA).
Note that in practice, for large-scale problems, we do not need to perform explicitly the unfolding
of the full data tensor. Instead, we may apply fast sampling of tubes (columns) of data tensors [120].
For the Tucker decomposition, let Y ≈G; B(1), B(2), . . . , B(N) and assume that only the factor
matrices (whose columns represent the components) B(n), (n = 1, 2, . . . , N), are subject of interest.
Then we have
Yn = YT
(n) ∼= An[B(n)]T
(n = 1, 2, . . . , N),
(21.238)
where the basis matrices have a Kronecker structure:
An = (B(N) ⊗· · · ⊗B(n+1) ⊗B(n−1) ⊗· · · ⊗B(1))GT
(n)
=
⎛
⎝
N
3
p=1,p̸=n
B(p)
⎞
⎠GT
(n).
(21.239)

1.21.5 Future Directions
1217
For the CP model, the basis (mixing) matrices An take much simpler forms (with Khatri-Rao
structure):
An = (B(N) ⊙· · · ⊙B(n+1) ⊙B(n−1) · · · ⊙B(1)) =
⎛
⎝
N
4
p=1,p̸=n
B(p)
⎞
⎠,
(21.240)
where  is a diagonal matrix whose diagonal elements consist of the diagonal entries λ j of the core
tensor G =  (see Figure 21.9).
Note that the above models correspond to group BSS/ICA models (21.7) with the following corre-
spondences: Yn = YT
(n), An = An, and Bn = B(n), forall n, if we impose desired constraints on the
component matrices B(n). From (21.238), we can observe that the columns of the mode-n matricization
of Y are just the linear mixtures of the mode-n component matrices B(n), (n = 1, 2, . . . , N). This sug-
gests that we can use various available BSS algorithms to directly extract factor matrices with speciﬁc
properties and diversities. For example, if apply SVD in each mode, we obtain HOSVD, and if apply
ICA algorithms we have multilinear ICA [32,175].
Once all desired component matrices B(n), n = 1, 2, . . . , N have been estimated, the core tensor
can be computed using any suitable matrix factorization method which provides an essentially unique
decomposition method. In the simplest scenario we can apply the following formula:
G = Y ×1 B(1)† ×2 B(2)† · · · ×N B(N)†,
(21.241)
where B† denotes the Moore–Penrose inverse of a matrix B.
In general, G ̸= G due to the unavoidable ambiguities of the BSS methods. However, the links
between the extracted components remain unchanged, except for the ambiguities due to permutation
and scaling. The approach described above is called multilinear BSS (MBSS), where multiple sets of
components B(1), B(2), . . . , B(N) can be extracted separately from multiple modes of a tensor.
The MBSS approach has two major advantages [32]:
1. The extracted component matrices B(n) are essentially unique (neglecting scaling and permutation
ambiguities) and have speciﬁc desired properties or feature information, under conditions that
apply a suitable BSS/PMD algorithm which provide a unique decomposition in each mode.
2. The component matrices of interest can be extracted directly, i.e., independently of the knowledge
of other component matrices, due to application of a standard BSS for each mode separately or
independently. In other words, using the MBSS, tensor decompositions can be achieved without
ALS iterations. Note that in the ALS-based algorithms tensors have to be unfolded to each mode
frequently, which is the main bottleneck to achieve high efﬁciency, especially for large-scale data.
In contrast, the MBSS approach allows us to dramatically reduce the computational complexity,
since each component matrix can be estimated independently on other modes by employing
efﬁcientBSSalgorithms,particularlyinthecasewhereonlyafewpreselectedcomponentmatrices
are of interest [32].
It should be noted that the unfolding matrices YT
(n) ∈R ˘In×In and the bases matrices ¯An ∈R ˘In×Jn are
usually very large-scale (very tall) matrices with ˘In
≫
In and In
≥
Jn. Thus the MBSS

1218
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
problems (21.238) are usually highly over-determined constrained matrix factorization problems and
we need to perform dimensionality reduction ﬁrst. Moreover, we need to estimate the number of com-
ponents in each mode Jn, which in general are unknown. We can solve both problems by applying
low-rank approximations, especially PCA/SVD decompositions or CUR decompositions.
1.21.5.5 CUR tensor decomposition for dimensionality reduction and
compression of large-scale data
Although the PCA/SVD method can be relatively fast, optimal in the least squares sense, and quite
robust with respect to Gaussian noise, unfortunately, it does not preserve the properties of original raw
tensor data and it is rather time consuming for very large-scale problems. For example, the data tensor
can be nonnegative while the reduced unfolding matrices may have in general positive and negative
entries. In may applications, for example in NTF and NTD decompositions, we need to preserve in the
reduced unfolded matrices the basic characteristic of the original data tensor. To overcome the above
problem, we adopted the CUR algorithm for tensor compression and dimensionality reduction [120].
The concept of CUR decomposition has been successfully generalized to tensors in [120,124]. A
practical and fast technique is called Fiber Sampling Tucker Decomposition (FSTD) [120]. Since real-
life data have often good low multilinear rank approximations, FSTD provides such a low-rank Tucker
decomposition that is directly expressed in terms of a relatively small number of ﬁbers (vector columns)
of the data tensor. By virtue of its construction from raw data elements, FSTD preserves the original
characteristics and features of tensor data and can be used for efﬁcient compressed representations of
the original data tensor.
For a given tensor Y ∈RI1×I2×···×IN for which an exact rank-(R1, R2, . . . , RN) Tucker repre-
sentation exists, FSTD selects R indices in each mode, which determine an intersection sub-tensor
W ∈RR×R×···×R so that the following exact Tucker representation is obtained:
Y = U; C(1), C(2), . . . , C(N) ,
(21.242)
in which the core tensor is computed as U = G =
W; W†
(1), W†
(2), . . . , W†
(n) , and the matrices
C(n) ∈RIn×I¯n contain the mode-n ﬁbers deﬁned by restricting the indices in modes m ̸= n to belong
to the selected subsets. This can also be written as a rank-(R, R, . . . , R) Tucker representation:
Y = W; C(1)W†
(1), C(2)W†
(2), . . . , C(N)W†
(N) .
(21.243)
Observe that for N = 2 this model simpliﬁes into the CUR matrix case, X = CUR, where C(1) =
C, C(2) = RT and the core matrix is U = W; W†
(1), W†
(2) = W†WW† = W†.
An efﬁcient strategy for the selection of suitable ﬁbers, only requiring access to a partial (small) subset
of entries of a data tensor through identifying the entries with maximum modulus within single ﬁbers
[120]. The indices are selected sequentially using a deﬂation approach making the FSTD algorithm
suitable for very large scale tensors (including tensors with missing ﬁbers or entries). There are several
strategies for the selection of suitable columns and rows. The main idea is to select columns and rows that
exhibit high “statistical leverage” and provide the best low-rank ﬁt for the data matrix [120,122,123].

1.21.5 Future Directions
1219
CUR decompositions are low-rank tensor decompositions that are explicitly expressed in terms of a
relatively small number of actual ﬁbers (vector columns) of the data tensor. Because they are constructed
from actual raw data elements, CUR tensor decompositions preserve the ordinal characteristics and
features of data and can be used as efﬁcient compressed representations of the original data tensor.
Multilinear BSS and generalized component analysis algorithms, e.g., a combination of PCA, ICA,
SCA,NMF,andMCA,areoftenconsideredaspuremathematicalformulas,powerful,butrathermechan-
ical procedures. There is a misconception that there is not very much left for the user to do after the
machinery has been optimally implemented. However, the successful and efﬁcient use of such tools
strongly depends on a priori knowledge, common sense, and appropriate use of preprocessing and
postprocessing tools. In other words, it is the preprocessing of data and postprocessing of models where
expertise is truly needed in order to extract and identify physically signiﬁcant and meaningful hidden
components.
1.21.5.6 Common component analysis: feature extraction for multi-block tensor
data
Dimensionality reduction, feature extraction and future selection are essential problems in the analysis
of multi-dimensional data sets with large number of variables [12]. We will ﬁrst illustrate the basic
concepts of dimensionality reduction and feature extraction for a set of large-scale sample matrices. Let
us consider that we have available a set of K matrices (2-D samples) Yk ∈RI1×I2, (k = 1, 2, . . . , K)
that represent multi-subjects or multi-trials 2D data, which belong to C different classes or categories
(e.g., multiple mental task/state data or different mental diseases). In order to perform dimensionality
reduction and extract essential features for all the training samples, we apply simultaneous (approxi-
mative and constrained) matrix factorizations (see Figure 21.10a):
Yk = B1FkBT
2 + Ek
(k = 1, 2, . . . , K),
(21.244)
where the two common factors (basis matrices) B1 ∈RI1×J1 and B2 ∈RI2×J2, Jn ≤In, ∀n, code
(explain) each sample Yk simultaneously along the horizontal and vertical dimensions and the extracted
features are represented by matrices Fk ∈RJ1×J2, typically, with J1 ≪I1 and J2 ≪I2. In special
cases Fk are squared diagonal matrices. Then this problem can be considered as a generalization of
Joint Approximative Diagonalization (JAD) [8,84].
The common method to solve such matrix factorizations problem is to minimize a set of cost functions
∥Yk −B1FkBT
2 ∥2
F, ∀k, sequentially or in parallel, with respect to all the factor matrices involved.
Alternatively, we can solve the problem by concatenation or tensorization of all matrices Yk along
the third dimension to form an I1 × I2 × K dimensional data tensor Y ∈RI1×I2×K and perform the
constrained Tucker decomposition (see Figure 21.10b).
In order to obtain meaningful components and a unique decomposition it is often convenient to
impose additional constraints [8,12]. In the special case when the feature matrices Fk are diagonal the
Tucker-2 model is reduced to the special form of the CP model [8].
This approach can be naturally and quite straightforwardly extended to multi-dimensional data.
Assume that we have available a multi-block of multi-dimensional tensors Y(k) ∈RI1×I2×···×IN ,
(k = 1, 2, . . . , K), representing training data [12]. In such a case we can apply simultaneous

1220
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
(a)
…
(
)
I
I
1
2
×
(
)
I
J
1
1
×
(
)
J
J
1
2
×
(
)
J
I
2
2
×
(
)
I
I
1
2
×
(
)
I
J
1
1
×
(
)
J
J
1
2
×
(
)
J
I
2
2
×
(
)
I
I
1
2
×
(
)
I
J
1
1
×
(
)
J
J
1
2
×
(
)
J
I
2
2
×
Y1
Y2
YK
B1
B1
B1
B2
T
B2
T
B2
T
F1
F2
F3
(b)
(
)
I
I
1
2
×
×K
(
)
I
J
1
1
×
(
)
J
J
1
2
×
×K
(
)
J
I
2
2
×
J2
J1
K
Y
B1
B2
T
FIGURE 21.10
Common Component Analysis for a multi-block set of matrices. (a) Simultaneous matrix factorizations for
dimensionality reduction and feature extraction. (b) This model is equivalent to a Tucker-2 decomposition
of a third-order tensor as Y = F ×1 B1 ×2 B2, where F ∈RJ1×J2×K is a constrained core tensor (representing
features). In the special case matrices Fk are diagonal metrices.
approximative tensor decompositions (see Figure 21.11)
Y(k) = G(k) ×1 B(1) ×2 B(2) · · · ×N B(N) + E(k),
(21.245)
(k = 1, 2, . . . , K), where the compressed core tensors G(k) ∈RJ1×J2×···×JN representing features are
of lower dimensions than the original data tensors Y(k), and we assume that the factors (basis matrices)
B(n) = [b(n)
1 , b(n)
2 , . . . , b(n)
Jn ] ∈RIn×Jn, (n = 1, 2, . . . , N) are common factors for all data tensors.
To compute the factor matrices B(n) and the core tensors G(k), we concatenate all training (sample)
tensors into one N + 1 order training data tensor

1.21.5 Future Directions
1221
G(1)
B(1)
B(3)
Y
(1)
B(2)T
G(2)
B(1)
B
(3)
Y
(2)
B(2)T
G(
)
K
B(1)
B
(3)
Y
B(2)T
…
(
)
I
J
3
3
×
(
)
I1 × I
I
2
3
×
(
)
I1 × J1 (
)
J1 × J
J
2
3
×
(
)
J2 × I2
(
)
K
…
FIGURE 21.11
Multiway Common Component Analysis for a multi-block set of tensors for dimensionality reduction and
feature extraction from a set of third-order data tensors Y(k) ∈RI1×I2×I3, (k = 1, 2, . . . , K). The objective is
to estimate the common factor matrices B(n) ∈RIn×Jn(n = 1, 2, 3) and the core tensors G(k) ∈RJ1×J2...×JN ,
typically with Jn ≪In, representing features.
Y = cat(Y(1), Y(2), . . . , Y(K), N + 1) ∈RI1×I2···×IN ×IN+1, with N + 1 = K and perform the
Tucker-N decomposition [12]:
Y = G ×1 B(1) ×2 B(2) · · · ×N B(N) + E,
(21.246)
where the sample tensors Y(k) can be extracted back from the concatenated tensor by ﬁxing the (N +1)th
index at a value k, i.e., Y( :
1, :
2, . . . , :
N, k
N+1 ) = Y(k), and the individual features (corresponding to differ-
ent classes) are extracted from the core tensor G ∈RJ1×J2···×JN ×JN+1 as G(k) = G( :
1, :
2, . . . , :
N, k
N+1 ),
with JN+1 = K. In other words, the features of a speciﬁc training data Y(k) are represented by the kth
row of the mode-(N +1) matricized version of the core tensor G. We can apply the procedure described
above to various feature extraction classiﬁcation problems [12].

1222
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
1.21.5.7 Linked multilinear BSS/ICA for multi-block data
In many applications, we need to perform a so called group analysis or multi-block data analysis which
seeks to identify hidden components, for example, stimuli driven brain patterns that are common in
two or more subjects in a group. In many scenarios links or relationships need to be considered to
analyze variability and consistency of the components across multi-block data sets. Furthermore, some
components do not need to be necessarily independent, they can instead be sparse, smooth or non-
negative (e.g., for spectral components). Moreover, it is often necessary to impose some additional
constraints in order to estimate some components, which are identical or maximally correlated (across
multi-block data sets) with regard to their spatial distributions, spectral, or temporal patterns. In the
simplest scenario a multi-block dataset consists set of matrices Y1, Y2, . . . , YK with K ≥2. In general,
each matrix may have different numbers of rows or different number of columns. Our objective is to
simultaneously perform matrix decompositions (in the case when the number of columns of the data
matrices is the same):
Yk = AkBT
C + AkBT
k + Ek,
(21.247)
consisting of three terms. The ﬁrst terms AkBT
C represents joint structure via the common matrix
BC ∈RIk×R, the second term AkBk represents individual structure and the last term Ek represents
error.
Alternatively, for a set of data matrices Yk with common number of rows, but possibly different
number of columns, we can consider the dual model (see Figure 21.12):
Yk = AkBT
k + Ek = ACBT
C,k + AkBT
I,k + Ek,
(21.248)
where Ak = [AC, Ak].
In the special case, when Ak = AC ∈RI1×J1 for k = 1, 2, . . . , K the problem simpliﬁes to Common
Component Analysis or Population Value Decomposition (PVD) [176]. In more general case, in order
to estimate desired common factor matrices AC ∈RI1×R with R < Jk, we can formulate the following
constrained optimization problem, with orthogonality constraints [177]:
min
K

k=1
∥Yk −ACBT
C,k∥
2
F
(21.249)
s.t. AT
CAC = I, AT
k Ak = I, AT
CAk = 0, Yk = AkBT
k ∀k.
These 2-way models can be naturally extended to linked multilinear BSS and in the special case
of linked multilinear ICA (assuming that mutual statistical independence constraints are used in all
modes).
In the linked multilinear BSS the problem is formulated as approximative decompositions of a set
of data tensors Y(k) ∈RI1×I2···×IN , (k = 1, 2, . . . , K) (see Figure 21.13):
Y(k) = G(k) ×1 B(1,k) ×2 B(2,k) · · · ×N B(N,k) + E(k),
(21.250)
where each factor (component) matrix B(n,k) = [B(n)
C , B(n,k)
I
] ∈RIn×Jn is composed of two bases:
B(n)
C
∈RIn×Rn (with 0 ≤Rn ≤Jn), which are common bases for all available data or more generally

1.21.5 Future Directions
1223
(
)
I
T
1×
(
)
I
R
1×
(
)
R T
×
(
)
I
J
1
1
×
(
)
×T
J1
Y1
A1
BC
T
B1
T
A1
~
(
)
I
T
2×
(
)
I
R
2×
(
)
R T
×
(
)
I
J
2
2
×
(
)
×T
J2
Y2
A2
BC
T
B2
T
A2
~
…
…
(
)
I
T
K×
(
)
I
R
K×
(
)
R T
×
(
)
I
J
K
K
×
(
)
×T
JK
YK
AK
BC
T
BK
T
AK
~
(a)
…
(
)
I
T
1
1
×
(
)
I
J
1
1
×
(
)
J1×T1
(
)
I
T
1
2
×
(
)
I
J
1
2
×
(
)
J2×T2
(
)
I
T
1×
K
(
)
I
J
1×
K
(
)
JK×TK
Y
Y
Y
2
1
K
2
1
K
{
{
AC A1
~
{
{
AC A2
~
{
{
AC AK
~
B1
T
B2
T
BK
T
(b)
FIGURE 21.12
Simple models of linked multi-block matrix decomposition. (a) The objective is to ﬁnd the constrained matrix
BC ∈RR×T representing common components (e.g., independent, nonnegative, or sparse components) and
individual components represented by matrices Bk ∈RT ×Jk . (b) Dual model, in which the objective is to
ﬁnd the common matrix AC ∈RI1×R and a set of individual matrices Ak ∈RI1×(Jk −R). These models can
be considered as generalization of low-rank matrix approximation methods, including, CCA/PLS, ICA, SCA
and NMF.

1224
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
(1,
)
K
(2,
)
K
(
)
K
(
)
K
(1,
)
K
(3,
)
K
(3,
)
K
(2,
)
K
FIGURE 21.13
A generalized model of multi-block tensor decompositions for linked multiway component analysis (espe-
cially, Linked Multilinear ICA). The objective is to ﬁnd the core tensors G(k) ∈RJ1×J2×J3 and constrained
factor matrices B(n,k) = [B(n)
C , B(n,k)
I
] ∈RIn×Jn, (n = 1, 2, 3), which are partially linked or locally maximally
correlated, i.e., they have the same common (or highly correlated) components b(n,k)
jn
for some selected
indexes jn, n, k.
represent similar or maximally correlated components, and B(n,k)
I
∈RIn×(Jn−Rn), which are generally
different and mutaually independent, for example, they correspond to stimuli/tasks independent indi-
vidual characteristics. Based on such a decomposition model, our objective is to estimate the common
factors B(n)
C , the independent factors B(n,k)
I
and the interactions between them via the core tensors G(k)
for k = 1, 2, . . . , K and n = 1, 2, . . . , N.
If B(n,k) = B(n)
C
∈RIn×Jn for all modes n, then we can concatenate all data tensors along this
mode, perform tensor decomposition by applying any standard BSS algorithm for unfolding matrices to

1.21.5 Future Directions
1225
compute the desired components in each mode (e.g., to estimate independent components, we apply any
standard ICA algorithm) [12,14]. In the more general case, when the number of common components
Rn are unknown, we can perform the unfolding of each data tensor Y(k) in each mode, and then apply
suitable constrained matrix factorizations (e.g., by applying standard algorithms for ICA, NMF, SCA,
PCA/SVD) for the reduced unfolding matrices:
Y(k)
(n) ∼= B(n,k)G(k)
(n)Z(n,k) = Bn,kAT
n,k,
(21.251)
for n = 1, 2, . . . , N and k = 1, 2, . . . , K, where the matrices B(n,k) represent individual components
b(n,k)
j
(some of them being linked), while the matrices AT
n,k = G(k)
(n)Z(n,k) represent basis (mixing)
vectors or linked mixing processes, and Z(n,k) = [B(N,k) ⊗· · · ⊗B(n+1,k) ⊗B(n−1,k) · · · ⊗B(1,k)]T .
In order to identify the common components b(n)
j , in the next stage we need to perform the statistical
analysis and to rank and compare the individual components b(n,k)
j
extracted in each mode n, for all
blocks K, by performing clustering and ordering the components to identify common or similar (highly
correlated) components (see e.g., [8,14,178]). In the last step, we compute the core tensors which
describe the functional links between common components.
Alternative approach in the case when Rn < Jn, is to unfold each data tensor Y(k) in each mode, and
to perform a set of linked and constrained matrix factorizations: Y(k)
(n) ∼= B(n)
C A(n,k)T
C
+ B(n,k)
I
A(n,k)T
I
through solving constrained optimization problems:
min
K

k=1
∥Y(k)
(n) −B(n)
C A(n,k)T
C
+ B(n,k)
I
A(n,k)T
I
∥
2
F
+ fn(B(n)
C ),
s.t. B(n)T
C
B(n,k)
I
= 0 ∀k, n,
(21.252)
where fn denotes the penalty terms which impose additional constraints on common components B(n)
C ,
in order to extract unique and desired components. In the special case of orthogonality constraints,
the problem can be transformed to a generalized eigenvalue problem and solved by the power method
[177]. The key point is to assume that common factor sub-matrices B(n)
C
are present in all multiple
data blocks and hence reﬂect structurally complex (hidden) latent and intrinsic links between them. In
practice, the number of common components Cn in each mode is unknown and should be estimated
(see [177] for detail).
Note that our linked multilinear BSS is different from the linked ICA [38] and group NMF/ICA,
since we do not limit components diversity by constraining them to be only statistically independent
or only nonnegative components and our model is based on constrained Tucker models instead of the
rather quite restrictive trilinear CP model.
We conclude that the proposed linked multilinear BSS model provides a framework that is quite
ﬂexible and general and it may substantially supplement many of the currently available techniques for
group ICA and feature extraction for multi-block data. Moreover, the tensors decompositions models
can be extended to a multiway Canonical Correlation Analysis (MCCA), and higher order Partial Least
Squares, in which we impose maximal correlations among normalized factor matrices (or subsets of
components) and/or core tensors [77].

1226
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
G
(1)
B
(1,1)
B
(3,1)
B
(2,1)T
BC
(1) BI
(1,1)
BC
(3) BI
(3,1)
BC
(2)T
BI
(2,1)T
Y
(
)
I1 × I
I
2
3
×
(
)
I1 × 1 (
)
1 ×
2
3
×
(
)
2 × I2
J
J
J
J
J
(
)
I1 × 1
(
)
1 ×
2
3
×
(
)
2 × I2
J
J
J
J
J
(
)
I3
3
×J
G
B
B
B
(
)
I3
3
×
C
(1) BI
BC
(2)T
BI
J
BC
(3) BI
B
+
+
(1,
)
K
(1,
)
K
(2,
)T
K
(2,
)T
K
(
)
K
(3,
)
K
(3,
)
K
FIGURE 21.14
A Block Term Decomposition (BTD) model of tensors decomposition for linked multiway component analysis
(e.g., Linked Multilinear ICA).
For the linked multilinear BSS, we can also exploit the constrained Block Tensor Decomposition
(BTD) models [179–181] for an averaging data tensor across subjects (see Figure 21.14):
¯Y =
K

k=1
(G(k) ×1 B(1,k) ×2 B(2,k) · · · ×N B(N,k)) + ¯E,
(21.253)
where ¯Y = K
k=1 Y(k). Such model may provide us with additional information.
For group and linked multilinear BSS and ICA applied to the analysis of EEG/MEG data, we usually
seek stimuli driven ERPs (event related responses) and/or task related common components (or common
basis) reﬂecting both intra subject and inter subject features as basis, which are independent by involving
individual on going brain activities. In other words, we seek event/task-related components B(n)
C , that
are similar in each mode or maximally correlated across subjects, and event/task independent individual
bases B(s,n)
I
, which are independent or even as far apart as possible (anti-correlated).
How to select common components will depend on the validity of the underlying assumptions and
a priori knowledge. For instance, identical spatial distributions can well be assumed for homogeneous
subject groups, but may be unacceptable in studies that include patients with different ages, mental dis-
eases, or abnormalities. Temporal components may be the same for stimulus- or task-evoked responses
that are related to a speciﬁc mental task or paradigm, but these will vary for the spontaneous ﬂuctua-
tions that occur in resting state experiments. In some experiments, responses may be assumed similar
or identical within, but different across subgroups [37].
1.21.5.8 Multiway sparse CCA/PLS using the generalized power method
The concept of standard (matrix) CCA and PLS can be extended to multiway (higher-order) models
by using the normalized and standardized tensors X ∈RI1×I2···×IN and Y ∈RK1×K2···×KM , with
I1 = K1 = I, instead of the matrices X ∈RI×N and Y ∈RI×M (see Section 1.21.2.6.6). We assume
without any loss of generality that the tubes in mode-1 are standardized to have mean zero and standard
deviation one. In such a case we can apply (after additional constraints) uniﬁed approach to multiway
CCA and PLS.

1.21.5 Future Directions
1227
The key factor is to ﬁnd projection vectors w(n)
x
∈RIn and w(m)
y
∈RKm such that the canonical
vectors, deﬁned as
t1 = X ¯×2w(1)
x
¯×3w(2)
x
· · · ¯×Nw(N−1)
x
∈RI1,
(21.254)
u1 = Y ¯×2w(1)
y
¯×3w(2)
y · · · ¯×Mw(M−1)
y
∈RI1,
(21.255)
are maximally correlated, that is
max{uT
1 t1} =
(21.256)
max{(X ¯×2w(1)
x
¯×3w(2)
x
· · · ¯×Nw(N−1)
x
)T (Y ¯×2w(1)
y
¯×3w(2)
y · · · ¯×Mw(M−1)
y
)},
subject to suitable constraints, typically ∥w(n)
x ∥2 ≤1 and ∥w(m)
y
∥2 ≤1.
It can be shown that a such optimization problem is equivalent to the following maximization problem
max
w(p)
1
{Z ¯×1w(1)
1
¯×2w(2)
1
· · · ¯×Pw(P)
1
},
s.t. ∥w1∥2 ≤1,
(21.257)
where w(p)
1
= w(p)
x
for p = 1, 2, . . . , N −1 and w(p)
1
= w(p−N+1)
y
for p = N, N + 1, . . . , P, and the
generalized covariance of two tensors is deﬁned as
Z = X×1Y = COV(1,1)(X Y) ∈RI2×I3···×IN ×K2×K3···×KM ,
(21.258)
with entries
zi2,...iN ,k2,...,kM =
I

i=1
xi,i2,...iN yi,k2,...,kM .
(21.259)
Note that the tensor Z has order P = N + M −2. In the special case when the tensor Y is reduced to
the matrix Y, we have
Z = X ×1 Y
(21.260)
or both data are matrices and we can use the standards notation Z = XT Y.
If we need to impose sparsity constraints we can use the following cost function
max
w(p)
1
{(Z ¯×1w(1)
1
¯×2w(2)
1
· · · ¯×Pw(P)
1
)2 −

p
αp∥w(p)
1 ∥1},
s.t. ∥w(p)
1 ∥2 ≤1.
(21.261)
This optimization of the cost function (21.261) leads to the following simple update rule
w(p)
1
=
S1(w(p)
1 , αp)
∥S1(w(p)
1 , αp)∥2
,
(21.262)
where
w(p)
1
= Z ¯×1w(1)
1 · · · ¯×p−1w(p−1)
1
¯×p+1w(p+1)
1
· · · ¯×Pw(P)
1
.
(21.263)

1228
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
In the special case when αn = 0 the above updated formula simpliﬁes to (without sparsity constraints)
w(p)
1
=
w(p)
1
∥w(p)
1 ∥2
.
(21.264)
Our objective in a Multiway CCA is to estimate the set of factor matrices or weights W(p) =
[w(p)
1 , w(p)
2 , . . . , w(p)
J ] ∈RQ p×J for p = 1, 2, . . . , P, that maximize the following cost function (which
can be considered as a generalization of the variance used in the standard sparse PCA and CCA).
max
{w(p)
j
}
5
(Z( j) ¯×1w(1)
j
¯×2w(2)
j
· · · ¯×Pw(P)
j
)2 −

p
αp∥w(p)
j ∥1
6
,
(21.265)
s.t. ∥w(p)
j ∥2 ≤1 ( j = 1, 2, . . . , J),
where the tensor Z( j) is estimated iteratively using a simple deﬂation approach
λ j = |Z( j) ¯×1w(1)
j
¯×2w(2)
j
· · · ¯×Pw(P)
j
|,
(21.266)
Z( j+1) = Z( j) −λ j(w(1)
j
◦w(2)
j
· · · ◦w(P)
j
),
(21.267)
with Z(1) = Z.
The above set of optimization problems leads to the following update rule (see Algorithm 6) which
is very similar to the sparse sequential factorization for the CP model (see Section 1.21.5.3):
w(p)
j
=
S1(w(p)
j , αp)
∥S1(w(p)
j , αp)∥2
( j = 1, 2, . . . , J),
where
w(p)
j
= Z( j) ¯×1w(1)
j
· · · ¯×p−1w(p−1)
j
¯×p+1w(p+1)
j
· · · ¯×Pw(P)
j
and
S1(w, α) = sign(w)[|w| −α1]+.
(21.268)
In multiway PLS our objective is to preform simultaneous constrained Tucker-1 decompositions,
with common or maximally correlated factor matrices (see Figure 21.15):
X = GX ×1 T(1) + EX,
(21.269)
Y = GY ×1 U(1) + EY,
(21.270)
where the additional constraints imposed are that: T(1) = [t1, t2, . . . , tJ] and U(1) = [u1, u2, . . . , uJ]
should be maximally correlated, while the other factor matrices should be essentially different (e.g.,
orthogonal, or mutually independent).

1.21.5 Future Directions
1229
T
(1)
X
(1)
(
)
I1 × I
I
2
3
×
(
)
I1 × J
(
)
(                    )
J × I
I
2
3
×
I2
I1
I3
U
(1)
X
(1)
(
)
K1 × K
K
2
3
×
(
)
I1 × J
(J × K
K
2
3
×
K2
K1=I1
K3
G x
G Y
I3
I2
K3
K2
J
J
FIGURE 21.15
Illustration of a generalized Multiway PLS model for third-order tensors (see Eqs. 21.269,21.270). The
objective is to perform approximative decompositions of the independent tensor X ∼= GX ×1 T(1) ∈RI1×I2×I3
and dependent tensor Y ∼= GY ×1 U(1) ∈RK1×K2×K3, with I1 = K1 = I , by imposing additional constraints,
speciﬁcally that the two factor matrices T(1) = [t1, t2, . . . , tJ] and U(1) = [u1, u2, . . . , uJ] are maximally
correlated in the sense that the covariances between all components are maximized, i.e., max{uT
j tj}, ∀j.
Algorithm 6. Power Method for Sparse Multiway CCA
Require: Data tensors X(1) = X, Y(1) = Y, sparsity coefﬁcients αp ≥0, and initial vectors
w(p)
j
for (p = 1, 2, . . . , P).
1: Compute Z(1) = X×1Y = COV(1,1)(X Y) ∈RI2×I3···×IN ×K2×K3···×KM ;
2: For p = 1, 2, . . . , P and j = 1, 2, . . . , J
(a) Repeat until convergence of w(p)
j
;
3:
w(p)
j
= Z( j) ¯×1w(1)
j
· · · ¯×p−1w(p−1)
j
¯×p+1w(p+1)
j
· · · ¯×Pw(P)
j
;
4:
w(p)
j
=
S1(w(p)
j
,αp)
∥S1(w(p)
j
,αp)∥2
(for αp = 0 take w(p)
j
=
w(p)
j
∥w(p)
j
∥2
);
5: (b) Perform optional Gram-Schmidt orthogonalization;
6: (c)
λ j = |Z( j) ¯×1w(1)
j
¯×2w(2)
j
· · · ¯×Pw(P)
j
|;
7: (d)
Z( j+1) = Z( j) −λ j(w(1)
j
◦w(2)
j
· · · ◦w(P)
j
);
8: return W(p) = [w(p)
1
, w(p)
2
, . . . , w(p)
J ] ∈RQ p×J ,
∗= diag{λ1, . . . , λJ };
9: return t j = X ¯×2w(1)
j
¯×3w(2)
j
· · · ¯×N w(N−1)
j
,
u j = Y ¯×2w(N)
j
¯×3w(N+1)
j
· · · ¯×K w(P)
j
; ( j = 1, 2, . . . , J).

1230
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
Such models allow for different types of structures on X and Y and provides a general framework for
solving multiway regression problems that explore complex relationships between multi-dimensional
dependent and independent variables. For example, tensor decompositions can be applied to emerging
neuroimaging genetics studies to investigate the links between biological parameters measured with
brain imaging and genetic variability.
1.21.6 Summary
In this chapter, we have discussed several important latent variables models, especially PCA, ICA,
NMF, CCA and PLS and their extensions. We have tried to unify different approaches and methods
by applying constrained matrix factorizations or penalized matrix decompositions. Imposing various
constraints on the factors, components, or latent variables allows us to extract the components with
desired properties. This approach has been extended to the important case of multi-dimensional data
by using constrained tensor decompositions. Moreover, the concept of blind source separation has been
extended to multilinear or multiway BSS using tensor factorizations.
Relevant Theory: Machine Learning
See this Volume, Chapter 22 Semi-Supervised Learning
References
[1] R. Bro, PARAFAC, Tutorial and applications, in: Second International Conference in Chemometrics
(INCINC’96), vol. 38, Chemometr. Intell. Lab. Syst., 1997, pp. 149–171 (special issue).
[2] J. Carroll, Jih-Jie Chang, Analysis of individual differences in multidimensional scaling via an n-way gen-
eralization of ‘Eckart-Young’ decomposition, Psychometrika 35 (3) (1970) 283–319.
[3] R.A. Harshman, Foundations of the PARAFAC procedure: models and conditions for an ‘explanatory’
multimodal factor analysis, UCLA Working Papers in Phonetics 16 (1), 1970.
[4] F.L. Hitchcock, Multiple invariants and generalized rank of a p-way matrix or tensor, J. Math. Phys. 7 (1927)
39–79.
[5] T.G. Kolda, B.W. Bader, Tensor decompositions and applications, SIAM Rev. 51 (3) (2009) 455–500.
[6] L. Tucker, Some mathematical notes on three-mode factor analysis, Psychometrika 31 (3) (1966) 279–311.
[7] L.R. Tucker, The extension of factor analysis to three-dimensional matrices, in: H. Gulliksen, N. Fred-
eriksen (Eds.), Contributions to Mathematical Psychology, Holt, Rinehart and Winston, New York, 1964,
pp. 110–127.
[8] A. Cichocki, R Zdunek, A.-H. Phan, S. Amari, Nonnegative Matrix and Tensor Factorizations: Applications
to Exploratory Multi-way Data Analysis and Blind Source Separation, Wiley, Chichester, 2009.
[9] L. De Lathauwer, B. De Moor, J. Vandewalle, A multilinear singular value decomposition, SIAM J. Matrix
Anal. Appl. 24 (2000) 1253–1278.
[10] L. De Lathauwer, B. De Moor, J. Vandewalle, On the best rank-1 and rank-(R1, R2, …, RN) approximation
of higher-order tensors, SIAM J. Matrix Anal. Appl. 21 (4) (2000) 1324–1342.
[11] F. Miwakeichi, E. Martnez-Montes, P. Valds-Sosa, N. Nishiyama, H. Mizuhara, Y. Yamaguchi, Decomposing
EEG data into space-time-frequency components using parallel factor analysis, NeuroImage 22 (3) (2004)
1035–1045.

References
1231
[12] A.H. Phan, A. Cichocki, Tensor decompositions for feature extraction and classiﬁcation of high dimensional
datasets, Nonlinear Theory Appl. IEICE 1 (1) (2010) 37–68.
[13] P. Comon, C. Jutten (Eds.), Independent Component Analysis and Applications, Handbook of Blind Source
Separation, Academic Press, 2010.
[14] A. Cichocki, S. Amari, Adaptive Blind Signal and Image Processing, John Wiley & Sons Ltd., New York,
2003.
[15] A. Cichocki, S.C. Douglas, S. Amari, Robust techniques for independent component analysis (ICA) with
noisy data, Neurocomputing 23 (1–3) (1998) 113–129.
[16] A. Cichocki, P. Georgiev, Blind source separation algorithms with matrix constraints, IEICE Trans. Fundam.
Electron. Commun. Comput. Sci. E86-A (1) (2003) 522–531. <http://www.bsp.brain.riken.jp/publications/
2003/CichockiGeIEICE.pdf>.
[17] A. Hyvärinen, J. Karhunen, E. Oja, Independent Component Analysis, John Wiley & Sons Ltd., New York,
2001.
[18] Xi-Lin Li, T. Adali, Independent component analysis by entropy bound minimization, IEEE Trans. Signal
Process. 58 (10) (2010) 5151–5164.
[19] A. Cichocki, R. Zdunek, S. Amari, Csiszar’s divergences for non-negative matrix factorization: family of
new algorithms, LNCS, vol. 3889, Springer, 2006, pp. 32–39.
[20] D.D. Lee, H.S. Seung, Learning the parts of objects by non-negative matrix factorization, Nature 401 (6755)
(1999) 788–791.
[21] Z. He, A. Cichocki, An efﬁcient K-hyperplane clustering algorithm and its application to sparse component
analysis, Lect. Notes Comput. Sci. 4492 (2007) 1032–1041.
[22] Z. He, S. Xie, L. Zhang, A. Cichocki, A note on Lewicki-Sejnowski gradient for learning overcomplete
representations, Neural Comput. 20 (3) (2008) 636–643.
[23] Y. Li, S. Amari, A. Cichocki, D. Ho, S. Xie, Underdetermined blind source separation based on sparse
representation, IEEE Trans. Signal Process. 54 (2006) 423–437.
[24] Y. Li, A. Cichocki, S. Amari, Blind estimation of channel parameters and source components for EEG
signals: a sparse factorization approach, IEEE Trans. Neural Networks 17 (2006) 419–431.
[25] Y. Washizawa, A. Cichocki, On line K-plane clustering learning algorithm for sparse component analysis,
in: Proceedings of the 2006 IEEE International Conference on Acoustics, Speech and Signal Processing,
ICASSP2006, Toulouse, France, May 14–19, 2006, pp. 681–684.
[26] J. Bobin, Y. Moudden, J. Fadili, J. Starck, Morphological diversity and sparsity for multichannel data restora-
tion, J. Math. Imag. Vis. 33 (2) (2009) 149–168.
[27] Y. Moudden, J. Bobin, Hyperspectral BSS using GMCA with spatio-spectral sparsity constraints, IEEE
Trans. Image Process. 20 (3) (2011) 872–879.
[28] S. Amari, A. Cichocki, Adaptive blind signal processing – neural network approaches, Proc. IEEE 86 (1998)
1186–1187.
[29] Z. He, A. Cichocki, K-EVD clustering and its applications to sparse component analysis, Lect. Notes Comput.
Sci. 3889 (2006) 438–445.
[30] A. Cichocki, R. Unbehauen, Neural Networks for Optimization and Signal Processing, John Wiley & Sons
Ltd., New York, 1994.
[31] K. Kreutz-Delgado, J.F. Murray, B.D. Rao, K. Engan, T.-W. Lee, T.J. Sejnowski, Dictionary learning algo-
rithms for sparse representation, Neural Comput. 15 (2) (2003) 349–396.
[32] G. Zhou, A. Cichocki, Fast and unique Tucker decompositions via multiway blind source separation, Bull.
Pol. Acad. Sci. 60 (3) (2012) 389–407.
[33] A. Cichocki, Generalized component analysis and blind source separation methods for analyzing mulitchan-
nel brain signals, in: Statistical and Process Models for Gognitive Neuroscience and Aging, Lawrence
Erlbaum Associates, 2007, pp. 201–272.

1232
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
[34] A. Cichocki, H.A. Phan, Fast local algorithms for large scale nonnegative matrix and tensor factorizations,
IEICE Trans. Fundam. Electron. Commun. Comput. Sci. E92-A (3) (2009) 708–721.
[35] V.D. Calhoun, J. Liu, T. Adali, A review of group ICA for fMRI data and ICA for joint inference of imaging,
genetic, and ERP data, Neuroimage 45 (2009) 163–172.
[36] Y. Guo, G. Pagnoni, A uniﬁed framework for group independent component analysis for multi-subject fMRI
data, NeuroImage 42 (3) (2008) 1078–1093.
[37] D.M.G. Langers, Unbiased group-level statistical assessment of independent component maps by means of
automated retrospective matching, Human Brain Map. 31 (2010) 727–742.
[38] A.R. Groves, C.F. Beckmann, S.M. Smith, M.W. Woolrich, Linked independent component analysis for
multimodal data fusion, NeuroImage 54 (1) (2011) 2198–21217.
[39] I.T. Jolliffe, Principal Component Analysis, Springer Verlag, New York, 1986.
[40] M. Moonen, B. de Moor (Eds.), SVD and Signal Processing. III. Algorithms, Applications and Architecture,
Elsevier Science Publishing Co. Inc., 1994.
[41] R. Rosipal, M. Girolami, L.J. Trejo, A. Cichocki, Kernel PCA for feature extraction and de-noising in nonlin-
earregression,NeuralComput.Appl.10(2001)231–243. <http://www.bsp.brain.riken.jp/publications/2001/
nca.pdf>.
[42] Z. He, A. Cichocki, S. Xie, K. Choi, Detecting the number of clusters in N–way probabilistic clustering,
IEEE Trans. Pattern Anal. Mach. Intell. 32 (11) (2010) 2006–2021.
[43] T.P. Minka, Automatic choice of dimensionality for PCA, in: T.K. Leen, T.G. Dietterich, V. Tresp (Eds.),
Advances in Neural Information Processing Systems, vol. 13, MIT Press, 2001, pp. 556–562.
[44] M.O. Ulfarsson, V. Solo, Dimension estimation in noisy PCA with SURE and random matrix theory, IEEE
Trans. Signal Process. 56 (12) (2008) 5804–5816.
[45] J. Karhunen, A. Cichocki, W. Kasprzak, P. Pajunen, On neural blind separation with noise suppression and
redundancy reduction, Int. J. Neural Syst. 8 (2) (1997) 219–237.
[46] M. Wax, T. Kailath, Detection of signals by information theoretic criteria, IEEE Trans. Signal Process. 33 (2)
(1985) 387–392.
[47] J. Niesing, Simultaneous Component and Factor Analysis Methods for Two or More Groups: A Comparative
Study, second ed., DSWO Press, Leiden University, Leiden, The Netherlands, 1997.
[48] P. Comon, G.H. Golub, Tracking of a few extreme singular values and vectors in signal processing, in: Proc.
IEEE 78 (8) (1990) 1327–1343 (published from Stanford Report 78NA-89-01, February 1989).
[49] S. Lipovetsky, PCA and SVD with nonnegative loadings, Pattern Recogn. 99 (9) (2009) (in print).
[50] B. Yang, Projection approximation subspace tracking, IEEE Trans. Signal Process. 43 (1) (1995) 95–107.
[51] V. Zipunnikov, B.C. Caffo, D.M. Yousem, C. Davatzikos, B.S. Schwartz, C.M. Crainiceanu, Functional
principal component model for high-dimensional brain imaging, NeuroImage 58 (3) (2011) 772–784.
[52] A. Cichocki, R. Unbehauen, Robust estimation of principal components in real time, Electron. Lett. 29 (21)
(1993) 1869–1870.
[53] A. Cichocki, W. Kasprzak, W. Skarbek, Adaptive learning algorithm for principal component analysis with
partial data, in: R. Trappl (Ed.), Cybernetics and Systems ’96, 13th European Meeting on Cybernetics and
Systems Research, vol. 2, Austrian Society for Cybernetic Studies, Vienna, 1996, pp. 1014–1019.
[54] M. Journée, Y. Nesterov, P. Richtarik, R. Sepulchre, Generalized power method for sparse principal compo-
nent analysis, J. Mach. Learn. Res. 1 (2010) 517–553.
[55] S.T. Roweis, EM algorithms for PCA and SPCA, in: Advances in Neural Information processing Systems
NIPS-98, vol. 10, 1998, pp. 452–456.
[56] M.E. Tipping, C.M. Bishop, Probabilistic principal component analysis, J. Roy. Stat. Soc. Ser. B 3 (1999)
600–616.
[57] A. d’Aspremont, F. Bach, L. El Ghaoui, Optimal solutions for sparse principal component analysis, J. Mach.
Learn. Res. 9 (2008) 1269–1294.

References
1233
[58] R. Jenatton, G. Obozinski, F. Bach, Structured sparse principal component analysis, J. Mach. Learn. Res.
Proc. Track 9 (2010) 366–373.
[59] I.T. Jolliffe, N.T. Trendaﬁlov, M. Uddin, A modiﬁed principal component technique based on the LASSO,
J. Comput. Graph. Stat. 12 (3) (2003) 1269–1294.
[60] R. Luss, M. Teboulle, Conditional gradient algorithms for rank-one matrix approximations with a sparsity
constraint, submitted for publication. Report: <http://archiv.org/pdf/1107.1163.pdf>.
[61] D.M. Witten, R. Tibshirani, T. Hastie, A penalized matrix decomposition, with applications to sparse principal
components and canonical correlation analysis, Biostatistics 10 (3) (2009) 515–534.
[62] H. Zou, T. Hastie, R. Tibshirani, Sparse principal component analysis, J. Comput. Graph. Stat. 15 (2) (2006)
265–286.
[63] G.I. Allen, M. Maletic-Savatic, Sparse non-negative generalized PCA with applications to metabolomics,
Bioinformatics 27 (21) (2011) 3029–3035.
[64] D.M. Witten. A penalized matrix decomposition, and its applications, PhD Dissertation, Department of
Statistics, Stanford University, 2010.
[65] H. Shen, J.Z. Huang, Sparse principal component analysis via regularized low rank matrix approximation,
J. Multivariable Anal. 99 (6) (2008) 1015–1034. <http://dx.doi.org/10.1016/j.jmva.2007.06.007>.
[66] J.Z. Huang, H. Shen, A. Buja, The analysis of two-way functional data using two-way regularized singular
value decompositions, J. Am. Stat. Assoc. 104 (488) (2009) 1609–1620. <http://EconPapers.repec.org/
RePEc:bes:jnlasa:v:104:i:488:y:2009:p:1609-1620>.
[67] M. Lee, H. Shen, J.Z. Huang, J.S. Marron, Biclustering via sparse singular value decomposition, Biometrics
66 (4) (2010) 1087–1095. <http://www.ncbi.nlm.nih.gov/pubmed/20163403>.
[68] G.I. Allen, Sparse higher-order principal components analysis, J. Mach. Learn. Res. – Proc. Track 22 (2012)
27–36.
[69] C. Croux, P Filzmoser, H. Fritz, Robust Sparse Principal Component Analysis, Open Access Publications
from Katholieke Universiteit Leuven, Katholieke Universiteit Leuven, 2011. <http://EconPapers.repec.org/
RePEc:ner:leuven:urn:hdl:123456789/310504>.
[70] L. Mackey, Deﬂation methods for sparse PCA, in: D. Koller, D. Schuurmans, Y. Bengio, L. Bottou (Eds.),
Advances in Neural Information Processing Systems (NIPS), MIT Press, 2009, pp. 1017–1024.
[71] D. Widjaja, C. Varon, A. Dorado, J.A.K. Suykens, S. Van Huffel, Application of kernel principal component
analysis for single-lead-ecg-derived respiration, IEEE Trans. Biomed. Eng. 59 (4) (2012) 1169–1176.
[72] H. Hotelling, Relations between two sets of variates, Biometrika 28 (1936) 321–377.
[73] D.R. Hardoon, J. Shawe-Taylor, Sparse canonical correlation analysis, Mach. Learn. 83 (3) (2011) 331–353.
[74] D.R. Hardoon, S. Szedmák, J. Shawe-Taylor, Canonical correlation analysis: an overview with application
to learning methods, Neural Comput. 16 (12) (2004) 2639–2664.
[75] R. Rosipal, N. Krämer, Overview and recent advances in partial least squares, in: Lecture Notes in Computer
Science, 2005, pp. 34–51.
[76] A. Krishnan, L.J. Williams, A.R. McIntosh, H. Abdi, Partial least squares (PLS) methods for neuroimaging:
a tutorial and review, NeuroImage 56 (2) (2011) 455–475.
[77] Q. Zhao, C. Caiafa, D. Mandic, Z. Chao, Y. Nagasaka, N. Fujii, L. Zhang, A. Cichocki, Higher-order partial
least squares (HOPLS): a generalized multi-linear regression method, in: PAMI, 2012. <http://archiv.org/pdf/
1207.1230.pdf>.
[78] H. Abdi, Partial least square regression, projection on latent structure regression, PLS-regression, Wiley
Interdiscipl. Rev.: Comput. Stat. 2 (2010) 97–106.
[79] B. McWilliams, G. Montana, Multi-view predictive partioning in high dimensions, Statist. Anal. Data Mining
5 (3) (2012) 304–321. Available from: <arXiv:1202.0825v1>.
[80] Xi-Lin Li, T. Adali, Blind separation of noncircular correlated sources using gaussian entropy rate, IEEE
Trans. Signal Process. 59 (6) (2011) 2969–2975.

1234
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
[81] L. Tong, V.C. Soon, Y.F. Huang, R. Liu, AMUSE: a new blind identiﬁcation algorithm, in: Proceedings for
the IEEE ISCAS, vol. 3, IEEE, 1990, pp. 1784–1787.
[82] A. Belouchrani, K. Abed-Meraim, J.-F. Cardoso, É. Moulines, A blind source separation technique using
second-order statistics, IEEE Trans. Signal Process. 45 (2) (1997) 434–444.
[83] A. Ziehe, K.-R. Müller, G. Nolte, B.-M. Mackert, G. Curio, Artifact reduction in biomagnetic recordings
based on time-delayed second order correlations, IEEE Trans. Biomed. Eng. 47 (2000) 75–87.
[84] L. De Lathauwer, A link between the canonical decomposition in multilinear algebra and simultaneous
matrix diagonalization, SIAM J. Matrix Anal. Appl. 28 (2006) 642–666.
[85] A. Ziehe, M. Kawanabe, S. Harmeling, K.-R. Müller, A fast algorithm for joint diagonalization with non-
orthogonal transformations and its application to blind source separation, J. Mach. Learn. Res. 5 (2004)
801–818.
[86] A. Belouchrani, A. Cichocki, Robust whitening procedure in blind source separation context, Electron. Lett.
36 (24) (2000) 2050–2053.
[87] S.A. Cruces, A. Cichocki, Combining blind source extraction with joint approximate diagonalization: thin
algorithmsforICA,in:Proceedingsof4thInternationalSymposiumonIndependentComponentAnalysisand
Blind Signal Separation (ICA2003), Kyoto, Japan, Riken, ICA, April 2003, pp. 463–468. <http://www.bsp.
brain.riken.jp/publications/2003/ICA03CrucesCich.pdf>.
[88] P. Tichavsky, Z. Koldovsky, Fast and accurate methods of independent component analysis: a survey,
Kybernetika 47 (3) (2011) 426–438.
[89] P. Tichavsky, A. Yeredor, Fast approximate joint diagonalization incorporating weight matrices, IEEE Trans.
Signal Process. 47 (3) (2009) 878–891.
[90] Z. Koldovsky, P. Tichavsky, E. Oja, Efﬁcient variant of algorithm fastica for independent component analysis
attaining the Cramér-Rao lower bound, IEEE Trans. Neural Networks 17 (5) (2006) 1265–1277.
[91] A.J. Bell, T.J. Sejnowski, An information maximization approach to blind separation and blind deconvolution,
Neural Comput. 7 (6) (1995) 1129–1159.
[92] S. Choi, A. Cichocki, L.L. Zhang, S. Amari, Approximate maximum likelihood source separation using
the natural gradient, IEICE Trans. Fundam. Electron. Commun. Comput. Sci, E86-A (1) (2003) 198–205.
<http://www.bsp.brain.riken.jp/publications/2003/ieice02.pdf>.
[93] A. Cichocki, R. Unbehauen, Robust neural networks with on-line learning for blind identiﬁcation and blind
separation of sources, IEEE Trans. Circ. Syst. I: Fundam. Theory Appl. 43 (11) (1996) 894–906.
[94] A. Cichocki, R. Unbehauen, L. Moszczy´nski, E. Rummert, A new on-line adaptive learning algorithm
for blind separation of sources, in: Proceedings of the 1994 International Symposium on Artiﬁcial Neural
Networks, ISANN-94, Tainan, Taiwan, December 1994, pp. 406–411.
[95] A. Cichocki, R. Unbehauen, E. Rummert, Robust learning algorithm for blind separation of signals, Electron.
Lett. 30 (17) (1994) 1386–1387.
[96] S. Fiori, A fully multiplicative orthoghonal-group ICA neural algorithm, Electron. Lett. 39 (24) (2003)
1737–1738.
[97] S.A. Cruces, A. Cichocki, S. Amari, On a new blind signal extraction algorithm: different criteria and stability
analysis, IEEE Signal Process. Lett. 9 (8) (2002) 233–236.
[98] S. Amari, A. Cichocki, H.H. Yang, A new learning algorithm for blind signal separation, in: M.C. Mozer,
D.S. Touretzky, M.E. Hasselmo (Eds.), Advances in Neural Information Processing Systems, December
1995, vol. 8, MIT Press, Cambridge MA, 1996, pp. 757–763.
[99] S.A. Cruces, L. Castedo, A. Cichocki, Robust blind source separation algorithms using cumulants, Neuro-
computing 49 (2002) 87–118.
[100] J.-F. Cardoso, B. Laheld, Equivariant adaptive source separation, IEEE Trans. Signal Process. 44 (1996)
3017–3030.

References
1235
[101] S. Choi, A. Cichocki, S. Amari, Equivariant nonstationary source separation, Neural Networks 15 (2002)
121–130.
[102] S.A. Cruces, A. Cichocki, L. Castedo, An iterative inversion approach to blind source separation, IEEE
Trans. Neural Networks 11 (6) (2000) 1423–1437.
[103] A. Cichocki, R. Thawonmas, On-line algorithm for blind signal extraction of arbitrarily distributed, but
temporally correlated sources using second order statistics, Neural Process. Lett. 12 (1) (2000) 91–98.
[104] J.V. Stone, Blind source separation using temporal predictability, Neural Comput. 13 (7) (2001) 1559–1574.
[105] A. Cichocki, R. Thawonmas, S. Amari, Sequential blind signal extraction in order speciﬁed by stochastic
properties, Electron. Lett. 33 (1) (1997) 64–65.
[106] A.K. Barros, A. Cichocki, Extraction of speciﬁc signals with temporal structure, Neural Comput. 13 (9)
(2001) 1995–2000.
[107] H.-Y. Jung, S.-Y. Lee, On the temporal decorrelation of feature parameters for noise-robust speech recogni-
tion, IEEE Trans. Speech Audio Process. 8 (7) (2000) 407–416.
[108] P. Paatero, U. Tapper, Positive matrix factorization: a nonnegative factor model with optimal utilization of
error estimates of data values, Environmetrics 5 (1994) 111–126.
[109] D.D. Lee, H.S. Seung, Algorithms for Nonnegative Matrix Factorization, vol. 13, MIT Press, 2001.
[110] A.Shashua,R.Zass,T.Hazan,Multi-wayclusteringusingsuper-symmetricnon-negativetensorfactorization,
in: European Conference on Computer Vision (ECCV), Graz, Austria, May 2006. <http://www.cs.huji.ac.il/
zass/>.
[111] R. Zass, A. Shashua, Nonnegative sparse PCA, in: Neural Information Processing Systems (NIPS), Vancuver,
Canada, December 2006. <http://www.cs.huji.ac.il/zass/>.
[112] M. Berry, M. Browne, A. Langville, P. Pauca, R. Plemmons, Algorithms and applications for approximate
nonnegative matrix factorization, Comput. Stat. Data Anal. 52 (1) (2007) 155–173. <http://www.wfu.edu/
∼plemmons/papers.htm>.
[113] B. Dong, M.M. Lin, M.T. Chu, Nonnegative rank factorization via rank reduction. Report: <http://www4.
ncsu.edu/mtchu/Research/Papers/Readme.html>.
[114] C. Ding, X. He, H.D. Simon, On the equivalence of nonnegative matrix factorization and spectral clustering,
in: Proceedings of the SIAM International Conference on Data Mining (SDM’05), 2005, pp. 606–610.
<http://crd.lbl.gov/cding/papers/nmfSIAM1.pdf>.
[115] N. Gillis, F. Glineur, Using underapproximations for sparse nonnegative matrix factorization, Pattern Recogn.
43 (4) (2010) 1676–1687.
[116] N. Gillis, F. Glineur, Accelerated multiplicative updates and hierarchical als algorithms for nonnegative
matrix factorization, Neural Comput. 24 (4) (2012) 1085–1105.
[117] C. Ding, T. Li, W. Peng, M.I. Jordan, Convex and semi-nonnegative matrix factorizations, IEEE Trans. PAMI
32 (2010) 44–51. <http://www.cs.berkeley.edu/jordan/papers/ding-li-jordan.pdf>.
[118] C. Ding, T. Li, W. Peng, H. Park, Orthogonal nonnegative matrix tri-factorizations for clustering, in: KDD06:
Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
New York, NY, USA, ACM Press, 2006, pp. 126–135, ISBN: 1-59593-339-5.
[119] A. Pascual-Montano, J.M. Carazo, K. Kochi, D. Lehmean, R. Pacual-Marqui, Nonsmooth nonnegative matrix
factorization (nsNMF), IEEE Trans. Pattern Anal. Mach. Intell. 28 (3) (2006) 403–415.
[120] C. Caiafa, A. Cichocki, Generalizing the column-row matrix decomposition to multi-way arrays, Linear
Algebra Appl. 433 (3) (2010) 557–573.
[121] P. Drineas, M.W. Mahoney, S. Muthukrishnan, Relative-error CUR matrix decompositions, SIAM J. Matrix
Anal. Appl. 30 (2008) 844–881.
[122] S.A. Goreinov, E.E. Tyrtyshnikov, N.L. Zamarashkin, A theory of pseudo-skeleton approximations, Linear
Alegebra Appl. 261 (1997) 1–21.

1236
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
[123] M.W. Mahoney, P. Drineas, CUR matrix decompositions for improved data analysis, in: Proc. Natl. Acad.
Sci. 106 (2009) 697–702.
[124] M.W. Mahoney, M. Maggioni, P. Drineas, Tensor-CUR decompositions and data applications, SIAM J.
Matrix Anal. Appl. 30 (2008) 957–987.
[125] J. Sun, Incremental pattern discovery on streams, graphs and tensors, PhD Thesis, CMU-CS-07-149, 2007.
[126] A. Cichocki, R. Zdunek, S.-I. Amari, Hierarchical ALS algorithms for nonnegative matrix and 3D tensor
factorization, Lecture Notes on Computer Science, LNCS, vol. 4666, Springer, 2007, pp. 169–176.
[127] A. Cichocki, R. Zdunek, Multilayer nonnegative matrix factorization, Electron. Lett. 42 (16) (2006) 947–948.
[128] L. Badea. Extracting gene expression proﬁles common to Colon and Pancreatic Adenocarcinoma using
simultaneous nonnegative matrix factorization, in: Proceedings of Paciﬁc Symposium on Biocomputing
PSB-2008, World Scientiﬁc, 2008, pp. 267–278.
[129] Z. Yang, E. Oja, Uniﬁed development of multiplicative algorithms for linear and quadratic nonnegative
matrix factorization, IEEE Trans. Neural Networks 22 (12) (2011) 1878–1891.
[130] A. Cichocki, S. Cruces, S. Amari, Generalized alpha-beta divergences and their application to rubust non-
negative matrix factorization, Entropy 13 (2011) 134–170.
[131] M.E. Daube-Witherspoon, G. Muehllehner, An iterative image space reconstruction algorthm suitable for
volume ECT, IEEE Trans. Med. Imag. 5 (1986) 61–66.
[132] C.L. Byrne, Accelerating the EMML algorithm and related iterative algorithms by rescaled block-iterative
(RBI) methods, IEEE Trans. Image Process. IP-7 (1998) 100–109.
[133] A.R. De Pierro, On the relation between the ISRA and the EM algorithm for positron emission tomography,
IEEE Trans. Med. Imag. 12 (2) (1993) 328–333.
[134] A.R. De Pierro, M.E. Beleza Yamagishi, Fast iterative methods applied to tomography models with general
Gibbs priors, in: Proceedings of the SPIE Conference Mathematical Modeling, Bayesian Estimation and,
Inverse Problems, vol. 3816, 1999, pp. 134–138.
[135] H. Lantéri, M. Roche, C. Aime, Penalized maximum likelihood image restoration with positivity constraints:
multiplicative algorithms, Inverse Problems 18 (2002) 1397–1419.
[136] R.M. Lewitt, G. Muehllehner, Accelerated iterative reconstruction for positron emission tomography based
on the EM algorithm for maximum-likelihood estimation, IEEE Trans. Med. Imag. MI-5 (1986) 16–22.
[137] A. Cichocki, A.H. Phan, R. Zdunek, L.-Q. Zhang, Flexible component analysis for sparse, smooth, nonneg-
ative coding or representation, in: Lecture Notes in Computer Science, LNCS, vol. 4984, Springer, 2008,
pp. 811–820.
[138] R. Bro, Multi-way analysis in the food industry – models, algorithms, and applications, PhD Thesis,
University of Amsterdam, Holland, 1998.
[139] N.-D. Ho, Nonnegative matrix factorization – algorithms and applications, Thesis/Dissertation, Universite
Catholique de Louvain, Belgium, FSA/INMA, Departement d’ingenierie mathematique, 2008.
[140] N.D. Ho, P. Van Dooren, Non-negative matrix factorization with ﬁxed row and column sums, Linear Algebra
Appl. 429 (5–6) (2008) 1020–1025, ISSN: 0024–3795.
[141] A.H. Phan, A. Cichocki, Multi-way nonnegative tensor factorization using fast hierarchical alternating least
squares algorithm (HALS), in: Proceedings of the International Symposium on Nonlinear Theory and its
Applications, Budapest, Hungary, 2008.
[142] M. Hasan, F. Pellacini, K. Bala, Matrix row-column sampling for the many-light problem, in: SIGGRAPH,
2007. <http://www.cs.cornell.edu/ mhasan/>.
[143] M. Hasan, E. Velazquez-Armendariz, F. Pellacini, K. Bala, Tensor clustering for rendering many-light ani-
mations, in: Eurographics Symposium on Rendering, vol. 27, 2008. <http://www.cs.cornell.edu/ mhasan/>.
[144] G. Zhou, A. Cichocki, S. Xie, Fast nonnegative matrix/tensor factorization based on low-rank approximation,
IEEE Trans. Signal Process. 60 (6) (2012) 2928–2940.

References
1237
[145] S. Amari, Integration of stochastic models by minimizing alpha-divergence, Neural Comput. 19 (2007)
2780–2796.
[146] M. Minami, S. Eguchi, Robust blind source separation by Beta-divergence, Neural Comput. 14 (2002)
1859–1886.
[147] T.P. Minka, Divergence measures and message passing, Microsoft Research Technical Report (MSR-TR-
2005), 2005.
[148] C. Févotte, N. Bertin, J.-L. Durrieu, Nonnegative matrix factorization with the Itakura-Saito divergence, with
application to music analysis, Neural Comput. 21 (3) (2009) 793–830.
[149] R. Kompass, A generalized divergence measure for nonnegative matrix factorization, Neural Comput. 19 (3)
(2006) 780–791.
[150] J. Kivinen, M.K. Warmuth, Exponentiated gradient versus gradient descent for linear predictors, Inform.
Comput. 132 (1997).
[151] P. Smaragdis, Non-negative matrix factor deconvolution; extraction of multiple sound sources from mono-
phonic inputs, Lect. Notes Comput. Sci. 3195 (2004) 494–499.
[152] M.N. Schmidt, M. Mørup, Nonnegative matrix factor 2-D deconvolution for blind single channel source
separation, LNCS, vol. 3889, Springer, 2006, pp. 700–707.
[153] A.H. Phan, A. Cichocki, P. Tichavský, Z. Koldovský, On connection between the convolutive and ordinary
nonnegative matrix factorizations, in: Latent Variable Analysis and Signal Separation, LNCS, Springer, 2012,
pp. 288–296.
[154] J. Eggert, E. Körner, Sparse coding and NMF, in: Proceedings of IEEE International Joint Conference on
Neural Networks, vol. 4, 2004, pp. 2529–2533.
[155] P. Smaragdis, J.C. Brown, Nonnegative matrix factorization for polyphonic music transcription, in: IEEE
Workshop on Applications of Signal Processing to Audio and Acoustics, New Paltz, NY, October 2003,
pp. 177–180.
[156] A.H. Phan, P. Tichavský, A. Cichocki, Z. Koldovský, Low-rank blind nonnegative matrix deconvolution,
in: Proceedings of 2012 IEEE International Conference on Acoustics, Speech, and Signal Processing
(ICASSSP), 2012, pp. 1893–1896.
[157] P. Smaragdis, Convolutive speech bases and their application to supervised speech separation, IEEE Trans.
Audio Speech Lang. Process. 15 (1) (2007) 1–12.
[158] H.A.L. Kiers, A.K. Smilde, Constrained three-mode factor analysis as a tool for parameter estimation with
second-order instrumental data, J. Chemometr. 12 (1998) 125–147.
[159] M. Mørup, L.K. Hansen, S.M. Arnfred, Algorithms for sparse nonnegative tucker decompositions, Neural
Comput. 20 (2008). <http://www2.imm.dtu.dk/pubdb/>.
[160] A.H. Phan, A. Cichocki, Extended HALS algorithm for nonnegative Tucker decomposition and its applica-
tions for multi-way analysis and classiﬁcation, Neurocomputing 74 (2011) 1956–1969.
[161] R.A. Harshman, M.E. Lundy, Data preprocessing and the extended PARAFAC model, in: H.G. Law, C.W.
Snyder, J.A. Hattic, R.P. McDonald (Eds.), Research Methods for Multimode Data Analysis, Praeger, New
York, pp. 216–284.
[162] L. De Lathauwer, Parallel factor analysis by means of simultaneous matrix decompositions, in: Proceedings
of the First IEEE International Workshop on Computational Advances in Multi-sensor Adaptive Processing
(CAMSAP 2005), Puerto Vallarta, Jalisco State, Mexico, 2005, pp. 125–128.
[163] J.D. Carroll, G. De Soete, S. Pruzansky, Fitting of the latent class model via iteratively reweighted least
squares CANDECOMP with nonnegativity constraints, in: R. Coppi, S. Bolasco (Eds.), Multiway Data
Analysis, Elsevier, Amsterdam, The Netherlands, 1989, pp. 463–472.
[164] C.A. Andersson, R. Bro, The N-way toolbox for MATLAB, Chemometr. Intell. Lab. Syst. 52 (1) (2000) 1–4.

1238
CHAPTER 21 Unsupervised Learning Algorithms and Latent Variable Models
[165] H. Kim, L. Eldén, H. Park. Non-negative tensor factorization based on alternating large-scale non-negativity-
constrained least squares, in: Proceedings of IEEE 7th International Conference on Bioinformatics and
Bioengineering (BIBE07), vol. II, 2007, pp. 1147–1151.
[166] P. Paatero, Least-squares formulation of robust nonnegative factor analysis. Chemometr. Intell. Lab. Syst.
37 (1997) 23–35.
[167] P. Paatero, A weighted non-negative least squares algorithm for three-way PARAFAC factor analysis,
Chemometr. Intell. Lab. Syst. 38 (2) (1997) 223–242.
[168] V. de Silva, L-H. Lim, Tensor rank and the ill-posedness of the best low-rank approximation problem, SIAM
J. Matrix Anal. Appl. 30 (2008) 1084–1127.
[169] S. Weiland, F. van Belzen, Singular value decompositions and low rank approximations of tensors, IEEE
Trans. Signal Process. 58 (3) (2010) 1171–1182.
[170] J.B. Kruskal, Three-way arrays: rank and uniqueness of trilinear decompositions, with application to arith-
metic complexity and statistics, Linear Algebra Appl. 18 (2) (1977) 95–138.
[171] N.D. Sidiropoulos, R. Bro, On the uniqueness of multilinear decomposition of N-way arrays, J. Chemometr.
14 (3) (2000) 229–239.
[172] L.-H. Lim, P. Comon, Nonnegative approximations of nonnegative tensors, J. Chemometr. 23 (2009)
432–441.
[173] E. Acar, D.M. Dunlavy, T.G. Kolda, M. Mørup, Scalable tensor factorizations for incomplete data,
Chemometr. Intell. Lab. Syst. 106 (1) (2011) 41–56. <http://www2.imm.dtu.dk/pubdb/p.php?5923>.
[174] I. Kopriva, I. Jeric, A. Cichocki, Blind decomposition of infrared spectra using ﬂexible component analysis,
Chemometr. Intell. Lab. Syst. 97 (2009) 170–178.
[175] M.A.O. Vasilescu, D. Terzopoulos, Multilinear analysis of image ensembles: tensorfaces, in: Proceedings
of the European Conference on Computer Vision (ECCV), vol. 2350, Copenhagen, Denmark, May 2002,
pp. 447–460.
[176] C. Crainiceanu, B. Caffo, S. Luo, V. Zipunnikov, N. Punjabi, Population value decomposition, a framework
for the analysis of image populations, J. Am. Stat. Assoc. (with discussion and rejoinder) 106 (495) (2011)
775–790. <http://pubs.amstat.org/doi/abs/10.1198/jasa.2011.ap10089>.
[177] G. Zhou, A. Cichocki, S. Xie, Common and individual features analysis: beyond canonical correlation
analysis, IEEE Trans. PAMI. <http://archiv.org/pdf/1212.3913.pdf>.
[178] A. Cichocki, S. Amari, K. Siwek, et al., ICALAB Toolbox, 2007. <http://www.bsp.brain.riken.jp/ICALAB>.
[179] L. De Lathauwer, Decompositions of a higher-order tensor in block terms – Part I and II, SIAM J. Matrix
Anal. Appl. (SIMAX) 30 (3) (2008) 1022–1066.
[180] L. De Lathauwer, D. Nion, Decompositions of a higher-order tensor in block terms – Part III: Alternating
least squares algorithms, SIAM J. Matrix Anal. Appl. (SIMAX) 30 (3) (2008) 1067–1083.
[181] L. De Lathauwer, Blind separation of exponential polynomials and the decomposition of a tensor in rank-
(Lr,Lr,1) terms, SIAM J. Matrix Anal. Appl. 32 (4) (2011) 1451–1474.

22
CHAPTER
Semi-Supervised Learning
Xueyuan Zhou* and Mikhail Belkin†
*Department of Computer Science, The University of Chicago, 1100 East 58th Street, Chicago, IL, USA
†Department of Computer Science and Engineering, The Ohio State University, Columbus, OH, USA
1.22.1 Introduction
Large amounts of data are being generated every second from a wide variety of sources including
online news, social media, images from personal digital cameras, images from telescopy, ﬁnancial data,
biological data, such as DNA sequences, just to name a few.
Of course, all that raw data are useless without efﬁcient tools for extracting and analyzing useful
information contained within. One of the most standard and useful tasks in machine learning and
data analysis is that of labeling, assigning each data point a certain tag, label or a numerical value
according to some prespeciﬁed criterion. These labeled data, in the form of training sets, can be used
to develop algorithms for making automatic inference about the future unseen data, for tasks, such as
image classiﬁcation, predicting disease susceptibility on the basis of genetic information, automatically
tagging new topics, and many others.
However, in many situations obtaining high-quality labeled information can be costly and time-
consuming as it may require human input (e.g., labeling speech, images, or news stories) or experi-
mentation (testing for disease). Thus, there is a growing need to develop effective and computationally
efﬁcient machine learning algorithms utilizing data, which combines a small number of labeled instances
with abundant unlabeled data. In addition, understanding the contribution of unlabeled data to learn-
ing from examples is closely related to the fundamental problem of understanding human and animal
cognitive process, many of them, arguably, unsupervised or weakly supervised. Semi-supervised or
partially supervised learning refers to a class of machine learning techniques which combine labeled
and unlabeled data to build classiﬁers.
Two basic questions of semi-supervised learning is whether unlabeled data are helpful for inference,
and if yes, how they can be used to improve the inference quality. These two questions have been studied
intensively in the last decade. It should be made clear from the outset that effective use of unlabeled
data requires certain assumptions. Certainly, it is not possible to have prediction purely on the basis
of unlabeled data alone. Thus the role of unlabeled data is to restrict the space of potential inference
rules, so that choosing a correct (or nearly correct) predictor requires less labeled data. To make this
possible the structure of unlabeled data has to be aligned with the structure provided by the labels. The
nature of this alignment can be encoded through several mathematical formulations leading to different
algorithms and theoretical analyses.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00022-X
© 2014 Elsevier Ltd. All rights reserved.
1239

1240
CHAPTER 22 Semi-Supervised Learning
The three most common assumptions of semi-supervised learning are the cluster assumption, mani-
fold assumption, and what can be called a “compatibility” assumption. For simplicity, we will discuss
these in the context of classiﬁcation, although other setting may also be applicable.
The cluster assumption suggests that the unlabeled data has clusters, with each cluster corresponding
to just a single class. Thus, identifying the clusters from unlabeled data dramatically reduces the space
of possible classiﬁcation rules. In the simplest noiseless case, one labeled instance per cluster is enough
for classiﬁcation.
The manifold assumption is a bit harder to formulate informally. Basically, it states that the data lie
on or close to low-dimensional submanifolds in a high-dimensional space and that the conditional class
likelihood varies smoothly along the manifold.
Finally, the compatibility assumption relies on data having a different set of attributes, such as web
pages having both text and link information. The assumption is that the classiﬁcation obtained separately
using these attributes need to be consistent when compared on the unlabeled part of the data set.
Some of the earliest theoretical work on semi-supervised learning is based on the cluster assumption.
For example, the papers [1–3] address the relative signiﬁcance of labeled and unlabeled data under a
special type of clustering assumption, when the classes are sampled from different mixture components.
It turns out that in that setting the labeled data is exponentially more informative than the unlabeled
points.Animportantearlyworkonsemi-supervisedlearningisbasedontheVapnik’sideaoftransductive
inference [4]. Transductive inference is not completely equivalent to semi-supervised learning as the
goal of transduction is to classify the unlabeled points without necessarily building a global classiﬁer
(known as the out-of-sample extension problem). Vapnik’s approach consisted in ﬁnding a classiﬁer
that maximized margin over unlabeled data and can be interpreted as a form of cluster assumption.
Another important early algorithm was co-training [5], which introduced the idea of semi-supervised
learning based on the “compatibility” assumption. A more recent class of algorithms has been based
on graph Laplacian, where unlabeled data is utilized to construct a graph representing the underlying
space. The Laplacian of that graph is then used for restricting the space of potential classiﬁers.
In a standard semi-supervised learning (SSL) problem, one is given two sample sets, the labeled
set X L together with its label set YL, and the unlabeled set XU. XL consists of l sample points X L =
{x1, . . . , xl}, with the corresponding labels as YL = {y1, . . . , yl}, and XU consists of u sample points
XU = {xl+1, . . . , xl+u}. The label for XU is YU = {yl+1, . . . , yl+u}, which is unknown. It is also
convenient to denote X = XL ∪XU, and similarly Y = YL ∪YU, where all xi ∈X and yi ∈Y. Let
n = l + u. The X can be a regular subset of Rd, a smooth manifold in Rd, or sequences in language
problems. Similarly, Y can be binary class labels {±1}, real-valued numbers, or complex structured
output. The data in SSL is also called partially observed data.
In inductive inference, the goal is to ﬁnd a function f (x) on the whole data domain X such that for
a new data point x ∈X, its label y = f (x) can be obtained by function evaluation. Instead of requiring
f (x) to be a good estimate on the whole domain, in transductive inference, f (x) is only required to be
deﬁned on the unlabeled sample points, i.e., samples in XU.
To deﬁne the setting for some of these problems more formally, let P(x, y) be a joint distribution on
X ×Y, where X is the domain for the attributes and Y is the domain for the labels. We will assume that the
data is sampled i.i.d. from the probability distribution P, a typical assumption in the machine learning
literature. Let the marginal distribution on X be P(x). Then goal of inference is to reconstruct the
conditional probability distribution P(y|x). Abundant unlabeled data mean that P(x) can be assumed

1.22.2 Semi-Supervised Learning Algorithms
1241
to be given, or at least can be accurately estimated. Thus we need to connect the conditional distribution
P(x|y) to the marginal P(x). Thus the three assumptions can be represented as follows:
•
Cluster assumption: Marginal distribution of the data P(x) has multiple clusters, and example points
of the same cluster are likely to be of the same class. It is possible for one class to have multiple
clusters. The cluster assumption is closely related to the low density separation assumption which
states that the decision boundary should pass through a region of low density.
•
Manifold assumption: The marginal distribution of the data P(x) is supported on a low-dimensional
manifold embedded in a high-dimensional ambient space. The conditional distribution P(y|x) is
smooth, as a function of x, with respect to this low-dimensional manifold.
•
Compatibility assumption: In multi-view learning, disagreement between views happens with zero
probability. Speciﬁcally, in the case of two views, for a data point x = (x1, x2), P( f1(x1)
̸=
f2(x2)) = 0, where f1( · ) and f2( · ) are classiﬁers in each view. This is one of the important
assumptions of co-training [5].
The chapter is organized as follows. A variety of algorithms have been introduced for SSL since
1990s; several of them are reviewed in Section 1.22.2 according to a rough historical order.
One of the earliest SSL algorithms is transductive SVM (TSVM), which is also closely related to
low density separation based algorithms. These methods try to ﬁnd classiﬁers that lie in the low density
regions which are more likely to be the decision boundary. Starting from the cluster assumption and
manifold assumption, graph-based SSL methods are another active area. Some of these methods are
based on mature function approximation theories, and also show promising results in practice.
Another early SSL algorithm is co-training, which explores the idea of using two different “views”
of data to improve learning. Following the co-training framework, multi-view and co-regularization
methods further explore the information gain from using more than one view.
SSL for structured outputs, a popular recent topic of research, is brieﬂy discussed in Section 1.22.3.
Applications include various natural language processing problems. The need for SSL algorithms is
obvious: the difﬁculty of obtaining high-quality labels is even higher in structured learning, since the
output is more complex than a single scalar value.
One key difﬁculty for a wider application of SSL in practice is the computational cost for most
SSL algorithms. Several works on large scale SSL are reviewed in Section 1.22.4. In particular, one
technique using graph Laplacian eigenvectors is discussed in detail.
The fundamental question for SSL: whether unlabeled data are helpful, and if so, how to use them
is still an interesting question. Several related studies are discussed in Section 1.22.5, and several open
issues of SSL are also brieﬂy discussed in Section 1.22.6. For other reviews on SSL, see [6–8].
1.22.2 Semi-supervised learning algorithms
A variety of algorithms have been introduced for SSL since 1990s. Several of them are reviewed in this
section.
1.22.2.1 TSVM and low density separation
Transductive support vector machines (TSVMs) generalize support vector machines to SSL setting with
unlabeled data. Similar to SVMs, TSVMs also learn a large margin hyperplane classiﬁer using labeled

1242
CHAPTER 22 Semi-Supervised Learning
training data. However, TSVMs simultaneously force this hyperplane to be far away from both the
labeled and the unlabeled data.
The linear TSVMs solve the following optimization problem:
min
w,ξ,yi,l+1≤i≤n
1
2∥w∥+ C
l
i=1
ξi + C∗
n

i=l+1
ξi,
(22.1)
s.t.
yi(wT xi) ≥1 −ξi,
ξi ≥0, 1 ≤i ≤l,
yi(wT xi) ≥1 −ξi,
ξi ≥0, l + 1 ≤i ≤n,
where w is the coefﬁcient of the hyperplane, C and C∗are the regularization penalty coefﬁcients. The
problem is similar to a regular SVM problem, while the difference lies in that the labels of unlabeled
points are treated as optimization variables. Then a large margin constraint is added to both labeled and
unlabeled data, hoping to ﬁnd a classiﬁer with the maximum classiﬁcation margin for both labeled and
unlabeled examples. Nonlinear classiﬁers can be obtained by choosing a reproducing Hilbert space as
the solution space.
An approximation solution by local search is proposed by Joachims [9]. The algorithm begins with a
labeling of the unlabeled data based on the classiﬁcation of an inductive classiﬁer. Then it improves the
solution by switching the labels of the unlabeled data so that the objective function decreases. Notice that
TSVMs are transductive, meaning that the algorithm only produces the prediction on unlabeled data,
instead of a function over the whole domain. Several other algorithms are introduced to generalize to
inductive learning, e.g., Laplacian SVM [10]. Since the optimization problem of TSVMs is non-convex,
many approximation methods are introduced. The SVM-light [9] is the ﬁrst widely used TSVMs imple-
mentation. A fast linear SVM (SVMlin) [11] is proposed for large scale application. ∇TSVM using
gradient descent is proposed in [12]. The concave-convex procedure (CCCP) is used in [13] to solve
TSVMs problem. CCCP iteratively optimizes non-convex cost functions that can be expressed as the
sum of a convex function and a concave function. The optimization is carried out iteratively by solving a
sequence of convex problems obtained by linearly approximating the concave function in the vicinity of
the solution of the previous convex problem. See [14] for a review of different optimization techniques.
TSVMs are shown to perform well in text classiﬁcation tasks [9]. Particularly in small labeled data set
cases, it generally outperforms supervised method such as SVMs. However, one drawback of TSVMs
is that it is a non-convex optimization problem, which poses a serious hurdle. Chapelle et al. [15] use
the branch and bound search method on small data sets to ﬁnd the global optimal solution. The authors
show that the globally optimal solution of the TSVMs problem is actually far better than what most
practical methods are able to achieve.
An algorithm combining TSVMs and graph-based distance is proposed in [12]. This graph-based
distance has a close relation to the geodesic on smooth manifolds. Many graph-based methods explicitly
or implicitly involve the low density separation idea, which will be discussed in detail later.
One way of understanding the TSVM algorithm is closely related to the low density separation
assumption, which states that the class separation boundaries should lie among the low density regions,
or samples of different classes are separated by large margin. TSVMs try to ﬁnd a decision boundary
that lies in low density regions.
A typical classiﬁcation result of low density separation algorithms is shown in Figure 22.1 [16,
Chapter 2]. The triangle and square are the only labeled points in a binary classiﬁcation problem and all

1.22.2 Semi-Supervised Learning Algorithms
1243
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
FIGURE 22.1
Low density separation on two-moon data.
other black dots are unlabeled data. The true decision boundary are the low density regions between the
two “moons.” Although the data are distributed in such a way that most supervised classiﬁers will fail,
several low density separation based SSL algorithms, e.g., TSVMs, can take advantage of the unlabeled
data and recover the decision boundary.
1.22.2.2 Co-training and multi-view
One of the most inﬂuential SSL algorithms, so-called co-training, was introduced [5]. The framework
of co-training is representative for various types of multi-view learning approaches.
In co-training, the data consist of a small labeled data set and a large unlabeled set, which is the
typical setting of SSL. However, in addition, each data sample has two distinct views. One typical
example studied in [5] is that the description of a web page can be partitioned into the words occurring
on that page, and the words occurring in hyperlinks that point to that page. Another example would
be two different feature representations of an image, e.g., SIFT [17] and GIST [18]. Notice that the
two views of the ﬁrst example are more likely created by different people, suggesting higher degree of
independence, while for the latter example the two views are created from the same data, in the hope
of capturing their different aspects.
Formally, let the instance space be X = X (1) × X (2), where X (1) and X (2) correspond to the two
different “views” of an example. Typically, a data point x is represented by two different feature vectors,
x(1) and x(2), such that x(1) ∈Rd1 and x(2) ∈Rd2. In this case, instance space X is simply Rd1+d2.
In co-training, each view is assumed to be sufﬁcient for correct classiﬁcation in itself. Let D be a
distribution over the instance space X, and let C(1) and C(2) be the concept classes deﬁned over X (1)
and X (2), respectively.

1244
CHAPTER 22 Semi-Supervised Learning
The co-training framework proceeds as given below:
•
a set XL of labeled examples, with the corresponding label YL;
•
a set XU of unlabeled examples.
Create a pool XU′ of examples by choosing u examples at random from XU, then loop for k iterations:
1. Use (X L, YL) to train a classiﬁer h1 that considers only the x1 portion of x.
2. Use (X L, YL) to train a classiﬁer h2 that considers only the x2 portion of x.
3. Allow h1 to label p positive and n negative examples from XU′.
4. Allow h2 to label p positive and n negative examples from XU′.
5. Add these self-labeled examples to X L.
6. Randomly choose 2p + 2n examples from XU to replenish XU′.
In steps 3–5, the p positive and n negative examples are the ones with the most conﬁdent predic-
tions. There are different variations on how to select candidates from predictions and how to add
them to X L. For example, “agreement” is a natural criteria to select candidates from two classiﬁers;
only a small fraction of the examples with the most conﬁdent predictions are allowed to be added
to X L, hoping to reduce noise; active learning can also be added to step 5 to further improves the
candidate quality. As will be discussed later, many multi-view SSL algorithms follow this co-training
framework.
Two important assumptions of co-training are: (i) the compatibility assumption and (ii) the indepen-
dence assumption.
The compatibility assumption provides an interesting take on SSL, saying that a target function
f = ( f (1), f (2)) ∈C(1) × C(2) is “compatible” with D if it satisﬁes the condition that D assigns
probability zero to the set of examples (x(1), x(2)) such that f (1)(x(1)) ̸=
f (2)(x(2)). It was pointed
out in [5] that C(1) and C(2) can be large concept classes with high complexity, but for a given D the set
of compatible target concept classes might be much smaller, which in turn reduces the requirement of
labeled samples for learning.
Following a PAC-style argument, combining these two assumptions leads to learnability guarantees.
Many different theoretical aspects and applications of multi-view learning are studied, see, e.g.,
[19,20]. One key step in multi-view learning is how to deal with view disagreement, as a result of
either the intrinsic error or noise. A multi-view learning approach that uses a conditional entropy
criterion to detect view disagreement is proposed in [21]. Once detected, samples with view disagree-
ment are ﬁltered and standard multi-view learning methods can be easily applied to the remaining
samples.
1.22.2.3 Co-regularization
Following the co-training framework, regularized regression in reproducing kernel Hilbert spaces
(RKHSs) is studied in multi-view learning in [22]. This method is called co-regularization. As the
name suggests, regularization on different views at the same time plays a key role in the method.
Consider the two views case, let data set X have two views X = X (1) × X (2). In general, cases with
more than two views follow a similar analysis. The classiﬁer spaces for different views are two RKHSs

1.22.2 Semi-Supervised Learning Algorithms
1245
H(1) and H(2). Then the co-regularization problem is deﬁned as follows:
( f (1)∗, f (2)∗) =
arg min
f (1)∈H(1), f (1)∈H(1)
γ1∥f (1)∥2
H(1) + γ2∥f (2)∥2
H(2)
+

x(1)
i
∈XL

yi −f (1)(x(1)
i
)
2
+ μ

x(2)
i
∈XL

yi −f (2)(x(2)
i
)
2
+γc

xi∈XU

f (1)(x(1)
i
) −f (2)(x(2)
i
)
2
.
(22.2)
The solutions can be found by solving a linear system of equations. See Ref. [22] for details. The
solutions are turned into binary classiﬁers through the sign function. The ﬁrst two RKHS norms measure
the complexity of the solutions in each RKHS. The next two terms are the data ﬁtting error in different
spaces. The last term is the “co-regularization” term, or called co-regularizer. It forces the estimators in
two spaces H(1) and H(2) to be close in an L2 sense, i.e., small square distance.
The theoretical properties of the co-regularization problem is studied in detail in [23]. It is shown
that the solution space is also an RKHS, called “co-regularization RKHS.” With this result, the kernels
can be found explicitly, since the reproducing kernels and the associated RKHS uniquely determine
each other. For regularization problems in an RKHS, solutions can be found easily by the Representer
Theorem, with the help of an explicit kernel function. Generalization bound based on Rademacher
complexity is also given in [23].
Instead of studying each view separately ﬁrst, then combining the results, co-regularization focuses
on a single joint function space, which is generated from two individual function spaces. This provides a
new understanding of the multi-view learning problem. The compatibility assumption of co-training can
be studied more naturally and potentially easily in this single joint space. For instance, the agreement
on unlabeled data by co-regularization is shown to reduce the function space complexity. By taking
advantage of the single space view, the method and proof in [23] are elegant and insightful.
The co-regularizer studied in [23] can potentially transform any supervised learning algorithm to an
SSL algorithm in a multi-view setting. This can be implemented by simply adding a co-regularizer term
to the combined supervised learning problems in each view.
The ensemble: Compared to ensemble methods such as AdaBoost which studies how to combine
different classiﬁers on the same data, multi-view learning studies how to combine different views of
the same data. Therefore, there exists a natural many-to-many mapping from the entity of classiﬁer to
the entity of data. The mapping provides a ﬂexible framework to integrate data and analysis methods,
which is illustrated in Figure 22.2. An instantiation means the application of the ith family classiﬁers
to the jth view of a data set. There are two “one-to-many” relations in this ensemble. For each speciﬁc
family of classiﬁers, multi-view learning can be used to improve learning performance. This step can
reduce the classiﬁer space complexity using unlabeled data. The other “one-to-many” relation is that
the “Super classiﬁer” chooses a strategy to combined different classiﬁers on different views. These two
steps can be designed and controlled carefully to produce powerful classiﬁers.
Co-training and multi-view methods provide a ﬂexible and yet powerful framework in practice to
combine different information sources and different classiﬁers. Notice that the speciﬁc classiﬁers do

1246
CHAPTER 22 Semi-Supervised Learning
Classifier 1
Classifier 2
Classifier k
View 1
View 2
View m
.
.
.
.
.
.
Instantiation(i,j)
Super
Classifier
Hidden
Information
FIGURE 22.2
The ensemble and multi-view.
not need to be from the same family. It is possible to combine artiﬁcial neural network in one view
and SVM in another view. This can potentially explore the advantage of different classiﬁer families on
different views. However, the combination should be carefully controlled to avoid overﬁtting.
The ensemble of classiﬁers and data with multi-view has shown impressive performance in practice,
e.g., see, Netﬂix competition grand prize winning algorithm and KDD Cup 2011 Track 1 winning
algorithm “A Linear Ensemble of Individual and Blended Models for Music Rating Prediction,” which
is a linear ensemble of more than 200 classiﬁers from different families.
1.22.2.4 EM and self-train
Expectation-Maximization (EM) [24] is a general approach for iterative computation of maximum
likelihood or maximum a posteriori estimation when the data is incomplete or have missing values.
First recall the deﬁnition of maximum likelihood estimation problem. Consider a probability density
function p(x|θ) which is governed by the set of parameter θ. Assume the i.i.d. data samples X =
{x1, x2, . . . , xn} are drawn from p(x|θ). Therefore, the density for the sample set is
p(X|θ) =
n

i=1
p(xi|θ) = L(θ|X).
(22.3)
The function L(θ|X) is a called the likelihood function given the data X. It is a function of θ for ﬁxed
data X. In a maximum likelihood problem, the goal is to ﬁnd a θ that maximizes L:
θ∗= arg max
θ
L(θ|X).
(22.4)
For certain family of probability density functions, it is possible to ﬁnd θ∗explicitly, e.g., a single
Gaussian distribution. However, for many problems, it is difﬁcult or even impossible to ﬁnd an analytical
expression for θ∗. The EM algorithm is such a general method of ﬁnding the maximum likelihood
estimate of the parameter θ.
Let the complete data be Z = (X, Y), where Y is either the missing data, or some hidden variables.
Assume a joint density function:
p(z|θ) = p(x, y|θ) = p(y|x, θ)p(x|θ).
(22.5)

1.22.2 Semi-Supervised Learning Algorithms
1247
For example, y can be the cluster label in a clustering problem. The likelihood function for Z is L(θ|Z) =
p(X, Y|θ). Due to the missing values, which are random variables, L(θ|Z) is in fact random.
The EM algorithm ﬁrst ﬁnds the expected value of the log-likelihood log p(X, Y|θ) with respect to
the unknown data Y, given the data X and the current parameter estimate θ(t−1).
Q

θ, θ(t−1)
= E[log p(X, Y|θ)|X, θ(t−1)].
(22.6)
The computation of the expectation is called the E-step. The ﬁrst argument θ is the parameter to be
optimized in the maximization of the likelihood function. The second argument θ(t−1) is the parameter
from previous estimate. This means the expectation is taken with respect to the distribution determined
by the data X and the density function with parameter θ(t−1).
The second step is called the M-step, which is to maximize the expectation from the E-step:
θ(t) = arg max
θ
Q

θ, θ(t−1)
.
(22.7)
These two steps are repeated as necessary. Each iteration is guaranteed to increase the log-likelihood
and the algorithm is guaranteed to converge to a local maximum of the likelihood function.
This is a general framework of EM algorithm. For different problems with different families of
probability density functions, the speciﬁc steps are different.
The setting of SSL is similar to the general EM setting: a subset of Y is observed but the values on
unlabeled data are missing. Therefore, EM can be applied naturally to SSL. In EM SSL, generative
models and certain assumptions are usually used to take advantage of the unlabeled data. For example,
one of the earliest successful applications of EM to SSL is text classiﬁcation [25], which estimates the
maximum a posteriori using EM on mixture of multinomials.
In a generative model, it is assumed that data are generated by a parametric model, such as a Gaussian
mixture or other exponential families, with unknown parameters. Maximum likelihood estimation using
the EM algorithm can be used to ﬁnd these parameters given the data. For a supervised maximum
likelihood estimation problem, the goal is to maximize p(X, Y|θ), where X and Y are the data, and θ
is the parameter for the chosen model. In an SSL setting, the data are XL, YL, and XU, therefore, the
goal is the maximize p(XL, YL, XU|θ). The key step is to ﬁll in the missing YU. EM algorithms for
SSL include the following two typical steps:
•
E-step: YU (t+1) = E[YU|X L, YL, XU, θ(t)];
•
M-step: θ(t+1) = arg maxθ p(X L, YL, XU, YU (t+1)|θ).
The E-step is to label the unlabeled data using a model learnt from labeled data and previous estimates.
Then the M-step is to estimate the parameters by treating all the data as labeled ones as in supervised
learning, where the labels of unlabeled data are the predictions from the model in E-step. These two
steps are similar to K-means method, which has a different cost function.
The ﬁrst question to ask, when using a generative approach, is whether the model is correct. When the
chosen model provides a good description of the data, generative approach can be powerful. However,
when the true data generating process is very different from the chosen generative model, unlabeled
data cannot improvement and, in fact, can even hurt performance.
Another issue for EM in generative model is that, typically it is a non-convex problem with local max-
ima. Different heuristics such as active learning and random restart can be used to improve the solution.

1248
CHAPTER 22 Semi-Supervised Learning
1.22.2.4.1
EM for mixture model
EM algorithm together with mixture models is used frequently in many aspects of machine learning,
including supervised, unsupervised and semi-supervised learning. Mixture model assumes that data are
sampled from a mixture of more than one distributions with densities pi, which are parameterized by θi:
p(x|) =
m

i=1
αi pi(x|θi),
(22.8)
where  = (α1, . . . , αm, θ1, . . . , θm), and ∀i, αi > 0, 	m
i=1 αi = 1. This means the data are gener-
ated in two steps: ﬁrst, choose probability density pi with probability αi; second, sample from pi. The
log-likelihood for sample X then become
log

L(|X)

= log
n

i=1
p(xi|) =
n

i=1
log
⎛
⎝
m

j=1
α j p j(xi|θ j)
⎞
⎠.
(22.9)
The form of a log of sum is difﬁcult to optimize. The EM algorithm solves this problem by assuming hid-
den variables Y = {yi, i = 1, . . . , n}. These hidden variables are the labels of components. For instance,
yi ∈{1, . . . , m} and yi = k means xi is generated by the kth component with probability density pk. If
Y is given, which is the case in supervised learning, the likelihood function for both X and Y is
log

L(|X, Y)

= log

p(X, Y|)

=
n

i=1
log

αyi pyi (xi|θyi )

.
(22.10)
This likelihood function is of the form of a sum of log functions, which is much easier to optimize.
In clustering, Y is not observed, given a guess of t, it is not difﬁcult to ﬁnd the distribution of the
unobserved Y, which is
p(yi|xi, t) =
αt
yi pyi (xi|θt
yi )
p(xi|t)
=
αt
yi pyi (xi|θt
yi )
	m
j=1αt
j p j(xi|θt
j).
(22.11)
Then
p(Y|X, t) =
n

i=1
p(yi|xi, t).
(22.12)
Giventheseexplicitresults,EMalgorithmcanproceedwithoutdifﬁculty.Theestimatedαi afterM-stepis
αk = 1
n
n

i=1
p(k|xi, t).
(22.13)
This means, given a guess t, the best estimate for α is the sum of the probability of all samples belong-
ing to that component. Similarly, θi can also be obtained. Then we have another better guess (t+1).
The EM algorithm repeats as necessary. See, e.g., Ref. [26] for more details on mixture of Gaussians
and EM in Hidden Markov Model.

1.22.2 Semi-Supervised Learning Algorithms
1249
In SSL, Y is partially observed. Then the likelihood function is
log

L(|X L, XU, YL)

= log

p(X L, XU, YL|)

,
(22.14)
which includes two parts, a likelihood function on unlabeled data with hidden variables and a regular
likelihood function on observed data.
1.22.2.4.2
Value of unlabeled data
The value of unlabeled data in parametric models is ﬁrst studied in [1–3], where l labeled and u unlabeled
samples were used together for learning. Let R(l, u) denote the optimal probability of error for l labeled
and u unlabeled samples, the main results under mild condition in [1,2] are
R(0, u) = R(0, ∞) = 1
2,
(22.15)
meaning that the unlabeled sample does not help without labeled points, and
R(l, ∞) = R∗+ exp

−αl + o(l)

,
(22.16)
where R∗is the optimal Bayes error, and α depends on the weights of the mixture and each mixture
density function. This means the labeled sample set reduces the probability of error exponentially fast
to the Bayes risk. In a sequel, the value of unlabeled sample set is studied. The main result is that
R(l, u) = R∗+ O(u−1) + exp ( −αl + o(l)).
(22.17)
This shows that the unlabeled set reduces the probability of error at the rate of u−1, a much slower rate
compared to labeled data.
A binary classiﬁcation problem under the Gaussian mixture model is studied in [3]. Given ﬁnite
labeled and unlabeled data, the goal is to estimate the Gaussian parameters and the mixture weights. The
misclassiﬁcation probability is shown to deviate from the minimal Bayes error rate by O(d3/5u−1/5) +
O(e−cl), where d is the dimensionality of the Gaussian and c is a positive constant.
1.22.2.4.3
Self-training
In SSL, the two steps of EM algorithm can also be seen as labeling unlabeled examples ﬁrst, then use
the estimated value to train the classiﬁer again. The process is repeated until certain criteria is met. This
type of algorithm is also called self-training. Other relevant names include “bootstrap learning” and
“automated labeling.” One of the earliest works on self-training is the application to the word sense
disambiguation problem [27]. Instead of using all the estimates from the E-step in the M-step in this
work, a subset of the result with higher “conﬁdence” is selected and added to the labeled set. Similarly,
active learning can also be used in this step to select different subsets to augment the labeled set. Co-
training and multi-view methods also explore the idea of augmentation of a small labeled set in SSL,
but from a different problem setting.
1.22.2.5 Graph-based SSL
Graph-based SSL is among the most popular SSL methods. As the name suggests, graph plays an
important role in this family of algorithms. The data in these problems appear in the form of a weighted

1250
CHAPTER 22 Semi-Supervised Learning
graph. This can be further categorized into two cases. First, the data itself is a weighted graph (directed
or undirected), such as various social networks. Second, data are in the form of i.i.d. samples from
certain domains other than graphs, then the data samples are converted into the weighted graph forms.
There are diverse algorithms for graph-based SSL methods, with plenty of variations and intuitions,
such as, random walk on graphs [28–30], graph cut [31], electronic networks [32], harmonic functions
[33,34], Green’s functions [35,36], regularization and reproducing kernels [10], etc. Among these
algorithms, some are transductive, meaning the predictions are only on the unlabeled data. In this case,
another step is needed for prediction on out-of-sample unseen examples: either add the unseen examples
to the unlabeled data set and solve the problem again, or use another step to approximate the prediction,
see, e.g., [37]. Others are inductive, meaning the algorithm output an estimator on the whole domain,
e.g., [10]. See [8] for more reviews.
Some graph-based SSL algorithms are proposed as a discrete approximation to the corresponding
continuous problem, while others are proposed for discrete graphs directly. This leads to an issue in the
inﬁnite sample analysis. Due to different problem settings and assumptions, certain graph-based SSL
algorithms might not have a meaningful limit in the limit of inﬁnite samples, either labeled or unlabeled.
For instance, if the graph is the Internet graph or other social networks, then strictly speaking, even the
“limit of inﬁnite sample” is not well deﬁned.
In this section, the focus is a family of graph Laplacian-based SSL methods whose limits are well
deﬁned under certain mild conditions. This family includes a large number of different algorithms,
most of which fall under the same framework from a functional analysis view. The typical setting
of these methods is as follows. The data samples xi are from a smooth unknown probability density
p(x) supported on a certain smooth subset  of Rd. This includes the cases when the domain is a
low-dimensional smooth submanifold. Notice that most algorithms in this family can still be applied to
ﬁnite graph data or samples from domains other than Rd, such as unit hypercube {0, 1}d or sequences in
natural languages given ﬁnite data. However, the limit analysis and interpretation with inﬁnite samples
for algorithms on these data will be different, when the limits do exist.
For i.i.d. samples from a subset of Rd, the graph construction proceeds as follows. Any sample point
xi ∈X = X L ∪XU is mapped into a vertex of a weighted graph G(V , E), and the edge weight wi j is
a similarity weight for each pair of example xi and x j deﬁned by certain weight function on the edge,
e.g., Gaussian weight wi j = e−∥xi−x j∥2/t with parameter t. In directed graph, the weight is asymmetric.
Graph G with vertex set V = X and weighted edge set E acts as a portal to the complex data source.
An example is that, the local neighborhood graph constructed from uniform samples on a unit sphere
acts as the approximate unit sphere.
1.22.2.5.1
Graph Laplacian regularization
Given sample sets X L, XU, and YL, one popular family of graph-based SSL method is called the graph
Laplacian regularization, which solves a regularized least squares problem as the following:
min
f ∈Rn
l
i=1

f (xi) −yi
2 + μ
n2
n

i, j=1
wi j

f (xi) −f (x j)
2 ,
(22.18)
where n = l + u is the total number of examples, wi j is a “similarity” value for example xi and x j,
and μ is a weight balancing the ﬁdelity to the observations on the labeled data and the second penalty

1.22.2 Semi-Supervised Learning Algorithms
1251
term. Notice that f is a vector in Rn, and f (xi) = fi. This notation is convenient for the limit analysis,
emphasizing that vector f is a projection of a continuous function on the sample set. Several different
algorithms belong to this family of algorithm, see, e.g., [8].
For an inverse problem, there are possibly inﬁnite functions that have zero cost. The second penalty
term provides a preference to certain functions. Consider the following typical weight function:
wi j = exp

−∥xi −x j∥2
t

,
(22.19)
where the closer xi and x j are, the larger wi j is. For data points that are far apart, the weight is close to
zero. This means the function value differences in the penalty term of the optimization problem (22.18)
only contribute to the summation for all data pairs that are near each other. The minimization drives the
solution to have small difference for nearby data points, implying that the solution is smooth locally.
On the other hand, there is little or no penalty for points that are far apart.
The quadratic term can be rewritten as the following matrix form using the graph Laplacian L:
f T L f = 1
2
n

i, j=1
wi j

f (xi) −f (x j)
2 .
(22.20)
The graph Laplacian matrix L is deﬁned as follows. Given a graph G(V , E) with weight matrix W, and
its diagonal degree matrix D deﬁned as D(i, i) = 	
j W(i, j), where W(i, j) is the element at the ith
row and jth column in matrix W, the commonly used graph Laplacian is deﬁned as the following:
L = D −W.
(22.21)
L is also called the unnormalized graph Laplacian, or the combinatorial Laplacian for 0/1 weights.
There are other version of graph Laplacians, see, e.g., [38,39].
Then the graph Laplacian regularization problem (22.18) can be rewritten as
min
f ∈Rn
l
i=1

f (xi) −yi
2 + μ
n2 f T L f ,
(22.22)
where the coefﬁcient 1
2 is ignored. The solution using pseudoinverse can be found to be
f =

S + μ
n2 L
+
SY.
(22.23)
Y is a column vector with Y(i) = yi for xi ∈X L and Y(i) = 0 for xi ∈XU, S = Diag(1, . . . , 1, 0,
. . . , 0) with the ﬁrst l diagonal entries as 1 and the rest 0.
Several variations of the problem (22.22) have been proposed. An interpolation problem, instead
of a “soft” regularization problem is introduced in [34]. The same regularization using a symmetric
normalized version of graph Laplacian is proposed in [40]. Regularization based on the spectrum of
the Laplacian is proposed in [33]. Tikhonov regularization interpretation and generalization error are
discussed in [41]. The quadratic form f T L f and its variations are frequently used in many other machine
learning areas. See [8] for more variations.
When data is from the Euclidean space Rd, and for a ﬁxed function f ∈C2(Rd), the following
limit of the quadratic form was studied in [42] as the number of data examples increases to inﬁnity and

1252
CHAPTER 22 Semi-Supervised Learning
the weight function adjusts accordingly
f T L f
p→c

∥∇f ∥2 p2(x)dx,
(22.24)
where c is a constant depending on the weight function in the deﬁnition of graph Laplacian, p(x) is the
probability density of the random sample, and ∥∇f ∥2 is the gradient square.
Notice that the limit analysis is based on the assumption that, the graph G is built on the data sample
X from some distribution in Rd. If graph G does not have this interpretation, e.g., some non-Euclidean
space with other distance measure, the limit might not even exit, and the interpretation can be very
different even the limit does exist.
The graph-based SSL problem in (22.22) then can be seen as an empirical version of the continuous
problem
min
f
l
i=1

f (xi) −yi
2 + μ

∥∇f ∥2 p2(x)dx.
(22.25)
Notice that the uniform convergence of solutions on certain smooth function space is needed to show a
rigorous equivalence.
Various explanations have been introduced to help the understanding of the graph Laplacian regular-
ization. For example, random walk on graphs is one of the most popular ones [34,40]. On the other hand,
random walk is also used to argue why the method fails in the limit of inﬁnite unlabeled data points in
high dimensions [43]. The seemly contradictory views might be better understood from another context:
there are pairwise equivalences between random walk (or diffusion), Laplace equation, and energy min-
imization problems. The Laplace equation governs a diffusion process, which is a random walk in ﬁnite
sample case; the minimizer of the potential energy

∥∇f ∥2dx is the solution of a Laplace equation,
which is a harmonic function. See, e.g., [44] for an application in the background of a boundary value
problem. This view connects many different variants of the graph Laplacian-based SSL methods.
An alternative view to the graph Laplacian regularization is the reproducing kernel Hilbert space
(RKHS) view. Since the graph Laplacian has a zero-valued eigenvalue, it has a null space spanned by
the associated eigenvector. For instance, for the unnormalized graph Laplacian L = D −W, the null
space is spanned by the constant eigenvector. The regularizer f T L f in fact is an RKHS norm in the
subspace that is orthogonal to the null space of L, see, e.g., [45, Chapter 6] or [35]. The reproducing
kernel matrix is the pseudoinverse of L. This means, by the Representer Theorem, the solution of the
graph Laplacian regularization is in the form of a summation of the rows of the kernel matrix.
These functional aspect views provide more insights to the graph Laplacian regularization problem,
going beyond the random walk intuition. The idea is further explored in detail next.
1.22.2.5.2
Iterated graph Laplacian regularization
The setting of a small labeled set with a large unlabeled set is unique to SSL. This involves two factors
in the large sample analysis of SSL: the limit analysis of both the labeled and unlabeled samples.
Considering that the unlabeled data are generated much faster than the high-quality labeling process,
the limit analysis of algorithms in the setting of a limited labeled set together with inﬁnite unlabeled
samples is attractive in practice.
In this SSL setting, the more unlabeled data become available to an SSL algorithm, the better
prediction it should produce, according to the philosophy of SSL. However, this may not be the case

1.22.2 Semi-Supervised Learning Algorithms
1253
for certain SSL algorithms. For example, consider the SSL problem in Eq. (22.22), when the labeled
set is ﬁxed, the more unlabeled data involve in the optimization problem, the worse the classiﬁcation
results become, as shown in [43]. The graph Laplacian regularization only works in one-dimensional
spaces with inﬁnite unlabeled data. On domains with dimensions higher than 1, the solution will overﬁt
labeled data perfectly. The solution is essentially a signed indicator function of labeled data for binary
classiﬁcations. This shows there is no generalization in the limit. In practice, the overﬁtting causes two
serious problems: ﬁrst, the solution shifts to the positive or negative side, which depends greatly on the
few labeled data examples; second, the solution is extremely close to zero. These two aspects make the
graph Laplacian regularization method numerically unstable.
The reason for this problem is analyzed from a functional analysis point of view in [36]. The reason
of the overﬁtting is that the solution space is too rich. With inﬁnite unlabeled data, the graph Laplacian
regularizer becomes the gradient square norm as

∥∇f (x)∥2dx,
(22.26)
where the probability density term p2(x) is not important and ignored when it is bounded from below.
This norm in fact is one of the Sobolev norms, see, e.g., [46].
The solution to the overﬁtting problem is solved in [36] by essentially shrinking the solution space.
This can be implemented through a higher order Sobolev norm. The norm used in [36] is called the
iterated graph Laplacian semi-norm, deﬁned as the following:
f T Lm f ,
(22.27)
where m is a positive integer for computational purpose, though m can also be real numbers. The
motivation is that the iterated Laplacian m operator is a natural object associated with Sobolev spaces,
and in particular, Sobolev spaces on smooth Riemannian submanifolds, see, e.g., [47, Chapter 5]. When
m = 1, it becomes the regular Laplacian, which in Rd is deﬁned as
 =
d

i=1
∂2
∂x2
i
.
(22.28)
By connecting the power of the graph Laplacian matrix and the iterated Laplacian operator, the
following iterated graph Laplacian regularization generally provides better solutions than the regular
graph Laplacian regularization, as shown empirically in [36]:
min
f ∈Rn
l
i=1

f (xi) −yi
2 + μn f T Lm f ,
(22.29)
where μn depends on n and the weight function.
The spectral transform of the graph Laplacian in ﬁnite sample cases has been discussed and tested
in [10,33,35]; the limit is analyzed in [36]. Sobolev embedding theorem [46] provides the condition of
the spectral transform for the graph Laplacian to give a smooth solution.
The spectral transform requirement is also closely related to RKHS view. In the ﬁnite sample case,
the norm f T L f corresponds to an RKHS, while in the limit of inﬁnite unlabeled data, the Sobolev

1254
CHAPTER 22 Semi-Supervised Learning
normed space may not be an RKHS, depending on the dimensionality of the underlying data domain.
Speciﬁcally, 2m > d needs to hold in order to make sure the Sobolev space is an RKHS. d should
be replaced with the intrinsic dimensionality when the data domain is a low-dimensional manifold. In
non-RKHS cases, the Green’s function is a useful tool to study the problem.
Finite sample case: In ﬁnite data cases, we always have an RKHS since any ﬁnite-dimensional Hilbert
space is an RKHS, see, e.g., [45]. This means the discrete Green’s function is the same as reproducing
kernel in the subspace orthogonal to its null. Let the kernel matrix be K and the discrete Green’s function
matrix be G, then for semi-norm f T L f , the reproducing kernel in the subspace orthogonal to its null is
the pseudoinverse of matrix L, i.e., K = L+ [45, Chapter 6]. Notice that the exact kernel for the semi-
norm includes another kernel in its null space. The discrete Green’s function should satisfy GL = I
and LG = I, which implies that G = L+. This is also true for symmetric semi-deﬁnite matrix Lm. We
can write both discrete Green’s function and reproducing kernel for Lm by eigenfunction expansion as
Gm(x, y) = Km(x, y) = (Lm)+(x, y) =
n

k=2
1
(λk)m φk(x)φk(y),
(22.30)
where λk and φk(x) are the kth eigenvalue and eigenvector of L. φk(x) means the x element of vector
φk. We can see that for a positive integer m, compared to λn, the smaller λk is, the larger 1/λm
k will be
relatively. This means Gm(x, y) will become smoother and smoother as m increases, since for graph
Laplacians, the smaller λk is, the smoother the associated eigenvector is, and λk are in increasing order.
In order to have an intuitive understanding of the effect of m, we generate an artiﬁcial data set and plot
the kernel functions in Figure 22.3. The data set includes a mixture of two Gaussians of unit variance
on R2 centered at ( ± 1.5, 0), and an additional uniform over [−3, 3] × [−3, 3] to avoid empty regions.
In Figure 22.3, we show the discrete Green’s function corresponding to Lm for two Gaussians with unit
variance in R2 at ( −2, −1.8). When m increases, the Green’s function or reproducing kernel “grows”
from “spikes” to smooth functions. From the contour plot, even the location of the kernel is not near the
means of the Gaussians, when m increases, the kernel function recovers the true boundary of the two
Gaussians. In fact, as long as the centers of kernel functions are in relatively high density regions of the
Gaussians, a proper m value will produce a kernel function which changes little within those regions.
This is exactly the basis for several graph Laplacian-based SSL algorithms.
Compared to the thin plate splines [48], the iterated Laplacian regularization can be viewed as
a generalization of the thin plate splines from regular domains to unknown submanifolds, from a
coordinate dependent Sobolev semi-norm deﬁned by partial derivatives to a coordinate free iterated
Laplacian semi-norm using Laplacians, from data independent reproducing kernels to data dependent
kernels. One key difference is the null space between the two methods. The null space of f T Lm f is
spanned only by the ﬁrst eigenvector, while in thin plate splines the null space is spanned by polynomials
of high degrees, whose dimension increases fast as d and m increase.
1.22.2.5.3
Finite-dimensional approximation
As regularization is a solution for an ill-posed inverse problem, the ﬁnite-dimensional approximation
is also an effective solution to an inﬁnite-dimensional problem. Consider the graph Laplacian regular-
ization problem (22.22), the solution lives in the span of all the eigenvectors of the graph Laplacian L.
Instead of using all the eigenvectors, a subset of these eigenvectors are chosen and used in [33].

1.22.2 Semi-Supervised Learning Algorithms
1255
−0.4
−0.2
−0.2
0
0
0.2
0.4
0.6
0.8
m=1
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
−0.8
−0.6
−0.6
−0.4
−0.4
−0.2
0
0.2
0.2
0.4
0.4
0.6
0.6
0.8
m=2
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
−0.8
−0.6
−0.6
−0.4
−0.4
−0.2
0
0.2
0.4
0.4
0.6
0.6
0.8
m=4
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
−0.8
−0.6
−0.6
−0.4
−0.4
−0.2
0
0.2
0.4
0.4
0.6
0.6
0.8
m=8
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
FIGURE 22.3
The Green’s functions at (−2, −1.8) for a mixture of two Gaussians.
For a graph Laplacian L, whose limit is the Laplace operator in Rd or Laplacian-Beltrami operator
on a submanifold [49], the eigenfunctions form an L2 basis of square integrable functions. Let φi and
λi be the ith eigenfunction and eigenvalue of the Laplace operator , i.e., φi = λiφi, then any L2
function f can be written as
f =
∞

i=1
αiφi,
s.t.
∞

i=1
α2
i < ∞.
(22.31)
Assume the eigenvalue and eigenfunctions are ordered in increasing-eigenvalue order. Then the ﬁnite-
dimensional approximation problem is
min
α1,...,αk
l
i=1
⎛
⎝yi −
k

j=1
α jφ j(xi)
⎞
⎠
2
,
(22.32)

1256
CHAPTER 22 Semi-Supervised Learning
where k is a ﬁxed number that can be chosen by validation. This problem is also called Laplacian
eigenmaps SSL, since these eigenvectors deﬁne the Laplacian eigenmaps [50]. The least squares solution
can easily be written down explicitly. Notice that the squares error is only computed on the labeled data
set, while the function basis φi are computed on all the data points. This is a key step for SSL. Function
basis can be computed without any label information. These basis are only supported on data samples,
which has both pros and cons. One advantage is that they reduce the function class complexity, e.g., on
intrinsic spaces; one disadvantage is that they are not deﬁned for out-of-sample data. In ﬁnite sample
case, the graph Laplacian eigenvectors are used instead of the continuous Laplacian eigenfunctions.
Another important advantage of using these eigenvectors is that they have an intrinsic smoothness
ordering, measured by the eigenvalue. By multiplying φi(x) to both side of φi(x) = λiφi(x), and
integrating both sides, the following relation can be found:

φi(x)φi(x)dx = λi

φ2
i (x)dx = λi,
(22.33)
since the eigenfunctions are orthonormal. With proper boundary conditions for the Laplace operator ,
the left-hand side is

φi(x)φi(x)dx =

∥∇φi(x)∥2dx.
(22.34)
This means the smaller the eigenvalue is, the smoother the corresponding eigenfunction is, in the sense
of gradient squares norm (one type of Sobolev norm). For instance, for the graph Laplacian λ1 = 0,
thus the gradient squares integral should be 0. This is consistent with fact that the ﬁrst eigenvector of
the graph Laplacian is constant. This nice property provides a key step for the asymptotic error analysis
of the method, and also provide a ﬂexible guidance for choosing eigenvectors in practice.
The optimal number of eigenfunctions and optimal integrated mean squares errors for the regression
problem (22.32) is studied in [51]. In the limit of inﬁnite unlabeled data, the error analysis shows that,
when the true regression functions are differentiable, the optimal number of eigenvectors and the optimal
integrated mean square error (IMSE) given l labeled and inﬁnite unlabeled points are
k∗∼
O(l
d
d+2 ),
(22.35)
IMSE∗∼
O(l−
2
d+2 ),
(22.36)
where d is the intrinsic dimensionality of the domain, and the notation 
O includes a log factor. The
results are further generalized to m-times differentiable functions.
Analysis in [51] also shows that Laplacian Eigenmaps SSL achieves the same error rate using the
same optimal dimensions compared to least squares estimates using tensor product spline spaces with
equidistant knots [52, Chapter 15.3], and the same rate as local polynomial regression [53].
Notice that the result is in a distribution-free setting. This means in cases when the cluster assumption
does not hold, or there is no relationships between the marginal distribution of data X and its label Y,
unlabeled data still can help learning asymptotically. This is achieved by reducing the function class
complexity, which in turn improves learning rate.
1.22.2.5.4
Manifold regularization
Graph Laplacian regularization in problem (22.22) is transductive. It cannot make prediction on out-of-
sample data. Manifold regularization [10] is introduced to generalize the graph Laplacian regularization

1.22.2 Semi-Supervised Learning Algorithms
1257
to inductive inference. The framework introduced a novel regularizer combining the inductive ability
and the geometry of the data domain. Manifold regularization includes a family of learning algorithms.
Variants generalizing SVM and regularized least square are proposed in [10]. The regularized least
square version of the inductive SSL problem is as follows:
min
f ∈HK
l
i=1

f (xi) −yi
2 + μn∥f ∥K + γn∥f ∥I ,
(22.37)
where ∥f ∥K is a norm of f in a chosen reproducing kernel Hilbert space HK , ∥f ∥I is a norm depending
on the intrinsic geometry of the data, e.g., ∥f ∥I = f T L f , and μn and γn are parameters to balance
each term in the objective function.
A Representer Theorem to problem (22.37) when ∥f ∥I = f T L f is given as
f (x) =
l+u

i=1
αi K(xi, x).
(22.38)
The ambient space norm ∥f ∥K provides the inductive ability of this algorithm, while the graph Laplacian
regularizer term provides data dependent regularization. For example, RBF Kernels or polynomial
kernels can be used as discussed in [10].
Manifold regularization connects spectral graph theory, manifold learning, and RKSH together in
one regularization framework. One of the important advantages of the manifold regularization is the
induction ability, which most other graph Laplacian regularization methods do not enjoy. The other
feature is the regularization framework, i.e., an intrinsic norm plus an extrinsic norm. The combined
regularization provides much ﬂexibility from induction in the ambient space to the data dependent
domain. More importantly, the problem can be easily solved with the help of the Representer Theorem.
Iterated Laplacian and Sobolev norms can also be used in the manifold regularization framework by
simply replacing ∥f ∥I with f T Lm f , as suggested in [10,36]. More Sobolev norms examples can be
found in, e.g., [45]. Manifold regularization is further studied in the setting of co-regularization in [23],
where different norms are treated to be in different views.
1.22.2.5.5
Measure-based regularization
A density based regularization method is proposed in [42] with the following regularizer:

⟨∇f (x), ∇f (x)⟩p(x)dx =

∥∇f (x)∥2 p(x)dx,
(22.39)
where ∇f (x) is the gradient vector and p(x) is the probability density of samples. The goal is to use a
gradient based regularizer that penalizes variations of the function more in high density regions and less
in low density regions. However, the kernel associated with this norm is not straightforward to obtain.
As an alternative solution, the gradients of functions in the span of a chosen basis and the density p(x)
are estimated directly. The experiments on real world data sets in [42] are not successful.
There are several difﬁculties in this method. First, density estimation is difﬁcult in high dimensions.
Although unlabeled data points are abundant in SSL, the computation is expensive. Second, the gradient
estimated in [42] is in ambient spaces. If data live on or near a smooth submanifold, this gradient can
be irrelevant to learning, which is ﬁrst discussed in [10].

1258
CHAPTER 22 Semi-Supervised Learning
With the help of limit analysis of graph Laplacians and the graph Laplacian regularizer [49,54,55]
and the two-step normalized graph Laplacian, next we show a simple method that solves both of the
above problems.
Two-step graph Laplacian: Besides the commonly used graph Laplacians L = D −W and its
variants, there is another family of more general “two-step” graph Laplacians, which is introduced by
Coifman and Lafon [54] and further studied by Hein et al. [49]. In this deﬁnition, one ﬁrst normalizes
the weight matrix W as follows:
Wα = D−αW D−α,
α ∈R.
(22.40)
Then the element in the matrix, or the normalized weight function is
Wα(xi, x j) = Wα(i, j) =
W(i, j)
[D(i, i)D( j, j)]α .
(22.41)
Given this new normalized weight matrix Wα, one can deﬁne empirical Laplacians accordingly. For
example, Lα = Dα −W α, where we use the superscript α to emphasize the normalization parameter
for the weight matrix. This should not be confused with the power of a matrix. Notice that when α = 0,
these two-step graph Laplacians become the usual ones. In particular, when α = 1
2, with proper scaling
the following limit exists [55]:
f T Lα f
p→

∥∇f (x)∥2 p(x)dx.
(22.42)
The gradient here is automatically computed on the data manifold when the data are sampled from a
smooth manifold. The density term is implicitly encoded in the two-step normalized graph Laplacian.
The reproducing kernel can also be found as the pseudoinverse of Lα.
The measure associated with the two-step graph Laplacian with different α brings another issue in
using the graph Laplacian. For instance, for the commonly used unnormalized graph Laplacian (α = 0),
the limit is as follows [42]:
f T L f
p→

∥∇f (x)∥2 p2(x)dx.
(22.43)
The discrepancy between the measure for the regularizer and the probability of the sample needs more
investigations.
1.22.2.5.6
Discussions
Graph-based SSL is closely related to two other areas: wavelet on graphs and kriging. Multiresolution
analysis and wavelet have been a successful tool for low-dimensional data analysis such as time series
and images. With the rising of graph data and high-dimensional data, the multiscale concept can be
particularly powerful. One reason is that the concept of location and scale are more abstract and also more
important in high-dimensional analysis. This include cases where data are not from a Euclidean space,
e.g., social networks. However, the methods and theories for wavelet for graph and high-dimensional
data are far behind their counterparts in low-dimensional data analysis. Several recent works in this area
are reviewed next, including the discussions of their applications in SSL.
The basis of multiresolution analysis is a hierarchy structure of function spaces. One natural choice
is the Sobolev spaces associated with iterated Laplacian. There are a family of generalized function

1.22.3 Semi-Supervised Learning for Structured Outputs
1259
spaces associated with iterated Laplacian which are closely related to Sobolev spaces, see, e.g., [47,
Chapters 4 and 5]. Iterated Laplacian is used in [36] to solve an illness problem of graph Laplacian
regularization in a limit setting.
A novel method is introduced in [56] for constructing wavelet transforms of functions deﬁned on
the vertices of an arbitrary ﬁnite weighted graph. The approach is based on deﬁning scaling using the
graph analog of the Fourier domain, with the help of the graph Laplacian.
Diffusion wavelet [57] is another technique which is closely related to graph Laplacian. The method
is built on a diffusion operator T. Functions at different scale corresponds to the diffusion process at dif-
ferent time. The main assumption is reduction of the numerical ranks as we take powers of the operator T.
Compared to graph spectral wavelet [56], the basis in diffusion wavelet are orthogonal. The orthog-
onalization step in the construction of wavelets destroys the compact support of the basis. Thus these
basis are functions deﬁned over the whole data domain. This might be preferable in SSL since global
basis can be used to ﬁt labeled data and make prediction on unlabeled data at the same time. On the
other hand, wavelets in [56] are not orthogonal, but have compact support.
Haar-like wavelet is proposed in [58] for tree and graph data. One advantage of the method is that
the wavelet coefﬁcients decay exponentially fast, which encourages sparse representation of functions.
Another interesting view to several graph-based SSL algorithms is through kriging, which is a
geostatistical technique to interpolate the value of a random ﬁeld at an unobserved location from
observations of its value at nearby locations. This idea resembles transductive SSL problems. In [59],
several popular graph Laplacian regularization solutions are shown to be equivalent to kriging predictors
with a ﬁxed covariance matrix, which is determined by the data sample density and the geometry of
the data domain. Kriging view brings more attention to the correlation among data samples, which is
closely related to reproducing kernels and function space view.
Kriging provides an alternative way of interpreting graph Laplacian regularization in ﬁnite sample
case, and also brings more insight under the Gaussian process framework. These results are consistent
with the function analysis view presented earlier.
The kriging view shows that many different graph Laplacian regularization problems use a covariance
matrix that is a given function of the graph adjacency matrix W. This might be a reasonable choice if the
vertices of a graph are i.i.d. samples from certain subset of Rd, e.g., the surface of a sphere. However,
for other types of graphs that do not correspond to i.i.d. samples, e.g., social network graphs, the graph
structure itself might not be enough to capture the correlations. One key step toward this covariance
matrix design is discussed in [59]. Instead of only using the adjacency matrix W from data samples,
response values are also used in the matrix design. Speciﬁcally, the covariance matrix consists of two
component: one is stationary based on the vertex similarity, which comes from W, while the second part
depends on vertex and need not to be stationary.
1.22.3 Semi-supervised learning for structured outputs
Complex output structures arise in speech recognition, various forms of object recognition tasks, and
also in a wide range of natural language tasks such as Part of Speech (POS) tagging and parsing,
just to name a few. Different supervised algorithms have been proposed to take advantage of the
internal structure through Markovian or graphical model assumptions. Compared to SSL in vector or

1260
CHAPTER 22 Semi-Supervised Learning
scaled-valued function learning tasks, there is less progress in SSL with structured data. For structured
data, the labeling process is even more expensive, which makes SSL more attractive in these tasks.
When each sample point xi ∈X and each label yi ∈Y are not vectors or scalars, but are sequences,
e.g., xi is a sentence, and yi is the Part of Speech (POS) tag, or even more complicated objects such as
trees, the learning problem is called structured learning. The task is to learn a mapping f : X →Y.
For each x, let Y(x) be the possible output set, which is ﬁnite in most practical problems.
A standard framework of learning structured models is by developing functions h : X →Y as
h(x) = arg max
y∈Y(x) F(x, y),
(22.44)
where F is a scoring function for each pair. For computational tractability and feasibility, F can be
broken into scores that are deﬁned on individual positions of the output sequence. This is also done to
take advantage of the Markovian dependence that is generally assumed over the output sequences.
The unique feature for structured learning problem is how to model the structure/dependency among
elements of data instance (both x and y). With proper models for these dependencies, many SSL algo-
rithms can be applied to the structured output problems. For instance, by deﬁning a graph-based kernel
for structured learning problems, kernel-based SSL methods can be applied directly to structured learn-
ing. See, e.g., [60,61]. Manifold regularization for structured learning is discussed by Altun et al.
[62]. Brefeld and Scheffer [63] studied a multiple-view regularizer for structured SVM in SSL. More
references on structured SSL can be found in [64–71].
1.22.4 Large scale semi-supervised learning
Large scale learning is particularly important for SSL, since the aim of SSL is to use the huge amount
of unlabeled data to improve learning. Toward this goal, various new SSL methods and approximations
of the existing methods have been proposed for scalable SSL.
From an optimization point of view, especially convex optimization, there are well-studied techniques
for large scale computing. For example, Newton method is used in linear semi-supervised SVM in
[11,72]. Sparse grid is used in regularization in SSL [73]. However, the size of the grid increases
exponentially fast with dimensions.
One attractive solution for large scale computing is online method. For instance, stochastic gradient
descent is used to train TSVM in [74]. This type of algorithms allows training through linear scans
of the whole data set. A graph-regularized transductive learning method that is based on minimizing
a Kullback-Leibler divergence based loss is introduced in [75], and the algorithm can solve problems
with as much as 120 millon samples.
For many graph Laplacian-based SSL methods, the unique structure of the graph Laplacian matrix
is explored in multi-grid method in [76]. The algorithm run-time and storage are linear in the number
of graph edges. For sparse graphs, this is of the order of vertices, or data samples. Multiscale solvers
for several (un)weighted graphs can be found in [77], with the software implementation.
There are methods based on the graph Laplacian on a smaller backbone graph, which is chosen in
different ways from the whole data set. For example, random sampling is used in [78]. The problem
with the approaches based on backbone graphs is that the spectrum of the graph Laplacian can change
dramatically with different backbone construction methods, thus the solution is not stable.

1.22.4 Large Scale Semi-Supervised Learning
1261
−6
−4
−2
0
2
4
6
−6
−4
−2
0
2
4
6
−6
−4
−2
0
2
4
6
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
(b) Single coordinate histogram
(a) Two rings data
FIGURE 22.4
Two-ring data and its marginal distribution histogram.
Most of above methods try to attack various approximations to the ﬁnite sample problems. On
the contrary, starting from a well-studied inﬁnite sample limit, Fergus et al. [79] proposed an attrac-
tive scalable SSL algorithm that can learn on much larger data sets. The eigenvectors of the graph
Laplacian on data samples can be used as function basis to estimate functions on graphs. In the limit, the
Laplacian eigenfunctions are natural square integrable function basis on smooth manifolds. With
abundant unlabeled data, the graph Laplacian is close to its limit. Thus [79] starts from the limit,
Laplacian on a smooth manifold, and under certain assumptions, takes advantage of a nice property of
Laplacian: for product form densities, the Laplacian eigenfunctions on the marginal distribution of a
single coordinate is also an eigenfunction of the Laplacian on the whole domain with the same eigen-
value. Then all these single-dimensional eigenvectors can be easily computed using various histogram
and interpolation methods on a one-dimensional space.
As an example, a two-ring toy data set is plotted in Figure 22.4. The right panel is the marginal
histogram. Since the data set is symmetric, marginal distributions of both dimensions are the same.
Notice that this data set does not satisfy the “product form” assumption.
The single-dimensional graph Laplacian eigenvectors are shown in Figure 22.5. Notice that the signs
of these single-dimensional eigenvectors decompose the square domain in a way similar to the axis
parallel decision stumps. Several examples of such products are illustrated in Figure 22.6. In particular,
the product of these eigenvectors provide a more powerful function basis such as squares, which can be
obtained by the product of the third eigenvectors from the two dimensions (along x and y). This square
is enough to separate the two circles apart. Roughly speaking, the method is similar to decision stumps
in AdaBoost. However, there are several differences. First, the products of eigenvectors provide a much
richer basis than stumps alone. Second, these eigenvectors are data dependent, and are deﬁned through
unlabeled data in general. Third, these basis are real-valued functions instead of step functions.

1262
CHAPTER 22 Semi-Supervised Learning
−6
−4
−2
0
2
4
6
−0.1
−0.05
0
0.05
0.1
0.15
−6
−4
−2
0
2
4
6
−0.1
−0.05
0
0.05
0.1
0.15
(b) The 3rd eigenvectorφ 3(x)
(a) The 2nd eigenvector   2 (x)
φ 
FIGURE 22.5
The second and third graph Laplacian eigenvectors of the marginal.
−6 −4 −2
0
2
4
6
−6
−4
−2
0
2
4
6
(a) sign(φ2 (x))
−6 −4 −2
0
2
4
6
−6
−4
−2
0
2
4
6
(b) sign(φ3 (x))
−6 −4 −2
0
2
4
6
−6
−4
−2
0
2
4
6
(c) sign(φ2 (x)φ2(y))
−6 −4 −2
0
2
4
6
−6
−4
−2
0
2
4
6
(d) sign(φ3 (x)φ3 (y))
−6 −4 −2
0
2
4
6
−6
−4
−2
0
2
4
6
(e) sign(φ2 (x)φ3 (y))
−6 −4 −2
0
2
4
6
−6
−4
−2
0
2
4
6
(f) sign(φ3 (x)φ2(y))
FIGURE 22.6
Sign (·) functions of the single-dimensional graph Laplacian eigenvectors and their products. Grey areas are
+1, while others are 0.

1.22.5 Theoretical Analysis Overview
1263
This approximation transforms a d-dimensional problem to d one-dimensional problems. It not only
eliminates the need to construct kNN graphs, which generally costs at least O(n2), but also removes
the matrix inversion or eigen-decomposition, which costs O(n3).
Finally, several related themes in large scale SSL include, (a) different approximation methods,
(b) large scale optimization techniques and online learning, e.g., [80], and (c) parallel computing, e.g.,
[81]. These areas also overlap with each other. For example, certain optimization techniques might
require certain type of approximation, which might also determines whether it is possible for parallel
computing.
1.22.5 Theoretical analysis overview
Some of the ﬁrst work on the theoretical analysis of SSL appeared in [1–3], where the authors analyze
parametric models based on the Gaussian mixture distributions. Error analysis of semi-supervised
classiﬁcation is further investigated in [82] under the more general cluster assumption. The main result
is consistent with that of [1,2]: unlabeled data reduce the error with a polynomial rate of the form
O(u−β) for some β > 0, while labeled may reduce the error exponentially fast.
Another result [83] shows that the pointwise mean squares error for a popular graph Laplacian
regularization method has the same leading term as a regular supervised kernel smoother, which implies
that the unlabeled data set does not help asymptotically.
On the other hand, Niyogi [84] studied SSL under the manifold assumption, and gives a constructive
example of a class of distributions supported on manifolds where the error of any supervised learner is
bounded from below by a constant, while a semi-supervised learner that can have error bounded from
above by O(n−1/2).
The value of unlabeled data is studied in a ﬁnite sample framework in [85], which evaluates the
performance gains with SSL under the cluster assumption using ﬁnite sample error bounds, which
involves ﬁnite labeled and unlabeled sets.
Inadifferentapproach,thevalueofunlabeleddatawasstudiedin[86]underthePACmodel.Thecom-
patibility between concept class and the distribution is introduced to link unlabeled data in the analysis.
1.22.6 Challenges
InthischapterwehavereviewedanumberofpopularSSLalgorithmsandvariousaspectsoftheiranalysis
and implementation. Here we point out some of the key challenges for research and applications of SSL:
Mode/parameter selection: This is a common issue for all SSL algorithms. Model selection methods
borrowed from supervised learning, such as cross-validation, may not work well in the SSL setting due
to the limited amount of labeled data. Furthermore, many SSL models introduce several parameters
which may be hard to choose appropriately for a new data domain.
Scalability: The premise of SSL is utilizing large amounts of unlabeled data to improve inference.
Thus scalability is one of the most important issues facing SSL and is a key to making these algorithms
widely used in various practical applications. While signiﬁcant amount of research has gone into making
SSL algorithms more scalable (see Section 1.22.4), there is a need for more work is that direction.

1264
CHAPTER 22 Semi-Supervised Learning
Theoretical analysis: The asymptotic analysis for SSL can shed light on various SSL algorithms
and guide their applications. However, existing analyses are far from being satisfactory. There are
two important questions for any SSL algorithm. First, what the estimators are. This is the problem of
consistency, including how to deﬁne consistency. Second, how fast an empirical estimator converges to
its limit, including how to deﬁne convergence. The rate should involve both l and u. Without consistency,
no matter how fast the convergence rates are, it does not provide any guarantee. Most existing analyses
focus on the large sample asymptotic analysis, which may not be sufﬁcient for explaining the ﬁnite
sample behaviors of some SSL algorithms.
With the recent advances in semi-supervised learning and as future developments addressing the
challenges above come along, we expect that semi-supervised learning algorithms will become a key
part in the toolbox of methods for large scale high-dimensional data analysis.
Glossary
Graph Laplacian
also called the Laplacian matrix, is a matrix representation of a
graph
Independent and identically
distributed
i.i.d.
Laplacian
also called the Laplace operator, a differential operator given by the
divergence of the gradient of a function on Euclidean space
Multiresolution analysis
(MRA)
data analysis methods by constructing orthonormal bases at differ-
ent scale
Regularization
any method of preventing overﬁtting of data by a model. Regular-
ization involves introducing additional information in order to solve
an ill-posed problem or to prevent overﬁtting
Semi-supervised learning
(SSL)
a class of machine learning techniques that make use of both labeled
and unlabeled data for training—typically a small amount of labeled
data with a large amount of unlabeled data
Relevant Websites
SVMlin: http://vikas.sindhwani.org/svmlin.html
SVM-light: http://svmlight.joachims.org/
Low Density Separation: http://olivier.chapelle.cc/lds/
Collection “Semi-supervised Learning”: http://olivier.chapelle.cc/ssl-book/
Semi-Supervised Learning Literature Survey: http://pages.cs.wisc.edu/jerryzhu/
Relevant Theory: Signal Processing Theory and Machine Learning
See this Volume, Chapter 6 Digital Filter Structures and Their Implementation
See this Volume, Chapter 16 Kernel Methods and SVMs
See this Volume, Chapter 20 Clustering

References
1265
References
[1] Vittorio Castelli, Thomas M. Cover, On the exponential value of labeled samples, Pattern Recogn. Lett. 16
(0) (1995) 105–111.
[2] Vittorio Castelli, Thomas M. Cover, The relative value of labeled and unlabeled samples in pattern recognition
with an unknown mixing parameter, IEEE Trans. Inf. Theory 42 (6) (1996) 2102–2117.
[3] Joel Ratsaby, Santosh S. Venkatesh, Learning from a mixture of labeled and unlabeled examples with para-
metric side information, in: Eighth Annual Conference on Computational Learning Theory (COLT95), 1995,
pp. 412–417.
[4] V. Vapnik, Statistical Learning Theory, Wiley-Interscience Press, New York, 1998.
[5] A. Blum, T. Mitchell, Combining labeled and unlabeled data with co-training, in: COLT: Proceedings of the
Workshop on Computational Learning Theory, 1998, pp. 92–100.
[6] O. Chapelle, B. Schölkopf, A. Zien (Eds.), Semi-Supervised Learning, MIT Press, Cambridge, MA, 2006.
<http://www.kyb.tuebingen.mpg.de/ssl-book>.
[7] M. Seeger, Learning with labeled and unlabeled data, Technical Report, University of Edinburgh, 2001.
[8] X. Zhu, Semi-supervised learning literature survey, Technical Report, University of Wisconsin, Madison,
2008.
[9] Thorsten Joachims, Transductive inference for text classiﬁcation using support vector machines, in: 16th
International Conference on Machine Learning (ICML), vol. 15, 1999, pp. 200–209.
[10] M. Belkin, P. Niyogi, S. Sindhwani, Manifold regularization: a geometric framework for learning from labeled
and unlabeled examples, J. Mach. Learn. Res. 7 (2006) 2399–2434.
[11] Vikas Sindhwani, Sathiya S. Keerthi, Large scale semi-supervised linear SVMs, in: 29th Annual International
ACM SIGIR, 2006, pp. 477–484.
[12] O. Chapelle, A. Zien, Semi-supervised classiﬁcation by low density separation, in: Proceedings of the 10th
International Workshop on Artiﬁcial Intelligence and Statistics, 2005, pp. 57–64.
[13] Ronan Collobert, Fabian Sinz, Jason Weston, Léon Bottou, Large scale transductive SVMs, J. Mach. Learn.
Res. 7 (2006) 1687–1712.
[14] Olivier Chapelle, Vikas Sindhwani, Sathiya S. Keerthi, Optimization techniques for semi-supervised support
vector machines, J. Mach. Learn. Res. 9 (2008) 203–233.
[15] Olivier Chapelle, Vikas Sindhwani, S. Sathiya Keerthi, Branch and bound for semi-supervised support vector
machines, in: B. Schölkopf, J. Platt, T. Hoffman (Eds.), Advances in Neural Information Processing Systems,
vol. 19, MIT Press, Cambridge, MA, 2007, pp. 217–224.
[16] Balaji Krishnapuram, Shipeng Yu, R. Bharat Rao (Eds.), Cost-Sensitive Machine Learning, CRC Press, 2011.
[17] David G. Lowe, Object recognition from local scale-invariant features, in: Proceedings of the International
Conference on Computer Vision, 1999, pp. 1150–1157.
[18] AudeOliva,AntonioTorralba,Modelingtheshapeofthescene:aholisticrepresentationofthespatialenvelope,
Int. J. Comput. Vis. 42 (3) (2001) 145–175.
[19] Michael Collins, Yoram Singer, Unsupervised models for named entity classiﬁcation, in: 1999 Joint SIGDAT
Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, 1999, pp. 100–
110.
[20] Sham M. Kakade, Dean P. Foster, Multi-view regression via canonical correlation analysis, in: Conference on
Computational Learning Theory (COLT), 2007, pp. 82–96.
[21] C. Mario Christoudias, Raquel Urtasun, Trevor Darrell, Multi-view learning in the presence of view disagree-
ment, in: 24th Conference on Uncertainty in Artiﬁcial Intelligence (UAI 2008), 2008.

1266
CHAPTER 22 Semi-Supervised Learning
[22] V. Sindhwani, P. Niyogi, M. Belkin, A co-regularization approach to semi-supervised learning with multi-
ple views, in: Workshop on Learning with Multiple Views, International Conference on Machine Learning
(ICML), 2005.
[23] V. Sindhwani, D. Rosenberg, An RKHS for multi-view learning and manifold co-regularization, in: Interna-
tional Conference on Machine Learning (ICML), 2008.
[24] A.P. Dempster, N.M. Laird, D.B. Rubin, Maximum likelihood from incomplete data via the EM algorithm, J.
Roy. Statist. Soc. Ser. B (Methodol.) 39 (7) (1977) 1–38.
[25] Kamal Nigam, Andrew Kachites Mccallum, Sebastian Thrun, Tom Mitchell, Text classiﬁcation from labeled
and unlabeled documents using EM, Mach. Learn. 39 (2–3) (2000) 103–134.
[26] Jeff A. Bilmes, A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian
mixture and hidden Markov models, Technical Report, University of Berkeley, 1998.
[27] D. Yarowsky, Unsupervised word sense disambiguation rivaling supervised methods, in: Proceedings of the
33rd Annual Meeting of the Association for Computational Linguistics, 1995, pp. 189–196.
[28] M. Meila, J. Shi, Learning segmentation by random walks, in: Neural Information Processing Systems, vol.
13, 2000, pp. 873–879.
[29] Marina Meila, Jianbo Shi, A random walks view of spectral segmentation, in: AI and STATISTICS (AISTATS),
2001.
[30] M. Szummer, T. Jaakkola, Partially labeled classiﬁcation with Markov random walks, in: Neural Information
Processing Systems (NIPS), vol. 14, 2001.
[31] Avrim Blum, Shuchi Chawla, Learning from labeled and unlabeled data using graph mincuts, in: 18th Inter-
national Conference on Machine Learning, 2001, pp. 19–26.
[32] Xiaojin Zhu, Zoubin Ghahramani, Learning from labeled and unlabeled data with label propagation, Technical
Report, CMU-CALD-02-107, Carnegie Mellon University, 2002.
[33] M. Belkin, P. Niyogi, Semi-supervised learning on Riemannian manifolds, Mach. Learn. 56 (2004) 209–239
(special issue on Clustering).
[34] X. Zhu, J. Lafferty, Z. Ghahramani, Semi-supervised learning using Gaussian ﬁelds and harmonic function,
in: 20th International Conference on Machine Learning, 2003.
[35] A.J. Smola, R. Kondor, Kernels and regularization on graphs, in: 16th Annual Conference on Learning Theory
and 7th Kernel Workshop, 2003, pp. 144–158.
[36] Xueyuan Zhou, Mikhail Belkin, Semi-supervised learning by higher order regularization, in: Geoffrey Gordon,
David Dunson, Miroslav Dudík (Eds.), 14th International Conference on Artiﬁcial Intelligence and Statistics,
vol. 15, JMLR W&CP, 2011, pp. 892–900.
[37] Yoshua Bengio, Jean francois Paiement, Pascal Vincent, Olivier Delalleau, Nicolas Le Roux, Marie Ouimet,
Out-of-sample extensions for LLE, Isomap, MDS, Eigenmaps, and spectral clustering, in: Advances in Neural
Information Processing Systems, vol. 16, MIT Press, 2004, pp. 177–184.
[38] Fan R.K. Chung, Spectral graph theory, in: CBMS Regional Conference Series in Mathematics, No. 92, 1992.
[39] U. von Luxburg, A tutorial on spectral clustering, Statist. Comput. 17 (4) (2007) 395–416.
[40] Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, Bernhard Schölkopf, Learning with
local and global consistency, in: Sebastian Thrun, Lawrence Saul, Bernhard Schölkopf (Eds.), Advances in
Neural Information Processing Systems, vol. 16, MIT Press, Cambridge, MA, 2004, pp. 321–328.
[41] M. Belkin, I. Matveeva, P. Niyogi, Regularization and semi-supervised learning on large graphs, in: John
Shawe-Taylor, Yoram Singer (Eds.), Computational Learning Theory (COLT), Lecture Notes in Computer
Science, vol. 3120, Springer, 2004, pp. 624–638.
[42] Olivier Bousquet, Olivier Chapelle, Matthias Hein, Measure based regularization, in: Sebastian Thrun,
Lawrence Saul, Bernhard Schölkopf (Eds.), Advances in Neural Information Processing Systems, vol. 16,
MIT Press, Cambridge, MA, 2004.

References
1267
[43] Boaz Nadler, Nathan Srebro, Xueyuan Zhou, Statistical analysis of semi-supervised learning: the limit of
inﬁniteunlabelleddata,in:Y.Bengio,D.Schuurmans,J.Lafferty,C.K.I.Williams,A.Culotta(Eds.),Advances
in Neural Information Processing Systems, vol. 22, 2009, pp. 1330–1338.
[44] B. Daya Reddy, Introductory Function Analysis, with Applications to Boundary Value Problems and Finite
Elements, Springer, 1997.
[45] A. Berlinet, C. Thomas-Agnan, Reproducing Kernel Hilbert Spaces in Probability and Statistics, Kluwer
Academic Publishers, 2003.
[46] R.A. Adams, Sobolev Spaces, Academic Press, New York, 1975.
[47] Michael E. Taylor, Partial Differential Equations I: Basic Theory, Springer, New York, 1996.
[48] G. Wahba, Spline models for observational data, in: CBMS-NSF Regional Conference Series in Applied
Mathematics, vol. 59, SIAM, Philadelphia, 1990.
[49] Matthias Hein, Jean yves Audibert, Ulrike Von Luxburg, Graph Laplacians and their convergence on random
neighborhood graphs, J. Mach. Learn. Res. 8 (2007) 1325–1368.
[50] M. Belkin, P. Niyogi, Laplacian eigenmaps for dimensionality reduction and data representation, Neural
Comput. 15 (6) (2003) 1373–1396.
[51] Xueyuan Zhou, Nathan Srebro, Error analysis of Laplacian eigenmaps for semi-supervised learning, in: Geof-
frey Gordon, David Dunson, Miroslav Dudík (Eds.), 14th International Conference on Artiﬁcial Intelligence
and Statistics, vol. 15, JMLR W&CP, 2011, pp. 901–908.
[52] László Györﬁ, Michael Kohler, Adam Krzy˙zak, Harro Walk, A Distribution-Free Theory of Nonparametric
Regression, Springer, 2002.
[53] Peter J. Bickel, Bo Li, Local polynomial regression on unknown manifolds, Complex Datasets and Inverse
Problems: Tomography, Networks and Beyond, IMS Lecture Notes—Monograph Series, vol. 54, 2007, pp.
177–186.
[54] R.R. Coifman, S. Lafon, Diffusion maps, Appl. Comput. Harmon. Anal. 21 (1) (2006) 5–30.
[55] Xueyuan Zhou, Mikhail Belkin, Behavior of graph Laplacians on manifolds with boundary, 2011.
arXiv:1105.3931v1 [cs.LG].
[56] David K. Hammond, Pierre Vandergheynst, Rémi Gribonval, Wavelets on graphs via spectral graph theory,
Appl. Comput. Harmon. Anal. 30 (2) (2011) 129–150.
[57] R.R. Coifman, M. Maggioni, Diffusion wavelets, Appl. Comput. Harmon. Anal. 21 (1) (2006) 53–94.
[58] Matan Gavish, Boaz Nadler, Ronald R. Coifman, Multiscale wavelets on trees, graphs and high dimensional
data: theory and applications to semi supervised learning, in: International Conference on Machine Learning,
2010, pp. 367–374.
[59] Y. Xu, J.S. Dyer, A.B. Owen, Empirical stationary correlations for semi-supervised learning on graphs, Ann.
Appl. Statist. 4 (2) (2010) 589–614.
[60] John Lafferty, Xiaojin Zhu, Yan Liu, Kernel conditional random ﬁelds: representation and clique selection,
in: Proceedings of the 21st International Conference on Machine Learning, 2004.
[61] Ben Taskar, Carlos Guestrin, Daphne Koller, Max-margin Markov networks, in: Sebastian Thrun, Lawrence
Saul, Bernhard Schölkopf (Eds.), Advances in Neural Information Processing Systems, vol. 16, MIT Press,
Cambridge, MA, 2004, pp. 25–32.
[62] Yasemin Altun, David McAllester, Mikhail Belkin, Maximum margin semi-supervised learning for structured
variables, in: Y. Weiss, B. Schölkopf, J. Platt (Eds.), Advances in Neural Information Processing Systems,
vol. 18, MIT Press, Cambridge, MA, 2006, pp. 33–40.
[63] Ulf Brefeld, Tobias Scheffer, Semi-supervised learning for structured output variables, in: William Cohen,
Andrew Moore (Eds.), 23rd International Conference on Machine Learning (ICML), Omni Press, 2006, pp.
624–638.
[64] G. Bakir, T. Hofmann, B. Schölkopf, A. Smola, B. Taskar, S.V.N. Vishwanathan, Predicting Structured Data,
MIT Press, Cambridge, MA, 2007.

1268
CHAPTER 22 Semi-Supervised Learning
[65] Ulf Brefeld, Semi-Supervised Structured Prediction Models, PhD thesis, Humboldt-Universität zu Berlin,
2008.
[66] Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell Greiner, Dale Schuurmans, Semi-supervised condi-
tional random ﬁelds for improved sequence segmentation and labeling, in: Association for Computational
Linguistics (ACL), 2006, pp. 209–216.
[67] Amarnag Subramanya, Slav Petrov, Fernando Pereira, Efﬁcient graph-based semi-supervised learning
of structured tagging models, in: Proceedings of the 2010 Conference on Empirical Methods in Natural
Language Processing (EMNLP), 2010, pp. 167–176.
[68] Ben Taskar, Simon Lacoste-Julien, Michael Jordan, Structured prediction, dual extragradient and Bregman
projections, J. Mach. Learn. Res. 7 (2006) 1627–1653.
[69] I. Tsochantaridis, T. Joachims, T. Hofmann, Y. Altun, Large margin methods for structured and interdependent
output variables, J. Mach. Learn. Res. 6 (2005) 1453–1484.
[70] Chun-Nam John Yu, T. Joachims, Learning structural SVMs with latent variables, in: Andrea Pohoreckyj
Danyluk, Léon Bottou, Michael L. Littman (Eds.), 26th Annual International Conference on Machine
Learning (ICML), 2009, pp. 1169–1176.
[71] Jun Zhu, Eric P. Xing, Maximum entropy discrimination Markov networks, J. Mach. Learn. Res. 10 (2009)
2531–2569.
[72] V. Sindhwani, P. Niyogi, M. Belkin, SVMlin: fast linear SVM solvers for supervised and semi-supervised
learning, in: Workshop on Machine Learning Open Source Software, Neural Information Processing Systems
(NIPS), 2006.
[73] Jochen Garcke, Michael Griebel, Semi-supervised learning with sparse grids, in: 22nd ICML Workshop on
Learning with Partially Classiﬁed Training Data, 2005.
[74] Michael Karlen, Jason Weston, Ayse Erkan, Ronan Collobert, Large scale manifold transduction, in: Andrew
Mccallum, Sam Roweis (Eds.), 25th International Conference on Machine Learning (ICML 2008), 2008, pp.
448–455.
[75] Amarnag Subramanya, Jeff Bilmes, Entropic graph regularization in non-parametric semi-supervised classi-
ﬁcation, in: Y. Bengio, D. Schuurmans, J. Lafferty, C.K.I. Williams, A. Culotta (Eds.), Advances in Neural
Information Processing Systems, vol. 22, 2009, pp. 1803–1811.
[76] Oren E. Livne, Achi Brandt, Lean algebraic multigrid (LAMG): fast graph Laplacian linear solver, 2011.
arXiv:1108.1310v1 [math.NA].
[77] Ilya Safro, Dorit Ron, Achi Brandt, Multilevel algorithms for linear ordering problems, J. Exp. Algor. 13
(2009).
[78] Ameet Talwalkar, Sanjiv Kumar, Henry Rowley, Large-scale manifold learning, in: Computer Vision and
Pattern Recognition, 2008 (CVPR 2008), 2008, pp. 1–8.
[79] Rob Fergus, Yair Weiss, Antonio Torralba, Semi-supervised learning in gigantic image collections, in: Y.
Bengio, D. Schuurmans, J. Lafferty, C.K.I. Williams, A. Culotta (Eds.), Advances in Neural Information
Processing Systems, vol. 22, 2009, pp. 522–530.
[80] Andrew B. Goldberg, Ming Li, Xiaojin Zhu, Online manifold regularization: a new learning setting and
empirical study, in: 2008 European Conference on Machine Learning and Knowledge Discovery in Databases
(ECML08), 2008, pp. 393–407.
[81] Amol Ghoting, Rajasekar Krishnamurthy, Edwin Pednault, Berthold Reinwald, Vikas Sindhwani, Shirish
Tatikonda, Yuanyuan Tian, Shivakumar Vaithyanathan, SystemML: declarative machine learning on
MapReduce, in: IEEE International Conference on Data Engineering (ICDE), 2011, pp. 231–242.
[82] Philippe Rigollet, Generalization error bounds in semi-supervised classiﬁcation under the cluster assumption,
J. Mach. Learn. Res. 8 (2007) 1369–1392.

References
1269
[83] John Lafferty, Larry Wasserman, Statistical analysis of semi-supervised regression, in: J.C. Platt, D. Koller,
Y. Singer, S. Roweis (Eds.), Advances in Neural Information Processing Systems, vol. 20, MIT Press,
Cambridge, MA, 2008, pp. 801–808.
[84] Partha Niyogi, Manifold regularization and semi-supervised learning: some theoretical analyses, Technical
Report, Department of Computer Science, University of Chicago, 2008.
[85] Aarti Singh, Robert Nowak, Xiaojin Zhu, Unlabeled data: now it helps, now it doesn’t, in: D. Koller, D.
Schuurmans, Y. Bengio, L. Bottou (Eds.), Advances in Neural Information Processing Systems, vol. 21,
2009, pp. 1513–1520.
[86] Maria-Florina
Balcan,
Avrim
Blum,
A
PAC-style
model
for
learning
from
labeled
and
unla-
beled
data,
in:
18th
Annual
Conference
on
Computational
Learning
Theory
(COLT),
2005,
pp. 111–126.

23
CHAPTER
Sparsity-Aware Learning and
Compressed Sensing:
An Overview⋆
Sergios Theodoridis*, Yannis Kopsinis*, and Konstantinos Slavakis†
*Department of Informatics and Telecommunication, University of Athens, Athens, Greece
†Digital Technology Center, University of Minnesota, Minneapolis, USA
1.23.1 Introduction
The notion of regularization has been widely used as a tool to address a number of problems that are
usually encountered in Machine Learning. Improving the performance of an estimator by shrinking the
norm of the Minimum Variance Unbiased (MVU) estimator, guarding against overﬁtting, coping with
ill-conditioning, providing a solution to an underdetermined set of equations are some notable examples
whereregularizationhasprovidedsuccessfulanswers.Anotableexampleistheridgeregressionconcept,
where the LS loss function is combined, in a tradeoff rationale, with the Euclidean norm of the desired
solution.
In this chapter, our interest will be on alternatives to the Euclidean norms and in particular the focus
will revolve around the ℓ1 norm; this is the sum of the absolute values of the components comprising
a vector. Although seeking a solution to a problem via the ℓ1 norm regularization of a loss function
has been known and used since the 1970s, it is only recently that has become the focus of attention of
a massive volume of research in the context of compressed sensing. At the heart of this problem lies
an underdetermined set of linear equations, which, in general, accepts an inﬁnite number of solutions.
However, in a number of cases, an extra piece of information is available: the true model, whose estimate
we want to obtain, is sparse; that is, only a few of its coordinates are nonzero. It turns out that a large
number of commonly used applications can be cast under such a scenario and can be beneﬁted by a
so-called sparse modeling.
Besides its practical signiﬁcance, sparsity-aware processing has offered to the scientiﬁc community
novel theoretical tools and solutions to problems that only a few years ago seemed to be intractable.
This is also a reason that this is an interdisciplinary ﬁeld of research encompassing scientists from,
e.g., mathematics, statistics, machine learning, signal processing. Moreover, it has already been applied
in many areas ranging from biomedicine, to communications and astronomy. At the time this chapter
is compiled, there is a “research happening” in this ﬁeld, which poses some difﬁculties in assembling
related material together. We have made an effort to put together, in a unifying way, the basic notions and
ideas that run across this new ﬁeld. Our goal is to provide the reader with an overview of the major con-
tributions which took place in the theoretical and algorithmic fronts and have been consolidated over the
⋆This paper is based on a chapter of a new book on Machine Learning, by the ﬁrst and third authors, which is currently under
preparation.
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00023-1
© 2014 Elsevier Ltd. All rights reserved.
1271

1272
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
last decade or so. Besides the methods and algorithms which are reviewed in this article, there is another
path of methods based on the Bayesian learning rationale. Such techniques will be reviewed elsewhere.
1.23.2 Parameter estimation
Parameter estimation is at the heart of what is known as Machine Learning; a term that is used more
and more as an umbrella for a number of scientiﬁc topics that have evolved over the years within
different communities, such as Signal Processing, Statistical Learning, Estimation/Detection, Control,
Neurosciences, Statistical Physics, to name but a few.
In its more general and formal setting, the parameter estimation task is deﬁned as follows. Given
a set of data points (yn, xn), yn ∈R, xn ∈Rl, n = 1, 2, . . . , N, known as the training data, and a
parametric set of functions
F := { fθ, θ ∈A ⊆Rk},
ﬁnd a function in F, which will be denoted as f (·) := fθ∗(·), such that given the value of x ∈Rl, f (x)
best approximates the corresponding value of y ∈R. After all, the main goal of Machine Learning is
prediction. In a more general setting, y can also be a vector y ∈Rm. Most of our discussion here will
be limited to real valued variables. Obviously, extensions to complex valued data are readily available.
Having adopted the parametric set of functions and given the training data set, the goal becomes
that of estimating the values of the parameters θ so that to “ﬁt” the data in some (optimal) way. There
are various paths to achieve this goal. In this chapter, our approach comprises the adoption of a loss
function
L(·, ·) : R × R −→[0, ∞),
and obtain θ∗such that
θ∗:= arg minθ J(θ),
where
J(θ) :=
N

n=1
L(yn, fθ(x)).
(23.1)
Our focus will be on the Least Squares loss function, i.e.,
L(y, fθ(x)) := (y −fθ(x))2.
Among the many parametric models, regression covers a large class of Machine Learning tasks.
In linear regression, one models the relationship of a dependent variable y, which is considered as
the output of a system, with a set of independent variables, x1, x2, . . . , xl, which are thought as the
respective inputs that activate the system in the presence of a noise (unobserved) disturbance, η, i.e.,
y = θ1x1 + · · · + θlxl + θ0 + η,
where θ0 is known as the bias or intercept, see Figure 23.1. Very often the previous input-output
relationship is written as
y = xT θ + η,
(23.2)

1.23.2 Parameter Estimation
1273
x =[x ,x ,...,xl]
T
System
y
1
2
FIGURE 23.1
Block diagram showing the input-output relation in a regression model.
where
θ := [θ1, . . . , θ0]T ,
and x := [x1, . . . , xl, 1]T .
(23.3)
Often, x is called the regressor. Given the set of training data points, (yn, xn), n = 1, 2, . . . , N,
(23.2) can compactly written as
y = Xθ + η,
(23.4)
where
X :=
⎡
⎢⎣
xT
1...
xT
N
⎤
⎥⎦, y =
⎡
⎢⎣
y1
...
yN
⎤
⎥⎦, η =
⎡
⎢⎣
η1
...
ηN
⎤
⎥⎦.
(23.5)
For such a model, the Least Squares cost function is written as
J(θ) =
N

n=1
(yn −θ T xn)2 = ∥y −Xθ∥2,
(23.6)
where ∥·∥denotes the Euclidean norm. Minimizing (23.6) with respect to θ results to the celebrated LS
estimate
ˆθ LS = (X T X)−1X T y,
(23.7)
assuming the matrix inversion is possible. However, for many practical cases, the cost function in (23.6)
is augmented with a so called regularization term. There are a number of reasons that justify the use of
a regularization term. Guarding against overﬁtting, purposely introducing bias in the estimator in order
to improve the overall performance, dealing with the ill conditioning of the task are cases which the use
of regularization has successfully addressed. Ridge regression is a celebrated example, where the cost
function is augmented as
L(θλ) = ∥y −Xθ∥2 + λ∥θ∥2,
λ ≥0
leading to the estimate
ˆθ R = (X T X + λI)−1X T y,
where I is the identity matrix. Note that the ﬁrst term in a regularized cost function measures the model
misﬁt and the second one the “size” of the model.
The major goal of this review chapter is to focus at alternative norms in place of the Euclidean norm,
which was employed in ridge regression. As we will see, there are many good reasons in doing that.

1274
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
1.23.3 Searching for a norm
Mathematicians have been very imaginative in proposing various norms in order to equip linear spaces.
Among the most popular norms used in functional analysis are the so-called ℓp norms. To tailor things
to our needs, given a vector θ ∈Rl, its ℓp norm is deﬁned as
∥θ∥p :=
	 l
i=1
|θi|p

 1
p
.
(23.8)
For p = 2, the Euclidean or ℓ2 norm is obtained, and for p = 1, (23.8) results in the ℓ1 norm, i.e.,
∥θ∥1 =
l
i=1
|θi|.
(23.9)
If we let p →∞, then we get the ℓ∞norm; let |θimax| := max {|θ1|, |θ2|, . . . , |θl|}, and notice that
∥θ∥∞:=
lim
p→∞
	
|θimax|p
l
i=1
 |θi|
|θimax|
p
 1
p
= |θimax|,
(23.10)
that is, ∥θ∥∞is equal to the maximum of the absolute values of the coordinates of θ. One can show that
all the ℓp norms are true norms for p ≥1; they satisfy all four requirements that a function Rl →[0, ∞)
must respect in order to be called a norm, i.e.,
1. ∥θ∥p ≥0.
2. ∥θ∥p = 0 ⇔θ = 0.
3. ∥αθ∥p = |α| ∥θ∥p , ∀α ∈R.
4. ∥θ1 + θ2∥p ≤∥θ1∥p + ∥θ2∥p.
The third condition enforces the norm function to be (positively) homogeneous and the fourth one is
the triangle inequality. These properties also guarantee that any function that is a norm is also a convex
one. Although strictly speaking, if we allow p > 0 to take values less than one in (23.8), the resulting
function is easily shown not to be a true norm, we can still call them norms, albeit knowing that this is
an abuse of the deﬁnition of a norm. An interesting case, which will be used extensively in this paper,
is the ℓ0 norm, which can be obtained as the limit, for p →0, of
∥θ∥0 := lim
p→0 ∥θ∥p
p = lim
p→0
l
i=1
|θi|p =
l
i=1
χ(0,∞)(|θi|),
(23.11)
where χA(·) is the characteristic function with respect to a set A, deﬁned as
χA(τ) :=
1,
if τ ∈A,
0,
if τ /∈A.
That is, the ℓ0 norm is equal to the number of nonzero components of the respective vector. It is very easy
to check that this function is not a true norm. Indeed, this function is not homogeneous, i.e., ∥αθ∥0 ̸=
|α| ∥θ∥0 , ∀α ̸= 1. Figure 23.2 shows the isovalue curves, in the two-dimensional space, that correspond

1.23.3 Searching for a Norm
1275
FIGURE 23.2
The isovalue curves for ∥θ∥p = 1 and for various values of p, in the two dimensional space. Observe that for
the ℓ0 norm, the respective values cover the two axes with the exception of the point (0, 0). For the ℓ1 norm
the isovalue curve is a rhombus and for the ℓ2 (Euclidean) norm, it is a circle.
FIGURE 23.3
Observe that the epigraph, that is, the region above the graph, is nonconvex for values p < 1, indicating
the nonconvexity of the respective |·|p function. The value p = 1 is the smallest one for which convexity is
retained. Also note that, for large values of p > 1, the contribution of small values of θ to the respective
norm becomes insigniﬁcant.
to ∥θ∥p = ρ ≡1, for p = 0, 0.5, 1, 2, and ∞. Observe that for the Euclidean norm the isovalue curve
has the shape of a “ball” and for the ℓ1 norm the shape of a rhombus. We refer to them as the ℓ2 and the ℓ1
balls, respectively, by slightly “abusing” the meaning of a ball.1 Observe that in the case of the ℓ0 norm,
the isovalue curve comprises both the horizontal and the vertical axes, excluding the (0, 0) element. If we
restrict the size of the ℓ0 norm to be less than one, then the corresponding set of points becomes a single-
ton, i.e., (0, 0). Also, the set of all the two-dimensional points that have ℓ0 norm less than or equal to two,
is the R2 space. This, slightly “strange” behavior, is a consequence of the discrete nature of this “norm.”
Figure 23.3 shows the graph of |·|p, which is the individual contribution of each component of a
vector to its ℓp norm, for different values of p. Observe that: (a) for p < 1, the region which is formed
1Strictly speaking, a ball must also contain all the points in the interior.

1276
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
above the graph (known as epigraph) is not a convex one, which veriﬁes what we have already said;
i.e., the respective function is not a true norm, (b) for values of the argument |θ| > 1, the larger the
value of p ≥1 and the larger the value of |θ| the higher its respective contribution to the norm. Hence,
if ℓp norms, p ≥1, are used to regularize a loss function, such large values become the dominant ones
and the optimization algorithm will concentrate on these by penalizing them to get smaller, so that the
overall cost to be reduced. On the other hand, for values of the argument |θ| < 1 and closer to zero, the
ℓ1 norm is the only one (among p ≥1) that retains relatively large values and, hence, the respective
components can still have a say in the optimization process and can be penalized by being pushed to
smaller values. Hence, if the ℓ1 norm is used to replace the ℓ2 one in the regularization equation, only
those components of the vector, that are really signiﬁcant in reducing the model misﬁt measuring term
in the regularized cost function, will be kept and the rest will be forced to zero. The same tendency, yet
more aggressive, is true for 0 ≤p < 1. The extreme case is when one considers the ℓ0 norm. Even a
small increase of a component from zero, makes its contribution to the norm large, so the optimizing
algorithm has to be very “cautious” in making an element nonzero.
From all the true norms (p ≥1), the ℓ1 is the only one that shows respect to small values. The rest
of the ℓp norms, p > 1, just squeeze them, to make their values even smaller and care, mainly, for the
large values. We will return to this point very soon.
1.23.4 The least absolute shrinkage and selection operator (LASSO)
We have already discussed some of the beneﬁts in adopting the regularization method for enhancing the
performance of an estimator. However, in this paper, we are going to see and study more reasons that
justify the use of regularization. The ﬁrst one refers to what is known as the interpretation power of an
estimator. For example, in the regression task, we want to select those components, θi, of θ that have
the most important say in the formation of the output variable. This is very important if the number of
parameters, l, is large and we want to concentrate on the most important of them. In a classiﬁcation task
[1], not all features are informative, hence one would like to keep the most informative of them and make
the less informative ones equal to zero. Another related problem refers to those cases where we know, a
priori, that a number of the components of a parameter vector are zero but we do not know which ones.
The discussion we had at the end of the previous section starts now to become more meaningful. Can
we use, while regularizing, an appropriate norm that can assist the optimization process (a) in unveiling
such zeros or (b) to put more emphasis on the most signiﬁcant of its components, those that play a
decisive role in reducing the misﬁt measuring term in the regularized cost function, and set the rest
of them equal to zero? Although the ℓp norms, with p < 1, seem to be the natural choice for such a
regularization, the fact that they are not convex makes the optimization process hard. The ℓ1 norm is
the one that is “closest” to them yet it retains the computationally attractive property of convexity.
The ℓ1 norm has been used for such problems for a long time. In the seventies, it was used in
seismology [2,3], where the reﬂected signal, that indicates changes in the various earth substrates, is
a sparse one, i.e., very few values are relatively large and the rest are small and insigniﬁcant. Since
then, it has been used to tackle similar problems in different applications, e.g., [4,5]. However, one can
trace two papers that were really catalytic in providing the spark for the current strong interest around
the ℓ1 norm. One came from statistics, [6], which addressed the LASSO task (ﬁrst formulated, to our

1.23.4 The Least Absolute Shrinkage and Selection Operator (LASSO)
1277
knowledge, in [5]), to be discussed next, and the other from the signal analysis community, [7], which
formulated the Basis Pursuit, to be discussed in a later section.
We ﬁrst address our familiar regression task
y = Xθ + η,
y, η ∈RN,
θ ∈Rl,
and obtain the estimate of the unknown parameter θ via the LS loss, regularized by the ℓ1 norm, i.e.,
for λ ≥0,
ˆθ := arg minθ∈Rl L(θ, λ)
(23.12)
:= arg minθ∈Rl
	 N

n=1
(yn −xT
n θ)2 + λ ∥θ∥1

= arg minθ∈Rl

(y −Xθ)T (y −Xθ) + λ ∥θ∥1

.
(23.13)
In order to simplify the analysis, we will assume hereafter, without harming generality, that the data are
centered. If this is not the case, the data can be centered by subtracting the sample mean ¯y from each
one of the output values. The estimate of the bias term will be equal to the sample mean ¯y. The task in
(23.13) can be equivalently written in the following two formulations
ˆθ : min
θ∈Rl (y −Xθ)T (y −Xθ),
s.t. ∥θ∥1 ≤ρ,
(23.14)
or
ˆθ : min
θ∈Rl ∥θ∥1 ,
s.t. (y −Xθ)T (y −Xθ) ≤ϵ,
(23.15)
given the user-deﬁned parameters ρ, ϵ ≥0. The formulation in (23.14) is known as the LASSO and the
one in (23.15) as the Basis Pursuit Denoising (BPDN), e.g., [8]. All three formulations can be shown
to be equivalent for speciﬁc choices of λ, ϵ, and ρ. The minimized cost function in (23.13) corresponds
to the Lagrangian of the formulations in (23.14) and (23.15). However, this functional dependence is
hard to compute, unless the columns of X are mutually orthogonal. Moreover, this equivalence does
not necessarily imply that all three formulations are equally easy or difﬁcult to solve. As we will see
later on, algorithms have been developed along each one of the previous formulations. From now on,
we will refer to all three formulations as the LASSO task, in a slight abuse of the standard terminology,
and the speciﬁc formulation will be apparent from the context, if not stated explicitly.
As it was discussed before, the Ridge regression admits a closed form solution, i.e.,
ˆθ R =

X T X + λI
−1
X T y.
In contrast, this is not the case for LASSO and its solution requires iterative techniques. It is straight-
forward to see that LASSO can be formulated as a standard convex quadratic problem with linear

1278
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
inequalities. Indeed, we can rewrite (23.13) as
min
{θi,ui}l
i=1
(y −Xθ)T (y −Xθ) + λ
l
i=1
ui
s.t.
 −ui ≤θi ≤ui,
ui ≥0,
i = 1, 2, . . . ,l,
which can be solved by any standard convex optimization method, e.g., [9,10]. The reason that develop-
ing algorithms for the LASSO has been a hot research topic is due to the emphasis in obtaining efﬁcient
algorithms by exploiting the speciﬁc nature of this task, especially for cases where l is very large, as it
is often the case in practice.
In order to get a better insight of the nature of the solution that is obtained by LASSO, let us assume
that the regressors are mutually orthogonal and of unit norm, hence X T X = I. Orthogonality of the
input matrix helps to decouple the coordinates and results to l one-dimensional problems, that can be
solved analytically. For this case, the LS estimator becomes
ˆθLS = (X T X)−1X T y = X T y,
and the ridge regression gives
ˆθ R =
1
1 + λ
ˆθLS,
(23.16)
that is, every component of the LS estimator is simply shrunk by the same factor,
1
1+λ.
In the case of the ℓ1 regularization, the minimized Lagrangian function is no more differentiable,
due to the presence of the absolute values in the ℓ1 norm. So, in this case, we have to consider the notion
of the subdifferential (see Appendix). It is known that if the zero vector belongs to the subdifferential
set of a convex function at a point, this means that this point corresponds to a minimum of the function.
Taking the subdifferential of the Lagrangian deﬁned in (23.13) and recalling that the subdifferential set
of a differentiable function includes only the respective gradient, we obtain that
0 ∈−2X T y + 2X T Xθ + λ∂∥θ∥1 ,
where ∂stands for the subdifferential operator (see Appendix). If X has orthonormal columns, the
previous equation can be written component-wise as follows
0 ∈−ˆθLS,i + ˆθ1,i + λ
2∂
 ˆθ1,i
 ,
∀i,
(23.17)
where the subdifferential of the function |·|, derived in Appendix, is given as
∂|θ| =
⎧
⎨
⎩
{1},
if θ > 0,
{−1},
if θ < 0,

−1, 1

, if θ = 0.

1.23.4 The Least Absolute Shrinkage and Selection Operator (LASSO)
1279
Thus, we can now write for each component of the LASSO optimal estimate
ˆθ1,i =
⎧
⎪⎨
⎪⎩
ˆθLS,i −λ
2,
if ˆθ1,i > 0,
(23.18)
ˆθLS,i + λ
2,
if ˆθ1,i < 0.
(23.19)
Notice that (36.18) can only be true if ˆθLS,i > λ
2, and (36.19) only if ˆθLS,i < −λ
2. Moreover, in the
case where ˆθ1,i = 0, then (23.17) and the subdifferential of |·| suggest that necessarily
 ˆθLS,i
 ≤λ
2.
Concluding, we can write in a more compact way that
ˆθ1,i = sgn

ˆθLS,i
  ˆθLS,i
 −λ
2

+
,
(23.20)
where (·)+ denotes the “positive part” of the respective argument; it is equal to the argument if this
is non-negative, and zero otherwise. This is very interesting indeed. In contrast to the ridge regression
that shrinks all coordinates of the unregularized LS solution by the same factor, LASSO forces all
coordinates, whose absolute value is less than or equal to λ/2, to zero, and the rest of the coordinates are
reduced, in absolute value, by the same amount λ/2. This is known as soft thresholding, to distinguish it
from the hard thresholding operation; the latter is deﬁned as θ·χ(0,∞)

|θ| −λ
2

, θ ∈R, where χ(0,∞)(·)
stands for the characteristic function with respect to the set (0, ∞). Figure 23.4 shows the graphs
illustrating the effect that the ridge regression, LASSO and hard thresholding have on the unregularized
LS solution, as a function of its value (horizontal axis). Note that our discussion here, simpliﬁed via
the orthonormal input matrix case, has quantiﬁed what we had said before about the tendency of the ℓ1
norm to push small values to become exactly zero. This will be further strengthened, via a more rigorous
mathematical formulation, in Section 1.23.6.
Example 1. Assume that the unregularized LS solution, for a given regression task, y = Xθ + η, is
given by:
ˆθLS = [0.2, −0.7, 0.8, −0.1, 1.0]T .
FIGURE 23.4
Output-input curves for the hard thresholding, soft thresholding operators together with the linear operator
associated with the ridge regression, for the same value of λ = 1.

1280
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
Derive the solutions for the corresponding ridge regression and ℓ1 norm regularization tasks. Assume
that the input matrix X has orthonormal columns and that the regularization parameter is λ = 1. Also,
what is the result of hard thresholding the vector ˆθLS with threshold equal to 0.5?
We know that the corresponding solution for the ridge regression is
ˆθ R =
1
1 + λ
ˆθLS = [0.1, −0.35, 0.4, −0.05, 0.5]T .
The solution for the ℓ1 norm regularization is given by soft thresholding, with threshold equal to
λ/2 = 0.5, hence the corresponding vector is
ˆθ1 = [0, −0.2, 0.3, 0, 0.5]T .
The result of the hard thresholding operation is the vector [0, −0.7, 0.8, 0, 1.0]T .
Remarks 1.
•
The hard and soft thresholding rules are only two possibilities out of a larger number of alternatives.
Note that the hard thresholding operation is deﬁned via a discontinuous function and this makes this
rule to be unstable, in the sense of being very sensitive to small changes of the input. Moreover,
this shrinking rule tends to exhibit large variance in the resulting estimates. The soft thresholding
rule is a continuous function, but, as it is readily seen from the graph in Figure 23.4, it introduces
bias even for the large values of the input argument. In order to ameliorate such shortcomings, a
number of alternative thresholding operators have been introduced and studied both theoretically
and experimentally. Although these are not within the mainstream of our interest, we provide two
popular examples for the sake of completeness; the Smoothly Clipped Absolute Deviation (SCAD):
ˆθSCAD =
⎧
⎨
⎩
sgn(θ)

|θ| −λSCAD

+ , |θ| ≤2λSCAD,
(α−1)θ−αλSCAD sgn(θ)
α−2
,
2λSCAD < |θ| ≤αλSCAD,
θ,
|θ| > αλSCAD,
and the non-negative garrote thresholding rule :
ˆθgarr =

0,
|θ| ≤λgarr,
θ −
λ2
garr
θ , |θ| > λgarr.
Figure 23.5 shows the respective graphs. Observe that, in both cases, an effort has been made to
remove the discontinuity (associated with the hard thresholding) and to remove/reduce the bias for
large values of the input argument. The parameter α is a user-deﬁned one. For a more detailed
discussion on this topic, the interested reader can refer, for example, to [11].
1.23.5 Sparse signal representation
In the previous section, we brought into our discussion the need for taking special care for zeros.
Sparsity is an attribute that is met in a plethora of natural signals, since nature tends to be parsimonious.
In this section, we will brieﬂy present a number of application cases, where the existence of zeros in a

1.23.5 Sparse Signal Representation
1281
−
−
FIGURE 23.5
Output-input graph for the SCAD and nonnegative garotte rules with parameters α = 3.7, and λSCAD =
λgarr = 1. Observe that both rules smooth out the discontinuity associated with the hard thresholding rule.
Notice, also, that the SCAD rule removes the bias, associated with the soft thresholding rule, for large values
of the input variable. On the contrary, the garrote thresholding rule allows some bias for large input values,
which diminishes as λgarr gets smaller and smaller.
FIGURE 23.6
The impulse response function of an echo-path in a telephone network. Observe that although it is of relatively
short duration, it is not a priori known where exactly in time will occur.
mathematical expansion is of paramount importance, hence it justiﬁes to further strengthen our search
for and developing related analysis tools.
Echo cancelation is a major task in Communications. In a number of cases, the echo path, represented
by a vector comprising the values of the impulse response samples, is a sparse one. This is the case, for
example, in internet telephony and in acoustic and network environments, e.g., [12–14]. Figure 23.6
shows the impulse response of such an echo path. The impulse response of the echo path is of short
duration; however, the delay with which it appears is not known. So, in order to model it, one has to

1282
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
use a long impulse response, yet only a relatively small number of the coefﬁcients will be signiﬁcant
and the rest will be close to zero. Of course, one could ask why not use an LMS or an RLS [15,16] and
eventually the signiﬁcant coefﬁcients will be identiﬁed. The answer is that this turns out not to be the
most efﬁcient way to tackle such problems, since the convergence of the algorithm can be very slow. In
contrast, if one embeds, somehow, into the problem the a priori information concerning the existence
of (almost) zero coefﬁcients, then the convergence speed can be signiﬁcantly increased and also better
error ﬂoors can be attained.
A similar situation, as in the previous case, occurs in wireless communication systems, which involve
multipath channels. A typical application is in high deﬁnition television (HDTV) systems, where the
involved communications channels consist of a few non-negligible echoes, some of which may have
quite large time delays with respect to the main signal, see, e.g., [17–20]. If the information signal is
transmitted at high symbol rates through such a dispersive channel, then the introduced intersymbol
interference (ISI) has a span of several tens up to hundreds of symbol intervals. This in turn implies
that quite long channel estimators are required at the receiver’s end in order to reduce effectively the
ISI component of the received signal, although only a small part of it has values substantially different
to zero. The situation is even more demanding whenever the channel frequency response exhibits deep
nulls. More recently, sparsity has been exploited in channel estimation for multicarrier systems, both
for single antenna as well as for MIMO systems [21,22]. A thorough and in depth treatment related to
sparsity in multipath communication systems is provided in [23].
Another example, which might be more widely known, is that of signal compression. It turns out that
if the signal modalities, with which we communicate, e.g., speech, and also we sense the world, e.g.,
images, audio, are transformed into a suitably chosen domain then they are sparsely represented; only a
relatively small number of the signal components in this domain are large and the rest are close to zero.
As an example, Figure 23.7a shows an image and Figure 23.7b the plot of the magnitude of the obtained
Discrete Cosine Transform (DCT) components, which are computed by writing the corresponding image
array as a vector in lexicographic order. Note that more than 95% of the total energy is contributed by
only the 5% of the largest components. This is at the heart of any compression technique. Only the
large coefﬁcients are chosen to be coded and the rest are considered to be zero. Hence, signiﬁcant gains
are obtained in memory/bandwidth requirements while storing/transmitting such signals, without much
perceptual loss. Depending on the modality, different transforms are used. For example, in JPEG-2000,
an image array, represented in terms of a vector that contains the intensity of the gray levels of the image
pixels, is transformed via the discrete wavelet transform (DWT) and results to a transform vector that
comprises only a few large components. Such an operation is of the form
S = Hs,
s, S ∈Cl,
(23.21)
where s is the vector of the “raw” signal samples, S the (complex-valued) vector of the transformed ones,
and  is the l × l transformation matrix. Often, this is an orthonormal matrix, H = I. Basically, a
transform is nothing else than a projection of a vector on a new set of coordinate axes, which comprise
the columns of the transformation matrix . Celebrated examples of such transforms are the wavelet,
the discrete Fourier (DFT) and the discrete cosine (DCT) transforms, e.g., [1]. In such cases, where the
transformation matrix is orthonormal, one can write that
s = S,
(23.22)
where  = . Eq. (23.21) is known as the analysis and (23.22) as the synthesis equation.

1.23.5 Sparse Signal Representation
1283
5
0
−50
1
5
2
3
x10
(a)
(b)
FIGURE 23.7
(a) A 512×512 pixel image and (b) The magnitude of its Discrete Cosine Transform components in descend-
ing order and logarithmic scale. Note that more than 95% of the total energy is contributed by only the 5%
of the largest components.
Compression via such transforms exploit the fact that many signals in nature, which are rich in
context, can be compactly represented in an appropriately chosen basis, depending on the modality of
the signal. Very often, the construction of such bases tries to “imitate” the sensory systems that the
human (and not only) brain has developed in order to sense these signals; and we know that nature (in
contrast to modern humans) does not like to waste resources. A standard compression task comprises the
following stages: (a) Obtain the l components of S, via the analysis step (23.21), (b) keep the, say, k most
signiﬁcant of them, (c) code these values, as well as their respective locations in the transform vector S,
and (d) obtain the (approximate) original signal s, when needed (after storage or transmission), via the
synthesis Eq. (23.22), where in place of S only its k most signiﬁcant components are used, which are the
ones that were coded, while the rest are set equal to zero. However, there is something unorthodox in this
process of compression, as it has been practised till very recently. One processes (transforms) large signal
vectors of l coordinates, where l in practice can be quite large, and then uses only a small percentage
of the transformed coefﬁcients and the rest are simply ignored. Moreover, one has to store/transmit the
location of the respective large coefﬁcients that were ﬁnally coded. A natural question that is now raised
is the following: Since S in the synthesis equation is (approximately) sparse, can one compute it via
an alternative path than the analysis equation in (23.21)? The issue here is to investigate whether one
could use a more informative way of obtaining measurements from the available raw data, so that less
than l measurements are sufﬁcient to recover all the necessary information. The ideal case would be to
be able to recover it via a set of k such measurement samples, since this is the number of the signiﬁcant
free parameters. On the other hand, if this sounds a bit extreme, can one obtain N (k < N << l) such
signal-related measurements, from which one can obtain the k needed components of S? It turns out
that such an approach is possible and it leads to the solution of an underdetermined system of linear

1284
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
equations, under the constraint that the unknown target vector is a sparse one. The importance of such
techniques becomes even more apparent when, instead of an orthonormal basis, as discussed before, a
more general type of expansion is adopted, in terms of what is known as overcomplete dictionaries.
A dictionary [24] is a collection of parameterized waveforms, which are discrete-time signal samples,
represented as vectors ψi ∈Cl, i ∈I. For example, the columns of a DFT or a DWT matrix comprise
a dictionary. These are two examples of what is known as complete dictionaries, which consist of l
(orthonormal) vectors, i.e., a number equal to the length of the signal vector. However, in many cases
in practice, using such dictionaries is very restrictive. Let us take, for example, a segment of audio
signal, from a news media or a video, that needs to be processed. This consists, in general, of different
types of signals, namely speech, music, environmental sounds. For each type of these signals, different
signal vectors may be more appropriate in the expansion for the analysis. For example, music signals are
characterized by a strong harmonic content and the use of sinusoids seems to be best for compression,
while for speech signals a Gabor type signal expansion (sinusoids of various frequencies weighted by
sufﬁciently narrow pulses at different locations in time, [1,25]), may be a better choice. The same applies
when one deals with an image. Different parts of an image, e.g., parts which are smooth or contain sharp
edges,maydemandadifferentexpansionvectorset,forobtainingthebestoverallperformance.Themore
recent tendency, in order to satisfy such needs, is to use overcomplete dictionaries. Such dictionaries
can be obtained, for example, by concatenating different dictionaries together, e.g., a DFT and a DWT
matrix to result in a combined l × 2l transformation matrix. Alternatively, a dictionary can be “trained”
in order to effectively represent a set of available signal exemplars, a task which is often referred to as
dictionary learning [26–29]. While using such overcomplete dictionaries, the synthesis equation takes
the form
s =

i∈I
θiψi.
(23.23)
Note that, now, the analysis is an ill-posed problem, since the elements {ψi}i∈I (usually called atoms)
of the dictionary are not linearly independent, and there is not a unique set of coefﬁcients {θi}i∈I which
generates s. Moreover, we expect most of these coefﬁcients to be (nearly) zero. Note that, in such cases,
the cardinality of I is larger than l. This necessarily leads to underdetermined systems of equations with
inﬁnite many solutions. The question that is now raised is whether we can exploit the fact that most of
these coefﬁcients are known to be zero, in order to come up with a unique solution, and if yes, under
which conditions such a solution is possible?
Besides the previous examples, there is a number of cases where an underdetermined system of
equations is the result of our inability to obtain a sufﬁciently large number of measurements, due to
physical and technical constraints. This is for example the case in MRI imaging, which will be presented
in more detail later on.
1.23.6 In quest for the sparsest solution
Inspired by the discussion in the previous section, we now turn our attention to the task of solving
underdetermined systems of equations, by imposing the sparsity constraint on the solution [30]. We
will develop the theoretical set up in the context of the regression task and we will adhere to the notation
that has been adopted for this task. Moreover, we will focus on the real-valued data case, in order to

1.23.6 In Quest for the Sparsest Solution
1285
simplify the presentation. The theory can be readily extended to the more general complex data case,
see, e.g., [31,32]. We assume that we are given a set of measurements, y := [y1, y2, . . . , yN]T ∈RN,
according to the linear model
y = Xθ,
y ∈RN,
θ ∈Rl,
l > N,
(23.24)
where X is the N × l input matrix, which is assumed to be of full row rank, i.e., rank(X) = N. Our
starting point is the noiseless case. The system in (23.24) is an underdetermined one and accepts an
inﬁnite number of solutions. The set of possible solutions lies in the intersection of the N hyperplanes2
in the l-dimensional space,

θ ∈Rl : yn = xT
n θ

,
n = 1, 2, . . . , N.
We know from geometry, that the intersection of N non-parallel hyperplanes (which in our case is
guaranteed by the fact that X has been assumed to be full row rank, hence xn are mutually independent)
is a plane of dimensionality l −N (e.g., the intersection of two (non-parallel) (hyper) planes in the
3-dimensional space is a straight line; that is, a plane of dimensionality equal to one). In a more formal
way, the set of all possible solutions, to be denoted as , is an afﬁne set. An afﬁne set is the translation
of a linear subspace by a constant vector. Let us pursue this a bit further, since we will need it later on.
Let the null space of X be the set null(X), deﬁned as the linear subspace
null(X) =

z ∈Rl : Xz = 0

.
Obviously, if θ0 is a solution to (23.24), i.e., θ0 ∈, then it is easy to verify that ∀θ ∈, X(θ −θ0) = 0,
or θ −θ0 ∈null(X). As a result,
 = θ0 + null(X),
and  is an afﬁne set. We also know from linear algebra basics that the null space of a full row rank
matrix, N × l,l > N, is a subspace of dimensionality l −N. Figure 23.8 illustrates the case for one
measurement sample in the 2-dimensional space, l = 2 and N = 1. The set of solutions  is a line,
which is the translation of the linear subspace crossing the origin (the null(X)). Therefore, if one wants
to determine a single point that lies in the afﬁne set of solutions, , then an extra constraint/a priori
knowledge has to be imposed
In the sequel, three such possibilities are examined.
1.23.6.1 The ℓ2 norm minimizer
Our goal now becomes to pick a point in (the afﬁne set) , that corresponds to the minimum ℓ2 norm.
This is equivalent to solving the following constrained task
min
θ∈Rl ∥θ∥2
2
s.t. xT
n θ = yn,
n = 1, 2, . . . , N.
(23.25)
2In Rl, a hyperplane is of dimension l −1. A plane has dimension lower than l −1.

1286
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
(a)
(b)
FIGURE 23.8
(a) The ℓ2 norm minimizer. The dotted circle corresponds to the smallest ℓ2 ball that intersects the set .
As such, the intersection point, ˆθ, is the ℓ2 norm minimizer of the task (23.25). Notice that the vector ˆθ
contains no zero component. (b) The ℓ1 norm minimizer. The dotted rhombus corresponds to the smallest
ℓ1 ball that intersects . Hence, the intersection point, ˆθ, is the solution of the constrained ℓ1 minimization
task of (23.28). Notice that the obtained estimate ˆθ = (0, 1) contains a zero.
The previous optimization task accepts a unique solution given in closed form as
ˆθ = X T 
X X T −1
y.
(23.26)
The geometric interpretation of this solution is provided in Figure 23.8a, for the case of l = 2 and
N = 1. The radius of the Euclidean norm ball keeps increasing, till it touches the plane that contains
the solutions. This point is the one with the minimum ℓ2 norm or, equivalently, the point that lies closest
to the origin. Equivalently, the point ˆθ can be seen as the (metric) projection of 0 onto .
Minimizing the ℓ2 norm, in order to solve a linear set of underdetermined equations, has been used
in various applications. The closest to us is in the context of determining the unknown coefﬁcients in
an expansion using an overcomplete dictionary of functions (vectors) [33]. A main drawback of this
method is that it is not sparsity preserving. There is no guarantee that the solution in (23.26) will give
zeros even if the true model vector θ has zeros. Moreover, the method is resolution limited [7]. This
means that, even if there may be a sharp contribution of speciﬁc atoms in the dictionary, this is not
portrayed in the obtained solution. This is a consequence of the fact that the information provided by
X X T is a global one, containing all atoms of the dictionary in an “averaging” fashion, and the ﬁnal
result tends to smooth out the individual contributions, especially when the dictionary is overcomplete.
1.23.6.2 The ℓ0 norm minimizer
Now we turn our attention to the ℓ0 norm (once more, it is pointed out that this is an abuse of the
deﬁnition of the norm, as stated before), and we make sparsity our new ﬂag under which a solution will
be obtained. Recall from Section 1.23.5 that such a constraint is in line with the natural structure that
underlies a number of applications. The task now becomes
min
θ∈Rl ∥θ∥0

1.23.6 In Quest for the Sparsest Solution
1287
s.t. xT
n θ = yn,
n = 1, 2, . . . , N,
(23.27)
that is, from all the points that lie on the plane of all possible solutions ﬁnd the sparsest one; i.e., the one
with the least number of nonzero elements. As a matter of fact, such an approach is within the spirit of
Occam’s razor rule. It corresponds to the smallest number of parameters that can explain the obtained
measurements. The points that are now raised are:
•
Is a solution to this problem unique and under which conditions?
•
Can a solution be obtained with low enough complexity in realistic time?
We postpone the answer to the ﬁrst question later on. As for the second one, the news is no good.
Minimizing the ℓ0 norm under a set of linear constraints is a task of combinatorial nature and as a
matter of fact the problem is, in general, NP-hard [34]. The way to approach the problem is to consider
all possible combinations of zeros in θ, removing the respective columns of X in (23.24) and check
whether the system of equations is satisﬁed; keep as solutions the ones with the smallest number of
nonzero elements. Such a searching technique exhibits complexity of an exponential dependence on l.
Figure 23.8a illustrates the two points ((1.5, 0) and (0, 1)) that comprise the solution set of minimizing
the ℓ0 norm for the single measurement (constraint) case.
1.23.6.3 The ℓ1 norm minimizer
The current task is now given by
min
θ∈Rl ∥θ∥1
s.t. xT
n θ = yn,
n = 1, 2, . . . , N.
(23.28)
Figure 23.8b illustrates the geometry. The ℓ1 ball is increased till it touches the afﬁne set of the possible
solutions. For this speciﬁc geometry, the solution is the point (0, 1). In our discussion in Section 1.23.3,
we saw that the ℓ1 norm is the one, out of all ℓp, p ≥1 norms, that bears some similarity with the spar-
sity favoring (nonconvex) ℓp, p < 1 “norms.” Also, we have commented that the ℓ1 norm encourages
zeros, when the respective values are small. In the sequel, we will state one lemma, that establishes this
zero-favoring property in a more formal way. The ℓ1 norm minimizer is also known as Basis Pursuit and
it was suggested for decomposing a vector signal in terms of the atoms of an overcomplete dictionary [7].
The ℓ1 minimizer can be brought into the standard Linear Programming (LP) form and then can be
solved by recalling any related method; the simplex method or the more recent interior point methods
are two possibilities, see, e.g., [9,35]. Indeed, consider the (LP) task
min
x
cT x
s.t. Ax = b
x ≥0.
To verify that our ℓ1 minimizer can be cast in the previous form, notice ﬁrst that any l-dimensional
vector θ can be decomposed as
θ = u −v,
u ≥0, v ≥0.

1288
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
Indeed, this holds true if, for example,
u := θ+,
v := ( −θ)+,
where x+ stands for the vector obtained after taking the positive parts of the components of x. Moreover,
notice that
∥θ∥1 = [1, 1, . . . , 1]

θ+
( −θ)+

= [1, 1, . . . , 1]
 u
v

.
Hence, our ℓ1 minimization task can be recast in the LP form, if
c := [1, 1, . . . , 1]T , x := [uT , vT ]T ,
A := [X, −X],
b := y.
1.23.6.4 Characterization of the ℓ1 norm minimizer
Lemma 1.
An element θ in the afﬁne set, , of the solutions of the underdetermined linear system
(23.24), has minimal ℓ1 norm if and only if the following condition is satisﬁed:


i:θi̸=0
sgn(θi)zi

≤

i:θi=0
|zi|,
∀z ∈null(X).
(23.29)
Moreover, the ℓ1 minimizer is unique if and only if the inequality in (23.29) is a strict one for all z ̸= 0
(see, e.g., [36]).
Remarks 2.
The previous lemma has a very interesting and important consequence. If ˆθ is the unique
minimizer of (23.28), then
card{i : ˆθi = 0} ≥dim (null(X)),
(23.30)
where card{·} denotes the cardinality of a set. In words, the number of zero coordinates of the unique
minimizer cannot be smaller than the dimension of the null space of X. Indeed, if this is not the case,
then the unique minimizer could have less zeros than the dimensionality of null(X). As it can easily be
shown, this means that we can always ﬁnd a z ∈null(X), which has zeros in the same locations where
the coordinates of the unique minimizer are zero, and at the same time it is not identically zero, i.e.,
z ̸= 0. However, this would violate (23.29), which in the case of uniqueness holds as a strict inequality.
Deﬁnition 1.
A vector θ is called k-sparse if it has at most k nonzero components.
Remarks 3.
If the minimizer of (23.28) is unique, then it is a k-sparse vector with
k ≤N.
This is a direct consequence of the Remark 2, and the fact that for the matrix X,
dim (null(X)) = l −rank(X) = l −N.
Hence, the number of the nonzero elements of the unique minimizer must be at most equal to N.
If one resorts to geometry, all the previously stated results become crystal clear.

1.23.6 In Quest for the Sparsest Solution
1289
(a)
(b)
FIGURE 23.9
(a) The ℓ1 ball intersecting with a plane. The only possible scenario, for the existence of a unique common
intersecting point of the ℓ1 ball with a plane in the Euclidean R3 space, is for the point to be located at
one of the corners of the ℓ1 ball, i.e., to be an 1-sparse vector. (b) The ℓ1 ball intersecting with lines. In this
case, the sparsity level of the unique intersecting point is relaxed; it could be an 1- or a 2-sparse vector.
1.23.6.5 Geometric interpretation
Assumethatourtargetsolutionresidesinthe3-dimensionalspaceandthatwearegivenonemeasurement
y1 = xT
1 θ = x11θ1 + x12θ2 + x13θ3.
Then the solution lies in the 2-dimensional (hyper)plane, which is described by the previous equation.
To get the minimal ℓ1 solution we keep increasing the size of the ℓ1 ball3 (the set of all points that have
equal ℓ1 norm) till it touches this plane. The only way that these two geometric objects have a single
point in common (unique solution) is when they meet at a corner of the diamond. This is shown in
Figure 23.9a. In other words, the resulting solution is 1-sparse, having two of its components equal to
zero. This complies with the ﬁnding stated in Remark 3, since now N = 1. For any other orientation
of the plane, this will either cut across the ℓ1 ball or will share with the diamond an edge or a side. In
both cases, there will be inﬁnite many solutions.
Let us now assume that we are given an extra measurement,
y2 = x21θ1 + x22θ2 + x23θ3.
The solution now lies in the intersection of the two previous planes, which is a straight line. However,
now, we have more alternatives for a unique solution. A line, e.g., 1, can either touch the ℓ1 ball at a
corner (1-sparse solution) or, as it is shown in Figure 23.9b, it can touch the ℓ1 ball at one of its edges,
e.g., 2. The latter case, corresponds to a solution that lies on a 2-dimensional subspace, hence it will
be a 2-sparse vector. This also complies with the ﬁndings stated in Remark 3, since in this case, we have
N = 2,l = 3 and the sparsity level for a unique solution can be either 1 or 2.
3Observe that in the 3-dimensional space the ℓ1 ball looks like a diamond.

1290
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
Note that uniqueness is associated with the particular geometry and orientation of the afﬁne set,
which is the set of all possible solutions of the underdetermined system of equations. For the case of the
square ℓ2 norm, the solution was always unique. This is a consequence of the (hyper)spherical shape
formed by the Euclidean norm. From a mathematical point of view, the square ℓ2 norm is a strict convex
function. This is not the case for the ℓ1 norm, which is convex, albeit not a strict convex function.
Example 2.
Consider a sparse vector parameter [0, 1]T , which we assume to be unknown. We will use
one measurement to sense it. Based on this single measurement, we will use the ℓ1 minimizer of (23.28)
to recover its true value. Let us see what happens. We will consider three different values of the “sensing”
(input) vector x in order to obtain the measurement y = xT θ: (a) x =
 1
2, 1
T , (b) x = [1, 1]T , and
(c) x = [2, 1]T . The resulting measurement, after sensing θ by x, is y = 1 for all the three previous cases.
Case (a): The solution will lie on the straight line
 =

[θ1, θ2]T ∈R2 : 1
2θ1 + θ2 = 1

,
which is shown in Figure 23.10a. For this setting, expanding the ℓ1 ball, this will touch the line (our
solutions’ afﬁne set) at the corner [0, 1]T . This is a unique solution, hence it is sparse, and it coincides
with the true value.
(a)
(b)
(c)
FIGURE 23.10
(a) Sensing with x =
 1
2, 1
T , (b) sensing with x = [1, 1]T , and (c) sensing with x = [2, 1]T . The choice of
the sensing vector x is crucial to unveiling the true sparse solution (0, 1). Only the sensing vector x =
 1
2, 1
T
identiﬁes uniquely the desired (0, 1).

1.23.7 Uniqueness of the ℓ0 Minimizer
1291
Case (b): The solutions lies on the straight line
 =

[θ1, θ2]T ∈R2 : θ1 + θ2 = 1

,
which is shown in Figure 23.10b. For this set up, there is an inﬁnite number of solutions, including two
sparse ones.
Case (c): The afﬁne set of solutions is described by
 =

[θ1, θ2]T ∈R2 : 2θ1 + θ2 = 1

,
which is sketched in Figure 23.10c. The solution in this case is sparse, but it is not the correct one.
This example is quite informative. If we sense (measure) our unknown parameter vector with appro-
priate sensing (input) data, the use of the ℓ1 norm can unveil the true value of the parameter vector,
even if the system of equations is underdetermined, provided that the true parameter is sparse. This now
becomes our new goal. To investigate whether what we have just said can be generalized, and under
which conditions holds true, if it does. In such a case, the choice of the regressors (which we just called
them sensing vectors) and hence the input matrix (which, from now on, we will refer to, more and more
frequently, as the sensing matrix) acquire an extra signiﬁcance. It is not enough for the designer to care
only for the rank of the matrix, i.e., the linear independence of the sensing vectors. One has to make
sure that the corresponding afﬁne set of the solutions has such an orientation, so that the touch with the
ℓ1 ball, as this increases from zero to meet this plane, is a “gentle” one, i.e., they meet at a single point,
and more important at the correct one; that is, at the point that represents the true value of the sparse
parameter, which we are searching for.
Remarks 4.
•
Often in practice, the columns of the input matrix, X, are normalized to unit ℓ2 norm. Although ℓ0
norm is insensitive to the values of the nonzero components of θ, this is not the case with the ℓ1
and ℓ2 norms. Hence, while trying to minimize the respective norms, and at the same time to fulﬁll
the constraints, components that correspond to columns of X with high energy (norm) are favored
more than the rest. Hence, the latter become more popular candidates to be pushed to zero. In order
to avoid such situations, the columns of X are normalized to unity, by dividing each element of the
column vector by the respective (Euclidean) norm.
1.23.7 Uniqueness of the ℓ0 minimizer
Our ﬁrst goal is to derive sufﬁcient conditions that guarantee uniqueness of the ℓ0 minimizer, which has
been deﬁned in Section 1.23.6.
Deﬁnition 2.
The spark of a full rank N × l (l ≥N) matrix, X, denoted as spark(X), is the smallest
number of its linearly dependent columns.
According to the previous deﬁnition, any m < spark(X) columns of X are, necessarily, linearly
independent. The spark of a square, N × N, full rank matrix is equal to N + 1.

1292
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
Remarks 5.
•
In contrast to the rank of a matrix, which can be easily determined, its spark can only be obtained by
resorting to a combinatorial search over all possible combinations of the columns of the respective
matrix, see, e.g., [8,37]. The notion of the spark was used in the context of sparse representation,
under the name of Uniqueness Representation Property, in [38]. The name “spark” was coined in
[37]. An interesting discussion relating this matrix index with other indices, used in other disciplines,
is given in [8].
Example 3.
Consider the following matrix
X =
⎡
⎢⎢⎣
1 0 0 0 1 0
0 1 0 0 1 1
0 0 1 0 0 1
0 0 0 1 0 0
⎤
⎥⎥⎦.
The matrix has rank equal to 4 and spark equal to 3. Indeed, any pair of columns are linearly independent.
On the other hand, the ﬁrst, the second and the ﬁfth columns are linearly dependent. The same is also
true for the combination of the second, third and sixth columns.
Lemma 2.
If null(X) is the null space of X, then
∥θ∥0 ≥spark(X),
∀θ ∈null(X), θ ̸= 0.
Proof.
To derive a contradiction, assume that there exists a 0 ̸= θ ∈null(X) such that ∥θ∥0 <
spark(X). Since by deﬁnition Xθ = 0, there exists a number of ∥θ∥0 columns of X that are lin-
early dependent. However, this contradicts the minimality of spark(X), and the claim of Lemma 2 is
established.
Lemma 3.
If a linear system of equations, Xθ = y, has a solution that satisﬁes
∥θ∥0 < 1
2spark(X),
then this is the sparsest possible solution. In other words, this is, necessarily, the unique solution of the
ℓ0 minimizer.
Proof.
Consider any other solution h ̸= θ. Then, θ −h ∈null(X), i.e.,
X(θ −h) = 0.
Thus, according to Lemma 2,
spark(X) ≤∥θ −h∥0 ≤∥θ∥0 + ∥h∥0 .
(23.31)
Observe that although the ℓ0 “norm” is not a true norm, it can be readily veriﬁed by simple inspection and
reasoning that the triangular property is satisﬁed. Indeed, by adding two vectors together, the resulting

1.23.7 Uniqueness of the ℓ0 Minimizer
1293
number of nonzero elements will always be at most equal to the total number of nonzero elements of
the two vectors. Therefore, if ∥θ∥0 < 1
2spark(X), then (23.31) suggests that
∥h∥0 > 1
2spark(X) > ∥θ∥0 .
Remarks 6.
•
Lemma 3 is a very interesting result. We have a sufﬁcient condition to check whether a solution is the
unique optimal in a, generally, NP-hard problem. Of course, although this is nice from a theoretical
point of view, is not of much use by itself, since the related bound (the spark) can only be obtained
after a combinatorial search. Well, in the next section, we will see that we can relax the bound by
involving another index, in place of the spark, which can be easily computed.
•
An obvious consequence of the previous lemma is that if the unknown parameter vector is a sparse
one with k nonzero elements, then if matrix X is chosen so that to have spark(X) > 2k, then the true
parameter vector is necessarily the sparsest one that satisﬁes the set of equations, and the (unique)
solution to the ℓ0 minimizer.
•
In practice, the goal is to sense the unknown parameter vector by a matrix that has as high a spark
as possible, so that the previously stated sufﬁciency condition to cover a wide range of cases. For
example, if the spark of the input matrix is, say, equal to three, then one can check for optimal sparse
solutions up to a sparsity level of k = 1. From the respective deﬁnition, it is easily seen that the
values of the spark are in the range 1 < spark(X) ≤N + 1.
•
Constructing an N ×l matrix X in a random manner, by generating i.i.d. entries, guarantees, with high
probability, that spark(X) = N + 1; that is, any N columns of the matrix are linearly independent.
1.23.7.1 Mutual coherence
Since the spark of a matrix is a number that is difﬁcult to compute, our interest shifts to another index,
which can be derived easier and at the same time can offer a useful bound on the spark. The mutual
coherence of an N × l matrix X [24], denoted as μ(X), is deﬁned as
μ(X) :=
max
1≤i< j≤l
|xT
i x j|
∥xi∥
  x j
  ,
(23.32)
where xi, i = 1, 2, . . . ,l, denote the columns of X (notice the difference in notation between a row xT
i
and a column xi of the matrix X). This number reminds us of the correlation coefﬁcient between two
random variables. Mutual coherence is bounded as 0 ≤μ(X) ≤1. For a square orthogonal matrix,
X, μ(X) = 0. For general matrices, with l > N, μ(X) satisﬁes
!
l −N
N(l −1) ≤μ(X) ≤1,
which is known as the Welch bound [39]. For large values of l, the lower bound becomes, approximately,
μ(X) ≥
1
√
N . Common sense reasoning guides us to construct input (sensing) matrices of mutual

1294
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
coherence as small as possible. Indeed, the purpose of the sensing matrix is to “measure” the components
of the unknown vector and “store” this information in the measurement vector y. Thus, this should be
done in such a way so that y to retain as much information about the components of θ as possible. This
can be achieved if the columns of the sensing matrix, X, are as “independent” as possible. Indeed, y is
the result of a combination of the columns of X, each one weighted by a different component of θ. Thus,
if the columns are as much “independent” as possible then the information regarding each component
of θ is contributed by a different direction making its recovery easier. This is easier understood if X
is a square orthogonal matrix. In the more general case of a non-square matrix, the columns should be
made as “orthogonal” as possible.
Example 4.
Assume that X is an N × 2N matrix, formed by concatenating two orthonormal bases
together,
X = [I, W],
where I is the identity matrix, having as columns the vectors ei, i = 1, 2, . . . , N, with elements equal to
δir =
 1,
if i = r,
0,
if i ̸= r,
for r = 1, 2, . . . , N. The matrix W is the orthonormal DFT matrix, deﬁned as
W =
1
√
N
⎡
⎢⎢⎢⎣
1
1
· · ·
1
1
WN
· · ·
W N−1
N
...
...
...
...
1 W N−1
N
· · · W (N−1)(N−1)
N
⎤
⎥⎥⎥⎦,
where
WN := exp

−j 2π
N

.
Such an overcomplete dictionary could be used to represent signal vectors in terms of the expansion
in (23.23), that comprise the sum of sinusoids with very narrow spiky-like pulses. The inner products
between any two columns of I and between any two columns of W are zero, due to orthogonality. On
the other hand, it is easy to see that the inner product between any column of I and any column of W
has absolute value equal to
1
√
N . Hence, the mutual coherence of this matrix is μ(X) =
1
√
N . Moreover,
observe that the spark of this matrix is spark(X) = N + 1.
Lemma 4.
For any N × l matrix X, the following inequality holds
spark(X) ≥1 +
1
μ(X).
(23.33)
The proof is given in [37] and it is based on arguments that stem from matrix theory applied on the
Gram matrix, X T X, of X. A “superﬁcial” look at the previous bound is that for very small values of
μ(X) the spark can be larger than N + 1! Looking at the proof, it is seen that in such cases the spark
of the matrix attains its maximum value N + 1.

1.23.8 Equivalence of ℓ0 and ℓ1 minimizers
1295
The result complies with a common sense reasoning. The smaller the value of μ(X) the more
independent are the columns of X, hence the higher the value of its spark is expected to be. Based
on this lemma, we can now state the following theorem, ﬁrst given in [37]. Combining the way that
Lemma 3 is proved and (23.33), we come to the following important theorem.
Theorem 1.
If the linear system of equations in (23.24) has a solution that satisﬁes the condition
∥θ∥0 < 1
2

1 +
1
μ(X)

,
(23.34)
then this solution is the sparsest one.
Remarks 7.
•
The bound in (23.34) is “psychologically” important. It relates an easily computed bound to check
whether the solution to a NP-hard task is the optimal one. However, it is not a particularly good
bound and it restricts the range of values in which it can be applied. As we saw in Example 4, while
the maximum possible value of the spark of a matrix was equal to N + 1, the minimum possible
value of the mutual coherence was
1
√
N . Therefore, the bound based on the mutual coherence restricts
the range of sparsity, i.e., ∥θ∥0, where one can check optimality, to around 1
2
√
N. Moreover, as the
previously stated Welch bound suggests, this O( 1
√
N ) dependence of the mutual coherence seems to
be a more general trend and not only the case for Example 4, see, e.g., [40]. On the other hand, as we
have already stated in the Remarks 6, one can construct random matrices with spark equal to N + 1;
hence, using the bound based on the spark, one could expand the range of sparse vectors up to 1
2 N.
1.23.8 Equivalence of ℓ0 and ℓ1 minimizers: sufﬁciency conditions
We have now come to the crucial point and we will establish the conditions that guarantee the equivalence
between the ℓ1 and the ℓ0 minimizers. Hence, under such conditions, a problem, that is in general NP-
hard problem, can be solved via a tractable convex optimization task. Under these conditions, the zero
value encouraging nature of the ℓ1 norm, that has already been discussed, obtains a much higher stature;
it provides the sparsest solution.
1.23.8.1 Condition implied by the mutual coherence number
Theorem 2.
Let the underdetermined system of equations
y = Xθ,
where X is an N × l (N < l) full row rank matrix. If a solution exists and satisﬁes the condition
∥θ∥0 < 1
2

1 +
1
μ(X)

,
(23.35)
then this is the unique solution of both, the ℓ0 as well the ℓ1 minimizers.

1296
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
This is a very important theorem and it was shown independently in [37,41]. Earlier versions of the
theorem addressed the special case of a dictionary comprising two orthonormal bases, [40,42]. A proof
is also summarized in [8]. This theorem established, for a ﬁrst time, what it was till then empirically
known: often, the ℓ1 and ℓ0 minimizers result in the same solution.
Remarks 8.
•
The theory that we have presented so far is very satisfying, since it offers the theoretical framework
and conditions that guarantee uniqueness of a sparse solution to an underdetermined system of
equations. Now we know that, under certain conditions, the solution, which we obtain by solving
the convex ℓ1 minimization task, is the (unique) sparsest one. However, from a practical point of
view, the theory, which is based on mutual coherence, does not say the whole story and falls short to
predict what happens in practice. Experimental evidence suggests that the range of sparsity levels,
for which the ℓ0 and ℓ1 tasks give the same solution, is much wider than the range guaranteed by
the mutual coherence bound. Hence, there is a lot of theoretical happening in order to improve this
bound. A detailed discussion is beyond the scope of this paper. In the sequel, we will present one of
these bounds, since it is the one that currently dominates the scene. For more details and a related
discussion the interested reader may consult, e.g., [43].
1.23.8.2 The restricted isometry property (RIP)
Deﬁnition 3.
For each integer k = 1, 2, . . ., deﬁne the isometry constant δk of an N × l matrix X as
the smallest number such that
(1 −δk) ∥θ∥2
2 ≤∥Xθ∥2
2 ≤(1 + δk) ∥θ∥2
2 ,
(23.36)
holds true for all k-sparse vectors θ.
This deﬁnition was introduced in [44]. We loosely say that matrix X obeys the RIP of order k
if δk is not too close to one. When this property holds true, it implies that the Euclidean norm of θ is
approximately preserved, after projecting it onto the rows of X. Obviously, if matrix X were orthonormal
then δk = 0. Of course, since we are dealing with non-square matrices this is not possible. However, the
closer δk is to zero, the closer to orthonormal all subsets of k columns of X are. Another view point of
(23.36) is that it preserves Euclidean distances between k-sparse vectors. Let us consider two k-sparse
vectors, θ1, θ2 and apply (23.36) to their difference θ1 −θ2, which, in general, is a 2k-sparse vector.
Then we obtain
(1 −δ2k) ∥θ1 −θ2∥2
2 ≤∥X(θ1 −θ2)∥2
2 ≤(1 + δ2k) ∥θ1 −θ2∥2
2 .
(23.37)
Thus, when δ2k is small enough, the Euclidean distance is preserved after projection in the lower
dimensional measurements’ space. In words, if the RIP holds true, this means that searching for a
sparse vector in the lower dimensional subspace formed by the measurements, RN, and not in the
original l-dimensional space, one can still recover the vector since distances are preserved and the
target vector is not “confused” with others. After projection onto the rows of X, the discriminatory
power of the method is retained. It is interesting to point out that the RIP is also related to the condition

1.23.8 Equivalence of ℓ0 and ℓ1 minimizers
1297
number of the Gramian matrix. In [44,45], it is pointed out that if Xr denotes the matrix that results
by considering only r of the columns of X, then the RIP in (23.36) is equivalent with requiring the
respective Gramian, X T
r Xr,r ≤k, to have its eigenvalues within the interval [1 −δk, 1 + δk]. Hence,
the more well conditioned the matrix is, the better is for us to dig out the information hidden in the
lower dimensional measurements space.
Theorem 3.
Assume that for some k, δ2k <
√
2 −1. Then the solution to the ℓ1 minimizer of (23.28),
denoted as θ∗, satisﬁes the following two conditions
∥θ −θ∗∥1 ≤C0 ∥θ −θk∥1 ,
(23.38)
and
∥θ −θ∗∥2 ≤C0k−1
2 ∥θ −θk∥1 ,
(23.39)
for some constant C0. In the previously stated formulas, θ is the true (target) vector that generates the
measurements in (23.28) and θk is the vector that results from θ if we keep its k largest components and
set the rest equal to zero, [44,46–48].
Hence, if the true vector is a sparse one, i.e., θ = θk, then the ℓ1 minimizer recovers the (unique)
exact value. On the other hand, if the true vector is not a sparse one, then the minimizer results in a
solution whose accuracy is dictated by a genie-aided procedure that knew in advance the locations of
the k largest components of θ. This is a groundbreaking result. Moreover, it is deterministic, it is always
true and not with high probability. Note that the isometry property of order 2k is used, since at the heart
of the method lies our desire to preserve the norm of the differences between vectors.
Let us now focus on the case where there is a k-sparse vector that generates the measurements,
i.e., θ = θk. Then it is shown in [46] that the condition δ2k < 1 guarantees that the ℓ0 minimizer
has a unique k-sparse solution. In other words, in order to get the equivalence between the ℓ1 and ℓ0
minimizers, the range of values for δ2k has to be decreased to δ2k <
√
2 −1, according to Theorem 3.
This sounds reasonable. If we relax the criterion and use ℓ1 instead of ℓ0, then the sensing matrix has
to be more carefully constructed. Although we are not going to provide the proofs of these theorems
here, since their formulation is well beyond the scope of this chapter, it is interesting to follow what
happens if δ2k = 1. This will give us a ﬂavor of the essence behind the proofs. If δ2k = 1, the left hand
side term in (23.37) becomes zero. In this case, there may exist two k-sparse vectors θ1, θ2 such that
X(θ1 −θ2) = 0, or Xθ1 = Xθ2. Thus, it is not possible to recover all k-sparse vectors, after projecting
them in the measurements space, by any method.
The previous argument also establishes a connection between RIP and the spark of a matrix. Indeed,
if δ2k < 1, this guarantees that any number of columns of X up to 2k are linearly independent, since for
any 2k-sparse θ, (23.36) guarantees that ∥Xθ∥2 > 0. This implies that spark(X) > 2k. A connection
between RIP and the coherence is established in [49], where it is shown that if X has coherence μ(X),
and unit norm columns, then X satisﬁes the RIP of order k with δk, where δk ≤(k −1)μ(X).
1.23.8.2.1
Constructing matrices that obey the RIP of order k
It is apparent from our previous discussion that the higher the value of k, for which the RIP property
of a matrix, X, holds true, the better, since a larger range of sparsity levels can be handled. Hence, a
main goal towards this direction is to construct such matrices. It turns out that verifying the RIP for

1298
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
a matrix of a general structure is a difﬁcult task. This reminds us of the spark of the matrix, which is
also a difﬁcult task to compute. However, it turns out that for a certain class of random matrices, the
RIP follows fairly easy. Thus, constructing such sensing matrices has dominated the scene of related
research. We will present a few examples of such matrices, which are also very popular in practice,
without going into details of the proofs, since this is out of our scope and the interested reader may dig
this information from the related references.
Perhaps, the most well known example of a random matrix is the Gaussian one, where the entries
X(i, j) of the sensing matrix are i.i.d. realizations from a Gaussian pdf N

0, 1
N

. Another popular exam-
ple of such matrices is constructed by sampling i.i.d. entries from a Bernoulli, or related, distributions
X(i, j) =
⎧
⎪⎪⎨
⎪⎪⎩
1
√
N
, with probability 1
2,
−1
√
N
, with probability 1
2,
or
X(i, j) =
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
+
#
3
N , with probability 1
6,
0, with probability 2
3,
−
#
3
N , with probability 1
6.
Finally, one can adopt the uniform distribution and construct the columns of X by sampling uni-
formly at random on the unit sphere in RN. It turns out, that such matrices obey the RIP of order k,
with overwhelming probability, provided that the number of measurements, N, satisfy the following
inequality
N ≥Ck ln (l/k),
(23.40)
where C is some constant, which depends on the isometry constant δk. In words, having such a matrix
at our disposal, one can recover a k-sparse vector from N < l measurements, where N is larger than the
sparsity level by an amount controlled by the inequality (23.40). More on these issues can be obtained
from, e.g., [45,50].
Besides random matrices, one can construct other matrices that obey the RIP. One such example
includes the partial Fourier matrices, which are formed by selecting uniformly at random N rows drawn
from the l × l DFT matrix. Although the required number of samples for the RIP to be satisﬁed may be
larger than the bound in (23.40) (see, [51]), Fourier-based sensing matrices offer certain computational
advantages, when it comes to storage (O(N ln l)) and matrix-vector products (O(l ln l)), [52]. In [53], the
case of random Toeplitz sensing matrices, containing statistical dependencies across rows, is considered
and it is shown that they can also satisfy the RIP with high probability. This is of particular importance
in signal processing and communications applications, where it is very common for a system to be
excited in its input via a time series, hence independence between successive input rows cannot be
assumed. In [54,55], the case of separable matrices is considered where the sensing matrix is the result
of a Kronecker product of matrices, which satisfy the RIP individually. Such matrices are of interest
for multidimensional signals, in order to exploit the sparsity structure along each one of the involved

1.23.9 Robust Sparse Signal Recovery from Noisy Measurements
1299
dimensions. For example, such signals may occur while trying to “encode” information associated with
an event whose activity spreads across the temporal, spectral, spatial, etc., domains.
In spite of their theoretical elegance, the derived bounds, that determine the number of the required
measurements for certain sparsity levels, fall short of what is the experimental evidence, e.g., [43].
In practice, a rule of thumb is to use N of the order of 3k −5k, e.g., [46]. For large values of l,
compared to the sparsity level, the analysis in [56] suggests that we can recover most sparse signals
when N ≈2k ln (l/N). In an effort to overcome the shortcomings associated with the RIP, a number
of other techniques have been proposed, e.g., [43,57–59]. Furthermore, in speciﬁc applications, the use
of an empirical study may be a more appropriate path.
Note that, in principle, the minimum number of measurements that are required to recover a k sparse
vector from N < l measurements is N ≥2k. Indeed, in the spirit of the discussion after Theorem 3, the
main requirement that a sensing matrix must fulﬁll is the following: not to map two different k-sparse
vectors to the same measurement vector y. Otherwise, one can never recover both vectors from their
(common) measurements. If we have 2k measurements and a sensing matrix that guarantees that any
2k columns are linearly independent, then the previously stated requirement is readily seen that it is
satisﬁed. However, the bounds on the number of measurements set in order the respective matrices to
satisfy the RIP are higher. This is because RIP accounts also for the stability of the recovery process.
We will come to this issue soon, in Section 1.23.10, where we talk about stable embeddings.
1.23.9 Robust sparse signal recovery from noisy measurements
In the previous section, our focus was on recovering a sparse solution from an underdetermined system
of equations. In the formulation of the problem, we assumed that there is no noise in the obtained
measurements. Having acquired some experience and insight from a simpler problem, we now turn our
attention to the more realistic task, where uncertainties come into the scene. One type of uncertainty may
be due to the presence of noise and our measurements’ model comes back to the standard regression form
y = Xθ + η,
(23.41)
where X is our familiar non-square N × l matrix. A sparsity-aware formulation for recovering θ from
(23.41) can be cast as
min
θ∈Rl ∥θ∥1
s.t. ∥y −Xθ∥2
2 ≤ϵ,
(23.42)
which coincides with the LASSO task given in (23.15). Such a formulation implicitly assumes that
the noise is bounded and the respective range of values is controlled by ϵ. One can consider a number
of different variants. For example, one possibility would be to minimize the ∥·∥0 norm instead of the
∥·∥1, albeit loosing the computational elegance of the latter. An alternative route would be to replace
the Euclidean norm in the constraints with another one.
Besidesthepresenceofnoise,onecouldseethepreviousformulationfromadifferentperspective.The
unknown parameter vector, θ, may not be exactly sparse, but it may consist of a few large components,

1300
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
while the rest are small and close to, yet not necessarily equal to, zero. Such a model misﬁt can be
accommodated by allowing a deviation of y from Xθ.
In this relaxed setting of a sparse solution recovery, the notions of uniqueness and equivalence,
concerning the ℓ0 and ℓ1 solutions, no longer apply. Instead, the issue that now gains in importance
is that of stability of the solution. To this end, we focus on the computationally attractive ℓ1 task. The
counterpart of Theorem 3 is now expressed as follows.
Theorem 4.
Assume that the sensing matrix, X, obeys the RIP with δ2k <
√
2 −1, for some k. Then
the solution θ∗of (23.42) satisﬁes the following ([47,48]),
∥θ −θ∗∥2 ≤C0k−1
2 ∥θ −θk∥1 + C1
√ϵ,
(23.43)
for some constants C1, C0.
This is also an elegant result. If the model is exact and ϵ = 0 we obtain (23.39). If not, the higher
the uncertainty (noise) term in the model, the higher our ambiguity about the solution. Note, also, that
the ambiguity about the solution depends on how far the true model is from θk. If the true model is
k-sparse, the ﬁrst term on the right hand side of the inequality is zero. The values of C1, C0 depend on
δ2k but they are small, e.g., close to ﬁve or six, [48].
The important conclusion, here, is that the LASSO formulation for solving inverse problems (which
in general tend to be ill-conditioned) is a stable one and the noise is not ampliﬁed excessively during
the recovery process.
1.23.10 Compressed sensing: the glory of randomness
The way in which this paper was deplored followed, more or less, the sequence of developments that
took place during the evolution of the sparsity-aware parameter estimation ﬁeld. We intentionally made
an effort to follow such a path, since this is also indicative of how science evolves in most cases.
The starting point had a rather strong mathematical ﬂavor: to develop conditions for the solution of
an underdetermined linear system of equations, under the sparsity constraint and in a mathematically
tractable way, i.e., using convex optimization. In the end, the accumulation of a sequence of individual
contributions revealed that the solution can be (uniquely) recovered if the unknown quantity is sensed
via randomly chosen data samples. This development has, in turn, given birth to a new ﬁeld with strong
theoretical interest as well as with an enormous impact on practical applications. This new emerged
area is known as compressed sensing or compressive sampling (CS). Although CS builds around the
LASSO and Basis Pursuit (and variants of them, as we will soon see), it has changed our view on how
to sense and process signals efﬁciently.
1.23.10.1 Compressed sensing
Incompressedsensing,thegoalistodirectlyacquireasfewsamplesaspossiblethatencodetheminimum
information, which is needed to obtain a compressed signal representation. In order to demonstrate this,
let us return to the data compression example, which was discussed in Section 1.23.5. There, it was
commented that the “classical” approach to compression was rather unorthodox, in the sense that ﬁrst

1.23.10 Compressed Sensing: The Glory of Randomness
1301
all (i.e., a number of l) samples of the signal are used, then they are processed to obtain l transformed
values, from which only a small subset is used for coding. In the CS setting, the procedure changes to
the following one.
Let X be an N × l sensing matrix, which is applied to the (unknown) signal vector, s, in order to
obtain the measurements, y, and  be the dictionary matrix that describes the domain where the signal
s accepts a sparse representation, i.e.,
s = θ,
y = Xs.
(23.44)
Assuming that at most k of the components of θ are nonzero, this can be obtained by the following
optimization task
min
θ∈Rl ∥θ∥1
s.t. y = Xθ,
(23.45)
provided that the combined matrix X complies with the RIP and the number of measurements, N,
satisﬁes the associated bound given in (23.40). Note that s needs not to be stored and can be obtained
any time, once θ is known. Moreover, as we will soon discuss, the measurements, yn, n = 1, 2, . . . , N,
can be acquired directly from an analog signal s(t), prior to obtaining its sample (vector) version, s!
Thus, from such a perspective, CS fuses the data acquisition and the compression steps together.
There are different ways to obtain a sensing matrix, X, that leads to a product X, which satisﬁes
the RIP. It can be shown, that if  is orthonormal and X is a random matrix, which is constructed as
discussed at the end of Section 1.23.8.2, then the product X obeys the RIP, provided that (23.40) is
satisﬁed, [48]. An alternative way to obtain a combined matrix, that respects the RIP, is to consider
another orthonormal matrix , whose columns have low coherence with the columns of  (coherence
between two matrices is deﬁned in (23.32), where, now, the pace of xi is taken by a column of  and
that of x j by a column of ). For example,  could be the DFT matrix and  = I or vice versa. Then
choose N rows of  uniformly at random to form X in (23.44). In other words, for such a case, the
sensing matrix can be written as R, where R is a N × l matrix that extracts N coordinates uniformly
at random. The notion of incoherence (low coherence) between the sensing and the basis matrices is
closely related to RIP. The more incoherent the two matrices are, the less the number of the required
measurements for the RIP to hold, e.g., [51,60]. Another way to view incoherence is that the rows of
 cannot be sparsely represented in terms of the columns of . It turns out that if the sensing matrix
X is a random one, formed as it has already been described in Section 1.23.8.2.1, then RIP and the
incoherence with any  are satisﬁed with high probability.
The news get even better to say that all the previously stated philosophy can be extended to the more
general type of signals, which are not, necessarily, sparse or sparsely represented in terms of the atoms
of a dictionary, and they are known as compressible. A signal vector is said to be compressible if its
expansion in terms of a basis consists of just a few large coefﬁcients θi and the rest are small. In other
words, the signal vector is approximately sparse in some basis. Obviously, this is the most interesting
case in practice, where exact sparsity is scarcely (if ever) met. Reformulating the arguments used in

1302
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
Section 1.23.9, the CS task for this case can be cast as:
min
θ∈Rl ∥θ∥1
s.t. ∥y −Xθ∥2
2 ≤ϵ,
(23.46)
and everything that has been said in Section 1.23.9 is also valid for this case, if in place of X we consider
the product X.
Remarks 9.
•
An important property in compressed sensing is that the sensing matrix, which provides the mea-
surements, may be chosen independently on the matrix ; that is, the basis/dictionary in which the
signal is sparsely represented. In other words, the sensing matrix can be “universal” and can be used
to provide the measurements for reconstructing any sparse or sparsely represented signal in any
dictionary, provided RIP is not violated.
•
Each measurement, yn, is the result of an inner product (projection) of the signal vector with a row,
xT
n , of the sensing matrix, X. Assuming that the signal vector, s, is the result of a sampling process on
an analog signal, s(t), then yn can be directly obtained, to a good approximation, by taking the inner
product (integral) of s(t) with a sensing waveform, xn(t), that corresponds to xn. For example, if X
is formed by ±1, as described in Section 1.23.8.2.1, then the conﬁguration shown in Figure 23.11
can result to yn. An important aspect of this approach, besides avoiding to compute and store the
l components of s, is that multiplying by ±1 is a relatively easy operation. It is equivalent with
changing the polarity of the signal and it can be implemented by employing inverters and mixers. It
is a process that can be performed, in practice, at much higher rates than sampling (we will come to
it soon). If such a scenario is adopted, one could obtain measurements of an analog signal at much
lower rates than required for classical sampling, since N is much lower than l. The only condition
is that the signal vector must be sparse, in the DFT dictionary, [61]. The dictionary is required only
during the reconstruction of s. Thus, in the CS rationale, processing complexity is removed from the
“front end” and is transferred to the “back end,” by exploiting ℓ1 optimization.
FIGURE 23.11
Sampling an analog signal s(t) in order to generate the measurement yn at the time instant n. The sampling
period Ts is much lower than that required by the Nyquist sampling.

1.23.10 Compressed Sensing: The Glory of Randomness
1303
One of the very ﬁrst applications that were inspired by the previous approach, is the so-called one
pixel camera [62]. This was one among the most catalytic examples, that spread the rumor about the
practical power of CS. CS is an example of what is commonly said: “There is nothing more practical
than a good theory”!
1.23.10.2 Dimensionality reduction and stable embeddings
We are now going to shed light to what we have said so far in this paper from a different view. In both
cases, either when the unknown quantity was a k-sparse vector in a high dimensional space, Rl, or if
the signal s was (approximately) sparsely represented in some dictionary (s = θ), we chose to work
in a lower dimensional space (RN); that is, the space of the measurements, y. This is a typical task of
dimensionality reduction. The main task in any (linear) dimensionality reduction technique is to choose
the proper matrix X, that dictates the projection to the lower dimensional space. In general, there is
always a loss of information by projecting from Rl to RN, with N < l, in the sense that we cannot
recover any vector, θl ∈Rl, from its projection θ N ∈RN. Indeed, take any vector θl−N ∈null(X),
that lies in the (l −N)-dimensional null space of the (full rank) X (see Section 1.23.6). Then, all vectors
θl + θl−N ∈Rl share the same projection in RN. However, what we have discovered in our tour in
this chapter is that if the original vector is sparse then we can recover it exactly. This is because all the
k-sparse vectors do not lie anywhere in Rl, but rather in a subset of it; that is, in a union of subspaces,
each one having dimensionality k. If the signal s is sparse in some dictionary , then one has to search
for it in the union of all possible k-dimensional subspaces of Rl, which are spanned by k column vectors
from , [63,64]. Of course, even in this case, where sparse vectors are involved, not any projection can
guarantee unique recovery. The guarantee is provided if the projection in the lower dimensional space is
a stable embedding. A stable embedding in a lower dimensional space must guarantee that if θ1 ̸= θ2,
then their projections remain also different. Yet this is not enough. A stable embedding must guarantee
that distances are (approximately) preserved; that is, vectors that lie far apart in the high dimensional
space, have projections that also lie far apart. Such a property guarantees robustness to noise. Well,
the sufﬁcient conditions, which have been derived and discussed throughout this paper, and guarantee
the recovery of a sparse vector lying in Rl from its projections in RN, are conditions that guarantee
stable embeddings. The RIP and the associated bound on N provides a condition on X that leads to
stable embeddings. We commented on this norm-preserving property of RIP in the related section. The
interesting fact that came out from the theory is that we can achieve such stable embeddings via random
projection matrices.
Random projections for dimensionality reduction are not new and have extensively been used in
pattern recognition, clustering and data mining, see, e.g., [1,65–67]. More recently, the spirit underlying
compressed sensing has been exploited in the context of pattern recognition, too. In this application,
one needs not to return to the original high dimensional space, after the information-digging activity in
the low dimensional measurements subspace. Since the focus in pattern recognition is to identify the
class of an object/pattern, this can be performed in the measurements subspace, provided that there is
no class-related information loss. In [68], it is shown, using compressed sensing arguments, that if the
data is approximately linearly separable in the original high dimensional space and the data has a sparse
representation, even in an unknown basis, then projecting randomly in the measurements subspace
retains the structure of linear separability.

1304
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
Manifold learning is another area where random projections have been recently applied. A manifold
is, in general, a nonlinear k-dimensional surface, embedded in a higher dimensional (ambient) space.
For example, the surface of a sphere is a two-dimensional manifold in a three-dimensional space. More
on linear and nonlinear techniques for manifold learning can be found in, e.g., [1]. In [69,70], the
compressed sensing rationale is extended to signal vectors that live along a k-dimensional submanifold
of the space Rl. It is shown that choosing a matrix, X, to project and a sufﬁcient number, N, of mea-
surements, then the corresponding submanifold has a stable embedding in the measurements subspace,
under the projection matrix, X; that is, pairwise Euclidean and geodesic distances are approximately
preserved after the projection mapping. More on these issues can be found in the given references and
in, e.g., [63].
1.23.10.3 Sub-Nyquist sampling: analog-to-information conversion
In our discussion in the Remarks presented before, we touched a very important issue; that of going
from the analog domain to the discrete one. The topic of analog-to-digital (A/D) conversion has been at
the forefront of research and technology since the seminal works of Shannon, Nyquist, Whittaker and
Kotelnikof were published, see, for example, [71] for a thorough related review. We all know that if the
highest frequency of an analog signal, s(t), is less than F/2, then Shannon’s theorem suggests that no
loss of information is achieved if the signal is sampled, at least, at the Nyquist rate of F = 1/T , where
T is the corresponding sampling period, and the signal can be perfectly recovered by its samples
s(t) =

n
s(nT )sinc(Ft −n),
where sinc(·) is the sampling function
sinc(t) = sin (πt)
πt
.
While this has been the driving force behind the development of signal acquisition devices, the increas-
ing complexity of emerging applications demands increasingly higher sampling rates, that cannot be
accommodated by today’s hardware technology. This is the case, for example, in wideband communica-
tions, where conversion speeds, as dictated by Shannon’s bound, have become more and more difﬁcult
to obtain. Consequently, alternatives to high rate sampling are attracting a strong interest with the goal
to reduce the sampling rate by exploiting the underlying structure of the signals at hand. In many appli-
cations, the signal comprises a few frequencies or bands, see Figure 23.12 for an illustration. In such
cases, sampling at the Nyquist rate is inefﬁcient. This is an old problem and it has been addressed by a
number of authors, in the case where the locations of the nonzero bands in the frequency spectrum are
known, see, e.g., [72–74]. CS theory has inspired research to study cases where the locations (carrier
frequencies) of the bands are not known a priori. A typical application of this kind, of high practical
interest, lies within the ﬁeld of Cognitive radio, e.g., [75–77]. In contrast to what we have studied so far
in this paper, the sparsity now characterizes the analog signal, and this poses a number of challenges
that need to be addressed. In other words, one can consider that
s(t) =

i∈I
θiψi(t),

1.23.10 Compressed Sensing: The Glory of Randomness
1305
FIGURE 23.12
The Fourier transform of an analog signal, s(t), which is sparse in the frequency domain; only a limited num-
ber of frequency bands contribute to its spectrum content S(), where  stands for the angular frequency.
Nyquist’s theory guarantees that sampling at a frequency larger than or equal to twice the maximum max
is sufﬁcient to recover the original analog signal. However, this theory does not exploit information related
to the sparse structure of the signal in the frequency domain.
where ψi(t), i ∈I, are the functions that comprise the dictionary, and only a small subset of the
coefﬁcients θi are nonzero. Note that although each one of the dictionary functions can be of high
bandwidth, the true number of degrees of freedom of the signal is low. Hence, one would like to sample
the signal not at the Nyquist rate but at a rate determined by the sparsity level of the coefﬁcients’ set.
We refer to such a scenario as Analog-to-Information sampling or sub-Nyquist sampling.
An approach inspired directly by the theory of CS was ﬁrst presented in [78] and later improved and
theoretically developed in [61]. The approach builds around the assumption that the signal consists of a
sum of sinusoids and the random demodulator of Figure 23.11 is adopted. In [75,79], the more general
case of a signal consisting of a number of frequency bands, instead of tones, was treated. In addition,
the task of extracting each band of the signal from the compressed measurements, that enables (low
rate) baseband processing, is addressed. In principle, CS related theory would enable far fewer data
samples than traditionally required when capturing signals with relatively high bandwidth, but with a
low information rate. However, from a practical point of view, there are still a number of hardware
implementation related issues, such as sampling jitter, to be solved ﬁrst, e.g., [80,81].
An alternative path to sub-Nyquist sampling embraces a different class of analog signals known as
multipulse signals; that is, signals that consist of a stream of short pulses. Sparsity now refers to the
time domain, and such signals may not even be bandlimited. Signals of this type can be met in a number
of applications, such as in radar, ultrasound, bioimaging and neuronal signal processing, see, e.g., [82].
An approach, known as ﬁnite rate of innovation sampling, passes an analog signal, having k degrees
of freedom per second, through a linear time invariant ﬁlter and then samples at a rate of 2k samples
per second. Reconstruction is performed via rooting a high-order polynomial, see, e.g., [83,84] and the
references therein. In [85], the task of sub-Nyquist sampling is treated using CS theory arguments and
an expansion in terms of Gabor functions; the signal is assumed to consist of a sum of a few pulses of
ﬁnite duration, yet of unknown shape and time positions.
The task of sparsity-aware learning in the analog domain is still in its early stages and there is
currently a lot of on-going activity; more on this topic can be obtained in [86,87] and the references
there in.

1306
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
1.23.11 Sparsity-promoting algorithms
In the previous sections, our emphasis was to highlight the most important aspects underlying the
theory of sparse signal/vector recovery from an underdetermined set of linear equations. We now turn
our attention to the algorithmic aspects of the problem [30]. The issue now becomes that of discussing
efﬁcient algorithmic schemes, which can achieve the recovery of the unknown set of parameters. In
Sections 1.23.4 and 1.23.6, we saw that the constrained ℓ1 norm minimization (Basis Pursuit) can
be solved via Linear Programming techniques and the LASSO task via convex optimization schemes.
However,suchgeneralpurposetechniquestendtobeinefﬁcient,since,often,theyrequiremanyiterations
to converge and the respective computational resources can be excessive for practical applications,
especially in high dimensional spaces, Rl. As a consequence, a huge research effort has been invested
with the goal to develop efﬁcient algorithms, that are tailored-made to these speciﬁc tasks. This is still
a hot on-going area of research and deﬁnite conclusions are still risky to be drawn. Our aim here is to
provide the reader with some general trends and philosophies that characterize the related activity. We
will focus on the most commonly used and cited algorithms, which at the same time are structurally
simple and the reader can follow them, without requiring a deeper knowledge on optimization. Moreover,
these algorithms involve, in one way or another, arguments that are directly related to points and notions
that we have already used while presenting the theory; thus, they can also be exploited from a pedagogical
point of view, in order to strengthen the reader’s understanding of the topic. We start our review with
the class of batch algorithms, where all data are assumed to be available prior to the application of the
algorithm, and then we will move onto online/time-adaptive schemes. Furthermore, our emphasis is on
algorithms that are appropriate for any sensing matrix. This is stated in order to point out that in the
literature efﬁcient algorithms have also been developed for speciﬁc forms of highly structured sensing
matrices; exploiting their particular structure can lead to reduced computational demands, [88,89].
There are currently three rough types of families along which this algorithmic activity is growing:
(a) greedy algorithms, (b) iterative shrinkage schemes, and (c) convex optimization techniques. We have
used the word rough, since, in some cases, it may be difﬁcult to assign an algorithm to a speciﬁc family.
1.23.11.1 Greedy algorithms
Greedy algorithms have a long history, see, for example, [90] for a comprehensive list of references.
In the context of dictionary learning, a greedy algorithm known as Matching Pursuit was introduced in
[24]. A greedy algorithm is built upon a series of locally optimal single-term updates. In our context,
the goals are: (a) to unveil the “active” columns of the sensing matrix X; that is, those columns that
correspond to the nonzero locations of the unknown parameters and (b) to estimate the respective sparse
parameter vector. The set of indices which correspond to the nonzero vector components is also known
as the support. To this end, the set of active columns of X (and the support) is increased by one at each
iteration step. In the sequel, an updated estimate of the unknown sparse vector is obtained. Let us assume
that, at the (i −1)th iteration step, the algorithm has selected the columns denoted as x j1, x j2, . . . , x ji−1,
with j1, j2, . . . , ji−1 ∈{1, 2, . . . ,l}. These indices are the elements of the currently available support,
S(i−1). Let X(i−1) be the N × (i −1) matrix having x j1, x j2, . . . , x ji−1 as its columns. Let, also, the
current estimate of the solution be θ(i−1), which is a (i −1)-sparse vector, with zeros at all locations
with index outside the support.

1.23.11 Sparsity-Promoting Algorithms
1307
Algorithm 1 (Orthogonal Matching Pursuit (OMP)).
The algorithm is initialized with θ(0) := 0, e(0) := y and S(0) := ∅. At iteration step i, the following
computational steps are performed:
1. Select the column x ji of X, which is maximally correlated to (forms the least angle with) the
respective error vector, e(i−1) := y −Xθ(i−1), i.e.,
x ji : ji := arg max j=1,2,...,l
xT
j e(i−1)
  x j
  
2
.
2. Update the support and the corresponding set of active columns: S(i) = S(i−1) ∪{ ji}, and X(i) =
[X(i−1), x ji ].
3. Update the estimate of the parameter vector: Solve the Least-Squares (LS) problem that minimizes
the norm of the error, using the active columns of X only, i.e.,
˜θ := arg minz∈Ri
   y −X(i)z
   
2
2 .
Obtain θ(i) by inserting the elements of ˜θ in the respective locations ( j1, j2, . . . , ji), which comprise
the support (the rest of the elements of θ(i) retain their zero values).
4. Update the error vector
e(i) := y −Xθ(i).
The algorithm terminates if the norm of the error becomes less than a preselected user-deﬁned
constant, ϵ0. The following observations are in order.
Remarks 10.
•
Since θ(i), in Step 3, is the result of a LS task, it is known that the error vector is orthogonal to the
subspace spanned by the active columns involved, i.e.,
e(i)⊥span

x j1, . . . , x ji

.
This guarantees that taking the correlation, in the next step, of the columns of X with e(i) none of
the previously selected columns will be reselected; they result to zero correlation, being orthogonal
to e(i), see Figure 23.13.
•
It can be shown that the column, which has maximal correlation (maximum absolute value of the
inner product) with the currently available error vector, is the one that maximally reduces (compared
to any other column) the ℓ2 norm of the error, when y is approximated by linearly combining the
currently available active columns. This is the point where the heart of the greedy strategy beats.
This minimization is with respect to a single term, keeping the rest ﬁxed, as they have been obtained
from the previous iteration steps [8].
•
Starting with all the components being zero, if the algorithm stops after k0 iteration steps, the result
will be a k0-sparse solution.

1308
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
FIGURE 23.13
The error vector at the ith iteration is orthogonal to the subspace spanned by the currently available set of
active columns. Here is an illustration for the case of the 3-dimensional Euclidean space R3, and for i = 2.
•
Note that there is no optimality in this searching strategy. The only guarantee is that the ℓ2 norm of
the error vector is decreased at every iteration step. In general, there is no guarantee that the algorithm
can obtain a solution close to the true one, see, e.g., [91]. However, under certain constraints on the
structure of X, performance bounds can be obtained, see, e.g., [92–94].
•
The complexity of the algorithm amounts to O(k0lN) operations, which are contributed by the
computations of the correlations, plus the demands raised by the solution of the LS task, in Step
3, whose complexity depends on the speciﬁc algorithm used. The k0 is the sparsity level of the
delivered solution and, hence, the total number of iteration steps that are performed.
Another more qualitative argument, that justiﬁes the selection of the columns based on their correla-
tion with the error vector, is the following. Assume that the matrix X is orthonormal. Let also y = Xθ.
Then, y lies in the subspace spanned by the active columns of X, i.e., those which correspond to the
nonzero components of θ. Hence, the rest of the columns are orthogonal to y, since X is assumed to
be orthonormal. Taking the correlation of y, at the ﬁrst iteration step, with all the columns, it is certain
that one among the active columns will be chosen. The inactive columns result in zero correlation. A
similar argument holds true for all subsequent steps, since all the activity takes place in a subspace that
is orthogonal to all the inactive columns of X. In the more general case, where X is not orthonormal,
we can still use the correlation as a measure that quantiﬁes geometric similarity. The smaller the corre-
lation/the magnitude of the inner product is, the more orthogonal two vectors are. This brings us back
to the notion of mutual coherence, which is a measure of the maximum correlation (least angle) among
the columns of X.
1.23.11.1.1
OMP can recover optimal sparse solutions: sufﬁciency condition
We have already stated that, in general, there are no guarantees that OMP will recover optimal solutions.
However, when the unknown vector is sufﬁciently sparse, with respect to the structure of the sensing
matrix X, then OMP can exactly solve the ℓ0 minimization task in (23.27) and recover the solution in
k0 steps, where k0 is the sparsest solution that satisﬁes the associated linear set of equations.
Theorem 5.
Let the mutual coherence (Section 1.23.7.1) of the sensing matrix, X, be μ(X). Assume,
also, that the linear system, y = Xθ, accepts a solution such as
∥θ∥0 < 1
2

1 +
1
μ(X)

.
(23.47)

1.23.11 Sparsity-Promoting Algorithms
1309
Then, OMP guarantees to recover the sparsest solution in k0 = ∥θ∥0 steps.
We know from Section 1.23.7.1 that, under the previous condition, any other solution will be nec-
essarily less sparse. Hence, there is a unique way to represent y in terms of k0 columns of X. Without
harming generality, let us assume that the true support corresponds to the ﬁrst k0 columns of X, i.e.,
y =
k0

j=1
θ jx j,
θ j ̸= 0, ∀j ∈{1, . . . , k0}.
The theorem is a direct consequence of the following proposition:
Proposition 1.
If the condition (23.47) holds true, then the OMP algorithm will never select a column
with index outside the true support, see, e.g., [93]. In a more formal way, this is expressed as
ji = arg max j=1,2,...,l
xT
j e(i−1)
  x j
  
2
∈{1, . . . , k0}.
A geometric interpretation of this proposition is the following: if the angles formed between all the
possible pairs among the columns of X are large enough in the Rl space, which guarantees that μ(X) is
small enough, then y will lean more (form smaller angle) towards any one of the active columns, which
contribute to its formation, compared to the rest, which are inactive and do not participate in the linear
combination that generates y. Figure 23.14 illustrates the geometry, for the extreme case of mutually
orthogonal vectors (Figure 23.14a) and for the more general case, where the vectors are not orthogonal,
yet the angle between any pair of columns is large enough (Figure 23.14b).
In a nutshell, the previous proposition guarantees that, during the ﬁrst iteration, a column correspond-
ing to the true support will be selected. In a similar way, this is also true for all subsequent iterations. In
the second step, another, different from the previously selected column (as it has already been stated),
will be chosen. At step k0, the last remaining active column, corresponding to the true support, is selected
and this necessarily results to zero error. To this end, it sufﬁces to set ϵ0 equal to zero.
(a)
(b)
FIGURE 23.14
(a) In the case of an orthogonal matrix, the measurement vector y will be orthogonal to any inactive column;
here, x3. (b) In the more general case, it is expected to “lean” closer (form smaller angles) to the active than
to the inactive columns.

1310
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
1.23.11.1.2
The LARS algorithm
The Least Angle Regression (LARS) algorithm, [95], shares the ﬁrst two steps with OMP. It selects
ji to be an index outside the currently available active set so that to maximize the correlation with the
residual vector. However, instead of performing an LS ﬁt to compute the nonzero components of θ(i),
these are computed so that the residual to be equicorrelated with all the columns in the active set, i.e.,
|xT
j (y −Xθ(i))| = constant,
∀j ∈S(i),
where we have assumed that the columns of X are normalized, as it is common in practice (recall, also,
the Remarks 4). In other words, in contrast to the OMP, where the error vector is forced to be orthogonal
to the active columns, LARS demands this error to form equal angles with each one of them. Likewise
OMP, it can be shown that, provided the target vector is sufﬁciently sparse and under incoherence of
the columns of X, LARS can exactly recover the sparsest solution, [96].
A further small modiﬁcation leads to the so-called LARS-LASSO algorithm. According to this
version, a previously selected index in the active set can be removed at a later stage. This gives the
algorithm the potential to “recover” from a previously bad decision. Hence, this modiﬁcation departs
from the strict rationale that deﬁnes the greedy algorithms. It turns out that this version solves the LASSO
optimization task. This algorithm is the same as the one suggested in [97] and it is known as homotopy
algorithm. Homotopy methods are based on a continuous transformation from one optimization task
to another. The solutions to this sequence of tasks lie along a continuous parameterized path. The idea
is that, while the optimization tasks may be difﬁcult to solve by themselves, one can trace this path of
solutions by slowly varying the parameters. For the LASSO task, it is the λ parameter which is varying,
see, e.g., [98–100]. Take as an example the LASSO task in its regularized version in (23.13). For λ = 0,
the task minimizes the ℓ2 norm and for λ →∞the task minimizes the ℓ1 norm, and for this case the
solution tends to zero. It turns out that the solution path, as λ changes from large to small values, is
polygonal. Vertices on this solution path correspond to vectors having nonzero elements only on a subset
of entries. This subset remains unchanged, till λ reaches the next critical value, which corresponds to a
new vertex of the polygonal path and to a new subset of potential nonzero values. Thus, the solution is
obtained via this sequence of steps along this polygonal path.
1.23.11.1.3
Compressed Sensing Matching Pursuit (CSMP) algorithms
Strictly speaking, these algorithms are not greedy, yet, as it is stated in [89], they are at heart greedy
algorithms. Instead of performing a single term optimization per iteration step, in order to increase the
support by one, as it is the case with OMP, these algorithms attempt to obtain ﬁrst an estimate of the
support and then use this information to compute a least squares estimate of the target vector, constrained
on the respective active columns. The quintessence of the method lies in the near-orthogonal nature of
the sensing matrix, assuming that this obeys the RIP.
Assume that X obeys the RIP for some small enough value δk and sparsity level, k, of the unknown
vector. Let, also, that the measurements are exact, i.e., y = Xθ. Then, X T y = X T Xθ ≈θ. Therefore,
intuition indicates that it is not unreasonable to select, in the ﬁrst iteration step, the t (a user-deﬁned
parameter) largest in magnitude components of X T y as indicative of the nonzero positions of the sparse
target vector. This reasoning carries on for all subsequent steps, where, at the ith iteration, the place of
y is taken by the residual e(i−1) := y −Xθ(i−1), where θ(i−1) indicates the estimate of the target vector

1.23.11 Sparsity-Promoting Algorithms
1311
at the (i −1)th iteration. Basically, this could be considered as a generalization of the OMP. However,
as we will soon see, the difference between the two mechanisms is more substantial.
Algorithm 2 (The CSMP Scheme).
1. Select the value of t.
2. Initialize the algorithm: θ(0) = 0, e(0) = y.
3. For i = 1, 2, . . ., execute the following.
a. Obtain the current support:
S(i) := supp

θ(i−1)
∪
indices of the t largest in magnitude
components of X T e(i−1)

.
b. Select the active columns: Construct X(i) to comprise the active columns of X in accordance to
S(i). Obviously, X(i) is a N × r matrix, where r denotes the cardinality of the support set S(i).
c. Update the estimate of the parameter vector: solve the LS task
˜θ := arg maxz∈Rr
   y −X(i)z
   
2
2 .
Obtain ˆθ
(i) ∈Rl having the r elements of ˜θ in the respective locations, as indicated by the
support, and the rest of the elements being zero.
d. θ(i) := Hk

ˆθ
(i)
. The mapping Hk denotes the hard thresholding operator; that is, it returns a
vector with the k largest in magnitude components of the argument, and the rest are forced to
zero.
e. Update the error vector: e(i) = y −Xθ(i).
The algorithm requires as input the sparsity level k. Iterations carry on until a halting criterion is
met. The value of t, that determines the largest in magnitude values in Steps 1 and 3a, depends on the
speciﬁc algorithm. In CoSaMP (Compressive Sampling Matching Pursuit, [89]), t = 2k and in the SP
(Subspace Pursuit, [101]), t = k.
Having stated the general scheme, a major difference with OMP becomes readily apparent. In OMP,
only one column is selected per iteration step. Moreover, this remains in the active set for all subsequent
steps. If, for some reason, this was not a good choice, the scheme cannot recover from such a bad
decision. In contrast, the support and hence the active columns of X are continuously updated in CSMP
and the algorithm has the ability to correct a previously bad decision, as more information is accumulated
and iterations progress. In [101], it is shown that if the measurements are exact (y = Xθ) then SP can
recover the k-sparse true vector in a ﬁnite number of iteration steps, provided that X satisﬁes the RIP
with δ3k < 0.205. If the measurements are noisy, performance bounds have been derived, which hold
true for δ3k < 0.083. For the CoSaMP, performance bounds have been derived for δ4k < 0.1.
1.23.11.2 Iterative shrinkage algorithms (IST)
This family of algorithms have also a long history, see, e.g., [102–105]. However, in the “early” days,
the developed algorithms had some sense of heuristic ﬂavor, without establishing a clear bridge with

1312
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
optimizing a cost function. Later attempts were substantiated by sound theoretical arguments concerning
issues such as convergence and convergence rate, e.g., [106–109].
The general form of this algorithmic family has a striking resemblance with the classical linear
algebra iterative schemes for approximating the solution of large linear systems of equations, known as
stationary iterative or iterative relaxation methods. The classical Gauss-Seidel and Jacobi algorithms,
e.g., [110], in numerical analysis can be considered as members of this family. Given a linear system
of l equations with l unknowns, z = Ax, the basic iteration at step i has the following form
x(i) =

I −Q A

x(i−1) + Qz
= x(i−1) + Qe(i−1),
e(i−1) := z −Ax(i−1),
which does not come as a surprise. It is of the same form as most of the iterative schemes for numerical
solutions! The matrix Q is chosen so that to guarantee convergence and different choices lead to
different algorithms with their pros and cons. It turns out that this algorithmic form can also be applied
to underdetermined systems of equations, y = Xθ, with a “minor” modiﬁcation, which is imposed
by the sparsity constraint of the target vector. This leads to the following general form of iterative
computation
θ(i) = Ti

θ(i−1) + Qe(i−1)
,
e(i−1) = y −Xθ(i−1),
starting from an initial guess of θ(0) (usually θ(0) = 0, e(0) = y). In certain cases, Q can be made to
be iteration-dependent. The operator Ti(·) is a nonlinear thresholding operator, that is applied entry-
wise, i.e., component-wise. Depending on the speciﬁc scheme, this can be either the hard thresholding
operator, denoted as Hk, or the soft thresholding operator, denoted as Sα. Hard thresholding, as we
already know, keeps the k largest components of a vector unaltered and sets the rest equal to zero. Soft
thresholding was introduced in Section 1.23.4. All components with magnitude less than α are forced
to zero and the rest are reduced in magnitude by α; that is, the jth component of a vector, θ, after soft
thresholding becomes
(Sα(θ)) j = sgn(θ j)(|θ j| −α)+.
Depending on (a) the choice of Ti, (b) the speciﬁc value of the parameter k or α, and (c) the matrix
Q, different instances occur. A most common choice for Q is μX T and the generic form of the main
iteration becomes
θ(i) = Ti

θ(i−1) + μX T e(i−1)
,
(23.48)
where μ is a relaxation (user-deﬁned) parameter, which can also be left to vary with each iteration step.
The choice of X T is intuitively justiﬁed, once more, by the near-orthogonal nature of X. For the ﬁrst
iteration step and for a linear system of the form y = Xθ, starting from a zero initial guess, we have
X T y = X T Xθ ≈θ and we are close to the solution.
Although intuition is most important in scientiﬁc research, it is not enough, by itself, to justify
decisions and actions. The generic scheme in (23.48) has been reached from different paths, following
different perspectives that lead to different choices of the respective parameters. Let us spend some more
time on that, with the aim to make the reader more familiar with techniques that address optimization

1.23.11 Sparsity-Promoting Algorithms
1313
tasks of non-differentiable loss functions. The term in the parenthesis in (23.48) coincides with the
gradient descent iteration step if the cost function were the unregularized LS loss, i.e.,
J(θ) = 1
2 ∥y −Xθ∥2
2 .
In this case, the gradient descent rationale leads to
θ(i−1) −μ
∂J

θ(i−1)
∂θ
= θ(i−1) −μX T (Xθ(i−1) −y)
= θ(i−1) + μX T e(i−1).
It is well known and it can easily be shown that the gradient descent can alternatively be viewed as the
result of minimizing a regularized version of the linearized loss function, i.e.,
θ(i) = arg minθ∈Rl

J

θ(i−1)
+

θ −θ(i−1)T ∂J

θ(i−1)
∂θ
+
1
2μ
   θ −θ(i−1)   
2
2

.
(23.49)
One can adopt this view of the gradient descent philosophy as a kick-off point to minimize iteratively
the following LASSO task, i.e.,
min
θ∈Rl

L(θ, λ) = 1
2 ∥y −Xθ∥2
2 + λ ∥θ∥1 = J(θ) + λ ∥θ∥1

.
The difference now is that the loss function comprises two terms. One which is smooth (differentiable)
and a non-smooth one. Let the current estimate be θ(i−1). The updated estimate is obtained by
θ(i) = arg minθ∈Rl

J

θ(i−1)
+

θ −θ(i−1)T ∂J(θ(i−1))
∂θ
+
1
2μ
   θ −θ(i−1)   
2
2 + λ ∥θ∥1

,
which, after ignoring constants, becomes
θ(i) = arg minθ∈Rl
1
2
   θ −˜θ
   
2
2 + λμ ∥θ∥1

,
(23.50)
where
˜θ := θ(i−1) −μ∂J(θ(i−1))
∂θ
.
(23.51)
Following exactly the same steps as those that led to the derivation of (23.20) from (23.13) (after
replacing ˆθ LS with ˜θ) we obtain
θ(i) = Sλμ(˜θ) = Sλμ
	
θ(i−1) −μ∂J(θ(i−1))
∂θ

(23.52)
= Sλμ

θ(i−1) + μX T e(i−1)
.
(23.53)

1314
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
This is very interesting and practically useful. The only effect of the presence of the non-smooth ℓ1
norm in the loss function is an extra simple thresholding operation, which as we know is an operation
performed individually on each component. It can be shown, e.g., [111], that this algorithm converges
to a minimizer θ∗of the LASSO (23.13), provided that μ ∈(0, 1/λmax(X T X)), where λmax(·) denotes
the maximum eigenvalue of X T X. The convergence rate is dictated by the rule
L(θ(i), λ) −L(θ∗, λ) ≈O(1/i),
which is known as sublinear global rate of convergence. Moreover, it can be shown that
L(θ(i), λ) −L(θ∗, λ) ≤
C
   θ(0) −θ∗
   
2
2
2i
.
The latter result indicates that if one wants to achieve an accuracy of ϵ, then this can be obtained by at
most
$
C
   θ(0)−θ∗
   
2
2
2ϵ
%
iterations, where ⌊·⌋denotes the ﬂoor operator.
In [107], (23.48) was obtained from a nearby corner, building upon arguments from the classi-
cal proximal-point methods in optimization theory, e.g., [112]. The original LASSO regularized cost
function is modiﬁed to the so called surrogate objective,
J(θ, ˜θ) = 1
2 ∥y −Xθ∥2
2 + λ ∥θ∥1 + 1
2d(θ, ˜θ),
where
d(θ, ˜θ) := c
   θ −˜θ
   
2
2 −
   Xθ −X ˜θ
   
2
2 .
If c is appropriately chosen (larger than the largest eigenvalue of X T X), the surrogate objective is
guaranteed to be strictly convex. Then it can be shown that the minimizer of the surrogate objective is
given by
ˆθ = Sλ/c

˜θ + 1
c X T (y −X ˜θ)

.
(23.54)
In the iterative formulation, ˜θ is selected to be the previously obtained estimate; in this way, one
tries to keep the new estimate close to the previous one. The procedure readily results to our generic
scheme in (23.48), using soft thresholding with parameter λ/c. It can be shown that such a strategy
converges to a minimizer of the original LASSO problem. The same algorithm was reached in [109],
using majorization-minimization techniques from optimization theory. So, from this perspective, the
IST family has strong ties with algorithms that belong to the convex optimization category.
In [31], the Sparse Reconstruction by Separable Approximation (SpaRSA) algorithm is proposed,
which is a modiﬁcation of the standard IST scheme. The starting point is (23.49); however, the multiply-
ing factor,
1
2μ, instead of being constant is now allowed to change from iteration to iteration according
to a rule. This results in a speed up in the convergence of the algorithm. Moreover, inspired by the homo-
topy family of algorithms, where λ is allowed to vary, SpaRSA can be extended to solve a sequence of
problems which are associated with a corresponding sequence of values of λ. Once a solution has been

1.23.11 Sparsity-Promoting Algorithms
1315
obtained for a particular value of λ, it can be used as a “warm-start” for a nearby value. Solutions can
therefore be computed for a range of values, at a small extra computational cost, compared to solving
for a single value from a “cold start.” This technique abides with to the so-called continuation strategy,
which has been used in the context of other algorithms as well, e.g., [113]. Continuation has been shown
to be a very successful tool to increase the speed of convergence.
An interesting modiﬁcation of the basic IST scheme has been proposed in [111], which improves the
convergence rate to O(1/i2), by only a simple modiﬁcation with almost no extra computational burden.
The scheme is known as Fast Iterative Shrinkage-Thresholding Algorithm (FISTA). This scheme is an
evolution of [114], which introduced the basic idea for the case of differentiable costs, and consists of
the following steps:
θ(i) = Sλμ

z(i) + μX T 
y −Xz(i)
,
z(i+1) := θ(i) + ti −1
ti+1

θ(i) −θ(i−1)
,
where
ti+1 :=
1 +
&
1 + 4t2
i
2
,
with initial points t1 = 1 and z(1) = θ(0). In words, in the thresholding operation, θ(i−1) is replaced by
z(i), which is a speciﬁc linear combination of two successive updates of θ. Hence, at a marginal increase
of the computational cost, a substantial increase in convergence speed is achieved.
In [115] the hard thresholding version has been used, with μ = 1 and the thresholding operator Hk
uses the sparsity level k of the target solution, that is assumed to be known. In a later version, [116], the
relaxation parameter is left to change so that, at each iterations step, the error is maximally reduced. It
has been shown that the algorithm converges to a local minimum of the cost function ∥y −Xθ∥2, under
the constraint that θ is a k-sparse vector. Moreover, the latter version is a stable one and it results to a
near optimal solution if a form of RIP is fulﬁlled.
A modiﬁed version of the generic scheme given in (23.48), that evolves along the lines of [117],
obtains the updates component-wise, one vector component at a time. Thus, a “full” iteration consists
of l steps. The algorithm is known as coordinate descent and its basic iteration has the form
θ(i)
j
= Sλ/∥x j∥2
2
	
θ(i−1)
j
+
xT
j e(i−1)
  x j
  2
2

,
j = 1, 2, . . . ,l.
(23.55)
This algorithm replaces the constant c, in the previously reported soft thresholding algorithm, with
the norm of the respective column of X, if the columns of X are not normalized to unit norm. It has
been shown that the parallel coordinate descent algorithm also converges to a LASSO minimizer of
(23.13), [108]. Improvements of the algorithm, using line search techniques to determine the most
descent direction for each iteration, have also been proposed, see, [118].
The main contribution to the complexity for the iterative shrinkage algorithmic family comes from
the two matrix-vector products, which amounts to O(Nl), unless X has a special structure, e.g., DFT,
that can be exploited to reduce the load.

1316
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
In [119], the so-called Two Stage Thresholding (TST) scheme is presented, which brings together
arguments from the iterative shrinkage family and the OMP. This algorithmic scheme involves two
stages of thresholding. The ﬁrst step is exactly the same as in (23.48). However, this is now used
only for determining “signiﬁcant” nonzero locations, just as in Compressed Sensing Matching Pursuit
(CSMP) algorithms, presented in the previous subsection. Then, a LS problem is solved to provide
the updated estimate, under the constraint of the available support. This is followed by a second step
of thresholding. The thresholding operations in the two stages can be different. If hard thresholding,
Hk, is used in both steps, this results to the algorithm proposed in [120]. For this latter scheme, con-
vergence and performance bounds are derived if the RIP holds for δ3k < 0.58. In other words, the
basic difference between the TST and CSMP approaches is that, in the latter case, the most signiﬁcant
nonzero coefﬁcients are obtained by looking at the correlation term X T e(i−1) and in the TST family at
θ(i−1) + μX T e(i−1). The differences among different approaches can be minor and the crossing lines
among the different algorithmic categories are not necessarily crispy clear. However, from a practical
point of view, sometimes small differences may lead to substantially improved performance.
Remarks 11.
•
The minimization in (23.52) bridges the IST algorithmic family with another powerful tool in
convex optimization, which builds upon the notion of proximal mapping or Moreau envelopes, see,
e.g., [112,121]. Given a convex function h : Rl →R, and a μ > 0, the proximal mapping,
Proxμh : Rl →Rl, with respect to h, and of index μ, is deﬁned as the (unique) minimizer
Proxμh(x) := arg minu∈Rl

h(u) + 1
2μ ∥x −u∥2
2

,
∀x ∈Rl.
(23.56)
Let us now assume that we want to minimize a convex function, which is given as the sum
f (θ) = J(θ) + h(θ),
where J(·) is convex and differentiable, and h(·) is also a convex, but not necessarily a smooth one.
Then it can be shown that the following iterations converge to a minimizer of f (·),
θ(i) = Proxμh
	
θ(i−1) −μ∂J(θ(i−1))
∂θ

,
(23.57)
where μ > 0 and it can also be made iteration dependent, i.e., μi > 0. If we now use this scheme
to minimize our familiar cost, i.e.,
J(θ) + λ ∥θ∥1 ,
we obtain (23.52); this is so, because the proximal operator of h(θ) := λ ∥θ∥1 is shown ([106,121])
to be identical to the soft thresholding operator, i.e.,
Proxh(θ) = Sλ(θ).
In order to feel more comfortable with this operator, note that if h(x) := 0, its proximal operator is
equal to x, and in this case (23.57) becomes our familiar gradient descent algorithm.

1.23.11 Sparsity-Promoting Algorithms
1317
•
All the non-greedy algorithms, which have been discussed so far, have been developed to solve the
task deﬁned in the formulation (23.13). This is mainly because this is an easier task to solve; once λ
has been ﬁxed, it is an unconstrained optimization task. However, there are algorithms which have
been developed to solve the alternative formulations.
The NESTA algorithm has been proposed in [122] and solves the task in its (23.15) formulation.
Adopting this path can have an advantage since ϵ may be given as an estimate of the uncertainty
associated with the noise, which can readily be obtained in a number of practical applications. In
contrast, selecting a priori the value for λ is more intricate. In [7], the value λ = ση
√
2 ln l, where
ση is the noise standard deviation, is argued to have certain optimality properties; however this
argument hinges on the assumption of the orthogonality of X. NESTA relies heavily on Nesterov’s
generic scheme ([114]), hence its name. The original Nesterov’s algorithm performs a constrained
minimization of a smooth convex function f (θ), i.e.,
min
θ∈Q f (θ),
where Q is a convex set, and in our case this is associated with the quadratic constraint in (23.15).
The algorithm consists of three basic steps. The ﬁrst one is similar with the step in (23.49), i.e.,
w(i) = arg minθ∈Q
⎧
⎨
⎩

θ −θ(i−1)T ∂J

θ(i−1)
∂θ
+ L
2
   θ −θ(i−1)   
2
2
⎫
⎬
⎭,
(23.58)
where L is an upper bound on the Lipschitz coefﬁcient, which the gradient of f (·) has to satisfy. The
difference with (23.49) is that the minimization is now a constrained one. However, Nesterov has
also added a second step involving another auxiliary variable, z(i), which is computed in a similar
way as w(i) but the linearized term is now replaced by a weighted cumulative gradient,
i−1

k=0
αk

θ −θ(k)T ∂J

θ(k)
∂θ
.
The effect of this term is to smooth out the “zig-zagging” of the path towards the solution, whose
effect is to increase signiﬁcantly the convergence speed. The ﬁnal step of the scheme involves an
averaging of the previously obtained variables,
θ(i) = tiz(i) + (1 −ti)w(i).
The values of the parameters αk, k = 0, . . . , i −1, and ti result from the theory so that convergence
is guaranteed. As it was the case with its close relative FISTA, the algorithm enjoys an O(1/i2)
convergence rate. In our case, where the function to be minimized, ∥θ∥1, is not smooth, NESTA
uses a smoothed prox-function of it. Moreover, it turns out that close-form updates are obtained for
z(i) and w(i). If X is chosen so that to have orthonormal rows, the complexity per iteration is O(l)
plus the computations needed for performing the product X T X, which is the most computational
thirsty part. However, this complexity can substantially be reduced if the sensing matrix is chosen to

1318
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
be a submatrix of a unitary transform, which admits fast matrix-vector product computations, e.g.,
a subsampled DFT matrix. For example, for the case of a subsampled DFT matrix, the complexity
amounts to O(l) plus the load to perform the two Fast Fourier Transforms (FFT). Moreover, the
continuation strategy can also be employed to accelerate convergence. In [122], it is demonstrated
that NESTA exhibits good accuracy results, while retaining a complexity that is competitive with
algorithms developed around the (23.15) formulation and scales in an affordable way for large size
problems. Furthermore, NESTA, and in general Nesterov’s scheme, enjoy a generality that allows
their use to other optimization tasks as well.
•
The task in (23.14) has been considered in [97,123]. In the former, the algorithm comprises a projec-
tion on the l1 ball ∥θ∥1 ≤ρ (see also, Section 1.23.13.4) per iteration step. The most computationally
dominant part of the algorithm consists of matrix-vector products. In [97], a homotopy algorithm is
derived for the same task, where now the bound ρ becomes the homotopy parameter which is left
to vary. This algorithm is also referred as the LARS-LASSO, as it has already been reported before.
1.23.11.3 Which algorithm then: some practical hints
We have already discussed a number of algorithmic alternatives to obtain solutions to the ℓ0 or ℓ1 norm
minimization tasks. Our focus was on schemes whose computational demands are rather low and they
scale well to very large problem sizes. We have not touched more expensive methods such as interior
point methods for solving the ℓ1 convex optimization task. A review of such methods is provided in
[124]. Interior point methods evolve along the Newton-type recursion and their complexity per iteration
step is at least of the order O(l3). As it is most often the case, there is a trade off. Schemes of higher
complexity tend to result in enhanced performance. However, such schemes become impractical in
problems of large size. Some examples of other algorithms, that were not discussed, can be found in
[31,123,125,126]. Talking about complexity, it has to be pointed out that what really matters at the
end is not so much the complexity per iteration step but the overall required resources in computer
time/memory for the algorithm to converge to a solution within a speciﬁed accuracy. For example, an
algorithm may be of low complexity per iteration step but it may need an excessive number of iterations
to converge.
Computational load is only one among a number of indices that characterize the performance of
an algorithm. Other performance measures, refer to convergence rate, tracking speed (for the adaptive
algorithms), and stability with respect to the presence of noise and/or ﬁnite word length computations.
No doubt, all these performance measures are also of interest here, too. However, there is an additional
aspect that is of particular importance when quantifying performance of sparsity-promoting algorithms.
This is related to the so called undersampling-sparsity tradeoff or the phase transition curve.
One of the major issues, on which we focused in this chapter, was to derive and present the conditions
thatguaranteeuniquenessoftheℓ0 minimizationanditsequivalencewiththeℓ1 minimizationtask,under
an underdetermined set of measurements, y = Xθ, for the recovery of sparse enough signals/vectors.
While discussing the various algorithms in this section, we reported a number of different RIP-related
conditions that some of the algorithms have to satisfy in order to recover the target sparse vector.
As a matter of fact, it has to be admitted that this was quite confusing, since each algorithm had to
satisfy its own conditions. In addition, in practice, these conditions are not easy to be veriﬁed. Although
such results are, no doubt, important to establish convergence and make us more conﬁdent and also

1.23.11 Sparsity-Promoting Algorithms
1319
FIGURE 23.15
For any algorithm, the transition between the regions of 100% success and of a complete failure is very
sharp. For the algorithm corresponding to the middle curve, this transition occurs at higher sparsity values
and, from this point of view, it is a better algorithm than the one associated with the curve on the left. Also,
given a algorithm, the higher the dimensionality the higher the sparsity level where this transition occurs, as
indicated by the middle and the dotted curve on the right.
understand better why and how an algorithm works, one needs further experimental evidence in order
to establish good performance bounds for an algorithm. Moreover, all the conditions that we have dealt
with, including coherence and RIP, are sufﬁcient conditions. In practice, it turns out that sparse signal
recovery is possible with sparsity levels much higher than those predicted by the theory, for given N
and l. Hence, proposing a new algorithm or selecting an algorithm from an available palette, one has
to demonstrate experimentally the range of sparsity levels that can be recovered by the algorithm, as a
percentage of the number of measurements and the dimensionality. Thus, in order to select an algorithm,
one should cast his/her vote for the algorithm which, for given l and N, has the potential to recover
k-sparse vectors with k being as high as possible, for most of the cases, that is, with high probability.
Figure 23.15 illustrates the type of curve that is expected to result in practice. The vertical axis is
the probability of exact recovery of a target k-sparse vector and the horizontal axis shows the ratio
k/N, for a given number of measurements, N, and the dimensionality of the ambient space, l. Three
curves are shown. The middle curve and the one on the right correspond to the same algorithm, for two
different values of the dimensionality, l, and the curve on the left corresponds to another algorithm.
Curves of this shape are expected to result from experiments of the following set up. Assume that we
are given a sparse vector, θ0, with k nonzero components in the l-dimensional space. Using a sensing
matrix X, we generate N measurements y = Xθ0. The experiment is repeated a number of, say, M
times, each time using a different realization of the sensing matrix and a different k-sparse vector. For
each instance, the algorithm is run to recover the target sparse vector. This is not always possible. We
count the number, m, of successful recoveries, and compute the corresponding percentage of successful
recovery (probability), m/M, which is plotted on the vertical axis of Figure 23.15. The procedure is
repeated for a different value of k, 1 ≤k ≤N. A number of issues now jump into the stage: (a) How
one selects the ensemble of sensing matrices and (b) how one selects the ensemble of sparse vectors.
There are different scenarios and some typical examples are described next.
1. The N × l sensing matrices X are formed by:

1320
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
a. Different i.i.d. realizations with elements drawn from a Gaussian N(0, 1/N).
b. Different i.i.d. realizations from the uniform distribution on the unit sphere in RN, which is also
known as the uniform spherical ensemble.
c. Different i.i.d. realizations with elements drawn from Bernoulli type distributions.
d. Different i.i.d. realizations of partial Fourier matrices, each time using a different set of N rows.
2. The k-sparse target vector θ0 is formed by selecting the locations of at most k nonzero elements
randomly, by “tossing a coin” with probability p = k/l, and ﬁll the values of the nonzero elements
according to a statistical distribution, e.g., Gaussian, uniform, double exponential, Cauchy.
Other scenarios are also possible. Some authors set all nonzero values to one, [66], or to ±1, with
the randomness imposed on the choice of the sign. It must be stressed out that the performance of an
algorithm may vary signiﬁcantly under different experimental scenarios, and this may be indicative of
the stability of an algorithm. In practice, a user may be interested in a speciﬁc scenario, which is more
representative of the available data.
LookingatFigure23.15,thefollowingconclusionsareinorder.Inallcurves,thereisasharptransition
between two levels. From the 100% success to the 0% success. Moreover, the higher the dimension-
ality, the sharper the transition is. This has also been shown theoretically in [127]. For the algorithm
corresponding to the two curves on the right, this transition occurs at higher values of k, compared to
the algorithm that generates the curve on the left. Provided that the computational complexity of the
former algorithm can be accommodated by the resources, which are available for a speciﬁc application,
this seems to be the more sensible choice between the two algorithms. However, if the resources are
limited, concessions are unavoidable.
Another way to “interrogate” and demonstrate the performance of an algorithm, with respect to its
robustness to the range of values of sparsity levels that can be successfully recovered, is via the so-called
phase transition curve. To this end deﬁne:
•
α := N
l , which is a normalized measure of the problem indeterminacy.
•
β := k
N , which is a normalized measure of sparsity.
In the sequel, plot a graph having α ∈[0, 1] in the horizontal axis and β ∈[0, 1] in the vertical
one. For each point, (α, β), in the [0, 1] × [0, 1] region, compute the probability of the algorithm
to recover a k-sparse target vector. In order to compute the probability, one has to adopt one of the
previously stated scenarios. In practice, one has to form a grid of points that cover densely enough the
region [0, 1] × [0, 1] in the graph. Use a varying intensity level scale to color the corresponding (α, β)
point. Gray (lower region) corresponds to probability one and red (upper region) to probability zero.
Figure 23.16, illustrates the type of graph that is expected to be recovered in practice, for large values
of l. That is, the transition from the region (phase) of “success” (gray/lower region) to that of “fail”
(red/upper region) is very sharp. As a matter of fact, there is a curve that separates the two regions. The
theoretical aspects of this curve have been studied in the context of combinatorial geometry in [127] for
the asymptotic case, l →∞, and in [128] for ﬁnite values of l. Observe that the larger the value of α
(larger percentage of measurements) the larger the value of β at which the transition occurs. This is in
line with what we have said so far in this paper, and the problem gets increasingly harder as one moves

1.23.11 Sparsity-Promoting Algorithms
1321
FIGURE 23.16
Typical phase transition behavior of a sparsity promoting algorithm. Gray (lower region) corresponds to 100%
success of recovering the sparsest solution and red (upper region) to 0%. For high dimensional spaces, the
transition is very sharp, as it is the case in the ﬁgure. For lower dimensionality spaces, the transition from
gray (lower region) to red (upper region) is smoother and involves a region of varying color intensity. (For
interpretation of the references to color in this ﬁgure legend, the reader is referred to the web version of this
book.)
up and to the left in the graph. In practice, for smaller values of l, the transition region between the two
regions is smoother, and it gets narrower as l increases. In practice, one can draw an approximate curve
that separates the “success” and “fail” regions, using regression techniques, see, e.g., [119].
The reader may already be aware of the fact that, so far, we have avoided to talk about the performance
of individual algorithms. We have just discussed some “typical” behavior that algorithms tend to exhibit
in practice. What the reader might have expected is to discuss comparative performance tests and
draw related conclusions. We have not done it since we feel that it is early in time to have “deﬁnite”
performance conclusions, and this ﬁeld is still in an early stage. Most authors compare their newly
suggested algorithm with a few other algorithms, usually within a certain algorithmic family and,
more important, under some speciﬁc scenarios, where the advantages of the newly suggested algorithm
are documented. However, the performance of an algorithm can change signiﬁcantly by changing the
experimental scenario, under which the tests are carried out. The most comprehensive comparative
performance study, so far, has been carried out in [119]. However, even in this one, the scenario of
exact measurements has been considered and there are no experiments concerning the robustness of
individual algorithms to the presence of noise. It is important to say that this study involved a huge effort
of computation. We will comment on some of the ﬁndings from this study, which will also reveal to the
reader that different experimental scenarios can signiﬁcantly affect the performance of an algorithm.
Figure 23.17a shows the obtained phase transition curves for (a) the iterative hard thresholding
(IHT), (b) the iterative soft thresholding scheme of (23.48) (IST), (c) the Two-Stage-Thresholding
scheme (TST), as discussed earlier on, (d) the LARS, and (e) the OMP algorithms, together with the

1322
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
(a)
(c)
(b)
FIGURE 23.17
(a) The obtained phase transition curves for different algorithms under the same experimental scenario,
together with the theoretical one. (b) Phase transition curve for the IST algorithm under different experimental
scenarios for generating the target sparse vector. (c) The phase transition for the IST algorithms under
different experimental scenarios for generating the sensing matrix X .
theoretically obtained one using ℓ1 minimization. All algorithms were tuned with the optimal values,
with respect to the required user-deﬁned parameters, after extensive experimentation. The results in the
Figure correspond to the uniform spherical scenario, for the generation of the sensing matrices. Sparse
vectors were generated according to the ±1 scenario, for the nonzero coefﬁcients. The interesting
observation is that, although the curves deviate from each other as we move to larger values of β,
for smaller values, the differences in their performance become less and less. This is also true for
computationally simple schemes, such as the IHT one. The performance of LARS is close to the

1.23.11 Sparsity-Promoting Algorithms
1323
optimal one. However, this comes at the cost of computational increase. The required computational
time for achieving the same accuracy, as reported in [119], favor the TST algorithm. In some cases,
LARS required excessively longer time to reach the same accuracy, in particular when the sensing
matrix was the partial Fourier one and fast schemes to perform matrix vector products can be exploited.
For such matrices, the thresholding schemes (IHT, IST, TST) exhibited a performance that scales very
well to large size problems.
Figure 23.17b indicates the phase transition curve for one of the algorithms (IST) as we change
the scenarios for generating the sparse (target) vectors, using different distributions; (a) ±1, with
equiprobable selection of signs (Constant Amplitude Random Selection (CARS)), (b) double expo-
nential (power), (c) Cauchy, and (d) uniform in [−1, 1]. This is indicative and typical for other algo-
rithms as well, with some of them being more sensitive than others. Finally, Figure 23.17c shows
the transition curves for the IST algorithm by changing the sensing matrix generation scenario. Three
curves are shown corresponding to (a) uniform spherical ensemble (USE), (b) random sign ensemble
(RSE), where the elements are ±1 with signs uniformly distributed, and (c) the uniform random pro-
jection (URP) ensemble. Once more, one can observe the possible variations that are expected due
to the use of different matrix ensembles. Moreover, changing ensembles affects each algorithm in a
different way.
Concluding this section it must be emphasized that the ﬁeld of algorithmic development is still an
ongoing research ﬁeld and it is early to come with deﬁnite and concrete comparative performance
conclusions. Moreover, besides the algorithmic front, existing theories often fall short to predict what
is observed in practice, with respect to their phase transition performance. For a related discussion, see,
e.g., [43].
Example 5.
We are given a set of N = 20 measurements stacked in the y ∈RN vector. These
were taken by applying a sensing matrix X on an “unknown” vector in R50, which is known to be
sparse with k = 5 nonzero components; the location of these nonzero components in the unknown
vector are not known. The sensing matrix was a random matrix with elements drawn from a normal
distribution N(0, 1) and then the columns were normalized to unit norm. There are two scenarios for
the measurements. In the ﬁrst one, we are given the exact measurements while in the second one white
Gaussian noise of variance σ 2 = 0.025 was added.
In order to recover the unknown sparse vector, the CoSaMP algorithm was used for both scenarios.
The results are shown in Figures 23.18a and 23.18b for the noiseless and noisy scenarios, respectively.
The values of the true unknown vector θ are represented with black stems topped with open circles. Note
that all but ﬁve of them are zero. In Figure 23.18a exact recovery of the unknown values is succeeded;
the estimated values of θi, i = 1, 2, . . . , 50, are indicated with squares in red color (for the web version).
In the noisy case of Figure 23.18b, the resulted estimates, which are denoted with squares, deviate from
the correct values. Note that estimated values very close to zero (|θ| ≤0.01) have been omitted from the
ﬁgure in order to facilitate visualizing. In both ﬁgures, the stemmed gray ﬁlled circles correspond to the
minimum ℓ2 norm LS solution. The advantages of adopting a sparsity-promoting approach to recover
the solution are obvious. The CoSaMP algorithm was provided with the exact number of sparsity. The
reader is advised to play with this example by experimenting with different values of the parameters
and see how results are affected.

1324
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
(a)
(b)
FIGURE 23.18
(a) Noiseless case. The values of the true vector, which generated the data for the Example 5, are shown
with stems topped with open circles. The recovered points, using the CoSaMP, are shown with squares. An
exact recovery of the signal has been obtained. The stems topped with gray ﬁlled circles correspond to the
minimum Euclidean norm LS solution. (b) This ﬁgure corresponds to the noisy counterpart of that in (a). In
the presence of noise, exact recovery is not possible and the more the power of the noise is the less accurate
the results are.

1.23.12 Variations on the Sparsity-Aware Theme
1325
1.23.12 Variations on the sparsity-aware theme
In our tour, so far, we have touched a number of aspects of the sparsity-aware learning that come from
the main stream of the theoretical developments. However, more and more variants appear, which are
developed with the goal to address problems of a more special structure and/or to propose alternatives,
which can be beneﬁcial in boosting the performance in practice, by serving the needs of speciﬁc
applications. These variants focus either on the regularization term in (23.13) or on the misﬁt-measuring
term or on both. Once more, research activity in this direction is dense and our purpose is to simply
highlight possible alternatives and make the reader alert of the various possibilities that spring from the
basic theory.
In a number of tasks, it is a priori known that the nonzero coefﬁcients in the target signal/vector
occur in groups and they are not randomly spread in all possible positions. Such a typical example is the
echo path in internet telephony, where the nonzero coefﬁcients of the impulse response tend to cluster
together, see Figure 23.6. Other examples of “structured” sparsity can be traced in DNA microarrays,
MIMO channel equalization, source localization in sensor networks, magnetoencephalography or in
neuroscience problems, e.g., [63,129–131]. As it is always the case in Machine Learning, being able to
incorporate a priori information in the optimization can only be of beneﬁt for improving performance,
since the estimation task is externally assisted in its effort to search for the target solution.
The group LASSO, [132–135] addresses the task where it is a priori known that the nonzero compo-
nents occur in groups. The unknown vector θ is divided into, say, L groups, i.e.,
θ T = [θ T
1 , . . . , θ T
L]T ,
each of them of a predetermined size, si, i = 1, 2, . . . , L, with *L
i=1 si = l. The regression model can
then be written as
y = Xθ + η =
L

i=1
Xiθi + η,
where each Xi is a submatrix of X comprising the corresponding si columns. The solution of the group
LASSO is given by the following LS regularized task
ˆθ = arg minθ∈Rl
⎛
⎝
     y −
L

i=1
Xiθi
     
2
2
+ λ
L

i=1
√si ∥θi∥2
⎞
⎠,
(23.59)
where ∥θi∥2 is the Euclidean norm (not the squared one) of θi, i.e.,
∥θi∥2 =
/
0
0
1
si

j=1
|θi, j|2.
In other words, the individual components of θ that contribute to the formation of the ℓ1 norm, in the
standard LASSO formulation are now replaced by the square root of the energy of each individual
block. In this setting, it is not the individual components but blocks of them which are forced to zero,

1326
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
when their contribution to the LS misﬁt measuring term is not signiﬁcant. Sometimes, this type of
regularization is coined as the ℓ1/ℓ2 regularization. It is straightforward to see that if L = l, then the
group LASSO becomes the standard LASSO method. An alternative formulation of the group sparse
model using greedy algorithms is considered in [136]. Theoretical results that extend the RIP to the
so-called block RIP have been developed and reported, see, e.g., [64,137].
In [129,138], the so-called model based Compressed Sensing is addressed. The (k, C) model allows
the signiﬁcant coefﬁcients of a k-sparse signal to appear in at most C clusters, whose size is unknown.
This is a major difference with the group LASSO, that was reported before. In Section 1.23.10, it was
commented that searching for a k-sparse solution takes place in a union of subspaces, each one of
dimensionality k. Imposing a certain structure on the target solution restricts the searching in a subset of
these subspaces and leaves a number of these out of the game. This obviously facilitates the optimization
task. In [138], a dynamic programming technique is adopted to obtain the solution. In [139], structured
sparsity is considered in terms of graphical models. An even more advanced block sparsity model is the
C-HiLasso, which allows each block to have a sparse structure itself, [140].
In [141], it is suggested to replace the ℓ1 norm by a weighted version of it. To justify such a choice, let
us recall Example 2 and the case where the “unknown” system was sensed using x = [2, 1]T , shown in
Figure 23.10c. We have seen that by “blowing” up the ℓ1 ball, the wrong sparse solution was obtained.
Let us now replace the ℓ1 norm in (23.28) with its weighted version
∥θ∥1,w := w1|θ1| + w2|θ2|,
w1, w2 > 0,
and set w1 = 4 and w1 = 1. Figure 23.19a shows the isovalue curve ∥θ∥1,w = 1, together with that
resulting from the standard ℓ1 norm. The weighted one is sharply “pinched” around the vertical axis,
and the larger the value of w1 is, compared to that of w2, the sharper the corresponding ball will be.
(a)
(b)
FIGURE 23.19
(a) The isovalue curves for the ℓ1 and the weighted ℓ1 norms for the same value. The weighted ℓ1 is sharply
pinched around one of the axis, depending on the weights. (b) Adopting to minimize the weighted ℓ1 norm,
for the setup of Figure 23.10 the correct sparse solution is obtained.

1.23.12 Variations on the Sparsity-Aware Theme
1327
Figure 23.19b shows what happens when “blowing” the weighted ℓ1 ball. It will ﬁrst touch the point
(0, 1), which is the true solution. Basically, what we have done is to “squeeze” the ℓ1 ball to be aligned
more to the axis that contains the (sparse) solution. For the case of our example, any weight w1 > 2
would do the job.
Considering now the general case of a weighted norm
∥θ∥1,w :=
l
j=1
w j|θ j|,
w j > 0.
The ideal choice of the weights would be
w j =

1
|θ0, j|,
θ0, j ̸= 0,
∞,
θ0, j = 0,
where θ0 is the target true vector, and where we have silently assumed that 0·∞= 0. In other words, the
smaller a coefﬁcient is the larger the respective weight becomes. This is justiﬁed, since large weighting
will force respective coefﬁcients towards zero during the minimization process. Of course, in practice,
the values of the true vector are not known, so it is suggested to use their estimates during each iteration
of the minimization procedure. The resulting scheme is of the following form.
Algorithm 3.
1. Initialize weights to unity, w(0)
j
= 1, j = 1, 2, . . . ,l.
2. Minimize the weighted ℓ1 norm,
θ(i) = arg minθ∈Rl ∥θ∥1,w
s.t. y = Xθ.
3. Update the weights
w(i+1)
j
=
1
θ(i)
j
 + ϵ
,
j = 1, 2, . . . ,l.
4. Terminate when a stopping criterion is met, otherwise return to step 2.
The constant ϵ is a small user-deﬁned parameter to guarantee stability when the estimates of the
coefﬁcients take very small values. Note that if the weights have constant preselected values, the task
retains its convex nature; this is no longer true when the weights are changing. It is interesting to
point out that this intuitively motivated weighting scheme can result if the ℓ1 norm is replaced by
*l
j=1 ln

|θ j| + ϵ

as the regularizing term of (23.13). Figure 23.20 shows the respective graph, in the
one-dimensional space together with that of the ℓ1 norm. The graph of the logarithmic function reminds
us of the ℓp, p < 0 < 1, “norms” and the comments made in Section 1.23.3. This is no more a convex
function and the iterative scheme, given before, is the result of a majorization-minimization procedure
in order to solve the resulting non-convex task, [141].

1328
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
FIGURE 23.20
One-dimensional graphs of the ℓ1 norm and the logarithmic regularizer ln
 |θ|
ϵ + 1

= ln

|θ| + ϵ

−ln ϵ, with
ϵ = 0.1. The term ln ϵ was subtracted for illustration purposes only and does not affect the optimization.
Notice the nonconvex nature of the logarithmic regularizer.
The concept of the iterative weighting, as used before, has also been applied in the context of the
iterative reweighted least squares algorithm. Observe that the ℓ1 norm can be written as
∥θ∥1 =
l
j=1
|θ j| = θ T Wθθ,
where
Wθ =
⎡
⎢⎢⎢⎢⎣
1
|θ1|
0 · · · 0
0
1
|θ2| · · · 0
...
...
...
...
0
0 · · ·
1
|θl|
⎤
⎥⎥⎥⎥⎦
,
and where in the case of θi = 0, for some i ∈{1, 2, . . . ,l}, the respective coefﬁcient of Wθ is deﬁned
to be 1. If Wθ were a constant weighting matrix, i.e., Wθ := W ˜θ, for some ﬁxed ˜θ, then obtaining the
minimum
ˆθ = arg minθ∈Rl ∥y −Xθ∥2
2 + λθ T W ˜θθ,
is straightforward and similar to the ridge regression. In the iterative reweighted scheme, Wθ is replaced
by Wθ(i), formed by using the respected estimates of the coefﬁcients, which have been obtained from
the previous iteration, i.e., ˜θ := θ(i), as we did before. In the sequel, each iteration solves a weighted
ridge regression task. Variants of this basic iteratively weighting scheme have also been proposed, see,
e.g., [125] and the references therein.
In [142], the LASSO task is modiﬁed by replacing the square error term with one involving corre-
lations and the minimization task becomes
ˆθ : min
θ∈Rl ∥θ∥1
s.t.
   X T (y −Xθ)
   
∞≤ϵ,

1.23.12 Variations on the Sparsity-Aware Theme
1329
where ϵ is related to l and the noise variance. This task is known as the Dantzig selector. That is, instead
of constraining the energy of the error, the constraint, now, imposes an upper limit to the correlation of
the error vector with any of the columns of X. In [57,143], it is shown that under certain conditions the
LASSO estimator and the Dantzig selector become identical.
Total Variation (TV) [144] is a closely related to ℓ1 sparsity promotion notion and it has been widely
used in image processing. Most of the grayscale image arrays, I ∈Rl×l, consist of slowly varying
pixel intensities except at the edges. As a consequence, the discrete gradient of an image array will be
approximately sparse (compressible). The discrete directional derivatives of an image array are deﬁned
pixel-wise as
∇x(I)(i, j) := I(i + 1, j) −I(i, j),
∀i ∈{1, 2, . . . ,l −1},
(23.60)
∇y(I)(i, j) := I(i, j + 1) −I(i, j),
∀j ∈{1, 2, . . . ,l −1},
(23.61)
and
∇x(I)(l, j) := ∇y(I)(i,l) := 0,
∀i, j ∈{1, 2, . . . ,l −1}.
(23.62)
The discrete gradient transform
∇: Rl×l →Rl×2l,
is deﬁned in terms of a matrix form as
∇(I)(i, j) := [∇x(i, j), ∇y(i, j)],
∀i, j ∈{1, 2, . . .l}.
(23.63)
The total variation of the image array is deﬁned as the ℓ1 norm of the magnitudes of the elements of the
discrete gradient transform, i.e.,
∥I∥TV :=
l
i=1
l
j=1
∥∇(I)(i, j)∥2 =
l
i=1
l
j=1
&
∇x(I)2(i, j) + ∇y(I)2(i, j).
(23.64)
Note that this is a mixture of ℓ2 and ℓ1 norms. The sparsity promoting optimization around the total
variation is deﬁned as
I∗∈arg min
I
∥I∥T V
s.t. ∥y −F(I)∥2 ≤ϵ,
(23.65)
where y ∈RN is the measurements vector and F(I) denotes the result in vectorized form of the
application of a linear operator on I. For example, this could be the result of the action of a partial two-
dimensional DFT on the image. Subsampling of the DFT matrix as a means to form sensing matrices
has already been discussed in Section 1.23.8.2. The task in (23.65) retains its convex nature and it
basically expresses our desire to reconstruct an image which is as smooth as possible given the available
measurements. The NESTA algorithm can be used for solving the total variation minimization task;
besides it, other efﬁcient algorithms for this task can be found in, e.g., [145,146].
It has been shown in [52], for the exact measurements case (ϵ = 0), and in [147], for the erroneous
measurements case, that conditions and bounds which guarantee recovery of an image array from the
task in (23.65) can be derived and are very similar with those that we have discussed for the case of the
ℓ1 norm.

1330
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
Example 6 (Magnetic Resonance Imaging (MRI)).
In contrast to ordinary imaging systems, which
directly acquire pixel samples, MRI scanners sense the image in an encoded form. Speciﬁcally, MRI
scanners sample components in the spatial frequency domain, known as “k-space” in the MRI nomen-
clature. If all the components in this transform domain were available, one could apply the inverse
2D-DFT to recover the exact MR image in the pixel domain. Sampling in the k-space is realized along
particular trajectories in a number of successive acquisitions. This process is time consuming, merely
due to physical constraints. As a result, techniques for efﬁcient image recovery from a limited number of
measurements is of high importance, since they can reduce the required acquisition time for performing
the measurements. Long acquisition times are not only inconvenient but even impossible, since the
patients have to stay still for long time intervals. Thus, MRI was among the very ﬁrst applications where
compressed sensing found its way to offer its elegant solutions.
Figure 23.21a shows the “famous” Shepp-Logan phantom, and the goal is to recover it via a limited
number of (measurements) samples in its frequency domain. The MRI measurements are taken across
17 radial lines in the spatial frequency domain, as shown in Figure 23.21b. A “naive” approach to recover
the image from this limited number of measuring samples would be to adopt a zero-ﬁlling rationale for
the missing components. The recovered image according to this technique is shown in Figure 23.21c.
Figure 23.21d shows the recovered image using the approach of minimizing the total variation, as
(a)
(c)
(b)
(d)
FIGURE 23.21
(a) The original Shepp-Logan image phantom. (b) The white lines indicate the directions across which the
sampling in the spatial Fourier transform were obtained. (c) The recovered image after applying the inverse
DFT having ﬁrst ﬁlled with zeros the missing values in the DFT transform. (d) The recovered image using the
total variation minimization approach.

1.23.13 Online Time-Adaptive Sparsity-Promoting Algorithms
1331
explained before. Observe that the results for this case are astonishingly good. The original image is
almost perfectly recovered. The constrained minimization was performed via the NESTA algorithm.
Note that if the minimization of the ℓ1 norm of the image array were used in place of the total variation,
the results would not be as good; the phantom image is sparse in the discrete gradient domain, since it
contains large sections which share constant intensities.
1.23.13 Online time-adaptive sparsity-promoting algorithms
In this section, online (time-recursive) schemes for sparsity-aware learning are presented. There is a
number of reasons that one has to resort to such schemes in various signal processing tasks, for example,
when the data arrive sequentially. Under such a scenario, using batch processing techniques to obtain
an estimate of an unknown target parameter vector would be highly inefﬁcient, since the number of
training points keeps increasing. Such an approach is prohibited for real time applications. Moreover,
time-recursive schemes can easily incorporate the notion of adaptivity, when the learning environment
is not stationary but it undergoes changes as time evolves. Besides signal processing applications, there
is an increasing number of machine learning applications where online processing is of paramount
importance, such as bioinformatics, hyperspectral imaging, and data mining. In such applications, the
number of training points easily amounts to a few thousand up to hundred of thousand points. Concerning
the dimensionality of the ambient (feature) space, one can claim numbers that lie in similar ranges. For
example, in [148], the task is to search for sparse solutions in feature spaces with dimensionality as high
as 109 having access to data sets as large as 107 points. Using batch techniques, in a single computer,
is out of question with today’s technology.
Let us assume that there is an unknown parameter vector that generates data according to the standard
regression model
yn = xT
n θ + ηn,
∀n,
and the training samples are received sequentially (yn, xn), n = 1, 2, . . . In the case of a stationary
environment, we would expect our algorithm to converge, asymptotically as n →∞, to or “near to”
the true parameter vector that gives birth to the measurements, yn, when it is sensed by xn. For time
varying environments, the algorithms should be able to track the underlying changes as time goes by.
Before we proceed, a comment is important. Since the time index, n, is left to grow, all we have said in
the previous sections with respect to underdetermined systems of equations, looses its meaning. Sooner
or later we are going to have more measurements than the dimensionality of the space. Our major
concern here becomes the issue of asymptotic convergence, for the case of stationary environments.
The obvious question, that is now raised, is why not using a standard algorithm, e.g., LMS, RLS, or
APSM [15,16,149], since we know that these algorithms converge to, or near enough in some sense,
to the solution; that is, the algorithm will identify the zeros asymptotically. The answer is that if such
algorithms are modiﬁed to be aware for the underlying sparsity, convergence is signiﬁcantly speeded
up; in real life applications, one has not the “luxury” to wait long time for the solution. In practice, a
good algorithm should be able to provide a good enough solution, and in the case of sparse solutions to
obtain the support, after a reasonably small number of iteration steps. In this section, the powerful theory
around the ℓ1 norm regularization will be used to obtain sparsity-promoting time adaptive schemes.

1332
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
1.23.13.1 LASSO: asymptotic performance
The notions of bias, variance and consistency, are major indices for assessing the performance of an
estimator. In a number of cases, such performance measures are derived asymptotically. For example, it
is well known that the maximum likelihood estimator is asymptotically unbiased and consistent [1]. Also
the LS estimator is asymptotically consistent. Moreover, under the assumption that the noise samples
are i.i.d., the LS estimate, ˆθ N, that is obtained using N measurement (training) samples, is itself a
random vector, that satisﬁes the
√
N-estimation consistency, e.g., [150], i.e.,
√
N

ˆθ N −θ0
 d→N

0, σ 2−1
,
where θ0 is the true vector that generates the measurements, σ 2 denotes the variance of the noise source
and  is the covariance matrix E[xxT ] of the input sequence, which has been assumed to be zero mean
and the limit denotes convergence in distribution.
The LASSO in (23.13) is the task of minimizing the ℓ1 norm regularized version of the LS cost.
However, nothing has been said, so far, about the statistical properties of this estimator. The only
performancemeasurethatwereferredtowastheerrornormboundgivenin(23.43).However,thisbound,
although important in the context it was proposed for, does not provide much statistical information.
Since the introduction of the LASSO estimator, a number of papers have addressed problems related to
its statistical performance, see, e.g., [151–154].
When dealing with sparsity-promoting estimators, such as the LASSO, two crucial issues emerge:
(a) whether the estimator, even asymptotically, can obtain the support, if the true vector parameter is
a sparse one and (b) quantify the performance of the estimator with respect to the estimates of the
nonzero coefﬁcients, i.e., those whose index belongs to the support. Especially for LASSO, the latter
issue becomes to study whether LASSO behaves as well as the unregularized LS with respect to these
nonzero components. This task was addressed, for a ﬁrst time and in a more general setting, in [153].
Let the support of the true, yet unknown, k-sparse parameter vector θ0 be denoted as S. Let also |S be
the k ×k covariance matrix E[x|SxT
|S], where x|S ∈Rk is the vector that contains only the k components
of x, with indices in the support S. Then, we say that an estimator satisﬁes asymptotically the oracle
properties if:
•
limN→∞Prob

Sˆθ N = S

= 1. This is known as support consistency.
•
√
N

ˆθ N|S −θ0|S
 d→N

0, σ 2−1
|S

. This is the
√
N-estimation consistency.
We denote as θ0|S and θ N|S the k-dimensional vectors which result from θ0, ˆθ N, respectively, if
we keep the components whose indices lie in the support S. In other words, according to the ora-
cle properties, a good sparsity-promoting estimator should be able (a) to predict, asymptotically, the
true support and (b) its performance with respect to the nonzero components should be as good as
that of a genie-aided LS estimator, which is informed, in advance, of the positions of the nonzero
coefﬁcients.
Unfortunately, the LASSO estimator cannot satisfy simultaneously both conditions. It has been
shown, [152–154] that:

1.23.13 Online Time-Adaptive Sparsity-Promoting Algorithms
1333
•
For support consistency, the regularization parameter λ := λN should be time varying such as
lim
N→∞
λN
√
N
= ∞,
lim
N→∞
λN
N = 0.
That is, λN must grow faster than
√
N, but slower than N.
•
For
√
N-consistency, λN must grow as
lim
N→∞
λN
√
N
= 0,
i.e., it grows slower than
√
N.
The previous two conditions are conﬂicting and the LASSO estimator cannot comply with the two
oracle conditions simultaneously. The proofs of the previous two points are somewhat technical and are
not given here. The interested reader can obtain them from the previously given references. However,
before we proceed, it is instructive to see why the regularization parameter has to grow slower than N,
in any case. Without being too rigorous mathematically, recall that the LASSO solution comes from
Eq. (23.13). This can be written as
0 ∈−2
N
N

n=1
xnyn + 2
N
	 N

n=1
xnxT
n

θ + λN
N ∂∥θ∥1 ,
(23.66)
where we have divided by N both sides. Taking the limit as N →∞, if λN/N →0, then we are left
with the ﬁrst two terms; this is exactly what we would have if the unregularized LS had been chosen as
the cost function. In this case, the solution asymptotically converges4 (under some general assumptions,
which are assumed to hold true, here) to the true parameter vector; that is, we have strong consistency,
e.g., [150].
1.23.13.2 The adaptive norm-weighted LASSO
There are two ways to get out of the previously stated conﬂict. One is to replace the ℓ1 norm with a
nonconvex function and this can lead to an estimator that satisﬁes the oracle properties simultaneously
[153]. The other is to modify the ℓ1 norm by replacing it with a weighted version. Recall that the
weighted ℓ1 norm was discussed in Section 1.23.12, as a means to assist the optimization procedure
to unveil the sparse solution. Here the notion of weighted ℓ1 norm comes as a necessity imposed by
our willingness to satisfy the oracle properties. This gives rise to the adaptive time-and-norm-weighted
LASSO (TNWL) cost estimator deﬁned as5
ˆθ = arg minθ∈Rl
⎧
⎨
⎩
n

j=1
βn−j 
y j −xT
j θ
2
+ λn
l
i=1
wi(n)|θi|
⎫
⎬
⎭,
(23.67)
4Recall that this convergence is with probability 1.
5To emphasize that the number of training points is now increasing, we have used n in place of N. Capital N was previously
used to denote a ﬁxed number of points.

1334
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
where β ≤1 is used as the forgetting factor to allow for tracking slow variations. The time varying
weighting sequences is denoted as wi(n). There are different options. In [154] and under a stationary
environment with β = 1, it is shown that if
wi(n) =
1
|θest
i |γ ,
where θest
i
is the estimate of the ith component obtained by any √n-consistent estimator, such as the
unregularized LS, then for speciﬁc choices of λn and γ the estimator satisﬁes the oracle properties
simultaneously. The main reasoning behind the weighted ℓ1 norm is that as time goes by, and the
√n-consistent estimator provides better and better estimates, then the weights corresponding to indices
outside the true support (zero values) are inﬂated and those corresponding to the true support converge
to a ﬁnite value. This helps the algorithm, simultaneously, to locate the support and obtain unbiased
(asymptotically) estimates of the large coefﬁcients.
Another choice for the weighing sequence is related to the so called Smoothly Clipped Absolute
Deviation (SCAD) [153,155]. This is deﬁned as
wi(n) = χ(0,μn)(|θest
i |) +

αμn −|θest
i |

+
(α −1)μn
χ(μn,∞)(|θest
i |),
where χ(·) stands for the characteristic function, μn = λn/n, and α > 2. Basically, this corresponds
to a quadratic spline function. It turns out, [155], that if λn is chosen to grow faster that √n and slower
that n, the adaptive LASSO, with β = 1 satisﬁes both oracle conditions, simultaneously.
A time adaptive scheme for solving the TNWL LASSO was presented in [156]. The cost function
of the adaptive LASSO in (23.67) can be written as
J(θ) = θ T Rnθ −rT
n θ + λn ∥θ∥1,w(n) ,
where
Rn :=
n

j=1
βn−jx jxT
j ,
rn :=
n

j=1
βn−j y jx j,
and ∥θ∥1,w(n) is the weighted ℓ1 norm. It is straightforward to see, that
Rn = β Rn−1 + xnxT
n ,
rn = βrn−1 + ynxn.
The complexity for both of the previous updates, for matrices of a general structure, amounts to O(l2)
multiply/add operations. One alternative is to update Rn and rn and then solve a convex optimization
task for each time instant, n, using any standard algorithm. However, this is not appropriate for real
time applications, due to its excessive computational cost. In [156], a time recursive version of a
coordinate descent algorithm has been developed. As we have seen in Section 1.23.11.2, coordinate
descent algorithms update one component at each recursive step. In [156], recursive steps are associated
with time updates, as it is always the case with the time-recursive algorithms. As each new training pair
(yn, xn) is received, a single component of the unknown vector is updated. Hence, at each time instant,

1.23.13 Online Time-Adaptive Sparsity-Promoting Algorithms
1335
a scalar optimization task has to be solved and its solution is given in closed form, which results in
a simple soft thresholding operation. One of the drawbacks of the coordinate techniques is that each
coefﬁcient is updated every l time instants, which, for large values of l, can slow down convergence.
Variants of the basic scheme that cope with this drawback are also addressed in [156], referred to as
online cyclic coordinate descent time weighted Lasso (OCCD-TWL). If the weighted norm is to be
used in place of the ℓ1, a RLS is run in parallel to provide the necessary weights; the resulting scheme
is referred as (OCCD-TNWL). The complexity of the scheme is of the order of O(l2). Computational
savings are possible, if the input sequence is a time series and fast schemes for the updates of Rn and
the RLS can then be exploited. However, if an RLS-type algorithm is used in parallel, the convergence
of the overall scheme may be slowed down, since the RLS-type algorithm has to converge ﬁrst, in order
to provide reliable estimates for the weights, as pointed out before.
1.23.13.3 Adaptive CoSaMP algorithm (AdCoSaMP)
In [157], an adaptive version of the CoSaMP algorithm, which was presented in Section 1.23.11.1.3,
was proposed. Iteration steps, i, now coincide with time updates, n, and the LS solver in Step 3c of the
general CSMP scheme is replaced by an LMS one.
Let us focus ﬁrst on the quantity X T e(i−1) in Step 3a of the CSMP scheme, which is used to
compute the support at iteration i. In the adaptive setting and at (iteration) time n, this quantity is now
“rephrased” as
X T e(n −1) =
n−1

j=1
x je( j).
In order to make the algorithm ﬂexible to adapt to variations of the environment, as the time index, n,
increases, the previous correlation sum is modiﬁed to
p(n) :=
n−1

j=1
βn−1−jx je( j) = βp(n −1) + xn−1e(n −1).
The LS task, constrained on the active columns that correspond to the indices in the support S in Step
3c, is performed in an adaptive rationale by involving the basic LMS recursions, i.e.,
˜e(n) := yn −xT
n|S ˜θ|S(n −1),
˜θ|S(n) := ˜θ|S(n −1) + μxn|S ˜e(n),
where ˜θ|S(·) and xn|S denote the respective subvectors corresponding to the indices in the support S.
The resulting algorithm is given as follows:
Algorithm 4 (The AdCoSaMP Scheme).
1. Select the value of t = 2k.
2. Initialize the algorithm: θ(1) = 0, ˜θ(1) = 0, p(1) = 0, e(1) = y1.
3. Choose μ and β.

1336
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
4. For n = 2, 3, . . ., execute the following steps:
a. p(n) = βp(n −1) + xn−1e(n −1).
b. Obtain the current support:
S = supp{θ(n −1)} ∪

indices of the t largest
in magnitude components of p(n)

.
c. Perform the LMS update:
˜e(n) = yn −xT
n|S ˜θ|S(n −1),
˜θ|S(n) = ˜θ|S(n −1) + μxn|S ˜e(n).
d. Obtain the set Sk of the indices of the k largest components of ˜θ|S(n).
e. Obtain θ(n) such that:
θ|Sk(n) = ˜θ|Sk,
and θ|Sc
k (n) = 0,
where Sc
k is the complement set of Sk.
f. Update the error: e(n) = yn −xT
n θ(n).
In place of the standard LMS, its normalized version can alternatively be adopted. Note that Step 4e is
directly related to the hard thresholding operation.
In [157], it is shown that if the sensing matrix, which is now time dependent and keeps increasing in
size, satisﬁes a condition similar to RIP, for each time instant, called Exponentially Weighted Isometry
Property (ERIP), which depends on β, then the algorithm asymptotically satisﬁes an error bound, which
is similar to the one that has been derived for CoSaMP in [89], plus an extra term that is due to the
excess Mean Square Error, which is the price paid by replacing the LS solver by the LMS.
1.23.13.4 Sparse adaptive parallel projection onto convex sets method
(SpAPSM)
The APSM family of algorithms is one among the most powerful techniques for adaptive learning [149].
A major advantage of this algorithmic family is that one can readily incorporate convex constraints. The
rationale behind APSM is that since our data are known to be generated by a regression model, then the
unknown vector could be estimated by ﬁnding a point in the intersection of a sequence of hyperslabs,
that are deﬁned by the data points, i.e., Sn[ϵ] :=

θ ∈Rl :
yn −xT
n θ
 ≤ϵ

. Such a model is most
natural when the noise is bounded, (which, after all, it is the case in any practical application). In case
the noise is assumed unbounded, a choice of ϵ of the order say, σ, can guarantee, with high probability,
that the unknown solution lies inside these hyperslabs.
The APSM family builds upon the elegant philosophy that runs across the classical projections onto
convex sets (POCS) theory. Recall that the basic rationale behind POCS is that starting from an arbitrary
point in the space and sequentially projecting onto a ﬁnite number of convex sets then the sequence
of projections converges, in some sense, into the intersection of all these sets, assuming this is not
empty. The theory was extended to embrace the online processing setting in [158–160]. In contrast to

1.23.13 Online Time-Adaptive Sparsity-Promoting Algorithms
1337
the classical POCS theory, here the number of the involved convex sets is inﬁnite. It turns out that, under
certain general conditions, a sequence of projections over all these sets also converges to a point in their
intersection.
To ﬁt the theory into our needs, the place of the aforementioned convex sets is taken by the hyperslabs,
which are formed by the received training data, as mentioned before. Thus, the resulting algorithms
involves (metric) projections onto these hyperslabs (see Appendix). However, when dealing with sparse
vectors, there is an extra projection associated with the convex set formed by the ℓ1 ball; that is, ∥θ∥1 ≤ρ
(see, also, the LASSO formulation (23.14)). Hence, this task ﬁts nicely in the APSM rationale and the
basic recursion can be readily written, without much thought or derivation, as follows; for any arbitrarily
chosen initial point θ(0), deﬁne ∀n,
θ(n) := PBℓ1[δ]

θ(n −1) + μn
 1
q
n

i=n−q+1
PSi[ϵ]

θ(n −1)

−θ(n −1)

,
where PSi[ϵ] is the metric projection onto the hyperslab Si[ϵ] (see Appendix). Note, that in the previous
recursion we have used q, instead of one, hyperslabs whose metric projections are averaged out at time n.
It turns out that such an averaging improves convergence signiﬁcantly. Parameter μn is an extrapolation
parameter, which takes values in the interval (0, 2Mn), where
Mn :=
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
*n
i=n−q+1 ω(n)
i
  PSi[ϵ](θ(n −1)) −θ(n −1)
  2
   
*n
i=n−q+1 ω(n)
i
PSi[ϵ](θ(n −1)) −θ(n −1)
   
2 ,
if
   *n
i=n−q+1 ω(n)
i
PSi[ϵ](θ(n −1)) −θ(n −1)
   ̸= 0,
1,
otherwise,
(23.68)
and PBℓ1[ρ](·) is the projection operator onto the ℓ1 ball Bℓ1[ρ] :=

θ ∈Rl : ∥θ∥1 ≤ρ

, since
the solution is constrained to live within this ball. Note, that the previous recursion is analogous to
the iterative soft thresholding shrinkage algorithm in the batch processing case, (23.53). There, we
saw that the only difference that the sparsity imposes on an iteration, with respect to its unconstrained
counterpart, is an extra soft thresholding. This is exactly the case here. The term in the parenthesis is
the iteration for the unconstrained task. Moreover, as it has been shown in [161], projection on the ℓ1
ball is equivalent to a soft thresholding operation. It can be shown that the previous iteration converges
arbitrarily close to a point in the intersection
Bℓ1[δ] ∩
2
n≥n0
Sn[ϵ],
for some ﬁnite value of n0 [149,158–160]. In [162,163] the weighted ℓ1 ball has been used to improve
convergence as well as the tracking speed of the algorithm, when the environment is time varying. The
weights were adopted in accordance to what was discussed in Section 1.23.12, i.e.,
wi(n) :=
1
|θi(n −1)| + ´ϵn
,
∀i ∈{1, 2, . . . ,l},

1338
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
FIGURE 23.22
Geometric illustration of the update steps involved in the SpAPSM algorithm, for the case of q = 2. The
update at time n + 1 is obtained by ﬁrst convexly combining the projections onto the current and previously
formed hyperslabs, Sn[ϵ], Sn−1[ϵ] and then projecting onto the weighted ℓ1 ball. This brings the update
closer to the target solution θ∗.
where (´ϵn)n≥0 is a sequence (can be also constant) of small numbers to avoid division by zero. The
basic time iteration becomes as follows; for any arbitrarily chosen initial point θ(0), deﬁne ∀n,
θ(n) := PBℓ1[w(n),ρ]
⎛
⎝θ(n −1) + μn
⎛
⎝
n

i=n−q+1
ω(n)
i
PSi[ϵ]

θ(n −1)

−θ(n −1)
⎞
⎠
⎞
⎠,
(23.69)
where μn ∈(0, 2Mn) and Mn is given in (23.68). Figure 23.22 illustrates the associated geometry
of the basic iteration in R2 and for the case of q = 2. It comprises two parallel projections on the
hyperslabs followed by one projection onto the weighted ℓ1 ball. In [162], it is shown that a good bound
for the weighted ℓ1 norm is the sparsity level k of the target vector, which is assumed to be known
and it is a user-deﬁned parameter. In [162], it is shown that asymptotically, and under some general
assumptions, this algorithmic scheme converges arbitrarily close to the intersection of the hyperslabs
with the weighted ℓ1 balls, i.e.,
2
n≥n0

PBℓ1[w(n),ρ] ∩Sj[ϵ]

,
for some non-negative integer n0. It has to be pointed out that, in the case of weighted ℓ1 norms, the
constraint is time varying and the convergence analysis is not covered by the standard analysis used for
APSM, and had to be extended to this more general case. The complexity of the algorithm amounts to
O(ql). The larger the q the faster the convergence rate, at the expense of higher complexity. In [163],

1.23.13 Online Time-Adaptive Sparsity-Promoting Algorithms
1339
in order to reduce the dependence of the complexity on q, the notion of the sub-dimensional projection
is introduced, where projections onto the q hyperslabs could be restricted along the directions of the
most signiﬁcant coefﬁcients, of the currently available estimates. The dependence on q now becomes
O(qkn) where kn is the sparsity level of the currently available estimate, which, after a few steps of the
algorithm, gets much lower than l. The total complexity amounts to O(l) + O(qkn), per iteration step.
This allows the use of large values of q, which drives the algorithm to a performance close to that of
the adaptive weighted LASSO, at only a small extra computational cost.
1.23.13.4.1
Projection onto the weighted ℓ1 ball
Projecting onto an ℓ1 ball is equivalent to a soft thresholding operation. Projection onto the weighted ℓ1
norm results to a slight variation of the soft thresholding, with different threshold values per component.
In the sequel, we give the iteration steps for the more general case of the weighted ℓ1 ball. The proof
is a bit technical and lengthy and it will not be given here. It was derived, for the ﬁrst time, via purely
geometric arguments, and without the use of the classical Lagrange multipliers, in [162]. Lagrange
multipliers have been used instead in [161], for the case of the ℓ1 ball.
Given a point outside the ball, θ ∈Rl \ Bℓ1[w, ρ], then its projection onto the weighted ℓ1 ball is the
point PBℓ1[w,ρ](θ) ∈Bℓ1[w, ρ] := {z ∈Rl : *l
i=1 wi|zi| ≤ρ}, that lies closest to θ in the Euclidean
sense. If θ lies within the ball then it coincides with its projection. Given the weights and the value of
ρ, the following iterations provide the projection.
Algorithm 5 (Projection onto the weighted ℓ1 ball Bℓ1[w, ρ]).
1. Form the vector [|θ1|/w1, . . . , |θl|/wl]T ∈Rl.
2. Sort the previous vector in a non-ascending order, so that |θτ(1)|/wτ(1) ≥· · · ≥|θτ(l)|/wτ(l). The
notation τ stands for the permutation, which is implicitly deﬁned by the sorting operation. Keep in
memory the inverse τ −1, which is the index of the position of the element in the original vector.
3. r1 := l.
4. Let m = 1. While m ≤l, do:
a. m∗:= m.
b. Find the maximum j∗among those j ∈{1, 2, . . . ,rm} such that |θτ( j)|
wτ( j) >
*rm
i=1 wτ(i)|θτ(i)|−ρ
*rm
i=1 w2
τ(i)
.
c. If j∗= rm then break the loop.
d. Otherwise set rm+1 := j∗.
e. Increase m by 1 and go back to Step 4a.
5. Form the vector ˆp ∈Rrm∗whose jth component, j = 1, ...,rm∗, is given by
ˆp j := |θτ( j)| −
*rm∗
i=1 wτ(i)|θτ(i)| −ρ
*rm∗
i=1 w2
τ(i)
wτ( j).
6. Use the inverse mapping τ −1 to insert the element ˆp j into the τ −1( j) position of the l-dimensional
vector p, ∀j ∈{1, 2, . . . ,rm∗}, and ﬁll in the rest with zeros.
7. The desired projection is PBℓ1[w,ρ](θ) = [sgn(θ1)p1, . . . , sgn(θl)pl]T .

1340
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
Remarks 12.
Projections onto both ℓ1 and weighted ℓ1 balls impose convex sparsity inducing con-
straints via properly performed soft thresholding operations. More recent advances within the SpAPSM
framework [164], allow the substitution of PBℓ1[ρ] and PBℓ1[w,ρ] with a generalized thresholding, built
around the notions of SCAD, non-negative garrote, as well as a number of thresholding functions
corresponding to the non-convex, ℓp, p < 1 penalties. Moreover, it is shown that such generalized
thresholding operators (GT) are nonlinear mappings with their ﬁxed point set being a union of sub-
spaces, i.e., the non-convex object which lies at the heart of any sparsity-promoting technique. Such
schemes are very useful for low values of q, where one can improve upon the performance obtained by
the LMS-based AdCoSAMP, at comparable complexity levels.
Example 7 (Time varying signal).
In this example, the performance curves of the most typical online
algorithms, mentioned before, are studied in the context of a time varying environment. A typical
simulation setup, which is commonly adopted by the adaptive ﬁltering community, in order to study
the tracking agility of an algorithm, is that of an unknown vector which undergoes an abrupt change
after a number of observations. Here, we consider a signal, s, with a sparse wavelet representation, i.e.,
s = θ, where  is the corresponding transformation matrix. In particular, we set l = 1024 with 100
nonzero wavelet coefﬁcients. After 1500 measurements (observations), ten arbitrarily picked wavelet
coefﬁcients change their values to new ones selected uniformly at random from the interval [−11].
Note that this may affect the sparsity level of the signal, and we can now end with up to 110 nonzero
coefﬁcients. A total of N = 3000 sensing vectors are used, which result from the wavelet transform
of the input vectors xn ∈Rl, n = 1, 2, . . . , 3000, having elements drawn from N(0, 1). In this way,
the adaptive algorithms do not estimate the signal itself, but its sparse wavelet representation, θ. The
observations are corrupted by additive white Gaussian noise of variance σ 2
n = 0.1. Regarding SpAPSM,
the extrapolation parameter μn is set equal to1.8×Mn, the hyperslabs parameter ϵ was set equal to 1.3σn
and q = 390. The parameters for all algorithms were selected in order to optimize their performance.
Since the sparsity level of the signal may change (from k = 100 up to k = 110) and since in practice it
is not possible to know in advance the exact value of k, we feed the algorithms with an overestimate, k,
of the true sparsity value and in particular we used ˆk = 150 (i.e., 50% overestimation up to the 1500th
iteration).
The results are shown in Figure 23.23. Note the enhanced performance obtained via the SpAPSM
algorithm. However, it has to be pointed out that the complexity of the AdCoSAMP is much lower
compared to the other two algorithms, for the choice of q = 390 for the SpAPSM. The interesting
observation is that SpAPSM achieves a better performance compared to OCCD-TWL, albeit at signif-
icantly lower complexity. If on the other hand complexity is of major concern, as it has already been
pointed out in 12, use of SpAPSM offers the ﬂexibility to use GT operators, which lead to improved
performance for small values of q at complexity comparable to that of LMS-based sparsity promoting
algorithms [165].
1.23.14 Learning sparse analysis models
All our discussion, so far, has been exhausted in the terrain of signals which are either sparse themselves
or they can be sparsely represented in terms of the atoms of a dictionary in a synthesis model, as

1.23.14 Learning Sparse Analysis Models
1341
−
−
−
−
−
−
−
−
FIGURE 23.23
MSE learning curves for AdCoSAMP, SpAPSM and OCCD-TWL for the simulation example discussed in 7.
The vertical axis shows the log10 of the Mean Squares Error, i.e., log10

1
2 ∥s −θ(n)∥2
2

and the horizontal
shows the time index. At time n = 1500, the system undergoes a sudden change.
introduced in (23.23), i.e.,
s =

i∈I
θiψi.
As a matter of fact, most of the research activity over the last decade or so has been focused on the
synthesis model. This may be partly due to the fact that the synthesis modeling path may provide a
more intuitively appealing structure to describe the generation of the signal in terms of the elements
(atoms) of a dictionary. Recall from Section 1.23.10 that the sparsity assumption was imposed on θ in
the synthesis model and the corresponding optimization task was formulated in (23.45) and (23.46) for
the exact and noisy cases, respectively.
However, this is not the only way to attack the task of sparsity modeling. Very early in this paper, in
Section 1.23.5, we referred to the analysis model,
S = Hs
and pointed out that in a number of real life applications the resulting transform S is sparse. To be fair,
for such an experimental evidence, the most orthodox way to deal with the underlying model sparsity
would be to consider
  Hs
  
0. Thus, if one wants to estimate s, a very natural way would be to cast the

1342
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
related optimization task as
min
s
   Hs
   
0 ,
s.t. y = Xs,
or
∥y −Xs∥2
2 ≤ϵ,
(23.70)
depending on whether the measurements via a sensing matrix, X, are exact or noisy. Strictly speaking,
the total variation minimization approach, which was used in Example 6, falls under this analysis model
formulation umbrella, since what is minimized is the ℓ1 norm of the gradient transform of the image.
The optimization tasks in either of the two formulations given in (23.70) build around the assumption
that the signal of interest has sparse analysis representation. The obvious question that is now raised is
whether the optimization tasks in (23.70) and their counterparts in (23.45) or (23.46) are any different.
One of the ﬁrst efforts to shed light in this problem was in [166]. There, it was pointed out that the
two tasks, although related, yet they are in general different. Moreover, their comparative performance
depends on the speciﬁc problem at hand. However, it is fair to say that this is a new ﬁeld of research
and more deﬁnite conclusions are currently being shaped. An easy answer can be obtained for the case
where the involved dictionary corresponds to an orthonormal transformation matrix, e.g., DFT. In this
case, we already know that the analysis and synthesis matrices are related as
 =  = −H,
which leads to an equivalence between the two previously stated formulations. Indeed, for such a
transform we have
S = Hs
3
45
6
Analysis
⇔
s = S
3 45 6
Synthesis
.
Using the last formula into the (23.70), the tasks in (23.45) or (23.46) are readily obtained by replacing
θ by s. However, this reasoning cannot be carried out to the case of overcomplete dictionaries; it is for
these cases, where the two optimization tasks may lead to different solutions.
The previous discussion, concerning the comparative performance between the synthesis or analysis-
based sparse representations, is not only of a “philosophical” value. It turns out that, often in practice, the
nature of certain overcomplete dictionaries does not permit the use of the synthesis based formulation.
These are the cases where the columns of the overcomplete dictionary exhibit high degree of dependence;
that is, the coherence of the matrix, as deﬁned in Section 1.23.7.1, has large values. Typical examples
of such overcomplete dictionaries are the Gabor frames, the curvelet frames and the oversampled DFT.
The use of such dictionaries lead to enhanced performance in a number of applications, e.g., [167,168].
Take as an example the case of our familiar DFT transform. This transform provides a representation of
our signal samples in terms of sampled exponential sinusoids, whose frequencies are multiples of 2π
lT ,
where T is the sampling frequency and lT is the length of our signal segment s; that is,
s :=
⎡
⎢⎢⎢⎣
s(0)
s(T )
...
s((l −1)T )
⎤
⎥⎥⎥⎦=
l−1

i=0
Siψi,
(23.71)

1.23.14 Learning Sparse Analysis Models
1343
where Si are the DFT coefﬁcients and ψi is the sampled sinusoid with frequency equal to 2π
lT i, i.e.,
ψi =
⎡
⎢⎢⎢⎣
1
exp

−j 2π
lT iT

...
exp

−j 2π
lT i(l −1)T

⎤
⎥⎥⎥⎦.
(23.72)
However, this is not necessarily the most efﬁcient representation. For example, it is highly unlikely that
a signal comprises only frequencies which are multiples of the basic one; only such signals can result
in a sparse representation using the DFT basis. Most probably, in general, there will be frequencies
lying in between the frequency samples of the DFT basis, which result in non-sparse representations.
Using these extra frequencies, a much better representation of the frequency content of the signal can be
obtained. However, in such a dictionary the atoms are no more linearly independent and the coherence
of the respective (dictionary) matrix increases.
Once a dictionary exhibits high coherence, then there is no way of ﬁnding a sensing matrix, X, so
that X to obey the RIP. Recall that at the heart of the sparsity-aware learning lies the concept of stable
embedding, that allows the recovery of a vector/signal after projecting it in a lower dimensional space;
this is what all the available conditions, e.g., RIP, guarantee. However, no stable embedding is possible
with highly coherent dictionaries. Take as an extreme example the case where the ﬁrst and second atoms
are identical. Then no sensing matrix X can achieve a signal recovery that distinguishes the vector
[1, 0, . . . , 0]T from [0, 1, 0, . . . , 0]T . Can then one conclude that for highly coherent overcomplete
dictionaries compressed sensing techniques are not possible? Fortunately, the answer to this is negative.
After all, our goal in compressed sensing has always been the recovery of the signal s = θ and not
the identiﬁcation of the sparse vector θ in the synthesis model representation. The latter was just a
means to an end. While the unique recovery of θ cannot be guaranteed for highly coherent dictionaries,
this does not necessarily cause any problems for the recovery of s, using a small set of measurement
samples. The escape route will come by considering the analysis model formulation. However, prior
to this treatment, it will be of no harm to refresh our basics concerning the theory of frames and recall
some key deﬁnitions.
1.23.14.1 Some hints from the theory of frames
In order to remain in the same framework as the one already adopted for this paper and comply with
the notation previously used, we will adhere to the real data case, although everything we are going to
say is readily extended to the complex case, by replacing transposition with its Hermitian counterpart.
A frame in a vector space6 V ⊆Rl is a generalization of the notion of a basis. Recall form our
linear algebra basics that a basis is a set of vectors ψi, i ∈I, with the following two properties:
(a) V = span{ψi : i ∈I}, where the cardinality card(I) = l and (b) ψi, i ∈I, are mutually
independent. If, in addition,
7
ψi, ψ j
8
= δi, j then the basis is known as orthonormal. If we now relax the
second condition and allow l < card(I) := p, we introduce redundancy in the signal representations,
6We constrain our discussion in this section to ﬁnite dimensional Euclidean spaces. The theory of frames has been developed
for general Hilbert spaces.

1344
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
which, as it has already been mentioned, can offer a number of advantages in a wide range of applications.
However, once redundancy is introduced we lose uniqueness in the signal representation
s =

i∈I
θiψi,
(23.73)
due to the dependency among the vectors ψi. The question that is now raised is whether there is a simple
and systematic way to compute the coefﬁcients θi in the previous expansion.
Deﬁnition 4.
The set ψi, i ∈I, which spans a vector space, V , is called a frame if there exist positive
real numbers, A and B, such that for any nonzero s ∈V ,
0 < A ∥s∥2
2 ≤

i∈I
 7
ψi, s
8 2 ≤B ∥s∥2
2 ,
(23.74)
where A and B are known as the bounds of the frame.
Note that if ψi, i ∈I, comprise an orthonormal basis, then A = B = 1 and (23.74) is the celebrated
Parseval’s theorem. Thus, (23.74) can be considered as a generalization of Parseval’s theorem. Looking
at it more carefully, we notice that this is a stability condition that closely resembles our familiar RIP
condition in (23.36). Indeed, the upper bound guarantees that the expansion never diverges (this applies
to inﬁnite dimensional spaces) and the lower bound guarantees that no nonzero vector, ∥s∦= 0, will
ever become zero after projecting it along the atoms of the frame. To look at it from a slightly different
perspective, form the dictionary matrix
 = [ψ1, ψ2, . . . , ψ p],
where we used p to denote the cardinality of I. Then, the lower bound in (23.74) guarantees that s
can be reconstructed from its transform samples T s; note that in such a case, if s1 ̸= s2, then their
respective transform values will be different.
It can be shown that if condition (23.74) is valid, then there exists another set of vectors, ˜ψi, i ∈I,
known as the dual frame, with the following elegant property
s =

i∈I
9
˜ψi, s
:
ψi =

i∈I
7
ψi, s
8 ˜ψi,
∀s ∈V .
(23.75)
Once a dual frame is available, the coefﬁcients in the expansion of a vector in terms of the atoms of a
frame are easily obtained. If we form the matrix ˜ of the dual frame vectors, then it is easily checked
out that since condition (23.75) is true for any s, it implies that
˜T =  ˜T = I,
(23.76)
where I is the l × l identity matrix. Note that all of us have used the property in (23.75), possibly in a
disguised form, many times in our professional life. Indeed, consider the simple case of two independent
vectors in the two-dimensional space (in order to make things simple). Then, (23.73) becomes
s = θ1ψ1 + θ2ψ2 = θ.

1.23.14 Learning Sparse Analysis Models
1345
Solving for the unknown θ is nothing but the solution of a linear set of equations; note that the involved
matrix  is invertible. Let us rephrase a bit our familiar solution
θ = −1s := ˜s :=
⎡
⎣˜ψ
T
1
˜ψ
T
2
⎤
⎦s,
(23.77)
where ˜ψ
T
i , i = 1, 2, are the rows of the inverse matrix. Using now the previous notation, it is readily
seen that
s =
9
˜ψ1, s
:
ψ1 +
9
˜ψ2, s
:
ψ2.
Moreover, note that in this special case of independent vectors, the respective deﬁnitions imply
⎡
⎣˜ψ
T
1
˜ψ
T
2
⎤
⎦[ψ1, ψ2] = I,
and the dual frame is not only unique but it also fulﬁlls the biorthogonality condition, i.e.,
9
˜ψi, ψ j
:
= δi, j.
(23.78)
In the case of a general frame, the dual frames are neither biorthogonal nor uniquely deﬁned. The latter
can also be veriﬁed by the condition (23.76) that deﬁnes the respective matrices. T is a rectangular
tall matrix and its left inverse is not unique. There is, however, a uniquely deﬁned dual frame, known
as the canonical dual frame, given as
˜ψi := (T )−1ψi,
or
˜ := (T )−1.
(23.79)
Another family of frames of special type are the so-called tight frames. For tight frames, the two
bounds in (23.74) are equal, i.e., A = B. Thus, once a tight frame is available, we can normalize each
vector in the frame as
ψi →
1
√
A
ψi,
which then results to the so-called Parseval tight frame; the condition (23.74) now becomes similar in
appearance with our familiar Parseval’s theorem for orthonormal bases

i∈I
 7
ψi, s
8 2 = ∥s∥2
2 .
(23.80)
Moreover, it can be shown that a Parseval tight frame coincides with its canonical dual frame (that is,
it is self dual) and we can write
s =

i∈I
7
ψi, s
8
ψi,
or in matrix form
˜ = ,
(23.81)

1346
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
which is similar with what we know for orthonormal bases; however in this case, orthogonality does
not hold.
We will conclude this subsection with a simple example of a Parseval (tight) frame, known as the
Mercedes Benz (MB),
 =
⎡
⎢⎢⎣
0
−1
√
2
1
√
2
#
2
3
−1
√
6
−1
√
6
⎤
⎥⎥⎦.
One can easily check that all the properties of a Parseval tight frame are fulﬁlled. If constructing a
frame, especially in high dimensional spaces, may sound a bit difﬁcult, the following theorem (due to
Naimark, see, e.g., [169]) offers a systematic way for such constructions.
Theorem 6.
A set {ψi}i∈I in a Hilbert space Hs is a Parseval tight frame, if and only if it can be
obtained via orthogonal projection, PHs : H →Hs, of an orthonormal basis {ei}i∈I in a larger Hilbert
space H, such that Hs ⊂H.
To verify the theorem, check that the MB frame is obtained by orthogonally projecting the three-
dimensional orthonormal basis
e1 =
⎡
⎢⎢⎢⎢⎣
0
−1
√
2
1
√
2
⎤
⎥⎥⎥⎥⎦
,
e2 =
⎡
⎢⎢⎢⎢⎢⎢⎣
#
2
3
−1
√
6
−1
√
6
⎤
⎥⎥⎥⎥⎥⎥⎦
,
e3 =
⎡
⎢⎢⎢⎢⎢⎣
1
√
3
1
√
3
1
√
3
⎤
⎥⎥⎥⎥⎥⎦
,
using the projection matrix
PHs :=
⎡
⎢⎢⎢⎢⎢⎣
2
3
−1
3
−1
3
−1
3
2
3
−1
3
−1
3
−1
3
2
3
⎤
⎥⎥⎥⎥⎥⎦
.
Observe that the effect of the projection
PHs[e1, e2, e3] = T ,
is to set e3 to the zero vector.
Frames were introduced by Dufﬁn and Schaeffer in their study on nonharmonic Fourier series in
1952 [170] and they remained rather obscured till they were used in the context of wavelet theory,
e.g., [171]. The interested reader can obtain the proofs of what has been said in this section from these
references. An introductory review with a lot of engineering ﬂavor can be found in [172], where the
major references in the ﬁeld are given.

1.23.14 Learning Sparse Analysis Models
1347
1.23.14.2 Compressed sensing for signals sparse in coherent dictionaries
Our goal in this subsection is to establish conditions that guarantee recovery of a signal vector, which
accepts a sparse representation in a redundant and coherent dictionary, using a small number of signal-
related measurements. Let the dictionary at hand be a tight frame, . Then, our signal vector is written
as
s = θ,
(23.82)
where θ is assumed to be k-sparse. Recalling the properties of a tight frame, as they were summarized
in the previous subsection, the coefﬁcients in the expansion (23.82) can be written as
7
ψi, s
8
, and the
respective vector as
θ = T s,
since a tight frame is self dual. Then, the analysis counterpart of the synthesis formulation in (23.46)
can be cast as
min
s
   T s
   
1 ,
s.t. ∥y −Xs∥2
2 ≤ϵ.
(23.83)
The goal now is to investigate the accuracy of the recovered solution to this convex optimization task.
It turns out that similar strong theorems are also valid for this problem as with the case of the synthesis
formulation, which was studied earlier.
Deﬁnition 5.
Let k be the union of all subspaces spanned by all subsets of k columns of . A sensing
matrix, X, obeys the restricted isometry property adapted to , (-RIP) with δk, if
(1 −δk) ∥s∥2
2 ≤∥Xs∥2
2 ≤(1 + δk) ∥s∥2
2 ,
(23.84)
for all s ∈k.
The union of subspaces, k, is the image under  of all k-sparse vectors. This is the difference
with the RIP deﬁnition given in Section 1.23.8.2. All the random matrices discussed earlier in this
paper can be shown to satisfy this form of RIP, with overwhelming probability, provided the number of
measurements, N, is at least of the order of k ln (p/k). We are now ready to establish the main theorem
concerning our ℓ1 minimization task.
Theorem 7.
Let  be an arbitrary tight frame and X a sensing matrix that satisﬁes the -RIP with
δ2k ≤0.08, for some positive k. Then the solution, s∗, of the minimization task in (23.83) satisﬁes the
following property
∥s −s∗∥2 ≤C0k−1
2
   T s −(T s)k
   
1 + C1
√ϵ,
(23.85)
where C0, C1 are constants depending on δ2k, (T s)k denotes the best k-sparse approximation of T s;
i.e., it results by setting all but the k largest in magnitude components of T s equal to zero.
The bound in (23.85) is the counterpart of that given in (23.43). In other words, the previous theorem
states that if T s decays rapidly, then s can be reconstructed from just a few (compared to the signal
length l) measurements. The theorem was ﬁrst given in [173] and it is the ﬁrst time that such a theorem
provides results for the sparse analysis model formulation in a general context.

1348
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
1.23.14.3 Cosparsity
In [174], the task of sparse analysis modeling was approached via an alternative route, employing
the tools which were developed in [137,175] for treating general union-of-subspaces models. This
complementary point of view will also unveil different aspects of the problem by contributing to its
deeper understanding. We have done it before, where the notions of spark, coherence and RIP were all
mobilized to shed light from different corners to the sparse synthesis modeling task.
In the sparse synthesis formulation, one searches for a solution in a union of subspaces, which are
formed by all possible combinations of k columns of the dictionary, . Our signal vector lies in one
of these subspaces; the one which is spanned by the columns of  whose indices lie in the support
set (Section 1.23.11.1). In the sparse analysis approach things get different. The kick off point is the
sparsity of the transform S := T s, where  deﬁnes the transformation matrix or analysis operator.
Since S is assumed to be sparse, there exists an index set I such that ∀i ∈I, Si = 0. In other words,
∀i ∈I, φT
i s :=
7
φi, s
8
= 0, where φi stands for the ith column of . Hence, the subspace in which s
lives is the orthogonal complement of the subspace formed by those columns of , which correspond
to a zero in the transform vector S. Assume, now, that card(I) = Co. The signal, s, can be identiﬁed by
searching on the orthogonal complements of the subspaces formed by all possible combinations of Co
columns of , i.e.,
7
φi, s
8
= 0,
∀i ∈I.
The difference between the synthesis and analysis problems is illustrated in Figure 23.24. To facilitate
the theoretical treatment of this new setting, the notion of cosparsity was introduced in [174].
Deﬁnition 6.
The cosparsity of a signal s ∈Rl with respect to a p × l matrix T is deﬁned as
Co := p −
   T s
   
0 .
(23.86)
In words, the cosparsity is the number of zeros in the obtained transform vector S = T s; in contrast,
the sparsity measures the number of the nonzero elements of the respective sparse vector. If one assumes
that  has “full spark,”7 i.e., l + 1, then any l of the columns of , and thus any l rows of T are
guaranteed to be independent. This indicates that for such matrices, the maximum value that cosparsity
can take is equal to Co = l −1. Otherwise, the existence of l zeros will necessarily correspond to a zero
signal vector. Higher cosparsity levels are possible, by relaxing the full spark requirement.
Let now the cosparsity of our signal with respect to a matrix T be Co. Then, in order to dig out the
signal from the subspace in which is hidden, one must form all possible combinations of Co columns of
 and search in their orthogonal complements. In case that  is full rank, we have seen previously that
Co < l, and hence any set of Co columns of  are linearly independent. In other words, the dimension
of the span of those columns is Co. As a result, the dimensionality of the orthogonal complement, into
which we search for s, is l −Co.
We have by now accumulated enough information to elaborate a bit more on the statement made
before, concerning the different nature of the synthesis and analysis tasks. Let us consider a synthesis
task using an l × p dictionary and let k be the sparsity level in the corresponding expansion of a signal
in terms of this dictionary. The dimensionality of the subspaces in which the solution is sought is k
7Recall by Deﬁnition 2 that spark() is deﬁned for an l × p matrix  with p ≥l and of full rank.

1.23.14 Learning Sparse Analysis Models
1349
(a)
(b)
FIGURE 23.24
Searching for a spare vector s. (a) In the synthesis model, the sparse vector lies in subspaces formed by
combinations of k (in this case k = 2) columns of the dictionary . (b) In the analysis model, the sparse
vector lies in the orthogonal compliment of the subspace formed by Co (in this case Co = 2) columns of the
transformation matrix .
(k is assumed to be less than the spark of the respective matrix). Let us keep the same dimensionality
for the subspaces in which we are going to search for a solution in an analysis task. Hence, in this case
Co = l −k (assuming a full spark matrix). Also, for the sake of comparison, assume that the analysis
matrix is p × l. Solving the synthesis task, one has to search
 p
k

subspaces, while solving the analysis
task one has to search for

p
Co = l −k

subspaces. These are two different numbers; assuming that k ≪l
and also that l < p/2, which are natural assumptions for overcomplete dictionaries, then the latter of the
two numbers is much larger than the former one (use your computer to play with some typical values).
In other words, there are much more analysis than synthesis low-dimensional subspaces to be searched
for. The large number of low-dimensional subspaces makes the algorithmic recovery of a solution from
the analysis model a tougher task, [174]. However, it might reveal a much stronger descriptive power
of the analysis model compared to the synthesis one.
Another interesting aspect that highlights the difference between the two approaches is the following.
Assume that the synthesis and analysis matrices are related as  = , as it was the case for tight frames.
Under this assumption, T s provides a set of coefﬁcients for the synthesis expansion in terms of the
atoms of  = . Moreover, if
  T s
  
0 = k, then the T s is a possible k-sparse solution for the
synthesis model. However, there is no guarantee that this is the sparsest one.
It is now the time to investigate whether conditions that guarantee uniqueness of the solution for
the sparse analysis formulation can be derived. The answer is afﬁrmative and it has been established in
[174], for the case of exact measurements.

1350
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
Lemma 5.
Let  be a transformation matrix of full spark. Then, for almost all N ×l sensing matrices
and for N > 2(l −Co), the equation
y = Xs,
has at most one solution with cosparsity at least Co.
The above lemma guarantees the uniqueness of the solution, if one exists, of the following optimiza-
tion
min
s
   T s
   
0
s.t. y = Xs.
(23.87)
However, solving the previous ℓ0 minimization task is a difﬁcult one and we know that its synthesis
counterpart has been shown to be NP-hard, in general. Its relaxed convex relative is the ℓ1 minimization
min
s
   T s
   
1
s.t. y = Xs.
(23.88)
In [174], conditions are derived that guarantee the equivalence of the ℓ0 and ℓ1 tasks, in (23.87) and
(23.88), respectively; this is done in a way similar to that for the sparse synthesis modeling. Also, in
[174], a greedy algorithm inspired by the Orthogonal Matching Pursuit, discussed in Section 1.23.11.1,
has been derived. Other algorithms that solve the ℓ1 optimization in the analysis modeling framework
can be found in, e.g., [176–178]. NESTA can also be used for the analysis formulation.
1.23.15 A case study: time-frequency analysis
The goal of this section is to demonstrate how all the previously stated theoretical ﬁndings can be
exploited in the context of a real application. Sparse modeling has been applied to almost everything.
So, picking up a typical application would not be easy. We preferred to focus on a less “publicized”
application; that of analyzing echolocation signals emitted by bats. However, the analysis will take
place within the framework of time-frequency representation, which is one of the research areas that
signiﬁcantly inspired the evolution of compressed sensing theory. Time-Frequency analysis of signals
has been the ﬁeld of intense research for a number of decades, and it is one of the most powerful signal
processing tools. Typical applications include speech processing, sonar sounding, communications,
biological signals, EEG processing, to name but a few, see, e.g., [179,180].
1.23.15.1 Gabor transform and frames
It is not our intention to present the theory behind the Gabor transform. Our goal is to outline some basic
related notions and use it as a vehicle for the less familiar reader so that (a) to better understand how
redundant dictionaries are used and (b) get more acquainted with their potential performance beneﬁts.
The Gabor transform was introduced in the middle 1940s by Dennis Gabor (1900–1979), who was a
Hungarian-British engineer. His most notable scientiﬁc achievement was the invention of holography,
for which he won the Nobel prize for Physics in 1971. The discrete version of the Gabor transform can

1.23.15 A Case Study: Time-Frequency Analysis
1351
(a)
(b)
0
FIGURE 23.25
(a) The Gaussian window with spreading factor σ centered at time instant m. (b) Pulses obtained by windowing
three different sinusoids with Gaussian windows of different spread and applied at different time instants.
be seen as a special case of the Short Time Fourier Transform (STFT), e.g., [180,181]. In the standard
DFT transform, the full length of a time sequence, comprising l samples, is used all in “one-go” in
order to compute the corresponding frequency content. However, the latter can be time varying, so the
DFT will provide an average information, which cannot be of much use. The Gabor transform (and the
STFT in general) introduces time localization via the use of a window function, which slides along the
signal segment in time, and at each time instant focusses on a different part of the signal; this is a way
that allows one to follow the slow time variations, which take place in the frequency domain. The time
localization in the context of the Gabor transform is achieved via a Gaussian window function, i.e.,
g(n) :=
1
√
2πσ 2 exp

−n2
2σ 2

.
(23.89)
Figure 23.25a shows the Gaussian window, g(n −m), centered at time instant m. The choice of the
window spreading factor, σ, will be discussed later on.
Let us now construct the atoms of the Gabor dictionary. Recall that in the case of the signal repre-
sentation in terms of the DFT in (23.71), each frequency is represented only once, by the corresponding
sampled sinusoid, (23.72). In the Gabor transform, each frequency appears l times; the corresponding
sampled sinusoid is multiplied by the Gaussian window sequence, each time shifted by one sample.
Thus, at the ith frequency bin, we have l atoms, g(m,i), m = 0, 1, . . . ,l −1, with elements given by
g(m,i)(n) = g(n −m)ψi(n),
n, m, i = 0, 1, . . . ,l −1,
(23.90)
where ψi(n) is the nth element of the vector ψi in (23.72). This results to an overcomplete dictionary
comprising l2 atoms in the l-dimensional space. Figure 23.25b illustrates the effect of multiplying
different sinusoids with Gaussian pulses of different spread and at different time delays. Figure 23.26

1352
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
Time
Frequency
FIGURE 23.26
Each atom of the Gabor dictionary corresponds to a node in the time-frequency grid. That is, it is a sampled
windowed sinusoid whose frequency and location in time are given by the coordinates of the respective node.
In practice, this grid may be subsampled by factors α and β for the two axes respectively, in order to reduce
the number of the involved atoms.
is a graphical interpretation of the atoms involved in the Gabor dictionary. Each node, (m, i), in this
time-frequency plot, corresponds to an atom of frequency equal to 2π
lT i and delay equal to m.
Note that the windowing of a signal of ﬁnite duration inevitably introduces boundary effects, espe-
cially when the delay m gets close to the time segment edges, 0 and l −1. A solution to it, that facilitates
the theoretical analysis, is to use a modulo l arithmetic to wrap around at the edge points (this is
equivalent with extending the signal periodically), see, e.g., [182].
Once the atoms have been deﬁned, they can be stacked one next to the other to form the columns of
the l × l2 Gabor dictionary, G. It can be shown that the Gabor dictionary is a tight frame, [183].
1.23.15.2 Time-frequency resolution
By the deﬁnition of the Gabor dictionary, it is readily understood that the choice of the window spread,
as measured by σ, must be a critical factor, since it controls the localization in time. As it is known from
our Fourier transform basics, when the pulse becomes short, in order to increase the time resolution, its
corresponding frequency content spreads out, and vice versa. From Heisenberg’s principle, we know that
we can never achieve high time and frequency resolution, simultaneously; one is gained at the expense
of the other. It is here where the Gaussian shape in the Gabor transform is justiﬁed. It can be shown that
the Gaussian window gives the optimal trade-off between time and frequency resolution, [180,181].

1.23.15 A Case Study: Time-Frequency Analysis
1353
FIGURE 23.27
The shorter the width of the pulsed (windowed) sinusoid is in time the wider the spread of its frequency
content around the frequency of the sinusoid. The Gaussian-like curves along the frequency axis indicate
the energy spread in frequency of the respective pulses. The values of σt and σf indicate the spread in time
and frequency, respectively.
The time-frequency resolution trade-off is demonstrated in Figure 23.27, where three sinusoids are
shown windowed with different pulse durations. The diagram shows the corresponding spread in the
time-frequency plot. The value of σt indicates the time spread and σ f the spread of the respective
frequency content around the basic frequency of each sinusoid.
1.23.15.3 Gabor frames
In practice, l2 can take large values and it is desirable to see whether one can reduce the number of the
involved atoms, without sacriﬁcing the frame-related properties. This can be achieved by an appropriate
subsampling, as this is illustrated in Figure 23.26. We only keep the atoms that correspond to the red
nodes. That is, we subsample by keeping every α nodes in time and every β nodes in frequency in order
to form the dictionary, i.e.,
G(α,β) = {g(mα,iβ)},
m = 0, 1, . . . , l
α −1, i = 0, 1, . . . , l
β −1,
where α and β are divisors ofl. Then, it can be shown, e.g., ([180]), that if αβ < l the resulting dictionary
retains its frame properties. Once G(α,β) is obtained, the canonical dual frame is readily available via
(23.79) (adjusted for complex data), from which the corresponding set of expansion coefﬁcients, θ,
results.

1354
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
1.23.15.4 Time-frequency analysis of echolocation signals emitted by bats
Bats are using echolocation for navigation (ﬂying around at night), for prey detection (small insects)
and for prey approaching and catching; each bat adaptively changes the shape and frequency content
of its calls in order to better serve the previous tasks. Echolocation is used in a similar way for sonars.
Bats emit calls as they ﬂy, and “listen” to the returning echoes in order to build up a sonic map of
their surroundings. In this way, bats can infer on the distance, the size of obstacles as well as of other
ﬂying creatures/insects. Moreover, all bats emit special types of calls, called social calls, which are used
for socializing, ﬂirting, etc. The fundamental characteristics of the echolocation calls, as for example,
the frequency range and the average time duration, differ from species to species since, thanks to
evolution, bats have adapted their calls in order to get best suited to the environment in which a species
operates.
Time-Frequency analysis of echolocation calls provides information about the species (species iden-
tiﬁcation) as well as of the speciﬁc task and behavior of the bats in certain environments. Moreover, the
bat-biosonar system is studied in order humans to learn more about nature and be inspired for subsequent
advances in applications such as sonar navigation systems, radars, medical ultrasonic devices, etc.
Figure 23.28a shows a case of a recorded echolocation signal from bats. Zooming at two different
parts of the signal, we can observe that the frequency is changing with time. In Figure 23.28b, the DFT
of the signal is shown, but there is no much information that can be drawn from it, except that the
signal is compressible in the frequency domain; most of the activity takes place within a short range of
frequencies.
Our echolocation signal was a recording of total length T = 21.845 ms, [184]. Samples were taken
at the sampling frequency fs = 750 kHz , which results in a total of l = 16384 samples. Although
the signal itself is not sparse in the time domain, we will take advantage of the fact that it is sparse in
a transformed domain. We will assume that the signal is sparse in its expansion in terms of the Gabor
dictionary.
Our goal in this example is to demonstrate that one does not really need all 16384 samples to
perform time-frequency analysis; all the processing can be carried out by using a reduced number of
measurements, by exploiting the theory of compressed sensing. To form the measurements vector, y,
the number of measurements was chosen to be N = 2048. This amounts to a reduction of eight times
with respect to the number of available samples. The measurements vector was formed as
y = Xs,
where X is a N × l sensing matrix comprising ±1 generated in a random way. This means that once
we obtain y, one does not need to store the original samples any more, leading to a saving in memory.
Ideally, one could have obtained the reduced number of measurements by sampling directly the analog
signal at sub-Nyquist rates, as it has already been discussed at the end of Section 1.23.10. Another goal
is to use both the analysis and synthesis models and demonstrate their difference.
Three different spectrograms were computed. Two of them, shown in Figures 23.29b and 23.29c,
correspond to the reconstructed signals obtained by the analysis, (23.88), and the synthesis, (23.44),
formulations, respectively. In both cases, the NESTA algorithm was used and the G(128,64) frame
was employed. Note that the latter dictionary is redundant by a factor of 2. The spectrograms are

1.23.15 A Case Study: Time-Frequency Analysis
1355
(a)
(b)
FIGURE 23.28
(a) The recorded echolocation signal. The frequency of the signal is time varying and this is indicated by
focussing on two different parts of the signal. (b) Plot of the energy of the DFT transform coefﬁcients, Si.
Observe that most of the frequency activity takes place within a short frequency range.
the result of plotting the time-frequency grid and coloring each node (t, i) according to the energy
|θ|2 of the coefﬁcient associated with the respective atom in the Gabor dictionary. The full Gabor
transform applied on the reconstructed signals to obtain the spectrograms, in order to get a better
coverage of the time-frequency grid. The scale is logarithmic and the darker areas correspond to larger
values. The spectrogram of the original signal obtained via the full Gabor transform is shown in Figure
23.29d. It is evident, that the analysis model resulted in a more clear spectrogram, which resembles the
original one better. When the frame G(64,32) is employed, which is a highly redundant Gabor dictionary
comprising 8l atoms, then the analysis model results in a recovered signal whose spectrogram is visually
indistinguishable from the original one in Figure 23.29d.

1356
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
(a)
(c)
(b)
(d)
FIGURE 23.29
(a) Plot of the magnitude of the coefﬁcients, sorted in decreasing order, in the expansion in terms of the
G(128,64) Gabor frame. The results correspond to the analysis and synthesis model formulations. The third
curve corresponds to the case of analyzing the original vector signal directly, by projecting it on the dual frame.
(b) The spectrogram from the analysis. (c) The spectrogram from the synthesis formulations, respectively.
(d) The spectrogram corresponding to G(64,32) frame using the analysis formulation. For all cases, the number
of measurements used was one eighth of the total number of signal samples. A, B, and C indicate different
parts of the signal, as explained in the text.
Figure 23.29a is the plot of the magnitude of the corresponding Gabor transform coefﬁcients, sorted
in decreasing values. The synthesis model provides a sparser representation, in the sense that the coefﬁ-
cients decrease much faster. The third curve is the one that results if we multiply the dual frame matrix
˜G(128,64) directly with the vector of the original signal samples and it is shown for comparison reasons.

1.23.16 From Sparse Vectors to Low Rank Matrices: A Highlight
1357
To conclude, the curious reader may wonder what do these curves in Figure 23.29d mean after all.
The call denoted by (A) belongs to a Pipistrellus pipistrellus (!) and the call denoted by (B) is either a
social call or belongs to a different species. The (C) is the return echo from the signal (A). The large
spread in time of (C) indicates a highly reﬂective environment, [184].
1.23.16 From sparse vectors to low rank matrices: a highlight
In this section, we move beyond sparse vectors and our goal is to investigate if and how notions
related to sparsity can be generalized to matrices. We will see that such a generalization builds upon
linear algebra tools and notions related to SVD decomposition, low rank approximation and dimen-
sionality reduction. Our goal is to simply highlight the basic concepts and deﬁnitions without delving
into a deeper treatment. Our aim is to make the reader alert of the problems and their potential for
applications.
1.23.16.1 Matrix completion
Consider a signal vector s ∈Rl, where only N of its components are observed and the rest are unknown.
This is equivalent with sensing s via a sensing matrix X having its N rows picked uniformly at random
from the standard (canonical) basis  = I, where I is the l × l identity matrix. The question which is
now posed is whether it is possible to recover the missing components of s based on these N components.
From the theory presented, so far, we know that one can recover all the components of s, provided that
s is sparse in some basis or dictionary, , which exhibits low mutual coherence with  = I, and N is
large enough, as it has been pointed out in Section 1.23.10.
Inspired by the theoretical advances in Compressed Sensing, a question similar in ﬂavor and with a
prominent impact regarding practical applications was posed in [185]. Given a l1 ×l2 matrix M, assume
that only N << l1l2 among its entries are known. The question now is whether one is able to recover
the exact full matrix. This problem is widely known as matrix completion [185]. The answer, although it
might come as a surprise, is “yes” with high probability, provided that (a) the matrix is well structured,
(b) it has a low rank, r << l, where l = min (l1,l2), and (c) that N is large enough. Intuitively, this is
plausible because a low rank matrix is fully described in terms of a number of parameters (degrees of
freedom), which is much smaller than its total number of entries. These parameters are revealed via its
Singular Value Decomposition (SVD)
M =
r

i=1
σiuivT
i = U
⎡
⎢⎣
σ1
0
...
0
σr
⎤
⎥⎦V T ,
(23.91)
where r is the rank of the matrix, ui ∈Rl1 and vi ∈Rl2, i = 1, 2, . . . ,r, are the left and right orthonor-
mal singular vectors, spanning the column and row spaces of M respectively, σi, i = 1, 2, , . . . ,r, are
the corresponding singular values and U = [u1, u2, · · · , ur], V = [v1, v2, · · · , vr].
Let σ M denote the vector containing all the singular values of M, i.e., σ M = [σ1, σ2, · · · , σl]T , then
rank(M) := ∥σ M∥0. Counting the parameters associated with the singular values and vectors in (23.91)

1358
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
turns out that the number of degrees of freedom of a rank r matrix is equal to dM = r(l1 + l2) −r2
[186]. When r is small, dM is much smaller than l.
Let us denote with  the set of N pairs of indexes, (i, j), i = 1, , 2, . . . ,l1, j = 1, , 2, . . . ,l2, of the
locations of the known entries of M, which have been sampled uniformly at random. Adopting the main
reasoning followed so far, one would attempt to recover M based on the following rank minimization
problem
min
ˆM∈Rl1×l2
  σ ˆM
  
0
s.t.
ˆMi, j = Mi, j,
(i, j) ∈.
(23.92)
It turns out that, assuming that there exist a unique low-rank matrix having the speciﬁc known entries,
then (23.92) leads to the exact solution [185]. However, compared to the case of sparse vectors, in the
matrix completion problem the uniqueness issue gets much more involved. The following issues play
a crucial part concerning the uniqueness of the task in (23.92).
1. If the number of known entries is lower than the degrees of freedom, i.e., N < dM, then there is no
way to recover the missing entries whatsoever, since there is an inﬁnite number of low rank matrices
consistent with the N observed entries.
2. Even if N ≥dM, uniqueness is still not guaranteed. It is required that the N elements with indices
in  are such that at least one entry per column and one entry per row is observed. Otherwise,
even a rank-1 matrix, i.e., M = σ1u1vT
1 , is not possible to be recovered. This becomes clear with
a simple example. Assume that M is a rank-1 matrix and that no entry in the ﬁrst column as well
as in the last row is observed. Then, since for this case M(i, j) = σ1u1iv1 j, it is clear that no
information concerning the ﬁrst component of v1 as well as the last component of u1 is available;
hence these singular vector components are impossible to be recovered, regardless which method
is used. As a consequence, the matrix can not be completed. On the other hand, if the elements of
 are picked at random and N is large enough, one can only hope that  is such that to comply
with the previous requirement; i.e., at least one entry per row and column is observed, with high
probability. It turns out that this problem resembles the famous in probability theory theorem known
as the coupon collector’s problem. According to this, at least N = C0l ln l entries are needed, where
C0 is a constant [187]. This is the information theoretic limit for exact matrix completion [186] of
any low-rank matrix.
3. Even if points (1) and (2) before are fulﬁlled, still uniqueness is not guaranteed. In fact, not every
low rank matrix is liable to exact completion, regardless of the number and the positions of the
observed entries. We will demonstrate that with the aid of an example. Let one of the singular
vectors be sparse. Assume, without loss of generality, that the third left singular vector, u3, is sparse
with sparsity level k = 1 and also that its nonzero component is the ﬁrst one, i.e., u31 ̸= 0. The rest
of ui and all vi are assumed to be dense. Let us return to the SVD for a while, and speciﬁcally to
the leftmost formula given in (23.91). Observe that the matrix M is written as the sum of r,l1 × l2
matrices σiuivT
i , i = 1, . . . ,r. Thus, in this speciﬁc case where u3 is k = 1 sparse, the matrix
σ3u3vT
3 has zeros everywhere except from its ﬁrst row. In other words, the information that σ3u3vT
3
brings to the formation of M is concentrated to its ﬁrst row only. This argument can also be viewed
from another perspective; the entries of M obtained from any row but the ﬁrst one, do not provide

1.23.16 From Sparse Vectors to Low Rank Matrices: A Highlight
1359
any useful information with respect to the values of the free parameters σ3, u3, v3. As a result, in
this case, unless if one incorporates extra information about the sparse nature of the singular vector,
the entries from the ﬁrst row that are missed are not recoverable, since the number of parameters
concerning this row is larger than the available number of data.
Intuitively, when a matrix has dense singular vectors is better rendered for exact completion, since
each one among the observed entries carries information associated with all the dM parameters that
fully describe it. To this end, a number of conditions, which evaluate the suitability of the singular
vectors, have been established. The simplest one is given next [185]:
∥ui∥∞≤
#μB
l1
,
∥vi∥∞≤
#μB
l2
,
i = 1, . . . ,r,
(23.93)
where μB is a bound parameter. In fact, μB is a measure of the coherence8 of matrix U (and similarly
of V ), (vis-à-vis the standard basis), deﬁned as follows:
μ(U) := l1
r
max
1≤i≤l1
∥PUei∥2 ,
(23.94)
where PU deﬁnes the orthogonal projection to subspace U and ei is the ith vector of the canonical
basis. It is easy to show that ∥PUei∥2 =
  U T ei
  2. In essence, coherence is an index quantifying
the extent to which the singular vectors are correlated with the standard basis, ei, i = 1, 2, . . . ,l.
The smaller the μB is the less “spiky” the singular vectors are likely to be, and the corresponding
matrix is better suited for exact completion. Indeed, assuming for simplicity a square matrix M, i.e.,
l1 = l2 = l, then if any one among the singular vectors is sparse having a single nonzero component
only, then, taking into account that uT
i ui = vT
i vi = 1, this value will have magnitude equal to one
and the bound parameter will take its largest value possible, i.e., μB = l. On the other hand, the
smaller value that μB can get is 1, something that occurs when the components of all the singular
vectors assume the same value (in magnitude). Note that in this case, due to the normalization, this
common component value has magnitude 1
l . Tighter bounds to the matrix coherence result via the
more elaborate incoherence property [185,188] and the strong incoherence property [186]. In all
cases, the larger the bound parameter is the larger the number of known entries, which is required
in order to guarantee uniqueness, becomes.
In Section 1.23.16.3, the aspects of uniqueness will be discussed in the context of a real life appli-
cation.
The problem described in (23.92) is of limited practical interest since it is an NP-hard task. Thus,
following the Compressed Sensing paradigm, it is replaced by a convexly relaxed counterpart of it, i.e.,
min
ˆM∈Rl1×l2
  σ ˆM
  
1
s.t.
ˆMi, j = Mi, j,
(i, j) ∈,
(23.95)
8This is a quantity different than the mutual-coherence already discussed in Section 1.23.7.1.

1360
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
where
  σ ˆM
  
1, i.e., the sum of the singular values, is referred to as nuclear norm of the matrix ˆM, often
denoted as
   ˆM
   
∗. The nuclear norm minimization was proposed in [189] as a convex approximation
of rank minimization, which can be cast as a semideﬁnite program.
Theorem 8 ([186], Corollary 1.5).
Let M be a l1 × l2 matrix of rank r, which is a constant much
smaller than l = max (l1,l2), obeying (23.93). Suppose that we observe N entries of M with locations
sampled uniformly at random. Then there is a positive constant C such that if
N ≥Cμ4
Bl ln2 l,
(23.96)
then ˆM is the unique solution to (23.95) with probability at least 1 −l−3.
There might be an ambiguity on how small the rank should be in order for the corresponding matrix
to be characterized as “low rank.” More rigorously, a matrix is said to be of low rank if r = O(1),
which means that r is a constant with no dependence (not even logarithmic), on l. Matrix completion
is also possible for more general rank cases where instead of the mild coherence property of (23.93),
the incoherence and the strong incoherence properties [185,186,188,190] are mobilized in order to get
similar theoretical guaranties. The detailed exposition of these alternatives is out of the scope of this
paper. In fact, Theorem 8 embodies the essence of the matrix completion task: with high probability,
nuclear-norm minimization recovers all the entries of a low rank matrix M with no error. More impor-
tantly, the number of entries N, which the convexly relaxed problem requires, is only by a logarithmic
factor larger than the information theoretic limit; recall that the latter is equal to C0l ln l. Moreover,
similarly to Compressed Sensing, robust matrix completion in the present of noise is also possible as
long as the request ˆMi, j = Mi, j in (23.92) and (23.95) is replaced by
   ˆMi, j −Mi, j
   
2 ≤ϵ [191].
Furthermore, the notion of matrix completion has also been extended to tensors [192,193].
1.23.16.2 Robust PCA
The developments on matrix completion theory led, more recently, to the formulation and solution of
anotherproblemofhighsigniﬁcance.Tothisend,thenotation∥M∥1,i.e.,theℓ1 normofamatrix,isintro-
duced and it is deﬁned as the sum of the absolute values of its entries, i.e., ∥M∥1 = *l1
i=1
*l2
j=1 |Mi, j|.
In other words, it acts on the matrix as if this were a long vector. Assume that M is expressed as the sum
of a low rank matrix, L, and a sparse matrix, S, i.e., M = L + S. The following convex minimization
problem [194–196], usually referred to as principal component pursuit (PCP),
min
ˆL∈Rl1×l2, ˆS∈Rl1×l2
  σ ˆL
  
1 + λ
   ˆS
   
1
s.t.
ˆL + ˆS = M,
(23.97)
is shown to recover both L and S according to the following theorem [194]:
Theorem 9.
The PCP recovers both L and S with probability at least 1−cl−10
1
, where c is a constant,
provided that:

1.23.16 From Sparse Vectors to Low Rank Matrices: A Highlight
1361
1. the support set  of S is uniformly distributed among all sets of cardinality N,
2. the number, k, of nonzero entries of S is relatively small, [197], i.e., k ≤ρl1l2, where ρ is a sufﬁciently
small positive constant,
3. L obeys the incoherence property, with bound μc, which is simultaneously larger than μ(U), μ(V )
and l1l2
r ∥UV T ∥2
∞
4. the regularization parameter, λ, is constant with value λ =
1
√l2 ,
5. rank(L) ≤C
l2
μc ln2 l1 , with C being a constant.
In other words, based on all the entries of a matrix M, which is known that is the sum of two unknown
matrices L and S, with the ﬁrst one being of low rank matrix and the second being sparse, then PCP
recovers exactly, with probability almost 1, both L and S, irrespective of how large the magnitude of
the entries of S are, provided that both r and k are sufﬁciently small.
The applicability of the previous task is very broad. For example, PCP can be employed in order to
ﬁnd a low rank approximation of M. It is well known that the task of low rank approximation is closely
related to the dimensionality reduction task, where the columns of M are expressed in terms of the r
(principal components) columns of U, e.g., Chapter 6, [1]. However, in contrast to the standard SVD
or PCA approach, PCP is robust and insensitive to the presence of outliers, since these are naturally
modeled, via the presence of S. For this reason, the above task is widely known as robust PCA via
nuclear norm minimization. (More classical PCA techniques are known to be sensitive to outliers and
a number of alternative approaches have in the past been proposed towards its robustiﬁcation, e.g.,
[198–202]).
When PCP serves as a robust PCA approach, the matrix of interest is L and S accounts for the outliers.
However, PCP provides estimates for both L and S. As it will be discussed in the next subsection, state-
of-the-art applications are well accommodated when the focus of interest is turned into the sparse matrix
S itself.
Remarks 13.
•
Just as ℓ1-minimization is the tightest convex relaxation of the combinatorial ℓ0-minimization prob-
lem in compressed sensing, the nuclear-norm minimization is the tightest convex relaxation of the
NP-hard rank minimization problem; i.e., the nuclear ball {M ∈Rl1×l2 : ∥M∥∗≤1} is the convex
hull of the set of rank-one matrices with spectral norm bounded by one. Besides the Nuclear norm,
other heuristics have also been proposed, such as the log-det heuristic [203] and the max-norm [204].
•
The nuclear norm, as a rank minimization approach, is the generalization of the trace-related cost,
which is often used in the control community for the rank minimization of positive semideﬁnite
matrices [205]. Indeed, when the matrix is symmetric and positive semideﬁnite, the nuclear norm of
M is the sum of the eigenvalues and thus it is equal to the trace of M. Such problems arise when, for
example, the rank minimization task refers to covariance matrices and positive semideﬁnite Toeplitz
or Hankel matrices (see, e.g., [203]).
•
Both matrix completion (23.95) and PCP (23.97) can be formulated as semideﬁnite programs and are
solved via mobilizing interior-point methods. However, whenever the size of a matrix becomes large
(e.g., 100 × 100), these methods are deemed to fail in practice due to excessive power and memory
requirements. As a result, there is an increasing interest, which has propelled intensive research
efforts, for the development of efﬁcient methods to solve (23.95), (23.97) or related approximations,

1362
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
which scale well with large matrices. Many of these methods revolve around the philosophy of
the iterative soft and hard thresholding techniques, as discussed in previous sections. However, in
the current low rank approximation setting, it is the singular values of the estimated matrix which
are thresholded. As a result, in each iteration, the estimated matrix, after thresholding its singular
values, tends to be of lower rank. The thesholding of the singular values is either imposed, such as
in the case of the singular value thresholding (SVT) algorithm [206] or it results as a solution of the
regularized versions of (23.95) and (23.97) (see, e.g., [207–211]). Moreover, algorithms inspired by
greedy methods such as CoSaMP, have also been proposed [212,213].
•
Further developments on PCP led to improved versions [197] allowing for exact recovery even
though the number of nonzero entries of S approaches l1l2 arbitrarily close, provided that the sign
pattern of S is random. Furthermore, even full columns are allowed to be corrupted [214,215].
Moreover, fusions of PCP with matrix completion and Compressed Sensing are possible, in the
sense that only a subset of the entries of M is available and/or linear measurements of the matrix in a
Compressed Sensing fashion can be used instead of matrix entries (see, e.g., [213,216]). Moreover,
stable versions of PCP dealing with noise have also been investigated (see, e.g., [217]).
1.23.16.3 Applications of matrix completion and PCP
The number of applications in which these techniques are involved is ever increasing and their extensive
presentation is out of the scope of this paper. Next, some key applications will be selectively discussed
since they reveal the potential of these methods and at the same time will assist the reader for a better
understanding of the underline notions.
1.23.16.3.1
Matrix completion
A typical application, where the matrix completion problem arises, is in the collaborative ﬁltering task
[218], which is essential for building up successful recommender systems. Let us consider that a group
of individuals provide their ratings concerning products, which they have enjoyed. Then a matrix with
ratings can be ﬁlled where each row indexes a different individual and the columns index the products.
As a popular example take the case where the products are different movies. Inevitably, the associated
matrix will be partially ﬁlled, since it is not common that all customers have watched all the movies
and submit ratings for all of them. Matrix completion comes to provide an answer, potentially in the
afﬁrmative, to the following question. Can we predict the ratings that the users would give to ﬁlms that
they have not seen yet? This is the task of a recommender system in order to encourage users to watch
movies, which are likely to be of their preference. The exact objective of competition for the famous
Netﬂix prize (http://www.netﬂixprize.com/) was the development of such a recommender system.
The aforementioned problem provides a good opportunity to build up our intuition about the matrix
completion task. First, an individual’s preferences or taste on movies are typically governed by a small
number of factors, such as the gender, the actors they play in it, the continent of origin, etc. As a result,
a matrix fully ﬁlled with ratings is expected to be low rank. Moreover, it is clear that each user need
to have at least one movie rated in order to have any hope to ﬁll out his/her ratings across all movies.
The same is true for each movie. This requirement complies with the second requirement in Section
1.23.16.1, concerning uniqueness, i.e., one needs to know at least one entry per row and column. Finally,
imagine a single user who rates movies with criteria that are completely different to those used by the

1.23.17 Conclusions
1363
rest of the users. He/She could, for example, provide ratings at random or depending on, let us say, the
ﬁrst letter of the movie title. Such a scenario complies with the third point concerning the uniqueness in
the matrix completion problem, as previously discussed. Unless all the ratings of the speciﬁc user are
known, the matrix cannot get fully completed.
In the previous application, the matrix of interest can be characterized as approximately low rank.
In other cases, such as in sensor network localization [219], the rank of the matrix assumes an exact
value. The goal of localization is to assign geographic coordinates to each node in the sensor network
based on a square matrix, which contains the pairwise distances between the nodes [220,221]. It turns
out that this matrix is of very low rank, e.g., two or three, depending on whether the sensors are placed
in the 2D or 3D space. As a result, matrix completion is possible using a limited number of distance
measurements. The number of distance measurements is reduced either intentionally, in order to save
power and/or due to the presence of irregularities and obstacles in the deployment area, which renders
the communication among nodes impossible. Other applications of matrix completion includes system
identiﬁcation [222], recovering structure from motion [223] and multi-task learning [224].
1.23.16.3.2
Robust PCA/PCP
In the collaborative ﬁltering task, robust PCA offers an extra attribute compared to matrix completion,
which can be proved very crucial in practice. The users are allowed to even tamper with some of the
ratings without affecting the estimation of the low rank matrix. This seems to be the case whenever the
rating process involves many individuals in an environment, which is not strictly controlled, since some
of them occasionally are expected to provide ratings in an ad hoc, or even malicious manner.
One of the ﬁrst applications of PCP was in video surveillance systems [194] and the main idea
behind it appeared to be popular and extendable to a number of computer vision applications. Take the
example of a camera recording a sequence of frames consisting of a merely static background and a
foreground with few moving objects, e.g., vehicles and/or individuals. A common task in surveillance
video is to extract from the background the foreground, in order, for example, to detect any activity or to
proceed with further processing such as face recognition. Suppose the successive frames are converted
to vectors in lexicographic order and then are placed as columns in a matrix M. Due to the background,
even though this may slightly vary due to changes in illumination, successive columns are expected to
be highly correlated. As a result, the background can be modeled as an approximately low rank matrix
L. On the other hand, the objects on the foreground, appear as “anomalies” concerning a fraction of
pixels of each frame, i.e., a limited number of entries in each column of M. Moreover, due to the motion
of the forground objects, the positions of these anomalies are likely to change from one column of M to
the next. Therefore, they can be modeled as a sparse matrix S. Note that in this application, the matrix
of interest is the sparse matrix rather than the low rank one.
1.23.17 Conclusions
In this paper, we provided an overview of the major theoretical advances as well as the main trends in
algorithmic developments in the area of sparsity-aware learning and compressed sensing. Both batch
processing and online processing techniques were considered. A case study in the context of time-
frequency analysis of signals was also presented. Our intent is to update the review from time to time,
since this is a very hot research area with a momentum and speed that is sometimes difﬁcult to follow up.

1364
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
Appendix
The stage of our discussion in this Appendix is the real Euclidean space Rl, where l is a positive
integer. Although all of the following arguments hold true even in the case where the Rl is substituted
by the much more general Hilbert space setting, we conﬁne ourselves here, for the sake of simplicity,
to the Euclidean space. Henceforth, the space Rl is considered to be equipped with an inner product,
which, in the present context, is denoted by ⟨θ1, θ2⟩, ∀θ1, θ2 ∈Rl. A standard example of such an
inner product is the classical vector/dot one, deﬁned by ⟨θ1, θ2⟩:= θ T
1 θ2, ∀θ1, θ2 ∈Rl, where the
superscript (·)T stands for vector transposition. Another example of an inner product for the space Rl is
the following weighted one; ⟨θ1, θ2⟩:= θ T
1 Wθ2, ∀θ1, θ2 ∈Rl, where W ∈Rl×l is any user-deﬁned
positive deﬁnite matrix. In order not to spare the generality of the following discussion, we let ⟨·, ·⟩stand
for any user-deﬁned inner product on the linear space Rl. Given such an inner product, the associated
norm is induced according to the following rule: ∥·∥:= √⟨·, ·⟩. Excellent resources for a deeper study
on the extremely rich subject of convex analysis are [225–228].
We start, now, with few notions of fundamental importance to convex analysis.
A.1 Closed convex sets and metric projection mappings
Deﬁnition 7 (Convex set, convex function).
A non-empty subset C of Rl is called convex if ∀θ1, θ2 ∈
Rl, and ∀λ ∈[0, 1], the following holds true: λθ1 + (1 −λ)θ2 ∈C.
Moreover, a function L : Rl →R is called convex if ∀θ1, θ2 ∈Rl, and ∀λ ∈[0, 1], L(λθ1 + (1 −
λ)θ2) ≤λL(θ1)+(1−λ)L(θ2). The function L is called strictly convex if ∀λ ∈(0, 1) and ∀θ1, θ2 ∈Rl,
such that θ1 ̸= θ2, we have L(λθ1 + (1 −λ)θ2) < λL(θ1) + (1 −λ)L(θ2).
The epigraph of a function L is deﬁned as the set
epi(L) :=

(θ,r) ∈Rl × R : L(θ) ≤r

.
In other words, the epigraph of L is the set of all points of Rl × R which belong to and lie above the
graph of L. Notice, also, by the deﬁnition of convexity, that L is convex if and only if epi(L) is convex.
Given a real number ξ, the lower level set of L at height ξ is deﬁned as the set
lev≤ξ(L) :=

θ ∈Rl : L(θ) ≤ξ

.
For the geometry behind the previous deﬁnitions, see Figure 23.30.
Deﬁnition 8 (The metric projection mapping).
Given a non-empty closed convex set C ⊂Rl,
the metric projection mapping onto C is deﬁned as the operator that maps to each θ ∈Rl the unique
PC(θ) ∈C such that
∥θ −PC(θ)∥= d(θ, C).
In other words, the point PC(θ) is the unique minimizer of the function ∥θ −x∥, x ∈C. Obviously, in
the case where θ ∈C, then PC(θ) = θ.
As an example, the metric projection mapping onto the hyperslab is given next.

Appendix
1365
FIGURE 23.30
A convex function L, its epigraph, and the lower level set of L at height 0.
Example 8 (Hyperslab).
A hyperslab S[ϵ] is the closed convex subset of Rl which is deﬁned as
S[ϵ] :=

x ∈Rl : | ⟨a, x⟩−c| ≤ϵ

,
for some nonzero a ∈Rl and some c ∈R. The projection mapping PS[ϵ] onto S[ϵ] is given as follows:
PS[ϵ](θ) = θ −
⎧
⎪⎨
⎪⎩
⟨a,θ⟩−c−ϵ
∥a∥2
a, if ⟨a, θ⟩> c + ϵ,
0,
if | ⟨a, θ⟩−c| ≤ϵ,
⟨a,θ⟩−c+ϵ
∥a∥2
a, if ⟨a, θ⟩< c −ϵ.
(23.98)
For the related geometry, see Figure 23.31. Notice that a stands for the normal vector deﬁning the
hyperplanes associated with the hyperslab.
A.2 The subgradient
Deﬁnition 9 (Subgradient, Subdifferential).
Given a convex function L, deﬁned on Rl, and a point
θ ∈Rl, the subgradient of L at θ is deﬁned as any vector, h, such that
⟨h, x −θ⟩+ L(θ) ≤L(x),
∀x ∈Rl.
(23.99)
If the function L is differentiable at θ then the subgradient coincides with the (unique) gradient. As it is
the case for the gradient, a subgradient deﬁnes a hyperplane. This hyperplane “supports” the epigraph
of L; that is, the epigraph is on the one side of this hyperplane (see Figure 23.32). At θ1, the convex
function is differentiable and there is only one subgradient, which coincides with the gradient. Thus at
this point, there is a simple hyperplane that supports the epigraph. At θ2, the function is not differentiable.

1366
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
FIGURE 23.31
A hyperslab and its associated projection mapping.
FIGURE 23.32
The graph of L and supporting hyperplanes generated by the subgradients at points, θ1, θ2, where the function
is differentiable and non-differentiable, respectively.
Hence there is an inﬁnity of subgradients that deﬁne hyperplanes that support the epigraph. The set of
all subgradients at a point θ is known as the subdifferential and is denoted as ∂L, i.e.,
∂L(θ) :=

h ∈Rl : ⟨h, x −θ⟩+ L(x) ≤L(x),
∀x ∈Rl
.
Next, the subdifferential of L(θ) := |θ|, θ ∈R is given:
∂L(θ) =
[−1, 1], if θ = 0,
sgn(θ), if θ ̸= 0,

References
1367
FIGURE 23.33
The graph of the function |·|, and the supporting hyperplanes generated by the subgradients of |·| at θ = 0.
where sgn(·) stands for the sign of a real number. For the geometry associated to this cost function see
Figure 23.33.
Relevant theory: Signal processing theory
See this Volume, Chapter 10 Frames
See this Volume, Chapter 11 Parametric Estimation
See this Volume, Chapter 12 Adaptive Filters
References
[1] S. Theodoridis, K. Koutroumbas, Pattern Recognition, fourth ed., Academic Press, 2009.
[2] J.F. Claerbout, F. Muir, Robust modeling with erratic data, Geophysics 38 (5) (1973) 826–844.
[3] H.L. Taylor, S.C. Banks, J.F. McCoy, Deconvolution with the ℓ1 norm, Geophysics 44 (1) (1979) 39–52.
[4] D.L. Donoho, B.F. Logan, Signal recovery and the large sieve, SIAM J. Appl. Math. 52 (2) (1992) 577–591.
[5] F. Santosa, W.W. Symes, Linear inversion of band limited reﬂection seismograms, SIAM J. Sci. Comput.
7 (4) (1986) 1307–1330.
[6] R. Tibshirani, Regression shrinkage and selection via the LASSO, J. Roy. Statist. Soc. B 58 (1) (1996)
267–288.
[7] S. Chen, D.L. Donoho, M. Saunders, Atomic decomposition by basis pursuit, SIAM J. Sci. Comput. 20 (1)
(1998) 33–61.
[8] A.M. Bruckstein, D.L. Donoho, M. Elad, From sparse solutions of systems of equations to sparse modeling
of signals and images, SIAM Rev. 51 (1) (2009) 34–81.
[9] S. Boyd, L. Vandenberghe, Convex Optimization, Cambridge University Press, 2004.
[10] Y. Ye, Interior Point Methods: Theory and Analysis, Wiley, New York, 1997.

1368
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
[11] A. Antoniadis, Wavelet methods in statistics: some recent developments and their applications, Statist. Surv.
1 (2007) 16–55.
[12] J. Arenas-Garcia, A.R. Figueiras-Vidal, Adaptive combination of proportionate ﬁlters for sparse echo can-
cellation, IEEE Trans. Audio Speech Lang. Process. 17 (6) (2009) 1087–1098.
[13] J. Benesty, T. Gansler, D.R. Morgan, M.M. Sondhi, S.L. Gay, Advances in Network and Acoustic Echo
Cancellation, Springer-Verlag, Berlin, 2001.
[14] P.A. Naylor, J. Cui, M. Brookes, Adaptive algorithms for sparse echo cancellation, Signal Process. 86 (2004)
1182–1192.
[15] S. Haykin, Adaptive Filter Theory, third ed., Prentice-Hall, NJ, 1996.
[16] A.H. Sayed, Fundamentals of Adaptive Filtering, John Wiley & Sons, New Jersey, 2003.
[17] S. Ariyavisitakul, N.R. Sollenberger, L.J. Greenstein, Tap-selectable decision feedback equalization, IEEE
Trans. Commun. 45 (12) (1997) 1498–1500.
[18] S.F. Cotter, B.D. Rao, Matching pursuit based decision-feedback equalizers, in: IEEE Conference on Acous-
tics, Speech and Signal Processing (ICASSP), Istanbul, Turkey, 2000.
[19] M. Ghosh, Blind decision feedback equalization for terrestrial television receivers, Proc. IEEE 86 (10) (1998)
2070–2081.
[20] A. Rondogiannis, K. Berberidis, Efﬁcient decision feedback equalization for sparse wireless channels, IEEE
Trans. Wireless Commun. 2 (3) (2003) 570–581.
[21] D. Eiwen, G. Taubock, F. Hlawatsch, H.G. Feichtinger, Group sparsity methods for compressive channel
estimation in doubly dispersive multicarrier systems, in: Proceedings IEEE SPAWC, Marrakech, Morocco,
June 2010.
[22] D. Eiwen, G. Taubock, F. Hlawatsch, H. Rauhut, N. Czink, Multichannel-compressive estimation of dou-
bly selective channels in MIMO-OFDM systems: exploiting and enhancing joint sparsity, in: Proceedings
International Conference on Acoustics, Speech and Signal Processing (ICASSP), Dallas, Texas, 2010.
[23] W.U. Bajwa, J. Haupt, A.M. Sayeed, R. Nowak, Compressed channel sensing: a new approach to estimating
sparse multipath channels, Proc. IEEE 98 (6) (2010) 1058–1076.
[24] S. Mallat, S. Zhang, Matching pursuit in a time-frequency dictionary, IEEE Trans. Signal Process. 41 (1993)
3397–3415.
[25] R.R. Coifman, M.V. Wickerhauser, Entropy-based algorithms for best basis selection, IEEE Trans. Inform.
Theory 38 (2) (1992) 713–718.
[26] Q. Qiu, V.M. Patel, P. Turaga, R. Chellappa, Domain adaptive dictionary learning, in: Proceedings of the
European Conference on Computer Vision (ECCV), Florene, Italy, 2012.
[27] R. Rubinstein, A. Bruckstein, M. Elad, Dictionaries for sparse representation modeling, Proc. IEEE 98 (6)
(2010) 1045–1057.
[28] I. Tosi´c, P. Frossard, Dictionary Learning, IEEE Signal Process. Mag. 28 (2) (2011) 27–38.
[29] M. Yaghoobi, L. Daudet, M. Davies, Parametric dictionary design for sparse coding, IEEE Trans. Signal
Process. 57 (12) (2009) 4800–4810.
[30] M. Elad, Sparse and Redundant Representations: From Theory to Applications in Signal and Image Process-
ing, Springer, 2010.
[31] S. Wright, R. Nowak, M. Figueiredo, Sparse reconstruction by separable approximation, IEEE Trans. Signal
Process. 57 (7) (2009) 2479–2493.
[32] A. Maleki, L. Anitori, Z. Yang, R. Baraniuk, Asymptotic analysis of complex LASSO via complex approx-
imate message passing (CAMP), IEEE Trans. Inform. Theory (2011). arXiv:1108.0477.
[33] I.Daubechies,Time-frequencylocalizationoperators:ageometricphasespaceapproach,IEEETrans.Inform.
Theory 34 (4) (1988) 605–612.
[34] B.K. Natarajan, Sparse approximate solutions to linear systems, SIAM J. Comput. 24 (1995) 227–234.

References
1369
[35] G.B. Dantzig, Linear Programming and Extensions, Princeton University Press, Princeton, NJ, 1963.
[36] A.M. Pinkus, On ℓ1-approximation, Cambridge Tracts in Mathematics, vol. 93, Cambridge University Press,
1989.
[37] D.L. Donoho, M. Elad, Optimally sparse representation in general (nonorthogonal) dictionaries via ℓ1 min-
imization, in: Proceedings of National Academy of Sciences, 2003, pp. 2197–2202.
[38] I.F. Gorodnitsky, B.D. Rao, Sparse signal reconstruction from limited data using FOCUSS: a re-weighted
minimum norm algorithm, IEEE Trans. Signal Process. 45 (3) (1997) 600–614.
[39] L.R. Welch, Lower bounds on the maximum cross correlation of signals, IEEE Trans. Inform. Theory 20 (3)
(1974) 397–399.
[40] D.L. Donoho, X. Huo, Uncertainty principles and ideal atomic decomposition, IEEE Trans. Inform. Theory
47 (7) (2001) 2845–2862.
[41] R. Gribonval, M. Nielsen, Sparse decompositions in unions of bases, IEEE Trans. Inform. Theory 49 (12)
(2003) 3320–3325.
[42] M. Elad, A.M. Bruckstein, A generalized uncertainty principle and sparse representations in pairs of basis,
IEEE Trans. Inform. Theory 48 (9) (2002) 2558–2567.
[43] D.L. Donoho, J.T. Precise undersampling theorems, Proc. IEEE, 98 (6) (2010) 913–924.
[44] E.J. Candès, T. Tao, Decoding by linear programming, IEEE Trans. Inform. Theory 51 (12) (2005) 4203–
4215.
[45] R.G. Baraniuk, M. Davenport, R. DeVore, M.B. Wakin, A simple proof of the restricted isometry property
for random matrices, Constr. Approx. 28 (2008) 253–263.
[46] E.J. Candès, J. Romberg, Practical signal recovery from random projections, in: Proceedings of the SPIE
17th Annual Symposium on Electronic Imaging, Bellingham, WA, 2005.
[47] E.J. Candès, J. Romberg, T. Tao, Stable recovery from incomplete and inaccurate measurements, Commun.
Pure Appl. Math. 59 (8) (2006) 1207–1223.
[48] E.J. Candès, M.B. Wakin, An introduction to compressive sampling, IEEE Signal Process. Mag. 25 (2)
(2008) 21–30.
[49] T.T. Cai, G. Xu, J. Zhang, On recovery of sparse signals via ℓ1 minimization, IEEE Trans. Inform. Theory
55 (7) (2009) 3388–3397.
[50] S. Mendelson, A. Pajor, N. Tomczak-Jaegermann, Uniform uncertainty principle for Bernoulli and subgaus-
sian ensembles, Constr. Approx. 28 (2008) 277–289.
[51] M. Rudelson, R. Vershynin, On sparse reconstruction from Fourier and Gaussian measurements, Commun.
Pure Appl. Math. 61 (8) (2008) 1025–1045.
[52] E. Candès, J. Romberg, T. Tao, Robust uncertainty principles: exact signal reconstruction from highly incom-
plete Fourier information, IEEE Trans. Inform. Theory 52 (2) (2006) 489–509.
[53] J. Haupt, W.U. Bajwa, G. Raz, R. Nowak, Toeplitz compressed sensing matrices with applications to sparse
channel estimation, IEEE Trans. Inform. Theory 56 (11) (2010) 5862–5875.
[54] M.F. Duarte, R.G. Baraniuk, Kronecker Compressive Sensing, IEEE Trans. Image Process. 21 (2) (2012)
494–504.
[55] Y. Rivenson, A. Stern, Compressed imaging with a separable sensing operator, IEEE Signal Process. Lett.
16 (6) (2009) 449–452.
[56] D.L. Donoho, J. Tanner, Counting faces of randomly projected polytopes when the projection radically
lowers dimension, Technical Report 2006–11, Stanford University, 2006.
[57] P. Bickel, Y. Ritov, A. Tsybakov, Simultaneous analysis of LASSO and Dantzig selector, Ann. Statist. 37 (4)
(2009) 1705–1732.
[58] A. Cohen, W. Dahmen, R. DeVore, Compressed sensing and best k-term approximation, J. Am. Math. Soc.
22 (1) (2009) 211–231.

1370
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
[59] G. Tang, A. Nehorai, Performance analysis of sparse recovery based on constrained minimal singular values,
IEEE Trans. Signal Process. 59 (12) (2011) 5734–5745.
[60] E. Candès, T. Tao, Near optimal signal recovery from random projections: universal encoding strategies,
IEEE Trans. Inform. Theory 52 (12) (2006) 5406–5425.
[61] J.A. Tropp, J.N. Laska, M.F. Duarte, J.K. Romberg, G. Baraniuk, Beyond Nyquist: efﬁcient sampling of
sparse bandlimited signals, IEEE Trans. Inform. Theory 56 (1) (2010) 520–544.
[62] D. Takhar, V. Bansal, M. Wakin, M. Duarte, D. Baron, K.F. Kelly, R.G. Baraniuk, A compressed sensing
camera: new theory and an implementation using digital micromirrors, in: Proceedings on Computational
Imaging (SPIE), San Jose, CA, 2006.
[63] R. Baraniuk, V. Cevher, M. Wakin, Low-dimensional models for dimensionality reduction and signal recov-
ery: a geometric perspective, Proc. IEEE 98 (6) (2010) 959–971.
[64] Y.M. Lu, M.N. Do, Sampling signals from a union of subspaces, IEEE Signal Process. Mag. 25 (2) (2008)
41–47.
[65] D. Achlioptas, Database-friendly random projections, in: Proceedings of the Symposium on Principles of
Database Systems (PODS), ACM Press, 2001, pp. 274–281.
[66] A. Blum, Random projection, margins, kernels and feature selection, in: Lecture Notes on Computer Science
(LNCS), 2006, pp. 52–68.
[67] Dasgupta S. Experiments with random projections, in: Proceedings of the 16th Conference on Uncertainty
in Artiﬁcial Intelligence, Morgan-Kaufmann, San Francisco, CA, USA, 2000, pp. 143–151.
[68] R. Calderbank, S. Jeafarpour, R. Schapire, Compressed learning: universal spars dimensionality reduction
and learning in the measurement domain, Technical Report, Rice University, 2009.
[69] R. Baraniuk, M. Wakin, Random projections of smooth manifolds, Found. Comput. Math. 9 (1) (2009)
51–77.
[70] M. Wakin, Manifold-based signal recovery and parameter estimation from compressive measurements, 2008.
preprint: http://arxiv.org/abs/1002.1247.
[71] M. Unser, Sampling: 50 years after Shannon, Proc. IEEE 88 (4) (2000) 569–587.
[72] Y.-P. Lin, P.P. Vaidyanathan, Periodically nonuniform sampling of bandpass signals, IEEE Trans. Circ. Syst.
II 45 (3) (1998) 340–351.
[73] R.G. Vaughan, N.L. Scott, D.R. White, The theory of bandpass sampling, IEEE Trans. Signal Process. 39
(9) (1991) 1973–1984.
[74] R. Venkataramani, Y. Bresler, Perfect reconstruction formulas and bounds on aliasing error in sub-Nyquist
nonuniform sampling of multiband signals, IEEE Trans. Inform. Theory 46 (6) (2000) 2173–2183.
[75] M. Mishali, Y.C. Eldar, A. Elron, Xampling: analog data compression, in: Proceedings Data Compression
Conference, Snowbird, Utah, USA, 2010.
[76] Z. Tian, G.B. Giannakis, Compressed sensing for wideband cognitive radios, in: Proceedings of the IEEE
Conference on Acoustics, Speech and Signal Processing (ICASSP), 2007, pp. 1357–1360.
[77] Z. Yu, S. Hoyos, B.M. Sadler, Mixed-signal parallel compressed sensing and reception for cognitive radio, in:
Proceedings IEEE Conference on Acoustics, Speech and Signal Processing (ICASSP), 2008, pp. 3861–3864.
[78] S. Kirolos, J.N. Laska, M.B. Wakin, M.F. Duarte, D. Baron, T. Ragheb, Y. Massoud, R.G. Baraniuk, Analog
to information conversion via random demodulation, in: Proceedings of the IEEE Dallas/CAS Workshop on
Design, Applications, Integration and Software, Dallas, USA, 2006, pp. 71–74.
[79] M. Mishali, Y.C. Eldar, A. Elron, Xampling: signal acquisition and processing in union of subspaces, IEEE
Trans. Signal Process. 59 (10) (2011) 4719–4734.
[80] F. Chen, A.P. Chandrakasan, V.M. Stojanovic, Design and analysis of hardware efﬁcient compressed
sensing architectures for compression in wireless sensors, IEEE Trans. Solid State Circ. 47 (3) (2012)
744–756.

References
1371
[81] P. Maechler, N. Felber, H. Kaeslin, A. Burg, Hardware-efﬁcient random sampling of Fourier-sparse signals,
in: Proceedings of the IEEE International Symposium on Circuits and Systems (ISCAS), 2012.
[82] P.L. Dragotti, M. Vetterli, T. Blu, Sampling moments and reconstructing signals of ﬁnite rate of innovation:
Shannon meets Strang-Fix, IEEE Trans. Signal Process. 55 (5) (2007) 1741–1757.
[83] T. Blu, P.L. Dragotti, M. Vetterli, P. Marziliano, L. Coulot, Sparse sampling of signal innovations, IEEE
Signal Process. Mag. 25 (2) (2008) 31–40.
[84] M. Vetterli, P. Marzilliano, T. Blu, Sampling signals with ﬁnite rate of innovation, IEEE Trans. Signal Process.
50 (6) (2002) 1417–1428.
[85] E. Matusiak, Y.C. Eldar, Sub-Nyquist sampling of short pulses, IEEE Trans. Signal Process. 60 (3) (2012)
1134–1148.
[86] M.F. Duarte, Y. Eldar, Structured compressed sensing: from theory to applications, IEEE Trans. Signal
Process. 59 (9) (2011) 4053–4085.
[87] M. Mishali, Y.C. Eldar, Sub-Nyquist sampling, IEEE Signal Process. Mag. 28 (6) (2011) 98–124.
[88] A.C. Gilbert, S. Muthukrisnan, M.J. Strauss, Improved time bounds for near-optimal sparse Fourier repre-
sentation via sampling, in: Proceedings of SPIE (Wavelets XI), San Diego, CA, 2005.
[89] D. Needell, J.A. Tropp, COSAMP: iterative signal recovery from incomplete and inaccurate samples, Appl.
Comput. Harmon. Anal. 26 (3) (2009) 301–321.
[90] V.N. Temlyakov, Nonlinear methods of approximation, Foun. Comput. Math. 3 (1) (2003) 33–107.
[91] R.A. DeVore, V.N. Temlyakov, Some remarks on greedy algorithms, Adv. Comput. Math. 5 (1996) 173–187.
[92] M.A. Davenport, M.B. Wakin, Analysis of orthogonal matching pursuit using the restricted isometry property,
IEEE Trans. Inform. Theory 56 (9) (2010) 4395–4401.
[93] J.A. Tropp, Greed is good, IEEE Trans. Inform. Theory 50 (2004) 2231–2242.
[94] T. Zhang, Sparse recovery with orthogonal matching pursuit under RIP, IEEE Trans. Inform. Theory 57 (9)
(2011) 6215–6221.
[95] B. Efron, T. Hastie, I.M. Johnstone, R. Tibshirani, Least angle regression, Ann. Statist. 32 (2004) 407–499.
[96] Y. Tsaig, Sparse solution of underdetermined linear systems: algorithms and applications, PhD Thesis,
Stanford University, 2007.
[97] M.R. Osborne, B. Presnell, B.A. Turlach, A new approach to variable selection in least squares problems,
IMA J. Numer. Anal. 20 (2000) 389–403.
[98] M.S. Asif, J. Romberg, Dynamic updating for ℓ1 minimization, IEEE J. Sel. Top. Signal Process. 4 (2) (2010)
421–434.
[99] D.M. Malioutov, M. Cetin, A.S. Willsky, Homotopy continuation for sparse signal representation, in: IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2005, pp. 733–736.
[100] M.D. Plumbley, Geometry and homotopy for L1 sparse representation, in: Proceedings of the International
Workshop on Signal Processing with Adaptive Sparse Structured Representations (SPARS), Rennes, France,
2005.
[101] W. Dai, O. Milenkovic, Subspace pursuit for compressive sensing signal reconstruction, IEEE Trans. Inform.
Theory 55 (5) (2009) 2230–2249.
[102] D.L. Donoho, I.M. Johnstone, Ideal spatial adaptation by wavelet shrinkage, Biometrika 81 (3) (1994)
425–455.
[103] J.C. Hoch, A.S. Stern, D.L. Donoho, I.M. Johnstone, Maximum entropy reconstruction of complex (phase
sensitive) spectra, J. Magn. Reson. 86 (2) (1990) 236–246.
[104] P.A. Jansson, Deconvolution: Applications in Spectroscopy, Academic Press, New York, 1984.
[105] N.G. Kingsbury, T.H. Reeves, Overcomplete image coding using iterative projection-based noise shaping,
in: Proceedings IEEE International Conference on Image Processing (ICIP), 2002, pp. 597–600.
[106] P.L. Combettes, V.R. Wajs, Signal recovery by proximal forward-backward splitting, SIAM J. Multiscale
Model. Simul. 4 (4) (2005) 1168–1200.

1372
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
[107] I. Daubechies, M. Defrise, C. De-Mol, An iterative thresholding algorithm for linear inverse problems with
a sparsity constraint, Commun. Pure Appl. Math. 57 (11) (2004) 1413–1457.
[108] M. Elad, B. Matalon, M. Zibulevsky, Coordinate and subspace optimization methods for linear least squares
with non-quadratic regularization, Appl. Comput. Harmon. Anal. 23 (2007) 346–367.
[109] M.A. Figueiredo, R.D. Nowak, An EM algorithm for wavelet-based image restoration, IEEE Trans. Image
Process. 12 (8) (2003) 906–916.
[110] L. Hageman, D. Young, Applied Iterative Methods, Academic Press, New York, 1981.
[111] A. Beck, M. Teboulle, A fast iterative shrinkage algorithm for linear inverse problems, SIAM J. Imag. Sci.
2 (1) (2009) 183–202.
[112] R.T. Rockafellar, Monotone operators and the proximal point algorithms, SIAM J. Control Optim. 14 (5)
(1976) 877–898.
[113] T. Hale, W. Yin, Y. Zhang, A ﬁxed-point continuation method for l1 regularized minimization with applica-
tions to compressed sensing, Technical Report TR07-07, Department of Computational and Applied Math-
ematics, Rice University, 2007.
[114] Y.E. Nesterov, A method for solving the convex programming problem with convergence rate O(1/k2),
Dokl. Akad. Nauk SSSR 269 (1983) 543–547 (in Russian).
[115] T. Blumensath, M.E. Davies, Iterative hard thresholding for compressed sensing, Appl. Comput. Harmon.
Anal. 27 (3) (2009) 265–274.
[116] T. Blumensath, M.E. Davies, Normalized iterative hard thresholding: guaranteed stability and performance,
IEEE Sel. Top. Signal Process. 4 (2) (2010) 298–309.
[117] Z.Q. Luo, P. Tseng, On the convergence of the coordinate descent method for convex differentiable mini-
mization, J. Optim. Theory Appl. 72 (1) (1992) 7–35.
[118] M. Zibulevsky, M. Elad, L1–L2 optimization in signal processing, IEEE signal Process. Mag. 27 (3) (2010)
76–88.
[119] A. Maleki, D.L. Donoho, Optimally tuned iterative reconstruction algorithms for compressed sensing, IEEE
J. sel. Top. Signal Process. 4 (2) (2010) 330–341.
[120] S. Foucart, Hard Thresholding pursuit: an algorithm for compressive sensing, SIAM J. Numer. Anal. 49 (6)
(2011) 2543–2563.
[121] P.L. Combettes, J.-C. Pesquet, Proximal splitting methods in signal processing, in: Fixed-Point Algorithms
for Inverse Problems in Science and Engineering, Springer-Verlag, 2011.
[122] S. Becker, J. Bobin, E.J. Candès, NESTA: a fast and accurate ﬁrst-order method for sparse recovery, SIAM
J. Imag. Sci. 4 (1) (2011) 1–39.
[123] E. van den Berg, M.P. Friedlander, Probing the pareto frontier for the basis pursuit solutions, SIAM J. Sci.
Comput. 31 (2) (2008) 890–912.
[124] S.-J. Kim, K. Koh, M. Lustig, S. Boyd, D. Gorinevsky, An interior-point method for large-scale ℓ1-regularized
Least Squares, IEEE J. Sel. Top. Signal Process. 1 (4) (2007) 606–617.
[125] I. Daubechies, R. DeVore, M. Fornasier, C.S. Güntürk, Iteratively reweighted least squares minimization for
sparse recovery, Commun. Pure Appl. Math. 63 (1) (2010) 1–38.
[126] W. Yin, S. Osher, D. Goldfarb, J. Darbon, Bregman iterative algorithms for ℓ1-minimization with applications
to compressed sensing, SIAM J. Imag. Sci. 1 (1) (2008) 143–168.
[127] D.L. Donoho, J. Tanner, Neiborliness of randomly-projected simplicies in high dimensions, in: Proceedings
on National Academy of Sciences, 2005, pp. 9446–9451.
[128] D.L. Donoho, J.T. Counting the faces of randomly projected hypercubes and orthants, with applications,
Discrete Comput. Geom. 43(3) (2010) 522–541.
[129] R.G. Baraniuk, V. Cevher, M.F. Duarte, C. Hegde, Model-based compressive sensing, IEEE Trans. Inform.
Theory 56 (4) (2010) 1982–2001.

References
1373
[130] F. Parvaresh, H. Vikalo, S. Misra, B. Hassibi, Recovering sparse signals using sparse measurement matrices
in compressed DNA microarrays, IEEE J. Sel. Top. Signal Process. 2 (3) (2008) 275–285.
[131] P.J. Garrigues, B. Olshausen, Learning horizontal connections in a sparse coding model of natural images,
in: Advances in Neural Information Processing Systems (NIPS), 2008.
[132] S. Bakin, Adaptive regression and model selection in data mining problems, PhD Thesis, Australian National
University, 1999.
[133] J. Friedman, T. Hastie, R. Tibshirani, A note on the group LASSO and a sparse group LASSO, 2010.
[134] G. Obozinski, B. Taskar, M. Jordan, Multi-task feature selection, Department of Statistics, Technical Report,
University of California, Berkeley, 2006.
[135] M. Yuan, Y. Lin, Model selection and estimation in regression with grouped variables, J. Roy. Statist. Soc.
68 (1) (2007) 49–67.
[136] Y.C. Eldar, P. Kuppinger, H. Bolcskei, Block-sparse signals: uncertainty relations and efﬁcient recovery,
IEEE Trans. Signal Process. 58 (6) (2010) 3042–3054.
[137] T. Blumensath, M.E. Davies, Sampling theorems for signals from the union of ﬁnite-dimensional linear
subspaces, IEEE Trans. Inform. Theory 55 (4) (2009) 1872–1882.
[138] V. Cevher, P. Indyk, C. Hegde, R.G. Baraniuk, Recovery of Clustered Sparse Signals from Compressive
Measurements, in: International Conference on Sampling Theory and Applications (SAMPTA), Marseille,
France, 2009.
[139] V. Cevher, P. Indyk, L. Carin, R.G. Baraniuk, Sparse signal recovery and acquisition with graphical models,
IEEE Signal Process. Mag. 27 (6) (2010) 92–103.
[140] P. Sprechmann, I. Ramirez, G. Sapiro, Y.C. Eldar, CHiLasso: a collaborative hierarchical sparse modeling
framework, IEEE Trans. Signal Process. 59 (9) (2011) 4183–4198.
[141] E.J. Candès, M.B. Wakin, S.P. Boyd, Enhancing sparsity by reweighted ℓ1 minimization, J. Fourier Anal.
Appl. 14 (5) (2008) 877–905.
[142] E.J. Candès, T. Tao, The Dantzig selector: statistical estimation when p is much larger than n, Ann. Statist.
35 (6) (2007) 2313–2351.
[143] M.S. Asif, J. Romberg, On the LASSO and Dantzig selector equivalence, in: Conference on Information
Sciences and Systems (CISS), Princeton, NJ, March 2010.
[144] L.I. Rudin, S. Osher, E. Fatemi, Nonlinear total variation based noise removal algorithms, Phys. Nonlinear
Phenom. 60 (1–4) (1992) 259–268.
[145] T. Goldstein, S. Osher, The split Bregman algorithm for ℓ1 regularized problems, SIAM J. Imag. Sci. 2 (2)
(2009) 323–343.
[146] J. Yang, Y. Zhang, W. Yin, A fast alternating direction method for TV ℓ1 - ℓ2 signal reconstruction from
partial Fourier data, IEEE Trans. Sel. Top. Signal Process. 4 (2) (2010) 288–297.
[147] D. Needell, R. Ward, Stable image reconstruction using total variation minimization, 2012.
[148] J. Langford, L. Li, T. Zhang, Sparse online learning via truncated gradient, J. Mach. Learn. Res. 10 (2009)
777–801.
[149] S. Theodoridis, K. Slavakis, I. Yamada, Adaptive Learning in a World of Projections, IEEE Signal Process.
Mag. 28 (1) (2011) 97–123.
[150] S. Kay, Statistical Signal Processing, Prentice Hall, 1993.
[151] D. Donoho, I. Johnstone, G. Kerkyacharian, D. Picard, Wavelet shrinkage: asymptopia? J. Roy. Statist. Soc.
Ser. B 57 (1995) 301–337.
[152] K. Knight, W. Fu, Asymptotics for the LASSO-type estimators, Ann. Statist. 28 (5) (2000) 1356–1378.
[153] J. Fan, R. Li, Variable Selection via nonconcave penalized likelihood and its oracle properties, J. Am. Statist.
Assoc. 96 (456) (2001) 1348–1360.
[154] H. Zou, The adaptive LASSO and its oracle properties, J. Am. Statist. Assoc. 101 (2006) 1418–1429.

1374
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
[155] H. Zou, R. Li, One-step sparse estimates in nonconcave penalized likelihood models, Ann. Statist. 36 (4)
(2008) 1509–1533.
[156] D. Angelosante, J.A. Bazerque, G.B. Giannakis, Online adaptive estimation of sparse signals: where RLS
meets the ℓ1-norm, IEEE Trans. Signal Process. 58 (7) (2010) 3436–3447.
[157] G. Mileounis, B. Babadi, N. Kalouptsidis, V. Tarokh, An adaptive greedy algorithm with application to
nonlinear communications, IEEE Trans Signal Process. 58 (6) (2010) 2998–3007.
[158] N.Ogura,I.Yamada,Non-strictlyconvexminimizationovertheﬁxedpointsetoftheasymptoticallyshrinking
nonexpansive mapping, Numer. Func. Anal. Optim. 23 (2002) 113–137.
[159] K. Slavakis, I. Yamada, N. Ogura, The adaptive projected subgradient method over the ﬁxed point set of
strongly attracting nonexpansive mappings, Numer. Func. Anal. Optim. 27 (7 and 8) (2006) 905–930.
[160] I. Yamada, N. Ogura, Hybrid steepest descent method for variational inequality problem over the ﬁxed point
set of certain quasi-nonexpansive mappings, Numer. Func. Anal. Optim. 25 (7 and 8) (2004) 619–655.
[161] J. Duchi, S.S. Shwartz, Y. Singer, T. Chandra, Efﬁcient projections onto the ℓ1-ball for learning in high
dimensions, in: Proceedings of the International Conference on Machine Leaning (ICML), 2008, pp.
272–279.
[162] Y. Kopsinis, K. Slavakis, S. Theodoridis, Online sparse system identiﬁcation and signal reconstruction using
projections onto weighted ℓ1 balls, IEEE Trans. Signal Process. 59 (3) (2011) 936–952.
[163] Y. Kopsinis, K. Slavakis, S. Theodoridis, S. McLaughlin, Reduced complexity online sparse signal recon-
struction using projections onto weighted ℓ1 balls, in: 17th International Conference on Digital Signal
Processing (DSP 2011), July 2011, pp. 1–8.
[164] Y. Kopsinis, K. Slavakis, S. Theodoridis, S. McLaughlin, Generalized thresholding sparsity-aware algorithm
for low complexity online learning, in: Proceedings of the IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP), Kyoto, Japan, March 2012, pp. 3277–3280.
[165] Y. Kopsinis, K. Slavakis, S. Theodoridis, S. McLaughlin, in: ISCASS 2013, Submitted for publication.
[166] M. Elad, P. Milanfar, R. Rubinstein, Analysis versus synthesis in signal priors, Inv. Prob. 23 (2007) 947–968.
[167] J.L. Starck, E.J. Cès, D.L. Donoho, The curvelet transform for image denoising, IEEE Trans. Image Pocess.
11 (6) (2002) 670–684.
[168] J.L. Starck, J. Fadili, F. Murtagh, The undecimated wavelet decomposition and its reconstruction, IEEE
Trans. Signal Process. 16 (2) (2007) 297–309.
[169] D. Han, D.R. Larson, Frames, Bases and Group Representations, American Mathematical Society, Provi-
dence, RI, 2000.
[170] R.J. Dufﬁn, A.C. Schaeffer, A class of nonharmonic Fourier series, Trans. Am. Math. Soc. 72 (1952) 341–366.
[171] I. Daubechies, A. Grossman, Y. Meyer, Painless nonorthogonal expansions, J. Math. Phys. 27 (1986)
1271–1283.
[172] J. Kovacevic, A. Chebira, Life beyond bases: the advent of frames, IEEE Signal Process. Mag. 24 (4) (2007)
86–104.
[173] E.J.Candès,Y.C.Eldar,D.Needell,P.Randall,Compressedsensingwithcoherentandredundantdictionaries,
Appl. Comput. Harmon. Anal. 31 (1) (2011) 59–73.
[174] S. Nam, M.E. Davies, M. Elad, R. Gribonval, The cosparse analysis model and algorithms, Appl. Comput.
Harmon. Anal. (in press).
[175] Y.M. Lu, M.N. Do, A theory for sampling signals from a union of subspaces, IEEE Trans. Signal Process.
56 (6) (2008) 2334–2345.
[176] J.F. Cai, S. Osher, Z. Shen, Split Bregman methods and frame based image restoration, Multiscale Model.
Simul. 8 (2) (2009) 337–369.
[177] M. Elad, J.L. Starck, P. Querre, D.L. Donoho, Simultaneous cartoon and texture image inpainting using
morphological component analysis (MCA), Appl. Comput. Harmon. Anal. 19 (2005) 340–358.

References
1375
[178] I.W. Selesnick, M.A.T. Figueiredo, Signal restoration with overcomplete wavelet transforms: comparison of
analysis and synthesis priors, in: Proceedings of SPIE, 2009.
[179] B. Boashash, Time Frequency Analysis, Elsevier, 2003.
[180] P. Flandrin Time-Frequency/Time-Scale Analysis, Academic Press, 1999.
[181] S.A. Mallat, Wavelet Tour of Signal Processing: The Sparse Way, third ed., Academic Press, 2008.
[182] T. Strohmer, Gabor Analysis and Algorithms: Theory and Applications, Birkhauser, Boston, MA, 1998,
pp. 267–294 (Chapter Numerical algorithms for discrete Gabor expansions).
[183] M. Zibulevsky, Y.Y. Zeevi, Frame analysis of the discrete Gabor scheme, IEEE Trans. Signal Process. 42 (4)
(1994) 942–945.
[184] Y. Kopsinis, E. Aboutanios, D.E. Waters, S. McLaughlin, Time-frequency and advanced frequency estimation
techniques for the investigation of bat echolocation calls, J. Acoust. Soc. Am. 127 (2) (2010) 1124–1134.
[185] E. Candès, B. Recht, Exact matrix completion via convex optimization, Found. Comput. Math. 9 (6) (2009)
717–772.
[186] E. Candès, T. Tao, The power of convex relaxation: near-optimal matrix completion, IEEE Trans. Inform.
Theory 56 (5) (2010) 2053–2080.
[187] R. Motwani, P. Raghavan, Randomized Algorithms, Cambridge University Press, 1995.
[188] B. Recht, A simpler approach to matrix completion, J. Mach. Learn. Res. 12 (2011) 3413–3430.
[189] M. Fazel, H. Hindi, S.P. Boyd, A rank minimization heuristic with application to minimum order system
approximation, in: Proceedings of the American Control Conference 2001, 2001, pp. 4734–4739.
[190] D. Gross, Recovering low-rank matrices from few coefﬁcients in any basis, IEEE Trans. Inform. Theory 57
(3) (2011) 1548–1566.
[191] E.J. Candès, Y. Plan, Matrix completion with noise, Proc. IEEE 98 (6) (2010) 925–936.
[192] S. Gandy, B. Recht, I. Yamada, Tensor completion and low-n-rank tensor recovery via convex optimization,
Inv. Prob. 27 (2) (2011).
[193] M. Signoretto, R. Van de Plas, B. De Moor, J. Suykens, Tensor versus matrix completion: a comparison with
application to spectral data, IEEE Signal Process. Lett. 18 (7) (2011) 403–406.
[194] E.J. Candès, X. Li, Y. Ma, J. Wright, Robust principal component analysis? J. ACM, 58 (3) (2011).
[195] V. Chandrasekaran, S. Sanghavi, P.A. Parrilo, A.S. Willsky, Rank-sparsity incoherence for matrix decompo-
sition, SIAM J. Optim. 21 (2) (2011) 572–596.
[196] J. Wright, Y. Peng, Y. Ma, A. Ganesh, S. Rao, Robust principal component analysis: exact recovery of
corrupted low-rank matrices by convex optimization, in: Neural Information Processing Systems (NIPS),
2009.
[197] A. Ganesh, J. Wright, X. Li, E.J. Candès, Y. Ma, Dense error correction for low-rank matrices via principal
component pursuit, in: IEEE International Symposium on Information Theory Proceedings (ISIT 2010) ,
2010, pp. 1513–1517.
[198] M. Hubert, S. Engelen, Robust PCA and classiﬁcation in biosciences, Bioinformatics 20 (11) (2004)
1728–1736.
[199] M. Hubert, P.J. Rousseeuw, K. Vanden Branden, ROBPCA: a new approach to robust principal component
analysis, Technometrics 47 (1) (2005) 64–79.
[200] J. Karhunen, J. Joutsensalo, Generalizations of principal component analysis, optimization problems, and
neural networks, Neural Netw. 8 (4) (1995) 549–562.
[201] F. De la Torre, M.J. Black, A framework for robust subspace learning, Int. J. Comput. Vis. 54 (1) (2003)
117–142.
[202] L. Xu, A. Yuille, Robust principal component analysis by self-organizing rules based on statistical physics
approach, IEEE Trans. Neural Netw. 6 (1) (1995) 131–143.
[203] M. Fazel, H. Hindi, S. Boyd, Rank minimization and applications in system theory, in: Proceedings of the
American Control Conference 2004, 2004, pp. 3273–3278.

1376
CHAPTER 23 Sparsity-Aware Learning and Compressed Sensing
[204] R. Foygel, N. Srebro, Concentration-based guarantees for low-rank matrix reconstruction, in: 24th Annual
Conference on Learning Theory (COLT), 2011.
[205] M. Mesbahi, G. Papavassilopoulos, On the rank minimization problem over a positive semideﬁnite linear
matrix inequality, IEEE Trans. Automat. Control 42 (2) (1997) 239–243.
[206] J.-F. Cai, E.J. Candès, Z. Shen, A Singular Value Thresholding Algorithm for Matrix Completion, SIAM
J. Optim. 20 (4) (2010) 1956.
[207] C. Chen, B. He, X. Yuan, Matrix completion via an alternating direction method, IMA J. Numer. Anal.
(2011).
[208] A. Ganesh, Z. Lin, J. Wright, L. Wu, M. Chen, Y. Ma, Fast algorithms for recovering a corrupted low-
rank matrix, in: 3rd IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive
Processing (CAMSAP 2009), December 2009, pp. 213–216.
[209] Z. Lin, M. Chen, Y. Ma, The Augmented Lagrange Multiplier Method for Exact Recovery of Corrupted
Low-Rank Matrices, September 2010. arXiv:1009.5055.
[210] K.C. Toh, S. Yun, An accelerated proximal gradient algorithm for nuclear norm regularized linear least
squares problems, Pac. J. Optim. 6 (2010) 615–640.
[211] X.M. Yuan, J.F. Yang, Sparse and low-rank matrix decomposition via alternating direction methods, Pac.
J. Optim. (2009).
[212] K. Lee, Y. Bresler, ADMiRA: atomic decomposition for minimum rank approximation, IEEE Trans. Inform.
Theory 56 (9) (2010) 4402–4416.
[213] A.E. Waters, A.C. Sankaranarayanan, R.G. Baraniuk, SpaRCS: recovering low-rank and sparse matrices from
compressive measurements, in: Procedings of Advances in Neural Processing Systems (NIPS), Granada,
Spain, 2011.
[214] M. McCoy, A.J. Tropp, Two proposals for robust PCA using semideﬁnite programming, Electron. J. Statist.
5 (2011) 1123–1160, Mathematical Reviews number (MathSciNet): MR2836771.
[215] H. Xu, C. Caramanis, S. Sanghavi, Robust PCA via outlier pursuit, IEEE Trans. Inform. Theory 58 (5) (2012)
3047–3064.
[216] J. Wright, A. Ganesh, K. Min, Y. Ma, Compressive Principal Component Pursuit, February 2012.
arXiv:1202.4596.
[217] Z. Zhou, X. Li, J. Wright, E. Candès, Y. Ma, Stable principal component pursuit, in: IEEE International
Symposium on Information Theory Proceedings (ISIT 2010), 2010, pp. 1518–1522.
[218] X. Su, T.M. Khoshgoftaar, A survey of collaborative ﬁltering techniques, Adv. Artif. Intell. 2009 (2009)
1–19.
[219] G. Mao, B. Fidan, B.D. Anderson, Wireless sensor network localization techniques, Comput. Netw. 51 (10)
(2007) 2529–2553.
[220] P. Biswas, T.-c. Liang, T.-c. Wang, Y. Ye, Semideﬁnite programming based algorithms for sensor network
localization, ACM Trans. Sens. Netw. 2 (2006) 2006.
[221] A. Montanari, S. Oh, On positioning via distributed matrix completion, in: IEEE Sensor Array and Multi-
channel Signal Processing Workshop (SAM 2010), October 2010, pp. 197–200.
[222] Z. Liu, L. Vandenberghe, Interior-point method for nuclear norm approximation with application to system
identiﬁcation, SIAM J. Matrix Anal. Appl. 31 (3) (2010) 1235–1256.
[223] P. Chen, D. Suter, Recovering the missing components in a large noisy low-rank matrix: application to SFM,
IEEE Trans. Pattern Anal. Mach. Intell. 26 (8) (2004) 1051–1063.
[224] A. Argyriou, T. Evgeniou, M. Pontil, Multi-task feature learning, in: Advances in Neural Information Pro-
cessing Systems 19, MIT Press, 2007.
[225] H.H. Bauschke, P.L. Combettes, Convex Analysis and Monotone Operator Theory in Hilbert Spaces,
Springer, 2011.

References
1377
[226] J. Hiriart-Urruty, C. Lemaréchal, Fundamentals of Convex Analysis, Springer-Verlag, Grundlehren Text
Editions Series, 2001.
[227] R.T. Rockafellar, R.J.-B. Wets, Variational Analysis, Springer, Berlin, 2004.
[228] T.R. Rockafellar, Convex Analysis, Princeton University Press, Princeton, NJ, 1970.

24
CHAPTER
Information Based Learning
José C. Príncipe*, Badong Chen† and Luis G.Sanchez Giraldo*
*Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA
†Institute of Artiﬁcial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, P.R. China
Traditional methods in machine learning and adaptive signal processing mainly exploit second order
signal statistics (covariance, L2 distance, correlation functions, etc.). The emphasis on second order
statistics as the choice of optimality criterion is due to the computational simplicity and the optimal-
ity under linear and Gaussian assumptions. Although second order statistics are still prevalent today
in machine learning community and provide successful engineering solutions to most practical prob-
lems, it has become evident that this approach can be improved, especially when data possess non-
Gaussian distributions (either fat tails or ﬁnite range). Recent studies suggest that machine learning
problems that cannot be successfully solved using second order statistics beneﬁt greatly from the use
of the information theoretic performance measures. This information based learning has been named
information theoretic learning (ITL), and it utilizes information concepts or descriptors from informa-
tion theory to adapt the learning machines (linear or nonlinear) in both supervised and unsupervised
paradigms. In this chapter, we present a brief but comprehensive introduction on general ITL methods
to implement learning algorithms with information theoretic criteria. Some basic information theoretic
descriptors (or information measures) are introduced, and a unifying learning framework based on these
descriptors is presented. We also provide nonparametric kernel estimators for information measures and
express them in reproducing kernel Hilbert space (RKHS). The information particle interaction model
for learning is also presented. Finally, several application examples are provided, including TDNN
training, classiﬁcation, clustering, image retrieval, and independent component analysis (ICA).
1.24.1 Introduction
The concept of learning was ﬁrst applied to biological organisms and describes the process of revising
behavior or understanding through acquisition of new experience from the environment. In the areas
of machine learning, learning means to build a model from data with the goal of extracting useful
structure contained in the data. The learning process is in essence a procedure of information process-
ing, decreasing data redundancy in the presence of uncertainty and encoding the data into a model.
Therefore, learning theory is intrinsically related to information theory, which was ﬁrst conceptualized
by Claude Shannon in the mathematical design of optimal communication systems [1,2]. A general
statistical description of learning processes was given in [3], where learning is deﬁned as a process in
which the system’s subjective entropy or, equivalently, its missing information decreases in time. In [4],
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00024-3
© 2014 Elsevier Ltd. All rights reserved.
1379

1380
CHAPTER 24 Information Based Learning
the mathematical concept of information was brought to biologically plausible information processing.
The principle of maximum information preservation (Infomax) was applied in self-organization [5].
Further, a unifying framework for learning based on information theoretic criteria has been presented
in [6,7].
To extract information from data, we have to build a model, which basically summarizes the process
of data generation. In the framework of statistical machine learning, the learning machine goal can be
thought of as approximating the a posteriori distribution of the targets given a set of examples (training
data). If the joint or conditional probability density functions (PDFs) of the data source can be obtained,
the data modeling can be directly implemented, by Bayesian reasoning. Examples of this approach
includeBayesianﬁltering,hiddenMarkovmodeling,andsoon.Anotherapproachforstatisticallearning,
which is much simpler computationally, deals with the construction of scalar descriptors of the PDFs,
the so called Bayes risks [8]. Theses descriptors, combined with a parametric data model, can be used
to measure the similarity or dependency between the model output and the data. They are often used as
cost functions for optimizing the learning systems’ parameters. Figure 24.1 shows a general scheme of
supervised machine learning. In this ﬁgure the cost criterion is, in general, an evaluator of the error’s PDF.
The goal of the learning is to search the parameter ω of the model (learning machine) f (x, ω), such that
the discrepancy (measured by the cost criterion) between the model output y and the desired (teacher)
signal d is minimized. In this scenario, the process of ﬁnding parameters from the data is called learning.
The most widely used descriptors of the PDFs are the statistical moments. The combination of
the second order moments (variance, correlation, etc.) and the Gaussian assumption in general leads
to mathematically convenient and analytically tractable optimal solutions. Familiar examples are the
mean square error (MSE) in least-squares linear regression and output variance in principal components
analysis (PCA). The second order statistics as a cost criterion, however, may be a poor descriptor of opti-
mality especially in nonlinear and non-Gaussian (e.g., heavy-tail or ﬁnite range distributions) situations.
Thus selecting a new criterion beyond second order statistics is attractive in both signal processing and
machine learning domains. Recent studies indicate that the information theoretic descriptors of entropy
and dissimilarity (divergence and mutual information) do not suffer from the limitation of Gaussian
assumption, and can improve performance in many realistic scenarios. Combined with nonparametric
estimators of entropy and divergence, one can easily develop a variety of new learning algorithms.
Learning Machine
f(x, w)
Cost
Criterion
Σ
Data x
Output 
y
Data d
Error 
e=d-y
+
–
Search Algorithm
FIGURE 24.1
Model building through supervised machine learning.

1.24.2 Information Theoretic Descriptors
1381
In this chapter, we will provide a brief but comprehensive picture of information theoretic learning
(ITL), which describes machine learning under a general framework of information theory, and designs
new learning costs based on information theoretic descriptors like entropy and mutual information.
1.24.2 Information theoretic descriptors
Let’s start with a brief introduction of some basic information theoretic descriptors. The concept of
information is so rich that a single information measure (or information descriptors) may never be
able to quantify information properly. Here we focus mainly on Claude Shannon and Alfred Renyi’s
deﬁnitions of entropy, divergence and mutual information (a special case of divergence).
1.24.2.1 Entropy
Given a discrete random variable X with probability mass function P{X = xk} = pk, k = 1, . . . , n,
Shannon’s entropy is deﬁned by [1, 2]
HS(X) =
n

k=1
pk I(pk),
(24.1)
where I(pk) = −log pk is Hartley’s amount of information associated with the discrete event xk with
probability pk. This information measure was originally devised by Claude Shannon in 1948 to study the
amount of information in a transmitted message. Since learning systems deal, in general, with continuous
random variables, we are more interested in information measure of a continuous random variable. Given
a continuous random variable X with PDF p(x), x ∈C, Shannon’s differential entropy is deﬁned as
HS(X) = −

C
p(x) log p(x)dx.
(24.2)
Shannon entropy measures the average information (or uncertainty) contained in the probability dis-
tribution, and can also be used to measure many other concepts such as diversity, similarity, disorder,
randomness, and so on.
A well-known generalization of Shannon entropy is Renyi entropy named after Alfred Renyi [9].
For a continuous random variable X, order- α Renyi entropy is deﬁned by (α > 0, α ̸= 1)
Hα(X) =
1
1 −α log Vα(X),
(24.3)
where Vα(X) is the order- α information potential (IP) [7,10,11], given by
Vα(X) =

C
pα(x)dx = E[pα−1(X)].
(24.4)
When α = 2, we have H2(X) = −log V2(X). By L’Hoptital’s rule one can easily show that as α →1,
Renyi entropy converges to Shannon entropy. The information potential Vα(X) can be interpreted as
the α -power of the α -norm in the PDF space. It was proposed as an optimality criterion for learning

1382
CHAPTER 24 Information Based Learning
systems (adaptive ﬁlters, neural networks, etc.) in [11]. The main reasons are: (1) due to the monotonic
property of the logarithm function, optimizing the information potential will be equivalent to optimizing
Renyi entropy; (2) the information potential, especially the quadratic information potential (α = 2), can
easily be estimated from sample data by kernel density estimation method (also referred to as Parzen
windowing approach [12,13]); (3) in general the learning seeks extrema (either minimum or maximum)
of the cost function, independently to its actual value, so the dependence on estimation error is decreased.
Thereisanimportantoptimizationprincipleofentropy,i.e.,themaximumentropy(MaxEnt)principle
enunciated by Jaynes [14,15]. According to MaxEnt, among all the distributions that satisfy certain con-
straints, one should choose the distribution that maximizes the entropy. MaxEnt is a powerful and widely
accepted method for statistical inference with incomplete knowledge of the probability distribution.
1.24.2.2 Divergence
Information divergence measures the discrepancy between two distributions. The Kullback-Leibler
(KL) divergence (or Shannon’s relative entropy) between two PDFs p1(x) and p2(x) is
DKL(p1∥p2) =

C
p1(x) log p1(x)
p2(x)dx.
(24.5)
In statistics, the KL divergence DKL(p1∥p2) is a measure of the equivocation of assuming that the
distribution is p2 when the true distribution is p1. One can easily show DKL(p1∥p2) ≥0, with equality
if and only if p1(x) and p2(x) are identical almost everywhere. Note that the KL divergence is a measure
of the “distance” between the two PDFs, but in general we have DKL(p1∥p2) ̸= DKL(p2∥p1), so it
does not obey all the properties of a distance.
Renyi’s order- α divergence between p1(x) and p2(x) is deﬁned by [16]
Dα(p1∥p2) =
1
α −1 log

C
p1(x)
 p1(x)
p2(x)
α−1
dx.
(24.6)
When α →1, we have lim
α→1 Dα(p1∥p2) = DKL(p1∥p2). In [17], a new Renyi’s divergence, called the
relative α -Renyi entropy between p1(x) and p2(x) is deﬁned as
DRα(p1∥p2) = log

pα−1
2
(x)p1(x)dx

1
(1−α) 	
pα
2 (x)dx

 1
α
	
pα
1 (x)dx

1
α(1−α)
.
(24.7)
Note that this deﬁnition is more robust than Renyi’s original deﬁnition in (24.6) since the denominator
in the argument of the log contains an integral.
In machine learning, to simplify the computation of the above divergences, some quadratic diver-
gences are frequently used. These quadratic divergences involve only a simple quadratic form of PDFs.
The Euclidean distance (ED) in probability spaces and Cauchy-Schwarz (CS) divergence are popular,

1.24.2 Information Theoretic Descriptors
1383
which are deﬁned respectively as [7]
DED(p1∥p2) =

C
(p1(x) −p2(x))2dx,
(24.8)
DCS(p1∥p2) = −log
	
C p1(x)p2(x)dx

2

C p2
1(x)dx

C p2
2(x)dx .
(24.9)
It is worth noting that DED can be expressed in terms of quadratic information potential (QIP):
DED(p1∥p2) = V2(p1) + V2(p2) −2V2(p1; p2)
(24.10)
where V2(p1; p2) ≜

p1(x)p2(x)dx is the cross information potential (CIP). Further, DCS can be
rewritten in terms of Renyi’s quadratic entropy:
DCS(p1∥p2) = 2H2(p1; p2) −H2(p1) −H2(p2),
(24.11)
where H2(p1; p2) ≜−log

p1(x)p2(x)dx is Renyi’s quadratic cross-entropy.
1.24.2.3 Mutual information
Mutual information measures the degree of dependence between two random variables, and it is actually
a special case of divergence. Let X1 and X2 be two random variables with joint PDF pX1X2(x1, x2) and
marginal PDFs pX1(x1) and pX2(x2), the mutual information between X1 and X2 is deﬁned as the KL
divergence between pX1X2(x1, x2) and pX1(x1)pX2(x2), that is
I(X1, X2) = DKL(pX1X2∥pX1 pX2)
=

C pX1X2(x1, x2) log
pX1 X2(x1,x2)
pX1(x1)pX2(x2)dx1 dx2.
(24.12)
Clearly, mutual information I(X1, X2) is symmetric and always greater than zero, which vanishes if
and only if X1 and X2 are statistically independent.
Mutual information (24.12) can alternatively be expressed as
I(X1, X2) = HS(X1) −HS(X1|X2),
(24.13)
where HS(X1|X2) is the conditional Shannon entropy of X1 given X2, deﬁned as
HS(X1|X2) = −

C
pX1X2(x1, x2) log pX1|X2(x1|x2)dx1 dx2,
(24.14)
where pX1|X2(x1|x2) denotes the conditional PDF of X1 given X2. As one can see from (24.13), mutual
information quantiﬁes the reduction of uncertainty in X1 after observing X2.
In a similar way one can also deﬁne Renyi’s order- α mutual information Iα(X1, X2), Euclidean
distance mutual information IED(X1, X2), and Cauchy-Schwartz mutual information ICS(X1, X2),
that is
⎧
⎨
⎩
Iα(X1, X2) = Dα(pX1X2∥pX1 pX2),
IED(X1, X2) = DED(pX1X2∥pX1 pX2),
ICS(X1, X2) = DCS(pX1X2∥pX1 pX2).
(24.15)

1384
CHAPTER 24 Information Based Learning
The previous information measures are all deﬁned based on PDFs (for continuous random variables).
Some researchers also propose to deﬁne information measures using distribution functions or survival
functions. For example, the cumulative residual entropy (CRE) of random variable X is deﬁned by [18]
E(X) = −

R+
¯F|X|(x) log ¯F|X|(x)dx,
(24.16)
where ¯F|X|(x) = P(|X| > x) is the survival function of |X|. In a recent paper [19], we deﬁne the order
α survival information potential (SIP) as (α > 0)
Sα(X) =

R+
¯Fα
|X|(x)dx.
(24.17)
The survival information potential (24.16) is just deﬁned by replacing the PDF with the survival func-
tion (of an absolute value transformation of X) in the original information potential (24.4). This new
deﬁnition seems more natural and reasonable, because the survival function is more regular and general
than the PDF.
1.24.3 Unifying information theoretic framework for machine learning
Information theoretic methodology provides a unifying framework for both supervised and unsupervised
machine learning [7]. Figure 24.2 shows a unifying view of learning based on information theoretic
cost criteria. In Figure 24.2, the cost C(Y, D) denotes generally the information measure (divergence
or mutual information) between Y and D, where Y is the output signal of the model (learning machine),
and D depends on which position the switch is in. Information theoretic learning (ITL) is then to adjust
the parameters ω such that the cost C(Y, D) is optimized (minimized or maximized).
1.24.3.1 Switch in position 1
When the switch is in position 1, the cost includes the model output Y and an external desired
signal Z. Then the learning is supervised, and the goal is to make the output signal and the desired signal
1
2
3
X
Input Signal
Z
Desired Signal
Y
Output Signal
(X,   )
Y=f
Learning Machine
Information Measure
( ,
)
C Y D
ω
FIGURE 24.2
Information theoretic learning framework.

1.24.3 Unifying Information Theoretic Framework for Machine Learning
1385
as “close” as possible. In this case the learning can be categorized into two categories: (1) ﬁltering (or
regression) and classiﬁcation; (2) feature extraction.
1.24.3.1.1
Filtering and classiﬁcation
In tradition ﬁltering and classiﬁcation, the cost function is in general the mean square error (the squares
error loss) or misclassiﬁcation error rate (the 0–1 loss). In ITL framework, the problem can be formulated
as minimizing the divergence or maximizing the mutual information between output Y and the desired
response Z, that is
min
ω DKL(Y∥Z) =

pY (x) log pY (x)
pZ(x)dx,
(24.18)
max
ω
I(Y, Z) = max
ω {HS(Y) −HS(Y|Z)}.
(24.19)
The above optimization criteria are based purely on PDFs of the output and the desired signal;
therefore we don’t require the same number of desired and output data to calculate the errors.
For ﬁltering and classiﬁcation problems, another important ITL criterion is the Minimum Error
Entropy (MEE) criterion [20–23], which aims at minimizing the entropy of the error between the output
and the desired responses:
min
ω HS(e) = min
ω

−

p(e) log p(e)de

,
(24.20)
where e = Z −Y is the error. As entropy measures the average uncertainty or dispersion of a random
variable, its minimization makes the error concentrated. Different from conventional Bayesian risks, like
MSE, the “loss function” in MEE is −log p(.), which is directly related to the error’s PDF, transforming
nonlinearly the error by its own PDF. If using Renyi’s entropy as the cost, the optimization problem in
(24.20) becomes
min
ω Hα(e) = min
ω

1
1 −α log Vα(e)

⇔
min
ω Vα(e) if α < 1,
max
ω
Vα(e) if α > 1,
(24.21)
where Vα(e) denotes error’s information potential, Vα(e) =

pα(e)de.
1.24.3.1.2
Feature extraction
In machine learning, when the input data are too large and the dimensionality is very high, it is nec-
essary to transform nonlinearly the input data into a reduced representation set of features. Feature
extraction (or feature selection) involves reducing the amount of resources required to describe a large
set of data accurately. The feature set will extract the relevant information from the input in order to
perform the desired task using the reduced representation instead of the full size input. Suppose the
desired signal is the class label, then an intuitive cost for feature extraction should be some measure of
“relevance” between the projection outputs (features) and the labels. In ITL, this problem can be solved

1386
CHAPTER 24 Information Based Learning
by maximizing the mutual information between the output Y and the label C:
max
ω
I(Y, C) = max
ω {HS(Y) −HS(Y|C)}.
(24.22)
There are many other mutual information based criteria for feature extraction. A summary of these
criteria can be found in [24], where a unifying framework for information theoretic feature selection
has been presented. The use of mutual information criterion for feature extraction can be justiﬁed using
Fano’s inequality, according to which one should maximize the mutual information to improve the lower
bound on the achievable probability of error [25].
1.24.3.2 Switch in position 2
When the switch is in position 2, the learning is in essence unsupervised because there is no external
signal besides the input and output signals. In this situation, the well-known optimization principle is the
Maximum Information Transfer, which aims to maximize the mutual information between the original
input data and the output of the system:
max
ω
I(Y, X) = max
ω

pXY (y, x) log
pXY (y, x)
pY (y)pX(x)dy dx.
(24.23)
This principle is also known as the principle of maximum information preservation (Infomax) described
by Linsker, which is a widely accepted optimization principle for neural networks, self-organization,
and other information processing systems [5]. Infomax is also related to an algorithm for independent
component analysis (ICA) described by Bell and Sejnowski in 1995 [26].
Another information optimization principle for unsupervised learning (clustering, principal curves,
vector quantization, etc.) is the Principle of Relevant Information (PRI), which can be formulated
as [27–29]
min
ω {HS(Y) + λDKL(Y∥X)} ,
(24.24)
where X denotes the original data set, Y is the compressed version of the original data, and λ is the
trade-off parameter. The basic idea of PRI is to minimize the data redundancy (entropy term) while
preserving the similarity to the original data (divergence term). Using Renyi’s deﬁnition of entropy and
divergence, (24.24) can be reformulated as
min
ω {Hα(Y) + λDα(Y∥X)} .
(24.25)
To simplify the computation, one often uses Renyi’s quadratic entropy to measure the redundancy, and
Cauchy-Schwarz divergence to measure the distortion, which yields
min
ω {H2(Y) + λDCS(Y∥X)} .
(24.26)
1.24.3.3 Switch in position 3
When the switch is in position 3, the only source of data is the model output, which in this case is
in general assumed multidimensional. Typical examples of this case include independent component
analysis (ICA), clustering, output entropy optimization, and so on:

1.24.3 Unifying Information Theoretic Framework for Machine Learning
1387
Independent component analysis: ICA is an unsupervised technique aiming to reduce the redundancy
between components of the system output [30]. Given a nonlinear multiple-input-multiple-output
(MIMO) system y = f (x, ω), the nonlinear ICA usually optimizes the parameter vector ω such that
the mutual information between the components of y is minimized:
min
ω I(y) =

p(y1 · · · ym) log p
	
y1 · · · ym

m
d=1 p(yd) dy.
(24.27)
Clustering: Clustering (or clustering analysis) is a common technique for statistical data analysis used
in machine learning, pattern recognition, bioinformatics, etc. The goal of clustering is to divide the input
data into groups (called clusters) so that the objects in the same cluster are more “similar” to each other
than to those in other clusters, and different clusters are deﬁned as compactly and distinctly as possible.
Information theoretic measures like entropy and divergence are frequently used as an optimization
criterion for clustering. For example, in the case of two clusters, one can use the symmetrized KL
divergence as the cost:
max
ω
DKL(p1(y)∥p2(y)) + DKL(p2(y)∥p1(y)).
(24.28)
Output entropy optimization: If the switch is in position 3, one can also optimize (minimize or maximize)
the entropy at system output (usually subject to some constraint on the weight norm or nonlinear
topology) so as to capture the underlying structure in high dimensional data:

min
ω or max
ω
H(y) = −

p(y) log p(y)d y,
s.t.
E[hi(y)] = αi,
i = 1, . . . , d.
(24.29)
This optimization formulation is useful in blind equalization, nonlinear PCA, ICA and novelty ﬁltering
[30,31].
1.24.3.4 Switch simultaneously in position 1 and 2
In Figure 24.2 the switch can be simultaneously in position 1 and 2. In this case, the cost has access to
input data X, output data Y, and the desired or reference data Z. A well-known example is the Information
Bottleneck (IB) method, introduced by Naftali Tishby et al. [32,33]. Given a random variable X and an
observed relevant variable Z, and assuming that the joint distribution between X and Z is known, the IB
method aims to compress X and try to ﬁnd the best tradeoff between: (1) the minimization of mutual
information between X and its compressed version Y; and (2) the maximization of mutual information
between Y and the relevant variable Z. Then the IB method minimizes the following cost:
min
ω I(X, Y) −βI(Y, Z),
(24.30)
where β is the trade-off parameter. The basic idea in IB is to ﬁnd a reduced representation of X while
preserving the information of X with respect to another variable Z.

1388
CHAPTER 24 Information Based Learning
1.24.4 Nonparametric information estimators
To implement the previous ITL learning methods, one should evaluate the information theoretic costs,
such as entropy and divergence. In practice, the data distributions are usually unknown, and the analytical
evaluation of these measures is not possible. Thus we have to estimate ITL costs using sample data. There
are two simple ways: (1) one way is to estimate the underlying distributions based on available samples,
and plug the estimated distributions directly into the cost functions to obtain the information estimators
(the so called “plug-in estimators”); and (2) another way is to substitute the estimated distributions into
the sample mean approximation of the information measures (approximating the expected values by
their sample mean). In the literature there are three possible techniques for estimating the PDF of a
randomvariablebasedonitssampledata:parametric,semi-parametric,andnonparametric.Accordingly,
there are also parametric, semi-parametric, and nonparametric approaches for estimating information
theoretic quantities. We refer the reader to [34] for a review of entropy estimation methods.
In the following, we focus only on the nonparametric kernel approach (Parzen windowing approach)
that has been widely and successfully applied in ITL learning. Further, we mainly discuss entropy
estimators. The estimation of other information measures is similar.
Given a set of independent and identically distributed (i.i.d.) samples {x1, . . . , xN} drawn from p(x),
the kernel density estimation (KDE) for p(x), assuming a ﬁxed-size kernel function κσ(.) for simplicity,
is given by [12,13]
ˆp(x) = 1
N
N

i=1
κσ(x −xi).
(24.31)
In general the kernel function satisﬁes κσ(x) ≥0, and

κσ(x)dx = 1, and hence ˆp(x) is still a PDF.
Further, to make the estimator smooth, the kernel function is usually continuous and differentiable (and
preferably symmetric and unimodal). The most widely used kernel function is the Gaussian function:
κσ(x) =
1
√
2πσ
exp

−∥x∥2
2σ 2

.
(24.32)
The kernel size (or bandwidth) for the Gaussian kernel can be optimized using maximum likelihood
(ML) principle, or selected according to rules-of-thumb, such as Silverman’s rule [13].
For ﬁxed kernel size σ, we have limN→∞ˆp(x) = p(x) ∗κσ(x)(here, “∗” denotes the convolution
operator).Usingasuitableannealingrateforthekernelsize,theKDEcanbeasymptoticallyunbiasedand
consistent.Speciﬁcally,iflimN→∞σ(N) = 0,andlimN→∞Nσ(N) = ∞,thenlimN→∞ˆp(x) = p(x)
in probability [35].
Substitute (24.31) in Shannon’s entropy expression (24.2), we get the following plug-in estimator:

HS(X) = −1
N

N

i=1
κσ(x −xi) log

1
N
N

i=1
κσ(x −xi)

dx.
(24.33)

1.24.4 Nonparametric Information Estimators
1389
To avoid the evaluation of integral, one can alternatively substitute (24.31) in the sample mean estimator
of Shannon entropy and obtain

HS(X) = −1
N
N

j=1
log

1
N
N

i=1
κσ(x j −xi)

.
(24.34)
Similarly, substituting (24.31) in Renyi’s entropy expression, we have

Hα(X) =
1
1 −α log
 
1
N
N

i=1
κσ(x −xi)
α
dx.
(24.35)
Further, the sample mean estimator of Renyi’s entropy will be

Hα(X) =
1
1 −α log 1
N
N

j=1

1
N
N

i=1
κσ(x j −xi)
α−1
=
1
1 −α log 1
N α
N

j=1
 N

i=1
κσ(x j −xi)
α−1
.
(24.36)
In (24.35), if α = 2 and the kernel function is Gaussian function, we can derive

H2(X) = −log
∞

−∞

1
N
N

i=1
κσ(x −xi)
2
dx
= −log 1
N 2
∞

−∞
⎛
⎝
N

i=1
N

j=1
κσ(x −x j)κσ(x −xi)
⎞
⎠
2
dx
= −log 1
N 2
N

i=1
N

j=1
∞

−∞
κσ(x −x j)κσ(x −xi)dx
= −log
⎛
⎝1
N 2
N

i=1
N

j=1
κ√
2σ(x j −xi)
⎞
⎠.
(24.37)
This result comes from the fact that the integral of the product of two Gaussian functions can be exactly
evaluated as the value of the Gaussian function computed at the difference of the arguments and whose
variance is the sum of the variances of the two original Gaussian functions. As one can see, the plug-
in estimator (24.37) is identical to the sample mean estimator of quadratic Renyi’s entropy but with
kernel size
√
2σ instead of σ. The argument of the logarithm in (24.37) is the estimator of quadratic
information potential (QIP):
V2(X) = 1
N 2
N

i=1
N

j=1
κ√
2σ(x j −xi).
(24.38)

1390
CHAPTER 24 Information Based Learning
Base on the estimator of QIP, one can easily estimate other quadratic information measures
(DED, DCS, IED, ICS). For example, the Cauchy-Schwarz divergence (24.9) can be estimated as
DCS(p1∥p2) = log
Vp1 Vp2
V 2c
,
(24.39)
where
Vp1 =
1
N 2
N

i=1
N

j=1
κ√
2σ(x1( j) −x1(i)),
(24.40)
Vp2 =
1
N 2
N

i=1
N

j=1
κ√
2σ(x2( j) −x2(i)),
(24.41)
Vc =
1
N 2
N

i=1
N

j=1
κ√
2σ(x1( j) −x2(i)),
(24.42)
where {x1(i)}N
i=1 and {x2(i)}N
i=1 are samples from p1 and p2, respectively, Vc is the cross information
potential estimator.
There are some nice properties of the nonparametric information estimators. For example, the entropy
estimatorspossessthesamelocationoftheextremum(maximumorminimum)astheactualentropy[10].
The following theorem shows that under a certain condition, the minimum value of entropy estimators
occurs when sample data are related to a δ distribution.
Theorem 1.
If the kernel function κσ(ξ) achieves its maximum value at ξ = 0, then the minimum
value of the entropy estimator in (24.36) is achieved when all sample data are equal to each other, that
is x1 = · · · = xN = c.
The above result can be easily proved. Actually, for α > 1, we have
N

j=1
 N

i=1
κσ(x j −xi)
α−1
≤
N

j=1
 N

i=1
max
x
κσ(x)
α−1
= N ακα−1
σ
(0).
(24.43)
And hence

Hα(X) =
1
1 −α log 1
N α
N

j=1
 N

i=1
κσ(x j −xi)
α−1
≥−log κσ(0).
(24.44)
When x1 = · · · = xN = c, the equality will hold. The proof for the case α < 1 is similar.
One can further prove that if the kernel function is continuous, differentiable, symmetric, and uni-
modal, then the global minimum in the above theorem will be smooth, that is, it has a zero gradient and
a positive semi-deﬁnite Hessian matrix [10]. In adaptive learning using numerical optimization tech-
niques, it is crucial that the global optimum be a smooth point in the weight space with zero-gradient.

1.24.5 Reproducing Kernel Hilbert Space Framework for ITL
1391
1.24.5 Reproducing kernel Hilbert space framework for ITL
ITL learning is closely related to kernel methods and Reproducing Kernel Hilbert Space (RKHS)
[36–38]. In this section, we present a RKHS framework for ITL. In fact, we can uniquely determine
a RKHS using the symmetric non-negative deﬁnite kernel function deﬁned as the cross information
potential (CIP).
1.24.5.1 RKHS for ITL
Let E be the set of all square integrable one-dimensional PDFs over the real numbers; that is, pi(x) ∈
E, ∀i ∈I, where

pi(x)2dx < ∞and I is an index set. Here, for simplicity, we focus on one
dimensional PDFs, the extension to multi-dimensions is straightforward. Then one can form a linear
manifold:

i∈K αi pi(x)

for any K ∈I, and αi ∈R. Let L2(E) be the Hilbert space that consists
of all linear combinations of PDFs and their limit points, and is equipped with inner product:
⟨fi(x), f j(x)⟩L2 =

fi(x) f j(x)dx
∀fi, f j ∈E.
(24.45)
Note that the above inner product is exactly the CIP between fi and f j. The Hilbert space L2(E) is not
a RKHS since the inner product is not reproducing in L2(E); that is, the evaluation of any element in
L2(E) cannot be reproduced via the inner product between two functions in L2(E). However, one can
use the inner product in (45) to induce a RKHS that is congruent with L2(E).
Let v( fi, f j) be a bivariate function on the set E, deﬁned as
v( fi, f j) =

fi(x) f j(x)dx
∀fi, f j ∈E.
(24.46)
It can be easily veriﬁed that the function v( fi, f j) is symmetric and non-negative deﬁnite, and is a
Mercer kernel function. According to the Moore-Aronszajn theorem [39,40], there is a unique RKHS,
denoted by Hv, associated with v( fi, f j). One can construct the RKHS Hv bottom-up. By Mercer’s
theorem [41], function v( fi, f j) has eigen-decomposition:
v( fi, f j) =
∞

k=1
λkψk( fi)ψk( f j),
(24.47)
where {ψk( fi), k = 1, 2, . . .} and {λk, k = 1, 2, . . .} are sequences of eigenfunctions and corresponding
eigenvalues of v( fi, f j), respectively. The above series converges absolutely and uniformly on E × E.
We then deﬁne a space Hv consisting of all functionals G(.) whose evaluation for any given PDF
fi ∈E is
G( fi) =
∞

k=1
λkakψk( fi),
(24.48)

1392
CHAPTER 24 Information Based Learning
where the sequence {ak, k = 1, 2, . . .} satisﬁes ∞
k=1 λka2
k < ∞. The inner product of two functionals
in Hv is deﬁned by
⟨G, F⟩Hv =
∞

k=1
λkakbk,
(24.49)
where F( fi) = ∞
k=1 λkbkψk( fi), ∞
k=1 λkb2
k < ∞. Now it can be veriﬁed that the space Hv induced
by the kernel function v( fi, f j) is indeed a RKHS, and this kernel function is a reproducing kernel
because it satisﬁes:
1. ∀fi ∈E,
v( fi, f j) as a functional of f j belongs to Hv,
2. ∀G ∈Hv, ⟨G, v( fi, .)⟩Hv = G( fi) (the so called reproducing property).
By the reproducing property, one can rewrite the kernel function v( fi, f j) as the inner product:
v( fi, f j) = ⟨v( fi, .), v( f j, .)⟩Hv,
v( fi, .) : fi →

λkψk( fi),
k = 1, 2, . . .
(24.50)
The reproducing kernel nonlinearly maps the original PDF fi into the RKHS Hv.
It is worth noting that, although the RKHS Hv and the Hilbert space L2(E) are much different,
they are congruent with each other. Actually one can prove that there exists a one-to-one congruence
mapping  from Hv to L2(E) such that (v( fi, .)) = fi [37].
1.24.5.2 ITL cost functions in RKHS
The ITL costs can be reformulated in RKHS Hv. First of all, the cross information potential (CIP) can
be expressed as the inner product in Hv:

p1(x)p2(x)dx = ⟨v(p1, .), v(p2, .)⟩Hv.
(24.51)
The inner product quantiﬁes similarity between two functionals and this agrees with the deﬁnition of
cross information potential. Then the quadratic information potential (QIP) can also be speciﬁed as the
inner product between a functional and itself:

p2(x)dx = ⟨v(p, .), v(p, .)⟩Hv = ∥v(p, .)∥2
Hv.
(24.52)
Therefore, maximizing QIP in ITL turns out to be maximization of the norm square in Hv. Using
(24.52), we can rewrite Renyi’s quadratic entropy as
H2(X) = −log ∥v(p, .)∥2
Hv.
(24.53)
Based on the reformulations of CIP (24.51) and QIP (24.52) one can easily rewrite other quadratic
information measures in terms of operations on functional in Hv. For example, the Cauchy-Schwarz
divergence measure can be expressed as
DCS(p1∥p2) = −2 log
⎛
⎝
⟨v(p1, .), v(p2, .)⟩Hv

∥v(p1, .)∥2
Hv∥v(p2, .)∥2
Hv
⎞
⎠= −2 log (cos θ),
(24.54)

1.24.5 Reproducing Kernel Hilbert Space Framework for ITL
1393
where θ is the angle in Hv between two functionals v
	
p1, .

and v
	
p2, .

. Therefore the RKHS Hv
provides an insightful geometric interpretation for CS-divergence; that is, the argument of the log of
CS-divergence truly measures the separation (angle) between two functional vectors in Hv.
1.24.5.3 Information estimator in RKHS
The estimators of information theoretic quantities can also be reinterpreted in RKHS (kernel space).
Consider the nonparametric kernel estimator of quadratic information potential (QIP) in (24.38). The
Gaussian kernel κ√
2σ(.) is a Mercer kernel and can be written as an inner product in a RKHS:
K√
2σ(xi −x j) = k(xi −x j) = ⟨ϕ(xi), ϕ(x j)⟩Hκ,
(24.55)
where ϕ(x) = κ(x, .) deﬁnes the nonlinear mapping between input space and kernel space Hκ. Hence
the estimated QIP in (38) can be expressed in terms of an inner product in kernel space:
V2(x) = 1
N 2
N

i=1
N

j=1
κ√
2σ(x j −xi)
= 1
N 2
N

i=1
N

j=1
⟨ϕ(xi), ϕ(x j)⟩Hκ
=

1
N
N

j=1
ϕ(Xi), 1
N
N

j=1
ϕ(Xi)

Hκ
= mT m = ∥m∥2.
(24.56)
It turns out that the estimated QIP may be expressed as the squared norm of the mean vector of the data
in kernel space. Other quadratic information estimators may also be expressed in terms of mean vectors
in the kernel space. Actually one can express the estimated ED-divergence and CS-divergence as
DED(p1∥p2) = ∥m1∥2 −2mT
1 m2 + ∥m2∥2 = ∥m1 −m2∥2,
(24.57)
DED(p1∥p2) = −2 log

mT
1 m2
∥m1∥∥m2∥

= −2 log ( cos ⟨(m1, m2)),
(24.58)
where m1 and m2 are kernel space mean vectors of the data points drawn from p1(x) and p2(x),
respectively. From (24.57) and (24.58), one can see the ED-divergence estimator measures the square
of the norm of the difference vector between m1 and m2, while the argument of the log of CS-divergence
estimator measures the cosine of the angle between mean vectors.
Figure 24.3 illustrates the relationship between the two RKHS Hv and Hκ. The information quan-
tities in Hv can be estimated by the mean operator of the projected functionals in Hκ, which was
effectively derived with Parzen’s nonparametric PDF estimator employed in ITL cost functions, and
with a Mercer kernel (symmetric and non-negative deﬁnite kernel) as the Parzen window.

1394
CHAPTER 24 Information Based Learning
FIGURE 24.3
The relationship between Hv and Hκ (from [37]).
1.24.6 Information particle interaction for learning from samples
In training of a multilayer network such as multilayer perceptrons (MLPs), the output (or output error)
is back-propagated through the layers to determine the gradient update rule for weight vector [42].
If the learning is supervised, the mean square error (MSE) is widely used as an adaptation cost. The
simple LMS algorithm is a special case of back-propagation (BP) for single layer network with linear
processing elements. The conventional BP algorithms can be readily extended to ITL costs. Under ITL
costs (e.g., error entropy), the gradient of the weight vector can be calculated by the chain rule of partial
derivatives as: (1) sensitivity of the output of the adaptive network with respective to its weights, and
(2) sensitivity of the ITL cost on the values of the samples. With the kernel estimator in Renyi’s entropy,
the latter sensitivity has an interesting analogy with physical particles interacting on an information
potential ﬁeld [43,44]. This analogy has its roots in the link between Renyi’s entropy and the norms
of the PDF of the data. Actually, since the kernels in PDF estimation are positive functions that decay
with the distance between samples, we can think that one kernel placed on a sample creates a potential
ﬁeld in the same place, just like physical particles create a gravity ﬁeld in space. In our case, however,
the law of interaction is dictated by the kernel shape. The potential ﬁeld in the sample space is thus an
approximation of the PDF that generates the data. In this context, samples can be named as information
particles and they interact in the information potential ﬁeld of the PDF through information forces [43].
The kernel estimator of quadratic information potential can alternatively be expressed as the average
sum of interactions from each sample in the sample set, that is
V2(X) =
1
N 2
N

i=1
N

j=1
κ√
2σ(x j −xi)

1.24.6 Information Particle Interaction for Learning from Samples
1395
= 1
N
N

j=1

1
N
N

i=1
κ√
2σ(x j −xi)

(24.59)
= 1
N
N

j=1

1
N
N

i=1
V2(x j, xi)

= 1
N
N

j=1
V2(x j),
where V2(x j, xi) = κ√
2σ(x j −xi), and V2(x j) = 1
N
N
i=1 V2(x j, xi). Here the term V2(x j, xi) mea-
sures the contribution of sample xi to the potential of x j, and the quantity V2(x j) measures the potential
ﬁeld in the space location occupied by the sample x j due to all the other samples. The information poten-
tial is thus just the average value of the information potential ﬁeld of the sample set (hence the name).
The derivative of the information potential with respect to the position of sample x j can be easily
evaluated as
∂
∂x j
V2(X) = 1
N
N

j=1
F2(x j),
F2(x j) = 1
N
N

i=1
F2(x j, xi),
(24.60)
where F2(x j) =
∂
∂x j V2(x j), F2(x j, xi) =
∂
∂x j V2(x j, xi). These two quantities are named the (total)
information force acting on sample x j and the information force on sample x j due to sample xi, respec-
tively. Note that the concepts of information potential ﬁelds and information forces can be generalized
to other information measures, such as quadratic divergence and quadratic mutual information [7].
In adaptive system training with ITL costs, the system parameters can be adapted by information
forces created among the pair-wise interactions in the sample set. In supervised learning, if the error
samples are generated by an MLP with weights ω, the gradient of the information potential with respect
to these weights would be [45]
∂
∂ω
V2(e) = 1
N 2
N

i=1
N

j=1
F2(x j, xi)
∂xi
∂ω −∂x j
∂ω

.
(24.61)
Thus, for the adaptation of an MLP, the principle of error back-propagation can be extended to informa-
tion force back-propagation by simply substituting the injected error of the MSE with the information
force F2(x j, xi).
Figure 24.4 shows a snapshot of a two-dimensional entropy maximization scenario, where the parti-
cles are bounded to within a unit square and interact under the quadratic force deﬁnition with Gaussian
kernel choice. The objective is to maximize the entropy of the sample ensemble, and hence the forces
become repulsive. Given a set of randomly spaced samples in the unit square, when the forces acting
on each sample are evaluated, it becomes clear that the information particles are pushed by the other
particles in order to move along the direction of maximal entropy. In Figure 24.4, the lines attached to
each sample are vectors that display intensity and point to the direction of change.

1396
CHAPTER 24 Information Based Learning
FIGURE 24.4
A snapshot of the locations of the information particles and the instantaneous quadratic information forces
acting on them to maximize the joint entropy in the two-dimensional unit square (from [43]).
1.24.7 Illustrative examples
1.24.7.1 MEE based adaptive system training
Back Propagation for MEE: The entropy of the error can be employed as a robust cost function to learn
the parameters of function approximation. Below, we derive a back propagation procedure to adjust the
parameters of a feedforward network based on the error entropy. In particular, we want to minimize the
Renyi’s second order entropy of the prediction error in a time series using a time delay neural network
as predictor. A sliding window of size L is utilized to compute the estimation of the error information
potential. Suppose we have an upcoming example at time t, the estimate of the information potential is
given by,
VE(t) =
t
i, j=t−L+1
κσ(∥e(i) −e( j)∥).
(24.62)

1.24.7 Illustrative Examples
1397
Although, (24.62) is conceptually the right cost, the computation of the gradient for multiple output
functions is rather cumbersome. Therefore a modiﬁed version of this cost is introduced,
VE(t) =
t
i, j=t−L+1
Mp

k=1
κσ(ek(i) −ek( j)).
(24.63)
The gradient equations with respect to the network parameters of the output layer p are:
∂VE(t)
∂w(p)k
=
1
σ 2L2
t
j=t−L+1
κσ(ek(i) −ek( j))(ek(i) −ek( j))
⎡
⎣f ′
p

w(p)T
k
δ(p−1)(i)

δ(p−1)(i)+
−f ′
p

w(p)T
k
δ(p−1)( j)

δ(p−1)( j)
⎤
⎦
(24.64)
and for the hidden layer p −1,
∂VE(t)
∂w(p−1)
l
=
1
σ 2L2
Mp

k=1
t
j=t−L+1
κ′
σ(ek(i) −ek( j))
×
⎡
⎣f ′
p

w(p)T
k
δ(p−1)(i)

w(p)
kl f ′
p−1

w(p−1)T
t
δ(p−2)(i)

δ(p−2)(i)+
−f ′
p

w(p)T
k
δ(p−1)( j)

w(p)
kl f ′
p−1

w(p−1)T
t
δ(p−2)( j)

δ(p−2)( j)
⎤
⎦,
(24.65)
where the matrices w(i) and fi are the weights and activation function of the ith layer and δ(i)(t) =
fi(w(i)δ(i−1)(t)) the outputs of the units at the same layer.
Experiments: In the following a feedforward neural network is trained to perform prediction of a
nonlinear time series. The Mackey-Glass system is a chaotic system deﬁned by
˙x(t) = −0.1x(t) +
0.2x(t −τ)
1 + x(t −τ)10 .
(24.66)
In particular, we use τ = 30 , the number of delays for the time embedding of the series is 7, the
generation of the samples follows [46]; a total of 5000 samples are generated. A TDNN with 10 units
in the hidden layer and sigmoidal tangent activation function is trained using the ﬁrst 500 samples from
the series and in the case of MEE the sliding window has L = 50. The kernel size is kept ﬁxed at
σ = 0.2. The parameters are adjusted using momentum gradient with parameter γ = 0.5 and the step
size is η = 0.08 for MSE and η = 0.02 for MEE. Figure 24.5 shows the desired and predicted test
signals for the two different cost functions.
The advantage of MEE can be observed from the distribution of the test error shown in Figure 24.6.
Note that the test errors obtained from the network trained with MEE is more concentrated around zero
than the one trained by MSE.
1.24.7.2 ED-divergence-based classiﬁcation
In this case, we are given two or more distributions, for which, we are interested in basically 2 types
of interactions: the particle interactions within a distribution, and the particle interactions among dis-
tributions. First, let us recall the deﬁnition of the so called Euclidean divergence between densities.

1398
CHAPTER 24 Information Based Learning
FIGURE 24.5
Predicted and desired outputs for TDNN trained with MSE and MEE.
For random variables (vectors) X1 and X2 and their respective densities p(x) and q(x). The Euclidean
divergence is the overall squared difference between p and q along their common support X ⊂Rd, that is
DED(p∥q) =

X
(p(x) −q(x))2dx.
(24.67)
The ﬁrst condition that naturally arises from this deﬁnition is that p and q must be square integrable
functions. Mutual information was originally conceived as a measure of common entropy associated
with two variables. In Shannon’s deﬁnition of mutual information leads to the more general concept of
KL-divergence that can be utilized to compute mutual information as a dissimilarity between the joint
and marginal densities (or probability mass functions). Extending this idea of dissimilarity we have the
Quadratic mutual information based on the Euclidean divergence QMIEDfor two random variables (or
vectors) X1 and X2 with joint p(x1, x2) and marginals p1(x) and p2(x), respectively:
QMIED(X1; X2) = DED(p(x1, x2)∥p1(x1)p2(x2)).
(24.68)
Notice this deﬁnition can be adapted for discrete random variables where densities are now probability
mass functions and integrals become ﬁnite or countable summations.

1.24.7 Illustrative Examples
1399
FIGURE 24.6
Densities of the test errors for MSE and MEE after training.
The dataset is divided into two classes; therefore we have a second variable Y which is the class
label. We can use the Euclidean divergence to compare the conditional density of X given the labelY
that yields DED(p(x|Y = 1)∥p(x|Y = 2)). We will denote the data points that belong to class 1 as x(1)
i
and class 2 as x(2)
i
, the number of instances for class 1 is N1 and N2 for class 2. Let us construct a kernel
matrix with all the data points available for the two classes in the following way:
Kfull =
$ K1 K12
K21 K2
%
,
(24.69)
where K1 corresponds to all pair-wise comparisons between points from class 1, likewise K2 is deﬁned
for points in class 2 and the elements of K12 = KT
21 are given by Ki j = κ(xi, x j). Having introduced
this notation, let’s compute the empirical estimate of the Euclidean divergence as:
⌢DED(p(x|Y = 1)∥p(x|Y = 2)) = 1
N 2
1
1TK11 + 1
N 2
2
1TK21 −
2
N1N2
1TK121.
(24.70)

1400
CHAPTER 24 Information Based Learning
The ﬁrst two terms of the right-hand side are the information potential within class and the third term
is the cross-information potential. Basically, we have two potential ﬁelds each one generated by the
samples of the corresponding class. Figure 24.7 shows the information potential ﬁelds created by each
one of the classes along with the forces experienced by each data point under that speciﬁc ﬁeld.
The total forces are presented in Figure 24.8 as well as the difference between the two potential ﬁelds
generated by each class.
The ﬁrst important observation is that the interaction of these two ﬁelds leads to the maximum likeli-
hood classiﬁcation rule. If we compute the difference between the information potentials of generated by
class 1 and class 2, we are basically comparing the likelihood of a data point with respect to a conditional
density given a particular class and the force experienced by the point will drive it to the class it is more
likely to belong. Figure 24.9 shows the partition obtained by using the Gaussian kernel with σ = 1.
An interesting problem to use mutual information with the data set is the assessment of the strength of
the relation between the class labels and the random vector describing the patterns that is QMIED(X; Y).
This problem involves the combination of a discrete random variable Y (the labels) and a real random
vector X. The quantity of interest reduces to:
QMIED(X; Y) = P(Y = 1)2DED(p(x|Y = 1)∥p(x))+P(Y = 2)2DED(p(x|Y = 2)∥p(x)). (24.71)
In this case we are measuring the difference between the two classes by weighting the divergences
between the PDF of the data when the class is not known and the density of the data given the class is
known. If we relate this problem to classiﬁcation we can think of it as an approximation to the MAP
rule for Parzen density estimation. In our setting P(Y = 1) =
N1
N1+N2 and P(Y = 2) =
N2
N1+N2 .
However, if we set N1 = N2 the result from the particle interaction should be the same than the one
using
⌢DED(p(x|Y = 1)∥p(x|Y = 2)).
FIGURE 24.7
Information potential and forces created by each class.

1.24.7 Illustrative Examples
1401
−
FIGURE 24.8
IP1 −IP2 ﬁeld and total information forces for the two class problem.
1.24.7.3 Information cut for clustering
Cauchy-Schwarz objective function: Clustering objective functions encourage the compactness of the
clusters. In information theoretic clustering each cluster is represented by a density that should be
as far apart as possible from the other densities (clusters). However, distances are deﬁned between
two distributions, and therefore it becomes necessary to ﬁnd a suitable extension that considers more
than 2 clusters. In [47], an extension based on Cauchy-Schwarz divergences is proposed. It uses the
ratio between the sum of cross-information potentials between all pairs of clusters and the product of
information potentials of each cluster. Consider the kernel matrix K with all pairwise computations of
the kernel function and the membership matrix M, where each row contains a binary vector mT
i that
indicates the membership of the ith point associated with the given row. The cost J associated with the
membership matrix M is given by:
J(M) = 1T K1 − diag(MT KM)
 diag(MT KM)
,
(24.72)
where the sum and product are over the elements of the diagonal of (MT KM). To solve this problem
using gradient descend techniques the membership vectors are allowed to take real values.
Experiments: Figure 24.10 present the resulting clusters from Cauchy-Schwarz criterion and k-means
for two synsthetic datasets. The matrix M is initialized with random cluster assignments. As expected
k-means perform poorly since it cannot identify non-convex clusters or cluster with different scales.
It has to be noted that the even though these results are achieved the majority of times since they still
depend on the initial assignments and the selection of proper kernel sizes.

1402
CHAPTER 24 Information Based Learning
FIGURE 24.9
Partition of the space created by the difference between information potentials due to each class. This is
equivalent to the maximum likelihood rule for classiﬁcation when using Parzen window estimation for the
conditional densities given the class.
1.24.7.4 Principle of relevant information
The PRI as a Weighting Problem: For the set F of probability density functions that are square integrable
in X ⊂Rd, the cross-information potential (CIP) is deﬁned as a bilinear form that maps densities
fi, f j ∈F to the real numbers trough the integral,
V ( fi, f j) =

X
fi(x) f j(x)dx.
(24.73)
It is easy to see that for a basis of uniformly bounded, square integrable probability density func-
tions, V ( fi, f j) is a positive semideﬁnite function on the span { fi ∈F}. Now, consider the set G =
{g = m
i=1 αiκσ(xi, ·)|xi ∈Rd, m
i=1 αi = 1, αi ≥0}, where κσ is a “Parzen” type of kernel which is
also square integrable, that is, κσ is symmetric, nonnegative, has bounded integral (can be normalized),
belongs to L2, and shift invariant with σ as the scale parameter. Clearly, for any g ∈G we have ∥g∥2 ≤
∥κσ(x, ·)∥2. Hence, G is bounded. However, if X is non-compact our search space is also non-compact.
The objective function for the principle of relevant information can be written in terms of IP func-
tionals. Using Parzen based estimation, we restrict the search problem to G ⊂F. In this case, we have
that Eq. (24.26) can be rewritten as:
J( f ) = −log V ( f , f ) −λ log
V ( f , g)2
V ( f , f )V (g, g).
(24.74)

1.24.7 Illustrative Examples
1403
(a) Crecent Data
(b) Triangle Data
FIGURE 24.10
Clustering results for synthetic datasets.
Straightforward manipulation of the terms yields an equivalent problem:
arg min
f
J( f ) = −(1 −λ) log V ( f , f ) −2λ log V ( f , g).
(24.75)
Twoimportantaspectsoftheaboveobjectiveare:thechoiceofthekernelanditssize σ,whichdetermines
different scale of the analysis; and the trade-off parameter λ, which deﬁnes a set of regimes for the
possible solutions to the problem. The only available information is contained in the sample S = {xi}N
i=1.
An approximation of the target density g is then given by its weighted Parzen window estimator ⌢g(x) =
N
i=1 αiκ(xi, x). In our experiments, we use αi = 1
N . To enforce compactness in our search space, we

1404
CHAPTER 24 Information Based Learning
look for a solution f that has the same form of ⌢g, that is
f (x) =
N

i=1
βiκ(xi, x),
(24.76)
where βi ≥0, N
i=1 βi = 1. By ﬁxing λ and evaluating the information potential between each pair
(xi, x j) ∈S × S, we can rewrite (24.69) in matrix notation as:
min
β
J(β) = (λ −1) log βTVβ −2λ log βTVα
s.t. β ≥0 and βT1 = 1,
(24.77)
where Vi j =

X κ(xi, x)κ(x j, x)dx. It is also important to highlight that for values of λ →1 the
regularization term almost vanishes, and the maximum information gain (inner product term) can be
related to the modes of ⌢g, which are approximated by the sharpest functions available in G, which in our
setting, are individual windows that satisfy convex combination constraints. Therefore, we can expect
the modes of ⌢g to be almost orthogonal components. When λ is very close to one, the cross- entropy
term is dominant in the objective and the solution will lie in one of the corners of the simplex. Figure
24.11 depicts the geometrical elements that explain this phenomenon. The semicircles are the loci of
minimum and maximum of the L2 norm that intersect with the simplex. The solution f ∗with the largest
norm is the one maximizing the dot product with the target distribution g.
This corner phenomenon motivates the extraction of more than one solution. A reasonable approach
is to deﬂate the target PDF ⌢g and run the algorithm on the new function.
We perform an illustrative experiment on a systhetic data set that correspond to a Gaussian surrounded
by a circular crown as depicted in Figure 24.13. Figure 24.12 shows different values of the entropy when
the trade off parameter λ is varied between 1 and 20. The actual plot shows the inverse of λ to facilitate
visualization.
FIGURE 24.11
Effect of the PRI objective based on weights.

1.24.7 Illustrative Examples
1405
FIGURE 24.12
Entropy of the weighted sample and cross-entropy with respect to the original sample Sfor different values
of λ. Notice that the values are plotted against 1
λ for better visualization.
In this case, we can observe two regimes; for λ < 1.5, all the importance is given to one point, which
corresponds to the largest peak on the Parzen density estimate from the observed data S. Once λ becomes
larger than 1.5 more points become active, and the weights progressively equalize across all data points.
Figure 24.13 display the resulting support vectors found after optimization for different values of λ.
Image Retrieval with Partially Occluded Data MNIST: We employ a subset of the MNIST database to
perform experiments on pattern retrieval from partially occluded queries. The weighting scheme for
the principle of relevant information is applied to learn the representative samples from the training
data. Results are compared against kernel PCA and kernel entropy component analysis for different
number of components and different values of λ in the case of PRI. The pattern retrieval application
requires pre-imaging of the patterns from the feature space back to the input space to apply KPCA
(KECA). We employ the method presented in [48] to compute the pre-images of the projected patterns
in the KPCA and KECA subspaces (For more details on this problem see [49,50]). The principle of
relevant information requires a different approach since it does not provide an explicit projection method
unlike KPCA and KECA. A simple pre-imaging algorithm can be based on the Nadaraya-Watson kernel
regression [51]. In our experiments, we use the Gaussian kernel. The pre-imaging for PRI consists of
the following steps:

1406
CHAPTER 24 Information Based Learning
FIGURE 24.13
Resulting support vectors for different values of λ.
•
Compute the k-nearest neighbors on the set of support vectors (training points with nonzero weights)
of the query pattern x.
•
Reconstruct using the following equation:
xrec =
k
i=1 κσ(xi, x)xi
k
i=1 κσ(xi, x)
,
(24.78)
where the indexes for xi are understood as the k nearest neighbors of x.

1.24.7 Illustrative Examples
1407
Table 24.1 Results for the Image Retrieval from Partially Occluded Queries
Number of components
3
8
15
30
50
100
KPCA
0.59(0.03)
0.55(0.015)
0.53(0.007)
0.50(0.007)
0.48(0.008)
0.47(0.006)
KECA
0.64(0.018)
0.57(0.018)
0.51(0.022)
0.49(0.013)
0.49(0.009)
0.48(0.007)
PRI
0.58(0.013)
0.53(0.004)
0.51(0.007)
0.50(0.009)
0.49(0.01)
0.48(0.01)
λ
2
3
4
5
6
8
N support vectors
62.7
126
181
232
276
347
Note: The high lighted values indicate the smallest “normalized SNR” in each column.
FIGURE 24.14
Sample queries with missing information and their corresponding complete versions.
In this case, we want to see if we can effectively retrieve a digit by presenting an image with a missing
portion. The model is trained with complete images and tested with images for which the lower half of
the matrix has been replaced with zeros.
Table 24.1 shows the normalized SNR for the retrieved patterns using a KPCA KECA and the
weighted PRI. The numbers correspond to averages from ten Monte Carlo simulations using 200 images
per digit (total 600) for training and 50 incomplete images of each digit (150 total) for testing. We also
present the average number of support vectors for the RKHS PRI. The standard deviations are also
shown. The kernel size is 5, and the number of neighbors for pre-imaging is 10.
Notice that differences between closest results for all methods and also the best performances are
within the range set by one standard deviation.
Figure 24.14 shows some of the exemplars that were used for testing along with the incomplete
versions that were input to the algorithms. Figure 24.15a shows the reconstructed images for KPCA
where each row corresponds to the number of components in Table 24.1. Likewise, Figure 24.15b
displays the results for KECA, and Figure 24.15c shows the results of PRI approach for the different λ
values in Table 24.1.
1.24.7.5 Independent component analysis
Minimum Renyi’s Mutual Information: The MRMI algorithm is motivated by minimization of mutual
dependence between the estimated sources in the context of non-parametric estimation [52,53]. The
basic idea is to use an alternative deﬁnition of entropy that facilitates the application of non-parametric
density estimation techniques such as Parzen windowing.

1408
CHAPTER 24 Information Based Learning
(a) KPCA
(b) KECA
(c) PRI
FIGURE 24.15
Sample queries with missing information and their corresponding complete versions.
Renyi’s mutual information is
Iα(Y) =
1
α −1 log
 ∞
−∞
fY (y)α
d
i=1 fYi (yi)α−1 dy

.
(24.79)
Although this deﬁnition generalizes the concept of KL-divergence, properties such as recursion fail to
hold since the formulation followed for Shannon’s entropy yields
d

i=1
Hα(Yi) −Hα(Y) =
1
α −1 log

 ∞
−∞fY (y)αdy
 ∞
−∞
d
i=1 fYi (yi)α−1dy

.
(24.80)
The expression in (24.80) is zero if the Yi s are independent. Unfortunately, zero value is not sufﬁcient
condition for independence. Assuming favorable condition on the distribution of the sources, we have
that for an orthogonal rotation R on a whitened version of the observed variables Xi the joint entropy
of the estimated sources Y = RPX = WX is invariant under orthonormal transformations. Thus mini-
mization of (24.80) can be done only on the sum of the marginal entropies. The objective function is then
J(R) =
d

i=1
Hα(Yi).
(24.81)

1.24.7 Illustrative Examples
1409
For α = 2, Eq. (24.81) is estimated by using Parzen windowing with a Gaussian kernel; it is easy to
note that the minimization of this cost will look for super-Gaussian distributions since sum of output
marginal entropies is minimized. Therefore we should expect the algorithm to fail in the presence of
sub-Gaussian sources. Nevertheless, switching the sign of some of the terms in the sum J(R) yields a
search procedure that can converge to sub-Gaussian distributions. Therefore an algorithm obtained by
assigning individual signs to the marginal entropies Hα((RY)i) can ﬁnd sub and super Gaussian sources.
Performance of the algorithms is measured using signal-to-distortion ratio (SDR):
SDR = −10log
⎡
⎣
1
d(d −1)
d

i=1
⎛
⎝
d
j=1 |Bi j|
max
j
|Bi j| −1
⎞
⎠
⎤
⎦,
(24.82)
where B = WA, being A the known mixing matrix.
Synthetic Data: The aim of the artiﬁcial data is to check the validity of our analysis of the expected
properties for the algorithms. For this purpose we create a set of artiﬁcial signals described as follows:
xs(t) = sin
2π
Ts
t

,
xw(t) = saw
2π
Tw
t + θ

,
xe(t) = 1
5sign
	
xw(t)

( exp (5xw(t)) −1).
(24.83)
In this case xs(t) and xw(t) are sub-Gaussian, and xe(t) is super-Gaussian. We create two sets of
sources xa(t) =
	
xs(t) xw(t)

T (both sub-Gaussian) and xa(t) =
	
xs(t) xe(t)

T (mixed type sub- and
super-Gaussian). Figure 24.16 displays the three simulated sources.
We only want to evaluate the properties of the estimated sources; therefore, we run the update
throughout the whole signal using on-line learning mode. Figure 24.17 display the estimated sources
using infomax for observed xa(t) and xb(t), respectively; the corresponding SDRs are 0.8414 and
2.72. As expected the algorithm failed to ﬁnd the correct de-mixing since the activation function can-
not model sub-Gaussian distributions and so prefers a solution for which the estimated sources are
apparently super-Gaussian.
For the MRMI criterion, given that we have a priori knowledge of the source distribution, we switch
the sign of the objective function for both sources for the observed xa(t). The estimated density is
calculated taking a uniform random sub-sample of 500 instances from the entire observed signal. The
sign of each marginal output entropy (maximum or minimum) is chosen according to the sign of the
deviation from the kurtosis for a normal random variable. The objective function from Eq. (24.81) can
be modiﬁed as follows:
J(R) = −
d

i=1
sign(kurt(Yi) −3)Hα(Yi).
(24.84)
Figure 24.18 shows the extracted sources by using the modiﬁed version of MRMI using kurtosis.
The SDRs are 22.2165 for xa(t) and xb(t). Notice that in both cases the modiﬁed version succeeded in
ﬁnding the independent components.

1410
CHAPTER 24 Information Based Learning
FIGURE 24.16
Sub-Gaussian and Super-Gaussian artiﬁcial sources.
(a)
(b)
FIGURE 24.17
Estimated sources for infomax.

1.24.7 Illustrative Examples
1411
(a)
(b)
FIGURE 24.18
Estimated sources for modiﬁed MRMI.
Table 24.2 SDR Behavior of 20 Simulations of Infomax in On-Line Mode for Different Training
Sample Sizes
Sub-sample size
SDR
100
1000
10000
Average
7.8
8.01
8.71
Std Deviation
3.39
4.12
4.26
Table 24.3 SDR Behavior of 20 Simulations of MRMI for Different Training Sample Sizes and
300 epochs
Sub-sample size
SDR
50
100
500
Average
13.92
14.56
14.77
Std Deviation
1.39
0.67
0.439
Mixture of Audio Signals: The problem is to separate an interest sound from the background activity
(noise). To see the stability and accuracy of the estimation based on the size of the training data we
randomly sub-sample the whole signal in different proportions. Table 24.2 displays the average SDR
of 20 simulations of infomax along with the standard deviation on sub-sample sizes of 100, 1000, and
10,000 instances. The number of epochs is inversely proportional to the sub-sample size providing the
same number of iterations (200,000); the stepsize is η = 0.3. A different procedure is followed for

1412
CHAPTER 24 Information Based Learning
MRMI. Sample sizes are 50, 100, and 500 and the number of epochs is ﬁxed to 300. Table 24.3 displays
the average SDR of 20 simulations of MRMI and their corresponding standard deviations. Kernel is
ﬁxed to σ = 0.5 since data is whitened, the stepsize is η = 0.5.
1.24.8 Conclusions and future trends
Traditional approaches to machine learning and signal processing in general aim to optimize the second
order statistics. When data is non-Gaussian, a more appropriate approach would be to capture higher
order statistics or information content of signals rather than simply their energy. Especially, information
theoretic quantities as optimality criteria attract ever-increasing attention to this end, since they provide
a natural framework where information content of available data can be assessed. Combining infor-
mation theoretic descriptors (such as entropy, divergence, and mutual information) and kernel density
estimation, one can obtain a general, nonparametric, and sample based methodology to learn arbitrary
nonlinear systems.
In this chapter, we brieﬂy reviewed the general methodology to implement machine learning and
adaptation algorithm with information theoretic criteria, which synergistically integrates the general
framework of information theory in the design of new cost functions for machine learning and adaptive
signal processing. Information based learning not only affects our understanding of optimal signal
processing, but also inﬂuence the way we approach machine learning, data compression, and adaptation.
With smooth nonparametric information estimators, ITL methodology allows simple gradient based
learning rule to be constructed for learning nonlinear topologies. In most situations, the performance
of ITL learning algorithms compare favorably with existing methods developed under second order
statistics criteria. For illustration purpose, we also include in this chapter several application examples.
Information-based learning is still one of the freshest ideas ﬂoating around, which has not yet become
prevalent today. There are many issues that need to be handled in the next stage. Examples are: (1) how to
choose the proper parameters such as the order of the information potential or the kernel size in entropy
estimator; (2) how to simplify the computational complexity especially in batch mode learning; (3) how
to efﬁciently implement the algorithms in the hardware. If these issues have been properly resolved,
information based learning will become more practical and powerful, and it will ﬁnd a widespread
acceptance in machine learning community.
Relevant Theory: Signal Processing Theory, Machine Learning and Statistical Signal Processing
See this Volume, Chapter 12 Adaptive Filters
See this Volume, Chapter 16 Kernel Methods and SVMs
See this Volume, Chapter 17 Online Learning in Reproducing Kernel Hilbert Spaces
See Vol. 3, Chapter 4 Bayesian Computational Methods in Signal Processing
References
[1] T. Cover, J. Thomas, Element of Information Theory, Wiley, New York, 1991.
[2] C. Shannon, W. Weaver, The Mathematical Theory of Communication, University of Illinois Press, Urbana,
1949.
[3] E. Pfaffelhuber, Learning and information theory, Int. J. Neurosci. 3 (1972) 83–88.

References
1413
[4] H. Barlow, Unsupervised learning, Neural Comput. 1 (1989) 295–311.
[5] R. Linsker, Self-organization in a perceptual network, IEEE Comput. 21 (1988) 105–117.
[6] J.C. Principe, D. Xu, Q. Zhao, J.W. Fisher III, Learning from examples with information theoretic criteria,
J. VLSI Signal Process. 26 (2000) 61–77.
[7] J.C. Principe, Information Theoretic Learning: Renyi’s Entropy and Kernel Perspectives, Springer, 2010.
[8] J.O. Berger, Statistical Decision Theory and Bayesian Analysis, second ed., Springer-Verlag, New York, 1985.
[9] A. Renyi (Ed.), Selected Papers of Alfred Renyi, vol. 2, Akademia Kiado, Budapest, 1976.
[10] D. Erdogmus, Information Theoretic Learning: Renyi’s Entropy and Its Applications to Adaptive Systems
Training, University of Florida, Gainesville, Ph. D. Dissertation, 2002.
[11] D. Erdogmus, J.C. Principe, Generalized information potential for adaptive systems training, IEEE Trans.
Neural Networks 13 (5) (2002) 1035–1044.
[12] E. Parzen, On the estimation of a probability density function and the mode, Ann. Math. Stat. 33 (1962)
1065–1067.
[13] B. Silverman, Density Estimation for Statistics and Data Analysis, Chapman and Hall, London, 1986.
[14] E. Jaynes, Information theory and statistical mechanics, Phys. Rev. 106 (1957) 620–630.
[15] E. Jaynes, Probability Theory, the Logic of Science, Cambridge University Press, Cambridge, UK, 2003.
[16] A. Renyi, Probability Theory, University Amsterdam, North-Holland, 1970.
[17] E. Lutwak, D. Yang, G. Zhang, Cramer-Rao and moment-entropy inequalities for Renyi entropy and gener-
alized Fisher information, IEEE Trans. Inform. Theory 51 (2) (2005) 473–479.
[18] M. Rao, Y. Chen, B.C. Venuri, F. Wang, Cumulative residual entropy: a new measure of information, IEEE
Trans. Inform. Theory 50 (6) (2004) 1220–1228.
[19] B. Chen, P. Zhu, J.C. Principe, Survival information potential: a new criterion for adaptive system training,
IEEE Trans. Signal Process. 60 (3) (2012) 1184–1194.
[20] H.L. Weidemann, E.B. Stear, Entropy analysis of estimating systems, IEEE Trans. Inform. Theory 16 (1970)
264–270.
[21] P. Kalata, R. Priemer, Linear prediction, ﬁltering and smoothing: an information theoretic approach, Inform.
Sci. 17 (1979) 1–14.
[22] D. Erdogmus, J.C. Principe, An error-entropy minimization algorithm for supervised training of nonlinear
adaptive systems, IEEE Trans. Signal Process. 50 (7) (2002) 1780–1786.
[23] J. Santos, L. Alexandre, J. Sa, The error entropy minimization algorithm for neural network classiﬁcation, in:
A Lofti (Ed.), Int. Conf. Recent Advances in Soft Computing, 2004, pp. 92–97.
[24] G. Brown, A. Pocock, M.-J. Zhao, M. Lujan, Conditional likelihood maximization: a unifying framework for
information theoretic feature selection, J. Mach. Learning Res. 13 (2012) 27–66.
[25] R. Fano, Transmission of Information, MIT Press, 1961.
[26] A.J. Bell, T.J. Sejnowski, An information maximization approach to blind separation and blind deconvolution,
Neural Comput. 7 (6) (1995) 1129–1159.
[27] S. Rao, Unsupervised Learning: An Information Theoretic Learning Approach, University of Florida,
Gainesville, Ph. D. Dissertation, 2008.
[28] L. Sanchez Giraldo, J.C. Principe, An efﬁcient rank-deﬁcient computation of the principle of relevant
information, in: International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2011),
pp. 2176–2179.
[29] L. Sanchez Giraldo, J.C. Principe, A reproducing kernel Hilbert space formulation of the principle of relevant
information, in: IEEE International Workshop on Machine Learning for, Signal Processing (MLSP), 2011.
[30] A. Hyvarinen, J. Karhunen, E. Oja, Independent Component Analysis, Wiley, New York, 2001.
[31] S. Haykin (Ed.), Blind Deconvolution, Prentice-Hall, Upper Saddle River, NJ, 1994.
[32] N. Tishby, F. Pereira, W. Bialek, The information bottleneck method, in: Proceedings of the 37th Annual
Allerton Conference on Communication, Control and Computing, 1999, pp. 368–377.

1414
CHAPTER 24 Information Based Learning
[33] N. Tishby, N. Slonim, Data clustering by markovian relaxation and the information bottleneck method, in:
Advances in Neural Information Processing Systems, vol.13, Denver, 2000, pp. 640–646.
[34] J. Beirlant, E.J. Dudewicz, L. Gyorﬁ, E.C. van der Meulen, Nonparametric entropy estimation: an overview,
Int. J. Math. Stat. Sci. 6 (1) (1997) 17–39.
[35] L. Devroye, G. Lugosi, Combinatorial Methods in Density Estimation, Springer, New York, 2001.
[36] J.-W. Xu, Nonlinear Signal Processing Based on Reproducing Kernel Hilbert Space, University of Florida,
Gainesville, Ph. D. Dissertation, 2008.
[37] J.-W. Xu, A. Paiva, I. Park, J.C. Principe, A reproducing kernel Hilbert space framework for information-
theoretic learning, IEEE Trans. Signal Process. 56 (12) (2008) 5891–5902.
[38] R. Jenssen, D. Erdogmus, J.C. Principe, T. Eltoft, Some equivalence between kernel and information theoretic
methods, J. VLSI Signal Process. 45 (2006) 49–65.
[39] E. Moore, On properly positive Hermitian matrices, Bull. Amer. Math. Soc. 23 (59) (1916) 66–67.
[40] N. Aronszajn, The theory of reproducing kernels and their applications, Cambridge Phil. Soc. Proc. 39 (1943)
133–153.
[41] J. Mercer, Functions of positive and negative type, and their connection with the theory of integral equations,
Phil. Trans. Roy. Soc. Lond. 209 (1909) 415–446.
[42] S. Haykin, Neural Networks: A Comprehensive Foundation, Prentice Hall, Upper Saddle River, NJ, 1999.
[43] D. Erdogmus, K. Hild, J.C. Principe, Beyond second order statistics for learning: a pairwise interaction model
for entropy estimation, J. Nat. Comput. 1 (1) (2003) 85–108.
[44] J.C. Principe, D. Xu, J. Fisher, Information theoretic learning, in: S. Haykin (Ed.), Unsupervised Adaptive
Filtering, Wiley, New York, 2000, pp. 265–319.
[45] D. Erdogmus, J.C. Principe, From linear adaptive ﬁltering to nonlinear information processing, IEEE Signal
Process. Mag. 23 (2006) 14–33.
[46] W. Liu, Il Park, J.C. Principe, An information theoretic approach of designing sparse kernel adaptive ﬁlters,
IEEE Trans. Neural Networks 20 (2009) 1950–1961.
[47] R. Jenssen, D. Erdogmus, II K. Hild, J.C. Principe, T. Eltoft, Information cut for clustering using a gradient
descent approach, Pattern Recog. 40 (2006) 796–806.
[48] J.T. Kwok, I.W. Tsang, The preimage problem in kernel methods, in: Proceeedings of the 20th International
Conference on Machine Learning, 2003, pp. 408–415.
[49] S. Mika, G. Ratch, J. Weston, B. Schölkopf, K.-R. Müller, Fisher discriminant analysis with kernels, in: IEEE
Signal Processing Society Workshop on Neural Networks for Signal Processing, August, 1999, pp. 41–48.
[50] R. Jenssen, T. Eltoft, M. Girolami, D. Erdogmus, Kernel maximum entropy data transformation and an
enhanced spectral clustering algorithm, in: NIPS, 2006, pp. 633–640.
[51] E.A. Nadaraya, On estimating regression, Theory Probab. Appl. 9 (1) (1964) 141–142.
[52] K.E. Hild II, D. Erdogmus, J.C. Principe, Blind source separation using Renyi’s mutual information, IEEE
Signal Process. Lett. 8 (6) (2001) 174–176.
[53] D. Erdogmus, K.E. Hild II, J.C. Principe, Blind source separation using Renyi’s α-marginal entropies,
Neurocomputing 49 (2002) 25–38.

25
CHAPTER
A Tutorial on Model Selection
Enes Makalic*, Daniel Francis Schmidt* and Abd-Krim Seghouane†
*Centre for M.E.G.A. Epidemiology, The University of Melbourne, Australia
†Department of Electrical and Electronic Engineering, The University of Melbourne, Australia
1.25.1 Introduction
A common and widespread problem in science and engineering is the determination of a suitable model
to describe or characterize an experimental data set. This determination consists of two tasks: (i) the
choice of an appropriate model structure, and (ii) the estimation of the associated model parameters.
Given the structure or dimension of the model, the task of parameter estimation is generally done by
maximum likelihood or least squares procedures. The choice of the dimension of a model is often
facilitated by the use of a model selection criterion. The key idea underlying most model selection
criteria is the parsimonity principle which says that there should be a tradeoff between data ﬁt and
complexity. This chapter examines three broad approaches to model selection commonly applied in the
literature: (i) methods that attempt to estimate the distance between the ﬁtted model and the unknown
distribution that generated the data (see Section 1.25.2); (ii) Bayesian approaches which use the posterior
distribution of the parameters to make probabilistic statements about the plausibility of the ﬁtted models
(see Section 1.25.3), and (iii) information theoretic model selection procedures that measure the quality
of the ﬁtted models by how well these models compress the data (see Section 1.25.4).
Formally, we observe a sample of n data points y = (y1, . . . , yn)′ from an unknown probability
distribution p∗(y). The model selection problem is to learn a good approximation to p∗(·) using only
the observed data y. This problem is intractable unless some assumptions are made about the unknown
distribution p∗(·). The assumption made in this chapter is that p∗(·) can be adequately approximated
by one of the distributions from a set of parametric models p(y|θ; γ ) where γ ∈ ⊆N deﬁnes the
model structure and the parameter vector θ = (θ1, . . . , θk)′ ∈γ ⊂Rk indexes a particular distribution
from the model γ . The dimensionality of θ will always be clear from the context. As an example, if
we are interested in inferring the order of a polynomial in a linear regression problem, the set  may
include ﬁrst-order, second-order and third-order polynomials. The parameter vector θ then represents
the coefﬁcients for a particular polynomial; for example, the intercept and linear term for the ﬁrst-order
model. The model selection problem is to infer an appropriate model structure γ and parameter vector
θ that provide a good approximation to the data generating distribution p∗(·).
This chapter largely focuses on the inference of the model structure γ . A common approach to
estimating the parameters θ for a given model γ is the method of maximum likelihood. Here, the
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00025-5
© 2014 Elsevier Ltd. All rights reserved.
1415

1416
CHAPTER 25 A Tutorial on Model Selection
parameter vector is chosen such that the probability of the observed data y is maximized, that is
ˆθ(y; γ ) = arg max
θ∈γ
p(y|θ; γ )
(25.1)
where p(y|θ; γ ) denotes the likelihood of the data under the distribution indexed by θ. This is a powerful
approach to parameter estimation that possesses many attractive statistical properties [1]. However, it
is not without its ﬂaws, and as part of this chapter we will examine an alternative method of parameter
estimation that should sometimes be preferred to maximum likelihood (see Section 1.25.4.2). The
statistical theory underlying model selection is often abstract and difﬁcult to understand in isolation,
and it is helpful to consider these concepts within the context of a practical example. To this end, we
have chosen to illustrate the application of the model selection criteria discussed in this chapter to the
important and commonly used linear regression model.
1.25.1.1 Nested and non-nested sets of candidate models
The structure of the set of candidate models  can have a signiﬁcant effect on the performance of model
selection criteria. There are two general types of candidate model sets: nested sets of candidate models,
and non-nested sets of candidate models. Nested sets of candidate models have special properties that
make model selection easier and this structure is often assumed in the derivation of model selection
criteria, such as in the case of the distance based methods of Section 1.25.2. Let us partition the model set
 = {0, 1, 2, . . .}, where the subsets k are the set of all candidate models with k free parameters.
A candidate set  is considered nested if: (1) there is only one candidate model with k parameters
(|k| = 1 for all k), and (2) models γ ∈k with k parameters can exactly represent all distributions
indexed by models with less than k parameters. That is, if γk ∈ is a model with k free parameters, and
model γl ∈ is a model with l free parameters, where l < k, then
∀θl ∈l, ∃θk ∈k : ∀y, ∀p(y|θl; γl) = p(y|θk; γk).
A canonical example of a nested model structure is the polynomial order selection problem. Here, a
second-order model can exactly represent any ﬁrst-order model by setting the quadratic term to zero.
Similarly, a third-order model can exactly represent any ﬁrst-order or second-order model by setting
the appropriate parameters to zero, and so on. In the non-nested case, there is not necessarily only a
single model with k parameters, and there is no requirement that models with more free parameters can
exactly represent all simpler models. This can introduce difﬁculties for some model selection criteria,
and it is thus important to be aware of the structure of the set  before choosing a particular criterion
to apply. The concept of nested and non-nested model sets will be made clearer in the following linear
regression example.
1.25.1.2 Example: the linear model
Linear regression is of great importance in engineering and signal processing as it appears in a wide
range of problems such as smoothing a signal with polynomials, estimating the number of sinusoids
in noisy data and modeling linear dynamic systems. The linear regression model for explaining data y
assumes that the means of the observations are a linear combination of q( ≥0) measured variables, or

1.25.1 Introduction
1417
covariates, that is,
y = Xβ + ε,
(25.2)
where X = (x1, . . . , xq) is the full design matrix, β ∈Rq are the parameter coefﬁcients and ε =
(ε1, . . . , εn)′ are independent and identically distributed Gaussian variates εi ∼N(0, τ). Model selec-
tion in the linear regression context arises because it is common to assume that only a subset of the
covariates is associated with the observations; that is, only a subset of the parameter vector is non-
zero. Let γ ⊆{1, . . . , q} denote an index set determining which of the covariates comprise the design
submatrix Xγ ; the set of all candidate subsets is denoted by . The linear model indexed by γ is then
y = Xγ β + ε
(25.3)
where β is the parameter vector of size |γ | and
Xγ = (xγ1, . . . , xγk).
The total number of unknown parameters to be estimated for model γ , including the noise variance
parameter, is k = |γ | + 1. The negative log-likelihood for the data y is
−log p(y|Xγ , β, τ; γ ) = n
2 log 2π + n
2 log τ + 1
2τ (y −Xγ β)′(y −Xγ β).
(25.4)
The maximum likelihood estimates for the coefﬁcient β and the noise variance τ are
ˆβ(y; γ ) =

X′
γ Xγ
−1
X′
γ y,
(25.5)
ˆτ(y; γ ) = 1
n (y −Xγ ˆβ(y; γ ))′(y −Xγ ˆβ(y; γ )),
(25.6)
which coincide with the estimates obtained by minimising the sum of squared errors (the least squares
estimates). The negative log-likelihood evaluated at the maximum likelihood estimates is given by
−log p(y|Xγ , ˆβ(y; γ ), ˆτ(y; γ ); γ ) = n
2 log 2π + n
2 log ˆτ(y; γ ) + n
2.
(25.7)
The structure of the set  in the context of the linear regression model depends on the nature and
interpretation of the covariates. In the case that the covariates are polynomials of increasing order, or
sinusoids of increasing frequency, it is often convenient to assume a nested structure on . For example,
let us assume that the q = 4 covariates are polynomials such that x1 is the zero-order term (constant),
x2 the linear term, x3 the quadratic term and x4 the cubic term. To use a model selection criterion to
select an appropriate order of polynomial, we can form the set of candidate models  as
 = {1, 2, 3, 4},
where
1 = {1},
2 = {1, 2},
3 = {1, 2, 3},
4 = {1, 2, 3, 4}.
From this structure it is obvious that there is only one model with k free parameters, for all k, and
that models with more parameters can exactly represent all models with less parameters by setting

1418
CHAPTER 25 A Tutorial on Model Selection
the appropriate coefﬁcients to zero. In contrast, consider the situation in which the covariates have no
natural ordering, or that we do not wish to impose an ordering; for example, in the case of polynomial
regression, we may wish to determine which, if any, of the individual polynomial terms are associated
with the observations. In this case there is no clear way of imposing a nested structure, as we cannot
a priori rule out any particular combination of covariates being important. This is usually called the
all-subsets regression problem, and the candidate model set then has the following structure
0 = {∅},
1 = {{1}, {2}, {3}, {4}},
2 = {{1, 2}, {1, 3}, {1, 4}, {2, 3}, {2, 4}, {3, 4}},
3 = {{1, 2, 3}, {1, 2, 4}, {1, 3, 4}, {2, 3, 4}},
4 = {1, 2, 3, 4},
where ∅denotes the empty set, that is, none of the covariates are to be used. It is clear that models
with more parameters can not necessarily represent all models with less parameters; for example, the
model {1, 3} cannot represent the model {2} as it is lacking covariate x2. Further, there is now no longer
just a single model with k free parameters; in the above example, there are four models with one free
parameter, and six models with two free parameters. In general, if there are q covariates in total, the
number of models with k free parameters is given by
 q
k

, which may be substantially greater than one
for moderate q.
Throughout the remainder of this chapter, the linear model example will be used to demonstrate the
practical application of the model selection criteria that will be discussed.
1.25.2 Minimum distance estimation criteria
Intuitively, model selection criteria can be derived by quantifying “how close” the probability density
of the ﬁtted model is to the unknown, generating distribution. Many measures of distance between two
probability distributions exist in the literature. Due to several theoretical properties, the Kullback-Leibler
divergence [2], and its variants, is perhaps the most frequently used information theoretic measure of
distance.
As the distribution p∗(·) that generated the observations y is unknown, the basic idea underlying the
minimum distance estimation criteria is to construct an estimate of how close the ﬁtted distributions
are to the truth. In general, these estimators are constructed to satisfy the property of unbiasedness,
at least for large sample sizes n. In particular, the distance-based methods we examine are the well
known Akaike information criterion (AIC) (see Section 1.25.2.1) and symmetric Kullback information
criterion (KIC) (see Section 1.25.2.2), as well as several corrected variants that improve the performance
of these criteria when the sample size is small.
1.25.2.1 The Akaike information criterion
Recall the model selection problem as introduced in Section 1.25.1. Suppose a collection of n observa-
tions y = (y1, . . . , yn)′ has been sampled from an unknown distribution P∗(y) having density function

1.25.2 Minimum Distance Estimation Criteria
1419
p∗(y). Estimation of p∗(y) is done within a set of nested candidate models  ⊆N, characterized by
probability densities p(y|θ; γ ), γ ∈, where θ ∈γ ⊆Rk and k = dim(γ ) is the number of free
parameters possessed by model γ . Let ˆθ(y; γ ) denote the vector of estimated parameters obtained by
maximizing the likelihood p(y|θ; γ ) over γ , that is,
ˆθ(y; γ ) = arg max
θ∈γ
{p(y|θ; γ )}
and let p(y|ˆθ(y; γ ); γ ) denote the corresponding ﬁtted model. To simplify notation, we introduce
the shorthand notation ˆθγ ≡ˆθ(y; γ ) to denote the maximum likelihood estimator, and p(y|ˆθγ ) ≡
p(y|ˆθ(y; γ ); γ ) to denote the maximized likelihood for model γ .
To determine which candidate density model best approximates the true unknown model p∗(y),
we require a measure which provides a suitable reﬂection of the separation between p∗(y) and an
approximating model p(y|ˆθγ ). The Kullback-Leibler divergence, also known as the cross-entropy, is
one such measure. For the two probability densities p∗≡p∗(x) and pθγ ≡p(x|θ; γ ), the Kullback-
Leibler divergence, 	n(·), between p∗(x) and p(x|θ; γ ) with respect to p∗(x) is deﬁned as
2	n(p∗, pθγ ) = Ex∼p∗

2 log
p∗(x)
p(x|θ; γ )

= Ex∼p∗
−2 log p(x|θ; γ )
	
−Ex∼p∗
−2 log p∗(x)
	
= dn(p∗, pθγ ) −dn(p∗, p∗),
where
dn(p∗, pθγ ) = Ex∼p∗
−2 log p(x|θ; γ )
	
,
(25.8)
and Ex∼p∗[·] represents the expectation with respect to p∗(x), that is, x ∼p∗(·). Ideally, the selection
of the best model from the set of candidate models  would be done by choosing the model with the
minimum KL divergence from the data generating distribution p∗(·), that is
ˆγ = arg min
γ ∈

	n

p∗, pˆθγ

,
where ˆθγ is the maximum likelihood estimator of the parameters of model γ based on the observed
data y. Since dn(p∗, p∗) does not depend on the ﬁtted model ˆθγ , any ranking of the candidate models
according to 	n(p∗, pˆθγ ) is equivalent to ranking the models according to dn(p∗, pˆθγ ). Unfortunately,
evaluating dn(p∗, pγ ) is not possible since doing so requires the knowledge of p∗(·) which is the aim
of model selection in the ﬁrst place.
As noted by Takeuchi [3], twice the negative maximized log-likelihood, −2 log p(y|ˆθγ ), acts as a
downwardly biased estimator of dn(p∗, pˆθγ ) and is therefore not suitable for model comparison by
itself. An unbiased estimate of the KL divergence is clearly of interest and could be used as a tool for
model selection. It can be shown that the bias in estimation is given by
Ey∼p∗

Ex∼p∗

−2 log p(x|ˆθ(y; γ ); γ )

−Ey∼p∗

−2 log p(y|ˆθ(y; γ ); γ )

,
(25.9)

1420
CHAPTER 25 A Tutorial on Model Selection
which, under suitable regularity conditions can be asymptotically estimated by [4,5]
2Tr(−1(θ0; γ )(θ0; γ )),
(25.10)
where
(θ0; γ ) = −Ex∼p∗
∂2 log p(x|θ; γ )
∂θ∂θ′

θ=θ0
,
(25.11)
(θ0; γ ) = Ex∼p∗
∂log p(x|θ; γ )
∂θ
 ∂log p(x|θ; γ )
∂θ
′
θ=θ0
,
(25.12)
and Tr(·) denotes the trace of a matrix. The parameter vector θ0 deﬁned by
θ0 = arg min
θ∈γ

	n(p∗, pθ)

(25.13)
indexes the distribution in the model γ that is closest to the data generating distribution p∗(·) in terms
of KL divergence. Unfortunately, the parameter vector θ0 is unknown and so one cannot compute the
required bias adjustment. However, Takeuchi [3] noted that under suitable regularity conditions, the
maximum likelihood estimate can be used in place of θ0 to derive an approximation to (25.10), leading
to the Takeuchi Information Criterion (TIC)
TIC(γ ; y) = −2 log p(y|ˆθγ ) + 2Tr(−1(y; γ )(y; γ )),
(25.14)
where
(y; γ ) = −∂2 log p(y|θ; γ )
∂θ∂θ′

θ=ˆθγ
,
(25.15)
(y; γ ) =
n

i=1
∂log p(yi|θ; γ )
∂θ

θ=ˆθγ
 ∂log p(yi|θ; γ )
∂θ

θ=ˆθγ
′
(25.16)
are empirical estimates of the matrices (25.11) and (25.12), respectively. To use the TIC for model
selection, we choose the model γ ∈ with the smallest TIC score, that is
ˆγTIC(y) = arg min
γ ∈{TIC(γ ; y)}.
The model with the smallest TIC score corresponds to the model we believe to be the closest in terms of
KL divergence to the data generating distribution p∗(·). Under suitable regularity conditions, the TIC is
an unbiased estimate of the KL divergence between the ﬁtted model pˆθγ and the data generating model
p∗(·), that is
Ey∼p∗
TIC(γ ; y)
	
= Ey∼p∗

dn(p∗, pˆθγ )

+ o(1),
where o(1) is a quantity that vanishes as n →∞. The empirical matrices (25.15) and (25.16) are a
good approximation to (25.11) and (25.12) only if the sample size n is very large. As this is often not
the case in practice, the TIC is rarely used.

1.25.2 Minimum Distance Estimation Criteria
1421
The dependence of the TIC on the empirical matrices can be avoided if one assumes that the data
generating distribution p∗(·) is a distribution contained in the model γ . In this case, the matrices (25.11)
and (25.12) coincide. Thus, Tr(−1(θ0; γ ) (θ0; γ )) reduces to the number of parameters k, and we
obtain the widely known and extensively used Akaike’s Information Criterion (AIC) [6]
AIC(γ ; y) = −2 log p(y|ˆθγ ) + 2k.
(25.17)
An interesting way to view the AIC (25.17) is as a penalized likelihood criterion. For a given model
γ , the negative maximized log-likelihood measures how well the model ﬁts the data, while the “2k”
penalty measures the complexity of the model. Clearly, the complexity is an increasing function of the
number of free parameters, and this acts to naturally balance the complexity of a model against its ﬁt.
Practically, this means that a complex model must ﬁt the data well to be preferred to a simpler model
with a similar quality of ﬁt.
Similar to the TIC, the AIC is an asymptotically unbiased estimator of the KL divergence between
the generating distribution p∗(·) and the ﬁtted distribution pˆθγ , that is
Ey∼p∗
AIC(γ ; y)
	
= Ey∼p∗

dn(p∗, pˆθγ )

+ o(1).
When the sample size is large and the number of free parameters is small relative to the sample size,
the o(1) term is approximately zero, and the AIC offers an excellent estimate of the Kullback-Leibler
divergence. However, in the case that n is small, or k is large relative to n, the o(1) term may be quite
large, and the AIC does not adequately correct the bias, making it unsuitable for model selection.
To address this issue, Hurvich and Tsai [7] proposed a small sample correction to the AIC in the
linear regression setting. This small sample AIC, referred to as AICc, is given by
AICc(γ ; y) = −2 log p(y, ˆθγ ) +
2kn
n −k −1
(25.18)
for a model γ with k free parameters, and has subsequently been applied to models other than linear
regressions. From (25.18) it is clear that the penalty term is substantially larger than 2k when the sample
size n is not signiﬁcantly larger than k, and that as n →∞the AICc is equivalent to the regular
AIC (25.17). Practically, a large body of empirical evidence strongly suggests that the AICc performs
signiﬁcantly better as a model selection tool than the AIC, largely irrespective of the problem. The
approach taken by Hurvich and Tsai to derive their AICc criterion is not the only way of deriving small
sample minimum distance estimators based on the KL divergence; for example, the work in [8] derives
alternative AIC-like criteria based on variants of the KL divergence, while [9] offers an alternative small
sample criterion obtained through bootstrap approximations.
For the reader interested in the theoretical details and complete derivations of the AIC, including all
regularity conditions, we highly recommend the excellent text by Linhart and Zucchini [5]. We also
recommend the text by McQuarrie and Tsai [10] for a more practical treatment of AIC and AICc.
1.25.2.2 The Kullback information criterion
Since the Kullback-Leibler divergence is an asymmetric measure, an alternative directed divergence
can be obtained by reversing the roles of the two models in the deﬁnition of the measure. A undirected

1422
CHAPTER 25 A Tutorial on Model Selection
measure of model dissimilarity can be obtained from the sum of the two directed divergences. This
measure is known as Kullback’s symmetric divergence, or J-divergence [11]. Since the J-divergence
combines information about model dissimilarity through two distinct measures, it functions as a gauge
of model disparity which is arguably more sensitive than either of its individual components. The
J-divergence, Jn(·), between the true generating distribution p∗(·) and a distribution pθγ is given by
2Jn(p∗, pθγ ) = 2 	n(p∗, pθγ ) + 2 	n(pθγ , p∗)
= dn(p∗, pθγ ) −dn(p∗, p∗) + dn(pθγ , p∗) −dn(pθγ , pθγ ),
where
dn(pθγ , pθγ ) = Ex∼pθγ

−2 log p(x|θ; γ )
	
,
(25.19)
dn(pθγ , p∗) = Ex∼pθγ

−2 log p∗(x)
	
.
(25.20)
As in Section (1.25.2.1), we note that the term dn(p∗, p∗) does not depend on the ﬁtted model θγ and
may be dropped when ranking models. The quantity
Kn(p∗, pγ ) = dn(p∗, pθγ ) + dn(pθγ , p∗) −dn(pθγ , pθγ )
may then be used as a surrogate for the J-divergence, and leads to an appealing measure of dissimilarity
between the generating distribution and the ﬁtted candidate model. In a similar fashion to the AIC,
twice the negative maximized log-likelihood has been shown to be a downwardly biased estimate of
the J-divergence. Cavanaugh [12] derives an estimate of this bias, and uses this correction to deﬁne a
model selection criterion called KIC (symmetric Kullback information criterion), which is given by
KIC(γ ; y) = −2 log p(y|ˆθγ ) + 3k.
(25.21)
Comparing (25.21) to the AIC (25.17), we see that the KIC complexity penalty is slightly greater. This
implies that the KIC tends to prefer simpler models (that is, those with less parameters) than those
selected by minimising the AIC. Under suitable regularity conditions, the KIC satisﬁes
Ey∼p∗
KIC(γ ; y)
	
= Ey∼p∗
Kn(p∗, pθγ )
	
+ o(1).
Empirically, the KIC has been shown to outperform AIC in large sample linear regression and autore-
gressive model selection, and tends to lead to less overﬁtting than AIC. Similarly to the AIC, when
the sample size n is small the bias correction term 3k is too small, and the KIC performs poorly as a
model selection tool. Seghouane and Bekara [13] derived a corrected KIC, called KICc, in the context
of linear regression that is suitable for use when the sample size is small relative to the total number of
parameters k. The KICc is
KICc(γ ; y) = −2 log p(y|ˆθγ ) +
2kn
n −k −1 −nψ
n −k −1
2

+ n log n
2,
(25.22)
where ψ(·) denotes the digamma function. The digamma function can be difﬁcult to compute, and
approximating the digamma function by a second-order Taylor series expansion yields the so called
AKICc, which is given by
AKICc(γ ; y) = −2 log p(y|ˆθγ ) + k(3n −k −1)
n −k −1
+
k −1
n −k −1.
(25.23)

1.25.2 Minimum Distance Estimation Criteria
1423
The second term in (25.22) appears as the complexity penalty in the AICc (25.18), which is not surprising
given that the J-divergence is a sum of two KL divergence functions. Empirically, the KICc has been
shown to outperform the KIC as a model selection tool when the sample size is small.
1.25.2.3 Example application: linear regression
We now demonstrate the application of the minimum distance estimation criteria to the problem of
linear regression introduced in Section 1.25.1.2. Recall that in this setting, γ speciﬁes which covariates
from the full design matrix X should be used in the ﬁtted model. The total number of parameters is
therefore k = |γ | + 1, where the additional parameter accounts for the fact that we must estimate the
noise variance. The various model selection criteria are listed below:
AIC(γ ; y) = n log (2π ˆτ(y; γ )) + n + 2k,
(25.24)
AICc(γ ; y) = n log (2π ˆτ(y; γ )) + n +
2kn
n −k −1,
(25.25)
KIC(γ ; y) = n log (2π ˆτ(y; γ )) + n + 3k,
(25.26)
KICc(γ ; y) = n log (2π ˆτ(y; γ )) + n +
2kn
n −k −1 −nψ
n −k −1
2

.
(25.27)
It is important to stress that all these criteria are derived within the context of a nested set of candidate
models  (see Section 1.25.1.1). In the next section we will brieﬂy examine the problems that can arise
if these criteria are used in situations where the candidates are not nested.
1.25.2.4 Consistency and efﬁciency
Let γ ∗denote the model that generated the observed data y and assume that γ ∗is a member of the set
of candidate models  which is ﬁxed and does not grow with sample size n; recall that the model γ ∗
contains the data generating distribution p∗(·) as a particular distribution. Furthermore, of all the models
that contain p∗(·), the model γ ∗has the least number of free parameters. A model selection criterion is
consistent if the probability of selecting the true model γ ∗approaches one as the sample size n →∞.
None of the distance based criteria examined in this chapter are consistent model selection procedures.
This means that even for extremely large sample sizes, there is a non-zero probability that these distance
based criteria will overﬁt and select a model with more free parameters than γ ∗. Consequently, if the
aim of the experiment is to discover the true model, the aforementioned distance based criteria are
inappropriate. For example, in the linear regression setting, if the primary aim of the model selection
exercise is to determine only those covariates that are associated with the observations, one should not
use the AIC or KIC (and their variants).
In contrast, if the true model is of inﬁnite order, which in many settings may be deemed more realistic
in practice, then the true model lies outside the class of candidate models. In this case, we cannot hope
to discover the true model, and instead would like our model selection criterion to at least provide good
predictions about future data arising from the same source. A criterion that minimizes the one-step
mean squared error between the ﬁtted model and the generating distribution p∗(·) with high probability
as n →∞is termed efﬁcient. The AIC and KIC, and their corrected variants are all asymptotically
efﬁcient [4]. This suggests that for large sample sizes, model selection by distance based criteria will
lead to adequate prediction errors even if the true model lies outside the set of candidate models .

1424
CHAPTER 25 A Tutorial on Model Selection
1.25.2.5 The AIC and KIC for non-nested sets of candidate models
The derivations of the AIC and KIC, and their variants, depend on the assumption that the candidate
model set  is nested. We conclude this section by brieﬂy examining the behavior of these criteria when
they are used to select a model from a non-nested candidate model set. First, consider the case that 
forms a nested set of models, and let γ ∗∈ be the true model, that is, the model that generated the data;
this means that γ ∗is the model in  with the least number of free parameters, say k∗, that contains the
data generating distribution p∗(·) as a particular distribution. If n is sufﬁciently large, the probability
that the model selected by minimising the AIC will have k∗+ 1 parameters is at least 16%, with the
equivalent probability for KIC being approximately 8% [10].
Consider now the case in which  forms a non-nested set of candidate models. A recent paper by
Schmidt and Makalic [14] has demonstrated that in this case, the AIC is a downwardly biased estimator
of the Kullback-Leibler divergence even asymptotically in n, and this downward bias is determined by
the size of the sets k. This implies, that in the case of all-subsets regression, the probability of overﬁtting
is an increasing function of the number of covariates under consideration, q, and this probability cannot
be reduced even by increasing the sample size.
As an example of how great this probability may be, we consider an all-subsets regression in which
the generating distribution is the null model; that is, none of the measured covariates xi are associated
with the observations y. Even if there is only as few as q = 10 covariates available for selection, the
probability of erroneously preferring a non-null model to the null model using the AIC is approximately
81%, and by the time q = 50, this probability rises to 99.9%. Although [14] examines only the AIC,
similar arguments follow easily for the KIC. It is therefore recommended that while both criteria are
appropriate for nested model selection, neither of these distance based criteria should be used for model
selection in a non-nested situation, such as the all-subsets regression problem.
1.25.2.6 Applications
Minimum distance estimation criteria have been widely applied in the literature, and we present below
an inexhaustive list of successful applications:
•
univariate linear regression models [7,13],
•
multivariate linear regression models with arbitrary noise covariance matrices [15,16],
•
autoregressive model selection [7,12,17],
•
selection of smoothing parameters in nonparametric regression [18],
•
signal denoising [19,20],
•
selection of the size of a multilayer perceptron network [21].
•
model selection in the presence of missing data [22].
For details of many of these applications the reader is referred to [10].
1.25.3 Bayesian approaches to model selection
Section 1.25.2 has described model selection criteria that are designed to explicitly minimise the distance
between the estimated models and the true model that generated the observed data y. An alternative

1.25.3 Bayesian Approaches to Model Selection
1425
approach, based on Bayesian statistics [23], is now discussed. The Bayesian approach to statistical anal-
ysis differs from the “classical” approach through the introduction of a prior distribution that is used to
quantify an experimenter’s a priori (that is, before the observation of data) beliefs about the source of the
observeddata.AcentralquantityinBayesiananalysisistheso-calledposteriordistribution;givenaprior
distribution, πθ(θ|γ ), over the parameter space θ ∈γ , and the likelihood function, p(y|θ, γ ), the pos-
terior distribution of the parameters given the data may be found by applying Bayes’ theorem, yielding
p(θ|y, γ ) = p(y|θ, γ )πθ(θ|γ )
p(y|γ )
,
(25.28)
where
p(y|γ ) =

γ
p(y|θ, γ )πθ(θ|γ )dθ
(25.29)
is known as the marginal probability of y for model γ , and is a normalization term required to render
(25.28) a proper distribution. In general, the speciﬁcation of the prior distribution may itself depend
on further parameters, usually called hyperparameters, but for the purposes of the present discussion
this possibility will not be considered. The main strength of the Bayesian paradigm is that it allows
uncertainty about models and parameters to be deﬁned directly in terms of probabilities; there is no need
to appeal to more abstruse concepts, such as the “conﬁdence interval” of classical statistics. However,
there is in general no free lunch in statistics, and this strength comes at a price: the speciﬁcation and
origin of the prior distribution. There are essentially two main schools of thought on this topic: (i) sub-
jective Bayesianism, where prior distributions are viewed as serious expressions of subjective belief
about the process that generated the data, and (ii) objective Bayesianism [24], where one attempts to
express prior ignorance through the use of uninformative, objective distributions, such as the Jeffreys’
prior [11], reference priors [25] and intrinsic priors [26]. The speciﬁcation of the prior distribution is
the basis of most criticism leveled at the Bayesian school, and an extensive discussion on the merits
and drawbacks of Bayesian procedures, and the relative merits of subjective and objective approaches,
is beyond the scope of this tutorial (there are many interesting discussions on this topic in the literature,
see for example, [27,28]). However, in this section of the tutorial, an approach to the problem of prior
distribution speciﬁcation based on asymptotic arguments will be presented.
While (25.28) speciﬁes a probability distribution over the parameter space γ , conditional on the
observed data, it gives no indication of the likelihood of the model γ given the observed data. A common
approach is based on the marginal distribution (25.29). If a further prior distribution, πγ (γ ), over the set
of candidate models γ ∈ is available, one may apply Bayes’ theorem to form a posterior distribution
over the models themselves, given by
p(γ |y) =
p(y|γ )πγ (γ )

γ ∈ p(y|γ )πγ (γ ).
(25.30)
Model selection then proceeds by choosing the model ˆγ (y) that maximizes the probability (25.30), that is
ˆγ (y) = arg max
γ ∈

p(y|γ )πγ (γ )

,
(25.31)
where the normalizing constant may be safely ignored. An interesting consequence of the posterior dis-
tribution (25.30) is that the posterior-odds in favor of some model, γ1, over another model γ0, usually

1426
CHAPTER 25 A Tutorial on Model Selection
called the Bayes factor [29,30], can be found from the ratio of posterior probabilities, that is,
BF(γ1, γ0) = p(y|γ1)πγ (γ1)
p(y|γ0)πγ (γ0).
(25.32)
This allows for a straightforward and highly interpretable quantiﬁcation of the uncertainty in the choice
of any particular model. The most obvious weakness in the Bayesian approach, ignoring the origin of
the prior distributions, is computational. As a general rule, the integral in the deﬁnition of the marginal
distribution (25.29), does not admit a closed-form solution and one must resort to numerical approaches
or approximations. The next section will discuss a criterion based on asymptotic arguments that circum-
vents both the requirement to specify an appropriate prior distribution as well as the issue of integration
in the computation of the marginal distribution.
1.25.3.1 The Bayesian information criterion (BIC)
The Bayesian Information Criterion (BIC), also sometimes known as the Schwarz Information Criterion
(SIC), was proposed by Schwarz [31]. However, the resulting criterion is not unique and was also derived
from information theoretic considerations by Rissanen [32], as well as being implicit in the earlier work
of Wallace and Boulton [33]; the information theoretic arguments for model selection are discussed
in detail in the next section of this chapter. The BIC is based on the Laplace approximation for high
dimensional integration [34]. Essentially, under certain regularity conditions, the Laplace approximation
works by replacing the distribution to be integrated by a suitable multivariate normal distribution, which
results in a straightforward integral. Making the assumptions that the prior distribution is such that its
effects are “swamped” by the evidence in the data, and that the maximum likelihood estimator converges
on the posterior mode as n →∞, one may apply the Laplace approximation to (25.29) yielding
πγ (γ )

γ
p(y|θ, γ )πθ(θ|γ )dθ ≃(2π)k/2 p(y|ˆθ(y; γ ), γ )πθ(ˆθγ (y; γ )|γ )πγ (γ )
|Jn(ˆθ(y; γ ); γ )|1/2
,
(25.33)
where ≃denotes that the ratio of the left and right hand side approaches one as the sample size n →∞.
In (25.33), k = dim(γ ) is the total number of continuous, free parameters for model γ, ˆθγ (y; γ ) is
the maximum likelihood estimate, and J(·) is the (k × k) expected Fisher information matrix for n data
points, given by
Jn(θ0; γ ) = −Eθ0

∂2 log p(y|θ, γ )
∂θ∂θ′

θ=θ0

.
(25.34)
The technical conditions under which (25.33) holds are detailed in [31]; in general, if the maximum
likelihood estimates are consistent, that is, they converge on the true, generating value of θ as n →∞,
and also satisfy the central limit theorem, the approximation will be satisfactory, at least for large sample
sizes. To ﬁnd the BIC, begin by taking negative logarithms of the right hand side of (25.33)
−log p(y|ˆθ(y; γ ), γ )+ 1
2 log |Jn(ˆθ(y; γ ); γ )|−log πθ(ˆθ(y; γ )|γ )−k
2 log 2π −log πγ (γ ). (25.35)
A crucial assumption made is that the Fisher information satisﬁes
J1(θ; γ ) = lim
n→∞
Jn(θ; γ )
n

,
(25.36)

1.25.3 Bayesian Approaches to Model Selection
1427
where J1(·) is the asymptotic per sample Fisher information matrix satisfying |J1(·)| > 0, and is free of
dependency on n. This assumption is met by a large range of models, including linear regression models,
autoregressive moving-average (ARMA) models and generalized linear models (GLMs) to name a few.
This allows the determinant of the Fisher information matrix for n samples to be rewritten as
|Jn(θ; γ )| ≈nk · |J1(θ; γ )|,
= nk · O(1),
where O(1) denotes a quantity that is constant in n. Using this result, (25.35) may be dramatically
simpliﬁed by assuming that n is large and discarding terms that are O(1) with respect to the sample
size, yielding the BIC
BIC(γ ; y) = −log p(y|ˆθ(y; γ ), γ ) + k
2 log n.
(25.37)
ModelselectionusingBICisthendonebyﬁndingtheγ ∈ thatminimisestheBICscore(25.37),thatis,
ˆγBIC(y) = arg min
γ ∈ {BIC(γ ; y)} .
(25.38)
It is immediately obvious from (25.37) that a happy consequence of the approximations that are
employed is the removal of the dependence on the prior distribution πθ(·). However, as usual, this
comes at a price. The BIC satisﬁes
−log

γ
p(y|θ, γ )πθ(θ|γ )dθ = BIC(γ ; y) + O(1),
(25.39)
where the O(1) term is only guaranteed to be negligible for large n. For any ﬁnite n, this term may
be arbitrarily large, and may have considerable effect on the behavior of BIC as a model selection
tool. Thus, in general, BIC is only appropriate for use when the sample size is quite large relative to
the number of parameters k. However, despite this drawback, when used correctly the BIC has several
pleasant, and important, properties that are now discussed. The important point to bear in mind is that
like all approximations, it can only be employed with conﬁdence in situations that meet the conditions
under which the approximation was derived.
1.25.3.2 Properties of BIC
1.25.3.2.1
Consistency of BIC
A particularly important, and deﬁning, property of BIC is the consistency of ˆγBIC(y) as n →∞. Let
γ ∗∈ be the true model, that is, the model that generated the data; this means that γ ∗is the model
in  with the least number of free parameters that contains the data generating distribution p∗(·) as
a particular distribution. Further, let the set of all models  be independent of the sample size n; this
means that the number of candidate models that are being considered does not increase with increasing
sample size. Then, if all models in  satisfy the regularity conditions under which BIC was derived,
and in particular (25.36), the BIC estimate satisﬁes
Pr( ˆγBIC(y) = γ ∗) →1
(25.40)

1428
CHAPTER 25 A Tutorial on Model Selection
as n →∞[35,36]. In words, this says that the BIC estimate of γ converges on the true, generating
model with probability one in the limit as the sample size n →∞, and this property holds irrespective of
whether  forms a nested or non-nested set of candidate models. Practically, this means that as the sample
sizegrowstheprobabilitythatthemodelselectedbytheminimisingtheBICscoreoverﬁtsorunderﬁtsthe
true, generatingmodel, tends tozero. This is animportant propertyif thediscoveryof relevant parameters
is of more interest than making good predictions, and is not shared by any of the distance-based criteria
discussed in Section 1.25.2. An argument against the importance of model selection consistency stems
from the fact that the statistical models under consideration are abstractions of reality, and that the true,
generating model is not be a member of the set . Regardless of this criticism, empirical studies suggest
that the BIC tends to perform well in comparison to asymptotically efﬁcient criteria such as AIC and
KIC when the underlying generating process may be described by a small number of strong effects
[10], and thus occupies a useful niche in the gallery of model selection techniques.
1.25.3.2.2
Bayesian Interpretations of BIC
Due to the fact that the BIC is an approximation of the marginal probability of data y (25.29), it admits
the full range of Bayesian interpretations. For example, by computing the BIC scores for two models
γ0 and γ1, one may compute an approximate negative log-Bayes factor of the form
−log BF(γ1, γ0) ≈BIC(γ0; y) −BIC(γ1; y).
Therefore, the exponential of the negative difference in BIC scores for γ1 and γ0 may be interpreted as
an approximate posterior-odds in favor of γ1. Further, the BIC scores may be used to perform model
averaging. Selection of a single best model from a candidate set is known to be statistically unstable
[37], particularly if the sample size is small and many models have similar criterion scores. Instability in
this setting is deﬁned as the variability in the choice of the single best model under minor perturbations
of the observed sample. It is clear that if many models are given similar criterion scores, then changing
a single data point in the sample can lead to signiﬁcant changes in the ranking of the models. In contrast
to model selection, model averaging aims to improve predictions by using a weighted mixture of all the
estimated models, the weights being proportional to the strength of evidence for the particular model.
By noting that the BIC score is approximately equal to the negative log-posterior probability of the
model, a Bayesian predictive density for future data y1, conditional on an observed sample y0, may be
found by marginalising out the model class indicator γ , that is,
p(y1|y0) ≈

γ ∈ p(y1|ˆθ(y0; γ ), γ )e−BIC(γ ;y0)

γ ∈ e−BIC(γ ;y0)
.
(25.41)
Although such a mixture often outperforms a single best model when used to make predictions about
future data [38], there are several drawbacks to the model averaging approach. The ﬁrst is that the
resulting density lacks the property of parsimonity, as no model is excluded from the mixture, and
therefore all model parameters contribute to the mixture. In the case of linear regression, for example,
this means the ability to decide whether particular covariates are “relevant” is lost. The second drawback
is that the predictive density (25.41) is in general not of the same form as any its component models,
which can lead to interpretability issues.

1.25.3 Bayesian Approaches to Model Selection
1429
1.25.3.3 Example application: linear regression
We conclude our examination of the BIC with an example of an application to linear regression models.
Recalling that in this context the model indicator γ ⊆{1, . . . , q} speciﬁes which covariates, if any,
should be used from the full design matrix X ∈R(n×q) to explain the observed samples y. The BIC
score for a particular covariate set γ ∈ is given by
BIC(γ ; y) = n
2 log (2π ˆτ(y; γ )) + n
2 +
k + 1
2

log n,
(25.42)
where ˆτ(·; γ ) is the maximum likelihood estimator of the noise variance for model γ , and the (k + 1)
accounts for the fact that the variance must be estimated from the data along with the k regression
parameters. The BIC score (25.42) contains the terms (1/2)(n + log n) which are constant with respect
to γ and may be omitted if the set of candidate models only consists of linear regressions; if the set 
also contains models from alternative classes, such as artiﬁcial neural networks, then these terms are
required to fully characterize the marginal probability in comparison to this alternative class of models.
The consistency property of BIC is particularly useful in this setting, as it guarantees that if the data
were generated from (25.42) then as n →∞, minimising the BIC score will recover the true model,
and thus determine exactly which covariates are associated with y.
Given the assumption of Gaussian noise, the Bayesian mixture distribution is a weighted mixture of
Gaussian distributions and thus has a particularly simple conditional mean. Let α(γ ) denote a (q × 1)
vector with entries
αi(γ ) =

ˆβ( j:γ j=i)(y; γ )
i ∈γ
0
otherwise ,
that is, the vector α(γ ) contains the maximum likelihood estimates for the covariates in γ , and zeros for
those covariates that are not in γ . Then, the conditional mean of the predictive density for future data
with design matrix X1, conditional on an observed sample y0, is simply a linear combination of the α(γ )
E

y1|X1, y0
	
= X1

γ ∈α(γ )e−BIC(γ ;y0)
,
which is essentially a linear regression with a special coefﬁcient vector. While this coefﬁcient vector
will be dense, in the sense that none of its entries will be exactly zero, it retains the high degree of
interpretibility that makes linear regression models so attractive.
1.25.3.4 Priors for the model structure γ
One of the primary assumptions made in the derivation of the BIC is that the effects of the prior
distribution for the model, πγ (·), can be safely neglected. However, in some cases the number of models
contained in  is very large relative to the sample size, or may even grow with growing sample size,
and this assumption may no longer be valid. An important example is that of regression models in the
context of genetic datasets; in this case, the number of covariates is generally several orders of magnitude
greater than the number of samples. In this case, it is possible to use a modiﬁed BIC of the form
BIC(γ ; y) = −log p(y|ˆθ(y; γ ), γ ) + k
2 log n −log πγ (γ ),
(25.43)

1430
CHAPTER 25 A Tutorial on Model Selection
which requires speciﬁcation of the prior distribution πγ (·). In the setting of regression models, there are
two general ﬂavors of prior distributions for γ depending on the structure represented by . The ﬁrst
case is that of nested model selection. A nested model class has the important property that all models
with k parameters contain all models with less than k parameters as special cases. A standard example
of this is polynomial regression in which the model selection criterion must choose the order of the
polynomial. A uniform prior over  is an appropriate, and standard, choice of prior for nested model
classes. Use of such a prior in (25.43) inﬂates the BIC score by a term of log ||; this term is constant
for all γ , and may consequently be ignored when using the BIC scores to rank the models.
In contrast, the second general case, known as all-subsets regression, is less straightforward. In this
setting, there are q covariates, and  contains all possible subsets of {1, . . . , q}. This implies that the
model class is no longer nested. It is tempting to assume a uniform prior over the members of , as
this would appear to be an uninformative choice. Unfortunately, such a prior is heavily biased towards
subsets containing approximately half of the covariates. To see this, note that  contains a total of
 q
k

subsets that include k covariates. This number may be extremely large when k is close to q/2, even
for moderate q, and thus these subsets will be given a large proportion of the prior probability. For
example, if q = 20 then || ≈106 and
 20
10

≈1.8 × 105. Practically, this means that subsets with
k = 10 covariates are given approximately one-ﬁfth of the total prior probability; in contrast, subsets
with a single covariate receive less than one percent of the prior probability. To address this issue, an
alternative approach is to use a prior which assigns equal prior probability to each set of subsets of k
covariates, for all k, that is,
−log πγ (γ ) = log
 q
|γ |

+ log (q + 1).
(25.44)
This prior (or ones very similar) also arise from both empirical Bayes [39] and information theoretic
arguments [27,40], and have been extensively studied by Scott and Berger [41]. This work has demon-
strated that priors of the form (25.44) help Bayesian procedures overcome the difﬁculties associated
with all-subsets model selection that adversely affect distance based criteria such as AIC and KIC [14].
In fact, Chen and Chen [42] have proposed a generalization of this prior as part of what they call
the “extended BIC.” An important (speciﬁc) result of this work is the proof that using the prior (25.44)
relaxes conditions required for the consistency of the BIC. Essentially, the total number of covariates
may now be allowed to depend on the sample size n in the sense that q = O(nκ), where κ is a constant
satisfying 0 < κ < ∞, implying that || grows with increasing n. Then, under certain identiﬁability
conditions on the complete design matrix X detailed in [42], and assuming that γ ∗∈, the BIC given
by (25.43) using the prior (25.44) almost surely selects the true model γ ∗as n →∞.
1.25.3.5 Markov-Chain Monte-Carlo Bayesian methods
WeconcludethissectionbybrieﬂydiscussingseveralalternativeapproachestoBayesianmodelselection
based on random sampling from the posterior distribution, p(θ|y), which fall under the general umbrella
of Markov Chain Monte Carlo (MCMC) methods [43]. The basic idea is to draw a sample of parameter
values from the posterior distribution, using a technique such as the Metropolis-Hastings algorithm
[44,45] or the Gibbs sampler [46,47]. These techniques are in general substantially more complex than
the information criteria based approaches, both computationally and in terms of implementation, and

1.25.4 Model Selection by Compression
1431
this complexity generally brings with it greater ﬂexibility in the speciﬁcation of prior distributions as
well as less stringent operating assumptions.
The most obvious way to apply MCMC methods to Bayesian model selection is through direct
evaluation of the marginal (25.29). Unfortunately, this is a very difﬁcult problem in general, and the
most obvious approach, the so called harmonic mean estimator, suffers from statistical instability and
convergence problems and should be avoided. In the particular case that Gibbs sampling is possible,
and that the posterior is unimodal, Chib [48] has provided an algorithm to compute the approximate
marginal probability from posterior samples.
An alternative to attempting to directly compute the marginal probability is to view the model
indicator γ as the parameter of interest and randomly sample from the posterior p(γ |y). This allows for
the space of models  to be explored by simulation, and approximate posterior probabilities for each
of the models to be determined by the frequency at which a particular model appears in the sample.
There have been a large number of papers published that discuss this type of approach, and important
contributions include the reversible jump MCMC algorithm of Green [49], the stochastic search variable
selection algorithm [50], the shotgun stochastic search algorithm [51], and an interesting paper from
Casella and Moreno [28] that combines objective Bayesian priors with a stochastic search procedure
for regression models.
Finally, there has been a large amount of recent interest in using regularisation and “sparsity inducing”
priors to combine Bayesian model selection with parameter estimation. In this approach, special priors
over the model parameters are chosen that concentrate prior probability on “sparse” parameter vectors,
that is, parameter vectors in which some of the elements are exactly zero. These methods have been
motivated by the Bayesian connections with non-Bayesian penalized regression procedures such as the
non-negative garotte [52] and the LASSO [53]. A signiﬁcant advantage of this approach is that one needs
only to sample from, or maximize over, the posterior of a single model containing all parameters of inter-
est, and there is no requirement to compute marginal probabilities or to explore discrete model spaces.
This is a rapidly growing area of research, and some important contributions of note include the relevance
vector machine [54], Bayesian artiﬁcial neural networks [55] and the Bayesian LASSO [56,57].
1.25.4 Model selection by compression
This section examines the minimum message length (MML) (see Section 1.25.4.1) and minimum
description length (MDL) (see Section 1.25.4.6) principles of model selection. Unlike the aforemen-
tioned distance based criteria, the MML and MDL principles are based on information theory and data
compression. Informally, given a dataset and a set of candidate models, both MML and MDL advocate
choosing the model that yields the briefest encoding of a hypothetical message comprising the model
and the data. This optimal model is the simplest explanation of the data amongst the set of competing
models. In this fashion, MML and MDL treat codelengths of compressed data as a measure of model
complexity and, therefore, a yardstick for evaluating the explanatory power of competing models. Since
data compression is equivalent to statistical learning of regularity in the data, this is intuitively a sensible
procedure. It is important to note that one does not need to encode the models or the data in order to do
MML and MDL inference. All that is required is to be able to calculate codelengths which can then be
used for model comparison. Before discussing how MML and MDL compute codelengths of models

1432
CHAPTER 25 A Tutorial on Model Selection
and data, a brief discussion of information theory is necessary. For a detailed treatment of information
theory, the interested reader is recommended [58].
To aid in understanding the fundamental information theory concepts, it is helpful to consider the
following hypothetical scenario. There exists a sender who is transmitting a data set (message) to
a receiver over a transmission channel. The receiver does not know the content of the message. The
message, M, is recorded using some alphabet X, such as a subset of the English alphabet X = {a, b, c},
and comprises a sequence of symbols formed from X; for example, the message using X = {a, b, c}
may be M = {bcca}. The transmission channel is noiseless and does not modify transmissions in any
fashion. Without loss of generality, we assume the coding alphabet used on the transmission channel
is the binary alphabet Y = {0, 1}. Prior to sending the message, the sender and the receiver agree on
a suitable codebook which will be used to transmit all messages on this channel. The codebook is a
mapping C : X →Y∗from the source alphabet X to the target alphabet Y∗, where Y∗is the set of all
strings obtained by concatenating elements of Y. Some possible codebooks are:
C1 = {a →0, b →10, c →11} ,
(25.45)
C2 = {a →00, b →01, c →10} ,
(25.46)
C3 = {a →0, b →01, c →001} .
(25.47)
For example, if the codebook C1 (25.45) is used by the sender, the message {bcca} is mapped to the
target code {1011110}. If the sender uses codebook C2 instead, the message is encoded as {01101000}.
The task of the sender and receiver is to choose a codebook such that the transmitted message is uniquely
decodable by the receiver and as brief as possible.
If the codebook C possess the preﬁx property, a message transmitted using C is uniquely decodable
requiring no delimiters between successive symbols. The preﬁx property states that no codeword is a
preﬁx of any other codeword. In the above example, codebooks C1 and C2 possess the preﬁx property,
while codebook C3 does not as the code 0 for source symbol a is a preﬁx to both codes 01 and 001.
Since there exist inﬁnitely many preﬁx codes, the sender and the receiver may choose the preﬁx code
that results in the briefest possible encoding of the message M. Intuitively, symbols in the message
that appear more frequently should be assigned smaller codewords, while symbols that are very rare
can be given longer codewords in order to reduce the average length of the encoding. This optimization
problem is now formally explored. Note, that in the rest of the section we only consider codebooks
which are uniquely decodable and possess the preﬁx property.
Let X denote a discrete random variable deﬁned over the support (source alphabet) X with probabil-
itydistributionfunction pX(·).TherandomvariableXrepresentsthesenderofthemessage.Furthermore,
let l : X →R+ denote the codelength function which gives the codeword length for each symbol in
the source alphabet X. For example, l(a) = 1 bit for the codebook C1, while l(a) = 2 bits, for the
codebook C2. The average codelength per symbol from source alphabet X with probability distribution
function pX(·) is deﬁned as
E

l(X)
	
=

x∈X
pX(x)l(x),
(25.48)
where E denotes the expected value of a random variable. We wish to choose the function l(·) such that
the codelength for data from the random variable X is on average the shortest possible; that is we seek

1.25.4 Model Selection by Compression
1433
a function l(·) that minimises
arg
min
l(x),x∈X{E[l(X)]}.
(25.49)
The solution to this optimization problem is
l(X) = log 1/pX(x),
(25.50)
which is the Shannon information of a random variable [59]. The unit of information is derived from
the base of the logarithm; commonly binary and natural logarithms are used corresponding to the bit
and the nit (nat) units respectively. In the rest of this section, all logarithms are assumed to be natural
logarithms, unless stated otherwise.
The Shannon information agrees with the previous intuition that high probability symbols should be
assigned shorter codewords, while low probability symbols should be given longer code words. At ﬁrst,
it may seem troubling that Shannon information allows for non-integer length codewords. After all, how
does one encode and transmit symbols of length, say, 0.2 of a nit? However, this is not an issue in practice
as there exist sophisticated coding algorithms, such as arithmetic codes [60], that allow for efﬁcient
encoding in the presence of non-integer codewords. It is important to re-iterate here that MML and MDL
model selection does not require any actual encoding of data to be performed; all that is required is a
function to compute codelengths for data and models. Model selection is then performed by computing
codelengths for all candidate models and choosing the one with the smallest codelength as optimal.
The minimum possible average codelength per symbol for a discrete random variable X is known as
the Shannon entropy of X and is deﬁned as
H(X) = E[log 1/pX(X)] =

x∈X
pX(x) log 1/pX(x)
(25.51)
with the convention that 0 log 0 = 0; since x log x →0 as x →0, this is reasonable. The entropy of
a random variable is always non-negative, H(X) ≥0, and is a measure of uncertainty in the random
variable. The maximum entropy is attained when there is maximum uncertainty in X, that is, when X is
uniformly distributed over the support X. For example, the entropy of a random variable X with support
X = {1, 2, 3, 4, 5, 6} and probability distribution function
pX( j) = 1
6
( j = 1, . . . , 6)
(25.52)
is approximately 2.585 bits or 1.792 nits. As an another example, the entropy of random variable X
following a Geometric distribution (X = {0, 1, 2, . . .}) with distribution function
pX(X|λ) = (1 −λ)xλ (0 < λ ≤1)
(25.53)
is given by
H(X|λ) = −1
λ

(1 −λ) log (1 −λ) + λ log λ
	
,
(25.54)
where H(X|λ) = 0 for λ = 1, and H(X|λ) →∞as λ →0.
The entropy of a random variable is the minimum average codelength per symbol with the length of
each codeword X given by l(X) = log 1/pX(·). By the law of large numbers, it can also be shown that

1434
CHAPTER 25 A Tutorial on Model Selection
using codelengths of the form l(X) results in a code that is optimal not only on average, but also optimal
for the actually generated sequences of data. It is of interest to quantify the inefﬁciency resulting
from encoding symbols generated by pX(·) using an alternative distribution function qX(X), where
pX(·) ̸= qX(·) for at least one X ∈X. The extra number of nits (or bits) required per symbol on average
if qX(·) is used instead of pX(·) is given by the Kullback-Leibler (KL) divergence (see Section 1.25.2).
The KL divergence is always non-negative, and is only equal to zero when pX(·) = qX(·), for all
X ∈X. In words, the briefest encoding for data from source pX(·) is achieved using pX(·) as the
coding distribution and any other distribution, say qX(·), will necessarily result in messages with a
longer average codelength.
Recall that both MML and MDL seek models that result in the shortest codelength of a hypothetical
message comprising the observed data and the model. That is, both MML and MDL seek models that
result in the best compression of the observed data. Formally, this is a sensible procedure since the set of
compressible strings from any data source is in fact quite small in comparison to incompressible strings
from the same source. This is quantiﬁed by the following no hypercompression inequality (p. 136 [61]).
Given a sample of n data points y = (y1, . . . , yn) generated from a probability distribution pX(·), the
probability of compressing y by any code is
p

log 1/pX(y1, . . . , yn) ≥log 1/qX(y1, . . . , yn) + K

≤2−K .
(25.55)
In words, the inequality states that the probability of compressing y by K bits using any code is small and
decreases exponentially with increasing K. Clearly, the sender and the receiver would like K to be as large
as possible leading to the briefest encoding of the observed data. For any moderate K, it is highly unlikely
for a random model to result in a compression of K-bits as the number of models that can compress
the data in such a manner is vanishingly small with K. In the next section, we examine model selection
with the minimum message principle which is based on information theory and data compression.
1.25.4.1 Minimum message length (MML)
The Minimum Message Length (MML) principle for model selection was introduced by Wallace [27]
and collaborators in the late 1960s [33], and has been continuously reﬁned in the following years
[62,63]. The MML principle connects the notion of compression with statistical inference, and uses
this connection to provide a uniﬁed framework for model selection and parameter estimation. Recall-
ing the transmitter-receiver framework outlined in Section 1.25.4, consider the problem of efﬁciently
transmitting some observed data y from the transmitter to the receiver. Under the MML principle, the
model that results in the shortest encoding of a decodable message comprising the model and data is
considered optimal. For this message to be decodable, the receiver and the transmitter must agree on a
common language prior to any data being transmitted. In the MML framework, the common knowledge
is a set of parametric models , each comprising a set of distributions p(y|θ, γ ) indexed by θ, and a
prior distribution π(θ, γ ) = πθ(θ|γ )πγ (γ ). The use of a subjective prior distribution means that MML
is an unashamedly Bayesian principle built on information theoretic foundations.
Given a model from γ and a prior distribution πθ(·) over γ we may deﬁne an (implicit) prior
probability distribution, r(y|γ ), on the n-dimensional data space Yn. This distribution r(·), called the
marginal distribution of the data, characterizes the probability of observing any particular data set y
given our choice of prior beliefs reﬂected by πθ(·), and plays an important role in conventional Bayesian

1.25.4 Model Selection by Compression
1435
inference (see Section 1.25.3). The marginal distribution is
r(y|γ ) =

θ∈γ
p(y|θ, γ )πθ(θ|γ )dθ.
(25.56)
The receiver and the transmitter both have knowledge of the prior density πθ(θ|γ ) and the likelihood
function p(y|θ, γ ), and therefore the marginal distribution. Under the assumption that our prior beliefs
accurately reﬂect the unknown process that generated the data y, the average length of the shortest
message that communicates the data y to the receiver using the model γ is the entropy of r(y|γ ). The
marginal distribution is then used to construct a codebook for the data resulting in a decodable message
with codelength
I0(y) = −logr(y|γ ) = −log

θ∈γ
p(y|θ, γ )πθ(θ|γ )dθ.
(25.57)
This is the most efﬁcient encoding of the data using the model γ under the assumed likelihood function
and prior density. While the marginal distribution is commonly used in Bayesian inference for model
selection (as discussed in Section 1.25.3), the fact that the parameter vector θ is integrated out means
that it may not be used to make inferences about the particular distribution in γ that generated the
data. That is, the message does not convey anything about the quality of any particular distribution in
γ in regards to encoding the observed data and cannot be used for point estimation. Following Wallace
([27], pp. 152–153), we shall refer to this code as the non-explanation code for the data.
In the MML framework, the message transmitting the observed data comprises two components,
commonly named the assertion and the detail. The assertion is a codeword naming a particular distri-
bution indexed by θ ∈γ from the model γ , rather than a range of possible distributions in γ . The
detail is a codeword that states the data y using the distribution θ that was named in the assertion. The
total, joint codelength of the message is then
I(y, θ, γ ) = I(θ, γ ) + I(y|θ, γ ),
(25.58)
where I(θ, γ ) and I(y|θ, γ ) denote the length of the assertion and detail respectively. The length
of the assertion measures the complexity of a particular distribution θ in model γ , while the detail
length measures the quality of the ﬁt of the distribution θ to the data y. Thus, the total, joint codelength
automaticallycapturesthetradeoffbetweenmodelcomplexityandmodelcapacity.Intuitively,acomplex
model with many free parameters can be tuned to ﬁt a large range of data sets, leading to a short detail
length. However, since all parameters need to be transmitted to the receiver, the length of the assertion
will be long. In contrast, a simple model with few free parameters requires a short assertion, but may
not ﬁt the data well, potentially resulting in a long detail. This highlights the ﬁrst deﬁning feature of
the MML principle: the measure of model complexity and model ﬁt are both expressed in the same unit,
namely the codelength. The minimum message length principle advocates choosing the model γ and
distribution θ that minimises this two-part message, that is,

ˆθ(y; γ ), ˆγMML(y)

= arg
min
γ ∈, θ∈γ
{I(y, θ, γ )} .
This expression highlights the second deﬁning feature of the MML principle: minimization of the
codelength is used to perform both selection of a model structure, γ ∈, as well as estimation of the

1436
CHAPTER 25 A Tutorial on Model Selection
model parameters, θ ∈. This is in contrast to both the distance based criteria of Section 1.25.2, and
the Bayesian information criterion introduced in Section 1.25.3, which only select a structure γ and do
not provide an explicit framework for parameter estimation. It remains to determine how one computes
the codelength (25.58).
In general, while the set of candidate models  will usually be countable,1 the parameter space γ
for a model γ is commonly continuous. The continuity of γ creates a problem for the transmitter when
designing a codebook for the members of γ needed for the assertion, as valid codebooks can only be
constructed from discrete distributions. This problem may be circumvented by quantizing the contin-
uum γ into a discrete, countable subset ¯γ = { ¯θ1, ¯θ2, . . .} ⊂γ . Given a subset ¯γ we can deﬁne
a distribution over ¯γ , say h(·|γ ). This distribution implicitly deﬁnes a codelength for the members
of ¯γ . The assertion length for stating a particular θ ∈¯γ is then I(θ, γ ) = −log h(θ|γ )πγ (γ ). The
length of the detail, encoded using the distribution indexed by θ ∈¯γ is I(y|θ, γ ) = −log p(y|θ, γ ),
which is the negative log-likelihood of the observed data y. The question remains: how do we choose
the quantisation ¯γ and the distribution h(·|γ )? The minimum message length principle speciﬁes that
the quantisation ¯γ and the distribution h(·|γ ) be chosen such that the average codelength of the result-
ing two-part message is minimum, the average being taken with respect to the marginal distribution.
Formally, for a model γ , the MML two-part codebook solves the following minimization problem:
min
¯,h
⎧
⎨
⎩

y∈Yn
r(y|γ )I(y, θ, γ )
⎫
⎬
⎭.
(25.59)
In the literature, this procedure is referred to as strict minimum message length (SMML) [62]. For
computational reasons, the SMML procedure commonly described in the literature solves an equivalent
minimization problem by partitioning the dataspace Yn, which implicitly deﬁnes the parameter space
quantisation ¯γ and the distribution h(·|γ ) (for a detailed exposition the reader is referred to Chapter 3 of
[27]). Model selection and parameter estimation by SMML proceeds by choosing the model/parameter
pair from ¯γ and  that results in the shortest two-part message, given h(·|γ ). Interestingly, this is
equivalent to performing maximum a posteriori (MAP) estimation over the quantised parameter space
¯γ , treating h(·|γ ) as a special “quantised prior” which is implicitly deﬁned by both the quantisation
¯γ ⊂γ and the original continuous prior distribution πθ(·) over γ .
Analysis of the SMML code shows that it is approximately (1/2)( log (kπ) −1) nits longer than
the non explanation code (25.57), where k is the dimensionality of γ (pp. 237–238, [27]). This is
not surprising given that the SMML code makes a statement about the particular distribution from γ
which was used to code the data, rather than using a mixture of distributions as in the non-explanation
code. This apparent “inefﬁciency” in SMML allows the codelength measure to be used not only for
model selection (as in the case of the non-explanation code), but also provides a theoretically sound
basis for parameter estimation.
While the SMML procedure possesses many attractive theoretical properties (see for example,
pp. 187–195 of [27] and the “false oracle” property discussed in [64]), the minimization problem
(25.59) is in general NP-hard [65], and exact solutions are computationally tractable in only a few
1A set A is countable if its members can be put into a one-to-one correspondence with the natural numbers, that is, A ↔N.

1.25.4 Model Selection by Compression
1437
simple cases. The difﬁculties arise primarily due to the fact that a complete quantisation of the con-
tinuous parameter space must be found, which is a non-trivial problem that does not scale well with
increasing dimensionality of Yn. In the next section we shall examine an approximation to the two-part
message length that exploits the fact that we are only interested in computing the length of the codes,
rather than the codebooks. This allows us to circumvent the problematic issue of complete quantisation
by using only local properties of the likelihood and prior distribution, resulting in an implementation
of the MML principle that is applicable to many commonly used statistical models.
1.25.4.2 The Wallace-Freeman message length approximation (MML87)
To circumvent the problem of determining a complete quantisation of the parameter space, Wallace and
Freeman [63] presented a formula that gives an approximate length of the two-message for a particular
θ ∈γ and data set y. Rather than ﬁnding an optimal quantisation for the entire parameter space γ , the
keyideaunderlyingtheirapproachistoﬁndanoptimallocalquantisationfortheparticularθ namedinthe
assertion. The important implication of this approach is that the quantisation, and therefore the resulting
message length formula, depends only on the particular θ that is of interest. Under certain regularity
conditions, the Wallace-Freeman approximation states that the joint codelength of a distribution θ ∈γ
in model γ , and data y, is
I87(y, θ, γ ) = −log πθ(θ|γ )πγ (γ ) + 1
2 log |Jn(θ; γ )| + k
2 log κk
"
#$
%
I87(θ,γ )
+ k
2 −log p(y|θ, γ )
"
#$
%
I87(y|θ,γ )
,
(25.60)
where k = dim(γ ) is the total number of continuous, free parameters for model γ and J(·) is the
Fisher information matrix for n data points given by
Jn(θ0; γ ) = −Eθ0

∂2 log p(y|θ, γ )
∂θ∂θ′

θ=θ0

.
(25.61)
The quantity κk is the normalized second moment of an optimal quantising lattice in k-dimensions
([66]). Following ([27], p. 237), the need to determine κk for arbitrary dimension k is circumvented by
use of the following approximation
k
2( log κk + 1) ≈−k
2 log (2π) + 1
2 log (kπ) + ψ(1) = c(k),
where ψ(·) is the digamma function, and ψ(1) ≈−0.5772. Model selection, and parameter estimation,
is performed by choosing the model/parameter pair that minimises the joint codelength, that is,

ˆγ87(y), ˆθ87(y; ˆγ87(y))

= arg
min
γ ∈, θ∈γ
{I87(y, θ, γ )} .
(25.62)
UnliketheSMMLcodelength,theMML87codelength(25.60)isacontinuousfunctionofthecontinuous
model parameters θ which makes the minimization problem (25.62) considerably easier. Again, it is
important to re-iterate that unlike the distance based criteria (AIC, KIC, etc.) and the BIC, minimization

1438
CHAPTER 25 A Tutorial on Model Selection
of the MML87 formula yields both estimates of model structure as well as model parameters. Further,
the MML87 parameter estimates possess attractive properties in comparison with estimates based on
other principles such as maximum likelihood or maximum posterior mode.
Animportantissueremains:underwhatconditionsistheMML87formulavalid?Inbrief,theMML87
codelength is generally valid if: (i) the maximum likelihood estimates associated with the likelihood
function obey the central limit theorem everywhere in γ , and (ii) the local curvature of the prior
distribution πθ(·) is negligible compared to the curvature of the negative log-likelihood everywhere in
γ . An important point to consider is how close the Wallace–Freeman approximate codelength is to the
exact SMML codelength given by (25.59). If the regularity conditions under which the MML87 formula
was derived are satisﬁed, then the MML87 codelength is, on average, practically indistinguishable from
the exact SMML codelength (pp. 230–231, [27]).
In the particular case that the prior πθ(θ|α, γ ) is conjugate2 with the likelihood p(y|θ, γ ), and
depends on some parameters α (usually referred to as “hyperparameters”), Wallace (pp. 236–237, [27])
suggested a clever modiﬁcation of the MML87 formula that makes it valid even if the curvature of the
prior is signiﬁcantly greater than the curvature of the likelihood. The idea is to ﬁrst propose some imagi-
nary “prior data” yα whose properties depend only on the prior hyperparameters α. It is then possible to
view the prior πθ(θ|α, γ ) as a posterior of the likelihood of this prior data yα and some initial uninfor-
mative prior π0(θ) that does not depend on the hyperparameters α. Formally, we seek the decomposition
πθ(θ|α, γ ) ∝π0(θ)p(yα|θ, γ ),
(25.63)
where p(yα|θ, γ ) is the likelihood of m imaginary prior samples and π0(θ) is an uninformative
prior. The new Fisher Information Jn+m(θ; γ ) is then constructed from the new combined likelihood
p(y, yα|θ, γ ) = p(y|θ)p(yα|θ, γ ), and the “curved prior” MML87 approximation is simply
I ∗
87(y, θ, γ ) = −log πθ(θ|α, γ )πγ (γ ) + 1
2 log |Jn+m(θ; γ )| −log p(y|θ, γ ) + c(k), (25.64)
= −log π0(θ)πγ (γ ) + 1
2 log |Jn+m(θ; γ )| −log p(y, yα|θ, γ ) + c(k).
(25.65)
ThisnewFisherinformationmatrixhastheinterestinginterpretationofbeingtheasymptoticlowerbound
of the inverse of the variance of the maximum likelihood estimator using the combined data (y, yα)
rather than for simply the observed data y. It is even possible in this case to treat the hyperparameters
as unknown, and use an extended message length formula to estimate both the model parameters θ and
the prior hyperparameters α in this hierarchical structure [67,68].
What is the advantage of using the MML87 estimates over the traditional maximum likelihood and
MAP estimates? In addition to possessing important invariance and consistency properties that are
discussed in the next sections, there is a large body of empirical evidence demonstrating the excel-
lent performance of the MML87 estimator in comparison to traditional estimators such as maximum
likelihood and the maximum a posteriori (MAP) estimator, particularly for small to moderate sam-
ple sizes. Examples include the normal distribution [27], factor analysis [69], estimation of multiple
means [67], the von Mises and spherical von Mises-Fisher distribution [70,71] (pp. 266–269, [27])
2The use of a conjugate prior ensures that the posterior distribution of the parameters is of the same mathematical form as
the conjugate prior distribution.

1.25.4 Model Selection by Compression
1439
and autoregressive moving average models [72]. In the next sections we will discuss some important
properties of the Wallace-Freeman approximation. For a detailed treatment of the MML87 approxima-
tion, we refer the interested reader to Chapter 5 of [27].
1.25.4.2.1
Model selection consistency
Recall from Section 1.25.3.2 that the BIC procedure yields a consistent estimate of the true, underlying
model, say γ ∗∈, that generated the data. The estimate ˆγ87 is also a consistent estimate of γ ∗as
n →∞under certain conditions. Assuming that the Fisher information matrix satisﬁes
J1(θ; γ ) = lim
n→∞
Jn(θ; γ )
n

,
(25.66)
with |J1(·)| > 0, the MML87 codelength approximation can be rewritten as
I87(y, θ, γ ) = −log p(y|θ, γ ) + k
2 log n + O(1).
(25.67)
This is clearly equivalent to the well known BIC discussed in Section 1.25.3.1, and thus the estimator
ˆγ87 is consistent as n →∞. For a more rigorous and general proof, the reader is directed to [73].
1.25.4.2.2
Invariance
There is in general no unique way to parameterise the distributions that comprise a statistical model. For
example, the normal distribution is generally expressed in terms of mean and variance parameters, say
(μ, τ). However, the alternate parameterisations (μ, √τ) and (e−μ, τ) are equally valid and there is no
reason, beyond ease of interpretation, to prefer any particular parameterisation. A sensible estimation
procedure should be invariant to the parameterisation of the model; that is, it should give the same
answer irrespective of how the inference question is framed. While the commonly used maximum
likelihood estimator possesses the invariance property, both the maximum a posteriori estimator and
posterior mean estimator based on the Bayesian posterior distribution p(θ|y, γ ) given by (25.28) are
not invariant. The SMML and MML87 are invariant under one-to-one reparameterisations, and thus
provide an important alternative Bayesian estimation procedure.
1.25.4.2.3
Parameter estimation consistency
Consider the case that the data y is generated by a particular distribution θ∗from the model γ , that is,
θ∗∈γ . A sequence of estimators, ˆθn, of parameter θ is consistent if and only if
Pr

|ˆθn −θ∗| ≥ε

→0
for any ε > 0 as n →∞. In words, this means that as we accumulate more and more data about
the parameters the estimator converges to the true parameter value θ∗. If we consider the case that the
dimensionality of γ does not depend on n, and the Fisher information matrix satisﬁes (25.36) then the
maximum likelihood estimator is consistent under suitable regularity conditions [74]. The consistency of
the MML87 estimator in this particular case follows by noting that minimising the asymptotic message
length (25.67) is equivalent to maximizing the likelihood; similar arguments can also be used to establish
the consistency of the MAP and mean posterior estimators in this case.

1440
CHAPTER 25 A Tutorial on Model Selection
The situation is strikingly different in the case that the dimensionality of the parameter space depends
on the sample size. In this setting there is usually a small number of parameters of interest for which
information grows with increasing sample size, and a large number of auxiliary “nuisance” parameters
for which an increasing sample size brings no new information. Maximum likelihood estimation, as
well as the MAP and mean posterior estimation, of the parameters of interest is in general inconsistent.
In contrast, the MML87 estimator has been shown to provide consistent estimates in the presence of
many nuisance parameters in several important statistical models, including the Neyman-Scott problem
[75], factor analysis [69,76] (also pp. 297–303 in [27]), multiple cutpoint estimation [77] and mixture
modeling ([78] and pp. 275–297 in [27]). While a general proof of the consistency of the MML principle
in the presence of nuisance parameters does not currently exist, there have been no problems studied so
far in which it has failed to yield consistent estimates.
1.25.4.2.4
Similarities with Bayesian inference
There are many similarities between the minimum message length principle and the more conventional
Bayesian approaches such as those discussed in Section 1.25.3 (see [79] for a detailed discussion). The
differences in codelengths (MML87 or otherwise) between two models may be used to compute an
approximate Bayes factor, that is,
−log BF(γ1, γ0) ≈I(y, θ, γ0) −I(y, θ, γ1).
The usual interpretations applied to regular Bayes factors apply equally well to the approximate Bayes
factors determined by codelength differences. Further, the codelength measure may be used to perform
model averaging over the set of candidate models  in a similar fashion to BIC, as discussed in Section
1.25.3.1. In this case, we use (25.41), replacing the BIC score with the codelength, and (ideally) replacing
the maximum likelihood estimate with the appropriate minimum message length estimate.
1.25.4.3 Other message length approximations
Recently, Schmidt [72,80] introduced a new codelength approximation that can be used for models for
which the application of the Wallace-Freeman approximation is problematic. The new approximation,
calledMML08,ismorecomputationallytractablethanthestrictMMLprocedure,butrequiresevaluation
of sometimes difﬁcult integrals. The joint MML08 codelength of data y and parameters θ, for model γ ,
is given by
I08(θ, y, γ ) = −log πγ (γ )

θ
πθ(φ|γ )dφ
"
#$
%
I08(θ,γ )
−log p(y|θ, γ ) +
&
1
'
θ π(φ|γ )dφ
 
θ
πθ(φ|γ )	(θ||φ)dφ
"
#$
%
I08(y|θ,γ )
,
(25.68)
where 	(·) is the Kullback-Leibler divergence and θ ⊂γ is chosen to minimise the codelength.
The MML08 is a generalization of the previously proposed MMLD codelength approximation [81,82]

1.25.4 Model Selection by Compression
1441
and is invariant under transformations of the parameters. The MML08 approximation has been applied
to the problem of order selection for autoregressive moving average models [72]. Efﬁcient algorithms
to compute approximate MMLD and MML08 codelengths are given in [72,83,84].
1.25.4.4 Example: MML linear regression
We continue our running example by applying the minimum message length principle to the selection
of covariates in the linear regression model. By choosing different prior distributions for the coefﬁ-
cients β we can arrive at many different MML criteria (for example, [85,86] and pp. 270–272, [27]);
however, provided the chosen prior distribution is not unreasonable all codelength criteria will perform
approximately equivalently, particularly for large sample sizes. Two recent examples of MML linear
regression criteria, called “MMLu” and “MMLg,” that do not require the speciﬁcation of any subjective
prior information are given in [68]. The MMLu criterion exploits some properties of the linear model
with Gaussian noise to derive a data driven, proper uniform prior for the regression coefﬁcients. Recall
that in the linear regression case, the model index γ speciﬁes the covariates to be used from the full
design matrix X. For a given γ , the MMLu codelength for a linear regression model is
n −k
2

log 2π +
n −k
2
 
log ˆτ87(y; γ ) + 1

+ k
2 log

πy′y

−log 
k
2 + 1

+ 1
2 log (k + 1) −log πγ (γ ),
(25.69)
where the MMLu parameter estimates are given by
ˆτ87(y; γ ) =

1
n −k

(y′y −R(γ )),
(25.70)
ˆβ87(y; γ ) =

X′
γ Xγ
−1
X′
γ y.
(25.71)
Here, k = |γ | denotes the number of covariates in the regression model γ, πγ (γ ) is a suitable prior for
the model index (see Section 1.25.3.4 for a discussion of suitable priors), and
R(γ ) = ˆβ(y; γ )′(X′
γ Xγ ) ˆβ(y; γ )
(25.72)
is the ﬁtted sum-of-squares for the least-squares estimates (25.5). Due to the use of a uniform prior on
the regression coefﬁcients, and the nature of the Fisher information matrix, the MML87 estimates of
the regression coefﬁcients coincide with the usual maximum likelihood estimates (25.5). In contrast to
the maximum likelihood estimate of τ, given by (25.6), the MML87 estimate is exactly unbiased even
for ﬁnite sample sizes n.
The MMLg criterion further demonstrates the differences in parameter estimation that are possible
by using the MML principle. This criterion is based on a special hierarchical Gaussian prior, and uses
some recent extensions to the MML87 codelength to estimate the parameters of the prior distribution in
addition to the regression coefﬁcients [67]. For a given γ , the MMLg codelength for a linear regression
model is
n −k + 2
2

( log ˆτ87(y; γ )+1)+
k −2
2

log
 R(γ )
δ

+ δ
2+1
2 log (n−k)k2−log πγ (γ ), (25.73)

1442
CHAPTER 25 A Tutorial on Model Selection
where δ = max(1, k −2). This formula is applicable only when k > 0, and m(γ ) > 0, where
m(γ ) =
 R(γ )
δ
−ˆτ87(y; γ )

+
,
and (·)+ = max(0, ·) is the positive-part function. The codelength for a “null” model (k = 0) is given by
n
2

( log ˆτ87(y; γ ) + 1) + 1
2 log (n −1) + 1
2.
(25.74)
We note that (25.73) corrects a minor mistake in the simpliﬁed form of MMLg in [68], which erroneously
excluded the additional “δ/2” term. The MMLg estimates of β and τ are given by
ˆτ87(y; γ ) =

1
n −k + 2

(y′y −R(γ )),
(25.75)
ˆβ87(y; γ ) =

m(γ )
m(γ ) + ˆτ87(y; γ )
 
X′
γ Xγ
−1
X′
γ y,
(25.76)
which are closely related to the unbiased estimate (25.70) of τ used by the MMLu criterion. The MMLg
estimates of the regression coefﬁcients β are the least-squares estimates scaled by a quantity less than
unity which is called a shrinkage factor [87]. This factor depends crucially on the estimated strength
of the noise variance and signal strength, and if the noise variance is too large, m(γ ) = 0 and the
coefﬁcients are completely shrunk to zero (that is, none of the covariates are deemed associated with
the observations). Further, it turns out that the MMLg shrinkage factor is optimal in the sense that for
k ≥3, the shrunken estimates of β will, on average, be better at predicting future data arising from the
same source than the corresponding least squares estimates [88].
1.25.4.5 Applications of MML
The MML principle has been applied to a large number statistical models. Below is an inexhaustive list
of some of the many successful applications of MML; a more complete list may be found in [82].
•
Mixture modeling [33,78,89].
•
Decision trees/graphs [90–92].
•
Casual Networks [93,94].
•
Artiﬁcial neural networks [95,96].
•
Linear Regression [68,85,86,97].
•
Autoregressive moving average (ARMA) models [72,98,99].
•
Discrete time series [77,100].
•
Hierarchical Bayesian Models [67,68].
Detailed derivations of the message length formulas for many of these models, along with discussions
of their behavior, can also be found in Chapters 6 and 7 of [27].
1.25.4.6 The minimum description length (MDL) principle
The minimum description length (MDL) principle [101,102] was independently developed by Jorma
Rissanen from the late 1970s [32,103,104], and embodies the same idea of statistical inference via data

1.25.4 Model Selection by Compression
1443
compression as the MML principle. While there are many subtle, and not so subtle, differences between
MML and MDL, the most important is the way in which they view prior distributions (for a comparison
of early MDL and MML, see [105]). The MDL principle views prior distributions purely as devices
with which to construct codelengths, and the bulk of the MDL research has been focused on ﬁnding
ways in which the speciﬁcation of subjective prior distributions may be avoided. In the MML principle,
the codebook is constructed to minimise the average codelength of data drawn from the marginal
distribution, which captures our prior beliefs about the data generating source. The MDL principle
circumvents the requirement to explicitly specify prior beliefs, and instead appeals to the concept of
minimising the worst-case behavior of the codebook. Formally, we require the notion of coding regret
R(y) = I(y, γ ) + log p(y|ˆθ(y; γ ), γ ),
(25.77)
where I(y, γ ) is the codelength of the data y using model γ , and ˆθ(y; γ ) ∈γ is the maximum like-
lihood estimator. As the maximum likelihood estimator minimises the codelength of the data (without
stating anything about the model), the negative log-likelihood evaluated at the maximum represents
an ideal target codelength for the data. Unfortunately, this is only realizable if the sender and receiver
have a priori knowledge of the maximum likelihood estimator, which in an inferential setting is clearly
nonsensical.
Clearly, one would like to keep the regret as small as possible in some sense, to ensure that the chosen
codebook is efﬁcient. The MML approach minimises average excess codelength over the marginal
distribution, which requires an explicit prior distribution. To avoid this requirement, Rissanen advocates
choosing a codebook that minimises the maximum (worst-case) coding regret (25.77), that is ﬁnding
the probability distribution f that solves the following problem:
min
f

max
y∈Yn

−log f (y) + log p(y|ˆθ(y; γ ), γ )

.
(25.78)
The solution to this optimization problem is known as the normalized maximum likelihood (NML)
[106,107] (or Shtarkov [108]) distribution, and the resulting codelength is given by
INML(y, γ ) = −log p(y|ˆθ(y; γ ), γ ) + log

x∈Yn p(x|ˆθ(x; γ ), γ )dx.
(25.79)
In MDL parlance, the second term on the right hand side of (25.79) is called the parametric complexity
of the model γ . The parametric complexity has several interesting interpretations: (i) it is the logarithm
of the normalizing constant required to render the infeasible maximum likelihood codes realizable;
(ii) it is the (constant) regret obtained by the normalized maximum likelihood codes with respect to the
unattainable maximum likelihood codes, and (iii) it is the logarithm of the number of distinguishable
distributions contained in the model γ at sample size n [109]. In addition to the explicit lack of prior
distributions, the NML code (25.79) differs from the MML two-part codes in the sense that it does
not explicitly depend on any particular distribution in the model γ . This means that like the marginal
distribution discussed in Section 1.25.4.1, the NML codes cannot be used for point estimation of the
model parameters θγ .
Practically, the normalizing integral in (25.79) is difﬁcult to compute for many models. Rissanen has
derived an asymptotic approximation to the NML distribution which is applicable to many commonly
used statistical models (including for example, linear models with Gaussian noise and autoregressive

1444
CHAPTER 25 A Tutorial on Model Selection
moving average models) [106]. Under this approximation, the codelength formula is given by
IANML(y, γ ) = −log p(y|ˆθ(y; γ ), γ ) + k
2 log
 n
2π

+ log

θ∈γ
(
|J1(θ; γ )|dθ,
(25.80)
where k is the number of free parameters for the model γ, n is the sample size and J1(·) is the per
sample Fisher information matrix given by (25.36). The approximation (25.80) swaps an integral over
the dataspace for an often simpler integral over the parameter space. For models that satisfy the regularity
conditions discussed in [106], the approximate NML codelength satisﬁes
INML(y, γ ) = IANML(y, γ ) + o(1),
where o(1) denotes a term that disappears as n →∞.
The NML distribution is only one of the coding methods considered by Rissanen and co-workers in
the MDL principle; the interested reader is referred to Grünwald’s book [61] for a detailed discussion
of the full spectrum of coding schemes developed within the context of MDL inference.
1.25.4.7 Problems with divergent parametric complexity
The NML distribution offers a formula for computing codelengths that is free of the requirement
to specify suitable prior distributions, and therefore appears to offer a universal approach to model
selection by compression. Unfortunately, the formula (25.79) is not always applicable as the parametric
complexity can diverge, even in commonly used models [110]; this also holds for the approximate
parametric complexity in (25.80). To circumvent this problem, Rissanen [106] recommends bounding
either the dataspace, for (25.79), or the parameter space, for (25.80), so that the parametric complexity
is ﬁnite. In the case of linear regression models with Gaussian noise, Rissanen extends this idea even
further by treating the hyperparameters that specify the bounding region as parameters, and re-applying
the NML formula to arrive at a parameter-free criterion (see [111] for details). Other models with
inﬁnite parametric complexity which have been addressed in a similar manner include the Poisson and
geometric distributions [110] and stationary autoregressive models [112].
Unfortunately, the choice of bounding region is in general arbitrary, and different choices will lead to
different codelength formulae, and hence different behavior. In this sense, the choice of bounding region
is akin to the selection of a prior distribution in MML; however, advocates of the Bayesian approach
argue that selection of a prior density is signiﬁcantly more transparent, and interpretable, than the choice
of a bounding region.
1.25.4.8 Sequential variants of MDL
One way to overcome the problems of inﬁnite parametric complexity is to code the data sequentially. In
this framework, one uses the ﬁrst t datapoints, (y1, . . . , yt), to construct a predictive model that is used
to code yt+1. This process may be repeated until the entire dataset (y1, . . . , yn) has been “transmitted”,
and the resulting codelength may be used for model selection. This idea was ﬁrst proposed as part of
the predictive MDL procedure [101,113].
More recently, the idea has been reﬁned with the introduction of the sequentially normalized max-
imum likelihood (SNML) model [114], and the related conditional normalized maximum likelihood
(CNML) model [115]. These exploit the fact that the parametric complexity, conditional on some
observed data, is often ﬁnite, even if the unconditional parametric complexity diverges. Additionally,

1.25.4 Model Selection by Compression
1445
the CNML distributions can be used to make predictions about future data arising from the same source
in a similar manner to the Bayesian predictive distribution. A recent paper comparing SNML and CNML
against Bayesian and MML approaches in the particular case of exponential distributions found that
the SNML predictive distribution has favorable properties in comparison to the predictive distribution
formed by “plugging in” the maximum likelihood estimates [116]. Further information on variants of
the MDL principle is available in [61,102].
1.25.4.9 Relation to MML and Bayesian inference
It is of interest to compare the NML coding scheme to the codebooks advocated by the MML principle.
ThisismosteasilydonebycomparingtheapproximateNMLcodelength(25.80)totheWallace-Freeman
approximate codelength (25.60). Consider the so-called Jeffreys prior distribution:
πθ(θ; γ ) =
(
|Jn(θ; γ )|
'
α∈γ
(
|Jn(α; γ )|dα
.
(25.81)
The normalizing term in the above equation is the approximate parametric complexity in (25.80).
Substituting (25.81) into the Wallace-Freeman code (25.60) leads to the cancellation of the Fisher
information term, and the resulting Wallace-Freeman codelength is within O( log k) of the approximate
NML codelength. This difference in codelengths is attributed to the fact that MML codes state a speciﬁc
member of the set γ which is used to transmit the data, while the NML distribution makes no such
assertion. The close similarity between the MDL and MML codelengths was demonstrated in [116] in
the context of coding data arising from exponential distributions. A further comparison of MDL, the
NML distribution and MML can be found in [27], pp. 408–415.
The close equivalence of the approximate NML and Wallace-Freeman codes implies that like the
BIC, the approximate NML criterion is a consistent estimator of the true model γ ∗that generated the
data, assuming γ ∗∈, and that the normalizing integral is ﬁnite. In fact, the earliest incarnation of
MDL introduced in 1978 [32] was exactly equivalent to the Bayesian information criterion, which was
also introduced in 1978. This fact has caused some confusion in the engineering community, where the
terms MDL and BIC are often used interchangeably to mean the “k-on-two log n” criterion given by
(25.37) (this issue and its implications are discussed, for example, in [117]).
We conclude this section with a brief discussion of the differences and similarities between the MML
and MDL principles. This is done to attempt to clear up some of the confusion and misunderstands
surrounding these two principles. The MDL principle encompasses a range of compression schemes
(such as the NML model, the Bayesian model, etc.), which are uniﬁed by the deeper concept of universal
models for data compression. In recent times, the MDL community has focused on those universal mod-
els that attain minimax regret with respect to the maximum likelihood codes to avoid the speciﬁcation
of subjective prior distributions. The two-part codebooks used in the MML principle are also types of
universal models, and so advocates of the MDL school often suggest that MML is subsumed in the
MDL principle. However, as the above coincidence of Wallace-Freeman and NML demonstrates, the
MDL universal models can be implemented in the MML framework by the appropriate choice of prior
distribution, and thus the MML school tend to suggest that MDL is essentially a special case of the
MML theory, particularly given the time-line of developments in the two camps.
Practically, the most important difference between the two principles is in the choice of two-part
(for MML) versus one-part (for MDL) coding schemes. The two-part codes chosen by MML allow for

1446
CHAPTER 25 A Tutorial on Model Selection
the explicit deﬁnition of new classes of parameter estimators, in addition to model selection criteria,
which are not obtainable by the one-part MDL codes. As empirical, and theoretical evidence strongly
supports the general improvement of the MML estimators over the maximum likelihood and Bayesian
MAP estimators, this difference seems to be of perhaps the greatest importance.
1.25.4.10 Example: MDL linear regression
An MDL criterion for the linear regression example is now examined. Given the importance of linear
regression, there have been a range of MDL inspired criteria developed, for example, the predictive MDL
criterion [101], g-MDL [118], Liang and Barron’s approach [119], sequentially normalized least squares
[120,121] and normalized maximum likelihood based methods [40,111,122]. We choose to focus on the
NML criterion introduced by Rissanen in 2000 [111] as it involves no user speciﬁed hyperparameters
and does not depend on the ordering of the data, a problem which affects the predictive and sequential
procedures. Recall that in the linear regression case, the model index γ speciﬁes which covariates are
to be used from the full design matrix X. For a given γ , the NML codelength, up to constants, is
n −k
2

log ˆτ(y; γ ) + k
2 log
& ˆR(γ )
n

−log 
n −k
2

−log 
k
2

−log πγ (γ ),
(25.82)
where ˆτ(y; γ ) is the maximum likelihood estimate of τ given by (25.6),
ˆR(γ ) = ˆβ(y; γ )′ 
X′
γ Xγ

ˆβ(y; γ )
(25.83)
and ˆβ(y; γ ) are the least-squares estimates of the model coefﬁcients β for subset γ given by (25.5). As
in the case of the BIC given by (25.43), and the MML linear regression criteria given by (25.69) and
(25.73, 25.74), the codelength includes a prior πγ (·) required to state the subset γ , and suitable choices
are discussed in Section 1.25.3.4.
Interestingly, Hansen and Yu [123] have shown that NML is either asymptotically consistent or
asymptotically efﬁcient depending on the nature of the data generating distribution. In this way, the
NML criterion can be viewed as a “bridge” between the AIC-based criteria and the BIC. Given the
close similarity between the NML and MML linear regression criteria [68], this property is expected to
also hold for the MML linear regression criteria.
1.25.4.11 Applications of MDL
Below is an incomplete list of successful applications of the MDL principle to commonly used statistical
models:
•
the multinomial distribution [124],
•
causal models and Naïve Bayes classiﬁcation [125,126],
•
variable bin-width histograms [127,128],
•
linear regression [111,118,122],
•
signal denoising [40,111],
•
autoregressive models [112],
•
statistical genetics [129,130].

1.25.5 Simulation
1447
Of particular interest is the multinomial distribution as it is one of the few models for which the exact
parametric complexity is ﬁnite without any requirement for bounding, and may be efﬁciently computed
in polynomial time using the clever algorithm discussed in [124]. This algorithm has subsequently been
used to compute codelengths in histogram estimation and other similar models.
1.25.5 Simulation
We conclude with a brief simulation demonstrating the practical application of the distance based,
Bayesian and information theoretic model selection criteria discussed in this chapter. The simulation
compares the AICc (25.25), KICc (25.27), BIC (25.42), MMLu (25.69), MMLg (25.73, 25.74) and NML
(25.82) criteria on the problem of polynomial order selection with linear regression. Datasets of various
sample sizes 25 ≤n ≤500 were generated from the polynomial model
y∗= x3 −0.5x2 −5x −1.5,
x ∈[−3, 3]
(25.84)
with design points x uniformly generated from the compact set x ∈[−3, 3] [13,102]. The noise variance
τ was chosen to yield a signal-to-noise ratio of ten. Recalling the linear regression example from Section
1.25.1.2, the complete design matrix consisted of polynomial bases xi for (i = 0, . . . , q),
X = (x0, x1, . . . , xq).
(25.85)
For each generated data set, each of the six criteria were used to select a nested polynomial model up to
the maximum polynomial of order q = 20. The simulation was repeated 104 times and the zero-order
polynomial model was not considered. The criteria were compared on two important metrics: (i) order
selection, and (ii) squared prediction error. The results are presented in Table 25.1.
The simulation is highly illustrative of the general behaviors of the model selection criteria under
consideration. For small sample sizes (n ≤50), the KICc criterion performs adequately in terms of order
selection, but like the AICc, this performance does not improve with increasing sample size. This is not
surprising given that both AICc and KICc are not consistent model selection procedures. While BIC
performs relatively poorly for the smallest sample sizes, its performance dramatically improves when the
sample size is larger. The asymptotic approximation from which BIC is derived is inadequate for small
sample sizes which explains its poor performance. In contrast, the MML and NML criteria perform well
for all sample sizes in terms of both order selection and prediction error. In this particular example, the
MMLu and NML criteria are virtually indistinguishable, while the MMLg criterion performs slightly
better. For larger sample sizes (n ≥125) all four consistent criteria performed essentially the same.
This brief simulation serves to demonstrate the general behavior of the six criteria and should not
be taken as indicative of their performance in other model selection problems. For a more detailed
comparison of some recent regression methods, including MMLu, MMLg and NML, see [122]. The
MMLu, MMLg and NML criteria were found to be amongst the best of the eight criteria considered in
all experiments conducted by the authors.3
3Unfortunately, through no fault of the authors, the formulae for MMLg given in [122] is missing the δ/2 term that is present
in formula (25.73) in this tutorial. However, this has not effected the experimental results as the simulation code uses the
correct formula (see Section 1.25.4.4).

1448
CHAPTER 25 A Tutorial on Model Selection
Table 25.1 Polynomial Order Selected by the Criteria (Expressed as Percentages) and Squared
Error in Estimated Coefﬁcients
Criterion
Sample size
25
50
75
100
125
150
200
500
ˆp = p
AICc
86.2
79.5
76.9
76.0
74.7
74.5
74.0
72.4
KICc
93.4
90.8
89.8
89.8
89.3
89.2
88.9
88.6
BIC
77.5
91.4
94.1
95.7
96.0
97.0
97.4
98.7
MMLu
93.5
96.4
97.2
97.7
97.7
98.5
98.5
99.2
MMLg
95.6
97.9
98.4
98.6
98.6
99.1
99.0
99.5
NML
94.4
96.8
97.4
97.9
97.9
98.7
98.6
99.3
ˆp > p
AICc
13.8
20.5
23.1
24.0
25.3
25.5
26.1
27.7
KICc
6.60
9.30
10.2
10.2
10.7
10.8
11.1
11.4
BIC
22.5
8.58
5.94
4.30
4.03
2.96
2.60
1.29
MMLu
6.50
3.64
2.82
2.29
2.42
1.48
1.50
0.78
MMLg
4.36
2.14
1.57
1.44
1.43
0.94
1.01
0.50
NML
5.62
3.16
2.57
2.07
2.13
1.30
1.39
0.72
Error
AICc
0.52
0.25
0.17
0.13
0.10
0.08
0.06
0.03
KICc
0.47
0.22
0.14
0.11
0.09
0.07
0.05
0.02
BIC
0.59
0.22
0.14
0.10
0.08
0.07
0.05
0.02
MMLu
0.47
0.21
0.13
0.10
0.08
0.06
0.05
0.02
MMLg
0.46
0.21
0.13
0.10
0.08
0.06
0.05
0.02
NML
0.46
0.21
0.13
0.10
0.08
0.06
0.05
0.02
References
[1] E.L. Lehmann, G. Casella, Theory of Point Estimation, Springer Texts in Statistics, Springer, fourth ed.,
2003.
[2] S. Kullback, R.A. Leibler, Ann. Math. Stat. 22 (1951) 79–86.
[3] K. Takeuchi, Suri-Kagaku, Math. Sci. 153 (1976) 12–18 (in Japanese).
[4] R. Shibata, Statistical aspects of model selection, in: J. C. Willems (Ed.), From Data to Model, Springer,
New York, 1989, pp. 215–240.
[5] H. Linhart, W. Zucchini, Model Selection, Wiley, New York, 1986.
[6] H. Akaike, IEEE Trans. Automat. Contr. 19 (1974) 716–723.
[7] C.M. Hurvich, C.-L. Tsai, Biometrika 76 (1989) 297–307.
[8] A.-K. Seghouane, S.-I. Amari, IEEE Trans. Neural Networks 18 (2007) 97–106.
[9] A.-K. Seghouane, Signal Process. 90 (2010) 217–224.
[10] A.D.R. McQuarrie, C.-L. Tsai, Regression and Time Series Model Selection, World Scientiﬁc, 1998.
[11] H. Jeffreys, Proc. Roy. Soc. Lond. Ser. A Math. Phys. Sci. 186 (1946) 453–461.
[12] J.E. Cavanaugh, Stat. Probab. Lett. 42 (1999) 333–343.

References
1449
[13] A.-K. Seghouane, M. Bekara, IEEE Trans. Signal Process. 52 (2004) 3314–3323.
[14] D.F. Schmidt, E. Makalic, Lect. Notes Artif. Int. 6464 (2010) 223–232.
[15] A.K. Seghouane, Signal Process. 86 (2006) 2074–2084.
[16] A.-K. Seghouane, IEEE Trans. Aero. Electron. Syst. 47 (2011) 1154–1165.
[17] A.K. Seghouane, IEEE Trans. Circ. Syst. Part I 53 (2006) 2327–2335.
[18] C.M. Hurvich, J.S. Simonoff, C.-L. Tsai, J. Roy. Stat. Soc. Ser. B 60 (1998) 271–293.
[19] M. Bekara, L. Knockaert, A.-K. Seghouane, G. Fleury, Signal Process. 86 (2006) 1400–1409.
[20] C.M. Hurvich, C.-L. Tsai, Biometrika 85 (1998) 701–710.
[21] N. Murata, S. Yoshizawa, S. Amari, IEEE Trans. Neural Networks 5 (1994) 865–872.
[22] A.K. Seghouane, M. Bekara, G. Fleury, Signal Process. 85 (2005) 1405–1417.
[23] C. Robert, The Bayesian Choice: From Decision-Theoretic Foundations to Computational Implementation,
Springer Texts in Statistics, Springer, 2001.
[24] J. Berger, L. Pericchi, in: P. Lahiri (Ed.), Model selection, vol. 38 of Lecture Notes Monograph Series,
Hayward, CA: pp. 135–207.
[25] J.M. Bernardo, J. Roy. Stat. Soc. Ser. B 41 (1979) 113–147.
[26] J. Berger, L. Pericchi, J. Am. Stat. Assoc. 91 (1996) 109–122.
[27] C.S. Wallace, Statistical and Inductive Inference by Minimum Message Length, Information Science and
Statistics, ﬁrst ed., Springer, 2005.
[28] G. Casella, E. Moreno, J. Am. Stat. Assoc. 101 (2006) 157–167.
[29] H. Jeffreys, The Theory of Probability, third ed., Oxford University Press, 1961.
[30] R.E. Kass, A.E. Raftery, J. Am. Stat. Assoc. 90 (1995) 773–795.
[31] G. Schwarz, The Ann. Stat. 6 (1978) 461–464.
[32] J. Rissanen, Automatica 14 (1978) 465–471.
[33] C.S. Wallace, D.M. Boulton, Comput. J. 11 (1968) 185–194.
[34] O.E. Barndorff-Nielsen, D.R. Cox, Asymptotic Techniques for Use in Statistics, Chapman & Hall: New
York, 1989.
[35] D.M.A. Haughton, Ann. Stat. 16 (1988) 342–355.
[36] C.R. Rao, Y. Wu, Biometrika 76 (1989) 369–374.
[37] L. Breiman, Ann. Stat. 24 (1996) 2350–2383.
[38] D. Madigan, A.E. Raftery, J. Am. Stat. Assoc. 89 (1994) 1536–1546.
[39] E.I. George, D.P. Foster, Biometrika 87 (2000) 731–747.
[40] T. Roos, P. Myllymäki, J. Rissanen, IEEE Trans. Signal Process. 57 (2009) 3347–3360.
[41] J.G. Scott, J.O. Berger, Ann. Stat. 38 (2010) 2587–2619.
[42] J. Chen, Z. Chen, Biometrika 95 (2008) 759–771.
[43] C.P. Robert, G. Casella, Monte Carlo Statistical Methods, Springer, 2004.
[44] A. Metropolis, M. Rosenbluth, A. Rosenbluth, E. Teller, J. Chem. Phys. 21 (1953) 1087–1092.
[45] W. Hastings, Biometrika 57 (1970) 97–109.
[46] S. Geman, D. Geman, IEEE Trans. Pattern Anal. Mach. Int. 6 (1984) 721–741.
[47] G. Casella, E.I. George, Am. Stat. 46 (1992) 167–174.
[48] S. Chib, J. Am. Stat. Assoc. 90 (1995) 1313–1321.
[49] P.J. Green, Biometrika 82 (1995) 711–732.
[50] E.I. George, R.E. McCulloch, J. Am. Stat. Assoc. 88 (1993) 881–889.
[51] C. Hans, A. Dobra, M. West, J. Am. Stat. Assoc. 102 (2007) 507–516.
[52] L. Breiman, Technometrics 37 (1995) 373–384.
[53] R. Tibshirani, J. Roy. Stat. Soc. Ser. B 58 (1996) 267–288.
[54] M.E. Tipping, J. Mach. Learn. Res. 1 (2001) 211–244.
[55] R.M. Neal, Bayesian learning for neural networks, Lecture Notes in Statistics, Springer Verlag, 1996.

1450
CHAPTER 25 A Tutorial on Model Selection
[56] T. Park, G. Casella, J. Am. Stat. Assoc. 103 (2008) 681–686.
[57] C. Hans, Biometrika 96 (2009) 835–845.
[58] T.M. Cover, J.A. Thomas, Elements of Information Theory, second ed., Wiley-Interscience, 2006.
[59] C.E. Shannon, Bell Syst. Tech. J. 27 (1948) 379–423 and 623–656.
[60] J. Rissanen, J.G.G. Langdon, IEEE Trans. Info. Theory IT-27 (1981) 12–23.
[61] P.D. Grünwald, The Minimum Description Length Principle, Adaptive Communication and Machine Learn-
ing, The MIT Press, 2007.
[62] C. Wallace, D. Boulton, Class. Soc. Bull. 3 (1975) 11–34.
[63] C.S. Wallace, P.R. Freeman, J. Roy. Stat. Soc. Ser. B 49 (1987) 240–252.
[64] C.S. Wallace, in: Proceedings of the International Conference on Information, Statistics and Induction in
Science, World Scientiﬁc, 1996, pp. 304–316.
[65] G.E. Farr, C.S. Wallace, Comput. J. 45 (2002) 285–292.
[66] J.H. Conway, N.J.A. Sloane, Sphere Packing, Lattices and Groups, third ed., Springer-Verlag, 1998.
[67] E. Makalic, D.F. Schmidt, Stat. Probab. Lett. 79 (2009) 1155–1161.
[68] D. Schmidt, E. Makalic, in: The 22nd Australasian Joint Conference on Artiﬁcial Intelligence, Melbourne,
Australia, pp. 312–321.
[69] C.S. Wallace, P.R. Freeman, J. Roy. Stat. Soc. Ser. B 54 (1992) 195–209.
[70] C.S. Wallace, D. Dowe, MML estimation of the von Mises concentration parameter, Technical Report,
Department of Computer Science, Monash University, 1993.
[71] D.L. Dowe, J. Oliver, C. Wallace, Lect. Notes Artif. Int. 1160 (1996) 213–227.
[72] D.F. Schmidt, Minimum message length inference of autoregressive moving average models, Ph.D. Thesis,
Clayton School of Information Technology, Monash University, 2008.
[73] A.R. Barron, T.M. Cover, IEEE Trans. Info. Theory 37 (1991) 1034–1054.
[74] L. LeCam, University of California Publications in Statistics, vol. 11 1953.
[75] D.L. Dowe, C.S. Wallace, in: Proceedings of 28th Symposium on the Interface, Computing Science and
Statistics, vol. 28, Sydney, Australia, pp. 614–618.
[76] C.S. Wallace, in: Proceedings of the 14th Biennial Statistical Conference, Queenland, Australia, p. 144.
[77] M. Viswanathan, C.S. Wallace, D.L. Dowe, K.B. Korb, Lect. Notes Artif. Int. 1747 (1999)
405–416.
[78] C.S. Wallace, D.L. Dowe, Stat. Comput. 10 (2000) 73–83.
[79] J.J. Oliver, R.A. Baxter, MML and Bayesianism: similarities and differences, Technical Report TR 206,
Department of Computer Science, Monash University, 1994.
[80] D.F. Schmidt, in: Proceedings of the 5th Workshop on Information Theoretic Methods in Science and
Engineering (WITMSE-11).
[81] L.J. Fitzgibbon, D.L. Dowe, L. Allison, in: Proceedings of the 19th International Conference on Machine
Learning (ICML’02), pp. 147–154.
[82] D.L. Dowe, Comput. J. 51 (2008) 523–560.
[83] E. Makalic, Minimum message length inference of artiﬁcial neural networks, Ph.D. Thesis, Clayton School
of Information Technology, Monash University, 2007.
[84] E. Makalic, L. Allison, in: Proceedings of the 85th Solomonoff Memorial Conference, Melbourne, Australia.
[85] M. Viswanathan, C. Wallace, in: Proceedings of the 7th Int. Workshop on Artif. Intelligence and Statistics,
Ft. Lauderdale, Florida, USA, pp. 169–177.
[86] G.W. Rumantir, C.S. Wallace, in: Proceedings of the 5th International Symposium on Intelligent Data Anal-
ysis (IDA 2003), vol. 2810, Springer-Verlag, Berlin, Germany, pp. 486–496.
[87] W. James, C.M. Stein, in: Proceedings of the 4th Berkeley Symposium, vol. 1, University of California Press,
pp. 361–379.
[88] S.L. Sclove, J. Am. Stat. Assoc. 63 (1968) 596–606.

References
1451
[89] N. Bouguila, D. Ziou, IEEE Trans. Knowl. Data Eng. 18 (2006) 993–1009.
[90] C.S. Wallace, J.D. Patrick, Mach. Learn. 11 (1993) 7–22.
[91] P. Tan, D. Dowe, Lect. Notes Artif. Int. 2903 (2003) 269–281.
[92] P. Tan, D. Dowe, Lect. Notes Artif. Int. 4293 (2006) 593–603.
[93] C.S.
Wallace,
K.B.
Korb,
in:
A.
Gammerman
(Ed.),
Causal
Models
and
Intelligent
Data
Management, Springer-Verlag, pp. 89–111.
[94] J.W.
Comley,
D.
Dowe,
Minimum
Message
Length
and
Generalized
Bayesian
Nets
with
Asymmetric Languages, M.I.T. Press (MIT Press), pp. 265–294.
[95] E. Makalic, L. Allison, D.L. Dowe, in: Proceedings of the IASTED International Conference on Artiﬁcial
Intelligence and Applications (AIA 2003).
[96] E. Makalic, L. Allison, A.P. Paplinski, in: Proceedings of the 8th Brazillian Symposium on Neural Networks
(SBRN 2004), Sao Luis, Maranhao, Brazil.
[97] D.F. Schmidt, E. Makalic, Shrinkage and denoising by minimum message length, Technical Report 2008/230,
Monash University, 2008.
[98] L.J.
Fitzgibbon,
D.L.
Dowe,
F.
Vahid,
in:
Proceedings
of
the
International
Conference
on
Intelligent Sensing and Information Processing (ICISIP), pp. 439–444.
[99] D.F.
Schmidt,
in:
Proceedings
of
the
85th
Solomonoff
Memorial
Conference,
Melbourne,
Australia.
[100] R.A. Baxter, J.J. Oliver, Lect. Notes Artif. Int. 1160 (1996) 83–90.
[101] J. Rissanen, Stochastic Complexity in Statistical Inquiry, World Scientiﬁc, 1989.
[102] J.
Rissanen,
Information
and
Complexity
in
Statistical
Modeling,
Information
Science
and
Statistics, ﬁrst ed., Springer, 2007.
[103] J. Rissanen, Circuits, Systems, and Signal Process. 1 (1982) 395–396.
[104] J. Rissanen, Ann. Stat. 11 (1983) 416–431.
[105] R.A. Baxter, J. Oliver, MDL and MML: similarities and differences, Technical Report TR 207, Department
of Computer Science, Monash University, 1994.
[106] J. Rissanen, IEEE Trans. Info. Theory 42 (1996) 40–47.
[107] J. Rissanen, IEEE Trans. Info. Theory 47 (2001) 1712–1717.
[108] Y.M. Shtarkov, Probl. Inform. Transm. 23 (1987) 3–17.
[109] V. Balasubramanian, in: I.J.M.P.D. Grünwald, M.A. Pitt (Eds.), Advances in Minimum Description Length:
Theory and Applications, MIT Press, pp. 81–99.
[110] S. de Rooij, P. Grünwald, J. Math. Psychol. 50 (2006) 180–192.
[111] J. Rissanen, IEEE Trans. Info. Theory 46 (2000) 2537–2543.
[112] D.F. Schmidt, E. Makalic, IEEE Trans. Signal Process. 59 (2011) 479–487.
[113] A.P. Dawid, J. Roy. Stat. Soc. Ser. A 147 (1984) 278–292.
[114] T. Roos, J. Rissanen, in: Proceedings of the 1st Workshop on Information Theoretic Methods in Science and
Engineering (WITMSE-08), Tampere International Center for Signal Processing (Invited Paper).
[115] J. Rissanen, T. Roos, in: Proceeding of the 2007 Information Theory and Applications Workshop (ITA-07),
IEEE Press, 2007, pp. 337–341 (Invited Paper).
[116] D.F. Schmidt, E. Makalic, IEEE Trans. Info. Theory 55 (2009) 3087–3090.
[117] D.F. Schmidt, E. Makalic, IEEE Trans. Signal Process. 60 (2012) 1508–1510.
[118] M.H. Hansen, B. Yu, J. Am. Stat. Assoc. 96 (2001) 746–774.
[119] F. Liang, A. Barron, IEEE Trans. Info. Theory 50 (2004) 2708–2726.
[120] J. Rissanen, T. Roos, P. Myllymäki, J. Multivariate Anal. 101 (2010) 839–849.
[121] D.F. Schmidt, T. Roos, in: Proceedings of the 3rd Workshop on Information Theoretic Methods in Science
and Engineering (WITMSE-10), Tampere International Center for Signal Processing. (Invited Paper).
[122] C.D. Giurcˇaneanu, S.A. Razavi, A. Liski, Signal Process. (2011).

1452
CHAPTER 25 A Tutorial on Model Selection
[123] M. Hansen, B. Yu, in: Science and Statistics: A Festchrift for Terry Speed, of Lecture Notes—Monograph
Series, vol. 40, Institute of Mathematical Statistics, pp. 145–164.
[124] P. Kontkanen, P. Myllymäki, Info. Process. Lett. 103 (2007) 227–233.
[125] T. Mononen, P. Myllymaki, in: Proceedings of the 10th International Conference on Discovery Science,
Lecture Notes in Computer Science, Sendai, Japan, vol. 4755, pp. 151–160.
[126] T. Silander, T. Roos, P. Myllymäki, in: Proceedings of the 12th International Conference on Artiﬁcial Intel-
ligence and Statistics (AISTATS-09).
[127] J. Rissanen, T.P. Speed, B. Yu, IEEE Trans. Info. Theory 38 (1992) 315–323.
[128] P. Kontkanen, P. Myllymäki, in: M. Meila, X. Shen (Eds.), in: Proceedings of the 11th International Confer-
ence on Artiﬁcial Intelligence and Statistics (AISTATS 2007), San Juan, Puerto Rico.
[129] Y. Yang, I. Tabus, in: IEEE International Workshop on Genomic Signal Processing and Statistics (GENSIPS
2007), pp. 1–4.
[130] I. Tabus, J. Rissanen, J. Astola, Signal Process. 83 (2003) 713–727.

26
CHAPTER
Music Mining
George Tzanetakis
Department of Computer Science, University of Victoria, Victoria, Canada
1.26.1 Introduction
During the ﬁrst ten years of the 21st century we have witnessed a dramatic shift in how music is
produced, distributed, and consumed. Several factors including advances in digital signal processing,
faster computers, and steadily increasing digital storage capacity and network bandwidth have made
digital music distribution a reality. It is now possible to purchase and listen to music as well as ﬁnd all
sorts of associated information about it from any computer or smart phone. Currently, portable music
players and phones can store thousands of music tracks, and millions of tracks are accessible through
streaming over the Internet in digital music stores and personalized radio stations.
Enabling anyone with access to a computer and the Internet to listen to essentially most of recorded
music in human history is a remarkable technological achievement that would probably be considered
impossible even twenty years ago. The research area of Music Information Retrieval (MIR) gradually
emerged during this time period in order to address the challenge of effectively accessing and interacting
with these vast digital collections of music and associated information such as meta-data, reviews, blogs,
rankings, and usage/download patterns.
As a research area, data mining emerged from the interaction between the database community that
needed to address the challenge of extracting useful not explicitly represented information from large
collections of data, and the machine learning community which explored algorithms that can improve
their performance over time as they are exposed to more data. Music mining refers to the application of
ideas from the ﬁeld of data mining to the extraction of useful information from large collections of music
and is the topic of this chapter. It can be viewed as a subset of music information retrieval research.
Music is pervasive and plays an important role in the daily lives of most people today. It is an
extremely complex human creation that somehow strongly affects our intellect and emotions. In order
to create effective tools for interacting with large music collections we need to design algorithms that
can extract high-level information from music signals and use that information in a variety of mining
tasks. Techniques from audio signal processing have been used to extract various low-level and mid-
level audio features that can subsequently be used to represent music tracks to perform various music
mining tasks. In addition it is possible to apply traditional data mining techniques to other sources of
music related information such a reviews, lyrics, blogs, rankings and usage patterns.
Music has several important characteristics and challenges that make it a particularly interesting
research area for data mining. Similarly to image search it requires sophisticated content analysis
Academic Press Library in Signal Processing. http://dx.doi.org/10.1016/B978-0-12-396502-8.00026-7
© 2014 Elsevier Ltd. All rights reserved.
1453

1454
CHAPTER 26 Music Mining
algorithms to extract useful information from the raw signal. At the same time it has very rich structured
context information associated with it. For example a particular song especially if it is popular might
be mentioned in thousands of web pages and blogs. It also has lyrics which can be analyzed and by
virtue of being performed by a particular artist has many associations to other pieces of music. This
rich tapestry of relevant information provides many opportunities for data mining but at the same time
its heterogeneity is a challenge for many algorithms that require more homogeneous and structured
data. The sheer amount of data required both for storing the audio but also for storing calculated audio
features poses signiﬁcant system scalability challenges and requires efﬁcient large scale algorithms. The
focus of this chapter is music mining in large music collections and does not cover tasks in MIR that
deal with individual music tracks such as transcription, audio-score alignment and structural analysis
among others. The choice of topics as well as how much detail they are described were to a large extent
determined by the corresponding volume of published work in each music mining topic.
The remainder of this chapter is organized as follows. The ﬁrst Section provides an overview of audio
feature extraction for audio signals which forms the foundation of many music mining algorithms. The
following sections describe various music mining tasks such as classiﬁcation, clustering, tag annotation,
advanced data mining, and visualization. The goal is to provide an overview of the music mining
problems that researchers have explored and describe in some detail basic system conﬁgurations to
solve these tasks. The particular techniques chosen are representative examples that are straightforward
to explain rather than a comprehensive list of all possibilities. We also discuss open problems and future
trends in music mining. The ﬁnal section provides pointers to further reading about these topics.
1.26.2 Ground truth acquisition and evaluation
In the data mining literature, frequently the primary concern is the algorithm(s) used for extracting
information from the data. This data is assumed to be readily available and in the format required for the
particular task. However, in many speciﬁc applications areas such as music mining the data acquisition
process is critical and not trivial. In mining algorithms typically there is a clear distinction between data
that is somehow automatically extracted and the desired information that the mining algorithm somehow
“extracts.” In order to assess how well a particular mining algorithm performs, we need to know in some
way, what the “correct” answer should be. There are several general strategies that have been used to
acquire this “ground truth” information. In most cases human users are involved in the process. With
access to this ground truth information for a particular dataset, different music mining algorithms and
systems can be evaluated, compared, and contrasted. In this section, we discuss the process of ground
truth acquisition and evaluation in a generic way that is applicable to most music mining tasks. Many
musicminingtaskscanbeviewedaswaysofassociatingmusicandtext.Free-formtext(frequentlycalled
tags) is the most general type of annotation and subsumes speciﬁc annotations such as genre, style, and
emotion/motion. In the following sections issues more speciﬁc to each task are examined in more detail.
In terms of ground truth acquisition the simplest case is when the desired annotations are readily
available by some external authority. For example online music stores provide genre labels for most of
their music that can be directly used to train genre classiﬁcation algorithms. For more specialized types
of information expert annotation can be used. For example the personalized radio company Pandora
utilizes music experts to annotate pieces of music with 400 attributes. Expert data is reliable and of high

1.26.2 Ground Truth Acquisition and Evaluation
1455
quality but it is costly to acquire and therefore harder to scale to large music collections. An alternative
approach is to utilize average users/listeners to perform the annotation. The time honored approach to
acquiring ground truth, especially in academic work, is the survey typically of undergraduate students.
Surveys can be carefully designed and therefore the data obtained tends to be reliable. However, they are
time consuming and costly and therefore limited in terms of scalability. More recently there has been a
surge in the use of “social” tags which are simply words entered by users to characterize their photos
or music. Last.fm is a music discovery Web site that relies on such social tags. By the beginning of
2007, Last’fm’s large base of 40 million monthly users had built an unstructured vocabulary of 960,000
free-text tags and used it to annotate millions of songs. By harvesting the collective effort of millions
of users social tagging can scale to large collections. However it comes with its own set of issues.
As there is no restriction in the vocabulary, used there is a lot of inconsistency and noise in the data.
Another important issue is that there is a sparsity (or lack of) tags for new artists/tracks which has been
termed the cold-start problem. This is a speciﬁc case of the more general problem of popularity bias
in which popular multimedia items tend to be recommended to most users simply because there is a
lot of information about them. An interesting alternative that combines some of the control of surveys
with the scalability and large number of users of social tags is the concept of annotation games. These
are games in which a large group of users are presented with a song, and a list of tags. The goal of the
game is to “guess” the tags that other users apply for that song. When a large group of users agree on a
tag then the song has a strong association with it. Such annotation games belong to the larger category
of games with a purpose in which players play the game because it is engaging and entertaining, while
at the same time in the process of playing provide valuable information. The most famous such game
is the ESP game that has been used for image annotation. Even though similar annotation games have
been proposed for music they have not yet received large scale usage.
Assuming that ground truth for a particular task is available it is important to be able to evaluate
different algorithms that attempt to solve it. Music mining is an emerging research area with a history of
about ten years. Typically early work in a particular music mining task involves assembling a collection
of music and associated ground truth for the task. In many cases only the original researchers have access
to the data and the work is hard to replicate. Over time some of these datasets have been shared helping
make more meaningful comparisons between different algorithms and systems. A big challenge in music
mining is the difﬁculty of sharing data given the multiple copyrights associated with music information.
The Music Information Retrieval Evaluation eXchange (MIREX) is an annual evaluation campaign for
music information retrieval (MIR) algorithms that is coupled to the International Conference of the
Society for Music Information Retrieval (ISMIR). It is organized by the graduate school of library and
informationsciencesattheUniversityofIllinoisatUrbana-Champaign.Participatinggroupssubmittheir
systems that follow speciﬁc input/output conventions and they are evaluated on data that for some tasks
is accessible only by the MIREX organizers and not the participants. In addition to dealing this way with
copyright issues it also helps avoid overﬁtting which is an important problem for data mining algorithms
in general. Overﬁtting refers to the situation where the performance of a mining algorithm in training
data is misleadingly higher than its performance on data it has not encountered before. If the full datasets
used for evaluation are available it is possible to over-optimize learning algorithms to ﬁt their speciﬁc
characteristics to the expense of generalization performance, i.e., the performance of the algorithm on
data that it has not encountered during training. In the remainder of this chapter representative results
from MIREX will be provided for the music mining tasks that are described, when they are available.

1456
CHAPTER 26 Music Mining
1.26.3 Audio feature extraction
Audio feature extraction forms the basis of many music mining tasks. Even though a full exposition
is beyond the scope of this chapter, a short overview is provided as it is important in understanding
what type of information is represented and how it is extracted. The goal of audio feature extraction is
to calculate a succinct representation that summarizes musical information about the underlying audio
signal. The representation should capture in a statistical sense the different types of musical information
that humans are aware of when listening to music.
The three basic facets of music information that have mostly been explored in the existing literature
are timbre, rhythm, and harmony. Rather than providing formal deﬁnitions, which are still debated
among musicologists, we describe these terms informally without going into details that would require
some knowledge of music theory. Timbre refers to the characteristics of the musical sound that are
independent of the actual notes played and are related to the instruments playing and their sound. For
example the exact same music piece played by a rock band with electric guitars and drums will sound
very different than the same music piece performed by a jazz big band. Rhythm refers to the periodic
repeating hierarchical structure that underlies the music independently of what instruments are playing.
For example the famous beginning of the 5th Symphony by Beethoven will have the same rhythm
independently of whether it is played by a symphonic orchestra or a cheap toy piano. Harmony refers
to the simultaneous sounding of groups of discrete pitches/notes as well as how these groups evolve
over time. For example a dance remix of a tune by the Beatles will have the same harmony or chord
structure with the original while the rhythm and timbre will be completely different.
There are many variations in timbral feature extraction but most systems follow a common general
template. The audio signal is broken into small slices (typically around 10–40 ms) and some form of
frequency analysis, such as the Discrete Fourier Transform, is performed, followed by a summarization
step in which a set of numbers (the feature vector) is calculated. This feature vector attempts to sum-
marize/capture the content information of that short slice in time. After this stage the music track can
then be represented as a sequence (trajectory) of feature vectors (points) in a high-dimensional feature
space. That sequence can then be characterized using a more compact representation for subsequent
classiﬁcation.
Most audio features are extracted in three stages: (1) spectrum calculation, (2) frequency-domain
summarization, and (3) time-domain summarization. In spectrum calculation, a short-time slice (typi-
cally around 10–40 ms) of waveform samples is transformed to a frequency domain representation. The
most common such transformation is the Short Time Fourier Transform (STFT). During each short-
time slice the signal is assumed to be approximately stationary and is windowed to reduce the effect of
discontinuities at the start and end of the frame. This frequency domain transformation preserves all the
information in the signal and therefore the resulting spectrum still has high dimensionality. For analysis
purposes, it is necessary to ﬁnd a more succinct description that has signiﬁcantly lower dimensionality
while still retaining the desired content information. Frequency domain summarization converts the high
dimensional spectrum (typically 512 or 1024 coefﬁcients) to a smaller set of number features (typically
10–30). A common approach is to use various descriptors of the spectrum shape such as the Spectral
Centroid and Bandwidth. Another widely used frequency domain summarization of the Spectrum are
the Mel-Frequency Cepstral Coefﬁcients (MFCCs), a representation which originated from the speech
and speaker recognition community. MFCC summarize spectral information (the energy distribution of

1.26.3 Audio Feature Extraction
1457
different frequencies) by taking into account, to some extent, the characteristics of the human auditory
system. Such features depend on the instrumentation of a piece, how the timbral “texture” changes over
time as well as how humans perceive this information. The goal of time domain summarization is to
characterize the musical signal at longer time scales than the short-time analysis slices. Typically this
summarization is performed across so called “texture” windows of approximately 2–3 s or it can be also
performed over the entire piece of music. Figure 26.1 shows graphically feature extraction, frequency
and time summarization.
Several variations on time domain summarization have been proposed. A popular approach is to ﬁt
Gaussian densities with diagonal covariance or full-covariance and then use the resulting parameters as
the feature vector. Another approach has been the use of auto-regressive models that model the evolution
of the feature vectors within the time window of interest.
FIGURE 26.1
Feature extraction and texture window.

1458
CHAPTER 26 Music Mining
(a)
(b)
(c)
FIGURE 26.2
The time evolution of audio features is important in characterizing musical content. The time evolution of the
spectral centroid for two different 30-second excerpts of music is shown in (a). The result of applying a moving
mean and standard deviation calculation over a texture window of approximately 1 s is shown in (b) and (c).
Amoredetailedviewofsummarization(orasissometimescalledaggregation)isshowninFigure 26.2.
It shows how the time evolution audio features, in this case the spectral centroid, can be summarized
by applying a moving mean and standard deviation.
Automatic music transcription is the process of converting an audio signal to a musical score (a
symbolic representation containing only information about pitch and rhythm). It is a hard problem
and existing techniques can only deal with simple “toy” examples. Instead the most commonly used
pitch-based representation are the pitch and pitch-class proﬁles (other alternative names used in litera-
ture are pitch histograms and chroma-vectors for Pitch Class Proﬁles). The pitch proﬁle measures the
occurrence of speciﬁc discrete musical pitches in a music segment and the pitch class proﬁle considers
all octaves equivalent essentially folding the pitch proﬁle into 12 pitch classes. Due to space limitations
we can not go into details about how pitch proﬁles are calculated. Broadly speaking they can either
be computed by summing the energy of different frequency bins that correspond to particular pitches
or alternatively multiple pitch estimation can be performed and the results can be accumulated into a
proﬁle. Figure 26.3 shows a chromagram (i.e., a sequence of chroma vectors over time) corresponding
to a continuous pitch glide over two octaves. The successive activation of the chromatic scale notes as
well as the warping in octaves can be observed.
Automatically extracting information related to rhythm is also important. Rhythmic information
is hierarchical in nature and involves multiple related periodicities. A typical representation is a beat
histogram (or sometimes called a beat spectrum) that provides a “salience” value for every possible
periodicity. A typical approach applies onset detection to generate an onset strength signal that has
high energy values at the time locations where there are signiﬁcant changes in the audio spectrum such
as the start of new notes. The periodicities of the onset strength signal at the range of human rhythm
(approximately 30–180 beats/min) are calculated using autocorrelation. Another more recent approach
is to identify rhythmic patterns that are characteristic of a particular genre automatically and characterize
each piece as a occurrence histogram over a set of basic rhythmic patterns.
Figure 26.4 shows two example beat histograms from 30 s clips of HipHop Jazz (left) and Bossa
Nova (right). As can be seen in both histograms the prominent periodicities or candidate tempos are
clearly visible. Once the tempo of the piece is identiﬁed the beat locations can be calculated by locally
ﬁtting tempo hypothesis with regularly spaced peaks of the onset strength signal.

1.26.3 Audio Feature Extraction
1459
FIGURE 26.3
Chromagram for pitch glide over two octaves.
FIGURE 26.4
Beat histograms of HipHop/Jazz and Bossa Nova.

1460
CHAPTER 26 Music Mining
1.26.4 Extracting context information about music
In addition to information extracted by analyzing the audio content of music, there is a wealth of informa-
tion that can be extracted by analyzing information on the web as well as patterns of downloads/listening.
We use the general term musical context to describe this type of information. In some cases such as song
lyrics the desired information is explicitly available somewhere on the web and the challenge is to appro-
priately ﬁlter out irrelevant information from the corresponding web pages. Text-based search engines
such as Google and Bing can be leveraged for the initial retrieval that can then be followed by some
post-processing based on heuristics that are speciﬁc to the music domain. Other types of information
are not as straightforward and can require more sophisticated mechanisms such as the term weighting
used in text retrieval systems, or natural language processing techniques such as entity detection. Such
techniques are covered in detail in the literature as they are part of modern day search engines. As an
illustrative example we will consider the problem of detecting the country of origin of a particular artist.
As a ﬁrst attempt one can query a search engine for various pairs of artist name and countries and simply
count the number of pages returned. The country with the highest number of pages returned is returned
as the country of origin. A more sophisticated approach is to analyze the retrieved web pages using term
weighting. More speciﬁcally consider country c as a term. The document frequency DF(c, a) is deﬁned
as the total number of web pages retrieved for artist a in which the country term c appears at least once.
The term frequency T F(c, a) is deﬁned as the total number of occurrences of the country term c in all
pages retrieved for artist a. The basic idea of term frequency-inverse document frequency weighting
(TF-IDF) is to “penalize” terms that appear in many documents (in our case the documents retrieved for
all artists) and increase the weight of terms that occur frequently in the set of web pages retrieved for a
speciﬁc artists. There are several TF-IDF weighting schemes. For example a logarithmic formulation is:
T F I DF(c, a) = ln (1 + T F(c, a)) ∗ln

1 +
n
DF(c)

,
(26.1)
where DF(c) is the document frequency of a particular country c over the documents returned for all
artists a. Using the above equation the weight of every country c can be calculated for a particular artist
query a. The country with the highest weight is then selected as the the predicted country of origin.
1.26.5 Similarity search
Similarity retrieval (or query-by-example) is one of the most fundamental MIR tasks. It is also one of the
ﬁrst tasks that were explored in the literature. It was originally inspired by ideas from text information
retrieval and this early inﬂuence is reﬂected in the naming of the ﬁeld as Music Information Retrieval
(MIR). Today most people with computers use search engines on a daily basis and are familiar with
the basic idea of text information retrieval. The user submits a query consisting of some words to the
search engine and the search engine returns a ranked list of web pages sorted by how relevant they are
to the query.
Similarity retrieval can be viewed as an analogous process where instead of the user querying the
system by providing text the query consists of an actual piece of music. The system then responds

1.26.5 Similarity Search
1461
by returning a list of music pieces ranked by their similarity to the query. Typically the input to the
system consists of the query music piece (using either a symbolic or audio representation) as well as
additional metadata information such as the name of the song, artist, year of release, etc. Each returned
item typically also contains the same types of meta-data. In addition to the audio content and meta-data
other types of user generated information can also be considered such as ratings, purchase history,
social relations and tags. Similarity retrieval can also be viewed as a basic form of playlist generation
in which the returned results form a playlist that is “seeded” by the query. However more complex
scenarios of playlist generation can be envisioned. For example a start and end seed might be speciﬁed
or additional constraints such as approximate duration or minimum tempo variation can be speciﬁed.
Another variation is based on what collection/database is used for retrieval. The term playlisting is more
commonly used to describe the scenario where the returned results come from the personal collection
of the user, while the term recommendation is more commonly used in the case where the returned
results are from a store containing a large universe of music. The purpose of the recommendation
process is to entice the user to purchase more music pieces and expand their collection. Although these
three terms (similarity retrieval, music recommendation, automatic playlisting) have somewhat different
connotations the underlying methodology for solving them is mostly similar so for the most part we
will use them interchangeably. Another related term, that is sometimes used, is personalized radio in
which the idea is to play music that is targeted to the preferences of a speciﬁc user.
One can distinguish three basic approaches to computing music similarity. Content-based similarity
is performed by analyzing the actual content to extract the necessary information. Metadata approaches
exploit sources of information that are external to the actual content such as relationships between
artists, styles, tags or even richer sources of information such as web reviews and lyrics. Usage-based
approaches track how users listen and purchase music and utilize this information for calculating
similarity. Examples include collaborative ﬁltering in which the commonalities between purchasing
histories of different users are exploited, tracking peer-to-peer downloads or radio play of music pieces
to evaluate their “hotness” and utilizing user generated rankings and tags.
There are trade-offs involved in all these three approaches and most likely the ideal system would be
one that combines all of them intelligently. Usage-based approaches suffer from what has been termed
the “cold-start” problem in which new music pieces for which there is no usage information can not be
recommended. Metadata approaches suffer from the fact that metadata information is frequently noisy or
inaccurate and can sometimes require signiﬁcant semi-manual effort to clean up. Finally content-based
methods are not yet mature enough to extract high-level information about the music.
From a data mining perspective similarity retrieval can be considered a ranking problem. Given a
query music track q and a collection of music tracks D the goal of similarity retrieval is to return a
ranked list of the music tracks in D sorted by similarity so that the most similar objects are at the
top of the list. In most approaches, this ranking is calculated by deﬁning some similarity (or distance)
metric between pairs of music tracks. The most basic formulation is to represent each music track as a
single feature vector of ﬁxed dimensionality x = [x1, x2, . . . , xn]T and use standard distance metrics
such as L1 (Manhattan) or L2 (Euclidean) or Mahalanobis on the resulting high dimensional space.
This feature vector is calculated using audio feature extraction techniques as described in the previous
section. Unless the distance metric is speciﬁcally designed to handle features with different dynamic
ranges the feature vectors are typically normalized for example by scaling all of them so that their
maximum value over the dataset is 1 and their minimum value is 0. A more complex alternative is

1462
CHAPTER 26 Music Mining
to treat each music track as a distribution of feature vectors. This is accomplished by assuming that
the feature vectors are samples of an unknown underlying probability density function that needs to be
estimated. By assuming a particular parametric form for the pdf (for example a Gaussian Mixture Model)
the music track is then represented as a parameter vector θ that is estimated from the data. This way
the problem of ﬁnding the similarity between music tracks is transformed to the problem of somehow
ﬁndinghowsimilararetwoprobabilitydistributionsthatareestimatedfromthesamplesavailabe.Several
such measures of probability distance have been proposed such as histogram intersection, symmetric
Kullback-Leibler divergence and earth mover’s distance. In general computation of such probabilistic
distances are more computationally intensive than geometric distances on feature vectors and in many
cases require numerical approximation as they can not be obtained analytically. An alternative to audio
feature extraction is to consider similarity based on text such as web pages, user tags, blogs, reviews,
and song lyrics. The most common model when dealing with text is the so called “bag of words”
representation in which each document is represented as an unordered set of its words without taking
into account any syntax or structure. Each word is assigned a weight that indicates the importance of
the word for some particular task. The document can then be represented as a feature vector comprising
of the weights corresponding to all the words of interest. From a data mining perspective the resulting
feature vector is no different than the ones extracted from audio feature extraction and can be handled
using similar techniques. In the previous section we described a particular example of text based feature
extraction for the purpose of predicting the country of origin as an artist.
As an example of how a text-based approach can be used to calculate similarity consider the problem
of ﬁnding how similar are two artists A and B. Each artist is characterized by a feature vector consisting
of term weights for the terms they have in common. The cosine similarity between the two feature
vectors is deﬁned as the cosine of the angle between the vectors and has the property that it is not
affected by the magnitude of the vector (which would correspond to the absolute number of times terms
appear and could be inﬂuenced by the popularity of the artist):
sim(A, B) = cos θ =

t A(t) × B(t)

t A(t)2 ×

t B(t)2
.
(26.2)
Another approach to calculating similarity is to assume that the occurrence of two music tracks or
artists within the same context indicates some kind of similarity. The context can be web pages (or page
counts returned by a search engine), playlists, purchase histories, and usage patterns in peer-to-peer
(P2P) networks. Collaborative ﬁltering (CF) refers to a set of techniques that make recommendations to
users based on preference information from many users. The most common variant is to assume that the
purchase history of a particular user (or to some extent equivalently their personal music collections)
is characteristic of their taste in music. As an example of how co-occurrence can be used to calculate
similarity, a search engine can be queried for documents that contain a particular artist A and B, as well
as documents that contain both A and B. The artist similarity between A and B can then be found by:
sim(A, B) =
co(A, B)
min (co(A), co(B)),
(26.3)
where co(X) is the number of pages returned for query X or the co-occurrences of A and B in some
context. A similar measure can be deﬁned based on co-occurrences between tracks and artists in playlists

1.26.5 Similarity Search
1463
and compilation albums based on conditional probabilities:
sim(A, B) = 1
2 ∗
co(A, B)
co(A)
+ co(A, B)
co(B)

.
(26.4)
Co-occurrences can also be deﬁned in the context of peer-to-peer networks by considering the number
of users that have both artists A and B in their shared collection. The popularity bias refers to the problem
of popular artists appearing more similar than they should be due to them occurring in many contexts.
A similarity measure can be designed to down weight the similarity between artists if one of them is
very popular and the other is not (the right-hand part of the following equation):
sim(A, B) = C(A, B)
C(B)
∗

1 −|C(A) −C(B)|
C(Max)

,
(26.5)
where C(Max) is the number of times the most popular artist appears in a context.
1.26.5.1 Evaluation of similarity retrieval
One of the challenges in content-based similarity retrieval is evaluation. In evaluating mining systems
ideally one can obtain ground truth information that is identical to the outcome of the mining algorithm.
Unfortunately this is not the case in similarity as it would require manually sorting large collections
of music in order of similarity to a large number of queries. Even for small collections and number of
queries, collecting such data would be extremely time consuming and practically impossible. Instead the
more common approach is to only consider the top K results for each query, where K is a small number,
and have users annotate each result as relevant or not relevant. Sometimes a numerical discrete score is
used instead of a binary relevance decision. Another possibility that has been used is to assume that tracks
by the same artist or same genre should be similar and use such groupings to assign relevance values.
Evaluation metrics based on information retrieval can be used to evaluate the retrieved results for a
particular query. They assume that each of the returned results has a binary annotation indicating whether
or not it is relevant to the query. The most common one is the F-measure which is a combination of
the simpler measures of Precision and Recall. Precision is the fraction of retrieved instances that are
relevant. Recall is the fraction of relevant instances that are retrieved. As an example, consider a music
similarity search in which relevance is deﬁned by genre. If for a given query of Reggae music it returns
20 songs and 10 of them are also Reggae the precision for that query is 10/20 = 0.5. If there are a total
of 30 Reggae songs in the collection searched then the Relevance for that query is 10/30 = 0.33. The
F-measure is deﬁned as the harmonic mean of Precision P and Recall R.
F = 2 × P × R
P + R .
(26.6)
These three measures are based on the list of documents returned by the system without taking into
account the order they are returned. For similarity retrieval, a more accurate measure is the Average
Precision which is calculated by computing the precision and recall at every position in the ranked
sequence of documents, creating a precision-recall curve and computing the average. This is equivalent
to the following ﬁnite sum:
AP =
n
k=1 P(k) × rel(k)
#relevant documents ,
(26.7)

1464
CHAPTER 26 Music Mining
Table 26.1 20120 MIREX Music Similarity and Retrieval Results
FS
BS
P@5
P@10
P@20
P@50
RND
17
0.2
8
9
9
9
TLN3
47
0.97
48
47
45
42
TLN2
47
0.97
48
47
45
42
TLN1
46
0.94
47
45
43
40
BWL1
50
1.08
53
51
50
47
PS1
55
1.22
59
57
55
51
PSS1
55
1.21
62
60
58
55
SSPK2
57
1.24
59
58
56
53
where P(k) is the precision at list position k and rel(k) is an indicator function that is 1 if the item at
list position (or rank) k is a relevant document and 0 otherwise.
All of the measures described above are deﬁned for a single query. They can easily be extended
to multiple queries by taking their average across the queries. The most common way of evaluating
similarity systems with binary relevance ground-truth is the Mean Average Precision (MAP) which is
deﬁned as the mean of the Average Precision across a set of queries.
The Music Information Retrieval Evaluation Exchange (MIREX) is an annual evaluation benchmark
in which different groups submit algorithms to solve various MIR tasks and their performance is eval-
uated using a variety of subjective and objective metrics. Table 26.1 shows representative results of the
music similarity and retrieval task from 2010. It is based on a dataset of 7000 30-second audio clips
drawn from 10 genres. The objective statistics are the precision at 5, 10, 20, and 50 retrieved items with-
out counting entries by the artist (artist ﬁltering). The subjective statics are based on human evaluation
of approximately 120 randomly selected queries and 5 results per query. Each result is graded with a
ﬁne score (between 0 and 100 with 100 being most similar) and a broad score (0 not similar, 1 somewhat
similar, 2 similar) and the results are averaged. As can be seen all automatic music similarity systems
perform signiﬁcantly better than the random baseline (RND). The differ in terms of the type of extracted
features utilized, the decision fusion strategy (such as simple concatenation of the different feature sets
or empirical combinations of distances from the individual feature sets), and whether post-processing
is applied to the resulting similarity matrix. There is also a strong correlation between the subjective
and objective measures although it is not perfect (for example SSPK2 is better than PSS1 in terms of
subjective measures but worst in terms of objective measures).
1.26.5.2 Cover song detection and audio ﬁngerprinting
In addition to content-based similarity there are two related music mining problems. The goal of audio
ﬁngerprinting is to identify whether a music track is one of the recordings in a set of reference tracks.
The problem is trivial if the two ﬁles are byte identical but can be considerably more challenging when
various types of distortion need to be taken into account. The most common distortion is perceptual

1.26.5 Similarity Search
1465
audio compression (such as the one used for mp3 ﬁles) which can result in signiﬁcant alterations to
the signal spectrum. Although these alterations are not directly perceptible by humans they make the
task of computer identiﬁcation harder. Another common application scenario is music matching/audio
ﬁngerprinting for mobile applications. In this scenario the query signal is acquired through a low quality
microphone on a mobile phone and contains signiﬁcant amount of background noise and interference.
At the same time the underlying signal is the same exact music recording which can help ﬁnd landmark
features and representations that are invariant to these distortions. Cover song detection is the more
subtle problem of ﬁnding versions of the same song possibly performed by different artists, instruments,
and tempo. As the underlying signals are completely different it requires the use of more high level
representations such as chroma vectors that capture information about the chords and the melody of
the song without being affected by timbral information. In addition it requires sophisticated sequence
matching approaches such as dynamic time warping (DTW) or Hidden Markov Models (HMM) to deal
with the potential variations in tempo. Although both of these problems can be viewed as content-based
similarity retrieval problems with an appropriately deﬁned notion of similarity they have some unique
characteristics. Unlike the more classic similarity retrieval in which we expect the returned results to
gradually become less similar, in audio ﬁngerprinting and cover song detection there is a sharper cutoff
deﬁning what is correct or not. In the ideal case copies or cover versions of the same song should receive
a very high similarity score and everything else a very low similarity score. This speciﬁcity is the reason
why typically approaches that take into account the temporal evolution of features are more common.
Audio ﬁngerprinting is a mature ﬁeld with several systems being actively used in industry. As a rep-
resentative example, we describe a landmark-based audio ﬁngerprinting system based on the ideas used
by Shazam, which is a music matching service for mobile phones. In this scheme, each audio track is
represented by the location in time and frequency of prominent peaks of the spectrogram. Even though
the actual amplitude of these peaks might vary due to noise and audio compression their actual location
in the time frequency plane is preserved quite well in the presence of noise and distortion. The landmarks
are combined into pairs and each pair is characterized by three numbers f1, f2, t which are the frequency
of the ﬁrst peak, the frequency of the second peak and the time between them. Both reference tracks and
the query track are converted into this landmark representation. The triplets characterizing each pair are
quantized with the basic idea being that if the query and a reference track have a common landmarks
with consistent timing they are a match. The main challenge in an industrial strength implementation is
deciding on the number of landmarks per second and the thresholds used for matching. The lookup of the
query landmarks into the large pool of reference landmarks can be performed very efﬁciently using hash-
ing techniques to effectively create an inverted index which maps landmarks to the ﬁles they originate.
To solve the audio cover song detection problem there are two issues that need to be addressed.
The ﬁrst issue is to compute a representation of the audio signal that is not affected signiﬁcantly by
the timbre of the instruments playing but still captures information about the melody and harmony
(the combination of discrete pitches that are simultaneously sounding) of the song. The most common
representation used in music mining for this purpose are chroma vectors (or pitch class proﬁles) which
can be thought of as histograms showing the distribution of energy among different discrete pitches.
The second issue that needs to be addressed is how to match two sequences of feature vectors (chroma
vectors in this case) that have different timing and length as there is no guarantee that a cover song is
played at the same tempo as the original and there might be multiple sections each with different timing.

1466
CHAPTER 26 Music Mining
1.26.5.3 Sequence matching
Sequence matching algorithms are used for a variety of tasks in MIR including polyphonic audio and
scores alignment, and real-time score following. More formally the problem is given two sequences of
feature vectors with different lengths and timings ﬁnd the optimal way of “elastically” transforming by
the sequences so that they match each other. A common technique used to solve this problem, and also
frequently employed in the literature for cover song detection, is dynamic time warping (DTW) a speciﬁc
variant of dynamic programming. Given two time series of feature vectors X = (x1, x2, . . . , xM) and
Y = (y1, y2, . . . , yN) with X, Y ∈Rd the DTW algorithm yields an optimal solution in O(M N) time
where M and N are the lengths of the two sequences. It requires a local distance measure that can be
used to compare individual feature vectors which should have small values when the vectors are similar
and large values when they are different:
d : Rd × Rd →R ≥0.
(26.8)
The algorithm starts by building the distance matrix C ∈RM×N representing all the pairwise distances
between the feature vectors of the two sequences. The goal of the algorithm is to ﬁnd the alignment or
warping path which is a correspondence between the elements of the two sequences with the boundary
constraint that the ﬁrst and last elements of the two sequences are assigned to each other. Intuitively for
matching sequences the alignment path will be roughly diagonal and will run through the low-cast areas
of the distance matrix. More formally the alignment is a sequence of points (pi, p j) ∈[1 : M]×[1 : N]
for which the starting and ending points must be the ﬁrst and last points of the aligned sequences, the
points are time-ordered and each step size is constrained to either move horizontally, vertically or diag-
onally. The cost function of the alignment path is the sum of all the pairwise distances associated with
its points and the alignment path that has the minimal cost is called the optimal alignment path and
is the output of the DTW.
Figure 26.5 shows two distance matrices that are calculated based on energy contours of different
orchestra music movements. The left matrix is between two performances by different orchestras of the
FIGURE 26.5
Similarity matrix between energy contours and alignment path using dynamic time warping. (a) Good
alignment (b) bad alignment.

1.26.5 Similarity Search
1467
same piece. Even though the timing and duration of each performance is different they exhibit a similar
overall energy envelope shown by the energy curves under the two axes. The optimal alignment path
computed by DTW is shown imposed over the distance matrix. In contrast the matrix on the right shows
the distance matrix between two unrelated orchestral movements where it is clear there is no alignment
and the optimal alignment path deviates signiﬁcantly from the diagonal.
Hidden Markov Models (HMM) are a probabilistic sequence modeling technique. The system is
modeled as going through a set of discrete (hidden) states over time following Markov transitions, i.e.,
each state only depends on the value of previous state. In regular Markov models the only parameters
are the state transition probabilities. In HMM the states are not directly visible but their probabilistic
output is visible. Each state has an associated probability density function and the goal of HMM training
is to estimate the parameters of both the transition matrix and the state-dependent observation matrix.
For sequence matching the estimated sequence of hidden states can be used.
1.26.5.4 Cover song detection
Cover song detection is performed by applying DTW between all the query song and all the references
and returning as a potential match the one with the minimum total cost for the optimal alignment
path. Typically the alignment cost between covers of the same song will be signiﬁcantly lower than
the alignment cost between two random songs. DTW is a relatively costly operation and therefore this
approach does not scale to large number of songs. A common solution for large scale matching is to
apply an audio ﬁngerprinting type of approach with efﬁcient matching to ﬁlter out a lot of irrelevant
candidates and once a sufﬁcient small number of candidate reference tracks have been selected apply
pair-wise DTW between the query and all of them.
Table 26.2 shows the results of the audio cover song detection task of MIREX 2009 in the so called
“mixed” collection which consists of 1000 pieces that contain 11 “cover song” each represented by 11
different versions. As can be seen the performance is far from perfect but it is still impressive given the
difﬁculty of the problem. An interesting observation is that the objective evaluation measures are not
consistent. For example the RE algorithm performs slightly worse than the SZA in terms of mean average
precision but has better mean rank for the ﬁrst correctly identiﬁed cover. Table 26.3 shows the results of
the MIREX 2009 audio cover song detection task for the Mazurkas collection which consists 11 different
performances/versions of 49 Chopin Mazurkas. As can be seen from the results this is a easier dataset to
ﬁnd covers probably due to the smaller size and more uniformity in timbre. The RE algorithm is based
on the calculation of different variants of chroma vectors utilizing multiple feature sets. In contrast to the
morecommonapproachofscoringthereferencesinarankedlistandsettingupathresholdforidentifying
covers it follows a classiﬁcation approach in which a pair is either classiﬁed as reference/cover or as
Table 26.2 2009 MIREX Audio Cover Song Detection-Mixed Collection
RE
SZA
TA
Mean # of covers in top 10
6.20
7.35
1.96
Mean Average Precision
0.66
0.75
0.20
Mean Rank of ﬁrst correct cover
2.28
6.15
29.90

1468
CHAPTER 26 Music Mining
Table 26.3 2009 MIREX Audio Cover Song Detection—Mazurkas
RE
SZA
TA
Mean # of covers in top 10
8.83
9.58
5.27
Mean Average Precision
0.91
0.96
0.56
Mean Rank of ﬁrst correct cover
1.68
1.61
5.49
reference/non-cover. The SZA algorithm is based on harmonic pitch class proﬁles (HPCP) which are
similar to chroma vectors but computed over a sparse harmonic representation of the audio signal.
The sequence of feature vectors of one song is transposed to the main tonality of the other song in
consideration. A state space representation of embedding m and time delay z is used to represent the
time series of HPCP with a recurrence quantiﬁcation measure used for calculating cover song similarity.
1.26.6 Classiﬁcation
Classiﬁcation is the task of assigning each object (in our case a music track) to one of several pre-deﬁned
categories of interest. In data mining it refers to principled ways of building classiﬁcation systems using
as input a set of objects with associated ground truth classiﬁcation labels which is called the training
set. It is a subset of the more general concept of supervised learning in which an algorithm “learns”
how to improve its performance over a particular task by analyzing the results on this task provided by
humans during a training phase. One of the simplest classiﬁers is based on techniques such as the ones
described in the similarity retrieval section. A new object represented by a vector of attributes/features
is classiﬁed to the category of its nearest neighbor in the training set. A common variant is to consider
the classiﬁcation label of the majority of k nearest neighbors where k is an odd number. In addition a
variety of techniques have been proposed speciﬁcally for this task. They include rule-based classiﬁers,
decision trees, neural networks, support vector machines and various parametric classiﬁers based on
Bayesian decision theory such as Naive Bayes and Gaussian Mixture Models. These techniques differ
in terms of the assumptions they make, the time they take to train, and the amount of data they require
to work well. Their goal is to ﬁt as well as possible the relationship between the attributes or features
that characterize the music tracks and the ground truth class label. The trained model should not only
predict the class label of the tracks it has encountered in training but of new music tracks it has never
encountered before.
Classiﬁcation techniques can be grouped into two large families: generative and discriminative. In
generative approaches the classiﬁcation problem is recast, through Bayesian decision, theory to the
problem of estimating a probability density function from a set of samples (the feature vectors of a
particular class in the training set). They are called generative because the estimated model can be
used to generate new random samples (features). Given a classiﬁcation task of M classes/categories,
c1, c2, . . . , cM and an unknown object (in our case music track) represented as a feature vector x the goal
is to calculate the M conditional probabilities (also referred to as a posteriori probabilities) P(ci|x) where
i = 1, 2, . . . , M. The predicted class label for the unknown vector x will then be the ci corresponding

1.26.6 Classiﬁcation
1469
to the maximum conditional probability. Using the Bayes Rule these conditional probabilities can be
written as:
P(ci|x) = p(x|ci)P(ci)
p(x)
.
(26.9)
The prior probabilities P(ci) can be calculated by simply counting the number of instances belonging to
each class in the training set, so the main challenge is to estimate the probability density function of the
feature vectors given the class p(x|ci). Many classiﬁcation algorithms solve this problem by assuming a
particular parametric form for the probability density function and then estimating the parameters from
the observed samples for the particular class. For example, the simple Naive Bayes classiﬁer assumes
that the probability density function corresponding to each feature is conditionally independent given the
class. Therefore it can be expressed as a the product of normal distributions (one for each feature). The
parameters that need to be estimated are the means and variances of each feature for the particular class.
It can be shown that these correspond to the statistical means and variances of the observed samples
in the training set. In music mining the most common generative modeling technique used is Gaussian
Mixture Models (GMM) in which each class is modeled as a weighted combination of samples from K
Gaussian models.
The main insight behind discriminative approaches is that what is important in a classiﬁcation prob-
lem is not to model the distribution of all the observed feature vectors, but rather to focus on the areas of
the feature space in which the decision of which class the vector belongs is unclear. Essentially discrim-
inative approaches try to directly solve the classiﬁcation problem by making some assumptions about a
discriminative function (i.e., a function that given as input an unknown feature vector returns a predicted
class) and then optimizing the parameters of this function according to some criterion. For example, the
assumption can be that the discriminant function is a linear combination of the features (geometrically
corresponding to a hyperplane in the feature space) and the criterion might be the number of errors in the
training set. In music mining the most common discriminative model used is Support Vector Machines
(SVM). Another way of grouping classiﬁcation algorithms is into parametric approaches in which each
class is characterized by a small set of parameters (both the GMM and SVM classiﬁers are paramet-
ric methods) and non-parametric methods in which there is no explicit parameter representation. The
canonical example of a non-parametric classiﬁer is the Nearest Neighbor rule which simply classiﬁes
an unknown instance with the label associated with the training instance that is closest to it in the feature
space. In many music mining classiﬁcation problems the predicted labels form natural hierarchies. For
example Folk Metal is a subgenre of Heavy Metal and Hostility is a subordinate emotion to Anger. A
straightforward way of solving hierarchical classiﬁcation problems is to apply standard classiﬁcation
techniques for the top level categories and then train classiﬁers for the subordinate categories separately
for each top-level category. One issue with this approach is that by making a hard decision at each level
errors can be propagated. An alternative is to view the hierarchy as a probabilistic graphical model
and essentially compute conditional probabilities for the decisions at each level of the hierarchy using
classiﬁers that output probabilities over all possible classes rather than a single classiﬁcation decision.
1.26.6.1 Genre classiﬁcation
Music can be grouped into categories in many different ways and most of them have been investi-
gated in the literature. Probably the oldest classiﬁcation task that was investigated was musical genre

1470
CHAPTER 26 Music Mining
classiﬁcation. Other tasks include artist/singer/performer classiﬁcation, mood and emotion detection,
instrument recognition and others. The boundaries between categories such as genre labels are fuzzy.
Even though there is considerable agreement when listeners are asked to annotate an unknown piece
of music with a genre label, there are also individual differences. Top-level genres such as classical or
Reggae are more consistently identiﬁed, while more obscure genres such as folk metal or grime are
meaningful only to smaller groups of listeners. Given the subjective nature of human genre annotations
it is unreasonable to expect perfect computer genre classiﬁcation performance (in fact the notion of
perfect performance is only meaningful in relation to some ground truth which in this case will be
inherently subjective). One fascinating ﬁnding is that average listeners are able to classify a music track
with a label from a set of pre-deﬁned top-level genres, with accuracy better than random with exposure
to as little as 250 ms (1/4 of a second or approximately the time it takes to say the word “word”) and
require only 3 s to reach their best classiﬁcation performance. This indicates that low level audio-related
features carry sufﬁcient information for reliable genre classiﬁcation. In a well-known user study (details
can be found in the Further Reading section) a mean subject agreement of about 70% to the genres
assigned by music companies was reported. Further studies have shown that on the same dataset com-
puter classiﬁcation achieves results comparable to some of the “worst” performing humans (69%) but
not as good as the “best” performing humans (95%). In this case the ground-truth is deﬁned as the genre
labels that is assigned by the majority of the 27 users that participated in the study rather some external
authority. Therefore the “worst” and “best” performance refer to subject agreement with the majority
rather than any form of music knowledge. It also has been pointed out that listeners probably can be
clustered into groups so that the perception of genres within a particular group is more consistent than
across groups. In the majority of published work in automatic genre classiﬁcation these issues are not
considered and the ground truth is known and externally provided.
Automatic genre classiﬁcation is one of the most popular topics in music mining to a large extent
due to the simplicity of obtaining ground truth, the availability of datasets for which there are published
results, and the direct mapping to classiﬁcation techniques. It has also served as a good initial test-bed
for experimenting with alternative audio and music feature sets. At the same time for a lot of music of
interest the genre labels are already available (although for some cases like World music they are too
generic to be useful).
1.26.6.2 Emotion and mood classiﬁcation
Listeners can easily identify emotions and moods that are present in pieces of music. It is also known
that music can induce emotions and moods to listeners. Therefore it is desirable to utilize emotion and
mood related information in music retrieval systems. Unlike other types of audio classiﬁcation such as
genre in which the classiﬁcation labels are to some extent pre-deﬁned and readily available for particular
music tracks, one of the challenges in emotion and mood classiﬁcation is deciding what labels should
be used and obtaining them for a particular set of music tracks.
Roughly speaking mood is a more persistent state of mind and can encompass different emotions
whereas emotions are more instinctive. Most of the literature in emotion and mood classiﬁcation relies on
various schemes for describing emotions/moods from psychology. For example Hevner experimentally
found eight adjective groups for describing music based on a user study of 450 subjects listening to 26
pieces of classical music. These groups are: digniﬁed, sad, dreamy, serene, graceful, happy, exciting,

1.26.6 Classiﬁcation
1471
Energy
Calm
Energy
Calm
Calm
Tiredness
Tiredness
Tension
Tiredness
Tension
Tension
Energy
Graceful
Serene
Dreamy
Happy
Sad
Dignified
Vigorous
Exciting
Hevner Adjective Groups
Thayer Emotion Map
FIGURE 26.6
Different ways of organizing emotions.
vigorous and were arranged in a circle in the order they were listed such that the changes are gradual.
Figure 26.6 shows this representation.
Schemes of describing emotion can be roughly divided into two general families: models based on
hierarchies of words describing the different emotions and two dimensional (or sometimes even three
dimensional) continuous emotion spaces where each axis corresponds to a pair of adjectives having
opposite meanings (for example, happy and sad). A particular emotion is characterized as a point in
the emotional space. The use of words to represent emotion (or affect) can be problematic as multiple
emotions can be experienced at the same time and there is individual variability in how a particular
experience is assessed. Figure 26.6 shows an example of emotion grouping and an example of an
emotion space.
In terms of data mining techniques, standard classiﬁcation approaches as the ones described above
can be used. The most common sources of training data in the literature are either audio features, text
features based on analyzing lyrics, or more generally tags which are described below. Hierarchical
classiﬁcation techniques can also be employed to deal with multiple levels of classiﬁcation. In order to
deal with the fact that multiple emotion/mood words might apply to the same music track a multi-label
classiﬁcation approach can be employed. In traditional classiﬁcation there is a single ground truth label
(from a predeﬁned set of categories) for each training instance and a single predicted label (from the same
set of categories) for each testing instance. In multi-label classiﬁcation there are multiple labels (still
from a predeﬁned set of categories) associated with each instance. One can categorize multiple-label
classiﬁcation approaches into two broad families. The ﬁrst family consists of problem transformation
methods that convert a multi-label classiﬁcation problem to a set of standard classiﬁcation problems.
For example each label can be treated separately as a binary classiﬁcation problem with instances
that are annotated with the label being positive examples and instances that do not contain the label
as negative examples. This approach requires training K binary classiﬁers where K is the number of
distinct labels. The second family consists of classiﬁcation approaches that can be directly applied to

1472
CHAPTER 26 Music Mining
multi-label classiﬁcation problems. A simple example is the Nearest Neighbor classiﬁer in which a new
instance is annotated by all the labels that are associated with the training instance that is closest to it
in the feature space.
When dealing with a continuous emotional space both the ground truth and the predicted outcome
are continuous rather than categorical. In data mining, regression refers to the prediction of a continuous
output label from a set of continuous attributes. There is a large variety of regression algorithms which
similarly to classiﬁcation algorithms can be categorized as parametric and non-parametric. In many cases
they are based on similar ideas to classiﬁcation algorithms. For example support vector regression (SVR)
is a counterpart to support vector machines (SVMs) and ensemble boosting regression (AdaBoost.RT)
is the counterpart of the AdaBoost ensemble classiﬁcation algorithm.
1.26.6.3 Evaluating classiﬁer performance
The goal of a classiﬁer is to be able to classify objects it has not encountered before. Therefore in order
to get a better estimate of its performance on unknown data it is necessary to use some of the instances
labeled with ground truth for testing purposes and not take them into account when training. The most
common such evaluation scheme is called K-fold cross-validation. In this scheme the set of labeled
instances is divided into K distinct subsets (folds) of approximately equal sizes. Each fold is used for
testing once with the K −1 remaining folds used for training. As an example if there are 100 feature
vectors then each fold will contain 10 feature vectors with each one of them being used one time for
testing and K −1 times for training. The most common evaluation metric is classiﬁcation accuracy
which is deﬁned as the percentage of testing feature vectors that were classiﬁed correctly based on
the ground truth labels. When classifying music tracks a common post-processing technique that is
applied is the so-called artist ﬁlter which ensures that the feature vectors corresponding to tracks from
the same artist are not split between training and testing and are exclusively allocated only to one of
them. The rationale behind artist ﬁltering is that feature vectors from the same artist will tend to be
artiﬁcially related or correlated due to similarities in the recording process and instrumentation. Such
feature vectors will be classiﬁed more easily if included in both training and testing and maybe inﬂate
the classiﬁcation accuracy. Similar considerations apply to feature vectors from the same music track
if each track is represented by more than one feature vector, in which case a similar track ﬁlter should
be applied.
The most common evaluation metric for automatic classiﬁcation is accuracy which is simply deﬁned
as the number of correctly classiﬁed instances in the testing data. It is typically expressed as a percentage.
Additional insight can be provided by examining the confusion matrix which is a matrix that shows the
correct classiﬁcations in the diagonal and shows how the misclassiﬁcation are distributed among the
other class labels.
Table 26.4 shows classiﬁcation results from MIREX 2010. The classiﬁcation results are shown as
percentage accuracy. They are sorted based on the performance on the largest of the datasets considered
(the Genre column). This dataset consists of 7000 clips each 30 s long. The following ten genres
are all equally represented (700 clips for each genre): blues, jazz, country, baroque, classical, romantic,
electronica, hiphop, rock, metal. The mood dataset consists of 600 30-second clips classiﬁed into 5 mood
clusters. The labeling was done by human judges. Table 26.5 shows the 5 mood clusters used. The Latin
datasetconsistsof322audioﬁlesrepresenting10Latinmusicgenres(axe,bachata,bolero,forro,gaucha,
merengue, pagode, sertaneja, tango) sourced from Brazil and labeled by music experts. These genres are

1.26.6 Classiﬁcation
1473
Table 26.4 2010 MIREX Classiﬁcation Tasks
Genres
Mood
Latin
SSPK1
73.64
63.83
79.86
BRPC1
70.67
58.67
70.75
BRPC2
70.00
59.00
–
GR1
69.80
60.67
60.18
FE1
69.64
60.83
69.32
RRS1
67.89
61.67
62.53
BPME2
67.66
54.67
–
MW1
67.57
54
37.93
TN4
66.66
57.5
48.54
WLB1
66.11
55.5
68.11
GP1
64.27
63.17
66.9
RK1
64.03
54.83
50.85
TN1
63.37
55.5
36.83
MBP1
63.29
54.0
61.02
HE1
61.16
54.17
50.76
RJ1
61.07
54.83
61.98
JR2
60.94
51.17
60.74
JR4
60.93
51.17
60.27
RK2
60.54
47.67
42.14
MP2
60.43
36.17
57.30
JR1
60.01
46.33
56.49
RJ2
59.73
50.17
59.75
JR3
59.54
46.83
57.95
WLB2
48.50
57.67
34.99
Table 26.5 2010 MIREX Mood Clusters
Cluster 1
Passionate
Rousing
Conﬁdent
Boisterous
Rowdy
Cluster 2
Rollicking
Cheerful
Fun
Sweet
Amiable
Good natured
Cluster 3
Literate
Poignant
Wistful
Bittersweet
Autumnal
Brooding
Cluster 4
Humorous
Silly
Campy
Quirky
Whimsical
Witty
Cluster 5
Aggressive
Fiery
Tense
Intense
Volatile
Visceral
more differentiated by rhythmic characteristics than the other datasets considered. For computing these
classiﬁcation accuracies, for all datasets, a three-fold cross-validation with artist ﬁlter (i.e., all songs of
an artists are either part of the training or testing) approach was used. The algorithms differ in terms of
the exact details of the feature extraction and the type of supervised learning classiﬁer they utilize.

1474
CHAPTER 26 Music Mining
1.26.6.4 Clustering
Clusteringistheproblemofpartitioningaﬁnitesetofobjects(musictracksinourcase)intogroupscalled
clusters such that objects belonging to the same cluster are similar to each other, and objects belonging
to different clusters are dissimilar. Similarly to classiﬁcation, this is a well investigated problem in the
area of data mining for which several algorithms have been proposed. In music mining it can be used to
automatically organize a music collection into coherent groups, or identify users with different musical
interest as well as automatically construct hierarchies of music tags. Similarly to classiﬁcation typically
some combination of audio features and text such as lyrics is utilized in music clustering. One of the
classic algorithms for clustering is K-means which partitions N instances into K clusters such that each
instance belongs to the “nearest” cluster and clusters are characterized by the mean of the instances that
are assigned to them. The most common algorithm for K-Means clustering uses an iterative reﬁnement
technique consisting of two steps. In the ﬁrst assignment step each instance is assigned to the cluster
with the closest mean. The initial means can be either random or somehow spread in the feature space.
In the second update step the means characterizing each cluster are updated to reﬂect the instances
that have been assigned to them. The two steps are repeated with this new set of cluster means until
the assignments no longer change. A somewhat similar approach can be used with the so called EM
algorithm for ﬁnding clusters based on a Gaussian Mixture Model.
1.26.7 Tag annotation
The term “tag” refers to any keyword associated to an article, image, video, or piece of music on the
web. In the past few years there has been a gradual shift from manual annotation into ﬁxed hierarchical
taxonomies, to collaborative social tagging where any user can annotate multimedia objects with tags
(so called folksonomies) without conforming to a ﬁxed hierarchy and vocabulary. For example, Last.fm
is a collaborative social tagging network which collects roughly 2 million tags (such as “saxophone,”
“mellow,” “jazz,” “happy”) per month and uses that information to recommend music to its users.
Another source of tags are “games with a purpose” where people contribute tags as a by-product of
doing a task that they are naturally motivated to perform, such as playing casual web games. For example
TagATune is a game in which two players are asked to describe a given music clip to each other using
tags, and then guess whether the music clips given to them are the same or different.
Tags can help organize, browse, and retrieve items within large multimedia collections. As evidenced
by social sharing websites including Flickr, Picasa, Last.fm, and You Tube, tags are an important com-
ponent of what has been termed as “Web 2.0.” The goal of automatic tag annotation is to automatically
predict tags by analyzing the musical content without requiring any annotation by users. Such systems
typically utilize signal processing and supervised machine learning techniques to “train” autotaggers
based on analyzing a corpus of manually tagged multimedia objects. Music classiﬁcation can be viewed
as a specialized restricted form of tag annotation where there is ﬁxed vocabulary and only one tag
applies to each music track.
There has been considerable interest for automatic tag annotation in multimedia research. Automatic
tags can help provide information about items that have not been tagged yet or are poorly tagged.
This avoids the so called “cold-start problem” in which an item can not be retrieved until it has been
tagged. Addressing this problem is particularly important for the discovery of new items such as recently

1.26.7 Tag Annotation
1475
released music pieces in a social music recommendation system. Another also automatic approach is
to use text mining techniques to associate tags to particular music tracks.
The training data for an automatic tag annotation system is typically represented as a tag-track matrix
X where each element x[t, s] represents the strength of association between a particular tag t, and a
particular song s. For example, the strength of association for a particular entry in the matrix can be the
number of users that annotated that particular song with a particular tag.
From a machine learning perspective automatic tag annotation can be viewed as a variation of multi-
label classiﬁcation. In contrast to traditional classiﬁcation in which each item is assigned one of k
mutually exclusive class labels, in multi-label classiﬁcation each item can be assigned to multiple labels
(tags). Many different approaches to multi-label classiﬁcation have been proposed in the literature.
They leverage feature information computed from a training set of examples annotated with multiple
labels to train models that can subsequently be used to predict labels for new examples. There are some
unique characteristics and related challenges when the ground truth tag data is obtained from the web.
The ground truth training data is noisy in the sense that the tags can contain synonyms (“calm” and
“mellow”), misspellings (“chello”) and hierarchical relations (“symphony” and “classical”). In addition
the data is sparse meaning that there can be few training examples for a given tag. Finally, the absence
of a tag cannot always be taken to mean that the tag is not applicable, as it might be the case that the
users have simply not yet considered that tag for the particular multimedia item. This phenomenon has
been termed weak labeling in contrast to regular strong labeling.
A common straightforward approach to auto-tagging is to train K binary classiﬁers that classify
each of the K tags independently. Instances that contain a tag are considered positive, and instances
that do not containx it are considered negative. A new instance is annotated with all the tags that the
binary classiﬁers predict as positive. Topic models such as Latent Dirichlet Allocation (LDA) make
the assumption that tags can be grouped into a unknown number of higher level groups and build a
probabilistic model around this assumption. Pre-processing and post-processing techniques that take
into account semantic similarities between tags can also be used to improve the results. For example
misspellings can be merged in a preprocessing step. A data driven approach that attempts to capture
some of the dependencies between tags directly from the data is called stacking.
Stacking is a method of combining the outputs of multiple independent classiﬁers for multi-label
classiﬁcation. The ﬁrst step of using stacking for multi-label classiﬁcation is to train |V | individual tag
classiﬁers using a training set (xi, yi), where xi denotes the feature vector for instance i and yi is the
associated set of labels. The output of these classiﬁers (binary or probabilistic) f1(x), f2(x), . . . , f|V |(x)
where x is the input feature vector that can then used as a feature to form a new feature set. Let the new
feature set be z1, z2, . . . , z|V |. This feature set, together with the original ground truth labels (zi, yi), is
then used for training a second stage of stacking classiﬁers. The goal is to have the stacking classiﬁers
make use of information like the correlation between tags and the accuracy of the ﬁrst stage classiﬁers
to improve the annotation performance. For example suppose that the stage 1 performance for the tag
“opera” is not very good but that most of the examples with the tag “opera” receive high probabilities for
the tags “classical” and “voice” at stage 1. The stacking stage 2 can take into account this information
from other tags and improve annotation performance: something not possible during stage 1, in which
each tag is treated independently. Figure 26.7 shows this process as a block diagram.
Evaluation of automatic tagging systems is not trivial. In general, the evaluation metrics used are gen-
eralizations of commonly used evaluation metrics for single label classiﬁcation. An annotated “training”

1476
CHAPTER 26 Music Mining
FIGURE 26.7
Stacking for automatic music tag annotation.
set of instances is used to “train” the classiﬁer, and then used to “predict” the tags for a set of instances
in a “test” set. We can also distinguish between evaluation metrics that are based on a predicted set
of discrete tags (sometimes referred to as the annotation task) and ones that are based on a predicted
set of tag afﬁnities/probabilities (sometimes referred to as the ranking task) for each instance in the
testing set. A common approach is to treat any classiﬁcation decision equally and simply count the
number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) to
derive well-known measures such as precision, recall and F-measure. In the case of probabilistic output,
multiple score thresholds can be considered as possible boundaries for binarization, in which case it
is common to use Receiver operating characteristic (ROC) curves. An ROC curve is a plot of the true
positive rate as a function of the false positive rate. The ROC curve can be summarized by the area
under curve (AUC-ROC) which can be found by integrating the ROC curve and is upper bound by 1.0.
Random guessing in a retrieval task results in an AUC-ROC of 0.5. Different tag annotation methods
can have different operating point characteristics in terms of the trade-off between true positives and
false positives. A ﬁnal complication is that calculating metrics over the entire set of tags can be mis-
leading as good performance on “popular” tags that appear in many instances will dominate. However,
typically a more balanced response where all tags are considered is desired. In order to address this
concern, evaluation metrics averaged across tags are used. Finally it is important to note that in most
cases evaluation metrics based on annotated ground truth underestimate what the true performance of
the system would be if evaluated by humans. The reason is that frequently, predicted tags that humans
would consider applicable are not present in the ground truth and therefore evaluated as mistakes.
Table 26.6 shows both binary (F-measure) and afﬁnity (AUC-ROC) evaluation metrics for automatic
tag annotation from MIREX 2009 using the MajorMiner data-set. This data-set consists of 2300 clips

1.26.7 Tag Annotation
1477
Table 26.6 MIREX 2009 Tag Annotation Results (MajorMiner, Mood)
Measure
BP2
CC4
GP
GT2
HBC
LWW2
MajorMiner
F-measure
0.29
0.26
0.01
0.29
0.04
0.31
AUC-ROC
0.76
0.75
–
0.79
0.74
0.80
Mood
F-measure
0.19
0.18
0.08
0.21
0.06
0.70
AUC-ROC
0.63
0.64
–
0.65
0.66
0.80
selected at random from 3900 tracks. Each clip is 10 s long. The clips represent a total of 1400 different
tracks on 800 different albums by 500 artists. To give a sense of the diversity of the music collection,
the following genre tags have been applied to these artists, albums, and tracks on Last.fm: electronica,
rock, indie, alternative, pop, britpop, idm, new wave, hiphop, singer-songwriter, trip-hop, post-punk,
ambient, jazz. The MajorMiner game has collected a total of about 73,000 taggings, 12,000 of which
have been veriﬁed by at least two users. In these veriﬁed taggings, there are 43 tags that have been
veriﬁed at least 35 times, for a total of about 9000 veriﬁed uses. These are the tags are used in this
task. The table also shows the results in the Mood dataset which consists of 3469 unique songs and 135
mood tags organized into 18 mood tag groups which where used as tags. The songs are Western pop
songs mostly from the USPOP collection. Each song may belong to multiple mood tag groups. The
main rationale on songs selection is: if more than one tag in a group were applied to a song, or if one
tag in a group was applied more than once to a song, this song is marked as belonging to this group.
Inthistask,theparticipatinggroupssubmittedvariantsofthesamealgorithm.Duetospaceconstraints
we only show the top performing variant of each group. The evaluation was performed using 3-fold
cross validation using artist ﬁltering, i.e., the training and test sets contained different artists. The results
are averaged across tags. The BP2 system is based on spectral and chroma features and for classiﬁcation
applies both feature selection and model selection. For model selection instead of using classiﬁcation
accuracy, as is common, they use a custom measure designed to deal better with unbalanced data-sets
and over-ﬁtting. Each tag is classiﬁed separately. The CC4 system is based on MFCC and delta-MFCC
that are then transformed into a super-vector using a universal background approach. In this approach,
a Gaussian Mixture Model is trained using a large corpus of music that is separate from the ﬁles
considered for tag annotation. This universal background model is then adapted to better ﬁt the data of
the tag dataset. The parameters of the adapted GMM are then used as features and an ensemble classiﬁer
consisting of a linear support vector machine and AdaBoost classiﬁer is utilized for classiﬁcation. Each
tag is treated separately. The GPP system uses spectral and chroma features followed by GMM modeling
both at the frame level and track level. Each tag is treated as a separate classiﬁcation problem. GT2
also utilized spectral and chroma features followed by a layer of tag-speciﬁc linear support vector
machines followed by a stacking layer of also linear support vector machines. The HBC system utilizes
bag of codewords representation (basically an occurrence histogram of different codewords which are
computed by k-means clustering-this approach is also known as vector quantization) with MFCC and

1478
CHAPTER 26 Music Mining
delta MFCC as the original feature vectors. For tag annotation a codeword bernoulli average approach is
utilized. Unfortunately it is hard to draw any reliable conclusions from these results. The exact details of
the features and tag annotation approach are not the same, so it is hard to tell if the observed differences
between algorithms are due to the feature set used or the tag annotation algorithm.
1.26.8 Visualization
Visualization is the display of information in such a way that relationships among data items and
attributes can be analyzed and understood. It takes advantage of the strong pattern recognition properties
of the human visual system. Traditional visualizations of music, such as the time domain waveforms
and spectrograms, popular in audio editors convey very little information about the musical content and
are focused on single music tracks. In this section we focus on visualizations of audio collections and
associated information about them with speciﬁc emphasis on visualizing large collections of music.
As we have already discussed a common approach in automatic analysis of music is to represent each
track as a feature vector of ﬁxed dimensionality (typical numbers range from 10 to 1000 dimensions).
Dimensionality reduction methods try to transform these feature vectors to either 2 or 3 dimensions
so that they can be visualized in a natural way by considering the transformed feature vector as a
coordinate/point in a visual space. One of the most common technique for dimensionality reduction
that can be used for visualization is Principal Component Analysis (or PCA). An alternative is the use
of self-organizing maps (SOM) which attempt to perform both dimensionality reduction and clustering.
These two methods underlie the majority of proposed music collection visualization systems. In this
section we describe these two algorithms and describe generic visualization interfaces that can be built
using them. Additional details such as how the user interacts with the generated visualization, the ability
to zoom in and out, display hierarchies, etc. can be found in the further reading section.
Principal component analysis (PCA) converts a set of feature vectors with possibly correlated
attributes into a set of feature vectors where the attributes are linearly uncorrelated. These new trans-
formed attributes are called the principal components and when the method is used for dimensionality
reduction their number is less than the number of original attributes. Intuitively this transformation
can be understood as a projection of the original feature vectors to a new set of orthogonal axes (the
principal components). The projection is such that each succeeding axis explains the highest variance
of the original dataset possible, with the constraint that it is orthogonal to the preceding component. In
a typical application scenario, where each song is represented by a 70 dimensional feature vector, the
application of PCA to these feature vectors can then be used to convert them to 3 dimensional feature
vectors which can be visualized as points in a 3D space. A common way of calculating PCA is based
on the covariance matrix which is deﬁned as:
C = 1
N

B × BT ,
(26.10)
where T denotes the transpose operator and B is the matrix resulting from subtracting the empirical mean
of each dimension of the original data matrix consisting of the N feature vectors. The eigenvectors and
eigenvalues of this covariance matrix are then computed and sorted in order of decreasing eigenvalue
and the ﬁrst K where K is the desired number of reduced dimensions are selected as the new basis

1.26.8 Visualization
1479
vectors. The original data can then be projected into the new space spanned by these basis functions,
i.e., the principal components. PCA is a standard operation in statistics and it is commonly available in
software package dealing with matrices.
One of the potential issues with PCA for music visualization is that , because it tries to preserve as
much as possible the distances between points from the original feature space to the transformed feature
space, it might leave large areas of the available visualized space empty of points. This is particularly
undesirable in touch based interfaces, such as tablets, where in the ideal case, everywhere a user might
press should trigger some music. Self-organizing maps are an approach that attempts to perform both
dimensionality reduction and clustering while mostly preserving topology but not distances. It can
result in more dense transformed feature spaces that are also discrete in nature in contrast to PCA which
produces a transformed continuous space that needs to be discretized.
The SOM is a type of neural network used to map a high dimensional input feature space to a
lower dimensional representation. It facilitates both similarity quantization and visualization. It was
ﬁrst documented in 1982 and since then it has been applied to a wide variety of diverse clustering
tasks. The SOM maps the original d-dimensional feature vectors X ∈Rd to two discrete coordinates
I ∈[1 . . . M] and J ∈[1 . . . N] on a rectangular grid.
The traditional SOM consists of a 2D grid of neural nodes each containing a n-dimensional vector,
x(t) of data. The goal of learning in the SOM is to cause different neighboring parts of the network to
respond similarly to certain input patterns. This is partly motivated by how visual, auditory and other
sensory information is handled in separate parts of the cerebral cortex in the human brain. The network
must be fed a large number of example vectors that represent, as closely as possible, the kinds of vectors
expected during mapping. The data associated with each node is initialized to small random values
before training. During training, a series of n-dimensional vectors of sample data are added to the map.
The “winning” node of the map known as the best matching unit (BMU) is found by computing the
distance between the added training vector and each of the nodes in the SOM. This distance is calculated
according to some pre-deﬁned distance metric which in our case is the standard Euclidean distance on
the normalized feature vectors.
Once the winning node has been deﬁned, it and its surrounding nodes reorganize their vector data
to more closely resemble the added training sample. The training utilizes competitive learning. The
weights of the BMU and neurons close to it in the SOM lattice are adjusted towards the input vector.
The magnitude of the change decreases with time and with distance from the BMU. The time-varying
learning rate and neighborhood function allow the SOM to gradually converge and form clusters at
different granularities. Once a SOM has been trained, data may be added to the map simply by locating
the node whose data is most similar to that of the presented sample, i.e., the winner. The reorganization
phase is omitted when the SOM is not in the training mode.
The update formula for a neuron with representative vector N(t) can be written as follows:
N(t + 1) = N(t) + (v, t)α(t)(x(t) −N(t)),
(26.11)
where α(t) is a monotonically decreasing learning coefﬁcient and x(t) is the input vector. The neigh-
borhood function (v, t) depends on the lattice distance between the BMU and neuron v.
Figure 26.8 illustrates the ability of the automatically extracted audio features and the SOM to rep-
resent musical content. The top subﬁgures (a)–(d) show how different musical genres are mapped to
different regions of the SOM grid (the black squares are the ones containing one or more songs from

1480
CHAPTER 26 Music Mining
FIGURE 26.8
Topological mapping of musical content by the self-organizing map. (a) Classical, (b) metal, (c) hiphop,
(d) rock, (e) Bob Marley, (f) radiohead, (g) Led Zappelin (h) Dexter Gorden.
each speciﬁc genre). As can be seen Classical, Heavy Metal and HipHop are well-localized and distinct
whereas Rock is more spread out reﬂecting its wide diversity. The SOM is trained on a collection of 1000
songs spanning 10 genres. The bottom subﬁgures (e)–(h) show how different artists are mapped to dif-
ferent regions of the SOM grid. The SOM in this case is trained on a diverse personal collection of 3000
songs spanning many artists and genres. It is important to note that in all these cases the only information
used is actual audio signal that is automatically analyzed and no meta data. The locations of the gen-
res/artists are emergent properties of the SOM and demonstrate how it organizes semantically the data.
1.26.9 Advanced music mining
In addion to the music mining tasks that have been presented in this chapter, there are several additional
topics and tasks that are more brieﬂy described in this section. The decision to not cover them in more
detail, had more to do with the limited amount of published work related to them and their usage of
more recent and complex mining algorithms, rather than their importance. In many cases, they also lack
commonly available data-sets and agreed upon evaluation methodologies. Multiple instance learning
is a classiﬁcation technique in which ground truth is provided for sets (bags) of instances rather than
individual instances. A classic example in music mining is that frequently it is possible to obtain
classiﬁcation or retrieval ground truth for artists but the desired classiﬁcation granularity is for songs.
As multiple songs correspond to the same artist and the ground truth labeling does not necessarily apply
to all of them, this problem is a natural ﬁt for multiple instance learning. Semi-supervised learning is

1.26.9 Advanced Music Mining
1481
a type of machine learning that makes use of both labeled and unlabeled data for training. It useful in
scenarios where there is a limited amount of ground truth data available but large amounts of data that
are unlabeled are also available.
1.26.9.1 Symbolic music mining
Music especially classical and to some extent popular music can be represented symbolically using
representations that are similar to a music score, i.e., they encode which notes are played and at what
time. A score is an abstract and structured representation. It can be interpreted in different ways by
varying timing or even instrumentation depending on the performer. It can be viewed as a platonic ideal
of a song that is normalized with respect to many details such as timbre, pitch and timing variations
that complicate audio analysis. At the same time, because the information is discrete and structured
it enables types of processing that are very difﬁcult to perform with audio signals. For example it is
possible to search efﬁciently for melodic patterns in large collections of symbolic data. There are several
possible symbolic representations for music. The simplest form consists of simply the start and duration
of a set of discrete notes or pitches. MIDI (Music Interface for Digital Instruments) ﬁles mostly contain
this information. More expressive formats, such as Music XML can express additional information
FIGURE 26.9
Piano roll symbolic representation (a) and associated spectrogram (b).

1482
CHAPTER 26 Music Mining
such as graphic characteristics of the musical scores. Figure 26.9 relates a spectrogram representation
with a corresponding symbolic representation using the so called piano roll notation. Algorithms from
text mining, discrete mathematics and theoretical computer science are used in this research. it is also
frequently driven by musicological considerations.
In the same way that the canonical example of music mining for audio signals is automatic genre
classsiﬁcation, the canonical example of symbolic music information is searching for a particular mono-
phonic melodic sequence in a database of melodies. This task has, for example, applications in Query-
by-humming (QBH) systems where the user sings a melody and the best match is returned as a music
ﬁle that can be played. Even though some systems only check for exact matches, in most cases, some
form of approximate matching is utilized. Melodies are represented as one dimensional strings of char-
acters, where each characters represents either a discrete pitch or interval between succeeding pitches.
The large literature on string matching algorithms such as editing distances, ﬁnding the longest com-
mon subsequence, or ﬁnding occurances of one string in another can then be utilized for symbolic
music mining. When melody matching needs to be performed in polyphonic audio or the query itself is
polyphonic more sophisticated techniques based on computational geometry are utilized. Finally most
of the music mining tasks described for audio signals can also be applied to symbolic music signals
provided an appropriate feature representation is used. For example features computed from MIDI ﬁle
such as average interval size, highest note, lowest note, etc. can be used as a feature representation for
classiﬁcation tasks.
1.26.9.2 Bridging symbolic and audio music mining
Automatic transcription refers to the process of essentially converting an audio recording to a symbolic
representation. Figure 26.9 shows this process graphically. As can been seen the overlapping harmonic
structure of notes makes it harder to separate them in a polyphonic context. Automatic transcription is a
hard problem, and although signiﬁcant progress has been made, it is still not very accurate for real world
recordings. There are various simpler tasks related to bridging audio and symbolic representations that
have been investigating with more successful results. Audio chord and key detection compute an abstract
symbolic representation, i.e., a list of chord symbols for a music track. As an example application, using
dynamic time warping (DTW) over chroma representations it is possible to ﬁnd the correspondence
between a symbolic representation and an audio recording. This problem is termed polyphonic audio
alignment or score following when it is performed in a real-time fashion. Combinations of symbolic
and audio representations can also be used to enable new forms of querying musical data. For example
in query-by-humming systems, the user sings or hums a melody and an audio recording that contains
the melody is returned. It is also possible to analyze the internal structure of a music piece in terms of
repeated sections such as the chorus or a bridge. This process has been termed structure analysis. By
combining symbolic and audio representations we gain more insight in the complexity and structure of
music. In turn, this can inform the more general music mining tasks we have described in this chapter.
A task that has received some attention in the popular press but for which there has been no convincing
demonstration is hit song science or popularity prediction. The idea is that somehow by analyzing audio
and text features one might be able to predict whether a song will be a hit or how popular it will be
before it is released on the market.

1.26.10 Software and Datasets
1483
1.26.10 Software and datasets
Although frequently researchers implement their own audio feature extraction algorithms, there are sev-
eral software collections that are freely available that contain many of the methods described in this chap-
ter. They have enabled researchers more interested in the data mining and machine learning aspects of
music analysis to build systems more easily. They differ in the programming language/environment they
arewritten,thecomputationalefﬁciencyoftheextractionprocess,theirabilitytodealwithbatchprocess-
ing of large collections, their facilities for visualizing feature data, and their expressiveness/ﬂexibility
in describing complex algorithms.
Table 26.7 summarizes information about some of the most commonly used software resources as
of the year 2012. The list is by no means exhaustive but does provide reasonable coverage of what
is available in 2012. Some of these links contains collections of code and others more integrated
frameworks. Most of the toolkits support some form of audio feature extraction. Some also include
music mining algorithms but in many cases researchers rely on other general purpose machine learning
and data mining software. Several of the ﬁgures in this chapter were created using Marsyas and some
using custom MATLAB code. In general, although there are exceptions, software in MATLAB tends
to be less efﬁcient and more suited for prototyping than C/C++ software that is more efﬁcient for large
scale problems. An alternative to using a software toolkit is to utilize audio features provided by a web
service such as the Echonest API.
Over time, several audio datasets and their associated ground truth have been collected and used
to evaluate different music mining tasks. For more mature music mining tasks they have enabled, to
some degree, comparison of different systems and reproducibility of results. Table 26.8 summarizes
information about some of the common datasets used and their characteristics as available in 2012.
Almostallofthedatasetscanbeusedforexperimentsinminingespeciallyfortasksthatcanbeperformed
Table 26.7 Software for Audio Feature Extraction (in 2012)
Name
URL
Programming Language
Auditory Toolbox
tinyurl.com/3yomxwl
MATLAB
CLAM
clam-project.org/
C++
D.Ellis Code
tinyurl.com/6cvtdz
MATLAB
HTK
htk.eng.cam.ac.uk/
C++
jAudio
tinyurl.com/3ah8ox9
Java
Marsyas
marsyas.info
C++/Python
MA Toolbox
pampalk.at/ma/
MATLAB
MIR Toolbox
tinyurl.com/365oojm
MATLAB
Sphinx
cmusphinx.sourceforge.net/
C++
VAMP plugins
www.vamp-plugins.org/
C++

1484
CHAPTER 26 Music Mining
Table 26.8 Datasets for Music Mining (in 2012)
Name
URL
Clips
Ground Truth
BEATLES
isophonics.net/datasets
179
Structure, Chord Beat
BILLBOARD
tinyurl.com/9mocd8y
649
Structure, Chord, Beat
CAL500
tinyurl.com/9j98jj6
500
Tags
GTZAN
tinyurl.com/9rff8js
1000
Genre, Key
LASTFM
tinyurl.com/9obzlfn
20K (artists)
Tags, Collaborative Filtering
LATIN
–
3227
Genres
MGNTUNE
tinyurl.com/8vkcoqx
25863
Tags
MSD
tinyurl.com/9mr4j8x
1000000
Genre, Tags, Ratings
RWC
tinyurl.com/cb9tpfk
315
Genre, Chords, Structure
YAHOO
tinyurl.com/c8pbeb8
717 million
Ratings
without ground truth such as similarity retrieval. In addition, several of them provide ground truth for
one or more tasks. In many cases, they also contain computed audio features which is particular useful
for researchers coming from the more general data mining research. The academic music information
retrieval community has been fortunate to have help from industry in creating datasets. For example,
the Magnatagatune (MGTTUNE) dataset was created with help from the Magnatune recording label.
Another example of a dataset created with help from industry (EchoNest) is the recent and impressive
in scope Million Song Dataset (MSD). It is a freely-available collection of audio features and metadata
for a million popular music tracks. It also contains additional data such as sets of cover songs, lyrics,
song-level tags and similarity and user data. Several datasets are also associated with MIREX tasks. In
many cases, they are not accessible for experimentation and participants have to submit their algorithms
which are executed by the MIREX organizers in order to get evaluation results.
1.26.11 Open problems and future trends
Music information retrieval and more speciﬁcally music mining are relatively new emerging research
areas so there many unexplored directions with signiﬁcant challenges that can be investigated. Music
information is multi-faceted, multi-disciplinary, multi-cultural, multi-modal and all these aspects com-
plicate music mining. In this section some open problems and future trends are brieﬂy described. In
many of the music mining tasks that have been described in this chapter the assumptions they make are
at the extremes of more nuanced differences encountered in the real world. For example music recom-
mendation systems are either general in the sense that for a particular query they recommend the same
set of similar tracks independently of who the user is, or they are highly personalized. Similarly, human
genre annotations is not consistent but at the same time is not completely personalized. The reality is
that probably listeners form social groups/clusters with similar taste. There is a lot of interesting work
that can be done to leverage this group membership for more effective recommendations. There is a
lot of context information that can also be taken into account. Typical listeners have different listening

1.26.11 Open Problems and Future Trends
1485
preferences depending on the time of the day or the activity they are performing. Given the ubiquity
of mobile devices that have geolocation capabilities can we leverage this information to adjust music
recommendation to particular activities for a particular user? Recent advances in neuro-imaging have
also made possible the monitoring of brain activity while listening to music. The resulting data is high
dimensional and difﬁcult to interpret but maybe in the future it will help lead to a better understanding of
how our brain interprets music. Such understanding might radically change how we think about music
mining.
A big area of future research is audio feature extraction. Existing features are rather crude, low level
statistical descriptions that clearly do not capture a lot of the structure and complexity of music. A
particularly interesting area that is receiving a lot of attention at the moment in the machine learning
community is Deep Belief Networks (DBN). They are a family of machine learning algorithms that
are able to learn higher level abstract representations from lower level input signals in the context of
classiﬁcation problems. Another interesting challenge is that music unfolds over time at multiple levels.
Many existing approaches to audio feature extraction and music mining ignore this temporal evolution
or approximate it using rather crude models of dynamics. The majority of existing systems utilize one
source of information and even in the cases where multiple source of information are utilized, their
fusion is performed simply and directly. The integration of information from multiple facts at many
different time scales is a big challenge for existing mining algorithms.
Another big limiting assumption that the majority of existing algorithm for music mining make is
that the music is characterized statistically as a mixture without taking into account that it is composed
of individual sound sources. At the same time, it is clear that as humans we pay a lot of attention and are
able, to a large extent, to characterize and follow these individual sound sources in complex mixtures
over time. This is true for both musically trained listeners and ones that are not. Separating a complex
mixture of sounds to the individual components is called sound source separation and is a problem with
a lot of existing literature but still far away from being solved. Even if it is solved it is unclear how all
these individual sound sources, and their corresponding information can be combined for higher level
music mining tasks.
Historically MIR originated from work in digital libraries and text retrieval. Therefore it has retained
this focus on processing of large archives of existing music. However, new music is created, performed
every day and increasingly computers are used for its production, distribution and consumption. There
is a lot of potential in using MIR techniques in the context of music creation and especially live music
performance. However, many of the developed techniques are not designed to work in real-time and be
interactive.
The amount of data that needs to be processed constantly poses scalability challenges. For example
if we expand the scope of research from recorded music by labels to all the amateur video and audio
recordings uploaded on the web it will increase the number of ﬁles to be processed by one or maybe
even two orders of magnitude. As audio signal processing and machine learning techniques become
more sophisticated they became more computationally demanding. Important issues of scalability and
parallel implementation will continue to arise making techniques that are practical in a few thousand
tracks obsolete or practically infeasible in collections of a million music tracks. As collections get bigger
an important consideration is music and recording quality. To most existing music mining systems a
piece performed by the same combination of instruments by professional musicians and a high school
cover band essentially have the same representation.

1486
CHAPTER 26 Music Mining
The way listeners react to music is constantly changing and varies among different age groups and
cultures. Even though western popular music has been dominating music listening around the world,
there is a lot of interesting work that can be done in applying music mining techniques to analyze other
types of music from around the world. Such work falls under the area of computational ethnomusicology.
The large diversity of music cultures can lead to speciﬁc mining problems that are variations of existing
ones or completely new.
To conclude, mining music information is a complex, challenging and fascinating area of data mining.
The challenges of scalability, time evolution, heterogeneous structured and unstructured representations,
and personalization among others pose signiﬁcant challenges to existing mining algorithms. Progress in
music mining has the potential to lead to signiﬁcant advances to data mining in general and the converse
is also probably true. Finally music mining has already and will probably continue to affect the way
music is produced, distributed, and consumed.
1.26.12 Further reading
In this section a number of pointers to relevant material to this chapter are provided. The list is not
comprehensive but is a good representation of activity in the ﬁeld. Music Information Retrieval is a
relative new ﬁeld with a history of a little bit more than 10 years [1]. Currently there is no comprehen-
sive textbook that covers it fully. However, there are several related books and overview articles that
are great starting points for learning more about the ﬁeld. Orio [2] is an excellent, although somewhat
outdated, tutorial and review of the ﬁeld of MIR. A more recent collection of articles mostly focusing
on music data management and knowledge discovery can be found in Shen et al. [3]. Many of the
music mining tasks described in this chapter are covered in more detail in a recent collection of articles
on music mining edited Li et al. [4]. The chapters on audio feature extraction, mood and emotional
classiﬁcation, web-based and community-based music information extraction, human computation for
music classiﬁcation, and indexing music with tags are particularly relevant.
The basic types of audio feature extraction, i.e., timbre, pitch and rhythm appear in a classic early
paper on automatic genre classiﬁcation [5]. Tempo induction refers to the speciﬁc problem of estimating
a global tempo for a piece of music but the basic algorithms used are common to any type of rhythm
analysis.Gouyonetal.[6]provideanexperimentalinvestigationofdifferenttempoinductionalgorithms.
Various types of pitch representations have been investigated especially in the context of chord and key
detection [7]. An early seminal work showing how to combine text and audio features for extracting
music information is Whitman and Smaragdis [8] and subsequent work has explored the text analysis
of lyrics [9], web-based data [10] and microblogs [11].
The strategies and challenges of ground truth acquisition have been explored for music similarity
[12], genre classiﬁcation [13], and tag annotation [14]. An overview of the Music Information Retrieval
eXchange (MIREX) for the years 2005–2007 can be found in Downie [15]. An early critical review of
content-based music similarity approaches can be found in Aucouturier and Pachet [16]. It has been
followed by many publications mostly focusing on how music similarity can be improved with better
audio features, integration with text and context features, and design of better similarity metrics [16,17].
High level overviews of audio ﬁngerprinting systems have been provided by Wang [18] and Haitsma
and Kalker [19], and a good review of different algorithms can be found in Cano et al. [20]. Two well

1.26.12 Further Reading
1487
known cover song detection systems are Ellis and Poliner [21] and Serra et al. [22]. A comparative study
of different component choices for cover song retrieval systems can be found in Liem and Hanjalic [23].
Classiﬁcation,clustering,andregressionarestandardproblemsindataminingandpatternrecognition
and therefore well covered in many textbooks [24,25]. There is a large literature in automatic genre
classiﬁcation [5,26,27] , in many cases showing that feature sets based on different musical aspects
such as harmony and rhythm are all important and their combination gives better results than they do in
isolation. Hierarchical classiﬁcation can be performed by modeling the hierarchy as a Bayesian network
[28]. A user study investigating the perception of top-level music genres by average listeners can be
found in [29] and a comparison between automatic classiﬁcation and annotation by users can be found
in Lippens et al. [13]. An overview and critical discussion of genre classiﬁcation can be found in McKay
and Fujinaga [30]. Representative examples of emotion spaces that have been used in music mining
are the groupings of adjectives used to describe classical music pieces by Hevner [31] as well as the
2D emotional space described by Theyer [32]. Emotion/mood recognition [33] using audio and text
features has been covered in [9,34] among others. Regression can also be used for automatic emotion
analysis of music [35]. Li et al. propose a clustering algorithm that combines both lyrics and audio
features to perform bimodal learning [36].
Work on associating music with text using audio content analysis and machine learning started as
early as 2002 [8], not using tags per se, but using keywords extracted from web-pages that ranked highly
in search engine results for particular artists. Around 2007–2008, as social tag annotation became more
common, some of the ﬁrst papers that focused on automatic tag annotation for music started appearing
in the literature, using different classiﬁcation approaches and audio feature sets. For example, AdaBoost
was used for tag prediction in Eck et al. [37]. A Gaussian Mixture Model over the audio feature space
is trained for each word in a vocabulary in the seminal paper on semantic annotation by Turnbull et al.
[38]. This work also provided the CAL500 dataset that since then has frequently been used to evaluate
tag annotation systems. Stacking for music tag annotation was originally proposed in [39]. The user
evaluation of automatic tag annotation system as well as an machine learning approach that works with
an open tag vocabulary has been described by Law et al. [40].
Various approaches to visualization in audio-based music information retrieval are surveyed in
Cooper et al. [41]. The use of self-organizing maps in music visualization was popularized by the
Islands of Music system that rendered the resulting self-organized map as a a set of islands (areas of the
grid where many songs where mapped) surrounded by ocean (areas of the grid where fewer songs where
mapped) [42]. A number of music mining tasks have been formulated as multiple instance learning by
Mandel and Ellis [43]. Symbolic music information retrieval can be done using polyphonic queries and
references using a variety of methods based on string and geometric approaches [44]. Structure analysis
is typically performed using self-similarity matrices [45]. The most common approach for polyphonic
audio score alignment is based on calculating the distance (or similarity) matrix between two sequences
typically of chroma vectors and performing dynamic time warping [46,47]. A excellent overview of
signal processing methods for music transcription has been edited by Klapuri and Davy [48]. A critical
view on hit song science can be found in Pachet and Roy [49]. The problem of identifying similar artists
using both lyrics and acoustic data has been explored using a semi-supervised learning approach in
which a small set of labeled samples are supplied for the seed labeling and then used to build classiﬁers
that improve themselves using unlabeled data [50].

1488
CHAPTER 26 Music Mining
Glossary
AdaBoost
a family of classiﬁcation algorithms
Artist Filter
a preprocessing technique used in music mining task that ensures
that tracks of the same artist are either part of the training set or
the testing set but not both
Audio Fingerprinting
an algorithm for determing whether two digital audio ﬁles
correspond to the same underlying recording
Beat Histrogram (Spectrum)
a representation of the rhythmic content of a piece of music in
terms of the amount of energy in different rhythm-related
periodicities that are present
Chroma
a representation of the pitch content of a piece of music
Chromagram
a sequence of chroma vectors that can be visualized as an image
Collaborative Filtering
a technique for recommending items based on the purchase
history of users
Cosine Similarity
a method of computing similarty between two vectors based on
the cosine of the angle between them
Discriminatory Classiﬁers
a family of classiﬁcation algorithms in which the goal is to
disciminate between the classes directly without modeling the
distribution of all the sample feature vectors but only the ones that
affect the decision boundary
EM-algorithm
a algorithm for training Gaussian Mixture Models
Emotion Space
a multi-dimensional representation that maps different human
emotions and their relations
F-Measure
a performance measure for retrieval algorithms
Games with a Purpose (GWAP)
computer games that in addition to being engaging and
entertaining provide useful information for training computers
Gaussian Models
a parametric family of probability density functions
Gaussian Mixture Model
a method for modeling a probability density function as a
combination of Gaussian components
Generative Classiﬁers
a family of classiﬁcation algorithms in which the training data is
modeled by estimating probability density functions that can
generate new sample feature vectors
Genre
a categorical label that characterizes a particular type of music
Ground Truth
the information needed for training and evaluating a mining
algorithm
Harmonic Pitch Class Proﬁles
a representation of pitch content for audio signals similar in nature
to Chroma
Harmony
the structure of piece of music in terms of combinations of
discrete notes or pitches

1.26.12 Further Reading
1489
Hidden Markov Models (HMM)
a probabilistic techniques for modeling sequence of feature
vectors
ISMIR
the annual International conference of the Society for Music
Information Retrieval
K-Fold Cross Validation
a methodology for evaluating classiﬁcation performance
K-Means
a standard clustering algorithm
Last.fm
an internet music recommendation service
Mean Average Precision:
a performance measure for retrieval algorithms across multiple
queries
Mel-Frequency Cepstral
Coefﬁcients (MFCC)
a set of audio features frequently used in MIR that originated in
speech processing
Musical Instrument Digital
Interface (MIDI)
a symbolic format for transmitting and storing music related
information
Music Information Retrieval
(MIR)
the research area dealing with all aspects of retrieving information
from music signals that are stored digitally
Music Information Retrieval
Evaluation Exchange (MIREX)
an annual campaign for evaluating different algorithms on a
variety of MIR tasks
Music XML
a symbolic format for representation all aspects of notated music
Naive Bayes
a family of paramettric classiﬁcation algorithms
Nearest Neighbor Classiﬁer
a family of non-parametric classiﬁcation algorithms
Pandora
an internet personalized radio station
Pitch Class Proﬁles
a representation of the pitch content of a piece of music
Precision
a performance measure for retrieval algorithms
Principle Component Analysis
a technique for dimensionality reduction
Query-by-Humming (QBH)
the task of matching a sung melody to a database of music tracks
in either symbolic, or audio format
Recall
a performance measure for retrieval algorithms
Receiver Opeating Characteristic
(ROC)
a graphical representation used to characterize retrieval systems
Rhythm
the structure of a piece of music over time
Self-Organizing Maps (SOM)
a dimensionality reduction and clustering techniques that maps
high dimensional feature vectors into a discrete set of locations
Short Time Fourier Transfrom
(STFT)
a transformation of a time varying signal to a time-frequency
representation
Similarity Retrieval
a music mining task where a query music track is provided by the
user and a set of tracks that are similar to it are returned by the
system
Spectrum
a representation of a signal in terms of the amounts of different
frequencies
Stacking
a methodology for combining binary classiﬁers for multi-label
classiﬁcation
Support Vector Machines
a family of discriminative classiﬁcation algorithms

1490
CHAPTER 26 Music Mining
Tags
words from an open vocabulary supplied by users to characterize
multimedia objects such as music tracks, images, and videos
Term Weighting
a technique from text mining in which different words get assigned
different weights when considering the similarity of documents
Timbre
the properties that characterize a particular sound source when
compared with other sound sources of the same pitch and
loudness
Acknowledgments
The author would like to thank Tiago Fernades Tavares and Steven Ness for help with preparing the ﬁgures for
this chapter. Moreover, the author would like to thank the many researchers from a variety of disciplines who have
helped music information retrieval and, more speciﬁcally, music mining grow and evolve. Their efforts have made
the work described in this chapter possible and the author is proud to have been a part of this amazing community
from its inception.
Relevant Theory: Signal Processing Theory and Machine Learning
See this Volume, Chapter 9 Discrete Transforms
See this Volume, Chapter 16 Kernel Methods and SVMs
See this Volume, Chapter 20 Clustering
References
[1] J. Downie, D. Byrd, T. Crawford, Ten years of ISMIR: Reﬂections on challenges and opportunities, in:
Proceedings of the 10th International Society for Music Information Retrieval Conference, 2009, pp. 13–18.
[2] N. Orio, Music retrieval: A tutorial and review, vol. 1. Now Pub, 2006.
[3] J. Shen, J. Shepherd, B. Cui, L. Liu, Intelligent music information systems: Tools and methodologies, Infor-
mation Science Reference, 2008.
[4] T. Li, M. Ogihara, G. Tzanetakis (Eds.), Music Data Mining, CRC Press, 2012.
[5] G. Tzanetakis, P. Cook, Musical genre classiﬁcation of audio signals, IEEE Trans. Audio Speech Lang.
Process. 10 (5) (2002) 293–302.
[6] F. Gouyon, A. Klapuri, S. Dixon, M. Alonso, G. Tzanetakis, C. Uhle, P. Cano, An experimental comparison
of audio tempo induction algorithms, IEEE Trans. Audio Speech Lang. Process. 14 (5) (2006) 1832–1844.
[7] E. Gómez, Tonal description of polyphonic audio for music content processing, INFORMS J. Comput. 18 (3)
(2006) 294–304.
[8] B. Whitman, P. Smaragdis, Combining musical and cultural features for intelligent style detection, in: Proc.
Int. Symposium on Music Inform. Retriev.(ISMIR), 2002, pp. 47–52.
[9] X. Hu, J. Downie, A. Ehmann, Lyric text mining in music mood classiﬁcation, American music 183 (5049)
(2009) 2–209.
[10] P.Knees,E.Pampalk,G.Widmer,Artistclassiﬁcationwithweb-baseddata,in:ProceedingsoftheInternational
Conference on Music Information Retrieval, 2004, pp. 517–24.
[11] M. Schedl, On the use of microblogging posts for similarity estimation and artist labeling, in: Proceedings of
the International Conference on Music Information Retrieval, 2010.
[12] D. Ellis, B. Whitman, A. Berenzweig, S. Lawrence, The quest for ground truth in musical artist similarity, in:
Proc. ISMIR, vol. 2, 2002, pp. 170–177.

References
1491
[13] S. Lippens, J. Martens, T. De Mulder, A comparison of human and automatic musical genre classiﬁcation,
in: Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing 2004
(ICASSP’04), vol. 4, IEEE, 2004, pp. iv–233.
[14] D. Turnbull, L. Barrington, G. Lanckriet, Five approaches to collecting tags for music, in: Proceedings of the
9th International Conference on Music Information Retrieval, 2008, pp. 225–30.
[15] J. Downie, The music information retrieval evaluation exchange (2005–2007): a window into music informa-
tion retrieval research, Acoust. Sci. Technol. 29 (4) (2008) 247–255.
[16] J. Aucouturier, F. Pachet, Music similarity measures: whats the use, in: Proceedings of the ISMIR, Citeseer,
2002, pp. 157–163.
[17] T. Li, M. Ogihara, Content-based music similarity search and emotion detection, in: Proceedings of the IEEE
International Conference on Acoustics, Speech, and Signal Processing, 2004 (ICASSP’04), vol. 5, IEEE,
2004, pp. V–705.
[18] A. Wang, The shazam music recognition service, Commun. ACM 49 (8) (2006) 44–48.
[19] J. Haitsma, T. Kalker, A highly robust audio ﬁngerprinting system, in: Proc. ISMIR, vol. 2002, 2002, pp.
144–148.
[20] P. Cano, E. Batle, T. Kalker, J. Haitsma, A review of algorithms for audio ﬁngerprinting, in: IEEE Workshop
on Multimedia Signal Processing, IEEE 2002, pp. 169–173.
[21] D. Ellis G. Poliner, Identifyingcover songs with chroma features and dynamic programming beat tracking,
in: IEEE International Conference on Acoustics, Speech and Signal Processing 2007, ICASSP 2007, vol. 4,
IEEE, 2007, pp. IV–1429.
[22] J. Serra, E. Gómez, P. Herrera, X. Serra, Chroma binary similarity and local alignment applied to cover song
identiﬁcation, IEEE Trans. Audio Speech Lang. Process. 16 (6) (2008) 1138–1151.
[23] C. Liem, A. Hanjalic, Cover song retrieval: A comparative study of system component choices, in: Proc. Int.
Conf. on Music Information Retrieval (ISMIR), 2009.
[24] S. Theodoridis, K. Koutroumbas, Pattern Recognition, Fourth (ed.) Academic Press, 2008.
[25] P. Tan, M. Steinbach, V. Kumar, et al. Introduction to Data Mining, Pearson Addison Wesley Boston, 2006.
[26] T. Li, M. Ogihara, Q. Li, A comparative study on content-based music genre classiﬁcation, in: Proceedings of
the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval,
ACM, 2003, pp. 282–289.
[27] A. Meng, P. Ahrendt, J. Larsen, Improving music genre classiﬁcation by short time feature integration,
in: Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing 2005
(ICASSP’05), vol. 5, IEEE, 2005, pp. v–497.
[28] C. DeCoro, Z. Barutcuoglu, R. Fiebrink, Bayesian aggregation for hierarchical genre classiﬁ-
cation, in: Proceedings of the International Conference on Music Information Retrieval, 2007,
pp. 77–80.
[29] R. Gjerdingen, D. Perrott, Scanning the dial: the rapid recognition of music genres, J. New Music Res. 37 (2)
(2008) 93–100.
[30] C. McKay, I. Fujinaga, Musical genre classiﬁcation: is it worth pursuing and how can it be improved, in:
Proceedings of the Seventh International Conference on Music Information Retrieval, 2006, pp. 101–106.
[31] K. Hevner, Experimental studies of the elements of expression in music, Am. J. Psychol. 48 (2) (1936)
246–268.
[32] R. Thayer, The Biophysiology of Mood and Arousal. Oxford University Press, New York, 1989.
[33] Y. Kim, E. Schmidt, R. Migneco, B. Morton, P. Richardson, J. Scott, J. Speck, D. Turnbull, Music emotion
recognition: a state of the art review, in: Proc. 11th Int. Symp. Music Information Retrieval, 2010, pp. 255–266.
[34] E. Schmidt, Y. Kim, Prediction of time-varying musical mood distributions using kalman ﬁltering, in: Ninth
International Conference on Machine Learning and Applications, 2010 (ICMLA), IEEE, 2010, pp. 655–660.

1492
CHAPTER 26 Music Mining
[35] Y. Yang, Y. Lin, Y. Su, H. Chen, A regression approach to music emotion recognition, IEEE Trans. Audio
Speech Lang. Process. 16 (2) (2008) 448–457.
[36] T.
Li,
M.
Ogihara,
S.
Zhu,
Integrating
features
from
different
sources
for
music
informa-
tion retrieval, in: Sixth International Conference on Data Mining 2006, ICDM’06, IEEE, 2006,
pp. 372–381.
[37] D. Eck, P. Lamere, T. Bertin-Mahieux, S. Green, Automatic generation of social tags for music recommen-
dation, in: Advances in Neural Information Processing Systems, vol. 20, 2007.
[38] D. Turnbull, L. Barrington, D. Torres, G. Lanckriet, Semantic annotation and retrieval of music and sound
effects, IEEE Trans. Audio Speech Lang. Process. 16 (2) (2008) 467–476.
[39] S.R. Ness, A. Theocharis, G. Tzanetakis, L.G. Martins, Improving automatic music tag annotation using
stacked generalization of probabilistic SVM outputs, in: Proc. ACM Multimedia, 2009.
[40] E. Law, B. Settles, T. Mitchell, Learning to tag from open vocabulary labels, in: Principles of Data Mining
and Knowledge Discovery, 2010, pp. 211–226.
[41] M. Cooper, J. Foote, E. Pampalk, G. Tzanetakis, Visualization in audio-based music information retrieval,
Comput. Music J. 30 (2) (2006) 42–62.
[42] E. Pampalk, S. Dixon, G. Widmer, Exploring music collections by browsing different views, Comput. Music
J. 28 (2) (2004) 49–62, (Summer 2004).
[43] M. Mandel, D. Ellis, Multiple-instance learning for music information retrieval, in: Proc. ISMIR, 2008, pp.
577–582.
[44] K. Lemström, A. Pienimäki, On comparing edit distance and geometric frameworks in content-based retrieval
of symbolically encoded polyphonic music, Musicae Scientiae 11(Suppl. 1) (2007) 135.
[45] R. Dannenberg, Listening to NAIMA: an automated structural analysis of music from recorded audio, in:
International Computer Music Conf. (ICMC), 2002.
[46] G.T.N. Hu, R.B. Dannenberg, Polyphonic audio matching and alignment for music retrieval, in: Workshop
on Applications of Signal Processing to Audio and Acoustics, 2003.
[47] M. Müller, Information Retrieval for Music and Motion, Springer-Verlag, New York, Inc., 2007.
[48] A. Klapuri, M. Davy (Eds.), Signal Processing Methods for Music Transcription, Spring, 2006.
[49] F. Pachet and P. Roy, Hit song science is not yet a science, in: Proc. Int. Conf. on Music Information Retrieval
(ISMIR), 2008, pp. 355–360.
[50] T. Li, M. Ogihara, Music artist style identiﬁcation by semi-supervised learning from both lyrics and content,
in: Proc. ACM Int. Conf. on Multimedia, 2004, pp. 364–367.

1493
A
AdaBoost, 1472, 1477–1478, 1487
Adaptive CoSaMP algorithm (AdCoSaMP), 1335
Adaptive decomposition design
block size, transform, 459
coding improvements, 457
pre-/post-processing support, 458
Adaptive ﬁ lters, 604–606
acoustic echo in hands-free telephony, 619–620
algorithms, 619
blind equalization, 734
constraints, 746
convex sets projection, 745
cooperative estimation, 738
data model, 692
distributed region, 743
echo path, 624
echo-cancellation, 624, 630
ﬁ nite-precision arithmetic, 718
IIR ﬁ lters, 744
inputs and outputs, 620
orthogonality condition, 626
reduced-rank, 746
regularization, 729
set-membership algorithm, 745
solution, 627
statistical analysis, 689
structure, 626
subband domain, 737
theory, 620
transform-domain, 737
Adaptive projected subgradient method (APSM)
formula, 959
kernel algorithm, 960
projection base, 924–925
Adaptive Resonance Theory (ART), 1123–1124
vigilance parameter, 1124–1125
Adatron algorithm, 834
Algorithm see speciﬁ c entries
AMUSE algorithm, 1181
Analog approximation problem, digital ﬁ lters
frequency transformations, 266
standard lowpass, 263
typical requirements, 262
Analog reconstruction ﬁ lter, D/A converter, using, 187
Analog signal acquisition systems, 
continuous-time system, 3–4
Analog signal processing (ASP)
in information processing, 169
practical applications, 169
quantization schemes, 169–170
Analog signals, deﬁ ned, 29
Analog to Digital Converter (ADC)
converter performance, 215
frequency response, 215
oversampling, 227, 230
quantization errors, 219
round-off errors, 222, 225
Approximation approach, image/video 
processing applications
complex-coefﬁ cient transforms, 444
direct scaling, 433
integer DCT design, 441
lifting step, 437, 439
lossless color transform design, 439
structural design, 436
Arithmetic operations, digital ﬁ lters
addition, 321, 324
bit-serial addition and subtraction, 322
multiplication, 322
ripple-carry addition, 322
subtraction, 321
Audio feature extraction, 1456
Autoregressive (AR) models
input-output relationship, 598–599
parameters, 598–599
process, 595
Axiomatic deﬁ nition, 76
B
Back-propagation algorithm
deﬁ ned, 822
numerical techniques, 822
recurrent networks, 828
training error minimization, 823
Balian-Low theorem, 564
Bandpass sampling, 195
random process, 165
Batch learning, algorithmic ingredients, 953
Bayesian information criterion (BIC)
in model selection, 1426
linear models, example, 1429
prior distribution effects, 1429
properties, 1427
Index

1494
Index
Bayesian network (BNs)
conditional independence properties, 1003
deﬁ ned, 1001
discriminative learning, 1032
for expert systems, 1053
factorization properties, 1001
generative learning, 1018
maximum likelihood, 1019
parameter learning, 1022
speech processing, 1046
structure, 1028
time-series modeling, 1046
Biclustering
algorithms, 1133
in bioinformatics community, 1128
relational data sets, 1129
Blind source extraction, 1185
multi-block tensor factorizations, 1208
temporal structures, 1186
Block matrices, inverses of, 639
Boltzmann machine, 826–827
Bounded-input bounded-output (BIBO)
in LTI system, 58
system stability, 58–59
C
Canonical Correlation Analysis (CCA), 1226
fundamentals, 858
two set variables, 866
sparciﬁ cation, 1175
Cascade correlation, 831–833
Case study, time-frequency analysis, machine 
learning, 1350
Chomsky hierarchy, 829
Classiﬁ cation task
linear mapping, 893–894
parameter estimation, 887
purpose, 888–889
in RKHS, 895
training points, 889–890
Clustering. See also Clustering algorithms; 
Clustering applications
Cauchy-Schwarz objective function, 1401
music mining, 1473
Clustering algorithms
deep learning, 1135
hierarchical clustering (HC), 1118
mixture density-based, 1121
neural network-based, 1123
open issues and problems, 1140
spectral, 1126
subspace, 1127
validation, 1139
Clustering applications
motion clustering, 1140
MRI images, 1139
vision-guided robot, 1140
Cognitive radio
digital signal processing (DSP), 339
frequencies, 400–401
receiver devices, 402
Commensurate-length transmission, 279
Richards’ variable, 280
unit elements, 281
Common component analysis (CCA)
multi-block tensor data, 1219
power method, 1226
Common formulation, 646
channel equalization, 646–647
useful tasks, 646–647
Complex variables and multi-channel 
ﬁ ltering, 661
optimum error, 662
optimum ﬁ lters, 662
Computational learning theory
agnostic PAC model, 800
halving algorithm, 802
mistake bound model, 801
noise models, 800
PAC model, 798
perceptron (online algorithm), 805
weighted majority, 803
winnow (online algorithm), 805
Computational properties, digital ﬁ lter
cyclic scheduling, 319
latency and throughput, 318
maximal sample rate, 318–319
Continuous-time signals
deﬁ ned, 30
Fourier transform, uses, 73–74
sampling, 173–174, 181
Continuous-time system
analog signal acquisition systems, 3–4
input signal, 4
linear and time-invariant (LTI), 4, 31
output signal, 4
Continuous Wavelet transform, 578–579
Contourlet transform, applications, 515
Convex optimization, 828
Convolution
for casual system, 45
frequency-domain, 72
graphic interpretation, 36
in LTI system, 55

1495
Index
Convolution (Continued)
in time-domain system, 45–48
input and output signal, 34
mathematical properties, 36, 74
transfer function, 55–56
zero-state solution, 35, 56
CP model
additional constraints, 1214
data tensor, 1212–1213
illustration, 1213
multilinear terms, 1213
power method, 1214–1215
restrictive trilinear, 1225
special form, 1219
Tucker model, advantages, 1213–1214
Cross-validation, choosing architecture, 823, 828
Cumulative probability distribution function (CDF)
deﬁ ned, 120–121
probability density function, 122
Curvelet transform
advantages, 534
amplitude and angular windows, 500
applications, 497, 514
coefﬁ cient decay rate, 504
continuous, 499, 501
discrete, 506, 512
localized functions, 533
properties, 503
reconstruction formula, 505
reproducing formula, 505
reverse operations, 512
USFFT implementation, 512–513
D
Deconvolution, 645, see also Inverse 
system identiﬁ cation
Deep learning architecture, 818, 824
semi-supervised clustering, 1135
Deterministic signals
Poisson’s formula (summation), 179, 181
uniform sampling, 178
Digital down converter (DDC)
architecture, 403
band pass ﬁ lter, 356, 359
building blocks, 356
frequency shifting, 346
resampling operations, 341–342
Digital ﬁ lter
ADC output, 399–400
bandwidth reduction, 362–364
complexity, 360
deﬁ ned, 366
design techniques, 343
domain characteristics, 360
multirate processing, 341
non-recursive ﬁ lters, 351
resampling theory, 339, 363–364
sampling clock, 352
signal processing, 349–350
system memories, 350–351
task performance, 351–352
Digital signal processing (DSP)
of analog signals, 175–177
communication system, 178
in information processing, 169
practical application, 169
quantization schemes, 169–170
Digital up converter (DUP)
center frequency, 378
interpolating ﬁ lters, 400
simulation results, 415
Direct scaling approach, image/video 
processing applications
integer DCT design, 435
rotation angle, 436
Discrete spaces, frames in, 571
Discrete-time signals
frequency responses, 238
input/output relations, 234
mixed systems, 232
Poisson’s summation formula, 235
second-order continuous-time-input, 238, 240
sequences, 172–173
stability factor, 237
Discrete-time systems
classiﬁ cation, 81
deﬁ ned, 79
in Linear time-invariant continuous-time 
systems, 83
inﬁ nite word length, 79
input-stability criterion, 97
input/output sequence, 81
MATLAB application, 101
multi-rate interval, 79
output-stability criterion, 96
stability in, 96, 98
Doubly resistively terminated lossless 
networks, 266
digital ﬁ lters, 266, 275
design, 273
element sensitivity, 268
error elements, 269
lossy elements, 270
maximal power transfer, 266
passband sensitivity, 268

1496
Index
Doubly resistively terminated lossless networks (Continued)
reﬂ ection function, 267
terminating errors, 269
E
Energy conservation method, steady-state 
analysis, 707
Energy signals, distortion measurement, 192, 194
Erasures, 570
Euclidean divergence, 1397–1398
Excess mean-square error (EMSE), 631
adaptation parameters, 713
step-size, 713
traditional statistical method, 701
Extreme learning machine, 824, 836
F
Factor graphs (FGs)
BN conversion, 1013
deﬁ ned, 1011
error-correcting codes, 1055
Feed-forward network, 847
Filter banks
cascade sequence, 523
DF, 529
directional, 523, 525, 531
Laplacian pyramid, 520, 529
quincunx, 524
Finite vector spaces
analysis operators, 572
Gram matrix, 573
matrices, 572
synthesis operators, 572
Fisher discriminant analysis
classiﬁ cation function, 868
fundamentals, 858
Fourier transform
application, 472
continuous, 471
deﬁ nition, 471–472
discrete, 472, 497
energy preservation, 471
input function, 472, 497
properties, 471, 481
pseudo-polar coordinates, 545
temporal and/or spatial data sequences, 467
windowed, 473
Frame bounds ratio, 569
Frame characterization problem, 566
Frame coefﬁ cients, 562
Frame expansion, 564
Frame of translates, 574–575
Frames
common approaches, 574
construction, 574
decomposition, 567
dilation, 574
inverse operator, 567
modulation, 574
operator, the, 566
reconstruction, 567
translation, 574
Frequency response
deﬁ ned, 60
magnitude plot, 61–64
RLC series circuit, 61
symmetry property, 62
voltage application, 60
Frequency response masking (FRM) structure, 
digital ﬁ lter
basics, 307
cosine-modulated transmultiplexer, 315
ﬁ lter bank, 314
half-band ﬁ lter, 311
hilbert transformer, 311
multirate ﬁ lter, 312
multistage, 310
recursive ﬁ lter, 316
two dimension, 316
Fuzzy c-means (FCM), 1120
Fuzzy clustering, 1115, 1121
G
Gabor frames, 574, 576
fast analysis and synthesis operators, 583
Heisenberg boxes, 581
in discrete spaces, 583
Gabor system
elements location, 577
windowed Fourier frame, see Weyl-Heisenberg 
or Gabor frame
Gaborgram, 581
Gain vector, 608
Games with a purpose, 1454–1455, 1474
Gaussian mixture model, 1123, 1131
for audio feature, 1487
EM algorithm, 1474
music mining classiﬁ cation, 1468
tag annotation, 1477–1478
weighted combination, 1468–1469

1497
Index
Gaussian process
applications, 877
Bayesian learning approach, 872–873
covariance function, 873–874
deﬁ ned, 874
fundamentals, 858
regression approach, 872–873
variational approximation, 877
Gaussian vectors, fourth-order moments, 715
General modular construction, image/video 
processing applications, lifting scheme, 451
Generative learning
of Bayesian networks, 1018
principles, 1016
Gradients, 637–638
Gradient descent
adaptive matrix, 839
neural network process, 833
pseudocode, 821–822
recurrent network, 827
stochastic decomposition, 822, 837
training error minimization, 823
Graph theory, 996
H
HALS algorithm, 1198
Hard clustering, 1115, 1120
Hebbian learning, 817–820, 826, 837
Hierarchical clustering
arbitrary shapes, 1120
controlling process, 1143
coupled two-way clustering (CTWC) 
algorithm, 1134–1135
deﬁ ned, 1115
example, 1118
image segmentation, 1139
open issues, 1140–1141
properties, 1117–1118
software implementation, 1123
squared error criterion, 1120
structure, 1118
Higher-order approach, image/video 
processing applications
blocking problems, 449–450
butterﬂ ies, 436–437, 441, 455–456
coding efﬁ ciency, 454
HD photo or JPEG-XR transform, 457
low-order building blocks, 436–437, 451–452
optimization, design, 449
pre-/post-processing operators, 452
scaling factors, 436–439, 457
Hilbert spaces, Fréchet’s differentiation, 915
Hodgin-Huxley model, 819
Hopﬁ eld network, 826
I
Image/video processing applications
approximation approach, 433
attenuation at mirror frequencies, 433
design strategy, 431
direct scaling approach, 435–436
energy compaction, 432
4 × 4 transformer design, 434
orthogonality, 432
stop-band attenuation, 432–433
zero DC leakage, 433
Implicit vs. physical models, 
deconvolution, 652
Importance sampling (IS), 1093
Impulse response
cascade system, 36
in casual system, 37, 59
convolution integral, 34
deﬁ ned, 33
frequency response, linear system, 60
in Fourier transform, 76
in Laplace transform, 55–56
in real positive poles, 59, 62
linear time-invariant system, 32
stimuli estimation, 34, 39
voltage application, 36
Impulse signal, 31
in narrow pulse, 30
Input and output signals, 34
Independent Component Analysis 
(ICA), 841
approaches, 1180
higher order statistics, 1183
multi-block data, 1220–1222
MRMI algorithm, 1407
Inference
approximate, 1044
joint probability distribution, 1037
types, 1036
Information based learning
areas of application, 1379
nonparametric estimators, 1388
probability density functions 
(PDFs), 1380
reproducing kernel Hilbert space 
(RKHSs), 1391
theoretical descriptions, 1381

1498
Index
Information based learning (Continued)
Input and output signal values, 
quantization, 216
Interconnection networks, digital ﬁ lter, 288
direct interconnection of adaptors, 296
parallel adaptors, 294
series adaptors, 291
symmetric two-port adaptor, 289
three-port parallel adaptor, 296
three-port series adaptor, 292
two-port parallel adaptor, 295
two-port series adaptor, 292
Interference cancellation, 642
applications of, 642
Inverse frames, 568
Inverse gabor frames, 582
prototype function, 583
Inverse system identiﬁ cation, 645
in communications, 645
simpliﬁ ed baseband communications 
system, 645
J
Joint cumulative probability distribution 
function, 132
Joint model, 116
Joint probability density function, 132–133
K
Kernel methods
applications, 877
Bayesian learning approach, 872
canonical correlation analysis (CCA), 866
computational issues, 875
Fisher discriminant analysis, 868
foundations, 858
Gaussian process, 872
multiple learning, 876
open issues, 878
prinicipal component analysis, 865
properties, 860
support vector machines (SVMs), 869
types, 862
Kernel trick
in machine learning, 858
optimization problems, 872
properties, 858
ridge regression, 859
Kernel, neural network, 835
arbitrary, 839
nonlinear components, 842
structure, 836, 847–848
trick, 836
L
Ladder structures, digital ﬁ lters
design, 275
lowpass ﬁ lters, 273
waves, 303
Lattice structures, digital ﬁ lters, 275
analog structure, 276
design, 278
reactances realization, 277
wave description of two-ports, 276
Learning vector quantization, 825, 837, 847
Least mean squares (LMS)
algorithm, 629–631, 675, 678
convex functions, 670
convexity, 669
counter-example, 667
ﬁ nite-precision effects, 718
joint probability density function, 666
linear functions, 916
particle ﬁ lters, 669
recursive function, 959
statistical analysis, 696
under non-sufﬁ ciently rich input, 660
variable step-size, 730
Levinson-Durbin recursion, 601–603
Linear complex least-mean squares, 663
complex gradient, 665
optimum error power, 665
orthogonality condition, 665
real or complex circular adaptive 
ﬁ lters, 664
Linear least-mean squares estimation
linearly parametrized classes, 649, 676
Wiener solution, 650
Linear and time-invariant (LTI) system
amplitude scaling, 55
continuous-time systems, 31
deﬁ ned, 31–32
discrete-time systems, 83
eigensignals, 55, 58, 72
ﬁ nal-value theorem, 48
frequency-domain, 72
impulse response, 59
Initial-value theorem, 48
pole location, 58
properties, 31–32
signal anlalysis function, 67

1499
Index
Linear and time-invariant system (Continued)
stability function, 58
step response, 33
transfer function, 55–56
M
Machine learning
adaptive CoSaMP algorithm (AdCoSaMP), 1335
algorithm, computational demand, 1318
classical regularization schemes, 765–767
classiﬁ cation (ITL framework), 1385
coherent dictionaries, 1347
compressed sensing, 1300
compressed sensing matching pursuit (CSMP) 
algorithms, 1310
computational aspects, 769
cosparsity, 1348
dimensionality reduction technique, 1303
feature selection, 1385
ﬁ ltering, 1385
frequency resolution, 1352–1353
Gabor frames, 1353
Gabor transform, 1350
geometric interpretation, 1289
greedy algorithms, 1306
iterative shrinkage algorithms (IST), 1311
ℓ0 norm minimizer, 1286, 1291
ℓ1 norm minimizer, 1287–1288
least absolute shrinkage and selection operator 
(LASSO), 1276, 1332–1333
least angle regression (LARS) algorithm, 1310
linear parametric models, application, 765
MSE learning curves, 1341
mutual coherence, 1293, 1295
noisy measurements, robust signal, 1299
nonparametric approaches, 768
norms, functional analysis, 1274
OMP solution, 1308
online (time-recursive) schemes, 1331
optimization parameters, 1272, 1342
parameter estimation, 1272, 1275
promoting algorithms (sparsity), 1306
properties, signal representation, 1343–1344
restricted isometry property (RIP), 1296
signal representation, sparse, 1280
solution, sparsity constraint, 1284–1285
sparse adaptive parallel projection onto convex 
sets method (SpAPSM), 1336
sparse vectors, 1357
sparsity-awareness, variations, 1325
sub-Nyquist sampling, 1304
switch in position 1, 1286, 1293
switch in position 2, 1289, 1293
switch in position 3, 1291
time-frequency analysis, case study, 1350
l2 norm minimizer, 1285
types, 1271
weighted 1 ball, 1339
Machine learning, theoretical perspectives, 775. 
see also Computational learning theory
binary function, VC dimension, 787
classiﬁ cation and regression, 778–779
concentration inequalities, 783
data driven penalties, 792
empirical means, 782
entropy logarithm, 787
ERM convergence, 781
ﬁ xed function class, 779
hold-out estimation, 793
loss function, 777
PAC-Bayesian analysis, 796
penalty function, 791
probabilistic formulations, 776
rademacher complexity, 784
real valued functions, 789
risk function, 777
sample compression, 793
stablility properties, 795
structural risk minimization (SRM), 790
Markov Chain Monte Carlo (MCMC) methods, 1077
acceptance probabilities, 1107
ﬁ nite case, 1078
Markov network (MNs)
conditional independence, 1007
deﬁ ned, 989
factorization properties, 1007
Markov random ﬁ elds (MRFs)
error correcting codes, 1046
for image analysis, 1053
Matching pursuit. see Machine learning, 
greedy algorithm
MATLAB application, discrete-time systems
space state function, 105
transfer function, 105
Matrix analysis, 639
Matrix inversion Lemma, 640
Mean squared error, 823, 827–828
Metropolis Hastings (MH) algorithm
convergence rate, 1083
Gibbs sampler, 1088
illustration, 1083
proposal density, 1081–1088, 1103–1104
symmetric proposal, 1084–1088
target density, 1088

1500
Index
Minimum description length (MDL) principle, 1442
application, 1446
Bayesian inference, 1445
linear model, example, 1446
sequential variants, 1444
Minimum distance estimation, 1418, 1424
Minimum error entropy (MEE)
back propogation, 1396
ITL critrerion, 1385
Minimum message length (MML)
applications, 1442
Bayesian inference, 1440
invariance, 1439
linear models, example, 1441
model selection, 1434
other approximations, 1440
parameter estimation, 1439
selection consistency, 1439
Wallace-Freeman approximation, 1437
Model selection
Akaike information criterion, 1418
Bayesian approaches, 1424
by compression, 1431
distance based criteria, 1431
linear model, example, 1416, 1423
Markov-Chain Monte-Carlo Bayesian methods, 1430
Minimum description length (MDL) principle, 1442
minimum distance estimation, 1418
minimum message length (MML), 1434
nested sets, 1416
nonnested sets, 1416, 1424
Schwarz Information Criterion, 1426
simulation criteria, 1447
Wallace-Freeman message length approximation 
(MML87), 1437
Monte Carlo (MC) method
advanced methods, 1101
application, 1065
ﬁ nite state case, 1078
illustration, 1065–1066
open issues and problems, 1111
principle, 1066
random sampling, 1065
sequential importance, 1092, 1096
stationary distribution, 1078
target distribution, 1080
Motivation-acoustic echo cancellation, 620
echo signal estimation, 623
general echo canceler conﬁ guration, 621
measured impulse response, 621–622
Multilayer perceptrons (MLPs)
supervised learning, 1394
Multilinear BSS, Tucker decompositions, 1216
Multimedia coding and processing
ﬁ lter bank connection, 428
lapped transform connection, 430
notation, 425
optimal orthogonal transform, 427
trasform fundamentals, 426
Multiple kernel learning
recent developments, 858, 878
sequential minimal optimization (SMO) 
algorithm, 877
Multirate ﬁ lter
application, 352–353, 363
deﬁ ned, 341, 349
sampling clock, 351–352
single processing architecture, 341–342
Multiresolution analysis
coding length criterion, 490
decomposition procedure, 521–522
denoising and modeling, 489
estimation problems, 488
LP ﬁ lter bank technique, 516, 523, 529
MRA theory, 479
noise model, problem statement, 489
numerical experiments, 492
properties, 480
tight frame analysis, 522
wavelets, 479
worst case noise, 491
Music mining
advanced techniques, 1480
algorithm desigining, 1453
audio feature extraction, 1456
audio ﬁ ngerprinting, 1464
automatic transcription, 1482
classiﬁ cation, 1468
clustering, 1473
context extraction, 1460
cover song detection, 1464, 1467
datasets, 1483
Gaussian models, 1468–1469
Genre classiﬁ cation, 1469
ground tooth acquisition, 1454
mirex task, 1473
mood, 1470
open problems, 1484
organizing emotion, 1470–1471
performance evaluation, 1472
sequence matching, 1466
similarity retrieval, 1460, 1463
software, 1483
symbolic representation, 1481
tag annotation, 1474
visualization, 1478

1501
Index
N
Neural gas, 845–846
Nonlinear autoregressive models, 612–613
Non-negative Matrix Factorization (NMF)
alpha-beta divergences, 1202–1203, 1205
basic models, 1190
convolutive, 1206
large-scale problems, 1200
non-smooth case, 1193
parameter estimation, 1195
robust algorithms, 1202
sparsity constraints, 1190
special forms, 1191
Non-parametric signal models, 595
Nonuniform sampling
DC offset, 210
error metrics, 208
generalizations, 204
problem formulations, 205
reconstruction, 207
relaxed problems, 206
time-interleaved ADCs, 204
time-varying FIR system, 207
Normalized least-mean-square (NLMS) 
algorithm, 678–679
afﬁ ne projections algorithm (APA), 737
matrix inversion lemma, 680
proportionate, 731
uniﬁ ed statistical analysis, 701
Normalized tight frames, 570
Norms and spectral radius, 641
NP-hard problem, 823
O
Online learning. see also Reproducing kernel 
Hilbert spaces (RKHSs)
application, 970
convex analytic toolbox, 941, 953, 958
Optimal mean-square error, 652
Optimum ﬁ ltering, 647
error power, 647
goals, 648
quadratic cost functions, 648
Ordinary differential equation (ODE)
constant coefﬁ cients, example, 39
differentiation property, 75–76
for input voltage, 40
Laplace transform application, 54
pole-zero plot or pole-zero diagram, 57
Orthogonality condition, 651
cross-correlation vector evaluation, 653
optimum error, 655
Overﬁ tting
machine learning task, 890
regularization method., 891–892, 895
in RLS algorithm, 929
Oversampling
in A/D converters, 227, 230
Nyquist theorem, 210
reconstruction, 210, 213
P
Parameter learning
Bayesian, 1022
classiﬁ cation rate, 1035
maximum likelyhood, 1019
Parametric estimation
deterministic and stochastic signals, 591
stochastic (random) signal models, 593
wide-sense stationary random signals, 594
Parametric modeling, 599
deterministic auto-correlation 
sequence, 601–603
impulse response, 601
input-output pairs observations, 599–601
joint process estimation, 604
linear prediction, 601–603
Wiener solution, 604–606
Partial Least Squares (PLS) methods, 1177
Perceptron
activation function, 820–821, 834
classiﬁ er, 820
deﬁ ned, 819
Hopﬁ eld networks, 826–827
support vector machine (SVM), 833, 
835–836
training, error minimization, 823, 835
Piecewise linear modeling, 612–613
context tree example, 613–614
overﬁ tting, 613
Polar coordinates
Cartesian coordinates, 497, 499
deﬁ nition, 497, 508
numerical conversion, 534
pseudo, 545
Polyphase channelizer
application, 342, 383, 411
deﬁ ned, 420
digital ﬁ lters, 339
down conversions, 365
operating parameters, 342
output sampling frequency, 383, 392
software deﬁ ned radios (SDRs), 364

1502
Index
Polyphase ﬁ lter bank
down converting, 371
DSP applications, 341
input signals, 420
output sample rate, 384
Positive-deﬁ nite matrices, 640
Postﬁ ltering. see Echo cancellation
Prediction, 644
applications, 644
chemes, 644
Principal component analysis 
(PCA), 842, 845
application, 1156
basic algorithms, 1166
Hotelling deﬂ ation procedure, 1173–1174
kernel methods, 1174
large scale problems, 1166, 1218
number detrermination, 1159
orthogonal directions, 865
power methods, 1169
self-association principle, 1167
sparse, 1170
standads, 1158
Probabilistic graphical model (PGM)
application, 991
applications, 1046
efﬁ cient inference, 992, 1035
generative learning, 991–992, 1033
graph theory, 996
in approximate inference, 1044
in Bayesian networks (BNs), 1001
in loopy graphs, 1045
in Markov Chains (MC), 1014
websites, 1057
Probability, 120
axiomatic approach, 115
Bayesian rule, 116
classical or a priori approach, 114
conditional model, 116–117
cumulative distribution function, 120
density function, 123
Gaussian distribution, 125, 129
interval estimate, 140
joint cumulative probability distribution function, 132
joint model, 116
joint probability density function, 132–133
mass function, 123
mean-ergodicity, 148
multiple random variable distributions, 131
Poisson distribution, 125
random variable sample, 127
relative frequency or a posteriori approach:, 114–115
steady-state vector, 155
total model, 117
transition model, 117
uniform distribution, 124
Probability density functions (PDFs)
information based learning, 1380
relevant principle, 1402
Probability theory, 992
Prony’s method, 603
Q
Quantization schemes, sparsiﬁ cation, 938–939, 966
R
Random process
bandpass, 165
complex, 153
continuous- or discrete-time model, 141
continuous-time white noise w(t), 159
deﬁ ned, 128, 141
distributions, 141
ergodicity, 148
ﬁ rst order moments, 142
Gaussian distribution, 151
linear time-invariant (LTI) systems, 161, 165
Markov chains, 154
for noisy signal, 149
Poisson distribution, 152
second order moments, 142
sequences, 141
spectral description, 155, 160
statistical independence, 142
time averages, 146
WSS processes, 145, 159
Random sequence
deﬁ ned, 143
Discrete-time white noise, 159
for discrete-time case, 141
in Markov chains, 154
mean-ergodicity, 148
power spectrum, 167
spectoral density, 157–158
time average, 147
Random variable
binomial distribution function, 125
characteristic function, 130, 136–137
complex, 138, 153
conditional distribution, 126
conditional distributions, 134
cumulative density function, 121–122
expected value of a real scalar function, 135
Gaussian distribution function, 124, 128, 137
inversion techniques, 1071

1503
Index
Random variable (Continued)
low-order moments, 128
Markov’s inequality, 129
mass probability function, 123
moment generating function, 136–137
multiple distributions, 131
orthogonal function, 135
parameter estimation, 139
Poisson distribution function, 125
probabilistic models, 119
probability density function, 122
processes and sequences, 142
statistical independence, 142
statistical mean, 127
statistical variation, 135–136
statistically independent, 134
stochastic process, 141
transformation function, 130
transformation in, 1073
uniform distribution function, 123–124
vector transformation, 137
Real time recurrent learning, 827–828
Recurrent neural network
bicausal neural networks, 830–831
deﬁ ned, 819
Elamn style, 827
simple models, 828–829
small weights, 830
temporal sequences, 827
time dependent signals, 818
training complexity, 828
without training, 828
Recursive Least Squares (RLS)
kernel algorithm, 938–939
linear function, 929
weighting coefﬁ cient, 933
Region of convergence (ROC)
ﬁ nite duration signals, 43
in Fourier transform, 72–73
integral, deﬁ ned, 43
in LTI system, 59
of Laplace transform, 43–44
on s-plane, 44–45
pole location, 49
Regression task
in kernel APSM algorithm, 960–961
in RKHS, 959
size estimation, 953–954
Regularization
cost function., 891–892
loss functions, 970–971
Machine Learning task, 890
overﬁ tting problem, 891–894, 929
user-deﬁ ned parameter., 921
Rejection sampling, 1075
Relevance learning, 847
Reproducing kernel Hilbert spaces 
(RKHSs), 860–861
complexiﬁ cation, 927–928
cost functions, 915
examples of kernels, 902
inﬁ nite dimensions, 895, 965
Information estimators, 1393
ITL costs, 1392
Kernel RLS, 933–934
nonlinear function, 917–918, 958
online learning schemes, 915–916
properties, 897–898, 904
regularization problems, 1245
Resampling algorithm, sequential 
importance, 1094
Reservoir computing, 818, 824, 828
Resonance circuits, digital ﬁ lter, 298
ﬁ rst-order circuits, 299
second-order parallel, 301
second-order richards’ structures, 301
second-order series, 300
RJ-MCMC algorithm
acceptance probabilities, 1107–1111
construction techniques, 1104
illustration, 1110
target density, 1107
RLC circuit
initial condition, 40, 52
transfer function, 61
voltage series, 39, 75–76
voltage-current relationship, 50
RLS algorithm
advantages, 681
comparing rate convergence, 687
conventional type, 685
dichotomous coordinate-descent (DCD) 
function, 721, 724–725
error sequence, 681–682
ﬁ nite-precision effects, 719
practical implementation, 686
uniﬁ ed statistical analysis, 701
Robust ﬁ lters, non-Gaussian noise, 732
S
Sampling theorem
aliasing, examples, 182
anti-aliasing ﬁ lter, 183
continuous-time signal, 181
distortion measurement, 192
reconstruction process, 184–185

1504
Index
Self-organizing feature maps (SOFM), 1123
Self-organizing map (SOM), 843
Semi-supervised learning
challenges, 1263
co-regularization, 1244
co-training framework, 1243
E-step parameter, 1247
expectation-maximization (EM) 
approach, 1246, 1248
Finite-dimensional approximation, 1254
graph Laplacian regularization, 1250
graph-based, 1249
iterated graph Laplacian regularization, 1252
large-scale computing, 1260
likelihood function, 1246
low density separation, 1241–1242
M-step parameter, 1247
manifold regularization, 1256
measure-based regularization, 1257
multiresolution analysis (MRA), 1258
partially observed data, 1240
self-training, 1249
structured outputs, 1259
theoretical analysis, 1263
transductive support vector machines 
(TSVMs), 1241–1242
unlabeled data value, 1249
Sequences
discrete-time signals, 79, 88
Fibonacci, 82
of signal values, 79
Sequential parametric modeling, 606
Shannon Sampling theorem, 575
Shearlet transform
advantages, 534
anisotropic components, 534
applications, 534, 546
cascade algorithm, 543
continuous, 497
edge orientation, 547, 550
frequency domain, 536
image features, 548, 552
localized function, 533
normal directions, 539
numerical implementation, 540, 543
properties, 550
translation parameters, 541
vertical direction, 541
Short Time Fourier Transform, 578–579
Signal analysis and synthesis, 565, 580
common frame constructions, 
examples, 566
perfect reconstruction ﬁ lter bank, 561
system model, 565
Signal processing, frames
characterization of, 569
dual frame, the, 564
frame coefﬁ cients, 562
notation, 562
overcompleteness, 561
signal representation, 561
windowed Fourier analysis schemes, 565
Signal processing theory
application areas, 3
adaptive ﬁ ltering, 23
continuous-time signal and systems, 3–4
digital processing system, 8
discrete-time signal and systems, 5, 18
ﬁ lter banks, 16
ﬁ lter structure (digital), 13
frames, 22
multirate processing, 14
parameter estimation, 23
random signals, 7
shear operation, in matrix theory, 437
stochastic process, 7
wavelets, 16
Signal representation
basis search, 487
classiﬁ cation, 171
functional bases, 470
Signals and systems, parametric models, 594
autoregressive (AR) models, 595
autoregressive moving average (ARMA) 
models, 598
input-output transfer function, 596–597
moving average (MA) models, 597
tracking colored noise, 597
Singular value decomposition (SVD)
application, 1156
basic algorithms, 1166
data analysis, 1162
large scale problems, 1166, 1218
SOBI algorithm, 1182
Software deﬁ ned radios (SDRs)
base-band shifting, 402
concept, origin of, 397
deﬁ nition, controversies, 395–396
multirate structures, 341
optimizing issues, 342
output sampling frequency, 383
polyphase channelizers, 339, 342, 364
signal parameters, 395, 397, 400
Sparse PCA
deﬂ ation technique, 1173
standard extension, 1170

1505
Index
Sparse vectors
matrix completion, 1357, 1362
robust PCA, 1360, 1363
Sparsiﬁ cation
coherence-based strategy, 920
computational complexity, 965
nonlinear modeling., 892
novelty criterion, 919, 923
quantization strategy, 938–939, 966
simulation results, 968
surprise-based strategy, 920–921
various strategies, 924, 926
Speech and audio coding applications, modulated 
design, 460
Speed and precision, tradeoff between
convergence rate and mis-adjustment, 
compromise, 632
eigenvalue spread of matrix, 636–637
gradient algorithm, 634
mis–adjustment, 631
quadratic cost function, 619–620
steepest descent algorithm performance, 636
State-space description
deﬁ ned, 84
ﬁ lter, deﬁ ned, 106
frequency domain equations, 88
general realization, 84–85, 93
IIR system structure, 91–93
input–output transfer function, 94, 100
transfer function, 110
vector equations, 86
Stochastic algorithms, 674
linearly parametrized classes, 674
output, 675
Stochastic gradient (LMS) algorithm, 609
Stochastic processes, sampling
error power, 202
reconstruction, 197
uniform, 195
Sub-Nyquist sampling
machine learning, 1304
Subspace clustering
matrix factorization, 1130
popular algorithms in, 1127
Sufﬁ ciently rich signals, 657
Support vector machine (SVM)
application, 847
fundamental kernal methods, 858
linear classiﬁ er, 834
nonlinear patterns, 857–858
structural risk minimization, 833
Symmetric matrices, 640
Synapse, simple perceptron, 819
T
Tensors
basic models, 1210
decompositions models, 1210
multilinear operations, 1208
Theoretical descriptions, information 
based learning
divergence, 1382
entropy, 1381
mutual information, 1383
Tight frames, 569–570
Time-frequency content analysis, 585
coefﬁ cients obtained, 586
step based decomposition 
algorithm, 586–587
Wigner-Ville distribution (WD), 585
Time-varying AR (1) model, 616
Topographic mapping, 843, 845–846
Training data
estimation problem, 953
ﬁ tting sets, 884–886, 888
hinge loss functions, 971
learning task, 958
quantization, 921
Transfer function
Bode Plots, 63–64
convolution integral, 56
in Laplace transform, 55
initial condition, 57
pole location, 58
pole-zero diagram, 57
U
Undermodeling, 653
Undersampling
distortions in, 190
examples, 190
Unit-step signal, 31
Universality, 612, 615
sequential adaptive algorithm, 615–616
superior steady state performance, 615
universal linear predictor performance, 616
Universal model order estimation, 609
min-max optimal performance, 611–612
number of parameters increased, 609–610
Unsupervised learning, 1116–1117, 1135
V
Vapnik-Chervonenkis dimension, 825
Vector quantization, loss minimization, 1121

1506
Index
W
Wave digital ﬁ lters, 282
current, 284
descriptions, 284
design, 282
power, 285
reﬂ ectance function, 285
voltage, 284
Wave-ﬂ ow building blocks, 285
circuit elements, 285
circulator, 288
matched termination, 286
open-ended unit element, 285, 287
resistive load, 286
short-circuited unit element, 286–287
voltage signal source, 287
Wavelet frame, 575, 578
deﬁ ned, 530
directional ﬁ lter banks, 531
Heisenberg boxes, 581
Laplacian ﬁ lter banks, 529
Laplacian pyramid, 522
multiresolution analysis, 479
subsampling, 474
Wavelet transform
applications, 514
classical 2-D function, 533–534
continous signal, 477
dilations and translations, 473–474
dimensionality, reduction, 478
dyadic sampling, 517
8-tap orthogonal ﬁ lter, 448
ﬁ lter banks, 444–445
5-/3-tap symmetric ﬁ lter, 447
4-tap orthogonal ﬁ lter, 447
Laplacian pyramid, 522
lifting step, 448
local cosine bases, 485
lowpass ﬁ lter, 545
mathematical properties, 467
multiscale analysis, 476
9-/7-tap symmetric ﬁ lter, 448
packet bases, 486
properties, 481
reconstruction formula, 476
smoothness, 481
spectral factorization, 444, 446
symmetry, 482
trous algorithm, 516
vanishing moments, 481
Weight-error, covariance matrix, 693–694
Weyl-Heisenberg or Gabor frame, 575
Whitening, 1161
Widely-linear complex
ﬁ lter, 663
least-mean squares, 662
Wirtinger’s Calculus, 672
application, 926–927
in complex LMS, 925–926
in Pure Complex KLMS (PCKLMS), 928
Wirtinger derivatives
Cauchy-Riemann conditions, 671–672
minimization problem, 670–671
quadratic function, 674
real functions of complex variables, 672
Z
Zero and non-zero mean variables, 655
extended regressor, 656–657
extended weight vector, 656–657
z-transform, polyphase decomposition, 472


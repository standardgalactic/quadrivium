
Foundations  of  Probabilistic  
Logic  Programming  
Languages,  Semantics,  lnference  and  Learning  
Second  Edition  

RIVER  PUBLISHERS  SERIES  IN  SOFTWARE  ENGINEERING   
The "River Publishers Series in Software Engineering" is a series of comprehensive academic 
and professional books which focus on the theory and applications of Computer Science in 
general, and more specifically Programming Languages, Software Development and Software 
Engineering. 
Books published in the series include research monographs, edited volumes, handbooks 
and textbooks. The books provide professionals, researchers, educators, and advanced students 
in the field with an invaluable insight into the latest research and developments. 
Topics covered in the series include, but are by no means restricted to the following: 
o  Software Engineering  
o  Software Development  
o  Programming Languages  
o  Computer Science  
o  Automation Engineering  
o  Research Informatics  
o  Information ModeHing  
o  Software Maintenance  
For a list of other books in this series, visit www.riverpublishers.com 

Foundations  of  Probabilistic  
Logic  Programming  
Languages,  Semantics,  lnference  and  Learning  
Second  Edition  
Fabrizio Riguzzi 
University of Ferrara, Italy 
~ 
~ ~~ ~~~J~!n~~~up
Rivcu Publi1he11  
NEW  YORK  AND  LONDON  

Published 2023 by River Publishers  
River Publishers  
Alsbjergvej 10, 9260 Gistrup, Denmark  
www.riverpublishers.com  
Distributed exclusively by Routledge  
605 Third Avenue, New York, NY 10017, USA  
4 Park Square, Milton Park, Abingdon, Oxon OX14 4RN  
Foundations  of Probabilistic  Logic  Programming  /by Fabrizio Riguzzi. 
©  2023 River Publishers. All rights reserved. No part of this publication may 
be reproduced, stored in a retrieval systems, or transmitted in any form or by 
any means, mechanical, photocopying, recording or otherwise, without prior 
written permission of the publishers. 
Routledge is an imprint of the Taylor &  Francis Group, an informa 
business 
ISBN 978-87-7022-719-3 (print) 
978-10-0092-321-6 (online) 
978-1-003-42742-1 (ebook master) 
While every effort is made to provide dependable information, the 
publisher, authors, and editors cannot be held responsible for any errors or omis­
sions. 

1 
Foreword 
Preface to the 2nd Edition 
Preface 
Acknowledgments 
List of Figures 
List of Tables 
List of Examples 
List of Definitions 
List of Theorems 
List of Acronyms 
Preliminaries 
1.1 
Orders, Lattices, Ordinals 
1.2 
Mappings and Fixpoints . 
1.3 
Logic Programming . . . 
1.4 
Semantics for Normal Logic Programs 
1.4.1 
Program completion . . 
1.4.2 
Well-founded semantics 
1.4.3 
Stable model semantics 
1.5 
Probability Theory . . . . . . . 
1.6 
Probabilistic Graphical Models . 
xi 
xiii 
XV  
xix 
xxi 
xxvii 
xxix 
xxxiii 
xxxvii 
xxxix 
1 
1 
3 
4 
13 
14 
16 
22 
24 
33 
Contents   
V  

vi 
Contents  
2 Probabilistic  Logic  Programming  Languages   
43  
2.1 
Languages with the Distribution Semantics . . . . . . 
43  
2.1.1 
Logic programs with annotated disjunctions 
44  
2.1.2 
ProbLog . . . . . . . . . . .  
45  
2.1.3 
Probabilistic hom abduction . . . . . . . . . 
45  
2.1.4 
PRISM . . . . . . . . . . . . . . . . . . . . 
46  
2.2  
The Distribution Semantics for Programs Without Function  
Symbols . . . . . . . . . . . . . 
47  
2.3  
Examples ofPrograms. . . . . . . . 
52  
2.4  
Equivalence of Expressive Power . . 
58  
2.5  
Translation into Bayesian Networks. 
60  
2.6  
Generality of the Distribution Semantics 
64  
2.7  
Extensions of the Distribution Semantics 
66  
2.8  
CP-logic . . . . . . . . . . . . . . . . . 
68  
2.9  
KBMC Probabilistic Logic Programming Languages 
74  
2.9.1 
Bayesian 1ogic programs .  
74  
2.9.2 
CLP(BN) . . . . . . . . . . . . . . . . . . . 
74  
2.9.3 
The prolog factor langnage . . . . . . . . . 
77  
2.10 
Other Semantics for Probabilistic Logic Programming . 
79  
2.10.1 
Stochastic logic programs . . .  
79  
2.10.2 
ProPPR . . . . . . . . . . . .  
80  
2.11 
Other Semantics for Probabilistic Logics  
82  
2.11.1 
Nilsson's probabilistic logic . .  
82  
2.11.2 
Markov logic networks . . . .  
83  
2.11.2.1  Encoding Markov logic networks with  
probabilistic logic programming 
83  
2.11.3 
Annotated probabilistic logic programs . . . . . . 
86  
3 Semantics  with  Function  Symbols   
89  
3.1  
The Distribution Semantics for Programs with Function Sym­
bols 
. . . . . . . . . . . . . . . . . . . . . . . 
91  
3.2  
Infinite Covering Set of Explanations . . . . . . 
95  
3.3  
Comparison with Sato and Kameya's Definition 
110  
4 Hybrid  Programs   
115  
4.1 
Hybrid ProbLog . . .  
115  
4.2 
Distributional Clauses  
118  
4.3 
Extended PRISM . .  
124  
4.4 
cplint Hybrid Programs  
126  

Contents  vii 
4.5 
Probabilistic Constraint Logic Programming  
130 
4.5.1  
Dealing with imprecise probability 
distributions . . . . . . . . . . . . 
135 
5  Semantics  for  Hybrid  Programs  with  Function  Symbols  
145  
5.1 
Examples ofPCLP with Function Symbols .  
145 
5.2 
Preliminaries . . . . . . . . . . . . . . .  
147 
5.3 
The Semantics of PCLP is Well-defined . .  
155 
6 Probabilistic  Answer  Set  Programming   
165  
6.1 
A Semantics for Unsound Programs  
165 
6.2 
Features of Answer Set Programming .  
170 
6.3 
Probabilistic Answer Set Programming  
172 
7  Complexity  of Inference   
175  
7.1 
Inference Tasks . . . . . . . . . . . . . .  
175 
7.2 
Background on Complexity Theory . . . .  
176 
7.3 
Complexity for Nonprobabilistic Inference  
178 
7.4 
Complexity for Probabilistic Programs . .  
180 
7 .4.1  
Complexity for acyclic and locally stratified pro­
grams .................... . 
180 
7.4.2  
Complexity results from [Maua and Cozman, 
2020] ..................... . 
182 
8  Exact  Inference   
185  
8.1 
PRISM ..  
186 
8.2 
Knowledge Compilation.  
190 
8.3 
ProbLog1  
191 
8.4 
cplint ..  
194 
8.5 
SLGAD .  
197 
8.6 
PITA ...  
198 
8.7 
ProbLog2  
202 
8.8 
Tp  Compilation  
216 
8.9 
MPE and MAP  
218 
8.9.1  
MAP and MPE in probLog 
218 
8.9.2  
MAP and MPE in PITA 
219 
8.10 
Modeling Assumptions in PITA .  
226 
8.10.1  
PITA(OPT) 
. . 
230 
8.10.2  VIT with PITA . . . . . 
235 

viii 
Contents  
8.11  Inference for Queries with an Infinite Number of  
Explanations . . . . . . . . . . . . . . . . . . . . 
235  
9  Lifted  Inference   
237  
9.1 
Preliminaries on Lifted Inference  
237  
9.1.1 
Variableelimination  
239  
9.1.2 
GC-FOVE ...... .  
243  
9.2 
LP2 . 
. ............ .  
244  
9.2.1 
Translating probLog into PFL .  
244  
9.3 
Lifted Inference with Aggregation Parfactars . 
247  
9.4 
Weighted First-order Model Counting  
249  
9.5 
Cyclic Logic Programs . . . .  
252  
9.6 
Comparison of the Approaches  
252  
10  Approximate  Inference   
255  
10.1 
ProbLog1 ......... .  
255  
10.1.1 
Iterative deepening.  
255  
10.1.2 
k-best ...  
257  
10.1.3 
Monte carlo  
258  
10.2  MCINTYRE ..... 
260  
10.3  Approximate Inference for Queries with an InfiniteNumber  
of Explanations . . . . . . . . . . . 
263  
10.4  Conditional Approximate Inference . . . . . . . . . . . . . 264  
10.5  k-optimal . . . . . . . . . . . . . . . . . . . . . . . . . . 266  
10.6  Explanation-based Approximate Weighted Model Counting 268  
10.7  Approximate Inference with Tp-compilation . . . . . . . . 270  
11  Non-standard  Inference   
273  
11.1 
Possibilistic Logic Programming  
273  
11.2 
Decision-theoretic ProbLog  
275  
11.3 
Algebraic ProbLog . . . .  
284  
12  Inference  for  Hybrid  Programs   
293  
12.1 
Inference for Extended PRISM  
293  
12.2 
Inference with Weighted Model Integration  
300  
12.2.1 
Weighted Model Integration . . . .  
300  
12.2.2 
Algebraic Model Counting . . . .  
302  
12.2.2.1  The probability density semiring  
and WMI . . . . . . . . . . . . . 
304  

Contents  ix  
12.2.2.2  Symbo . . . . . . . . . . . 
305  
12.2.2.3  Sampo . . . . . . . . . . . . 
307  
12.3  Approximate Inference by Sampling for Hybrid  
Programs . . . . . . . . . . . . . . . . . . . . . 
309  
12.4  Approximate Inference with Bounded Error for Hybrid Pro­
grams . . . . . . . . . . . . . . . . . . . . . . . . . 
311  
12.5  Approximate Inference for the DISTR and EXP Tasks . . . 314  
13  Parameter  Learning   
319  
13.1 
PRISM Parameter Learning . . . . . . . .  
319  
13.2 
LLPAD and ALLPAD Parameter Learning  
326  
13.3 
LeProbLog . . . . . . . . . . .  
328  
13.4 
EMBLEM ................ .  
332  
13.5 
ProbLog2 Parameter Learning ...... .  
342  
13.6 
Parameter Learning for Hybrid Programs .  
343  
13.7 
DeepProbLog ........... .  
344  
13.7.1 
DeepProbLog inference ..  
346  
13.7.2 
Learning in DeepProbLog .  
347  
14  Structure  Learning   
351  
14.1 
Inductive Logic Programming . . . . . . .  
351  
14.2 
LLPAD and ALLPAD Structure Learning  
354  
14.3 
ProbLog Theory Compression  
357  
14.4 
ProbFülL and ProbFOIL+  
358  
14.5 
SLIPCOVER . . . . . . . . . .  
364  
14.5.1 
The language bias . .  
364  
14.5.2 
Description of the algorithm .  
364  
14.5.2.1  Function lNITIALBEAMS 
366  
14.5.2.2  Beam search with clause refinements 
368  
14.5.3 
Execution Example ....... .  
369  
14.6 
Learning the Structure of Hybrid Programs .  
372  
14.7 
Scaling PILP . . . . . . . . . . .  
378  
14.7.1 
LIFTCOVER .....  
378  
14.7.1.1  Liftahle PLP 
379  
14.7.1.2  Parameter learning 
381  
14.7.1.3  Structure learning . 
386  
14.7.2 
SLEAHP . . . . . . . . . . .  
389  
14.7.2.1  Hierarchical probabilistic logic  
programs . . . . . . . . . . . . 
389  

x 
Contents  
14.7.2.2 Parameter leaming 
397  
14.7.2.3 Structure leaming . 
409  
14.8 
Examples of Datasets . . . . . . . . . 
416  
15  cplint  Examples  
417  
15.1 
cplint Commands .............. .  
417  
15.2 
Natural Langnage Processing ........ .  
421  
15.2.1 
Probabilistic context-free grammars . 
421  
15.2.2 
Probabilistic left comer grammars 
422  
15.2.3 
Hidden Markov models .. 
423  
15.3 
Drawing Binary Decision Diagrams . 
425  
15.4 
Gaussian Processes . . . . . . . . . 
426  
15.5 
Dirichlet Processes ........ .  
430  
15.5.1 
The stick-breaking process 
431  
15.5.2 
The Chinese restauraut process 
434  
15.5.3 
Mixturemodel . 
436  
15.6 
Bayesian Estimation .... 
437  
15.7 
Kaiman Filter . . . . . . . 
439  
15.8 
Stochastic Logic Programs 
442  
15.9 
Tile Map Generation ... 
444  
15.10 Markov Logic Networks .. 
446  
15.11 Truel ........... .  
447  
15.12 Coupon Collector Problem 
451  
15.13 One-dimensional Random Walk. 
454  
15.14 Latent Dirichlet Allocation 
455  
15.15 The Indian GPA Problem 
459  
15.16 Bongard Problems . 
461  
16  Conclusions  
465  
Bibliography  
467  
Index  
493  
About  the  Author  
505  

Foreward  
The computational foundations of Artificial Intelligence (AI) are supported 
by two comer stones: logics and machine leaming. Computationallogic has 
found its realization in a number of frameworks for logic-based approaches to 
knowledge representation and automated reasoning, such as Logic Program­
ming, Answer Set Programming, Constraint Logic Programming, Descrip­
tion Logics, and Temporal Logics. Machine Leaming, and its recent evolution 
to Deep Leaming, has a huge number of applications in video surveillance, 
social media services, big data analysis, weather predictions, spam filtering, 
online customer support, etc. 
Ernerging interest in the two communities for finding a bridge connecting 
them is witnessed, for instance, by the prize test-of-time,  20  years  assigned 
by the association for logic programming in 2017 to the paperHybrid  Prob­
abilistic  Programs.  Also in 2017, Holger H. Hoos was invited to give the 
talk The  best  of  both  worlds:  Machine  learning  meets  logical  reasoning  at 
the international conference on logic programming. Here, machine leaming 
is used to tune the search heuristics in solving combinatorial problems (e.g., 
encoded using SAT or ASP techniques). A couple of months later, in a panel 
organized by the Italian Association for Artificial Intelligence (AI*IA), the 
machine leaming researcher Marco Gori posed five questions to the commu­
nities. Among them: How  can  we  integrate  huge  knowledge  bases  naturally  
and  effectively  with  learning  processes?  How  to  break  the  barriers  of machine  
learning  vs  (inductive)  logic  programming  communities?  How  to  derive  a  
computational  model  capable  of dealing  with  learning  and  reasoning  both  
in  the  symbolic  and  sub-symbolic  domains?  How  to  acquire  latent  seman­
fies?  These are fundamental questions that need to be resolved to allow AI 
research to make another quantum leap. Logicallanguages can add structural 
semantics to statistical inference. 
This book, based on 15 years oftop-level research in the field by Fabrizio 
Riguzzi and his co-authors, addresses these questions and fills most of the 
gaps between the two communities. A mature, uniform retrospective of sev­
eral proposals of languages for Probabilistic Logic Programming is reported. 
xi 

xii 
Foreward  
The reader can decide whether to explore all the technical details or simply 
use such languages without the need of installing tools, by simply using the 
web site maintained by Fabrizio's group in Ferrara. 
The book is self-contained: all the prerequisites coming from discrete 
mathematics ( often at the foundation of logical reasoning) and continuous 
mathematics, probability, and statistics (at the foundation of machine learn­
ing) are presented in detail. Although all proposals are summarized, those 
based on the distribution semantics are dealt with in a greater level of detail. 
The book explains how a system can reason precisely or approximately when 
the size ofthe program (and data) increases, even in the case on non-standard 
inference (e.g., possibilistic reasoning). The book then moves toward param­
eter learning and structure learning, thus reducing and possibly removing the 
distance with respect to machine learning. The book closes with a lovely 
chapter with several encodings in PLP. A reader with some knowledge of 
logic programming can start from this chapter, having fun testing the pro­
grams (for instance, discovering the best strategy tobe applied during a truel,  
namely, a duel involving three gunners shooting sequentially) and then move 
to the theoretical part. 
As the president ofthe Italian Association for Logic Programming (GULP) 
I am proud that this significant effort has been made by one of our associates 
and former member of our Executive Committee. I believe that it will be­
come a reference book for the new generations that have to deal with the new 
challenges coming from the need of reasoning on Big Data. 
Agostino Dovier 
University of Udine 

Preface  to  the  Second  Edition  
The field of Probabilistic Logic Programming is rapidly growing and much 
has happened since the first edition of this book in 2018. This new edition 
aims at reporting the most exciting novelties since 2018. 
The semantics for hybrid programs with function symbols was placed on 
asound footing and this is presented in Chapter 5. 
Probabilistic Answer Set Programming gained a lot ofinterest and a whole 
chapter is now devoted to it (Chapter 6). Several works have started to appear 
on the complexity of inference in PLP and PASP and they are now surveyed 
in Chapter 7. 
Algorithms specifically devoted to solving the MPE and MAP tasks are 
described in Section 8.9. 
Inference for hybrid programs has changed dramatically with the intro­
duction of Weighted Model Integration (see Section 12.2) so that the whole 
set of inference approaches for hybrid programs is now collected in their own 
Chapter 12. 
With respect to learning, the first approaches for neuro-symbolic integra­
tion have appeared (DeeProbLog, see Section 13.7) together with algorithms 
for structure learning hybrid programs (DiceML, see Section 14.6). 
Moreover, given the cost of learning PLPs, various works proposed lan­
guage restrictions to speed up learning and improve its scaling: LIFTCOVER, 
see Section 14.7 .1, and SLEAHP, see Section 14.7 .2. 
Finally, this second edition gave me the opportunity to fix various errors 
and imprecisions that were unfortunately present in the first edition. 
xiii 


Preface  
The field of Probabilistic logic programming (PLP) was started in the early 
1990s by seminal works such as those of [Dantsin, 1991], [Ng and Subrah­
manian, 1992], [Poole, 1993b], and [Sato, 1995]. 
However, the problern of combining logic and probability has been stud­
ied since the 1950s [Carnap, 1950; Gaifman, 1964]. Then the problern be­
came prominent in the field of Artificial Intelligence in the late 1980s to 
early 1990s when researchers tried to reconcile the probabilistic and logical 
approaches to AI [Nilsson, 1986; Halpern, 1990; Fagin and Halpern, 1994; 
Halpern, 2003]. 
The integration of logic and probability combines the capability of the 
first to represent complex relations among entities with the capability of the 
latter to model uncertainty over attributes and relations. Logic programming 
provides a Turing complete langnage based on logic and thus represents an 
excellent candidate for the integration. 
Since its birth, the field of Probabilistic Logic Programming has seen a 
steady increase of activity, with many proposals for languages and algorithms 
for inference and learning. The langnage proposals can be grouped into two 
classes: those that use a variant of the Distribution semantics (DS) [Sato, 
1995] and those that follow a Knowledge Base Model Construction (KBMC) 
approach [Wellman et al., 1992; Bacchus, 1993]. 
Under the DS, a probabilistic logic program defines a probability distribu­
tion over normallogic programs and the probability of a ground query is then 
obtained from the joint distribution of the query and the programs. Some of 
the languages following the DS are: Probabilistic Logic Programs [Dantsin, 
1991], Probabilistic Horn Abduction [Poole, 1993b], PRISM [Sato, 1995], 
Independent Choice Logic [Poole, 1997], pD [Fuhr, 2000], Logic Programs 
with Allnotated Disjunctions [Vennekens et al., 2004], ProbLog [De Raedt 
et al., 2007], P-log [Baral et al., 2009], and CP-logic [Vennekens et al., 2009]. 
XV 

xvi 
Preface  
Instead, in KBMC languages, a program is seen as a template for generat­
ing a ground graphical model, be it a Bayesian network or a Markov network. 
KBMC 
languages 
include 
Relational 
Bayesian 
Network 
[Jaeger, 1998], CLP(BN) [Costa et al., 2003], Bayesian Logic Programs [Ker­
sting and De Raedt, 2001], and the Prolog Factor Language [Gomes and 
Costa, 2012]. The distinction among DS and KBMC languages is actually 
non-sharp as programs in languages following the DS can also be translated 
into graphical models. 
This book aims at providing an overview of the field of PLP, with a special 
emphasis on languages under the DS. The reason is that their approach to 
logic-probability integration is particularly simple and coherent across lan­
guages but nevertheless powerful enough tobe useful in a variety of domains. 
Moreover, they can be given a semantics in purely logical terms, without 
necessarily resorting to a translation into graphical models. 
The book doesn't aim though at being a complete account of the topic, 
even when restricted to the DS, as the field has grown large, with a dedicated 
workshop series started in 2014. My objective is to present the main ideas for 
semantics, inference, and learning and to highlight connections between the 
methods. 
The intended audience of the book are researchers in Computer Science 
and AI that want to getan overview of PLP. However, it can also be used 
by students, especially graduate, to get acquainted with the topic, and by 
practitioners that would like to get more details on the inner workings of 
methods. 
Many examples of the book include a link to a page of the web application 
cplint on SWISH (https://cplint.eu) [Riguzzi et al., 2016a; Alberti et al., 
2017], where the code can be run online using cplint, a system we devel­
oped at the University of Perrara that includes many algorithms for inference 
and learning in a variety of languages. 
The book starts with Chapter 1 that presents preliminary notions of logic 
programming and graphical models. Chapter 2 introduces the languages un­
der the DS, discusses the basic form of the semantics, and compares it with 
alternative approaches in PLP and AI in general. Chapter 3, describes the 
semantics for languages allowing function symbols. Chapters 4 and 5 present 
the semantics for languages with continuous random variables, without and 
with function symbols respectively. Probabilistic Answer Set Programming is 
discussed in Chapter 6 while Chapter 7 presents complexity results. Chapter 
8 illustrates various algorithms for exact inference. Lifted inference is dis­
cussed in Chatper 9 and approximate inference in Chapter 10. Non-standard 

Preface  xvii 
inference problems are illustrated in Chapter 11. Chapter 12 presents infer­
ence for programs with continuous random variables. Then Chapters 13 and 
14 treat the problern of learning parameters and structure of programs, re­
spectively. Chapter 15 presents some examples ofuse ofthe system cplint. 
Chapter 16 concludes the book discussing open problems. 


Acknowledgments   
I am indebted to many persons for their help and encouragement. Evelina 
Lamma and Paola Mello taught me to love logical reasoning and always sup­
ported me, especially during the bad times. My co-workers at the University 
of Perrara Evelina Lamma, Elena Bellodi, Riccardo Zese, Giuseppe Cota, 
Marco Alberti, Marco Gavanelli, Amaud Nguembang Fadja and Damiano 
Azzolini greatly helped me shape my view of PLP through exiting joint work 
and insightful discussions. I have been lucky enough to collaborate also with 
Theresa Swift, Nicola Di Mauro, Stefano Bragaglia, Vitor Santos Costa, and 
Jan Wielemaker and the joint work with them has found its way into the book. 
Agostino Dovier, Evelina Lamma, Elena Bellodi, Riccardo Zese, Giuseppe 
Cota, and Marco Alberti read drafts of the book and gave me very useful 
comments. 
I would also like to thank Michela Milano, Federico Chesani, Paolo Tor­
roni, Luc De Raedt, Angelika Kimmig, Wannes Meert, Joost Vennekens, and 
Kristian Kersting for many enlightening exchanges of ideas. 
This book evolved from a number of articles. In particular, Chapter 2 is 
based on [Riguzzi and Swift, 2018], Chapter 3 on [Riguzzi, 2016], 
Chapter 5 on [Azzolini et al., 2021], Section 8.6 on [Riguzzi and Swift, 
2010, 2011, 2013], Section 8.10 on [Riguzzi, 2014], Section 10.2 on [Riguzzi, 
2013], Chapter 9 on [Riguzzi et al., 2017a], Section 13.4 on [Bellodi and 
Riguzzi, 2013, 2012], Section 14.2 on [Riguzzi, 2004, 2007b, 2008b], Section 
14.5 on [Bellodi and Riguzzi, 2015], Section 14.7.1 on [Nguembang Fadja 
and Riguzzi, 2019], Section 14.7.2 on [Nguembang Fadja et al., 2021] and 
Chapter 15 on [Riguzzi et al., 2016a; Alberti et al., 2017; Riguzzi et al., 
2017b; Nguembang Fadja and Riguzzi, 2017]. 
Finally, I would like to thank my wife Cristina for putting up with a 
busband with the crazy idea of writing a book without taking a sabbatical. 
Without her love and support, I would not have been able to bring the idea 
into reality. 
xix 


List  of  Figures  
Figure 1.1 
SLD tree for the query path( a,  c) from the program 
of Example 1. . . . . . . . . . . . . . . . . . . . . 
11 
Figure 1.2 
SLDNF tree for the query ends(b,  c) from the pro­
gram of Examp1e 1. . . . . . . . . . . . . . . . . . 
12 
Figure 1.3 
SLDNF tree for the query c from the program of 
Examp1e 2 ................. . 
16 
Figure 1.4 
Ordinal powers of IFPP  for the program of 
Example 4 ........ . 
21 
Figure 1.5 
Gaussian densities. . ..... . 
29 
Figure 1.6 
Bivariate Gaussian density. . . . 
31 
Figure 1.7 
Examp1e of a Bayesian network. 
36 
Figure 1.8 
Markov blanket. Figure from https://commons. 
wikimedia.org/wiki/File:Diagram_of_a_Markov _ 
b1anket.svg. . . . . . . . . . . . . . . . . . . . . 
37 
Figure 1.9 
Example of a Markov newtork. . . . . . . . . . . 
39 
Figure 1.10 
Bayesian network equivalent to the Markov network 
of Figure 1.9. . . . .... 
40 
Figure 1.11 
Examp1e of a factor graph. . . . . . . . . . . . . . 
41 
Figure 2.1 
Example of a BN. . . . . . . . . . . . . . . . . . . 
58 
Figure 2.2 
BN ß(P)  equivalent to the program ofExample 29. 
62 
Figure 2.3 
Portion of 'Y(P)  relative to a clause Ci . ...... . 
63 
Figure 2.4 
BN 'Y(P)  equivalent to the program of Example 29. 
64 
Figure 2.5 
BN representing the dependency between a( i) and 
b( i). . ....................... . 
65 
Figure 2.6 
BN modeling the distribution over a(i),  b(i),  X1, 
X2,X3.................... . 
66 
Figure 2.7 
Probability tree for Example 2.11. . . . . . . 
69 
Figure 2.8 
An incorrect probability tree for Example 31. 
70 
Figure 2.9 
A probability tree for Examp1e 31. . . . . 
71 
Figure 2.10 
Ground Markov network for the MLN of 
Example 37 ................ . 
84 
xxi 

xxii 
List  of Figures  
Figure  4.1  
Credal set specification for Examples 60 and 62. 
137 
Figure  6.1  
Example of a probabilistic graph. . . . . . 
173 
Figure  8.1  
Explanations for query hmm([a,  b,  b])  of 
Example 74. . . . . . . . . . . . . . . . . 
187 
Figure  8.2  
PRISM formulas for query hmm([a,  b,  b])  of Ex­
ample 74. . . . . . . . . . . . . . . . . . . . . . . 188 
Figure  8.3  
PRISM computations for query hmm([a,  b,  b])  of 
Example 74. . . . . . . . . . . . . . . . . . . . . . 189 
Figure  8.4  
BDD representing Function 8.1. . . . . . . . . . . 192 
Figure  8.5  
Binary decision diagram (BDD) for query epidemic  
of Example 75. . . . . . . . . . . . . . . . . . . . 193 
Figure  8.6  
Multivalued decision diagram (MDD) for the diag­
nosis program of Example 20. . . . . . . . . . . . 196 
Figure  8.7  
BDD representing the function in Equation (8.2). . 201 
Figure  8.8  
Deterministic decomposable negation normal form 
(d-DNNF) for the formula ofExample 81. . . . . 
208 
Figure  8.9  
Arithmetic circuit for the d-DNNF of Figure 8.8. . 209 
Figure  8.10  BDD for the formula ofExample 81. . . . . . . . . 212 
Figure  8.11  
Sentential decision diagram (SDD) for the formula 
ofExample 81. . . . . . . . . . . . . . . 
213 
Figure  8.12  vtree for which the SDD of Figure 8.11 is 
normalized. . . . . . . . . . . . . . . . . 
213 
Figure  8.13  
CUDD BDD for the query ev of Example 83. La­
bels Xij indicates the jth Boolean variable of ith 
ground clause and the label of each node is a part of 
its memory address. . . . . . . . . . . . . . . . . . 222 
Figure  8.14  BDD for the MPE problern of Example 83. The 
variables XO_k and X1_k are associated with the 
second and first clause respectively. 
. . . . . . . . 224 
Figure  8.15  BDD for the MAP problern of Example 84. The 
variables XO_k and X1_k are associated with the 
second and first clause respectively. 
. . . . . . . . 224 
Figure  8.16  Translation from BDDs to d-DNNF. . . . . . . . . 226 
Figure  8.17  Examples of graphs satisfying some of the assump­
tions. 
. ................... . 
229 
Figure  8.18  Code for the or /3 predicate of PITA(OPT). 
231 
Figure  8.19  Code for the and/3  predicate of PITA(OPT). 
232 
Figure  8.20  Code for the exc/2  predicate of PITA(OPT).. 
233 
Figure  8.21  
Code for the ind/2  predicate of PITA(OPT).. 
234 

List  of Figures  xxiii 
Figure  8.22   Code for the ev /2 predicate of PITA(OPT). . . . . 234 
Figure  9.1   
BN representing an OR dependency between X 
and Y ......................... 240 
Figure  9.2   
Bayesian network (BN) representing a noisy-OR de­
pendency between X and Y. . . . 
241 
Figure  9.3   
BN of Figure 9.1 after deputation. 
243 
Figure  10.1   Program of Example 92. . . . . . 
256 
Figure  10.2   Probabilistic graph of Example 92. 
256 
Figure  10.3   SLD tree up to depth 4 for the query path( c, d)  from 
the program of Example 92. 
. . . . . . 
256 
Figure  11.1  
BDDdry(o-)  for Example 96. . . . . . . . . . . . . 278 
Figure  11.2  
BDDbroken  umbrella(o-)  for Example 96. . . . . . . 278 
Figure  11.3  
ADD(dry)  for Example 96. The dashed terminals 
indicate ADDutil(dry).  . . . . . . . . . . . . . . . 282 
Figure  11.4  
ADD(broken_umbrella)  for Example 96. The 
dashed terminals indicate ADDutil(broken_  
umbrella).  . . . . . . . . . . . . . . . . . . . . . 282 
Figure  11.5  
ADDf;fz  for Example 96. . . . . . . . . . . . . . . 283 
Figure  11.6  Worlds where the query calls(mary)  from Exam­
ple 100 is true. 
. . . . . . . . . . . 
287 
Figure  12.1  
d-DNNF circuit for Example 107. . . . . . . . . 
306 
Figure  12.2  Arithmetic circuit for Example 107. . . . . . . . 
306 
Figure  12.3  Hybridprobability tree (HPT) for Example 112. . 
313 
Figure  12.4  Partially evaluated hybrid probability tree (PHPT) 
for Example 113. . . . . . . . . . . . . . . . . . . 314 
Figure  12.5  Distribution of sampled values in the Program of 
Example 114. . . . . . . . . . . . . . . . . . . . . 317 
Figure  12.6  Distribution of sampled values from the Gaussian 
mixture of Example 115. . . . . . . . . . . . . . . 318 
Figure  13.1  
BDD for query epidemic  for Example 117. . . . . 333 
Figure  13.2  BDD after applying the merge rule only for Exam­
ple 117 ........................ 334 
Figure  13.3  Forward and backward probabilities. F  indicates the 
forward probability and B  the backward probability 
of each node. . . . . . . . . . . . . . . . . . . . . 341 
Figure  14.1  
A collection of DLTs corresponding to the JMP in 
Example 123. . . . . . . . . . . . . . . . . . . . . 377 

xxiv 
List  of Figures  
Figure  14.2  Bayesian Network representing the dependency be­
tween the query q  and the random variables associ­
ated with groundings of the clauses with the body 
true. . . . . . . . . . . . . . . . . . . . . . 
380 
Figure  14.3  
Probabilistic program tree. . . . . . . . . . 
390 
Figure  14.4  Probabilistic program tree for Example 125. 
391 
Figure  14.5  Arithmetic circuit/neural net. . . . . . . . . 
393 
Figure  14.6  Ground probabilistic program tree for Example 126. 395 
Figure  14.7  Arithmetic circuit/neural net for Example 126. . 
396 
Figure  14.8  Converted arithmetic circuit ofFigure 14.7. . . 
398 
Figure  14.9  Tree created from the bottom clause of 
Example 127. . . . . . . . . . . . . . . . . . . . . 413 
Figure  15.1  
BDD for query pandernie  in the epidernic. pl 
example, drawn using the CUDD function for ex­
porting the BDD to the dot format of Graphviz. . . 426 
Figure  15.2  Functions sampled from a Gaussian process with a 
squared exponential kemel in gpr. pl. . . . . . . 430 
Figure  15.3  
Functions from a Gaussian process predicting points 
with X  =  [0, ... , 10] with a squared exponential 
kemel in gpr. pl. . . . . . . . . . . . . . . . . . 430 
Figure  15.4  Distribution of indexes with concentration parame­
ter 10 for the stick-breaking example 
dirichlet_process. pl. .......... . 433 
Figure  15.5  Distribution of values with concentration parameter 
10 for the stick-breaking example 
dirichlet_process. pl. .......... . 433 
Figure  15.6  Distribution of unique indexes with concentration 
parameter 10 for the stick-breaking example 
dirichlet_process. pl. ........ . 
434 
Figure  15.7  Prior density in the dp_rnix. pl example... . 
437 
Figure  15.8  Posterior density in the dp_rnix. pl example. 
438 
Figure  15.9  Prior and posterior densities in 
gauss_rnean_est .pl. ......... . 
439 
Figure  15.10  Prior and posterior densities in kalrnan. pl. 
441 
Figure  15.11  Example of particle filtering in kalrnan. pl. 
442 
Figure  15.12  Partide filtering for a 2D Kaiman filter. 
. . . 
443 
Figure  15.13  Sampies of sentences of the language defined in 
slp_pcfg. pl. . 
444 
Figure  15.14  A random tile map. . . . . . . . . . . . . . . . 
446 

List  of Figures  xxv 
Figure  15.15  Probability tree of the true1 with opponents a  and b.  449 
Figure  15.16  Distribution of the number of boxes. . . . . . . . . 453 
Figure  15.17  Expected number of boxes as a function of the num­
ber of coupons. . . . . . . . . . . . . . . . . 
454 
Figure  15.18  Smoothed Latent dirich1et allocation (LDA). . . . . 456 
Figure  15.19  Va1ues for word in position 1 of document 1. . . . . 457 
Figure  15.20  Va1ues for coup1es (word,topic) in position 1  ofdoc­
ument 1.  . . . . . . . . . . . . . . . . . . . . . . . 458 
Figure  15.21  Prior distribution of topics for word in position 1  of 
document 1. . . . . . . . . . . . . . . . . . . . . . 458 
Figure  15.22  Posterior distribution of topics for word in  position 
1  of document 1.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  
459 
Figure  15.23  Density of the probability of topic 1  before and af­
ter observing that words 1  and 2 of document 1 are 
equal. . . . . . . 
459 
Figure  15.24  Bongard pictures. . . . . . . . . . . . . . . . . . . 462 


List  of  Tables  
Table  2.1  Conditional probability table for a 2 , n  stands 
for null  . .................. . 
61 
Table  2.2 
Conditional probability table for ch5 
. . . . 
62 
Table  7.1  Complexity of the Well-founded semantics (WFS) .. 
179 
Table  7.2  Complexity of Answer set programming (ASP). . . 
180 
Table  7.3  Complexity of inference for acyclic and locally strat­
ified programs, extracted from [Cozman and Maua, 
2017, Table 2].................... . 
181 
Table  7.4  Complexity of the credal semantics, extracted from 
[Maua and Cozman, 2020, Table 1]......... . 
183 
Table  8.1   Tractability of operations.? means "unknown", .( means 
"tractable" and o means "not tractable unless P=NP". 
Operations are meant over a bounded number of operands 
and BDDs operands should have the samevariable or­
der and SDDs the same vtree. . . . . . . . . . . . . . 215 
Table  11.1   Inference tasks and corresponding semirings for 
aProbLog. . . . . . . . . . . . . . . . . 
287 
Table  14.1  An example of a database. 
. . . . . . . 
372 
Table  14.2  Available statistical model atoms (M~). 
375 
Table  15.1  Associations between variable indexes and ground 
rules . . . . . . . . . . . . . . . . . . . . . . . . . . 426 
xxvii 


5 
10 
15 
20 
25 
30 
1 
Example (Path- Prolog) . . . 
10 
2 
Example (Clark's completion) 
15 
3 
Example (WFS computation) . 
18 
4 
Example (Fixpoint of fppP  beyond w) 
20 
Example (Answer set computation) . . . 
22 
6 
Example (Cautious and brave consequences) . 
23 
7 
Example (Probability spaces) . . . . . . 
26 
8 
Example (Discrete random variables) . . 
26 
9 
Example (Continuous random variables) 
28 
Example (Joint distributions) 
30 
11  
Example (Alarm- BN) . . . . . . . . . 
36 
12 
Example (University- MN) . . . . . . 
39 
13 
Example (Medical symptoms- LPAD) . 
44 
14 
Example (Medical symptoms - ProbLog) 
45 
Example (Medical symptoms- ICL) . . 
46 
16 
Example (Coin tosses- PRISM) . . . . . 
47 
17 
Example (Medical symptoms- PRISM) . 
47 
18 
Example (Medical symptoms - worlds - ProbLog) 
50 
19 
Example (Medical symptoms- worlds- LPAD) . 
52 
Example (Detailed medical symptoms - LPAD) 
53 
21 
Example (Coin- LPAD) . . . . . . . . 
53 
22 
Example (Eruption- LPAD) . . . . . . . 
53 
23 
Example (Monty Hall puzzle - LPAD) . . 
54 
24 
Example (Three-prisoner puzzle- LPAD) 
55 
Example (Russian roulette with two guns- LPAD) 
56 
26 
Example (Mendelian rules of inheritance- LPAD) 
56 
27 
Example (Path probability - LPAD) 
57 
28 
Example (Alarm BN- LPAD) . . . . . . . . . . . 
57 
29 
Example (LPAD to BN). . . . . . . . . . . . . . . 
61 
Example (CP-logic program - infection [Vennekens et al., 
2009]) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
68 
List  of  Examples  
xxix 

xxx 
List  of Examples  
31 
Example (CP-logic program- pneumonia [Vennekens et al., 
2009]) . . . . . . . . . . . . . . . . . . . . . . . . . . 
70  
32 
Example (Invalid CP-logic program [Vennekens et al.,  
2009]) . . . . . . . . . . . . . . . . . . . . . . . . . . 
72  
33 
Example (Sound LPAD - invalid CP-theory Vennekens et al.  
[2009]) . . . . . . . . . . . . . . . . . . . . . . . . 
73  
34 
Example (PFL program) . . . . . . . . . . . . . . . 
78  
35 
Example (Probabilistic context-free grammar- SLP) 
80  
36 
Example (ProPPR program) . . . . . . . . . 
81  
37 
Example (Markov Logic Network) . . . . . . 
83  
38 
Example (Program with infinite set of worlds) 
90  
39 
Example (Game of dice) . . . . . . . . . . . 
90  
40 
Example (Covering set of explanations for Example 38) . 
94  
41 
Example (Covering set of explanations for Example 39) . 
95  
42 
Example (Pairwise incompatible covering set of explanations  
for Example 38) . . . . . . . . . . . . . . . . . . . . . . . . 
95  
43 
Example (Pairwise incompatible covering set of explanations  
for Example 39) . . . . . . . . . . . . . . . . . . . 
95  
44 
Example (Probability of the query for Example 38) 
100  
45 
Example (Probability of the query for Example 39) 
100  
46 
Example (Gaussian mixture- Hybrid ProbLog) . . 
115  
47 
Example (Query over a Gaussian mixture- Hybrid  
ProbLog) . . . . . . . . . . . . . . . . . . . . . . 
117  
48 
Example (Gaussian mixture- DCs) . . . . . . . . 
119  
49 
Example (Moving people- DCs [Nitti et al., 2016]) . 
119  
50 
Example (STp  for moving people -
DC [Nitti et al.,  
2016]) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122  
51 
Example (STp  for moving people- DC [Nitti et al., 2016]) . 123  
52 
Example (Negation in DCs [Nitti et al., 2016]) . . . . . . 
124  
53 
Example (Gaussian mixture - Extended PRISM) . . . . 
125  
54 
Example (Gaussian mixture and constraints- Extended  
PRISM) . . . . . . . . . . . . . . . . . . . . . . 
125  
55 
Example (Gaussian mixture- cplint) . . . . . 
127  
56 
Example (Estimation of the mean of a Gaussian ­
cplint) . . . . . . . . . . . . . . . . . . . . 
127  
57 
Example (Kaiman filter- cplint) . . . . . . . 
128  
58 
Example (Fire on a ship [Michels et al., 2015]) . 
131  
59 
Example (Probability of fire on a ship [Michels et al.,  
2015]) . . . . . . . . . . . . . . . . . . . . . . . . . 
133  
60 
Example (Credal set specification- continuous variables) . 
136  
61 
Example (Credal set specification- discrete variables) 
138  
62 
Example (Conditional probability bounds) . . . . . . . . . 
139  

List  of Examples  xxxi 
63 
Example (Gambling) . . . . . . . . . . . . . . . . . . . . . 145  
64 
Example (Hybrid Hidden Markov Model) . . . . . . . . . . 146  
65 
Example (Pairwise incompatible covering set of explanations  
for Example 63) . . . . . . . . . . . . . . . . . . . . . . . . 150  
66 
Example (Probability of queries for Example 63) . . . . . . 151  
67 
Example (Pairwise incompatible covering set of explanations  
for Example 64) . . . . . . . . . . . . . . . . . . 
153  
68 
Example (Probability of queries for Example 64) . . . . . . 155  
69 
Example (Insomnia [Cozman and Mami, 2017]) . . . . . . . 165  
70 
Example (Insomnia- continued- [Cozman and Maua, 20 17]) 167  
71 
Example (Barber paradox- [Cozman and Maua, 2017]) . . . 167  
72 
Example (Graph Three-Coloring) . . . . . . . . . . . . . . . 172  
73 
Example (Probabilistic Graph Three-Coloring - [Cozman and  
Maua, 2020].) . . . . . . . . . . . . . . . . . . . . . . . . . 172  
7 4 
Example (Hidden Markov model- PRISM [Sato and Kameya,  
2008]) . . . . . . . . . . . . . . . . . . . . . . 
186  
75 
Example (Epidemie- ProbLog) . . . . . . . . 
192  
76 
Example (Detailed medical symptoms- MDD) 
195  
77 
Example (Medical example - PITA) . . . . . . 
200  
78 
Example (Alarm- ProbLog2 [Fierens et al., 2015]) 
203  
79 
Example (Alarm- grounding - ProbLog2 [Fierens et al.,  
2015]) . . . . . . . . . . . . . . . . . . . . . . . . . . . 
204  
80 
Example (Smokers- ProbLog [Fierens et al., 2015]) . . 
205  
81 
Example (Alarm- Boolean formula- ProbLog2 [Fierens et al.,  
2015]) . . . . . . . . . . . . . . . . . . . . . . . 
206  
82 
Example (VIT vs MPE [Shterionov et al., 2015]) 
219  
83 
Example (Bag of balls - MPE) . . . . . . . . 
220  
84 
Example (Bag of balls - MAP) . . . . . . . . 
220  
85 
Example (Disease - Difference between MPE  
and MAP) . . . . . . . . . . . . . . . . . . . 
220  
86 
Example (Running example for lifted inference - ProbLog) . 238  
87 
Example (Running example- PFL program) . . . . . . 
239  
88 
Example (Translation of a ProbLog program into PFL) 
245  
89 
Example (ProbLog program to PFL - LP2 ) 
. . . • 
246  
90 
Example (ProbLog program to PFL - aggregation  
parfactors) . . . . . . . . . . . . . . . . . . . . . . 
248  
91 
Example (ProbLog program to Skolem normal form) 
251  
92 
Example (Path- ProbLog -iterative deepening) . 
255  
93 
Example (Path- ProbLog- k-best) 
. 
257  
94 
Example (Epidemie- LPAD) . . . . . . . . . . . 
261  
95 
Example (Possibilistic logic program) . . . . . . 
274  
96 
Example (Remaining dry [Van den Broeck et al., 2010]). 
276  

xxxii 
List  of Examples  
97 
Example (Continuation of Example 96) . . . . . . . . . 
277  
98 
Example (Continuation of Example 96) . . . . . . . . . 
280  
99 
Example (Viral marketing [Van den Broeck et al., 2010]) 
283  
100 Example (Alarm- aProbLog [Kimmig et al., 2011b]) 
287  
101 Example (Symbolic derivation) . . . . . . . 
294  
102 Example (Success functions of msw  atoms) 
295  
103 Example (Join operation) . . . . . . . . . . 
296  
104 Example (Projection operation) . . . . . . . 
297  
105 Example (Integration of a success function) 
298  
106 Example (Success function of a goal) 
. . . 
298  
107 Example (Broken- SMT(.CRA) [Zuidberg Dos Martires et al.,  
2019, 2018]) . . . . . . . . . . . . . . . . . . . . . . . . . . 300  
108 Example (Broken- WMI [Zuidberg Dos Martires et al., 2019,  
2018]) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301  
109 Example (Broken machine - PCLP [Zuidberg Dos Martires  
et al., 2018]) . . . . . . . . . . . . . . . . . . . . . . . . . . 302  
110 Example (WMI of the broken  theory [Zuidberg Dos Martires  
et al., 2018, 2019].) . . . . . . . . . . . . . . . . . . . 
305  
111 Example (Sampo [Zuidberg Dos Martires et al., 2019]) . . . 308  
112 Example (Machine diagnosis problern [Michels et al.,  
2016]) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312  
113 Example (Machine diagnosis problern - approximate infer­
ence- [Michels et al., 2016]) . . . . . . . . . . . . . 
313  
114 Example (Generative model) . . . . . . . . . . . . . 
316  
115 Example (Gaussian mixture- sampling arguments­
cplint) . . . . . . . . . . . . . . . . . . . . . . 
317  
116 Example (Bloodtype- PRISM [Sato et al., 2017]) . . 
319  
117 Example (Epidemie - LPAD - EM) . . . . . . . . . 
333  
118 Example (Sum ofhandwritten digits) . . . . . . . . . 
344  
119 Example (DeepProbLog program for the sum of handwritten  
digits [Manhaeve et al., 2021]) . . . . . . . 
346  
120 Example (ILP problem) . . . . . . . . . . . . . . . . 
352  
121 Example (Examples of theta subsumption) . . . . . . 
352  
122 Example (Bottom clause example) . . . . . . . . . . 
354  
123 Example (JMP for the banking domain [Kumar et al.,  
2022]) . . . . . . . . . . . . . . . . . . . . . . . 
374  
124 Example (Liftable PLP for the UW-CSE domain) . . 
381  
125 Example (HPLP for the UW-CSE domain) . . . . . . 
391  
126 Example (Complete HPLP for the UW-CSE domain) 
393  
127 Example (Bottom clause for the UW-CSE dataset) . . 
411  
128 Example (HPLP generated by SLEAHP for the UW-CSE do­
main) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 414  

1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
Definition (Tp  operator) . . . . . . . . . . . . 
11  
Definition ( OpTruei  and OpFalsei  operators) 
17  
Definition (Iterated fixpoint) . . . . . . . . . . 
18  
Definition (Acyclic, stratified and locally stratified programs) 
20  
Definition (Reduction) . . . . . . . . . . . . 
22  
Definition (Stable model) . . . . . . . . . . . 
22  
Definition (Cautious and brave consequences) 
23  
Definition (Algebra) . . . . . . . 
24  
Definition (17-algebra) . . . . . . 
24  
Definition (MinimalO"-algebra) . 
24  
Definition (Probability measure) 
25  
Definition (Finitely additive probability measure) 
25  
Definition (Random variable) . . . . . . . . . . . 
26  
Definition (Cumulative distribution and probability density) . 
27  
Definition (Product O"-algebra and product space) . . . 
30  
Definition (Conditional probability) . . . . . . . . . . 
31  
Definition (Independence and conditional indepedence) 
33  
Definition (d-separation [Murphy, 2012]) . . 
36  
Definition (Probability tree - positive case) . 
68  
Definition (Hypothetical derivation sequence) 
70  
Definition (Probability tree- general case) . . 
71  
Definition (Valid CP-theory) . . . . . . . . . 
73  
Definition (Parameterized two-valued interpretations) 
101  
Definition (Parameterized three-valued interpretations) 
101  
Definition ( OpTruePi' ( Tr)  and OpFalsePi' (Fa)) . . 
102  
Definition (Iterated fixpoint for probabilistic programs) 
103  
Definition (Infinite-dimensional product O"-algebra and space) 112  
Definition (Valid program [Gutmann et al., 201lc]) 
120  
Definition (STp  operator [Gutmann et al., 201lc]) . . . . . 122  
List  of  Definitions  
xxxiii 

xxxiv 
List  of Definitions  
30 
Definition (STp  operator [Nitti et al., 
2016]) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122  
31 
Definition (STp  operator- cplint) . . . . . . . . . . . . 129  
32 
Definition (Probabilistic Constraint Logic Theory [Michels  
et al., 2015]) . . . . . . . . . . . . . . . . . 
131  
33 
Definition (Credal set specification) . . . . . . . . . . . . . 135  
34 
Definition (Conditiona1 probability bounds) . . . . . . . . . 138  
35 
Definition (Probabi1istic Constraint Logic Theory - updated) 
148  
36 
Definition (Parameterized two-valued interpretations- PCLP) 156  
37 
Definition (Parameterized three-valued interpretations ­
PCLP) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156  
38 
Definition (OpTruePi(Tr)  and OpFalsePi(Fa)- PCLP) . 157  
39 
Definition (Iterated fixpoint for probabi1istic constraint 1ogic  
programs - PCLP) . . . . . . . . . . . . . . . . . . . . . . . 158  
40 
Definition (Parameterized interpretation [Vlasse1aer et al., 2015,  
2016]) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216  
41 
Definition (Tcp  operator [Vlasselaer et al., 2015, 2016]) . . 216  
42 
Definition (Fixpoint of Tcp  [Vlasselaer et al., 2015, 2016]) . 217  
43 
Definition (PITA MAP Problem) . . . . . . . 
220  
44 
Definition (Semiring) . . . . . . . . . . . . . . . . . 
284  
45 
Definition (aProbLog [Kimmig et al., 201lb]) . . . . 
285  
46 
Definition (Symbolic derivation [Islam et al., 2012b]) 
293  
47 
Definition (Derivation variables [Islam et al., 2012b]) 
294  
48 
Definition (Join operation) . . . . . . . . . . 
296  
49 
Definition (Projection of a success function) . . . 
296  
50 
Definition (Integration of a success function) 
. . 
297  
51 
Definition (Marginalization of a success function) 
298  
52 
Definition (Success function of a goal) . . 
298  
53 
Definition (Satisfiability Modulo Theory) 
300  
54 
Definition (Interpretation and Model) 
. . 
301  
55 
Definition (Weighted Model Integration) . 
301  
56 
Definition (Algebraic Model Counting (AMC) [Kimmig et al.,  
2017]) . . . . . . . . . . . . . . . 
302  
57 
Definition (Neutral-sum property) . . . . . 
303  
58 
Definition (Labeling function a)  . . . . . .  
304  
59 
Definition (Probability density semiring S)  
304  
60 
Definition (PRISM parameter learning problem) . 
319  
61 
Definition (LLPAD Parameter learning problem) 
326  
62 
Definition (Mutually exclusive bodies) . . . . . . 
327  

List  of Definitions  xxxv 
63 
Definition (LeProbLog parameter learning problem) . 
328  
64 
Definition (EMBLEM parameter learning problem) 
332  
65 
Definition (LFI-ProbLog leaming problem) 
342  
66 
Definition (Neural annotated disjunction) . . . . . 
345  
67 
Definition (DeepProbLog program) 
. . . . . . . . 
346  
68 
Definition (Inductive Logic Programming -learning from en­
tailment) . . . . . . . . . . . . . . . . . . . . . . 
351  
69 
Definition (ALLPAD Structure learning problem) . . . . . . 355  
70 
Definition (Theory compression) . . . . . . . . . . . . . . . 357  
71 
Definition (ProbFOIL/ProbFoil+ learning problern [Raedt et al.,  
2015]) . . . . . . . . . . . . . . . . . . . 
358  
72 
Definition (Parameter Learning Problem) 
397  
73 
Definition (Structure Leaming Problem) . 
409  


List  of  Theorems  
1 
Proposition (Monotonie Mappings Have a Least and Greatest  
Fixpoint) 
. . . . . . . . . . . . . . . . . . . . . . . . . . . 
3  
1 
Theorem (WFS for locally stratified programs [Van Gelder  
et al., 1991]) . . . . . . . . . . . . . . . . . . 
20  
2 
Theorem (WFS total model vs stable models) . . . . . . . . 
22  
3 
Theorem (WFS vs stable models) . . . . . . . . . . . . . . . 
22  
4 
Theorem (Cardinality of the set of worlds of a ProbLog pro­
gram) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
89  
1 
Lemma (Infinite Product) . . . . . . . . . . . . . . . . . . . 
90  
5 
Theorem (Existence of a pairwise incompatible set of com­
posite choices [Poole, 2000]) . . . . . . . . . . . . . . . . . 
92  
6 
Theorem (Equivalence of the probability of two equivalent  
pairwise incompatible finite sets of finite composite choices  
[Poole, 1993a]) . . . . . . . . . . . . . . . . . . . . . . . 
92  
7 
Theorem (Algebra of a program) . . . . . . . . . . . . . . 
93  
8 
Theorem (Finitely additive probability space of a program) 
93  
2 
Lemma (O"-algebra of a Program) . . . . . . . . . . . . . . 
97  
3 
Lemma (Existence of the Iimit of the measure of countable  
union of countable composite choices) . . . . . . . . . . . . 
98  
9 
Theorem (Probability space of a pro gram) . . . . . . . . . . 
99  
2 
Proposition (Monotonicity of OpTruePj  and OpFalsePj).  102  
3 
Proposition (Monotonicity of IFPPP)  
103  
4 
Lemma (Soundness of OpTruePj)  
103  
5 
Lemma (Soundness of OpFalsePj)  
104  
6 
Lemma (Partial evaluation) . . . . . 
105  
7 
Lemma (Soundness of IFPPP)  . . 
106  
8 
Lemma (Completeness of IFPPP)  . 
107  
10 
Theorem (Soundness and completeness of IFPPP)  
109  
11  
Theorem (Well-definedness of the distribution semanti es) 
109  
9 
Lemma (Elements of \ll.r  as Countahle Unions) . . . . . 
112  
xxxvii  

xxxviii 
List  of Theorems  
10 
Lemma (r.r  is Bijective) . . . . . . . . . . . . . . . . . . 
113 
12 
Theorem (Equivalence with Sato and Kameya's definition) 
113 
4 
Proposition (Valid DC Program [Gutmann et al., 2011c]) . 
121 
5 
Proposition (Conditions for exact inference [Michels et al., 
2015]) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134 
6 
Proposition (Computation ofprobability bounds) . . . . . . 136 
7 
Proposition (Conditional probability bounds formulas [Michels 
et al., 2015]) . . . . . . . . . . . . . . . . . . . . . . . . . . 139 
13 
Theorem (Conditions for exact inference of probability bounds 
[Michels et al., 2015]). . . . . . . . . . . . . . . . . . . . . 140 
14 
Theorem (Theorem 6.3.1 from Chow and Teicher [2012]) . . 149 
8 
Proposition (Monotonicity of Op  TrueP'j_  and OpFalseP'j_  ­
PCLP) . . . . . . . . . . . . . . . . . . . . . . 
157 
9 
Proposition (Monotonicity of IFPPP- PCLP) . 
158 
11 
Lemma (Soundness of OpTrueP'j_- PCLP) 
158 
12 
Lemma (Soundness of OpFalseP'j_- PCLP) 
159 
13 
Lemma (Soundness of IFPP'P- PCLP) . . 
160 
14 
Lemma (Completeness of IFPPP  - PCLP) 
162 
15 
Theorem (Soundness and completeness of IFPP'P- PCLP) . 163 
16 
Theorem (Well-definedness of the distribution semantics ­
PCLP) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164 
15 
Lemma (Correctness of the ProbLog Program Transforma­
tion [Fierens et al., 2015]) . . . . . . . . . . . . . . . . . . . 206 
17 
Theorem (Model and weight equivalence [Fierens et al., 2015])206 
18 
Theorem (Correctness of Algorithm 17 [Kimmig et al., 2017]) 303 
16 
Lemma (The probability density semiring is commutative [Zuid­
berg Dos Martires et al., 2018, 2019]) . . . . . . . . . . . . 305 
17 
Lemma (The probability density semiring has neutral addi­
tion and labeling function) . . . . . . . . . . . . . . . . . . 305 
19 
Theorem (AMC to perform WMI [Zuidberg Dos Martires 
et al., 2018, 2019]) . . . . . . . . . . . . . . . . . . . . . . 305 
20 
Theorem (Monte Carlo approximation ofWMI [Zuidberg Dos 
Martires et al., 2019]) . . . . . . . . . . . . . . . . . . . . . 307 
21 
Theorem (Parameters as relative frequency) . . . . . . . . . 327 
22 
Theorem (Maximum of the L1 regularized objective function) 408 
23 
Theorem (Maximum of the L2 regularized objective function) 408 

List  of  Acronyms  
ADD  Algebraic decision diagram 
AMC  Algebraic model counting 
ASP  Answer set programming 
BDD  Binary decision diagram 
BN  Bayesian network 
CBDD  Complete binary decision diagram 
CLP  Constraint logic programming 
CNF  Conjunctive normal form 
CPT  Conditional probability table 
DC  Distributional clauses 
DCG  Definite clause grammar 
d-DNNF  Deterministic decomposable negation normal form 
DLT  Distributionallogic tree 
DP  Dirichlet process 
DS  Distribution semantics 
EM  Expectation maximization 
ESS  Expected sufficient statistics 
FED  Factored explanation diagram 
xxxix 

xl 
List  of Acronyms  
FG  Factor graph 
GP  Gaussian process 
HMM  Hidden markov model 
HPT  Hybrid probability tree 
ICL  Independent choice logic 
IHPMC  Iterative hybrid probabilistic model counting 
liD  independent and identically distributed 
ILP  Inductive logic programming 
JMP  Jointmodel program 
LDA  Latent dirichlet allocation 
LL  Log likelihood 
LPAD  Logic program with annotated disjunctions 
MCMC  Markov chain monte carlo 
MDD  Multivalued decision diagram 
MLN  Markov logic network 
MN  Markov network 
NAD  Neural annotated disjunction 
NLP  N aturallanguage processing 
NN Neural network 
NNF  Negation normal form 
PASP  Probabilistic answer set programming 

List  of Acronyms  xli 
PCFG  Probabilistic context-free grammar 
PCLP  Probabilistic constraint logic programming 
PHA  Probabilistic hom abduction 
PHPT  Partially evaluated hybrid probability tree 
PILP  Probabilistic inductive logic programming 
PLCG  Probabilistic left comer grammar 
PLP  Probabilistic logic programming 
POS  Part-of-speech 
PPDF  Product probability density function 
PPR  Personalized page rank 
PRV  Parameterized random variable 
QSAR  Quantitative structure-activity relationship 
SAT  Satisfiability 
SDD  Sentential decision diagram 
SLP  Stochastic logic program 
SMT  Satisfiability modulo theory 
WFF  Well-formed formula 
WFM  Well-founded model 
WFOMC  Weighted first order model counting 
WFS  Well-founded semantics 
WMC  Weighted model counting 
WMI  Weighted model integration 


1   
Prel im i naries  
This chapter provides basic notions of logic programing and graphical models 
that are needed for the book. After the introduction of a few mathematical 
concepts, the chapter presents logic programming and the various semantics 
for negation. Then it provides abrief recall of probability theory and graphical 
models. 
Foramore in-depth treatment of logic programming see [Lloyd, 1987; 
Sterling 
and 
Shapiro, 
1994] 
and 
of 
graphical 
models 
see 
[Koller and Friedman, 2009]. 
1.1  Orders,  Lattices,  Ordinals  
A partial  order  ~ is a reflexive, antisymmetric, and transitive relation. A par­
tially  ordered  set  S  is a set with a partial order ~. For example, the natural 
numbers N (non-negative integers), the positive integers N1,  and the real num­
bers IR with the standard less-than-or-equal relation ~ are partially ordered 
sets and so is the powerset JPl(S) (the set of all subsets) of a set S  with the 
inclusion relation between subsets ~. a  ES  is an upper  bound  of a subset X  
of S  if x  ~ a  for all x  E  X  and b  E  S  is a lower  bound  of X  if b  ~ x  for 
all x  E  X.  If it also holds that a  E  X  and b  E  X,  then a  is called the Zargest  
element  of X  and b  the smallest  element  of X.  
An element a  E  S  is the least  upper  bound  of a subset X  of S  if a  is an 
upper bound of X  and, for all upper bounds a'  of X,  a  ~ a'.  An element 
b  E  S  is the greatest  lower  bound  of a subset X  of S  if b  is a lower bound of 
X  and, for alllower bounds b'  of X,  b'  ~ b.  The least upper bound of X  may 
not exist. If it does, it is unique and we denote it with lub(X). Similarly, the 
greatest lower bound of X  may not exist. If it does, it is unique and is denoted 
by glb(X). For example, for JPl(S) and X  ~ JPl(S), lub(X) = UxEX x,  and 
glb(X) = nxEX X.  
1 

2 Preliminaries  
A partially ordered set L  is a complete  lattice  if lub(X) and glb(X) exist 
for every subset X  of L.  We denote with T the top  element  lub(L) and with 
_l the bottarn  element  glb(L) of the complete lattice L.  For example, the 
powerset is a complete lattice. 
A relation <,  defined by a  <  b iff a  ~ band a  =1- b,  is associated with any 
partial order ~ on S.  
A partial order ~ on a set S  is a total  order  if it is total,  i.e., for any 
a,  b  E  S,  either a  ~ b  or b  ~ a.  A set S  with a total order is called a totally  
ordered  set.  A set S  is well-ordered  if it has a total order ~ suchthat every 
subset of S  has a smallest element. The set N is well-ordered by its usual 
order, the realline IR is not. 
A function f  :  A  -
B  is one-to-one  if f- 1 ( {  b}) is a set containing a 
single element. f  is onto  B  if j(A)  =  B.  A set Ais equipotent  with a set B  
iffthere is a one-to-one function f  from A  onto B.  Equipotency captures the 
intuitive notion of having the same number of elements. 
A set S  is denumerable  iff S  is equipotent with N. A set S  is countable  
iff it is finite or denumerable, S  is uncountable  otherwise. 
Ordinal numbers are a generalization of natural numbers. The set n  of 
ordinal  numbers  is well-ordered. We call its elements ordinals  and we denote 
them by Greek smallcase letters. Since n  is well-ordered, it has a smallest 
element, that we denote with 0. If a  <  ß,  we say that a  is a predecessor  of 
ß  and ß  is a successor  of a.  a  is the immediate  predecessor  of ß  and ß  is 
the immediate  successor  of a  if a  is the largest ordinal smaller than ß.  Every 
ordinal a  has an immediate successor, denoted with a  + 1. Some ordinals have 
predecessors but no immediate predecessor, which are called limit  ordinals.  
The others are called successor  ordinals.  We denote the immediate successor 
of the least element 0 with 1, the immediate successor of 1 with 2, and so 
Oll. So the first elements 0, 1, 2, ... of n  are the naturals. Since n  is well­
ordered, there is a smallest ordinallarger than 0, 1, 2, ... that we denote with 
w.  It  is the firstinfinite  ordinal  and is countable. We can form its successors 
as w  + 1, w  + 2, ..., effectively adding a "copy" of N to the tail of w.  The 
smallest ordinallarger than w  + 1, w  + 2, ... is called 2w and we can continue 
in this way building 3w,  4w,  and so on. 
The smallest ordinal larger than these is w2 . We can repeat the process 
countably many times obtaining w3 ,  w4 ,  and so on. 
A canonical representation of the ordinals (the so-called von  Neumann  
ordinals)  sees each ordinal as the set of its predecessors, so 0 =  0.  1 =  {0}.  
2 =  {0,  {0} },  3 =  {0,  {0},  {0,  {0} },  ...  In this case, the order is set 
membership. 

1.2  Mappings  and  Fixpoints  3 
The sequence of ordinals is also called transfinite.  Mathematical induc­
tion is extended to the ordinals with the principle  of  transfinite  induction.  
Suppose P  ( a)  is a property defined for all ordinals a  E  n.  To prove that P  is 
true for all ordinals by transfinite induction, we need to assume the fact that 
P(ß)  is true for all ß  <  a  and prove that P(a)  is true. Proofs by transfinite 
induction usually consider two cases: that a  is a successor ordinal, or a limit 
ordinal. 
See [Srivastava, 2013] for a full formal definition of ordinal numbers and 
[Willard, 1970; Hitzier and Seda, 2016] for accessible introductions. 
1.2  Mappings  and  Fixpoints  
A function T  :  L  ~ L  from a lattice L  to itself is a mapping.  A mapping T  is 
monotonic  ifT(x) ~ T(y),  for all x  and y  suchthat x  ~ y.  a  E  L  is afixpoint  
ofT ifT(a) = a.  a  E  L  is the leastfixpoint  ofT if a  is a fixpoint and, for all 
fixpoints b  ofT, it holds that a  ~ b.  Similarly, we define the greatest fixpoint.  
Consider a complete lattice L  and a monotonic mapping T  :  L  ~ L.  We 
define the increasing  ordinal  powers  ofT as: 
• Ti 0 =  .l; 
• T  ja= T(T  j (a- 1)), if a  is a successor ordinal; 
• T  i  a  = lub({T  i  ßlß  <  a} ), if a  is a limit ordinal; 
and the decreasing  ordinal  powers  ofT as: 
•  T  ~ 0 =  T; 
• T  ~ a  = T(T  ~ (a- 1)), if a  is a successor ordinal; 
• T  ~ a  = glb( {T  ~ ßlß  <  a} ), if a  is a limit ordinal. 
The Knaster-Tarski theorem [Knaster and Tarski, 1928; Tarski, 1955] states 
that if L  is complete lattice and T  a monotonic mapping, then the set of 
fixpoints ofT in L  is also a lattice. An important consequence of the theorem 
is the following proposition. 
Proposition  1  (Monotonie Mappings Have a Least and Greatest Fixpoint). 
Let  L  be  a  complete  lattice  and  T  :  L  ~ L  be  monotonic.  Then  T  has  a  least  
fixpoint,  lfp(T) and  a  greatestfixpoint  gfp(T). 
A sequence {xala E  !1} is increasing  if Xß  ~ Xa  for all ß  ~ a  and is 
decreasing  if Xa  ~ Xß  for all ß  ~ a.  
The increasing and decreasing ordinal powers of a monotonic mapping 
form an increasing and a decreasing sequence, respectively. Let us prove it 

4 
Preliminaries  
for T  j  a  by transfinite induction. If a  is a successor ordinal, T  j  ( a  - 2) 
~ T  j  ( a  - 1)  for the inductive hypothesis. By the monotonicity of T,  
T(T  i (a- 2))  ~ T(T  i (a  - 1)),  so T  i (a- 1)  ~ T  i a.  Since 
T  i ß  ~ T  i ( a  - 1)  for all ß  ~ a  - 1,  for the transitivity of ~. the thesis is 
proved. If a  is a limit ordinal, Ti a  = lub({Ti ßlß  <  a} ), so the thesis is 
proved. It  can be proved forT t  a  similarly. Note that in general the fact that 
T  is a monotonic mapping does not imply that x  ~ T(x)  for all x.  
1.3  Logic  Programming  
This section provides some basic notions of first-order logic languages and 
logic programming. 
Afirst-order  logic  language  is defined by an alphabetthat consists of the 
following sets of symbols: variables, constants, functions symbols, predicate 
symbols, logical connectives, quantifiers, and punctuation symbols. The last 
three are the same for alllogic languages. The connectives are --. (negation), 
A  (conjunction), v  (disjunction), +--- (implication), and ~ (equivalence); 
the quantifiers are the existential quantifier ::J and the universal quantifier V, 
and the punctuation symbols are "(", ")", and ",". 
Well-formed formulas (WFFs) of the language are the syntactically cor­
rect clauses of the language and are inductively defined by combining ele­
mentary formulas, called atomic  formulas,  by means of logical connectives 
and quantifiers. On their turn, atomic formulas are obtained by applying the 
predicates symbols to elementary terms. 
A term  is defined recursively as follows: a variable is a term, a constant 
is a term, if f  is a function symbol with arity  n  and t1, ... , tn  are terms, then 
j(t1, ... ,tn)  is a term. An atomic  formula  or atom  a  is the application of a 
predicate symbol p  with arity n  to n  terms: p(t1, ... ,tn)·  
The following notation for the symbols will be adopted: predicates, func­
tions, and constants start with a lowercase letter, while variables start with 
an uppercase letter (as in the Prolog programming language, see below). 
So x,  y,  ...  are constants and X,  Y,  ...  are variables. Bold typeface is used 
throughout the book for vectors, so X,  Y,  . . .  are vectors of logical 
variables. 
An example of a term is mary,  a constant, or father(mary),  a complex 
term, where father  is a function symbol with arity 1. An example of an atom 
is parent(father( mary),  mary)  where parent is a predicate with arity 2. To 
take into account the arity, function symbols and predicates are usually indi­

1.3  Logic  Programming  5 
cated as father /1 and parent/2.  In this case, the symbols father  and parent  
are called functors. Atomsare indicated with lowercase letters a,  b,  ...  
A WFF is defined recursively as follows: 
• every atom a  is a WFF; 
• if A  and Bare WFFs, then also ---.A,  A  1\  B,  A  v B,  A  ~ B,  A  ~ B  
are WFFs (possibly enclosed in balanced brackets); 
• if Ais  WFF and Xis a variable, VX A  and :JX Aare WFF. 
A variant  cp'  of a formula cp  is obtained by renaming all the variables in c/J. 
The class of formulas called clauses has important properties. A clause  is 
a formula of the form 
vxlvx2  ... VXs(al  V  ...  V  an  V  ---.bl  V  ...  V  ---.bm)  
where each ai,  bi  are atoms and X1, X2, ... , Xs  are all the variables aceur­
ring in (a1 v ... v an  v ---.bl  v ... v ---.bm)·  The clause above can also be 
represented as follows: 
a1, ... ;an~bl, ...  ,bm  
where commas stand for conjunctions and semicolons for disjunctions. The 
part preceding the symbol ~ is called the head  of the clause, while the part 
following it is called the body.  Anatom or the negation of an atom is called 
a literal.  A positive  literal  is an atom, and a negative  literal  is the negation of 
an atom. Sometimes, clauses will be represented by means of a set of literals: 
{ a1, ... , an,  ---.bl,  ...  ,  ---.bm} 
An example of a clause is 
male(X);  female(X)  ~ human(X).  
A clause is a denial  if it has no positive literal, definite  if it has one positive 
literal, and disjunctive  if it has more than one positive literal. AHorn  clause  is 
either adefinite clause or a denial. Afact isadefinite clause without negative 
literals, the ~ symbol is omitted for facts. A clause C  is range  restricted  
if and only if the variables appearing in the head are a subset of those in the 
body. A definite  logic  program  P  is a finite set of definite clauses. 
Examples of a definite clause, a denial, and a fact are, respectively: 
human(X)  ~ female(X)  
~ male(X),Jemale(X)  
female(mary)  

6 
Preliminaries  
In this book, I will also present clauses in monospaced font especially when 
they form programs that can be directly input into a logic programming sys­
tem (concrete  syntax).  In that case, the implication symbol is represented by 
:- as in 
human (X) : -
female (X). 
In logic programming, another type of negation is also taken into account, 
namely, the so-called defauZt  negation  "'· The formula "'a  where a  is an atom 
is called a defauZt  negative  literal,  sometimes abbreviated as simply negative  
literal  if the meaning is clear from the context. A defauZt  literal  is either an 
atom (a positive  literal)  or adefault negative lüeral. Again the word default 
can be dropped if the meaning is clear from the context. 
A normal  clause  is a clause of the form 
a  +- b1,. · ·, bm  
where each bi  is a default Iitera!. A normal  logic  program  is a finite set of 
normal clauses. Default negation in the concrete syntax is represented ei­
ther with \ +  (Prolog, see page 9) or not (Answer Set Programming, see 
Section 1.4.3). In this book, we will use the Prolog convention and use \ +.  
A normal clause is range  restricted  if and only if the variables appearing 
in the head are a subset of those appearing in positive literals in the body. A 
normal logic program is range  restricted  if and only if all of its clauses are 
range restricted. 
A Substitution  e  =  {XI/tl, .. . ' Xk /tk}  is a function mapping variables 
to terms. e = {X/father(mary),  Y  jmary}  is an example of a substitution. 
The application  qyB of a substitution e to a formula qy  means replacing all the 
occurrences of each variable Xj  in qy  by the same term tj .  So parent(X, Y)B  
is, for example, parent(Jather( mary) , mary).  
The composition BO",  of two Substitutions e  = {XI/tl, . . . ' Xk /tk}  and 
O"  = {YI/ s1 , ... , Ym/ sm} is defined as the substitution obtained by removing 
from the set 
{XI/tw, ... ,Xk/ tkO", YI/sl , ...  , Ym/ sm}  
the pairs Xi/tw  such that Xi  is equal to tw  and the pairs Yi/ Si  such that 
Yi  E  {X1, ... ,Xk}.  Composition is associative and qy (BO")  = (qyB)O".  
Given two Substitutions, e and 0", we we say that e is at  least  as  generat  
as  0"  (and we write B  ~ 0") if there exists a Substitution / suchthat Br  =  0". 
The relation ~ is a preorder (a binary relation that is reflexive and transitive). 

1.3  Logic  Programming  7  
Given two formulas, cjJ  and cp',  a substitution ()  such that cp()  = cp'  ()  is 
called a unijier.  The mostgenerat  unijier  of cjJ  and cp'  is a unifier ()  that is most 
general, i.e., such that, for every other unifier a-,  ()  ~ a-.  
A ground  clause (term) is a clause (term) without variables. A substitution 
()  is grounding  for a formula cjJ  if cp()  is ground. 
The Herbrand  universe  U  of a language or a program is the set of all 
the ground terms that can be obtained by combining the symbols in the lan­
guage or program. The Herbrand  base  B  of a language or a program is the 
set of all possible ground atoms built with the symbols in the language or 
program. Sometimes they will be indicated with Up  and ßp  where Pis the 
program. The grounding  ground(P)  of a  program  Pis obtained by replacing 
the variables of clauses in P  with terms from Up  in all possible ways. 
If P  does not contain function symbols, then Up  is equal to the set of 
constants and is finite; otherwise, it is infinite (e.g., if P  contains constant 
0 and function symbol s/1, then Up  =  {0, s(O), s(s(O)), ...}). Therefore, 
if P  does not contain function symbols, ground(P)  is finite and is infinite 
if P  contains function symbols and at least one variable. The language of 
programs without function symbols is called Datalog.  
The semantics of a set of formulas can be defined in terms of inter­
pretations and models. We will here consider the special case of Herbrand  
interpretations  and Herbrandmodels  that are sufficient for giving a semantics 
to sets of clauses. Foradefinition of interpretations and models in the general 
case, see [Lloyd, 1987]. A Herbrandinterpretation  or two-valued  interpre­
tation  I  is a subset of the Herbrand base, i.e., I  <:; B.  Given a Herbrand 
interpretation, it is possible to assign a truth value to formulas according to 
the following rules. A ground atom p(t1,  t2, ... , tn)  is true under the interpre­
tation I  if and only if p( t 1, t 2, ...  ,  tn)  E  I.  A conjunction of atomic formulas 
b1, ... , bm  is true in I  if and only if {b1, ... , bm} <:; I.  A ground clause 
a1 ; ... ; an  +--- b1, ... , bm  is true in an interpretation I  if and only if at least 
one of the atoms of the head is true in the case in which the body is true. A 
clause C  is true in an interpretation I  if and only if all of its ground instances 
with terms from U  are true in I.  A set of clauses ~ is true in an interpretation 
I  if and only if all the clauses C  E  ~ are true. 
A two-valued interpretation I  represents the set oftrue ground atoms and 
we say that a  is false if a  tf; I.  The set Int2  of two-valued interpretations for a 
program P  forms a complete lattice where the partial order ~ is given by the 
subset relation <:;. The least upper bound and greatest lower bound are thus 
lub(X) =  UrEX I  and glb(X) =  niEX I.  The bottom and top elementare, 
respectively, 0  and ßp.  

8 Preliminaries  
An interpretation I  satisfies  a set of clauses ~. notation I  I=  ~. if ~ is 
true in I;  we also say that I  is a model  of ~. A set of clauses is satisfiable  if it 
is satisfied by some interpretation, unsatisfiable  otherwise. If all models of a 
set of clauses ~ arealso models of a clause C,  we say that ~ logically  entails  
C  or Cis a logical  consequence  of ~. and we write ~ I=  C.  We use the same 
symbol for the entailment relation and for the satisfaction relation between 
interpretations and formulas in order to follow the standard logic practice. In 
cases where this may cause misunderstanding, the intended meaning will be 
indicated in words. 
Herbrand interpretations and models are sufficient for giving a semantics 
to sets of clauses in the following sense: a set of clauses is unsatisfiable if 
and only if it does not have a Herbrand model, a consequence of Herbrand's 
theorem [Herbrand, 1930]. For sets of definite clauses, Herbrand models are 
particularly important because they have the relevant property that the inter­
section of a set of Herbrand models for a set of definite clauses P  is still a 
Herbrand model of P.  The intersection of all the Herbrand models of P  is 
called the minimal  Herbrandmodel  of P  and is represented with lhm(P). 
The least Herbrand model of P  always exists and is unique. The model­
theoretic  semantics  of a program P  is the set of all ground atoms that are 
logical consequences of P.  The least Herbrand model provides the model 
theoretic semantics for P:  P  I=  a  if and only if a  E  lhm(P) where a  is a 
ground atom. 
For example, the program P  below 
human(X)  ~ female(X)  
female(mary)  
has the lhm(P) =  {female(mary),  human(mary)}.  
A proof procedure  is an algorithm for checking whether a formula is prov­
able from a theory. If formula cp  is provable  from the set of formulas ~. we 
write ~ I- cp.  Two important properties of proof procedures are soundness  and 
completeness.  A proof procedure is so und,  with respect to the model-theoretic 
semantics, if ~ I- cp  implies ~ I=  cp;  it is complete  if ~ I=  cp  implies ~ I- cp.  
A proof procedure for clausallogic that is particularly suitable tobe auto­
mated on a computer is resolution  [Robinson, 1965]. The resolution inference 
rule allows one to prove, from two clauses F1 v  h  and F2 v  -.[2 where 
F1 and F2 are disjunctions of literals, the clause (F1 v F2)(), where ()  is 
the most general unifier of h  and l2. For definite clauses, this consists in 
matehing the head of one clause with a literal in the body of another. To 
prove a conjunction of literals cp  (a query  or goal)  from a set of clauses ~. 

1.3  Logic  Programming  9 
cjJ  is negated obtaining +--- cjJ  and added to ~: if the empty denial +--- can be 
obtained from ~ u  {  +--- cjJ}  using a sequence of applications of the resolution 
inference rule, then cjJ  is provable from ~. ~ I- cj;.  For example, if we want to 
prove that P  I- human( mary ), we need to prove that +--- can be derived from 
Pu {+--- human(mary) }. In this case, from human( X)  v -.Jemale(X)  and 
female(mary),  we can derive human(mary)  by resolution. 
Logic  programming  was originally proposed by considering Horn clauses 
and adopting a particular version of resolution, SLD  resolution,[Kowalski,  
1974]. SLD resolution builds a sequence of formulas c/J1, c/J2, ... ,c/Jn.  where 
c/J1 =+--- cp,  that is called a derivation.  At each step, the new formula c/Ji+l is 
obtained by resolving the previous formula c/Ji  with a variant of a clause from 
the program P.  If no resolution can be performed, then c/Ji+l =  fail and the 
derivation cannot be further extended. If c/Jn  =+---,  the derivation is successful;  
if c/Jn  =  fail, the derivation is unsuccessful.  The query cjJ  is proved (succeeds)  
if there exists a successful derivation for it and fails  otherwise. SLD resolution 
was proven tobe sound and complete (under certain conditions) for Horn 
clauses (the proofs can be found in [Lloyd, 1987]). 
A particular logic programming language is defined by choosing a rule 
for the selection of the literal in the current formula to be reduced at each step 
(selection  rule)  and by choosing a search strategy that can be either depth 
first or breadth first. In the Prolog  [Colmerauer et al., 1973] programming 
language, the selection rule selects the left-most  literal in the current goal 
and the search strategy is depth first with chronological backtracking. Prolog 
builds an SLD  tree  to answer queries: nodes are formulas and each path from 
the root to a leaf is an SLD derivation; brauehing occurs when there is more 
than one clause that can be resolved with the goal of a node. Prolog adopts 
an extension of SLD resolution called SLDNF  resolution  that is able to deal 
with normal clauses by negation  asfailure  [Clark, 1978]: a negative selected 
literal ~a is removed from the current goal if a proof for a  fails. An SLDNF  
tree  is an SLD tree where the literal ~ a  in a node is handled by building a 
nested tree for a: if the nested tree has no successful derivations, the literal 
~ a  is removed from the node and derivation proceeds; otherwise; the node 
has fail as the only child. 
If the goal cjJ  contains variables, a solution  to cjJ  is a substitution ()  obtained 
by composing the substitutions along a branch of the SLDNF tree that is 
successful. The success  set  of cjJ  is the set of solutions of cj;.  If a normal 
program is range restricted, every successful SLDNF derivation for goal cjJ  
completely grounds cjJ  [Muggleton, 2000a]. 

10  Preliminaries  
Example  1  (Path - Prolog). The  following  program  computes  paths  in  a  
graph:  
path(X,X).  
path(X,  Y)  ~ edge(X,  Z),path(Z,  Y).  
edge(a,  b).  
edge(b,  c). 
edge(a,  c). 
path(X,  Y)  is  true  ifthere  is  a  pathfrom  X  to  Y  in  the  graph  where  the  edges  
are  represented by facts  for the predicate edge/2.  This program computes the  
transitive  closure  of the  relation  edge.  This  is  possible  because  the  program  
contains  an  inductive definition, that  of path/2.  
Computing  transitive  closures  is  an  example  of a  problern  for  which  first  
order  logic  is  not  sufficient  and  a  Turing-complete  language  is  required.  
Inductive  definitions  provide  Prolog  this  expressive  power.  
The first clause states that there  is a path from a node to itself.  The second  
states  that  there  is  a  path from  a  node  X  to  a  node  Y  if there  exists  a  node  
Z  such  that  there  is  an  edge  from  X  to  Z  and  there  is  a  path  from  Z  to  Y.  
Variables  appearing  in  the  body  only,  such  as  Z  above,  become  existentially  
quantified  when  the  universal  quantifier  for  them  is  moved  to  have  the  body  
as  scope.  
Figure  1.1  shows  the  SLD  tree  for  the  query  path( a,  c). The  labels  of the  
edges  indicate  the  most  generat  unifiers  used.  The  query  has  two  successful  
derivations,  corresponding  to  the  paths  from  the  root  to  the  ~ leaves.  
Suppose  we  add  the  following  clauses  to  the  program  
ends(X,  Y)  ~ path(X,  Y),  ~source(Y). 
source(X)  ~ edge(X,  Y).  
ends(X,  Y)  is  true  ifthere  is  a  pathfrom  X  to  Y  and  Y  isaterminal  node,  
i.e.,  it  has  no  outgoing  edges.  
The  SLDNF  treefor  the  query  ends(b,  c) is  shown  in  Figure  1.2:  to  prove  
~source(c), an  SLDNF  tree  is  builtfor  source(c),  shown  in  the  rectangle.  
Since  the  derivation  of  source(c)  fails,  then  ~source(c) succeeds  and  can  
be  removed from  the  goal.  
A ground atom is relevant  with respect to a query cp  if it occurs in some 
proof of an atom from atoms(cp),  where atoms(cp)  returns the set of atoms 
appearing in the conjunction of literals c/J.  A ground rule is relevant if it 
contains only relevant atoms. In Example 1, path(b,  c), and path( c, c) are rel­
evant for path(a,  c), while path(a,  b),  path(b,  a),  path(c,  a),  andpath(c,  b)  

1.3  Logic  Programming  11  
aren't. For the query ends(b ,  c), instead path(b,  c), path( c, c), and sour ce( c) 
are relevant, while source( b)  is not. 
Proof procedures provide a method for answering queries in a top-down 
way. We can also proceed bottom-up by computing all possible consequences 
of the program using the Tp  or immediate  consequence  operator.  
Definition  1  (Tp  operator). Fora  definite  program  P,  we  define  the  operator  
Tp  : Int2  -
Int2  as  
Tp (I)  = {al  there  is  a  clause  b -
h, ... , Zn  in  P,  a  grounding  Substitution  e  
suchthat  a  = bB  and,  for  every  1  :(  i  :(  n ,  zie E  I}.  
The Tp  operator is such that its least fixpoint is equal to the least Herbrand 
model of the program: lfp(Tp)  =  lhm(P). 
It can be shown that the least fixpoint of the Tp  operator is reached at 
most at the first infinite ordinal w  [Hitzler and Seda, 20 16]. 
In logic programming, data and code take the same form, so it is easy 
to write programs that manipulate code. For example, meta-interpreters  or 
programs for interpreting other programs are particularly simple to write. A 
meta-interpreter for pure Prologis [SterJjng and Shapiro, 1994]: 
solve  (true ).  
solve (( A , B ) )  :­
<- path(a,  c) 
I  
._ edge(a,  Zo),  
path(Zo ,  c) 
Zo/V  
"'--.Zo/c  
path(b,  c) 
<- path(c,  c) 
<- edg~(b, Zr),  
I<- ~ge(c, Z3),  
path(Zr , c)  
._ 
path (Z3,c)  
Zr/cl 
<- path( c,  c)  
fail  
I<- ~ge(c , Z2), 
<-
path(Z2,  c) 
I  
fail  
Figure  1.1  SLD tree for the query path( a ,  c) from the program of 
Example 1.  

12 
Preliminaries  
<--- ends(b, c) 
I  
<--- path(b, c), 
~source(c) 
I  
<--- edge(b,  Zo),  
path(Zo,  c), 
~source(c) 
Zo/cl 
<--- path(c,  c) , 
~source(c) 
/ 
::. edge(c,  Z1) , 
<-~source(c) 
path(Z1 , c), 
~source(c) 
I  
<--- source(c)  
fail  
I  
<--- edge(c,  Yo)  
I  
fail  
I  
Figure  1.2  SLDNF tree for the query ends(b,  c) from the program of 
Example 1. 
solve (A),  
solve (B) .  
solve (Goal ) 
clause (Goal , Body), 
solve (Body). 
where clause (Go a l , Body ) is a predicate that succeeds if Goal : ­
Body is a clause of the program. 
Pure Prolog is extended to include primitives for controlling the search 
process. An example is cut  which is a predicate !  I  0 that always succeeds 
and cuts choice points. If we have the program 
p (Sl ) 
: -
Al . 
p ( Sk ) :-
B , 
!, C . 

1.4  Semanticsfor Normal Logic Programs  13  
p ( Sn )  
: -
An .  
when the k-th  clause is evaluated, if B succeeds, then !  succeeds and the 
evaluation proceeds with C. In the case of backtracking, all the alternative for 
B are eliminated, as well as all the alternatives for p ( .. . ) provided by the 
clauses from the k-th  to the n-th.  
Cut is used in the implementation of an if-then-else construct in Prolog 
that is represented as 
(B->Cl ; C2 ) . 
The construct evaluates B; it if succeeds, it evaluates C 1; if it fails, it evalu­
ates C2 . 
The construct is implemented as 
if_then_else (B, Cl , 
C2 ) 
: -
B, 
! ,  Cl. 
if_then_else (B, Cl , 
C2 ) :-
C2 . 
Prologsystems usually include many built-in predicates. The predicate i  s I  2 
(usually written in infix notation) evaluates the second argument as an arith­
metic expression and unifies the first argument with the result. So for exam­
ple, A is 3+2 is a goal that, when executed, unifies A with 5. 
1.4  Semanlies  for  Normal  Logic  Programs  
In a normal program, the clauses can contain default negative literals in the 
body, so a general rule has the form 
C  = h  +- b1 , ... , bn ,  ~cl , . . · , ~ cm 
(1.1) 
where h ,  b1 , ... , bn ,  c1, ... , Cm are atoms. 
Many semantics have been proposed for normallogic programs; see [Apt 
and Bol, 1994] for a survey. Of them, the one based on Clark's completion, 
the Well-founded Semantics (WFS), and the stable model semantics are the 
most widely used. 
1.4.1  Program  completion  
Program  completion  or Clark's  completion  [Clark, 1978] assigns a semantics 
to a normal program by building a first-order logical theory representing the 
meaning of the program. The idea of the approach is to formally model the 

14 
Preliminaries  
fact that atoms not inferable from the rules in  the program are to be regarded 
as false. 
The completion  of a  clause  
p(t1, ... ,tn)  ~ b1, ... , bm,  "'CI, ... , "'Cz.  
with variables Y1, ... , Yd  is the clause 
p(X1, ... , Xn)  ~  :JY1, ... , :JYd((X1 = t1) 1\  ...  1\  (Xn  = tn)  1\  
b1 1\  •.. 1\  bm  1\  ---.q 1\  ..• 1\  ---.cz)  
where = is a new predicate symbol representing equality. If  the program 
contains k  :;;:: 1 clauses for predicate p,  we obtain k  formulas of the form 
p(X1,  ...  ,Xn)  ~ ~1 
p(X1, ... , Xn)  ~ ~k· 
The competed  definition  of p  is then 
vx1,  ... VXn(p(X1,  ...  ,Xn)  ~ ~1 V  ...  V  ~k)· 
If a predicate appearing in  the program does not appear in  the head of any 
clause, we add 
VX1, ... VXn---.p(X1,  ...  , Xn)·  
The predicate = is constrained by the following formulas that form an equal­
ity  theory:  
1.  c #- d  for all pairs c, d  of distinct constants, 
2.  V(f(X1, ... , Xn)  #- g(Y1,  ...  , Ym)) for all pairs j,  g  ofdistinct function 
symbols, 
3.  V(t[X] #-X) for each term t[X] containing X  and different from X,  
4.  V((X1 #- Y1) V •.. V (Xn  #- Yn)- j(X1, ... , Xn)  #- j(Y1, ... , Yn))  
for each function symbol f,  
5.  V(X =X), 
6.  V((X1 = Y1) 1\  ...  1\  (Xn  = Yn)- j(X1, ... , Xn)  = j(Y1,  ... , Yn))  
for each function symbol f,  
7.  V((X1 = Y1) 1\  ..•  1\  (Xn  = Yn)- p(X1, ... , Xn)  -
p(Y1,  ... , Yn))  
for each predicate symbol p  (including =  ).  

1.4  Semanticsfor  Normal  Logic  Programs  15 
Given anormal program P,  the completion comp(P)  of P  is the theory 
formed by the completed definition of each predicate of P  and the equality 
theory. 
Note that the completion of a program may be inconsistent. In  this case, 
the theory has no models and everything is a logical consequence. This is a 
situation to be avoided so restrictions are imposed on the form of the pro gram. 
The idea of the semantics based on Clark's completion is to consider a 
conjunction of ground literals 
b1 , ... , bm,  "-Cl , ... , "-Cl  
true if 
b1 A ... A bm  A -.cl A ... A -.q 
is a logical consequence of comp(P),  i.e., if 
comp(P)  F=  b1 1\  ... 1\  bm  1\  -.q 1\  ... 1\  -.q. 
SLDNF resolutionwas proven sound and complete (under certain conditions) 
with respect to the Clark's completion semantics [Clark, 1978]. 
Example  2 (Clark's completion). 
Consider  the  program  P1  
b  <c-"'a.  
c <c-"'b.  
c <c- a.  
Its  completion  camp( PI)  is  
b~ -.a.  
c ~ -.b  v a.  
-.a  
We  see  that  camp( PI)  F=  -.a,  b,  -.c so  "'a,  band  "'c can  be  derived  with  
SWNF  resolution.  The  SLDNF  tree  for  query  c is  shown  in  Figure  1.3  and,  
as  expected,  returns  false.  
Consider  the  program  P2 
p  <c- p.  
Its  completion  comp(P2)  is  
p~p. 
Here  comp(P)  ltf p  and  comp(P)  ltf -.p.  SLDNF  resolution  for  query  p  
would  loop  forever.  This  shows  that  SLNDF  does  not  handle  well  loops,  in  
this  case  positive loops. 
Consider  the  program  P3 

16 Preliminaries  
+-C  
/~
+-"'b  
+--- a  
I  
+-b  
fail 
I  
+-"'a  
+-a  
I  
fail 
+--­
fail 
Figure  1.3  SLDNF tree for the query c from the program of Example 2. 
p  +-"'P·  
Its  completion  comp(P3)  is  
p~----.p. 
which  is  inconsistent,  so  everything  is  a  consequence.  SLDNF  resolution  for  
query p would loop forever.  SLNDF resolution does not handle well also loops  
through  negation  or  negative loops. 
1.4.2  Well-founded  semanlies  
The WFS [Van Gelder et al., 1991] assigns a three-valued model to a pro gram, 
i.e., it  identifies a consistent three-valued interpretation as the meaning ofthe 
pro gram. 
A three-valued  interpretation  I  is a pair <Ir,  IF)  where Ir  and IF  are 
subsets of ß p  and represent, respectively, the set of true and false atoms. 
So a  is true in I  if a  E Ir  and is false in I  if a  E IF,  and "'a  is true in I  
if a  E  IF  and is false in I  if a  E  fr.  If a  rf  Ir  and a  rf  IF,  then a  assumes 

1.4  Semantics  for  Normal  Logic  Programs  17 
the third truth value, undefined.  We also write I  I=  a  if a  E  Ir  and I  l=~a 
if a  E  Ip.  A consistent  three-valued interpretation I  = (Ir,  Ip)  is suchthat 
Ir  n  Ip  = 0.  The union of two three-valued interpretations (Ir,  Ip)  and 
(Jr,  Jp)is definedas (fr,Jp)u(Jr, Jp)  =(Ir  u Jr,IF  u Jp).  Theinter­
section of two three-valued interpretations (Ir,  Ip)  and (Jr,  Jp)  is defined 
as (fr,Jp) n  (Jr,  Jp)  = (Ir  n  Jr,IF  n  Jp).  Sometimes we represent a 
three-valued interpretation I  = (Ir,  I  F)  as a single set of literals, i.e., 
I=  Ir  u  {~aia E  Ip}.  
The set I  nt3  of three-valued interpretations for a program P  forms a com­
plete lattice where the partial order :S;  is defined as (fr,Jp) :S;  (Jr,  Jp)  if 
Ir~ Jr  and Ip  ~ Jp.  The least upper bound and greatest lower bound are 
defined as lub(X) = U:rEX I  and glb(X) = n:rEX I.  The bottom and top 
element are, respectively, (0,  0)  and ( B p,  B p). 
Given a three-valued interpretation I=  (Ir,  Ip ), we define the functions 
true(I)  =  Ir.  false(I)  =  Ip,  and undef(I)  =  ßp\fr\IF  that retum the 
set of true, false and undefined atoms, respectively. 
The WFS was given in [Van Gelder et al., 1991] in terms of the least 
fixpoint of an operator that is composed by two sub-operators, one computing 
consequences, and the other computing unfounded sets. We give here the 
alternative definition of the WFS of [Przymusinski, 1989] that is based on an 
iterated fixpoint. 
Definition  2 (OpTruei  and OpFalsei  operators). Fora  normal  program  
P,  sets  Tr  and  Fa  of ground  atoms,  and  a  three-valued  interpretation  I,  we  
define  the  operators  OpTruei  : Int2  ~ Int2  and  OpFalsei  : Int2  ~ 
Int2  as  
OpTruei( Tr)  = {ala  is  not  true  in  I;  and  there  is  a  clause  b  ~ h,  ... , ln  in  
P,  a  grounding  substitution  ()  such  that  a  = b()  and for  every  1 :S;  i  :S;  n  
either  li()  is  true  in  I,  or  li()  E  Tr};  
OpFalsei(Fa)  = {ala  is  notfalse  in  I;  andfor  every  clause  b  ~ h,  ...  ,Zn  
in  P  and  grounding  substitution  ()  such  that  a  = b()  there  is  some  i  
(1  :S;  i  :S;  n)  suchthat  li()  is  false  in  I  or  li()  E  Fa}.  
In  words, the operator OpTruei (Tr)  extends the interpretation I  to add 
the new true atoms that can be derived from P  knowing I  and true atoms 
Tr,  while OpFalsei (Fa)  computes new false atoms in P  by knowing I  and 
false atoms Fa.  OpTruei  and OpFalsei  are both monotonic [Przymusinski, 

18 
Preliminaries  
1989], so they both have least and greatest fixpoints. An iterated fixpoint 
operator builds up dynamic  strata  by constructing successive three-valued 
interpretations as follows. 
Definition  3 (Iterated fixpoint). Fora  normal program  P,  let  IFPlf  I  nt3  ~ 
I  nt3  be  defined  as  
IFPP(I)  =I u  <lfp(OpTrue.i'),  gfp( OpFalsei)).  
IFPP  is monotonic [Przymusinski, 1989] and thus has a least fixpoint 
lfp(IFPP).  The Well-founded model (WFM) WFM(P)  of Pis  lfp(IFPP).  
Let 6  be the smallest ordinal suchthat WFM(P)  =  IFPP  j  5.  We refer to 
6  as the depth  of P.  The stratum  of atom a  is the least ordinal ß  suchthat 
a  E  IFPP  j  ß  (where a  may be either in the true or false component of 
IFPP  j  ß).  Undefined atoms of the WFM do not belong to any stratum­
i.e., they arenot added to IFPP  j  6  for any ordinal5. 
If undef(WFM(P))  =  0.  then the WFM is called total  or two-valued  
and the program dynamically  stratified.  
Example  3 (WFS computation). 
Let  us  consider  program  P1  of  
Example  2:  
b  <c-"'a.  
c <c-"'b.  
c <c- a.  
Its  iterated fixpoint  is  
IFPP  j  Ü  
<0,  0);  
IFPP  i  1 
<0,  {a}); 
IFPP  i  2 = <{b}, {a}); 
IFPP  i  3 = <{b}, {a,  c}); 
IFPP  j  4  = IFPP  j  3  = WFM(PI)·  
Thus,  the  depth  of P1  is  3  and  the  WFM  of P1  is  given  by  
true(  WFM(P1))  
{b}  
undef(WFM(PI))  =  0  
false(  WFM(PI))  
=  
{a,  c}. 
So  WFM(PI)  is  two-valued  and  P1  is  dynamically  stratified.  
Let  us  consider program  P4jrom  [Przymusinski,  1989]  
b  <c-"'a.  
c <c-"'b.  
c <c- a,  "'P·  
p  <c-"'q.  
q  <c-"'p,  b.  

1.4  Semanticsfor  Normal  Logic  Programs  19 
Its  iterated fi:xpoint  is  
fppP  j  0 
<0,0);   
fppP  j  1 
<0,  {a});  
fppP  j  2 
<{b}, {a});  
fppP  j  3 
<{b},{a,c});  
fppP  j  4 
fppP  j  3 = WFM(P4).   
So  the depth  of P4  is 3  and the WFM of P4  is given by  
true(  WFM(P4))  
= {b}   
undef(  WFM(P4))  = 
{p,  q}  
false(  WFM(P4))  
= {a,  c}.  
Consider  now  the  program  P2 of Example  2:  
p~p. 
Its  iterated fi:xpoint  is  
fppP  j  0 = <0,  0);   
fppP  j  1 = <0,  {p});  
JppP  j  2 = fppP  j  1 = WFM(P2).   
P2 is  dynamically  stratified  and assigns  value  false  top.  So  positive  loops  are  
resolved  by  assigning  value  false  to  the  atom.  
Let  us  consider  the  program  P3 of Example  2:  
p~~p. 
Its  iterated fi:xpoint  is  
fppP  j  0 = <0,  0);  
JppP  j  1 
=  
fppP  j  0 =  WFM(P3).  
P3 is  not  dynamically  stratified  and  assigns  value  undefined  top.  So  negative  
loops  are  resolved  by  assigning  value  undefined  to  the  atom.  
Let  us  consider  the  program  P5:  
p~~q. 
q~~p. 
Its  iterated fi:xpoint  is  
fppP  j  0 
=  <0,  0);   
JppP  j  1 = fppP  j  0 = WFM(P5).   
Thus,  the depth  of P5 is 0  and the WFM of P5 is given by  
true(  WFM(P5))  
0  
undef(  WFM(P5))  = {p,  q} 
false(  WFM(P5))  
= 0.  
So  WFM(P5)  is  three-valued  and  P5  is  not  dynamically  stratified.  

20 Preliminaries  
Given its similarity with the Tp  operator, it can be shown that OpTrue[l  
reaches its fixpoint at most at the firstinfinite ordinal w.  IFPP  instead doesn't 
satisfy this property, as the next example shows. 
Example  4  (Fixpoint of IFPP  beyond w). Consider  thefollowing  program  
inspiredfrom  [Hitzler  and  Seda,  2016,  Program  2.6.5,  page  58]:  
p(O,  0). 
p(Y,  s(X))  ~ r(Y),p(Y,  X).  
q(Y,  s(X))  ~"'p(Y, X).  
r(O).  
r(s(Y))  ~"'q(Y, s(X)).  
t  ~"'q(Y, s(X)).  
The  ordinal  powers  of IFPP  are  shown  in  Figure  1.4,  where  sn(o)  is  the  term  
where  the  functor  s  is  applied  n  times  to  0.  We  need  to  reach  the  immediate  
successor  of w  to  compute  the  WFS  ofthispro gram.  
The following properties identify important subsets of programs. 
Definition  4  (Acyclic, stratified and locally stratified programs). 
• A  Ievel mappingfor a  program  Pis  afunction  II  :  ßp  ~ Nfrom  ground  
atoms  to  natural  numbers.  Fora  E  ßp,  Iai  is  the  Ievel of a.  If l  = ...,a  
where  a  E  ßp,  we  define  lll  = Iai.  
• A  program  T  is  called  acyclic [Apt  and  Bezem,  1991]  if there  exists  a  
level  mapping  such  as,  for  every  ground  instance  a  ~ B  of a  clause  of  
T,  the  level  of a  is  greater  than  the  level  of each  literal  in  B.  
• A  program  T  is  called  locally stratified if  there  exists  a  level  mapping  
such  as,  for  every  ground  instance  a  ~ B  of a  clause  ofT,  the  level  of  
a  is  greater  than  the  level  of each  negative  literal  in  B  and  greater  or  
equal  than  the  level  of each  positive  literals.  
• A  program  T  is  called  stratified if there  exists  a  level  mapping  according  
to  which  the  program  is  locally  stratified  and  such  that  all  the  ground  
atoms  for  the  same  predicate  can  be  assigned  the  same  level.  
The WFS for locally stratified programs enjoys the following property. 
Theorem  1  (WFS for locally stratified programs [Van Gelder et al., 1991]). 
If P  is  locally  stratified,  then  it  has  a  total  WFM.  
Programs P1 and P2 of Example 3 are (locally) stratified while programs P3, 
P4, and P5  arenot (locally) stratified. Note that stratification is stronger than 
local stratification that is stronger than dynamic stratification. 
SLG  resolution  [Chen and Warren, 1996] is a proof procedure that is 
sound and complete (under certain conditions) for the WFS. SLG uses tabling:  

1.4  Semantics for  Normal  Logic  Programs  21 
[ppP  t 0 
(0,0);  
[ppP  t  1 
<{p(O,sn(O)In  E N}  v {r(O)}, 
{q(sm(o),O)Im E  N});  
[ppP  t 2 
({p(O,sn(O))In  E  N}  v  {r(O)}, 
{q(O,sn(O))In  E  NI} v {q(sm(o),O)Im E  N}); 
[ppP  t  3 
({p(sm(o),sn(o))ln  E  N,m E  {0, 1}}v 
{r(O), r(s(O))},  
{q(O,sn(O))In  E  NI} v {q(sm(O),O)Im E  N}); 
[ppP  t  4  
({p(sm(o),sn(O))In  E  N,m E  {0, 1}}v 
{r(O),r(s(O)),r(s(s(O)))},  
{q(O, sn(O)),  q(s(O),  sn(O))In  E  N1}v  
{q(sm(o),O)Im E  N});  
[ppP  t  2(i +  1)  
({p(sm(o), sn(O)))In  E N, m  E {0, ... , i}}v 
{r(sm(O))Im E {0, ... ,i +  1}}, 
{q(sm(o),sn(O))In  E  N1,m E  {0, ... ,i}}v 
{q(sm(o),O)Im E  N});  
[ppP  t W  
<{p(sm(o),  sn(O)))In,  m  E  N}v  
{r(sm(O))Im E N}, 
{q(sm(o),  sn(O))In,  m  E  N});  
[ppP  t W +  1 
({t} v  {p(sm(O),sn(O)))In,m  E  N}v  
{r(sm(O))Im E N}, 
{q(sm(o),  sn(O))In,  m  E  N});  
Figure  1.4  Ordinal powers of IFPP  for the program of Example 4. 
it keeps a store of the subgoals encountered in a derivation together with 
answers to these subgoals. If one of the subgoals is encountered again, its 
answers are retrieved from the store rather than recomputing them. Besides 
saving time, tabling ensures termination for programs without function sym­
bols. For a discussion of the termination properties of SLG in the general 
case, see [Riguzzi and Swift, 2014]. SLG resolution is implemented in the 
Prolog systems XSB [Swift and Warren, 2012], YAP [Santos Costa et al., 
2012], and SWI-prolog [Wielemaker et al., 2012]. 
1.4.3  Stable  model  semantics  
The stable model semantics [Gelfond and Lifschitz, 1988] associates zero, 
one, or more two-valued modelstoanormal program. 

22 Preliminaries  
A model I  of a program P  is minimal if there is no other model J  such 
that J  ~I. 
Definition  5  (Reduction). Given  anormal program  P  and  a  two-valued  in­
terpretation  I,  the  reduction pi  of P  relative  to  I  is  obtainedfrom  ground(P)  
by  deleting  
1.  each  rule  that  has  a  negative  literal  rova  such  that  a  E  I  
2.  all  negative  literals  in  the  body  of the  remaining  rules.  
Definition  6  (Stable model). A  two-valued  interpretation  I  is  a  stable model 
or  an  answer set of a program P  ifit isaminimal model of PI.  
Since pi  is a program without negation as failure, it has a unique minimal 
modelthat is the least Herbrand modellhm(Pr). 
The stable  model  semantics  of a program Pis the set of its stable model­
s/answer sets and is denoted by AS(P).  
The relationship between the WFS and the stable model semantics is 
given by the following two theorems [Van Gelder et al., 1991]. 
Theorem  2 (WFS total model vs stable models). If P  has a total WFM,  then  
that  model  is  the  unique  stable  model.  
Theorem  3 (WFS vs stable models). The  WFM  of  Pisasubset  of every  
stable model  of P  seen as a three-valued interpretation.  
ASP is a problem-solving paradigm based on the computation of the answer 
sets of a program. 
Example  5  (Answer set computation). 
Let  us  consider program  P1  of Example  2:  
b  <c-rova,  
C  <c-rovb,  
c <c- a.  
Its  only  answer  set  is  {b}  so  AS(P1)){ {b}  }.  
Let  us  consider program  P4jrom  [Przymusinski,  1989]  
b  <c-rova.  
C  <c-rovb.  
c <c- a,  rovp.  
p  <c-rovq,  
q  <c-rovp,  b.  
The  program  has  two  answer  sets:  AS(P4)  =  {  {b,  p}, {b,  q} }. WFM(P4)  =  
({b}, {a,  c}) is  a  subset  of  both  seen  as  the  three-valued  interpretations  
({b,p}, {a,  c, q}) and  ({b,  q}, {a,  c,p}) 

1.5  Probability  Theory  23 
Let  us  consider  program  P2 of Example  2:  
p~p. 
Its  only  answer  set  is  0.  
Program  P3 of Example  2  
p  ~"'P· 
has  no  answer  sets.  
Program  P5:  
p  ~"'q. 
q  ~"'P· 
has  the  answer  sets  {p} and  {q} and  WFM(P5)  =  (0,  0)  is  a  subset  of  
both  seen  as  the  three-valued  interpretations  ({p}, {q}) and  ({q}, {p}). 
In  general,  loops  through  an  odd  number  of negations  may  cause  the  pro­
gram  to  have  no  answer  set,  while  loops  through  an  even  number  of negations  
may  cause  the  program  to  have  multiple  answer  sets.  In  this  sense,  the  stable  
model  semantics  differs  from  the  WFS  that  makes  the  atoms  involved  in  the  
loops  undejined  in  both  cases.  
We can also define brave and cautious consequences. 
Definition  7 (Cautious and brave consequences). A  ground  atom  q  isabrave 
consequence of a  program  P  if 3A  E  AS(P)  suchthat  q  E  A.  We  denote  
the  set  containing  all  the  brave  consequences  with  BC(P).  Similarly,  q  is  
a  cautious consequence if  VA  E AS(P),  q  E A,  and  we  denote  the  set  
containing  all  the  cautious  consequences  with  CC(P).  
Example  6  (Cautious and brave consequences). For  the  program  P4 from  
Example  5  we  have  
BC(P4)  
{b,p,  q} 
CC(P4)  
{b}  
For  the  program  P5  from  Example  5  we  have  
BC(P4)  
{p,q} 
CC(P4)  
0  
DLV 
[Leone 
et 
al., 
2006; 
Alviano 
et 
al., 
2017], 
Smodels 
[Syrjänen and Niemelä, 2001], and Potassco [Gebser et al., 2011] are ex­
amples of systems for ASP. 

24 Preliminaries  
1.5  Probability  Theory  
Probability theory provides a formal mathematical framework for dealing 
with uncertainty. The notions used in the book are briefly reviewed here; 
for a more complete treatment, please refer to textbooks such as [Ash and 
Doleans-Dade, 2000; Chow and Teicher, 2012]. 
Let W  be a set called the sample  space,  whose elements are the outcomes  
of the random process we would like to model. For example, if the process 
is that of throwing a coin, the sample space can be wcoin  =  {  h,  t} with 
h  standing for heads and t  for tails. If the process is that of throwing a 
die, Wdie  = {1, 2, 3, 4, 5, 6}. If we throw two coins, the sample space is 
W 2-coins  = {(h,  h),  (h,  t), (t,  h),  (t,  t) }. If we throw an infinite sequence of 
coins, wcoins  = { ( a1, a2, ...)Iai E  {  h,  t}}. If we measure the position of 
an object along a single axis, wpos_x  = IR, Oll a plane wpos_x_y  = JR2 
andin the space Wpos_x_y_z  = JR3 . If the object is limited to the unit in­
terval, square, and cube, then wunit_x  
1], wunit_x_y  
1]2, and 
wunit_x_y_z  = [0, 1 p.  
Definition  8 (Algebra). The  set  0  of  subsets  of  W  is  an  algebra an  the  
set  W  if.f  
•  WE  0; 
•  0  is  closed  under  complementation,  i.e.,  W  E 0  -
Wc  =  (0\w)  E  0;  
•  0  is  closed  under finite  Union,  i.e.,  W1  E 0,  W2  E 0- (w1  U w2)  E 0  
Definition  9 (O"-algebra). The  set  0 of subsets  ofW  is  a  O"-algebra an  the  set  
W  if.f  it  is  an  algebra  and  
•  0   is  closed  under  COUntahle  Union,  i.e.,  ij Wi  E  0  jor  i  = 1,  2,  ...  then  
Uiwi E  0. 
Definition  10  (Minimal O"-algebra). Let  A  be  an arbitrary  collection  of sub­
sets  ofW.  The  intersection  of all  O"-algebras  containing  all  elements  of Ais  
called  the  O"-algebra generated by A,  or  the  minimal O"-algebra containing A.  
lt  is  denoted  by  O"(A).  
dA)  is suchthat ~ 2  dA)  whenever ~ 2  A  and ~ is a O"-algebra. O"(A)  
always exists and is unique [Chow and Teicher, 2012, page 7]. 
The elements of a O"-algebra 0 on Ware  called measurable  sets  or events  
and (W,  0)  is called a measurable  space.  When W  is finite, 0  is usually 
the powerset of W.  In  general, however, not every subset of W  need be 
present in 0.  
= [0, 
= [0, 

1.5  Probability  Theory  25 
When throwing a coin, the set of events may be ocoin  = IP(wcoin)  and 
{h} is an event corresponding to the coin landing heads. When throwing 
a die, an example of the set of events isOdie = IP(Wdie)  and {1, 3, 5} is an 
event, corresponding to obtaining an odd result. When throwing two coins, 
(W2_coin,  0 2_coins) with 02_coins = 1P(W2_coins)  is a measurable space. 
When throwing an infinite sequence of coins, {(h, t,  03,  .. .)loi E  {h, t}} 
may be an event, corresponding to obtaining head and tails in the first two 
throws. When measuring the position of an object Oll an axis and Wpos_x  = 
JR, opos_x  may be the Bore[  (J-algebra  B  an  the  set  of  real  numbers,  the 
one generated, for example, by closed intervals {[a,  b]  :  a,  b  E  JR}. Then 
[-1, 1] may be an event, corresponding to observing the object in the [ -1, 1] 
interval. If the object is constrained to the unit interval wuit_x,  ounit_x  may 
be (J({[a, b]:  a,  b  E  [0, 1]}). 
Definition  11  (Probability measure). Given  a  measurable  space  (W,  0) of  
subsets  of W,  a  probability measure is  a  function  JL  : 0 ~ lR that  satisfies  
the  following  axioms:  
JL-l  JL(w)?  Oforallw  E  0; 
JL-2   JL(W)  = 1; 
J.L-3  JL  is  countably  additive,  i.e.,  ifO  = {w1,w2, ...} <:;  0 is  a  countable  
collection  ofpairwise  disjoint  sets,  then  JL(UwEO) =  .l:i JL(wi)·  
(W,  0, JL)  is  called  a  probability space. 
We also consider finitely additive probability measures. 
Definition  12  (Finitely additive probability measure). Given  a  sample  space  
Wand  an  algebra  0 of subsets  ofW,  a  finitely additive probability measure 
is  a function  JL  : 0 ~ lR that satisfies axioms ( JL-1)  and ( JL-2) of Definition 11  
and  axiom  
m-3  JL  isfinitely  additive,  i.e.,  w1 nw2 = 0 ~ JL(wl  uw2) = JL(wl)  + JL(w2)  
jor  all  Wl, W2 E 0. 
(W,  0, JL)  is  called  a  finitely additive probability space. 
Example  7  (Probability spaces ). When  throwing  a  coin,  (Wcoin,  ocoin,  J-lcoin)  
with  J-lcoin(0)=0 J.Lcoin({h})=0.5, J.Lcoin({t})=0.5, and  J-lcoin({h,  t})=1 is  a  
(finitely  additive)  probability  space.  When  throwing  a  die,  (Wdie,  Odie,  J-ldie)  
with  J.Lcoin  (w)  =  Iw I ·  
~ is  a  (finitely  additive)  probability  space.  

26 Preliminaries  
When  throwing  two  coins,  (W2_coins,  02_coins, JL2_coins)  with  JL2_coins  
( w)  = Iw I  ·  3~ is  a  (jinitely  additive)  probability  space.  For  the  position  of  
an  object  on  an  axis,  (wunit_x'  nunit_x'  J-lunit_x)  with  J-lunit_x  (I)=  Sr  dx  is  
a  probability  space.  (Wpos_x,  npos_x,  J.Lpos_x)  with  J-lpos_x  (I)=  Sr  n[O,l] dx  is  
also  a  probability  space.  
Let (W,  n,  JL)  be a probability space and (S,  ~) a measurable space. A func­
tion X : W  -
S  is said to be measurable  if the preimage of fJ  under X is in 
0 for every fJ  E  ~. i.e., 
x- 1(rJ) = {w E  Wl  X(w)  E  rJ} E  0,  'tfrJ  E  ~-
Definition  13  (Random variable). Let  (W,  0, JL)  be  a  probability  space  and  
let  (S,  ~) be  a  measurable  space.  A  measurable  function  X  :  W  -
S  is  a  
random variable. We  call  the  elements  of S  the  values ofX.  With  P(X  E  rJ)  
for  fJ  E  ~. we  indicate  the  probability that random variable X has value in fJ,  
dejined  as  JL(X-1(rJ)). 
If  
~ is  finite  or  countable,  X is  a  discrete random variable. If  ~ is  un­
countable,  then  X is  a  continuous random variable. 
We indicate random variables with Roman uppercase letters X, Y, ... and the 
values with Roman lowercase letters x, y, .... 
When (W,  0) = (S,  ~).Xis often the identity function and P(X  E  w)  = 
JL(w).  
In  the discrete case, the values of P  (X E  {  x}) for all x E  S  define the 
probability  distribution  of random variable X, often abbreviated as P(X=x)  
and P(x).  We indicate the probability distribution for random variable X 
with P(X).  
Example  8 (Discrete random variables). An  example  of a  discrete  random  
variable Xfor the probability space  (Wcoin, ncoin, J.Lcoin)  is the identity func­
tion  and  the  probability  distribution  is  P(X=h) = 0.5, P(X=t) = 0.5. 
When  throwing  a  die,  a  discrete  random  variable  X  can  be  the  identity  and  
P(X=n)=-k for  all  n E  {1, 2, 3, 4, 5, 6}. Another  discrete  random  variable  E 
foradie  can  represent  whether  the  outcome  was  even  or  odd  with  (S,  ~) = 
({e,o},IP'(S)) andE:  Wdie- SdejinedasE={l- o,2- e,3­
o, 4 -
e, 5 -
o, 6 -
e}. The  probability  distribution  is  then  P(E  =  e) =  
J.Ldie({2,4,6}) = -k  x 3 = ~ andP(E  = o) = J.Ldie({1,3,5}) = -k  x 3 = ~-

1.5  Probability  Theory  27 
For  the  probability  space  (W2-coins,  0 2-coins,  t-t2_coins),  a  discrete  ran­
dom  variable  may  be  the  function  X  :  W 2-coins  ~ wcoin  defined  as  
X({c1,c2}) =  c1, 
i.e.,  thefunction  returning  the  outcome  ofthefirstcoin.  Forvalue  h  E  wcoin,  
we  have  
x- 1 ({h}) = {(h, h), (h, t)} 
and  
P(X  =  h) =  t-t2_coins( {(h, h), (h, t)}) =  0.5. 
In  the continuous case, we define the cumulative distribution and the proba­
bility density. 
Definition  14  (Cumulative distribution and probability density). The  cumu­
lative distribution of a  random  variable  X : (W,  0)  ~ (IR, B)  is  the  function  
F(x)  :IR~ [0, 1] defined  as  
F(x)  =  P(X  E  {tit ~ x}). 
We  write  P(X  E  {tlt ~ x}) also  as  P(X  ~ x). The  probability density of  
Xis  afunction  p(x)  suchthat  
P(X  E  A)  = JA  p(x)dx  
for  any  measurable  set  A  E  B.  
It  holds that: 
F(x)  
J~oo p(t)dt  
dF(x)  
p(x)  
dx  
P(X  E  [a, b])  
F(b)- F(a) =I: p(x)dx  
A discrete random variable is described by giving its probability distribu­
tion P(X)  while a continuous random variable is described by giving its 
cumulative distribution F(x)  or probability density p(x).  

28 
Preliminaries  
Example  9  (Continuous random variables). For  the  probability  space  of an  
object  in  the  unit  interval  (wunit_x,  ounit_x,  J.Lunit_x),  the  identity  X  is  a  
continuous  random  variable  with  cumulative  distribution  and  density  
F(x)  = Lx  dt  = x 
p(x)  = 1 
for  x E  [0, 1]. For  probability  space  (Wpos_x,  opos_x,  J-lpos_x),  the  identity  X 
is  a  continuous  random  variable  with  cumulative  distribution  and  density  
0 ifx  <  0 
F(x)  =  
X ijxE [0,1]
{ 1 ifx  >  1 
1 ifx  E  [0, 1]
p(x)  =  { 0 otherwise  
This  is  an  example  of an  uniform  density.  In  general,  the  uniform density in 
the interval [ a, b] is  
(x) = { b~a ifx  E [a, b]  
p  
0 
otherwise  
Another  notable  density  is  the  normal or Gaussian density, with  
1 
(x-~2
p(x)  = 
e-
2a  
~ 
where  JL  and  (]  are  parameters  denoting  the  mean  and  the  standard  deviation  
( (]2 is  the  variance ).  We  use  N  (JL,  (])  to  indicate  a  normal  density.  Exam­
ples  of Gaussian  densities  for  various  values  of the  parameters  are  shown  in  
Figure  1.5.  
When the values of a random variable are numeric, we can compute its ex­
pected  value  or expectation,  which intuitively is the average of the values 
obtained by repeating infinitely often the experiment it  represents. The ex­
pectation of a discrete variable X is 
E(X) =  .2:: xP(x)  
X  
while the expectation of a continuous variable X with domain lR is 
+oo 
E(X) = -oo xp(x)dx. 
J  

1.5  Probability  Theory  29 
II  
I  1  
Distributions  
00   
I 
I  
-­
~=0 o'2=1 
0  
I  
I  
~=0 a'2=0.2 
I  
I  
I  
I  
~=0 a'2=5.0  
I  
I  
·--
~=-2 a'2=0.5  
I  
I  
I  
I  
"' 
I  
I  
I  
I  
0 
,. \ 
I  
I  
I  
I 
I  
I 
~ 
I  
I  
"'c: 
I  
I  
I  
I  
<ll  
.,. 
0 
I  
I  
I  
I  
0 
1 /"""- 1 
I  
I  
I  
I   
I  
I 
I  
I  
I 
N  
I  
I  I  
I 
0 
I  
_. ."..--·  ·-· . ·- - - ~ ----· 
I  
1\ 
\ 
I  '.  
I 
.. ·~.":~ .....  
I  
\  
\  
0  
_.,. 
/  
~ ....._____,. _ .... __ 
::::::::::::::.::::--- ­
0 
-4 
-2 
0 
2 
4 
X  
Figure  1.5  Gaussian densities. 
Multiple random variables can be defined on the same space (W,  n, J..l  ), es­
pecially when (W, 0) =1=  (S,  2:) . Suppose two random variables X and Y are 
defined on the probability space (W,  n, J..l)  and measurable spaces (S1, 2:1) 
and (S2 , 2:2), respectively. We can define theJoint probability  P(X  E  0"1 , Y  E  
0"2) as J..l(X- 1(0"1) n  y - 1(0"2)) for 0"1 E  2:1 and 0"2 E  2:2. Since X and Y are 
random variables and 0 is a 0"-algebra, then x- 1(0"1) fl  y - l(0"2) E  0 and 
the joint probability is well defined. If  X and Y are discrete, the values of 
P(X= {x}, Y ={y}) for all x E  S1 and y E  S2 define the Jointprobability  
distribution  of X and Y often abbreviated as P(X=x, Y = y) and P(x, y) . 
We call P(X,  Y) the joint probabilüy distribution of X and Y.  If  X and Y 
are continuous, we can define the Joint  cumulative  distribution  F(x,  y) as 
P(X  E {tlt:::; x}, Y E {tlt:::; y}) andJointprobabilitydensity  as the function 
p(x, y) suchthat P(X  E  A,  Y E  B)  =  SA SB  p(x,  y)dxdy.  
If X is discrete and Y continuous, we can still define the joint cumulative 
distribution F(x,  y) as P(X=x,  Y E  {  t lt :::; y}) and the joint probability 
density as the function p(x, y) suchthat P(X=x, Y E  B)  = SB p(x,  y)dy.  
Example  10  (Joint distributions). When  throwing  two  coins,  we  can  define  
two  random  variables  X1 and  X2 as  X1((c1 , c2)) =  c1 and  X2((c1, c2 )) =  

30 Preliminaries  
c2: the  first  indicates  the  outcome  of the  first  coin  and  the  second  of the  sec­
ond.  The  Jointprobability  distribution  is  P(Xl =Xl , x2 =x2) =~ for  all  
X1 , X2 E {h,t}. 
For  the  position  of an  obJect  an  a  plane,  we  can  have  random  variable  
X  for  its  position  an  the  X-axis  and  Y  an  the  Y-axis.  An  example  of Joint  
probability  density  is  the  bivariate  normal  or  Gaussian  density  
x _ exp(-~(x-J-L)T~ -
1 (x-J-L)) 
, n,
p(  ) -
,_ 
(1.2) 
where  x isareal  two-dimensional  column  vector,  J-L  isareal  two-dimensional  
column  vector,  the  mean, ~ is  a  2 x 2 symmetric  positive-definite1  matrix  
called  covariance matrix, and  det ~ is  the  determinant  of~. 
A  bivariate Gaussian  density  with  parameters  p,  =  [0,Of and  
~= [ ~~ ] = I  
is  shown  in  Figure  1.6.  
Definition  15  (Product O"-algebra and product space). Given  two  measurable  
spaces  (W1 , 01) and  (W2, 02), define  the  product O"-algebra 01 ® 02 as  
01 @02 = 0"  ({wl X W2 lw1 E 01,W2 E 02}). 
-4 
4 
Figure  1.6  Bivariate Gaussian density. 
1 All its eigenvalues are positive. 
 

1.5  Probability  Theory  31 
Note  that  01 0  02 differs  from  the  Cartesian  product  01 x 02 because  it  is  
the  minimal  a--algebra  generated  by  all  possible  Cartesian  products  ofpairs  
of elements  from  01 and  02. 
Then  define  the  product space (W1, 01) x (W2, 02) as  
(W1, 01) x (W2, 02) =  (W1 x w2, o1 0 02). 
Given two random variables X  and Y  that are defined on the probability space 
(W,  0, J.L) and measurable spaces (81, ~1) and (82, ~2), respectively, we can 
define the function XY suchthat XY(w)  =  (X(w), Y(w)). Such a function 
is a random variable defined on the same probability space and the product 
measurable space (81, ~1) x  (82, ~2) [Chow and Teicher, 2012, Theorem 
1, page 13]. So, for example, two discrete random variables X and Y with 
values Xi and Yj with their joint distribution P(X,  Y) can be interpreted as 
a random variable XY with values (xi, Yj) and distribution P((X,  Y)). The 
results in the following thus apply also by replacing a random variable with 
a vector (or set) of random variables. We usually denote vectors of random 
variables with boldface capitalletters such as X, Y, ... and vectors of values 
with boldface lowercase letters such as x, y, .... 
Definition  16 (Conditional probability). Given  two  discrete  random  vari­
ables  X and  Y and  two  values  x and  y for  them,  if P(y)  >  0, we  can  define  
the  conditional probability P(xly) as  
P(  I  )  = P(x,y)   
X  y 
P(y)   
lf P(y)  = 0, then  P(xly) is  not  defined.  
P(xly) provides  the  probability  distribution  ofvariable  X  given  that  ran­
dom  variable  Y was  observed  having  value  y. 
From this, we get the important product  rule  
P(x,  y) = P(xly)P(y) 
expressing a joint distribution in terms of a conditional distribution. 
Moreover, x and y can be exchanged obtaining P(x,  y) = P(ylx)P(x), 
so by equating the two expressions, we find Bayes'  theorem  
=  P(ylx)P(x)
P(  I  ) 
xy 
P(y)  
.  
Now consider two discrete random variables X  and Y, with X  having a finite 
set of values { x 1, ... , Xn}, and let y be a value of Y. The sets 
x-1({x1}) n  y-1({y}), ... 'x-1({xn}) n  y-1({y}) 

32 Preliminaries  
are mutually exclusive because X  is a function and 
y- 1 (y) = x- 1 ({xi}) n  y-1 ({y}) u 
u x- 1 ({xn}) n  y- 1 ({y})o
0 
0 
0 
Given the fact that probability measures are additive, then 
n  
P(X  E {x1, 
0 
0, Xn}, y) = P(y)  = .2:  P(xi,  y)
0 
i=l 
This is the sum  rule  of probability theory, often expressed as 
P(y)  =  .2:  P(x,  y) 
X  
The sum rule eliminates a variable from a joint distribution and we also say 
that we sum  out  the variable and that we marginalize  the joint 
distributiono 
For continuous random variables, we can similarly define the conditional  
density  p(XIy) as 
p(xly) = p(x,  y) 
p(y)  
when p(y)  >  0, and get the following versions of the product and sum rules 
p(x,  y) =  p(xly)p(y) 
p(y)  =  J: p(x,  y)dx  
We can also define conditional  expectations  as 
E(XIy) = .2:  xP(xly) 
X  
J
+oo  
E(XIy) = 
-oo xp(xly)dxo  
for X a discrete or continuous variable, respectivelyo 
For continuous random variables, we may have evidence on Y in the form 
of the measurement of a value y for it. In many practical cases, such as the 
Gaussian distribution, P  (y) = 0 for all values y ofY, so the conditional prob­
ability P(X  E  w1IY)  is not definedo This is known as the Borel-Kolmogorov  
paradox  [Gyenis et al., 2017]. 

1.6  ProbabilistiG  Graphical  Models  33 
In some cases, it can be solved by defining the conditional probability as 
. 
P(X  E w1, Y E [y- dv/2,  y +  dvj2]) 
P(x  E W1 I y)  =  lIm  
. 
dv---->0  
P(Y  E  [y- dv/2,  y +  dv/2]) 
However, the limit is not always defined. 
Definition  17 (Independence and conditional indepedence ). Two  random  vari­
ables  X and  Y are  independent, indicated  with  I(X,  Y, 0)  iff  
P(xly) = P(x)  whenever  P(y)  >  0 
Two  random  variables  X and  Y are  conditionally independent given Z, indi­
cated  with  I  (X, Y, Z) iff  
P(xly, z) = P(xlz) whenever  P(y,  z) >  0 
1.6  Probabilistic  Graphical  Models  
It  is often convenient to describe a domain using a set of random variables. 
For example, a home intrusion detection system can be described with the 
random variables 
• Earthquake E, which assumes value true ( t)  if an earthquake occurred 
and false (f)  otherwise; 
• Burglary B, which assumes value true ( t)  if a burglary occurred and false 
(f)  otherwise; 
• Alarm A, which assumes value true ( t)  if the alarm went off and false 
(f)  otherwise; 
• Neighbor call N, which assumes value true (t)  ifthe neighbor called and 
false (f)  otherwise. 
These variables may describe the situation of the system at a particular point 
in time, such as last night. We would like to answer the questions such as 
• What is the probability of a burglary? (compute P(B=t),  belief compu­
tation) 
• What is the probability  of a burglary given that the neighbor called? 
( compute P  (B=t IN=t), belief updating) 
• What is the probability of a burglary given that there was an earthquake 
and the neighbor called? (compute P(B=tiN=t, E=t), beliefupdating) 
• What is the probability  of a burglary and of the alarm ringing given 
that there was an earthquake and the neighbor called? (compute P(A=t,  
B=tiN=t, E=t), beliefupdating) 

34 
Preliminaries  
• What is the rnost likely value for burglary given that the neighbor called? 
(argrnaxb P(biN =t), beliefrevision). 
When assigning a causal rneaning to the variables, the problerns are also 
called 
• Diagnosis: cornputing P(causelsymptom).  
• Prediction: cornputing P(symptomlcause).  
Moreover, another inference problern is 
• Classification: cornputing argrnaxclass P(classldata).  
In general, we want to cornpute the probability P(qle) of a query  q (assign­
rnent of values to a set of variables Q) given the evidence e (assignrnent 
of values to a set of variables E).  This problern is called inference.  If X 
denotes the set of all variables describing the dornain and we know the joint 
probability distribution P(X),  i.e., we know P(x)  for all x, we can answer 
all types of queries using the definition of conditional probability and the 
surn rule: 
P(q,  e) 
P(qle) 
P(e)  
.l:y,Y=X\Q\E P(y,  q, e) 
.l:z,Z=X\E P(z,  e) 
However, if we have n  binary variables (lXI =  n), knowing the joint prob­
ability distribution requires storing 0(2n) different values. Even if we had 
the space to store all the 2n different values, cornputing P(qle) would re­
quire 0 (2n) operations. Therefore, this approach is irnpractical for real-world 
problerns and a different solution rnust be adopted. 
Firstnote that if X =  {X1, ... , Xn}. a value of Xis a tuple (x1, ... , xn)  
also called a joint  event  and we can write 

1.6  ProbabilistiG  Graphical  Models  35 
P(x)  =  P(x1, ... , Xn)  = 
P(xniXn-l,  ...  ,xi)P(xn-l,  ...  ,x1) =  
n
P(xniXn-l,  ...  , xi) ... P(x2lxi)P(xi) =  
(1.3) 
n 
P(xiiXi-1, ... ,xi) 
i=l 
where Equation (1.3) is obtained by repeated application of the product rule. 
This formula expresses the so-called chain  rule.  
Now if we knew, foreachvariable Xi, a subset P~ of {Xi-l, ... , X1} 
such that Xi is conditionally independent of {Xi-l, ... , XI}\P~ 
given Pai, i.e., 
P(xilxi-1, ... , xi) =  P(xiiPai) whenever P(xi-1, ... , xi) >  0, 
then we could write 
P(x)  =   P(x1, ... , Xn)  =  
P(xniXn-l,  ...  , xi) ... P(x2lxi)P(xi) = 
n
P(xniP~) ... P(x2lpa1)P(x1IPa1) = 
n 
P(xiiPai) 
i=l 
Therefore, in order to compute P(x),  we have to store P(xiiPai) for all val­
ues Xi and pai. The set of values P(xiiPai) is called the Conditional proba­
bility table (CPT) ofvariable Xi. IfPai is much smallerthan {Xi-l, ... , XI}, 
then we have huge savings. For example, if k  is the maximum size of Pai, 
then the storage requirements are O(n2k)  instead of 0(2n). So it is important 
to take into account independencies among the variables as they enable much 
faster inference. One way to do this is by means of graphical  models  that are 
graph structures that represent independencies. 
An example of a graphical model is a BN [Pearl, 1988] that represents a 
set of variables with a directed graph with a node per variable and an edge 
from Xj to Xi only if Xj E  P~. The variables in Pai are in factalso called 
the parents  of Xi and Xi is called a child  of every node in Pai. Given that 
P~ ~ {Xi-l, ... , X1}, the parents of Xi will always have an index lower 
than i  and the ordering <X1, ... , Xn) is a topological sort of the graph that 
is therefore acyclic. A BN tagether with the set of CPTs P(xi IPai) defines a 
joint probability distribution. 

36 Preliminaries  
I  bur,g I  t I  f I  
I  earthquake I  0~2 I  0~8 I 
0.1 
0.~ 
t 
f 
1.0 
0.0 
call 
a=t 
a=f 
t 
0.9 
0.05 
f 
0.1 
0.95 
cb'~~·l··· 
0.2 
1 b=f, e=t 1 o.8 
0.2 
1 b=f, e=r 1 o.1 
0.9 
Figure  1.7  Example of a Bayesian network. 
Example  11  (Alarm - BN). For  our  intrusion  detection  system  and  the  
variable  order  <E, B, A, N), we  can  identify  the  following  independencies  
P(e)  
P(e)  
P(ble) 
P(b) 
P(alb,e) 
P(alb, e) 
P(nla, b, e) 
P(nla) 
which  resuZt  in  the  BN  of Figure  1.  7 that  also  shows  the  CPTs.  
When a CPT contains only the values 0.0 and 1.0, the dependency ofthe child 
from its parents is deterministic  and the CPT encodes a function: given the 
values of the parents, the value of the child is completely determined. For 
example, if the variables are Boolean, a deterministic  CPT  can encode the 
AND or the OR Boolean function. 
The concepts of ancestors of a node and of descendants of a node are de­
fined as the transitive closure of the parent and child relationships: if there is a 
directed path from Xi to Xj, then Xi is an ancestor  of Xj and Xj is a descen­
dant  of Xi. Let ANC(X) (DESC(X)) be the set of ancestors (descendants) 
ofX. 
From the definition of BN, we know that, given its parents, a variable is 
independent of its other ancestors. However, BNs allow reading other inde­
pendence relationship using the notion of d-separation. 
Definition  18  (d-separation [Murphy, 2012]). An  undirected path P  in  a  BN  
is  a  path  connecting  two  nodes  not  considering  edge  directions.  
An  undirected path  P  is  d-separated by  a  set  of nodes  C if.f  at  least  one  of  
the  following  conditions  hold:  

1.6  Probabilistic  Graphical  Models  37 
• P  contains  a  chain,  S----* M ----* TorS~ M ~ T ; where  ME C; 
• P  contains  afork,  S ~ M ----* T, where  ME C 
• P  contains  a  collider  S 
----* 
M 
~ T, where  M 
1.  C and  
VX E  DESC(M) : X 1.  C. 
Two  sets  of random  variables  A and  B are  d-separated given  a  set  C if and  
only  if each  undirected path from  every node  A  E A to  every  node  B  E B is  
d-separated  by  C. 
It is possible to prove that A is independent from B given C iff A is 
d-separated from B given C, so d-separation and conditional independence 
are equivalent. 
The set of parents, children, and other children of the parents of a variable 
d-separates it  from the other variables, so it forms a sufficient set of nodes 
to make the variable independent from all the others. Such a set is called a 
Markov  blanket  and is shown in Figure 1.8. 
Graphical models are a way to represent a factorization of a joint proba­
bilüy distribution. BNs represent a factorization in terms of conditional prob­
abilities. In general, afactorized  model  takes the form 
P(x)  = TI: 1 rPi (xi) 
z  
where each rPi (xi) for i  =  1, ... , m  isapotential  orfactor,  a function taking 
non-negative values on a subset Xi of X. Factors are indicated with r/J, 1/J, ...  
Figure  1.8  Markov blanket. Figure from https://commons.wikimedia.org/ 
wiki/File:Diagram_of_a_Markov _blanket.svg. 

38 
Preliminaries  
Since the numerator of the fraction may not lead to a probability distri­
bution (the sumofall possible values at the numerator may not be one), the 
denominator Z  is added that makes the expression a probability distribution 
since it is defined as 
z  = 2.:  n<Pi(xi)  
X 
i  
i.e., it is exactly the sum of all possible values of the numerator. Z  is thus a 
normalizing  constant  and is also called partition  function.  
A potential (/Ji(xi)  can be seen as a table that, for each possible combi­
nation of values of the variables in Xi. returns a non-negative number. Since 
all potentials are non-negative, P(x)  ):  0 for all x. Potentials influence the 
distribution in this way: all other things being equal, a larger value of a po­
tential for a combination of values of the variables in its domain corresponds 
to a larger probability value of the instantiation of X. 
An example of a potential for a university domain is a function defined 
over the random variables Intelligent and GoodMarks expressing the fact that 
a student is intelligent and that he gets good marks, respectively. The potential 
may be represented by the table 
Intelligent 
false 
GoodMarks 
false 
(/Ji(I, G) 
4.5 
false 
true 
4.5 
true 
false 
1.0 
true 
true 
4.5 
where the configuration where the student is intelligent but he does not get 
good marks has a lower value than the other configurations. 
A BN can be seen as defining a factorized model with a potential for each 
family  ofnodes (a node and its parents) defined as </Ji(xi, paJ = P(xiiP~). 
It  is easy to see that in this case Z  = 1. 
If all the potentials are strictly positive, we can replace each factor <Pi  ( Xi) 
with the exponential exp(wdi(xi))  with Wi  a real number called weight  and 
fi(xi) a function from the values of Xi to the reals (often only to {0, 1}) 
called feature.  If the potentials are strictly positive, this reparameterization is 
always possible. In  this case, the factorized model becomes: 
P(x)  =  exp(Li wdi(xi)) 
z  
Z  = l:exp(l:wdi(xi)) 
X 
i  

1.6  ProbabilistiG  Graphical  Models  39 
This is also called a log-linear  model  because the logarithm of the joint is a 
linear function of the features. An example of a feature corresponding to the 
example potential above is 
f ·(I 
lli 
G 
dM k ) _ { 1 if --.IntelligentvGoodMarks 
2 nte gent, oo 
ar s -
th 
.
0 o erw1se 
If Wi  =  1.5, then cPi  (i, g) =  exp(wdi (i, g)) for all values i, g for random 
variables I, G in our university example. 
A Markov network (MN) or Markov random field [Pearl, 1988] is an 
undirected graph that represents a factorized model. An  MN has a node for 
each variable and each pair of nodes that appear together in the domain of a 
potential are connected by an edge. In other words, the nodes in the domain of 
each 
potential 
form 
a 
clique,  
a 
fully-connected 
subset 
of 
the nodes. 
Example  12  (University -MN). An  MN for  a  university  domain  is  shown  in  
Figure  1.9.  The  network  contains  the  example  potential  above  plus  a  potential  
involving  
the  
three  
variables  
GoodMarks,  
CouDifficulty,  
and  
TeachAbility.  
As BNs, MNs also allow reading off independencies from the graph. In  an 
MN where P(x)  >  0 for all x (a strictly positive distribution), two sets 
of random variables A and B are independent given a set C if and only if 
each path from every node A E  A to every node B E  B passes through an 
element of C [Pearl, 1988]. Then the Markov blanket of a variable is the set 
of its neighbors. So reading independencies from the graph is much easier 
than for BNs. 
MNs and BNs can each represent independencies that the other cannot 
represent [Pearl, 1988], so their expressive power is not comparable. MNs 
have the advantage that the potentials/features can be defined more freely, 
because they do not have to respect the normalization that conditional prob­
abilities must respect. On the other hand, parameters in an MN are diffi­
Figure  1.9  Example of a Markov network. 

40 
Preliminaries  
cult to interpret, because their infiuence on the joint distribution depends on 
the whole set of potentials, while in a BN, the parameters are conditional 
probabilities, so they are easier to interpret and to estimate. 
Given a BN, an MN representing the same joint distribution can be ob­
tained by moralizing  the graph: adding edges between all pairs of nodes that 
have a common child (marrying the parents), and then making all edges in 
the graph undirected. In this way, each family {Xi, Pai} of the BN forms a 
clique associated with the potential c/Ji(xi, p~) = P(xiiPai). 
Given an MN, an equivalent BN can be obtained containing a node for 
each variable plus a Boolean node Fi foreachpotential c/Ji· Edges are then 
added so that a potential node has all the nodes in its scope as parents. The 
BN equivalent to the MN of Figure 1.9 is shown in Figure 1.10. 
The CPT for node F i  is 
P(Fi  = 1lxi) = c/Ji(xi) 
Ci  
where Ci  is a constant depending on the potential that ensures that the values 
are probabilities, i.e., that they are in [0, 1]. It can be chosen freely provided 
that Ci  ? maxxi cj;(xi), for example, it can be chosen as Ci  = maxxi cj;(xi) 
or Ci  = .L:xi c/Ji(xi). The CPT foreachvariable node assigns uniform prob­
ability to its values, i.e., P(xj)  = 1.  where kj  is the number of values 
J  
ofXj. 
Then the joint conditional distribution P(xiF =  1)  of the BN is equal to 
the joint of the MN. In fact 
n 
n 
n 
cp·(x·) n 1 
P(x,F  = 1)  
P(Fi  = 1lxi) 
P(xj)  = 
_t -.t 
~ = 
i  
j  
i  
ct  
j  kJ  
Figure  1.10  Bayesian network equivalent to the Markov network of 
Figure 1.9. 

1.6  ProbabilistiG  Graphical  Models  41 
Figure 1.11 Example of a factor graph. 
Tii q)i(xi) 
I_  
Tii Ci  
. kj 
J  
and 
P(x,F  = 1)  
P(x,F=1) 
P(xiF  =  1)  
P(F  = 1)  
.L:x' P(x',  F  = 1)  
ni  cf;;(x;) TI.  _l_ 
O;Ci 
J  kj  
.L: 
I  n;  cf;;  (x';) TI  . _l_ 
X  
O;Ci 
J  kj  
1  
0; C;  Tij fi  Tii q)i (xi) 
Tii <Pi(xi) 
1
0; C; Tij fj  .L:x' Tii q)i(x'i) 
z  
So, a query q  given evidence e to the MN can be answered using the equiva­
lent BN by computing P(qle,  F =  1).  
The equivalent BN encodes the same conditional independencies as the 
MN provided that the set of condition nodes is extended with the set of factor 
nodes, so an independence I(A,  B, C) holding in the MN will hold in the 
equivalent BN in the form J(A, B, Cu F). For example, a pair of nodes 
from the scope Xi of a factor given the factor node F i cannot be made inde­
pendent no matter what other nodes are added to the condition set because of 
d-separation, just as in the MN where they are neighbors. 
A third type of graphical model is the Factor graph (FG) which can rep­
resent general factorized models. An  FG is undirected and bipartite, i.e., its 
nodes are divided into two disjoint sets, the set of variables and the set of fac­
tors, and the edges always have an endpoint in a set and the other in the other 
set. A factor corresponds to a potential in the model. An FG contains an edge 
connecting each factor with all the variables in its domain. So from an FG, 
n 

42 
Preliminaries  
one can immediately read the factorization of the model. An FG representing 
the factorized model of Figure 1.9 is shown in Figure 1.11.  
FGs are specially useful for expressing inference algorithms, in particular 
the formulas for belief propagation are simpler to express for FGs than for 
BNs andMNs. 

2   
Probabilistic  Logic  Programming  Languages  
Various approaches have been proposed for combining logic programming 
with probability theory. They can be broadly classified into two categories: 
those based on the Distribution  Semantics  (DS) [Sato, 1995] and those that 
follow a Knowledge  Base  Model  Construction  (KBMC) approach. 
For languages in  the first category, a probabilistic logic program without 
function symbols defines a probability distribution over normal logic pro­
grams (termed worlds).  Todefine the probability of a query, this distribution is 
extended to a joint distribution of the query and the worlds and the probability 
of the query is obtained from the joint distribution by marginalization, i.e., 
by summing out the worlds. For probabilistic logic programs with function 
symbols, the definition is more complex, see Chapter 3. 
The distribution over programs is defined by encoding random choices 
for clauses. Bach choice generates an alternative version of the clause and 
the set of choices is associated with a probability distribution. The various 
languages that follow the DS differ in how the choices are encoded. In all 
languages, however, choices are independent from each other. 
In the KBMC approach, instead, a probabilistic logic program is a com­
pact way of encoding a large graphical model, either a BN or MN. In  the 
KBMC approach, the semantics of a program is defined by the method for 
building the graphical model from the program. 
2.1  Languages  with  the  Distribution  Semantics  
The languages following the DS differ in  how they encode choices for clauses, 
and how the probabilities for these choices are stated. As will be shown in  
Section 2.4, they all have the same expressive power. This fact shows that 
the differences in the languages are syntactic, and also justifies speaking 
ofthe  DS. 
43  

44 Probabilistic  Logic  Programming  Languages  
2.1.1  Logic  programs  with  annotated  disjunctions  
In Logic Programs with Allnotated Disjunctions (LPADs) [Vennekens et al., 
2004], the alternatives are expressed by means of annotated disjunctive heads 
of clauses. An annotated  disjunctive  clause  Ci  has the form 
hil  : Ili1 ; . . . ; hin;  : IIin; ~ bil,  ... , bim;  
where hil,  ... , hin;  are logical atoms, bil,  ... , bim;  are logical literals, and 
Ili1, ... , ITin;  arereal numbers in the interval [0, 1] suchthat .2::~~ 1 ITik  =  1. 
An LPAD is a finite set of annotated disjunctive clauses. 
Each world is obtained by selecting one atom from the head of each 
grounding of each annotated disjunctive clause. 
Example  13  (Medical symptoms- LPAD). The  following  LPAD  models  the  
appearance  of medical  symptoms  as  a  consequence  of disease.  A  person  may  
sneeze  if she  has  the  flu  or  if she  has  hay fever:  
sneezing(X)  : 0.7; null:  0.3 ~ fiu(X).  
sneezing(X)  : 0.8; null:  0.2 ~ hayJever(X).  
fiu(bob).  
hayJever(bob).  
The  first  clause  can  be  read  as:  if X  has  the  flu,  then  X  sneezes  with  prob­
ability  0.7  and  nothing  happens  with  probability  0.3.  Similarly,  the  second  
clause  can  be  read  as:  if X  has hay fever,  then X  sneezes with probability 0.8  
and nothing happens with probability 0.2. Here,  and for the other languages  
based  an  the  distribution  semantics,  the  atom  null  does  not  appear  in  the  
body  of any  clause  and  is  used  to  represent  an  alternative  in  which  no  atom  
is  selected.  lt  can  also  be  omitted  obtaining  
sneezing(X)  : 0.7 ~ fiu(X).  
sneezing(X)  : 0.8 ~ hayJever(X).  
fiu(bob).  
hayJever(bob).  
As can be seen from the example, LPADs encode in a natural way programs 
representing causal mechanisms: flu and hay fever are causes for sneezing, 
which, however, is probabilistic, in the sense that it may or may not hap­
pen even when the causes are present. The relationship between the DS, and 
LPADs in particular, and causal reasoning is discussed in Section 2.8. 

2.1  Languages  with  the  Distribution  Semantics  45 
2.1.2  Problog  
The design of ProbLog [De Raedt et al., 2007] was motivated by the desire to 
make the simplest probabilistic extension of Prolog. In  ProbLog, alternatives 
are expressed by probabilistic facts  of the form 
rri .. !t  
where IIi E  [0, 1] and fi  is an atom, meaning that each ground instantia­
tion fi()  of fi  is true with probability rri and false with probability 1 - rri. 
Each world is obtained by selecting or rejecting each grounding of each 
probabilistic fact. 
Example  14  (Medical symptoms- ProbLog). Example  13  can  be  expressed  
in  ProbLog  as:  
sneezing(X)  ~ fiu(X),fiu_sneezing(X).  
sneezing(X)  ~ hayJever(X),  hayJever _sneezing(X).  
fiu(bob).  
hayJever(bob).  
0.7 :: fiu_sneezing(X).  
0.8 :: hayJever _sneezing(X).  
2.1.3  Probabilistic  horn  abduction  
Probabilistic horn abduction (PHA) [Poole, 1993b] and Independent choice 
logic (ICL) [Poole, 1997] express alternatives by facts, called disjoint  State­
ments,  having the form 
disjoint([ail  : IIil, ... , ain  : IIini]).  
where each aik  is a logical atom and each IIik  a number in [0, 1] suchthat 
.L:~~ 1 IIik  =  1. Such a statement can be interpreted in terms of its ground 
instantiations: for each substitution ()  grounding the atoms of the statement, 
the aik()s  are random alternatives and aik()  is true with probability IIik·  Each 
world is obtained by selecting one atom from each grounding of each dis­
joint statement in the program. In practice, each ground instantiation of a 
disjoint statement corresponds to a random variable with as many values as 
the alternatives in the statement. 

46 
Probabilistic  Logic  Programming  Languages  
Example  15  (Medical symptoms - ICL). Example  13  can  be  expressed  in  
/CL  as:  
sneezing(X)  ~ fiu(X),fiu_sneezing(X).  
sneezing(X)  ~ hayJever(X),  hayJever _sneezing(X).  
fiu(bob).  
hayJever(bob).  
disjoint([fiu_sneezing(X)  : 0.7, null:  0.3]). 
disjoint([hayJever _sneezing(X)  : 0.8, null  : 0.2]). 
In ICL, LPADs, and ProbLog, each grounding of a probabilistic clause is 
associated with a random variable with as many values as altematives/head 
disjuncts for ICL and LPADs and with two values for ProbLog. The random 
variables corresponding to different instantiations of a probabilistic clause are 
independent and identically distributed (IID). 
2.1.4  PRISM  
The language PRISM [Sato and Kameya, 1997] is similar to PHA/ICL but 
introduces random factsvia the predicate msw/3  (multi-switch):  
msw(SwitchName,  Trial!d,  Value).  
The first argument of this predicate is a random  switch  name,  a term repre­
senting a set of discrete random variables; the second argument is an integer, 
the trial  id;  and the third argument represents a value for that variable. The 
set of possible values for a switch is defined by a fact of the form 
values(SwitchName,  [v1, ... ,vn]).  
where SwitchName  is again a term representing a switch name and each 
Vi  is a term. Each ground pair (SwitchName,  Trial!d)  represents a distinct 
random variable and the set of random variables associated with the same 
switch are IID. 
The probability distribution over the values of the random variables asso­
ciated with SwitchN ame  is defined by a directive of the form 
~ set_sw(SwitchName,  [Ih, ... , IIn]). 
where rri is the probability that variable SwitchName  takes value Vi.  Each 
world is obtained by selecting one value for each trial id of each random 
switch. 

2.2 The  Distribution  Semanticsfor  Programs  Without  Function  Symbols  47 
Example  16  (Coin tosses- PRISM). The  modefing  of coin  tosses  shows  dif­
ferences  in  how  the  various  PLP  languages  represent  IID  random  variables.  
Suppose  that  coin  c1  is  known  not  to  be  fair,  but  that  all  tosses  of c1  have  the  
same  probabilities  of outcomes- in  other  words,  each  toss  of c1  is  takenfrom  
afamily  of IID  random  variables.  This  can  be  represented  in  PRISMas  
values(cl,  [head,  tail]).  
~ set_sw(cl,  [0.4, 0.6]) 
Different  tosses  of  CI can  then  be  identified  using  the  trial id argument  of  
msw/3.  
In  PHA/ICL  and many  other  PLP  languages,  each  ground  instantiation  of  
a  disjoint/1 statement  represents  a  distinct  random  variable,  so  that  IID  ran­
dom  variables  need  to  be  represented  through  the  statement's  instantiation  
patterns:  e.g.,  
disjoint([coin(cl,  TossNumber,  head)  : 0.4, 
coin(c1,  TossNumber,  tail)  : 0.6]). 
In  practice, the PRISM system accepts an msw /2 predicate whose atoms 
do not contain the trial id and for which each occurrence in a program is 
considered as being associated with a different new variable. 
Example  17  (Medical symptoms - PRISM). Example  15  can  be  encoded  in  
PR/SM  as:  
sneezing(X)  ~ fiu(X),  msw(fiu_sneezing(X),  1).  
sneezing(X)  ~ hayJever(X),  msw(hayJever _sneezing(X),  1).  
fiu(bob).  
hayJever(bob).  
values(fiu_sneezing(X),  [1, 0]). 
values(hayJever _sneezing(X),  [1, 0]). 
~ set_sw(fiu_sneezing(X),  [0.7, 0.3]). 
~ set_sw(hayJever _sneezing(X),  [0.8, 0.2]). 
2.2  The  Distribution  Semanlies  for  Programs  Without  Function  
Symbols  
We present first the DS for the case of ProbLog as it  is the language with 
the simplest syntax. A ProbLog program P  is composed by a set of normal 

48 
Probabilistic  Logic  Programming  Languages  
rules R  and a set F  of probabilistic facts. Each probabilistic  fact  is of the 
form IIi  ::  fi  where IIi  E  [0, 1] and fi  is an atom 1,  meaning that each ground 
instantiation fi()  of fi  is true with probability IIi  and false with probability 
1 - IIi.  Each world is obtained by selecting or rejecting each grounding of 
each probabilistic fact. 
An atomic  choice  indicates whether grounding f  ()  of a probabilistic fact 
F  = II  ::  f  is selected or not. It is represented with the triple (f,  (),  k)  where 
k  E  {0, 1} and k  = 1 means that the fact is selected, k  = 0 that it is not. A 
set "" of atomic choices is consistent  if it does not contain two atomic choices 
(f,  (),  k)  and (f,  (),  j)  with k  =I=  j  ( only one alternative is selected for a ground 
probabilistic fact). The function consistent("")  retums true if"" is consistent. 
A composite  choice  ""is a consistent set of atomic choices. The probability of 
composite choice "" is 
P("")  =  n  IIi  n  1- IIi.  
(f;,O,l)Er;, 
(f;,O,O)Er;, 
A selection  0'  is a total composite choice, i.e., it contains one atomic choice 
for every grounding of every probabilistic fact. A world Wu  is a logic program 
that is identified by a selection 0'.  The world Wu  is formed by adding f  for 
each atomic choice (f,  (),  1)  of 0'  to R.  
The probability of a world Wu  is P( Wu)  = P(O').  Since in  this section we 
are assuming programs without function symbols, the set of groundings of 
each probabilistic fact is finite, and so is the set of worlds Wp.  Accordingly, 
foraProbLog program P,  Wp  = {w1, ... , wm}· Moreover, P(wu)  is a dis­
tribution over worlds: .L:wEWp  P( w)  = 1. We call sound  a program for which 
every world has a two-valued WFM. We consider here sound programs, for 
non-sound ones, see Chapter 6. 
Let q  be a query in  the form of a ground atom. We define the conditional 
probability of q  given a world w  as: P(qlw)  = 1 if q  is true in  w  and 0 
otherwise. Since the program is sound, q  can be only true or false in a world. 
The probability of q  can thus be computed by summing out the worlds from 
the joint distribution of the query and the worlds: 
P(q)  =  l:P(q,w)  =  l:P(qlw)P(w)  =  :.2:: P(w).  
(2.1)  
w  
w  
wl=q  
So the probability of q  is the sum of the probabilities of the worlds where 
q  is true. This formula can also be used for computing the probability of a 
1  With an abuse of notation, sometimes we use F  to indicate the set containing the atoms 
f;s.  The meaning ofFwill be clear from the context. 

2.2 The  Distribution  Semanticsfor  Programs  Without  Function  Symbols  49 
conjunction q1 ,  ...  ,  qn  of ground atoms since the truth of a conjunction of 
ground atoms in  a world is well defined. So we can compute the conditional 
probability of a query q  given evidence e  in  the form of a conjunction of 
ground atoms e 1, ... , em  as 
(2.2) 
We can also assign a probability to a query q  by defining a probability space. 
Since Wp  is finite, then (Wp,  IP(Wp))  is a measurable space. For an element 
w  E  IP(Wp),  define JL(w)  as 
JL(w)  = 2.: P(w)  
WEW  
with the probability of a world P( w)  defined as above. Then it's easy to see 
that (Wp,  IP(Wp ),  JL)  is a finitely additive probability space. 
Given a ground atom q,  define the function Q: Wp- {0, 1} as 
1 ifwF=q 
(2.3)
Q ( w)  = { 0 otherwise 
Since the set of events is the powerset, then Q-1(1)  E  IP(Wp)  for all 
1 ~ {0, 1} and Q  is a random variable. The distribution of Q  is defined 
by P(Q=1)  (P(Q=O)  is given by 1 - P(Q=1)) and we indicate P(Q=1)  
with P(q).  
We can now compute P(q)  as 
P(q)  = J.L(Q- 1 ({1})) = JL({wlw  E  Wp,  w  F= q}) = 2.: P(w)  
wl=q  
obtaining the same formula as Equation (2.1). 
The distribution over worlds also induces a distribution over interpreta­
tions: given an interpretation I,  we can define the conditional probability of 
I  given a world was: P(IIw)  = 1 is I  is the model of w  (I  F= w) and 0 
otherwise. The distribution over interpretations is then given by a formula 
similar to Equation (2.1): 
P(I)  = 2.: P(I,  w)  = 2.: P(IIw )P( w)  = 2.: P( w)  
(2.4) 
w  
w  
Ii=w  
We call the interpretations I  for which P(I)  >  0 possible  models  because 
they are models for at least one world. 

50 
Probabilistic  Logic  Programming  Languages  
Now define the function I : Wp  ~ {0, 1} as 
1 ifiF=w 
I (I)  =  { 0 
(2.5)
otherwise 
I-1  ( 1) E  JPl(Wp)  for all 1 ~ {0, 1} so I is a random variable for probability 
space (Wp,  P(Wp ), Ji,).  The distribution of I is defined by P(I=1) and we 
indicate P  (I= 1)  with P  (I).  
We can now compute P(I)  as 
P(I)  = tt(r 1 ({1})) = tt({wlw  E  Wp,I  F= w}) = .2: P(w)  
Ii=w  
obtaining the same formula as Equation (2.4). 
The probability of a query q  can be obtained from the distribution over 
interpretations by defining the conditional probability of q  given an interpre­
tation I  as P(qii)  =  1 if I  F= q  and 0 otherwise and by marginalizing the 
interpretations obtaining 
P(q)  = .2: P(q,I)  = .2: P(qii)P(I)  = .2: P(I)  = .2: 
P(w)  (2.6) 
I  
I  
Ii=q  
Ii=q,Ii=w  
So the probability of a query can be obtained by summing the probability of 
the possible models where the query is true. 
Example  18  (Medical symptoms - worlds - ProbLog). Consider  the  pro­
gram  of Example  14.  The  program  has Jour  worlds  
W1  = { 
W2  = { 
ftu_sneezing(bob).  
hayJever _sneezing(bob).  
hayJever _sneezing(bob).  
} 
}  
P(wl)  = 
0.7 X 0.8 
P(w2)  = 0.3 X 0.8  
W3  = { 
W4  = {  
ftu_sneezing(bob).  
} 
}  
P(w3)  = 
0.7 X 0.2 
P(w4)  = 0.3 X 0.2  
The  query  sneezing(bob)  is  true  in  three  worlds  and  its  probability  
P(sneezing(bob))  =  0.7 x 0.8 +  0.3 x 0.8 +  0.7 x 0.2 =  0.94. 

2.2 The  Distribution  Semanticsfor  Programs  Without  Function  Symbols  51 
Note  that  the  contributions from  the  two  clauses  are  combined  disjunctively.  
The  probability  of the  query  is  thus  computed  using  the  rule  giving  the  prob­
ability  of the  disjunction  of two  independent  Boolean  random  variables:  
P(a  v b)  = P(a)  +  P(b)- P(a)P(b)  = 1- (1- P(a))(1- P(b)).  
In  our  case,  P(sneezing(bob))  =  0.7 +  0.8-0.7 · 0.8 =  0.94. 
We now give the semantics for LPADs. A clause 
Ci  = hil  : Ili1 ; . . . ; hini  : IIini  +-- bil,  ... , bimi  
stands for a set of probabilistic clauses, one for each ground instantiation C/J 
of Ci.  Each ground probabilistic clause represents a choice among ni  normal 
clauses, each of the form 
hik  +-- bil,  · · · , bimi  
for k  = 1 ... , ni.  Moreover, another clause 
null  +-- bil,  .. · , bimi  
is implicitly encoded which is associated with probability IIo =  1-.2::~~ 1 IIk.  
So, for the LPAD P,  an atomic  choice  is the selection of a head atom for a 
grounding Ci()j  of a probabilistic clause Ci,  including the atom null.  An  
atomic choice is represented in this case by the triple (Ci, () j,  k),  where () j  is 
a grounding Substitution and k  E  {0, 1, ... , ni}· An atomic choice represents 
an equation of the form Xij  =  k  where Xij  is a random variable associated 
with Ci()j·  The definition of consistent set of atomic choices, of composite 
choices, and of the probability of a composite choice is the same as for 
ProbLog. Again, a selection  0'  is a total composite choice ( one atomic choice 
for each grounding of each probabilistic clause ). A selection 0'  identifies a 
logic program Wu  (a world)  that contains the normal clauses obtained by 
selecting head atom hik ()  for each atomic choice (Ci, (),  k): 
Wu  =  {   
(hik  +-- bi1 1 • • ·, bimJBI(Ci, (),  k)  E  0'1 
Ci  =  hil  : rril; ... ; hini  : IIini  +-- bil,  ... '  biffiil Ci  E  P}  
As for Prob Log, the probability of Wu  is P( Wu)  =  P( (}')  =  n(C· ().  k)Eu  IIiko  
~' J'  
the set of worlds Wp  = {w1, ... , wm} is finite, and P(wu)  is a distribution 
over worlds. 
If q  is a query, we can define P(qlw)  as for ProbLog and again the 
probability of q  is given by Equation (2.1)  

52 
Probabilistic  Logic  Programming  Languages  
Example  19  (Medical symptoms - worlds - LPAD). The  LPAD  of Exam­
ple  13  has Jour  worlds:  
W1  = { 
sneezing(bob)  ~ fiu(bob).  
sneezing(bob)  ~ hayJever(bob).  
fiu(bob).  hayJever(bob).  
}  
P(w1)  = 0.7 x 0.8 
W2 = { 
null~ fiu(bob).  
sneezing(bob)  ~ hayJever(bob).  
fiu(bob).  hayJever(bob).  
}  
P( W2) = 0.3 X 0.8 
W3  =  {  
sneezing(bob)  ~ fiu(bob).  
null~ hayJever(bob).  
fiu(bob).  hayJever(bob).  
}  
P(w3)  =  
0.7 x 0.2 
W4  =  {  
null~ fiu(bob).  
null~ hayJever(bob).  
fiu(bob).  hayJever(bob).  
}  
P( W4)  =  
0.3 X  0.2 
sneezing(bob)  is  true  in  three  worlds  and  its  probability  is  
P(sneezing(bob))  =  0.7 x 0.8 +  0.3 x 0.8 +  0.7 x 0.2 =  0.94 
2.3  Examples  of  Programs  
In this section, we provide some examples of programs to better illustrate the 
syntax and the semantics. 

2.3  Examples  of Programs  53 
Example  20  (Detailed medical symptoms - LPAD). The  following  LPAD2  
models  a  program  that  describe  medical  symptoms  in  a  way  that  is  slightly  
more  elaborated  than  Example  13:  
strong_sneezing(X)  : 0.3; moderate_sneezing(X)  : 0.5 ~ 
fiu(X).  
strong_sneezing(X)  : 0.2; moderate_sneezing(X)  : 0.6 ~ 
hayJever(X).   
fiu(bob).   
hayJever(bob).   
Here  the  clauses  have  three  alternatives  in  the  head  of which  the  one  asso­
ciated  with  atomnull  is  left  implicit.  This  program  has  nine  worlds,  the  query  
strong_sneezing(bob)  is  true  in  Jive  of  them,  and  P(strong_sneezing  
(bob))  = 0.44. 
Example  21  (Coin- LPAD). The  coin  example  of [Vennekens  et  al.,  2004]  
is  represented  as3:  
heads(Coin)  : 1/2; tails(Coin)  : 1/2 ~  
toss(Coin),  "'biased(Coin).   
heads(Coin)  : 0.6; tails(Coin)  : 0.4 ~  
toss(Coin),  biased(Coin).   
fair(Coin)  : 0.9; biased(Coin)  : 0.1.  
toss(coin).   
The  jirst  clause  states  that,  if we  toss  a  coin  that  is  not  biased,  it  has  equal  
probability  of  landing  heads  and  tails.  The  second  states  that,  if the  coin  is  
biased,  it has a slightly higher probability  of landing heads.  The third states  
that  the  coin  is  fair  with  probability  0.9  and  biased  with  probability  0.1  and  
the  last  clause  states  that  we  toss  the  coin  with  certainty.  This  program  has  
eight worlds,  the query heads( coin) is true infour  of them,  and its probability  
is  0.51.  
Example  22 (Eruption- LPAD). Consider  this  LPAD4  from  [Riguzzi  and  
Di  Mauro,  2012]  that  is  inspired  by  the  morphological  characteristics  ofthe  
Italian  island  of Stromboli:  
2  https://cplint.eu/e/sneezing.pl  
3 https://cplint.eu/e/coin.pl  
4 https://cplint.eu/e/eruption.pl  

54  Probabilistic  Logic  Programming  Languages  
C1 = eruption  : 0.6 ; earthquake  : 0.3 :- sudden_energy_release,  
fault_rupture(X). 
c2 = sudden_energy_release:  0.7. 
c3  = fault_rupture(southwest_northeast). 
c4 = fault_rupture(east_west).  
The  island of StrombaZi is located at the  intersection  of two  geological faults,  
one  in  the  southwest-northeast  direction,  the  other  in  the  east-west  direc­
tion,  and  contains  one  of  the  three  volcanoes  that  are  active  in  Italy.  This  
program  models  the  possibility  that  an  eruption  or  an  earthquake  occurs  at  
Stromboli.  If there  is  a  sudden  energy  release  under  the  island  and  there  is  a  
fault  rupture,  then  there  can  be  an  eruption  of the  volcano  an  the  island  with  
probability  0.6  or  an  earthquake  in  the  area  with  probability  0.3.  The  energy  
release  occurs  with  probability  0.7  and  we  are  sure  that  ruptures  occur  in  
both  faults.  
Clause  C1 has  two  groundings,  C1fh  with  
fh  =  {X/ southwest_northeast}  
and  C1()2 with  
()2 =  {Xjeast_west},  
while  clause  C2has  a  single  grounding  C20.  Since  C1has  three  head  atoms  
and  C2 two,  the  program  has  3  X 3  X 2  WOrlds.  The  query  eruption  is  true  
infive  ofthem  and  its  probability  is  P(eruption)  =  0.6 · 0.6 · 0.7 +  0.6 · 0.3 · 
0.7 +  0.6. 0.1. 0.7 +  0.3. 0.6. 0.7 +  0.1. 0.6. 0.7 =  0.588. 
Example  23 (Monty Hall puzzle -
LPAD). The  Monty  Hall  puzzle  
[Baral  et  al.,  2009]  refers  to  the  TV  game  show  hosted  by  Monty  Hall  in  
which  a  player  has  to  choose  which  ofthree  closed  doors  to  open.  Behind  one  
door  there  is  a  prize,  while  behind  the  other  two  there  is  nothing.  Once  the  
player  has  selected  the  door,  Monty  Hall  opens  one  of the  remaining  closed  
doors  which  does  not  contain  the  prize,  and  then  he  asks  the  player  if  he  
would  like  to  change  his  door  with  the  other  closed  door  or  not.  The  problern  
of this  game  is  to  determine  whether  the  player  should  switch.  The  following  
program  provides  a  solution5.  The  prize  is  behind  one  ofthe  three  doors  with  
the  same  probability:  
prize(1)  : 1/3; prize(2)  : 1/3; prize(3)  : 1/3. 
The  player  has  selected  door  1:  
selected(1).  
5 https://cplint.eule/monty.swinb 

2.3  Examples  of Programs  55 
Monty  opens  door  2  with  probability  0.5  and  door  3  with  probability  0.5  ifthe  
prize  is  behind  door  1:  
open_door(2)  : 0.5; open_door(3)  : 0.5 ~ prize(1).  
Monty  opens  door  2  ifthe  prize  is  behind  door  3:  
open_door(2)  ~ prize(3).  
Monty  opens  door  3  ifthe  prize  is  behind  door  2:  
open_door(3)  ~ prize(2).  
The  player  keeps  his  choice  and  wins  if he  has  selected  a  door  with  the  prize:  
win_keep  ~ prize(1).  
The  player  switches  and  wins  if the  prize  is  behind  the  door  that  he  has  not  
selected  and  that  Monty  did  not  open:  
win_switch  ~ prize(2),  open_door(3).  
win_switch  ~ prize(3),  open_door(2).  
Querying  win_keep  and  win_switch  we  obtain  probability  1/3  and  2/3  re­
spectively,  so  the  player  should  switch.  Note  that  ifyou  change  the  probability  
distribution  of Monty  selecting  a  door  to  open when  the  prize  is  behind  the  
door  selected  by  the  player,  then  the  probability  of  winning  by  switching  
remains  the  same.  
Example  24  (Three-prisoner puzzle- LPAD). The  following  program6  from  
[Riguzzi  et  al.,  2016a]  encodes  the  three-prisoner  puzzle.  In  [ Grünwald  and  
Halpern,  2003 ],  the  problern  is  described  as:  
Ofthree  prisoners  a,  b,  and  c, two  aretobe  executed,  but  a  does  not  
know  which.  Thus,  a  thinks  that  the  probability  that  i  will  be  exe­
cuted  is  213  for  i  E {  a,  b,  c}. He  says  to  the  jailer,  "Since  either  b  or  
c is  certainly  going  to  be  executed,  you  will  give  me  no  information  
about  my  own  chances  if you  give  me  the  name  of one  man,  either  
b  or  c, who  is  going  tobe  executed."  But  then,  no  matter  what  the  
jailer  says,  naive  conditioning  leads  a  to  believe  that  his  chance  of  
execution  went  down  from  2/3  to  1/2.  
Each  prisoner  is  safe  with  probability  113:  
safe(a)  : 1/3; safe(b)  : 1/3; safe(c)  : 1/3. 
lf a  is  safe,  the  jailer  tells  that  one  of  the  other  prisoners  will  be  executed  
uniformly  at  random:  
tell_executed(b)  : 1/2; tell_executed(c)  : 1/2 ~ safe(a).  
Otherwise,  he  tells  that  the  only  unsafe  prisoner  will  be  executed:  
6 https://cplint.eu/e/jail.swinb 

56 Probabilistic  Logic  Programming  Languages  
tell_executed(b)  +--- safe(c).  
tell_executed(c)  +--- safe(b).  
The  jailer  speaks  if he  tells  that  somebody  will  be  executed:  
tell  +--- tell_executed(_).  
where  _ indicates  a  distinct  anonymaus variable, i.e.,  a  variable  that  is  there  
as a placeholder and for  which  we  don 't care  about its  value.  
a  is  safe  after  the  jailer  utterance  if he  is  safe  and  the  jailer  speaks:  
safe_after _tell  : -safe(a),  tell.  
By  computing  the  probability  of safe(a)  and  safe_after _tell,  we  get  the  same  
probability  of 1/3,  so  the  jailer  utterance  does  not  change  the  probability  of a  
of being  safe.  
We  can  see  this  also  by  considering  conditional  probabilities:  the  proba­
bility  of safe  (a)  given  the  jailer  utterance  tell  is  
P(safe(a)itell)  =  P(safe(a),  tell)  =  P(safe_after  tell)  _ 1/3 
P(tell)  
P(tell)  
- -1 =  1/3 
because  the  probability  oftell  is  1.  
Example  25  (Russian roulette with two guns- LPAD). The  following  exam­
ple7  models  a  Russian  roulette  game  with  two  guns  [Baral  et  al.,  2009 ].  The  
death  of the  player  is  caused  with  probability  1/6  by  triggering  the  left  gun  
and  similarly  for  the  right  gun:  
death:  1/6 +--- pull_trigger(left_gun).   
death:  1/6 +--- pull_trigger(right_gun).   
pull_trigger( lejt_gun).   
pull_trigger( rig ht_gun).   
Querying  the  probability  of  death  we  get  the  probability  of  the  player  of  
dying.  
Example  26  (Mendelian rules of inheritance - LPAD). Blockeel  [2004]  
presents  a  program8  that  encodes  the  Mendelian  rules  of inheritance  of the  
color  of pea  plants.  The  color  of a  pea  plant  is  determined  by  a  gene  that  
exists  in  two  forms  (alleles),  purple,  p,  and  white,  w.  Each  plant  has  two  
alleles  for  the  color  gene  that  reside  on  a  pair  of chromosomes.  cg(X,  N,  A)  
indicates  that  plant  X  has  allele  A  on  chromosome  N.  The  program  is:  
color(X,  white)  +--- cg(X,  1, w),  cg(X,  2, w).   
color(X,purple)  +--- cg(X,  _A,p).   
7 https://cplint.eu/e/trigger.pl  
8 https://cplint.eu!e/mendel.pl  

2.3  Examples  of Programs  57  
cg(X,  1, A)  :  0.5; cg(X,  1, B)  :  0.5 +­
mother(Y,  X),  cg(Y,  1, A),  cg(Y,  2, B).  
cg(X,  2, A)  :  0.5; cg(X,  2, B)  :  0.5 +­
father(Y,  X),  cg(Y,  1, A),  cg(Y,  2, B).  
mother(m,  c). 
father(f,  c). 
cg(m,  1, w).  cg(m,  2, w).  cg(f,  1,p). cg(f,  2, w).  
The  facts  of the  program  express  that  c is  the  offspring  of m  and  f  and  that  
the  alleles  of m  are  ww  and  of  f  are  pw.  The  disjunctive  rules  encode  the  
fact  that  an  offspring  inherits  the  allele  an  chromosome  1  from  the  mother  
and  the  allele  an  chromosome  2  from  the  father.  In  particular,  each  allele  of  
the  parent  has  a  probability  of 50%  of being  transmitted.  The  definite  clauses  
for  color  express  the  fact  that  the  color  of a  plant  is  purple  if at  least  one  of  
the  alleles  is  p,  i.e.,  that  the  p  allele  is  dominant.  In  the  second  definite  clause,  
_A  indicates  an  anonymaus  variable.  In  a  similar  way,  the  rules  of blood  type  
inheritance  can  be  written  in  an  LPAD9.  
Example  27 (Path probability- LPAD). An  interesting  application  of PLP  
under  the  DS  is  the  computation  of  the  probability  of  a  path  between  two  
nodes  in  a  graph  in  which  the  presence  of  each  edge  is  
probabilistic10:  
path(X,  X).  
path(X,  Y)  +-- path(X,  Z),  edge(Z,  Y).  
edge(a,  b)  : 0.3. 
edge(b,  c) : 0.2. 
edge(a,  c) : 0.6. 
This  program,  coded  in  ProbLog,  was  used  in  [De  Raedt  et  al.,  2007]  for  
computing  the  probability  that  two  biological  concepts  are  related  in  the  
BIOMINE  network  [Sevon  et  al.,  2006].  
PLP under the DS can encode BNs [Vennekens et al., 2004]: each value of 
each random variable is encoded by a ground atom, each row of each CPT is 
encoded by a rule with the value of parents in the body and the probability 
distribution of values of the child in the head. 
Example  28 (Alarm BN- LPAD). The  BN of Example  10,  that  we  repeat  in  
Figure  2.1  for  readability,  can  be  encoded  with  the  program11  
9 https://cplint.eu/e/bloodtype.pl  
10 https://cplint.eu/e/path.swinb  
11 https://cplint.eu/e/alarm.pl  

58 
Probabilistic  Logic  Programming  Languages  
I  bur,g I  t I  f I  I  earthquake I  
0~2 I  
0~8 I 
0.1 
0.~ 
1.0 
0.0 
0.2
cb I  
t 
f 
call 
t 
f 
--~· I  0.8 
l  b=f, e=t 1  0.8 
0.2 
a-t 
0.9 
0.1 
l  b=f, e=r 1  o.1 
0.9
a-f 
0.05 
0.95 
Figure  2.1  Example of a BN. 
burg(t)  : 0.1; burg(f)  : 0.9. 
earthquake(t)  : 0.2; earthquake(f)  : 0.8. 
alarm(t)  ~ burg(t),  earthq(t).  
alarm(t)  : 0.8; alarm(f)  : 0.2 ~ burg(t),  earthq(f).  
alarm(t)  : 0.8; alarm(f)  : 0.2 ~ burg(f),  earthq(t).  
alarm(t)  : 0.1; alarm(f)  : 0.9 ~ burg(f),  earthq(f).  
call(t)  : 0.9; call(f)  : 0.1 ~ alarm(t).  
call(t)  : 0.05; call(f)  : 0.95 ~ alarm(f).  
2.4  Equivalence  of  Expressive  Power  
To show that all these languages have the same expressive power, we discuss 
transformations 
among 
probabilistic 
constructs 
from 
the 
various 
languages. 
The mapping between PHAIICL and PRISM translates each PHAIICL 
disjoint statement to a multi-switch declaration and vice versa in the obvious 
way. The mapping from PHAIICL and PRISM to LPADs translates each dis­
joint 
statement/multi-switch 
declaration 
to 
a 
disjunctive 
LPAD fact. 
The translation from an LPAD into PHAIICL (first shown in [Vennekens 
and Verbaeten, 2003]) rewrites each clause Ci  with v  variables X  
h1: Ih; ... ; hn:  IIn ~ B.  
into PHAIICL by adding n  new predicates { choiceiljv,  ... , choicein/v}  and 
a disjoint statement: 

2.4  Equivalence  of Expressive  Power  59 
h1 +--- B,choiceil(X).  
hn  +--- B,  choicein(X).  
disjoint([choiceil(X):  Ih, ... ,choicein(X):  IIn]).  
For instance, the first clause of the medical symptoms LPAD of Example 20 
is translated to 
strong_sneezing(X)  +--- fiu(X),choicen(X).  
moderate_sneezing(X)  +--- fiu(X),  choice12(X). 
disjoint([choicen(X)  : 0.3, choice12(X) : 0.5, choice13 : 0.2]). 
where the clause null+--- flu(X),  choice13· is omitted since null  does not 
appear in the body of any clause. 
Finally, as shown in [De Raedt et al., 2008], to convert LPADs into 
ProbLog, each clause Ci  with v  variables X  
h1 : II1 ; ... ; hn  : IIn  +--- B.  
is translated into ProbLog by adding n  - 1 probabilistic facts for predicates 
{fil/v,  · · ·, fin/v}:  
h1 +--- B,  fil(X).   
h2 +--- B,  "'fi1(X), !i2(X).  
hn  +--- B,  "'fil(X),  ... ,  "'fin-l(X).  
1r1 :: fil(X).  
1fn-1  ::  fin-l(X).  
where 
1r1 = Ih 
rr2 
7r2 = l-7rl 
rr3 
7r3 =  (1-7rl)(l 7r2) 
In  general 
rri 
1ri =  fli.-1 (1- 7rj)
J=l 

60 
Probabilistic  Logic  Programming  Languages  
Note that while the translation into ProbLog introduces negation, the in­
troduced negation involves only probabilistic facts, and so the transformed 
program will have a two-valued model whenever the original program does. 
For instance, the first clause of the medical symptoms LPAD of 
Example 20 is translated to 
strong_sneezing(X)  ~ fiu(X),  fn(X).  
moderate_sneezing(X)  ~ fiu(X),  ~Ju(X), !I2(X). 
0.3 :: !u(X).   
0.714285 :: !I2(X).  
2.5 Translation  into  Bayesian  Networks  
We discuss here how an acyclic ground LPAD can be translated to a BN. 
Let us first define the acyclic property for LPADs, extending Definition 4. 
An LPAD is acyclic  if an integer level can be assigned to each ground atom 
so that the level of each atom in the head of each ground rule is the same and 
is higher than the level of each atom in the body. 
An acyclic ground LPAD P  can be translated to a BN ß(P)  [Vennekens 
et al., 2004]. ß(P)  is built by associating each atom a  in ßp  with a binary 
variable a  with values true (1) and false (0). Moreover, for each rule Ci  ofthe 
following form 
h1 : Ih; ... ; hn:  IIn  ~ b1, ... bm,  ~q.... , ~cz 
in ground(P),  we add a new variable chi (for "choice for rule C/')  to ß(P).  
chi has b1, ... , bm,  q,  ...  ,  cz  as parents. The values for chi are h1, ..., hn  
and null,  corresponding to the head atoms. The CPT of chi is 
. . .  
b1 = 1, ... , bm  = 1, Cl = 0, ... , Ct = 0 
...  
chi = h1 
0.0 
Ih 
0.0 
...  
chn = hn  
0.0 
IIn  
0.0 
chi =null  1.0 
1- .l:~- 1 rri 
1.0 
that can be expressed as 
l-
Ih 
if eh; =  hk,  b1 =  1, o 
o o, q =  0
ifch; =  null,b1  =  1, 
P(ch;lbl, ... 'cl)  =  
.l:7=1 IIj 
0 
0 
0 ,cl  =  0
(2o7)
{ 
ifch; =  null,~(b1 =  1, 0 
0 
0 ,q =  0)
otherwise 

2.5  Translation  into  Bayesian  Networks  61 
If the body is empty, the CPT for chi is 
chi = h1 
Ih 
...  
chn = hn  
IIn  
chi =null  1- 2:~'•=1 II· • 
Moreover, for each variable a  corresponding to atom a  E  ß p,  the parents are 
all the variables chi of rules Ci  that have a  in the head. The CPT for a  is the 
following deterministic table: 
At least one parent equal to a  Remaining columns 
a  = 1  
1.0 
0.0 
a=O  
0.0 
1.0 
encoding the function 
a  = f(cha)  = {  1 if:Jchi E  cha: chi = a  
0 otherwise 
where cha  are the parents of a.  Note that in order to convert an LPAD con­
taining variables to a BN, its grounding must be generated. 
Example  29  (LPAD to BN). Consider  the  following  LPAD  P:  
C1  
a1  :  0.4 ; a2  :  0.3. 
c2  
a2  :  0.1; a3  : 0.2. 
c3 
a4  : 0.6; as  : 0.4 ~ al. 
c4 
as  : 0.4 ~ a2,  a3.  
Cs 
a5  : 0.3; a7  : 0.2 ~ a2,  as.  
Its corresponding network ß(P)  is shown in Figure 1.7, where the CPT for 
a2  and chs are shown in Tables 2.1 and 2.2 respectively. 
Table  2.1  Conditional probability table for a 2 , n  stands for null  
ch1, ch2 
a1, a2 
a1,a3  
a1,n,  
a2,a2 
a2,a3  
a2,n  n,a2  
n,a3  n,n  
a2 = 1  
1.0 
0.0 
0.0 
1.0 
1.0 
1.0 
1.0 
0.0 
0.0 
a2 = 0 
0.0 
1.0 
1.0 
0.0 
0.0 
0.0 
0.0 
1.0 
1.0 
An alternative translation !(P)  for a ground program Pis  built by includ­
ing random variables a  for each atom a  in ßp  and chi for each clause Ci  as 
for ß(P).  Moreover, !(P)  includes the Boolean random variable bodyi and 
the random variable Xi with values h1, ..., hn  and null  for each clause Ci.  
The parents of bodyi are b1, ... , bm,  and c1, ... , cz  and its CPT encodes 
the deterministic AND Boolean function: 

62 
Probabilistic  Logic  Programming  Languages  
Figure  2.2 BN ß(P)  equivalent to the program ofExample 29. 
Table  2.2 
Conditional probability table for chs 
a2, as  
1,1 
1,0 
0,1 
0,0 
chs = x6  
0.3 
0.0 
0.0 
0.0 
chs = x7  
0.2 
0.0 
0.0 
0.0 
chs =null  0.5 
1.0 
1.0 
1.0 
...  
b1  = 1, ... , bm  = 1, c1  = 0, ... , cz  = 0 
. ..  
bodyi = 0 
1.0 
0.0 
1.0 
bodyi = 1 
0.0 
1.0 
0.0 
If the body is empty, the CPT makes bodyi surely true 
bodyi = o 1  o.~ 1  
bodyi = 1 
Xi has no parents and has the CPT 
chi = h1 
rrl 
...  
chi = hn  
IIn  
chi =null  1- .2:~'•=1 II " 
chi has Xi and bodyi as parents with the deterministic CPT 
bodyi, xi 
O,h1 
...  O,hn  
0, null  1, hl 
. ..  1,hn  1, null  
chi = h1 
0.0 
...  
0.0 
0.0 
1.0 
. ..  
0.0 
0.0 
. . .  
chi = hn  
0.0 
...  
0.0 
0.0 
0.0 
. ..  
1.0 
0.0 
chi =null  
1.0 
...  
1.0 
1.0 
0.0 
. ..  
0.0 
1.0 

2.5  Translation  into  Bayesian  Networks  63 
Figure  2.3 
Portion of !(P)  relative to a clause Ci.  
encoding the function 
xi 
ifbodyi = 1 
chi=f(bodyi,Xi)= { null  ifbodyi=O 
The parents of each variable a  in !(P)  are the variables chi of rules Ci  that 
have a  in the head as for ß(P),  with the same CPT as in ß(P).  
The portion of !(P)  relative to a clause Ci  is shown in Figure 2.3. 
If we compute P  (chi I b1, ... , bm,  c1, ... , cz)  by marginalizing 
P(chi,  bodyi, Xilb1, ... , bm,  ct, ... , ez)  
we can see that we obtain the same dependency as in ß(P):  
P(chilb1, ... ,cz) = 
= _2:  _2:  P(chi,bodyi,xilbl, ... ,cz) 
x, body, 
= _2:  _2:  P(chilbodyi, xi)P(xi)P(bodyilb1, ... , cz) 
Xi  body, 
= _2:  P(xi)  _2:  P(chilbodyi, xi)P(bodyilb1, ... , cz) 
Xi  
body, 
1 ifbodyi = 1,b1 = 1, ... ,cz = 0 
= _2:  P(xi)  _2:  P(chilbodyi,xi) 
1 ifbodyi = 0, -,(b1 = 1, ... , cz = 0)
{
x, 
body, 
0 otherwise 
1 if chi = xi, bodyi = 1, b1 = 1, ... , cz = 0  
= _2:  P(xi)  _2:  
1 if chi = null,  bodyi = 0, -,(b1 = 1, ... , cz = 0) 
{
x, 
body, 
0 otherwise 

64 
Probabilistic  Logic  Programming  Languages  
Figure  2.4  BN !(P)  equivalent to the program ofExample 29. 
1 if chi = xi, b1 = 1, ... , cz = 0  
= Lx, P(xi) 
1 if chi = null,  ~(bl = 1, ... , Cz = 0) 
{ 0 otherwise 
Ih 
ifchi = hk,bl  = 1, ... ,cz = 0 
=  
1 - .2:7=1 IIj  ~f chi : null,~ = : ... ,c1  = ~  
1  
1f eh, - null,  (b1 -
1,  ...  ,  cz - 0) 
{ 
0 
otherwise 
which is the same as Equation (2.7). 
From Figure 2.3 and using d-separation (see Definition 17), we can see 
that the Xi variables are all pairwise unconditionally independent as between 
every pair there is the collider Xi ~ chi ~ bodyi. 
Figure 2.4 shows !(P)  for Example 29. 
2.6  Generality  of  the  Distribution  Semantics  
The assumption of independence of the random variables associated with 
ground clauses may seem restrictive. However, any probabilistic relationship 

2.6  Generality  ofthe  Distribution  Semantics  65 
~ 
@   
P(b( i)Ia( i)}  
a(i) 
0 
1  
b{i) =  0 
1-P2 
l-p3 
b{i) =  1 
P2 
P3 
Figure  2.5 BN representing the dependency between a( i) and b( i). 
between Boolean random variables that can be represented with a BN can be 
modeled in this way. For example, suppose you want to model a general 
dependency between the ground atoms a( i) and b( i) regarding predicates a/1 
and b/1  and constant i. This dependency can be represented with the BN of 
Figure 2.5. 
The joint probability distribution P( a( i), b( i)) over the two Boolean ran­
dom variables a( i) and b( i) is 
P(O,  0) 
(1- Pl)(1- P2)  
P(O,  1)  
(1- pl)p2   
P(1,  0)  
Pl(1-p3)  
P(1,  1)  
PlP3   
This dependency can be modeled with the following LPAD P: 
C1 = a(i)  :  Pl 
C2 = b(X)  :  P2 ~ a(X)  
C3 = b(X)  :  P3  ~ ~a(X) 
We can associate Boolean random variables X1 with C1, X2, with C2{X/i}, 
and X3 with C3{Xji}, where X1, X2, and X3 are mutually independent. 
These three random variables generate eight worlds. --.a( i) 1\  --.b( i) for ex­
ample is true in the worlds 
whose probabilities are wl =  0,  w2 =  {b(i) ~ a(i)}  
P'(w1)  
(1- Pl)(1- P2)(1- P3)  
P'(w2)  
(1- Pl)(1- P2)P3  
so 
P' (--.a( i), --.b( i)) =  (1-Pl) (1-p2) (1-p3) +  (1-Pl) (1-p2)P3 =  P(O,  0). 

66 
Probabilistic  Logic  Programming  Languages  
P"(X2) 
I  x2 =  nu.n ll -P2  I 
x2 =  b(z)  
P2  
Figure  2.6 BN modelillg the distributioll over a(i),  b(i),  X1, X2, X3. 
We can prove similarly that the distributiolls P  and P'  coillcide for all joillt 
states of a(i)  and b(i).  
Modelillg the depelldellcy betweell a( i) and b( i) with the program above 
is equivalellt to represellt the BN of Figure 2.5 with the lletwork r(P)  of 
Figure 2.6. 
Sillce r(P)  defilles the same distributioll as P,  the distributiolls P  and 
P",  the olle defilled by r(P),  agree Oll the variables a(i)  and b(i),  i.e., 
P(a(i),  b(i)) =  P"(a(i),  b(i)) 
for ally value of a(i)  alld b(i).  FromFigure 2.6, itis also clearthatX1, X2, and 
X3 are mutually UllCOllditiollally illdepelldellt, thus showillg that it is possible 
to represellt any depelldellcy with illdepelldellt random variables. So we call 
model gelleral depelldellcies amollg groulld atoms with the DS. 
This collfirms the results of Sectiolls 2.3 alld 2.5 that graphical models call 
be translated illto probabilistic logic programs ullder the DS alld vice versa. 
Therefore, the two formalisms are equally expressive. 
2. 7  Extensions  of  the  Distribution  Semanti es  
Programs ullder the DS may colltaill flexible  probabilities  [De Raedt and 
Kimmig, 2015] or probabilities that depelld Oll values computed durillg 

2.  7  Extensions  of the  Distribution  Semanti es  67  
program execution. In this case, the probabilistic annotations are variables,  
as in the program 12 from [De Raedt and Kimmig, 2015]  
red (Prob ) : Prob .  
draw_red (R, G) :­
Prob is R/ (R + G), 
red (Prob). 
The query draw_red ( r , g ), where r and g are the number of green and 
red balls in an um, succeeds with the same probability as that of drawing a 
red ball from the um. 
Flexible probabilities allow the computation of probabilities on the fty 
during inference. However, flexible probabilities must be ground when their 
value is evaluated during inference. Many inference systems support them by 
imposing constraints on the form of programs. 
The body of rules may also contain literals for a meta-predicate such as 
probI  2 that computes the probability of an atom, thus allowing nested or 
meta-probability computations [De Raedt and Kimmig, 2015]. Among the 
possible uses of such a feature De Raedt and Kimmig [2015] mention: fil­
tering proofs on the basis of the probability of subqueries, or implementing 
simple forms of combining rules. 
An example of the first use is 13 
a : 0.2:­
prob (b , P ) , 
P>O.l . 
where a succeeds with probability 0.2 only if the probability of b is larger 
than 0.1. 
An example of the latter is 14  
p  (P )  : P .  
max_true (Gl , G2 ) 
prob (Gl , Pl ), 
prob (G2 , P2 ), 
max (Pl , P2 , P ), p (P ). 
where ma x_ true (Gl , G2 ) succeeds with the success probability ofits more 
likely argument. 
12  https://cplint.eu/e/flexprob.pl  
13 https://cplint.eu/e/meta.pl  
14  https://cplint.eu/e/metacomb.pl  

68 
Probabilistic  Logic  Programming  Languages  
2.8  CP-Iogic  
CP-logic [Vennekens et al., 2009] is a langnage for representing causallaws. 
It  shares many similarities with LPADs but specifically aims at modeling 
probabilistic causality. Syntactically, CP-logic  programs,  or CP-theories,  are 
identical to LPADs15 : they are composed of annotated disjunctive clauses. 
Foreach grounding 
h1 : Ih ; ... ; hm  : IIn ~ B  
of a clause of the pro gram, B  represents an event whose effect is to cause at 
most one of the hi  atoms to become true and the probability of hi  of being 
caused is Ili. Consider the following medical example. 
Example  30  (CP-logic program - infection [Vennekens et al., 2009]). A  
patient  is  infected  by  a  bacterium.  Infection  can  cause  either  pneumonia  
or  angina.  In  turn,  angina  can  cause  pneumonia  and  pneumonia  can  cause  
angina.  This  can  be  represented  by  the  CP-logic  program:  
angina  : 0.2 ~ pneumonia.  
(2.8) 
pneumonia  : 0.3 ~ angina.  
(2.9) 
pneumonia  : 0.4; angina  : 0.1 ~ infection.  
(2.10) 
infection.  
(2.11) 
The semantics of CP-logic programs is given in terms of probability trees 
that represent the possible courses of the events encoded in the program. We 
consider first the case where the program is positive, i.e., the bodies of rules 
do not contain negative literals. 
Definition  19  (Probability tree- positive case). A  probability tree16 T  for  
a  program  P  is  a  tree  where  every  node  n  is  labeled  with  a  two-valued  
interpretation  I(n)  and  a  probability  P(n).  T  is  constructed  asfollows:  
•  The  root  node  r  has  probability  P(r)  =  
1.0  and  interpretation  
I(r)  =  0.  
• Each  inner  node  n  is  associated  with  a  ground  clause  Ci  such  that  
- no  ancestor  of n  is  associated  with  Ci,  
- all  atoms  in  body(Ci)  are  true  in  I(n),  
15 There are versions of CP-logic that have a more general syntax but they arenot essential 
for the discussion here 
16  We follow here the definition of [Shterionov et al., 2015] for its simplicity. 

2.8  CP-logic  69 
n  has  one  child  node  for  each  atom  hk  E  head( Ci)· The  k-th  child  has  
interpretation  I(n)  u  {  hk}  and probability  P(n)  · IIk.  
• No  leaf can  be  associated with  a  clause following  the  rule  above.  
A probability tree defines a probability distribution P(I)  over the interpreta­
tion of the program P: the probability of an interpretation I  is the sum of the 
probabilities of the leaf nodes n  such that I  = I  ( n) . 
The probability tree for Example 2.11 is shown in Figure 2.7. The proba­
bility distribution over the interpretations is 
{inf ,pn,  ang}  
{inf,  ang}  
0.11 
0.07 
There can be more than one probability tree for a program but Vennekens 
et al. [2009] show that all the probability trees for the program define the 
same probability distribution over interpretations. So we can speak of the  
probability tree for P  and this defines the semanti es of the CP-logic pro gram. 
Moreover, each program has at least one probability tree. 
Vennekens et al. [2009] also show that the probability distribution de­
fined by the LPADs semantics is the same as that defined by the CP-logic 
semantics. So probability trees represent an alternative definition of the DS 
forLPADs. 
If the program contains negation, checking the truth of the body of a 
clause must be made with care because an atom that is currently absent from 
I  ( n)  may become true later. Therefore, we must make sure that for each 
0 
Cla~t 2.11 
nA  
n<
~~~~tnf:;~------.___ 
~ 0.1 
~ 
{inf) 
{inf,pn)  
{inf,ang)  
0.5 
, y(ause~ 
0 
y(ause~ 
0
,
0;1  
~8 
0.!/ 
~.7 
{inf,  pn,  ang)  
{inf,pn)  
{inf,  ang,  pn)  
{inf,  ang)  
0.08 
0.32 
0.03 
0.07 
Figure  2.7 Probability tree for Example 2.11. From [Vennekens et al., 
2009]. 

70  Probabilistic  Logic  Programming  Languages  
negative literal ~ a  in body  (Ci), the positive literal a  cannot be made true 
starting from I  ( n).  
Example  31  (CP-logic program - pneumonia [Vennekens et al., 2009]). A  
patient  has  pneumonia.  Because  of pneumonia,  the  patient  is  treated.  lf the  
patient  has  pneumonia  and  is  not  treated,  he  may  get  fever.  
pneumonia.  
(2.12) 
treatment  : 0.95 ~ pneumoma.  
(2.13) 
fever  : 0. 7 ~ pneumonia,  ~treatment. 
(2.14) 
Two  probability  trees  for  this  program  are  shown  in  Figures  2.8  and  2.9.  Both  
trees  satisfy  Definition  19  but  define  two  different  probability  distributions.  
In  the  tree  of Figure  2.8,  Clause  2.14  has  negative  literal  ~treatment in  its  
body  and  is  applied  at  a  stage  where  treatment  may  still  become  true,  as  
happens  in  the  level  below.  
In  the  tree  of  Figure  2.9,  instead  Clause  2.14  is  applied  when  the  only  
rule  for  treatment  has  already  fired,  so  in  the  right  child  of the  node  at  the  
second  leveltreatmentwill  never  become  true  and  Clause  2.14  can  safely  
be  applied.  
In  order to formally define this, we need the following definition that uses 
three-valued logic. A conjunction in three-valued logic is true or undefined if 
no literal in it is false. 
Definition  20  (Hypothetical derivation sequence). A  hypothetical derivation 
sequence in  a  node  n  is  a  sequence  (Ii)O~i~n ofthree-valued  interpretations  
that  satisfy  the  following  properties.  Initially,  Io  assigns  false  to  all  atoms  
0 
CI·~r.l2 
{pn} 
n" ~ause2.~ no 
7  
~ 
{pn,jvr}  
{pn} 
• <)lose~ n 
• <)lose~ n 
0.9;; 
~.05 
0.9;;-
~.05 
{pn,jvr,  tr} 
{pn,jvr} 
{pn, tr} 
{pn} 
0.665 
0.035 
0.0285 
0.015 
Figure  2.8 An incorrect probability tree for Example 31. From [Vennekens 
et al., 2009]. 

2.8  CP-logic  71 
not  in  I(n).  Foreach  i  >  0, Ii+l = <Ir,i+l, JF,i+l) is  obtainedfrom  Ii  = 
<Ir,i,  IF,i)  by  considering  a  rule  R  with  body(R)  true  or  undefined  in  Ii  and  
an  atom  a  in  its  head  that  is  false  in  I.  Then  Ir,i+l = Ir,i+l and  IF,i+l  = 
IF,i+l  \{a }. 
Every hypothetical derivation sequence reaches the same Iimit. For a node n  
in a probabilistic tree, we denote this unique Iimit as I(n).  It  represents the 
set of atoms that might still become true; in other words, all the atoms in the 
false part of I(n)  will never become true and so they can be considered as 
false. 
The definition of probability tree of a program with negation becomes the 
following. 
Definition  21  (Probability tree- general case). A  probability tree T  for  a  
program  P  is  a  tree  
• satisfying  the  conditions  of Definition  19,  and  
• for  each  node  n  and  associated  clause  Ci,  for  each  negative  literal  ~a 
in  body(Ci),  a  E  lp  withi(n)  = <fr,Jp).  
All the probability trees according for the program according to Definition 21 
establish the same probability distribution over interpretations. 
It can be shown that the set of false atoms of the Iimit of the hypothetical 
derivation sequence is equal to the greatest fixpoint of the operator OpFalse[l  
(see Definition 2) with I  = (I(n),  0)  and P  a program that contains, for 
each rule 
h1 : rr1 ; 
;hm:IIn~B 
0 
CI·~r·l2 
nn<
7  ~.~:;.~ 
0.05~ 
{pn, tr} 
{pn} 
0.95 
~ SJtfuse ~4 n
o./ 
~.3 
{pn,.fvr} 
{pn} 
0.035 
0.015 
Figure  2.9 
A probability tree for Example 31. From [Vennekens et al., 
2009]. 

72 
Probabilistic  Logic  Programming  Languages  
of 
the rules 
h1 ~ B.  
hm  ~ B.  
In other words, ifi(n) =(Ir, lp)  and gfp( OpFalsei)  = F,  then lp  = F.  
In fact, for the body of a clause tobe true or undefined in Ii  =  (lr,i,  IF,i).  
each positive literal a  must be absent from IF,i  and each negative literal 
"'a  must be suchthat a  is absent from lr,i.  which are the complementary 
conditions in the definition of the operator OpFalsei (Fa).  
On the other hand, the generation of a child n'  of a node n  using a rule 
Ci  that adds an atom a  to I  (n)  can be seen as part of an application of 
OpTruef(n)·  So there is a strong connection between CP-logic and the WFS. 
In the trees of Figures 2.8 and 2.9, the child n  = {pn} of the root has 
lp  = 0. so Clause 2.14 cannot be applied as treatment  tf; lp  and the only 
tree allowed by Definition 21 isthat of Figure 2.9. 
The semantics of CP-logic satisfies these causality principles: 
• The principle  of universal  causation  states that all changes to the state 
of the domain must be triggered by a causallaw whose precondition is 
satisfied. 
• The principle  of  sufficient  causation  states that if the precondition to 
a causal law is satisfied, then the event that it triggers must eventually 
happen. 
and therefore the logic is particularly suitable for representing causation. 
Moreover, CP-logic satisfies the temporal  precedence  assumption  that 
states that a rule R  will not fire until its precondition is in its final state. In 
other words, a rule fires only when the causal process that determines whether 
its precondition holds is fully finished. This is enforced by the treatment of 
negation of CP-logic. 
There are CP-logic programs that do not admit any probability tree, as the 
following example shows. 
Example  32 (Invalid CP-logic program [Vennekens et al., 2009]). In  a  two­
player  game,  white  wins  if black  does  not  win  and  black  wins  if white  does  
not  win:  
win(white)  ~ rovwin(black).  
(2.15) 
win(black)  ~ rovwin(white).  
(2.16) 
P,  

2.8  CP-logic  73 
At  the  root  of  the  probability  tree  for  this  program,  both  Clauses  2.15  and  
2.16  have  their  body  true  but  they  cannot  fire  as  lp  for  the  root  is  0.  So  
the  root  is  a  leafwhere  however  two  rules  have  their  body  true,  thus  violating  
the  condition  of Definition  19  that  requires  that  leaves  cannot  be  associated  
with  rules.  
This theory is problematic from a causal point of view, as it is impossible to 
define a process that follows the causallaws. Therefore, we want to exclude 
these cases and consider only valid  CP-theories. 
Definition  22 (Valid CP-theory). A  CP-theory  is  valid if  it  has  at  least  one  
probability  tree.  
The equivalence of the LPADs and CP-logic semantics is also carried to 
the general case of programs with negation: the probability tree of a valid 
CP-theory defines the same distribution as that defined by interpreting the 
program as an LPAD. 
However, there are sound LPADs that are not valid CP-theories. Recall 
that a sound LPAD is one where each possible world has a two-valued WFM. 
Example  33 (Sound LPAD - invalid CP-theory Vennekens et al. [2009]). 
Consider  the  program  
p  : 0.5 ; q  : 0.5 +- r.  
r  +-~p. 
r  +-~q. 
Such  a  program  has  no  probability  tree,  so  it  is  not  a  valid  CP-theory.  1ts  
possible  worlds  are  
{p  +- r;  r  +-~p; r  +-~q} 
and  
{q  +- r;  r  +-~p; r  +-~q} 
that  both  have  total  WFMs,  {r,p} and  {r, q}, respectively,  so  the  LPAD  is  
so und.  
In  fact,  it  is  difficult  to  imagine  a  causal  process  forthispro gram.  
Therefore, LPADs and CP-logic have some differences but these arise only 
in comer cases, so sometimes CP-logic and LPADs are used as a synonyms. 
This also shows that clauses in LPADs can be assigned in many cases a causal 
interpretation. 
The equivalence of the semanti es implies that, foravalid CP-theory, each 
leaf of the probability tree is associated with the WFM of the possible world 

74 
Probabilistic  Logic  Programming  Languages  
obtained by considering all the clauses used in the path from the root to the 
leaf with the head selected according to the choice of child. If the program is 
deterministic, the only leaf is associated with the total-weH founded model of 
the program. 
2.9  KBMC  Probabilistic  Logic  Programming  Languages  
In this section, we present three examples of KBMC languages: Bayesian 
Logic Programs (BLPs), CLP(BN), and the Prolog Factor Language (PFL). 
2.9.1  Bayesian  logic  programs  
BLPs [Kersting and De Raedt, 2001] use logic programming to compactly 
encode a large BN. In BLPs, each ground atom represents a (not necessarily 
Boolean) random variable and the clauses define the dependencies between 
ground atoms. A clause ofthe form 
ala1, ... ,am  
indicates that, for each of its groundings (ala1, ... , am)B,  aB  has a1B,  ..., 
amB  as parents. The domains and CPTs for the ground atoms/random vari­
ables are defined in a separate portion of the model. In the case where a 
ground atom aB  appears in the head of more than one clause, a combining  
rule  is used to obtain the overall CPT from those given by individual clauses. 
For example, in the Mendelian genetics program of Example 26, the de­
pendency that gives the value of the color gene on chromosome 1 of a plant 
as a function of the color genes of its mother can be expressed as 
cg(X,l Jlmother(Y,X),cg(Y,l ),cg(Y,2).  
where the domain of atoms built on predicate cg/2  is {p,  w}  and the domain of 
mother(Y,X)  is Boolean. A suitable CPT should then be defined that assigns 
equal probability to the alleles of the mother to be inherited by the plant. 
Various learning systems use BLPs as the representation language: RBLP 
[Revoredo and Zaverucha, 2002; Paes et al., 2005], PFORTE [Paes et al., 
2006], and ScooBY  [Kersting and De Raedt, 2008]. 
2.9.2  CLP(BN)  
In a CLP(BN) program [Costa et al., 2003], logical variables can be random. 
Their domain, parents, and CPTs are defined by the program. Probabilistic 

2.9  KBMC  Probabilistic  Logic  Programming  Languages  75  
dependencies are expressed by means of constraints as in Constraint logic 
programming (CLP): 
Var -
Function with p (Va1ues , Dist ) 
Var = Function with p (Va1ues , Dist , Parents ) 
The firstform indicates that the logical variable Var is random with domain 
Va1ues and CPT Dist but without parents; the second form defines a ran­
dom variable with parents. In both forms, Function is a term over logical 
variables that is used to parameterize the random variable: a different random 
variable is defined for each instantiation of the logical variables in the term. 
For example, the following snippet from a school domain: 
course_ difficu1ty (CKey , Dif ) 
{ Dif = difficu1ty (CKey ) with p ([ h , m, 1 ] , 
[0.25 , 0.50 , 0.25 ] ) }. 
defines the random variable D i  f  with values h, m, and 1 representing the dif­
ficulty of the course identified by CKey. There is a different random variable 
for every instantiation of CKey, i.e., for each course. In a similar manner, the 
intelligence Int of a student identified by SKey is given by 
student_ inte11igence (SKey , Int ) 
{ Int = inte11igence (SKey ) with p ( [h , m, 1 ] , 
[0.5 , 0.4 , 0.1 ]) }. 
Using the above predicates, the following snippet predicts the grade received 
by a student when taking the exam of a course. 
regi strat i on_gr ade (Key , Grade ) 
reg i strat i on (Key , CKey , SKey ), 
course_dif f i cu1ty (CKey , Dif ), 
student_inte11igence (SKey , Int ), 
{ Grade = grade (Key ) with 
p ([ 'A' , 'B' , ' C ' , 'D' ], 
% h /h  h/m  h/1  m/h  
m/m  m/1  1 /h  1 /m  1 / 1  
[ 0.20 , 0.70 , 0.85 , 0.10 , 0.20 , 0.50 , 0.01 , 0.05 , 0.10 , 
% 'A,  
0.60 , 0.25 , 0.12 , 0.30 , 0.60 , 0.35 , 0.04 , 0.15 , 0.40 , 
%  'B,  
0 .15 , 0. 04 , 0. 02 , 0. 40 , 0 .15 , 0 .12 , 0. 50 , 0. 60 , 0. 40 , 
% ' C ' 

76  Probabilistic  Logic  Programming  Languages  
0.05 , 0.01 , 0.01 , 0.20 , 0.05 , 0.03 , 0.45 , 0.20 , 0.10 ) , 
% ' D I   
[Int , Dif )) }.  
Here Grade indicates a random variable parameterized by the identifier Key 
of a registration of a student to a course. The code states that there is a dif­
ferent random variable Grade foreach student's registration in a course and 
each such random variable has possible values 
1 A  
1  
1 B  
1  
1 
, 
,  
c  
1  and 
1 o  
1 
•  The
actual value of the random variable depends on the intelligence of the student 
and on the difficulty of the course, that are thus its parents. Together with 
facts for registration / 3 such as 
registration (r0 , c16 , s0 ) 
registration (r1 , c10 , s0 ) 
registration (r2 , c57 , s0 ) 
registration (r3 , c22 , s1 ) 
the code defines a BN with a Grade random variable for each registration. 
CLP(BN) is implemented as a library of YAP Prolog. The library performs 
query answering by constructing the sub-network that is relevant to the query 
and then applying a BN inference algorithm. 
The unconditional probability of a random variable can be computed by 
simply asking a query to the YAP command line. 
The answer will be a probability distribution over the values of the logical 
variables of the query that represent random variables, as in 
? -
registration_grade (rO , G). 
p (G=a ) =0. 4115 , 
p (G=b ) =0. 356 , 
p (G=c ) =0.16575 , 
p (G=d ) =0.06675 ? 
Conditional queries can be posed by including in them ground atoms repre­
senting the evidence. 
For example, the probability distribution of the grades of registration rO 
given that the intelligence of the student is high (h) is given by 
? -
registration_grade (rO , G), 
student_inte1ligence (s0 , h ). 
p (G=a ) =0.6125 , 
p (G=b ) =0.305 , 
p (G=c ) =0.0625 , 
p (G=d ) =0.02 ? 

2.9  KBMC  Probabilistic  Logic  Programming  Languages  77  
As you can see, the probability of the student receiving grade ' A '  is in­
creased. 
In general, CLP provides a useful tool for PLP, as is testified by the pro­
posals clp(pdf(Y)) [Angelopoulos, 2003, 2004] and Probabilistic Constraint 
Logic Programming [Michels et al., 2015], see Section 4.5. 
2.9.3  The  prolag  factor  language  
The PFL [Gomes and Costa, 2012] is an extension ofProlog for representing 
first-order probabilistic models. 
Most graphical models such as BNs and MNs concisely represent a joint 
distribution by encoding it as a set of factors. The probability of a set of 
variables X tak:ing value x can be expressed as the product of n  factors as: 
P(X  =  x) =  Oi=l,... ,n cPi(xi)
z  
where Xi is a sub-vector of x on which the i-th factor depends and Z  is the 
normalization constant. Often, in a graphical model, the same factors appear 
repeatedly in the network, and thus we can parameterize these factors in order 
to simplify the representation. 
A Parameterized random variables (PRVs) is a logical atom representing 
a set of random variables, one for each of its possible ground instantiations. 
We indicate PRV as X, Y, ... and vectors ofPRVs as X,  Y,  ...  
A parametric  factor  or pmfactor  [Kisynski and Poole, 2009b] is a triple 
<C, V,  F)  where C  is a set of inequality constraints on parameters (logical 
variables), V  is a vector of PRV s and F  is a factor that is a function from the 
Cartesian product of ranges of PRV s in V  to real values. A parfactor is also 
represented as F(V)  IC or F(V)  if there are no constraints. A constrained 
PRV is of the form VIC, where V =  p(X1, ... , Xn)  is a non-ground atom 
and Cis a set of constraints on logical variables X  =  {X1, ... , Xn}·  Each 
constrained PRV represents the set ofrandom variables {p(x)  lx  E  C}, where 
x  is the tuple of constants (x1, ... , xn).  Given a (constrained) PRV V, we 
use RV(V)  to denote the set of random variables it represents. Each ground 
atom is associated with one random variable, which can take any value in 
range(V).  
The PFL extends Prolog to support probabilistic reasoning with paramet­
ric factors. A PFL factor is a parfactor of the form 
Type  F;  cjJ;  C,  

78 
Probabilistic  Logic  Programming  Languages  
where Type  refers to the type of the network over which the parfactor is 
defined (bayes  for directed networks or markov  for undirected ones); Fis a 
sequence of Prolog goals each defining a PRV under the constraints in C  (the 
arguments of the factor). If L  is the set of alllogical variables in F,  then C is a 
list ofProlog goals that impose bindings on L  (the successful substitutions for 
the goals in C are the valid values for the variables in L  ). cp  is the table defining 
the factor in the form of a list of real values. By default, all random variables 
are Boolean but a different domain may be defined. Each parfactor represents 
the set of its groundings. To ground a parfactor, allvariables of L  are replaced 
with the values permitted by constraints in C.  The set of ground factors defines 
a factorization of the joint probability distribution over all random variables. 
Example  34  (PFL program). The  following  PFL  program  is  inspired  by  the  
workshop  attributesproblern  of [Milchet  al.,  2008].  It  models  the  organiza­
tion  of a  workshop  where  a  number  of people  have  been  invited.  series 
indicates  whether  the  workshop  is  successful  enough  to  start  a  series  of re­
lated  meetings  while  attends (P) indicates  whether  person  P attends  the  
workshop.  
This  problern  can  be  modeled  by  a  PFL  program  such  as  
bayes series, attends(P); [0.51, 0.49, 0.49, 0.51]; 
[person (P)] . 
bayes attends(P), at(P,A); [0.7, 0.3, 0.3, 0.7]; 
[person(P),attribute(A)]. 
A  workshop  becomes  a  series  because  people  attend.  People  attend  the  work­
shop  depending  an  the  workshop's  attributes  such  as  location,  date,  fame  
of the  organizers,  etc.  The  probabilistic  atom  a t  (  P, A) represents  whether  
person  P attends because  of attribute  A. 
Thefirst PFLfactor has the  random variables  series and  attends (P) 
as  arguments  (both  Boolean),  [ 0. 51, 0. 4 9, 0. 4 9, 0. 51] as  table  and  
the  list  [person (P) ] as  constraint.  
Since KBMC languages are defined on the basis of a translation to graphical 
models, translations can be built between PLP languages under the DS and 
KBMC languages. The first have the advantage that they have a semantics 
that can be understood in logical terms, without necessarily referring to an 
underlying graphical model. 

2.10  Other  Semanticsfor  Probabilistic  Logic  Programming  79 
2.10  Other  Semantics  for  Probabilistic  Logic  Programming  
Here we briefiy discuss a few examples of PLP frameworks that don't follow 
the distribution semantics. Our goal in this section is simply to give the fia­
vor of other possible approaches; a complete account of such frameworks is 
beyond the scope of this book. 
2.1  0.1  Stochastic  logic  programs  
Stochastic logic programs (SLPs) [Muggleton et al., 1996; Cussens, 2001] 
are logic programs with parameterized clauses which define a distribution 
over refutations of goals. The distribution provides, by marginalization, a 
distribution over variable bindings for the query. SLPs are a generalization 
of stochastic grammars and hidden Markov models. 
An SLP  S  is a definite logic program where some of the clauses are of 
the form p  : C  where p  E  JR,p  :? 0, and Cisadefinite clause. Let n(S)  
be the definite logic program obtained by removing the probability labels. A 
pure  SLP is an SLP where all clauses have probability labels. A normalized  
SLP is one where probability labels for clauses whose heads share the same 
predicate symbol sum to one. 
In pure SLPs, each SLD derivation for a query q  is assigned a reallabel 
by multiplying the labels of each individual derivation step. The label of a 
derivation step where the selected atom unifies with the head of clause Pi  : Ci  
is Pi·  The probability of a successful derivation from q  is the label of the 
derivation divided by the sum of the labels of all the successful derivations. 
This forms a distribution over successful derivations from q.  
The probability of an instantiation q()  is the sum of the probabilities of the 
successful derivations that produce q().  It can be shown that the probabilities 
of all the atoms for a predicate q  that succeed in n(S)  sum to one, i.e., S  
defines a probability distribution over the success set of q  in n(S).  
In impure SLPs, the unparameterized clauses are seen as non-probabilistic 
domain knowledge acting as constraints. Derivations are identified with the 
set of the parameterized clauses they use. In this way, derivations that differ 
only on the unparameterized clauses form an equivalence class. 
In practice, SLPs define probability distributions over the children of 
nodes of the SLD tree for a query: a derivation step u  ~ v  that connects 
node u  with child node v  is assigned a probability P(vlu).  This induces a 
probability distributions over paths from the root to the leaves of the SLD 
tree and in turn over answers for the query. 

80 
Probabilistic  Logic  Programming  Languages  
Given their similarity with stochastic grammars and hidden Markov mod­
els, SLPs are particularly suitable for representing these kinds of models. 
They differ from the DS because they define a probability distribution over 
instantiations of the query, while the DS usually defines a distribution over 
the truth values of ground atoms. 
Example  35 (Probabilistic context-free grammar- SLP). Consider  the  prob­
abilistic  context free  grammar:  
0.2: S- aS  
0.2: S- bS  
0.3: S- a  
0.3: s- b  
The  SLP  
0.2 : s([aiR])  +--- s(R).  
0.2: s([biR])  +--- s(R).  
0.3: s([a]).  
0.3: s([b]).  
dejines  a  distribution  over  the  values  of S  in  the  success  set  of s(S)  that  is  
the  same  as  the  one  dejined  by  the  probabilistic  context-free  grammar  above.  
For  example,  P(s([a,  b]))  =  0.2 · 0.3 =  0.6 according  to  the  program  and  
P(ab)  =  0.2 · 0.3 =  0.6 according  to  the  grammar.  
Various 
approaches 
have 
been 
proposed 
for 
learning 
SLPs. 
Muggleton [2000a,b] proposed to use an Inductive logic programming (ILP) 
system, Progoi [Muggleton, 1995], for learning the structure ofthe programs, 
and a second phase where the parameters are tuned using a generalization of 
relative frequency. 
Parameters are also learned by means of optimization in failure-adjusted 
maximization [Cussens, 2001; Angelopoulos, 2016] and by solving algebraic 
equations [Muggleton, 2003]. 
2.1  0.2  ProPPR  
ProPPR [Wang et al., 2015] is an extension of SLPs that that is related to 
Personalized pagerank (PPR) [Page et al., 1999]. 
ProPPR extends SLPs in two ways. The first is the method for computing 
the labels of the derivation steps. A derivation step u  -
v  is not simply 
assigned the parameter associated with the clause used in the step. Instead, 
the label ofthe derivation step, P( vlu)  is computed using a log-linear model 
P( vlu)<X  exp(w·<Pu--->v)  where w  is a vector ofreal-valued weights and <Pu--->v  
is a 0/1 vector of "features" that depend on the clause being used. The features 

2.10  Other  Semanticsfor  Probabilistic  Logic  Programming  81 
are user defined and the association between clauses and features is indicated 
using annotations. 
Example  36 (ProPPR program). The  ProPPRprogram  [Wang  et  al.,  2015]  
about(X,  Z)  ~ handLabeled(X,  Z).  
#base  
about(X,  Z)  ~ sim(X,  Y),  about(Y,  Z).  
#prop  
sim(X,  Y)  ~ link(X,  Y).  
#sim,  link  
sim(X,  Y)  ~ hasWord(X,  W),  hasWord(Y,  W),  
linkedBy(X,  Y,  W).  
#sim,word  
linkedBy(X,  Y,  W).  
#by(W)  
can  be  used  to  compute  the  topic  of web  pages  on  the  basis  of possible  hand  
labeling  or  similarity  with  other  web  pages.  Similarity  is  defined  as  well  in  a  
probabilistic  way  depending  on  the  links  and  words  between  the  two  pages.  
Clauses are annotated with a list of atoms (indicated after the # symbol) that 
may contain variables from the head of clauses. In the example, the third 
clause is annotated with the list of atoms sim,  link  while the last clause is 
annotated by the atom by(W).  Each grounding of each atom in the list stands 
foradifferent feature, so for example sim,  link,  and by( sprinter)  stand for 
three different features. The vector c/Ju-.v  is obtained by assigning value 1 to 
the features associated with the atoms in the annotation of the clause used 
for the derivation step u  ~ v  and value 0 otherwise. If the atoms contain 
variables, these are shared with the head of the clause and are grounded with 
the values of the clause instantiation used in u  ~ v.  
So a ProPPR program is defined by an annotated program plus values 
for the weights w. This annotation approach considerably increases the fiex­
ibility of SLP labels: ProPPR annotations can be shared across clauses and 
can yield labels that depend on the particular clause grounding that is used 
in the derivation step. An SLP is a ProPPR program where each clause has a 
different annotation consisting of an atom without arguments. 
The second way in which ProPPR extend SLPs consists in the addition of 
edges to the SLD tree: an edge is added (a) from every solution leaf to itself; 
and (b) from every node to the root. 
The procedure for assigning probabilities to queries of SLP can then be 
applied to the resulting graph. The self-loop links heuristically upweight so­
lution nodes and the restartlinks make SLP's graph traversal a PPR procedure 
[Page et al., 1999]: a PageRank can be associated with each node, represent­
ing the probability that a random walker starting from the root arrives in that 
node. 

82 
Probabilistic  Logic  Programming  Languages  
The restartlinks favor the results of short proofs: if the restart probability 
is a  for every node u,  then the probability of reaching any node at depth d  is 
bounded by (1- a)d.  
Parameter learning for ProPPR is performed in [Wang et al., 2015] by 
stochastic gradient descent. 
2.11  Other  Semantics  for  Probabilistic  Logics  
In this section, we discuss semantics for probabilistic logic languages that are 
not based on logic programming. 
2.11.1  Nilsson's  probabilistic  logic  
Nilsson's probabilistic logic [Nilsson, 1986] takes an approach for combining 
logic and probability that is different from the DS: while the first considers 
sets of distributions, the latter computes a single distribution over possible 
worlds. In  Nilsson's logic, a probabilistiG  interpretation  Pr  defines a prob­
ability distribution over the set of interpretations Int2.  The probability  of a  
logiGalformula  F  according to Pr,  denoted Pr(F),  is the sumofall Pr(!)  
suchthat I  E  Int2  and I  F= F.  A probabilistiG  knowledge  base  K  is a set 
of probabilistic formulas of the form F  ):  p  where F  is a logical formula 
and p  a number in [0, 1]. A probabilistic interpretation Pr  satisfies  F  ): p  
iff Pr(F)  ): p.  Pr  satisfies  K,  or Pr  is a model  of K,  iff Pr  satisfies all 
F  ): p  E  K.  Pr(F)  ): p  is a tight  logiGal  GonsequenGe  of K  iff p  is the 
infimum of Pr(F)  in the set of all models Prof  K.  Computing tight logical 
consequences from probabilistic knowledge bases can be done by solving a 
linear optimization problem. 
With Nilsson's logic, the consequences that can be obtained from logi­
cal formulas differ from those of the DS. Consider a ProbLog program (see 
Section 2.1) composed of the facts 0.4 :: c(a)  and 0.5 :: c(b),  and a prob­
abilistic knowledge base composed of c(a)  ): 0.4 and c(b)  ): 0.5. For the 
DS, P(c(a)  v c(b)) =  0.7, while with Nilsson's logic, the lowest p  suchthat 
Pr(c(a)  v c(b)) ): p  holds is 0.5. This difference is due to the fact that, 
while Nilsson's logic makes no assumption about the independence of the 
statements, in the DS, the probabilistic axioms are considered as indepen­
dent. While independencies can be encoded in Nilsson's logic by carefully 
choosing the values of the parameters, reading off the independencies from 
the theories becomes more difficult. 

2.11  Other  Semantics  for  Probabilistic  Logics  83 
However, the assumption of independence of probabilistic axioms does 
not restriet expressiveness as shown in Section 2.6. 
2.11.2  Markov  logic  networks  
A Markov Logic Network (MLN) is a first-order logical theory in which each 
sentence is associated with a real-valued weight. An  MLN is a template for 
generating MNs. Given sets of constants defining the domains of the logical 
variables, an MLN defines an MN that has a Boolean node for each ground 
atom and edges connecting the atoms appearing together in a grounding of 
a formula. MLNs follow the KBMC approach for defining a probabilistic 
model [Wellman et al., 1992; Bacchus, 1993]. The probability distribution 
encoded by an Markov logic network (MLN) is 
1 
P(x)  = z  exp( .2:: Wini(x))  
fiEM 
where x is a joint assignment of truth value to all atoms in the Herbrandbase 
(finite because of no function symbols ), M  is the MLN, fi  is the i-th formula 
in M,  Wi  is its weight, ni (x) is the number of groundings of formula fi  that 
are satisfied in x, and Z  is a normalization constant. 
Example  37 (Markov Logic Network). The  following  MINencodes  a  theory  
on  the  intelligence  offriends  and  on  the  marks  people  get:  
1.5 Intelligent(x) => GoodMarks(x) 
1.1  Friends(x,y) => (Intelligent(x)<=> 
Intelligent (y)) 
The  jirst  formula  gives  a  positive  weight  to  the  fact  that  if someone  is  intel­
ligent,  then  he  gets  good  marks  in  the  exams  he  takes.  The  second  formula  
gives  a  positive  weight  to  the  fact  that  friends  have  similar  intelligence:  in  
particular,  the  formula  states  that  if x and  y  are  friends,  then  x is  intelligent  
if and  only  if y  is  intelligent,  so  they  are  either  both  intelligent  or  both  not  
intelligent.  
lf the  domain  contains  two  individuals,  Anna  and  Bob,  indicated  with  A  
and  B,  we  get  the  ground  MN  of Figure  2.10.  
2.11.2.1   Encoding  Markov  logic  networks  with  probabilistic  logic  
programming  
It is possible to encode MNs and MLNs with LPADs. The encoding is based 
on the BN that is equivalent to the MN as discussed in Section 1.6: an MN 

84 
Probabilistic  Logic  Programming  Languages  
Figure  2.10  Ground Markov network for the MLN of Example 37. 
factor can be represented with an extra node in the equivalent BN that is 
always observed. In order to model MLN formulas with LPADs, we can add 
an extra atom clausei(X)  for each formula Fi  =  Wi  Ci  where Wi  is the 
weight associated with Ci  and Xis the vector of variables appearing in Ci.  
Then, when we ask for the probability of query q  given evidence e,  we have 
to ask for the probability of q  given e  1\  ce,  where ce  is the conjunction of the 
groundings of clausei(X)  for all values of i. 
Clause Ci  must be transformed into a Disjunctive Normal Form (DNF) 
formula eil V ... V Cini'  where the disjuncts are mutually exclusive and the 
LPAD should contain the clauses 
clausei(X):  ewi/(1 +  ewi)  ~ Cij  
for all j  in 1, ... , ni,  where 1 + ew;  :? maxxi cj;(xi) = max{1, ewi  }. Similarly, 
---.Ci  must be transformed into a DNF Di1 v  ...  v  Dirn;  and the LPAD should 
contain the clauses 
clausei(X)  : 1/(1 +  ewi)  ~ Dil  
for alll in 1, ... , mi. 
Moreover, for each predicate pjn,  we should add the clause 
p(X): 0.5. 
to the pro gram, assigning a  priori  uniform probability to every ground atom. 
Altematively, if Wi  is negative, ewi  will be smaller than 1 and maxxi cp  
(xi) = 1. So we can use the two probability values ewi  and 1 with the clauses 
clausei(X)  : ewi  ~ Cij·  
clausei(X)  
~ 
Dil.  
This solution has the advantage that some clauses are non-probabilistic, re­
ducing the number of random variables. If Wi  is positive in the formula Wi  Ci,  
we can consider the equivalent formula -wi  ---.Ci. 

2.11  Other  Semantics  for  Probabilistic  Logics  85 
The transformation above is illustrated by the following example. Given 
theMLN 
1 . 5 Intelligent (x) => GoodMarks (x) 
1 . 1 Friends (x, y ) => (Intelligent (x) <=>Intelligent (y )) 
the first formula is translated to the clauses: 
clause1 (X) : 0 . 8175 :- \+intelligent (X). 
clause1 (X) :0 . 1824 : -
intelligent (X), 
\+good_marks (X). 
clause1 (X) : 0 . 8175 : - intelligent (X), good_marks (X). 
where 0.8175 =  el.5/ (1 +  el.5 ) and 0.1824 =  1/ (1 +  el. 5). 
The second formula is translated to the clauses 
clause2 (X, Y) : 0 . 7502 : -
\+friends (X, Y). 
clause2 (X, Y) : 0. 7502 : -
friends (X, Y), 
intelligent (X), 
intelligent (Y) . 
clause2 (X, Y) : 0.7502 : -
friends (X, Y), 
\+ intelligent (X), 
\+ intelligent (Y). 
clause2 (X, Y) : 0 . 24 97 : -
friends (X, Y), 
intelligent (X), 
\+ intelligent (Y). 
clause2 (X, Y) : 0.2497 : -
friends (X, Y), 
\+intelligent (X), 
intelligent (Y). 
where 0.7502 =  el.l/ (1 +  el.l) and 0.2497 =  1/ (1 +  e1.1). 
A  priori  we have a uniform distribution over student intelligence, good 
marks, and friendship: 
intelligent (_ ) :0.5 . 
good_marks (_ ) : 0 . 5 . 
friends (_ , _ ) : 0.5 . 
and there are two students: 
student (anna ). 
student (bob ). 
We have evidence that Anna is friend with Bob and Bob is intelligent. The 
evidence must also include the truth of all groundings of the clausei  predi­
cates: 

86 
Probabilistic  Logic  Programming  Languages  
evidence_mln : - clausel (anna ), clausel (bob ), 
clause2 (anna , anna ), clause2 (anna , bob ), 
clause2 (bob , anna ), clause2 (bob , bob ). 
ev_intelligent_bob_friends_anna_bob : ­
intelligent (bob ), friends (anna , bob ), 
evidence_ mln . 
The probability that Anna gets good marks given the evidence is thus 
P(good_marks (anna )l ev_ intelligent_ bob_friends_anna_bob) 
while the prior probability of Anna getting good marks is given by 
P(good_ marks (anna ) ). 
The probability resulting from the first query is higher (P  = 0. 733) than the 
second query (P = 0.607), since it is conditioned to the evidence that Bob is 
intelligent and Anna is his friend. 
In the alternative transformation, the first MLN formula is translated to: 
clausel (X) : -
\+intelligent (X).  
clausel (X) : 0 . 2231 : - intelligent (X), \+good_marks (X)  
clausel (X) : - intelligent (X), good_marks (X).  
where 0.2231 = e-1.5_ 
MLN formulas can also be added to a regular probabilistic logic program. 
In this case, their effect is equivalent to a soft form of evidence, where certain 
worlds are weighted more than others. This is the same as soft evidence in 
Figaro [Pfeffer, 2016]. MLN hard constraints, i.e., formulas with an infinite 
weight, can instead be used to rule out completely certain worlds, those vio­
lating the constraint. For example, given hard constraint Ci  equivalent to the 
disjunction eil V ... V Gin;'  the LPAD should contain the clauses 
clausei (X)  +-- Cij  
for all j,  and the evidence should contain clausei (x)  for all groundings 
x  of X.  In this way, the worlds that violate Ci  are ruled out. 
2.11.3  Annotated  probabilistic  logic  programs  
In Annotated Probabilistic Logic Programming (APLP) [Ng and Subrahma­
nian, 1992], program atoms are annotated with intervals that can be inter­
preted probabilistically. An example rule in this approach is: 
a  : [0. 75 , 0.85] +-- b  : [1 , 1], c  : [0.5, 0.75] 

2.11  Other  Semantics  for  Probabilistic  Logics  87 
that states that the probability of a  is between 0. 75 and 0.85 if b  is certainly 
true and the probability of c is between 0.5 and 0. 75. The probability interval 
of a conjunction or disjunction of atoms is defined using a combinator  to 
construct the tightest bounds for the formula. For instance, if d  is annotated 
with [ld,  hd]  and e  with [le,  he].  the probability of e  1\  d  is annotated with 
[max(O,  ld  +  le- 1),  min(hd,  he)].  
U sing these combinators, an inference operator and fixpoint semantics is de­
fined for positive Datalog programs. A model theory is obtained for such 
programs by considering the annotations as constraints on acceptable proba­
bilistic worlds: an APLP thus describes a family of probabilistic worlds. 
APLPs have the advantage that deduction is of low complexity, as the 
logic is truth-functional, i.e., the probability of a query can be computed di­
rectly using combinators. The corresponding disadvantages are that APLPs 
may be inconsistent if they are not carefully written, and that the use of 
the above combinators may quickly lead to assigning overly slack proba­
bility intervals to certain atoms. These aspects are partially addressed by 
hybrid APLPs [Dekhtyar and Subrahmanian, 2000], which allow different 
fiavors of combinators based on, e.g., independence or mutual exclusivity of 
given atoms. 


3   
Semantics  with  Function  Symbols  
When a program contains variables, function symbols, and at least one con­
stant, its grounding is denumerable. In this case, the set of atomic choices in 
a selection that defines a world is denumerable and there is an uncountable 
set of worlds. 
Theorem  4  (Cardinality of the set of worlds of a ProbLog program). For  a  
ProbLog  program  P,  the  set  of all  the  worlds  Wp  is  uncountable.  
Proof.  The proof uses Cantor's diagonal argument. 
If the program contains at least one function symbol and one constant, the 
Herbrand base Bp  is denumerable and so is the grounding of the pro gram. In  
this case, each element of Wp  can be represented as a denumerable sequence 
of Boolean values. Equivalently, we can represent it with a denumerable 
sequence of bits b1 , b2, b3, ... 
If we suppose that Wp  is denumerable, it is possible to write its element 
in a list such as 
bl,l'  b1,2, b1,3, .. . 
b2,1' b2,2, b2,3, .. . 
b3,1' b3,2, b3,3, .. . 
Since Wp  is denumerable, the list should contain all its elements. 
Ifwe pick element -.bl,l,  -.b2,2, -.b3,3, ..., this belongs to Wp  because it 
is a denumerable sequence of Booleans. However, it is not in the list, because 
it differs from the first element in the first bit, from the second element in the 
second bit, and so on. In other words, it differs from each element of the list. 
This is against the hypothesis that the list contains all elements of Wp.  Thus, 
W p  is not denumerable and so W p  is uncountable. 
D 
The probability of each individual world is given by an infinite product. We 
recall the following result from [Knopp, 1951, page 218]. 
89 

90 
Semantics  with  Function  Symbols  
Lemma  1  (Infinite Product). If Pi  E  [0, b]  for  all  i  = 1, 2, ... with  b  E  [0, 1), 
then  theinfinite  product  0~ 1 Pi  converges  to  0.  
Each factor in the infinite product giving the probability of a world is bounded 
away from one, i.e., it belongs to [0, b]  for b E  [0, 1). To see this, it is enough 
to pick b  as the maximum of all the probabilistic parameters that appear in the 
pro gram. This is possible if the program does not have flexible probabilities 
or probabilities that depend on values computed during program execution. 
So if the program does not contain flexible probabilities, the probability 
of each individual world is zero and the semantics of Section 2.2 is not well­
defined [Riguzzi, 20 16]. 
Example  38 (Program with infinite set of worlds). Consider  the  ProbLog  
program  
p(O)  +--- u(O).  
p(s(X))  +--- p(X),  u(X).  
t  +---"-'8.  
s  +--- r,q.   
q  +--- u(X).   
F1 =  a  ::  u(X).   
F2 =  b  :: r.   
This  program  contains function  symbol  s /1 and constant  0 and a  probabilistic  
fact  with  a  variable,  so  its  set  of worlds  is  uncountable.  
Example  39 (Game of dice). 
Consider  the  game  of  dice  proposed  in  
[Vennekens  et  al.,  2004]:  the  player  repeatedly  throws  a  six-sided  die.  When  
the  outcome  is  six,  the  game  stops.  A  ProbLog  version  of this  game  where  the  
die  has  three  sides  and  the  game  stops  when  the  outcome  is  3  is:  
F1 =  1/3 :: one(X).  
F2 =  1/2 :: two(X).  
on(O,  1)  +--- one(O).  
on(O,  2) +-rvone(O),  two(O).  
on(O,  3) +-rvone(O),  rvtwo(O).  
on(s(X),  1) +--- on(X,_),  rvon(X,  3), one(s(X)).  
on(s(X),  2) +--- on(X,_),  rvon(X,  3), rvone(s(X)),  two(s(X)).  
on(s(X),  3) +--- on(X,_),  rvon(X,  3), rvone(s(X)),  "'two(s(X)).  
If we  add  the  clauses  
at_least_once_1  +--- on(_,  1).  
never _1 +-rvat_least_once_l.  

3.1  The  Distribution  Semanticsfor  Programs  with  Function  Symbols  91 
we  can  askfor  the  probability  that  at  least  once  the  die  landed  onface  1  and  
that  the  dienever  landed  onface  1.  As  in  Example  38,  this  program  has  an  
infinite  and  uncountable  set  ofworlds.  
3.1   The  Distribution  Semanlies  for  Programs  with  Function  
Symbols  
We now present the definition of the DS for ProbLog programs with function 
symbols following [Poole, 1997]. The semanti es for a probabilistic logic pro­
gram P  with function symbols of [Poole, 1997] is given by defining a finitely 
additive probability measure t-t  over an algebra Op  on the set of worlds Wp.  
We first need some definitions. The set  of worlds  w/'i,  compatible  with  a  
composite  choice  ""is w/'i,  = {wu  E  Wpl""  ~ o"}.  Thus, a composite choice 
identifies a set of worlds. For programs without function symbols, P("")  
.L:wEw"  P(w),  where 
P("")  = n  IIi  n  1- IIi  
(fi,O,I)EK 
(/;,O,ü)EK 
For program with function symbols .L:wEw",  P( w)  may not be defined as w/'i,  
may be uncountable and P(w)  =  0. However, P("")  is still well defined. Let 
us call it t-t  so t-t  ("") =  P  (""). 
Given a set  of composite choices K,  the set  ofworlds  wx  compatible  with  
K  is wx  =  UKEK w/'i,.  Two composite choices ""I and ""2 are incompatible  
if their union is not consistent. A set K  of composite choices is pairwise  
incompatible  if for all ""I E K,  ""2 E K,  ""I #- ""2 implies that ""I and ""2 are 
incompatible. 
Regardless of whether a probabilistic logic program has a finite number 
of worlds or not, obtaining pairwise incompatible sets of composite choices is 
an important problem. This is because for program without function symbols, 
the probability  of a  pairwise  incompatible  set  K  of composite  choices  can be 
defined as P(K)  =  .L:KEK  P("")  which is easily computed. Let us call it t-t  so 
t-t(K)  =  P(K).  Two sets KI  and K2 of composite choices are equivalent  if 
they correspond to the same set of worlds: w K  1 = w x 2 • 
One way to assign probabilities to a set K  of composite choices is to 
construct an equivalent set that is pairwise incompatible; such a set can be 
constructed through the technique of splitting.  More specifically, if j(}  is an 
instantiated fact and "" is a composite choice that does not contain an atomic 
choice (j,  (},  k)  for any k,  the split  of"" on j(}  is the set of composite choices 

92 
Semantics  with  Function  Symbols  
s~,f(} = {t~; u  {(!, e,  0)}, /'\; u  {(!, e,  1)}}. It  is easy to see that /'\; and s~,f(} 
identify the same set of possible worlds, i.e., that w~ = ws~<.JB, and that S~,te 
is pairwise incompatible. The technique of splitting composite choices on 
formulas is used for the following result [Poole, 2000]. 
Theorem 5 (Existence of a pairwise incompatible set of composite choices 
[Poole, 2000]). Given  a finite  set  K  of composite  choices,  there  exists  a finite  
set  K'  of pairwise  incompatible  composite  choices  such  that  K  and  K'  are  
equivalent.  
Proof  Given a finite set of composite choices K,  there are two possibilities 
to form a new set K'  of composite choices so that K  and K'  are equivalent: 
1.  Removing dominated elements: if t~; 1 , t~;2 E  K  and t~; 1 c 
t~;2 , let K'  = 
K\{t~;2}· 
2.  Splitting elements: if t~; 1 , t~;2 E  Kare compatible (and neither is a su­
perset of the other), there is a (f,  0,  k)  E  t~;1 \t~;2· We replace t~;2 by the 
split of t~;2 on fO.  Let K'  = K\{t~;2} u  S~2 ,Je· 
In both cases, w K  =  w K'.  If we repeat this two operations until neither is 
applicable, we obtain a splitting algorithm (see Algorithm 1) that terminates 
because K  is a finite set ofcomposite choices. The resulting set K'  is pairwise 
incompatible and is equivalent to the original set. 
D 
Algorithm 1 Function SPLIT: Splitting Algorithm. 
1:  function SPLIT(K) 
2: 
Ioop 
3: 
if 3~1, ~2 E K  suchthat ~1 c  ~2 then 
4: 
K  +--- K\{~2} 
5: 
eise 
6: 
if 3~ 1 , ~ 2 E  K  compatibie then 
7: 
choose (!,  8,  k)  E  ~1 \~2 
s: 
K  +--- K\{~2} u  s1<2,Fo  
9: 
eise 
10: 
return K  
11: 
end if 
12: 
end if 
13: 
end Ioop 
14: end function 
Theorem 6 (Equivalence of the probability of two equivalent pairwise in­
compatible finite sets offinite composite choices [Poole, 1993a]). If K1 and  

3.1  The  Distribution  Semanticsfor  Programs  with  Function  Symbols  93 
K  2 are  both  pairwise  incompatible  finite  sets  of finite  composite  choices  such  
that  they  are  equivalent,  then  P(K1)  = P(K2).  
Proof.  Consider the set D  of all instantiated facts f  ()  that appear in an atomic 
choice in either K  1 or K  2. This set is finite. Bach composite choice in K  1 and 
K  2 has atomic choices for a subset of D.  For both K  1 and K  2, we repeatedly 
replace each composite choice "'of K1 and K2 with its split S,.",J;Oi  on an 
Ji()j  from D  that does not appear in"'· This procedure does not change the 
total probability as the probabilities of (fi,  ()j,  0) and (fi,  ()j,  1) sum to 1. 
At the end of this procedure, the two sets of composite choices will be 
identical. In fact, any difference can be extended into a possible world be­
longing to WK1  but not to WK2  or vice versa. 
D 
Fora ProbLog program P,  we can thus define a unique finitely additive prob­
ability measure J-t~ : Op  ~ [0, 1] where Op  is defined as the set of sets of 
worlds identified by finite sets of finite composite choices: Op  =  {wKIK  is 
a finite set of finite composite choices } . 
Theorem  7  (Algebra of a program). Op  is  an  algebra  over  Wp.  
Proof.  We need to prove that Op  respects the three conditions ofDefinition 8. 
Wp  = WK  with K  = {0}.  The complement wx  of WK  where K  is a 
finite set of finite composite choice is WJ(  where K  is a finite set of finite 
composite choices. In  fact, K  can be obtained with function DUALS(K) of 
[Poole, 2000] shown in Algorithm 2 for the case of Prob Log. Such a function 
performs Reiter's hitting set algorithm over K,  generating an element "'of K  
by picking an atomic choice (!,  (),  k)  from each element of K  and inserting in 
"' the atomic choice (f,  (),  1-k).  After this process is performed in all possible 
ways, inconsistent sets of atom choices are removed obtaining K.  Since the 
possible choices of atomic choices are finite, so is K.  Finally, closure under 
finite union holds since the union of WK1  with WK2  is equal to WK1 uK2 for 
the definition of w K.  
D 
The corresponding measure J-t~ is defined by J-t~ (w K)  = J-t(  K')  where 
K'  is a pairwise incompatible set of composite choices equivalent to K.  
Theorem  8  (Finitely additive probability space of a program). The  triple  
<Wp,  Op,  J-t~) with  
J-t~(wK) =  J-t(K')  
where  K'  is  a  pairwise  incompatible  set  of composite  choices  equivalent  to  
K,  is  afinitely  additive  probability  space  according  to  Definition  12.  

94 
Semantics  with Function  Symbols  
Algorithm 2 Function DUALS: Duals computation. 
1: function DUALS(K) 
2: 
suppose K  =  {1>:1, ... , Kn}  
3: 
Da+-- {0}  
4: 
for i  +-- 1 ~ n  do 
5: 
Di  +--{du{(!, 0,1- k)}ld E Di-1, (f,  0,  k)  E Ki} 
6: 
remove inconsistent e1ements from Di  
7: 
remove any Ii  from Di  if 3K' E Di  
1  
suchthat K c Ii  
8: 
end for 
9: 
return Dn  
10: end function 
Proof.  JL~(w{0 }) is equal to 1. Moreover, JL~(wK) :? 0 for all K  and if 
WK1  n  WK2  = 0  and K~ (K~) is pairwise incompatible and equivalent to K1 
(K2), then K~ u  K~ is pairwise incompatible and 
JL~(WK1 U  WK2)  = 
2.:  
P("')  = 2.:  P("'1)  +  2.:  P("'2)  = 
1\:EKiuK~ 
I\:1EKi 
I\:2EK~ 
JL~(WK1) +  JL~(WK2). 
D  
Given a query q,  a composite choice "'is an explanation  for q  if Vw  E  
wl\;  :  w  I=  q.  A  set K  of composite choices is covering  with respect to q  if 
every world in which q  is true belongs to w K.  
Fora probabilistic logic program Panda ground atom q,  we define the 
function Q  : Wp  -
{0, 1} as 
1 ifwl=q 
Q ( w)  
(3.1) 
=  { 0 otherwise 
If  q  has a finite set K  of finite explanations such that K  is covering then 
Q-1({1})  =  {wlw  E  Wp  1\  w  I=  q} =  WK  E  Dp  so Q  is measurable. 
Therefore, Q  is a random variable whose distribution is defined by P  ( Q  =  1) 
(P(Q  =  0) is given by 1- P(Q  =  1)). We indicate P(Q  =  1) with P(q)  
and we say that P(q)  isfinitely  well-defined  for the distribution semantics. A 
program P  is finitely  well-defined  if the probability of all ground atoms in the 
grounding of Pis  finitely well-defined. 
Example 40 (Covering set of explanations for Example 38). Consider  the  
program  of Example  38.  The  set  K  = { "'} with  
"'= {(f1,{)(/0},1),(f1,{)(/s(0)},1)} 

3.2  Infinite  Covering  Set  of Explanations  95 
is a pairwise incompatible finite  set  of finite  explanations that are  covering for  
the  query  p(s(O)). Then  P(p(s(O)))  isfinitely  well-defined  and  P(p(s(O)))  = 
P(K,)=a2 .  
Example  41  (Covering set of explanations for Example 39). Now  consider  
Example  39.  The  set  K  =  {  K,I, t\,2}  with  
ti,I = {(JI,{)(/0},1),(JI,{)(js(0)},1)} 
ti,2 = {(JI,{)(j0},0),(f2,{)(j0},1),(fi,{)(js(0)},1)} 
is  a  pairwise  incompatible  finite  set  of finite  explanations  that  are  covering  
for  the  query  on(s(O),  1).  Then  P(on(s(O),  1))  isfinitely  well-and  
P(on(s(O),  1)) =  P(K)  =  1/3 · 1/3 +  2/3 · 1/2 · 1/3 =  2/9. 
3.2  Infinite  Covering  Set  of  Explanations  
In  this section, we go beyond [Poole, 1997] and we remove the requirement 
of the finiteness of the covering set of explanations and of each explanation 
for a query q  [Riguzzi, 2016]. 
Example  42  (Pairwise incompatible covering set of explanations for 
Example 38). In  Example  38,  the  query  s  has  the  pairwise  incompatible  
covering  set  of explanations  
Ks  =  {K,(), K,l, · · .} 
with  
K,i  =   {(!2,0,1),(!1,{)(/0},0), ... , 
(JI,  {)(jsi-I(O)},O), (JI,{)(jsi(O)},  1)} 
where  si(O) is  the  term  where  the  functor  s  is  applied  i  times  to  0.  So  K 8  is  
denumerable.  A  pairwise  incompatible  covering  set  of explanations fort  is  
Kt  =  { {  (!2, 0,  0)}, K,t} 
where  K,t  is  the  infinite  composite  choice  
ti,t  =  {(J2,0, 1), (JI,  {)(/0},0), (JI,  {)(js(O)},O), ... } 
Example  43  (Pairwise incompatible covering set of explanations for 
Example 39). In  Example  39,  the  query  at_least_once_1  has  the  pairwise  
incompatible  covering  set  of explanations  
K+  =  {  K,t'  K,i' · · ·} 

96 
Semantics  with  Function  Symbols  
with  
11;(j = {(fi, {X/0}, 1)}  
11;i =  {(!1,  {X/0}, 0), (h,  {X/0}, 1),  (!1,  {X/s(O)}, 1)}   
11;{  =  {(fi, {X/0}, 0), (h,  
1
{X/0}, 1),  ...  ,  (fi, {X/si- (0)}, 0), 
1
(j2, {X/si- (0)}, 1),  (j1, {X/si(O)}, 1)}  
So  K+  is  countable  and  infinite.  The  query  never _1 has  the  pairwise  incom­
patible  covering  set  of explanations  
K- =  {  11;(), /1;1' ·· ·} 
with  
A;o = {(fi, {X/0}, 0), (h,  {X/0}, 0)}  
11;1 =  {  (!1,  {X/0}, 0), (h,  {X/0}, 1),  (!1,  {X/s(O)}, 0),  
(f2,{Xjs(O)},O)}  
A;i 
=   {  (!1,  {X/0}, 0), (h,  
1
{X/0}, 1),  ...  ,  (!1,  {X/si- (0)}, 0), 
(h,  
1
{X/si- (0)}, 1),  (!1,  {X/si(O)}, 0), (h,  {X/si(O)}, 0)} 
Fora probabilistic logic program P,  we can define the probability measure 
f.tp  :  Op  ~ [0, 1] where Op  is defined as the set of sets of worlds identi­
fied by countable sets of countable composite choices: Op  = {wKIK  is a 
countable set of countable composite choices } . 
Before showing that Op  is a O"-algebra, we need some definitions and 
results regarding sequences of sets. For any sequence of sets {An In  ?: 1}, 
define [Chow and Teicher, 2012, page 2] 
CIJ  
CIJ  
limn_,ooAn  u n   
Ak  
n=1k=n  
CIJ  
CIJ  
limn_,ooAn  = nUAk  
n=1k=n  

3.2  Infinite  Covering  Set  of Explanations  97  
Note that [Chow and Teicher, 2012, page 2] 
limn--->ooAn  
{ala  E  An  i.o.} 
limn--->ooAn  
{aia  E  An  for all but a finite number of indices n} 
where i.o. denotes infinitely often. The two definitions differ because an el­
ement a  of limn--->ooAn  may be absent from An  for an infinite number of 
indices n,  provided that there is a disjoint, infinite set of indices n  for which 
a  E  An.  For each a  E  limn--->ooAn,  instead, there is a m  ? 1 such that 
Vn?  m,a  E  An.  
Then limn--->ooAn  ~ limn--->ooAn.  If limn--->ooAn  = limn--->ooAn  = A,  then 
Ais called the limit  ofthe  sequence  and we write A  = limn--->oo An.  
A sequence {An ln  ? 1} is increasing  if An-1  ~ An  for all n  = 2, 3, .... 
If a sequence {An In  ? 1} is increasing, the limit limn--->oo An  exists and is 
equal to U~= 1 An  [Chow and Teicher, 2012, page 3]. 
Lemma  2 (O"-algebra of a Program). Op  is  a  O"-algebra  over  Wp.  
Proof.  Wp  E  Op  is true as in the algebra case. To see that the complement 
wK_  of WK  is in Op,  let us prove that the dual K  of K  isacountableset of 
countable composite choices and then that wK_  = WJ(·  Let us consider first the 
case where K  is finite, i.e., let K  be Kn  = {11;1, ... , A;n}· We will prove the 
thesis by induction. In  the base case, if K  1 = { 11;1}, then we can obtain K  1 by 
picking each atomic choice (!,  (),  k)  of 11;1 and inserting in K  1 the composite 
choice { (!,  (),  1 - k)}.  As there is a countable number of atomic choices in 
11;1, K  1 isacountableset of composite choices each with one atomic choice. 
In the inductive case, assume that Kn-1  =  
{11;1, ••. , A;n-d  and that 
Kn-1  is a countable set of composite choices. Let Kn  =  Kn-1  u  {11;n} 
and Kn-1  =  
{11;~, 11;~, ••. }. We can obtain Kn  by picking each 11;~ and each 
atomic choice (!,  (),  k)  of A;n·  If (!,  (),  k)  E 11;~, we discard 11;~, else if (!,  (),  k')  
E 11;~ with k'  -=!=  k,  we insert 11;~ in K  n·  Otherwise, we generate the composite 
choice 11;7  where 11;7  =  
11;~ u  {  (f,  (),  1 - k)}  and insert it in K  n.  Doing this 
for all atomic choices (f,  (),  k)  in A;n  generates a finite set of finite composite 
choices if A;n  is finite and a denumerable set of finite composite choices if A;n  
is denumerable. Doing this for all 11;~, we obtain that K  n  is a countable union 
of countable sets which isacountableset [Cohn, 2003, page 3]. wK_  =  WJ(  
because all composite choices of K  are incompatible with each world of w K,  
as they are incompatible with each composite choice of K.  So wK_  E  Op.  
If K  is denumerable, then let K  =  {  11;1, 11;2, •.. } . Consider the subsets 
Kn  of the form Kn  =  {11;1, ••. , A;n}· Using the construction above, build 

98 
Semantics  with  Function  Symbols  
K n  for all n  and consider the sets limn--.ooK n  and limn_, 00 K  n·  Consider 
a r;,'  that belongs to limn--.ooKn.  Suppose r;,'  E  Kj  and r;,'  tf; Kj+l·  This 
means that r;,'  was removed because ""J+1 ~ r;,
1  or because it was replaced by 
an extension of it. Then r;,'  will never be re-added to a K n  with n  >  j  +  1 
because otherwise w Kn  and WJ( n  would have a non-empty intersection. So for 
a composite choice r;,1  to appear infinitely often, there must exist an integer 
m  :;;:: 1 such that r;,'  E  K n  for all n  :;;:: m. In other words, r;,1  belongs to 
limn-->ooK n·  Therefore, limn-->CXJKn  = limn--.ooK n  = limn--.oo K  n·  Let us 
call K  this limit. K  can thus be expressedas U~= 1 n~=n K  n·  
n~n K  n  is countable as it is a countable intersection of countable sets. 
So K  is countable as it is a countable union of countable sets. Moreover, each 
composite choice of K  is incompatible with each composite choice of K.  In 
fact, let r;,1  be an element of K  and let m  :? 1 be the smallest integer such that 
r;,1  E  K n  for all n  :? m. Then r;,1  is incompatible with all composite choices 
of Kn  for n  :? m  by construction. Moreover, it was obtained by extending 
a composite choice r;,
11  from K  m-1 that was incompatible with all composite 
choices from Km-1· As r;,1  is an extension of r;,
11 ,  it is also incompatible with 
all elements of Km-1· So w'k  =  WJ(  and w'k  E  Dp.  
Closure under countable union is true as in the algebra case. 
D 
Given K  =  {  ""1, r;,2, ...} where the r;,is may be infinite, consider the sequence 
{Knln  :? 1} where Kn  =  {r;,1, ... , r;,n}·  Since Kn  is an increasing se­
quence, the limit limn--.oo K  n  exists and is equal to K.  Let us build a sequence 
{K~In :? 1} as follows: K~ =  {r;,1} and K~ is obtained by the union of 
K~_ 1 with the splitting of each element of K~_ 1 with ""n·  By induction, it is 
possible to prove that K~ is pairwise incompatible and equivalent to Kn.  
Foreach K~, we can compute JL(K~). noting that JL(r;,)  = 0 for infinite 
composite choices. Let us consider the limit limn--.oo JL(K~). 
Lemma  3 (Existence ofthe limit ofthe measure of countable union of count­
able composite choices). limn--.oo JL(K~) exists.  
Proof  We can see JL(K~) for n  =  1, ... as the partial sums of a series. A 
non-decreasing series converges if the partial sums are bounded from above 
[Brannan, 2006, page 92], so, if we prove that JL(K~) :? JL(K~_ 1 ) and that 
JL(K~) is bounded by 1, the lemma is proved. Remove from K~ theinfinite 
composite choices, as they have measure 0. Let 'Dn  be a ProbLog program 
containing a fact IIi :: fiB  for each instantiated facts fiB  that appears in 
an atomic choice of K~. Then 'Dn-1  ~ 'Dn.  The triple (Wvn,  Dvn,  JL)  is a 

3.2  Infinite  Covering  Set  of Explanations  99 
finitely additive probability space (see Section 2.2), so J-L(K~) ~ 1. Moreover, 
since WK'  
~ WK'  , then J-L(K~) :? J-L(K~_ 1 ). 
D 
n-l  
n  
We can now define the probability space of a program. 
Theorem  9  (Probability space of a program). The  triple  (Wp,  Op,  J-Lp)  with  
J-Lp(wK)  =  lim J-L(K~)
n--->CXJ  
where  K  = {11;1, 11;2, ... } and  K~ is  a  pairwise  incompatible  set  of compos­
ite  choices  equivalent  to  {11;1, •.. , A;n}, is  a  probability  space  according  to  
Definition  11.  
Proof  (J-L-l) and (J-L-2) holdas for the finite case. For (J-L-3), let 
0  = {WL1 ,  WL 2 ,  •• •  }  
be a countable set of subsets of Op  such that the w L;  s are the set of worlds 
compatible with countable sets of countable composite choices Lis and are 
pairwise disjoint. Let L~ be the pairwise incompatible set equivalent to Li  
and let L  be u~1 L~. Since the WL;S  are pairwise disjoint, then L  is pairwise 
incompatible. L  is countable as it is a countable union of countable sets. Let 
L  be {11;1, 11;2, ••• } and let K~ be {11;1, ••• , A;n}· Then 
CXJ 
J-Lp(O)  =  lim J-L(K~) =  lim " 
J-L(A;) =  "J-L(A;i) =  "J-L(A;).
n--->CXJ  
n--->CXJ  L.J  
L.J  
L.J  
K-EKi"  
i=1 
K-EL 
Since .L:~ 1 J-L(  11;) is convergent and a sum of non-negative terms, it is also ab­
solutely convergent and its terms can be rearranged [Knopp, 1951, Theorem 
4, page 142]. We thus get 
CXJ 
CXJ 
J-Lp(O)  =  2:: J-L(A;) =  2:: J-L(L~) =  2:: J-Lp(wLn).  
K-EL 
n=1 
n=1 
D  
For a probabilistic logic program P  and a ground atom q  with a countable set 
K  of explanations suchthat K  is covering for q,  then {wlw  E  Wp  A  w  I=  
q} = WK  E  Op.  So function Q  ofEquation (3.1) is a random variable. 
Again we indicate P(Q  =  1) with P(q)  and we say that P(q)  is well­
defined  for the distribution semantics. A program P  is well-defined  if the 
probability of all ground atoms in the grounding of Pis well-defined. 

100 
Semantics  with  Function  Symbols  
Example  44  (Probability of the query for Example 38). Consider Example  
42.  The  explanations  in  K 8  are  pairwise  incompatible,  so  the  probability  of s  
can  be  computed  as  
P(s)  =  ba  +  ba(1- a)  +  ba(1- a)  2 +  ...  =  
ba  
,  =  b.  
since  the  sum  is  a  geometric  series.  Kt  is  also  pairwise  incompatible,  and  
P(Kf)  =  0 so  P(t)  =  1- b  +  0 =  1 - b  which  is  what  we  intuitively  expect.  
Example  45  (Probability of the query for Example 39). In  Example  43,  the  
explanations  in  K+  are  pairwise  incompatible,  so  the  probability  ofthe  query  
at_least_once_1  is  given  by  
2 
1 
2 1 1 
(2 1) 
1
P(at  least  once  1)  =  - + - · - · - +  
- · -
· - + ...  
---
3 
323 
32 
3 
1 
1 
1 
3 +  9 +  27 ... 
1  
1  
1
_3__ ]_ __ 
1-~-~-2 
since  the  sum  is  a  geometric  series.  
For  the  query  never _1, the  explanations  in  K-
are  pairwise  
incompatible,  so  the  probability  of never _1 can  be  computed  as  
2 1 
2 1 2 1 
P(never _1) 
-·-+-·-·-·-+ 
3 2 
3 2 3 2 
(~·~)2·~·~+ ...  = 
1 
1 
1 
1 
3 +  9 +  27 ... = 2"  
This  is  expected  as  never _1 ="-'at_least_once_l.  
We now want to show that every program is well-defined, i.e., it has a count­
able set of countable explanations that is covering for each query. In the 
following, we consider only ground programs that, however, may be denu­
merable, and thus they can be the result of grounding a program with function 
symbols. 
Given two sets of composite choices K1 and K2, define the conjunc­
tion K1 ® K2 of K1 and K2 as K1 ® K2 =  {A;l u  A;2IA;1 E K1, A;2 E 
K2, consistent(A;l  u  A;2) }. It  is easy to see that WK1Q9K2  =  WK1  n  WK2 •  

3.2  Infinite  Covering  Set  of Explanations  101 
Similarly to [Vlasselaer et al., 2015, 2016], we define parameterized inter­
pretations and an fpppP  operator that are generalizations of interpretations 
and the fppP  operator for normal programs. Differently from [Vlasselaer 
et al., 2015, 2016], here parameterized interpretations associate each atom 
with a set of composite choices rather than with a Boolean formula. 
Definition  23  (Parameterized two-valued interpretations). A  parameterized 
positive two-valued interpretation Tr  for  a  ground  probabilistic  logic  pro­
gram  P  with  Herbrandbase  Bp  is  a  set  ofpairs  (a,Ka)  with  a  E  Bp  and  
Ka  a  set  of  composite  choices,  such  that  for  each  a  E  Bp  there  is  only  
one  such  pair  (Tr  is  really  afunction).  A  parameterized negative two-valued 
interpretation Fa  for  a  ground  probabilistic  logic  program  P  with  Herbrand  
base  ßp  is  a  set  ofpairs  (a,  K~a) with  a  E  ßp  and  K~a a  set  of composite  
choices,  such  that for  each  a  E  ßp  there  is  only  one  such  pair.  
Parameterized two-valued interpretations form a complete lattice where the 
partialorderisdefinedasl ~ Jif'lf(a,Ka)  E  I,(a,La)  E  J  :wKa  ~ WLa·  
The least upper bound and greatest lower bound of a set T  always exist and 
are 
lub(T) = { ( a,  u  Ka)la  E  Bp}  
IET,(a,Ka)EI  
and 
glb(T) = {(a, 
(8) 
Ka)la  E  ßp}.  
IET,(a,Ka)EI  
The top element T is 
{(a, {0})1a E ßp}  
and the bottom element l_  is 
{(a,0)laEßp}. 
Definition  24 (Parameterized three-valued interpretations). A  parameterized 
three-valued interpretation I  for a ground probabilistic logic program P  with  
Herbrandbase  Bp  is  a  set  oftriples  (a,  Ka,  K~a) with  a  E  Bp  and  Ka  and  
K  ~a sets  of composite  choices  such  that  for  each  a  E  Bp  there  is  only  one  
such  triple.  A  consistent parameterized three-valued interpretation I  is  such  
that  V(a,  Ka,  K~a) EI: WKa  n WK-a  = 0.  
Parameterized three-valued interpretationsform a complete lattice where the 
partial order is defined as I  ~ J  if V(a,  Ka,  K~a) EI, (a,  La,  L~a) E J  : 

102 
Semantics  with  Function  Symbols  
WKa  ~ WLa  and WK-a  ~ WL-a·  The least upper bound and greatest lower 
bound of a set T  always exist and are 
lub(T) = { ( a,  
u  
Ka,  
u  
K~a)laEßp} 
IET,(a,Ka,K-a)EI  
IET,(a,Ka,K-a)EI  
and 
glb(T) = {(a, 
(8) 
Ka,  
(8) 
K~a) I  a  E  ßp}.  
IET,(a,Ka,K-a)EI  
IET,(a,Ka,K-a)EI  
The top element T is 
{(a, {0}, {0})1a E  ßp}  
and the bottom element l_  is 
{(a, 0, 0)la E  ßp}.  
Definition  25 (OpTruePi(Tr)  and OpFalsePi(Fa)).  Fora  ground  pro­
gram P with rules  Randfacts F,  a two-valued parameterized positive inter­
pretation  Tr  with  pairs  (a,  La),  a  two-valued  parameterized  negative  inter­
pretation  Fa  with pairs  (a,  M  ~a), and a  three-valued parameterized interpre­
tation'I with  triples  (a,  Ka,  K~a), we  define  OpTruePi (Tr)  = { (a,  L~) Ia  E  
ßp}  where  
L~ ~ 
{{(a,0,1)}} 
ifa  E  F  
{ 
Ua<-h,  ... ,bn,~cl,···•~cmER((Lh U  Kb1)  ® · · · 
ifa  E  ßp\F  
®(Lbn  U  Kbn)  ® K~q ® • • • ® K~cm) 
and  OpFalsePi (Fa)  = { ( a,  M~) Ia  E  ßp}  where  
{ { ( a,  0,  0)}} 
ifa  E  F  
M' ~a 
 
=  
®a<-bl, 
X  
... ,bn,~Cl,... ,~cmER(( M~b 1 ® K  ~b 1) u  ... 
{ 
ifa  E  ßp\F 
u(M~bn ®K~bn) u  Kq  u  ...  u  Kcm)  
Proposition  2  (Monotonicity of OpTruePi  and OpFalsePi).  OpTruePi  
and  OpFalsePi  are  monotonic.  
Proof.  Let us consider Op  TruePi.  We have to prove that if Tr1  ~ Tr2,  
then OpTruePi(Trl)  ~ OpTruePi(Tr2).  Tr1 ~ Tr2  means that 
V(a,  La)  E  Tr1,  (a,  Ma)  E  Tr2  : WLa  ~ WMa·  

3.2  Infinite  Covering  Set  of Explanations  103 
Let ( a,  L~) be the elements of Op  TruePi (Tri) and ( a,  M~) the elements of 
OpTruePi ( Tr2). Wehave to prove that WL~ ~ WM~ 
If a  E  F,  then L~ = M~ = { { (a,  (),  1)} }. If a  E  ßp\F,  then L~ and M~ 
have the same structure. Since Vb  E  ßp  :  WLb  ~ WMb'  then WL~ ~ WM~. 
We can prove similarly that OpFalsePi  is monotonic. 
D 
Since OpTruePi  and OpFalsePi  are monotonic, they have a least fixpoint 
and a greatest fixpoint. 
Definition  26  (Iterated fixpoint for probabilistic programs). Fora  ground  
program  P,  let  IFPPP  be  defined  as  
IFPPP(I)  = {(a,Ka,K~a)l(a,Ka) E  lfp(OpTruePi),  
(a,  K~a) E  gfp( OpFalsePi) }. 
Proposition  3  (Monotonicity of IFPPP).  IFPPP  is  monotoniG.  
Proof.  Wehavetoprovethat,if'II :::;.I2,theniFPPP(II):::;;.  IFPPP(I2 ).  
LI  :::;;.  I2 means that 
\:f(a,  La,  L~a) E  LI,  (a,  Ma,  M~a) E  'I2: WLa  ~ WMa,  WL-a  ~ WM-a·  
Let (a,  L~, L~a) be the elements of IFPPP (II) and (a,  M~, M~a) the ele­
ments of IFPPP (I2). Wehave to prove that WL~ ~ WM~ and WL~a ~ WM!_a.  
This follows from the monotonicity of OpTruePi  and OpFalsePi  proved 
in Proposition 2 and from their monotonicity in  I  that can be proved as in  
Proposition 2. 
D 
So IFPPP  has a least fixpoint. Let WFMP(P)  denote lfp(IFPPP),  and let 
6  the smallest ordinal suchthat IFPPP  j  6  =  WFMP(P).  We refer to 6  as 
the depth  of P.  
Let us now prove that OpTruePi  and OpFalsePi  are sound. 
Lemma  4  (Soundness of OpTruePi).  Fora  ground  probabilistiG  logiG  
program  p  with  probabilistiG  jaGts  F  and  rules  n,  and  a  parameterized  
three-valued  interpretation  I,  let  L~ be  the  set  associated  with  atom  a  in  
OpTruePi  ja. For  every  atom  a,  total  GhoiGe  0',  and  iteration  a,  we  have:  
Wa  E  WLfi  ~ WFM(waii)  I=  a  
where  waii  is  obtained  by  adding  the  atoms  afor  whiGh  (a,  Ka,  K~a) EI 
and  Wa  E  WKa  to  Wa,  and  by  removing  all  the  rules  with  a  in  the  head  for  
whiGh  (a,  Ka,  K~a) EI and  Wa  E WK-a·  

104 
Semantics  with  Function  Symbols  
Proof.  Let us prove the lemma by transfinite induction: let us assume the 
thesis for all ß  <  a  and let us prove it for a.  If a  is a successor ordinal, then 
it is easily verified for a  E  F.  Otherwise assume Wu  E  wvg;  where 
L~ =  
u  
((L~1-
1 u  Kb1)  ® ... ® (L~n-l u  Kbn)®  
a<f-bt  , ... ,bn, .........  Cl  , ... ,-··vCmER  
K~c1 ®  · · · ®  
K~cm) 
This means that there is rule a  ~ b1, ... , bn,  ~ c1, ... , ~ Cm E R  such that 
Wu  E  wL~.-luKb; for i  = 1, ... , n  and Wu  E  WK-ci  for j  = 1 ... , m. By the 
inductive ~ssumption and because ofhow wa-II is built, then WFM(wuii)  F= 
bi  and WFM(wuii)  F=~cj so WFM(wuii)  F= a.  
If a  is a limit ordinal, then 
L~ =  lub({L~Iß <  a})  =  UL~ 
ß<a.  
If  Wu  E wv",  then there mustexist aß <  a  suchthat Wu  E WLß·  By the 
inductive as;umption, the hypothesis holds. 
a  
D 
Lemma 5 (Soundness of OpFalsePi).  For  a  ground  probabilistic  logic  
program  P  with  probabilistic  facts  F  and  rules  R,  and  a  parameterized  
three-valued  interpretation  I,  let  M;:,a  be  the  set  associated  with  atom  a  in  
OpFalsePi  ~ a.  For  every  atom  a,  total  choice  rJ,  and  iteration  a,  we  have:  
Wu  E  WM;;;a  ~ WFM(wuii)  F=~a 
where  Wu  li is  built  as  in  Lemma  4.  
Proof.  As before, we prove the lemma by transfinite induction: we assume 
that the thesis is true for all ordinals ß  <  a  and we prove it for a.  Consider a  a 
successor ordinal. If a  E  F  the statement is easily verified since probabilistic 
facts do not appear in the head of any rule. If a  rf; F  consider Wu  E  WM;;;a  
where 
Ma.  = 
(8) 
((M~b1 ® K~b1 ) u  ...  
~a 
a'f--bl , ... ,bn  )''-'Cl , ... ,"-'CmER 
u(M~bn ® K~bn) u  Kc1  u  ...  u  Kcm)  

3.2  Infinite  Covering  Set  of Explanations  105 
This means that, for each a  ~ b1, ... , bn,  ~ c1, ... , ~ Cm  E R  there exists 
an index i  suchthat Wu  E  wMa-1  K  
, or there exists an index j  suchthat 
-b;  n  
-b;  
Wu  E  WK-c  . By the inductive assumption and because ofhow Wu  I  I  is built, 
J  
either WFM(wu  I  I)  l=~bi or WFM(wu  I  I)  I=  Cj  hold, so WFM(wu  I  
I)  l=~a. 
Consider now a  a limit ordinal. Then, 
M;:a  = glb(M~a I  ß  <  a})  = nM~a· 
ß<a  
If Wu  E  w M;:;a,  for all ß  <  a  we have that Wu  E  w Mf!a.  By the inductive 
assumption, this implies that WF M  (w  I I)  I=  ~a. 
D 
To prove the soundness of IF P p'P,  we first need a Iemma regarding the model 
of a program obtained by a partial evaluation of the semantics. 
Lemma  6 (Partial evaluation). Fora  ground  normallogic program  P  and  a  
three-valued  interpretation  I=  (Ir,  IF)  suchthat  I~ WFM(P),  let  PIII  
be  dejined  as  the  program  obtained from  P  by  adding  all  atoms  a  E  Ir  and  
by  removing  all  rulesforatoms  a  E  IF.  Then  WFM(P)  = WFM(PIII).  
Proof  We first prove that WFM(P)  is a fixpoint of fppPIII.  Pickanatom 
a  E  OpTruefvFM(P) (Ir).  If  a  E  Ir.  then a  is a fact in PIII,  so it is in 
Op  True fJ~M (P)  (Ir).  Otherwise, there exists a rule a  ~ b1, ... , bn  in P  
suchthat bi  is true in WFM(P)  or bi  E  Ir  for i  =  1, ... , n.  Such a rule is 
.  
PIII 
also m PIII  so a  E  OpTrueWFM(P)(fr).  
Now pick an atom a  E  OpFalsefvFM(P)  (IF ). If a  E  JF,  then there are no 
rules for a  t in PIII,  so it is in OpFalsefJ~M(P) (IF ). Otherwise, for all rules 
a  ~ b1, ... , bn  in P,  there exists a bi  such that it is false in WF M  (P)  or bi  E 
IF.  The set of rules for a  in PIII  is the same, so a  E  OpFalsefJ~M(P) (IF ). 

106 
Semantics  with  Function  Symbols  
We can prove similarly that WFM(PIII)  is a fixpoint of IFPP.  
Since WFM(P)  isafixpointofiFpPIII, then WFM(PIII)  ~ WFM(P)  
because WFM(PIII)  is the least fixpoint of fppPIII.  Since WFM(PIII)  
is a fixpoint of IFPP,  then WFM(P)  ~ WFM(PIII).  So WFM(P)  
WFM(PIII).  
D  
The following lemma shows that fppp'P  is sound. 
Lemma  7 (Soundness of IFPPP).  Fora  ground  probabilistic  logic  pro­
gram  P  with  probabilistic  facts  F  and  rules  R,  let  K;;  and  K:!;a  be  the  sets  
associated  with  atom  a  in  fppp'P  ja. For  every  atom  a,  total  choice  rJ,  and  
iteration  a,  we  have:  
Wu  E  WK;t  ~ WFM(wu)  F=  a  
(3.2) 
Wu  E  WK~a ~ WFM(wu)  F"'a  
(3.3) 
Proof  We prove the thesis and the formula 
WFM(wu  I  fpppP  i  a)  =  WFM(wu)  
(3.4) 
by transfinite induction. Let us consider a successor ordinal a  and let us 
assume 
WFM(wu  I  fppp'P  j  CY- 1)  =  WFM(wu)  
(3.5) 
and that 
Wu  E  WKa-1  ~ WFM(wu)  F=  a  
a  
Wu  E  WK~-;;1 ~ WFM(wu)  F"'a  
From the soundness of OpTrueP'jpppPi(a-l)  and OpFalseP'jpppPj(a-l)  
we have that 
Wu  E  WLß  ~ WFM(wuiiFPP'P  j  (a- 1))  F=  a  
a  
Wu  E  wLr:_a  ~ WFM(wa-IIFPP'P  j  (a- 1))  F"'a  
where L~ is the formula associated to a  in OpTrueP'jpppPj(a-l)  i  ß  and 
Lf!..a  is the formula associated to a  in OpFalseP'jpppPj(a-l)  i  ß.  
Since this is true for all ß,  it is also true for the 1  and 5  such that 
OpTrueP'jpppPja-l  j  1 =  lfp( OpTrueP'jpppPja-l)  
OpFalseP'jpppPja-l  j  5  = gfp( OpFalseP'jpppPja-l)  

3.2  Infinite  Covering  Set  of Explanations  107 
so 
Wa  E  WK;t  ~ WFM(wa  I  IFPPP  j  (a- 1))  F= a   
Wa  E  WK;:;a  ~ WFM(wa  I  IFPPP  j  (a- 1))  F=~a  
Because of Equation (3.5), then 
Wa  E  WK;t  ~ WFM(wa)  F= a  
(3.6) 
Wa  E  WK;:;a  ~ WFM(wa)  F=~a 
(3.7) 
N ow let us prove that 
WFM(waiiFPPP  ja) =  WFM(wa)  
(3.8) 
Let Ia  = <Ir,  !p)  be a three-valued interpretation defined as Ir  = {alwa  E  
wx;;}  and !p  = {alwa  E  wx;;_J.  Then, for Equation (3.6), Va  E  Ir  : 
WFM(wa)  F= a  and, for Equation (3.7), Va  E  !p  : WFM(wa)  F=~ a.  
So'Ia  ~ WFM(wa).  
Since Wa  I  IFPPP  ja= waiiia, by Lemma 6 
WFM(wa)  =  WFM(waiiia)  =  WFM(wa  I IFPPP  ja). 
Let us now consider a limit ordinal a  and let us assume that 
Wa  E  WKß  ~ WFM(wa)  F= a  
(3.9)
a  
Wa  E  WK~a ~ WFM(wa)  F=~a 
(3.10) 
for all ß  <  CY  Then K;;  =  uß<a Kg  and K:!;a  =  uß<a K~a so Wa  E  
wxa  ~ :Jß: Wa  E  Wxß·  ForEquation (3.9), then WFM(wa)  F= a.  Moreover 
Wa  aE  WK;:;a  ~ :Jß: w:  E  wxg·  ForEquation (3.10), then WFM(wa)  F=~a. 
The proof of Equation (3.4) for a  limit ordinal is the same as for the case 
of successor ordinal. 
D 
The following lemma shows that JpppP  is complete. 
Lemma  8 (Completeness of IFP PP).  Fora  ground  probabilistiG  logiG  pro­
gram  P  with  probabilistiG  jaGts  F  and  rules  R,  let  K;;  and  K:!;a  be  the  sets  
assoGiated  with atom  a  in  IFPPP  j  a.  For  every  atom  a,  total  GhoiGe  (],  and  
iteration  a,  we  have:  
a  E  Jppwa  j  CY  ~ Wa  E  WK;t  
~a E Jppwa  j  CY  ~ Wa  E WK;;_a  

108 
Semantics  with  Function  Symbols  
Proof.  Let us prove it by double transfinite induction. If a  is a successor 
ordinal, assume that 
a  E  fppwu  j  (a- 1)  ~ Wu  E  Wx;;-1  
~a E  fppwu  j  (a- 1)  ~ Wu  E  WK~-;;1 
Let us perform transfinite induction on the iterations of OpTrue7J.pwut(a-l)  
and OpFalse7J.pwut(a-l)'  Let us consider a successor ordinal8: assume that 
a  E  OpTrue7J.pwut(a-l)  j  (8- 1)  ~ Wu  E  WL~-1 
~a E  OpFalse7J.pwut(a-l)  l  (8- 1)  ~ Wu  E  WM~7.1 
where (a,L~-
1 
) are the elements of OpTrue"jFPPPta-l  j  (8- 1) and 
(a,M!~
1 
) aretheelements of OpFalse"jFPPPta-l  l  (8  -1). Weprovethat 
a  E OpTrue7J.pwut(a-l)  i  8  ~ Wu  E WL~ 
(3.11) 
~a E OpFalse7J.pwut(a-l)  l  8  ~ Wu  E WMfa  
(3.12) 
Consider a.  If a  E  F,  it is easily proved. 
For the other atoms, a  E  OpTrue7J.pwut(a-l)  j  8  means that there is a 
rule a  ~ b1, ... , bn,  ~c1, ... , ~em suchthat for all i  = 1, ... , n  
bi  E  OpTrue7J.pwut(a-l)  j  (8- 1)  V  bi  E  fppwu  j  (a- 1)  
and for all j  = 1, ... , m,  ~ Cj  E  fppwu  j  (a - 1). For the inductive 
hypotheses for the inner and outer inductions, Vi : Wu  E  WL8-1  v  Wu  E  Wxa-1  
b;  
b;  
and Vj: Wu  E  wK~-;} so Wu  E  L~. Analogously for ~a. 
J  
If 8  is a limit ordinal, then L~ = UJ-t<Ö L~ and M!a  = ®J-t<O  Mi!:,a·  If 
a  E  OpTrue7J.pwut(a-l)  i  8,  then there exists a f--t  <  8  suchthat 
a  E  OpTrue7J.pwut(a-l)  j  f--t·  
For the inductive hypothesis, Wu  E  wL~" and so Wu  E  WL8.  
If ~a E  OpFalse7J.pwut(a-l)  l  8,  then, for all f--t  <  8,  
~a E  OpFalse7J.pwut(a-l)  l  f--t·  
For the inductive hypothesis, Wu  E  wM~" and so Wu  E  WM8  . 
~a 
~a 

3.2  Infinite  Covering  Set  of Explanations  109 
Since fppwu  j  a  is obtained from the fixpoints of OpTrue~;pwutCa- 1 ) 
and OpFalse~;pwutCa- 1 ), and equations (3.11) and (3.12) hold for all8, they 
also hold at the fixpoints and the thesis is proved. 
Consider a limit a.  Then Kg:  = Uß<a Kf  and K:?;a  = Uß<a Kea.  If 
a  E  fppwu  ja, then there exists aß <  a  suchthat a  E  fppwu  j ß.  For the 
inductive hypothesis, w(J"  E  WKß  so w(J"  E  w K"'.  Similarly for ~a. 
D  
a  
a  
We can now prove that fpppP  is sound and complete. 
Theorem  10  (Soundness and completeness of IFPPP).  Fora  ground  prob­
abilistiG  logic  program  P,  let  Kg:  and  K:?;a  be  the  sets  associated  with  atom  
a  in  fpppP  j  a.  For  every  atom  a  and  total  choice  (],  there  is  an  iteration  
ao  such  thatfor  all  a  >  ao  we  have:  
w(J"  E  WK;t  ~ WFM(w(J")  F= a  
(3.13) 
w(J"  E  WK;:;a  ~ WFM(w(J")  F=~a 
(3.14) 
Proof  The -
direction of equations (3.13) and (3.14) is Lemma 7. In  the 
otherdirection, WFM(w(J")  F= aimplies :lao'v'a:  a?  ao- IFPwu  i  a  F= 
a.  For Lemma 8, w(J"  E  WK;t·  WFM(w(J")  F=~a implies :lao'v'a:  a?  ao­
fppwu  j  a  F=~a. For Lemma 8, w(J"  E  WKa  . 
D 
-a  
We can now prove that every query for every sound program has a countable 
set of countable explanations that is covering. 
Theorem  11  (Well-definedness of the distribution semantics). Forasound  
ground  probabilistiG  logic  program  P,  JLp({wlw  E  Wp,w  F= a}) for  all  
a  E  Bp  is  well  defined.  
Proof  Let Kg  and K~a be the formulas associated with atom a  in fpppP  j  8  
where 8  is the depth of the pro gram. For the soundness and completeness of 
IFPPP,  then {wlw  E  Wp,  w  F= a} =  wK"·  
Each iteration of OpTrueP";pppPi;  and OpFalseP";FPPPiß  for all 
ß  generates countable sets of countable explanations since the set of rules is 
countable. So Kg  is a countable set ofcountable explanations and JLP  ( { w  I w  E  
Wp,  w  F= a}) is well defined. 
D 
Moreover, if the program is sound, for all atoms a,  wKo  = w~8 where 8  
is the depth of the pro gram, as otherwise there would exist a world w(J"  such 
that w(J"  rf;  wK"  and w(J"  rf;  wK"  . But w(J"  has a two-valued WFM so either 
a  
-a  

110 
Semantics with  Function  Symbols  
WFM(w()")  I=  a  or WFM(w()")  l=~a. In the first case, wO"  E  wKo  andin the 
latter, WO"  E  WK8  against the hypothesis. 
a  
To give a se;;antics to a program with function symbols in the other lan­
guages under the DS, we can translate it into ProbLog using the techniques 
from Section 2.4 and use the above semantics for ProbLog. 
3.3 Comparison  with  Sato  and  Kameya's  Definition  
Sato and Kameya [2001] define the distribution semantics for definite pro­
grams, i.e., programs without negative literals. They build a probability mea­
sure on the set of Herbrandinterpretations from a collection of finite distri­
butions. Let the set of ground probabilistic facts F  be {!I, h,  ... }  and let Xi 
be a random variable associated with fi  whose domain Vi  is {0, 1}. 
They define the sample space VF  as a topological space with the product 
topology as the event space suchthat each {0, 1} is equipped with the discrete 
topology. 
In  order to clarify this definition, let us introduce some topology termi­
nology. A  topology  on a set V  [Willard, 1970, page 23] is a collection w of 
subsets of V,  called the open  sets,  satisfying: (t-1) any union of elements of w  
belongs to w,  (t-2) any finite intersection of elements of w belongs to w,  (t-3) 
0  and V  belong to w.  We say that (V,  w)  is a topological  space.  The discrete  
topology  of a set V  [Steen and Seebach, 2013, page 41] is the powerset JPl(V) 
ofV. 
Theinfinite  Cartesian  product  of sets 'l/Ji  for i  = 1, 2, ... is 
CIJ 
X  'l/Ji  =  {(s1, s2, ...)lsi E  'l/Ji,  i  =  1, 2, ...} 
i=1 
A product  topology  [Willard, 1970, page 53] on theinfinite Cartesian product 
X ~ 1 Vi  is a set containing all possible unions of open sets of the form 
X ~ 
1  Vi  where (p-1) Vi  is open Vi and (p-2) for all but finitely many i, 
vi  =  Vi.  Sets satisfying (p-1) and (p-2) are called cylinder  sets.  There exists 
a countable number of them. 
So VF  =  X ~dO, 1}, i.e., it is an infinite Cartesian product with Vi  =  
{0, 1} for all i. Sato and Kameya [2001] define a probability measure 1JF  over 
the sample space VF  from a collection of finite joint distributions PJn)  (X1  = 
k1, ... , Xn  =  kn) for n):  1 suchthat 

3.3  Camparisan  with  Sato  and  Kameya  's  Definition  111 
0 ~ P~n)(X1 =  k1, o o o ,Xn  =  kn) ~ 1 
.L:k1 , ... ,kn p~n)(X1 =  k1, o o o ,Xn  =  kn) =  1 
{ L:;kn+l P~n+l) (X1 =  k1, o o o, Xn+l  =  kn+l) =  P~n) (X1 =  k1, o o o, Xn  =  kn) 
(3o15) 
The last equation is called the consistency  condition  or compatibility  con­
ditiono  The Kolmogorov  consistency  theorem  [Chow and Teicher, 2012, page 
194] states that, if the distributions Pf) (X 1 = k1, 0 0 0 , Xn  = kn) satisfy the 
compatibility condition, there exists a probability space (V F,  w F,  TJF)  where 
TJF  is a unique probability measure on w F·  the minimal (J-algebra containing 
open sets of VF,  suchthat for any n,  
TJF(X1  = k1, o o o ,Xn  = kn) = P~n)(X1 = k1, o o o ,Xn  = kn)o 
(3016) 
P~n) (X1 =  k1, o o o, Xn  =  kn) is defined by Sato and Kameya [2001] as 
P~n) (X1 = k1, o o o, Xn  = kn) = 7r1 °  o o o · 1fn  
where 1ri = rri if ki = 1 and 1ri = 1 -
rri if ki = 0, with rri the annotation 
of fact k  This definition clearly satisfies the propetlies in Equation (3.15). 
The distribution Pf) (X1 = k1, 0 0 0, Xn  = kn) is then extended to a 
probability measure over the set of Herbrand interpretations of the whole 
program. Let ßp  be {a1, a2, ...} and let Yi be a random variable associated 
with ai  whose domain is {0, 1}. Moreover, let ak  =  a  if k =  1 and ak  ="'a  
if k =  0. Vp  istheinfinite Cartesian product Vp  =  X ~ 1 {0, 1 }. 
Measure TJF  is extended to TJP  by introducing a series of finite joint dis­
tributions P~n)(Y1 = k1, ... ,Yn  = kn) for n  = 1, 2, ... by setting 
[a~
1 1\  ..  01\  a~n]F = {v E  VFilhm(v) I=  a~
1 1\0  0.1\  a~n} 
where lhm(V)  is the least Herbrand model of n  u  Fv'  with Fv  =  {  fi  I Vi  =  1}. 
Then let 
P~n)(Y1 = k1, o o., Yn  = kn) = TJF([a~
1 1\  o o o 1\  a~n]F) 
Sato and Kameya state that [a~
1 1\  
1\  a~n]Fis TJpmeasurable and that, by 
0 
0. 
definition, P~n) satisfy the compatibility condition 
.2: P~n+
1
)(Y1 = k1, ... , Yn+1 = kn+l) = P~n\Y1 = k1, · · ·, Yn  = kn) 
kn+l 
Hence, there exists a unique probability measure TJP  over Wp  which is an 
extension of TJF.  

112 Semantics  with  Function  Symbols  
In  order to relate this definition to the one of Section 3.2, we need to 
introduce some more terminology on O"-algebras. 
Definition  27  (Infinite-dimensional product O"-algebra and space). For  any  
measurable  spaces  (Wi,  Oi), i  = 1, 2, ..., define  
00 
00 
g  = u { X  Wi  lwi E  oi' 1 ~ i  ~ m  and Wi  = wi'  i  >  m} 
m=1 i=1 
00 
(g)ni = O"(g)  
i=1 
00 
00 
00 
X  (Wi,  ni) =  (X  wi,  (8) ni) 
i=1 
i=1 
i=1 
Then  X ~ 1 (Wi,  Oi) istheinfinite-dimensional product space and  (8)~ 1 Oi 
istheinfinite-dimensional product O"-algebra This  definition  generalizes  Def­
inition  15 for  the  case  of infinite  dimensions.  
It  is clear that if wi  =  {0, 1} and oi =  IP'({0, 1}) for all i, then g  is the set of 
all possible unions of cylinder sets, so it is the product topology on X ~ 1 Wi  
and 
00 
X  (Wi,  Oi) = (V.r,  W.r) 
i=1 
because, according to [Chow and Teicher, 2012, Exercise 1.3.6], \]!  .r  is the 
minimal O"-algebra generated by cylinder sets. So the infinite-dimensional 
product space and the topological space (V.r,  \]!  .r)  coincide. 
An infinite Cartesian product p  =  X ~ 1 vi is consistent  if it is different 
from the empty set, i.e., if l/i  -=1- 0  for all i  =  1, 2, .... We can establish a 
bijective map '"YF  between infinite consistent Cartesian products p  =  X ~ 1 vi 
and composite choices: '"'f.r(  X ~ 1 vi) =  {  (Ji,  0,  ki) lvi =  {ki}}. 
Lemma  9  (Elements of \]!.ras Countahle Unions). Each  element  of\ll .r  can  
be  written  as  a  countable  union  of possibly  infinite  Cartesian  products:  
l  
1/J  =  
(3.17) 
j=1 u 
Pj  
where  l  E  [0, oo], Pj  
X~1l/i and  l/i  E  {{0}, {1}, {0, 1}} for  all  i  
1, 2, .... 

3.3  Camparisan  with  Sato  and  Kameya's  Definition  113 
Proof.  We will show that w 
F  and <I>,  the set of all elements of the form of 
Equation (3.17), coincide. Given a 'lj;  for the form of Equation (3.17), each PJ  
can be written as a countable union ofcy linder sets, so it belongs to w F.  Since 
w Fis a (J-algebra and 'lj;  is a countable union, then 'lj;  E  w Fand <I>  ~ w F·  
We can prove that <I>  is a (J-algebra using the same technique of Lemma 
2, where Cartesian products replace composite choices. <I>  contains cylinder 
sets: even if each PJ  must be consistent, l  can be 0 so 'lj;  can be the empty set. 
As w Fis the minimal (J-algebra containing cylinder sets, then w 
F  ~ <I>.  
D 
So each element 'lj;  of W F  can be written as a countable union of consistent 
possibly infinite Cartesian products. 
Lemma  10  (rF is Bijective). Consider  thefunction  rF: WF- Dp  dejined  
by  r:F('l/J) = WK  where  'lj;  = u;=l Pj  and  K  = u;=l hF(Pj)}  with  l  E 
[0, oo]. Then  r  F  is  bijective.  
Proof.  Immediate because /F  is bijective. 
D  
Theorem  12  (Equivalence with Sato and Kameya's definition). Probability  
measure  f.tp  coincides  with  TJP  for  definite  programs.  
Proof.  Consider (X1 = k1, ... , Xn  = kn) and let K  be 
{{(JI,~,kl), ... ,(Jn,~,kn)}}. 
Then K  = rF({(kl, ... 'kn, Vn+l,  .. .)lvi E  {0, 1}, i  = n  +  1, ...}) and f.tp  
assigns probability 1r1 ·... ·1rn  to K,  where 1ri = Ili if ki = 1 and 1ri = 1-Ili 
otherwise. 
So f.tp  is in accordance with PJn).  But Pf)  can be extended in only one 
way to a probability measure TJF  and there is a bijection between w Fand Dp,  
so f.tp  is in accordance with TJF  on all w F.  
Now consider C  = af1  1\  ...  1\  a~n. Since JpppP  j  5  is suchthat Kg  
and K~a are countable sets of countable composite choices for all atoms a,  
we can compute a covering set of explanations K  for C  by taking a finite 
conjunction of countable sets of countable composite choices, so K  is a 
countable set of countable composite choices. 
Clearly, P~n) (Y 1 =  k1, ... , Y n  =  kn) coincides with f.tp(wx  ).  But P~n) 
can be extended in only one way to a probability measure TJp,  so f.tp  is in 
accordance with TJP  on all Wp  when Pisa definite program. 
D 


4   
Hybrid  Programs  
The languages presented in Chapter 2 allow the definition of discrete random 
variables only. However, some domains naturally include continuous random 
variables. Probabilistic logic programs including continuous variables are 
called hybrid.  
In this chapter, we discuss some languages for hybrid programs together 
with their semantics. 
4.1  Hybrid  Problog  
Hybrid ProbLog [Gutmann et al., 20lla] extends ProbLog with continuous 
probabilistic facts of the form 
(X,  cp)  :: f  
where X  is a logical variable appearing in the atom f  and cp  is an atom 
specifying a continuous distribution, such as, for example, gaussian(O,  1)  
to indicate a Gaussian distribution with mean 0 and standard deviation 1. 
Variables X  of this form are called continuous  variables.  
A Hybrid ProbLog program P  is composed of definite rules R  and facts 
F  = Fd  u  Fe  where Fd  are discrete probabilistic facts as in ProbLog and 
Fe  are continuous probabilistic facts. 
Example  46  (Gaussian mixture - Hybrid ProbLog). A  Gaussian  mixture  
model  is  a  way  to  generate  values  of a  continuous  random  variable:  a  discrete  
random  variable  is  sampled  and,  depending  an  the  sampled  value,  a  different  
Gaussian  distribution  is  selected  for  sampling  the  value  of  the  continuous  
variable.  
A  Gaussian  mixture  model  with  two  components  can  be  expressed  in  
Hybrid  ProbLog  as  [Gutmann  et  al.,  20lla]:  
115 

116 
Hybrid  Programs  
0.6 :: heads.   
tails  ~ ~heads.  
(X,  gaussian(O,  1)) :: g(X).   
(X,  gaussian(5,  2)) :: h(X).   
mix(X)  ~ heads,g(X).   
mix(X)  ~ tails,  h(X).   
pos  ~ mix(X), above(X,  0).  
where,  for  example,  (X,  gaussian(O,  1))  ::  g(X)  is  a  continuous  probabilis­
tic  facts  stating  that  X  follows  a  Gaussian  with  mean  0  and  standard  devia­
tion  1.  The  values  of X  in  mix(X)  are  distributed  according  to  a  mixture  of  
two  Gaussians.  The  atom  pos  is  true  if X  is  positive.  
A number of predicates are defined for manipulating a continuous variable 
X:  
• below(X,  c) succeeds if X  <  c where c is a numeric constant; 
• above(X,  c) succeeds if X  >  c where c is a numeric constant; 
• ininterval(X,  c1, c2) succeeds if X  E [c1, c2] where c1 and c2 are 
numeric constants. 
Continuous variables cannot be unified with terms or used in Prolog compar­
ison and arithmetic operators, so in the example above, it is not possible to 
use expressions g(O),  (g(X),  X  >  0), (g(X),  3 *X+  4 >  4) in the body of 
rules. The first two, however, can be expressedas g(X),  ininterval(X,  0, 0) 
and g(X),  above(X,  0), respectively. 
Hybrid ProbLog assumes a finite set of continuous probabilistic facts 
and no function symbols, so the set of continuous variables is finite. Let us 
indicate the set with X =  {X1, ... , Xn}. defined by the set of atoms for 
probabilistic facts F  =  {!I, ... , fn}  where each fi  is ground except for 
variable xi.  An  assignment X =  {xl, ... 'Xn} to X defines a Substitution 
Bx  =  {XI/x1, ... Xn/xn}  and, in turn, a set of ground facts FOx.  
Given a selection fJ  for discrete facts and an assignment x to continuous 
variables, a world w(}",x  is defined as 
w(}",x  =Ru {!BI(!,(),  1) E  rJ} u F()x  
Each continuous variable Xi is associated with a probability density Pi  (Xi). 
Since all the variables are independent, p(x)  =  0~= 1 Pi(xi)  is ajoint proba­
bility density over X. p(X)  and P( fJ)  then define a joint probability density 
over the worlds: 
p(w(}",x)  = p(x)  n  rri 
(/;,O,O)E(J"  1- rri 
(/;,O,l)E(J" 
n 

4.1  Hybrid  ProbLog  117 
The probability of a ground atom q  different from the atom of a con­
tinuous probabilistic fact is then defined as in the DS for discrete programs 
as 
P(q)  
p(q,  WO",x)d!JdX  = 
I  
O"E8p,xEJF!.n  
I  
P(qlwO",x)p(wO",x)d!JdX  =  
O"E8p,xEJF!.n  
I  
p( WO",x)d!Jdx  = 
O"ESp  ,xEJF!.n :w".,x l=q  
O"~p LEJF!.n:w".,xl=q  p( WO",x)dx  
where Sp  is the set of all selections over discrete probabilistic facts. If the 
set { ( IJ,  x) IIJ  E  Sp,  x  E  JRn  : wO",x  F=  q} is measurable, then the probability 
is well defined. Gutmannet al. [201la] prove that, for each selection IJ,  the 
set {xlx  E JRn  : wO",x  F=  q} is an n-dimensional interval I  = [a1, b1] x 
... x [an,  bn]  of ]Rn  where ai  and bi  can be -oo and +oo, respectively, for 
i  = 1, ... , n.  The probability that X  E  I  is then given by 
Ibn 
P(X  E  I)  =  
h  . . .  
p(x)dx   
(4.1)
I 
a1  
an  
The proof is based on considering a discretized  theory  Pn  obtained as 
Pn  =   Ru Fd  u {below(X,  C), above(X,  C), ininterval(X,  Cl, C2)} u 
{f{X/f}I(X,q)) ::JE  Fe} 
The discretized program is a regularProbLog program, so we can consider 
its worlds. In each world, we can derive the query by SLD resolution and 
keep track of the instantiations of the facts for the comparison predicates 
used in  each proof. This yields a set of comparison facts that defines an 
n-dimensional interval. So we can compute the probability of the query in  
the world with Equation (4.1). By summing these values for all proofs, we 
get the probability of the query in the world of the ProbLog program. The 
weighted sum of the probabilities in the worlds over all worlds of Pn  gives 
the probability of the query, where the weights are the probabilities of the 
worlds according to the discrete facts. 
Example  47  (Query over a Gaussian mixture - Hybrid ProbLog). For  Ex­
ample  46,  the  discretized  program  is  

118 
Hybrid  Programs  
0.6 :: heads.   
tails  ~ ~heads.  
g(g(X)).   
h(h(X)).   
mix(X)  ~ heads,g(X).   
mix(X)  ~ tails,  h(X).   
pos  ~ mix(X),  above(X,  0).  
below(X,  C).  
above(X,  C).   
ininterval(X,  Cl, C2).  
which  is  a  ProbLog  program  with  two  worlds.  In  the  one  containing  heads,  
the  only  proof  for  the  query  pos  uses  the  fact  above(g(g(X)),  0), so  the  
probability  of pos  in  this  world  is  
P(posiheads)  = Loo p(x)dx  = 1- F(O,  0, 1) 
where  F(x, J-l,  17) is  the  Gaussian  cumulative  distribution  with  mean  11  and  
standard  deviation  O".  Therefore,  P(posiheads)  =  0.5. 
In  the  world  not  containing  heads,  the  only  proof  for  the  query  pos  
uses  thefact  above(h(h(X)),O),  so  the  probability  ofpos  in  this  world  is  
P(posl  ~heads) =  1- F(O,  5, 2) ~ 0.994. So  overall  
P(pos)  =  0.6 · 0.5 +  0.4 · 0.994 ~ 0.698 
This approach for defining the semantics also defines a strategy for perform­
ing inference. However, Hybrid ProbLog imposes severe restrictions on the 
programs, not allowing the use of continuous variables in expressions possi­
bly involving other variables. 
4.2  Distributional  Clauses  
Distributional Clauses (DCs) [Gutmann et al., 2011c] aredefinite clause with 
an atom h  ~ 'D  in the head, where h  is a term, ~ is a binary predicate used in 
infix notation, and 'D  is a term specifying a discrete or continuous probability 
distribution. Foreach ground instance (h  ~ 'D  ~ b1 ,  ...  ,  bn)B.  where ()  is a 
substitution over the Herbrand universe of the logic pro gram, a distributional 
clause defines a random variable h()  with the distribution indicated by '[)()  
whenever all the bi()  hold. The term 'D  can be non-ground, i.e., distribution 
parameters can be related to conditions in the body, similarly to flexible 
probabilities, see Section 2.7. 

4.2  Distributional  Clauses  119 
The reserved functor ~/1 is used to represent the outcome of a random 
variable: ~d. for example, indicates the outcome of random variable d.  The 
set of special predicates 
dist_rel  = { dist_eq/2,  dist_lt/2,  dist_leq/2,  dist_gt/2,  dist_geq/2}  
can be used to compare the outcome of a random variable with a constant 
or the outcome of another random variable. These predicates are assumed 
to be defined by ground facts, one for each true atom for each of the pred­
icates. Terms of the form ~(h)  can also be used in other predicates with 
some restrictions, e.g., it does not make sense to unify the outcome of a 
continuous random variable with a constant or another random variable as 
the probability that they are equal has measure 0. The predicate rov=/2 is used 
to unify random variables with terms: hrov=v  means ~(h) =  v,  which is true 
iff the value of the discrete random variable h  unifies with v.  For continuous 
random variables, "'=/2 can be used to unify random variables with logical 
variables. 
A DC programs P  is composed by a set of definite clauses R  and a set 
of DCs C.  A world  of Pis  the programRuF where Fis  the set of ground 
atoms for the predicates in dist_rel  that are true for each random variable h()  
defined by the program. 
Let us now see two examples. 
Example  48  (Gaussian mixture - DCs). The  Gaussian  mixture  model  of  
Example  46  can  be  expressed  with  DCs  as  
coin  "' [0.6 : heads,  0.4 : tails].  
g  "' gaussian(O,  1).  
h  "'gaussian(5,  2). 
mix(X)  ~ dist_eq(~coin, heads),  grov=X.  
mix(X)  ~ dist_eq(~coin, tails),  hrov=X.  
pos  ~ mix(X),  dist_gt(X,  0). 
where,  for  example,  g  follows  a  Gaussian  with  mean  0  and  standard  devia­
tion  1.  
This  example  shows  that  Hybrid  ProbLog  programs  can  be  expressed  
inDCs.  
Example  49  (Moving people - DCs [Nitti et al., 2016]). 
The  following  
program  models  a  set  ofpeople  moving  on  a  realline:  
n  "'poisson(6).  
pos(P)  "' uniform(O,  M)  ~ nrov=N,  between(1,  N,  P),  
M  is  10 *  N.   
left(A,  B)  ~ dist_lt(~pos(A), ~pos(B)).  

120  Hybrid  Programs  
where  between(1,  N,  P)  is  a  predicate  that,  when  Pis  not  bound,  generates  
all  integers  between  1  and  N,  and  is/2 is  the  standard  Prolog  predicate  for  
the  evaluation  of expressions.  
The  first  clause  defines  the  number  of people  n  as  a  random  variable  that  
follows  a  Poisson  distribution  with  mean  6.  The  position  of  each  person  is  
modeled  with  the  random  variable  pos(P)  defined  in  the  second  clause  as  
a  continuous  random  variable  uniformly  distributed  from  0  to  M  = 10n 
(that  is,  10  times  the  number  of people),  for  each  integer  person  identifier  
P  such  that  1 :S;  P  :S;  n.  lf  the  value  of  n  is  2,  for  example,  there  will  
be  two  independent  random  variables  pos(1)  and  pos(2)  both  with  distribu­
tion  uniform(O,  20). The  last  clause  defines  relation  left/2  between  people  
positions.  
A DC program must satisfy some conditions to be considered valid. 
Definition  28 (Valid program [Gutmann et al., 2011c]). A  DC program  Pis  
called  valid if the  following  conditions  are  fulfilled:  
(VI)  In  the  relation  h  "' 'D  that holds  in the least fixpoint  of a pro gram,  there  
is  a  functional  dependency  from  h  to  'D,  so  there  is  a  unique  ground  
distribution  'D  for  each  ground  random  variable  h.  
(V2)   The  program  is  distribution-stratified,  that  is,  there  exists  a  function  
rank(·)  that  maps  ground  atoms  to  N  and  satisfies  the  following  prop­
erties:  ( 1)  for  each  ground  instance  of a  clause  h  "' 'D  ~ b1, ... bn,  it  
holds  that  rank(h  "' 'D) >  rank(bi)  for  all  i;  (2)  for  each  ground  
instance  of a  regularprogram  clause  h  ~ b1, ... bn,  it  holds  rank(h)  ? 
rank(bi)  for  all  i;  and  (3)  for  each  ground  atom  b  that  contains  (the  
name  of)  a  random  variable  h,  rank(b)  ? rank(h  ,...., 'D) (with  h  ,...., 'D  
being  the  head  of  the  distribution  clause  defining  h).  The  ranking  is  
extended  to  random  variables  by  setfing  rank(h)  = rank(h  ,...., 'D). 
(V3)  The indicator functions ( see below) for all ground probabilistic facts are  
Lebesgue-measurable.  
(V4)  Each  atom  in  the  least  fixpoint  can  be  derived  from  a  finite  number  of  
probabilistic  facts  (finite  support  condition  [Sato,  1995] ).  
We now define the series of distributions Pf)  as in Sato and Kameya's defini­
tion ofthe DS ofEquation (3.15). We consider an enumeration {!I, h,  ... }  of 
atoms for the predicates in dist_rel  suchthat i  <  j  ~ rank(fi)  :S;  rank(fj)  
where rank(·)  is a ranking function as in Definition 28. We also define, for 
each predicate rel/2  E  dist_rel,  an indicator function: 

4.2  Distributional  Clauses  121 
1 if rel(XI,  X2) is true, 
ffez(XI,  X2) 
{ 0 if rel(XI,  X2) is false 
I~ez(XI, X2) 
1.0- ffez(XI,  X2) 
We define the probability distributions Pj;)  over finite sets of ground facts 
JI,  ...  ,  fn  using expectations of indicator functions. Let Xi  E  {1, 0} for i  = 
1, ... , n  be truth values for the facts fi,  ...  ,  f n.  Let { rvi,  ....rvm}  be the set 
ofrandom variables these n  facts depend upon, ordered such thatifrank(rvi)  <  
rank(rvj ), then i  <  j,  and let fi  = reli(til,  ti2). Lete-I = {(rvi)/VI,  ...  ,  
(rvm)/Vm}  be an antisubstitution that replaces evaluations of the random 
variables with real variables for integration. Then the probability distributions 
P~n) are given by 
P~n)(JI =XI,···,  fn  = Xn)  = 
J 
E[I:~h 
· · · 
J 
(tu, ii2), · · ·, I:Jn  (tni,  tn2)] =  
1:~h (tue-I, h20-I) ... 1::zn  (tnie-I,  tn2e-I)  
d'Drv1  (VI) · · · d'Drvm  (Vm)  
(4.2) 
For valid DC programs, the following proposition can be proved. 
Proposition  4  (Valid DC Program [Gutmann et al., 2011c]). Let  P  be  a  valid  
DC  pro gram.  P  dejines  a  probability  measure  Pp  over  the  set  of jixpoints  of  
operator  Tw  were  w  is  a  world  ofP.  Hence,  for  an arbitrary  formula  q  over  
atoms,  P  also  dejines  the  probability  that  q  is  true.  
The semantics of DCs coincides with that of hybrid ProbLog on programs 
that satisfy the constraints imposed by the latter. 
An alternative view ofthe semanti es can be given by means of a stochastic 
Tp  operator, STp,  extending the Tp  operator of Definition 1 to deal with 
probabilistic facts dist_rel(ti,  t2). We need a function READTABLE(·) that 
evaluates probabilistic facts and stores the sampled values for random vari­
ables. READTABLE applied to probabilistic fact dist_rel(h,  t 2 ) retums the 
truth value of the fact evaluated on the basis of the values of the random 
variables in the arguments. The values are either retrieved from the table or 
sampled according to their distribution the first time they are accessed. In the 
case they are sampled, they are stored for future use. 

122 Hybrid  Programs  
Definition  29 (STp  operator [ Gutmannet al., 2011c]). Let  P  be  a  valid  DC  
program.  Startingfrom  a  set  ofgroundfacts  I,  the  STp  operator  is  defined  as  
STp(I)  =  {hlh  ~ bi  ...  ,  bn  E ground(P)  1\  Vbi  : bi  EI v  
(4.3) 
bi  = dist_rel(ti,  t2) 1\  
(tj  =  ~h =? h"'  '[)EI  1\  READTABLE(bi) =  true)}  
Computing the least fixpoint of the STp  operator retums a possible model of 
the pro gram. The STp  operator is stochastic, so it defines a sampling process. 
The distribution over models defined by STp  is the sameasthat defined by 
Equation ( 4.2). 
Example  50  (STp  for moving people- DC [Nitti et al., 2016]). Given  the  
DC program  P  defined  in  Example  49,  a  possible  sequence  of application  of  
the  STp  operator  is  
STp  i  a  
Table  
0  
0  
{n,....,  poisson(6)}  
{n  = 2} 
{n,....,  poisson(6),pos(1),....,  uniform(0,20),  
{n  = 2,pos(1) = 3.1, 
pos(2)  ,...., uniform(O,  20)} 
pos(2)  = 4.5} 
{n  "'poisson(6),pos(1)  "'uniform(0,20),  
{n  = 2,pos(1) = 3.1, 
pos(2)  "' uniform(O,  20), left(1,  2)} 
pos(2)  = 4.5} 
Nitti et al. [2016] proposed a modification ofDCs where the relation sym­
bols are replaced with their Prolog versions, the clauses can contain negation, 
and the STp  operator is defined slightly differently. 
The new version ofthe ST p  operator does not use a separate table for stor­
ing the values sampled for random variables and does not store distribution 
atoms. 
Definition  30 (STp  operator [Nitti et al., 2016]). Let  P  be  a  valid  DC  
pro gram.  Startingfrom  a  set  of groundfacts  I,  the  STp  operator  is  defined  as  
STp(I)  = {h  = vlh"'  'D  ~ bi,  ...  ,  bn  E  ground(P)  1\  Vbi  : 
bi  EI v  bi  = dist_rel(ii,  t2) 1\  ti  = VI  EI 1\  t2 = v2 EI 1\  
dist_rel(vi,  v2) 1\  v  is  sampledfrom  'D} u 
{ hl  h  ~ bi,  ... , bn  E  ground(P)  1\  h  -=!- (r  "' 'D) 1\  Vbi  : 
bi  EI v  bi  = dist_rel(l},  t2) 1\  ti  =VI  EI 1\  
t2 =  v2 EI 1\  dist_rel(vi,v2)}  

4.2  Distributional  Clauses  123 
where  dist_rel  is  one  of  =, <, :S;, >, ?. In  practice,  for  each  distributional  
clause  h  ~ V  ~ b1 ... , bn,  whenever  the  body  b1 ... , bn  is  true  in  I,  a  
value  v  for  random  variable  h  is  sampled from  the  distribution  V  and  h  = v  
is  added  to  the  interpretation.  Similarly,  for  deterministic  clauses,  ground  
atoms  are  added  whenever  the  body  is  true.  
We can provide an alternative definition of world. A world is obtained in a 
number of steps. First, we must distinguish the logical variables that can hold 
continuous values from those that can hold values from the logical Herbrand 
universe. We replace the discrete logical variables with terms from the Her­
brand universe in all possible ways. Then we sample, for each distributional 
clause h  ~ V  ~ b1  ...  ,  bn  in the resulting program, a value for h  from 
distribution V  and replace the clause with h~=v ~ b1 ... , bn.  
If we compute the least fixpoint ofTw  for a world w  sampled in this way, 
we get a modelthat is the same as the one that is the least fixpoint of STp  if all 
random variables are sampled in the same way. So the least fixpoint of STp  
is a model of at least one world. The probability measure over models defined 
by STp  of Definition 30 coincides with that defined by STp  of Definition 29 
and Equation (4.2). Nitti et al. [2016] call worlds the possible models but we 
prefer to use the word world for the sampled normal programs. 
Example  51  (STp  for moving people- DC [Nitti et al., 2016]). 
Given the  DC program P  defined in Example  49 and the sequence  of ap­
plications  ofthe  STp  operator  of Definition  29,  the  corresponding  sequence  
of applications  for  STp  of Definition  30  is  
STp  i  0 = 0   
STp  j  1 = {n  = 2}  
STp  i  2 = {n = 2,pos(1) = 3.1,pos(2) = 4.5}  
STp  i  3 = {n = 2,pos(1) = 3.1,pos(2) = 4.5, left(1,  2)}  
STp  i  4 =  STp  i  3 =  lfp(STp)  
so  {n  = 2, pos(1)  = 3.1, pos(2)  = 4.5, left(1,  2)} is  a  possible  model  or  a  
model  of a  world  of the  pro gram.  
Regarding negation, since programs need tobe stratified tobe valid, nega­
tion poses no particular problem: the STp  operator is applied at each rank 
from lowest to highest, along the same lines as the perfect model semantics 
[Przymusinski, 1988]. 
Given a DC program P  and a negative literal l  =  ~ a,  to determine its 
truth in P,  we can proceed as follows. Consider the interpretation I  obtained 

124 Hybrid  Programs  
by applying the STp  operator until the least fixpoint for rank(l)  is reached 
( or exceeded). If a  is a non-comparison atomic formula, I  is true if a  ~ I  
and false otherwise. If a  is a comparison atom involving a random variable 
r  such as l  = (r~=val), then l  is true whenever ?JVal'  : r  = val'  E  I  with 
val  =I=  val'  or ~val' : r  = val'  E  I,  i.e., r  is not defined in I.  Note that I  is the 
least fixpoint for rank(r  ~V) (or higher), thus ~val' : r  = val'  EI implies 
that r  is not defined also in the following applications of the STp  operator 
and so also in the possible model. 
Example  52 (Negation in DCs [Nitti et al., 2016]). Consider  an  example  
where  we  draw  nballs  balls  with  replacement  from  an  urn  containing  an  
equal number  of red,  blue,  and black balls.  nballs follows a Poisson distri­
bution  and  the  color  of each  ball  is  a  random  variable  uniformly  distributed  
over  the  set  {red,  blue,  black }: 
nballs  ~ poisson(6).  
color(X)  ~ uniform([red,  blue,  black])  +­
nballs:::::::.N,  between(1,  N,  X).  
not_red  +---
~color(2) :::::.red.  
not_red  is  true  in  those  possible  models  where  the  value  of color(2)  is  not  
red  or  in  those  in  which  color(2)  is  not  dejined,  for  example,  when  n  =  1. 
4.3  Extended  PRISM  
Islam et al. [20 12b] proposed an extension of PRISM that includes continuous 
random variables with a Gaussian or gamma distribution. 
The set_sw  directives allow the definition of probability density func­
tions. For instance, set_sw(r,  norm(Mu,  Var))  specifies that the outcomes 
of random processes r  have Gaussian distribution with mean Mu  and vari­
ance Var.  
Parameterized families of random processes may be specified, as long as 
the parameters are discrete-valued. For instance, 
set_sw(w(M),  norm(Mu,  Var))  
specifies a family of random variables, with one for each value of M.  As in 
PRISM, the distribution parameters may be computed as functions of M.  
Moreover, PRISM is extended with linear equality constraints over reals. 
Without loss of generality, we assume that constraints are written as linear 

4.3  Extended  PR/SM  125 
equalities ofthe form Y  = a1 · X1 +  ...  +an·  Xn  +  b  where ai  and bare all 
fioating point constants. 
Example  53 (Gaussian mixture- Extended PRISM). The  Gaussian  mixture  
model  of Example  46  can  be  expressed  with  Extended  PR/SM  as  
mix(X)  +-- msw(coin,  heads),  msw(g,  X). 
mix(X)  +-- msw(coin,  tails),  msw(h,  X) 
values(coin,  [heads,  tails]).  
values(g,  real).  
values(h,  real),  
+-- set_sw(coin,  [0.6, 0.4]).  
+-- set_sw(g,  norm(O,  1)).   
+-- set_sw(h,  norm(5,  2)).  
Let us now show an example with constraints. 
Example  54  (Gaussian mixture and constraints- Extended PRISM). Con­
sider  a  factory  with  two  machines  a  and  b.  Each  machine  produces  a  widget  
with a continuous feature.  A  widget is produced by machine a  with probability  
0.3  and  by  machine  b  with  probability  0.7. If the  widget  is  produced  by  ma­
chine  a,  the  feature  is  distributedas  a  Gaussian  with  mean  2.0  and  variance  
1.0.  If  the  widget  is  produced  by  machine  b,  the  feature  is  distributed  as  a  
Gaussian  with  mean  3.0  and  variance  1.0.  The  widget  is  then  processed  by  
a  third  machine  that  adds  a  random  quantity  to  the  feature.  The  quantity  is  
distributedas  a  Gaussian  with  mean  0.5  and  variance  1.5.  This  is  encoded  by  
the  program:  
widget(X)  +­
msw(m,  M),  msw(st(M),  Z),  msw(pt,  Y),  X=  Y  +  Z.   
values(m,  [a,  b]).   
values(st(_),  real).   
values(pt,  real).   
+-- set_sw(m,  [0.3, 0.7]).  
+-- set_sw(st(a),  norm(2.0,  1.0)).  
+-- set_sw(st(b),  norm(3.0,  1.0)).  
+-- set_sw(pt,  norm(0.5,  0.1)).  
The semantics extends the DS for the discrete case by defining a probability 
space for the msw  switches and then extending it to a probability space for the 
entire program using the least model semantics of constraint logic programs 
[Jaffar et al., 1998]. 

126 
Hybrid  Programs  
The probability space for the probabilistic facts is constructed from those 
of discrete and continuous random variables. The probability space for N  
continuous random variables is the Borel (J-algebra over JRN  and a Lebesgue 
measure on this set is the probability measure. This is combined with the 
space for discrete random variables using the Cartesian product. The prob­
ability space for facts is extended to the space of the entire program using 
the least model semantics: a point in this space is an arbitrary interpretation 
of the program obtained from a point in the space of the facts by means of 
logical consequence. A probability measure over the space of the program is 
then defined by using the measure defined for the probabilistic facts alone. 
The semantics for programs without constraints is essentially equivalent to 
the one of DC. 
The authors propose an exact inference algorithm that extends the one 
of PRISM by reasoning symbolically over the constraints on the random 
variables, see Section 12.1. This is permitted by the restrictions on the types 
of distributions, Gaussian and gamma, and on the form of constraints, linear 
equations. 
4.4  cplint  Hybrid  Programs  
cplint handles continuous random variables with its sampling inference 
module. The user can specify a probability density on an argument Var  of an 
atom a  with rules of the form 
a  : Density  ~ Body  
where Density  is a special atom identifying a probability density on variable 
Var  and Body  (optional) is a regular clause body. Density  atoms can be: 
• uniform(  Var,  L,  U): Var  is uniformly distributed in [ L,  U].  
• gaussian( Var,  M ean,  Variance):  Var  follows a Gaussian distribution 
with parameters Mean  and Variance.  The distribution can be multivari­
ate if Mean  is a list and Variance  a list of lists representing the mean 
vector and the covariance matrix, respectively. In  this case, the values of 
Var  are lists of real values with the same length as that of M  ean.  
• dirichlet(  Var,  Par):  Var  is a list of real numbers following a Dirichlet 
distribution with parameters a  specified by the list Par.  
• gamma(  Var,  Shape,  Scale):  gamma distribution with parameters 
Shape  and Scale.  

4.4  cplint  Hybrid  Programs  127 
•  beta(  Var,  Alpha,  Beta):  beta distribution with parameters Alpha  and 
Beta.  
• poisson( Var,  Lambda):  Poisson distribution with parameter Lambda.  
• binomial( Var,  N,  P):  binomial distribution with parameters N  and P.  
• geometric(  Var,  P):  geometric distribution with parameter P.  
For example 
g(X)  : gaussian(X,  0, 1). 
states that argument X  of g(X)  follows a Gaussian distribution with mean 0 
and variance 1, while 
g(X):  gaussian(X,  [0,0], [[1,0], [0, 1]]). 
states that argument X  of g(X)  follows a Gaussian multivariate distribution 
with mean vector [0, 0] and covariance matrix 
[ ~ ~ ]   
Example  55 (Gaussian mixture- cplint). Example  46  ofa  mixture  oftwo  
Gaussians  can  be  encoded  as1:  
heads  : 0.6 ; tails  : 0.4. 
g(X)  : gaussian(X,  0, 1). 
h(X)  : gaussian(X,  5, 2). 
mix(X)  ~ heads,  g(X).  
mix(X)  ~ tails,  h(X).  
The  argument  X  of mix(X)  follows  a  distribution  that  is  a  mixture  of two  
Gaussians,  one  with  mean  0  and  variance  1  with  probability  0.6  and  one  with  
mean  5  and  variance  2  with  probability  0.4.  
The parameters of the distribution atoms can be taken from the probabilistic 
atom. 
Example  56  (Estimation of the mean of a Gaussian- cplint). The  pro­
gram2  
value(I,  X) ~ mean(M),  value(I,  M,  X). 
mean(M)  : gaussian(M,  1.0, 5.0). 
value(_,  M,  X) : gaussian(X,  M,  2.0). 
1  https://cplint.eu/e/gaussian_mixture.pl  
2 https://cplint.eu!e/gauss_mean_est.pl  

128 Hybrid  Programs  
states  that,  for  an  index  I,  the  continuous  variable  X  is  sampled from  a  Gaus­
sian  whose  variance  is  2  and  whose  mean  M  is  sampled from  a  Gaussian  with  
mean  1  and  variance  5.  
This  program  can  be  used  to  estimate  the  mean  of a  Gaussian  by  querying  
mean( M)  given  observations for  atom  value( I,  X)  for  different  values  of I.  
Any operation is allowed on continuous random variables. 
Example  57 (Kaiman filter- cplint). A  Kaimanfilter [Harvey,  1990]  is  a  
dynamical  system,  i.e.,  a  system  that  evolves  with  time.  At  every  integer  time  
point  t,  the  system  is  in  a  state  S  which  is  a  continuous  variable  and  emits  one  
value  V  =  S  + E, where E  is an error that follows a probability distribution  
that  does  not  depend  on  time.  The  system  transitions  to  a  new  state  N  extS  
at  time  t  +  1, with  N  extS  = S  +  X  where  X  is  also  an  error  that  follows  
a  probability  distribution  that  does  not  depend  on  time.  Kaimanfilters  have  
a  wide  variety  of applications,  especially  in  the  estimation  of trajectories  of  
physical  systems.  Kaimanfilters  differ  from  Hidden  markov  models  (HMMs)  
because  the  state  and  output  are  continuous  instead  of discrete.  
The  program  below3,  adapted  from  [Islam  et  al.,  2012b ],  encodes  a  
Kaimanfilter  in  cplint:  
kf(N,  0)  ~ init(S),  kf _part(O,  N,  S,  0).  
kj _part(I,  N,  S,  [VIRO])  ~I< N,  Nextl  is  I+  1, 
trans(S,  I,  N  extS),  emit(N extS,  I,  V),  
kf _part(Nexti,  N,  NextS,  RO).  
kf _part(N,  N,_s,  0).  
trans(S,  I,  N  extS)  ~ 
{NextS  =:=  E  +  S}, trans_err(I,  E).  
emit(S,I,  V)  ~{V=:= S  +X}, obs_err(I,  X).  
init(S)  : gaussian(S,  0, 1). 
trans_err(_,  E)  : gaussian(E,  0, 2). 
obs_err(_,  E)  : gaussian(E,  0, 1). 
kf(N,  0)  means  that  the  filter  run  for  N  time  points  produced  the  output  
sequence  0.  
Continuous  random  variables  appear  in  arithmetic  expressions  (in  clauses  
for  trans/3  and  emit/3).  lt  is  often  convenient,  as  in  this  case,  to  use  CLP(R)  
constraints  as  in  this  way  the  expressions  can  be  used  in  multiple  directions  
and  the  same  clauses  can  be  used  both  to  sample  and  to  evaluate  the  weight  
of the  sample  on  the  basis  of evidence  ( see  Section  12.3 ).  For  example,  the  
3 https://cplint.eule/kalman_filter.pl 

4.4  cplint  Hybrid  Programs  129 
expression  {N  extS  =: =  E  +  S}  can  be  used  to  compute  the  value  of any  
variable  given  values  for  the  other  two.  
The semantics is given in terms of a stochastic Tp  operator as the one ofDCs. 
Definition  31 (STp  operator - cplint). Let  P  be  a  program  and  
ground(P)  be  the  set  of all  instances  of clauses  in  P  with  allvariables  in  
the  body  replaced  by  constants.  Starfing  from  a  set  of  ground  facts  I,  the  
STp  operator  returns  
STp(I)  =  {h'lh: Density  ~ b1, ... , bn  E ground(P)  1\  Vbi  : bi  EI 1\  
h'  =  h{V ar jv}  with  V  ar  the  continuous  variable  of h  and  
v  sampled from  Density}  u  
{hiDist  ~ b1, ... , bn  E ground(P)  1\  Vbi  : bi  EI 
with  h  sampled from  discrete  distribution  Dist}  
Dif.ferently  from  STp  of Definition  30,  there  is  no  need for  special  treatment  
for  the  atoms  in  the  body,  as  they  are  alllogical  atoms.  Foreach  probabilistic  
clause  h  : Density  ~ b1 ... , bn  whenever  the  body  b1 ... , bn  is  true  in  I,  a  
value  v  for  the  continuous  variable  Var  of h  is  sampledfrom  the  distribution  
Density  and  h{ Var jv} is added to  the  interpretation.  Similarly  for discrete  
and  deterministic  clauses.  
cplint also allows the syntax of DC and Extended PRISM by translating 
clauses in these languages to cplint clauses. 
For DC, this amounts to replacing head atoms of the form 
p(h,  ...  ,  tn),...,.,density(parl,  ...  ,parm)  
with 
p(t1, ... , tn,  Var)  : density(  Var,  i},  ...  ,  tn)  
and body atoms ofthe formp(t1, ... , tn),...,.,=X  withp(t1, ... , tn,  X).  On the 
other band, terms of the form ~ (h)  for h  a random variable are not allowed, 
and the value of a random variable can be used by unifying it with a logical 
variable using h,...,.,=X.  
For extended PRISM, atoms of the form msw(p(t1,  ...  ,  tn),  val)  in the 
body of clauses defined by a directive such as 
~ set_sw(p(tl,  ...  ,  tn),  density(par1,  ...  ,parm)).  

130 
Hybrid  Programs  
are replaced by probabilistic facts of the form 
p(it, ... , tn,  Var):  density( Var,par1, ... ,parm)·  
The syntax of cplint allows a considerable freedom, allowing to express 
both constraints on random variables and a wide variety of distributions. 
No syntactic checks are made on the program to ensure validity. In  case, 
for example, random variables arenot sufficiently instantiated to exploit ex­
pressions for inferring the values of other variables, inference will retum an 
error. 
4.5 Probabilistic  Constraint  Logic  Programming  
Michels et al. [2013, 2015] and Michels [2016] proposed the langnage of 
Probabilistic constraint logic programming (PCLP) that allows continuous 
random variables and complex constraints on them. PCLP differs from hybrid 
ProbLog because it allows constraints that are more general than comparing 
a continuous random variable with a constant. In  this sense, it is more similar 
to DCs. However, DCs allow generative  dejinitions,  i.e., definition where a 
parameter of a distribution could depend on the value of another one, while 
PCLP doesn't. PCLP provides an alternative definition for the semantics of 
hybrid programs that is not based on a stochastic Tp  operator but on an ex­
tension of Sato's DS. Moreover, PCLP allows the specification of imprecise 
probability distributions by means of credal  sets:  instead of specifying ex­
actly the probability distribution from which a variable is sampled, a set of 
probability distributions (a credal set) is specified that includes the unknown 
correct distribution. 
In  PCLP, a program Pis split into a set of rules Rand a set of facts F.  
The facts define the random variables and the rules define the truth value of 
the atoms in the Herbrandbase of the program given the values of the random 
variables. The set of random variables is countable X =  {X1, X2, ... } and 
each has a range Rangei  that is not limited tobe Boolean but can be a general 
set, for example, N or ffi.  or even ffi.n.  
The sample space Wx  is given by 
Wx  =  Range1  x  Range2  x  ...  
The event space Ox is user-defined but it should be a O"-algebra. A probability 
measure J.Lx  is also given suchthat (Wx,  Ox, J.Lx) is a probability space. 
A constraint is a predicate cp  that takes values of the random variables 
as arguments, i.e., it is a function from { (x1, x2, ...) lx1 E Range1,  x2 E 

4.5  ProbabilistiG  Constraint  Logic  Programming  131 
Range2,  ... }  to {0, 1 }. Given a constraint tp, its constraint  solution  space  
C S  S  ('P)  is the set of samples where the constraint holds: 
CSS(tp)  = {x E  Wxi'P(x)} 
Definition  32 (Probabilistic Constraint Logic Theory [Michels et al., 2015]). 
A  Probabilistic  Constraint  Logic  Theory  P  is  a  tuple  
(X, Wx,  Ox, f.tx,  Constr,  R)  
where  
•  Xis  a  countableset  ofrandom  variables  {X1, X2, ...} each  with  range  
Rangei;  
•  Wx  =  Range1  x Range2  x ... is  the  sample  space;  
• Ox is  the  event  space,  a  rJ-algebra;  
•  t-tx  is  a  probability  measure  such  that  (Wx,  Ox, t-tx)  is  a  probability  
space;  
•  Gonsir  is  a  set  of  constraints  closed  under  conjunction,  disjunction,  
and  negation  such  that  the  constraint  solution  space  of each  constraint  
is  included  in  Ox:  
{CSS('P)I'P  E Constr}  E Ox; 
•  R  is  a  set  of logical  rules  with  constraints:  
h  ~ h, ... ,ln,  ('PI(X)), ... , ('Pm(X)) 
where  'Pi  
E  
Constr  and  ('Pi(X)) is  called  constraint atom 
for  1 ~ i  ~ m.  
Random variables are expressed similarly to DC, for example, 
time_comp1 "' exp(1)  
state that time_comp1 is a random variable with an exponential distribution 
with rate 1. 
Example  58  (Fire on a ship [Michels et al., 2015]). Suppose  a  fire  breaks  
out  in  a  compartment  of a  ship.  After  0.  75  minutes  also  the  next  compartment  
will  be  on  fire.  After  1.25  minutes  the  fire  will  breach  the  hull.  With  this  
information,  we  know  for  sure  that  if  the  fire  is  under  control  within  0.75  
minutes  the  ship  is  saved.  This  can  be  represented  as:  

132 Hybrid  Programs  
saved  ~ (time_comp1 <  0.75) 
In  detail,  the  previous  line  says  that  the  value  of  the  continuous  random  
variable  time_comp1 should  be  less  than  0.75  in  order for  saved  tobe  true.  
The  second  compartment  is  morefragile  than  the  first  one,  and  the  fire  
must  be  extinguished  within  0.625  minutes.  However,  for  the  firemen  to  reach  
the  second  compartment,  the  fire  in  the  first  one  must  be  under  control.  This  
means  that  bothfires  must  be  extinguished  in  0.75  +  0.625  =I.375  minutes.  
In  the  second  compartment Jour  people  can  work  simultaneously,  since  it  is  
not  as  isolated  as  the  first  one.  This  means  that  the  fire  will  be  extinguished  
Jour  times  faster.  We  can  encode  this  situation  with:  
saved  ~ (time_comp1 <  1.25),  
(time_comp1 +  0.25 · time_comp2 <  1.375)  
We  also  suppose  that  both  time  durations  to  extinguish  the  fire  are  exponen­
tially  distributed:  
time_comp1 ~ exp(1)  
time_comp2 ~ exp(1)  
Given  these  time  constraints  and  these  distributions,  we  want  to  know  the  
probability  that  the  ship  is  saved,  i.e.,  P(saved).  
This  example  is  a  probabilistic constraint  logic  theory  where  
X= {time_comp1, time_comp2}, 
Range1  =  Range2  =  ffi., the  constraint  language  includes  linear  inequal­
ities  and  the  probability  measure  makes  the  two  variables  independent  and  
distributed  according  to  an  exponential  distribution.  
A probability distribution over the logical atoms of the program (the elements 
of the Herbrand base ßp)  is defined as in PRISM: the logical atoms form a 
countableset ofBoolean random variables Y =  {Y1, Y2, ...}and the sample 
space is Wy  =  {  (y1, Y2, ...) IYi E {0, 1}, i  =  1, 2, ...}. 
According to Michels et al. [2015], the sample space Wy  is countable, 
and so the event space of the logic part of the program is defined as nR  =  
IP'(Wy) (the powerset ofthe sample space). However, Theorem 4 showed that 
Wy  is uncountable. 
The sample space for the entire theory is Wp  =  Wx  x Wy  and the event 
space is the product O"-algebra (see Definition 15), the O"-algebra generated by 
the products of elements of Ox and Oy: 
Op  =  nx Q9  Oy =  17({wx Xwylwx E nx,wy E Oy} 
We now define a probability measure f..LP  that, tagether with Wp  and Op,  
forms probability space (W p,  Op,  f..LP).  

4.5  ProbabilistiG  Constraint  Logic  Programming  133 
Given wx  E  Wx,  the set satisfiable(wx)  contains all the constraints 
from Constr  satisfied in sample wx.  Thus, wx  determines a logic theory 
Ru satisfiable(wx)  that must have a unique model denoted with Mp(wx).  
Then probability measure f.tp  (wp)  for wp  E  Op  is defined by considering the 
event of Ox identified by wp:  
f.tp(wp)  = f.tx({wxl(wx,wy) E  wp,Mp(wx)  F= wy}) 
(4.4) 
Michelset al. [2015] state that {wxl(wx,wy) E  wp,Mp(wx)  F= wy} is 
measurable, i.e., that it belongs to Ox; however, the statement is not obvious 
if Ox is not the powerset of Wx.  In  Chapter 5 we provide a proof of this 
statement, similar to the one of Section 3.2 for ProbLog. 
The probability of a query q  (a ground atom) is then given by 
P(q)  =  f.tp({wplwp  F= q}) 
(4.5) 
Ifwe define the solution  event  SE(q)  as 
SE(q)  = {wx E  WxiMp(wx)  F= q} 
then P(q)  = f.tx(SE(q)).  
Example  59  (Probability of fire on a ship [Michels et al., 2015]). Contin­
uing  Example  58,  wx  = (time_compi, time_comp2) = (xi, x2), wy  = 
saved  = YI with  Rangei  = Rangei  = [0, +oo), and  YI E  {0, 1}. So  from  
Equation  (4.5)  we  get  
P(saved)  = f.tp({(xi,x2,YI) E  WpiYI  = 1}) 
andfrom  Equation  (4.4):  
P(saved)  =  f.tx({(xi,x2)l(xi,x2,YI) E  Wp,Mp((xi,x2))  F= YI =  1}) 
The  solution  event  is  
SE(saved)  = { (xi, x2) lxi < 0.75 v (xi < 1.25 1\  XI+ 0.25 · x2 < 1.375)} 
so  
P(saved)  =  f.tx({(xi,x2)1xi < 0.75v(xi < 1.25Axi+0.25·x2 < 1.375)}) 
Since  the  two  constraints  'PI  =  XI < 0.75 and  'P2 =  XI < 1.25 1\  XI + 
0.25 · x2 < 1.375 arenot mutually exclusive,  we can use the formula  f.LX  ('PI v 
'P2) = f.tx('PI)  + f.tx(-.'PI  1\  'P2) with  
-,'PI 1\  'f?2 = 0. 75 < XI < 1.25 1\  XI + 0.25 · X2 < 1.375 = 
0.75 <XI < 1.25 1\  X2 < 5.5- 4XI 

134 Hybrid  Programs  
Knowing  that  X1 and  X2 are  distributed  according  to  an  exponential  distri­
bution  withparameter  1  (density  p(x)  = e-x),  we  get  
/-LX('Pd  =  Loo p(x1)I~1 (xl)dx1 
J-Lx(  ---,'Pl A  'P2) = Loo Loo p(x1)p(x2)J~'P1 Acp2  (x1, x2)dx1dx2 
where  1~ 1 (xl) and  I~'P 1 Acp2  (x1, x2) are  indicator functions  that  take  value  1  
if the  respective  constraint  is  satisfied  and  0  otherwise.  So  
10.75 
0 75
J-Lx('Pl) 
Jo 
p(x1)dx1 = 1 - e- · 
~ 0.53 
1.25 
(15.5-4xl 
)
/-LX  (---,'Pl 1\  'P2) 
p(x1) 
p(x2)dx2 dx1 =  
10.75 
0 
1.25 
e-x1 (1 _ e-5.5+4xl )dxl =  
10.75 
1.25 
e-x1 _ e-5.5+3xldx1 =  
10.75 
e-5.5+3·1.25 
e-5.5+3·0.75 
= -e-1.25 +  e-0.75 _ 
+  
~ 
3 
3 
0.14 
So  
P(saved)  ~ 0.53 +  0.14 ~ 0.67 
Proposition  5 (Conditions for exact inference [Michels et al., 2015]). The  
probability  of an  arbitrary  query  can  be  computed  exactly  if  the  following  
conditions  hold:  
1.   Finite-relevant-constraints  condition:  There  are  only  finitely  many  con­
straint atoms that are  relevant for each query atom ( see  Section  1.3)  and  
finding  them  and  checking  entailment  can  be  done  in  finite  time.  
2.   Finite-dimensional-constraints  condition:  Each  constraint puts  a  condi­
tion  only  on  afinite  number  ofvariables.  
3.   Computable-measure  condition:  It  is  possible  to  compute  the  probability  
of finite-dimensional  events,  i.e.,  finite-dimensional  integrals  over  the  
probability  density  of the  random  variables.  

4.5  ProbabilistiG  Constraint  Logic  Programming  135 
The example 
forever _sun(X)  +-- <Weatherx  = sunny),Jorever _sun(X  +  1)  
does not fulfill  the finite-relevant-constraints condition as the set of relevant 
constraints for forever _sun(O)  is <Weatherx  = sunny)  for X  = 0, 1, .... 
4.5.1  Dealing  with  imprecise  probability  distributions  
In  order to relax the conditions for exact inference of Proposition 5, Michels 
et al. [2015] consider the problern of computing bounds on the probability of 
queries. 
Credal sets, see also Chapter 6, are sets of probability distributions. They 
can be defined by assigning probability mass to sets of values without speci­
fying exactly how the mass is distributed over those values. For example, for 
a continuous random variable, we can assign some probability mass to the set 
of all values between 1  and 3. This mass can be distributed uniformly over the 
entire set or uniformly over only parts of it or distributed in a more complex 
manner. 
Definition  33  (Credal set specification). A  credal set specification C  is  a  
sequence  of  finite-dimensional credal set specifications C1, C2, ... Each  Ck  
is  a  finite  set  of probability-event  pairs  (Pl, w1), (P2, w2), ... , (Pn,  Wn)  such  
that,  for  each  Ck,  
1.  The  events  belang  to  a  finite-dimensional  event  space  0~ over  the  sam­
ple  space  Wx  =  Range1  x Range2  x ... x Rangek.  
2.  The  sum  ofthe  probabilities  is  1.0:  l:(p,w)ECk  p  =  1.0. 
3.  The  events  must  not  be  the  empty  set  V(p,  w)  E  Ck  : w  -=!=  0.  
Moreover,  Ck  must  be  compatible,  i.e.,  for  all  k,  Ck  = 7rk(Ck+l) where  
7rz  (ck)  for  l  <  k  is  defined  as  
7rz(Ck)  = { ( 
.l:  
p,  w')  Iw' E 
{7rz(w)l(p, w)  ECk}}  
(p,w  )ECk  ,1r1  (w)=w'  
and  7rz  (w)  is  the  projection of event  w  over  the  first  l  components  
7rz(w)  = {(xl, ... ,xz)l(xl, ... ,xz, ...) Ew} 
Each Ck  identifies a set of probability measures Y~ such that, for each 
measure 11x  E  Y~ on n~ and each event w  E  n~. the following holds: 
p::::;  J-lx(w)  ::::; 
p 
.L:  
.L:   
(p,1/J)ECk,1/J<;;;_w  
(p,1/J)ECk,1/Jnw#O  

136 
Hybrid  Programs  
In fact, the probability mass of events completely included in event w  cer­
tainly contributes to the probability of w,  so they are in its lower bound, while 
the probability of events with a non-empty intersection may fully contribute 
to the probability, so they are in its upper bound. 
Michelset al. [2015] show that, under mild conditions, a credal set spec­
ification C  identifies a credal set of probability measures Y  x  over the space 
(Wx,  Ox) suchthat all measures J.Lx  of Yx  agree with each Ci  in C. More­
over, this credal set can be extended to credal set Y  p  of probability measures 
over the whole program P,  which in tums originates a set P  of probability 
distributions over queries. 
Given a credal set specification for a program P,  we want to compute the 
lower and upper bounds on the probability of a query q  defined as: 
P(q)  =  
min J.Lp(q) 
J.LpEYp  
P(q)  =  
max J.Lp(q) 
J.LpEYp  
These bounds can be computed as indicated by the next proposition. 
Proposition  6  (Computation of probability bounds ). Given  a  finite-dimensi­
onal credal set specijication  Ck,  the lower  and upper probability bounds  of a  
query  q fulfilling  the  finite-dimensional-constraints  condition  are  given  by:  
P(q)   
p 
2:   
(p,w)ECk,wr:;;;_SE(q)  
P(q)   
p 
2:  
(p,w)ECk,wnSE(q)#0  
Example  60  (Credal set specification- continuous variables). Consider  ex­
ample  58  and  the  following  finite-dimensional  credal  set  specijication  
C2 ={  (0.49,{(xl,x2)IO~x1~1,0~x2~1}), 
(0.14, {(x1,x2)l1 ~ x1 ~ 2,0 ~ x2 ~ 1}), 
(0.07, {(x1,x2)l2 ~ x1 ~ 3,0 ~ x2 ~ 1}), 
(0.14, {(x1,x2)IO ~ x1 ~ 1,1 ~ x2 ~ 2}), 
(0.04, {(x1,x2)11 ~ x1 ~ 2,1 ~ x2 ~ 2}), 
(0.02, {(x1,x2)l2 ~ x1 ~ 3,1 ~ x2 ~ 2}), 
(0.07, { (x1, x2) IO ~ x1 ~ 1, 2 ~ x2 ~ 3} ), 

4.5  ProbabilistiG  Constraint  Logic  Programming  137 
(0.02, { (x1, x2) 11 ::::; x1 ::::; 2, 2 ::::; x2 ::::; 3} ), 
(0.01, {(x1, X2)12::::; Xl::::; 3, 2::::; X2::::; 3})} 
Figure  4.1  shows  how  the  probability  mass  is  distributed  over  the  (x1, x2) 
plane.  The  solution  event for  q  =  saved  is  
SE(saved)  =  {  (x1, x2) lx1 <  0.75 v (x1 <  1.25 1\  x1 +  0.25 · x2 <  1.375)} 
and  corresponds  to  the  area  of Figure  4.1  to  the  left  ofthe  solid  line.  We  can  
see  that  the  first  event,  0 ::::; x1 ::::; 1 1\  0 ::::; x2 ::::; 1, is  such  that  C  SS(O  ::::; 
x1 ::::; 1 1\  0 ::::; x2 ::::; 1) ~ SE(saved),  so  its  probability  is  part  ofthe  lower  
bound.  
The  next  event  1 ::::; x1 ::::; 2 1\  0 ::::; x2 ::::; 1 instead  is  not  a  subset  
of  SE(saved)  but  has  a  non-empty  intersection  with  SE(saved),  so  the  
probability  of the  event  is  part  of the  upper  bound.  
Event  2 ::::; x1 ::::; 3 1\  0 ::::; x2 ::::; 1 instead  has  an  empty  intersection with  
SE(saved),  so its probability is not part  of any bound.  
Ofthefollowing  events,  0::::; x1 ::::; 1 1\  1 ::::; x2 ::::; 2, 1 ::::; x1 ::::; 2 1\  1 ::::; 
x2 ::::; 2, and  0 ::::; x1 ::::; 1 1\  2 ::::; x2 ::::; 3 have  a  non-empty  intersection  with  
SE(saved)  while  the  remaining  have  an  empty  intersection,  so  overall  
P(q)  = 0.49 
P(q)  = 0.49 +  0.14 +  0.14 +  0.04 +  0.07 = 0.88 
Credal set specifications can also be used for discrete distributions when the 
information we have on them is imprecise. 
3 
0.02 
0.01
0.0, 
\  
X2 -
9.-14- -9.94-
-G:G-2-. 
\  
0.49 
0.07
p.14 
0 
0 
Xl 
3 
Figure 4.1 
Credal set specification for Examples 60 and 62. 

138 
Hybrid  Programs  
Example  61  (Credal set specification - discrete variables). Suppose  you  
have  a  model  with  a  single  random  variable  X1  with  Range1  = { sun,  rain}  
representing  the  weather  tomorrow.  A  credal  set  specification  may  consist  of  
C1 = { (0.2, { sun,  rain}),  (0.8, { sun})} 
The  probability  distributions  that  are  included  in  this  credal  set  are  those  of  
theform  
P(X1  =  sun)  
0.8 +ry   
P(X1  =  rain)  
0.2 -ry   
for  ry  E  [0, 0.2]. Thus,  the  probability  that  tomorrow  is  sunny  is  greater  than  
0.8  but  we  don  't  know  its  precise  value.  
In the general case of an infinite dimensional credal set specification, the 
following holds: 
P(q)  
lim 
2.: 
p 
k->00  
(p,w)ECk,wr:;;;_SE(q)  
P(q)  
lim 
2.: 
p 
k->00  
(p,w)ECk,wnSE(q)#0  
A consequence of this is that 
P(q)  = 1- P( ~q)  
P(q)  =  1- P(~q)  
We can now consider the problern of computing bounds for conditional prob­ 
abilities. We first define them. 
Definition  34  (Conditional probability bounds). The  lower  and  upper  condi­
tional  probability  bounds  of a  query  q  given  evidence  e  are  defined  as:  
P(qle)  
min P(qle) 
PEP 
P(qle)  
maxP(qle)
PEP 
Bounds for conditional probabilities can be computed using the following 
proposition. 

4.5  ProbabilistiG  Constraint  Logic  Programming  139 
Proposition  7  (Conditional probability bounds formulas [Michels et al., 2015]). 
The  lower and upper conditional probability bounds  of a query q are deter­
mined  by:  
P(q,  e)
P(qle)   
(4.6)
P  (q,  e) +  P  ("'q,  e) 
P(q,  e)
P(qle)   
(4.7)
P  (q,  e) +  P  ("'q,  e) 
Example  62 (Conditional probability bounds). Consider Example  60  and  
add  the  rule  e  ~ <time_comp2 <  1.5). 
Suppose  the  query  is  q  =  saved  and  the  evidence  is  e.  To  compute  the  lower  
and  upper  bounds  for  P(qle),  we  need  to  compute  lower  and  upper  bounds  
for  q  1\  e  and  "'q  1\  e.  The  solution  eventfor  SE(e)  is  
SE(e)  =  {(xi,x2)lx2 <  1.5} 
and  is  shown  in  Figure  4.1  as  the  area  below  the  dashed  line.  The  solution  
event for  q  1\  e  is  
SE(q  1\  e) =   {(xi,x2)lx2 <  1.5 1\  
(XI< 0.75 V (XI< 1.25 1\  XI+ 0.25 · X2 < 1.375))} 
and for  "'q  1\  e  is  
SE("'q  1\  e) =  {(xi,x2)lx2 <  1.51\ 
---,(XI < 0. 75 V (XI < 1.25 1\  XI + 0.25 · X2 < 1.375))} 
We  can  see  that  the  jirst  event,  0 ~ XI ~ 1 1\  0 ~ x2 ~ 1, is  such  that  
CSS(O  ~XI ~ 1  1\0  ~ x2 ~ 1)  <:;  SE(q  1\  e), so  its  probability  contributes  
to  P(q  1\  e) but  not  toP( "'q  1\  e). 
The  next  event  1 ~ XI ~ 2 1\  0 ~ x2 ~ 1 has  a  non-empty  intersec­
tion  with  SE(q)  and  is  included  in  SE(e),  so  the  probability  of  the  event  
contributes  to  P( q  1\  e) and  P( "'q  1\  e ). 
Event  2 ~ XI ~ 3 1\  0 ~ x2 ~ 1 instead  has  an  empty  intersection  with  
SE (q)  and  is  included  in  SE (e), so  it  contributes  to  P  ("' q  1\  e) but  not  to  
P(q  1\  e). 
The  event  0 ~ XI ~ 1 1\  1 ~ x2 ~ 2 has  a  non-empty  intersection  with  
SE(q  1\  e) and  an  empty  one  with  SE( "'q  1\  e), so  its  probability  is  part  of  
P(q  1\  e). 

140 Hybrid  Programs  
The  event  1 ~ XI ~ 2 1\  1 ~ x2 ~ 2 has  a  non-empty  intersection  
with  SE(q  1\  e) and  SE( ~q 1\  e), so  its  probability  is  part  of P(q  1\  e) and  
P( ~q 1\  e). 
The  event  2 ~ XI ~ 3 1\  1 ~ x2 ~ 2 has  a  non-empty  intersection  with  
SE( ~q 1\  e), so  it  contributes  toP( ~q 1\  e). 
The  remaining  events  are  included  in  SE (~e), so  they are  not part of any  
bound.  Overall,  we  have  
P(q  1\  e) = 0.49 
P(q  1\  e) =  0.49 +  0.14 +  0.14 +  0.04 =  0.81 
P( ~q 1\  e) = 0.07 
P( ~q 1\  e) =  0.14 +  0.07 +  0.04 +  0.02 =  0.27 
0.49 
(   I )  
0 64
p q  e  =  0.49 +  0.27 ~ ·
p  
= 
0.81 
~ 
2 
(qle)  
0.81 +  0.07 °·9 
Michels et al. [2015] prove the following theorem that states the conditions 
under which exact inference of probability bounds is possible. 
Theorem  13  (Conditions for exact inference of probability bounds [Michels 
et al., 2015]). The  probability  bounds  of an  arbitrary  query  can  be  computed  
in  finite  time  under  the  following  conditions:  
1.  Finite-relevant-constraints  condition:  as  condition  1  of Proposition  5. 
2.  Finite-dimensional-constraints  
condition:  
as  
condition  2  of  
Proposition  5. 
3.   Disjoint-events-decidability  condition:  for  two  finite-dimensional  events  
WI  and  w2 in  the  event  space  Dx,  one  can  decide  whether  they  are  
disjoint  or  not  ( WI  n  w2 =  0  ).  
Credal sets can be used to approximate continuous distributions arbitrarily 
well. One has to provide a credal set specification dividing the domain of the 
variable xi  in n  intervals: 
{ (P(h  <  Xi  <  ui),  h  < Xi <  ui),  ...  ,  
(P(ln  <  Xi  <  Un),  ln  <  Xi  <  Un)}  
with lj  ~ Uj,  lj  E  IR u  {-oo} and Uj  E  IR u  {+oo} for j  = 1, ... ,n. 
P(lj  <  xi  <  Uj)  must besuchthat P(lj  <  xi  <  Uj)  =  s~_i p(xi)dxi  
J  
F( Uj)  - F(lj)  where p(xi)  is the probability density of xi  and F(xi)  is its 

4.5  ProbabilistiG  Constraint  Logic  Programming  141 
cumulative distribution. The more intervals we provide, the better we approxi­
mate the continuous distribution. The probability bounds of the query provide 
the maximum error of the approximation. 
PCLP fixes a syntax for expressing probability distributions precisely and 
approximately with credal set specifications. 
PCLP allows the definition of random variables as 
(X1, ... , Xn)(Al,  ...  ,  Am)  ~ Density.  
where X1, ... , Xn  are random variables represented by terms, A1, ... , Am  
are logical variables appearing in X1, ... , Xn.  and Density  is a probability 
density, such as exponential,  normal,  etc. There is a different multidimen­
sional random variable 
(X1, ... , Xn)(tl, .. ·, tm)  
for each tuple of ground terms t1, ... , tm  replacing parameters A1, ... , Am.  
Variables X1, ... , Xn  can also be used individually in constraints, the set of 
them is 
{Xi(lt, ... , tm)li =  1, ... , n,  (t1, ... ,tm)  is a tuple ofterms}. 
Credal set (approximate) specifications are represented as 
(X1, ... , Xn)(A1,  ...  ,  Am)  ~ {Pl : <p1, ... ,pz  : <pz}  
where Pi  is a probability, IPi  is a satisfiable constraint, and .l:~=l Pi  =  1. The 
PiS and ipiS can include logical variables. 
Examples of specifications are 
temperature(Day)  
{0.2 : temperature <  0, 0.8 : temperature >  0}. 
temperature(Day)  
{0.2: temperature <  Day/1000,  
0.8: temperature >  Day/1000}.  
where Day  in the second specification must assume numeric values. 
In case the program contains multiple definitions for the same random 
variable, only the first is considered. 
PCLP actually defines a family of languages depending on the constraint 
language Constr:  a specific language is indicated with PCLP( Constr).  The 
constraint language must have some properties: infinite dimensional prob­
ability measures can be constructed from an infinite number of finite ones 

142 
Hybrid  Programs  
with increasing dimensionality and the satisfiability of constraints must be 
decidable, which is equivalent to decidability of the disjointness of events. 
An interesting instance is PCLP(R,D) where the constraints deal with real 
numbers (R) and discrete domains (D), with the restriction that constraints 
can include only variables of a single type, either real or discrete. 
The constraint theory R is the same as that of CLP over the reals CLP(R): 
variables can assume real values and the constraints consist of linear equal­
ities and inequalities. The constraint theory D is similar to that of CLP over 
finite domains CLP(FD): variables can assume discrete values and the con­
straints consist of set membership (E and ~) and equality (=  and #).  Differ­
ently from CLP(FD), the domains can be countably infinite. 
A PCLP program with some random variables defined approximately 
with formulas of the form 
(XI,  ...  ,Xn)(Al,···,Am)  ~{PI: i.pi,  ...  ,pz:  c.pz}  
defines a credal set specifications C =  {  C 1, C2 , ... } by combining the spec­
ification of the individual variables. We first fix the enumeration of random 
variables and denote the set of the definitions for the first k  random variables 
as Dk.  The credal set specification C ofthe program is defined as: 
Ck  = 7rk  ({(p,CSS(c.p))l(p,c.p)  E  ndEDkd})  
where the product of two random variable definitions d1 x d2 is defined as: 
d1 xd2 =  {(p1. P2, i.p1 1\  i.p2)l((p1, c.pi),  (p2, i.p2)) E d1 x  d2} 
For example, the following random variable definitions yield the credal set 
specification of Example 60: 
time_comp1 
{0.7: 0 ~ time_comp1 ~ 1, 0.2: 1 ~ time_comp1 ~ 2, 
0.1 : 2 ~ time_comp1 ~ 3} 
time_comp2 ~ {0.7: 0 ~ time_comp2 ~ 1, 0.2: 1 ~ time_comp2 ~ 2, 
0.1 : 2 ~ time_comp2 ~ 3} 
The solution  constraint  of a query q  is defined as: 
SC(q)  = 
f\c.p 
V   
cp<;;;_  Constr ,Mp (4>)  l=q  <pEcp  

4.5  ProbabilistiG  Constraint  Logic  Programming  143 
where Mp(</J)  is the model ofthe theory Ru {('P)I'P E  </J}. 
Function 
check  : Constr  ~ {sat,  unsat,  unknown}  
checks satisfiability of constraints and retums 
• sat:  the constraint is certainly satisfiable, i.e., there is a solution. 
• unsat:  the constraint is certainly unsatisfiable, i.e., there is no solution. 
• unknown:  satisfiability could not be decided, i.e., nothing is said about 
the constraint. 
If the constraint theory is decidable, unknown  is never retumed. 
Given function check,  we can compute the lower and upper probability 
bounds of a query q  taking into account only the first n  random variables from 
a PCLP program fulfilling the exact inference conditions, as 
P(q)  
(p,<p  )ECn,check( 1.: 
<p  1\  
p 
 ~SC(q))=unsat 
P(q)  
1.:   
p 
(p,<p )ECn  ,check( <p  1\  SC( q)  )=sat 
If the constraints are partially decidable, we get the following result 
P(q)  ? 
1.:  
p 
(p,<p  )ECn,check( <p  1\  ~SC(q))=unsat 
P(q)  ::::; 
(p,<p )ECn  ,check( 1.: 
<p  
p 
 1\  SC( q)  )#unsat 
Inference from a PCLP program can take three forms if Constr  is decidable: 
• exact computation of a point probability, if the random variables are all 
precisely defined; 
• exact computation of lower and upper probabilities, if the information 
about random variables is imprecise, i.e., it is given as credal sets; 
• approximate inference with probability bounds, ifinformation about ran­
dom variables is precise but credal sets are used to approximate contin­
uaus distributions. 
If Constr  is partially decidable, we can perform only approximate inference 
and obtain bounds in all three cases. In particular, in the second case, we 
obtain a lower bound on the lower probability and an upper bound on the 
upper probability. 

144 
Hybrid  Programs  
PCLP, despite the name, is only loosely related to other probabilistic logic 
formalisms based on CLP such as CLP(BN), see Section 2.9.2, or clp(pdf(y)) 
[Angelopoulos, 2003] because it uses constraints to denote events and define 
credal set specifications, while CLP(BN) and clp(pdf(y)) consider probability 
distributions as constraints. 

5   
Semantics  for  Hybrid  Programs  with  Function   
Symbols   
In  this chapter we prove that the semantics of PCLP given in Section 4.5 is 
well-defined also when function symbols are present, i.e., that each query 
can be assigned a probability. In other words, we prove that the solution 
event SE(q)  is measurable for every query q.  These results were presented 
in [Azzolini et al., 2021]. 
5.1  Examples  of  PCLP  with  Function  Symbols  
The following two examples use integers, that are representable only by using 
function symbols (for example, 0 for 0, s(O) for 1, s(  s(O)) for 2, ... ). 
Example  63 (Gambling). Consider  a  gambling  game  that  repeatedly  con­
sists  in  spinning  a  roulette  wheel  and  drawing  a  card  from  a  deck.  The  card  
is  reinserted  in  the  deck  after  each  play.  The  player  keeps  track  of the  final  
position  of  the  axis  of  the  wheel  (the  angle  it  creates  with  the  geographic  
east).  The  game  continues  until  the  player  draws  a  red  card.  
There are Jour available prizes that can be won,  depending on the position  
of  the  wheel  and  the  color  of  the  card:  prize  a  if  the  card  is  black  and  the  
angle  is  less  than  1r, prize  b  if  the  card  is  black  and  the  angle  is  greater  
than  1r, prize  c if  the  card  is  red  and  the  angle  is  less  than  1r and  prize  d  
otherwise.  We  describe  the  angle  of the  wheel  with  a  uniform  distribution  in  
[0, 27r), and  the  color  ofthe  card  with  a  Bernoulli  distribution  with  P(red)  = 
P(black)  = 0.5. In  this  program  there  is  a  random  variable  for  every  ground  
instantiation  of  the  ( anonymous)  logical  variable  in  the  random  variables  
card(_) and  angle(_). 
card(_) ~ {red: 0.5, black:  0.5}  
angle(_) ~ uniform(O,  21r)  
145 

146 
Semantics  for  Hybrid  Programs  with  Function  Symbols  
prize(O,  a)  +--- <card(O) = black),  <angle(O) <  n) 
prize(O,  b)  +--- <card(O) =  black),  <angle(O) ): 1r) 
prize(O,  c) +--- <card(O) =red),  <angle(O) <  1r) 
prize(O,  d)  +--- <card(O) =red),  <angle(O) ): 1r) 
prize(s(X),  a)  +--- prize(X,  _), <card(X) = black),  
<card(s(X)) = black),  <angle(s(X)) <  1r) 
prize(s(X),  b)  +--- prize(X,  _), <card(X) =  black),  
<card(s(X)) = black),  <angle(s(X)) ): 1r) 
prize(s(X),  c) +--- prize(X,  _), <card(X) =  black),  
<card(s(X)) =red),  <angle(s(X)) <  1r) 
prize(s(X),  d)  +--- prize(X,  _), <card(X) =  black),  
<card(s(X)) =red),  <angle(s(X)) ): 1r) 
at_least_once_prize_a  +--- prize(_X,  a)  
never _prize_a  +---"' at_least_once_prize_a  
Given  this  program,  P(at_least_once_prize_a)  is  the  probability  that  the  
player  wins  at  least  once  prize  a.  Similarly,  the  probability  that  the  player  
never  wins  prize  a  is  P(never _prize_a).  
Example  64  (Hybrid Hidden Markov Model). A  Hybrid  Hidden  Markov  
Model  (Hybrid  HMM)  is  the  combination  of a  Hidden  Markov  Model  (HMM,  
with  discrete  states)  and  a  Kaiman  Filter  (with  continuous  states).  For  ev­
ery  integer  time  point  t  the  system  is  in  a  state  [s(t), type(t)] described  
by  a  discrete  random  variable  type(t) taking  values  in  {a,  b},  and  a  con­
tinuous  variable  s(t) taking  values  in  R At  time  t,  the  model  emits  one  
value  v(t) =  s(t) +  obs_err(t), where  obs_err(t) is  an  error  that  follows  
a  probability  distribution  depending  an  type(t), a  or  b,  and  not  an  time.  At  
time  t'  =  t  +  1, the  system  moves  to  a  new  state  [s(t'), type(t')], where  
s(t') =  s(t) +  trans_err(t). Again,  trans_err(t) is  an  error  thatfollows  a  
probability  distribution  that  depends  an  type(t). Likewise,  type(t') depends  
an  type(t). We  use  the  random  variables  init and  type_init to  describe  the  
state  at  time  0.  All  the  random  variables,  except  init and  type_init, are  in­
dexed  by  an  integer  value  (time  step).  We  want  to  compute  the  probability  
that  the  value  emitted  at  time  step  1  is  Zarger  than  2  (predicate  ok/0).  As  
before,  there  is  a  random  variable  for  every  ground  instantiation  of the  terms  

5.2  Preliminaries  147 
indicating  the  random  variables.  
ok  ~ kf(2),  (v(1) >  2)  
kf(N)  ~ (s(O) = init), (type(O) = type_init), kf _part(O,  N)  
kf _part(I,  N) ~I< N,  Nextl  is  I+  1,  
trans(I,  N  exti),  emit(I),  
kf _part(N extl,  N)  
kf _part(N,  N) ~ N-=!=  0  
trans(I,  N  exti)  ~  
(type(I)  = a),  (s(Nexti)  = s(J) + trans_err_a(J)), 
(type(Nexti)  = type_a(NextJ)) 
trans(I,  N  exti)  ~  
(type(!)= b),(s(Nexti)  = s(J) +trans_err_b(J))  
(type(Nexti)  = type_b(NextJ))  
emit(I)  ~  
(type(!) = a),  (v(J) = s(J) + obs_err_a(J))  
emit(I)  ~  
(type(!) = b), (v(J) = s(J) + obs_err_b(J))  
init ~ gaussian(O,  1)  
trans_err_a(_) ~ gaussian(O,  2)  
trans_err_b(_) ~ gaussian(O,  4)  
obs_err_a(_) ~ gaussian(O,  1)  
obs_err_b(_) ~ gaussian(O,  3)  
type_init ~ {a  :  0.4, b  : 0.6}  
type_a(_) ~ {a  :  0.3, b  : 0. 7}  
type_b(_) ~ {a:  0.7, b:  0.3}  
5.2 Preliminaries  
In  this section we introduce some preliminaries. We provide a new defini­
tion of PCLP that differs from the one of Section 4.5 because we separate 
discrete from continuous random variables. The former are encoded using 
probabilistic facts as in ProbLog. Recall that with Boolean probabilistic facts 
it is possible to encode any discrete random variable (see Section 2.4). 

148 
Semantics  for  Hybrid  Programs  with  Function  Symbols  
A probabilistic constraint logic program is composed of a set of rules R,  a 
set of Boolean probabilistic facts F,  and a countableset ofcontinuous random 
variables X = X1, X2, ... Each Xi  has an associated Rangei  that can be IR 
or :!Rn.  The rules in R  define the truth value of the atoms in the Herbrandbase 
of the program given the values ofthe random variables. We define the sample 
space ofthe continuous random variables as Wx  = Range1  x Range2  x ... 
As previously discussed, the probability spaces of individual variables gen­
erate an infinite dimensional probability space (Wx,  Ox, t-tx). The updated 
definition of a probabilistic  constraint  logic  theory  follows. 
Definition  35 (Probabilistic Constraint Logic Theory - updated). A  proba­
bilistic  constraint  logic  theory  Pisa  tuple  (X, Wx,  Ox, t-tx,  Constr,  R,  F)  
where:  
•  X   is  a  countable  set  of  continuous  random  variables  {X 1, X  2, ...} , 
where  each  random  variable  Xi  has  a  range  Rangeiequal  to  IR or  :!Rn.  
• Wx  =  Range1  x Range2  x ... is  the  sample  space.  
• Ox is  the  event  space.  
•  t-tx  is  a  probability  measure,  i.e.,  (Wx,  Ox, t-tx) is  a  probability  space.  
•  Constr  is  a  set  of  constraints  closed  under  conjunction,  disjunction,  
and  negationsuchthat  Vcp E  Constr,  CSS(cp)  E  Ox, i.e.,  suchthat  
C  S  S  (cp) is  measurable  for  all  cp.  
• R  is a set  of rules with logical constraints  of the form:  
h  ~ h, ... ,ln,('PI(X)), ... ,(cpm(X)) where  li  is  a  literalfor  i  
1, ... , n,  'PJ  E  Constr  and  (cpj(X)) is  called  constraint atomfor  j  =  
1, ... ,m. 
• F  is  a  set  of probabilistic  facts.  
This definition differs from Definition 32 since we separate discrete and con­
tinuous probabilistic facts: X is the set containing continuous variables only, 
while F  is a set of discrete probabilistic facts. The probabilistic facts form a 
countableset of Boolean random variables Y =  {Y1, Y2, ... } with sample 
space Wy  = {(y1, y2, ...) I Yi  E {0, 1}, i  E 1, 2, ...}. The event space is the 
O"-algebra of set of worlds identified by countableset of countable composite 
choices: a composite choice"' = {(!I, (h,  YI),  (h,  (h,  Y2), ... } can be inter­
preted as the assignments Y1 =  Yl,  Y2 =  Y2, . . . if Y1 is associated to !I  fh,  
Y2 to f202 and so on. Finally, the probability space for the entire program 
(Wp,  Op,  f.LP)  is the product of the probability spaces for the continuous 

5.2  Preliminaries  149 
(Wx,  Ox, t-tx) and discrete (Wy,  Oy, f.LY) random variables, which exists 
in light of the following theorem. 
Theorem  14  (Theorem 6.3.1 from Chow and Teicher [2012]). Given  two  
probability  spaces  (Wx,  Ox, t-tx) and  (Wy,  Oy, f.LY  ), there  exists  a  unique  
probability  space  (W,  0,  f..t), called  the  product  space,  suchthat  W  = Wx  x 
Wy,  0 = Ox <8)0y, and  
t-t(wx  x  wy)  =  t-tx(wx)  · f.ty(wy)  
for  wx  E  Ox and  wy  E  Oy. Measure  t-t  is  called  the  product measure of t-tx  
and  f.ty,  and  it  is  also  denoted  by  t-tx  x  f.LY·  Moreover,  for  any  w  E  0,  we  
define  its  sections as  
w(X)(wx)  = {wy I  (wx,wy)  E  w} 
w(Y)(wy)  = {wx I  (wx,wy)  E  w}. 
Both  w(X)  (wx)  and w(Y) (wy)  are  measurable  according  to  (Wy,  Oy, f.LY) 
and  (Wx,  Ox, t-tx) respectively,  i.e.,  w(X)(wx)  E Oy and w(Y)(wy)  E Ox. 
Consequently,  f.ty(w(X)(wx)) and  t-tx(w(Y)(wy )) arereal  values.  
Measure  f.t  = t-tx  X  f.LY  for  every  w  E  n also  satisfies  
t-t(w)  = f  t-tx(w(Y)(wy))dt-ty  = f  f.ty(w(X)(wx))dt-tx· 
Jwy  
Jwx  
In  our case, Wp  =  Wx  x Wy,  and Op  =  Ox 0  Oy. We indicate with 
satisfiable( wx)  the set of constraints that are satisfiable given a valuation 
wx  of the random variables in X. A world  is an assignment of values to 
both the continuous and discrete random variables, i.e., w  =  (  wx,  wy)  with 
wx  E  Wx  and wy  E  Wy.  We say that a world w  satisfies  a constraint rp, 
and we write w  F= rp, if the values of the continuous random variables in the 
worlds satisfy the constraint. Starting from a world w  =  (  wx,  wy)  from W  p,  
a ground normallogic program P w  is composed of: 
• the groundings of the rules whose constraint are in the set satisfiable  
(wx),  with the constraints removed from the body, 
• the probabilistic facts associated to random variables Yi  with value 1. 
The well-founded model of w  E  Wp,  WFM(w),  is defined as the well­
founded model of Pw.  WFM(Pw).  and we require that it is two-valued: if 
this constraint is satisfied for every sample w  E  Wp,  we call the program 
so und.  
The probability of a query q  is then given by 
P(q)  =  f.tp({w  I w  E  Wp,  WFM(w)  f=  q}. 

150 Semanticsfor  Hybrid  Programs  with  Function  Symbols  
An explanation  for a query q  of a PCLP is a set of worlds Wi  where the 
query is true in every element of the set, i.e., Vw  E  wi,  WFM(w)  F=  q.  If 
every world w  suchthat WFM(w)  F=  q  belongs to the set, the set is termed 
covering.  A pairwise incompatible set w  = ui  Wi  is suchthat Vj =I=  k,  Wj  n  
Wk  = 0.  We define the probability of a query q  as the measure of a covering 
set of explanations: P(q)  = p,p( {w  I  w  F=  q} ). We are now ready to compute 
the probability the examples previously discussed. 
Example  65 (Pairwise incompatible covering set of explanations for Exam­
ple 63). Consider  Example  63.  We  represent  the  extraction  of a  black  card  
with  ft  =  black(_)  : 0.5. (!1,  (),  1) means  that  the  card  is  black  while  
(!1,  (),  0) means  that  the  card  is  not  black  (red).  We  associate  a  random  
variable  Yi  to  black(si(O)):  Yi  =  1 means  that  in  round  i  a  black  card  was  
picked.  The  query  at_least_once_prize_a  has  the  pairwise  incompatible  set  
of explanations  
w+  = w6  u  wi  u  ...  
and  w+  is  covering  for  it.  The  sets  wt  are  defined  as:  
w6  =  {(wx,wy) I wx  =  (xo,xl,  ... ),wy  =  (yo,YI,  ... ),  
xo  E  [0, 1f], Yo  = 1} 
wi  =  {(wx,wy) I wx  =  (xo,xl,  ... ),wy  =  (yo,YI,  ... ),  
xo  E  [1r,21r],yo = 1,x1 E  [0,1r],y1 = 1} 
Similarly,  the  query  never _prize_a  has  the  pairwise  incompatible  covering  
set  of explanations  
w  =  w u  w
u  w
u  
0  
1 
2 
w u  ...  
3  

5.2  Preliminaries  151  
with  
w0  =  {(wx,wy)  I  wx  = (xo,xi,  ... ),wy  = (yo,YI,  ... ),  
Xo  E  [0, 1r], Yo  =  0} 
w}  =  {(wx,wy)  I  wx  = (xo,xi,  ... ),wy  = (yo,YI,  ... ),  
Xo  E  [1r, 21r], Yo  =  0} 
w2 = {(wx,wy)  I  wx  = (xo,xi,  ... ),wy  = (yo,YI,  ... ),  
Xo  E  [1r, 21r], Yo  =  1, XI  E  [0, 1r], YI  =  0} 
w3  = {{(wx,wy) I  wx  = (xo,xi,  ... ),wy  = (yo,YI,  ... ),  
xo  E  [1r, 27r], Yo  = 1, XI  E  [1r, 27r], YI  = 0} 
Once we have computed the pairwise incompatible covering set of explana­
tions, we can calculate the probability of the query. 
Example  66  (Probability of queries for Example 63). Consider  sets  w+  and  
w- from  Example  65.  From  Theorem  14,  
J-l(wt)   =  I  J-ly(w(X)(wx))dJ-lx  =  I  J-lY({wy  I  (wx,wy)  
Jwx 
E  wt})dJ-lx 
  
Jwx  
andso  
J-l(wt)  =Ln  /1Y({(yo,YI, ...) I  Yo  =  1})dJ1x 
= Ln  ~ .2~ dxo  = ~ . ~ = l  
since,for  the  discrete  variables,  J1Y  ( { (yo,  Y1, ...) I  Yo  = 0}) = J-lY  ( { (yo,  YI,  
... )  I  yo  = 1}) = 1/2, and  J1X  is  the  measure  of  the  uniform  density  in  
[0, 27r] on  the  set  [0, 1r]. Similarly,  
J-l(wi)   = I  
Jwx 
/1Y({(yo,YI, ...) I  Yo  = 1,yi = 1})dJ1x
 
= ln   (i
2 
n  · _!__dxo)  · _!__dxi  = ln   · _!__dxi  =  ·  = _!_ 
) 
n  
0   
4 21r 
21r 
)0  8 21r 
8 2 
16 
! 
! 
! ! 

152 
Semanticsfor  Hybrid  Programs  with  Function  Symbols  
Forw-:  
J-l(wr;)  =  r  /1Y({(yo,Yl,  ... )  I 
Jwx 
Yo  =  O})dJ-lx
 
L~ ~ · 
2~ dxo  = ~ · ~ = ~. 
J1(w1) = r  J-lY({(yo,Yl,···)  I Yo  = 
Jwx 
O})dJ-lx
 
= s:~ ~ .2~ dxo  = ~ . ~ = ~. 
J-l(w:;)  = r  
Jwx 
/1Y({(yo,yl,  .. . ) I Yo  = 1,YI = O})dJ-lx
 
2 
= 
~~ (1
2
~  · 
Jo 
_!_dxo)  · _!_dx1  = 1 ~  · _!_dx1  =  ·  = 2_ 
~ 4  27l"  
27l"  
~ 8  27l"  
8  2  
16  
J1(w3) = r  
Jwx 
/1Y({(yo,Yl,  .. . ) 
 
I  Yo  = 1,yl = O})dJ-lx
1
2 
= 
~ (1
2
~  · 
1
2 
_!_dxo)  · _!_dx1  = 
~  · _!_dx1  =  ·  = 2_  
~ 
~ 4  27!"  
27!"  
~ 8  27!"  
8  2  
16  
The  sets  wt  from Example  65  are  pairwise  incompatible,  so  the  measure  of  
w+  can  be  computed  by  summing  the  measures  of  the  wt  s.  By  iteratively  
applying  the  previous  computations,  we  can  calculate  the  probability  of the  
query  at_least_once_prize_a  as  
1 
1 
1 
P(at_least_once_prize_a)  =  
+  
+  
+  ...  
4 
16  
32  
=   4 + 4   . (!) 
4   + 
4   . 4 (!)  
2 
 
+ ...  
1 
1 
1 4  
1 
4"1-:1=4·3=3 
since  the  sum  represents  a  geometric  series.  Similarly,  for  query  never _ 
prize_a,  the  sets  in  w- are  pairwise  incompatible,  so  its  probability  can  be  
! 
! 
! ! 
! 
! 
! ! 
! 
! 
! 

5.2  Preliminaries  153 
computed  as  
1
1 
P(never_prize_a)  = (l + l) + ( 
+ 
) +
16 
16 
1
1
(64+ 64) + ... 
=  
+ ...
~+~· (l) +~· (l)
2 
1 
1 
1 4 
2 
2"1-~ =2·3=3 
P(never _prize_a)  =  1 - P(at_least_once_prize_a),  as  expected.  
Example  67 (Pairwise incompatible covering set of explanations for Exam­
ple 64). Consider  Example  64.  The  version  of  the  program  with  discrete  
random  variables  represented  using  probabilistic facts  is  
ok  ~ kf(2),  (v(1) >  2)  
kf(N)  ~ (s(O) =  init), typeO(T),  kf _part(O,  T,  N)  
kf _part(I,  T,  N) ~I< N,  Nextl  is  I+  1,  
trans(I,  T,  N  extl,  N  extT),  emit(I,  T),  
kf _part(N extl,  N  extT,  N)  
kf _part( N,  _, N) ~ N  =I=  0  
trans(I,  a,  N  extl,  N  extT)  ~  
(s(Nextl) = s(I) +  trans_err_a(J)), 
type(N extl,  a,  N  extT)   
trans(I,  b,  N  extl,  N  extT)  ~  
(s(Nextl) = s(I) +  trans_err_b(J)), 
type(Nexti,b,NextT)   
emit(I,a)  ~  
(v(I) =  s(I) +  obs_err_a(I)) 
emit(I,  b)  ~ 
(v(I) =  s(I) +  obs_err_b(I)) 
typeO(a)  ~ type_init_a.   
typeO(b)  ~"'type_init_a.  

154  Semanticsfor  Hybrid  Programs  with  Function  Symbols  
type(!,  a,  a)  +--- type_a_a(I)o   
type(!,  a,  b)  +-"'type_a_a(I)o   
type( I,  b,  a)  +--- type_b_a(I)o   
type(!,  b,  b)  +-"'type_b_a(I)o   
init '"" gaussian(O,  1)  
trans_err_a(_) '"" gaussian(O,  2)  
trans_err_b(_) '"" gaussian(O,  4)  
obs_err_a(_) '"" gaussian(O,  1)   
obs_err_b(_) '"" gaussian(O,  3)  
type_init_a  : 0040  
type_a_a(_)  : Oo3o  
type_b_a(_)  : 0070  
We  use  the  following  denotation  for  discrete  random  variables:  Yo  for  type_  
init_a,  Yla  for  type_a_a(1 ), Ylb  for  type_b_a(1 ), Y2a  for  type_a_a(2),  Y2b  
for  type_b_a(2),  o o o, with  value  1  ofthe  y  variables  meaning  that  the  corre­
sponding  fact  is  trueo  A  covering  set  of explanations for  the  query  ok  is:  
W  =  Wo  U WI  U W2 U W3  
with  
wo  = { ( wx,  wy)  I  
wx  =  (init, trans_err_a(O), trans_err_a(1), obs_err_a(1), 0  o),
0  
Wy  = (yo,  Yla,  Ylb,  0 0 0),  
init +  trans_err_a(O) +  trans_err_a(1) +  obs_err_a(1) >  2,  
Yo  =  1, Yla  =  1}  
WI  = { ( wx' Wy)  I  
wx  =  (init, trans_err_a(O), trans_err_b(1), obs_err_b(1), 0  o),
0  
Wy  = (yo,  Yla,  Ylb,  0 0 0), 
init +  trans_err_a(O) +  trans_err_b(1) +  obs_err_b(1) >  2, 
Yo  = 1, Yla  = 0} 
W2 =  { (  wx' Wy)  I  
wx  = (init, trans_err_b(O), trans_err_a(1), obs_err_a(1), 0 0 0), 
Wy  =  (yo,  Yla,  Ylb,  o o o), 

5.3  The  Semantics  of PCLP  is  Well-defined  155 
init + trans_err_b(O) + trans_err_a(1) + obs_err_a(1) >  2, 
Yo  =  0, Ylb  =  1} 
W3 = { ( wx' Wy)  I  
wx  = (init, trans_err_b(O), trans_err_b(1), obs_err_b(1), ...), 
Wy  = (yo,  Yla,  Ylb,  · · .), 
init + trans_err_b(O) + trans_err_b(1) + obs_err_b(1) >  2, 
Yo  =  O,Ylb  =  0} 
Example  68  (Probability of queries for Example 64). Consider  the  set  wo  
from  Example  67.  From  Theorem  14,  
J-L(wo)  = f  
Jwx 
J-Ly(w(X)(wx))dJ-Lx  = f  J-LY({wy 
 
Jwx  
I  (wx,wy)  E  wo})dJ-Lx. 
Continuous  random  variables  are  independent  and  normally  distributed.  lf  
X  "'  gaussian(J-Lx,  aJJ, Y  "'  gaussian(J-LY,  (J'f,  ),  and  Z  =  X  +  Y,  
then  Z  "'  gaussian(J-Lx  +  /-LY,  (J'i  +  (J'f,  ).  We  indicate  with  N(x,  J-L,  
2
(J ) 
the  Gaussian  probability  density function  with  mean  J-L  and  variance  2
(J . The  
measure for  wo  can  be  computed  as:  
J-L(wo)  = f  
Jwx  
J-Lx({(yo,Yla,Ylb,···)  I  Yo  = 1,Yla = 1}) dJ-Lx 
= J~oo 0.4·0.3·N(x,0+0+0+0,1+2+2+1)dx= 
= 0.12. 0.207 = 0.0248. 
The  values for  w1, w2, and  W3 can  be  similarly  computed.  The  probability  of  
w  is:  
P(w)  = J-L(wo)  + J-L(wl) + J-L(w2) + J-L(w3) = 0.25. 
5.3  The  Semantics  of  PCLP  is  Well-defined  
In  this section we show that every ground query to every sound program is 
assigned a probability. Here, we focus only on ground programs, but we allow 
them to be denumerable. This may seem a restriction, but it is not since the 
number of groundings of a clause can at most be denumerable if the program 
has function symbols. 

156 
Semanticsfor  Hybrid  Programs  with  Function  Symbols  
Herewe follow the same strategy of Section 3.2 where we proved that 
every query to a probabilistic logic program with function symbols can be 
assigned a probability. To do so, we partly reformulate some of the definitions 
there. In order to avoid introducing new notation, we keep the previous names 
and notation but here they assume a slightly different meaning. 
Definition  36  (Parameterized two-valued interpretations - PCLP). Given  a  
ground  probabilistiG  constraint  logic  program  P  with  Herbrand  base  ßp,  a  
parameterized positive two-valued interpretation Tr  is  a  set  of pairs  (a,  wa)  
where  a  E Bp  and  Wa  E Op  such  that for  each  a  E Bp  there  is  only  one  such  
pair  (Tr  is  really  afunction).  Similarly,  a  parameterized negative two-valued 
interpretation Fa  is  a  set  ofpairs  (a,  W~a) where  a  E  Bp  and  W~a E  Op  such  
that for  each  a  E  Bp  there  is  only  one  such  pair.  
Parameterized two-valued interpretationsform a complete lattice where the 
partial order is defined as I  ~ J  if Va  E Bp,  (a,  wa)  E I,  (a,  Ba)  E J,  Wa  <:;  
Ba.  Foraset T  of parameterized two-valued interpretations, the least upper 
bound and greatest lower bound always exist and are respectively 
lub(T) =  {(a, u  Wa)  I a  E  ßp}  
I  ET, ( a,wa  )EI 
and 
n 
glb(T) =  {(a, 
Wa)  I  a  E  ßp}.  
IET,(a,wa)EI  
The top element T is 
{ (  a,  Wx  x  Wy)  I a  E  ßp}  
and the bottom element l_  is 
{(a, 0)  I  a  E  Bp}.  
Definition  37 (Parameterized three-valued interpretations - PCLP). Given  
a  ground  probabilistic constraint  logic  program  P  with  Herbrandbase  ßp,  
a  parameterized three-valued interpretation I  is  a  set  of triples  (a,  wa,  w~a) 
where  a  E ßp,  Wa  E Op,  and  W~a E Op,  and  such  that,  for  each  a  E 
ßp,  there  is  one  such  triple.  A  parameterized  three-valued  interpretation  I  is  
consistent ifV(a,  Wa,  W~a) EI, Wa  n  W~a = 0. 
Parameterized three-valued interpretationsform a complete lattice where the 
partial orderis defined as I~ J  ifVa E ßp,  (a,  Wa,  W~a) EI, (a,  Ba,  B~a) E 

5.3  The  Semantics  of PCLP  is  Well-defined  157 
:1,  Wa  ~ Oa,  and W~a ~ ()~a· The least upper bound and greatest lower 
bound for a set T  of parameterized three-valued interpretations always exist 
and are respectively 
lub(T) =  {(a, 
U 
Wa,  
U 
W~a) I  a  E  ßp}  
IET, (a,wa  ,W-a )EI 
IET,(a,wa  ,W-a )EI, 
and 
glb(T) = {(a, 
n 
Wa,  n 
W~a) I a  E  ßp}.  
IET,(a,wa,W-a)EI  
IET,(a,wa,W-a)EI  
The top element T is 
{ (  a,  Wx  x Wy,  Wx  x Wy)  I  a  E  ßp}  
and the bottom element _l is 
{(a, 0,  0)  I a  E  ßp}.  
Definition38(0pTruePi'(Tr) and OpFalsePi'(Fa)  -PCLP). Foraground  
probabilistic  constraint  logic  program  P  with  rules  R  and  facts  F,  a  pa­
rameterized  two-valued  positive  interpretation  Tr  with  pairs  (a,  Oa),  a  pa­
rameterized  two-valued  negative  interpretation  Fa  with  pairs  (a,  ()~a), and  
a  parameterized  three-valued  interpretation  I  with  triplets  (a,  Wa,  W~a), we  
define  OpTruePi'(Tr)  = {(a,/a)  I  a  E  ßp}  where  
~ 
Wx  x W{{(a,0,1)}} 
ifa  E  F  
Ua<-bl,···,bn,~Cl,···,~cm,'Pl,···,'PlER ( ( ()bl  u  Wbl)  n  ...  
7a 
{ 
n(()bn  U  Wbn)  n  W~q n  ...  n  W~cm 
ifa  E  ßp\F  
nCSS(~.pi) x Wy  n ... n CSS(~.pz) x Wy)  
and  OpFalsePi'(Fa)  = {(a,/~a) I a  E  ßp}  where  
l 
Wx  X W{{(a,0,0)}} 
ifa  E F  
_ 
na<-bl, ... ,bn,-cl, ... ,-c",,<pl, ... ,<pzER((e_bl  n  W-bl)  U  ·  ·  ·  
"f-a  -
u(e-bn  n  W-bJ  u Wc 1  u ... u Wc",  
ifa  E  l3p\F  
u(Wx\CSS(cpl)) x  Wy  u ... 
u(Wx\CSS(cpt))  x  Wy)  
Proposition  8 (Monotonicity of OpTruePi'  and OpFalsePi'  - PCLP). 
OpTruePi'  and  OpFalsePi'  are  monotonic.  

158 
Semanticsfor  Hybrid  Programs  with  Function  Symbols  
Proof.  Herewe only consider OpTruePi,  since the proof for OpFalsePi  
can be similarly constructed. Essentially, we have to prove that if Tr1 :S;  
Tr2  then OpTruePi ( Tr1) :S;  OpTruePi ( Tr2). By definition, Tr1 :S;  Tr2  
means that 
Va  E ßp,  (a,  Wa)  E Tr1,  (a,  Ba)  E Tr2  : Wa  ~ Ba.  
Let (a,  w~) be the elements of OpTruePi ( Tr1) and (a,  B~) the elements of 
OpTruePi(Tr2).  Toprovethemonotonicity, wehavetoprovethatw~ ~ 0~. 
If a  E F,  then w~ =  B~ =  Wx  x w{{(a,0 ,l)}}· If a  E ßp\F,  then w~ and 
(}~ have the same structure. Since Vb  E  ßp,  Wb~ ob.  then w~ ~ 0~. 
D  
The monotonicity property ensures that both OpTruePi  and OpFalsePi  
have a least fixpoint and a greatest fixpoint. We now define an iterated fixpoint 
operator and prove its monotonicity. 
Definition  39  (Iterated fixpoint for probabilistic constraint logic programs ­
PCLP). Fora ground probabilistiG Gonstraint  logiG program P  and a param­
eterized three-valued interpretation  I,  we define  JpppP (I)  as  
IFPPP(I)  =  {(a,wa,W~a) I  (a,wa)  E  lfp(OpTruePi),  
(a,  W~a) E  gfp( OpFalsePi) }. 
Proposition  9 (Monotonicity of JpppP- PCLP). JpppP  is  monotoniG.  
Proof.  As before, we have to prove that, if I 1 :S;  I 2, then IFPPP(I1)  :S;  
JpppP (I2). By definition, I 1 :S;  I 2 means that 
Va  E ßp,  (a,  Wa,  W~a) E 'I1, (a,  Ba,  B~a) E 'I2: Wa  ~Ba, W~a ~ B~a· 
Let (a,  w~, w~a) be the elements of fpppP ('LI) and (a,  0~, B~a) the elements 
of JpppP ('I2). We have to prove that w~ ~ 0~ and w~a ~ B~a· This is 
a direct consequence of the monotonicity of OpTruePi  and OpFalsePi  
proved in Proposition 8 and of their monotonicity in I  that can be proved as 
in Proposition 8. 
D 
IFP pP  is monotonic and so it has a least fixpoint. We identify lfp( IFP PP)  
with WFMP(P).  We call depth  ofP the smallest ordinal8 suchthat JpppP  j  
8  = WFMP(P).  Now we prove that OpTruePi  and OpFalsePi  are sound. 
Lemma  11  (Soundness of OpTruePi- PCLP). Fora  ground  probabilistiG  
Gonstraint  logiG  program  P  with  probabilistiG jaGts  F,  rules  R,  and a  param­
eterized  three-valued  interpretation  I,  denote  with  B~ the  set  associated  to  

5.3  The  Semantics  of PCLP  is  Well-defined  159 
atom  a  in  OpTruePi  ja. For  every  atom  a,  world  w,  and  iteration  a:  
w  E  0~- WFM(w  I I)  I=  a  
where  w  I  I  is  obtained  by  adding  to  Pw  the  atoms  aforwhich  (a,  Wa,  W~a) E  
I  and  w  E  Wa,  and  by  removing  all  the  rules  with  a  in  the  head  for  which  
(a,  Wa,  W~a) EI and  w  E  W~a· 
Proof.  We prove the lemma by transfinite induction: we assume that the the­
sis is true for all ordinals ß  <  a and we prove it  for a. There are two cases 
to cover: a successor ordinal and a limit  ordinal. Consider a a successor 
ordinal. If a  E  F,  w  E  0~ means that the random variable Ya  corresponding 
to a  is set to 1 so a  is a fact in P w  and WF M  (w  I I)  I=  a.  
If a  ~ F,  consider w  E  0~ where 
0~ = 
u  
((0~1-
1 u  wbt)  n  ...  
a<-bt  , ... ,bn,~ct , ... ,~cm ,<p1 , ... ,<ptER  
n (O~n-
1 u Wbn)  n W~c1 n ... n W~cm 
n CSS(r.pi)  x Wy  n ... n CSS(r.pz)  x Wy ). 
This means that there is a rule a  ~ b1, ... , bn,  "'CI, ... , "'cm, r.p1, ... , r.pz  E 
J?  suchthat W  E 0~-
1 U Wbi  for i  =  1, ... , n,  W  E W~cj for j  =  1 ... , m  and 
w  I=  'Pk  for k  =  1, ... , l.  By the inductive assumption and because of how 
w  I  I  is built, WFM(w  I  I)  I=  bi,  WFM(w  I  I)  F"'Cj and w  I=  'Pk  so 
WFM(w  I  I)  I=  a.  
Consider now a  a limit ordinal. Then 
0~ =  lub({O~ I  ß  <  a})  =  U 0~. 
ß<a  
If w  E  0~, there must exist a ß  <  a such that w  E  og.  By the inductive 
assumption the hypothesis holds. 
D 
Lemma  12  (Soundness of OpFalsePi  - PCLP). Fora  ground  probabilistic  
constraint  logic  program  P  with  probabilistic  facts  F,  rules  R,  and  a  pa­
rameterized  three-valued  interpretation  I,  denote  with  o:::._a  the  set  associated  
with  atom  a  in  OpFalsePi  l  a. For  every  atom  a,  world  wand  iteration  a, 
the  following  holds:  
w  E  o:::a- WFM(w  I I)  F"'a 
where  w  I I  is  built  as  in  Lemma  11.  

160 
Semanticsfor  Hybrid  Programs  with  Function  Symbols  
Proof.  As before, we prove the lemma by transfinite induction: we assume 
that the thesis is true for all ordinals ß  <  a  and we prove it  for a.  Again, we 
need to cover two cases: a  successor ordinal and a  limit ordinal. Consider 
a  a successor ordinal. If a  E  F  w  E  er;::,a  means that the random variable Ya  
corresponding to a  is set to 0 so a  isanot a fact in Pw.  Since probabilistic 
facts do not appear in the head of any rule, then WF M  (w  I I) I=  ~a. 
If a  tf; F,  consider w  E  er;::,a  where 
e:::a  = 
n 
( ( er:t;
1 n  W~b1 ) U  · · ·  
1 
a+-bl  , .. . ,bn ,,-.,..,Cl, ... ,,-.,..,Cm ,<pl , ... ,<pt  ER  
u  (Or:t;n1 n  W~bn) U Wc1  U ... U Wem  
u (Wx\CSS(~t?l)) x Wy  u ... u (Wx\CSS(~t?z)) x Wy).  
This means that, for each a  ~ b1, ... , bn,  ~cl, ... , ~cm, ipl, ... , ipz  E 'R  
there exists an index i  suchthat w  E  er:t;/ n  w~b;• or there exists an index 
j  such that w  E  Wci,  or there exists an index k  such that w  ~ it?k·  By the 
inductive assumption and because of how w  I  I  is built, either WF M  (w  I  
I) l=~bi, WFM(w  I I) I=  Cj,  or w  ~ it?k  hold, so WFM(w  I I) l=~a. 
Consider now a  a limit ordinal. Then, 
e:::,a  =  glb( { e~a I ß  <  a})  =  ne~a. 
ß<a  
If w  E  er;::,a,  for all ß  <  a  we have that w  E  ()~a· By the inductive assumption, 
the hypothesis hold. 
D 
We now prove the soundness and completeness ofthe operator IFPPP.  
Lemma  13  (Soundness of JpppP- PCLP). Fora  ground probabilistiG  logiG  
program  P  with  probabilistiG  jaGts  Fand  rules  R,  denote  with  wz:'  and  wr;::,a  
the  formulas  assoGiated  with  atom  a  in  JpppP  .  For  every  atom  a,  world  w  
and  iteration  a,  the  following  holds:  
w  E  w~ ~ WFM(w)  I=  a.  
(5.1)  
w  E  w<;:,a  ~ WFM(w)  l=~a. 
(5.2) 
Proof.  Let us prove it  by transfinite induction. Let us consider a successor 
ordinal a  and let us assume 
WFM(PwiiFPPP  ja- 1)  =  WFM(w)  
(5.3) 

5.3  The  Semantics  of PCLP  is  Well-defined  161 
and that 
w  E  w~-
1 ~ WFM(w)  F= a.  
(5.4) 
w  E  w::;;_- 1  ~ WFM(w)  F=~a. 
(5.5) 
From the soundness of OpTruePfFPPPt(a- )  
1 and OpFalsePfFPPPt(a- )  
1
wehave that 
w  E  (J~ ~ WFM(w(TIIFPPP  j  (a- 1)) F= a  
w  E  (J~a ~ WFM(w(TIIFPPP  j  (a- 1))  F=~a 
where eg  is the formula associated to a  in OpTruePfFPPPt(a- )  
1 i  ß  and
(J~a is the formula associated to a  in OpFalsePfFPPPt(a- )  
1
~ ß.  Since this 
is true for all ß,  it is also true for the 1 and 5  suchthat 
OpTruePfFPPPta-1  j  1  = lfp( OpTruePfFPPPta-
)  
1
OpFalsePfpppPta-1  ~ 5  =  gfp( OpFalsePfpppPta-
)  
1
so 
w  E  w~ ~ WFM(wiiFPPP  j  (a- 1))  F= a  
w  E  w::a  ~ WFM(wiiFPPP  j  (a- 1))  F=~a 
Because of Equation (5.3), then 
w  E  w~ ~ WFM(w)  F= a  
(5.6) 
w  E  w::a  ~ WFM(w)  F=~a 
(5.7) 
N ow let us prove that 
WFM(PwiiFPPP  ja) =  WFM(w)  
(5.8) 
Let Ia  =  <Ir ,I F) be a three-valued interpretation defined as Ir  =  {  a  I w  E 
w~} andJp =  {alw  E w~a}· Then,forEquation(5.6), Va  E Ir:  WFM(w)  F= 
a  and, forEquation 5.7, Va  E  lp:  WFM(w)  F=~a. Soia  ~ WFM(w).  
Since PwiiFPPP  i  a  =  Pwiiia,  by Lemma 6 
WFM(w)  =  WFM(Pwiiia)  =  WFM(PwiiFPPP  ja). 
Let us now consider a limit ordinal a  and let us assume that 
w  E  w~ ~ WFM(w)  F= a  
(5.9) 
w  E  w~a ~ WFM(w)  F=~a 
(5.10) 

162 
Semanticsfor  Hybrid  Programs  with  Function  Symbols  
for all ß  <  a  Then w~ =  uß<a wg  and w~a =  uß<a W~a so w  E  w~ ~ 
~ß: w  E  wg.  ForEquation (5.9), then WFM(w)  I=  a.  Moreover w  E  w~a ~ 
~ß: w  E  wg.  ForEquation (5.10), then WFM(w)  l="'a.  
D 
Lemma  14  (Completeness of fpppP  - PCLP). Fora  ground probabilistiG  
Gonstraint  logiG  program  P  with  probabilistiG  jaGts  F  and  rules  R,  let  w~ 
and  w~a be  the  sets  assoGiated  with  atom  a  in  fpppP  j  a.  For  every  atom  
a,  world  wand iteration  a,  we  have:  
a  E  fppPw  j  a  ~ w  E  w~. 
"'a  E  fppPw  ja~ w  E  w~a· 
Proof  We prove it by double transfinite induction. If a  is a successor ordinal, 
assume that 
a  E  fppPw  j  (a- 1)  ~ w  E  w~-
1 
"'a  E  fppPw  j  (a- 1)  ~ w  E  w::;;,- 1  
Let us perform transfinite induction on the iterations of OpTruef;pPwt(a-1)  
and OpFalsef;Pwt(a-1).  Consider a successor ordinal5 and assume that 
a  E  OpTruef;pPwt(a-1)  j  (5-1)~ w  E  ~~-
1 
. 
a  E  OpFalsef;pPwt(a-1)  l  (6- 1)  ~ w  E  e~-;;,
1 
. 
where (a,  ,g-1) are the elements of OpTruefFPPPta-1  i  (5 -
1)  and 
(a,  e~-;;,
1 
) are the elements of OpFalsefpppPta-1  l  (6- 1). We now prove 
that 
a  E  OpTruef;pPwt(a-1)  j  6  ~ W  E  /~· 
(5.11) 
a  E  OpFalsef;pPwt(a-1)  l6  ~ w  E  e~a· 
(5.12) 
Consider an atom a.  If a  E  F,  the previous statement can be easily proved. 
Otherwise, a  E  OpTruef;pPwt(a-1)  i  6  means that there is a rule a  +­
b1, ... , bn,  "'Ci,  ...  ,  cm, 'f?1, ... , 'Pl  in R  suchthat for all i  =  1, ... , n,  
bi  E  OpTruef;pPwt(a-1)  j  (5-1) V  bi  E  fppPw  j  (a- 1), 

5.3  The  Semantics  of PCLP  is  Well-defined  163 
forallj = 1,ooo,m,~cj E  fppPw  j (a-1),andforallk = 1,ooo,l, 
w  1= 'Pko  For the inductive hypothesis, Vi : w  E  1t-1  v w  E  w~-
1 and Vj : 
w  E  w<;:,-;_:},  so w  E  ,go  The proof is similar for a  E  OpFalsefppwi(a-1)  t  80  
Consider now 8  a limit ordinal, so ,g  = UJ.L<i5 ~~. and ()~a = nJ.L<i5 O~ao 
If a  E  OpTruef;pPwt(a- )  j 8,  then there exists a 11  <  8  suchthat
1
a  E  OpTruef;pPwj(a-1)  i  J-lo  
For the inductive hypothesis, w  E ~~ and so w  E ,g 0 
If a  E  OpFalsef;pPwt(a- )  l  8,  then, for all 11  <  8, 
1
a  E  OpFalsef;pPwj(a-1)  l  J-lo  
For the inductive hypothesis, w  E  ()~ and so w  E  ()~0 
Since fppPw  j a  is obtained from the fixpoints of OpTruef;pPwtCa-l)  
and OpFalsef;pPwi(a-l)'  and equations (5oll) and (5o12) hold for all8, they 
also hold at the fixpoints and the thesis is provedo 
Consider now a  a Iimit ordinal. Then, w~ =  Uß<a wg  and w<;:,a  =  
uß<aW~ao 
If a  E  fppPw  ja, there exists aß <  a  suchthat a  E  fppPw  j ßo  For the 
inductive hypothesis w  E  wg,  so w  E  w~0 The proof is similar for ~ao 
D 
Now we can prove that fpppP  is sound and completeo 
Theorem  15  (Soundness and completeness of fpppP- PCLP)o Forasound  
ground  probabilistic  constraint  logic  program  P,  let  w~ and  w<;:,a  be  the  sets  
associated  with  atom  a  in  fpppP  j  ao  For  every  atom  a  and  world  w  there  
is  an  iteration  ao  such  thatfor  all  a  >  ao  we  have:  
w  E  w~ ~ WFM(w)  I=  ao  
(5013) 
w  E  w~a ~ WFM(w)  l=~ao 
(5014) 
Proof.  The ~ direction of equations (5o13) and (5o14) is proveninLemma 130 
In  the other direction, WF M  (w)  I=  a  implies that there exists an a 0  such 
that Va : a  ?: a 0 ,  fppw  j a  1= ao  For Lemma 14, w  E  w~o Similarly, 
WFM(w)  l=~a implies that there exists an ao  suchthat Va : a  ?: ao  ~ 
fppw  ja l=~ao As before, for Lemma 14, w  E  w<;:,ao  
D 

164 
Semanticsfor  Hybrid  Programs  with  Function  Symbols  
Finally, we prove that every query for every soundprogram is well-defined. 
Theorem  16  (Well-definedness of the distribution semantics - PCLP). Fora  
sound  ground  probabilistic  constraint  logic  program  P,  for  all  ground  atoms  
a,  p,p({w  I  w  E  Wp,  WFM(w)  I=  a}) iswell-defined.  
Proof  Let w~ and w~a be the sets associated with atom a  in IFPPP  j  
6,  where 6  denotes the depth of the program. Since IFPPP  is sound and 
complete, {w I  w  E  Wp,  WFM(w)  I=  a} =  w~. 
Each iteration of OpTrueP"jpppPtß  and OpFalseP"jpppPtß  for all ß gen­
erates sets belanging to Op,  since the set of rules is countable. So p,p  ( { w  I  
w  E  Wp,  WFM(w)  I=  a}) is well-defined. 
D 
Moreover, ifthe program is sound, for all atoms a,  w~ = (w~a)c holds, where 
6  is the depth of the program and the superscript c denotes the complement. 
Otherwise, there would exist a world w  such that w  rf; w~ and w  rf; w~a. 
But w  has a two-valued well-founded model, so either WFM(w)  I=  a  or 
WFM(w)  l="'a.  In the first case w  E w~ andin the latter w  E w~a• against 
the hypothesis. 

6   
Probabilistic  Answer  Set  Programming  
In Section 2.2 we considered on1y sound programs, those for which every 
world has a two-valued WFM. In this way, we avoid non-monotonic aspects 
of the program and we deal with uncertainty on1y by means of probabi1ity 
theory. 
When a program is not sound in fact, assigning a semantics to probabi1is­
tic 1ogic programs is not obvious, as the next section shows. 
6.1  A  Semantics  for  Unsound  Programs  
We start by giving an examp1e of an unsound program. 
Example  69  (Insomnia [Cozman and Mami, 2017]). Consider  the  program  
sleep  +---"'work,  "'insomnia.  
work  +---"'sleep.  
a  :: insomnia.  
This  program  has  two  worlds,  w1 containing  insomnia  and  w2 not  contain­
ing  it.  The  first  has  the  single  stable  model  ( see  Section  1.4.3)  and  total  WFM  
(see  Section  1.4.2)  
h  = {insomnia,  "'sleep,  work}  
The  latter  has  two  stable  models  
h  =  {  "'insomnia,  "'sleep,  work}  
h  =  {  "'insomnia,  sleep,  "'work}  
and  a  WFM  I2 where  insomnia  is  false  and  the  other  two  atoms  are  unde­
fined.  
lf we  ask for  the  probability  of sleep,  the  first  world,  w1, with  probability  
a,  surely  doesn  't  contribute.  We  are  not  sure  instead  what  to  da  with  the  
second,  as  sleep  is  included  in  only  one  of the  two  stable  models  and  it  is  
undefined  in  the  WFM.  
165 

166 
Probabilistic  Answer  Set  Programming  
To handle programs like the above, Hadjichristodoulou and Warren [2012] 
proposed the WFS for probabilistic logic programs where a program defines 
a probability distribution over WFMs rather than two-valued models. This in­
duces a probability distribution over random variables associated with atoms 
that are, however, three-valued instead of Boolean. 
An alternative approach, the credal semantics  [Lukasiewicz, 2005, 2007] 1, 
sees such programs as defining a set ofprobability measures over the interpre­
tations. The name derives from the fact that sets of probability distributions 
are often called credal  sets.  
The semantics considers programs syntactically equal to ProbLog (i.e., 
non-probabilistic rules and probabilistic facts) and generates worlds as in 
ProbLog. The semantics requires that each world of the program has at least 
one stable models. Such programs are called consistent.  
A program then defines a set of probability distributions over the set of 
all possible two-valued interpretations of the program. Each distribution P  in 
the set is called a probability  model  and must satisfy two conditions: 
1.  every interpretation I  for which P(I)  >  0 must be a stable model ofthe 
world Wu  that agrees with I  on the truth value of the probabilistic facts; 
2.  the sum of the probabilities of the stable models of Wu  must be equal 
to P(a-).  
A set of distributions is obtained because we do not fix how the probability 
mass P(a)  of a world Wu  is distributed over its stable models when there 
is more than one. We indicate with P the set of probability models and call 
it the credal  semantics  of the program. Given a probability model, we can 
compute the probability of a query q  as for the DS, by summing P(I)  for all 
the interpretations I  where q  is true. 
In  this case, given a query q,  we are interested in the lower  and  upper  
probabilities  of q  defined as 
P(q)  
inf P(q) 
PEP 
P(q)   
sup P(q)  
PEP 
1  It  was called answer  set  semantics  in [Lukasiewicz, 2005, 2007], here we follow the 
terminology of [ Cozman and Maua, 20 17]. 

6.1  A  Semantics  for  Unsound  Programs  167 
If we are also given evidence e,  we can define lower  and  upper  conditional  
probabilities  as 
P(qle)  
inf 
P(q  I  e)
PEP,P(e)>O  
P(qle)  
sup 
P(q  I  e) 
PEP,P(e)>O  
and leave them undefined when P(e)  =  0 for all PEP. 
Example  70  (Insomnia - continued - [Cozman and Mami, 2017]). Con­
sider  again  the  program  of Example  69.  A  probability  model  that  assigns  the  
following  probabilities  to  the  models  of the  program  
P(h)  = a  
P(h)  =  1'(1 - a)  
P(h)  =  (1  -')')(1  - a)  
for  I'  E  [0, 1 ], satisfies  the  two  conditions  of the  semantics,  and  thus  belongs  
toP.  The  elements  ofP  are  obtained  by  varying  I'·  
Considering  the  query  sleep,  we  can  easily  see  that  P( sleep  =  true)  =  0 
and  P(sleep  =  true)  =  1- a.  
With  the  semantics  of  [Hadjichristodoulou  and  Warren,  2012]  instead,  
we  have  
P(h)  =  a  
P(I2)  =  1- a  
so  
P(sleep  = true)  = 0 
P(sleep  = false)  = a  
P(sleep  = undefined)  = 1 - a.  
Example  71  (Barber paradox - [Cozman and Mami, 2017]). The  barher  
paradox  was  introduced  by  Russell  [1967].  If  the  village  barher  shaves  all,  
and  only,  those  in  the  village  who  don  't  shave  themselves,  does  the  barher  
shave  himself?  
A  probabilistic  version  ofthisparadox  can  be  encoded  with  the  program  
shaves(X,  Y)  ~ barber(X),  villager(Y),  "'shaves(Y,  Y).  
villager (a).  
barber(b).  
0.5 :: villager(b).  
and  the  query  shaves(b,  b).  

168 Probabilistic  Answer  Set  Programming  
The  program  has  two  worlds,  w1 and  w2, the  first  not  containing  the  
fact  villager(b)  and  the  latter  containing  it.  The  first  world  has  a  single  
stable  model  h  = {villager(a),  barber(b),  shaves(b,  a)}  that  is  also  the  
total  WFM.  In  the  latter  world,  the  rule  has  an  instance  that  can  be  simplified  
to  shaves(b,  b)  +---~ shaves(b,  b).  Since  it  contains  a  loop  through  an  odd  
number  of  negations,  the  world  has  no  stable  modeland  the  three-valued  
WFM:  
'I2 =  {villager(a),  villager(b),  barber(b),  
shaves(b,  a),  ~shaves(a, a),  ~shaves(a, b)  }. 
So  the  program  is  not  consistent  and  the  credal  semantics  is  not  definedfor  it,  
while  the  semantics  of [Hadjichristodoulou  and  Warren,  2012]  is  still  defined  
and  would  yield  
P(shaves(b,  b)  = false)  = 0.5 
P(shaves(b,  b)  = undefined)  = 0.5 
The WFS for probabilistic logic programs assigns a semantics to more pro­
grams. However, it introduces the truth value undefined  that expresses uncer­
tainty and, since probability is used as well to deal with uncertainty, some 
confusion may arise. For example, one may ask what is the value of (q  = 
truele  = undefined).  If e  = undefined  means that we don't know anything 
about e,  then P(q  =  truele  =  undefined)  should be equal to P(q  =  true)  
but this is not true in general. The credal semantics avoids these problems by 
considering only two truth values. 
Cozman and Mami [2017] show that the set Pis the set of all probability 
measures that dominate an infinitely monotone Choquet capacity. 
An infinitely  monotone  Choquet  capacity  is a function P  from an algebra 
non a set W  to the real interval [0, 1] suchthat 
1. P(W)  = 1 - P(0)  = 1, and 
2. for any W1, ... , Wn  ~ f2, 
:?: 
2.:  
(-1)1JI+1
P(uiwi)  
p(njEJWj)  
(6.1) 
J<:;{l, ... ,n} 
Infinitely monotone Choquet capacities are a generalization of finitely ad­
ditive probability measures: the latter are special cases of the first where 
Equation (6.1) holds with equality. In fact, the right member ofEquation (6.1) 
is an application of the inclusion-exclusion principle that gives the probabil­
ity of the union of non-disjoint sets. Infinitely monotone Choquet capacities 
also appear as belief functions of Dempster-Shafer theory [Shafer, 1976]. 

6.1  A  Semantics  for  Unsound  Programs  169 
Given an infinitely monotone Choquet capacity P,  we can construct the 
set of measures D(P)  that dominate Pas 
D(P)  = {PI\fw  E  0: P(w)?  P(w)}  
We say that P  generates  the credal set D(P)  and we call D(P)  an infinitely  
monoton  credal  set.  It  is possible to show that the lower probability of D(P)  
is exactly the generating infinitely monotone Choquet capacity: P  ( w)  =  
infPED(E) P(w).  
Infinitely monotone credal sets are closed and convex. Convexity here 
means that if P1 and P2 are in the credal set, then aP1  +  (1- a)P2 is also in 
the credal set for a  E  [0, 1]. Given a consistent program, its credal semantics 
is thus a closed and convex set of probability measures. 
Moreover, given a query q,  we have 
P(q)  =  
P(O")  P(q)  =  
P(O") 
2:  
2:   
wEW,AS(w)<;;Jq  
wEW,AS(w)nJq#0  
where Jq  is the set of interpretations where q  is true and AS (w)  is the set of 
stable models of world w.  
The lower and upper conditional probabilities of a query q  are given by 
[Augustin et al., 2014]: 
P(q,  e)
P(q  I  e) 
(6.2)
P(q,  e) +  P( ~q, e) 
If P(q,  e) +  P( ~q, e) =  0 and P(q,  e) >  0, P(q  I  e) =  1. If both P(q,  e) 
and P( ~q, e) are 0, this value is undefined. 
P(q  I  e) 
P(q,  e) 
(6.3)
P(q,  e) +  P( ~q, e) 
If P(q,  e) +  P( ~q, e) =  0 and P( ~q, e) >  0, P(q  I  e) =  0. As before, if 
both P(q,  e) and P( ~q, e) are 0, this value is undefined. 
Note that these formulas are the same as equations (4.6) and (4.7) of 
Section 4.5.1. 
A world w  contributes to the upper probability of a conjunction of ground 
literals q  if q  is true in at least one of its stable models and to the lower proba­
bility if q  is true in all its stable models. Algorithm 3 shows this computation. 

170 
Probabilistic  Answer  Set  Programming  
Algorithm 3 Function CREDAL: Computation of the probability of a query. 
1: function CREDAL(P,  q)  
2: 
E_(q)  ~ o,  P(q)  ~ o  
3: 
for all worlds w  of P  do 
4: 
if'v'A E  AS(w),  AI=  q  then 
5: 
E_(q)  ~ E_(q)  +  P(w)  
6: 
end if 
7: 
if3A E  AS(w),A  I=  qthen 
8: 
P(q)  ~ P(q)  +  P(w)  
9: 
end if 
10: 
end for 
11: 
return [E_(q),  P(q)]  
12: end function 
For computing the bounds of the conditional probability of conjunction 
of ground literals q  given the conjunction of ground literals e,  we must keep 
four values, as in Algorithm 4. 
In the case in which the probabilistic program is sound, i.e., if every world 
has a total WFM, then, by Theorem 2, every world has a single stable model 
and the credal set contains a single probability distribution. In this case 
P(q)  =  P(q)  =  P(q)  
and 
P(qle)  =  P(qle)  =  P(qle)  
and the probability is the same as that assigned by the distribution semantics, 
so the credal and the distribution semantics coincide. 
6.2 Features  of  Answer  Set  Programming  
Answer Set Programming allows syntactic features beyond normallogic pro­
grams (see Section 1.3). However, the definition ofthe semantics remains the 
same as that of Section 1.4.3. 
An ASP  program  is a set of rules  of the form 
a1  V  ...  V  an+---bl,···,bm.  
where each ai  is an atom and each bj  is a literal. a1  v  ...  v  an  is called the 
head  of the rule and b1, ... , bm  is called the body  . If n  >  1 the rule is called 
disjunctive.  If n  =  1 is called normal  and if n  =  0 is called a constraint.  

6.2  Features  of Answer  Set  Programming  171 
Algorithm  4  Function CREDALCOND: Computation of the probability of a 
query given evidence. 
1: function CREDALCOND(P,  q,  e) 
2: 
UPqe  ~ 0, lpqe  ~ 0, UPnqe  ~ 0, lpnqe  ~ 0 
3: 
for all worlds w  of P  do 
4: 
ifVA E  AS(w),A  I= q,ethen 
5: 
lpqe  ~ lpqe  +  P(w)  
6: 
endif 
7: 
if3A E  AS(w),  AI=  q,  e  then 
8: 
UPqe  ~ UPqe  +  P(w)  
9: 
endif 
10: 
ifVA E  AS(w),  A  i=~q, e  then 
11: 
lpnqe  ~ lpnqe  +  P(w)  
12: 
endif 
13: 
if3A E  AS(w),  A  i=~q, e  then 
14: 
UPnqe  ~ UPnqe  +  P(w)  
15: 
endif 
16: 
end for 
17: 
if UPqe  +  UPnqe  =  0 then 
18: 
return fai1ure 
19: 
eise 
20: 
if UPqe  +  lpnqe  =  0 and UPnqe  >  0 then 
21: 
return [0, 0] 
22: 
eise 
23: 
if lpqe  +  UPnqe  =  0 and UPqe  >  0 then 
24: 
return [1, 1] 
25: 
eise 
return [ 
lpqe  
UPqe  
]  
lPqe+UPnqe'  UPqe+lPnqe   
26: 
27: 
endif 
28: 
endif 
29: 
endif 
30: end function 
A  positive  program  is a program that does not contain negative literals 
(but that can contain disjunctive rules). 
A  ground rule is satisfied in a two-valued interpretation if some literals in 
the body are false or if all the literals in the body are true and some literals 
in the head are true in the interpretation. If the rule is a constraint it is thus 
satisfied only if some literals in the body are false. 
A  model  of a program is a two-valued interpretation that satisfies all 
grounding of all the rules of the program. A  model I  is minimal if there 
is no other model J  suchthat J  <:;  I.  

172 Probabilistic  Answer  Set  Programming  
Using the notion of reduct given in Definition 5, an answer  set  of a pro­
gram P  is an interpretation I  that is a minimal model of the reduct pi  of P  
relative to I.  
This definition is the same as that of Definition 6 but note that, while for 
normal logic programs the reduct is a definite program and it has a single 
minimal model, the least Herbrand model, for ASP programs the reduct is a 
positive program that may have more than one minimal model. 
Answer Set Programming is a paradigm in which the user writes rules 
and constraints describing a problern and the answer sets are solution to the 
problem. Usually, the strategy adopted is "Guess & Check", where disjunc­
tive ruless are used to guess solutions nondeterministically and constraints 
serve to discard interpretations that are not solutions. Let us see an example. 
Example  72 (Graph Three-Coloring). Graph  colaring  is  a  way  to  assign  
colors  to  nodes  of an  undirected  graph  such  that  two  adjacent  nodes  have  
different  colors.  The  following  Answer  Set  Program  describes  the  problern  
when  there  are  three  colors:  
red(X)  v  green(X)  v  blue(X)  +--- node(X).  
+--- edge(X,  Y),  red( X),  red(Y).  
+--- edge(X,  Y),  green(X),  green(Y).  
+--- edge(X,  Y),  blue(X),  blue(Y).  
This  program  should  be  completed  with  facts  for  predicates  node/1  and  
edges/2  describing  a  specific  graph.  
Each  answer  set for  this  program  describes  a  three-coloring  of the  graph.  
The  disjunctive  rule  serves  to  guess  an  assignment  of color  to  nodes  and  the  
constraints  check  the  assignments  by  ruling  out  interpretations  where  two  
nodes  connected  by  an  edge  share  the  same  color.  
ASP also offers other features such as strong negation, see [Eiteret al., 2009], 
and aggregate literals, see [Faber et al., 2011a], that we won't discuss here. 
6.3  Probabilistic  Answer  Set  Programming  
When we add probabilistic facts to ASP programs and we use the credal 
semantics of Section 6.1, we obtain Probabilistic answer set programming 
(PASP), which is an expressive probabilistic formalism. Let us see an exam­
ple. 
Example  73 (Probabilistic Graph Three-Coloring- [Cozman and Mami, 2020].). 
Consider  the  programfrom  Example  72  but  suppose  that  the  graph  is  proba­

6.3  ProbabilistiG  Answer  Set  Programming  173 
5 
4 
0.8 
Figure  6.1  Example of a probabilistic graph. 
bilistic,  such  as  the  graph  shown  in  Figure  6.1  and  described  by  thefollowing  
probabilistic  facts:  
0.6 :: edge(1,  2). 
0.1 :: edge(1,  3). 
0.4 :: edge(2,  5). 
0.3 :: edge(2,  6). 
0.3 :: edge(3,  4). 
0.8 :: edge(4,  5). 
0.2 :: edge(5,  6). 
Suppose  also  that  the  program  contains  the  following  certain  facts  
red(1).  
green(4).  
green(6).  
Each  total  choice  identifies  a  graph.  If a  graph  has  multiple  three-colorings,  
the  corresponding  world  has  multiple  answer  sets  and  the  probability  of the  
world  can  be  spread  over  the  answer  sets.  Consider  for  instance  node  3.  If  
edges  to  1  and  4  are  both  present,  then  its  color  will  be  blue  in  all  answer  
sets,  as  1  is  red  and  4  is  green.  The  probability  that  edges  1-3  and  3-4  are  
present  is  0.1 · 0.3 =  0.03, so  the  lower  probability  ofblue(3)  will  be  at  least  
0.03.  In  fact,  P(blue(3))  =  0.03. Moreover,  we  also  have  P(blue(3))  =  1, 
P(red(3))  =  0.0 and  P(red(3))  =  0.9. 


7   
Complexity  of  lnference  
Inference includes a variety of tasks. In this chapter, we first provide a list of 
inference tasks and then, after introducing some background on complexity 
theory, we present the result on the complexity of the various tasks that are 
present in the literature. 
7.1  lnference  Tasks  
We list here the various inference tasks. Let q  and e  be conjunctions of ground 
literals, the query and the evidence respectively. 
The tasks of inference are: 
• The EVID task is to compute an unconditional probability  P(e),  the 
probability of evidence. This terminology is especially used when P  (e) 
is computed as part of a solution for the COND task. When there is no 
evidence, we will speak of P(q),  the probability ofthe query. 
• In the COND task, we want to compute the conditional probability dis­
tribution ofthe query given the evidence, i.e., compute P(qle).  A related 
task is CONDATOMS where we are given a set of ground atoms Q  and 
we want to compute P(qle)  for each q  E  Q.  
• The MPE task, or mostprobable  explanation,  is to find the most likely 
truth value of all non-evidence atoms given the evidence, i.e., solving 
the optimization problern 
argmaxP(qle) 
q  
with q  being the unobserved atoms, i.e., Q  =  B\E,  where Eis the set of 
atoms appearing in e  and q  is an assignment of truth values to the atoms 
inQ. 
175 

176 
Complexity  of lnference  
• The MAP task, or maximum  a  posteriori,  is to find the most likely value 
of a set of non-evidence atoms given the evidence, i.e., finding 
argmaxP(qle) 
q  
where q  is a set of ground atoms. MPE is a special case of MAP where 
Q  uE = B.  
• The VIT task, or finding the  Viterbi  proof  or most  probable  proof,  is 
the task of finding the composite choice "' that is an explanation (in the 
sense of Section 3.1) and has maximum probability, i.e. 
argmax 
P("')·  
"' is an explanation for q  
It differs from both MPE and MAP because it aims to find an assignment 
to a small set of variables sufficient for explaining a query. It  is called 
Viterbi proof because in a program that implements a Hidden Markov 
Model such as Example 74 it corresponds to the output of the Viterbi 
algorithm on the model, the sequence of states that most likely originated 
the output. 
• The DISTR task involves computing the probability distribution or den­
sity of the non-ground arguments of a conjunction of literals q,  e.g., 
computing the probability density of X  in goal mix(X)  ofthe Gaussian 
mixture of Example 55. If the argument is a single one and is numeric 
(integer or real), then EXP is the task of computing the expected value 
of the argument (see Section 1.5). 
7.2 Background  on  Complexity  Theory  
Here we provide abrief review of concepts from complexity theory following 
[Cozman and Mami, 2020]. Foramore in depth treatment see [Papadimitriou 
and Steiglitz, 1998]. 
Astring is a sequence of 0 and 1. A language is a set of strings. A com­
plexity class of languages is a set of languages. A language defines a decision  
problem;  that is, the problern of deciding whether an input string is in the 
language. 
A Turing machine is an abstract machine containing a tape and a head that 
can move over the tape and read/write symbols. The tape initially contains the 
input and the machine can accept or reject the input. It is formally represented 
by a tuple ( Q,  L.,  qo,  qa,  qr,  6)  where Q  is a set of states, with qo/qq/qr  the 

7.2 Background  on  Complexity  Theory  177 
initial/accepting/rejecting states. ~ is the alphabet of symbols on the tape, 
including 0, 1, and blank, a symbol never present in the input. 8 is a transition 
function that takes the current state and the symbol under the head in the tape 
(a pair from Q  x ~) and returns a subset 8(q,  s) of Q  x ~ x { -1, 0, 1} where 
each triple ( q',  s',  m) in the subset indicates the next state q',  the next symbol 
to write to the tape s'  and the direction in which to move the head ( -1: left, 
0: no motion, 1: right). If the machine reaches qa.  the input is accepted, if the 
machine reaches qr.  the input is rejected. 
If 8 (q,  s) is always a singleton, then the machine is deterministic, other­
wise it is nondeterministic. 
A  language is decided by a Turing machine if the machine accepts all 
strings in the language and rejects all strings not in the language. If the 
number of steps to reach a decision is polynomial in the size of the input, 
the machine is said to be polynomial-time. The complexity class P is the set 
of languages that can be decided by a deterministic polynomial-time Turing 
machine, while NP  is the set oflanguages that can be decided by a nondeter­
ministic polynomial-time Turing machine. The complexity class EXP  is the 
set of languages that can be decided by a deterministic Turing machine in 
time exponential in the input, 
A  probabilistic Turing machine differs from the above definition because 
it defines a uniform probability distribution over triples in 8 ( q,  s). The com­
plexity class PP consists of the languages that are decided by a probabilistic 
Turing machine in a number of steps polynomial in the input size, with an 
error probability strictly less than 1/2 for all input strings. Equivalently, the 
complexity class PP  consists of those languages L  that satisfy the following 
property: there is a polynomial time nondeterministic Turing machine such 
that e E  C  if and only if more than half of the computations of the machine 
on input e end up accepting. 
The class PEXP  consists of those languages L  suchthat there is an expo­
nential time nondeterministic Turing machine such that e E  C  iff more than 
half of the computations of the machine on input e end up accepting. 
The complement  of a  decision  problern  is the decision problern resulting 
from reversing the YES and NO answers. The complement of a complexity 
class Cis the set of complement of every problern in the class and is indicated 
by coC. 
A Turing machine has an oracle for a language L  if it has a second pair 
tape/head such that it can write strings to the secondary tape and then, in a 
single step, read whether the string is in the language L.  

178 
Complexity  of lnference  
The class of languages that can be decided by machines defining com­
plexity class C, with each machine using an oracle for L,  is denoted by C c.  If 
Dis another complexity class, then C0 is defined a UcED  (L. 
p 
p 
Lp 
p 
p 
p 
Lp
Let us define classes Lo  =  P, Lk+l =  NP k, nk =  COLk, .6.k+l =  p k. 
These classes form the polynomial hierarchy PH. 
Wagner counting hierarchy includes classes PP and ppi:~ for k  >  0. 
A many-one reduction from language L  to language L 1  is a polynomial 
time algorithm that transforms a string .e  E L  into a string .e'  E L 1  such that 
.e  E  L  if and only if .e'  E  L 1•  Fora complexity class C, a decision problern L  is 
C-hard if each language in C can be reduced to L  with a many-one reduction. 
A language is then C-complete if it is in C and it is C-hard. 
Besides decision problems, we also consider function  problems,  where 
the output is not simply YES or NO as for decision problems but is more 
complex. Formally, a function problern is defined as a relation R  between 
strings such that an algorithm solves the problern if, for every input x  such 
that there is an y  satisfying (x,  y)  ER,  the algorithm produces one such y.  
The complexity class # P is the class of function problems where the 
relation R  associates input with the number of accepting paths in a nondeter­
ministic polynomial-time Turing machine. A #P problern is at least as hard as 
the corresponding NP prob lern: in fact to solve the NP prob lern, it is enough 
to count solutions with the # P problern and retum YES if the number of 
solutions is greater than 0. #Pis also at least as hard as PP that asks whether 
the majority ofthe computation paths accept. With a PP problern we can find 
the first bit of the solution of the corresponding # P prob lern. 
7.3 Complexity  for  Nonprobabilistic  lnference  
In order to discuss the complexity of the probabilistic inference problems 
seen above, let us first consider the complexity of nonprobabilistic inference. 
Recall the definitions of acyclic and locally stratified program from Def­
inition 4. Here we extend them to the case of disjunctive rules. A program is 
acyclic  if there exists a level mapping for ground atoms such that the level of 
each atom in the head of each ground rule is strictly greater that the level of 
each literal in the body. A program is locally  stratified  if there exists a level 
mapping for ground atoms such that the level of each atom in the head of 
each ground rule is strictly greater than the level of each negative literal in 
the body and greater or equal than the level of each positive literal. 
We distinguish two restrictions on the class of programs 

7.4  Complexity for  Probabilistic  Programs  179 
Table  7.1  Complexity ofthe WFS [Dantsin et al., 2001]. 
Langnage 
Propositional 
Unrestricted 
{}  
p  
EXP 
{not} 
p  
EXP 
• propositional programs  are finite ground programs 
•  bounded-arity  programs  are programs where the arity of predicates is 
bounded. So, when considering the inferential complexity, we want to 
know the computational cost as the size of the program increases but the 
arity of predicates is bounded. In this case, the Herbrand base contains a 
polynomial number of ground atoms but the grounded program may be 
exponentially large as there is no bound on the number of variables in 
each rule. 
Consider first the decision problern 
•  Input:  a logic program P  without function symbols and a conjunction 
of ground literals q  over atoms in ßp  
• Output:  YES if WFM(P)  I=  q  and NO otherwise. 
The complexity of this problern as a function of the encoding in binary of the 
program and the query is discussed in [Dantsin et al., 2001] and the results 
are shown in Table 7.1 for the case of positive ( {}) and normal programs 
( {not}) and for the case of propositional or unrestricted programs. The cells 
report the complexity class for which the problern is complete. As you can 
see, negation doesn't add complexity to the problem. 
For ASP we consider the problern of cautious  inference:  
•  Input:  a logic program P  without function symbols and a conjunction 
of ground literals q  over atoms in ßp  
•  Output:  YES if q  is a cautious consequence of P  under the answer set 
semantics and NO otherwise. 
The complexity of this problern was studied in [Faber et al., 2011 b] and the 
results are shown in Table 7 .2. The rows are associated to the presence or not 
ofvarious syntactic features: not8  indicates negation with local stratification; 
not  non locally stratified negation; and v disjunction in the head. 
In the case of ASP, locally stratified negation, negation and disjunction 
all add complexity 

180 
Complexity  of lnference  
Table  7.2 Complexity of ASP [Faber et al., 201lb]. 
Langnage 
Propositional 
Bounded-arity 
{}  
p  
NP  
{nots} 
{not} 
{v} 
{not 8 ,  v} 
{not,  v} 
p  
coNP  
coNP  
np
2 
np 
2 
fj.P2
np
2np
2 
np
3 
np
3 
7.4  Complexity  for  Probabilistic  Programs  
We consider probabilistic programs P  in the form of ProbLog programs with­
out function symbols, composed of a set of rules R  and a set of probabilistic 
facts F.  
If the rules R  of a program P  are acyclic (locally stratified), then each 
world of P  is acyclic (locally stratified), as probabilistic facts do not impose 
any constraint on the level mapping. 
By Theorem 1, a locally stratified non-probabilistic program has a two­
valued WFM, so a locally stratified probabilistic program has a two-valued 
WFM for each world, and thus a single answer set. Therefore, the distribution 
and the credal semantics for locally stratified probabilistic programs coincide. 
Since every acyclic program is also locally stratified, the same holds for 
acyclic programs. 
We now present the complexity results for acyclic and locally stratified 
program from [Cozman and Maua, 2017] and the the results for general 
programs from [Maua and Cozman, 2020]. 
7.4.1  Complexity  for  acyclic  and  locally  stratified  programs  
Since the distribution and the credal semantics coincide for acyclic and lo­
cally stratified programs, we can consider decision problems involving sharp 
probabilities. 
The decision problern in this case is: 
•  Input:  a probabilistic logic program P  whose probabilities are rational 
numbers, a pair ( q,  e) where q  and e  are conjunction of ground literals 
over atoms in ßp  and a rational ry  E  [0, 1] 
•  Output:  YES if P  (q  I  e) >  ry  and NO otherwise. If P  (e) =  0 the output 
is by convention NO (the input is rejected). 

7.4  Complexity  for  Probabilistic  Programs  181 
Table  7.3 
Complexity of inference for acyclic and locally stratified pro­
grams, extracted from [Cozman and Mami, 2017, Table 2]. 
Langnage 
Propositional 
Bounded Arity 
Unrestricted 
Query 
{nota}  
PP 
ppNP 
PEXP 
PP 
{nots} 
PP 
PP 
PEXP 
PP 
The requirement of rationality of the numbers is imposed in order to be able 
to represent them as a pair of binary integers in the input. The complexity of 
this problern as a function of the encoding in binary of the program and the 
numbers is called the inferential  complexity.  
We also take into account the case where the program is fixed: 
• Fixed:  a probabilistic logic progam P  whose probabilities are rational 
numbers 
•  Input:  a pair ( q,  e) where q  and e  are conjunction of ground literals over 
atoms in Bp  and a rational 1 E  [0, 1] 
• Output:  whether or not P(q  I  e) >  I· If P(e)  = 0 the output is by 
convention NO (the input is rejected). 
The complexity for this problern is the query  complexity.  
Cozman and Mami [2017] provided various results regarding the com­
plexity of the above problems for propositional, bounded-arity and unre­
stricted programs. These results are summarized in Table 7.3, where {nota}  
indicates acyclic programs and {not 8 }  indicates locally stratified programs. 
The column are associated to propositional, bounded arity and unrestricted 
programs, while the last column refers to the query complexity. The cells 
contain a complexity class for which the inferential or query complexity is 
complete. As you can see, there is no advantage in restricting the program to 
be acyclic. 
The function problern of computing P(qle)  from an acyclic program is 
#P-complete. In fact it is #P-hard because a Bayesian network can be con­
verted into an acyclic program (see Example 28) and inference in Bayesian 
network is #P-complete [Koller and Friedman, 2009, Theorem 9.2]. On the 
other band the problern is in # P because an acyclic program can be converted 
to a Bayesian network (see Section 2.5). 

182 
Complexity  of lnference  
7 .4.2  Complexity  results  from  [Maua  and  Cozman,  2020]  
Mami and Cozman [2020] provided more complexity results, taking into ac­
count also positive programs, general programs and the problems ofMPE and 
MAP. 
Differently from the previous section, here the credal semantics does not 
assign sharp probability values to queries, so we must considerer lower and 
upper probabilities. The first problern is called cautious  reasoning  (CR) and 
is defined as 
Input:   a probabilistic logic program P  whose probabilities arerational num­
bers, a pair ( q,  e) where q  and e  are conjunction of ground literals 
over atoms in ßp  and a rational 1 E  [0, 1] 
Output:   YES if P(q  I  e) >  1 and NO otherwise. If P(e)  =  0 the output is 
by convention NO (the input is rejected). 
This problern corresponds to the inferential complexity problern of the pre­
vious section. It  is called cautious reasoning as it involves checlcing the truth 
of queries in all answer sets so it is similar to the problern of computing cau­
tious consequences in non probabilistic reasoning. Similarly, we can call the 
problern of checlcing whether P(q  I  e) >  1 brave  reasoning.  Note that one 
problern reduces to the other as P( q  I  e) ~ 1 ifand only if P( ~q I  e) >  1-r·  
Consider the decision problern for MPE: 
Input:   a probabilistic logic program P  whose probabilities arerational num­
bers, a conjunction of ground literals e  over atoms in ßp  and a 
rational 1 E  [0, 1] 
Output:   YES if maxq  P(qle)  >  1 and NO otherwise, with Q  = ßp\E,  
where E  is the set of atoms appearing in e,  and q  is an assignment 
of truth values to the atoms in Q.  
andforMAP: 
Input:   a probabilistic logic program P  whose probabilities arerational num­
bers, a conjunction of ground literals e  over atoms in ßp,  a set Q  of 
atoms in ßp  and a rational 1 E  [0, 1] 
Output:   YES if maxq P(qle)  >  1 and NO otherwise, with q  an assignment 
of truth values to the atoms in Q.  
The complexity for CR and MPE is given in Table 7 .4, an extract from [Mami 
and Cozman, 2020, Table 1].  The columns correspond to the distinction be­
tween propositional and bounded-arity programs and to CR and MPE. 

7.4  Complexity  for  Probabilistic  Programs  183 
Table  7.4  Complexity of the credal semantics, extracted from [Maua and 
Cozman, 2020, Table 1]. 
Propositional 
Bounded Arity 
Langnage 
CR 
MPE 
CR 
MPE 
{}  
{nots}  
PP  
PP  
NP  
NP  
PP'"~ 
PP'"~ 
p  2 
Lp
2 
{not} 
ppNP 
p 2 
ppi:2 
p 3 
{v}  
ppNP 
p 3 
ppi:2 
p 4 
{nots,  V}  
ppi:2 
p 3 
ppi:3 
p 4 
{not,  v} 
ppi:2 
p 3 
ppi:3 
p 4 
MAP is absent from the table as its complexity is always ppNP  for propo­
sitional programs and ppNPNP  for bounded-arity programs. 


8   
Exact  lnference  
Several approaches have been proposed for inference. Exact inference aims 
at solving the tasks listed in Chapter 7 in an exact way, modulo errors of com­
puter fioating point arithmetic. Exact inference can be performed in various 
ways: dedicated algorithms for special cases, knowledge compilation, con­
version to graphical models, or lifted inference. This chapter discusses exact 
inference approaches except lifted inference which is presented in Chapter 9. 
Exact inference is very expensive as shown in Chapter 7. Therefore, in 
some cases, it is necessary to perform approximate inference, i.e., finding an 
approximation of the answer that is eheaper to compute. The main approach 
for approximate inference is sampling, but there are others such as iterative 
deepening or bounding. Approximate inference is discussed in Chapter 10. 
In Chapter 3, we saw that the semantics for programs with function sym­
bols is given in terms of explanations, i.e., sets of choices that ensure that the 
query is true. The probability of a query (EVID task) is given as a function of 
a covering set of explanations, i.e., a set containing all possible explanations 
for a query. 
This definition suggests an inference approach that consists in finding a 
covering set of explanations and then computing the probability of the query 
from it. 
To compute the probability of the query, we need to make the explanations 
pairwise incompatible: once this is done, the probability is the result of a 
summation. 
Early inference algorithms such as [Poole, 1993b] and PRISM [Sato, 
1995] required the program to be such that it always had a pairwise incom­
patible covering set of explanations. In  this case, once the set is found, the 
computation of the probability amounts to performing a sum of products. For 
programs to allow this kind of approach, they must satisfy the assumptions 
of independence of subgoals and exclusiveness of clauses, which mean that 
[Sato et al., 2017]: 
185  

186 Exact  Inference  
1.  the probability of a conjunction (A,  B)  is computed as the product of 
the probabilities of A  and B  (independent-and  assumption),  
2.  the probability of a disjunction (A;  B)  is computed as the sum of the 
probabilities of A  and B  (exclusive-or  assumption).  
See also Section 8.10. 
8.1  PRISM  
PRISM [Sato, 1995; Sato and Kameya, 2001, 2008] performs inference on 
programs respecting the assumptions of independent-and and exclusive-or by 
means of an algorithm for computing and encoding explanations in a factor­
ized way instead of explicitly generating all explanations. In fact, the number 
of explanations may be exponential, even if they can be encoded compactly. 
Example  74  (Hidden Markov model- PRISM [Sato and Kameya, 2008]). 
An  HMM  [Rabiner,  1989]  is  a  dynamical  system  that,  at  each  integer  time  
point  t,  is  in  a  state  S  from  a  finite  set  and  emits  one  symbol  0  according  
to  a  probability  distribution  P( OIS)  that  is  independent  of time.  Moreover,  
it  transitions  to  a  new  state  N  extS  at  time  t  +  1,  with  N  extS  chosen  ac­
cording  to  P(NextSIS),  again  independently  oftime.  HMMs  are  so  called  
because  they  respect  the  Markov  condition:  the  state  at  time  t  depends  only  
from  the  state  at  time  t  - 1 and  is  independent  of previous  states.  Moreover,  
the  states  are  usually  hidden:  the  task  is  to  obtain  information  on  them  from  
the  sequence  of output  symbols,  modeling  systems  that  can  be  only  observed  
from  the  outside.  HMMs  and  Kaimanfilters  ( see  Example  57)  are  similar,  they  
differ  because  the  first  uses  discrete  states  and  output  symbols  and  the  latter  
continuous  ones.  HMMs  have  applications  in  many  fields,  such  as  speech  
recognition.  
The  following  program  encodes  an  HMM  with  two  states,  {s1, s2}, of  
which  s 1 is  the  start  state,  and  two  output  symbols,  a  and  b:  
values(tr(s1),  [s1, s2]). 
values(tr(s2),  [s1, s2]). 
values(out(_),  [a,  b]).  
hmm(Os)  ~ hmm(s1,  Os).  
hmm(_S,  []). 
hmm(S,  [OIOs])  ~ 

8.1  PRJSM  187 
E1 =  m(out(sl), a),  m(tr(sl), sl), m(out(sl), b),  m(tr(sl), sl), 
m(out(sl), b),  m(tr(sl), sl), 
E2 =  m(out(sl), a),  m(tr(sl), sl), m(out(sl), b),  m(tr(sl), sl), 
m(out(sl), b),  m(tr(sl), s2), 
E3 =  m(out(sl), a),  m(tr(sl), sl), m(out(s2), b),  m(tr(sl), s2), 
m(out(s2),b),m(tr(s2),s1),  
Es  =  m(out(sl), a),  m(tr(sl), s2), m(out(s2), b),  m(tr(s2), s2), 
m(out(s2),b),m(tr(s2),s2)  
Figure  8.1  Explanations for query hmm([a,  b,  b])  ofExample 74. 
msw(out(S),  0), msw(tr(S),  NextS),  hmm(NextS,  Os). 
+-- set_sw(tr(sO),  [0.2, 0.8]). 
+-- set_sw(tr(sl),  [0.8, 0.2]). 
+-- set_sw(out(sO),  [0.5, 0.5]). 
+-- set_sw(out(sl),  [0.6, 0.4]). 
An  example  ofEVID  task  an  this  program  is  computing  P(hmm(Os))  where  
Os  is  a  list  of as  and  bs,  that  is  the  probability  that  the  sequence  of symbols  
Os  is  emitted.  
Note  that msw  atoms  have  two  arguments  here,  so  each  call  to  such  atoms  
is  intended to  refer  to  a  different  random  variable.  This  means  that  ifthe  same  
msw  is  encountered  again  in  a  derivation,  it  is  associated  with  a  different  
random  variable,  differently  from  the  other  languages  under  the  DS  where  a  
ground  instance  of a  probabilistic  clause  is  associated  with  only  one  random  
variable.  The  latter  approach  is  also  called  memoing, meaning  that  the  asso­
ciations  between  atoms  and  random  variables  are  stored for  reuse,  while  the  
approach  of PR/SM  is  often  adopted  by  non-logic probabilistic programming  
languages.  
Consider  the  query  hmm([a,  b,  b])  and  the  problern  of  computing  the  
probability  of  output  sequence  [a,  b,  b].  Such  a  query  has  the  eight  expla­
nations  shown  in  Figure  8.1  where  msw  is  abbreviated  by  m,  repeated  atoms  
correspond  to  different  random  variables,  and  each  explanation  is  a  conjunc­
tion  of msw  atoms.  In  general,  the  number  of explanations  is  exponential  in  
the  length  of the  sequence.  
If the query q  has the explanations E1 ... , En.  we can build the formula 
q  {::?  E1 V ... V En  
expressing the truth of q  as a function of the msw  atoms in the explanations. 
The probability of q  (EVID task) is then given by P(q)  =  
.l:~=l P(Ei)  

188 
Exact Inference  
hmm([a, b,  b])  <* hmm(sl,  [a,  b,  b])  
hmm(sl,  [a,  b,  b])  <* m(out(sl), a),  m(tr(sl), sl), hmm(sl,  [b,  b])v 
m(out(sl), a),  m(tr(sl), s2), hmm(s2,  [b,  b])  
hmm(sl,  [b,  b])  <* m(out(sl), b),  m(tr(sl), sl), hmm(sl,  [b])v 
m(out(sl), b),  m(tr(sl), s2), hmm(s2,  [b])  
hmm(s2,  [b,b])  <* m(out(s2),b),m(tr(s2),sl),hmm(sl,  [b])v 
m(out(s2), b),  m(tr(s2), s2), hmm(s2,  [b])  
hmm(sl,  [b])  <* m(out(sl), b),  m(tr(sl), sl), hmm(sl,  O)v 
m(out(sl), b),  m(tr(sl), s2), hmm(s2,  m  
hmm(s2,  [b])  <* m(out(s2), b),  m(tr(s2), sl), hmm(sl,  O)v 
m(out(s2), b),  m(tr(s2), s2), hmm(s2,  m   
hmm(sl,  m 
<* true   
hmm(s2,  m 
<* true   
Figure  8.2 PRISM formulas for query hmm([a,  b,  b])  ofExample 74. 
and the probability of each explanation is the product of the probability of 
each atom, because explanations are mutually exclusive, as each explanation 
differs from the others in the choice for at least one msw  atom. 
PRISM performs inference by deriving the query with tabling and storing, 
for each subgoal g,  the switches and atoms on which g  directly depends. In 
practice, for each subgoal g,  PRISM builds a formula 
g  ~ sl  V  ...  V  Sn  
where each Si  is a conjunction of msw  atoms and subgoals. For the query 
hmm([a,  b,  b])  from the program ofExample 74, PRISM builds the formulas 
shown in Figure 8.2. 
Differently from explanations, the number of such formulas is linear rather 
than exponential in the length of the output sequence. 
PRISM assumes that the subgoals in the derivation of q  can be ordered 
{gl, ... , 9m} such that 
9i  ~ sil  V  ...  V  Sini  
where q  =  91 and each Sij  contains only msw  atoms and subgoals from 
{9i+l, ... , 9m}· This is called the acyclic  support  condition  and is true if 
tabling succeeds in evaluating q,  i.e., if it doesn't go into a loop. 
From these formulas, the probability of each subgoal can be obtained by 
means of Algorithm 5 that computes the probability of each subgoal bot­
tom up and reuses the computed values for subgoals higher up. This is a 
dynamic  programming  algorithm: the problern is solved by breaking it down 
into simpler sub-problems in a recursive manner. 

8.1  PRJSM  189 
Algorithm 5 Function PRISM-PROB: Computation of the probability of a 
query. 
1: function PRISM-PROB(q) 
2: 
for all i, k  do 
3: 
P(msw(i,  vk))  +-- ITik  
4: 
end for 
5: 
for i  +-- m  ~ 1 do 
6: 
P(g;)  +-- 0 
1>- P(g;)  is the probability of goa1 g;  
7: 
for j  +-- 1 ~ n;  do 
8: 
Let S;j  be h1, ... , ho  
9: 
R(g;,  S;j)  +-- [1~= 1 P(hl)  
1>- R(g;,  S;j)  is the probability of exp1anation 
10: 
S;j  of goal g;  
11: 
P(g;)  +-- P(g;)  +  R(gi,Sii)  
12: 
end for 
13: 
end for 
14: 
return P(q)  
15: end function 
P(hmm(s1,  0))  = 1   
P(hmm(s2,  0))  = 1   
P(hmm(s1,  [b])  =  
P(m(out(s1),  b))  · P(m(tr(s1),  s1)) · P(hmm(s1,  0))+ 
P(m(out(s1),  b))  · P(m(tr(s1),  s2)) · P(hmm(s2,  0))  
P(hmm(s2,  [b])  = 
P(m(out(s2),  b))  · P(m(tr(s2),  s1)) · P(hmm(s1,  0))+ 
P(m(out(s2),  b))  · P(m(tr(s2),  s2)) · P(hmm(s2,  0))  
Figure 8.3 
PRISM computations for query hmm([a,  b,  b])  ofExample 74. 
For the example above, the probabilities are computed as in Figure 8.3. 
The cost of computing the probability of the query in this case is thus 
linear in the length of the output sequence, rather than exponential. 
In the case of HMMs, computing the probability of an output sequence 
using PRISM has the same complexity O(T)  as the specialized forward al­
gorithm [Rabiner, 1989], where T  is the length ofthe sequence. 
The MPE task can be performed by replacing summation in Algorithm 5 
with max and arg max. In the case of HMM, this yields the most likely 
sequence of states that originated the output sequence, also called the Viterbi 
path. Such a path is computed for HMMs by the Viterbi algorithm [Rabiner, 
1989] and PRISM has the same complexity O(T).  

190 
Exact  Inference  
Writing programs satisfying the assumptions of independence of subgoals 
and exclusiveness of clauses is not easy and significantly limits the modeling 
power of the language. Therefore, work was dedicated to lifting these lim­
itations, leading to the AILog2 system Poole [2000] that used the splitting 
algorithm (Algorithm 1), and to the adoption ofknowledge compilation. 
8.2 Knowledge  Compilation  
Knowledge  compilation  [Darwiche and Marquis, 2002] is an approach for 
solving certain hard inference tasks on a Boolean formula by compiling it 
into a form on which the tasks are tractable. Clearly, the complexity is not 
eliminated but moved to the compilation process. The inference tasks of 
COND, MPE, and EVID can be solved using well-known techniques from 
the SAT community applied to weighted Boolean formulas, such as tech­
niques for Weighted model counting (WMC) or weighted MAX-SAT. For 
example, EVID reduces to WMC, i.e., computing the sum of the weights of 
the worlds where the formula is true. WMC is a generalization of the prob­
lern of counting the number of worlds where the query is true, also known 
as model  counting.  Model counting and WMC are #P-complete in general 
but can be computed in polynomial time for Boolean formulas in certain 
forms. 
The knowledge compilation approach to inference follows the two-step 
method where first the program, the query, and the evidence are converted 
into a Boolean formula encoding a covering set of explanations and then 
knowledge compilation is applied to the formula. 
The formula is a function of Boolean variables, each encoding a choice, 
that takes value 1 exactly for assignments corresponding to worlds where 
the query is true. Therefore, to compute the probability of the query, we can 
compute the probability that the formula takes value 1 knowing the proba­
bility distribution of all the variables and that they are pairwise independent. 
This is done in the second step, which amounts to converting the formula into 
a form from which computing the probability is easy. The conversion step 
uses knowledge compilation because it compiles the formula into a special 
form with the property that the cost of solving the problern is polynomial. 
This second step is a well-known problern also called disjoint-sum  and is an 
instance of WMC. 

8.3  ProbLogl  191 
8.3 Problog1  
The ProbLog1 system [De Raedt et al., 2007] compiles explanations for 
queries from ProbLog programs to BDDs. It  uses a source to source transfor­
mation of the program [Kimmig et al., 2008] that replaces probabilistic facts 
with clauses that store information about the fact in the dynamic database of 
the Prolog interpreter. When a successful derivation for the query is found, 
the set of facts stored in the dynamic database is collected to form a new 
explanation that is stored away and backtracking is triggered to find other 
possible explanations. 
If K  is the set of explanations found for query q,  the probability of q  is 
given by the probability of the formula 
!K(X)  = V  1\  xij  1\  -.xij  
KEK  (F;,Oj,l)EK 
(F;,Oj,O)EK  
where Xij  is a Boolean random variable associated with grounding Fi(}j  of 
fact Fi  and P(Xij  =  1) =  Ik 
Consider Example 14 that we repeat here for ease of reading 
sneezing(X)  ~ fiu(X),Jlu_sneezing(X).  
sneezing(X)  ~ hayJever(X),  hayJever _sneezing(X).  
fiu(bob).  
hayJever(bob).  
H  
=  
0.7 :: fiu_sneezing(X).  
F2 
=  
0.8 :: hayJever _sneezing(X),  
A set of covering explanations for sneezi ng (bob)  is K  = { "'1 , "'2} with 
"'I= {(FI,{X/bob},1)} "'2 = {(F2,{Xjbob}, 1)} 
Ifwe associate Xn with F1 {X/bob}  and X21 with F2{Xjbob},  the Boolean 
formula is 
!K(X)  = Xn v X21 
(8.1)  
In  this simple case, the probability that fK(X)  takes value 1 can be computed 
by using the formula for the probability of a disjunction: 
P(Xn v  X21) =  P(Xn) +  P(X2I) - P(Xn 1\  X2I) 
and, since X11 and X21 are independent, we get 
P(JK(X))  =  P(Xn v X2I) =  P(Xn) +  P(X21)- P(Xn)P(X21)· 

192 
Exact  Inference  
----~ 
Xn 
X21 
Figure  8.4  BDD representing Function 8.1. 
In the general case, however, simple formulas like this can't be applied. 
BDDs are a target language for knowledge compilation. A BDD for a 
function of Boolean variables is a rooted graph that has one Ievel for each 
Boolean variable. A node n  has two children: one corresponding to the 1 
value of the variable associated with the Ievel of n  and one corresponding 
to the 0 value of the variable. When drawing BDDs, the 0-branch is distin­
guished from the 1-branch by drawing it with a dashed line. The leaves store 
either 0 or 1. For example, a BDD representing function (8.1) is shown in 
Figure 8.4. 
Given values for all the variables, the computation of the value of the 
function can be performed by traversing the BDD starting from the root and 
returning the value associated with the leaf that is reached. 
To compile a Boolean formula f(X)  into a BDD, software packages 
incrementally combine sub-diagram using Boolean operations. 
BDDs can be built with various software packages that perform knowl­
edge compilation by providing Boolean operations between diagrams. So the 
diagram for a formula is obtained by applying the Boolean operations in 
the formula bottom-up, combining diagrams representing Xij  or -.Xij  into 
progressively more complex diagrams. 
After the application of an operation, isomorphic portians of the resulting 
diagram are merged and redundant nodes are deleted, possibly changing the 
order of variables if useful. This often allows the diagram to have a number of 
nodes much smaller than exponential in the number of variables that a naive 
representation of the function would require. 
In  the BDD of Figure 8.4, a node for variable X21 is absent from the path 
from the root to the 11eaf. The node has been deleted because both arcs from 
the node would go to the 11eaf, so the node is redundant. 
Example  75 (Epidemie - ProbLog). The  following  ProbLog  program  P  
encodes  a  very  simple  model  ofthe  development  of an  epidemic:  

8.3  ProbLogl  193 
-----------~--~ 
Xn 
x21 
X22 
Figure  8.5 BDD for query epidemic  of Example 75. 
epidemic  ~ flu()(),ep()(),cold.  
flu( david).  
flu(robert).  
F1 = 0.7 ::cold.  
F2 = 0.6 :: ep()().  
This  program  models  the  fact  that,  if somebody  has  the  flu  and  the  climate  is  
cold,  there  is  the  possibility  that  an  epidemic  arises.  We  are  uncertain  about  
whether  the  climate  is  cold  but  we  know  for  sure  that  David  and  Robert  have  
the  jlu.  The  atom  ep()()  models  the  fact  that  individual)(  enables  the  onset  
of the  epidemic.  Fact  F1 has  one  grounding,  associated  with  variable  Xn, 
while  F2 has  two  groundings,  associated  with  variables  X21 and  X22· The  
query  epidemic  is  true  if the  Boolean  formula  
f(X)  =  Xu 1\  (X21 V x22) 
is  true.  The  BDD  representing  this  formula  is  shown  in  Figure  8.5.  As  you  
can  see,  the  subtree  rooted  at  X22 can  be  reached  by  more  than  one  path.  
In  this  case,  the  BDD  compilation  system  recognized  the  presence  of  two  
isomorphic subgraphs and merged them,  besides deleting nodes for  X21 from  
the  pathfrom  Xn to  X22 andfor  X22from  the  pathfrom  X21 to  the  0  leaf  
BDDs perform a Shannon expansion of the Boolean formula: they express 
the formula as 
!K(X)  =  x1 1\  ii?  (X)  v  --.xl 1\  Ji/
1  (X)  
where X is the variable associated with the root node ofthe diagram, jj{1  
1 
(X)  
is the Boolean formula where X1 is set to 1, and fi/ 1  (X)  is the Boolean 
formula where X1 is set to 0. The expansion can be applied recursively to the 
functions jj{1  (X)  and fi/ 1  (X).  

194 
Exact  Inference  
The formula is thus expressed as a disjunction of mutually exclusive 
terms, as one contains X1 and the other -.Xl. Moreover X1 is indepen­
dent from JJ?  (X)  and fi/ 1  (X)  because it does not appear there, so the 
probability of the formula can be computed with 
P(JK(X))  = P(Xl)P(ff1 (X))  +  (1- P(Xl))P(fi/1 (X)).  
For the BDD of Figure 8.5, this becomes 
P(JK(X))  = 0.7 · P(ff11 (X))  +  0.3 · P(fi/11 (X))  
This means that the probability of the formula, and so of the query, can be 
computed with Algorithm 6: the BDD is recursively traversed and the proba­
bility of a node is computed as a function of the probabilities of its children. 
Note that a tableis updated to store the probability of nodes already visited: 
in fact, BDDs can have multiple paths to a node in case two sub-diagrams 
are merged and, if a node already visited is encountered again, we can simply 
retrieve its probability from the table. This ensures that each node is visited 
exactly once, so the cost of the algorithm is linear in the number of nodes. 
This is an instance of a dynamic programming algorithm. 
Algorithm  6 Function PROB: Computation ofthe probability of a BDD. 
1: function PROB(node)  
2: 
if node  is a terminal then 
3: 
return 1 
4: 
eise 
5: 
if Table(node)  #-null  then 
6: 
return Table(node)  
7: 
eise 
8: 
pO  ~PROB(childo(node)) 
9: 
pl ~PROB(child1 (node )) 
10: 
Iet 1r be tbe probability of being true of var( node)  
11:  
Res~pl·n+pO·(l-n) 
12: 
add node-->  Res  to Table  
13: 
return Res  
14: 
endif 
15: 
endif 
16: eud function 
8.4  cplint  
The cplint system (CPLogic INTerpreter) [Riguzzi, 2007a] applies knowl­
edge compilation to LPADs. Differently from ProbLog1, the random vari­

8.4  cplint  195 
ables associated with clauses can have more than two values. Moreover, with 
cplint, programs may contain negation. 
To handle multivalued random variables, we can use MDDs [Thayse et al., 
1978], an extension of BDDs. Similarly to BDDs, an MDD represents a 
function f(X)  taking Boolean values on a set of multivalued variables X 
by means of a rooted graph that has one Ievel for each variable. Each node 
has one child for each possible value of the multivalued variable associated 
with the Ievel of the node. The leaves store either 0 or 1. Given values for all 
the variables X, we can compute the value of f(X)  by traversing the graph 
starting from the root and retuming the value associated with the leaf that is 
reached. 
In order to represent sets of explanations with MDDs, each ground clause 
Ci()j  appearing in the set of explanations is associated with a multivalued 
variable Xij with as many values as atoms in the head of Ci.  Each atomic 
choice (Ci, () j,  k)  is represented by the propositional equation Xij =  k.  
Equations for a single explanation are conjoined and the conjunctions for 
the different explanations are disjoined. The resulting function takes value 1 
if the values taken by the multivalued variables correspond to an explanation 
for the goal. 
Example  76  (Detailed medical symptoms - MDD). Consider  Example  20  
that  we  repeat  here  for  readability:  
C1 = strong_sneezing(X)  : 0.3; moderate_sneezing(X)  : 
0.5 +--- fiu(X).  
C2 = strong_sneezing(X)  : 0.2; moderate_sneezing(X)  : 
0.6 +--- hayJever(X).  
fiu(bob).  
hayJever(bob).  
A  set  of explanationsfor  strong_sneezing(bob)  is  K  = {t~;l, t~;2} with  
t1;1 = {(C1, {X/bob},  1)} 
t1;2 = {(C2, {X/bob},  1)} 
This  set  of explanations  can  be  represented  by  the  function  
fx(X)  =  (Xn =  1)  v (X21 =  1)  
(8.2) 
The  corresponding  MDD  is  shown  in  Figure  8.6.  
The  probability  of the  goal  is  given  by  the  probability  of fx(X)  taking  
value  1.  

196 
Exact  Inference  
Figure  8.6 MDD for the diagnosis program of Example 20. 
As BDDs, MDDs represent a Boolean function f(X)  by means of a 
generalization of the Shannon expansion: 
f(X)  =  (X1 =  1) A !X1 =1(X) v ... v (X1 =  n)  A fx 1 =n(X) 
where X1 is the variable associated with the root node of the diagram and 
fX 1 =k(X) is the function associated with the k-th child of the root node. 
The expansion can be applied recursively to the functions fX 1 =k(X). This 
expansion allows the probability of f(X)  tobe expressed by means of the 
following recursive formula 
P(f(X))  =   P(X1 =  1)  ·  P(JX1 = 1(X))  +  ...  +  
P(X1 =  n)  ·  P(JX1 =n(X)) 
because the disjuncts are mutually exclusive due to the presence of the X1 =  
k  equations and X1 doesn't appear in fX 1 =k(X). Thus, the probability of 
f(X)  can be computed by means of a dynamic programming algorithm sim­
ilar to Algorithm 6 that traverses the MDD and sums up probabilities. 
Knowledge compilation libraries for MDDs combine diagrams represent­
ing equations of the form Xi =  k  into progressively more complex diagrams 
using Boolean operations. However, most libraries are restricted to work on 
BDD, i.e., decision diagrams where all the variables are Boolean. To work 
on MDDs with a BDD package, we must represent multivalued variables by 
means ofbinary variables. Various options are possible, in cplint [Riguzzi, 
2007a], the choice was to use a binary encoding. A multivalued variable X 
with n  values is encoded with b  =  rzog2nl Boolean variables X1, ... , Xb: if 
k  =  kb  ...  k1 is the binary encoding of value k,  then X =  k  is encoded as 
x1 = k1 1\  ...  1\  xb  = kb.  
When compiling the formula to a BDD, we need to ensure that Boolean 
variables encoding the same multivalued variable are kept tagether in the 
BDD. Then, the dynamic programming algorithm for computing the prob­
ability is adjusted to correctly retrieve the probability values IIk  by detecting 
the value k  from the configuration of Boolean variables. 

8.5  SLGAD  197 
cplint finds explanations by using a meta-interpreter (see Section 1.3) 
that resolves the query and keeps a list of the choices encountered during the 
derivation. Negativegoals of the form ~a are handled by finding a covering 
set K  of explanations for a  and then computing their complement, i.e., a set of 
explanations K  that identifies all the worlds where a  is false. This is done with 
an algorithm similar to function DUALS(K) of Algorithm 2 [Poole, 2000]: 
an explanation "' in K  is generated by picking an atomic choice (Ci, Bj,  k)  
from each explanation of K  and inserting in K  an explanation containing 
(Ci, Bj,  k')  with k'  =I=  k.  By doing this in all possible ways, the complement 
K  of K  is obtained. 
The system PICL [Riguzzi, 2009] applies this approach to ICL and, to­
gether with the cplint system, formed the initial core ofthe cplint suite 
of algorithms. 
8.5 SLGAD  
SLGAD [Riguzzi, 2008a, 2010] was an early system for performing infer­
ence from LPADs using a modification of SLG resolution [Chen and Warren, 
1996], see Section 1.4.2. SLG resolution is the main inference procedure for 
normal programs under the WFS and uses tabling to ensure termination and 
correctness for a large class of programs. 
SLGAD is of interest because it performs inference without using knowl­
edge compilation, exploiting the feature of SLG resolution that answers are 
added to the table only once, the first time they are derived, while the fol­
lowing calls to the same subgoal retrieve the answers from the table. So the 
decision of considering an atom as true is an atomic operation. Since this 
happens when the body of the grounding of a clause with that atom in the 
head has been proved true, this corresponds to making a choice regarding 
the clause grounding. SLGAD modifies SLG by performing backtracking on 
such choices: when an answer can be added to the table, SLGAD checks 
whether this is consistent with previous choices and, if so, it adds the current 
choice to the set of choices of the derivation, adds the answer to the table 
and leaves a choice point, so that the other choices are explored in backtrack­
ing. SLGAD thus retums, for a ground goal, a set of explanations that are 
mutually exclusive because each backtracking choice is made among incom­
patible alternatives. Therefore, the probability of the goal can be computed 
by summing the probability of the explanations. 
SLGAD was implemented by modifying the Prolog meta-interpreter of 
[Chen et al., 1995]. Its comparison with systems performing knowledge com­

198 
Exact  Inference  
pilation, however, showed that SLGAD was slower, probably because it is not 
able to factor explanations as, for example, PRISM does. 
8.6 PITA  
The PITA system [Riguzzi and Swift, 2010, 2011, 2013] performs inference 
for LPADs using knowledge compilation to BDDs. 
PITA applies a program transformation to an LPAD to create a normal 
program that contains calls for manipulating BDDs. In the implementation, 
these calls provide a Prologinterface to the CUDD [Somenzi, 2015] C library 
and use the following predicates 1 
•  init  and end:  for allocation and deallocation of a BDD manager, a data 
structure used to keep track of the memory for storing BDD nodes; 
•  zero( -BDD),  one( -BDD),  and( +BDD1, +BDD2,  -BDDO),  
or( +BDD1, +BDD2,  -BDDO)  
and 
not( +BDDI,  -BDDO):  
Boolean operations between BDDs, where +  denotes input arguments 
and- output arguments; 
•  add_ var( + N  _Val,  +Probs,  - Var):  addition of a new multivalued vari­
able with N  _Val  values and parameters Probs  (a list). It  retums an 
integer; 
•  equality( + Var,  + Value,  -BDD):  retums aBDDrepresentingtheequal­
ity Var  = Value,  i.e., that the random variable Var  is assigned Value  
in theBDD; 
• ret_prob( + BDD,-P):  retums the probability of the formula encoded 
by BDD.  
add_var( +N _Val,  +Probs,- Var)  adds a new random variable associated 
with a new instantiation of a rule with N  _Val  head atoms and parameters 
list Probs,  and retums an integer indexing the new variable. The auxiliary 
predicate get_var _nj 4  is used to wrap add_var /3  and avoids adding a new 
variable when one already exists for an instantiation. As shown below, a new 
fact var(R,  S,  Var)  is asserted each time a new random variable is created, 
where R  is an identifier for the LPAD clause, S  is a list of constants, one for 
each variables of the clause, and Var  is a integer that identifies the random 
variable associated with the specific grounding of clause R  identified by S.  
The auxiliary predicate has the following definition: 
1  BDDs are represented in CUDD as pointers to their root node. 

8.6  PITA  199 
get_var _n(R,  S,  Probs,  Var)  +­ 
(var(R,  S,  Var)  ~  
true   
length(Probs,  L),   
add_var(L,  Probs,  Var),   
assert(var(R,  S,  Var))   
).  
where Probs  is a list of fioats that stores the parameters in the head of rule 
R.  R,  S,  and Probs  are input arguments while Var  is an output argument. 
assert/1  is a builtin Prolog predicate that adds its argument to the program, 
allowing its dynamic extension. 
The PITA transformation applies to atoms, literals, conjunction ofliterals, 
andclauses. Thetransformationforan atoma anda variable D,  PITA(a,  D),  
is a  with the variable D  added as the last argument. The transformation for a 
negative literal b  ="-'a,  P IT A(b,  D),  is the expression 
(PITA(a,  DN)  ~ not(DN,  D);  one(D))  
which is an if-then-else construct in Prolog: if P  IT A(a,  DN)  evaluates to 
true, then not(DN,  D)  is called, otherwise one(D)  is called. 
A conjunction of literals b1, ... , bm  becomes: 
PITA(b1,  ... , bm,  D)  =  one(DDo),   
PITA(b1,  D1), and(DDo,  D1, DDI),  ... ,   
PITA(bm,  Dm),  and(DDm-l,  Dm,  D).   
The disjunctive clause Cr  =  h1 : Ih v  ...  v  hn  : IIn  +-- b1, ... , bm,  where 
the parameters sum to 1, is transformed into the set of clauses P IT A (Cr)  =  
{PITA(Cr,  1),  ...  PITA(Cn  n)} with: 
PITA(Cr,  i) =  PITA(hi,  D)  +-- PITA(b1,  ...  , bm,  DDb),  
get_var _n(r,  S,  [II1, ... , IIn],  Var),  equality(Var,  i, DD),  
(8.3) 
and(DDb,  DD,  D).  
for i  = 1, ... , n,  where S  is a list containing all the variables appearing in Cr.  
A non-disjunctive fact Cr  = h  is transformed into the clause 
PITA(Cr)  = PITAh(h,  D)  +-- one(D).  
A disjunctive fact Cr  =  hl  : rrl V ... V hn  : IIn.  where the parameters sum 
to 1, is transformed into the set of clauses 
PITA(Cr)  =  {PITA(Cn  1),  ...  PITA(Cr,  n)} 
with: 

200 
Exact Inference  
PIT A(Cr,  i) = PIT A(hi,  D)  ~ get_var _n(r,  S,  [Ih, ... ,IIn], Var),  
equality(V ar,  i, D).  
for i  =  1, ... , n.  
In  the case where the parameters do not sum to one, the clause is first 
transformed into null  : 1- .L:7  rri V hl : rrl V ••. V hn  : IIn. and then into 
the clauses above, where the Iist of parameters is [1 - .L:7  IIi, II1, ... , IIn,] 
but the 0-th clause (the one for null)  is not generated. 
The definite clause Cr  = h  ~ b1, b2, ... , bm.  is transformed into the 
clause 
PITA(Cr)  =  PITA(h,  D)  ~ PITA(b1,  ... , bm,  D).  
Example  77 (Medical example - PITA). Clause  C1 from  the  LPAD  of Ex­
ample  76  is  translated  to  
strong_sneezing(X,  D)  ~ one(DDo),fiu(X,  Dl),   
and(DDo,  D1, DDl),   
get_var _n(1, [X],  [0.3, 0.5, 0.2], Var),   
equality(Var,  1, DD),  and(DD1,  DD,  D).   
moderate_sneezing(X,  D)  ~ one(DDo),fiu(X,  Dl),   
and(DDo,  D1, DDl),   
get_var _n(1, [X],  [0.3, 0.5, 0.2], Var),   
equality(Var,  2, DD),  and(DD1,  DD,  D).   
while  clause  c3 is  translated  to  
flu(david,  D)  ~ one(D).  
In order to answer queries, the goal prob( Goal,P)  is used, which is defined by 
prob( Goal,  P)  ~ init,  retractall( var(_,  _, _) ),  
add_bdd_arg(Goal,  BDD,  GoalBDD),   
(call(GoalBDD)  ~ ret_prob(BDD,  P);  P  = 0.0),  
end.   
whereadd_bdd_arg(Goal,  BDD,  GoalBDD)  returns PITA(Goal,  BDD)  
in GoalBDD.  
Since variables may by multivalued, an encoding with Boolean variables 
must be chosen. The encoding used by PITA is the same as that used to trans­
late LPADs into ProbLog seen in Section 2.4  that was proposed in [De Raedt 
et al., 2008]. 
Consider a variable Xij  associated with grounding () j  of clause Ci  having 
n  values. We encode it using n  - 1 Boolean variables 
Xijl, ... , Xijn-1· 

8.6  PITA  201 
We represent the equation Xij = k  for k  = 1, ... n  - 1 by means of the 
conjunction Xijl 1\  ... AXijk-1 1\Xijk·  and the equation Xij = n  by means of 
the conjunction Xijl 1\  ...  1\  Xijn-l·  The BDD representation ofthe function 
in Equation (8.2) is given in Figure 8.7. The Boolean variables are associated 
----G 
Xu1 
X121 
Figure  8.7 BDD representing the function in Equation (8.2). 
with the following parameters: 
P(Xijl)  
P(Xijl  = 1)  
P(Xij  =  k) 
P(Xijk)  
TI7~1
1 (1- P(Xijt))  
PITA uses tabling, see Section 1.4.2, that ensures that, when a goal is asked 
again, the answers for it already computed are retrieved rather than recom­
puted. This saves time because explanations for different goals are factored 
as in PRISM. Moreover, it also avoids non-termination in many cases. 
PITA also exploits a feature of XSB Prolog called answer  subsumption  
[Swift and Warren, 2012] that, when a new answer for a tabled subgoal is 
found, combines old answers with the new one according to a partial or­
der or lattice. For example, if the lattice is on the second argument of a 
binary predicate p, answer subsumption may be specified by means of the 
declaration 
:-table p(_, lattice(orl3)). 
where o r  I  3 is the join operation of the lattice. Thus, if a table has an answer 
p ( a, dl) and a new answer p ( a, d2) is derived, the answer p ( a, dl) is 
replaced by p ( a, d3) , where d3 is obtained by calling o r (dl, d2, d3) . 
In PITA, various predicates should be declared as tabled. For a predicate 
p In,  the declaration is 
:-table p(_l, ... ,_n, lattice(orl3)). 

202 
Exact  Inference  
which indicates that answer subsumption is used to form the disjunction of 
BDDs in the last argument, using the disjunction operation between BDDs. 
At a minimum, the predicate of the goal and all the predicates appearing 
in negative literals should be tabled with answer subsumption. 
Thanks to answer subsumption, a call to a goal P  IT A(a,  D),  where bis 
ground, retums in D  a BDD encoding the set of all of the explanations for 
a.  Therefore, the transformation P IT A(b,  D)  for a negative literal b  =~ a  
in the body of a rule, first calls P  IT A( a,  D N).  If the call fails, a  does not 
have any explanation, so the BDD for b  should be the one Boolean function. 
Otherwise, the BDD DN  is negated with not/2.  So the transformation of 
the body of a clause makes sure that, to compute the BDD associated with 
the head atom, the BDDs of allliterals are computed and conjoined with the 
BDD encoding the choice relative to the clause. 
If predicates appearing in negative literals are not tabled with answer 
subsumption, PITA is not correct as a call to a subgoal doesn't collect all 
explanations. It is usually useful to table every predicate whose answers have 
multiple explanations and are going to be reused often since in this way 
repeated computations are avoided and explanations are factored. 
PITA was originally available only for XSB because it was the only Pro­
log offering answer subsumption. Recently, answer subsumption was included 
in SWI-prolog and PITA is now available also for it in the cplint suite for 
SWI-prolog. In Prologs without answer subsumption, such as YAP, answer 
subsumption can be simulated using a slightly different PITA transformation 
with explicit findall/3  calls. findall(Template,  Goal,  Bag)  creates the list 
Bag  of instantiations of Template  for which Goal  succeeds. For example, 
findall(X,  p(X),  Bag)  retums in Bag  the list of values of X  for which 
P(X)  succeeds. 
PITA was shown correct and terminating under mild conditions [Riguzzi 
and Swift, 2013] . 
8. 7  Problog2  
The ProbLog2 system [Fierens et al., 2015] is a new version of ProbLogl 
(Section 8.3) that performs knowledge compilation to d-DNNFs rather than 
to BDDs. 
ProbLog2 can perform the tasks CONDATOMS, EVID, and MPE over 
ProbLog programs. It also allows probabilistic intensional facts of the form 
II  ::  j(X1, X2, ... , Xn)  ~ Body  

8.  7 ProbLog2  203 
with Body  a conjunction of calls to non-probabilistic facts that define the 
domains ofthe variables XI,  X2, ... , Xn.  Moreover, ProbLog2 allows anno­
tated disjunctions in LPAD style of the form 
IIii :: hii;  ... ; IIin;  ::hin;~ bii,  ...  ,  bim;  
which are equivalent to an LPAD clauses of the form 
hii  IIii , . . . ; hin;  : IIin;  ~ bii,  ... , bim;  
and are handled by translating them into probabilistic facts using the tech­
nique of Section 2.4. Let us call ProbLog2 this extension of the language of 
ProbLog. 
Example  78  (Alarm- ProbLog2 [Fierens et al., 2015]). The  following  pro­
gram  is  similar  to  the  alarm  BN of Examples  11  and  28:  
0.1 :: burglary.  
0.2 :: earthquake.  
0.7 :: hears_alarm(X)  ~ person(X).   
alarm  ~ burglary.   
alarm  ~ earthquake.   
calls(X)  ~ alarm,  hears_alarm(X).   
per son (mary).   
per son (j ahn).   
Dif.ferently from the alarm  BN of Examples 11  and 28,  two people can call in  
case  they  hear  the  alarm.  
ProbLog2 converts the program into a weighted Boolean formula and then 
performs WMC. A weighted Boolean formula  is a Boolean formula over a set 
of variables V  =  {VI,  ... , Vn}  associated with a weight function w(·)  that 
assigns a real number to each literal (a variable or its negation) built on V.  
The weight function is extended to assign a real number to each assignment 
w  =  {VI  =VI,  ...  , Vn  =  vn}  to the variables of V: 
w(w)  =  n w(l)  
lEw  
where each variable assignment in w  is interpreted as a literal. Given the 
weighted Boolean formula c/J, the weighted  model  count  of c/J, WMCv(cfJ),  
with respect to the set of variables V,  is defined as 
WMCv(cfJ)  = 
L.:  
w(w).  
wESAT(cf>)  

204 
Exact  Inference  
where S  AT (cp)  is the set of assignments satisfying cp  (the assignments for 
which cp  is true). 
ProbLog2 converts the program into a weighted formula in three steps: 
1.  Grounding P  yielding a program P ,  
9 taking into account q  and ein order 
to consider only the part of the program that is relevant to the query given 
the evidence. 
2.  Converting the ground rules in P9  to an equivalent Boolean
formula c/Jr.  
3.  Taking into account the evidence and defining a weight function. A 
Boolean formula c/Je  representing the evidence is conjoined with c/Jr  ob­
taining formula cp  and a weight function is defined for allliterals built on 
variables in c/J. 
The grounding of the program can be restricted to the relevant rules only 
(see Section 1.3). SLD resolution is used to find relevant rules by proving 
all atoms in q,  e.  Tabling is used to avoid proving the same atom twice and 
to avoid going into infinite loops if the rules are cyclic. As programs are 
range restricted, all the atoms in the rules used during the SLD resolution 
will eventually become ground, and hence also the rules themselves. 
Moreover, inactive rules encountered during SLD resolution are omitted. 
A ground rule is inactive  if the body of the rule contains a literal l  that is 
false in the evidence (Z can be an atom that is false in e,  or the negation of an 
atom that is true in e). Inactive rules do not contribute to the probability of 
the query, so they can be safely omitted. 
The relevant ground program contains all the information necessary for 
solving the corresponding EVID, COND, or MPE task. 
Example  79 (Alarm - grounding - ProbLog2 [Fierens et al., 2015]). If  
q  = {burglary}  and  e  = calls(john)  in  Example  78,  the  relevant  ground  
program  is  
0.1 :: burglary.  
0.2 :: earthquake.  
0.7 :: hears_alarm(john).   
alarm  ~ burglary.   
alarm  ~ earthquake.   
calls(john)  ~ alarm,  hears_alarm(john).   
The relevant ground program is now converted to an equivalent Boolean for­
mula. The conversion is not merely syntactical as logic programming makes 
the Closed World Assumption while first-order logic doesn't. 

8.  7 ProbLog2  205 
If rules are acyclic, Clark's completion (see Section 1.4.1) can be used 
[Lloyd, 1987]. If rules are cyclic, i.e., they contain atoms that depend pos­
itively on each other, Clark's completion is not correct [Janhunen, 2004]. 
Two algorithms can then be used to perform the translation. The first [Jan­
hunen, 2004] removes positive loops by introducing auxiliary atoms and rules 
and then applies Clark's completion. The second [Mantadelis and Janssens, 
2010] first uses tabled SLD resolution to construct the proofs of all atoms in 
atoms(q)  u  atoms(e),  then collects the proofs in a data structure (a set of 
nested tries), and breaks the loops to build the Boolean formula. 
Example  80  (Smokers - ProbLog [Fierens et al., 2015]). The  following  
program  models  causes  for  people  to  smoke:  either  they  spontaneously  start  
because  of stress  or  they  are  influenced  by  one  of their friends:  
0.2 :: stress(P)  ~ person(P).  
0.3 :: infiuences(P1,  P2) ~ friend(P1,  P2).  
person(p1).   
person(p2).   
person(p3).   
friend (p1, p2).  
friend (p2, p1).  
friend (p1, p3).   
smokes(X)  ~ stress(X).   
smokes(X)  ~ smokes(Y),  infiuences(Y,  X).   
With  the  evidence  smokes(p2)  and  the  query  smokes(p1),  we  obtain  the  
following  ground pro gram:  
0.2 :: stress(p1).  
0.2 :: stress(p2).  
0.3 :: infiuences(p2,p1).  
0.3 :: infiuences(p1,p2).   
smokes(p1)  ~ stress(p1).   
smokes(p1)  ~ smokes(p2),  infiuences(p2,p1).   
smokes(p2)  ~ stress(p2).   
smokes(p2)  ~ smokes(p1),  infiuences(p1,p2).   
Clark's  completion  would  generate  the  Booleanformula  
smokes(p1)  ~ stress(p1)  v  smokes(p2),  infiuences(p2,p1).  
smokes(p2)  ~ stress(p2)  v  smokes(p1),  infiuences(p1,p2).  
which  has  a  model  
{smokes(p1),  smokes(p2),  -.stress(p1),  -.stress(p2),  
infiuences(p1,  p2), infiuences(p2, p1), ...} 

206  Exact  Inference  
which is not a model  of any world  of the  ground ProbLog pro gram:  for total  
choice  
{--.stress(p1),  --.stress(p2),  infiuences(p1,  p2), infiuences(p2,  p1)} 
the  model  assignsfalse  to  both  smokes(p1)  and  smokes(p2).  
The  conversion  algorithm  of [Mantadelis  and  Janssens,  2010]  generates:  
smokes(p1)  ~ aux1  v  stress(p2)  
smokes(p2)  ~ aux2  v  stress(p1)  
aux1  ~ smokes(p2)  1\  infiuences(p2,p1)  
aux2  ~ stress(p1)  1\  infiuences(p1,p2)  
Note  that  the  loop  in  the  original  ProbLog  program  between  smokes(p1)  and  
smokes(p2)  has  been  broken  by  using  stress(p1)  instead of smokes(p1)  in  
the  last formula.  
Lemma  15  (Correctness of the ProbLog Program Transformation [Fierens 
et al., 2015]). Let  P9  be  a  ground  ProbLog.  SAT(c/Jr)  = MOD(P9 )  where  
MOD(P9 )  is  the  set  ofmodels  ofinstances  ofP9 •  
The final Boolean formula cp  is built  from the one for the rules, c/Jr,  and that 
for the evidence c/Je  obtained as 
c/Je  =   1\  --.a  1\  1\  a  
~aEe 
aEe  
Then cp  =  c/Jr  1\  c/Je·  Weillustrate this on the Alarm example, which is acyclic. 
Example  81  (Alarm- Boolean formula- ProbLog2 [Fierens et al., 2015]). 
For  Example  79,  the  Boolean  formula  cjJ  is  
alarm  ~ burglary  v  earthquake  
calls(john)  ~ alarm  1\  hears_alarm(john)  
calls(john)  
where  the  three  rows  are  meant  to  be  conjoined.  
Then the weight function w  ( ·)  is defined as: for each probabilistic fact II :: f,  
f  is assigned weight II and --. f  is assigned weight 1 - II. All the other literals 
are assigned weight 1.  The weight of a world w  is given by the product of the 
weight of allliterals in w.  The following theorem establishes the relationship 
between the relevant ground program and the weighted formula. 
Theorem  17  (Model and weight equivalence [Fierens et al., 2015]). Let  P9  
be  the  relevant  ground  program  for  some  ProbLog  program  with  respect  to  

8.  7 ProbLog2  207 
q  and  e.  Let  MODe(P )  
9 be  those  models  in  MOD(P )  
9 that  are  consistent  
with  the  evidence  e.  Let  cp  denote  the  formula  and  w( ·) the  weight function  of  
the  weightedformula  derivedfrom  P •  
9 Then:  
• (model  equivalence)  SAT(cp)  =  MODe(P ),  
9 
• (weight  equivalence)  Vw  E  SAT(cp)  : w(w)  =  Pp (w),  i.e.,  the  weight 
9 
 
ofw  according  to  w(·)  is  equal  to  the  probability  ofw  according  to  P .  
9 
If V  is the set ofthe variables associated with ßp,  then WMCv(c/J)  = P(e).  
When V  is clear from the context, it is omitted. So 
P(e)  =  
2:: n w(l)  
wESAT(cp)  lEw  
The inference tasks of COND, MPE, and EVID can be solved using state-of­
the-art algorithms for WMC or weighted MAX-SAT. 
By knowledge compilation, ProbLog2 translates cp  to a smooth d-DNNF 
Boolean formula that allows WMC in polynomial time by in turn converting 
the d-DNNF into an arithmetic circuit. A Negation normal form (NNF) for­
mula is a rooted directed acyclic graph in which each leaf node is labeled with 
a literal and each internal node is labeled with a conjunction or disjunction. 
Smooth d-DNNF formulas also satisfy 
• Decomposability (D): for every conjunction node, no pair of children of 
the node have any variable in common. 
• Determinism (d): for every disjunction node, every pair of children rep­
resent formulas that are logically inconsistent with each other. 
• Smoothness: for every disjunction node, all children use exactly the 
same set of variables. 
Compilers for d-DNNF usually start from formulas in Conjunctive normal 
form (CNF). A CNF is a Boolean formula that takes the form of a conjunction 
of disjunctions of literals, i.e., a formula of the form: 
(Zn V··· V hm1 )  1\  · • • 1\  (lnl  V··· V lnmn)  
where each lij  is a literal. Examples of compilers from CNF to d-DNNF 
are c2d [Darwiche, 2004] and DSHARP [Muise et al., 2012]. The formula 
ofExample 81 is translated to the d-DNNF ofFigure 8.8. 
The conversion of a d-DNNF formula into an arithmetic circuit is done 
in two steps [Darwiche, 2009, Chapter 12]: first conjunctions are replaced 
by multiplications and disjunctions by summations, and then each leaf node 
labeled with a literall is replaced by a subtree consisting of a multiplication 

208 
Exact  lnference  
Figure  8.8 d-DNNF for the formula of Example 81. From [Fierens et al., 
2015]. 
node with two children, a leaf node with a Boolean indicator variable >.(Z)  
for the literal Z  and a leaf node with the weight of Z.  The circuit for the 
Alarm example is shown in Figure 8.9. This transformation is equivalent to 
transforming the WMC formula into 
AC(r/>)  = 
L:  TI w(z)>.(z)  =  
L:  TI w(z)  TI >.(z)  
wESAT(</J)  lEw  
wESAT(</J)  lEw  
lEw 
Given the arithmetic circuit, the WMC can be computed by evaluating the 
circuit bottom-up after having assigned the value 1 to all the indicator vari­
ables. The value computed for the root, f ,  is the probability of evidence and 
so solves EVID. Figure 8.9 shows the values that are computed for each node: 
the value for the root, 0.196, is the probability of evidence. 
With the arithmetic circuit, it is also possible to compute the probability 
of any evidence, provided that it extends the initial evidence. To compute 
P( e,  h  .. . Zn)  for any conjunction of literals h, ...  , Zn,  it is enough to set 
the indicator variables as >.(Zi) = 1, >.( -.zi)  = 0 for i  = 1, ... , n  (where 
-. -.a  = a)  and >.(Z) = 1 for the other literals Z,  and evaluate the circuit. In 
fact, the value f(h  .. . Zn)  of the root node will give: 
j(Z  ...  Z) =  "'Tiw(Z)TI{1,if{h .... Zn}<;;w
1 
n  
L;  
0, otherwise 
wESAT (</J)  lEw  
lEw  
L:  
TI w(Z)  = 
wESAT (</J),{fi ... ln }<;;w  lEw  
P(e, h  .. . Zn)  

8.  7 ProbLog2  209 
So in theory, one could build the circuit for formula r/Jr  only, since the proba­
bility of any set of evidence can then be computed. The formula for evidence 
however usually simplifies the compilation process and the resulting circuit. 
Given the definition of conditional probability, P(ql e)  =  P~(~) ), COND 
can solved by computing the probability of evidence q,  e  and dividing by 
P(e).  
Figure  8.9 Arithmetic circuit for the d-DNNF ofFigure 8.8. From [Fierens 
et al., 2015]. 
To compute CONDATOMS, we are given a set of ground atoms Q  and 
evidence e  and we want to compute P(ql e)  for all atoms q  E  Q.  One could 
solve COND for all atoms in Q.  However, consider the partial derivative 
8~~(4>) for an atom q:
q  
oAC(cp)  
2:: n 
w(l)  n >.(l) 
o>.q  
wESAT(<,b) ,qEw  lEw  
lEw,l # q  
If we assign 1 to all indicator variables we obtain -Jf  that is given by 
q  
of  
2:: n w(l)  = P(e,  q) 
o>.q  
wESAT (<,b),qEw  lEw  
So if we compute the partial derivatives of f  for all indicator variables >.  ( q),  
we get P(q,  e) for all atoms q.  We can solve this problern by traversing the 
circuit twice, once bottom-up and once top-down; see [Darwiche, 2009, Al­
gorithms 34 and 35]. The algorithm computes the value v(n)  of each node 

210 
Exact  Inference  
n  and the derivative d(n)  of the value of the root node r  with respect to n,  
i.e., d(n)  = ~~~~~· v(n)  is computed by traversing the circuit bottom-up and 
evaluating it at each node. d(n)  can be computed by observing that d(r)  = 1 
and, by the chain rule of calculus, for an arbitrary non-root node n  with p  
indicating its parents 
d(n)  = L.:  ov(r)  ov(p)  = l.:d(p)  ov(p).  
ov(p)  ov( n)  
ov(n) 
p  
p  
If parent p  is a multiplication node with n'  indicating its children 
ov(p)  - ov(n)  nn'#n  v(n')  = n v(n').  
ov(n)  -
ov(n)  
n'#n  
If parent p  is an addition node with n'  indicating its children 
ov(p)  
ov(n)  +  .l:n'#n  v(n') 
---
-1
ov(n)  -
ov(n)  
-
.  
So, if we indicate with +p  an addition parent of n  and with *Pa multiplication 
parent of n,  then 
d(n)  = l.: d(  +p)  +  l.: d(  *P)  n v(n').  
+p  
*P  
n 1#n  
Moreover ov(*P)  can be computed as ov(*P)  = v(*P)  if v(n)  =1=  0. If all
' ov(n)  
ov(n)  
v(n)  
indicator variables are set to 1, as required to compute f,  and if no parameter 
is 0, which can be assumed as otherwise the formula could be simplified, then 
v  (n)  =I=  0 for all nodes and 
d(n)  =  l.:d(+p)  +  l.:d(*p)v(*P)/v(n).  
+p  
*P  
This leads to Procedure CIRCP shown in Algorithm 7 that is a simplified 
version of [Darwiche, 2009, Algorithm 35] for the case v(n)  =I=  0 for all 
nodes. v(n)  may be 0 if f  is evaluated at additional evidence (see f(h  ...  Zn)  
above), in that case, [Darwiche, 2009, Algorithm 35] must be used that takes 
into account this case and is slightly more complex. 
ProbLog2 also offers compilation to BDDs. In this case, EVID and COND 
can be performed with Algorithm 6. In  fact, Algorithm 6 performs WMC 

8.7  ProbLog2  211 
Algorithm  7 Procedure CIRCP: Computation of value and derivatives of 
circuit nodes. 
I : procedure CIRCP(cir cuit) 
2: 
assign values to Jeaves 
3: 
for all non-leaf node n  with children c (visit children before parents) do 
4: 
if n  is an addition node then 
5: 
v(n)  <- I:c v(c)  
6: 
eise 
7: 
v(n)  <- Ticv(c)  
8: 
end if 
9: 
end for 
10: 
d(r)  <- 1, d(n)  =  0 for all non-root nodes 
11: 
for all non-root node n  (visit parents before children) do 
12: 
for all parents p  of n  do 
13: 
if p  is an addition parent then 
14: 
d(n)  =  d(n)  +  d(p)  
15: 
eise 
16: 
d(n)  <- d(n)  +  d(p)v(p)j v(n)  
17: 
end if 
18: 
end for 
19: 
end for 
20: end procedure 
over BDDs. This can be seen by observing that BDDs are d-DNNF that also 
satisfy the properties of decision and ordering [Darwiche, 2004]. A d-DNNF 
satisfies the property of decision  iff the root node is a decision node, i.e., a 
node labeled with 0, 1 or the subtree 
where a  is a variable and a  and ß  are decision nodes. a  is called the deci­
sion  variable.  A d-DNNF satisfying the property of decision also satisfies the 
property of ordering  if decision variables appear in the same order along any 
path from the root to any leaf. A d-DNNF satisfying decision and ordering is 
a BDD. d-DNNF satisfying decision and ordering may seem different from a 
BDD as we have seen it: however, if each decision node of the form above is 
replaced by 
Jb  

212 
Exact  Inference  
we obtain a BDD. 
The d-DNNF ofFigure 8.8, for example, does not satisfy decision and or­
dering. The same Boolean formula, however, can be encoded with the BDD of 
Figure 8.10 that can be converted to a d-DNNF using the above equivalence. 
Algorithm 6 for computing the probability of a BDD is equivalent to the 
evaluation of the arithmetic circuit that can be obtained from the BDD by 
seeing it as a d-DNNF. 
More recently, ProbLog2 has also included the possibility of compiling 
the 
Boolean 
function 
to 
SDDs 
[Vlasselaer et al., 2014; Dries et al., 2015]. An SDD for the formula of 
Example 81 is shown in Figure 8.11. 
An SDD [Darwiche, 2011] contains two types of nodes: decision  nodes,  
represented as circles, and elements,  represented as paired boxes. Elements 
are the children of decision nodes and each box in an element can contain a 
pointer to a decision node or a terminal  node,  either a literal or the constants 0 
or 1. In an element (p,  s ), p  is called a prime  and s  is called a sub.  Adecision 
node with children (p1, sl), ... , (Pn,  sn)  represents the function (Pl 1\  s1) v 
\  
\ 
\ 
\ 
\  
\  
\  
\  
\  
\ 
I   
I   
\   
\   
\  
\  
\  
\  
\  
\  
\  
I  
\I 
\I 
I  
'tj   
Figure  8.10  BDD for the formula of Example 81. 

8.  7 ProbLog2  213 
Figure  8.11  SDD for the formula of Bxample 81. 
... v (Pn  1\  Sn).  Prim es Pl,  ... ,Pn  must form a partition: Pi  #- 0, Pi  1\  Pj  =  0 
for i  #- j,  and Pl  v ... v Pn  =  1. 
A vtree  is a full binary tree whose leaves are in one-to-one correspon­
dence with the formula variables. Bach SDD is normalized for some vtree. 
Figure 8.12 shows the vtree for which the SDD ofFigure 8.11 is normalized. 
Bach SDD node is normalized for some vtree node. The root node of the 
SDD is normalized for the root vtree node. Terminal nodes are normalized 
for leaf vtree nodes. If a decision node is normalized for a vtree node v,  then 
its primes are normalized for the left child of v,  and its subs are normalized 
for the right child of v.  As a result, primes and subs of same decision node 
do not share variables. The SDD of a Boolean formula is unique once a vtree 
is fixed. In Figure 8.11, decision nodes are labeled with the vtree nodes they 
are normalized for. 
,/3~5  
0/J 
J~  
-. _,  -~/\ 
burglary  
earthquake  
Figure  8.12  vtree for which the SDD of Figure 8.11 is normalized. 

214 
Exact  Inference  
SDDs are special cases of d-DNNF: if one replaces circle-nodes with 
or-nodes, and paired-boxes with and-nodes, one obtains a d-DNNF. SDDs 
satisfy two additional properties with respect to d-DNNF: structured decom­
posability and strong determinism. 
Todefine structured decomposability, consider a d-DNNF 5  and assume, 
without loss of generality, that all conjunctions are binary. 5  respects  a vtree V  
if for every conjunction a  1\  ß  in 5,  there is a node v  in V  such that var s  (a)  ~ 
vars(vz)  and vars(ß)  ~ vars(vr)  where vz  and Vr  are the left and right 
child of v  and vars (a)  and vars (v)  are the sets of variables appearing in the 
subdiagram rooted at a  and the sub-vtree rooted at v  respectively. 5  enjoys 
structured  decomposability  if it satisfies some vtree. 
Strong determinism requires not only that the children of or nodes are 
pairwise inconsistent butthat they form a strongly deterministic decomposi­
tion. An (X,  Y)-decomposition  of a function f(X,  Y)  over non-overlapping 
variables X and Y is a set {(p1, s1), ... , (pn,  sn)} suchthat 
f  =  Pl(X) 1\  s1(Y) V  •..  V  Pn(X)  1\  sn(Y)  
If Pi  1\ Pj  =  0 for i  -=1- j,  the decomposition is said tobe strongly deterministic.  
A d-DNNF is strongly  deterministic  if each or node is a strongly deterministic 
decomposition. 
BDDs are a special case of SDDs where decompositions are all Shan­
non: formula f  is decomposed into {(X,  fx),  (-.X,  f~x) }.  SDDs generalize 
BDDs by considering non-binary decisions based on the value of primes and 
by considering vtrees instead of linear variable orders. 
In  ProbLog2, the user can choose whether to use d-DNNFs, BDDs or 
SDDs. The choice of the compilation language depends on the tradeoff be­
tween succinctness and tractability. Succinctness is defined by the size of a 
know ledge base once it is compiled. Tractability is determined by the set of 
operations that can be performed in polynomial time. The more tractable a 
representation is, the less succinct it is. 
A language is at  least  as  succinct  as  another if, for every sentence in 
the second language, there exists an equivalent sentence in the first with a 
polynomially smaller size. The succinctness order for BDD, SDD, and d­
DNNFis 
d-DNNF <  SDD :S;  BDD 
meaning that d-DNNF is strictly more succinct than SDDs and that SDDs 
are at least as succinct as BDDs (whether SDD <  BDD is an open prob­
lern). Since SDD :t  d-DNNF, there exist formulas whose smallest SDD 
representation is exponentially larger that its d-DNNF representation. 

8.7  ProbLog2  215 
Table  8.1  Tractability of operations. ? means ''unknown", ./ means 
"tractable" and o means "not tractable unless P=NP". Operations are meant 
over a bounded number of operands and BDDs operands should have the 
samevariable order and SDDs the same vtree. From [Vlasselaer et al., 2014] 
Langnage 
Negation 
Conjunction 
Disjunction 
Model Counting 
d-DNNF 
? 
0 
0 
.(  
SDD 
.(  
.(  
.(  
.(  
BDD 
.(  
.(  
.(  
.(  
The operations for which we consider tractability are those that are useful 
for probabilistic inference, namely, negation, conjunction, disjunction, and 
model counting. The situation for the languages we consider is summarized 
in Table 8.1. 
SDDs and BDDs support tractable Boolean combination operators. This 
means that they can be built bottom-up starting from the cycle-free ground 
program, similarly to what is done by ProbLog1 or PITA. d-DNNF compilers 
instead require the formula tobe in CNF. Converting the Clark's completion 
of the rules into CNF introduces a new auxiliary variable for each rule which 
has a body with more than one literal. This adds cost to the compilation 
process. 
Another advantage of BDDs and SDDs is that they support minimiza­
tion: their size can be reduced by modifying the variable order or vtree. 
Minimization of d-DNNF is not supported, so circuits may be larger than 
necessary. 
The disadvantage ofBDDs with respect to d-DNNF and SDDs isthat they 
have worse size upper bounds, while the upper bound for d-DNNF and SDD 
is the same [Razgon, 2014]. 
Experimental comparisons confirm this and show that d-DNNF Ieads to 
faster inference than BDDs [Fierens et al., 2015] and that SDDs Iead to faster 
inference than d-DNNF [Vlasselaer et al., 2014]. 
The ProbLog2 system can be used in three ways. It  can be used online 
at https://dtai.cs.kuleuven.be/problog/: the user can enter and solve ProbLog 
problems with just a web browser. 
It can be used from the command line as a Python program. In  this case, 
the user has full control over the system settings and can use all machine 
resources, in contrast with the online version that has resource Iimits. In  this 
version, ProbLog programs may use external functions written in Python, 
such as those offered by the wide Python ecosystem. 

216 
Exact  Inference  
ProbLog2 can also be used as a library that can be called from Python for 
building and querying probabilistic ProbLog models. 
8.8  Tp  Compilation  
Previous inference algorithms work backward, by reasoning from the query 
toward the probabilistic choices. Tp  compilation [Vlasselaer et al., 2015, 
2016] is an approach for performing probabilistic inference in ProbLog using 
forward reasoning. In particular, it  is based on the operator Tcp,  a gen­
eralization of the Tp  operator of logic programming that operates on pa­
rameterized interpretations. We have encountered a form of parameterized 
interpretations in Section 3.2 where each atom is associated with a set of 
composite choices. The parameterized interpretations of [Vlasselaer et al., 
2016] associate ground atoms with Boolean formulas built on variables that 
represent probabilistic ground facts. 
Definition  40  (Parameterized interpretation [Vlasselaer et al., 2015, 2016]). 
A  parameterized interpretation  'I of a  ground probabilistic  logic  program  P  
with probabilistic facts F  and atoms  ßp is a set  of tuples  (a,  Aa)  with  a  E  ßp  
and Aa a propositional formula over F  expressing in which interpretations a  
is  true.  
The Tcp  operator takes as input a parameterized interpretation and returns 
another parameterized interpretation obtained by applying the rules once. 
Definition  41  (Tcp  operator [Vlasselaer et al., 2015, 2016]). Let  P  be  a  
ground  probabilistic  logic  program  with  probabilistic  facts  F,  rules  R  and  
atoms  ßp.  Let  'I  be  a  parameterized  interpretation  with  pairs  (a,  A.a).  Then,  
the  Tcp  operator  is  Tcp('I)  = { (a,  A.a)  Ia  E  ßp}  where  
a  
ifa  E  F   
A.' a-- { 
Va.-bl,... ,bn,~cl, ... ,~cmER  
ifa  E  ßp\F 
(Abl  1\  . . . 1\  Abn  1\  -.., Acl 1\  . . . 1\  -.., Acm  ) 
The ordinal powers of Tcp  start from {(a,O)Ia  E  ßp}.  The concept of 
fixpoint must be defined in a semantic rather than syntactic way. 
Definition  42  (Fixpoint of Tcp  [Vlasselaer et al., 2015, 2016]). A  parame­
terized  interpretation  'I  is  afixpoint  ofthe  Tcp  operatorifand  only  iffor  all  
a  E  ßp,  Aa  =  A.~, where  Aa  and  A.~ are  theformulasfora  in  'I  andTcp('I),  
respectively.  

8.8  Tp  Compilation  217 
Vlasselaer et al. [2016] show that, for definite programs, Tcp  has a least 
fixpoint lfp(Tcp) = { ( a,  A~) Ia  E  ßp}  where the A~s exactly describe the 
possible worlds where a  is true and can be used to compute the probability 
for each atom by WMC, i.e., P(a)  = WMC(A~). 
The probability of an atomic query q  given evidence e  can be computed as 
WMC(Aq  A  Ae)  
P(qle)  = 
WMC(Ae)  
where Ae  = /\eiEe Ae;.  
The approach can also be applied to stratified normallogic programs by 
iterating the Tcp  operator stratum by stratum: the fixpoint ofTep  is computed 
by considering the rules for each stratum in turn. 
So, to perform exact inference on a ProbLog program P,  the Tcp  op­
erator should be iterated stratum by stratum until the fixpoint is reached in 
each stratum. The parameterized interpretation that is obtained after the last 
stratum can then be used to perform COND and EVID. 
The algorithm of [Vlasselaer et al., 2016] represents the formulas in the 
interpretations by means of SDDs. So the Boolean formulas Aa  in the defini­
tion ofTcp  are replaced by SDD structures Aa.  Since negation, conjunction, 
and disjunction are efficient for SDDs, then so is the application ofTcp.  
Moreover, the Tcp  operator can be applied in a granular way one atom 
at a time, which is useful for selecting more effective evaluation strategies in 
approximate inference, see Section 10.7. The operator Tcp(a,'I)  considers 
only the rules with a  in the head and updates only the formula Aa.  So an 
application ofTcp  is modified in 
1. select an atom a  E  ßp,  
2. compute Tcp(a,'I).  
Vlasselaer et al. [2016] show that if each atom is selected frequently enough 
in step 1, then the same fixpoint lfp(Tcp) is reached as for the naive algorithm 
that considers all atoms at the same time, provided that the operator is still 
applied stratum by stratum in normallogic programs. 
Tp  compilation can also be used for performing inference in the case 
of updates of the program, where (ground) facts and rules can be added or 
removed. For definite programs, past compilation results can be reused to 
compute the new fixpoint. Moreover, Tp  compilation can be used for dynamic 
models, where each atom has an argument that represents the time at which it 
is true. In this case, the rules express the dependency of an atom at a time step 
from atoms at the same or previous time step. An extension ofTp  compilation 

218 
Exact  Inference  
can then be used for filtering, i.e., computing the probability of a query at a 
time t  given evidence up to time t.  
Experiments in [Vlasselaer et al., 2016] show that Tp  compilation com­
pares favorably with ProbLog2 with both d-DNNF and SDD. 
8.9  MPE  and  MAP  
In this section we discuss algorithms for performing the MPE and MAP tasks. 
8.9.1  MAP  and  MPE  in  Problog  
Shterionov et al. [2015] describe how MPE inference is performed in ProbLog. 
When the ProbLog program does not contain annotated disjunctions, the 
ProbLog2 algorithm for COND is initially followed: the relevant ground pro­
gram is identified, it is translated into a CNF and the CNF is compiled to 
a d-DNNF. At this point the algorithms for COND and MPE differ in the 
way the formula is transformed into an arithmetic circuit: for MPE, the v 
nodes are trasnformed into max nodes instead of +  nodes. The evaluation 
of the circuit gives the MPE probability of the query, see [Darwiche, 2004, 
Section 12.3.2]. To produce the explanation as well, the algorithms that visits 
the circuit has to keep track of which children of max nodes are selected. 
When the ProbLog program contains annotated disjunctions, the transla­
tion of Section 2.4 using ni  - 1 Boolean variables for a clause with ni  heads 
does not work, as configurations of the variables exist that do not correspond 
to any value for the rule random variable. The problern is that the encoding 
is redundant and a value for the random variable associated with a rule may 
be encoded by multiple tuples of values for the Boolean variables besides the 
intended one. One of those unintended encodings may get chosen because it 
has a higher probability but this does not reflect on the correct choice of the 
multivalued variable. Shterionov et al. [2015] propose a different encoding, 
where ni  Boolean variables Xijk  for a clause Ci(}j  =  head  ~ body  with 
ni  heads are used and constraints are imposed, namely that one and only one 
Xijk  must be true. This is achieved by building the constraint formula 
(V 
Xijk  ~ body)  1\  nAl  A ( 
-.xijk  V -.xijm)  
k=l  
k=l  m=k+l  
for each multi-valued variable Xij.  translating it into a CNF and conjoining 
it with the CNF built for the query. 

8.9  MPE  and  MAP  219 
For MAP, ProbLog uses DTProbLog, an algorithm for maximizing an 
utility function by making decisions, see Section 11.2. In DTProbLog utility 
values are assigned to some ground literals, some ground atoms are proba­
bilistic and some are decision. The aim is to find an assignment to decision 
variables that maximizes utility, given by the sum of the utility for the liter­
als that are made true by the decisions. DTProbLog uses Algebraic decision 
diagrams (ADDs) as a target compilation language. ADDs are BDDs where 
leaves are associated with real numbers instead of Boolean values. ADDs 
built by DTProbLog contain only decision variables, probabilistic variables 
are compiled away. 
Shterionov et al. [2015] also illustrate the difference between VIT and 
MPE. 
Example  82 (VIT vs MPE [Shterionov et al., 2015]). Consider  the  program:  
0 . 4 :: red .  
0 . 9 :: green .  
0 . 5 : : blue .  
0 . 6 : : yellow .  
win  : - red,  green .  
win  : - blue,  yellow .  
The  query  win  has  two  explanations  (or  proofs):  thefirst  usesfacts  red  and  
green  and  has  probability  0.4 x  0.9 = 0.36, the  second  uses  facts  bl ue  
and  yellow  and  has  probability  0.5 x 0.6 = 0.3. The  Viterbi  proof  is  the  
first.  The  MPE  statefor  the  query  win,  however,  is  
[ \+red,  green ,  blue,  yellow]  
with  probability  0.6 x 0.9 x 0.5 x 0.6 = 0.162. This  state  doesn  'textend  the  
Viterbi  proof  
8.9.2  MAP  and  MPE  in  PITA  
Bellodi et al. [2020] present an extension of PITA for performing MPE and 
MAP. They consider the following problem. 
Definition  43  (PITA MAP Problem). Given  an  LPAD  P,  a  conjunction  of  
ground  atoms e ,  the  evidence, and  a  set  of random  variables  X  ( query  ran­
dom  variables),  associated  to  some  ground  rules  oJP,  the  MAP  problern  is  
to  find  an  assignment  x of values  to  X  such  that  P  (x I e) is  maximized,  i. e.,  
solve  
argmaxP(xle) 
X  

220 Exact  lnference  
The  MPE  problern  is  a  MAP  problern  where  X  includes  all  the  random  vari­
ables  associated  with  all  ground  clauses  ofP.  
In PITA we indicate the query random variables in the program by prepending 
the functor map_ query to the rules. Thus an MPE problern is one where 
map_ query is in front of every probabilistic clause. 
Example  83 (Bag of balls - MPE). Consider  a  game  with  a  single  bag  
containing  red,  green  and  blue  balls,  where  the  player  randomly  decides  to  
pick  a  ball  or  not.  This  can  be  encoded  with  
map_query  red (b1 )  :0.6;  green (b1 )  :0.3;  b1ue (b1 )  :0.1  
pick  (b1 ).  
map_query pick (b1 ) : 0 . 6;  no_ pick (b1 ) : 0 . 4 .  
ev : -
\+ blue (b1 ) .  
evidence  ev has  the  MPE  assignment  x:  
[ru1e (O,  red (b1 ),  [red (b1 )  : 0.~ green (b1 ) :0.3,  b1ue (b1 ) : 0.1 ],   
pick  (b1 )),   
ru1e (1,  pick (b1 ),  [pick (b1 ) : 0 . 6,  no_pick (b1 ) : 0 . 4 ] ,  true ),   
where predicate  ru 1 e I  4  specifies  clause  number  ( zero-based),  selected  head,  
clause head, clause body, in that order.  Forthis assignment,  P(xlev)  = 0.36, 
meaning  that  the  mostprobable  explanation  x has  a  probability  of 0.36.  
Example  84  (Bag of balls - MAP). Given  the  program  
red (b1 ) : 0 . 6;  green (b1 ) : 0 . 3;  b1ue (b1 ) :0.1  :- pick (b1 ).  
map_query  pick (b1 )  :0.6;  no_pick (b1 )  : 0.4 .  
ev : -
\+ b1ue (b1 ).  
Evidence  ev has  the  MAP  assignment:  
[ru1e (1,  pick (b1 ),  [pick (b1 ) :0.6,  no_pick (b1 ) :0.4 ] ,  true )].  
Forthis  assignment,  P(xlev)  =  0.54. 
Example  85 (Disease - Difference between MPE and MAP). Consider  the  
following  LPAD:  
map_query  disease : 0.05 .  
map_query  malfunction : 0 . 05 .  
positive  : - ma1function .  
map_query  positive : 0.999  : - disease .  
map_query  positive : 0 . 0001  :-
\+ (ma1function ),  \+ (disease ).  

8.9  MPE  and  MAP  221 
The  LPAD  models  the  diagnosis  of  a  disease  by  means  of  a  lab  test.  The  
disease  probability  is  0.05, and,  in  case  of  disease,  the  test  resuZt  will  be  
positive  with  probability  0.999. However,  there  is  a  5%  chance  of an  equip­
ment  malfunction;  in  this  case,  the  test  will  always  be  positive.  Additionally,  
even  in  absence  of  disease  or  malfunction,  the  test  resuZt  will  be  positive  
with  probability  0.0001. The  LPAD  has  16  worlds,  each  corresponding  to  
selecting,  or  not,  the  head  of each  annotated  disjunctive  clause.  
Let  us  suppose  for  the  testresuZt  tobe  positive:  is  the  patient  ill?  Given  
evidence  ev  =  positive,  the  MPE  assignment  is  
[ rule ( O,  disease ,  
[ disease : 0 . 05 ,  
"  
: 0 . 95 ] ,  true ),  
rule ( l ,  
" ,  
[malfunction : 0 . 05 ,  
"  
: 0 . 95 ] ,  true ),  
rule (2 ,  positive,  
[positive : 0.999,  
"  
: 0.001 ] ,  disease ),  
rule ( 3 ,  
" ,  
[positive : O. OOOl ,  
"  
: 0 . 9999 ] ,  (\ +malfunction ,  
\+disease )) J  
where  1 1 indicates  the  null  head.  The  mostprobable  world  is  the  one  where  
an  actual  disease  caused  the  positive  result,  and  its  probability  is  P(xl e)  = 
0.04702. 
Likewise,  if we  perform  a  MAP  inference  taking  only  the  choice  of the  first  
clause  as  query  variable,  the  resuZt  is  
[ rule ( O,  disease ,  
[ disease : 0.05 ,  
"  
:  0 . 95 ] ,  true ) ]  
so  the  patient  is  ill.  However,  if we  take  the  choices  for  the  first  two  clauses  as  
query  variables,  i.e.,  ifwe  lookfor  the  most  likely  combination  of disease  
and  mal  funct  i  on  given  positive,  the  MAP  task  produces  
[ rule ( O,  
" ,  
[ disease : 0.05 ,  
"  
:  0 . 95 ] ,  true ) ]   
rule ( l ,  malfunction ,  
[malfunction : 0 . 05 ,  
"  
:  0.95 ] ,  true ),   
meaning  that  the  patient  is  not  ill  and  the  positive  test  is  explained  by  an  
equipment  malfunction.  This  examples  shows  that  the  value  assigned  to  a  
query  variable  in  a  MAP  task  can  be  affected  by  the  presence  of other  vari­
ables  in  the  set  of query  variables;  in  particular,  MPE  and  MAP  inference  
over  X  may  assign  different  values  to  the  same  variable  given  the  same  
evidence.  
PITA solves the MPE task [Bellodi et al., 2020] using BDDs. In particu­
lar, it exploits the CUDD library. In  CUDD, BDD nodes are described by 
two fields: pointer,  a pointer to the node, and camp,  a Boolean indicating 
whether the node is complemented. In fact three types of edges are admitted: 

222 
Exact Inference  
xo_o 
Xl_O 
Xl_l 
Figure  8.13  CUDD BDD for the query ev of Example 83. Labels Xij 
indicates the jth Boolean variable of ith ground clause and the label of each 
node is a part of its memory address. 
an edge to a 1-child, an edge to a 0-child and a complemented edge to a 0­
child, meaning that the function encoded by the child must be complemented. 
Moreover, the root node can be complemented. For these types of BDDs, only 
the 1 leaf is needed. For example, the CUDD BDD for the EVID query ev of 
Example 83 is shown in Figure 8.13. To compute the probability ofthe query 
(solving the EVID task), PITA uses Algorithm 8 which is the CUDD version 
of Algorithm 6. 
PITA encodes random variables for query annotated disjunctive clauses 
as in [Shterionov et al., 2015]: ni  Boolean variables Xijk  for a clause with ni  
heads are used and the following constraints are imposed 
(V 
Xijk)  1\  nAl  A ( 
--.xijk  V --.xijm) 
(8.4) 
k=l  
k=l  m=k+l  
for each multi-valued variable Xij·  The constraints are translated into a BDD 
and conjoined it with the BDD built for the query with PITA. The BDDs built 
for examples 83 and 84 with this new encoding are shown in figures 8.14 and 
8.15. 
PITA solves MPE using the dynamic programming algorithm proposed 
by [Darwiche, 2004, Section 12.3.2] for computing MPE over d-DNNFs, 
which generalize BDDs. In  fact, BDDs can be seen as d-DNNFs as shown 
in Section 8.7: a BDD node (Figure 8.16(a)) for variable a  with children a  

8.9  MPE  and  MAP  223 
Algorithm  8 Function CUDDPROB: computation of the probability of a 
CUDDBDD. 
1: function CUDDPROB(node) 
2: 
if node  is a terminal then 
3: 
return 1 
4: 
eise 
5: 
if Table(node.pointer)  =F- null  then 
6: 
return Table(node.pointer)  
7: 
eise 
8: 
pO  +-CUDDPROB(child0 (node)) 
9: 
p1 +-CUDDPROB(child1(node)) 
10: 
if childo(node).comp  then 
11: 
pO  +-- (1- pO)  
12: 
endif 
13: 
Let 1r be the probability of being true of var (node)  
14: 
Res+-- p1 · 1r +  pO  · (1- 1r) 
15:  
Add  node.pointer  ~ Res  to Table  
16: 
return Res  
17: 
endif 
18: 
end if 
19: end function 
and ß is translated into the d-DNNF portion shown in Figure 8.16(b), where 
o/ and ß'  are the translations of the BDDs a  and ß  respectively. The algo­
rithm proposed by [Darwiche, 2004] computes the probability of the MPE 
by replacing A-nodes with product nodes and v-nodes with max-nodes: the 
result is an arithmetic circuit (Figure 8.16( c)) that, when evaluated bottom­
up, gives the probability of the MPE and can be used to identify the MPE 
assignment. The equivalent algorithm operating on BDDs- Function MAP­
INT in Algorithm 9 - modifies Algorithm 8 and retums both a probability and 
a set of assignments to random variables. At each node, instead of computing 
Res  ~ p 1·n:  + pO  · (1 - n:)  as in Algorithm 8 line 14, it retums the assignment 
of the children having the maximum probability. This is computed in lines 
41-45 in Algorithm 9. In MPE there are no non-query variables, so the test in 
line 26 succeeds only for the BDD leaf. MAPINT in practice computes the 
probability of paths from the root to the lleaf and retums the probability and 
the assignment corresponding to the most probable path. 
Function MAP in Algorithm 9 takes as input the BDD representing the 
evidence root,  builds the BDD representing Equation (8.4) for all i, j,  con­
joins it with root  and calls MAPINT on the result. 

224 
Exact  Inference  
Algorithm  9 Function MAP: computation of the maximum a posterior state 
of a set of query variables and of its probability 
1:  function MAP(raat) 
2: 
for all query variables var  do 
3: 
AtLeastOne  +--- BDD_ZERO 
4: 
AtMastOne  +--- BDD_ONE 
5: 
for i  +--- 1 to values( var)  - 1 do 
6: 
7: 
AtLeastOne  +--- BDD_OR(AtLeastOne,bVar(var,i))  
for j  +--- i  +  1 to values( var)  do 
8: 
Bath  +--- BDD_AND(bVar( var,  i), bVar( var,  j)) 
9: 
NatBath  +--- BDD_NoT(Bath)  
10: 
AtMastOne  +--- BDD_AND(AtMastOne,  NatBath)  
11:  
end for 
12:  
AtLeastOne  +--- BDD_OR(AtLeastOne,  bVar(var,  values(var)))  
13: 
endfor 
14: 
canst  +--- BDD_AND(AtLeastOne,  AtMastOne)  
15: 
raat  +--- BDD_AND(raat, canst)  
16: 
end for 
17:  
Reorder BDD raat  so that variables associated to query variables come first in the order 
18: 
Let raat'  be the new root 
19: 
TableMAP  +--- 0  
20: 
Table  +--- 0  
21:  
(Prob,  MAP)  +-MAPINT(raat', false) C>  M  AP:  assignment to Boolean random variables 
22: 
return (Prob,  MAP)  
23:  end function 
24:  function MAPINT(nade, camp)  
C>  Intemal function implementing the dynarnic programming 
algorithm 
25: 
camp  +--- nade.comp  ffi  camp   
C>  ffi  is exor 
26: 
if var( nade)  is not associated to a query var then 
27: 
p  +-CVDDPROB(nade)   
C>  Algorithm 8 
28: 
if camp  then 
29: 
return (1 - p,  []) 
30: 
eise 
31: 
return (p,  [])  
32: 
endif 
33: 
eise 
34: 
if TableMAP(nade.painter)  -:f.  null  then 
35: 
return TableMAP(nade.painter)  
36: 
eise 
37: 
(pü, MAPO)  +-MAPINT(childo(nade),  camp)  
38: 
(p1, MAP1)  +-MAPINT(child1 (nade),  camp)  
39: 
Let 1r be the probability of being true of the variable associated to nade  
40: 
p1 +--- p1 . 7r 
41: 
ifp1 >  pO  then 
42: 
Res+--- (p1, [var(nade)  =  1IMAP1]) 
43: 
eise 
44: 
Res  +--- (pO,  MAPO)  
45: 
end if 
46: 
Add (nade.painter)-->  Res  to TableMAP  
47: 
return Res  
48: 
endif 
49: 
end if  
50: end function 

8.9  MPE  and  MAP  225 
B   
XD_O  
XO_l  
Xl_O  
Xl_l  
Xl_2  
Figure  8.14  BDD for the MPE problern ofExample 83. The variables XO_k 
and Xl_k are associated with the second and first clause respectively. 
xo_o  
XO_l 
Xl_O  
Xl_l  
Figure  8.15  BDD for the MAP problern ofExample 84. The variables XO_k 
and Xl_k are associated with the second and first clause respectively. 
In a MAP task, i.e., when we have non-query variables, function MAP­
INT cannot be used because when a node for a non-query variable is reached, 
it must be summed out instead of maximized out, and maximization and 
summation operations are not commutative. However, ifits children are nodes 
for query variables, which of the two assignments for the children should be 
propagated towards the root? In case the non-query variables appear last in 
the ordering, when MAPINT reaches a node for a non-query variable, it can 

226 
Exact  lnference  
ß  
fA  
0 
'B ia  
(a) Node for variable (b) d-DNNF equivalent por-
(c) Arithmetic circuit. 
a  in a BDD. 
tion. 
Figure  8.16  Translation from BDDs to d-DNNF. 
sum out all non-query variables using function CUDDPROB from Algorithm 
8. This assigns a probability to the node that can be used by MAPINT to 
identify the most probable path from the root. So PITA solves MAP by re­
ordering variables in the BDD (line 17 in Algorithm 9), putting first the query 
variables. 
With CUDD we can either create BDDs from scratch with a given vari­
able order or modify BDDs according to a new variable order. Changing 
the position of a variable is made by successive swapping of adjacent vari­
ables [Somenzi, 2001]: the swap can be performed in a time proportional to 
the number of nodes associated with the two swapped variables. Changing the 
order of two adjacent variables does not affect the other Ievels of the BDD, 
so changes can be applied directly to the current BDD saving memory. To 
further reduce the cost of the swapping, the CUDD library keeps in memory 
an interaction matrix specifying which variables directly interact with others. 
This matrix is updated only when a new variable is inserted into the BDD, 
is symmetric and can be stored by using a single bit for each pair, making 
it  very small. Moreover, the cost of building it is negligible compared to 
the cost of manipulating the BDD without checking it. Jiang et al. [2017]. 
empirically demonstrated that changing the order of variables by means of 
sequential swapping is usually much more time efficient than rebuilding the 
BDD following a fixed variable order. 
8.10  Modeling  Assumptions  in  PITA  
Let us recall here PRISM modeling assumptions: 
1.   The probability of a conjunction (A, B)  is computed as the product of 
the probabilities of A  and B  (independent-and  assumption).  

8.10  Modeling  Assumptions  in  PITA  227 
2. The probability of a disjunction (A;  B)  is computed as the sum of the 
probabilities of A  and B  (exclusive-or  assumption).  
These assumptions can be stated more formally by referring to explanations. 
Given an explanation "'· let RV("')  = {CiOji(Ci, Oj,  k)  E  "'}·  Given a set 
of explanations K,  let RV(K)  = U~~;EK RV("').  Two sets of explanations, 
K1 and K2, are independent  if RV(K1)  n  RV(K2)  = 0  and exclusive  if, 
'1/"'1 E K1, "'2 E K2, "'1 and "'2 are incompatible. 
The independent-and assumption means that, when deriving a covering 
set of explanations for a goal, the covering sets of explanations Ki  and Kj  
for two ground subgoals in the body of a clause are independent. 
The exclusive-or assumption means that, when deriving a covering set 
of explanations for a goal, two sets of explanations Ki  and Kj  obtained for 
a ground subgoal h  from two different ground clauses are exclusive. This 
implies that the atom h  is derived using clauses that have mutually exclusive 
bodies, i.e., that their bodies are not true at the same time in 
any world. 
The systems PRISM [Sato and Kameya, 1997] and PITA(IND, EXC) 
[Riguzzi and Swift, 2011] exploit these assumptions to speed up the com­
putation. In fact, these assumptions make the computation of probabilities 
"truth-functional" [Gerla, 2001] (the probability of conjunction/disjunction 
of two propositions depends only on the probabilities of those propositions ), 
while, in the general case, this is false. PITA(IND, EXC) differs from PITA 
in the definition ofthe one/1,  zero/1,  not/2,  and/3  and or/3  predicates that 
now work on probabilities P  rather than on BDDs. Their definitions are 
zero(O).  
one(1).  
not(A,  B)  ~Bis 1- A.  
and(A,  B,  C) ~Cis A  *  B.  
or(A,  B,  C)  ~Cis A  +  B.  
Moreover, in the PITA transformation P  IT A( Cr,  i) for a clause Cr  (Equa­
tion (8.3)), the conjunction 
get_var _n(r,  S,  [II1, ... ,IIn],  V  ar),  equality(V ar,  i, D D)  
is replaced by 
nth(i,  [II1, ... ,IIn],  DD)  
where nth(I,  List,  Element)  is a predicate that retums the Jth element of 
List  in Element.  
Instead of the exclusive-or  assumption,  a program may satisfy the follow­
ing assumption: 

228 
Exact  Inference  
3.  The probability of a disjunction (A;  B)  is computed as if A  and B  were 
independent (independent-or  assumption).  
This means that, when deriving a covering set of explanations for a goal, two 
sets of explanations Ki  and K1  obtained for a ground subgoal h  from two 
different ground clauses are independent. If A  and B  are independent, the 
probability of their disjunction is 
P(A  v  B)  =   P(A)  +  P(B)  - P(A  1\  B)  =  
P(A)  +  P(B)  - P(A)P(B)  
by the laws of probability theory. PITA(IND, EXC) can be used for programs 
respecting this assumption by changing the or /3 predicate in this way 
or(A,  B,  P)  +---PisA+  B-A*  B.  
PITA(IND,IND) is the resulting system. 
The exclusiveness assumption for conjunctions of literals means that the 
conjunction is true in 0 worlds and thus has probability 0, so it does not make 
sense to consider a PITA(EXC,_) system. 
The following program 
path(Node,  Node).   
path(Source, Target):  0.3 +--- edge(Source,  Node),   
path(Node, Target).   
edge(O,  1) : 0.3.  
satisfies the independent-and and independent-or assumptions depending on 
the structure of the graph. For example, the graphs in Figures 8.17(a) and 
8.17(b) respect these assumptions for the query path(O,  1).  Similar graphs 
of increasing sizes can be obtained [Bragaglia and Riguzzi, 2011]. We call 
the firstgraphtype a "lanes" graph and the second a "branches" graph. The 
graphs of the type of Figure 8.17 ( c ), called "parachutes" graphs, instead, 
satisfy only the independent-and assumption for the query path(O,  1).  
All three types of graphs respect the independent-and assumption be­
cause, when deriving the goal path(O,  1),  paths are built incrementally start­
ing from node 0 and adding one edge at a time with the second clause of the 
definition of path/2.  Since the edge that is added does not appear in the rest 
of the path, the assumption is respected. 
Lanes and branches graphs respect the independent-or assumption be­
cause, when deriving the goal path(O,  1),  ground instantiations of the second 
path clause have path( i, 1)  in the head and originate atomic choices of the 
form (C2, {Sourceji,  Target/!,  Node/j},  1). 

8.10  Modeling  Assumptions  in  PITA  229 
(c) Parachutes.
(a) Lanes. 
(b) Branches. 
Figure  8.17  Examples of graphs satisfying some of the assumptions. From 
[Bragaglia and Riguzzi, 2011]. 
Explanations for path( i, 1) also contain atomic choices ( eij,  0, 1) for 
every fact edge( i, j)  : 0.3. in the path. Each explanation corresponds to a 
path. In the lanes graph, each node except 0 and 1 lies on a single path, so 
the explanations for path( i, 1) do not share random variables. In the branches 
graphs, each explanation for path( i, 1) depends on a disjoint set of edges. In  
the parachutes graph, instead this is not true: for example, the path from 2 to 
1 shares the edge from 2 to 1 with the path 3, 2, 1. 
Another program satisfying the independent-and and independent-or as­
sumptions is the following 
sametitle(A,  B)  : 0.3 ~  
haswordtitle(A,  word_10),   
haswordtitle(B,  word_10).   
sametitle(A,  B)  : 0.3 ~  
haswordtitle(A,  word_1321),  
haswordtitle(B,  word_1321).  
which computes the probability that the titles of two different citations are 
the same on the basis of the words that are present in the titles. The dots 
stand for clauses differing from the ones above only for the words considered. 
The haswordtitle/2  predicate is defined by a set of certain facts. This is 
part of a program to disambiguate citations in the Cora database [Singla and 
Domingos, 2005]. The program satisfies the independent-and assumption for 
the query 
sametitle(tit1,  tit2) 
because 
haswordtitle/2  

230 
Exact  Inference  
has no uncertainty. It  satisfies the independent-or assumption because each 
clause for sametitle/2  defines a different random variable. 
8.10.1  PITA(OPT)  
PITA(OPT) [Riguzzi, 2014] differs from PITA because it checks for the truth 
of the assumptions before applying BDD logical operations. If the assump­
tions hold, then the simplified probability computations are used. 
The data structures used to store probabilistic information in PITA(OPT) 
are pairs ( P,  T)  where P  is a real number representing a probability and T  is 
a term formed with the functors zero/0,  one/0,  c/2, or /2, and/2,  notj1,  and 
the integers. If T  is an integer, it represents a pointer to the root node of a 
BDD. If T  is not an integer, it represents a Boolean expression of which the 
terms of the form zero,  one,  c(var,  val)  and the integers represent the base 
case: c(var,  val)  indicates the equation var  = val  while an integer indicates 
a BDD. In  this way, we are able to represent Boolean formulas by means of 
either a BDD, a Prolog term, or a combination thereof. 
For example, or (Ox94 ba008,  and (c(1  , 1 ) , not (c ( 2,  3)))  represents the 
expression: B  v  (X1 = 1 1\  --.(X2 = 3))  where Bis  the Boolean function 
represented by the BDD whose root node address in memory is the integer 
Ox94ba008  in Prolog hexadecimal notation. 
PITA(OPT) differs from PITA also in the definition of zero/1,  one/1,  
not/2,  and/3  and or/3  that now work on pairs (P,  T)  rather than on BDDs. 
Moreover, the PITA transformation P IT A( Cr,  i) (Equation (8.3)) for a 
clause Cr.  is modified by replacing the conjunction 
get_var _n(r,  S,  [II1, ... , IIn],  Var),  equality(Var,  i, DD)  
with 
get_var _n(r,  S,  [II1, ... , IIn],  Var),  nth(i,  [II1, ... , IIn],  IIi), 
DD  = (IIi, c(V ar,  i) 
The one/1  and zero/1  predicates are defined as 
zero( (0, zero)).  
one((l,one)).  
The or /3 and and/3  predicates first check whether one of their input argu­
ment is (an integer pointing to) a BDD. If so, they also convert the other 
input argument to a BDD and test for independence using the library func­
tion bdd_ind(B1,  B2, I).  Such a function is implemented in C and uses the 
CUDD function Cudd_Support Index that retums an array indicating 
which variables appearin aBDD (the support variables). bdd_ind(B1,  B2, I)  
checks whether there is an intersection between the set of support variables 
of B1 and B2 and retums I  =  1 if the intersection is empty. If the two BDDs 

8.10  Modeling  Assumptions  in  PITA  231 
are independent, then the value of the resulting probability is computed using 
a formula and a compound term is retumed. 
If none of the input arguments of or /3 and and/3  are BDDs, then these 
predicates test whether the independent-and or the exclusive-or assumptions 
hold. If so, they update the value of the probability using a formula and retum 
a compound term. If not, they convert the terms to BDDs, apply the corre­
sponding operation, and retum the resulting BDD together with the probabil­
ity it represents. The code for or/3  and and/3  is shown in Figures 8.18 and 
8.19, respectively, where Boolean operation between BDDs are prefixed with 
bdd_.  
In these predicate definitions, ev /2 evaluates a term returning a BDD. In  
and/3,  after the first bdd_and/3  operation, a test is made to check whether 
the resulting BDD represent the 0 constant. If so, the derivation fails as this 
branch contributes with a 0 probability. These predicates make sure that, 
ar((PA,TA),(PB,TB),(PC,TC))  +­
((integer(TA);  integer(TB))­
ev(TA,  BA), ev(TB,  BB),  
bdd_ind(BA,  BB, I),  
(I=  1­
PCis  PA+  PB- PA*  PB,  
TC=  ar(BA,  BB)  
bdd_ar(BA,  BB, TC), reLprob(TC,  PC)  
) 
(ind(T A, TB)­
PCis  PA+  PB- PA*  PB,  
TC=  ar(BA,  BB)  
(exc(TA,  TB)­
PCisPA+PB,  
TC  =  ar(BA,  BB)  
ev(TA,  BA), ev(TB,  BB),  
bdd_ar(BA,  BB, TC), reLprob(TC,  PC)  
)   
)   
). 
Figure  8.18  Code for the or /3 predicate of PITA(OPT). 

232 
Exact  Inference  
and((PA, TA),  (PB, TB),  (PC, TC))+­
((integer(TA);integer(TB))-+  
ev(TA,  BA), ev(TB,  BB),  
bdd_ind(A,  BB, I),  
(I=  1--+  
PCisPA*PB,  
TC  =  and(BA,  BB)  
bdd_and(BA,  BB, TC),  reLprob(TC,  PC)  
(bdd_zero(TC)  -+  
fail  
true  
)   
)   
(ind(T A, TB)  -+  
PCisPA*PB,  
TC  =  and(BA,  BB)  
(exc(TA,TB)-+  
fail  
ev(T A,  BA), ev(T B,  BB),  
bdd_and(BA,  BB, TC),  reLprob(TC,  PC)  
)   
)   
). 
Figure  8.19  Code for the and/3  predicate of PITA(OPT). 
once a BDD has been built, it  is used in the following operations, avoiding 
the manipulation of terms and exploiting the work performed as much as 
possible. 
The not/2  predicate is very simple: it complements the probability and 
retums a new term: 
not((P,  B),  (Pl,not(B)))  +-Plis  1- P.  
The predicate exc/2  checks for the exclusiveness between two terms with a 
recursion through the structure of the terms, see Figure 8.20. 
For example, the goal 
exc(or(c(1,  1), c(2, 1)), and(c(1,  2), c(2, 2))) 

8.10  Modeling  Assumptions  in  PITA  233 
exc(zero,  _) +-!.  
exc( _, zero)  +-!.  
exc(c(V,N),c(V,Nl))  +-!,N\ =  Nl.  
exc(c(V,  N), or(X, Y))  +-!, exc(c(V,  N), X),  
exc(c(V,N), Y).  
exc(c(V,  N), and(X, Y))  +-!, (exc(c(V,  N), X); 
exc( c(V,  N), Y)).  
exc(or(A,  B), or(X, Y))  +-!, exc(A,X),  exc(A, Y),  
exc(B, X), exc(B, Y).  
exc(or(A,  B), and(X, Y))  +-!, (exc(A,X);  exc(A,  Y)), 
(exc(B,  X); exc(B,  Y)). 
exc(and(A,  B), and(X, Y))  +-!, exc(A,  X); exc(A, Y);  
exc(B, X); exc(B, Y).  
exc(and(A,  B),or(X, Y))  +-!, (exc(A,X);  exc(B,  X)), 
(exc(A,  Y);exc(B,  Y)). 
exc(not(A),  A)  +-!. 
exc(not(A),  and(X, Y))  +-!, exc(not(A), X);  
exc(not(A),  Y).  
exc(not(A),  or(X, Y))  +-!, exc(not(A),  X), 
exc(not(A),  Y).  
exc(A,  or(X,  Y))  +-!, exc(A,  X), exc(A,  Y).  
exc( A, and( X,  Y))  +- exc( A,  X); exc( A, Y).  
Figure  8.20  Code for the exc/2 predicate of PITA(OPT). 
matches the 7th clause and calls the subgoals 
exc(c(1, 1), c(1, 2)), exc(c(1, 1), c(2, 2)), exc(c(2, 1), c(1, 2)), 
exc(c(2, 1), c(2, 2)). 
Of the first two calls, exc(c(1, 1), c(1, 2)) succeeds, thus satisfying the first 
conjunct in the body. Of the latter two calls, exc(c(2, 1), c(2, 2)) succeeds, 
thus satisfying the second conjunct in the body and proving the goal. 
The ind/2  predicate checks for independence between two terms. It visits 
the structure of the first term until it reaches an atomic choice. Then it checks 
for the absence of the variable in the second term with the predicate absent/2.  
The code for ind/2  and absent/2  is shown in Figure 8.21. For example, 
the goal 
ind(or(c(1,  1), c(2, 1)), and(c(3,  2), c(4, 2))) 
matches the 6th clause and calls 
ind(c(1,  1), and(c(3,  2), c(4, 2))), ind(c(2,  1), and(c(3,  2), c(4, 2))). 
The first call matches the 5th clause and calls 
absent(1,  and(c(3,  2), c(4, 2))) 

234 
Exact  Inference  
ind(one,  -) +-!.  
ind(zero,  _) +-!.  
ind(_,  one) +-!.  
ind(_,  zero)  +-!.  
ind(c(V,  _N),  B)  +-!,  absent(V,  B).  
ind(or(X,  Y),  B)  +-!,  ind(X,  B),  ind(Y,  B).  
ind(and(X,  Y),  B)  +-!,  ind(X,  B), ind(Y,  B).  
ind(not(A),  B)  +- ind(A,  B).  
absent(V,c(Vl,_Nl))  +-!,V\=  Vl.  
absent(V,  or(X, Y))  +-!,  absent(V,  X),  absent(V,  Y).  
absent(V,  and(X,  Y))  +-!,  absent(V,  X), absent(V,  Y).  
absent(V,  not(A))  +- absent(V,  A).  
Figure  8.21  Code for the ind/2  predicate of PITA(OPT). 
which, in turn, calls absent(1,  c(3, 2)) and absent(1,  c(4, 2)). Since they 
both succeed, ind(c(1,  1), and(c(3,  2), c(4, 2))) succeeds as well. The sec­
ond call matches the 5th clause and calls absent(2,  and(c(3,  2), c(4, 2))) 
which, in turn, calls absent(2,  c(3, 2)) and absent(2,  c(4, 2) ). They both suc­
ceed 
so 
ind(c(2,  1), and(c(3,  2), c(4, 2))) 
and 
the 
original 
goal 
is proved. 
The predicates exc/2  and ind/2  define sufficient conditions for exclusion 
and independence, respectively. If the arguments of exc/2  and ind/2  do not 
contain integer terms representing BDDs, then the conditions are also nec­
essary. The code for predicate ev /2 for the evaluation of a term is shown in 
Figure 8.22. 
When the program satisfies the (IND,EXC) or (IND,IND) assumptions, 
the PITA(OPT) algorithm answers the query without building BDDs: terms 
are combined in progressively larger terms that are used to check the assump­
ev(B,  B)  +- integer(B),!.  
ev(zero,  B)  +-!,  bdd..zero(B).  
ev(one,  B)  +-!,  bdd_one(B).  
ev(c(V,  N), B)  +-!,  bdd_equality(V,  N,  B).  
ev(and(A,  B),  C)  +-!,  ev(A,  BA), ev(B,  BE),  
bdd_and(BA,  BE,  C). 
ev(or(A,  B), C) +-!,  ev(A,  BA),  ev(B,  BE),  
bdd_or(BA,  BB, C). 
ev(not(A),  C) +- ev(A,  B),  bdd_not(B,  C). 
Figure  8.22  Code for the ev /2 predicate of PITA(OPT). 

8.11  lnference  for  Queries  with  an  InfiniteNumber  of Explanations  235 
tions, while the probabilities of the combinations are computed only from the 
probabilities of the operands without considering their structure. 
When the program satisfies neither assumption, PITA(OPT) can still be 
beneficial since it delays the construction of BDDs as much as possible and 
may lead to the construction of less intermediate BDDs than PITA. While 
in PITA the BDD for each intermediate subgoal must be kept in memory 
because it is stored in the table and has to be available for future use, in 
PITA(OPT), BDDs are built only when necessary, leading to a smaller mem­
ory footprint and a leaner memory management. 
8.1  0.2  VIT  with  PITA  
VIT inference when the IND assumption holds can be computed by modify­
ing PITA(IND,EXC) so that the probability data structure includes the most 
probable explanation for the subgoal besides the highest probability of the 
subgoal. In this case, the support predicates are modified as follows: 
or(e(El,  Pl),  e(_E2, P2), e(El,  Pl)) ~ Pl  >= P2,!. 
or(e(_El, _Pl),  e(E2, P2), e(E2, P2)). 
and(e(El,  Pl),  e(E2, P2), e(E3,  P3)) ~ P3  is  Pl  *  P2, 
append(El,  E2, E3).  
zero(e(null,  0)). 
one( e([], 1)).  
ret_prob(B,  B).  
Moreover, in the PITA transformation P  IT A( Cr,  i) for a clause Cr  (Equa­
tion (8.3)), the conjunction 
get_var _n(r,  S,  [II1, ... , IIn],  Var),  equality(Var,  i, DD)  
is replaced by 
nth(i,  [II1, ... , IIn],  IIi), DD  = e([(R,  S,  i)], IIi) 
In  this way, we obtain PITAVIT(IND) that does not require the exclusiveness 
assumption. 
8.11   lnference  for  Queries  with  an  Infinite  Number  
of  Explanations  
When a discrete program contains function symbols, the number of explana­
tions may be infinite and the probability of the query may be the sum of a 
convergent series. In this case, the inference algorithm has to recognize the 
presence of an infinite number of explanations and identify the terms of the 
series. Sato and Meyer [2012, 2014] extend PRISM by considering programs 

236 
Exact  Inference  
under the generative  exclusiveness  condition:  at any choice point in any exe­
cution path of the top-goal, the choice is done according to a value sampled 
from a PRISM probabilistic switch. The generative exclusiveness condition 
implies the exclusive-or condition and that every disjunction originates from 
a probabilistic choice made by some switch. 
In this case, a cyclic  explanation  graph  can be computed that encodes 
the dependency of atoms on probabilistic switches. From this, a system of 
equations can be obtained defining the probability of ground atoms. Sato 
and Meyer [2012, 2014] show that first assigning all atoms probability 0 
and then repeatedly applying the equations to compute updated values re­
sult in a process that converges to a solution of the system of equations. For 
some pro gram, such as those computing the probability of prefixes of strings 
from Probabilistic context-free grammars (PCFGs), the system is linear, so 
solving it is even simpler. In general, this provides an approach for perform­
ing inference when the number of explanations is infinite but the generative 
exclusiveness condition holds. 
Gorlin et al. [2012] present the algorithm PIP (for Probabilistic Inference 
Plus), which is able to perform inference even when explanations are not 
necessarily mutually exclusive and the number of explanations is infinite. 
They require the programs tobe temporally  well-formed,  i.e., that one of the 
arguments of predicates can be interpreted as a time that grows from head to 
body. In this case, the explanations for an atom can be represented succinctly 
by Definite clause grammars (DCGs). Such DCGs are called explanation  gen­
erators  and are used to build Factored explanation diagrams (FEDs) that have 
a structure that closely follows that of BDDs. From FEDs, one can obtain a 
system of polynomial equations that is monotonic and thus convergent as in 
[Sato and Meyer, 2012, 2014]. So, even when the system is non-linear, a least 
solution can be computed to within an arbitrary approximation bound by an 
iterative procedure. 

9   
Lifted  lnference  
Reasoning with real-world models is often very expensive due to their com­
plexity. However, sometimes the cost can be reduced by exploiting symme­
tries in the model. This is the task of lifted  inference, that answers queries 
by reasoning on populations of individuals as a whole instead of considering 
each entity individually. The exploitation of the symmetries in the model can 
significantly speed up inference. 
Lifted inference was initially proposed by Poole [2003]. Since then, many 
techniques have appeared such as lifted versions of variable elimination and 
belief propagation, using approximations and dealing with models such as 
parfactor graphs and MLNs [de Salvo Braz et al., 2005; Milchet al., 2008; 
Van den Broeck et al., 2011]. 
9.1  Preliminaries  on  Lifted  lnference  
Applying lifted inference to PLP languages under the DS is problematic be­
cause the conclusions of different rules are combined with noisy-OR that 
requires aggregations at the lifted Ievel when existential variables are present. 
For example, consider the following ProbLog program from [De Raedt and 
Kimrnig, 2015]: 
p :: famous (Y) .  
popular (X) :- friends (X, Y), famous (Y).  
In  this case, P(popul ar ( j  ohn)) =  1 - (1 - p )m where m  is the number 
of friends of j  ohn. This is because the body contains a logical variable not 
appearing in the head, which is thus existentially quantified. A grounding of 
the atom in the head of this clause represents the disjunction of a number 
of ground bodies. In  this case, we don't need to know the identities of these 
friends, we just need to know how many there are. Hence, we don't need to 
ground the clauses. 
237 

238 
Lifted  Inference  
Example  86 (Running example for lifted inference- ProbLog). We  consider  
a  ProbLog  program  representing  the  workshop  attributesproblern  of [Milch  
et  al.,  2008].  It  models  the  organization  of  a  workshop  where  a  number  
of people  have  been  invited.  The  predicate  series  indicates  whether  the  
workshop  is  successful  enough  to  start  a  series  of  related  meetings  while  
attends  (P )  indicates  whether  person  P  will  attend  the  workshop.  We  can  
model  this  problern  with  the  ProbLog  program:  
series  :- self .  
series  :- attends (P ).  
attends (P )  :- at (P , A ).  
O.l:: self .  
0.3:: at (P , A )  :- person (P ),  attribute (A ).  
Note  that  all  rules  are  range  restricted,  i.e.,  all  variables  in  the  head  also  
appear  in  a  positive  literal  in  the  body.  A  workshop  becomes  a  series  ei­
ther  because  of  its  own  merits  with  a  10%  probability  ( represented  by  the  
probabilistic  fact  se 1  f )  or  because  people  attend.  People  attend  the  work­
shop  depending  on  the  workshop  's  attributes  such  as  location,  date,  fame  
of  the  organizers,  etc.  (modeled  by  the  probabilistic  fact  at  (P , A )  ).  The  
probabilistic  fact  a  t  ( P ,  A )  represents  whether  person  P  attends  because  
of  attribute  A.  Note  that  the  last  statement  corresponds  to  a  set  of  ground  
probabilistic  facts,  one  for  each  person  P  and  attribute  A  as  in  ProbLog2  
(Section  8.7).  For  the  sake  of  brevity,  we  omit  the  (non-probabilistic)  facts  
describing  the  person /  1  and  attributeil predicates.  
Parameterized Random Variables (PRVs) and parfactors  have been de­
fined in Section 2.9.3. We briefiy recall here their definition. PRVs represent 
sets of random variables, one for each possible ground substitution to all of 
its logical variables. Parfactars are triples 
(C, V, F) 
where C  is a set of inequality constraints on logical variables, V  is a set of 
PRVs and F  is a factorthat is a function from the Cartesian product of ranges 
of PRV s of V  to real values. A parfactor is also represented as F  (V)  IC  or 
F(V) if there are no constraints. A constrained  PRV  V  is of the form VIC,  
where V = p(X  1, ... , Xn)  is a non-ground atom and C  is a set of constraints 
on logical variables X  =  {X1 ,  ...  ,  Xn}·  Each constrained PRV represents 
the set of random variables {p(x) lx  E  C}, where x  is the tuple of constants 
(x1 ,  ...  ,  xn)·  Given a (constrained) PRV V, we use RV(V)  to denote the set 

9.1  Preliminaries  on  Lifted  lnference  239 
of random variables it  represents. Bach ground atom is associated with one 
random variable, which can take any value in range(V).  
PFL [Gomes and Costa, 2012] described in Section 2.9.3 extends Prolog 
to support probabilistic reasoning with parametric factors. We repeat below 
Example 34 for ease of reading. 
Example  87  (Running example- PFL pro gram). A  version  of the  workshop  
attributesproblern  can  be  modeled  by  a  PFL  program  such  as  
bayes series, attends(P); [0.51, 0.49, 0.49, 0.51]; 
[person (P)] . 
bayes attends(P), at(P,A); [0.7, 0.3, 0.3, 0.7]; 
[person(P),attribute(A)]. 
Thejirst  PFLfactor  has  series and  attends (P) as  Boolean  random  
variable  arguments,  
[ 0 . 51 , 0 . 4 9, 0 . 4 9 , 0 . 51 J 
as  table  and  
[person (P) ] as  constraint.  
This  model  is  not  equivalent  to  the  one  of Example  86,  but  it  corresponds  
to  a  ProbLog  program  that  has  only  the  second  and  the  third  clause  of Ex­
ample  86.  Models  equivalent  to  Example  86  will  be  given  in  Examples  89  
and90.  
9.1.1  Variable  elimination  
Variable Elimination (VE) [Zhang and Poole, 1994, 1996] is an algorithm for 
probabilistic inference on graphical models. VE takes as input a set of factors 
F,  an elimination order p,  a query variable X, and a list y  of observed val­
ues. After setting the observed variables in all factors to their corresponding 
observed values, VE eliminates the random variables from the factors one by 
one until only the query variable X remains. This is done by selecting the 
firstvariable Z from the elimination order p  and then calling SUM-OUT that 
eliminates Z by first multiplying all the factors that include Z into a single 
factor and summing out Z from the newly constructed factor. This procedure 
is repeated until p  becomes empty. In the final step, VE multiplies together the 
factors of F  obtaining a new factor 1 that is normalized as l(x)/.L:x'  1(x') to 
give the posterior probability. 
In many cases, we need to represent factors where a Boolean variable X 
with parents Y  is true if any ofthe Yi is true, i.e., the case where Xis the dis­
junction of the variables in Y. This may, for example, represent the situation 
where the Yis are causes of X, each capable of making X true independently 

240  Lifted  lnference  
Figure  9.1  BN representing an OR dependency between X and Y. 
of the values of the others. This is represented by the BN of Figure 9.1 where 
the CPT for X is deterministic and is given by 
At least one Yi equal to 1 
Remairring columns 
X= 1 
1.0 
0.0 
X=O 
0.0 
1.0 
In practice, however, each parent Y i may have a noisy inhibitor variable Ii 
that independently blocks or activates Yi, so X is true if either any  of the 
causes Yi holds true and  is not inhibited. This can be represented with the BN 
of Figure 9.2 where the Y~ are given by the Boolean formula Y~ = Yi 1\  -.Ii, 
i.e., Y~ is true if Yi is true and is not inhibited. So the CPT for the Y~ is 
deterministic. The Ii variables have no parents and their CPT is P(Ii = 0) = 
Ih where IIi is the probability that Yi is not inhibited. 
This represents the fact that X is not simply the disjunction of Y but 
depends probabilistically from Y with a factor that encodes the probability 
that the Yi variables are inhibited. 
If we marginalize over the Ii variables, we obtain a BN like the one of 
Figure 9.1 where, however, the CPT for Xis not anymore that of a disjunction 
but takes into account the probabilities that the parents are inhibited. This is 
called a noisy-OR  gate.  Handling this kind of factor isanon-trivial problem. 
Noisy-OR gatesarealso called causalindependent  models. An example of 
an entry in a noisy-OR factor is 
n  
P(X=  1IY1 = 1, ... ,Yn = 1) = 1- n(l-IIi) 
i=l 
In fact, X is true iff none of its causes is inhibited. 
The factor for a noisy-OR can be expressed as a combination of factors 
by using the intermediate Y~ variables that represent the effect of each cause 
taking into account the inhibitor. 

9.1  Preliminaries  on  Lifted  lnference  241 
Figure  9.2 BN representing a noisy-OR dependency between X and Y. 
For example, suppose X has two causes Y 1 and Y 2 and let cfJ(YI, Y2, x) be 
the noisy-OR factor. Let the variables Y~ and Y~ be defined as in Figure 9.2 
and consider the factors 1/J1 (y1, YD and 1/J2(y2, y~) modeling the dependency 
of Y~ from Yi. These factors are obtained by marginalizing the Ii variables, 
so, if P(Ii =  0) =  Ili, they are given by 
'lj;(yl, yi) 
yi  = 1 
yi  =0 
y~ = 1 
rri 
0.0 
y~ = 0 
1- rri 
1.0 
Then the factor cfJ(YI, y2, x) can be expressedas 
c/J(yl,Y2,x) =  .2: 
1/JI(YI,YD'I/J2(Y2,Y~) 
(9.1)  
y~ vy~=x 
where the summation is over all values y~ and y~ of Y~ and Y~ whose 
disjunction is equal to x. The X variable is called convergent  as it is where 
independent contributions from different sources are collected and combined. 
Non-convergent variables are called regular  variables.  
Representing factors such as cp  with 1/J1 and 1/J2 is advantageaus when the 
number of parents grows large, as the combined size of the component factors 
grows linearly, instead of exponentially. 
Unfortunately, a Straightforward use of VE for inference would lead to 
construct 0  ( 2n)  tables where n  is the number of parents and the summation 
in Equation (9.1)  will have an exponential number of terms. A modified 
algorithm, VE 1 [Zhang and Poole, 1996], combines factors through a new 
operator (8): 

242 Lifted  lnference  
<;i>®7f>(E1 = a1, ... ,Ek  = ak,A,B1,B2) = 
2:  
... 
2:   
0:11  va12=a1  
akl  vak2=ak  
<;i>(E1 = an, ... ,Ek = akl,A,BI)7f>(EI = a12, ... ,Ek = ak2,A,B2) 
(9.2) 
Here, cp  and 'ljJ  are two factors that share convergent variables E1 ... Ek,  Ais 
the list of regularvariables that appear in both cp  and '1/J,  while B1 and B2 are 
the lists of variables appearing only in cp  and '1/J,  respectively. By using the 
® operator, factors encoding the effect of parents can be combined in pairs, 
without the need to apply Equation (9 .1)  on all factors at once. 
Factors containing convergent variables are called heterogeneaus  while 
the remaining factors are called homogeneous.  Heterogeneaus factors sharing 
convergent variables must be combined with the operator ®, called heteroge­
neaus  multiplication.  
Algorithm VE 1  exploits causal independence by keeping two lists of fac­
tors: a list of homogeneaus factors F1 and a list of heterogeneaus factors 
F2. Procedure SUM-OUT is replaced by SUM-OUTl that takes as input F1 
and F2 and a variable Z to be eliminated. First, all the factors containing 
Z are removed from F1 and combined with multiplication to obtain factor 
cp.  Then all the factors containing Z are removed from F2 and combined 
with heterogeneaus multiplication obtaining '1/J.  If there are no such factors 
'ljJ  = nil.  In  the latter case, SUM-OUTl adds the new (homogeneous) factor 
.L:z  cp  to F1; otherwise, it adds the new (heterogeneous) factor .L:z  cp'l/J  to F2. 
Procedure VEl is the same as VE with SUM-OUT replaced by SUM-OUTl and 
with the difference that two sets of factors are maintained instead of one. 
However, VE 1 is not correct for any elimination order. Correctness can 
be ensured by deputizing  the convergent variables: every such variable X is 
replaced by a new convergent variable X' ( called a deputy  variable)  in the 
heterogeneaus factors containing it, so that X becomes a regular variable. Fi­
nally, a new factor ~(X, X') is introduced, called deputy factor,  that represents 
the identity function between X and X', i.e., it is defined by 
I  ~(x, X') ~~~~ I  ~~ I  o~~ ~~~~ I  
The network on which VE 1 works thus takes the form shown in 
Figure 9.3. Deputizing ensures that VEl is correct as long as the elimination 
order is suchthat p(X')  <  p(X).  

9.1  Preliminaries  on  Lifted  lnference  243 
Figure  9.3 
BN ofFigure 9.1 after deputation. 
9.1.2  GC-FOVE  
Work on Iifting VE started with FOVE [Poole, 2003] and led to the definition 
of 
C-FOVE 
[Milch 
et 
al., 
2008]. 
C-FOVE 
was 
refined 
in 
GC-FOVE [Taghipour et al., 2013], which represents the state of the art. 
Then, Gomes and Costa [Gomes and Costa, 2012] adapted GC-FOVE to PFL. 
First-order  variable  elimination  (FOVE) [Poole, 2003; de Salvo Braz 
et al., 2005] computes the marginal probability distribution for a query ran­
dom variable by repeatedly applying operators that are lifted Counterparts 
of VE's operators. Models are in the form of a set of parfactors that are 
essentially the same as in PFL. 
GC-FOVE tries to eliminate all (non-query) PRVs in a particular order by 
applying the following operations: 
1. Lifted  Sum-Out  that excludes a PRV from a parfactor cp  if the PRV s only 
occurs in cp;  
2.  Lifted  Multiplication  that multiplies two aligned parfactors. Matehing 
variables must be properly aligned and the new coefficients must be 
computed taking into account the number of groundings in the con­
straints C;  

244 
Lifted  lnference  
3.  Lifted  Absorption  that eliminates n  PRVs that have the same observed 
value. 
If these operations cannot be applied, an enabling operation must be chosen 
such as splitting  a parfactor so that some ofits PRV s match another parfactor. 
If no operation can be executed, GC-FOVE completely grounds the PRV s and 
parfactors and performs inference on the ground level. 
GC-FOVE also considers PRVs with counting formulas, introduced in 
C-FOVE [Milchet al., 2008]. A counting formula takes advantage of symme­
tries existing in factors that are products of independent variables. It  allows 
the representation of a factor of the form q)(p(xl),  p(x2), ... ,p(xn) ), where 
all PRVs have the same domain, as q)(#x[p(X)]),  where #x[p(X)]  is the 
counting formula.  The factor implements a multinomial distribution, suchthat 
its values depend on the number of variables n  and the domain size. Counting 
formulas may result from summing-out, when we obtain parfactors with a 
single PRV, or through Counting  Conversion  that searches for factors of the 
form 
q)(n (s(xj)p(xj,  Yi)))  
i  
and counts on the occurrences of Yi.  
GC-FOVE employs a constraint-tree to represent arbitrary constraints C,  
whereas PFL simply uses sets of tuples. Arbitrary constraints can capture 
more symmetries in the data, which potentially offers the ability to perform 
more operations at a lifted level. 
9.2  LP2  
LP2 [Bellodi et al., 2014] is an algorithm for performing lifted inference 
in ProbLog that translates the program into PFL and uses an extended GC­
FOVE version for managing noisy-OR nodes. 
9.2.1  Translating  Problog  into  PFL  
In order to translate ProbLog into PFL, the program must be acyclic 
(Definition 4, see Section 9.5 for the case of cyclic programs). If this con­
dition if fulfilled, the ProbLog program can be converted first into a BN with 
noisy-OR nodes. Here we specialize the conversion for LPADs presented in 
Section 2.5 to the case of ProbLog. 

6.2  LP2  245 
For each atom a  in the Herbrand base of the program, the BN contains a 
Boolean random variable with the same name. Bach probabilistic fact p  :: a  
is represented by a parentless node with the CPT: 
lal1~p~~~  
For each ground rule Ri  =  h  ~ b1, ... , bn,  "'c1, ... , "'Cm. we add to the 
network a random variable called hi  that has as parents the random variables 
representing the atoms b1, ... , bn,  c1, ... , Cm and the following CPT: 
hi  
b1 = 1, ... , bn  = 1, Cl = 0, ... , Cm  = 0 
All other columns 
0 
0.0 
1.0 
1 
1.0 
0.0 
In  practice, hi  is the result of the conjunction of the random variables repre­
senting the atoms in the body. Then, for each ground atom h  in the Herbrand 
base not appearing in a probabilistic fact, we add random variable h  to the 
network, with all the his of the ground rules with h  in the head as parents and 
with CPT: 
h  At least one hi  =  1 
All other columns 
0 
0.0 
1.0 
1 
1.0 
0.0 
representing the result of the disjunction of the random variables hi.  These 
families of random variables can be directly represented in PFL without the 
need to first ground the program, thus staying at the lifted level. 
Example  88 (Translation of a ProbLog program into PFL). The  translation  
ofthe  ProbLog  program  of Example  86  into  PFL  is  
bayes series1, self; [1, 0, 0, 1] ;  [ l.  
bayes series2, attends (P); 
[1, 0, 0, 1] ; 
[person (P) l  .  
bayes series, series1, series2 ;  [1, 0, 0, 0, 0, 
1, 1, 1] ; 
[ l.  
bayes attends1(P), at (P,A); 
[1, 0, 0, 1] ; 
[person(P),attribute(A)]. 
bayes attends(P), attends1 (P); 
[1, 0, 0, 1] ; 
[person (P) l  .  

246  Lifted  lnference  
bayesself; [0.9, 0.1]; []. 
bayes at(P,A); [0.7, 0.3] ; 
[person(P), 
attribute(A)]. 
Notice that series2 and attends1 (P) can be seen as or-nodes, since 
they are in fact convergent variables. Thus, after grounding, factors derived 
from the second and the fourth parfactor should not be multiplied tagether 
but should be combined with heterogeneaus multiplication. 
To do so, we need to identify heterogeneaus factors and add deputy vari­
ables and parfactors. We thus introduce two new types of parfactors to PFL, 
het and deputy. As mentioned before, the type of a parfactor refers to 
the type of the network over which that parfactor is defined. These two new 
types are used in order to define a noisy-OR (Bayesian) network. The first 
parfactor is such that its ground instantiations areheterogeneaus factors. The 
convergent variables are assumed to be represented by the first atom in the 
parfactor list of atoms. Lifting identity is straightforward: it corresponds to 
two atoms with an identity factor between their ground instantiations. Since 
the factor is fixed, it is not indicated. 
Example  89  (ProbLog program to PFL- LP2). The  translation  of the  Prolog  
program  of Example  86,  shown  in  Example  88,  is  modified  with  the  two  new  
factors  het and  deputy as  shown  below:  
bayes series1p, self; [1, 0, 0, 1] ; []. 
het series2p, attends(P); [1, 0, 0, 1]; 
[person (P)] . 
deputy series2, series2p; [ J. 
deputy series1, series1p; [ J. 
bayes series, series1, series2; [1, 0, 0, 0, 0, 1, 
1' 1 J ; 
[ J • 
het attends1p (P), at (P,A); [1, 0, 0, 1]; 
[person(P),attribute(A)]. 
deputy attends1 (P), attends1p (P); [person (P)] . 
bayes attends (P), attends1 (P); [1, 0, 0, 1]; 
[person (P)] . 
bayesself; [0.9, 0.1]; []. 
bayes at(P,A); [0.7, 0.3] ; 
[person(P), 
attribute(A)]. 
Here,  series1p, series2p, and  attends1p (P) 
are  the  new  
convergent  deputy  random  variables,  and  series1, series2, and  

9.3  Lifted  lnference  with  Aggregation  Parfactars  247 
a t  t  ends 1 ( P) are  their  corresponding  regular  variables.  The  fifth  factor  
represents  the  OR  combination  of  seriesl and  series2 to  variable  
series. 
GC-FOVE must be modified in order to take into account heterogeneaus par­
factors and convergent PRVs. The VE algorithm must be replaced by VE1, 
i.e., two lists of factors must be maintained, one with homogeneaus and the 
other with heterogeneaus factors. When eliminating variables, homogeneaus 
factors have higher priority and are combined with homogeneaus factors only. 
Then heterogeneaus factors are taken into account and combined before start­
ing to mix factors from both types, to produce a final factor from which the 
selected random variable is eliminated. 
Lifted heterogeneaus multiplication considers the case in which the two 
factors share convergent random variables. The SUM-OUT operator must be 
modified as well to take into account the case that random variables must 
be summed out from a heterogeneaus factor. The formal definition of these 
two operators is rather technical and we refer to [Bellodi et al., 2014] for the 
details. 
9.3 Lifted  lnference  with  Aggregation  Parfactcrs  
Kisynski and Poole [Kisynski and Poole, 2009a] proposed an approach based 
on aggregation  paifactors  instead of parfactors. Aggregation parfactors are 
very expressive and can represent different kinds of causal independence 
models, where noisy-OR and noisy-MAX are special cases. They are of the 
form (C, P, C, Fp, [g], CA),  where P and C are PRVs which share all the pa­
rameters except one -let's say A  which is in P but not in C- and the range 
of P (possibly non-Boolean) is a subset ofthat of C; C  and CA  are sets of in­
equality constraints respectively not involving and involving A;  Fp is a factor 
from the range of P to real values; and [g] is a commutative and associative 
deterministic binary operator over the range of C. 
When [g] is the MAX operator, of which the OR operator is a special case, 
a total ordering -<  on the range of C can be defined. An  aggregation parfactor 
can be replaced with two parfactors of the form (C u  CA,  {P, C'}, Fe) and 
(C, {C, C'}, Fb.),  where C' is an auxiliary PRV that has the same parameteri­
zation and range as C. Let v be an assignment of values to 
random variables, then Fc(v(P), v(C')) =  Fp(v(P)) when v(P) :S  v(C'), 
Fc(v(P), v(C')) =  0 otherwise, while Fb.(v(C), v(C')) =  1 if v(C) 
v(C'), -1 if v(C) is equal to a successor of v(C') and 0 otherwise. 

248 
Lifted  lnference  
In  ProbLog, we can use aggregation parfactors to model the dependency 
between the head of a rule and the body, when the body contains a single lit­
eral with an extra variable. In this case in fact, given a grounding of the head, 
the contribution of all the ground clauses with that head must be combined 
by means of an OR. Since aggregation parfactors are replaced by regular par­
factors, the technique can be used to reason with ProbLog by converting the 
program into PFL with theseadditional parfactors. The conversion is possible 
only if the ProbLog program is acyclic. 
In  the case of ProbLog, the range of PRVs is binary and ~ is OR. For 
example, the clause series2:- attends (P) can be represented with 
the aggregation parfactor 
(0,  attends (P), series2, Fp, v, 0),  
where Fp(O) = 1 and Fp(1) = 1. This is replaced by the parfactors 
(0, {attends (P), series2p},Fc) 
(0, {series2, series2p}, F,0,.) 
with Fc(O,  0) =  1, Fc(O,  1) =  1, Fc(1, 0) =  0, Fc(1, 1) =  1, F,0,.(0,  0) =  1, 
F,0,.(0,  1) =  0, F,0,.(1, 0) =  -1, and F,0,.(1, 1) =  1. 
When the body of a rule contains more than one literal and/or more than 
one extra variable with respect to the head, the rule must be first split into 
multiple rules ( adding auxiliary predicate names) satisfying the constraint. 
Example  90  (ProbLog program to PFL- aggregation parfactors). The  pro­
gram  of  Example  86  using  the  above  encoding  for  aggregation  
paifactors  is  
bayes series1p, self; [1, 0, 0, 1] 
;  [ J. 
bayes series2p, attends(P) [1, 0, 1, 1 J ; 
[person (P)] . 
bayes series2, series2p; [1, 0, -1, 1 J ; [ J. 
bayes series1, series1p; [1, 0, -1, 1 J ; [ J. 
bayes series, series1, series2; [1, 0, 0, 0, 0, 1, 
1, 1 J ;  
[ l.  
bayes attends1p(P), at (P,A); [1, 0, 1, 1] ; 
[person(P),attribute(A)]. 
bayes attends1(P), attends1p(P); [1, 0, -1, 1] ; 
[person (P) l .  
bayes attends(P), attends1(P); [1, 0, 0, 1] ; 

9.4  Weighted  First-order  Model  Counting  249 
[person (P)] . 
bayes self; [0.9, 0.1]; []. 
bayes at(P,A); [0.7, 0.3] ; 
[person(P),attribute(A)]. 
Thus, by using the technique of [Kisynski and Poole, 2009a], we can perform 
lifted inference in ProbLog by a simple conversion to PFL, without the need 
to modify PFL algorithms. 
9.4  Weighted  First-order  Model  Counting  
A different approach to lifted inference for PLP uses Weighted first order 
model counting (WFOMC). WFOMC takes as input a triple (ß,  w,  w),  where 
ß  is a sentence in first-order logic and wand w  are weight functions which 
associate a real number to positive and negative literals, respectively, depend­
ing on their predicate. Given a triple (ß, w,  w)  and a query cp,  its probability 
P( cp)  is given by 
WFOMC(ß  A cp,w,w)  
P(cp)  =  
WFOMC(ß,  w,  w)  
Here, WFO M C (ß, w,  w)  corresponds to the sum of the weights of all Her­
brand models of ß,  where the weight of a model is the product of its literal 
weights. Hence 
WFOMC(ß,  w,  w)  = .2:: n w(pred(l))  n w(pred(l))  
wi=ßlEwo 
lEWl 
where wo  and w1  are, respectively, false and true literals in the interpretation 
w  and pred  maps literals l  to their predicate. Two lifted algorithms exist for 
exact WFOMC, one based on first-order knowledge compilation [Van den 
Broeck et al., 2011; Van den Broeck, 2011; Van den Broeck, 2013] and the 
other based on first-order DPLL search [Gogate and Domingos, 2011]. They 
both require the input theory tobe injirst-order  CNF.  A first-order CNF is a 
theory consisting of a conjunction of sentences of the form 
vxl  ...  'VXn  h  V  ...  V  lm.  
A ProbLog program can be encoded as a first-order CNF using Clark's com­
pletion, see Section 1.4.1. For acyclic logic programs, Clark's completion is 
correct, in the sense that every model of the logic program is a model of 

250 
Lifted  lnference  
the completion, and vice versa. The result is a set of rules in which each 
predicate is encoded by a single sentence. Consider ProbLog rules of the 
form p(X)  ~ bi(X,  Yi)  where Yi  isavariable that appears in the body bi  
but not in the head p(X).  The corresponding sentence in the completion is 
VX  p(X)  ~ Vi 3Yi  bi(X,  Y;). For cyclic programs, see Section 9.5 below. 
Since WFOMC requires an input where existential quantifiers are absent, 
Van den Broeck et al. [2014] presented asound and modular Skolemization 
procedure to translate ProbLog programs into first-order CNF. Regular Skolem­
ization cannot be used because it introduces function symbols, that are prob­
lematic for model counters. Therefore, existential quantifiers in expressions 
of the form 3X  </J(X,  Y)  are replaced by the following formulas 
[Van den Broeck et al., 2014]: 
VY  VX  z(Y)  v  ----.<jJ(X,  Y)  
VY  s(Y)  v  z(Y)  
VY  VX  s(Y)  v  ----.<jJ(X,  Y)  
Here z  is the Tseitin predicate (w(z)  = w(z)  = 1) and s  is the Skolem 
predicate (w(s)  = 1,  w(s)  = -1).  This substitution can also be used for 
eliminating universal quantifiers since 
VX  <P(X,  Y)  
can be seen as 
----.:JX ----.<jJ(XY).  
Existential quantifiers are removed until no more substitutions can be applied. 
The resulting program can then be encoded as a first-order CNF with standard 
transformations. 
This replacement introduces a relaxation of the theory, thus the theory 
admits more models besides the regular, wanted ones. However, for every 
additional, unwanted model with weight W,  there is exactly one additional 
model with weight - W,  and thus the WFOMC does not change. The inter­
action between the three relaxed formulas and the model weights follows the 
behavior: 
1.  When z(Y)  is false, then 3X  <P(X,  Y)  is false while s(Y)  is true, this 
is a regular model whose weight is multiplied by 1. 
2.  When z(Y)  is true, then either: 
(a)  3X  <P(X,  Y)  is true and s(Y)  is true, this is a regular model whose 
weight is multiplied by 1; or 

9.4  Weighted  First-order  Model  Counting  251 
(b)  3X  </J(X,  Y)  is false and s(Y)  is true, this is an additional model 
with a positive weight W,  or 
(c)  3X  </J(X,  Y)  is true and s(Y)  is false, this is an additional model 
with weight - W.  
The last two cases cancel out. 
The WFOMC encoding for a ProbLog program exploits two mapping func­
tions which associate the probability Ili  and 1 - Ili  of a probabilistic fact 
with the positive and negative literals of the predicate, respectively. After the 
application of Clark's completion, the result may not be in  Skolem normal 
form; thus, the techniques described above must be applied before executing 
WFOMC. The system WFOMC1 solves the WFOMC problern by compiling 
the input  theory into first-order d-DNNF diagrams [Darwiche, 2002; Chavira 
and Darwiche, 2008]. 
Example  91  (ProbLog program to Skolem normal form). The  translation  
of  the  ProbLog  program  of Example  86  into  the  WMC  input  format  of  the  
WFOMC  system  is  
predicate series1 1 1 
predicate series2 1 1 
predicate self 0.1 0.9 
predicate at(P,A) 0.3 0.7 
predicate z1 1 1 
predicate s1 1 -1 
predicate z2 (P) 1 1 
predicate s2(P) 1 -1 
series v ! z1 
!series v z1 
z1 v !self 
z1 v !attends(P) 
z1 v s1 
s1 v !self 
s1 v !attends(P) 
attends(P) v ! z2(P) 
!attends(P) v 
z2(P) 
1 https://dtai.cs.kuleuven.be/software/wfomc 

252 Lifted  lnference  
z2(P)  v  !at(P,A)  
z2(P)  v s2(P)  
s2(P)  v !at(P,A)  
Here,  predicate  is  the  mapping  functionfor  the  probability  values  while  
z  1 and  z 2 are  Tseitin  predicates  and  s  1 and  s 2 are  Skolem  predicates.  
9.5 Cyclic  Logic  Programs  
LP2  and aggregation parfactors, described in Sections 9.2 and 9.3, respec­
tively, require a conversion from ProbLog to PFL for performing inference. 
The first step of this translation is the transformation of a ProbLog program 
into a BN with noisy-OR nodes. However, since BNs cannot have cycles, 
this conversion is not correct if the program is cyclic or non-tight,  i.e., if the 
program contains positive cycles. A similar problern occurs with WFOMC: 
Clark's completion [Clark, 1978] is correct only for acyclic logic programs. 
Fages [1994] proved that if an LP program is acyclic, then the Herbrand 
models of its Clark's completion [Clark, 1978] are minimal and coincide 
with the stable models of the original LP program. The consequence of this 
theoretical result is that, if the ProbLog program is acylic, we can correctly 
convert it into a first-order theory by means of Clark's completion. 
To apply these techniques to cyclic programs, we need to remove posi­
tive loops. We could first apply the conversion proposed by Janhunen [2004] 
(also see Section 8.7) that converts normallogic programs to atomic  normal  
programs  then to clauses. An atomic normal program contains only rules of 
the form 
a  ~"'Cl,···, "'Cm· 
where a  and Ci are atoms. Suchprograms are tight and, as a consequence, it is 
possible to translate them into PFL programs and use Clark's 
completion. 
However, this conversion was proposed only for the case of ground LPs. 
Proposing a conversion for non-ground programs is an interesting direction 
for future work, especially if function symbols are allowed. 
9.6  Comparison  of  the  Approaches  
Riguzzi et al. [2017a] experimentally compared LP2 , C-FOVE with aggrega­
tion parfactors (C-FOVE-AP), and WFOMC on five problems: 

9.6  Camparisan  afthe  Approaches  253 
• workshops  attributes  Milch et al. [2008]; 
• two different versions of campefing  workshopsMilchet  al. [2008]; 
• two different versions of Example 7 in Poole [2008], that we call 
plates.  
According to Jaeger and Van den Broeck [2012] nad Van den Broeck [2011], 
function-free first-order logic with equality and two variables per formula 
(2-FFFOL(=)) is domain-liftable, i.e., the complexity of reasoning is poly­
nomial in the domain size. Alltheseproblems fall in 2-FFFOL(=) and the 
experiments confirm that systems take polynomial time. However, WFOMC 
performs much better that the other systems, while LP2 and C-FOVE-AP 
show approximately the same performance on all problems. 


10   
Approximate  lnference  
Approximate inference aims at computing the results of inference in an ap­
proximate way so that the process is eheaper than the exact computation of 
the results. 
We can divide approaches for approximate inference into two groups: 
those that modify an exact inference algorithm and those based on sampling. 
10.1  Problog1  
ProbLogl includes three approaches for approximately solving the EVID 
task. The first is based on iterative deepening and computes a lower and an 
upper bound for the probability of the query. The second instead approxi­
mates the probability of the query only from below using a fixed number of 
proofs. The third uses Monte Carlo sampling. 
1 0.1.1  Iterative  deepening  
In  iterative deepening, the SLD tree is built only up to a certain depth [De 
Raedt et al., 2007; Kimmig et al., 2008]. Then two sets of explanations are 
built: Kz,  encoding the successful proofs present in the tree, and Ku.  encoding 
the successful and still open proofs present in the tree. The probability of Kz  is 
a lower bound on the probability of the query, as some of the open derivations 
may succeed, while the probability of Ku  is an upper bound on the probability 
of the query, as some of the open derivations may fail. 
Example  92 (Path- ProbLog-iterative deepening). Consider  the  program  
of Figure  10.1  which is a probabilistic version  ofthe program of Example 1  
and  represents  connectivity  in  the  probabilistic  graph  of Figure  1 0.2.  
The  query  path( c, d)  has  the  covering  set  of explanations  
K  =  {{ce, ef,fd},{cd}}  
255 

256 
Approximate  lnference  
path(X,X)o  
path(X, Y)  +- edge(X,  Z),path(Z,  Y)o  
Oo8 :: edge(a,  c)o 
Oo7 :: edge(a,b)o  
Oo8 :: edge(c,  e)o 
Oo6 :: edge(b,  c)o 
Oo9 :: edge(c,  d)o  
00625 :: edge(e,  f)o  
Oo8 :: edge(f,  d)o  
Figure  10.1  Program of Example 920 
Figure  10.2  Probabilistic graph of Example 920 
where  atomic  choices  (f,  0,  l)forfacts  oftheform  f  = II  ::  edge(x,  y)  are  
represented  as  xyo  K  can  be  made  pairwise  incompatible  as  
K'  =  {{ce, ej,jd,  ----.cd}, {cd}} 
where  ----.cd  indicates  choice  (f,  0, 0) for  f  = 009 :: edge(c,  d)o  The  prob­
ability  of  the  query  is  P(path(c,  d))  = 008 °  00625 
008 °  001 +  009
°  
= 
00940 
+- path( c,  d)  
I  
+- edge(c,  Zo),  
path(Zo,d)  
Zo/v  
~Zo/d 
path( e,  d)  
+- path( d,  d)  
+- edgJ(e,  Z1), 
/ 
+- ~ge(d, Z2), 
path(Z1,  d)  
+-
path(Z2,  d)  
ZI/fl  
I  
+- path(J,  d)  
fail 
Figure  10.3  SLD tree up to depth 4 for the query path( c, d)  from the 
program of Example 920 

10.1  ProbLog1  257 
Fora  depth  limit  of 4,  we  get  the  tree  of Figure  10.3.  This  tree  has  one  
successful  derivation,  associated  with  the  explanation  t~;l = {cd}, one  failed  
derivation,  and  one  derivation  that  is  still  open,  the  one  ending  with  path(f,  d),  
that  is  associated  with  composite  choice  t~;l = {ce,  ef},  so  Kz  = {t~;I} and  
Ku  = {t~;l, t~;2}· Wehave  P(Kz)  = 0.9 and  P(Ku)  = 0.95 and  P(Kz)  ~ 
P(path(c,  d))  ~ P(Ku).  
The iterative deepening algorithm of ProbLog1 takes as input an error bound 
E, a depth bound d,  and a query q.  It  constructs an SLD tree for q  up to 
depth d.  Then it builds sets of composite choices Kz  and Ku  and computes 
their probabilities. If the difference P  (Ku)  - P  (Kz)  is smaller than the error 
bound E, this means that a solution with a satisfying accuracy has been found 
and the interval [P(Kz),  P(Ku)]  is retumed. Otherwise, the depth bound is 
increased and a new SLD tree is built up to the new depth bound. This process 
is iterated until the difference P  (Ku)  - P  (Kz)  becomes smaller than the error 
bound. 
Instead of a depth bound, ProbLog1 can use abound on the probability 
of the proof: when the probability of the explanation associated with a proof 
drops below a threshold, the proof is stopped. The threshold is reduced in the 
following iterations by multiplying it with a constant smaller than one. 
1 0.1.2  k-best  
The second approach for approximate inference in ProbLog1 uses a fixed 
number of proofs to obtain a lower bound of the probability of the query 
[Kimmig et al., 2008, 201la]. Given an integer k,  the best k  proofs are found, 
corresponding to the set ofbest k  explanations Kk,  and the probability of Kk  
is used as an estimate of the probability of the query. 
Best is here intended in terms of probability: an explanation is better than 
another if its probability is higher. 
Example  93 (Path- ProbLog- k-best). Consider  the  program  of Example  92  
with  the  query  path( a,  d).  This  query  has  Jour  explanations  that  are  listed  
below  tagether  with  their  probabilities:  
t~;l = {ac, cd} 
P(t~;I) = 0.72 
t~;2 =  {ab,  bc,  cd} 
P(t~;I) =  0.378 
t~;3 =  {ac,  ce,  ef,Jd}  
P(t~;I) =  0.32 
t~;4 = {ab,  bc,  ce,  ef,  fd}  P( t~;l) = 0.168 
If  k  =  1, ProbLogl  considers  only  the  best  proof  and  P(KI)  =  0.72. 
For  k  =  2, ProbLogl  takes  into  account  the  best  two  explanations,  K2 =  

258 
Approximate  lnference  
{t~;I, t~;2}· By  making  thempairwise  incompatible,  we  get  
K~ = {t~;I, {ab,  bc,  cd,  --.ac}} 
and  P(K~) = 0.72 +  0.378 · 0.2 = 0.7956. For  k  = 3, K3 = {t~;l, t~;2, t~;3} 
and  P(K3)  = 0.8276. For  k  = 4, K4 = 
{t~;l, ... , t~;4} and  P(K4)  
P(path(a,  d))  = 0.83096, the  samefor  k  >  4. 
To perform k-best inference, ProbLog uses a branch-and-bound approach: the 
current best k  explanations are kept and, when the probability of a derivation 
drops below the probability of the k-th best explanation, the derivation is cut. 
When a new explanation is found, it is inserted in the list of k-best explana­
tions in order of probability, possibly removing the last one if the list already 
contains k  explanations. 
The algorithm returns a lower bound on the probability of the query, the 
larger the k  the better the bound. 
1 0.1.3  Monte  Carlo  
The Monte Carlo approach for approximate inference is based on the follow­
ing procedure, to be repeated until convergence 
1.  Sampie a world, by sampling each ground probabilistic fact in turn. 
2.  Check whether the query is true in the world. 
3.  Compute the probability ß of the query as the fraction of samples where 
the query is true. 
Convergence is reached when the size ofthe confidence interval ofß drops be­
low a user-defined threshold 5.  In order to compute the confidence interval of 
ß,  ProbLogl uses the centrallimit theorem to approximate the binomial dis­
tribution with a normal distribution. Then the binomial proportion confidence 
interval is calculated as 
ß ±  zl-a/2-Jß  (1 n- ß)  
where n  is the number of samples and z1-a;2 is the 1 - a/2 percentile of 
a standardnormal distribution with a = 0.05 usually. If the width of the 
interval is below the user-defined threshold 5,  ProbLog I stops and returns ß.  
This estimate of the confidence interval is good for a sample size larger 
than 30 and if ß  is not too close to 0 or 1. The normal approximation fails 
totally when the sample proportion is exactly zero or exactly one. 

10.1  ProbLogl  259 
The above approach for generating samples, however, is not efficient on 
large programs, as proofs are often short while the generation of a world 
requires sampling many probabilistic facts. So ProbLog1 generates samples 
lazily by sampling probabilistic facts only when required by a proof. In fact, 
it is not necessary to sample facts not needed by a proof, as any value for 
them would do. 
ProbLog1 performs a so-called source-to-source  transformation  ofthe pro­
gram 
where 
probabilistic 
facts 
are 
transformed 
using 
the 
t  e rm_  expan s  i  on mechanism of Prolog. For example, facts of the form 
0.8  ::  edge(a ,  c). 
0.7  ::  edge(a ,  b).  
are transformed into 
edge(A ,  B)  +-- problog_edge(ID , A , B ,  LogProb) ,   
grounding_id(edge(A , B) , ID , GroundiD) ,   
add_to_proof(GroundiD, LogProb) .   
problog_edge(O , a,  c, - 0.09691). 
problog_edge(1 , a, b,  - 0.15490). 
where problog_edge  is a new predicate for the intemal representation of facts 
for the predicate edge/2,  grounding_id/ 3  is used in the case in which the 
probabilistic facts are not ground for obtaining a different identifier for each 
grounding, and add_to_proof / 2 adds the fact to the current proof, stored in 
a global storage area. This approach is shared by all inference algorithms of 
ProbLogl. 
The computation of ß  is usually done after taking a user defined small 
number of samples n  instead of after every sample, see Algorithm 10. 
The algorithm converges because the Samples  variables is always increas­
ing, and thus the condition 2z1-a;2 
;!;~~f~s <  5 in line 13 of Algorithm 10 
will eventually become true, unless the query has probability 0 or 1. 
The function SAMPLE(q) is implemented by asking the query over the 
transformed program. ProbLog1 uses an array with an element for each ground 
probabilistic fact that stores one of three values: sampled true, sampled false, 
or not yet sampled. When a literal matehing a probabilistic fact is called, 
ProbLogl first checks whether the fact was already sampled by looking at 
the array. If it wasn't sampled, ProbLogl samples it and stores the result in 
the array. Probabilistic facts that are non-ground in the program are treated 
differently: samples for groundings of these facts are stored in the intemal 
database of the Prolog interpreter (YAP in the ProbLog1 case) and the sam­
pled value is retrieved when they are called. If no sample has been taken for 

260 
Approximate  lnference  
Algorithm  10  Function MONTECARLO: Monte Carlo algorithm of 
ProbLog I. 
1: function MONTECARLO(P, q,  n,  0)  
2: 
transform P  
3: 
Samples  ~ 0 
4: 
TrueSamples  ~ 0 
5: 
repeat 
6: 
for i  =  1 ~ n  do 
7: 
Samples  ~ Samples  +  1 
8: 
if SAMPLE(q) succeeds then 
9: 
TrueSamples  ~ TrueSamples  +  1 
10: 
end if 
11:  
12: 
end for 
ß +--- TrueSamples 
Samp~lc::e.::_s__  
13: 
"l 2 
ß(l-ß) 
s: 
t 
UD I 
Zl-<>/2 
Samples  <  u  
14: 
return ß  
15: end function 
a grounding, a sample is taken and recorded in the database. No position in 
the array is reserved for them since their grounding is not known at the start. 
Approximate inference by sampling is also available in the ProbLog2 
system. 
10.2  MCINTYRE  
MCINTYRE (Monte Carlo INference wiTh Yap REcord) [Riguzzi, 2013] 
applies the Monte Carlo approach of ProbLogl to LPADs using the YAP 
internal database for storing all samples and using tabling for speeding up 
inference. 
MCINTYRE first transforms the program and then queries the transformed 
program. The disjunctive clause 
Ci=  hil  : rril V ... V hin:  IIin; : -bil,  ... 'bim;,  
where the parameters sum to 1, is transformed into the set of clauses 
MC(Ci) =  {MC(Ci, 1), ... , MC(Ci,  ni)}: 

10.2  MCINTYRE  261 
MC(Ci,  1)  =  
hil  : -bil,  ...  ,  bimi'  
sarnple_head(ParList,  i, VC,  NH),  NH  = 1.  
MC(Ci,  ni)  =  hini  : -bil,  ...  ,  bimi'  
sarnple_head(ParList,  i, VC,  NH),  NH  = ni·  
where VC  is a list containing each variable appearing in Ci  and ParList  is 
[IIil, ... , ITini].  If the parameters do not sum up to 1, the last clause ( the one 
for null)  is omitted. MCINTYRE creates a clause for each head and samples 
a head index at the end of the body with sample_head/ 4. If  this index 
coincides with the head index, the derivation succeeds; otherwise, it fails. 
Thus, failure can occur either because one of the body literals fails or because 
the current clause is not part of the sample. 
Example  94  (Epidemie - LPAD). The  following  LPAD  models  the  develop­
ment  of an  epidemic  or a  pandernie  and  is  similar  to  the  ProbLog  program  of  
Example  75:  
C1 = epidernic  : 0.6; pandernie  : 0.3 +-- flu(X),  cold.  
c2  = cold:  0.7. 
c3 
flu(david). 
c4 
flu(robert).  
Clause  C1 has  two  groundings,  both  with  three  atoms  in  the  head,  while  
clause  c2  has  a  single  grounding  with  two  atoms  in  the  head,  so  overall  
there  are  3 x 3 x 2 = 18 worlds.  The query epidernic is true in Jive  of them  
and  its  probability  is  
P(epidernic)  = 0.6 · 0.6 · 0.7 + 0.6 · 0.3 · 0.7 + 0.6 · 0.1 · 0.7+ 
0.3. 0.6. 0.7 + 0.1. 0.6. 0.7 
0.588 
Clause  C1 is  transformedas  
MC(C1,  1)  = epidernic:  - flu(X),  cold,  
sarnple_head([0.6,0.3,0.1],  1, [X],NH),NH  =  1.  
MC(C1,  2) =  
pandernie:  - flu(X),  cold,  
sarnple_head([0.6,0.3,0.1],  1, [X],NH),NH  =  2. 
The predicate sample_head/ 4 samples an index from the head of a clause 
and uses the built-in YAP predicates recorded/ 3 and recorda/ 3 for, 
respectively, retrieving or adding an entry to the internal database. 
Since sample_head/ 4 is at the end of the body and since we assume 
the program to be range restricted, all the variables of the clause have been 
grounded when sample_head/ 4 is called. 

262 
Approximate  Inference  
Ifthe rule instantiation was already sampled, sample_ head/ 4 retrieves 
the head index with recorded/ 3; otherwise, it samples a head index with 
samp l e/2: 
sample_head (_ ParList , R, VC , NH ) :­
recorded (exp , (R, VC , NH ), _ ), !  .  
sample_head (ParList , R, VC , NH ) :­
sample (ParList , NH ), 
recorda (exp , (R, VC , NH ), _ ). 
sample (ParList , Headid) 
random (Prob ), 
sample (ParList , 0 , 0 , Prob , Headid). 
sample ([ HeadProb iTail ], Index , Prev, Prob , 
Headid )  
Succ is Index +  1 ,  
Next is Prev +  HeadProb ,  
(Prob =< Next ->  
Headid =  Index  
sample (Tail , Succ , Next , Prob , Headid )  
)  
Tabling can be effectively used to avoid re-sampling the same atom. To take 
a sample from the program, MCINTYRE uses the following predicate 
sample (Goal ) :­
abo l ish_all_ tables , 
eraseall (exp ), 
call (Goal ). 
For example, if the query is epidemic,  resolution matches the goal with the 
head of clause MC(C1 , 1). Suppose flu(X)  succeeds with X jdavid  and 
cold  succeeds as weil. Then 
sample_head([0.6,  0.3, 0.1] , 1, [david],  N  H)  
is called. Since clause 1 with X  replaced by david  was not yet sampled, a 
number between 1 and 3 is sampled according to the distribution [0.6, 0.3, 0.1] 
and stored in N  H.  If N  H  =  1, the derivation succeeds and the goal is true in 

10.3  Approximate lnferencefor Queries with an InfiniteNumber  of Explanations  263 
the sample, if N H  = 2 or N H  = 3, then the derivation fails and backtracking 
is performed. This involves finding the solution X  jrobert  for flu(X).  cold  
was sampled as true before, so it succeeds again. The 
sample_head([0.6 ,  0.3, 0.1], 1, [robert],  NH)  
is called to take another sample. 
Differently from ProbLogl, MCINTYRE takes into account the validity 
of the binomial proportion confidence interval. The normal approximation 
is good for a sample size larger than 30 and if ß  is not too close to 0 or 1. 
Empirically, it has been observed that the normal approximation works well 
as long as Sample  · ß  > 5 and Sample  · (1 - ß)  > 5 [Ryan, 2007]. Thus, 
MCINTYRE changes the condition in line 13 of Algorithm 10 to 
1 
2zl-a/2 ß( -/) < o 1\  Sampies  · ß  > 51\  Sampies  · (1 - ß)  >  5 
amp  es  
Recent versions of MCINTYRE for SWI-prolog (included in the cpl i nt 
suite) use dynamic clauses for storing samples, as in SWI-prolog these are 
faster. sample_head/ 4  is then defined as: 
sampl e_ head (R, VC , _ HeadList , N) :­
sampled (R, VC , N),!. 
sampl e_ head (R, VC , HeadList , N) :­
sample (HeadList , N), 
assertz (sampl ed (R, VC , N)). 
Monte Carlo sampling is attractive for the simplicity of its implementation 
and because the estimate can be improved as more time is available, making 
it an anytime  algorithm.  
10.3  Approximate  lnference  for  Queries  with  an  Infinite  Number  
of  Explanations  
Monte Carlo inference can also be used for programs with function symbols, 
in which goals may have an infinite number of possibly infinite explanations 
and exact inference may loop. In fact, a sample of a query corresponds natu­
rally to an explanation. The probability of taking that sample is the same as 
the probability of the corresponding explanation. The risk is that of incurring 

264 
Approximate  lnference  
in an infinite explanation. But infinite explanations have probability zero, so 
the probability that the computation goes down such a path and does not 
terminate is zero as well. As a consequence, Monte Carlo inference can be 
used on programs with an infinite number of possibly infinite explanations. 
Similarly, iterative deepening can also avoid infinite loops as the proof 
tree is built only up to a certain point. If the bound is on the depth, com­
putation will eventually stop because the depth bound will be exceeded. If 
the bound is on the probability, it will eventually be exceeded as well, as the 
probability of an explanation goes to zero as more choices are added. 
For an example of Monte Carlo inference on a program with an infinite 
set of explanations, see Section 15.11. 
10.4  Conditional  Approximate  lnference  
Monte Carlo inference also provides smart algorithms for computing the 
probability of a query given evidence (COND task): rejection sampling or 
Metropolis-Hastings Markov chain monte carlo (MCMC). 
In  rejection sampling [Von Neumann, 1951], the evidence is first queried 
and, if it is successful, the query is asked in the same sample; otherwise, the 
sample is discarded. Rejection sampling is available bothin cplint andin 
ProbLog2. 
In  Metropolis-hastings MCMC, a Markov chain is built by taking an ini­
tial sample and by generating successor samples, see [Koller and Friedman, 
2009] for a description of the general algorithm. 
Nampally and Ramakrishnan [2014] developed a version ofMCMC spe­
cific to PLP. In their algorithm, the initial sample is built by randomly sam­
pling choices so that the evidence is true. A successor sample is obtained 
by deleting a fixed number (lag)  of sampled probabilistic choices. Then the 
evidence is queried again by sampling starting with the undeleted choices. If 
the evidence succeeds, the query is then also asked by sampling. The query 
sample is accepted with a probability of 
Ni-l} 
min {  1, Ni  
where Ni-l  is the number of choices sampled in the previous sample and 
Ni  is the number of choices sampled in the current sample. The number of 
successes of the query is increased by 1 if the query succeeded in the last 
accepted sample. The final probability is given by the number of successes 

10.4  Conditional  Approximate  lnference  265 
over the total number of samples. Nampally and Ramakrishnan [2014] prove 
that this is a valid Metropolis-hastings MCMC algorithm if lag  is equal to 1. 
Metropolis-hastings MCMC is also implemented in cplint [Alberti 
et al., 2017]. Since the proof of the validity of the algorithm in [Nampally 
and Ramakrishnan, 2014] also holds when forgetting more than one sampled 
choice, lag  is user-defined in cplint. 
Algorithm 11 shows the procedure. Function INITIALSAMPLE returns a 
composite choice containing the choices sampled for proving the evidence. 
Function SAMPLE takes a goal and a composite choice as input and samples 
the goal retuming a pair formed by the result of sampling (true or false) and 
the set of sampled choices extending the input composite choice. Function 
RESAMPLE(h;, lag)  deletes lag  choices from /\;.In [Nampally and Ramakr­
ishnan, 2014], lag  is always 1. Function ACCEPT(h;i-1, 1\;i) decides whether 
to accept sample 1\;i·  
Algorithm  11  Function MCMC: Metropolis-hastings MCMC algorithm. 
1: function MCMC(P, q,  Samples,  lag)  
2: 
TrueSamples  +--- 0 
3: 
Ko  +--- INITIALSAMPLE(e) 
4: 
(rq,  K) +-SAMPLE(q, Ko) 
5: 
for i  =  1 ---+  Samples  do 
1 
6: 
K 
+--- RESAMPLE(K, lag)  
7: 
(re,  Ke) +-SAMPLE(e, K
1
) 
8: 
if re=true then 
9: 
(r~, Kq)  +-SAMPLE(q, Ke)  
10: 
if ACCEPT(K, Kq)  then 
11: 
K  +--- Kq  
12: 
rq  +--- rq 
I  
13: 
endif 
14: 
endif 
15: 
if r q=true then 
16: 
TrueSamples  +--- TrueSamples  +  1  
17: 
endif 
18: 
end for 
19: 
ß  +--- TrueSamples  
Samples  
20: 
returnß 
21: end function 
Function INITIALSAMPLE builds the initial sample with a meta-interpreter 
(see Section 1.3) that starts with the goal and randomizes the order in which 
clauses are used for resolution during the search so that the initial sample 
is unbiased. This is achieved by collecting all the clauses that match a sub­

266  Approximate  lnference  
goal and trying them in random order. Then the goal is queried using regular 
sampling. 
10.5  k-optimal  
In k-best, the set of proofs can be highly redundant with respect to each other. 
k-optimal [Renkens et al., 2012] improves on k-best for definite clauses by 
trying to find the set of k  explanations K  = {t~; 1 , ...  ,  t~;k} of the query q  that 
lead to the largest probability P  ( q),  in order to obtain the best possible lower 
bound given the limit on the number of explanations. 
k-optimal follows a greedy procedure shown in Algorithm 12. The opti­
mization in line 4 is performed with an algorithm similar to 1-best: a branch­
and-bound search is performed where, instead of evaluating the current partial 
explanation t~; using its probability, the value P  (K  u  {  t~;}) - P  (K)  is used. 
Algorithm  12  Function K-OPTIMAL: k-optimal algorithm. 
1: function K-OPTIMAL(tPr, <Pq,  maxTime)  
2: 
K  +--- 0 
3: 
fori=l~kdo 
4: 
K  +---Ku argmax" is an explanation P(K  u {!~:}) 
5: 
end for 
6: 
return K  
7: end function 
In order to efficiently compute P  (K  u  {  t~;}), k-optimal uses compilation 
to BDDs. Instead of building the BDD for K  u  {  t~;} from scratch, k-optimal 
uses a smarter approach. Let dnf  represent the DNF formula for K  and let 
fi  1\  ...  1\  fn  represent the Boolean formula fort~;, where the fis are Boolean 
variables for ground probabilistic facts. Then 
P(JI  1\  ... 1\  fn  V dnf)  =  P(JI  1\  . · · 1\  fn)  +  
P( ----.JI  1\  dnf)  +  
P(JI  1\  ----.f2 1\  dnf)  +  ...  
P(JI  1\  ••• 1\  fn-l  1\  ----.Jn  1\  dnf)  
Since probabilistic facts are independent, P(JI  1\  .•• 1\  fn)  can be easily 
computed as P(JI)  · ... · P(fn).  The other terms become 
P(JI  1\  ...  1\  fi-l  1\  ----.Ji  1\  dnf)  =  P(JI)  · ... · PUi-l)  · (1- P(fi))  · 
P(dnfi!I  1\  ...  1\  fi-l  1\  ----.fi)  

10.5  k-optimal  267 
The factor P  (dnf I !I  A . . . A fi-1 A -.., fi)  can be computed cheaply if the 
BDD for dnf  is available: we can apply function PROB  of Algorithm 6 by 
assuming that P(fj)  = 1 for the conditional facts j  <  i  and P(fi)  = 0. 
So, at the beginning of a search iteration, K  is compiled to a BDD and 
P  (Ku {'""}) is computed for each node of the SLD tree for the query q,  where 
'"" is the composite choice corresponding to the node, representing a possibly 
partial explanation. If '"" has n  elements, n  conditional probabilities must be 
computed. 
However, when the probability P(JI  A ... A fn  A fn+l  v dnf)  for the 
partial proof !I  A ... A fn  A fn+l  must be computed, the probability P(JI  A 
... A f n  v dnf)  was computed in the parent of the current node. Since 
P(JI  1\  .•• 1\  fn  1\  fn+l  V dnf)  =  P(JI  1\  ... 1\  fn  V dnf)  +  
P(JI  1\  · · · 1\  fn  1\  -.fn+l  1\  dnf)  
then only P(JI  A ... A fn  A -.fn+l  A dnf)  must be computed. 
In practice, Renkenset al. [2012] observe that computing P(K  u  {'""})  
for each node of the SLD tree is still too costly because partial proofs have 
alsotobe considered and these may lead to dead ends. They found experi­
mentally that using the bound P('"")  of k-best for pruning incomplete proofs 
and computing P  (K  u  {  '""})  only when a complete proof is found provides 
better performance. This works because P('"")  is an upper bound on P(K  u  
{'""})- P(K),  so cutting a branch because P('"")  has become smaller than 
P  (K  u  {  ,.",})  - P  (K)  for the best explanation ,.", found so far does not prune 
good solutions. 
However, this approach performs less pruning, as it is based on an upper 
bound, and the computation of P  (K  u  {  '""})  at the parent of an SLD node of 
a complete proof is no Ionger available. 
k-optimal, as k-best, still suffers from the problernthat k  is set beforehand 
and fixed, so it may happen that, of the k  proofs, many provide only very 
small contributions and a lower value of k  could have been used. k-B-optimal 
puts a threshold 0  Oll the added proof probability: k-optimal is stopped before 
k  proofs are found if no more proof has an added probability P  (K  u  {  '""})  ­
p  ( K)  larger than e.  This is achieved by Setting the bound to e at the beginning 
of each iteration of k-optimal. 
Renkens et al. [2012] prove that the k-optimal optimization problern is 
NP-hard. They also show that the proposed greedy algorithm achieves an 
approximation which is not worse than 1 -
~ times the probability of the 
optimal solution. 

268 
Approximate  lnference  
The experiments in [Renkens et al., 2012] performed on biological graphs 
show that k-best is about an order of magnitude faster that k-optimal but that 
k-optimal obtains better bounds especially when k  is low compared to the 
number of available proofs. 
10.6  Explanation-based  Approximate  Weighted  Model  Counting  
Renkens et al. [2014] solve EVID approximately by computing lower and 
upper bounds of the probability of evidence from ProbLog programs. The ap­
proach is based on computing explanations for evidence e  one by one, where 
an explanation here is represented as a conjunction of Boolean variables 
representing ground probabilistic facts. So an explanation 
~ = {(JI,O,kl),  ...  ,(fn,~,kn)} 
for ground probabilistic facts {!I, ... , fn}  is represented as 
expK,  =  I\  AJ  1\  I\  ---.).,!  
(!,0,1)EK-
(!,0,0)EK-
Where >.. f  is the Boolean variable associated with fact f.  
An explanation exp  is suchthat c/Jr  1\  exp  I=  c/Je,  where c/Jr  and c/Je  are the 
propositional formulas representing the rules and the evidence, respectively, 
computed as in ProbLog2 (see Section 8.7). 
Renkenset al. [2014] show that, given any set of explanations 
{exp1, ... , expm},  
it holds that 
WMCv(c/J  1\  (exp1  v  ...  v  expm))  =  WMCE(expl  v  ...  v  expm)  
where cjJ  =  c/Jr  1\  c/Je  and V  is the set of allvariables of the programs, i.e., those 
for probabilistic facts and those of the head of clauses in the grounding of the 
pro gram, while E  is the set of variables of the program for probabilistic facts 
only. 
Given two weighted formulas 'ljJ  and e over the same set of variables V,  
we have that WMCv('l/J):?  WMCv('l/J  1\  e)  as the models of'!/J 1\  e are a 
subset of those of 'ljJ.  So WM C v  (cjJ  1\  ( exp1  v ... v expm))  is a lower bound 
for WMCv(c/J)  and tends toward it as the number explanations increases, as 
each explanation encodes a set of worlds. Since the number of explanations 
is finite, when considering all explanations, the two counts will be equal. 

10.6  Explanation-based  Approximate  Weighted  Model  Counting  269 
Moreover, we can compute WMCv(q)  A  (exp1  v  ...  v  expm))  more 
quickly by computing WM CE (exp1  v . . . v expm)  because it has fewer 
variables. This leads to Algorithm 13 for computing a lower bound of the ev­
idence, where function NEXTEXPL retums a new explanation. The algorithm 
is anytime, we can stop it at any time still obtaining a lower bound of P  ( e). 
Algorithm  13 Function AWMC: Approximate WMC for computing a lower 
bound of P(e).  
1: function AWMCCc/>r, cf>e,  maxTime)  
2: 
'ljJ  +-- 0 
3: 
while time  <  maxTime  do 
4: 
exp  +-NEXTEXPL(c/>r 1\  cf>e) 
5: 
'ljJ  +-- 'ljJ  v  exp  
6: 
end while 
7: 
return WMCv(,p)('l/J)  
8: end function 
NEXTEXPL looks for the next best explanation, i.e., the one with maximal 
probability (or WMC). This is done by solving a weighted MAX-SAT prob­
lern: given a CNF formula with non-negative weights assigned to clauses, find 
assignments for the variables that minimize the sum of the weights of the vi­
olated clauses. An appropriate weighted CNF formula built over an extended 
set of variables is passed to a weighted MAX-SAT solver that retums an 
assignment for all the variables. An explanation is built from the assignment. 
To ensure that the same explanation is not found every time, a new clause 
excluding it is added to the CNF formula for every found explanation. 
An upper bound of P  (e) can be computed by observing that, for ProbLog, 
WMC(q)r)  = 1, because the variables for probabilistic facts can take any 
combination of values, the weights for their literals sum to 1 (represent a 
probability distribution) and the weight for derived literals (those of atoms 
appearing in the head of clauses) are all1, so they don't influence the weight 
of worlds. Therefore 
WMC(q)r  A q)e) = WMC(q)r)- WMC(q)r  A -.q)e) = 1- WMC(q)r  A -.q)e) 
As a consequence, ifwe compute a lowerbound on WMC(q)r  A -.q)e). we can 
derive an upperbound on WMC(q)r  A q)e). The lowerbound on WMC(q)r  A 
-.q)e) is computed as for WMC(q)r  A  q)e), by looking for explanations for 
q)r  1\  --, q)e · 
This leads to Algorithm 14 that, at each iteration, updates the bound that 
at the previous iteration had the largest change in value. The algorithm is 

270 
Approximate  lnference  
anytime: at any time point, low  and up  are the lower and upper bounds of 
P  ( e), respectively. 
Algorithm  14  Function AWMC: Approximate WMC for computing lower 
and upperbounds of P(e).  
1: function AWMCCc/>r, cf>e,  maxTime)  
2: 
improveTop  ~ 0.5 
3: 
improveBot  ~ 0.5 
4: 
top~ 1 
5: 
bot~ 0 
6: 
up  ~ 1.0 
7: 
low  ~ 0.0 
8: 
while time  <  maxTime  do 
9: 
if improveTop  >  improveBot  then 
10: 
exp  ~NEXTEXPL(c/>r 1\  -,cf>e)  
11: 
next  ~wMC(top A -,exp)  
12:  
improveTop  ~ up- next  
13: 
top~ top  1\  -,exp  
14: 
up  ~ next  
15: 
eise 
16: 
exp  ~NEXTEXPL(c/>r 1\  cf>e)  
17:  
next  ~WMC(bot v  exp)  
18: 
improveBot  ~ next  -low  
19: 
bot~ bot  v  exp  
20: 
low  ~ next  
21:  
end if 
22: 
end while 
23: 
return [low,  up]  
24: end function 
10.7  Approximate  lnference  with  Tp-compilation  
Tp  compilation [Vlasselaer et al., 2015, 2016] discussed in Section 8.8 can be 
used to perform approximate CONDATOMS inference by computing a lower 
and an upper bound of the probabilities of query atoms, similarly to iterative 
deepening of Section 10.1.1. 
Vlasselaer et al. [2016] show that, for each iteration i  of application of 
Tcp,  if )..~ is the formula associated with atom a  in the result of Tcp  j  i, 
then WMC(.A~) is a lower bound on P(a).  So Tp  compilation is an anytime 
algorithm for approximate inference: at any time, the algorithm provides a 
lower bound of the probability of each atom. 

10.7  Approximate  lnference  with  Tp-compilation  271 
Moreover, Tcp  is applied using the one atom at a time approach where 
the atom to evaluate is selected using a heuristic that is 
• proportional to the increase in the probability of the atom; 
• inversely proportional to the complexity increase  of the SDD for the 
atom; 
• proportional to the importance of the atom for the query, computed as 
the inverse of the minimal depth of the atom in the SLD trees for each 
of the queries of interest. 
An upper bound for definite programs is instead computed by selecting a 
subset F'  of the facts F  of the program and by assigning them the value 
true, which is achieved by conjoining each Aa  with Ap  = 1\fEF'  A f.  If we 
then compute the fixpoint, we obtain an upper bound: WMC(A~) :? P(a).  
Moreover, conjoining with Ap  simplifies formulas and so also compilation. 
The subset F'  is selected by considering the minimal depth of each fact in 
the SLD trees for each of the queries and by inserting into F'  only the facts 
with a minimal depth smaller than a constant d.  This is done to make sure 
that the query depends on at least one probabilistic fact and the upper bound 
is smaller than 1. 
While the lower bound is also valid for normal programs, the upper bound 
can be used only for definite programs. 


11   
Non-standard  lnference  
This chapter discusses inference problems for languages that are related to 
PLP, such as Possibilistic Logic Programming, or are generalizations ofPLP, 
such as Algebraic ProbLog. Moreover, the chapter illustrates how decision­
theoretic problems can be solved by exploiting PLP techniques. 
11.1  Possibilistic  Logic  Programming  
Possibilistic Logic [Dubois et al., 1994] is a logic ofuncertainty for reasoning 
under incomplete evidence. In this logic, the degree of necessity  of a formula 
expresses to what extent the available evidence entails the truth of the for­
mula and the degree of possibility  expresses to what extent the truth of the 
formula is not incompatible with the available evidence. 
Given a formula cp,  we indicate with II(cp)  the degree of possibility as­
signed by possibility  measure  II to it, and with N  (cp)  the degree of necessity 
assigned by necessity  measure  N  to it. Possibility and necessity measures 
must satisfy the constraint N( cp)  =  1 - II( --.cp)  for all formulas cp.  
A possibilistic  clause  is a first-order logic clause C  associated with a num­
ber that is a lower bound on its necessity or possibility degree. We consider 
here the possibilistic logic CPLl [Dubois et al., 1991] in which only lower 
bounds on necessity are considered. Thus, ( C,  a)  means that N  (C)  ):  a.  A 
possibilistic  theory  is a set of possibilistic clauses. 
A possibility measure satisfies  a  possibilistic  clause  (C,  a)  if N  (C)  ):  a  
or, equivalently, if II(--.C) ~ 1 - a.  A possibility measure satisjies  a  possi­
bilistic  theory  if it satisfies every clause in it. A possibilistic clause ( C,  a)  is a 
consequence  of a possibilistic theory F  ifevery possibility measure satisfying 
F  also satisfies ( C,  a).  
Inference rules of classicallogic have been extended to rules in possi­
bilistic logic. Here we report two sound inference rules [Dubois and Prade, 
2004]: 
273 

274 
Non-standard  Inference  
•   (<jJ,a),  ('lj;,ß)  1- (R(</J,'lj;),min(a,ß)) where R(<jJ,'lj;)  is the resolvent 
of <P  and 'lj;  ( extension of resolution); 
•   (<P,  a),  (<P,  ß)  1- (</J, max(a, ß))  (weight fusion). 
Dubois et al. [1991] proposed a Possibilistic Logic Programming language. 
A program in such a language is a set of formulas of the form ( C,  a)  where 
C  is a definite clause 
h  ~ b1, ... , bn.  
and a  is a possibility or necessity degree. We consider the subset of this 
language that is included in CPLl, i.e., a  is a real number in (0,1] that 
is a lower bound on the necessity degree of C.  The problern of inference 
in this language consists in computing the maximum value of a  such that 
N  ( q)  ?: a  holds for a query q.  The above inference rules are complete for this 
language. 
Example  95 (Possibilistic logic program). The  following  possibilistic  pro­
gram  computes  the  least  unsure  path  in  a  graph,  i.e.,  the  path  with  maximal  
weight,  the  weight  of  a  path  being  the  weight  of  its  weakest  edge  [Dubois  
et  al.,  1991].  
(path(X,  X),  
1)  
(path(X,  Y)  ~ path(X,  Z),  edge(Z,  Y),  
1)  
(edge(a,  b),  
0.3) 
We restricted our discussion here to positive programs. However, approaches 
for normal Possibilistic Logic programs have been proposed in [Nieves et al., 
2007; Nicolas et al., 2006; Osorio and Nieves, 2009], and [Bauters et al., 
2010]. 
PITA(IND,IND), see Section 8.10, can also be used to perform inference 
in Possibilistic Logic Programming where a program is composed only of 
clauses of the form h  : a  ~ b1, ... , bn  which are interpreted as possibilistic 
clauses ofthe form (h  ~ b1, ... , bn,  a).  
The PITA transformation ofPITA(IND,IND) can be used unchanged pro­
vided that the support predicates are defined as 

11.2  Decision-theoretic  ProbLog  275 
or(A,  B,  C)  ~Cis max(A,  B).  
and(A,  B,  C)  ~Cis min(A,  B).  
zero(O.O).  
one(l.O).  
ret_prob(P,  P).  
We obtain in this way PITA(POSS). 
Cornputing the possibility is rnuch easier than cornputing the general 
probability, which rnust solve the disjoint surn problern to obtain answers. 
11.2  Decision-theoretic  Problog  
Decision-Theoretic ProbLog [Van den Broeck et al., 2010] or DTPROBLOG 
tackles decision  problems:  the selection of actions frorn a set of alternatives 
so that a utility function is rnaxirnized. In  other words, the problern is to 
choose the actions that bring the rnost expected reward ( or the least expected 
cost) to the acting agent. DTPROBLOG supports decision problerns where the 
dornain is described using ProbLog so that probabilistic effects of actions can 
be taken into account. 
DTPROBLOG extends ProbLog by adding decision facts D  and utility 
facts U.  Decisionfacts  rnodel decision variables, i.e., variables on which we 
can act by setting their truth value. They are represented as 
? :: d.  
where d  is an atorn, possibly non-ground. 
A utility  fact  is of the form 
u~r 
where u  is a literal and r  E  lR is a reward or utility for achieving u.  It  
rnay be interpreted as a query that, when succeeding, gives a reward of r.  
u  rnay be non-ground; in this case, the reward is given once if any grounding 
succeeds. 
A strategy  fJ  is a function D  ~ [0, 1] that assigns a decision fact to 
a probability. All grounding of the same decision fact are assigned the sarne 
probability. We call ~ the set of all possible strategies. We indicate with rJ('D)  
the set of probabilistic facts obtained by assigning probability rJ( d)  to each 
decision fact? :: d.  of'D, i.e., rJ('D)  =  {rJ(d)  ::  dl? :: d  E  D}. A deterministic 
strategy is a strategy that only assigns probabilities 0 and 1 to decision facts. 
It is thus equivalent to a Boolean assignrnents to the decision atorns. 

276 
Non-standard  Inference  
Given a DTPROBLOG program VT  = BJC  u  V  program and a strategy (],  
the probability to a query q  is the probability Pu(q)  assigned by the ProbLog 
program BJC  u  dV).  
The utility  of a logic program P  given a set of utility facts U  is defined as 
Util(P) = 
r 
2:: 
u->rEU,Pi=u  
The expected  utility  of a ProbLog program P  given a set of utility facts U  can 
thus be defined as 
Util(P) = 2:: P(w)  
2:: 
r.  
wEWp  
u->rEU,wi=u  
By exchanging the sum, we get 
Util(P) = 2:: 
2:: 
r  · P(w)  = 2:: rP(u)  
u->rEU  wEWp,wl=u  
u->rEU  
The expected  utility  of a DTPROBLOG program VT  =  BJC  u  V  u  U  and a 
strategy (]  is the expected utility of BJC  u  (](V)  given U  
Util((J(VT)) = Util(BJC u  dV)). 
If we call U til ( u,  (]  (VT))  =  r  · P  (u)  the expected utility due to atom u  for 
a strategy (],  we have 
Util(dVT)) = 2:: Util(u, dVT)).  
u->rEU  
Example  96 (Remaining dry [Van den Broeck et al., 2010]). Consider  the  
problern  of remaining  dry  even  when  the  weather  is  unpredictable.  The  pos­
sible  actions  are  wearing  a  raincoat  and  carrying  an  umbrella:  
? :: umbrella.   
? :: raincoat.   
0.3 :: rainy.  
0.5 :: windy.   
broken_umbrella  ~ umbrella,  rainy,  windy.   
dry  ~ rainy,  umbrella,  "'broken_umbrella.   
dry  ~ rainy,  raincoat.   
dry  ~"'rainy.  
Utility  facts  associate  real  numbers  to  atoms   
umbrella  ~ -2 
dry~ 60  
raincoat  ~ -20 broken_umbrella  ~ -40  

11.2  Decision-theoretic  ProbLog  277 
The inference  problern  in DTPROBLOG is to compute Util(o"('DT)) for a 
particular strategy (}.  
The decision  problern  instead consists of finding the optimal strategy, 
i.e., the one that provides the maximum expected utility. Formally, it means 
solving 
arg max Util((J(DT)). 
(J  
Since all the decisions are independent, we can consider only deterministic 
strategies. In fact, if the derivative of the total utility with respect to the prob­
ability assigned to a decision variable is positive (negative), the best result 
is obtained by assigning probability 1 (0). If the derivative is 0, it does not 
matter. 
The inference problern is solved in DTPROBLOG by computing P(u)  
with probabilistic inference for all decision facts u  ~ r  in U.  DTPROBLOG 
uses compilation of the query to BDDs as ProbLogI.  
Example  97  (Continuation ofExample 96). For  the  utility fact  dry,  ProbLog1  
builds  the  BDD  of Figure  11.1.  For  the  strategy  
(}  =  {  umbrella  ~ 1, raincoat  ~ 0}, 
the  probability  of  dry  is  0.7 +  0.3 · 0.5 = 0.85, so  Util(dry,  (J(DT))  
60. 0.85 = 51. 
For  the  utility  fact  broken_umbrella,  ProbLog1  builds  the  BDD  of  
Figure  11.2.  For  the  strategy  {umbrella  ~ 1, raincoat  ~ 0}, the  proba­
bility  ofbroken_umbrella  is  0.3 · 0.5 = 0.15 and  
Util(broken_umbrella,  (J(DT))  =  -40 · 0.15 =  -6. 
Overall,  we  get  
Util((J(DT)) =51+ (-6) + ( -2) = 43. 
To solve decision problems, DTPROBLOG uses ADDs [Bahar et al., 1997] 
that are a generalization of BDDs where the leaves store a value in IR instead 
of 0 or 1. An ADD thus represents a function from Boolean variables to the 
real numbers f  :  {0, 1 }n ~IR using a form of Shannon expansion: 
f(xl,  X2, ... , Xn)  = Xl · f(l, X2, ... , Xn)  +  (1 - xl)  · f(O,  X2, ... , Xn)·  
As BDDs, ADDs can be combined with operations. We consider here scalar 
multiplication c · g  of an ADD g  with the constant c, addition f  EB g  of two 

278 
Non-standard  Inference  
Figure  11.1  BDDdry(O")  for Example 96. From [Van den Broeck et al., 
2010]. © [2010] Association for the Advancement of Artificial Intelligence. 
All rights reserved. Reprinted with permission. 
tti' 
I  
Figure  11.2  BDDbroken_umbrella(O")  for Example 96. From [Van den 
Broeck et al., 2010]. © [2010] Association for the Advancement of Artificial 
Intelligence. All rights reserved. Reprinted with permission. 

11.2  Decision-theoretic  ProbLog  279 
ADDs, 
and 
if-then-else 
ITE(b, 
g)  
where 
b  is 
a 
Boolean 
variable. 
In scalar multiplication, h  = c · g  with c E  lR  and g  : { 0,  1} n  ~ JR, the 
output h  :  {0, 1 }n ~ lR  is defined as: Vx : h(x)  = c · g(x).  
In addition, h  = f  EB g  with J,  g  : {0, 1}n ~ JR, the output h: {0, 1}n ~ 
lR  is defined as Vx : h(x)  = f(x)  +  g(x).  
The version of if-then-else of interest here, ITE(b, J,  g)  with b  E {0, 1} 
and J,  g  : {0, 1 }n ~ JR, retums h  : {0, 1 }n+l ~ lR  computed as 
if b  = 1
Vb,  x : h(b,  x) =  {  f(x) 
g(x)  ifb = 0 
The CUDD [Somenzi, 2015] package, for example, offers these operations. 
DTPROBLOG stores three functions: 
•  Pa(u),  the probability ofliteral u  as a function ofthe strategy; 
•  Util(u, o"('DT)),  the expected utility of literal u  as a function of the 
strategy; 
• Util(O"), the total expected utility as a function of the strategy. 
Since we can consider only deterministic strategies, one of them can be rep­
resented with a Boolean vector d with an entry for each decision variable di.  
Therefore, all these functions are Boolean functions and can be represented 
with the ADDs ADD(u),  ADDutil(u),  and ADDfJ11• respectively. 
Given ADDfJ11, finding the best strategy is easy: we just have to identify 
the leaf with the highest value and retum a path from the root to the leaf, 
represented as a Boolean vector d. 
To build ADD(u),  DTPROBLOG builds the BDD BDD(u)  representing 
the truth of the query u  as a function of the probabilistic and decision facts: 
given an assignment f, d  for those, BDD(u)  retums either 0 or 1. BDD(u)  
represents the probability P( ulf, d)  for all values of f,  d.  
Function Pa(u)  requires P(uld) that can be obtained from P(ulf,  d)  by 
summing out the f  variables, i.e., computing 
P(uld) = l.:  P(u,  fld) = l.:  P(ulf,  d)P(fld) = 
l.: 
f  
 
f  
P(ulf,  d)P(f) = l.:  P(ulf,  d) 
IIJ  = 
f 
l.: 
  
f  
n 
 
f~ 
IIh  l.:  IIh  ... l.:  IIfnP(ulft,  ...  ,  fn,  d)  
f1 
f2 
fn  
J,  

280 
Non-standard  Inference  
Wecanobtain ADD(u)  fromBDD(u) bytraversing BDD(u)  from theleaves 
to the root and, for each sub-BDD with root in node n,  building a correspond­
ing sub-ADD with root node m.  If n  is the 0-terminal (I-terminal), we retum 
a 0-terminal (I-terminal). If n  is associated with a probabilistic variable f,  we 
have already built the ADDs ADDz  and ADDh  for its 0- and 1-child. They 
represent P(ulf  = 0, d') and P(ulf  = 1, d'), respectively, for all values d' 
where D' is the set of Boolean decision variables of ADDz  and ADDh.  We 
must sum out variable f,  so 
P(uld') = Ilt  ·  P(ulf  = O,d') +  (1- Ilt)  ·  P(ulf  = 1,d') 
TheADD 
IT1  · ADDz  EB  (1- IT1)  · ADDh  
represents P(uld') and is the ADD we are looking for. 
If n  is associated with decision variable d  with ADDs ADDz  and ADDh  
for its 0- and 1-child, the ADD for representing P( uld,  d') is 
ITE(d, ADDh,  ADDz).  
The conversion from BDD(u)  to ADD(u)  is computed by function 
PROBABILITYDD of Algorithm 15. 
Once we have ADD(u),  ADDutil(u)  is simply given by r  · ADD(u)  if 
u  ~ r  E  U.  Finally, ADDfJ:l  = ffiu--.rEU  ADDutil(u).  This gives Algo­
rithm 15 that solves the decision problern exactly. The function EXACTSOLU­
TION initializes ADDfJ:l  to the zero function and then cycles over each utility 
fact in turn, building BDD(u), ADD(u),  and ADDutil(u).  Then ADDfJ:l  is 
updated by summing ADDutil( u)  to the current value. 
Example  98 (Continuation of Example 96). For  the  utility  fact  dry,  DT­
PROBLOG builds  ADD(dry)  and ADDutil(dry)  ofFigure  11.3.  For  the  strat­
egy  
fJ  =  {umbrella  ~ 1, raincoat  ~ 0}, 
thefigure  confirms  that  Util(dry,  rJ(DT))  = 60 · 0.85 =51. 
For  broken_umbrella,  DTPROBLOG builds  ADD(broken_umbrella)  
and  ADDutil (broken_umbrella)  of Figure  11.4.  For  the  strategy  fJ,  the  fig­
ure  confirms  that  
Util(broken_umbrella,  rJ(DT))  = -40 · 0.15 = -6. 
Figure  11.5  shows  ADDfJfl  that  confirms  that,for  strategy  fJ,  
Util(rJ(DT)) =  43. 

11.2  Decision-theoretic  ProbLog  281 
Algorithm  15  Purretion EXACTSOLUTION: solving the DTPROBLOG deci­
sion problern exactly. 
I: function EXACTSOLUTION(Vn 
2: 
Annr;:~ +- o 
3: 
forall(u~r)EUdo 
4: 
BuildBDD(u), theBDDforu 
5: 
6: 
7: 
ADD(u)  +- PROBABILITYDD(BDD(u)) 
ADDuti1(u)  +- r  ·  ADD(u)  
ADDf;I1 +- ADDf;I1 E8 ADDutil ( u)  
8: 
end for 
9: 
IO: 
let tmax  be the terminal node of ADDf;f1 with the highest utility 
let p  be a path from tmax  to the root of ADDf;f1 
II: 
return the Boolean decisions made on p  
I2: end function 
13: function PROBABILITYDD(n) 
I4: 
if n  is the I-terminal then 
I5: 
return aI-terminal 
I6: 
end if 
I7: 
if n  is the 0-terminal then 
I8: 
return a 0-terminal 
I9: 
end if 
20: 
let h  and l  be the high and low children of n  
2I: 
ADDh  +- PROBABILITYDD(h) 
22: 
ADDt  +- PROBABILITYDD(h) 
23: 
if n  represents a decision d  then 
24: 
return ITE(d, ADDh,  ADDt)  
25: 
end if 
26: 
if n  represents a fact with probability p  then 
27: 
return (p  ·  ADDh)  E8 ((1- p)  · ADDt)  
28: 
end if 
29: end function 
Moreover,  this  is  also  the  optimal  strategy.  
Algorithm 15 can be optimized by pruning ADDfJ11 to remove the por­
tians that may never lead to an optimal strategy. Derrote by max ADD and 
min ADD the maximum and minimum value, respectively, that ADD assigns 
to any combination of its variables, i.e., the maximum and minimum values 
that appear in its leaves. 
When ADDutil(u·)  is summed to ADDutil  a leaf of ADDutil  with value 
2 
tot  • 
tot  
v  before the sum belongs to [v  +  minADDutil(ui), v  +  maxADDutil(ui)]  
after the sum. Thus, if m  is max ADDfJ11 before the sum, any leaf with value 
v  suchthat v  +  maxADDutil(ui)  <  m  +  minADDutil(ui) will never lead 

--
--
§  
' ' 0  
I  
I  
I  -6  ' 
I  
I  0 ' 
I  
\  
I  
\  
I  
282 
Non-standard  Inference  
'  ' 9P 
88 
I  
\  
I  
\  
I  
\  
I  51 
I  
I  60 I  
I 42  I  
\  
I  
\  
I  
\  
I  
Figure  11.3 ADD(dry)  for Example 96. The dashed terminals indicate 
ADDutil(dry).  From [Van den Broeck et al., 2010]. © [2010] Association 
for the Advancement of Artificial Intelligence. All rights reserved. Reprinted 
with permission. 
Figure  11.4  ADD(broken_umbrella)  for Example 96. The dashed ter­
minals indicate ADDutil(broken_umbrella).  From [Van den Broeck et al., 
2010]. © [2010] Association for the Advancement of Artificial Intelligence. 
All rights reserved. Reprinted with permission. 
to an optimal strategy. All such leaves can be pruned by merging them and 
assigning them the value -oo. 
If we compute the impact of utility attribute Ui  as 
Im(ui) =  maxADDutil(ui)- minADDutil(ui)  
we can prune all the leaves of ADDfJil  that, before the sum, have a value 
below 
maxADDutil- Im(u·)
tot  
t 

11.2  Decision-theoretic  ProbLog  283 
\   
\  
0 
0   
Figure  11.5  ADDfJ11 for Example 96. From [Van den Broeck et al., 2010]. 
© [2010] Association for the Advancement of Artificial Intelligence. All 
rights reserved. Reprinted with permission. 
We then perform the summation with the simplified ADDfJ11 which is eheaper 
than the summation with the original ADD. 
Moreover, we can also consider the utility attributes still to be added and 
prune all the leaves of ADDfJ11 that have a value below 
max ADDutil  -
~ Im(u ·)
tot   
L.J 
J  
j-;:.i  
Finally, we can sort the utility attributes in order of decreasing Im(Ui)  and 
add them in this order, so that the maximum pruning is achieved. 
The decision problern can also be solved approximately by adopting two 
techniques that can be used individually or in combination. 
The first technique uses local search: a random strategy fJ  is initially 
selected by randomly assigning values to the decision variables and then a 
cycle is entered in which a single decision is flipped obtaining strategy rJ1•  If 
Util(rJ'(VT))  is larger than Util(rJ(VT) ), the modification is retained and rJ1  
becomes the current best strategy. Util(rJ(VT)) can be computed using the 
BDD BDD(u)  for each utility attribute using function PROB  of Algorithm 6 
for computing P  (u).  
The second technique involves computing an approximation of the util­
ity by using k-best  (see Section 10.1.2) for building the BDDs for utility 
attributes. 
Example  99  (Viral marketing [Van den Broeck et al., 2010]). A  firm  is  in­
terested  in  marketing  a  new  product  to  its  customers.  These  are  connected  in  
a  social  network  that  is  known  to  the  firm:  the  network  represents  the  trust  
relationships  between  customers.  The  firm  wants  to  choose  the  customers  on  

284 Non-standard  Inference  
which  to  perform  markefing  actions.  Each  action  has  a  cost,  and  a  reward  is  
givenfor  each  person  that  buys  the  product.  
We  can  model  this  domain  with  the  DTPROBLOG program  
?  ::  market (P)  :- person (P ).  
0.4  ::  v i ral (P, Q).  
0.3  ::  from_ market i ng (P ).  
market (P)  - >  -2  :- person (P ).  
buys (P)  - >  5  :- person (P ).  
buys (P )  :- market (P ),  from_ marketing (P ).  
buys (P)  :- trusts (P, Q),  buys (Q),  viral (P, Q).  
Here  the  notation  :- person  (P )  . after  decision  and  utility  facts  means  
that there is a fact for each grounding  of person (P),  i.e.,  a fact for each  
person.  The  program  states  that  a  person  P  buys  the  product  if he  is  the  target  
of a  markefing  action  and  the  action  causes  the  person  to  buy  the  product  
(  from_ market i ng  (P ) ) ,  or  if he  trusts  a  person  Q  that  buys  the  product  
and there  isaviral effect (v i ral (P, Q)  ).  The decisions are  market (P)  
for  each  person  P.  A  reward  of 5  is  assigned  for  each  person  that  buys  the  
product  and  each  markefing  action  costs  2. 
Solving  the  decision  problern  for  this  program  means  deciding  on  which  
person  to  perform  a  markefing  action  so  that  the  expected  utility  is  
maximized.  
11.3  Algebraic  Problog  
Algebraic ProbLog (aProbLog) [Kimmig, 2010; Kimmig et al., 2011b] gener­
alizes ProbLog to deal with Iabels offacts that are more general than probabil­
ities. In particular, the Iabels are required to belong to a semiring,  an algebraic 
structure. 
Definition  44  (Semiring). A  semiring is  a  tuple  (A, EB, (8), eEB,  e®)  suchthat  
• Ais  a  set;  
•  EB  is  a  binary  operations  over  A  called  addition that  is  commutative  and  
associative  and  has  neutral element eEB,  i.e.,  Va ,  b,  c  E  A,  
aEBb  
bEBa  
(aEBb)  EB c  
aEB(bEBc)  
a  EB  eEB  
a.  

11.3  Algebraic  ProbLog  285 
• Q9  is  a  binary  operations  over  A  called  multiplication that  left  and  right  
distributes  over  addition  and  has  neutral element e®,  i.e.,  Va,  b,  c E  A,  
(aEBb)Q9c  = (aQ9c)EB(bQ9c)  
aQ9(bEBc)c  =  (a®b)EB(a®b)  
a  Q9  e®  =  e®  Q9  a  =  a.  
•  effi  annihilates  A,  i.e.,  Va  E  A  
a  Q9  effi  = effi  Q9  a  = effi.  
A  commutative semiring is  a  semiring  (A,  EB, @, effi,  e®)  such  that  multipli­
cation  is  commutative,  i.e.,  Va,  b  E  A,  
aQ9b=bQ9a  
An example ofacommutative semiring is ([0, 1],  +,  x, 0, 1)  where [0, 1]  <:;  
lR and +  and x are addition and multiplication over the reals. ProbLog as­
sociates each probabilistic fact with an element of [0, 1] and computes the 
probability of queries by using addition and multiplication, so it can be seen 
as operating on the semiring ( [0, 1], +,  x, 0, 1) that we call probabilistic  
semiring.  
Another example of a commutative semiring is ( [0, 1], max, min, 0, 1) 
where max and min are maximum and minimum operations over the reals. 
PITA(POSS) can be seen as operating on the semiring ( [0, 1], max, min, 0, 1) 
that we call possibilistic  semiring.  
ProbLog can be generalized to operate on a semiring. For a set of ground 
atoms A,  let L (A)  be the set of literals that can be built on atoms from A,  
i.e., L(A)  = Au {~aia E  A}. Fora two-valued interpretation J  over a set 
of ground atoms A,  define the complete  interpretation  c( J)  = J  u { ~aia E  
A\J}  and the set of all possible complete interpretations I(A)  = {c(J)IJ  <:;  
A}. Consistent sets of literals built on atoms A  form the set C (A)  = { H  I H  <:;  
I,I  
E  I(A)}. 
Definition  45 (aProbLog [Kimmig et al., 201lb]). An  aProbLog program  
consists  of  
• a  commutative  semiring  (A,  EB, @, effi,  e®);  
• afinite  set  of ground  atoms  F  =  {!1, ... , fn}  called  algebraic facts; 
• a finite  set  of rules  R;  
• a  labeling  function  a  : L(F)  ~ A.  

286 
Non-standard  Inference  
The possible cornplete interpretations for F  arealso called worlds  and I(F)  
is the set of all worlds. aProbLog assigns labels to worlds and sets of worlds 
as follows. The label of a world I  E  I(F)  is the product of the labels of its 
literals 
A(I)  =  (8) a(l)  
lEI  
The label of a set of cornplete interpretations S  ~ I(F)  is the surn of the 
labels of each interpretation 
A(S)  =  ffi  (8) a(l).  
!ES  lEI  
A query is a set of ground literals. Given a query q,  we denote the set of 
cornplete interpretations where the query is true as I(q)  defined as 
I(q)  = {III  E  I(F)  AI  uR F=  q} 
The label of the query q  is then defined as the label ofI  ( q): 
A(q)  = A(I(q))  = ffi  (8) a(l).  
IEI(q)  lEI  
Since both operations are cornrnutative and associative, the label of a query 
does not depends on the order of literals and of interpretations. 
The inference problern in aProbLog consists of cornputing the labels of 
queries. Depending on the choice of cornrnutative semiring, an inference task 
rnay correspond to a certain known problern or new problerns. Table 11.1  lists 
sorne known inference tasks with their corresponding cornrnutative sernir­
ings. 

11.3  Algebraic  ProbLog  287 
Table  11.1  Inference tasks and corresponding semirings for aProbLog. 
Adapted from [Kimmig et al., 201lb] © [2011] Association for the Ad­
vancement of Artificial Intelligence. All rights reserved. Reprinted with 
permission. 
task 
A  
e""  
e'CI  
a(t)b  
a®b  
a(f)  
a(-f)  
PROB 
[0, 1] 
0 
I  
a+b  
a·b  
a(f)  
1- a(f)  
POSS 
[0, 1] 
0 
I  
max(a, b)  
min(a,b) 
a(f)  
1 
MPE 
[0, 1] 
0 
I  
max(a, b)  
a·b  
a(f)  
1 
a(f)  
MPE 
State 
[0, 1]x 
li'(C(F))  
(0,0) 
(1,{0}) 
Eq.ll.2 
Eq. 11.1 
(p,  {{f)})  
(1- p,  
{{-f)})  
SAT 
{0, 1} 
0 
I  
a  v b  
a  Ab  
1 
1 
#SAT 
N  
0 
I  
a+b  
a·b  
1 
1 
BDD 
BDD(V) 
bdd(O)  
bdd(1)  
a  v  bdd  b  
a  A bdd  b  
bdd(f)  
~bddbdd(f) 
Sensitivity 
IR[X] 
0 
I  
a+b  
a·b  
x  or 
in [0, 1] 
1- a(f)  
Gradient 
[0, 1] X  IR 
(0, 0) 
(1, 0) 
Eq. 11.3 
Eq.ll.4 
Eq. 11.5 
Eq.ll.6 
Example  100  (Alarm- aProbLog [Kimmig et al., 201lb]). Consider  the  
following  aProbLog  program  similar  to  the  ProbLog  program  of Example  78:  
calls(X)  +--- alarm,  hears_alarm(X).  
alarm  +--- burglary.  
alarm  +--- earthquake.  
0.7 :: hears_alarm(john).  
0.7 :: hears_alarm(mary).  
0.05 :: burglary.  
0.01 :: earthquake.  
where  the  Iabels  of positive  literals  are  attached  to  each  fact  f.  This  pro­
gram  is  a  variation  of  the  alarm  BN  of Example  11.  The  program  has  16  
worlds  and  the  query  calls(mary)  is  true  in  six  of  them,  those  shown  in  
Figure  11.6.  For  the  PROB  task,  we  consider  the  probability  semiring  and  
the  label  of negative  literals  are  defi.ned  as  a("'  f)  =  1 - a(f).  Then  the  
hears_alarm(john),  
hears_alarm(john),  
hears_alarm(john),  
~hears_alarm(john),  
~hears_alarm(john), 
~hears_alarm(john), 
hears_alarm(mary),  
hears_alarm(mary),  
hears_alarm(mary),  
hears_alarm(mary),  
hears_alarm(mary),  
hears_alarm(mary),  
lYurglary,  
lYurglary,  
~lYurglary, 
lYurglary,  
lYurglary,  
~lYurglary, 
earthquake  
~earthquake 
earthquake  
earthquake  
~earthquake 
earthquake  
Figure  11.6  Worlds where the query calls(mary)  from Example 100 is 
true. 

288 
Non-standard  Inference  
label  of calls(mary)  is  
A(calls(mary))  =  0.7 · 0.7 · 0.05 · 0.01 +  
0.7. 0.7. 0.05. 0.99 +  
0.7. 0.7. 0.95. 0.01 +  
0.3. 0.7. 0.05. 0.01 +  
0.3. 0.7. 0.05. 0.99 +  
0.3. 0.7. 0.95. 0.01 =  
0.04165 
For  the  MPE  task,  the  semiring  is  ([0, 1],  max, x, 0, 1),  the  label  ofnegative  
literals  are  defined  as  a( ~f)  = 1 - a(f),  and  the  labe[  of calls(mary)  is  
A(calls(mary))  =  0.7 · 0.7 · 0.05 · 0.99 =  
0.001995 
For  the  SAT  task,  the  semiring  is  ( { 0, 1}, v, 1\,  0, 1 ), the  label  of literals  is  
always  1,  and  the  label  of calls( mary)  is  1  as  there  are  six  worlds  where  the  
query  is  true.  
The MPE State task is an extension of the MPE task that also retums the 
world with the highest Iabel. The set Ais  [0, 1] x IP'(CF) where IP'(C(F)) is 
the powerset ofC (F),  so the second element of the Iabels is a set ofconsistent 
sets of literals built on algebraic facts. The aim is for the Iabel of queries to 
have as first argument the maximum of the probability of the worlds and as 
second argument the set of worlds with that probability (there can be more 
than one if they share the same probability). The operations are defined as 
(p,S)&;(q,T)  = (p·q,{IuJIIES,JET}  
(11.1) 
(p,  S)if p  >  q  
(p,  S)  EB (q,  T)  = 
(q,  T)  
if p  <  q  
(11.2)
{ (p,  S  u  T)  
if p  =  q  
The Iabel for the query mary( calls)  of Example 100 is 
A(calls(mary))  =  (0.7 · 0.7 · 0.05 · 0.99, I)  =  (0.001995, I)   
I=  {hears_alarm(john),  hears_alarm(mary),  burglary,   
~earthquake} 
We can count the number of satisfying assignment with the #SAT task with 
semiring (N,  +,  x, 0, 1)  and Iabels a(fi)  =  a( ~ fi)  =  1. We can also have 

11.3  Algebraic  ProbLog  289 
labels encoding functions or data structures. For example, labels may encode 
Boolean functions represented as BDDs and algebraic facts may be Boolean 
variables from a set V.  In  this case, we can use the semiring 
(BDD(V),  Vbdd,  1\bdd,  bdd(O),  bdd(1))  
and assign labels as a(fi)  = bdd(fi)  and a( ~ fi)  = ---,bddbdd(fi)  where 
BDD(V)  is the set of BDDs over variables V,  Vbdd,  Abdd,  ---,bdd  are Boolean 
operations over BDDs, and bdd(·)  can be applied to the values 0, 1, f  E  F  
retuming the BDD representing the 0, 1, or f  Boolean functions. 
aProbLog can also be used for sensitivity analysis, i.e., estimating how a 
change in the probability of the facts changes the probability of the query. 
In  this case, the labels are polynomials over a set of variables (indicated 
with JR[X]) in Table 11.1. In  Example 100, if we use variables x  and y  to 
label the facts burglary  and hears_alarm(mary),  respectively, the label of 
calls(mary)  becomes 0.99 · x  · y  +  0.01 · y  that is also the probability of 
calls(mary).  
Another task is gradient computation where we want to compute the gra­
dient of the probability of the query, as, for example, is done by LeProbLog, 
see Section 13.3. We consider here the case where we want to compute the 
derivative with respect to the parameter Pk  ofthe k-th fact. The labels are pairs 
where the first element stores the probability and the second its derivative 
with respect to Pk·  Using the rule for the derivative of a product, it is easy to 
see that the operations can be defined as 
(a1, a2) E8 (b1, b2) 
(a1 +  b1, a2 +  b2) 
(11.3) 
(a1, a2) 0  (b1, b2) 
(al  · b1,a1 · b2 +  a2 · b1) 
(11.4) 
and the labels of the algebraic facts as 
a  (fi)  = { (Pi,  1)  if i = k  
(11.5)
(Pi,  0) if i  =I=  k  
a(~!i) = { (1- Pi,  -1)  ifi = k  
(11.6)
(1 -Pi,  0) 
if i  =I=  k  
To perform inference, aProbLog avoids the generation of all possible worlds 
and computes a covering set of explanations for the query, as defined in Sec­
tion 3.1, similarly to what ProbLog1 does for PROB. We represent here an 
explanation E  as a set of literals built on F  that are sufficient for entailing the 

290 
Non-standard  Inference  
query, i.e., Ru E  F=  q,  and a covering set of explanations E(q)  as a setsuch 
that 
VI  E T(q),  3J  E E(q)  : J  c;;  I  
We define the label of an explanation E  as 
A(E)  =  A(T(E))  =  ffi  {8)  a(l)  
IEI(E)  lEI  
We call A(E)  a neutral  sum  if 
A(E)  =   {8)  a(l)  
lEE  
If V f  E  F  : a(f)  EB a( ~f)  = /i9,  then A(E)  isaneutral sum. 
We call EB EEE(q)  A(E)  a disjoint  sum  if 
ffi  A(E)  =  ffi  A(I)  
EEE(q)  
IEI(q)  
EBis idempotentifVa  E  A:  affia  = a.  IfEBis idempotent, then EBEEE(q)  A(E)  
is a disjoint sum. 
Given a covering set of explanations E(q),  we define the explanation  sum  
as 
S(E(q))  = ffi  {8)  a(l)  
EEE(q)  lEE  
If A(E)  isaneutral sum for allE E  E(q)  and EBEEE(q)  A(E)  is a disjoint 
sum, then the explanation sum is equal to the label of the query, i.e., 
S(E(q))  = A(q).  
In this case, inference can be performed by computing S(E(q)).  Otherwise, 
the neutral  sum  and/or disjoint  sum  problems  must be solved. 
To tack:le the neutral-sum problem, let free(E)  denote the variables not 
occurring in an explanations E: 
free(E) = {flf  
E  F,  f  ~ E,  ~J ~ E} 
We can thus express A(E)  as 
A(E)  = Q9  a(l)  Q9  Q9  (a(l)  EB a( ~l)) 
lEE  
lEfree(E) 

11.3  Algebraic  ProbLog  291 
given the propetlies of commutative semirings. 
The sum A(Eo) EB A(E1) of two explanations can be computed by ex­
ploiting the following property. Let Vi  = {f I f  E  Ei  v ~ f  E  Ei}  be the 
variables appearing in explanation Ei,  then 
A(Eo) EB A(E1) = (Pl(Eo) EB Po(El)) Q9  
(8) 
(a(f)  EBa(~f)) 
fO\(Vou V1) 
(11.7) 
with 
Pj(Ei)  =  (8) a(Z) Q9  (8) (a(f)  EBa(~f)) 
ZEE;  
jEVj\Vi  
So we can evaluate A ( Eo)  EBA (E1) by taking into account the set of variables 
on which the two explanations depend. 
To solve the disjoint-sum problem, aProbLog builds a BDD representing 
the truth of the query as a function of the algebraic facts. If sums are neutral, 
aProbLog assigns a label to each node n  as 
label(1) = e&J  
label(O) =  eEB  
label(n) = (a(n)  QS)label(h)) EB (a( ~n) QS)label(Z)) 
where h  and Z denote the high and low child of n.  In  fact, a full Boolean 
decision tree expresses E ( q)  as an expression of the form 
Vh  1\  ·  ·  ·  1\  VZn  1\  1 ( {  h,  ...  , Zn}  E  [  ( q))  
h  
Zn  
where Zi  is a literal built on variable fi  and 1 ( { h, ... , Zn}  E  E (q))  is 1  if 
{h, ... ,Zn}  E  E(q)  and 0 otherwise. By the properties of semirings, 
A(q)  =  ffi  h Q9  .••  Q9  Et)ln Q9  e({h, ... , Zn}  E  f(q)) 
(11.8) 
h  
Zn  
where e({h, ... , Zn}  E  E(q))  is e&Y  if {h, ... , Zn}  E  E(q)  and eEB  otherwise. 
So given a full Boolean decision tree, the label of the query can be computed 
by traversing the tree bottom-up, as for the computation of the probability of 
the query with Algorithm 6. 
BDDs are obtained from full Boolean decision trees by repeatedly merg­
ing isomorphic subgraphs and deleting nodes whose children are the same 
until no more operations are possible. The merging operation does not affect 

292 
Non-standard  Inference  
Equation (11.8), as it simply identifies identical sub-expressions. The deletion 
operation deletes a node n  when its high and low children are the same node s.  
In this case, the label of node n  would be 
label(n) =  (a(n)  ®label(s)) EB (a("'n)  ®label(s)) 
that is equal to Iabel(s) if sums are neutral. If not, Algorithm 16 is used that 
uses Equation (11.7) to take into account deleted nodes. Function LABEL 
is called after initializing Table(n)  to null  for all nodes n.  Table(n)  stores 
intermediate results similarly to Algorithm 6 to keep the complexity linear in 
the number of nodes. 
Algorithm  16  Function LABEL: aProbLog inference algorithm. 
1: function LABEL(n) 
2: 
if Table(n)  "#  null  then 
3: 
return Table(n)  
4: 
eise 
5: 
if n  is the I-terminal then 
6: 
return (e®,  0)  
7: 
end if 
8: 
if n  is the 0-terminal then 
9: 
return (eEEl, 0)  
10: 
end if 
11: 
let h  and l  be the high and low children of n  
12: 
(H,  Vh)  +--- LABEL(h) 
13: 
(L,  V!)  +--- LABEL(l) 
14: 
Pt(h)  +--- H  ® ®xEVl\Vh  (o:(x) EB o:( ~x)) 
15: 
Ph(l)  +--- L®®xEVh\v (o:(x)  EBo:(~x))
1 
16: 
label(n) +--- (o:(n)®Pt(h))EB(o:(~n)®Ph(l)) 
17: 
Table(n)  +--- (label(n), {n} u  Vh  u  V!)  
18: 
return Table(n)  
19: 
endif 
20: end function 

12   
lnference  for  Hybrid  Programs  
In this chapter we present approaches for performing various inference tasks 
on hybrid programs. We start from the DISTR task in Extended PRISM, then 
we consider COND by Weighted Model Integration. We describe approxi­
mate inference: first for the EVID and COND tasks, also with bounds on 
error, and then for the DISTR and EXP tasks. 
12.1  lnference  for  Extended  PRISM  
Islam et al. [2012b]; Islam [2012] propose an algorithm for performing the 
DISTR task for Extended PRISM, see Section 4.3. Since it is impossible 
to enumerate all explanations for the query because there is an uncountable 
number of them, the idea is to represent derivations symbolically. 
In the following, we use Constr  to denote a set (conjunction) of linear 
equality constraints. We also denote by X  a vector of variables and/or values, 
explicitly specifying the size only when it is not clear from the context. This 
allows us to write linear equality constraints compactly (e.g., Y  = a  ·X+ b). 
Definition  46  (Symbolic derivation [Islam et al., 2012b]). A  goal  9  directly  
derives  goal  9
1
,  denoted  9  ~ 9
1
,  if one  of the  following  conditions  holds  
PCR  if 9  =  q1 (X1), 91, and  there  exists  a  clause  in  the  program,  q1 (Y)  ~ 
r1(Yl), r2(Y2), ... , rm(Ym)  suchthat  (}  =  m9u(q1(X1), q1(Y)) then  
9
1  =  (r1 (Y1), r2(Y2), ... , rm(Ym),  91)(}  
MSW  if 9  = msw(rv(X),  Y),  91 then  9
1  = 91 
CONSTR  if 9  =  Constr,  91 and  Constr  is  satisfiable:  then  91  =  91· 
where  PCR  stands  for  program clause resolution. A  symbolic  derivation  of  
9  is  a  sequence  of  goals  9o,  91, ... such  that  9  =  9o  and,  for  all  i  ):  0, 
9i  ~ 9i+1· 
293 

294  lnference  for  Hybrid  Programs  
Example  101  (Symbolic derivation). Consider  Example  54  that  we  repeat  
here  for  ease  of reading.  
wid9et(X)  ~ 
msw(m,  M), msw(st(M),  Z),  msw(pt,  Y),  X=  Y  +  Z.  
values(m,  [a,  b]).  
values(st(_),  real).  
values(pt,  real).  
~ set_sw(m,  [0.3, 0.7]). 
~ set_sw(st(a),  norm(2.0,  1.0)). 
~ set_sw(st(b),  norm(3.0,  1.0)). 
~ set_sw(pt,  norm(0.5,  0.1)). 
The  symbolic  derivationfor  goal  wid9et(X)  is  
91 : wid9et(X)  
~ 
92: msw(m,  M), msw(st(M),  Z),  msw(pt,  Y),  X=  Y  +  Z  
~ 
93: msw(st(M),  Z),  msw(pt,  Y),  X=  Y  +  Z  
~ 
94: msw(pt,  Y),X  =  Y  +  Z  
~ 
95: X=  Y  +  Z  
~ 
true  
Given a goal, the aim of inference is to retum a probability density function 
over the variables of the goal. To do so, all successful symbolic derivations 
are collected. Then a representation of the probability density associated with 
the variables of each goal is built bottom-up starting from the leaves. This 
representation is called a success function.  
First, we need to identify, for each goal 9i  in a symbolic derivation, the set 
of its derivation  variables  V  (9i), the set of variables appearing as parameters 
or outcomes of msws  in some subsequent goal 9j.  j  >  i. V  is further parti­
tioned into two disjoint sets, Vc  and Vd,  representing continuous and discrete 
variables, respectively. 
Definition  47  (Derivation variables [Islam et al., 2012b]). Let  9  ~ 9'  such  
that 9'  is  derived from  9  using  
PCR  Let()  be  the  mgu  in  this  step.  Then  Vc(9)  and  Vd(9)  are  the  Zargestset  
ofvariables  in  9  suchthat  Vc(9)()  <:;  Vc(9')  and  Vd(9)()  <:;  Vd(9')  

12.1  lnferencefor  Extended  PRJSM  295 
MSW  Let  9  = msw(rv(X),  Y),  91· Then  Vc(9)  and  Vd(9)  are  the  Zargest  
set  ofvariables  in  9  suchthat  Vc(9)B  ~ Vc(9')  u  {Y} and  Vd(9)B  ~ 
Vd(9')  uX ifY  is  continuous,  otherwise  Vc(9)B  ~ Vc(9')  and  Vd(9)B  ~ 
Vd(9')  u  X  u  {Y} 
CONSTR  Let  9  =  Constr,  91· Then  Vc(9)  and  Vd(9)  are  the  Zargestset  of  
variables  in  9  suchthat  Vc(9)B  ~ Vc(9')  u  vars( Constr)  and  Vd(9)B  ~ 
Vd(9')  
So  V  (9)  is  built from  V  (9')  and  it  can  be  computed for  all  goals  in  a  symbolic  
derivation  bottom-up.  
Let C denote the set of all linear equality constraints using set of variables 
V  and Iet L  be the set of all linear functions over V.  Let Nx(f.t,  (}2) be 
the PDF of a univariate Gaussian distribution with mean f.t  and variance (}2, 
and 5x(X)  be the Dirac delta function which is zero everywhere except at 
x  and integration of the delta function over its entire range is 1.  A Product  
probability  density  function  (PPDF)  cp  over V  is an expression ofthe form 
c/J  =  k  ·  0  5v(Vz)  0  Nli  (f.ti,  5[)  
l  
i  
where k  is a non-negative real number, Vz  E  V  fi  E  L.  A pair ( cp,  C) where 
C  ~ Cis called a constrained  PPDF.  A sum ofa finite number ofconstrained 
PPDFs is called a success  function,  represented as 
'lj;  = .2:( c/Ji, Ci) 
i  
We use Ci  ('lj;)  to denote the constraints (i.e., Ci)  in the i-th constrained PPDF 
of success function 'lj;  and Di ( 'lj;)  to denote the i-th PPDF c/Ji  of 'lj;.  
The success function of the query is built bottom-up from the set of 
derivations for it. The success function of a constraint Cis (1,  C). The suc­
cess function of true  is (1,  true).  The success function of msw(rv(X),  Y)  
is ( 'lj;,  true)  where 'lj;  is the probability density function of rv's  distribution if 
rv  is continuous, and its probability mass function if rv  is discrete. 
Example  102  (Success functions of msw  atoms). The  success  function  of  
msw(m,  M)for  the  program  in  Example  101  is  
'l/Jmsw(m,M)(M)  = 0.35a(M)  +  0.75b(M)  
We  can  represent  success  functions  using  tables,  where  each  table  row  de­
notes  discrete  random  variable  valuations.  For  example,  the  above  success  
function  can  be  represented  as  

296 
lnference  for  Hybrid  Programs  
M  
'l/Jmsw(m,M)(M)  
a  
0.3  
b  
0.7  
For a g  -
g'  derivation step, the success function of g  is computed from the 
success function of g'  using the join and marginalization operations, the first 
for MSW  and  CONSTR  steps and the latter for PCR  steps. 
Definition  48  (Join operation). Let  'l/J1  = .l:i(Di,  Ci) and  'l/J2  = .l:j (Dj,  Cj) 
be  two  success  functions,  then  the  join 'l/J1  *  'l/J2  of 'l/J1  and  'l/J2  is  the  success  
function  
'i.(DiDj,  Ci  A  Cj) 
i,j  
Example  103  (Join operation). Let 'l/Jmsw(m,M)  (M)  and 'lj;9 (X,  Y,  Z,  M)  be  
defined  as follows:  
M  
'l/Jmsw(m,M)(M)  
M  
'l/Jc(X,  Y,  Z,  M)  
a  
0.3  
a  
(Nz(2.0,  l.O)Ny(0.5,  0.1), X=  Y  + Z)  
b  
0.7  
b  
(Nz(3.0,  l.O)Ny(0.5,  0.1), X=  Y  + Z)  
Thejoin  of'l/Jmsw(m,M)(M)  and  'l/Jc(X,  Y,  Z,  M)  is:  
M  
'l/Jmsw(m,M)(M)  *  'l/Jc(X,  Y,  Z,  M)  
a  
(0.3Nz(2.0, l.O)Ny(0.5,  0.1), X=  Y  +  Z)  
b  
(0.7Nz(3.0,  l.O)Ny(0.5,  0.1), X=  Y  +  Z)  
Since  6a(M)6b(M)  =  0 because  M  cannot  be  both  a  and  bat  the  same  
time,  we  simplified  the  resuZt  by  eliminating  any  such  inconsistent  PPDF  term  
in  'lj;.  
In the case of a PCR  derivation step g  -
g',  g  may contain a subset of 
the variables of g'.  To compute the success function for g,  we thus must 
marginalize over the eliminated variables. If an eliminated variable is discrete, 
marginalization is done by summation in the obvious way. If an eliminated 
variable V  is continuous, marginalization is donein two steps: projection and 
integration. The goal of projection is to eliminate any linear constraint on V.  
The projection operation finds a linear constraint V  = aX  +  b  on V  and 
replaces all occurrences of V  in the success function by aX  +  b.  
Definition  49  (Projection of a success function). The  projection of a  suc­
cessfunction  'lj;  w.r.t.  a  continuous  variable  V,  denoted  by  'l/J~v' is  a  success  

12.1  lnferencefor  Extended  PRJSM  297 
function  'lj;'  such  that  Vi:  
Di('l/J')  =  Di('l/J)[V jaX  +  b]  
and  
Ci('l/J') =  (Ci('l/J)- Cip)[VjaX  +  b]  
where  Cip  isalinear  constraint  V  = aX  +  b  on  V  in  Ci('l/J) and  t[xjs]  
denotes  the  replacement  of all  occurrences  of x  in  t  by  s.  
If 'lj;  does not contain any linear constraint on V,  then the projected form 
remains the same. 
Example  104  (Projection operation). Let  'l/J1 be  the  success  function  
'l/J1 =  (0.3Nz(2.0, l.O)Ny(0.5, 0.1), X=  Y  +  Z)  
The  projection  of 'l/J1 w.r.t.  Y  is  
'l/Jdy  = 0.3Nz(2.0, l.O)Nx-z(0.5, 0.1), true)  
We can now define the integration operation. 
Definition  50  (Integration of a success function). Let  'lj;  be  a  success function  
that  does  not  contain  any  linear  constraints  on  V.  Then  the  integration of 'lj;  
w.r.t.  to  V,  denoted  by  ~v 'lj;,  is  the  success  function  'lj;'  suchthat  
Vi: Di('l/J')  =I Di('l/J)dV  
Islam et al. [2012b]; Islam [2012] prove that the integration of a PPDF with 
respect to a variable V  is a PPDF, i.e., that 
+oo m  
m' 
a  Lw  1] NakXk+bk  (JLb O"~)dV =  a'  D
Na;x; +b;  (JL~, O"?)  
where V  E  Xk  and V  rf  X{.  
For example, the integration of Na1  v -x1 (JLl, O"i)Na2 v  -x2 (JL2, 17~) w.r.t. 
variable V  is 
t: Nal V -X1 (JLl, O"i)Na2V-X2 (JL2, O"~)dV = 
(12.1)
~ r  
( 
2 2 
2 2)
JVa2X1-a1X2 a1JL1- a2JL1,a20"1 +  a10"2 

298 
lnference  for  Hybrid  Programs  
where X  1 and X  2 are linear combinations of variables except V.  
Example  105  (Integration of a success function). Let  1/J2 represent  the  fol­
lowing  success function  
1/J2 = (0.3Nz(2.0,  l.O)Nx-z(0.5,  0.1), true)  
Then  integration  of 1/J2 w.r.t.  Z  yields  
f 
1/J2 = (J 0.3Nz(2.0,  l.O)Nx-z(0.5,  0.1)dZ,  true)  = 
z  
(0.3Nx(2.5, 1.1), true)  
by  Equation  ( 12.1 ).  
The marginalization operation is the composition of the join and integra­
tion operations. 
Definition  51  (Marginalization of a success function). The  marginalization 
of a  success function  1/J  with  respect  to  a  variable  V,  denoted  by  M(1/J,  V),  is  
a  success function  1/J'  such  that  
1/J'  =  f 
1/J~v 
V  
The  marginalization  over  a  set  ofvariables  is  defined  as  M(1/J,  {V} u  X) = 
M(M('I/J, V),  X) andM(1/J,0)  = 1/J.  
The set of all success functions is closed under join and marginalization Op­
erations. The success function for a derivation can now be defined as follows. 
Definition  52 (Success function of a goal). The  success function  of a  goal 9,  
denoted  by  'ljJ9,  is  computed  based  an  the  derivation  9  ~ 9':  
2:: ,  M('ljJ9,,  V(9')- V(9))  forallPCR  9  ~ 9' 
9
1/Jg  = 
1/Jmsw(rv(X),Y)  *  1/Jg'  
~9  = msw(rv(X), Y),  91
{ 1/Jconstr  *  1/Jg'  
if 9  =  Constr,  91 
Example  106  (Success function of a goal). Consider the  symbolic  derivation  
ofExample101.  Thesuccessfunctionofgoal9s  is'ljJ95 (X,Y,Z)  =  (1,X =  
Y  +  Z).  Ta  obtain  'ljJ94 ,  we  must perform  a  join  operation:  
1/J94  (X,  Y,  Z)  = 1/Jmsw(pt,Y)(Y)*1/Jg5  (X,  Y,  Z)  = (Ny(0.5,  0.1), X  = Y  +Z)  
The success function  of goal 93  is 1/Jmsw(st(M),Z)  (Z)  *  1/Jg4  (X,  Y,  Z):  

12.1  lnferencefor  Extended  PRJSM  299 
M  
'lj;93  (X,  Y,  Z,  M)  
a  
(Nz(2.0,  l.O)Ny(0.5,  0.1), X=  Y  + Z)  
b  
(Nz(3.0,  l.O)Ny(0.5,  0.1), X=  Y  + Z)  
Then  wejoin  'l/Jmsw(m,M)(M)  and 'lj;93 (X,  Y,  Z,  M):  
M  
'lj;92 (X,  Y,  Z,  M)  
a  
(0.3Nz(2.0,  l.O)Ny(0.5,  0.1), X=  Y  + Z)  
b  
(0.7Nz(3.0,  l.O)Ny(0.5,  0.1), X=  Y  + Z)  
The  successfunction  ofgl  is  'lj;91 (X) =  M('lj;92 (X, Y,  Z,  M),  {M, Y,  Z} ). We  
marginalize  'lj;92 (X,  Y,  Z,  M)  first  w.r.t.  M:  
'l/J~2 = M('lj;g2, M)  = j  
'l/Jg2~M = 
M  
(0.3Nz(2.0,  l.O)Ny(0.5,  0.1), X=  Y  + Z)  + 
(0.7Nz(3.0,  l.O)Ny(0.5,  0.1), X=  Y  +  Z)  
Then  we  marginalize  'lj;~2 (X,  Y,  Z)  w.r.t.  Y:  
'l/JZ2 = M('lj;~2' Y)  = j  
'lj;~2~y = 
y  
=  0.3Nz(2.0,  l.O)Nx-z(0.5,  0.1) +  
0.7Nz(3.0,  l.O)Nx-z(0.5,  0.1) 
Finally,  we  get  'lj;91 (X)  by  marginalizing  'l/J;2(X,  Z)  w.r.t.  Z:  
'l/Jg1  (X)  = M('l/JZ2' Z)  = j 'l/JZ2h = 
z  
=  0.3Nx(2.5, 1.1) + 0.7Nx(3.5,  1.1) 
So the algorithm returns the probability density of the continuous random 
variables in the query. 
This algorithm is correct if the program satisfies PRISM's assumptions: 
independent-and and exclusive-or. The first is equivalent to requiring that an 
instance of a random variable occurs at most once in any derivation. In  fact, 
the join operation used in a g  -
g'  MSW  step is correct only if the random 
variable defined by the msw  atom does not appear in g1•  Moreover, the sum 

300 lnference  for  Hybrid  Programs  
over all PCR  steps in Definition 52 is correct only if the terms are rnutually 
exclusive. 
A subgoal rnay appear rnore than once in  the derivation tree for a goal, so 
tabling can be effectively used to avoid redundant cornputation. 
12.2  lnference  with  Weighted  Model  Integration  
Weighted rnodel integration (WMI) [Belle et al., 2015b,a, 2016; Morettin 
et al., 2017] is a recent approach that generalizes WMC to formulas with 
continuous variables. 
12.2.1  Weighted  model  integration  
Satisfiability (SAT) is the problern of deciding whether there is a satisfying 
assignrnent to a propositional formula or not. Satisfiability rnodulo theory 
(SMT) generalizes SAT by allowing expressions frorn a background theory. 
Definition  53  (Satisfiability Modulo Theory). Let  ffi. denote  the  set  of real  
numbers  and  lB =  {0, 1} the  set  of  Boolean  values,  let  B  be  a  set  of  m  
Boolean  variables  and  X  a  set  of  n  real  variables.  SMT(RA)  (real  arith­
metics)  theories are  combinations  by  means  of the  standard  Boolean  opera­
tors  {---., 1\,  v, -, ~} of atomic  propositions from  B  and  of  atornic forrnulas 
in  theform  g(X)  1><1  c where  c E  ffi.,  I><IE  {  =, =!=,  :S;, :?, <>} and  g:  ffi.n- R  
SMT(NRA)  (non-linear  real  arithmetics)  is  SMT(RA)  where  atomicfor­
mulas  are  restricted  to  use  functions  g  : ffi.n  -
ffi. of the  form  .L:i Ci · Xfi with  
Xi EX and  Ci, PiE (Q. 
SMT(/2RA)  (linear  real  arithmetics)  is  SMT(NRA)  where  atomic  for­
mulas  are  restricted  to  use  functions  g  : ffi.n  -
ffi. of the  form  .L:i Ci · Xi with  
Xi E X and  Ci E (Q. 
The  SMT(RA)  (SMT(NRA),  SMT(/2RA))  problern consists  in  deciding  
whether  there  is  an  assignment  of the  Boolean  and  real  variables  that  satisfy  
an  SMT(RA)  (SMT(NRA),  SMT(/2RA))  theory.  
Example  107  (Broken- SMT(/2RA) [Zuidberg Dos Martires et al., 2019, 
2018]). Consider  thefollowing  SMT(/2RA)  theory  broken:  
(no_cool 1\  (t >  20)) v (t >  30) 
(12.2) 
where  no_cool is  a  Boolean  variable  and  t a  real  variable.  SMT( 12RA)  
means  deciding  whether  there  is  an  assignment  to  no_cool and  t that  satisfy  
the  formula.  

12.2  Inference  with  Weighted  Model  Integration  301 
Definition  54  (Interpretation and Model). Given  an  SMT formula  cp  built  on  
Boolean  variables  B  and  real  variables  X, an  interpretation is  an  assignment  
(b,  x) to  (B,  X). A  interpretation  (b,  x) is  a  model of cp  if cp  evaluates  to  1  
when  (B,  X) are  assigned  to  (b,  x). The  set  of all  the  models  of aformula  
cp  is  denoted  by  M(cp)  = {(b,  x) I  cp(b, x) = 1}. The  projection MB  (cp)  of  
M(cp)  over  Bis  
MB(cfJ) =  {b  I :Jx: (b,x) E  M(cfJ)} 
and  contains  partial models. Given  a  set  of Boolean  values  b,  we  dejine  the  
formula  cpb  as  the  formula  cp  where  the  Boolean  variables  B  are  replaced  by  
b.  Then  M(cpb) is  the  set  
M(cpb) = {x I  (b,x) E  M(cp)} 
of elements  x that  can  extend  a  partial  model from  MB  (cp)  to  a  model.  
We are now ready to define WMI. 
Definition  55  (Weighted Model Integration). Given  a  set  B  of m  Boolean  
variables,  a  set  X  of n  real  variables,  a  weight  function  w(X,  B)  :  lffim  x 
]Rn  ~ JR+ 1,  and  an  SMT  formula  cp(X,  B),  the  weighted  modelintegral  
(WMI)  is  
WMI(cp,  w  I  X, B)  =  .l:bEMs(<P) SxEM(cpb)  w(x,  b)dx  
(12.3) 
The weight function is used to map a set of variable assignments to their 
weight. The weight function usually factorizes  as the product of the weights 
over the different variables, i.e., 
w(x,  b)  = Wx(x)wb(b)  = Wx(x)  n Wbi(bi)  
(12.4) 
biEb 
where Wx  : ]Rn  ~ JR+, wb  : lffim  ~ JR+ and wbi  : lffi  ~ JR+. This does not 
Iimit the generality as a weight function that does not factorize in this way 
can be rewritten as a sum of weight functions over mutually exclusive partial 
assignments to the Boolean variables, with each individual term factorizing 
as Equation (12.4). 
Example  108  (Broken- WMI [Zuidberg Dos Martires et al., 2019, 2018]). 
Consider  again  the  theory  broken  of  Example  107.  Assurne  that  wx(t)  
1  JR+ =  {x  E  lR  I  x;::  0}.  

302 lnference  for  Hybrid  Programs  
M(20, 5) and  wb(no_cool = 1) = 0.01. Then  we  have  
WMI(broken,  w  I  t, no_cool) = 0.01 f  
M(20, 5)dt +  
J20<t:;;;30 
i  
M(2o, 5)dt. 
t>30 
WMI can be used to perform inference over hybrid probabilistic logic pro­
grams. Consider PCLPs. We can collect a set of explanation with an algorithm 
such as PRISM, ProbLog or PITA and then build an SMT formula whose 
WMI is the probability of the query. 
Example  109  (Broken machine- PCLP [Zuidberg Dos Martires et al., 2018]). 
The  following  PCLP  models  a  machine  that  breaks  down  if the  temperature  
is  above  30  degrees  or  if it  is  above  20  degrees  in  case  there  is  no  cooling  
0.01 :: no_cool.  
t "'gaussian(20,  5).  
broken  ~ no_cool, (T  >  20).  
broken  ~ (T  >  30).  
The  query  broken  has  the  set  of explanations:  w  = w1  u  w2  with  
WI  = { ( t, no_cool) lt >  20, no_cool = 1} 
w2  = {(t,no_cool)lt >  30} 
These  two  explanations  can  be  easily foundfrom  the  above  program  using  in­
ference  techniques for  discrete  programs.  They  can  be  immediately  translated  
into  the  SMT formula  of Example  107.  Computing  the  WMI  ofthat formula  
produces  the  probability  of the  query  broken.  
There are various techniques for performing WMI: [Belle et al., 2015b,a, 
2016; Morettin et al., 2017], see [Morettin et al., 2021] for a survey. Here 
we present the approach of [Zuidberg Dos Martires et al., 2018, 2019] that is 
based on Algebraic model counting (AMC). 
12.2.2  Algebraic  model  counting  
AMC [Kimmig et al., 2017] generalizes WMC to commutative semirings, see 
Section 11.3. 
Definition  56  (Algebraic Model Counting (AMC) [Kimmig et al., 2017]). 
Given:  
• a propositionallogic formula  rjJ over a set  of Boolean variables  B  
• a  commutative  semiring  (A,  EB, ®, effi,  e<8l)  

12.2  Inference  with  Weighted  Model  Integration  303 
• a  labelingfunction  a:  L  ~ A,  mapping  literals  builtfrom  the  variables  
in  B  to  values  of the  semiring  A  
The algebraic model count  of cp  is:  
AMC(cp,  a I  B) = E9 @ a(bi)  
bEM(cp) biEb 
where  M  (cp)  are  the  models  of cp  
AMC can be performed using knowledge compilation. Once the formula is 
translated to a d-DNNF, Algorithm 17 can be applied 
Algorithm  17  Function EVAL: Evaluating an NNF circuit. 
1: function EVAL(n, EB, ®, eE!l, e®) 
2: 
if n  is a true node then 
3: 
return e®  
4: 
end if 
5: 
if n  is a false node then 
6: 
return eEEI 
7: 
end if 
8: 
if n  is a literal node l  then 
9: 
return a(Z)  
10: 
end if 
11: 
if n  is a disjunction v~1 ni  then 
12: 
return EB~= 1 EVAL(ni, EB, ®, eEEI, e®) 
13: 
end if 
14: 
if n  is a conjunction /\~ 1 ni  then 
15: 
return ®~= 1 EVAL(ni, EB, ®, eEEI, e®) 
16: 
end if 
17: end function 
We first define a useful property of semiring addition and then present a 
theorem on the correctness of Algorithm 17. 
Definition  57 (Neutral-sum property). A  semiring  addition  and  labefing  
function  pair  (EB, a) is  neutral  if and  only  if 'v'b E  B : a (b) EB a (---. b) =  e&J  
Theorem  18  (Correctness of Algorithm 17 [Kimmig et al., 20 17]). Evalu­
ating  a  d-DNNF  representation  of the  propositional  theory  using  Algorithm  
17 for  a  semiring  and  labefing  function  with  neutral  tuple  (EB, a),  is  a  correct  
computation  of the  algebraic  model  count.  

304 
lnference  for  Hybrid  Programs  
12.2.2.1  The  probability  density  semiring  and  WMI  
In order to cast WMI as AMC, we define a probability  density  semiring  
[Zuidberg Dos Martires et al., 2018, 2019]. 
Let c(X) be an atomic formula from Definition 53, then absc(X)  is a 
Boolean variable called the atomic  formula  abstraction,  or simply abstrac­
tion,  of c(X). 
Definition  58 (Labeling function a  ). If the  literall  represents  either  a  Boolean  
variable  b  or  its  negation  ---.b  then  its  label  is  
a(b)  = (wb(b),  0)  
(12.5) 
a( --.b)  =  (1- wb(b),  0)  
(12.6) 
where  we  assume  that  the  weights  of  the  individual  Boolean  variables  are  
probabilities  (they  sum  up  to  one).  If  this  is  not  true,  they  must  be  normal­
ized.  Otherwise  if the  literall  corresponds  to  an  atomic  formula  abstraction  
absc(X)  or  its  negation,  then  the  labe[  of l  is  given  by:  
a(absc(X))  =  ([c(X)], X) 
(12.7) 
a(--.absc(X))  =  ([--.c(X)],X) 
(12.8) 
where  Xis  the  set  ofvariables  appearing  in  c(X) and  square  brackets  denote  
the  so-called  Iverson  brackets  [Knuth,  1992]:  [a]  evaluate  to  I  ifformula  a  
evaluates  to  true  and  to  0  otherwise.  
We can now define the set of elements of the semiring: 
Definition  59 (Probability density semiring S).  The  semring  S  is  defined  by  
A  = {(a, V(a))} 
(12.9) 
where  a denotes any algebraic expression  over  RA plus Iverson  brackets and  
V(a)  denotes  the  set  ofreal  variables  occurring  in  a.  The  neutral  elements  EB  
and  ® are:  
effi  :=  (0, 0)  
e®  :=  (1,  0)  
(12.10) 
Addition  and  multiplication  are  defined  as:  
(a1, V(ai))  EB  (a2, V(a2)) = (a1 + a2, V(a1  + a2)) 
(12.11) 
(a1, V(ai))  ® (a2, V(a2)) =  (al  x a2, V(al  x a2)) 
(12.12) 

12.2  Inference  with  Weighted  Model  Integration  305 
For instance, for a  = 0.01[20 <  t ~ 30] +  [t >  30], V(a)  = {t} we have 
that (a,  V(a))  ES.  
Lemma  16  (The probability density semiring is commutative [Zuidberg Dos 
Martires et al., 2018, 2019]). 
The  structure  S  = (A,  EB, <8), eEB, e®) is  a  
commutative  semiring.  
Lemma  17  (The probability density semiring has neutral addition and label­
ing function). The  pair  (EB, a)  is  neutral.  
The following theorem shows how to exploit AMC to perform WMI: since 
there is no integration in AMC, we need to perform it last, yielding WMI  = 
S AMC.  Moreover, AMC has tobe performed on the abstracted formula, as 
it is defined on propositional formulas. 
Theorem  19  (AMC to perform WMI [Zuidberg Dos Martires et al., 2018, 
2019]). Let  cp  be  an  SMT(RA)  theory,  w  a  factorized  weight function  over  
the  Boolean  variables  B  and  continuous  variables  X. Let  c/Ja  be  the  propo­
sitionallogic formulas  over the set  of Boolean variables  B  and  Bx,  where  
Bx  is  the  set  of abstraction  of atomic  formulas.  Furthermore,  assume  that  
AMC(c/Ja,  aiBx  u  B)  evaluates  to  (w,  V(w))  in  the  semiring  S.  Then  
WMI(cp,  wiX, B)  = f  Wwx(x)dx 
JxEX  
where  Xis  the  set  of all  possible  assignments  to  the  variables  in  V(w).  
12.2.2.2  Symbo  
Symbo is an algorithm that produces the weighted model integral of an 
SMT(NRA)  formula cp  via knowledge compilation. Using theorems 18 and 
19 we obtain Algorithm 18. 
Symbo actually uses SDDs instead of d-DNNFs. Since SDDs are d-DNNFs, 
Theorem 18 continues to hold. For symbolic integration Symbo uses the 
PSI-Solver [Gehret al., 2016]. 
Example  110  (WMI of the broken  theory [Zuidberg Dos Martires et al., 
2018, 2019].). Consider  the  SMT  formula  of  Example  107.  The  jirst  two  
steps  of Symbo  produce  the  compiled  logic  formula  that  is  shown  in  Figure  
12.1.  Thefollowing  two  step  yield  the  arithmetic  circuit  in  Figure  12.2.  

306 
Inference  for  Hybrid  Programs  
Algorithm  18  Function SYMBO:  symbolic evaluation ofWMI [Zuidberg Dos 
Martires et al., 2018, 2019]. 
1: 
function  SYMBO(q'J) 
2: 
Abstract of all the atomica formulas in <P  in order to obtain <Pa  
3: 
Compile <Pa  into a d-DNNF representation cPcompiled  
4: 
Transform the logic formula <Pcompiled  into an arithmetic circuit ACq,  
5: 
Label the literals in ACq,  according to the labeling function given in Definition 58 
6: 
Symbolically evaluate ACq,  obtaining (\lf , V(\lf))  
7: 
Symbolically multiply \lf,  which is a sum-product of weighted indicator functions 
(Iverson brackets) by the weights of the continuous variables in V(\lf)  
8: 
Symbolically integrate the formula by calling a symbolic inference engine 
9: 
end  function  
Figure  12.1  d-DNNF circuit for Example 107. 
Then  the  arithmetic  circuit  is  evaluated  (line  6),  the  resulting  expression  
is  multiplied  by  the  weightfor  t (line  7)  and  the  integral  is  solved symbolically  
+  
*  
Figure  12.2  Arithmetic circuit for Example 107. 

12.2  Inference  with  Weighted  Model  Integration  307 
using  PSI-Solver  (line  8):  
WMI(broken,  w  I  t, no_cool) =  
(12.13) 
=I  (0.01[t>20][t~30] + [t>30]) J\/t(20, 5)dt 
= 0.01 f  
J\/t(20, 5)dt+ i  J\/t(20, 5)dt 
J20<t~30 
t>30 
_§.v's+2Q. 
_5v'8+~ 
v'8 
2 
2 
v'8 
2
f 
=  1- 0.01 
e-x  dx- 0.99 
e-x  dx 
f  
2 
-00 
-00 
The  formula  obtained  includes  integrals  S~oo e-t 
2  dt  that  are  not  solvable  
symbolically  and  must  be  integrated  numerically.  
WMI(broken,  w  I  t, no_cool) is  also  equal  to  P(broken)  in  the  PCLP  
of Example  109.  
12.2.2.3  Sampo  
The system Sampo [Zuidberg Dos Martires et al., 2019] replaces symbolic 
integration with Monte Carlo sampling. This is useful when the symbolic ma­
nipulation of the formula is too expensive or when it is not possible because 
the resulting integral needs numeric integration, as in Example 110. In these 
cases Monte Carlo methods serve to approximate integrals with summations, 
as the next theorem shows. 
Theorem  20  (Monte Carlo approximation of WMI [Zuidberg Dos Martires 
et al., 2019]). Let  rjJ  be  an  SMT(RA)  theory,  w  a  factorizable  weight  
function  over  the  Boolean  variables  B and  continuous  variables  X. Further­
more,  let  AMC( rp,  aiXB  u  B),  where  Bx  is  the  set  of abstraction  of atomic  
formulas,  evaluate  to  (w,  V(w)).  Then  the  Monte  Carlo  approximation  of  
WMI(rp,  wiX,  B) is  given  by:  
1 N  
WMIMc(r/J,wiX,B)  =-l.:  w(xi) 
N  i=l 
where  the  Xi 's  are  N  independent  and  identically  distributed  values  for  the  
continuous  random  variables  drawnfrom  the  density  w.  
The formula obtain by knowledge compilation must thus be evaluated N  
times, each time with different values for the real variables. This shows an ad­
vantage ofknowledge compilation: the formula is complied once with an ex­

308 
lnference  for  Hybrid  Programs  
pensive process 
and then it is 
evaluated often with a cheap 
computation. 
Moreover, the arithmetic circuit resulting from knowledge compilation 
can be transformed into a computation graph to be handled by TensorFlow 
[Abadi et al., 2015], a numerical computation library specialized for manip­
ulating computation graphs over tensors. In this case the labels of literals in 
the compiled formula must be expressed as tensors. 
Sampo implements Algorithm 19. Sampo uses TensorFlow for numer­
ical computation and the Edward library [Tran et al., 2016] for sampling 
variables. 
Algorithm 19 Function SAMPO: Monte Carlo computation of WMI [Zuid­
berg Dos Martires et al., 2019] 
1: function SAMPO(<)J) 
2: 
Abstract all the atomica formulas in <P  in order to obtain <Pa  
3: 
Compile <Pa  into a d-DNNF representation <Pcompiled  
4: 
Transform the logic formula <Pcompiled  into an arithmetic circuit AC.p  
5: 
Label the literals in AC.p  according to the labeling function given in Definition 58 
with corresponding tensors 
6: 
Symbolically evaluate AC.p  obtaining (\11, V(\11))  represented as a computation graph 
CG  
7: 
Run the CG N  times, where N  is the number of samples 
8: 
Take the mean ofthe values ofthe N  samples 
9: end function 
Example 111 (Sampo [Zuidberg Dos Martires et al., 2019]). Consider  the  
formula  in  Example  107.  We  sample  N  values  for  the  random  variable  t. 
Suppose  N  = 5 and  the  values  are:  
{12.8, 35.1, 17.6, 22.2, 21.4} 
The  Boolean  variable  no_cool is  mapped  to  a  1D  tensor  whose  entries  are  all  
0.01.  The  arithmetic  circuit  of  Figure  12.2  results  in  the  following  
computation:  

12.3  Approximate  Inference  by  Samplingfor  Hybrid  Programs  309 
WMc  = 
0.01 
[12.8 >  20] 
[12.8 ::::; 30] 
[12.8 >  30] 
0.01 
[35.1 >  20] 
[35.1 ::::; 30] 
[35.1 >  30] 
0.01 
0 
[17.6 >  20] 
0 
[17.6 ::::; 30] 
+  [17.6 >  30] 
0.01 
[22.2 >  20] 
[22.2 ::::; 30] 
[22.2 >  30] 
0.01 
[21.4 >  20] 
[21.4 ::::; 30] 
[21.4 >  30] 
-
0.01 
0 
1 
0 
0 
0.01 
1 
0 
1 
1 
0.01 
0 
0 
0 
1 +  0 
= 
0 
0.01 
1 
1 
0 
0.01 
0.01 
1 
1 
0 
0.01 
where  o is  elementwise  multiplication  of  tensors.  Thus  the  estimate  of  the  
WM!is  
1  5 
WMI MG  = - 2:  W MC,i  = 1.02/5 = 0.204
5 i= l 
The example shows that sampling is embarrassingly parallelizable, thus al­
lowing Sampo to exploit modern GPUs. 
12.3  Approximate  lnference  by  Sampling  for  Hybrid  Programs  
Monte Carlo inference for programs with only discrete variables has the at­
tractive feature that it can be used almost directly for approximate inference 
for hybrid programs. For example, to handle a hybrid clause of the form 
Ci  =  g(X,  Y)  : gaussian(Y,  0, 1) +----- object(X).  
MCINTYRE transforms it into [Riguzzi et al., 2016a; Alberti et al., 2017]: 
g(X, Y)  +----- object(X) ,  sample_gauss(i,  [X], 0, 1, Y).  
Sampies for continuous random variables are stored using asserts as for dis­
crete variables. In fact, predicate sample_gauss/4  is defined by 
sampl e_ gauss (R, VC , _ Mean , _ Variance , S ) : ­
sampled (R, VC , S ),!. 
sampl e_gauss (R, VC , Mean , Va riance , S) :­
gauss (Mean , Va riance , S ), 
assert z (sampl ed (R, VC , S )) 

310 
lnferencefor  Hybrid  Programs  
where gauss( M ean,  Variance,  S)  retums inS a value sampled from a Gaus­
sian distribution with parameters Mean  and Variance.  
Monte Carlo inference for hybrid programs derives his correctness from 
the stochastic Tp  operator: a clause that is ground except for the continuous 
random variable defined in the head defines a sampling process that extracts 
a sample of the continuous variables according to the distribution and pa­
rameters specified in the head. Since programs are range restricted, when the 
sampling predicate is called, all variables in the clause are ground except for 
the one defined by the head, so the Tp  operator can be applied to the clause 
to sample a value for the defined variable. 
Monte Carlo inference is the most common inference approach also for 
imperative or functional probabilistic programming languages, where a sam­
ple of the output of a program is taken by executing the program and gen­
erating samples when a probabilistic primitive is called. In probabilistic pro­
gramming usually memoing is not used, so new samples are taken each time 
a probabilistic primitive is encountered. 
Conditional inference can be performed in a similar way by using re­
jection sampling or Metropolis-hastings MCMC, unless the evidence is on 
ground atoms that have continuous values as arguments. In this case, rejec­
tion sampling or Metropolis-hastings cannot be used, as the probability of 
the evidence is 0. However, the conditional probability of the query given 
the evidence may still be defined, see Section 1.5. In this case, likelihood  
weighting  [Nitti et al., 2016] can be used. 
For each sample to be taken, likelihood weighting samples the query and 
then assigns a weight to the sample on the basis of evidence. The weight 
is computed by deriving the evidence backward in the same sample of the 
query starting with a weight of one: each time a choice should be taken or a 
continuous variable sampled, if the choice/variable has already been sampled, 
the current weight is multiplied by the probability of the choicelby the density 
value of the continuous variable. 
Then the probability of the query is computed as the sum of the weights of 
the samples where the query is true divided by the total sum of the weights of 
the samples. This technique is useful also for non-hybrid programs as samples 
are never rejected, so sampling can be faster. 
Likelihood weighting in cplint [Alberti et al., 2017; Nguembang Fadja 
and Riguzzi, 2017] uses a meta-interpreter that randomizes the choice of 
clauses when more than one resolves with the goal, in order to obtain an 
unbiased sample ofthe query. This meta-interpreter is similar to the one used 
to generate the first sample in Metropolis-hastings. 

12.4  Approximate  lnference  with  Bounded  Error for  Hybrid  Programs  311 
Then a different meta-interpreter is used to evaluate the weight of the 
sample. This meta-interpreter starts with the evidence as the query and a 
weight of 1. Each time the meta-interpreter encounters a probabilistic choice, 
it first checks whether a value has already been sampled. If so, it computes 
the probability/density of the sampled value and multiplies the weight by it. 
If the value has not been sampled, it takes a sample and records it, leaving the 
weight unchanged. In  this way, each sample of the query is associated with a 
weight that refiects the infiuence of evidence. 
In some cases, likelihood weighting encounters numerical problems, as 
the weights of samples may go rapidly to very small numbers that can be 
rounded to 0 by fioating point arithmetic. This happens, for example, for 
dynamic models, where predicates depend on time and we have evidence for 
many time points. In  these cases, particle  jiltering  can be used [Nitti et al., 
2016], which periodically resamples the individual samples/particles so that 
their weight is reset to 1. 
In particle filtering, the evidence is a list of literals. A number n  of sam­
ples of the query are taken that are weighted by the likelihood of the first ele­
ment of the evidence list. Each sample constitutes a particle and the sampled 
random variables are stored away. 
After weighting, n  particles are resampled with replacement with a prob­
ability proportional to their weight. Specifically, the weights of the previous 
n  particles are normalized. Let Wi  be the normalized weight of particle Si.  
Each of the new n  particles is sampled from the set of previous particles 
with particle Si  selected with probability Wi.  After each sample, the sampled 
particle is replaced back into the set so that the same particle can be sampled 
repeatedly. 
After resampling, the next element of the evidence is considered. A new 
weight for each particle is computed on the basis of the new evidence element 
and the process is repeated until the last evidence element is reached. 
12.4  Approximate  lnference  with  Bounded  Error  for  Hybrid  
Programs  
Michels et al. [2016] present the Iterative hybrid probabilistic model counting 
(IHPMC) algorithm for computing bounds on queries to hybrid programs. 
They consider both the EVID and COND tasks for the PCLP language (see 
Section 4.5). IHPMC builds trees that split the variables domains and builds 
them to an increasing depth in order to achieve the desired accuracy. 

312 lnference  for  Hybrid  Programs  
A HPT is a binary tree where each node n  is associated with a propo­
sitional formula I{Jn  and a range denoted by range(n,  X) for each random 
variable X. For the root node r,  the range of each variable is its whole range: 
range(r,  X) = Rangex.  Bach non-leaf node n  splits the range of a ran­
dom variable into two parts and the propositional formula of each child is 
obtained from that for the node by conditioning on the split made. Since the 
variables may be continuous, the range of a random variable can be split 
several times along the same tree branch. If the children of n  are { q,  c2}, 
each edge n  ~ Ci, i  E  {1, 2}, is associated with a range Tni  for a random 
variable Y such thatTnl UTn2 = range(n,  Y) andTnl nTn2  = 0.  Moreover, 
range(Ci,  Y) = Tni  and range(ci,  Z) = range(n,  Z) ifY =I=  Z. 
Then the formula !{Je;  associated to Ci is obtained from I{Jn  by imposing a 
restriction on the range on Y and the formula is simplified if some primitive 
constraints can be replaced by T or _L Bach edge n  ~ Ci is associated with 
aprobability Pni  such thatpni =  P(Y  E TniiY  E range(n,  Y)). Leafnodes l  
are those where <pz  =  T or <pz  =  _L 
Given an HPT, the probability of the event represented by the formula 
associated with the root can be computed using Algorithm 6 for computing 
the probability of a BDD. 
Example  112  (Machine diagnosis problern [Michels et al., 2016]). Consider  
a  diagnosis  problern  very  sirnilar  to  the  one  of Exarnple  109  where  a  rnachine  
fails  if the  ternperature  is  above  a  threshold.  If  cooling  fails  (noc  =  true),  
then  the  threshold  is  lower.  This  problern  can  be  rnodeled  with  the  PCLP  
prograrn:  
fail  ~ <t  >  30.0) 
fail  ~ <t  >  20.0), <noc  =  true)  
t  "' gaussian(20.0,  50.0) 
noc"'  {0.01 : true,  0.99 : false}  
The  event  that  the  rnachine  fails  is  then  represented  by  the  forrnula  
(noc  = true  At  >  20.0) v  t  >  30.0 
The  HPT for  this  forrnula  is  shown  in  Figure  12.3.  
The  probability  of fails  is  then  
P(fails)  = 0.9772 · 0.01 · 0.4884 +  0.0228 ~ 0.0276 
In Bxample 112, we were able to compute the probability ofthe query exactly. 
In general, this may not be possible because the query event may not be 
representable using hyperrectangles over the random variables. In this case, 

12.4  Approximate  lnference  with  Bounded  Error for  Hybrid  Programs  313 
(noc  =  true A  t  >  20.0) v t  >  30.0 
t  E (-00,30] 
/ 
~
t  E (30,00) 
0.9772/ 
~-0228 
noc  =  true A  t  >  20.0 
T 
{<-•··'} / 
~
noce {true} 
noc  e '7  
~ 0.01 
.L  
t  >  20.0 
t  E  (-00,20] 
/ 
~
t  E  (20,30] 
0.5116/ 
~.4884 
.L  
T 
Figure  12.3  HPT for Example 112. From [Michels et al., 2016] .. © 
[2016] Association for the Advancement of Artificial Intelligence. All rights 
reserved. Reprinted with permission. 
the propositional formulas may never simplify to l_  or T.  However, we can 
consider PHPTs, HPTs where not allleaves are l_  or T.  From a PHPT, we 
can obtain a lower and an upper bound on the probability of the query: the T 
leaves in the tree contribute to the lower bound P  ( q)  and all the leaves of the 
tree except the l_  ones contribute to the upper bound P  ( q).  
Example  113  (Machine diagnosis problern- approximate inference- [Michels 
et al., 2016]). Consider  the  program:  
fail  +- <t  >  l)  
t  ~ gaussian(20.0,  50.0) 
l  ~ gaussian(30.0,  50.0) 
The  event  that  the  machine fails  is  then  represented  by  the  formula  t  >  l  and  
the  PHPT for  this formula  is  shown  in  Figure  12.4.  
The  lower  bound  of the  probability  of fails  is  then  
P(fails)  = 0.9772 °  005 
Since  the  only  l_  leaf has  probability  000228 °  005, then  
P(fails)  = 1 - 000228 °  005 
Given a precision E, IHPMC builds a PHPT suchthat P(q)  - P(q)  ~ 
E and P(q)  - P(q)  ~ Eo To do so, the tree must be built to a sufficient 
deptho 
To compute bounds on conditional probabilities, IHPMC needs to com­
pute bounds for q  1\  e  and ---.q  1\  e  where q  is the query and e  the evidenceo 
IHPMC computes the bounds using two PHPTso The bounds on the condi­
tional probability are those given by equations (406) and (407) of Section 405010 

314 
lnference  for  Hybrid  Programs  
t  >  l  
tE (-oo,20] 
/ 
~
tE (20,00) 
0.5/ 
~-5 
t  >  l  
t  >  l 
l 
E (-00,20]/ \l 
E (20,00) l 
E (-00,20]/ \l 
E (20,00)
0.9772 
0.0228 
0.9772 
0.9772
t>l  
t>l 
_!_ 
TtE (20, 23.372]/ \t E (23.372, oo) 
0.5 
0.5 
t  >  l  
t  >  l  
Figure  12.4  PHPT for Example 113. From [Michels et al., 2016]. © 
[2016] Association for the Advancement of Artificial Intelligence. All rights 
reserved. Reprinted with permission. 
Arbitrary precision can still be achieved by building the trees to a sufficient 
depth. 
When building trees, IHPMC uses heuristics for selecting the next node to 
expand, the variable to be used for splitting, and the partitioning of the range 
of the variable. IHPMC expands the leaf node with the highest probability 
mass. Variables occurring more frequently in the formula are preferred for 
splitting, unless a different choice may eliminate a primitive constraint. For 
partitioning continuous variables, split points that can lead to a simplification 
of the formula are selected. If there are no such points, a partitioning is chosen 
that splits evenly the probability of the node. 
12.5  Approximate  lnference  for  the  DISTR  and  EXP  Tasks  
Monte Carlo inference can also be used to solve DISTR and EXP tasks, 
i.e., computing probability distributions or expectations over arguments of 
queries. In  this case, the query contains one or more variables. In the DISTR 
task, we record the values of these arguments in successful samples of the 
query. If the program is range restricted, queries succeed with all arguments 
instantiated, so these values are guaranteed to exist. For unconditional infer­
ence or conditional inference with rejection sampling or Metropolis-hastings, 
the result is a list of terms, one for each sample. For likelihood weighting, 
the results is a list of weighted terms, where the weight of each term is the 
sample weight. 

12.5  Approximate  lnferencefor  the  DISTR  and  EXP  Tasks  315 
From these lists, approximations of probability distributions or proba­
bility densities can be built, depending on the type of values, discrete or 
continuous, respectively. For an unweighted list of discrete values, the prob­
ability of each value is the number of occurrences of that value in the list, 
divided by the total number of values. For a weighted list of discrete values, 
the probability of each value is the sum of the weights of each occurrence of 
the value in the list, divided by the sum of all the weights. 
For an unweighted list of continuous values, a line plot of the probability 
density function can be drawn by dividing the domain of the variable in a 
number of intervals or bins. The function then has a point for each interval, 
whose y  value is the number of values in the list that fall in the interval, 
divided by the total number of values. For a weighted list of continuous 
values, the y  value is the sum of the weights of each value in the list that 
falls in the interval, divided by the sum of all the weights. A bar plot can be 
drawn similarly. 
Note that, if likelihood weighting is used, the set of samples without the 
weight can be interpreted as the density of the variable prior to the observa­
tion of the evidence. So we can draw a plot of the density before and after 
observing the evidence. 
The EXP task can be solved by first solving the DISTR task, where we 
collect a list of numerical values, possibly weighted. Then the required value 
can be computed as a (weighted) mean. For unweighted samples, this is 
given by 
E(qie) = 2::~=1 Vi  
n  
where [ v1, ... , vn]  is the list of values. For weighted samples, it is given by 
"'n  w·  ·  Vi 
L..li=1 
2 
E(qle) =  
2::~=
Wi 
1 
where [ ( v1, w1), ... , ( Vn,  Wn)]  is the list of weighted values, with Wi  the 
weights. 
cplint [Alberti et al., 2017; Nguembang Fadja and Riguzzi, 2017] of­
fers functions for performing both DISTR and EXP using sampling, rejection 
sampling, Metropolis-hastings, likelihood weighting, and particle filtering, 
see Section 15.1. 
Notice that a query may retum more than one value for an output argu­
ment in a given world. The query retums a single value for each sample only 
if the query predicate is determinate in each world. A predicate is determinate  

316 
Inferencefor  Hybrid  Programs  
if, given values for input arguments of a query over that predicate, there is a 
single value for output arguments that makes the query true. The user should 
be aware of this and write the program so that predicates are determinate, or 
otherwise consider only the first value for a sample. Or he may be interested 
in all possible values for the output argument in a sample, in which case a 
call to findall / 3  should be wrapped around the query and the resulting list of 
sampled values will be a list of lists of values. 
If the program is not determinate, the user may be interested in a sampling 
process where first the world is sampled, and then the value of the output 
argument is sampled uniformly from the set of values that make the query 
true. In this case, to ensure uniformity without computing first all values, 
a meta-interpreter should be used that randomizes the choice of clauses for 
resolution, such as the one used by cp1 i n t  in likelihood weighting. 
Programs satisfying the exclusive-or assumption, see Chapter 8, are de­
terrninate, because, in program satisfying this assumption, clauses sharing 
an atom in the head are mutually exclusive, i.e., in each world, the body 
of at most one clause is true. In fact, the semantics of PRISM, where this 
assumption is made, can also be seen as defining a probability distribution 
over the values of output arguments. 
Some probabilistic logic languages, such as SLPs (see Section 2.10.1), di­
rectly define probability distributions over arguments rather than probability 
distributions over truth values of ground atoms. Inference in such programs 
can be simulated with programs under the DS by solving the DISTR task. 
Example  114  (Generative model). 
The  following  program2  encodes  the  
modelfrom  [Goodman  and  Tenenbaum,  2018]  for  generating  randomfunc­
tions:  
eval (X, Y)  : -
random_fn (X, O, F ),  Y  is  F .  
op  ( + )  :  0 . 5 ; 
op ( - )  :  0 . 5 . 
random_fn (X, L , F )  : -
comb (L ),  random_fn (X, 1 (L ), F1 ),  
random_fn (X, r (L ), F2 ),  op (Op ),  F = . . [Op, F1 , F2 ] .  
random_fn  (X,  L ,  F )  : -
\  +comb (L ), base_random_fn (X,  L , F ).  
comb  (_ )  :  0. 3 .  
base_random_fn (X, L , X )  : ­
identity (L ).  
base_random_fn  (_ ,  L ,  C)  : ­
\  + ident i  ty ( L ),  
random_const (L , C).  
identity (_ )  : 0 . 5 .  
random_const (_ , C) :  discrete (C,  [ 0 : 0 . 1 , 1 : 0 . 1 , 2 : 0 . 1 ,  
3 : 0 . 1 , 4 : 0 . 1 , 5 : 0 . 1 ,  6 : 0 . 1 ,  7 : 0 . 1 , 8 : 0 . 1 , 9 : 0 . 1 ] ).  
2 https://cplint.eu/e/arithm.pl 

12.5  Approximate  lnference  for  the  DJSTR  and  EXP  Tasks  317 
[3] 
[4] 
[6] 
[2] 
[5] 
[1] 
[0] 
[7]  r   
I   
0 
50 
100 
150 
200 
250 
300 
350 
400 
450 
500 
Figure  12.5  Distribution of sampled values in the Program of Example 114. 
A  randomfunction  is  eitheran  operator  ("+"  or  "- ")  applied  to  two  random  
functions  or  a  base  random  function.  A  base  random  function  is  either  an  
identity  or  a  constant  drawn  uniformly  from  the  integers  0, ... , 9. 
You  may  be  interested  in  the  distribution  of all  possible  output  values  of  
the  randomfunction  with  input  2  given  that  thefunction  outputs  3  for  input  1.  
If  we  take  1000  samples  with  Metropolis-hastings,  we  may  get  the  bar  
plot  of  the  frequencies  of  the  sampled  values  shown  in  Figure  12.5.  Since  
each  world  of  the  program  is  determinate,  there  is  a  single  value  of  Y  that  
makes  eval  ( 2 ,  Y)  true  in  each  world  and  the  list  ofvalues  in  each  sampled  
world  contains  a  single  element.  
Example  115  (Gaussian mixture- sampling arguments- cp l int). Exam­
ple  55  encodes  a  mixture  of two  Gaussians  with  the  program3  that  we  report  
below  
heads : 0.6; tails : 0.4 .  
g (X) :  gaussian (X, O,  1 ).  
h (X) :  gaussian (X, 5,  2 ).  
3 https://cplint.eu/e/gauss_mean_est.pl 

318 
Inferencefor  Hybrid  Programs  
800­
800­
§ <00­
200­
o­
10 
Figure  12.6  Distribution of sampled values from the Gaussian mixture of 
Example 115. 
mix  ( X )  
heads ,  g (X ).  
mix  (X )  
tails ,  h  (X ).  
lf we  take  10000  samples  of argument  X  of mi x  (X ),  we  may  get  the  distri­
bution  of values  that  is  shown  in  Figure  12.6.  

13   
Parameter  Learning  
This chapter discusses the problern of learning the parameters of probabilistic 
logic programs with a given structure. We are given data, in the form of 
ground atoms or interpretations, and a probabilistic logic program, and we 
want to find the parameters of the program that assign maximum probability 
to the examples. 
13.1  PRISM  Parameter  Learning  
The PRISM system included a parameter learning algorithm since the orig­
inal article [Sato, 1995]. The learning task considered there is given in the 
following definition. 
Definition  60  (PRISM parameter learning problem). Given  a  PR/SM  pro­
gram  P  and  a  set  of examples  E  = {e1, ... , er} which  are  ground  atoms,  
find  the  parameters  of  msw  facts  so  that  the  likelihood of  the  atoms  L  = 
Tii=l P( et) is  maximized.  Equivalently,  find  the  parameters  of msw facts  so  
that  the  Log  likelihood  (LL)  ofthe  atoms  LL  = L.i'= 
1 log P(et) is  maximized.  
Example  116  (Bloodtype - PRISM [Sato et al., 2017]). The  following  pro­
gram  
values (gene ,  [ a , b , o ]).  
bloodtype (P )  :­
genotype  (X ,  Y),  
(  X= Y  - >  P = X  
;  X= o  - >  P = Y  
;  Y= o  - >  P = X  
;  P =ab  
)  .  
genotype  (X ,  Y)  
msw (gene , X ), msw (gene , Y).  
319 

320 
Parameter  Learning  
encodes  how  a  person 's  blood  type  is  determined  by  his  genotype,  formed  by  
a  pair of two  genes  ( a,  b  or  o).  
Learning  in  PR/SM  can  be  performed  using  predicate  learn /1  that  
takes  a  Iist  of ground  atoms  (the  examples)  as  argument,  as  in  
? -
learn  ( [  
count (bloodtype (a ), 40 ),  
count (bloodtype (b ), 20 ),  
count (bloodtype (o ), 30 ),  
count (bloodtype (ab ), 10 )  
]).  
where count (At , N)  denotes the repetition  of atom At N times. After pa­
rameter learning,  the  parameters  can  be  obtained  with predicate  show_ sw I  0, 
e.g.,  
?-
show_ sw .  
Switch  gene :  unfixed :  a  (0 . 292329558535712 )  
b  (0.163020241540856)  
0 
(0.544650199923432 )  
These  values  represents the probability distribution  over the values  a,  b ,  and  
o of switch  gene.  
PRISM looks for the maximum likelihood parameters of the msw  atoms. 
However, these are not observed in the dataset, which contains only derived 
atoms. Therefore, relative frequency cannot be used for computing the param­
eters and an algorithm for learning from incomplete data must be used. One 
such algorithm is Expectation maximization (EM) [Dempster et al., 1977]. 
To perform EM, we can associate a random variable X ijl  with values 
D  = {xil , ... , XinJ  to each occurrence l  of the ground switch name iBj  of 
msw( i, X)  with domain D,  with ej  being a grounding Substitution for i. Since 
PRISM learning algorithm uses msw/ 2  instead of m sw/ 3  (the trial identifier 
is omitted), each occurrence of m sw( iBk , x )  represents a distinct random 
variable. PRISM will learn different parameters for each ground switch name 
i BJ.  
The EM algorithm a1ternates between the two phases: 
• Expectation: compute E[Cijk  le] for all examples e,  switches msw( iBj,  x )  
and k  E  {1 , ... , ni},  where Cijk  is the number of times the switch 
m sw(iBj , x )  takes value Xik·  E[ciJk le] is given by 2::1 P(Xijl  =  Xikl e).  

13.1  PRISM  Parameter  Learning  321 
• Maximization: compute IIijk  for all msw(iBj,  x)  and k  = 1, ... , ni  as 
II·. -
.L:eEE  E[cijkle] 
t]k  -
""  
""n;  E[ 
I  ]  
LleEE  Llk=l  
Cijk  e  
Foreach e,  Xijl·  and Xik·  we compute P(Xijl  = Xikle),  the probability 
distribution of Xijl  given the example, with k  E  {1, ... , ni}.  In  this way 
we complete the dataset and we can compute the parameters by relative fre­
quency. If Cijk  is number of times a variable Xijl  takes value Xik  for all l,  
E[cijkle] is its expected value given example e.  If E[cijk]  is its expected 
value given all the examples, then 
T  
E[cijk]  =  2.:: E[cijklet] 
t=l 
and 
E[cijk]  
IIijk  = ""n;  E[ .. ] · 
Llk=l  
Ct]k  
Ifthe program satisfies the exclusive-or assumption, P(Xijl  = Xikle)  can be 
computed as 
P(X·.  -
.  I ) -P(Xijl  =  Xikl  e) -
.L:K-EKe,msw(iOj,Xik)EK- P(K,)  
tJl  -
Xtk  e  -
P(e)  
-
P(e)  
where Ke  is the set of explanations of e  and each explanation ""is a multiset 
of ground msw  atoms ofthe form msw(iBj,  Xik)·  Then 
~ 
.L:K-EK  nijkK-P(K,) 
E[cijkle] = LJ  P(Xijl  = Xikle)  = -----'~e'------'---
l  
P(e)  
where nij kK- is the number oftim es msw ( i() j,  Xik)  appears in mulltiset ""· This 
leads to the naive learning function of Algorithm 20 [Sato, 1995] that iterates 
the expectation and maximization steps until the LL converges. 
This algorithm is naive because there can be exponential numbers of 
explanations for the examples, as in the case of the HMM of Example 74. 
As for inference, a more efficient dynamic programming algorithm can be 
devised that does not require the computation of all the explanations in case 
the program also satisfies the independent-and assumption [Sato and Kameya, 
2001]. Tabling is used to find formulas of the form 
9i  ~ sil  V .•• V Sis;  

322 
Parameter  Leaming  
Algorithm 20 Function PRISM-EM-NAIVE: Naive EM learning in PRISM. 
1: function PRISM-EM-NAIVE(E, P,  E) 
2: 
LL  =   -inf  
3: 
repeat 
4: 
LL0  =  LL  
5:  
for all i, j,  k  do 
C> Expectation step 
E[ .. ] 
"' 
~«EKe nijk«P(~<)
6: 
c,Jk  ~ LJeEE 
P(e)  
7: 
end for 
8: 
for all i, j,  k  do  
C> Maximization step 
E[cijkl
9:  
II ijk  +---
ni  
~k'~l E[cijk 1 ]  
10: 
end for 
11:  
LL  ~ .l:eEE log P(e)  
12: 
until LL  - LLo  <  E  
13: 
retum LL,  IIijk  for all i, j,  k  
14: end function 
where the 9iS are subgoals in the derivation of an example e  that can be 
ordered as {91, ... , 9m} such that e  =  91 and each Sij  contains only msw  
atoms and subgoals from {9i+  1, ... , 9m}. 
The dynamic programming algorithm computes, for each example, the 
probability P(9i)  of the subgoals {91, ... , 9m}. also called the inside  prob­
ability,  and the value Q(9i),  which is called the outside  probability.  These 
names derive from the fact that the algorithm generalizes the Inside-Outside 
algorithm for Probabilistic context-free grammar [Baker, 1979]. It  also gener­
alizes the forward-backward algorithm used for parameter learning in HMMs 
by the Baum-Welch algorithm [Rabiner, 1989]. 
The inside probabilities are computed by procedure GET-INSIDE-PROBS 
shown in Algorithm 21 that is the same as function PRISM-PROB of Algo­
rithm 5. 
Outside probabilities instead are defined as 
Q(9i)  =  ~P~e~ 
and are computed recursively from i  =  1 to i  =  m  using an algorithm sim­
ilar to Procedure CIRCP of Algorithm 7  for d-DNNF. The derivation of the 
recursive formulas is also similar. Suppose 9i  appears in the ground program 
as 
nn  W   
n1i1 W 
b   
b 
1 +--- 9i  '  
11 
1 +--- 9i  
' 
1il 
nKiK 
nK1  W   
w 
b K  +--- 9i  
, 
K1  
bK  +--- 9i  
, 
KiK  

13.1  PRISM  Parameter  Learning  323 
Algorithm  21  Procedure GET-INSIDE-PROBS: Computation of inside 
probabilities. 
1: 
procedure GET-INSIDE-PROBS(e) 
2: 
for all i, j,  k  do 
3: 
P(msw(iOj,Vk))  +-- IIijk  
4: 
end for 
5: 
fori+-m~ldo 
6: 
P(gi)  +-- 0 
7: 
for j  +-- 1 ~ Si  do 
8: 
Let Sij  be h1, ... , ho  
9: 
10: 
P(gi,  Sij)  +-- TI~=l P(hl)  
P(gi)  +-- P(gi)  +  P(gi,  Sii)  
11: 
end for 
12: 
end for 
13: end procedure 
where g~ik indicates atom 9i  repeated njk  times. In case 9i  is not an msw  
atom, then njk  is always 1. Then 
P(bl)  = P(g? 11 ,  Wn) + ... + P(g~
1
i
1 
, W1h) 
P(bK)  = P(g?K\  WKl)  + ... + P(g~KiK' WKiK)  
and 
P(b1)  = P(gi)n 11  P(Wn)  + ... + P(gi)nlil  P(Wlh)  
P(bK)  = P(gi)nK1 P(WKl)  + ... + P(gi)nKiK PWKiK)  
because P(gi,  Wjk)  =  P(gi)P(Wjk)  for the independent and-assumption. 
Wehave that Q(gl)  =  1 as e  =  91· For i  =  2, ... , m,  we can derive 
Q(gi)  by the chain rule of the derivative knowing that P( e) is a function 

0 
324 
Parameter  Leaming  
of P(b1),  0 
P(bK): 
0, 
oP(e)  oP(g~
11 
' Wn) 
oP(e)  oP(g~KiK' WKiK)  
0 
0
Q(gi)  =  
+  
0
oP(b1)  
oP(g1)  
+  oP(bK)  
oP(gl)  
oP(gi)n11  P(Wu) 
Q(b  )
1 
oP(gi)  
+  
0 
+ 
0 
0 
Q(bK)  oP(gi)nKiK  P(W KiK)  =   
oP(gi)   
Q(bl)nuP(gi)nn-1 P(Wu)  + 
+ 
0 
0 
0 
Q(bK)nKiKP(gi)nKiK-1 P(WKiK)  =  
P(g~
11 
, Wn) 
Q(bl)nu  
P(g~, 
+  
0 
+  
0 
0 
P(  nKiK  W  
) 
Q(b  ) 
gi  
KiK 
l  
_  
K  nKtK  
P(gi)  
­
0 
i1 
P(  n1s  W  ) 
Q(bl)  .2:: n1s  ;( .:  
1s +  
0 
+   
s=1 
9t   
0 
0 
iK  
P(  nK 8  W  )
Q(bK)  .2:: nKs  gi  
, 
Ks   
s=1 
P(gi)   
If we consider each repeated occurrence of 9i  independently, we can obtain 
the following recursive formula 
Q(gl)  
1 
Q(b1) t  
P(gi,  W1s) + 
+ Q(bK)  ~ P(gi,  WKs)  
0 
0 
0
Q(gi)  
s=1 
P(gi)  
s=1 
P(gi)  
that can be evaluated top-down from q  = 91 down to 9mo  
In  fact, in case 9i  is a msw  atom that is repeated njk  times in the body 
of clause bj  ~ g~ik, Wjk  and if we apply the algorithm to each occur­
rence independently, we obtain the following contribution from that clause 
to Q(gi):  
Q(b  ·) P(g~jk, Wjk)  + 
+ Q(b  ·) P(g~jk, Wjk)  = Q(b  ·) . P(g~jk, Wjk)  
J  
P(gi)  
J  
P(gi)  
J  nJk  
P(gi) 
0 
0 
0 
n  "k  
because the term Q(bj)  P(g~(;,~jk) is repated njk  timeso 

---
13.2  LLPAD  and  ALLPAD  Parameter  Learning  325 
Procedure GET-ÜUTSIDE-PROBS of Algorithm 22 does this: for each 
subgoal bk  and each ofits explanations 9i,  Wks·  it updates the outside proba­
bility Q(gi)  for the subgoal 9i·  
If 9i  = msw(iBj,  xk),  we can divide the explanations for e  into two 
sets, Kel• that includes the explanations containing msw(iBj,  xk),  and Ke2· 
that includes the other explanations. Then P(e)  = P(Kel)  +  P(Ke2)  and 
"  
nijkKP(K-)  s·  
h  
1  
. 
.  
K  
.
E[ Cijk  I e ]  = LlK-EKel 
P(e)  . mce eac exp anat10n m 
el contams 9i  = 
msw(iBj,xk),  Kel  takes the form {{g~\ Wl}, ... , {g~·, Ws}} where nj  is 
the multiplicity of 9i  in the various explanations. So we obtain 
1 
E[cijkle] = P(e)  
l.:  
nP(gi)n P(W)  = 
{gf,W}EKel 
~~~i? 
n  L.:  
nP(gi)n-l P(W)  =  
{9;  ,W}EKel 
P(gi)  L.:  oP("')  =   
P(e)  K-EKel oP(gi)   
P(gi)  oP(Kel)  
(13.1)
P(e)  oP(gi)  
P(gi)  oP(Ke)  
P(e)  oP(gi)  
P(gi)  oP(e) 
P(e)  oP(gi)  
Q(gi)P(gi)  
P(e)  
where equality (13.1) holds because 
8:~~:)) =  0. 
Procedure PRISM-EXPECTATION of Algorithm 23 updates the expected 
values of the counters. Function PRISM-EM of Algorithm 24 implements 
the overall EM algorithm [Sato and Kameya, 2001]. The algorithm stops 
when the difference between the LL of the current and previous iteration 
drops below a threshold E. 
Sato and Kameya [2001] show that the combination of tabling with Al­
gorithm 24 yields a procedure that has the same time complexity for pro­
grams encoding lllv:IMs and PCFGs as the specific parameter learning al­
gorithms: the Baum-Welch algorithm for HMMs [Rabiner, 1989] and the 
Inside-Outside algorithm for PCFGs [Baker, 1979]. 

326 
Parameter  Leaming  
Algorithm 22 Procedure GET-ÜUTSIDE-PROBS: Computation of outside 
probabilities. 
1: 
procedure GET-ÜUTSIDE-PROBS(e) 
2: 
Q(gl) ~ 1.0 
3: 
for i  ~ 2--+ m  do 
4: 
Q(gi)  ~ 0.0 
5: 
for j  ~ 1 --+ Si  do 
6: 
Let Sij  be h1, ... , ho  
7: 
for l  ~ 1 --+ o  do 
8: 
Q(h!)  ~ Q(hl)  +  Q(gi)P(gi,  Sii)/P(hl)  
9: 
end for 
10: 
end for 
11: 
end for 
12: end procedure 
Algorithm 23 Procedure PRISM-EXPECTATION. 
1: function PRISM-EXPECTATION(E) 
2: 
LL  =  0 
3: 
for alle E  E  do 
4: 
GET-INSIDE-PROBS(e) 
5: 
GET-ÜUTSIDE-PROBS(e) 
6: 
for all i, j  do 
7: 
for k  =  1 to ni  do 
8: 
E[cijk] =  E[cijk] +  Q(msw(iBj,Xk))IIijk/P(e)  
9: 
end for 
10: 
end for 
11: 
LL  =  LL  +  logP(e) 
12: 
end for 
13: 
return LL  
14: end function 
13.2  LLPAD  and  ALLPAD  Parameter  Learning  
The systems LLPAD [Riguzzi, 2004] and ALLPAD [Riguzzi, 2007b, 2008b] 
consider the problern of learning both the parameters and the structure of 
LPADs from interpretations. We consider here parameter learning; we will 
discuss structure learning in Section 14.2. 
Definition 61 (LLPAD Parameter learning problem). Given  a  set  
E  =  {(I,pr)II  E  Int2,pr  E  [0, 1]} 

13.2  LLPAD  and  ALLPAD  Parameter  Learning  327 
Algorithm  24  Function PRISM-EM. 
1: 
function PRISM-EM(E, P,  E) 
2: 
LL  =  -inf  
3: 
repeat 
4: 
LL0  =  LL  
5: 
LL  =  PRISM-EXPECTATION(E) 
6: 
for all i, j  do 
7: 
Sum  +-- .l:~!" 1 E[cijk]  
8: 
for k  =  1 to ni  do 
9· 
· 
II.. =  E[Cijk]  
'Jk  
Sum  
10: 
end for 
11: 
end for 
12: 
until  LL  - LLo  <  E  
13: 
return LL,  IIijk for all i, j,  k  
14: end function 
such  that  'L.(I,pr)EE  PI  =  1,  find  the  value  of  the  parameters  of  a  ground  
LPAD  P,  if they  exist,  such  that  
V(I,pi)  E  E: P(I)  =PI·  
E  may  also  be  given  as  a  multiset  E'  of interpretations.  From  this  case,  we  
can obtain a learning problern  of the form  above by computing a probability  
for  each  distinct  interpretation  in  E'  by  relative  frequency.  
Notice that, if V(I,pi)  E E  : P(I)  =PI.  then VI  E Int2  : P(I)  =PI  if 
we define PI  =  0 for those I  not appearing in E,  as P(I)  is a probability 
distribution over Int2  and L.(I,pr)EEPI  =  1. 
Riguzzi [2004] presents a theorem that shows that, if all the pairs of 
clauses of P  that share an atom in the head have mutually exclusive bodies, 
then the parameters can be computed by relative frequency. 
Definition  62  (Mutually exclusive bodies). Ground  clauses  h1 ~ B1 and  
h2 ~ B2 have  mutually  exclusive  bodies  over  a  set  of interpretations  J  if,  
V I  E J,  B1 and  B2 are  not  both  true  in  I.  
Mutual exclusivity of bodies is equivalent to the exclusive-or assumption. 
Theorem  21  (Parameters as relative frequency). Consider  a  ground  locally  
stratijied  LPAD  P  and  a  clause  C  E  P  of the  form  
C  =  h1 : Ih ; h2 : II2 ; ... ; hm  : IIm ~ B.  

328 
Parameter  Leaming  
Suppose  all  the  clauses  ofP  that  share  an  atom  in  the  head  with  C  have  mu­
tually  exclusive  bodies  with  Cover  the  set  ofinterpretations  :1  = {IIP(I)  >  
0}. In  this  case:  
P(hiiB)  = Ili  
This theorem means that, under certain conditions, the probabilities in a 
clause's head can be interpreted as conditional probabilities of the head atoms 
given the body. Since P(hiiB)  = P(hi,  B)/P(B),  the probabilities of the 
head disjuncts of a ground rule can be computed from the probability dis­
tribution P(I)  defined by the program over the interpretations: for a set of 
literals S,  P(S)  = .L:sci  P(I).  Moreover, since VI  E  Int2  : P(I)  = PI.  
then P(S)  = .L:sr;;;;.I  PI·­
In  fact, if the clauses have mutually exclusive bodies, there is no more 
uncertainty on the values of the hidden variables: for an atom in the head of 
a clause to be true, it must be selected by the only clause whose body is true. 
Therefore, there is no need of an EM algorithm and relative frequency can be 
used. 
13.3  LeProblog  
LePrabLag [Gutmann et al., 2008] is a parameter learning system that starts 
from a set of examples annotated with a probability. The aim of LePrabLag 
is then to find the value of the parameters of a ProbLog program so that the 
probability assigned by the program to the examples is as close as possible to 
the one given. 
Definition  63 (LeProbLog parameter learning problem). Given  a  ProbLog  
program  Pandaset  oftraining  examples  E  = {(ei,Pi),  ...  ,(eT,PT)}  
where  et is a ground atom and Pt  E  [0, 1] fort  = 1, ... ,T,  find  the  parameter  
of the  program  so  that  the  mean  squared  error  
1 T  
MSE  =- l:(P(et)- Pt) 2 
T  t=l 
is  minimized.  
To perform learning, LePrabLag uses gradient descent, i.e., it iteratively up­
dates the parameters in the opposite direction of the gradient of M  SE.  This 

13.3  LePrabLag  329 
requires the computation of the gradient which is 
T  
oP(et) 
oMSE  = ~ L.:(P(et)- Pt).  oll 
oll1  
T  t=l 
1  
LeProbLog compiles queries to BDDs; therefore, P( et)  can be computed 
with Algorithm 6. To compute 8:}tt),  it uses a dynamic programming algo­
J  
rithm that traverses the BDD bottom up. In fact 
oP(et)  _ oP(f(X))  
oll1  -
oll1  
where f(X)  is the Boolean function represented by the BDD. f(X)  is 
f(X)  =  Xk  ·  fxk(X)  +  --.xk  ·  f~xk(X) 
where Xk  is the random Boolean variable associated with the root and to 
ground fact llk :: fk,  so 
P(f(X))  =  llk · P(fxk (X)) +  (1 - llk) · P(f~xk (X)) 
and 
oP(f(X))  = P(fxk(X))  _ P(f~xk(X))
oll1  
if k  = j,  or 
oP(f(X))  = ll . oP(fxk(X))  ( _ ll ) . oP(f~xk(X)) 
oll .
 1 
k  
oll .
+ 
k  
oll .
J  
J  
J  
if k  -=!=  j.  Moreover 
oP(f(X))  =  o 
oll1  
if X
does not appear in X. 
1  
When performing gradient descent, we have to ensure that the param­
eters remain in the [0, 1] interval. However, updating the parameters using 
the gradient does not guarantee that. Therefore, a reparameterization is used 
by means of the sigmoid function o-( x)  = 
1+~-x that takes a real value 
x  E  (-oo, +oo) and returns a real value in ( 0, 1). So each parameter is 
expressedas llj =  O"(aj)  and the ajs  are used as the parameters to update. 
Since aj  E  (  -oo, +oo), we do not risk to get values outside the domain. 

330 
Parameter  Leaming  
Algorithm  25 Function GRADIENT. 
1:  function GRADIENT(BDD,j) 
2: 
(val,seen)  +-GRADIENTEVAL(BDD,j) 
3: 
if seen  =  1 then 
4: 
return val·  u(aj)  · (1- o-(aj))  
5: 
eise 
6: 
return 0 
7: 
end if 
8:  end function 
9:  function GRADIENTEVAL(n,j) 
10: 
if n  is the I-terminal then 
11: 
return (1, 0) 
12: 
end if 
13: 
if n  is the 0-terminal then 
14: 
return (0, 0) 
15: 
end if 
16: 
(val(child1 (n)),  seen(child1 (n)))  +-GRADIENTEVAL(child1 (n),  j) 
17:  
( val(childo(n)),  seen(childo(n)))  +-GRADIENTEVAL(childo(n), j) 
18: 
ifvarindex(n)  =  j  then 
19: 
return (val(child1(n))- val(childo(n)),  1)  
20: 
eiseif seen(child1(n))  =  seen(childo(n))  then 
21: 
return (a-( an) ·val( child1 (n)) + (1-a-(an)) ·val( childo(n)),  seen( child1 (n)))  
22: 
eiseif seen(child1(n))  =  1 then 
23: 
return (a-(an)  · val (child1  (n)),  1)  
24: 
eise if seen(childo(n))  =  1 then 
25: 
return ((1- o-(an))  · val(childo(n)),  1)  
26: 
end if 
27: end function 
Given that d~~) = (}  (x)  ·  ( 1 - (}  (x)),  using the chain rule of derivatives, 
weget 
oP(et)  = ( ·). (1 _ 
( ·)) oP(f(X)) 
oa  .  
(}  aJ  
(}  aJ  
oii  .  
J   
J  
LeProbLog dynamic programming function for computing oP)fa(X))  is shown 
J  
in Algorithm 25. GRADIENTEVAL( n, j) traverses the BDD n  and returns two 
values: a real number and a Boolean variable seen  which is 1 if the variable 
Xj  was seen in n.  We consider three cases: 
1.  Ifthe variable ofnode n  is below Xj  in the BDD order, then GRADIEN­
TEVALreturns the probability ofnode n  and seen  =  0. 

13.3  LePrabLag  331 
2.  Ifthe variable ofnode n  is X ,  then GRADIENTEVAL retums seen  
1
= 1 
and the gradient given by the difference of the values of its two children 
val(child1(n))- val(childo(n)).  
3.  Ifthe variable ofnode n  is above X in the BDD order, then GRADIEN­
1  
TEVAL retums O"(an)  · val(child1(n))  +  (1- dan))  · val(childo(n))  
unless Xj  does not appear in one of the sub-BDD, in which case the 
corresponding term is 0. 
GRADIENTEVAL determines which of the cases applies by using function 
varindex( n)  that retums the index of the variable of node n  and by consid­
ering the values seen( child1 (n))  and seen( childo (n))  ofthe children: if one 
of them is 1 and the other is 0, then Xj  is below the variable of node n  and we 
fall in the third case above. If they are both 1, we are in the third case again. 
If they are both 0, we are either in the first case or in third case but Xj  does 
not appear in the BDD. We deal with the latter situation by returning 0 in the 
outer function GRADIENT. 
The overall LeProbLog function is shown in Algorithm 26. Given a 
ProbLog program P  with n  probabilistic ground facts, it  retums the values 
of their parameters. It  initializes the vector of parameters a = (a1 ,  ...  ,  an)  
randomly and then computes an update ~a by computing the gradient. a is 
then updated by substracting ~a multiplied by a leaming rate 'Tl·  
Algorithm  26 Function LEPROBLOG: LeProbLog algorithm. 
1:  function LEPROBLOG(E, P,  k,  TJ) 
2: 
initialize all ai  randomly 
3: 
while not converged do 
4: 
ßa~o 
5: 
fort ~ 1 ---+  T  do 
6: 
find k  best proofs and generate BDDt  for et  
7: 
y  ~ .q,(P(et)- Pt)  
8: 
for j  ~ 1 ---+  n  do 
9: 
derivj  ~GRADIENT(BDDt,j) 
10: 
ßai  ~ ßai  +  y  · derivi  
11: 
end for 
12: 
end for 
13: 
a ~ a -
TJ  • ßa 
14: 
end while 
15: 
return {a-(a1), o 
o 
o ,a-(an)) 
16:  end function 
The BDDs for examples are built by computing the k  best explanations 
for each example, as in the k-best inference algorithm, see Section 1 0.1.2. As 

332 Parameter  Leaming  
the set of the k  best explanations may change when the parameters change, 
the BDDs are recomputed at each iteration. 
13.4  EMBLEM  
EMBLEM [Bellodi and Riguzzi, 2013, 2012] applies the algorithm for per­
forming EM over BDDs proposed in [Thon et al., 2008; Ishihata et al., 2008a,b; 
Inoue et al., 2009] to the problern of learning the parameters of an LPAD. 
Definition  64  (EMBLEM parameter learning problem). Given  an  LPAD  
P  with  unknown  parameters  and  two  sets  E+  =  {  e1, ... , er} and  E-
=  
{er+1, ... , eQ} of  ground  atoms  (positive  and  negative  examples ),  find  the  
value  ofthe parameters  II ofP  that  maximize  the  likelihood  ofthe  examples,  
i.e.,  solve  
r  
Q  
argmaxP(E+, ,..._,ß-) =  argmax n P(et)  n  P("'et)·  
II 
II 
t=l 
t=r+l 
The  predicates  for  the  atoms  in  E+  and  E- are  called  target because  the  
objective  is  to  be  able  to  better  predict  the  truth  value  of atoms for  them.  
Typically, the LPAD P  has two components: a set of rules, annotated with 
parameters and representing general knowledge, and a set of certain ground 
facts, representing background knowledge on individual cases of a specific 
world, from which consequences can be drawn with the rules, including the 
examples. Sometimes, it is useful to provide information on more than one 
world. For each world, a background knowledge and sets of positive and 
negative examples are provided. The description of one world is also called a 
mega-interpretation  or mega-example.  In this case, it is useful to encode the 
positive examples as ground facts ofthe mega-interpretation and the negative 
examples as suitably annotated ground facts (such as neg(a)  for negative 
example a) for one or more target predicates. The task then is maximizing 
the product of the likelihood of the examples for all mega-interpretations. 
EMBLEMgenerates a BDD for each example in E  = {e1, ... , er,"'  
er+l, ... , "'eQ} using PITA. The BDD for an example e  encodes its expla­
nations in the mega-example to which it belongs. Then EMBLEM enters the 
EM cycle, in which the steps of expectation and maximization are repeated 
until the log likelihood of the examples reaches a local maximum. 

13.4  EMBLEM  333 
Xn1 
' '  
X121 
X2n 
.....  
I 
--tb  
Figure  13.1  BDD for query epidemic  for Example 117. From [Bellodi and 
Riguzzi, 2013]. 
Let us now present the formulas for the expectation and maximization 
phases. EMBLEM adopts the encoding of multivalued random variable with 
Boolean random variables used in PITA, see Section 8.6. Let g( i) be the set 
of indexes od such substitutions: 
g( i) =  {j I() j  is a grounding Substitution for clause Ci  } . 
Let Xijk  for k  =  1, ... , ni  - 1 and j  E  g( i) be the Boolean random variables 
associated with grounding Ci()j  of clause Ci  of P  where ni  is the number of 
head atoms of Ci  and jE g(i).  
Example  117  (Epidemie - LPAD - EM). Let  us  recall  Example  94  about  
the  development  of an  epidemic  
C1 
epidemic  : 0.6; pandernie  : 0.3 ~ flu(X),  cold.  
c2  
cold:  0.7. 
c3 
flu(david). 
c4 
flu(robert).  
Clause  C1 has  two  groundings,  both  with  three  atoms  in  the  head,  the  first  
associated  with  Boolean  random  variables  X  111 and  X  112 and  the  latter  with  
X  121 and  X  122· C2 has  a  single  grounding  with  two  atoms  in  the  head  and  
is  associated  with  variable  X211· The  probabilities  are  1r11 = 0.6, 1r12 = 
= 8:~ = 0.75 and  1r21 = 0.7. The  BDD  for  query  epidemic  is  shown  
1 ~:11 
in  Figure  13.1.  
The EM algorithm altemates between the two phases: 
• Expectation: compute E[cikole] and E[ciklle] for all examples e,  rules 
Ci  in P  and k  =  1, ... , ni  -
1, where cikx  is the number of times a 

334 
Parameter  Leaming  
Xn1 
X121 
X2n 
-----~ 
Figure  13.2  BDD after applying the merge rule only for Example 117. 
From [Bellodi and Riguzzi, 2013]. 
variable Xijk  takes value x  for x  E  {0, 1 }, with j  E  g( i). E[Cikx  le] is 
given by .l:jEg(i) P(Xijk  = xle). 
• Maximization: compute 1rik  for all rules Ci  and k  =  1, ... , ni  - 1 as 
.l:eEE E[ciklle]
1rik  =  
.  
.l:eEEE[cikole] +  E[ciklle] 
_  I  ) .  .  
b 
P(Xijk=x,e) 
P(x ijk  - x  e  lS gtven y 
P(e)  
. 
Now consider a BDD for an example e  built by applying only the merge 
rule, fusing together identical sub-diagrams but not deleting nodes. For ex­
ample, by applying only the merge rule in Example 117, the diagram in 
Figure 13.2 is obtained. The resulting diagram, that we call Complete binary 
decision diagram (CBDD), is suchthat every path contains a node for every 
level. 
P( e) is given by the sum of the probabilities of all the paths in the CBDD 
from the root to a 1 leaf, where the probability of a path is defined as the 
product of the probabilities of the individual choices along the path. Variable 
Xijk  is associated with a levell in the sensethat all nodes at that level test 
variable Xijk·  All paths from the root to a leaf pass through a node of levell. 
We can express P(e)  as 
P(e)  = .2:: n 1r(d)  
pER(e)  dEp  
where R( e) is the set of paths for query e  that lead to a 1 leaf, d  is an edge 
of path p,  and 1r(d)  is the probability associated with the edge: if d  is the 
1-branch from a node associated with a variable Xijko  then 1r(d)  =  1rik;  if 

13.4  EMBLEM  335 
d  is the 0-branch from a node associated with a variable Xijk·  then 1r(d) = 
1 - 1rik·  
We can further expand P  ( e) as 
P(e)  = 
1.:  
1rikx  n  
7r(d) n 7r(d) 
nEN(Xijk),pER( e),xE{O,l} 
dEpn,x  
dEpn  
where N(Xijk)  is the set of nodes associated with variable Xijk.  Pn  is the 
portion of path p  up to node n,  pn,x  is the portion of path p  from childx (n)  
to the 1leaf, and 1rikx  is 1rik  if x  = 1 and 1 - 1rik  otherwise. Then 
P(e)  
1.:  
1rikx  n  7r(d) n 7r(d) 
nEN(X;j k  ),Pn ERn ( q),xE{ 0,1} pn,x ERn (q,x)  
dEpn,x  
dEpn  
where Rn (q)  is the set containing the paths from the root to n  and Rn (q,  x)  
is the set of paths from childx (n)  to the 1 leaf. 
To compute P(Xijk  = x,  e), we can observe that we need to consider 
only the paths passing through the x-child of a node n  associated with vari­
able Xijk.  so 
P(Xijk  =  x,  e) 
1.:  
1rikx  n 7r(d) n 7r(d) 
nEN(Xijk),pnERn(q),pnERn(q,x)  
dEpn  
dEpn  
We can rearrange the terms in the summation as 
P(Xijk  =  x,  e) 
1.:  
1.:  
1.:  
1rikx  n 7r(d) n 7r(d) 
nEN(Xijk)  PnERn(q)  pnERn(q,x)  
dEpn  
dEpn  
1.:  
1rikx  1.:  n 7r(d) 
1.:  n 7r(d) 
nEN(Xijk)  
PnERn(q)  dEpn  
pnERn(q,x)  dEpn  
1.:  
1rikxF(n)B(childx(n))  
nEN(Xijk)  
where F(n)  is theforward probability  [Ishihata et al., 2008b], the probability 
mass of the paths from the root to n,  while B  (n)  is the backward probability  
[Ishihata et al., 2008b], the probability mass of paths from n  to the 1 leaf. If 
root  is the root of a tree for a query e,  then B(root)  =  P(e).  
The expression 1rikxF(n)B(childx(n))  represents the sum ofthe proba­
bility of all the paths passing through the x-edge of node n.  We indicate with 
ex  (n)  such an expression. Thus 
P(Xijk  = x,  e) = 
1.:  
ex(n)  
(13.2) 
nEN(Xijk)  

336 
Parameter  Leaming  
For the case of a BDD, i.e., a diagram obtained by also applying the deletion 
rule, Equation (13.2) is no Ionger valid since paths where there is no node 
associated with Xijk  can also contribute to P(Xijk  = x,  e). In fact, it is 
necessary to also consider the deleted paths: suppose a node n  associated with 
variable Y  has a level higher than variable Xijk  and suppose that childo(n)  
is associated with variable W  that has a levellower than variable Xijk·  The 
nodes associated with variable Xijk  have been deleted from the paths from n  
to childo(n).  One can imagine that the current BDD has been obtained from 
a BDD having a node m  associated with variable Xijk  that is a descendant 
of n  along the 0-branch and whose outgoing edges both point to childo(n).  
The probability mass ofthe two paths that were merged was e0 (n)(1- 1rik)  
and e0 (n)1rik  for the paths passing through the 0-child and 1-child of m  re­
spectively. The first quantity contributes to P(Xijk  =  0, e) and the latter to 
P(Xijk  =  1, e). 
Formally, let Dezx (X)  be the set of nodes n  such that the level of X  is 
below that of n  and is above that of childx(n),  i.e., X  is deleted between 
n  and childx(n).  For the BDD in Figure 13.1, for example, Del1(X121) =  
{n1}. Del0(X12I)  =  0.  Del1(X22I) =  0.  and Del0(X22I)  =  {n3}. Then 
P(Xijk  = 0, e) = 
.2:: 
e0(n)  +  
nEN(Xijk)  
(1- 1rik)  ( 
.2:: 
e0(n)  +  
.2:: e1(n)) 
nEDel0 (Xijk)  
nEDezl (Xijk)  
1
P(Xijk  =  1, e) 
.2:: 
e (n) +  
nEN(Xijk)  
0
1rik  ( 
.2:: 
e (n)  +  
.2:: e
1 (n)) 
nEDel0(Xijk)  
nEDezl(Xijk)  
Having shown how to compute the probabilities, we now describe EMBLEM 
in detail. The typical input for EMBLEM will be a set of mega-interpretations, 
i.e., sets of ground facts, each describing a portion of the domain of interest. 
Among the predicates for the input facts, the user has to indicate which are 
target predicates: the facts for these predicates will then form the examples, 
i.e., the queries for which the BDDs are built. The predicates can be treated 
as closed-world or open-world. In the first case, a closed-world assumption 

13.4  EMBLEM  337 
is made, so the body of clauses with a target predicate in the head is resolved 
only with facts in the interpretation. In the second case, the body of clauses 
with a target predicate in the head is resolved both with facts in the interpre­
tation and with clauses in the theory. If the last option is set and the theory is 
cyclic, EMBLEM uses a depth bound on the derivations to avoid going into 
infinite loops, as proposed by [Gutmann et al., 2010]. 
EMBLEM, shown in Algorithm 27, consists ofa cycle in which the proce­
dures EXPECTATION and MAXIMIZATION are repeatedly called. Procedure 
EXPECTATION retums the LL of the data that is used in the stopping crite­
rion: EMBLEM stops when the difference between the LL of the current and 
previous iteration drops below a threshold E or when this difference is below 
a fraction 6  of the current LL. 
Procedure EXPECTATION, shown in Algorithm 28, takes as input a list 
of BDDs, one for each example, and computes the expectation for each one, 
i.e., P( e,  Xijk  =  x)  for all variables Xijk  in the BDD. In the procedure, 
we use TJx  (i, k)  to indicate .l:jEg(i) P( e,  Xijk  =  x ). EXPECTATION first calls 
GETFORWARD and GETBACKWARD that compute the forward, the backward 
probability of nodes and TJx  ( i, k)  for non-deleted paths only. Then it updates 
TJx  ( i, k)  to take into account deleted paths. 
Algorithm  27 Function EMBLEM. 
1: 
function EMBLEM(E, P,  E, 8)  
2: 
build BDDs  
3: 
LL  =  -inf  
4:  
repeat 
5: 
LLo  =  LL  
6: 
LL  =  EXPECTATION(BDDs) 
7: 
MAXIMIZATION 
8: 
until LL  - LLo  <  E  v LL  - LLo  <  - LL  · 8  
9: 
return LL, 1rik  for all i, k  
10: end function 
Procedure MAXIMIZATION (Algorithm 29) computes the parameters val­
ues for the next EM iteration. 
Procedure GETFORWARD, shown in Algorithm 30, computes the value of 
the forward probabilities. It  traverses the diagram one level at a time starting 
from the root level. For each level, it considers each node n  and computes 
its contribution to the forward probabilities of its children. Then the forward 
probabilities of its children, stored in table F,  are updated. 
Function GETBACKWARD, shown in Algorithm 31, computes the back­
ward probability of nodes by traversing recursively the tree from the root to 

338 
Parameter  Leaming  
Algorithm  28 Function EXPECTATION. 
1: function EXPECTATION(BDDs) 
2: 
LL  =  0 
3: 
for all BDD  E BDDs  do 
4: 
for all i  do 
5: 
for k  =  1 to n;  - 1 do 
6: 
 
TJO(i,  k)  =  0; TJ 1(i, k)  =  0 
7:  
endfor 
8: 
end for 
9: 
for all variables X do 
10: 
c;(X) =  0 
11: 
end for 
12: 
GETFORWARD(root(BDD))  
13: 
Prob=GETBACKWARD(root(BDD))  
14: 
T=O  
15: 
for l  =  1 to levels(BDD)  do 
16: 
Let X  ij k  be the variable associated with levell 
17: 
18: 
19: 
T  =  T  +  c;(Xijk)  
TJ0(i,  k)  =  TJ0(i,  k)  +  T  x (1- 7r;k) 
 
 
TJ 1(i, k)  =  TJ 1 (i, k)  +  T  X  1rik  
20: 
end for 
21: 
for all i  do 
22: 
for k  =  1 to n;  - 1 do 
23: 
24: 
E[Ciko]  =  E[c;ko] +  TJ0(i,  k)/Prob  
E[Cikl] =  E[cikl] +  TJ1(i, k)/Prob  
25: 
end for 
26: 
end for 
27: 
LL  =  LL  +  Iog(Prob)  
28: 
endfor 
29: 
return LL  
30: end function 
Algorithm  29 Procedure MAXIMIZATION. 
1: procedure MAXIMIZATION 
2: 
for all i  do 
3: 
for k  =  1 to n;  - 1 do 
4·  
· 
7!"· •k  
=  
E[cikll 
E[c;kol+E[c;kl] 
5: 
end for 
6: 
end for 
7: end procedure 
the leaves. When the calls of GETBACKWARD for both children of a node n  
retum, we have all the information that is needed to compute the ex  values 
and the value of TJx  ( i, k)  for non-deleted paths. 
The array <;  stores, for every variable Xijk.  an algebraic sum of ex(n):  
those for nodes in upper levels that do not have a descendant in the level l  
of Xijk  minus those for nodes in upper levels that have a descendant in level 

13.4  EMBLEM  339 
Algorithm  30 Procedure GETFORWARD: Computation of the forward 
probability. 
1:  procedure GETFORWARD(root)  
2: 
F(root)  =  1 
3: 
F(n)  =  0 for all nodes 
4: 
for l  =  1 to levels  do 
C>  levels  is the number of Ievels of the BDD rooted at root  
5: 
Nodes(l)  =  0  
6: 
end for 
7: 
Nodes(1)  =  {root}  
8: 
for l  =  1 to levels  do 
9: 
for all node  E N  odes(l)  do 
10: 
Iet Xijk  be v(node),  the variable associated with node  
11: 
if childo(node)  is uot terminal then 
12: 
F(childo(node))  =  F(childo(node))  +  F(node)  · (1- 1rik)  
13:  
add childo(node)  to Nodes(level(childo(node)))  
e> level(node)  returns the 
Ievel of node  
14: 
end if 
15: 
if child1(node)  is uot terminal then 
16: 
F(child1(node))  =  F(child1(node))  +  F(node)  · 1rik  
17: 
add child1(node)  to Nodes(level(child1(node)))  
18: 
end if 
19: 
end for 
20: 
end for 
21:  end procedure 
l.  In this way, it is possible to add the contributions of the deleted paths by 
starting from the root Ievel and accumulating c;(Xijk)  for the various Ievels in 
a variable T  (see lines 15-20 of Algorithm 28): an ex (n)  value that is added to 
the accumulator T  for the Ievel of Xijk  means that n  is an ancestor for nodes 
in that Ievel. When the x-branch from n  reaches a node in a levell' ~ l,  ex  (n)  
is subtracted from the accumulator, as it is not relative to a deleted node on 
the path anymore, see lines 14 and 15 of Algorithm 31. 
Let us see an example of execution. Consider the program of Example 
117 and the single example epidemic.  The BDD  ofFigure 13.1 (also shown 
in Figure 13.3) is built and passed to EXPECTATION in the form of apointer 
to its root node n1. After initializing the TJ  counters to 0, GETFORWARD is 
called with argument n1. The F  table for n1 is set to 1 since this is the root. 
Then Fis computed for the 0-child, n 2 , as 0 +  1 · 0.4 = 0.4 and n 2 is added 
to N  odes(2),  the set of nodes for the second Ievel. Then Fis computed for 
the 1-child, n3,  as 0 +  1 · 0.6 = 0.6, and n3  is added to Nodes(3).  At the 
next iteration of the cycle, Ievel 2 is considered and node n2 is fetched from 
N  odes (2). The 0-child is a terminal, so it is skipped, while the 1-child is n3  
and its F  value is updatedas 0.6 + 0.4 · 0.6 =  0.84. In  the third iteration, node 

340 
Parameter  Leaming  
Algorithm  31 Procedure GETBACKWARD: Computation of the backward 
probability, updating of TJ  and of <;. 
1: function ÜETBACKWARD(node) 
2: 
if node  is a terminal then 
3: 
return value(node)  
4: 
eise 
5: 
Iet Xijk  be v(node)  
6: 
B(childo(node))  =GETBACKWARD(childo(node))  
7: 
B(child1(node))  =GETBACKWARD(child1(node)) 
8: 
9: 
10: 
11: 
0 
e (node)  =  F(node)  · B(childo(node))  · (1- 7r;k) 
e1(node)  =  F(node)  · B(child1(node))  · 7r;k 
0 
 
0 
TJ (i,  k)  =  TJ0 (i,  k)  +  e (node)  
 
 
TJ 1(i, k)  =  TJ 1(i, k)  +  e1(node)  
12: 
13: 
VSucc  =  succ(v(node))  
C>  succ(X)  retnms the variable following X  
0 
c:;(VSucc)  =  c:;(VSucc)  +  e (node)  +  e1(node)  
in the order 
14: 
15: 
16: 
c:;(v(childo(node)))  =  c:;(v(childo(node)))- e0 (node)  
c:;(v(childi(node)))  =  c:;(v(childi(node)))- e1(node)  
return B(childo(node))  · (1- 7r;k) +  B(child1(node))  · 7r;k 
17: 
endif 
18: end function 
n3  is fetched but, since its children are leaves, F  is not updated. The resulting 
forward probabilities are shown in Figure 13.3. 
Then GETBACKWARD is called on n1. The function calls GETBACK­
WARD(n2) that in turn calls GETBACKWARD(O). This call returns 0 because 
it isaterminal node. Then GETBACKWARD(n2) calls GETBACKWARD(n3) 
that in turn calls GETBACKWARD(1) and GETBACKWARD(O), returning re­
spectively 1 and 0. Then GETBACKWARD(n3) computes e0(n3 )  and e1(n3) 
in the following way: 
e0 (n3)  = F(n3)  · B(O)  · (1- 1r21) = 0.84 · 0 · 0.3 = 0 
e1(n3) = F(n3)  · B(1) · 1r21 = 0.84 · 1 · 0.7 = 0.588. Now the counters 
for clause c2 are updated: 
TJ0 (2, 1) =  0 
TJ 1(2, 1) =  0.588 
while we do not show the update of <; since its value for the Ievel of the 
leaves is not used afterward. GETBACKWARD(n3) now returns the backward 
probability of n3  B(n3)  = 1 · 0.7 +  0 · 0.3 = 0.7. GETBACKWARD(n2) can 
proceed to compute 
e0 (n2) = F(n2)  · B(O)  · (1- 1r11) = 0.4 · 0.0 · 0.4 = 0 
e1(n2) = F(n2)  · B(n3)  · 1r11 = 0.4 · 0.7 · 0.6 = 0.168, 
and "7°(1, 1) =  0, TJ 1(1, 1) =  0.168. The variable following X121 is X2n. 
so <;(X2n) = e0 (n2) +  e1(n2) = 0 +  0.168 = 0.168. Since X121 is also 

13.4  EMBLEM  341 
Xn1 
x121 
X2n 
0.41 
I  
..._> 
I  
[ 
0 
l  
Figure  13.3  Forward and backward probabilities. F  indicates the forward 
probability and B  the backward probability of each node. From [Bellodi and 
Riguzzi, 2013]. 
associated with the 1-child n2, then ~(X2u) = ~(X2u)- e1(n2) = 0. The 
0-child is a leaf so ~ is not updated. 
GETBACKWARD(n2) then returns B(n2)  = 0.7 · 0.6 +  0 · 0.4 = 0.42 to 
GETBACKWARD(n1) that computes e0 (nl)  and e1(n1) as 
e0 (n1) =  F(nl)  · B(n2)  · (1- 1ru) =  1 · 0.42 · 0.4 =  0.168 
1
e (n1) =  F(nl)  · B(n3)  · 1r11 =  1 · 0.7 · 0.6 =  0.42 
1
and updates the TJ  counters as "7°(1, 1) =  0.168, TJ (1, 1) =  0.168 +  0.42 =  
0.588. 
Finally, ~ is updated: 
~(X121) =  e0 (n1) +  1
e (nl) =  0.168 +  0.42 =  0.588 
~(X121) =  ~(X121) - e0 (nl)  =  0.42 
1
~(X2u) =  ~(X2u)- e (nl) =  -0.42 
GETBACKWARD(n1) returns B(n1)  =  0.7 · 0.6 +  0.42 · 0.4 =  0.588 to 
EXPECTATION, that adds the contribution of deleted nodes by cycling over 
the BDD Ievels and updating T.  Initially, T  is set to 0, and then, for variable 
Xn1. T  is updated toT  =  ~(Xnt) =  0 which implies no modification of 
1
TJ 0 (1,  1) and TJ (1, 1). For variable X121. T  is updated toT=  0 +  ~(X121) =  
0.42 and the TJ  tableis modified as 
"7°(1, 1) = 0.168 +  0.42 · 0.4 = 0.336 
"71(1, 1) = 0.588 +  0.42. 0.6 = 0.84 
For variable X2n. T  becomes 0.42 +  ~(X2u) = 
1
0, so TJ 0 (2, 1) and TJ (2, 1) 
are not updated. At this point, the expected counts for the two rules can be 
computed 
E[cno] =  0 +  0.336/0.588 =  0.5714285714 

342 
Parameter  Leaming  
E[c111] = 0 + 0.84/0.588 = 1.4285714286 
E[c21o] = 0 + 0/0.588 = 0 
E[c2u] = 0 + 0.588/0.588 = 1 
13.5  Problog2  Parameter  Learning  
ProbLog2 [Fierens et al., 2015] includes the algorithm LFI-ProbLog [Gut­
mann et al., 2011b] that learns the parameters of ProbLog programs from 
partial interpretations. 
Partial interpretations are three valued interpretations: they specify the 
truth value of some but not necessarily all ground atoms. A partial interpreta­
tion I  = ( h, I  F)  states that the atoms in h  are true and those in I  F  are false. 
A partial interpretation I  = (h, Ip)  can be associated with a conjunction 
q(I)  = 1\aEir  1\  1\aElp  ~a. 
Definition  65 (LFI-ProbLog learning problem). Given  a  ProbLog  program  
P  with  unknownparameters  and  a  set  E  = {I1, ... ,Ir}  ofpartial  interpre­
tations  ( the  examples ),  find  the  value  of the  parameters  II ofP  that  maximize  
the  likelihood  ofthe  examples,  i.e.,  solve  
T  
argmaxP(E) =  argmax n P(q('It))  
TI 
TI 
t=l 
If all interpretations in E  are total and the clauses that share an atom in the 
head have mutually exclusive bodies, then Theorem 21 can be applied and the 
parameters can be computed by relative frequency, see Section 13.2. If some 
interpretations in E  are partial or bodies are not mutually exclusive, instead, 
an EM algorithm must be used that is similar to the one used by PRISM and 
EMBLEM. 
LFI-ProbLog generates a d-DNNF circuit foreachpartial interpretation 
I  
=  (h, Ip)  by using the ProbLog2 algorithm of Section 8.7 with the 
evidence q(I).  
Then it associates a Boolean random variable Xij  with each ground prob­
abilistic fact fi()j·  Foreach example I,  variable XiJ•  and x  E  {0, 1}, LFI­
ProbLog computes P(Xij  = xii). Then it uses this to compute E[cixii], the 
expected value given example I  of the number of times variable Xij  takes 
value x  for any j  in g(i),  the set of grounding Substitutions of k  E[cix] is 
the expected value given all the examples. As in PRISM and EMBLEM, these 

13.6  Parameter  Learning for  Hybrid  Programs  343 
are given by: 
T  
E[Cix] = .2: E[cixiit] 
t=l 
and 
E[cixiit] =  .2: P(Xij  =  xiit). 
jEg(i) 
In  the maximization phase, the parameter 1ri of probabilistic fact fi  can be 
computed as 
E[cil]
7ri  =  ----"----,"...::...---=:..."---....".
E[cw] +  E[cil] 
LFI-ProbLog computes P(Xij  =  xii)  by computing P(Xij  =  x,  I)  using 
Procedure CIRCP shown in Algorithm 7: the d-DNNF circuit is visited twice, 
once bottarn upto compute P(q(I))  and once top down to compute P(Xij  =  
x,  I)  for all the variables Xij  and values x.  Then P(Xij  =  xii)  is given by 
P(Xii=x,I)  
P(I)  
Nishino et al. [2014] extended LFI-ProbLog in ordertoperform sparse 
parameter learning, i.e., parameter learning while trying to reduce the number 
of parameters different from 0 or 1, in order to obtain a simpler program. To 
do so, they add a penalty term to the objective function and use gradient 
descent to optimize the parameters. 
13.6  Parameter  Learning  for  Hybrid  Programs  
Gutmann [20 11] proposes an approach for leaming the parameters of hybrid 
ProbLog programs that is based on the LePrabLag algorithm described in 
Section 13.3. In  particular, Gutmann [2011] shows how the gradient of the 
objective function can also be computed for hybrid ProbLog programs. 
Islam [2012]; Islam et al. [2012a] present a parameter learning algorithm 
for Extended PRISM. The algorithm is based on PRISM's EM procedure and 
involves the computation of the Expected sufficient statistics (ESS) of the 
random variables. The ESS for discrete random variables are represented by 
a tuple of the expected counts of each values of the variable. 
The ESS of a Gaussian random variable X  are the triple 
(ESSx,  ESSx
2 
,  ESscount)  
where the components denote the expected sum, expected sum of squares, 
and the expected number of uses of random variable X,  respectively. 

344  Parameter  Leaming  
ESS can be computed by deriving examples individually. A quicker ap­
proach consists in building a symbolic derivation, computing ESS functions 
instead of plain ESS, and then applying them to each example. The deriva­
tion of ESS functions is similar to the one of success functions for Extended 
PRISM discussed in Section 12.1. 
13.7  DeepProblog  
DeepProbLog [Manhaeve et al., 2018, 2021] combines ProbLog with Neural 
networks (NNs) and is an instance of neuro-symbolic integration, a recent 
approach that aims to combine the advantages of symbolic reasoning with 
those of connectionist methods [d' Avila Garcez et al., 2019]. 
In  fact, NNs, and Deep Learning [Goodfellow et al., 2016] in particular, 
excel at low-level perception, beingable to deal with images, text and speech 
with high accuracy, while systems employing logic and probability perform 
complex symbol manipulation and high-level reasoning. Combining the two 
approaches would thus allow the exploitation of the benefits of both. 
The approach taken by DeepProbLog for such an integration is to min­
imally modify ProbLog in order accomodate NNs that return a probability 
distribution over a finite set: the output of NNs is used to assign probability 
values to atoms. DeepProbLog has the interesting feature that it retains the 
three reasoning mechanisms, NNs, graphical models and logic programming, 
as special cases. Let us illustrate the idea with an example. 
Example  118  (Sum of handwritten digits). Consider  the  problern  of pre­
dicting  the  surn  of  two  handwritten  digits:  we  are  given  two  irnages,  each  
containing  a  handwritten  digit,  and  we  want  to  infer  their  surn.  For  exarn­
ple,  the  irnages  of handwritten  digits  could  be  taken  frorn  the  farnous  MNIST  
dataset  [LeCun,  /998].  
Using  rnachine  learning,  we  can  solve  this  problern  by  giving  to  the  rna­
chine  learning systern a dataset cornposed by pairs  of digit irnages  and a label  
which  is  an  integer  between  0  and  18.  The  systern  will  then  extract  a  rnodel  
for  classifying  pairs  of digits  into  one  of 19  classes.  
Deep  Learning  can  be  applied  to  this  problern  by  using  a  network  that  
takes  as  input  the  two  irnages  and  classifies  thern  into  19  classes.  However,  a  
Deep  Learning  systern  cannot  incorporate  background  inforrnation  regarding  
the  addition  operation,  that  can  be  used  to  guide  learning.  
In  DeepProbLog  you  could  have  a  predicate  addition(X,  Y,  Z),  where  
X  and  Y  are  representation  of irnages  of digits  and  Z  is  an  integer  between  0  

13.7  DeepProbLog  345 
and  18  corresponding  to  the  sum  of these  digits.  Examples  then  take  the  form  
of atoms  like  addition(IJ,  ., 8). The  background  knowledge  on  addition  is  
expressed  by  a  rule  of the  form  
addition(Ix,Jy,Nz)  +-- digit(Ix,Nx),digit(Iy,Ny),Nz  is  Nx  +  
Ny.  
where digit/2 is a predicate  tobe learned based on a  NN ( a  neural predicate)  
that  maps  the  image  of a  digit  I D  to  the  corresponding  natural  number  ND·  
This  approach  improves  over  a  NN  classijier  in  terms  of speed  of conver­
gence  and  of accuracy  of the  model  because  the  NN  needs  to  learn  making  
a  decision  for  the  pair  of  input  digits  ( and  there  are  100  different  possible  
pairs),  whereas  the  DeepProbLog's  neural  predicate  only  needs  to  recognize  
individual  digits  (with  only  10  possibilities).  
Moreover,  the  digit/2  neural  single  digit  classijier  can  be  reused  for  
further  tasks  such  as  addition  ofmulti-digit  numbers  withoutfurther  training.  
Let us now illustrate the DeepProbLog language that is based on neural an­
notated disjunctions. 
Definition  66 (Neural annotated disjunction). A  Neural annotated disjunc­
tion (NAD) has  the  form  
nn(mr,  I,  0,  d)  ::  r(I,  0).  
where  nn  is  a  reserved  predicate,  mr  identifies  a  NN  with  k  inputs  and  n  
outputs  dejining  a  probability  distribution  over  n  classes,  I  = h, ... , Ik  is  
a  sequence  of  input  variables  for  the  NN,  0  is  the  output  variable,  d  = 
d1, ... , dn  is  a  sequence  of ground  terms  (the  classes  of  the  NN)  and  r  is  a  
predicate,  the  neural predicate. 
A  ground NAD has  the  form  
nn(mr,i,dl)::r(i,dl);  ... ; nn(mr,i,dn)::r(i,dn)·  
where  i  = i1, ... , ik  is  a  sequence  of ground  terms  (the  input  to  the  NN)  and  
d1, ... , dn  are  ground  terms  (the  classes  ofthis  NN).  
The nn( mr,  i, dj)  term in a ground NAD can be seen as a function that re­
tums the probability of class dj  when evaluating the network mr  on input i. 
Therefore, from a ground NAD, a normal AD can be obtained by evaluating 
the NN and replacing the term with the computed probability. This normal 
AD is called the instantiated  NAD  of the ground NAD. For example, in the 
digit addition case, we could have the NAD 
nn(m_digit,  [X],  Y,  [0, ... ,9]) :: digit(X,  Y).  
where m_digit  is a network that classifies single digits. To generate the 
grounding we consider input image II and obtain the ground NAD: 

346 
Parameter  Learning  
nn(m_digit,  [JJ],  0) :: digit(m, 0) ; 
nn(m_digit,  [JJ] , 9) :: digit(m, 9)0 
Evaluating this would result in the instantiated NAD: 
po::digit(m,o) ; .. o ;pg::digit(m,9) 
0 
where [po,  0 0 0, pg]  is the output vector of the m_digit  network when evalu­
ated onmo 
There is no constraint on the type of NN except that its output layer re­
turns a probability distributiono So the NN could be convolutional, recurrent, 
Definition  67 (DeepProbLog program)o A  DeepProbLog  program  is  com­
posed  of  a  set  of  probabilistic  facts  F,  a  set  of  NADs  and  a  set  of  rules  
Ro  
From a DeepProbLog program, a ground DeepProbLog program can be ob­
tained by grounding probabilistic and NADso By instantiating the ground 
NADs we get a regularProbLog programo The semantics of the DeepProgLog 
program is the one of this ProbLog pro gram, 
13.7.1  DeepProblog  inference  
A strategy similar to the one of ProbLog2 is used to perform inference in 
DeepProbLog: the program is first grounded with respect to the queryo Then 
the ground NADs are obtained from NADs and ground NADs are instanti­
ated by making a forward pass on the NNs with the ground input. At this 
point, a ground ProbLog programs is obtained and inference can proceed 
as in ProbLog2: the program is rewritten into a propositional formula, the 
formula is compiled and transformed into an arithmetic circuit and the circuit 
is evaluated to compute the probabilityo 
Example  119  (DeepProbLog program for the sum of handwritten digits [Man­
haeve et al., 2021])0 Consider  the  MNIST  additionproblern  of Example  I I  80  
The  DeepProbLog  program  for  the  problern  is  
nn (m_ digit ,  
[ X ] ,  Y,  [0 oo .9] )  :: digit (X, Y)o  
addition (X, Y, Z )  : -
digit (X, N1 ), digit ( Y, N2 ),  Z is  N1 +N2 .  
that  includes  a  NAD  and  the  background  rule  regarding  additiono  
Consider  the  query  addit ion(li), ß , 1)0 Grounding  with  respect  to  the  
query  produces  the  ground  DeepProbLog  program:  
nn  (m_ digit ,  rli)J,  0 ):  : digit  (Ii),  0 );  
nn  (m_ digit ,  rli)J,  1 ):  : digit  {Ii),  1 )  0 
nn  (m_digit ,  rßJ,  0 ):  : digit  rß,  0 );  

13.7  DeepProbLog  347 
nn  (m_ digit ,  rD I ,  1;:  : digit  (ß,  1;.  
addition  iftl,ß,  1 )  : - digit  (m,  0 ),  digit  ., 1 ) .  
addition !ftl,ß, 1 J  : - digit (IJ, 1 J,  digit 8, 0 J.  
As  in  ProbLog2,  only  the  relevant  part  of  the  program  is  grounded,  so  in  
this  case  only  digits  0 and  1 are  kept  as  the  Zarger  digits  cannot  sum  to  1. 
Thus  the  irrelevant  terms  (digit(IJ, 2), ...,digit(IJ, 9) and  digit(ß, 2), 
... , digit(ß,  9))  are  removedfrom  the  ground  NAD.  
Instantiating  the  ground  NADs  into  regular  ADs  could  give:  
0 . 8 ::  digit !ftl, OJ;  0.1  ::  digit !ftl, 1 J.  
0. 2::  digit 8, 0 J;  0 . 6 ::  digit 8, 1 J.  
addition !ftl,ß, 1 J  : - digit (IJ, OJ,  digit 8, 1 J.  
addition !ftl,ß, 1 J  : - digit (IJ, 1 J,  digit 8, 0 J.  
where  the  probabilities  of  the  ADs  do  not  sum  up  to  one  because  of the  re­
moval  of  irrelevant  terms,  even  if  the  NNs  still  assigns  probability  mass  to  
them.  
At this  point  inference  continues  as  in  ProbLog  to  compute  the  probability  
of the  query.  
13.7.2  Learning  in  DeepProbLog  
Learning in DeepProbLog means finding values for the parameters of the 
NNs (which we will call neural parameters) and the parameters in the logic 
program (which we call probabilistic parameters). The same approach of 
LeProbLog of Section 13.3 is followed: the definition ofthe learning problern 
is the one given in Definition 63, where we are given a set E  of atoms each 
annotated with a probability and we want to minimize a loss function such as 
the mean squared error, where the error is the difference between the proba­
bility assigned by the DeepProbLog program to an atom and its probability 
Iabel. 
Often in DeepProbLog only atoms labeled with probability 1 are given, in 
this case the aim is to maximize the probability assigned to the atoms, which 
can be expressed by by minimizing the average negative log likelihood of the 
atoms 
1 
arg min L.:  = arg min lEI .I:  - log Po  (q)  
0  
0  
(q,l)EE 
where L.:  is the loss, 8  is the vector of all parameters and P0 (q)  is the proba­
bility assigned by the DeepProbLog program to the query q.  
Similarly to LeProbLog, gradient descent is used to solve the learning 
problem, so that the neural and probabilistic parameters can be tuned in 

348 
Parameter  Leaming  
an integrated way. The gradient is computed by resorting to aProbLog (see 
Section 11.3) and AMC (see Section 12.2.2) With respect to probabilistic 
parameters, the aProbLog approach is basically unchanged. 
Totune the neural parameters, we must propagate the gradient to the NNs. 
We can use the chain rule of derivatives to compute the partial derivative of 
the loss with respect to a neural parameter ()k:  
d/2  _ ~ .2: oP(q)  oßi  
dBk  - oP(q)  . 
oßi  oBk  
2 
where P  ( q)  is the probability of the output and ß is the vector of probabilities 
of an instantiated NAD whose neural network contains ()k·  The terms oßd ()()k  
are thus the derivatives of the ith component of the neural network's output 
with respect to ()k  and can be computed by backpropagation in the network. 
The terms a:~;) instead can be computed by using aProbLog and the gradient 
semiring. 
In  Section 11.3 we saw the gradient semiring but there it was restricted 
to the case of a single parameter to be tuned. We now present the gradient 
semiring for several parameters. Its elements are tuples (p,  '\lp),  where p  is 
a probability and '\7 p  is the gradient of that probability with respect to (},  the 
vector composed of all the probabilistic parameters and the concatenations of 
the vectors of probabilities ( outputs of the NN) for the instantiated NADs. 
The E8 and ® operations with their neutral elements are a simple general­
ization of those for the single parameter case: 
(a1, a2)  E8 (b1, b2)  = (a1 + b1, a2  + b2)  
(13.3) 
(a1, a2)  ® (b1, b2)  =  (a1b1, b1a2  +  a1b2)  
(13.4) 
effi  = (0, 0) 
(13.5) 
e@  =  (1, 0) 
(13.6) 
The labeling function a  for probabilistic facts (Pi):: fi  is 
a(fi)  = (Pi,  ei) 
(13.7) 
a( "'Ii) =  (1  -Pi,  -ei) 
(13.8) 
where the vector ei has a 1 in the position corresponding to the ith probabilis­
tic parameter and 0 in all others. 
The label for an NAD is: 
a(fj)  =  (:ßj,  ej) 
(13.9) 

13.7  DeepProbLog  349 
where fj  is r(i,  dj)  in a ground NAD ... ; nn(m,  i, dj)  :: r(i,  dj)  ; 
and the vector ej has a 1 in the position corresponding to the jth output of 
the NN of the parameter and 0 in all others. 
In the case where different ground NAD correspond to the same neural 
network, the contributions to the gradient of the different ground NAD are 
summed. 
DeepProbLog has been successfully applied also to the following prob­
lems, besides the MNIST addition problern ofExample 118 [Manhaeve et al., 
2021]: 
• addition of lists of MNIST digit images, where we are given two lists of 
digits and we want to compute the sum of the numbers represented by 
the lists in base 10; 
• program induction, in particular it was used to infer programs in differ­
entiable Forth for addition, sorting and word algebra problems; 
• probabilistic programming, where the aim is to recognize when two 
coins show the same face or to predict the probability of winning in 
a simplified poker game from images of cards; 
• training embeddings, where DeepProbLog is used to indirectly tune em­
beddings for images using soft unification; 
• compositionallanguage understanding, where the aim is to extract logi­
cal facts from naturallanguage sentences. 


14   
Structure  Learning  
The techniques presented in this chapter aim at inducing whole programs 
from data, including both their structure and parameters. 
We first briefly review some concepts from Inductive logic programming 
and then discuss systems for Probabilistic inductive logic programming (PILP) 
[De Raedt et al., 2008; Riguzzi et al., 2014]. 
14.1  lnductive  Logic  Programming  
The ILP field is concerned with learning logic programs from data. One of 
the learning problems that is considered is learning  from  entailment.  
Definition  68 (Inductive Logic Programming - learning from entailment). 
Given  two  sets  E+  = {e1, ... ,er} and  E- = {eT+l, ... ,eQ} ofground  
atoms  (positive  and  negative  examples),  a  logic  program  B  (background  
knowledge),  and  a  space  of  possible  programs  1i  (language  bias),  find  a  
program  P  E  1i  such  that  
•  Ve  E  E+,  PuB  F=  e  (completeness),  
•  Ve  E  E-,  PuB  !rf  e  (consistency),  
An  example  e  such  that  P  u  B  F=  e  is  said  to  be  covered. We  also  define  the  
following  functions  
• covers(P,  e) = true  if B  u P  F=  e,  
• covers(P,  E)  =  {ele E  E,  covers(P,  e) =  true}.  
The  predicates  for  which  we  are  given  examples  are  called  target predicates. 
There  is  often  a  single  target  predicate.  
351 

352 
Structure  Learning  
Example  120  (ILP problem). Suppose  we  have  
E+  =  {  father(john,  mary),jather(david,  steve)  } 
E-
=  {  father(kathy,mary),jather(john,steve)  } 
B  = parent(john,  mary),  
parent( david,  steve),  
parent(kathy,  mary),  
female(kathy),  
male(john),  
male(david)  } 
then  a  solution  of the  learning  from  entailment problern  is  
father(X,  Y)  ~ parent(X, Y),  male( X). 
ILP systems that solve the learning form entailment problern differ in the 
way they search the program space. They are usually based on two nested 
loops, an extemal covering  loop  which adds a clause to the current theory 
and removes the covered positive examples, and an intemal clause  search  
loop  that searches the space of clause. Examples of ILP systems are POIL 
[Quinlan, 1990], mFOIL [Dzeroski, 1993], Aleph [Srinivasan, 2007], and 
Progoi [Muggleton, 1995]. 
The space of clauses is organized in terms of a generality relation that 
directs the search. A clause Cismoregeneral  than D  if covers( {C}, U)  2 
cover s ( { D}, U)  where U  is the set of all ground atoms built over target pred­
icates. If B,  {C} I=  D,  then C  is more general than D  because B,  {D} I=  e  
implies B,  { C} I=  e.  However, entailment is semi-decidable, so simpler gen­
erality relations are used in practice. The one most used is 11-subsumption: C  
11-subsumes  D  (C ?: D)  if there exists a Substitution 11  suchthat CO  <:;  D  
[Plotkin, 1970], where the clauses are intended as sets of literals. If C  ?: D,  
then C  I=  D,  so C  is more general than D.  However, the opposite is not true 
in general, i.e., C  ?: D  ==I?  C  I=  D.  While 11-subsumption is not equivalent 
to entailment, it is decidable (even if NP-complete), so it is chosen as the 
generality relation in practice. 
Example  121  (Examples of theta subsumption). Let  
C1 =  father(X,  Y)  ~ parent(X, Y)  
C2 = father(X,  Y)  ~ parent(X, Y),  male(X)  
c3 =  father(john,  steve)  ~ parent(john,  steve),  male(john)  

14.1  1nductive  Logic  Programming  353 
Then  
• C1 ? C2 with  ()  = 0; 
• C1 ? C3 with  ()  = {Xfjohn,  Y  jsteve};  
• C2 ? C3 with  ()  = {Xfjohn,  Y  jsteve}.  
ILP systems differ in the direction of search in the space of clauses or­
dered by generality: top-down systems search the space from more general 
clauses to less general ones and bottom-up systems do the opposite. Aleph 
and Progoi are examples of top-down systems. In  top-down systems, the 
clause search loop consists of gradually specializing clauses using heuristics 
to guide the search, for example, by using beam  search.  Clause specializa­
tions are obtained by applying a refinement  operator  p  that, given a clause 
C,  returns a set of its specializations, i.e., p(C)  ~ {DID  E  L,  C  ? D}  
where L  is the space of possible clauses that is described by the language  
bias.  A refinement operator usually generates only minimal specializations 
and typically applies two syntactic operations: 
• a substitution, or 
• the addition of a literal to the body. 
In  Progol, for example, the refinement operator adds a literal from the bottarn  
clause  l_  after having replaced some of the constants with variables. l_  is the 
most specific clause covering an example e,  i.e., l_  =  e  ~ Be.  where Be  is 
set of ground literals that are true regarding example e  that are allowed by 
the language bias. In this way, we are sure that, at all times during the search, 
the refinements at least cover example e,  that can be selected at random 
from the set of positive examples. 
In turn, the language bias is expressed in Progoi by means of mode  dec­
larations. Following [Muggleton, 1995], a mode declaration m  is either a 
head declaration modeh(r,  s)  or a body declaration modeb(r,  s),  where s,  
the schema,  is a ground literal and r  is an integer called the recall.  A schema 
is a template for literals in the head or body of a clause and can contain 
special placemarker terms of the form #type,  +type,  and -type,  which 
stand, respectively, for ground terms, input variables, and output variables 
of a type. An input variable in a body literal of a clause must be either an 
input variable in the head or an output variable in a preceding body literal 
in the clause. If M  is a set of mode declarations, L(M)  is the language  of  
M,  i.e., the set of clauses { C  =  h  ~ b1, ... , bm} suchthat the head atom 
h  (resp. body literals bi)  is obtained from some head (resp. body) declaration 

354 
Structure  Learning  
in M  by replacing all # placemarkers with ground terms and all +  (resp. -) 
placemarkers with input (resp. output) variables. 
The bottom clause is built with a procedure called saturation,  shown 
in Algorithm 32. This method is a deductive procedure used to find atoms 
related to e.  
Suppose modeh(r,  s)  is a head declaration suchthat e  is an answer for 
the goal schema(s),  where schema(s)  denotes the literal obtained from s  
by replacing all placemarkers with distinct variables X1, ... , Xn.  The terms 
in e  are used to initialize a growing set of input terms InTerms:  these are 
the terms corresponding to +  placemarkers ins. Then each body declaration 
m  is considered in turn. The terms from In Termsare  substituted into the +  
placemarkers of m  to generate a set Q  of goals. Each goal is then executed 
against the database and up to r  (the recall) successful ground instances (or 
all if r  =  *) are added to the body of the clause. Any term corresponding to a 
- placemarker in m  is inserted in In Terms  if it is not already present. This 
cycle is repeated for an user-defined number N  S  of times. 
The resulting ground clause l_  =  e  ~ b1, ... , bm  is then processed to 
obtain a program clause by replacing each term in a +  or - placemarker with 
a variable, using the same variable for identical terms. Terms corresponding 
to # placemarkers are instead kept in the clause. 
Example  122  (Bottom clause example). Consider  the  learning  problern  of  
Example  120  and  the  language  bias  
modeh(father( +per son,  +per son)).  
modeb(parent( +person,  -person)).  
modeb(parent(#person,  +person)).  
modeb(male( +person)).  
modeb(female (#per son)).  
then  the  bottom  clausefor  father(john,  mary)  is  
father(john,  mary)  ~ parent(john,  mary),  male(john),  
parent( kathy,  mary),  female  (kathy).  
After  replacing  constants  with  variables  we  get  
father(X,  Y)  ~ parent(X, Y),  male(X),  parent(kathy,  Y),  
female  (kathy).  
14.2  LLPAD  and  ALLPAD  Structure  Learning  
LLPAD [Riguzzi, 2004] and ALLPAD [Riguzzi, 2007b, 2008b] were two 
early systems for performing structure learning. They leamed ground LPADs 

14.2  LLPAD  andALLPAD  Structure  Learning  355 
Algorithm  32 Function SATURATION. 
1:  function SATURATION(e, NS)  
2: 
for all types t  do 
3: 
InTerms(t)  =  0  
4: 
end for 
5: 
_l_  =  0   
C>  _i: bottom clause 
6: 
Iet modeh(r,  s) be a mode declaration where schema(s)  unifies with e  
7:  
for all arguments t  of e  do 
8: 
ift corresponds to a +type in modeh(r,  s) then 
9: 
addttolnTerms(type)  
10: 
endif 
11: 
end for 
12: 
Iet _l's head be e  
13: 
repeat 
14: 
Steps  <--- 1 
15: 
for all modeb declarations modeb(r,  s) do 
16:  
for all possible subs. u  ofvariables corresponding to +type in schema( s) by terms from 
InTerms(type)  do 
17: 
for j  =  1 -->  r  do 
18: 
if goal b  =  schema( s)u  succeeds with answer Substitution u'  then 
19:  
for all V  jt  E u'  do 
20:  
if V  corresponds to a -type  then 
21:  
add t  to the set In Terms( type)  ifnot already present 
22:  
endif 
23:  
end for 
24:  
Add b  to _i's body 
25: 
end if 
26: 
end for 
27: 
end for 
28: 
end for 
29: 
Steps  <--- Steps  +  1 
30: 
until Steps  >  N  S  
31: 
replace constants with variables in _l, using the same variable for identical terms 
32: 
return j_  
33:  end function 
from interpretations. We discussed parameter learning in Section 13.2; we 
consider here the problern of learning the structure. 
Definition  69 (ALLPAD Structure learning problem). Given  a  set  
E  = {(I,pi)II  E  Int2,PI  E  [0, 1]} 
suchthat  ~(I,p1 )EEPI 
1,  and  a  space  of possible  programs  S,  find  an  
LPAD  P  E  S  such  that  
Err  = 2.:  IP(I)- PII  
(I,p1 )EE  
is  minimized,  where  P(I)  is  the  probability  assigned  by  P  to  I.  

356 
Structure  Learning  
As for parameter learning, E  may also be given as a multiset E'  of inter­
pretations. 
LLPAD and ALLPAD learn ground programs satisfying the exclusive-or 
assumption, so each pair of clauses that share an atom in the head have mu­
tually exclusive bodies, i.e., not both true in the same interpretation from I.  
The systems perform learning in three phases: they first find a set of 
clause structures satisfying some constraints, then compute the annotations 
of head atoms of such clauses using Theorem 21, and finally solve a con­
straint optimization problern for selecting a subset of clauses to include in the 
solution. 
The first phase can be cast in the framework proposed by [Stolle et al., 
2005] in which the problern of descriptive ILP is seen as the problern of 
finding all the clauses in the language bias that satisfy a number ofconstraints. 
Exploiting the properties of constraints, the search in the space of clauses can 
be usefully pruned. 
A constraint is monotonic  if it is the case that when a clause does not 
satisfy it, none of its generalizations (in the B-subsumption generalization 
order) satisfy it. A constraint is anti-monotanie  if it is the case that when a 
clause does not satisfy it, none of its specializations satisfy it. 
The first phase can be formulated in this way: find all the disjunctive 
clauses that satisfy the following constraints: 
Cl have their body true in at least one interpretation; 
C2 are satisfied in all the interpretations; 
C3 their atoms in the head are mutually exclusive over the set of interpreta­
tions where the body is true (i.e., no two head atoms are both true in an 
interpretation where the body is true); 
C4 they have no redundant head atom, i.e., no head atom that is false in all 
the interpretations where the body is true. 
The systems search the space of disjunctive clauses by first searching depth­
first and top-down for bodies true in at least one interpretation and then, for 
each such body, searching for a head satisfying the remaining constraints. 
When a body is found that is true in zero interpretations, the search along that 
branch is stopped (constraint Cl is anti-monotonic). 
The systems employ bottom-up search in the space of heads, exploiting 
the monotonic constraint C2 that requires the clause to be true in all the 
interpretations for pruning the search. 
The second phase is performed using Theorem 21: given a ground clause 
generated by the second phase, the probabilities of head atoms are given by 

14.3  ProbLog  Theory  Compression  357  
the conditional probability of the head atoms given the body according to the 
distribution p 1.  
In the third phase, the systems associate a Boolean decision variable with 
each of the clauses found in the first phase. Thanks to the exclusive-or as­
sumption, the probability of each interpretation can be expressed as a function 
of the decision variables, so we set Err  as the objective function of an op­
timization problem. The exclusive-or assumption is enforced by imposing 
constraints among pairs of decisions. 
Both the constraints and the objective function are linear, so we can use 
mixed-integer programming techniques. 
The restriction to ground programs satisfying the exclusive-or assumption 
limits the applicability of LLPAD and ALLPAD in practice. 
14.3  Problog  Theory  Compression  
De Raedt et al. [2008] consider the problern of compressing a ProbLog theory 
given a set of positive and negative examples. The problern can be defined as 
follows. 
Definition  70  (Theory compression). Given  a  ProbLog  program  P  contain­
ing  the  set  of probabilistiG  facts  F  = {Ih :: JI,  ...  , IIn  :: fn},  two  sets  
E+  = {e1, ... , er} and  E- = {er+1, ... , eQ}  of ground  atoms  (positive  
and  negative  examples ),  and  a  constant  k  E  N,  find  a  subset  of the  probabilis­
ticfacts  9  ~ F  ofsize  at  most  k  (i.e.,  191  :S;  k)  that  maximizes  the  likelihood  
ofthe  examples,  i.e.,  solve  
r  
Q  
argmax n P(ei)  n  P(-·vei) 
Q.:;F,IQI:;;;k  i=l 
i=r+l  
The aim is to modify the theory by removing clauses in order to maximize the 
likelihood of the examples. This is an instance of a theory  revision  process. 
However, in case an example has probability 0, the whole likelihood would 
be 0. Tobe able to consider also this case, P(e)  is replaced by P(e)  =  
min(E, P(e))  for a small user-defined constant E. 
The ProbLog compression algorithm of [De Raedt et al., 2008] proceeds 
by greedily removing one probabilistic fact at a time from the theory. The fact 
is chosen as the one whose removal results in the largest likelihood increase. 
The algorithm continues removing facts if there are more than k  of them and 
there is a fact whose removal can improve the likelihood. 

358 
Structure  Learning  
The algorithm first builds the BDDs for all the examples. Then it enters 
the removal cycle. Computing the effect of the removal of a probabilistic fact 
fi  on the probability of an example is easy: it is enough to set IIi to 0 and re­
evaluate the BDD using function PROB of Algorithm 6. The value of the root 
will be the updated probability ofthe example. Computing the likelihood after 
the removal of a probabilistic fact is thus quick, as the expensive construction 
of the BDDs does not have to be redone. 
14.4  ProbFOIL  and  ProbFOIL+  
ProbPOIL [De Raedt and Thon, 2011] and ProbPOIL+ [Raedt et al., 2015] 
learn rules from probabilistic examples. The learning problern they consider 
is defined as follows. 
Definition  71 (ProbPOIL/ProbPoil+ learning problern [Raedt et al., 2015]). 
Given  
1.  a  set  of training  examples  E  =  { (  e1, PI),  ...  ,  (er,  PT)}  where  each  ei  
is  a  groundfactfor  a  target  predicate;  
2.  a  background  theory  B  containing  information  about  the  examples  in  
the  form  of a  ProbLog  pro gram;  
3.  a  space  of possible  clauses  12.  
find  a  hypothesis  H  ~ 12  so  that  the  absolute  error  AE  = ~f= 1 IP(ei)- Pi  I  
is  minimized,  i.e.,  
T  
argmin 2.:  IP(ei)- Pil  
Hr::;;;_L  i=l 
The difference between ProbPOIL and ProbPOIL+ is that in ProbPOIL the 
clauses in H  are definite, i.e., of the form h  +--- B,  while in ProbPOIL+ they 
are probabilistic, i.e., of the form x  :: h  +--- B,  with x  E  [0, 1]. Such rules are 
to be interpreted as the combination of 
h  +--- B,prob(id).  
x  :: prob(id).  
where id  is an identifier of the rule and x  :: prob( id)  is a ground proba­
bilistic fact associated with the rule. Note that this is different from an LPAD 
rule of the form h  : x  +--- B,  as this stands for the union of ground rules 
h'  : x  +--- B'.  obtained by grounding h  : x  +--- B.  So LPAD rules generate an 
independent random variable for each of their groundings, while the above 

14.4  ProbFOIL  and ProbFOIL+  359 
ProbFOIL+ rule generates a single random variable independently on the 
number of groundings. 
ProbFOIL+ generalizes the mFOIL system [Dzeroski, 1993], itself a gen­
eralization of FülL [Quinlan, 1990]. It  adopts the standard technique for 
learning sets of rules consisting of a covering loop in which one rule is added 
to the theory at each iteration. A nested clause search loop builds the rule 
by iteratively adding literals to the body of the rule. The covering loop ends 
when a condition based on a global scoring function is satisfied. The con­
struction of single rules is performed by means of beam search as in mFOIL 
and uses a local scoring function as the heuristic. Algorithm 33 shows the 
overall approach 1 . 
Algorithm  33 Function PROBFOIL+. 
1: 
function PROBFOIL+(target) 
2: 
H  +--- 0  
3: 
while true do 
4: 
clause  +--- LEARNRULE(H, target)  
5: 
ifGSCORE(H) <  GSCORE(H u {clause})  1\  SIGNIFICANT(H, clause)  then 
6: 
H +--- H  u  {clause}  
7: 
else 
8: 
return H  
9: 
end if 
10: 
end while 
11: end function 
12: function LEARNRULE(H, target)  
13: 
candidates  +--- {x  :: target  +--- true}  
14: 
best  +--- (x  :: target  +--- true)  
15: 
while candidates  #  0  do 
16: 
next_cand  +--- 0  
17: 
for all x  :: target  +--- body  E candidates  do 
18: 
for all (target  +--- body,  refinement)  E p(target  +--- body)  do 
19: 
ifnot REJECT(H, best,  (x  :: target  +--- body,  refinement))  then 
20: 
next_cand  +--- next_cand  u  {(x  :: target  +--- body,  refinement)}  
21: 
ifLSCORE(H, (x  :: target  +--- body,  refinement))  >  LSCORE(H, best)  then 
22: 
best  +--- (x  :: target  +--- body,  refinement)  
23: 
end if 
24: 
end if 
25: 
end for 
26: 
endfor 
27: 
candidates  +--- next_cand  
28: 
end while 
29: 
return best  
30: end function 
1  The description of ProbFOIL+ is based on [Raedt et al., 2015] and the code at https: 
I /bitbucket.org/antondries/prob2foil 

360 
Structure  Learning  
The global scoring function is the accuracy over the dataset, given by 
TPH  +  TNH 
accuracyH  = 
T  
where T  is the number of examples and TPH  and TN H  are, respectively, 
the number of true  positives  and of true  negatives,  i.e., the number of positive 
(negative) examples correctly classified as positive (negative). 
The local scoring function is an m-estimate  [Mitchell, 1997] of the preci­
sion,  or the probability that an example is positive given that it is covered by 
the rule: 
. 
TPH  +  m-lfN  
m-estlmateH  =  T'n  
F'n 
rH+  rH+m  
where m  is a parameter ofthe algorithm, F  PH  is the number ofjalse positives  
(negative examples classified as positive), and P  and N  indicate the number 
of positive and negative examples in the dataset, respectively. 
These measures are based on usual metrics for rule learning that assume 
that the training set is composed of positive and negative examples and that 
classification is sharp. ProbFOIL+ generalizes this settings as each example 
ei  is associated with a probability Pi·  The deterministic setting is obtained 
by having Pi  = 1 for positive examples and Pi  = 0 for negative examples. 
In the probabilistic setting, we can see an example ( ei,  Pi)  as contributing a 
part Pi  to the positive part of training set and 1 - Pi  to the negative part. So 
in this case, P  = ~[= 1 Pi  and N  = ~[= 1 (1 -Pi)·  Similarly, prediction is 
generalized: the hypothesis H  assigns a probability PH,i  to each example ei,  
instead of simply saying that the example is positive (pH,i  = 1) or negative 
(pH,i  = 0). The number of true positive and true negatives can be generalized 
as well. The contribution tpH;  of example ei  to TP H  will be PHi  ifPi  >PHi  
'(< 
' 
' 
and Pi  otherwise, because if Pi  <  PH,i·  the hypothesis is overestimating ei.  
The contribution fPH,i  of example ei  to FPH  will be PH,i- Pi  if Pi  <  PH,i  
and 0 otherwise, because if Pi  >  PH,i  the hypothesis is underestimating ei.  
Then TPH  = ~f=l tPH,i•  FPH  = ~f=dPH,i• TN H  = N- FPH,  and 
FN H  = P- TPH  as for the deterministic case, where FN His the number 
ofjalse  negatives,  or positive examples classified as negatives. 
The function LSCORE(H, x  :: C)  computes the local scoring function for 
the addition of clause C(x)  = x  :: C  to H  using the m-estimate. However, 
the heuristic depends on the value of x  E  [0, 1]. Thus, the function has to find 
the value of x  that maximizes the score 
TPHuC(x)  +  mPjT 
M  (x)  =  -==------'-::==---­
TPHuC(x)  +  FPHuC(x)  +  m  

14.4  ProbFOIL  and  ProbFOIL+  361 
To do so, we need to compute TP Hue(x)  and FP Hue(x)  as a function of x.  
In  turn, this requires the computation of tp Hue(x),i  and fp Hue(x),i•  the con­
tributions of each example. 
Note that PHue(x),i  is monotonically increasing in x,  so the minimum 
and maximum values are obtained for x  = 0 and x  = 1, respectively. Let 
US call them li  and Ui,  SO li  = PHue(o),i  = PH,i  and Ui  = PHue(l),i·  Since 
ProbFülL differs from ProbFüiL+ only for the use of deterministic clauses 
instead of probabilistic ones, Ui  is the value that is used by ProbFülL for the 
computation ofLSCORE(H, C)  which thus retums M(1). 
In ProbFüiL+, we need to study the dependency of PHue(x),i  on x.  If 
the clause is deterministic, it adds probability mass Ui  - li  to PH,i·  We can 
imagine the Ui  as being the probability that the Boolean formula F  =  XH  v  
--.XH  1\  XB  takes value 1, with XH  a Boolean variable that is true if H  
covers the example, P(XH)  =  PH,i.  XB  a Boolean variable that is true if 
the body of clause C  covers the example and P( --.XH  1\  XB)  =  ui  -li.  In  
fact, since the two Boolean terms are mutually exclusive, P(F)  =  P(XH)  +  
P( --.XH  1\  XB)  =  PH,i  +  ui  - PH,i  =  ui.  If the clause is probabilistic, 
its random variable Xe  is independent from all the other random variables, 
so the probability of the example can be computed as the probability that 
the Boolean function F'  =  XH  v  Xe  1\  --.XH  1\  XB  takes value 1, with 
P(Xe)  = x.  So PHue(x),i  = P(F')  = PH,i  +  x( Ui  - li)  and PHue(x),i  is a 
linear function of x.  
We can isolate the contribution of C(x)  to tp Hue(x),i  and fp Hue(x),i  as 
follows: 
tp Hue(x),i  =  tp H,i  +  tpC(x),i  fp Hue(x),i  =  fp H,i  +  fPe(x),i  
Then the examples can be grouped into three sets: 
E1 :Pi ~ li.  the clause overestimates the example independently ofthe value 
of x,  so tPe(x),i  =  0 and fPe(x),i  =  x( Ui  - li)  
E2 : Pi  ? Ui,  the clause underestimates the example independently of the 
value of x,  so tpe(x),i  = x( Ui  - li)  and !Pe(x),i  = 0 
E3 : li  ~ Pi  ~ ui.  there is a value of x  for which the clause predicts the 
correct probability for the example. This value is obtained by solving 
x(ui  -li)  =Pi-li  for x,  so 
Xi  =   Pi-li  
Ui  -li  

362 
Structure  Learning  
For x  ~ Xi.'  ~P~(x),i =  x(ui_- li)  ~d- fP_G(x),i_  =.  0. For x  >  Xi,  
tpc(x),i  -Pt  lt  and fPc(x),i  - x( ut  
lt)  
(Pt  
lt).  
We can express TP HuC(x)  and FP HuC(x)  as 
TPHuC(x)  
TPH  +  TP1(x)  +  TP2(x)  +  TP3(x)  
FPHuC(x)  
FPH  +  FP1(x)  +  FP2(x)  +  FP3(x)  
where TPz(x)  and FPz(x)  are the contribution of the set of examples Ez.  
These can be computed as 
TP1(x)  =  0 
FP1(x)  =  x  2.:: (ui  -li)  =  xU1  
iEE1 
TP2(x)  =  x  2.:: (ui  -li)  =  xU2  
iEE2 
FP2(x)  = 0 
TP3(x)  =  x  
2.:: 
(ui  -li)  +  
2.:: 
(pi  -li)  =  xUf'xi  +  P{'xi  
i:iEE3,X~Xi 
i:iEE3,x>x;  
FP3(x)  =  x  
2.:: 
(ui  -li)-
2.:: 
(pi  - li)  =  xu;xi  - P{'Xi  
i:iEE3,x>x;  
i:iEE3,x>x;  
By  replacing these formulas into M(x),  we get 
M(x)  = (U2 +  uf'Xi)x  +  TPH  +  P{'Xi  +  mP/T  
(Ul  +  u2 +  U3)x  +  TP H  +  F  PH  +  m  
where u3  =  X .L:iEE3 (ui  -li)  =  (  TP3(x)  +  FP3(x))jx.  
Since uf'x;  and P{'xi  are constant in the interval between two consec­
utive values of Xi,  M  ( x)  is a piecewise function where each piece is of the 
form 
Ax+B   
Cx+D   

14.4  ProbFOIL  and  ProbFOIL+  363 
with A,  B,  C,  and D  constants. The derivative of a piece is 
dM(x)  
AD-BC  
dx  
(Cx  +  D) 2 
which is either 0 or different from 0 everywhere in each interval, so the max­
imum of M(x)  can occur only at the XiS values that are the endpoints of 
the intervals. Therefore, we can compute the value of M(x)  for each Xi  and 
pick the maximum. This can be done efficiently by ordering the Xi  values and 
computing u;xi  =  l:i:iEE3,x:;;;xi  (Ui  - li)  and P[Xi  =  l:i:iEE3,X>Xi  (Pi  - li)  
for increasing values of Xi,  incrementally updating u;xi  and P[Xi.  
ProbFOIL+ prunes refinements (line 19 of Algorithm 33) when they can­
not lead to a local score higher than the current best, when they cannot lead 
to a global score higher than the current best or when they are not significant, 
i.e., when they provide only a limited contribution. 
By adding a literal to a clause, the true positives and false positives can 
only decrease, so we can obtain an upper bound of the local score that any 
refinement can achieve by setting the false positives to 0 and computing the 
m-estimate. If this value is smaller than the current best, the refinement is 
discarded. 
By adding a clause to a theory, the true positives and false positives can 
only increase, so if the number of true positives of H  u  C  ( x)  is not larger 
than the true positives of H,  the refinement C  ( x)  can be discarded. 
ProbFOIL+ performs a significance  test  borrowed from mFOIL that is 
based on the likelihood  ratio  statistics.  
ProbFOIL+ computes a statistics LhR(H,  C)  that takes into account the 
effect of the addition of C  to H  on TP  and FP  so that a clause is discarded 
if it has limited effect. LhR(H,  C)  is distributed according to x2 with one 
degree of freedom, so the clause can be discarded if LhR(H,  C)  is outside 
the interval for the confidence chosen by the user. 
Another system that solves the ProbPOIL learning problern is SkiLL 
[Cörte-Real et al., 2015]. Differently from ProbFOIL, it is based on the ILP 
system TopLog [Muggleton et al., 2008]. In  order to prune the universe of 
candidate theories and speed up learning, SkiLL uses estimates of the predic­
tions of theories [Cörte-Real et al., 2017]. 

364 
Structure  Learning  
14.5  SLIPCOVER  
SLIPCOVER [Bellodi and Riguzzi, 2015] leams LPADs by first identifying 
good candidate clauses and then by searching for a theory guided by the 
LL of the data. As EMBLEM (see Section 13.4), it takes as input a set of 
mega-examples and an indication of which predicates are target, i.e., those 
for which we want to optimize the predictions of the final theory. The mega­
examples must contain positive and negative examples for all predicates that 
may appear in the head of clauses, either target or non-target (background 
predicates). 
14.5.1  The  language  bias  
The language bias for clauses is expressed by means of mode  declarations. 
as in Progoi [Muggleton, 1995], see Section 14.1. SLIPCOVER extends this 
type of mode declarations with placemarker terms of the form -# which 
are treated as # when variabilizing the clause and as -
when performing 
saturation, see Algorithm 32. 
SLIPCOVER also allows head declarations of the form 
modeh(r,  [s1, ... , sn],  [a1, ... , an],  [Pt/Ar1,  ...  ,  Pk/Ark]).  
Theseare used to generate clauses with more than two head atoms: s1, ... , Sn  
are schemas, a1, ... , an  areatomssuch that ai  is obtained from Si  by replac­
ing placemarkers with variables, and Pi/ Ari  are the predicates admitted in 
the body. a1, ... , an  are used to indicate which variables should be shared by 
the atoms in the head. 
Examples of mode declarations can be found in Section 14.5.3. 
14.5.2  Description  of  the  algorithm  
The main function is shown by Algorithm 34: after the search in the space of 
clauses, encoded in lines 2-27, SLIPCOVER performs a greedy search in the 
space of theories, described in lines 28-38. 
The first phase aims at finding a set of promising ones (in terms of LL 
of the data), that will be used in the following greedy search phase. By start­
ing from promising clauses, the greedy search is able to generate good final 
theories. The search in the space of clauses is split in turn in two steps: (1) 
the construction of a set of beams containing bottom clauses (function INI­
TIALBEAMS at line 2 of Algorithm 34) and (2) a beam search over each of 
these beams to refine the bottom clauses (function CLAUSEREFINEMENTS 

14.5  SLIPCOVER  365 
at line 11). The overall output of this search phase is represented by two 
lists of promising clauses: TC  for target predicates and BC  for background 
predicates. The clauses found are inserted either in TC,  if a target predicate 
appears in their head, or in BC.  These lists are sorted by decreasing LL. 
Algorithm  34 Function SLIPCOVER. 
1:  function SLIPCOVER(Nint, NS,  NA,  NI,  NV,  NB,  NTC,  NBC,  D,  NEM,  E, 8)  
2: 
IB  =INITIALBEAMS(Nint, NS,  NA)  
C>  Clause search 
3: 
TC<--- D 
4:  
BC  <--- []  
5: 
for all (PredSpec,  Beam)  E IB  do 
6: 
7: 
Steps  <--- 1 
NewBeam  <--- D 
8: 
repeat 
9: 
while Beam  is not empty do 
10: 
remove the first triple (Cl,  Literals,  LL)  from Beam  
C>  Remove the first clause 
11:  
Refs  <-CLAUSEREFINEMENTS((Cl,  Literals),  NV)  C>  Find all refinements Refs  
of (Cl,  Literals)  with at most NV  variables 
12: 
for all (Cl',  Literals')  E  Refs  do 
13: 
(LL",{Cl"})  <-EMBLEM({Cl'},D,NEM,E,O) 
14: 
NewBeam  <-INSERT(( Cl",  Literals',  LL"),  NewBeam,  NB)  
15: 
if Cl"  is range-restricted then 
16:  
if  Cl"  has a target predicate in the head then 
17:  
TC  <-INSERT(( Cl",  LL"),  TC,  NTC)  
18:  
eise 
19:  
BC  <-INSERT(( Cl",  LL"),  BC,  NBC)  
20:  
end if  
21: 
end if 
22: 
end for 
23: 
end while 
24: 
Beam  <--- NewBeam  
25: 
Steps  <--- Steps  +  1 
26: 
until Steps  >  NI  
27: 
end for 
28: 
Th  <--- 0,  ThLL  <--- -oo  
C>  Theory search 
29: 
repeat 
30: 
remove the firstpair (Cl,  LL)  from TC  
31: 
(LL',  Th')  <-EMBLEM(Th u {Cl}, D,  NEM,  E, 8)  
32: 
if LL'  >  ThLL  then 
33: 
Th  <--- Th',  ThLL  <--- LL'  
34: 
endif 
35: 
until TC  is empty 
36: 
Th  .- Th  Uccl,LL)EBc{  Cl}  
37: 
(LL,  Th)  <-EMBLEM(Th, D,  NEM,  E, 8)  
38: 
return Th  
39:  end function 
The second phase starts with an empty theory T h  which is assigned the 
lowest value ofLL (line 28 of Algorithm 34). Then one target clause Cl  at a 

366 
Structure  Learning  
time is added from the list TC.  After each addition, parameter learning with 
EMBLEM is run on the extended theory Th  u  {Cl} and the LL LL'  of the 
data is used as the score of the resulting theory Th'.  If LL'  is better than the 
current best, the clause is kept in the theory; otherwise, it is discarded (lines 
31-34). This is done for each clause in TC.  
Finally, SLIPCOVER adds all the (background) clauses from the list BC  
to the theory composed of target clauses only (line 36) and performs parame­
ter learning on the resulting theory (line 37). The clauses that are never used 
to derive the examples will get a value of 0 for the parameters of the atoms in 
their head and will be removed in a post-processing phase. 
In  the following, we provide a detailed description of the two support 
functions for the first phase, the search in the space of clauses. 
14.5.2.1  Function  INITIALBEAMS  
Algorithm 35 shows how the initial set ofbeams IB,  one for each predicate P  
(with arity Ar)  appearing in a modeh declaration, is generated by building a 
set ofbottom clauses as in Progol, see Section 14.1. The algorithm outputs the 
initial clauses that will be then refined by Function CLAUSEREFINEMENTS. 
In  order to generate a bottom clause for a mode declaration modeh(r,  s)  
specified in the language bias, an input mega-example I  is selected and an 
answer h  for the goal schema(s)  is selected (lines 5-9 of Algorithm 35). 
The mega-example and the atom h  are both randomly sampled with replace­
ment, the former from the available set of training mega-examples and the 
latter from the set of all answers found for the goal schema(s)  in the mega­
example. Bach of these answers represents a positive example. 
Then h  is saturated using Algorithm 32 modified so that, when a term 
in an answer Substitution (line 20) corresponds to a -#type  argument, it 
is added to InTerms  as for -type  arguments. Moreover, when replacing 
constants with variables, terms corresponding to -# placemarkers are kept 
in the clause as for # placemarker. This is useful when we want to test the 
equality of the value of an argument with a constant but we also want to 
retrieve other atoms related to that constant. 
The initial beam Beam  associated with predicate P/Ar  of h  contains the 
clause with empty body h  : 0.5 ~ true  for each bottom clause of the form 
h:- b1,  ...  ,  bm  (lines 10 and 11 of Algorithm 35). This process is repeated 
for a number Nint  of input mega-examples and a number NA  of answers, 
thus obtaining Nint  ·NA  bottom clauses. 
The generation of a bottom clause for a mode declaration 
m  =  modeh(r,  [s1, ... , sn],  [a1, ... ,an],  [Pl/Ar1,  ...  ,  Pk/Ark])  

14.5  SLIPCOVER  367 
is the same except for the fact that the goal to call is composed of more than 
one atom. In order to build the head, the goal a 1,  ...  ,  an  is called and NA  
answersthat ground all ais  are kept (lines 15-19). From these, the set of input 
terms In Terms  is built and body literals are found by Function SATURATION 
(line 20 of Algorithm 35) as above. The resulting bottom clauses then have 
the form 
a 1 , . . . ; an  ~ b1, ... , bm  
and the initial beam Beam  will contain clauses with an empty body of the 
form 
1 
. 
1 
; an  : -- ~ true. 
a1:  n  +  1 ' 
n+1 
Finally, the set of the beams for each predicate P  is retumed to Function 
SLIPCOVER. 
Algorithm  35 Function lNITIALBEAMS. 
1:  
2: 
function INITIALBEAMS(Nint, NS,  NA)  
IB  <--- 0  
3: 
for all predieates PI Ar  do 
4:  
Beam  <--- []  
5: 
for all modeh declarations modeh(r,  s)  with PI Ar  predicate of s  do 
6: 
for i  =  1  --->  Nint  do 
7: 
seleet randomly a mega-example I  
8: 
for j  =  1--->  NA  do 
9: 
10: 
seleet randomly an atom h  from I  matehing schema( s)  
bottom clause BC  <-SATURATION(h, NS),  Iet BC be H  ead:- Body  
11: 
Beam  <--- [(h: 0.5 <--- true,  Body,  -oo)IBeam] 
12: 
end for 
13: 
end for 
14: 
endfor 
15:  
for all modeh declarations modeh(r,  [s1, ... ,sn],  [a1, ... ,an],PL)  with PI Ar  in PL  
appearing in s1, ... , Sn  do 
16: 
for i  =  1  --->  Nint  do 
17: 
se1eet randomly a mega-examp1e I  
18: 
for j  =  1  --->  NA  do 
19: 
se1eet randomly a set of atoms h 1, ... , hn  from I  matehing a 1, ... , an  
20:  
bottom clause BC  
<-SATURATION((h1, ... , hn),  NS),  Iet 
BC  be 
Head:- Body  
21: 
Beam  <--- [(a1: n~ 1 ;  ...  ;  an:  n~ 1 <--- true,  Body,  -oo)IBeam] 
22: 
end for 
23: 
end for 
24: 
endfor 
25: 
IB  <--- IB  u  {(PIAr,Beam)}  
26: 
end for 
27: 
return IB  
28:  end function 

368 
Structure  Learning  
14.5.2.2  Beam  search  with  clause  refinements  
SLIPCOVER then performs a cycle over each predicate, either target or back­
ground (line 5 of Algorithm 34): in each iteration, it runs a beam search in 
the space of clauses for the predicate (line 9). 
For each clause Cl  in the beam, with Literals  admissible in the body, 
Function CLAUSEREFINEMENTS, shown in Algorithm 36, computes refine­
ments by adding a literal from Literals  to the body or by deleting an atom 
from the head in the case of multiple-head clauses with a number of disjuncts 
(including the null  atom) greater than 2. Furthermore, the refinements must 
respect the input-output modes of the bias declarations, must be connected 
(i.e., each body literal must share a variable with the head or a previous body 
literal), and their number of variables must not exceed a user-defined number 
NV.  The pair (Cl',  Literals')  indicates a refined clause Cl'  together with 
the new set Literals'  of literals allowed in the body of Cl';  the pair (Cl~. 
Literals)  indicates a specialized clause Cl'  where one disjunct in its head has 
been removed. 
At line 13 of Algorithm 34, parameter learning is performed using EM­
BLEM, see Section 13.4, on a theory composed of the single refined clause. 
This clause is then inserted into a list of promising clauses: either into 
TC,  if a target predicate appears in its head, or into BC.  The insertion is in 
order of decreasing LL. If the clause is not range-restricted, i.e., if some of 
the variables in the head do not appear in a positive literal in the body, then 
it is not inserted in TC  nor in BC.  These lists have a maximum size: if an 
insertion increases the size over the maximum, the last element is removed. In 
Algorithm 34, Function INSERT(!, Score,  List,  N)  is used to insert in order 
a clause I  with score Score  in a List  with at most N  elements. Beam search 
is repeated until the beam becomes empty or a maximum number NI  of 
iterations is reached. 
The separate search for clauses has similarity with the covering loop of 
ILP systems such as Aleph and Progol. Differently from ILP, however, the 
test of an example requires the computation of all its explanations, while, 
in ILP, the search stops at the first successful derivation. The only interaction 
among clauses in PLP happens if the clauses are recursive. If not, then adding 
clauses to a theory only adds explanations for the example - increasing its 
probability - so clauses can be added individually to the theory. If the clauses 
are recursive, the examples for the head predicates are used to resolve literals 
in the body; thus, the test of examples on individual clauses approximates the 
test on a complete theory. 

14.5  SLIPCOVER  369 
Algorithm  36 Function CLAUSEREFINEMENTS. 
1: function C LAUSEREFI NEM ENTS(( Cl ,  Literals ), N V)  
2: 
Refs  =  0,  Nvar  =  0; 
~ Nvar:  number of different variables in a clause 
3: 
for all b  E Literals  do 
4: 
Literals'  <-- Literals\{b}  
5: 
add b  to Cl  body obtaining Cl'  
6: 
Nvar  <-- number of Cl'  variables 
7: 
if Cl'  is connected " Nvar  <  N V  then 
8: 
Refs  <-- Refs  u  {  (Cl',  Literals' )}  
9: 
end if 
10: 
end for 
II: 
if Cl  is a multiple-head clause then 
~ It  has 3 or more disjuncts including the null  atom 
12: 
remove one atom from Cl  head obtaining Cl~ 
~ Not the null  atom 
13: 
adjust the probabilities on the remaining head atoms 
14:  
R efs  <-- Refs  u  {  ( Cl~ , Literals' )}  
15: 
end if 
16: 
return Refs  
17: end function 
14.5.3  Execution  example  
We now show an example of execution on the UW-CSE dataset [Kok and 
Domingos, 2005a] that describes the Computer Science Department of the 
University ofWashington with 22 different predicates, such as advisedby I  2 , 
yearsinpro graml 2, and taughtbyI  3. The aim is to predict the pred­
icate advisedby I  2, namely, the fact that a person (student) is advised by 
another person (professor). 
The language bias contains modeh  declarations for two-head clauses 
such as 
modeh (* , advisedby (+person , +person )) 
and modeh  declarations for multi-head clauses such as 
modeh (* , [advisedby (+person , +person ), 
tempadvisedby (+person , +person ) ] , 
[advisedby (A, B), tempadvisedby (A, B) ] , 
[professor/l , student /l , hasposition /2 , inphase/2 , 
publication/2 , 
taughtby /3 , ta /3 , courselevel /2 , yearsinprogram/2 ] ) 
modeh (* , [student (+person ), professor (+person ) ] , 
[student (P ), professor (P ) ] , 
[hasposition /2 , inphase/2 , taughtby/3 , ta /3 , 
courselevel /2 , 

370 
Structure  Learning  
yearsinprogram/2 , advisedby /2 , tempadvisedby /2 ]). 
modeh (* , [i nphase (+person , pre_ quals ), inphase 
(+p e rson , post_ quals ), 
inphase (+person , post_ generals )], 
[inphase (P , pre_quals ), inphase (P , post_quals ), 
inphase (P , post_generals )], 
[professor/l , student /l , taughtby /3 , ta /3 , courselevel /2 , 
yearsinprogram/2 , advisedby /2 , tempadvisedby /2 , 
hasposition /2 ]) . 
Moreover, the bias contains modeb  declarations such as 
modeb (* , courselevel (+course , - level )). 
modeb (* , COurselevel (+course , #level )). 
An example of a two-head bottom clause that is generated from the first modeh  
declaration 
and 
the 
example 
advi sedby (pe r sonl SS , 
pe r son l Ol ) is 
advisedby (A, B) :0.5 :- professor (B), student (A), 
hasposition (B, C), 
hasposition (B, faculty ), inphase (A, D), 
i nphase (A, pre_ quals ), 
yearsinprogram (A, E), taughtby (F, B, G), taughtby (F , B, H), 
taughtby (I , B, J ), taughtby (I , B, J ), taughtby (F , B, G), 
taughtby (F , B, H), 
ta (I , K, L) , ta (F , M, H) , ta (F , M, H) , ta (I , K, L) , ta (N, K, O), 
ta (N, A, P ),  
ta (Q, A, P ) , ta (R, A, L) , ta (S, A, T) , ta (U, A, O) , ta (U, A, O),  
ta (S, A, T),  
ta (R, A, L) , ta (Q, A, P ) , ta (N, K, O) , ta (N, A, P ) , ta (I , K, L),  
ta (F , M, H).  
An example of a multi-head bottom clause generated from the second modeh  
declaration and the examples 
student (person218 ). 
professor (person218 ). 
is 
student (A) :0.33 ; professor (A) :0.33 
inphase (A, B), 
inphase (A, post_ generals ), 
yearsinprogram (A, C). 

14.5  SLIPCOVER  371 
When searching the space of clauses for the advisedby /2 predicate, an 
example of a refinement from the first bottom clause is 
advisedby (A, B) :0.5 :- professor (B). 
EMBLEM is then applied to the theory composed of this single clause, using 
the positive and negative facts for advi s edby I  2 as queries for which to 
build the BDDs. The single parameter is updated obtaining: 
advisedby (A, B) : 0.108939 
professor (B). 
The clause is further refined to 
advisedby (A, B) :0.108939 :- professor (B), 
hasposition (B, C). 
An example of a refinement that is generated from the second bottom clause is 
student (A) :0.33 ; professor (A) :0.33 
inphase (A, B) . 
The updated refinement after EMBLEM is 
student (A) : 0. 5869 ; professor (A) : 0. 09832 
inphase (A, B) . 
When searching the space  of theories  for the target predicate advi sedby, 
SLIPCOVER generates the program: 
advi sedby (A, B) :0.1198 :- professor (B), 
inphase (A, C) . 
with an LL of -350.01. Then a clause is added 
advisedby (A, B) :0.1198 :­ professor (B), 
inphase (A, C) . 
advisedby (A, B) :0.1198 :­ professor (B), student (A). 
and EMBLEM is run obtaining 
advisedby (A, B) :0.05465 
professor (B), 
inphase (A, C) . 
advisedby (A, B) :0.06893 
professor (B), 
student (A) . 

372 
Structure  Learning  
client 
hasAcc 
hasLoan 
clild  
age  
creditScore  
clild  
accld  
accld  
loanld  
ann 
33 
-
ann 
a_ll 
a_ll 
1_20  
bob 
40 
500 
bob 
a_ll 
a_lO 
1_20  
carl 
-
450 
ann 
a_20 
a_20 
1_31  
john 
55 
700 
john 
a_lO 
a_20 
1_41  
loan 
account 
loanld  
loanAmt  
status  
accld  
savings  
freq  
1_20 
20050 
appr 
a_lO 
3050 
high 
1_21 
-
pend 
a_ll 
-
low 
1_31 
25000 
decl 
a_19 
3010 
?  
1_41 
10000 
-
a_20 
?  
?  
Table  14.1  An example of a database. From [Kumar et al., 2022]. 
with an LL of -318.17. Since the LL decreased, the last clause is retained and 
at the next iteration, a new clause is added: 
advi sedby (A, B) :0.12032 :- haspos i tion (B, C), 
inphase (A, D) . 
advi sedby (A, B) :0.05465 :- professor (B), 
i nphase (A, C) . 
advi sedby (A, B) :0.06893 :- professor (B), student (A). 
14.6  Learning  the  Structure  of  Hybrid  Programs  
DiceML [Kumar et al., 2022] is a system for learning the structure and the 
parameters of DC programs, see Section 4.2. In particular, it tackles the prob­
lern of relational  autocompletion,  where the goal is to automatically fill out 
some entries in multiple related tables. 
For example, consider the relational database shown in Table 14.1 about a 
banking domain: clients are described by their age and credit score, they have 
accounts that are described by savings and frequency and in turn accounts are 
connected to loans described by amount and status 
In this database, some entries are unknown, marked by -, and some are 
missing, marked by "?". Thetaskis to predict the missing cells, that are those 
identified by the user as being of interest. Some of these cells regard discrete 

14.6  Learning  the  Structure  of Hybrid  Programs  373 
attributes, such as freq in the account table, some regard continuous attributes, 
such savings in the account table. 
DiceML learns a hybrid program that is able to predict the missing cells. 
To do so, it assumes that the database is divided into entity tables and asso­
ciative tables. The entity tables contain no foreign keys and describe entities, 
while associative tables contain only foreign keys and describe associations 
among entities. A database VB  is transformed into a set AvB  u  RvB  of facts 
that will be used as training data: 
• For every primary key t  in an entity table e,  we add the fact e(t) to Rvß.  
Forexample, c l ient (ann). 
• Foreach tuple (t1, t2) in an associative table r,  we add a fact r(t1, t2) to 
Rvß.  For example, hasAcc (ann, a_ll ) . 
• For each primary key  t  of an entity table with an attribute a  of value 
v,  we add a deterministic fact a(t)  "'  val(v)  to Avß.  For example, 
age (ann) "' val (33). This notation extends DCs and means that 
the variable age (ann) gets value 33. 
The DC program models all the attributes of the database as random variables. 
Two extensions are used with respect to the usual DC syntax: aggregate atoms 
and statistical models. 
Aggregate atoms express the dependency of a random variable from a set 
of values for other random variables. Remernher that there must be a unique 
ground h  "' D  in the least fixpoint of the STp  operator for each ground 
random variable h.  This means that if there are two rules defining h,  their 
bodies must be mutually exclusive. Now suppose that the credit score of a 
client depends on the frequency of operations on their bank accounts. We 
could write a rule such as 
creditScore (C) - gaussian (300 , 10 . 1 ) := client (C), 
hasAcc (C, A), freq (A) -=low . 
creditScore (C) - gaussian (400 , 10 . 1 ) 
client (C), 
hasAcc (C, A), freq (A) -=high . 
where : = is concrete syntax for +---.  However, if a client has two accounts, 
one with frequency low and one with frequency high, we would have two 
different definitions for the density of their credit score. In order to solve this 
problem, we can use an aggregate function agg(T,  Q, R)  where agg  is an 
aggregation function, Q  is a query, T  is a variable in Q  and R  is the result 
of the aggregation. As aggregation functions we consider the mode, mean, 
maximum, minimum, cardinality, etc. For example, we could have the credit 

374 
Structure  Learning  
score depend on the mode of the frequency on the accounts of a client with 
the clause 
creditScore (C) 
~ gaussian (300 , 10.1 ) := client (C), 
mod (X, (hasAcc (C, A), freq (A) ~=X ), Z), Z==low . 
The second extension, statistical models, are used to specify the dependence 
of the distribution of the variable in the head from continuous variables in the 
body. A DC with statistical model takes the form 
h"'  V q,  ~ b1 , ... , bn , M q,  
where M q,  is an atom encoding a function that computes the values of the 
parameters cjJ  of V q,  from the values of continuous random variables in the 
body b1 ... , bn.  For example, the clause 
creditScore (C) 
~ gaussian (M, O.l ) := age ( C ) ~= Y , 
linear ( [Y], [ 10 . 1 , 200 ], M). 
uses a linear model with coefficients 10.1 and 200 to compute the mean of 
the Gaussian distribution of the credit score of a client given their age. 
Table 14.21ists the available statistical models, which include, besides the 
linear one, also the logistic and softmax models. 
Inference in these programs is performed by likelihood weighting, see 
Section 12.3: sampling is performed by backward reasoning from the query 
and each sample is assigned a weight. The probability of the query is then the 
fraction of the weights of the samples of the evidence where the query is also 
true. 
DiceML learns DC programs for the relational autocompletion problern 
that are called Jointmodel programs (JMPs) and are composed of 1. the facts 
in the transformed RvB;  2. a set of learned DCs }{  that define all the attributes 
in the database. 
Example  123  (JMP for the banking domain [Kumar et al., 2022]). Form  the  
database  in  Table  14.1  thefollowing  JMP  could  be  learned:  
client  (ann ).  client  ( john ).  
hasAcc (ann , a_ll ).  hasAcc (ann , a_20 ).  
freq (A )  
~ discrete ([ 0.2: 1ow, 0.8: high ])  :=  account (A ).  
savings (A )  
~ gaussian (2002 , 10 . 2 )  :=  account (A ),  freq 
(A ) ~=X, 
X==low .  
savings (A )  
~ gaussian (3030 , 11.3 )  
account (A ),  freq 
(A ) ~=X, 
X==high .  
age  (C)  
~ gaussian  (Mean ,  3 )  :=  client  (C),  

14.6  Learning  the  Structure  of Hybrid  Programs  375  
type of 
random 
variable 
(X)  
statistical model atom 
(M,;, ) in the body 
function
implemented 
by M ,;, 
the head 
continuous I  linear([Y1 , . .. , Yn],  
M=Z  
X  ~ 
2 
gaussian(M,  0" )  
(W1 , ... , W n+l ],  M) 
boolean 
logistic([Y1,  .. .  , Yn],  
X~ discrete([P1  :
I  
pl = 
z  
(W1 , ... ,Wn+ l] ,  
1  +  e-
true,  P2  : false])  
(P1 , P2])  
p2  = 1- pl  
discrete 
e  1 
sojtmax( [Y1 , ... , Yn],  
P1=­
X~ discrete([P1  :
I  
N 
[(Wll ' . .. 'Wn+ll ],  ... '  
l1, . . . , Pd  : ld])  
[W1d, ... , Wn+ld ]] ,  
(P1 , .. . , Pd])  
ezrl  
Pd = ­N  
where Z  is Y1.W1 +  ...  +  Yn.Wn  +  Wn+l,  
Z;  is Y1.W1 , +  ...  +  Yn .Wn,  +  W n+l; ,  
N  is L,~=l ez' ,  
d  is the size of domain of X  (dom(X)) and l;  E  dom(X)  
Table  14.2  Available statistical model atoms (M 'ljJ). From [Kumar et al., 
2022]. 
avg (X,  (hasAcc (C, A ),  savings (A )  -=X ),  Y),  creditScore (C)  -=Z,  
linear ( [ Y, Z 1 ,  [  30 ,  0. 2 ,  -0. 4 1 ,  Mean )  .  
loanAmt  (L )  -
gaussian  (Mean ,  10 )  : =  loan  (L ),  
avg (X, ( hasLoan (A , L ),  savings (A )  -=X ), Y),  
linear ([ Y1 ,  [ 100 . 1 ,  101 , Mean ).  
loanAmt (L )  -
gaussian (25472.3 , 10.2 )  : =  loan (L ),  
\  +avg (X,  (hasLoan  (A ,  L ), savings  (A )  -=X ),  Y).  
status (L ) 
-
discrete ([P1: appr,  P2 : pend,  P3 : decl 1)  : =  loan (L ),  
avg (X,  
(hasLoan (A , L ), hasAcc (C, A ), creditScore (C) -=X ), Y),  
loanAmt (L ) -=Z,  softmax ( [ Y, Z1 ,  [[ 0 . 1 , -0 . 3 , -2 . 41 ,  [ 0 . 3 , 0 . 4 , 0 . 2 1,  
[ 0.8 , 1.9, -2.911 , [ P1 , P2 , P31 ).  
creditScore (C) 
-
gaussian (300 , 10 . 1 )  : =  client (C),  
mod (X,  (hasAcc (C, A ),  freq (A ) -=X ), Z ),  Z==low .  
creditScore  (C)  -
gaussian  (Mean ,  15.  3 )  :=  client  (C),  
mod (X,  (hasAcc (C, A ),  freq (A ) -=X ), Z ),  Z==high,  
max (X,  (hasAcc (C, A ),  savings (A )  -=X),  Y),  
linear  (  [ Y1 ,  [ 600 ,  0 . 2 1 , Mean ).  
creditScore (C)  -
gaussian (Mean , 12.3 )  : =  client (C),  
\+mod (X, ( hasAcc (C, A ),  freq (A ) -=X), Z ),   
max (X,  (hasAcc (C, A ),  savings (A )  -=X ),  Y),   
linear ([ Y1 ,  [ 500 , 0 . 81 , Mean ).  

376 
Structure  Learning  
Here  negated  aggregate  atoms  succeed  if they  query  does  not  have  any  an­
swer.  
For  example,  the  clause  for  status  expresses  that  its  distribution  depends  
on  the  average  credit  score  of the  clients  with  an  account  on  which  the  loan  
is  requested  and  on  the  amount  of  the  loan.  The  distribution  for  status  is  
discrete  with  three  values  and  the  probabilities  are  obtained  from  a  softmax  
model  with  the  coefficients  specified  in  the  statistical  model  atom.  
DiceML learns a JMP 1{  from the data AvB  using the relational structure 
RBv  and possibly a background knowledge BJC.  
Since DCs defining the same random variable must have mutually ex­
clusive bodies, DiceML learns Distributionallogic trees (DLTs), a kind of 
first-order decision trees [Blockeel and Raedt, 1998], so that mutual exclusion 
is enforced by construction. 
A DLT for an attribute, a(T),  is a rooted tree, where the root is an entity 
atom e(T),  each leaf is labeled by a probability distribution 'Dcp  and/or a sta­
tistical model M1f;.  and each internal node is labeled with an atom bi.  Interna! 
nodes bi  can be of two types: 
• a binary atom of the form aj (T)  ~=V that unifies the outcome of an 
attribute aj(T)  with a variable V.  
• an aggregation atom ofthe form aggr(X,  Q,  V),  where Q is ofthe form 
(r(T,  T1), aj(TI)~=X) in which r  isalink relation and aj(TI)  is an 
attribute. 
The children of nodes depend on the value V  can take: 
• if V  takes discrete values { v1, ... , Vn}  then there is one child for each 
value Vi.  
• if V  takes numeric values then its value is used to estimate the parame­
ters of the distribution TJ4>  and/or it is used in the statistical model M1/J  
in the leaves. 
Moreover, the binary and the aggregation atom bi  can fail, the first in case 
the attribute is not defined. Therefore there is also an extra child that captures 
that V  is undefined. 
A DLT can be converted to a set of distributional clauses by translating 
every path from the root to a leaf node in the DLT to a distributional clause 
of the form h  ~ TJ4>  +--- b1, ... , bn,  M1/J.  Figure 14.1 shows a set of DLTs that 
together encode the JMP ofExample 123. 
Learning then consists in building a DLT for each attribute using AvB  
as training data. The algorithm follows the structure of decision tree learning 

14.6  Learning  the  Structure  of Hybrid  Programs  377 
client(C)  
----r-­
avg(X ,  (hasAcc(C , A)  , savings(A) :!:X)  , Y)  
credi tScore  (C)  :!Z  
Figure  14.1  A collection of DLTs corresponding to the JMP  in Example 
123. 
algorithms such as [Blockeel and Raedt, 1998]: the set of examples is recur­
sively partitioned into smaller sets by choosing a test for the node. The test 
is selected on the basis of a refinement operator, that generates literals to be 
added to the current path according to the Janguage bias, and on the basis of a 
scoring function, that evaluates the test. Once the test is chosen, the children 
of the nodes are generated and the algorithm is called recursively on the chil­
dren. The process terminates when examples are sufficiently homogeneaus or 
when there are no more literals to test. In  this case, a leaf is generated and the 
parameters of the distribution and/or the parameters for the statistical model 
must be estimated. This is done by means of inference and log-likelihood 
maximization. 
In order to score refinements, DiceML uses the Bayesian Information 
Criterions (BIC [Schwarz, 1978]). The score of a clause h  ~ Dcp -
Q, M 'I/J,  
corresponding to a path from the root to a leaf, is 
s(h  ~ D<P -
Q,M 'I/J)  = 2E(<;3) - kln iEI  
(14.1)  
where E( <;3)  is the expected log-likelihood of the examples E  in the leaf 
for the maximum likelihood parameters if3  and k  is the number of parame­
ters. The score trades off model complexity with model accuracy, preventing 
overfitting. 

378 
Structure  Learning  
To determine the score of the refinement (Q,  l(V))  of the clause, the 
score of the children are summed, where each child is considered as a leaf, 
completed with a distribution and/or a statistical model, and the parameters 
estimated. 
In  order to induce a JMP, an order among the attributes is decided and 
a DLT for each attribute is learned following the order, with the constraint 
that only previous attributes can be used in the DLT. in this way, circular 
definitions are avoided. 
To handle missing attributes, two approaches are possible: one can either 
exploit the extra child corresponding to the absence of a value, or resort to an 
EM algorithm. 
The learned JMP can then be used for the relational autocompletion task: 
given a set of cells, predict the values of the others. This can be done using 
inference. 
DiceML has been experimented on a synthetic university data set, on a 
financial dataset [Berka, 2000] and on a dataset about the NBA [Schulte and 
Routley, 2014]. 
14.7  Scaling  PILP  
PLP usually requires expensive learning procedures due to the high cost of 
inference. SLIPCOVER (see Section 14.5) for example performs structure 
learning of probabilistic logic programs using knowledge compilation for 
parameter learning: the expectations needed for the EM parameter learning 
algorithm are computed using the BDDs that are built for inference. Compil­
ing explanations for queries into BDDs has a #P cost in the number of random 
variables. 
Two systems that try to remedy this are LIFTCOVER [Nguembang Fadja 
and Riguzzi, 2019] and SLEAHP [Nguembang Fadja et al., 2021] 
14.7.1  LIFTCOVER  
Lifted inference, see Chapter 9 was proposed for improving the performance 
of reasoning in probabilistic relational models by reasoning on whole popu­
lations of individuals instead of considering each individual separately. 
Herewe propose a simple PLP language (called liftable  PLP)  where pro­
grams contain clauses with a single annotated atom in the head and the pred­
icate of this atom is the same for all clauses. In this case, all the approaches 
for lifted inference coincide and reduce to a simple computation. 

14.7  Scaling  PILP  379 
14.7 .1.1  Liftable  PLP  
We restriet the language of LPADs by allowing only clauses of the form 
Ci  =  hi  : IIi ~ bil,  ... , biui  
in the program where all the clauses share the same predicate for the single 
atom in the head, let us call this predicate targetja  with a  the arity. The 
literals in the body have predicates other than targetja  and are defined by 
facts and rules that are certain, i.e., they have a single atom in the head with 
probability 1. The predicate targetja  is called target  and the others input  
predicates.  Suppose there are n  probabilistic clauses of the form above in the 
program. We call this language Iiftahle  PLP.  
The problern is to compute the probability of a ground instantiation q  
of targetja.  This can be done at the lifted level. We should first find the 
number of ground instantiations of clauses for targetja  suchthat the body 
is true and the head is equal to q.  Suppose there are mi  such instantiations 
{eil, ... , BimJ, for rule Ci  for i  =  1, ... , n.  Each instantiation Bij  corre­
sponds to a random variable Xij  taking values 1 with probability IIi and 0 
with probability 1 - IIi. The query q  is true if at least one of the random 
variables for a rule takes value 1: q  =  true  ~ v~=l Vj!;l (Xij  =  1). In  
other words q  is false only if no random variable takes value 1. All the random 
variables are mutually independent so the probability that none takes value 1 
is TI~=l Tij~ 1 (1- IIi) =  TI~=l (1- IIi)mi and the probability of q  being true 
is P  ( q)  =  1 -
TI~= 1  (  1 - IIi)mi. So once the number of clause instantiations 
with the body true is known, the probability of the query can be computed in 
logarithmic time. Note that finding an assignment of a set of logical variables 
that makes a conjunction true is an NP-complete problern [Kietz and Lübbe, 
1994], therefore computing the probability of the query may be prohibitive. 
However, when using knowledge compilation, to the cost of finding the 
assignment, we must sum the cost of performing the compilation, that is #P 
in the number of satisfying logical variables assignments (clause instantia­
tions with the body true). Therefore inference in liftable PLP is significantly 
eheaper than in the general case. Moreover, in machine learning the conjunc­
tions are usually short and the knowledge compilation cost dominates. 
The generallanguage of PLP is necessary when the user wants to induce 
a knowledge base or an ontology regarding the domain. In that case, the 
possibility of having more than one head, possibly involving more than one 
predicate, and the possibility of learning probabilistic rules for subgoals is 
useful because the resulting program can thus represent and organize gen­
eral knowledge about the domain. Moreover, the resulting program can then 

380 
Structure  Learning  
be used for answering different types of queries instead of being restricted 
to answering queries about a single predicate. This is similar to the prob­
lern of learning multiple predicates in Inductive Logic Programming. While 
this problern has received considerable attention, most work concentrated 
on learning a single predicate, for example systems such as FOIL [Quinlan, 
1990], Progoi [Muggleton, 1995] and Aleph [Srinivasan, 2007] learn a single 
predicate at a time. Furthermore, most benchmark datasets are focused on 
predicting the truth value of atoms for a single predicate. 
We can picture the dependence of the random variable q  associated with 
the query from the random variables of clause groundings with the body true 
as in Figure 14.2. Here the conditional probability table of q  is that of an or: 
P( q)  =  1 if at least one of its parents is equal to 1 and 0 otherwise. The 
variables from clause groundings are 
{Xn, . . . , X1mu X21 , ... , X  2m2 ,  • • • ,  Xnl , .. . ,  Xnmn}.  
These are parentless variables, with X ij  having the conditional probability 
table (CPT) P(Xij  = 1) = Ili and P(Xij  = 0) = 1 - Ili. 
This is an example of a noisy-OR  model [Good, 1961; Pearl, 1988]: (see 
also Section 9 .1.1) an event is associated to a number of conditions each 
of which alone can cause the event to happen. The conditions/causes are 
noisy, i.e., they have a probability of being active, and they are mutually 
unconditionally independent. A Jjftable PLP program encodes a noisy-OR 
model where the event is the query q  being true and causes are the ground 
instantiations of the clauses that have the body true: each can cause the query 
to be true with the probability given by the clause annotation. 
Figure  14.2  Bayesian Network representing the dependency between the 
query q  and the random variables associated with groundings of the clauses 
with the body true. 

14.7  Scaling  PILP  381 
Example  124  (Liftable PLP for the UW-CSE domain). Let  us  consider  
the  UW-CSE  domain  [Kak  and  Domingos,  2005b]  where  the  objective  is  to  
predict  the  "advised  by"  relation  between  students  and  professors.  In  this  
case  the  target  predicate  is  advisedby/2  and  a  programfor  predicting  such  
predicate  may  be  
advisedby(A,  B)  : 0.4 ~ 
student(A),  professor(B),  publication( C,  A),  publication( C,  B).  
advisedby(A,  B)  : 0.5 ~ 
student(A),professor(B),  ta( C,  A),  taughtby(C,  B).  
where  publication(A,  B)  means  that  A  is  a  publication  with  author  B,  
ta(C,  A)  means  that  A  is  a  teaching  assistant  for  course  C  and  
taughtby(C,  B)  means  that  courseC  is  taught  by  B.  The  probability  that  a  
student is  advised by a professor depends  on  the  number  ofjoint publications  
and  the  number  of courses  the  professor  teaches  where  the  student  is  a  TA,  
the  higher  these  numbers  the  higher  the  probability.  
Suppose  we  want  to  compute  the  probability  of q  =  advisedby(harry,  
ben)  where  harry  is  a student,  ben  is  a  professor,  they  have  4  joint  publica­
tions  and  ben  teaches  2  courses  where  harry  is  a  TA.  Then  the  first  clause  
has  4  groundings  with  head  q  where  the  body  is  true,  the  second  clause  has  2  
groundingswithhead q  where  the  body  is  true  and  P(advisedby(harry,  ben))  
= 1 - (1 - 0.4)4(1 - 0.5) 2 = 0.9676. 
14.7.1.2  Parameter  learning  
We consider discriminative learning because we want to predict only atoms 
for the target predicate, while the atoms for the input predicates are assumed 
as given. 
The problern of discriminative leaming of the parameters of a liftable PLP 
P  =  {  C1, ... , Cn}  can be expressedas follows: given a liftable PLP P,  a set 
E+  = {e1, ... , eQ}  
of positive examples (ground atoms for the target predicate) and a set 
E- =  {eQ+l, ... ,eR} 
of negative examples (ground atoms for the target predicate) and background 
knowledge B,  find the parameters of P  suchthat the likelihood 
Q  
R  
L  =  n 
P(eq)  n  P(-·..,er)  
q=l 
r=Q+l  
is maximized. 

382 
Structure  Learning  
The background knowledge B  is anormal logic program defining the 
input predicates with certainty. In the simplest case it is a set of ground facts, 
i.e., an interpretation I,  describing the domain by means of the observed 
facts for the input predicates. It  can also be a mega-example because we can 
consider the case where we have a set of interpretations I  = {h,  ...  , I  u} 
each describing a different sub-domain from the universe considered. In that 
case, each mega-example Iu  will be associated with its set of positive and 
negative examples E:!;  and E;;  that aretobe evaluated against Iu.  
The likelihood can be unfolded to 
Q  (  
n  
) 
R  
n  
(14.2)
L  =  D 1 - D
(1 - ITz)mlq  r=IJ+l D
(1 - ITz)mlr  
where miq  (mir) is the number of instantiations of Ci  whose head is eq  (er) 
and whose body is true. We can aggregate the negative examples 
n  
Q  (  
n  
)
L  =  D
(1 - ITz)ml-D 1 - D
(1 - ITz)mlq  
(14.3) 
where mz_  =  .l:!Q+l mzr·  
We can maximize L  using an Expectation Maximization (EM) algorithm 
[Dempster et al., 1977] since the Xij  variables are hidden. To perform EM, 
we need to compute the conditional probabilities P(Xij  =  1le) and P(Xij  =  
11---.e) where e  is an example ( a ground atom) and Xij  are its parents. 
Alternatively, we can use gradient descent to optimize L.  In this case, we 
need to compute the gradient of the likelihood with respect to the parameters. 
In the following subsections we consider each method in turn. 
14.7 .1.2.1  EM  Algorithm  To perform EM, we need to compute the distri­
bution of the hidden variables given the observed ones, in our case P(Xij  =  
1le) and P(Xij  =  11 "'e). e  is a single example that is a ground atom for the 
target predicate. The Xij  variables arerelative to the ground instantiations of 
the probabilistic clauses whose body is true when the head is unified with e.  
Different examples don't share clause groundings, as the constants in them 
are different. Therefore the Xij  variables are not shared among examples. 
Let us now compute P(Xij  = 1, e): 
P(Xij  =  1, e) =  P(eiXij  =  1)P(Xij  =  1) =  P(Xij  =  1) =  IIi 

14.7  Scaling  PILP  383 
since P(eiXij  = 1)  = 1,  so 
P(Xij  = 1, e) 
rri
P(Xij  = 1le)  
(14.4)
P(e)  
1- nr: 
t=l  (1- II·)mi 
t 
II· 
P(Xij  =Oie) = 1-
t
(14.5)
1- nr: 
t=l  (1- II·)mi
t 
P(Xij  = 11 ~e) is given by 
P(Xij  = 1, ~e) 
P( ~eiXiJ = 1)P(Xij  = 1) = 0 
since P( ~eiXij = 1)  = 0, so 
P(XiJ  =  11 ~e) 
0 
(14.6) 
P(Xij  =  Ol  ~e) 
1.  
(14.7) 
This leads to the EM algorithm of Algorithm 37, with the EXPECTATION and 
MAXIMIZATION functions shown in Algorithms 38 and 39. 
Function EM stops when the difference between the current value of the 
LL and the previous one is below a given threshold or when such a difference 
relative to the absolute value of the current one is below a given threshold. 
Function EXPECTATION updates, for each clause Ci,  two counters, cw  
and eil, one for each value of the random variables Xij  associated with clause 
Ci.  These counters accumulate the values of the conditional probability of the 
values of the hidden variables. The counters are updated taking into account 
first the negative examples and then the positive ones. Negative examples can 
be considered in bulk because their contribution is the same for all groundings 
of all examples, while positive examples must be considered one by one, for 
each one updating the counters of all the clauses. 
Function Maximization then simply computes the new values of the pa­
rameters by dividing eil by the sum of cw  and eil· 
14.7 .1.2.2  Gradient-Based Optimization Gradient -based methods include 
gradient descent and its derivatives, such as second-order methods like Limited­
memory BFGS (LBFGS) [Nocedal, 1980], an optimization algorithm in the 
family of quasi-Newton methods that approximates the Broyden-Fletcher­
Goldfarb-Shanno (BFGS) algorithm using a limited amount of computer 
memory. 
To perform gradient-based optimization we need to compute the partial 
derivatives of the likelihood with respect to the parameters. Let us recall the 

384 
Structure  Learning  
Algorithm  37 Function LIFTCOVER-EM 
1: function LIFTCOVER-EM(restarts, max_iter,  E, 8)  
2: 
BestLL  <--- -inf  
3: 
BestPar  <--- []  
4: 
for j  <--- 1 --->  restarts  do 
5: 
for i  <--- 1 --->  n  do 
C>  n:  number ofrules 
6: 
IIi <--- random  
7:  
end for 
8: 
LL  =  -inf  
9: 
iter  <--- 0 
10: 
11: 
repeat 
iter  <--- iter  +  1 
12: 
LLo  =  LL  
13: 
LL  =  EXPECTATION 
14: 
MAXIMIZATION 
15: 
until LL- LLo  <  E v  LL- LLo  <  -LL · 8  v  iter  >  max_iter  
16: 
if LL  >  BestLL  then 
17: 
BestLL  <--- LL  
18: 
BestPar  <--- [II1, ... , IIn] 
19: 
end if 
20: 
endfor 
21: 
return BestLL,  BestPar  
22: end function 
likelihood 
D 
n  
IIz)m1 D 
Q  (  D 
n  
IIz)mlq  )
L  =  
(1  -
_  
1 -
(1  -
(14.8) 
where mz_  =  .l:~=Q+ 1 mzr·  Its partial derivative with respect to IIi  is 
aL  
a TI~=1 (1 ~ rrz)mz_ rl (1-rl(1- IIz)mzq) 
arri 
arr, 
q=1 
1=1 
n 
n  
oflQ  (1- fln (1- II )mzq)
+ 
(1 - Ilz)mz_ 
q=1 
1=1 
l   
=
arri 
1 1 
1 
-mi-(1- IIi)m;-- n 
n   (1- Ilz)mz- I] 
Q  ( 1-n 
n  (1- Ilz)mzq )
n
l=1,l#t  
q-1 
1=1 
n 
Q  
n  
+  
.2:  
1 
(1- Ilz)mz-
miq(1- IIi)m;q- n (1- Ilz)mzq 
1=1 
q=1 
l=1,l#i  
(14.9)
q'=ö'#q  ( 1 -}] (1 - Ilz)mzq') 

14.7  Scaling  PILP  385 
Algorithm  38 Function Expectation 
1: function EXPECTATION 
2: 
LL  <--- 2:;iERules  m;- log(1- TI;) 
3: 
C>  m;_: total number of groundings ofrule i  with the body true in a negative examp1e 
4: 
for i  <--- 1 -->  n  do 
5: 
Cil <--- 0 
6: 
Cio ,f- mi­
7: 
end for 
8: 
for r  <--- 1 -->  P  do  
C>  P: number of positive examp1es 
9: 
probex  <--- 1- niERules(l- IT;)mir  
10:  
11: 
C>  m;r: number of groundings of rule i  with the body true in example r  
LL  <--- LL  +  logprobex  
12:  
for i  <--- 1 -->  n  do 
13: 
14: 
15: 
II·
condp  <--- probex  
eil <--- c;1 +  m;rcondp  
c;o <--- c;o +  m;r(1- condp)  
16: 
endfor 
17: 
end for 
18: 
return LL  
19: end function 
Algorithm  39 Function Maximization 
1: procedure MAXIMIZATION 
2: 
for i  <--- 1 -->  n  do 
3 · · 
rr  ·  =  
z 
---".i.L_ 
eil  +cio 
4: 
end for 
5: end procedure 
for the differentiation product rule, and 
aL   
n  
-mi_(l- IIi)m,_-1 TI (1- Ilz)mz- (1- IIi)m,_ 
arri D 
l=1,l#i  
(1  - IIi)m,_
(1- n(l- IIz)mlq)  +  
TI
n  
Q 
(1  - IIz)mz_ .2:  miq  TI~= 1 (1  - II1)mzq  
1=1 
q=1 
1 - rri 
TI 
(1 -TI (1 - IIz)mzq')  . 1 - TI~- 1 (1 - II1)mzq 
q'=1,q'#q  
1=1 
1 -
TI~=1 (1  -
Ilz)~· (14.10) 

386 
Structure  Learning  
by dividing and multiplying for (1 - IIi)mi-, (1 - IIi) and 1 -
O~=l (1 ­
IIz)mlq  various factors. Then 
aL  
mi_(1- IIi)m,_-1L +  ~ 
miq  TI~= 1 (1- IIt)mlqL  
arri 
(1- Ili)m,-
~1 (1- Ili)(1- TI~= 1 (1- Ilt)mlq) 
mi_L  +  ~ 
miq  TI~= 1 (1- IIt)m1qL 
1- rri 
~1 (1- IIi)(1- TI~=1(1- Ilt)mlq) 
L  
( ~ miq  TI~= 1 (1 - Ilt)mlq 
) 
1- II· 
LJ  1- fln_ (1- II1)mlq  - mi­
' 
q=1 
l-1 
L  
( Q  
1- P(eq)  
)  
1- Ili 
~1 miq  P(eq)  
- mi­
_L  (f miq  (-1 -1) -mi-) 
(14.11)
1-Ili q=1 
P(eq)  
by simple algebra. 
The equation gfii  = 0 does not admit a closed form solution, not even 
where there is a single clause, so we must use optimization to find the maxi­
mumof L.  
14.7 .1.3 Structure  learning  
The discriminative structure learning problern can be expressed as: given a 
set E+  =  {e1, ... , eQ} of positive examples, a set E-
=  {eQ+l, ... , eR}  
of negative examples and a background knowledge B,  find a liftable PLP P  
suchthat the likelihood is maximized. The background knowledge B  may be 
a normallogic program defining all the predicates except the target (the input 
predicates). 
We solve this problern by first identifying good clauses guided by the log 
likelihood (LL) of the data. Clauses are found by a top-down beam search. 
The refinement operator adds a literal to the body of the current clause, the 
literal is taken from a bottarn  clause  built as in Progoi [Muggleton, 1995]. 
The set of clauses found in this phase is then considered as a single theory 
and parameter learning is performed on it. Then the clauses with a parameter 
below a user define threshold WMin  are discarded and the theory is retumed. 
The resulting algorithm, LIFTCOVER, is very similar to SLIPCOVER, see 
Section 14.5. The difference between the two isthat LIFTCOVER uses lifted 
parameter learning instead of the EM algorithm over BDDs of [Bellodi and 

14.7  Scaling  PILP  387 
Riguzzi, 2013]. Moreover they use a different approach for performing the 
selection of the rules to be included in the final model: while SLIPCOVER 
does a hill-climbing search in which it  adds one clause at a time to the theory, 
learns the parameters and keeps the clause if the LL is smaller than before, 
LIFTCOVER learns the parameters for the whole set of clauses found during 
the search in the space of clauses. This is allowed by the fact that parameter 
learning in LIFTCOVER is much faster so it  can be applied to large theories. 
Then rules with a small parameter can be discarded as they provide small 
contributions to the predictions. In  practice structure search is thus performed 
in LIFTCOVER by parameter learning, as is done for example in [Nishino 
et al., 2014; Wang et al., 2014]. 
Algorithm  40  Function LIFTCOVER 
1:  function LIFTCOVER(NB, NI,  Nint,  NS,  NA,  NV)  
2: 
3: 
Beam  =INITIALBEAM(Nint, NS,  NA)   
C>  Bottom clauses building 
cc <--- 0  
4: 
Steps  <--- 1 
5: 
NewBeam  <--- D 
6: 
repeat 
7: 
Remove the firstpair ((Cl,  Literals),  LL)  from Beam  
C>  Remove the first clause 
8:  
Refs  <-CLAUSEREFINEMENTS((Cl, Literals,  NV))  
C>  Find all refinements Refs  of 
(Cl,  Literals)  
9: 
for all (Cl',  Literals')  E  Refs  do 
10: 
 
(LL 11 ,  {Cl"}) <-LEARNWEIGHTS({Cl'}) 
11:  
NewBeam  <-INSERT((Cl",  Literals'),  LL",  NewBeam,  NB)  
e>The 
refinement is inserted in the beam in order of likelihood, possibly removing the last clause if the size 
of the beam NB  is exceeded 
12: 
CC  <--- CC  u  {Cl'} 
13: 
end for 
14: 
Beam  <--- NewBeam  
15: 
Steps  <--- Steps  +  1 
16: 
until Steps  >  NI  or Beam  is empty 
17: 
(LL,  Th)  <-LEARNWEIGHTS(CC) 
18: 
Remove from Th  the clauses with a weight smaller than WMin  
19: 
return Th  
20:  end function 
Algorithm 40 shows the main LIFTCOVER function. Line 2 calls INI­
TIALBEAM that builds an initial beam Beam  consisting of bottom clauses 
and is very similar to Algorithm 35 
As in SLIPCOVER, a beam is a set of tuples ( (Cl,  Literals),  LL)  with 
Cl  a clause, Literals  the set of literals admissible in the body of Cl  and LL  
the log-likelihood of Cl.  Function INITIALBEAM returns returns an initial 
beam containing tuples ( ( h  : 0.5 :- true,  Literals),  -oo) for each bottom 
clause h  : 0.5 :- Literals.  The likelihood is initialized to -oo. 

388 
Structure  Learning  
Then LIFTCOVER runs a beam search in the space of clauses for the 
target predicate. 
In  each beam search iteration, the first clause of the beam is removed and 
all its refinements are computed. Each refinement Cl'  is scored by perform­
ing parameter learning with P  = {Cl'} and using the resulting LL as the 
heuristic. The scored refinements are inserted back into the beam in order 
of heuristic. If the beam exceeds a maximum user-defined size, the bottom 
elements are removed. Moreover, the refinements are added to a set of clauses 
cc.  
For each clause Cl  with Literals  admissible in the body, Function 
CLAUSEREFINEMENTS, the same as that of SLIPCOVER in Algorithm 36, 
computes refinements by adding a literal from Literals  to the body. Fur­
thermore, the refinements must respect the input-output modes of the bias 
declarations, must be connected (i.e., each body literal must share a variable 
with the head or a previous body literal) and their number of variables must 
not exceed a user-defined number NV.  Refinements are ofthe form (Cl',  L')  
where Cl'  is the refined clause Cl'  and L'  is the new set of literals allowed in 
the body of Cl'.  
Beam search is iterated a user-defined number of times or until the beam 
becomes empty. The output of this search phase is represented by the set 
CC  of clauses. Then parameter learning is applied to the whole set CC,  i.e., 
P  =  CC.  Finally clauses with a weight smaller than WMin  are removed. 
The separate search for clauses has similarity with the covering loop of 
ILP systemssuch as Aleph [Srinivasan, 2007] and Progoi [Muggleton, 1995]. 
Differently from the ILP case, however, the positive examples covered are not 
removed from the training set because coverage is probabilistic, so an exam­
ple that is assigned nonzero probability by a clause may have its probability 
increased by further clauses. A selection of clauses is performed by parameter 
learning: clauses with very small weights are removed. 
LIFTCOVER has been tested on 12 datasets in its two versions 
LIFTCOVER-EM and LIFTCOVER-LBFGS. Overall we see that both lifted 
algorithms are usually faster, sometimes by a large margin, with respect to 
SLIPCOVER, especially LIFTCOVER-EM. Moreover, this system often finds 
better quality solutions, showing that structure search by parameter learning 
is effective. 

14.7  Scaling  PILP  389 
14.7.2  SLEAHP  
SLEAHP [Nguembang Fadja et al., 2021] tries to achieve scalability by re­
stricting the langnage to Hierarchical Probabilistic Logic Programs (HPLPs) 
which, by construction, satisfy the independent-or assumption. This makes 
the computation of probabilities in such programs "truth-functional" [Gerla, 
2001]. With the independent-or assumption we can collect the contribution 
of multiple groundings of a clause. Therefore it is suitable for domains where 
entities may be related to a varying number of other entities. The differences 
in numbers are taken into account by the semantics. The assumption is not 
weaker or stronger than the exclusive-or one. It is probably less easy for users 
to write HPLP programs than PRISM programs but our aim is to devise an 
algorithm for automatically learning them. 
14.7.2.1  Hierarchical  probabilistic  logic  programs  
Suppose we want to compute the probability of atoms for a single predicate 
r  using a PLP. In particular, we want to compute the probability of a ground 
atom r(t),  where t  is a vector ofterms. Let us call r  the target predicate. This 
is a common situation in relationallearning. 
We consider a specific form of LPADs defining r  in terms of input  predi­
cates  (their definition is given as input and is certain) and hidden  predicates,  
defined by clauses of the pro gram. We discriminate between input predicates, 
which encapsulate the input data and the background knowledge, and tar­
get predicates, which are predicates we are interested in predicting. Hidden 
predicates are disjoint from input and target predicates. Each clause in the 
program has a single head atom annotated with a probability. Furthermore, 
the program is hierarchically defined so that it can be divided into layers. 
Each layer defines a set of hidden predicates in terms of predicates of the 
layer immediately below or in terms of input predicates. A generic clause C  
is of the form 
C  =  p(X): 1r ~ cp(X,  Y),  b1(X, Y),  ... , bm(X,  Y)  
where cp(  X,  Y)  is a conjunction ofliterals for the input predicates. The vector 
X  represents variables appearing in the head of C  and Y  represents the 
variables introduced by input predicates. bi(X,  Y)  for i  = 1, ... , m  is a 
literal built on a hidden predicate. Variables in Y  are existentially quantified 
with scope the body. Only literals for input predicates can introduce new 
variables into the clause. Moreover, all literals for hidden predicates must 
use the whole set of variables of the predicate in the head X  and input 
predicates Y,  see predicate r1_1 (A,  B,  C)  of Example 125. This restriction 

390 
Structure  Learning  
is imposed to allow independence among ground clauses associated to tar­
getlhidden predicates, see Section 14.7.2.1.1. Moreover, we require that the 
predicate of each bi  (X,  Y)  does not appear elsewhere in the body of C  or 
in the body of any other clause, i.e each hidden predicate literal is unique in 
the program. We call hierarchical PLP (HPLP) the language that admits only 
programs ofthisform Nguembang Fadja et al. [2017]. A generic hierarchical 
program is defined as follows: 
C1 =  r(X)  :  7!"1 
+---
c/J1, b1_1, ... , b1_m1 
Cn  =  r(X)  :  7rn 
+-
c/Jn,  bn_l  , · · · , bn_mn   
Cl_l_l =  n_1 (X) : 7rl_l_l 
+---
c/Jl_l_l, b1_1_1_1, . · · , b1_1_1_m111  
C1_1_n11 =  r1_1 (X) : 7rl_l_n11 
+---
c/Jl_l_nn, bl_l_nn_l, · · · , bl_l_nn_mnn11 
Cn_l_l =  Tn_l  (X) : 7l"n_l_l 
+---
c/Jn_l_l , bn_l_l_l , · · · , bn_l_l_mn 11 
Cn_l_nn1  =  Tn_l(X): 7l"n_l_nn1 
+---
4>n_1_nnl,  bn_l_nnl-1,  • • • 'bn_l_nnl-ffinlnnl  
where r  is the target predicate and r1_1..._n  is the predicate of b1_1..._n,  e.g. 
r1_1 and rn_l  are the predicates of b1_1 and bn_l  respectively. The bodies of 
the lowest layer of clauses are composed only of input predicates and do not 
contain hidden predicates, e.g C2 and C1_1_1 in Example 125. Note that here 
the variables were omitted except for rule heads. 
A generic program can be represented by a tree, see Figure 14.3, with a 
node for each clause and literal for hidden predicates. Each clause (literal) 
node is indicated with Cp  (bp) where p  is a sequence of integers encoding 
the path from the root to the node. The predicate of literal bp  is r P  which is 
different for every value of p.  
~~~  
~ 
...  
~ 
/1~ 
/1~ 
bt_t 
· · · 
bLm1 
bn_l  
· · · 
bn_mn  
/I~ 
/I~ 
/I~ 
/I~ 
Ct_l_l 
· · · 
CLLnn 
CLm1-l 
· · · 
CLm1-n1m 1 Cn_Ll 
· · · 
Cn_Lnnl  Cn_mn_l  
· · · 
Cn_mn-nnmn  
Figure  14.3  Probabilistic program tree. 

14.7  Scaling  PILP  391 
Example  125  (HPLP for the UW-CSE domain). Let  us  consider  a  modified  
version  ofthe  program  of Example  124:  
C1 =  advised_by(A,  B)  : 0.3 ~ 
student(A),professor(B),project(C,  A),project( C,  B),  
r1_1(A, B,  C).  
C2 = advised_by(A,  B)  : 0.6 ~  
student(A),professor(B),  ta( C,  A),  taughtby(C,  B).   
C1_1_1 = r1_1(A, B,  C): 0.2 ~ 
publication(P,  A,  C),publication(P,  B,  C).  
where  publication(P,  A,  C)  means  that  P  is  a  publication  with  author  A  
produced  in  project  C  and  studentj1,professorj1,projectj2,  ta/2,  
taughtby/2  and  publication/3  are  input  predicates.  
In  this  case,  the  probability  of q  =  advised_by(harry,  ben)  depends  not  
only  on  the  number  of joint  courses  and  projects  but  also  on  the  number  of  
joint publications from projects.  Note  that the set  of variables of the hidden  
predicate  r1_1 (A,  B,  C) is  the  union  of  the  variables  X  =  {  A,  B} of  the  
predicate  advised_by(A,  B)  in  the  head  ofthe  clause  and  Y  =  {  C} which  is  
the variable introduced by the input predicate  project( C, A) in the  body.  The  
clause for  r1_1 (A,  B,  C) ( C1_1_1) computes  an  aggregation  over publications  
of a  project  and  the  clause  in  the  level  above  (Cl)  aggregates  over  projects.  
This program can be represented with the tree of Figure 14.4. 
advisedby(A,  B) 
/~
C1 
c2 
I  
TI_l (A,  B,  C) 
I  
CLLl 
Figure  14.4  Probabilistic program tree for Example 125. 
14.7.2.1.1  Inference  in  HPLP  HPLPs satisfy the independent-or assump­
tion as every hidden predicate appears just once in the body of a single clause 

392 
Structure  Learning  
and has all the variables of the clause as arguments. If we consider the ground­
ing of the program, each ground atom for a hidden predicate appears in the 
body of a single ground clause. A ground atom for a hidden predicate may 
appear in the head of more than one ground clauses and each ground clause 
can provide a set of explanations for it but these sets do not share random 
variables. Moreover, HPLPs also satisfy the independent-and assumptions as 
every ground atom for a hidden predicate in the body of a ground clause has 
explanations that depend on disjoint sets of random variables. 
Remernher that atoms for input predicates are not random, they are either 
true or false in the data and they play a role in determining which groundings 
of clauses contribute to the probability of the goal: the groundings whose 
atoms for input predicates are false do not contribute to the computation of 
the probability. 
Given a ground clause Gpi  =  ap  : 1rpi  ~ bpil, ... , bpimp
• where p  is a 
1  
path, we can compute the probability that the body is true by multiplying the 
probability of being true of each individualliterals. If the literal is positive, 
its probability is equal to the probability of the corresponding atom. Other­
wise it is one minus the probability of the corresponding atom. Therefore the 
probability of the body of Gpi  is P(bpil,  ...  ,  bpimp )  =  TIZ?k  P(bpik)  and 
1 
P(bpik)  =  1 - P(apik)  if bpik  ="'apik·  If P(bpik)  is a literal for an input 
predicate, P(bpik)  =  1 if it is true and P(bpik)  =  0 otherwise. We can use 
this formula because HPLPs satisfy the independent-and assumption. 
Given a ground atom ap  for a hidden predicate, to compute P(ap)  we 
need to take into account the contribution of every ground clause for the 
predicate of ap.  Suppose these clauses are { Gp1, ... , Gpap}.  If we have a 
single ground clause Gpl = ap  : 7rpl ~ bpn,  ...  ,  bplmpl' then 
P(ap)  = 7rpl · P(body(Gpl)).  
If we have two clauses, the contribution of each clause is computed as above. 
Note that, random variables associated with these contributions are indepen­
dent since we allow hidden predicates in the body of each clause to use the 
whole set of variables appearing in the head and those introduced by input 
predicates. It  is also worth noting that the probability of a disjunction of two 
independent random variables is 
P(a  v  b)  =  P(a)  +  P(b)- P(a)  · P(b)  =  1- (1- P(a))  · (1- P(b)).  

14.7  Scaling  PILP  393 
Therefore, if we have two clauses their contributions are combined as follows: 
P(ap)  =  1- (1- 1rpl · P(body(Gpi))  · (1- 1rp2 · P(body(Gp2)))  
= (7rpl · P(body(Gpi)))  EB (7rp2 · P(body(Gp2))).  
where we used the operator EB that combines two probabilities as follows 
p  EB q  =  1 - (1 - p)  · (1 - q).  This operator is commutative and associative 
and we can compute sequences of applications as 
EB  Pi  =  1 - n 
(1  - Pi)  
(14.12) 
i  
The operator is called probabilistic sum and is the t-norm of the product fuzzy 
logic Hajek [1998]. Therefore, if the probabilistic program is ground, the 
probability of the example atom can be computed with the Arithmetic Circuit 
(AC) of Figure 14.5, where nodes are labeled with the operation they perform 
and edges from EB nodes are labeled with a probabilistic parameter that must 
be multiplied by the child output before combining it with EB. We can thus 
compute the output p  in time linear in the number of ground clauses. Note that 
the AC can also be interpreted as a deep neural network with nodes ( or neu­
rons) whose (non linear) activation functions are x and EB. When the program 
is not ground, we can build its grounding obtaining a circuit/neural network 
of the type of Figure 14.5, where however some of the parameters can be 
the same for different edges. In  this case the circuit will exhibit parameter 
sharing. 
p  I  
~~~ 
X 
··· 
X 
PL/ I  ~Lm1 
Pn_/ I  ~n-=n 
ffi  
ffi  
ffi  
ffi  
~1-~ I ~Ln11 ~Lm~ I ~=1-n1m1 ~n-~ I ~Lnn1 ~n-=/ I ~=n-nnmn 
X  
X 
X  
X 
X  
X 
X  
X  
Figure  14.5  Arithmetic circuit/neural net. 
Example  126  (Complete HPLP for the UW-CSE domain). Consider  the  
completed  version  of Example  125:  An  online  implementation  can  be  found  
at  https:/1 cplint.eu/ e/philluwcse.pl  

394 
Structure  Learning  
C1 = advised_by(A,  B)  : 0.3 ~ 
student(A),professor(B),  project(C,  A),project(C,  B),  
r1_1(A, B,  C). 
C2 = advised_by(A,  B)  : 0.6 ~  
student(A),professor(B),  ta(C,  A),  taughtby(C,  B).   
C1_1_1 =  r1_1 (A,  B,  C) : 0.2 ~ 
publication(P,  A,  C),publication(P,  B,  C).  
student( harry).   
professor(ben).   
project(pr1,  harry).  project(pr2,  harry).   
project(pr1,  ben).  project(pr2,  ben).   
taught_by(q,  ben).  taught_by(c2,  ben).   
ta(q,  harry).  ta(c2,  harry).   
publication(pl,  harry, pr1).  publication(p2,  harry,  pri).   
publication(p3,  harry,pr2).  publication(p4,  harry,pr2).   
publication(p1,  ben,pr1).  publication(p2,  ben,pr1).   
publication(p3,  ben,pr2).  publication(p4,  ben,pr2).   
where  we  suppose  that  harry  and  ben  have  two  joint  courses  c1 and  c2, two  
joint  projects  pr1  and  pr2,  two  joint  publications  Pl  and  P2 from  project  pr1  
and  two  joint  publications  P3  and  P4  from  project  pr2.  The  resulting  ground  
program  is  
G1 = advisedby(harry,  ben)  : 0.3 ~ 
student(harry),  professor(ben), project(pr1,  harry),  
project(pr1,  ben),  r1_1 (harry,  ben,pri).  

14.7  Scaling  PILP  395 
G2 
= advisedby(harry,  ben)  : 0.3 ~ 
student( harry), pro fessor(ben),  project(pr2,  harry),  
project(pr2,  ben),  r1_1(harry,  ben,pr2).  
G3 
advisedby(harry,  ben)  : 0.6 ~ 
student(harry),professor(ben),  ta(q,  harry),  
taughtby(c1,  ben).  
G4 
advisedby(harry,  ben)  : 0.6 ~ 
student(harry),professor(ben),  ta(c2,  harry),  
taughtby(c2,  ben).  
G1_1_1 
r1_1(harry,  ben,pr1):  0.2 ~ 
publication(p1,  harry,  pri),  publication(p1,  ben,  pr1).  
G1 1 2 
r1_1(harry,  ben,pr1):  0.2 ~ 
publication(p2,  harry,  pr1),  publication(p2,  ben,  pr1).  
G2_1_1 
r1_1(harry,  ben,pr2):  0.2 ~ 
publication(p3,  harry,pr2),publication(p3,  ben,pr2).  
G2_1_2 
r1_1(harry,  ben,pr2):  0.2 ~ 
publication(p4,  harry,pr2),publication(p4,  ben,pr2).  
The  program  tree  is  shown  in  Figure  14.6  and  the  corresponding  arithmetic  
circuit  in  Figure  14.7  tagether  with  the  values  computed  by  the  nodes.  Infer­
ence  in  the  AC  of Figure  14.7  can  be  computed  at  https://cplint.eule/phill  
uwcse.pl  by  running  the  query  
inference_hplp(advisedby(harry,  ben),  ai,  Prob).  
adivsedby(harry,  ben)  
G1 ~/~~
G2 
G3 
 
G4 
rl_l  (harry, I 
  
ben, 
I   
GLLl 1\ 
GLL2  
 prl)  
rl_l  (harry, 
1\ 
 ben, 
  
 pr2)  
G2_Ll 
G2_L2 
Figure  14.6  Ground probabilistic program tree for Example 126. 

396 
Structure  Learning  
0.8731 
EB  
0.361~
1 
X 
~ 
X 
X 
~ 
X 
0.36' 
0.36' 
EB  
EB  
1 
1 
o(\'  o(\'  
1 
1 
1 
1 
Figure  14.7  Arithmetic circuit/neural net for Example 126. 
14.7 .2.1.2  Building  the  Arithmetic  Circuit  The network can be built by 
performing inference using tabling and answer subsumption using PITA (IND, 
IND) [Riguzzi, 2014; Riguzzi and Swift, 2010], see Section 8.10. Computa­
tion by PITA(IND, IND) is thus equivalent to the evaluation of the program 
arithmetic circuit. 
We use an algorithm similar to PITA(IND,IND) to build the Arithmetic 
circuit or the neural network instead of just computing the probability. To 
build the arithmetic circuit, it is enough to use the extra argument for stor­
ing a term representing the circuit instead of the probability and changing 
the implementation of the predicates for combining the values of the extra 
arguments in the body and for combining the values from different clause 
groundings. The result of inference would thus be a term representing the 
arithmetic circuit. 
There is a one to one correspondence between the data structure built by 
PITA(IND,IND) and the circuit so there is no need of an expensive compila­
tion step. The term representing the AC in Figure 14.7 is2 
2  Obtaining by running the query inference_hplp(advisedby(harry,  ben),  ai,  Prob,  
Circuit).  at https://cplint.eu/e/philluwcse.pl 

14.7  Scaling  PILP  397 
or([and([O, or([and([2])])]), and([O, or([and([2])])]), 
and([O, or([and([2])])]), and([O, or([and([2])])]), and([1]), and([1])]) 
where the operators and  and or  represent the product x and the proba­
bilistic sum EB respectively. Once the ACs are built, parameter learning can 
be performed over them by applying gradient descent/back-propagation or 
Expectation Maximization. The following section presents these algorithms 
and their regularized versions. Because of the constraints imposed on HPLPs, 
writing these programs may be unintuitive for human so we also propose, in 
Section 14.7.2.3, an algorithm for learning both the structure and the param­
eters from data. 
14.7.2.2  Parameter  learning  
Here we present the algorithm Parameter learning for Hlerarchical probabilis­
tic Logic programs (PHIL), which learns HPLP's parameters from data. We 
present two versions ofPHIL. The first, Deep PHIL (DPHIL), learns HPLP's 
parameters by applying a gradient-based method and the second, Expectation 
Maximization PHIL (EMPHIL), applies Expectation Maximization (EM). 
Different regularized versions of each algorithm will also be presented. The 
parameter learning problern can be defined as follows: 
Definition  72 (Parameter Learning Problem). Given  a  HPLP  H  with  param­
eters  II  =  {7rl · · · 7rn}, an  interpretation  I  defining  input  predicates  and  a  
training  set  E  =  {e1, ... , eQ,  "'eQ+l, ... , "'eR}  where  each  ei  is  a  ground  
atom  for  the  target  predicate  r,  find  the  values  of II  that  maximize  the  log  
likelihood  (LL)  
Q  
R  
LL  = argmax(l: logP(ei) +  2.:  log(1- P(ei))) 
(14.13) 
rr  
i=l 
i=Q+l 
where  P( ei) is  the  probability  assigned  to  ei  by  H  u I.  
Maximizing the LL can be equivalently seen as minimizing the sum of 
cross  entropy  errors  erri  for all the examples 
R  
err  =  2.: ( -yi  log P( ei) - (1 - Yi)  log(1 - P( ei))) 
(14.14) 
i=l 

398 
Structure  Learning  
where ei  is an example with Yi  indicating its sign (Yi = 1 if the example is 
positive and Yi  = 0 otherwise) and P  (ei) indicating the probability that atom 
ei  is true. 
DPHIL and EMPHIL minimize the cross entropy error or equivalently 
maximize the log-likelihood of the data. These algorithms (and their regular­
ized variants) are presented in subsections 14.7.2.2.1 and 14.7.2.2.3 respec­
tively. 
14.7.2.2.1  Gradient  Descent:  DPHIL  DPHIL computes the gradient of 
the error err  (14.14) with respect to each parameter and updates the pa­
rameters. We do this by building an AC for each example and by running 
a dynamic programming algorithm for computing the gradient. To simplify 
gradient computation, we transformed the AC of Figure 14.5 as follows: 
weight, 1ri, labeling arcs from EB to x nodes, are set as children leaves of 
x nodes and shared weights are considered as individual leaves with many 
x parents. Moreover, negative literals are represented by nodes of the form 
not(a)  with the single child a.  The AC in Figure 14.7 is converted into the 
one shown in Figure 14.8. Note that, the ACs in figures 14.7 and 14.8 are 
equivalent but the one in Figure 14.8 highlights parameters sharing which is 
more convenient for illustrating the algorithm. 
Figure  14.8  Converted arithmetic circuit ofFigure 14.7. 
The standard gradient descent algorithm computes gradients at each it­
eration using all examples in the training set. If the training set is large, the 

14.7  Scaling  PILP  399 
algorithm can converge very slowly. To avoid slow convergence, gradients 
can be computed using a single example, randomly selected in the training 
set. Even if in this case the algorithm can converge quickly, it is generally 
hard to reach high training set accuracy. A compromise often used is mini 
batch stochastic gradient descent (SGD): at each iteration a mini batch of 
examples is randomly sampled to compute the gradient. This method usually 
provides fast converge and high accuracy. DPHIL, shown in Algorithm 41, 
implements SGD. 
After building the ACs and initializing the weights, the gradients and the 
moments, line 2-6, DPHIL performs two passes over each AC in the current 
batch, line 8-15. In  the first, the circuit is evaluated so that each node is 
assigned a real value representing its probability. This step is bottom-up or 
forward (line 12) from the leaves to the root. The second step is backward 
(line 13) or top-down, from the root to the leaves, and computes the deriva­
tives of the loss function with respect to each node. At the end of the backward 
step, G  contains the vector of the derivatives of the error with respect to each 
parameter. Line 16 updates the weights. 
The parameters are repeatedly updated until a maximum number of steps, 
M ax!ter,  is reached, or until the difference between the LL of the current and 
the previous iteration drops below a threshold, E, or the difference is below 
a fraction 6  of the current LL. Finally, function UPDATETHEORY (line 18) 
updates the parameters of the theory. We reparametrized the program using 
weights between -oo and +  oo and expressing the parameters using the sigma 
1
function 7ri =  D"(Wi)  =  l+e wi  (14.15). In this way we do not have to 
impose the constraint that the parameters are in [0,1]. 
Function FORWARD of Algorithm 42 is a recursive function that takes 
as input an AC node  (root node) and evaluates each node from the leaves 
to the root, assigning value v(n)  to each node n.  If node  =  not(n),  p  =  
FORWARD(n) is computed and 1 -
p  is assigned to v(node),  lines 2-5. 
If node  =  ffi(nl,  ... nm).  function v(ni)  =  FORWARD(ni) is recursively 
called on each clrild node, and the node value is given by v(node)  = v(n1 )  EB 
... EB v(ni),  lines 7-13. If node  = x (1ri, n1, ... nm).  function v(ni)  = 
FORWARD(ni) is recursively called on each child node, and the node value is 
given by v(n)  = 7ri · v(nl)  · ... · v(nn).  lines 14-20. 
Procedure BACKWARD takes an evaluated AC node  and computes the 
derivative ofthe contribution of the AC to the cost function, err  = -y log(p)­
(1 - y)  log(1 - p)  where p  is the probability of the atom representing the 

400 
Structure  Learning  
Algorithm  41  Function DPHIL. 
1:  function PHIL(Theory,  E, ö, Maxlter,  ß1, ß2, TJ,  f., Strategy)  
2: 
Examples  +--- BVILDACs(Theory)   
t> Build the set of ACs 
3: 
for i  +--- 1 ~ ITheoryl  do 
t> Initialize weights,gradient and moments vector 
4: 
W[i]  +--- random(Min,  Max)  
t> initially W[i]  E [Min,  M  ax].  
5: 
G[i] +--- 0.0, Mo[i] +--- 0.0, Ml[i] +--- 0.0 
6: 
end for 
7: 
Iter  +--- 1 
8: 
repeat 
9: 
LL  +--- 0 
10:  
Batch  +--- NEXTBATCH(Examples)  
t> Select the batch according to the 
strategy 
11: 
for all node  E  Batch  do 
12: 
P +--- FORWARD(node)  
13: 
BACKWARD(G, -j,,  node)  
14: 
LL  +--- LL  +  logP 
15: 
end for 
16: 
UPDATEWEIGHTSADAM(W, G,  Mo,  M1, ß1, ß2, TJ,  f., lter)  
17: 
until LL- LLo  <  E  v LL- LLo  <  -LL.o  v Iter  >  Maxlter  
18: 
FinalTheory  +--- VPDATETHEORY(Theory,  W)  
19: 
return FinalTheory  
20:  end function 
example. This derivative is given in Equation 14.16 
oerr  
d  
1
---- (n)- 
(14.16)
ov(n)  -
v(r)  
with 
d(pan)  v~~~  
if n  is a EB  node, 
d(pan)  
1
~~v(~))  
if n  is a x  node
d(n)  =  
(14.17) 
.l:Pan  d(pan).v(pan)-(1  - 7ri) 
if n  =  i7(Wi)  
-d(pan)   
if pan  =  not(n)  
where pan  indicates the parents of n.  
This Ieads to Procedure BACKWARD shown in Algorithm 43 which is a 
simplified version for the case v(n)  =I=  0 for all EB  nodes. To compute d(n),  
BACKWARD proceeds by recursily propagating the derivative of the parent 
node to the children. Initially, the derivative of the error with respect to the 
root node, - v(lr),  is computed. If the current node is not( n),  with derivative 
AccGrad,  the derivative of its unique child, n,  is - AccGrad,  line 2-3. If the 

14.7  Scaling  PILP  401 
Algorithm  42  FUNCTION FORWARD 
1: 
function FORWARD(node) 
1>  node is an AC 
2: 
if node  =  not(n)  then 
3: 
v(node)  ~ 1- FORWARD(n) 
4: 
return v(node)  
5: 
eise 
6: 
1>  Compute the output example by recursively call Forward on its sub AC 
7: 
if node  =  EBCn1, ... nm)  then 
1>  EB node 
8: 
for all ni  do 
9: 
v(nj)  ~ FORWARD(nj) 
10: 
end for 
11: 
v(node)  ~ v(n1)  E8 ... E8 v(nm)  
12: 
return v(node)  
13: 
eise 
1>  and Node 
14: 
if node  =  x ('rr;, n1, ... nm)  then 
15: 
for all ni  do 
16: 
v(nj)  ~ FORWARD(nj) 
17: 
end for 
18: 
v(node)  ~ 7r; · v(n1)  · ... · v(nm)  
19: 
return v(node)  
20: 
end if 
21: 
end if 
22: 
end if 
23: end function 
current node is a EB  node, with derivative AccGrad,  the derivative of each 
child, n,  is computed as follows: 
AccGrad'  = AceGrad.  1- v(node)  
and back-propagated, line 5-9. If the current node is a x node, the derivative 
of a non leaf child node n  is computed as follows: 
AccGrad~ =  AceGrad.  v(node) 
v(n)  
The derivative for a leaf child node n  = 1ri is 
AccGrad~ = AceGrad  · v(node)  · (1- O"(Wi)) 
and back-propagated, line 11-15. For leaf node, i.e a 1ri node, the derivative 
is accumulated, line 20. 

402 
Structure  Learning  
Algorithm 43 PROCEDURE BACKWARD 
1: 
procedure BACKWARD(G, AccGrad, node)  
2: 
ifnode =  not(n)  then 
3: 
BACKWARD(G,-AccGrad,n)  
4: 
eise 
5: 
if node  =  EBCn1, ... nm)  then 
t>EB node 
6: 
for ali nj  do 
7: 
AccGradi  ~ AceGrad  · v~'(~~)) 
8: 
BACKWARD(G, AccGrad',  ni)  
9: 
end for 
10: 
eise 
11: 
if node  =  x(rri ·n1,  ... nm)  then 
t> x  node 
12: 
for ali ni  do 
t> non leaf child 
13: 
AccGrad'  ~ AceGrad  · 
1
~~~c~:)l 
14: 
BACKWARD(G,  AccGradi, nj)  
15: 
end for 
16: 
AccGrad~ ~ AceGrad  · v(node).(l- a-(Wi))  
t> leaf child 
17: 
BACKWARD(G, AccGrad~, 1ri) 
18: 
eise 
t> leafnode 
19: 
let node  =  'll"i 
20: 
G[i] ~ G[i] +  AceGrad  
21: 
end if 
22: 
end if 
23: 
end if 
24: end procedure 
After the computation of the gradients, weights are updated. Standard 
gradient descent adds a fraction 'f/, called learning rate, of the gradient to 
the current weights. 17  is a value between 0 and 1 that is used to control 
the parameter update. Small 17  can slow down the algorithm and find local 
minimum. High 17  avoids local minima but can swing araund global minima. 
A good compromise updates the learning rate at each iteration combining 
the advantages of both strategies. We use the update method Adam,  adaptive 
moment estimation [Kingma and Ba, 2014], that uses the first order gradient 
to compute the exponential moving averages of the gradient and the squared 
gradient. Hyper-parameters ß1. ß2 E[0, 1) control the exponential decay rates 
of these moving averages. These quantities are estimations of the first  moment  
(the mean Mo) and the second  moment  (the uneentered variance MI) of the 
gradient. The weights are updated with a fraction, current learning rate, of 
the combination of these moments, see Procedure UPDATEWEIGHTSADAM 
in Algorithm 44. 

14.7  Scaling  PILP  403 
Algorithm  44  PROCEDURE UPDATEWEIGHTSADAM 
1: 
procedure UPDATEWEIGHTSADAM(W, G,  Mo,  M1, ß1, ß2, TJ,  s,  iter)  
~ 
2: 
T/iter  ~ TJ--'-1:--::ßtiit':::;er,--­
-
1 
3: 
for i  ~ 1 ~ lW I  do 
4: 
5: 
Mo[i] ~ ß1 · Mo[i] +  (1- ß1) · G[i] 
Mt[i] ~ ß2 · Mt[i] +  (1- ß2) · G[i] · G[i] 
6: 
W[i]  ~ W[i]  -
T/iter  · ~
• 
(yMl[,])+< 
7: 
end for 
8: 
end procedure 
14.7.2.2.2  DPHIL  regularization:  DPHIL1  and  DPHIL2  In deep learn­
ing and machine learning in general, a technique called regularization  is 
often used to avoid over-fitting. Regularization penalizes the loss function by 
adding a regularization  term for favoring small parameters. In the literature, 
there exist two main regularization techniques called L1 and L2 regulariza­
tion that differ on the way they penalize the loss function. While L 1  adds to 
the loss function the sum of the absolute values of the parameters, L2 adds 
the sum of their squares. Given the loss function defined in Equation 14.14, 
the corresponding regularized loss function are given by equations 14.18 and 
14.19. 
R  
k  
err1  = .2:: -ydog  P(ei)- (1- Yi)  log(1- P(ei))  +  1.2:: l1ril  (14.18) 
i=1 
i=1 
R  
k  
err2  = .2:: -yi  log P(ei)- (1- Yi)  log(1- P(ei))  +  '1  .2:: 1r[  
(14.19)
2 
. 1 
. 1
2= 
2= 
where the regularization hyper-parameter 1 determines how much to penalize 
the parameters and k  is the number ofparameters. When 1 is zero, the regular­
ization term becomes zero and we areback to the originallass function. When 
1 is large, we penalize the parameters and they tend to become small. Note 
also that we add the regularization term to the initiallass function because we 
are performing minimization. The main difference between these techniques 
is that, while L1 favor sparse parameters (many parameters equal to zero ), 
L2 brings parameters to be close to 0, not necessarily equal to 0. Moreover 
in general, L1 (resp. L2) is computationally inefficient (resp. efficient due to 
having analytical solutions). 

404 
Structure  Learning  
Now let us compute the derivative of the regularized error with respect to 
each node in the AC. The regularized term depends only on the leaves (the 
parameters 7ri) ofthe AC. So the gradients ofthe parameters can be calculated 
by adding the derivative of the regularization term with respect to 1ri to the 
one obtained in Equation 14.17. The regularized errors are given by: 
E  
- {' .L:7=1 1ri 
for L1 
reg  -
'Y  
k   
(14.20) 
2 .L:i=l 1r[  for L2 
where 1ri = O"(Wi)·  Note that since 0 ~ 1ri ~ 1 we can consider 1ri rather 
than l1ril in L1. So 
1 ou(Wi)  = /. O"(Wi).  (1- O"(Wi))  = r  · 1ri · (1- 1ri)
:'lß  
aw, 
u  reg  _ 
{ 
oWi   
2 
1.  oo-(Wi) = /. O"(W·).  O"(Wi).  (1- O"(Wi))  = r  ·  1r[  · (1- 1ri) 
2 awi  
t 
(14.21) 
So Equation 14.17 becomes 
d(pa  ) v(pan)  
if 
 
n is a EB  node,
n v(n)   
 
d(pa  ) l-v(pan)   
if n  is a x node 
d(n)  =  
n  l-v(n) 
.L:pan  d(pan). v(pan). (1  - 1ri) +  0:-Ji?  if n  = O"(Wi)   
-d(pan)   
' 
if pan  = not(n)  
(14.22) 
In order to implement the regularized version of DPHIL (DPHIL1 and 
DPHIL2), the forward and the backward passes described in algorithms 42 
and 43 remain unchanged. The unique change occurs while updating the 
parameters in Algorithm 44. UpdateWeightsAdam  line 6 becomes 
. 
Mo[i] 
+  oEreg  
W[i]  ~ W[z]-
(14.23)
rliter  *  (.JMJi))  +  E  
oWi  
14.7.2.2.3  Expectation  Maximization:  EMPHIL  EMPHIL learns the pa­
rameters of HPLP by applying Expectation Maximization (EM). The algo­
rithm maximizes the log-likelihood LL  defined in Equation 14.13 by al­
temating between an Expectation (E) and a Maximization (M) step. E-step 
computes the expected values of the incomplete data given the complete data 
and the current parameters and the M -step determines the new values of the 

14.7  Scaling  PILP  405 
parameters that maximize the likelihood. Bach iteration is guaranteed to in­
crease the log-likelihood. Given a hierarchical  PLP H  = {Cili = 1, · · · , n} 
(each Ci  annotated with the parameter 7ri) and a training set of positive and 
negative examples E  = {e1, ... , eQ,  ~eQ+1, ... , ~eR}. EMPHIL proceeds 
as follows: 
Let Ci  be a generic rule and g( i) = {j IBj is a substitution grounding Ci}. 
Forasingle example e,  the E-step computes E[ cwle] and E[ Ci1le] for all rules 
Ci.  Cix  is the number of times a variable Xij  takes value x  for x  E  {0, 1 }, 
for all j  E  g(i).  So E[cixle] = .l:jEg(i) P(Xij  = xle). These values are 
aggregated over all examples obtaining 
No[i] = E[cw] = .2: .2: P(Xij  =Oie) 
(14.24) 
eEE  jEg(i) 
N1[i] = E[cil] = .2: .2: P(Xij  = 1le) 
(14.25) 
eEE  jEg(i) 
· b 
. 
lik l"h  d . 
· -
NI[i]
Then the M-step computes 7r2 y max1mum 
e 1  oo , 1.e. 1r2 -
No[i]+NI[i]" 
Note that for a single substitution Bj  of clause Ci  we have P(Xij  = Oie) +  
P(Xij  = 1le) = 1. So E[cw]  +  E[cil] = .l:eEE lg(i)l. So the M-step 
computes 
N1[i] 
1ri =  
(14.26) 
.l:eEE lg( i) I 
Therefore to perform EMPHIL, we have to compute P(Xij  = 1le) for each 
example e  by performing the belief propagation algorithm over the factor 
graph associated with the AC. Message passing is then applied over the AC. 
The messages in the bottom-up, CN,  and top-down, tN,  directions are respec­
tively given as 
CN  = v(N)  
(14.27) 
tp  
if P  is a E8 node
t p  +v( P)8v( N)·tp + (1-v(P)Sv( N) )·(1-tp) 
tp-~+(1-tp)·(1-~)
f  
-
v(N)  
v(N)  
if P  is a x node 
N  -
tp-~+(1-tp)·(1-~)+(1-tp)
[ 
v(N)  
v(N) 
1- tp  
if P  is a not  node 
(14.28) 

406 
Structure  Learning  
where v(N)  is the value of node N,  p  is its parent and the Operatoreis 
defined as 
1 -v(p)  
(14.29)
v(p)  ev(n)  = 1- 1- v(n)  
CN  is computed by applying the forwardpass described in Algorithm 42. 
Since the belief propagation algorithm (for ACs) converges after two 
passes, we can compute the unnormalized belief of each parameter during 
the backward pass by multiplying tN  by v(N)  (that is all incoming mes­
sages). Algorithm 45  performs the backward pass of belief propagation algo­
rithm and computes the normalized belief of each parameter, i.e., tN.  It  also 
computes the expectations No[i] and N1 [i] for each parameter, lines 17-19. 
Algorithm 45 PROCEDURE BACKWARD IN EMPHIL 
1: 
procedure BACKWARDEM(tp, node,  No,  Nl) 
2: 
ifnode  =   not(n)  then 
3: 
BACKWARD(l- tp,  n,  B, Count)  
4: 
eise 
5: 
if node  =  E9(n1, ... nm)  then  
1> E8  node 
6: 
for ali child ni  do 
7•   
·  
t  
+--
tp  
n;  
tp+v(node)8v(n;)·tp+(1  v(node)8v(n;))·(1  tp)  
8:  
BACKWARDEM(tn;, n;,  B,  Count)  
9: 
end for 
10: 
eise 
11: 
if node  =  x(n1,  ... nm)  then  
1> x node 
12:  
for ali child ni  do 
13: 
t  .  v(node)  +(1-t )·(1- v(node)) 
f  
P  
v(ni)  
P  
v(ni) 
n;  +--
t  .  v(node)  +( 1-t )·(1- v(node)  )+(1-t )
P  
v(ni)  
P  
v(ni)  
P  
14:  
BACKWARDEM(tn;, n;,  B,  Count)  
15:  
end for 
16:  
eise 
1> leaf node 7r; 
17: 
18:  
let E-
1r;tp 
-
7r;tp+(1-7r;)(1-tp) 
Nl[i] +-- N1[i] +  E  
19:  
No[i] +-- No[i] +  1- E  
20: 
end if 
21: 
end if 
22: 
end if 
23: end procedure 
EMPHIL is then presented in Algorithm 46. After building the ACs (shar­
ing parameters) for positive and negative examples and initializing the pa­
rameters, the expectations and the counters, lines 2-5, EMPHIL proceeds 

14.7  Scaling  PILP  407 
by altemating between expectation step 8-13 and maximization step 13-24. 
The algorithm stops when the LL converges. The theory is then updated and 
retumed (lines 26-27). 
Algorithm 46 Function EMPHIL. 
1: 
function EMPHIL(Theory,  E, ö, Maxlter,"(,  a,  b,  Type)  
2: 
Examples  +-- BVILDACs(Theory)   
1>- Build the set of ACs 
3: 
for i  +-- 1 ~ ITheoryl  do 
4: 
ll[i] +-- random;  B[i],  Count[i]  +-- 0 
1>- Initialize the parameters 
5: 
end for 
6: 
LL  +-- -inf;  Iter  +-- 0 
7: 
repeat 
8: 
LL0  +-- LL,  LL  +-- 0  
1>- Expectation step 
9: 
for all node  E  Examples  do 
10: 
P  +-- FORWARD(node) 
11: 
BACKWARDEM(1, node,  No,  N1) 
12: 
LL  +-- LL  +  logP 
13: 
end for  
1>- Maximization step 
14: 
for i  +-- 1  ~ ITheoryl  do 
15:  
switch Type  
16: 
.] 
N1[i]
case 0 : II[ z +-- No[i]+Nl[i] 
17:  
4
case 1: ll[i] +--
N1 [iJ 
2(-y+No [i]+N1[i]+V(No[i]+N1[i]) 2 +1' 2 +2/'(No [i]-N1 [i])) 
18:  
case 2: 
19: 
( arccos(V3No+iNl+'Y(~-9Nl+'Y)\ 
\ 
_ 
_ 
3No+2" · -
I  
• 
-
; 
-
27r I 
0 
~) 
20: 
21: 
22: 
23: 
24: 
25: 
until LL  - LL0  <  E  v LL  - LL0  <  - LL.o  v Iter  >  M  axlter  
26: 
FinalTheory  +-- UPDATETHEORY(Theory, II) 
27: 
return FinalTheory  
28: end function 
14.7.2.2.4 EMPHIL regularization: EMPHIL1, EMPHIL2 and EMPHILB 
In  this section, we propose three regularized versions of EMPHIL. As de­
scribed in [Li et al., 2005], EM can be regularized for two reasons: first, 
for highlighting the strong relationship existing between the incomplete and 
the missing data, and second for favoring smaller parameters. We regularize 

408  Structure  Learning  
EMPHIL mainly for the latter reason. As in gradient descent regularization, 
we define the following regularization objective functions for L1 and L2 
respectively. 
J(B)  =  {N1log (}+No  log(1- B)  - ,e  
for L1 
(14.30)
N1log (}  +No  log(1 - B)  - 1B2 for L2 
where (}  = 7ri, No  and N1 are the expectations computed in the E-step (see 
equations 14.24 and 14.25). The M-step aims at computing the value of (}  
that maximizes J(B). This is done by solving the equation iJ~~e) = 0. The 
following theorems give the optimal value of (}  in each case. 
Theorem  22 (Maximum of the L1 regularized objective function). The  L1 
regularized  objective function:  
J1 (B)   =  N1log (}  +  No  log(1 - B)  - ,e  
(14.31) 
is  maximum  in  
=  
4Nl
01 
2(1 +No+  N1 +  -yi(No  +  N1) 2 +  12 +  2/(No- N1)) 
Theorem  23 (Maximum of the L2 regularized objective function). The  L2 
regularized  objective function:  
J2(B)  =  Ndog (}+No  log(1- B)  -
~(}
2 
(14.32) 
is  maximum  in  
V3No+M 1 +-r(~-
9NI+-r)) 
)
arccos (
3No+3Nl +-r 
2,.j3No+3Nl +'Y  COS 
_ 27r 
'Y  
3 
3 
( 
1 
~= 
3 
+~ 
We consider another regularization method for EMPHIL (called EMPHILB) 
which is based on a Bayesian update of the parameters assuming a prior that 
takes the form of a Dirichlet distribution with parameters [ a,  b].  In the M -step, 
instead of computing 1ri = No~
1N
, EMPHILB computes 
1 
N1 +a  
1ri =   ------------
(14.33)
No+  N1 +  a  +  b  

14.7  Scaling  PILP  409 
as described in Bisbop [2016]. a  and bare hyper-parameters. We choose a  = 
0 and b  as a fraction of the training set size, since we want small parameters. 
So algorithms EMPHIL (the standard EM), EMPHIL1, EMPHIL2 and 
EMPHILB  differ in the way they update the parameters in the M -step, Algo­
rithm 46 lines 15-22. 
14.7.2.3  Structure  learning  
In  the previous section, the structure of an HPLP was given and the task was 
to learn its parameters from data. Since hidden predicates could be difficult 
to interprete for humans in many domains of interest, providing the structure 
of an HPLP may be unintuitive and tedious even for experts of the domain. In  
this section, we propose an algorithm for learning both the structure and the 
parameters from data. The structure is learned by mean ofpredicate invention. 
The structure learning problern is defined as follows: 
Definition  73 (Structure Learning Problem). Given  a  set  of mega-examples  
( interpretations ),  each containing positive  and negative examples for a target  
predicate  and facts  for  input predicates,  I  = { q,  ...  ,  eQ,  ~eQ+1, ... , ~eR}, 
find  the  HPLP  with  parameters  II  that  maximizes  the  (log)  likelihood  
Q  
R  
LL  =  arg max L)l:  log P( ei) +  2.:  log(1 - P( ei))) 
(14.34) 
rr  
I  i=l 
i=Q+l  
where  P( ei) is  the  probability  assigned  to  ei  (an  example  from  the  interpre­
tation  I).  
SLEAHP learns HPLPs by generating an initial set of bottom clauses (from 
a language bias) from which a large HPLP is derived. Then SLEAHP per­
forms structure learning by using parameter learning. Regularization is used 
to bring as many parameters as possible close to 0 so that their clauses can 
be removed, thus pruning the initiallarge program and keeping only useful 
clauses. 
14.7.2.3.1  Description  of  the  algorithm  In order to learn an HPLP, 
SLEAHP (Algorithm 47) initially generates a set of bottom clauses, line 2. 
Then an n-ary tree, whose nodes are literals appearing in the head or in the 
body of bottom clauses is constructed, line 3. An initial HPLP is generated 
from the tree, line 4 and a regularized version ofPHIL is performed on the ini­
tial program. Finally clauses with very small probabilities are removed, line 5. 
The components of this algorithm are described in the following subsections. 

410 
Structure  Learning  
Algorithm 47 Function STRUCTURE LEARNING 
1: function SLEAHP(Nint,  NS,  NA,  MaxProb,  NumLayer,  Maxiter,  E, li,MinProb)  
2: 
Clauses  <-GENCLAUSES(NI  nt,  N  S,  NA)  
C>  Generate clauses 
3: 
Tree  <-GENTREE(Clauses) 
C>  Build the tree 
4: 
init_H P  LP  <-GENHPLP(C1auses,MaxProb,NumLayer) 
C>  Generate the initia1 HPLP 
5: 
(LL,  final_H P  LP)  <---PHIL_REG(init_HP  LP,  Maxi ter,  E, Ii)  C>  Learns the parameters 
6: 
return final_H P  LP  
7: 
end function 
14.7.2.3.2 Clause generation Algorithm 48 generates a set of bottom 
clauses as in Progol, [Muggleton, 1995]. Thesebottom clauses are then used 
for creating a tree of literals, Algorithm 49. 
Each ground bottom clause BC  = h  :- b1, ... , bm  is then processed 
to obtain a probabilistic program clause by replacing each term in a +  or 
- place-marker with a variable, using the samevariable for identical terms. 
Terms corresponding to # or -# place-markers are instead kept in the clause. 
This process is repeated for a number Nlnt  of input mega-examples and a 
number NA  of answers, thus obtaining Nlnt  ·NA  bottom clauses. 
Algorithm 48 Function GENERATECLAUSES 
1: function GENCLAUSES(NI  nt,  N  S,  NA)  
2: 
for all predicates PI Ar  do 
3: 
Clauses  <--- []  
4: 
for all modeh declarations modeh( rec,  s) with PI Ar  predicate of s  do 
5: 
fori=l--->Nintdo 
6: 
Se1ect randomly a mega-examp1e I  
7: 
for j  = 1 ---> NA  do 
8: 
Select randomly an atom h  from I  matehing schema( s) 
9: 
Bottom clause BC  <-SATURATION(h, rec,  N  S),  1et BC be H  ead  : - Body  
10: 
Clauses  <--- [Head:  0.5:- BodyiClauses]  
11: 
end for 
12: 
end for 
13: 
end for 
14: 
end for 
15: 
return Clauses  
16: end function 
14.7.2.3.3 Tree generation Since an HPLP can be mapped into a tree as 
described in Section 14.7.2.1, we create a tree whose nodes areliterals ap­
pearing in the head or in the body of bottom clauses generated in the previous 
section. Every node in the tree shares at least one variable with its parent. The 
tree is then converted into the initial HPLP, see Section 14.7.2.3.4. 

14.7  Scaling  PILP  411 
To create the tree, Algorithm 49 starts by considering each bottom clause 
in turn, line 3. Each bottom clause creates a tree, lines 4 - 11. Consider the 
following bottom clause 
BC  = r(Arg):- b1(Arg1),  ... , bm(Argm)  
where Arg  and Argi  are tuples of arguments and the bi(Argi)  for i  
1, ... , m  are literals. Initially r(Arg)  is set as the root of the tree, lines 5. 
Literals in the body are considered in turn from left to right. When a literal 
bi(Argi)  is chosen, the algorithm tries to insert the literal in the tree, see 
Algorithm 50. If bi(Argi)  cannot be inserted, it is set as the right-most child 
ofthe root. This proceeds until all the bi(Argi)  are inserted into a tree, lines 
6-10. Then the resulting tree is appended into a Iist of trees (initially empty), 
line 11, and the list is merged obtaining a unique tree, line 13. The trees in L  
are merged by unifying the arguments of the roots. 
Algorithm  49  GENERATE TREE 
1: 
function GENTREE(Bottoms)  
2: 
L~ [] 
3: 
for all Bottom in Bottoms do 
4: 
let Bottom  be r(Arg)  :- b1 (Arg1),  ...  , bm(Argm)  
5: 
Tree  ~ r(Arg)  
1> r(Arg)  is the root ofthe tree 
6: 
for all bi(Argi)  do 
7: 
if not(INSERTTREE(Tree, bi(Argi)))  then 
8: 
addChild(r(Arg),bi(Argi))  
9: 
end if 
10: 
end for 
11: 
L  ~ L  · append(Tree)  
12: 
end for 
13: 
final_Tree  ~ mergeTrees(L)  
14: 
retum final_Tree  
15: end function 
To insert the literal bi(Argi)  into the tree, Algorithm 50, nodes in the 
tree are visited depth-first. When a node b(Arg)  is visited, if Arg and Argi  
share at least one variable, bi(Argi)  is set as the right-most child of b(Arg)  
and the algorithm stops and returns True.  Otherwise I nsertTree  is recur­
sively called on each child of b(Arg),  lines 6- 12. The algorithm returns 
False  if the literal cannot be inserted after visiting all the nodes, line 3. 

412 
Structure  Learning  
Algorithm 50 INSERT A LITERAL INTO A TREE 
1:  function INSERTTREE(Tree,bi(Argi))  
2: 
if Tree=NULL then 
1>- All nodes are visited 
3: 
retum False  
4: 
eise 
5: 
let Tree  be b(Arg)  
6: 
if shareArgument(Arg,  Argi)  then 
7:  
addChild(Tree,bi(Argi)) 
8: 
retum True  
9: 
eise 
10: 
for ali Child  of Tree  do 
11: 
retum INSERTTREE(Child,  bi(Argi))  
12: 
end for 
13: 
end if 
14: 
end if 
15: end function 
Example 127 (Bottom clause for the UW-CSE dataset). Consider  the  fol­
lowing  bottom  clause  from  the  UW-CSE  dataset:  
advised_by(A,  B)  +­
student(A),professor(B),  has_position(B,  C), 
publ ication (D,  B),  publ ication (D,  E),  i  n_phase (A,  F),  
taught_by(G,  E,  H),  ta(I,  J,  H).  
In  order to build the tree, advised_by(A,  B)  is initially set as the root of 
the tree. Then predicates in the body are considered in turn. The predicates 
student(A)  (resp. professor(B),  hasposition(B,  C)  andpublication(D,  
B))  are set as the children of advised_by(A,  B)  because their arguments 
sharevariable A  (resp. B).  Then the predicate publication(D,  E)  is set as a 
child of publication(D,  B)  because their arguments share variable 
D,  in_phase(A,  F)  as a child of advised_by(A,  B)  (they sharevariable A),  
taughtby(G,  E,  H)  as a child of publication(D,  E)  (they sharevariable E)  
and finally ta(I,  J,  H)  as a child of taughtby( G,  E,  H)  (they sharevariable 
H).  The corresponding tree is shown in Figure 14.9. 

14.7  Scaling  PILP  413 
advised_by(A,B)  
~~~  
student(A)  
professor(B)  
has-posistion(B,C)  publication(D,B)  
inphase(A,F)  
I  
publication(D,E)  
I  
taughtby(G,E,H)  
I  
ta(I,J,H)  
Figure  14.9  Tree created from the bottom clause ofExample 127. 
14.7.2.3.4  HPLP  generation  Once the tree is built, an initial HPLP is gen­
erated at random from the tree. Before describing how the program is created, 
note that for computation purposes, we consider clauses with at most two 
literals in the body. This can be extended to any number of literals. Algorithm 
51 takes as input the tree, Tree,  an initial probability, 0 ~ M  axProb  ~ 1, 
a rate, 0 ~ rate  ~ 1, and the number of layers 1 ~ NumLayer  ~ 
height(Tree)  of the HPLP we are about to generate. Let X,  Y,  Z  and W  
be tuples of variables. 
In order to generate the initial HPLP, the tree is visited breadth-first, start­
ing from levell. Foreach node ni  at level Level  (1 ~ Level  ~ N  umLayer),  
ni  is visited with probability Prob.  Otherwise ni  and the subtree rooted at ni  
are not visited. Prob  is initialized to M  axProb,  1.0 by default. The new 
value of Prob  at each level is Prob  x rate  where rate  E  [0, 1] is a constant 
value, 0.95 by default. Thus the deeper the level, the lower the probability 
value. Supposing that ni  is visited, two cases can occur: ni  is a leaf or an 
intemal node. 
If ni  = bi(Y)  is a leafnode with parent Parenti,  we consider two cases. 
If Parenti  = r(X)  (the root ofthe tree), then the clause 
C  = r(X)  : 0.5:- bi(Y).  
is generated, lines 9-11. Otherwise let path  is the path from the root to 
Parenti,  Parenti  be bpath(X)  and ni  be bpath_i(Y).  Then clause 
C  =  hiddenpath(Z)  : 0.5:- bpath_i(Y).  
is generated, lines 13-15. 

414 
Structure  Learning  
If ni  = bi(Y)  is an intemal node having parent Parenti,  we consider 
two cases. If Parenti  is the root, the clause 
C  =  r(X)  : 0.5:- bi(Y),  hidden_i(Z).  
is generated, where Z  =  X  u  Y,  lines 19-20. If Parenti  is an intemal node, 
let path  is the path from the root to Parenti  and ni  be bpath_i (Y).  Then the 
clause 
C  = hiddenpath(Z)  : 0.5:- bpath_i(Y),  hiddenpath_i(W).  
is generated where W  =Zu Y,  lines 23-24. 
The generated clause C  is added to a list (initially empty), line 27, and 
the algorithm proceeds for every node at each level untillayer N  umLayer  
is reached or all nodes in the tree are visited, line 5. Then hidden predicates 
appearing in the body of clauses without associated clauses (in the next layer) 
are removed, line 33, and the program is reduced, line 34. To reduce the 
program, a set of clauses (without hidden predicates in the body) that are 
renaming of each other are reduced to a single representative of the set. 
Note that atoms in the head of the generated clauses are all annotated with 
probability 0.5 for exposition purposes. These values are replaced by random 
values between 0 and 1 at the beginning the parameter leaming process, see 
Algorithm 47line 5. 
Example  128  (HPLP generated by SLEAHP for the UW-CSE domain). The  
HPLP  generatedfrom  the  tree  of Figure  14.9  is:  
advised_by(A,  B)  : 0.5 +--- student(A).  
advised_by(A,  B)  : 0.5 +--- professor(B).  
advised_by(A,  B)  : 0.5 +--- has_position(B,  C). 
advised_by(A,  B): 0.5 +--- publication(D,  B),  hidden1(A,  B,  D).  
advised_by(A,  B)  : 0.5 +--- in_phase(A,  E).  
hidden1 (A,  B,  D)  : 0.5 +--- publication(D,  F),  hidden1_1  (A,  B,  D,  F).  
hidden1_1(A,B,D,F):  0.5 +--- taught_by(G,F,H),  
hidden1_1_1  (A,  B,  D,  F,  G,  H).  
hidden1_1_1  (A,  B,  D,  F,  G,  H)  : 0.5 +--- ta(I,  J,  H).  
SLEAHP was compared with PLP systems such as SLIPCOVER and PROB­
FülL+ and with with Statistical Relational Leaming methods such as MLN­
BC and MLN-BT [Khot et al., 2011] for leaming Markov Logic Networks 
(MLNs) and with RDN-B [Natarajan et al., 2012] for learning Relational 
Dependency Networks. The comparison was performed on five datasets. 

14.7  Scaling  PILP  415 
Algorithm  51  FUNCTION GENERATEHPLP 
1: 
2: 
function GENERATEHPLP(Tree, MaxProb,  Rate,  NumLayer)  
HPLP  +-- D 
3: 
Level+-- 1 
4: 
Prob  +-- M  axProb  
5: 
while Level  <  N  umLayer  and  all nodes in Tree  are not visited do 
6: 
for all node n;  at level Level  having parent Parent;  do 
7: 
if maybe(Prob) then 
8: 
if n;  is a leaf node then 
1>- n;  is a leaf node 
9: 
if Parent;  is the root node then 
10: 
let n;  be b;(Y)  and Parent;  be r(X)  
11: 
C  +-- r(X)  : 0.5:- b;(Y).  
12: 
eise 
13: 
let Parent;  be bpath(X)  
14: 
let n;  be bpath_;(Y)  
15: 
C  +-- hiddenpath(Z):  0.5:- bpath_i(Y).  
16: 
end if 
17: 
eise 
1>- n;  is an intemal node 
18: 
if Parent;  is the root node then 
19: 
let n;  be b;(Y)  and Z  be X  u Y  
20: 
C  +-- r(X)  : 0.5:- b;(Y),  hidden_i(Z).  
21: 
eise 
22: 
let Parent;  be bpath(X)  
23: 
n;  be bpath_i  (Y)  and W  be Z  u Y  
24: 
C  +-- hiddenpath(Z):  0.5:- bpath_i(Y),hiddenpath_i(W).  
25: 
end if 
26: 
end if 
27: 
HPLP  +-- [CIHPLP]  
28: 
end if 
29: 
end for 
30: 
Prob  +-- Prob  *  Rate  
31: 
level  +-- level  +  1 
32: 
end while 
33: 
HPLP  +-- removeHidden(HPLP)  
34: 
initial_HPLP  +-- reduce(HPLP)  
35: 
Return initial_HPLP 
36: end function 
SLEAHP beats the other systems in terms of computation time in almost 
all datasets and achieves a similar quality of the solution. Thus SLEAHP 
achieves a good compromise between accuracy and learning time. 

416 
Structure  Learning  
14.8  Examples  of  Datasets  
PILP systems have been applied to many datasets. Some of them are: 
• UW-CSE [Kok and Domingos, 2005a]: see Section 14.5.3. 
• Mutagenesis [Srinivasan et al., 1996]: a dassie ILP benchmark dataset 
for Quantitative structure-activity relationship (QSAR), i.e., predicting 
the biological activity of chemieals from their physicochemical proper­
ties or molecular structure. In this case, the goal is to predict the mu­
tagenicity (a property correlated with cancerogenicity) of compounds 
from their chemical structure. 
• Carcinogenesis [Srinivasan et al., 1997]: another dassie ILP benchmark 
dataset for QSAR where the goal is to predict the cancerogenicity of 
compounds from their chemical structure. 
• Mondial [Schulte and Khosravi, 2012]: a dataset containing information 
regarding geographical regions of the world, induding population size, 
political system, and the country border relationship. 
• Hepatitis [Khosravi et al., 2012]: a dataset derived from the Discovery 
Challenge Workshop of ECML/PKDD 2002 containing information on 
laboratory examinations of hepatitis B and C infected patients. The goal 
is to predict the type of hepatitis of a patient. 
• Bupa [McDermott and Forsyth, 2016]: diagnosing patients with liver 
disorders. 
• NBA [Schulte and Routley, 2014]: predicting the results  of basketball 
matches from NBA. 
• Pyrimidine, Triazine [Layne and Qiu, 2005]: QSAR datasets for pre­
dicting the inhibition of dihydrofolate reductase by pyrimidines and tri­
azines, respectively. 
• Financial [Berka, 2000]: predicting the success of loan applications by 
dients of a bank. 
• Sisyphus [Blockeel and Struyf, 2001]: a dataset regarding dients of an 
insurance business, the aim is to classify households and persons in 
relation to private life insurance. 
• Yeast [Davis et al., 2005]: predicting whether a yeast gene codes for a 
protein involved in metabolism. 
• Event Calculus [Schwitter, 2018]: learning effect axioms for the Event 
Calculus [Kowalski and Sergot, 1986]. 

15   
cplint  Examples  
This chapter shows some examples of programs and how the cp1 int  system 
can be used to reason on them. 
15.1  cplint  Commands  
cplint uses two Prologmodules for performing inference, pita for exact 
inference with PITA (see Section 8.6) and mcintyre for approximate in­
ference with MCINTYRE (see Section 10.2).  We present here the predicates 
provided by these two modules. 
The unconditional probability of an atom can be asked using pita with 
the predicate 
prob (+Query: atom, - Probability: float ). 
where +  and - mean that the argument is input or output, respectively, and 
the annotation of arguments after the colon indicates their type. 
The conditional probability of a query atom given an evidence atom can 
be asked with the predicate 
prob (+Query: atom, +Evidence: atom, - Probability: float ). 
The BDD representing the explanations for the query atom can be obtained 
with the predicate 
bdd_ dot_string (+Query :atom, - BDD : string , - Var: list ). 
that returns astring encoding the BDD in the dot format of Graphviz [Kout­
sofios et al., 1991]. See Section 15.3 for an example of use. 
With mcintyre, the unconditional probability of a goal can be com­
puted by taking a given number of samples using the predicate 
mc_sample (+Query : atom, +Samples: int , - Probability: float ). 
417 

418 
cplint  Examples  
Moreover, the following predicate samples arguments of queries: 
mc_sample_arg (+Query : atom, +Samples : int , ?Arg :var , - Values : list ). 
where ? means that the argument must be a variable. The predicate samples 
Query Samples times. Arg must be a variable in Query. The predicate 
returns a Iist of pairs L - N  in Values where L  isthelist of all values of Arg 
for which Que ry succeeds in a world sampled at random and N  is the number 
of samples returning that Iist of values. If L  is the empty Iist, it means that for 
that sample, the query failed. If L  is a Iist with a single element, it  means 
that forthat sample, the query is determinate. If,  in all pairs L - N,  L  is a Iist 
with a single element, it means that the program satisfies the exclusive-or 
assumption. 
The version 
mc_sample_arg_first (+Query : atom, +Samples : int , ?Arg :var , 
- Values : list ). 
also samples arguments ofqueries but just returns the firstanswer of the query 
for each sampled world. 
Conditional queries can be asked with rejection sampling or Metropolis­
hastings MCMC. In the first case, the predicate is: 
mc_rejection_sample (+Query : atom, +Evidence :atom, 
+Samples : int , - Successes : int , - Failures : int , 
- Probability : float ). 
In the latter case, the predicate is 
mc_mh_sample (+Query : atom, +Evidence : atom, Samples : int , 
+Lag : int , - Successes : int , - Failures : int , - Probability : float ) 
Moreover, the arguments of the queries can be sampled with rejection sam­
pling and Metropolis-hastings MCMC using 
mc_rejection_sample_arg (+Query : atom, +Evidence : atom, 
+Samples : int , ?Arg :var , - Values : list ). 
mc_mh_sample_arg (+Query : atom, +Evidence : atom, 
+Samples : int , +Lag : int , ?Arg :var , - Values : list ) 
Expectations can be computed with 
mc_expectation (+Query : atom, +Samples : int , ?Arg : var , - Exp : float ) . 
that returns the expected value of the argument Arg in Query computed by 
sampling. 

15.1  cplint  Commands  419 
The predicate 
mc_mh_expectation (+Query : atom, +Evidence: atom, +Samples: int , 
+Lag: int , ?Arg: var , - Exp: float ). 
computes conditional expectations using Metropolis-hastings MCMC. 
The cpl int on SWISH web application (https://cplint.eu/) [Riguzzi et al., 
2016a; Alberti et al., 2017] allows the user to write and run probabilistic 
programs online. It  is based on the SWISH [Wielemaker et al., 2015, 2019] 
web front-end for SWI-prolog. cplint on SWISH also adds graphics ca­
pabi lities to cp1 int: the results of sampling arguments can be rendered as 
bar charts. All the predicates shown above have a form with an extra last 
argument +Options that accepts a list of terms specifying options. If  the 
option b ar ( - Chart : dict ) is used, the predicate returns in Chart an 
SWI-prolog dictionary tobe rendered with C3.js1  as a bar chart. For example, 
t00rf)ltmps:l/cplint.eu/e/markov _chain.pl returns a chart with a bar for each 
possible sampled value whose size is the number of samples returning that 
value. 
When the program has continuous random variables, the user can build a 
probability density of the sampled argument. When the evidence is on ground 
atoms with continuous values as arguments, the user needs to use likelihood 
weighting or particle filtering (see Section 10.4). 
The predicate 
mc_lw_sample_arg (+Query: atom, +Evidence: atom, +Samples: int , 
?Arg: var , - ValList : list ). 
returns in ValList a list of pairs V- W where V isa value of Arg for which 
Query succeeds and W is the weight computed by likelihood weighting ac­
cording to Evidence. 
In particle filtering, the evidence is a Iist of atoms. The predicate 
mc_particle_sample_arg (+Query : atom, +Evidence+term, 
+Samples : int , ?Arg: var , - Values: list ). 
samples the argument Arg of Query using particle filtering given Evidence. 
Evidence is a list of goals and Query can be either a single goaloralist 
of goals. 
1  http://c3js.org/ 

420 
cplint  Examples  
The samples obtained can be used to draw the probability density function 
of the argument. The predicate 
histogram (+List: list , - Chart: dict , +Options: list ) 
takes a Iist of weighted samples and draws a histogram of the samples using 
C3.js in cpl int on SWISH. 
The predicate 
density (+List: list , - Chart: dict , +Options: list ) 
draws a line chart of the density of the weighted samples in List . 
In histograml 3 and densi ty I  3, the options can be used to specify 
the bounds and the number of bins on the X  -axis. 
The predicate 
densities (+PriorList: list , +PostList : list , - Chart: dict , +Options : 
list ) 
draws a line chart of the density of two sets of samples, usually prior to and 
postobservations. The same options as for histograml 3 and dens i ty I  3 
are recognized. 
For example, the query 
?-
mc_sample_arg (val ( 0, X) , 1000 , X, LO , [] ) , histogram (LO , Chart , [] ) . 
from https://cplint.eu/e/gauss_mean_est.pl takes 1000 samples of argument X 
of v  a 1 ( 0 , X )  and draws the density of the samples using an histogram. 
For discrete arguments, the predicate 
argbar (+Values : list , - Chart: dict ) 
returns a bar chart with a bar for each value, where Va l ues is a Iist of pairs 
V - N  with V  the value and N  the number of samples returning that value. 
The predicates dens i ty_r I  1, dens i ties_r12 ,  histogram_r 12 ,  
and argbar_r I  1 are the Counterparts ofthose above for drawing graphs in 
cpl int on SWISH using the R language for statistical computing2 . 
EMBLEM (see Section 13.4) can be run with the predicate 
induce_par (+ListOfFolds: list , - Program: list ) 
that induces the parameters of a program starting from the examples con­
tained in the indicatedfolds (groups of examples). The predicate 
induce (+ListOfFolds : list , - Program: list ) 
instead induces a program using SLIPCOVER (see Section 14.5). 
2 https://www.r-project.org/ 

15.2  Natural  Language  Processing  421  
The induced programs can be tested on a set of folds with 
test (+Program: list , +ListOfFolds: list , - LL: float , 
- AUCROC: float , - ROC: list , - AUCPR: float , - PR: list ) 
that returns the log likelihood of the test examples (LL), the ROC and preci­
sion recall curves (ROC and PR) for rendering with C3.js, and the areas under 
the curves (AUCROC and AUCPR) that are standard metrics for the evaluation 
of machine learning algorithms [Davis and Goadrich, 2006]. 
Predicate test_ r I  5 is similar to test /7 but plots the graphs using R. 
15.2  Natural  Language  Processing  
In Naturallanguage processing (NLP), a common task is checking whether 
a sentence respects a grammar. Another common task is tagging each word 
of a sentence with a Part-of-speech (POS) tag. For NLP, the grammars that 
are used in the theory of formallanguages such as context-free grammars or 
left corner grammars don't work weil because the rules are too strict. Natural 
language is more flexible and is characterized by many exceptions to rules. 
To model naturallanguage, probabilistic versions of the grammars above have 
been developed, such as Probabilistic context-free grammar or Probabilistic 
left corner grammar (PLCG). Similarly, for POS tagging, statistical tools such 
as HMMs give good results. These models can all be encoded with PLP 
[Riguzzi et al., 20 17b]. 
15.2.1  Probabilistic  context-free  grammars  
A PCFG consists of: 
1.  A context-free grammar G  =  (N,  L: ,  I ,  R)  where N  is a finite set of 
non-terminal symbols, L;  is a finite set of terminal symbols, I  E  N  is 
a distinguished start symbol, and R  is a finite set of mies of the form 
X~ Y1, ... ,Yn,  where XE N  and Yi  E (Nu L:).  
2. A parameter B for each rule o:  ~ ß  E  R.  Therefore, we have probabilis­
tic rules of the form e : 0:  ~ ß  
This kind of model can be represented by PLP. For instance, consider the 
PCFG 
0.2: S  ~ aS  
0.2: S  ~ bS  
0.3: S  ~ a  
0.3: s  ~ b,  
where N  is {S}  and L;  is {a ,  b}. 

422 
cplint  Examples  
The program https://cplint.eule/pcfg.pl (adapted from [Sato and Kubota, 
2015]) computes the probability of strings using top-down parsing: 
pcfg (L ) :- pcfg ( [ ' S ' ], [ ], _ Der , L , [] ). 
pcfg ( [A IR ], Der O, Der , LO, L2 ) :­
rule (A, Der O, RHS ), 
pcfg (RHS , [rule (A, RHS ) IDerO ] , Der l , LO , Ll ), 
pcfg (R, Derl , Der , Ll , L2 ) . 
pcfg ( [A IR ], Der O, Der , [A I Ll ] , L2 ) :­
\+  rule (A, _ , _ ), 
pcfg (R, Der O, Der , Ll , L2 ). 
pcfg ( [] , Der , Der , L, L ) .  
rule ( ' S ' , Der , [a , ' S ' ] ) : 0 . 2 ; rule ( ' S ' , Der , [b , ' S ' ] ) : 0 .2 ;  
r u le ( ' S ' , Der , [a ] ) : 0 . 3 ; rule ( ' S ' , Der , [b ] ) : 0 . 3 .  
In this example, if we want to ask the probability of the string abaa using 
exact inference, we can use the query ?- prob (pcfg ( [a, b, a, a]), 
Prob) . We obtain the value 0.0024. In this case, the grammar is not am­
biguous so there exists only one derivation with probability 0.2·0.2·0.2 ·0.3 =  
0.0024. 
15.2.2  Probabilistic  left  corner  grammars  
A PLCG is a probabilistic version of a left-corner  grammar  which uses the 
same set of rules as a PCFG. Whereas PCFGs assume top-down parsing, 
PLCGs are based on bottom-up parsing. PLCGs set probabilities to three 
elementary operations in bottom-up parsing, i.e., shift, attach and project, 
rather than to expansion of non-terminals. As a result, they define a class of 
distributionsdifferent from that of PCFGs. 
Programs for PLCGs Iook very different from those for PCFGs. Consider 
the PLCG with the rules 
s~ss 
s~a 
s~b 
The program https://cplint.eu/e/plcg.pl (adapted from Sato et al. [2008]) be­
low encodes such a grammar: 
plc (Ws ) 
: -
g_call ( [ 'S ' ] , Ws , [ ], [] , _ Der ).  
g_call ( [] , L , L , Der , Der ).  
g_call ( [G IR ], 
[G IL ], L2 , Der0 , Der ) 
: -% shift   
terminal (G), 
g_call (R, L , L2 , Der0 , Der ) . 

15.2  Natural  Language  Processing  423 
g_call ( [GIR] , 
[Wd i L] , L2 , DerO , Der ) : 
\+ terminal (G), first (G, DerO , Wd), 
lc_call (G, Wd , L, Ll , [first (G, Wd ) I DerO ] , Derl ), 
g_call (R, Ll , L2 , Derl , Der ). 
lc_call (G, B, L, Ll , DerO , Der ) : -
% attach  
lc (G, B, DerO , rule (G, 
[B IRHS2 ] )), 
attach_or_project (G, DerO , attach ), 
g_call (RHS2 , L, Ll , [ lc (G, B, rule (G, 
[B I RHS2 ] )), 
attach i DerO ], Der ). 
lc_call (G, B, L, L2 , Der0 , Der ) : -
% project  
lc (G, B, DerO , rule (A, 
[B IRHS2 ] )), 
attach_or_project (G, DerO , project ), 
g_call (RHS2 , L, Ll , [ lc (G, B, rule (A, 
[B I RHS2 ] )), 
project iDerO ] , Derl ), 
lc_call (G, A, Ll , L2 , Derl , Der ). 
lc_call (G, B, L, L2 , Der0 , Der ) 
\+  lc (G, B, DerO , rule (G, [B I_ ] )), 
lc (G, B, DerO , rule (A, 
[B IRHS2 ] )), 
g_call (RHS2 , L, Ll , [lc (G, B, rule (A, 
[B IRHS2 ] )) IDerO ] , 
Derl ), 
lc_call (G, A, Ll , L2 , Derl , Der ). 
attach_or_project (A, Der , Op ) : 
lc (A, A, Der , _ ), attach (A, Der , Op ) 
attach_or_project (A, Der , attach ) 
\+ lc (A, A, Der , _ ). 
lc ( ' S ' , ' S ' , _ Der , rule ( ' S ' , [ ' S ' , ' S ' ] ) )  
lc ( ' S ' , a , _ Der , rule ( ' S ' , [a ] )).  
lc ( ' S ' , b , _ Der , rule ( ' S ' , [b ] )) .  
first ( ' S ' , Der , a ) :0.5 ; first ( ' S ' , Der , b ) : 0.5 .  
attach ( ' S ' , Der , attach ) : 0 . 5 ; attach ( ' S ' , Der , project ) : 0 . 5 .  
terminal (a ). terminal (b ).  
If we want to know the probability that the string ab is generated by the gram­
mar, we can use the query ? -
mc_ prob  (plc  (  [a , b ]), P).  and obtain 
~ 0.031.  
15.2.3  Hidden  Markov  models  
HMMs (see Example 74) can be used for POS tagging: words can be con­
sidered as output symbols and a sentence as the sequence of output symbols 
emitted by an HMM. In this case, the states are POS tags and the sequence 
of states that most probably originated the sequence of output symbols is the 
POS tagging of the sentence. So we can perform POS tagging by solving an 
MPE task. 

424 
cplint  Examples  
Program https://cplint.eu/e/hmmpos.pl (adapted from [Lager, 2018; Nivre, 
2000; Sato and Kameya, 2001]) encodes a simple HMM  where the output 
probabilities are set to 1  (for every state, there is only one possible output). 
The assumption is that a POS of a word depends only on the POS of the 
preceding word ( or on the start state in case there is no preceding word). The 
program is: 
hmm (O) : - hmm (_ , O). 
hmm (S , 0 ) : ­
trans (start , QO , [] ), hmm (QO , [] , SO , O), reverse (SO , S ). 
hmm ( Q, S 0 , S , [ L I 0 ]  )  : ­
trans (Q, Q1 , SO ), 
out (L, Q, SO ), 
hmm ( Q 1 , [ Q I  S 0 ] , S , 0 )  
hmm (_ , S , S , [ l ).  
trans (start , det , _ ) : 0 . 30 ; trans (start , aux , _ ) : 0 . 20 ; 
trans (start , v , _ ) : 0 . 10 ; trans (start , n , _ ) : 0 . 10 ; 
trans (start , pron , _ ) : 0 . 30 . 
trans (det , det , _ ) : 0 . 20 ; trans (det , aux , _ ) : 0 . 01 ; 
trans (det , v , _ ) : 0 . 01 ; trans (det , n , _ ) : 0.77 ; 
trans (det , pron , _ ) : 0 . 0 1. 
trans (aux , det , _ ) : 0 . 18 ; trans (aux , aux , _ ) : 0 . 10 ; 
trans (aux , v , _ ) : 0 . 50 ; trans (aux , n , _ ) : 0 . 01 ; 
trans (aux , pron , _ ) : 0 . 21 . 
trans (v , det , _ ) : 0 . 36 ; trans (v , aux , _ ) : 0 . 01 ; 
trans (v , v , _ ) : 0 . 01 ; trans (v , n , _ ) : 0.26 ; trans (v , pron , _ ) 
:0 . 36 . 
trans (n , det , _ ) : 0 . 01 ; trans (n , aux , _ ) : 0 . 25 ; trans (n , v , _ ) 
: 0.39 ; 
trans (n , n , _ ) : 0 . 34 ; trans (n , pron , _ ) : 0 . 0 1. 
trans (pron , det , _ ) : 0 . 01 ; trans (pron , aux , _ ) : 0 . 45 ; 
trans (pron , v , _ ) : 0 . 52 ; trans (pron , n , _ ) : 0 . 01 ; 
trans (pron , pron , _ ) : 0 . 01 . 
out (a , det , _ ). 
out (can , aux , _ ). 
out (can , v , _ ). 
out (can , n , _ ). 
out (he , pron , _ ). 
For instance, we may want to know the most probable POS sequence for the 
sentence "he can can a can." By using the query 
?- mc_samp1 e_arg ( hmm (S , [he , can , can , a , can ] ), 100 , S , O). 
we obtain that the sequence [pro n , aux , v , det , n ] appears most 
frequently in 0 . 

15.4  Gaussian  Processes  425 
15.3  Drawing  Binary  Decision  Diagrams  
Example 94 models the development of an epidemic or a pandernie and is 
https://cplint.eu/e/epidemic.pl: 
epidemic : 0.6 ; pandemic : 0.3 
flu (_ ), cold .  
cold : 0.7 .  
flu (david).  
flu (robert ).  
In order to compute the probability that a pandernie arises, we can call the 
query: 
?- prob (pandemic , Prob). 
The corresponding BDD can be obtained with: 
?- bdd_dot_string (pandemic , BDD , Var ). 
The call returns the BDD in the form of a graph in the dot format of Graphviz 
that the cplint on SWISH system renders and visualizes as shown in Fig­
ure 15.1. Moreover, the call returns a data structure in Va r  encoding Ta­
ble 15.1 that associates multivalued variable indexes with ground instantia­
tions of rules. 
The BDD built by CUDD are discussed in Section 43: they differ from 
those introduced in Section 8.3 because edges to 0-children can be negated, 
i.e., the function encoded by the 0-child is negated before being used in the 
parent node. Negated edges to 0-children are represented in the graph by 
dotted arcs, while edges to 1-children and regular edges to 0-children with 
solid and dashed arcs, respectively. Moreover, the output of the BDD can 
be negated as weil, indicated by a dotted arc connecting an Out node to the 
root of the diagram, as in Figure 15.1. CUDD uses this form of BDDs for 
computational reasons, for example, negation is very cheap, as it just requires 
changing the type of an edge. 
Each Ievel of the BDD is associated with a variable of the form Xi_k  
indicated on the left: i  indicates the multivalued variable index and k  the index 
of the Boolean variable. The association between the multivalued variables 
and the clause groundings is encoded in the Var  argument. For example, 
multivalued variable with index 1 is associated with the rule with index 0 
(the first rule of the program) with grounding _ I  david and is encoded with 
two Boolean variables, X1_0 and X1_1, since it can take three values. The 
hexadecimal numbers in the nodes are part of their memory address and are 
used to uniquely identify nodes. 

426 
cplint  Examples  
B   
xo_o 
Xl_O 
Xl_l 
X2_0  
X2_1  
Figure  15.1  BDD for query pandernie  in the epidemic .pl example, 
drawn using the CUDD function for exporting the BDD to the dot format 
of Graphviz. 
Table  15.1  Associations between variable indexes and ground rules 
Multivalued Variable Index 
Rule Index 
Grounding Substitution 
0 
1  
[]  
0 
[david] 
2 
0 
[robert] 
15.4  Gaussian  Processes  
A Gaussian process (GP) defines a probability distribution over functions 
[Bishop, 2016, Section 6.4]. This distribution has the property that, given N  
values, their image through a function sampled from the Gaussian process fol­
lows a multivariate normal with N  dimensions, mean 0, and covariance ma­
trix K.  In other words, if function f(x)  is sampled from a Gaussian process, 
then, for any finite selection ofpoints X= {x1, ... , xN}, the density is 
p(f(x1), ... ,j(xN)) =N(O,K),  

15.4  Gaussian  Processes  427 
i.e., it is a Gaussian with mean 0 and covariance matrix K. A GP is defined 
by a kernel function k  that determines K  as K ij  =  k(xi ,  xJ). 
GPs can be used for regression: the random functions predict the y  value 
corresponding to a x  value using the model 
Y  =  f(x)  +  E  
where E  is a random noise variable with variance 8 2 .  
Given sets (columns vectors) X=(x1, .. .  ,  xN)T  and Y = (y1,  ...  ,  yNf  
of observed values, the task is to predict the y  value for a new point x .  It can 
be proved [Bishop, 2016, Equations (6.66) and (6.67)] that y  is Gaussian 
distributed with mean and variance 
krc- ly  
(15.1)
fJ  
0"2 
k(x,x)- krc- 1k  
(15.2) 
where k  is the column vector with elements k  (xi ,  x)  and C has elements 
Cij  =  k(xi ,  xJ)  +  8 26ij·  with 8 2  user defined (the variance that is assumed 
for the random noise in the linear regression model) and (jij  the Kronecker 
function ({jij  = 1 if i  = j  and 0 otherwise). So C = K  +  8 21 and C = Kif 
8 2  =  0. 
A popular choice of kerne! is the squared exponential 
-(x - x') 2 ]
k(x,  x')  =  CT2 exp  
2l2
[ 
with parameters CT  and l.  The user can define a prior distribution over the 
parameters instead of choosing particular values. In this case, the kernel itself 
is a random function and the predictions of regression will be random as weil. 
The program below (https://cplint.eu/e/gpr.pl) can sample kernels (and 
thus functions) and compute the expected value of the predictions for a squared 
exponential kernet (defined by predicate sq_ exp_ p /  3) with parameter l  
uniformly distributed in 1, 2, 3 and CT  uniformly distributed in [-2, 2]. 
Goal gp  (X ,  Kerne 1,  Y ) ,  given a list of values X  and a kernet name, 
returns in Y the Iist of values f  ( x)  where x  belongs to X and f  is a function 
sampled from the Gaussian process. 
Goal compute_ cov ( X ,  Kernel , Var ,  C)  returns in C  the matrix C 
defined above with Var= 82 . It is called by gp /  3 with Var=O  in order to 
return K  
gp (X, Kernel , Y) : 
compute_cov (X, Kernel , O, C), 

428  cplint  Examples  
gp (C1 Y). 
gp (Cov1Y) : gaussian (Y1Mean 1Cov ) :­
1ength (Cov1N) 1  
1ist0 (N1Mean ). 
compute_cov (X1Kerne1 1Var 1C) 
1ength (X1N) 1 
cov (X1N1Kerne1 1Var 1CT 1CND ) 1 
transpose (CND 1CNDT ) 1 
matrix_sum (CT 1CNDT 1C). 
CQV  (  []  1 -
1 -
1 -
1  [  l  1  
[ l  )  •  
cov ( [XH IXT ] 1N1Ker 1Var 1 [KH IKY ] 1 [KHND IKYND ] ) 
1ength (XT 1  LX ) I  
Nl is N- LX-1 1 
1 ist 0 ( N 1 1 KH 0 ) I  
cov_row (XT 1XH 1Ker 1KH1 ) 1 
ca11 (Ker 1XH 1XH 1KXH0 ) 1 
KXH is KXHO +Var 1 
append ( [KH0 1 [KXH ] 1KH1 ] 1KH ) 1  
append ( [KH0 1 [ 0 ] 1KH1 ] 1KHND ) 1  
cov (XT 1N1Ker 1Var 1KY 1KYND ). 
cov_row ( [ ] 1 -
1  l  ) .  
1  -
[  
cov_row ( [H IT ] 1XH 1Ker 1 [KH IKT ] ) 
ca11 (Ker 1H1XH 1KH ) 1 
cov_row (T1XH 1Ker 1KT ). 
sq_exp_p (X1XP 1K) 
sigma (Sigma ) I  
1  ( L )  I  
K is Sigma A2*exp (- ( (X- XP ) A2 ) /2 / (LA2 )). 
1 (L) : uniform (L1 [ 1 12 1 3 ] ).  
sigma (Sigma ) : uniform (Sigma 1-2 12 ). 
Here  1 i s t 0 ( N, L ) is  true  if  L  is  a  Jjst  with  N  elements  all  0.  This  program  
exploits  the  possibility  offered  by  cplint of  defining  multivariate  Gaussian  
distributions.  
gp_predi c t (XP , Kernel, Var , XT, YT , YP ), given  the  points  de­
scribed  by  the  lists  XT  and  YT, a  kerne],  and  a  list  of  points  XP, predicts  y  
values  of  points  with  x  values  in  XP  and  returns  them  in  YP. The  predictions  
are  the  mean  of  y  given  by  Equation  (15.1),  with  Var being  the  s 2  parameter:  

15.4  Gaussian  Processes  429 
gp_predict (XP , Kernel , Var , XT , YT , YP ) 
compute_cov (XT , Kernel , Var , C), 
matrix_inversion (C, C_l ), 
transpose ( [YT ] , YST ), 
matrix_multiply (C_l , YST , C_lT ), 
gp_predict_single (XP , Kernel , XT , C_lT , YP ) 
gp_predict_single ( [] , _ , _ , _ , [] ). 
gp_predict_single ( [XH IXT ] , Kernel , X, C_lT , [YH IYT ] ) 
compute_k (X, XH , Kernel , K), 
matrix_multiply ( [K] , C_lT , [[ YH ]] ), 
gp_predict_single (XT , Kernel , X, C_lT , YT ). 
compute_k ( [] , _ , _ , [] ). 
compute_k ( [XH IXT ] , X, Ker , [HK ITK ] ) 
call (Ker , XH , X, HK ), 
compute_k (XT , X, Ker , TK ). 
Since the kernel here is random, the predictions of gp_ predict I  6 will be 
random as weil. 
By calling the query 
?- numlist (O, lO , X), 
mc_sample_arg_first (gp (X, sq_exp_p , Y), 5 , Y, L) 
we get five functions sampled from the Gaussian process with a squared ex­
ponential kernel at points X  = [0, ... , 10]. An example of output is shown in 
Figure 15.2. 
The query 
?- numlist (O, lO , X), 
XT= [ 2 . 5 , 6. 5 , 8. 5 ] , 
YT= [ 1 , -0. 8 , 0. 6 ] , 
mc_lw_sample_arg (gp_predict (X, sq_exp_p , 
0. 3 , XT , YT , Y), gp (XT , Kernel , YT ), 5 , Y, L). 
draws five functions with a squared exponential kernel predicting points with 
X  values in [0, ... , 10] given the three pairs of points XT  =  [2.5, 6.5, 8.5], 
YT  =  [1, -0.8, 0.6]. The graph of Figure 15.3 shows three of the functions 
together with the given points. 

430 
cplint  Examples  
1.5 
0.5 
-o.5  
-1  
-1. 5  
·2 
-2.5 -+r--.-----,---.--.-----.--.----r---.----,,---" 
10 
•  11  •  12  •  13  •  14  •  15  
Figure  15.2  Functions sampled from a Gaussian process with a squared 
exponential kerne! in gpr. pl. 
0.8 
0.6 
0.4 
0.2 
·0.2 
-Q.4  
-Q.6  
-o.s  
10 
• y  • 
11  •  12  •  13  
Figure  15.3  Functions from a Gaussian process predicting points with X  =  
[0, ... , 10] with a squared exponential kerne] in gpr. pl. 
15.5  Dirichlet  Processes  
A Dirichlet process (DP) [Teh, 2011]  is a probability distribution whose range 
is itself a set of probability distributions. The DP is specified by a base distri­
bution, which represents the expected value of the process. When sampling 
from a distribution in turn sampled from a DP, new samples have a non-zero 
probability of being equal to already sampled values. The process depends on 
a parameter a,  called concentration  parameter:  with a  ~ 0, a single value 
is sampled; with a  ~ ro, the distribution is equal to the base distribution. A 
DP with base distribution H  and concentration parameter a  is indicated with 

15.5  Dirichlet  Processes  431 
DP(H, a ).  A sample from DP(H, a )  is a distribution P .  We are interested in 
sampling values from P .  With abuse of terminology, we say that these values 
are sampled from the DP. There are several equivalent views of the DP, we 
present two of them in the following. 
15.5.1  The  stick-breaking  process  
Example https://cplint.eu/e/dirichlet_process.pl encodes a view of DPs called 
stick-breaking  process.  
In this view, the procedure for sampling values from DP(H,  a)  can be 
described as follows. To sample the first value, a sample ß1  is taken from the 
beta distribution B eta(1 ,  a)  and a coin with heads probability equal to ß1  is 
ftipped. If the coin Iands on heads, a sample x 1 from the base distribution is 
taken and returned. Otherwise, a sample ß2 is taken again from B eta(1 ,  a)  
and a coin is ftipped. This procedure is repeated until heads are obtained, the 
index i of ßi  being the index of the value Xi  to be returned. The following 
values are sampled in a similar way, with the difference that, if for an index 
i, values Xi  and ßi  were already sampled, that value is returned. 
This view is called stick -breaking because we can see the process as start­
ing with a stick of length 1 which is progressively broken: first a piece ß1  long 
is broken off, then a piece ß2 long is broken off from the remaining piece, and 
so on. The length of the i-th piece is therefore 
i-1 
f1 (1 - ßk) ßi  
k= l 
and indicates the probability that the i-th sample Xi  from the base distribution 
is returned. The smaller a  is, the moreprobable high values of ßi  are and the 
more often already sampled values are returned, yielding a more concentrated 
distribution. 
In the example below, the base distribution is a Gaussian with mean 0 
and variance 1, N(O ,  1).  The distribution of values is handled by predicates 
dp_ value (NV , Alpha , V), which returns (in V) the NV-th sample from 
the DP with concentration parameter Alpha, and 
, which returns in L a list of N- NO samples from the DP with concentra­
tion parameter Alpha. 
The distribution of indexes is handled by predicate 
dp_ stick_ index/4 . 
dp_value (NV, Alpha , V) :­
dp_ stick_ index (NV, Alpha , I ), 

432 
cplint  Examples  
dp_pick_value (I , V). 
dp_ p i ck_ va l ue (_ , V) :gaussian (V, O, l ). 
dp_ st i ck_ i ndex (NV , Alpha , I ) 
dp_ stick_ index (l , NV, Alpha , I ). 
dp_ stick_ index (N, NV , Alpha , V) 
stick_ proportion (N, Alpha , P), 
choose_ prop (N, NV , Alpha , P , V). 
choose_prop (N, NV , _ Alpha , P , N) 
pick_ portion (N, NV , P ). 
choose_prop (N, NV , Alpha , P , V) 
ne g_ p i ck_ por tion (N, NV, P ), 
Nl is N+l , 
dp_ st i ck_ i ndex (Nl , NV, Alpha , V) 
stick_ proportion (_ , Alpha , P ) :beta (P , l , Alpha ). 
pick_por t i on (_ , _ , P ) : P; ne g_ p i ck_port i on (_ , _ , P ) :1-P . 
dp_ n_values (N, N, _ Alpha , []) :-
! . 
dp_ n_ values (NO , N, Alpha , [ [V]- l iVs ]) 
NO <N, 
dp_value (NO , Alpha , V), 
Nl is NO +l , 
dp_ n_values (Nl , N, Alpha , Vs ). 
The query 
?- mc_ sample_ arg (dp_ stick_ index (l , l0.0 , V), 2000 , V, L), 
histogram (L, Chart , [nbins (100 ) ] ). 
draws the density of indexes with concentration parameter 10 using 2000 
samples (see Figure 15.4). 
The query 
?- mc_ sample_ arg_ first (dp_ n_ values (0 , 2000 , 10.0 , V), l , V, L), 
L= [Vs - _ ] , 
histogram (Vs , Chart , [nbins (l00 ) ]) . 
draws the density of values over 2000 samples from a DP with concentration 
parameter 10 (see Figure 15.5). 
The query 

15.5  Dirichlet  Processes  433 
200   
180   
160   
140   
120   
100   
80   
60   
40   
20   
10  
20  
30  
40  
50 
60  
70 
80  
Figure  15.4  Distribution of indexes with concentration parameter 10 for the 
stick-breaking example dir i chl et_process. pl. 
·6 
-4 
·2 
Figure  15.5  Distribution of values with concentration parameter 10 for the 
stick-breaking example dirichl et_ process. pl. 
?- hist_repeated_indexes (lOOO , lOO , G). 
called over the program: 
hist_repeated_indexes (Samples , NBins , Chart ) 
repeat_sample (O, Samples , L), 
histogram (L, Chart , [nbins (NBins ) ]) . 
repeat_sample (S , S, [] ) : -
!. 
repeat_sample (SO , S, [ [N] -1 1LS ] ) 
mc_sample_arg_first (dp_stick_index (l , l , lO . O, V), lO , V, L), 

434 
cplint  Examples  
l  
350 
300 
250 
200 
150 
100 
50 
0 4 
4.5 
5 
5.5 
6.5 
7.5 
8.5 
9.5 
Figure  15.6  Distribution of unique indexes with concentration parameter 
10 for the stick-breaking example d i richlet_process. p l. 
length (L, N),  
Sl is SO +l ,  
repeat_sample (Sl , S, LS ).  
shows the distribution of the number of unique indexes over 10 samples from 
a DP with concentration parameter 10 (see Figure 15.6). 
15.5.2  The  Chinese  restaurant  process  
According to the Chinese restaurant view, a DP is a discrete-time stochastic 
process, analogous to seating customers at tables in a Chinese restaurant. 
When a new customer arrives at the restaurant, it is seated to a random table. It  
can be an existing table, chosen with a probability proportional to the number 
of clients already sitting at the table, or a new table, chosen with a probability 
proportional to a.  
Formally, a sequence of samples x 1 ,  x2 ,  ...  is drawn as follows. x 1  is 
drawn from the base distribution ( corresponding to a new table as no customer 
is present). For n  > 1, Jet xn  =  {  x 1 ,  ...  ,  xm}  be the set of distinct values 
previously sampled. Xn  is set to a value xi  E  xn  with probability a+~­ 1 
where ni  is the number of previous observations Xj,  j  <  n,  suchthat Xj  = xi  
(seating at an existing table), and is drawn from the base distribution with 
probability a+~- 1 (seating at a new table). Since 
m  
n  
a  
n - 1  
a  
'\' 
2 
+--­ ---- +  
=1
L.Ja + n - 1  
a + n - 1  
a + n - 1  
a +n - 1 
2= 1 

15.5  Dirichlet  Processes  435 
this is a valid sampling process. 
In example https://cplint.eu/e/dp_chinese.pl, the base distribution is a 
Gaussian with mean 0 and variance 1. Countsare kept and updated by predi­
cate update_ counts / 5. 
dp_n_values (NO , N, Alpha , [ [V] -l iVs ] , CountsO , Counts ) 
NO <N,  
dp_value (NO , Alpha , CountsO , V, Countsl ),  
Nl is NO +l ,  
dp_n_values (Nl , N, Alpha , Vs , Countsl , Counts ).  
dp_value (NV , Alpha , Counts , V, Countsl ) 
draw_sample (Counts , NV , Alpha , I ), 
update_counts (O, I , Alpha , Counts , Countsl ), 
dp_pick_value (I , V). 
update_counts (_ IO , _ I , Alpha , [_ C] , [l , Alpha ] ) : -
!  .  
update_counts (I , I , _Alpha , [C IRest ] , [Cl iRest ] ) 
Cl is C+l . 
update_counts (IO , I , Alpha , [C IRest ] , [C IRestl ] ) 
Il is IO +l , 
update_counts (Il , I , Alpha , Rest , Restl ). 
draw_sample (Counts , NV , Alpha , I ) :­
NS is NV+Alpha , 
maplist (div (NS ), Counts , Probs ), 
length (Counts , LC ), 
numlist (l , LC , Values ), 
maplist (pair , Values , Probs , Discrete ), 
take_sample (NV , Discrete , I ). 
take_sample (_ , D, V) : discrete (V, D). 
dp_pick_value (_ , V) : gaussian (V, O, l ). 
div (Den , V, P ) : -
P is V/ Den . 
pair (A, B, A: B). 
Here ma p 1ist I  3 is a library predicate encoding the ma p 1ist primitive 
of functional programming: mapl i s t (Goal , Listl , List2 ) is true if 
Goal can be successfully applied to all pairs of elements in the same position 
in the two lists. 
The query 
? -
mc_sample_arg_first (dp_n_values ( 0 , 2000 , 10 . 0, V, [ 10 . 0 ] , _ ) , 
1 , V, L), 

436 
cplint  Examples  
L= [Vs - _ ] , 
histogram (Vs , 100 , Chart ). 
draws the density of values over 2000 samples from a DP with concentration 
parameter 10. The resulting graph is similar to the one of Figure 15.5. 
15.5.3  Mixture  model  
DPs can be used as a prior probability distribution in infinite mixture models. 
The objective is to build a mixture model without specifying in advance the 
number k  of components. In example https://cplint.eule/dp_mix.pl, samples 
are drawn from a mixture of normal distributions whose parameters are de­
fined by means of a DP. For each component, the variance is sampled from 
a gamma distribution and the mean is sampled from a Gaussian with mean 
0 and variance 30 times the variance of the component. The program in this 
case is equivalent to the one encoding the stick-breaking example, except for 
the dp_p i c k_va lue / 3 predicate that is shown below: 
dp_pick_value (I , NV, V) 
ivar (I , IV), 
Var is 1. 0/ IV, 
mean (I , Var , M), 
value (NV, M, Var , V) 
ivar (_ , IV ) : gamma (IV, 1 , 0.1 ). 
mean (_ , VO , M) : gaussian (M, O, V) :- V is V0 *30. 
value (_ , M, V, Val ) : gaussian (Val , M, V). 
Given a vector of Observations obs ( [ -1 , 7 , 3 ] ) , the queries 
?- prior (1000 , 100 , G). 
?- post (1000 , 30 , G). 
called over the program 
prior (Samples , NBins , Chart ) 
mc_sample_arg_first (dp_n_values (O, Samples , 10.0 , V), 1, V, L), 
L= [Vs - _ ] , 
histogram (Vs , Chart , [nbins (NBins ) ]) . 
post (Samples , NBins , Chart ) 
obs (0 ), 
maplist (to_ val , 0 , 01 ), 
length (01 , N), 

15.6  Bayesian  Estimation  437 
mc_lw_sample_arg_log (dp_value (O, lO . O, T), 
dp_n_values (O, N, lO . O, Ol ), Samples , T, L), 
maplist (keys , L, LW), 
min_list (LW , Min ), 
maplist (e xp (Min ), L, Ll ), 
histogram (Ll , Chart , [nbins (NBins ), min (-8 ), max (15 ) ]) 
keys (_ - W, W). 
e xp (Min , L- W, L- Wl ) 
Wl is e xp (W- Min ). 
to_val (V, [V] -1 ). 
draw the prior and the posterior densities, respectively, using 200 samples 
(Figures 15.7 and 15.8). Likelihood weighting is used because the evidence 
involves values for continuous random variables. mc_ lw_ sample_ arg_ 
logI  5 differs from mc_ lw_ sample_ arg I  5 because it returns the natural 
logarithm of the weights, useful when the evidence is very unlikely. 
15.6  Bayesian  Estimation  
Let us consider a problern proposed for the Anglican system for probabilis­
tic programming [Wood et al., 2014]3. We are trying to estimate the true 
value of a Gaussian-distributed random variable, given some observed data. 
110 
100 
90 
80 
70 
60 
50 
40 
30 
20 
10 
· 120 
· 100 
·80 
·60 
·40 
·20 
20 
40 
60 
80 
100 
Figure  15.7  Prior density in the dp_mix. pl example. 
3 https://bitbucket.org/probprog/anglican-examples/src/master/worksheets/ 
gaussi an-posteri ors.clj 

438 
cplint  Examples  
The variance is known (its va1ue is 2) and we suppose that the mean has 
itse1f a Gaussian distribution with mean 1 and variance 5. We take different 
measurements (e.g., at different times), indexed by an integer. 
The program https://cplint.eu/e/gauss_mean_est.pl 
val ( I , X) 
: -
mean (M), val ( I , M, X) 
mean (M) : gaussian (M, 1 . 0 , 5 . 0 ). 
val (_ , M, X) : gaussian (X, M, 2 . 0 ). 
models this problem. 
Given that we observe 9 and 8 at indexes 1 and 2, how does the distribu­
tion of the random variable (value at index 0) change with respect to the case 
of no Observations? This examp1e shows that the parameters of the distribu­
tion atoms can be taken from the probabilistic atoms (gauss i an (X , M, 2. 0 ) 
and va l ue (_, M, X) respectively). The query 
?- mc_sample_arg (val (O, Y), lOOO , Y, LO ), 
mc_lw_sample_arg (val (O, X), (val (1 , 9 ), val (2 , 8 )), 1000 , X, L ), 
densities (LO , L, Chart , [nbins (40 ) ] ). 
4.0e+204  
3.5e+204  
3.0e+204  
2.5e+204  
2.0e+204  
1.5e+204  
1.0e+204  
0.5e+204  
·6  
·4  
·2  
0  
2  
4  
6  
8  
10  
12  
14  
Figure  15.8  Posterior density in the dp_mix. pl example. 

15.7  Katman  Filter  439 
takes 1000 samples of argument X  of v a  1  (  0 , X )  before and after the obser­
vation of v  a 1  (1  ,  9 ) , v  a 1  ( 2 , 8 ) and draws the prior and posterior densi­
ties of the samples using a line chart. Figure 15.9 shows the resulting graph 
where the posterior is clearly peaked at around 8. 
15.7  Kaiman  Filter  
Example 57 represents a Kaiman filter, i.e., a hidden Markov model with 
a real value as state and a real value as output. Program https://cplint.eu/e/ 
kalman_filter.pl (adapted from [Nampally and Ramakrishnan, 2014]) encodes 
the example: 
kf_fin (N, O, T ) 
init (S ), 
kf_part (O, N, S , O, T ). 
kf_part ( I , N, S , [V I RO ] , T ) 
I <  N, 
Nexti is I +l , 
trans (S , I , NextS ), 
emit (NextS , I , V), 
kf_part (Nexti , N, NextS , RO , T ) 
kf_part (N, N, S , [ ] , S ) . 
trans (S , I , NextS ) :­
{NextS =:= E+S } , 
trans_err ( I , E ). 
400 
350 
300 
250 
200 
150 
100 
50 
·6 
·4 
·2 
10 
•  pre  •  post  
Figure  15.9  Prior and posterior densities in gauss_mean_est.  pl. 

440  cplint  Examples  
emit (NextS , I , V) 
: 
{V =:= NextS +X}, 
obs_err ( I , X) . 
init (S ) : gaussian (S , O, l ). 
trans_err (_ , E ) : gaussian (E , 0 , 2 ). 
obs_err (_ , E ) : gaussian (E , O, l ). 
The next state is given by the current state plus Gaussian noise (with mean 
0 and variance 2 in this example) and the output is given by the current state 
plus Gaussian noise (with mean 0 and variance 1 in this example). A Kaiman 
filter can be considered as modeling a random walk of a single continuous 
state variable with noisy Observations. 
Thegoals { NextS  =:=  E + S }  and { V=:=  NextS+ X }  areCLP(R) 
constraints. 
Given that, at time 0, the value 2.5 was observed, what is the distribution 
of the state at time 1 (filtering problem)? Likelihood weighting can be used to 
condition the distribution on evidence on a continuous random variable (ev­
idence with probability 0). With CLP(R) constraints, it is possible to sample 
and to weight samples with the same program: when sampling, the constraint 
{ V=:  =NextS + X }  is used to compute V  from X  and NextS.  When weight­
ing, the constraint is used to compute X  from V  and Next S.  The above query 
can be expressed with 
?- mc_sample_arg (kf_fin ( l , _ Ol , Y), lOOO , Y, LO ), 
mc_lw_sample_arg (kf_fin (1 , _ 02 , T ), kf_fin (1 , [2. 5 ] , _ T ), 1000 , 
T , L), densities (LO , L, Chart , [nbins ( 40 ) ] ) . 
that returns the graph of Figure 15.1 0, showing that the posterior distribution 
is peaked around 2.5. 
Given a Kaiman filter with four Observations, the value of the state at 
those time points can be sampled by running particle filtering: 
?-
[01 , 02 , 03 , 04 ] = [-0.133 , -1.183 , -3 . 212 , -4.586 ], 
mc_particle_sample_arg ( [ kf_fin ( l , Tl ), kf_fin (2 , T2 ), 
kf_fin (3 , T3 ) , kf_fin ( 4 , T4 ) ] , [kf_o ( l , Ol ) , kf_o (2 , 02 ), 
kf_o ( 3 , 03 ) , kf_o ( 4 , 04 ) ] , 100 , [Tl , T2 , T3 , T4 ] , 
[Fl , F2 , F3 , F4 ]) . 
where k f o I  2 is defined as 
kf_ o (N, ON ) :­
init (S ), 

15.8  Stochastic  Logic  Programs  441 
160 
140 
120 
100 
80 
60 
40 
20 
~ 
~ 
~ 
4  
~ 
~ 
~ 
•  pre  •  post  
Figure  15.10  Prior and posterior densities in ka l man. pl. 
Nl is N-1 , 
kf_part (O, Nl , S , _ O, _ LS , T ), 
emit (T , N, ON ). 
The list of samples is returned in [F 1 , F 2 , F 3 , F 4 ] , with each element 
being the samples foratime point. 
Given the true states from which the Observations were obtained, Figure 
15.11 shows a graph with the distributions of the state variable at time 1, 2, 3, 
and 4 (S1, S2, S3, S4, density on the Ieft Y-axis) and with the points for the 
Observationsand the states with respect to time (time on the right Y-axis). 
A two-dimensional Kalman filter can be used to track the movements of 
an object over a plane. For example,4 the object may perform a noisy circular 
motion. We receive noisy Observations of the position and the objective is to 
estimate its position at the next time point. A Kaiman filter may produce a 2­
dimensional distribution of the next position of the object such as that shown 
in Figure 15.12, where the true and observed trajectories are shown in the 
upper part as red and green lines, respectively. 
4 Inspired by 
https://bitbucket.org/probprog/anglican-examples/src/master/worksheets/ 
kalman.clj. 

442 
cplint  Examples  
15.8  Stochastic  Logic  Programs  
SLPs (see Section 2.10.1) are used most commonly for defining a distribution 
over the values of arguments of a query. SLPs are a direct generalization of 
PCFGs and are particularly suitable for representing them. For example, the 
grammar 
0 . 2 :S->aS  
0 . 2 :S->bS  
0 . 3 :S->a  
0 . 3 :S->b  
can be represented with the SLP 
0 . 2 :: s ( [ a i R J )  : ­
s ( R ).  
0 . 2 :  :  s ( [ b  I R ] )  : ­
s ( R ).  
0 . 3 :: s  (  [ a ] ).  
0 . 3 :: s ( [b ]). 
This SLP is encoded in cplint as program https://cplint.eu/e/slp_pcfg.pl: 
s_as (N )  : 0 . 2 ; s_bs (N )  : 0 . 2 ; s_a (N )  : 0 . 3 ; s_b (N )  : 0 . 3 .  
s ( [ a i R ] , NO ) : ­
s_as  (NO ),  
-~ "' 
~14 
25 -l  ,3  
3 
20 
15 
-1  
-2 
-3 
10 
-4  
i 
I  
I  
I  
I  
I  
-7 
-10 
-8  
-6 
-4 
-2 
•  True  State  •  Obs  •  S1  •  S2  •  S3  •  S4  
Figure  15.11  Example of particle filtering in kalman. pl. 

15.9  Tile  Map  Generation  443 
Figure  15.12  Particle filtering for a 2D Kaiman filter. 
Nl  is  NO+l ,   
s  (R , Nl ).   
s ( [ b i R ] , NO ) :­
s_bs  (NO ),  
Nl  is  NO +l ,  
s (R, Nl ) .  
s ( [ a ], NO ) :­
s_a  (NO ).  
s ( [ b ], NO )  :­
s_ b  (NO ).  
s ( L ) :­
s (L , 0 ). 
where the predicate s I  2 has one more argument with respect to the SLP, 
which is used for passing a counter to ensure that different calls to s I  2 are 
associated with independent random variables. 
Inference with cpl int can then simulate the behavior of SLPs. For 
example, the query 
?-
mc_ sample_ arg_ bar ( s ( S), lOO , S, P ),  
argbar (P , C).  
samples 100 sentences from the language and draws the bar chart of 
Figure 15.13. 

444 
cplint  Examples  
[[b]] 
[[a]] l~~~~§~:==========:---•
[[a,b]J  
[[a,a]]  
[[b,a]]  
[[a,b,a]]  
[[b,b,b]]  
[[b,b]]  
[[a,a,a]]  
[[a,a,a,b]J  
[[a,a,b]J  
[[a,b,a,a]]  
[[a,b,b,a]]  
[[a,b,b,b]]  
[[a,b,b,b,b,a]]  
[[b,a,a]]  
[[b,a,b]J  
[[b,a,b,b]J  
[[b,b,a,b,b,a,b]]  
[[b,b,b,a]]  
[[b,b,b,b,b]] -1"''-------r----r------r----r---~----r----, 
10 
15 
20 
25 
30 
Figure  15.13  Sampies of sentences of the language defined m 
s lp_pcfg. pl. 
15.9  Tile  Map  Generation  
PLP can be used to generate random complex structures. For example, we 
can write programs for randomly generating maps of video games. Suppose 
that we are given a fixed set of tiles and we want to combine them to obtain 
a 2D map that is random but satisfies some soft constraints on the placement 
of tiles. 
Suppose we want to draw a lOxlO map with a tendency to have a lake 
in the center. The tiles are randomly placed such that, in the central area, 
water is more probable. The problern can be modeled with the example https: 
1/cplint.eu/e/tile_map.swinb, where map  (H ,  w, M)  instantiates M  to a map of 
height H and width W: 
map (H, W, M) : ­
tiles (Tiles ), 
length (Rows , H), 
M= .. [map , Tiles iRows ] , 
foldl (select (H, W), Rows , l , _ ) 
select (H, W, Row , NO , N) : ­
length (RowL , W), 
N is NO +l , 
Row= . . [row iRowL ] , 
foldl (pick_row (H, W, NO ), RowL , l , _ ) 
pick_row (H, W, N, T, MO , M) : ­

15.9  Tile  Map  Generation  445 
M is MO +l , 
pick_tile (N, MO , H, W, T) 
Here foldl I  4 is an SWI-prolog [Wielemaker et al., 2012] library predicate 
that implements the foldl meta-primitive from functional programming: it 
aggregates the results of the application of a predicate to one or more lists. 
foldl / 4 is defined as: 
foldl (P , 
[Xll , ... , Xln ] , 
[Xml , ... , Xmn ] , VO , Vn ) 
P (Xll , Xml , VO , Vl ), 
P (Xln , Xmn , Vn-1 , Vn ). 
pick_ ti l e (Y, X, H, W, T ) returns a tile for position (X, Y) of a map of 
size W*  H. The center tile is water: 
pick_tile (HC , WC , H, W, wate r ) :­
HC is H//2 , 
WC is W//2 ,!. 
In the central area, water is more probable: 
pick_tile (Y, X, H, W, T) : 
discrete (T, [grass : 0.05 , water : 0.9 , tree : 0.025 , rock : 0.025 ] ) :­
central_area (Y, X, H, W),! 
central_ area (Y, X, H, W) is true if (X, Y) is adjacent to the center of 
the W* H map (definition omitted for brevity). In other places, tiles are chosen 
at random with distribution 
grass : O.S , water : 0.3 , tree : 0.1 , rock : 0.1 : 
pick_tile (_ , _ , _ , _ , T) : 
discrete (T, [grass : 0.5 , water : 0.3 , tree : O.l , rock : O.l ] ) 
We can generate a map by taking a sample of the query map (10 , 10 , M) 
and collecting the value of M.  For example, the map of Figure 15.14 can be 
obtained5 . 
5 Tiles from https://github.com/silveira/openpixels 

446  cplint  Examples  
... 
~ 
..  
.
'f  
'f  
~ 
...  
~ ~ 
~ 
..  
Figure 15.14 A random tile map. 
., 
15.10  Markov  logic  Networks  
We have seen in Section 2.11.2.1 that the MLN 
1 . 5 Intelligent (x) => GoodMarks (x) 
1 . 1 Friends (x, y ) => (Intelligent (x) <=> Intelligent (y )) 
can be translated to the program below (https://cplint.eu/e/inference/mln. 
swinb): 
clausel (X) : 0 . 8175744762 : -
\ +intelligent (X).  
clausel (X) : 0 . 1824255238:- intelligent (X), \ +good_ marks (X).  
clausel (X) : 0 . 8175744762 : - intelligent (X), good_marks (X).  
clause2 (X, Y) : 0 . 7502601056:­ 
\+friends (X, Y). 
clause2 (X, Y) : 0 . 7502601056 : ­
friends (X, Y), intelligent (X), intelligent (Y). 
clause2 (X, Y) : 0 . 7502601056 : ­
friends (X, Y), \+intelligent (X), \+intelligent (Y) 
clause2 (X, Y) : 0.2497398944 : ­
friends (X, Y), intelligent (X), \+intelligent (Y). 

15.11  Truel  447 
clause2 (X, Y) : 0 . 2497398944 : ­
friends (X, Y), \+intelligent (X), intelligent (Y). 
intelligent (_ ) : 0 . 5 . 
good_marks (_ ) : 0 . 5 . 
friends (_ , _ ) : 0 . 5 . 
student (anna ). 
student (bob ) . 
The evidence must include the truth of all groundings of the c 1 aus e i  pred­
icates: 
evidence_mln : - clausel (anna ), clausel (bob ), clause2 (anna , anna ), 
clause2 (anna , bob ), clause2 (bob , anna ), clause2 (bob , bob). 
We have also evidence that Anna is friend with Bob and Bob is intelligent: 
ev_intelligent_bob_friends_anna_bob : ­
intelligent (bob ), friends (anna , bob ), 
evidence_mln . 
If  we want to query the probability that Anna gets good marks given the 
evidence, we can ask: 
?- prob (good_marks (anna ), 
ev_intelligent_bob_friends_anna_bob , P ) 
while the prior probability of Anna getting good marks is given by: 
?- prob (good_marks (anna ), evidence_mln , P ). 
We obtain P = 0.733 from the first query and P = 0.607 from the second: 
given that Bob is intelligent and Anna is her friend, it is more probable that 
Anna gets good marks. 
15.11  Truel  
A truel [Kilgour and Brams, 1997] is a duel among three opponents. There 
are three truelists, a,  b,  and c, that take turns in shooting with a gun. The 
firing order is a,  b,  and c. Each truelist can shoot at another truelist or at 
the sky ( deliberate miss). The truelists have these probabilities of hitting the 
target (if they are not aiming at the sky): 1/3, 2/3, and 1  for a,  b,  and c, 
respectively. The aim for each truelist is to kill all the other truelists. The ques­
tion is: what should a  do to maximize his probability of winning? Aim at b,  
c  or the sky? 

448  cplint  Examples  
Let us see first the strategy for the other truelists and situations, following 
[Nguembang Fadja and Riguzzi, 2017]. When only two players are left, the 
best strategy is to shoot at the other player. 
When all three players remain, the best strategy for b  is to shoot at c,  
since if c  shoots at him he his dead and if c  shoots at a,  b  remains with c  
which is the best shooter. Similarly, when all three players remain, the best 
strategy for c  is to shoot at b,  since in this way, he remains with a,  the worst 
shooter. 
For a,  it is more complex. Let us first compute the probability of a  to win 
a duel with a single opponent. When a  and c  remain, a  wins if it shoots c,  with 
probability 1/3. If he misses c,  c  will surely kill him. When a  and b  remain, 
the probability p  of a  winning can be computed with 
p  =   P(a  hits b)  +  P(a  misses b)P(b  misses a)p  
1 
2 
1
p=-+-x-xp 
3 
3 
3  
3 
p  =­7  
The probability can also be computed by building the probability tree of 
Figure 15.15. The probability that a  survives is thus 
1 
2 1 1 
2 1 2 1 1 
p  =   -+-·-·-+-·-·-·-·-+ ...  = 
3 
3 3 3 
3 3 3 3 3  
22 
1 
2 
1 
CIJ 2 ( 2) i  
1 
3
23 
3 +  33 +  35 +  ...  = 3 +  ~ 33 9 
= 3 +  1 -
~ = 
2 
2
1 
33 
1 
3 
1 
2 
9 
3 
3 +   1  = 3 +  7 = 3 +  21 = 21 = 7 
9 
When all three players remain, if a  shoots at b,  b  is dead with probability 
1/3 but then c  will kill a.  If b  is not dead (probability 2/3), b  shoots at c  and 
kills him with probability 2/3. In  this case, a  is left in a duel with b,  with a 
probability of surviving of 317. If b  doesn't kill c  (probability 1/3), c  surely 
kills band a  is left in a duel with c,  with a probability of surviving of 1/3. So 
overall, if a  shoots at b,  his probability of winning is 
2 2 3 
2 1 1 
4 
2 
36 +  15 
50 
- . - . - + - . - . - = -
+ -
= 
= -
~ 0.2645 
3 3 7 
3 3 3 
21 
27 
189 
189 
When all three players remain, if a  shoots at c,  c  is dead with probability 113. 
b  then shoots at a  and a  survives with probability 113 and a  is then in a duel 

15.11  Truel  449 
{\~  
•- ~-~m-
·- l-i  
m~ ;r'L   
··~ l\  
b  killed 
... 
Figure  15.15  Probability tree of the truel with opponents a  and b.  From 
[Nguembang Fadja and Riguzzi, 2017]. 
with b  and surviving with probability 317. If c  survives (probability 2/3), b  
shoots at c  and kills him with probability 2/3, so a  remains in a duel with b  
and wins with probability 317. If c  survives again, he surely kills b  and a  is 
left in a duel with c,  with a probability 1/3 of winning. So overall, if a  shoots 
at c,  his probability of winning is 
1 1 3 
2 2 3 
2 1 1 
1 
4 
2 
59
- . - . - + - . - . - + - . - . - =  -
+ - + -
=  -
~ 0.3122 
3 3 7 
3 3 7 
3 3 3 
21 
21 
27 
189 
When all three players remain, if a  shoots at the sky, b  shoots at c  and kills 
him with probability 2/3, with a  remaining in a duel with b.  If b  doesn't kill 
c,  c  surely kills band a  remains in a duel with c.  So overall, if a  shoots at the 
sky, his probability of winning is 
2 3 
1 1 
2 
1 
25 
- . - + - . - =  - + - =  -
~ 0.3968. 
3 7 
3 3 
7 
9 
63 
So the best strategy for a  at the beginning of the game is to aim at the sky, 
contrary to intuition that would suggest trying to immediately eliminate one 
of the adversaries. 

450 
cplint  Examples  
This problern can be modeled with an LPAD [Nguembang Fadja and 
Riguzzi, 2017]. However, as can be seen from Figure 15.15, the number of 
explanations may be infinite, so we need to use an appropriate exact inference 
algorithm, such as those discussed in Section 8.11, or a Monte Carlo inference 
algorithm. We discuss below the program https://cplint.eu/e/truel.pl. that uses 
MCINTYRE. 
survi ves_ action (A ,  L O, T ,  S )  is true if A  survives the truel per­
forming action S with L O still alive in turn T : 
survives_action (A, LO , T, S) : ­
shoot (A, S, LO , T, L1 ),  
remaining (L1 , A, Rest ),  
survives_round (Rest , L1 , A, T).  
shoot  (H,  S,  L O,  T ,  L )  is true when H  shoots at S  in round T  with L O  and 
L  the list of truelists still alive before and after the shot: 
shoot (H, S , LO , T, L) : ­
(S=sky -> L=LO 
(hit (T, H) - > 
de1ete (LO , S, L) 
L=LO 
)  .  
The probabilities of each truelist to hit the chosen target are  
hit (_ , a ) : 1/3.  
hit (_ , b ) : 2/3 .  
hit (_ , c ) : 1.  
s u  rv i ve s  ( L , A , T )  is true if individual A  survives the truel with truelists  
L at round T:  
survives ( [A] , A, _ ) : - !.  
survives (L, A, T) : ­
survives_round (L, L, A, T) 
survi ves_ round  (Rest ,  L O,  A ,  T )  is true if individual A  survives the 
truel at round T  with Rest  still to shoot and LO  still alive: 
survives_ round ( [] , L, A, T) :­
survives (L, A, s (T)) . 
survives_round ( [HI_ Rest ] , LO , A, T) :­
base_best_strategy (H, LO , S), 
shoot (H, S, LO , T, L1 ), 
remaining (L1 , H, Rest1 ), 
member (A, L1 ), 
survives_round (Rest1 , 11 , A, T) 

15.12  Coupon  Collector  Problem  451 
The following strategies areeasy to find: 
base_best_strategy (b , [b , c ] , c ) 
base_best_strategy (c , [b , c ] , b ) 
base_best_strategy (a , [a , c ] , c ) 
base_best_strategy (c , [a , c ] , a ) 
base_best_strategy (a , [a , b ] , b ) 
base_best_strategy (b , [a , b ] , a ) 
base_best_strategy (b , [a , b , c ] , c ) 
base_best_strategy (c , [a , b , c ] , b ) 
Auxiliary predicate remainingI  3 is defined as 
remaining ( [A I Rest ] , A, Rest ) : - ! . 
remaining ( [_ IRestO ] , A, Rest ) : ­
remaining (RestO , A, Rest ). 
We can decide the best strategy for a by asking the probability of the queries 
?- survives_action (a , [a , b , c ] , O, b ) 
?- survives_action (a , [a , b , c ] , O, c ) 
?-
survives_action (a , [a , b , c ] , O, sky ) 
By taking 1000 samples, we may get 0.256, 0.316, and 0.389, respectively, 
showing that a  should aim at the sky. 
15.12  Coupon  Collector  Problem  
In the coupon collector problern [Kaminski et al., 2016], a company sells 
boxes of cereals each containing a coupon belanging to N  different types. 
The probability of a coupon type in a box is uniform over the set of coupon 
types. The customers buying the boxes collect the coupons and, one they have 
one coupon of each type, they can obtain a prize. The problern is: if there are 
N  different coupons, how many boxes, T,  do I have to buy to get the prize? 
This problern is modeled by program https://cplint.eule/coupon.swinb 
defining predicate coupons I  2 such that goal c o upons (N ,  T )  is true if 
we need T  boxes to get N  coupons. The coupons are represented with a term 
for functor cp i N with the number of coupons as arity. The i-th argument of 
the term is I if the i-th coupon has been collected and is a variable otherwise. 
The term thus represents an array: 
coupons (N, T) : ­
length (CP , N), 
CPTerm= .. [cp i CP ] , 
new_coupon (N, CPTerm , O, N, T ) 

452 
cplint  Examples  
If 0 coupons remain to be collected, the collection ends: 
new_coupon (O, _ CP , T, _ N, T). 
If NO coupons remain tobe collected, we collect one and recurse: 
new_coupon (NO , CP , TO , N, T) :­
NO >O , 
collect (CP , N, TO , Tl ), 
Nl is N0 -1 , 
new_coupon (Nl , CP , Tl , N, T) 
collect / 4 collects one new coupon and updates the number of boxes 
bought: 
collect (CP , N, TO , T) :­
pick_a_box (TO , N, I ), 
Tl is TO +l , 
arg (I , CP , CP I ) , 
(var (CPI ) - >  
CPI =l , T=Tl  
collect (CP , N, Tl , T) 
)  .  
pick_ a _ box / 3 random1y picks a box and so a coupon type, an e1ement  
from the 1ist [1 ... N]:  
pick_a_box (_ , N, I ) : uniform (I , L) :- numlist (l , N, L).  
If there are five different coupons, we may ask:  
• How many boxes do I have to buy to get the prize? 
• What is the distribution of the number of boxes I have to buy to get the 
prize? 
• What is the expected number of boxes I have to buy to get the prize? 
To answer the first query, we can take a single samp1e for coupans (5 , T ) : 
in the sample, the query will succeed as coupans I  2 is a determinate predi­
cate and the result will instantiate T to a specific value. For example, we may 
get T=15. Note that the maximum number ofboxes to buy is unbounded but 
the case where we have to buy an infinite number of boxes has probabi1ity 0, 
so samp1ing will sure1y finish. 
To compute the distribution on the number of boxes, we can take a number 
of samp1es, say 1000, and p1ot the number of times a va1ue is obtained as a 
function ofthe va1ue. By doing so, we may get the graph in Figure 15.16. 
To compute the expected number of boxes, we can take a number of 
samples, say 100, of coupans (5 , T). Each sample will instantiate T. By 

15.13  One-dimensional  Random  Walk  453 
summing all these values and dividing by 100, the number of samples, we 
can get an estimate of the expectation. This computation is performed by the 
query 
?- mc_expectation (coupons (S, T), lOO , T, Exp ). 
For example, we may get a value of 11.4 7. 
We can also plot the dependency of the expected number of boxes from 
the number of coupons, obtaining Figure 15.17. As observed in [Kaminski 
et al., 2016], the number of boxes grows as O(N  log N)  where N  is the 
number of coupons. The graph also includes the curve 1 + 1.2N log N  that is 
close to the first. 
The coupon collector problern is similar to the sticker collector problem, 
where we have an album with a space for every different sticker, we can buy 
stickers in packs and our objective is to complete the album. A program for 
the coupon collector problern can be applied to solve the sticker collector 
problem: if you have N  different stickers and packs contain P  stickers, we 
can solve the coupon collector problern for N  coupons and get the number of 
boxesT. Then the number of packs you have to buy to complete the collection 
is rr I Pl. So we can write: 
stickers (N, P , T) : -
coupons (N, TO ), T is ceiling (TO / P ). 
If there are 50 different stickers and packs contain four stickers, by sampling 
the query stickers (50 , 4 , T ), we can get T=4 7, i.e., we have to buy 47 
packs to complete the entire album. 
450  
400  
350  
300  
250  
200  
150  
100  
50  
10  
15  
20  
25  
30  
35  
40  
45  
•  dens  
Figure  15.16  Distribution of the number of boxes. 

454 
cplint  Examples  
40 
35 
30 
25 
20 
15 
10 
11 
•  Expected  number  of  boxes  •  1 + 1 .2NiogN  
Figure  15.17  Expected number of boxes as a function of the number of 
coupons. 
15.13  One-dimensional  Random  Walk  
Let us consider the version of a random walk described in [Kaminski et al., 
2016]: a particle starts at position x  =  10 and moves with equal probability 
one unit to the left or one unit to the right in each turn. The random walk 
stops if the particle reaches position x  = 0. 
The walk terminates with probability 1 [Hurd, 2002] but requires, on av­
erage, an infinite time, i.e., the expected number oftums is infinite [Kaminski 
et al., 2016]. 
We can compute the number of turns with program https://cplint.eu/e/ 
random_walk.swinb. The walk starts at time 0 and x  = 10: 
walk (T ) :- walk ( lO , O, T ).  
If x  is 0, the walk ends; otherwise, the particle makes a move:  
walk ( O, T , T ).  
walk (X, TO , T ) :­
X>O , 
move (TO , Move ), 
Tl is TO +l , 
Xl is X+Move , 
walk (Xl , Tl , T ). 
The move is either one step to the left or to the right with equal probability.  
move (T , l ) :0.5 ; move (T , -1 ) :0.5 .  
By sampling the query wa lk (T), we obtain a success as wa l k /1 is deter­ 
minate. The value for T represents the number of turns. For example, we may  
get T =  3692 .  

15.14  Latent  Dirichlet  Allocation  455 
15.14  Latent  Dirichlet  Allocation  
Text mining [Holzinger et al., 2014] aims at extracting knowledge from texts. 
LDA [Blei et al., 2003] is a text mining technique which assigns topics to 
words in documents. Topics are taken from a finite set { 1, ... , K}. The model 
describes a generative process where documents are represented as random 
mixtures over latent topics and each topic defines a distribution over words. 
LDA assumes the following generative process for a corpus D  consisting of 
M  documents each of length Ni: 
1.  Sampie (}i  from Dir( a), where i  E {  1, ... , M} and Dir( a) is the Dirich­
let distribution with parameter a. 
2.  Sampie 'Pk  from Dir(,ß), where k  E  {1, ... , K}. 
3.  Foreach of the word positions i , j,  where i  E  {1, ... , M} and j  E  
{1, ... , Ni}: 
(a) Sampie a topic Zi,j  from Discrete( (}i).  
(b) Sampie a word Wi  y·  from Discrete(rpz .).
'  
t ,J  
This is a smoothed LDA model to be precise. Subscripts are often dropped, 
as in the plate diagrams in Figure 15.18. 
The Dirichlet distribution is a continuous multivariate distribution whose 
parameter a is a vector ( a1,  ...  ,  ax)  and a value x =  (  x1 ,  ...  ,  x K)  sampled 
from Dir(a) is suchthat Xj  E  (0, 1) for j  =  1, ... , K  and :L:f=1 Xi  =  
 
1. A 
sample x from a Dirichlet distribution can thus be the parameter for a discrete 
distribution Discrete(x) with as many values as the components of x: the 
distribution has P( Vj)  =  Xj  with Vj  a value. Therefore, Dirichlet distributions 
are often used as priors for discrete distributions. The ,ß  vector above has V  
components where V  is the number of distinct words. 
The aim is to compute the probability distributions of words for each 
topic, of topics for each word, and the particular topic mixture of each docu­
ment. This can be done with inference: the documents in the dataset represent 
the observations ( evidence) and we want to compute the posterior distribution 
of the above quantities. 
This problern can modeled by the MCINTYRE program https://cplint.eu/ 
e/lda.swinb, where predicate 
word (Doc , Position , Word) 
indicates that document Doc in position Pos i t i o n (from 1 to the number 
of words of the document) has word Word  and predicate 
topic (Doc , Position , Topic ) 

456  cplint  Examples  
J(  
~cr8 
N1NJ  
Figure  15.18  Smoothed  LDA.  From  [Nguembang  Fadja  and  Riguzzi,  
2017].  
indicates  that  document  Doc associates  topic  Topi c to  the  word  in  position  
Positio n . We  also  assume  that  the  distributions  for  both  8i  and  'Pk  are  
symmetric  Dirichlet  distributions  with  scalar  concentration  parameter  17  set  
using  a  fact  forthe  predicate  eta /1 , i.e.,  o:  = [17, ... ,17] and  ß  = [17, ... ,TJ].  
The  program  is  then:  
theta (_ , Theta ) : dirichlet (Theta , Alpha ) : ­
alpha (Alpha ). 
topic (DocumentiD , _ , Topic ) : discrete (Topic , Dist ) : ­
theta (DocumentiD , Theta ), 
topic_list (Topics ), 
maplist (pair , Topics , Theta , Dist ) 
word (DocumentiD , WordiD , Word ) : discrete (Word, Dist ) : ­
topic (DocumentiD , WordiD , Topic ), 
beta (Topic , Beta ), 
word_list (Words ), 
maplist (pair , Words , Beta , Dist ) 
beta (_ , Beta ) : dirichlet (Beta , Parameters ) : ­
n_words (N), 
eta (Eta ), 
findall (Eta , between (l , N, _ ), Parameters ) 
alpha (Alpha ) : ­
eta (Eta ), 
n_topics (N) , 
findall (Eta , between (l , N, _ ), Alpha ). 

15.14  Latent  Dirichlet  Allocation  457 
[9]  
[4)  
(7)  
[2)  
(5)  
[6]  
[8]  
(1 0)  
(1)  
(3)  
0  
2  
4  
6  
8  
10  
12 
14  
Figure  15.19  Values for word in position 1 of document 1. 
eta (2 ). 
pair (V, P , V:P ). 
Suppose we have two topics, indicated with integers 1 and 2, and 10 words, 
indicated with integers 1, ... , 10: 
topic_ list (L ) :­
n_ topics (N) , 
numlist ( l , N, L ). 
word_list (L ) : ­
n_words (N), 
numlist (l , N, L ). 
n_ topics (2 ). 
n_words ( lO ). 
We can, for example, use the model generatively and sample values for the 
word in position 1 of document 1. The histogram of the frequency of word 
values when taking 100 samples is shown in Figure 15.19. 
We can also sample values for pairs (word, topic) in position 1 of docu­
ment 1.  The histogram ofthe frequency ofthe pairs when taking 100 samples 
is shown in Figure 15.20. 
We can use the model to classify the words into topics. Herewe use con­
ditional inference with Metropolis-hastings. A priori both topics are about 

458 
cplint  Examples  
[(9,1)]  
[(9,2)]  
[(8,2)]  
[(2,1)]  
[(8,1)]  
[(10,2)]  
[(3,2)]  
[(5,1)]  
[(5,2)]  
[(6,1)]  
[(7, 1)]  
[(1 ,2)]  
[(4,2)]  
[(10,1)]  
[(3,1)]  
[(6,2)]  
[(7,2)]  
[(1 ,1)]  
[(2,2)]  
[(4, 1)] -!"" -
-.----.------r--r---.----.----.---.------r--r--.----"  
0 
2 
3 
4 
10  
11  
12  
Figure  15.20  Values for couples (word,topic) in position 1 of document 1. 
[2[ 
[1[ 
Figure  15.21  Priordistribution of topics for word in position 1 of document 
1. 
equally probable for word 1 of document, so if we take 100 samples of 
topic (1 , 1 , T ), we get the histogram in Figure 15.21. Ifwe observe that 
words 1 and 2 of document 1 are equal (word ( 1 , 1 , 1 ) , word ( 1 , 2 , 1 ) as 
evidence) and take again 100 samples, one of the topics gets more probable, 
as the histogram of Figure 15.22 shows. You can also see this if you look 
at the density of the probability of topic 1 before and after observing that 
words 1 and 2 of document 1 are equal: the observation makes the distribution 
less uniform, see Figure 15.23. piercebayes [Turliuc et al., 2016] is a 

15.15  The  Indian  GPA  Problem  459 
[1[ 
[2[ 
100 
Figure  15.22  Posterior distribution of topics for word in position 1 of 
document 1. 
35 
30 
25 
20 
15 
10 
0.1 
0.2 
0.3 
0.4  
0.5 
0.6 
0.7 
0.8 
0.9 
•  pre  •  post  
Figure  15.23  Density of the probability of topic 1 before and after observ­
ing that words 1 and 2 of document 1 are equal. 
PLP language that allows the specification of Dirichlet priors over discrete 
distribution. Writing LDA models with it is particularly simple. 
15.15  The  Indian  GPA  Problem  
In the Indian GPA problern proposed by Stuart Russel [Perov et al., 2017; 
Nitti et al., 2016], the question is: if you observe that a student GPA is ex­

460 
cplint  Examples  
actly 4.0, what is the probability that the student is from India, given that the 
American GPA score is from 0.0 to 4.0 and the Indian GPA score is from 0.0 
to 10.0? Stuart Russel observed that most probabilistic programming systems 
are not able to deal with this query because it requires combining continuous 
and discrete distributions. This problern can be modeled by building a mixture 
of a continuous and a discrete distribution for each nation to account for grade 
inftation (extreme values have a non-zero probability). Then the probability 
ofthe student's GPA is a mixture ofthe nation mixtures. Given this modeland 
the fact that the student's GPA is exactly 4.0, the probability that the student 
is American is thus 1.0. 
This problern can be modeled in MCINTYRE with program https://cpljnt. 
eu/e/indian_gpa.pl. The probability distribution of GPA scores for American 
students is continuous with probability 0.95 and discrete with probability 
0.05: 
is_density_A :0 . 95 ; is_discrete_A : 0 . 05 . 
The GPA of an American student follows a beta distribution if the distribution 
is continuous: 
agpa (A) : beta (A, 8 , 2 ) 
is_density_A . 
The GPA of an American student is 4.0 with probability 0.85 and 0.0 with 
probability 0.15 if the distribution is discrete: 
american_gpa (G) 
discrete (G, [4 . 0:0 . 85 , 0 . 0 : 0 . 15 ] ) 
is discrete_A . 

15.16  Bongard  Problems  461 
or is obtained by rescaling the value of retumed by agpa I  1 to the (0.0,4.0)  
interval:  
american_gpa (A) : -
agpa (AO ), A is A0 *4 . 0 .  
The probability distribution of GPA scores for Indian students is continuous  
with probability 0.99 and discrete with probability 0.01.  
is_density_I : 0 . 99 ; is discrete 1 : 0 . 01 .  
The GPA of an Indian student follows a beta distribution if the distribution is  
continuous:  
igpa ( I ) : beta ( I , 5 , 5 ) :- is_density_I.  
The GPA of an Indian student is 10.0 with probability 0.9 and 0.0 with  
probability 0.1 if the distribution is discrete:  
indian_gpa ( I ) : discrete ( I , [ 0 . 0 : 0 . 1 , 10 . 0 : 0 . 9 ] ) : -
is_discrete I.  
or is obtained by rescaling the value retumed by igpa /1 to the (0.0,10.0)  
interval:  
indian_gpa ( I ) 
: -
igpa ( IO ), I is IO dO . O.  
The nation is America with probability 0.25 and India with probability 0.75.  
nation (N) 
: discrete (N, [a : 0 . 25 , i : 0 . 75 ] ).  
The GPA of the student is computed depending on the nation:  
student_gpa (G) 
: -
nation (a ), american_gpa (G) 
student_gpa (G) 
: -
nation (i ), indian_gpa (G) . 
If we query the probability that the nation is America given that the student 
got 4.0 in his GPA, we obtain 1.0, while the prior probability that the nation 
is America is 0.25. 
15.16  Bongard  Problems  
The Bongard Problems Bongard [1970] were used in [De Raedt and Van Laer, 
1995] as a testbed for ILP. Each problern consists of a number of pictures 
divided into two classes, positive and negative. The goal is to discriminate 
between the two classes. 
The pictures contain geometric figures such as squares, triangles, and cir­
cles, with different properties, such as small, large, and pointing down, and 
different relationships between them, such as inside and above. Figure 15.24 
shows some of these pictures. 

462 
cplint  Examples  
'v'6  b tJ 
rnb0 
90
14\l 
0 
14\l
4
"Li  
'o 
4o
65J o  
'Li  6/',.
4\1'@ 
0  
b  
I  
16 
oc:J 
'3o"o  
oß 
'v 
'o  
10\7
9 
v,b  
"o  ~7/",.
~ 
13/",. 
Figure  15.24  Bongard pictures. 
A Bongard problern is encoded by https://cplint.eu/e/bongard_R.pl. Each 
picture is described by a mega-interpretation, in this case, contains a single 
example, either positive or negative. One such mega-interpretation can be 
begin (model (2 )). 
pos . 
triangle (o5 ). 
config (o5 , up ). 
square (o4 ). 
in (o4 , o5 ). 
circle (o3 ). 
triangle (o2 ). 
config (o2 , up ). 
in (o2 , o3 ). 
triangle (ol ). 
config (ol , up ) . 
end (model (2 )). 
where begin  (mo del  ( 2 ))  and end (mo del  ( 2 ))  denote the beginning 
and end of the mega-interpretation with identifier 2 . The target predicate 
is p os I  0 that indicates the positive class. The mega-interpretation above 
includes one positive example. 
Consider the input LPAD 
pos : 0.5 : ­
circle (A), 
in (B, A). 
pos : 0 . 5 : ­
circle (A), 
triangle (B). 
and definitions for folds (sets of examples) such as 
fold (train , [2 , 3 , ... ] ) . 
fold (test , [490 , 491 , ... ] ) . 

15.16  Bongard  Problems  463 
We can learn the parameters of the input program with EMBLEM using the 
query 
?- induce_par ( [train ] , P). 
The result is a program with updated values for the parameters: 
pos : 0.0841358 
circle (A), 
in (B, A). 
pos : 0.412669 
circle (A), 
triangle (B). 
We can perform structure learning using SLIPCOVER by specifying a lan­
guage bias: 
modeh (* , pos ).  
modeb (* , triangle (- obj )).  
modeb (* , square (- obj )).  
modeb (* , circle (- obj )).  
modeb (* , in (+obj , - obj )).  
modeb (* , in (- obj , +obj )).  
modeb (* , config (+obj , -#dir )).  
Then the query 
?- induce ( [train ] , P). 
performs structure learning and returns a program: 
pos : 0 . 220015 : ­
triangle (A), 
config (A, down ). 
pos : O. l2513 : ­
triangle (A), 
in (B, A). 
pos : 0.315854 : ­
triangle (A). 


16   
Conclusions  
Wehave come to the end of our joumey through probabilistic logic program­
ming. I sincerely hope that I was able to communicate my enthusiasm for 
this field which combines the powerful results obtained in two previously 
separated fields: uncertainty in  artificial intelligence and logic programming. 
PLP is growing fast but there is still much to do. An important open problern 
is how to scale the systems to large data, ideally of the size of Web, in order 
to exploit the data available on the Web, the Semantic Web, the so-called 
"knowledge graphs," big databases such as Wikidata, and semantically anno­
tated Web pages. Another important problern is how to deal with unstructured 
data such as natural language text, images, videos, and multimedia data in  
general. 
For facing the scalability challenge, faster systems can be designed by 
exploiting symmetries in  model using, for example, lifted inference, or re­
strictions can be imposed in order to obtain more tractable sublanguages. 
Another approach consists in exploiting modern computing infrastructures 
such as clusters and clouds and developing parallel algorithms, for example, 
using MapReduce [Riguzzi et al., 2016b]. 
For unstructured and multimedia data, handling continuous distributions 
effectively is fundamental. Inference for hybrid programs is relatively new 
but is already offered by various systems. The problern of leaming hybrid 
programs, instead, is less explored, especially as regards structure leaming. 
In  domains with continuous random variables, neural networks and deep 
leaming [Goodfellow et al., 2016] achieved impressive results. An interesting 
avenue for future work is how to exploit the techniques of deep leaming for 
leaming hybrid probabilistic logic programs. 
465   

466 
Conclusions  
Some works have already started to appear on the topic, see sections 13.7 
and 14.6, but an encompassing framework dealing with different levels ofcer­
tainty, complex relationships among entities, mixed discrete and continuous 
unstructured data, and extremely large size is still missing. 

Bibliography  
M. Abadi, A. Agarwa1, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Cor­
rado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I.  Goodfellow, A. Harp, 
G. Irving, M. Isard, Y. Jia, R.  Jozefowicz, L.  Kaiser, M. Kud1ur, J. Lev­
enberg, D. Mane, R.  Monga, S. Moore, D. Murray, C. Olah, M. Schuster, 
J. Sh1ens, B. Steiner, I.  Sutskever, K.  Ta1war, P. Tucker, V.  Vanhoucke, 
V.  Vasudevan, F. Viegas, 0. Vinya1s, P. Warden, M. Wattenberg, M. Wicke, 
Y. Yu, and X. Zheng. TensorFlow: Large-sca1e machine 1earning on het­
erogeneaus systems, 2015. URL https://www.tensorflow.org/. Software 
avai1ab1e from tensorflow.org. 
M. A1berti, E. Bellodi, G. Cota, F. Riguzzi, and R.  Zese. cp1 int  on SWISH: 
Probabilistic 1ogica1 inference with a web browser. Intelligenza  Artijiciale,  
11(1):47-64, 2017. doi:10.3233/IA-170105. 
M. A1viano, F.  Calimeri, C. Dodaro, D. Fusca, N. Leone, S. Perri, F.  Ricca, 
P. Veltri, and J. Zangari. The ASPsystem DLV2. In  M. Ba1duccini and 
T. Janhunen, editors, 14th  International  Conference  on  Logic  Program­
ming  and  Non-monotonie  Reasoning  (LPNMR  2017),  vo1ume 10377 of 
LNCS.  Springer, 2017. doi:10.1007/978-3-319-61660-5_19. 
N. Ange1opou1os. clp(pdf(y)): Constraints for probabilistic reasoning in 1ogic 
programming. In F. Rossi, editor, 9th  International  Conference  on  Prin­
ciples  and  Practice  of Constraint  Programming  (CP  2003),  vo1ume 2833 
of LNCS,  pages 784--788. Springer, 2003. doi:10.1007/978-3-540-45193­
8_53. 
N. Ange1opou1os. Probabilistic space partitioning in constraint 1ogic pro­
gramming. In M. J. Maher, editor, 9th  Asian  Computing  Science  Confer­
ence  (ASIAN  2004),  vo1ume 3321 of LNCS,  pages 48-62. Springer, 2004. 
doi: 10.1007 /978-3-540-30502-6_ 4. 
N. Ange1opou1os. Notes on the imp1ementation of FAM. In  A. Hommersom 
and S. A. Abdallah, editors, 3rd  International  Workshop  on  Probabilis­
tic  Logic  Programming  (PLP  2016),  vo1ume 1661 of CEUR  Workshop  
Proceedings,  pages 46-58. CEUR-WS.org, 2016. 
467  

468 
Bibliography  
K.  R.  Apt and M. Bezem. Acyclic programs. New  Generation  Computing,  9 
(3/4):335-364, 1991. 
K.  R.  Apt and R.  N. Bol. Logic programming and negation: A survey. Journal  
of Logic  Programming,  19:9-71, 1994. 
R.  Ash and C.  Doleans-Dade. Probability  and  Measure  Theory.  Harcourt/A­
cademic Press, 2000. ISBN 9780120652020. 
T. Augustin, F.  P. Coolen, G. De Cooman, and M. C. Troffaes. Introduction  
to  imprecise  probabilities.  John Wiley &  Sons, Ltd., 2014. 
D. Azzolini, F.  Riguzzi, and E. Lamma. A semantics for hybrid probabilistic 
logic programs with function symbols. Artificial  Intelligence,  294:103452, 
2021. ISSN 0004-3702. doi:10.1016/j.artint.2021.103452. 
F.  Bacchus. Using first-order probability logic for the construction of 
bayesian networks. In  9th  Conference  Conference  on  Uncertainty  in  
Artificial  Intelligence  (UAI  1993),  pages 219-226, 1993. 
R.  I.  Bahar, E. A. Frohm, C. M. Gaona, G. D. Hachtel, E. Macii, 
A. Pardo, and F.  Somenzi. Algebraic decision diagrams and their ap­
plications. Formal  Methods  in  System  Design,  10(2/3):171-206, 1997. 
doi: 10.1023/ A: 1008699807 402. 
J. K.  Baker. Trainahle grammars for speech recognition. In D. H. Klatt and 
J. J. Wolf, editors, Speech  Communication  Papersfor  the  97th  Meeting  of  
the  Acoustical  Society  of America,  pages 547-550, 1979. 
C. Baral, M. Gelfond, and N. Rushton. Probabilistic reasoning with answer 
sets. Theory  and  Practice  of  Logic  Programming,  9(1):57-144, 2009. 
doi: 10.1017/S1471068408003645. 
L.   Bauters, S. Schockaert, M. De Cock, and D. Vermeir. Possibilistic an­
swer set programming revisited. In 26th  International  Conference  on  
Uncertainty  in  Artificial  Intelligence  (UAI  2010).  AUAI Press, 2010. 
V.  Belle, G. V. den Broeck, and A. Passerini. Hashing-based approximate 
probabilistic inference in hybrid domains. In M. Meila and T. Hes­
kes, editors, 31st  International  Conference  on  Uncertainty  in  Artificial  
Intelligence  (UAI  2015),  pages 141-150. AUAI Press, 2015a. 
V. Belle, A. Passerini, and G. V. den Broeck. Probabilistic inference in hybrid 
domains by weighted model integration. In Q. Yang and M. Wooldridge, 
editors, 24th  International  Joint  Conference  on  Artificial  Intelligence  
(IJCAI  2015),  pages 2770-2776. AAAI Press, 2015b. 
V.  Belle, G. V. den Broeck, and A. Passerini. Component caching in hy­
brid domains with piecewise polynomial densities. In  D. Schuurmans and 
M. P. Wellman, editors, 30th  National  Conference  on  Artificial  Intelligence  
(AAAI  2015),  pages 3369-3375. AAAI Press, 2016. 

Bibliography  469 
E. Bellodi and F.  Riguzzi. Experimentation of an expectation maximization 
algorithm for probabilistic logic programs. Intelligenza  Artificiale,  8(1): 
3-18, 2012. doi:10.3233/IA-2012-0027. 
E.  Bellodi and F.  Riguzzi. Expectation maximization over binary decision 
diagrams for probabilistic logic programs. Intelligent  Data  Analysis,  17 
(2):343-363, 2013. 
E. Bellodi and F.  Riguzzi. Structure leaming of probabilistic logic programs 
by searching the clause space. Theory  and  Practice  ofLogic  Programming,  
15(2):169-212, 2015. doi:10.1017/S1471068413000689. 
E.  Bellodi, E. Lamma, F.  Riguzzi, V.  S. Costa, and R. Zese. 
Lifted variable elimination for probabilistic logic programming. The­
ory  and  Practice  of  Logic  Programming,  14(4-5):681-695, 2014. 
doi: 10.1017 /S1471068414000283. 
E.  Bellodi, M. Alberti, F.  Riguzzi, and R. Zese. MAP inference for proba­
bilistic logic programming. Theory  and  Practice  of Logic  Programming,  
20(5):641-655, 2020. doi:10.1017/S1471068420000174. 
P.  Berka. Guide to the financial data set. In ECMUPKDD  2000  Discovery  
Challenge,  2000. 
C. Bishop. Pattern  Recognition  and  Machine  Learning.  Information Science 
and Statistics. Springer, 2016. ISBN 9781493938438. 
D. M. Blei, A.  Y. Ng, and M. I.  Jordan. Latent Dirichlet allocation. Journal  
ofMachine  Learning  Research,  3:993-1022,2003. 
H. Blockeel. Probabilistic logical models for Mendel's experiments: An exer­
cise. In Inductive  Logic  Programming  (ILP  2004),  Work  in  Progress  Track,  
pages 1-5, 2004. 
H.  Blockeel and L.  D. Raedt. Top-down induction of first-order log­
ical decision trees. Artificial  Intelligence,  101(1-2):285-297, 1998. 
doi:10.1016/S0004-3702(98)00034-4. 
H. Blockeel and J. Struyf. Frankenstein classifiers: Some experiments on the 
Sisyphus data set. In Workshop  on  Integration  of Data  Mining,  Decision  
Support,  and  Meta-Learning  (IDDM  200I),  2001. 
M.  M. Bongard. Pattern  Recognition.  Hayden Book Co., Spartan Books, 
1970. 
S. Bragaglia and  F.  Riguzzi. Approximate inference for logic programs 
with annotated disjunctions. In 21st  International  Conference  on  Induc­
tive  Logic  Programming  (ILP  2011),  volume 6489 of LNAI,  pages 30-37, 
Florence, Italy, 27-30 June 2011. Springer. 
D. Brannan. A  First  Course  in  Mathematical  Analysis.  Cambridge University 
Press, 2006. ISBN 9781139458955. 

470 
Bibliography  
R. Camap. Logical  Foundations  of Probability.  University of Chicago Press, 
1950. 
M. Chavira and A. Darwiche. On probabilistic inference by weighted model 
counting. Artificial  Intelligence,  172(6-7):772-799, 2008. 
W.  Chen and D. S. Warren. Tabled evaluation with delaying for 
general logic programs. Journal  of  the  ACM,  43(1):20-74, 1996. 
doi: 10.1145/227595.227597. 
W.  Chen, T. Swift, and D. S. Warren. Efficient top-down computation of 
queries under the well-founded semantics. Journal  of Logic  Programming,  
24(3):161-199, 1995. 
Y. S. Chow and H. Teicher. Probability  Theory:  Independence,  Interchange­
ability,  Martingales.  Springer Texts in Statistics. Springer, 2012. 
K.  L.  Clark. Negation as failure. In Logic  and  data  bases,  pages 293-322. 
Springer, 1978. 
P. Cohn. Basic  Algebra:  Groups,  Rings,  and  Fields.  Springer, 2003. 
A.  Colmerauer, H. Kanoui, R. Pasero, and P. Roussel. Un systeme de 
communication homme-machine en fran<;ais. Technical report, Groupe 
de Recherche en Intelligence Artificielle, Universite d'Aix-Marseille, 
1973. 
J.  Cörte-Real, T. Mantadelis, I.  de Castro Dutra, R. Rocha, and E. S. 
Bumside. Sk:ILL - A stochastic inductive logic leamer. In  T. Li,  L.  A. 
Kurgan, V. Palade, R. Goebel, A. Holzinger, K.  Verspoor, and M. A. 
Wani, editors, 14th  IEEE  International  Conference  on  Machine  Learn­
ing  and  Applications  (ICMLA  2015),  pages 555-558. IEEE Press, 2015. 
doi: 1 0.1109/ICMLA.20 15.159. 
J.  Cörte-Real, I.  de Castro Dutra, and R. Rocha. Estimation-based search 
space traversal in PILP environments. In J. Cussens and A. Russo, 
editors, 26th  International  Conference  on  Inductive  Logic  Program­
ming  (ILP  2016),  volume 10326 of LNCS,  pages 1-13. Springer, 2017. 
doi: 10.1007 /978-3-319-63342-8_1. 
V.  S. Costa, D. Page, M. Qazi, and J. Cussens. CLP(BN): Constraint logic 
programming for probabilistic knowledge. In  19th  International  Confer­
ence  on  Uncertainty  in  Artificial  Intelligence  (UAI  2003),  pages 517-524. 
Morgan Kaufmann Publishers, 2003. 
F.  G. Cozman and D. D. Maua. On the semantics and complexity of prob­
abilistic logic programs. Journal  of Artificial  Intelligence  Research,  60: 
221-262, 2017. doi:10.1613/jair.5482. 
F.  G. Cozman and D. D. Maua. The joy of probabilistic answer set pro­
gramming: Semantics, complexity, expressivity, inference. International  

Bibliography  471 
Journal  of Approximate  Reasoning,  125:218-239, 2020. ISSN 0888-613X. 
doi: 10.10 16/j.ijar.2020.07 .004. 
J.  Cussens. Parameter estimation in stochastic logic programs. Machine  
Learning,  44(3):245-271, 2001. doi:10.1023/A:1010924021315. 
E. Dantsin. Probabilistic logic programs and their semantics. In Russian  Con­
ference  on  Logic  Programming,  volume 592 of LNCS,  pages 152-164. 
Springer, 1991. 
E. Dantsin, T. Eiter, G. Gottlob, and A.  Voronkov. Complexity and expressive 
power of logic programming. ACM  Computing  Surveys,  33(3):374--425, 
2001. doi:10.1145/502807.502810. 
A.  Darwiche. A logical approach to factoringbelief networks. In  D. Fensel, 
F. Giunchiglia, D. L.  McGuinness, and M. Williams, editors, 8th  Inter­
national  Conference  on  Principles  and  Knowledge  Representation  and  
Reasoning,  pages 409-420. Morgan Kaufmann, 2002. 
A.  Darwiche. New advances in compiling CNF into decomposable negation 
normal form. In R.  L.  de Mantaras and L.  Saitta, editors, 16th  European  
Conference  on  Artificial  Intelligence  (ECAI  20014),  pages 328-332. lOS 
Press, 2004. 
A.  Darwiche. Modefing  and  Reasoning  with  Bayesian  Networks.  Cambridge 
University Press, 2009. 
A.  Darwiche. SDD: A new canonical representation of propositional knowl­
edge bases. In  T. Walsh, editor, 22nd  International  Joint  Conference  on  
Artificial  Intelligence  (IJCAI  2011),  pages 819-826. AAAI Press/IJCAI, 
2011. doi: 10.55911978-1-57735-516-8/IJCAI11-143. 
A.  Darwiche and P. Marquis. A knowledge compilation map. Journal  of  
Artificial  Intelligence  Research,  17:229-264, 2002. doi:10.1613/jair.989. 
A.  S. d' Avila Garcez, M. Gori, L.  C. Lamb, L.  Serafini, M. Spranger, and S. N. 
Tran. Neural-symbolic computing: An effective methodology for princi­
pled integration of machine learning and reasoning. Journal  of Applied  
Logics  - IfCoLog  Journal  of Logics  and  their  Applications,  6( 4):611-632, 
2019. 
J.  Davis and M. Goadrich. The relationship between precision-recall and 
ROC curves. In European  Conference  on  Machine  Learning  (ECML  
2006),  pages 233-240. ACM, 2006. 
J.  Davis, E. S. Bumside, I.  de Castro Dutra, D. Page, and V. S. Costa. An  
integrated approach to learning bayesian networks of rules. In J. Gama, 
R.  Camacho, P. Brazdil, A.  Jorge, and L.  Torgo, editors, European  Confer­
ence  on  Machine  Learning  (ECML  2005),  volume 3720 of LNCS,  pages 
84-95. Springer, 2005. doi:10.1007/11564096_13. 

472 
Bibliography  
L.  De Raedt and A.  Kimmig. Probabilistic (logic) programming concepts. 
Machine  Learning,  100(1):5-47, 2015. doi:10.1007/s10994-015-5494-z. 
L.  De Raedt and I.  Thon. Probabilistic rule learning. In  P. Frasconi and F. A.  
Lisi, editors, 20th  International  Conference  on  Inductive  Logic  Program­
ming  (ILP  2010),  volume 6489 of LNCS,  pages 47-58. Springer, 2011. 
doi: 10.1007 /978-3-642-21295-6_9. 
L.  De Raedt and W. Van Laer. Inductive constraint logic. In 5th  Confer­
ence  on  Algorithmic  Learning  Theory,  volume 997 of LNAI,  pages 80-94. 
Springer, 1995. 
L.  De Raedt, A.  Kimmig, and H. Toivonen. ProbLog: A probabilistic Prolog 
and its application in link discovery. In M. M. Veloso, editor, 20th  Interna­
tional  Joint  Conference  on  Artificial  Intelligence  (IJCAI  2007),  volume 7, 
pages 2462-2467. AAAI Press/IJCAI, 2007. 
L.  De Raedt, B. Demoen, D. Fierens, B. Gutmann, G. Janssens, A.  Kimmig, 
N. Landwehr, T. Mantadelis, W. Meert, R.  Rocha, V. Santos Costa, I.  Thon, 
and J. Vennekens. Towards digesting the alphabet-soup of statistical rela­
tionallearning. In NIPS  2008  Workshop  on  Probabilistic  Programming,  
2008. 
L.  De Raedt, P. Frasconi, K.  Kersting, and S. Muggleton, editors. Probabilis­
tic  Inductive  Logic  Programming,  volume 4911 of LNCS,  2008. Springer. 
ISBN 978-3-540-78651-1. 
L.  De Raedt, K.  Kersting, A.  Kimmig, K.  Revoredo, and H. Toivonen. 
Compressing probabilistic Prolog programs. Machine  Learning,  70(2-3): 
151-168, 2008. doi:10.1007/s10994-007-5030-x. 
R.  de Salvo Braz, E. Amir, and D. Roth. Lifted first-order probabilistic infer­
ence. In  L.  P. Kaelbling and A.  Saffiotti, editors, 19th  International  Joint  
Conference  on  Artificial  Intelligence  (/JCAI  2005),  pages 1319-1325. 
Professional Book Center, 2005. 
A.  Dekhtyar and V.  Subrahmanian. Hybrid probabilistic programs. Journal  of  
Logic  Programming,  43(2):187-250, 2000. 
A.  P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from 
incomplete data via the EM algorithm. Journal  of  the  Royal  Statistical  
Society.  Series  B  (methodological),  39(1):1-38, 1977. 
A.  Dries, A.  Kimmig, W. Meert, J. Renkens, G. Van den Broeck, J. Vlasselaer, 
and L.  De Raedt. ProbLog2: Probabilistic logic programming. In European  
Conference  on  Machine  Learning  and  Principles  and  Practice  of Knowl­
edge  Discovery  in  Databases  (ECMLPKDD  2015),  volume 9286 of LNCS,  
pages 312-315. Springer, 2015. doi:10.1007/978-3-319-23461-8_37. 

Bibliography  473 
D. Dubois and H. Prade. Possibilistic logic: a retrospective and prospective 
view. Fuzzy  Setsand  Systems,  144(1):3-23, 2004. 
D. Dubois, J. Lang, and H. Prade. Towards possibilistic logic programming. 
In 8th  International  Conference  on  Logic  Programming  (ICLP  1991),  
pages 581-595, 1991. 
D. Dubois, J. Lang, and H. Prade. Possibilistic logic. In D. M. Gabbay, C. J. 
Hogger, and J. A.  Robinson, editors, Handbook  of logic  in  artijicial  intel­
ligence  and  logic  programming,vol.  3,  pages 439-514. Oxford University 
Press, 1994. 
S. Dzeroski. Handling imperfect data in inductive logic programming. In 
4th  Scandinavian  Conference  onArtijicial  Intelligence  (SCAI  1993),  pages 
111-125, 1993. 
T. Eiter, G. Ianni, and T. Krennwallner. Answerset programming: A primer. 
InS. Tessaris, E. Franconi, T. Eiter, C. Gutierrez, S. Handschuh, M. Rous­
set, and R.  A.  Schmidt, editors, Reasoning  Web.  Semantic  Technologies  for  
Information  Systems,  5th  International  Summer  School,  volume 5689 of 
LNCS,  pages 40-110. Springer, 2009. doi:10.1007/978-3-642-03754-2_2. 
W.  Faber, G. Pfeifer, and N. Leone. Semantics and complexity of recursive 
aggregates in answer set programming. Artificial  Intelligence,  175(1):278­
298, 2011a. doi:10.1016/j.artint.2010.04.002. 
W.  Faber, G. Pfeifer, and N. Leone. Semantics and complexity of recursive 
aggregates in answer set programming. Artificial  Intelligence,  175(1):278­
298, 2011b. 
F. Fages. Consistency of Clark's completion and existence of stable models. 
Journal  of Methods  of Logic  in  Computer  Science,  1(1):51-60, 1994. 
R.   Fagin and J. Y. Halpem. Reasoning about knowledge and probability. 
Journal  ofthe  ACM,  41(2):340-367, 1994. doi:10.1145/174652.174658. 
D. Fierens, G. Van den Broeck, J. Renkens, D. S. Shterionov, B. Gutmann, 
I.  Thon, G. Janssens, and L. De Raedt. Inference and leaming in probabilis­
tic logic programs using weighted Boolean formulas. Theory  and  Practice  
of Logic  Programming,  15(3):358--401, 2015. 
N. Fuhr. Probabilistic datalog: Implementing logical information retrieval for 
advanced applications. Journal  of  the  American  Society  for  Information  
Science,  51:95-110, 2000. 
H.  Gaifman. Conceming measures in first order calculi. Israel  Journal  of  
Mathematics,  2:1-18, 1964. 
M. Gebser, B. Kaufmann, R.  Kaminski, M. Ostrowski, T. Schaub, and 
M. T. Schneider. Potassco: The Potsdam answer set solving collection. AI  
Commununications,  24(2):107-124, 2011. doi:10.3233/AIC-2011-0491. 

474 
Bibliography  
T.  Gehr, S. Misailovic, and M. Vechev. PSI: exact symbolic inference for 
probabilistic programs. In S. Chaudhuri and A.  Farzan, editors, 28th  Inter­
national  Conference  on  Computer  Aided  Verification  (CAV  20I6),  Part  I,  
volume 9779 of LNCS,  pages 62-83. Springer, 2016. doi:10.1007/978-3­
319-41528-4_4. 
M.  Gelfond and V.  Lifschitz. The stable model semantics for logic pro­
gramming. In 5th  International  Conference  and  Symposium  on  Logic  
Programming  (ICLP/SLP  I988),  volume 88, pages 1070-1080. MIT Press, 
1988. 
G.  Gerla. Fuzzy  Logic,  volume 11 of Trends  in  Logic.  Springer, 2001. 
doi: 10.1007 /978-94-015-9660-2_8. 
V. Gogate and P. M. Domingos. Probabilistic theorem proving. In F. G. Coz­
man and A.  Pfeffer, editors, 27th  International  Conference  on  Uncertainty  
inArtificial  Intelligence  (UAI  2011),  pages 256-265. AUAI Press, 2011. 
Gomes and V. S. Costa. Evaluating inference algorithms for the Prolog 
factor language. In F. Riguzzi and F. Zelezny, editors, 21st  International  
Conference  on  Inductive  Logic  Programming  (ILP  2012),  volume 7842 of 
LNCS,  pages 74-85. Springer, 2012. 
J. Good. A causal calculus (i). The  British  journal  for  the  philosophy  of  
science,  11(44):305-318, 1961. 
oodfellow, Y. Bengio, and A.  Courville. Deep  learning,  volume 1. MIT 
Press, 2016. 
 D. Goodman and J. B. Tenenbaum. Inducing arithmetic functions, 2018. 
http://forestdb.org/models/arithmetic.html, accessed January 5, 2018. 
   Gorlin, C.  R.  Ramakrishnan, and S. A.  Smolka. Model checking with 
probabilistic tabled logic programming. Theory  and  Practice  of  Logic  
Programming,  12(4-5):681-700, 2012. 
Grünwald and J. Y. Halpem. Updating probabilities. Journal  of Artificial  
Intelligence  Research,  19:243-278, 2003. doi:10.1613/jair.1164. 
Gutmann. On  continuous  distributions  and  parameter  estimation  in  prob­
abilistiG  logic  programs.  PhD thesis, Katholieke Universiteit Leuven, 
Belgium, 2011. 
Gutmann, A.  Kimmig, K.  Kersting, and L.  D. Raedt. Parameter learning in 
probabilistic databases: Aleast squares approach. In European  Conference  
on  Machine  Learning  and  Principles  and  Practice  of  Knowledge  Dis­
covery  in  Databases  (ECMLPKDD  2008),  volume 5211 of LNCS,  pages 
473-488. Springer, 2008. 
T.  
I.  
I.  G
N. 
A.
P.  
B. 
B. 

Bibliography  475 
B. Gutmann, A.  Kimmig, K.  Kersting, and L. De Raedt. Parameter estima­
tion in ProbLog from annotated queries. Technical Report CW 583, KU 
Leuven, 2010. 
B. Gutmann, M. Jaeger, and L. De Raedt. Extending problog with continu­
ous distributions. In P. Frasconi and F. A.  Lisi, editors, 20th  International  
Conference  on  Inductive  Logic  Programming  (ILP  2010),  volume 6489 
of LNCS,  pages 76-91. Springer, 2011a. doi:10.1007/978-3-642-21295­
6_12. 
B. Gutmann, I.  Thon, and L. De Raedt. Learning the parameters of proba­
bilistic logic programs from interpretations. In D. Gunopulos, T. Hofmann, 
D. Malerba, and M. Vazirgiannis, editors, European  Conference  on  Ma­
chine  Learning  and  Principles  and  Practice  of  Knowledge  Discovery  in  
Databases  (ECMLPKDD  2011),  volume 6911 of LNCS,  pages 581-596. 
Springer, 2011b. 
B. Gutmann, I.  Thon, A.  Kimmig, M. Bruynooghe, and L. De Raedt. The 
magic of logical inference in probabilistic programming. Theory  and  
Practice  of Logic  Programming,  11(4-5):663-680, 2011c. 
Z. Gyenis, G. Hofer-Szabo, and M. Redei. Conditioning using conditional ex­
pectations: the Borel-Kolmogorov paradox. Synthese,  194(7):2595-2630, 
2017. 
S.  Hadjichristodoulou and D. S. Warren. Probabilistic logic program­
ming with well-founded negation. In D. M. Miller and V.  C. 
Gaudet, editors, 42nd  IEEE  International  Symposium  on  Multiple-Valued  
Logic,  (ISMVL  2012),  pages 232-237. IEEE Computer Society, 2012. 
doi:10.1109/ISMVL.2012.26. 
P. Hajek. Metamathematics  offuzzy  logic,  volume 4. Springer, 1998. 
J. Halpem. Reasoning  About  Uncertainty.  MIT Press, 2003. 
J.  Y.  Halpem. An analysis of first-order logics of probability. Artificial  
Intelligence,  46(3):311-350, 1990. 
A.   C. Harvey. Forecasting,  structural  time  series  models  and  the  Kaiman  
filter.  Cambridge University Press, 1990. 
J.  Herbrand. Recherehes  sur  la  theorie  de  la  demonstration.  PhD thesis, 
Universite de Paris, 1930. 
P.  Hitzier and A.  Seda. Mathematical  Aspects  of  Logic  Programming  Se­
mantics.  Chapman & Hall/CRC Studies in Informatics Series. CRC Press, 
2016. 
A.  Holzinger, J. Schand, M. Schroettner, C. Seifert, and K.  Verspoor. Biomed­
ical text mining: State-of-the-art, open problems and future challenges. 
In A.  Holzinger and I.  Jurisica, editors, Interactive  Knowledge  Discovery  

476 
Bibliography  
and  Data  Mining  in  Biomedical  Informatics,  volume 8401 of LNCS,  pages 
271-300. Springer, 2014. doi:10.1007/978-3-662-43968-5_16. 
J.  Hurd. A formal approach to probabilistic termination. In V. Carrefio, C. A. 
Mufioz, and S. Tahar, editors, 15th  International  Conference  on  Theorem  
Proving  in  Higher  Order  Logics  (TPHOLs  2002),  volume 2410 of LNCS,  
pages 230-245. Springer, 2002. doi:10.1007/3-540-45685-6_16. 
K.  Inoue, T. Sato, M. Ishihata, Y. Kameya, and H.  Nabeshima. Evaluating ab­
ductive hypotheses using an EM algorithm on BDDs. In 21st  International  
Joint  Conference  on  Artificial  Intelligence  (IJCAI  2009),  pages 810-815. 
Morgan Kaufmann Publishers Inc., 2009. 
M. Ishihata, Y. Kameya, T. Sato, and S. Minato. Propositionalizing the EM 
algorithm by BDDs. In Late  Breaking  Papers  of  the  18th  International  
Conference  on  Inductive  Logic  Programming  (ILP  2008),  pages 44-49, 
2008a. 
M. Ishihata, Y. Kameya, T. Sato, and S. Minato. Propositionalizing the EM 
algorithm by BDDs. Technical Report TR08-0004, Dep. of Computer 
Science, Tokyo Institute of Technology, 2008b. 
M. A. Islam.  Inference  and  learning  in  probabilistic  logic  programs  with  
continuous  random  variables.  PhD thesis, State University of New York 
at Stony Brook, 2012. 
M.  A. Islam, C. Ramakrishnan, and I.  Ramakrishnan. Parameter learn­
ing in PRISM programs with continuous random variables. CoRR,  
abs/1203.4287, 2012a. 
M.  A. Islam, C.  Ramakrishnan, and I.  Ramakrishnan. Inference in prob­
abilistic logic programs with continuous random variables. Theory  and  
Practice  of  Logic  Programming,  12:505-523, 2012b. ISSN 1475-3081. 
doi: 10.1017 /S 1471068412000154. 
M.  Jaeger. Reasoning about infinite random structures with relational 
bayesian networks. In A. G. Cohn, L.  K.  Schubert, and S. C. Shapiro, 
editors, 4th  International  Conference  on  Principles  of Knowledge  Repre­
sentation  and  Reasoning,  pages 570-581. Morgan Kaufmann, 1998. 
M. Jaeger and  G. Van den Broeck. Liftability of probabilistic inference: 
Upper and lower bounds. In 2nd  International  Workshop  on  Statistical  
Relational AI (StarAI  2012),  pages 1-8, 2012. 
J.   Jaffar, M. J.  Maher, K.  Marriott, and P. J.  Stuckey. The semantics of 
constraint logic programs. Journal  of Logic  Programming,  37(1-3):1-46, 
1998. doi:10.1016/S0743-1066(98)10002-X. 
T.  Janhunen. Representing normal programs with clauses. In R.  L.  de Man­
taras and L.  Saitta, editors, 16th  European  Conference  on  Artificial  

Bibliography  477 
IntelligenGe  (ECAI  20014),  pages 358-362. lOS Press, 2004. 
C. Jiang, J. Babar, G. Ciardo, A.  S. Miner, and B. Smith. Variable reordering 
in binary decision diagrams. In 26th  International  Workshop  on  LogiG  and  
Synthesis,  pages 1-8, 2017. 
B.  L.  Kaminski, J.-P. Katoen, C. Matheja, and F. Olmedo. Weakest pre­
condition reasoning for expected run-times of probabilistic programs. In  
P. Thiemann, editor, 25th  European  Symposium  on  Programming,  on  Pro­
gramming  Languages  and  Systems  (ESOP  2016),  volume 9632 of INCS,  
pages 364-389. Springer, 2016. doi:10.1007/978-3-662-49498-1_15. 
K.  Kersting and L.  De Raedt. Towards combining inductive logic pro­
gramming with Bayesian networks. In 11th  International  ConferenGe  on  
InduGtive  LogiG  Programming  (ILP  2001),  volume 2157 of LNCS,  pages 
118-131, 2001. 
K.   Kersting and L.  De Raedt. Basic principles of learning Bayesian logic 
programs. In  ProbabilistiG  InduGtive  LogiG  Programming,  volume 4911 of 
INCS,  pages 189-221. Springer, 2008. 
H. Khosravi, 0. Schulte, J. Hu, and T. Gao. Learning compact Markov logic 
networks with decision trees. MaGhine  Learning,  89(3):257-277, 2012. 
T. Khot, S. Natarajan, K.  Kersting, and J. W. Shavlik. Learning Markov Logic 
Networks via functional gradient boosting. In ProGeedings  of  the  11th  
IEEE  International  ConferenGe  on  Data  Mining,  pages 320-329. IEEE, 
2011. 
J. Kietz and M. Lübbe. An  efficient subsumption algorithm for inductive logic 
programming. In  W. W. Cohen and H. Hirsh, editors, 11th  International  
ConferenGe  on  MaGhine  Learning,  pages 130-138. Morgan Kaufmann, 
1994. ISBN 1-55860-335-2. 
D.  M. Kilgour and S. J. Brams. The truel. Mathematics  Magazine,  70(5): 
315-326, 1997. 
A.  Kimmig. A  ProbabilistiG  Prolog  and  its  AppliGations.  PhD thesis, 
Katholieke Universiteit Leuven, Belgium, 2010. 
A.  Kimmig, V. Santos Costa, R. Rocha, B. Demoen, and L.  De Raedt. On the 
efficient execution of ProbLog programs. In 24th  International  ConferenGe  
on  LogiG  Programming  (ICLP  2008),  volume 5366 of INCS,  pages 175­
189. Springer, December 2008. doi:10.1007/978-3-540-89982-2_22. 
A.  Kimmig, B. Demoen, L.  De Raedt, V. S. Costa, and R. Rocha. On the 
implementation of the probabilistic logic programming language ProbLog. 
Theory  and  PraGtiGe  of LogiG  Programming,  11(2-3):235-262, 2011a. 
A.   Kimmig, G. V. den Broeck, and L.  D. Raedt. An  algebraic Prolog for 
reasoning about possible worlds. In W. Burgard and D. Roth, editors, Pro­

478 
Bibliography  
ceedings  of  the  Twenty-Fifth  AAAI  Conference  on  Artificial  Intelligence  
(AAAI  20/1).  AAAI Press, 2011b. 
A.  Kimmig, G. Van den Broeck, and L.  De Raedt. Algebraic model count­
ing. J.  of  Applied  Logic,  22(C):46-62, July 2017. ISSN 1570-8683. 
doi: 10.1016/j.jal.2016.11.031. 
D.  Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv  
preprint  arXiv:14I2.6980,  2014. 
J.  Kisynski and D. Poole. Lifted aggregation in directed first -order probabilis­
tic models. In  C. Boutilier, editor, 21st  International  Joint  Conference  on  
Artificial  Intelligence  (IJCAI  2009),  pages 1922-1929, 2009a. 
J.   Kisynski and D. Poole. Constraint processing in lifted probabilistic infer­
ence. In  J. Bilmes and A.  Y. Ng, editors, 25th  International  Conference  on  
Uncertainty  in  Artificial  Intelligence  (UAI  2009),  pages 293-302. AUAI 
Press, 2009b. 
B.  Knaster and A.  Tarski. Un theoreme sur les fonctions d'ensembles. 
Annales  de  la  Societe  Polonaise  de  Mathematique,  6:133-134, 1928. 
K.   Knopp. Theory  and  Application  of  Infinite  Series.  Dover Books on 
Mathematics. Dover Publications, 1951. 
D. E. Knuth. Two notes on notation. The  American  Mathematical  Monthly,  
99(5):403-422, 1992. 
S.  Kok and P. Domingos. Learning the structure of Markov logic networks. 
In L.  De Raedt and S. Wrobel, editors, 22nd  International  Conference  on  
Machine  learning,  pages 441-448. ACM Press, 2005a. 
S. Kok and P. Domingos. Learning the structure of Markov Logic Networks. 
In 22nd  International  Conference  on  Machine  learning,  pages 441-448. 
ACM, 2005b. 
D.  Koller and N. Friedman. Probabilistic  Graphical  Models:  Principles  
and  Techniques.  Adaptive computation and machine learning. MIT Press, 
Cambridge, MA, 2009. 
E.  Koutsofios, S. North, et al. Drawing graphs with dot. Technical Report 
910904-59113-08TM, AT&T Bell Laboratories, 1991. 
R.  A.  Kowalski. Predicate logic as programming language. In IFIP  Congress,  
pages 569-574, 1974. 
R.   A.  Kowalski and M. J.  Sergot. A logic-based calculus of events. New  
Generation  Computing,  4(1):67-95, 1986. doi:10.1007/BF03037383. 

Bibliography  479 
N. Kumar, 0.  Kuzelka, and L.  De Raedt. Learning distributional programs for 
relational autocompletion. Theory  and  Practice  of Logic  Programming,  22 
(1 ):81-114, 2022. doi: 10.1017 /S 1471068421000144. 
T. Lager. Spaghetti and HMMeatballs, 2018. 
https ://web.archive.org/web/20 15061901351 Olhttp://www.ling.gu.se/ 
-lager/Spaghetti/spaghetti.html, accessed June 14, 2018, snapshot at 
the Internet Archive from June 6, 2015 of http://www.ling.gu.se/-lager/ 
Spaghetti/spaghetti.html, no more accessible. 
L.   J. Layne and S. Qiu. Prediction for compound activity in large drug 
datasets using efficient machine leaming approaches. In M. Khosrow­
Pour, editor, International  Conference  of  the  Information  Resources  
Management  Association,  pages 57-61. Idea Group Publishing, 2005. 
doi: 10.4018/978-1-59140-822-2.ch014. 
Y.  LeCun. The mnist database of handwritten digits. http://yann.  lecun.  
comlexdb/mnist/,  1998. 
N. Leone, G. Pfeifer, W. Faber, T. Eiter, G. Gottlob, S. Perri, and 
F. Scarcello. The DLV system for knowledge representation and reason­
ing. ACM  Transactions  on  Computational  Logic,  7(3):499-562, 2006. 
doi:10.1145/1149114.1149117. 
H. Li,  K.  Zhang, and T. Jiang. The regularized em algorithm. In AAAI,  pages 
807-812, 2005. 
J. W. Lloyd. Foundations ofLogic Programming,  2nd Edition.  Springer, 1987. 
ISBN 3-540-18199-7. 
T. Lukasiewicz. Probabilistic description logic programs. In  L.  Godo, editor, 
Symbolic  and  Quantitative  Approaches  to  Reasoning  with  Uncertainty,  
8th  European  Conference,  ECSQARU  2005,  Barcelona,  Spain,  July  6-8,  
2005,  Proceedings,  volume 3571 ofLNCS,  pages 737-749. Springer, 2005. 
doi:10.1007/11518655_62. 
T.  Lukasiewicz. Probabilistic description logic programs. Interna­
tional  Journal  of  Approximate  Reasoning,  45(2):288-307, 2007. 
doi: 10.10 16/j.ijar.2006.06.012. 
R.  Manhaeve, S. Dumancic, A.  Kimmig, T. Demeester, and L.  De 
Raedt. 
DeepProbLog: 
Neural 
probabilistic 
logic 
programming. 
In  S. 
Bengio, H. 
M. 
Wallach, 
H. 
Larochelle, 
K.  Grauman, 
N. Cesa-Bianchi, and R. Gamett, editors, Advances  in  Neural  
Information  Processing  Systems  3I  (NeuriPS  2018),  pages 3753­
3763, 
2018. 
URL 
https://proceedings.neurips.cc/paper/2018/hash/ 
dc5d637ed5e62c36ecb73b654b05ba2a-Abstract.html. 

480 
Bibliography  
R.  Manhaeve, S. Dumancic, A.  Kimmig, T. Demeester, and L.  De Raedt. 
Neural probabilistic logic programming in DeepProbLog. Artijicial  Intel­
ligence,  298:103504, 2021. doi:10.1016/j.artint.2021.103504. 
T. Mantadelis and G. Janssens. Dedicated tabling for a probabilistic setting. 
In M. V.  Hermenegildo and T. Schaub, editors, Technical  Communications  
ofthe  26th  International  Conference  on  Logic  Programming  (ICLP  2010),  
volume 7 of LIPics,  pages 124-133. Schloss Dagstuhl- Leibniz-Zentrum 
fuer Informatik, 2010. doi: 10.4230/LIPics.ICLP.2010.124. 
D. D. Maua and F.  G. Cozman. Complexity results for probabilistic answer 
set programming. International  Journal  of Approximate  Reasoning,  118: 
133-154, 2020. doi:10.1016/j.ijar.2019.12.003. 
J.  McDermott and R.  S. Forsyth. Diagnosing a disorder in a clas­
sification benchmark. Pattern  Recognition  Letters,  73:41-43, 2016. 
doi: 10.1016/j.patrec.2016.01.004. 
S. Michels. Hybrid  Probabilistic  Logics:  TheoreticalAspects,  Algorithms  and  
Experiments.  PhD thesis, Radboud University Nijmegen, 2016. 
S. Michels, A.  Hommersom, P. J. F.  Lucas, M. Velikova, and P. W. M. Koop­
man. Inference for a new probabilistic constraint logic. In F.  Rossi, editor, 
IJCAI  2013,  Proceedings  of  the  23rd  International  Joint  Conference  on  
Artificial  Intelligence,  Beijing,  China,  August  3-9,  2013,  pages 2540-2546. 
AAAI Press/IJCAI, 2013. 
S.  Michels, A.  Hommersom, P. J. F.  Lucas, and M. Velikova. A new 
probabilistic constraint logic programming language based on a gen­
eralised distribution semantics. Artificial  Intelligence,  228:1-44, 2015. 
doi: 10.1016/j.artint.2015.06.008. 
S. Michels, A.  Hommersom, and P. J. F.  Lucas. Approximate probabilistic in­
ference with bounded error for hybrid probabilistic logic programming. In 
S. Kambhampati, editor, 25th  International  Joint  Conference  on  Artijicial  
Intelligence  (/JCAI  2016),  pages 3616-3622. AAAI Press/IJCAI, 2016. 
B.  Milch, L.  S. Zettlemoyer, K.  Kersting, M. Haimes, and L.  P. Kaelbling. 
Lifted probabilistic inference with counting formulas. In D. Fox and C. P. 
Gomes, editors, 23rd  AAAI  Conference  on  Artijicial  Intelligence  (AAAI  
2008),  pages 1062-1068. AAAI Press, 2008. 
T. M. Mitchell. Machine  learning.  McGraw Hill series in computer science. 
McGraw-Hill, 1997. ISBN 978-0-07-042807-2. 
P.  Morettin, A.  Passerini, and R.  Sebastiani. Efficient weighted model in­
tegration via SMT -based predicate abstraction. In C. Sierra, editor, 26th  
International  Joint  Conference  on  Artificial  Intelligence  (IJCAI  2017),  
pages 720--728. IJCAI, 2017. doi:10.24963/ijcai.2017/100. 

Bibliography  481 
P.  Morettin, P. Zuidberg Dos Martires, S. Kolb, and A. Passerini. Hy­
brid probabilistic inference with logical and algebraic constraints: a 
survey. In Z. Zhou, editor, 30th  International  Joint  Conference  on  
Artijicial  Intelligence  (IJCAI  2021),  pages 4533-4542. UCAI, 2021. 
doi: 10.24963/ijcai.2021/617. 
S. Muggleton. Inverse entailment and Progol. New  Generation  Computing,  
13:245-286, 1995. 
S. Muggleton. Learning stochastic logic programs. Electronic  Transaction  on  
Artijicial  Intelligence,  4(B):141-153, 2000a. 
S. Muggleton. Learning stochastic logic programs. In L.  Getoor and 
D. Jensen, editors, Learning  Statistical  Models  from  Relational  Data,  
Papers  from  the  2000  AAAI  Workshop,  volume WS-00-06 of AAAI  Work­
shops,  pages 36-41. AAAI Press, 2000b. 
S. Muggleton. Learning structure and parameters  of stochastic logic pro­
grams. In S. Matwin and C.  Sammut, editors, 12th  International  Confer­
ence  on  Inductive  Logic  Programming  (ILP  2002),  volume 2583 of LNCS,  
pages 198-206. Springer, 2003. doi:10.1007/3-540-36468-4_13. 
S. Muggleton, J. C. A. Santos, and A. Tamaddoni-Nezhad. Toplog: ILP using 
a logic program declarative bias. In M. G. de la Banda and E. Pontelli, ed­
itors, 24th  International  Conference  on  Logic  Programming  (ICLP  2008),  
volume 5366 of LNCS,  pages 687-692. Springer, 2008. doi:10.1007/978­
3-540-89982-2_58. 
S. Muggleton et al. Stochastic logic programs. Advances  in  inductive  logic  
programming,  32:254--264, 1996. 
C.  J. Muise, S. A. Mcllraith, J. C. Beck, and E. I.  Hsu. Dsharp: Fast d­
DNNF compilation with sharpSAT. In L.  Kosseim and D. Inkpen, editors, 
25th  Canadian  Conference  on  Artijicial  Intelligence,  Canadian  AI  2012,  
volume 7310 of LNCS,  pages 356-361. Springer, 2012. doi:10.1007/978­
3-642-30353-1_36. 
K.  P. Murphy. Machine  learning:  a  probabilistic  perspective.  The MIT Press, 
2012. 
A. Nampally and  C.  Ramakrishnan. Adaptive MCMC-based inference in 
probabilistic logic programs. arXiv  preprint  arXiv:1403.6036,  2014. 
S. Natarajan, T. Khot, K.  Kersting, B. Gutmann, and J. Shavlik. Gradient­
based boosting for statistical relational learning: The relational depen­
dency network case. Machine  Learning,  86(1):25-56, 2012. 
R.  T. Ng and V.  S. Subrahmanian. Probabilistic logic programming. Informa­
tion  and  Computation,  101(2):150-201, 1992. 

482 
Bibliography  
A.  Nguembang Fadja and F.  Riguzzi. Probabilistic logic programming in ac­
tion. In A.  Holzinger, R.  Goebel, M. Ferri, and V. Palade, editors, Towards  
Integrative  Machine  Learning  and  Knowledge  Extraction,  volume 10344 
of LNCS,  pages 89-116. Springer, 2017. doi:10.1007/978-3-319-69775­
8_5. 
A.  Nguembang Fadja and F.  Riguzzi. Lifted discriminative learning of prob­
abilistic logic programs. Machine  Learning,  108(7):1111-1135, 2019. 
doi: 10.1007 /s10994-018-5750-0. 
A.  Nguembang Fadja, E. Lamma, and F.  Riguzzi. Deep probabilistic logic 
programming. In C. Theil Have and R.  Zese, editors, 4th  International  
Workshop  on  Probabilistic  Logic  Programming  (PLP  2017),  volume 1916 
of CEUR-WS,  pages 3-14. Sun SITE Central Europe, 2017. 
A.  Nguembang Fadja, F.  Riguzzi, and E. Lamma. Learning hierarchical 
probabilistic logic programs. Machine  Learning,  110(7):1637-1693, 2021. 
doi: 10.1007 /s 10994-021-06016-4. 
P.  Nicolas, L.  Garcia, I.  Stephan, and C. Lerevre. Possibilistic uncertainty 
handling for answer set programming. Annals of Mathematics  and Artifi­
cial  Intelligence,  47(1-2):139-181, 2006. 
J. C. Nieves, M. Osorio, and U. Cortes. Semantics forpossibilistic disjunctive 
programs. In 9th  International  Conference  on  Logic  Programming  and  
Non-monotonie  Reasoning  (LPNMR  2007),  volume 4483 of LNCS,  pages 
315-320. Springer, 2007. 
N. J. Nilsson. Probabilistic logic. Artificial  Intelligence,  28(1):71-87, 1986. 
M.  Nishino, A.  Yamamoto, and M. Nagata. A sparse parameter learning 
method for probabilistic logic programs. In Statistical  Relational  Artificial  
Intelligence,  Papersfrom  the  2014  AAAI  Workshop,  volume WS-14-13 of 
AAAI  Workshops.  AAAI Press, 2014. 
D. Nitti, T. De Laet, and L.  De Raedt. Probabilistic logic programming for hy­
brid relational domains. Machine  Learning,  103(3):407--449, 2016. ISSN 
1573-0565. doi: 10.1007 /s10994-016-5558-8. 
J.  Nivre. Logic programming tools for probabilistic part-of-speech tagging. 
Master thesis, School of Mathematics and Systems Engineering, Växjö 
University, October 2000. 
J.  Nocedal. Updating Quasi-Newton matrices with limited storage. Mathe­
matics  ofComputation,  35(151):773-782, 1980. 
M.  Osorio and J. C.  Nieves. Possibilistic well-founded semantics. In 8th  
Mexican  International International  Conference  on  Artificial  Intelligence  
(MICAI  2009),  volume 5845 of LNCS,  pages 15-26. Springer, 2009. 

Bibliography  483 
A.  Paes, K.  Revoredo, G. Zaverucha, and V.  S. Costa. Probabilistic first­
order theory revision from examples. In S. Kramer and B. Pfahringer, 
editors, 15th  International  ConferenGe  on  lnduGtive  LogiG  Programming  
(JLP  2005),  volume 3625 of LNCS,  pages 295-311. Springer, 2005. 
doi: 10.1007 /11536314_18. 
A.  Paes, K.  Revoredo, G. Zaverucha, and V.  S. Costa. PFORTE: re­
vising probabilistic FOL theories. In J. S. Sichman, H. Coelho, and 
S. 0.  Rezende, editors, 2nd  International  Joint  ConferenGe,  1Oth  lbero­
AmeriGan  ConferenGe  on  Al,  18th  Brazilian  AI  Symposium,  JBERAMIA­
SBJA  2006,  volume 4140 of LNCS,  pages 441-450. Springer, 2006. 
doi:10.1007/11874850_ 48. 
L.   Page, S. Brin, R.  Motwani, and T. Winograd. The PageRank citation 
ranking: Bringing order to the web. Technical report, Stanford InfoLab, 
1999. 
C. Papadimitriou and K.  Steiglitz. Combinatorial  Optimization:  Algorithms  
and  Complexity.  Dover Books on Computer Science. Dover Publications, 
1998. ISBN 9780486402581. 
J. Pearl. ProbabilistiG  Reasoning  in  Intelligent  Systems:  Networks  of Plausi­
ble  InferenGe.  Morgan Kaufmann, 1988. 
Y. Perov, B. Paige, and F. Wood. The Indian GPA problem, 2017. 
https://bitbucket.org/probprog/anglican-examples/src/master/worksheets/ 
indian-gpa.clj, accessed June 1, 2018. 
A.   Pfeffer. PraGtiGal  ProbabilistiG  Programming.  Manning Publications, 
2016. ISBN 9781617292330. 
G.  D. Plotkin. A note on inductive generalization. In MaGhine  lntelligenGe,  
volume 5, pages 153-163. Edinburgh University Press, 1970. 
D.  Poole. Probabilistic Horn abduction and Bayesian networks. ArtijiGial  
IntelligenGe,  64(1):81-129, 1993a. 
D.  Poole. Logic programming, abduction and probability - a top-down 
anytime algorithm for estimating prior and posterior probabilities. New  
Generation  Computing,  11(3):377-400, 1993b. 
D. Poole. The Independent Choice Logic formodeHing multiple agents under 
uncertainty. ArtijiGial  IntelligenGe,  94:7-56, 1997. 
D.  Poole. Abducing through negation as failure: Stable models within the 
independent choice logic. Journal  of LogiG  Programming,  44(1-3):5-35, 
2000. doi: 10.1016/S0743-1066(99)00071-0. 
D.  Poole. First-order probabilistic inference. In G. Gottlob and T. Walsh, 
editors, JJCAI-03,  ProGeedings  of  the  Eighteenth  International  Joint  

484 
Bibliography  
Conference  on  Artijicial  Intelligence,  Acapulco,  Mexico,  August  9-15,  
2003,  pages 985-991. Morgan Kaufmann Publishers, 2003. 
D. Poole. The independent choice logic and beyond. In L.  De Raedt, P. Fras­
coni, K.  Kersting, and S. Muggleton, editors, Probabilistic  Inductive  Logic  
Programming,  volume 4911 of LNCS,  pages 222-243. Springer, 2008. 
T.  C. Przymusinski. Perfeet model semantics. In R. A.  Kowalski and K.  A.  
Bowen, editors, 5th  International  Conference  and  Symposium  on  Logic  
Programming  (ICLP/SLP  1988),  pages 1081-1096. MIT Press, 1988. 
T.  C.  Przymusinski. Every logic program has a natural stratification and an 
iterated least fixed point model. In Proceedings  of the  8th  ACM  SIGACT­
SIGMOD-SIGART Symposium  on  Principles  of Database  Systems  ( PODS­
1989),  pages 11-21. ACM Press, 1989. 
J. R. Quinlan. Learning logical definitions from relations. Machine  Learning,  
5:239-266, 1990. doi:10.1007/BF00117105. 
L.  R. Rabiner. A tutorial on hidden Markov models and selected applications 
in speech recognition. Proceedings  ofthe  IEEE,  77(2):257-286, 1989. 
L.  D. Raedt, A.  Dries, I.  Thon, G. V. den Broeck, and M. Verbeke. Inducing 
probabilistic relational rules from probabilistic examples. In Q. Yang and 
M. Wooldridge, editors, 24th  International  Joint  Conference  on  Artificial  
Intelligence  (IJCAI  2015),  pages 1835-1843. AAAI Press, 2015. 
I.  Razgon. On OBDDs for CNFs of bounded treewidth. In C.  Baral, G. D. 
Giacomo, and T. Eiter, editors, 14th  International  Conference  on  Princi­
ples  of Knowledge  Representation  and  Reasoning  (KR  2014).  AAAI Press, 
2014. 
J. Renkens, G. Van den Broeck, and S. Nijssen. k-optimal: a novel approxi­
mate inference algorithm for ProbLog. Machine  Learning,  89(3):215-231, 
2012. doi: 10.1007 /s10994-012-5304-9. 
J. Renkens, A.  Kimmig, G. Van den Broeck, and L.  De Raedt. Explanation­
based approximate weighted model counting for probabilistic logics. In 
Proceedings  of  the  Twenty-Eighth  AAAI  Conference  on  Artijicial  In­
telligence,  July  27  -31,  2014,  Quebec  City,  Quebec,  Canada,  pages 
2490-2496. AAAI Press, 2014. 
K.  Revoredo and G. Zaverucha. Revision of first-order Bayesian classifiers. 
In S. Matwin and C. Sammut, editors, 12th  International  Conference  on  
Inductive  Logic  Programming  (ILP  2002),  volume 2583 of LNCS,  pages 
223-237. Springer, 2002. doi:10.1007/3-540-36468-4_15. 
F. Riguzzi. Learning logic programs with annotated disjunctions. In A.  Srini­
vasan and R. King, editors, 14th  International  Conference  on  Inductive  

Bibliography  485 
Logic  Programming  (ILP  2004),  volume 3194 of LNCS,  pages 270-287. 
Springer, Sept. 2004. doi:10.1007/978-3-540-30109-7_21. 
F. Riguzzi. A top down interpreter for LPAD and CP-logic. In 10th  Congress  
of the  Italian  Association  for  Artificial  Intelligence,  (AI*IA  2007),  volume 
4733 of LNAI,  pages 109-120. Springer, 2007a. doi:10.1007/978-3-540­
74782-6_11. 
F.  Riguzzi. ALLPAD: Approximate learning of logic programs with anno­
tated disjunctions. In S. Muggleton and R.  Otero, editors, 16th  Interna­
tional  Conference  on  Inductive  Logic  Programming  (ILP  2006),  volume 
4455 of LNAI,  pages 43-45. Springer, 2007b. doi:10.1007/978-3-540­
73847-3_11. 
F.  Riguzzi. Inference with logic programs with annotated disjunctions un­
der the well founded semantics. In 24th  International  Conference  on  
Logic  Programming  (ICLP  2008),  volume 5366 of LNCS,  pages 667-771. 
Springer, 2008a. doi: 10.1007 /978-3-540-89982-2_54. 
F.  Riguzzi. ALLPAD: Approximate learning of logic programs with 
annotated disjunctions. Machine  Learning,  70(2-3):207-223, 2008b. 
doi: 10.1007 /s1 0994-007-5032-8. 
F.  Riguzzi. Extended semantics and inference for the independent 
choice logic. Logic  Journal  of  the  IGPL,  17(6):589-629, 2009. 
doi:10.1093/jigpalljzp025. 
F.  Riguzzi. SLGAD resolution for inference on logic programs with an­
notated disjunctions. Fundamenta  Informaticae,  102(3-4):429-466, Oct. 
2010. doi:l0.3233/FI-2010-392. 
F.  Riguzzi. MCINTYRE: A Monte Carlo system for probabilistic 
logic programming. Fundamenta  Informaticae,  124(4):521-541, 2013. 
doi:10.3233/FI-2013-847. 
F.  Riguzzi. Speeding up inference for probabilistic logic programs. The  
Computer  Journal,  57(3):347-363, 2014. doi: 10.1093/comjnllbxt096. 
F.  Riguzzi. The distribution semantics for normal programs with function 
symbols. International  Journal  of Approximate  Reasoning,  77:1-19,2016. 
doi: 10.10 16/j.ijar.20 16.05.005. 
F.  Riguzzi and N. Di Mauro. Applying the information bottleneck to 
statistical relational learning. Machine  Learning,  86(1):89-114, 2012. 
doi:10.1007/s10994-011-5247-6. 
F.  Riguzzi and T. Swift. Tabling and answer subsumption for reasoning on 
logic programs with annotated disjunctions. In Technical  Communications  
ofthe  26th  International  Conference  on  Logic  Programming  (ICLP  2010),  

486 
Bibliography  
volume 7 of LIPics,  pages 162-171. Schloss Dagstuhl- Leibniz-Zentrum 
fuer Informatik, 2010. doi:10.4230/LIPics.ICLP.2010.162. 
F. Riguzzi and T. Swift. The PITA system: Tabling and answer subsumption 
for reasoning under uncertainty. Theory  and  Practice  of Logic  Program­
ming,  11(4-5):433--449, 2011. doi:10.1017/S147106841100010X. 
F.  Riguzzi and T. Swift. Well-definedness and efficient inference for 
probabilistic logic programming under the distribution semantics. 
Theory  and  Practice  of  Logic  Programming,  13(2):279-302, 2013. 
doi: 10.1017/S1471068411000664. 
F. Riguzzi and T. Swift. Terminating evaluation of logic programs with finite 
three-valued models. ACM  Transactions  on  Computational  Logic,  15(4): 
32:1-32:38, 2014. ISSN 1529-3785. doi:10.1145/2629337. 
F.  Riguzzi and T. Swift. Probabilistic logic programming under the dis­
tribution semantics. In M. Kifer and Y. A. Liu, editors, Declarative  
Logic  Programming:  Theory,  Systems,  and  Applications.  Association for 
Computing Machinery and Morgan & Claypool, 2018. 
F. Riguzzi, E. Bellodi, and R. Zese. A history of probabilistic inductive logic 
programming. Frontiers  in  Robotics  and  AI,  1(6), 2014. ISSN 2296-9144. 
doi: 1 0.3389/frobt.20 14.00006. 
F. Riguzzi, E. Bellodi, E. Lamma, R. Zese, and G. Cota. Probabilistic logic 
programming on the web. Software:  Practice  and  Experience,  46(10): 
1381-1396, 10 2016a. doi:10.1002/spe.2386. 
F.  Riguzzi, E. Bellodi, R. Zese, G. Cota, and E. Lamma. Scaling struc­
ture learning of probabilistic logic programs by MapReduce. In M. Fox 
and G. Kaminka, editors, 22nd  European  Conference  on  Artificial  Intel­
ligence  (ECAI  2016),  volume 285 of Frontiers  in  Artificial  Intelligence  
and  Applications,  pages 1602-1603. lOS Press, 2016b. doi:10.3233/978­
1-61499-672-9-1602. 
F. Riguzzi, E. Bellodi, R. Zese, G. Cota, and E. Lamma. A survey of lifted 
inference approaches for probabilistic logic programming under the dis­
tribution semantics. International  Journal  of Approximate  Reasoning,  80: 
313-333, 1 2017a. doi:10.1016/j.ijar.2016.10.002. 
F. Riguzzi, E. Lamma, M. Alberti, E. Bellodi, R. Zese, and G. Cota. Proba­
bilistic logic programming for naturallanguage processing. In F. Chesani, 
P. Mello, and M. Milano, editors, Workshop  on  Deep  Understanding  and  
Reasoning,  URANIA  2016,  volume 1802 of CEUR  Workshop  Proceedings,  
pages 30-37. Sun SITE Central Europe, 2017b. 
J. A. Robinson. A machine-oriented logic based on the resolution principle. 
Journal  ofthe  ACM,  12(1):23-41, 1965. doi:10.1145/321250.321253. 

Bibliography  487 
B. Russen. Mathematical logic  as based on the theory of types. In J. van 
Heikenoort, editor, From  Frege  to  Godel,  pages 150-182. Harvard Univ. 
Press, 1967. 
T. P. Ryan. Modern  Engineering  Statistics.  John Wiley & Sons, 2007. 
V.   Santos Costa, R.  Rocha, and L.  Damas. The YAP Prolog system. Theory  
and  Practice  of Logic  Programming,  12(1-2):5-34, 2012. 
T.  Sato. A statistical learning method for logic programs with distribution 
semantics. In L.  Sterling, editor, Logic  Programming,  Proceedings  of the  
Twelfth  International  Conference  on  Logic  Programming,  Tokyo,  Japan,  
June  13-16,  1995,  pages 715-729. MIT Press, 1995. 
T. Sato and Y. Kameya. PRISM: a language for symbolic-statistical modeling. 
In  15th  International  Joint  Conference  on  Artificial  Intelligence  (IJCAI  
1997),  volume 97, pages 1330-1339, 1997. 
T. Sato and Y. Kameya. Parameter learning of logic programs for symbolic­
statistical modeling. Journal  of Artificial  Intelligence  Research,  15:391­
454,2001. 
T. Sato and Y. Kameya. New advances in logic-based probabilistic modeling 
by PRISM. In L.  De Raedt, P. Frasconi, K.  Kersting, and S. Muggle­
ton, editors, Probabilistic  Inductive  Logic  Programming  - Theory  and  
Applications,  volume 4911 of LNCS,  pages 118-155. Springer, 2008. 
doi: 10.1007 /978-3-540-78652-8_5. 
T.  Sato and K.  Kubota. Viterbi training in PRISM. Theory  and  Practice  of  
Logic  Programming,  15(02):147-168, 2015. 
T.  Sato and P. Meyer. Tabling for infinite probability computation. In  
A.  Dovier and V. S. Costa, editors, Technical  Communications  of  the  
28th  International  Conference  on  Logic  Programming  (ICLP  2012),  vol­
ume 17 of L/Plcs,  pages 348-358. Schloss Dagstuhl - Leibniz-Zentrum 
fuer Informatik, 2012. 
T. Sato and P. Meyer. Infinite probability computation by cyclic explanation 
graphs. Theory  and  Practice  ofLogic  Programming,  14:909-937, 11 2014. 
ISSN 1475-3081. doi:10.1017/S1471068413000562. 
T. Sato, Y. Kameya, and K.  Kurihara. Variational Bayes via propositionalized 
probability computation in PRISM. Annals of Mathematics  and Artificial  
Intelligence,  54(1-3): 135-158, 2008. 
T.  Sato, N.-F. Zhou, Y.  Kameya, Y. Izumi, K.  Kubota, and R.  Kojima. 
PRISM User's Manual (Version 2.3), 2017. http://rjida.meijo-u.ac.jp/ 
prism/download/prism23.pdf, accessed June 8, 2018. 
0. Schulte and H. Khosravi. Learning graphical models for relational data via 
lattice search. Machine  Learning,  88(3):331-368, 2012. 

488 
Bibliography  
0.  Schulte and K.  Routley. Aggregating predictions vs. aggregating fea­
tures for relational classification. In  IEEE  Symposium  on  Computational  
Intelligence  and  Data  Mining  (CIDM  2014),  pages 121-128. IEEE, 2014. 
G. Schwarz. Estimating the dimension of a model. The  Annals  of Statistics,  6 
(2):461-464, 1978. 
R.   Schwitter. Learning effect axioms via probabilistic logic program­
ming. In R.  Rocha, T. C. Son, C.  Mears, and N. Saeedloei, edi­
tors, Technical  Communications  of  the  33rd  International  Conference  
on  Logic  Programming  (ICLP  2017),  volume 58 of OASICS,  pages 
8:1-8:15. Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, 2018. 
doi: 10.4230/0ASics.ICLP.2017.8. 
P. Sevon, L.  Eronen, P. Hintsanen, K.  Kulovesi, and H. Toivonen. Link discov­
ery in graphs derived from biological databases. In International  Workshop  
on  Data  Integration  in  the  Life  Sciences,  volume 4075 of LNCS,  pages 
35-49. Springer, 2006. 
G. Shafer. A  Mathematical  Theory  of Evidence.  Princeton University Press, 
1976. 
D. S. Shterionov, J. Renkens, J. Vlasselaer, A.  Kimmig, W. Meert, and 
G. Janssens. The mostprobable explanation for probabilistic logic pro­
grams with annotated disjunctions. In  J. Davis and J. Ramon, editors, 24th  
International  Conference  on  Inductive  Logic  Programming  (ILP  2014),  
volume 9046 of LNCS,  pages 139-153, Berlin, Heidelberg, 2015. Springer. 
doi: 10.1007 /978-3-319-23708-4_10. 
P. Singla and P. Domingos. Discriminative training ofMarkov logic networks. 
In 20th  National  Conference  on  Artificial  Intelligence  (AAAI  2005),  pages 
868-873. AAAI Press/The MIT Press, 2005. 
F.  Somenzi. Efficient manipulation of decision diagrams. Interna­
tional  Journal  on  Software  Tools  for  Technology  Transfer,  3(2):171­
181, 2001. doi:10.1007/s100090100042. URL https://doi.org/10.1007/ 
s100090100042. 
F. Somenzi. CUDD:  CU  Decision  Diagram  Package  Release  3.0.0.  Univer­
sity of Colorado, 2015. URL http://vlsi.colorado.edu/-fabio/CUDD/cudd. 
pdf. 
A.  Srinivasan. The aleph manual, 2007. http://www.cs.ox.ac.uk/activities/ 
machlearn/Aleph/aleph.html, accessed April3, 2018. 
A.  Srinivasan, S. Muggleton, M. J. E. Stemberg, and R.  D. King. Theories for 
mutagenicity: A study in first-order and feature-based induction. Artificial  
Intelligence,  85(1-2):277-299, 1996. 

Bibliography  489 
A.  Srinivasan, R.  D. King, S. Muggleton, and M. J. E. Stemberg. Carcino­
genesis predictions using ILP. In N. Lavrac and S. Dzeroski, editors, 7th  
International  Workshop  on  Inductive  Logic  Programming,  volume 1297 of 
INCS,  pages 273-287. Springer Berlin Heidelberg, 1997. 
S. Srivastava.  A  Course  on  Borel  Sets.  Graduate Texts in Mathematics. 
Springer, 2013. 
L.  Steen and J. Seebach. Counterexamples  in  Topology.  Dover Books on 
Mathematics. Dover Publications, 2013. 
L.   Sterling and E. Shapiro. The  Art  of  Prolog:  Advanced  Programming  
Techniques.  Logic programming. MIT Press, 1994. ISBN 9780262193382. 
C. Stolle, A.  Karwath, and L.  De Raedt. Cassic 'cl:  An integrated ILP system. 
In  A.  Hoffmann, H. Motoda, and T. Scheffer, editors, 8th  International  
Conference  on  Discovery  Science  ( DS  2005 ),  volume 3735 of INCS,  pages 
354-362. Springer, 2005. 
T.  Swift and D. S. Warren. XSB: Extending prolog with tabled logic pro­
gramming. Theory  and  Practice  of Logic  Programming,  12(1-2):157-187, 
2012. doi: 10.1017/Sl471068411000500. 
T.  Sytjänen and I.  Niemelä. The Smodels system. In T. Eiter, W. Faber, 
and M. Truszczynski, editors, 6th  International  Conference  on  Logic  Pro­
gramming  and  Non-Monotonie  Reasoning  (LPNMR  2001),  volume 2173 
of INCS.  Springer, 2001. doi:l0.1007/3-540-45402-0_38. 
N.  Taghipour, D. Fierens, J. Davis, and H. Blockeel. Lifted variable elimi­
nation: Decoupling the operators from the constraint language. Journal  of  
Artificial  Intelligence  Research,  47:393-439, 2013. 
A.  Tarski. A lattice-theoretical fixpoint theorem and its applications. Pacific  
Journal  of Mathematics,  5(2):285-309, 1955. 
Y.  W. Teh. Dirichlet process. In Encyclopedia  of  machine  learning,  pages 
280-287. Springer, 2011. 
A.  Thayse, M. Davio, and J. P. Deschamps. Optimization of multivalued 
decision algorithms. In 8th  International  Symposium  on  Multiple-Valued  
Logic,  pages 171-178. IEEE Computer Society Press, 1978. 
I.   Thon, N. Landwehr, and L.  D. Raedt. A simple model for sequences of 
relational state descriptions. In European  conference  on  Machine  Learning  
and  Knowledge  Discovery  in  Databases,  volume 5212 of INCS,  pages 
506-521. Springer, 2008. ISBN 978-3-540-87480-5. 
D. Tran, A.  Kucukelbir, A.  B. Dieng, M. Rudolph, D. Liang, and D. M. Blei. 
Edward: A library for probabilistic modeling, inference, and criticism. 
arXiv  preprint  arXiv:l610.09787,  2016. 

490 
Bibliography  
C. Turliuc, L.  Dickens, A.  Russo, and K.  Broda. Probabilistic abductive logic 
programming using Dirichlet priors. International  Journal  of Approximate  
Reasoning,  78:223-240, 2016. doi:10.1016/j.ijar.2016.07.001. 
G. Van den Broeck. On the completeness of first-order knowledge compila­
tion for lifted probabilistic inference. In J. Shawe-Taylor, R.  S. Zemel, P. L.  
Bartlett, F. C. N. Pereira, and K.  Q. Weinberger, editors, Advances  in  Neu­
ral  Information  Processing  Systems  24  (NIPS  2011),  pages 1386-1394, 
2011. 
G.  Van den Broeck. Lifted  Inference  and  Learning  in  Statistical  Relational  
Models.  PhD thesis, Ph. D. Dissertation, KU Leuven, 2013. 
G. Van den Broeck, I.  Thon, M. van Otterlo, and L.  De Raedt. DTProbLog: 
A decision-theoretic probabilistic Prolog. In M. Fox and D. Poole, ed­
itors, Proceedings  of  the  Twenty-Fourth  AAAI  Conference  on  Artificial  
Intelligence,  pages 1217-1222. AAAI Press, 2010. 
G. Van den Broeck, N. Taghipour, W. Meert, J. Davis, and L.  De Raedt. Lifted 
probabilistic inference by first-order knowledge compilation. In T. Walsh, 
editor, 22nd  International  Joint  Conference  on  Artificial  Intelligence  
(IJCAI  2011),  pages 2178-2185. IJCAIIAAAI, 2011. 
G. Van den Broeck, W. Meert, and A.  Darwiche. Skolemization for weighted 
first-order model counting. In C. Baral, G. D. Giacomo, and T. Eiter, 
editors, 14th  International  Conference  on  Principles  of  Knowledge  Rep­
resentation  and  Reasoning  (KR  2014),  pages 111-120. AAAI Press, 
2014. 
A.  Van Gelder, K.  A.  Ross, and J. S. Schlipf. The well-founded semantics for 
generallogic programs. Journal  ofthe  ACM,  38(3):620-650, 1991. 
J. Vennekens and S. Verbaeten. Logic programs with annotated disjunctions. 
Technical Report CW386, KU Leuven, 2003. 
J.  Vennekens, S. Verbaeten, and M. Bruynooghe. Logic programs with 
annotated disjunctions. In  B. Demoen and V. Lifschitz, editors, 20th  Inter­
national  Conference  on  Logic  Programming  (ICLP  2004),  volume 3131 
of LNCS,  pages 431-445. Springer, 2004. doi:10.1007/978-3-540-27775­
0_30. 
J.  Vennekens, M. Denecker, and M. Bruynooghe. CP-logic: A lan­
guage of causal probabilistic events and its relation to logic program­
ming. Theory  and  Practice  of  Logic  Programming,  9(3):245-308, 2009. 
doi: 10.1017/S1471068409003767. 
J.  Vlasselaer, J. Renkens, G. Van den Broeck, and L.  De Raedt. Compil­
ing probabilistic logic programs into sentential decision diagrams. In  1st  

Bibliography  491 
International  Workshop  on  Probabilistic  Logic  Programming  (PLP  2014),  
pages 1-10, 2014. 
J.  V1asse1aer, G. Van den Broeck, A.  Kimmig, W. Meert, and L.  De Raedt. 
Anytime inference in probabi1istic 1ogic programs with Tp-compi1ation. 
In  24th  International  Joint  Conference  on  Artificial  Intelligence  (IJCAI  
2015),  pages 1852-1858, 2015. 
J.  V1asse1aer, G. Van den Broeck, A.  Kimmig, W. Meert, and 
L.  De Raedt. Tp-compi1ation for inference in probabi1istic 1ogic pro­
grams. International  Journal  of Approximate  Reasoning,  78:15-32, 2016. 
doi: 10.10 16/j.ijar.20 16.06.009. 
J. Von Neumann. Various techniques used in connection with random digits. 
Nattional  Bureau  of Standard  (U.S.),  Applied  Mathematics  Series,  12:36­
38, 1951. 
W. Y. Wang, K.  Mazaitis, and W. W. Cohen. Structure 1earning via parameter 
1earning. In J. Li,  X. S. Wang, M. N. Garofa1akis, I.  Soboroff, T. Sue1, and 
M. Wang, editors, 23rd  ACM  International  Conference  on  Conference  on  
Information  and  Knowledge  Management,  CIKM  2014,  pages 1199-1208. 
ACM Press, 2014. doi:10.1145/2661829.2662022. 
W. Y. Wang, K.  Mazaitis, N. Lao, and W. W. Cohen. Efficient inference and 
1earning in a 1arge know1edge base. Machine  Learning,  100(1):101-126, 
Ju12015. doi:10.1007/s10994-015-5488-x. 
M. P. Wellman, J. S. Breese, and R.  P. Go1dman. From know1edge bases to 
decision mode1s. The  Knowledge  Engineering  Review,  7(1):35-53, 1992. 
J.  Wie1emaker, T. Schrijvers, M. Triska, and T. Lager. SWI-Pro1og. 
Theory  and  Practice  of  Logic  Programming,  12(1-2):67-96, 2012. 
doi: 10.1017 /S1471068411000494. 
J. Wie1emaker, T. Lager, and F.  Riguzzi. SWISH: SWI-Pro1og for sharing. In  
S. Ellmauthaler and C. Schu1z, editors, International  Workshop  on  User­
Oriented  Logic  Programming  (IULP  2015),  2015. 
J.  Wie1emaker, F.  Riguzzi, B. Kowa1ski, T. Lager, F.  Sadri, and M. Ca1ejo. 
Using SWISH to realise interactive web based tutorials for 1ogic based 
1anguages. Theory  and  Practice  of  Logic  Programming,  19(2):229-261, 
2019. doi: 10.1017/S1471068418000522. 
S. Willard. General  Topology.  Addison-Wes1ey series in mathematics. Dover 
Publications, 1970. 
F.   Wood, J. W. van de Meent, and V. Mansinghka. A new approach to 
probabilistic programming inference. In  17th  International  conference  on  
Artificial  Intelligence  and  Statistics  (AISTAT  2014),  pages 1024-1032, 
2014. 

492 
Bibliography  
N. L.  Zhang and D. Poole. A simple approach to bayesian network compu­
tations. In  1Oth  Canadian  Conference  on  Artijicial  Intelligence,  Canadian  
AI  1994,  pages 171-178, 1994. 
N.  L.  Zhang and D. L.  Poole. Exploiting causal independence in Bayesian 
network inference. Journal  of Artijicial  Intelligence  Research,  5:301-328, 
1996. 
P. Zuidberg Dos Martires, A.  Dries, and L.  De Raedt. Knowledge compilation 
with continuous random variables and its application in hybrid proba­
bilistic logic programming. In  8th  International  Workshop  on  Statistical  
Relational  Al,  (StarAI  2018),  2018. 
P.  Zuidberg Dos Martires, A.  Dries, and L.  De Raedt. Exact and approxi­
mate weighted model integration with probability density functions using 
knowledge compilation. In Proceedings  of the  Thirty-Third  AAAI Confer­
ence  onArtificial  Intelligence  (AAA/-19),  pages 7825-7833. AAAI Press, 
2019. doi: 10.1609/aaai.v33i01.33017825. 
Wielemaker20 19general 

Index  
O"-algebra, 24, 97, 131, 148  
product, 31  
0-subsumption, 352, 356  
#P-complete, 181, 190  
#SAT, 288  
A  
accuracy,344  
acyclic graph, 27  
acyclicicity, 253  
acyclicity, 20, 180  
LPAD, 52  
Adam optimizer, 406  
aggregation parfactor, 247  
Aleph, 368, 380  
algebra, 24, 93  
algebraic decision diagram, 219  
algebraic fact, 289  
algebraic model counting, 302  
ALLPAD, 326, 355  
structure learning problem,  
355  
ancestor, 36  
Anglican, 437  
annotated disjunctive clause, 44,  
68  
Annotated Probabilistic Logic  
Programming, 86  
answer subsumption, 201  
answer set, 6 see  model, stable  
answer set programming, 6, 24,  
172  
answer subsumption, 201  
anytime algorithm, 263, 270  
approximate inference, 255  
aProbLog, 284  
argument sampling, 443  
arithmetic circuit, 209, 306, 346,  
393  
arity, 4  
atom, 4, 45  
atomic choice, 48, 51, 89  
atomic formulas, 4  
atomic normal program, 252  
attach operation, 422  
B  
background predicate, 364  
backpropagation, 348  
backward probability, 335, 340  
base distribution, 430  
Baum-Welch algorithm, 322  
Bayes' theorem, 31  
Bayesian estimation, 437  
Bayesian Information Criterions,  
377  
Bayesian Logic Programs, 74  
Bayesian network, 36, 40, 60  
beam search, 353, 359, 364, 368,  
386  
belief propagation, 42  
beta distribution, 127, 431  
binary decision diagram, 193,  
334,425,469  
493  

494 
Index  
binomial distribution, 127  
BIOMINE network, 57  
bipartite graph, 42  
bivariate Gaussian density, 31, see   
bivariate Gaussian  
distribution  
bivariate normal density, 31 see   
bivariate Gaussian  
distribution  
Borel-Kolmogorov paradox, 32  
bottom clause, 354, 364, 370, 411  
bottom element, 2, 102, 156  
bottom-up parsing, 422  
bottom-up reasoning, 7  
branch-and-bound,258,266  
brave consequence, 23  
Broyden-Fletcher-Goldfarb-Shanno  
algorithm, 383  
c 
C-FOVE, 243  
c2d,207  
Cartesian product, 31, 110  
infinite, 7, 11  
consistent, 100  
causalindependence,242  
causal mechanism, 44  
causal reasoning, 44  
causality, 72  
cautious consequence, 23  
centrallimit theorem, 258  
chain, 35  
chain rule, 35  
chi squared, 363  
child random variable, 32  
Chineserestaurant process, 434  
Choquet capacity, 168  
infinitely monotoner, 168  
Clark's completion, 15, 205, 215,  
249  
classification, 34  
clause, 5  
definite, 5  
normal, 6  
clause body, 126  
clause head, 220  
clique, 39  
closed-world assumption, 204,  
336  
closure under countable union, 98  
CLP(BN), 74  
CLP(R), 142  
collider, 37  
compatibility condition, see   
consistency conditio  
complete binary decision  
diagram, 334  
complete interpretation, 285  
complete lattice, 2, 7, 101  
completeness, 8, 107  
complexity 87, 176, 177, 179  
acyclic and locally stratified  
probabilistic programs,  
180  
answer set programming, 180  
credal semantics, 183  
well-founded semantics, 179  
composite choice, 92  
concentration parameter, 430  
conditional density, 32  
conditional inference, 314  
conditional probability, 31, 48,  
357  
conditional probability table, 61  
deterministic, 36  
conditionally independent random  
variables, 33  

Index  495 
confidence interval, 258  
binomial proportion, 258  
consistency condition, 111  
consistent set of atomic choices,  
48  
constraint 75, 77, 86  
anti-monotonic, 356  
monotonic, 356  
context free grammar, 80  
convergent random variable, 247  
convergentseries,235  
Cora, 229  
countable set, 89, 91, 96  
counting formulas, 244  
coupon collector problem, 451  
covariance matrix, 30, 427  
coveringloop,352,359  
covering set of composite choices,  
95  
covering set of explanations, 94,  
96, 113  
CP-logic, 68  
semantics, 74  
valid theory, 73  
CPL1, 273  
cplint, 194, 315  
continuous variables, 136  
credal semantics, 166  
credal set, 166  
credal sets, 169  
CUDD, 222, 426  
cumulative distribution, 27  
cut, 12  
cyclic explanation graph, 236  
cylinder sets, 110  
D  
d-separation, 36, 41  
Datalog, 7  
decision facts, 275  
decision problem, 176, 177  
decision variable, 211  
decreasing sequence, 3  
deep learning, 344  
DeepProbLog, 344  
program, 346  
default negation, 6  
deleted path, 336  
deletion rule, 336  
depth, 9, 18, 82  
program, 18  
depth bound, 257  
deputy variable, 242  
derivation, 9, 79  
derivation variable, 294  
descendant, 36  
determinant of a matrix, 30  
determinate predicate, 315  
determinate program, 315  
deterministic decomposable  
negation normal form,  
208  
decision, 213  
ordering, 211  
smooth, 207  
strongly deterministic, 214  
structured decomposability,  
214  
deterministic dependency, 36  
diagnosis, 34  
DiceML, 372  
Dirichlet distribution, 126, 408  
Dirichlet process, 430  
discrete topology, 110  
discrete-time stochastic process,  
434  

496 
Index  
disjoint statements, 45  
disjoint sum, 290  
disjoint-sum problem, 291  
distribution semantics, 43, 47, 64,  
91  
Distributional Clauses, 118, 376  
valid program, 120  
DLV, 23  
DPLL, 249  
Dsharp, 207  
DTProbLog, 219  
duel, 447  
dynamic clauses, 263  
dynamic models, 311  
dynamic programming, 188, 194,  
196,222  
dynamically stratification, 20  
E  
Edward, 308  
eigenvalues, 30  
elimination order, 239  
EMBLEM, 332, 342, 364  
learning problem, 328  
equality theory, 15  
equivalent networks, 40  
equivalent sets of composite  
choices, 92  
error bound, 257  
event, see  measurable set  
Event Calculus, 416  
evidence, 32, 76  
exclusive-or assumption, 186,  
227,231  
existential quantifier, 250  
expectation, 28, 121, 314  
conditional, 33  
expectation maximization, 320,  
382,397  
expected cost, 275  
expected reward, 275  
expected sufficient statistics, 343  
explanation, 95, 187, 235, 368  
infinite, 11, 24  
explanation sum, 290  
expressive power, 10, 39, 43  
extended PRISM, 124, 129  
extension of resolution, 27 4  
F  
fact, 5  
factor, 37  
heterogeneous, 242  
factor graph, 41  
factorization, see  factorized  
model  
factorized model, 37  
family of nodes, 38  
feature, 38  
Figaro, 86  
finite support condition, 120  
finitely additive probability  
measure, 25  
finitely additive probability space,  
25,49  
finitely well-defined program, 94  
first-order decision tree, 376  
first-order logic, 4  
fixpoint, 11, 1 03  
greatest, 3, 17  
least, 3, 11,  17, 18, 105, 219  
flexible probabilities, 66, 90  
FülL,  352, 359  
fork, 37  
forward probability, 335  
forward-backward, 322  
FOVE, 243  
fully-connected, 39  

Index  497 
function problems, 178  
function symbol, 179  
function symbols, 47, 89, 145  
G  
gamma distribution, 124, 126  
Gaussian density, see  Gaussian  
distribution  
Gaussian distribution, 32, 115,  
124,295  
Gaussian mixture model, 115,  
119, 125  
Gaussian noise, 440  
Gaussian process, 430  
GC-FOVE, 243  
generality relation, 352  
generative definitions, 130  
generative exclusiveness  
condition, 236  
geometric distribution, 127  
global scoring function, 360  
GPU, 309  
gradient, 328  
gradient descent, 329, 343, 347  
grammar, 421  
graph three-coloring, 172  
graphical model, 33, 35, 43  
greatest lower bound, 7, 101  
greedy search, 364  
grounding, 7, 11  
H  
Herbrand base, 7  
Herbrand universe, 7  
heuristic, 271  
Hidden Markov Model, 423  
hidden Markov model, 79, 128,  
176,423  
hierarchical probabilistic logic  
programs, 389  
hybrid ProbLog, 115  
parameter learning, 319  
hybrid program, 115  
hypothetical derivation sequence,  
70  
I  
idempotent, 290  
immediate consequence operator,  
11  
imprecise probability  
distributions, 130  
increasing sequence, 3  
increasing sequence of sets, 96  
independent and identically  
distributed, 46  
Independent Choice Logic, 45,  
483  
independent random variables, 66  
independent-and assumption, 186,  
226,229,321,392  
independent-or assumption, 228  
Indian GPA problem, 459  
indicator function, 120  
inductive definition, 10  
inductive logic programming, 80,  
351  
inference, 34  
approximate, 142  
COND, 171, 175  
CONDATOMS, 202  
DISTR, 176,314  
EVID, 175, 187  
exact, 185  
EXP, 314,418  
EXPEC, 456  
MAP, 218  
MPE, 176, 182, 202, 218,  
224  

498 
Index  
PROB, 289  
VIT, 176,219  
infinite mixture model, 436  
infinite product, 90  
infinite-dimensional measurable  
space, 112  
infinitely often, 97  
inhibitor, 240  
input variable, 353  
inside probability, 322  
Inside-outside algorithm, 325  
interpretation, 7, 17, 101  
Herbrand, 7  
three-valued, 17  
consistent, 17  
two-valued, 7  
iterated fixpoint, 17, 18, 103  
iterative deepening, 185, 255  
J  
joint cumulative distribution, 29  
joint event, 34  
joint probability, 34  
joint probability density, 29  
joint probability distribution, 29,  
30,34  
K  
k-best, 257, 283  
k-optimal, 270  
Kaiman filter, 439  
kernel, 427, 430  
function, 295  
squared exponential, 427  
Knaster-Tarski theorem, 3  
know ledge base model  
construction, 43  
knowledge compilation, 190, 192  
knowledge graph, 465  
Kolmogorov consistency theorem,  
111  
L  
labeling function, 285  
language bias, 353, 364  
latent Dirichlet allocation, 455  
learning from entailment, 351  
least unsure path in a graph, 274  
least upper bound, 17, 101  
Lebesgue measure, 126  
Lebesgue-measurable, 120  
left corner grammar, 421  
LeProbLog,289,328,343,347  
parameter learning problem,  
397  
Ievel mapping, 20  
LFI-ProbLog, 342  
Liftahle PLP, 379  
LIFTCOVER, 384  
lifted absorption, 244  
lifted inference, 185, 237  
lifted multiplication, 243  
lifted sum-out, 243  
likelihood ratio statistics, 363  
likelihood weighting, 310, 311,  
314,437  
limit of a sequence of sets, 96  
literal, 5  
negative, 6, 13  
positive, 6  
LLPAD, 326, 354  
parameter learning problem,  
326  
local scoring function, 359  
local search, 283  
log likelihood, 319, 332  
log-linear model, 39  
logic program, 4, 44, 74, 86  

Index  499 
definite, 5  
normal, 6, 13  
Logic Program with Annotated  
Disjunctions, 44  
logic programming, 4  
logical consequence, see  logical  
entailment  
logical entailment, 8, 15  
loop, 16, 92, 263  
negative, 13  
positive, 13  
lower bound, 102, 136  
lower bound of a set, 101   
greatest, 1   
LP2 , 244  
M  
m-estimate, 360, 363  
many-one reduction, 178  
mapping, 3  
MapReduce, 486  
marginalization, 43, 79  
Markov blanket, 37, 39  
Markov chain, 264  
Markov chain Monte Carlo, 264  
Markov logic network, 83, 446  
Markov network, 40, 84  
Markov random field, see  Markov  
network  
maximum likelihood, 320  
MCINTYRE, 260, 309, 450, 455  
mean, 30  
measurable function, 26  
measurable set, 24  
measurable space, 24  
mega-examp1e, see   
mega-interpretation  
mega-interpretation, 332, 462  
memoing, 187, 310  
Mendelian ru1es of inheritance, 56  
merge rule, 334  
meta-interpreter, 11   
meta-predicate, 67  
Metropolis-hasting MCMC, 264  
Metropolis-hastings MCMC, 264,  
310  
mFOIL, 359, 363  
mixed-integer programming, 357  
MNIST, 344  
mode declaration, 353, 364  
model, 7, 8, 11, 16  
Herbrand, 7  
minimal, 8, 22  
stab1e, 22  
well-founded, 16, 149  
model counting, 190  
monotonic mapping, 3, 4  
Monte Carlo, 258, 260, 307,450  
Monty Hall puzzle, 54  
moralizing a graph, 40  
most general unifier, 7, 9  
multi-switch, see  random switch  
multiset, 327  
multivalued decision diagrams,  
196  
multivariate Gaussian  
distribution, 428  
N  
natural1anguage processing, 421  
necessity, 273  
necessity degree, 273  
negation as failure, 9  
neural annotated disjunction, 345  
neural network, 348, 393, 465  
neuro-symbolic integration, 344  
neutral sum, 290  
neutral-sum, 303  

500 
Index  
problem, 294  
Nilsson's probabilistic logic, 82  
noisy-OR, 237, 240, 380  
normal approximation, 258  
normal density, see  Gaussian  
distribution  
normal distribution, see  Gaussian  
distribution  
normallogic program, 6  
normalization, 39  
normalizing constant, 38  
NP-hard, 267  
0  
open sets, 110  
open-world assumption, 336  
optimal strategy, 277  
ordinal powers, 3  
outcome, 26  
output variable, 353  
outside probability, 322  
p  
pairwise incompatible set of  
composite choices, 92  
parameter learning, 319, 366  
parameterized interpretation, 216  
parameterized negative  
two-valued  
interpretation, 101, 156  
parameterized positive two-valued  
interpretation, 101, 156  
parameterized random variable,  
77,238  
parameterized three-valued  
interpretation, 101, 156  
consistent, 101  
parametric factor, see  parfactor  
parent random variable, 36  
parfactor, 77, 238  
parfactor graph, 237  
parial order, 1  
part-of-speech tagging, 482  
partial derivative, 209  
partially ordered set, 1  
particle filtering, 311, 315, 419  
partition function, 38  
personalized Page Rank, 80  
PHIL, 397  
DPHIL, 398  
EMPHIL, 398  
PICL, 197  
PIP, 236  
PITA, 198,230  
correctness, 197  
transformation, 199  
PITA(EXC,_), 228  
PITA(IND, EXC), 228  
PITA(IND,EXC), 228  
PITA(IND,IND), 228  
PITA(OPT), 230  
placemarker, 354  
Poisson distribution, 120, 124,  
127  
positive-definite matrix, 30  
possibilistic clause, 273  
possibilistic logic, 273  
possibilistic logic program, 273  
possibilistic theory, 273  
possibility, 273  
possibility degree, 273  
possibility measure, 273  
possible models, 49  
Potassco, 23  
potential, see  factor  
powerset, 1  
precision, 314  
precision recall curve, 421  

Index  501 
prediction, 34  
principle of sufficient causation,  
72  
principle of universal causation, 
72  
PRIS~. 186,235,236,293,316  
language, 44, 56  
learning task, 319  
probabilistic answer set  
programming, 165  
Probabilistic Constraint Logic  
Programming, 130, 158,  
156  
probabilistic context-free  
grammar, 80, 236, 322,  
421  
probabilistic fact, 45, 48  
Probabilistic Horn Abduction, 45  
probabilistic left corner grammar,  
422  
probabilistic logic program, 83  
probabilistic programming, 310  
probabilistic sum, 393  
probability, 24, 69, 135, 449  
lower, 168  
upper, 168  
probability density, 27, 116, 141  
probability distribution, 26, 43  
probability measure, 25, 96  
probability space, 25, 99, 125  
product, 132  
probability theory, 24  
probability tree, 68  
ProbFOIL, 358  
ProbFOIL+, 358  
learning problem, 358  
ProbLo45, 110,115,216  
ProbLog,45, 115,191  
language,43-45,49,55  
system, 186  
theory compression, 357  
ProbLog1,45,255,284  
ProbLog2,205,346  
parameter learning problem,  
326  
product fuzzy logic, 393  
product rule, 31  
product space, 31  
product topology, 110  
Progol, 353, 364  
program, 13, 52, 115, 256  
bounded-arity, 181  
depth, 105  
propositional, 181  
temporally well-formed, 236  
unsound, 165  
project operation, 426  
Prolog, 9  
Prolog Factor Language, 77  
proof procedure, 8  
ProPPR, 80  
PSI-Solver, 307  
Python, 215  
Q  
quantitative structure-activity  
relationship, 416  
query, 9, 35, 50  
R  
random structure, 476  
random function, 317  
random switch, 46  
name, 46  
random variable, 46, 47  
continuous,27  
discrete, 27  
random walk, 440  

502 
Index  
one-dimensional, 454  
range restrictedness, 5  
normal programs, 9  
range-restrictedness, 372  
recall, 73  
recursive clauses, 368  
reduction, 22, 178  
regression, 427  
regular random variable, 242  
regularization, 403  
expectation maximization,  
404  
gradient descent, 328  
rejection sampling, 264, 314, 418  
relational autocompletion, 374  
relative frequency, 327, 328  
relevant, 8, 11, 204  
ground atom, 11, 136  
rule, 11, 207  
reparameterization, 38, 329  
resolution, 8  
SLD, 9, 204  
tree, 9, 10  
SLDNF, 9, 15, 16  
tree, 9, 15  
SLG, 20  
ROC curve, 471  
s 
sample space, 24  
sampling, 122, 185, 255  
Sampo, 307  
SAT, 190  
satisfiability, 300  
satisfiability modulo theory, 300  
satisfiable set of clauses, 8  
saturation, 354, 364  
scalar multiplication, 277  
schema, 353  
selection, 48, 51  
selection rule, 9   
semantic web, 465  
semantics, 13  
Clark's completion, 15  
model theoretic, 8  
stable models, 13  
well-founded, 16  
semiring, 284  
commutative, 285, 302  
gradient, 289, 329  
neutral-sum, 303  
possibilistic, 273  
probabilistic, 273  
probability density, 304  
sensitivity analysis, 289  
sentential decision diagram, 213,  
490  
set of worlds compatible with a  
composite choice, 99   
set of worlds compatible with a  
set of composite choices,  
99  
Shannon expansion, 196, 277  
shift operation, 422  
sigma-algebra, 399  
minimal, 22  
product, 110  
significance test, 363  
Skolem normal form, 251  
Skolem predicate, 250  
Skolemization, 250  
SLEAHP, 378, 389  
SLIPCOVER, 364  
Smodels, 23  
social network, 283  
soft evidence, 86  
sound program, 48, 109  
soundness, 8, 109  

Index  503 
source-to-source transformation,  
259  
splitting, 91  
algorithm, 92, 178  
standard deviation, 28  
stick-breaking process, 431  
sticker collector problem, 453  
stochastic Tp  operator, 121  
stochastic Tp  operator, 310  
cplint, 194  
stochastic grammar, 80  
stochastic logic program, 79, 442  
strategy, 275, 448  
stratification, 20, 179  
dynamic, 18, 19  
local, 20, 180  
stratum, 18  
strictly positive distribution, 39  
substitution, 6  
success function, 295  
integration, 296  
join, 296  
marginalization, 296  
projection, 296  
succinctness, 214  
sumrule, 32  
summing out, see  marginalization  
SWI-prolog, 21, 202, 263, 445  
Symbo, 305  
symbolic derivation, 293, 344  
symmetric matrix, 30  
T  
Tp  compilation, 216, 270  
t-norm, 393  
tabling,20, 188,197,201,204  
target predicate, 332, 365  
target predicates, 351  
temporal precedence assumption,  
72  
TensorFlow, 308  
term, 4  
term expansion, 259  
text mining, 455  
theory compression, 357  
theta-subsumption, 352  
three-prisoner puzzle, 55  
tight logic program, 252  
tile map, 444  
top element, 7, 17, 101  
top-down parsing, 422  
top-down reasoning, 11  
topic, 458  
topological sort, 35  
topological space, 110  
topology, 110  
total composite choice, see   
selection  
transfinite induction, 104  
transitive closure, 10  
translation, 58, 60, 226, 245  
BN to LPAD, 57  
LPAD to BN, 60  
LPADs to PHAIICL, 58  
LPADs to ProbLog, 59  
MLNs to LPADs, 85  
PHAIICL/PRISM to LPADs,  
60  
tree 9, 11, 68, 255  
SLD, 259, 271, 275  
trial id, 46  
truel, 447  
truth-functional, 227  
Tseitin predicate, 252  
Turing machine, 176  
Turing-complete language, 10  

504 
Index  
u 
uncertainty, 24  
undirected graph, 39  
unfounded sets, 17  
uniform distribution, 145  
upperbound,255,269-271  
upper bound of a set, 1  
least, 1  
utility, 276  
utility fact, 275  
utility function, 275  
UW-CSE, 381  
V  
variable elimination, 239  
variance,30  
video game, 444  
viral marketing, 283  
Viterbi algorithm, 189  
Viterbi path, 189  
vtree, 213  
w 
weight, 38  
weight fusion, 274  
weighted Boolean formula, 190,  
203  
weighted first order model  
counting, 249  
weighted MAX-SAT, 190,207,  
269  
weighted model counting, 190,  
268  
weighted model integration, 300  
well-defined program, 99  
well-formed formula, 4, 5  
world, 45-47, 50, 53, 91, 290  
X  
XSB, 201, 202  
y  
)'AP,21, 76,202,259  
internal database, 260  

About  the  Author  
Fabrizio  Riguzzi  is Full Professor of Computer Science at the Department 
of Mathematics and Computer Science of the University of Ferrara. He was 
previously Associate Professor and Assistant Professor at the same university. 
He got his Master and PhD in Computer Engineering from the University of 
Bologna. Fabrizio Riguzzi is Editor in Chief of Intelligenza Artificiale, the 
official joumal of the Italian Association for Artificial Intelligence. He is the 
author of more than 200 peer reviewed papers in the areas of Machine Leam­
ing, Inductive Logic Programming and Statistical Relational Learning. His 
aim is to develop intelligent systems by combining in novel ways techniques 
from artificial intelligence, logic and statistics. 
505  


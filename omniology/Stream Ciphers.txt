Andreas Klein
Stream 
Ciphers

Stream Ciphers

Andreas Klein
Stream Ciphers

Andreas Klein
Dept. of Pure Mathem. & Computer Algebra
State University of Ghent
Ghent, Belgium
ISBN 978-1-4471-5078-7
ISBN 978-1-4471-5079-4 (eBook)
DOI 10.1007/978-1-4471-5079-4
Springer London Heidelberg New York Dordrecht
Library of Congress Control Number: 2013936538
Mathematics Subject Classiﬁcation: 94A60, 68P25, 11T71
© Springer-Verlag London 2013
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed. Exempted from this legal reservation are brief excerpts in connection
with reviews or scholarly analysis or material supplied speciﬁcally for the purpose of being entered
and executed on a computer system, for exclusive use by the purchaser of the work. Duplication of
this publication or parts thereof is permitted only under the provisions of the Copyright Law of the
Publisher’s location, in its current version, and permission for use must always be obtained from Springer.
Permissions for use may be obtained through RightsLink at the Copyright Clearance Center. Violations
are liable to prosecution under the respective Copyright Law.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
While the advice and information in this book are believed to be true and accurate at the date of pub-
lication, neither the authors nor the editors nor the publisher can accept any legal responsibility for any
errors or omissions that may be made. The publisher makes no warranty, express or implied, with respect
to the material contained herein.
Printed on acid-free paper
Springer is part of Springer Science+Business Media (www.springer.com)

Preface
Cryptographic ciphers come in two ﬂavours: symmetric (AES, etc.) and asymmetric
(RSA, etc.). The symmetric ciphers are further divided into block ciphers and stream
ciphers. Block ciphers work on large blocks simultaneously (typically comprising
128 or 256 bits) and have no internal state (at least not in their basic version). Stream
ciphers work on single bits or single words and need to maintain an internal state to
change the cipher at each step.
Typically stream ciphers can reach higher speeds than block ciphers, but their
theory is less developed. This is why stream ciphers are often skipped in books on
cryptography.
This does not reﬂect the real importance of stream ciphers. They are used in
several everyday applications (for example RC4 is used in wireless LAN and mobile
telephones use A5). This book should ﬁll the gap and provide a detailed introduction
to stream ciphers.
I wrote this book in the years 2008–2010 when I had a research position at Ghent
University.
I want to thank all my colleagues in Ghent for the pleasant time I had there, but
especially Prof. Leo Storme who ﬁrst gave me the opportunity to come to Ghent.
We did some nice research together.
I also thank the team of the Springer Verlag who did a great job in improving
this book. In addition I want to thank the anonymous referee, without whom the
chapter on the Blum-Blum-Shub generator would be missing and there would be no
exercises.
Andreas Klein
Wettenberg, Germany
v

Contents
1
Introduction to Stream Ciphers . . . . . . . . . . . . . . . . . . . . .
1
1.1
History I: Antique Ciphers . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Lessons from History: The Classiﬁcation of Ciphers . . . . . . . .
3
1.3
History II: The Golden Age of Stream Ciphers . . . . . . . . . . .
8
1.4
Lessons from the Enigma . . . . . . . . . . . . . . . . . . . . . .
8
1.5
History III: Towards Modern Cryptography . . . . . . . . . . . . .
10
1.6
When to Use Stream Ciphers? . . . . . . . . . . . . . . . . . . . .
11
1.7
Outline of the Book . . . . . . . . . . . . . . . . . . . . . . . . .
11
Part I
Shift Register-Based Stream Ciphers
2
Linear Feedback Shift Registers . . . . . . . . . . . . . . . . . . . . .
17
2.1
Basic Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
2.2
Algebraic Description of LFSR Sequences . . . . . . . . . . . . .
18
2.2.1
Generating Functions
. . . . . . . . . . . . . . . . . . . .
19
2.2.2
Feedback Polynomials Without Multiple Roots . . . . . . .
20
2.2.3
Feedback Polynomials with Multiple Roots . . . . . . . . .
21
2.2.4
LFSR Sequences as Cyclic Linear Codes . . . . . . . . . .
23
2.3
Properties of m-Sequences . . . . . . . . . . . . . . . . . . . . . .
24
2.3.1
Golomb’s Axioms . . . . . . . . . . . . . . . . . . . . . .
24
2.3.2
Sequences with Two Level Auto-Correlation . . . . . . . .
27
2.3.3
Cross-Correlation of m-Sequences
. . . . . . . . . . . . .
29
2.4
Linear Complexity . . . . . . . . . . . . . . . . . . . . . . . . . .
30
2.4.1
Deﬁnition and Basic Properties . . . . . . . . . . . . . . .
30
2.4.2
The Berlekamp-Massey Algorithm . . . . . . . . . . . . .
33
2.4.3
Asymptotic Fast Computation of Linear Complexity . . . .
37
2.4.4
Linear Complexity of Random Sequences
. . . . . . . . .
42
2.5
The Linear Complexity Proﬁle of Pseudo-random Sequences
. . .
44
2.5.1
Basic Properties . . . . . . . . . . . . . . . . . . . . . . .
44
2.5.2
Continued Fractions . . . . . . . . . . . . . . . . . . . . .
46
vii

viii
Contents
2.5.3
Classiﬁcation of Sequences with a Perfect Linear
Complexity Proﬁle . . . . . . . . . . . . . . . . . . . . . .
48
2.6
Implementation of LFSRs . . . . . . . . . . . . . . . . . . . . . .
50
2.6.1
Hardware Realization of LFSRs . . . . . . . . . . . . . . .
51
2.6.2
Software Realization of LFSRs . . . . . . . . . . . . . . .
52
3
Non-linear Combinations of LFSRs . . . . . . . . . . . . . . . . . . .
59
3.1
De Bruijn Sequences . . . . . . . . . . . . . . . . . . . . . . . . .
59
3.2
A Simple Example of a Non-linear Combination of LFSRs
. . . .
64
3.3
Different Attack Classes . . . . . . . . . . . . . . . . . . . . . . .
65
3.3.1
Time-Memory Trade-off Attacks . . . . . . . . . . . . . .
65
3.3.2
Algebraic Attacks . . . . . . . . . . . . . . . . . . . . . .
65
3.3.3
Correlation Attacks
. . . . . . . . . . . . . . . . . . . . .
66
3.4
Non-linear Combinations of Several LFSR Sequences . . . . . . .
66
3.4.1
The Product of Two LFSRs . . . . . . . . . . . . . . . . .
67
3.4.2
General Combinations . . . . . . . . . . . . . . . . . . . .
70
3.5
Non-linear Filters
. . . . . . . . . . . . . . . . . . . . . . . . . .
72
3.6
Correlation Immune Functions
. . . . . . . . . . . . . . . . . . .
75
3.6.1
Deﬁnition and Alternative Characterizations . . . . . . . .
75
3.6.2
Siegenthaler’s Inequality
. . . . . . . . . . . . . . . . . .
78
3.6.3
Asymptotic Enumeration of Correlation Immune Functions
80
4
Correlation Attacks
. . . . . . . . . . . . . . . . . . . . . . . . . . .
91
4.1
CJS-Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
4.1.1
The Basic Version . . . . . . . . . . . . . . . . . . . . . .
91
4.1.2
Using Relations of Different Size . . . . . . . . . . . . . .
94
4.1.3
How to Search Relations . . . . . . . . . . . . . . . . . . .
96
4.1.4
Extended Relation Classes . . . . . . . . . . . . . . . . . .
98
4.1.5
Twice Step Decoding
. . . . . . . . . . . . . . . . . . . . 101
4.1.6
Evaluation of the Relations
. . . . . . . . . . . . . . . . . 103
4.2
Attacks Based on Convolutional Codes . . . . . . . . . . . . . . . 105
4.2.1
Introduction to Convolutional Codes
. . . . . . . . . . . . 105
4.2.2
Decoding Convolutional Codes . . . . . . . . . . . . . . . 107
4.2.3
Application to Cryptography
. . . . . . . . . . . . . . . . 111
4.3
Attacking LFSRs with Sparse Feedback Polynomials
. . . . . . . 114
5
BDD-Based Attacks
. . . . . . . . . . . . . . . . . . . . . . . . . . . 117
5.1
Binary Decision Diagrams . . . . . . . . . . . . . . . . . . . . . . 117
5.1.1
Ordered BDDs . . . . . . . . . . . . . . . . . . . . . . . . 118
5.1.2
Free BDDs . . . . . . . . . . . . . . . . . . . . . . . . . . 124
5.2
An Example of a BDD-Based Attack . . . . . . . . . . . . . . . . 126
5.2.1
The Cipher E0 . . . . . . . . . . . . . . . . . . . . . . . . 126
5.2.2
Attacking E0 . . . . . . . . . . . . . . . . . . . . . . . . . 127
6
Algebraic Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
6.1
Tools for Solving Non-linear Equations . . . . . . . . . . . . . . . 131
6.1.1
Gröbner Bases . . . . . . . . . . . . . . . . . . . . . . . . 131

Contents
ix
6.1.2
Linearization . . . . . . . . . . . . . . . . . . . . . . . . . 143
6.2
Pre-processing Techniques for Algebraic Attacks . . . . . . . . . . 147
6.2.1
Reducing the Degree . . . . . . . . . . . . . . . . . . . . . 147
6.2.2
Dealing with Combiners with Memory . . . . . . . . . . . 149
6.3
Real World Examples . . . . . . . . . . . . . . . . . . . . . . . . 151
6.3.1
LILI-128 . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
6.3.2
E0
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
7
Irregular Clocked Shift Registers . . . . . . . . . . . . . . . . . . . . 155
7.1
The Stop-and-Go Generator and the Step-Once-Twice Generator
. 155
7.2
The Alternating Step Generator . . . . . . . . . . . . . . . . . . . 157
7.3
The Shrinking Generator . . . . . . . . . . . . . . . . . . . . . . . 158
7.3.1
Description of the Cipher . . . . . . . . . . . . . . . . . . 159
7.3.2
Linear Complexity of the Shrinking Generator . . . . . . . 159
7.3.3
Correlation Attacks Against the Shrinking Generator . . . . 161
7.4
Side Channel Attacks
. . . . . . . . . . . . . . . . . . . . . . . . 163
Part II
Some Special Ciphers
8
The Security of Mobile Phones (GSM) . . . . . . . . . . . . . . . . . 169
8.1
The GSM Protocol . . . . . . . . . . . . . . . . . . . . . . . . . . 169
8.2
A5/2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
8.2.1
Description of A5/2 . . . . . . . . . . . . . . . . . . . . . 170
8.2.2
An Instance of a Ciphertext-Only Attack . . . . . . . . . . 172
8.2.3
Other Attacks Against A5/2 . . . . . . . . . . . . . . . . . 175
8.3
A5/1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
8.3.1
Description of A5/1 . . . . . . . . . . . . . . . . . . . . . 176
8.3.2
Time-Memory Trade-off Attacks . . . . . . . . . . . . . . 176
8.3.3
Correlation Attacks
. . . . . . . . . . . . . . . . . . . . . 179
9
RC4 and Related Ciphers . . . . . . . . . . . . . . . . . . . . . . . . 183
9.1
Description of RC4
. . . . . . . . . . . . . . . . . . . . . . . . . 183
9.2
Application of RC4 in WLAN Security . . . . . . . . . . . . . . . 184
9.2.1
The WEP Protocol . . . . . . . . . . . . . . . . . . . . . . 184
9.2.2
The WPA Protocol . . . . . . . . . . . . . . . . . . . . . . 185
9.2.3
A Weakness Common to Both Protocols . . . . . . . . . . 187
9.3
Analysis of the RC4 Key Scheduling . . . . . . . . . . . . . . . . 190
9.3.1
The Most Likely and Least Likely RC4 Permutation . . . . 191
9.3.2
Discarding the First RC4 Bytes . . . . . . . . . . . . . . . 196
9.4
Chosen IV Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . 199
9.4.1
Initialization Vector Precedes the Main Key
. . . . . . . . 199
9.4.2
Variants of the Attack . . . . . . . . . . . . . . . . . . . . 200
9.4.3
Initialization Vector Follows the Main Key . . . . . . . . . 202
9.5
Attacks Based on Goli´c’s Correlation . . . . . . . . . . . . . . . . 202
9.5.1
Initialization Vector Follows the Main Key . . . . . . . . . 204
9.5.2
Initialization Vector Precedes the Main Key
. . . . . . . . 205
9.5.3
Attacking RC4 with the First n Bytes Discarded . . . . . . 207

x
Contents
9.5.4
A Ciphertext-Only Attack . . . . . . . . . . . . . . . . . . 209
9.6
State Recovering Attacks
. . . . . . . . . . . . . . . . . . . . . . 209
9.7
Other Attacks on RC4 . . . . . . . . . . . . . . . . . . . . . . . . 212
9.7.1
Digraph Probabilities
. . . . . . . . . . . . . . . . . . . . 213
9.7.2
Fortuitous States . . . . . . . . . . . . . . . . . . . . . . . 218
9.8
RC4 Variants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
9.8.1
An RC4 Variant for 32-Bit Processors
. . . . . . . . . . . 222
9.8.2
RC4A
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
9.8.3
Modiﬁcations to Avoid Known Attacks . . . . . . . . . . . 227
10
The eStream Project . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
10.1 Trivium . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
10.2 Rabbit
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232
10.3 Mosquito and Moustique
. . . . . . . . . . . . . . . . . . . . . . 235
11
The Blum-Blum-Shub Generator and Related Ciphers . . . . . . . . 241
11.1 Cryptographically Secure Pseudo-random Generators
. . . . . . . 241
11.2 The Blum-Blum-Shub Generator . . . . . . . . . . . . . . . . . . 244
11.3 Implementation Aspects . . . . . . . . . . . . . . . . . . . . . . . 247
11.4 Extracting Several Bits per Step . . . . . . . . . . . . . . . . . . . 251
11.5 The RSA Generator and the Power Generator . . . . . . . . . . . . 253
11.6 Generators Based on Other Hard Problems . . . . . . . . . . . . . 254
11.7 Unconditionally Secure Pseudo-random Sequences . . . . . . . . . 256
Part III
Mathematical Background
12
Computational Aspects . . . . . . . . . . . . . . . . . . . . . . . . . . 261
12.1 Bit Tricks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
12.1.1 Inﬁnite 2-adic Expansions . . . . . . . . . . . . . . . . . . 261
12.1.2 Sideway Addition . . . . . . . . . . . . . . . . . . . . . . 262
12.1.3 Sideway Addition for Arrays
. . . . . . . . . . . . . . . . 263
12.2 Binary Decision Diagrams, Implementation Aspects . . . . . . . . 264
12.2.1 Memory Management . . . . . . . . . . . . . . . . . . . . 264
12.2.2 Implementation of the Basic Operations
. . . . . . . . . . 266
12.2.3 Implementation of Reordering Algorithms . . . . . . . . . 267
12.2.4 Emulating a BDD Base . . . . . . . . . . . . . . . . . . . 271
12.3 The O-Notation
. . . . . . . . . . . . . . . . . . . . . . . . . . . 272
12.4 The Complexity Classes P and NP
. . . . . . . . . . . . . . . . 273
12.5 Fast Linear Algebra . . . . . . . . . . . . . . . . . . . . . . . . . 278
12.5.1 Matrix Multiplication . . . . . . . . . . . . . . . . . . . . 278
12.5.2 Other Matrix Operations . . . . . . . . . . . . . . . . . . . 289
12.5.3 Wiedmann’s Algorithm and Black Box Linear Algebra
. . 291
13
Number Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
13.1 Basic Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293
13.2 The Group (Z/nZ)× . . . . . . . . . . . . . . . . . . . . . . . . . 294
13.3 The Prime Number Theorem and Its Consequences . . . . . . . . . 295

Contents
xi
13.4 Zsigmondy’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . 297
13.5 Quadratic Residues
. . . . . . . . . . . . . . . . . . . . . . . . . 299
13.6 Lattice Reduction
. . . . . . . . . . . . . . . . . . . . . . . . . . 301
14
Finite Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
14.1 Basic Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
14.2 Irreducible Polynomials . . . . . . . . . . . . . . . . . . . . . . . 305
14.3 Primitive Polynomials . . . . . . . . . . . . . . . . . . . . . . . . 307
14.4 Trinomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
14.5 The Algebraic Normal Form . . . . . . . . . . . . . . . . . . . . . 309
15
Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
15.1 Measure Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
15.2 Simple Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
15.2.1 The Variation Distance
. . . . . . . . . . . . . . . . . . . 312
15.2.2 The Test Problem
. . . . . . . . . . . . . . . . . . . . . . 313
15.2.3 Optimal Tests
. . . . . . . . . . . . . . . . . . . . . . . . 314
15.2.4 Bayesian Statistics . . . . . . . . . . . . . . . . . . . . . . 315
15.3 Sequential Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . 316
15.3.1 Introduction to Sequential Analysis . . . . . . . . . . . . . 316
15.3.2 Martingales
. . . . . . . . . . . . . . . . . . . . . . . . . 316
15.3.3 Wald’s Sequential Likelihood Ratio Test . . . . . . . . . . 319
15.3.4 Brownian Motion
. . . . . . . . . . . . . . . . . . . . . . 322
15.3.5 The Functional Central Limit Theorem . . . . . . . . . . . 326
16
Combinatorics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
16.1 Asymptotic Calculations . . . . . . . . . . . . . . . . . . . . . . . 329
16.2 Permutations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332
16.3 Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334
Part IV
Exercises with Solutions
17
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
17.1 Proposals for Programming Projects
. . . . . . . . . . . . . . . . 344
18
Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347
Part V
Programs
19
An Overview of the Programs . . . . . . . . . . . . . . . . . . . . . . 365
20
Literate Programming . . . . . . . . . . . . . . . . . . . . . . . . . . 371
20.1 Introduction to Literate Programming . . . . . . . . . . . . . . . . 371
20.2 Pweb Design Goals
. . . . . . . . . . . . . . . . . . . . . . . . . 371
20.3 Pweb Manual
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 372
20.3.1 Structure of a WEB-Document . . . . . . . . . . . . . . . 372
20.3.2 Text Sections . . . . . . . . . . . . . . . . . . . . . . . . . 372
20.3.3 Code Sections and Modules . . . . . . . . . . . . . . . . . 373
20.3.4 Macros . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374

xii
Contents
20.3.5
Special Variable Names . . . . . . . . . . . . . . . . . . . 375
20.3.6
Include Files . . . . . . . . . . . . . . . . . . . . . . . . . 375
20.3.7
Conditional Compilation . . . . . . . . . . . . . . . . . . 375
20.3.8
More pweb Commands . . . . . . . . . . . . . . . . . . . 376
20.3.9
Compatibility Features . . . . . . . . . . . . . . . . . . . 376
20.3.10 Common Errors . . . . . . . . . . . . . . . . . . . . . . . 376
20.3.11 Editing pweb Documents . . . . . . . . . . . . . . . . . . 377
20.3.12 Extending pweb . . . . . . . . . . . . . . . . . . . . . . . 377
Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379
References
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
Index
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395

List of Figures
Fig. 1.1
Encrypting a text with a Vigenère cipher . . . . . . . . . . . . . .
3
Fig. 1.2
Stream-oriented block cipher modes . . . . . . . . . . . . . . . .
5
Fig. 1.3
Encrypting a text with an auto key cipher
. . . . . . . . . . . . .
5
Fig. 1.4
A synchronous stream cipher . . . . . . . . . . . . . . . . . . . .
6
Fig. 1.5
A self-synchronizing stream cipher . . . . . . . . . . . . . . . . .
7
Fig. 2.1
A feedback shift register
. . . . . . . . . . . . . . . . . . . . . .
18
Fig. 2.2
The sum of two LFSRs . . . . . . . . . . . . . . . . . . . . . . .
32
Fig. 2.3
Construction for the Berlekamp-Massey algorithm
. . . . . . . .
35
Fig. 2.4
Combination of the two LFSRs of Fig. 2.3 . . . . . . . . . . . . .
35
Fig. 2.5
The linear complexity proﬁle of 1010111100010011010111100
.
45
Fig. 2.6
A typical linear complexity proﬁle . . . . . . . . . . . . . . . . .
45
Fig. 2.7
The Fibonacci implementation of an LFSR
. . . . . . . . . . . .
51
Fig. 2.8
The Galois implementation of an LFSR
. . . . . . . . . . . . . .
51
Fig. 2.9
Right shift over several words
. . . . . . . . . . . . . . . . . . .
53
Fig. 3.1
The smallest de Bruijn graphs
. . . . . . . . . . . . . . . . . . .
60
Fig. 3.2
The Geffe generator . . . . . . . . . . . . . . . . . . . . . . . . .
64
Fig. 3.3
A simple non-linear ﬁlter . . . . . . . . . . . . . . . . . . . . . .
72
Fig. 3.4
1 −cos(x) ≥2
π2 x2
. . . . . . . . . . . . . . . . . . . . . . . . .
86
Fig. 4.1
A simple convolutional code . . . . . . . . . . . . . . . . . . . . 105
Fig. 4.2
Three different encoders of the same code . . . . . . . . . . . . . 106
Fig. 4.3
An example for the Viterbi algorithm . . . . . . . . . . . . . . . . 108
Fig. 4.4
A tree diagram for a (2,1) encoder . . . . . . . . . . . . . . . . . 109
Fig. 5.1
A non-reduced binary decision diagram . . . . . . . . . . . . . . 118
Fig. 5.2
A reduced binary decision diagram . . . . . . . . . . . . . . . . . 118
Fig. 5.3
Reducing a binary decision diagram . . . . . . . . . . . . . . . . 118
Fig. 5.4
Algorithm 5.2 applied to the diagram of Fig. 5.1 . . . . . . . . . . 122
Fig. 5.5
The melt of two BDDs . . . . . . . . . . . . . . . . . . . . . . . 123
Fig. 5.6
A free binary decision diagram . . . . . . . . . . . . . . . . . . . 125
Fig. 5.7
The control graph of the free BDD in Fig. 5.6 . . . . . . . . . . . 125
xiii

xiv
List of Figures
Fig. 5.8
The cipher E0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
Fig. 5.9
Basic BDDs for attacking E0 . . . . . . . . . . . . . . . . . . . . 129
Fig. 6.1
A combiner with memory . . . . . . . . . . . . . . . . . . . . . . 149
Fig. 6.2
The LILI-128 keystream generator . . . . . . . . . . . . . . . . . 152
Fig. 7.1
The stop-and-go generator
. . . . . . . . . . . . . . . . . . . . . 156
Fig. 7.2
The alternating step generator
. . . . . . . . . . . . . . . . . . . 158
Fig. 7.3
The shrinking generator . . . . . . . . . . . . . . . . . . . . . . . 159
Fig. 8.1
Outline of the GSM protocol . . . . . . . . . . . . . . . . . . . . 170
Fig. 8.2
Diagram of A5/2
. . . . . . . . . . . . . . . . . . . . . . . . . . 171
Fig. 8.3
Diagram of A5/1
. . . . . . . . . . . . . . . . . . . . . . . . . . 177
Fig. 9.1
The S-box of the Temporal Key Hash (Part 1) . . . . . . . . . . . 188
Fig. 9.2
Temporal Key Hash (Part 1)
. . . . . . . . . . . . . . . . . . . . 188
Fig. 9.3
Temporal Key Hash (Part 2)
. . . . . . . . . . . . . . . . . . . . 188
Fig. 9.4
The graph representation of S = (0 1)(2 3) = (3 1)(2 3)(1 2)(0 1)
193
Fig. 9.5
The FMS-attack key scheduling
. . . . . . . . . . . . . . . . . . 200
Fig. 9.6
Digraph repetition . . . . . . . . . . . . . . . . . . . . . . . . . . 217
Fig. 9.7
Example of a 3-fortuitous state . . . . . . . . . . . . . . . . . . . 218
Fig. 10.1
The cipher Trivium . . . . . . . . . . . . . . . . . . . . . . . . . 230
Fig. 10.2
The cipher Rabbit . . . . . . . . . . . . . . . . . . . . . . . . . . 233
Fig. 10.3
The cipher Moustique . . . . . . . . . . . . . . . . . . . . . . . . 236
Fig. 10.4
Mapping between qi
j and a(0)
k
. . . . . . . . . . . . . . . . . . . 237
Fig. 12.1
Memory layout . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
Fig. 12.2
A BDD node in memory . . . . . . . . . . . . . . . . . . . . . . 265
Fig. 12.3
Variable swapping . . . . . . . . . . . . . . . . . . . . . . . . . . 268
Fig. 12.4
The variable 3 jumps up
. . . . . . . . . . . . . . . . . . . . . . 269
Fig. 12.5
Moving a variable from the top to the bottom
. . . . . . . . . . . 270
Fig. 12.6
Sifting down
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
Fig. 12.7
A Turing machine . . . . . . . . . . . . . . . . . . . . . . . . . . 274
Fig. 15.1
A Brownian motion path . . . . . . . . . . . . . . . . . . . . . . 323
Fig. 16.1
Comparison of
 b
a f (x)dx and 1/2f (a) + f (a + 1) + ··· +
f (b −1) + 1/2f (b)
. . . . . . . . . . . . . . . . . . . . . . . . 330
Fig. 16.2
A labeled tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335
Fig. 18.1
Binomial coefﬁcients modulo 2 . . . . . . . . . . . . . . . . . . . 352
Fig. 18.2
Basic BDD for attacking the self-shrinking generator . . . . . . . 355
Fig. 18.3
The densities of N(0,1) and N(0,2)
. . . . . . . . . . . . . . . 362
Fig. 19.1
An example of the Doxygen documentation . . . . . . . . . . . . 366
Fig. 19.2
An example of the pweb documentation . . . . . . . . . . . . . . 366
Fig. 20.1
The literate programming environment . . . . . . . . . . . . . . . 372

List of Tables
Table 1.1
The Vigenère tableau . . . . . . . . . . . . . . . . . . . . . . .
4
Table 2.1
Tests for the algorithms . . . . . . . . . . . . . . . . . . . . . .
42
Table 2.2
Speed of different LFSR implementations (128 bit LFSR) . . . .
57
Table 2.3
Speed of an LFSR with feedback polynomial z127 + z + 1 . . . .
57
Table 4.1
A Fano metric for a (2,1) convolutional code and a BSC with
p = 0.25 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
Table 4.2
Example of the sequential decoding algorithm . . . . . . . . . . 111
Table 9.1
A 4-order, 7-generative pattern . . . . . . . . . . . . . . . . . . 213
Table 9.2
Digraph probabilities of RC4 . . . . . . . . . . . . . . . . . . . 216
Table 9.3
The number of fortuitous states and their expected occurrence . . 222
Table 10.1
Number of bits per cell in the CCSR of Moustique . . . . . . . . 237
Table 10.2
Bit updating in the CCSR of Moustique
. . . . . . . . . . . . . 238
Table 12.1
Comparing sideway addition algorithms for arrays . . . . . . . . 264
Table 12.2
Speed of Pan’s algorithm . . . . . . . . . . . . . . . . . . . . . 287
Table 14.1
Primitive and irreducible polynomials over F2 of low weight
. . 310
Table 18.1
Comparison of block cipher modes . . . . . . . . . . . . . . . . 347
Table 20.1
Files needed by pweb . . . . . . . . . . . . . . . . . . . . . . . 378
xv

List of Algorithms
Algorithm 2.1
The Berlekamp-Massey algorithm . . . . . . . . . . . . . .
34
Algorithm 2.2
Massey(i,i′) . . . . . . . . . . . . . . . . . . . . . . . . .
38
Algorithm 2.3
feedback(i′)
. . . . . . . . . . . . . . . . . . . . . . . . .
42
Algorithm 2.4
Right shift over several words . . . . . . . . . . . . . . . .
53
Algorithm 2.5
Sideway addition mod 2 (32 bit version)
. . . . . . . . . .
53
Algorithm 2.6
Sideway addition mod 2 (without multiplication) . . . . . .
54
Algorithm 2.7
LFSR byte-oriented implementation (table look-ups) . . . .
55
Algorithm 2.8
Parallel sideway addition mod 2 . . . . . . . . . . . . . . .
56
Algorithm 2.9
LFSR update with parallel sideway addition mod 2 . . . . .
57
Algorithm 2.10
Generating an LFSR sequence with the feedback
polynomial zn + zk + 1
. . . . . . . . . . . . . . . . . . .
58
Algorithm 4.1
Simple fast correlation attack (CJS) . . . . . . . . . . . . .
93
Algorithm 4.2
Twice step decoding . . . . . . . . . . . . . . . . . . . . . 102
Algorithm 4.3
Fast Fourier transform over the group Z2 . . . . . . . . . . 105
Algorithm 4.4
Viterbi decoding . . . . . . . . . . . . . . . . . . . . . . . 108
Algorithm 4.5
Sequential decoding . . . . . . . . . . . . . . . . . . . . . 110
Algorithm 4.6
Meier’s and Staffelbach’s attack against LFSRs with sparse
feedback polynomials . . . . . . . . . . . . . . . . . . . . 115
Algorithm 5.1
Counting solutions of an ordered BDD
. . . . . . . . . . . 119
Algorithm 5.2
Reducing an ordered BDD . . . . . . . . . . . . . . . . . . 120
Algorithm 5.3
Check that a given BDD is free
. . . . . . . . . . . . . . . 124
Algorithm 6.1
Multivariate division with remainder
. . . . . . . . . . . . 135
Algorithm 6.2
Buchberger’s algorithm
. . . . . . . . . . . . . . . . . . . 140
Algorithm 6.3
The Gröbner walk . . . . . . . . . . . . . . . . . . . . . . 142
Algorithm 6.4
XL-algorithm . . . . . . . . . . . . . . . . . . . . . . . . . 146
Algorithm 6.5
F4 algorithm (simpliﬁed)
. . . . . . . . . . . . . . . . . . 147
Algorithm 7.1
The alternating step generator . . . . . . . . . . . . . . . . 158
Algorithm 7.2
The alternating step generator (alternative form)
. . . . . . 159
xvii

xviii
List of Algorithms
Algorithm 8.1
A5/2 initialization . . . . . . . . . . . . . . . . . . . . . . 174
Algorithm 8.2
A5/1 initialization . . . . . . . . . . . . . . . . . . . . . . 176
Algorithm 8.3
Enumerating special states of A5/1
. . . . . . . . . . . . . 178
Algorithm 9.1
RC4 key scheduling . . . . . . . . . . . . . . . . . . . . . 184
Algorithm 9.2
RC4 pseudo-random generator . . . . . . . . . . . . . . . . 184
Algorithm 9.3
Temporal Key Hash
. . . . . . . . . . . . . . . . . . . . . 186
Algorithm 9.4
Temporal Key Hash S-box . . . . . . . . . . . . . . . . . . 187
Algorithm 9.5
CRC encoding . . . . . . . . . . . . . . . . . . . . . . . . 189
Algorithm 9.6
CRC decoding . . . . . . . . . . . . . . . . . . . . . . . . 189
Algorithm 9.7
Idealized RC4 key scheduling . . . . . . . . . . . . . . . . 191
Algorithm 9.8
Computing the key from an early permutation state . . . . . 203
Algorithm 9.9
A simple internal state recovering attack
. . . . . . . . . . 211
Algorithm 9.10
Computing the digraph probabilities . . . . . . . . . . . . . 213
Algorithm 9.11
Computing the digraph probabilities (1. Transformation) . . 215
Algorithm 9.12
Computing the digraph probabilities (2. Transformation,
inner loops) . . . . . . . . . . . . . . . . . . . . . . . . . . 215
Algorithm 9.13
Searching fortuitous states . . . . . . . . . . . . . . . . . . 219
Algorithm 9.14
Enumerating fortuitous states (fast) . . . . . . . . . . . . . 220
Algorithm 9.15
RC4(n,m) key scheduling
. . . . . . . . . . . . . . . . . . 223
Algorithm 9.16
RC4(n,m) pseudo-random generator . . . . . . . . . . . . . 223
Algorithm 9.17
RC4(n,m) pseudo-random generator, old version . . . . . . 224
Algorithm 9.18
RC4A pseudo-random generator . . . . . . . . . . . . . . . 225
Algorithm 9.19
RC4 key scheduling . . . . . . . . . . . . . . . . . . . . . 227
Algorithm 9.20
Paul’s suggestion for key scheduling
. . . . . . . . . . . . 228
Algorithm 10.1
Trivium key stream generation . . . . . . . . . . . . . . . . 230
Algorithm 10.2
Trivium key scheduling
. . . . . . . . . . . . . . . . . . . 231
Algorithm 11.1
The Blum-Micali generator
. . . . . . . . . . . . . . . . . 243
Algorithm 11.2
Discrete logarithm generator . . . . . . . . . . . . . . . . . 243
Algorithm 11.3
The Blum-Blum-Shub generator . . . . . . . . . . . . . . . 244
Algorithm 11.4
Enhancing the success probability . . . . . . . . . . . . . . 245
Algorithm 11.5
Montgomery reduction . . . . . . . . . . . . . . . . . . . . 250
Algorithm 11.6
Variation of the Blum-Blum-Shub generator for use with
Montgomery reduction . . . . . . . . . . . . . . . . . . . . 251
Algorithm 11.7
A variation of the Blum-Micali generator that outputs j
bits per step
. . . . . . . . . . . . . . . . . . . . . . . . . 252
Algorithm 11.8
The RSA generator . . . . . . . . . . . . . . . . . . . . . . 253
Algorithm 11.9
The Fisher-Stern generator . . . . . . . . . . . . . . . . . . 255
Algorithm 11.10 The QUAD cipher . . . . . . . . . . . . . . . . . . . . . . 256
Algorithm 12.1
Sideway addition based on table look-up
. . . . . . . . . . 262
Algorithm 12.2
Sideway addition (64 bit words) . . . . . . . . . . . . . . . 263
Algorithm 12.3
Sideway addition Harley-Seal method . . . . . . . . . . . . 264
Algorithm 12.4
Winograd’s algorithm for multiplying small matrices . . . . 279

List of Algorithms
xix
Algorithm 12.5
Strassen’s algorithm to multiply 2 × 2 matrices . . . . . . . 280
Algorithm 12.6
Pan’s matrix multiplication
. . . . . . . . . . . . . . . . . 286
Algorithm 12.7
Multiplication of 64 × 64 binary matrices . . . . . . . . . . 287
Algorithm 12.8
Multiplication of 8 × 8 binary matrices (MXOR) . . . . . . 288
Algorithm 13.1
Evaluating the Jacobi symbol
. . . . . . . . . . . . . . . . 301
Algorithm 13.2
LLL basis reduction . . . . . . . . . . . . . . . . . . . . . 302
Algorithm 13.3
Coppersmith’s method (univariate case) . . . . . . . . . . . 303
Algorithm 14.1
Choosing a random primitive element of Fq . . . . . . . . . 307
Algorithm 15.1
Wald’s sequential test
. . . . . . . . . . . . . . . . . . . . 320
Algorithm 17.1
Sideway addition mod 2 . . . . . . . . . . . . . . . . . . . 340
Algorithm 17.2
A weak variation of the RC4 pseudo-random generator . . . 342
Algorithm 18.1
Choosing a random de Bruijn sequence . . . . . . . . . . . 351
Algorithm 18.2
Binary Decision Diagrams: The ternary-and operator . . . . 353
Algorithm 18.3
Binary Decision Diagrams: The constrain operator . . . . . 355
Algorithm 18.4
Sideway addition for sparse words . . . . . . . . . . . . . . 358

Chapter 1
Introduction to Stream Ciphers
1.1 History I: Antique Ciphers
The art of writing secret messages is very old. In the early days few people could
write, so effectively every text was encrypted. It took a millennium for true cryp-
tosystems to appear. An early example was the Scytale (σκνταλη) which was
used by the Spartanians in the Persian wars (3rd century BC). Cryptography has
been reinvented many times independently. For an extensive history of the subject,
see [141].
Another cipher was used by the Roman emperor Gaius Julius Caesar. Sueton
writes:
Exstant et [epistolae] ad Ciceronem, item ad familiares de rebus, in quibus, si
qua occultius perferenda erant, id est sic structo litterarum ordine, ut nullum
verbum efﬁci poset; quae si qui investigare et persequi velit, quartam elemen-
torum litteram, id est D pro A et perinde reliquas commutet.
In English this reads:
There exist [letters from Caesar] to Cicero and his friends in which he uses a
cipher, when something has to be transmitted conﬁdentially, i.e. he changed
the order of the letters in such a way that no word could be recognized. If one
wants to read the content, he must convert the fourth letter, i.e. D, into an A
and must proceed with the other letters in the same way.
Ancient cryptology did not distinguish between the algorithm used for encryption
(the cipher) and the secret key. It took more than a millennium for the modern dis-
tinction between cipher and key to be introduced. In 1883 Kerckhoffs [146] stated
his famous principle: The security of an encrypted message must not rely on the
security of the encryption algorithm, but only on the security of the secret key.
History has proved Kerckhoffs’ principle to be true many times. In modern cryp-
tography we always require that the cipher has to be public and that there is public
research about its security. Many people have thought that they could violate the
principle and use a secret cipher. The result has always been the same: sooner or
A. Klein, Stream Ciphers, DOI 10.1007/978-1-4471-5079-4_1,
© Springer-Verlag London 2013
1

2
1
Introduction to Stream Ciphers
later (most times sooner) the cipher was leaked to the public and usually the cipher
had some serious ﬂaws.
We transform Caesar’s cipher into a “modern” cipher with a key by declaring that
the cipher is the substitution of each letter by another and that the key should be a
permutation of the alphabet which the sender and the receiver have to agree on. This
class of ciphers is called monoalphabetic. The key space has size 26! ≈288 which
is, even for modern computers, too big to do an exhaustive search.
However, with the development of statistics it became clear that simple monoal-
phabetic ciphers can be broken by analyzing letter frequencies. At ﬁrst, this was
only known to some experts in the military and the secret service, but in time the
approach became publicly known. In the 19th century attacks against monoalpha-
betic ciphers had become a popular theme in adventure literature (see Edgar Allan
Poe [213] or Arthur Conan Doyle [81]).
So the simple idea of the monoalphabetic cipher needs an extension. There are
three ways to obfuscate the letter frequency.
• In a homophone cipher we assign several ciphertext symbols to each letter. Com-
mon letters like e are assigned many different ciphertext symbols and rare letters
like z get only a few. Each time we want to encode a letter we choose one of the
associated ciphertext symbols at random.
The Beale cipher [275], which is probably the most famous cryptogram in
history, is of this type. The oldest known usage of a homophone cipher is dated
at 1401 (see [141]).
• In a polyalphabetic cipher one uses very simple substitutions for each letter (nor-
mally cyclic shifts or involutions), but the substitution is changed for every letter
in a previously agreed way. Changing the substitution masks the redundancy in
the plaintext.
The oldest use of a polyalphabetic cipher is dated at 1568 (see [141]). The
Enigma machine, which is famous for its role in the second world war (see
Sect. 1.3), is a sophisticated example of a polyalphabetic cipher.
• In a polygraphic cipher one groups the letters in blocks and uses a substitution on
the block. This masks the letter frequency and, if the block size is large enough,
blocks will almost never repeat, which is a good defense against attacks based on
the redundancy in the plaintext.
Polygraphic ciphers are relatively new. The Playfair cipher, which was in-
vented in 1854 by Charles Wheatstone (see [141]), is the oldest known example.
All three approaches provide security against simple attacks based on letter fre-
quency. Homophone ciphers have the disadvantage that the ciphertext is longer than
the plaintext, which is unacceptable in many applications. Furthermore they are not
well suited for automatic encryption, which is the reason that homophone ciphers
do not play a role in modern cryptography.
The other two approaches work well. The modern descendants of polyalphabetic
ciphers are known as stream ciphers while the polygraphic ciphers are the ancestors
of the modern block ciphers.

1.2
Lessons from History: The Classiﬁcation of Ciphers
3
SECRETSECRETSECRETSECRETSECRETSECRETSECRETSECRETSECRET
MANYYEARSAGOICONTRACTEDANINTIMACYWITHAMRWILLIAMLEGRAND
EEPPCWETJEYSKTSFXTRGLIFRRARVZQSGANMLLCDVOMNCMSQNVKJEPU
Fig. 1.1 Encrypting a text with a Vigenère cipher
1.2 Lessons from History: The Classiﬁcation of Ciphers
To get a feeling for the modern ciphers, it helps to understand their historic counter-
parts. The most famous polygraphic cipher is the Vigenère cipher. It is named after
the French cryptologist BLAISE DE VIGENÈRE, but it is older (see [141]).
The idea is simply to change the width of the cyclic-shift used in the Caesar
cipher for every letter. One selects a keyword, for example “SECRET”. On the ﬁrst
letter of the plaintext we apply the cyclic shift that would move A to the ﬁrst letter
of the keyword. (In the example we would apply the shift A →S, B →T , ...,
Z →R.) On the second letter we apply the shift that maps A to the second letter
of the keyword and so on. After we reach the last letter of the keyword we then go
back and use the ﬁrst letter again.
Example 1.1 Let us encrypt the ﬁrst sentence of Edgar Allan Poe’s novel “The
Gold-Bug”. The keyword is “SECRET”. In Fig. 1.1 you see in the ﬁrst row the
repeated keyword, in the second row the plaintext, and in the third row the cipher-
text.
The ﬁrst few characters already demonstrate that the Vigenère cipher can map
different plaintext characters to the same character in the ciphertext and that dif-
ferent characters in the ciphertext may encode the same plaintext character. Thus it
prevents the simple cryptanalysis that works against the monoalphabetic ciphers.
The decryption is similar: one must simply apply the reverse shift. As an aid
for carrying out the encryption and decryption one can use the Vigenère tableau
(Table 1.1) which shows the results of all possible shifts.
The Vigenère cipher is easily susceptible to cryptographic attacks that ﬁrst re-
cover the length of the keyword (such as the Kasiski test and Friedman’s coincidence
index, see for example [259]). Here the attacker must solve several Caesar ciphers,
which can be done by searching for the most frequent letter (usually corresponding
to the letter E).
Nevertheless the Vigenère cipher has some interesting features:
• If the keyword is completely random and has the same length as the plaintext,
one obtain the ‘one-time pad’, which is unconditionally secure.1 One can think
1This is an important point that is often missed. Cryptography and proofs have a very special
relation.
What does it mean to say that a one-time pad is provably unconditionally secure? It is of course
pointless to try to guess a pattern in a truly random sequence. This is exactly what the proof says.
However, there are some rare examples where people try to use a one time-pad, but use a (weakly)

4
1
Introduction to Stream Ciphers
Table 1.1 The Vigenère tableau
a b c d e f g h i j k l m n o p q r s t u v w x y z
a
A B C D E F G H I J K L M N O P Q R S T U V W X Y Z
b
B C D E F G H I J K L M N O P Q R S T U V W X Y Z A
c
C D E F G H I J K L M N O P Q R S T U V W X Y Z A B
d
D E F G H I J K L M N O P Q R S T U V W X Y Z A B C
e
E F G H I J K L M N O P Q R S T U V W X Y Z A B C D
f
F G H I J K L M N O P Q R S T U V W X Y Z A B C D E
g
G H I J K L M N O P Q R S T U V W X Y Z A B C D E F
h
H I J K L M N O P Q R S T U V W X Y Z A B C D E F G
i
I J K L M N O P Q R S T U V W X Y Z A B C D E F G H
j
J K L M N O P Q R S T U V W X Y Z A B C D E F G H I
k
K L M N O P Q R S T U V W X Y Z A B C D E F G H I J
l
L M N O P Q R S T U V W X Y Z A B C D E F G H I J K
m
M N O P Q R S T U V W X Y Z A B C D E F G H I J K L
n
N O P Q R S T U V W X Y Z A B C D E F G H I J K L M
o
O P Q R S T U V W X Y Z A B C D E F G H I J K L M N
p
P Q R S T U V W X Y Z A B C D E F G H I J K L M N O
q
Q R S T U V W X Y Z A B C D E F G H I J K L M N O P
r
R S T U V W X Y Z A B C D E F G H I J K L M N O P Q
s
S T U V W X Y Z A B C D E F G H I J K L M N O P Q R
t
T U V W X Y Z A B C D E F G H I J K L M N O P Q R S
u
U V W X Y Z A B C D E F G H I J K L M N O P Q R S T
v
V W X Y Z A B C D E F G H I J K L M N O P Q R S T U
w
W X Y Z A B C D E F G H I J K L M N O P Q R S T U V
x
X Y Z A B C D E F G H I J K L M N O P Q R S T U V W
y
Y Z A B C D E F G H I J K L M N O P Q R S T U V W X
z
Z A B C D E F G H I J K L M N O P Q R S T U V W X Y
of a stream cipher as an attempt to replace the true random key of the one-time
pad by a pseudo-random key.
• If one uses several periods of the key stream sequence of the stream cipher, attacks
such as the one used against the Vigenère cipher become possible. Therefore the
period of any stream cipher must be high (> 264) and we should use only short
messages (< 232 bits).
• The distinction between block ciphers and stream ciphers is merely a matter of
taste. One can say the Vigenère cipher is a polyalphabetic cipher which change the
biased random sequence. In this case there is a real chance to do some analysis and there are
examples of successful attacks against such pseudo one-time pads.
The one-time pad is a pure cipher, it does not secure the message against active attacks. So
if you want to use a one-time pad, you should consider using it in addition to a perfect MAC to
guarantee the authentication of the message.
Finally, even the best cipher does not help if you use a malfunction protocol. There are a lot
of examples where a system has been broken by ignoring the cipher completely and just using a
protocol failure. Section 9.2.3 contains an interesting example of this kind.
In a nutshell, security proofs are not worthless, but you must carefully check what the proof
exactly says. It is often not what you really want (see also the discussion in Chap. 11).

1.2
Lessons from History: The Classiﬁcation of Ciphers
5
Fig. 1.2 Stream-oriented block cipher modes
SECRETMANYYEARSAGOICONTRACTEDANINTIMACYWITHAMRWILLIAML
MANYYEARSAGOICONTRACTEDANINTIMACYWITHAMRWILLIAMLEGRAND
EEPPCXMRFYESITGNZFIEHRWRNKGXLMNKLPQFHCKNEBSLURITPRZAZV
Fig. 1.3 Encrypting a text with an auto key cipher
encryption function for every letter, but one could also say that it is a polygraphic
cipher which work on blocks of the length of the keyword.
In modern cryptography block ciphers are normally used in a stream-oriented
method. The naive idea of using a block cipher by applying it successively to the
message blocks (ci = E(mi,k)) is called the electronic code book (ECB) mode.
The disadvantage of this mode is that the same plaintext is always encrypted
into the same ciphertext block, which leaks information. Figure 1.2 show three
popular operation modes for block ciphers. In all these modes the block cipher is
used as a source of pseudo-random numbers. For further reference, see [90].
When the important idea is the changing internal state we use the term ‘stream
cipher’, and when it is the division of the plaintext into blocks we use the term
‘block cipher’.
Several variants of the Vigenère cipher have been introduced to deal with the
problem of the short period in the cipher. An interesting idea is the auto key cipher.
In the Vigenère cipher the keyword is repeated until it has the same length as the
message. In an auto key cipher the message itself is used as part of the key
Example 1.2 We encrypt the same text as in Example 1.1 with an auto key cipher.
The keyword is again “SECRET”. In Fig. 1.3 you see the encryption. Note how the
message is used as part of the key.
The Vigenère and auto key ciphers exhibit an important difference, which is used
to classify stream ciphers. Either the cipher generates its key stream independently
from the message or the message becomes a part of the feedback function. In the
ﬁrst case one speaks of a synchronous stream cipher and in the second case one
speaks of a self-synchronizing or asynchronous stream cipher.

6
1
Introduction to Stream Ciphers
Fig. 1.4 A synchronous stream cipher
In general a synchronous stream cipher has the form
xi+1 = f (xi,k),
zi = g(xi,k),
ci = h(zi,mi),
where k denotes the key, xi is the internal state at time i and mi and ci are the ith
bit (letter) in the message and the ciphertext, respectively (see Fig. 1.4). f is the
feedback function of the cipher, g is the key stream extractor and h combines the
key stream (zi)i∈N with the message stream (mi)i∈N. x0 is called the initial state
and may depend on the key.
In most applications we will take the ‘exclusive or’ operation as the combiner
(ci = zi ⊕mi) and the feedback and extraction function do not depend on the key
(xi+1 = f (xi), zi = g(xi)), i.e. the key is only used to choose the initial internal
state x0. In this special case we speak of a binary additive stream cipher.
An important feature of synchronous stream ciphers is that they assure only the
conﬁdentially of the data, but not its integrity. An active attacker can simply ﬂip
the bits of the ciphertext, which ﬂips the corresponding plaintext bits. To prevent
active attacks one needs in addition a message authentication code (MAC). It is
remarkable how many applications fail to observe this simple fact (see for example
the GSM-protocol Chap. 8 or WEP Sect. 9.2.1).
To prevent active attacks and to transmit data over a noisy channel one must use
Algorithm 1.1. The important part is that the error-correcting code must be applied
last (see the lesson from the Enigma code (Sect. 1.4)).
Algorithm 1.1 Submitting data over a noisy channel using a synchronous stream
cipher
1. Compute the hash h of the message m under a cryptographic hash function.
2. Encrypt m using the synchronous stream cipher. Append h to the ciphertext c.
3. Apply an error-correcting code to c∥h and transmit the result.
A self-synchronizing stream cipher generates the keystream as a function of the
key and a ﬁxed number of preceding ciphertext digits (or, what is equivalent, a ﬁxed
number of preceding plaintext digits).

1.2
Lessons from History: The Classiﬁcation of Ciphers
7
Fig. 1.5 A self-synchronizing stream cipher
The encryption has the form
xi = (ci−1,...,ci−t),
zi = g(xi,k),
ci = h(zi,mi),
where the initial state x0 = (c−1,...,c−t) may depend on the key (see Fig. 1.5).
The CFB-mode of block ciphers is an example of a self-synchronizing cipher.
Self-synchronizing stream ciphers have advantages over synchronous stream ci-
phers.
• A deletion or insertion of a bit in the ciphertext will cause only a ﬁnite number
of plaintext bits to change, i.e. the cipher establishes proper decryption automati-
cally after a loss of synchronization (the self-synchronizing property).
• If an attacker ﬂips some bits in the ciphertext, the errors will propagate and several
other bits in the plaintext will ﬂip. Most likely this results in a nonsense text, i.e.
we detect the active attack. So, in contrast to synchronous stream ciphers, one
needs no extra hash function to secure the message against active attacks.
• Since every bit of the plaintext inﬂuences all subsequent bits of the ciphertext, the
statistical properties of the plaintext are dispersed through the ciphertext. Hence
self-synchronizing stream ciphers may be more resistant against attacks based
on redundancy in the plaintext. The reader can try an experiment and attempt to
break a Vigenère cipher and an auto key cipher (see Exercises 17.3 and 17.4).
Most people ﬁnd the ﬁrst task easier.
However, self-synchronizing stream ciphers also have disadvantages. The sepa-
ration of key stream generation and encryption in synchronous stream ciphers makes
the implementation easier. It also makes the analysis of the cipher easier and helps
in security proofs. So self-synchronizing stream ciphers may be more secure, but
there is always a risk of large undetected security holes, since we understand them
less. Most modern stream ciphers are synchronous stream ciphers.

8
1
Introduction to Stream Ciphers
1.3 History II: The Golden Age of Stream Ciphers
At the beginning of the 20th century cryptography took the step from simple cryp-
tosystems which can be applied manually to complex systems which need machines
to implement. Since this time, the question of whether a given cipher can ﬁt on the
available hardware has always been important for the success of the cipher.
The challenge was to implement the new cryptosystems on, for example, me-
chanical typewriters and telegraphs. People ﬁrst began to experiment with electric
typewriters where the keys connected to the output in some random fashion (Hebren
1915). However, such ciphers are only monoalphabetic. The next step was to put the
wires on a rotor that change its position after each letter. Hebren advertised such a
cipher in 1921 as “unbreakable”, but it was still very weak. Combining several rotors
of different speed ﬁnally gave a satisfactory system.
In the 1920s rotor machines were independently invented several times in dif-
ferent countries (Hugo Alexander, Netherlands; Arvid Gerhard Damm, Sweden;
Arthur Scheribus, Germany) and quickly became a standard for cryptography. The
fact that rotor machines work so well with telegraphy and typewriters, together with
the high level of security that can be achieved by these machines, left almost no
room for other types of cryptosystem.
The most famous rotor machine of all time is the German Enigma machine in-
vented by the engineer Arthur Scheribus, who founded his Chiffriermaschinen Ak-
tiengeselschaft in 1923. Despite all advertisements, the Enigma was not a commer-
cial success at ﬁrst. This changed in 1934 when Hitler started to rearm Germany
and the Wehrmacht decided that the Enigma should become the new cryptography
machine for the German army.
The main difference between the Enigma and other rotor machines of that time is
that it reﬂects the signal at the end and sends it through the rotors a second time. This
effectively doubles the number of rotors, but has the consequence that the cipher
becomes involutionary, i.e. if X is sent to A then A must be sent to X. Being involu-
tionary must not generally be regarded as a disadvantage for a cipher. The fact that
decrypting and encrypting can be done by the same machine can be considered as
positive. In fact many modern ciphers (including all binary additive stream-ciphers)
have this property. However, in the case of the Enigma, it was a serious ﬂaw which,
together with other ﬂaws, made it possible for the allies in the second world war to
break the cipher. The cryptographic success of the allies had a signiﬁcant impact on
the course of the war.
1.4 Lessons from the Enigma
The Enigma had several ﬂaws that could be used in cryptanalysis, but the operators
also made several protocol failures. One interesting aspect is the following (see also
[16]).

1.4
Lessons from the Enigma
9
For each message the operator selects a message key consisting of three letters
αβγ . Then the message key is repeated (αβγ αβγ ) and these six letters are en-
crypted with the current day’s key and transmitted to another station. The repetition
should help to detect transmission errors. This protocol violates the advice given
in Algorithm 1.1 that the error detecting code should always be applied last and in
this case the mistake led to the following attack developed by Polish cryptographers
under Rejewski.
The technique of sending the signal through the rotors twice ensures that the
Enigma applies a permutation of the 26 letters of the alphabet consisting of 13 dis-
joint cycles to the plaintext. This is itself already a weakness, since it is impossible
to map a letter to itself. Thus the ciphertext leaks information about the plaintext,
but we will not discuss this weakness.
Call a permutation of the 26 letters of the alphabet that consists of 13 disjoint
cycles an Enigma permutation. The attack is based on the following lemma.
Lemma 1.1 Let π and π′ be two Enigma permutations. Then for every l ∈N the
permutation ππ′ contains an even number of cycles of length l.
If (αβ) is a transposition in π or π′ then α and β lie in different cycles of ππ′ of
the same length.
Proof The enigma permutations partition the set of letters into parts of the form
{p1,...,p2k} where π contains the involutions (p1p2),(p3p4),...(p2k−1p2k)
while π′ contains the involutions (p2kp1),(p2p3),...,(p2k−2p2k−1).
On the set {p1,...,p2k} we ﬁnd that ππ′ is (p1p3 ...p2k−1)(p2p4 ...p2k). Thus
cycles of length l come in pairs. Furthermore an involution (pipi+1) of π or π′ has
one letter in each cycle.
□
Lets assume that the following 6-tuples are encrypted session keys from one day.
HKI CED
HTN CYA
HGI CCD
DPN BUA
WDB XAU
SHZ SHV
QGU QCN
UQT DBG
DEF BGH
EJN GOA
ZFN WLA
RDC OAY
GPR IUO
MSO EDR
YWW MWT
KNA LQM
SGK SCE
VFY ULC
BAM NZL
BAJ NZI
NIT PFG
JMH VTB
XPH AUB
TTT JYG
KWS LWW
ERV GXX
JTT VYG
PNJ TQI
ILM KNL
DSP BDJ
AIF ZFH
EAY GZC
ZAM WZL
ZFP WLJ
IOS KPW
UBC DRY
CZK RSE
LWM FWL
BFO NLR
VNV UQX
TWM JWL
JPF VUH
LST FDG
KST LDG
VNV UQX
JPF VUH
PND TQS
YNJ MQI
GYJ IJI
PSB TDU
What can we derive from this observation? Denote by π1, ..., π6 the six unknown
permutations the Enigma performs with the day’s key. Let us try to reconstruct π1π4.
In the collection of the 50 session keys 24 different letters occur in the ﬁrst position.
We know from the observed message AIF ZFH that π1π4 maps A to Z and so on.
Only the images of F and O are missing, but Lemma 1.1 gives us enough extra
information to ﬁll the gap. One obtains
π1π4 = (AZWX)(CROH)(BNPTJVUD)(EGIKLFYM)(Q)(S)

10
1
Introduction to Stream Ciphers
This is a lot of information. We certainly know that π1 and π4 interchange Q and
S, so we have already determined some part of the session keys. For the other parts
our uncertainly has decreased dramatically. For example, we know that if we see an
A in the ﬁrst position the session key must start with either C, R, O or H.
In addition the knowledge of π1π4 can help to determine the wires on the ﬁrst
rotor of the Enigma. (This was important before the allies were able to capture an
Enigma machine.) Operation failures such as choosing weak keys like AAA help
the cryptanalyst further. This short sketch is of course not a full cryptanalysis of
the Enigma, but it shows how Polish cryptanalysts and later the English team at
Bletchley Park could attack the Enigma. It also shows that choosing the wrong order
of encryption and error detecting code is a serious mistake that helps the attacker.
More about the cryptanalysis of the Enigma can be found in [220, 268].
At the time this attack was of course a military secret, but now the second world
war is long over, the enemies have become friends and the military secret has be-
come a textbook exercise. So why do people continue to repeat the errors from
the Enigma and implement the same protocol failure in our modern mobile phones
(Chap. 8) and computers (Sect. 9.2.1)?
1.5 History III: Towards Modern Cryptography
In the days of the rotor machines stream ciphers dominated cryptography. There was
almost nothing else. Being at the very top is not always a good position, you can only
lose. For stream ciphers modern cryptography is the story of decline. However, it
is better to say that modern cryptography is a story of normalization. Block ciphers
were underestimated for many centuries.
The change to electronic devices was no problem for stream ciphers. (Linear)
feedback shift registers are perfectly suited to the new hardware and give satisfactory
results.
The ﬁrst setback for stream ciphers was the data encryption standard (DES) ci-
pher in 1973, a block cipher. This was the ﬁrst time that a cryptosystem had become
a public standard. Naturally it drew much research interest.
In 1977, with the RSA cryptosystem, the ﬁrst example of asymmetric cryptog-
raphy was published. Asymmetric cryptography is today an important part of many
protocols.
With the success of modern computers stream ciphers encountered more prob-
lems. A processor loads a word (or a block) into its registers, manipulates it and
then writes it back. This ﬁts perfectly with the idea of block ciphers, but less to the
idea of a stream cipher. Many modern block ciphers (IDEA, AES, ...) are perfectly
adapted to software implementation.
In the 1990s stream ciphers had a renaissance in mobile devices (telephones,
wireless LAN, bluetooth). The ﬁrst generation of mobile devices had no general
purpose processor and energy efﬁciency had top priority. Stream ciphers were per-
fect for this job. However, this renaissance was not without troubles.

1.6
When to Use Stream Ciphers?
11
One point was that often a design criterion for the new ciphers was: “Do not make
it too safe.” This was especially true for the ciphers A5/1 and A5/2 used in GSM
mobile phones (see Chap. 8), and the ﬁrst WLAN standard WEP (see Chap. 9) also
contains some needless weaknesses. This gave many people the wrong impression
that stream ciphers must be insecure.
The second point is that mobile devices rapidly become more powerful. A mod-
ern smart phone is just a very small general purpose computer. Together with the
more powerful embedded processors, block ciphers become a more and more at-
tractive solution for these devices.
1.6 When to Use Stream Ciphers?
Block ciphers are better understood than stream ciphers. The main reason is that
for block ciphers it is easy to modularize the problem. One can study the operation
mode without looking at the underlying cipher or one can look at a single round of
a DES-like cipher and begin to study APN-functions. For stream ciphers there is al-
most no such modularity and often the key scheduling and the keystream generation
interact in a complicated way. The result is that block ciphers are easier to use.
So if one has no special requirement, my advice is always: use a standard block
cipher (AES is perfect), but use it in a stream cipher mode. (The ECB mode is
worse, but the CBC mode also loses against most stream cipher modes with respect
to information leakage and parallelism. This is especially true if you compare the
CBC mode with the modern counter mode CTR, see Exercise 17.1.)
However, there are applications where this is not practicable (otherwise I would
not have written this book). In embedded devices the goal is to save gates and energy.
A shift register-based stream cipher needs fewer gates by several magnitudes than
even a simple embedded CPU.
Stream ciphers can reach a higher speed than block ciphers. On a standard com-
puter a factor of 3 is not unusual if one compares an implementation of AES with a
stream cipher designed for software implementation. If one is willing to use special-
ized hardware, even more is possible, for example the cipher Trivium (Sect. 10.1)
can generate 64 bits per clock cycle. This is far higher than anything which is pos-
sible with block ciphers.
At the time of writing, hard disk space is growing faster than CPU speed. This
may be an indication that in future we will have a greater need for high speed ci-
phers. Another area in which stream ciphers may ﬁnd an important application are
RFID devices. Here low energy consumption is important and stream ciphers beat
block ciphers in this aspect.
1.7 Outline of the Book
The book is divided into ﬁve parts. Part I covers the theory of shift register-based
stream ciphers. Shift registers are perfect for specialized hardware and, despite all

12
1
Introduction to Stream Ciphers
attempts to design good software stream ciphers, shift register-based stream ciphers
are still the most important class of stream cipher.
Chapter 2 is a survey of linear feedback shift registers. It develops the theory of
generating functions, describes the famous Berlekamp-Massey algorithm and covers
implementation aspects.
Pure linear functions are weak as cryptographic functions. So one must use non-
linear combinations of linear feedback shift registers to obtain good stream ciphers.
Chapter 3 contains the basic concepts of non-linear combinations of linear feedback
shift registers. It gives an overview of different attack classes and introduces basic
concepts such as algebraic complexity and correlation immune functions.
With Chap. 4 we begin the cryptanalysis of stream ciphers. The ﬁrst attack class
we study are correlation attacks. These attacks try to use statistical abnormalities to
recover a part of the internal state. This attack principle is old, but in recent years
many improvements have been made.
Chapter 5 covers a relative new type of attack. Binary Decision Diagram-based
attacks were introduced in 2002 by M. Krause. The idea behind these attacks is
remarkably simple. The set of internal states that is consistent with the observed
output sequence describes a Boolean function. BDDs are a tool to efﬁciently handle
the Boolean functions. The attack successively computes BDDs that describe the
internal state with increasing accuracy until it ﬁnally yields a unique solution. This
is more efﬁcient that the complete key search, but requires a lot of memory.
Chapter 6 covers algebraic attacks. The idea of these attacks is to express the
stream cipher as a system of non-linear equations. The chapter has a short introduc-
tion to the branch of computer algebra which is used to solve such equations (es-
pecially Gröbner bases). Examples of algebraic attacks against real world ciphers
complete the chapter.
Chapter 7 will introduce stream ciphers with irregular clock control. Irregular
clock control is an attractive way to create strong ciphers and some of the sim-
plest examples of stream ciphers with irregular clock control are still unbroken. The
drawback is that it is very hard to prove any property of the cipher, which makes
undetected weaknesses more likely. Ciphers with irregular clock control are also es-
pecially susceptible to side channel attacks. This may be the reason why most real
world ciphers have regular clock control.
Part II contains the description and cryptanalysis of some special ciphers.
The ciphers A5/1 and A5/2 which are used in GSM security are presented in
Chap. 8. They are shift register-based and we use them as real world examples for
the attacks described in the ﬁrst part.
Chapter 9 is about the cipher RC4. This cipher was optimized for use on 8-bit
processors and is not based on shift registers. It is especially famous since it is used
in the wireless LAN standard. When used correctly, RC4 is unbroken, but the key
scheduling of RC4 is weak and its careless use allows related key attacks.
The ECRYPT Stream Cipher Project [85] was run from 2004 to 2008 to identify a
portfolio of promising new stream ciphers. Chapter 10 describes some of the ciphers
from this project as examples of modern stream cipher design.
Chapter 11 covers some ciphers which are provable as secure as some (hopefully)
hard number theoretic problem. These ciphers are very secure, but unfortunately

1.7
Outline of the Book
13
slow in comparison to other ciphers in this book. They are mostly used as part of a
key generation protocol.
I assume that the reader is familiar with basic mathematics (number theory, al-
gebra, combinatorics and statistics), but sometimes I have had to use more exotic
concepts. The chapters in Part III collect some background material.
Exercises with solutions can be found in Part IV.
Implementation for most algorithms covered by this book can found at http://
cage.ugent.be/~klein/streamcipher. Chapter 19 gives an overview of the programs.
To document the programs I wrote a new literate programming tool. It is freely
available and, perhaps after reading this book, the reader may want to use it for their
own projects. Chapter 20 contains the user manual for this tool.

Part I
Shift Register-Based Stream Ciphers

Chapter 2
Linear Feedback Shift Registers
2.1 Basic Deﬁnitions
In a hardware realization of a ﬁnite state machine it is attractive to use ﬂip-ﬂops
to store the internal state. With n ﬂip-ﬂops we can realize a machine with up to
2n states. The update function is a Boolean function from {0,1}n to {0,1}n. We
can simplify both the implementation and the description if we restrict ourselves to
feedback shift registers.
In a feedback shift register (see Fig. 2.1) we number the ﬂip-ﬂops F0, ..., Fn−1.
In each time step Fi takes the value of Fi−1 for i > 0 and F0 is updated according
to the feedback function f : {0,1}n →{0,1}. We will always assume that the value
of Fn−1 is the output of the shift register.
Feedback shift registers are useful tools in coding theory, in the generation of
pseudo-random numbers and in cryptography. In this chapter we will summarize
all results on linear feedback shift registers relevant to our study of stream ciphers.
For other applications of feedback shift registers I recommend the classical book of
Solomon W. Golomb [115].
Mathematically the sequence (ai)i∈N generated by a shift register is just a se-
quence satisfying the n-term recursion
ai+n = f (ai,...,ai+n−1).
(2.1)
This deﬁnition is, of course, not restricted to binary sequences and most of our
results will hold for shift register sequences deﬁned over any (ﬁnite) ﬁeld or some-
times even for sequences deﬁned over rings.
We will call a shift register linear if the feedback function is linear. Thus:
Deﬁnition 2.1
A linear feedback shift register (LFSR) sequence is a sequence
(ai)i∈N satisfying the recursion
ai+n =
n−1

j=0
cjai+j.
(2.2)
A. Klein, Stream Ciphers, DOI 10.1007/978-1-4471-5079-4_2,
© Springer-Verlag London 2013
17

18
2
Linear Feedback Shift Registers
Fig. 2.1 A feedback shift
register
Since the next value depends only on the preceding n values, the sequence must
become periodic. The state (ai,...,ai+n−1) = (0,...,0) leads to the constant se-
quence 0, thus the period of an LFSR sequence over Fq can be at most qn −1. If in
addition c0 ̸= 0, we can extend the sequence backwards in time via
ai = c−1
0

ai+n −
n−1

j=1
cjaj+n

which proves that it is ultimately periodic.
As we have already seen in the introduction, a necessary condition for the secu-
rity of a system is that the generated pseudo-random sequence has a large period.
Thus the sequences of maximal period are of special interest.
Deﬁnition 2.2 An LFSR sequence over Fq with period qn −1 is called an m-
sequence (maximal sequence).
2.2 Algebraic Description of LFSR Sequences
In this section we develop an algebraic description of LFSR sequences. We espe-
cially want to ﬁnd a closed formula for an LFSR sequence. One way to reach this
goal is to study the companion matrix of the LFSR sequence. We have
⎛
⎜⎜⎜⎝
ak+1
...
ak+n−1
ak+n
⎞
⎟⎟⎟⎠=
⎛
⎜⎜⎜⎝
0
1
0
...
...
0
0
1
c0
c1
...
cn−1
⎞
⎟⎟⎟⎠
⎛
⎜⎜⎜⎝
ak
...
ak+n−2
ak+n−1
⎞
⎟⎟⎟⎠
(2.3)
and thus
⎛
⎜⎜⎜⎝
ak
...
ak+n−2
ak+n−1
⎞
⎟⎟⎟⎠=
⎛
⎜⎜⎜⎝
0
1
0
...
...
0
0
1
c0
c1
...
cn−1
⎞
⎟⎟⎟⎠
k ⎛
⎜⎜⎜⎝
a0
...
an−2
an−1
⎞
⎟⎟⎟⎠.
(2.4)
Transforming the companion matrix to Jordan normal form makes it easy to com-
pute the k-th power and transforming it back gives a closed formula for the LFSR
sequence.
In the next section we will take another approach that is based on generating
functions.

2.2
Algebraic Description of LFSR Sequences
19
2.2.1 Generating Functions
This section contains the part of the theory of generating functions that we need, but
for those who want to learn more about generating functions, I recommend [119].
Deﬁnition 2.3 The generating function A(z) associated to a sequence (ai)i∈N is the
formal power series A(z) = ∞
i=0 aizi.
A generating function is useful because it describes an entire sequence with a
single algebraic object.
By the recursion (2.2) we ﬁnd:
A(z) −
n−1

j=0
cjA(z)zn−j = g(z)
⇐⇒
A(z)

1 −
n−1

j=0
cjzn−j

= g(z)
(2.5)
for some polynomial g(z) of degree at most n −1.
The polynomial 1 −n−1
j=0 cjzn−j is important enough to deserve a name.
Deﬁnition 2.4 For an LFSR sequence with recursion formula (2.2) we call
f (z) = zn −
n−1

j=0
cjzj
(2.6)
the feedback polynomial of the LFSR. The reciprocal polynomial1 is denoted by
f ∗(z) = znf
1
z

= 1 −
n−1

j=0
cjzn−j.
(2.7)
From Eq. (2.5) we derive a closed formula for the generation function of an LFSR
sequence:
A(z) = g(z)
f ∗(z).
(2.8)
For the derivation of the closed form of ai it is useful to begin with the case
where the feedback polynomial f (z) has no multiple roots.
1f ∗(z) is sometimes called the feedback polynomial. As the literature has not adopted a unique
notation, it is important to check which notation is being used.

20
2
Linear Feedback Shift Registers
2.2.2 Feedback Polynomials Without Multiple Roots
Let f (z) be a feedback polynomial without multiple roots and let ξ1,...,ξn be the
different zeros of f (z). Then f ∗(z) = n
j=1(1 −zξj) and thus we get the partial
fraction decomposition
A(z) = g(z)
f ∗(z) =
n

j=1
bj
1 −zξj
.
(2.9)
All we need to obtain a closed formula from the partial fraction decomposition
is the geometric sum
∞

i=0
zi =
1
1 −z
and thus
A(z) =
n

j=1
bj
1 −zξj
=
n

j=1
bj
∞

i=0
(ζjz)i
=
∞

i=0
 n

j=0
bjζ i
j

zi.
(2.10)
This gives us the closed formula
ai =
n

j=0
bjζ i
j
(2.11)
for the LFSR sequence.
Formula (2.11) holds if the feedback polynomial has no multiple roots. For sep-
arable irreducible feedback polynomials we can transform (2.11) to the following
theorem. Note that over ﬁnite ﬁelds and ﬁelds with characteristic 0 every polynomial
is separable. We will not deal with other ﬁelds in this book.
Theorem 2.1 Let (ai)i∈N be an LFSR sequence over Fq and let ξ be a zero of the
irreducible feedback polynomial. Then
ai = TrFqn/Fq

αξi
(2.12)
for some α ∈Fqn.

2.2
Algebraic Description of LFSR Sequences
21
Proof We have already proved that the sequence (ai)i∈N has a unique closed
form (2.11). Since the feedback polynomial is irreducible, its zeros have the form ξθ
where θ is an automorphism of Fqn/Fq. But aθ
i = ai for all i. Thus Equation (2.11)
is invariant under θ. Therefore the coefﬁcients bj are conjugated, i.e.
ai =

θ∈Aut(Fqn/Fq)
αθ
ξθi = TrFqn/Fq

αξi
.
□
Corollary 2.1 Under the conditions of Theorem 2.1 the period of the sequence is
the multiplicative order o(ξ) of ξ.
As already mentioned in the previous section, the period qn −1 is of special
interest. Thus the following feedback polynomials are special.
Deﬁnition 2.5 An irreducible polynomial of degree n over Fq is primitive if the
order of its zeros is qn −1.
2.2.3 Feedback Polynomials with Multiple Roots
Now we want to determine all possible periods of LFSR sequences.
First we consider the easy case where the feedback polynomial is reducible, but
has no multiple roots. In this case we can factor the feedback polynomial f and
write the generating function (see Eq. (2.8)) of (ai)i∈N as
A(z) = g(z)
f ∗(z) =
k

j=1
gj(z)
f ∗
j (z)
where the polynomials fj are the different irreducible factors of the feedback poly-
nomial f .
Thus the sequence (ai)i∈N can be represented as a sum of k LFSR sequences
(a(j)
i
)i∈N with irreducible feedback polynomial. By Corollary 2.1 the period of
(a(j)
i
)i∈N divides qnj −1 where nj = degfj and hence the sequence (ai)i∈N =
k
j=1(a(j)
i
)i∈N has period
p = lcm(π1,...,πk)
where πj is the period of (a(j)
i
)i∈N.
To analyze the case of multiple roots we need an additional tool. In this case the
partial fraction decomposition of the generation function yields:
A(z) = g(z)
f ∗(z) =
n1

j=1
bj,1
1 −zξj
+
n2

j=1
bj,2
(1 −zξj)2 + ··· +
nr

j=1
bj,r
(1 −zξj)r

22
2
Linear Feedback Shift Registers
with n1 ≥n2 ≥··· ≥nr ≥nr+1 = 0 where ξnk+1,...,ξnk are roots of f of multi-
plicity k. So to get a closed formula we need in addition the power series of
1
(1−z)k .
We can ﬁnd the power series either by computing the (k −1)th derivative of
1
1−z = ∞
i=0 zi or we use the binomial theorem
(1 + x)r =
∞

i=0
r
i

xi.
For a negative integer we get
1
(1 −z)k =
∞

i=0
−k
i

(−1)izi
=
∞

i=0
k + i −1
i

zi
=
∞

i=0
k + i −1
k −1

zi.
This leads to the closed formula
ai =
n1

j=0
bj,1ζ i
j +
n2

j=0
bj,2
i + 1
1

ζ i
j + ··· +
nk

j=0
bj,k
i + k −1
k −1

ζ i
j
=
n1

j=0
b′
j,1ζ i
j +
n2

j=0
b′
j,2iζ i
j + ··· +
nk

j=0
b′
j,kik−1ζ i
j
(2.13)
where the last transformation uses the fact that
k−1+i
k−1

, k = 1,...,n, is a basis for
the polynomials of degree less than k. Note that the converse is also true. Given a
sequence in the form of Eq. (2.13) we can reverse all previous steps and ﬁnd the
linear recurrence satisﬁed by that sequence.
From Eq. (2.13) we can immediately see the period of the sequence (ai)i∈N.
The power series (ζ i
j)i∈N has a period πi where πi|qnj −1 and nj is the degree
of the minimal polynomial of ζj. And since we are working in Fq, the period of a
polynomial series (ik)i∈N is the characteristic p of Fq. Thus
π = p lcm(π1,...,πk)
where π1,...,πk are the different orders of ζ1,...,ζn1.
We summarize the results of this section in the following theorem.
Theorem 2.2 Let (ai)i∈N be an LFSR sequence over Fq, q = pe. Then the period
π of (ai)i∈N is either
π = lcm(π1,...,πk)
(2.14)

2.2
Algebraic Description of LFSR Sequences
23
where πj|qnj −1 and k
j=1 nj ≤n or
π = p lcm(π1,...,πk)
(2.15)
where πj|qnj −1 and n1 + k
j=1 nj ≤n.
Proof We have already proved that the period π must have either the form (2.14) or
(2.14). Now we prove the converse that for each such π there is an LFSR sequence
with period πa.
Let π be of the form (2.14). Choose ζj ∈Fqnj such that ζj has order πj. Without
loss of generality we may assume that Fq(ζj) = Fqnj , if not just replace nj by a
smaller n′
j. The sequence
xi =
k

j=1
TrFqnj /Fq

ζ i
j

is a linear shift register sequence with feedback polynomial
f (z) =
k
j=1
nj −1

l=0

1 −zζ ql
j

.
The sequence ζj has period π since the “subsequences” ζ i
j and hence TrFqnj /Fq(ζ i
j)
have period πj (1 ≤j ≤k).
If π is of the form (2.15), we ﬁnd that the sequence
xi = i TrFqnj /Fq

ζ i
1

+
k

j=2
TrFqnj /Fq

ζ i
j

is a linear shift register sequence with feedback polynomial
f (z) =
nj −1

l=0

1 −zζ ql
1
2
 k
j=1
nj −1

l=0

1 −zζ ql
j


and period π = p lcm(π1,...,πk). The additional factor p is for the period of the
polynomial i in Fq.
□
2.2.4 LFSR Sequences as Cyclic Linear Codes
Another description of LFSR sequences is based on coding theory.

24
2
Linear Feedback Shift Registers
The LFSR deﬁnes a linear mapping from its initial state (a0,...,an−1) to its
output sequence (ai)i∈N. For ﬁxed N we may interpret the mapping
C : (a0,...,an−1) →(a0,...,aN−1)
as a linear code of length N and dimension n.
A parity check matrix of the code is
H =
⎛
⎜⎜⎜⎜⎜⎜⎜⎝
c0
...
cn−1
−1
0
...
0
0
c0
...
cn−1
−1
0
...
0
...
...
...
0
···
0
c0
...
cn−1
−1
⎞
⎟⎟⎟⎟⎟⎟⎟⎠
.
(2.16)
If we look at a full period of the LFSR, i.e. if we choose N = p, then the resulting
linear code is cyclic and f ∗(z) is its parity check polynomial.
The code C also has a unique systematic generator matrix
G =
⎛
⎜⎝
1
0 cn,0
···
cN−1,0
...
...
...
0
1 cn,n−1
···
cN−1,n−1
⎞
⎟⎠.
(2.17)
We have (a0,...,aN−1) = (a0,...,an−1)G, i.e.
ak =
n−1

i=0
ck,iai.
(2.18)
We will use this linear representation of the element ak in terms of the initial state
in several attacks.
2.3 Properties of m-Sequences
2.3.1 Golomb’s Axioms
Linear shift register sequences of maximal length (m-sequences) have many desir-
able statistical properties.
The best known of these properties is that they satisfy Golomb’s axioms for
pseudo-random sequences [115].
We study a periodic binary sequence (ai)i∈N with period length p. Then the three
axioms for (ai)i∈N to be a pseudo-random sequence are:

2.3
Properties of m-Sequences
25
(G1) In every period the number of ones is nearly equal to the number of zeros,
more precisely the difference between the two numbers is at most 1:

p

i=1
(−1)ai
 ≤1.
(G2) For any k-tuple b, let N(b) denote the number of occurrences of the k-tuple
b in one period.
Then for any k with 1 ≤k ≤log2 p we have
N(b) −N

b′ ≤1
for any k-tuples b and b′.
(G2′) A sequence of consecutive ones is called a block and a sequence of consecu-
tive zeros is called a gap. A run is either a block or a gap.
In every period, one half of the runs has length 1, one quarter of the runs has
length 2, and so on, as long as the number of runs indicated by these fractions
is greater than 1.
Moreover, for each of these lengths the number of blocks is equal to the
number of gaps.
(G3) The auto-correlation function
C(τ) =
p−1

i=0
(−1)ai(−1)ai+τ
is two-valued.
Axiom (G1) is called the distribution test, Axiom (G2) is the serial test and
Axiom (G3) is the auto-correlation test. In [115] Golomb uses (G2′) instead of
(G2). Axiom (G2) was introduced in [169] and is in some respects more useful than
the original axiom.
The distribution test (G1) is a special case of the serial test (G2). However, (G1)
is retained for historical reasons, and sequences which satisfy (G1) and (G3), but
not (G2), are also important.
Theorem 2.3 (Golomb [115]) Every m-sequence satisﬁes (G1)–(G3).
Proof An m-sequence is characterized by the fact that the internal state of the lin-
ear feedback shift register runs through all elements of Fn
2\{(0,...,0)}. Since at
any time the next n output bits form the current internal state, this means that
(at,...,at+n−1) runs over all elements of Fn
2\{(0,...,0)} where t runs from 0 to
2n −1. This proves
N(a1,...,ak) =

2n−k −1
for a1 = ··· = ak = 0,
2n−k
otherwise.

26
2
Linear Feedback Shift Registers
Thus an m-sequence passes the serial test for blocks of length up to n and hence
it satisﬁes (G2) and (G1).
A run of length k is just a subsequence of the form 1,0,0,...,0,1 with k ze-
ros and a block of length k is a subsequence of the form 0,1,1,...,1,0. We have
already proved that an m-sequence contains exactly 2n−k−2 subsequences of type
k ≤n −2. This is the statement of (G2′).
We ﬁnd C(0) = 2n −1 as one value of the auto-correlation function. We now
prove C(τ) = −1 for 0 < τ < 2n −1. By Theorem 2.1 we have ai = TrF2n/F2(αξi)
for a primitive element ξ of F2n and ai+τ = TrF2n/F2(α′ξi). Note that we have the
same ξ in both equations, since (ai)i∈N and (ai+τ)i∈N satisfy the same recurrence.
Thus ai + ai+τ = TrF2n/F2((α + α′)ξi) and hence (ai + ai+τ)i∈N is also an m-
sequence. By (G1) we have
C(τ) =
p−1

i=0
(−1)ai+ai+τ = −1.
Thus the auto-correlation function takes just the two values 2n −1 and −1.
□
Besides the Golomb axioms, m-sequences also satisfy other interesting equa-
tions:
Theorem 2.4 Every m-sequence satisﬁes:
(a) For every 0 < k < 2n −1 there exists a δ for which
ai + ai+k = ai+δ
for all i ∈N. This is called the shift-and-add property.
(b) There exists a τ such that
ai2j +τ = ai+τ
for all i,j ∈N. This is called the constancy on cyclotomic cosets.
Proof We have already used and proved the shift-and-add property when we demon-
strated that an m-sequence satisﬁes the auto-correlation test.
By Theorem 2.1 we know that ai = TrF2n/F2(αξi) for some α ∈F2n and a prim-
itive ξ ∈F2n. We choose τ such that ξτ = α−1.
Then
ai+τ = TrF2n/F2

αξi+τ
= TrF2n/F2

ξi
= TrF2n/F2

ξi2j 
since x →x2j is an automorphism of F2n/F2
= TrF2n/F2

αξi2j +τ
= ai2j +τ.
□

2.3
Properties of m-Sequences
27
The shift-and-add property is of special interest since it characterizes the m-
sequences uniquely.
Theorem 2.5 Every sequence which satisﬁes the shift-and-add property is an m-
sequence.
Proof Let A = (ai)i∈N be a sequence of period p which has the shift-and-add prop-
erty. Then the p shifts of the sequence, together with the zero sequence, form an
elementary Abelian group. It follows that p + 1 = 2n for some n ∈N. Let Ak de-
note the sequence (ai+k)i∈N. Any n successive shifts of the sequence A form a basis
of the elementary Abelian group, thus we can write An as a linear combination of
A0, ..., An−1, i.e.
An =
n−1

k=0
ckAk.
Reading the last equation element-wise gives
ai+n =
n−1

k=0
ckai+k,
i.e. the sequence A satisﬁes a linear recurrence. Since the period of A is p = 2n −1,
it is an m-sequence.
□
2.3.2 Sequences with Two Level Auto-Correlation
It is a natural question whether the converse of Theorem 2.3 holds. Golomb conjec-
tured that it does and indicated in a passage of his book (Sect. 4.7 in [115]) that he
had a proof, but the actual answer turns out to be negative (see also [114]).
To put this answer in a bigger context we will study sequences which satisfy Ax-
iom (G3), which have a strong connection to design theory. We make the following
deﬁnition.
Deﬁnition 2.6 Let G be an additive group of order v and let D be a k-subset of G.
D is called a (v,k,λ)-difference set of G, if for every element h ̸= 0 in G the
equation
h = d −d′
has exactly λ solutions with d,d′ ∈D. If G = Z/vZ is a cyclic group we speak of a
cyclic (v,k,λ)-difference set.
The connection between sequences satisfying (G3) and difference sets is given
by the following theorem.

28
2
Linear Feedback Shift Registers
Theorem 2.6 The following statements are equivalent.
(1) There exists a periodic sequence of period length v over F2 with two level auto-
correlation and k ones in its period.
(2) There exists a cyclic (v,k,λ)-difference set.
Proof Let a0,...,av−1 be the period of a sequence with two level auto-correlation.
This means the auto-correlation function satisﬁes C(0) = v and C(τ) = x < v for
1 ≤τ ≤v −1.
For 1 ≤τ ≤v −1 let λτ = |{i | ai = ai+τ = 1,0 ≤i ≤v −1}|. Then
{i | ai = 1,ai+τ = 0,0 ≤i ≤v −1}
 = k −λτ,
{i | ai = 0,ai+τ = 1,0 ≤i ≤v −1}
 = k −λτ,
{i | ai = ai+τ = 0,0 ≤i ≤v −1}
 = v + λτ −2k.
Thus x = λτ + (v + λτ −2k) −2(k −λτ) = v −4(k −λτ), i.e. λτ = λ independent
of τ.
Let D = {i | ai = 1}. Then h = d −d′ has exactly λ = λh solutions with d,d′ ∈
D.
For the converse, let D be a cyclic (v,k,λ)-difference set and deﬁne the sequence
(ai) by ai = 1 if i ∈D and ai = 0 for i /∈D. The deﬁnition of a difference set
says that there are λ indices with ai = ai+τ = 1 and, as above, we obtain C(τ) =
λτ + (v + λτ −2k) −2(k −λτ) = v −4(k −λτ) for 1 ≤t ≤v −1, i.e. the auto-
correlation function is two leveled.
□
If we apply Theorem 2.6 to an m-sequence we obtain a difference set with pa-
rameters v = 2n −1, k = 2n−1 and λ = 2n−3. This is an example of a Hadamard
difference set.
Deﬁnition 2.7 A (4n −1,2n −1,n −1)-difference set is called a Hadamard dif-
ference set.
Hadamard difference sets are of course strongly connected with the well-known
Hadamard matrices.
Deﬁnition 2.8 A Hadamard matrix of order n is an n×n matrix H with entries ±1
which satisﬁes HH t = nI.
The connection is that we can construct a (4n) × (4n) Hadamard matrix from a
(4n −1,2n −1,n −1)-difference set. Let D be a (4n −1,2n −1,n −1)-difference
set over the group G = {g1,...,g4n−1} and deﬁne H = (hi,j)i,i=0,...,4n−1 by
hij =
⎧
⎪⎨
⎪⎩
1
if i = 0 or j = 0,
1
if i,j ≥1 and gi + gj ∈D,
0
if i,j ≥1 and gi + gj /∈D.

2.3
Properties of m-Sequences
29
Then a short calculation shows that H is a Hadamard matrix.
The opposite implication is false since the Hadamard difference set needs the
group G acting on it, whereas a Hadamard matrix need not have any symmetries.
Nevertheless, we see that there are many links between pseudo-random sequences
and other interesting combinatorial objects. (Besides those we have mentioned here,
there are also strong links to designs and coding theory.)
This is not the place to go deeper into the theory of Hadamard Matrices, but
to answer the question we posed at the beginning of this section we mention that
Cheng and Golomb [51] use the Hadamard (127,63,31)-difference set of type E
(given by Baumert [17]) to construct the sequence:
1111101111001111111001001011101010111100011000001001101110011000
110110111010010001101000010101001101001010001110110000101000000
which satisﬁes (G1), (G2) and (G3) but is not an m-sequence.
2.3.3 Cross-Correlation of m-Sequences
Sequences with low correlation are intensively studied in the literature (see [126]
for an overview). Since m-sequences have ideal auto-correlation properties, it is
interesting to study the cross-correlation function for pairs of m-sequences. Many
families of low correlation functions have been constructed in this way.
We use Eq. (2.12) to represent the m-sequence. We can shift it so that we get the
form
ai = Tr

ξi
.
The second m-sequence can be assumed, without loss of generality, to be
bi = Tr

ξdi
for some d with gcd(d,qn −1) = 1. The cross-correlation function depends only
on d. This motivates the following deﬁnition:
Deﬁnition 2.9 Let ξ be a primitive element of Fqn and let ω ∈C be a q-th root of
unity. For each d with gcd(d,qn −1) = 1 we deﬁne the cross-correlation function
θ1,d(τ) =

x∈F∗
qn
ωTr(ξτ x−xd).
We will not need θ1,d in the following, so we keep this section short and just
sketch a link to bent functions, which are important in cryptography. We state the
following without proof.

30
2
Linear Feedback Shift Registers
Theorem 2.7 (Gold [106], Kasami [144]) Let q = 2 and ω = −1. For 1 ≤k ≤n let
e = gcd(n,k). If n/e is odd and d = 2k + 1 or d = 22k −2k + 1, then θ1,d takes the
following three values:
• −1 + 2(n+e)/2 is taken 2n−e−1 + 2(n−e−2)/2 times.
• −1 is taken 2n −2n−e −1 times.
• −1 −2(n+e)/2 is taken 2n−e−1 −2(n−e−2)/2 times.
A pair of m-sequences whose cross-correlation function takes only the three val-
ues −1, −1+2⌊(n+2)/2⌋and −1−2⌊(n+2)/2⌋is called a preferred pair. Theorem 2.7
allows the construction of preferred pairs for n not divisible by 4. For n ≡0 mod 4
preferred pairs do not exist (see [185]).
What makes this interesting for cryptographic purposes is the following. To
avoid attacks such as differential cryptanalysis [26] or linear cryptanalysis [179],
the S-box of a block cipher must be as far from a linear mapping as possible. The
appropriate measure of distance between two functions is provided by the Walsh
transform.
Deﬁnition 2.10 Let f : F2n →F2. The Walsh transform of f is
f W(a) =

x∈F2n
(−1)f (x)+Tr(ax).
The image of f W is the Walsh spectrum of f .
A linear function will contain ±2n in its Walsh spectrum. A function provides
the strongest resistance against a linear cryptanalysis if its Walsh spectrum contains
only values of small absolute value. One can prove that the Walsh spectrum of f
must contain at least a value of magnitude 2⌈n/2⌉(see, for example, [48]).
Deﬁnition 2.11 For even n, a function f : F2n →F2 is bent if f W(x) ≤2n/2 for all
x ∈F2n.
For odd n, we call a function almost bent if f W(x) ≤2(n+1)/2 for all x ∈F2n.
From two m-sequences ai = Tr(ξi) and bi = Tr(ξdi) we construct a Boolean
function f : Fn
2 →Fn
2 by deﬁning f (ξi) = ξdi and f (0) = 0. Applying this con-
struction to a preferred pair with odd n results in an almost bent function.
2.4 Linear Complexity
2.4.1 Deﬁnition and Basic Properties
We have seen in the previous section that m-sequences have very desirable statistical
proprieties. However, as linear functions, LFSR sequences are unusable as crypto-
graphic pseudo-random generators. First note that the ﬁrst n output bits of an LFSR

2.4
Linear Complexity
31
form its initial state, thus a LFSR fails against a known plaintext attack. Even if the
feedback polynomial of the LFSR is unknown to the cryptanalyst, the system is still
insecure. The ﬁrst n output bits give us the initial state and the next n output bits
give us n equations of the form
⎛
⎜⎜⎜⎜⎝
an−1
an−2
...
a0
an
an−1
...
...
...
a2n−2
a2n−3
···
an−1
⎞
⎟⎟⎟⎟⎠
⎛
⎜⎜⎜⎝
cn−1
cn−2
...
c0
⎞
⎟⎟⎟⎠=
⎛
⎜⎜⎜⎝
an
an+1
...
a2n−1
⎞
⎟⎟⎟⎠.
(2.19)
The determination of the unknown feedback coefﬁcients ci therefore only requires
the solution of a system of n linear equations. A matrix of the form given in
Eq. (2.19) is called a Toeplitz matrix. Toeplitz matrices appear in many different
contexts. A lot is known about the solution of Toeplitz systems (see, for exam-
ple, [142]).
In Sect. 2.4.2 we will learn a quadratic algorithm that computes not only the
solution of the system (2.19), but gives us a lot of extra information. So an LFSR is
not secure, even if its feedback polynomial is secret.
On the other hand, it is clear that every periodic sequence can be generated by a
linear feedback shift register—simply take an LFSR of the same size as the period. It
is therefore natural to use the length of the shortest LFSR that generates the sequence
as a measure of its cryptographic security.
Deﬁnition 2.12 The linear complexity L((ai)i=0,...,n−1) of a ﬁnite sequence
a0,...,an−1 is the length of the shortest linear feedback shift register that produces
that sequence.
The following theorem summarizes some very basic properties of linear com-
plexity which follows directly from the deﬁnition.
Theorem 2.8
(a) A sequence of length n has linear complexity at most n:
L

(ai)i=0,...,n−1

≤n.
(b) The linear complexity of the subsequence (ai)i=0,...,k−1 of (ai)i=0,...,n−1 satis-
ﬁes
L

(ai)i=0,...,k−1

≤L

(ai)i=0,...,n−1

.
Proof
(a) The ﬁrst n output bits of a shift register of length n is simply its initial state.
Thus every shift register of length n can produce any ﬁnite sequence of length
n as output.

32
2
Linear Feedback Shift Registers
Fig. 2.2 The sum of two
LFSRs
(b) Any shift register that produces the sequence (ai)i=0,...,n−1 also produces the
subsequence (ai)i=0,...,k−1.
□
It is noteworthy that the bound of Theorem 2.8 (a) is sharp, as the following
example shows.
Lemma 2.1 The sequence 0,...,0,1 (n −1 zeros) has linear complexity n.
Proof Assume that the sequence is generated by a shift register of length k < n.
Since the ﬁrst k symbols are 0 the shift register must be initialized with 0. But any
LFSR initialized with 0 generates the constant sequence 0.
□
Lemma 2.1 demonstrates that a high linear complexity is just a necessary but not
sufﬁcient condition for a good pseudo-random sequence.
Next we study the sum of two LFSR sequences, which is generated by the device
shown in Fig. 2.2.
Theorem 2.9 For two sequences (ai) and (bi) we have
L(ai + bi) ≤L(ai) + L(bi).
Proof Consider the generating functions
A(z) = gA(z)
f ∗
A(z)
and
B(z) = gB(z)
f ∗
B(z)
of the two LFSR sequences. Then the sum (ai + bi) has the generating function
S(z) = A(z) + B(z).
Thus
S(z) = ga(z)f ∗
B(z) + gB(z)f ∗
A(z)
f ∗
A(z)f ∗
B(z)
which implies by Sect. 2.2.1 that (ai + bi) can be generated by an LFSR with feed-
back polynomial fA(z)fB(z), i.e.
L(ai + bi) ≤L(ai) + L(bi).

2.4
Linear Complexity
33
(Note that lcm(fA(z),fB(z)) is the feedback polynomial of the minimal LFSR that
generates (ai + bi).)
□
Theorem 2.9 shows that a linear combination of several linear feedback shift
registers does not result in a cryptographic improvement. In fact there is a much
more general theorem: Any circuit of n ﬂip-ﬂops which contains only XOR-gates
can be simulated by an LFSR of size at most n (see [46]).
Theorem 2.9 has a corollary, which will be crucial for the next section.
Corollary 2.2 If
L

(ai)i=0,...,n−2

< L

(ai)i=0,...,n−1

then
L

(ai)i=0,...,n−1

≥n −L

(ai)i=0,...,n−2

.
(2.20)
Proof Let (bi) be the sequence 0,...,0,1 (n −1 zeros) and let a′
i = ai + bi.
Since L((ai)i=0,...,n−2) < L((ai)i=0,...,n−1), the minimal LFSR that produces the
sequence (ai)i=0,...,n−2 will produce an−1 + 1 as nth output, i.e.
L

(ai)i=0,...,n−2

= L

a′
i

i=0,...,n−1

.
Applying Theorem 2.9 to the sequences (ai) and (a′
i) we obtain
L

(ai)i=0,...,n−1

+ L

a′
i

i=0,...,n−1

≤L

(bi)i=0,...,n−1

= n,
where the last equality is due to Lemma 2.1.
□
In the next section we will prove that we even have equality in (2.20).
2.4.2 The Berlekamp-Massey Algorithm
In 1968 E.R. Berlekamp [20] presented an efﬁcient algorithm for decoding BCH-
codes (an important class of cyclic error-correcting codes). One year later, Massey
[178] noticed that the decoding problem is in its essential parts equivalent to the
determination of the shortest LFSR that generates a given sequence.
We present the algorithm, which is now known as Berlekamp-Massey algorithm,
in the form which computes the linear complexity of a binary sequence.
In cryptography we are interested only in binary sequences, in contrast to coding
theory where the algorithm is normally used over larger ﬁnite ﬁelds. To simplify the
notation we specialize Algorithm 2.1 to the binary case.
Theorem 2.10 In the notation of Algorithm 2.1, Li is the linear complexity of the
sequence x0,...,xi−1 and fi is the feedback polynomial of the minimal LFSR that
generates x0,...,xi−1.

34
2
Linear Feedback Shift Registers
Algorithm 2.1 The Berlekamp-Massey algorithm
1: {initialization}
2: f0 ←1, L0 ←0
3: f−1 ←1, L−1 ←0
4: {Compute linear complexity}
5: for i from 0 to n −1 do
6:
Li = degfi
7:
di ←Li
j=0 coeff(fi,Li −j)xi−j
8:
if di = 0 then
9:
fi+1 ←fi
10:
else
11:
m ←

max{j | Lj < Lj+1}
if {j | Lj < Lj+1} ̸= ∅
−1
if {j | Lj < Lj+1} = ∅
12:
if m −Lm ≥i −Li then
13:
fi+1 ←fi + X(m−Lm)−(i−Li)fm
14:
else
15:
fi+1 ←X(i−Li)−(m−Lm)fi + fm
16:
end if
17:
end if
18: end for
Proof As a ﬁrst step we will prove by induction on i that fi is a feedback poly-
nomial of an LFSR that produces x0,...,xi−1. In the second step we prove the
minimality.
We start the induction with i = 0. The empty sequence has, by deﬁnition, linear
complexity 0 and the “generating LFSR” has feedback polynomial 1.
Now suppose that fi is the feedback polynomial of an LFSR which generates
x0,...,xi−1. We prove that fi+1 is the feedback polynomial of an LFSR which
generates x0,...,xi. In line 7, Algorithm 2.1 tests if the sequence x0,...,xi is also
generated by the LFSR with feedback polynomial fi. If this is the case we can keep
the LFSR.
Now consider the case that the LFSR with feedback polynomial fi fails to gener-
ate x0,...,xi. In this case we need to modify the LFSR. To this we use in addition
to the LFSR with feedback polynomial fi(x) = Li
i=1 aixi the latest time step m in
which the linear complexity of the sequence was increased. Let fm(x) = Lm
j=0 bjxj
be the feedback polynomial for that time step. For the ﬁrst steps in which no such
time step is present, we use the conventional values m = −1, f−1 = 1, L−1 = 0.
The reader can easily check that the following argument works with this deﬁnition.
With these two shift registers we construct the automaton described in Fig. 2.3.
In the lower part we see the feedback shift register with feedback polynomial fi.
It generates the sequence x0,...,xi,xi+1 +1. So at time step i −Li it computes the
wrong feedback xi+1 + 1. We correct this error with the feedback register shown in
the upper part. We use the feedback polynomial fm to test the sequence x0,... . In
the ﬁrst time steps this test will output 0. At time step m −Lm we will use at ﬁrst

2.4
Linear Complexity
35
Fig. 2.3 Construction for the
Berlekamp-Massey algorithm
the value xm+1 and the test will output 1. We will feed back this 1 in such a way that
it cancels with the 1 in the wrong feedback xi+1 + 1 of the ﬁrst LFSR. To achieve
this goal we have to delay the upper part by i −m.
We can combine the two registers into one LFSR by reﬂecting the upper part to
the lower part. Here we must distinguish between the cases m −Lm ≥i −Li and
m −Lm ≤i −Li. (For m −Lm = i −Li both cases give the same result.)
If Lm + (i −m) ≥Li, then the upper part contains more ﬂip-ﬂops. In this case
the combination of both LFSRs looks like Fig. 2.4 (a).
If Lm + (i −m) ≥Li, then the lower part contains more ﬂip-ﬂops. In this case
the combination of both LFSRs looks like Fig. 2.4 (b).
In both cases the reader can check that the diagrams in Fig. 2.4 are described
algebraically by the formulas given in lines 13 and 15 of the algorithm.
(a) The case Lm + (i −m) ≥Li (drawing with Li = 4, Lm = 2, i −m = 3)
(b) The case Lm + (i −m) ≥Li (drawing with Li = 5, Lm = 1, i −m = 3)
Fig. 2.4 Combination of the two LFSRs of Fig. 2.3

36
2
Linear Feedback Shift Registers
At this point we have proved that fi is a feedback polynomial of an LFSR that
generates x0,...,xi. Now we prove the minimality.
At time t = 0 we have the empty sequence and the empty shift register, which is
clearly minimal. Now look at the step i →i + 1. If degfi+1 = degfi then fi+1 is
clearly minimal (part (b) of Theorem 2.8).
The only interesting case is when degfi+1 > degfi. This happens if and only if
di ̸= 0 and m −Lm < i −Li.
In this case we may apply Corollary 2.2 to conclude
L(x0,...,xi) ≥i + 1 −Li.
To prove degfi+1 = Li+1 = L(x0,...,xi) it sufﬁces to show Li+1 = i + 1 −Li.
But
Li+1 = (i −Li) + (m −Lm) + degfi
= i −m + Lm
= (i + 1) −(m + 1 −Lm).
Since we have chosen m in such a way that degfm < degfm+1, we may use the
induction hypothesis on that step and conclude Lm+1 = m + 1 −Li. But again, by
choice of m, we have degfm+1 = degfm+2 = ··· = degfi. So we get
Li+1 = (i + 1) −(m + 1 −Lm)
= (i + 1) −Li.
(2.21)
By Corollary 2.2 this proves Li+1 = L(x0,...,xi).
□
As part of the proof we have improved the statement of Corollary 2.2.
Corollary 2.3 If
L

(ai)i=0,...,n−2

< L

(ai)i=0,...,n−1

then
L

(ai)i=0,...,n−1

= n −L

(ai)i=0,...,n−2

.
Proof The only case in which L((ai)i=0,...,n−2) < L((ai)i=0,...,n−1) is treated in
line 15 of Algorithm 2.1. In the proof of Theorem 2.10 we have shown (Eq. (2.21))
that in this case Ln = n −Ln−1.
□
In each loop of the Berlekamp-Massey algorithm we need O(Li) = O(i) time
steps, thus the computation of the linear complexity of a sequence of length n takes
just O(n2) time steps. In comparison, the solution of a system of n linear equations,
which we used in the beginning of this section, needs O(n3) time steps. In addition
it has the advantage that it computes the linear complexity of all preﬁxes of the
sequence. This is helpful if we want to study the linear complexity proﬁle of the
sequence (see Sect. 2.5).

2.4
Linear Complexity
37
2.4.3 Asymptotic Fast Computation of Linear Complexity
The Berlekamp-Massey algorithm is already very fast, so it may come as a surprise
that we can compute the linear complexity even more rapidly.
One can lower the complexity of the computation of L(a0,...,an−1) from O(n2)
to O(nlogn(loglog(n))2) (see, for example, [142]). The following algorithm is my
own development and has the advantage that it has a small constant in the O-term
and it computes, in addition to the feedback polynomial fn, the whole sequence
L(a0,...,ai−1), i ≤n.
The idea is a divide and conquer variant of the original Berlekamp-Massey algo-
rithm. The main part is the function Massey(i,i′) which allows us to go from time
step i directly to time step i′.
We use the following notation: For any time step i, let fi be the minimal feedback
polynomial of the sequence a0,...,ai−1. The degree of fi is Li and by m(i) we
denote the largest m < i with Lm < Li. If such an m does not exist we use the
conventional values m(i) = −1 and f−1 = 1.
By A(z) = N
j=0 ajzj we denote the generating function of the sequence Ai and
by A∗(z) = zNA(1/z) we denote the reciprocal polynomial.
To simplify the presentation of Algorithm 2.2 we take the values m(i), Li and so
on as known. We will see later how to compute these values quickly.
Lines 3 and 4 need some further explanation. As we can see in the ﬁnal recursion
step (line 8), we access only the coefﬁcient N −i + Li of Di. So there is no need
to compute the full polynomial Di = A∗fi. We will prove in Theorem 2.13 that
only the coefﬁcients from zN−i′+Lm+2 to zmax{Li−i,Lm(i)−m(i)} (inclusive) are needed
when we call Massey(i,i′). Since for a sequence with typical complexity proﬁle
m ≈i and Li ≈Lm, this means that we need only about i′ −i coefﬁcients. For the
complexity of the algorithm it is crucial to implement this optimization.
Before we prove the correctness of the algorithm we look at its running time. Let
T (d) be the running time of Massey(i,i + d).
For d > 1 we have two recursive calls of the function Massey and the computa-
tions in lines 3, 4 and 6. This leads to the recursion
T (d) = 2T (d/2) + 4M(d/2,d/4) + 8M(d/4,d/4).
We neglect the time for the additions and memory access in this analysis. Asymp-
totically it does not matter anyway, and computer experiments show that even for
small d it has very little impact on the constants.
M(k,l) denotes the complexity of multiplying a polynomial of degree k by a
polynomial of degree l.
Even with “school multiplication” M(k,l) = kl we get T (n) = 2n2 which is not
too bad in comparison with the original Berlekamp-Massey algorithm, which needs
≈n2/4 operations.
With multiplication based on the fast Fourier transform we get T (n) =
O(nlog2 nloglogn). However, the constant is not so good in this case.

38
2
Linear Feedback Shift Registers
Algorithm 2.2 Massey(i,i′)
Require: The check polynomials Di = A∗fi and Dm(i) = A∗fm(i), Li = degfi,
Lm(i) = degfm(i)
Ensure: Polynomials g00, g01, g10 and g11 with fm(i′) = fm(i)g00+fig01 and fi′ =
fm(i)g10 + fig11.
1: if i′ −i > 1 then
2:
Call Massey(i, i+i′
2 ) to get polynomials g′
00, g′
01, g′
10 and g′
11
3:
Dm( i+i′
2 ) := Dig′
00 + Dm(i)g′
01 {Compute just the coefﬁcients needed later!}
4:
D i+i′
2 := Dig′
10 + Dm(i)g′
11 {Compute just the coefﬁcients needed later!}
5:
Call Massey( i+i′
2 ,i′) to get polynomials g′′
00, g′′
01, g′′
10 and g′′
11
6:
Compute gkj = g′
0jg′′
k0 + g′
1jg′′
k1 for j,k ∈{0,1}
7: else
8:
if coeff(Di,N −i + Li) = 1 then
9:
g00 = g11 = 1, g01 = g10 = 0
10:
else
11:
if m(i) −Lm(i) > i −Li then
12:
g00 = 1, g01 = 0, g10 = x(m(i)−Lm(i))−(i−Li), g11 = 1
13:
else
14:
g00 = 0, g01 = 1, g10 = 1, g11 = x(i−Li)−(m(i)−Lm(i)),
15:
end if
16:
end if
17: end if
With different multiplication algorithms such as, for example, Karatsuba (see
also Sect. 11.3), we can get T (n) = O(n1.59) with a very good constant. Fortu-
nately there are plenty of good libraries which implement fast polynomial arith-
metic, and the designers of the libraries have performed timings to choose the
best multiplication algorithm for each range of n. So we can just implement Al-
gorithm 2.2 and be sure that the libraries will guarantee an asymptotic running time
of O(nlog2 nloglogn) and select special algorithms with good constants for small
n. For the timings at the end of this section we choose NTL [244] as the underling
library for polynomial arithmetic.
We prove by induction that the polynomials g00, g01, g10 and g11 have the desired
property.
Theorem 2.11 Algorithm 2.2 computes polynomials g00, g01, g10 and g11 with
fm(i′) = fm(i)g00 + fig01 and fi′ = fm(i)g10 + fig11.
Proof We prove the theorem by induction on d = i′ −i.
For d = 1 lines 8–16 are just a reformulation of lines 8–17 of Algorithm 2.1. Note
that we ﬂipped the sequence Ai and thus we have to investigate the bit at position
N −i + Li instead of the bit at position i.

2.4
Linear Complexity
39
Now we have to check the case d > 1. By induction the call of Massey(i, i+i′
2 )
gives us polynomials g′
00, g′
01, g′
10 and g′
11 with fm( i+i′
2 ) = fm(i)g′
00 + fig′
01 and
f i+i′
2 = fm(i)g′
10 + fig′
11.
To prove that the call of Massey( i+i′
2 ,i′) in line 5 gives the correct result, we
have to show that the values Dm( i+i′
2 ) and D i+i′
2
computed in lines 3 and 4 satisfy
the requirements of the algorithm.
In line 3 we compute Dm( i+i′
2 ) as
Dm( i+i′
2 ) = Dig10 + Dm(i)g11
= A∗fig01 + A∗fm(i)g01
(required input form)
= A∗fm( i+i′
2 )
(induction)
and similarly D i+i′
2
= A∗f i+i′
2 . Thus we meet the requirement for Massey( i+i′
2 ,i′)
and by induction we obtain polynomials g′′
00, g′′
01, g′′
10 and g′′
11 with fm(i′) =
fm( i+i′
2 )g′′
00 + f i+i′
2 g′′
01 and fi′ = fm( i+i′
2 )g′′
10 + f i+i′
2 g′′
11.
Thus
fm(i′) = fm( i+i′
2 )g′′
00 + f i+i′
2 g′′
01
=

fm(i)g′
00 + fig′
01

g′′
00 +

fm(i)g′
10 + fig′
11

g′′
01.
This proves g00 = g′
00g′′
00 + g′
10g′′
01 and so on, i.e. the polynomials computed in line
6 satisfy the statement of the theorem.
□
In order to do the test in line 11 of Algorithm 2.2 we need to know m(i), Li and
Lm(i). We assume that the algorithm receives these numbers as input and we must
prove that we can rapidly compute m(i′), Li′ and Lm(i′).
In Algorithm 2.2 we have at any time fi′ = fm(i)g10 + fig11. We must compute
Li′ = degfi. The problem is that we cannot compute fi′ for all i′, since it will take
O(n2) steps just to write the results. However, we don’t have to compute fi′ to
determine degfi′, as the following theorem shows.
Theorem 2.12 Using the notation of Algorithm 2.2
degfi′ = degfi + degg11
and if m(i′) > m(i) then
degfm(i′) = degfi + degg01.
Proof It is enough to prove degfi′ = degfi + degg11, since the second part follows
from the ﬁrst simply by changing i to m(i).

40
2
Linear Feedback Shift Registers
We will prove deg(fm(i)g10) < deg(fig11), which implies the theorem. The proof
is by induction on d = i′ −i. This time we do not go directly from d to 2d, but
instead we will go from d to d + 1.
For d = 1 the only critical part is line 12. In all other cases we have degfi >
degfm(i) and degg11 > degg10.
However, if we use line 12, then degfi+1 = degfi, since m(i) −Lm(i) > i −Li
and hence degfi+1 = degfi + degg11.
Now look at the step d to d + 1 as described in the algorithm. Let g′
00, g′
01, g′
10
and g′′
11 be the polynomials with fm(i+d) = fm(i)g′
00 + fig′
01 and fi+d = fm(i)g′
10 +
fig′
11. By induction, degfi+d = deg(fig′
1) > fm(i)g′
0. Now observe how fi+d+1 is
related to fi. If we use line 9 in Algorithm 2.1 then fi+d+1 = fi+d and there is
nothing left to prove.
If we use line 13 in Algorithm 2.1, then degfi+d+1 = degfi+d (since m −Lm >
i −Li) and hence g′
1 = 1 and degfi+d+1 = deg(fi+dg′′
1) = deg(fig′
1g′′
1) = degfi +
degg11.
If we are in the case of line 15 in Algorithm 2.1 then degg′′
1 > degg′′
0 = 0
and hence degfig′′
1 > degfm(i)g′′
0 and thus degfi+d+1 = deg(fi+dg′′
1) = degfi +
degg11.
This proves the theorem.
□
Theorem 2.12 allows us to compute Li′ = degfi′ and Lm(i′) = degfm(i′) with
just four additions from Li and Lm(i). Since degfi ≤i ≤n the involved numbers
have at most log(n) bits, i.e. we need just O(logn) extra time steps per call of
Massey(i,i′).
The last thing we have to explain is how the algorithm computes the function
m(i). At the beginning m(0) = −1 is known. If i′ −i = d = 1 we obtain m(i + 1)
as follows: If we use line 9 or line 12 then m(i + 1) = m(i), and if we use line 14
then m(i + 1) = i.
If d > 1 then we obtain the value m( i+i′
2 ) needed for the call of Massey( i+i′
2 ,i′)
resulting from the call of Massey(i, i+i′
2 ).
Similarly the algorithm can recursively compute Li = degfi and Lm(i) =
degfm(i), which is even faster than using Theorem 2.12.
Finally, we show that we can trim the Polynomials Di and Dm(i). We used these
to get the sub-quadratic bound for the running time.
Theorem 2.13 The function Massey(i,i′) needs only the coefﬁcients between
zN−i′+Lm+2 and zmax{Li−i,Lm(i)−m(i)} (inclusive) from the polynomials Di and
Dm(i).
Proof In the ﬁnal recursion steps the algorithm will have to access coeff(Dj,N −
j + Lj) for each j in {i,...,i′ −1}. So we have to determine which parts of the
polynomials Di and Dm(i) are needed to compute the relevant coefﬁcients.
Let g(j)
0
and g(j)
1
be the polynomials with Dj = Dm(i)g(j)
0
+ Dig(j)
1 .

2.4
Linear Complexity
41
By Theorem 2.12 we know that degfi + degg(j)
1
= degfj and degfm +
degg(j)
0
< degfj .
Thus max{degg(j)
0 ,degg(j)
1 } ≤Lj −Lm −1. Therefore we need only the coefﬁ-
cients from z(N−j+Lj )−(Lj −Lm−1) = zN−j+Lm+1 to zN−j+Lj of Di and Dm to com-
pute coeff(Dj,N −j +Lj). (Note that in the algorithm we compute Dj not directly,
but in intermediate steps so as not to change the fact that coeff(Dj,N −j + Lj) is
affected only by this part of the input.)
Since j < i′ we see that we need no coefﬁcient below zN−i′+Lm+2.
To get an upper bound we have to bound Lj −j in terms of Li, Lm and i. By
induction on j we prove
max

Lj −j,Lj(m) −m(j)

≤max

Li −i,Lm(i) −m(i)

.
For j = i this is trivial. Now we prove
max

Lj+1 −(j + 1),Lm(j+1) −m(j + 1)

≤max

Lj −j,Lm(j) −m(j)

.
To this end, we study what happens in lines 9, 13 and 15 of Algorithm 2.1.
In the ﬁrst two cases we have Lj+1 = Lj, m(j + 1) = m(j) and the inequality is
trivial. In the last case Lj+1 = Lj + (j −Lj) −(m(j) −Lm(j)) and m(j + 1) = j
thus
max

Lj+1 −(j + 1),Lm(j+1) −m(j + 1)

= max

Lm(j) −m(j) −1,Lj −j

.
This proves Lj −j < max{Li −i,Lm(i) −m(i)}, which gives the desired upper
bound N −j + Lj.
□
We have seen that the algorithm keeps track of the values m(i), Li and Lm(i). So
the only thing we have to do to get the full sequence L1,...,Ln is to output L i+i′
2
when the algorithm reaches line 5. This costs no extra time.
If we are interested in the feedback polynomials we have to do more work. We
need an array R of size n. Each time the algorithm reaches its end (line 17) we store
the values (i,g00,g01,g10,g11) at R[i′].
When the computation of the linear complexity is ﬁnished the array R is com-
pletely ﬁlled. Now we can compute the feedback polynomials by the following re-
cursive function (Algorithm 2.3).
Finally, we can also use the algorithm in an iterative way. If we have observed
N bits, we can call Massey(0,N) to compute the linear complexity of the sequence
a0,...,aN−1. We will remember m(N) (computed by the algorithm), fN = g10 +
g11 and fm(N) = g00 +g01. If we later observe N′ extra bits of the sequence, we can
call Massey(N,N + N′) to get the linear complexity of a0,...,aN+N′−1.
In the extreme case we can always stop after one extra bit. In this case the al-
gorithm will of course need quadratic time, since it must compute all intermediate
feedback polynomials. Computer experiments show that the new algorithm beats

42
2
Linear Feedback Shift Registers
Algorithm 2.3 feedback(i′)
1: Get the values i,g00,g01,g10,g11 from R[i′].
2: if i = 1 then
3:
fi′ = g10 + g11, fm(i′) = g00 + g01
4: else
5:
Obtain fi and fm(i′) by calling feedback(i).
6:
fi′ = fm(i)g10 + fig11, fm(i′) = fm(i)g00 + fig01
7: end if
Table 2.1 Tests for the
algorithms
n
100
500
1000
5000
10000 100000
Algorithm 2.1 0.00002 0.00065 0.0036 0.098 0.39
36.11
Algorithm 2.2 0.001
0.00157 0.0042 0.049 0.094
0.94
the original Berlekamp-Massey algorithm if it receives at least 5000 bits at once.
The full speed is reached only if we receive the input in one step.
Table 2.1 shows that the asymptotic fast Berlekamp-Massey algorithm beats the
classical variant even for very small n.
2.4.4 Linear Complexity of Random Sequences
Since we want to use linear complexity as a measure of the randomness of a se-
quence, it is natural to ask what the expected linear complexity of a random se-
quence is.
Theorem 2.14 (Rueppel [228]) Let 1 ≤L ≤n. The number N(n,L) of bi-
nary sequences of length n having linear complexity exactly L is N(n,L) =
2min{2n−2L,2L−1}.
Proof We are going to ﬁnd a recursion for N(n,L). For n = 1 we have N(1,0) = 1
(the sequence 0) and N(1,1) = 1 (the sequence 1).
Now consider a sequence a0,...,an−1 of length n and linear complexity L. Let
f (z) be a feedback polynomial of a minimal LFSR generating a0,...,an−1.
We have one way to extend the sequence a0,...,an−1 by an an without changing
the feedback polynomial f (z). (This already proves N(n + 1,L) ≥N(n,L).)
Now consider the second possible extension a0,...,an−1,an. By Corollary 2.3
we have either
L(a0,...,an−1,an) = L(a0,...,an−1) = L

2.4
Linear Complexity
43
or
L(a0,...,an−1,an) = n + 1 −L > L.
If L ≥n+1
2 , the second case is impossible, i.e. we have N(n + 1,L) ≥2N(n,L)
for L ≥n+1
2 .
Now let L < n+1
2
and let m be the largest index with Lm = L(a0,...,am−1) < L.
Then by Corollary 2.3 we ﬁnd
Lm = m −L(a0,...,am−2) > L(a0,...,am−2)
and hence Lm > m/2. Therefore n −L > n+1
2
> m/2 > m −Lm, which means by
Algorithm 2.1 that
L(a0,...,an−1,an) > L(a0,...,an−1) = L.
This proves the recursion
N(n + 1,L) =
⎧
⎪⎨
⎪⎩
2N(n,L) + N(n,n + 1 −L)
if 2L > n + 1,
2N(n,L)
if 2L = n + 1,
N(n,L)
if 2L < n + 1.
With this recursion it is just a simple induction to prove
N(n,L) = 2min{2n−2L,2L−1}.
□
With Theorem 2.14 we need only elementary calculations to obtain the expected
linear complexity of a ﬁnite binary sequence.
Theorem 2.15 (Rueppel [228]) The expected linear complexity of a binary se-
quence of length n is
n
2 + 4 + (n&1)
18
−2−n
n
3 + 2
9

.
Proof For even n we get
n

i=1
iN(n,l) =
n/2

i=1
i · 22i−1 +
n/2−1

i=0
(n −i)22i
=

n2n
3 + 2n+1
9
+ 2
9

+

n2n
6 + 2n+2
9
−n
3 −4
9

= n2n−1 + 2n+1
9
−n
3 −2
9

44
2
Linear Feedback Shift Registers
and similarly for odd n we get
n

i=1
iN(n,l) =
(n−1)/2

i=1
i · 22i−1 +
(n−1)/2

i=0
(n −i)22i
= n2n−1 + 5
182n −n
3 −2
9.
Multiplying by the probability 2−n for a sequence of length n we get the expected
value.
□
It is also possible to determine the expected linear complexity of a periodic se-
quence of period n with random elements. However, since in cryptography a cipher
stream is broken if we are able to observe more than one period (see the Vigenère
cipher in Sect. 1.1), this kind of result is of less interest. We state the following
without proof.
Theorem 2.16 A random periodic sequence with period n has expected linear com-
plexity:
(a) n −1 + 2−n if n is power of 2;
(b) (n −1)(1 −
1
2o(2,n) ) + 1
2 if n is an odd prime and o(2,n) is the order of 2 in F×
n .
Proof
(a) See Proposition 4.6 in [228].
(b) See Theorem 3.2 in [186].
□
2.5 The Linear Complexity Proﬁle of Pseudo-random Sequences
2.5.1 Basic Properties
We have introduced linear complexity as a measure of the cryptographic strength
of a pseudo-random sequence. However, a high linear complexity is only a neces-
sary but not sufﬁcient condition for cryptographic strength. Take for example the
sequence
1010111100010011010111100
which is generated by an LFSR with feedback polynomial z4+z+1. It is a weak key
stream and its linear complexity is 4. By changing just the last bit of the sequence
the linear complexity rises to 22 (see Corollary 2.3), but changing just one bit does
not make a keystream secure.
One way to improve linear complexity as a measure of the randomness of a se-
quence is to look at the linear complexity proﬁle.

2.5
The Linear Complexity Proﬁle of Pseudo-random Sequences
45
Fig. 2.5 The linear
complexity proﬁle of
1010111100010011010111101
Fig. 2.6 A typical linear
complexity proﬁle
Deﬁnition 2.13 The linear complexity proﬁle of a binary sequence (an)n∈N is the
function LP : N+ →N with n →L(a0,...,an−1).
If we draw the linear complexity proﬁle for the sequence
1010111100010011010111101
we see (Fig. 2.5) that the linear complexity jumps with the last bit to the high value
22. Prior to this we have the low value 4, which means that the sequence is a weak
key stream.
By Theorem 2.15 the expected linear complexity of a sequence of length n is
about n
2, i.e. the linear complexity proﬁle of a good pseudo-random sequence should
lie around the line n →n/2 as shown in Fig. 2.6.
In the remaining part of this section we will study sequences with a linear com-
plexity proﬁle which is “as good as possible”.
Deﬁnition 2.14 A sequence (an)n∈N has a perfect linear complexity proﬁle if
L(a0,...,an−1) =
n + 1
2

.

46
2
Linear Feedback Shift Registers
The linear complexity proﬁle is good if
L(a0,...,an−1) −n
2
 = O

log(n)

.
H. Niederreiter [198] classiﬁed all sequences with a good linear complexity pro-
ﬁle by means of continued fractions. We will follow his proof in the remaining part
of the section.
2.5.2 Continued Fractions
In this section we classify all sequences with a good linear complexity proﬁle. To
that end we establish a connection between the continued fraction expansion of the
generation function and the complexity proﬁle.
Consider the ﬁeld F((z−1)) = {
i≥n aiz−i | n ∈Z,ai ∈F} of formal Laurent
series in z−1. For S = 
i≥n aiz−i ∈F((z−1)) we denote by [S] = 
0≥i≥n aiz−i
the polynomial part of S.
A continued fraction is an expression of the form
b0 +
a1
b1 +
a2
b2 +
a3
b3 + ...
For continued fractions we use the compact notation of Pringsheim:
b0 + a1|
|b1
+ a2|
|b2
+ a3|
|b3
+ ···
For S ∈F((z−1)) recursively deﬁne
Ai =

R−1
i−1
 
,
Ri = R−1
i−1 −Ai
for i ≥0
(2.22)
with R−1 = S0. This gives the continued fraction expansion
S = A0 + 1 |
|A1
+ 1 |
|A2
+ ···
(2.23)
of S.
The term
A0 + 1 |
|A1
+ ··· + 1 |
|Ak
= Pk
Qk
with Pk,Qk ∈F[z] is called the kth convergent fraction of f .

2.5
The Linear Complexity Proﬁle of Pseudo-random Sequences
47
Let us recall some basic properties of continued fractions (see, for exam-
ple, [211]).
The polynomials Pk,Qk satisfy the recursion
6P−1 = 1,
P0 =A0,
Pk = AkPk−1 + Pk−2,
(2.24)
Q−1 =0,
Q0 = 1,
Qk =AkQk−1 + Qk−2.
(2.25)
In addition we have the identities
Pk−1Qk −PkQk−1 = (−1)k,
(2.26)
gcd(Pk,Qk) = 1,
(2.27)
S = Pk + RkPk−1
Qk + RkQk−1
.
(2.28)
The above identities hold for every continued fraction. The next identities use the
degree function and hold only for continued fractions deﬁned over F((z−1)). Using
the recursion (2.25) we get degQi = i
j=1 degAi for j ≥1.
Lemma 2.2 For all j ∈N we have
deg(QjS −Pj) = −deg(Qj+1).
Proof We prove this by induction on j. For j = 0 this follows immediately from
Eq. (2.28) with
−degR0 = degR−1
0
= deg

R−1
0
 
= degA1 = degQ1.
Now let j ≥1. By Eq. (2.28) we have
SQj −Pj = Bj(SQj−1 −Pj−1).
By induction degSQj−1 −Pj−1 = degQj and since degBj = −degAj+1 and
degQj+1 = degAj+1 + degQj we get
deg(SQj −Pj) = −degQj+1.
□
The connection of linear complexity and the Berlekamp-Massey algorithm with
continued fractions and the Euclidean algorithm has been observed several times.
The formulation of the following theorem is from [197].
Theorem 2.17 Let (an)n∈N be a sequence over the ﬁeld F and let S(z) =
−∞
j=0 ajz−j−1. Let
Pk
Qk be the kth convergent fraction of the continued fraction
expansion of S.
Then for every n ∈N+ the linear complexity Ln = L(a0,...,an−1) is given by
Ln = 0 for n < degQ0 and Ln = degQj where j ∈N is determined by
degQj−1 + degQj ≤n < degQj + degQj+1.

48
2
Linear Feedback Shift Registers
Proof By Lemma 2.2 we have
deg

S −Pj
Qj

= −degQj −degQj+1.
This means that the ﬁrst degQj + degQj+1 elements of the sequence with the
rational generating function Pj
Qj coincide with (an)n=0,...,degQj +degQj+1−1.
But the rational generating function Pj
Qj belongs to an LFSR with feedback poly-
nomial Q∗
j, which proves that
Ln ≤degQj
for n < degQj + degQj+1.
(2.29)
This already establishes one part of the theorem. Now we prove the equality.
That Ln = 0 if n < degQ0 is just a reformulation of the fact that degQ0 denotes the
number of leading zeros in the sequence (an)n∈N.
By induction we know now that Ln = degQj for degQj−1 + degQj ≤n <
degQj + degQj+1.
If k is the smallest integer with Lk > degQj then by Corollary 2.3 we have
Lk = k −degQj. The only possible value of k for which Lk satisﬁes Eq. (2.29)
is k = degQj + degQj+1. Thus k = degQj + degQj+1 and Lk = degQj+1.
By Eq. (2.29) we get Ln = degQj+1 for degQj + degQj+1 ≤n < degQj+1 +
degQj+2, which ﬁnishes the induction.
□
2.5.3 Classiﬁcation of Sequences with a Perfect Linear Complexity
Proﬁle
By Theorem 2.17 it is easy to characterize sequences with a good linear complexity
proﬁle in terms of continued fractions (see [197, 198]). As a representative of all
results of this kind, we present Theorem 2.18 which treats the case of a perfect
linear complexity proﬁle.
Theorem 2.18 (see [197]) A sequence (an)n∈N has a perfect linear complexity pro-
ﬁle if and only if the generating function S(z) = −∞
j=0 ajz−j−1 is irrational and
has a continued fraction expansion
S = 1 |
|A1
+ 1 |
|A2
+ 1 |
|A3
+ ···
with degAi = 1 for all i ≥1.
Proof A perfect linear complexity proﬁle requires that the linear complexity grows
at most by 1 at each step. By Theorem 2.17 this means that the sequence degQi
contains all positive integers, i.e. degAi = 1 for all continued fraction denomina-
tors Ai.

2.5
The Linear Complexity Proﬁle of Pseudo-random Sequences
49
On the other hand, degAi = 1 for all i implies degQi = i and by Theorem 2.17
we get Li = ⌊i+1
2 ⌋.
□
By Theorem 2.18 we can deduce a nice characterization of sequences with a
perfect linear complexity proﬁle.
Theorem 2.19 (see [274]) The binary sequence (ai)i∈N has a perfect linear com-
plexity proﬁle if and only if it satisﬁes a0 = 1 and a2i = a2i−1 + ai for all i ≥1.
Proof Deﬁne the operation D : F2((z−1)) →F2((z−1)) by
D : T →z−1T 2 +

1 + z−1
T + z−1.
A short calculation reveals the following identities:
D(T + U + V ) = D(T ) + D(U) + D(V )
for T,U,V ∈F2

z−1
,
D

T −1
= D(T )T −2
for T ∈F2

z−1
,
D(z) + D(c) = c + 1
for c ∈F2.
Now assume that the sequence (ai)i∈N has a perfect linear complexity proﬁle and
let
S(z) =
∞

j=0
ajz−j−1 = 1 |
|A1
+ 1 |
|A2
+ 1 |
|A3
+ ···
be the corresponding generating function with its continued fraction expansion.
By Theorem 2.18 we have Ai = z + ai with ai ∈F2. By Eq. (2.22) we have
R−1
i
= Ri−1 −Aj and hence
D(Ri−1)R−2
i−1 = D

R−1
i−1

= D(Ri + z + ai) = D(Ri) + 1 + ai.
By deﬁnition, S = R−1, and by induction on i we have
D(S) =
i−1

j=0
(aj + 1)
j−1

k=−1
R2
k +
i−1

k=−1
R2
kD(Ri).
We can turn F2((z−1)) into a metric space by deﬁning d(Q,R) = 2−deg(Q−R). Since
degRi < 0 for all i we get
lim
i→∞
i−1

k=−1
R2
kD(Ri) = 0
and hence
D(S) =
∞

j=0
(aj + 1)
j−1

k=−1
R2
k.

50
2
Linear Feedback Shift Registers
Since all summands lie in F2((z−2)), we get D(S) = U2 for some U ∈F2((z−1))
or equivalently
S2 + (z + 1)S + 1 = zU2.
(2.30)
Comparing the coefﬁcients of z0 we get a0 = 1, and comparing the coefﬁcients
of z2i (i ∈N+) we get ai + a2i−1 + a2i = 0.
For the opposite direction note that the recursion a0 = 1 and ai +a2i−1 +a2i = 0
imply that Eq. (2.30) is satisﬁed for some suitable U ∈F2((z−1)).
Assume that the linear complexity proﬁle of the sequence is not perfect.
Then we ﬁnd an index j with degAj > 1 and by Lemma 2.2 we have
deg(SQj −Pj) = −degQj+1 < −degQj −1.
It follows that
deg

P 2
j + (x + 1)PjQj + Q2
j + xU2
= deg

Q2S2 −P 2
j + (x + 1)Qj(SQj −Pj)

≤max

deg

Q2S2 −P 2
j

deg

(x + 1)Qj(SQj −Pj)

< 0.
(2.31)
In particular the constant term Pj(0)2 + Pj(0)Qj(0) + Q2
0 is 0, but this implies
Pj(0) = Qj(0) = 0 and hence gcd(Pj,Qj) ̸= 1, contradicting Eq. (2.27).
This proves that the sequence that satisﬁes the recurrence given in Theorem 2.19
has a perfect linear complexity proﬁle.
□
We remark that even a sequence with a perfect linear complexity proﬁle can be
highly regular. For example, consider the sequence (ai)i∈N with a0 = 1, a2j = 1 for
j ∈N and aj = 0 for j ∈N\{1,2k | k ∈N} given by Dai in [71]. This sequence is
obviously a very weak key stream, but as one can check by Theorem 2.19, it has an
optimal linear complexity proﬁle.
2.6 Implementation of LFSRs
This book is about the mathematics of stream ciphers, but in cryptography math-
ematics is not everything. We have mentioned in the introduction that LFSRs are
popular because they can be implemented very efﬁciently. This section should jus-
tify this claim.

2.6
Implementation of LFSRs
51
Fig. 2.7 The Fibonacci
implementation of an LFSR
Fig. 2.8 The Galois
implementation of an LFSR
2.6.1 Hardware Realization of LFSRs
For implementing LFSRs in hardware there are two basic strategies. Either we con-
vert Fig. 2.1 directly into the hardware, which then looks like Fig. 2.7. If the feed-
back polynomial is ﬁxed we can save the AND-gates (multiplication).
This way to implement an LFSR is called the Fibonacci implementation or some-
times the simple shift register generator (SSRG).
The alternative implementation (see Fig. 2.8) is called the Galois implementation
(alternative names: multiple-return shift register generator (MRSRG) or modular
shift register generator (MSRG)).
The advantage of the Galois implementation is that every signal must pass
through at most one XOR-gate. By contrast, with a dense feedback polynomial the
feedback signal in the Fibonacci implementation must pass through approximately
n/2 XOR-gates.
As indicated in the ﬁgure, one has to reverse the feedback coefﬁcients in the
Galois implementation. The internal states of the Fibonacci implementation and the
Galois implementation are connected by Theorem 2.20.
Theorem 2.20 The Galois implementation generates the same sequence as the Fi-
bonacci implementation if it is initialized with F ′
i = i
j=0 Fi−jcn−j,(0 ≤i ≤n)
where F0,...,Fn−1 is the initial state of the Fibonacci implementation of the LFSR.
Proof We have F ′
0 = F0, so the next output bit is the same in both implementations.
We must prove that the next state ˆFn−1,..., ˆF0 of the Fibonacci implementation
and the next state ˆF ′
n−1,..., ˆF ′
0 of the Galois implementation again satisfy ˆF ′
i =
i
j=0 ˆFi−jcn−j,(0 ≤i ≤n).

52
2
Linear Feedback Shift Registers
For i ≤n −2 we get
ˆF ′
i = F ′
i+1 + cn−1−iF ′
0
=
i+1

j=0
Fi+1−jcn−j + cn−1−iF ′
0
=
i

j=0
Fi+1−jcn−j
=
i

j=0
ˆFi−jcn−j.
For i = n −1 we have
ˆF ′
n−1 = F ′
0 = F0
=
n−1

i=0
ciFi +
n−1

i=1
ciFi
= ˆFn−1 +
n−1

i=1
ci ˆFi−1.
So both implementations give the same result.
□
2.6.2 Software Realization of LFSRs
Now we look at software implementation of LFSRs. All modern processors have
instructions that help to achieve fast implementations. The main problem with soft-
ware implementations is that we lose the advantage of low power consumption that
we have with specialized hardware. Also, block ciphers with implementations based
on table look-up become a good alternative.
2.6.2.1 Bit-Oriented Implementation
We ﬁrst describe a bit-oriented implementation. The advantage is that we have a
direct simulation of the LFSR in the software. For optimal performance it would be
better to use a byte-oriented implementation.
We use an array of words w0,...,wˆn which represent the internal state of
the shift register. We can easily implement the shift operation on this bit ﬁeld
by calling the shift and rotation instructions of our processor (see Algorithm 2.4

2.6
Implementation of LFSRs
53
Fig. 2.9 Right shift over several words
and Fig. 2.9). Unfortunately, even a language like C does not provide direct ac-
cess to the rotation operations. So we have to use hand-written assembler code.
A portable C-implementation of the shift operation is a bit slower. Good code can be
found in [255], which contains many tips about implementing cryptographic func-
tions.
Algorithm 2.4 Right shift over several words
1: RSH wˆn−1 {Right shift by 1}
2: for k ←ˆn −1 to ˆn −1 do
3:
RCROL wk {Right roll by 1, use the carry ﬂag}
4: end for
If the feedback polynomial has only a few non-zero coefﬁcients we can compute
the feedback value by f = x[f1] + ··· + x[fk]. However, if the feedback polyno-
mial has many non-zero coefﬁcients, this method is too slow. A better technique is
to store the feedback polynomial in the bit ﬁeld f . We compute x&f and count
the number of non-zero bits in x&f modulo 2. The operation of counting the num-
bers of set bits in a bit ﬁeld is known as sideway addition or population count (see
Sect. 12.1.2). If our computer supports this operation we should use it, otherwise
we should use Algorithm 2.5 which directly computes the number of non-zero bits
in x&f modulo 2 (see also Sect. 5.2 in [276]).
Algorithm 2.5 Sideway addition mod 2 (32 bit version)
Ensure: y = x0 + ··· + x31 mod 2
1: y ←x ⊕(x ≫1)
2: y ←y ⊕(y ≫2)
3: y ←a(y &a) mod 232 {with a = (11111111)16}
4: y ←(y ≫28)&1
Theorem 2.21 Given the input x = (x0,...,xn−1)2, Algorithm 2.5 computes x0 +
x1 + ··· + xn mod 2.
Proof After the ﬁrst line we have y0 + y2 + ··· + y62 = x0 + ··· + x63 and after the
second line we have
y0 + y4 + ··· + y60 = x0 + ··· + x63.

54
2
Linear Feedback Shift Registers
Since y &a has at most 8 non-zero bits, we can be sure that the multiplication
does not overﬂow, i.e.
a(y &a) =
 7

i=0
y4j,
6

i=0
y4j,...,y0 + y4,y0

4
.
In the ﬁnal step we extract the bit 7
i=0 y4j mod 2.
□
If we work with 64-bit words, we must add an extra shift y ←y ⊕(y ≫4) after
line two and use the mask a = (11111111)256 instead of a = (11111111)16.
Some processors (such as the IA32 family) have a slow multiplication routine
(≈10 clock cycles, while the shift and XOR takes only 1 clock cycle). In this case
Algorithm 2.6, which avoids multiplication, may be faster.
Algorithm 2.6 Sideway addition mod 2 (without multiplication)
Require: x = (xn−1 ...x0) is n-bit word
Ensure: y = SADD(x) mod 2
1: y ←x
2: for k ←0 to ⌊(log2(n −1)⌋do
3:
y ←y ⊕(y ≫2k)
4: end for
5: y ←y &1 {y ←y mod 2}
2.6.2.2 Word-Oriented Implementation
The bitwise generation of an LFSR sequence is attractive for simulating hardware
realizations. However, on most computers it will be faster to generate the sequence
word-wise. Let s be the word size of our computer, i.e. s = 8 on a small embedded
processor (such as Intel’s MCS-51 series) or s = 32 or s = 64 on a Desktop machine.
We assume that s is a power of 2 and that s = 2d.
For simplicity we assume that the length n of the feedback shift register is divis-
ible by the word size s. Let n = s ˆn.
Let cj,k be the coefﬁcients of the generator matrix associated with the LFSR (see
Eq. (2.17)). Deﬁne
fi

a0 + ··· + 27a7

=
7
!
k=0
s−1

j=0
2jakc8i+k,n+j

.
(2.32)

2.6
Implementation of LFSRs
55
Let x = (xn−1,...,x0)2 be the internal state of the LFSR. Then the next word
x′ = (xn+s,...,xn)2 is
x′ =
n−1
!
i=0
xici,n+s,...,
n−1
!
i=0
xici,n

2
=
n−1
!
i=0
s−1

j=0
2jxici,j.
(2.33)
(This is just the deﬁnition of the generator matrix.)
Now write x = (ˆxˆn−1,..., ˆx) and regroup the sum in Eq. (2.33), yielding:
x′ =
ˆn−1
!
i=0
fi(ˆxi).
(2.34)
Equation (2.34) gives us a table look-up method to compute the word of the LFSR
sequence. We just have to pre-compute the functions fi and evaluate Eq. (2.33).
Algorithm 2.7 shows this in pseudo-code.
Algorithm 2.7 LFSR byte-oriented implementation (table look-ups)
1: output w0
2: w ←f0(w0)
3: for k ←1 to ˆn −1 do
4:
w ←w ⊕fi
5:
wi−1 ←wi
6: end for
7: wˆn−1 ←w
Algorithm 2.7 uses huge look-up tables. This may be a problem in embedded
devices. In this case we can use the following algorithm that is based on the idea that
we can use byte operations to compute several sideway additions simultaneously
and which needs no look-up table.
The core of our program is Algorithm 2.8, which takes 2k words w0,...,w2k−1
of 2k bits each and computes the word y = (y2k−1 ···y0) with yi = SADD(wi)
mod 2. (SADD denotes the sideway addition.)
Theorem 2.22 The result y = PSADD(d,0) of Algorithm 2.8 satisﬁes yi =
SADD(wi) mod 2.
Proof We prove by induction on d′ that y(d′,k) = PSADD(d′,k) satisﬁes
2d−d′
!
j=0
y(d′,k)
i+j2d′ = SADD(wk+i)
mod 2
(2.35)

56
2
Linear Feedback Shift Registers
Algorithm 2.8 Parallel sideway addition mod 2
Ensure: PSADD(d,0) returns the word
(SADD(w2s−1) mod 2,...,SADD(w0) mod 2)2
1: if d′ = 0 then
2:
return wi
3: else
4:
y ←PSADD(d′ −1,k)
5:
y ←(y ≫2d′−1 ⊕y)
6:
y′ ←PSADD(d′ −1,k + 2d−1)
7:
y′ ←(y′ ≪2d′−1 ⊕y′)
8:
return (y &μd′−1)|(y′ &μd′−1)
9: end if
for i ∈{0,...,2d′ −1}.
For d′ = 0 we have simply y(0,k) = wk and Eq. (2.35) is trivial.
If d′ > 0 we call PSADD(d′ −1,k) in line 4 and the return value satisﬁes
Eq. (2.35). The shift and XOR operation in step 4 give us a word y = y(d′−1,k)
which satisﬁes
2d−d′+1
!
j=0
y(d′−1,k)
i+j2d′−1 = SADD(wk+i)
mod 2
for i ∈{0,...,2d′−1 −1}. The shift and XOR operation in line 5 computes the sums
y(d′−1,k)
i+(2j)2d′−1 ⊕y(d′−1,k)
i+(2j+1)2d′−1. Thus after line 5 the word y satisﬁes
2d−d′
!
j=0
yi+j2d′ = SADD(wk+i)
mod 2
for i ∈{0,...,2d′−1 −1}.
Similarly we process the word y′ in the lines 6 and 7 and we have
2d−d′
!
j=0
y′
i+2d′−1+j2d′ = SADD(w(k+2d−1)+i)
mod 2
for i ∈{0,...,2d′−1 −1}. (That is, we use a left shift in line 7 instead of a right
shift, resulting in the +2d−1 term in the index of y′.)
Finally we use in line 8 the mask μd−1 (see Sect. 12.1.1) to select the right bits
from words w and w′.
□
A problem is how to ﬁnd the right input words wi for Algorithm 2.8. One possi-
bility is to pre-compute feedback polynomials f0,...,fs−1 in Algorithm 2.7. This

2.6
Implementation of LFSRs
57
Algorithm 2.9 LFSR update with parallel sideway addition mod 2
Require: x is the internal state of the LFSR
Ensure: y is the next s bits in the LFSR sequence
1: for i ←0 to n −1 do
2:
wi ←byte-wise XOR of (x ≫i)&f
3: end for
4: Compute y with yi = SADD(wi) mod 2
5: s ←0,z ←y
6: for i ←1 to n −1 do
7:
s ←s ⊕f [n −i + 1] = 1
8:
y ←y ⊕(z ≫i)
9: end for
Table 2.2 Speed of different
LFSR implementations (128
bit LFSR)
Bitwise generation
74.7 Mbits/sec
bytewise generation (table look-up)
666 Mbits/sec
bytewise generation (PSADD)
84.4 Mbits/sec
Table 2.3 Speed of an LFSR
with feedback polynomial
z127 + z + 1
Generic bitwise generation
74.7 Mbits/sec
trinomial bitwise generation
132 Mbits/sec
Algorithm 2.10
15300 Mbits/sec
needs ns bits in a look-up table, but with a few extra operations we can avoid storing
the extra polynomials f1,...,fs−1.
We can use Algorithm 2.8 to compute the next 2k bits in the LFSR sequence as
follows. The internal state of our LFSR is stored in the bit ﬁeld x.
The idea of Algorithm 2.9 is that SADD((X ≫i)&f ) mod 2 is almost xn+i.
The only thing that is missing is xnfn−i+1 ⊕··· ⊕xn+i−1fn−1. The loop in lines
6–9 computes this correction term.
All of the above implementations were designed for arbitrary feedback polyno-
mials f (z) = zn −n
j=0 cjzj. However, if we choose a feedback polynomial with
few coefﬁcients, with the additional property that f (z) −zn has a low degree, we
can obtain a very fast implementation. This is especially true if we use a trinomial
f (z) = zn + zk + 1 as a feedback polynomial.
Algorithm 2.10 describes how we can compute the next n −k bits of the LFSR
sequence. The internal state of the LFSR is denoted by x.
Such a special algorithm is of course much faster than the generic algorithms.
However, feedback polynomials of low weight do not only help to speed up the
implementation of an LFSR, there are also special attacks against stream ciphers
based on these LFSRs (see Sect. 4.3). One should keep this in mind when designing
an LFSR-based stream cipher. Most often, the extra speed up of an LFSR with a
sparse feedback polynomial is not worth the risk of a special attack.

58
2
Linear Feedback Shift Registers
Algorithm 2.10 Generating an LFSR sequence with the feedback polynomial zn +
zk + 1
1: y ←((x ≫k) ⊕x)&(2k −1)
2: output y
3: x ←(x ≫k)|(y ≪k)
We close this section with some timings for the implementation of our algo-
rithms. All programs run on a single core of a 32-bit Intel Centrino Duo with
1.66 Gz.

Chapter 3
Non-linear Combinations of LFSRs
3.1 De Bruijn Sequences
We have seen in the previous chapter that linear feedback shift register sequences
have some very desirable statistical properties, but provide no security against cryp-
tographic attacks.
One approach to this problem is to allow non-linear feedback functions in the
shift registers. We can describe a general shift register sequence by a graph of 2n ver-
tices labeled by the words of {0,1}n. The edges show possible transitions of a shift
register, i.e. we have an edge from the vertex (a0,...,an−1) to (a1,...,an−1,0)
and to (a1,...,an−1,1). These graphs are now named after de Bruijn for his work
in [73], but they also appeared at the same time in the work of Good [117]. Fig-
ure 3.1 shows the smallest de Bruijn graphs.
We obtain the state diagram of a non-linear feedback shift register out of the
corresponding de Bruijn graph by selecting for every vertex one of the two outgoing
edges.
As in the case of linear shift register sequences, the sequences of maximal period
are of special interest and furthermore we can prove that a sequence of maximal
period will satisfy the second Golomb axiom (see Theorem 2.3). Since a non-linear
shift register can use the state 0,...,0 we can even prove that every n-tuple has to
occur as a subsequence. For the following we take this property as a deﬁnition.
Deﬁnition 3.1 A de Bruijn sequence of order n is a periodic sequence with period
length 2n which contains every n-tuple as a subsequence.
The following lemma connects the de Bruijn sequences with the de Bruijn
graphs.
Lemma 3.1 A de Bruijn sequence of order n corresponds to a Hamiltonian cycle
(a cycle which visits every vertex exactly once) in the de Bruijn graph Dn and to an
Eulerian cycle (a cycle which visits every edge exactly once) in the de Bruijn graph
Dn−1.
A. Klein, Stream Ciphers, DOI 10.1007/978-1-4471-5079-4_3,
© Springer-Verlag London 2013
59

60
3
Non-linear Combinations of LFSRs
Fig. 3.1 The smallest de
Bruijn graphs
Proof We can map any sequence to a path in a de Bruijn Graph and conversely a
path in a de Bruijn Graph deﬁnes a binary sequence.
By deﬁnition, a period of a de Bruijn sequence contains every n-tuple exactly
once as a subsequence. So if we map it to the de Bruijn graph Dn we visit every
vertex (a1,...,an) exactly once, i.e. we have a Hamiltonian cycle. And in the oppo-
site direction, a Hamiltonian cycle deﬁnes a periodic sequence which contains every
n-tuple exactly once as a subsequence, i.e. a de Bruijn sequence.
If we map the de Bruijn sequence to Dn−1 we visit every edge
(a1,...,an−1) →(a2,...,an)
exactly once, i.e. we have an Eulerian cycle.
□
It is easy to prove the existence of de Bruijn sequences. We already know that
an m-sequence runs through all vertices of the de Bruijn graph except for the ver-
tex 0...0. The only thing we have to do to obtain a de Bruijn sequence from
an m-sequence is to replace the subsequence of n −1 zeros by a subsequence
of n zeros, i.e. we replace the transition 10...0 →0...01 with the transitions
10...0 →0...0 →0...01.
Now we want to count the number of de Bruijn sequences. This was done by
de Bruijn in 1946 (see [73]), however the problem of enumerating the de Bruijn
sequences had already been solved in 1894 by Fly Sainte-Marie [231] as a solution
to a problem proposed by de Rivière [223] (see [74]).

3.1
De Bruijn Sequences
61
Theorem 3.2 The number of de Bruijn sequences of order n is 22n−1.
By Lemma 3.1 we must count the number of Eulerian cycles in the de Bruijn
graph Dn−1. We will use the following theorem to do this.
Let us ﬁrst recall some basic notions from graph theory. A directed graph consists
of a vertex set V and an edge set E ⊂V ×V . The edge (a,b) is said to start at a and
end at b. The graph is balanced if for every vertex v the number of edges starting
at v is equal to the number of edges ending at v. The graph is connected if for
every two vertices v ̸= v′, there exists a path v0 = v,v1,...,vn−1,vn = v′ such that
(vi−1,vi) is an edge for i = 1,...,n. A tree is an unorientated connected graph
which contains no cycle. In a orientated tree in addition a vertex is distinguished as
root an all edges points towards the root. A vertex u′ of a tree with root lies above
a vertex u if the path from u′ to the root contains u. A spanning subtree of G is a
subgraph of G which is a tree and contains all vertices of G.
Theorem 3.3 (Aardenne-Ehrenfest, de Bruijn [1]) Let G be a connected, balanced,
directed graph with vertex set V . Let e = (v,w) be an edge and let τ(G,v) denote
the number of oriented spanning subtrees of G with root v. Then the number ϵ(G,e)
of Eulerian tours of G starting at the edge e is
ϵ(G,e) = τ(G,v)

u∈V

outdeg(u) −1

!
Proof Let E = e1,...,eq be an Eulerian tour in G. For each vertex u ̸= v we denote
by e(u) the last exit from u, i.e. e(u) = ei with init(ei) = u and init(ej) ̸= u for
j > i.
We claim that the edges e(u), u ∈V \{v}, form an oriented spanning subtree T
of G with root v. To prove this, it sufﬁces to note the following.
1. For every vertex u of V \{v} we selected exactly one leaving edge.
2. T contains no cycle, since if we have a path from u to u′ in T , the last exit from
u′ must occur after the last exit from u.
Both observations together imply that T must be an oriented tree with root v.
Thus we can associate with every Eulerian tour starting at e a directed tree with
vertex v.
In the converse direction, we start with a directed tree T with vertex v and con-
struct an Eulerian tour in which the edges of T are the last exits. The construction
is simple. In the ﬁrst step we walk along the edge e. In each of the following steps
we choose any possible edge to continue the tour, the only restriction being that we
never choose an edge in T unless there is no other possible alternative. Since G is a
balanced graph our tour can only stop at v, at which point all edges of v will have
been used.
We claim that no matter which choices are made, this tour must be Eulerian,
i.e. we must use all edges of G. Assume otherwise. Then there are unused edges.
If u ̸= v is a vertex and if we haven’t used all outgoing edges from u, then we

62
3
Non-linear Combinations of LFSRs
haven’t used the edge e(u) ∈T , because such exit edges are saved for last. Let
e(u) = (u,u′). We haven’t used all incoming edges of u′ in our tour, so we haven’t
used all outgoing edges of u′. This proves that every vertex of T above uwhich
follows u have not yet been used. In particular we haven’t used all outgoing edges
of v. But then the tour is not ﬁnished, a contradiction.
Thus we have associated with each Eulerian tour the tree of last exits. Conversely
we have shown how to obtain all Eulerian tours from a given last exit tree. For each
tree and each vertex u of G there are (outdeg(u) −1)! choices to be made, i.e. there
are 
u∈V (outdeg(u) −1)! Eulerian tours for each possible last exit tree.
□
To use Theorem 3.3 we need a formula for τ(G,vk). Theorem 3.4 connects
τ(G,vk) with the Laplacian matrix of G.
For a graph G with vertex set V = {v1,...,vm} the Laplacian matrix L is the
n × n matrix with
Li,j =

−mi,j
if i ̸= j and there are mi,j edges from vi to vj,
outdeg(vi) −mi,i
if i = j and there are mi,i loops from vi to vi.
If all vertices have the same out-degree d, then L = dIn −A, where A =
(mi,j)1≤i,j≤n is the adjacency matrix of G.
Theorem 3.4 (see [267], Theorem 3.6) Let G be a directed graph with vertex set
V = {v1,...,vn}. Let L be the Laplacian matrix of G and let L0 be the matrix L
with the k-th row and k-th column deleted.
Then
τ(G,vk) = detL0.
If, furthermore, G is balanced and μ1,...,μn−1,μn = 0 are the eigenvalues
of L, then
τ(G,vk) = 1
nμ1 ···μn−1.
Proof We prove τ(G,vk) = detL0 by induction on the number of edges of G.
First note that if G is disconnected, τ(G,vk) = 0, G1 is the component contain-
ing vk and G2 is the rest of the graph, then we have detL0 = detL0(G1)detL(G2) =
0, since the Laplacian matrix L(G2) has eigenvalue 0. So we turn our attention to
connected graphs.
For connected graphs we prove the theorem by induction on the number of edges
of G. The smallest number of edges of G is n −1, since G is connected and in
this case the undirected graph corresponding to G is a tree. If G is not an oriented
tree with root vk, then there exists a vertex vj with out-degree 0. In this case, L0
contains a zero row, i.e. detL0 = 0 = τ(G,vk).
If, on the other hand, G is an oriented tree with root vk (and hence τ(G,vk) = 1),
then there is an ordering of the vertex set V \{vk} such that L0 is an upper triangular
matrix with ones on the main diagonal, i.e. detL0 = 1 = τ(G,vk).

3.1
De Bruijn Sequences
63
Now suppose that G has m > n −1 edges and that the theorem holds for all
graphs with at most m−1 edges. We can further assume that G has no edge starting
at vk, since such an edge neither contributes to τ(G,vk) nor to L0. Since G has at
least n edges, there must be a vertex vj ̸= vk with out-degree at least 2. Let e be one
of these edges and G1 be the graph with e removed. Let G2 be the graph where all
the edges starting from vj, except e, have been removed.
By induction, detL0(G1) = τ(G1,vk) and detL0(G2) = τ(G2,vk).
Since an oriented tree with root vk contains exactly one edge starting at vj, we
have τ(G,vk) = τ(G1,vk) + τ(G1,vk). On the other hand, the multi-linearity of
the determinant implies detL0(G) = detL0(G1) + detL0(G2). So by induction
detL0 = τ(G,v,k ).
Now we prove detL0 = 1
nμ1 ···μn−1 for a balanced digraph G. Note ﬁrst that
for a balanced digraph the Laplacian matrix L has the property that every row sum
and every column sum is 0.
We compute the characteristic polynomial χ(L) = det(L −xIn). Add all rows
except the last row to the last row. This operation does not change the determinant
and the last row is now (−x,...,−x), since the column sums of L are 0. We can
put the −x before the determinant. Now add all columns except the last column
to the last column. The determinant is not changed and, since the row sums of L
are 0, we have the transformed the last column to (−x,...,−x,n)t. Developing the
determinant by the last column shows that the coefﬁcient of x in χ(L) = det(L −
xIn) is −ndetL0.
On the other hand χ(L) = n
j=1(μj −x) and hence the coefﬁcient of x is
−n
j=1

k̸=j μk = −n−1
j=1 μj, which proves the second part of the theorem.
□
Now we are ready to count the de Bruijn sequences.
Proof of Theorem 3.2 Let A be the adjacency matrix of the de Bruijn graph Dn−1,
i.e. let A = (aij) with
aij =

1
if (Vi,vj) is an edge,
0
otherwise.
We will determine the eigenvalues of A.
Note that two vertices (a1,...,an−1) and (b1,...,bn−1) of the de Bruijn
graph Dn−1 are connected by exactly one path of length n −1, namely the path
(a1,...,an−1), (a2,...,an−2b1), ..., (an−1,b1,...,bn−2), (b1,...,bn−1). Thus
An−1 = J , where J is the 2n−1 × 2n−1 matrix with all entries 1. As J has eigen-
value 0 with multiplicity 2n−1 −1 and the eigenvalue 2n−1 once, the eigenvalues of
A must be 0 (2n−1 −1 times) and 2λ (once) where λ is a 2n−1th root of unity.
Since Dn−1 has two loops, the trace of A is 2, and hence λ = 1.
Thus L0 = 2I −A has the eigenvalues 2 (2n−1 −1 times) and 0 (once).

64
3
Non-linear Combinations of LFSRs
Fig. 3.2 The Geffe generator
By Theorem 3.4
τ(Dn−1,vk) =
1
2n−1
2n−1−1

j=1
2 = 22n−1−n
which is, by Theorem 3.3, the number of Eulerian cycles in the de Bruijn Graph
Dn−1. Each Eulerian cycle leads to 2n possible de Bruijn sequences (with different
starting points).
□
3.2 A Simple Example of a Non-linear Combination of LFSRs
In the previous section we have learned about de Bruijn sequences which have some
desirable statistical properties, but we don’t have a fast algorithm to generate such
sequences (see also Exercise 17.9).
An approach that retains the best parts of LFSRs (statistical properties, fast gen-
eration, simple design) but removes their cryptographic weakness is to use a non-
linear combination of linear feedback shift registers. This technique yields very
good results and many stream ciphers are based on this idea. For the remaining
part of the chapter we will only consider stream ciphers of this type.
A very simple generator of this kind was described by Geffe [104] (see Fig. 3.2).
A Geffe generator consists of three linear feedback shift registers. The output of
one of the three registers is used to decide which of the other two LFSRs is to be
used. In Fig. 3.2 we see that the output of the ﬁrst LFSR will be the output of the
Geffe generator if the second LFSR produces a 1. If the second LFSR produces a 0
we will see the output of the third LFSR as the output of the Geffe generator.
The Geffe generator exhibits many interesting ideas. Its output sequence is a
mixture of two LFSR sequences and it will inherit most of their statistical properties.
The non-linear elements will guarantee a high linear complexity of the resulting
sequence. (In the case of Fig. 3.2, the linear complexity is 161, as we will prove
later in this chapter.)
However, the Geffe generator fails to produce a secure keystream, as we will
see in the next section. In this chapter we will ﬁrst learn the basic attack strategies

3.3
Different Attack Classes
65
against stream ciphers based on non-linear combinations of LFSRs and then we
will learn how to select effective non-linear functions for the combination which
can avoid these attacks.
3.3 Different Attack Classes
3.3.1 Time-Memory Trade-off Attacks
The simplest possible attack against any cipher is a complete search over all possible
keys. If stream ciphers had no specialities at this point, this fact would not have been
worth mentioning.
First we can search the internal state instead of the key. This limits the effective
key size by the internal state. Further, there exists a time-memory trade-off, appar-
ently ﬁrst mentioned by Golic [109].
Assume that the cipher has 2s internal states. We generate a list of 2a random
states and the next few bits generated by the cipher form these states. Once we
have the list, we can attack the cipher by searching in the output sequence for a
subsequence that corresponds to one of the states in our list. If we have an output
sequence of length 2b and a + b > s, it is very probable that we can ﬁnd a good
subsequence.
Thus the internal state of the cipher must be signiﬁcantly larger than the key
size. The cipher A5/1 (see Sect. 8.3) has only 264 internal states, which makes it
vulnerable against this attack.
3.3.2 Algebraic Attacks
Let (xi)i∈N be the sequence generated by the Geffe generator and let (ai)i∈N,
(bi)i∈N and (ci)i∈N be the corresponding LFSR sequences. Then we can express
the plugging diagram of Fig. 3.2 by
xi = aibi + ci(bi + 1)
where all operations are performed over F2.
With the representation of an LFSR sequence as a linear code (see Sect. 2.2.4,
Eq. (2.18)) we can write
ai =
La−1

j=0
αj,iai,
bi =
Lb−1

j=0
βj,ibi,
ci =
Lc−1

j=0
γj,ici
with known coefﬁcients αj,i, βj,i and γj,i. Thus we can express xi as a non-linear
function of the unknown initial values a0,...,aLa,b0,...,bLb,c0,...,cLc.

66
3
Non-linear Combinations of LFSRs
This lead us to the so-called algebraic attacks. The attacker tries to solve a system
of non-linear equations.
The basic approach to solve such systems is to replace all products of variables
by a new artiﬁcial variable and then solve the resulting system of linear equations.
A stream cipher is strong against algebraic attacks if the non-linear function used
in the combination has a high degree. To be precise, the smallest degree of an anni-
hilator of the combination function must be large (see Sect. 6.2).
We will deal with algebraic attacks in detail in Chap. 6.
3.3.3 Correlation Attacks
Correlation attacks follow a divide and conquer principle.
In the case of the Geffe Generator we observe the following: The output of the
Geffe generator is always equal to the output of the ﬁrst shift register if the second
LFSR outputs 1, but if the second LFSR outputs 0, we still have a 1/2 chance that
the third LFSR, and therefore the Geffe generator, will produce the same output as
the ﬁrst LFSR. Thus
P(xi = ai) ≈3
4.
An attacker can use this correlation in the following way: Instead of searching
over all 2LA+LB+LC possible internal states he enumerates only all 2LA internal
states of the ﬁrst LFSR and checks if the corresponding sequence (ai) is correlated
to the output of the Geffe generator. Once he knows a part of the internal state,
recovering the remaining parts is easy.
We will treat such attacks in Chap. 4. Later in this chapter we will deﬁne the
notion of a k-correlation immune function as a measure of resistance against these
attacks.
3.4 Non-linear Combinations of Several LFSR Sequences
In this section we study the combination of LFSR sequences in general. Thus let
(a(j)
i
)i∈N, (1 ≤j ≤k), be different LFSR sequences and let C : {0,1}k →{0,1} be
a Boolean function. We deﬁne a new sequence (bi)i∈N by
bi = C

a(1)
i
,...,a(k)
i

for all i ∈N.
We have to ask ourselves: Can we give necessary and sufﬁcient criteria for the
sequence (bi)i∈N to be good for cryptographic purposes?
We start by determining the linear complexity of the sequence (bi)i∈N. Let Lj
be the linear complexity of (a(j)
i
)i∈N and let fi denote the corresponding feedback
polynomial.

3.4
Non-linear Combinations of Several LFSR Sequences
67
For the case k = 2 and C(a(1),a(2)) = a(1) + a(2) we solved this problem in
Theorem 2.9. The next basic function we have to consider is multiplication.
3.4.1 The Product of Two LFSRs
For this section, let k = 2 and C(a(1),a(2)) = a(1)a(2). Note that this combiner is it-
self not suitable for cryptographic purposes. If a(1) and a(2) are independent random
values with P(a(1) = 1) = P(a(2) = 1) = 1
2, the output C(a(1),a(2)) will take the
value 1 only with probability 1
4. This is clearly unacceptable for a key stream. We
study multiplication only because it is needed as basic operation in more complex
combiners.
We are especially interested in the cases with the highest possible linear com-
plexity. The following theorem characterizes these cases.
Theorem 3.5 Let (a(1)
i
)i∈N and (a(2)
i
)i∈N be two LFSR sequences. Let f1 and f2 be
the feedback polynomials of the sequences and let Li = degfi. The linear complex-
ity L of the sequence (bi)i∈N = (a(1)
i
a(2)
i
)i∈N satisﬁes
L ≤L1L2
(3.1)
with equality if and only if at most one of the polynomials f1 and f2 has a multiple
root and ζ1ζ2 = ζ ′
1ζ ′
2 for zeros ζ1,ζ ′
1 of f1 and zeros ζ2,ζ ′
2 of f2 implies ζ1 = ζ ′
1 and
ζ2 = ζ ′
2.
Proof We use the closed formula (2.13) to express a(1)
i
and a(2)
i
, i.e. we have
a(l)
i
=
n(l)
1

j=0
b(l)
j,1

ζ (l)
j
i +
n(l)
2

j=0
b(l)
j,2i

ζ (l)
j
i + ··· +
n(l)
kl

j=0
b(l)
j,kik−1
ζ (l)
j
i
for l = 1,2
with n(l)
1 ≥n(l)
2 ≥··· ≥n(l)
ki . The ζ (l)
j
are the roots of the polynomial f (l)∗, i.e.
f (l) =
kl

k′=1
nk′

j=1

1 −zζ (l)
j

=
n(i)
1
j=1

1 −zζ (l)
j
e(l)
j
where el
j = max{i | n(l)
i
≥j} is the multiplicity of the root ζ (l)
j
of f (l).
Multiplying the closed formulas for a(1)
i
and a(2)
i
we get
a(1)
i
a(2)
i
=
nl
1

j=0
n2
1

k=0
pj,k(i)

ζ (1)
j
ζ (2)
k
i

68
3
Non-linear Combinations of LFSRs
where pj,k is a polynomial of degree (e(i)
j −1) + (e(2)
k
−1). From the closed form
we see that the feedback polynomial of an LFSR generating (a(1)
i
a(2)
i
)i∈N is
f (z) =
n1

j=1

1 −zζ (1)
j
ζ (2)
k
(e(1)
j +e(2)
k −1).
If all products ζ (1)
j ζ (2)
j
are different then f (z) is also the feedback polynomial of
the minimal LFSR generating (a(1)
i
a(2)
i
)i∈N.
Since n(1)
1
j=1
n(2)
1
k=1(e(1)
j
+ e2
k −1) ≤n(1)
1
j=1 e1
j
n(2)
1
k=1 e2
k = L1L2 with equality if
and only if either all e(1)
j
= 1 or all e(2)
k
= 1, this is the desired bound for the linear
complexity of (a(1)
i
a(2)
i
)i∈N.
□
If the feedback polynomials f1 and f2 are irreducible or primitive we can sim-
plify Theorem 3.5.
Corollary 3.6 Let (a(1)
i
)i∈N ∈FN and (a(2)
i
)i∈NFN be two LFSR sequences of linear
complexity L1 and L2, respectively. Let L be the linear complexity of the sequence
(bi)i∈N = (a(1)
i
a(2)
i
)i∈N. Then L = L1L2
(a) if gcd(L1,L2) = 1 and the feedback polynomials f1 and f2 are irreducible,
separable and do not have roots ζ ̸= ζ ′ with ζζ ′−1 ∈F; or
(b) if 1 < L1 < L2 and the feedback polynomials are primitive.
Proof
(a) The condition gcd(L1,L2) = 1 implies that F = F(f!) ∩F(f2). Hence ζ1ζ2 =
ζ ′
1ζ ′
2 with roots ζ1,ζ ′
1 of f1 and roots ζ2,ζ ′
2 of f2 implies ζ1ζ ′−1
1
= ζ ′
2ζ −1
2
∈
F(f!) ∩F(f2) = F. By assumption this implies ζ1 = ζ ′
1 and ζ2 = ζ ′
2. Thus the
conditions of Theorem 3.5 hold and L = L1L2.
(b) Let F = Fq, n = lcm(L1,L2) and let ζ be a primitive element of Fqn. Then
ζ (qn−1)/(qL1−1) is a primitive element of FqL1 and ζ (qn−1)/(qL2−1) is a primitive
element of FqL2 .
The zeros of the primitive polynomial f1 are of the form ζ (qn−1)/(qL1−1)a1qx
with x ∈{0,...,L1−1} and a1 is some constant relatively prime to qn−1. Sim-
ilarly the zeros of f2 are of the form ζ (qn−1)/(qL2−1)a2qy with y ∈{0,...,L2−1}
and a2 is some constant relatively prime to qn −1.
The existence of two different zeros ζ1,ζ ′
1 of f1 and two different zeros ζ2,ζ ′
2
with ζ1ζ ′−1
1
= ζ −1
2 ζ ′
2 is equivalent to
ζ (qn−1)/(qL1−1)a1qx(qx′−1) = ζ ±(qn−1)/(qL2−1)a2qy(qy′−1)
(3.2)
with x,x′ < L1 and y,y′ < L2.

3.4
Non-linear Combinations of Several LFSR Sequences
69
Since ζ is a generator of the cyclic group F×
q , we get
qn −1
qL1 −1a1qx
qx′ −1

≡qn −1
qL2 −1a2qy
qy′ −1

mod qn −1.
Multiplication by qL1 −1 and the inverse element of a2qy yields
0 ≡qn −1
qL2 −1

qL1 −1

qy′ −1

mod qn −1
and hence
qL2 −1 |

qL1 −1

qy′ −1

.
But this is false since
r =

qL1 −1

qy′ −1

−qL1+y′−L2
qL2 −1

= qL1+y′−L2 + 1 −qL1 −qy′
lies between qL2 −1 and −(qL2 −1) and is non-zero.
Thus we have veriﬁed the conditions of Theorem 3.5, i.e. L = L1L2.
□
Theorem 3.5 sufﬁces if we are only interested in the linear complexity of the
sequence, but sometimes we also want the feedback polynomial of the resulting
sequence. The proof of Theorem 3.5 is constructive in the sense that we have given
a formula for the feedback polynomial of the sequence (bi)i∈N. The disadvantage
of that formula is that we have to compute the roots of the polynomials f1 and f2.
Hence we must leave Fq and work in a bigger ﬁeld Fqn. This is computationally
very expensive, so it is desirable to ﬁnd a way to do the computation in the ground
ﬁeld Fq.
The right tool for this is the resultant. Let us recall the basic properties of resul-
tants.
Let f (z) = anzn + ··· + a0 = an(z −α1)···(z −αn) and g(z) = bmzm + ··· +
b0 = bm(z −β1)···(z −βm) be two polynomials. Then the resultant of f (z) and
g(z) is
Res

f (z),g(z)

=

an
an−1
...
a0
an
an−1
...
a0
...
...
...
an
an−1
...
a0
bm
bm−1
...
b0
bm
bm−1
...
b0
...
...
...
bn
bm−1
...
b0

= am
n bn
m
n

i=0
m

j=0
(αi −βj).

70
3
Non-linear Combinations of LFSRs
Theorem 3.7 describes how to use the resultant to compute the feedback polyno-
mial of a product of two LFSRs.
Theorem 3.7 Let (a(1)
i
)i∈N and (a(2)
i
)i∈N be two (binary) LFSR sequences. Let f1
and f2 be the feedback polynomials of the sequences. Let (bi)i∈N = (a(1)
i
a(2)
i
)i∈N.
Then
f (z) = Resx

f1(x),f ∗
2 (zx)

(3.3)
is the feedback polynomial of an LFSR generating (bi)i∈N. (Here f ∗
2 (x) =
f2(1/x)xd2 denotes the reciprocal polynomial to f2(x)—see also Deﬁnition 2.4.)
Under the condition given in Theorem 3.5, this is the feedback polynomial of the
minimal LFSR generating (bi)i∈N.
Proof Let f1(z) = L1
j=1(1 −zζ (1)
j
) and f2(z) = L2
j=1(1 −zζ (2)
j
). In the proof
of Theorem 3.5 we saw that f (z) = L1
j=1
L2
k=1(1 −zζ (1)
j
ζ (2)
k ) is the feedback
polynomial of an LFSR generating (bi)i∈N.
The polynomial f1(x) has zeros
1
ζ (1)
j
and leading coefﬁcient L1
j=1 ζ (1)
j
. The poly-
nomial f ∗
2 (x) has leading coefﬁcient 1 and zeros ζ (2)
k
and hence f ∗
2 (zx) has zeros
ζ (2)
k
z
and leading coefﬁcient zL2 (as a polynomial in x).
Thus the resultant of f1(x) and f (∗)
2
(zx) is
Resx

f1(x),f ∗
2 (zx)

=
 L1

j=1
ζ (1)
j
L2
zL2L1
L1

j=1
L2

k=1
 1
ζ (1)
j
−ζ (2)
k
z

=
L1

j=1
L2

k=1

z −ζ (1)
j
ζ (2)
k

,
i.e. it is the polynomial determined in the proof of Theorem 3.5.
□
3.4.2 General Combinations
With Theorem 2.9 and Theorem 3.5 we have covered the two basic operations that
allow us to determine the linear complexity of arbitrary non-linear combinations
of linear feedback shift registers. We summarize the results in Theorem 3.8 (see
also [230]).
Theorem 3.8 For a Boolean function C : Fn
2 →F2 in algebraic normal form, let
ˆC : Z →Z be the function we obtain if we replace the operations in F2 by operations
in Z.

3.4
Non-linear Combinations of Several LFSR Sequences
71
Let (x(k)
i
)i∈N be LFSRs with linear complexity Li. Then the linear complexity of
the sequence C(x(1)
i
,...,x(n)
i
) is at most ˆC(L1,...,Ln).
If, in addition, the feedback polynomials fi of the LFSR (x(k)
i
)i∈N are all prim-
itive and of pairwise different degree, then the linear complexity of the sequence
C(x(1)
i
,...,x(n)
i
) is exactly ˆC(L1,...,Ln).
Proof The upper bound follows directly from the upper bound given in Theorem 2.9
and Theorem 3.5.
The proof that the upper bound is sharp for primitive feedback polynomials of
pairwise different degree is a variation of the proof for two polynomials (Corol-
lary 3.6).
Let L = lcm(L1,...,Ln) and let ζ be a primitive element of FqL. Then
ζ (qL−1)/(qLi −1) is a primitive element of FqLi and the zeros of fi have the form
ζ (qL−1)/(qLi −1)aiqx for some constant ai relatively prime to qL −1.
Now let (ζ1,...,ζn) ̸= (ζ ′
1,...,ζ ′
n) where ζi and ζ ′
i are either roots of fi or 1.
We have to prove that
ζ1 ···ζn ̸= ζ ′
1 ···ζ ′
n.
Assume conversely that ζ1 ···ζn = ζ ′
1 ···ζ ′
n. Without loss of generality we can as-
sume that Ln > ··· > L1 and let n′ be the largest index with ζn′ ̸= ζ ′
n′.
This leads to the equation
ζ (qL−1)/(qL1−1)a1qx1b1 ···ζ (qL−1)/(qLn′−1−1)a1qxn′−1bn′−1
= ζ (qL−1)/(qLn′ −1)an′qxn′ bn′
(3.4)
where the factor bi is either ±(qx′
i −1) for some x′
i ∈{1,...,Li −1} if ζi and ζ ′
i
are different roots of fi. If either ζi or ζ ′
i is 1 then bi = ±1 and bi = 0 if ζi = ζ ′
i .
Compare Eq. (3.4) to Eq. (3.2) in the proof of Corollary 3.6.
We continue as in the proof of Corollary 3.6 and obtain

qL −1

/

qL1 −1

a1qx1b1 + ··· +

qL −1

/

qLn′−1 −1

an′−1qxn−1b1
≡

qL −1

/

qLn′ −1

an′qx1bn′
mod qL −1,
since ζ is a generator of the cyclic group FqL.
Multiplying by (qL1 −1)···(qLn′−1 −1) we get
0 ≡

qL −1

/

qLn′ −1

anqx1bn

qL1 −1

···

qLn′−1 −1

mod qL −1
and hence
qLn′ −1 | bn

qL1 −1

···

qLn′−1 −1

.
(3.5)
By Zsigmondy’s theorem (see Sect. 13.4) we know that qLn′ −1 contains a prime
factor not in Ln′−1
j=1 (qj −1) except for the cases q = 2, Ln′ = 6 or where q + 1 is
a power of 2 and Ln′ = 2.

72
3
Non-linear Combinations of LFSRs
Fig. 3.3 A simple non-linear
ﬁlter
Equation (3.5) is obviously false if such a prime factor exists. In the case Ln′ = 2,
i.e. if only two factors exist, we have proved the result in Corollary 3.6.
So the only case left is q = 2, Ln′ = 6, i.e. we have feedback polynomials of
degree 2, 3 and 6. For this case we can check directly that the products of different
roots are different.
□
3.5 Non-linear Filters
A special case of a non-linear combination of feedback shift registers is a non-
linear ﬁlter. In this case we have just a single LFSR sequence (ai)i∈N and deﬁne a
new sequence by bi = C(ai+t0,ai+t1,ai+tk−1) for a non-linear function C and some
constants 0 = t0 < t1 < ··· < tk−1.
Figure 3.3 shows an example of a non-linear ﬁlter. For an example from a real
world application, see Sect. 8.2.1.
The main advantage of non-linear ﬁlters is that against algebraic attacks they are
almost as resistant as non-linear combinations of different LFSRs, but they use less
internal memory. A disadvantage is that their statistical behavior is harder to analyze
and so correlation attacks become more dangerous.
The upper bound of Theorem 3.8 of course still holds for non-linear ﬁlters, but
we cannot expect it to be sharp. The following theorem improves the bound on the
linear complexity of non-linear ﬁlters.
Theorem 3.9 Let (ai)i∈N ∈FN
2 be a binary LFSR sequence with primitive feedback
polynomial of degree L over Fq. Let C be a Boolean function of degree d in k
variables.
Then for 0 = t0 < t1 < ··· < tk−1 the linear complexity of the sequence bi =
C(ai+t0,ai+t1,ai+tk−1) is bounded by
L

(bi)i∈N

≤
d

j=1
L
j

.
Proof Let f (x) = L−1
j=0 (x −ζ 2j ) be the feedback polynomial of the LFSR gener-
ating (ai)i∈N.

3.5
Non-linear Filters
73
By Theorem 3.5 the feedback polynomial of the product of d LFSR sequences
with feedback polynomial f has zeros of the form ζ 2t1+···+2td = ζ a, where a is a
number which has at most d ones in its binary representation.
By Theorem 2.9, any linear combination of products of up to d LFSRs with
feedback polynomial f can be generated by an LFSR with feedback polynomial
g(x) =

x −ζ a
where the product runs over all a which have at most d ones in its binary represen-
tation. The degree of g is d
j=1
L
j

.
□
For constructing strong ciphers, lower bounds are of greater interest than upper
bounds. In the sequel we will derive such a lower bound.
We start with a lemma that allows us ﬁnd roots of the minimal polynomial of the
product of distinct phases of an LFSR sequence.
Lemma 3.10 Let (ai)i∈N be a binary LFSR sequence generated by an LFSR of
degree L with primitive feedback polynomial f and let ζ be a root of f .
Let g be the minimal feedback polynomial of the sequence (bi)i∈N deﬁned by
bi = ai+t0 ···ai+tk−1.
Let 0 < e < 2L be a number whose binary representation needs k ones, i.e. e =
2e0 + ··· + 2ek−1 with 0 ≤e0 < e1 < ··· < ek−1 < L.
Then ζ e is a root of g if and only if the determinant
De =

ζ t02e0
ζ t12e0
···
ζ tk−12e0
ζ t02e1
ζ t12e1
···
ζ tk−12e1
...
...
...
...
ζ t02ek−1
ζ t12ek−1
···
ζ tk−12ek−1

is not equal to 0.
Proof Without loss of generality we may assume that
ai = TrF2L/F2

ζ i
(see Theorem 2.1).

74
3
Non-linear Combinations of LFSRs
Then
bi =
k−1

j=0
ai+tj
=
k−1

j=0
TrF2L/F2

ζ i+tj 
=
k−1

j=0

ζ i+tj + ζ 2(i+tj ) + ··· + ζ 2L−1(i+tj )
.
ζ e is a root of the minimal feedback polynomial for the sequence (bi)i∈N if in the
expansion bi = 
e Aeζ ei the coefﬁcient Ae of ζ ei is non-zero.
Expanding the product and comparing the coefﬁcients we get for e = 2e0 + ··· +
2ek−1, that
Ae =

σ∈Sn
k−1

j=0
ζ 2ej tσ(j) = per
⎛
⎜⎜⎜⎝
ζ t02e0
ζ t12e0
···
ζ tk−12e0
ζ t02e1
ζ t12e1
···
ζ tk−12e1
...
...
...
...
ζ t02ek−1
ζ t12ek−1
···
ζ tk−12ek−1
⎞
⎟⎟⎟⎠
where per(M) denotes the permanent of the matrix M.
For characteristic 2, addition and subtraction is the same, and hence the perma-
nent and determinant of a matrix coincide, which completes the proof.
□
Corollary 3.11 Let (ai)i∈N be a binary LFSR sequence generated by an LFSR of
degree L and let
bi = aiai+j ···ai+(k−1)j.
Then L((bi)i∈N) ≥
L
k

.
Proof With the notation of Lemma 3.10 we must verify that
De =

ζ 0·j2e0
ζ 1·j2e0
···
ζ (k−1)·jtk−12e0
ζ 0·j2e1
ζ 1·jt12e1
···
ζ (k−1)·j2e1
...
...
...
...
ζ 0·j2ek−1
ζ 1·jt12ek−1
···
ζ (k−1)·j2ek−1

is non-zero.
But this is a Vandermonde determinant, thus
De =

0≤t<t′<k

ζ j2et′
−ζ j2et 
̸= 0.

3.6
Correlation Immune Functions
75
Hence for every e = 2e0 + ··· + 2ek−1 with 0 ≤e0 < e1 < ··· < ek−1 < L the mini-
mal polynomial of (bi)i∈N has ζ e as a zero.
□
Corollary 3.12 (see [21]) Let (ai)i∈N ∈FN
2 be a binary LFSR sequence with prim-
itive feedback polynomial of degree L.
Let C be a Boolean function which contains the kth order term
aiai+j ···ai+(k−1)j
and an arbitrary number of terms of lower order.
Then the linear complexity of the sequence bi = C(ai,ai+1,ai+k) is bounded by
L

(bi)i∈N

≥
L
d

.
Proof Let ζ be a root of the feedback polynomial of the sequence (ai)i∈N.
As we have seen in Corollary 3.11, ζ e is a root of the feedback polynomial of the
sequence b′
i = aiai+j ···ai+(k−1)j if e has binary weight k.
Let b′′
i = bi −aiai+j ···ai+(k−1)j = C′(ai) where C′ is a function of degree
k′ < k. By Theorem 2.9 and Theorem 3.5 the feedback polynomial of (b′′
i )i∈N has
only zeros of the form ζ e′ where e′ has binary weight at most k′.
Thus, by Theorem 2.9, the feedback polynomial of the sum bi = b′
i + b′′
i has at
least the roots ζ e where the binary weight of e is k, since these roots occur only in
the summand b′
i and cannot cancel.
□
In [228] (page 84) this Corollary is generalized a little to allow certain combina-
tions of kth order terms.
3.6 Correlation Immune Functions
3.6.1 Deﬁnition and Alternative Characterizations
We learned in Sect. 3.3.3 that correlations between the input and the output of a
non-linear combiner can be explored by an attacker. This motivates the following
deﬁnition:
Deﬁnition 3.2
Let X1,...,Xn be uniformly independent and identically dis-
tributed (henceforth, iid.) random variables on Fq.
A function f : Fn
q →Fq is correlation immune of order t if for every set
{i1,...,it} of size t the random variable
f (X1,...,Xn)

76
3
Non-linear Combinations of LFSRs
is stochastically independent from
(Xi1,...,Xit ).
The function f is balanced if P(f (Xi1,...,Xin) = α) = 1
q .
A correlation immune function of order t that is balanced is called t-resilient.
There are several equivalent characterizations of correlation immune functions
(see [41, 118]). The stochastic characterization chosen for Deﬁnition 3.2 empha-
sizes the idea that we want to avoid dependencies between the input and the output
of the non-linear functions. The next lemma provides a purely combinatorial char-
acterization in terms of orthogonal arrays.
Deﬁnition 3.3 An orthogonal array A of size m, n constraints, strength t and index
λ over an alphabet F of size q is an m×n array of elements in F with the following
properties:
1. No two rows of A are the same.
2. For any subset Ft of t columns of A, each of the qt vectors of Ft appears exactly
λ times as a row. (Clearly m = λqt.)
Lemma 3.13 The function C : Fn
q →Fq is correlation immune of order t if and
only if for every y ∈Fq the set f −1(y) forms the rows of an orthogonal array of
strength t.
Proof Let X1,...,Xn be uniformly iid. random variables on Fq, then
P

f (X1,...,Xn) = y

= |f −1(y)|
qn
.
For a set i1,...,it of t columns, and for ˆxi1,..., ˆxit ∈Fq, let λ denote the number
of solutions of f (x1,...,xn) = y with xik = ˆxik for 1 ≤k ≤n. Then
P

f (X1,...,Xn) = y | Xik = ˆxik for 1 ≤k ≤t

=
λ
qn−t .
If f is correlation immune of order t, this probability must be equal to
P(f (X1,...,Xn) = y) for all choices of i1,...,it and ˆxi1,..., ˆxit ∈Fq, i.e. f −1(y)
forms the rows of an orthogonal array of strength t.
Conversely, if f −1(y) forms an orthogonal array then
P

f (X1,...,Xn) = y | Xik = ˆxik for 1 ≤k ≤t

=
λ
qn−t = m/qt
qn−t = |f −1(y)|
qn
for all choices i1,...,it and ˆxi1,..., ˆxit ∈Fq, i.e. f is correlation immune of or-
der t.
□

3.6
Correlation Immune Functions
77
Another characterization of correlation immunity uses the Walsh transform. In
Deﬁnition 2.10 we deﬁned the Walsh transform for functions f : F2n →F2. Now
we need it for functions f : Fn
2 →F2.
Deﬁnition 3.4
Let f : Fn
2 →F2 then the Walsh transform f W : Fn
2 →R of f is
deﬁned by
f W(x) =

y∈{0,1}n
f (y1,...,yn)(−1)x1y1+···+xnyn.
The following characterization of correlation immune functions is due to
G.-Z. Xiao and J.L. Massey [285].
Lemma 3.14 Let f : Fn
2 →F2. Then the following statements are equivalent:
• f is correlation immune of order t.
• f W(x) = 0 for every x ∈Fn
2 with 0 < w(x) ≤t.
Proof Let xS denote the characteristic vector of the set S.
Then
f W(xs)
=

T ⊆S
(−1)|T |2

y : f (y) = 1 ∧(i ∈T ⇒yi = 1) ∧(i ∈S\T ⇒yi = 0)


−2n−|S|.
(3.6)
If f is correlation immune of order t and |S| ≤t, the deﬁnition of correlation
immunity implies that

y : f (y) = 1 ∧(i ∈T ⇒yi = 1) ∧(i ∈S\T ⇒yi = 0)

is independent from T . Hence for every correlation immune function of order t,
Eq. (3.6) simpliﬁes to f W(xs) = 0.
We prove the converse implication by induction on t. For t = 0 there is nothing
to prove. Now let t ≥1. By induction we have that

y : f (y) = 1 ∧(i ∈T ⇒yi = 1) ∧(i ∈S\T ⇒yi = 0)

is independent from T for every |S| = t −1. For |S| = t and j ∈S and T ⊂S\{j}
we can write

y : f (y) = 1 ∧(i ∈T ⇒yi = 1) ∧

i ∈

S\{j}

\T ⇒yi = 0

=

y : f (y) = 1 ∧(i ∈T ⇒yi = 1) ∧(i ∈S\T ⇒yi = 0)

+

y : f (y) = 1 ∧(i ∈

T ∪{j} ⇒yi = 1

∧

i ∈S\

T ∪{j}

⇒yi = 0
.

78
3
Non-linear Combinations of LFSRs
This proves that |{y : f (y) = 1 ∧(i ∈T ⇒yi = 1) ∧(i ∈S\T ⇒yi = 0)}| depends
only on the parity of T .
By Eq. (3.6) we conclude that |{y : f (y) = 1 ∧(i ∈T ⇒yi = 1) ∧(i ∈S\T ⇒
yi = 0)}| is independent from t, i.e. f is correlation immune of order t.
□
3.6.2 Siegenthaler’s Inequality
To avoid correlation attacks we want the combiner function to be highly correlation
immune and to avoid algebraic attacks we want (among other things) the degree
of the combiner function to be high. In 1984 Siegenthaler [248] proved a trade-
off between these two parameters. A low degree is the price one has to pay for a
high correlation immunity. This result was generalized to non-binary functions by
Camion and Canteaut [42].
Theorem 3.15 (see [42] Theorem 7) Let f : Fn
q →Fq be correlation immune of
order t, then for every monomial μ in the algebraic normal form of f there exists a
subset T of size t such that degxi μ ≤q −2 for i ∈T .
If, in addition, f is balanced and n ̸= t + 1, then there exists a subset T of size
t + 1 such that degxi μ ≤q −2 for i ∈T .
Proof Let Lα(x) = 
α′∈Fq\{α}
x−α′
α−α′ denote the Lagrange interpolation polynomial.
Fix j of the n variables. Without loss of generality we may choose xn−j+1 =
ˆxn−j+1, ..., xn = ˆxn.
We denote the set {a ∈Fn−j
q
| f (a1,...,an−j,
ˆ
xn−j+1,..., ˆxn) = α} by Ij(α).
Since f is correlation immune of order t, we have
Ij(α)
 = |f −1(α)|
qj
for all j ≤t. Hence |Ij(α)| = qt−j|It(α)| is divisible by q for j < t.
The algebraic normal form of f (·, ˆxn−j+1,..., ˆxn) is
f (x1,...,xn−j, ˆxn−j+1,..., ˆxn) =

α∈Fq
α

a∈Ij (α)
n

j=1
Lai(xi).
Since each Lai is a monic polynomial of degree (q −1), the coefﬁcient of degree
(q −1)(n −j) of 
a∈Ij (α)
n
j=1 Lai(xi) is |Ij(α)|. For j < t and n ̸= t + 1 this is
divisible by q, and hence the coefﬁcient is 0 in Fq.
So if we ﬁx t −1 variables in f we always obtain an algebraic normal form of
degree less than (q −1)(n−t +1). This is only possible if the algebraic normal form
f does not contain a product of more than n −t variables simultaneously having
degree q −1.

3.6
Correlation Immune Functions
79
If, in addition, f is balanced then |f −1(α)| = qn−1 and hence |Ij(α)| = qn−j−1
for 1 ≤j ≤t. Thus |Ij(α)| = 0 in Fq, i.e. if we ﬁx t variables we always obtain an
algebraic form of degree less than (q −1)(n −t).
□
Corollary 3.16 (Siegenthaler’s inequality) Let f : Fn
q →Fq be correlation immune
of order t and let d be the algebraic degree of f . Then
d + t ≤(q −1)n.
(3.7)
If in addition f is balanced then
d + t ≤(q −1)n −1.
(3.8)
Functions that achieve equality in Corollary 3.16 have optimal non-linearity.
The next lemma describes how to construct functions with optimal non-linearity.
Lemma 3.17 Let f1 and f2 be correlation immune of order t in n variables with
p(f1 = α) = p(f2 = α) for all α. Then
f (x1,...,xn+1) = xq−1
n+1f1(x1,...nxn) +

1 −xq−1
n+1

f2
is also correlation immune of order t.
Proof The function f satisﬁes
P(f = α) = P(xn+1 ̸= 0)P(f1 = α) + P(xn+1 = 0)P(f2 = α)
= P(f1 = α) = P(f2 = α).
Since f1 is correlation immune of order m, we have for ˆxn+1 ̸= 0
P(f = α|ˆxi1,...,xim = ˆxim−1,xn+1 = ˆxn+1)
= P(f1 = α|ˆxi1,...,xim = ˆxim−1)
= P(f1 = α) = P(f = α).
Similarly, for ˆxn+1 = 0 the equation follows from the correlation immunity of f2.
For 1 ≤ˆxi1,...,xim ≤n we have
P(f = α | xi1 = ˆxi1,...,xim = ˆxim)
= P(xn+1 ̸= 0)P(f1 = α,xi1 = ˆxi1,...,xim = ˆxim)
+ P(xn+1 = 0)P(f2 = α,xi1 = ˆxi1,...,xim = ˆxim)
= P(xn+1 ̸= 0)P(f1 = α) + P(xn+1 = 0)P(f2 = α)
= P(f = α).
Thus f is correlation immune.
□

80
3
Non-linear Combinations of LFSRs
If both functions f1 and f2 have optimal non-linearity and deg(f1) = deg(f2) =
deg(f1 −f2), the newly constructed function f also has optimal non-linearity.
So all we need to construct arbitrary functions of optimal non-linearity are n −1
correlation immune functions in n variables of high degree as a starting point for a
recursive application of Lemma 3.17.
For q = 2, linear functions will do the job. For example, we can start with
f1(x1,x2,x3,x4) = x1 + x2 + x3
f2(x1,x2,x3,x4) = x1 + x2 + x4
and apply Lemma 3.17 to obtain the function
f (x1,...,x5) = x5f1(x1,x2,x3,x4) + (1 + x5)f2(x1,x2,x3,x4)
= x1 + x2 + x4 + x3x5 + x4x5
which is 3-resilient and of degree 2. Permuting the variables of f we get a sec-
ond function f ′ for the next application of Lemma 3.17. For example the cyclic
permutation gives us
f ′(x1,...,x5) = x2 + x3 + x5 + x4x1 + x5x1.
By Lemma 3.17
ˆf (x1,...,x5,x6) = x6f (x1,...,x5) + (1 + x6)f ′(x1,...,x4)
= x2 + x3 + x5 + x1x4 + x1x5 + x1x6 + x3x6 + x4x6
+ x5x6 + x1x4x6 + x1x5x6 + x3x5x6 + x4x5x6
is 4-resilient.
This recursive construction leads to:
Corollary 3.18 For every n and every degree d there exists an (n −d)-resilient
binary function f of degree d in n variables.
Remark: In [42] an analogous result is proved for all q ≡2 mod 3.
3.6.3 Asymptotic Enumeration of Correlation Immune Functions
Lemma 3.17 gives us a way of explicitly constructing a correlation immune function
of order t. However, we may also ask: what is the probability that a random Boolean
function is correlation immune of order t? To answer this question we must deter-
mine the number N(n,t) of correlation immune functions of order t in n variables.

3.6
Correlation Immune Functions
81
We will ﬁnd an asymptotic expression for the number N(n,t,λ) of correlation
immune functions of order t in n variables and weight λ2n. The case of balanced
functions λ = 1
2 is especially important for cryptography.
The special case t = 1 has also been studied under the name balanced color-
ings of a hypercube. These are placements of equal weight to some vertices of a
hypercube such that the centroid is at the center of the hypercube. Upper and lower
bounds are given in [12, 193, 287]. There also exist exact enumerations for this
case [202, 288].
Bounds for the general case are given in [44, 45, 235].
The ﬁrst person to solve the asymptotic enumeration of correlation immune
Boolean functions was O.V. Denisov [76]. However his arguments were quite com-
plicated. In a later article [77] he withdraw his result and presented a “corrected”
value. Unfortunately the correction contained a mistake and in fact Denisov’s ﬁrst
result was correct. In the following we will follow E.R. Canﬁeld, Z. Gao, C. Green-
hill, B.D. McKay and R.W. Robinson [43] who found a more elegant proof of
Denisov’s Theorem.
The complement of a correlation immune function of order t is also correlation
immune of order t. Hence N(n,t,λ) = N(n,t,1 −λ). In the following we will
therefore restrict ourselves to the case λ ≤1/2.
Theorem 3.19 (Theorem 1.2 of [43]) Let
Mt,n =
t
j=0
n
j

and
Qt,n =
t
j=0
j
n
j

.
Consider a sequence of triples (n,t,λ) with n →∞and
ω

26t−nn6t+3M3
t,n

≤λ ≤1
2.
(3.9)
Then
N(n,t,λ) = 2Qn,t 
λλ(1 −λ)1−λ−2n
πλ(1 −λ)2n+1−Mt,n/2
1 + O

η(n,t,λ)

(3.10)
where
η(n,t,λ) = 2−n/2+3tn3t+3/2M3/2
n,t λ−1/2(1 −λ)−1/2 = o(1).
(3.11)
We will divide the proof of Theorem 3.19 into several lemmas.
Lemma 3.20 Let It be the set of all subsets of {1,...,n} of size at most t. Let
x = (xS)S∈It be a vector of Mt,n variables.
Let F : CMt,n →C be deﬁned by
F(x) =

α∈{±1}n

1 + D

S∈It
xαS
S

(3.12)

82
3
Non-linear Combinations of LFSRs
where
αs =

j∈S
αj
(3.13)
and D is an arbitrary constant.
Then
N(n,t,λ) =
1
(2πi)Mt,nD2nλ
"
···
"
F(x)
x2nλ
∅

S∈It xS
dx
(3.14)
where each xS is integrated anticlockwise around a circle of radius 1 centered at
the origin.
Proof For α ∈{±1}n let ¯α denote the vector in {0,1}n that arises from α by chang-
ing the 1 entries into 0s and the −1 entries into 1s. The map α →¯α is an isomor-
phism from {±1}n with pointwise multiplication to the Boolean vector space Fn
2.
With this notation we can write αS = 
j∈S αj = (−1)¯αws where wS is the inci-
dence vector of the set S. Note that (−1)¯αws is precisely the term that occurred in
the Walsh transform.
When expanded, F(x) is a sum of 22n terms. Each summand corresponds to one
Boolean function. For a Boolean Function g we have the term
tg =

α∈{±1}n
g(¯α)=1

D

S∈It
xαS
S

= D|{α|g(¯α=1)}| 
S∈It

α∈{±1}n
g(¯α)=1
x(−1)¯αws
S
= D ˆg(0) 
S∈It
x

¯α∈{0,1}n g(¯α)(−1)¯αws
s
= D ˆg(0) 
S∈It
x ˆg(wS)
S
.
By Lemma 3.14 the Boolean function g is correlation immune of order t if and
only if its Walsh transform ˆg vanishes for all wS with ∅̸= S ∈It. Therefore the coef-
ﬁcient of xλ2n
∅
in F(x) is the number N(n,t,λ) of all correlation immune functions
of order t with ˆg(0) = λ2n.
Consider the function F(x)/(xλ2n
∅

S∈It xs). It has a pole at 0 and the residue of
the pole is Dλ2nN(n,t,λ). By Cauchy’s integration formula we obtain the residue
by integrating anticlockwise around the pole, which is exactly the statement of the
lemma.
□
Using the parametrization eiθ, θ ∈[0,2π), for the unit circle in the complex
plane, we can change the complex integral (3.14) into a real integral. We set xS =

3.6
Correlation Immune Functions
83
eiθS, then dxS = ieiθSdθS. A change of variables gives:
N(n,t,λ) =
1
(2π)Mt,nD2nλ
# 2π
0
···
# 2π
0
e−i2nλθ∅

α∈{±1}n

1 + Deifα(θ)
dθ
=
(1 + D)2n
(2π)Mt,nD2nλ
# 2π
0
···
# 2π
0
e−i2nλθ∅

α∈{±1}n
1 + Deifα(θ)
1 + D
dθ
(3.15)
where
fα(θ) =

S∈It
αSθS.
(3.16)
We denote the integrand of the integral in (3.15) by G(θ). Note that G(θ) is a
product of terms of the form 1+Deifα(θ)
1+D
. For D > 0 we have |1 + Deifα(θ)| ≤1 + D
with equality if and only if fα(θ) ≡0 mod2π. Thus G(θ) ≤1.
We will prove that G(θ) = 1 only at 2Qt,n points. Outside a critical region around
these points G(θ) is a product of many terms strictly less than 1, i.e. G(θ) is small
outside the critical region and this part of the integral will be hidden in the O-term of
Theorem 3.19. The only signiﬁcant contribution will come from the integral inside
the critical region.
Let us start by determining all points with G(θ) = 1.
Lemma 3.21 Let
C =

θ ∈[0,2π)Mt,n  G(θ)
 = 1

.
(3.17)
Then
C =
$
θ ∈[0,2π)Mt,n
 2|S|

T ∈It,T ⊇S
θT ≡0
mod 2π for each S ∈It
%
(3.18)
and |C| = 2Qt,n.
Proof We already know that a point θ with G(θ) = 1 corresponds to a solution of
fα(θ) ≡0 mod2π for all α ∈{±1}n. All we have to do is to ﬁnd a linear transfor-
mation that brings this system into the form of Eq. (3.18).
For a set S, let AS denote the set of all α ∈{±1}n with αi = 1 for i /∈S. Let
σ(α) = n
i=1 αi.
Then

α∈AS
σ(α)fα(θ) =

α∈AS

T ∈It
αT θT
=

T ∈It
θT

α∈AS
σ(α)αT

84
3
Non-linear Combinations of LFSRs
and for S ⊆T we have σ(α) = n
i=1 αi = 
i∈T αi = αT since αi = 1 for i /∈S ⊂
T . So the inner sum simpliﬁes to 
α∈AS α2
T = 
α∈AS 1 = 2|S|.
If S ⊈T , let j ∈S\T . Let α′ differ from α by changing the sign of αj. We have
σ(α) = −σ(α′) but since j /∈T we have αT = α′
T . The inner sum consists of 2|S|−1
pairs of the form σ(α)αT + σ(α′)α′
T which are equal to 0. Hence the coefﬁcient
variable θT vanishes.
Thus

α∈AS
σ(α)fα(θ) = 2|S|

T ∈It,T ⊇S
θT
which proves that
C ⊆
$
θ ∈[0,2π)Mt,n
 2|S|

T ∈It,T ⊇S
θT ≡0
mod 2π for each S ∈It
%
but the linear system given in Eq. (3.18) is upper triangular and hence it has full
rank, which proves the equality.
(Note that we used in our argument only that the elements of As agree on every
coordinate not in S. The only difference is that the sign of the factor 2|S| may swap.)
Since the system in (3.18) is upper triangular we can solve it by successively
assigning values to θS, starting with the largest sets ﬁrst. We work modulo 2π and
the coefﬁcient of θS is 2|S|, so we have 2|S| possible choices for θS (which differ
by an integral multiple of
2π
2|S| ). Altogether we have 
S∈It 2|S| = 2Qt,n solutions
of (3.18).
□
Now we deﬁne a small rectangular region R around the origin. Let
R =

θ ∈RMt,n | |θS| ≤Δ(2n)−|S| for all S ∈It

(3.19)
where Δ is a constant factor which will be determined later in the proof.
We split the domain of integration into the critical region C + R (the addition
must be done modulo 2π) and its complement C + R.
Lemma 3.22 For θ /∈C + R we have
|G| < exp

−
4D
π2(1 + D)2 2n−2|S|
2 −e−1/2
Δn−|S|

.
(3.20)
Furthermore
#
C+R
G(θ)dθ < (2π)Mt,n exp

−
4D
π2(1 + D)2 2n−2|S|
2 −e−1/2
Δn−|S|

. (3.21)
Proof Let d(x) = min{|x −k2π| | k ∈Z}. d deﬁnes a metric on the set of real
numbers modulo 2π.

3.6
Correlation Immune Functions
85
We claim that
d

2|S|

T ∈It,T ⊇S
θT

> Δn−|S|
for some S.
Choose θ′ ∈C as close as possible to θ with respect to the metric d. By the
deﬁnition of R we can ﬁnd an S with the following properties:
• d(θS −θ′
S) > Δ(2n)−|S|.
• d(θT −θ′
T ) ≤Δ(2n)−|T | for all T ⊃S.
Then
d

2|S|

T ∈It,T ⊇S
θT

= d

2|S|

T ∈It,T ⊇S
θT

+ d

2|S|

T ∈It,T ⊇S
θ′
T

since θ′ ∈C
≥2|S|

d

θS −θ′
S

−

T ∈It,T ⊃S
d

θT −θ′
T

triangle inequality
> 2|S|
&
(2Δ)−|S| −
t−|S|

j=1
n −|S|
j

Δ(2n)−|S|−j
'
by choice of S
> Δn−|S|
&
1 −
t−|S|

j=1
2−j
j!
'
> Δn−|S|
2 −e−1/2
.
Partition the set {±1}n into 2n−|S| sets A(k)
S
with the property that the elements
of each part A(k)
S
agree on all coordinates not in S. As we have seen in the proof of
Lemma 3.21:

α∈A(k)
S
σ(α)fα(θ) = ±2|S|

T ∈It,T ⊇S
θT .
Hence

α∈A(k)
S
d

fα(θ)

≥d
 
α∈A(k)
S
d

fα(θ)

= d

2|S|

T ∈It,T ⊇S
θT

>

2−e−1/2
Δn−|S|.
(3.22)
We now estimate a part of G(θ). The following sequence of transformations is
long and difﬁcult to ﬁnd, but the individual steps are simple and quite standard.
The only unusual bound we use is 1 −cos(x) ≥
2
π2 x2 for x ∈[−π,π], which we

86
3
Non-linear Combinations of LFSRs
Fig. 3.4 1 −cos(x) ≥2
π2 x2
illustrate in Fig. 3.4. It is easily proved by determining the extrema of 1 −cos(x) −
2
π2 x2.

α∈A(k)
S

1 + Deifα(θ)
1 + D

2
=

α∈A(k)
S
(1 + D cosfα(θ))2 + (sinfα(θ))2
(1 + D)2

=

α∈A(k)
S

1 −2D(1 −cosfα(θ))
(1 + D)2

=

α∈A(k)
S

1 −2D(1 −cosd(fα(θ)))
(1 + D)2

≤

α∈A(k)
S

1 −4Dd(fα(θ))2
π2(1 + D)2

since 1 −cos(x) ≥2
π2 x2 for x ∈[−π,π]
≤

α∈A(k)
S
exp

−4Dd(fα(θ))2
π2(1 + D)2

since 1 −x ≤e−x (Taylor series)
= exp

−
4D
π2(1 + D)2

α∈A(k)
S
d

fα(θ)
2

≤exp

−
4D
π2(1 + D)2
1
|A(k)
s |
 
α∈A(k)
S
d

fα(θ)
2
by Cauchy-Schwarz inequality

3.6
Correlation Immune Functions
87
< exp

−
4D
π2(1 + D)2 2−|S|
2 −e−1/2
Δn−|S|

by (3.22).
Since G(θ) is the product of 2n−|S| parts we get:
G(θ) < exp

−
4D
π2(1 + D)2 2n−2|S|
2 −e−1/2
Δn−|S|

for all θ /∈C + R.
Thus
#
C+R
G(θ)dθ < |C + R| exp

−
4D
π2(1 + D)2 2n−2|S|Δn−|S|

< (2π)Mt,n exp

−
4D
π2(1 + D)2 2n−2|S|
2 −e−1/2
Δn−|S|

since the domain of integration C + R lies in the hypercube [0,2π]Mt,n.
□
Lemma 3.22 bounds the integral outside the critical region C + R. We will later
choose the factor Δ such that this part of the integral will be hidden in the O-term
of Theorem 3.19.
Next we look at the integral inside the critical region.
Lemma 3.23 Let D =
λ
1−λ and Δ = 2−n/2+t+3λ−1/2nt+1/2M1/2
t,n , then
#
R
G(θ)dθ =

2π
λ(1 −λ)2n
Mt,n/2
1 + O

2n D(1 −D)Δ3
3(1 + D)3

.
(3.23)
Proof First we write G(θ) = eg(θ). This gives
g(θ) = −i2nλθ∅+

α∈{±1}n
log
1 + Deifα(θ)
1 + D

.
Using the Taylor series
h(x) = log
1 + Deix
1 + D

= i
D
1 + D x −1
2
D
(1 + D)2 x2 + R(x)
where
R(x) =
# x
0
1
2h′′′(t)(x −t)2 dt
(3.24)
we get
g(θ) = −i2nλθ∅+

α∈{±1}n

i
D
1 + D fα(θ) −1
2
D
(1 + D)2 fα(θ)2 + R

fα(θ)

.
(3.25)

88
3
Non-linear Combinations of LFSRs
Now remember that fα(θ) = 
S∈It αSθS and hence the coefﬁcient of θS (S ̸= ∅) is

α∈{±1}n αS = 0.
The coefﬁcient of θ∅is
−i2nλ + i
D
1 −D

α∈{±1}n
α∅= −i2nλ + i
D
1 −D 2n = 0
for D =
λ
1−λ, which explains the choice of D.
Hence
g(θ) =

α∈{±1}n

−1
2
D
(1 + D)2 fα(θ)2 + R

fα(θ)

.
(3.26)
The next step used to bound the error term R(x) is Eq. (3.24). We have
h′′′(x) = iD2(eix)2 −iDeix
(D + eix)3
.
For x = o(1) we get |h′′′(x)| = D(1−D)
(1+D)3 + o(1). Thus
R(x)
 ≤
D(1 −D)
(1 + D)3 + o(1)
# x
0
(x −t)2 dt = D(1 −D)x3
3(1 + D)3
+ o

x3
.
Note that R is symmetric about the origin and the reﬂection θ →−θ maps G(θ)
to its complex conjugate. Hence

R G(θ)dθ is real and the integral is equal to the
integral over the real part of the integrand.
Let r(θ) = 
α∈{±1}n R(fα(θ)) then
#
R
G(θ)dθ =
#
R
ℜ

G(θ)

dθ
=
#
R
ℜ

eg(θ)
dθ
=
#
R
ℜ

er(θ)
exp

−1
2
D
(1 + D)2

α∈{±1}n
fα(θ)2

= ℜ

er(θ0)#
R
exp

−1
2
D
(1 + D)2

α∈{±1}n
fα(θ)2

for some θ0 by the intermediate value theorem.
Since
r(θ0) =

α∈{±1}n
R(θ0) ≤2n D(1 −D)Δ3
3(1 + D)3
+ o

2nΔ3
= o(1)

3.6
Correlation Immune Functions
89
for Δ = o(2−n) we get by Taylor’s expansion
ℜ

er(θ0)
= 1 + O

2n D(1 −D)Δ3
3(1 + D)3

.
Now check that

α∈{±1}n
fα(θ)2 = 2n 
S∈It
θ2
S.
(The square terms in f 2
α always have coefﬁcient 1. The mixed terms have coefﬁ-
cients ±1 and they vanish since the number of odd subsets of a non-empty set is
always equal to the number of even subsets.)
Thus we must calculate
#
R
exp

−1
2
D
(1 + D)2

α∈{±1}n
fα(θ)2

=
#
R
exp
1
2λ(1 −λ)22 
S∈It
θ2
S

dθ
=

S∈It
# Δ(2n)−|S|
−Δ(2n)−|S| exp
1
2λ(1 −λ)2nθ2
S

.
Using the bound
 xσ
−xσ e−t22σ 2 dt = σ
√
2π(1 + e−x2/2) for x →∞with σ =
√λ(1 −λ)2n and x = Δ(2n)−|S|σ −1 >
(
32nMt,n we get
#
R
G(θ) =

2π
λ(1 −λ)2n
Mt,n/2
1 + O

2n D(1 −D)Δ3
3(1 + D)3

+ O

M−16nMt,n
t,n

.
The lemma follows by noting that the ﬁrst O-term dominates the second.
□
Proof of Theorem 3.19 By Lemma 3.20 and Eq. (3.15) we have
N(n,t,λ) =
(1 + D)2n
(2π)Mt,nD2nλ
# 2π
0
···
# 2π
0
e−i2nλq

α∈{±1}n
1 + Deifα(θ)
1 + D
dθ
=
(1 + D)2n
(2π)Mt,nD2nλ
#
C+R
G(θ)dθ +
#
C+R
G(θ)dθ

.
The second integral is bounded by Lemma 3.22 and for D =
λ
1−λ and Δ =
2−n/2+t+3λ−1/2nt+1/2M1/2
t,n the bound is lower than the O-term in Theorem 3.19.
The ﬁrst integral is 2Qt,n 
R G(θ)dθ and by Lemma 3.23 we know the value of
the integral. This yields the value for N(n,t,λ) given by Theorem 3.19.
□

Chapter 4
Correlation Attacks
4.1 CJS-Attacks
Under the name CJS-attacks we collect a class of correlation attacks that extend the
approach described by V. Chepyzhov, T. Johansson and B. Smeets in [52].
4.1.1 The Basic Version
In this subsection we describe the basic version of the attack which can be found
in [52].
Represent the LFSR as a linear code and let
G =
⎛
⎜⎝
1
0 cn,0
···
cN−1,0
...
...
...
0
1 cn,n−1
···
cN−1,n−1
⎞
⎟⎠
(4.1)
be the systematic generator matrix associated with the LFSR (see Sect. 2.2.4).
Since the LFSR register produces a pseudo-random sequence the column vectors
(ck,0,...,ck,n−1)t, n ≤k < N, are also pseudo-random. For our analysis of the fast
correlation attacks described in this section, we model the columns by independent
uniformly random vectors.
Choose a constant B. We make the following deﬁnition.
Deﬁnition 4.1 A relation of weight 2 is a pair (r,r′), n ≤r < r < N′, with cr,j =
cr′,j for all j ∈{B,...,n −1}.
By Ω we denote the set of all relations (of weight 2). The expected size of Ω is
given by Theorem 4.1.
A. Klein, Stream Ciphers, DOI 10.1007/978-1-4471-5079-4_4,
© Springer-Verlag London 2013
91

92
4
Correlation Attacks
Theorem 4.1
E

|Ω|

=
N −n
2

2−n+B ≈N22−n+B−1.
Proof The probability that a pair (r,r′) is a relation is 2−n−B. Since there exist
N−n
2

pairs (r,r′) with n ≤r < r < N′ and the expected value is additive, we have
E(|Ω|) =
N−n
2

2−n+B.
□
The fast correlation attack is based on the following observation:
Theorem 4.2 Let x0,...,xN−1 be the true LFSR sequence and let z0,...,zN−1 be
the sequence observed by the attacker. Assume that the sequences (xi) and (zi) are
correlated by p = P(xi = zi) > 1/2 and let (r,r′) be a relation, then:
P

zr + zr′ =
B−1

j=0
(cr,j + cr′,j)xj

= p2 + (1 −p)2
(4.2)
and for (ˆx0,..., ˆxB−1) ̸= (x0,...,xB)
P

zr + zr′ =
B−1

j=0
(cr,j + cr′,j)ˆxj

= 1
2.
(4.3)
Proof By deﬁnition
xr =
n−1

j=0
cr,jxj
and
xr′ =
n−1

j=0
cr′,jxj
and hence
xr + xr =
n−1

j=0
(cr,j + cr′,j)xj
=
B−1

j=0
(cr,j + cr′,j)xj
since cr,j = cr′,j for B ≤j < n.

4.1
CJS-Attacks
93
Now
P

zr + zr′ =
B−1

j=0
(cr,j + cr′,j)xj

= P(zr + zr′ = xr + xr′)
= P(zr = xr,zr′ = xr′) + P(zr ̸= xr,zr′ ̸= xr′)
= p2 + (1 −p)2.
On the other hand, δc = (cr,j +cr′,j)j=0,...,B−1 is uniformly random in ZB
2 , since
cr and cr′ are independent random vectors. (Remember our model for the generator
matrix!)
Hence, for ˆx ̸= x ∈ZB
2 the scalar product δ = δc · (ˆx −x) is a random variable
independent from ˆx and x with P(δ = 1) = P(δ = 0) = 1/2.
Therefore
P

zr + zr′ =
B−1

j=0
(cr,j + cr′,j)ˆxj

= P

zr + zr′ = δ =
B−1

j=0
(cr,j + cr′,j)ˆxj

= 1
2.
□
Theorem 4.2 shows that the relation distinguishes between the correct vector
(x0,...,xB−1) and the incorrect guess ˆx = (ˆx0,..., ˆxB−1). This leads us to the at-
tack described by Algorithm 4.1.
Algorithm 4.1 Simple fast correlation attack (CJS)
1: for ˆx ∈ZB
2 do
2:
Count the number kˆx of relations with zr + zr′ = B−1
j=0 (cr,j + cr′,j)ˆxj.
3: end for
4: Return ˆx with kˆx = max as the most likely guess for x = (x0,...,xB−1).
For the moment we do not say how we count the number kˆx of satisﬁed relations
(see Sect. 4.1.6).
To ﬁnish the analysis of the simple fast correlation attack we must determine for
which parameters N, n and B the success probability of Algorithm 4.1 is reasonable
large. Let us recall Shannon’s fundamental theorem of coding.
Theorem 4.3 (Shannon’s coding theorem)
Denote by C(p) = p logp + (1 −
p)log(1 −p) the capacity of a binary symmetric channel with error probability p.
Let 0 < R < C and let log(Mn)/n < R for all n ∈N. Then there exists a sequence
of codes Cn such that:

94
4
Correlation Attacks
• Cn contains Mn code words of length n; and
• the probability Perr(Cn) of a decoding error satisﬁes Perr(Cn) ≤4e−NA for some
constant A > 0.
Proof Shannon proves that a random code Cn almost surely satisﬁes the bound of
Theorem 4.3. See, for example, [170].
□
Theorem 4.2 states that C′ = {(B−1
j=0 (cr,j +cr′,j)ˆxj)(r,r′)∈Ω | ˆx ∈ZB
2 } and (zr +
zr′)(r,r′)∈Ω behaves like a random code and codewords are transmitted over a binary
symmetric channel with error probability q = 1 −p2, where p2 = p2 + (1 −p)2.
Hence by Shannon’s coding theorem we expect that the maximum likeli-
hood decoding algorithm (Algorithm 4.1) will work if |Ω|C(p2) > B. Substi-
tuting the expected size of Ω from Theorem 4.1 we get that the decoding Al-
gorithm 4.1 succeeds with high probability if the attacker can observe at least
1/2
√
B ln2(p −1/2)−22(n−B)/2 bits (see [52] Theorem 2).
4.1.2 Using Relations of Different Size
The ﬁrst possible extension of the CJS-attack is to use relations of size larger than 2.
This extension increases the number of available relations at the price of a weaker
correlation. The effect is that the attack, and especially the pre-processing, becomes
slower, but the attacker needs less data.
This extension was described in the original paper [52].
The principle is very simple. We deﬁne a relation of weight w as a w-tuple
(r0,...,rw−1) with n ≤r0 < ··· < rw−1 < N and xr0,j + ··· + xrw−1,j = 0 for all
j ∈{B,...,n −1}. By analogy with Theorem 4.1 we ﬁnd that there exist about
N−n
w

2−n+B relations of weight w. So the number of available relations increases
quickly if we allow a higher weight. The price is that a relation of weight w is
satisﬁed only with probability
pw =
⌊w/2⌋

j=0
w
2j

(1 −p)2jpw−2j.
(4.4)
pw quickly approaches 1/2 as w increases, so the number of necessary relations for
a successful attack increases quickly.
In [171] we ﬁnd the following theorem.
Theorem 4.4 Let p = 1/2 + δ = P(zi = xi). Then the success probability of the
simple fast correlation attack (Algorithm 4.1) with weight w + 1 relations is better
than the success probability of the same algorithm with weight w relations if and
only if
2δ
)
N
w + 1 > 1.

4.1
CJS-Attacks
95
Proof See [171] Theorem 4.10.
□
In practice we will not use relations only of weight exactly w, but instead we will
use relations of weight up to w. This limits the application of Theorem 4.4. In fact
the success probability always increases if we increase the admissible weight, since
extra information can never hurt.
In the literature there seems to be some confusion over how to use relations of
different weight. For a maximum likelihood algorithm we cannot simply count the
number of satisﬁed relations, but we must use weighted sums. Here we follow the
presentation of [83].
Let ni be the number of relations of weight i and ki be the number of satisﬁed
relations of weight i. Under the assumption that our guess is correct, the probability
of observing this pattern of satisﬁed relations is
pguess =
w

i=1
pki
i (1 −pi)ni−ki,
(4.5)
where
pi =
⌊i/2⌋

j=0
 i
2j

p2j(1 −p)i−2j
is the probability that a relation of weight i is satisﬁed.
By the maximum likelihood principle we have to ﬁnd the guess for which pguess
becomes maximal. Multiplying small ﬂoating point numbers is unfavorable, so we
take the logarithm.
logpguess =
w

i=1
ki log(pi) + (ni −ki)log(1 −pi).
Subtracting the constant w
i=1 ni log(1 −pi) we see that we must maximize
w

i=1
ki

log(pi) −log(1 −pi)

.
Finally we divide by log(pw) −log(1 −pw), which leaves us the goal to maximize
w

i=1
kiwi,
with
wi = log(pi) −log(1 −pi)
log(pw) −log(1 −pw).
Some textbooks (such as [78]) recommend replacing the real weights by integer
approximations. This advice comes from the “good old times” when ﬂoating point

96
4
Correlation Attacks
units were very slow. On modern hardware ﬂoating point units are highly optimized
and are often faster than integer units, so we can work directly with ﬂoating point
weights.
4.1.3 How to Search Relations
At this point we still need to explain how to ﬁnd the relations used by the CJS-
attack. For weight 2 relations the solution is simple. We just sort the columns in the
generator matrix to ﬁnd all pairs (r,r′) with cr,j = cr′,j for B ≤j < n. For larger
weights the solution is less trivial.
The following theorem is in its main part due to [53] with some corrections and
improvements due to [83].
Theorem 4.5 It is possible to compute all relations of weight up to w in O(N⌈w/2⌉)
time and O(N⌈(w−1)/4⌉) memory.
Proof First we deal with the case where w is even.
We generate a table which contains for all k-tuples with k ≤⌈w/4⌉the values
ci1 + ··· + cik and sort these partial relations by their characteristic (hash based
sorting). Building the table requires O(N⌈w/4⌉) time and memory.
Next we choose r such that 2r = Θ(N⌊w/2⌋−⌈w/4⌉). Let π denote the projection
to the ﬁrst r coordinates.
For each a ∈Fr
2 enumerate the partial relations (i1,...,is) of size ⌈w/4⌉+ 1 ≤
s ≤w/2 with the property π(ci1 + ··· + cis) = a.
For the enumeration we run through all partial relations (i1,...,i⌈s/2⌉) of size
⌈s/2⌉and search the pre-computed table for relations (i⌈s/2⌉+1,...,is) such that
π(ci1 + ··· + ci⌊s/2⌋) = a + π(ci⌊s/2⌋+1 + ··· + cis). Thus for ﬁxed a the time needed
to enumerate the partial relations (i1,...,is) of size ⌈w/4⌉+ 1 ≤s ≤w/2 with
π(ci1 + ··· + cis) = a is the number Na of such relations plus O(N⌈w/4⌉) time
steps for the algorithm describe above. The memory required is Na = Θ(Ns/2r) =
Θ(N⌈w/4⌉).
Once we have all partial relations with π(ci1 + ··· + cis) = a, we can ﬁnd all
relations by searching for solutions of ci1 + ··· + ci⌊w/2⌋= cj1 + ··· + cj⌈w/2⌉.
Altogether the algorithm needs
2rO

N⌊w/4⌋
+

a
Na = O

N⌊w/2⌋−⌈w/4⌉
O

N⌊w/4⌋
+ O

Nw/2
= O

Nw/2
time steps.
Now we explain how to deal with the case of odd w. In principle we would like
to run the same algorithm as for w −1. However, if we change nothing we need
Θ(N⌈w/2⌉/2r) = Θ(N⌈w/4⌉+1) space to store all partial relations of size ⌈w/2⌉
with π(ci1 + ··· + ci⌈w/2⌉) = a.

4.1
CJS-Attacks
97
But since w is odd, we will use a partial relation (j1,...,j⌈w/2⌉) of size ⌈w/2⌉
only once, when we must search all partial relations (i1,...,i⌊w/2⌋) with ci1 + ··· +
ci⌊w/2⌋= cj1 + ··· + cj⌈w/2⌉. Each solution of this equation gives us a relation of
size w.
Since we use the partial relations of size ⌈w/2⌉only once, we do not need to
store them. Thus we store only the partial relations of size up to ⌊w/2⌋and to store
them we need a memory of only Θ(N⌊w/2⌋/2r) = Θ(N⌈w/4⌉).
□
The proof of Theorem 4.5 gives no hint as to which data structure we should use.
To obtain the bounds of the theorem, we need a data structure which allows us to
store and search a partial relation with given characteristic in O(1) time. In addition
we should not use more than O(N⌈(w−1)/4⌉) memory to store the O(N⌈(w−1)/4⌉)
partial relations with π(ci1 + ··· + cis) = a.
This calls for a hash table. We want to avoid costly memory allocating operations
during the pre-processing, so we decide to choose a closed hash. Since the charac-
teristics of the partial relations are almost uniformly distributed we can simply use
the projection of the characteristic to the last h coordinates as the hash function.
We choose the size 2h of the hash table to be about twice the expected number of
partial relations, so will have only few collisions. To deal with collisions we choose
a quadratic probing strategy, i.e. if we want to store a partial relation at an already
occupied position n we next probe the positions n + 12, n + 22, n + 33, ... until
we ﬁnd an empty place. (Tests of alternative collision avoiding strategies reveal that
linear probing gives poor results, while more complicated probing strategies are not
worth the additional effort.)
As a small implementation trick we note that we use a table of size 2h +100. The
100 extra places at the end guarantees that the quadratic probing will almost surely
never go beyond the end of the table. This saves us a modulo operation.
Finally we have to describe how we clear the hash table when the loop advances
to the next value of a. The trick is to store the value a of the ﬁrst r coordinates of the
characteristic inside the hash. This has the advantage that the algorithm can detect
an empty place with the old value of a. The idea of saving space by not storing the
known value a results in a serious slow down, since then one must clear the hash
table in every iteration of the loop, i.e. we would have 2r extra operations per loop.
Having explained in detail the advantages of hashing over sorting we must men-
tion that sorting is sometimes more efﬁcient than hashing. The reason is that sorting
can make better use of the cache in modern processors, while hashing produces
many cache misses. See Chap. 6 “The birthday paradox: Sorting or not?” of [138]
for an extensive discussion. In general, one can expect sorting to be effective if the
data is read only once and hashing to be effective if the data is read several times.
For the problem of ﬁnding relations this mean that hashing is best if the weight
w of the relations is odd, but the case of even w can beneﬁt from a good sorting
algorithm.
The most interesting cases of Theorem 4.5 are those with memory consumption
O(N), i.e. w ≤5. For larger w the memory consumption grows rapidly, which
often forces time-memory trade-offs in the pre-processing (see [83]). For w ≤5 the
following tricks speed up the pre-processing by a constant factor.

98
4
Correlation Attacks
• If we want to get relations c1 + c2 + c3 + c4 = 0 of weight 4 we use the following
special algorithm.
Without loss of generality we can assume that c1 and c2 have the same ﬁrst
component. Hence c3 and c4 have the same ﬁrst component too.
Therefore it is enough to enumerate all partial relations of weight 2 where the
ﬁrst component of ci1 + ci2 is 0.
So we must generate just ≈
N
2

/2 partial relations instead of the
N
2

partial
relations required in the basic algorithm.
• For w = 3 we can improve the trick a bit. There are two cases. In the ﬁrst case
the ﬁrst component of c1 and c2 is 1 while the ﬁrst component of c3 is 0. To deal
with this case we must enumerate all pairs (c1,c2) with ﬁrst component 1. There
are ≈
N/2
2

such pairs.
The second case is that all three vectors c1, c2 and c3 have ﬁrst component 0.
In this case we can apply the same trick for the second component.
Altogether we enumerate only
N/2
2

+
N/4
2

+ ··· ≈N2/6 pairs.
• For w = 5 we do the following: Look at the projection to the ﬁrst three coordi-
nates. The equation c1 + c2 + c3 + c4 + c5 = 0, ci ∈F3
2 has only the following
solutions:
Either we have two equal vectors (without loss of generality we can assume
c4 = c5). We can cover this case by enumerating all
N
3

/23 partial relations for
which c1 + c2 + c3 = 0.
The second case is that no two vectors are equal. But this means that ci + cj ̸=
ck for i ̸= j, j ̸= k, i ̸= k.
The only possibility (up to permutation of indices) is c1 = 0, c5 = c2 +c3 +c4.
This lead us to four different solutions (up to permutation) either c2 = (100),
c3 = (010), c4 = (001) or the weight of c2 and c3 is even. For each of these four
subcases there are only N3
83 partial relations of weight 3 that fall into the subcase.
Hence we must enumerate only N3( 1
3!·8 +4 1
83 ) of the
N
3

possible partial rela-
tions. This is a speed-up factor of almost 8 in comparison to the basic algorithm.
4.1.4 Extended Relation Classes
This subsection describes a substantial improvement of the original CJS-attack. The
idea is to allow extended relation sets.
Deﬁnition 4.2 We call a w-tuple (r0,...,rw−1) with n ≤r0 < r2 < ··· < rw−1 < N
an extended relation of weight w. The vector
χ =
w−1

j=0
crj ,B,...,
w−1

j=0
crj ,n−1

∈Fn−B
2
is called the characteristic of the extended relation (r0,...,rw−1).

4.1
CJS-Attacks
99
Let I ⊂B,...,n −1. By ΩI we denote the set of all extended relations (up to
weight w) with characteristic χ, where χ is the characteristic vector of I.
The relations of the original CJS-attack are the extended relations with charac-
teristic 0. In the new attack we also allow other characteristics.
The idea of extended relation classes is due to P. Lu and L. Huang [171]. In [171]
the authors use only the extended relation sets ΩI with |I| = 1. They appear in full
generality for the ﬁrst time in this book.
For simplicity we assume in the following that all extended relations have
weight 2. Let ΩI be an extended relation set and let (ˆx0,..., ˆxB−1) be a guess
for the ﬁrst B bits. We say an extended relation (r,r′) ∈Ωi is satisﬁed if and only if
zr + zr′ =
B

j=1
(cr,j + cr,j)ˆxi.
We denote the number of satisﬁed relations by k and the size of ΩI by n.
Similarly to Theorem 4.2 we have for an extended relation (r,r′) ∈ΩI the equa-
tion
P

zr + zr′ =
B−1

j=0
(cr,j + cr′,j)xj + ξI

= p2 = p2 + (1 −p)2
(4.6)
where ξI = n−1
j=B(cr,j + cr′,j)xj depends only on I. Thus the probability that kI
of the nI relations are satisﬁed under the condition xi = ˆxi (0 ≤i < B) and ξI = 0
is P0 = pkI
2 pnI −kI
2
. Similarly the probability that kI of the nI relations are satisﬁed
under the condition xi = ˆxi (0 ≤i < B) and ξI = 1 is P1 = pnI −kI
2
pkI
2 .
If all possible initial values have equal probability then ξI is a random variable
with p(ξI = 0) = p(ξI = 1) = 1/2.
So in a likelihood test we have to compute
LI = log
1
2P0 + 1
2P1

= log

pkI
2 (1 −p2)nI −kI + pnI −kI
2
(1 −p2)kI 
−1. (4.7)
The expression in the form of Eq. (4.7) is unwieldy, but for large nI the expected
difference between nI and kI will be large, thus max{P0,P1} ≫min{P0,P1} and
we can safely replace Eq. (4.7) by
Li ≈logmax{P0,P1} −1 = ˆk logp2 + (nI −ˆk)log(1 −p2) −1
(4.8)
where ˆk = max{kI,ni −ki}.
If nI is small we transform Eq. (4.7) as follows:
Assume without loss of generality that kI > nI −kI . Then
LI = log

pkI
2 (1 −p2)nI −kI + pnI −kI
2
(1 −p2)kI 
−1
= log

pkI
2 (1 −p2)nI −kI 
+ log
*
1 +

pi/(1 −pi)
nI −2ki+
−1.

100
4
Correlation Attacks
The ﬁrst term is the approximation we used in Eq. (4.8). The second term can be
interpreted as a correction value which, for large nI, will be small and hence can be
considered negligible. If nI is small there are only a few possible values of nI −2kI
and we can tabulate the correction values.
It is possible to use more than one extended relation set at the same time. Under
the assumption that everything is independent we just add the likelihood values, i.e.
we compute
L = L0 +
e

k=1
LIk.
This assumption is valid if the extended relation sets do not contain overlapping
relations and if the characteristic vectors of the index sets Ik are linearly indepen-
dent. As already stated in the description of the simple CJS-attack we will ignore
any dependency that may occur in our relation sets. So we should expect that the
real attack needs more data than estimates based on the model of independent rela-
tions suggest. Experiments show that the effects that come from the dependencies
are negligible.
One problem with extended relations is that one must compute many more like-
lihood values than in the original CJS-attack. Even if we take into account that each
extended relation set will be much smaller than the relation set used in the original
CJS-attack, this is an expensive operation. A way to deal with this problem is to use
sequential tests (see Sect. 15.3). The idea is the following:
The likelihood values of a wrong guess are smaller on average than the like-
lihood values for the right guess. So instead of directly computing the full value
L = L0 + e
k=1 LIk for each guess and selecting the largest value as the answer for
the decoding algorithm, we compute only a part of the sum: L′ = L0 + e′
k=1 LIk,
where e′ < e. If this value is small we can guess that the full likelihood value L is
also small and stop the computation.
Turning this heuristic argument into a precise statistic with proved error bound
needs some work. Section 15.3 gives an introduction to sequential tests. In the
present case we can use a slight modiﬁcation of Wald’s sequential ratio test in which
we formally set the error probability of the ﬁrst kind to 0.
The main advantage of extended relation sets is that we can choose I so that ΩI
contains as many relations as possible. Therefore extended relation sets are of most
use if we can use low weight relations and small relation sets.
For example consider an LFSR of length 50 with feedback polynomial
f (z) = z50 + z49 + z48 + z47 + z46 + z39 + z34 + z32 + z30 + z29 + z27 + z24
+ z23 + z19 + z18 + z17 + z15 + z14 + z13 + z12 + z8 + z7 + z6 + z5 + z4
+ z3 + z2 + z + 1
and the moderately strong correlation of 1 −p = 0.65. We want to use relations
of weight 2. As parameter B we choose B = 20. The standard relation set which

4.1
CJS-Attacks
101
could be used by the simple CJS-attack has only 8 relations, but one can ﬁnd 1500
extended relations with sizes between 35 and 51.
It remains to explain how we ﬁnd large sets of extended relations.
For each vector x ∈Fl−B
2
let Nx be the number of columns in the generator
matrix for which the projection to the last l −B coordinates is x. Then the number
of k-tuples (i1,...,ik) for which sum of the k columns ci1,...,cik of the generator
matrix has only zeros in the last l −B coordinates is
N(k)
x
=

x1+···+xk=x
Nx1 ···Nxk.
(4.9)
Equation (4.9) describes a convolution. So we can compute all N(k)
x
efﬁciently by
computing the Fourier transform ˆN = θ(N), raising each element of ˆN to the k
power, and then applying the inverse Fourier transform.
For l −B < 35 this is no problem. The number N(k)
x
is almost the number of
extended relations in Ωx of size k. We just count every relation k! times and we
also count pseudo-relations such as (x2,x2). The correction is very simple. Let R(k)
x
be the number of relations of weight k in Ωx. In the extreme case k = 0 we have
R(0)
0
= 1 and R(0)
x
= 0 for x ̸= 0.
Then
Nk
x =
⌊k/2⌋

j=0
Rk−2j
x
N + j −1
N −1

.
(
N+j−1
n+1

is the number of ways of choosing, with repetition, j columns from the
N columns in the generator matrix. Each relation of weight k −2j can be extended
to a pseudo-relation of weight k by adding j pairs of equal columns.)
For small l −B this allows us to determine the exact size of all extended relation
sets and we can choose the largest one for the attack. For large l −B we switch
to a heuristic method. We can compute the exact size of the extended relation sets
for a B′ > B. Then we choose a large extended relation set with respect to B′. This
splits into 2B′−B extended relation sets with respect to B which are most likely
large.
4.1.5 Twice Step Decoding
Twice step decoding is a technique to improve the success rate (or the speed) of fast
correlation attacks. In the basic form of the fast correlation attack (Algorithm 4.1)
we loop over all ˆx ∈ZB
2 and search for the value ˆx with the maximal likelihood
value.
In twice step decoding we compute the likelihood values for all ˆx ∈ZB
2 . Instead
of stopping at this point, we run Algorithm 4.2.

102
4
Correlation Attacks
Algorithm 4.2 Twice step decoding
1: Compute likelihood values for all ˆx ∈ZB
2 and sort the vectors ˆx by the likeli-
hood values
2: loop
3:
Get the ˆx = (ˆx0,..., ˆxB−1) with the highest likelihood value.
4:
Assume that (x0,...,xB−1) = ˆx.
5:
Under this assumption decode a linear [N,n −B]. Denote by (ˆxB,..., ˆxn−1)
the output of the decoding algorithm.
6:
Generate the LFSR sequence ˆx0, ..., ˆxN−1.
7:
if |{i | ˆxi = ˆzi}| > T then
8:
return ˆx as the most likely guess for the LFRS sequence x.
9:
else
10:
Delete (ˆx0,..., ˆxB−1) from the list of possible B-tuples.
11:
end if
12: end loop
The idea of Algorithm 4.2 is to interpret the likelihood values computed by the
simple correlation attack as a hint of the order in which we should test the vectors ˆx
(line 1).
Next we take an ˆx with highest a posteriori probability (line 3) and try to recon-
struct the LFSR sequence under the assumption that ˆx is the true x. This leaves us
with the problem of decoding an [N,n−B]-code. We will use the simple fast corre-
lation attack (Algorithm 4.1) for this purpose. So we must choose a new parameter
B′ and compute the corresponding relations. Since now we assume the ﬁrst B val-
ues of the LFSR as known, the number of relations is much larger, so in the second
decoding step even a small B′ gives us a high success probability. This means that
each of the internal decoding steps is very fast.
After line 5 of Algorithm 4.2 we have reconstructed a possible initial state of
the LFSR. We know that this internal state is correct with high probability if the
initial assumption x0 = ˆx0, ..., xB−1 = ˆxB−1 was correct. To test if this is the case
we generate the LFSR sequence ˆx0, ..., ˆxN−1 (line 6). If all our assumptions were
correct we have xi = ˆxi and hence P(ˆxi = zi) = p > 1/2. If our assumptions are
wrong we get P(ˆxi = zi) = 1/2.
To distinguish these cases we use a classical one-sided test, i.e. we choose the
smallest T such that P(Bin(N,p) ≥T ) > 1 −ϵ for some small ϵ and accept the
hypothesis H0 : ˆxi = xi if and only if ˆxi = zi for at least T indices 0 ≤i < N.
Otherwise we reject H0. This test is done in line 7 of Algorithm 4.2.
If we reject the guess ˆx we go back to the beginning of the loop and test the ˆx
with the second highest a posteriori probability and so on, until we ﬁnd the right ˆx.
One way to interpret twice step decoding is that we run the simple Algorithm 4.1
with the large parameter B + B′ and test the ˆx in the order that is determined by the
ﬁrst step.
So in twice step decoding we must choose B + B′ so that the simple attack with
that search window succeeds with a probability of almost 1. If B is too small, the

4.1
CJS-Attacks
103
ﬁrst step of the twice step decoding will produce a random order of the candidates ˆx.
In this case twice step decoding will be not faster than the simple algorithm and
we will even lose speed because we must test the ˆx in some artiﬁcial order. If,
on the other hand, B is too large, the ﬁrst step will most likely assign the highest
a posteriori probability to ˆx = x. In this case Algorithm 4.2 will run the loop for the
second step only once (or at least only a few times). The effect is that Algorithm 4.2
spends its whole time in line 1, i.e. in the ﬁrst step. Hence it is no better than the
simple algorithm. We must choose B between these two extreme cases to beneﬁt
from twice step decoding.
As mentioned above, one normally uses the simple Algorithm 4.1 as a decod-
ing algorithm for the second step in twice step decoding (line 5 of Algorithm 4.2).
However, it is possible to iterate twice step decoding. Ultimately, each step of a
multi-step decoding algorithm adds only one bit. We will discuss this variant when
we come to the application of convolutional decoding algorithms in cryptography
(see Sect. 4.2.3.2).
4.1.6 Evaluation of the Relations
In the previous sections we just stated that the attacker has to compute for all 2B pos-
sible values of (x0,...,xB−1) the quality w
j=1 wjej where ej denotes the number
of satisﬁed relations of weight j. In this section we want to answer the question how
the attacker should do this. There are basically two methods, both having their pros
and cons.
4.1.6.1 Simple Counting
For the ﬁrst method we use a bit ﬁeld b to indicate, for each relation, whether it is
satisﬁed or not. To get the number of satisﬁed relations we must perform a sideway
addition (see Sect. 12.1.2). In fact the sideway addition is the most time-consuming
part of the whole algorithm, so it should be carefully optimized.
We use a gray code to loop over the 2B possible values of (x0,...,xB−1), i.e. in
every step we ﬂip only one xi. In the pre-processing step we can compute the bit
ﬁeld bi, in which all relations that include xi are marked by a 1. The update of the
bit ﬁeld b of satisﬁed relations is then performed by b ←b ⊕bi, i.e. we need only
a simple XOR of two bit ﬁelds.
This algorithm for the evaluation of the relations needs O(2BR) steps where R
denotes the number of relations.
The greatest pro for the simple counting algorithm is that it opens the door for the
use of sequential statistics. To understand why, we look at the simple case in which
all relations are of the same quality. Let p > 1/2 be the probability that a relation is
satisﬁed for the correct guess (x0,...,xB−1). Let Rk be the random variable that is
1 if the kth relation is satisﬁed and 0 if it is unsatisﬁed.

104
4
Correlation Attacks
Our goal is to distinguish the hypothesis H0 : P(Rk = 1) = p (i.e. the guess
(x0,...,xB−1) is correct) from the alternative H1 : P(Rk = 1) = 1/2 (i.e. the guess
(x0,...,xB−1) is wrong).
By the Neyman-Pearson lemma the optimal test to distinguish H0 and H1 is to
compute
LR =
R−1

k=0
P0(Rk)
P1(Rk)
and reject H0 in favor of Hi if LR lies below a critical value cα. (The value cα
depends on the chosen error probability α.)
One can show (see Lemma 15.2) that under H0, LR →∞with probability 1 and
that under H1, LR →0 with probability 1. Now suppose we have a large number
of relations. It would be a waste of time to successively compute L0,...,LR−1
to see that LR−1 is small. We should be able to distinguish a sequence that tends
to 0 from a sequence that tends to ∞by looking only at the ﬁrst few elements.
Sequential statistics (see Sect. 15.3) formalizes this intuition. Especially when using
extended relation sets, sequential tests should be used to skip unnecessary relation
sets. Sequential statistics can easily speed up the simple counting by a factor of 2 or
even higher.
4.1.6.2 Fourier Transformation
The second way to evaluate the relations uses a Fourier transform.
For c = (c0,...,cB−1) ∈ZB
2 denote by nc the total quality of relations with char-
acteristic c. By kc we denote the total quality of relations with characteristic c, which
satisfy in addition the equation xr1 +···+xrw = 0. Note that the values nc can be de-
termined in the pre-processing phase, while the values kc must be calculated during
the attack.
Now consider the guess x = (x0,...,xB−1) for the ﬁrst B bits. The total quality
of all relations satisﬁed by this guess is
Q(x) =

x·c≡0 mod 2
kc +

x·c≡1 mod 2
(nc −kc).
Since the value of nc is known after the pre-processing, one can pre-compute

x·c≡1 mod 2 nc, thus the attacker need only compute
Q′(x) =

x·c≡0 mod 2
kc −

x·c≡1 mod 2
kc =

c∈ZB
2
(−1)x·ckc
for all x ∈ZB
2 .
Computing Q′(x) is a Fourier transformation with respect to the group Zn
2. As
for the ordinary Fourier transform there exists a very efﬁcient divide and conquer
algorithm (see Algorithm 4.3).

4.2
Attacks Based on Convolutional Codes
105
Algorithm 4.3 Fast Fourier transform over the group Z2
1: {Input: Q′(c) = kc for all c ∈ZB
2 .}
2: for i from 0 to B −1 do
3:
for x ∈ZB
2 with xi = 0 do
4:
(Q′(x),Q′(x + ei)) ←(Q′(x) + Q′(x + ei),Q′(x) −Q′(x + ei))
5:
end for
6: end for
7: {Output: Q′(c) = 
c(−1)x·ckc for all c ∈ZB
2 .}
Fig. 4.1 A simple
convolutional code
If R ≪2B the effort of calculating the kc is negligible. The Fourier transform
needs O(2BB) steps, i.e. the complexity is independent of the number of relations.
Therefore the Fourier transform is superior to simple counting if one uses many
relations of low quality.
A hybrid algorithm that combines the simple counting technique with the Fourier
transform is also possible (see [53]).
4.2 Attacks Based on Convolutional Codes
4.2.1 Introduction to Convolutional Codes
When mathematicians speak of error-correcting codes they usually mean block
codes, but there is another class of codes, the convolutional codes, that are to block
codes as stream ciphers are to block ciphers in cryptology. In this section we will
give a quick introduction to convolutional codes and show how to use them in cor-
relation attacks.
An (n,k) convolutional encoder maps k inﬁnite (binary) streams to n output
streams. Normally the encoding is done by a ﬁnite state machine like the one shown
in Fig. 4.1.
If we associate with a sequence (ai)i∈N its generating function ∞
i=0 aiDi, we
can deﬁne an (n,k) convolutional code as a k-dimensional subspace of F((D))n in
complete analogy to the deﬁnition of block codes. Now we can transfer the notions
of generator matrix and parity check matrix from block codes to convolution codes.
For example, the encoder shown in Fig. 4.1 is represented by the generator matrix
(1 + D2 1 + D + D2).

106
4
Correlation Attacks
Fig. 4.2 Three different encoders of the same code
One big difference between block codes and convolutional codes is that, since
a convolutional code has inﬁnitely many codewords, the encoder as a ﬁnite de-
scription of the code becomes more important. For example, decoding algorithms
for convolutional codes (e.g. Viterbi decoding and sequential decoding) depend on
the used encoder, while many block decoding algorithms (e.g. syndrome decoding,
BCH decoding, etc.) attempt to reconstruct the codeword and are independent of the
encoder. A consequence of this fact is that many measures for the performance of
convolutional codes are deﬁned as properties of an encoder and, in a slight abuse of
terminology, we use “the convolutional code has property X” as an abbreviation for
“the convolutional code has an encoder with property X”.
Figure 4.2 exhibits three different encoders of the same code.
The corresponding generator matrices are
G1 =
1 1 + D
,
G2 =

1 + D 1 + D2
,
G3 =

1
1 + D 1

(4.10)
As one can check, G2 and G2 are scalar multiples of G1, i.e. all matrices generate
the same code.
G2 is an example of a catastrophic encoder. It maps the inﬁnite sequence 111...
to the sequence 11010000..., so only three errors can ﬂip the output to 0000...,
which would be decoded as 000.... If a ﬁnite number of errors in the received
codeword results in an inﬁnite number of decoding errors, we speak of a catastrophic
error propagation. So G2 should not be used as an encoder.
G3 is an example that shows how feedback can be used to construct ﬁnite state
machines that belong to non-polynomial generator matrices.
For convolutional codes the minimal distance between two codewords is called
the free distance df ree of the code. The free distance is the most important quality
measure for convolutional codes.
Another important distance measure is the column distance and distance pro-
ﬁle, which is a property of the encoder. Let (c0,...) = C(e0,...) and (c′
0,...) =
C(e′
0,...) be the two codewords with e0 ̸= e′
0. The column distance di measures
the distance between the ﬁrst i outputs of the encoder, i.e. di is the minimal dis-
tance between (c0,...,ci−1) and (c′
0,...,c′
i−1) under the restriction that e0 ̸= e′
0.
For non-catastrophic encoders, di = df ree for large i.
The sequence d1,d2,... is called the distance proﬁle of the convolutional en-
coder.
At this point we stop our introduction and direct the reader to the further ref-
erences (see [78] and the references given there). The handbook of coding theory
also contains a good introduction [184], but it does not cover what are, for us, the
important decoding algorithms, which we will cover in the next section.

4.2
Attacks Based on Convolutional Codes
107
4.2.2 Decoding Convolutional Codes
The decoding algorithms for convolutional codes that are used in practice are mostly
the convolutional code equivalent of the simple decoding of a block code that com-
pares the received word with all code words. Since convolutional codes have in-
ﬁnitely many code words, even this simple algorithm must truncate the words, and
the decoding algorithms differ in their truncation strategies. Here we describe only
two of the most popular variants.
4.2.2.1 Viterbi Decoding
Viterbi proposed his decoding algorithm in 1967 [271], which was later proved to
be optimal in the maximum likelihood sense by Forney [96].
We describe the Viterbi algorithm in the more general setting of discrete state
Markov processes.
Deﬁnition 4.3 A discrete state Markov process consists of a ﬁnite state set S and
output alphabet A and a set of transitions T ⊂S × S × A × [0,1].
For each state s we have a ﬁnite number of transitions (s,sk,ak,pk), where:
• sk is the successor state;
• ak is the output associated to the transition; and
• pk is the probability of the transition.
The Viterbi algorithm solves the following problem: Given a discrete state
Markov process and sequences of observations A ﬁnd the state sequence X with
the maximal a posteriori probability P(X|A).
Viterbi’s idea is quite simple. The transition probabilities
P(xt | xt−1,...,x0,at,...,a0) = P(xt | xt−1,at)
are known in advance. By
P(xt,...,x0|at,...,a0) = P(xt−1,...,x0|at−1,...,a0)P(xt | xt−1,at)
(4.11)
one can compute the a posteriori probabilities recursively. A consequence of
Eq. (4.11) is that P(xt−1,...,x0|at,...,a0) > P(x′
t−1,...,x′
0|at,...,a0) and
xt−1 = x′
t−1 implies P(xt,...,x0|at,...,a0) > P(xt,x′
t−1,...,x′
0|at,...,a0).
Since the decoder needs to ﬁnd only the sequence xT ,...,x0 for which P(xT ,...,
x0|at,...,a0) becomes maximal, it need only store, for every t ∈{0,...,T } and
every state s, the probabilities
P(t,s) =
max
xt−1,...,x0 P(xt = s,xt−1,...,x0|at,...,a0).
The Viterbi algorithm uses Eq. (4.11) to compute these probabilities recursively.
Algorithm 4.4 shows the Viterbi algorithm in pseudo-code.

108
4
Correlation Attacks
Algorithm 4.4 Viterbi decoding
1: P(−1,xinitial state) ←1, P(−1,x) ←0 for x ̸= xinitial state {The Markov chain
states in xinitial state}
2: for t from 0 to T do
3:
for all states s do
4:
P(t,s) = maxs′ P(t −1,s′)P(xt = s | xt−1 = s′,at)
5:
end for
6: end for
7: Find s with P(T,s) = max and output the corresponding sequence xT , ..., x0
as the result.
Fig. 4.3 An example for the
Viterbi algorithm
The best way to understand the algorithm is to look at an example. Consider the
convolution encoder from Fig. 4.1. It starts with all ﬂip-ﬂops set to zero. The input
11101... is mapped to the codeword 1110011000.... Assume that the transmission
adds two errors and the receiver receives 1000011000 instead of 1110011000.
The run of the Viterbi algorithm is illustrated by the trellis diagram in Fig. 4.3.
We loop over all possible inputs and remember the number of corresponding
errors. For example the entry 1 in the row 00 at time step 1 stands for the input
sequence 00 that sends the encoder to the state in which both ﬂip-ﬂops are 0 and
shows that the corresponding output sequence 0000 differs in one place from the re-
ceived sequence 1000. At the time step t = 3 there are two possible input sequences
000 and 100 that send the encoder to the internal state 00. If 000 is the input, the
number of errors would be 2 and if 100 is the input, the number of errors would
be 3. We do not know if the internal state of the encoder at time step 2 is 00, but it
is more likely that the input starts with 000 than with 100. We indicate this in the
trellis diagram by drawing the line that connects the state 00 at time step 2 with the
state 00 at time step 3 and not drawing the line that connects the state 01 at time
step 2 with state 00 at time step 3.
At the end we decode the input sequence that produces a code word which is
closest to the received sequence (nearest neighborhood decoding). In the example
this input corresponds to the internal state 10 and, as we can see from the diagram,
the corresponding input sequence is 11101, sending the encoder through the states
00 −10 −11 −11 −01 −10.

4.2
Attacks Based on Convolutional Codes
109
Fig. 4.4 A tree diagram for a (2,1) encoder
The error correction capability of the Viterbi algorithm depends mostly on the
free distance of the convolutional code and its complexity depends mostly on the
number of ﬂip-ﬂops used in the encoder.
4.2.2.2 Sequential Decoding
Sequential decoding was proposed in 1961 by J.M. Wozencraft and B. Rieffen [283]
and was the ﬁrst practical method for decoding convolutional codes. There are sev-
eral interesting variations of the algorithm, the most important being the Fano algo-
rithm [86] and the ZJ-algorithm [135, 289].
Sequential decoding is a tree search algorithm. We represent the computation of
an (n,k) convolutional encoder as a tree where each node has 2k outgoing branches,
one for each possible k-bit input word. For example, the (2,1)-encoder of Fig. 4.1
has the tree diagram shown in Fig. 4.4.
In the diagram the nodes of the tree are labeled by the internal state of the en-
coder. The branches are labeled by the output produced by the encoder in the cor-

110
4
Correlation Attacks
Table 4.1 A Fano metric for
a (2,1) convolutional code
and a BSC with p = 0.25
00
01
10
11
00
0.17
−1.42
−1.42
−3.00
01
−1.42
0.17
−3.00
−1.42
10
−1.42
−3.00
0.17
−1.42
11
−3.00
−1.42
−1.42
0.17
responding step. Every possible computation of the encoder is represented as a path
in the tree. As one can see, the diagram is highly redundant. For example the entire
inﬁnite tree is isomorphic to the subtree in the upper half. The idea of sequential
decoding is to search in the tree diagram for the “best path”. This is done by Algo-
rithm 4.5.
Algorithm 4.5 Sequential decoding
1: {initialization}
2: Store the root of the tree diagram into a priority list.
3: while the path stored with the weight has not reached a terminal node do
4:
Remove the path with the smallest weight from the priority list.
5:
Compute the weights of its successors and store the successors together with
their weights in the priority list.
6: end while
7: Return the path with the smallest weight as the decoding result.
There are different variants of how to compute the weight of a path. The follow-
ing method proposed by Fano [86] is very common.
The Fano metric is essentially just the normal log likelihood metric with an ad-
justment term to make paths of different length comparable. The log-likelihood met-
ric of a code word x0,...,xt−1 given that we received y0,...,yt−1 is
M(y0 ...yt−1|x0 ...xt−1) = logP(y0 ...yt−1|x0 ...xt−1).
In the Fano metric we use for an (n,k) convolutional code the adjustment
M(y0 ...yt−1|x0 ...xt−1) = logP(y0 ...yt−1|x0 ...xt−1) + t(n −k).
Algorithm 4.5 becomes clear if we look at an example. Assume again that we
use the (2,1) convolutional code of Fig. 4.1 to encode 11101..., i.e. we send the
codeword 1110011000.... The channel is a binary symmetric channel with error
probability p = 0.25. Assume that the transmission adds two errors and the receiver
receives 1000011000 instead of 1110011000.
Table 4.1 shows the corresponding Fano metric.
Now we run the sequential decoding. Table 4.2 shows for each step the content
of the priority list.

4.2
Attacks Based on Convolutional Codes
111
Table 4.2 Example of the sequential decoding algorithm
Step 1
Step 2
Step 3
Step 4
Step 5
Step 6
0(−1.42)
00(−1.25)
1(−1.42)
000(−2.67)
001(−2.67)
0011(−2.50)
1(−1.42)
1(−1.42)
000(−2.67)
001(−2.67)
10(−2.84)
10(−2.84)
01(−4.42)
001(−2.67)
10(−2.84)
11(−2.84)
11(−2.84)
01(−4.42)
11(−2.84)
0000(−4.09)
0000(−4.09)
01(−4.42)
0001(−4.09)
0001(−4.09)
01(−4.42)
01(−4.42)
0010(−5.67)
Step 7
Step 8
Step 9
Step 10
Step 11
10(−2.84)
11(−2.84)
111(−2.67)
1110(−2.50)
11101(−2.33)
11(−2.84)
00110(−3.92)
00110(−3.92)
00110(−3.92)
00110(−3.92)
00110(−3.92)
00111(−3.92)
00111(−3.92)
00111(−3.92)
00111(−3.92)
00111(−3.92)
0000(−4.09)
0000(−4.09)
0000(−4.09)
0000(−4.09)
0000(−4.09)
0001(−4.09)
0001(−4.09)
0001(−4.09)
0001(−4.09)
0001(−4.09)
100(−4.26)
100(−4.26)
100(−4.26)
100(−4.26)
01(−4.42)
101(−4.26)
101(−4.26)
101(−4.26)
101(−4.26)
0010(−5.67)
01(−4.42)
01(−4.42)
01(−4.42)
01(−4.42)
0010(−5.67)
0010(−5.67)
0010(−5.67)
11100(−5.50)
110(−5.84)
1111(−5.67)
0010(−5.67)
110(−5.84)
1111(−5.67)
110(−5.84)
After the ﬁrst step we have two possible paths: 0 and 1. Both paths give one error
and hence they have the same Fano metric. If there is a tie, we choose one of the best
paths, in our example we choose to extend the path 0. In the next few steps nothing
special happens, but in step 7 we reach a terminal node with the path 00110 and the
path 00111. Since the weight of these paths is not minimal we cannot stop. So we
continue until we reach, in step 11, the point where the ﬁrst element of the priority
list is a terminal node. So we obtain 11101 as the output of the sequential decoding.
Optimal performance for sequential decoding is achieved by using encoders with
a rapidly growing distance proﬁle. In contrast to Viterbi decoding, the number of
ﬂips-ﬂops used by the encoder does not have a signiﬁcant inﬂuence on the decoding
speed.
4.2.3 Application to Cryptography
We are now ready to apply ideas from convolutional coding to cryptography.

112
4
Correlation Attacks
Take a relation (r,r′) (of weight 2) as deﬁned for the CJS-attack, i.e. cr,j =
cr′,j for j ≥B. The important observation for the CJS-attack was Theorem 4.2,
especially
P

zr + zr′ =
B−1

j=0
(cr,j + cr′,j)xj

= p2 + (1 −p)2.
(4.12)
Since the linear code generated by an LFSR is cyclic, we can shift the indices in
Eq. (4.12), i.e. Eq. (4.12) implies
P

zr+t + zr′+t =
B−1

j=0
(cr,j + cr′,j)xj+t

= p2 + (1 −p)2
(4.13)
for all t ∈Z.
In the CJS attacks we used Eq. (4.12), reducing the correlation attack to the
problem of decoding the [R,B] block code
C′ =
B−1

j=0
(cr,j + cr′,j)ˆxj

(r,r′)∈Ω
 ˆx ∈ZB
2
,
.
In the same way we can use Eq. (4.13) to reduce the correlation attack to the problem
of decoding a (R,1) convolutional code with memory size B.
EC′ : (ˆxt)t∈N →
B−1

j=0
(cr,j + cr′,j)ˆxj

(r,r′)∈Ω

t∈N
.
This idea is due to Johansson and Jönsson [136] and is even older than the CSJ-
attacks.
4.2.3.1 A Fast Correlation Attack Based on Viterbi’s Algorithm
First we apply Viterbi’s algorithm to correlation attacks. This is the idea described
in [136] and analyzed in detail in [137].
In comparison to the original Viterbi algorithm, there are some necessary vari-
ations. The most important is that in convolutional coding we always assume that
the encoder is initialized with the state 0. This allows us to assign the probability
1 to the initial state in the Viterbi algorithm (see line 1 of Algorithm 4.4). For the
convolutional (R,B) code constructed in the previous section we have no such a
priori information. Thus line 1 of Algorithm 4.4 must be replaced by a CSJ-attack
which calculates the correct likelihood values. The work that we have to do for the
initial CSJ-attack is approximately the same as we have to do for a single Viterbi
step.

4.2
Attacks Based on Convolutional Codes
113
In [83] the following variation of the attack based on Viterbi’s algorithm was
developed. Consider k time steps of the LFSR as one step of the convolutional code.
Thus we reduce the correlation attack to the decoding of a (Rk,k) convolutional
code of memory size B.
Each Viterbi step of the (Rk,k) convolutional code of memory order B needs
2k+B operations. In fact, the number of operations also depends on Rk; we must
evaluate the relations. However, as we have seen in Sect. 4.1.6.2, the time needed
to count the satisﬁed relations via a Fourier transform is almost independent of the
number of relations.
For the correlation attack this means that we can process t time steps of the LFSR
in about t
k2k+B operations. Since 21/1 = 22/2, the attack using the (R2,2) convo-
lutional code is almost as fast as the original attack using the (R1,1) convolutional
code.
The advantage of the new attack is that R2 ≈3R1, and hence the variation with
the (R2,2) convolutional code evaluates about 3/2 more relations than the original
attack for almost no extra work.
The evaluation of the relations via simple counting (see Sect. 4.1.6.1) needs some
adjustments. As pointed out in [83], the problem of counting the satisﬁed relations
reduces naturally to the problem of counting the 1 bits in the columns of a bit ma-
trix. Since transposing a bit matrix is an expensive operation (see [276] Sect. 7.3),
one cannot reduce this problem to the normal bit count algorithms. In [82, 83] the
authors give a specialized vertical bit count algorithm that solves this problem efﬁ-
ciently.
The Viterbi algorithm can also beneﬁt from the use of sequential tests. We can
detect paths which have a bad likelihood value early and avoid performing further
computations in these paths. The underlying statistical problem is the following.
Given a random sequence of independent random variables X1,X2,... we know
Xi ∼F for i < t and Xi ∼G for i ≥t. But the value t is unknown and may be
inﬁnite. The problem is to detect as early as possible that we have passed the change
point t. We will not go into the details of the statistics of such change point problems
(see Sect. 15.3 and the references given there, especially [249]).
4.2.3.2 A Fast Correlation Attack Based on Sequential Decoding
Sequential decoding applied to correlation attacks also gives an interesting algo-
rithm. It is closely related to twice step decoding (see Sect. 4.1.5).
First note that in the sequential decoding, we always have full paths. This means
if we extend a path from length k −1 to length k, we can use relations where the
columns of the generator matrix satisﬁes ci1 +···+cir = c with ck−1 = 1 and cj = 0
for j ≥k. Thus in the pre-processing phase we generate for every k an extra relation
set Ω(k). Since for each relation in Ω(k) we have 2k degrees of freedom in the sum
c, the size of the relation sets Ωk grows exponentially.
In comparison to classical sequential decoding this has the effect that we reach
the point where a path is either detected as deﬁnitively true or false very quickly.
One may interpret the sequential decoding as a variation of the twice step decoding

114
4
Correlation Attacks
(Sect. 4.1.5) where we take multiple steps and increase the search window by only
one bit at each step.
Sequential decoding also works very well with extended relation sets. If our cur-
rent search window has width k we can interpret the relations from the set Ω(k+1) as
extended relations with characteristic set {k}. So it is very attractive to use this ex-
tended relation set. If we must extend the path, we ﬁx one more bit and the extended
relations by normal relations, so we can reuse the previously determined number of
satisﬁed (extended) relations.
To speed up the decoding one can start by computing all paths of length B with a
simple CSJ-attack for some suitably chosen B. If B is small enough the sequential
decoding algorithm would generate these paths anyway and we save some memory
operations when we build the stack in one step. If B is too large, the CSJ-attack
will generate many useless paths and we will lose the speed-up from sequential
decoding.
4.3 Attacking LFSRs with Sparse Feedback Polynomials
The attacks described in the previous sections work independently of the feedback
polynomial of the LFSR under attack. If the feedback polynomial is sparse, we can
use a very efﬁcient attack due to W. Meier and O. Staffelbach. In their paper [187]
from 1989 they begin the development of correlation attacks with two attacks. Their
algorithm A is somewhat similar to CJS-attacks and is now outdated by its modern
descendants.
Their algorithm B works perfectly for LFSRs with sparse feedback polynomi-
als. The key idea is the following. In the correlation attacks of the previous sections
we always assumed that the correlations were independent. Some correlations in the
relations could be safely ignored, but they always led to slightly worse success prob-
abilities. If the feedback is sparse it is easy to generate several relations which have
a common column. The attack of Meier and Staffelbach explores the dependency
between such relations.
Let f = xn0 + ··· + xnk be the sparse feedback polynomial of the LFSR under
attack. By (xi)i∈N we denote the true LFSR sequence and by (zi)i∈N we denote
the observed sequence. We know that these sequences are correlated, i.e. we know
P(xi = zi) = p.
The feedback polynomial gives us a relation between the xi. We know
0 = xn0+(i−nj ) + ··· + xnk+(i−nj )
and hence
P

xi = zn0+(i−nj ) + ··· + znj−1+(i−nj ) + znj+1+(i−nj ) + ··· + znk+(i−nj )

= pk =
⌊k/2⌋

j=0
 k
2j

(1 −p)2jpk−2j.
(4.14)

4.3
Attacking LFSRs with Sparse Feedback Polynomials
115
We start with the “a priori probability” P(xi = zi) = p and use the relations of
Eq. (4.14) to get a better “a posteriori probability”.
Example 4.1 Consider the LFSR with feedback polynomial xn + xk + 1. Let p =
0.75 be the strength of the correlation between the true LFSR and the observed
sequence.
Assume zi = 1, i.e. we know P(xi = 1) = 0.75. In addition (4.14) gives us the
relations
P(xi = zi+n + zi+k) = p2 = 0.625,
P(xi = zi+(n−k) + zi−k) = p2 = 0.625,
P(xi = zi+(k−n) + zi−n) = p2 = 0.625.
Suppose that two of the three observed relations are satisﬁed, e.g. let us assume
that zi+n + zi+k = zi+(n−k) + zi−k = 1 and zi+(k−n) + zi−n = 1.
The a posteriori probability is computed as:
P(xi = 1|observed relations) =
pp2
2(1 −p2)
pp2
2(1 −p2) + (1 −p)(1 −p2)2p2
.
(p2
2(1 −p2) is the conditional probability that two of three relations are satisﬁed
under the condition that xi = 1. The factor p is for the a priori probability that
xi = 1. Similarly (1−p)(1−p2)2p2 is the probability that we observe two satisﬁed
relations and that xi = 0.)
For the concrete values we get P(xi = 1|observed relations) = 0.833. The a pos-
teriori probability is much better than the a priori probability 0.75.
This leads to Algorithm 4.6.
Algorithm 4.6 Meier’s and Staffelbach’s attack against LFSRs with sparse feedback
polynomials
(1) Allocate an array of N ﬂoating points to store the probabilities P(xi = 1) for
i = 1,...,N. Initialize the array with p or 1 −p according to the observed
sequences.
(2) For all i = 1,...,N compute the a posteriori probability using the relations
from Eq. (4.14).
(3) Repeat step (2) until at least n probabilities are below ϵ or above 1 −ϵ.
(4) At this point we have determined n outputs of the LFSR with a probability of at
least 1 −ϵ. Use these n outputs of the LFSR to reconstruct the initial values by
solving a linear system of equations.
The correlation of Eq. (4.14) becomes very weak when the feedback polynomial
is not sparse. Even a feedback polynomial of weight 10 is sufﬁcient to guarantee
that the probabilities in Algorithm 4.6 will not converge to 0 or 1. However, for
trinomials Algorithm 4.6 will terminate after a few iterations with the correct result,
even if the LFSR has size 1000 and we observe only 100000 bits.

Chapter 5
BDD-Based Attacks
Binary decision diagram-based attacks, introduced in 2002 by M. Krause [166], are
a type of time-memory trade-off attack based on a remarkably simple idea. The set
of internal states that is consistent with the observed output sequence describes a
Boolean function. The attack is successful if only few solutions are left. It is more
efﬁcient to compute the function than to perform a complete search over all possible
keys, but the binary decision diagram used to store the Boolean function requires a
lot of memory.
5.1 Binary Decision Diagrams
A binary decision diagram (BDD) is a method of representing and manipulating
Boolean functions. The diagram consists of nodes labeled with numbers {1,...,n}
for variables {x1,...,xn} and sinks labeled with ⊥for false and ⊤for true. Each
node has two successors. One successor represents the case that the correspond-
ing variable is false (drawn as a dashed line) and the other represents the case that
corresponding variable is true (drawn as a straight line).
Figure 5.1 shows a simple example that represents the majority function x1x2 +
x1x3 + x2x3 in three variables.
The important point that makes binary decision diagrams an effective represen-
tation of Boolean functions is that nodes can share a successor. A BDD is called
reduced if for each node the two successors are different and if no two nodes have
the same label and successors. The diagram in Fig. 5.1 is not reduced. Figure 5.2
shows a reduced BDD.
Reducing a BDD simply requires the repeated application of the two reduction
steps shown in Fig. 5.3. The reduction process is conﬂuent, so one always reaches
the same reduced form regardless of the order in which the reduction steps are ap-
plied. See also Algorithm 5.2 for an effective implementation of the reduction pro-
cess. In the following we will always assume that the BDD is reduced.
There will still exist many different BDDs describing the same Boolean function.
One can choose at each node which variable to test. The problem of ﬁnding the
A. Klein, Stream Ciphers, DOI 10.1007/978-1-4471-5079-4_5,
© Springer-Verlag London 2013
117

118
5
BDD-Based Attacks
Fig. 5.1 A non-reduced
binary decision diagram
Fig. 5.2 A reduced binary
decision diagram
smallest BDD which represents a given Boolean function is NP-complete, so we are
interested in subclasses of BDDs for which the computational part is efﬁcient.
5.1.1 Ordered BDDs
The most common type of binary decision diagram is the ordered binary decision
diagram. A binary decision diagram is ordered if the label of each node is greater
than the label of its father, i.e. the variables are read from x1 to xn when evaluating
the diagram. Prescribing the order of the variables has the advantage that there exists
a unique reduced ordered binary decision for each function.
Many authors use the term ‘binary decision diagram’ as a synonym for ‘reduced
ordered binary decision diagram’. To emphasize the distinction from free binary
decision diagrams (see Sect. 5.1.2) we will always speak of ordered binary decision
diagrams.
(a) Deleting unnecessary tests
(b) Identifying nodes with the same successors
Fig. 5.3 Reducing a binary decision diagram

5.1
Binary Decision Diagrams
119
For the following algorithms we assume that the nodes of a BDD are stored
as a list of branch instructions Is,...,I0. Each branch instruction Ik is a triple
(vk,Lk,Hk), where vk is the value of the node and Lk and Hk are the indices of
the branch nodes for the false and true case, respectively. By convention we require
that Iv is the root, I1 = (n + 1,1,1) represents the sink ⊤and I0 = (n + 1,0,0)
represents the sink ⊥.
Furthermore, we assume that the instructions are ordered according to the values
vk, which makes some algorithms easier to describe.
Counting all solutions of an ordered binary decision diagram is a simple bottom
up algorithm that assigns to each node the number of paths that lead from the node
to ⊤(see Algorithm 5.1).
Algorithm 5.1 Counting solutions of an ordered BDD
1: c0 ←0, c1 ←1
2: for k from 2 to v do
3:
l ←Lk, h ←Hk
4:
ck ←2vk−vl−1cl + 2vk−vh−1ch
5: end for
6: return 2vs−1cs
Enumerating all solutions of an ordered binary decision diagram is even simpler.
Since every variable occurs at most once per path, every path from the root to ⊤
describes a solution. A deep-ﬁrst search yields all solutions of the BDD in lexico-
graphic order.
One way to build a reduced ordered BDD is to start with a non-reduced BDD and
reduce it. Since BDD algorithms usually run short of memory rather than time, we
want to do the reduction without too much extra storage. The following algorithm is
due to D. Sieling and I. Wegener [251]. It uses an additional pointer AUX per node
and two extra bits.
In the following we assume that the extra bits are the sign bits of the AUX and
the LO pointer of the node. If we want to test if the extra bit is set we simply check
AUX < 0. If we want to set the extra bit and store the value a we set AUX ←a
where a denotes the bitwise complement of a. We cannot use AUX ←−a for this
purpose since a could be 0.
Algorithm 5.2 contains a lot of low level pointer operations. The AUX pointers
are used to link nodes at the same level, while the algorithm searches for duplicated
nodes. When a node is marked for deletion the low-pointer becomes negative and
points to the replacement. All deleted nodes form a stack and are linked by their
high ﬁelds. The pointer AVAIL denotes the top of the stack.
To help our understanding of the algorithm we follow the pointer operations by
hand in a small example. Figure 5.4 shows some important intermediate steps of
Algorithm 5.2. The AUX ﬁelds are indicated by dotted lines.

120
5
BDD-Based Attacks
Algorithm 5.2 Reducing an ordered BDD
1: AUX(0) ←AUX(1) ←AUX(ROOT) ←−1 {Initialize}
2: HEAD(v) ←−1 for 1 ≤v ≤vmax.
3: s ←ROOT
4: while s ̸= 0 do
5:
p ←s, s ←AUX(p), AUX(p) ←HEAD(V (p)), HEAD(V (p)) ←p
6:
if AUX(LO(p)) ≥0 then
7:
AUX(LO(p)) ←s, s ←LO(p)
8:
end if AUX(0) ←AUX(1) ←0
9:
if AUX(LO(p)) ≥0 then
10:
AUX(LO(p)) ←s, s ←LO(p)
11:
end if
12: end while
13: {Now nodes with the same value are linked by their AUX ﬁelds.}
14: for v from vmax downto V (HEAD) do
15:
p ←HEAD(v), s ←0
16:
while p ̸= 0 do
17:
p′ ←AUX(p) {This will be the next value of p.}
18:
q ←HI(p)
19:
if LO(q) < 0 then
20:
HI(p) ←LO(q) {If HI(p) was deleted, set the pointer to the new
value.}
21:
end if
22:
q ←LO(p)
23:
if LO(q) < 0 then
24:
q ←LO(p) ←LO(q) {If LO(p) was deleted, set the pointer to the
new value.}
25:
end if
26:
if q = HI(p) then
27:
LO(p) = q, HI(p) ←AVAIL, AUX(p) ←0, AVAIL ←p {Both
successors are the same, delete the node, see Fig. 5.3 (a).}
28:
else ifAUX(q) ≥0 then
29:
AUX(p) ←s, s ←q, AUX(q) ←p
30:
else
31:
AUX(p) ←AUX(AUX(q)), AUX(AUX(q)) ←p
32:
end if
33:
p ←p′
34:
end while
35:
{Nodes with LO(p) = x ̸= HI(p) are now linked together via their AUX
ﬁeld.}
36:
r ←s, s ←0
37:
while r ≥0 do
38:
q ←AUX(r), AUX(r) ←0
39:
if s = 0 then

5.1
Binary Decision Diagrams
121
Algorithm 5.2 (Continued)
40:
s ←q
41:
else
42:
AUX(p) ←q
43:
end if
44:
p ←q
45:
while AUX(p) > 0 do
46:
p ←AUX(p)
47:
end while
48:
r ←AUX(p)
49:
end while
50:
q ←p ←s
51:
while p ̸= 0 do
52:
s ←LO(p)
53:
repeat
54:
r ←HI(q)
55:
if AUX(r) ≥0 then
56:
AUX(r) ←q
57:
else
58:
LO(q) ←AUX(r), HI(q) ←AVAIL, AVAIL ←q
59:
end if
60:
q ←AUX(q)
61:
until q = 0 or LO(q) ̸= s
62:
repeat
63:
if LO(p) ≥0 then
64:
AUX(HI(p)) ←0
65:
end if
66:
p ←AUX(p)
67:
until p = q
68:
end while
69: end for
70: if LO(ROOT) < 0 then
71:
ROOT < LO(ROOT)
72: end if
We must also be able to build binary decision diagrams step by step. For exam-
ple we might have the binary decision diagrams for f and g and want to ﬁnd the
diagram for f ∧g.
Let ◦be any Boolean operator. If either f or g is constant then f ◦g has an
obvious value (constant, f , g or the complement of f or g).
Otherwise we write f = x1 ?fH : fL and g = x1 ?gH : gL. (Here X ?Y : Z =
(X ∧Y) ∨( ¯X ∧Z) denotes the C-style if-then-else operator.) Then
f ◦g = x1 ?(fH ◦gH) : (fL ◦gL).
(5.1)

122
5
BDD-Based Attacks
(a) State at line 13
(b) State after line 49, ﬁrst iteration of the loop
(c) State after line 68, ﬁrst iteration of the loop
Fig. 5.4 Algorithm 5.2 applied to the diagram of Fig. 5.1
This suggest a deep-ﬁrst recursive algorithm to compute the binary decision diagram
for f ◦g. To ensure that the new BDD is reduced one uses memoization. Each time
we want to create a node, we ﬁrst consult a look-up table to check if an equivalent
node already exists. In addition one must memoize the calls of the synthesis function
to avoid unnecessary calls.
This is the algorithm of choice if we want to build a so-called BDD-base, which
represents f , g and f ◦g and several other BDDs simultaneously. The disadvantage
is that this algorithm needs dynamic storage allocation.
An alternative is a breadth-ﬁrst algorithm based on the idea of melting two BDDs.
Let a = (v,l,h) and a′ = (v′,l′,h′) be two BDD nodes. Then the melt a ⋄a′ is

5.1
Binary Decision Diagrams
123
Fig. 5.5 The melt of two
BDDs
deﬁned by
a ⋄a′ =
⎧
⎪⎨
⎪⎩
(v,l ⋄l′,h ⋄h′)
if v = v′,
(v,l ⋄a′,h ⋄a′)
if v < v′,
(v,a ⋄l′,a ⋄h′)
if v > v′.
(5.2)
Figure 5.5 shows an example of the melting procedure. The melted diagram has
for sinks ⊥⋄⊥, ⊥⋄⊤, ⊤⋄⊥and ⊤⋄⊤.
The melted diagram contains all the information needed to compute any Boolean
function f ◦g, one just replaces the sink ⊥⋄⊥by ⊥◦⊥, and so on.
The breadth-ﬁrst algorithm constructs the melt of the two BDDs and reduces it
afterwards by a variation of Algorithm 5.2. With some careful optimizations one
can make this very efﬁcient with respect to memory consumption and cache ac-
cess. However the pointer operations are very involved. We will skip the detailed
description of this algorithm (see the fully documented reference implementation
that comes with this book or [156] Algorithm S).

124
5
BDD-Based Attacks
Algorithm 5.3 Check that a given BDD is free
Require: BDD nodes are ordered, each node comes after its successors
1: used-variables[⊥] ←used-variables[⊤] ←∅
2: for all nodes a = (v,l,h) do
3:
used-variables[a] ←used-variables[l] ∪used-variables[h]
4:
if v ∈used-variables[a] then
5:
return false
6:
else
7:
used-variables[a] ←used-variables[a] ∪{v}
8:
end if
9: end for
10: return true
The above algorithms are the most important BDD algorithms. We mention just
two more common extensions, which are very helpful in many applications but are
not so important for BDD-based attacks.1
• Many applications have a natural variable order. For the case where there is no
natural order, dynamic variable reordering can be used to ﬁnd small BDDs. In
BDD-based attacks against stream ciphers we have a natural variable order, the
order in which the variables are computed by the cipher.
• In many applications many knots have a high pointer to the false sink. Zero sup-
pressed BDDs are a variation which stores such knots implicitly. This can reduce
the memory requirements, however in cryptography we ﬁnd that the low pointer
points to a sink just as often as the high pointer, so this variant is of less interest.
5.1.2 Free BDDs
Free binary decision diagrams are a generalization of ordered binary decision dia-
grams. For a free BDD we require that on each path from the root to a sink every
variable occurs at most once, but the order of the variables may be different from
path to path.
Free BDDs share many properties with ordered BDDs. One can still enumerate
all solutions efﬁciently, and with a little modiﬁcation Algorithm 5.1 counts the num-
ber of solutions of a free BDD. The reduction process described in Fig. 5.3 is still
conﬂuent.
Furthermore it is easy to check that a given BDD is free (Algorithm 5.3).
With a free BDD we associate a control graph. One obtains the control graph
from a free BDD by identifying the two sinks and reducing the resulting graph
1BDD packages are usually optimized in equal parts for speed, memory efﬁciency and ﬂexibility.
In cryptography we can scarify most of the ﬂexibility for memory efﬁciency (see also Sect. 12.2).

5.1
Binary Decision Diagrams
125
Fig. 5.6 A free binary
decision diagram
Fig. 5.7 The control graph of
the free BDD in Fig. 5.6
using only rule (b) of Fig. 5.3. Figure 5.7 shows the control graph of the free BDD
given in Fig. 5.6.
The control graph of an ordered BDD is just a line of nodes. Free BDDs with
prescribed control graphs behave nearly identically to ordered BDDs. For a given
control graph each Boolean function has a unique minimal BDD. If all enlisted
free BDDs have the same control graph, the synthesis algorithm from the previous
subsection can be translated to free BDDs.
The size of the control graph appears as a linear factor in the time and space
complexity.
At this point we close our short introduction and refer the reader to the literature,
see [156, 189, 278].

126
5
BDD-Based Attacks
5.2 An Example of a BDD-Based Attack
The idea of a BDD-based attack is to generate a BDD that describes all internal
states consistent with the observed output. If the BDD has only one solution then
we have found the internal state. BDD-based attacks are a special variant of time-
memory trade-off attacks. We illustrate the idea with the example of the cipher E0
that is used in Bluetooth applications.
5.2.1 The Cipher E0
Bluetooth is a technology used for short range communication. It has been devel-
oped since 1998 by the Bluetooth Special Interest Group, which has more than 5000
member companies.
At several points cryptography plays a role in the Bluetooth protocol (authen-
tication, key exchange, etc.). The payload is encrypted by the stream cipher E0
described below. We will focus on the cipher and skip the key-scheduling algorithm
as well as the reinitialization protocol used in Bluetooth. The reader may consult the
Bluetooth speciﬁcation [31] for the other parts of the protocol.
E0 consist of four linear feedback shift registers with lengths 25, 31, 33 and 39
(in total 128 bits). In addition it has a non-linear combiner with 4 internal bits. The
feedback polynomials of the four LFSRs are
f1 = x25 + x20 + x12 + x8 + 1,
f2 = x31 + x24 + x16 + x12 + 1,
f3 = x33 + x28 + x24 + x4 + 1
and
f4 = x39 + x36 + x28 + x4 + 1.
The bluetooth protocol speciﬁes that the output of LFSR 1 is the bit at position 1.
The outputs of the LFSRs 2, 3 and 4 are the bits at position 7, 1 and 7, respectively.
This is only important if one uses the original key scheduling protocol, so in the
following we assume that the four LFSRs are in canonical form and always use the
bit at position 0 as output.
Figure 5.8 illustrates the E0 algorithm.
Denote the sequences generated by the four LFSRs by s(i)
t
(i ∈{1,2,3,4}).
The combiner has four internal bits denoted by c(0)
t
, c(1)
t
, c(0)
t−1 and c(1)
t−1. Let

5.2
An Example of a BDD-Based Attack
127
Fig. 5.8 The cipher E0
ct = (c(1)
t
c(0)
t
)2 = 2c1
t + c0
t . The register ct is updated by the following equations:
zt+1 =
s(1)
t
+ s(2)
t
+ s(3)
t
+ s(4)
t
+ ct
2

ct+1 = zt+1 ⊕ct ⊕T (ct−1)
T

c(1),c(0)
=

c(0),c(0) + c(1)
and the output of E0 is deﬁned as
xt = s(1)
t
⊕s(2)
t
⊕s(3)
t
⊕s(4)
t
⊕c(0)
t
.
In Fig. 5.8 the small numbers at the wires indicate the number of bits transported
on that wire.
5.2.2 Attacking E0
At the moment E0 is still practically secure. The known attacks fall into two classes:
either they need a very long key stream which is not available in the Bluetooth
protocol or they need too much time for a practical attack.
The best attack of the ﬁrst kind is a correlation attack (see [172]) which needs
≈229 bits of keystream and ≈238 operations.
In the second class we ﬁnd guess and check attacks. The simplest of these attacks
(see [134]) simply guesses the initial bits of the LFSRs 1, 2 and 3 and the internal
state of the combiner and computes the 35 bits of LFSR 4 from the output. This at-
tack needs about 293 operations. Improved versions (see [94]) lower the complexity
to about 284 operations.
In the following we describe a BDD-based attack due to Y. Shaked and
A. Wool [239].

128
5
BDD-Based Attacks
For each time step we use 4 Boolean variables s(1)
t
, s(2)
t
, s(3)
t
and s(4)
t
that rep-
resent the output of the four LFSRs. The two variables c(0)
t
and c(1)
t
represent the
internal state of the compressor. In addition we introduce variables z(0)
t
and z(1)
t
to
represent the two bits of
z =
4
i=1 s(i)
t
+ ct
2

.
We will construct a BDD that represents all allocations which are consistent with
the observed output. As variable order we choose s(1)
t
, s(2)
t
, s(3)
t
, s(4)
t
, c(0)
t
, c(1)
t
, z(1)
t
,
z(0)
t
(t = 1,2,3,...). (This is just the order in which the bits are generated.)
The BDD is synthesized from small BDDs that describe the LFSR feedback poly-
nomials and the compressor.
The elementary BDDs are:
A BDD that determines whether the XOR of some variables is true (or false).
We use this BDD to describe the output s(1)
t
⊕s(2)
t
⊕s(3)
t
⊕s(4)
t
⊕c(0)
t
, the feed-
back polynomials of the four LFSRs and the update function of the compressor
0 = ct+1 ⊕zt+1 ⊕ct ⊕T (ct−1). Figure 5.9 (a) shows a BDD of this type.
In addition we need a BDD for the deﬁning relation of zt (shown in Fig. 5.9 (b)).
The attack against E0 requires us to compute the AND of all these BDDs. Since
we are only interested in the combined BDD, we do not build a BDD base. We will
use the breadth-ﬁrst algorithm described in Sect. 5.1.1.
To determine the complexity of the attack we must estimate the size of the in-
termediate BDDs. When we look at the deﬁnition of the melt of two BDDs (see
Eq. (5.2)) we see that the BDD for B1 ∧B2 needs at most all the nodes of the form
a ⋄a′, i.e. its size is bounded by |B1| · |B2|. This is the worst case upper bound on
the size of the melt (and indeed there are BDDs for which the size of the melt grows
quadratically). However, if we look a bit closer at the deﬁnition of the melt, we see
that if B2 is a BDD of the form shown in Fig. 5.9 (a), then every node is at most
doubled.
Similarly, if we look at the melt of all BDDs that deal with the compressor,
we see that we need at most 160 nodes per time step to describe the behavior of
the compressor. The BDD consists of 16 groups of nodes, one for each possible
compressor state, and each group has 10 nodes to describe the sum s(1)
t
+ s(2)
t
+
s(3)
t
+ s(4)
t
, like the ﬁrst 10 nodes in the BDD of Fig. 5.9 (b)). We do not need
any nodes to describe the variables ct and zt because we can apply an existential
quantiﬁer to those variables after the melt. Thus this part results in a BDD of size
160t.
Each recursion equation of an LFSR is described by a BDD like the one in
Fig. 5.9 (a). Since the BDD contains only two nodes per level, melting it with an-
other BDD at most doubles the nodes. We need t −25 recursion equations to de-
scribe the ﬁrst LFSR, t −31 equations for second LFSR, t −33 for the third LFSR
and t −39 for the fourth LFSR. Thus for t ≥39 we get 24t−128 · (160t) as an upper
bound for the size of the BDD that describes the ﬁrst t time steps of E0.

5.2
An Example of a BDD-Based Attack
129
(a) XOR of some variables
(b) z = ⌊
4
i=1 s(i)
t +ct
2
⌋
Fig. 5.9 Basic BDDs for attacking E0
Another bound on the size of the BDD comes from the number of solutions.
A BDD with n variables and only s solutions consists of s (partly overlapping)
paths of length n. Thus it has at most ns nodes. In the case of the BDD describing
E0 we obtain the following: E0 has 128 degrees of freedom and each time step
yields one equation, so after t steps we expect about 2128−t possible solutions. This
bounds the size of the BDD to 4t · 2128−t.
Putting both bounds for the BDD together, we see that the BDD reaches its largest
size approximately at time step 50. At this point it has about 286 nodes which is

130
5
BDD-Based Attacks
also a good estimate for the time complexity of the attack. As we can see, the sim-
ple BDD-based attack already reaches a time complexity comparable with the best
guess and check attacks. The problem is that the space complexity of 286 is clearly
unacceptable. So we need some time-memory trade-off.
The natural approach is to guess some of the initial bits and apply the BDD-based
attack to a reduced version of E0. We guess all 25 bits of the ﬁrst LFSR. That has the
advantage that we can compute the whole output of the ﬁrst LFSR which removes
some BDDs from the melt. Furthermore, we guess the ﬁrst 14 bits of the second and
third LFSRs. Then we can compute the ﬁrst 14 bits of the fourth LFRS directly. For
the size of the BDD at time step t we get the following bounds (16 · 8 · t)23t−128+25
(from melting the single BDDs) and 16 · 2128−25−2·14−t (from the number of paths).
This bounds the size of the BDD to ≈233 which ﬁts into the main memory of a
well-equipped computer. Computing the BDD takes only 233 steps.
Thus the time complexity of the attack is about 225+2·14233 = 286.
The time complexity of this attack is still quite high, but all other attacks against
E0 that need only a few output bits have a similar complexity (in Sect. 6.2.2 we
will present an algebraic attack against E0). So, at the moment, E0 still provides an
acceptable level of security.

Chapter 6
Algebraic Attacks
6.1 Tools for Solving Non-linear Equations
Let us recall the basic idea behind algebraic attacks (see Sect. 3.3.2). The output
of a non-linear combination of LFSRs can be described by a non-linear function
applied to the initial state. The goal of the attacker is to solve a system of non-linear
equations to recover the initial state.
Therefore our study of algebraic attacks begins with a collection of tools for
solving non-linear systems of equations.
We will present two methods for solving systems of non-linear equations. Gröb-
ner bases are a general method and linearization is a specialized method for the
over-deﬁned systems common to cryptography.
6.1.1 Gröbner Bases
6.1.1.1 Ordering on Monomials
Gröbner bases are used to solve the ideal membership problem. Let us examine two
cases in which we know how to solve this problem.
In the univariate case we solve the ideal membership problem by computing
the greatest common divisor with the Euclidean algorithm and the main step is the
division with remainder. The key idea is to eliminate high degree terms.
The second case is a system of linear equations which we solve by Gaussian
elimination. Here the key idea is to eliminate variables step by step.
Both cases involve the ordering of terms in polynomials and a common key ele-
ment is the elimination of high order terms.
We start with the deﬁnition of monomial orderings in the multivariate case.
A monomial in F[x1,...,xn] is an expression of the form xα1
1 ···xαn
n . For short,
we write xα with α = (α1,...,αn) for the monomial xα1
1 ···xαn
n . The degree of xα
is |α| = α1 + ··· + αn.
A. Klein, Stream Ciphers, DOI 10.1007/978-1-4471-5079-4_6,
© Springer-Verlag London 2013
131

132
6
Algebraic Attacks
Deﬁnition 6.1 A monomial order on F[x1,...,xn] is a relation ≺on Nn or equiv-
alently a relation on the monomials xα with α ∈Nn which satisﬁes the following
properties:
(1) ≺is a total ordering on Nn.
(2) If α ≺β then α + γ ≺β + γ for all γ ∈Nn.
(3) ≺is a well-ordering of Nn, i.e. every non-empty subset of Nn has a smallest
element with respect to ≺.
An alternative formulation of the well-ordering condition is that ≺has no inﬁnite
descending chain, i.e. there exists no sequence (αi)i∈N with ai+1 ≺αi.
For the univariate case F[x], the only monomial order is the normal order < on
the non-negative integers.
Example 6.1 Some important monomial orderings are:
(1) The lexicographic ordering ≺lex, which is deﬁned by (α1,...,αn) ≺lex (β1,...,
βn) if and only if there exists a k ∈{1,...,n} with αk < βk and αj = βj for all
j < k.
(2) The graded lexicographic ordering ≺grlex, which is deﬁned by α ≺grlex β if
either
|α| =
n

j=1
αi < |β| =
n

j=1
βi
or
|α| = |β|
and
α ≺lex β.
(3) The graded reverse lexicographic ordering ≺grevlex, which is deﬁned by
a ≺grevlex β if and only if either |α| < |β| or |α| = |β| and α ≻lex′ β where
≺lex′ is the lexicographic ordering with xn ≺xn−1 ≺··· ≺x1.
Theorem 6.1 ≺lex, ≺grlex and ≺grevlex are monomial orderings.
Proof Note that α ≺lex β if and only if the ﬁrst non-zero coordinate of α −β ∈Zn
is negative. From this we see immediately that ≺lex is a total order since for α ̸= β,
α −β ̸= 0 and hence the ﬁrst element is either negative or positive.
Furthermore, since α −β = (α + γ ) −(β + γ ) we have α ≺lex β if and only if
α + γ ≺lex β + γ .
Assume that ≺lex is not a well-ordering, i.e. assume that there exists an inﬁnite
descending chain
α(1) ≻lex α(2) ≻lex a(3) ≻lex ··· .
By deﬁnition of ≺lex we ﬁnd that the ﬁrst coordinates of the α(i) must be monotoni-
cally decreasing, i.e. α(1)
1
≥α(2)
1
≥···. Since < is a well-ordering on N, each mono-

6.1
Tools for Solving Non-linear Equations
133
tonically decreasing sequence must become stationary, i.e. there exists an N1 ∈N
and an α1 with α(i)
1 = α1 for i ≥N1.
By the same argument we see that α(N1)
2
≥α(N1+1)
2
≥··· is a monotonically
decreasing sequence and hence it becomes stationary, i.e. α(i)
2 = α2 for some α2 and
i ≥N2. Since N2 ≥N1 we also have α(i)
1 = α1 for all i ≥N2.
Repeating these steps we ﬁnd a bound Nn such that a(i)
k
= αk for all i > N2, but
this means that the sequence α(i) becomes stationary, which contradicts the assump-
tion, i.e. ≺lex is a well-ordering.
Both orders ≺grlex and ≺grevlex sort the monomials ﬁrst by their degree. The only
difference is their tie breaking strategies. Since we use a total order to break the ties,
≺grlex and ≺grevlex are total orders.
Since |α + γ | = |α| + |γ | we have |α| > |β| if and only if |α + γ | > |α + γ |,
hence ≺grlex and ≺grevlex satisfy condition (b) of Deﬁnition 6.1.
For every α ∈Nn the number of elements β ∈Nn with |β| ≤|α| is ﬁnite (to be
exact, the number is
n+|α|−1
|α|

). Hence every descending chain in a graded order
must become stationary.
□
A uniform way to describe all possible monomial orderings is via matrix orders.
Deﬁnition 6.2 For a regular matrix M ∈Rn×n with only non-negative entries the
matrix order ≺M on Nn is deﬁned by α ≺M β if and only if Mα ≺lex Mβ. (Here
≺lex denotes the lexicographic order on RN.)
Theorem 6.2 (Robbiano [224]) Every matrix order is a monomial ordering. For
every monomial ordering ≺there exists a regular matrix M ∈Rn×n with only non-
negative entries with α ≺β ⇐⇒α ≺M β.
Proof Let M ∈Rn×n be a regular matrix with only non-negative entries. We prove
that >M is a monomial ordering.
Since M is regular, Mα ̸= Mβ and hence either Mα ≺lex Mβ or Mα ≻lex Mβ.
If α ≺M B, i.e. if Mα ≺lex Mβ then Mα + Mγ ≺lex Mβ + Mγ , i.e. α + γ ≺M
β + γ .
Assume that ≺M is not a well-ordering, i.e. assume that there exists an inﬁnite
sequence αi with αi ≻M αi+1. Let M = (m1,...,mn) be the rows of M. Since the
set {m1x | x ∈Zn and m1x < c} is ﬁnite for every c ∈R, the sequence m1αi must
become stationary, i.e. there exists an N1 with m1αi = c1 for all i > N1. By the
same argument we ﬁnd an N2 > N1 with m2αi = c2, and so on.
However, for i > Nn this means Mαi = c and since M is regular the sequence αi
must be stationary.
For the opposite implication see [224].
□
Example 6.2 The identity matrix deﬁnes the lexicographic order.

134
6
Algebraic Attacks
The graded lexicographic order is deﬁned by the matrix
Mgrlex =
⎛
⎜⎜⎜⎝
1
···
1
1
1
0
0
...
...
0
1
0
⎞
⎟⎟⎟⎠
and the graded reverse lexicographic order is deﬁned by
Mgrevlex =
⎛
⎜⎜⎜⎝
1
1
···
1
1
···
1
0
...
...
...
...
1
0
···
0
⎞
⎟⎟⎟⎠.
With a monomial order we can extend the notion of leading terms from univariate
to multivariate polynomials.
Deﬁnition 6.3 Let f ∈
α∈Nn cαxα ∈F[x1,...,xn] be a non-zero polynomial and
let ≺be a monomial order.
(a) Each cαxα with cα ̸= 0 is a term of f .
(b) The multidegree of f is
mdeg(f ) = max ≺

α ∈Nn | cα ̸= 0

,
where max≺is the maximum with respect to ≺.
(c) The leading coefﬁcient of f is lc(f ) = cmdeg(f ).
(d) The leading monomial of f is lm(f ) = xmdeg(f ).
(e) The leading term of f is lt(f ) = lc(f )lm(f ).
Now the transfer of the division algorithm to multivariate polynomials is straight-
forward (see Algorithm 6.1).
Note that the remainder is not necessary unique, since there could be several
possible values of i in line 4.
Example 6.3 Let f = 3x2y + 2, f1 = xy −1 and f2 = x2 −x and choose ≺lex as
the monomial order.
Then lt(f1) = xy divides lt(f ) = 2x2y and lt(f2) = x2 also divides 2x2y.
If we choose i = 1 in line 4 of Algorithm 6.1 we obtain, after the ﬁrst step,
p = 3x +2 and the division algorithm terminates immediately with q1 = 3x, q2 = 0
and r = 3x + 2.
If, on the other hand, we choose i = 2 in line 4 of Algorithm 6.1, we obtain
p = 3xy + 2 after the ﬁrst step and the division algorithm needs one more step
before ending with the result q1 = 3, q2 = 3y and r = 5.
As one can see, neither q1, q2, r nor the number of reduction steps are uniquely
determined.

6.1
Tools for Solving Non-linear Equations
135
Algorithm 6.1 Multivariate division with remainder
Require: f,f1,...,fk ∈F[x1,...,xn] are non-zero polynomials, f is a ﬁeld and
≺is a monomial order on F[x1,...,xn].
Ensure: q1,...,qk,r ∈F[x1,...,xn] satisfy f = q1f1 + ··· + qkfk + r and no
monomial in r is divisible by any lt(f1), ..., lt(fk).
1: r ←0, p ←0
2: qi ←0 for i ∈{1,...,k}
3: while p ̸= 0 do
4:
if lt(fi) divides lt(p) for some i then
5:
qi ←qi + lt(p)
lt(fi), p ←p −lt(p)
lt(fi)fi
6:
else
7:
r ←r + lt(p), p ←p −lt(p)
8:
end if
9: end while
10: return q1, ..., qk, r
We will make Algorithm 6.1 deterministic by always choosing the smallest pos-
sible value of i in line 4. We will write
r = f rem(f1,...,fk)
for the remainder, which is now uniquely deﬁned.
For k = 1 the remainder solves the ideal membership problem, f ∈⟨f1⟩if and
only if f rem(f1) = 0. For k ≥2 this is not the case as the following example shows.
Example 6.4 Let f = xy2 + 2x, f1 = xy + x and f2 = y2 + 2. Then with the lexi-
cographic order ≺lex the algorithm gives us q1 = y, q2 = 0 and r = −xy + 2x, i.e.
f rem(f1,f2) ̸= 0. But f = xf2 and hence f ∈⟨f1,f2⟩.
Next we want to deﬁne special ideal bases for which the result of the multivariate
division with remainder is unique (no matter which i is chosen in line 4) and hence
give the correct answer for the ideal membership problem.
6.1.1.2 Monomial Ideals and the Hilbert Basis Theorem
Deﬁnition 6.4 A monomial ideal I is an ideal generated by monomials, i.e. there
exists a set A ⊆Nn with
I =
-
xA.
=
-
xα | α ∈A
.
.
Lemma 6.3 Let I = ⟨xα | α ∈A⟩be a monomial ideal. Then xβ ∈I if and only if
xβ is a multiple of xα for some α ∈A.
Proof Together with xα, I also contains every multiple xβ.

136
6
Algebraic Attacks
For the opposite direction let xβ = qixαi with αi ∈A. Then xβ occurs in at
least one term qixαi and so xβ is a multiple of xαi.
□
The next lemma states that one can determine if f lies in a monomial ideal I by
looking only at the monomials in I.
Lemma 6.4 Let I be a monomial ideal and let f ∈F[x1,...,xn]. Then the follow-
ing are equivalent:
(a) f ∈I.
(b) f is a linear combination of monomials in I.
Proof Let f = qixαi. Then every monomial that occurs in f must also occur on
the right hand side, i.e. it must be a multiple of xαi for some i. By Lemma 6.3 this
means that every monomial of f lies in I.
The implication (b) =⇒(a) is trivial and holds for every ideal I.
□
Corollary 6.5 Two monomial ideals are the same if they contain the same monomi-
als.
The next lemma states that a monomial ideal is ﬁnitely generated. This is a big
step towards Hilbert’s basis theorem.
Lemma 6.6 (Dickson’s lemma) Every monomial ideal is generated by a ﬁnite set
of monomials.
Proof Let A ∈Nn be the set of all α with xα ∈I. The set Nn is partial ordered by
≤with
α ≤β
⇐⇒
ai ≤βi
for all i ∈{1,...,n}.
Let B be the set of minimal elements in A, i.e. B = {β ∈A | ∄α ∈A : α < β}. By
the preceding lemmas, the monomials xβ with β ∈B generate I.
We claim that B is ﬁnite.
For n = 1 the partial order < is a well-ordering and B contains just the unique
minimal element of A. Let Ai = {(a1,...,αi−1,αi+1,...,αn) | α ∈A} be the pro-
jection of A onto all but the ith coordinate. By induction, the set Bi = { ¯β(1),..., ¯βki}
of minimal elements of Ai is ﬁnite. For each ¯β(j) ∈Bi choose a β(j) ∈A which
projects to ¯β(j).
Let bi = max1≤j≤ki{β(j)
i
}. We claim that for every α = (a1,...,αn) ∈A there
exists a β ≤α with βi ≤bi. By deﬁnition of Bi, there exists a ¯β(j) with ¯β(j)
i′
≤ai′
for i′ ̸= i. If β(j)
i
≤αi then β(j) is the element we are searching for. If αi < β(j)
i
then αi ≤bi and α ≤α is the element we seek.
This proves that the ith coordinate of a minimal element of A is bounded by bi.
Since we have such a bound for all i ∈{1,...,n}, there are only ﬁnitely many min-
imal elements.
□

6.1
Tools for Solving Non-linear Equations
137
Monomial ideals give a very elegant proof of Hilbert’s basis theorem.
Theorem 6.7 (Hilbert’s basis theorem) Every ideal of F[x1,...,xn] is ﬁnitely gen-
erated.
Proof Let I ′ = ⟨lt(I)⟩be the monomial ideal corresponding to I. Let B′ =
{m1,...,mk} be a ﬁnal set of monomials generating I ′, which exists by Lemma 6.6.
For i ∈{1,...,k} choose fi ∈I with lt(fi) = mi. We claim that B = {f1,...,fk}
is a basis for I.
Let f ∈I. The division with remainder algorithm (Algorithm 6.1) gives us poly-
nomials g1, ..., gk and r with f = g1f1 + ··· + gkfk + r and either r = 0 or no
term of r is divisible by a leading term of any fi.
Since r = f −g1f1 −···−gkfk ∈I we have lt(r) ∈I ′ = ⟨lt(f1),...,lt(fk)⟩and
hence r = 0. This proves I = ⟨f1,...,fn⟩.
□
An important consequence of Hilbert’s basis theorem is:
Corollary 6.8 (Ascending chain condition) The ring F[x1,...,xn] is Noetherian,
i.e. any ascending chain I1 ⊆I2 ⊆I3 ⊆··· stabilizes, that is IN = IN+1 = ··· = I
for N large enough.
Proof Let I = /∞
j=1 Ij. By Hilbert’s basis theorem I has a ﬁnite basis I =
⟨g1,...,gk⟩. Set N = min{j ∈N | g1,...,gk ∈Ij}, then IN = IN+1 = ··· = I.
□
6.1.1.3 Gröbner Bases and Buchberger’s Algorithm
The proof of Theorem 6.7 suggests that an ideal basis {f1,...,fk} with
-
lt(f1),...,lt(fk)
.
=
-
lt(I)
.
is something special. We give such a basis a name.
Deﬁnition 6.5 Let ≺be a monomial order and the I ⊆F[x1,...,xn] be an ideal.
A ﬁnite set G ⊆I is a Gröbner basis for I with respect to ≺if ⟨lt(G)⟩= ⟨lt(I)⟩.
The name Gröbner basis was given to such a basis in 1965 by Bruno Buchberger
in honor of his advisor Wolfgang Gröbner.
As we have already seen in the proof of Hilbert’s basis theorem, every ideal of
F[x1,...,x2] has a ﬁnite Gröbner basis.
One special property of Gröbner bases is that, for these bases, the division algo-
rithm produces a unique remainder.
Theorem 6.9 Let G be a Gröbner basis for the ideal I and let f ∈F[x1,...,xn]
be an arbitrary polynomial. Then there exists a unique polynomial r ∈R such that:

138
6
Algebraic Attacks
(a) f −r ∈I; and
(b) no term of r is divisible by any monomial in lt(G).
Proof The existence of r follows from the multivariate division algorithm (Algo-
rithm 6.1).
Assume that f = h1 + r1 = h2 + r2 with h1,h2 ∈I and no term of r1 or r2 is
divisible by any leading monomial in lt(G).
Then r1 −r2 = h2 −h1 ∈I and since G is a Gröbner basis lt(r1 −r2) ∈⟨lt(G)⟩.
By Lemma 6.3, lt(r1 −r2) is divisible by some monomial in lt(G), i.e. r1 −r2 = 0. □
As a consequence, we conclude that for a Gröbner basis the division algorithm
(Algorithm 6.1) produces the same remainder r no matter which i we choose in
line 4. Note that the quotients q1, ..., qk still depend on the order of reduction steps
in Algorithm 6.1. An immediate consequence is that Gröbner bases solve the ideal
membership problem.
Corollary 6.10 Let G be a Gröbner basis for the ideal I. Then f ∈I if and only if
f remG = 0.
Now we consider the problem of constructing a Gröbner basis. One reason why
not all bases are Gröbner bases is that the leading term of the linear combination
axαf + bxβg may not be divisible by lt(f ) or lt(g), since the leading terms of
axαf and bxβg cancel. We will look at such cancellations in detail, which leads to
the deﬁnition of S-polynomials.
Deﬁnition 6.6 Let g,h ∈F[x1,...,xn].
(a) If mdeg(f ) = α and mdeg(g) = β, let γ = (γ1,...,γn) with γi = max{αi,βi}.
We call xγ the least common multiple of lm(f ) and lm(g) and write xγ =
lcm(lm(f ),lm(g)).
(b) The Syzygien polynomial or S-polynomial is
S(g,h) = xγ
lt(f )f −xγ
lt(g)g.
Lemma 6.11 A ﬁnite set G = {g1,...,gk} is a Gröbner basis of the ideal I = ⟨G⟩
if and only if
S(gi,gj)remG = 0
for 1 ≤i < j ≤k.
Proof S(gi,gj) ∈I and by Corollary 6.10 this implies S(gi,gj)remG = 0 if G is
a Gröbner basis.
Now assume that S(gi,gj)remG = 0 for 1 ≤i < j ≤k. We have to prove that
for f ∈I, the leading term satisﬁes lt(f ) ∈⟨lt(G)⟩.

6.1
Tools for Solving Non-linear Equations
139
Let f = k
j=1 cigi be a representation of f with the property that δ =
max≺{mdeg(cigi) | 1 ≤i ≤k} is minimal under all possible representations of f .
Such a minimum exists, since ≺is a well-order.
If δ = mdegf then
lt(f ) =

1≤i≤k
mdeg(cigi)=δ
lt(cigi)
and hence lt(f ) ∈⟨lt(G)⟩.
Now assume that mdegf ≺δ, i.e. the leading terms cancel. Let j be the largest
index for which mdeg(cigi) = δ. Then

1≤i≤k
mdeg(cigi)=δ
lt(ci)gi =

1≤i<j
mdeg(cigi)=δ
lt(ci)
lcm(lmgi,lmgk)/lmgi
S(gi,gj)
because otherwise the leading terms do not cancel.
By assumption S(gi,gj)remG = 0, i.e. the multivariate division with re-
mainder (Algorithm 6.1) gives us a representation S(gi,gj) = k
l=1 c(i,j)
l
gl with
mdeg(c(i,j)
l
gl) ≤mdegS(gi,gj).
Then
f =
k

j=1
cigi −

1≤i≤k
mdeg(cigi)=δ
lt(ci)gi +

1≤i<j
mdeg(cigi)=δ

lt(ci)
lcm(lmgi,lmgk)/lmgi
k

l=1
c(i,j)
l
gl

is a representation of f with a smaller δ, contradicting the assumption that the orig-
inal representation was already minimal.
□
Lemma 6.11 suggest the following way to compute a Gröbner basis. Compute
S(p,q)remG for all Syzygien polynomial and add the remainders to the basis if
necessary. This leads to Algorithm 6.2.
Theorem 6.12 Algorithm 6.2 terminates after ﬁnitely many steps and returns a
Gröbner basis G of I = ⟨f1,...,fk⟩.
Proof First note that at every step ⟨G⟩= I. If the algorithm terminates then
S(gi,gj)remG = 0 for all g1,gj ∈G and, by Lemma 6.11, G is a Gröbner ba-
sis.
So we need only show that Algorithm 6.2 terminates. Every time we add an ele-
ment to G in line 7 the leading monomial of the new element does not lie in ⟨lt(G)⟩.
So Algorithm 6.2 produces a chain ⟨lt(G1)⟩⊃⟨lt(G1)⟩⊃···. By the ascending
chain condition (Corollary 6.8) this chain cannot be inﬁnite, i.e. Algorithm 6.2 en-
larges G only a ﬁnite number of times.
□
The presentation of Algorithm 6.2 was chosen for simplicity. An efﬁcient im-
plementation must contain several improvements. First, each S-polynomial must be

140
6
Algebraic Attacks
Algorithm 6.2 Buchberger’s algorithm
Ensure: G is a Gröbner basis of I = ⟨f1,...,fk⟩
1: G ←{f1,...,fk}
2: repeat
3:
G′ ←G
4:
for each pair (p,q), p ̸= q in G′ do
5:
Compute S = S(p,q)remG
6:
if S ̸= 0 then
7:
G ←G ∪{S}
8:
end if
9:
end for
10: until G = G′
11: return G
evaluated at most once. Secondly, there are many criteria to detect S-polynomial that
reduce to zero quickly, and the speed of implementation of Buchberger’s algorithm
depends mainly on how these criteria are applied (see for example [39, 103]).
Finally, the more recent algorithms of Jean-Charles Faugère (F4 [87] and
F5 [88]) are more efﬁcient than the original Buchberger algorithm. In Sect. 6.1.2.4
we will sketch the F4 algorithm to highlight its similarities with the XL-algorithm.
There are many good implementations (commercial and open source) of Gröbner
basis algorithms. To give just one example, the open source computer algebra sys-
tem Singular [253] contains effective implementations of Gröbner basis algorithms.
6.1.1.4 The Gröbner Walk
One problem with Gröbner bases is that elimination orders such as the lexicographic
order usually perform badly. The reason is that eliminating x1 can increase the de-
gree of all other variables arbitrarily. Graded orders are more efﬁcient and normally
graded reverse lexicographic orders are the best. The Gröbner walk (see [6, 55]) is
a technique that allows one to convert a Gröbner basis G≺with respect to an order
≺to a Gröbner basis G≺′ with respect to another order ≺′ very quickly. It is often
better to compute the Gröbner basis with respect to a good order such as the graded
reverse lexicographic order and then convert it into the order needed for the current
application.
Consider two matrix orders ≺A and ≺B. For simplicity we assume that only the
ﬁrst rows of A and B are different (see [55] for general matrices). The walk consists
of computing the Gröbner bases with respect to ≺(1−t)A+tB and letting t range from
0 to 1. The important part is that one can do this in discrete steps and the update
process is much cheaper than computing the Gröbner basis from the start.
To formulate the Gröbner walk we need some notation and terminology.
Deﬁnition 6.7 For each matrix order M the ﬁrst row ω of M is called the leading
form of M.

6.1
Tools for Solving Non-linear Equations
141
ω deﬁnes an ω-degree on K[x1,...,xn]. A monomial xe = n
i=1 xei
i
has ω-
degree
degω

xe
= (e,ω) =
n

i=1
eiωi.
The ω-degree of a polynomial f ∈K[x1,...,xn] is the largest ω-degree of its
monomials. The initial form inω(f ) of f is the part of all monomials with maximal
ω-degree.
A polynomial f is ω-homogeneous if all its monomials have the same ω-degree,
i.e. f = inω(f ).
Let < be a partial order on the monomials and let ≺′ be a monomial order. The
reﬁnement of < by ≺′ is deﬁned by
α ≺β
⇐⇒
(α < β) ∨

(α ≮β) ∧(β ≮α) ∧

α ≺′ β

.
Lemma 6.13 Let A and B be two regular matrices that differ only in the ﬁrst row.
Let ωA and ωB be the ﬁrst row of A and B respectively.
Let G be a Gröbner basis of I with respect to ≺A. Then G is a Gröbner basis
with respect to ≺B if inωA(g) = inωB(g) for all g ∈G.
Proof Since inωA(g) = inωB(g) for all g ∈G and the matrix orders differ only in
the ﬁrst row, we get ltA(g) = ltB(g) for all g. Hence G is either a Gröbner basis for
both orders or for none.
□
Lemma 6.13 assures us that the Gröbner walk proceeds in discrete steps. Let Gt
be a Gröbner basis with respect to the matrix order ≺(1−t)A+tB. Let t′ be deﬁned as
t′ = min

τ ∈(t,1] | in(1−τ)ωA+τωB(g) ̸= in(1−t)ωA+tωB(g) for one g ∈G

. (6.1)
By Lemma 6.13, Gt is a Gröbner basis for all ≺(1−τ)A+τB for all τ ∈(t,t′).
≺1=≺(1−t)A+tB reﬁnes ω = (1 −t′)ωA + t′ωB, i.e. lt≺1(inω(f )) = lt(f ). Hence
-
lt≺1
-
inω(I) |
..
=
-
ltprec1(I)
.
=
-
lt≺1(Gt)
.
since Gt is a Gröbner basis for I
with respect to ≺1
=
-
lt≺1

inω(Gt)
.
which proves that inω(Gt) is a Gröbner basis of ⟨inω(I)⟩.
Now we convert inω(Gt) to a Gröbner basis G′
t′ of ⟨inω(I)⟩with respect to
≺2=≺(1−t′)A+t′B. The crucial point is that this conversion is much simpler that
converting Gt directly into a Gröbner basis Gt′ with respect to ≺2. One point which
makes the conversion efﬁcient is that ⟨inω(I)⟩is much simpler than I. The other
point is that inω(Gt) consists only of ω-homogeneous polynomials. For such ideals

142
6
Algebraic Attacks
we can replace the complex Buchberger algorithm by a very fast method described
by Traverso [266].
Let Gt = {g1,...,gk} and G′
t′ = {m1,...,mh}. Since m1,...,mh are ω-
homogeneous we can determine ω-homogeneous polynomials pij with
mi =
k

j=1
pij inω(gj).
(6.2)
Replacing inω(gj) by gj we get
fi =
k

j=1
pijgj
(6.3)
and we set Gt′ = {f1,...,fh}. By construction inω(fi) = mi and hence
-
lt≺2(I)
.
=
-
lt≺2

inω(I)
.
=
-
lt≺2

G′
t′
.
since G′ is a Gröbner basis of
-
lt≺2

inω(I)
.
=
-
lt≺2

in(Gt′)
.
by (6.3)
=
-
lt≺2(Gt′)
.
since ≺2 reﬁnes ω.
Thus Gt′ is a Gröbner basis of I with respect to ω′.
This leads to Algorithm 6.3, which transforms a Gröbner basis from one mono-
mial order to another.
Algorithm 6.3 The Gröbner walk
Require: GA is a Gröbner basis of I with respect to ≺A.
Ensure: GB is a Gröbner basis of I with respect to ≺B.
1: t = 0, G0 = GA
2: while t < 1 do
3:
Find t′ (see (6.1))
4:
Convert the Gröbner basis in(1−t′)ωA+t′ωB(Gt) of ⟨in(1−t′)ωA+t′ωB(I)⟩to a
Gröbner basis with respect to ≺(1−t′)A+t′B.
5:
Compute the conversion polynomials (see (6.2)) and the new Gröbner basis
Gt′ as in (6.3).
6:
t ←t′
7: end while
A good implementation of Algorithm 6.3 requires a lot of work in addition to the
basic form described above. The choice of the path is especially important, since the
number of steps and the complexity of the steps heavily depends on it. We do not
go into details here and instead refer to [6, 55].

6.1
Tools for Solving Non-linear Equations
143
For ideals of dimension 0, i.e. ideals with only a ﬁnite number of solutions, there
is a specialized method for the change of the monomial ordering due to Faugère,
Gianni, Lazard and Mora [89] which is more efﬁcient than the general Gröbner
walk.
6.1.2 Linearization
Linearization is a technique to solve massive over-deﬁned systems of non-linear
equations. We start our description with the simple original linearization method
and then treat modern variants such as relinearization and the XL-Algorithm. Since
the following algorithms involve a lot of linear algebra, efﬁcient matrix operations
become essential (see also Sect. 12.5).
6.1.2.1 Ordinary Linearization
A very simple way to solve an over-deﬁned system of non-linear equations is to
replace all monomials by new variables and solve the resulting system of linear
equations.
For example, consider the Geffe generator (see Sects. 3.2 and 3.3.2).
Let xi be the output of the Geffe Generator and ai, bi and ci be the output of the
three LFSRs. Then
xi = aibi + ci(bi + 1).
(6.4)
Writing ai, bi and ci as linear combinations of the initial values of the LFSRs we
get
xi =
La−1

j=0
Lb−1

k=0
α(i)
j,kajbk +
Lb−1

j=0
Lc−1

k=0
β(i)
j,kbjck +
Lc−1

j=0
γ (i)
j cj
(6.5)
where the coefﬁcients α(i)
j,k, β(i)
j,k and γ (i)
j
can be computed from the feedback poly-
nomials fa, fb and fc of the three LFSRs.
Now we substitute ajbk by dj,k and bjck by ej,k and get the linear system
xi =
La−1

j=0
Lb−1

k=0
α(i)
j,kdj,k +
Lb−1

j=0
Lc−1

k=0
β(i)
j,kej,k +
Lc−1

j=0
γ (i)
j cj.
(6.6)
In Eq. (6.6) we have L = LaLb + LbLc + Lc unknowns (LaLb variables dj,k,
LbLc variables ej,k and Lc variables cj). Provided that the attacker can observe
enough output bits of the Geffe generator, he can solve the system of linear equa-
tions (6.6) by Gaussian elimination in O(L3) steps.
If L is small, it is no problem to obtain L independent equations and the cost of
the Gaussian elimination is feasible.

144
6
Algebraic Attacks
Once we have the solution of the linear system it is easy to obtain the solution
of the original non-linear system. In the example the linear system contains the
variables ci and at least one of these variables will be non-zero. By investigating
ej,i = bjci for a ci ̸= 0 we ﬁnd the values of bj, and so on.
Linearization is a very effective method. Surprisingly many stream ciphers are
vulnerable to this kind of attack. In the following we will see how to improve the
basic idea further.
6.1.2.2 Relinearization
A drawback of the linearization method is that it needs a lot of equations. Relin-
earization was introduced in 1999 by Kipnis and Shamir [147] with the aim of low-
ering the number of required equations.
The technique can best be described by an example. Consider a system of homo-
geneous quadratic equations

1≤j≤k≤n
α(i)
j,kxjxj = b(i)
i ∈{1,...,m}.
(6.7)
Such equations arise, for example, from the simple non-linear ﬁlter generator shown
in Fig. 3.3.
Using linearization we transform it into a system of m linear equations in
n
2

+ n
variables xj,k:

1≤j<k≤n
α(i)
j,kxj,k = b(i)
i ∈{1,...,m}.
(6.8)
If m <
n
2

+ n, the linear system is under-deﬁned. We apply Gaussian elimination
to express each xj,k as a linear combination of l parameters t1,...,tl. (If the m
equations of (6.8) are linearly independent, l =
n
2

+ n −m. In practice almost all
equations will be linearly independent.)
Next consider the equations
xj,kxl,h = (xjxk)(xlxh) = (xjxl)(xkxh) = xj,lxk,h.
(6.9)
This can be viewed as a quadratic equation in the xj,k variables and hence as a
quadratic equation in the parameters t1,...,tl expressing them.
This system of quadratic equations is now solved by ordinary linearization.
The new system has 2
n
4

+ 3
n
3

+
n
2

equations. We have
n
4

equations of the
form xj,kxl,h = xj,lxk,h = xj,hxk,h where j,k,l,h are all different, each of them
delivering 2 equations to the system. In addition we have 3
n
3

equations of the form
xj,jxk,h = xj,kxj,h and
n
2

equations of the form xj,jxk,k = x2
j,k.
Experiments show that almost all of these equations are linearly independent.
The number of variables in the second linear system is
l
2

+ l.

6.1
Tools for Solving Non-linear Equations
145
Thus relinearization will solve the system (6.8) if
2
n
4

+ 3
n
3

+
n
2

≥
l
2

+ l,
2
n
4

+ 3
n
3

+
n
2

≥1
2
1
2n(n + 1) −m
1
2n(n + 1) −m + 1

,
m ≥−
√
3
√
2n4 −2n2 + 27 −3n2 −3n −9
6
,
m ≥
1
2 −1
√
6

n + O(√n).
This is a signiﬁcant improvement over ordinary linearization, which needs
m ≥n2. The price is that one has to solve a larger system of linear equations.
The idea of relinearization can of course also be applied to higher degree systems,
but for higher degrees regularization generates a lot of redundant equations. There
are no good theoretical bounds on how many equations are needed for a successful
higher degree linearization. Nevertheless, in practice relinearization has proved to
be quite an effective weapon for the attacker.
6.1.2.3 The XL-Algorithm
In 2000 N. Courtois, A. Klimov, J. Patarin and A. Shamir presented an improved
variant of the relinearization algorithm [63] which they called the extended lin-
earization algorithm (or XL-algorithm, for short). Again the real complexity of the
algorithm is not known, but the algorithm behaves well in practice.
Let fi, 1 ≤i ≤m, be a set of multivariate polynomials. We assume that the
system fi = 0 has only a ﬁnite number of solutions (normally only one solution).
The extended linearization algorithm (XL-algorithm) is given by Algorithm 6.4.
We illustrate Algorithm 6.4 with a small example:
Example 6.5 Consider the problem of solving
x2 + αxy = β,
y2 + γ xy = δ
with α ̸= 0 and γ ̸= 0.
Basic linearization does not help because we have three different monomials but
only two equations.
Starting the XL-algorithm with D = 4 gives in the ﬁrst step the following equa-
tions
x4 + αx3y = βx2,
(6.10)

146
6
Algebraic Attacks
Algorithm 6.4 XL-algorithm
1. Multiply: Generate all products of the form xdfi with degxdfi ≤D, (xd =
n
j=1 x
dj
j may be any monomial).
2. Linearize: Consider each monomial of degree ≤D as a new variable and per-
form Gaussian elimination on the equations obtained in step 1.
Choose the variable order in such a way that all terms containing one variable
(say x1) are eliminated last.
3. Solve: Step 2 should result in a univariate equation in powers x1. If this is not
the case, the choice of D in step 1 was too small.
Otherwise, solve the univariate equation in x1 (for example by using
Berlekamp’s algorithm, see for example Algorithm 14.31 in [100]).
4. Repeat: Substitute the values of x1 found in step 3 into the equations, simplify
them and run the algorithm again to ﬁnd the values of the other variables.
x3y + αx2y2 = βxy,
(6.11)
x2y2 + αxy3 = βy2,
(6.12)
x2y2 + γ x3y = δx2,
(6.13)
xy3 + γ x2y2 = δxy,
(6.14)
y4 + γ xy3 = δy2.
(6.15)
Gaussian elimination applied to these 6 equation in addition to the two original
equations yields the equation
β2 + x2
αβγ −α2δ −2α

+ x4(1 −αγ ) = 0.
Now the XL-algorithm proceeds to step 3 and ﬁnds candidates for x. Substituting
these into the original equation yields the corresponding values for y.
6.1.2.4 The XL-Algorithm and Gröbner Bases
The XL-algorithm has a very strong relation to Gröbner bases and especially to the
F4 algorithm for calculating Gröbner bases, which will be sketched in this subsec-
tion.
The idea of the F4 algorithm is to replace the polynomial operations in the Buch-
berger algorithm by matrix operations which allows the use of fast linear algorithms
and to perform several polynomial reductions in one step.
We do not discuss F4 in detail. In the outer loop you see that F4 loops as the
Buchberger algorithm over all pairs to check the Syzygien polynomials. The impor-
tant part of the F4 algorithm is the creation of the list L (which we simpliﬁed a bit
in Algorithm 6.5). The algorithm spends most of its time in line 12 (computation of

6.2
Pre-processing Techniques for Algebraic Attacks
147
Algorithm 6.5 F4 algorithm (simpliﬁed)
Require: I = ⟨F⟩
Ensure: G is a Gröbner basis of I with respect to ≺.
1: G ←F
2: Pairs ←{(f,g)|f,g ∈G,f ̸= g}
3: while Pairs ̸= 0 do
4:
Select a pair P = (fi,fj) from Pairs.
5:
Pairs ←Pairs\{P}
6:
cm ←lcm(lm(fi),lm(fj))
7:
Add all polynomials of the form mfi with lm(mfi)|cm to the list L.
8:
Add all polynomials of the form mfj with lm(mfj)|cm to the list L.
9:
Add all polynomials of the form mg with g ∈G and lm(mg)|cm to the list L.
10:
{the full F4 has a symbolic pre-processing to reduce the number of polyno-
mials generated in this step}
11:
Linearize the polynomials in L, i.e. interpret L as a matrix.
12:
¯L ←row echelon form of L with respect to ≺.
13:
¯L+ ←{f ∈¯L|lm(f ) /∈lm(L)}
14:
for h ∈¯L+ do
15:
Pairs ←Pairs ∪{(h,g)|g ∈G}
16:
G ←G ∪{h}
17:
end for
18: end while
the row echelon form). But this is just Gaussian elimination (or a clever variation of
it using methods from fast and sparse linear algebra).
As one can see, F4 and the XL-algorithm are both using linearization and Gaus-
sian elimination and there are only minor differences in way the matrix is con-
structed. In [9] the relation is studied in detail and the main result is that the XL-
algorithm does indeed calculate a Gröbner basis.
Thus the XL-algorithm is just another way to describe Gröbner bases. It is nev-
ertheless an interesting point of view which is worth knowing.
6.2 Pre-processing Techniques for Algebraic Attacks
6.2.1 Reducing the Degree
The use of pre-processing techniques can best be explained with an example. Sup-
pose we want to attack a simple combination of ﬁve LFSRs in which the output of
the combiner is the majority function, i.e. zt = f (s(1)
t
,s(2)
t
,s(3)
t
,s(4)
t
,s(5)
t
) with
f (x1,x2,x3,x4,x5) =

1
if at least three of the ﬁve input variables are 1
0
otherwise.

148
6
Algebraic Attacks
One can express f as a function of degree 4:
f (x1,x2,x3,x4,x5) = σ3 + σ4
where σi denotes the ith elementary symmetric function in x1,...,x5, i.e.
σ3 = x1x2x3 + x1x2x4 + x1x2x5 + x1x3x4 + x1x3x5 + x1x4x5
+ x2x3x4 + x2x3x5 + x2x4x5 + x3x4x5,
σ4 = x1x2x3x4 + x1x2x3x5 + x1x2x4x5 + x1x2x4x5 + x2x3x4x5.
Thus f is a function of degree 4. Suppose all LFSRs have length 20, then lineariza-
tion would result in a system of
5
3

203 +
5
4

204 = 880000 variables.
However, there is a much cheaper solution. Assume that we observe 0 as output.
Then we know that at least three of the ﬁve LFSRs have 0 as output. Hence at least
one of the ﬁrst three LFSRs must be 0, i.e. we know that s(1)
t
s(2)
t
s(3)
t
= 0. Applying
linearization to this equation gives a system of 203 = 8000 variables.
In the above example we found the low degree equation by an ad hoc argument.
In general we are searching for a polynomial g such that deg(fg) is low. Here
fg is taken in the ring of Boolean polynomials
R = F2[x1,...,xn]/
-
x2
1 −x1,...,x2
n −xn
.
.
Then f = 0 implies fg = 0 which can be used to lower the complexity. This is
exactly what happened in our example (with g = x1x2x3 and fg = g). To ﬁnd such
a g we can compute the Gröbner basis of ⟨f,x2
1 −x1,...,x2
n −xn⟩with respect to a
graded monomial order. Here we can make use of the fact that x2
1 −x1,...,x2
n −xn
are in the ideal. One consequence is that no exponent is larger than 1, which makes
it cheap to store such Boolean polynomials. Many implementations (for example
Magma) have efﬁcient Gröbner basis algorithms for this special case.
To use the time steps in which the output of the combiner is non-zero, we search
for a low degree polynomial g with gf = 0. Then f (s(1)
t
,...,s(n)
t
) = 1 implies
g(s(1)
t
,...,s(n)
t
) = 0 which we can use to mount an algebraic attack.
If we are lucky, we might ﬁnd two distinct low degree polynomials g and h with
gf = h. Then we get a low degree equation for both cases f = 0 and f ̸= 0.
The following theorem gives us an upper bound on the size of the equations
needed in an algebraic attack.
Theorem 6.14 (Theorem 6.0.1 in [64]) Let f be a Boolean function in n variables.
Then there exists a Boolean function g ̸= 0 of degree at most ⌈n/2⌉and a Boolean
function h of degree at most ⌈n/2⌉such that gf = h.
Proof Let A be the set of all monomials of degree at most ⌈n/2⌉and let B = Af .
Then |A| + |B| is 2⌈n/2⌉
i=0
n
i

= n
i=0
n
i

+

n
⌈n/2⌉

> 2n. Hence some linear de-
pendency exists between the Boolean functions in A and B. Since the monomials

6.2
Pre-processing Techniques for Algebraic Attacks
149
Fig. 6.1 A combiner with
memory
in A are linearly independent, the dependency must use elements of B, i.e. we get a
g ̸= 0 with gf = h.
□
The techniques described above appeared in 2003 under the name ‘fast algebraic
attacks’ [8, 62, 64]. Our presentation is based on [64].
When implementing these kinds of algebraic attack one has to be careful when
substituting the key stream into the new equations [124]. A naive approach can
easy result in a high complexity (sometimes even higher than the time needed for
the linearization itself). Techniques like fast Fourier transforms and the asymp-
totic fast Berlekamp-Massey algorithm (see Algorithm 2.2) help to avoid this prob-
lem.
A special case of degree decrease is provided by differential attacks. The operator
D : f (x1,x2,...,xn) →f (x1,x2,...,xn) + f (x1 + 1,x2,...,xn)
is the binary analog of the differential operator
∂
∂x1 . The practical use is that it re-
duces the degree. A natural point where it can be applied in a cryptographic at-
tack is at the key scheduling when the attacker observes two sessions which differ
only in the initialization vector bit x1; he can learn all that is needed to compute
Df (x1,x2,...,xn). The application of the differential operator in attacks is limited,
since designers of cryptosystems tend to choose the algebraic functions f so that
Df is still very complex. There are only very rare cases in which the differential
operator yields good equations and the more general degree reduction techniques
do not ﬁnd these equations automatically. A special case of a differential attack is
the cube-attack which we describe later in the example of a round reduced version
of Trivium (see Sect. 10.1).
6.2.2 Dealing with Combiners with Memory
Combiners with memory are an improvement on the simple non-linear combiners
discussed in Chap. 3. In Sect. 5.2.1 we presented E0 as an example of a combiner
with memory. In general, such combiners take the form indicated in Fig. 6.1.

150
6
Algebraic Attacks
In addition to k LFSRs, the combiner also has an internal memory of l bits. We
denote the output of the LFSRs by s(i)
t , (t ∈N, i ∈{1,...,k}). The internal state
at time step t is denoted by zt = (z(1)
t ,...,z(l)
t ). The output of the combiner with
memory is
xt = f

s(1)
t
,...,s(k)
t
,z(1)
t ,...,z(l)
t

.
The internal state is updated according to
z(i)
t+1 = gi

s(1)
t
,...,s(k)
t
,z(1)
t ,...,z(l)
t

(6.16)
for i = 1,...,l.
Linearization cannot be applied directly to combiners with memory. If one in-
troduce new variables for all monomials containing a z(i)
t
the number of variables
explodes, and if one uses Eq. (6.16) to express the z(i)
t
by the LFSR outputs and the
initial values z(i)
1 of the internal state, the degree of the equations explodes.
So in order to apply linearization to a combiner with memory we must search in
a pre-processing step for a relation which uses only the outputs xt of the combiner
and the outputs s(i)
t
of the LFSRs.
Once more, Gröbner bases are our tool of choice. Consider the ideal
I =
-
xt′ −f

s(1)
t′ ,...,s(k)
t′ ,z(1)
t′ ,...,z(l)
t′

,
z(j)
t′+1 −gj

s(1)
t′ ,...,s(k)
t′ ,z(1)
t′ ,...,z(l)
t′

,
x2
t′ −xt′,

s(i)
t′
2 −s(i)
t′ ,

z(j)
t′
2 −z(j)
t′
.
where t′ runs from t to t + δt, i runs from 1 to k and j runs from 1 to l. Again we
work in the ring of Boolean polynomials, which means that the ideal contains all
polynomials of the form x2 −x.
For the monomial ordering we divide the variables into three groups. Group 1
contains the variables z(j)
t′ , group 2 contains the variables s(i)
t′ and group 3 contains
the variables xt′. Between the groups we use a lexicographic order: Every mono-
mial containing a variable of group 1 is bigger than any monomial containing only
variables of group 2 and 3 and every monomial containing a variable of group 2 is
bigger than any monomial containing only variables of group 3.
Inside the groups we use a graded ordering, i.e. if two monomials contain both
variables of group 1 we compare the degree in the z(j)
t′
to decide which is smaller.
For our application it is not important how the graded ordering inside the groups
is reﬁned to a full order.
Now compute the Gröbner basis of I with respect to this order. The chosen mono-
mial order guarantees that the ﬁrst element of the Gröbner basis will not contain the
variables z(j)
t′
describing the internal state if possible, and that the degree in the vari-
ables s(i)
t′ describing the output of the LFSRs is as small as possible. This means if

6.3
Real World Examples
151
there is any relation between the LFSRs and the output of the combiner with mem-
ory that is suitable for an attack based on linearization, the Gröbner basis will ﬁnd
the best relation.
In Sect. 6.3.2 we will see how this kind of attack is used in practice.
One can prove that for δt large enough the above approach is always successful.
Theorem 6.15 (see [8] Theorem 1) For any combiner with memory that uses k
LFSRs as input and has a memory of size l, there exists a relation using only the
output of the LFSRs and the output of the combiner at l + 1 successive time steps.
This relation has degree at most ⌈k(l + 1)/2⌉in the k(l + 1) variables describing
the output of the LFSRs.
For l = 0, Theorem 6.15 degenerates to Theorem 6.14. Since Theorem 6.15
makes no use of the degree of the combination function or the feedback function
of the internal memory, the bound of Theorem 6.15 is in practice often much higher
than the real equations. This limits the application of Theorem 6.14.
6.3 Real World Examples
We close this chapter with two examples of an algebraic attack against real ciphers
(see also Sect. 8.2.2, which contains an algebraic attack against the GSM-protocol
and Sect. 10.1 which contains an algebraic attack against a reduced variant of Triv-
ium).
6.3.1 LILI-128
The cipher LILI-128 [72] was a submission for the NESSIE-Project (New European
Schemes for Signatures, Integrity, and Encryption). It was submitted in 2000 by the
ISRC (Information Security Research Centre). The name LILI-128 indicates that
this generator is only one of a family of ciphers with different key lengths.
The cipher was withdrawn when the ﬁrst security problems were reported and it
is now totally broken (see [132]).
We use it as example of an algebraic attack in a more complex setting.
LILI use the idea of irregularly clocked shift registers. (We will treat this class of
ciphers in the next chapter in detail.)
LILI consists of two LFSRs. The clock control LFSR Lc has length 39 and feed-
back polynomial x39 +x35 +x31 +x17 +x15 +x14 +x2 +1. At every time step, Lc
is clocked exactly once. Then one computes fc = 2x12 +x20 +1 (the computation is
done in Z). Then the data-generation LFSR Ld is clocked fc times. Ld is an LFSR
of size 89 with feedback polynomial x89+x83+x80+x55+x53+x42+x39+x +1.
A non-linear ﬁlter fd is applied to Ld to produce the output. Let z0,...,z88 be the
internal state of Ld. The output is

152
6
Algebraic Attacks
Fig. 6.2 The LILI-128 keystream generator
fd = z12 + z7 + z3 + z1 + z80z20 + z80z7 + z65z3 + z65z0
+ z44z1 + z44z0 + z30z20 + z80z65z12 + z80z65z7 + z80z65z3
+ z80z65z1 + z80z44z7 + z80z44z3 + z80z30z20
+ z80z30z12 + z80z30z7 + z65z44z20 + z65z44z3
+ z65z30z20 + z65z30z7 + z65z30z3 + z80z65z44z20
+ z80z65z44z7 + z80z65z44z3 + z80z65z44z0 + z80z65z30z20 + z80z65z30z7
+ z80z65z30z1 + z80z44z30z12 + z80z44z30z3 + z65z44z30z7 + z65z44z30z1
+ z65z30z20z12 + z65z30z20z7 + z80z65z44z30z7 + z80z65z44z30z3
+ z80z65z30z20z12 + z80z65z30z20z7 + z65z44z30z20z12 + z65z44z30z20z7
+ z80z65z44z30z20z12 + z80z65z44z30z20z7.
The designers of LILI claim that fd was chosen according to principles given
in [232] to provide optimal resistance against correlation attacks.
Figure 6.2 illustrates the algorithm.
We want to attack LILI-128 using the techniques from this chapter. First we must
get rid of the irregular clock control. In case of LILI-128 we have several ways to
achieve this.
• The size of the clock register is relative small. One can simply guess the 39 bits
of Lc. This adds a factor of 239 to the attack.
• In a pre-processing step you can compute for each of the 239 possible values of
the register Lc relations in the output. Storing the relations needs a lot space, but
is feasible. Then you check which relations are satisﬁed by the output LILI and
obtain the content of LC. So you can avoid the factor 239 during the attack. (See
also Sect. 8.2.2, where such a technique is applied to A5/2.)
• In 239 −1 time steps (one period of the LFSR Lc) the register Ld is clocked
exactly Δd = 5 · 238 −1 times. So if we can observe output bits with difference
Δd we can ignore the irregular clocking of Ld.

6.3
Real World Examples
153
Now we attack the linear ﬁlter generator build of Ld and fd. The degree of fd is
6 so simple linearization would give a system of more than
89
6

≈229.2 variables.
Let us try the pre-processing techniques of Sect. 6.2.1 to reduce the degree.
In case of LILI-128 we need no complicated algebraic techniques to ﬁnd lower
degree equations. Note the degree 5 and 6 terms of fd. They are divisible by z65 and
by z30 as a common factor, so fd(z65 + 1) and fd(z30 + 1) have at most degree 5.
(The degree 5 and 6 terms of f vanish and the degree 4 terms give new degree 5
terms.) One can iterate the trick with fd(z30 + 1) and ﬁnd that fd(z30 + 1)(z80 + 1)
has degree 4.
Using this equation for linearization we get a system of only 4
j=1
89
j

≈221.2
variables. Using fast linear algebra such as Strassen’s matrix multiplication (see
Sect. 12.5 or [100] for an overview) this results in a complexity of 260 for the alge-
braic attack. This may not seem great, but it is much better than the complete key
search (2128 operations) and was reason enough to abandon LILI-128.
We can also apply Gröbner basis techniques to the problem. Using the graded
lexicographic order we ﬁnd that the Gröbner basis of
I =
-
fd,z2
0 −1,z2
1 −1,...,z2
80 −1
.
contains 13 elements of degree 4.
As noted in [64] LILI-128 is extraordinarily weak against algebraic attacks.
A random function fd would have provided more resistance.
6.3.2 E0
We use once more the cipher E0 as an example (see Sect. 5.2.1).
Let us recall the structure of E0:
• E0 consists of 4 LFSRs.
• Additional Memory of E0 has size 4 bits ct = (c(1)
t
,c(0)
t
) and ct−1 = (c(1)
t−1,c(0)
t−1).
Thus, by Theorem 6.15 we know that there must be an equation using only 5
successive time steps and having degree at most 10. Since the combination function
and feedback polynomials of E0 are quite simple, we expect that Theorem 6.15
overestimates the complexity.
Recall the equations deﬁning E0:
zt+1 =
s(1)
t
+ s(2)
t
+ s(3)
t
+ s(4)
t
+ ct
2

,
ct+1 = zt+1 ⊕ct ⊕T (ct−1),
T

c(1),c(0)
=

c(0),c(0) + c(1)
,
xt = s(1)
t
⊕s(2)
t
⊕s(3)
t
⊕s(4)
t
⊕c(0)
t
.

154
6
Algebraic Attacks
Here s(i)
t
denotes the output of the LLSRs, ct the internal state and xt the output
of E0.
We write the equation deﬁning ct+1 as Boolean equations
c(1)
t+1 =

σ4(t) + σ3(t)c(0)
t
+ σ2(t)c(1)
t
 
+ c(1)
t
+ c(0)
t−1,
c(0)
t+1 =

σ2(t) + c(1)
t
+ c(0)
t
σ1(t)
 
+ c(0)
t
+ c(1)
t−1 + c(0)
t−1
where σi(t) denotes the elementary symmetric function of degree i in the variables
s(1)
t
, s(2)
t
, s(3)
t
, s(4)
t
.
Now we apply the Gröbner basis technique described in Sect. 6.2.2 to ﬁnd re-
lations between the output of the LFSRs and the output of E0. The Gröbner basis
gives us the following relation of degree 4 combining 4 successive time steps.
1 = xt + xt+1 + xt+2 + xt+3
+ σ1(t + 1)

(xt+1 + 1)xt+3 + (xt+1 + 1)xt+2 + (xt+1 + 1)xt + 1

+ σ2(t + 1)(1 + xt + xt+1 + xt+2 + xt+2) + σ3(t + 1)xt+1 + σ4(t + 1)
+ σ1(t) + σ1(t)σ1(t + 1)(1 + xt+1) + σ1(t)σ2(t + 1) + σ1(t + 2)xt+2
+ σ1(t + 2)σ1(t + 1)xt+2(1 + xt+1) + σ1(t + 2)σ2(t + 1)xt+2
+ σ2(t + 2) + σ2(t + 2)σ1(t + 1)(1 + xt+1) + σ2(t + 2)σ2(t + 1)
+ σ1(t + 3) + σ1(t + 3)σ1(t + 1)(1 + xt+1) + σ1(t + 3)σ2(t).
Using this equation as a basis for an algebraic attack, linearization produces a
system of ≈223 variables. With fast linear algebra it takes only ≈265 operations
to solve such a system. The applicability of this attack is limited, since the Blue-
tooth protocol does not allow us to observe 223 variables. E0 still provides enough
security.

Chapter 7
Irregular Clocked Shift Registers
In the previous chapters we have studied non-linear combinations of linear feedback
shift registers. Another way to generate a sequence with high linear complexity
from an LFSR is to use an irregular clocking. An advantage of such a mechanism
is that it avoids most algebraic attacks and thus allows the use of smaller LFSRs.
A disadvantage is that these constructions are harder to analyze, so there is a risk
that an attacker might ﬁnd a trick we have missed. In addition, the speed of irregular
clocked generators is lower than the speed of regular clocked ciphers and we risk
side channel attacks.
7.1 The Stop-and-Go Generator and the Step-Once-Twice
Generator
We begin our analysis with two simple generators.
The stop-and-go generator [25] consists of two LFSRs. The ﬁrst LFSR is regular
clocked and its output is the clock control for the second LFSR, i.e. R2 is clocked
at time step t if R1 outputs 1 at this time step. The output of the stop-and-go gen-
erator is the output of R2. It differs from a simple linear shift register sequence by
duplicating some outputs. Figure 7.1 illustrates the construction.
The stop-and-go generator is not suitable for cryptography. First, it has poor sta-
tistical properties. For example, two consecutive bits of the stop-and-go generator
are equal if either
• the output of R1 at this time step is 0 and hence R2 is not clocked (probability
1/2); or
• the output of R1 is 1, i.e. R2 is clocked but the two consecutive outputs of R2 are
equal (probability 1/4).
Thus the bigram aa occurs in the output of the stop-and-go generator with probabil-
ity 3/4, which is much higher than the probability 1/2 for a sequence of independent
and uniformly distributed bits.
A. Klein, Stream Ciphers, DOI 10.1007/978-1-4471-5079-4_7,
© Springer-Verlag London 2013
155

156
7
Irregular Clocked Shift Registers
Fig. 7.1 The stop-and-go
generator
Another problem is that every switch from 1 to 0 and vice versa indicates that the
output of R1 has to be 1. Thus an attacker can reconstruct without difﬁculty many
outputs of R1, i.e. he can recover the initial seed of R1. As soon as the clock control
is known, the problem of breaking the stop-and-go generator is equivalent to the
problem of breaking a linear shift register.
This is a typical problem for all irregular clocked stream ciphers. As soon as
the clocking mechanism is known they become very weak ciphers. So one generic
attack is to guess the initial state of the LFSR that controls the clock. Therefore it is
important that this LFSR must be large enough to make this attack impractical (see
also the attack against A5/2 in Sect. 8.2.2).
The stop-and-go generator is the basis for the better alternating step generator,
thus it is of interest to ﬁnd its linear complexity.
Theorem 7.1 Consider a stop-and-go generator with the following properties:
• The control sequence is a de Bruijn sequence (κi)i∈N of period 2k.
• The generator sequence is an m-sequence (μi)i∈N with minimal polynomial p.
Let m = deg(p) and let M = 2m −1 denote the period of the m-sequence.
Then the period of the stop-and-go generator is 2kM and the sequence generated
by the stop-and-go generator has minimal polynomial pj for some j with 2k−1 <
j ≤2k.
Proof Let the coefﬁcients of p be denoted by pi, i.e. p(x) = m
i=0 pixi.
Let ft = t
j=0 κj (the summation is done in N) denote the number of times the
generator sequence is clocked at time step t. The output of the stop-and-go generator
at time step t is μf (t).
Let s be the period of the stop-and-go generator, i.e. μf (t+s) = μf (t) for all t ∈N.
This implies
f (t + s) ≡f (t)
mod M
(7.1)
for all t ∈N. Subtracting the equation f (t −1 + s) ≡f (t −1) mod M we get
κt = κt+s. Thus s = x2k. Since exactly half of the elements in a de Bruijn sequence
are equal to 1, we obtain ft+s = x2k−1 + ft. Together with gcd(2k−1,M) = 1,
Eq. (7.1) implies x = M, i.e. s = 2kM.
Since μ is an m-sequence, it has the constancy on cyclotomic cosets property
(see Theorem 2.4), i.e. p is not only the minimal polynomial of (μi)i∈N but also of
(μi2k−1)i∈N.

7.2
The Alternating Step Generator
157
We claim that p(x2k) = p(x)2k is a feedback polynomial of the sequence gener-
ated by the stop-and-go generator, i.e. we claim m
i=0 piμft+i2k = 0 for all t ∈N.
m

i=0
piμft+i2k =
m

i=0
piμft+i2k−1

2k is a full period of the de Bruijn sequence

,
=
m

i=0
piμft+τ
(τ exists by the constancy on cyclotomic cosets property),
= 0
(since p is the feedback polynomial of the LFSR sequence μ).
Since p is irreducible this proves that the minimal polynomial of the stop-and-go
generator sequence is p(x)j for some j ≤2k.
Assume that j ≤2k−1. Then p(x)j|(xM −1)j|(xM −1)2k−1|xM2k−1 −1, i.e.
the stop-and-go generator sequence has period 2k−1M, a contradiction as we have
already proved that the period is 2kM.
□
Setting p(x) = x −1 in the proof of Theorem 7.1 we obtain that the linear com-
plexity of a de Bruijn sequence lies between 2k−1 and 2k. Theorem 7.1 is just a
variation of the theorem on de Bruijn sequences given in [49].
The stop-and-go generator shows that duplicating bits in an LFSR sequence leads
to problems. A better approach is to delete bits. One variant is the step-once-twice
generator. It consists of one control register and one generation register. If the output
of the control register is 0 the generation register is stepped once and if the output
of the control register is 1 the generation register is stepped twice.
Thus, on average, every third bit of the generation sequence is deleted. A practical
consequence is that the encryption speed decreases. An analysis of the step-once-
twice generator can be found in [272]. We will skip it here and concentrate instead
on the better alternating step generator and shrinking generator.
7.2 The Alternating Step Generator
The alternating step generator [120] is an improvement of the stop-and-go generator.
It consists of three LFSRs. The register R1 controls the clock and decides which of
the registers R2 and R3 is clocked (see Algorithm 7.1 and Fig. 7.2).
The alternating step generator preserves the higher speed of the stop-and-go
generator in comparison to the step-once-twice generator, but avoids the statistical
weakness of the stop-and-go generator.

158
7
Irregular Clocked Shift Registers
Algorithm 7.1 The alternating step generator
clock the register R1
if the output of R1 is 1 then
clock the register R2 and repeat the previous output of R3
else
clock the register R3 and repeat the previous output of R2
end if
add the output of R2 and R3 and return it as the result
Fig. 7.2 The alternating step
generator
The alternating step generator is normally described as a sum of two stop-and-
go generators with complementary control sequences. This has the advantage that
many properties follow directly from the stop-and-go generator. For example, The-
orem 7.1 immediately translates to the alternating step generator (see also [120]).
For some applications the following, which describes the alternating step gener-
ator as two interleaved LFSR sequences, is a better approach:
Let (wi)i∈N, (xi)i∈N and (yi)i∈N be the output of the LFSRs L1, L2 and L3,
respectively. Let τn = n
i=1 wi be the number of 1s that LFSR L1 produces in the
ﬁrst n steps. Then the shrinking generator sequence is zn = xτn + yn−τn.
Consider the linear transformation ˆzn = zn + zn+1. For cryptographic purposes
the sequence (ˆzn)n∈N is as good as the original sequence (zn)n∈N.
Since either τn+1 = τn (if wn+1 = 0) or tn+1 = tn + 1 (if wn+1 = 1) we get
ˆzn =

xτn + xτn+1
if wn+1 = 1,
yn−τn + yn−τn+1
if wn+1 = 0.
If (xi)i∈N is an LFSR sequence then ˆxi = xi + xi+1 is also an LFSR sequence with
the same feedback polynomial.
Thus the sequence (ˆzi) is generated by Algorithm 7.2. The linear feedback shift
registers L1, L2 and L3 have the same feedback polynomial as the register used in
Algorithm 7.1 but the initial values are changed.
7.3 The Shrinking Generator
The shrinking generator is a remarkably simple cipher which has now been unbro-
ken for more than 15 years.

7.3
The Shrinking Generator
159
Algorithm 7.2 The alternating step generator (alternative form)
clock the register R1
if the output of R1 is 1 then
clock the register R2 and output its result
else
clock the register R3 and output its result
end if
Fig. 7.3 The shrinking
generator
7.3.1 Description of the Cipher
The shrinking generator [58] is a variation of the step-once-twice generator. As for
the step-once-twice generator, its output is a subsequence of an LFSR sequence.
The difference is that the shrinking generator can delete several successive bits. The
rule is if the ith bit of the control sequence is 1, then the ith bit of the generator
sequence become a part of the output. If the ith bit of the control sequence is 0, then
the corresponding bit of the generator sequence is deleted. Figure 7.3 illustrates the
shrinking generator.
The advantage in comparison to the step-once-twice generator is that the deletion
points are less regular, which make correlation attacks harder. A disadvantage is that
the generation speed is variable, which makes it necessary to use a buffer to prevent
side channel attacks.
7.3.2 Linear Complexity of the Shrinking Generator
The shrinking generator has a large period and a high linear complexity, as the
following theorem shows. The method of proof is quite similar to Theorem 7.1.
Theorem 7.2 Replace the LFSR sequence generated by R1 by a de Bruijn sequence
(ci)i∈N of period 2c.
Assume that R2 has a primitive feedback polynomial p and the period of the
sequence X = (xi)i∈N generated by R2 is 2n −1.
Then the shrinking generator has period 2c−1(2n −1) and its minimal feedback
polynomial is pj for some j with 2c−2 < j < 2c−1.
Proof Let Y = (yi)i∈N be the output sequence of the shrinking generator.

160
7
Irregular Clocked Shift Registers
Let τi denote the ith occurrence of a 1 in the control sequence (ci)i∈N. Consider
a full period of the de Bruijn sequence. In these 2c steps the shrinking generator
produces 2c−1 output bits, i.e. τi+2c−1 = τi + 2c or yi+2c−1 = xτi+2c.
Since the period of R2 is 2n −1 and gcd(2c,2n −1) = 1 it follows that the cipher
ﬁrst repeats after 2c(2n −1) time steps, in which it generates 2c−1(2n −1) output
bits.
Since the feedback polynomial p of R2 is primitive, (ci)i∈N has the constancy
on cyclotomic cosets property (see Theorem 2.4), i.e. p is not only the minimal
polynomial of (xi)i∈N but also of (xi2c)i∈N.
Next we prove that p(x2c−1) = p(x)2c−1 is a feedback polynomial of the se-
quence generated by the shrinking generator. Let p = n
i=0 pixi then:
n

i=0
piyj+2c−1i =
m

i=0
pixτj +i2c

a full period of the de Bruijn sequence results in 2c−1
output bits

,
=
m

i=0
pixτ ′
j

τ ′
j exists by the constancy on cyclotomic cosets property

,
= 0
(since p is the feedback polynomial of the LFSR R2).
Since p is irreducible, this proves that the minimal polynomial of the shrinking
generator sequence is p(x)j for some j ≤2c−1.
Assume that j ≤2c−2. Then
p(x)j
x2n−1 −1
j
x2n−1 −1
2c−2x(2n−1)2c−2 −1,
i.e. the shrinking generator sequence has period (2n −1)2c−2, a contradiction as we
have already proved that the period is (2n −1)2c−1.
□
Note that the proof for the period length does not make full use of the properties
of the de Bruijn sequence. For the maximal period it is only necessary that the
periods of R1 and R2 are relatively prime. Since the linear complexity is much
lower than the period and an m-sequence differs from a de Bruijn sequence only
if one looks at a full period, we have also proved a lower bound for the shrinking
generator with a control sequence generated by an LFSR R1 with primitive feedback
polynomial.

7.3
The Shrinking Generator
161
7.3.3 Correlation Attacks Against the Shrinking Generator
As already mentioned at the beginning of this section, shrinking generators with
reasonably large registers are still unbroken. One of the strongest known attacks
against shrinking generators is a correlation attack found by Golic [113].
Let X = (xi)i∈N be the generator sequence, C = (ci)i∈N be the control sequence
and Y = (yi)i∈N be the output sequence of a shrinking generator. By Xn we denote
the subsequence of the ﬁrst n bits. By Xn
m we denote the subsequence xm,...,xn of
n −m + 1 bits.
For a correlation attack against the generator sequence the attacker must compute
the a posteriori probability ˆpi = P(xi = 1 | Y n).
In our model we replace the control and generator sequence by iid. random vari-
ables with p(xi = 1) = p(ci = 1) = 1
2. The output sequence is also a sequence of
iid. random variables.
Note that the ﬁrst i output bits of the generator sequence can at most inﬂuence
the ﬁrst i output bits of the shrinking generator. Thus
ˆpi = P

xi = 1 | Y n
= P

xi = 1 | Y i
.
By Bayes’ theorem
ˆpi = P(xi = 1)P(Y i | xi = 1)
P(Y i)
= 2i−1P

Y i | xi = 1

.
(7.2)
The problem is to compute the probability P(Y i | xi = 1). We start by distin-
guishing the cases w(Ci) = j for j = 0,...,i. This gives:
P

Y i | xi = 1

=
i

j=0
P

Y i,w

Ci
= j | xi = 1

,
=
i

j=0
P

Y i
j+1 | Y j,w

Cn
= j,xi = 1

,
P

Y j,w

Cn
= j | xi = 1

.
If the weight of Ci is j, the output Y i
j+1 is computed from the tails Ci+1 =
(ck)k≥i+1 and Yi+1 = (yk)k≥i+1. Thus we can drop the condition in the ﬁrst proba-
bility.
P

Y i | xi = 1

=
n

j=0
P

Y i
j+1

P

Y j,w

Cn
= j | xi = 1

=
i

j=0
2i−jP

Y j,w

Cn
= j | xi = 1

.

162
7
Irregular Clocked Shift Registers
When computing the probability P(Y j,w(Ci) = j | xi = 1) we must distinguish
two cases:
• In the ﬁrst case, ci = 0. In this case the ith output of the generator sequence is
deleted. Thus it has no inﬂuence at all, whether it is 1 or 0. Thus
P

Y j,w

Ci
= j,ci = 0 | xi = 1

= P(ci = 0)P

Y j
P

w

Ci−1
= j

= 1
2 · 1
2j ·
i−1
w

2i−1 .
• The second case is ci = 1. In that case the ith bit xi of the generator sequence
becomes a part of the output sequence. Since w(Ci) = j we have yj = xi = 1.
Thus
P

Y j,w

Ci
= j,ci = 1 | xi = 1

= P(ci = 1)P

Y j,yj = 1

P

w

Ci−1
= j −1

= 1
2 · yj
2j ·
 i−1
w−1

2i−1 .
Plugging everything into Eq. (7.2) we get
ˆpi = 21−i
i

j=0
i −1
j

+ 2
i −1
j −1

yj
= 1
4 + 2−i
i

j=1
i −1
j −1

yj
(7.3)
which is Theorem 2 in [113].
Next we want to estimate
 ˆpi −1
2
 = 2−i

i

j=1
i −1
j −1

yj −1
2
.
(7.4)
The factor
i−1
j−1

increases quickly for j ≤(i + 1)/2 and decreases quickly for j ≥
(i + 1)/2. Thus only the middle terms of the sum make a signiﬁcant contribution.
By Stirling’s formula n! ≈
√
2πnnne−n we can estimate the size of the binomial
coefﬁcients. For k ∈[n/2 −√n,n/2 + √n] we get
n
k

= Θ
 2n
√n

.

7.4
Side Channel Attacks
163
So we may continue from Eq. (7.4) by
 ˆpi −1
2
 ≈c2−i

i/2+
√
i

j=i−
√
i
2i
√
i

yj −1
2

≈c
√
i

i/2+
√
i

j=i−
√
i
yj −1
2
.
Thus the problem is reduced to the average absolute difference of a binomial vari-
able and its mean. By the central limit theorem we may replace the binomial distri-
bution by a normal distribution and get

i/2+
√
i

j=i−
√
i

yj −1
2
 = Θ
 4√nE
N(0,1)

= Θ
 4√
i

and thus
 ˆpi −1
2
 = Θ
 1
√
i

Θ
 4√
i

= Θ
 1
4√
i

,
(7.5)
i.e. the correlation between the shrinking generator sequence and its LFSR generator
sequence decreases with
4√
i. The constant hidden in the Θ-term of Eq. (7.5) is about
0.15 (see [113]). We can expect to use 10000 to 100000 bits in a correlation attack.
The correlation is very weak and for reasonable large LFSRs (about 128 bits)
the shrinking generator is still secure, but the analysis shows that attacking irregular
clocked stream ciphers is not impossible. Advanced versions of this attack can use
probabilities of bigrams and so on.
7.4 Side Channel Attacks
Even if a cipher is mathematically secure, the attacker can still hope to succeed.
He can try to measure the time used for encryption or the energy consumed by the
chip and use this measurement to draw conclusions on the performed calculations.
Attacks of this kind are called side channel attacks. Of all stream ciphers, the ir-
regular clocked shift registers are most vulnerable to side channel attacks, so it is
appropriate to discuss this type of attack in this chapter.
A Simple Power Analysis (SPA) is possible when some operations are directly
dependent on the value of the key. The classical example is the implementation of
the square and multiply algorithm for computing xe mod N. If e = (ek−1 ...e0)2,
the i-th step requires a square operation and a multiplication if ei = 1, but only
one square operation if ei = 0. If the implementation takes no precaution, one can
determine ei directly from the consumed energy.

164
7
Irregular Clocked Shift Registers
Irregularly clocked stream ciphers are highly vulnerable against simple power
analysis and timing attacks. For example, in a straightforward implementation of
the shrinking generator, the time needed to generate a bit is proportional to the
number of consecutive 0s in the control sequence. Every implementation must take
countermeasures such as inserting dummy operations, using caching to hide the
single steps, etc. Normally a simple power analysis can be prevented effectively by
such countermeasures. The price for using such countermeasures is always extra
gates. One should never forget this when comparing the relatively small size of
irregular clocked ciphers with the larger regular clocked ciphers.
The advanced Differential Power Analysis (DPA) [162] exploits the fact that the
power consumption of an operation is inﬂuenced by the value of the manipulated
bits. This inﬂuence is too small to be measured directly, so differential power anal-
ysis needs to measure several thousand encryptions. The general form of the attack
is as follows: Assume that we can measure for several encryptions the power used
for the encryption. The attacker guesses a part of the key K′. Under the assumption
that the guess is correct he can separate the encryptions into two sets S0 and S1.
Each of the encryptions in S0 must calculate f (0) and each encryption in S2 must
calculate f (1) for some simple function f . All other operations distribute equally
over the sets S0 and S1. If the power used in the calculation of f (0) is different from
the power used for f (1) then
Δ = power(S0)
|S0|
−power(S1)
|S1|
≈power

f (0)

−power

f (1)

will differ signiﬁcantly from 0. If the guess K′ is incorrect, then all operations will
distribute randomly over the sets S0 and S1 and we will get Δ ≈0. This allows us
to deduce whether the guess K′ was correct. The art of differential power analysis
is to identify good functions f and determine the separation of the encryptions into
S0 and S1.
The design of a synchronous stream cipher provides a high security against a
differential power analysis. The key stream generation is separated from the encryp-
tion, which is only a simple XOR-operation. Thus it is nearly impossible to collect
the several thousands of measurements needed for a differential power analysis.
With respect to differential power analysis, the weakest point is the initialization
of the stream cipher. It is quite common for stream ciphers to frequently reseed (one
reason for this is to make encryption and decryption parallelizable and to add syn-
chronization points). In this case the key scheduling runs several times with the same
key and different initialization vectors. Running an algorithm several times with the
same key and different inputs is exactly what is needed for a differential power anal-
ysis. In his PhD thesis [167] J. Lano shows for the examples of E0 (see Sect. 6.3.2)
and A5/1 (see Sect. 8.3) how differential power analysis might be applied to the key
scheduling of stream ciphers.
Power analysis is the most important side channel attack. Other side channel
attacks use the cache and the branch prediction unit of modern processors (see, for

7.4
Side Channel Attacks
165
example, [22]). Since stream ciphers are mostly used on dedicated hardware or on
simple embedded processors, this class of side channel attack is less important for
stream ciphers.
In fault analysis (see [129]) it is assumed that the attacker can insert faults in
the execution of a cipher (e.g., always setting a bit to 0 or skipping an instruction).
Although these attack are not impossible, they are less practical.

Part II
Some Special Ciphers

Chapter 8
The Security of Mobile Phones (GSM)
8.1 The GSM Protocol
GSM is the most widely used protocol for mobile phones. There are three types
of cryptography used in the protocol: a stream cipher named A5 that is used to
encrypt the data; an authentication code named A3; and a key agreement algorithm
named A8. Figure 8.1 shows how the algorithms interact.
First the central computer chooses a random number. The SIM card of the mo-
bile phone computes a MAC from its key and this random number using the Algo-
rithm A3. The central computer, which also knows the key of the SIM card, checks
if the received MAC is correct. This procedure ensures that the phone company
knows that they are communicating with the right mobile phone.
The key generation algorithm A8 takes the same random vector to compute a
key for the current session. The data itself is secured by the A5 stream cipher, which
exists in two variants: the “strong” version A5/1, which has export limitations; and
the weaker export version A5/2. The newer UMTS protocol uses the block cipher
KASUMI, which is sometimes called A5/3. As indicated in Fig. 8.1 the protocol
adds an error-correcting code to the message before it gets encrypted. This is a
serious protocol ﬂaw (compare with Algorithm 1.1). As we will see in the next
sections, in the case of the GSM protocol, this ﬂaw is fatal.
All ciphers were initially kept secret. It is never a good idea to violate Kerckhoffs’
principle and try to ensure the security of a system by keeping the cipher secret. The
GSM protocol shows once more that this leads only to a weak system.
The design of A3 and A8 is not speciﬁed by the GSM protocol and can be freely
chosen by the telephone company. Most companies choose the example, called
COMP128, given in the GSM speciﬁcation. The speciﬁcation was never ofﬁcially
published, but in 1988 it was leaked to the public [36]. Immediately afterwards the
ciphers were broken, allowing the cloning of a GSM smart card [37].
Both variants of the A5 algorithm were reverse engineered in 1999 by M. Briceno
[38]. The ﬁrst attacks were published immediately afterwards. In the following sec-
tion we will describe the ciphers and the attacks against them.
A. Klein, Stream Ciphers, DOI 10.1007/978-1-4471-5079-4_8,
© Springer-Verlag London 2013
169

170
8
The Security of Mobile Phones (GSM)
Fig. 8.1 Outline of the GSM protocol
8.2 A5/2
8.2.1 Description of A5/2
The export version A5/2 of A5 consists of four linear feedback shift registers of
length 19, 22, 23 and 17. The registers R1, R2 and R3 are used with a non-linear
ﬁlter generator and the register R4 controls the clock for the ﬁrst three registers.
Figure 8.2 illustrates A5/2. As you can see in the ﬁgure, the registers have feed-
back polynomials f1 = z19 + z5 + z2 + z + 1, f2 = z22 + z + 1, f3 = z23 + z15 +
z2 + z + 1 and f4 = z17 + z5 + 1, respectively.
In each time step the following happens:
• The bits R4[6], R4[13] and R4[9] form the input of the clocking unit, which
computes the majority function of the three bits. The register R1 is clocked if
R4[6] agrees with the majority. Similarly R2 and R3 are clocked if R4[13] or
R4[9] agrees with the majority, respectively.
• A majority function is applied as a non-linear ﬁlter to each of the ﬁrst three regis-
ters. For register R1 we compute maj(R1[3],R1[4],R1[6]), for register R2 the ﬁl-
ter is maj(R2[5],R2[8],R2[12]) and for R3 the ﬁlter is maj(R3[4],R3[6],R3[9]).

8.2
A5/2
171
Fig. 8.2 Diagram of A5/2

172
8
The Security of Mobile Phones (GSM)
• The XOR of the outputs of the three non-linear ﬁlters and the outputs of the three
registers R1, R2 and R3 is the output bit of A5/2.
• The register R4 is clocked.
A5/2 has numerous design ﬂaws:
• First the total state consists of just 81 bits, which is hardly enough to prevent
a brute force attack. In any modern application we should use at least 128 bit
security to be on the safe side.
• The register R4 which controls the clocking unit is, with just 17 bits, very small.
An attacker can simply iterate over all possible states of R4 and attack just the
weak non-linear combiners.
• The non-linear combiners of three registers R1, R2 and R3 are of degree 2. This
gives an algebraic complexity of just
19
2

+ 19 +
22
2

+ 22 +
23
2

+ 23 = 702.
(Note that if you put the 19 + 22 + 23 = 64 bits of the registers R1, R2 and R3
into a single LFSR, a quadratic ﬁlter would achieve a higher algebraic complexity
(
64
2

+ 64 = 2080)!)
Before we can describe the attack on A5/2 we must describe the initialization
process and the operation mode.
The operation mode is of particular interest. It has two points relevant for the
attack. The ﬁrst point is that after 228 output bits, A5/2 is reseeded with a new ini-
tialization vector. This is a good idea, it makes it impossible for the attacker to gather
enough material for an algebraic attack before A5/2 is reseeded. As a consequence,
we must analyze data from different frames in one attack, which introduces some
complications.
The second important point is that GSM uses error-correcting codes in addition to
the encryption, and that the error-correcting code is applied before the encryption.
This is a terribly bad idea. It adds redundancy to the plaintext, which converts a
known plaintext attack into a ciphertext-only attack.
To complete the description of A5/2 we show how the registers are initialized
(see Algorithm 8.2).
8.2.2 An Instance of a Ciphertext-Only Attack
The best attack against A5/2 is due to E. Barkan, E. Biham and N. Keller [15] and
breaks A5/2 secured communication in real time. We will develop their attack step
by step.
For the moment we ignore the operation mode and the initialization process of
A5/2 used in the GSM protocol. We simply assume that we can observe enough
output bits to reconstruct the initial state.
As we have seen in Chap. 7, the simplest attack against any LFSR-based cipher
with irregular clock control is to guess the state of the clock register and then attack
the resulting (weak) cipher.
In this special case the clock register has only 19 bits (18 bits if we take into
account that R4[6] is set to 1 in the GSM protocol), which makes this kind of attack
fast enough in practice.

8.2
A5/2
173
After guessing the register R4 we are left with a linear combination of three non-
linear combiners. Each combiner has algebraic degree 2, so linearization gives us a
system of linear equations with
19
2

+ 19 +
22
2

+ 22 +
23
2

+ 23 = 702 variables.
(This number reduces to 656 if we take into account that one bit of each shift regis-
ter is set to 1 in the initialization phase of the GSM protocol, i.e. it is known to the
attacker.) We can solve the linear system either with the normal Gaussian algorithm
or with the XL-Algorithm (see Sect. 6.1.2.3). In the estimation of the complexity
of the attack we will use the Gaussian algorithm. To solve a system of n variables
with the Gaussian algorithm we need about 1
2n3 operations. So the attack needs
219( 7023
2 ) ≈246 bit operations, if we take into account that a word operation per-
forms many bit operations in parallel and there are about 241 word operations on a
standard computer. This is already fast, but the attack still takes several minutes. It
needs only 7022 bits of memory (less than 2 kB) to store the system of linear equa-
tions and we need to observe less than 800 bits to obtain enough equations. Note
that in the GSM protocol the cipher is reseeded after only 228 bits, so obtaining the
data is still a problem. We solve this problem later in this section.
Our next goal is to speed up the attack. The idea is that we should be able to iden-
tify a wrong guess of the register R4 without solving a system of linear equations.
More precisely we can move the time needed to solve the system of linear equations
into a pre-processing step.
The trick is that can we express for each possible initialization value of R4 the
output sequence of A5/2 by quadratic functions in the initial values of the registers
R1, R2 and R3. By linearization we transform this into linear expressions in 702
variables and with the Gaussian algorithm we can ﬁnd linear relations satisﬁed by
the output sequence. For each possible initialization value of R4 we store about 30
of these relations. This is feasible since each relation connects only 1000 bits, so we
must store only 30 · 1000 · 219 bits (≈1.8 GB).
In the attack we can loop over all initial values of the register R4 and check if
the observed sequence satisﬁes the stored relations. If we guess the correct initial
value of the register R4 all pre-computed relations are satisﬁed. For a wrong guess
each relation is satisﬁed only with probability 1
2. Thus we must check on average
i2−i = 2 relations to identify a wrong guess. To check a relation we must just
evaluate the bitwise-and-function for bit ﬁelds of less than 1000 bits and then per-
form a sideway addition mod 2 (see Algorithm 2.5). The checks are very fast.
By this method we identify the right initial value for the register R4. The initial
values for the remaining registers are found by solving a system of 702 linear equa-
tions as in the basic version of the attack. Solving the system of linear equations is
approximately as expensive as the search over all 219 initial states of R4. The new
attack needs less than a second on a standard computer.
Now we go back to the GSM protocol. The problem with the attack we have so
far is that we need about 800 bits from the output sequence. In the GSM protocol
the cipher is reseeded after only 228 output bits, so we need data from at least 4
frames for the attack.
First note that for an algebraic attack it does not matter that the ﬁrst 99 output bits
of the cipher are discarded (step 5 in Algorithm 8.1). Only the number of observed

174
8
The Security of Mobile Phones (GSM)
Algorithm 8.1 A5/2 initialization
1. Set all registers to zero. Now clock every register 64 times and in each step add
one bit of the key (from the least signiﬁcant to the most signiﬁcant bit) to the
most signiﬁcant bit of the registers.
2. Clock every register for 22 additional cycles and this time add the bits of the
initialization vector to the most signiﬁcant bit of the register. The initialization
vector is increased by one in the next frame.
3. Set the bits R1[3], R2[5], R3[4] and R4[6] to 1.
4. Run the cipher for 99 steps and discard the output.
bits is important. Discarding the ﬁrst output bits does not defend against algebraic
attacks, but against attacks utilizing a weak key scheduling. RC4 (see Chap. 9) is a
famous example of a cipher that is strong against algebraic attacks, but which has
a weak key scheduling. In such cases discarding the ﬁrst output bits is essential for
the security of the system.
Note also that if we obtain the initial state of the shift registers (the state after
step 4 of Algorithm 8.1), we can guess the old values of the bits that were set to 1
in step 3. Since we know the initialization vector (the frame number) we can invert
step 2.
The states of the registers after step 1 of Algorithm 8.1 depend only on the main
key. If we want, we can interpret the state after step 1 of Algorithm 8.1 as the main
key. This would be enough to predict the cipher in the future, but retrieving the
original key from this state is also no problem. All operations in step 1 are linear, so
we need only solve a system of 64 linear equations.
Now let us deal with the problem of using four frames in one algebraic attack. In
the simplest version we just guess the state of the register R4 after the ﬁrst step of
Algorithm 8.1. Then we can simulate steps 2 to 4 of Algorithm 8.1 for all frames in
question and generate the equations for the algebraic attack.
For the advanced form of the attack with the pre-computed relations note that, if
R4(f ) and R4(f ′) are the states of the register R4 after step 2 of Algorithm 8.1, the
value R4(f ) ⊕R4(f ′) depends only on f ⊕f ′ and not on the unknown key.
If f is divisible by 4, i.e. the two least signiﬁcant bits of f are zero, we have
f + 1 = f ⊕1, f + 2 = f ⊕2 and f + 3 = f ⊕3.
This allows us to transfer the time-memory trade-off to the GSM protocol. What
we do is the following: For all possible values R4(f ) of the register R4 after step 2
of Algorithm 8.1 we can compute R4(f + 1), R4(f + 2) and R4(f + 3) under the
condition that f is divisible by 4. Once we know the values R4(f ), R4(f + 1),
R4(f + 2) and R4(f + 3) we can simulate the four frames and search, using the
Gaussian algorithm, for relations in the output bits. This can be done in the pre-
processing phase.
In the attack we have to wait until we get the output from four frames f , f + 1,
f + 2 and f + 3 with f is divisible by 4. Then we can loop over all possible values
R4(f ) and check if the pre-computed relations are satisﬁed. The test is fast and once
we know R4(f ) we need only solve a system of 656 linear equations to reconstruct

8.2
A5/2
175
the initial states of the registers R1, R2 and R3. All computations can be done in
less than one second.
There are some variations of the attack. Numbers divisible by 4 are not the only
numbers for which we can compute the XOR of f , f + 1, f + 2 and f + 3. For
example if f ≡1 mod 8 we have f + 1 = f ⊕3, f + 2 = f ⊕2, f + 3 = f ⊕5. If
we want we can also pre-compute relations for these cases. This increases the time
needed in the pre-computation phase and the disk space needed to store the data.
The small beneﬁt is that we don’t have to wait for a frame number divisible by 4.
Another variation is that we can assume that f is divisible by 8. Then we can
compute R4(f + 4) from R4(f ) and use the relation between the output in the
frames f + 4, f + 5, f + 6 and f + 7 in addition to the relations we have the basic
version. This doubles the number of available relations, which allows us to store
fewer relations per possible value of R4(f ). However, disk space is cheap, so this
variant is of less interest.
8.2.3 Other Attacks Against A5/2
The attack presented in the previous subsection is so good that there is no need to
look at other attacks. We will describe only one other attack. A5/2 was originally
kept secret and the reverse engineered algorithm was presented at Crypto 1999. At
the same conference, Ian Goldberg, David Wagner and Lucky Green [107] presented
an attack against the A5/2. It took them less than ﬁve hours to ﬁnd this attack. In the
following we will sketch this ﬁrst attack.
The key observation is that since the initialization (Algorithm 8.1) sets the bit
R4[6] to 1, the initial value of R4 is the same for two initialization vectors which
differ only at the 11th bit. (Remember that the initial value is added to the most
signiﬁcant bit and R4[6] is the 11th bit of R4 if you start counting at the most
signiﬁcant bit.)
So two frames at distance 211 will have the same R4 register and the registers
R1, R2 and R3 will differ by a known δ. We guess the value of R4. Then the output
of A5/2 is a known quadratic function in R1, R2 and R3.
Since for a quadratic function f and ﬁxed δ the difference g(x) = f (x + δ) −
f (x) is a linear function in x, the difference of the two frames at distance 211 will
give us a linear system of equations for the internal values of R1, R2 and R3.
Note that, as in the attack described in the previous section, we can use the re-
dundancy of the output to rapidly check if our guess for R4 is correct or not. Thus
we must solve the linear system only for a small number of possible values of R4,
i.e. the attack needs only about 216 operations.
In comparison to the advanced attack described in the previous section this ﬁrst
attack has some drawbacks: It is a known plaintext attack and it needs two frames
at distance 211 (which lie about six seconds apart). In contrast the advanced attack
needs only a few milliseconds before it can start and it is a ciphertext-only attack.

176
8
The Security of Mobile Phones (GSM)
Algorithm 8.2 A5/1 initialization
1. Set all registers to zero. Now clock every register 64 times and in each step add
one bit of the key (from the least signiﬁcant to the most signiﬁcant bit) to the
most signiﬁcant bit of the registers.
2. Clock every register for 22 additional cycles and this time add the bits of the
initialization vector to the most signiﬁcant bit of the register. The initialization
vector is increased by one in the next frame.
3. Run the cipher for 100 steps and discard the output.
Nevertheless, the attack of Goldberg, Wagner and Green is very good and it be-
comes even more impressive if we remember that it was published only a few days
after the description of A5/2.
8.3 A5/1
8.3.1 Description of A5/1
A5/1 uses the same registers R1, R2 and R3 as A5/2, but a different clocking mech-
anism. In each step the clocking unit reads the bits R1[10], R2[11] and R3[12]
(marked in gray in Fig. 8.3) and computes the majority function of these three bits.
The register R1 is clocked if R1[10] agrees with the majority. Similarly R2 and R3
are clocked if R2[11] or R3[12] respectively agrees with the majority.
The protocol is also nearly identical to the protocol used for A5/2. Again the
error-correcting code is applied before the cipher and the cipher is reseeded after
228 bits. The initialization of A5/1 (Algorithm 8.2) is very similar to Algorithm 8.1.
8.3.2 Time-Memory Trade-off Attacks
The main ﬂaw of A5/1 is the very small internal state of only 64 bits (equal to the
key size). As mentioned in Sect. 3.3.1, this allows the attacker to search for the
internal state instead of the key. Since the internal state is always changing, there
are several possible time-memory trade-offs that make this attack practicable. The
following attack was developed by A. Biryukov, A. Shamir and D. Wagner [28] and
later improved in [14].
Let us start with the basic form of the attack. We must create a table which lists
2k internal states and the corresponding output. Now observe 264−k time steps of
the output of A5/1. By the birthday paradox we can expect that one of the 264−k
internal states belonging to the output will be in the pre-computed table of 2k states.
Choosing k ≈32 we already get an attack of quite reasonable time and space com-
plexity. The space needed to store the pre-computed table is some gigabytes, which
is very acceptable, but the number of bits we need to observe is bit higher than we
would like.

8.3
A5/1
177
Fig. 8.3 Diagram of A5/1
The attack recovers an internal state, but A5/1 can be effectively inverted, i.e. for
an internal state one can compute the list of all predecessors. The majority rule used
for the clock control makes it possible for a state to have up to four predecessors.
The average number of predecessors is one and hence moving even several steps
backward in time will result only in a small list of predecessors. As A5/1 is reseeded
quite often we can reconstruct the initial state of a frame.
The initialization of A5/1 (see Algorithm 8.2) is linear in the key. Thus an attack
that recovers an internal state is also a key recovering attack.
Now let us see how to improve the attack. The main problem is that one needs
a large list of internal states and for each fraction of the output sequence we must

178
8
The Security of Mobile Phones (GSM)
Algorithm 8.3 Enumerating special states of A5/1
Input: A 16 bit pattern Output: The list of internal states that generate the 16 bit
pattern as output
1. Choose any value for the shortest register R1 (219 possibilities).
2. Choose any value for the leftmost 11 bits in the registers R2 and R3.
3. This gives a list of 241 partial states. We extend each of the partial states to a
special state that generates the 16 bit pattern.
4. Given a partial state, the clocking tabs are known, so we can determine the clock
control for the next few time steps. We can determine the clock control until
either R2 or R3 clocks 12 times. Each register is clocked with probability 3/4,
so we expect that we can predict the clock for about 16 time steps.
(Of course there are some “wild” partial states in which R2 (or R3) is always
clocked and we can predict the clock control for only 11 time steps. The practical
way to deal with these wild partial states is to throw them away. We do not want
to enumerate all 248 special states, because it would take too much space and
time, so it doesn’t hurt if we drop some wild partial states.)
5. Due to the majority clocking rule either R2 or R3 must be clocked at every time
step. This moves a fresh bit (unspeciﬁed by the partial state) to the right. By
assigning the suitable value to this fresh bit we can force the output to the given
pattern.
probe the disk to see if we have stored the corresponding internal state. Disk probing
is very slow in comparison to computation in the processor, hence we want to avoid
it. A simple idea that always works is to store only special states which correspond
to a given output form in the table of pre-computed states. The advantage is that
we can check the observed output and must only probe the disk if it has a special
form. The generic way to create such a table of special states is to choose a random
state, compute the output and check if it is special. So using a table of special states
reduces the time needed in attack but increases the pre-processing time dramatically.
In the case of A5/1 we are lucky. We can generate all internal states with a given
output directly (see Algorithm 8.3).
Algorithm 8.3 generates only special states and no special state is missed. So one
can generate any number of c < 248 random special states in O(c) time steps.
The next trick to reduce the complexity of the attack uses the fact that A5/1
is not uniquely invertible. On average, every initial state has one predecessor, but
the number of predecessor can vary between zero and four. For every state we can
compute the tree of all its predecessors at level 1,2,... .
Since the A5/1 protocol (Algorithm 8.2 Step 3) discards the ﬁrst 100 bits of the
output we know that there is no need to store a state which has no predecessor at
level 100.
Experiments (see [28]) show that this removes about 85 % of all states because
their tree dies out before reaching level 100. For the remaining states the size of the
tree can reach more than 26000, but the average size over all states is still 1. The

8.3
A5/1
179
probability that a state occurs in a collision is proportional to the size of the tree of
predecessors. So in order to improve the success probability one should store only
special states with large trees of predecessors.
In [28] the authors chose to use a table 235 special states. This means they se-
lected only one state out of 219 possible states. This small fraction means that the
average size of the tree of predecessors of the selected states is about 12500. The
probability of hitting one of these states is ≈60 % (given that one can observe 71
frames which corresponds to about 2 minutes transmission time).
For an attack based on the birthday paradox this is high. Pushing the probability
towards 1 becomes more and more expensive. Remember that the only way to get
the probability to exactly 1 is to store all possible states.
There is one more optimization we must mention. All the calculations described
above must simulate the three linear shift registers R1, R2 and R3. In Sect. 2.6.2
we have seen several algorithms for this problem. Which of these algorithms should
we use in this special application? The answer is none of them. The largest register
R3 has size 23 and period 223 −1. This is small enough to pre-compute the whole
period of the linear shift register sequence and replace all computations by a table
look-up. This makes the simulation very efﬁcient on standard PCs.
In the form as described above the time-memory trade-off attack is already practi-
cal. In [28] even more trade-offs between pre-processing time, computational power
needed for the attack and the number of observed bits are described.
8.3.3 Correlation Attacks
The attack presented in the last section is sufﬁcient to break A5/1, but we could
imagine a generalized A5/1 in which the three registers are bigger. Then time-
memory trade-off attacks rapidly become impractical. The following attack due to
P. Ekdahl and T. Johansson [84] utilizes the ideas of correlation attacks to avoid this
problem. This was later improved in [181]. For the small parameters used in the real
A5/1 the time-memory trade-off attacks are better, but the correlation attack will
also work with bigger shift registers.
The basic observation is the following. Let us approximate the clocking proce-
dure by a random process. In each time step the clocking unit selects one of four
possibilities, clock R1 and R2, clock R1 and R3, clock R2 and R3 or clock all three
with probability 1/4. Thus we can compute for each time step t the a priori proba-
bility p(t)
a,b,c for the register R1 to be clocked exactly a times, the register R2 to be
clocked exactly b times and the register R3 to be clocked exactly c times.
The values p(t)
a,b,c satisfy the following recurrence
p(0)
0,0,0 = 1,
(8.1)
p(0)
a,b,c = 0
for (a,b,c) ̸= 0,
(8.2)

180
8
The Security of Mobile Phones (GSM)
p(t+1)
a,b,c = 1
4

p(t)
a−1,b−1,c + p(t)
a−1,b,c−1 + p(t)
a,b−1,c−1 + p(t)
a−1,b−1,c−1

.
(8.3)
For example, for time step 101 (the ﬁrst observable output) we get p(101)
76,76,76 =
9.74 · 10−4. That means the probability that the ﬁrst output z1 of A5/1 is the sum of
the output values x(1)
76 , x(2)
76 , x(3)
76 of the three LFSRs after 76 clockings is p(101)
76,76,76 +
(1 −p(101)
76,76,76) 1
2 ≈0.5 + 4.87 · 10−4. Since the initial values of the LFSRs are linear
combinations of the key and the frame number this gives us a correlation between
the observable output and the key. It may seem that this is a very weak correlation,
but one should not forget that the attacker has access to a large number of these
correlations (one for each a,b,c,t and each frame number).
Before we proceed with the attack we will modify the basic relation a bit. Let us
look at the sum zt +zt+1 of two successive output bits. We assume that the clocking
unit advances only the registers R1 and R2 in that time step (with probability 1/4).
Then the output of register R3 is the same in both time steps and cancels. Thus the
attacker need only guess the current state of the registers R1 and R2. The probability
that at time step t the register R1 is clocked exactly a times and the register R2 is
clocked exactly b times is p(t)
a,b,⋆= 
c p(t)
a,b,c. Since 1/4p(t)
a,b,⋆> p⋆
a,b,c the corre-
lation with two consecutive bits is stronger than simple correlation with one output
bit. Similarly one also gets correlations for the case in which the registers R1 and
R3 or R2 and R3 are clocked.
What the attacker has at this point is a lot of probabilities of the form
P

fa,b,⋆(K) = 0

= p
(8.4)
where fa,b,⋆is some linear function in the key bits and p is close but different
from 1
2. The important thing is that the linear function fa,b,⋆does not depend on
the time step t and the frame number. This means that if the attacker has access
to several thousand frames the attack will have many observations of the form (8.4)
with the same linear function fa,b,⋆. Under the (simpliﬁed) assumption that all these
observations are independent, one can compute the joint probability. Assuming we
have N independent observations of the form P(fa,b,⋆(K) = 0) = pk then
P

fa,b,⋆(K) = 0

=
N
k=1 pk
N
k=1 pk + N
k=1(1 −pk)
.
Normally the joint probability will be far from 1/2.
At this point the attacker has a standard decoding problem to solve. He has a
number of linear equations which are satisﬁed with a high probability and must
ﬁnd the solution. For the parameters of A5/1 the code size is small and standard
decoding algorithms from the theory of error-correcting codes will do the job. Sim-
ulations [84] show that 70000 frames are enough for a successful attack.
The above analysis was under the simpliﬁed assumption that all events are inde-
pendent, which is in the real world not the case. In [181] the authors show how to
take the dependent events into account, which improves the attack. This lowers the
number of frames needed for a successful attack to 10000.

8.3
A5/1
181
A important lesson we can learn from the correlation attack against A5/1-like
ciphers is not to use a linear function in the frame counter and the key as initial-
ization. If in the above attack the attacker cannot combine many different frames,
he would obtain only equations which are satisﬁed with a probability of 1
2 ± 10−3.
The decoding of the error-correcting code would be much more complicated in this
case (most likely the number of necessary operations would be impractical). This
is a quite typical behavior. We will see in the next chapter that RC4 is also quite
weak if one uses session keys of the form initialization vector∥main key. One way
to avoid these problems is to use a hash function to compute the session key from
the main key and the initialization vector. Even a weak hash function like MD5 will
be sufﬁcient. This prohibits related key attacks which use several frames effectively
and is easy to implement.

Chapter 9
RC4 and Related Ciphers
9.1 Description of RC4
In 1987 Ron Rivest designed for RSA data security Inc. the stream cipher RC4,
which is optimized for use in 8-bit processors. The design was kept secret until 1994,
when the secret leaked out in a usenet posting [258]. The cipher is extremely fast
and exceptionally simple. It is used in several network protocols, of which wireless
LAN is the best known. This makes it, in the words of its author, “the most widely-
used stream cipher in the world” [221].
There is a book [207] devoted completely to RC4 which, of course, covers more
details than this chapter. [207] is focused on identifying potential weaknesses (like
Theorem 9.12), but is less concerned with turning them into real attacks. Section 9.4
and Sect. 9.5, which cover attacks that utilize weak key scheduling, are more de-
tailed than the corresponding sections in [207].
The internal state of RC4 consists of two pointers i and j, and a permutation of
the values 0,...,255. The value of pointer i is easy to predict, so i is called the
public pointer, and j is the private pointer.
The key scheduling (Algorithm 9.1) will generate the initial permutation from
a (random) key of length l bytes. Typically, l will be in the range between 5 and
32. (Due to some export limitations, the key length of RC4 was originally restricted
to 5 bytes. Some commercial implementations still have this restriction. Of course,
5 bytes or 40 bits are not enough to secure the cipher against brute force attacks.)
The maximal key length is l = 256 bytes. The main part of the algorithm is a pseudo-
random generator (Algorithm 9.2) that produces one byte of output in each step.
As usual for stream ciphers, the encryption will be an XOR of the pseudo-random
sequence with the message.
For the analysis of RC4 it is convenient to replace the original algorithm that
works on bytes (Z/256Z) by a generalization that works on Z/nZ for some n ∈N.
For n = 256 we obtain the original algorithm.
RC4 does not specify how to construct the session key from the main key and the
initialization vector. The following three variants are in use.
A. Klein, Stream Ciphers, DOI 10.1007/978-1-4471-5079-4_9,
© Springer-Verlag London 2013
183

184
9
RC4 and Related Ciphers
Algorithm 9.1 RC4 key scheduling
1: {initialization}
2: for i from 0 to n −1 do
3:
S[i] ←i
4: end for
5: j ←0
6: {generate a random permutation}
7: for i from 0 to n −1 do
8:
j ←(j + S[i] + K[i mod l]) mod n {K is the session key}
9:
Swap S[i] and S[j]
10: end for
Algorithm 9.2 RC4 pseudo-random generator
1: {initialization}
2: i ←0
3: j ←0
4: {generate pseudo-random sequence}
5: loop
6:
i ←(i + 1) mod n
7:
j ←(j + S[i]) mod n
8:
Swap S[i] and S[j]
9:
k ←(S[i] + S[j]) mod n
10:
output S[k]
11: end loop
1. Use session keys of the form IV∥main key. This version is used in the WEP
protocol.
2. Use session keys of the form main key∥IV. All published attacks that break the
ﬁrst variant can also break this variant.
3. The RC4 key scheduling algorithm is not very strong. Both of the ﬁrst two vari-
ants fail two key recovering attacks. To protect the main key it is wise to compute
the session key as H(IV,main key), where H is some hash function. Any hash
function seems to be feasible. The WPA protocol uses a newly constructed hash
function. None of the known attacks are able to break RC4 using this variant of
key scheduling.
9.2 Application of RC4 in WLAN Security
9.2.1 The WEP Protocol
In 1997 the IEEE released the ﬁrst version of the 802.11 standard for wireless net-
working. Part of the standard is the encryption protocol WEP (Wireless Equivalent
Privacy).

9.2
Application of RC4 in WLAN Security
185
The session key in WEP has the form IV∥main key. The initialization vector is
public and 3 bytes long. The length of the main key can vary but the default is
5 bytes. The messages are generated with a standard network protocol, i.e. they
contain ﬁxed headers and use a CRC-code to detect errors.
WEP contains many ﬂaws that have nothing to do with RC4:
1. A 5 byte (40 bit) key is much too short. Any modern application should use 128
bit keys.
2. An initialization vector of only 3 bytes is much too short. If we use random
numbers as IV the birthday paradox says that we can expect a collision after
only 212 = 4096 sessions.
Of course we can use a counter as IV instead of a random number, but this
leads to other problems. Some implementations of WEP use a counter that starts
at 0 after each reboot. Since (private) PCs are rebooted quite often, the attacker
can easily observe different sessions with the same IV.
3. If one wants to use error-correcting codes and cryptography together, the correct
order is to ﬁrst encrypt the message and then add parity check symbols (see
Algorithm 1.1). If you add parity check symbols ﬁrst and encrypt later, as in the
case of WEP, you add extra redundancy to the message that can be used by an
attacker. We will see in Sect. 9.2.3 how to explore this error.
4. Publicly known header information should not be encrypted. First of all it is a
waste of time to encrypt something which is not secret and secondly it allows
known plaintext attacks. Every successful attack against WEP uses this weak-
ness.
The WEP protocol is now fully broken. There are publicly available programs
like aircrack-ng [4, 216] which break WEP secured networks in less than one
minute. These programs use some variant of the attack described in Sect. 9.5. For a
recent summary of all attacks against WEP (including hacking tricks not related to
cryptography), see [262, 263].
9.2.2 The WPA Protocol
After the weaknesses of the WEP protocol were published, the IEEE released a
new protocol WPA (Wi-Fi Protocol Access). It differs from WEP in the following
points:
• WPA includes a key hash function to prevent simple key recovering attacks. The
designers did not use a standard hash function, but invented a new one.
• A message integrity code (MIC) was added to prevent manipulations of the en-
coded message.
WPA retains some of the weaknesses of WEP.
• It still encrypts publicly known protocol information, which turns every known
plaintext attack into a ciphertext-only attack.

186
9
RC4 and Related Ciphers
Algorithm 9.3 Temporal Key Hash
1: {initialization of P1K}
2: P1K[0] ←Lo16(IV 32)
3: P1K[1] ←Hi16(IV 32)
4: P1K[2] ←Mk16(T A[1],T A[0])
5: P1K[3] ←Mk16(T A[3],T A[2])
6: P1K[4] ←Mk16(T A[5],T A[4])
7: {shufﬂe P1K}
8: for i from 0 to 7 do
9:
j ←2 · (i&1) {j = 0 if i is even and j = 2 for i odd}
10:
P1K[0] ←P1K[0] + S[P1K[4] ⊕Mk16[T K[1 + j],T K[0 + j]]]
11:
P1K[1] ←P1K[1] + S[P1K[0] ⊕Mk16[T K[5 + j],T K[4 + j]]]
12:
P1K[2] ←P1K[2] + S[P1K[1] ⊕Mk16[T K[9 + j],T K[8 + j]]]
13:
P1K[3] ←P1K[3] + S[P1K[2] ⊕Mk16[T K[13 + j],T K[12 + j]]]
14:
P1K[4] ←P1K[4] + S[P1K[3] ⊕Mk16[T K[1 + j],T K[0 + j]]] + i
15: end for
16: {initialization of PPK}
17: PPK[0] ←P1K[0]
18: PPK[1] ←P1K[1]
19: PPK[2] ←P1K[2]
20: PPK[3] ←P1K[3]
21: PPK[4] ←P1K[4]
22: PPK[5] ←P1K[5] + IV 16
23: {shufﬂe PPK}
24: PPK[0] ←PPK[0] + S[PPK[5] ⊕Mk16(T K[1],T K[0])]
25: PPK[1] ←PPK[1] + S[PPK[0] ⊕Mk16(T K[3],T K[2])]
26: PPK[2] ←PPK[2] + S[PPK[1] ⊕Mk16(T K[5],T K[4])]
27: PPK[3] ←PPK[3] + S[PPK[2] ⊕Mk16(T K[7],T K[6])]
28: PPK[4] ←PPK[4] + S[PPK[3] ⊕Mk16(T K[9],T K[8])]
29: PPK[5] ←PPK[5] + S[PPK[4] ⊕Mk16(T K[11],T K[10])]
30: PPK[0] ←PPK[0] + RotR[PPK[5] ⊕Mk16(T K[13],T K[12])]
31: PPK[1] ←PPK[1] + RotR[PPK[0] ⊕Mk16(T K[15],T K[14])]
32: PPK[2] ←PPK[2] + RotR[PPK[1]]
33: PPK[3] ←PPK[3] + RotR[PPK[2]]
34: PPK[4] ←PPK[4] + RotR[PPK[3]]
35: PPK[5] ←PPK[5] + RotR[PPK[4]]
36: {compute the RC4 key K}
37: K[0] ←Hi8(IV 16)
38: K[1] ←(Hi8(IV 16)|0x20)&0x7F
39: K[2] ←Lo8(IV 16)
40: K[3] ←Lo8((PKK[5] ⊕Mk16(T k[1],T k[0])) ≫1)
41: for i from 0 to 6 do
42:
K[4 + 2 · i] = Lo8(PPK[i])
43:
K[5 + 2 · i] = Hi8(PPK[i])
44: end for

9.2
Application of RC4 in WLAN Security
187
• It uses an error-correcting code in the same wrong fashion as WEP. In contrast to
WEP, the design adds some ad hoc rules to avoid the worst consequences of this
mistake.
The most interesting part of WPA is the temporal key hash (Algorithm 9.3). The
algorithm uses a 48 bit initialization vector, which is split into 16 least signiﬁcant
bits IV 16 and the most signiﬁcant 32 bits IV 32. Further, it takes a 6 byte transmitter
address T A and a 16 byte key T K as input.
The notation Loi(X) denotes the lower i bits of X, Hi i(X) denotes the higher i
bits of X, RotR is a rotation to the right by one bit, ⊕is the XOR and all additions
are done modulo 216. By & we denote the bitwise AND (as in the language C), i.e.
i &1 = 0 if i is even and i &1 = 1 for odd i.
As non-linear part it uses an S-box that is based on the AES S-box. The S-box
is described in Algorithm 9.4. The non-linear step interprets a byte as an element of
F256 = F[x]/⟨x8 + x4 + x3 + x + 1⟩and computes the multiplicative inverse. The
other steps are linear and are added just to increase the algebraic complexity of the
operation.
Algorithm 9.4 Temporal Key Hash S-box
1: for j from 0 to 1 do
2:
{The next two steps are the AES S-box and are performed by table look up.}
3:
Invert X[j] as an element of F256; 0 goes to 0.
4:
Apply the F8
2 afﬁne transformation
b′
i ←bi + b(i+4) mod 8 + b(i+5) mod 8 + b(i+6) mod 8 + b(i+7) mod 8 + ci,
where the bi denote the bits of X[j] and c = 01100011.
5:
Compute Y[i] = x · X[j] in F256 {by table look up.}
6: end for
7: return Mk16(Y[0] ⊕X[1] ⊕Y[1],X[0] ⊕Y[0] ⊕Y[1])
The temporal key hash is rather complicated, so we include some ﬁgures that
visualize the structure of the Algorithm. The S-Box (Algorithm 9.4) is displayed in
Fig. 9.1. Figure 9.2 visualizes lines 9–14 of Algorithm 9.3 and Fig. 9.3 shows the
computation done in lines 24–35.
Algorithm 9.3 is not a good hash function. As Moen, Raddum and Hole [194]
show, it is possible to invert the temporal key hash with just 232 operations given
two values with the same IV 32. At the moment, it is unknown if one can turn their
result into a practical attack against the WPA protocol.
9.2.3 A Weakness Common to Both Protocols
It is not clear why the protocols require that the error-correcting code has to be
applied before the cryptographic code. Most likely it was simply the easiest way

188
9
RC4 and Related Ciphers
Fig. 9.1 The S-box of the Temporal Key Hash (Part 1)
Fig. 9.2 Temporal Key Hash (Part 1)
Fig. 9.3 Temporal Key Hash (Part 2)

9.2
Application of RC4 in WLAN Security
189
to add the cryptography to the older network protocol. We have already seen in
the previous chapter that applying an error-correcting code before the cryptographic
code is a serious mistake that weakens the whole system. In the case of the WEP
protocol one can use this error to break the system without breaking RC4 itself.
This was ﬁrst observed by W.A. Arbaugh [7]. In the following we describe a vari-
ant of this attack that is better suited for a real implementation [164]. The problem
is that the used CRC-code is of variable length. This allows the attacker to obtain
many relations.
A cyclic redundancy check code (CRC-code) is described by a generator polyno-
mial g(x) ∈F2 of degree k. The encoding and decoding is done by Algorithms 9.5
and 9.6.
Algorithm 9.5 CRC encoding
1: Get message m(x) = m0 + ··· + mn−kxn−k ∈F2[x]
2: r(x) = xkm(x) mod g(x)
3: Send w(x) = xkm(x) + r(x)
Algorithm 9.6 CRC decoding
1: Get received codeword w(x) = w0 + ··· + wnxn ∈F2[x]
2: r(x) = w(x) mod g(x)
3: if r(x) ̸= 0 then
4:
Report an error and require that the message is transmitted again
5: else
6:
Decode as m(x) = wk + wk+1x + ··· + wnxn−k
7: end if
Now suppose that we receive a cryptographically secured CRC-codeword, i.e.
we receive ci = wi + xi where wi are the coefﬁcients of a codeword computed
by Algorithm 9.5 and x0,...,xn is some (pseudo-)random sequence. Normally the
leading coefﬁcient will be sent ﬁrst, i.e. the word will be sent in the order cn,...,c0
and the pseudo-random sequence x will also be computed by the stream cipher in
the order xn,...,x0.
Assume that the receiver accepts not only the codewords of length n + 1 but
also codewords of length n + 1 −m. Since the CRC-code does not specify the
length of the codewords this is a reasonable assumption. Most real applications will
accept any number of bytes as a codeword, i.e. we have m = 8. The attacker tries
to alter the last m bits of the sequence cn,...,cm in such a way that it will be
accepted as a CRC-code word. To that end he simply enumerates all 2m possible
words t0,...,tm−1 and sends cn,...,c2m,c2m−1 +tm,...,cm +t0. The receiver will
decrypt it as wn,...,w2m,w2m−1 + tm−1,...,wm + t0 and use this word as input
for the CRC-decoding algorithm. By the protocol (line 4 of Algorithm 9.6) he has
to report an error if (wnxn−m + ··· + wm) + (tm−1xm−1 + ··· + t0) is not a multiple
of g(x). In this way the attacker learns the polynomial t(x) = tm−1xm−1 + ··· + t0

190
9
RC4 and Related Ciphers
for which w′(x) + t(x) with w′(x) = wnxn−m + ··· + wm is a multiple of g(x). In
addition he knows that w(x) = wnxn + ··· + w0 is a multiple of g(x) and hence
0 ≡w(x) + xm
w′(x) + t(x)

mod g(x),
0 ≡

wm−1xm−1 + ··· + w0

+ xmt(x)
mod g(x),
xmt(m) ≡

wm−1xm−1 + ··· + w0

mod g(x).
This congruence has a unique solution wm−1,...,w0. So the attacker has learned
with only 2m operations the last m bits of the message. Continuing in this fashion,
he can recover all but the ﬁrst m bits of the message.
As we have seen, this attack works even if the cipher is a true one-time pad.
The main problem is that, besides the mistake of applying the error-correcting code
before the cryptographic code, the receiver accepts different messages encrypted
with the same (pseudo-)random sequence. The correct response to an error for the
receiver would be to reject the pseudo-random sequence x0,...,xn and require that
the whole message be encrypted again with a new unused pseudo-random sequence.
The WPA protocol has some elements that go in this direction.
The attack is therefore a variation of the old theme, that the security of a one-time
pad vanishes immediately if you use it twice. If the protocol designers had chosen
the correct order, applying the error-correcting code after the cryptographic code,
then the receiver would have avoided this trap automatically.
This it is an important point that many people miss. Applying an error-correction
code before the cryptographic code is a serious mistake. It helps the attacker to
mount a known plaintext attack. In the best case the cipher is still good enough
to ensure the security of the system, but even then there is no reason to help the
attacker. In the worst case it is a protocol failure that kills the security of the whole
system. If you choose the correct order and apply the error-correcting code after the
cryptographic code, there are no problems at all.
9.3 Analysis of the RC4 Key Scheduling
The key scheduling algorithm is the weakest part of RC4. At the moment all suc-
cessful attacks use at least some weakness of the key scheduling. The ﬁrst published
analysis of the RC4 key scheduling dates back to the year 1981 [225] and is even
older than RC4 itself.
For the analysis we assume that the key has full length. In this case we can write
line 8 of Algorithm 9.1 as j ←f (K0,...,Ki−1)+Ki mod n. We prove that j will
be a uniformly distributed random variable.
Lemma 9.1 Assume that K0,...,Ki are iid. random variables uniformly dis-
tributed in the set {0,...,n −1}. Then for any function f the variable
Xi =

f (K0,...,Ki−1) + Ki

mod n
is uniformly distributed in {0,...,n −1} and independent from K0,...,Ki−1.

9.3
Analysis of the RC4 Key Scheduling
191
Proof We claim that
p

K0 = a0,...,Ki−1 = ai−1,

f (K0,...,Ki−1) + Ki

mod n = ai

=
1
ni+1
for all possible values (a0,...,ai). But this is clear, since
p(K0 = a0,...,Ki−1 = ai−1,Xi = ai)
= p

K0 = a0,...,Ki−1 = ai−1,Ki =

ai −f (a0,...,ai−1)

mod n

.
By summing over the elementary events we ﬁnd
p

f (K0,...,Ki−1) + Ki

mod n = a

= 1
n
for 0 ≤a < n and
p

(K0,...,Ki−1) ∈A,Xi = a

= |A|
ni+1 = p

(K0,...,Ki−1) ∈A

p(Xi = a),
i.e. (f (K0,...,Ki−1) + Ki) mod n is uniformly distributed in {0,...,n −1} and
is independent from K0,...,Ki−1.
□
So we reach the idealized RC4 key scheduling given by Algorithm 9.7.
Algorithm 9.7 Idealized RC4 key scheduling
1: {initialization}
2: for i from 0 to n −1 do
3:
S[i] ←i
4: end for
5: {generate a random permutation}
6: for i from 0 to n −1 do
7:
Choose j random in {0,...,n −1}
8:
Swap S[i] and S[j]
9: end for
A well-known algorithm for generating true random permutations (see for exam-
ple [152]) replaces line 7 of Algorithm 9.7 by “Choose j random in {i,...,n −1}”.
9.3.1 The Most Likely and Least Likely RC4 Permutation
As we can see, Algorithm 9.7 produces nn results of equal probability. Since n! is
not a divisor of nn, some permutations must be more likely than others. In fact there

192
9
RC4 and Related Ciphers
is an asymptotically exponential gap between the probability of the most likely and
least likely permutation.
We begin our study with the determination of the least likely permutation.
Theorem 9.2 (Robbins and Bolker [225]) Algorithm 9.7 produces the right cycle,
i.e. the permutation S[0] = n, S[i] = i −1 for i > 0, with probability 2n−1
nn .
Proof After the i-th loop in Algorithm 9.7, we say the elements S[0],...,S[i −1]
lie in the past. From the deﬁnition of the algorithm, we obtain:
1. A value that lies in the past will remain in the past in all future iterations.
2. Values that lie in the past can move only to the right (from position i to a position
i′ > i).
3. If a value moves to the left, it will be in the past afterwards.
4. Every value can move at most once to the left.
Now let us count the number of shufﬂes which produce the right cycle as a result.
We prove by induction that there are 2n−1 such shufﬂes. For n = 1 there is nothing
to prove. Now let n > 1.
Since n can move at most once to the left, there are only two ways to get n at
position 0.
One possibility is that n moves to position 0 in the ﬁrst step and stays there in the
remaining steps. Then after the ﬁrst step we have the permutation n,1,...,n −1,0
and the remaining n−1 steps must cycle the permutation 1,...,n−1,0 to the right.
By induction there are 2n−2 such shufﬂes.
The second possibility is that n moves only in the last step. So the ﬁrst n−1 steps
must produce the permutation n −1,0,...,n −2,n and the last step exchanges
n −1 and n. By induction there are 2n−2 shufﬂes which produce the right cycle
n −1,0,...,n −2.
In total we have 2n−1 shufﬂes which produce the right cycle.
□
The probability 2n−1
nn
is very low in comparison to the average probability 1
n! ≈
1
√
2πn
en
nn . The right cycle is indeed the least likely permutation (although for small n
there are other permutations that are equally likely).
Theorem 9.3 (Robbins and Bolker [225]) Every permutation is generated with at
least probability 2n−1
nn .
Proof We will compute the exact probability for the identity permutation later (The-
orem 9.7). Now we prove for all other permutations S that there are at least 2n
shufﬂes that produce S.
Since S is not the identity, we ﬁnd a position i with j = S[i] < i.
Consider the permutation S′ = (i j)S. It ﬁxes i and by induction there are at least
2n−2 ways to write S′ in the form
S′ = (n −1 rn−1)...(i + 1 ri+1)(i ri)...(1 r1)(0 r0)

9.3
Analysis of the RC4 Key Scheduling
193
Fig. 9.4 The graph
representation of
S = (0 1)(2 3) =
(3 1)(2 3)(1 2)(0 1)
where all rk differ from i. Multiply by (i j) on the right. By conjugating we can
move (i j) from the right to the middle and get
S =

n −1 r′
n−1

...

i + 1 r′
i+1

(i j)(i −1 ri−1)...(1 r1)(0 r0),
where (k r′
k) = (k rk)(i j) = (i j)(k rk)(i j) for k > i. At this point we use j < i so
that k is ﬁxed by (i j).
Similarly we can start with the permutation S′′ = S(i j) which ﬁxes j to ﬁnd
2n−2 solutions of
S′′ = (n −1 rn−1)...(j + 1 rj+1)(j −1 rj−1)...(0 r0)
or after multiplication by (i j) on the left
S = (n −1 rn−1)...(j + 1 rj+1)(j i)

j −1 r′
j−1

...

0 r′
0

where rk ̸= j. Again we use j < i.
Thus we have altogether 2n−1 shufﬂes that produce S. These shufﬂes are all
different since in the ﬁrst 2k−1 shufﬂes the j-th transposition is (j rj) with rj ̸= i
and in the last 2n−1 shufﬂes the j-th transposition is (j i).
□
The determination of the most likely permutation is more complicated. We start
with a graph representation of our problem. Let S be a permutation which is ob-
tained by the shufﬂe S = (n −1 rn−1)...(0 r0). Then we represent the shufﬂe by a
directed graph with vertex set {0,...,n−1} and edge set {(0,r0),...,(n−1,rn−1)}.
We also say that the graph represents S.
Every vertex has out degree 1. So each connected component of the graph must
contain a unique directed cycle. To avoid confusion with the cycles of the permuta-
tion we call the graph cycle a ring. In addition to the ring the connected component
can contain trees with root on the ring and edges pointing towards the ring. In the
example of Fig. 9.4 we have the ring 1 →2 →3 →1 and one tree with root 1.
Permutations from different connected components do not interact. So the shufﬂe
induces permutations on connected components of the graph.
The next lemma will help us to count the number of shufﬂes producing a given
permutation.
Lemma 9.4 (Schmidt and Simion [234]) If the ring has length 1, the permutation
S′ induced on the connected component with m vertices is an m-cycle. If the ring
has length at least 2 the induced permutation S′ consists of two cycles whose lengths
add up to m.

194
9
RC4 and Related Ciphers
Proof If the ring has length 1, then we have a representation of S′ as rooted tree
with a loop at the root. The loop has no effect on the generated permutation, so we
ignore it. This leaves us with a permutation of the form
S′ = (am−1 bm−1)···(a1 b1)
(9.1)
whose corresponding graph has edge set E = {(ai,bi) | i = 1,...,m−1} and vertex
set V .
We prove by induction on m that S′ is an m-cycle. For m = 1 this is trivial. For
m > 1 choose an edge (ai,bi) of the graph with ai as a leaf vertex. Deleting this
edge gives us a tree on V \{ai}. By induction
S′′ = (ai−1 bi−1)···(a1 b1)(am bm)···(ai+1 bi+1)
is a cycle, i.e.
S′′ = (ci h1 ... hm−2).
Then the permutation S′ is conjugate to
S′′(ai bi) = (ci bi h1 ... hm−2).
The conjugate of an m-cycle is an m-cycle, which completes the proof.
Now suppose that the ring has length at least 2. Let
S′ = (dm em)···(d1 e1).
Choose an edge (di ei) from the ring. Then
S′′ = (di−1 ei−1)···(d1 e1)(dm em)···(di+1 ei+1)
is of the form (9.1) and we have just proved that S′′ is an m-cycle, i.e.
S′′ = (di g1 ... gj ei gj+1 ... gm−2).
So S′ is conjugate to
S′′(di ei) = (ei g1 ... gj)(di gj+1 ... gm−2),
which is the product of two cycles whose lengths add up to m. Conjugation does not
change the cycle structure, so the proof is complete.
□
We deﬁne N(S) as the number of shufﬂes that generate the permutation S. By
Ntree(S′) we denote the number of shufﬂes whose graph has a ring of length 1 that
generates S′ and by Nring(S′) we denote the number of shufﬂes whose graph has a
ring of length at least two and that generates S. With this notation we obtain:

9.3
Analysis of the RC4 Key Scheduling
195
Lemma 9.5 For any permutation S which is the product πq ···π1 of q disjoint
cycles, we have
N(S) =

χ is involution
of {1,...,q}

χ(i)=i
Ntree(πi)

χ(i)=j<i
Nring(πi,πj).
(9.2)
Proof Any shufﬂe that generates S deﬁnes a partition of the set {π1,...,πq} into
sets of size one and two. The one element blocks are those cycles that are generated
by a tree and the two element blocks are pairs of cycles that are generated by a graph
with ring length at least 2. For each partition
χ =

{πi1},...,{πik},{πj1,πj′
1},...,{πjk′ ,πj′
k′ }

we get the number of corresponding graphs by multiplying the numbers Ntree(πil)
and N(πjl′ ,πj′
l′ ). The number N(S) is obtained by summing over all partitions. □
As an immediate consequence we get that the RC4-shufﬂe (Algorithm 9.7) pro-
duces the n-cycles as often as it should.
Corollary 9.6 (Robbins and Bolker [225]) Algorithm 9.7 produces an n-cycle with
probability n−1.
Proof By Lemma 9.5 the frequency of an n-cycle π is N(π) = Ntree(π). To get
the frequency of all n-cycles we just count the directed trees with n vertices.
Cayley’s theorem (see Sect. 16.3) says that there are nn−2 labeled trees on n
vertices and thus nn−1 directed trees.
Thus the probability of an n-cycle is nn−1
nn = 1
n.
□
With Lemma 9.5 we directly obtain the probability for the identity permutation.
Corollary 9.7 (Robbins and Bolker [225]) Algorithm 9.7 produces the identity per-
mutation with probability tn
nn , where tn denotes the number of involutions in Sn.
Proof The identity permutation consists of n cycles π1 ···πn of length 1. For 1-
cycles πi we have Ntree(πi) = 1 and Nring(πi,πj) = 1, so (9.2) simpliﬁes to
N(id) = 
χ is involution of {1,...,q} 1 = tn.
□
Since tn =
1
√
2nn/2e−n/2+√n−1/4(1 + O(n−1/2)) (Sect. 16.2) the identity permu-
tation occurs much more often than it should. For n > 18 it is the most likely shufﬂe.
Theorem 9.8 (Goldstein and Moews [108]) For n ≥18 the most like permutation
generated by Algorithm 9.7 is the identity. For 4 ≤n ≤17 the most frequent permu-
tation is (n −1 ··· m)(m −1 ··· 0) where m is n/2 for n even and either ⌊n/2⌋or
⌈n/2⌉for n odd.
Proof See [108].
□

196
9
RC4 and Related Ciphers
9.3.2 Discarding the First RC4 Bytes
The results in the previous subsection show that the permutations produced by Al-
gorithm 9.7 are far from random. This has serious consequences for the security
of RC4. The ﬁrst output bytes of RC4 are particularly highly biased. For example,
the second output byte of RC4 is zero with a probability twice as high as it should
be [93].
As a consequence, many researchers have suggested discarding the ﬁrst output
bytes of RC4 or to run the key scheduling several times. The natural question that
arises at this point is how many bytes should we discard?
The best answer to this question is due to I. Mironov [192]. He found the follow-
ing distinguisher for RC4 permutations.
Theorem 9.9 Assume that the permutation S is generated by t shufﬂing steps of the
form (ai bi) where the variables bi are iid. and uniformly distributed in {0,n}. Then
P

sign(S) = (−1)t
=
⌊n/2⌋

i=0
 t
2i

1 −1
n
t−2i 1
n2i .
For n,t →∞and n/t →λ, we have
P

sign(S) = (−1)t
−−−−→
n,t→∞
n/t→λ
1
2

1 + e−2λ
.
Proof The swap (ai bi) changes the sign of the permutation S if ai ̸= bi and leaves
the sign unchanged for ai = bj. Since bi is uniformly distributed the probability of
ai ̸= bi is 1 −1
n and the probability of ai = bi is 1
n. So the probability of having
exactly k passes with ai = ai is
t
k

(1 −1
n)t−k 1
nk .
The sign of S is (−1)t if we have an even number of steps i with ai = bi, i.e.
P

sign(S) = (−1)n
=
⌊t/2⌋

i=0
 t
2i

1 −1
n
t−2i 1
n2i .
To compute the limit for n,t →∞, t/n →λ, we factor:
=

1 −1
n
t⌊n/2⌋

i=0
2i−1
j=0 (t −j)
n2i

1 −1
n
−2i
1
(2i)!

.
The ﬁrst factor tends to e−λ. For ﬁxed i the summand
2i−1
j=0 (t−j)
n2i
(1 −1
n)−2i
1
(2i)! of
the second factor tends to λ2i
(2i)!.

9.3
Analysis of the RC4 Key Scheduling
197
So the whole sum tends to
∞

i=0
λ2i
(2i)! = 1
2
∞

i=0
λi
i! + 1
2
∞

i=0
(−λ)i
i!
= 1
2eλ + 1
2e−λ.
The order of the limits is not an issue since all terms are positive.
So
lim
n,t→∞
t/n→λ
P

sign(S) = (−1)t
= e−λ
1
2eλ + 1
2e−λ

= 1
2

1 + e−2λ
.
□
If we observe a permutation S and want to guess if it was generated by λn iter-
ations of Algorithm 9.7 or uniformly at random, we compute sign(S) and conclude
that S comes from λn iterations of Algorithm 9.7 if sign(S) = (−1)λn. Our chance
of guessing correctly is about 1
2e−2λ.
If we want to lower the quality of the distinguisher below ϵ we should choose
λ > 1
2 ln 1
2ϵ . For example ϵ =
1
256 would give a bound of λ ≈2.4. Under these cir-
cumstances, discarding the ﬁrst 2n−3n outputs seems to be a reasonable precaution.
As we will see in Sect. 9.5.3, there are practical attacks against RC4 if we discard
only the ﬁrst n outputs.
Now we ask the natural question: Can we ﬁnd an upper bound for the number
of outputs we have to discard to prevent any attack that is based on the weak key
scheduling? The answer is yes, as we will show in the remaining part of this section.
The success probability of a distinguisher is bounded by the variation distance
(see Sect. 15.2.1). The appropriate tool to obtain a bound for the variation distance
in our case is the concept of strongly uniform stopping times.
Deﬁnition 9.1 (Diaconis [79]) Suppose a process generates a sequence Qi, i ∈N,
of random permutations of {0,...,n −1}.
A stopping time τ is said to be strongly uniform if Qτ is uniformly distributed
under the condition τ < ∞, i.e. if
P(Qτ = π | τ < ∞) = 1
n!
for any permutation π of {0,...,n −1}.
Theorem 9.10 (Diaconis [79]) Let Q be a shufﬂing process and let τ be a strongly
uniform stopping time. Then for all times t, the variation distance between Qt and
the uniform shufﬂe U satisﬁes
∥Qt −U∥≤P(τ > t).
Proof See [79].
□
So we have to ﬁnd a uniform stopping time τ and compute the probabilities
P(τ > t).

198
9
RC4 and Related Ciphers
We deﬁne τ by the following procedure.
1. At the beginning the elements S[0],...,S[n −2] are unmarked and S[n −1] is
marked.
2. Every time we swap S[i] with S[j], S[i] becomes marked if either j = i or S[j]
is already marked. Note this is not symmetric in i and j, we will not mark S[j]
if S[i] is already marked.
3. τ is the ﬁrst time step in which all elements of the permutation are marked.
Theorem 9.11 (Mironov [192]) The stopping time deﬁned by the above procedure
is strongly uniform.
Proof We claim that at any time step the permutation of all marked symbols is
uniformly chosen.
At t = 1 this is trivial, since only one symbol is marked.
Now suppose this is true for the time t −1. If at time t two different unmarked
symbols are exchanged, then the permutation of marked symbols does not change,
so it stays uniformly distributed.
If S[i] is marked and S[j] is unmarked, the swap of S[i] and S[j] corresponds to
a permutation π of the marked symbols. If π′ is a uniformly distributed permutation
then π ◦π′ is uniformly distributed, which proves this case.
Now suppose that S[i] becomes marked at time step t. In this case, j must be
either i or point to a marked position. All alternative values of j have the same
probability. This means we insert S[i] at a random position of the permutation of
all marked symbols. This operation again yields a uniform distribution of marked
symbols.
□
The computation of the exact distribution of τ is rather complicated. To demon-
strate the method we will compute the expected value for the simpliﬁed case in
which i is also chosen at random.
If i is chosen at random, the probability that a new symbol becomes marked in
one step if x symbols are already marked is px = n−x
n
· x+1
n .
Thus the expected number of steps we need to mark one more symbol if x sym-
bols are already marked is 1/px. Summing over x gives us the expected value
E(τ) =
n−1

i=1
1/px = n2
n−1

i=1
1
(n −x)(x + 1) =
n2
n + 1

1
n −x +
1
x + 1

.
Using Euler’s approximation for the harmonic numbers Hn = n
i=1
1
i = ln(n) +
γ + O( 1
n) (see Theorem 16.2 in Sect. 16.1) we obtain E(τ) =
n2
n+1(2ln(n) + 2γ −
1 + O( 1
n)).
For n = 256 this means that we should discard the ﬁrst 12 · 256 elements of the
pseudo-random sequence to be on the safe side.
An interesting modiﬁcation of the RC4 algorithm would be to run the shufﬂe
until we reach the stopping time τ. For this modiﬁcation, we ﬁnd some interesting
research problems.

9.4
Chosen IV Attacks
199
• The real key has ﬁxed length and is cyclically repeated, so we cannot apply The-
orem 9.11 to conclude that Sτ is uniformly distributed. Indeed, Sτ cannot be uni-
formly distributed since n! is not a divisor of nl. How large is the difference be-
tween Sτ and the uniform distribution? Experiments with small n suggest that the
new algorithm produces a better distribution than the original RC4 key schedul-
ing.
• Prove that τ must be ﬁnite for ﬁxed key length.
• The running time of the modiﬁed key scheduling depends on the key. This allows
side channel attacks. How can we prevent them?
9.4 Chosen IV Attacks
The ﬁrst really practical attack against RC4 is due to S. Fluhrer, I. Mantin and
A. Shamir [93]. It is a chosen IV attack or related key attack that explores the weak-
ness of the key scheduling algorithm.
The attack assumes that the attacker has control over some bytes of the session
key. This is the case if the session key is of the form IV∥main key or main key∥IV.
9.4.1 Initialization Vector Precedes the Main Key
Assume that the attacker knows the ﬁrst A bytes of the session key, either because
they are part of the public initialization vector or because he has recovered them in
previous steps of the attack.
Assume further that the attacker can choose the ﬁrst two bytes of the IV to be
(A,n−1). The following happens in the ﬁrst two steps of the key scheduling (Algo-
rithm 9.1). In the ﬁrst step, j is increased by S[0]+K[0] = A and we swap S[0] = 0
and S[A] = A. In the next step, i is 1 and j is increased by S[1]+K[1] ≡0 mod n,
so we swap S[1] and S[A]. After the ﬁrst two swaps, we have S[0] = A and S[1] = 0
(see Fig. 9.5).
What happens in the next A −2 steps of the key scheduling is known to the
attacker, since we assume the ﬁrst A bytes of the session key to be known. So we
can express the value that is swapped to S[A] in the (A + 1)th step as a function of
K[A].
The probability that S[0], S[1] and S[A] remain unchanged during the remaining
n −A −1 steps of the key scheduling phase is approximately (1 −3
n)n−1−A ≈1
e3 ≈
0.05.
If this happens, the ﬁrst step of RC4 will set i to 1 and j will stay 0, since
S[1] = 0, so the output will be S[A] = f (K[A]).
The attacker will compute f −1(X) for every session, where X is the ﬁrst output
byte. As we have seen above, f −1(X) is with probability ≈0.05 equal to K[A]. In
the other cases f −1(X) will approximately be equally distributed in {0,...,n −1}.

200
9
RC4 and Related Ciphers
Fig. 9.5 The FMS-attack key scheduling
So the attacker gets the correct value of K[A] with probability ≈0.05+0.95 1
n and a
wrong value with probability ≈0.95 1
n. For n = 256, about 60 sessions with a chosen
IV are enough to recover K[A] almost certainly.
It is a remarkable property of the FMS-attack that it becomes stronger as n in-
creases. In the limit n →∞, we can stop the FMS-attack at the ﬁrst time we get the
same value f −1(X) for two distinct sessions. With a probability of 50 %, this is the
case after just 33 sessions.
Since in most applications the attacker cannot manipulate the IV directly, he has
to wait until by pure chance he observes enough sessions which have an IV of the
required value. Since we must prescribe two IV bytes, this will slow down the attack
by a factor of n2 provided that the IV is chosen at random.
If the initialization vector is generated by a counter, the number of sessions will
depend on whether the counter is little or big endian [93]. A little endian counter
(K[0] increments quickly) causes no problem; the attack will succeed after about
4,000,000 observed sessions. The case of a big endian counter is much more compli-
cated. For an IV of only 3 bytes, the attack will be even faster (1,000,000 sessions),
but we can run into some problems if the counter does not start with zero (see [93]
Sect. A.2). For larger IVs the attacker will have to wait for more than n3 sessions
before the counter starts generating IVs of the required form. However, there are
ways to adapt the attack to these cases (see also Sect. 9.5.3).
9.4.2 Variants of the Attack
After the publication of the FMS-attack, people tried to secure WEP encrypted net-
works by avoiding initialization vectors of the form (∗,n −1,∗). This only helps
against inexperienced attackers, since the FMS-attack is very variable and can be

9.4
Chosen IV Attacks
201
mounted with a lot of different initialization vectors. Perhaps the most complete list
of FMS-like attacks is [165].
Some of these variations do not use the ﬁrst but the second byte of the output,
other variants do not ﬁnd a hint for a value, but exclude some possible values for the
key with a high probability. We give an example for both variants.
Suppose the attacker knows the ﬁrst A bytes of the session key and wants to
get information about the (A + 1)th byte. The attack computes the ﬁrst A steps of
the RC4 key scheduling (Algorithm 9.1) and uses only sessions which satisfy the
following conditions:
• After A steps of the key scheduling, we have S[1] = B and S[B] = 0 for some
B < A.
• The ﬁrst byte of the RC4 pseudo-random sequence is B.
If the remaining scheduling steps do not change S[1] and S[B], we always get B
as ﬁrst byte of the pseudo-random sequence. This event occurs with probability e−2.
If at least S[0] or S[B] is changed in the key scheduling, we still have a 1
n chance
to get B as the ﬁrst pseudo-random byte.
So the conditional probability that S[1] and S[B] remain unchanged given the
observation of B as the ﬁrst pseudo-random byte is
e−2
e−2 + (1 −e2) 1
n
=
n
n + e2 −1.
This means with high probability (≈97.5 % for n = 256) we can conclude from
the observation of B as the ﬁrst byte of the pseudo-random sequence that S[1] and
S[B] remain unchanged in the last n −A steps of the key scheduling. This excludes
some possible values for the key.
The oldest variant of a chosen IV attack that uses the second instead of the ﬁrst
pseudo-random byte is due to Hulton [133].
The structure of the attack is quite similar to the FMS-attack. The attacker
chooses an A byte initialization vector such that after the ﬁrst A steps of the key
scheduling the following holds: SA[1] < A, SA[2] < A, SA[1] + SA[2] = S < A,
SA[2] + SA[S] = A, SA[1] ̸= 2, SA[2] ̸= 0. In the next step, S[A] is a value
X = fIV (K[0]) that depends only on the IV and the ﬁrst unknown key byte.
With probability (1 −4
n)n−A ≈e−4, the four S-box entries at the positions 1, 2,
S and A are not changed in the remaining part of the key scheduling. Then the ﬁrst
step of the RC4 pseudo-random generator advances j to SA[1]. At the second step it
sets j to S = SA[1] + SA[2]. Since SA[2] + SA[S] = A, the second pseudo-random
byte will be fIV (K[0]).
So the attacker learns fIV (K[0]) with probability e−4. This is lower than the
success probability of the FMS-attack, but still acceptable. The real drawback is
that the attacker has to prescribe 3 S-box values which reduces the number of usable
initialization vectors.

202
9
RC4 and Related Ciphers
9.4.3 Initialization Vector Follows the Main Key
If the initialization vector follows the main key, the FMS-attack becomes more com-
plicated. Instead of directly recovering the Ath key byte the attacker tries to recon-
struct the values SA[0],...,SA[A −1] of the internal state after the ﬁrst A steps of
the key scheduling.
We explain the basic version of the attack which works only for weak keys with
SA[1] = X < A and X + SA[X] = A. The fraction of weak keys is very small
(≈0.6 h for A = 13), but the class of weak keys will be extended later.
The attacker must guess the value jA of the pointer j after the ﬁrst A steps, i.e.
the attacker will try all possible values for jA. If jA is assumed to be known, the
attacker can compute the (A + 1)th step of the key scheduling. He will investigate
only those initialization vectors which will swap S[A] with S[Y] for some Y ∈
{0,2,...,X −1,X + 1,...,A −1}.
With probability (1 −3
n)n−A−1 ≈e−3 the next n −a −1 shufﬂes of the key
scheduling will not touch S[1], S[X] and S[A]. In the ﬁrst step of the RC4 algorithm
we will set i = 1, j = S[i] = X and k = S[i]+S[j] = X +S[X] = A. So the output
will be S[A], which is identical to the byte SA[Y].
By observing about 60 sessions (for n = 256) with such an IV, the attacker learns
SA[Y]. There is still a technical problem, since the value of j after the ﬁrst A + 1
steps will always be the same. Different observations from different sessions are not
independent. This will cause the attack to fail for some values of Y, but we learn
enough values SA[Y] to ﬁnd the rest by brute force (see [93] for the details).
By this procedure, the attacker learns the values SA[0],...,SA[A −1] of the S-
box after the ﬁrst A steps of the key scheduling. From these values, he has to derive
K[0],...,K[A −1]. Since there are nA possible keys, but only n · (n −1)···(n −
A + 1) =
n!
(n−A)! different partial S-boxes, the attacker must test different possible
keys. One way to do this is described by Algorithm 9.8 (see [93]).
The algorithm will need Aλ+1 time steps where λ is the number of values
0 ≤i < A with SA[i] < A. Typically this bound is quite small, so the algorithm
is efﬁcient. We will need it again in Sect. 9.5.1. An alternative algorithm which is a
little faster for large values of A can be found in [148].
If the initialization vector has length at least 3, we can extend the class of weak
keys. If the length is at least 4, we can attack almost all keys, but the success proba-
bility of the attack will decrease. A detailed description can be found in [93].
As we can see, the FMS-attack does not perform well in this case. It is now
outdated by the attack described in Sect. 9.5.1.
9.5 Attacks Based on Goli´c’s Correlation
In 2000 Goli´c mentioned in the paper [112] a correlation between the output of RC4
and parts of the internal state. Namely
p

S[k] + S[j] ≡i
mod n

≈2
n.
(9.3)

9.5
Attacks Based on Goli´c’s Correlation
203
Algorithm 9.8 Computing the key from an early permutation state
1: {initialization}
2: for i from 0 to n −1 do
3:
S[i] ←i
4: end for
5: for i from 0 to A do
6:
X ←S−1[SA[i]]
7:
if i < X < A then
8:
Choose X non-deterministic in {0,...,A −1} and run the remaining part
of the algorithm for all possible choices.
9:
end if
10:
K[i] ←X −j −S[i]
11:
j ←X
12:
Swap S[i] and S[j]
13: end for
14: Verify that S[i] = SA[i] for i = 0,...,A
He gave no proof for this observation and found no useful application. Later it was
rediscovered by several authors. Recently, Mantin [175] and I [148] found inde-
pendent attacks that are based on Goli´c’s correlation. Attacks from this class are
currently the best known attacks against RC4.
We will derive Goli´c’s correlation in a way that makes it easy to generalize to
other RC4 like ciphers. For the theorem, we need only steps 9 and 10 (computation
of k and output of S[k]) of the RC4 pseudo-random generator (Algorithm 9.2).
Theorem 9.12 (Klein [148])
Assume that the internal states are uniformly dis-
tributed. Then for a ﬁxed public pointer i, we have:
P

S[j] + S[k] ≡i
mod n

= 2
n,
(9.4)
P

S[j] + S[k] ≡i
mod n | S[j] = x

= 2
n for all x ∈{0,...,n −1}.
(9.5)
For c ̸≡i mod n, we have:
P

S[j] + S[k] ≡c
mod n

=
n −2
n(n −1),
(9.6)
P

S[j] + S[k] ≡c
mod n | S[j] = x

=
n −2
n(n −1) for all x ∈{0,...,n −1}.
(9.7)
Proof First note that (9.4) follows from (9.5), and (9.6) follows from (9.7).
To prove (9.5), we count all internal states with S[j] + S[k] ≡i mod n and
S[j] = x. First we use k ≡S[j] + S[i] mod n (line 9 of Algorithm 9.2) to write
S[j] + S[k] ≡i mod n as k + S[k] ≡i + S[i] mod n.

204
9
RC4 and Related Ciphers
We have to distinguish between two cases:
1. i = k.
Then S[i] = S[k] and the equivalence is satisﬁed trivially. In this case, S[i] =
S[k] ≡i −S[j] ≡i −x mod n and there are (n −1)! ways to choose the re-
maining n −1 entries of the S-box.
2. i ̸= k.
Then we have to set S[k] ≡i −x mod n and S[i] = k + S[k] −i mod n. We
may still choose k (n −1 possibilities) and the remaining n −2 entries of the
S-box ((n −2)! possibilities).
Hence there are altogether (n −1)! + (n −1)(n −2)! = 2(n −1)! ways in which
S[j] + S[k] ≡i mod n and S[j] = x, but there exist n! possible internal states
where S[j] = x, which proves (9.5).
The proof of (9.7) is quite similar.
Here we have to distinguish between three cases:
1. i = k.
In this case, S[i] = S[k] and k + S[k] ≡c + S[i] mod n cannot be satisﬁed,
since c ̸= i = k.
2. c = k.
In this case, k + S[k] ≡c + S[i] mod n implies S[k] ≡S[i] mod n. But this is
impossible, since k ̸= i and therefore S[i] ̸= S[k].
3. i ̸= k and c ̸= k.
In this case, we have to set S[k] ≡i −x mod n and S[i] = k +S[k]−c mod n.
We may still choose k (n −2 possibilities) and the remaining n −2 entries of the
S-box ((n −2)! possibilities).
Therefore only the third case yields a contribution to the number of possible
internal states. Thus
P

S[j] + S[k] ≡c
mod n | S[j] = x

= (n −2)(n −2)!
n!
=
n −2
n(n −1).
This proves the theorem.
□
9.5.1 Initialization Vector Follows the Main Key
Attacks based on Goli´c’s correlation use the observed output to recover a part of
the internal space and then use the weak key scheduling algorithm to conclude the
main key from the internal space. In the case that the initialization vector follows
the main key, the nature of the attack as an internal space recovering attack can be
seen in its purest form.
Let A be the length of the main key, then the ﬁrst A steps of the key scheduling
depend only on the main key. Let SA[0], ..., SA[A −1] be the ﬁrst A bytes of the

9.5
Attacks Based on Goli´c’s Correlation
205
S-box after the ﬁrst A steps of the key scheduling algorithm. As in Sect. 9.4.3 the
attacker aims at recovering these bytes.
For 1 ≤i < A, we ﬁnd that the probability that S[i] remains unchanged in the
last n −A steps of the key scheduling algorithm and the ﬁrst i −1 steps of the
pseudo-random generator is (1 −1
n)(n−A)+(i−1) ≈1
e .
If this happens, the ith step of Algorithm 9.2 will do the following: j will be
set to some unknown value and S[i] = SA[i] is swapped with S[j]. Now we have
S[j] = SA[i]. Then the algorithm outputs S[k] as the ith pseudo-random byte, which
can be observed by the attacker.
By Theorem 9.12, we have
p

S[j] + S[k] ≡i mod n

= 2
n.
The values S[k] = X[i] and i are known to the attacker so he can conclude that
S[j] = SA[i].
If S[i] is changed during the last n−A steps of the key scheduling algorithm and
the ﬁrst i −1 steps of the pseudo-random generator, the attacker still has the chance
that SA[i] + X[i] ≡i mod n by accident.
This leads us to the probability
p

SA[i] + X[i] ≡i mod n

≈1
e · 2
n +

1 −1
e
 n −2
n(n −1) ≈1.36
n ,
which is signiﬁcantly larger than the probability 1
n we would expect from a true
random sequence.
So what the attacker has to do is compute i −X[i] mod n for every observed
session and select the most frequent result as the value for SA[i]. After 25000 ses-
sions he will have the correct value SA[i] with high probability. Only SA[0] cannot
be obtained by this method, but we can ﬁnd that value by brute force.
By this method the attacker recovers the early state SA[0],...,SA[A −1]. We
have already seen in Sect. 9.4.3 how to reconstruct the key from this information
(Algorithm 9.8).
9.5.2 Initialization Vector Precedes the Main Key
The case where the initialization vector precedes the main key has drawn more
attention, since this variant is used in the WEP protocol.
The basic idea is the following (see also [148]).
Since the initialization vector (A bytes) is known to the attacker, he can simulate
the ﬁrst A steps of the key scheduling. Thus he can express the value S[A] after the
(A + 1)-th step as a function fIV (K[0]). With high probability ≈e−1, the value of
S[A] is not changed during the remaining key scheduling and the ﬁrst A −1 steps
of the RC4 pseudo-random generator.

206
9
RC4 and Related Ciphers
If this happens, the A-th step of the RC4 pseudo-random generator will swap
fIV (K[0]) to S[j] and by Theorem 9.12, we get the correlation
P

fIV

K[0]

+ XA ≡i mod n

= 1
e · 2
n +

1 −1
e
 n −2
(n −1)n ≈1.36
n .
(9.8)
This allows us to derive K[0]. Interpreting K[0] as part of the IV we can repeat
the attack to obtain the other key bytes.
We can speed up the attack if we use additional correlations to determine the ﬁrst
key byte. If possible, we can also use the special IVs of the chosen IV attacks in
Sect. 9.4, but these attacks use only one of n2 IVs. For n = 256, we can expect to
see at most one IV suitable for the FMS-attack before the attack based on Eq. (9.8)
is ﬁnished. So the FMS-attack is of no help.
The following variant of Goli´c’s correlation is better. It is possible that after the
A-th step of the key scheduling S[1] = A. If this happens there is a high probability
that neither S[1] = A nor S[A + 1] = fIV (K[0]) are changed in the remaining part
of the key scheduling. This has two important consequences for our attack.
First the probability that S[A + 1] = fIV (K[0]) remains unchanged in the key
scheduling and the ﬁrst (A −1) steps of the RC4 algorithm is just e−1(1 −e−1). (If
the value S[1] remains unchanged then S[A+1] will be changed in the ﬁrst RC4 key
stream generation. So S[1] must be changed, which explains the additional factor
1 −e.) Thus we must replace Eq. (9.8) by
P

fIV

K[0]

+ XA ≡i mod n

= 1
e

1 −1
e
2
n +

1 −1
e

1 −1
e
 n −2
(n −1)n
≈1.23
n ,
(9.9)
i.e. we should trust the correlation less.
The second case is that S[1] = A and S[A + 1] = fIV (K[0]) remain unchanged.
This happens with probability e−2. In this case, the ﬁrst step of the RC4 pseudo-
random generator will swap fIV (K[0]) to S[1] and the output XA of the A-th step
will not be correlated to fIV (K[0]). However, we can use a variant of Goli´c’s cor-
relation
P

S[i] + S[k] ≡j mod n

= 2
n.
The proof is identical to the proof of Theorem 9.12, we need only exchange i and j.
Normally this correlation is not helpful since it contains two unknowns S[i] and
j, but in this special case, j is known to be A. Thus we obtain the correlation
P

fIV

K[0]

+ X0 ≡A mod n

= 1
e2 · 2
n +

1 −1
e2
 n −2
(n −1)n ≈1.14
n . (9.10)
The correlation of Eq. (9.10) is weaker than the correlation of Eq. (9.8) and it uses
only 1
n of all possible IVs, but it still contributes enough information to signiﬁcantly
speed up the attack.

9.5
Attacks Based on Goli´c’s Correlation
207
If we measure the complexity of the attack by counting the number of used ses-
sions, this is the best we can do, but if we take into account that the computation of
fIV costs time, things are different. In the case of the WEP protocol, the attacker
can easily observe many sessions by ARP-spooﬁng. The fact that we have to recover
the key bytes sequentially rather than in parallel is particularly unsuitable for a fast
implementation. In this case we can use the following variant from [216], which is
implemented in the aircrack tool suite [4].
The basic idea is that in the normal case fIV = S[jIV + K[0] + S[A]]. It is
very likely that in the next steps of key scheduling the swaps will play no role,
so we can guess i
j=0 K[j] by calculating S−1
IV [A −1 + i −XA−1+i] −jIV −
A+i
j=A SIV [j]. The advantage of the new method is that it saves some computation
steps and recovers the key bytes in parallel. The disadvantage is that there are strong
keys for which the swaps matter. In these cases the attack will fail, but it is still faster
to try this method ﬁrst and switch to the slower, but exact, method only for strong
keys.
9.5.3 Attacking RC4 with the First n Bytes Discarded
Combining the ideas of the chosen IV attacks and the attacks described in the previ-
ous subsections, one gets an attack which can break RC4 even if the ﬁrst 256 bytes
of the pseudo-random sequence are unobservable.
We assume that the initialization vector precedes the main key. The ﬁrst byte of
the main key gets the number b. The attacker knows the IV, so he can compute the
ﬁrst b steps of the key scheduling. The attacker uses only the sessions in which S[1]
is set to b. For random initialization vectors this will happen with probability 1
n.
Since the initialization vector is known to the attacker, he can compute the ﬁrst b
steps of the key scheduling phase and check if S[1] = b.
The next step of the key scheduling assigns the value f (K[b]) to S[b]. The at-
tacker knows the function f and he wants to ﬁnd the unknown key value K[b]. In
the remaining steps of the key scheduling phase each of the values of S[1] = b and
S[k] = f (K[b]) will remain unchanged with a high probability of 1
e2 .
The ﬁrst step of the pseudo-random generator will set j to S[1] = b and inter-
change S[1] with S[j] = S[b] = f (K[b]). Thus we know that after the ﬁrst step of
the pseudo-random generator S[1] = f (K[b]) with probability 1
e2 . In the next n −1
steps of the pseudo-random generator the pointer j will be different from 1 with
probability ≈1
e , i.e. at the beginning of the second round S[1] will have the value
f (K[b]) with probability ≈1
e3 .
Now we analyze the output of the ﬁrst step of the second round. The pointer i
will be equal to 1 and j has a value that is not known to us. After we have swapped
S[i] and S[j], we know that S[j] = f (K[b]) with probability
1
e3 . Theorem 9.12

208
9
RC4 and Related Ciphers
says that S[j] = 1 −S[k] with probability 2
n. As in Sect. 9.5.2, we obtain
P

f

K[b]

= 1 −S[k]

≈1
e3 · 2
n +

1 −1
e3

·
n −2
n(n −1) ≈1.05
n .
(9.11)
The difference with the uniform distribution is smaller than before, but still signiﬁ-
cant.
We obtain an estimator for f (K[b]) by observing S[k]. Inverting the known func-
tion f , we obtain K[b]. Since the success probability is smaller than the one used
for the 1-round attack we have to study more sessions (approximately 813nln(n)
sessions that satisfy S[1] = b).
After we have obtained K[b], we can treat it as part of the initialization vector
and apply the algorithm described above to obtain K[b + 1], K[b + 2], and so on.
Since we cannot choose the initialization vectors, we have to wait long enough
to get enough initialization vectors (with S[1] = b, S[1] = b + 1, etc.) for our at-
tacks. For random initialization vectors, the estimated number of required sessions
is therefore n(813nln(n)).
If the initialization vector is not random but generated by a counter, we must
modify the attack a little.
If the initialization vector is generated by a little endian counter, the assumption
S[1] = b is satisﬁed in every nth session. Thus in this case we need n(813nln(n))
sessions for a successful attack.
If the initialization vector is generated by a big endian counter, things are a bit
different. If S[1] = b is satisﬁed once, it will remain equal to b for many sessions,
but it will take a long time until the assumption S[1] = b is ﬁrst satisﬁed.
If the initialization is b bytes long and we want to attack the ﬁrst byte K[b] of
the main key, we have to wait until K[0] + K[1] = b −1 to obtain S[1] = b. This
happens ﬁrst for K[0] = 0 and K[1] = b −1. The counter value is at this time
(b −1)nb−2. Thus we have to wait a long time before we can start our attack. For
typical values like n = 256 and b = 16, this will be unacceptable.
The following modiﬁcation of the attack will help in this case.
Instead of the investigation of initialization vectors that will result in S[1] =
f (K[b]) after the ﬁrst steps of the pseudo-random generator, we will look at initial-
ization vectors that will result in S[j] = f (K[b]) after the jth step of the pseudo-
random generator. This is possible, but now we must assume that the ﬁrst j bytes of
the S-box are not changed during the last n −j steps of the key scheduling phase.
The probability of this is (1 −j
n)n−j ≈1
ej . This means that the success probability
will be smaller, i.e. we have to observe more sessions, but if we choose j near to b,
we can use every nth session for the attack. For certain values of b, j and n, this can
be faster than the basic variant of the attack.
We can adapt the attack to the case that the initialization vector follows the main
key, but then the attack becomes more difﬁcult and less practical. The key length l
should be small in comparison with n (e.g., l = 16 and n = 256). In the ﬁrst l steps
of the key scheduling phase, j will be set to a value jl which depends only on the
main key. The permutation S will be similar to the identity permutation.

9.6
State Recovering Attacks
209
If the key scheduling started with i = l and j = jl (S initialized with the iden-
tity), we would be able to apply the attack as in the case IV∥main key. Only minor
modiﬁcations are necessary for the new starting values of i and j.
We do not know the value jl, i.e. we have to guess it. Since after l steps the
permutation S differs from the identity, we must further assume that the difference
does not matter, i.e. the success probability for the attack becomes smaller.
9.5.4 A Ciphertext-Only Attack
There are many ways to vary attacks based on Goli´c’s correlation [148]. Perhaps
one of the most interesting variants is that we can mount a ciphertext-only attack.
Suppose that we have a small set of frequent symbols F. The probability that
a frequent symbol occurs is p. In many applications the set of printable ASCII
characters could form this set.
For simplicity we assume that all frequent symbols have the probability p/|F|
and that each of the rare symbols have the same probability (1 −p)/(n −|F|).
Let us take the case that the initialization vector follows the main key. We want
to recover SA[1].
By Goli´c’s correlation (see also Sect. 9.5.1) we have that the ﬁrst output byte X1
takes the value 1 −SA[1] with probability 1
e · 2
n + (1 −1
e)
n−2
(n−1)n ≈1.36
n
and it takes
all other values with probability ≈1
n −
1.36
n(n−1).
We cannot observe X1 directly, but we can see the ciphertext C1 = X1 +M1. The
known a priori distribution of the message bytes allows us to mount a likelihood
attack.
The likelihood we ﬁnd is
P

SA[1] = k

=
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
1.36
n
p
|F| + 1
n −
1.36
n(n−1)(p 1−|F|
|F|
+ (1 −p))
if C1 + k −1 ∈F,
1.36
n
1−p
n−|F| + 1
n −
1.36
n(n−1)(p + (1 −p) n−|F|−1
n−|F| )
if C1 + k −1 /∈F.
We will observe many sessions and select the value for SA[1] with the largest
likelihood.
The attack can be improved by taking a better model for the a priori distribution
of the message. For many real cases, we can achieve a ciphertext-only attack that
works as fast as the FMS-attack with known plaintext.
9.6 State Recovering Attacks
As stated at the beginning of this chapter, the key scheduling is the weakest part
of RC4. We have seen in Sect. 9.4 attacks that are based solely on the weakness of

210
9
RC4 and Related Ciphers
the key scheduling. Goli´c’s correlation (Theorem 9.12) is a weakness of the RC4
pseudo-random generator, but the attacks that are based on this weakness still need
the weak key scheduling to succeed. In this section we study attacks that aim at
recovering the internal state of RC4 directly without using the key scheduling.
Since the internal state of RC4 is huge (n · n!, so for n = 256, it has size 1692
bits), state recovering attacks are very difﬁcult. Even a big speed-up in comparison
to the exhaustive search would not be helpful. At the moment the fastest internal
state recovering is due to A. Maximov and D. Khovratovich [182]. They estimate the
complexity of their attack to 2241 operations for n = 256, which limits the effective
key length to about 30 bytes. However, the attack needs to examine at least 2120
bytes of the output sequence.
The basic idea of all state recovering attacks is the following (see [149]).
At any time step, there are four unknown variables St[it], jt, St[jt] and kt. We
can simply simulate the RC4 pseudo-random generator (Algorithm 9.2) and guess
an unknown value as soon as we need it. If we reach a contradiction we go back to
the last guess and try the next value. This leads to Algorithm 9.9.
In Algorithm 9.9 “Choose” means a non-deterministic choice. Every time the
algorithm reaches a “Fail” the corresponding computation path ends. In most pro-
gramming languages one has to program a simulation of non-determinism explicitly,
but some languages like Lisp allow a non-deterministic operator, which allows us to
transform the pseudo-code of Algorithm 9.9 directly into real code.
It is remarkable that even the direct implementation of this idea leads to an algo-
rithm that is much faster than the complete search. We want to calculate the expected
running time. As in our previous analysis, we approximate the pointers j and k by
true random variables.
If the algorithm is in line 5, we have two possibilities: Either S[t + t′] already
has a value or it doesn’t. Since we already have s S-box entries with an assigned
value and t′ of these values were assigned to S[t],...,S[t + t′ −1], the probability
that S[t + t′] has an assigned value is s−t′
n−t′ . This gives us the recursion:
c1

t′,s

= s −t′
n −t′ c2

t′,s

+

1 −s −t′
n −t′

(n −s)c2

t′,s + 1

.
(9.12)
If the algorithm is in line 10, we have an s
n chance that S[jt] already has an
assigned value. If this is the case, there is a 1
n chance that S[k] already has the
correct value and a (1 −s
n)2 chance that S[k] has no assigned value and that Xt+t′
was not assigned to some other S-box element. In the remaining case, we have found
a contradiction and the computation path ends here (cost 1). If S[jt] has no assigned
value, we go directly to line 23. This gives us the recursion:
c2

t′,s

= s
n
2s
n −1
n −s2
n

+

1 −s
n
2
c1

t′ + 1,s + 1

+ 1
nc1

t′ + 1,s

+

1 −s
n

c3

t′,s

.
(9.13)

9.6
State Recovering Attacks
211
Algorithm 9.9 A simple internal state recovering attack (see [149])
1: t′ = 0,
2: s = 0 {s denotes the number of assigned S-box values}
3: Choose j−1 in {0,...,n −1}
4: loop
5:
{From here the algorithm needs c1(t′,s) steps}
6:
if S[t + t′] has no assigned value then
7:
Choose S[t + t′] different from all other S-box entries.
8:
s ←s + 1
9:
end if
10:
{From here the algorithm needs c2(t′,s) steps}
11:
jt′ ←jt′−1 + S[t + t′]
12:
Swap S[t + t′] and S[jt′]
13:
if S[t + t′] has an assigned value then
14:
k ←S[t + t′] + S[jt′]
15:
if S[k] has an assigned value then
16:
Check if S[k] = Xt+t′. If Yes goto line 33, otherwise Fail
17:
else
18:
Assign Xt+t′ to S[k] if possible, otherwise Fail
19:
s ←s + 1
20:
goto line 33
21:
end if
22:
end if
23:
{From here the algorithm needs c3(t′,s) steps}
24:
if Xt+t′ was assigned to some S-box position then
25:
k ←S−1[Xt+t′]
26:
Assign k −S[t + t′] to S[jt′] if possible, otherwise Fail
27:
s ←s + 1
28:
else
29:
Choose a value for S[jt′] such that no value is assigned to S[k] with k ←
S[t + t′] + S[jt′].
30:
Assign Xt+t′ to S[k] is possible, otherwise Fail
31:
s ←s + 2
32:
end if
33:
{Test if we reached the end}
34:
if s = n then
35:
Test if the reconstructed state produces the observed output. If Yes stop,
otherwise Fail
36:
else
37:
t′ ←t′ + 1
38:
end if
39: end loop

212
9
RC4 and Related Ciphers
Now we are in line 23. We have an s
n probability that Xt+t′ was already assigned
to some S-box element. In this case, we reach a contradiction if k −S[t + t′] was
already assigned to some S-box element different from S[jt′]. If neither S[jt′] has an
assigned value nor Xt+t′ is assigned to some S-box element, we have (n−s) choices
for S[jt′], but only (n −s)(1 −s
n) do not lead to an immediate contradiction. Thus
we have the recursion:
c3

t′,s

= s
n
 s
n +

1 −s
n

c1

t′ + 1,s + 1

+

1 −s
n

(n −s)

1 −s
n

c1

t′ + 1,s + 2

.
(9.14)
The starting values for the recursion are c∗(t′,s) = 0 for s > n or t′ > s and
c∗(t′,n) = 1 for t′ ≤n. Solving the recurrence, we ﬁnd that the attack of the original
RC4 (n = 256) needs 2780 steps. Simulations for small values of n support our
analysis.
An improvement of the simple state recovering algorithm is the following idea
(see [182]): If we know S[i] we can compute the secret pointer j and simulate all
swaps. Assume we know the value S[i] for x consecutive steps. We call these x
consecutive steps the search window. Now we can guess an S[j] or k to proceed.
Instead of always choosing the ﬁrst unknown value as in the simple state recover-
ing attack (Algorithm 9.9), we choose the value that implies as many other values
as possible. This will reduce the number of necessary non-deterministic steps and
speed up the search. In addition, we could be lucky and the search window might
grow during the search. For details we refer to [182].
For a good start we should use a large search window. In [182] the authors suggest
using d-order, g-generative patterns for this purpose.
Deﬁnition 9.2 A d-order pattern is a quadruple (i,j,S) where S is a partial func-
tion Z/nZ →Z/nZ which gives the value of d S-box entries.
The pattern is g-generative if we can compute the secret pointer j for the next g
steps of the RC4 algorithm out of the pattern.
Table 9.1 illustrates a 7-generative pattern of order 4.
The largest pattern found in [182] is a 14-order, 76-generative pattern. If we
were able to start with such a pattern, the attack would be quite fast (about 2240
operations), but such a pattern occurs only every 256 · 256!/(256 −14)! ≈2120
output bytes, so it is near to impossible to ﬁnd this special pattern. For a practical
attack, we would need a huge number of good patterns, but at this time no one knows
how to ﬁnd or even store such a pattern set.
9.7 Other Attacks on RC4
An alternative class of attacks against RC4 searches for distinguishers between the
RC4 pseudo-random sequence and a true random sequence.

9.7
Other Attacks on RC4
213
Table 9.1 A 4-order,
7-generative pattern
i
j
S[0]
S[1]
S[2]
S[3]
S[4]
S[5]
S[6]
−1
−2
6
−1
2
∗
∗
*
−2
0
4
*
−1
2
∗
6
*
−2
1
3
*
∗
2
−1
6
*
−2
2
5
*
∗
*
−1
6
2
−2
3
4
*
∗
*
6
−1
2
−2
4
3
*
∗
*
−1
6
2
−2
5
5
*
∗
*
−1
6
2
−2
6
3
*
∗
*
−2
6
2
−1
The ﬁrst published result of this kind is due to J.Dj. Goli´c [110, 111]. He proves
that the sum of the last bits at time step t and t +2 is correlated to 1. The correlation
coefﬁcient is ≈15 · 2−24. He then concludes that 240 bytes of the RC4 pseudo-
random sequence are distinguishable from a true random sequence.
9.7.1 Digraph Probabilities
Better distinguishers are based on the computation of digraph probabilities.
In [95] we ﬁnd an analysis of digraph probabilities. Under the assumption that
the internal state is random, we can compute the exact digraph probabilities by the
following state counting algorithm.
Algorithm 9.10 Computing the digraph probabilities
1: for i from 0 to n −1 do
2:
for j from 0 to n −1 do
3:
{Loop over all possible values for i and j in line 9 of Algorithm 9.2.}
4:
Choose non-deterministic S[i]
5:
Choose non-deterministic S[i + 1]
6:
Choose non-deterministic S[j] if j /∈{i,i + 1}
7:
Compute k = S[i] + S[j], i + 1 and j′ = j + S[i]
8:
Choose non-deterministic S[j′] if j′ /∈{i,i + 1,j}
9:
Compute k′ = S[i + 1] + S[j′]
10:
for all possible values of S[k] and S[k′] do
11:
Increase P[i,S[k],S[k′]] by 6−|{i,i+1,j,j′,k,k′}|
i=1
(n −6 + i)
12:
end for
13:
end for
14: end for
A naive implementation of Algorithm 9.10 would require a loop for every
“Choose” which results in a run time of O(n8). For n = 256, this is still in the

214
9
RC4 and Related Ciphers
range of modern computers, but takes too long. Therefore the authors of [95] did
the calculation only for n = 8,16,32 and extrapolated the results to n = 256.
However, it is possible to transform Algorithm 9.10 into a much faster program.
To that end we have to successively apply the following transformation steps.
• If A and B are two independent program fragments we can replace
for i ∈A do A, B end for
by
for i ∈A do A end for, for i ∈A do B end for.
• We can replace
for i ∈A do if i /∈B then A end if end for
by
for i ∈A\B do A end for.
Similarly we can remove all if-statements after a for-statement.
We use these two rules to separate in the computation of Algorithm 9.10 the
general case where i, i + 1, j, j′, k and k′ are all different from the special cases in
which some values coincide. The algorithm then looks like Algorithm 9.11.
As we can see, the 8 nested loops give a running time of O(n8). The special cases
have at most 7 nested loops and contribute only an O(n7) to the total running time.
Now we use the following transform to reduce the number of nested loops.
• We can transform
1: for i ∈A do
2:
for j ∈B\{f (i)} do
3:
A
4:
end for
5: end for
to
1: for j ∈B do
2:
for i ∈A\{f −1(j)} do
3:
A
4:
end for
5: end for
This transform allows us to change the order of the for-loops.
• We can replace the loop
for i ∈A do Increase counter by k end for
by the single instruction
Increase counter by k · |A|.

9.7
Other Attacks on RC4
215
Algorithm 9.11 Computing the digraph probabilities (1. Transformation)
1: {Do the general case ﬁrst}
2: for i from 0 to n −1 do
3:
for j ∈{0,...,n −1}\{i,i + 1} do
4:
for S[i] ∈{0,...,n −1}\{i −S[i],i + 1 −S[i]} do
5:
for S[i + 1] ∈{0,...,n −1}\{S[i]} do
6:
for S[j] ∈{0,...,n −1}\{S[i],S[i + 1],i −S[i],i + 1 −S[i],j −
S[i]} do
7:
for S[j′] ∈{0,...,n−1}\{S[i],S[i +1],S[j],i −S[i +1],i +1−
S[i +1],j −S[i +1],(i +S[i])−S[i +1],(S[i]+S[j])−S[i +1]}
do
8:
for S[k] ∈{0,...,n −1}\{S[i],S[i + 1],S[j],S[j′]} do
9:
for S[k′] ∈{0,...,n −1}\{S[i],S[i + 1],S[j],S[j′],S[k′]}
do
10:
Increase P[i,S[k],S[k′]] by 1
11:
end for
12:
end for
13:
end for
14:
end for
15:
end for
16:
end for
17:
end for
18: end for
19: {Now do the special cases}
20: {This part is skipped}
We use the ﬁrst of these two rules to move the loop over S[j′] (line 7 of Algo-
rithm 9.11) into the loops over S[k] and S[k′] (line 8 and 9 of Algorithm 9.11). Then
we can use the second rule to replace the loop over S[j′] by a simple multiplication.
In our case lines 7 to 13 of Algorithm 9.11 are transformed to Algorithm 9.12.
Algorithm 9.12 Computing the digraph probabilities (2. Transformation, inner
loops)
1: for S[k] ∈{0,...,n −1}\{S[i],S[i + 1],S[j]} do
2:
for S[k′] ∈{0,...,n −1}\{S[i],S[i + 1],S[j],S[k′]} do
3:
Increase counter P[i,S[k],S[k′]] by
|{0,...,n−1}\{S[i],S[i +1],S[j],S[k],S[k′],i −S[i +1],i +1−S[i +
1],(i + S[i]) −S[i + 1],(S[i] + S[j]) −S[i + 1]}|
4:
end for
5: end for
This saves us one loop and reduces the running time to O(n7).

216
9
RC4 and Related Ciphers
Table 9.2 Digraph
probabilities of RC4
Digraph
Value(s) of i
Difference from the
uniform distribution
(0,0)
i = 1
+7.81 h
(0,0)
i ̸= 1,255
+3.91 h
(0,1)
i ̸= 0,1
+3.91 h
(i + 1,255)
i ̸= 254
+3.91 h
(255,i + 1)
i ̸= 1,254
+3.91 h
(255,i + 2)
i ̸= 0,253,254,255
+3.91 h
(255,0)
i = 254
+3.91 h
(255,1)
i = 255
+3.91 h
(255,2)
i = 0,1
+3.91 h
(129,129)
i = 2
+3.91 h
(255,255)
i ̸= 254
−3.91 h
(0,i + 1)
i ̸= 0,255
−3.91 h
This is not the end. We can again split the loop into the general part in which
S[i],S[i + 1],S[j],S[k],S[k′],i −S[i + 1],i + 1 −S[i + 1] are all different and
into the special part in which some of the values coincide. Then we apply the above
transformation again to reduce the running time to O(n6).
Repeating these steps again and again we ﬁnally reach an O(n3) implementation
of Algorithm 9.10, but the constant will be extremely large. Taking this into ac-
count, and that the transformation itself costs time, we will stop the transformation
as soon as we reach an O(n5) algorithm. This is fast enough to compute the digraph
probabilities for n = 256. Table 9.2 shows the largest differences from the uniform
distribution.
A very interesting observation on digraph probabilities is the following.
Theorem 9.13 (Mantin [176]) For small g, the probability of the pattern ABSAB
where S is a word of length g is
≈

1 + e(4−8g)/n
n
 1
n2 .
Proof Suppose that the following happens: Suppose further that in the g −2 time
steps from t + 1 to t + g −2, none of the 8 S-box entries displayed in Fig. 9.6 are
changed. Then the computation goes as shown in Fig. 9.6 and produces the pattern
ABSAB.
The probability that all these conditions are satisﬁed is
1
n3

1 −g + 4
n
2
1 −6
n
g−2
.

9.7
Other Attacks on RC4
217
Fig. 9.6 Digraph repetition
( 1
n3 is the probability that jt−1 = t + g −1, jt+g−1 = t −1 and St−1[t] = 1. Neither
X + Y nor Z + 1 may lie in the interval [t −1,t + g + 2], which gives us the
probability (1−g+4
n )2. The probability that j is never X+Y or Z+1 is (1−6
n)g−2.)
Using the approximation (1 −6
n)g−2 ≈e6(g−2)/N and (1 −g+4
n ) ≈e(g+4)/N we
ﬁnd that our special event has probability e(4−8g)/n
n
n−3.
If the special event does not happen, the pattern can still occur by chance (with
probability 1
n2 ), which proves the theorem.
□
Theorem 9.13 has a very high bias. This gives a good distinguishing attack.
I want to close this section with a research problem. From simulations with small
values of n, I found the following correlation.

218
9
RC4 and Related Ciphers
Fig. 9.7 Example of a
3-fortuitous state
• If we observe the output t −1 at time step t, then we may predict with probability
1
n + 1
n2 the output 1 at time step t + n.
However, I have no proof for it and I have not found an application in an attack.
9.7.2 Fortuitous States
Another very interesting idea is the notion of fortuitous states introduced in [95].
Deﬁnition 9.3 A state i, j, S of RC4 is called k-fortuitous if the next k steps of the
RC4 algorithm use only the values S[i + 1],...,S[i + k].
The name ‘fortuitous state’ refers to the fact that such a state is a fortune for the
cryptanalyst. Figure 9.7 illustrates a 3-fortuitous state.
We can ﬁnd fortuitous states by Algorithm 9.13. Every time the computation
reaches a “Choose non-deterministic” (line 8 and 15), the computation branches to
cover all possible choices.
Every time we must choose a non-deterministic value for S[i] or S[j] respec-
tively, we have at most k choices which do not lead to an immediate contradiction.
Thus the worst case complexity of the search is O(n2kk). However, most often the
search will reach a dead end before we have assigned values to all k active S-box
elements, so the computation is much faster than the worst case bound suggests. Al-
gorithm 9.13 is good enough to enumerate k-fortuitous states for k ≤8. For larger k
it becomes very slow.

9.7
Other Attacks on RC4
219
Algorithm 9.13 Searching fortuitous states
1: for i from 0 to n −1 do
2:
for j from 0 to n −1 do
3:
{Search fortuitous states with pointers i, j}
4:
{The active part of the S-box is S[i + 1],...,S[i + k]}
5:
for k times do
6:
advance i by 1
7:
if S[i] has no assigned value then
8:
Choose non-deterministic S[i]
9:
end if
10:
j ←j + S[i]
11:
if S[j] is not an active part of S-box then
12:
Stop the search {Not a k-fortuitous state}
13:
end if
14:
if S[j] has no assigned value then
15:
Choose non-deterministic S[i]
16:
end if
17:
if S[k] with k ←S[i] + S[j] is not an active part of S-box then
18:
Stop the search {Not a k-fortuitous state}
19:
end if
20:
Swap S[i] and S[j]
21:
end for
22:
Output i, j and S {k-fortuitous state found}
23:
end for
24: end for
The search can be improved signiﬁcantly. There are two ideas in the new algo-
rithm:
1. In the ﬁrst pass, we do not choose a value for S[j]. We only store restrictions.
2. In the ﬁrst pass, we have to choose values for S[i] and calculate the pointers j.
These operations are translation invariant, so we can do it in parallel for all i.
These ideas lead to the new Algorithm 9.14.
Algorithm 9.14 is a bit complicated. We now describe it in detail.
The algorithm stores in the pair (m,M) the smallest and the largest value of k, i.e.
all S[i] + S[j] lie in the interval [m,M]. Since we are working in Z/nZ, the terms
“smallest” and “largest” are a bit ambiguous. It is entirely possible that m = n −1
and M = 1, which means that all k are either n −1, 0 or 1. As long as r < n/2 we
will have no difﬁculty detecting which of two given numbers is the “largest”. At the
start the values (m,M) are undeﬁned. Since we are searching for an r-fortuitous
state, the values k have to be in the range [i,i + r −1] for some i, i.e. M −m can
be at most r. In line 11, we use this to cut off branches in the search tree.
Similarly to (m,M), we keep for every position S[i] the smallest value m[i] and
the largest value M[i] that was added to S[i].

220
9
RC4 and Related Ciphers
Algorithm 9.14 Enumerating fortuitous states (fast)
1: i := n −1
2: for j from 0 to n −1 do
3:
{Find possible S[i]}
4:
{The active part of the S-box is S[1],...,S[r]}
5:
for r times do
6:
advance i by 1
7:
if S[i] has no assigned value then
8:
Choose non-deterministic S[i]
9:
if m[i] and M[i] have values then
10:
(m,M) := (min{m,m[i] + S[i]},max{M,M[i] + S[i]})
11:
Stop the search if M −m ≥r {Not an r-fortuitous state}
12:
end if
13:
end if
14:
j := j +S[i]
15:
if S[j] is not an active part of the S-box then
16:
Stop the search {Not an r-fortuitous state}
17:
end if
18:
if S[j] has an assigned value then
19:
(m,M) := (min{m,S[i] + S[j]},max{M,S[i] + S[j]})
20:
Stop the search if M −m > r {Not an r-fortuitous state}
21:
else
22:
(m[j],M[j]) := (min{m,S[i]},max{M,S[i]})
23:
Stop the search if M[j] −m[j] ≥r {Not an r-fortuitous state}
24:
end if
25:
Swap((S[i],m[i],M[i]) ; (S[j],m[j],M[j]))
26:
end for
27:
{We have found suitable values for S[i].}
28:
{Now loop over all i and translate the S-box.}
29:
for i from M −r to m do
30:
{loop from 0 to n −1 if (m,M) has no value}
31:
for x from 0 to r −1 do
32:
if S[x] has no assigned value then
33:
Choose non-deterministic S[x] ∈{i −m[x],...,i + r −1 −M[x]}
34:
if No value for S[x] left then
35:
Stop the search {Not an r-fortuitous state}
36:
end if
37:
end if
38:
end for
39:
Output fortuitous state found i −1, j + i −1, S→i
40:
{S→i denotes the S-box shifted by i places to the right}
41:
end for
42: end for

9.7
Other Attacks on RC4
221
When we reach line 27, we have found a partial state of RC4 with the following
properties: We start with i = n −1. In the next r steps the values j will be in the
range [0,r −1]. We can translate this partial state by a, just by adding a to all
pointers and shifting the S-box to the right.
The loop in line 29 runs over all possible shifts for which all values k are in the
range [i,i + r −1], i.e. for which [m,M] ⊆[i,i + r −1].
Up to now we have assigned values only to the S-box elements S[i] which we
needed for computing the pointer j. Now we have to assign values to all other S-
box elements S[x]; this is done in lines 32–38. Since we have collected all necessary
information to decide which values are allowed in the previous steps, this part is fast.
Algorithm 9.14 still visits all fortuitous states. If our only goal is just to count
them, we can improve the running time by replacing the enumeration in lines 29–
42. When we reach line 29, we have to solve the following counting problem:
Given ﬁnite sets A1,...,Ar, what is the number of r-tuples (x1,...,xr) with
xi ∈Ai and xi ̸= xj for i ̸= j. In Algorithm 9.14, we enumerate these r-tuples.
There is no closed formula for the number N(A1,...,Ar) of such r-tuples, but
we can count them by using a special case of the Möbius inversion
N(A1,...,Ar) =

p is a partition of {1,...,r}
c(p)

π is a part of p

0
i∈π
Ai
.
(9.15)
The coefﬁcients c(p) satisfy the recurrence
c

{1},...,{r}

= 1
and

p′ is a reﬁnement of p
c

p′
= 0
for all partitions p different from {{1},...,{r}}.
What happens in Eq. (9.15) becomes clear if we look at the special case r = 3.
N(A1,A2,A3) = |A1| · |A2| · |A3| −|A1 ∩A2| · |A3| −|A1 ∩A3| · |A2|
−|A2 ∩A3| · |A1| + 2|A1 ∩A2 ∩A3|.
The ﬁrst term just counts all triples, the next three terms subtract the number of
triples in which two of the three values are even. The last term corrects the error we
made when we subtracted the triples of the form (x,x,x) three times.
The coefﬁcients c(p) are known as Möbius functions of the lattice of all parti-
tions with respect to reﬁnement. For this lattice one can prove c((π1,...,πk)) =
k
j=1(−1)|πk|(|πk| −1)! (see, for example, [256] Sect. 3.10).
What makes the fortuitous state a fortune for the cryptanalyst is the following:
Let us take the 3-fortuitous state of Fig. 9.7. Under the assumption that every
internal state has the same probability we know that the initial condition holds with
probability (256 · 256 · 255 · 254)−1. Every time the initial condition is satisﬁed, the
next three output bytes will be 1,0,1. The probability of generating this speciﬁc
output sequence is about
1
2563 . This means that the probability that the sequence
1,0,1 is caused by the fortuitous state is ≈2563/(256 · 256 · 255 · 254) ≈
1
253. For

222
9
RC4 and Related Ciphers
Table 9.3 The number of
fortuitous states and their
expected occurrence
Length
Number
Frequency
Expected
False Hits
2
516
22.9
255
3
290
31.8
253
4
6540
35.2
250
5
25,419
41.3
246
6
101,819
47.2
241
7
1,134,639
51.8
236
8
14,495,278
56.1
229
9
169,822,607
60.5
222
10
1,626,661,109
65.1
214
11
18,523,664,501
69.6
205
12
233,003,340,709
73.9
197
13
3,250,027,739,130
78.0
187
14
46,027,946,087,463
82.1
178
15
734,406,859,761,986
86.0
169
16
12,652,826,949,516,042
93.1
159
the attacker this means that if he observes the output sequence 1,0,1 beginning at a
time with i = 2, he can conclude S[2] = 2, S[3] = 0, S[1] = 1 with probability
1
253,
which is much more than the probability
1
2563 we would expect for a random guess.
In Table 9.3 we list the number of k-fortuitous states together with the logarithm
of the expected frequency (base 2) and the expected number of false hits. By a false
hit, we mean that we observe an output pattern, which looks like the output of a
fortuitous state, but which is not created by a fortuitous state.
As one can see, the occurrence rate of fortuitous states is not high. Even when
one occurs and the attacker has guessed correctly a part of the internal state it is
not clear how to proceed. The internal state of RC4 is big enough that even the
knowledge of 8 bytes of the S-box does not seem very helpful.
The idea of fortuitous states can be extended to non-successive outputs or to
states in which a known S-box values allow the prediction of b outputs (see [177,
209]).
9.8 RC4 Variants
9.8.1 An RC4 Variant for 32-Bit Processors
RC4 is designed for 8-bit processors. This is good for embedded devices but on a
modern computer we would like to have a cipher that uses 32-bit or even 64-bit
words. An interesting RC4 variant that has this property is described in [116].

9.8
RC4 Variants
223
The reason why we cannot extend RC4 directly to 32 bits is that for n = 232
the S-box would be far too large. Algorithm 9.15 and Algorithm 9.16 solve this
problem by introducing a new parameter. The S-box still has only n elements, but
all elements are taken in Z/mZ for some multiple of n. The authors suggest taking
n = 256 and m either 232 or 264.
Algorithm 9.15 RC4(n,m) key scheduling
1: {initialization}
2: for i from 0 to N −1 do
3:
S[i] ←ai
4: end for
5: j ←0, k ←0
6: {Shufﬂe S}
7: for counter from 1 to r do
8:
for i from 0 to N −1 do
9:
j ←(j + S[i] + K[i mod l]) mod n
10:
Swap(S[i],S[j])
11:
S[i] ←S[i] + S[j] mod m
12:
k ←k + S[i] mod m
13:
end for
14: end for
Algorithm 9.16 RC4(n,m) pseudo-random generator
1: {initialization}
2: i ←0, j ←0
3: {Generate pseudo-random sequence}
4: loop
5:
i ←i + 1 mod n
6:
j ←j + S[i] mod n
7:
k ←k + S[j] mod m
8:
output S[S[i] + S[j] mod n] + k mod m
9:
S[S[i] + S[j] mod n] = k + S[i] mod m
10: end loop
Note that the key scheduling is repeated r times, which should avoid attacks that
explore the weak key scheduling. In [116] the authors use methods similar to that
of Sect. 9.3.2 to determine good values for the parameter r in the key scheduling.
They advise taking r = 20 for m = 232 and r = 40 for m = 264.
The biggest difference in comparison to the original RC4 is the counter k which
is used during the update of the S-box and to obfuscate the output. Perhaps the best
way to understand the role of k is to look at a preliminary version of Algorithm 9.16
which had no such counter (see Algorithm 9.17).

224
9
RC4 and Related Ciphers
Algorithm 9.17 RC4(n,m) pseudo-random generator, old version from [121]
1: {initialization}
2: i ←0, j ←0
3: {Generate pseudo-random sequence}
4: loop
5:
i ←i + 1 mod n
6:
j ←j + S[i] mod n
7:
Swap(S[i],S[j])
8:
output S[S[i] + S[j] mod n]
9:
S[S[i] + S[j] mod n] ←S[i] + S[j] mod m
10: end loop
Algorithm 9.17 has a serious ﬂaw that allows strong distinguishers [284]. The
problem is the update S[S[i] + S[j] mod n] = S[i] + S[j] mod m. It is possible
that S[i], S[j] and S[S[i] + S[j] mod n] are selected as outputs. So we expect to
ﬁnd three values a, b, c with c = a + b in the output sequence of Algorithm 9.17
much more frequently than we would expect in a true random sequence.
Another problem of Algorithm 9.17 is that it is susceptible to attacks based on
Goli´c’s correlation [148].
9.8.2 RC4A
Another interesting variant of the RC4 algorithm is the RC4A algorithm (Algo-
rithm 9.18) described in [209]. The idea is to use two S-boxes in “parallel” to obtain
a stronger algorithm.
In each step, RC4A produces two output bytes. It advances i only once per two
output bytes, i.e. it needs one fewer instruction per two output bytes in comparison
to RC4. In addition, RC4A can easily be parallelized, since most instructions touch
only one of the two S-boxes.
RC4A is immune to most attacks that work against RC4. The most dangerous
attack is based on a variant of Goli´c’s correlation.
Theorem 9.14 Assume that the permutations S1 and S2 are chosen independently
and uniformly at random. Then:
P

S1[j1] + S1[k1] + S2[j2] + S2[k2] ≡2i
mod n

=
1
n −1.
(9.16)
Proof The proof is very similar to the proof of Theorem 9.12.
We use k2 ≡S1[i] + S1[j1] mod n and k1 ≡S2[i] + S2[j2] mod n to write
S1[j1] + S1[k1] + S2[j2] + S2[k2] ≡2i
mod n

9.8
RC4 Variants
225
Algorithm 9.18 RC4A pseudo-random generator
1: {initialization}
2: i ←0
3: j1 ←0, j2 ←0
4: {generate pseudo-random sequence}
5: loop
6:
i ←(i + 1) mod n
7:
j1 ←(j1 + S1[i]) mod n
8:
Swap S1[i] and S1[j]
9:
k2 ←(S1[i] + S1[j]) mod n
10:
print S2[k2]
11:
j2 ←(j2 + S2[i]) mod n
12:
Swap S2[i] and S2[j]
13:
k1 ←(S2[i] + S2[j]) mod n
14:
print S1[k1]
15: end loop
as

k1 + S1[k1]

+

k2 + S2[k2]

≡

i + S1[i]

+

i + S2[i]

mod n.
Now we count the states that satisfy this condition. For this, we have to distin-
guish between several cases:
1. k1 = k2 = i
In this case, we may choose S1 and S2 without any restriction. Thus we have
(n!)2 possible combinations.
2. k1 = i, k2 ̸= i
In this case, we may choose S1 as we want (n! possibilities). Next we may choose
k2 (n −1 possibilities), then we have to choose S2[i] (n possibilities) which
determines S2[k2]. Finally, we may choose the remaining part of S2 ((n −2)!
possibilities). Altogether we have (n!)((n −1)n(n −2)!) = (n!)2 possibilities in
this case. (Compare this with Theorem 9.12.)
3. k1 ̸= i, k2 = i
This is analogous to the previous case, with (n!)2 possibilities.
4. k1 ̸= i, k2 ̸= i
We distinguish between two subcases:
(a) k1 + S[k1] ≡i + S1[i] mod n
This is equivalent to k2 + S[k2] ≡i + S2[i] mod n.
We may count the number of possibilities for both S-boxes separately. As
in Theorem 9.12 or in the two cases above, we ﬁnd (n!)2 possibilities in this
case.
(b) k1 + S[k1] ̸≡i + S1[i] mod n
In this case, we may ﬁrst choose k1 (n possibilities) and S1[i] (n possibil-
ities). Now we choose S1[k1] (n −2 possibilities, since S1[k1] ̸= S1[i] and

226
9
RC4 and Related Ciphers
k1 + S[k1] ̸≡i + S1[i] mod n by assumption). There are (n −2)! possibili-
ties left to choose the remaining part of S1.
By assumption k2 ̸= i. Furthermore, (k1 + S[k1]) + k2 ̸≡(i + S1[i]) + i
mod n, since S2[k2] ̸= S2[i]. Thus we have n −2 possibilities to choose k2.
Now we may choose S2[i] (n possibilities), which determines S2[k2]. For the
remaining part of S2 we have (n −2)! possibilities.
This makes [(n −1)n(n −2)(n −2)!][(n −2)n(n −2)!] possibilities in
this case.
Altogether there are n · n!(n −2)! internal states with

k1 + S1[k1]

+

k2 + S2[k2]

≡

i + S1[i]

+

i + S2[i]

mod n.
Since the total number of possible internal states is (n · n!)2, this proves the theo-
rem.
□
There are also analogs to Eqs. (9.5), (9.6) and (9.7) of Theorem 9.12.
The correlation of Theorem 9.14 is weaker than Goli´c’s correlation, but it is still
strong enough to be exploited in an attack.
Theorem 9.14 allows us to estimate S1[j1] + S2[j2] mod n. Whether we are
able to use this information to obtain the main key depends on the key scheduling
algorithm.
Suppose we have two independent subkeys K1 and K2 and use these subkeys
with the normal RC4 key scheduling algorithm (Algorithm 9.1) to initialize S1 and
S2 respectively. Then we can use Theorem 9.14 and the technique of our 1-round
attack to compute a relation between K1 and K2. This reduces the effective key
length by 1
2.
If one generates K2 from K1 by a simple RC4 algorithm as suggested in [209],
we can mount the following attack. Assume that the initialization vector precedes
the main key. With a chosen initialization vector, we may enforce that the ﬁrst two
bytes of K2 depend in a simple way on the ﬁrst two bytes of the main key. (Use ini-
tialization vectors similar to the one used in the FMS-attack.) Now we analyze the
ﬁrst two output bytes of the RC4A pseudo-random generator with Theorem 9.12.
S1[j] depends only on the ﬁrst two bytes of K1. These bytes are part of the ini-
tialization vector which are known to the attacker. Thus the attacker may compute
S2[j2] from the observed values. This byte depends only on the ﬁrst two bytes of
the main key. The attack now follows the pattern of attack described in Sect. 9.5.1.
For the attack described above, more information about the initialization vector
is needed than for the FMS-attack, furthermore the correlation is rather weak. This
means that it needs an unrealistically high number of observed sessions.
As a conclusion of this subsection, we may say: RC4A has weaknesses that are
very similar to the weaknesses of RC4, but the consequences are not so dramatic.

9.8
RC4 Variants
227
9.8.3 Modiﬁcations to Avoid Known Attacks
A common design principle in cryptography is to study successful attacks and only
make small changes to the algorithm to avoid attacks of these classes. In this section,
we collect some ideas for improvements of RC4 that are based on this idea.
The chosen IV attacks use the fact that the ﬁrst bytes of the S-box depend with
high probability only on the ﬁrst bytes of the session key. Besides discarding the
ﬁrst output bytes or iterating the key scheduling, we can simply let i run from n −1
down to 0 in the key scheduling, i.e. we use Algorithm 9.19.
Algorithm 9.19 RC4 key scheduling
1: {initialization}
2: for i from 0 to n −1 do
3:
S[i] ←i
4: end for
5: j ←0
6: {generate a random permutation}
7: for i from n −1 down to 0 do
8:
j ←(j + S[i] + K[i mod l]) mod n
9:
Swap S[i] and S[j]
10: end for
Besides preventing chosen IV attacks, Algorithm 9.19 also slows down attacks
that are based on Goli´c’s correlation (see [148]).
Goli´c’s correlation says that P(i + S[i] ≡k + S[k] mod n) = 2
n. We cannot
change this probability but we can modify RC4 in such a way that it is less use-
ful to the attacker. For example:
• We move in Algorithm 9.2 the swap of S[i] and S[j] behind the output of S[k].
Now the attacker still has the correlation P(S[j] + S[k] ≡i mod n) = 2
n, but he
has less control over S[j] (see [148]).
• We compute k as a(S[i] + S[k]) for some a ̸= 1 which is coprime to n. Now the
attacker cannot remove S[i] from
P

i + S[i] ≡a

S[i] + S[j]

+ S[k] mod n

= 2
n.
Having all three values S[i], S[j] and S[k] in the equation makes it much less
useful.
Another suggestion, made by G.K. Paul in his PhD thesis [208], is to use Al-
gorithm 9.20 to obtain the initial permutation. He gives several simulations and
theoretical results to support his suggestion.
The algorithm also uses random swaps, but changes the order in which the cards
are swapped. Experiments indicate that Algorithm 9.20 removes most irregularities
from the RC4 key shufﬂe.

228
9
RC4 and Related Ciphers
Algorithm 9.20 Paul’s suggestion for key scheduling
1: {initialization}
2: for i from 0 to n −1 do
3:
S[i] ←i
4: end for
5: j ←0
6: for i from 0 to n −1 do
7:
j ←(j + S[i] + K[i]) {K is the main key}
8:
Swap S[i] and S[j]
9: end for
10: for i down from n/2 −1 to 0 do
11:
j ←(j + S[i]) ⊕(K[i] + IV [i]) {IV initialization vector (session id)}
12:
Swap S[i] and S[j]
13: end for
14: for i from n/2 to n −1 do
15:
j ←(j + S[i]) ⊕(K[i] + IV [i]) {IV initialization vector (session id)}
16:
Swap S[i] and S[j]
17: end for
18: for y from 0 to n −1 do
19:
if y is even then
20:
i ←y/2
21:
else
22:
i ←n −(y + 1)/2
23:
end if
24:
j ←(j + S[i] + K[i])
25:
Swap S[i] and S[j]
26: end for

Chapter 10
The eStream Project
The ECRYPT Stream Cipher Project [85] was run from 2004 to 2008 to identify
a portfolio of promising new stream ciphers. Currently none of these ciphers has
proved itself in a widespread application, but all of them show state of the art devel-
opments in stream cipher design. Since there is already a whole book [226] devoted
to the eStream ﬁnalists, we keep this chapter short and describe only three examples.
10.1 Trivium
Trivium is a synchronous stream cipher designed by C. De Cannière and B. Pre-
neel [75]. It is optimized for hardware implementations. One design goal was to
achieve the security with minimal effort, so the cipher has no large security mar-
gins.
Trivium uses 80-bit keys and 80-bit initialization vectors. The internal state is
has size 288 bits.
The key stream generation is described by Algorithm 10.1. Figure 10.1 shows a
graphical representation of the key stream generation process.
The design is based on the following key ideas:
• Trivium should have a compact implementation to allow low power applications.
Thus it is based on shift registers. Since non-linear combinations of LFSRs need
larger internal states, Trivium uses non-linear feedback functions, even if this
makes the cipher harder to analyze.
• To allow high speed implementations it is necessary that Trivium is parallelizable.
After an internal state of Trivium is changed, it is not used for at least 64 steps.
Therefore a hardware implementation of Trivium can generate 64 bits of output
per clock cycle. Such an implementation needs only 288 ﬂip-ﬂops, 192 AND-
gates and 704 XOR-gates. In comparison to the bit-oriented implementation with
288 ﬂip-ﬂops, 3 AND-gates and 11 XOR-gates this is quite cheap. The price is
that Trivium uses sparse update functions, which is always risky.
A. Klein, Stream Ciphers, DOI 10.1007/978-1-4471-5079-4_10,
© Springer-Verlag London 2013
229

230
10
The eStream Project
Algorithm 10.1 Trivium key stream generation
1: loop
2:
t1 ←s66 + s93
3:
t2 ←s162 + s177
4:
t3 ←s243 + s288
5:
output t1 + t2 + t3
6:
t1 ←t1 + s91s92 + s171
7:
t2 ←t2 + s175s176 + s264
8:
t3 ←t3 + s286s287 + s69
9:
(s1,s2,...,s93) ←(t3,s1,...,s92)
10:
(s94,s95,...,s177) ←(t1,s94,...,s176)
11:
(s178,s179,...,s288) ←(t2,s178,...,s287)
12: end loop
Fig. 10.1 The cipher Trivium
The key scheduling of Trivium is also quite simple (Algorithm 10.2). It writes
the main key and the initialization vector into the internal memory. To avoid related
key attacks the registers are clocked for four full cycles.
Even two full cycles are enough to guarantee that every bit of the internal state
depends on every bit of the key. So four rounds seem to be sufﬁcient to mask the
main key.
The output of Trivium can be described by sparse equations, but the degree of
these equations increases fast enough to make the standard linearization techniques

10.1
Trivium
231
Algorithm 10.2 Trivium key scheduling
1: (s1,...,s93) ←(K1,...,K80,0,...,0)
2: (s94,...,s177) ←(IV1,...,IV80,0,...,0)
3: (s178,...,s288) ←(0,...,0,1,1,1)
4: for i from 1 to 4 · 288 do
5:
t1 ←s66 + s93 + s91s92 + s171
6:
t2 ←s162 + s177 + s175s176 + s264
7:
t3 ←s243 + s288 + s286s287 + s69
8:
(s1,s2,...,s93) ←(t3,s1,...,s92)
9:
(s94,s95,...,s177) ←(t1,s94,...,s176)
10:
(s178,s179,...,s288) ←(t2,s178,...,s287)
11: end for
impracticable. At the moment no practical algebraic attack is known. The best attack
of this type is a so-called cube-attack [80].
The idea of a cube-attack is to shift computation from the attack into pre-
processing by using chosen initialization vectors. Let v denote the initialization
vector and let x be the key. Let {vi1,...,vik} be a part of the initialization vector.
Assume that you have an algebraic relation of the form
F(v,x) = vi1 ···vikf (v,x) + g(v,x) = 0
(10.1)
where f (v,x) does not depend on vi1,...,vik and g(v,x) contains no monomial
divisible by vi1 ···vik.
Assume that the attacker can observe 2k sessions in which the initialization vector
takes all possible values in {vi1,...,vik} and is ﬁxed in all other positions, then
summing over the 2k possibilities gives the equation
v=v(2k)

v=v(1)
F(v,x) = f (v,x) = 0.
The new equation is of course of much lower degree than the old one.
The value of a cube-attack should not be overestimated, since a good cipher does
not allow one to ﬁnd many equations of the form (10.1) (see [23]). If you are able
to ﬁnd enough equations for a cube-attack it is quite probable that a straightforward
algebraic attack will succeed anyway. The real advantage of the cube-attack is that
the search for equations of the form (10.1) can be done in a pre-processing phase,
i.e. it can afford a lot of computation without slowing down the actual attack.
Now consider a variation of Trivium that is clocked only 672 times instead of
1152 times during the key scheduling phase (see line 4 of Algorithm 10.2). Note that
in the reduced variant every bit of the internal state already depends on every key bit,
but with the cube-attack one can even ﬁnd linear equations. For example, starting
with the set {v2,v13,v20,v24,v43,v46,v53,v55,v57,v67} we obtain the linear term

232
10
The eStream Project
f = 1 + x0 + x9 + x50, which corresponds to the 675 output bit (starting the count
with the 672 bits that are discarded during the key scheduling).
In [80] the authors give a list of 63 linear equation that can be obtained in this
way, which is enough to recover the 80 bit key by trying only 217 possibilities. Thus
the reduced Trivium variant is broken.
However, the non-linear elements in Trivium increase the degree of the equations
quickly and there is no obvious way to extend the attack. Even for the reduced
Trivium, the attack seems less practical, since the attacker must have access to an
enormous number of sessions. Assume that the initialization vector is just a counter.
Then you must wait for 267 +257 +···+22 ≈1.4·1020 sessions before you observe
a full cube of type {v2,v13,v20,v24,v43,v46,v53,v55,v57,v67}. If the initialization
vector is pseudo-random, it is very unlikely that you can get such a cube in shorter
time. The protocol will most likely force a key change earlier.
Even with all these problems, the cube-attack is probably the most dangerous
algebraic attack against Trivium.
Perhaps the most likely attack against Trivium would be of the state recov-
ering type. The straightforward attack guess (s25,...,s93), (s97,...,s177) and
(s244,...,s288) and the rest of the internal state can be immediately determined
from the keystream. This reduces the effective internal state to 195 bits. It is quite
possible that this can be reduced further, but it is still far from the 80 bit key size.
10.2 Rabbit
Rabbit is a synchronous stream cipher developed by M. Boesgaard, M. Vesterager,
T. Peterson, J. Christiansen and O. Scavenius [34]. Rabbit works internally on 32-bit
words which makes it suitable for software implementation.
The internal state consist of eight 32-bit variables xj,t (0 ≤j < 8) and eight 32-
bit counters cj,t (0 ≤j < 8). Here t denotes the number of iterations. In addition, a
carry bit φ7,t has to be stored during the iterations. Altogether the internal state has
size 513 bits.
The core of the Rabbit algorithm is the next-state function.
Let
gj,t =

(xj,t + cj,t+1)2 ⊕

(xj,t + cj,t+1)2 ≫32

mod 232
where ⊕denotes the XOR operation and all additions are done modulo 232.
The internal states xj,t are updated according to
x0,t+1 = g0,t + (g7,t ≪16) + (g6,t ≪16),
x1,t+1 = g1,t + (g0,t ≪8) + g7,t,
x2,t+1 = g2,t + (g1,t ≪16) + (g0,t ≪16),
x3,t+1 = g3,t + (g2,t ≪8) + g1,t,

10.2
Rabbit
233
Fig. 10.2 The cipher Rabbit
x4,t+1 = g4,t + (g3,t ≪16) + (g2,t ≪16),
x5,t+1 = g5,t + (g4,t ≪8) + g3,t,
x6,t+1 = g6,t + (g5,t ≪16) + (g4,t ≪16),
x7,t+1 = g7,t + (g6,t ≪8) + g5,t.
(Here ≪denotes left rotation.)
Figure 10.2 gives a graphic impression of the next-state function.
The counters are updated according to:
c0,t+1 = c0,t + a0 + φ7,t
mod 232,
c1,t+1 = c1,t + a1 + φ0,t+1
mod 232,
c2,t+1 = c2,t + a2 + φ1,t+1
mod 232,
c3,t+1 = c3,t + a3 + φ2,t+1
mod 232,
c4,t+1 = c4,t + a4 + φ3,t+1
mod 232,
c5,t+1 = c5,t + a5 + φ4,t+1
mod 232,
c6,t+1 = c6,t + a6 + φ5,t+1
mod 232,
c7,t+1 = c7,t + a7 + φ6,t+1
mod 232,

234
10
The eStream Project
where φj,t+1 is the carry bit when computing cj,t+1, i.e.
φj,t+1 =
⎧
⎪⎨
⎪⎩
1
if j = 0 and c0,t + a0 + φ7,t > 232,
1
if j ̸= 0 and cj,t + aj + φj−1,t+1 > 232,
0
otherwise.
The constants aj are
a0 = 0x4D34D34D,
a1 = 0xD34D34D3,
a2 = 0x34D34D34,
a3 = 0x4D34D34D,
a4 = 0xD34D34D3,
a5 = 0x34D34D34,
a6 = 0x4D34D34D,
a7 = 0xD34D34D3.
At each time step 128 output bits are extracted from the internal state. The ex-
traction is done according to
s[15...0]
t
= x[15...0]
0,t
⊕x[31...16]
5,t
,
s[31...16]
t
= x[31...16]
0,t
⊕x[15...0]
3,t
,
s[47...32]
t
= x[15...0]
2,t
⊕x[31...16]
7,t
,
s[63...48]
t
= x[31...16]
2,t
⊕x[15...0]
5,t
,
s[79...64]
t
= x[15...0]
4,t
⊕x[31...16]
1,t
,
s[95...80]
t
= x[31...16]
4,t
⊕x[15...0]
7,t
,
s[111...96]
t
= x[15...0]
6,t
⊕x[31...16]
3,t
,
s[127...112]
t
= x[31...16]
6,t
⊕x[15...0]
1,t
.
Here x[15...0]
j,t
denotes the rightmost 16 bits of xj,t and similarly x[31...16]
j,t
denotes
the leftmost 16 bits. The numbers in the brackets indicate the position of the ex-
tracted bits.
Rabbit borrows ideas from chaos theory, which explains the use the square func-
tion as non-linear part. The idea of using the square function for pseudo-random
generation dates back to the (very weak) von-Neumann-generator, which takes the
middle digits of the square as next state function. The combination of the highest
and the lowest bits, as done in Rabbit, provides better results (smaller correlation
coefﬁcients).
The counter system is used to guarantee a large cipher period, based on a tech-
nique developed in [242].
The design of Rabbit is based on 32-bit registers, but it can be also be efﬁ-
ciently implemented on 8-bit processors. The only critical operation is computing
the square of a 32-bit integer. In 8-bit arithmetic this requires 10 multiplications,
which is still good.
The simple symmetric structure of Rabbit also helps in hardware implementa-
tions. In the design paper the authors mention a design with 4100 gates and a speed
of 500 MBit/s (optimized for size) and a design with 100000 gates and a speed of
12.4 GBit/s (optimized for speed). This is much better than a modern block cipher,
but lies far behind a stream cipher designed for hardware implementation such as
Trivium.

10.3
Mosquito and Moustique
235
There are no known attacks against Rabbit. What currently comes closest to an
attack is a bias in the keystream of Rabbit found by J.-P. Aumasson [11]. It leads to
a distinguisher which requires 2254 bits of the keystream.
10.3 Mosquito and Moustique
Self-synchronizing stream ciphers are very rare. Only two proposals in the eStream
project were self-synchronizing stream ciphers. SSS [123] was a candidate in the
ﬁrst round which was broken by J. Daemen, J. Lano and B. Preneel [70].
The second candidate, Mosquito [68], was designed by J. Daemen and P. Kit-
sos. It was broken during round two of the eStream competition by A. Joux and
F. Muller [139]. Based on this experience, the authors of Mosquito designed the up-
dated variant Moustique [69], which removes some ﬂaws in the cipher. Moustique
was broken during the third and ﬁnal round of the eStream project by E. Käsper, V.
Rijmen, T.E. Biørstad, C. Rechberger, M. Robshaw and G. Sekar [145].
We describe Moustique and its attack as an example of the difﬁculty of designing
self-synchronizing stream ciphers.
The internal state of Moustique consists of 8 bit ﬁelds a(0), ..., a(7). The size of
a(0) is 128 bits and the bits are indexed from 1 to 128. The size of a(1), a(2), a(3),
a(4) and a(5) is 53 bits and their bits are indexed from 0 to 52. The register a(6) has
12 bits indexed from 0 to 11 and a(7) has 3 bits indexed a(7)
0 , a(7)
1
and a(7)
2 .
Figure 10.3 gives an overview of the cipher. The eight levels of bit ﬁelds make
the delay of the self-synchronizing stream cipher equal to 9.
The update is done by the simple Boolean functions:
g0(a,b,c,d) = a + b + c + d,
g1(a,b,c,d) = a + b + c(d + 1) + 1,
g1(a,b,c,d) = a(b + 1) + c(d + 1) + 1.
At any time step the output bit z of Moustique is computed as
z = a(7)
0
+ a(7)
1
+ a(7)
2
and the message bit m is encrypted to the cipher bit c by
c = m + z = g0

m,a(7)
0 ,a(7)
1 ,a(7)
2

.
The next state of the bit ﬁeld a(i) (1 ≤i ≤7) depends only on the bit ﬁeld a(i−1).
The update of the bit ﬁeld a(i) (1 ≤i ≤7) is given in the next equation.
a(7)
i
←g0

a(6)
4i ,a(6)
4i+1,a(6)
4i+2,a(6)
4i+3

for 0 ≤i < 3,
a(6)
i
←g1

a(5)
4i ,a(5)
4i+3,a(5)
4i+1,a(5)
4i+2

for 0 ≤i < 12,

236
10
The eStream Project
Fig. 10.3 The cipher Moustique
a(5)
4i mod 53 ←g1

a(4)
i
,a(4)
i+3,a(4)
i+1,a(4)
i+2

for 0 ≤i < 53,
a(4)
4i mod 53 ←g1

a(3)
i
,a(3)
i+3,a(3)
i+1,a(3)
i+2

for 0 ≤i < 53,
a(3)
4i mod 53 ←g1

a(2)
i
,a(2)
i+3,a(2)
i+1,a(2)
i+2

for 0 ≤i < 53,
a(2)
4i mod 53 ←g1

a(1)
i
,a(1)
i+3,a(1)
i+1,a(1)
i+2

for 0 ≤i < 53,
a(1)
4i mod 53 ←g1

a(0)
128−i,a(0)
i+18,a(0)
113−i,a(0)
i+1

for 0 ≤i < 53.
The bit ﬁeld a(0) is organized as what the authors call a conditional complement-
ing shift register (CCSR). To describe the update of a(0) we rename the bits. The
128 bits of a(0) are partitioned into 96 cells qi indexed from 1 to 96. The number of
bits in cell i is denoted by ni and given by Table 10.1.
The mapping between the labels qi
j and a(0)
k
is illustrated in Fig. 10.4. The index
i is given on the right, the index j on the bottom and the index k stands in the cells.
For example q95
2 is identical to a(0)
107.

10.3
Mosquito and Moustique
237
Table 10.1 Number of bits
per cell in the CCSR of
Moustique
Range of i
ni
1 ≤i ≤88
1
89 ≤i ≤92
2
93 ≤i ≤94
4
i = 95
8
i = 96
16
Fig. 10.4 Mapping between
qi
j and a(0)
k
Formally we set q0
0 = c + 1 and n0 = 1 where c is the last cipher text bit.
The update of the CCSR is then done by
q96
j ←g2

q95
j mod 8,q95−j
0
,q94
j
mod 4,q94−j
[j≤5]

for 0 ≤j ≤16.
Here [j ≤5] denotes the indicator function, i.e. [j ≤5] is 1 for j ≤5 and 0
otherwise.
qi
j ←gx

qi−1
j mod ni−1,kj−1,qv
j mod nv,qw
j mod nw

for 0 ≤j ≤ni and i = 95,...,3.

238
10
The eStream Project
Table 10.2 Bit updating in
the CCSR of Moustique
Index
Function gx
v
w
(i −j) ≡1
mod 3
g0
2(i −j −1)/3
i −2
(i −j) ≡2
mod 3
g1
i −4
i −2
(i −j) ≡3
mod 6
g1
0
i −2
(i −j) ≡0
mod 6
g1
i −5
0
Here kj−1 denotes the jth bit of the key. The values of x, v and w are given by
Table 10.2
q2
0 ←q1
0 + k1 + 1 = g1

q1
0,k1,0,0

,
q1
0 ←c + k1 + 1 = g0

q0
0,k0,0,0

.
Now we sketch the attack from [145] which removed Moustique from the list of
eStream ﬁnalists. The starting point is the following observation:
Lemma 10.1 Let i ≤77 and i ≡2 mod 3 then qi
0 occurs in the update function of
the CCSR only linearly.
Proof The value qi
0 occurs only in the following update steps:
• qi+1
0
= g1(qi
0,ki,qv
0,qw
0 ) and g1 is linear in its ﬁrst two components.
• qi+2
0
= g0(qi+1
0
,ki+1,q2(i+1)/3
0
,qi−2
0
), but q0 is linear.
• It cannot occur as v = i′ −4 with i′ = i + 4 in the computation of qi+4
0
since
(i + 4) −0 ≡0 ̸≡1 mod 3.
• It cannot occur as v = i′ −5 with i′ = i + 5 in the computation of qi+5
0
since
(i + 5) −0 ≡1 mod 3, i.e. (i + 5) −0 ≡0 mod 6.
• It occurs as v = 2(i′ −j′ −1)/3 in the computation of qi′
0 with i′ = (3i +1)/2 and
j′ = 0, if i′ ≡1 mod 3. But then the equation is qi′
0 = g0(qi′−1
0
,ki′−1,qi
0,qi′−2
0
)
and g0 is linear.
□
The practical consequence of Lemma 10.1 is that it can be used to mount a related
key attack.
Assume that the attacker can observe the output of two instances of Moustique,
which encrypt the same plain text. The difference between these two instances is that
the initial bit q71
0 is switched. In addition, the bits k70, k71 and k72 of the key differ
in both instances. By Lemma 10.1 the bit q71
0
occurs only linearly in the update
function of the CCSR and then by switching k70, k71 and k72 we assure that during
the iteration of the CCSR the differences remain in the bit q71
0 .

10.3
Mosquito and Moustique
239
The only way that this difference can affect the output is if it affects the regis-
ter a(1). This is done via the equation
a(1)
9
= a(1)
4×42 mod 53
= g1

a(0)
86 ,a(0)
60 ,a(0)
71 ,a(0)
43

= g1

q86
0 ,q60
0 ,q71
0 ,q43
0

= q86
0 + q60
0 + q71
0

q43
0 + 1

+ 1.
If q43
0 = 1, the error does not propagate.
The attacker observes the two related cipher texts and searches for the ﬁrst differ-
ence. This immediately gives one bit of information (the corresponding bit q43
0 must
be 0). However, the attacker can also guess the key bits k0,...,k42 and compute the
ﬁrst 43 bits q1
0,...,q43
0
of the CCSR. In all previous steps the bit q43
0
must be 1.
This allows the attacker to check if the guess was correct.
This is only the most basic version of the attack. There is still much work to do to
effectively test the keys and to exclude false positives. See the original paper [145]
for the details.
Related key attacks like this may be a bit exotic and it is not immediately clear
how the attack should be used in practice (see also the discussion in [24]). However
the attack is a clear sign that something is wrong in Moustique and it was absolutely
correct to remove it from the eStream portfolio.
Moustique provides a good example of the problems that occur when de-
signing a self-synchronizing stream ciphers. Everyone who is interested in self-
synchronizing stream ciphers should study the examples of Moustique and its pre-
decessors Mosquito, Υ Γ [66] and KNOT [67] carefully.
Self-synchronizing stream ciphers have, in comparison to synchronous stream
ciphers, the appealing property that they also secure the ciphertext against active
attacks, i.e. you do not need an additional message authentication code (MAC).
However, for the moment I would not recommend them, due the problems with the
design process. If you really need a self-synchronizing stream cipher you can still
run a block cipher in CFB-mode, but even this has problems. It is probably best not
to mix different security goals. Always use two crypto-primitives: a cipher for data
security and a MAC to ensure the data integrity. (High level block-cipher modes like
CWC [163] or OCB [199] use the same block cipher for two purposes, once as a
cipher and once as a MAC, but even these modes clearly separate the two security
goals.)

Chapter 11
The Blum-Blum-Shub Generator and Related
Ciphers
This chapter covers a class of highly unusual stream ciphers. Instead of being de-
signed to ﬁt real hardware, these ciphers borrow ideas from public key cryptography
to enable security proofs. The development of these ciphers started shortly after the
development of public key cryptography [33, 240]. The advantage is that we can be
very sure that an attacker has no chance of breaking the cipher. The disadvantage
is that these ciphers are much slower and need much more memory than normal
stream ciphers. They even have problems which come into the range of modern
block ciphers. Thus these ciphers are rarely applied as stream ciphers, but they can
be applied as part of a key generation protocol.
11.1 Cryptographically Secure Pseudo-random Generators
Every pseudo-random generator can be broken by a brute force search over its in-
ternal state. So we cannot expect a pseudo-random generator to be unconditionally
secure in the same sense as the one-time pad is unconditionally secure. The best
we can expect is that the complexity of breaking the generator is very high. To be
able to speak of complexity classes we need a generator family with an arbitrary
large internal state. We use the size of the internal state as a parameter to measure
the complexity. To avoid additional complications we assume that there is no key
scheduling, the key is just the initial state. As usual in complexity theory, we say a
problem is easy if it can be solved in polynomial time and it is hard otherwise. This
leads us to the following deﬁnition.
Deﬁnition 11.1 (see [33]) A pseudo-random generator family is said to be crypto-
graphically secure if:
1. The time needed to generate the next bit is bounded by a polynomial in the size
of the internal state.
2. There exists no polynomial time algorithm which predicts without knowledge of
the internal state the next bit with probability 1/2 + ϵn, where ϵn →0 as the size
n of the internal state tends to inﬁnity.
A. Klein, Stream Ciphers, DOI 10.1007/978-1-4471-5079-4_11,
© Springer-Verlag London 2013
241

242
11
The Blum-Blum-Shub Generator and Related Ciphers
Note: A non-deterministic Turing machine can break every generator in polyno-
mial time simply by guessing the internal state and verifying the guess. So the best
we can hope for is that breaking the generator is in NP\P. Since it is still an open
problem whether P = NP or P ̸= NP (see Sect. 12.4 for a short introduction to
this problem), our proofs will have the form: “If some problem is difﬁcult, then the
generator is cryptographically secure”.
One should also be aware that not being in P does not automatically mean
that an algorithm is useless in practice. For example, computing Gröbner bases is
EXPSPACE-complete, which is several steps above NP in the hierarchy of com-
plexity classes, nevertheless Gröbner bases are an important and practical tool.
Before we can present some examples of cryptographically secure generators we
introduce some notation and terminology.
Deﬁnition 11.2 Let x1,...,xn be a sequence of pseudo-random bits.
An ε-distinguisher is an algorithm which, given the sequence x1,...,xn and
a true random sequence y1,...,yn, recognizes the pseudo-random sequence with
probability 1/2 + ε.
An ε-next bit predictor is an algorithm which computes from x1,...,xn the next
bit xn+1 with probability 1/2 + ε.
An ε-previous bit predictor is an algorithm which computes from x1,...,xn the
preceding bit x0 with probability 1/2 + ε.
The concepts of distinguisher, next bit predictor and previous bit predictor may
look different, but they are essentially the same.
Lemma 11.1 The following statements are equivalent.
1. There exist a distinguisher for a pseudo-random sequence.
2. There exist a next bit predictor for a pseudo-random sequence.
3. There exist a previous bit predictor for a pseudo-random sequence.
Proof Assume that you have a predictor for a pseudo-random generator and you
want a distinguisher. To decide whether a sequence x was generated by the pseudo-
random generator you apply the predictor to a part of x and compare the prediction
with the observed bits. If the prediction is correct you accepts the hypothesis that
x was generated by the pseudo-random generator. If the prediction in wrong you
reject the hypothesis.
On the other hand you can construct a predictor from a distinguisher by applying
it to the pair x1,...,xn,0 and x1,...,xn,1. If xn+1 is 0 the distinguisher will tell
you that x1,...,xn,1 must come from a true random source and vice versa.
□
The idea of Blum and Micali [33] is to turn a one-way function into a stream
cipher. Before we can state their main result we need some more terminology.

11.1
Cryptographically Secure Pseudo-random Generators
243
Deﬁnition 11.3 Let (Sn) be a series of sets and for each i ∈Sn let Di be some set.
We call B a set of predicates if
B =

Bi : Di →{0,1}

.
B is an accessible set of predicates if there exists a polynomial time algorithm
that selects random elements of In = {(i,x) | i ∈Sn,x ∈Di}. Each element must be
selected with probability
1
|In|.
B is input hard if there exists no probabilistic polynomial time algorithm that
decides Bi(x) = 1 with probability 1/2 + ε for some ε > 0.
Theorem 11.2 Let B be a set of accessible, input hard predicates. For each i let fi
be a permutation of Di that is computable in polynomial time.
If h : Di →{0,1} with H : x →Bi(fi(x)) is computable in polynomial time then
Algorithm 11.1 is a cryptographically secure pseudo-random generator.
Algorithm 11.1 The Blum-Micali generator
1: loop
2:
output Bi(fi(x))
3:
x ←fi(x)
4: end loop
Proof By assumption, the computations in Algorithm 11.1 are done in polynomial
time.
Assume now that the sequence produced with Algorithm 11.1 can be predicted
in polynomial time.
Then there also exists by Lemma 11.1 a previous bit predictor. Apply it to the
sequence sk = Bi(f k
i (x)) (k = 1,...) and you get an efﬁcient probabilistic decision
procedure for Bi(x) = 1 as Bi(x) = s0.
□
As an example, Blum and Micali give the following generator based on the dis-
crete logarithm problem (see Algorithm 11.2).
Algorithm 11.2 Discrete logarithm generator
Require: p is a prime of the form 2q + 1, g is an element of Z/pZ of order q,
x ∈{g0,g,...,gp−1}
1: loop
2:
output 1 if gx > p/2 and 0 otherwise
3:
x ←gx
4: end loop

244
11
The Blum-Blum-Shub Generator and Related Ciphers
11.2 The Blum-Blum-Shub Generator
We come now to the cipher that gives its name to this chapter. In [32] L. Blum,
M. Blum and M. Shub improved the ideas presented in the last section. Their cipher,
which is now known as the Blum-Blum-Shub generator, was the ﬁrst cipher that is
cryptographically secure in the sense of Deﬁnition 11.1 and also fast enough that
it can be applied at least in some special cases in which speed does not have high
priority.
The cipher is quite simple (see Algorithm 11.3).
Algorithm 11.3 The Blum-Blum-Shub generator
Require: n = pq where p,q are primes ≡3 mod 4
1: loop
2:
s ←s2 mod n
3:
output s mod 2
4: end loop
We reduce the problem of a previous bit predictor for the Blum-Blum-Shub gen-
erator to the Composite Quadratic Residue Problem which is assumed to be hard
(see Sect. 13.5).
Theorem 11.3 A previous bit predictor for the Blum-Blum-Shub generator gives a
Monte Carlo algorithm that solves the composite quadratic residue problem for n.
The Monte Carlo algorithm and the predictor have the same time complexity and
the same success probability.
Proof The Blum-Blum-Shub generator is just a particular instance of Algo-
rithm 11.1. Hence we can simply apply Theorem 11.1. But since it is so easy and
important, we just repeat the arguments for the special case of the Blum-Blum-Shub
generator.
Given s with
 s
n

= 1, since −1 is a quadratic non-residue modulo p and q, either
s or −s is a quadratic residue modulo n. Since n is odd, s and n −s have different
parity.
Compute the Blum-Blum-Shub sequence xi = (s2i mod n) mod 2. The previous
bit must be either x0 = s mod 2 if s is a quadratic residue modulo n or x0 = (n −
s) mod 2 if s is a quadratic non-residue modulo n and hence n −s is a quadratic
residue modulo n. So a previous bit predictor solves the quadratic residue problem
modulo n.
□
Theorem 11.3 transforms a previous bit predictor for the Blum-Blum-Shub gen-
erator into a Monte Carlo algorithm for the composite quadratic residue problem,
but the success probability could be very low. In the next step we show how to
enhance the success probability to any value we like.

11.2
The Blum-Blum-Shub Generator
245
Algorithm 11.4 Enhancing the success probability
Require: n = pq with p ≡q ≡3 mod 4 and
 s
n

= 1
1: repeat
2:
Select x with gcd(x,n) at random.
3:
Select e ∈{±1} at random.
4:
s′ ←ex2s
5:
Call the basic Monte Carlo algorithm to decide if s′ is a quadratic residue
modulo n.
6:
if s′ is guessed to be a quadratic residue and e = 1 or s′ is guessed to be a
quadratic non-residue and e = 0 then
7:
This is a hint for s to be a quadratic residue.
8:
else
9:
This is a hint for s to be a quadratic non-residue.
10:
end if
11: until you collected enough hints {Sequential tests could be used to control the
error probability of the ﬁrst and second kind simultaneously}
What Algorithm 11.4 does is select a random s′ with
 s′
n

= 1 (in lines 2–4),
then it decides whether or not s′ is a quadratic residue modulo n. The result can be
used to decide whether or not s is a quadratic residue (lines 6–10). Each decision is
correct with probability 1/2+ε. Standard statistics (see Sect. 15.3.3) can be used to
combine the single tests into a result which is correct with arbitrary high probability.
By Lemma 11.1, we know at this point that if the composite quadratic residue
problem is hard that there cannot exist an efﬁcient predictor or distinguisher for the
Blum-Blum-Shub generator. The absence of an efﬁcient distinguisher also implies
that the Blum-Blum-Shub generator must have good pseudo-random properties.
It is also possible to reduce the security of the Blum-Blum-Shub generator di-
rectly to the factoring problem. This was ﬁrst done in [270]. In the following we use
the approach from [247]. The key is the following lemma.
Lemma 11.4 Let xi = 2ia mod n and si = xi mod 2. If 2k+1 ≥n one can recon-
struct a from the sequence s0,...,sk with O(k) arithmetic operations.
Proof Let x ∈{0,...,n−1} and y = 2x mod n. Since n is odd y mod 2 = 0 implies
y = 2x and if y mod 2 = 1 then y = 2x −n. If ˆy is an approximation of y with
| ˆy −y| ≤B, then
ˆx =
 ˆy
2
if y mod 2 = 0,
ˆy+n
2
if y mod 2 = 1
(11.1)
is an approximation of x with |ˆx −x| ≤B/2.
Successive application of Eq. (11.1) to the sequence xi starting with the approx-
imation ˆxk = n
2 yields an approximation ˆx0 of x0 with |ˆx0 −x0| ≤1. Since x0 is an
integer this means that x0 is determined.
□

246
11
The Blum-Blum-Shub Generator and Related Ciphers
Theorem 11.5 A previous bit predictor for the Blum-Blum-Shub generator leads to
an efﬁcient factoring algorithm for the modulus n.
Proof To simplify the presentation of the reduction algorithm we assume that the
predictor is always correct and that 2 is a square in Z/nZ. If the predictor is not
perfect the success probability of the factoring algorithm will be less than 1, but
with techniques similar to Algorithm 11.4 one can get it very close to 1. For details,
and for the case where 2 is not a square, see [247].
Choose x with
 x
n

= −1. Without loss of generality we assume that
 x
p

= 1 and
 x
q

= −1. Compute the Blum-Blum-Shub sequence starting with y ≡x2 mod n.
y has four roots in Z/nZ, but only one of these roots ˆx is also a square in Z/nZ.
This is the preceding element of the Blum-Blum-Shub sequence.
The predictor will give us the bit ˆx mod 2. Applying the predictor to the Blum-
Blum-Shub sequence starting with 4y will give us the bit (2ˆx mod n) mod 2, be-
cause 2 is a square modulo n. Continuing in this fashion we get a sequence of bits
which is, by Lemma 11.4, enough to determine ˆx.
Since ˆx ≡±x mod p and
 ˆx
p

= −
 x
p

= 1 we get ˆx ≡−x mod p and simi-
larly we have ˆx ≡x mod q. Thus ˆx + x is a multiple of p but not of q. We obtain
p by computing gcd(ˆx + x,n).
□
Now that we have completed the security proof for the Blum-Blum-Shub gener-
ator, this is an appropriate moment to stop for while and ask ourselves what such a
proof means. Does it mean that an attacker should simple give up? Of course not.
If you want to attack a Blum-Blum-Shub generator you can simply polish your
social skills. Perhaps in a local pub a clerk will tell you the secret information after
the third glass of beer. Most of the time humans are the weakest part of the security
chain. See Chap. 7 of [210] for an introduction to social engineering and further
references.
Another approach is to put aside the mathematics behind the Blum-Blum-Shub
generator and instead try side channel attacks. Since the Blum-Blum-Shub generator
is a complex algorithm it will most likely be implemented on a general purpose
computer. This opens up a lot of new possibilities. You can try to play around with
the branch prediction unit or the cache. Or perhaps you may ﬁnd a security hole in
the underling big-integer library. There exists a lot of work on side channel attacks
against the famous RSA cipher [2, 160, 161] and the Blum-Blum-Shub generator
is similar enough that most of these methods can be transferred. This is a very real
trend. In recent years implementations of RSA-like ciphers have failed regularly to
side channel or hacking attacks.
Now let us go back to the mathematical part. We have proved that a distinguisher
or predictor for the Blum-Blum-Shub generator will lead to a solution of the com-
posite quadratic residue problem or factorization problem. Let us assume for the mo-
ment that the conjectures are true and the best way to solve the composite quadratic
residue problem is to compute the factorization, and factorization is indeed a hard
problem. This mean that the worst-case complexity of factorization is not polyno-
mial or, if we are lucky, even the average-case complexity is not polynomial. But

11.3
Implementation Aspects
247
the attacker is not interested in the worst-case or the average-case, he wants to fac-
tor just the special number used in the special system that is under attack. There
is still hope that, due to some protocol failure, someone has chosen a special n for
which a fast factorization algorithm exists. Again the RSA literature provides a lot
of material you can try (see [128, 183]). History shows that this is a real possibility.
In any case, factorization may not be such a hard problem. After the development
of RSA, a lot of progress had been made in factorization (quadratic sieve, number
ﬁeld sieve), see [138, 196] for an introduction to this topic. So even if everything
else fails, an attacker can still hope that the modulus n was chosen too small and he
is able to factorize n.
Finally, the Blum-Blum-Shub generator is only a part of the security chain. It
will usually be used to generate a pseudo-random seed for a faster cipher. Perhaps
if you can break this cipher, then breaking the Blum-Blum-Shub generator will no
longer be necessary. The seed for the Blum-Blum-Shub generator must come from
some (external) entropy source. Perhaps you can compromise this source.
All these considerations do not mean that security proofs are useless, but they
are a warning that the proofs cover only a small aspect of the true implementation.
Even if a cipher is secure, the implementation may have ﬂaws. History has provided
enough examples. There is no reason to become too ﬁxated on security proofs [159].
11.3 Implementation Aspects
An efﬁcient implementation of the Blum-Blum-Shub generator is essentially an ef-
ﬁcient implementation of large integer arithmetic. This is a classical problem of
computer algebra, so we give only a brief introduction to this topic (for further in-
formation, see [100, 152]).
A large number is stored as an array of digits with respect to some base B. On
a binary computer we will usually work with the base B = 232 or whatever is the
internal word size.
Addition and subtraction of n-digit numbers is not a problem. We needs just n
elementary additions (subtractions) and we keep track of the carry (which is auto-
matically done by standard hardware).
Multiplication is more difﬁcult. The normal multiplication algorithm that we
learn at school needs n2 elementary multiplications.
A better solution was found by Karatsuba [143]:

a1Bn+a0

c1Bn+c0

= (a1c1)B2n+

(a0+a1)(b0+b1)−a1b1−a0b0
 
Bn+a0c0.
(11.2)
As one can see from Eq. (11.2), one can reduce the problem of multiplying two
2n-digit number to that of multiplying three n-digit numbers and some n-digit ad-
ditions.
The trick is to apply Eq. (11.2) recursively, which leads to the following recursion
for the complexity M(n) of multiplying two n-digit numbers
M(2n) = 3M(n) + 4n.
(11.3)

248
11
The Blum-Blum-Shub Generator and Related Ciphers
Equations like (11.3) appear quite often in the analysis of recursive algorithms.
It is not hard to solve them in general.
Theorem 11.6 Let f (x) = O(xe) and let M satisfy the recurrence M(1) = 1 and
M

qk
= rM

qk−1
+ f

qk−1
then
M(x) =

O(xlogq r)
if e < logq r,
O(xe logx)
if e ≥logq r.
Proof Let r′ = qe.
By assumption we have f (qk) = O(r′k) and hence
f

qk
≤cr′k
for some c > 0.
In the case e < logq r we have r′ < r. Let C = max{ r′c
r3 ,(r −r′)−1}. We claim
that
M

qk
≤C

rk+1 −r′k+1
for k ≥0.
For k = 0 we have C(r −r′) ≥(r −r′)−1(r −r′) = 1.
Now let k > 0. Then
M

qk
= rM

qk−1
+ f

qk−1
≤rC

rk −r′k
+ cr′k−1
= Crk+1 −C
 r
r′ −
c
Cr2

r′k+1
= C

rk+1 −r′k+1
.
Now we deal with the case e ≥logq r.
Let C = max{ c
r′2 ,(r −r′)−1}. We claim that
M

qk
≤C(k + 1)r′k+1
for k ≥0.
For k = 0 we have C(r −r′) ≥(r −r′)−1(0 + 1)(r −r′) = 1.

11.3
Implementation Aspects
249
Now let k > 0 then
M

qk
= rM

qk−1
+ f

qk−1
≤rCkr′k + cr′k−1
=

k rC
r′ + c
r′2

r′k+1
≤

kC + c
r′2

r′k+1
= C(k + 1)r′k+1.
This proves the theorem.
□
By Theorem 11.6 we get that Karatsuba’s algorithm multiplies two n-digit num-
bers in O(nlog2(3)) = O(n1.58) steps, which is much better than the “school algo-
rithm”. The constant hidden in the O-term is very small and even for a few digits
Karatsuba’s method is faster than the naive multiplication.
With a variation of Karatsuba’s idea one can show that multiplication of n-
digit numbers can be done in O(n1+ϵ) for each ϵ > 0. The Toom-Cook algorithm,
see [56, 265], does exactly this.
The asymptotically fastest multiplication algorithm interprets n-digit numbers as
polynomials of degree n in B. The goal is to compute a convolution. This is done by
computing the Fourier transform, performing a pointwise multiplication and trans-
forming the result back. Problems arise since we must switch into a ring with nth
roots of unity. A careful analysis of the necessary steps shows that multiplication
of n-digit numbers can be done in O(nlognloglogn) steps. This algorithm was
designed by Schönhage and Strassen [236].
For the size used in typical cryptographic applications (a few hundred to a few
thousand bits) one uses Karatsuba’s algorithm or a variation of the Toom-Cook al-
gorithm, but not the Schönhage-Strassen algorithm. Good free libraries exist that
implement these algorithms (for example the GNU multiple precision library [105]).
The developers of these libraries always take care to select the best algorithm for the
given size of input.
The last operation we need is integer division with remainder. This operation is a
bit tricky. The normal “school algorithm” for division involves some guessing steps,
which are difﬁcult to automate (see, for example, [152] for a full description of the
algorithm). However the “school algorithm” is quite slow (quadratic in the number
of digits). The asymptotically fastest division algorithm uses Newton’s method to
iteratively compute a good approximation of m−1 and then computes n/m as nm−1
and rounds the result to integers. This proves that division of n-bit integers can be
done in O(M(n)) time, where M(n) denotes the time needed for multiplication
of n-bit integers, i.e. if we use the Schönhage-Strassen multiplication we get the
asymptotic bound O(nlognloglogn).

250
11
The Blum-Blum-Shub Generator and Related Ciphers
However the constant hidden in the O-term is quite big. In the case where we
always work modulo the same number p, Montgomery [195] found a way to replace
the expensive division by p with a cheap division by a power of 2.
The core of Montgomery’s method is Algorithm 11.5.
Algorithm 11.5 Montgomery reduction
Require: y < pR
Ensure: x = yR−1 mod p, 0 ≤x < p −1
1: u ←−yp−1 mod R {p−1 mod R can be computed in a pre-processing step}
2: x ←(y + up)/R
3: if x ≥p then
4:
x ←x −p
5: end if
6: return x
Theorem 11.7 Given as input a non-negative integer y < pR, Algorithm 11.5 com-
putes x with 0 ≤x < p and x ≡yR−1 mod p.
Proof As u ≡−yp−1 mod p we have (y + up) ≡y −y ≡0 mod R. Hence the
division in line 2 produces an integer.
We have x ≡(y + up)/R ≡yR−1 mod p. The only thing that remains is to
prove that 0 ≤x < 2p and hence the steps in lines 3–5 guarantee that the result of
Algorithm 11.5 lies in the interval [0,p −1].
To see that (y + up)/R < 2p note that y < pR and u < R, hence y + up <
pR + R < 2pR.
□
Algorithm 11.5 needs only two integer multiplications (one in line 1 and one in
line 2). In addition it needs a division by R and a reduction modulo R. If R is a
power of 2, a division by R is just a shift, which is very cheap. Lines 3–5 contain
only one comparison and perhaps a subtraction. So the cost of Algorithm 11.5 is ap-
proximately the same as for two integer multiplications, but it performs a reduction
modulo p.
Instead of working with the representatives x of the remaining classes directly,
we work with their Montgomery transforms ⟨x⟩= xN mod p.
Adding two Montgomery transforms is easy: ⟨x + y⟩= ⟨x⟩+ ⟨y⟩mod p. Since
⟨x⟩+ ⟨y⟩< 2p, the reduction modulo p can be done by a comparison and one
subtraction (see lines 3–5 of Algorithm 11.5).
To compute ⟨xy⟩one simply applies Montgomery reduction to the product
⟨x⟩⟨y⟩.
The Blum-Blum-Shub Algorithm is well-suited for using Montgomery multi-
plication. One can do the internal computations with the Montgomery transforms.
For the output, the Montgomery transforms must be converted back into the usual

11.4
Extracting Several Bits per Step
251
representation, but this can be done in parallel, especially if dedicated hardware is
used [205].
Even better, one can use Algorithm 11.6 and save the conversion of the output.
Algorithm 11.6 Variation of the Blum-Blum-Shub generator for use with Mont-
gomery reduction
Require: n = pq where p,q are primes ≡3 mod 4
1: loop
2:
s ←s2R−1 mod n {Compute this with Montgomery reduction}
3:
output s mod 2
4: end loop
If R is a square, the proof of Theorem 11.3 works for Algorithm 11.6 as well as
for the original Blum-Blum-Shub generator. (If R is not a square, some extra work
is needed to prove the security.) The advantage of Algorithm 11.6 is not only the
speed, but also the avoidance of an implementation of a complex integer division
algorithm and hence some possible hacking attacks.
Another computational issue is how to generate the primes necessary to set up the
Blum-Blum-Shub generator. Here we can ﬁnd much material common to the RSA
cipher. One approach is to generate random numbers and test them for primality.
There now exists a deterministic polynomial time algorithm for primality testing [3].
In practice we normally use a faster probabilistic test (like the Solovay-Strassen
test [254] or the Miller-Rabin test [191, 217]).
Also noteworthy are the sieving algorithms which ﬁnd all primes in a given inter-
val. Even with the ancient sieve of Eratosthenes one can do amazing things [214].
The newer Atkin-Bernstein sieve [10] is also worth knowing.
Particularly interesting is the idea of making factoring even harder by using cryp-
tographically strong primes (which provide higher resistance against some special
factoring methods that work fast if n −1 or n + 1 is smooth) [252], but this is
probably not worth the effort [222].
For most purposes it is not necessary to implement our own prime generation,
since good free implementations are available, for example [201].
11.4 Extracting Several Bits per Step
With Algorithm 11.6 we have already seen an example which demonstrates that the
Blum-Blum-Shub generator must not be based on the least signiﬁcant bit. This leads
us directly to the questions of which bits and how many bits of the Blum-Blum-Shub
sequence can be used without weakening the generator. This problem was studied
in [5, 270].
We return to the general setting of Theorem 11.2. This time we have j accessible
sets of predicates B(k) = {B(k)
i
: Di →{0,1}} (1 ≤k ≤j). Modify Algorithm 11.1

252
11
The Blum-Blum-Shub Generator and Related Ciphers
by outputting in line 2 the j bits B(1)
i
(fi(x)),...,B(j)
i
(fi(x)). The question is what
are necessary and sufﬁcient conditions for the j-bit Blum-Micali generator to be
secure.
A linear transformation of a pseudo-random sequence must still be pseudo-
random. Thus an obvious necessary condition is that for each S with ∅̸= S ⊆
{1,...,j} the predicate BS = {BS
i : Di →{0,1}} where BS
i is deﬁned by
BS
i (X) =
!
k∈S
B(k)
i
(x)
(11.4)
satisﬁes all the assumptions of Theorem 11.2.
This necessary condition is also sufﬁcient.
Theorem 11.8 (XOR-Condition, see [270]) Let B(k) = {B(k)
i
: Di →{0,1}} (1 ≤
k ≤j) be a set of accessible predicates and fi be a permutation of Di such that
fi(x) and B(k)
i
(fi(x)) are computable in polynomial time.
If for each non-empty subset S of {1,...,j} the predicate BS deﬁned by
Eq. (11.4) is input hard then Algorithm 11.7 is a cryptographically secure pseudo-
random generator.
Algorithm 11.7 A variation of the Blum-Micali generator that outputs j bits per
step
1: loop
2:
output B(1)
i
(fi(x)),...,B(j)
i
(fi(x))
3:
x ←fi(x)
4: end loop
Proof First note that Algorithm 11.7 runs in polynomial time.
Assume that Algorithm 11.7 is not cryptographically secure. Then there exists a
previous bit predictor. For some known starting value x one can compute the output
of Algorithm 11.7 and use the previous bit predictor to get a probabilistic algorithm
T for B(j)
i
(x) given B(1)
i
(fi(x)), ..., B(j−1)
i
(fi(x)).
For all possible (j −2)-tuples b1,...,bj−2 compute T (b1,...,bj−2,0) and
T (b1,...,bj−2,1). If both values are equal, you have a good guess for B(j)
i
(x)
given B(1)
i
(fi(x)), ..., B(j−2)
i
(fi(x)). If the output differs, there is still a chance to
guess B(j)
i
(x).
Similarly if T (b1,...,bj−2,0) and T (b1,...,bj−2,1) are different, we get a
good guess for B(j)
i
(x) ⊕B(j−1)
i
(x) given B(1)
i
(fi(x)), ..., B(j−2)
i
(fi(x)) and we
have still a chance to guess B(j)
i
(x) ⊕B(j−1)
i
(x) if the outputs are equal.
At this point we have a good probabilistic algorithm T ′ for B(j)
i
(x) or B(j)
i
(x) ⊕
B(j−1)
i
(x), depending on which case is more likely. In any case, T ′ depends only on
j −2 bits instead of j −1 bits.

11.5
The RSA Generator and the Power Generator
253
Continuing in this fashion we ﬁnally reach a probabilistic algorithm that depends
on no bits and predicts some XOR-sum BS
i (X) = 1
k∈S B(k)
i
(x).
But we assumed that such an algorithm does not exist. Hence Algorithm 11.7
must be cryptographically secure.
□
With arguments similar to the one used in Sect. 11.2 one can prove the XOR-
condition for the Blum-Blum-Shub generator and show that extracting loglogn bits
leaves the Blum-Blum-Shub generator asymptotically secure (see [270]). However,
this is only half of the truth: the probability of a possible distinguisher still tends to
zero, but more slowly when you extract more bits. A more detailed analysis appears
in [247]. The authors prove inequalities which let you select, for a given distin-
guishing advantage ε (the security level) and the number m of bits which should
be generated, the size of the modulus n and the number j of bits extracted per
step which lead to an optimal speed. The examples given in [247] indicate that you
should not extract many bits, often one reaches a higher speed for a given security
level by using a smaller n and extracting only one bit.
On the other hand Coppersmith’s method (see Sect. 13.6) shows that if we extract
more than n −√n bits, the Blum-Blum-Shub generator becomes insecure even if
the attacker can observe only two successive time steps.
11.5 The RSA Generator and the Power Generator
We have already mentioned several times that the Blum-Blum-Shub generator has
many similarities to the RSA public key system. One can make it even more similar,
which leads us to Algorithm 11.8.
Algorithm 11.8 The RSA generator
Require: n = pq where p,q are primes, e is an integer with gcd(e,ϕ(n)) = 1
1: loop
2:
s ←se mod n
3:
output s mod 2
4: end loop
The power generator is a generalization of the Blum-Blum-Shub generator and
the RSA generator. It is based on the recursion
si+1 = se
i mod n
without any restrictions to e.
We now prove a result on the period of the power generator.

254
11
The Blum-Blum-Shub Generator and Related Ciphers
Theorem 11.9 (see [97]) For any positive integer m and any numbers K1,K2 ≥1,
let W denote the number of seeds (s,e) of the power generator that lead to a period
of at most λ(λ(n))/(K1K2). Then W is bounded by
W ≤ϕ(n)ϕ

λ(n)
τ(λ(n))
K1
+ τ(λ(λ(n)))
K2

.
Proof In Sect. 13.2 Eq. (13.3) we derive a bound for the number of elements in
Z/nZ of small order. Theorem 11.9 is essentially a two-fold application of this
bound.
The number of elements s ∈Z/nZ with ordn s > λ(n)/K1 is at least ϕ(n)(1 −
τ(λ(n)))/K1. For the moment ﬁx s with ordn s = λ(n). The discrete logarithm with
base s of an element in ⟨s⟩is an element of Z/λ(n)Z. The discrete logarithms of the
series si generated by the power generator are powers of e. The period of the power
generator is ordλ(n) e. The number of elements e ∈Z/λ(n)Z with order ordλ(n) e >
λ(λ(n))/K2 is at least ϕ(λ(n))(1−τ(λ(λ(n))))/K2. If the order of s is λ(n)/K then
the period will decrease to ordλ(n)/K e ≥(ordλ(n) e)/K.
Thus there are at least

ϕ(n)

1 −τ(λ(n))
K1

ϕ

λ(n)

1 −τ(λ(λ(n)))
K2

seeds (s,e) of the power generator that lead to a period of at least λ(λ(n))/K1K2. □
With similar arguments one can also prove a lower bound for the period of the
Blum-Blum-Shub generator. Since the Blum-Blum-Shub generator has fewer de-
grees of freedom the lower bound will be a bit weaker, see [245] Theorem 23.7.
Since the Blum-Blum Shub generator uses only squaring it is the fastest variant
of the power generator family.
11.6 Generators Based on Other Hard Problems
Up to now we have essentially two problems as a basis for security proofs: factor-
ing integers and discrete logarithms. These two problems were the ﬁrst problems
that were used in cryptography and stand behind most applications. However, there
are other hard problems that can serve as basis for security proofs. They are not as
successful as factoring. Lattice based attacks are a real threat for these systems, for
example the knapsack-based public key cryptosystem is broken [200, 243], the ba-
sic HFE-system (based on multivariate quadratic equations) is broken [241], and the
original McEliece system (based on error-correcting codes) is broken [257]. When-
ever you see a security proof that uses one of these problems, you should check in
detail if it avoids the mistakes made in the broken systems. Caution is advisable.
The original knapsack-based system was proposed by M.E. Hellman [190] who is
famous for being one of the inventors of public key cryptography. HFE [206] was

11.6
Generators Based on Other Hard Problems
255
proposed as an improvement of an older scheme after the author of HFE was able
to break it. The construction of correct security proofs is clearly incredibly difﬁcult
if even such skilled cryptologists have make mistakes in that respect.
An early attempt at using the knapsack problem for stream ciphers is the knap-
sack or subset sum generator [229].
The idea is to compute an LFSR x1,...,xn and output
yi =
xiw0 + ··· + xi−kww
B

for some weight w1,...,wk and some divisor B. Usually one chooses B as a power
of 2, i.e. one discards the lower bits of xiw0 + ··· + xi−kww. The idea behind the
generator is that addition is highly non-linear when interpreted as a Boolean func-
tion.
The knapsack generator has no rigorous security proof and several instances of
the knapsack generator are broken (see [101]).
From coding theory we know that, in general, decoding a linear code is a hard
problem. J.-B. Fisher and J. Stern [92] suggest Algorithm 11.9 as a stream cipher
based on the decoding problem.
Let ρ < 1. Given M ∈F⌊ρn⌋×n
2
and y = Mx with δn ≤w(x) ≤δ′n, determine x.
If the possible weight of x is too small, the problem is easy (there are simply not
enough possibilities for x). If the weight of x is about n
2 the problem is again easy
(there are too many possible solutions). Fisher and Stern suggest choosing δ and δ′
near the Gilbert-Warshamov bound.
Algorithm 11.9 The Fisher-Stern generator
Require: M ∈F⌊ρn⌋×n
2
, x ∈Fn
2, w(x) = δn
1: loop
2:
y ←Mx
3:
Split y into y1 and y2 with y1 has ⌈log2(
 n
δn

)⌉bits.
4:
Set x to the y1-th word in Fn
2 of weight δn.
5:
output y2
6: end loop
At the moment no attacks against the Fisher-Stern generator are known. The se-
curity proof is similar to Theorem 11.2 (for details see [92]). Despite its foundation
on a (presumably) hard problem, I am personally not quite convinced. Since
 n
δn

is
not a power of 2, one has to accept a small bias in the mapping performed in line 4.
Can an attacker really make no use of this bias? The order of the words of weight
δn used in line 4 is not used in the security proof. It is no problem to construct an
order which leads to a cycle of length 1 and hence a weak cipher.
Another hard problem that is used as a basis for security proofs is the solution of
multivariate quadratic equations over ﬁnite ﬁelds. A stream cipher that is based on
this problem is QUAD [19]. The structure of QUAD is simple (Algorithm 11.10).

256
11
The Blum-Blum-Shub Generator and Related Ciphers
Algorithm 11.10 The QUAD cipher
Require: S = (Q1,...,Qkn) is a secret system of kn random multivariate
quadratic equations over Fq in n variables.
1: loop
2:
Compute S(x) = (Q1(x),...,Qkn(x)) where x is the current internal state
3:
output Qn+1(x),...,Qkn(x)
4:
Set the new internal state to Q1(x),...,Qn(x).
5: end loop
QUAD is very fast, but the parameter mentioned in the QUAD design paper
is deﬁnitely too small, as shown in [286]. If even the designers of QUAD have
difﬁculties in choosing the right parameters, it seems very risky to use it in practice.
So if you want to go for maximal security and use a stream cipher with a security
proof, I recommend the Blum-Blum-Shub generator. Its simple structure makes it
unlikely to miss some hidden requirements. Its speed is fast enough for the gener-
ation of pseudo-random keys, and for all other purposes normal ciphers are better
than the available ciphers with security proofs. There is deﬁnitely no need to use
a more complicated cipher; you will have enough problems securing your imple-
mentation of the Blum-Blum-Shub generator against side channel attacks and other
programming errors. Do not forget to plan a big security margin. One never knows
what future progress will be in factoring algorithms.
For the same reason I recommend RSA as a public key cipher. It is simple, which
helps to avoid mistakes in the implementation. Even if it is slower than the alterna-
tives, this does not matter, since in most applications you use it only as a signature
scheme or in the key exchange protocol for a fast symmetric cipher. Simplicity is
an advantage at this point. Don’t forget that it has been unbroken for more than 30
years and is widely used. Thus if, contrary to common belief, factoring is not hard,
you will not miss the new developments and hopefully will be able to replace RSA
by a new and better cipher before someone breaks your system.
11.7 Unconditionally Secure Pseudo-random Sequences
We have seen that it is possible to construct pseudo-random generators that are
computationally secure under the assumption that the attacker only has access to
a small number of bits. It is a natural question whether one can also construct un-
conditionally secure pseudo-random sequences. The answer is yes, as was shown
by U.M. Maurer and J.L. Massey [180].
Deﬁnition 11.4 A function f : Fk
2 →Fn
2 is a (k,n,e) perfect local randomizer if
whenever X1,...,Xk are independent random variables with p(Xi = 1) = 1/2 then
every subset of size e of the n random variables f (X1,...,Xk) is a set of indepen-
dent random variables.

11.7
Unconditionally Secure Pseudo-random Sequences
257
The idea of Deﬁnition 11.4 is that we expand a small true random sequence of
length k to a pseudo-random sequence of length n. An attacker that only has access
to e bits of the pseudo-random sequence should not be able to distinguish it from a
true random sequence.
Theorem 11.10 A linear function f : Fk
2 →Fn
2 is a (k,n,e) perfect local random-
izer if and only if it is an (e + 1)-error detecting code.
Proof Let f (x) = Gx.
G is the generator matrix of an e + 1 error detecting code if and only if each set
of e columns of G is linearly independent.
If G has e columns that are linearly dependent then the corresponding e-bits of
the pseudo-random sequence satisﬁes a linear equation, which clearly distinguishes
them from a true random sequence.
If, on the other hand, each set of e columns of G is linearly independent then for
every projection P onto e coordinates, fP : x →PGx is a surjective linear mapping
from Fn
2 to Fe
2. Hence |f −1
p (y)| = 2n−e for all y ∈Fe
2. This proves that every e-tuple
has the same probability 2−e, i.e. f is a (k,n,e) perfect local randomizer.
□
Linear perfect local randomizers are, by Theorem 11.10, just linear codes. We
can apply all bounds from coding theory. For example, the Gilber-Varshamov bound
shows that a (k,n,e) perfect local randomizer exists if e ≤
k
log2 n.
One can view Theorem 11.10 as a warning. Linear functions are weak crypto-
graphic functions, but even they can be locally unpredictable. If you see a security
proof for a cipher you should always check if the assumptions are plausible. Can
the attacker really access only a small (polynomial) part of the pseudo-random bits?
What happens if the attacker is more powerful? Is the cipher robust against a more
powerful attacker? The Blum-Blum-Shub generator still seems to be secure if the
attacker has access to exponentially many bits, but can use only polynomial time.
This is not part of the security proof, but it is a good sign and makes us trust the
Blum-Blum-Shub generator even more. Even better, it is not realistic to assume that
the attacker will be able to access exponentially many bits, because this would mean
that the sender can compute exponentially many steps. However, if this were possi-
ble, the attacker could buy the same hardware and break the generator by factoring.
In contrast, the linear codes of Theorem 11.10 are broken immediately if the
attacker can access e + 1 bits and not only e bits as assumed in the security proof.
This is a bad feature which rules linear codes out as cryptographic codes. It is also
very plausible that an attacker can observe more bits than we expect.

Part III
Mathematical Background

Chapter 12
Computational Aspects
12.1 Bit Tricks
We often need to work with bitwise operations. The performance of many correla-
tion attacks depends on choosing efﬁcient bit manipulating algorithms.
Many bit tricks are not commonly known. To help the reader, this section col-
lects all the bit operation material we need. For a more extensive treatment, see the
excellent book [156].
We denote by a &b the bitwise AND operation, a |b is the bitwise OR and a ⊕b
is the bitwise XOR. All three operations satisfy the commutative and associative
law. In addition we have the following distributive laws:
(a |b)&c = (a &c)|(b &c),
(12.1)
(a &b)|c = (a |c)&(b |c),
(12.2)
(a ⊕b)&c = (a &c) ⊕(b &c).
(12.3)
By ¯a we denote the bitwise complement of a. We have the De Morgan laws:
a &b = ¯a | ¯b,
a |b = ¯a & ¯b,
a ⊕b = ¯a ⊕¯b.
(12.4)
We denote the left-shift by k-bits by a ≪k and the right-shift by a ≫k.
12.1.1 Inﬁnite 2-adic Expansions
It is often useful to work with inﬁnite 2-adic numbers a = (...a3a2a1a0)2. In
this representation we have −1 = (...1111)2 and we have fractions like −1/3 =
(...010101)2.
The equation x + ¯x = (...111)2 = −1 connects addition with the bitwise com-
plement.
A. Klein, Stream Ciphers, DOI 10.1007/978-1-4471-5079-4_12,
© Springer-Verlag London 2013
261

262
12
Computational Aspects
We will often use the masks
μ0 = (...0101010101010101)2 = −1/3,
μ1 = (...0011001100110011)2 = −1/5,
μ2 = (...0000111100001111)2 = −1/17
and so on. In general we have μk = −1/(22k + 1) and μk ≪2k = μk.
On a computer we only need ﬁnite precision. In the case of a 2d-bit register we
can compute the truncated constant μk by (22d −1)/(22k + 1).
12.1.2 Sideway Addition
An operation that is extremely useful for cryptography is the sideway addition.
Some correlation attacks (see Sect. 4.1.6.1) are effectively just a repeated appli-
cation of the sideway addition, hence optimizing this operation is very important for
the efﬁciency of these attacks.
For a word w = (wn−1 ...w1w0), we deﬁne
SADD(w) =
n−1

i=1
wi.
The sideway addition is also known as the population count or Hamming weight
of w.
Because it is so important, many processors have a special machine instruction
for this operation. For example, on a Mark I or Knuth’s MMIX, it is named SADD,
in the SSE4.2 instruction set it is POPCNT, and on a NEC SX-4 it is VPCNT. Unfor-
tunately the widespread Intel ix86 platform has no sideway addition.
On computers that do not have a sideway addition instruction there are essentially
two algorithms to perform sideway addition. The ﬁrst is based on table look-up
(Algorithm 12.1).
Algorithm 12.1 Sideway addition based on table look-up
Require: w is a k-byte word w = (wk−1,...,w0)256.
Ensure: S = SADD(w)
1: T := [SADD(0),SADD(1),...,SADD(255)] {T is computed at compile
time}
2: S := 0
3: for j := 0 to k −1 do
4:
S := S + T [wj]
5: end for

12.1
Bit Tricks
263
Alternatively we can use Algorithm 12.2, which is due to D.B. Gillies and
J.C.P. Miller ([281]). We present the algorithm in its version for 64-bit numbers
(originally it was developed for the 35-bit numbers on EDSAC).
Algorithm 12.2 Sideway addition (64 bit words)
1: S := w −((w ≫1)&μ0)
2: S := (S &μ1) + ((S ≫2)&μ1)
3: S := (S + (S ≫4))&μ2
4: S := (aS mod 264) ≫56 {where a = (11111111)256}
When Algorithm 12.2 reaches line 4. We have S = (s7 ...s0)8 with s0+···+s7 =
SADD(w). Since SADD(w) ﬁts without difﬁculty into 8 bits we can do the last
summation by a multiplication. If multiplication is slow on our CPU, we can replace
line 4 by S := S + (S ≫8), S := S + (S ≫16), S := (S + (S ≫32))&255.
In general Algorithm 12.1 is faster on 32-bit CPUs and Algorithm 12.2 is the
best on 64-bit CPUs.
If we know that SADD(w) is small (less than 4) there is a special algorithm due
to P. Wegner (see Exercise 17.23).
12.1.3 Sideway Addition for Arrays
Computing the sideway addition for a large array of words is a special problem. The
naive method simply calls one of the algorithms of the previous subsection for every
word and adds the results.
A better method was found by Harley and later improved by Seal [237, 277]. It
is based on a circuit called a carry save adder (CSA). A carry save adder is just a
sequence of independent full adders. A full adder takes the input bits a, b, c and
outputs the low-bit (sum) l and high-bit (carry) h. In Boolean notation:
l ←(a ⊕b) ⊕c,
h ←(a ∧b) ⊕

(a ⊕b) ∧c
 
.
Thus a carry save adder can be implemented with ﬁve operations and a standard
processor.
The simplest way to use the CSA is to group the elements of the array into groups
of three and apply one CSA and two word size sideway additions. The number of
ones is twice the output of the sideway addition of the high word plus the sideway
addition of the low word. This is Harley’s original idea. Seal made some improve-
ments and simpliﬁcations (see Algorithm 12.3).
An even more elaborate algorithm for this problem is developed in [82].
Table 12.1 (taken from [82]) shows how specialized algorithms can speed up the
sideway addition of arrays.

264
12
Computational Aspects
Algorithm 12.3 Sideway addition Harley-Seal method
1: S ←0, l ←0 {initialize}
2: for i from 0 to ⌊(n −1)/2⌋do
3:
(h,l) ←CSA(w2i,w2i+1,l)
4:
S ←S + SADD(h)
5: end for
6: S ←2S + SADD(l) {the high bits count double}
7: if n even then
8:
S ←S + SADD(wn) {If n is even we must treat the last word separately}
9: end if
10: return S
Table 12.1 Comparing
sideway addition algorithms
for arrays
Algorithm
Relative speed
Simple loop (table look-up)
100 %
Harley-Seal
73 %
Third iteration of Harley-Seal
59 %
Algorithm developed in [82]
39 %
12.2 Binary Decision Diagrams, Implementation Aspects
In Chap. 5 we described the idea of binary decision diagrams and related basic
algorithms. In this Appendix we have a closer look at the implementation details.
12.2.1 Memory Management
There are two strategies to maintain a BDD in memory. One uses a reference counter
per node and the other stores all nodes of a BDD in a continuous region. Both have
their pros and cons.
12.2.1.1 Nodes with Reference Counter
In this implementation we store for each node, its label, two pointers to the successor
nodes and a reference counter, which counts how many pointers point to this node.
If the reference counter reaches zero, we know that the node is no longer reach-
able and can be removed from the memory. In the C++ programming language
reference counters must be implemented explicitly, but there are languages like Java
which adds a reference counter to each object implicitly.
An advantage of this memory model is that nodes can be allocated one by one
and that the BDD algorithms needn’t care about memory management, since this

12.2
Binary Decision Diagrams, Implementation Aspects
265
Fig. 12.1 Memory layout
Fig. 12.2 A BDD node in memory
is done by the node structure. The nodes with reference counter model works well
with top-down algorithms and shows its full strength when it comes to automatic
reordering strategies.
A disadvantage is that the reference counters (and other required data structures
such as unique tables) generally need a lot of memory. A typical implementation
will use about 200 bits per node. Another drawback is that with this memory model
it is difﬁcult to make use of the cache.
12.2.1.2 An RPN Calculator for BDDs
For the cryptographic applications, I prefer an alternative memory model. The idea
is to implement a reverse polish notation (RPN) style calculator for BDDs. The
primary data structure is a stack of BDD-nodes.
Figure 12.1 shows the basic memory layout of the package.
One advantage of the stack-based approach is the memory management. There
is none. At the beginning the program allocates space for the stack and afterwards
it just updates the pointer NTOP that indicates the current top.
All BDDs on the stack are stored as a sequence of triples (v,l,h) of value, low
pointer and high pointer. According to the rules:
• The ﬁrst two nodes are (vmax + 1,0,0) and (vmax + 1,1,1) representing false
and true, respectively.
• The node of the BDD must be ordered by the index V of the variables. The nodes
with the highest value V comes ﬁrst.
• The last node must be the root of the BDD.
This allows a very compact representation (Fig. 12.2). A 64-bit word can hold a
node with two 27-bit pointers and a 9-bit value and still leaves us an extra status bit
which we interpret as a sign bit of the low pointer.

266
12
Computational Aspects
With this memory layout we can store a BDD with 227 nodes (about 1 GB of
memory). Taking into account that we need additional space to manipulate the BDD,
this is enough for a standard desktop machine with only 2–4 GB RAM. (This im-
plementation needs roughly only half of the memory in comparison with implemen-
tations based on the nodes with reference counter model.)
In addition, the sequential storage of the nodes works perfectly with breadth-ﬁrst
algorithms and makes the most out of the memory hierarchy of the computer system.
Of course the scheme also has disadvantages. The entire memory management
moves into the BDD manipulation algorithms, resulting in code with very involved
pointer operations (which is hard to maintain). Dynamic variable reordering es-
pecially requires a lot work, but can also be implemented very efﬁciently (see
Sect. 12.2.3).
Another drawback is that one can manipulate only the top BDD on the stack and
that different BDDs cannot share nodes (see Sect. 12.2.4 for a partial solution). In
the remainder of the section we describe how to implement an RPN calculator for
BDDs.
12.2.2 Implementation of the Basic Operations
In the stack-based setting the implementation of the most basic BDD operations is
straightforward.
Counting the number of solutions requires us to loop over all BDD nodes. This
algorithm proﬁts directly from the fact that the nodes are continuous in memory.
Additional storage is taken from the top of the allocated memory (again continu-
ous). Everything behaves well with respect to cache effects. In this way one can
also implement similar algorithms (determining the generating function, Boolean
optimization, evaluating the reliability polynomial, etc.).
Evaluating the BDD requires us to follow a path from the root to a sink. Here we
cannot make use of the continuous memory layout, but the stack-based approach
is no worse than a classical depth ﬁrst approach. Similarly there are algorithms
for ﬁnding the lexicographic least solution, enumerating all solutions, generating
random solutions, etc.
Simple transforms (complement and dual) can also make little use of the con-
tinuous memory. Computing the complement is an especially interesting case. In a
classical pointer based approach we can just exchange the two sinks (provided there
are no overlaps) and get the complement in O(1) time. In our stack-based approach
we must keep the order of the sinks (vmax +1,0,0) and (vmax +1,1,1) unchanged,
thus we have to loop over all BDD nodes and exchange 0 pointers by 1 pointers and
vice versa (O(B) time).
The reduction algorithm is more complicated. Here we pay the price for the con-
tinuous memory layout. See Algorithm 5.2 for a full description of the pointer op-
erations.
The reduction algorithm is the core of the package and all other complicated
algorithms are only variations. In Sect. 5.1.1 we have already seen how to apply a

12.2
Binary Decision Diagrams, Implementation Aspects
267
Boolean operation to two BDDs. The idea is to build the generic melt and then apply
reduction.
Another situation in which we can use reduction is the following.
Consider a BDD for the function f (x1,...,xn,y,z1,...,zm) with variable order
x1,...,xn,y,z1,...,zn. Assume that in every solution of f the variable y satisﬁes
y = g(x1,...,xn). We call y a check variable.
This is quite often the case. For example, one method to compute the BDD for
the functional composition
f

h1(x1,...,xn),...,hk(x1,...,xn)

is to compute
f (y1,...,yk) ∧

y1 = h1(x1,...,xn)

∧··· ∧

yk = hk(x1,...,xn)

.
BDD nodes labeled by a check variable have a very special form: either the low
or the high pointer must point to the sink ⊥.
Computing the BDD ∃y : f for a check variable y is simple. For every node la-
beled by y we give the pointer pointing to ⊥the value of the other pointer. This
yields a non-reduced BDD (all nodes labeled y are now superﬂuous since both
pointers point to the same node) for ∃y : f .
Reducing this BDD is faster than applying the unspecialized quantiﬁer ∃to the
BDD (variation of the melting process).
The restricted existential operator can be applied quite frequently, so it deserves
a specialized algorithm.
12.2.3 Implementation of Reordering Algorithms
Dynamic variable reordering [227] is an important technique that automatically
ﬁnds a good variable order for the given problem.
The elementary step in the variable reorder algorithm is swapping two adjacent
variables. Figure 12.3 illustrates the swapping operation. There are four types of
nodes. Nodes in level i are either tangled or solitary depending on whether or not
they have a j node as descendant. (In Fig. 12.3 a and c are tangled and b is solitary.)
Nodes of level j are either hidden or visible, depending on whether or not they can
be reached only from i nodes. (In Fig. 12.3 d and e are hidden but f is visible.)
The swap moves all solitary nodes one level down and all visible nodes one level
up. The tangled nodes change their labels from i to j. The hidden nodes are deleted
and new nodes have to be created to represent the calculation behind tangled nodes.
Several aspects of variable swapping do not cooperate well with the representa-
tion of the BDD chosen in our stack-based approach.
• The order of the nodes is changed. (In the ﬁgure the node order was a, b, c, d, e,
f and it became a, c, f , new, new, b, new.)

268
12
Computational Aspects
Fig. 12.3 Variable swapping
This is in general a big disadvantage for breadth-ﬁrst algorithms since it de-
stroys locality of the nodes, with negative effects with respect to caching.
In our special approach we would have to copy a node from one location to
another, which leads to the problem that all pointers pointing to that node would
have to be updated.
• Without reference pointers it is difﬁcult to determine the visible nodes. In fact, it
is impossible without scanning all the levels above i.
In [218] several strategies were considered to integrate dynamic reordering into
breadth-ﬁrst packages. The best solution of [218] was to temporarily change the data
structure, use conventional depth-ﬁrst algorithms for reordering and then change the
data structure back.
In the next sections another solution which is better suited to the breadth-ﬁrst
setting, and which can make more use from the cache, will be developed.
12.2.3.1 Jumping up
The easiest reordering operation is the “jump up” operation (one variable moves to
the top).
Figure 12.4 shows how to build a (non-reduced) BDD for the “jump up” opera-
tion. One simply doubles every node above the variable that jumps up and adds a
new node at the top.
Doubling the nodes is quite cheap and behaves well with respect to the cache.
The complexity of the reduction algorithm is not too bad.
12.2.3.2 Sifting down
Testing all possible variable orders is far too expensive. Instead of testing all possible
orders one usually uses sifting [227]. The idea is to temporarily ﬁx the order for all
but one variable and search the best possible position for the remaining variable.

12.2
Binary Decision Diagrams, Implementation Aspects
269
Fig. 12.4 The variable 3
jumps up
Sifting down is the key algorithm in our dynamic reordering tool set. We explain
the idea for the special case where the variable x1 should move to the bottom.
Consider the simple BDD shown in Fig. 12.5 (a). It is easy to construct a BDD for
the THEN-case (x1 equals true) with x1 moved down. Similarly we can construct
a BDD for the ELSE-case with x1 moved down. These two BDDs are shown in
Fig. 12.5 (b). (Note that it is only a coincidence that the THEN-BDD and the ELSE-
BDD are disjoint. We do not need this property.) To get the full BDD with x1 moved
down we must compute the OR of the THEN-BDD and the ELSE-BDD.
To do this we use the melting algorithm described in Sect. 5.1.1. It builds the
BDD one level at a time, which ﬁts perfectly with our continuous memory layout
and makes good use of the cache.
Note that the last level in melt must always consist of the three nodes 1T ⋄⊥,
⊥⋄1F and 1T ⋄1F independent from the BDD. We immediately see one possible
reduction: The node 1T ⋄1F is superﬂuous and can be replaced by ⊤. No other
reduction steps are possible since the melt itself is reduced and replacing a link to
1T ⋄1F by a link to ⊤cannot result in a new superﬂuous node, since the original
diagram for the melt contains no direct links to ⊤.
The basic form of the algorithm as described above moves the variable x1 down
into the last level. Now we extend the algorithm to sifting.
• Instead of choosing the THEN-BDD and the ELSE-BDD with respect to the ﬁrst
variable, we can use any other variable to construct the two sub-BDDs, i.e. we
can jump down with any variable.
• In the same way, if we construct in Fig. 12.5 the THEN-BDD with x1 at the last
level, we can get the THEN-BDD with x1 at any other level. The only difference
is that we must place in the THEN-BDD and the ELSE-BDD the check nodes for
the variable x1 not in the last level, but at some higher levels. Figure 12.6 shows,
for example, the THEN-BDD and the ELSE-BDD for the BDD of Fig. 12.5 (a)
with x1 moved to level 2.

270
12
Computational Aspects
Fig. 12.5 Moving a variable
from the top to the bottom
(a) A simple BBD
(b) the two sub-BBDs (x1 moved down)
(c) the melt
Fig. 12.6 Sifting down
• At this point we can let a variable jump from any level to any lower level. This
is still not quite what we need for dynamic variable reordering, but look a little
closer at the process of jumping down.

12.2
Binary Decision Diagrams, Implementation Aspects
271
When we let a = (1 ? b : c) be the ﬁrst node of the BDD (as in Fig. 12.5)—for
simplicity assume that the nodes b and c both lie at level 2—then the ﬁrst node
of the BDD in the THEN-case will be b (no matter how many levels the variable
x1 jumps down) and the ﬁrst node of the ELSE-case will be c. So, no matter how
many levels we jump down, the ﬁrst level of the melt will be b ⋄c.
In the case V (b) < V (c) the LO pointer of b ⋄c will point to LO(b) ⋄c. Thus
it does not matter if we call the node b ⋄c or b ⋄1F ; the structure of the melt is
unchanged. The case V (b) > V (c) is similar.
This leads to a variation of the sifting algorithm. We start the melting to process
for x1 jumping from the top to the bottom as shown in Fig. 12.5. Melt is computed
level wise as described in Sect. 5.1.1. After each level we compute the size of the
BDD under the condition that x1 jumps to that level. The size is the number of
nodes α ⋄α′ we have already constructed in the melt plus the number of nodes
below the current level in the original BDD plus the number of nodes labeled 1
we would have to create when stopping at this level.
By this procedure we ﬁnd for each level l the size SB(l) of the BDD under the
condition that x1 jumps to level l. Then we can select the l for which the BDD
size is the smallest and let x1 jump directly to that place. This is quite cheap: we
still have the original BDD that contains all nodes that lie below the new level of
x1 and the melt which contains all nodes which lie above the new level of x1, so
all we have to do is to compute the new nodes labeled with 1. This is again just a
variation of the melting algorithm.
Since the new sifting algorithm is a variation of the melting algorithm that
works level wise, it ﬁts well into the breadth-ﬁrst style of our package. Most
memory access will be made in a small memory range which is good for cache
effects.
• It remains to explain how to compute the size SB(l) of the BDD where x1 jumps
to level l. The problem is how to compute the number of nodes labeled with 1.
In the package we take an extreme approach to this problem. The algorithm
doesn’t simply calculate the number of 1 nodes. As an effect, the sifting algo-
rithm will sometimes place a variable one or two positions out of the optimal
place. However, in practice the speed-up that comes from the simpler algorithms
outweighs the small errors.
A way to get the correct number is the following. For each node α ⋄α′ in the
melt, let l be the level of the node and l′ be the smallest level of a node that has
α ⋄α′ as successor. If x1 jumps at some level between l′ and l then the node
α ⋄α′ would appear as a node with label 1 as result. So one can determine the
exact values SB(l) without computing the 1-nodes at every level.
12.2.4 Emulating a BDD Base
The biggest drawback of the implementation of a BDD package as an RPN-
calculator is that the different BDDs cannot share states.

272
12
Computational Aspects
Here is an idea to deal with this problem. To store two overlapping BDDs f and
g, add a special variable S and store the BDD for S?f : g. By adding further special
variables one can store more than two overlapping BDDs. To support this style
of storing several BDDs, the package should have a lot of exotic quantiﬁers such
as
Y
xi : f = f [xi/0] ∧f [xi/1] which replace, in this case, the normal Boolean
operations on two different BDDs.
This emulation of a BDD base by a single BDD works quite well in several appli-
cations. For example, it is not difﬁcult to compute the BDDs representing a binary
product (in [156] this problem was used as an example explaining the necessity of
BDD bases).
Nevertheless this style of emulating a BDD base has its limitations since we need
to handle all BDDs in the base simultaneously.
So if one often works with several overlapping BDDs, the node with reference
counter implementation is the best, but then one must accept the bigger memory
overhead and one cannot make as much use of the cache as the stack-based imple-
mentation.
12.3 The O-Notation
The O-notation is a convenient but somewhat ambiguous way to express the asymp-
totic behavior of a function. We use it mainly to express the asymptotic behavior of
an algorithm, hence its appearance in this chapter about computer science.
Deﬁnition 12.1 For functions f and g from N or R+ to R+ we write:
1. f (n) = O(g(n)) if
limsup
n→∞
f (n)
g(n) < ∞.
2. f (n) = o(g(n)) if
lim
n→∞
f (n)
g(n) = 0.
3. f (n) = Ω(g(n)) if
limsup
n→∞
f (n)
g(n) > 0.
4. f (n) = ω(g(n)) if
lim
n→∞
f (n)
g(n) = ∞.
5. f (n) = Θ(g(n)) if f (n) = O(n) and f (n) = Ω(n), i.e. if
0 < limsup
n→∞
f (n)
g(n) < ∞.

12.4
The Complexity Classes P and NP
273
What makes the O-notation ambiguous is the fact that there is no way to express
which variable should tend to inﬁnity. We may want to say f (m) = m2+1
m+1 = O(m),
in which case we mean that limsupm→∞f (m)/m < ∞. Or we say f (n) is polyno-
mial if there exists a constant c for which f (n) = O(nc), and here we clearly want
n →∞and not c →∞. Which variable is intended becomes clear in context. In
this book we follow the convention that the variable that tends to inﬁnity is called
n, N, m or similar, while c, d or similar denote constants.
To make the notation even more ambiguous it is a convention that if we use x,
y or h as a variable in the O-notation we mean that x, y or h should tend to zero
instead of inﬁnity. So we can write “A differentiable function f satisﬁes f (x) =
f (0) + xf ′(0) + o(x)” and mean limx→0
f (x)−f (0)−xf ′(0)
x
= 0.
Another aspect of the O-notation is how it is used in equality chains. If we write
O(f (n)) = O(g(n)) we mean that h(n) = O(f (n)) implies h(n) = O(g(n)). In
particular, in the context of the O-notation the equality is not symmetric. O(f (n)) =
O(g(n)) and O(g(n)) = O(f (n)) mean totally different things. For example we can
write f (n) = O(nlogn) = o(n2) and mean that f (n) = O(nlogn) and that implies
f (n) = o(n2). In contrast o(n2) = O(nlogn) is wrong. For example n1.5 = o(n2),
but n1.5 = O(nlogn) is false.
12.4 The Complexity Classes P and N P
Turing machines are a very simple abstract computer model which are commonly
used in complexity theory. To really understand the complexity classes P and NP
which feature in the basic theory behind most of the security proofs in cryptography
we need at least some knowledge of Turing machines. We present here a very short
introduction. For further reading, see [130] (ﬁrst edition; the section edition was
adapted to the lower lever of present day students and skips a lot of the interesting
material) or [219].
Deﬁnition 12.2 A non-deterministic Turing machine is an abstract computer which
consists of the following components:
• A control unit which can take its states from a ﬁnite set Q. Some states Qt ⊆Q
are marked as terminal states. The machine stops its computation if it reaches a
terminal state. One state q0 is marked as the initial state.
• A ﬁnite number k of memory tapes. Each position on a memory tape can contain a
symbol from a ﬁnite memory alphabet Σ. One element
∈Σ is a black symbol.
• A write-protected input tape which contain a word over the ﬁnite input alphabet
ΣI . The special symbol $ ∈ΣI is used to mark the end of the input.
• An output tape which contains the output which will be a word over the ﬁnite
output tape ΣO.
• A program or transition relation
δ ⊆

Q × ΣI × Σk
×

Q × (ΣI × MI) × (Σ × M)k × (ΣO × MO)


274
12
Computational Aspects
Fig. 12.7 A Turing machine
where MI , M, MO are the possible movements of input-head, memory-heads and
output-head. Possible movements are left, right and don’t move. The transition
function must ensure that the input head never leaves the ﬁnite input tape.
Figure 12.7 illustrates a Turing machine.
The Turing machine is said to be deterministic if δ is a function

Q × ΣI × Σk
→

Q × (ΣI × MI) × (Σ × M)k × (ΣO × MO)

.
The Turing machine starts the computation in the following state. The ﬁnite con-
trol is in the state q0. If the input is w, then $w$ is written on the input tape and the
input head is on the left $. All memory tapes and the output tape are ﬁlled with the
blank symbol.
The computation is a sequence of steps in which the machine performs the fol-
lowing actions:
• It changes the current state of the control.
• It changes the content of the memory tapes and the output tape at the current head
position.
• It moves all heads.
Allowed changes are given by the transition relation δ.
The computation stops if the machine reaches a terminal state. Whenever this
happens the content of the output tape is said to be the result of the computation.
Note that for deterministic Turing machines there exists a unique sequence of
allowed computation steps, but a non-deterministic Turing machine can have many
possible computation paths. It is possible that a non-deterministic Turing machine
can produce more than one result for an input. It is also possible that a Turing ma-
chine might produce no output at all, either because it runs in a inﬁnite loop and
never reaches a terminal state or, in the case of a non-deterministic Turing machine,
that there is simply no valid computation path.
It is rather difﬁcult to get an intuition of what computation in a non-deterministic
Turing machine means. A typical computation of a non-deterministic Turing

12.4
The Complexity Classes P and NP
275
machine is divided into two phases. In the ﬁrst phase the machine uses non-
deterministic steps to write down a possible solution of the problem. In the second
phase the machine uses deterministic steps to verify the guess from the ﬁrst phase.
If the guess was wrong, then the computation path will end without reaching a ter-
minal state. If the guess was correct it will be written to the output tape before the
machine switches to a terminal state.
The model 12.2 can be varied in several aspects. Here are some common variants
you can ﬁnd in the literature:
• The Turing machine has no special input tape. The input is just written on the ﬁrst
memory tape. (The special input tape is just a technical detail which allows us to
deﬁne sublinear tape complexity classes.)
• In many parts of theoretical computer science it is convenient to restrict the focus
to language recognition instead of computing the value of a function. In this case
the only possible outputs are “Yes” and “No” and we can remove the output tape
and signal the output by two special terminal states qYes and qNo.
• One can restrict the model to only one memory tape. There are several tape re-
duction theorems that prove that the exact number of tapes does not affect the
classical complexity classes P and NP.
• Several small variations may restrict the possible tape alphabets (for example one
can require Σ = ΣI = ΣO) or the movement of the heads (the input head may be
only read from left to right). Sometimes the transition relation of a deterministic
Turing machine is allowed to be a partial function.
In the following we will restrict our attention to language recognition, i.e. Turing
machines which produce either the output “Yes” or “No”.
Deﬁnition 12.3 For a ﬁnite alphabet Σ we denote by Σ∗the set of all ﬁnite words
over Σ.
A formal language L over a ﬁnite alphabet Σ is a subset of Σ∗.
For a Turing machine M the formal language recognized by M is the set of all
words x ∈Σ∗
I for which at least one computation path produces the output “Yes”.
(Note that in the case of a non-deterministic machine, it is perfectly acceptable that
some computation paths produce no output at all or even the output “No”. The word
can be accepted anyway.)
Let f : N →N.
The formal language L lies in the complexity class NTime(f ) (DTime(f )) if
there exists a non-deterministic (deterministic) Turing machine that accepts L and
for each word in w ∈L there exists an accepting computation path of at most f (|w|)
steps. L lies in NSpace(f ) (DSpace(f )) if there exists a non-deterministic (deter-
ministic) Turing machine that accepts L and for each word in w ∈L there exists an
accepting computation path which uses at most f (|w|) memory cells.
Classes like Dtime(f ) are model dependent; small changes of the model (such
as restricting the number of tapes) already have an affect. To get model independent
classes we take unions. This leads to the classical complexity hierarchy that starts
with:

276
12
Computational Aspects
• L = /
k∈N DSpace(k logn)
• NL = /
k∈N NSpace(k logn)
• P = /
k∈N DTime(nk)
• NP = /
k∈N NTime(nk)
• PSPACE = /
k∈N DSpace(nk) = /
k∈N NSpace(nk)
• EXP = /
k∈N DTime(kn)
• NEXP = /
k∈N NTime(kn)
• EXPSPACE = /
k∈N DSpace(kn) = /
k∈N NSpace(kn)
and then continues indeﬁnitely via double-exponentiation, and so on.
Hidden in the deﬁnition of the hierarchy there are several results from complexity
theory:
• The union theorem, which states that the classes are complexity classes in the
sense of Deﬁnition 12.3, i.e. there exists a function f : N →N with P =
DTime(f ), and so on.
• Hierarchy theorems which prove that each class is a subclass of the next one and
the inclusions NL ⊊PSPACE, P ⊊EXP, NP ⊊NEXP and so on are proper
inclusions.
In particular, the hierarchy is inﬁnite: there is no maximal complexity class.
• The theorem DSpace(f ) ⊆NSpace(f ) ⊆DSpace(f 2) which proves that
PSPACE is the same for deterministic and non-deterministic machines.
Most problems we face in mathematics belong to the lower classes of the hier-
archy. Almost all numerical methods belong to P. NP is the typical complexity of
combinatorial problems. In cryptography NP is the highest complexity an attack
can have. As the encryption algorithm should be in P, a non-deterministic Turing
machine can break the cipher in polynomial time by guessing the key and verifying
the guess. PSPACE and EXP are typical complexities for backtracking algorithms
as they appear in the analysis of games. Gröbner bases are one of the rare examples
in which we have to go up to EXPSPACE. Higher classes can appear in formal
logic. For example, deciding if a sentence in Presburger arithmetic (everything you
can write down with ∀, ∃, ∧, ∨, ¬, + and ≤where the variables are integers) is true
needs double exponential non-deterministic time.
Up to now no one has been able to prove that two neighbors in the hierarchy
are distinct. We know that NL ⊊PSPACE and hence at least one of the inclu-
sions NL ⊆P ⊆NP ⊆PSPACE must be a proper one. It is conjectured that all
classes in the hierarchy are distinct. P = NP would have a dramatic impact on sev-
eral areas, including cryptography. To prove that P ̸= NP is considered the most
important open problem in complexity theory.
The concepts of reduction and complete problems give some insight into the
nature of the complexity classes. These notions were originally deﬁned for NP, but
we can also introduce them in their full generality.
Deﬁnition 12.4 Let L1 ⊆Σ∗
1 and L2 ⊆Σ∗
2 . L1 is said to be reducible to L2 with
respect to some complexity class C if there exists a function f : Σ∗
1 →Σ∗
2 in C with
w ∈L1
⇐⇒
f (w) ∈L2

12.4
The Complexity Classes P and NP
277
for all w ∈Σ1. We write this as L1 ≤C L2.
We say L1 and L2 are equivalent with respect to C-reduction if L1 can be reduced
to L2 and vice versa:
L1 ≡C L2
⇐⇒
L1 ≤C L2 ∧L2 ≤C L1.
From now, ﬁx some reduction relation ≤. A language L is called hard for the
class C′ if for all L′ ∈C′ we have L′ ≤L and L is called complete for the class C′ if
L ∈C′ and is hard.
When we speak of NP-completeness or PSPACE-completeness we implicitly
assume that ≤is reduction with respect to deterministic polynomial time.
When we speak of NL-completeness or P-completeness we also implicitly as-
sume that ≤is reduction with respect to deterministic logarithmic space.
Reduction is a central concept, not only in complexity theory: all security proofs
in cryptography are reduction theorems. Reduction can also be used to design algo-
rithms (see, for example, Sect. 12.5.2).
Here is a small list of NP-complete problems:
• The satisﬁability problem: Given a Boolean expression, does there exist an al-
location of the variables such that it evaluates to true? This is usually the ﬁrst
problem which is proved to be NP-complete. All other NP-complete problems
are reduced either directly or indirectly to this problem.
• The knapsack problem: Given a set S of integers and an integer n, does there
exists S′ ⊂S with 
s∈S′ s = n? This problem is infamous in cryptography be-
cause several broken cryptography systems tried to base their security on this
problem [200].
• The linear decoding problem: Given a generator matrix G of a linear code C and
a word w, ﬁnd the codeword c ∈C with minimal distance to w.
• Integer linear programming: Given a matrix A ∈Zn×m and a vector b ∈Zn, does
there exist a vector x ∈Zm with Ax ≤b? This problem is interesting in two
respects. Firstly, it is one of the rare cases for which it is much easier to prove that
the problem is NP-hard than to prove that it is in NP. Secondly, the analogous
problem for rational linear programming is in P.
• Given a lattice L, determine the shortest vector in L.
As mentioned before, it is still an open problem to prove P ̸= NP. Here is an
explanation of why it is so difﬁcult. One can extend the Turing machine model by
oracles. An oracle gives a Turing machine an answer to a speciﬁc question in one
step. Almost all proof techniques that work for normal Turing machines also work
for Turing machines with oracles and give the same result. However, there exists an
oracle O1 such that PO1 = NPO1 and another oracle O2 with PO2 ̸= NPO2.
So a proof of P ̸= NP must make use of the fact that no oracle is present, which
requires new proof techniques.
It is not known whether or not NP is closed under complements. The class of
problems for which the complement is in NP is denoted by co-NP. It is conjec-
tured that NP ̸= co-NP, which would also imply P ̸= NP.

278
12
Computational Aspects
The breaking of a cryptosystem is a problem in co-NP ∩NP—just guess the
key and you can verify everything in polynomial time.
A big problem for security proofs is that no problem is known to be complete
for the class co-NP ∩NP. There are essentially two strategies to deal with the
problem:
• Choose a hopefully difﬁcult problem in co-NP ∩NP (factoring, discrete loga-
rithm) and reduce the security of the cipher to it.
• Choose an NP-complete problem. Restrict it somehow so that the resulting prob-
lem lies in co-NP ∩NP and reduce the security of the cipher to it.
The ﬁrst strategy seems more reasonable and most systems that use this strat-
egy remain unbroken. The second strategy is attractive since there is a wide variety
of NP-complete problems to choose from, but restricting an NP-complete prob-
lem does not guarantee that the restricted problem is still hard. Many systems that
have followed the second strategy are now broken (especially if they start with the
knapsack problem [200]).
12.5 Fast Linear Algebra
In algebraic attacks (see Sect. 6.1.2) we have to deal with very large systems of
linear equations. For systems of that size the usual methods from linear algebra
are too slow. The attacks beneﬁt a lot from advanced methods from linear algebra.
(Some of these methods are infamous because they are numerically not as stable as
the classical methods, which is of course irrelevant to cryptography, where we deal
only with ﬁnite ﬁelds.)
12.5.1 Matrix Multiplication
12.5.1.1 Simple Algorithms
Let R be a ring and let A,B ∈Rn×n be two n × n matrices. Our goal is to compute
the product C = AB.
By deﬁnition of matrix multiplication
ci,j =
n

k=1
ai,kbk,j.
Converting this deﬁnition into a computer program easy. We summarize it as fol-
lows.
Theorem 12.1 The naive matrix multiplication algorithm needs n3 ring multiplica-
tions and n3 −n2 ring additions.

12.5
Fast Linear Algebra
279
Until the computer era arrived, no one had ever considered if there could be other
ways to perform matrix multiplication. Before we start with the asymptotically fast
algorithms, we give a small variation of the naive algorithm (see also [282]).
Algorithm 12.4 Winograd’s algorithm for multiplying small matrices
Require: R commutative ring, n even, A,B ∈Rn×n.
Ensure: C = AB
1: for i from 1 to n do
2:
Ai = n/2
k=1 ai,2k−1ai,2k
3: end for
4: for j from 1 to n do
5:
Bj = n/2
k=1 b2k−1,jb2k,j
6: end for
7: for i from 1 to n do
8:
for j from 1 to n do
9:
ci,j = n/2
k=1(ai,2k−1 + b2k,j)(ai,2k + b2k−1,j) −Ai −Bj
10:
end for
11: end for
Theorem 12.2 If R is a commutative ring, Algorithm 12.4 is correct and needs
n3/2 + n2 ring multiplications and 3
2n3 + 2n2 −n ring additions.
Proof To prove the correctness we must just expand the expression in line 9 and get
ci,j =
n/2

k=1
(ai,2k−1 + b2k,j)(ai,2k + b2k−1,j) −Ai −Bj
=
n/2

k=1
ai,2k−1ai,2k +
n/2

k=1
b2k,jai,2k
+
n/2

k=1
ai,2k−1b2k−1,j +
n/2

k=1
b2k,jb2k−1,j −Ai −Bj
=
n/2

k=1
b2k,jai,2k +
n/2

k=1
ai,2k−1b2k−1,j
=
n

k=1
ai,kbk,j.
Note how we used that the ring is commutative in the last equation.
Algorithm 12.4 needs n(n/2) multiplications to compute the Ai and n(n/2) to
compute the Bj. The main loop in line 9 needs n2(n/2) multiplications. Counting
the additions is similar.
□

280
12
Computational Aspects
Algorithm 12.4 trades multiplications for additions. Since addition is normally
faster than multiplication we can expect Algorithm 12.4 to be better than the naive
algorithm on most machines.
12.5.1.2 Strassen’s Algorithm
The naive matrix multiplication algorithm needs 8 multiplications and 4 additions
to multiply 2 × 2 matrices.
In 1969 Strassen found a method to multiply two 2 × 2 matrices (over non-
commutative rings) with only 7 multiplications and 18 additions [260].
Later Winograd improved this to 7 multiplications and only 15 additions (see
Algorithm 12.5).
Algorithm 12.5 Strassen’s algorithm to multiply 2 × 2 matrices
A =
A11 A12
A21 A22

B =
B11 B12
B21 B22

S1 ←A21 + A22
T1 ←B12 −B11
S2 ←S1 −A11
T2 ←B22 −T1
S3 ←A11 −A21
T3 ←B22 −B12
S4 ←A12 −S2
T4 ←T2 −B21
P1 ←A11B11
P5 ←S1T1
P2 ←A12B21
P6 ←S2T2
P3 ←S4B22
P7 ←S3T3
P4 ←A22T4
U1 ←P1 + P2
U5 ←U4 + P3
U2 ←P1 + P6
U6 ←U3 −P4
U3 ←U2 + P7
U7 ←U3 + P5
U4 ←U2 + P5
C = AB =
U1 U5
U6 U7

The trade-off of one multiplication for 11 additions in a 2 × 2 matrix multiplica-
tion may seem insigniﬁcant. However, Algorithm 12.5 holds for non-commutative

12.5
Fast Linear Algebra
281
rings, and so we can apply recursion. Interpret a 2k ×2k matrix as a 2×2 block ma-
trix with block size 2k−1 × 2k−1. So one can perform the multiplication of 2k × 2k
matrices by 7 multiplications and 15 additions of 2k−1 ×2k−1 matrices. To multiply
the 2k−1 × 2k−1 matrices you use Strassen’s formula again. This is very similar to
Karatsuba’s algorithm for multiplying n-digit numbers (see Sect. 11.3). Let M(n)
be the number of operations needed to compute the product of two n × n matrices
by Strassen’s algorithm. Then
M

2k
= 7M

2k−1
+ 15

2k−12.
(12.5)
Applying Theorem 11.6 to Eq. (12.5) we get that for n = 2k, the multiplication of
two n × n matrices takes O(7k) = O(nlog2(7)) operations. Since log2 7 ≈2.807 < 3
this is much faster than the naive algorithm.
To extend Strassen’s algorithm to matrix dimensions different from 2k one has
to pad the matrices with zeros. The asymptotic complexity stays the same but the
constant gets slightly worse if n is not a power of 2.
How practical is Strassen’s algorithm? It trades one multiplication of n/2 × n/2
matrices for 11 additions. Assuming that we multiply the n/2 × n/2 matrices using
the naive algorithm, for n ≥22 we get (n/2)3 ≥11(n/2)2, so one may expect that
Strassen recursion is favorable for matrix dimensions above 22. In real world im-
plementations there is some additional overhead, but Strassen’s algorithm is already
favorable for small matrix dimensions. The crossover point lies between 32 and 128,
depending on the hardware [13, 127].
It should be mentioned that Strassen’s algorithm is numerically less stable than
the naive algorithm. This is a general disadvantage of the advanced algorithms. For
applications in cryptography, these considerations are irrelevant, since we work only
with ﬁnite ﬁelds.
12.5.1.3 Pan’s Algorithm
After Strassen’s discovery it was natural to ask what the smallest ω is such that an
O(nω) algorithm for n × n matrix multiplication exists. The naive algorithm shows
ω ≤3 and Strassen’s algorithm shows ω ≤log2 7. In 1978 Pan was able to improve
Strassen’s algorithm [203]. The current world record is ω ≤2.376 (an algorithm of
Coppersmith and Winograd [59]). The interesting ﬁeld of fast matrix multiplications
is treated in detail in the books [40, 204].
The only lower bound on ω is the trivial lower bound ω ≥2. It is conjectured that
for every ϵ > 0 there is an O(n2+ϵ) algorithm for matrix multiplication. Perhaps
there is an O(n2 logn) algorithm for matrix multiplication (somewhat analogous to
the famous O(nlog(n)) algorithm for polynomial multiplication).
Most textbooks on computer algebra describe Strassen’s algorithm in detail and
then mention the advanced algorithms as we did above. Then they note that the con-
stants in the algorithm of Coppersmith and Winograd are so large that the algorithm
is only advantageous for astronomical large n. This leaves the reader with impres-
sion that, from the practical point of view, Strassen’s algorithm is the fastest matrix

282
12
Computational Aspects
multiplication algorithm, i.e. for all practical purposes ω ≈2.808. Indeed it is the
only algorithm implemented in most systems.
However, Pan gives in his book [204] a class of algorithms that achieve ω ≈
2.775 and which are superior to Strassen’s algorithm for matrices of dimension
above a few thousand. Since we can easily reach such dimensions in algebraic at-
tacks, these algorithms are important to cryptology.
In the remaining section we will describe Pan’s algorithm and, in the process, also
introduce techniques which are important for the understanding of more theoretical
results like the current world record of Coppersmith and Winograd.
The ﬁrst ingredient is the notion of bilinear algorithms as trilinear forms.
The naive matrix multiplication is described by the equations
ci,j =
n

k=1
ai,kbk,j.
To transform these n2 equations into a single algebraic expression we multiply the
right-hand side by the formal variables ci,j and add the equations together. Thus
matrix multiplication is described by the single expression
n

i=1
n

j=1
n

k=1
ai,kbk,jci,j.
(12.6)
Using this notation, Strassen’s algorithm translates to the following identity.
2

i=1
2

j=1
2

k=1
ai,kbk,jci,j
= a1,1b1,1(c11 + c1,2 + c2,1 + c2,2)
+ a1,2b1,2c1,1
+ (a1,2 −a2,1 −a2,2 + a1,1)b2,2c1,2
−a2,2(b2,2 −b1,2 + b1,1 −b2,1)c2,1
+ (a2,1 + a2,2)(b1,2 −b1,1)(c1,2 + c2,2)
+ (a2,1 + a2,2 −a1,1)(b2,2 −b1,2 + b1,1)(c1,2 + c2,1 + c2,2)
+ (a1,1 −a2,1)(b2,2 −b1,2)(c2,1 + c2,2).
(12.7)
To obtain the algorithm from Eq. (12.7), one must collect the coefﬁcients of the
formal variables ci,j on both sides.
That Strassen’s algorithm needs only 7 multiplications to obtain the product of
2 × 2 matrices means for the trilinear Eq. (12.7) simply that the right hand side has
only 7 terms. Thus the search for fast algorithms is equivalent to the problem of
decomposing a given trilinear form into few terms.

12.5
Fast Linear Algebra
283
Deﬁnition 12.5 The rank of a trilinear form T (A,B,C) is the minimal number of
terms in all possible decompositions.
To make the trilinear form in Eq. (12.6) more symmetric, we swap the indices of
ci,j and write:
T (A,B,C) =
n

i=1
n

j=1
n

k=1
ai,kbk,jcj,i.
(12.8)
The goal is to ﬁnd good upper bounds for the rank of T (A,B,C).
For the following let R be a (non commutative) ring with 1. The second ingredi-
ent for Pan’s algorithm is Lemma 12.3.
Lemma 12.3 Let n ∈N and assume n+1 is invertible in R. For a1,...,an,b1,...,
bn ∈R, let
a0 = −
n

i=1
ai,
b′
0 = −
n
i=1 bi
n + 1 ,
b′
i = bi + b0.
Then n
i=0 ai = n
i=0 b′
i = 0 and
n

i=1
aibi =
n

i=0
aib′
i.
Proof Let A = n
i=1 ai and B = n
i=1 bi then
n

i=0
aib′
i = A
B
n + 1 +
n

i=1
ai

bi −
B
n + 1

=
n

i=1
aibi +
n

i=1
ai

−
B
n + 1

+ AB
n + 1
=
n

i=1
aibi.
□
Lemma 12.3 allows us to transform two n × n matrices A and B into two (n +
1)×(n+1) matrices A′ and B′ with the property that AB is just a submatrix of A′B′
and that the row sums and column sums of A′ and B′ are 0. Having the matrices in
this special form is advantageous, since many terms will cancel automatically.

284
12
Computational Aspects
Pan’s algorithm is best described as the problem of computing three matrix prod-
ucts C = AB, W = UV and Z = XY simultaneously.
In the notation with trilinear forms we have to compute

i,j,k
ai,jbj,kck,i + uj,kvk,iwi,j + xk,iyi,jzj,k
(12.9)
(3n3 terms). Note the way we have chosen the indices.
We assume that the row and column sums are 0, i.e. 
i ai,j = 
j ai,j = 0 and
so on. Lemma 12.3 shows how to bring the input matrices A, B, U, V , X and Y
into this form, and we will see later how to ensure this form for the output matrices
C, W, Z.
Now consider the following trilinear form:

i,j,k
(ai,j + uj,k + xk,j)(bj,k + vk,i + yi,j)(ck,i + wi,j + zj,k)
(n3 terms).
Pan used the more graphical aggregating table
ai,j
bj,k
ck,i
uj,k
vk,i
wi,j
xk,i
yi,j
zj,k
to describe it.
Multiplying out we get all terms of Eq. (12.9) and most other terms cancel, for
example

i,j,k
ai,j ⊗bj,k ⊗wi,j =

i,j
ai,j ⊗

k
bj,k

⊗wi,j =

i,j
ai,j ⊗0 ⊗wi,j = 0.
The only remaining terms are the diagonal correction terms ai,jvk,izj,k,
uj,kyi,jck,i and xk,ibj,kwi,j and the anti-diagonal correction terms xk,ivk,ick,i,
uj,kbj,kzj,k and ai,jyi,jwi,j.
First consider the anti-diagonal correction terms. In each term only two different
indices appear. Thus there are just 3n2 different anti-diagonal correction terms. So
computing this correction is cheap in comparison to computing the main terms.
We need a way to deal with the diagonal correction terms. Here comes the third
ingredient for Pan’s algorithm: double the size of the matrices.
We now have 2n × 2n matrices with the property that
n

i=1
ai,j =
2n

i=n+1
aij =
n

j=1
ai,j =
2n

j=n+1
aij = 0.
We say that such matrices have Pan-normal-form.
Instead of one aggregating table, we now have 8 aggregating tables. In the tables
we use ˜i as an abbreviation for i + n and similarly ˜j = j + n, ˜k = k + n

12.5
Fast Linear Algebra
285
ai,j
bj,k
ck,i
uj,k
vk,i
wi,j
xk,i
yi,j
zj,k
−ai,j
bj,˜k
−c˜k,i
u ˜j,k
vk,i
wi, ˜j
xk,˜i
y˜i,j
zj,k
ai, ˜j
b ˜j,k
ck,i
−uj,k
vk,˜i
−w˜i,j
x˜k,i
yi,j
zj,˜k
a˜i,j
bj,k
ck,˜i
uj,˜k
v˜k,i
wi,j
−xk,i
yi, ˜j
−z ˜j,k
a˜i, ˜j
b ˜j,˜k
c˜k,˜i
u ˜j,˜k
v˜k,˜i
w˜i, ˜j
x˜k,˜i
y˜i, ˜j
z ˜j,˜k
−a˜i, ˜j
b ˜j,k
−ck,˜i
uj,˜k
v˜k,˜i
w˜i,j
x˜k,i
yi, ˜j
z ˜j,˜k
a˜i,j
bj,˜k
c˜k,˜i
−u ˜j,˜k
v˜k,i
−wi, ˜j
xk,˜i
y˜i, ˜j
z ˜j,k
ai, ˜j
b ˜j,˜k
c˜k,i
u ˜j,k
vk,˜i
w˜i, ˜j
−x˜k,˜i
y˜i,j
−zj,˜k
Observe that every diagonal correction term appears exactly twice, once with
positive sign and once with negative sign. Hence the diagonal correction terms can-
cel.
Now have a closer look at the remaining 24n2 anti-diagonal correction terms.
Consider the following 8 anti-diagonal correction terms:
n(aijyijwij −aijy˜ijwi ˜j −ai ˜jyijw˜ij + a˜ijyi ˜jwij
+ a˜i ˜jy˜i ˜jw˜i ˜j −a˜i ˜jyi ˜jw˜ij −a˜ijy˜i ˜jwi ˜j + ai ˜jy˜ijw˜i ˜j).
This is essentially the same trilinear form as the product of 2 × 2 matrices. So one
can apply Strassen’s trick to replace the eight terms by only 7 terms. This reduces
the number of anti-diagonal correction terms from 24n2 to 21n2.
At this point we have decomposed the trilinear form of three matrix products of
Pan normalized 2n × 2n matrices into only 8n3 + 21n2 terms instead of 24n3 terms
(naive implementation). This is a signiﬁcant speed-up.
Lemma 12.3 explains how to transform the input matrices to Pan normal form.
We must still deal with the output matrices. If one computes all products of the
aggregation tables and the anti-diagonal correction terms and collect the results in
the variables indicated by the trilinear form, we do not get the pan normalized output
C =
 C11 C12
C21 C22

but a matrix ˆC =
 ˆC11 ˆC12
ˆC21 ˆC22

.
The relation between C11 = (cij)1≤i,j≤n and ˆC11 = (ˆcij)1≤i,j≤n is
ˆci,j = ci,j + di + ej.
(12.10)
Similar relations hold between C12 and ˆC12, and so on.

286
12
Computational Aspects
To compute the Pan-normalized result one has to ﬁnd di and ej in Eq. (12.10).
Note that with ei and dj, e′
i = ei −x and d′
j = dj + x are also solutions of (12.10).
So one can assume without loss of generality that d1 = 0. Then
n

j=1
ej =
n

j=1
ˆc0,j,
di = n−1
 n

j=1
ˆci,j −
n

j=1
ej

,
ej = n−1
 n

i=1
ˆci,j −
n

i=1
di

.
So one can compute correction values di and ei and hence the Pan-normalized
result C with O(n2) operations.
Pan’s algorithm can also be adapted to compute just one matrix product instead
of three matrix products simultaneously. One just has to specialize the nine matrices
to A = U = X, B = V = Y and C = W = Z.
The algorithm becomes less symmetric. We omit the details, which can be found
in Chap. 31 of [203] or in our fully documented reference implementation. Theo-
rem 12.4 summarizes the results (using the same terminology as Lemma 12.3).
Theorem 12.4 If n ∈N is even and n+2
2
is invertible in R, then the product of two
matrices in Rn×n can be computed by using only
1
3n3 + 15
4 n2 + 97
6 n + 20
ring multiplications.
Algorithm 12.6 shows a bird’s eye view of Pan’s algorithm.
Algorithm 12.6 Pan’s matrix multiplication
Input: A,B ∈R2n×2n , n + 1 must be invertible in R
1. Convert A, B
to A′, B′ ∈R(2n+2)×(2n+2) into Pan-normal-form using
Lemma 12.3. This takes O(n2) operations (only scalar multiplications, no ring
multiplication).
2. Compute the 8 aggregating tables and the anti-diagonal corrections terms and
collect the results. There are n3/3 + 15
4 n2 + 97
6 n + 20 ring multiplications.
3. Renormalize the result to get C′ = A′B′. (O(n2) operations, again no ring mul-
tiplication).

12.5
Fast Linear Algebra
287
Table 12.2 Speed of Pan’s algorithm
Size of the matrix (2n)
44
46
48
50
52
Number of ring multiplication
36386
41144
46300
51870
57870
Exponent in asymptotic algorithm
2.77522
2.77509
2.77508
2.7752
2.77532
Of course, as in the case of Strassen’s algorithm, Pan’s algorithm shows its full
strength only if applied recursively to block matrices. The optimal speed is achieved
if one chooses n ≈25 in Algorithm 12.6. In Table 12.2 one can see the achievable
speed for different sizes of n. As one can see, Pan’s algorithm needs asymptoti-
cally n2.775 operations to multiply n × n matrices, which is better than Strassen’s
algorithm (n2.807 operations). The crossover point is quite low, between 1000 and
10000, so Pan’s algorithm is really interesting in practice.
12.5.1.4 Binary Matrices
When multiplying binary matrices we must take care to make use of the inherent
parallelism of bitwise operations.
In this section we always work with 64-bit words.
One can store a 64 × 64 binary matrix as an array of 64 words. Each word repre-
sents a whole row. Algorithm 12.7 shows how to multiply two such 64 × 64 binary
matrices.
Algorithm 12.7 Multiplication of 64 × 64 binary matrices
Require: A,B two arrays of 64 words, each word represent a matrix row (index
numeration starts with 0).
Ensure: C = AB
1: for i from 0 to 63 do
2:
x ←A[i]
3:
if x odd then
4:
C[i] ←B[0]
5:
else
6:
C[i] ←0
7:
end if
8:
for j from 1 to 63 do
9:
X ←X/2 {Division by 2 is just a right shift}
10:
if x odd then
11:
C[i] ←C[i] ⊕B[j]
12:
end if
13:
end for
14: end for

288
12
Computational Aspects
Algorithm 12.7 uses just the deﬁnition of matrix multiplication, but note how we
use the bitwise XOR in line 11 to do 64 additions in parallel. The multiplication is
hidden in the IF-statements; since F2 has only 0 and 1 as elements an IF-statement
is a good replacement for a multiplication.
Algorithm 12.7 is very efﬁcient with respect to its use of the inherent parallelism
of bitwise operations, but it makes no use of the mathematics that stands behind
Strassen’s algorithm and the other advanced matrix multiplications. Addition of two
64 × 64 binary matrices needs only 64 word operations, but multiplication with Al-
gorithm 12.7 needs more than 10000 operations. This is a clear sign that we should
switch from the naive algorithm to Strassen’s algorithm earlier.
The right idea is to interpret a 64-bit word as an 8 × 8 binary matrix, i.e. we
interpret the word (a63 ...a1a0)2 as the binary matrix
A =
⎛
⎜⎜⎜⎝
a63
a62
···
a56
a55
a54
···
a48
...
...
...
...
a7
a6
···
a0
⎞
⎟⎟⎟⎠.
The bitwise XOR is matrix addition. In [155] Knuth describes the abstract ma-
chine MMIX which he uses in his books “The Art of Computer Programming”.
The MMIX machine has the instruction MXOR, which is the multiplication of two
64-bit words as 8 × 8 binary matrices. Knuth notes that the MXOR instruction has
many applications (rearranging bytes, multiplication in F256, etc.), but at the mo-
ment real machines do not include it. Instead real machines “have a dozen or so ad
hoc instructions that handle only the most common special cases”.
As long as we do not have an MMIX machine we can use Algorithm 12.8 to
simulate MXOR by common instructions (see also [154], which describes a program
simulating an MMIX machine).
Algorithm 12.8 Multiplication of 8 × 8 binary matrices (MXOR)
Require: A,B two 64-bit words, representing 8 × 8 binary matrices
Ensure: C is a 64-bit word representing the 8 × 8 binary matrix C = AB
1: last_column ←0x0101010101010101
2: last_row ←0x00000000000000FF
3: C ←0
4: for i from 0 to 7 do
5:
C ←C ⊕(((a ≫i)&last_column) · last_row)&
(((b ≫(8i))&last_row) · last_column)
6: end for
Algorithm 12.8 looks very involved, since it makes heavy use of bit manipula-
tions, but it is just the naive matrix multiplication. Let’s have a closer look at the

12.5
Fast Linear Algebra
289
individual operations. When multiplying
A =
⎛
⎜⎝
a00
···
a07
...
...
...
a70
···
a77
⎞
⎟⎠
and
B =
⎛
⎜⎝
b00
···
b07
...
...
...
b70
···
b77
⎞
⎟⎠
we need
C′ =
⎛
⎜⎝
a07b70
···
a07b77
...
...
...
a77b70
···
a77b77
⎞
⎟⎠
as part of the result.
To get C′ we need
A′ =
⎛
⎜⎝
a07
···
a07
...
...
...
a77
···
a77
⎞
⎟⎠.
We obtain A′ by extracting the last column of A (done by the bit operation
A &0x0101010101010101) and copy the elements to the other columns (multi-
plication by 0xFF, the multiplication will not produce a carry).
Similarly we obtain
B′ =
⎛
⎜⎝
b70
···
b77
...
...
...
b70
···
b77
⎞
⎟⎠
by extracting the last row of B (B &0xFF) and copying this row to the other rows
(multiplication by 0x0101010101010101). This is exactly what is done in line 5 of
Algorithm 12.8.
With Algorithm 12.8 and the bitwise XOR we have all we need to deal efﬁciently
with 8×8 binary matrices. Larger binary matrices are built from these basic blocks.
12.5.2 Other Matrix Operations
In the previous section we focused on matrix multiplication algorithms. In this sec-
tion we want to explain why.
Let
ω(MM) = inf

ω|there is an O

nω
algorithm for n × n matrix multiplication

.
As we learned in the previous section ω(MM) ≤2.376 (see [59]). It is still an open
problem whether the exponent ω(MM) is independent of the ground ﬁeld. For the
moment all we know is that if ω(MM) is at all dependent on the ground ﬁeld then

290
12
Computational Aspects
it depends only on the characteristic of the ground ﬁeld. We will write ωF (MM) if
the ground ﬁeld F is important.
Similarly we deﬁne ω(MI) as the inﬁmum over all ω for which an O(nω) algo-
rithm for inverting an n × n matrix exists. The exponent for the problem of comput-
ing the determinant of an n × n matrix is denoted by ω(DET) and ω(SLE) is the
exponent for solving a system of n linear equations in n variables.
Theorem 12.5 For an arbitrary ﬁeld F we have
ωF (SLE) ≤ωF (MM) = ωF (MI) = ωF (DET).
Proof Consider the following identity for block matrices.
Let
X =
X11
X12
X21
X22

and assume that X11 is invertible. This is no restriction, just choose a suitable basis.
Let
Z = X22 −X21X−1
11 X12
then
X =

I
0
X21X−1
11
I
X11
0
0
Z

I
X−1
11 X12
0
I

,
(12.11)
X−1 =

I
−X−1
11 X12
0
I

X−1
11
0
0
Z−1

I
0
−X21X−1
11
I

.
(12.12)
Let T (MI(n)) be the number of ring operations needed to compute the inverse
of an n × n matrix. Equation (12.12) shows that inverting a 2n × 2n matrix can be
done by two inversions of n×n matrices (computing X−1
11 and Z−1) and 6 multipli-
cations of n × n matrices (two multiplications to compute Z and 4 multiplications
to evaluate Eq. (12.12)). Furthermore, we need two matrix additions. Altogether
T

MI(2n)

≤2T

MI(2n)

+ 6O

nω(MM)
+ 2n2 ,
which proves T (MI(2n)) = O(nω(MM)) or ω(MI) ≤ω(MM).
From Eq. (12.11) we see det(X) = detX11 detZ. Hence the number T (DET(n))
of operations needed to compute the determinant of an n × n matrix satisﬁes
T

DET(2n)

≤2T

DET(n)

+ 2O

nω(MM)
+ n2 + 1
and hence ω(DET) ≤ω(MM).
To prove ω(MM) ≤ω(MI) choose X11 = I. Then the lower right submatrix of
X−1 is Z−1 = (X22 −X21X21)−1. We obtain the product X21X21 by ﬁrst inverting
the 2n × 2n matrix X and then inverting the n × n matrix Z−1. Therefore
T

MM(n)

≤T

MI(2n)

+ T

MI(n)

+ n2

12.5
Fast Linear Algebra
291
which proves ω(MM) ≤ω(MI).
For the proof that ω(MI) ≤ω(DET), see [18].
Solving a linear system of equations can be done by inverting the matrix and
multiplying the inverse by a vector which shows
T

SLE(n)

≤T

MI(n)

+ n2
or ω(SLE) ≤ω(MI). It is conjectured that ω(SLE) = ω(MI), but at the moment this
is an open problem.
□
Theorem 12.5 is a theorem on the asymptotic behavior of matrix algorithms, but
note that the constructions used in the proof all have very small constants, so they
give feasible algorithms.
12.5.3 Wiedmann’s Algorithm and Black Box Linear Algebra
The problem of solving linear equations is of special importance to cryptography.
An interesting algorithm for this problem is due to Wiedmann [280]. It is especially
useful for sparse linear systems (which are quite common in cryptography). It is
also a good application for the Berlekamp-Massey algorithm (see Sect. 2.4.2). Both
points justify a section devoted just to Wiedmann’s algorithm.
The main idea is the following. Let A ∈F n×n be a non-singular matrix and let
b ∈F n. Consider the sequence ai = Aib. This is a linear recurrence sequence. In
contrast to the linear recurrence sequence we have studied in Chap. 2, it is not de-
ﬁned over a ﬁeld but over the vector space F n.
The sequence ai still has a minimal polynomial m which is the unique monic
polynomial m = d
i=0 mixi of least degree for which
d

i=0
miAib = 0
(12.13)
holds. m is a divisor of the minimal polynomial of A which is in turn a divisor of
the characteristic polynomial of A. In particular, the degree d is bounded by n. We
will see later how to compute m.
Winograd’s idea is to simply multiply Eq. (12.13) by A−1 to obtain
A−1b = −m−1
0
d

i=1
miAi−1b.
(12.14)
Equation (12.14) can be efﬁciently evaluated in a Horner-like fashion. The inter-
esting point is that Winograd’s algorithm uses a matrix only to evaluate it at vectors.
Thus instead of storing the matrix as an n × n table, we just need a black box which
returns for an input v ∈F n the result Av ∈F n. We call this way of doing linear

292
12
Computational Aspects
algebra black box linear algebra to distinguish it from the traditional explicated
linear algebra in which we manipulate the entries of A explicitly. A special case
is sparse linear algebra in which a matrix is stored as a list of triples (i,j,mi,j)
with mi,j ̸= 0. Winograd ﬁrst proposed his algorithm in the context of sparse linear
algebra in which the evaluation Av is very fast.
We still have to explain how to compute the minimal polynomial m of the se-
quence ai = Aib. The problem is that for sequences over vector spaces there is no
analog of the Berlekamp-Massey algorithm. The idea is to choose a random u and
compute the minimal polynomial bu of the linear recurrence sequence (utAib)i∈N
with the normal Berlekamp-Massey algorithm. mu is a divisor of the minimal poly-
nomial m and if the ﬁeld F has enough elements there is a high probability that
mu = m. Thus we have an effective probabilistic algorithm to compute m. Again
the algorithm needs only a black box to evaluate A at some vectors v, so we stay in
the framework of black box linear algebra.
We do not go into the details. See Sect. 12.4 of [100] or [280] for an analysis of
the probability that mu = m.

Chapter 13
Number Theory
The Blum-Blum-Shub generator (Chap. 11) uses a lot of number theory. This chap-
ter collects together all the results from number theory that we need.
13.1 Basic Results
We write a ≡b mod n if and only if n|(a −b). For each congruence class b + nZ
there exists a unique representative a ∈{0,...,n−1}. We denote this representative
by a = b mod n (note the different use of = and ≡). Most programming languages
provide a built in operator for a = b mod n (in C-style syntax a = b%n).
By τ(n) we denote the number of divisors of n. If n = pe1
1 ···pek
k then τ(n) =
(e1 + 1)···(ek + 1).
Recall the Chinese Remainder Theorem:
Theorem 13.1 (Chinese Remainder Theorem) Let n1, ..., nk be pairwise relatively
prime. Then the system
x ≡a1
mod n1
...
x ≡ak
mod nk
has a unique solution x ≡a mod n with n = n1 ···nk.
With the extended Euclidean algorithm the solution a can be effectively com-
puted.
A. Klein, Stream Ciphers, DOI 10.1007/978-1-4471-5079-4_13,
© Springer-Verlag London 2013
293

294
13
Number Theory
13.2 The Group (Z/nZ)×
By the Chinese remainder theorem we get for n = pe1
1 ···pek
k that (Z/nZ)× is iso-
morphic to the direct product (Z/pe1
1 Z)× × ··· × (Z/pek
k Z)×.
By ϕ(n) we denote the number of elements in {1,...,n −1} relatively prime
to n. ϕ is called the Eulerian function. If n = pe1
1 ···pek
k then
ϕ(n) = pe1−1
1
(p1 −1)···pek−1
k
(pk −1).
(13.1)
The order of the group (Z/nZ)× is ϕ(n) and hence we obtain:
Lemma 13.2 If a is relatively prime to n then
aϕ(n) ≡1
mod n.
Theorem 13.3 If n = pe1
1 ···pek
k then
ϕ(n) = pe1−1
1
(p1 −1)···pek−1
k
(pk −1).
By the Carmichael function λ(n) we denote the smallest integer m such that
am = 1 for all a in the group (Z/nZ)×. More explicitly we have
λ

pe
=

pe−1(p −1)
if p ≥3 or e ≤2,
2e−2
if p = 2 and e ≥3
(13.2)
for a prime power pe and for n = pe1
1 ···pek
k :
λ(n) = lcm

λ

pe1
1

,...,λ

pek
k

.
To prove the result on the period of the power generator we need the following
estimate of the number of elements in Z/nZ of small multiplicative order.
Lemma 13.4 (Friedlander, Pomerance, Shparlinski [97]) Let n be a positive integer
and let j be a divisor of λ(n). The number Nj(n) of elements in Z/nZ with order
dividing λ(n)/j is bounded by
Nj(n) ≤ϕ(n)
j
.
Proof Let n = pe1
1 ···pek
k and let λ(n) = qd1
1 ···qdl
l . If ordnx| λ(n)
j
then for all i =
1,...,k:
ordpei
i x

λ(pei
i )
ji
where ji contains all prime factors qg of j for which qdg
g |λ(pei
i ).

13.3
The Prime Number Theorem and Its Consequences
295
The number of elements of Z/pei
i Z with ordpei
i x| λ(pei
i )
ji
is at most ϕ(pei
i )/ji. The
bound is sharp except for the case pi = 2, ei ≥3, in which case it can be lowered
by a factor of 1/2.
Using the Chinese remainder theorem to combine the results modulo prime pow-
ers we obtain
Nj ≤
k
i=1
ϕ(pei
i )
ji
=
ϕ(n)
j1 ···jk
≤ϕ(n)
j
.
□
Let τ(n) be the number of divisors of n.
Summing over all j with j ≥K (at most τ(λ(n)) summands less than ϕ(n)
K )
we get that the number Sk(n) of elements in Z/nZ with order at most λ(n)/K is
bounded by
Sk(n) ≤ϕ(n)τ(λ(n))
K
.
(13.3)
13.3 The Prime Number Theorem and Its Consequences
To estimate how fast we can generate random primes for cryptographic applications,
we need to know the asymptotic density of the primes.
Deﬁnition 13.1 For x > 0 let
π(x) =

p≤x
1
and
ϑ(x) =

p≤x
lnp
where the sums run over all primes less than or equal to x.
A famous result of number theory is:
Theorem 13.5
π(x) ∼x
lnx
(13.4)
and
ϑ(x) = Θ(x).
(13.5)
Proof See, for example, [122] Chap. 22. Equation (13.4) is known as the prime
number theorem and Eq. (13.5) is an important step in the classical proof of the
prime number theorem.
□
Other results in this direction are:

296
13
Number Theory
Theorem 13.6

p≤n
1
p = lnlnn + B1 + o(1)
(13.6)
and

p≤n

1 −1
p

∼eσ
lnn
(13.7)
for some constant σ.
Proof See, for example, [122] Theorem 427 and Theorem 429. Equation (13.7) is
known as Mertens’ theorem.
□
Equation (13.4) tells us that a random integer of size approximately en is prime
with probability 1/n. Choosing random integers and testing them for primality is
therefore fast enough for practical purposes. In most cryptographic applications n ≈
550 (about 800 bits) is secure enough.
Next we will use Eq. (13.5) to estimate the size of the Eulerian function.
Theorem 13.7
n
ϕ(n) = O

lnln(n)

.
Proof By Eq. (13.1) we have
ϕ(n) = n

p|n

1 −1
p

where the product runs over all prime divisors of n.
Let ν(n) denote the number of prime divisors of n. The factor (1 −1
p) is smaller
if p is smaller, hence
ϕ(n) ≥n
ν(n)

i=1

1 −1
pi

where the product runs over the ν(n) smallest primes. By Eq. (13.7) we have
ϕ(n) ≥nc
1
lnpν(n)
(13.8)
for some constant c.
On the other hand
n ≥

p|n
p ≥
ν(n)

i=1
pi =

p≤pν(n)
p

13.4
Zsigmondy’s Theorem
297
and hence
lnn ≥

p≤pν(n)
lnp = ϑ(pν(n)) ≥c′pν(n)
(13.9)
by Eq. (13.5).
By combining Eqs. (13.8) and (13.9) we get
ϕ(n) ≥
c
lnc′
n
lnlnn
which is precisely the statement of the theorem.
□
Theorem 13.7 has some applications in cryptology. It tells us, for example, that
we can efﬁciently generate random primitive polynomials (see Sect. 14.3).
13.4 Zsigmondy’s Theorem
Zsigmondy’s theorem is useful in the theory of ﬁnite linear groups.
Theorem 13.8 (see [290]) Let a,b,n ∈Z with gcd(a,b) = 1, |a| > 1 and n ≥1.
Then an −bn has a prime divisor p that does not divide am −bm for any 0 < m < n,
except in the following cases:
(a) n = 1 and a −b = 1.
(b) n = 2 and a + b = ±2μ.
(c) n = 3 and {a,b} = {2,−1} or {a,b} = {−2,1}.
(d) n = 6 and {a,b} = {2,1} or {a,b} = {−2,−1}.
We will not need Theorem 13.8 in its full generality, so we will prove it only for
a > 1 and b = 1. The following simple proof is due to Lüneburg [173].
Like all proofs of Zsigmondy’s theorem, Lüneburg’s proof is based on cyclo-
tomic polynomials. For n ≥1 the cyclotomic polynomial Φn is deﬁned as
Φn(x) =
ϕ(n)

i=1
(x −ζi),
where ζ1,...,ζϕ(n) are the primitive roots of unity of order n. We note without proof
that Φn ∈Z[x] (see, for example, [140]).
Lemma 13.9 Let a > 1 and n > 1 be integers. Let p be a prime dividing Φn(a) and
let f be the order of a modulo p. Then n = fpi for some non-negative integer i. If
i > 0 then p is the largest prime dividing n. If in addition p2|Φn(a) then n = p = 2.

298
13
Number Theory
Proof Since Φn(a) divides an −1, so does p. Hence f |n. Let n = fpiw with
gcd(w,p) = 1. Assume that w ̸= 1, let r = fpi. Then p|ar −1 and
an −1
ar −1 = ((ar −1) + 1)w −1
ar −1
= w +
w

j=2
w
j

ar −1
j−1 ≡w ̸≡0
mod p.
But for each proper divisor r of n the number Φn(a) (and hence p) is a divisor of
(an −1)/(ar −1). Thus w = 1, i.e. n = fpi.
Now let i > 0. Since f is a divisor of p −1 (Fermat’s little theorem) p must be
the largest prime divisor of n.
Now assume i > 0 and p2|Φn(a). For p > 2 we have
an −1
an/p −1 = ((an/p −1) + 1)p −1
an/p −1
= p +
p
2

an/p −1

+
p

j=3
p
j

an/p −1
j−1
≡p
mod p2
and hence p2 ∤Φn(a).
If p = 2, then n = 2i since p is the largest prime divisor of n. But then Φn(a) =
(an −1)/(an/2 −1) = a2i−1 + 1. For i > 2 we have a2i−1 + 1 ≡2 mod 4, i.e.
p2 = 4 ∤Φn(a). Thus n = 2, as stated in the lemma.
□
Now we can prove the special case of Zsigmondy’s theorem.
Proof of Theorem 13.8 for a > 1 and b = 1 We ﬁrst check that the cases mentioned
in Theorem 13.8 are indeed exceptions.
Suppose now that there is no prime p such that the order of a modulo p is n.
If n = 2, Lemma 13.9 implies Φn(a) = a + 1 = 2s for some integer s, which is
condition (b) of Theorem 13.8.
From now on assume n > 2. By Lemma 13.9 Φn(a) = p, where p is the largest
prime factor of n.
If p = 2 then n = 2i and Φn(a) = a2i−1 + 1 > 2, a contradiction.
Now let p > 2. Then n = pir where r divides p −1. Let ζ1,...,ζφ(r) be the
primitive roots of unity of order r, then
p = Φn(a)
= Φr(api)
Φr(api−1)
=
φ(r)
j=1(cp −ζj)
φ(r)
j=1(c −ζj)
with c = api−1

13.5
Quadratic Residues
299
>
bp −1
b + 1
φ(r)
≥cp−2.
Hence p = 3, c = a = 2 and i = 1. Thus either n = 3 or n = 6. For n = 3 and a = 2
we have 7|an −1 but 7 ∤ak −1 for k < n. Thus n = 6 and a = 2, which is case (d)
in Theorem 13.8, is the only exception.
□
13.5 Quadratic Residues
An integer a relative prime to n is said to be a quadratic residue modulo n if x2 ≡a
mod n has a solution. We denote the set of all quadratic residues by QR(n). The
quadratic residue problem is to decide whether a given number lies in QR(n).
Deﬁnition 13.2 (Legendre symbol) Let p be an odd prime. The Legendre symbol
 a
p

is deﬁned by:
 a
p

=
⎧
⎪⎨
⎪⎩
0
if a ≡0
mod p
1
if a is a quadratic residue modulo p
−1
otherwise.
The Legendre symbol can be efﬁcient evaluated.
Lemma 13.10 (Euler’s criterion) Let p be an odd prime then
 a
p

≡a(p−1)/2
mod p.
Proof By Fermat’s little theorem ap−1 ≡1 mod p if a ̸≡0 mod p.
Hence if a ≡x2 mod p then a(p−1)/2 ≡xp−1 ≡1 mod p. Since the equation
a(p−1)/2 ≡1 mod p has at most (p −1)/2 solutions and we have proved that the
(p −1)/2 quadratic residues are solutions of a(p−1)/2 ≡1 mod p, we know that
the quadratic non-residues must satisfy a(p−1)/2 ≡−1 mod p.
□
One simple consequence of Euler’s criterion is:
Corollary 13.11
ab
p

=
 a
p
 b
p

.

300
13
Number Theory
Proof By Euler’s criterion
ab
p

≡(ab)(p−1)/2
mod p
≡a(p−1)/2b(p−1)/2
mod p
≡
 a
p
 b
p

.
□
Lemma 13.10 already describes a way to solve the quadratic residue problem
modulo primes, but one can do better:
Theorem 13.12 (quadratic reciprocity law) If p and q are odd primes then
 p
q

=
−
 q
p

if p ≡q ≡3 mod 4 and
 p
q

=
 q
p

otherwise.
In addition,
 2
p

= 1 if and only if p ≡±1 mod 8 and
 −1
p

= 1 if and only if
p ≡1 mod 4.
Proof The proof of this classical theorem can be found in any number theory book,
see for example [122] Theorems 99, 95 and 82.
□
Theorem 13.12 allows a Euclidean algorithm style computation of the Legendre
symbol.
We generalize the Legendre symbol by:
Deﬁnition 13.3 (Jacobi symbol)
Let b ≥3 be odd and let b = pe1
1 ···pek
k be the
factorization of b. The Jacobi symbol
 a
b

is deﬁned by:
a
b

=
 a
p1
e1
···
 a
pk
ek
.
Algorithm 13.1 computes the Jacobi symbol in an efﬁcient way (see also Exer-
cise 17.25).
Next we study the quadratic residue problem modulo a composite number. We
deal only with the case of distinct prime factors which is relevant for applications in
cryptography (for prime powers, see [122] Sect. 8.3).
Theorem 13.13 Let n = p1 ···pk be the product of k distinct odd primes.
If gcd(a,n) = 1, then the equation x2 ≡a mod n has either 0 or 2k solutions.
Proof Assume that x2 ≡a mod n has a solution. As gcd(a,n) = 1, for each prime
factor pi the congruence x2 ≡a mod pi has exactly two solutions x(i)
1/2. By the
Chinese remainder theorem the system of linear congruences x ≡x(i)
ji
mod pi has
exactly one solution for every j ∈1,2k. Each such x is a solution of x2 ≡a mod n
and on the other hand each solution of x2 ≡a mod n must be of this form.
□

13.6
Lattice Reduction
301
Algorithm 13.1 Evaluating the Jacobi symbol
Require: a, b are integers, a < b, b odd
Ensure: return
 a
b

1: if a = 1 then
2:
return 1
3: end if
4: if a even then
5:
Write a = 2ka′ with a′ odd.
6:
if k odd and b ≡±3 mod 8 then
7:
return −
 a′
b

{recursive call}
8:
else
9:
return
 a′
b

{recursive call}
10:
end if
11: else
12:
b′ = b mod a
13:
if b′ = 0 then
14:
return 0
15:
end if
16:
if a ≡b ≡3 mod 4 then
17:
return −
 b′
a

{recursive call}
18:
else
19:
return
 b′
a

{recursive call}
20:
end if
21: end if
Let n = pq be the product of two distinct primes. The Jacobi symbol gives at
least partial information as to whether a number a is a quadratic residue. If
 a
n

= −1
then either a is a quadratic non-residue modulo p or modulo q, and in both cases a
cannot be a quadratic residue modulo n.
If
 a
n

= 1 there are two cases: either
 a
p

=
 a
q

= 1 and hence a is a quadratic
residue modulo n or
 a
p

=
 a
q

= −1 and a is a quadratic non-residue modulo n.
Both cases are, with (p−1)(q−1)
4
residues, equally likely.
If the factorization n = pq is known it is easy to decide between the cases. At the
moment no other method than factoring is known to decide whether or not a with
 a
n

= 1 is a quadratic residue. Every method which does this would also determine
 a
p

and
 a
q

and hence leak at least some information about the prime factors.
13.6 Lattice Reduction
Deﬁnition 13.4 Let b1, ..., bk be a set of linearly independent vectors in Rn. Then
L = Zb1 + ··· + Zbk is a k-dimensional lattice in Rn. The vectors b1, ..., bk form
a basis of the lattice L.

302
13
Number Theory
In many applications we are interested in a basis consisting of short vectors. The
Lenstra, Lenstra, Lovász basis reduction (LLL-reduction, for short) [168] gives us
a (partial) solution of the short vector problem. It essentially transfers the Gram-
Schmidt orthogonalization to lattices.
Let us recall the Gram-Schmidt orthogonalization algorithm.
Deﬁnition 13.5 Let (·,·) be a bilinear form of Rn and f1,...,fn be a basis of Rn.
The Gram-Schmidt orthogonal basis f ∗
1 ,...,f ∗
n consists of the vectors
f ∗
i = fi −
i−1

j=1
μi,jf ∗
j
where μi,j = (fi,fj )
(f ∗
j ,f ∗
j ).
The core idea of LLL-reduction is to calculate the Gram-Schmidt orthogonaliza-
tion and round the coefﬁcients μi,j to stay inside the lattice (see Algorithm 13.2).
Algorithm 13.2 LLL basis reduction
Require: f1,...,fn is the basis of a lattice L ⊂Rn
1: i ←2
2: while i < n do
3:
Compute the Gram Schmidt orthogonalization {Actually this is just an update
step which can use data from the previous computation}
4:
fi = fi −k
j=1⌊μi,j⌉fj
5:
if ∥f ∗
i−1∥2 > 2∥f ∗
i ∥2 then
6:
Swap fi−1 and fi
7:
i ←i −1 unless i = 2
8:
else
9:
i ←i + 1
10:
end if
11: end while
Theorem 13.14 Algorithm 13.2 uses O(n4 logn) arithmetic operations. It com-
putes a basis f1,...,fn of the lattice L with the following properties:
(a) ∥f ∗
i ∥2 ≤2∥f ∗
i−1∥2 for 2 ≤i ≤n.
(b) ∥f1∥≤2(n−1)/2∥f ∥for each f ∈L\{0}.
Proof The proof of the LLL-basis reduction can be found in most computer algebra
textbooks. See, for example, Chap. 16 of [100].
□
LLL-reduction has several applications in cryptography. In the context of the
Blum-Blum-Shub generator, Coppersmith’s method for ﬁnding small roots of mod-
ular equations is of special interest. The problem is: Given a polynomial f and an

13.6
Lattice Reduction
303
integer of unknown factorization, ﬁnd all x with f (x) ≡0 mod and |x| ≤x0 for
a bound x0 as large as possible. Coppersmith published in 1997 a nice LLL-based
method (Algorithm 13.3) for solving this problem (see [60, 131]). His method is
relevant to the bit security of RSA-like ciphers.
Algorithm 13.3 Coppersmith’s method (univariate case)
(1) Given a polynomial f of degree d and a modulus n of unknown factorization
which is a multiple of b.
(2) Choose δ with b ≥Nβ and ε ≤β. Let m = ⌈β2
dε ⌉and t = ⌊δm( 1
β −1)⌋.
(3) Compute the polynomials
gi,j(x) = xjNif m−i(x)
for i = 0,...,m −1, j = 0,...,d −1
hi(x) = xif m(x)
for i = 0,...,t −1
(4) Let X = ⌈Nβ2/d−ε⌉. Construct a lattice with basis consisting of the coefﬁcients
of gi,j(Xx) and hi(xX).
(5) Find a short vector in this lattice by using the LLL method. You have found
a polynomial ˆf which is a linear combination of the polynomial gi,j(Xx) and
hi(xX). Hence for f (x0) ≡0 mod n implies ˆf (x0) ≡0 mod b.
(6) As the norm of ˆf is small, x0 ≤X and ˆf (x0) ≡0 mod nm implies ˆf (x0) = 0.
Compute all integer solutions of ˆf using standard methods. For every root x0
check if f (x0) ≡0 mod b.
(7) You have now found all solutions of f (x) ≡0 mod b with |x| ≤X.
We do not go deeper into the mathematics of Coppersmith’s method. Instead we
mention without proof:
Theorem 13.15 Algorithm 13.3 works correctly and needs at most O(ε−7δ5 log2 N)
arithmetic operations.
Proof See [60] or [131].
□
Note that the essential point of Coppersmith’s method is that it works for inte-
gers with unknown factorization. If the factorization of n is known, there are better
methods for solving f (x) ≡0 mod n which use the Chinese remainder theorem
and Hensel lifting.

Chapter 14
Finite Fields
Finite ﬁelds are the basis for the mathematics of shift registers. We will especially
need the constructive parts of the theory of ﬁnite ﬁelds. In the next subsections we
summarize the most important facts. For further references see, for example, [140].
14.1 Basic Properties
• The characteristic of a (ﬁnite) ﬁeld is a prime.
• For every prime power q there exists up to isomorphism only one ﬁnite ﬁeld. We
denote the ﬁnite ﬁeld of size q by Fq.
• The multiplicative group F×
q of Fq is cyclic.
• Fpe is a subﬁeld of Fpd if and only if e|d.
• The Galois group GL(Fqe/Fq) is cyclic and is generated by the Frobenius auto-
morphism x →xq.
• An element of Fq is primitive if it generates the multiplicative group. Fq contains
ϕ(q −1) primitive elements.
14.2 Irreducible Polynomials
Deﬁnition 14.1 A polynomial is called monic if its leading coefﬁcient is 1.
We want to count the number of irreducible monic polynomials of degree n
over Fq.
Let us recall the Möbius inversion. Let A and B be functions from N+ into an
additive group G, then
B(n) =

d|n
A(d)
A. Klein, Stream Ciphers, DOI 10.1007/978-1-4471-5079-4_14,
© Springer-Verlag London 2013
305

306
14
Finite Fields
if and only if
A(n) =

d|b
μ(d)B
n
d

where μ is the Möbius function deﬁned by
μ(n) =
⎧
⎪⎨
⎪⎩
1
if n = 1,
(−1)k
if n is the product of k distinct primes,
0
otherwise.
Theorem 14.1 The number of irreducible monic polynomials of degree n over Fq
is
1
n

d|n
μ(d)qn/d = qn
n + O

qn/2/n

.
Proof Let A be the number of elements ζ in Fqn with Fq(ζ) = Fq and let B(n) =
|Fqn|.
Then B(n) = qn = 
d|n A(d), since every element of Fqn generates a subﬁeld
Fqd of Fqn, with d|n.
By the Möbius inversion we have A(n) = 
d|n μ(d)qn/d. Since every irre-
ducible monic polynomial of degree n has n roots, A(n)/n is the number of ir-
reducible monic polynomials of degree n.
The bounds B(n) = qn ≥A(n) = 
d|n μ(d)qn/d ≥qn −n/2
j=1 qj give A(n) =
qn + O(qn/2).
□
Since the number of monic polynomials of degree n over Fq is qn, the probability
that a random monic polynomial of degree n is irreducible is about 1/n, which is
quite high. So repeatedly testing monic polynomials until we ﬁnd an irreducible
one is an attractive probabilistic algorithm. To make it effective we need a criterion
which decides if a given polynomial is irreducible. Theorem 14.2 is such a criterion.
Theorem 14.2 Let f ∈Fq[x] be a monic polynomial of degree n. Then f is irre-
ducible if and only if gcd(xqd −x,f ) = 1 for all proper divisors d of n.
Proof Suppose f is not irreducible, then f has a zero ζ which lies in Fqd for some
proper divisor of n.
Since the multiplicative group of Fqd is cyclic, ζ qd −ζ = 0 and hence gcd(xqd −
x,f ) ̸= 1.
On the other hand, gcd(xqd −x,f ) ̸= 1 implies that f has a zero ζ which satisﬁes
ζ qd −ζ = 0. Hence ζ ∈Fqd , which is the splitting ﬁeld of xqd −x over Fq.
□

14.3
Primitive Polynomials
307
For a given polynomial f we can compute xqd mod f in only O(logqd) steps
by using the square-and-multiply algorithm, so Theorem 14.2 is indeed an efﬁcient
test for irreducibility.
14.3 Primitive Polynomials
We call a polynomial f ∈Fq[x] of degree n primitive if its roots have order qn −1
(see Deﬁnition 2.5). Since the multiplicative group of Fqn is cyclic, the number
of primitive elements in Fqn is φ(qn −1) and hence Fq[x] contains φ(qn −1)/n
primitive polynomials of degree n.
An obvious way to generate a random primitive element (and, with that element,
also a random primitive polynomial) is Algorithm 14.1.
Algorithm 14.1 Choosing a random primitive element of Fq
1: Compute the factorization q −1 = pe1
1 ...pess .
2: repeat
3:
Choose c random in F×
q .
4: until c(q−1)/pi ̸= 1 for i ∈{1,...,s}
The problematic part of Algorithm 14.1 is the factorization of q −1, but there
are specialized algorithms for factoring numbers of the form pk −1, so we can run
it even for large q. At the time of writing 2827 −1 is the smallest Mersenne number
which is not completely factored and 21061 −1 is the smallest Mersenne number
which is not a prime, but no factor is known.
Another inﬂuence on the performance of Algorithm 14.1 is the probability that a
random element is primitive.
Note that it possible to ﬁnd a sequence of prime numbers pk such that the chance
that a random element of F×
pk is primitive approaches 0 for k →∞. One simply
chooses pk ≡1 mod k!, which is possible due to Dirichlet’s theorem. Then
φ(pk −1)
pk −1
≤

p≤k
pprime

1 −1
p

→0
for k →∞.
However, for all practical sizes of q the probability that a random element is
primitive is quite high. We recall the asymptotic behavior of the Eulerian ϕ-function
(see Theorem 13.7)
m
ϕ(m) = O(lnln(m)).
Thus the chance that a random element of F×
q is primitive is Ω(1/lnlnq), which
is for all reasonable sizes of q large enough.
At the moment no algorithm is known that ﬁnds a primitive element in polyno-
mial time (see [50] for an overview of what has been done in that direction).

308
14
Finite Fields
14.4 Trinomials
Linear feedback shift registers with sparse feedback polynomials can be imple-
mented very efﬁciently (see Algorithm 2.10). So there is special interest in irre-
ducible and primitive polynomials of low weight. In this section we collect some
facts about trinomials.
Theorem 14.3 The trinomial xn + xk + 1 is irreducible over F2 if and only if xn +
xn−k + 1 is irreducible over F2.
Proof If f (x) = xn + xk + 1 then g(x) = xn + xn−k + 1 = xnf (1/x). So every
factorization of f gives a factorization of g.
□
Richard G. Swan [261] proved a very strong result on the number of irreducible
factors of a trinomial.
Theorem 14.4 Let f be a monic polynomial with integral coefﬁcients over Qp.
Assume that the reduction ¯f of f over Fp has no multiple roots. Let r be the number
of irreducible factors of ¯f . Then r ≡n mod 2 if and only if the discriminant D(f )
is a square in Qp.
In particular, for trinomials over F2 we get the following. Let n > k > 0 and let
exactly one of n and k be odd. Then xn + xk + 1 ∈F2[x] has an even number of
irreducible factors if and only if one of the following cases hold:
(a) n is even, k is odd and n ̸= 2k and nk/2 ≡0 or 1 mod 4.
(b) n ≡±3 mod 8, k is even, k ∤2n.
(c) n ≡±1 mod 8, k is even, k|2n.
Proof See [261] Theorem 1 and Corollary 5.
□
Two corollaries of Swan’s Theorem are:
Corollary 14.5 If n is a multiple of 8, then every trinomial of degree n is reducible
over F2.
Proof First note that for even n and k, xn + xk + 1 = (xn/2 + xk/2 + 1)2 is not
irreducible. If n is a multiple of 8 and k is odd then 2k ̸= n and nk/2 ≡0 mod 4,
i.e. by Theorem 14.4 xn + xk + 1 has an even number of factors. In particular, it is
reducible.
□
Corollary 14.6 If n ≡3 mod 8 or n ≡5 mod 8 and xn + x2d + 1 ∈F2[x] is irre-
ducible, then d is divisor of n.
Proof By Theorem 14.4 xn +x2d +1 contains an even number of irreducible factors
if d ∤n.
□

14.5
The Algebraic Normal Form
309
There are also very efﬁcient algorithms for testing a trinomial for irreducibility
(see [29, 30]). This allows us to generate irreducible trinomials of high degree.
We close this section with a table of primitive and irreducible polynomials of low
weight (Table 14.1).
14.5 The Algebraic Normal Form
Consider the polynomial ring R = F[x1,...,xn]. With a polynomial p ∈R we as-
sociate the polynomial mapping x →p(x). For inﬁnite ﬁelds this is a one-to-one
correspondence and there exist functions which are not polynomial. For ﬁnite ﬁelds
the situation is different:
Theorem 14.7 For any function f : Fn
q →Fq there exists a unique polynomial p
with degi(p) < q for 1 ≤i ≤n and f (x) = p(x) for all x ∈Fn
q.
Proof Let f : Fn
q →Fq. By Lagrange interpolation we have
f (x) =

α∈Fnq
f (α)
n

j=1
(xq
i −xi)
xi −αi
.
Hence f can be written as a polynomial p with degi(p) < q for 1 ≤i ≤n.
To prove the uniqueness of p, note that there are qqn functions from f : Fn
q →Fq
and the same number of polynomials with degi(p) < q for 1 ≤i ≤n.
□
Deﬁnition 14.2 The polynomial p of Theorem 14.7 is called the algebraic normal
form of f .
The algebraic normal form allows us to speak of the algebraic degree of a func-
tion over ﬁnite ﬁelds.

310
14
Finite Fields
Table 14.1 Primitive and irreducible polynomials over F2 of low weight
Degree
Primitive polynomial
Irreducible polynomial
2
x2 + x + 1
x2 + x + 1
3
x3 + x + 1
x3 + x + 1
4
x4 + x + 1
x4 + x + 1
5
x5 + x2 + 1
x5 + x2 + 1
6
x6 + x + 1
x6 + x + 1
7
x7 + x + 1
x7 + x + 1
8
x8 + x4 + x3 + x2 + 1
x8 + x4 + x3 + x2 + 1
9
x9 + x4 + 1
x9 + x4 + 1
10
x10 + x3 + 1
x10 + x3 + 1
11
x11 + x2 + 1
x10 + x3 + 1
12
x12 + x6 + x4 + x + 1
x12 + x3 + 1
13
x13 + x4 + x3 + x + 1
x13 + x4 + x3 + x + 1
14
x14 + x5 + x3 + x + 1
x14 + x5 + 1
15
x15 + x + 1
x15 + x + 1
16
x16 + x5 + x3 + x2 + 1
x16 + x5 + x3 + x2 + 1
17
x17 + x3 + 1
x17 + x3 + 1
18
x18 + x7 + 1
x18 + x7 + 1
19
x19 + x5 + x2 + x + 1
x19 + x5 + x2 + x + 1
20
x20 + x3 + 1
x20 + x3 + 1
21
x21 + x2 + 1
x21 + x2 + 1
22
x22 + x + 1
x22 + x + 1
23
x23 + x5 + 1
x23 + x5 + 1
24
x24 + x4 + x3 + x + 1
x24 + x4 + x3 + x + 1
25
x25 + x3 + 1
x25 + x3 + 1
26
x26 + x6 + x2 + x + 1
x26 + x6 + x2 + x + 1
27
x27 + x5 + x2 + x + 1
x27 + x5 + x2 + x + 1
28
x28 + x3 + 1
x28 + x3 + 1
29
x29 + x2 + 1
x29 + x2 + 1
30
x30 + x6 + x4 + x + 1
x30 + x + 1
31
x31 + x3 + 1
x31 + x3 + 1
32
x32 + x7 + x6 + x2 + 1
x32 + x7 + x6 + x2 + 1
33
x33 + x13 + 1
x33 + x13 + 1
34
x34 + x8 + x4 + x3 + 1
x34 + x7 + 1
35
x35 + x2 + 1
x35 + x2 + 1

Chapter 15
Statistics
15.1 Measure Theory
Although most probabilities we meet in cryptography are simple discrete proba-
bilities, it is convenient to describe the statistical background in the more general
measure theoretic context. In this section we collect all the measure theoretic no-
tions we need.
For a ﬁnite set Ω we can assign to every subset of Ω a probability, but for inﬁnite
sets this is no longer possible. So we assign a probability measure only to “nice”
sets. The right deﬁnition of a nice set is given in the following.
Deﬁnition 15.1 F ⊆P(Ω) is called σ-algebra if:
(a) ∅∈F.
(b) If A ∈F then ¯A = Ω\A ∈F.
(c) If Ai ∈F for i ∈N then /∞
i=0 Ai ∈F.
The pair (Ω,F) is called a measurable space.
Deﬁnition 15.2 Let (Ω,F) be a measurable space, then μ : F →R≥0 ∪{∞} is a
measure if:
(a) μ(∅) = 0.
(a) If Ai ∈F for i ∈N with Ai ∩Aj = ∅for i ̸= j then μ(/∞
i=0 Ai) = ∞
j=0 μ(Ai).
μ is a probability measure if μ(Ω) = 1.
From analysis we know the special case of the Borel σ-algebra B on R, which is
the smallest σ-algebra that contains all intervals. Once we have a measurable space
and a measure, we can proceed as in analysis and deﬁne the measure integral, which
we will denote by

A f (ω)dμ. The famous Lebesgue integral is just the special case
with (R,B) as measurable space and the measure μ with μ([a,b]) = b −a.
Let (Ω,F) and (Ω′,F′) be two measurable spaces. Then a function f : Ω →Ω′
is called measurable (with respect to F and F′) if f −1(F ′) ∈F for all F ′ ∈F′.
A. Klein, Stream Ciphers, DOI 10.1007/978-1-4471-5079-4_15,
© Springer-Verlag London 2013
311

312
15
Statistics
If f : Ω →R+ is measurable then μ′(A) =

A f (ω)dμ deﬁnes a new measure
μ′. We say μ′ has μ-density f .
15.2 Simple Tests
15.2.1 The Variation Distance
In cryptography we often observe a random variable X which is either distributed
by P or by Q and we must decide which of the two possible cases hold.
Deﬁnition 15.3 Let P and Q be two probability distributions on a common mea-
surable space (Ω,F). A distinguisher is a measurable function D : Ω →{0,1}.
The probability of success is |P(D = 1) −Q(D = 1)|.
Of course we are interested in the best possible distinguisher. This leads us to the
deﬁnition of the variation distance.
Deﬁnition 15.4 Let P and Q be two probability measures on (Ω,F). The variation
distance between P and Q is
∥P −Q∥= sup
F∈F
P(F) −Q(F)
.
The variation distance is a useful and common measure for the distance between
two probability distributions. Theorem 15.1 shows how to compute the variation
distance for probability measures with densities.
Theorem 15.1 Let μ be a measure on (Ω,F). If P has μ-density φ1 and P has
μ-density φ2, then
∥P −Q∥= 1
2
# φ1(ω) −φ2(ω)
dμ.
In particular, if Ω = {ω1,...,ωn} is a ﬁnite set, then
∥P −Q∥= 1
2
n

j=1
P(ωi) −Q(ωi)
.
Proof Let F = {ω | φ1(ω) ≥φ2(ω)}, then
P(F) −Q(F)
 =
#
F
φ1(ω) −φ2(ω)dμ =
#
F
φ1(ω) −φ2(ω)
dμ

15.2
Simple Tests
313
and similarly
P(F) −Q(F)
 =

1 −
#
¯F
φ1(ω)dμ

−

1 −
#
¯F
φ2(ω)dμ

=
#
¯F
φ1(ω) −φ2(ω)
dμ,
hence
P(F) −Q(F)
 = 1
2
# φ1(ω) −φ2(ω)
dμ.
Now let F ′ ̸= F . Let F1 = F\F ′ and F2 = F ′\F , then
P

F ′
−Q

F ′
=

P(F) −Q(F)

−

P(F1) −Q(F1)

+

Q(F2) −F(F2)

≤P(F) −Q(F),
i.e. |P(F) −Q(F)| = supF ′∈F |P(F ′) −Q(F ′)|.
□
15.2.2 The Test Problem
In this section we will give a short introduction to statistical tests. The basic setting
in statistics is that we have a measurable space (Ω,F) and a parameter space Θ.
For each parameter θ ∈Θ we have probability measures Pθ on (Ω,F). A partition
of Θ into two sets H0 and H1 deﬁnes a test problem. H0 is called the hypothesis
and H1 is called the alternative.
Every measurable function φ : Ω →[0,1] is a test. The value φ(x) is the proba-
bility that the test rejects the hypothesis H0 given the observation x. If φ takes only
the values 0 and 1 it is a deterministic test. Otherwise we speak of a randomized
test. We do not use randomized tests in this book, but the theory does not become
simpler if we restrict ourselves to deterministic tests.
The quality of a test is measured by:
Deﬁnition 15.5 Let (Ω,F,(Pθ)θ∈Θ,H0,H1) be a test problem and φ be a test.
The function βφ : Θ →[0,1] with
βφ(θ) =
#
φxdPθ
is the quality function of φ (given the test problem (Ω,F,(Pθ)θ∈Θ,H0,H1)).
For a test φ we call supθ∈H0 βφ(θ) the error probability of the ﬁrst kind. φ is a
test at level α if the error probability of the ﬁrst kind is at most α. The set of all level
α tests is denoted by Φα.

314
15
Statistics
For θ ∈H1 we call 1 −βφ(θ) the error probability of the second kind of φ at
point θ.
15.2.3 Optimal Tests
Once we have a notion of a general statistical test, it is natural to ask how we can
ﬁnd good tests.
Deﬁnition 15.6 For a set of tests Φ the test φ ∈Φ is most powerful if for every
φ′ ∈Φ and every θ ∈H1 the inequality
βφ(θ) ≥βφ′(θ)
holds.
Note that it is not guaranteed that a most powerful test exists if H1 contains more
than one element. We are interested in a most powerful level α test.
For the cases that H0 and H1 are simple, i.e. if both contain only one element,
the Neyman-Pearson Lemma characterizes these tests.
Theorem 15.2 (Neyman-Pearson Lemma)
Let P0 and P1 be two probability mea-
sures with densities f and g, respectively, with respect to some measure μ.
Then a most powerful α test for H0 = {f } against H1 = {g} exists and every such
test φ satisﬁes
φ(ω) =

1
g(ω) > cf (ω),
0
g(ω) < cf (ω),
(15.1)
where c is the smallest value for which

g(ω)>cf (ω) f (ω)dμ ≤α, and
βφ(f ) = α
(15.2)
if c > 0.
Conversely, every test that satisﬁes (15.1) and (15.2) is most powerful for the
class Φα of level α tests.
Proof Let φ be a test that satisﬁes (15.1) and (15.2) and let φ′ be any other level α
test. Then (φ(ω) −φ′(ω))(g(ω) −cf (ω)) ≥0 for all ω ∈Ω. Hence
0 ≤
# 
φ(ω) −φ′(ω)

g(ω) −cf (ω)

dμ = βφ(g) −βφ′(g) + cβφ′(f ) −cβφ(f ).
(15.3)
Since βφ(f ) = α (condition (15.2)) and βφ′ ≤α since φ′ is a level α test, we get
0 ≤βφ(g) −βφ′(g),
i.e. φ is a most powerful test in the class Φα.

15.2
Simple Tests
315
If, on the other hand, φ′ does not satisfy (15.1), then (φ(ω) −φ′(ω))(g(ω) −
cf (ω)) > 0 on a set of measure > 0 and the inequality in Eq. (15.3) is strict. If c > 0
and φ′ does not satisfy (15.1) then cβφ′(f )−cβφ(f ) > 0 and hence βφ′(g) < βφ(g).
So every most powerful test in Φα must satisfy (15.1) and (15.2).
□
Note that the optimal level α test is non-deterministic if

g(ω)=cf (ω) f (ω)dμ ̸= 0.
So even if almost all tests that are used in practice are deterministic tests, we need
non-deterministic tests to get a nice theory.
Normally we apply Theorem 15.2 in the following form:
Let P0 and P1 be two probability measures with densities f and g, respectively,
with respect to some measure μ. We observe iid. random variables X1,...,Xn and
want to test the hypothesis H0 : Xi ∼f against the alternative H1 : Xi ∼g.
Then the most powerful level α-test is to compute the likelihood ratio
Ln =
n

i=1
g(Xi)
f (Xi)
(15.4)
and reject H0 in favor of H1 if Ln exceeds a critical value cα. This kind of test is
also called a likelihood quotient test.
15.2.4 Bayesian Statistics
In statistics the usual assumption is that we have no a priori information on the
unknown parameter. In most cases this is quite reasonable. The assumption: “Nature
selects the mortality rate of a disease uniformly at random in the range between 40 %
and 60 %” does not make sense. (Even more crazy is the assumption that a universe
selects it at creation with probability 50 %, using a god for that purpose, which was
made in [269].)
In cryptography we are in the fortunate situation of having a priori information
on the unknown parameter. The unknown parameter is the key, or a part of it, and
we know the key generation protocol (most times it selects the key uniformly at
random).
In this case we can utilize Bayes’ Theorem for our statistics. In the simple case
of discrete probabilities we observe an event A and compute for each possible pa-
rameter θ the a posteriori probability of θ by
P(θ|A) = P(θ)Pθ(A)
P(A)
.
The a posteriori probability describes all the information we have about θ after
the observation of A. Thus the best we can do is to select the θ which maximizes
P(θ | A).

316
15
Statistics
15.3 Sequential Tests
15.3.1 Introduction to Sequential Analysis
Classical statistics deals with the following problem: We observe n (identical and
independently distributed) random variables X1,...,Xn. We have a hypothesis H0
and an alternative H1 and must decide from the observation X1,...,Xn whether we
accept H0 or reject H0 and favor H1.
Sequential analysis extends this classical model in the following way: We have
a potentially inﬁnite sequence of random variables X1,X2,... . At each time step
t we examine the observed random variables X1,...,Xt and must decide between
the following possibilities:
• we accept the hypothesis H0 and stop;
• we reject the hypothesis H0 and stop; or
• we decide that we have not enough data and wait until we observe the next random
variable Xt+1.
Classical statistics is just a special case of sequential statistics in which we always
choose the third alternative if t < n and choose one of the ﬁrst alternatives for t = n.
The idea of sequential statistics ﬁts well with stream ciphers. A stream cipher
generates a potentially inﬁnite key stream and after each observed bit the attacker
can decide if he has seen enough data or if he has to wait for more data.
Sequential analysis was developed in the 1940s by Wald. His book [273] is still
a good introduction. For a more recent introduction to sequential analysis I recom-
mend [249]. The remaining part of this section will introduce the parts of sequential
analysis we need.
15.3.2 Martingales
The theoretical framework needed to formulate sequential tests uses martingales.
Martingales are a standard topic in probability theory that can be found in many
books (in the following I will always refer to the nice book of P. Billingsley [27]).
We start with the deﬁnition of conditional expectation.
Deﬁnition 15.7
Let X be a random variable on (Ω,F,P) and let G ⊆F be a σ-
ﬁeld. Then E[X∥G] is called the conditional expected value of X given G if:
1. E[X∥G] is G measurable.
2. E[X∥G] satisﬁes
#
G
E[X∥G]dP =
#
G
XdP
for all G ∈G.

15.3
Sequential Tests
317
It is easy to see that the conditional expectation is unique up to values on a set of
measure 0, but it is difﬁcult to prove that the conditional expectation always exists
(see the Radon-Nikodym Theorem, Sect. 32 in [27]).
Example 15.1 Suppose (Ω,F,P) is a discrete probability space with P(ωi) = pi.
Choose G = {∅,A, ¯A,Ω}. Then E[X∥G] is constant on A. Let E[X∥G](ω) =
E[X∥A] for ω ∈A.
The equation
#
A
E[X∥G]dP =
#
A
XdP
simpliﬁes to

ω∈A
piX(ωi) = P(A)E[X∥A]
and hence
E[X∥A] =

ω∈A piX(ωi)
P(A)
.
Thus in the case of discrete probability spaces the complicated Deﬁnition 15.7
simpliﬁes to the notion of conditional expectation we know from school.
Now we can introduce martingales.
Deﬁnition 15.8 Let (Ω,F,P) be a probability space. A ﬁltration of σ-algebras is
a sequence F1,F2,... of σ-algebras with Fi ⊆Fi+1.
A martingale adapted to the ﬁltration is a sequence X1,Xn,... of random vari-
ables with:
1. Xi is Fi measurable.
2. E[|Xi|] < ∞.
3. With probability 1
E[Xi+1∥Fi] = Xi.
(15.5)
Sometimes the ﬁltration F1,F2,... is not important. We say that X1,Xn,... is
a martingale if it is a martingale with respect to some ﬁltration F1,F2,... . In this
case the ﬁltration Fn = σ(X1,...,Xn) always works.
If we replace Eq. (15.5) in the deﬁnition of a martingale by the inequality
E[Xi+1∥Fi] ≥Xi
(15.6)
we get the deﬁnition of a submartingale and if we replace it by the opposite inequal-
ity
E[Xi+1∥Fi] ≤Xi
(15.7)
we get a supermartingale.
An important special case of a submartingale is the following example.

318
15
Statistics
Example 15.2 Let Δ1,Δ2,... be a sequence of independent random variables with
E[Δn] ≥0 and E[|Δn|] < ∞for all n ∈N. Let Xn = n
i=1 Δi. Then
E[Xn+1∥X1,...,Xn] = E
&n+1

i=1
Δi
22222Δ1,...,Δn
'
=
n

i=1
Δi + E[Δn+1]
≥Xn
and hence X1,X2,... is a submartingale.
In our applications we always have submartingales of this type.
The next important concept is the stopping time.
Deﬁnition 15.9 Let τ be a random variable which takes values in N ∪{∞}. It is
a stopping time with respect to the ﬁltration F1 ⊆F2 ⊆··· if {τ = n} ∈Fn for all
n ∈N.
For a stopping time τ we deﬁne the σ-ﬁeld Fτ by
Fτ =

F ∈F|F ∩{τ = n} ∈Fn for all n ∈N

=

F ∈F|F ∩{τ ≤n} ∈Fn for all n ∈N

.
(15.8)
Supermartingales describe typical casino games. Whatever the player does, the
expected value is negative. As many players ﬁnd out the hard way, this does not
change if you play several games and follow some stopping strategy. The optimal
sampling theorem is a precise formulation of this observation.
Theorem 15.3 (Optimal sampling theorem) Let X1,...,Xn be a (sub)martingale
with respect to F1,...,Fn and let τ1, τ2 be stopping times which satisfy 1 ≤τ1 ≤
τ2 ≤n, then Xτ1, Xτ2 is a (sub)martingale with respect to Fτ1, Fτ2.
Proof For every n ∈N we have {τ2 ≤n} ⊆{τ1 ≤n}, since τ1 ≤τ2. Hence for every
F ∈Fτ1 we have F ∩{τ2 ≤n} = (F ∩{τ1 ≤n}) ∩{τ2 ≤n} ∈Fn, i.e. F ∈Fτ2.
Assume that X1,...,Xn is a submartingale with respect to F1,...,Fn.
Since |Xτi| ≤n
j=1 |Xj| and
 n
j=1 |Xj| < ∞, Xτi is integrable. We must
prove E[Xτ2|F1] ≥Xτ1 or equivalently

A Xτ2 −Xτ1 ≥0 for all A ∈Fτ1.

15.3
Sequential Tests
319
F ∈F1 implies F ∩{τ1 < k ≤τ2} = [F ∩{τ1 ≤k −1}] ∩{τ2 ≤k −1}C lies in
Fk−1. Hence
#
F
Xτ2 −Xτ1dp =
#
F
n

k=2
I{τ1<k≤τ2}Xk −Xk−1
telescope sum
=
n

k=2
#
F∩{τ1<k≤τ2}
Xk −Xk−1
≥0
by the submartingale property.
If X1,...,Xn is a martingale with respect to F1,...,Fn we apply the same ar-
gument to the martingale −X1,...,−Xn and get E[Xτ2|F1] = Xτ1.
□
For two stopping times τ1 and τ2 we let τ1 ∧τ2 = minτ1,τ2. A short calculation
proves that τ1 ∧τ2 is again a stooping time. An important special case of the optimal
sampling theorem is:
Corollary 15.1 Let X1,X2,... be a (sub)martingale and let τ be a stopping time.
Then Xτ∧1,Xτ∧2,... is also a (sub)martingale.
Proof τ1 = τ ∧(n −1) and τ2 = τ ∧n satisfy the assumptions of Theorem 15.3. □
Finally we mention without proof an important limit theorem for martingales.
Theorem 15.4 (Doob’s martingale convergence theorem) Let X1,X2,... be a sub-
martingale. If K = supn E[|Xn|] < ∞then Xn →X with probability 1, where X is
a random variable satisfying E[|X|] < K.
Proof See Theorem 35.5 in [27].
□
15.3.3 Wald’s Sequential Likelihood Ratio Test
Remember that for the (non-sequential) test problem with simple hypotheses against
a simple alternative, the likelihood quotient test (Neyman-Pearson test) described in
Sect. 15.2.3 is optimal.
With classical tests we can test to minimize the type-I error, but we have no
control over the size of the type-II error. Wald’s sequential likelihood ratio test (Al-
gorithm 15.1) lets us control both error types.
We will prove that Algorithm 15.1 controls both error types in several steps.
Lemma 15.1 If P is absolutely continuous with respect to Q then Ln, n ≥1, is a
martingale with respect to the ﬁltration Fn = σ(X1,...,Xn) under H0.

320
15
Statistics
Algorithm 15.1 Wald’s sequential test
• Let X1,... be iid. Let H0 be the hypothesis “Xi has density f ” and H1 be the
alternative “Xi has density g”.
• Let the type-I error probability be α and the type-II error probability be β.
• Set A =
β
1−α and B = 1−β
α .
• Let τ be the stopping time
τ = inf
n {n ≥1 : Ln ≤A or Ln ≥B}
where Ln = n
i=1
g(Xi)
f (X)i is the likelihood quotient.
• Accept H0 if Lτ ≤A and reject H0 in favor of H1 if Lτ ≥B.
Proof
E0[Ln+1∥Fn] = LnE0
 g(Xn+1)
f (Xn+1)
2222Fn

= Ln
# g(x)
f (x)f (x)dμ
= Ln
#
g(x)dμ = Ln.
Implicitly we have shown E0[Ln] = E0[L0] = E0[1] = 1.
□
Obviously Ln ≥0 and hence E0[|Ln|] = E0[Ln] = 1 and by Doob’s Theorem
(Theorem 15.4) Ln converges almost surely under E0. We will now determine the
limit. For that we need the Kullback-Leibner information as a tool. This is a gener-
alization of the well-known Shannon entropy for the case of probability measures
with densities.
Deﬁnition 15.10 For two probability measures Q and P the Kullback-Leibner in-
formation K(Q,P) is
K(Q,P) =

[ln dQ
dP ] dQ
dP dP
if Q ≪P,
∞
otherwise.
(15.9)
Theorem 15.5 K(Q,P) ≥0 with equality if and only if Q = P .
Proof ln(t) ≤t −1 with equality for t = 1. Thus
K(Q,P) ≥
# dQ
dP −1dP =
#
dQ −
#
dP = 0.
Since Q ̸= P on a set of positive P measure, K(Q,P) > 0 for Q ̸= P .
□

15.3
Sequential Tests
321
Lemma 15.2 limn→∞Ln = 0 with probability 1 under H0.
Proof If P0 is not absolutely continuous with respect to P1 then there exists a set A
with P0(A) > 0 and P1(A) = 0. In this case, with probability 1 there are inﬁnitely
many i with Xi ∈A. Hence with probability one, g(Xi) = 0 inﬁnitely often, i.e.
limn→∞Ln = 0 with probability 1.
Now assume that P0 ≪P1. Without loss of generality we can also assume P1 ≪
μ, i.e. g(ω) ̸= 0.
Now assume g(Xi) ̸= 0 for all i and take the logarithm
lim
n→∞
n

i=1
ln g(Xi)
f (Xi) = −∞.
By the strong law of large numbers, under H0 we get
lim
n→∞
1
n
n

i=1
ln g(Xi)
f (Xi) = E0

ln g(Xi)
f (Xi)

=
# 
ln g(ω)
f (ω)

f (ω)dμ
= −
# 
ln f (ω)
g(ω)
f (ω)
g(ω) g(ω)dμ
= −
# 
ln dP0
dP1
dP0
dP1
dP1
= −K(P0,P1) < 0
and hence limn→∞
n
i=1 ln g(Xi)
f (Xi) = limn→∞−nK(P0,P1) = −∞.
□
Interchanging the role of P0 and P1 we get
lim
n→∞L0 = ∞
with probability 1 under H1.
This proves that for every A < 1 < B the stopping time
τ = inf

n ≥1|Ln ≤A or Ln ≥B

is almost surely ﬁnite under H0 and under H1.
By the optimal sampling theorem (Theorem 15.3), the sequence Lτ∧n, n ≥1, is
a martingale under H0. In particular, E0[Lτ∧n] = E0[L1] = 1.
Furthermore, for each F ∈Fτ
P1

F ∩{τ < ∞}

=
∞

n=1
P1

F ∩{τ = n}

.

322
15
Statistics
We may write F ∩{τ < n} = {ω : (X1,...,Xn)(ω) ∈Gn} for an appropriate Borel
subset of Rn. Since the observations X1,...,Xn are independent
P1

F ∩{τ = n}

=
#
Gn
g(x1)···g(xn)μd(x1)···μd(xn)
=
#
Gn
n

i=1
g(xi)
f (xi)f (x1)···f (xn)μd(x1)···μd(xn)
=
#
F∩{τ=n}
LτdP0.
Hence
P1

F ∩{τ < ∞}

=
#
F∩{τ<∞}
LτdP0.
(15.10)
We know already that τ is almost surely ﬁnite under H0 and under H1, so setting
F = Ω in (15.10) we get
1 =
#
LτdP0.
We are now in the position that we can analyze the test. We must determine A
and B in such a way that the error probability of type-I is α = P0(Lτ ≥B) and the
error probability of type-II is β = P1(Lτ ≤A).
Then
β = P1(Lτ ≤A)
=
#
Lτ ≤A
LτdP0
by (15.10)
≤AP0(Lτ ≤A) = A(1 −α).
Interchanging the role of P0 and P1 we get α ≤B−1(1 −β).
Using the approximations β ≈A(1 −α) and α ≈B−1(1 −β) we get the thresh-
old values A =
β
1−α and B = 1−β
α
given in Algorithm 15.1.
15.3.4 Brownian Motion
In classical statistics the central limit theorem is the justiﬁcation that we can replace
the often unknown or unwieldy exact distribution by the normal distribution. We
would like to have something similar for sequential analysis. The appropriate tool
is Brownian motion.
Deﬁnition 15.11 (Brownian motion) A Brownian motion or a Wiener process with
drift μ and variance σ 2 is a stochastic process [Wt : t ∈R≥0] on some probability
space (Ω,F,P) which satisﬁes the following three properties:

15.3
Sequential Tests
323
Fig. 15.1 A Brownian motion path
1. The process starts at 0, i.e. P(W0 = 0) = 1.
2. The increments are independent, i.e. if 0 ≤t1 ≤t2 ≤··· ≤tn then
P(Wti −Wti−1 ∈Ai,i ≤n) =

i≤n
P(Wti −Wti−1 ∈Ai)
for all measurable sets Ai.
3. For 0 ≤s ≤t the increment Wt −Ws is normally distributed with mean μ(t −s)
and variance σ 2(t −s).
4. Wt is a continuous function of t with probability 1.
Figure 15.1 shows a typical Brownian motion path.
It is rather difﬁcult to prove that such a process exists (see, for example, Theo-
rem 37.1 in [27]).
If not speciﬁed otherwise, by Brownian motion we always mean the standard
Brownian motion with drift μ = 0 and variance σ 2 = 1.
The next theorem collects all the properties of Brownian motion we need:
Theorem 15.6 Let [Wt :∈R≥0] be a standard Brownian motion with drift μ = 0
and variance σ 2 = 1. Then:
(a) For every c > 0 the process [W ′
t : t ∈R≥0] deﬁned by
W ′
t (ω) = c−1Wc2t(ω)
(15.11)
is a standard Brownian motion.
(b) The process [W ′′
t : t ∈R≥0] deﬁned by
W ′′
t (ω) =

tW1/t(ω)
for t > 0,
0
for t = 0
(15.12)
is a standard Brownian motion.

324
15
Statistics
(c) Let τ be any stopping time and let Fτ be the σ-ﬁeld of all measurable sets M
with M ∩τ ≤t ∈Ft = σ(Ws,s ≤t).
Then [W ∗
t : t ∈R≥0] with
W ∗
t (ω) = Wτ(ω)+t(ω) −Wτ(ω)(ω)
(15.13)
is a standard Brownian motion and [W ∗
t : t ≥0] is independent from Fτ .
This property is called the strong Markov property.
(d) Let τ be a stopping time and let
W ′′′
t (ω) =

Wt(ω)
for t ≤τ(ω),
Wτ(ω)(ω) −(Wt(ω) −Wτ(ω)(ω))
for t ≥τ(ω).
(15.14)
Then [W ′′′
t : t ∈R≥0] is a standard Brownian motion.
This property is called the reﬂection principle.
(e) For every a ≥0 we have
P
*
max
0≤t′≤t Wt′ ≥a
+
= 2P(Wt ≥a) = 2Φ(−a/t).
(15.15)
(f) For a > 0 and y < a + bt the following equation holds:
P

Wt′ ≥a + bt′ for some t′ ∈[0,t] | Wt = y

= exp

−2a(a + bt −y)
t

.
(15.16)
(g) For a > 0 and x < a + bt we have
P

Wt′ ≥a + bt′ for some t′ ∈[0,t],Wt ≤x

= exp(−2ab)Φ
x −2a
√t

(15.17)
and in particular
P

Wt′ ≥a + bt′ for some t′ ∈[0,t]

= 1 −Φ
a + bt
√t

+ exp(−2ab)Φ
(a + bt) −2a
√t

.
(15.18)
Proof
(a) Since t →c2t is a strictly increasing function, the process [W ′
t : t ∈R≥0] has
independent increments. Furthermore, W ′
t −W ′
s = c−1(Wc2t −Wc2s) is normal
distributed with mean 0 and variance c−2(c2t −c2s) = t −s. Since the paths
of W ′(ω) are continuous and start at 0 we have proved that [W ′
t : t ≥0] is a
standard Brownian motion.

15.3
Sequential Tests
325
(b) t →1/t is strictly decreasing and hence [W ′′
t : t ∈R≥0] has independent in-
crements. As in (a) we check that the increments are normally distributed. The
paths are continuous for t > 0. That W ′′(ω) is continuous at 0 with probability 1
is just the strong law of large numbers.
(c) See [27] p. 508.
(d) By the strong Markov property we know that [W ∗
t : t ≥0] is a standard Brow-
nian motion independent from Fτ . By (a) [−W ∗
t : t ∈R≥0] is also a standard
Brownian motion independent from Fτ .
The Brownian motion [Wt : t ∈R≥0] satisﬁes
Wt(ω) =

Wt(ω)
for τ(ω) < t,
Wτ(ω)(ω) + W ∗
t−τ(ω)(ω)
for τ(ω) ≥t.
(15.19)
We can replace the random variable W ∗
t−τ(ω)(ω) in Eq. (15.19) by any other
standard Brownian motion independent from Fτ , without changing the distri-
bution. In particular, we get a Brownian motion if we replace W ∗
t−τ(ω)(ω) by
−W ∗
t−τ(ω)(ω) in Eq. (15.19).
(e) Consider the stopping time τ = inf{s|Ws ≥a} and the stopping time τ ′′′ =
inf{s|W ′′′
s ≥a} with the process W ′′′ deﬁned in part (d). Then τ = τ ′′′ and
P
*
max
0≤t′≤t Wt′ ≥a
+
= P(τ ≤t)
= P(τ ≤t,Wt ≤a) + P(τ ≤t,Wt ≥a)
= P

τ ′′′ ≤t,W ′′′
t ≤a

+ P(τ ≤t,Wt ≥a)
= 2P(τ ≤t,Wt ≥a)
= 2P(Wt ≥a).
(f) See [250] p. 375.
(g) Integrating (15.16) we get:
P

Wt′ ≥a + bt′ for some t′ ∈[0,t],Wt ≤x

=
# x
−∞
P

Wt′ ≥a + bt′ for some t′ ∈[0,t] | Wt = y

dPt(y)
=
# x
−∞
exp

−2a(a + bt −y)
t

1
√
2πt
exp

−y2
2t

dy
= exp(−2ab)
# x
−∞
1
√
2πt
exp

−(y −2a)2
2t

dy
= exp(−2ab)Φ
x −2a
√t

.

326
15
Statistics
Furthermore,
P

Wt′ ≥a + bt′ for some t′ ∈[0,t]

= P

Wt′ ≥a + bt′ for some t′ ∈[0,t],Wt ≥a + bt

+ P

Wt′ ≥a + bt′ for some t′ ∈[0,t],Wt ≤a + bt

= P(Wt ≥a + bt) + exp(−2ab)Φ
(a + bt) −2a
√t

= 1 −Φ
a + bt
√t

+ exp(−2ab)Φ
(a + bt) −2a
√t

.
□
15.3.5 The Functional Central Limit Theorem
We are now ready to formulate the functional central limit theorem which is the
justiﬁcation that we can replace a sequence of accumulated independent variables
by a Brownian motion.
Theorem 15.7 (Functional central limit theorem) Suppose that X1,X2,... are iid.
random variables with mean 0, variance σ 2 and ﬁnite fourth moment. Let
Yn(t) =
1
σ√n
k

i=1
xi
for k −1
n
< t ≤k
n.
(15.20)
Then there exists for each n (on some probability space) a process [Zn(t) : 0 ≤
t ≤1] and [Wn(t) : 0 ≤t ≤1] such that [Zn(t) : 0 ≤t ≤1] has the same ﬁnite
dimensional distributions as [Yn(t) : 0 ≤t ≤1] and such that [Wn(t) : 0 ≤t ≤1] is
a standard Brownian motion, and
P
*
sup
0≤t≤1
Zn(t) −Wn(t)
 ≥ϵ
+
→0
for ϵ > 0.
(15.21)
Proof See [27] Theorem 37.8.
□
As an application, consider a sequence X1,X2,... of iid. random variables with
mean 0 and variance σ 2. What is the limit distribution of
Mn = max
1≤i≤n
i

j=1
Xi ?
This is a typical question we have to answer when we deal with sequential tests. By
the functional central limit theorem Mn
σ√n = sup0≤t≤1 Yn has the same distribution as

15.3
Sequential Tests
327
sup0≤t≤1 Zn and by (15.21)
P
* sup
0≤t≤1
Zn(t) −sup
0≤t≤1
Wn(t)
 ≥ϵ
+
→0
for ϵ > 0.
By Theorem 15.6 (e) P(sup0≤t≤1)Wn ≥x = 2Φ(x) for x ≥0. Therefore,
P
 Mn
σ√n ≥x

→2Φ(x)
for x ≥0.
To answer similar questions for random variables with mean μ ̸= 0 we use
part (g) of Theorem 15.6.

Chapter 16
Combinatorics
Combinatorics has several applications in cryptography. We use it a lot in the anal-
ysis of the RC4 key-scheduling (Sect. 9.3.1) where most of the results from this
chapter ﬁnd their application.
16.1 Asymptotic Calculations
This chapter collects the solution to some counting problems that arise during the
analysis of RC4. We start with Euler’s famous summation method.
First we need the Bernoulli numbers which are the coefﬁcients in the following
Taylor series:
x
ex −1 = B0 + B1x + B2
2! x2 + B3
3! x3 + ··· .
(16.1)
Since
x
ex −1 + x
2 = x
2 · ex + 1
ex −1 = −x
2 · e−x + 1
e−x −1
is an even function, we see that B2k+1 = 0 for k ≥1.
We deﬁne the Bernoulli polynomial by
Bn(x) =
n

k=0
n
k

Bkxn−k.
(16.2)
A short calculation gives
B′
n(x) =
n

k=0
n
k

Bk(n −k)xn−k−1
= m
n−1

k=0
n −1
k

Bkx(n−1)−k
= mBn−1(x).
A. Klein, Stream Ciphers, DOI 10.1007/978-1-4471-5079-4_16,
© Springer-Verlag London 2013
329

330
16
Combinatorics
Fig. 16.1 Comparison of
 b
a f (x)dx and
1/2f (a) + f (a + 1) + ··· +
f (b −1) + 1/2f (b)
Another important identity arises if we multiply the deﬁning equation (16.1) of
the Bernoulli numbers by ex −1 = ∞
n=1
1
n!xn and compare the coefﬁcients of xn
in the corresponding power series. This gives
n

k=0
Bk
k!
1
(n −k)! = δn1
or, after some simple transformations,
n

k=0
n
k

Bk = Bn + δn1.
(16.3)
For the Bernoulli polynomials Eq. (16.3) implies
Bn(1) = Bn = Bn(0) for n > 1.
(16.4)
As Fig. 16.1 shows,
 b
a f (x)dx is approximately 1/2f (a) + f (a + 1) + ··· +
f (b−1)+1/2f (b). Euler’s formula gives precise error terms. One can view it either
as a method to evaluate sums by integration or as a tool for numerical integration.
Theorem 16.1 (Euler’s summation formula) Let f : R →R be m times differen-
tiable. Then for a,b ∈Z
b

k=a
f (k) =
# b
a
f (x)dx + f (a) + f (b)
2
+
m

k=2
Bk
k! f (k−1)(x)

b
a
+ Rm
(16.5)
where
Rm = (−1)m+1
# b
a
Bm({x})
m!
f m(x)dx.
(16.6)
({x} denotes the fractional part of x.)

16.1
Asymptotic Calculations
331
Proof We begin with the following identity:
# k+1
k

{x} −1
2

f ′(x)dx =

x −k −1
2

f (x)

k+1
k
−
# k+1
k
f (x)dx
= 1
2

f (k + 1) + f (k)

−
# k+1
k
f (x)dx
(16.7)
which is just integration by parts. Summation from a to b gives
# b
a

{x}−1
2

f ′(x)dx = 1
2f (a)+f (a+1)+···+f (b−1)+ 1
2f (b)−
# b
a
f (x)dx
which is Eq. (16.5) for m = 1.
Integrating by parts gives
1
m!
# b
a
Bm

{x}

f (m)(x)dx =
1
(m + 1)!Bm+1

{x}

f (m)(x)
b
a
−
1
(m + 1)!
# b
a
Bm+1

{x}

f (m+1)(x)dx.
(16.8)
Here we use that B({x}) is continuous by Eq. (16.4).
Applying (16.8) repeatedly to (16.1) proves the theorem.
□
As an example of how to apply Euler’s summation formula we derive an asymp-
totic expansion for the harmonic numbers Hn = n
i=1
1
i .
Theorem 16.2 There exists a constant γ (Euler’s constant) such that
Hn−1 = lnn + γ +
m

k=1
(−1)k−1Bk
knk
+
# ∞
n
Bm(x)
xm+1 dx
= lnn + γ +
m

k=1
(−1)k−1Bk
knk
+ O

1
nm+1

.
(γ ≈0.57721...)
Proof By Euler’s summation formula, we get
Hn−1 = lnn +
m

k=1
Bk
k!
(−1)k−1(k −1)!
xk

n
1
+ (−1)m+1
# n
1
Bm({x})
m!
· (−1)mm!
xm+1
dx.
(16.9)

332
16
Combinatorics
Taking the limit we get
γ = Hn−1 −lnn =
n

k=1
Bk
k (−1)k −
# ∞
1
Bm(x)
xm+1 .
(16.10)
Since Bm(x) is bounded in the interval [0,1], the limit exists.
Putting (16.9) and (16.10) together proves the theorem.
□
More examples of the use of Euler’s formula can be found in [119, 151].
16.2 Permutations
The bijective functions from {1,...,n} to {1,...,n} form the symmetric group Sn.
Sn has n! = n · (n −1) · ··· · 2 · 1 elements. The elements of Sn are called permuta-
tions.
For the permutations of Sn we use the famous cycle notation, i.e. we write
(123)(45) for (1 →2,2 →3,3 →1,4 →5,5 →4). When we multiply permu-
tations we read the terms from right to left (think of function composition), i.e.
(12)(23) = (231).
An involution is a permutation of order 2. In our analysis of RC4 (see Theo-
rem 9.7) we need the number of involutions in Sn, which will be determined in this
section.
Let tn be the number of involution in Sn. The element n may either be a ﬁxed
point or exchanged with any of the other elements. This leads to the recursion
tn = tn−1 −(n −1)tn−2,
which in turn leads to an easy way to tabulate tn for small n.
An alternative is to count the number tn,k of permutations with k two-cycles. To
generate such a permutation write down any permutation of n elements. Then pair
the ﬁrst 2k elements into k two-cycles and declare the last n −2k elements as ﬁxed
points. By this procedure, one counts every permutation of tn,k exactly k!2k(n−2k)!
times (k! ways to order k two-cycles, each two-cycle can be written in two ways as
(ab) or (ba), and the n −2k ﬁxed points can be in any order). Thus
tn,k =
n!
k!2k(n −2k)!.
No explicit formula for tn = ⌊n/2⌋
k=0 tn,k is known, but we can determine its
asymptotic behavior. Our derivation of the asymptotic behavior follows [153].
Theorem 16.3 The number tn of involutions in Sn satisﬁes
tn = 1
√
2
nn/2e−n/2+√n−1/4

1 + 7
24n−1/2 + O

n−3/4
.

16.2
Permutations
333
Proof First note that
tn,k+1
tn,k
= (n −2k)(n −2k −1)
2(k + 1)
so tn,k increases until k reaches approximately 1
2(n −√n).
The transformation k = 1
2(n −√n) + x brings the large parts near the origin.
To get rid of the factorials we apply Stirling’s formula.
If |x| ≥nϵ+1/4, Stirling’s formula gives the bound
tn,k ≤e−2n2ϵ exp
1
2nlnn −1
2n + √n −1
4 lnn −1
4 −1
2 lnπ + O

n3ϵ−1/4
.
Because of the term e−2n2ϵ, this is much smaller than any polynomial in n and, as
we will see, these terms do not contribute to the order of tn.
So we may restrict x to the interval [−nϵ+1/4,nϵ+1/4] for some ϵ. For x in that
interval we get by Stirling’s formula the expansion
tn,k = exp
1
2nlnn −1
2n + √n −1
4 lnn −2x2
√n −1
4 −1
2 lnπ
−4x3
3n + 2x
√n +
1
3√n −4x4
3n√n + O

n5ϵ−3/4
.
(16.11)
The common factor
exp
1
2nlnn −1
2n + √n −1
4 lnn −1
4 −1
2 lnπ +
1
3√n

in (16.11) causes no difﬁculty. So we must sum
exp

−2x2
√n −4x3
3n + 2x
√n −4x4
3n√n + O

n5ϵ−3/4
= exp

−2x2
√n

1 −4
3
x3
n + 8
9
x6
n2

1 + 2 x
√n + 2x2
n

×

1 −4
3
x4
n√n

1 + O

n9ϵ−3/4
(16.12)
from α to β where α and β are integers near ±nϵ+1/4.
Multiplying out we must evaluate the sum
β

x=α
xte−x2
for integers t. As e−x2 approaches 0 quickly for |x| →∞, we can replace the limits
α and β by −∞and +∞without changing the asymptotic behavior.

334
16
Combinatorics
Applying Euler’s summation formula we get
+∞

x=−∞
f (x) =
# +∞
−∞
f (x)dx −1
2f (x)

+∞
−∞
+ B2
2
f ′(x)
1!

+∞
−∞
+ ···
+
Bm
m + 1
f (m)(x)
m!

+∞
−∞
+ Rm+1.
Thus for f (x) = xte−x2 we get f m(x)|+∞
−∞= 0 and Rm+1 = O(n(t+1−m)/4), so only
the integral gives a signiﬁcant contribution.
The integral
 +∞
−∞xte−x2dx is not difﬁcult to evaluate. For odd t a simple sub-
stitution x2 = y does the job. For even t we use partial integration to reduce the
problem to
 ∞
−∞e−x2dx. This integral is solved by
# ∞
−∞
e−x22
dx =
# +∞
−∞
# +∞
−∞
e−x2−y2dxdy
then transforming Cartesian coordinates to polar coordinates
=
# 2π
0
# ∞
0
re−r2drdφ
= π.
Plugging everything together gives the asymptotic expansion of tn. With a bit
of extra work the error term could be improved to O(n−k) for any k, instead of
O(n−3/4).
□
16.3 Trees
A tree is a graph without cycles and a labeled tree with n vertices is a tree whose
vertices are numbered from 1 to n.
We encountered labeled trees in Sect. 9.3.1. There we cited a famous theorem of
Cayley on the number of ﬁnite trees, which will be proved in this section.
Theorem 16.4 (Cayley [47]) There are nn−2 labeled trees with n vertices.
Proof A very elegant proof of Cayley’s theorem is due to Prüfer [215].
Let T be a labeled tree with n vertices. We deﬁne a sequence starting with T1 =
T . To obtain Ti+1 from Ti delete the monovalent vertex of Ti with the smallest label.
Let xi be the label of that vertex and let yi be the adjacent vertex in Ti. The Prüfer
code of T is (y1,...,yn−2).

16.3
Trees
335
Fig. 16.2 A labeled tree
For example the tree of Fig. 16.2 has the Prüfer code (6,1,2,2,6,6). Note
that in the example y7 = 8 as for all trees with 8 vertices and the sequence xi is
(3,4,1,5,2,7,6).
We claim that the Prüfer code is a bijection from the set labeled trees to the set
{1,...,n}n−2. To prove this we must reconstruct the tree from the Prüfer code.
Note the following simple facts:
• Since every tree has at least two monovalent vertices, we never delete the vertex
n, i.e. yn−1 = n.
• The edges of Tk are (xk,yk),...,(xn−1,yn−1).
• The number of occurrences of a vertex v in the sequence yk,...,yn−2 is
(degTk v)−1. (v occurs degTk v times in the list of edges (xk,yk),...,(xn−1,yn−1)
and exactly once in xk,...,xn−1, yn−1.)
• xk is the smallest number missing in {x1,...,xk−1,yk,...,yn−1}.
Thus one can reconstruct the sequence x1,...,xn−1 uniquely from the Prüfer
code (y1,...,yn−2) and hence the Prüfer code determines the unique tree with edges
(x1,y1),...,(xn−1,yn−1).
□

Part IV
Exercises with Solutions

Chapter 17
Exercises
The exercises are sorted roughly according to the corresponding chapters. The num-
ber inside the parentheses indicates the difﬁculty ranging from 1 (easy) to 5 (hard).
The letter P indicates that the exercise involves programming and the letter M in-
dicates that the exercise needs a bit more mathematics than the other exercises.
17.1 (1) What are the differences between the different operation modes of a block
cipher? In addition to the three stream-oriented modes OFB, CFB and CTR (see
Fig. 1.2) also consider the block-oriented modes ECB (electronic code book) and
CBC (cipher block chaining) deﬁned by ci = E(mi,k) and ci = E(ci−1 ⊕mi,k),
respectively.
Complete the following table:
Mode
ECB
CBC
OFB
CFB
CTR
parallel encoding?
Yes
parallel decoding?
Yes
The equation ci = cj
implies
mi = mj
an input bit affects
only the current block
a ﬂipped bit in the
ciphertext affects
a whole block
17.2 (1) What are the differences between synchronous and self-synchronizing
stream ciphers with respect to bit ﬂipping and deletion errors?
17.3 (2) Break the following Vigenère cipher
OVSTL ZOHKI LLUZL HALKM VYZVT LOVBY
ZPUZP SLUJL DPAOO PZSVU NAOPU IHJRJ
BYCLK VCLYH JOLTP JHSCL ZZLSP UDOPJ
OOLDH ZIYLD PUNHW HYAPJ BSHYS FTHSV
KVYVB ZWYVK BJAOP ZOLHK DHZZB URBWV
UOPZI YLHZA HUKOL SVVRL KMYVT TFWVP
A. Klein, Stream Ciphers, DOI 10.1007/978-1-4471-5079-4_17,
© Springer-Verlag London 2013
339

340
17
Exercises
UAVMC PLDSP RLHZA YHUNL SHURI PYKDP
AOKBS SNYLF WSBTH NLHUK HISHJ RAVWR
UVA
Hint: The keyword has length ﬁve characters.
17.4 (3) Break the following auto key cipher:
LSHSF LYTRO REETS ZSFEG PFQLL IOSUL
LBMOM LUOMY NQLRO GUUAR XHVRG LMBUD
RTMOU ROLBW ORWXJ FHHBJ MVZRZ SYQVP
FNWST DHXML PTOVJ XEMON OMGG
Hint: The message contains the word “secret”.
17.5 (1) Explain why it is an error to use a binary additive stream cipher two times
with the same keystream for encryption. What is the consequence for the minimal
length of an initialization vector? (Assume for simplicity that the initialization vec-
tor is chosen at random.)
17.6 (2) There exist six m-sequences of order 5. Compute for all pairs of them the
cross-correlation function and verify in these special cases the results of Sect. 2.3.3.
17.7 (P3) Algorithm 17.1 is a variation of Algorithm 2.6. Prove that it computes the
sideway addition mod 2.
Algorithm 17.1 Sideway addition mod 2
Require: x = (x31 ...x0) is 32-bit word
Ensure: y = SADD(x) mod 2
1: y ←x ⊕(x ≫16)
2: y ←y ⊕(y ≫8)
3: y ←y ⊕(y ≫4)
4: y ←0x6996 ≫(y &0xF) {0x6996 and 0xF are hexadecimal constants}
5: y ←y &1 {y ←y mod 2}
17.8 (2) Enumerate all binary sequences a0,...,a2n which have a perfect linear
complexity proﬁle.
17.9 (P5) Develop an algorithm that chooses a random de Bruijn sequence of order
n. All 22n−1 possibilities should have equal probability.
17.10 (2) Consider a Geffe generator built from three LFSRs with feedback poly-
nomials f1 = x5 +x2 +1, f2 = x2 +x +1 and f3 = x4 +x +1. Compute the linear
complexity of the generator and the feedback polynomial of the minimal LFSR that
generates the same sequence.

17
Exercises
341
17.11 (M2) Let b1,...,bn ∈{0,1}. Let s = b1 +···+bn (summation is done in the
domain of integers). Determine the Boolean function fj,n for the jth bit of s. What
is the degree of fj?
17.12 (M2) In a correlation attack you observe a sequence z1,z2 ... and want to
compare it with a possible internal sequence x1,x2,... .
Test the hypothesis
• H0: The observed sequence is correlated to the internal sequence by P(zi = xi) =
1
2 + ε.
against the alternative
• H1: both sequences are uncorrelated.
How must you choose the stopping conditions to bound the error probability for
an error of the ﬁrst type by 1 % and for an error of the second type by 0.1 %?
Let ε = 0.1. Assuming that the hypothesis is false, what is the expected number of
observed bits before rejection?
17.13 (P3) Prove: If f = x ?fh : fl, g = x ?gh : gl and h = x ?hh : hl then f ∧g ∧
h = x ?(fh ∧gh ∧hh) : (fl ∧gl ∧hl).
Turn this into an algorithm with computes the BDD of f ∧g ∧h directly. Find
functions f , g and h for which the ternary algorithm is faster than computing f ∧
g ∧h in any order by a binary algorithm.
Also ﬁnd functions f , g and h for which the binary computation (f ∧g) ∧h is
faster than the ternary algorithm.
17.14 (P4) Sometimes we are not interested in the complete Boolean function f ,
but only in the values that f takes on some set S ⊆{0,1}n. In these cases we can
try to simplify f by changing its values outside S.
Deﬁne f ↓g (read: f is constrained with respect to g) by f ↓g = 0 if g = 0 and
(f ↓g)(y) = f (y′) where y′ is the ﬁrst element in the series y,y ⊕1,y ⊕2,... for
which g(y′) = 1 otherwise.
Prove that f ↓g satisﬁes:
(a) 0 ↓g = 0.
(b) f ↓1 = f .
(c) 1 ↓g = g ↓g = 1 for g ̸= 0.
(d) If f = (x0 ?fh : fl) ̸= 0 and g = (x0 ?gh : gl) ̸= 0 then
f ↓g =
⎧
⎪⎨
⎪⎩
fl ↓gl
if gh = 0,
fh ↓gh
if gl = 0,
x0 ?(fh ↓gh) : (fl ↓gl)
otherwise.
Turn this equation into a top down algorithm for computing the BDD of f ↓g
similar to the one described in the text for Boolean operations.

342
17
Exercises
17.15 (P5) The self-shrinking generator is constructed from an LFSR by applying
the following transformation to the output. Group the output bits in pairs, delete
the pairs 00 and 01 and replace the pairs 10 and 11 by 0 and 1, respectively. For
example 11 01 10 00 10 is mapped to 1 0 0.
Implement a BDD-based attack against the self-shrinking generator. Estimate the
complexity of the attack.
17.16 (M3) Let f,g ∈R[x,y]. Prove that if (ˆx, ˆy) is a solution of f (x,y) =
g(x,y) = 0 then ˆx is a zero of Resy(f,g).
17.17 (2) Let ≺satisfy (1) and (2) of Deﬁnition 6.1. Show that the graded version
of ≺deﬁned by α ≺gr β ⇐⇒(|α| < |β|) ∨(|α| = |β| ∧α ≺β) is a monomial
order.
17.18 (1) Consider a shrinking generator in which the control and the output se-
quence are generated by LFSRs with the same feedback polynomial. Why is it a
terrible idea to also use the same initial state for both LFSRs?
17.19 (3) Transfer the ideas of Sect. 7.3.3 to the self-shrinking generator (described
in Exercise 17.15). Develop a correlation attack against the self-shrinking generator.
17.20 (2) Prove that if RC4 starts in the state j = i + 1, S[j] = 1, then it runs
through a cycle of length n.
Why can this cycle never cause problems?
17.21 (5) Consider the following variation of RC4 which replaces the additions
mod 256 by bitwise XOR (see Algorithm 17.2).
Algorithm 17.2 A weak variation of the RC4 pseudo-random generator
1: {initialization}
2: i ←0
3: j ←0
4: {generate pseudo-random sequence}
5: loop
6:
i ←(i + 1)265
7:
j ←j ⊕S[i]
8:
Swap S[i] and S[j]
9:
k ←S[i] ⊕S[j])
10:
output S[k]
11: end loop
Give a reason why Algorithm 17.2 is weaker than the original RC4.

17
Exercises
343
17.22 (2) Bring the Blum-Blum-Shub generator into the formal form of Algo-
rithm 11.1, so that you can apply Theorem 11.1 directly.
17.23 (M2) Prove that the number SADD(x) of set bits in 32-bit integer (standard
two’s complement representation) satisﬁes:
SADD(x) = −
31

i=0
(x ≪i).
(17.1)
(Here x ≪i denotes the rotation by i bits.)
Does this lead to an efﬁcient way to compute the sideway addition?
17.24 (P3) Show that x ←x &(x −1) removes the leftmost bit of x. Turn this into
a method that computes SADD(x) with O(SADD(x)) operations.
17.25 (M3) Show that Algorithm 13.1 works correctly. Divide the task into the
following steps:
(a) The Jacobi symbol is multiplicative in both arguments, i.e.
 aa′
b

=
 a
b
 a′
b

and
 a
bb′

=
 a
b
 a′
b′

.
(b) If b ≥3 is odd then
a
b

=
a mod b
b

.
(c) If a,b ≥3 are odd then
a
b
b
a

= (−1)(a−1)(b−1)/4.
(d)
 2
b

= 1 if b ≡±1 mod 8 and
 2
b

= −1 if b ≡±3 mod 48.
(e) Prove that Algorithm 13.1 works correctly.
17.26 (M5) The basic variant of the knapsack-based public key cryptosystem [190]
works as follows:
Choose a sequence (bi)i=0,...,k−1 with bi+1 ≥2bi and m > k−1
i=0 bi. Choose c
with gcd(c,m) = 1. Publish the values ai = bic mod m.
To encrypt a binary number x = k−1
i=0 xi2i (xi ∈{0,1}) one computes s =
k−1
i=0 aixi.
You can easily compute s′ = sc−1 mod m and solve the easy knapsack problem
s′ = k−1
i=0 bixi.
The (general?) knapsack problem, where s = k−1
i=0 aixi, had been assumed to be
secure because it is NP-complete (average case complexity). However, the weights
ai have a very special form which is far from the average case.
Break the cipher by writing the knapsack problem as a lattice problem and using
the LLL-algorithm.

344
17
Exercises
Try your attack on the following toy example. The pairs AA, AB, ..., ZZ of letters
are identiﬁed with the binary numbers from 0,...,262 −1 = 675.
The weights ai are given by the following table.
i
0
1
2
3
4
5
6
7
8
9
ai
4881
2662
1774
4879
4285
5020
2940
3661
3772
3698
The message is
12938,14196,16432,18620
No rigorous proof is needed as long as you recover the plaintext.
Bonus question:
Find at least one protocol failure in the toy example, besides using a broken
cipher and it being too small.
17.27 (M3) Let n = pq be the product of two primes. Show that if |p −q| ≤
4√n
then the integer n can be factored in polynomial time.
17.28 (M3) Use Zsigmondy’s theorem to prove Wedderburn’s theorem: Every ﬁnite
skew ﬁeld is a (commutative) ﬁeld.
Hint:
Write down the class formula for the conjugation classes. Use Zsigmondy’s the-
orem to obtain a contradiction.
17.29 (M2) Compute the variation distance of an N(0,1) and an N(0,2) distribu-
tion.
17.1 Proposals for Programming Projects
Every algorithm described in the book provides a programming project. The follow-
ing proposals should help you to ﬁnd interesting starting points. All the proposals
are open-ended problems.
Project 1 (correlation attacks) Implement a correlation attack. The program
can beneﬁt a lot from clever data structures and memory layout (Sect. 4.1.3 gives
some hints). You may want to optimize the bitset handling (Sect. 12.1.2). There
is also plenty of opportunity for parallel programming. The Fourier transform
(Sect. 4.1.6.2) is suitable for GPU computing (perhaps a good reason to learn
Opencl). The pre-processing phase can be implemented as distributed programs.
To test your correlation attack I recommend a simple non-linear combiner (for
example 2n + 1 LFSRs with the majority function as combiner).
Project 2 (correlation attack) Adopt your correlation attack program to a real
cipher, for example the shrinking generator (Sect. 7.3.3) or A5/1 (Sect. 8.3.3).
In comparison to the theory from Chap. 4, some modiﬁcations will be necessary
since the correlations are now less regular.

17.1
Proposals for Programming Projects
345
What is the biggest size of the shrinking generator you can break?
Project 3 (algebraic attack) Implement the XL-Algorithm (Sect. 6.1.2.3). Play
around with the parameters and try to ﬁnd good heuristics. Your implementation
can beneﬁt a lot from fast linear algebra (Sect. 12.5).
Project 4 (RC4) The attacks based on Golic’s correlation (Sect. 9.5) are better
than the FMS-attack (Sect. 9.4) because they can use more information.
Try to integrate all the information you can get into one algorithm (Sect. 9.5.2
gives some ideas).
This can become a programming contest. The winner is the one who needs the
fewest sessions to break RC4.
Project 5 (RC4) Implement a state recovering attack against RC4 (Sect. 9.6).
Try to optimize the program as much as possible (d-order, g-generative patterns or
fortuitous states can be of help).
What is the largest n for which you can break RC4 working on Z/nZ? This is
another possible programming contest.
Project 6 (side channel attacks) Choose any cipher (for example an irregular
shift register or the Blum-Blum-Shub generator) and try to secure it against side
channel attacks (Sect. 7.4). The RC4 implementation I put on the homepage of the
book shows some ideas.
Perhaps you can do this project together with a friend. After the implementation
you can exchange your programs and try to hack the other’s implementation.
Project 7 (BDD-based attack) Implement a BDD-based attack against your
favorite cipher (see also Exercise 17.15). Try time-memory trade-off techniques
(Sect. 5.2.2).
Can you make use of BDD-variants (like zero-suppressed BDDs) or of advanced
BDDs techniques (like variable reordering)?
This project beneﬁts from a lot of clever memory layout (Sect. 12.2 gives some
ideas).

Chapter 18
Solutions
17.1 The complete table is:
Table 18.1 Comparison of block cipher modes
Mode
ECB
CBC
OFB
parallel encoding?
Yes
No
NO
parallel decoding?
Yes
Yes
NO
The equation ci = cj
implies
mi = mj
mi ⊕mj =
ci−1 ⊕cj−1
mi ̸= mj
an input bit affects
only the current
block
all following blocks
only the
corresponding
ciphertext bit
a ﬂipped bit in the
ciphertext affects
a whole block
a whole block and
one bit in the
following block
only one bit
Mode
CFB
CTR
parallel encoding
No
Yes
parallel decoding?
Yes
Yes
The equation ci = cj
implies
mi ̸= mj
mi ̸= mj
an input bit affects
all following blocks
only the
corresponding
ciphertext bit
a ﬂipped bit in the
ciphertext affects
a bit and the next
block
only one bit
Note that in the block-oriented modes two identical cipher blocks (ci = cj) leak
one equation (128 bit information on a standard block cipher like AES), but the
stream-oriented modes leak almost no information. This is one reason why the
stream-oriented modes are considered superior.
A. Klein, Stream Ciphers, DOI 10.1007/978-1-4471-5079-4_18,
© Springer-Verlag London 2013
347

348
18
Solutions
Note also that the two modes OFB and CTR which use the clock cipher as syn-
chronous stream cipher have no error propagation. So they must be secured against
active attacks by some other means (see Algorithm 1.1).
17.2 In a synchronous stream cipher, bit ﬂipping affects only the current bit. So a
synchronous cipher provides absolutely no resistance against an active attack. One
always needs an extra cryptographic primitive to secure it against an active attack
(see Algorithm 1.1). In a self-synchronizing stream cipher with delay k, bit ﬂipping
affects the k following bits, providing at least a minimal security against active
attacks.
In a self-synchronizing stream cipher with delay k, a deletion error affects only
the k following bits. This is what the name self-synchronizing means.
In a synchronous stream cipher a deletion error affects all following bits (the
synchronization is lost and will not be established automatically). You must ensure
the synchronization by other parts of the protocol.
17.3 Since we know already that the key word has length 5, we must break just 5
simple Caesar ciphers.
Since the length of the cipher text is relative short, we cannot expect that ‘E’ is
always the most frequent letter. (Only three of the ﬁve Caesar ciphers are solved
by searching for the most frequent letter.) However, if we look at the whole letter
distribution it is not difﬁcult to ﬁnd the key word DANCE and to decrypt the cipher
as:
“Holmes had been seated for some hours in silence with his long, thin back
curved over a chemical vessel in which he was brewing a particularly malodorous
product. His head was sunk upon his breast, and he looked from my point of view
like a strange, lank bird, with dull grey plumage and a black top-knot.”
The message is the beginning of the novel [81].
17.4 This problem shows that the auto-key cipher provides no security against a
known plaintext attack. If we know or suspect that the plaintext contains the word
“secret” we can try each possible position of the key-phrase “secret” and test if we
get a plausible plain text. The best way to do this manually is to shift the cipher text
by S, E, C, R, E and T respectively and write these texts in shifted rows as shown
below.
Then we must search for columns which contain meaningful text.
TAPANTGB ... TUJCLZBUWCZWTJEWZEFRNPPJRUDHZHAGY ...
HODOBHUPN ... IXQZNPIKQNKHXSKNSTFBDDXFIRVNVOUMR ...
JQFQDJWRPM ... ZSBPRKMSPMJZUMPUVHDFFZHKTXPXQWOTN ...
UBQBOUHCAXA ... DMACVXDAXUKFXAFGSOQQKSVEIAIBHZEYO ...
HODOBHUPNKNA ... ZNPIKQNKHXSKNSTFBDDXFIRVNVOUMRLBJ ...
SZOZMSFAYVYLL ... YATVBYVSIDVYDEQMOOIQTCGYGZFXCWMUD ...
We ﬁnd two such columns. From the distance between the two columns we
can conclude that the key has 7 letters and that the plaintext contains the phrase
... ETHODO?SECRET?RITING .... A plausible possibility for the two miss-
ing letters is ... ETHODOFSECRETWRITING ....

18
Solutions
349
Decoding the whole cipher under this assumption reveals the message:
“Few persons can be made to believe that it is not quite an easy thing to invent a
method of secret writing which shall bafﬂe investigation.”
The key is GOLDBUG.
The message is a part of Edgar Allan Poe’s essay “A Few Words on Secret
Writing” [212]. The key was of course chosen in honor of Poe’s novel “The Gold-
Bug” [213].
More techniques for attacking classical ciphers can be found in [98].
17.5 Let ci = mi ⊕zi and c′
i = m′
i ⊕zi be two cipher texts that belong to different
messages, but to the same keystream.
Then the attacker can compute d = ci ⊕c′
i = mi ⊕m′
i. Now the messages mi and
m′
i can be reconstructed with Friedman’s attack (see also Problem 17.4).
As a practical consequence we must require that an initialization vector is never
used twice. If we want to use 2k sessions the birthday paradox forces us to work with
an initialization vector of length 22k (see also the protocol failure in WEP described
in 9.2.1).
Normally we want to have initialization vectors of size 80 bits or more.
17.6 The multiplicative group F×
25 is cyclic of order 31. As 31 is prime all elements
of F25\{0,1} are primitive elements. Hence there exist 30/5 = 6 primitive polyno-
mials of degree 5. The polynomial are f1 = x5 +x2 +1, f2 = x5 +x4 +x3 +x2 +1,
f3 = x5 + x4 + x2 + x + 1, f4 = x5 + x3 + x2 + x + 1, f5 = x5 + x4 + x3 + x + 1
and f6 = x5 + x3 + 1.
The corresponding LFSR sequences are
L1 = 0010110011111000110111010100001
L2 = 1111011100010101101000011001001
L3 = 0010110011111000110111010100001
L4 = 0010011000010110101000111011111
L5 = 1101100111000011010100100010111
L6 = 0000101011101100011111001101001
The cross-correlation function of L1 and L6 take 5 values (5 × −1,1 × 11,10 ×
3,5 × −5,5 × −9). The cross-correlation of L2 and L3 and the cross-correlation of
L4 and L5 have the same spectrum.
The spectrum of all the other cross-correlation functions is (15×−1,10×7,6×
−9).
The spectrum of the auto-correlation functions is (1×31,30×−1), as we proved
in Theorem 2.3.
17.7 After line 3 the last four bits y0, y1, y2 and y3 of y satisfy
yi =
 7

j=0
xi+4j

mod 2.

350
18
Solutions
Algorithm 2.6 adds two further steps to compute y0 + y1 + y2 + y3 mod 2. In
Algorithm 17.1 this computation is replaced by a table look up.
There are only 16 possible inputs and a table of 16 bits ﬁts in the register. What
happens in line 4 is that the ith bit of the constant 0x6996 is moved to the left where
i = (y3,...,y0)2. The constant 0x6996 is chosen in such a way that this bit has the
right value.
In comparison to Algorithm 2.6 this “in register table look-up” saves two instruc-
tions (at least if loading the constants does not count).
This trick was found by Falk Hueffner [277].
17.8 The simplest way to enumerate the sequences with a perfect linear complexity
proﬁle is to use Theorem 2.19.
Theorem 2.19 allows us to choose a1,a3,... arbitrarily, while a0,a2,... are
given by the recursion a2i = a2i−1 + ai.
So all we have to is to enumerate the n-tuples a1,...,a2n−1. There are many
ways to do this: lexicographic, Gray codes, ...see [157].
Note that we can determine in advance which ai we have to change if we toggle
a2j−1 (namely all ai with i ∈{2kj,2kj −2k−1 | k ∈N}).
So the enumeration must be no slower than the simple enumeration of all n-
tuples.
17.9 By Theorem 3.3 we get a random Eulerian cycle of the de Bruijn graph starting
with the edge 0 →0 in the de Bruijn graph Dn by choosing a random oriented
spanning subtree with root 0.
To do this we turn the counting procedure of Theorem 3.4 into an algorithm (see
Algorithm 18.1).
A small example makes clear what happens in Algorithm 18.1. We want a ran-
dom de Bruijn sequence of order 2.
The Laplacian matrix of D2 is
L =
⎛
⎜⎜⎝
1
−1
0
0
0
2
−1
−1
−1
−1
2
0
0
0
−1
1
⎞
⎟⎟⎠.
Deleting the 0th column and row of L we get
L0 =
⎛
⎝
2
−1
−1
−1
2
0
0
−1
1
⎞
⎠.
Now we choose a vertex v = 10 and an edge e = 10 →01. Our matrix splits into
L′ =
⎛
⎝
2
−1
−1
0
1
0
0
−1
1
⎞
⎠
and
L′′ =
⎛
⎝
2
−1
−1
−1
1
0
0
−1
1
⎞
⎠.

18
Solutions
351
Algorithm 18.1 Choosing a random de Bruijn sequence
1: {initialization}
2: Let L be the Laplacian matrix
3: L ←L0 {see Theorem 3.4}
4: Set the current subtree to the empty set.
5: repeat
6:
Choose a vertex v different from 0 for which you don’t have an edge in the
current subtree.
7:
Choose an edge e starting at v.
8:
Write L = L′ + L′′ where L′ is matrix for the graph with the edge e removed
and L′′ is the graph with all edges starting at v except with e removed.
9:
Add e to your current subtree with probability p = detL′′
detL .
10:
If you add e, set L ←L′′, otherwise set L ←L′.
11: until you have an oriented spanning subtree.
12: Turn the oriented spanning subtree into an Eulerian cycle as described in Theo-
rem 3.3.
13: Choose a random starting point in the Eulerian cycle and output the correspond-
ing de Bruijn sequence.
Computing the determinants we get detL0 = 2 (as proved in Sect. 3.1), detL′ = 2
and detL′′ = 0. Hence p = 0, which means that e cannot be part of any oriented
spanning subtree. So we must put the e = 10 →00 into the tree and continue with
L = L′.
In the next step we choose v = 10 and e = 10 →01. Now we get
L′ =
⎛
⎝
1
0
−1
0
1
0
0
−1
1
⎞
⎠
and
L′′ =
⎛
⎝
1
−1
0
0
1
0
0
−1
1
⎞
⎠.
We get detL′ = detL′′ = 1 and hence p = 1/2. So we have to choose with proba-
bility 1/2 either e = 10 →01 or 10 →11 as part of our oriented spanning subtree.
After this choice, the remaining part of the oriented spanning subtree is uniquely
determined and we have made a random choice between the two possible Eulerian
cycles.
17.10 The non-linear combination function of the Geffe generator is f (a,b,c) =
ab + bc + c.
A feedback polynomial of the sequence L1L2 is, by Theorem 3.7:
f12(z) = Resx

f1(x),f ∗
2 (zx)

= z10 + z8 + z6 + z5 + 1.

352
18
Solutions
Fig. 18.1 Binomial
coefﬁcients modulo 2
Similarly we get a feedback polynomial of the sequence L2L3:
f23(z) = Resx

f3(x),f ∗
2 (zx)

= z8 + z7 + z6 + z4 + 1.
Finally we get a feedback polynomial of the Geffe generator, by Theorem 2.9, as
f (z) = f12(z)f23(z)f3(z)
= z22 + z21 + z18 + z17 + z15 + z14 + z10 + z7 + z6 + z + 1.
Since the feedback polynomials f1, f2 and f3 are all primitive and of pairwise
different degree, Theorem 3.8 shows that the Geffe generator has linear complexity
5 · 2 + 2 · 4 + 4 = 22 and hence f is indeed the minimal feedback polynomial of the
Geffe generator.
17.11 For any prime p, let n = 
i nipi and ki = 
i kipi, then
n
k

≡

i
ni
ki

mod p.
(18.1)
Specializing for p = 2 and k = 2j we get
 n
2j

≡1 mod 2 if and only if the jth
position in the binary representation of n is a 1.
Hence
fj,n(b1,...,bn) =

S∈(
{1,...,n}
2j )

i∈S
xi
where the sum runs over all subsets of {1,...,n} of size 2j.
One can avoid the use of Eq. (18.1) and use a more elementary approach. Just
draw the binomial coefﬁcients modulo 2 (Fig. 18.1). The pattern is obvious and can
easily be proved.
17.12 Use Wald’s sequential test (Algorithm 15.1).

18
Solutions
353
Let
li =

1 + 2ε
if zi = xi,
1 −2ε
if zi ̸= xi.
The test statistic is Ln = n
i=1 li. We choose the alternative H1 if the test statistic
becomes ≤A and we choose the hypothesis H0 if the test statistic becomes ≥B.
The thresholds are A =
0.001
1−0.01 ≈10−3 and B = 1−0.0001
0.01
≈102.
Now let ε = 0.1. Then EH1[lnli] = 1
2(ln1.2 + ln0.8) = −0.04.
Let τ be the stopping time for our test. We have
EH1[lnLτ] ≈0.999lnA + 0.001lnB = −6.88.
(The ≈sign would be an = if we could be sure that the bounds A and B are met
exactly.)
By the optimal sampling theorem (Theorem 15.3) we have
EH1[lnLτ] = EH1[τ]EH1[lnli]
and hence EH1[τ] = 172.15, i.e. we expect that we must observe 172.15 bits before
we can safely reject the hypothesis.
17.13 By deﬁnition f = (x ∧fh)∨(¯x ∧fl), and so on. The distributive law gives f ∧
g ∧h = (x ∧fh ∧gh ∧hh)∨(¯x ∧fl ∧gl ∧hl), which proves the recursion formula.
One can turn the recursion formula directly into a top down algorithm by adding
the starting point 0 ∧g ∧h = 0, and so on. All function calls must be memoized to
avoid duplicated work. In the simple Common Lisp implementation that I prepared
for this book, the real code looks almost like pseudo-code (see Algorithm 18.2).
Algorithm 18.2 Binary Decision Diagrams: The ternary-and operator
(defun BDD-ternary-and (x y z)
(cond ((eql x T) (BDD-and y z))
((eql x nil) nil)
((eql y T) (BDD-and x z))
((eql y nil) nil)
((eql z T) (BDD-and x y))
((eql z nil) nil)
((eql x y) (BDD-and x z))
((eql x z) (BDD-and y z))
((eql y z) (BDD-and x y))
(t (let ((m (min (V x) (V y) (V z))))
(BDD-make-node
m
(BDD-ternary-and (left x m) (left y m) (left z m))
(BDD-ternary-and (right x m) (right y m) (left z m)))))))
(memoize ’BDD-ternary-and)
Let f = (x1 ∧x2 ∧f1) ∨(x1 ∧x2 ∧f2), g = (x1 ∧x2 ∧g1) ∨(x1 ∧x2 ∧g2)
and h = (x1 ∧x2 ∧h1) ∨(x1 ∧x2 ∧h2) then f ∧g = (x1 ∧x2 ∧f1 ∧g1), f ∧h =

354
18
Solutions
(x1 ∧x2 ∧f2 ∧h1) and g ∧h = (x1 ∧x2 ∧g2 ∧h2). Thus for general f1, f2, g1,
g2, h1 and h2 the size of the BDD for the binary-and will grow quadratically with
the size of the input BDDs. But the ternary-and will reach, after two recursion steps,
the endpoints of the recursion that simpliﬁes to 0.
On the other hand, let f ′, g′ and h be three functions for which the size BDD
of f ′ ∧g′ ∧h is cubic in comparison to the input BDDs. Let f = f ′ ∧xn and
g = g′ ∧xn. Then computing f ∧g needs at most quadratic time and space. Since
f ∧g = 0, the time needed for the computation of (f ∧g)∧h is essentially the time
needed to compute f ∧g. Hence, in this case, two applications of the binary-and
algorithm are faster than the ternary-and.
17.14
(a) If f = 0 then f (y′) = 0, no matter which value y′ has.
(b) If g = 1 then by deﬁnition y′ = y and hence (f ↓1)(y) = f (y) for all y.
(c) If g ̸= 0 then (1 ↓g)(y) = 1(y′) = 1 for all y and (g ↓g)(y) = g(y′) = 1 (the
last equality follows from the deﬁnition of y′).
(d) If gh = 0 then for x = 0x2x3 ···xn and y = 1x2x3 ···xn we get x′ = y′ =
0x′
2 ···x′
n and hence f ↓g(x) = f ↓g(y) = f (x′) = fl(x′
2,...,x′
n) = fl ↓
gl(x2,...,xn).
Similarly we get f ↓g = fh ↓gh if gl = 0.
If gl ̸= 0 and gh ̸= 0 then the ﬁrst coordinate of y′ is always identical to the
ﬁrst coordinate of y and hence
f ↓g(y) = f

y′
= y′
1?fh

y′
2,...,y′
n

: fl

y′
2,...,y′
n

= y1?fh ↓gh(y2,...,yn) : fl ↓gl(y2,...,yn).
Turning this into a recursive algorithm is straightforward. The conditions (a)–
(c) are the endpoints of the recursion. Again we need memoization to make the
algorithm effective.
Algorithm 18.3 shows example code in Common Lisp which is almost as simple
as pseudo-code.
The constraint operator was introduced in [61].
17.15 As variables we choose the outputs x1,x2,x3,... of the LFSR in their natural
order. We need a BDD that describes the feedback function of the LFSR. Here we
can use essentially the same BDD as in the attack against E0, see Fig. 5.9 (a).
In addition, we need a BDD that describes the compression function. We need
two nodes to describe the next step for the case where the ﬁrst 2k bits of the LFSR
sequences produce j output bits (see Fig. 18.2). Whether the high or low pointer
of the node label xi is connected to ⊥depends on the value zj+1 of the (j + 1)-th
observed bit. Figure 18.2 shows the drawing for the case zj+1 = 1.

18
Solutions
355
Algorithm 18.3 Binary Decision Diagrams: The constrain operator
(defun BDD-constrain (f g)
(cond ((eql g nil) nil)
; note that we must check for g=0 first
((eql g T) f)
((eql f nil) nil)
((eql f T) T)
((eql f g) g)
(T (let ((m (min (V f) (V g)))
(fl (left f m))
(fh (right f m))
(gl (left f m))
(gh (right f m)))
(cond ((eql gl nil) (BDD-constrain fh gh))
((eql gh nil) (BDD-constrain fl gl))
(T (BDD-make-node
m
(BDD-constrain fl gl)
(BDD-constrain fh gh))))))))
(memoize ’BDD-constrain)
Fig. 18.2 Basic BDD for
attacking the self-shrinking
generator
Instead of building the BDD directly from Fig. 18.2 we can use melting to con-
struct it from more elementary BDDs. All we need is a BDD for the predicate
Pj,k(x1,...,xk) =

1
if exactly j of the k variables x1, ..., xk are true,
0
otherwise.
The BDD for Pj,k is very simple and I have added it as an elementary function
to my BDD C-library. Once we have the predicate Pj,k we can build the BDD for

356
18
Solutions
the expression

Pj,k(x1,...,x2k−1) ∧(x2k+1 = 1)
 
⇒(x2k+2 = zj+1)
by melting. The advantage of this approach is that it is not necessary to decide how
many output bits we want to use in advance. We can extend the BDD step by step.
Let us now estimate the size of the BDDs. When we use 2k bits, the size of
the BDD sketched in Fig. 18.2 is 2
k
2

, which is quite small. The size of the BDD
describing the LFSR grows as 22k (as long as 2k < n). So the size of the BDD that
describes the self-shrinking generator is bounded by k222k.
On the other hand, there are 22k possible bit sequences of length 2k, but only
3k of them are mapped by the self-shrinking compression to the observed output
sequence. So we estimate that only 2n 3k
22k of the 2n possible initial values of the
LFSR (the size of the LFSR is n) will lead to a sequence for which the ﬁrst k
steps are compatible with the observed output. This bounds the size of the BDD by
(2k)2n 3k
22k .
These two bounds meet for k ≈
n
4−log2 3. At this point the BDD reaches its largest
size, which is ≈20.82n.
The self-shrinking generator was introduced in [188]. A BDD-attack against the
self-shrinking generator was ﬁrst described in [166].
17.16 f (ˆx, ˆy) = g(ˆx, ˆy) = 0 means that the polynomials f (ˆx,y) and g(ˆx,y) have
a common zero ˆy. Hence Resy(f (ˆx,y),g(ˆx,y)) = 0. Thus Resy(f,g), which is a
polynomial in x, has the zero ˆx.
Thus by resultants one can reduce the problem of solving a system of non-linear
equations to the problem of ﬁnding zeros of a univariate polynomial.
Solving non-linear systems of equations by resultants is quite successful in com-
putational algebraic geometry, but in cryptography Gröbner bases or linearization
are normally more effective. Anyway, resultants are tool worth knowing about.
17.17 For every α ∈Nn the number of elements β ∈Nn with |β| ≤|α| is ﬁnite (to
be exact the number is
n+|α|−1
|α|

). Hence every descending chain in a graded order
must become stationary.
17.18 In this case the control and the output sequence are identical. Hence all zeros
will be deleted from the output sequence. The output of the shrinking generator will
be constant.
17.19 Let (si)i∈N be the sequence generated by the LFSR of the self-shrinking gen-
erator. The self-shrinking generator can be interpreted as a normal shrinking gen-
erator with the control sequence deﬁned by ci = s2i−1 and the generation sequence
deﬁned by xi = s2i.
Golic’s correlation attack (see Sect. 7.3.3) makes no assumption about the form
of the control and the generation sequence. So we can use it to reconstruct the se-
quence xi = s2i. Once the attacker knows every second bit of the LFSR sequence it
is not difﬁcult to invert the LFSR and reconstruct the seed.

18
Solutions
357
As shown in Sect. 7.3.3, the correlation decreases as
1
4√
i , so one can use only the
ﬁrst 10000 to 100000 bits before the loss of synchronization becomes relevant.
If the LFSR is large enough (128 bits or better 256 bits) the self-shrinking gen-
erator is still secure.
17.20 If j = i + 1 and S[j] = 1 then the following happens during one step of the
RC4 generator: i is increased by i. Now S[i] = S[j] = 1 and hence j is increased
by one. S[i] and S[j] are swapped. After the swap S[j] = 1.
Thus the condition j = i + 1 and S[j] = 1 is again satisﬁed and the only thing
that the RC4 generator does is to increase i and j by 1. Thus after n steps we will
be back at the same point where we started.
Since RC4 is invertible we cannot reach this cycle if we are not in it right from
the beginning. However, RC4 is initialized with i = j = 0, i.e. at the beginning we
are not in the cycle and hence it can never be reached.
This cycle was ﬁrst noted by Finny [91].
17.21 This is an open-ended problem. Here is just one possible answer:
The many subgroups of (Z/2Z)256 allow a lot of fortuitous states. For example,
if the ﬁrst 8 elements of S form a permutation of {0,...,7}, this partial S-box is
a fortuitous state of length 8. This already gives 8! fortuitous states of length 8.
Moving the region around further increases the number of fortuitous states. A high
number of fortuitous states increases the probability that an adversary can mount an
attack on them.
Another reason why Algorithm 17.2 is weak is that the more regular jumps of j
reduce the number of necessary guessing steps in a state recovering attack. Hence
state recovering attacks against Algorithm 17.2 will be faster.
17.22 Let Dn be the set of integers x in the range [1,..., n−1
2 ] with ( x
n) = 1. Let
f : Dn →Dn be the permutation that is deﬁned by
f (x) =

x2 mod n
if x2 mod n ≤n−1
2 ,
−x2 mod n
if n+1
2
≤x2 mod n ≤n −1.
The predicate Bn : Dn →{0,1} is
Bn(x) =

x mod 2
if x is a quadratic residue modulo n,
x + 1 mod 2
if x is a quadratic non-residue modulo n.
With these functions Algorithm 11.1 computes the Blum-Blum-Shub sequence.
Bn is hard, since it is the decision of the quadratic residue problem. You can
easily choose a random element in Dn, so the predicate is accessible. Thus all con-
ditions of Theorem 11.1 are satisﬁed.
17.23 Consider a 1-bit of x, by the rotations it is moved to every position, thus
the contribution of this bit to the whole sum is 31
i=0 = 2i = 232 −1. In the two’s
complement representation 232 −1 is identiﬁed with −1. So the sum 31
i=0(x ≪i)
adds −1 for every 1-bit of x, which is exactly the statement of Eq. (17.1).

358
18
Solutions
Computing SADD(x) by Eq. (17.1) needs 64 operations which is much slower
than any of the algorithms presented in Sect. 12.1.2.
17.24 Here is an example of what the single operations do:
x
1001101000
x −1
1001100111
x &(x −1)
1001100000
x −1 differs from x by changing the tailing zeros into ones and the preceding 1
into a 0. Hence x ←x &(x −1) removes the leftmost bit.
One can turn this into an algorithm for computing SADD(x) by successively
removing bits and counting how often this is possible.
Algorithm 18.4 Sideway addition for sparse words
1: s ←0
2: while x ̸= 0 do
3:
x ←x &(x −1)
4:
s ←s + 1
5: end while
6: return s
Algorithm 18.4 needs 4 operations per loop (including the comparison with 0)
and hence 4SADD(x) in total. It beats the algorithm from Sect. 12.1.2 if SADD(x)
is at most 3 or 4.
This method was developed by Wegner [279].
17.25
(a)
 a
bb′

=
 a
b
 a
b′

is part of the deﬁnition of the Jacobi symbol.
 aa′
b

=
 a
b
 a′
b

follows immediately from Corollary 13.11 and the deﬁnition of the Jacobi sym-
bol.
(b) Let b = pe1
1 ···pek
k . Since for each prime factor pi of b the two numbers a and
a mod b are in the same equivalence class modulo pi and
 a
pi

=
 a mod b
pi

by
the deﬁnition of the Legendre symbol. Hence by deﬁnition of the Jacobi symbol
a
b

=
 a
p1
e1
···
 a
pn
en
=
a mod b
p1
e1
···
 a
pn
en
=
a mod b
b

.
(c) We prove the result by induction on a and b. If a and b are primes this is the
statement of the quadratic reciprocity law (Theorem 13.12).

18
Solutions
359
If a ≡a′a′′, with a′,a′′ ≥3 then
a
b

=
a′
b
a′′
b


by (a)

= (−1)(a′−1)(b−1)/4
 b
a′

(−1)(a′′−1)(b−1)/4
 b
a′′

(by induction)
= (−1)(a−1)(b−1)/4
b
a


by (a) and checking all 8 cases for a′, a′′, b modulo 4.

Exactly the same argument handles the case b = b′b′′.
(d) Let b = pe1
1 ···pek
k qd1
1 ···q
dj
j
be the factorization of b with pi ± 1 mod 8 for
i = 1,...,k and qi ≡±3 mod 8 for i = 1,...,j.
Then by deﬁnition of the Jacobi symbol and Theorem 13.12 we obtain:
2
b

=
 2
p1
e1
···
 2
pk
ek 2
q1
d1
···
 2
qj
dj
= 1e1 ···1ek(−1)d1 ···(−1)dj
= (−1)d1+···+dk.
But b ≡±3 mod 8 if and only if it has an odd number of factors of the form
±3 mod 8, which proves (d).
(e) By deﬁnition
 1
b

= 1 for all b. This is what Algorithm 13.1 does in line 2. If
a = 2ka′ with a′ odd then by (a), we obtain
a
b

=
2
b
ka′
b

.
By (d)
 2
b

= −1 if and only if b ≡±3 mod 8. Thus the condition in line 6
holds if
 2
b
k = −1.
If a is odd, Algorithm 13.1 uses (c) to express
 a
b

in terms of
 b
a

.
 b
a

is
immediately reduced to
 b mod a
a

by (b). There are cases where b mod a = 0
and hence
 b
a

=
 0
a

= 0. These cases are caught by lines 13–15. If b ≡a ≡3
mod 4 then (−1)(a−1)(b−1)/4 = −1 and by (c)
 a
b

= −
 b
a

(line 17) or ﬁnally
(−1)(a−1)(b−1)/4 = 1 and by (c)
 a
b

=
 b
a

(line 19).
In the recursive step Algorithm 13.1 reduces the size of a and b. So the algo-
rithm must ﬁnally stop at line 2 if gcd(a,b) = 1 or at line 14 if gcd(a,b) > 1.
As in the Euclidean algorithm, the worst case is if a and b are consecutive
Fibonacci numbers. Thus the runtime is bounded by log(a)/log((
√
5 + 1)/2).

360
18
Solutions
17.26 Consider the lattice with basis
⎛
⎜⎜⎜⎝
1
0
0
...
...
0
1
0
a0
···
ak−1
s
⎞
⎟⎟⎟⎠.
If s = k−1
i=0 aixi then v = (x0,...,xk−1,0)t is an element of the lattice. The
length of v is at most
√
k which is very short in comparison to the basis vectors
used to deﬁne the lattice, which are all of length about 2k.
Thus v is (most likely) the ﬁrst element of the LLL-base.
Applying the LLL algorithm to the given weights ai and s = 12938 in the exer-
cise yields vmin = (1,0,0,0,1,0,0,0,1,0,0)t as ﬁrst vector in the LLL-basis, i.e.
x = 273 or KN.
The second number in the message s = 14196 gives the vector vmin =
(1,1,1,1,0,0,0,0,0,0,0)t
The third number s = 16432 is a bit tricky. We obtain
vmin = (1,0,0,−1,0,0,0,0,0,0,2)t
as the minimal element in the LLL-basis, but the second element in the basis is
v = (0,0,1,0,1,0,1,0,1,1,1,0,0)t, which is the desired solution of the knapsack
problem. This is a heuristic approach, there is no guarantee that it will always work.
Finally s = 18620 gives us v = (0,1,1,1,1,1,0,0,0,0,0)t as the second ele-
ment in the LLL-basis.
The message is KN AP SA CK.
This attack is only the beginning. More elaborate versions (see [200, 243]) use
the LLL-algorithm to construct a c′ and m′ such that the sequence b′
i = aic−1
mod m′ satisﬁes b′
i+1 ≥2b′
i. This allows the attacker to decode the ciphertext with-
out always running the relatively slow LLL-algorithm. Nevertheless, our basic at-
tack shows the danger that the LLL-algorithm provides for most knapsack-based
cryptosystems.
Answer to the bonus question:
The toy example fails against a guess and check attack. If the attacker is able
to guess a part of the plaintext, he can easily check his guess was correct. This is
normally something you want to prevent.
A way to deal with this problem is to add some random bits as padding, but the
padding expands the cipher’s ciphertext which is often unacceptable. This is another
reason why asymmetric ciphers play almost no role in data encryption.
17.27 Let m = ⌈√n⌉. Then p = m + ϵ1 and q = m −ϵ2 with 0 ≤ϵ1,ϵ2 ≤
4√n.
The equation n = pq is equivalent to
n = m2 + m(ϵ1 −ϵ2) −ϵ1ϵ2

18
Solutions
361
or
ϵ1 −ϵ2 = n −m2 + ϵ1ϵ2
m
=
3n −m2
m
4
where the last equation holds since ϵ1ϵ2 ≤m.
Once we know δ = ϵ1 −ϵ2 we just solve the quadric equation n = (m + ϵ1)(m −
ϵ1 + δ) to obtain ϵ1 and factor n.
(Note: This exercise is a special case of Coppersmith’s method to factor n = pq
if at least the upper half bits of p are known [57].)
17.28 Let F be a skew ﬁeld over Fp of order n and let C be the centralizer of F .
The class formula states that
|F| = |C| +

x
C(x)

where the sum runs over all classes. The size of the class C(x) is pn−1
pnx −1 where pnx
is the size of the stabilizer of x. Hence we have:
pn −1 = pc −1 +

x
pn −1
pnx −1.
(18.2)
By Zsigmondy’s theorem either n = 2 or n = 6 and p = 2 or there exists a prime
q that divides pn −1, but not pk −1 for k < n. In the small cases we can check
directly that F must be a commutative ﬁeld.
So assume that there exist a Zsigmondy prime q such that q divides the left-hand
side of Eq. (18.2) and all terms in the sum on the right-hand side. Then it must also
divide the last term pc −1. By the property of a Zsigmondy prime we conclude that
c = n and hence F is a commutative ﬁeld.
This is one of Wedderburn’s original proofs.
17.29 This is just a calculus problem related to Theorem 15.1.
Both distributions have a Lebesgue density. The density of N(0,1) is ϕ1(x) =
1
√
2π e−x2/2 and the density of N(0,2) is ϕ2(x) =
1
2√π e−x2/4 (see Fig. 18.3).
The graphs of ϕ1 and ϕ2 intersect at x0 =
(
4ln
√
2 ≈1.17 and −x0. Hence by
Theorem 15.1
22N(0,1) −N(0,2)
22 = 1
2
# ∞
−∞
ϕ1(x) −ϕ2(x)
dx
= 1
2
# x0
−x0
ϕ1(x) −ϕ2(x)dx +
# x0
−∞
ϕ2(x) −ϕ1(x)dx
=
# x0
0
ϕ1(x) −ϕ2(x)dx +
# x0
−∞
ϕ2(x) −ϕ1(x)dx.
Since the antiderivative of e−x2 has no closed form the integrals must be evalu-
ated numerically. We reduce the problem to calculating the quantiles of the standard

362
18
Solutions
Fig. 18.3 The densities of N(0,1) and N(0,2)
normal distribution
Φ(x) =
# x
−∞
1
√
2π
ex2/2.
The function Φ is tabulated.
22N(0,1) −N(0,2)
22 = Φ(x0) −Φ(x0/
√
2) + Φ(−x0/
√
2) −Φ(−x0)
= 2Φ(X0) −2Φ(x0/
√
2)
≈0.1646.
The maximal success probability for distinguishing an N(0,1) and an N(0,2)
distribution is therefore 0.1646.

Part V
Programs

Chapter 19
An Overview of the Programs
For the reader’s convenience I have implemented most of the algorithms described
in this book. Most programs are in C++, but I also use Common Lisp and C where it
seems appropriate. I do not claim that these programming languages are the best for
the given problem. I just like C and C++ as near-machine languages and Common
Lisp is my favorite language for high level abstraction.
You can download the programs for free at http://cage.ugent.be/~klein/
streamcipher. The reader may use and modify the programs as he wishes. You may
use small code snippets without giving a reference, but if you distribute modiﬁed
versions of the programs you should cite this book and allow the user to modify
your modiﬁcations.
The programs are planned for research experiments and not for use in high secu-
rity settings, so most of the work was focused on clear and simple code; I did not
strengthen the implementation against side channel attacks.
Special attention was given to extensive documentation. For the interface of the
C++ libraries I use the tool Doxygen [125]. It can generate interface documentation
in different formats (HTML, LATEX (PDF), Unix-Manpages). Figure 19.1 shows as
an example a part of the documentation of the LFSR library in an HTML-browser.
For the documentation of the implementation of the algorithms I developed a
new literate programming tool which is described in Chap. 20. Figure 19.2 shows as
an example a page from the implementation of the Berlekamp-Massey algorithm.
Here follows a list of all programs. Unless otherwise stated, the programs are
written in C++ and run on all platforms.
Chapter 2
• LFSR.hpp and LFSR.web: A library for LFSR. It is optimized for convenience not
for speed.
• linear_complexity.hpp and linear_complexity.web: An implementation of the
Berlekamp-Massey algorithm and the asymptotic fast variant described in
Sect. 2.4.2.
• LFSR-test.web: A C-Program which implements the different algorithms for sim-
ulating LFSRs in software, which are described in Sect. 2.6.2. This program con-
A. Klein, Stream Ciphers, DOI 10.1007/978-1-4471-5079-4_19,
© Springer-Verlag London 2013
365

366
19
An Overview of the Programs
Fig. 19.1 An example of the Doxygen documentation
Fig. 19.2 An example of the
pweb documentation

19
An Overview of the Programs
367
tains a small piece of assembler code (see Algorithm 2.4). Thus it needs an ix86
platform and the GNU gcc.
Chapter 3
• Geffe.hpp and Geffe.web: An implementation of the Geffe generator.
Chapter 4
• preprocessing2.web: There are different programs for searching relations (see
Sect. 4.1.3). The reason is that, even if many parts of the programs are equal,
each possible weight has its specialities. This program is specialized for relations
of weight 2. It can also search for extended relations as described in Sect. 4.1.4.
All pre-processing programs have no input routines. You must enter the infor-
mation about the LFSR that should be attacked into the source code.
• preprocessing3.web: This is the program for relations of weight 3. It implements
the trick described in Sect. 4.1.3 to speed up the program by a factor of 3 in
comparison to the generic algorithm.
• preprocessing4.web: This is the program for relations of weight 4. As explained
in Sect. 4.1.3, for this case it is better to use a sorted list instead of a hash table as
the main data structure.
It implements the trick described in Sect. 4.1.3 to speed up the program by a
factor of 2 in comparison to the generic algorithm.
• preprocessing5.web: This is the program for relations of weight 5. It implements
the trick described in Sect. 4.1.3 to speed up the program by a factor of about 8
in comparison to the generic algorithm.
• text2bin.web and bin2text.web: To keep the output platform independent, the pre-
processing program writes the relation set in a simple text format. If you want
you can convert the relation set to a binary format. The binary format is faster and
needs less space on the hard disk, but it is no longer portable between different
architectures.
• attack-CJS-simple-count.web and attack-CJS-Fourier.web: These programs im-
plement the basic version of the CJS-attack. They differ in the method used
to evaluate the relations. The ﬁrst program uses the sideway addition in arrays
(see Sect. 4.1.6.1) and the second program uses the fast Fourier transform (see
Sect. 4.1.6.2).
Compare the speed of the two programs. In general the ﬁrst is better for cases
with few relations of low weight and the second is better for many relations of
high weight.
• attack-CJS-extended-rel.web: A version of the CJS attack that uses extended re-
lations (see Sect. 4.1.4).
• attack-Viterbi.web: An attack based on convolutional codes and Viterbi decoding
(see Sect. 4.2.3.1).
• attack-sparse.web: An implementation of the attack of W. Meier and O. Staffel-
bach against an LFSR with a sparse feedback polynomial (see Sect. 4.3).

368
19
An Overview of the Programs
Chapter 5
• A strange feature of the programming language C is that it has many functions
for dealing with ﬂoating point numbers, but almost no functions for dealing with
integers. Even the function max with integer arguments is missing. Since I work
a lot with integers, I decided to solve the problem once and for all.
inttools.h and inttools.c provide a full featured C library for dealing with inte-
gers.
• BDD.h and BDD.web: A full BDD library written in C. It is optimized for com-
pact memory and efﬁcient use of the cache.
To test the library I have prepared the following programs.
queen.web: A program solving the n-queen problem. Note that BDDs are not
the optimal way to solve the n-queen problem, but it is a good stress test and can
be used to compare different BDD libraries.
BDDcalc.web A reverse Polish notation style calculator for BDDs.
• simpleBDD.web: A simple BDD library written in Common Lisp. It has almost
no memory management and it is intended to be used to quickly test new ideas.
• E0.hpp and E0.web: A simpliﬁed implementation of the cipher E0 as described
in Sect. 5.2.1.
• E0-attack.web: A BDD-based attack against E0, as described in Sect. 5.2.2.
Chapter 6
• LILI.sing: An input ﬁle for the computer algebra system Singular, which does the
degree reduction for LILI-128 as described in Sect. 6.3.1.
• E0.sing: An input ﬁle for Singular, which does the pre-processing for the alge-
braic attack against E0 as described in Sect. 6.3.2.
Chapter 7
• StopAndGo.hpp and StopAndGo.web: An implementation of the stop-and-go gen-
erator (see Fig. 7.1).
• StepOnceTwice.hpp and StepOnceTwice.web: An implementation of the step-
once-twice generator (see Sect. 7.1).
• AlternatingStep.hpp and AlternatingStep.web: An implementation of the alternat-
ing step generator (Algorithm 7.1).
• Shrinking.hpp and Shrinking.web: A simple implementation of the shrinking gen-
erator (see Sect. 7.3.1). It takes no precaution to avoid side channel attacks. The
program timing_attack.web demonstrates a simple timing attack on this example.
For measurement it uses the ix86 instruction RDTSC. Thus this program is not
platform independent.
Chapter 8
• A5-1.web and A5-2.web: Simulation of the A5 algorithms in software.
• A5-1-special.web: Generating special states of A5/1 as described by Algo-
rithm 8.3.
• A5-1-invert.web: The fact that A5/1 can be effectively inverted is crucial for the
attack described in Sect. 8.3.2. This program demonstrates this part of the attack.

19
An Overview of the Programs
369
Chapter 9
• RC4.hpp and RC4.web: An implementation of RC4. The implementation contains
a lot of checks against fault attacks and trades speed against security.
• digraph-simple.web: A direct translation of the pseudo-code of Algorithm 9.10
into Common Lisp.
• fortuitous-state.web: A highly optimized program to search fortuitous states that
was used to create Table 9.3.
• RC4-FMS.web: A demonstration of the FMS-attack (see Sect. 9.4).
• RC4-attack.web: A demonstration of the attack based on Goli´c’s relation (see
Sect. 9.5).
Chapter 10
• trivium4.hpp and trivium.web: An implementation of the cipher Trivium. The
program favors clarity over speed.
• rabbit.hpp and rabbit.web: An implementation of the cipher Rabbit. The program
favors clarity over speed.
• moustique.hpp and moustique.web the cipher Moustique.
Chapter 12
• popcnt.web: A C implementation of different algorithms for the sideway addition
(population count) problem.
• EK-bitset.tar.gz: An optimized C++ bitset library similar to the standard bitset
class.
• matrix-multiplication.web: The matrix multiplication algorithms described in
Sect. 12.5.1.
Chapter 14
• irrpoly.gap: A GAP program that generates a random irreducible polynomial with
Theorem 14.2.
• primelt.gap: A GAP program that generates a random primitive with Algo-
rithm 14.1.
Chapter 20
• pweb.tar.gz: The pweb system used to document the programs described in
Chap. 20.
To make use of all programs you must install the following software on your
system. The makeﬁles assumes that everything is stored in standard places. If you
have installed the software in unusual places you will have to add appropriate path
information.
• A working make (for example [174]).
• A working LATEX environment (for example [264]).
• A implementation of the M4 macro processor (for example [238]).
• The pweb system (Chap. 20).
• The optimized C++ bitset library EK-bitset (see above).

370
19
An Overview of the Programs
• A C-Compiler that supports (at least partially) the C99 standard. The GNU C
Compiler will work [102].
• A C++ compiler (for example [102]).
• The Boost C++ Libraries [35].
• The curses library for text based interfaces (for example [65]).
• The Doxygen code documentation tool [125].
• V. Shoup’s NTL: A Library for doing Number Theory [244].
• The GNU Multiple Precision Arithmetic Library (needed by NTL) [105].
• A Common Lisp environment. I have tested all Lisp programs with clisp [54] and
sbcl [233], but other Common Lisp implementations should work without any
difﬁculties.
• The computer algebra system Singular [253].
• The computer algebra system GAP [99].
For all these components there exist freely available implementations for all rel-
evant platforms.

Chapter 20
Literate Programming
20.1 Introduction to Literate Programming
Most people working in mathematics know D.E. Knuth’s TEX typesetting system.
Less well-known is that as a companion to TEX Knuth developed a new program-
ming style called literate programming (see [158]). The idea is that a program should
read like an article.
A literate programming environment, also called a web-system, consists of two
programs called tangle and weave. Figure 20.1 illustrates the work ﬂow in a
literate programming environment.
The tangle program extracts the source code of the web-ﬁle and generates an
input ﬁle for the compiler (interpreter) of our programming language. It must resort
the code from the human readable form into the machine readable form.
The weave program generates the input for a text processor. It must apply syntax
highlighting to the code and generate an index, etc.
In principle you can use any text processing tool and any computer language
as a basis for a literate programming environment. In practice the weave and tangle
program need some information about the destination language, so a speciﬁc literate
programming environment supports only a few languages.
20.2 Pweb Design Goals
When I started this project, I found that no literate programming tool satisﬁes my
exact needs. So I began with the development of a new literate programming tool. I
called it pweb, since its main part comprises a bunch of Perl scripts.
I designed pweb with the following goals in mind:
• It should be easy to add support for additional programming languages. To guar-
antee this, pweb should only use regular expressions and not a complete parser.
I was willing to scarify advanced formatting functions, such as auto-detection of
variables, for this goal.
A. Klein, Stream Ciphers, DOI 10.1007/978-1-4471-5079-4_20,
© Springer-Verlag London 2013
371

372
20
Literate Programming
Fig. 20.1 The literate programming environment
• Not every language has such a good pre-processor as LISP. Therefore pweb
should provide an extra pre-processor. I decided to use the macro processor M4
for that purpose.
• It should be possible to have (in the documentation) variable names like α, x′, ˆx.
20.3 Pweb Manual
20.3.1 Structure of a WEB-Document
All web commands start with an @ symbol. The ﬁrst line of a pweb-document must
be of the form @<programing language>2<formating language>.
For example a C++ Program that is documented in LATEX starts with @cpp2latex.
The next lines up to the ﬁrst @* command form the limbo section. The contents
of the limbo section are copied directly into the header of the documentation (the
part between \documentclass{article} and \begin{document} if you
use LATEX or the part between <head> and </head> if you use HTML).
If you use LATEX as a formatting language the limbo section must deﬁne the title
and the author.
The remaining part of the web-document splits into text sections, code sections,
named modules and macros. A text section must precede each section of another
type.
20.3.2 Text Sections
The ﬁrst line of a text section has the form
@*n <title of the section>.
where n ∈N. This deﬁnes a section of depth n with the given title. pweave trans-
forms it to the LATEX commands \section, \subsection and so on or the

20.3
Pweb Manual
373
HTML tags <H1>...</H1>. It depends on the chosen formatting language how
many substructures exist.
@* is a short form for the highest level @*0.
Inside a text section you can use all commands from the chosen formatting lan-
guage (like \emph{...} in LATEX or <em>...</em> in HTML). Of course,
once you start using these commands you cannot change the formatting language.
20.3.3 Code Sections and Modules
A code section starts either with @a or @c and ends at the next text section. Both
commands are equivalent and the only reason for having two commands is the
smooth transition from other literate programming environments (cweb uses @c
and fweb uses @a). I recommend to use only @c in new pweb projects.
The effect of a code section is that ptangle copies that code directly into the
source code of the program.
In addition to code sections fweb uses modules. A module starts with the line
@<Module name@>= and ends at the next text section.
You can insert a module in any code section or any other module by
@<Module name@>.
It is not necessary to deﬁne a module before its ﬁrst use. You can deﬁne it after-
wards.
A typical use of a module is an initialization code like:
@* Initialize variables.
Explain what has to be done
@<init@>=
set some variables ...
@* The main Program.
Explain the program.
@c
int main() {
@<init@>
Do something ...
}
The presence of modules changes the programming style. It is no longer neces-
sary to use functions for structuring the program. You should only deﬁne a function
if it really is a function; for structuring the code, modules do a better job.
In addition you can split the code into smaller parts than is normally possible in a
programming language. D.E. Knuth [150] recommends that a single section should
contain no more than 12 lines of code. If you ever ﬁnd yourself writing larger code
sections, you should consider deﬁning a new module to structure the code.

374
20
Literate Programming
20.3.4 Macros
Macros are a useful technique for doing computations at compile time, but the sup-
port of macros is very different between programming languages. For example,
Common Lisp has a perfect macro support, where macros can be written in almost
the same syntax as the rest of the programming language. C has limited macro sup-
port (no recursion in macros) and the macro language differs completely from the
programming language. Java has no macros at all.
pweb offers an interface to the M4 macro processor that can be used in addition
to the macros provided by the chosen programming language.
A simple macro has the form
@define MACRO-NAME SUBSTITUTION
and the effect is that in the program MACRO-NAME is replaced by SUBSTITUTION.
This corresponds to the C-Macro
#define MACRO-NAME SUBSTITUTION
More complex macros must be deﬁned in a special section.
The macro section must start with @m and end at the beginning of the next text
section. Inside a macro section you can use the entire syntax of the M4 macro lan-
guage. In particular, you have conditions and recursion. Here is a small example of
how to deﬁne a macro for the factorial function.
@m
define(‘FACTORIAL’,‘ifelse($1,1,1,
eval($1*‘FACTORIAL(eval($1-1)’)))’)
A complete description of M4 can be found in [238].
A side-effect of using macros is that a single line of the fweb document can
expand to multiple lines in the programming language. This tweaks the line num-
bering and makes it difﬁcult to interpret a later error message which refers to the
wrong line number. To help you fweb contains a special command @l which resets
the line numbering.
You should always use long macros like this:
@c
some commands
A_BIG_LONG_MACRO @l
some more commands
The @l command only works with programming languages that support redirec-
tion of error messages like C and C++ do with #line macro. In other languages
the @l command will be ignored.

20.3
Pweb Manual
375
20.3.5 Special Variable Names
Most programming languages require that a variable name must start with a letter
and allow only letters, digits and the underscore sign (_) afterwards. However, in
mathematics we often use variables like α, x′ or ˆx. Fweb pays attention to this by
providing special commands for such variables.
With the command @alpha you deﬁne a variable (or to be more precise, a code
fragment) named alpha in the source code and printed α in the documentation. All
Greek letters are supported by this facility.
With x@’ you deﬁne a variable x′. It becomes x_prime or xprime in the
programming language. Similarly you can deﬁne variables ˆx, ˜x by x@hat and
x@tilde, respectively.
Combinations like @beta@’ for β′ are possible.
20.3.6 Include Files
You can use @include <filename> to include other pweb documents in your
document. The include mechanism is similar to the one used by C and C++.
• If the include-ﬁle has a preamble it will be ignored.
• Each include-ﬁle must start with a text section and it can contain only complete
sections.
• The @include command must stand alone in a line.
20.3.7 Conditional Compilation
In addition to the conditional compilation features that may be provided by the un-
derlying programming languages, pweb provides an extra mechanism.
Code between @tangle off and @tangle on is ignored by ptangle. The
commands @tangle off and @tangle on must stand alone in a line and are
not nestable. Typically one uses them to give a short example code in the documen-
tation.
@* How to use the new functions.
Some explanation ...
@tangle off
@c
some lines of example code ...
@tangle on
Similarly pweave ignores the code between @weave off and @weave on.
One can use these commands to hide some deﬁnitions in the documentation. Since

376
20
Literate Programming
the purpose of literate programming is to provide full documentation, one should
only rarely use @weave off and @weave on. An exception is that it can be a
good idea to use code like
@weave off
@include somefile.web
@weave on
and run pweave directly on somefile.web. This gives two separate articles,
one for the main program and one for the include-ﬁle.
In addition, fweb has comments. Everything from @% until the end of a line will
be ignored by fweb.
20.3.8 More pweb Commands
Inside a text section you can insert small code examples by |code example|.
Since @ and | have a special meaning in pweb documents you must escape them
if you want use them in the text. Write @@ for a literal @ and @| for a literal |.
20.3.9 Compatibility Features
To improve the compatibility with other literate programming systems pweb ig-
nores the following commands:
• @; used by fweb as an invisible semicolon.
• @\ used by fweb as a forced line break.
• @~ used by fweb to inhibit a line break.
You should not use any of these commands in a new pweb project.
20.3.10 Common Errors
Typical errors when typing a pweb document are:
• The title of a text section must end with a dot! If you forget the dot at the end of
the line the pweave formation goes wrong.
• If you forget to deﬁne a module ptangle will wait on the command line until
you type the module. No error message is displayed! Stop the program by typing
Control-d and add the module deﬁnition to your ﬁle.

20.3
Pweb Manual
377
20.3.11 Editing pweb Documents
You can use any text editor to create your pweb documents, but the correct syntax
highlighting can be problem. If you use Emacs for editing you can use the mmm-
mode from M.A. Shulman [246] to patch together the syntax highlighting for LATEX,
C and M4. Install the MMM Mode and add the following code to your .emacs.
(mmm-add-group
’web
’((web1
:submode c-mode
:face mmm-code-submode-face
:delimiter-mode nil
:front "@<[^>]*@>="
:back
"@[* ]"
)
(web2
:submode c-mode
:face mmm-code-submode-face
:delimiter-mode nil
:front "@[ac]"
:back "@[* ]" )
(web3
:submode m4-mode
:face mmm-code-submode-face
:delimiter-mode nil
:front "@[m]"
:back "@[* ]" )
))
(setq mmm-global-mode ’maybe)
(mmm-add-mode-ext-class ’latex-mode "\\.web\\’" ’web)
(add-to-list ’auto-mode-alist ’("\\.web\\’" . latex-mode))
20.3.12 Extending pweb
The formatting of pweb is driven by a bunch of Perl scripts. For each programming
language lang and each formatting language text you need the following ﬁles
listed in Table 20.1.

378
20
Literate Programming
Table 20.1 Files needed by pweb
begin_lang2text.pl
Prints the text at the beginning of the formatted document (like
\documentclass{article} if you use LATEX).
middle_lang2text.pl
Prints whatever comes after the limbo section (if you use
HTML it would most likely be </head><body>).
end_lang2text.pl
Prints the text that closes the formatted document (like
\end{document} if you use LATEX).
formater_lang2text
Converts the source code of the programming language in
printable code. A typical entry of formater_cpp2tex is
s/(?<![a-zA-Z0-9_])\b(asm|auto|bool|
...|while)\b/\{\\rm \\bf $1\}/g;
This regular expression prints the keywords in boldface.
formater_m4lang2text
Like formater_lang2text but treats the macro sections.
codestart_lang2text
Contains things that have to be printed at the start of a code
section.
codeend_lang2tex
Same as before but for the end of the code section.
codestart_m4lang2text
Contains things that have to be printed at the start of a macro
section.
codeend_m4lang2text
End of a macro section.
In principle you can add any combination of programming languages and for-
matting languages.

Notations
x ≡y mod n
x ≡y mod m ⇐⇒m|(x −y) (mod as equivalence relation)
x = y mod n
x is the unique integer in the interval [0,m −1] with x ≡
y mod m (mod as function)
N
the set of non-negative integers
N+
the set of positive integers
Z
the set of integers
R
the set of real numbers
C
the set of complex numbers
ℜ(x)
the real part of the complex number x
Fq
the ﬁnite ﬁeld with q elements (q is a prime power)
F[z]
the polynomial ring over the ﬁeld F in the variable z
F[[z]]
the ring of formal power series over the ﬁeld F
F((z))
the Laurent series over F
TrF ′/F
the trace function in the ﬁeld extension F ′/F
Sn
the symmetric group on n elements
≺
a monomial ordering
≺lex
the lexicographic order
≺grlex
the graded lexicographic order
≺grevlex
the graded reverse lexicographic order
≺M
the matrix order deﬁned by M
a ∧b
logic AND
a ∨b
logic OR
¬a,a
logic NOT
a&b
bitwise AND operation ((1011)2&(1110)2 = (1010)2)
a |b
bitwise OR operation ((1001)2|(1100)2 = (1101)2)
a ⊕b
bitwise XOR operation ((1001)2|(1100)2 = (0101)2)
¯a
bitwise complement, a = (1001)2 =⇒¯a = (0110)2
a ≫k
right shift of k bits (11010100)2 ≫3 = (00011010)2
a ≪k
left shift of k bits (11010100)2 ≪3 = (10100000)2
a ≫k or RotR(a,k) right rotation of k bits (11000101)2 ≫1 = (11100010)2
a ≪k or RotL(a,k) left rotation of k bits (11000101)2 ≪1 = (10001011)2
A. Klein, Stream Ciphers, DOI 10.1007/978-1-4471-5079-4,
© Springer-Verlag London 2013
379

380
Notations
μk
the inﬁnite 2-adic fraction −1/(22k + 1)
SADD(w)
sideway addition SADD(w) = w0 + ··· + wn
⌊x⌋
the greatest integer smaller than or equal to a ∈R
⌈x⌉
the smallest integer greater than or equal to a ∈R
x
m

the binomial coefﬁcient x(x−1)···(x−m+1)
m!
, x ∈R, m ∈N
gcd(a,...,ak)
the greatest common divisor of a1,...,ak
lcm(a1,...,ak)
the least common multiple of a1,...,ak
 a
b

the Jacobi symbol (see Deﬁnition 13.3)
ϕ(n)
Eulerian ϕ-function, ϕ(n) = |{a | gcd(a,n) = 1,1 ≤a ≤n}|
λ(n)
the Carmichael function λ(n) = min{e|ae ≡1 mod n for all e
with gcd(e,n) = 1}
O(g(n))
f (n) = O(g(n)) ⇐⇒limsupn→∞
f (n)
g(n) < ∞
O∼(g(n))
f (n) = O∼(g(n))
⇐⇒
f (n) = O(g(n)logk g(n)) for
some k ∈N
o(g(n))
f (n) = o(g(n)) ⇐⇒limn→∞
f (n)
g(n) = 0
Ω(g(n))
f (n) = Ω(g(n)) ⇐⇒limsupn→∞
g(n)
f (n) = 0
ω(g(n))
f (n) = ω(g(n)) ⇐⇒limn→∞
g(n)
f (n) = 0
Θ(g(n))
f (n) = Θ(g(n)) ⇐⇒f (n) = O(g(n)) and f (n) = Ω(g(n))
f (n) ∼g(n)
f (n) = Θ(g(n)) ⇐⇒limn→∞
f (n)
g(n) = 1
(Ω,F ,P)
a probability space on Ω with σ-algebra F and probability
measure P
iid.
identically and independently distributed (random variables)
N(μ,σ 2)
the normal distribution with mean μ and variance σ 2
Φ(x)
the distribution function of the standard normal distribution,
Φ(x) =
 x
−∞
1
√
2π e−u2/2du
[Wt : t ≥0]
a Brownian motion (Wiener process)
ν ≪μ
the measure ν is absolutely continuous with respect to μ,
μ(A) = 0 =⇒ν(A) = 0

References
1. van Aardenne-Ehrenfest, T., de Bruijn, N.G.: Circuits and trees in oriented linear graphs.
Bull. Belg. Math. Soc. Simon Stevin 28, 203–217 (1951) (p. 61)
2. Aciiçmetz, O., Koç, Ç.K., Seifert, J.P.: Predicting secret keys via branch prediction. In:
Abe, M. (ed.) CT-RSA. LNCS, vol. 4377, pp. 225–242 (2007) (p. 246)
3. Agrawal, M., Kayal, N., Saxena, N.: PRIMES is in P . Ann. Math. 2, 781–793 (2002) (p. 251)
4. Aircrack-ng a toolsuite. http://www.aircrack-ng.org (pp. 185, 207)
5. Alexi, W., Chor, B., Goldreich, O., Schnorr, C.P.: RSA and Rabin functions: certain parts are
as hard as the whole. SIAM J. Comput. 17, 194–200 (1988) (p. 251)
6. Amrhein, B., Gloor, O., Küchlin, W.: On the walk. Theor. Comput. Sci. 187, 179–202 (1997)
(pp. 140, 142)
7. Arbaugh, W.A.: An inductive chosen plaintext attack against WEP/WEP2 (2001). http://
www.cs.umd.edu/~waa/attack/v3dcmnt.htm (p. 189)
8. Armknecht, F., Krause, M.: Algebraic attacks on combiners with memory. In: Proceedings
of Crypto 2003. LNCS, vol. 2729, pp. 162–176. Springer, Berlin (2003) (pp. 149, 151)
9. Ars, G., Faugère, J.C., Imai, H., Kawazoe, M., Sugita, M.: Comparison between XL and
Gröbner basis algorithms. In: Advances in Cryptology—ASIACRYPT 2004. LNCS, vol.
3329, pp. 338–353. Springer, Berlin (2004) (p. 147)
10. Atkin, A.O.L., Bernstein, D.J.: Prime sieves using binary quadratic forms. Math. Comput.
73, 1023–1030 (2004) (p. 251)
11. Aumasson, J.P.: On a bias of Rabbit. http://www.ecrypt.eu.org/stream/papersdir/2006/058.pdf
(p. 235)
12. Bach, E.: Improved asymptotic formulas for counting correlation-immune Boolean func-
tions. Technical report 1616, Computer Science Dept., University of Wisconsin (2007) (p. 81)
13. Bailey, D.H., Lee, K., Simon, H.D.: Using Strassen’s algorithm to accelerate the solution of
linear systems. J. Supercomput. 4(4), 357–371 (1990) (p. 281)
14. Barkan, E., Biham, E.: Conditional estimators: an effective attack on A5/1. In: Selected Areas
in Cryptography. Lecture Notes in Comput. Sci., vol. 3897, pp. 1–19. Springer, Berlin (2006)
(p. 176)
15. Barkan, E., Biham, E., Keller, N.: Instant ciphertext-only cryptanalysis of GSM encrypted
communication. In: Advances in Cryptology—CRYPTO 2003. LNCS, vol. 2729, pp. 600–
616. Springer, Berlin (2003) (p. 172)
16. Bauer, F.L.: Kryptologie, Methoden und Maximen, 2 Auﬂage. Springer, Berlin (1994) (p. 8)
17. Baumert, L.D.: Cyclic Difference Sets. LNM, vol. 182. Springer, Berlin (1971) (p. 29)
18. Baur, W., Strassen, V.: On the complexity of partial derivatives. Theor. Comput. Sci. 22,
317–330 (1983) (p. 291)
A. Klein, Stream Ciphers, DOI 10.1007/978-1-4471-5079-4,
© Springer-Verlag London 2013
381

382
References
19. Berbain, C., Gilbert, H., Patarin, J.: Quad: a practical stream cipher with provable security.
In: Vaudenay, S. (ed.) EUROCRYPT. Lecture Notes in Computer Science, vol. 4004, pp.
109–128. Springer, Berlin (2006) (p. 255)
20. Berlekamp, E.R.: Algebraic Coding Theory. McGraw-Hill, New York (1968) (p. 33)
21. Bernasconi, J., Günter, C.G.: Analysis of nonlinear feedback logic for binary sequence gen-
erators. In: Advances in Cryptology—Eurocrypt ’85. LNCS, vol. 219, pp. 161–166 (1986)
(p. 75)
22. Bernstein, D.J.: Cache-timing attacks on AES. http://cr.yp.to/antiforgery/cachetiming-
20050414.pdf (p. 165)
23. Bernstein, D.J.: Why haven’t cube attacks broken anything. http://cr.yp.to/cubeattacks.html
(p. 231)
24. Bernstein, D.J.: Related-key attacks: who cares? eStream discussion forum. http://www.
ecrypt.eu.org/stream/phorum, 22 June 2005 (p. 239)
25. Beth, T., Piper, F.: The stop-and-go generator. In: Advances in Cryptology—Eurocrypt’84.
LNCS, vol. 209, pp. 88–92 (1985) (p. 155)
26. Biham, E., Shamir, A.: Differential cryptanalysis of DES-like cryptosystems. J. Cryptol. 4(1),
3–72 (1991) (p. 30)
27. Billingsley, P.: Probability and measure. In: Wiley Series in Probability and Mathematical
Statistics, 3rd edn. Wiley-Interscience, New York (1995) (pp. 316, 317, 319, 323, 325, 326)
28. Biryukov, A., Shamir, A., Wagner, D.: Real time cryptanalysis of A5/1 on a PC. In: Fast
Software Encryption (2000). http://cryptome.org/a5.ps (pp. 176, 178, 179)
29. Blake, I.F., Gao, S., Lambert, R.: Constructive problems for irreducible polynomials over
ﬁnite ﬁelds. In: Information Theory and Applications, Rockland, ON, 1993. Lecture Notes
in Comput. Sci., vol. 793, pp. 1–23. Springer, Berlin (1994) (p. 309)
30. Blake, I.F., Gao, S., Lambert, R.J.: Construction and distribution problems for irreducible
trinomials over ﬁnite ﬁelds. In: Applications of Finite Fields, Egham, 1994. Inst. Math. Appl.
Conf. Ser. New Ser., vol. 59, pp. 19–32. Oxford Univ. Press, New York (1996) (p. 309)
31. Bluetooth special interest group: bluetooth speciﬁcation, vol. 2, part H, November
2003. Available online http://bluetooth.org/foundry/adopters/document/Bluetooth_Core_
Speciﬁcation_v1.2 (p. 126)
32. Blum, L., Blum, M., Shub, M.: A simple unpredictable pseudorandom number generator.
SIAM J. Comput. 15(2), 364–383 (1986) (p. 244)
33. Blum, M., Micali, S.: How to generate cryptographically strong sequences of pseudo-random
bits. SIAM J. Comput. 13, 850–864 (1984) (pp. 241, 242)
34. Boesgaard, M., Vesterager, M., Peterson, T., Christiansen, J., Scavenius, O.: Rabbit: a new
high-performance stream cipher. In: Johansson, T. (ed.) Proceedings of Fast Software En-
cryption 2003. LNCS, vol. 2887, pp. 307–329. Springer, Berlin (2003) (p. 232)
35. Boost C++ libraries. http://www.boost.org/ (p. 370)
36. Briceno, M., Goldberg, I., Wagner, D.: An implementation of GSM A3A8 algorithm (1998).
http://www.iol.ie/~kooltex/a3a8.txt (p. 169)
37. Briceno, M., Goldberg, I., Wagner, D.: GSM cloning (1998). http://www.isaac.cs.berkeley.
edu/isaac/gsm-faq.html (p. 169)
38. Briceno, M., Goldberg, I., Wagner, D.: A pedagogical implementation of the GSM A5/1
and A5/2 “voice privacy” encryption algorithms (1999). http://cryptome.org/gsm-a512.htm
(p. 169)
39. Buchberger, B.: Gröbner bases: an algorithmic method in polynomial ideal theory. In: Bose,
N.K., Reidel, D. (eds.) Multidimensional Systems Theory, pp. 184–232. Reidel, Dordrecht
(1985) (p. 140)
40. Bügisser, P., Clausen, M., Shokrollahi, M.A.: Algebraic Complexity Theory. Grundlehren
der mathematischen Wissenschaften, vol. 315. Springer, Berlin (1997) (p. 281)
41. Camion, P., Carlet, C., Charpin, P., Sendrier, N.: On correlation-immune functions. In:
Feigenbaum, J. (ed.) Advances in Cryptology—CRYPTO’91. Lecture Notes in Computer
Science, vol. 576, pp. 86–100. Springer, Berlin (1992) (p. 76)

References
383
42. Camion, P., Canteaut, A.: Generalization of Siegenthaler inequality and Schnorr-Vaudenay
multipermutations. In: Advances in Cryptology—CRYPTO ’96 (Santa Barbara, CA). Lec-
ture Notes in Comput. Sci., vol. 1109, pp. 372–386. Springer, Berlin (1996) (pp. 78, 80)
43. Canﬁeld, E.R., Gao, Z., Greenhill, C., McKay, B.D., Robinson, R.W.: Asymtotic enumer-
ation of correlation-immune Boolean functions. Cryptogr. Commun. 2, 111–126 (2010).
arXiv:0909.3321 (p. 81)
44. Carlet, C., Gouget, A.: An upper bound on the number of m-resilient Boolean functions. In:
ASIACRYPT 2002. LNCS, vol. 2501, pp. 484–496. Springer, Berlin (2002) (p. 81)
45. Carlet, C., Klapper, A.: Upper bounds on the number of reslient functions and of bent func-
tions. In: Lecture Notes Dedicated to Philippe Desarte. Springer Verlag, to appear. A shorter
version has appeared in the Proceedings of the 23rd Symposium on Information Theory in
the Benelux, Louvain-La-Neuve, Belgian, 2002 (p. 81)
46. Casti, J.L.: Dynamical Systems and Their Applications: Linear Theory. Academic Press, San
Diego (1977) (p. 33)
47. Cayley, A.: A theorem on trees. Q. J. Math. 23, 376–378 (1889) (p. 334)
48. Chabaud, F., Vaudenay, S.: Links between differential and linear cryptanalysis. In: San-
tis, A.D. (ed.) Advances in Cryptology EUROCRYPT 94. LNCS, vol. 950, pp. 356–365.
Springer, New York (1995) (p. 30)
49. Chan, A.H., Games, R.A., Key, E.L.: On the complexity of de Bruijn sequences. J. Comb.
Theory, Ser. A 33, 233–246 (1982) (p. 157)
50. Cheng, Q.: On the construction of ﬁnite ﬁeld elements of large order. Finite Fields Appl. 11,
358–366 (2005) (p. 307)
51. Cheng, U., Golomb, S.W.: On the characterisation of PN sequences. IEEE Trans. Inf. Theory
29, 600 (1983) (p. 29)
52. Chepyzhov, V., Johansson, T., Smeets, B.: A simple algorithm for fast correlation attacks on
stream ciphers. In: Proceedings of the 7th International Workshop on Fast Software Encryp-
tion. LNCS, vol. 1978, pp. 181–195 (2001) (pp. 91, 94)
53. Chose, P., Joux, A., Mitton, M.: Fast correlation attacks: an algorithmic point of view. In:
Knudsen, L.R. (ed.) EUROCRYPT 2002. LNCS, vol. 2332, pp. 209–221. Springer, Berlin
(2002) (pp. 96, 105)
54. GNU CLISP—an ANSI Common Lisp Implementation. http://clisp.cons.org/ (p. 370)
55. Collart, S., Kalkbrener, M., Mall, D.: Converting bases with the Gröbner walk. J. Symb.
Comput. 24, 465–469 (1997) (pp. 140, 142)
56. Cook, S.A.: On the minimum computation time of functions. PhD thesis, Harvard University
(1966) (p. 249)
57. Coppersmith, D.: Finding a small root of a bivariate equation; factoring with high bits known.
In: Maurer, U.M. (ed.) EUROCRYPT. LNCS, vol. 1070, pp. 178–189. Springer, Berlin
(1996) (p. 361)
58. Coppersmith, D., Krawczyk, H., Mansour, Y.: The shrinking generator. In: Advances in
Cryptology—CRYPTO ’93, Santa Barbara, CA, 1993. LNCS, vol. 773, pp. 22–39. Springer,
Berlin (1994). doi:10.1007/3-540-48329-2_3 (p. 159)
59. Coppersmith, D., Winograd, S.: Matrix multiplications via arithmetic progressions. J. Symb.
Comput. 9, 251–280 (1990) (pp. 281, 289)
60. Coppersmith, D.: Small solutions to polynomial equations and low exponent vulnerabilities.
J. Cryptol. 10(4), 223–260 (1997) (p. 303)
61. Coudert, O., Berthet, C., Madre, J.C.: Veriﬁcation of synchronous sequential machines based
on symbolic execution. In: Automatic Veriﬁcation Methods for Finite State Systems. LNCS,
vol. 407, pp. 365–373. Springer, Berlin (1989) (p. 354)
62. Courtois, N.: Fast algebraic attacks on stream ciphers with linear feedback. In: Proceedings
of Crypto 2003. LNCS, vol. 2729, pp. 177–194. Springer, Berlin (2003) (p. 149)
63. Courtois, N., Klimov, A., Patarin, J., Shamir, A.: Efﬁcient algorithms for solving overdeﬁned
systems of multivariate polynomial equations. In: Advances in Cryptology—EUROCRYPT
2000. LNCS, vol. 1807, pp. 392–407. Springer, Berlin (2000) (p. 145)

384
References
64. Courtois, N., Meier, W.: Algebraic attacks on stream ciphers with linear feedback. In: Pro-
ceedings of Eurocrypt 2003. LNCS, vol. 2656, pp. 345–359. Springer, Berlin (2003). An
extended version is available at http://www.cryptosystem.net/stream/ (pp. 148, 149, 153)
65. ncurses. http://www.gnu.org/software/ncurses/ncurses.html (p. 370)
66. Daemen, J.: Cipher and hash function design strategies based on linear and differential crypt-
analysis. Doctoral dissertation. K.U. Leuven, March 1995 (p. 239)
67. Daemen, J., Govaert, R., Vandewalle, J.: On the design of high speed self-synchonizing
stream ciphers. In: Kam, P.Y., Hirota, O. (eds.) Singapore ICCS/ISITA ’92 Conference Pro-
ceedings, pp. 183–279. IEEE, New York (1992) (p. 239)
68. Daemen, J., Kitsos, P.: The self-synchonizing stream cipher MOSQUITO: eSTREAM docu-
mentation, version 2 (2005). http://www.ecrypt.eu.org/stream/p3ciphers/mosquito/mosquito.
pdf (p. 235)
69. Daemen, J., Kitsos, P.: The self-synchonizing stream cipher MOUSTIQUE (2006).
http://www.ecrypt.eu.org/stream/p3ciphers/mosquito/mosquito_p3.pdf (p. 235)
70. Daemen, J., Lano, J., Preneel, B.: Chosen ciphertext attack on SSS (2005). http://www.
ecrypt.eu.org/stream/papersdir/044.pdf (p. 235)
71. Dai, Z.d.: Proof of Rueppel’s linear complexity conjecture. IEEE Trans. Inf. Theory 32, 440–
443 (1986) (p. 50)
72. Dawson, E., Clark, A., Goli´c, J., Millan, W., Penna, L., Simpson, L.: The LILI-128 keystream
generator. In: Proc. of First NESSIE Workshop (2001) (p. 151)
73. de Bruijn, N.G.: A combinatorial problem. Nedel. Akad. Wetensch. Proc. 49, 758–764
(1946). other name: Indag. Math. 8, 461–467 (1946) (pp. 59, 60)
74. de Bruijn, N.G.: Acknowledgment of priority to C. Fyle Sainte-Marie on the counting of cir-
cular arrangements of 2n zeroes and ones that show each n-letter word exactly once. Techni-
cal Report TH-Report 75-WSK-06, Technolocical University Eidhoven, June 1975 (p. 60)
75. De Cannière, C., Preneel, B.: Trivium. http://www.ecrypt.eu.org/stream/triviump3.html
(p. 229)
76. Denisov, O.V.: An asymptotic formula for the number of correlation-immune boolean func-
tions of order k. Discrete Appl. Math. 2(4), 407–426 (1992). English Translation from Diskr.
Math. 3(2), 25–46 (1991) (p. 81)
77. Denisov, O.V.: A local limit theorem for the distribution of a part of the spectrum of a random
binary function. Discrete Appl. Math. 10, 87–101 (2000). English Translation from Diskr.
Math. 12, 1 (2000) (p. 81)
78. Dholakia, A.: Introduction to convolutional codes with applications. In: The Kluwer Inter-
national Series in Engineering and Computer Science. Kluwer Academic, Boston (1994)
(pp. 95, 106)
79. Diaconis, A.: In: Group Representations in Probability and Statistics. Lecture Notes-
Monographs Series, vol. 11. IMS, Hayward (1988) (p. 197)
80. Dinur, I., Shamir, A.: Cube attacks on tweakable black box polynomials. In: Joux, A. (ed.)
EUROCRYPT. LNCS, vol. 5479, pp. 278–299. Springer, Berlin (2009). Also available as
Cryptology ePrint Archive, Report 2008/385 http://eprint.iacr.org/ (pp. 231, 232)
81. Doyle, A.C.: The Return of Sherlock Holmes, chapter The Adventure of the Danc-
ing Men. Georges Newnes, Ltd (1905). Originally published 1903, Available online
http://en.wikisource.org/wiki/The_Adventure_of_the_Dancing_Men (pp. 2, 348)
82. Edel, Y., Klein, A.: Computational aspects of fast correlation attacks. Submitted, available
online http://cage.ugent.be/~klein/corr-comp.pdf (pp. 113, 263, 264)
83. Edel, Y., Klein, A.: Population count in arrays. Submitted, available online http://cage.
ugent.be/~klein/popc.html (pp. 95, 96, 97, 113)
84. Ekdahl, P., Johansson, T.: Another attack on A5/1. IEEE Trans. Inf. Theory 49(1), 284–289
(2003) (pp. 179, 180)
85. The estream project. http://www.ecrypt.eu.org/stream/ (pp. 12, 229)
86. Fano, R.M.: A heuristic discussion of probabilistic decoding. IEEE Trans. Inf. Theory IT-9,
64–74 (1963) (pp. 109, 110)

References
385
87. Faugère, J.C.: A new efﬁcient algorithm for computing Gröbner bases (F4). J. Pure Appl. Al-
gebra 139(1), 61–88 (1999). Available online http://fgbrs.lip6.fr/@papers/F99a.pdf (p. 140)
88. Faugère, J.C.: A new efﬁcient algorithm for computing Gröbner bases without reduc-
tion to zero (F5). In: Proceedings of the 2002 International Symposium on Symbolic and
Algebraic Computation (ISSAC), pp. 75–83. ACM, New York (2002). Available online
http://fgbrs.lip6.fr/@papers/F02a.pdf (p. 140)
89. Faugère, J.C., Gianni, P., Lazard, D., Mora, T.: Efﬁcient computation of zero-dimensional
Gröbner bases by change of ordering. J. Symb. Comput. 16(4), 329–344 (1993) (p. 143)
90. Ferguson, N., Schneier, B.: Practical Cryptography. Wiley, New York (2003) (p. 5)
91. Finny: A RC4 cycle that can’t happen. Posting to sci.crypt, September 1994 (p. 357)
92. Fisher, J.B., Stern, J.: An efﬁcient pseudo-random generator provably as secure as syndrome
decoding. In: Advances in Cryptology—EUROCRYPT ’96. LNCS, vol. 1070, pp. 245–255
(1996) (p. 255)
93. Fluhrer, S., Mantin, I., Shamir, A.: Weaknesses in the key scheduling algorithm of RC4.
In: Selected Areas in Cryptography. LNCS, vol. 2259, pp. 1–24. Springer, Berlin (2001)
(pp. 196, 199, 200, 202)
94. Fluhrer, S.R., Lukes, S.: Analysis of the E0 encryption system. In: Proc. 8th Workshop on
Selected Areas in Cryptography. LNCS, vol. 2259. Springer, Berlin (2001) (p. 127)
95. Fluhrer, S.R., McGrew, D.A.: Statistical analysis of the alleged RC4 keystream generator.
In: Proceedings of the 7th International Workshop on Fast Software Encryption. LNCS, vol.
1978, pp. 19–20. Springer, Berlin (2000) (pp. 213, 214, 218)
96. Forney, G.D. Jr.: The Viterbi algorithm. Proc. IEEE 61(3), 268–278 (1973) (p. 107)
97. Friedlander, J.B., Pomerance, C., Shparlinski, I.E.: Period of the power generator and small
values of the Carmicael’s function. Math. Comput. 70, 1591–1605 (2001) (pp. 254, 294)
98. Gaines, H.F.: Cryptanalysis. Dover, New York (1956) (p. 349)
99. Gap—groups, algorithms, programming—a system for computational discrete algebra.
http://www.gap-system.org/ (p. 370)
100. von zur Gathen, J., Gerhard, J.: Modern Computer Algebra. Cambridge University Press,
Cambridge (1999) (pp. 146, 153, 247, 292, 302)
101. von zur Gathen, J., Shparlinski, I.: Predicting subset sum pseudorandom generators. In:
Selected Areas in Cryptography. LNCS, vol. 3357, pp. 241–251. Springer, Berlin (2005)
(p. 255)
102. GCC, the GNU Compiler Collection. http://gcc.gnu.org/ (p. 370)
103. Gebauer, R., Möller, H.M.: On an installation of Buchberger’s algorithm. In: Robbiano, L.
(ed.) Computational Aspects of Communicative Algebra, pp. 141–152. Academic Press, New
York (1988) (p. 140)
104. Geffe, P.R.: How to protect data with ciphers that are really hard to break. Electronics 4,
129–156 (1973) (p. 64)
105. The GNU Multiple Precision Arithmetic Library. http://gmplib.org/ (pp. 249, 370)
106. Gold, R.: Maximal recursive sequences with 3-valued cross-correlation functions. IEEE
Trans. Inf. Theory 14, 154–156 (1968) (p. 30)
107. Goldberg, I., Wegner, D., Green, L.: The (real-time) cryptanalysis of A5/2. Presented at the
Rump Session of Crypto ’99 (1999). Slides available at www.cs.berley.edu/~daw/tmp/a52-
sliedes.ps (p. 175)
108. Goldstein, D., Moews, D.: The identity is the most likely exchange shufﬂe for large n. Aequ.
Math. 65(1–2), 3–30 (2003) (p. 195)
109. Goli´c, J.Dj.: Cryptanalysis of the alleged A5 stream cipher. In: Advances in Cryptology—
EUROCRYPTO ’97. LNCS, vol. 1233, pp. 239–255. Springer, Berlin (1997) (p. 65)
110. Goli´c, J.Dj.: Linear statistical weakness of alleged RC4 keystream generator. In: Advances in
Cryptology—EUROCRYPTO ’97. LNCS, vol. 1233, pp. 226–238. Springer, Berlin (1997)
(p. 213)
111. Goli´c, J.Dj.: Linear models for a time-variant-permutation generator. IEEE Trans. Inf. The-
ory 45(7), 2374–2382 (1999) (p. 213)

386
References
112. Goli´c, J.Dj.: Iterative probabilistic cryptanalysis of RC4 keystream generator. In: ACISP
2000, pp. 220–233 (2000) (p. 202)
113. Goli´c, J.Dj.: Correlation analysis of the shrinking generator. In: Advances in Cryptology—
CRYPTO 2001 (Santa Barbara, CA). Lecture Notes in Comput. Sci., vol. 2139, pp. 440–457.
Springer, Berlin (2001) (pp. 161, 162, 163)
114. Golomb, S.W.: On the classiﬁcation of balanced binary sequences of period 2n −1. IEEE
Trans. Inf. Theory 26, 730–732 (1980) (p. 27)
115. Golomb, S.W.: Shift Register Sequences. Aegean Park, Laguna Hills, revised edition (1982)
(pp. 17, 24, 25, 27)
116. Gong, G., Gupta, K.C., Hell, M., Nawaz, Y.: Towards a general RC4-like keystream genera-
tor. In: Information Security and Cryptology. Lecture Notes in Comput. Sci., vol. 3822, pp.
162–174. Springer, Berlin (2005) (pp. 222, 223)
117. Good, I.J.: Normal recurring decimals. J. Lond. Math. Soc. 21(3), 169–172 (1946) (p. 59)
118. Gopalakrishnan, K., Stinson, D.R.: Three characterisations of non-binary correlation-
immune and resilient functions. Des. Codes Cryptogr. 5, 241–251 (1995) (p. 76)
119. Graham, R.L., Knuth, D.E., Patashnik, O.: Concrete Mathematics, 2nd edn. Addison-Wesley,
Reading (1994) (pp. 19, 332)
120. Günther, C.G.: Alternating step generators controlled by De Bruijn sequences. In: Advances
in Cryptography, Eurocrypt ’87. LNCS, vol. 304, pp. 5–14 (1988) (pp. 157, 158)
121. Gupta, K., Gong, G., Nawaz, Y.: A 32-bit RC4-like keystream generator. Technical Report
CACR 2005-21, Center for Applied Cryptographic Research, University of Waterloo, 2005.
http://www.cacr.math.uwaterloo.ca/tech_reports.html (p. 224)
122. Hardy, G.H., Wright, E.M.: The Theory of Numbers, 4th edn. Oxford University Press, Lon-
don (1960) (pp. 295, 296, 300)
123. Hawkes, P., Paddon, M., Rose, G.G., de Vries, M.W.: Primitive speciﬁcation for SSS (2005).
http://www.ecrypt.eu.org/stream/ciphers/sss/sss.pdf (p. 235)
124. Hawkes, P., Rose, G.G.: Rewriting variables: the complexity of fast algebraic attacks on
stream ciphers. In: Advances in Cryptology—CRYPTO 2004. Lecture Notes in Comput.
Sci., vol. 3152, pp. 390–406. Springer, Berlin (2004) (p. 149)
125. van Heesch, D.: Doxygen, source code documentation generator tool. http://www.stack.nl/~
dimitri/doxygen/ (pp. 365, 370)
126. Helleseth, T., Kumar, P.V.: Sequences with low correlation. In: Pless, V.S., Huffman, W.C.
(eds.) Handbook of Coding Theory, vol. II, pp. 1765–1853. Elsevier, Amsterdam (1998).
Chap. 21 (p. 29)
127. Higham, N.J.: Exploiting fast matrix multiplication within the level 3 BLAS. ACM Trans.
Math. Softw. 16(4), 352–368 (1990) (p. 281)
128. Hinek, M.J.: Cryptanalysis of RSA and its variants. In: Cryptography and Network Security.
CRC Press, Boca Raton (2010) (p. 247)
129. Hoch, J., Shamir, A.: Fault analysis of stream ciphers. In: Joye, M., Quisquater, J. (eds.)
Cryptographic Hardware and Embedded Systems—CHES 2004. LNCS, vol. 3156, pp. 240–
253. Springer, Berlin (2004) (p. 165)
130. Hopcraft, J.E., Ullman, J.D.: Introduction to Automata Theory, Languages and Computation,
1st edn. Addision-Wesley, Reading (1979) (p. 273)
131. Howgrave-Graham, N.: Finding small roots of univariate modular equations revisited. In:
Proceeding of Cryptography and Coding. LNCS, vol. 1355, pp. 45–50. Springer, Berlin
(1997) (p. 303)
132. Huang, X., Huang, W., Liu, X., Wang, C., Wang, Z.J., Wang, T.: Reconstructing the non-
linear ﬁlter function of LILI-128 stream cipher based on complexity (2007). http://arxiv.org/
abs/cs.CR/0702128 (p. 151)
133. Hulton, D.: Practical exploration of RC4 weaknesses in WEP environments. Presented at
HiverCon (2002) (p. 201)
134. Jakobsson, M., Wetzel, S.: Security weaknesses in bluetooth. In: Proc. RSA Security Conf.—
Cryptographer’s Track. LNCS, vol. 2020, pp. 176–191. Springer, Berlin (2001) (p. 127)

References
387
135. Jelinek, F.: Sequential decoding algorithm using a stack. IBM J. Res. Dev. 13, 675–678
(1969) (p. 109)
136. Johansson, T., Jönsson, J.J.: Improved fast correlation attacks on stream ciphers via con-
volutional codes. In: Advances in Cryptology—EUROCRYPT ’99. LNCS, vol. 1592, pp.
347–362. Springer, Berlin (1999) (p. 112)
137. Johansson, T., Jönsson, J.J.: Theoretical analysis of a correlation attack based on convolu-
tional codes. IEEE Trans. Inf. Theory 48(8) (2002) (p. 112)
138. Joux, A.: Algorithmic Cryptanalysis. CRC Press, Boca Raton (2009) (pp. 97, 247)
139. Joux, A., Muller, F.: Chosen-ciphertext attacks against MOSQUITO. In: Robshaw, M. (ed.)
Fast Software Encryption 2006. LNCS, vol. 4047, pp. 390–404. Springer, Berlin (2006)
(p. 235)
140. Jungnickel, D.: Finite Fields: Structure and Arithmetics. BI Wissenschaftsverlag, Mannheim
(1993) (pp. 297, 305)
141. Kahn, D.: The Codebreakers. MacMillan, New York (1967) (pp. 1, 2, 3)
142. Kailath, T., Sayed, A.H.: Displacement structure: theory and applications. SIAM Rev. 35,
297–386 (1995) (pp. 31, 37)
143. Karatsuba, A., Ofman, Yu.: Multiplication of multidigit numbers on automata. Sov. Phys.
Dokl. 7, 595–596 (1963). Original in: Dokl. Akad. Nauk SSSR 145, 293–394 (1963) (p. 247)
144. Kasami, T.: Weight distribution formula for some class of cyclic codes. Technical Report
R-285, Coordinated Science Laboratory, University of Illinois, Urbana, April 1966 (p. 30)
145. Käsper, E., Rijmen, V., Bjørstad, T.E., Rechberger, C., Robshaw, M.J.B., Sekar, G.: Cor-
related keystreams in Moustique. In: AFRICACRYPT, pp. 246–257 (2008) (pp. 235, 238,
239)
146. Kerckhoffs, A.: La cryptographie militaire. Journal des sciences militaires, 9th series, 1883.
(January 1883) pp. 5–83, (Feburary 1883) pp. 161–191 (p. 1)
147. Kipnis, A., Shamir, A.: Cryptanalysis of the HFE public key cryptosystem. In: Proceedings
of CRYPTO ’99. Springer, Berlin (1999) (p. 144)
148. Klein, A.: Attacks against the RC4 stream cipher. Des. Codes Cryptogr. 48, 269–286 (2008)
(pp. 202, 203, 205, 209, 224, 227)
149. Knudsen, L.R., Meier, W., Preneel, B., Rijmen, V., Verdoolaege, S.: Analysis methods for
(alleged) RC4. In: Ohta, K., Pei, D. (eds.) Advances in Cryptology—ASIACRYPT’98. Lec-
ture Notes in Computer Science, vol. 1998, pp. 327–341. Springer, Berlin (1998) (pp. 210,
211)
150. Knuth, D.E.: Literate programming. Comput. J., 27, 97–111 (1985). Reprinted with correc-
tions in Knuth, D.E.: Literate Programming, Number, 27, CSLI Lecture Notes. Center for
the Study of Language and Information, 1992, Stanford, California (p. 373)
151. Knuth, D.E.: The Art of Computer Programming, vol. 1. Fundamental Algorithms, 3rd edn.
Addison-Wesley, Reading (1998) (p. 332)
152. Knuth, D.E.: The Art of Computer Programming, vol. 2. Seminumerical Algorithms, 3rd
edn. Addison-Wesley, Reading (1998) (pp. 191, 247, 249)
153. Knuth, D.E.: The Art of Computer Programming, vol. 3. Sorting and Searching, 3rd edn.
Addison-Wesley, Reading (1998) (p. 332)
154. Knuth, D.E.: MMIXware: A RISC Computer for the Third Millennium. Lecture Notes in
Computer Science, vol. 1750. Springer, Berlin (1999) (p. 288)
155. Knuth, D.E.: MMIX, the Art of Computer Programming. Fasc. 1. Addison-Wesley, Upper
Saddle River (2005) (p. 288)
156. Knuth, D.E.: The Art of Computer Programming, vol. 4. Bitwise Tricks and Techniques,
Binary Decision Diagrams. Addison-Wesley, Upper Saddle River (2009) (pp. 123, 125, 261,
272)
157. Knuth, D.E.: The Art of Computer Programming, vol. 4. Generating All Tuples and Permu-
tations. Addison-Wesley, Upper Saddle River (2005) (p. 350)
158. Knuth, D.E.: Literate Programming. Number 27 in CSLI Lecture Notes. Center for the Study
of Language and Information, Stanford, California (1992) (p. 371)

388
References
159. Koblitz, N., Menezes, A.: Another look at “provable security”. Journal of Cryptology 20
(2004). See also Cryptology ePrint Archive, Report 2004/152 http://eprint.iacr.org/ (p. 247)
160. Koç, Ç.K. (ed.): Cryptographic Engineering Springer, Berlin (2009) (p. 246)
161. Kocher, P.: Timing attacks on implementations of Difﬁ-Hellman, RSA, DSS and other sys-
tems. In: Kobliz, M. (ed.) CRYPTO ’96. LNCS, vol. 1109, pp. 104–113 (1996) (p. 246)
162. Kocher, P., Jaffe, J., Jun, B.: Differential power analysis. In: Wiener, M. (ed.) Advances
in Cryptology—CRYPTO 1999. LNCS, vol. 1666, pp. 288–297. Springer, Berlin (1999)
(p. 164)
163. Kohno, T., Viega, J., Whiting, D.: Cwc: A high-performance conventional authenticated en-
cryption mode. Cryptology ePrint Archive, Report 2003/106 (2003). http://eprint.iacr.org/
(p. 239)
164. KoreK:
chopchop
(experimental
WEP
attacks)
(2004).
http://www.netstumbler.org/
showthread.php?t=12489 (p. 189)
165. KoreK: Next generation of WEP attacks? (2004). http://www.netstumbler.org/showthread.
php?p=93942&postcount=35 (p. 201)
166. Krause, M.: BDD-based attacks of keystream generators. In: Knudson, L. (ed.) Advances
in Cryptology—EUROCRYPT ’02. LNCS, vol. 1462, pp. 222–237. Springer, Berlin (2002)
(pp. 117, 356)
167. Lano, J.: Cryptanalysis and design of synchronous stream ciphers. PhD thesis, Katholieke
Universiteit Leuven, Faculteit Ingenierswtenschappen, Departement Elektrotechniek-ESAT,
Kasteelpark Arenberg 10, 3001 Leuven-Heverlee. Juni 2006 (p. 164)
168. Lenstra, A.K., Lenstra, H.W., Lovász, L.: Factoring polynomials with rational coefﬁcients.
Math. Ann. 261, 515–572 (1982) (p. 302)
169. Lidl, R., Niederreiter, H.: Introduction to Finite Fields and Their Applications. Cambridge
University Press, Cambridge (1986) (p. 25)
170. van Lint, J.H.: Introduction to Coding Theory, 3rd edn. Springer, Berlin (1998) (p. 94)
171. Lu, P., Huang, L.: A new correlation attack of LFSR sequences. In: Feng, K., Niederreiter,
H., Xing, C. (eds.) Coding, Cryptography and Combinatorics. Progress in Computer Science
and Applied Logic, vol. 23, pp. 67–84. Birkhäuser, Basel (2004) (pp. 94, 95, 99)
172. Lu, Y., Vaudenary, S., Meier, W.: The conditional correlation attack: a practical attack on
bluetooth encryption. In: Crypto 2005. LNCS, vol. 3621, pp. 97–117 (2005). Available online
http://www.terminodes.org/micsPublicationsDetail.php?pubno=1216 (p. 127)
173. Lüneburg, H.: Ein einfacher Beweis für den Satz von Zsigmondy über primitive Primteiler
von AN −1. In: Aigner, M., Jungnickel, D. (eds.) Geometries and Codes. Lecture Notes in
Mathematics, vol. 893, pp. 219–222. Springer, Berlin (1981) (p. 297)
174. GNU make. http://www.gnu.org/software/make/ (p. 369)
175. Mantin, I.: A practical attack against RC4 in the WEP mode. In: Roy, B.K. (ed.) ASI-
ACRYPT. LNCS, vol. 3788, pp. 395–411. Springer, Berlin (2005) (p. 203)
176. Mantin, I.: Predicting and distinguishing attacks on RC4 keystream generator. In: Cramer, R.
(ed.) Advances in Cryptology—EUROCRYPT 2005. LNCS, vol. 3494, pp. 491–506.
Springer, Berlin (2005) (p. 216)
177. Mantin, I., Shamir, A.: A practical attack on broadcast RC4. In: Matsui, M. (ed.) Revised
Papers from the 8th International Workshop on Fast Software Encryption. LNCS, vol. 2355,
pp. 152–164. Springer, London (2001) (p. 222)
178. Massey, J.L.: Shift-register synthesis and BCH-decoding. IEEE Trans. Inf. Theory 15, 122–
127 (1969) (p. 33)
179. Matsui, M.: Linear cryptanalysis method for DES cipher. In: Desmedt, Y. (ed.) Advances in
Cryptology, Eurocrypt ’93. LNCS, vol. 839, pp. 1–11. Springer, Berlin (1994) (p. 30)
180. Maurer, U., Massey, J.L.: Perfect local randomness in pseudo-random sequences. J. Cryptol.
4, 135–149 (1993) (p. 256)
181. Maximov, A., Johansson, T., Babbage, S.: An improved correlation attack on A5/1. In: Se-
lected Areas in Cryptography. Lecture Notes in Comput. Sci., vol. 3357, pp. 1–18. Springer,
Berlin (2005) (pp. 179, 180)

References
389
182. Maximov, A., Khovratovich, D.: New State Recovering Attack on RC4. Technical report,
Laboratory of Algorithmics, Cryptology and Security, University of Luxembourg (2008).
http://eprint.iacr.org/2008/017 (pp. 210, 212)
183. May, A.: Using LLL-Reduction for solving RSA and Factorization Problems: A Survey.
Available online http://citeseerx.ist.edu/viewdoc/summary?doi=10.1.1.86.9908 (p. 247)
184. McEliece, R.J.: The algebraic theory of convolutional codes. In: Pless, V.S., Huffman, W.C.
(eds.) Handbook of Coding Theory, vol. I, pp. 1065–1138. Elsevier, Amsterdam (1998).
Chap. 12 (p. 106)
185. McGuire, G., Calerbank, A.R.: Proof of a conjecture of Sarwarte and Pursley regarding pairs
of binary m-sequences. IEEE Trans. Inf. Theory 41, 1153–1155 (1995) (p. 30)
186. Meidel, W., Niederreiter, H.: Linear complexity, k-error linear complexity, and the discrete
Fourier transform. J. Complex. 18, 87–103 (2002) (p. 44)
187. Meier, W., Staffelbach, O.: Fast correlation attacks on stream ciphers. J. Cryptol. 1, 159–176
(1989) (p. 114)
188. Meier, W., Staffelbach, O.: The self-shrinking generator. In: Proceedings of EUROCRYPT
’94. LNCS, vol. 950, pp. 205–214 (1994) (p. 356)
189. Meinel, C., Theobald, T.: Algorithmen und Datenstrukturen im VLSI-Design: OBDD –
Grundlagen und Anwendungen. Springer, Berlin (1997) (p. 125)
190. Merkel, R.C., Hellman, M.E.: Hiding information and signatures in trapdoor knapsack. IEEE
Trans. Inf. Theory IT-24(5), 525–530 (1978) (pp. 254, 343)
191. Miller, G.L.: Riemann’s hypothesis and tests for primality. J. Comput. Syst. Sci. 13, 300–317
(1976) (p. 251)
192. Mironov, I.: (Not so) random shufﬂes of RC4. In: Advances in Cryptology—CRYPTO 2002.
LNCS, vol. 2442, pp. 304–319. Springer, Berlin (2002) (pp. 196, 198)
193. Mitchell, C.: Enumerating Boolean functions of cryptographic signiﬁcance. J. Cryptol. 2,
155–170 (1990) (p. 81)
194. Moen, V., Raddum, H., Hole, K.J.: Weakness in the temporal key hash of WPA. Mob. Com-
put. Commun. Rev. 8(2), 76–83 (2004) (p. 187)
195. Montgomery, P.L.: Modular multiplication without trial division. Math. Commun. 44, 519–
521 (1985) (p. 250)
196. Montgomery, P.L.: A survey of modern integer factoring algorithms. Quart. - Cent. Wiskd.
Inform. 7, 337–366 (1994) (p. 247)
197. Niederreiter, H.: Sequences with almost perfect linear complexity proﬁle. In: Chaum, D.,
Price, W.L. (eds.) Advances in Cryptology, Eurocrypt ’87. LNCS, vol. 304, pp. 37–51.
Springer, Berlin (1988) (pp. 47, 48)
198. Niederreiter, H.: Keystream sequncence with a good linear complexity proﬁle for every start-
ing point. In: Advances in Cryptology—Eurocrypt ’89. Lecture Notes in Computer Science,
vol. 434, pp. 523–532 (1990) (pp. 46, 48)
199. The OCB authenticated-encryption algorithm. http://datatracker.ietf.org/doc/draft-krovetz-
ocb/?include_text=1 (p. 239)
200. Odlyzko, A.M.: The rise and fall of the knapsack cryptosystem. In: Pomerance, C. (ed.)
Cryptology and Computational Number Theory. Proceeding of Symposia in Applied Math-
ematics, vol. 42, pp. 75–88. American Mathematical Society, Providence (1990) (pp. 254,
277, 278, 360)
201. The openSSL library. http://www.openssl.org (p. 251)
202. Palmer, E.M., Read, R.C., Robonson, R.W.: Balancing the n-cube: a census of colorings.
J. Algebr. Comb. 1, 257–273 (1992) (p. 81)
203. Pan, V.: Strassen’s algorithm is not optimal. Trilinear technique of aggregating, uniting and
canceling for constructing fast algorithms for matrix muliplication. In: Proc. Nineteenth Ann.
Symp. on Foundations of Computer Science, pp. 28–38 (1978) (pp. 281, 286)
204. Pan, V.: How to Multiply Matrices Faster. Lecture Notes in Computer Science., vol. 179.
Springer, Berlin (1984) (pp. 281, 282)
205. Parker, M.G., Kemp, A.H., Shepherd, S.J.: Fast Blum-Blum-Shub sequence generation using
Montgomery multiplication. IEEE Proc. Comput. Digit. Techn. 147, 252–254 (2000) (p. 251)

390
References
206. Patarin, J.: Hidden ﬁeld equations (HFE) and isomorphisms of polynomials (IP): two new
families of asymmetric algorithms. In: Eurocrypt ’96, pp. 33–48. Springer, Berlin (1996).
An extended version can be found at http://www.minrank.org/courtois/hfe.ps (p. 254)
207. Paul, G., Maitra, S.: RC4 Stream Cipher and Its Variants. Discrete Mathematics and Its Ap-
plications. CRC Press, Boca Raton (2011) (p. 183)
208. Paul, G.K.: Analysis and design of RC4 and its variants. PhD thesis, Department of Computer
Science & Engineering, Jadavpur University, Kolkata, India (2008) (p. 227)
209. Paul, S., Preneel, B.: A new weakness in the RC4 keystream generator and an approach to
improve the security of the cipher. In: FSE 2004. LNCS, vol. 3017, pp. 245–259 (2004)
(pp. 222, 224, 226)
210. Peikari, C., Chuvakin, A.: Security Warrior. O’Reilly (2004) (p. 246)
211. Perron, O.: In: Die Lehre von den Kettenbrüchen. Elementare Kettenbrüche. Band 1. 3 Au-
ﬂage. Teubner, Stuttgart (1954) (p. 47)
212. Poe, E.A.: A Few Words on Secret Writing. Graham’s Magazine. July 1841 (p. 349)
213. Poe, E.A.: The Gold-Bug. The Dollar Newspaper (Philadelphia, PA), vol. I, no. 23, pp. 1 and
4, June 28 1843. Available online http://www.eapoe.org/works/tales/goldbga2.htm (pp. 2,
349)
214. Pritchard, P.: A sublinear additive sieve for ﬁnding prime numbers. Commun. ACM 24(1),
18–23 (1981) (p. 251)
215. Prüfer, H.: Neuer Beweis eines Satzes über Permutationen. Arch. Math. Phys. 27, 742–744
(1918) (p. 334)
216. Pyshkin, A., Tews, E., Weinmann, R.P.: Breaking 104 bit WEP in less than 60 seconds. In:
WISA. LNCS, vol. 4867, pp. 188–202 (2007). http://eprint.iacr.org/2007/120.pdf (pp. 185,
207)
217. Rabin, M.O.: Probabilistic algorithms for testing primality. J. Number Theory 12, 128–138
(1980) (p. 251)
218. Ranjan, R.K., Gosti, W., Brayton, R.K., Sangiovanni-Vincentelli, A.: Dynamic reordering in
a breadth-ﬁrst manipulation based BDD package: challenges and solutions. In: International
Conference on Computer Design IEEE, pp. 344–351, October (1997) (p. 268)
219. Reischuk, K.R.: Komplexitätstheorie. Grundlagen, Band 1. B.G. Teubner, Stuttgart, Leipzig
(1999) (p. 273)
220. Rejewski, M.: An application of the theory of permutations in breaking the Enigma cipher.
Appl. Math. 16(4), 543–559 (1980) (p. 10)
221. Rivest, R.: RSA: Security response to weaknesses in key scheduling algorithm of RC4.
Technical report, RSA Security, Inc. (2001). http://www.rsasecurity.com/rsalabs/technotes/
wep.html (p. 183)
222. Rivest, R.L., Silverman, R.D.: Are ‘strong’ primes needed for RSA? Technical report, The
RSA Laboratories Seminar Series (1997) (p. 251)
223. de Riviére, A.: Question 48. l’Intermédiare des Mathématiciens 1, 19–20 (1894) (p. 60)
224. Robbiano, L.: Term orderings on the polynomial ring. In: EUROCAL’85. LNCS, vol. 204,
513–517 (1985) (p. 133)
225. Robbins, D., Bolker, E.: The bias of three pseudo-random shufﬂes. Aecquationes Mathemat-
icae 22, 268–292 (1981) (pp. 190, 192, 195)
226. Robshaw, M., Billet, O. (eds.): New Stream Cipher Designs, the ESTREAM Finalists. Lec-
ture Notes in Computer Science, Security and Cryptology, vol. 4986. Springer, Berlin (2008)
(p. 229)
227. Rudell, R.: Dynamic variable ordering for binary decision diagrams. In: Proc. Intl. Conf. on
Computer-Aided Design, pp. 42–47, November 1993 (pp. 267, 268)
228. Rueppel, R.A.: Analysis and Design of Stream Chiphers. Springer, Berlin (1986) (pp. 42, 43,
44, 75)
229. Rueppel, R.A., Massey, J.L.: Knapsack as nonlinear function. In: IEEE Intern. Symp. of
Inform. Theory. IEEE Press, New York (1985) (p. 255)
230. Rueppel, R.A., Staffelbach, O.J.: Products of linear recurring sequences with maximum com-
plexity. IEEE Trans. Inf. Theory 33, 124–131 (1987) (p. 70)

References
391
231. Sainte-Marie, Fly C.: Solution to question 48. l’Intermédiare des Mathématiciens 1, 107–110
(1894) (p. 60)
232. Sarkar, P., Maitra, S.: Nonlinearity bounds and construction of resilient Boolean functions.
In: Advances in Cryptology—CRYPTO 2000. LNCS, vol. 1880, pp. 515–532. Springer,
Berlin (2000) (p. 152)
233. Steel Bank Common Lisp. http://www.sbcl.org/ (p. 370)
234. Schmidt, F., Simion, R.: Card shufﬂing and a transformation on Sn. Aequations Mathemati-
cae 44, 11–34 (1992) (p. 193)
235. Schneider, M.: A Note on the Construction and Upper Bounds of Correlation-Immune Func-
tions. LNCS, vol. 1355, 295–306 (1997) (p. 81)
236. Schönhage, A., Strassen, V.: Schnelle Multiplikation großer Zahlen. Computing 7, 281–292
(1971) (p. 249)
237. Seal, D.: Newsgroup comp.arch.arithmetic, 13 May 1997 (p. 263)
238. Seindal, R., Pinard, F., Vaughan, G.V., Blake, E.: GNU M4, version 1.4.11 (2008). Available
online http://www.gnu.org/software/m4/manual/index.html (pp. 369, 374)
239. Shaked, Y., Wool, A.: Cryptanalysis of the bluetooth E0 cipher using OBDDs. In: Informa-
tion Security. LNCS, vol. 4176, pp. 187–202. Springer, Berlin (2006) (p. 127)
240. Shamir, A.: On the generation of cryptographically strong pseudo-random sequences. In: 8th
International Colloquium on Automata Languages and Programming. LNCS, vol. 62 (1981)
(p. 241)
241. Shamir, A., Kipnis, A.: Cryptanalysis of the HFE public key cryptosystem. In: CRYPTO ’99
(1990). Available online http://www.minrank.org/courtois/hfesubreg.ps (p. 254)
242. Shamir, A., Tsaban, B.: Guaranteeing the diversity of number generators. Inf. Comput.
171(2), 350–363 (2001) (p. 234)
243. Shamir, A., Zippel, R.E.: On the security of the Merkel-Hellman cryptographic scheme.
IEEE Trans. Inf. Theory IT-26(3), 339–340 (1980) (pp. 254, 360)
244. Shoup, V.: NTL: A library for doing number theory. http://www.shoup.net/ntl/ (pp. 38, 370)
245. Shparlinski, I.: Cryptographic Applications of Analytic Number Theory: Complexity,
Lower Bounds and Pseudorandomness. Progress in Computer Science and Applied Logic.
Birkhäuser, Basel (2003) (p. 254)
246. Shulman, M.A.: MMM Mode for Emacs, version 0.4.8 edition (2004). http://mmm-mode.
sourceforge.net/ (p. 377)
247. Sidorenko, A., Schoenmakers, B.: Concrete security of the Blum-Blum-Shub pseudorandom
generator. In: Smart, N.P. (ed.) Cryptography and Coding 2005. LNCS, vol. 3796, pp. 355–
375. Springer, Berlin (2005) (pp. 245, 246, 253)
248. Siegenthaler, T.: Correlation-immunity of nonlinear combining functions for cryptographic
applications. IEEE Trans. Inf. Theory 30, 776–780 (1984) (p. 78)
249. Siegmund, D.: Sequential Analysis (Tests and Conﬁdence Intervals). Springer Series in
Statistics. Springer, New York (1985) (pp. 113, 316)
250. Siegmund, D.: Boundary crossing probabilities and statistical application. Ann. Stat. 14,
361–404 (1986) (p. 325)
251. Sieling, D., Wegener, I.: Reduction of OBDDs in linear time. Inf. Process. Lett. 48, 139–144
(1993) (p. 119)
252. Silverman, R.D.: Fast generation of random, strong RSA primes. Technical report, RSA Cry-
poBytes, volume 3, No 2, 1997. Available online http://www.rsa.com/rsalabs/node.asp?id-
2149 (p. 251)
253. The SINGULAR computer algebra system. http://www.singular.uni-kl.de/ (pp. 140, 370)
254. Solovay, R., Strassen, V.: A fast Monte Carlo test for primality. SIAM J. Comput. 6, 84–85
(1977) (p. 251)
255. St Denis, T.: Cryptography for Developers. Syngress (2007) (p. 53)
256. Stanley, R.P.: Enumerative Combinatorics. Cambridge Studies in Advanced Mathematics,
vol. 49. Cambridge University Press, Cambridge (1997) (p. 221)

392
References
257. Stern, J.: A method for ﬁnding codewords of small weight. In: Cohen, G.D., Wolfmann,
J. (eds.) Coding Theory and Applications. Lecture Notes in Computer Science, vol. 388,
pp. 106–113. Springer, Berlin (1989) (p. 254)
258. Sterndark, D.: RC4 algorithm revealed. Usenet posting sternCVKL4B.Hyy@netcom.com.
September 1994 (p. 183)
259. Stinson, D.R.: Cryptography, Theory, Practice, Discrete Mathematics and Its Applications,
3rd edn. Chapman & Hall/CRC, London (2006) (p. 3)
260. Strassen, V.: Gaussian elimination is not optimal. Numer. Math. 13, 354–356 (1969) (p. 280)
261. Swan, R.C.: Factorisation of polynomials over ﬁnite ﬁelds. Pac. J. Math. 12, 1099–1106
(1962) (p. 308)
262. Tews, E.: Attacks on the WEP protocol. Master’s thesis, TU Darmstadt, Fachgebiet Theo-
retische Informatik (CDC) (2007) (p. 185)
263. Tews, E., Klein, A.: Attacks on Wireless LANs: About the security of IEEE 802.11 based
wireless networks. Vdm Verlag Dr. Müller (2008) (p. 185)
264. TEX Live. http://www.tug.org/texlive/ (p. 369)
265. Toom, A.L.: The complexity of a scheme of functional elements realising the multiplication
of integers. J. Sov. Math. 3, 714–716 (1963). Original in: Dokl. Akad. Nauk SSSR 150,
496–498 (1963) (p. 249)
266. Traverso, C.: Hilbert functions and Buchberger’s algorithm. J. Symb. Comput. 22, 355–376
(1997) (p. 142)
267. Tutte, W.T.: The dissection of equilateral triangles into equilateral triangles. Proc. Camb.
Philos. Soc. 44, 463–482 (1948) (p. 62)
268. Ulbricht, H.: Die Chriffriermaschine ENIGMA Trügerische Sicherheit, Ein Beitrag zur
Geschichte der Nachrichtendienste. PhD thesis, Fachbereich Mathematik und Informatik,
Technische Universität Carolo-Wilhelmina zu Braunschweig (2005) (p. 10)
269. Unwin, S.: The Probability of God: A Simple Calculation Proves the Ultimate Truth. Crown
Forum, New York (2003) (p. 315)
270. Vazirani, U., Vazirani, V.: Efﬁcient and secure pseudorandom number generation. In: Pro-
ceedings of the 25th Annual Symposium on the Foundations of Computer Science, pp. 458–
463. IEEE Press, New York (1984) (pp. 245, 251, 252, 253)
271. Viterbi, A.J.: Error bounds for convolutional codes and an asymptotically optimum decoding
algorithm. IEEE Trans. Inf. Theory IT-13(2), 260–269 (1967) (p. 107)
272. Vogel, R.: On the linear complexity of cascaded sequences. In: Advances in Cryptology—
Eurocrypt ’84. LNCS, vol. 209, pp. 99–109 (1985) (p. 157)
273. Wald, A.: Sequential Analysis. Wiley, New York (1947) (p. 316)
274. Wang, M.Z., Massey, J.L.: The characterisation of all binary sequences with a perfect linear
complexity proﬁle. Paper presented at the Eurocrypt ’86 (1986) (p. 49)
275. Ward, J.B.: The Beal Papers. Virginian Book and Job Print (1885) (p. 2)
276. Warren, H.S. Jr.: Hacker’s Delight. Addison-Wesley, Boston (2003). Revisions and additional
material are on the homepage of the book. http://www.hackersdelight.org/ (pp. 53, 113)
277. Warren, H.S. Jr: Homepage of Hacker’s Delight. http://www.hackersdelight.org/. Contains
example programs, errata and additional material (pp. 263, 350)
278. Wegener, I.: Branching Programs and Binary Decision Diagrams. SIAM, Philadelphia (2000)
(p. 125)
279. Wegner, P.: A technique for counting ones in a binary computer. Commun. ACM 3, 322
(1960) (p. 358)
280. Wiedemann, D.H.: Solving sparse linear equations over ﬁnite ﬁelds. IEEE Trans. Inf. Theory
IT-32(1), 54–62 (1986) (pp. 291, 292)
281. Wilkes, M.V., Wheeler, D.J., Gill, S.: The Preparation of Programs for an Electronic Digital
Computer. 2nd edn. Addison-Wesley, Reading (1957) (p. 263)
282. Winograd, S.: A new algorithm for inner product. IEEE Trans. Comput. C-18, 693–694
(1968) (p. 279)
283. Wozencraft, J.M., Rieffen, B.: Sequential Decoding. MIT Press/Wiley, Cambridge (1961)
(p. 109)

References
393
284. Wu, H.: Cryptanalysis of a 32-bit RC4-like Stream Cipher. Technical report, Katholieke Uni-
versiteit Leuven, Dept. ESAT/COSIC (2005). http://eprint.iacr.org/2005/219.pdf (p. 224)
285. Xiao, G.Z., Massey, M.L.: A spectral characterisation of correlation immune combining
functions. IEEE Trans. Inf. Theory 34, 569–571 (1988) (p. 77)
286. Yang, B.Y., Chen, O.C.H., Bernstein, D.J., Chen, J.M.: Analysis of QUAD. In: Biryukov, A.
(ed.) Fast Software Encryption: 14th International Workshop, FSE 2007. Lecture Notes in
Computer Science, vol. 4593, pp. 290–308. Springer, Berlin (2007) (p. 256)
287. Yang, Y.X., Guo, B.: Further enumerating Boolean functions of cryptographic signiﬁcance.
J. Cryptol. 8, 115–122 (1995) (p. 81)
288. Zhang, J.Z., You, Z.S., Li, Z.L.: Enumeration of binary orthogonal arrays of strength 1.
Discrete Math. 239, 191–198 (2001) (p. 81)
289. Zigangirov, K.S.: Some sequential decoding procedures. Probl. Inf. Transm. 2, 13–25 (1966)
(p. 109)
290. Zsigmondy, K.: Zur Theorie der Potenzreste. Monatshefte Math. Phys. 3, 265–284 (1892)
(p. 297)

Index
0–9
2-adic numbers, 261
A
A5/1, 176
A5/2, 170–176
Accessible predicate, 243
Adjacency matrix, 62
Aggregating table, 284
Aircrack, 185
Algebraic attacks, 65
Algebraic normal form, 309
Algorithm
Berlekamp-Massey ~, 33
Almost bent function, 30
Alternating step generator, 157, 158
Alternative, 313
Ascending chain condition, 137
Asynchronous stream cipher, 5
Attack
~based on Goli´c’s correlation, 202–209
algebraic ~, 65
CJS ~, 91–105
correlation ~, 66
second round ~against RC4, 207–209
state recovering ~against RC4, 209–212
Auto key cipher, 5
Auto-correlation, 25
sequences with two level ~, 27
Auto-correlation test, 25
B
Balanced, 76
Balanced colorings of a hypercube, 81
Basis
Gröbner~, 137
BDD, 117
Beale cipher, 2
Bent function, 30
Berlekamp-Massey algorithm, 33
Berlekamp’s algorithm, 146
Bernoulli numbers, 329
Binary decision diagram, 117
Birthday paradox, 185
Black box linear algebra, 292
Bluetooth, 126
Blum-Blum-Shub generator, 244–247
Blum-Micali generator, 243
Borel σ-algebra, 311
Brownian motion, 322
Buchberger, Bruno, 137
Buchberger’s Algorithm, 140
C
Caesar cipher, 1
Carry save adder, 263
Characterization
~of m-sequences, 27
Chinese remainder theorem, 293
Cipher
asynchronous stream ~, 5
auto key ~, 5
homophone ~, 2
polyalphabetic ~, 2
polygraphic ~, 2
RC4 ~, 183
self-synchronizing stream ~, 5, 6
synchronous stream ~, 5
Vigenère ~, 3
CJS-attacks, 91–105
Code
cyclic redundancy ~(CRC), 189
Code of LFSR, 24
Column distance, 106
A. Klein, Stream Ciphers, DOI 10.1007/978-1-4471-5079-4,
© Springer-Verlag London 2013
395

396
Index
Combiner with memory, 149
Companion matrix, 18
Complete problem, 277
Complexity class
DSpace, 275
DTime, 275
NSpace, 275
NTime, 275
Conditional expectation, 316
Constancy on cyclotomic cosets, 26
Continued fraction, 46
Control graph, 124
Convolution, 101
Convolutional codes, 105–111
Coppersmith’s method, 303
Correlation
Goli´c’s ~, 203
Correlation attack, 66
Correlation immune function, 75
Correlation-attack, 91–115
Correlation-attack on the shrinking generator,
161
CRC-code, 189
Cross-correlation, 29
Cyclic redundancy check code, 189
Cyclotomic polynomial, 297
D
De Bruijn graph, 59
De Bruijn sequence, 59, 157
Decoding
sequential ~, 109–111
twice step ~, 101
Viterbi ~, 107–109
Degree
ω-~, 141
Degree reduction, 147
Deterministic test, 313
Dickson’s lemma, 136
Difference set, 27
Differential power analysis, 164
Digraph probabilities, 213
Distance proﬁle, 106, 111
Distinguisher, 242, 312
Distribution test, 25
Doob’s theorem, 319
DPA, 164
DSpace, 275
E
E0, 126, 153
Enigma, 8
Error probability
~of the ﬁrst kind, 313
~of the second kind, 314
Eulerian function, 294
Euler’s constant, 331
Euler’s summation formula, 330
Extended linearization, 145
F
F4 algorithm, 147
Fano metric, 110
Fast algebraic attacks, 149
Feedback polynomial, 19
irreducible ~, 20
primitive ~, 21
reducible ~, 21
Feedback shift register, 17
Fibonacci implementation, 51
Filter
non-linear ~, 72
Filtration, 317
Fisher Stern generator, 255
FMS-attack, 199–202
Formal language, 275
Fortuitous states, 218
Fourier transform, 101
Fourier transformation, 104, 105
Free binary decision diagrams, 124
Free distance, 106
Frobenius automorphism, 305
Function
almost bent ~, 30
bent ~, 30
Functional central limit theorem, 326
G
Galois group of a ﬁnite ﬁeld, 305
Galois implementation, 51
Geffe generator, 64, 143
Generating function, 19
Generator matrix, 24
GGHN generator, 222
Goli´c correlation, 203
Golomb axioms, 24
Graded lexicographic ordering, 132
Graded reverse lexicographic ordering, 132
Gram-Schmidt orthogonalization, 302
Gröbner basis, 137
Gröbner walk, 140
GSM protocol, 169
H
Hadamard difference set, 28
Hadamard matrix, 28
Hamming weight, see sideway addition
Hard problem, 277
Harmonic numbers, 331

Index
397
Hilbert’s basis theorem, 137
Homogeneous polynomial, 141
Homophone cipher, 2
Hypothesis, 313
I
Ideal
monomial ~, 135
Initial form, 141
Input hard predicate, 243
Involution, 332
J
Jacobi symbol, 300
K
Karatsuba algorithm, 247
Kerckhoffs’ principle, 1, 169
Key scheduling
~of RC4, 190–199
Knapsack generator, 255
Kullback-Leibner information, 320
L
Labeled tree, 334
Language
formal ~, 275
Laplacian matrix, 62
Lattice, 301
Laurent series, 46
Leading coefﬁcient, 134
Leading form, 140
Leading monomial, 134
Leading term, 134
Legendre symbol, 299
Lexicographic ordering, 132
LFSR, 17
closed formula of an ~sequence, 22
Fibonacci implementation of a ~, 51
Galois implementation of a ~, 51
linear complexity of an ~sequence, 31
non-linear combination of ~, 66
periods of ~, 22
product of two ~sequences, 67
sum of two ~sequences, 32
trace representation of an ~sequence, 20
Likelihood quotient test, 315
LILI-128, 151
Linear complexity, 31
~of the stop-and-go generator, 156
Linear complexity proﬁle, 45
good ~, 46
perfect ~, 45
Linear feedback shift register, see LFSR
Linearization, 143
M
M-sequence, 18, 24–30
characterization of a ~by the shift-and-add
property, 27
Martingale, 317
Martingale convergence theorem, 319
Massey, J.L., 77
Matrix
Hadamard ~, 28
Toeplitz ~, 31
Matrix order, 133
Measurable function, 311
Measurable space, 311
Measure, 311
Metric
Fano ~, 110
Möbius function, 306
Modular shift register generator, see Galois
implementation
Monic polynomial, 305
Monoalphabetic cipher, 2
Monomial ideal, 135
Monomial order, 132
Montgomery multiplication, 250
Mosquito, 235
Most powerful test, 314
Moustique, 235
MRSRG, see Galois implementation
MSRG, see Galois implementation
Multidegree, 134
Multiple-return shift register generator, see
Galois implementation
MXOR, 288
N
NESSIE-Project, 151
Next bit predictor, 242
Neyman-Pearson test, 314
Noetherian ring, 137
Non-linear ﬁlter, 72
Normal form
algebraic ~, 309
NSpace, 275
Ntime, 275
O
O-Notation, 272
ω-degree, 141
ω-homogeneous, 141
One-time pad, 4
Optimal non-linearity, 79
Optimal sampling theorem, 318

398
Index
Ordering on monomials, 132
Orthogonal array, 76
P
Pan-normal-form, 284
Parameter space, 313
Parity check matrix, 24
Pattern
~of RC4, 212
Perfect local randomizer, 256
Permanent, 74
Permutation, 332
Playfair cipher, 2
Polyalphabetic cipher, 2
Polygraphic cipher, 2
Polynomial
cyclotomic ~, 297
monic ~, 305
Population count, see sideway addition
Power generator, 253
Predictor
next bit ~, 242
previous bit ~, 242
Preferred pair, 30
Previous bit predictor, 242
Prime number theorem, 295
Primitive polynomial, 21
Protocol
WEP ~, 184
WPA ~, 185–187
Q
QUAD, 256
Quadratic reciprocity law, 300
R
Rabbit, 232
Randomized test, 313
RC4, 183
key scheduling of ~, 190–199
RC4A, 224
Reduction, 277
Reﬂection principle, 324
Relinearization, 144
Resilient, 76
Resultant, 69
Rotor machines, 8
RSA generator, 253
S
S-polynomial, 138
Self-shrinking generator, 342
Self-synchronizing stream cipher, 5, 6
Sequential decoding, 109–111, 113
Sequential likelihood ratio test, 319
Sequential test, 100
Serial test, 25
Shannon’s coding theorem, 93
Shift-and-add property, 26
Shrinking generator, 158–163
Side channel attacks, 163
Sideway addition, 53, 103, 262, 263, 343
Siegenthaler’s inequality, 79
σ-algebra, 311
Simple power analysis, 163
Simple shift register generator, see Fibonacci
implementation
SPA, 163
Sparse feedback polynomials, 114
Sparse linear algebra, 292
SSRG, see Fibonacci implementation
Step-once-twice generator, 157
Stop-and-go generator, 155–157
Stopping time, 318
Strassen’s algorithm, 280
Stream cipher
Mosquito, 235
Moustique, 235
Rabbit, 232
Trivium, 229
Strong Markov property, 324
Submartingale, 317
Subset sum generator, see knapsack generator
Supermartingale, 317
Symmetric group, 332
Synchronous stream cipher, 5
Syzygien polynomial, 138
T
Temporal key hash, 187
Term, 134
Test
auto-correlation ~, 25
distribution ~, 25
likelihood quotient ~, 315
most powerful ~, 314
Neyman-Pearson ~, 314
sequential likelihood ratio ~, 319
serial ~, 25
Test problem, 313
Test (statistical), 313
Toeplitz matrix, 31
Transform
Walsh ~, 30, 77
Tree diagram, 109
Trilinear form, 282
Trinomial, 308
Trivium, 229

Index
399
Turing machine, 273
Twice step decoding, 101
V
Variation distance, 312
Vertical bit count algorithm, 113
Vigenère cipher, 3
Viterbi decoding, 107–109, 112
Von-Neumann-generator, 234
W
Wald’s test, 319
Walsh spectrum, 30
Walsh transform, 30, 77
Well-ordering, 132
WEP, 184
Wiener process, 322
Wireless LAN, 184–190
WPA, 185–187
X
XL-Algorithm, 145
Z
Zsigmondy’s theorem, 71, 297

